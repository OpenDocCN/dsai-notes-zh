- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:52:03'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2108.08876] Deep Learning-based Spacecraft Relative Navigation Methods: A
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.08876](https://ar5iv.labs.arxiv.org/html/2108.08876)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning-based Spacecraft Relative Navigation Methods: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jianing Song¹¹1Equal contribution²²2Postdoctoral Research Fellow, Department
    of Electrical and Electronic Engineering [jianing.song@city.ac.uk](mailto:jianing.song@city.ac.uk)
    Duarte Rondao³³3Equal contribution⁴⁴4Postdoctoral Research Fellow, Department
    of Electrical and Electronic Engineering [duarte.rondao@city.ac.uk](mailto:duarte.rondao@city.ac.uk)
    Nabil Aouf⁵⁵5Professor of Robotics and Autonomous Systems, Department of Electrical
    and Electronic Engineering [nabil.aouf@city.ac.uk](mailto:nabil.aouf@city.ac.uk)
    City, University of London, ECV1 0HB London, United Kingdom
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Autonomous spacecraft relative navigation technology has been planned for and
    applied to many famous space missions. The development of on-board electronics
    systems has enabled the use of vision-based and LiDAR-based methods to achieve
    better performances. Meanwhile, deep learning has reached great success in different
    areas, especially in computer vision, which has also attracted the attention of
    space researchers. However, spacecraft navigation differs from ground tasks due
    to high reliability requirements but lack of large datasets. This survey aims
    to systematically investigate the current deep learning-based autonomous spacecraft
    relative navigation methods, focusing on concrete orbital applications such as
    spacecraft rendezvous and landing on small bodies or the Moon. The fundamental
    characteristics, primary motivations, and contributions of deep learning-based
    relative navigation algorithms are first summarised from three perspectives of
    spacecraft rendezvous, asteroid exploration, and terrain navigation. Furthermore,
    popular visual tracking benchmarks and their respective properties are compared
    and summarised. Finally, potential applications are discussed, along with expected
    impediments.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Deep learning , Space relative navigation , Terrain navigation , Asteroid exploration^†^†journal:
    Acta Astronautica\setabbreviationstyle'
  prefs: []
  type: TYPE_NORMAL
- en: '[acronym]long-postshort-user \glssetcategoryattributeacronymnohyperfirsttrue
    \DTMusemodulebritishen-GB'
  prefs: []
  type: TYPE_NORMAL
- en: Glossary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2D
  prefs: []
  type: TYPE_NORMAL
- en: two-dimensional
  prefs: []
  type: TYPE_NORMAL
- en: 3D
  prefs: []
  type: TYPE_NORMAL
- en: three-dimensional
  prefs: []
  type: TYPE_NORMAL
- en: AI
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence
  prefs: []
  type: TYPE_NORMAL
- en: ALHAT
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous Landing Hazard Avoidance Technology
  prefs: []
  type: TYPE_NORMAL
- en: ANN
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: BCE
  prefs: []
  type: TYPE_NORMAL
- en: Binary Cross-Entropy
  prefs: []
  type: TYPE_NORMAL
- en: CD
  prefs: []
  type: TYPE_NORMAL
- en: Crater Detection
  prefs: []
  type: TYPE_NORMAL
- en: CI
  prefs: []
  type: TYPE_NORMAL
- en: Crater Identification
  prefs: []
  type: TYPE_NORMAL
- en: CL
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layer
  prefs: []
  type: TYPE_NORMAL
- en: CNN
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: COCO
  prefs: []
  type: TYPE_NORMAL
- en: Common Objects in Context
  prefs: []
  type: TYPE_NORMAL
- en: CRO
  prefs: []
  type: TYPE_NORMAL
- en: Candidate for a Regional Object
  prefs: []
  type: TYPE_NORMAL
- en: DEM
  prefs: []
  type: TYPE_NORMAL
- en: Digital Elevation Map
  prefs: []
  type: TYPE_NORMAL
- en: DL
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: DNN
  prefs: []
  type: TYPE_NORMAL
- en: Deep Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: DoF
  prefs: []
  type: TYPE_NORMAL
- en: Degree-of-Freedom
  prefs: []
  type: TYPE_NORMAL
- en: DRCNN
  prefs: []
  type: TYPE_NORMAL
- en: Deep Recurrent Convolutional Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: EKF
  prefs: []
  type: TYPE_NORMAL
- en: Extended Kalman Filter
  prefs: []
  type: TYPE_NORMAL
- en: ESA
  prefs: []
  type: TYPE_NORMAL
- en: European Space Agency
  prefs: []
  type: TYPE_NORMAL
- en: FCL
  prefs: []
  type: TYPE_NORMAL
- en: Fully Connected Layer
  prefs: []
  type: TYPE_NORMAL
- en: FPGA
  prefs: []
  type: TYPE_NORMAL
- en: Field-Programmable Gate Array
  prefs: []
  type: TYPE_NORMAL
- en: GPOPS II
  prefs: []
  type: TYPE_NORMAL
- en: General Purpose Optimal Control Software
  prefs: []
  type: TYPE_NORMAL
- en: HDA
  prefs: []
  type: TYPE_NORMAL
- en: Hazard Detection and Avoidance
  prefs: []
  type: TYPE_NORMAL
- en: HRNet
  prefs: []
  type: TYPE_NORMAL
- en: High-Resolution Net
  prefs: []
  type: TYPE_NORMAL
- en: ICP
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Closest Point
  prefs: []
  type: TYPE_NORMAL
- en: KPEC
  prefs: []
  type: TYPE_NORMAL
- en: Kelvins Pose Estimation Challenge
  prefs: []
  type: TYPE_NORMAL
- en: KRN
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint Regression Network
  prefs: []
  type: TYPE_NORMAL
- en: LCLF
  prefs: []
  type: TYPE_NORMAL
- en: Lunar-Centred, Lunar-Fixed Coordinates
  prefs: []
  type: TYPE_NORMAL
- en: LoG
  prefs: []
  type: TYPE_NORMAL
- en: Laplacian of Gaussian
  prefs: []
  type: TYPE_NORMAL
- en: LRO
  prefs: []
  type: TYPE_NORMAL
- en: Lunar Reconnaissance Orbiter
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory
  prefs: []
  type: TYPE_NORMAL
- en: LVLH
  prefs: []
  type: TYPE_NORMAL
- en: Local-Vertical, Local-Horizontal
  prefs: []
  type: TYPE_NORMAL
- en: ML
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: MLP
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer Perceptron
  prefs: []
  type: TYPE_NORMAL
- en: MSE
  prefs: []
  type: TYPE_NORMAL
- en: Mean Square Error
  prefs: []
  type: TYPE_NORMAL
- en: NASA
  prefs: []
  type: TYPE_NORMAL
- en: National Aeronautics and Space Administration
  prefs: []
  type: TYPE_NORMAL
- en: NEA
  prefs: []
  type: TYPE_NORMAL
- en: Near-Earth Asteroid
  prefs: []
  type: TYPE_NORMAL
- en: NN
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: ODN
  prefs: []
  type: TYPE_NORMAL
- en: Object Detection Network
  prefs: []
  type: TYPE_NORMAL
- en: PDS
  prefs: []
  type: TYPE_NORMAL
- en: Planetary Data System
  prefs: []
  type: TYPE_NORMAL
- en: P$n$P
  prefs: []
  type: TYPE_NORMAL
- en: Perspective-n-Point
  prefs: []
  type: TYPE_NORMAL
- en: PyCDA
  prefs: []
  type: TYPE_NORMAL
- en: Python Crater Detection Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: RANSAC
  prefs: []
  type: TYPE_NORMAL
- en: Random Sample Consensus
  prefs: []
  type: TYPE_NORMAL
- en: R-CNN
  prefs: []
  type: TYPE_NORMAL
- en: Region-based Convolutional Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: RGB
  prefs: []
  type: TYPE_NORMAL
- en: Red-Green-Blue
  prefs: []
  type: TYPE_NORMAL
- en: RMSE
  prefs: []
  type: TYPE_NORMAL
- en: Root Mean Square Error
  prefs: []
  type: TYPE_NORMAL
- en: RNN
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: RoI
  prefs: []
  type: TYPE_NORMAL
- en: Region of Interest
  prefs: []
  type: TYPE_NORMAL
- en: RPN
  prefs: []
  type: TYPE_NORMAL
- en: Region Proposal Network
  prefs: []
  type: TYPE_NORMAL
- en: S/C
  prefs: []
  type: TYPE_NORMAL
- en: spacecraft
  prefs: []
  type: TYPE_NORMAL
- en: SoC
  prefs: []
  type: TYPE_NORMAL
- en: System-on-a-Chip
  prefs: []
  type: TYPE_NORMAL
- en: SPEED
  prefs: []
  type: TYPE_NORMAL
- en: Spacecraft Pose Estimation Dataset
  prefs: []
  type: TYPE_NORMAL
- en: SPN
  prefs: []
  type: TYPE_NORMAL
- en: Spacecraft Pose Network
  prefs: []
  type: TYPE_NORMAL
- en: TRN
  prefs: []
  type: TYPE_NORMAL
- en: Terrain Relative Navigation
  prefs: []
  type: TYPE_NORMAL
- en: URSO
  prefs: []
  type: TYPE_NORMAL
- en: Unreal Rendered Spacecraft On-Orbit
  prefs: []
  type: TYPE_NORMAL
- en: VO
  prefs: []
  type: TYPE_NORMAL
- en: Visual Odometry
  prefs: []
  type: TYPE_NORMAL
- en: WAC
  prefs: []
  type: TYPE_NORMAL
- en: Wide Angle Camera
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent years, there has been a growing interest in [Artificial Intelligence
    (AI)](#glo.main.ai), [Machine Learning (ML)](#glo.main.ml), and [Deep Learning
    (DL)](#glo.main.dl), especially amongst science, technology, engineering, and
    mathematics disciplines. There have been several approaches to define [AI](#glo.main.ai)
    historically; the most common refers to techniques enabling machines to mimic
    human intelligence. Then, [ML](#glo.main.ml) is the key component responsible
    for automatically processing data inside an [AI](#glo.main.ai). A [Neural Network
    (NN)](#glo.main.nn) is a specific [ML](#glo.main.ml) model aiming to approximate
    a certain function $f^{\ast}$ relating training examples ${\bm{x}}$ to labels
    $y$ by defining a mapping $y=f({\bm{x}},{\bm{\theta}})$ and learning the ${\bm{\theta}}^{\ast}$
    parameters that result in the best approximation. [NNs](#glo.main.nn) work by
    stacking many different functions, called layers, and the number of layers defines
    the depth of the [NN](#glo.main.nn). The term [DL](#glo.main.dl) derives from
    this wording, typically signifying a [NN](#glo.main.nn) with large depth [[1](#bib.bib1)].
    A rough relationship among these three concepts is summarised and illustrated
    in Fig. [7](#footnote7 "footnote 7 ‣ Figure 1 ‣ 1 Introduction ‣ Deep Learning-based
    Spacecraft Relative Navigation Methods: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the field of space exploration, autonomous vision-based [spacecraft (S/C)](#glo.main.sc)
    navigation is one key area with the potential of greatly benefiting from \glsxtrshortdnn-based
    (\glsxtrlong*dnn) estimation methods. Cameras are rapidly becoming the preferred
    sensor for autonomous rendezvous thanks to the introduction of compact and lightweight
    passive optical sensors as feasible onboard instruments [[2](#bib.bib2)]. Additionally,
    vision-based techniques have been used in-flight for deep space navigation tasks
    [[3](#bib.bib3)]. Potential future applications of domains include: 1) non-cooperative
    rendezvous with a spacecraft; 2) terrain navigation for descent and landing; and
    3) asteroid explorations and asteroid patch pinpoint localisation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9501a22acbe0b2f5860b723973f5069.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Relationship between [AI](#glo.main.ai), [ML](#glo.main.ml) and [DL](#glo.main.dl)
    (reproduced from Mathworks.⁷⁷7[https://explore.mathworks.com/machine-learning-vs-deep-learning/chapter-1-129M-833I7.html](https://explore.mathworks.com/machine-learning-vs-deep-learning/chapter-1-129M-833I7.html).)'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these scenarios involve the estimation of a chaser or lander spacecraft’s
    relative state, typically through the six [Degree-of-Freedom (DoF)](#glo.main.dof)
    pose ${\bm{T}}_{ct}$ of the target object frame $\underaccent{\vec{}}{\bm{\mathcal{F}}}_{t}$
    relative to the chaser frame $\underaccent{\vec{}}{\bm{\mathcal{F}}}_{c}$, composed
    of a rotation ${\bm{R}}_{ct}$, and a translation $\prescript{c}{}{{\bm{t}}}_{ct}$
    (see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning-based Spacecraft
    Relative Navigation Methods: A Survey")). Pose estimation methods have traditionally
    worked by relating features of the target (expressed in $\underaccent{\vec{}}{\bm{\mathcal{F}}}_{t}$),
    typically obtained from a model, to their images captured by the onboard camera
    (expressed in $\underaccent{\vec{}}{\bm{\mathcal{F}}}_{c}$), whereas using [DNN](#glo.main.dnn)
    models would adequately capture the intrinsic nonlinearities between the input
    sensor data and the state estimates, especially for images or [Digital Elevation
    Maps (DEMs)](#glo.main.dem).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6c619fa8453a8933de24dbfaa5e5bc7a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Rendezvous with non-cooperative spacecraft
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fffcc6314257398bd341bfe04f501ced.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Asteroid pinpointing via patch classification
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e4a253d6fe1a6510ea04c60a8a65c0a.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Terrain navigation
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Identification of potential relative navigation scenarios for the
    application of \Glsxtrshortpldnn.'
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies have approached the topic of [DL](#glo.main.dl)-based navigation
    for space. Kothari et al. [[4](#bib.bib4)] collated various applications of [DL](#glo.main.dl)
    for space, briefly discussing the achieved and prospective goals of onboard systems
    for spacecraft positioning during docking and landing. Aiming at non-cooperative
    spacecraft rendezvous specifically, Cassinis et al. [[5](#bib.bib5)] first provided
    a review of \glsxtrshortcnn-based (\glsxtrlong*cnn) schemes in the context of
    monocular pose estimation systems discussing in detail several works (e.g.  [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)]).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a53f953f610c5234c599fddedc54a2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The tree diagram of [DL](#glo.main.dl)-based [S/C](#glo.main.sc)
    relative navigation approaches reviewed in this paper. The boxes in yellow, blue,
    and white represent applications, methods, and candidate references, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a shortage of comparative analysis of [DL](#glo.main.dl)
    methods for general relative navigation in space. With this survey, we thus intend
    to bridge this gap and provide a comprehensive reference for researchers and engineers
    aspiring to leverage deep learning for this subject, specifically for the three
    main applications identified in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep
    Learning-based Spacecraft Relative Navigation Methods: A Survey"). Fig. [3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Deep Learning-based Spacecraft Relative Navigation
    Methods: A Survey") shows the set of research methods and application domains
    covered by our survey. In Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Learning-based
    Spacecraft Relative Navigation Methods: A Survey"), direct [DNN](#glo.main.dnn)
    methods are end-to-end methods using [DNNs](#glo.main.dnn), which constitute a
    direct, uninterrupted pipeline from inputs $x$ to the desired quantity to estimate
    $y$. In contrast, indirect [DNN](#glo.main.dnn) methods are those in which the
    [DNN](#glo.main.dnn) is exclusively tasked with performing the image processing
    functions on the input, while the actual quantity to be estimated is achieved
    by combining this output with other methods, such as classical ML, geometry-based
    optimisation, and Kalman filtering.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper is organized as follows. Section [2](#S2 "2 \glsfmtshortdl-based
    Pose Estimation for Spacecraft Relative Navigation ‣ Deep Learning-based Spacecraft
    Relative Navigation Methods: A Survey") presents a review of [DL](#glo.main.dl)-based
    pose estimation algorithms for spacecraft rendezvous. Section [3](#S3 "3 Crater
    and Hazard Detection for Terrain Navigation Using \glsfmtshortdl ‣ Deep Learning-based
    Spacecraft Relative Navigation Methods: A Survey") contains a detailed review
    of crater and hazard detection of [Terrain Relative Navigation (TRN)](#glo.main.trn)
    using [DNNs](#glo.main.dnn). Section [4](#S4 "4 \glsfmtshortdl-based Relative
    Navigation for Asteroid Research ‣ Deep Learning-based Spacecraft Relative Navigation
    Methods: A Survey") provides a review of [DL](#glo.main.dl) techniques with a
    focus on asteroid exploration. Finally, Section [5](#S5 "5 Summary and Conclusion
    ‣ Deep Learning-based Spacecraft Relative Navigation Methods: A Survey") lists
    the main conclusions and discussions.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 \glsfmtshortdl-based Pose Estimation for Spacecraft Relative Navigation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Related Works on Terrestrial Pose Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the successful application of deep learning approaches in various research
    areas, \glsxtrshortdl-based camera-relative pose determination techniques for
    terrestrial scenarios have been attracting a considerable amount of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Kendall et al. [[9](#bib.bib9)] proposed the PoseNet architecture for 6-[DoF](#glo.main.dof)
    motion estimation in an end-to-end manner. To develop the pose regression network,
    they used a modified pre-trained GoogLeNet [[10](#bib.bib10)] by replacing all
    softmax classifiers with affine regressor. A weighted sum of the $L^{2}$ error
    norms of the position vector and the attitude quaternion is selected as the loss
    function for better training of the location and orientation simultaneously. Their
    results demonstrate a $2\text{\,}\mathrm{m}$ and $3\text{\,}\mathrm{d}\mathrm{e}\mathrm{g}$
    accuracy for large scale outdoor scenes and $0.5\text{\,}\mathrm{m}$ and $5\text{\,}\mathrm{d}\mathrm{e}\mathrm{g}$
    accuracy indoors.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than self-localising with respect to a known world model, Wang et al.
    [[11](#bib.bib11)] presented the DeepVO architecture to obtain a vehicle’s egomotion
    from frame to frame based on monocular [Visual Odometry (VO)](#glo.main.vo). The
    pipeline follows the architecture of a [Deep Recurrent Convolutional Neural Network
    (DRCNN)](#glo.main.drcnn) [[12](#bib.bib12)], in which a pre-trained FlowNet [[13](#bib.bib13)]
    first learns features from sequences of [Red-Green-Blue (RGB)](#glo.main.rgb)
    images, which are then processed by [Long Short-Term Memory (LSTM)](#glo.main.lstm)
    cells to estimate poses. The end-to-end [DRCNN](#glo.main.drcnn) framework achieves
    an average [Root Mean Square Error (RMSE)](#glo.main.rmse) drift of $5.96\text{\,}\mathrm{\char
    37\relax}$ and $6.12\text{\,}\mathrm{d}\mathrm{e}\mathrm{g}$ per trajectory for
    position and attitude, respectively, on lengths of $100\text{\,}\mathrm{m}800\text{\,}\mathrm{m}$,
    showing a competitive performance relative to Monocular VISO2 [[14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: Differing from the above end-to-end, or direct, methods, some works opt instead
    by following indirect methods, for which the [DNN](#glo.main.dnn) is exclusively
    tasked with performing the image processing functions on the input, while the
    actual quantity to be estimated is achieved by combing this output with other
    methods, such as classical [ML](#glo.main.ml) or Kalman filtering. For instance,
    Rad and Lepetit [[15](#bib.bib15)] developed the BB8 algorithm for object pose
    estimation by combining a [CNN](#glo.main.cnn) to regress the [two-dimensional
    (2D)](#glo.main.2d) locations of the eight [three-dimensional (3D)](#glo.main.3d)
    points defining their bounding box with a [Perspective-n-Point (P$n$P)](#glo.main.pnp)
    algorithm [[16](#bib.bib16)] to retrieve the pose based on those correspondences.
    The VGG architecture [[17](#bib.bib17)] was chosen as the basis for their work,
    and the classical reprojection (or geometric) error was used as the corresponding
    loss function [[18](#bib.bib18)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c106dc173fd75732405326547fafaff7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Direct versus indirect methods for \Glsxtrshortdl-based pose estimation.
    The former use a \Glsxtrshortdnn to directly estimate the pose from the input,
    whereas the latter use it exclusively to identify features, or landmarks, on the
    target, which are then input to a \Glsxtrshortml algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Related Works on Terrestrial Pose Estimation
    ‣ 2 \glsfmtshortdl-based Pose Estimation for Spacecraft Relative Navigation ‣
    Deep Learning-based Spacecraft Relative Navigation Methods: A Survey") illustrates
    the difference between direct and indirect methods, which are explored further
    in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Challenges and Motivations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent advancements in [DL](#glo.main.dl) exhibit promising alternatives with
    respect to classical approaches, and related terrestrial frameworks also inspire
    the idea of \glsxtrshortdl-based spacecraft relative navigation. However, there
    still exists a gap between the two domains of application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Relative pose estimation of objects in space is a different problem from pose
    determination of objects on Earth due to the vast differences in environment.
    Additionally, real labelled on-orbit images required for training [DL](#glo.main.dl)
    algorithms are expensive and hard to obtain, which leads to a lack of space imagery
    datasets. Challenges in space missions for applying vision-based [DL](#glo.main.dl)
    methods can be summarised from previous research [[7](#bib.bib7), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)] as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planets and stars acting as background distractors for the navigation system;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenging visual conditions due to lack of atmosphere and light diffusion;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much stronger shadows and varied illumination conditions resulting in extreme
    image contrast and low signal-to-noise ratio;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limited properties of space hardware in power consumption and computational
    resources (e.g., low sensor resolution);
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training datasets for non-cooperative navigation of spaceborne objects are scarce;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concerns over the reliability of [DL](#glo.main.dl) technique preventing their
    practice in the space industry.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The characteristics of space images also challenge conventional vision-based
    navigation algorithms for spacecraft, while the [DL](#glo.main.dl) technique provides
    promising solutions and performance that can alleviate these issues. In terms
    of dynamic lighting, \glsxtrshortdnn-based schemes show increased robustness in
    attitude initialisation [[22](#bib.bib22), [27](#bib.bib27)]. With the deployment
    of high-performance devices, \glsxtrshortcnn-based methods can not only provide
    a lower computational complexity in pose acquisition process, but also reduce
    the need for complicated dynamics models [[28](#bib.bib28)]. Additionally, [DNN](#glo.main.dnn)
    pipelines are able to output various information and be combined with navigation
    filters or other processes [[25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by the attractiveness described above, and to overcome current limitations
    in spacecraft relative pose estimation, [European Space Agency (ESA)](#glo.main.esa)
    launched the [Kelvins Pose Estimation Challenge (KPEC)](#glo.main.kpec) ⁸⁸8[https://kelvins.esa.int/satellite-pose-estimation-challenge](https://kelvins.esa.int/satellite-pose-estimation-challenge).
    in 2019, inviting the community to propose and validate new approaches directly
    from greyscale images acquired by an on-board camera. The data for training and
    testing in this challenge consisted of Stanford’s [Spacecraft Pose Estimation
    Dataset (SPEED)](#glo.main.speed), which contains labelled synthetic images of
    the Tango satellite, and a smaller, real set of images acquired in laboratory
    using a replica of the target.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Direct Frameworks for Spacecraft Relative Pose Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this survey, direct \glsxtrshortdl-based frameworks are defined as those
    in which the estimation of the desired quantity is entirely relayed to the [DNN](#glo.main.dnn),
    thus forming a continuous, uninterrupted pipeline from input to output. For spacecraft
    relative navigation, the problem is posited as estimating the 6-[DoF](#glo.main.dof)
    pose of a target, $\underaccent{\vec{}}{\bm{\mathcal{F}}}_{t}$, in the frame of
    reference of a chaser, $\underaccent{\vec{}}{\bm{\mathcal{F}}}_{c}$ (as shown
    in Fig. [2(a)](#S1.F2.sf1 "In Figure 2 ‣ 1 Introduction ‣ Deep Learning-based
    Spacecraft Relative Navigation Methods: A Survey")). The target may be non-cooperative,
    in which case it will not relay any explicit information to the chaser’s onboard
    navigation system, and the relative pose is estimated from acquired images of
    the target only.'
  prefs: []
  type: TYPE_NORMAL
- en: By partitioning the relative pose space into discrete hypotheses, a classification
    framework may be established if the target spacecraft has a known model. Sharma
    et al. [[7](#bib.bib7)] have proposed a deep [CNN](#glo.main.cnn) for relative
    pose classification of non-cooperative spacecraft. Taking advantage of transfer
    learning, AlexNet model [[29](#bib.bib29)] pre-trained on the large ImageNet dataset
    [[30](#bib.bib30)] is modified by replacing the last few layers to adapt to the
    space imagery of the Tango spacecraft flown in the Prisma mission [[31](#bib.bib31)].
    Ten datasets with different added noises are created from synthetic images. The
    proposed approach demonstrated greater accuracy than a baseline method using classical
    pose estimation techniques from [2D](#glo.main.2d)-[3D](#glo.main.3d) feature
    matching but is deemed not fine enough for any application other than a coarse
    initialisation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sharma and D’Amico [[8](#bib.bib8)] later on improve their original work with
    the creation of the [Spacecraft Pose Network (SPN)](#glo.main.spn). The [SPN](#glo.main.spn)
    (Fig. [5](#S2.F5 "Figure 5 ‣ 2.3 Direct Frameworks for Spacecraft Relative Pose
    Estimation ‣ 2 \glsfmtshortdl-based Pose Estimation for Spacecraft Relative Navigation
    ‣ Deep Learning-based Spacecraft Relative Navigation Methods: A Survey")) uses
    a five-layer [CNN](#glo.main.cnn) backbone of which the activations are connected
    to three different branches. The first branch uses the Faster [Region-based Convolutional
    Neural Network (R-CNN)](#glo.main.rcnn) architecture [[32](#bib.bib32)] to detect
    the [2D](#glo.main.2d) bounding box of the target in the input image. To be robust
    towards intrusive background elements (i.e., presence of Earth), specific features
    output by the final activation map of the first branch are extracted using [R-CNN](#glo.main.rcnn)’s
    [Region of Interest (RoI)](#glo.main.roi) pooling technique, and then fed to the
    other two branches of the [CNN](#glo.main.cnn) containing three fully connected
    layers.'
  prefs: []
  type: TYPE_NORMAL
- en: The second branch classifies the target attitude in terms of a probability distribution
    of discrete classes. It minimises a standard cross-entropy loss for the $N$ closest
    attitude labels in the viewsphere. Lastly, the third branch takes the $N$ candidates
    obtained from the previous branch and minimises another cross-entropy loss to
    yield the relative weighting of each. The final refined attitude is obtained via
    quaternion averaging with resort to the computed weights, which can be seen as
    a soft classification method.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee137b520d4c2072fa16f2fc9150e89d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The \glsxtrlong*spn (\glsxtrshortspn) architecture. Reproduced from
    Sharma and D’Amico [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, the [SPN](#glo.main.spn) utilises a Gauss-Newton algorithm to
    solve a minimisation problem for the estimate of relative position, for which
    the required initial guess is obtained from the bounding box (analogously to Kehl
    et al. [[34](#bib.bib34)]). The network is initially trained on the ImageNet dataset,
    and then the branch layers are further trained with an $80\text{\,}\mathrm{\char
    37\relax}20\text{\,}\mathrm{\char 37\relax}$ train-validation split on the [SPEED](#glo.main.speed)
    dataset. As they report, the [SPN](#glo.main.spn) method performs at degree-level
    and centimetre-level on relative attitude and position error, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In Ref. [[33](#bib.bib33)], Sharma and D’Amico expand their conference paper
    [[8](#bib.bib8)] by discussing two features of the [SPN](#glo.main.spn), target-in-target
    pose estimation and uncertainty quantification. The capability of estimating the
    uncertainty associated with the estimated pose of the [SPN](#glo.main.spn) emphasises
    that [SPN](#glo.main.spn) can be integrated with conventional navigation filters.
    Additionally, the authors detail the proposed [SPEED](#glo.main.speed) dataset,
    considering the solar illumination of the synthetic images and the ground truth
    calibration of the relative pose by the real images. The [SPN](#glo.main.spn)
    is also trained in three versions by using different datasets, including [SPEED](#glo.main.speed),
    ”Apogee Motor”, ”Imitation-25”, and ”PRISMA-25”. Experiments are also carried
    out to demonstrate two key features of [SPN](#glo.main.spn) method and compare
    it with their previous work, namely \glsxtrshortcnn-based [[7](#bib.bib7)] and
    image processing-based feature detection and correspondence [[35](#bib.bib35)]
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of employing a bounding box feature detection, Proença and Gao [[36](#bib.bib36)]
    modify a pre-trained ResNet architecture [[37](#bib.bib37)] with initial weights
    trained on the [Common Objects in Context (COCO)](#glo.main.coco) dataset to keep
    spatial feature resolution. Similarly to Ref. [[8](#bib.bib8)], two branches are
    designed to estimate [3D](#glo.main.3d) location and orientation, respectively.
    The position estimation consists of a simple regression branch with two fully
    connected layers and the relative error is minimised for better generalisation
    in terms of loss weight magnitudes. The continuous attitude estimation is then
    realised via a soft classification method [[38](#bib.bib38)]. Additionally, the
    authors present their own synthetic [Unreal Rendered Spacecraft On-Orbit (URSO)](#glo.main.urso)
    dataset for training featuring Soyuz. Experiments on renders of [URSO](#glo.main.urso)
    and [SPEED](#glo.main.speed) datasets are conducted to evaluate the proposed framework,
    with which their model achieved a third and a second place on the synthetic and
    real test set categories of [SPEED](#glo.main.speed) in [KPEC](#glo.main.kpec),
    respectively. Moreover, the experimental results show that estimating the orientation
    by soft classification performs better than direct regression methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/05b53d1699b53beb85fbb4032d3fdd44.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Simplified architecture model
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ada0fc472a650429a8980446f94a4460.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) ResNet block
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: The [CNN](#glo.main.cnn) pipeline in Ref. [[36](#bib.bib36)]. The
    [CNN](#glo.main.cnn) front-end is based on ResNet, of which the elementary blocks
    implement skip connections that help mitigate the vanishing gradient problem in
    very deep networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Hirano et al. [[24](#bib.bib24)] present a [3D](#glo.main.3d) keypoint estimator
    by using an AlexNet-based [CNN](#glo.main.cnn) architecture to regress spacecraft
    pose information directly, rather than retrieving [3D](#glo.main.3d) objects from
    the location of [2D](#glo.main.2d) keypoints. The parameters of AlexNet are changed
    for the purpose of the pose estimation task, and batch normalisation layers [[39](#bib.bib39)]
    are utilised in all [Convolutional Layer (CL)](#glo.main.cl) and [Fully Connected
    Layer (FCL)](#glo.main.fl) for convergence in training. Synthesised images of
    a [3D](#glo.main.3d) model with the [3D](#glo.main.3d) keypoint position labels
    are generated on the Gazebo simulator [[40](#bib.bib40)] and used to train the
    [CNN](#glo.main.cnn). Real images taken by hardware simulators are imported to
    evaluate the trained [CNN](#glo.main.cnn). Images in both training and test dataset
    include the effects of lighting, shadows, and random noise, which leads the proposed
    framework to a potential application in practical space missions.
  prefs: []
  type: TYPE_NORMAL
- en: Arakawa et al. [[41](#bib.bib41)] also treat the attitude estimation as a \glsxtrshortcnn-based
    regression problem to obtain spacecraft attitude quaternion from the constructed
    images, in which the output of the proposed [CNN](#glo.main.cnn) is four independent
    real numbers corresponding to four quaternion elements. A [3D](#glo.main.3d) model
    of the JCSAT-3 satellite is built in the Blender software to generate a training
    image dataset. A point spread function is applied to the renders for simulating
    atmospheric fluctuations and optical effects. Compared with conventional image
    matching approaches, their results clarify an improved performance on the accuracy,
    robustness, and computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that natural feature-based methods for spacecraft pose estimation
    are not always sufficient, Sonawani et al. [[19](#bib.bib19)] develop a modified
    model to assist a cooperative object tracker in space assembly tasks. The proposed
    [CNN](#glo.main.cnn) architecture is similar to Ref. [[7](#bib.bib7)], but uses
    VGG-19 as a backbone and replace the last layer with a 7-node one instead of an
    activation function. Two different models, namely a branch-based model and a parallel-based
    model, are developed to estimate relative poses. The frameworks of the two models
    are illustrated in Fig. [7](#S2.F7 "Figure 7 ‣ 2.3 Direct Frameworks for Spacecraft
    Relative Pose Estimation ‣ 2 \glsfmtshortdl-based Pose Estimation for Spacecraft
    Relative Navigation ‣ Deep Learning-based Spacecraft Relative Navigation Methods:
    A Survey"), in which the parallel model contains two parallel streams for position
    prediction and attitude estimation, respectively. Synthetic images are generated
    in Gazebo, including truss-shaped objects labelled with the pose. The Euclidean
    distance error between the predicted poses and actual ones is defined as the loss
    function. Simulation results show their models are comparable to the current feature-selection
    methods and are robust to other types of spacecraft.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/52368456eeebf5ab463229998af35f0a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Branch model
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d635e484e226970fd1efcfb96b4e8439.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Parallel model
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: The VGG-19-based architecture of Sonawani et al. [[19](#bib.bib19)].
    The branch is used to preserve feature-position information discarded by the later
    pooling layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Aiming at vision-based uncooperative docking operations, Phisannupawong et al.
    [[42](#bib.bib42)] construct a spacecraft pose estimation model by proposing an
    advanced GoogLeNet pre-trained on [URSO](#glo.main.urso). The original GoogLeNet
    framework is modified by using 23 layers of [CNN](#glo.main.cnn) presented in
    Ref. [[9](#bib.bib9)], and the output for spacecraft pose is a seven-element vector.
    Experiments are carried out with an exponential loss function and a weighted Euclidean
    loss function, separately. The simulating results suggest that the weighted Euclidean-based
    pose estimation model successfully achieves moderately high prediction accuracy,
    but the exponential-based model results in poor orientation estimation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of estimating poses at individual timesteps, Kechagias-Stamatis et al.
    [[43](#bib.bib43)] propose a [DRCNN](#glo.main.drcnn) to regress the relative
    pose of spacecraft from frame to frame. For a relative spacecraft navigation system,
    these chained poses serve as continuous outputs, of which the continuity is vital
    to autonomous missions such as rendezvous and formation flyover. Specifically,
    the [DRCNN](#glo.main.drcnn) consists of a [CNN](#glo.main.cnn) module and followed
    a [LSTM](#glo.main.lstm) module to extract features of the input images and automatically
    modelling the relative dynamics, respectively (see Fig. [8](#S2.F8 "Figure 8 ‣
    2.3 Direct Frameworks for Spacecraft Relative Pose Estimation ‣ 2 \glsfmtshortdl-based
    Pose Estimation for Spacecraft Relative Navigation ‣ Deep Learning-based Spacecraft
    Relative Navigation Methods: A Survey")). [3D](#glo.main.3d) lidar data is projected
    onto the image plane, yielding three different [2D](#glo.main.2d) depth images
    to be processed by a regular [CNN](#glo.main.cnn). As in Ref. [[11](#bib.bib11)],
    the loss minimises the pose [Mean Square Error (MSE)](#glo.main.mse), but the
    attitude is represented via a direction cosine matrix. Trials are conducted on
    both synthetic and real data. For the former, the Elite target satellite platform
    is used to create a self-occluded point cloud. The real dataset is acquired with
    a scaled mock-up of Envisat. Their results on both simulated and real lidar data
    scenarios demonstrate that the [DRCNN](#glo.main.drcnn) achieves better odometry
    accuracy at lower computational requirements than current algorithms such as [Iterative
    Closest Point (ICP)](#glo.main.icp) [[44](#bib.bib44)] and descriptor matching
    with $H_{\infty}$ filtering.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/be6a8fee6c13f798dbf2ddd7c3bddce9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The [DRCNN](#glo.main.drcnn) architecture of Kechagias-Stamatis et al.
    [[43](#bib.bib43)]. A shallow [CNN](#glo.main.cnn) architecture is utilised to
    extract low-level features for projected images, which are then modelled with
    [LSTMs](#glo.main.lstm).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Oestreich et al. [[22](#bib.bib22)] study on-orbit relative pose initialisation
    by employing AlexNet-based transfer learning and a post-classification attitude
    refinement algorithm, which provides a foundation for future work in \glsxtrshortcnn-based
    spacecraft pose initialisation. Their research puts focus on answering several
    questions on the applicability of [DL](#glo.main.dl) to this domain, including
    the necessary amount of training imagery, attitude label discretisation, and the
    effects of lighting and image background on [CNN](#glo.main.cnn) performance.
    Thus, AlexNet, used as the backbone of the proposed framework, only changes the
    final [FCL](#glo.main.fl) to yield attitude labels. The output attitude, obtained
    from a single branch unlike Ref. [[8](#bib.bib8)], is then refined using the eight
    most likely labels via direction cosine matrix averaging. Synthetic images of
    the SpaceX Dragon capsule are rendered using Blender at a fixed range of $20\text{\,}\mathrm{m}$.
    Four different synthetic image sets are created to study the performance of the
    presented scheme and answer the proposed questions, namely considering a black
    and empty background, Sun angle variation, Earth background variation, and sensor
    noise. Based on their experimental results, it is indicated that: 1) both classification
    accuracy and attitude error exhibit an asymptotic trend; 2) the [CNN](#glo.main.cnn)
    performs well in more challenging light conditions of the Sun variation dataset
    but poorly for the Earth background and sensor noise datasets; and 3) using the
    confidence rejection threshold in the refinement step can improve estimation accuracy
    slightly.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Cosmas and Kenichi [[23](#bib.bib23)] first investigated the feasibility
    of \glsxtrshortcnn-based spacecraft pose estimation by assessing the onboard inference
    capabilities of the model. Accounting for power consumption and cost-effectiveness,
    the Xilinx Zynq Ultrascale+ multiprocessor [System-on-a-Chip (SoC)](#glo.main.soc)
    hybrid [Field-Programmable Gate Array (FPGA)](#glo.main.fpga) is proposed as a
    suitable solution. Two typical approaches and one presented framework are trained
    in Google Colab using the [SPEED](#glo.main.speed) dataset, showing that a U-Net-based
    detection network performs better than the ResNet-50 based direct regression scheme,
    albeit poorer than the developed ResNet34-U-Net model. Later, the ResNet34-U-Net
    pipeline is implemented on the proposed hardware, starting with a YOLOv3 for [RoI](#glo.main.roi)
    detection, followed by a landmark localisation network to predict keypoints. Inference
    experiments, including an evaluation of the performance, compared to a desktop-based
    implementation, [DL](#glo.main.dl) processing unit resource utilisation, and power
    consumption are analysed with results of satisfactory accuracy and low on-chip
    power consumption of $3.5\text{\,}\mathrm{W}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a clear comparison between the aforementioned approaches, Table [1](#S2.T1
    "Table 1 ‣ 2.3 Direct Frameworks for Spacecraft Relative Pose Estimation ‣ 2 \glsfmtshortdl-based
    Pose Estimation for Spacecraft Relative Navigation ‣ Deep Learning-based Spacecraft
    Relative Navigation Methods: A Survey") summarises the surveyed \glsxtrshortdl-based
    direct frameworks for relative pose estimation. As shown, over half of the solutions
    employ transfer learning, which traditionally has also been considered by the
    most successful applications of [DNNs](#glo.main.dnn) to terrain navigation problems.
    In terms of framework types, most are seen to adopt an estimation by regression
    or soft classifier. Open datasets of real images are limited; only PRISMA-25 and
    [SPEED](#glo.main.speed) are available. On the other hand, synthetic imagery can
    be simulated by different software or platforms, such as OpenGL, Gazebo, Unreal
    Engine 4, and Blender, leading to datasets such as [URSO](#glo.main.urso). Moreover,
    the recent research output volume demonstrates there is an increasing interest
    in \glsxtrshortdl-based spacecraft pose estimation, including even a first report
    on onboard implementations with [FPGAs](#glo.main.fpga).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of \glsxtrshortdl-based direct pose estimation methods for
    spacecraft relative navigation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | Backbone | Transfer | Type | Dataset | Comments |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | learning |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[7](#bib.bib7)] | AlexNet | ImageNet | Classifier | PRISMA, synthetic |
    Coarse initialisation |'
  prefs: []
  type: TYPE_TB
- en: '| [[8](#bib.bib8)] | Faster [R-CNN](#glo.main.rcnn) | ImageNet | Soft classifier
    | [SPEED](#glo.main.speed), synthetic | Introduction of [SPN](#glo.main.spn) |'
  prefs: []
  type: TYPE_TB
- en: '| [[33](#bib.bib33)] | [SPN](#glo.main.spn) | ImageNet | Soft classifier |
    [SPEED](#glo.main.speed), PRISMA, synthetic (OpenGL) | Outperforms Ref. [[7](#bib.bib7)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[36](#bib.bib36)] | ResNet-50 | [COCO](#glo.main.coco) | Soft classifier
    | [URSO](#glo.main.urso) (Soyuz [S/C](#glo.main.sc)) | Soft classifier outperforms
    regressor in attitude |'
  prefs: []
  type: TYPE_TB
- en: '| [[24](#bib.bib24)] | AlexNet | ✗ | Regressor | Synthetic (Gazebo), real |
    Direct [3D](#glo.main.3d) keypoint regression |'
  prefs: []
  type: TYPE_TB
- en: '| [[41](#bib.bib41)] | 2-layer [CNN](#glo.main.cnn) | ✗ | Regressor | Synthetic
    (Blender, JCSAT-3 [S/C](#glo.main.sc)) | Evaluates robustness to noise, outputs
    quaternions |'
  prefs: []
  type: TYPE_TB
- en: '| [[19](#bib.bib19)] | VGG-19 | ImageNet | Regressor | Synthetic (Gazebo) |
    Cooperative object tracker |'
  prefs: []
  type: TYPE_TB
- en: '| [[42](#bib.bib42)] | GoogLeNet | PoseNet | Regressor | Synthetic (Unreal
    Engine 4, Soyuz [S/C](#glo.main.sc)) | Comparison of two loss functions |'
  prefs: []
  type: TYPE_TB
- en: '| [[43](#bib.bib43)] | Shallow [CNN](#glo.main.cnn) + [LSTM](#glo.main.lstm)
    | ✗ | Regressor | Synthetic (Elite [S/C](#glo.main.sc)), real (Envisat [S/C](#glo.main.sc))
    | Frame to frame motion estimator |'
  prefs: []
  type: TYPE_TB
- en: '| [[23](#bib.bib23)] | U-Net, ResNet, YOLOv3 | ✗ | Regressor | [SPEED](#glo.main.speed)
    | Onboard [FPGA](#glo.main.fpga) implementation |'
  prefs: []
  type: TYPE_TB
- en: '| [[22](#bib.bib22)] | AlexNet | ImageNet | Classifier | Synthetic (Dragon
    [S/C](#glo.main.sc)) | Analysis of Sun angles, Earth presence, noise |'
  prefs: []
  type: TYPE_TB
- en: 2.4 Indirect Frameworks for Spacecraft Relative Pose Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Estimating the pose from images using end-to-end \glsxtrshortdl-based methods
    has been argued to yield inadequate feature representation and limited explainability,
    either of which has so far achieved subpar performances as opposed to geometry-based
    methods. Sattler et al. [[45](#bib.bib45)] discuss the limitations of end-to-end
    \glsxtrshortcnn-based terrain pose regression and suggest that there is a gap
    for practical applications. Moreover, the [DNN](#glo.main.dnn) model has a risk
    of overfitting, which results in unpredictable drops in performance between the
    training images and test images due to memorising, rather than learning, properties
    of the former set that do not function well on the latter [[1](#bib.bib1)]. Therefore,
    some research avenues have recently refocused on the indirect methods, which aim
    to combine [DL](#glo.main.dl) and conventional geometry-based techniques to refine
    the estimation of the pose.
  prefs: []
  type: TYPE_NORMAL
- en: 'To promote the practical use of \glsxtrshortdl-based pose estimation in space
    missions, Park et al. [[46](#bib.bib46)] take the [SPN](#glo.main.spn) framework
    [[8](#bib.bib8)] and modify it by employing both a novel [CNN](#glo.main.cnn)
    for target detection and [Random Sample Consensus (RANSAC)](#glo.main.ransac)
    algorithms for solving the [P$n$P](#glo.main.pnp) problem. The proposed [CNN](#glo.main.cnn)
    is decoupled into the detection and pose estimation networks to determine the
    [2D](#glo.main.2d) bounding box of the [RoI](#glo.main.roi) and to regress the
    [2D](#glo.main.2d) locataion of keypoints, respectively. As demonstrated in Fig. [9(a)](#S2.F9.sf1
    "In Figure 9 ‣ 2.4 Indirect Frameworks for Spacecraft Relative Pose Estimation
    ‣ 2 \glsfmtshortdl-based Pose Estimation for Spacecraft Relative Navigation ‣
    Deep Learning-based Spacecraft Relative Navigation Methods: A Survey"), the [Object
    Detection Network (ODN)](#glo.main.odn) and the [Keypoint Regression Network (KRN)](#glo.main.krn)
    closely follow the pipeline of YOLOv2/YOLOv3 [[47](#bib.bib47)], but use MobileNetv2
    [[48](#bib.bib48)] and MobileNet [[49](#bib.bib49)], respectively. To drastically
    reduce the number of network parameters, traditional convolution operations of
    the network are replaced with depth-wise convolutions followed by point-wise convolutions
    (see Fig. [9(b)](#S2.F9.sf2 "In Figure 9 ‣ 2.4 Indirect Frameworks for Spacecraft
    Relative Pose Estimation ‣ 2 \glsfmtshortdl-based Pose Estimation for Spacecraft
    Relative Navigation ‣ Deep Learning-based Spacecraft Relative Navigation Methods:
    A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the lack of real space-based datasets with representative texture
    and surface illumination properties, Park et al. [[46](#bib.bib46)] also contribute
    with a new training procedure to improve the robustness of [CNNs](#glo.main.cnn)
    to spaceborne imagery when trained solely on synthetic data. Inspired by Ref. [[50](#bib.bib50)],
    they generate a new dataset by applying neural style transfer techniques [[51](#bib.bib51)]
    to a custom synthetic dataset with the same pose distribution as [SPEED](#glo.main.speed).
    After training with the new texture-randomised dataset, the proposed network performs
    better on spaceborne images and scores 4th place in [KPEC](#glo.main.kpec).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/66716a84c8d0e46152c0015a2ef30f86.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Pose estimation network
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b8cba0744ac12e7639bad0d4cd231ae9.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Different convolution operations
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Proposed [DNN](#glo.main.dnn) framework in Ref. [[46](#bib.bib46)]
    and comparison of three convolution operations.'
  prefs: []
  type: TYPE_NORMAL
- en: The 1st place [KPEC](#glo.main.kpec) solution is also an indirect \glsxtrshortdl-based
    scheme proposed by Chen et al. [[20](#bib.bib20)], where [DL](#glo.main.dl) and
    geometric optimisation are combined to present a \glsxtrshortcnn-based pipeline
    for pose estimation from a single image. Firstly, [3D](#glo.main.3d) landmarks
    of the satellite are computed from the training set via multiview triangulation.
    A [High-Resolution Net (HRNet)](#glo.main.hrnet) [[52](#bib.bib52)] is then trained
    to regress the location of projected [2D](#glo.main.2d) corner point landmarks
    on the spacecraft from the input greyscale image. Finally, the optimal poses are
    obtained by the proposed geometric optimisation algorithm based on simulated annealing,
    where the initial pose is estimated from a [P$n$P](#glo.main.pnp) solver. More
    specifically, the proposed [DNN](#glo.main.dnn) framework contains two modules.
    The first uses an [HRNet](#glo.main.hrnet) front-end/Faster [R-CNN](#glo.main.rcnn)
    combination to detect the [2D](#glo.main.2d) bounding box of the target in the
    input image. The [RoI](#glo.main.roi) is then cropped and resized for use in the
    second model, which consists of a pure [HRNet](#glo.main.hrnet) and is trained
    on an [MSE](#glo.main.mse) loss between the predicted and ground truth heatmaps
    of the visible landmarks in each image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve a fast and accurate estimate of the pose, Huo et al. [[53](#bib.bib53)]
    developed a novel [DLs](#glo.main.dl)-based approach combining [P$n$P](#glo.main.pnp)
    and geometric optimisation. A new and lightweight tiny-YOLOv3 based framework
    is designed to predict the [2D](#glo.main.2d) locations of the projected keypoints
    of the constructed [3D](#glo.main.3d) model. Fig. [10](#S2.F10 "Figure 10 ‣ 2.4
    Indirect Frameworks for Spacecraft Relative Pose Estimation ‣ 2 \glsfmtshortdl-based
    Pose Estimation for Spacecraft Relative Navigation ‣ Deep Learning-based Spacecraft
    Relative Navigation Methods: A Survey") shows the corresponding regression network,
    in which the output of tiny-YOLOv3 is modified to establish a box reliability
    judgement mode for detecting the [S/C](#glo.main.sc) and predicting the [2D](#glo.main.2d)
    [RoI](#glo.main.roi). Next, the regression of [S/C](#glo.main.sc) keypoints is
    achieved by replacing the [FCLs](#glo.main.fl) with [CLs](#glo.main.cl) to yield
    heatmaps. Finally, [P$n$P](#glo.main.pnp) and bundle adjustment are utilised to
    generate the initial pose and optimise it, respectively, which improves the accuracy
    and robustness of the proposed approach. Their method is evaluated on the [SPEED](#glo.main.speed)
    dataset and achieves competitive performance in spacecraft pose estimation with
    a lighter computational footprint.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1c9c38ee9bb692e3d832bd46c2dc697.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The overall structure of the network designed by Huo et al. [[53](#bib.bib53)].'
  prefs: []
  type: TYPE_NORMAL
- en: Another indirect [DLs](#glo.main.dl)-based scheme combines a \glsxtrshortcnn-based
    feature detector with a [P$n$P](#glo.main.pnp) solver and an [Extended Kalman
    Filter (EKF)](#glo.main.ekf) to guarantee a robust pose estimation [[27](#bib.bib27)].
    The authors build an hourglass-shaped [CNN](#glo.main.cnn) composed of a six-block
    encoder and a six-block decoder to estimate the heatmaps of $16$ predefined corners
    on the Envisat spacecraft. A target detection module is not incorporated since
    the presence of Earth in the background is not considered. Using the weights from
    the heatmaps, an associated landmark covariance is calculated. Two testing campaigns
    are then performed. The first one uses a dataset composed of singular images and
    computes the relative pose by incorporating the covariance of the regressed landmarks
    into the [P$n$P](#glo.main.pnp) procedure [[54](#bib.bib54)]. The second campaign
    considers a sequential dataset simulating a V-bar approach with Envisat at a fixed
    relative distance, where the target performs a roll rotation with respect to the
    [Local-Vertical, Local-Horizontal (LVLH)](#glo.main.lvlh) frame of reference.
    The relative pose is estimated by a tightly coupled [EKF](#glo.main.ekf) based
    on a Clohessy-Wiltshire dynamical model. Sensor measurements input of the filter,
    the landmark locations and covariances, come from the [CNN](#glo.main.cnn). The
    filter achieves steady-state position errors inferior to $0.2\text{\,}\mathrm{m}$
    for all axes, and the attitude errors are under $2\text{\,}\mathrm{deg}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pipeline similar to Ref. [[53](#bib.bib53)] is investigated by Huan et al.
    [[21](#bib.bib21)], achieving results nearly an order of magnitude better in the
    precision and accuracy of position and attitude estimation relative to the [SPN](#glo.main.spn)
    framework. The training methodology consists of four steps: 1) manual selection
    of images from the training dataset to be used for the reconstruction of the target
    [3D](#glo.main.3d) model, 2) detection of the [2D](#glo.main.2d) bounding box
    by an [ODN](#glo.main.odn), 3) estimation of the [2D](#glo.main.2d) image location
    of keypoints from a [KRN](#glo.main.krn), and 4) projection of the [3D](#glo.main.3d)
    groundtruth keypoints onto the image plane and solving the [P$n$P](#glo.main.pnp)
    problem from the correspondences with the estimated keypoints. Differing from
    Ref. [[53](#bib.bib53)], the proposed target detection network and [KRN](#glo.main.krn)
    in Ref. [[21](#bib.bib21)] apply the state-of-the-art [HRNet](#glo.main.hrnet)
    as backbone. The 6-[DoF](#glo.main.dof) pose is finally predicted by non-linear
    minimisation of a Huber reprojection loss. The training dataset is constructed
    of synthesised greyscale images, and the test set images are captured in real-time
    using a monocular camera.'
  prefs: []
  type: TYPE_NORMAL
- en: Shi et al. [[6](#bib.bib6)] transfer the state-of-the-art [CNN](#glo.main.cnn)
    techniques to target CubeSat detection, but with no further discussion on pose
    estimation. Inception-ResNet-V2 [[55](#bib.bib55)] and ResNet-101 [[37](#bib.bib37)]
    are combined and trained to estimate the bounding box of the target [S/C](#glo.main.sc)
    with a laboratory test platform. The final [FCL](#glo.main.fl) of ResNet-101 is
    reduced to two classes to differentiate between the "1U_CubeSat" and "3U_CubeSat"
    labels. The pre-trained weights of the [CNN](#glo.main.cnn) are obtained from
    the [COCO](#glo.main.coco) dataset [[56](#bib.bib56)] and further trained on a
    mixture of real and synthetic CubeSat images. Their simulation results indicate
    that the Inception-ResNet-V2 framework achieves a slightly higher accuracy and
    precision for [S/C](#glo.main.sc) detection, whereas the ResNet-101 network is
    less computationally heavy. To tackle a similar problem, Ming [[57](#bib.bib57)]
    constructs a feature extraction network and [Region Proposal Network (RPN)](#glo.main.rpn)
    structure framework based on Faster [R-CNN](#glo.main.rcnn) [[32](#bib.bib32)]
    on the [CNN](#glo.main.cnn) Caffe [[58](#bib.bib58)] open platform. The proposed
    network performs intelligent identification of a spacecraft module in the image
    sequence and filters out the certain components of interest.
  prefs: []
  type: TYPE_NORMAL
- en: For other space missions beyond rendezvous, Yi [[59](#bib.bib59)] studies the
    relative position estimation problem of a docking mission (below $10\text{\,}\mathrm{m}$).
    Assuming relative attitude has been adjusted, the method utilises a modified VGG-16
    to regress the relative position between docking rings. Further position smoothing
    and relative speed estimation are achieved by Kalman filtering. Additionally,
    a satellite positioning error compensation technique based on [DL](#glo.main.dl)
    is discussed by Jiaming [[60](#bib.bib60)]. Large amounts of data are collected
    to generate a robust model; a [CNN](#glo.main.cnn), a depth belief network, and
    a [Recurrent Neural Network (RNN)](#glo.main.rnn) are trained on satellite location
    data are collected by the Institute of Technology of the Chinese Academy of Sciences,
    which aims to generate a robust model. A [CNN](#glo.main.cnn), a depth belief
    network, and a [RNN](#glo.main.rnn) are trained on the collected data, of which
    the [CNN](#glo.main.cnn) performs the best compensation result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of \glsxtrshortdl-based indirect pose estimation methods for
    spacecraft relative navigation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[b] Ref. Backbone Transfer Type Dataset Pose estimation by learning [[46](#bib.bib46)]
    YOLO, MobileNet ✗ Keypoint regressor Synthetic (\glsxtrshortnst¹ ), [SPEED](#glo.main.speed)
    [P$n$P](#glo.main.pnp) [[20](#bib.bib20)] [HRNet](#glo.main.hrnet) + Faster [R-CNN](#glo.main.rcnn)
    ✗ Keypoint regressor [SPEED](#glo.main.speed) [P$n$P](#glo.main.pnp) [[53](#bib.bib53)]
    tiny-YOLOv3 [COCO](#glo.main.coco) Keypoint regressor [SPEED](#glo.main.speed)
    [P$n$P](#glo.main.pnp) [[27](#bib.bib27)] Hourglass network ✗ Keypoint regressor
    Synthetic (Cinema 4D, Envisat [S/C](#glo.main.sc)) [P$n$P](#glo.main.pnp) + [EKF](#glo.main.ekf)
    [[21](#bib.bib21)] 2-layer [CNN](#glo.main.cnn) ✗ Keypoint regressor Synthetic,
    real (lab) [P$n$P](#glo.main.pnp) [[59](#bib.bib59)] VGG-16 ✗ Position regressor
    Synthetic (Blender) [EKF](#glo.main.ekf) [[6](#bib.bib6)] ResNet [COCO](#glo.main.coco)
    Classifier Synthetic, real CubeSat detection [[57](#bib.bib57)] Faster [R-CNN](#glo.main.rcnn)
    ✗ Regressor Synthetic Object detection [[60](#bib.bib60)] [LSTM](#glo.main.lstm)
    ✗ Regressor Real Position error compensation'
  prefs: []
  type: TYPE_NORMAL
- en: 1\glsxtrlong
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*nst; applied to randomise the texture of the spacecraft.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table [2](#S2.T2 "Table 2 ‣ 2.4 Indirect Frameworks for Spacecraft Relative
    Pose Estimation ‣ 2 \glsfmtshortdl-based Pose Estimation for Spacecraft Relative
    Navigation ‣ Deep Learning-based Spacecraft Relative Navigation Methods: A Survey")
    contains a brief summary of indirect \glsxtrshortdl-based  algorithms for spacecraft
    pose estimation and related applications. As illustrated, the earlier studies
    (last four referenced) are more focused on parts of pose estimation missions,
    such as detection and position estimation. [P$n$P](#glo.main.pnp) and [EKF](#glo.main.ekf)
    are commonly combined to refine poses output by [DNNs](#glo.main.dnn). Moreover,
    transfer learning and very deep networks are rarely utilised when [DL](#glo.main.dl)
    methods are combined with optimisers. This could potentially be due to the fact
    that the pipelines rely heavily on these optimisation steps at the end, which
    are able to guarantee a decent estimate of the pose. In this way, shallow networks
    can reduce the computational cost, which is beneficial for practical use and potential
    onboard implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Crater and Hazard Detection for Terrain Navigation Using \glsfmtshortdl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exploring and landing on the lunar surface has long been a challenge of great
    interest within space technology and science. Recent developments in [DL](#glo.main.dl)
    have led to a renewed interest in learning-based [TRN](#glo.main.trn). Craters
    are ideal landmarks for relative navigation on or around the Moon and asteroids
    [[61](#bib.bib61), [62](#bib.bib62)]. Additionally, hazards should be avoided
    for a successful landing mission. This section, therefore, reviews the field of
    \glsxtrshortdl-based terrain navigation in three aspects: crater detection, hazard
    detection, and [TRN](#glo.main.trn) methods, all using [DNNs](#glo.main.dnn).
    Figure [11](#S3.F11 "Figure 11 ‣ 3 Crater and Hazard Detection for Terrain Navigation
    Using \glsfmtshortdl ‣ Deep Learning-based Spacecraft Relative Navigation Methods:
    A Survey") shows schematic diagram and difference between the three aspects.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75897c183cc696dc39ae6de7b5b96275.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Scenarios of crater detection, hazard detection for safe landing
    area, and terrain navigation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Crater detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With advances in computer vision and successful applications of [CNNs](#glo.main.cnn)
    in the object detection area, CNN-based crater detection algorithms are also emerging.
    However, most of the earlier methods only utilise a [CNN](#glo.main.cnn) as a
    classifier to validate selected features, such as in Refs. [[63](#bib.bib63),
    [64](#bib.bib64), [65](#bib.bib65)]. The shapes of natural craters vary in morphology,
    including peak rings, central pits, and wall terraces [[66](#bib.bib66)]. Some
    craters may also overlap with others. Considering the illumination conditions
    and different poses of on-board cameras, the imaged craters can be diverse in
    terms of dimensions and appearance [[61](#bib.bib61)]. Conversely, robust crater
    detection algorithms have been developed by applying [DNNs](#glo.main.dnn) to
    fully process raw crater images, exhibiting promising results which have attracted
    a lot of interest.
  prefs: []
  type: TYPE_NORMAL
- en: The [Python Crater Detection Algorithm (PyCDA)](#glo.main.pycda) [[67](#bib.bib67)]
    is an open-source crater detection library composed of a detector, extractor,
    and classifier, which focuses on detecting new craters that have never been catalogued.
    [PyCDA](#glo.main.pycda) uses a downsized U-Net architecture to compute the per-pixel
    likelihoods of a crater rim from inputs of greyscale intensity images. The pixel
    prediction map is then fed to the extractor to generate a list of crater candidates.
    A classifier [CNN](#glo.main.cnn) is finally applied to determine true craters.
    Thanks to [PyCDA](#glo.main.pycda), a considerable amount of craters have been
    detected and categorised, thus helping to generate new labelled datasets for training
    and testing of [DL](#glo.main.dl) algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/48e5a04cb00d63f76365458d61f28abe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Framework of CraterIDNet reproduced from Wang et al. [[61](#bib.bib61)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wang et al. [[61](#bib.bib61)] proposed an end-to-end fully [CNN](#glo.main.cnn),
    CrateIDNet, for simultaneous crater detection and identification. CraterIDNet
    takes remote sensing images in various sizes and outputs detected crater positions,
    apparent diameters, and indices of the identified craters. Instead of using large
    off-the-shelf [DNN](#glo.main.dnn) models, a small [CNN](#glo.main.cnn) architecture
    pre-trained on Martian crater samples [[68](#bib.bib68)] is first developed to
    extract feature maps. Next, two pipelines, namely [Crater Detection (CD)](#glo.main.cd)
    and [Crater Identification (CI)](#glo.main.ci) are proposed for simultaneous detecting
    and identifying craters. The [CD](#glo.main.cd) process involves detecting the
    presence of craters and locating them within the image if they exist. The output
    of [CD](#glo.main.cd) is then fed to the [CI](#glo.main.ci) process to match the
    detected craters to surface landmarks in a known database, and matches of [CI](#glo.main.ci)
    will provide position estimation. Fig. [12](#S3.F12 "Figure 12 ‣ 3.1 Crater detection
    ‣ 3 Crater and Hazard Detection for Terrain Navigation Using \glsfmtshortdl ‣
    Deep Learning-based Spacecraft Relative Navigation Methods: A Survey") shows the
    whole framework of CraterIDNet. The [CD](#glo.main.cd) modifies the [RPN](#glo.main.rpn)
    architecture [[32](#bib.bib32)] as the backbone, regressing objectness scores
    and crater diameters from feature maps. Due to different craters sizes, two [CD](#glo.main.cd)
    pipelines are designed by sharing same [CLs](#glo.main.cl) but with different
    parameters. Later, craters are identified by [CI](#glo.main.ci) that combines
    a proposed grid pattern layer and [CNN](#glo.main.cnn) framework. For the training
    and testing dataset, $1600$ craters are manually catalogued and enlarged to a
    final sample set of $16\,000$ instances through data augmentation. Experiments
    reveal that the light CraterIDNet with a size of $4\text{\,}\mathrm{MB}$ performs
    better than previous algorithms [[64](#bib.bib64)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Silburt et al. [[69](#bib.bib69)] employ a [CNN](#glo.main.cnn) architecture
    for robust crater detection on the lunar surface using [DEMs](#glo.main.dem).
    The method relies on the developed DeepMoon network to identify the craters in
    terms of their centroid and radius, and outputs pixel-wise confidence maps of
    crater rims on the surface of a rocky body. DeepMoon modifies U-Net [[70](#bib.bib70)]
    by changing the input image size, the number of filters in each convolution layer,
    and the use of dropout [[71](#bib.bib71)] in the expansive path for memory limitations
    and regularisation respectively. Fig. [13](#S3.F13 "Figure 13 ‣ 3.1 Crater detection
    ‣ 3 Crater and Hazard Detection for Terrain Navigation Using \glsfmtshortdl ‣
    Deep Learning-based Spacecraft Relative Navigation Methods: A Survey") presents
    the DeepMoon architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/169cec966705f58b7a07924a3dc257ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Architecture of DeepMoon network [[69](#bib.bib69)]'
  prefs: []
  type: TYPE_NORMAL
- en: For training, the data used in DeepMoon is generated by merging two human-generated
    crater catalogues, which are the [Lunar Reconnaissance Orbiter (LRO)](#glo.main.lro)
    [Wide Angle Camera (WAC)](#glo.main.wac) Global Lunar [DEM](#glo.main.dem) [[72](#bib.bib72)]
    and the [LRO](#glo.main.lro) Lunar Orbiter Laser Altimeter [DEM](#glo.main.dem)
    [[73](#bib.bib73)]. The dataset is split into equal train-validation-test parts,
    yielding $30\,000$ [DEM](#glo.main.dem) images per part. The minimised loss function
    is chosen as the pixel-wise [Binary Cross-Entropy (BCE)](#glo.main.bce). DeepMoon
    produces a crater rim prediction mask, which is then fed to a low-level image
    process and a template matching procedure to determine the actual craters. The
    median fractional longitude, latitude and radius errors are $11\text{\,}\mathrm{\char
    37\relax}$ or less, representing good agreement with the human-generated datasets.
    Additionally, transfer learning from training on lunar maps to testing on maps
    of Mercury is qualitatively demonstrated successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Downes et al. [[25](#bib.bib25)] propose the LunaNet framework to detect craters
    for lunar [TRN](#glo.main.trn), which is quite similar to DeepMoon with the exception
    that it takes greyscale images as inputs. Thus, the method is more suitable for
    implementation aboard a spacecraft equipped with an optical camera without the
    need for a depth sensor. The output of the [CNN](#glo.main.cnn) is, like DeepMoon,
    a crater rim prediction mask. However, the craters are extracted through a different
    method and, Fig. [14](#S3.F14 "Figure 14 ‣ 3.1 Crater detection ‣ 3 Crater and
    Hazard Detection for Terrain Navigation Using \glsfmtshortdl ‣ Deep Learning-based
    Spacecraft Relative Navigation Methods: A Survey") shows each feature extraction
    step of the LunaNet, including prediction mask, eroded and thresholded prediction,
    contour detection, and ellipse fitting. The data preparation is also akin to the
    process followed by DeepMoon, with the [LRO](#glo.main.lro) [WAC](#glo.main.wac)
    Global Lunar [DEM](#glo.main.dem) dataset [[72](#bib.bib72)], followed by a histogram
    rescaling of the input greyscale images to match the intensity distribution of
    a [DEM](#glo.main.dem) image. Based on the pre-trained DeepMoon weights, LunaNet
    reduces the training effort and final detection results. Experimental results
    indicate that LunaNet’s performance surpasses DeepMoon and [PyCDA](#glo.main.pycda)
    in terms of robustness to noisy images, location accuracy, and average crater
    detection time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/739d448e5b654c1100639e7e56542209.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Feature extraction steps of LunaNet [[25](#bib.bib25)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'It has been observed that areas with low solar angles, where there is heavy
    shadowing, result in reduced crater detection reliability. Lee et al. [[74](#bib.bib74)]
    employ a \glsxtrshortcnn-based object detector to distinguish likely landmark
    candidates and predict detection probabilities along various lighting geometric
    flight paths, aiming to identify high-value landmarks by using optical navigation
    systems. A massive dataset based on real lunar-surface data is collected. A [Candidate
    for a Regional Object (CRO)](#glo.main.cro) is defined as an image object with
    specific latitudes and longitudes. The LunarNet architecture (Fig. [15](#S3.F15
    "Figure 15 ‣ 3.1 Crater detection ‣ 3 Crater and Hazard Detection for Terrain
    Navigation Using \glsfmtshortdl ‣ Deep Learning-based Spacecraft Relative Navigation
    Methods: A Survey"); see also the process of LunarNet-based landmark selection
    in Ref. [[74](#bib.bib74)]) is then used and trained to identify [CROs](#glo.main.cro)
    by maximising the discrimination between local areas of the Moon. Finally, the
    [CRO](#glo.main.cro) performance map is formed based on the scored [CROs](#glo.main.cro)
    arranged by considering the azimuth and elevation angles of the Sun during the
    year. Numerical experimental results demonstrate that the proposed landmark detection
    pipeline can provide usable navigation information even at Sun angle elevations
    of less than $1.8\text{\,}\mathrm{deg}$ in highland areas, which indicates a successful
    application for the worst dark highlands near the South Pole.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1bdd0b7aab2e0a87de612524ba2915a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: \glsxtrshortcnn-based [CRO](#glo.main.cro) discriminator (LunarNet)
    [[74](#bib.bib74)]'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Hazard detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hazard detection is considered the vital research field of space [TRN](#glo.main.trn)
    to avoid failures during landing. In 1974, Apollo program officials introduced
    manual hazard and target selection for lunar descent guidance [[75](#bib.bib75)].
    In the past decade, many algorithms have been developed which benefit from the
    increasing computational power of processor devices. Since 2006, promoted by the
    [Autonomous Landing Hazard Avoidance Technology (ALHAT)](#glo.main.alhat) project
    [[76](#bib.bib76)] conducted by the [National Aeronautics and Space Administration
    (NASA)](#glo.main.nasa), there has been growing interest in hazard estimation
    based on [DEMs](#glo.main.dem). In 2012, Furfaro et al. [[77](#bib.bib77)] implemented
    an [AI](#glo.main.ai) system to autonomously select a soft landing site in a region
    with safe terrains for Venus and Titan. In 2015, Maturana and Scherer [[78](#bib.bib78)]
    used what they called a [3D](#glo.main.3d)-[CNN](#glo.main.cnn) to create a safety
    map for autonomous landing zone detection for helicopters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier research in \glsxtrshortnn-based [Hazard Detection and Avoidance (HDA)](#glo.main.hda)
    for lunar landing is studied by Lunghi and Lavagna [[79](#bib.bib79)] and Lunghi
    et al. [[80](#bib.bib80)], who demonstrate the ability and attractive properties
    of [Artificial Neural Networks (ANNs)](#glo.main.ann) for real-time applications.
    The ground truth is calculated from the corresponding [DEM](#glo.main.dem) by
    thresholding pixel-wise figures. Input images of the terrain are manually processed
    at a resolution of $1024\text{\times}1024\text{\,}\mathrm{p}\mathrm{x}$ to extract
    a $13$-dimensional vector per pixel comprising the image intensity mean, standard
    deviation, gradient and the [Laplacian of Gaussian (LoG)](#glo.main.log) at three
    different scales, and the Sun’s inclination angle. Following this, the crafted
    features are fed to a neural network, outputting a $256\text{\times}256\text{\,}\mathrm{p}\mathrm{x}$
    hazard map with each pixel value denoting a confidence value. From the output
    hazard map, candidate landing sites are obtained via pixel thresholding and scored
    global landing potential by analysing minimum radial dimension requirements, distance
    to an a priori nominal landing site, and the [NN](#glo.main.nn) scores of pixels
    inside the candidate radius. The target landing site is selected as the one that
    maximises the global score. Two different pipelines are developed: one based on
    a [Multilayer Perceptron (MLP)](#glo.main.mlp) with $15$ nodes, and the other
    based on a cascading [NN](#glo.main.nn) with successive layers of hidden information
    added during training. A test set of $8$ images including four landscapes in two
    Sun inclination angles are utilised to evaluate two proposed pipelines. The predicted
    hazard maps during training have a negligible difference, with $0.0194$ for the
    [MLP](#glo.main.mlp) and $0.020\,39$ for the cascade of pixel-wise [MSE](#glo.main.mse).
    However, the former proved better at determining safe landing sites. In addition,
    qualitative results have been presented for asteroid images acquired by the Rosetta
    probe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Moghe and Zanetti [[81](#bib.bib81)] presented a more modern approach
    towards tackling the same problem. Aiming at the hazard detection of the [ALHAT](#glo.main.alhat)
    project, the authors implement an hourglass-like [CNN](#glo.main.cnn) architecture
    with copy and crop connections based on U-Net [[70](#bib.bib70)]. The framework
    processes [DEMs](#glo.main.dem) directly and classifies safe and hazardous landing
    spots with the output map. Through data augmentation and transforming existing
    datasets, they create a new dataset from the [LRO](#glo.main.lro) dataset [[63](#bib.bib63)].
    The output, similarly to Ref. [[80](#bib.bib80)], is a confidence map followed
    by a threshold to yield a binary landing/non-landing score, despite not provide
    a specific landing site. Results on a set of $100$ testing images demonstrate
    an average hazard mapping Dice accuracy score of $83\text{\,}\mathrm{\char 37\relax}$
    and indicate the potential of real-time processing in future missions. Later,
    Moghe and Zanetti [[81](#bib.bib81)] expand and modify their work in Ref. [[82](#bib.bib82)],
    using the same network architecture but featuring improved layers covering the
    input size, output size, and layer width. The topology of the modified network
    is illustrated in Fig. [16](#S3.F16 "Figure 16 ‣ 3.2 Hazard detection ‣ 3 Crater
    and Hazard Detection for Terrain Navigation Using \glsfmtshortdl ‣ Deep Learning-based
    Spacecraft Relative Navigation Methods: A Survey"). Similarly, the Albumentations
    data augmentation suite [[83](#bib.bib83)] is used to prepare data. The modified
    \glsxtrshortcnn-based network outputs a mean pixel accuracy of $\sim 92\text{\,}\mathrm{\char
    37\relax}$ on the same testing dataset of Ref. [[81](#bib.bib81)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a996c183ebb625f1485424dbc8ca3fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The topology of the network in [[82](#bib.bib82)]'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Terrain navigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the field of image-based planetary [TRN](#glo.main.trn), Campbell et al.
    [[28](#bib.bib28)] first utilise a [CNN](#glo.main.cnn) architecture trained on
    a series of images rendered from a [DEM](#glo.main.dem) simulating the Apollo
    16 landing site to output the position of a spacecraft relative to the ground
    along one direction. The problem is posed by taking a centre $128\text{\,}\mathrm{p}\mathrm{x}$
    wide strip from the original $1024\text{\times}1024\text{\,}\mathrm{p}\mathrm{x}$
    nadir base image, considering each pixel location along the on-track dimension
    as its own class. $128\text{\times}128\text{\,}\mathrm{p}\mathrm{x}$ training
    images are generated by sampling every $8\text{\,}\mathrm{p}\mathrm{x}$ horizontally
    across the strip and rendering it $11$ times at different Sun illumination angles.
    The $1024$-dimensional one-hot vector, which labels the position along the track
    line, is then applied to each image. The [CNN](#glo.main.cnn) is composed of three
    [CLs](#glo.main.cl) and each followed by a max pooling layer. Thirty images are
    rendered at unseen Sun angles to make up a test dataset. Six of these are classified
    correctly, while in general, the maximum error observed is equal to $5\text{\,}\mathrm{p}\mathrm{x}$.
    For a ground sample distance of $0.5\text{\,}\mathrm{m}$, this means that achieved
    position errors are bounded at $2.5\text{\,}\mathrm{m}$. The testing is repeated
    for training images resampled at $4\text{\,}\mathrm{p}\mathrm{x}$, and the errors
    dropped to a maximum of $3\text{\,}\mathrm{p}\mathrm{x}$ (or $1.5\text{\,}\mathrm{m}$).
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, Downes et al. [[26](#bib.bib26)] explored how their LunaNet could be
    applied to the [TRN](#glo.main.trn) problem and reported a system for the robust
    estimation of relative position and velocity information. Thus, LunaNet is utilised
    to detect and match craters to known lunar landmarks from frame to frame across
    a trajectory. The matched craters are treated as features feeding to a feature-based
    [EKF](#glo.main.ekf), where the state of the filter is the position and velocity
    of the camera in [Lunar-Centred, Lunar-Fixed Coordinates (LCLF)](#glo.main.lclf),
    as well as the location of detected features in this same reference frame. Compared
    to an image processing-based crater detection method [[84](#bib.bib84)], the LunaNet
    + [EKF](#glo.main.ekf) combination produces considerable improvements on the accuracy
    of the [TRN](#glo.main.trn), with reliable performance in variable lighting conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/728b3509b5018e25d5fb478916c8646a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) \glsxtrshortnn-based classification flow
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95aa2b8c920dd0d92b31a698b47dc7fd.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The structure of \glsxtrshortdmlp[NN](#glo.main.nn)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 17: \glsxtrshortdmlp[NN](#glo.main.nn) classification flow and proposed
    architecture of Bai et al. [[85](#bib.bib85)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accurately identifying the detected terrain environment helps to achieve successful
    missions relying on planetary rovers. However, vision-based [TRN](#glo.main.trn)
    systems are difficult to effectively perceive the material and mechanical characteristics
    of the terrain environment. Thus, Bai et al. [[85](#bib.bib85)] and Chengchao
    [[86](#bib.bib86)] investigate several terrain classification and recognition
    methods from vibration using [DNNs](#glo.main.dnn). The experimental and [NN](#glo.main.nn)
    classification flows are illustrated in Fig. [17(a)](#S3.F17.sf1 "In Figure 17
    ‣ 3.3 Terrain navigation ‣ 3 Crater and Hazard Detection for Terrain Navigation
    Using \glsfmtshortdl ‣ Deep Learning-based Spacecraft Relative Navigation Methods:
    A Survey"). The authors compare three different learning-based approaches towards
    terrain material perception and classification: an improved [NN](#glo.main.nn)
    algorithm, a \glsxtrshortdmlp[NN](#glo.main.nn) algorithm, and [CNN](#glo.main.cnn)-[LSTM](#glo.main.lstm)
    based algorithm. Among these three schemes, the \glsxtrshortdmlp[NN](#glo.main.nn)
    achieves the best performance [[85](#bib.bib85)]. To classify textures, \glsxtrshortdmlp[NN](#glo.main.nn)
    (shown in Fig. [17(b)](#S3.F17.sf2 "In Figure 17 ‣ 3.3 Terrain navigation ‣ 3
    Crater and Hazard Detection for Terrain Navigation Using \glsfmtshortdl ‣ Deep
    Learning-based Spacecraft Relative Navigation Methods: A Survey")) adopts a five-[FCL](#glo.main.fl)
    architecture, in which the activation functions are ReLU and softmax for the first
    four and last layers, respectively. For the dataset and training, three-dimensional
    raw vibration data collected by sensors is first segmented to a vector with a
    fixed duration. Using the fast Fourier transform, the vector is then transferred
    to the frequency domain, in which the eigenvectors are obtained for network training.
    Five different textures, including brick, sand, flat, cement, soil, are trained
    and recognised by \glsxtrshortdmlp[NN](#glo.main.nn) with high overall classification
    accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an autonomous lunar landing scenario, Furfaro et al. [[87](#bib.bib87)]
    propose a [DNN](#glo.main.dnn) architecture that predicts the fuel-optimal control
    actions only using raw greyscale images taken by an on-board lander camera (Fig. [18](#S3.F18
    "Figure 18 ‣ 3.3 Terrain navigation ‣ 3 Crater and Hazard Detection for Terrain
    Navigation Using \glsfmtshortdl ‣ Deep Learning-based Spacecraft Relative Navigation
    Methods: A Survey")). The architecture is a five-layer [CNN](#glo.main.cnn) with
    three sequential images as input for each timestep. The [DNN](#glo.main.dnn) is
    modified with an [LSTM](#glo.main.lstm) back-end connected to two further branches:
    one for regression and one for classification. For training the network, a set
    of optimal trajectories is computed numerically via Gauss pseudo-spectral sampling
    methods using the [General Purpose Optimal Control Software (GPOPS II)](#glo.main.gpops)
    [[88](#bib.bib88)], producing a set of initial and final relative positions and
    velocities. Each state of the optimum trajectory is simulated by raytracing a
    [DEM](#glo.main.dem) of a patch on the Lunar surface, resulting in $562$ images
    with $256\text{\,}\mathrm{p}\mathrm{x}$ $\times$ $256\text{\,}\mathrm{p}\mathrm{x}$
    of resolution. For better performance, the model is retrained explicitly with
    subsets of data that do not produce satisfactory results on the first try [[89](#bib.bib89)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4586c8113e426047c5276b9338e427cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The architecture proposed by Furfaro et al. [[87](#bib.bib87)]'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Brief summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of [DNNs](#glo.main.dnn) in crater and hazard detection has not been
    widely investigated due to the lack of labelled databases. Datasets containing
    crater images which are open to the public do exist, e.g. the Lunar Crater Database⁹⁹9[https://astrogeology.usgs.gov/search/map/Moon/Research/Craters/lunar_crater_database_robbins_2018](https://astrogeology.usgs.gov/search/map/Moon/Research/Craters/lunar_crater_database_robbins_2018).
    [[63](#bib.bib63)] or the Robbins Mars Crater Database [[68](#bib.bib68)]. Yet,
    manually catalogued craters are required for applying supervised [DL](#glo.main.dl)
    methods as presented in [[61](#bib.bib61), [90](#bib.bib90), [25](#bib.bib25),
    [72](#bib.bib72), [73](#bib.bib73)]. There exists still a gap towards the automatic
    generation of [DL](#glo.main.dl) crater datasets, and in the past two years there
    have been increasing studies of [DNNs](#glo.main.dnn) with promising performance
    for [TRN](#glo.main.trn) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4 \glsfmtshortdl-based Relative Navigation for Asteroid Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Challenges and Motivations for \glsxtrshortdl-based Asteroid Exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent trends in small planetary exploration have led to a proliferation of
    studies that include asteroids and comets, pushed by scientific, planetary defence,
    and resource exploitation motivations [[91](#bib.bib91), [92](#bib.bib92)]. Autonomous
    navigation is demanded due to the long communication delay and complicated dynamic
    environment in the vicinity of asteroids [[93](#bib.bib93)]. Thus, it becomes
    necessary to develop new autonomous navigation algorithms for future asteroid
    sample and return missions, for which [DL](#glo.main.dl) techniques may provide
    a potential alternative.
  prefs: []
  type: TYPE_NORMAL
- en: 'The aforementioned studies demonstrate the potential of [DNNs](#glo.main.dnn)
    for image patch classification invariant under illumination changes applied to
    terrain navigation. The same principle could be used for other relative navigation
    applications, such as asteroid location pinpointing, illustrated in Fig. [2(b)](#S1.F2.sf2
    "In Figure 2 ‣ 1 Introduction ‣ Deep Learning-based Spacecraft Relative Navigation
    Methods: A Survey"). [Near-Earth Asteroid (NEA)](#glo.main.nea) missions, however,
    are more challenging than lunar missions; this is because one has limited information
    on the gravitation and environment of asteroids. If the celestial body and its
    orbit environment are in great uncertainty, all plans elaborated on-ground may
    dramatically fail when implemented in space [[94](#bib.bib94)]. Additionally,
    the lack of labelled ground truth data for asteroids challenges the application
    and development of [DL](#glo.main.dl) techniques in asteroid detection and landing
    [[95](#bib.bib95)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Previous Works Contributing to the Field
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For asteroid missions, earlier researchers have made various contributions towards
    \glsxtrshortnn-based orbit and dynamics uncertainty estimation. Harl et al. [[96](#bib.bib96)]
    develop a \glsxtrshortnn-based state observer to estimate gravitational uncertainties
    that spacecraft experience in an asteroid orbiting scenario. The [NN](#glo.main.nn)
    of the proposed state observer outputs the uncertainty as a function of the states
    instead of discrete values of an [EKF](#glo.main.ekf). Guffanti [[94](#bib.bib94)]
    trains a neural network as an autonomous motion planning unit to compute the optimal
    spacecraft orbital configuration, which takes the uncertain [NEA](#glo.main.nea)
    dynamics parameters created by navigation filters and the selected trade-off.
    Song et al. [[97](#bib.bib97)] also employ a six-hidden-layer [DNN](#glo.main.dnn)
    to quickly estimate the gravity and gradient of irregular asteroids and further
    apply the [DNN](#glo.main.dnn)-based gravitational model in orbital dynamic analysis.
    Instead of focusing on-orbit estimation, Kalita et al. [[98](#bib.bib98)] introduce
    an [NN](#glo.main.nn) to the formulation of asteroid missions in terms of the
    planning and design phases, while Feruglio et al. [[99](#bib.bib99)] utilise a
    feed-forward [NN](#glo.main.nn) to autonomously identify a [S/C](#glo.main.sc)
    impact event. Viavattene and Ceriotti [[100](#bib.bib100)] take advantage of a
    [NN](#glo.main.nn) to map the transfer time and cost for [NEA](#glo.main.nea)
    rendezvous trajectory.
  prefs: []
  type: TYPE_NORMAL
- en: \glsxtrshort
  prefs: []
  type: TYPE_NORMAL
- en: 'dnn-based optical navigation is an increasingly important area in asteroid
    exploration missions, which can manage challenges of previous schemes, including
    traditional high-cost and high-risk spacecraft systems, irregular and illuminated
    asteroids, and conventional image processing techniques. In such a scenario (Fig. [2(b)](#S1.F2.sf2
    "In Figure 2 ‣ 1 Introduction ‣ Deep Learning-based Spacecraft Relative Navigation
    Methods: A Survey")), the chaser may be commanded to inspect a particular patch
    on the surface of the asteroid it has rendezvoused with (observed on frame $\underaccent{\vec{}}{\bm{\mathcal{F}}}_{c}$),
    which is intrinsically a localisation task requiring the estimation of ${\bm{T}}_{ct}$.
    If the asteroid has been previously mapped, and there exists a codebook with annotated
    landmarks (on frame $\underaccent{\vec{}}{\bm{\mathcal{F}}}_{t}$) for comparison,
    there are two possible approaches. The first follows the same direct classification
    procedure as Ref. [[28](#bib.bib28)], where a [DNN](#glo.main.dnn) is used to
    match the observed patch with the corresponding patch in the codebook, which is
    annotated with the relative pose, but with the dataset of Lunar surface. The alternative
    approach is to have a single class per patch on the database and train the [DNN](#glo.main.dnn)
    to be robust to viewpoint distortion, and then rely on classical image processing
    techniques to infer the pose based on the different observed features between
    the observations and matched patches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pugliatti and Topputo [[101](#bib.bib101)] first present \glsxtrshortcnn-based
    methods for on-board small-body shape classification since shape information can
    enhance the image processing and autonomy of self-task planning. A set of $8$
    well-known models from the [Planetary Data System (PDS)](#glo.main.pds) node^(10)^(10)10[https://sbn.psi.edu/pds/shape-models](https://sbn.psi.edu/pds/shape-models).
    is selected to represent the most important features of small asteroids at a global
    scale. Fig. [19](#S4.F19 "Figure 19 ‣ 4.2 Previous Works Contributing to the Field
    ‣ 4 \glsfmtshortdl-based Relative Navigation for Asteroid Research ‣ Deep Learning-based
    Spacecraft Relative Navigation Methods: A Survey") presents a sketch of the steps
    for building the database and the proposed [CNN](#glo.main.cnn) framework for
    classifying asteroids. The database is generated in Blender with an assumed camera
    pointing and illumination, and further augmented in TensorFlow [[102](#bib.bib102)]
    with random rotations, translations, and scaling. The database composed of $20\,988$
    images is divided into training, validation and test sets according to a $80\text{\,}\mathrm{\char
    37\relax}$-$10\text{\,}\mathrm{\char 37\relax}$-$10\text{\,}\mathrm{\char 37\relax}$
    split. Their [CNN](#glo.main.cnn) architecture has five [CLs](#glo.main.cl) in
    sequence, with each followed by a pooling layer, a reshape operation, and three
    [FCLs](#glo.main.fl) to classify. A hyperparameter search is used to obtain network
    parameters. Three traditional approaches, such as Hu invariant moments, Fourier
    descriptors, and polar outlines, are compared, in which the proposed \glsxtrshortcnn-based
    scheme performs best.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba072dd4834518d16ef93a6a6f371e47.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Step flows for database generation
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20905c786d96f64a478d981548c85886.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Schematic representation of the proposed [CNN](#glo.main.cnn)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 19: Database generation flow and proposed [CNN](#glo.main.cnn) architecture
    of Pugliatti and Topputo [[101](#bib.bib101)].'
  prefs: []
  type: TYPE_NORMAL
- en: Later, Pugliatti and Topputo [[91](#bib.bib91)] proposed on-board autonomous
    navigation using segmentation maps and a [CNN](#glo.main.cnn) to estimate spacecraft
    position concerning an asteroid fixed reference frame. The [CNN](#glo.main.cnn)
    transferred from the MobileNetV2 network [[48](#bib.bib48)] classifies the segmentation
    maps to generate a rough estimate of the position information from the input.
    The relative position is finally obtained by refining the output of the [CNN](#glo.main.cnn)
    using an advanced normalised cross-correlation method. Didymos and Hartley are
    selected as representatives of regular and irregular small-bodies to create the
    dataset, which includes $49\,716$ samples of synthetic maps for five different
    scenarios. Experimental results indicate the capability of [CNN](#glo.main.cnn)
    in predicting the correct class and achieve a relative position error below $5\text{\,}\mathrm{\char
    37\relax}8\text{\,}\mathrm{\char 37\relax}$ of the range from the asteroid.
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, Ravani et al. [[95](#bib.bib95)] developed a novel Mask-Region [CNN](#glo.main.cnn)
    to detect landing sites for autonomous soft-landing on asteroids. Since there
    is no open public dataset of potential landing sites labelled with ground truth,
    the authors first gather image mosaics of the asteroid Vesta from the [PDS](#glo.main.pds)
    of [NASA](#glo.main.nasa) ^(11)^(11)11[https://pds.nasa.gov](https://pds.nasa.gov).
    and then fragment the large images into smaller ones with a fixed size. Next,
    the training dataset is labelled manually. For the Mask‑Region [CNN](#glo.main.cnn)
    pipeline, it follows a backbone network of ResNet-50-C4 for initialisation; an
    [RPN](#glo.main.rpn) for extracting feature maps; Faster [R-CNN](#glo.main.rcnn)
    for [RoI](#glo.main.roi) alignment; the network head is structured using [FCLs](#glo.main.fl)
    for computing the bounding box; and the mask head of a [FCL](#glo.main.fl) network
    [[103](#bib.bib103)] is used for calculating the pixel-level mask. Comparing with
    conventional image processing methods, the proposed network on their dataset results
    in an accuracy of $94\text{\,}\mathrm{\char 37\relax}$ with lower computational
    time cost in the implementation phase.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Summary and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work surveyed recent trends in deep learning techniques for 6-[DoF](#glo.main.dof)
    relative pose estimation in spaceborne applications. Contributions in the field
    of computer vision were presented, followed by concrete applications from the
    literature to autonomous spacecraft navigation, including spaceborne pose estimation,
    crater and hazard detection of terrain relative navigation, and [DL](#glo.main.dl)-based
    asteroid navigation. This survey is motivated by the applicability of [DL](#glo.main.dl)
    techniques in relative spacecraft navigation for future space missions, i.e. rendezvous,
    docking, formation flying, descent and landing on the lunar surface, orbiting
    and inspecting asteroids. The general [DNN](#glo.main.dnn) framework for the applications
    in this research area was reviewed in terms of network structure, type of network,
    training method, dataset topology and generation, and attained performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a review of [DNN](#glo.main.dnn)-based [S/C](#glo.main.sc) relative
    pose estimation techniques was given, in which a top level distinction between
    supervised and unsupervised methods was made, whereby contributions in the space
    domain were found to belong exclusively to the former. Context in terms of preceding
    ground-based applications was established. Further lower level categorisations
    were made; in particular, it was found that many techniques favoured a direct
    approach (so called “end-to-end”), where a [DNN](#glo.main.dnn) pipeline is trained
    directly on images to yield the relative state. Indeed, this is a very appealing
    property of deep learning, as not only is the feature extraction task relayed
    to a [CNN](#glo.main.cnn), but so is the modelling task, eliminating the ”middleman”
    and allowing the user to focus mainly on the architecture design and optimisation
    of learnable parameters. However, it was seen that more accurate solutions were
    obtained by combining them with classical methods. For these indirect methods,
    a [CNN](#glo.main.cnn) was tasked with regressing the locations of [2D](#glo.main.2d)
    keypoints on the target and estimating the relative pose from geometrical correspondences
    with their [3D](#glo.main.3d) counterparts, using techniques such as [P$n$P](#glo.main.pnp)
    or nonlinear optimisation. Furthermore, such solutions are easily incorporated
    into navigation filters to further refine the estimate with continuous, smooth
    consistency (also beyond pose estimation). The role of [RNNs](#glo.main.rnn),
    particularly [LSTMs](#glo.main.lstm), is highlighted in the processing of a continuous
    stream of images. Tables [1](#S2.T1 "Table 1 ‣ 2.3 Direct Frameworks for Spacecraft
    Relative Pose Estimation ‣ 2 \glsfmtshortdl-based Pose Estimation for Spacecraft
    Relative Navigation ‣ Deep Learning-based Spacecraft Relative Navigation Methods:
    A Survey") and [2](#S2.T2 "Table 2 ‣ 2.4 Indirect Frameworks for Spacecraft Relative
    Pose Estimation ‣ 2 \glsfmtshortdl-based Pose Estimation for Spacecraft Relative
    Navigation ‣ Deep Learning-based Spacecraft Relative Navigation Methods: A Survey")
    summarises these findings in terms of relative pose estimation error for spacecraft
    rendezvous [DL](#glo.main.dl) applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, the applications of [DNNs](#glo.main.dnn) to [TRN](#glo.main.trn) were
    divided into three aspects for surveying, in which the [DNN](#glo.main.dnn)-based
    crater and hazard detection methods were recognised as contributors towards building
    a terrain navigation system. It was pointed out that public open data for training
    and testing of [DNN](#glo.main.dnn)-based [TRN](#glo.main.trn) frameworks is limited.
    Furthermore, [DL](#glo.main.dl)-based relative navigation methods focusing on
    asteroid missions were provided. The challenges and motivations were discussed
    before a detailed review of this field.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, regarding unsupervised learning methods (i.e. concerning cases in which
    the desired output for each input is not given during training), far too little
    attention has been paid to this kind of technique for space navigation. However,
    unsupervised techniques such as [CNN](#glo.main.cnn)-\glsxtrshortslam (\glsxtrlong*slam)
    or unsupervised [VO](#glo.main.vo) are underlined as a potential novel approach
    for the space domain and may be investigated in future. Additionally, most publications
    study the application of [DL](#glo.main.dl) in space in a theoretical way without
    being concerned with computational performance; indeed, only a few publications
    [[19](#bib.bib19), [22](#bib.bib22), [81](#bib.bib81)] focus on actual deployments
    on hardware, considering things like execution time, and size of the training
    dataset. Therefore, it can be concluded that these studies towards the actual
    engineering practice have been little discussed and require further development.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Goodfellow et al. [2016] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
    *Deep learning*. MIT press, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wie et al. [2014] Bong Wie, Vaios Lappas, and Jesús Gil-Fernández. Attitude
    and orbit control systems. In Malcolm Macdonald and Viorel Badescu, editors, *The
    International Handbook of Space Technology*, chapter 12, page 362\. Springer Praxis
    Books, 2014. doi:[10.1007/978-3-642-41101-4](https://doi.org/10.1007/978-3-642-41101-4).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhaskaran et al. [1998] Shyam Bhaskaran, S Desai, P Dumont, B Kennedy, G Null,
    W Owen Jr, J Riedel, S Synnott, and R Werner. Orbit determination performance
    evaluation of the deep space 1 autonomous navigation system. In *AAS/AIAA Space
    Flight Mechanics Meeting*, 1998. URL [https://trs.jpl.nasa.gov/bitstream/handle/2014/19040/98-0222.pdf?sequence=1](https://trs.jpl.nasa.gov/bitstream/handle/2014/19040/98-0222.pdf?sequence=1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kothari et al. [2020] Vivek Kothari, Edgar Liberis, and Nicholas D. Lane. The
    final frontier: Deep learning in space. In *Proceedings of the 21st International
    Workshop on Mobile Computing Systems and Applications*, HotMobile ’20, page 45–49,
    New York, NY, USA, 2020\. Association for Computing Machinery. ISBN 9781450371162.
    doi:[10.1145/3376897.3377864](https://doi.org/10.1145/3376897.3377864).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cassinis et al. [2019] Lorenzo Pasqualetto Cassinis, Robert Fonod, and Eberhard
    Gill. Review of the robustness and applicability of monocular pose estimation
    systems for relative navigation with an uncooperative spacecraft. *Progress in
    Aerospace Sciences*, 110:100548, 2019. ISSN 0376-0421. doi:[https://doi.org/10.1016/j.paerosci.2019.05.008](https://doi.org/https://doi.org/10.1016/j.paerosci.2019.05.008).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. [2018] Jian Feng Shi, Steve Ulrich, and Stéphane Ruel. Cubesat simulation
    and detection using monocular camera images and convolutional neural networks.
    In *2018 AIAA Guidance, Navigation, and Control Conference*, page 1604, 2018.
    doi:[10.2514/6.2018-1604](https://doi.org/10.2514/6.2018-1604).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. [2018a] Sumant Sharma, Connor Beierle, and Simone D'Amico. Pose
    estimation for non-cooperative spacecraft rendezvous using convolutional neural
    networks. In *2018 IEEE Aerospace Conference*. IEEE, March 2018a. doi:[10.1109/aero.2018.8396425](https://doi.org/10.1109/aero.2018.8396425).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma and D’Amico [2019] Sumant Sharma and Simone D’Amico. Pose estimation
    for non-cooperative spacecraft rendezvous using neural networks. In *Proceedings
    of the AIAA/AAS Space Flight Mechanics Meeting*, 2019. URL [https://arxiv.org/abs/1906.09868v1](https://arxiv.org/abs/1906.09868v1).
    report number: AAS 19-350.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kendall et al. [2015] Alex Kendall, Matthew Grimes, and Roberto Cipolla. PoseNet:
    A convolutional network for real-time 6-DOF camera relocalization. In *2015 IEEE
    International Conference on Computer Vision (ICCV)*, pages 2938–2946, Santiago,
    Chile, October 2015\. IEEE. doi:[10.1109/iccv.2015.336](https://doi.org/10.1109/iccv.2015.336).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. [2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    Going deeper with convolutions. In *2015 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*. IEEE, June 2015. doi:[10.1109/cvpr.2015.7298594](https://doi.org/10.1109/cvpr.2015.7298594).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2017] Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. DeepVO:
    Towards end-to-end visual odometry with deep recurrent convolutional neural networks.
    In *2017 IEEE International Conference on Robotics and Automation (ICRA)*. IEEE,
    May 2017. doi:[10.1109/icra.2017.7989236](https://doi.org/10.1109/icra.2017.7989236).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Donahue et al. [2015] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
    Marcus Rohrbach, Subhashini Venugopalan, Trevor Darrell, and Kate Saenko. Long-term
    recurrent convolutional networks for visual recognition and description. In *2015
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. IEEE, June
    2015. doi:[10.1109/cvpr.2015.7298878](https://doi.org/10.1109/cvpr.2015.7298878).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. [2015] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
    Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers,
    and Thomas Brox. FlowNet: Learning optical flow with convolutional networks. In
    *2015 IEEE International Conference on Computer Vision (ICCV)*. IEEE, October
    2015. doi:[10.1109/iccv.2015.316](https://doi.org/10.1109/iccv.2015.316).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geiger et al. [2011] Andreas Geiger, Julius Ziegler, and Christoph Stiller.
    StereoScan: Dense 3D reconstruction in real-time. In *2011 IEEE Intelligent Vehicles
    Symposium (IV)*. IEEE, June 2011. doi:[10.1109/ivs.2011.5940405](https://doi.org/10.1109/ivs.2011.5940405).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rad and Lepetit [2017] Mahdi Rad and Vincent Lepetit. BB8: A scalable, accurate,
    robust to partial occlusion method for predicting the 3D poses of challenging
    objects without using depth. In *2017 IEEE International Conference on Computer
    Vision (ICCV)*, October 2017. doi:[10.1109/iccv.2017.413](https://doi.org/10.1109/iccv.2017.413).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szeliski [2010] Richard Szeliski. *Computer Vision: Algorithms and Applications*.
    Springer Science & Business Media, 2010. doi:[10.1007/978-1-84882-935-0](https://doi.org/10.1007/978-1-84882-935-0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very deep
    convolutional networks for large-scale image recognition, 2014. URL [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hartley and Zisserman [2004] Richard Hartley and Andrew Zisserman. *Multiple
    View Geometry in Computer Vision*. Cambridge University Press, 2nd edition, 2004.
    doi:[10.1017/CBO9780511811685](https://doi.org/10.1017/CBO9780511811685).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sonawani et al. [2020] Shubham Sonawani, Ryan Alimo, Renaud Detry, Daniel Jeong,
    Andrew Hess, and Heni Ben Amor. Assistive relative pose estimation for on-orbit
    assembly using convolutional neural networks. In *AIAA Scitech 2020 Forum*. American
    Institute of Aeronautics and Astronautics, January 2020. doi:[10.2514/6.2020-2096](https://doi.org/10.2514/6.2020-2096).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] Bo Chen, Jiewei Cao, Alvaro Parra, and Tat-Jun Chin. Satellite
    pose estimation with deep landmark regression and nonlinear pose refinement. In
    *2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)*.
    IEEE, October 2019. doi:[10.1109/iccvw.2019.00343](https://doi.org/10.1109/iccvw.2019.00343).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huan et al. [2020] Wenxiu Huan, Mingmin Liu, and Qinglei Hu. Pose estimation
    for non-cooperative spacecraft based on deep learning. In *2020 39th Chinese Control
    Conference (CCC)*, pages 3339–3343, 2020. doi:[10.23919/CCC50068.2020.9189253](https://doi.org/10.23919/CCC50068.2020.9189253).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oestreich et al. [2020] Charles Oestreich, Tae W. Lim, and Randy Broussard.
    On-orbit relative pose initialization via convolutional neural networks. In *AIAA
    Scitech 2020 Forum*. American Institute of Aeronautics and Astronautics, January
    2020. doi:[10.2514/6.2020-0457](https://doi.org/10.2514/6.2020-0457).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosmas and Kenichi [2020] Kiruki Cosmas and Asami Kenichi. Utilization of FPGA
    for onboard inference of landmark localization in CNN-based spacecraft pose estimation.
    *Aerospace*, 7(11), 2020. ISSN 2226-4310. doi:[10.3390/aerospace7110159](https://doi.org/10.3390/aerospace7110159).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hirano et al. [2018] Daichi Hirano, Hiroki Kato, and Tatsuhiko Saito. Deep learning
    based pose estimation in space. In *Proceedings of the International Symposium
    on Artificial Intelligence, Robotics and Automation in Space (i-SAIRAS)*, 2018.
    URL [https://robotics.estec.esa.int/i-SAIRAS/isairas2018/Papers/Session%203c/4_isairas2018_deep_ver1-39-77-Hirano-Daichi.pdf](https://robotics.estec.esa.int/i-SAIRAS/isairas2018/Papers/Session%203c/4_isairas2018_deep_ver1-39-77-Hirano-Daichi.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downes et al. [2020a] Lena Downes, Ted J. Steiner, and Jonathan P. How. Deep
    learning crater detection for lunar terrain relative navigation. In *AIAA Scitech
    2020 Forum*. American Institute of Aeronautics and Astronautics, January 2020a.
    doi:[10.2514/6.2020-1838](https://doi.org/10.2514/6.2020-1838).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downes et al. [2020b] Lena M. Downes, Ted J. Steiner, and Jonathan P. How. Lunar
    terrain relative navigation using a convolutional neural network for visual crater
    detection. In *2020 American Control Conference (ACC)*, pages 4448–4453, 2020b.
    doi:[10.23919/ACC45564.2020.9147595](https://doi.org/10.23919/ACC45564.2020.9147595).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cassinis et al. [2020] Lorenzo Pasqualetto Cassinis, Robert Fonod, Eberhard
    Gill, Ingo Ahrns, and Jesus Gil Fernandez. CNN-based pose estimation system for
    close-proximity operations around uncooperative spacecraft. In *AIAA Scitech 2020
    Forum*. American Institute of Aeronautics and Astronautics, January 2020. doi:[10.2514/6.2020-1457](https://doi.org/10.2514/6.2020-1457).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Campbell et al. [2017] Tanner Campbell, Roberto Furfaro, Richard Linares, and
    David Gaylor. A deep learning approach for optical autonomous planetary relative
    terrain navigation. In *27th AAS/AIAA Space Flight Mechanics Meeting, 2017*, pages
    3293–3302\. Univelt Inc., 2017. URL [http://arclab.mit.edu/wp-content/uploads/2018/10/2017_06.pdf](http://arclab.mit.edu/wp-content/uploads/2018/10/2017_06.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. [2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    Imagenet classification with deep convolutional neural networks. In F. Pereira,
    C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, *Advances in Neural
    Information Processing Systems*, volume 25. Curran Associates, Inc., 2012. URL
    [https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE
    Conference on Computer Vision and Pattern Recognition*, pages 248–255, 2009. doi:[10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persson et al. [2006] Staffan Persson, Per Bodin, Eberhard Gill, Jon Harr, and
    John Jörgensen. PRISMA - An autonomous formation flying mission. In *Small Satellite
    Systems and Services - The 4S Symposium*, 2006. URL [https://elib.dlr.de/46839/](https://elib.dlr.de/46839/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2017] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
    R-CNN: Towards real-time object detection with region proposal networks. *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, 39(6):1137–1149, June
    2017. doi:[10.1109/tpami.2016.2577031](https://doi.org/10.1109/tpami.2016.2577031).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma and D’Amico [2020] Sumant Sharma and Simone D’Amico. Neural network-based
    pose estimation for noncooperative spacecraft rendezvous. *IEEE Transactions on
    Aerospace and Electronic Systems*, 56(6):4638–4658, 2020. doi:[10.1109/TAES.2020.2999148](https://doi.org/10.1109/TAES.2020.2999148).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kehl et al. [2017] Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan
    Ilic, and Nassir Navab. SSD-6D: Making RGB-based 3D detection and 6D pose estimation
    great again. In *2017 IEEE International Conference on Computer Vision (ICCV)*.
    IEEE, October 2017. doi:[10.1109/iccv.2017.169](https://doi.org/10.1109/iccv.2017.169).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. [2018b] Sumant Sharma, Jacopo Ventura, and Simone D’Amico. Robust
    model-based monocular pose initialization for noncooperative spacecraft rendezvous.
    *Journal of Spacecraft and Rockets*, 55(6):1414–1429, nov 2018b. doi:[10.2514/1.a34124](https://doi.org/10.2514/1.a34124).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proença and Gao [2020] P. F. Proença and Y. Gao. Deep learning for spacecraft
    pose estimation from photorealistic rendering. In *2020 IEEE International Conference
    on Robotics and Automation (ICRA)*, pages 6007–6013, Paris, France, 2020. doi:[10.1109/ICRA40945.2020.9197244](https://doi.org/10.1109/ICRA40945.2020.9197244).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition (CVPR)*, pages 770–778, 2016. doi:[10.1109/cvpr.2016.90](https://doi.org/10.1109/cvpr.2016.90).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2011] Lingqiao Liu, Lei Wang, and Xinwang Liu. In defense of soft-assignment
    coding. In *2011 International Conference on Computer Vision*, pages 2486–2493,
    2011. doi:[10.1109/iccv.2011.6126534](https://doi.org/10.1109/iccv.2011.6126534).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe and Szegedy [2015] Sergey Ioffe and Christian Szegedy. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In *International
    conference on machine learning*, pages 448–456\. PMLR, 2015. URL [http://proceedings.mlr.press/v37/ioffe15.html](http://proceedings.mlr.press/v37/ioffe15.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koenig and Howard [2004] Nathan Koenig and Andrew Howard. Design and use paradigms
    for gazebo, an open-source multi-robot simulator. In *2004 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566)*,
    volume 3, pages 2149–2154, 2004. doi:[10.1109/iros.2004.1389727](https://doi.org/10.1109/iros.2004.1389727).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arakawa et al. [2019] Ryohei Arakawa, Yuri Matsushita, Toshiya Hanada, Yasuhiro
    Yoshimura, and Shuji Nagasaki. Attitude estimation of space objects using imaging
    observations and deep learning. In *Advanced Maui Optical and Space Surveillance
    Technologies Conference*, page 21, 2019. URL [https://amostech.com/TechnicalPapers/2019/Non-Resolved-Object-Characterization/Arakawa.pdf](https://amostech.com/TechnicalPapers/2019/Non-Resolved-Object-Characterization/Arakawa.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phisannupawong et al. [2020] Thaweerath Phisannupawong, Patcharin Kamsing, Peerapong
    Torteeka, Sittiporn Channumsin, Utane Sawangwit, Warunyu Hematulin, Tanatthep
    Jarawan, Thanaporn Somjit, Soemsak Yooyen, Daniel Delahaye, and Pisit Boonsrimuang.
    Vision-based spacecraft pose estimation via a deep convolutional neural network
    for noncooperative docking operations. *Aerospace*, 7(9):126, 2020. doi:[10.3390/aerospace7090126](https://doi.org/10.3390/aerospace7090126).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kechagias-Stamatis et al. [2020] Odysseas Kechagias-Stamatis, Nabil Aouf, Vincent
    Dubanchet, and Mark A Richardson. DeepLO: Multi-projection deep LIDAR odometry
    for space orbital robotics rendezvous relative navigation. *Acta Astronautica*,
    177:270–285, 2020. doi:[10.1016/j.actaastro.2020.07.034](https://doi.org/10.1016/j.actaastro.2020.07.034).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besl and McKay [1992] P.J. Besl and Neil D. McKay. A method for registration
    of 3-D shapes. *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    14(2):239–256, February 1992. doi:[10.1109/34.121791](https://doi.org/10.1109/34.121791).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sattler et al. [2019] Torsten Sattler, Qunjie Zhou, Marc Pollefeys, and Laura
    Leal-Taixe. Understanding the limitations of CNN-based absolute camera pose regression.
    In *2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    pages 3302–3312, 2019. doi:[10.1109/cvpr.2019.00342](https://doi.org/10.1109/cvpr.2019.00342).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. [2019] Tae Ha Park, Sumant Sharma, and Simone D’Amico. Towards robust
    learning-based pose estimation of noncooperative spacecraft. 2019. URL [http://arxiv.org/abs/1909.00392](http://arxiv.org/abs/1909.00392).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon and Farhadi [2018] Joseph Redmon and Ali Farhadi. Yolov3: An incremental
    improvement. 2018. URL [http://arxiv.org/abs/1804.02767](http://arxiv.org/abs/1804.02767).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler et al. [2018] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks.
    In *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*. IEEE,
    jun 2018. doi:[10.1109/cvpr.2018.00474](https://doi.org/10.1109/cvpr.2018.00474).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Howard et al. [2017] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient
    convolutional neural networks for mobile vision applications. 2017. URL [http://arxiv.org/abs/1704.04861](http://arxiv.org/abs/1704.04861).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geirhos et al. [2018] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias
    Bethge, Felix A Wichmann, and Wieland Brendel. ImageNet-trained CNNs are biased
    towards texture; increasing shape bias improves accuracy and robustness. 2018.
    URL [http://arxiv.org/abs/1811.12231](http://arxiv.org/abs/1811.12231).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang and Belongie [2017] Xun Huang and Serge Belongie. Arbitrary style transfer
    in real-time with adaptive instance normalization. In *2017 IEEE International
    Conference on Computer Vision (ICCV)*, pages 1501–1510, 2017. doi:[10.1109/iccv.2017.167](https://doi.org/10.1109/iccv.2017.167).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2019] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution
    representation learning for human pose estimation. In *2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. IEEE, June 2019. doi:[10.1109/cvpr.2019.00584](https://doi.org/10.1109/cvpr.2019.00584).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huo et al. [2020] Yurong Huo, Zhi Li, and Feng Zhang. Fast and accurate spacecraft
    pose estimation from single shot space imagery using box reliability and keypoints
    existence judgments. *IEEE Access*, 8:216283–216297, 2020. doi:[10.1109/ACCESS.2020.3041415](https://doi.org/10.1109/ACCESS.2020.3041415).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferraz et al. [2014] Luis Ferraz, Xavier Binefa, and Francesc Moreno-Noguer.
    Leveraging feature uncertainty in the PnP problem. In *Proceedings of the British
    Machine Vision Conference 2014*. British Machine Vision Association, 2014. doi:[10.5244/c.28.83](https://doi.org/10.5244/c.28.83).
    URL [https://doi.org/10.5244%2Fc.28.83](https://doi.org/10.5244%2Fc.28.83).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. [2017] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
    Alexander Alemi. Inception-v4, Inception-ResNet and the impact of residual connections
    on learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    2017. URL [http://arxiv.org/abs/1602.07261](http://arxiv.org/abs/1602.07261).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO:
    Common objects in context. In *Computer Vision – ECCV 2014*, pages 740–755, 2014.
    doi:[10.1007/978-3-319-10602-1_48](https://doi.org/10.1007/978-3-319-10602-1_48).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ming [2018] Yang Ming. Multi-pattern 3d intelligent reconstruction method for
    non-cooperative space targets based on deep learning. Master’s thesis, Harbin
    Institute of Technology, Harbin, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. [2014] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev,
    Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional
    architecture for fast feature embedding. In *Proceedings of the 22nd ACM international
    conference on Multimedia*, pages 675–678, 2014. doi:[10.1145/2647868.2654889](https://doi.org/10.1145/2647868.2654889).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi [2020] Lin Yi. Research on method of docking ring spatial position intelligent
    perception. Master’s thesis, Harbin Institute of Technology, Harbin, 2020. URL
    [https://cdmd.cnki.com.cn/Article/CDMD-10213-1020396416.htm](https://cdmd.cnki.com.cn/Article/CDMD-10213-1020396416.htm).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiaming [2018] Chen Jiaming. Research and simulation of satellite positioning
    error compensation technology based on convolution neural network. Master’s thesis,
    Beijing University of Posts and Telecommunications, Beijing, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2018] Hao Wang, Jie Jiang, and Guangjun Zhang. CraterIDNet: An
    end-to-end fully convolutional neural network for crater detection and identification
    in remotely sensed planetary images. *Remote sensing*, 10(7):1067, 2018. doi:[10.3390/rs10071067](https://doi.org/10.3390/rs10071067).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaniman et al. [1991] David Vaniman, Robert Reedy, Grant Heiken, Gary Olhoeft,
    and Wendell Mendell. The lunar environment. *The lunar Sourcebook*, pages 27–60,
    1991. URL [https://ci.nii.ac.jp/naid/10003734174/en/](https://ci.nii.ac.jp/naid/10003734174/en/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emami et al. [2015] Ebrahim Emami, George Bebis, Ara Nefian, and Terry Fong.
    Automatic crater detection using convex grouping and convolutional neural networks.
    In *International symposium on visual computing*, pages 213–224, 2015. doi:[10.1007/978-3-319-27863-6_20](https://doi.org/10.1007/978-3-319-27863-6_20).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen et al. [2016] Joseph Paul Cohen, Henry Z Lo, Tingting Lu, and Wei Ding.
    Crater detection via convolutional neural networks. 2016. URL [http://arxiv.org/abs/1601.00978](http://arxiv.org/abs/1601.00978).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Palafox et al. [2017] Leon F Palafox, Christopher W Hamilton, Stephen P Scheidt,
    and Alexander M Alvarez. Automated detection of geological landforms on Mars using
    convolutional neural networks. *Computers & Geosciences*, 101:48–56, 2017. doi:[10.1016/j.cageo.2016.12.015](https://doi.org/10.1016/j.cageo.2016.12.015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'O’Keefe and Ahrens [1999] J. D. O’Keefe and T. J. Ahrens. Complex craters:
    Relationship of stratigraphy and rings to impact conditions. *Journal of Geophysical
    Research Planets*, 104:27091–27104, 1999. doi:[10.1029/1998je000596](https://doi.org/10.1029/1998je000596).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klear [2018] Michael R Klear. Pycda: An open-source library for automated crater
    detection. *Proceedings of the 9th Planetary Crater Consort, Boulder, CO*, 2018.
    URL [http://planetarycraterconsortium.nau.edu/KlearPCC9.pdf](http://planetarycraterconsortium.nau.edu/KlearPCC9.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robbins and Hynek [2012] Stuart J. Robbins and Brian M. Hynek. A new global
    database of mars impact craters $\geq$1 km: 2\. global crater properties and regional
    variations of the simple-to-complex transition diameter. *Journal of Geophysical
    Research: Planets*, 117(E6), June 2012. doi:[10.1029/2011je003967](https://doi.org/10.1029/2011je003967).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silburt et al. [2019] Ari Silburt, Mohamad Ali-Dib, Chenchong Zhu, Alan Jackson,
    Diana Valencia, Yevgeni Kissin, Daniel Tamayo, and Kristen Menou. Lunar crater
    identification via deep learning. *Icarus*, 317:27–38, 2019. doi:[10.1016/j.icarus.2018.06.022](https://doi.org/10.1016/j.icarus.2018.06.022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    U-net: Convolutional networks for biomedical image segmentation. In *Lecture Notes
    in Computer Science*, pages 234–241\. Springer International Publishing, 2015.
    doi:[10.1007/978-3-319-24574-4_28](https://doi.org/10.1007/978-3-319-24574-4_28).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. [2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural
    networks from overfitting. *Journal of Machine Learning Research*, 15(1):1929–1958,
    2014. URL [http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Povilaitis et al. [2018] RZ Povilaitis, MS Robinson, CH Van der Bogert, Harald
    Hiesinger, HM Meyer, and LR Ostrach. Crater density differences: Exploring regional
    resurfacing, secondary crater populations, and crater saturation equilibrium on
    the moon. *Planetary and Space Science*, 162:41–51, 2018. doi:[10.1016/j.pss.2017.05.006](https://doi.org/10.1016/j.pss.2017.05.006).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Head et al. [2010] James W Head, Caleb I Fassett, Seth J Kadish, David E Smith,
    Maria T Zuber, Gregory A Neumann, and Erwan Mazarico. Global distribution of large
    lunar craters: Implications for resurfacing and impactor populations. *science*,
    329(5998):1504–1507, 2010. doi:[10.1126/science.1195050](https://doi.org/10.1126/science.1195050).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2020] Hoonhee Lee, Han-Lim Choi, Dawoon Jung, and Sujin Choi. Deep
    neural network-based landmark selection method for optical navigation on lunar
    highlands. *IEEE Access*, 8:99010–99023, 2020. doi:[10.1109/ACCESS.2020.2996403](https://doi.org/10.1109/ACCESS.2020.2996403).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Klumpp [1974] Allan R Klumpp. Apollo lunar descent guidance. *Automatica*, 10(2):133–146,
    1974. doi:[10.1016/0005-1098(74)90019-3](https://doi.org/10.1016/0005-1098(74)90019-3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Epp et al. [2008] Chirold D Epp, Edward A Robertson, and Tye Brady. Autonomous
    landing and hazard avoidance technology (ALHAT). In *2008 IEEE Aerospace Conference*,
    pages 1–7, 2008. doi:[10.1109/aero.2008.4526297](https://doi.org/10.1109/aero.2008.4526297).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furfaro et al. [2012] Roberto Furfaro, Wolfgang Fink, and Jeffrey S Kargel.
    Autonomous real-time landing site selection for Venus and Titan using evolutionary
    fuzzy cognitive maps. *Applied Soft Computing*, 12(12):3825–3839, 2012. doi:[10.1016/j.asoc.2012.01.014](https://doi.org/10.1016/j.asoc.2012.01.014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maturana and Scherer [2015] Daniel Maturana and Sebastian Scherer. 3D convolutional
    neural networks for landing zone detection from LiDAR. In *2015 IEEE International
    Conference on Robotics and Automation (ICRA)*, pages 3471–3478, 2015. doi:[10.1109/icra.2015.7139679](https://doi.org/10.1109/icra.2015.7139679).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lunghi and Lavagna [2014] Paolo Lunghi and Michèle Lavagna. Autonomous vision-based
    hazard map generator for planetary landing phases. In *65th International Astronautical
    Congress (IAC)*, pages 5103–5114, 2014. URL [http://hdl.handle.net/11311/861150](http://hdl.handle.net/11311/861150).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lunghi et al. [2016] Paolo Lunghi, Marco Ciarambino, and Michèle Lavagna. A
    multilayer perceptron hazard detector for vision-based autonomous planetary landing.
    *Advances in Space Research*, 58(1):131–144, July 2016. doi:[10.1016/j.asr.2016.04.012](https://doi.org/10.1016/j.asr.2016.04.012).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moghe and Zanetti [2020a] Rahul Moghe and Renato Zanetti. On-line hazard detection
    algorithm for precision lunar landing using semantic segmentation. In *AIAA Scitech
    2020 Forum*. American Institute of Aeronautics and Astronautics, January 2020a.
    doi:[10.2514/6.2020-0462](https://doi.org/10.2514/6.2020-0462).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moghe and Zanetti [2020b] Rahul Moghe and Renato Zanetti. A deep learning approach
    to hazard detection for autonomous lunar landing. *The Journal of the Astronautical
    Sciences*, 67(4):1811–1830, 2020b. doi:[10.1007/s40295-020-00239-8](https://doi.org/10.1007/s40295-020-00239-8).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Buslaev et al. [2020] Alexander Buslaev, Vladimir I Iglovikov, Eugene Khvedchenya,
    Alex Parinov, Mikhail Druzhinin, and Alexandr A Kalinin. Albumentations: Fast
    and flexible image augmentations. *Information*, 11(2):125, 2020. doi:[10.3390/info11020125](https://doi.org/10.3390/info11020125).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh and Lim [2008] Leena Singh and Sungyung Lim. On lunar on-orbit vision-based
    navigation: Terrain mapping, feature tracking driven EKF. In *AIAA Guidance, Navigation
    and Control Conference and Exhibit*, page 6834, 2008. doi:[10.2514/6.2008-6834](https://doi.org/10.2514/6.2008-6834).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2019] Chengchao Bai, Jifeng Guo, Linli Guo, and Junlin Song. Deep
    multi-layer perception based terrain classification for planetary exploration
    rovers. *Sensors*, 19(14):3102, 2019. doi:[10.3390/s19143102](https://doi.org/10.3390/s19143102).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chengchao [2019] Bai Chengchao. *Research on vision/vibration based Terrain
    perception for rovers*. PhD thesis, Harbin Institute of Technology, Harbin, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furfaro et al. [2018] Roberto Furfaro, Ilaria Bloise, Marcello Orlandelli, Pierluigi
    Di Lizia, Francesco Topputo, Richard Linares, et al. Deep learning for autonomous
    lunar landing. In *2018 AAS/AIAA Astrodynamics Specialist Conference*, pages 3285–3306,
    2018. URL [http://hdl.handle.net/11311/1063150](http://hdl.handle.net/11311/1063150).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patterson and Rao [2014] Michael A. Patterson and Anil V. Rao. GPOPS-II: A
    MATLAB software for solving multiple-phase optimal control problems using hp-adaptive
    gaussian quadrature collocation methods and sparse nonlinear programming. *ACM
    Transactions on Mathematical Software*, 41(1):1–37, October 2014. doi:[10.1145/2558904](https://doi.org/10.1145/2558904).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furfaro and Law [2016] Roberto Furfaro and Andrew M. Law. Relative optical navigation
    around small bodies via extreme learning machines. In *2015 AAS/AIAA Astrodynamics
    Specialist Conference*, volume 156, pages 1959–1978, 2016. URL [http://www.scopus.com/inward/record.url?scp=85007336458&partnerID=8YFLogxK](http://www.scopus.com/inward/record.url?scp=85007336458&partnerID=8YFLogxK).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silburt et al. [2018] Ari Silburt, Chenchong Zhu, Mohamad Ali-Dib, Kristen
    Menou, and Alan Jackson. DeepMoon: Convolutional neural network trainer to identify
    Moon craters. *Astrophysics Source Code Library*, 2018. URL [https://ui.adsabs.harvard.edu/abs/2018ascl.soft05029S](https://ui.adsabs.harvard.edu/abs/2018ascl.soft05029S).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pugliatti and Topputo [2021] Mattia Pugliatti and Francesco Topputo. Navigation
    about irregular bodies through segmentation maps. In *31st Space Flight Mechanics
    Meeting*, pages AAS21–383, 2021. URL [http://hdl.handle.net/11311/1163932](http://hdl.handle.net/11311/1163932).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beauchamp et al. [2017] P. M. Beauchamp, J. A. Cutts, C. Mercer, and L. A. Dudzinski.
    Technology Planning for NASA’s Future Planetary Science Missions. In *Planetary
    Science Vision 2050 Workshop*, volume 1989, page 8051, February 2017. URL [https://ui.adsabs.harvard.edu/abs/2017LPICo1989.8051B](https://ui.adsabs.harvard.edu/abs/2017LPICo1989.8051B).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuang and Pingyuan [2008] Li Shuang and Cui Pingyuan. Landmark tracking based
    autonomous navigation schemes for landing spacecraft on asteroids. *Acta Astronautica*,
    62(6-7):391–403, 2008. doi:[10.1016/j.actaastro.2007.11.009](https://doi.org/10.1016/j.actaastro.2007.11.009).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guffanti [2018] Tommaso Guffanti. Multi-objective autonomous spacecraft motion
    planning around near-earth asteroids using machine learning. Technical Report
    1-6, 2018. URL [http://cs229.stanford.edu/proj2018/report/223.pdf](http://cs229.stanford.edu/proj2018/report/223.pdf).
    CS 229: Final Project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravani et al. [2021] Khilan Ravani, S. Mathavaraj, and Radhakant Padhi. Site
    detection for autonomous soft-landing on asteroids using deep learning. *Transactions
    of the Indian National Academy of Engineering*, 6(2):365–375, 2021. doi:[10.1007/s41403-021-00207-0](https://doi.org/10.1007/s41403-021-00207-0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harl et al. [2013] Nathan Harl, Karthikeyan Rajagopal, and SN Balakrishnan.
    Neural network based modified state observer for orbit uncertainty estimation.
    *Journal of Guidance, Control, and Dynamics*, 36(4):1194–1209, 2013. doi:[10.2514/1.55711](https://doi.org/10.2514/1.55711).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2019] Yu Song, Lin Cheng, and Shengping Gong. Fast estimation of
    gravitational field of irregular asteroids based on deep neural network and its
    application. In *Advances in the Astronautical Sciences AAS/AIAA Spaceflight Mechanics*,
    volume 168, pages AAS 19–397, 2019. URL [https://www.researchgate.net/profile/Yu-Song-45/publication/331523846_FAST_ESTIMATION_OF_GRAVITATIONAL_FIELD_OF_IRREGULAR_ASTEROIDS_BASED_ON_DEEP_NEURAL_NETWORK_AND_ITS_APPLICATION/links/5c7e5d9c458515831f8421a7/FAST-ESTIMATION-OF-GRAVITATIONAL-FIELD-OF-IRREGULAR-ASTEROIDS-BASED-ON-DEEP-NEURAL-NETWORK-AND-ITS-APPLICATION.pdf](https://www.researchgate.net/profile/Yu-Song-45/publication/331523846_FAST_ESTIMATION_OF_GRAVITATIONAL_FIELD_OF_IRREGULAR_ASTEROIDS_BASED_ON_DEEP_NEURAL_NETWORK_AND_ITS_APPLICATION/links/5c7e5d9c458515831f8421a7/FAST-ESTIMATION-OF-GRAVITATIONAL-FIELD-OF-IRREGULAR-ASTEROIDS-BASED-ON-DEEP-NEURAL-NETWORK-AND-ITS-APPLICATION.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalita et al. [2017] Himangshu Kalita, Erik Asphaug, Stephen Schwartz, and Jekanthan
    Thangavelautham. Network of nano-landers for in-situ characterization of asteroid
    impact studies. In *68th International Astronautical Congress (IAC)*, Adelaide,
    Australia, 2017. URL [http://arxiv.org/abs/1709.02885](http://arxiv.org/abs/1709.02885).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feruglio et al. [2016] Lorenzo Feruglio, Sabrina Corpino, and Daniele Calvi.
    Neural networks for event detection: an interplanetary CubeSat asteroid mission
    case study. In *AIAA SPACE 2016*. American Institute of Aeronautics and Astronautics,
    sep 2016. doi:[10.2514/6.2016-5615](https://doi.org/10.2514/6.2016-5615).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viavattene and Ceriotti [2019] Giulia Viavattene and Matteo Ceriotti. Artificial
    neural network for preliminary multiple nea rendezvous mission using low thrust.
    In *70th International Astronautical Congress (IAC), Washington, DC, USA*, 2019.
    URL [http://eprints.gla.ac.uk/202035/1/202035.pdf](http://eprints.gla.ac.uk/202035/1/202035.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pugliatti and Topputo [2020] Mattia Pugliatti and Francesco Topputo. Small-body
    shape recognition with convolutional neural network and comparison with explicit
    features based method. In *2020 AAS/AIAA Astrodynamics Specialist Conference*,
    pages 1–20, 2020. URL [https://re.public.polimi.it/retrieve/handle/11311/1145538/537728/PUGLM01-20.pdf](https://re.public.polimi.it/retrieve/handle/11311/1145538/537728/PUGLM01-20.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi et al. [2016] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
    Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu
    Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed
    systems. 2016. URL [http://arxiv.org/abs/1603.04467](http://arxiv.org/abs/1603.04467).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. [2015] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
    convolutional networks for semantic segmentation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 3431–3440, 2015.
    URL [https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
