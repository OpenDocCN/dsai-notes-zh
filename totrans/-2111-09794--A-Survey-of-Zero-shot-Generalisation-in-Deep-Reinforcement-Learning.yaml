- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:49:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:49:43'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2111.09794] A Survey of Zero-shot Generalisation in Deep Reinforcement Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2111.09794] 深度强化学习中的零样本泛化调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.09794](https://ar5iv.labs.arxiv.org/html/2111.09794)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2111.09794](https://ar5iv.labs.arxiv.org/html/2111.09794)
- en: A Survey of Zero-shot Generalisation in Deep
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度
- en: Reinforcement Learning
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: \nameRobert Kirk \emailrobert.kirk.20@ucl.ac.uk
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \name罗伯特·柯克 \emailrobert.kirk.20@ucl.ac.uk
- en: \addrUniversity College London, Gower St, London
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \addr伦敦大学学院，戈沃街，伦敦
- en: WC1E 6BT, United Kingdom \AND\nameAmy Zhang \emailamyzhang@fb.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: WC1E 6BT, 英国 \AND\name艾米·张 \emailamyzhang@fb.com
- en: \addrUniversity of California, Berkeley, Berkeley
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \addr加州大学伯克利分校，伯克利
- en: CA, United States
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CA, 美国
- en: Meta AI Research, \AND\nameEdward Grefenstette \emaile.grefenstette@ucl.ac.uk
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Meta AI Research, \AND\name爱德华·格雷芬斯特 \emaile.grefenstette@ucl.ac.uk
- en: \addrUniversity College London, Gower St, London
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \addr伦敦大学学院，戈沃街，伦敦
- en: WC1E 6BT, United Kingdom \AND\nameTim Rocktäschel \emailtim.rocktaschel@ucl.ac.uk
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: WC1E 6BT, 英国 \AND\name蒂姆·罗克塔舍尔 \emailtim.rocktaschel@ucl.ac.uk
- en: \addrUniversity College London, Gower St, London
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \addr伦敦大学学院，戈沃街，伦敦
- en: WC1E 6BT, United Kingdom
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: WC1E 6BT, 英国
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The study of zero-shot generalisation (ZSG) in deep Reinforcement Learning (RL)
    aims to produce RL algorithms whose policies generalise well to novel unseen situations
    at deployment time, avoiding overfitting to their training environments. Tackling
    this is vital if we are to deploy reinforcement learning algorithms in real world
    scenarios, where the environment will be diverse, dynamic and unpredictable. This
    survey is an overview of this nascent field. We rely on a unifying formalism and
    terminology for discussing different ZSG problems, building upon previous works.
    We go on to categorise existing benchmarks for ZSG, as well as current methods
    for tackling these problems. Finally, we provide a critical discussion of the
    current state of the field, including recommendations for future work. Among other
    conclusions, we argue that taking a purely procedural content generation approach
    to benchmark design is not conducive to progress in ZSG, we suggest fast online
    adaptation and tackling RL-specific problems as some areas for future work on
    methods for ZSG, and we recommend building benchmarks in underexplored problem
    settings such as offline RL ZSG and reward-function variation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（RL）中的零样本泛化（ZSG）研究旨在生成在部署时能够很好地推广到新颖未见情况的强化学习算法，避免对训练环境的过拟合。如果我们要在现实世界场景中部署强化学习算法，这一点至关重要，因为这些环境将是多样化的、动态的和不可预测的。本调查概述了这一新兴领域。我们依靠统一的形式主义和术语来讨论不同的ZSG问题，基于之前的工作。我们进一步分类了现有的ZSG基准，以及当前解决这些问题的方法。最后，我们对该领域的现状进行了批判性讨论，包括对未来工作的建议。我们得出的结论之一是，纯粹采用程序化内容生成方法来设计基准不利于ZSG的进展，我们建议快速在线适应和解决RL特定问题作为未来ZSG方法的研究方向，并推荐在尚未探索的问题设置中建立基准，如离线RL
    ZSG和奖励函数变化。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Reinforcement Learning (RL) has the potential to be used in a wide range of
    applications from autonomous vehicles (?) and algorithm control (?) to robotics
    (?), but to fulfil this potential we need RL algorithms that can be used in the
    real world. Reality is dynamic, open-ended and always changing, and RL algorithms
    will need to be robust to variations in their environments, and have the capability
    to transfer and adapt to unseen (but similar) environments during their deployment.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）具有广泛应用的潜力，从自动驾驶车辆 (?) 和算法控制 (?) 到机器人技术 (?)，但为了实现这种潜力，我们需要可以在现实世界中使用的强化学习算法。现实世界是动态的、开放的且不断变化的，强化学习算法需要对环境的变化具有鲁棒性，并且在部署期间能够迁移和适应未见（但类似）的环境。
- en: '![Refer to caption](img/056818c30cf16165d55cc79fff5cd5fa.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/056818c30cf16165d55cc79fff5cd5fa.png)'
- en: 'Figure 1: Zero-shot Generalisation in Reinforcement Learning. A visualisation
    of three types of environment (columns) with respect to their graphical model,
    training and testing distribution and example benchmarks (rows). Classical RL
    has focused on environments where training and testing are identical (singleton
    environments, first column). We focus on an underexplored setting, inspired by
    likely real-world scenarios, where training and testing environments will be different,
    with environment instances either from the same distribution (Independent and
    Identically Distributed (IID) ZSG Environments, second column) or from different
    distributions (OOD ZSG Environments, third column). The split between the second
    and third columns is just one example of the way in which ZSG is a class of problems
    rather than an individual problem. The top row visualises the differences in graphical
    models between singleton environments and environments where ZSG is required.
    For more information on the CMDP formalism see [Section 3.3](#S3.SS3 "3.3 Contextual
    Markov Decision Processes ‣ 3 Formalising Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：强化学习中的零样本泛化。对三种环境（列）在其图形模型、训练和测试分布以及示例基准（行）方面的可视化。经典RL关注于训练和测试相同的环境（单例环境，第一列）。我们关注一个尚未充分探索的设置，灵感来自于可能的真实世界场景，其中训练和测试环境将不同，环境实例可能来自相同分布（独立同分布（IID）ZSG环境，第二列）或不同分布（OOD
    ZSG环境，第三列）。第二列和第三列之间的分割只是ZSG作为一类问题的一个示例，而不是单个问题。顶行可视化了单例环境与需要ZSG的环境之间的图形模型差异。有关CMDP形式化的更多信息，请参见[第3.3节](#S3.SS3
    "3.3 上下文马尔可夫决策过程 ‣ 3 强化学习中的零样本泛化形式化 ‣ 深度强化学习中的零样本泛化调查")。
- en: 'However, much current RL research works on benchmarks such as Atari (?) and
    MuJoCo (?, ?), which do not have the attributes described above: they evaluate
    the policy on exactly the same environment it was trained on, which often does
    not match with real-world scenarios ([Fig. 1](#S1.F1 "In 1 Introduction ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning") left column). This
    is in stark contrast to the standard assumptions of supervised learning where
    the training and testing sets are disjoint and is likely to lead to strong evaluation
    overfitting (?). This has resulted in policies that perform badly on even slightly
    adjusted environment instances (specific levels or tasks within an environment)
    and often fail on unseen random seeds used for initialisation (?, ?, ?, ?).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前的许多RL研究基准（如Atari（？）和MuJoCo（？，？，？））并没有上述描述的属性：它们在完全相同的环境下评估策略，这通常与真实世界场景不符（[图1](#S1.F1
    "在1介绍 ‣ 深度强化学习中的零样本泛化调查")左列）。这与监督学习的标准假设形成了鲜明对比，在监督学习中，训练集和测试集是不同的，这可能导致严重的评估过拟合（？）。这导致了即使在稍微调整的环境实例（环境中的特定关卡或任务）上表现不佳的策略，并且通常在用于初始化的未见随机种子上失败（？，？，？，？）。
- en: 'In this work we survey the recent literature studying zero-shot generalisation
    in deep RL, a field focused on producing algorithms with the robustness, transfer
    and adaptation properties required to perform well in the real world. We offer
    a unifying framework that builds on previous work (?, ?, ?, ?, ?, ?) that formalises
    the problem of ZSG in RL as a *class* of problems, rather than a single problem.
    While prior work (?, ?, ?) uses the contextual MDP framework or related frameworks
    to describe how an agent can encounter new, unseen states at test time to generalise
    to, we further extend and break down the types of generalisation that can be possible,
    e.g. combinatorial, interpolation vs. extrapolation, single-factor vs. multi-factor
    ([Fig. 3](#S4.F3 "In Controllable Environment Evaluation Protocols. ‣ 4.2 Evaluation
    Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")).
    We formalise more fully the ZSG problem, formally specifying the policy class,
    making clear the choice of whether the context is observed or not, and including
    cases where the context distribution is controllable during training [Definition 5](#Thmdefinition5
    "Definition 5 (ZSPT controllable context). ‣ 3.4 Training And Testing Contexts
    ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning") This breakdown enables
    us to clearly and crisply compare previous works, as well as understand how to
    choose future research directions. For example, improving “generalisation” without
    any additional assumptions is inherently underspecified; it is unlikely that we
    can generically improve generalisation, given this class of problems is so broad
    that some analogy of the No Free Lunch theorem (?) applies: improving generalisation
    in some settings could harm generalisation in others. Two broad categories of
    ZSG problem are shown in [Fig. 1](#S1.F1 "In 1 Introduction ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning") in the centre and right columns.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们调查了近期研究零-shot 泛化在深度强化学习中的文献，这一领域专注于产生具有现实世界表现所需的鲁棒性、迁移性和适应性特性的算法。我们提供了一个统一的框架，基于之前的工作（？，？，？，？，？，？，？）将
    ZSG 问题在强化学习中形式化为一类问题，而不是单一问题。尽管先前的工作（？，？，？）使用上下文 MDP 框架或相关框架来描述代理如何在测试时遇到新的、未见过的状态并进行泛化，但我们进一步扩展并细化了可能的泛化类型，例如组合、插值与外推、单因子与多因子（[图
    3](#S4.F3 "在可控环境评估协议中。 ‣ 4.2 Zsg 的评估协议 ‣ 4 强化学习中零-shot 泛化的基准 ‣ 深度强化学习中零-shot 泛化的调查")）。我们更全面地形式化了
    ZSG 问题，正式指定了策略类，明确选择上下文是否被观察，并包括在训练期间上下文分布可控的情况 [定义 5](#Thmdefinition5 "定义 5 (ZSPT
    可控上下文)。 ‣ 3.4 训练和测试上下文 ‣ 3 形式化强化学习中的零-shot 泛化 ‣ 深度强化学习中零-shot 泛化的调查")。这种细分使我们能够清晰地比较之前的工作，并理解如何选择未来的研究方向。例如，在没有任何额外假设的情况下改进“泛化”本质上是不够明确的；鉴于这一类问题如此广泛，一些类似于无免费午餐定理（？）的类比适用：在某些设置中改进泛化可能会损害其他设置中的泛化。两个广泛的
    ZSG 问题类别如 [图 1](#S1.F1 "在 1 引言 ‣ 深度强化学习中零-shot 泛化的调查") 中的中间和右列所示。
- en: Using this formalism, we survey and examine the range of benchmarks available
    for ZSG in RL, and go on to discuss methods aimed at tackling different ZSG problems.
    Finally, we propose several settings within ZSG which are underexplored but still
    vital for various real-world applications of RL, as well as many avenues for future
    work on methods that can solve different generalisation problems. Throughout,
    we critically review the state of the field and provide recommendations for ensuring
    future research is robust and useful. We aim to make the field more legible to
    researchers and practitioners both in and out of the field and make discussing
    new research directions easier by providing a common reference and framework.
    This new clarity can improve the field, and enable robust progress towards more
    general RL methods.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这一形式化方法，我们调查和审视了 ZSG 在强化学习中的各种基准，并讨论了针对不同 ZSG 问题的方法。最后，我们提出了几个在 ZSG 中未被充分探讨但对
    RL 各种现实应用仍然至关重要的设置，以及解决不同泛化问题的方法的未来工作方向。在整个过程中，我们批判性地回顾了这一领域的现状，并提供了确保未来研究稳健且有用的建议。我们的目标是使这一领域对研究人员和从业者更具可读性，并通过提供一个共同的参考和框架来使讨论新研究方向变得更容易。这种新的清晰度可以改善该领域，并推动向更通用的
    RL 方法的稳健进展。
- en: Scope.
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 范围。
- en: Generalisation in RL captures a large amount of research, so to make this survey
    feasible we limit the scope of our review in several ways. First, we focus on
    the specific problem setting of zero-shot generalisation (ZSG), where the policy
    is evaluated zero-shot on a collection of environment instances different to those
    it was trained on. Crucially, this setting disallows any additional training in
    or data from the test environment instances, meaning methods such as domain adaptation
    and many meta-RL approaches are not applicable. This is different from classical
    RL, which has historically focused on online learning in a single MDP, where generalisation
    refers to the notion of generalising to novel states in the same MDP. While this
    setting can be useful to study, we instead focus on situations which require policies
    that can be deployed in situations they haven’t been trained in, and generalise
    well zero-shot to those situations. This setting is especially relevant for current
    deep RL algorithms, which often aren’t sample-efficient or safe enough to be deployed
    online without significant offline or in-simulation training first. This motivates
    our focus on ZSG. We discuss and motivate this setting and its restrictions more
    in [Section 3.7](#S3.SS7.SSS0.Px2 "Motivating Zero-Shot Policy Transfer ‣ 3.7
    Remarks And Discussion ‣ 3 Formalising Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning").
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的泛化涵盖了大量的研究，因此为了使本调查可行，我们以几种方式限制了回顾的范围。首先，我们关注零-shot 泛化（ZSG）的特定问题设置，其中策略在与训练时不同的环境实例上进行零-shot
    评估。至关重要的是，这种设置不允许在测试环境实例中进行额外的训练或获取数据，这意味着如领域适应和许多元强化学习方法不适用。这与经典的强化学习不同，经典强化学习历史上关注于单一马尔科夫决策过程（MDP）中的在线学习，其中泛化指的是在同一
    MDP 中对新状态的泛化。虽然这种设置可能有用，但我们更关注于需要能够在未经过训练的情况中进行部署的策略，并且能在这些情况中零-shot 泛化。这种设置对于当前的深度强化学习算法特别相关，因为这些算法通常在没有大量离线或仿真训练之前，样本效率或安全性都不足以在线部署。这激励了我们对
    ZSG 的关注。我们在[第3.7节](#S3.SS7.SSS0.Px2 "激励零-shot 策略转移 ‣ 3.7 备注与讨论 ‣ 3 强化学习中的零-shot
    泛化形式化 ‣ 深度强化学习中零-shot 泛化的调查")中对这一设置及其限制进行了更多讨论和阐述。
- en: Second, we only cover single-agent RL in this work. There are generalisation
    problems within multi-agent reinforcement learning (MARL), such as being general
    enough to defeat multiple different opponent strategies (?, ?) and generalising
    to new team-mates in cooperative games (?, ?), but we do not cover any work in
    this area here. While mathematically these problems could be modelled equivalently
    (if co-players are modelled as parts of the dynamics function, rather than as
    agents as in Games-based formulations (?)), approaches to these problems tend
    to be quite different, explicitly utilising the fact that variation and generalisation
    challenges come from other co-players rather than other parts of the environment.
    Relatedly, there is work on using multiple agents in a single-agent setting to
    increase the diversity of the environment and hence the generality of the policy
    (?), which we do cover.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们在本工作中只涵盖了单智能体强化学习（RL）。在多智能体强化学习（MARL）中存在泛化问题，例如需要足够的泛化能力以击败多种不同的对手策略（？，？）以及在合作游戏中对新队友的泛化（？，？），但我们在此不涉及这一领域的工作。尽管从数学上讲，这些问题可以等效建模（如果将合作者建模为动态函数的一部分，而不是像基于博弈的表述中那样建模为智能体（？）），但这些问题的解决方法往往大相径庭，明确利用了变异和泛化挑战来自其他合作者而非环境的其他部分。相关地，也有工作利用多个智能体在单智能体环境中增加环境的多样性，从而提高策略的泛化能力（？），我们在这里进行了涵盖。
- en: Finally, we do not cover theoretical work on generalisation in RL. While there
    is recent work in this area (?, ?) that is valuable, we focus on empirical research
    as it’s more widely studied.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们不涉及强化学习中泛化的理论工作。尽管在这一领域有近期的工作（？，？）是有价值的，但我们更关注于实证研究，因为它的研究更为广泛。
- en: Overview of the Survey.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调查概述。
- en: 'The structure of the survey is as follows. We first briefly describe related
    work such as other surveys and overviews in [Section 2](#S2 "2 Related Work: Surveys
    In Reinforcement Learning Subfields ‣ A Survey of Zero-shot Generalisation in
    Deep Reinforcement Learning"). We introduce the formalism and terminology for
    ZSG in RL in [Section 3](#S3 "3 Formalising Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"),
    including the relevant background. We then proceed to use this formalism to describe
    current benchmarks for ZSG in RL in [Section 4](#S4 "4 Benchmarks For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"), discussing both environments ([Section 4.1](#S4.SS1
    "4.1 Environments ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"))
    and evaluation protocols ([Section 4.2](#S4.SS2 "4.2 Evaluation Protocols For
    Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning ‣ A
    Survey of Zero-shot Generalisation in Deep Reinforcement Learning")). We categorise
    and describe work producing methods for tackling ZSG in [Section 5](#S5 "5 Methods
    For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning"). Finally, we present a critical
    discussion of the current field, including recommendations for future work in
    both methods and benchmarks, in [Section 6](#S6 "6 Discussion And Future Work
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"), and conclude
    with a summary of the key takeaways from the survey in [Section 7](#S7 "7 Conclusion
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '调查的结构如下。我们首先简要描述相关工作，如其他调查和概述，在[第 2 节](#S2 "2 Related Work: Surveys In Reinforcement
    Learning Subfields ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")。我们在[第 3 节](#S3 "3 Formalising Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")介绍
    RL 中 ZSG 的形式化方法和术语，包括相关背景。接下来，我们使用这些形式化方法在[第 4 节](#S4 "4 Benchmarks For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")描述当前 RL 中 ZSG 的基准，讨论环境（[第 4.1 节](#S4.SS1 "4.1
    Environments ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")）和评估协议（[第
    4.2 节](#S4.SS2 "4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")）。我们在[第 5 节](#S5 "5 Methods For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")对处理 ZSG 的方法进行分类和描述。最后，我们在[第 6 节](#S6 "6 Discussion And Future Work ‣
    A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")对当前领域进行批判性讨论，包括对未来方法和基准的建议，并在[第
    7 节](#S7 "7 Conclusion ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")总结调查的关键要点。'
- en: Contributions.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贡献。
- en: 'To summarise, our key contributions are:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的主要贡献是：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present a unified formalism and terminology for discussing the broad class
    of ZSG problems and breaking down the assumptions necessary to achieve ZSG, building
    on formalisms and terminology presented in multiple previous works (?, ?, ?, ?, ?, ?).
    Our contribution here is the unification of these prior works into *a clear formal
    description of the class of problems referred to as ZSG in RL*, which captures
    the full space of problems, which wasn’t done by any one existing formalism.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种统一的形式化方法和术语，用于讨论广泛的 ZSG 问题，并分解实现 ZSG 所需的假设，基于多个先前工作的形式化方法和术语 (?, ?, ?, ?, ?, ?)。我们在此的贡献是将这些先前的工作统一成*一种清晰的形式化描述，称为
    RL 中的 ZSG 问题类别*，这涵盖了所有问题空间，而现有的任何形式化方法都没有做到这一点。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We propose a taxonomy of existing benchmarks that can be used to test for ZSG,
    splitting the discussion into categorising environments and evaluation protocols.
    Our formalism allows us to cleanly describe weaknesses of the purely Procedural
    Content Generation (PCG) approach to ZSG benchmarking and environment design:
    *having a completely PCG environment limits the precision of the research that
    can be done on that environment*. We recommend that *future environments should
    use a combination of PCG and controllable factors of variation*.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种现有基准的分类方法，用于测试 ZSG，将讨论分为环境分类和评估协议。我们的形式化方法允许我们清晰地描述纯粹程序内容生成（PCG）方法在 ZSG
    基准测试和环境设计中的局限性：*完全使用 PCG 环境限制了对该环境进行研究的精确度*。我们建议*未来的环境应使用 PCG 和可控变化因素的组合*。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*We propose a categorisation of existing methods to tackle various ZSG problems*,
    motivated by a desire to make it easy both for practitioners to choose methods
    given a concrete problem and for researchers to understand the landscape of methods
    and where novel and useful contributions could be made. We point to many under-explored
    avenues for further research, including fast online adaptation, tackling RL-specific
    ZSG issues, novel architectures, model-based RL and environment instance generation.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*我们提出了对现有方法的分类，以解决各种ZSG问题*，这旨在使从业者在面对具体问题时选择方法更加便捷，并帮助研究人员了解方法的现状以及在哪里可以做出新颖和有用的贡献。我们指出了许多未被充分探索的进一步研究方向，包括快速在线适应、解决RL特定的ZSG问题、新颖的架构、基于模型的RL和环境实例生成。'
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We critically discuss the current state of ZSG in RL research, recommending
    future research directions. In particular, we argue that *building benchmarks
    would enable progress in offline RL generalisation and reward-function variation*,
    both of which are important settings. Further, we point to several different settings
    and evaluation metrics that are worth exploring: *investigating context-efficiency
    and working in a continual RL setting* are both areas where future work is necessary.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们批判性地讨论了RL中ZSG的当前状态，并推荐未来的研究方向。特别是，我们认为*建立基准将促进离线RL泛化和奖励函数变化的进展*，这两者都是重要的设置。此外，我们指出了值得探索的几种不同设置和评估指标：*研究上下文效率和在持续RL设置中的工作*都是未来工作必需的领域。
- en: '2 Related Work: Surveys In Reinforcement Learning Subfields'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作：强化学习子领域的调查
- en: While there have been previous surveys of related subfields in RL, none have
    covered zero-shot generalisation in RL explicitly. ? (?) motivated and surveyed
    continual reinforcement learning (CRL), which is closely related to ZSG in RL
    as both settings require adaptation to unseen tasks or environments; however,
    they explicitly do not discuss the zero-shot setting that is the concern of this
    paper (for more discussion of CRL see [Section 6.1](#S6.SS1 "6.1 Generalisation
    Beyond Zero-Shot Policy Transfer ‣ 6 Discussion And Future Work ‣ A Survey of
    Zero-shot Generalisation in Deep Reinforcement Learning")). ? (?) gave a brief
    overview of Robust RL (RRL) (?), a field aimed at tackling a specific form of
    environment model misspecification through worst-case optimisation. This is a
    sub-problem within the class of generalisation problems we discuss here, and ? (?)
    only briefly survey the field. ? (?) survey methods for modelling other agents,
    which can be seen as a form of generalisation problem (even in single-agent RL),
    if the environment contains a distribution over agents.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前已经有关于强化学习（RL）相关子领域的调查，但没有专门覆盖RL中的零样本泛化。? (?) 激发并调查了持续强化学习（CRL），这与RL中的ZSG密切相关，因为这两种情境都要求适应未见过的任务或环境；然而，它们并没有明确讨论本论文所关注的零样本情境（有关CRL的更多讨论请参见[第6.1节](#S6.SS1
    "6.1 Generalisation Beyond Zero-Shot Policy Transfer ‣ 6 Discussion And Future
    Work ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")）。? (?)
    简要概述了稳健强化学习（RRL）（?），这是一个旨在通过最坏情况优化解决特定形式环境模型错误的领域。这是我们在这里讨论的一类泛化问题中的一个子问题，而? (?)
    只是简要调查了该领域。? (?) 调查了建模其他代理的方法，这可以视为一种泛化问题（即使在单代理RL中），如果环境包含代理的分布。
- en: ? (?) survey methods for sim-to-real transfer for deep RL in robotics. Sim-to-real
    is a concrete instantiation of the generalisation problem, and hence there is
    some overlap between our work and ? (?), but our work covers a much broader subject
    area, and some methods for sim-to-real transfer rely on data from the testing
    environment (reality), which we do not assume here. ? (?) and ? (?) survey methods
    for transfer learning in RL (TRL). TRL is related to generalisation in that both
    topics assume a policy is trained in a different setting to its deployment, but
    TRL generally assumes some form of extra training in the deployment or target
    environment, whereas we are focused on zero-shot generalisation. Finally, surveys
    on less related topics include ? (?) who survey multi-task deep RL, ? (?) who
    survey exploration in RL, and ? (?) who survey curriculum learning in RL.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 对深度强化学习中机器人模拟到现实转移的调查方法。模拟到现实是泛化问题的具体体现，因此我们的工作与? (?)之间有一些重叠，但我们的工作涵盖了更广泛的主题领域，有些模拟到现实转移的方法依赖于来自测试环境（现实）的数据，而我们在这里不做假设。? (?)
    和 ? (?) 调查了强化学习中的迁移学习（TRL）方法。TRL 与泛化相关，因为这两个主题都假设在不同的设置中训练一个策略，然后在其部署时使用，但TRL
    通常假设在部署或目标环境中进行某种形式的额外训练，而我们则专注于零-shot 泛化。最后，关于相关性较低的主题的调查包括 ? (?) 调查了多任务深度强化学习，? (?)
    调查了强化学习中的探索，以及 ? (?) 调查了强化学习中的课程学习。
- en: None of these surveys focuses on the zero-shot generalisation setting that is
    the focus of this work, and there is still a need for a formalism for the class
    of ZSG problems which will enable research in this field to discuss the differences
    between different problems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些调查都未聚焦于本工作所关注的零-shot 泛化设置，因此仍需要一种 ZSG 问题类别的形式化方法，以便该领域的研究能够讨论不同问题之间的差异。
- en: 3 Formalising Zero-shot Generalisation In Reinforcement Learning
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 正式化零-shot 泛化在强化学习中的应用
- en: In this section, we present a formalism for understanding and discussing the
    class of zero-shot generalisation (ZSG) problems in RL. We first review the relevant
    background in supervised learning and RL before motivating the formalism itself.
    Formalising ZSG in this way shows that it refers to a *class* of problems, rather
    than a specific problem, and hence research on ZSG needs to specify which group
    of ZSG problems it is tackling. Having laid out this class of problems in [Section 3.4](#S3.SS4
    "3.4 Training And Testing Contexts ‣ 3 Formalising Zero-shot Generalisation In
    Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning"), we discuss additional assumptions of structure that could make generalisation
    more tractable in [Section 3.6](#S3.SS6 "3.6 Additional Assumptions For More Feasible
    Generalisation ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"); this
    is effectively specifying sub-problems of the wider ZSG problem.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了一种形式化方法来理解和讨论 RL 中零-shot 泛化（ZSG）问题的类别。我们首先回顾了监督学习和 RL 中的相关背景，然后再引入形式化方法。以这种方式形式化
    ZSG 显示它指的是一个*类别*的问题，而不是特定的问题，因此 ZSG 的研究需要明确其所处理的 ZSG 问题组。在 [第 3.4 节](#S3.SS4 "3.4
    训练与测试背景 ‣ 3 正式化零-shot 泛化在强化学习中的应用 ‣ 零-shot 泛化调查")中列出了这一问题类别，我们在 [第 3.6 节](#S3.SS6
    "3.6 更可行的泛化附加假设 ‣ 3 正式化零-shot 泛化在强化学习中的应用 ‣ 零-shot 泛化调查") 中讨论了可能使泛化更可行的额外结构假设；这实际上是指定了更广泛的
    ZSG 问题的子问题。
- en: '3.1 Background: Generalisation In Supervised Learning'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 背景：监督学习中的泛化
- en: Generalisation in supervised learning is a widely studied area and hence is
    more mature than generalisation in RL (although it is still not well understood).
    In supervised learning, some predictor is trained on a training dataset, and the
    performance of the model is measured on a held-out testing dataset. It is often
    assumed that the data points in both the training and testing dataset are drawn
    independently and identically distributed (IID) from the same underlying distribution,
    although this is not always the case (see for example the Domain Generalisation
    literature (?)). Generalisation performance is then synonymous with the test-time
    performance, as the model needs to “generalise” to inputs it has not seen before
    during training. The generalisation gap in supervised learning for a model $\phi$
    with training and testing data $D_{train},D_{test}$ and loss function $\mathcal{L}$
    is defined as
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习中的泛化是一个广泛研究的领域，因此比RL中的泛化更成熟（尽管仍然不完全理解）。在监督学习中，某个预测器在训练数据集上进行训练，模型的性能在保留的测试数据集上进行测量。通常假设训练和测试数据集中的数据点是从相同的底层分布中独立同分布（IID）抽取的，尽管这并非总是如此（例如，参见领域泛化文献
    (?)）。在监督学习中，泛化性能通常与测试时性能同义，因为模型需要“泛化”到在训练过程中未见过的输入。对于一个模型 $\phi$，训练和测试数据 $D_{train},D_{test}$
    以及损失函数 $\mathcal{L}$，监督学习中的泛化差距定义为
- en: '|  | $\textrm{GenGap}(\phi):=\mathbb{E}_{(x,y)\sim D_{test}}[\mathcal{L}(\phi,x,y)]-\mathbb{E}_{(x,y)\sim
    D_{train}}[\mathcal{L}(\phi,x,y)].$ |  | (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textrm{GenGap}(\phi):=\mathbb{E}_{(x,y)\sim D_{test}}[\mathcal{L}(\phi,x,y)]-\mathbb{E}_{(x,y)\sim
    D_{train}}[\mathcal{L}(\phi,x,y)].$ |  | (1) |'
- en: 'This gap is normally used as a measure of generalisation specifically, independently
    of the train or test performance: for a given level of training performance, a
    smaller gap means a model generalises better. This metric isn’t perfect, as a
    model that performs at random chance in both training and testing will get a gap
    of 0\. Further, if the train and test datasets aren’t drawn IID, then it’s possible
    that the test dataset is easier (or harder), and hence a gap of zero doesn’t necessarily
    imply perfect generalisation. However, it can be used to measure generalisation
    performance across benchmarks where absolute performance may not be comparable,
    or to motivate improvement from methods that may improve generalisation by lowering
    the gap without changing the test performance (in effect by lowering the training
    performance). These methods may then be combined with ones that improve training
    performance to improve the overall test performance, assuming that the methods
    don’t conflict. We introduce this metric for completeness, as it has often been
    used in the literature in addition to test performance. In general, we think it’s
    useful as a metric in addition to test performance, but not as a replacement for
    it. For more discussion, especially in the RL setting, see [Section 3.4](#S3.SS4.SSS0.Px1
    "Evaluating Zero-Shot Generalisation. ‣ 3.4 Training And Testing Contexts ‣ 3
    Formalising Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个差距通常被用作泛化的度量，特别是独立于训练或测试性能：对于给定的训练性能水平，较小的差距意味着模型的泛化能力更强。这个指标并不完美，因为一个在训练和测试中都表现为随机的模型会得到一个0的差距。此外，如果训练和测试数据集不是独立同分布的，那么测试数据集可能更容易（或更难），因此差距为零并不一定意味着完美的泛化。然而，它可以用来测量跨基准的泛化性能，其中绝对性能可能不可比，或者用来推动那些通过降低差距而不改变测试性能（实际上是降低训练性能）来改善泛化的方法。这些方法随后可以与提高训练性能的方法结合使用，以提高整体测试性能，前提是这些方法不冲突。我们引入这个指标是为了完整性，因为它在文献中经常被用作测试性能的补充。一般而言，我们认为它作为测试性能的补充是有用的，但不能替代测试性能。有关更多讨论，特别是在RL设置中，请参见[第3.4节](#S3.SS4.SSS0.Px1
    "评估零-shot泛化 ‣ 3.4 训练与测试背景 ‣ 3 形式化零-shot泛化在强化学习中的应用 ‣ 深度强化学习中零-shot泛化的综述")。
- en: 'One specific type of generalisation examined frequently in supervised learning
    which is relevant to RL is compositional generalisation (?, ?). We explore a categorisation
    of compositional generalisation here introduced by ? (?). While this was designed
    for generalisation in language, many of those forms are relevant for RL. The five
    forms of compositional generalisation defined are:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中经常检查的一种特定类型的泛化，与RL相关的是组成性泛化 (?, ?)。我们在这里探索了由? (?) 引入的组成性泛化的分类。虽然这主要是为语言中的泛化设计的，但许多形式与RL相关。定义的五种组成性泛化形式是：
- en: '1.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'systematicity: generalisation via systematically recombining known parts and
    rules,'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统性：通过系统地重新组合已知部分和规则进行泛化，
- en: '2.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'productivity: the ability to extend predictions beyond the length seen in training
    data,'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生产力：在训练数据中看到的长度之外扩展预测的能力，
- en: '3.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'substitutivity: generalisation via the ability to replace components with synonyms,'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 替代性：通过能够用同义词替换组件进行泛化，
- en: '4.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'localism: if model composition operations are local vs. global,'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本地性：如果模型组成操作是本地的还是全局的，
- en: '5.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'overgeneralisation: if models pay attention to or are robust to exceptions.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过度泛化：如果模型注意到或对例外情况具有鲁棒性。
- en: For intuition, we will explore examples of some of these different types of
    compositional generalisation in a block-stacking environment. An example of *systematicity*
    is the ability to stack blocks in new configurations once the basics of block-stacking
    are mastered. Similarly, *productivity* can be measured by how many blocks the
    agent can generalise to, and the complexity of the stacking configurations. *Substitutivity*
    can be evaluated by the agent’s ability to generalise to blocks of new colours,
    understanding that the new colour does not affect the physics of the block. In
    [Section 4.3](#S4.SS3.SSS0.Px5 "Compositional Generalisation in Contextual MDPs.
    ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")
    we discuss how assumptions of compositional structure in the RL environment can
    enable us to test these forms of generalisation. In [Section 4.2](#S4.SS2 "4.2
    Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")
    we will discuss how some of these forms of generalisation can be evaluated in
    current RL benchmarks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观理解，我们将探索在块堆叠环境中这些不同类型的组合泛化的示例。*系统性*的一个例子是，在掌握了块堆叠的基础知识后，能够以新的配置堆叠块。类似地，*生产力*可以通过代理能够泛化到多少块以及堆叠配置的复杂性来衡量。*替代性*可以通过代理对新颜色块的泛化能力来评估，理解新颜色不会影响块的物理特性。在[第4.3节](#S4.SS3.SSS0.Px5
    "Compositional Generalisation in Contextual MDPs. ‣ 4.3 Discussion ‣ 4 Benchmarks
    For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning")我们讨论了RL环境中组合结构的假设如何使我们能够测试这些形式的泛化。在[第4.2节](#S4.SS2
    "4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")我们将讨论如何在当前的RL基准中评估这些形式的泛化。
- en: '3.2 Background: Reinforcement Learning'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 背景：强化学习
- en: The standard formalism in RL is the Markov Decision Process (MDP). An MDP consists
    of a tuple $M=(S,A,R,T,p)$, where $S$ is the state space; $A$ is the action space;
    $R:S\times A\times S\rightarrow\mathbb{R}$ is the scalar reward function; $T(s^{\prime}|s,a)$
    is the possibly stochastic Markovian transition function; and $p(s_{0})$ is the
    initial state distribution. We also consider partially observable MDPs (POMDPs).
    A POMDP consists of a tuple $M=(S,A,O,R,T,\phi,p)$, where $S,A,R,T$ and $p$ are
    as above, $O$ is the observation space, and $\phi:S\rightarrow O$ is the emission
    or observation function. In POMDPs, the policy only observes the observation of
    the state produced by $\phi$.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: RL中的标准形式是马尔可夫决策过程（MDP）。一个MDP由一个元组 $M=(S,A,R,T,p)$ 组成，其中 $S$ 是状态空间；$A$ 是动作空间；$R:S\times
    A\times S\rightarrow\mathbb{R}$ 是标量奖励函数；$T(s^{\prime}|s,a)$ 是可能的随机马尔可夫转移函数；$p(s_{0})$
    是初始状态分布。我们还考虑部分可观察的MDP（POMDPs）。一个POMDP由一个元组 $M=(S,A,O,R,T,\phi,p)$ 组成，其中 $S,A,R,T$
    和 $p$ 如上所述，$O$ 是观察空间，$\phi:S\rightarrow O$ 是发射或观察函数。在POMDPs中，策略只观察由 $\phi$ 产生的状态的观察结果。
- en: 'The standard problem in an MDP is to learn a policy $\pi(a|s)$ which produces
    a distribution over actions given a state, such that the cumulative reward of
    the policy in the MDP is maximised:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MDP中的标准问题是学习一个策略 $\pi(a|s)$，该策略在给定状态下产生一个动作分布，使得策略在MDP中的累积奖励最大化：
- en: '|  | $\pi^{*}=\underset{\pi\in\Pi}{\textrm{argmax }}\mathbb{E}_{s\sim p(s_{0})}\left[\mathcal{R}(s)\right],$
    |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=\underset{\pi\in\Pi}{\textrm{argmax }}\mathbb{E}_{s\sim p(s_{0})}\left[\mathcal{R}(s)\right],$
    |  |'
- en: where $\pi^{*}$ is the optimal policy, $\Pi$ is the set of all policies, and
    $\mathcal{R}:S\to\mathbb{R}$ is the *return* of a state, calculated as
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi^{*}$ 是最优策略，$\Pi$ 是所有策略的集合，$\mathcal{R}:S\to\mathbb{R}$ 是一个状态的*回报*，计算为
- en: '|  | $\mathcal{R}(s):=\mathbb{E}_{a_{t}\sim\pi(a_{t}&#124;s_{t}),s_{t+1}\sim
    T(s_{t+1}&#124;s_{t},a_{t})}\left[\sum_{t=0}^{\infty}R(s_{t},a_{t},s_{t+1})&#124;s_{0}=s\right].$
    |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}(s):=\mathbb{E}_{a_{t}\sim\pi(a_{t}&#124;s_{t}),s_{t+1}\sim
    T(s_{t+1}&#124;s_{t},a_{t})}\left[\sum_{t=0}^{\infty}R(s_{t},a_{t},s_{t+1})&#124;s_{0}=s\right].$
    |  |'
- en: This is the total expected reward gained by the policy from a state $s$. The
    goal in a POMDP is the same, but with the policy taking observations rather than
    states as input. This sum may not exist if the MDP does not have a fixed horizon,
    so we normally use one of two other forms of the return, either assuming a fixed
    number of steps per episode (a *horizon* $H$) or an exponential discounting of
    future rewards by a discount factor $\gamma$. Note that we formalise the policy
    here as Markovian (i.e. that only takes the previous state as input) for simplicity,
    but the policy can take in the full history $(s_{1},a_{1},r_{1},\ldots s_{t-1},a_{t-1},r_{t-1},s_{t})$
    as input, for example using a recurrent neural network. We define the set of possible
    histories for a state and action space as $H[S,A]=\{(s_{1},a_{1},r_{1},\dots s_{t-1},a_{t-1},r_{t-1},s_{t})|t\in\mathbb{N}\}$,
    similarly for an observation space. A policy being non-Markovian allows it to
    be adaptive (for further discussion see [Section 3.7](#S3.SS7.SSS0.Px2 "Motivating
    Zero-Shot Policy Transfer ‣ 3.7 Remarks And Discussion ‣ 3 Formalising Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从状态 $s$ 获得的策略的总期望回报。在 POMDP 中，目标相同，但策略是以观察而非状态作为输入。如果 MDP 没有固定的时间跨度，这个总和可能不存在，因此我们通常使用另外两种回报形式之一，要么假设每个回合有固定步数（一个
    *时间跨度* $H$），要么通过折扣因子 $\gamma$ 对未来回报进行指数折扣。注意，我们在这里将策略形式化为马尔可夫的（即只考虑前一个状态作为输入）以简化说明，但策略可以接收完整的历史
    $(s_{1},a_{1},r_{1},\ldots s_{t-1},a_{t-1},r_{t-1},s_{t})$ 作为输入，例如使用递归神经网络。我们定义状态和动作空间的所有可能历史集合为
    $H[S,A]=\{(s_{1},a_{1},r_{1},\dots s_{t-1},a_{t-1},r_{t-1},s_{t})|t\in\mathbb{N}\}$，观察空间也是类似的。非马尔可夫策略使其能够适应（进一步讨论请参见
    [第3.7节](#S3.SS7.SSS0.Px2 "动机零样本策略转移 ‣ 3.7 备注与讨论 ‣ 3 强化学习中的零样本概括 ‣ 深度强化学习中零样本概括的调研")）。
- en: 3.3 Contextual Markov Decision Processes
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 上下文马尔可夫决策过程
- en: 'To talk about zero-shot generalisation, we desire a way of reasoning about
    a *collection* of tasks, environment instances or levels: the need for generalisation
    emerges from the fact we train and test the policy on different collections of
    environment instances. Consider as a didactic example OpenAI Procgen (?): in this
    benchmark suite, each game is a collection of procedurally generated levels. Which
    level is generated is completely determined by a level seed, and the standard
    protocol is to train a policy on a fixed set of 200 levels and then evaluate performance
    on the full distribution of levels. Almost all other benchmarks share this structure:
    they have a collection of levels or tasks, which are specified by some seed, ID
    or parameter vector, and generalisation is measured by training and testing on
    different distributions over the collection of levels or tasks. To give a different
    example, in the Distracting Control Suite (?), the parameter vector determines
    a range of possible visual distractions applied to the observation of a continuous
    control task, from changing the colours of objects to controlling the camera angle.
    While this set of parameter vectors has more structure than the set of seeds in
    Procgen, both can be understood within the framework we propose. See [Section 4.2](#S4.SS2
    "4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning") for a discussion of the differences between these styles of environments.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要讨论零-shot泛化，我们希望找到一种方法来推理关于*任务集合*、环境实例或级别的情况：泛化的需求源于我们在不同的环境实例集合上训练和测试策略的事实。以OpenAI
    Procgen（?）为一个教学例子：在这个基准套件中，每个游戏是一个程序生成级别的集合。生成哪个级别完全由级别种子决定，标准协议是在固定的200个级别上训练策略，然后在全部级别的分布上评估性能。几乎所有其他基准都共享这种结构：它们有一个级别或任务的集合，这些集合由某些种子、ID或参数向量指定，泛化通过在不同的级别或任务集合分布上进行训练和测试来衡量。举一个不同的例子，在Distracting
    Control Suite（?）中，参数向量决定了一系列可能的视觉干扰，应用于连续控制任务的观察中，从改变物体的颜色到控制相机角度。虽然这组参数向量比Procgen中的种子集有更多结构，但这两者都可以在我们提出的框架内理解。有关这些环境风格之间差异的讨论，请参见[第4.2节](#S4.SS2
    "4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")。
- en: To formalise the notion of a collection of tasks, we start with the Contextual
    Markov Decision Process (CMDP), as originally formalised by ? (?), but using the
    alternative formalism from ? (?). This formalism also builds on those presented
    by ? (?, ?), but we extend them to consider different distributions over context
    parameters; we include both observed and unobserved contexts settings; we include
    cases where the context is controllable; and we define formally how to produce
    subset CMDPs (see the end of this section for more discussion comparing the formalism
    we present here and existing works).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了形式化任务集合的概念，我们从上下文马尔可夫决策过程（CMDP）开始，这最初由? (?)形式化，但使用了? (?)中的替代形式。这个形式化方法也建立在? (?, ?)提出的基础上，但我们扩展了它们以考虑不同的上下文参数分布；我们包括了已观察和未观察的上下文设置；我们包括了上下文可控的情况；并且我们正式定义了如何生成子集CMDP（有关我们在这里提出的形式化方法与现有工作的比较，请参见本节末尾的更多讨论）。
- en: Definition 1.
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1.
- en: A contextual MDP (CMDP) is a tuple
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文MDP（CMDP）是一个元组
- en: '|  | $\mathcal{M}=\left(S^{\prime},A,O,R,T,C,\phi:S^{\prime}\times C\rightarrow
    O,p(s^{\prime}&#124;c),p(c)\right).$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}=\left(S^{\prime},A,O,R,T,C,\phi:S^{\prime}\times C\rightarrow
    O,p(s^{\prime}&#124;c),p(c)\right).$ |  |'
- en: '$A,O,R,T,\phi$ are as in the definition of the POMDP in [Section 3.2](#S3.SS2
    "3.2 Background: Reinforcement Learning ‣ 3 Formalising Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning"). $C$ is the context space (a set over which it is possible to have
    a distribution). The CMDP is a POMDP with state space $S:=S^{\prime}\times C$,
    initial state distribution $p((s^{\prime},c))=p(c)p(s^{\prime}|c)$, that is the
    POMDP $\left(S^{\prime}\times C,A,O,R,T,\phi,p(s^{\prime}|c)p(c)\right)$. Hence,
    $R$ has type $R:S^{\prime}\times C\rightarrow\mathbb{R}$ and $T((s,c),a)$ is the
    form of the transition probability distribution. For the tuple to be a CMDP, the
    transition function must be factored such that the context doesn’t change within
    an episode, that is $T((s,c),a)((s^{\prime},c^{\prime}))=0\textrm{ if }c^{\prime}\neq
    c$. We call $S^{\prime}$ the underlying state space, and $p(c)$ the context distribution.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: $A,O,R,T,\phi$ 与 [第 3.2 节](#S3.SS2 "3.2 背景：强化学习 ‣ 3 强化学习中零样本泛化的形式化 ‣ 深度强化学习中零样本泛化的调查")
    中 POMDP 的定义相同。$C$ 是上下文空间（一个可以有分布的集合）。CMDP 是一个具有状态空间 $S:=S^{\prime}\times C$ 的
    POMDP，初始状态分布为 $p((s^{\prime},c))=p(c)p(s^{\prime}|c)$，即 POMDP 为 $\left(S^{\prime}\times
    C,A,O,R,T,\phi,p(s^{\prime}|c)p(c)\right)$。因此，$R$ 的类型为 $R:S^{\prime}\times C\rightarrow\mathbb{R}$，$T((s,c),a)$
    是转换概率分布的形式。为了使元组成为 CMDP，转换函数必须被分解，以确保上下文在一个回合内不改变，即 $T((s,c),a)((s^{\prime},c^{\prime}))=0\textrm{
    如果 }c^{\prime}\neq c$。我们称 $S^{\prime}$ 为基础状态空间，$p(c)$ 为上下文分布。
- en: To give an intuition for this definition, the context takes the role of the
    seed, ID or parameter vector which determines the level. Hence why it should not
    change within an episode, only between episodes. The CMDP is the entire collection
    of tasks or environment instances; in Procgen, each game (e.g. starpilot, coinrun,
    etc.) is a separate CMDP. The context distribution $p(c)$ is what is used to determine
    the training and testing collections of levels, tasks or environment instances;
    in Procgen this distribution is uniform over the fixed 200 seeds at training time,
    and uniform over all seeds at testing time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地理解这个定义，上下文扮演了种子、ID 或参数向量的角色，决定了级别。因此，它不应在一个回合内改变，只能在回合之间改变。CMDP 是任务或环境实例的整个集合；在
    Procgen 中，每个游戏（例如 starrpilot、coinrun 等）都是一个独立的 CMDP。上下文分布 $p(c)$ 用于确定训练和测试的级别、任务或环境实例的集合；在
    Procgen 中，这个分布在训练时对固定的 200 个种子均匀分布，在测试时对所有种子均匀分布。
- en: 'Note that this definition leaves it unspecified whether the context is observed
    by the agent: if $O=O^{\prime}\times C$ for some underlying observation space
    $O^{\prime}$ and $\phi((s^{\prime},c))=(\phi^{\prime}(s),c)$ for some underlying
    observation function $\phi^{\prime}:S^{\prime}\rightarrow O^{\prime}$ then we
    say the context is observed, otherwise it isn’t. The context needs to be observed
    for the CMDP to be an MDP (and not a POMDP), but the opposite isn’t true - even
    if the context is observed, $\phi^{\prime}$ could not be the identity, in which
    case the POMDP isn’t likely to be an MDP. Note that we will generally use “MDP”
    to refer to environments that are either MDPs or POMDPs.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一定义没有说明上下文是否被代理观察到：如果 $O=O^{\prime}\times C$ 对于某些基础观察空间 $O^{\prime}$，且
    $\phi((s^{\prime},c))=(\phi^{\prime}(s),c)$ 对于某些基础观察函数 $\phi^{\prime}:S^{\prime}\rightarrow
    O^{\prime}$，则我们说上下文是被观察到的，否则不是。上下文需要被观察到，以便 CMDP 成为 MDP（而非 POMDP），但反之则不成立——即使上下文被观察到，$\phi^{\prime}$
    也可能不是恒等的，在这种情况下，POMDP 可能不被视为 MDP。请注意，我们通常会使用“MDP”来指代那些既是 MDP 也可能是 POMDP 的环境。
- en: As the reward function, transition function, initial state distribution and
    emission function all take the context as input, the choice of context determines
    everything about the resulting MDP apart from the action space, which we assume
    is fixed. Given a context $c^{*}$, we call the MDP resulting in the restriction
    of the CMDP $\mathcal{M}$ to the single context a *context-MDP* $\mathcal{M}_{c^{*}}$.
    Formally, this is a new CMDP with $p(c):=1\textrm{ if }c=c^{*}\textrm{ else }0$.
    This is a specific task or environment instance, for example, a single level of
    a game in Procgen, as specified by a single random seed that is the context.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于奖励函数、转换函数、初始状态分布和发射函数都将上下文作为输入，因此上下文的选择决定了除了行动空间（我们假设是固定的）以外的所有 MDP 结果。给定一个上下文
    $c^{*}$，我们称将 CMDP $\mathcal{M}$ 限制到单一上下文所得到的 MDP 为 *context-MDP* $\mathcal{M}_{c^{*}}$。正式地，这是一个新的
    CMDP，其中 $p(c):=1\textrm{ 如果 }c=c^{*}\textrm{ 否则 }0$。这是一个特定的任务或环境实例，例如，Procgen
    中的单一游戏关卡，由一个单一随机种子指定，这个种子即为上下文。
- en: Some MDPs have stochastic transition or reward functions. When these MDPs are
    simulated, researchers often have control of this stochasticity through the choice
    of a random seed. In theory, these stochastic MDPs could be considered deterministic
    contextual MDPs, where the context is the random seed. We do not consider stochastic
    MDPs as automatically contextual in this way and assume that the random seed is
    always chosen randomly, rather than being modelled as a context. This more closely
    maps to real-world scenarios with stochastic dynamics where we cannot control
    the stochasticity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一些MDPs具有随机转移或奖励函数。当这些MDPs被模拟时，研究人员通常可以通过选择随机种子来控制这种随机性。在理论上，这些随机MDPs可以被视为确定性上下文MDPs，其中上下文是随机种子。我们不认为随机MDPs自动是上下文的，并假设随机种子总是随机选择的，而不是作为上下文进行建模。这更接近于实际世界中具有随机动态的情况，在这种情况下我们无法控制随机性。
- en: 3.4 Training And Testing Contexts
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 训练和测试上下文
- en: We now describe the class of generalisation problems we focus on, using the
    CMDP formalism. As mentioned, the need for generalisation emerges from a difference
    between the training and testing environment instances, and so we want to specify
    both a set of training context-MDPs and a testing set. We specify these sets of
    context-MDPs by their context sets, as the context uniquely determines the MDP.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们描述我们关注的泛化问题的类别，使用CMDP形式。正如所提到的，泛化的需求源于训练和测试环境实例之间的差异，因此我们希望指定一组训练上下文-MDPs和一个测试集。我们通过上下文集来指定这些上下文-MDPs，因为上下文唯一地确定了MDP。
- en: First, we need to describe how to use training and testing context sets to create
    new CMDPs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要描述如何使用训练和测试上下文集来创建新的CMDPs。
- en: Definition 2.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义2。
- en: For any CMDP $\mathcal{M}=\left(S^{\prime},A,O,R,T,C,\phi,p(s^{\prime}|c),p(c)\right)$,
    we can choose a subset of the context set $C^{\prime}\subseteq C$, and then produce
    a new CMDP
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何CMDP $\mathcal{M}=\left(S^{\prime},A,O,R,T,C,\phi,p(s^{\prime}\vert c),p(c)\right)$，我们可以选择上下文集$C^{\prime}\subseteq
    C$的一个子集，然后生成一个新的CMDP
- en: '|  | $\mathcal{M}&#124;_{C^{\prime}}=\left(S^{\prime},A,O,R,T,C^{\prime},\phi,p(s^{\prime}&#124;c),p^{\prime}(c)\right)$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}\vert_{C^{\prime}}=\left(S^{\prime},A,O,R,T,C^{\prime},\phi,p(s^{\prime}\vert
    c),p^{\prime}(c)\right)$ |  |'
- en: where $p^{\prime}(c)=\frac{p(c)}{Z}\textrm{ if }c\in C^{\prime}\textrm{ else
    }0$ and $Z$ is a renormalisation term $Z=\sum_{c\in C^{\prime}}p(c)$ that ensures
    $p^{\prime}(c)$ is a probability distribution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p^{\prime}(c)=\frac{p(c)}{Z}\textrm{ 如果 }c\in C^{\prime}\textrm{ 否则 }0$，$Z$是归一化项$Z=\sum_{c\in
    C^{\prime}}p(c)$，以确保$p^{\prime}(c)$是一个概率分布。
- en: This allows us to split the total collection of context-MDPs into smaller subsets,
    as determined by the contexts. For example, in Procgen any possible subset of
    the set of all seeds can be used to define a different version of the game with
    a limited set of levels.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们将上下文-MDP的总集合拆分成较小的子集，由上下文决定。例如，在Procgen中，所有种子集合的任何可能子集都可以用来定义一个不同版本的游戏，具有有限的关卡。
- en: 'For the objective, we use the expected return of a policy:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个目标，我们使用策略的期望回报：
- en: Definition 3.
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义3。
- en: For any CMDP $\mathcal{M}$ we can define the expected return of a policy in
    that CMDP as
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何CMDP $\mathcal{M}$，我们可以定义在该CMDP中策略的期望回报为
- en: '|  | $\textbf{R}(\pi,\mathcal{M}):=\mathbb{E}_{c\sim p(c)}[\mathcal{R}(\pi,\mathcal{M}_{c})],$
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{R}(\pi,\mathcal{M}):=\mathbb{E}_{c\sim p(c)}[\mathcal{R}(\pi,\mathcal{M}_{c})],$
    |  |'
- en: where $\mathcal{R}$ is the expected return of a policy in a (context) MDP and
    $p(c)$ is the context distribution as before.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{R}$是策略在（上下文）MDP中的期望回报，$p(c)$是上下文分布，如前所述。
- en: We can now formally define the Zero-Shot Policy Transfer (ZSPT) problem class.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以正式定义Zero-Shot策略迁移（ZSPT）问题类别。
- en: Definition 4  (Zero Shot Policy Transfer).
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义4（零-shot策略迁移）。
- en: 'A ZSPT problem is defined by a choice of CMDP $\mathcal{M}$ with context set
    $C$ and a choice of training and testing context sets $C_{\textrm{train}},C_{\textrm{train}}\subseteq
    C$. The objective is to produce a non-Markovian policy $\pi:H[O,A]\rightarrow
    A$ which maximises the expected return in the testing CMDP $\mathcal{M}|_{C\textrm{test}}$:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ZSPT问题由选择一个带有上下文集$C$的CMDP $\mathcal{M}$和一个训练与测试上下文集$C_{\textrm{train}},C_{\textrm{test}}\subseteq
    C$定义。目标是生成一个非Markovian策略$\pi:H[O,A]\rightarrow A$，以最大化在测试CMDP $\mathcal{M}|_{C_{\textrm{test}}}$中的期望回报：
- en: '|  | $J(\pi):=\textbf{R}(\pi,\mathcal{M}&#124;_{C\textrm{test}}).$ |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\pi):=\textbf{R}(\pi,\mathcal{M}\vert_{C_{\textrm{test}}}).$ |  |'
- en: This policy can be produced through interaction with the training CMDP $\mathcal{M}|_{C_{\textrm{train}}}$
    for a fixed number of environment and episode samples $N_{s},N_{e}$ respectively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略可以通过与训练 CMDP $\mathcal{M}|_{C_{\textrm{train}}}$ 的交互来生成，固定数量的环境和剧集样本分别为 $N_{s},N_{e}$。
- en: ZSG research is generally concerned with developing algorithms that can solve
    a variety of ZSPT problems. For example, in Procgen we aim to produce an algorithm
    that can solve the ZSPT problem for every game. Specifically, we want to achieve
    the highest return possible on the testing distribution (which is the full distribution
    over levels) after training for 25 million steps ($N_{s}=25\times 10^{6},N_{e}=\infty$)
    on the training distribution of levels (which is a fixed set of 200 levels). The
    name *Zero-Shot Policy Transfer* comes from prior works (?, ?).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ZSG 研究通常关注开发能够解决各种 ZSPT 问题的算法。例如，在 Procgen 中，我们的目标是生产一个可以解决每个游戏 ZSPT 问题的算法。具体而言，我们希望在训练
    2500 万步（$N_{s}=25\times 10^{6},N_{e}=\infty$）之后，在测试分布（即全级别分布）上实现尽可能高的回报。*零-shot
    策略转移* 这一名称源于之前的工作（？，？）。
- en: 'Some algorithms assume that the context distribution can be adjusted during
    interaction with the training CMDP, as long as sampled contexts are only within
    the fixed training context set:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法假设在与训练 CMDP 交互期间，背景分布可以被调整，只要抽样背景仅限于固定的训练背景集合：
- en: Definition 5  (ZSPT controllable context).
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 5 （ZSPT 可控背景）。
- en: 'A *controllable context* ZSPT problem is the same as a ZSPT problem above,
    except the learning algorithm can adjust the context distribution of the training
    CMDP $p_{\textrm{train}}(c)$ during training, so long as it maintains the property
    of only sampling from the training context set: $p_{\textrm{train}}(c)=0\textrm{
    if }c\not\in C_{\textrm{train}}$'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*可控背景* ZSPT 问题与上述 ZSPT 问题相同，不同之处在于学习算法可以在训练期间调整训练 CMDP $p_{\textrm{train}}(c)$
    的背景分布，只要它保持仅从训练背景集合中抽样的特性：$p_{\textrm{train}}(c)=0\textrm{ 如果 }c\not\in C_{\textrm{train}}$'
- en: '*Note that this formalism defines a *class* of problems, each determined by
    a choice of CMDP, training and testing context sets and whether the context is
    controllable*. This means that we do not make any assumptions about shared structure
    within the CMDP between context-MDPs: for any specific problem some assumption
    of this kind (either implicit or explicit) will likely be required for learning
    to occur ([Section 3.6](#S3.SS6 "3.6 Additional Assumptions For More Feasible
    Generalisation ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")), but
    we do not believe there is a unifying assumption behind all ZSG problems apart
    from those stated here.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，这种形式化定义了一类问题，每个问题由 CMDP、训练和测试背景集合的选择以及背景是否可控决定*。这意味着我们没有对背景-MDP 之间 CMDP
    的共享结构做任何假设：对于任何特定的问题，某种假设（无论是隐含的还是明确的）可能是学习发生的前提（[第 3.6 节](#S3.SS6 "3.6 Additional
    Assumptions For More Feasible Generalisation ‣ 3 Formalising Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")），但我们不认为所有 ZSG 问题有一个统一的假设，除了这里所述的假设。'
- en: Evaluating Zero-Shot Generalisation.
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估零-shot 泛化。
- en: 'As in supervised learning, we can consider the gap between training and testing
    performance as a measure of generalisation. We define this analogously to how
    it’s defined in supervised learning ([Eq. 1](#S3.E1 "In 3.1 Background: Generalisation
    In Supervised Learning ‣ 3 Formalising Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")),
    swapping the ordering between training and testing (as we maximise reward, rather
    than minimise loss):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于监督学习，我们可以将训练和测试性能之间的差距视为泛化的衡量标准。我们定义的方式类似于监督学习中的定义（[公式 1](#S3.E1 "In 3.1
    Background: Generalisation In Supervised Learning ‣ 3 Formalising Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")），交换训练和测试之间的顺序（因为我们最大化奖励，而不是最小化损失）：'
- en: '|  | $\textrm{GenGap}(\pi):=\textbf{R}(\pi,\mathcal{M}&#124;_{C_{\textrm{train}}})-\textbf{R}(\pi,\mathcal{M}&#124;_{C_{\textrm{test}}}).$
    |  | (2) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textrm{GenGap}(\pi):=\textbf{R}(\pi,\mathcal{M}|_{C_{\textrm{train}}})-\textbf{R}(\pi,\mathcal{M}|_{C_{\textrm{test}}}).$
    |  | (2) |'
- en: This metric is frequently used in the literature in addition to test performance
    to evaluate ZSG algorithms (?, ?, ?). In general, it’s not clear what the best
    evaluation metric for ZSG algorithms is. In supervised learning, the generalisation
    capabilities of different algorithms are usually evaluated via final performance
    on an evaluation task. When the tasks used to evaluate a model are close to (or
    the same as) the tasks that the model will eventually be deployed on, it is clear
    that final performance is a good metric to evaluate on. However, in RL the benchmark
    tasks we use are often very dissimilar to the eventual real-world tasks we want
    to apply these algorithms to. Further, RL algorithms are currently still quite
    brittle and performance can vary greatly depending on hyperparameter tuning and
    the specific task being used (?). In this setting, we may care more about the
    zero-shot generalisation *potential* of algorithms by decoupling generalisation
    from training performance and evaluating using the generalisation gap instead.
    For example, if algorithm A has higher testing performance than algorithm B, but
    A also has a much larger generalisation gap, we may prefer to use algorithm B
    in a new setting, so we have better assurance that the deployment performance
    won’t deviate as much from the training performance, and the algorithm may be
    more robust. This is the reason the previous literature has often reported this
    metric alongside test performance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 除了测试性能外，这一指标在文献中常用于评估 ZSG 算法 (?, ?, ?)。总体而言，尚不清楚 ZSG 算法的最佳评估指标是什么。在监督学习中，不同算法的泛化能力通常通过在评估任务上的最终表现来评估。当用于评估模型的任务接近（或与）模型最终部署的任务相同时，显然最终性能是一个良好的评估指标。然而，在强化学习中，我们使用的基准任务往往与我们希望应用这些算法的实际任务差异很大。此外，强化学习算法目前仍相当脆弱，性能可能因超参数调整和特定任务的不同而大相径庭（?）。在这种情况下，我们可能更关注算法的零样本泛化*潜力*，通过将泛化与训练性能解耦，并改用泛化差距进行评估。例如，如果算法
    A 的测试性能高于算法 B，但 A 的泛化差距也大得多，我们可能更愿意在新环境中使用算法 B，这样我们可以更好地确保部署性能不会偏离训练性能太多，且算法可能更具鲁棒性。这就是为何之前的文献经常将该指标与测试性能一起报告的原因。
- en: 'However, the generalisation gap in RL has the same problems as discussed for
    the supervised learning generalisation gap: a gap of zero doesn’t necessarily
    imply good performance (i.e. a random policy is likely to get a gap of 0), and
    if the reward functions aren’t comparable across training and testing, then the
    magnitude of the gap my not be informative (and it could then only be used to
    compare between different algorithms). This means using it as the only metric
    for improved performance will likely not lead to robust progress in ZSG. Further,
    given how broad the current set of assumptions is, it is unlikely there is a single
    general measure of progress towards tackling ZSG: across such a broad problem
    class, objectives may even be conflicting (?).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，强化学习中的泛化差距存在与监督学习泛化差距相同的问题：零差距并不一定意味着良好的性能（即，随机策略可能得到 0 差距），如果奖励函数在训练和测试之间不可比，那么差距的大小可能没有信息量（这样它可能只能用于比较不同算法之间的差异）。这意味着将其作为提高性能的唯一指标可能不会在
    ZSG 上取得稳健的进展。此外，鉴于当前假设集的广泛性，单一的泛化进展度量不太可能存在：在如此广泛的问题类别中，目标甚至可能存在冲突（?）。
- en: Therefore, our recommendation is first and foremost to focus on problem-specific
    benchmarks and revert to the SL standard of using overall performance in specific
    settings (e.g. visual distractors, stochastic dynamics, sparse reward, hard exploration).
    The generalisation performance of various RL algorithms is likely contingent on
    the type of environment they are deployed on and therefore careful categorisation
    of the type of challenges present at deployment is needed to properly evaluate
    ZSG capability (for further discussion see [Section 4.3](#S4.SS3.SSS0.Px4 "The
    Downsides of Procedural Content Generation for Zero-shot Generalisation. ‣ 4.3
    Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning") and [Section 6.2](#S6.SS2
    "6.2 Real World Reinforcement Learning Generalisation ‣ 6 Discussion And Future
    Work ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")).
    As in the literature, generalisation gap can be used as an additional auxiliary
    metric to evaluate the performance of ZSG algorithms as well as test performance,
    to either break ties between algorithms with very similar test time performance,
    or to inform users in situations where it is more important to have strong assurances
    on test time performance than to have expected test time performance as high as
    possible.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的建议首先是专注于特定问题的基准测试，并回到使用特定设置下的整体性能的 SL 标准（例如视觉干扰物、随机动态、稀疏奖励、困难探索）。各种 RL
    算法的泛化性能可能取决于它们部署的环境类型，因此需要对部署时存在的挑战类型进行仔细分类，以正确评估 ZSG 能力（进一步讨论见 [第 4.3 节](#S4.SS3.SSS0.Px4
    "生成内容对于零-shot 泛化的缺点。 ‣ 4.3 讨论 ‣ 4 强化学习中的零-shot 泛化基准 ‣ 深度强化学习中的零-shot 泛化调查") 和
    [第 6.2 节](#S6.SS2 "6.2 现实世界强化学习泛化 ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零-shot 泛化调查")）。正如文献中所示，泛化差距可以作为评估
    ZSG 算法性能的附加辅助指标，也可以用于测试性能，以在算法的测试时性能非常相似的情况下打破僵局，或在需要对测试时性能有强保证的情况下告知用户，而不是期望测试时性能尽可能高。
- en: 3.5 Real World Examples of This Formalism
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 这种形式的现实世界示例
- en: 'We chose this formalism as it is simple to understand, captures all the problems
    we are interested in, and is based on prior work. To further justify why this
    formalism is useful, and to give intuition about how it can be used in a variety
    of settings, we give several examples of real-world scenarios where this formalism
    naturally applies:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这种形式是因为它易于理解，涵盖了我们感兴趣的所有问题，并且基于已有的工作。为了进一步证明这种形式的实用性，并且给出它如何在多种设置中使用的直观理解，我们提供了几个现实世界场景的示例，其中这种形式自然适用：
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sim-to-real is a classic problem of ZSG and one which can be captured in this
    framework. Here the outer CMDP is a union between the simulation and real-world
    MDPs. The context set will be split into those contexts which correspond to simulation,
    and those that correspond to reality. The context generally conditions the dynamics,
    observation function and state distribution, but likely not the reward (so $\forall
    s^{\prime},c:R((s^{\prime},c))=R^{\prime}(s^{\prime})$). Domain randomisation
    approaches are motivated by the idea that producing a wide range of possible contexts
    in simulation (the training CMDP) will make it more likely that the testing distribution
    of contexts is closer to the expanded training distribution. In a simulation setting,
    we’d normally assume access to the context distribution, and that the context
    could be made observable, but that the context won’t be observable at testing
    time, and so it may not be useful to have a policy that explicitly conditions
    on the context.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sim-to-real 是 ZSG 的经典问题之一，可以在这个框架中进行捕捉。这里的外部 CMDP 是模拟和现实世界 MDP 的联合。上下文集将被分为对应于模拟的上下文和对应于现实的上下文。上下文通常会影响动态、观察函数和状态分布，但可能不会影响奖励（所以
    $\forall s^{\prime},c:R((s^{\prime},c))=R^{\prime}(s^{\prime})$）。领域随机化方法的动机在于，通过在模拟中生成广泛的可能上下文（即训练
    CMDP），可以使得测试上下文分布更接近扩展的训练分布。在模拟环境中，我们通常假设能够访问上下文分布，并且上下文可以被观察，但在测试时上下文可能不可观察，因此可能没有必要使用一个明确依赖于上下文的策略。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Healthcare is a promising domain for deploying future RL methods, as there
    are many sequential decision-making problems. For example, the task of diagnosing
    and treating individual patients can be understood as a CMDP where the patient
    effectively specifies the context: patients will react differently to tests and
    treatments (dynamics variation) and may provide different measurements (state
    variation). Generalising to treating new patients is then exactly generalising
    to novel contexts. In this setting, we may be able to assume some part of the
    context (or some information about the context) is observable, as we will have
    access to the patient’s medical history and personal information.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗保健是部署未来 RL 方法的一个有前景的领域，因为这里有许多顺序决策问题。例如，对个体患者进行诊断和治疗的任务可以理解为一个 CMDP，其中患者有效地指定了情境：患者对测试和治疗的反应（动态变化）可能不同，并且可能提供不同的测量值（状态变化）。将治疗新患者的过程泛化，实际上就是将其泛化到新的情境。在这种情况下，我们可以假设情境的某些部分（或关于情境的一些信息）是可观察的，因为我们将可以访问患者的病史和个人信息。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Autonomous vehicles are another area where RL methods could be applied. These
    vehicles will be goal-conditioned in some sense, such that they can perform different
    journeys, which means that the context will likely control the reward function
    and that the part of the context that controls the read function will be observable
    (so the policy knows what task to perform). Driving in different locations (different
    contexts changing the initial state distribution), under different weather and
    lighting conditions due to the time of day (observation functions) and on different
    road surfaces (transition functions) are all problems that need to be tackled
    by these systems. We can understand this in the CMDP framework, where the context
    contains information about the weather, time of day, location and goal, as well
    as information about the state of the current vehicle. Some of this context will
    be observed directly, and some may be inferred from observation. In this setting,
    we may only be able to train in certain contexts (i.e. certain cities, or restricted
    weather conditions), but we require the policy to generalise zero-shot to the
    unseen contexts well.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动驾驶车辆是另一个可以应用 RL 方法的领域。这些车辆将在某种意义上受到目标条件的约束，这样它们可以执行不同的旅程，这意味着情境很可能会控制奖励函数，并且控制奖励函数的情境部分将是可观察的（因此策略知道执行什么任务）。在不同的地点驾驶（不同的情境改变初始状态分布）、在不同的天气和光照条件下（观察函数）以及在不同的道路表面（转移函数）上行驶，都是这些系统需要解决的问题。我们可以在
    CMDP 框架中理解这一点，其中情境包含关于天气、时间、地点和目标的信息，以及当前车辆状态的信息。其中一些情境会被直接观察到，而有些可能从观察中推断出来。在这种情况下，我们可能只能在某些情境（即某些城市或受限的天气条件）下进行训练，但我们需要策略能够在未见过的情境中零样本泛化。
- en: 3.6 Additional Assumptions For More Feasible Generalisation
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 更多可行泛化的附加假设
- en: In choosing the CMDP formalism we opted to formalise ZSG in a way that captures
    the full class of problems we are concerned with, but this means that it is almost
    certainly impossible to prove any formal theoretical guarantees on learning performance
    using solely the CMDP structural assumptions. While we do not prove this, it is
    easy to see how one could design pathological CMDPs where generalisation to new
    contexts is entirely impossible without strong domain knowledge of the new contexts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 CMDP 形式化方法时，我们选择将 ZSG 形式化为一种能够涵盖我们关注的全部问题类别的方式，但这也意味着几乎肯定无法仅凭 CMDP 结构假设证明任何形式的理论保证。虽然我们没有证明这一点，但很容易看出，设计一些病态
    CMDP 的方法可能使得在没有对新情境的强领域知识的情况下，无法对新情境进行泛化。
- en: To have any chance of solving a specific ZSG problem then, further assumptions
    (either explicit or implicit) have to be made. These could be assumptions on the
    type of variation, the distributions from which the training and testing context
    sets are drawn, or additional underlying structure in the context set. We describe
    several popular or promising assumptions here and note that the taxonomy in [Section 4](#S4
    "4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning") also acts as a set
    of possible additional assumptions to make when tackling a ZSG problem.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要有解决特定ZSG问题的任何机会，就必须做出进一步的假设（无论是明确的还是隐含的）。这些假设可能涉及变化的类型、训练和测试上下文集所抽取的分布，或上下文集中的额外基础结构。我们在此描述了一些流行或有前景的假设，并注意到[第4节](#S4
    "4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")中的分类法也作为解决ZSG问题时可能需要做出的附加假设的一组参考。
- en: Assumptions on the Training and Testing Context Set Distributions.
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于训练和测试上下文集分布的假设。
- en: One assumption which is often made is that while the training and testing context
    sets are not identical, the elements of the two sets have been drawn from the
    same underlying distribution, analogously to the IID data assumption in supervised
    learning. For example, this is the setup of OpenAI Procgen (?), where the training
    context set is a set of 200 seeds sampled uniformly at random from the full distribution
    of seeds, and the full distribution is used as the testing context set.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的假设是，尽管训练和测试上下文集并不完全相同，但这两个集的元素是从相同的基础分布中抽取的，类似于监督学习中的IID数据假设。例如，这就是OpenAI
    Procgen (?)的设置，其中训练上下文集是从完整种子分布中均匀随机抽取的200个种子集合，而完整分布被用作测试上下文集。
- en: However, many works on ZSG in RL do not assume that the train and test environment
    instances are drawn from the same distribution. This is often called *Domain Generalisation*,
    where we refer to the training and testing environments instances as different
    *domains* that may be similar but are not from the same underlying generative
    distribution. Concrete examples occur in robotics such as the *sim-to-real* problem.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多关于RL中ZSG的研究并不假设训练和测试环境实例来自相同的分布。这通常被称为*领域泛化*，其中我们将训练和测试环境实例称为不同的*领域*，这些领域可能类似但不是来自相同的基础生成分布。在机器人技术中，*从模拟到现实*问题就是一个具体的例子。
- en: Note that while the testing context set could be a single context, this would
    likely lead to a not particularly robust algorithm - it’s possible that it overfits
    to the problem of producing a policy that performs well on this specific context,
    which may not generalise to other similar contexts (which is the high-level goal
    of this research direction).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管测试上下文集可能是一个单一的上下文，但这可能导致一个不特别稳健的算法——它可能会过度拟合于在这个特定上下文中表现良好的策略问题，这可能无法推广到其他类似的上下文（这是该研究方向的高层目标）。
- en: Further Formal Assumptions of Structure.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 进一步的结构形式假设。
- en: Another kind of assumption that can be made is on the structure of the CMDP
    itself, e.g. the context space or transition function. There are several families
    of MDPs with additional structure which could enable ZSG. However, these assumptions
    are often not explicitly made when designing benchmarks and methods, which can
    make understanding why and how generalisation occurs difficult. A detailed discussion
    and formal definitions for these structures can be found in [Section A](#S1a "A
    Other Structural Assumptions on MDPs ‣ A Survey of Zero-shot Generalisation in
    Deep Reinforcement Learning"), but we provide a high-level overview here, focusing
    on assumptions that have been used in practice, and those that hold particular
    promise for ZSG.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可以做出的假设是关于CMDP本身的结构，例如上下文空间或转移函数。有几类具有附加结构的MDP可能使ZSG成为可能。然而，这些假设在设计基准和方法时通常没有被明确提出，这可能使理解泛化发生的原因和方式变得困难。关于这些结构的详细讨论和形式定义可以在[第A节](#S1a
    "A Other Structural Assumptions on MDPs ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")中找到，但我们在此提供一个高层次的概述，重点介绍那些在实践中已被使用的假设，以及那些对ZSG特别有前景的假设。
- en: 'An example of a structured MDP that has been used to improve generalisation
    is the *block MDP* (?). It assumes a *block structure* in the mapping from a latent
    state space to the given observation space, or that there exists another MDP described
    by a smaller state space with the same behaviour as the given MDP. This assumption
    is relevant in settings where we only have access to high-dimensional, unstructured
    inputs, but know that there exists a lower-dimensional state space that gives
    rise to an equivalent MDP. ? (?) use this assumption for improved bounds on exploration
    that relies on the size of the latent state space rather than the given observation
    space. ? (?) develop a representation learning method that disentangles relevant
    from irrelevant features, improving generalisation to environments instances where
    only the irrelevant features change, a simple form of *systematicity* ([Section 3.1](#S3.SS1
    "3.1 Background: Generalisation In Supervised Learning ‣ 3 Formalising Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"), ?). This is a rare example of a method explicitly
    utilising additional assumptions of structure to improve generalisation. Block
    MDPs can be combined with contextual MDPs by introducing an emission mapping from
    state space to observation space that is also dependent on context, as defined
    by ? (?).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个已被用来改善泛化的结构化 MDP 示例是 *块 MDP* (?). 它假设在从潜在状态空间到给定观测空间的映射中存在 *块结构*，或者存在一个描述为较小状态空间的另一个
    MDP，其行为与给定 MDP 相同。这一假设在我们只能访问高维、非结构化输入的情况下尤其相关，但知道存在一个低维状态空间能够生成等效的 MDP。? (?)
    使用这一假设来改进探索的界限，依赖于潜在状态空间的大小而非给定的观测空间。? (?) 开发了一种表示学习方法，该方法将相关特征与不相关特征区分开来，从而改进了对环境实例的泛化，其中只有不相关特征发生变化，这是
    *系统性* 的一种简单形式 ([第 3.1 节](#S3.SS1 "3.1 背景：监督学习中的泛化 ‣ 强化学习中零样本泛化的形式化 ‣ 深度强化学习中的零样本泛化调查")，?）。这是一个明确利用附加结构假设来改进泛化的稀有示例。块
    MDP 可以通过引入一个从状态空间到观测空间的发射映射来与上下文 MDP 结合，该映射还依赖于上下文，如 ? (?) 所定义。
- en: Factored MDPs (?, ?) can be used to describe object-oriented environments or
    multi-agent settings where the state space can be broken up into independent factors,
    i.e. with sparse relationships over the one-step dynamics. This can be leveraged
    to learn dynamics models that explicitly ignore irrelevant factors in prediction
    or to compute improved sample complexity bounds for policy learning (?) and seems
    particularly relevant for generalisation as additional structure in the context
    set could map onto the factored structure in the transition and reward functions.
    An initial example of using a similar formalism to a factored MDP in a multi-domain
    RL setting is demonstrated by ? (?), although it does not target the zero-shot
    policy transfer setting directly. We note that contextual MDPs can be trivially
    represented by factored MDPs with two factors, the state and context. However,
    factored MDPs are capable of modelling more structure in a domain if present.
    Therefore, if an environment is capable of being modelled by a factored MDP in
    addition to a contextual MDP, better generalisation guarantees and results are
    likely possible if this structure is exploited. We hope to see more work applying
    these kinds of structural assumptions to the zero-shot generalisation problems
    discussed in this work.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 分解 MDP (?, ?) 可用于描述面向对象的环境或多智能体设置，其中状态空间可以被分解为独立的因子，即在一步动态中的稀疏关系。这可以被利用来学习显式忽略预测中不相关因子的动态模型，或计算改进的样本复杂度界限以进行策略学习（?），并且似乎特别相关于泛化，因为上下文集中的附加结构可能映射到转移和奖励函数中的分解结构。在多领域
    RL 设置中使用与分解 MDP 类似的形式的初步示例由 ? (?) 展示，尽管它并未直接针对零样本策略迁移设置。我们注意到，上下文 MDP 可以通过两个因子——状态和上下文——的分解
    MDP 进行简单表示。然而，如果领域中存在更多结构，分解 MDP 能够建模更多结构。因此，如果环境能够通过分解 MDP 进行建模，除了上下文 MDP 之外，如果利用这种结构，可能会获得更好的泛化保证和结果。我们希望看到更多将这些类型的结构假设应用于本研究讨论的零样本泛化问题的工作。
- en: 3.7 Remarks And Discussion
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 备注与讨论
- en: Angles to Tackle the ZSPT Problem.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决 ZSPT 问题的角度。
- en: While we aim to improve test-time performance [Definition 3](#Thmdefinition3
    "Definition 3\. ‣ 3.4 Training And Testing Contexts ‣ 3 Formalising Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"), we often do not have access to that performance
    metric directly (or will not when applying our methods in the real world). In
    research, to develop algorithms that improve the test-time performance, we could
    aim to produce algorithms that (when compared to existing work) either (1) increase
    the train-time performance while keeping the generalisation gap constant; (2)
    decrease the generalisation gap while keeping the train-time reward constant;
    or (3) do a mixture of the two approaches. Work in RL not concerned with generalisation
    tends to (implicitly) take the first approach, assuming that the generalisation
    gap will not change.¹¹1If the training environment instances are identical to
    the testing environment instances, then the generalisation gap will always be
    0. Work on ZSG in RL instead normally aims at (2) reducing the generalisation
    gap explicitly, which may reduce train-time performance but increase test-time
    performance. Some work also aims at (1) improving train-time performance in a
    way that is likely to keep the generalisation gap constant.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们旨在提高测试时间性能 [定义3](#Thmdefinition3 "Definition 3\. ‣ 3.4 Training And Testing
    Contexts ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning ‣
    A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")，但我们通常无法直接获取该性能指标（或在将方法应用于现实世界时无法获取）。在研究中，为了开发改善测试时间性能的算法，我们可以目标是产生（与现有工作相比）(1)
    提高训练时间性能，同时保持泛化差距不变；(2) 减少泛化差距，同时保持训练时间奖励不变；或者(3) 两者的混合。强化学习中不涉及泛化的工作往往（隐含地）采取第一种方法，假设泛化差距不会改变。¹¹1如果训练环境实例与测试环境实例相同，则泛化差距将始终为0。强化学习中的ZSG工作通常则旨在（2）明确减少泛化差距，这可能会降低训练时间性能，但提高测试时间性能。一些工作也旨在（1）改善训练时间性能，可能会保持泛化差距不变。
- en: Motivating Zero-Shot Policy Transfer
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 激励零样本策略转移
- en: 'In this work, we focus on zero-shot policy transfer (?, ?): a policy is learned
    from the training CMDP and evaluated zero-shot in the testing CMDP. This field
    is important for several reasons.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们专注于零样本策略转移（?, ?）：从训练CMDP中学习一个策略，并在测试CMDP中进行零样本评估。这个领域有几个重要的原因。
- en: 'First, as mentioned above, current deep RL algorithms often aren’t safe or
    sample-efficient enough to perform online learning in the real world. Hence, a
    significant amount of offline or in-simulation training is required, and policies
    need to generalise zero-shot to the real-world deployment setting. As access to
    more compute and richer simulations becomes available, we expect many successful
    real-world deployments of RL to follow this workflow at least in part: training
    offline or in simulation and then transferring the policy zero-shot to the deployment
    environment. Even if the policy will continue learning during deployment, it still
    needs to be reasonably good at deployment time (i.e. zero-shot), otherwise it
    wouldn’t be safe to deploy it. In this way, we view work on ZSG as mostly complementary
    to work on continual RL, as we think both are important for enabling the deployment
    of robust and competent RL policies. Note that there may be tradeoffs between
    good zero-shot performance and good continual learning performance, and how to
    choose between these two desiderata will be determined by the specific problem
    setting being faced.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如上所述，当前的深度强化学习算法往往不够安全或样本效率不足，无法在现实世界中进行在线学习。因此，需要大量的离线或仿真训练，并且策略需要在现实世界部署环境中进行零样本泛化。随着计算能力的提升和更丰富的仿真环境的出现，我们预计许多成功的现实世界强化学习部署将至少部分遵循这种工作流程：离线或在仿真中训练，然后将策略零样本转移到部署环境中。即使策略在部署过程中会继续学习，它在部署时仍需要在合理的水平上（即零样本），否则部署将不安全。通过这种方式，我们将ZSG的工作视为与持续强化学习的工作主要是互补的，因为我们认为这两者对实现强健且有能力的强化学习策略的部署都很重要。注意，良好的零样本性能和良好的持续学习性能之间可能存在权衡，如何选择这两者将由具体问题设置决定。
- en: Second, from a safety, interpretability and verification perspective, ZSG may
    be preferable to a continually updated policy. It’s likely in high-stakes scenarios
    that models will be verified (?) or audited with interpretability or explainability
    methods (?), and that this process will be expensive. In this scenario, it will
    be beneficial to have a single model which performs well zero-shot without having
    to be continually updated, as after each update these verification and auditing
    steps will likely have to be repeated.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，从安全性、可解释性和验证的角度来看，ZSG 可能比持续更新的策略更为可取。在高风险场景中，模型可能会通过可解释性或解释性方法进行验证或审计，并且这个过程可能会非常昂贵。在这种情况下，拥有一个在零-shot
    下表现良好的单一模型将是有利的，因为每次更新后，这些验证和审计步骤可能需要重复进行。
- en: 'Finally, note that while we do not cover methods that relax the zero-shot assumption,
    we believe that in a real-world scenario it will likely be possible to do so.²²2For
    example by using unsupervised data or samples in the testing environment instances,
    utilising some description of the contexts such that zero-shot generalisation
    is possible, or enabling the agent to train in an online way in the testing context-MDPs.
    However, zero-shot policy transfer is still a useful problem to tackle, as solutions
    are likely to help with a wide range of settings resulting from different relaxations
    of the assumptions made here: zero-shot policy transfer algorithms can be used
    as a base which is then built upon with domain-specific knowledge and extra data.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，虽然我们没有涵盖放宽零-shot 假设的方法，但我们相信在实际场景中，可能会实现这一点。²² 例如，通过使用无监督数据或测试环境实例中的样本，利用某些描述上下文的方法使零-shot
    泛化成为可能，或者使代理能够在测试上下文 MDPs 中以在线方式进行训练。然而，零-shot 策略转移仍然是一个有用的问题，因为解决方案可能有助于处理由这里所作假设的不同放宽引起的各种设置：零-shot
    策略转移算法可以作为基础，然后基于领域特定知识和额外数据进行构建。
- en: 'Note that while “training” and “learning” are contentious terms, our definition
    grounds them in the production of a non-Markovian policy after some number of
    samples from the training context MDP. The fact that the objective is the expected
    testing return within a *single episode* of the non-Markovian policy means that
    online learning or adaptation across more than 1 episode isn’t possible, and so
    the adaptation would have to happen within a single episode to be useful (i.e. using
    a recurrent policy). Several methods do take this approach, as described in [Section 5.2.4](#S5.SS2.SSS4
    "5.2.4 Adapting Online ‣ 5.2 Handling Differences Between Training And Testing
    ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning"). To be clear, we
    are grounding ourselves in this specific *objective*, and not placing any restrictions
    on the *properties* of methods as long as they satisfy the constraints: policies
    need not be Markovian, and can adapt within a single episode to the environment
    instances they’re placed in if that improves performance.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管“训练”和“学习”这两个术语存在争议，我们的定义将其基础设在从训练上下文 MDP 中采样若干次后产生一个非马尔可夫策略上。由于目标是非马尔可夫策略在*单次试验*中的预期测试回报，这意味着无法进行在线学习或在超过一个试验中的适应，因此适应必须在单次试验内完成才有用（即使用递归策略）。如[第5.2.4节](#S5.SS2.SSS4
    "5.2.4 Adapting Online ‣ 5.2 Handling Differences Between Training And Testing
    ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")所述，确实有几种方法采用了这种方法。需要明确的是，我们以这一特定*目标*为基础，而不是对方法的*属性*施加任何限制，只要它们满足约束条件：策略不必是马尔可夫的，并且如果能够提升性能，可以在单次试验中适应其所处的环境实例。
- en: Relationship to Previous Notions of Generalisation in RL.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与 RL 中先前泛化概念的关系。
- en: Historically, generalisation in RL has referred to the notion of generalising
    to novel states or state-action pairs within a single MDP. While this notion is
    useful in the online learning single-MDP setting that is the focus of that work,
    we here focus on a more recent and more realistic setting where the policy is
    trained on a collection of MDPs and then deployed on possibly unseen MDPs. This
    setting more closely mirrors the supervised learning notion of generalisation.
    We believe this type of workflow for deploying RL-trained policies in the real
    world is much more feasible than training a policy online from scratch, as current
    RL algorithms aren’t sample-efficient or safe enough to train online in the real
    world. This means a large amount of off-line or in-simulation training will have
    to occur before the policy is deployed, and the deployed policy will still need
    to perform reasonably well as soon as it’s deployed (i.e. zero-shot).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，强化学习中的泛化指的是在单个MDP中对新状态或状态-动作对进行泛化。虽然这一概念在那项工作关注的在线学习单个MDP设置中很有用，但我们这里关注的是一种更近期、更现实的设置，即策略在一组MDPs上进行训练，然后在可能未见过的MDPs上部署。这种设置更接近于监督学习中的泛化概念。我们认为，这种在现实世界中部署强化学习训练策略的工作流程比从头开始在线训练策略更可行，因为当前的强化学习算法在现实世界中的样本效率和安全性都不足。因此，在部署策略之前必须进行大量的离线或模拟训练，且部署后的策略在部署时仍需表现良好（即*零-shot*）。
- en: Relationship to Previous Formalisms of Collections or Distributions of MDPs.
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与MDPs集合或分布的先前形式主义的关系。
- en: As mentioned previously, the formalism we presented above builds on multiple
    previous works. Here we briefly present the differences between our formalism
    and these works.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们所呈现的形式主义建立在多个先前的工作之上。这里我们简要介绍了我们的形式主义与这些工作之间的差异。
- en: ? (?) present Hidden-parameter MDPs (Hi-MDPs). These are MDPs with a hidden
    parameter which controls the transition function and a distribution over the set
    of these hidden parameters that implicitly defines a set of MDPs. ? (?) builds
    on Hi-MDPs to present Generalised Hidden-parameter MDPs (GHP-MDPs), where the
    hidden parameters now also control the reward function in addition to the dynamics.
    Our work uses a context parameter which is analogous to the hidden parameter,
    which may be hidden or observed, and that controls the initial state distribution,
    transition function and reward function, rather than just the transition and reward
    functions. We also more formally discuss how different distributions of context
    parameters may be used during training and testing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 提出了隐藏参数MDPs（Hi-MDPs）。这些是具有一个隐藏参数的MDPs，该参数控制转移函数，并且对这些隐藏参数的集合有一个分布，这个分布隐式地定义了一组MDPs。? (?)
    在Hi-MDPs的基础上提出了广义隐藏参数MDPs（GHP-MDPs），其中隐藏参数现在不仅控制动态，还控制奖励函数。我们的工作使用一个上下文参数，这个参数类似于隐藏参数，可以是隐藏的也可以是可观察的，它控制初始状态分布、转移函数和奖励函数，而不仅仅是转移函数和奖励函数。我们还更正式地讨论了在训练和测试过程中如何使用不同的上下文参数分布。
- en: ? (?, ?) introduce variations on the term "zero-shot policy transfer", which
    we use as the name for the formal class of problems we study. They both study
    problems within this class but don’t formalise the entire class of problems as
    we do here, instead focusing on presenting methods for improving performance in
    the empirical settings they investigate.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?, ?) 引入了“零-shot策略转移”这一术语的变体，我们用它作为我们研究的正式问题类的名称。他们都在这一类问题中进行研究，但不像我们在这里所做的那样对整个问题类进行形式化，而是专注于提出在他们研究的经验设置中提高性能的方法。
- en: ? (?) discusses a distribution over MDPs which are sampled from during training,
    and uses a kind of CMDP but where the context parameter adjusts only the observation
    function, rather than any other parts of the MDP. They also assume the distribution
    over MDPs is the same between training and testing.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 讨论了在训练过程中从中抽样的MDPs的分布，并使用了一种CMDP，但上下文参数仅调整观察函数，而不是MDP的其他部分。他们还假设训练和测试之间的MDP分布是相同的。
- en: ? (?, ?) present Contextual MDPs (CMDPs), which is the formalism we use as the
    base of our definitions. ? (?) present a formalism based on the context being
    part of an underlying state space, which is the one we use, while ? (?) present
    a formalism more similar to Hi-MDPs, where there is a collection of MDPs parameterised
    by a context variable. Both of these works don’t consider a shift in distribution
    over contexts between training and testing apart from a shift to the full distribution
    in ? (?). They don’t present a formal definition of the policy class and don’t
    define problem settings where the context distribution is controllable.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?, ?) 提出了上下文MDP（CMDPs），这是我们用作定义基础的形式化方法。? (?) 提出了基于上下文作为潜在状态空间一部分的形式化方法，这是我们使用的，而
    ? (?) 提出了更类似于Hi-MDPs的形式化方法，其中存在由上下文变量参数化的MDP集合。这些工作在训练和测试之间的上下文分布变化方面没有考虑，除了在
    ? (?) 中到完整分布的变化。它们没有提供策略类别的正式定义，也没有定义上下文分布可控的问题设置。
- en: 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 零样本泛化基准
- en: In this section, we give a taxonomy of benchmarks for ZSG in RL. A key split
    in the factors of variation for a benchmark is those factors concerned with the
    environment and those concerned with the evaluation protocol. A benchmark task
    is a combination of a choice of the environment (a CMDP, covered in [Section 4.1](#S4.SS1
    "4.1 Environments ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"))
    and a suitable evaluation protocol (a train and test context set, covered in [Section 4.2](#S4.SS2
    "4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")). This means that all environments support multiple possible evaluation
    protocols, as determined by their context sets.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们给出了RL中ZSG基准的分类。基准的变异因素中的一个关键区分是环境相关因素与评估协议相关因素。基准任务是环境选择（一个CMDP，详见 [第4.1节](#S4.SS1
    "4.1 环境 ‣ 4 零样本泛化基准 ‣ 深度强化学习中的零样本泛化综述")）与合适的评估协议（一个训练和测试上下文集，详见 [第4.2节](#S4.SS2
    "4.2 ZSG的评估协议 ‣ 4 零样本泛化基准 ‣ 深度强化学习中的零样本泛化综述")）的组合。这意味着所有环境都支持由其上下文集决定的多个可能的评估协议。
- en: Having categorised the set of benchmarks, we point out the limitations of the
    purely PCG approach to building environments ([Section 4.3](#S4.SS3.SSS0.Px4 "The
    Downsides of Procedural Content Generation for Zero-shot Generalisation. ‣ 4.3
    Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")), as well
    as discuss the range of difficulty among ZSG problems ([Section 4.3](#S4.SS3.SSS0.Px6
    "What Generalisation Can We Expect? ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")). More discussion of future work on benchmarks
    for ZSG can be found in [Sections 6.1](#S6.SS1 "6.1 Generalisation Beyond Zero-Shot
    Policy Transfer ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"), [6.2](#S6.SS2 "6.2 Real World Reinforcement
    Learning Generalisation ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning") and [6.4](#S6.SS4 "6.4 Tackling
    Stronger Types Of Variation ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning").
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在对基准集进行分类后，我们指出了纯PCG方法在构建环境时的局限性 ([第4.3节](#S4.SS3.SSS0.Px4 "程序化内容生成在零样本泛化中的缺点
    ‣ 4.3 讨论 ‣ 4 零样本泛化基准 ‣ 深度强化学习中的零样本泛化综述"))，同时讨论了ZSG问题的难度范围 ([第4.3节](#S4.SS3.SSS0.Px6
    "我们可以期望什么样的泛化？ ‣ 4.3 讨论 ‣ 4 零样本泛化基准 ‣ 深度强化学习中的零样本泛化综述"))。有关ZSG基准的未来工作的更多讨论可以在
    [第6.1节](#S6.SS1 "6.1 超越零样本策略转移的泛化 ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零样本泛化综述")、[第6.2节](#S6.SS2
    "6.2 现实世界强化学习的泛化 ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零样本泛化综述") 和 [第6.4节](#S6.SS4 "6.4 应对更强的变异类型
    ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零样本泛化综述") 中找到。
- en: \ssmall
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: \ssmall
- en: '| Name | Style | Contexts | Variation |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 风格 | 背景 | 变体 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Alchemy † (?) | 3D | PCG | D, R, S |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 炼金术 † (?) | 3D | PCG | D, R, S |'
- en: '| Animal-AI (?) | 3D | D-C, D-O | S, O |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Animal-AI (?) | 3D | D-C, D-O | S, O |'
- en: '| Atari Game Modes (?) | Arcade | D-C | D, O, S |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Atari Game Modes (?) | 街机 | D-C | D, O, S |'
- en: '| BabyAI (?) | Grid, LC | D-C, D-O, PCG | R, S |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| BabyAI (?) | 网格, LC | D-C, D-O, PCG | R, S |'
- en: '| CARL (?) | Varied | Con, D-C, D-O | D, O, R, S |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| CARL (?) | 多样 | Con, D-C, D-O | D, O, R, S |'
- en: '| CARLA (?) | 3D, Driving | D-C | O |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| CARLA (?) | 3D, 驾驶 | D-C | O |'
- en: '| CausalWorld † (?) | 3D, ConCon | Con, D-C, D-O | D, O, R, S |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| CausalWorld † (?) | 3D, ConCon | Con, D-C, D-O | D, O, R, S |'
- en: '| Construction (?) | 2D, Structured | Con, D-C, D-O, PCG | R, S |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Construction (?) | 2D, 结构化 | Con, D-C, D-O, PCG | R, S |'
- en: '| Crafter (?) | Arcade, Grid | PCG | S |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Crafter (?) | 街机, 网格 | PCG | S |'
- en: '| Crafting gridworld (?) | Grid, LC | D-C | R, S |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Crafting gridworld (?) | 网格, LC | D-C | R, S |'
- en: '| DACBench (?) | Structured | PCG, D-C | D, R, S |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| DACBench (?) | 结构化 | PCG, D-C | D, R, S |'
- en: '| DCS (?) | ConCon | Con, D-C | O |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| DCS (?) | ConCon | Con, D-C | O |'
- en: '| DistractingCarRacing (?) | Arcade | D-C | O |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DistractingCarRacing (?) | 街机 | D-C | O |'
- en: '| DistractingVizDoom (?) | 3D | D-C | O |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DistractingVizDoom (?) | 3D | D-C | O |'
- en: '| DMC-GB (?) | ConCon | Con, D-C | O |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| DMC-GB (?) | ConCon | Con, D-C | O |'
- en: '| DMC-Remastered (?) | ConCon | Con, D-C | O |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| DMC-Remastered (?) | ConCon | Con, D-C | O |'
- en: '| DM-Memory (?) | Arcade, 3D | PCG, Con, D-O, D-C | R, D, S |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| DM-Memory (?) | 街机, 3D | PCG, Con, D-O, D-C | R, D, S |'
- en: '| GenAsses (?) | ConCon | Con | D, S |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| GenAsses (?) | ConCon | Con | D, S |'
- en: '| GVGAI (?) | Grid | D-C | D, O, S |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| GVGAI (?) | 网格 | D-C | D, O, S |'
- en: '| HALMA † (?) | Grid | D-C, D-O | O, S |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| HALMA † (?) | 网格 | D-C, D-O | O, S |'
- en: '| iGibson (?) | 3D | D-C | O, S |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| iGibson (?) | 3D | D-C | O, S |'
- en: '| Jericho † (?) | Text | D-C | D, R, S |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Jericho † (?) | 文本 | D-C | D, R, S |'
- en: '| JumpingFromPixels (?) | Arcade | Con | S |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| JumpingFromPixels (?) | 街机 | Con | S |'
- en: '| KitchenShift (?) | 3D, ConCon | D-C | O, S, R |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| KitchenShift (?) | 3D, ConCon | D-C | O, S, R |'
- en: '| Malmo (?) | 3D, Arcade | D-C, D-O | R, S |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Malmo (?) | 3D, 街机 | D-C, D-O | R, S |'
- en: '| MarsExplorer (?) | Grid | PCG | S |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| MarsExplorer (?) | 网格 | PCG | S |'
- en: '| MazeExplore (?) | 3D | PCG | O, S |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| MazeExplore (?) | 3D | PCG | O, S |'
- en: '| MDP Playground † (?) | ConCon, Grid | Con, D-C, D-O | D, O, R, S |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| MDP Playground † (?) | ConCon, 网格 | Con, D-C, D-O | D, O, R, S |'
- en: '| Meta-World † (?) | 3D, ConCon | Con, D-C | R, S |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Meta-World † (?) | 3D, ConCon | Con, D-C | R, S |'
- en: '| MetaDrive (?) | 3D, Driving | D-C, D-O, PCG | D, S |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| MetaDrive (?) | 3D, 驾驶 | D-C, D-O, PCG | D, S |'
- en: '| MiniGrid (?) | Grid | PCG | S |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| MiniGrid (?) | 网格 | PCG | S |'
- en: '| MiniHack (?) | Grid | D-C, D-O, PCG | S |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| MiniHack (?) | 网格 | D-C, D-O, PCG | S |'
- en: '| NaturalEnvs CV (?) | Grid | PCG | O, R, S |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| NaturalEnvs CV (?) | 网格 | PCG | O, R, S |'
- en: '| NaturalEnvs MuJoCo (?) | ConCon | D-C | O |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| NaturalEnvs MuJoCo (?) | ConCon | D-C | O |'
- en: '| NLE (?) | Grid | PCG | S |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| NLE (?) | 网格 | PCG | S |'
- en: '| Noisy MuJoCo (?) | ConCon | Con, D-C | D, O |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Noisy MuJoCo (?) | ConCon | Con, D-C | D, O |'
- en: '| NovelGridworlds (?) | Grid | D-C | D, S |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| NovelGridworlds (?) | 网格 | D-C | D, S |'
- en: '| Obstacle Tower (?) | 3D | D-C, PCG | O, S |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Obstacle Tower (?) | 3D | D-C, PCG | O, S |'
- en: '| OffRoadBenchmark (?) | 3D, Driving | D-C, | O, S, R |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| OffRoadBenchmark (?) | 3D, 驾驶 | D-C | O, S, R |'
- en: '| OpenAI Procgen (?) | Arcade | PCG | O, S |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI Procgen (?) | 街机 | PCG | O, S |'
- en: '| OverParam Gym (?) | ConCon | Con | O |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| OverParam Gym (?) | ConCon | Con | O |'
- en: '| OverParam LQR (?) | LQR | Con | O |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| OverParam LQR (?) | LQR | Con | O |'
- en: '| ParamGen (?) | 3D, LC | D-C, D-O | R, S |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| ParamGen (?) | 3D, LC | D-C, D-O | R, S |'
- en: '| RLBench † (?) | 3D, ConCon, LC | Con, D-C, D-O | R, S |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| RLBench † (?) | 3D, ConCon, LC | Con, D-C, D-O | R, S |'
- en: '| RoboSuite (?) | 3D, ConCon | D-C | O |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| RoboSuite (?) | 3D, ConCon | D-C | O |'
- en: '| Rogue-gym (?) | Grid | PCG | S |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Rogue-gym (?) | 网格 | PCG | S |'
- en: '| RTFM (?) | Grid, LC | PCG | D, R, S |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| RTFM (?) | 网格, LC | PCG | D, R, S |'
- en: '| RWRL † (?) | ConCon | Con | D |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| RWRL † (?) | ConCon | Con | D |'
- en: '| Sokoban (?) | Grid | PCG | S |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Sokoban (?) | 网格 | PCG | S |'
- en: '| TextWorld † (?) | Text | Con, D-C, PCG | D, O, R, S |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| TextWorld † (?) | 文本 | Con, D-C, PCG | D, O, R, S |'
- en: '| Toybox † (?) | Arcade | Con, D-C, D-O | D, O, S |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Toybox † (?) | 街机 | Con, D-C, D-O | D, O, S |'
- en: '| TrapTube (?) | Grid | Con, D-C | D, O, S |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| TrapTube (?) | 网格 | Con, D-C | D, O, S |'
- en: '| WordCraft (?) | LC, Text | Con, D-C, D-O | R, S |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| WordCraft (?) | LC, 文本 | Con, D-C, D-O | R, S |'
- en: '| XLand (?) | 3D, LC | Con, D-C, D-O, PCG | D, O, R, S |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| XLand (?) | 3D, LC | Con, D-C, D-O, PCG | D, O, R, S |'
- en: '| Phy-Q (?) | Arcade | D-C, PCG | S |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Phy-Q (?) | 街机 | D-C, PCG | S |'
- en: 'Table 1: Categorisation of Environments for ZSG. In the Style column, LC stands
    for Language-Conditioned, ConCon for Continuous Control. In the Contexts column,
    PCG stands for Procedural Content Generation, Con for continuous, D-C for discrete
    cardinal and D-O for discrete ordinal. In the Variation column, S, D, O and R
    are respectively state, dynamics, observation or reward function variation. In
    the Name column, † refers to environments that were not originally designed as
    zero-shot policy transfer benchmarks but could be adapted to be. See main text
    for a more detailed description of the columns.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：ZSG环境的分类。在风格列中，LC代表语言条件，ConCon代表连续控制。在上下文列中，PCG代表程序化内容生成，Con代表连续，D-C代表离散基数，D-O代表离散序数。在变异列中，S、D、O和R分别表示状态、动态、观察或奖励函数变异。在名称列中，†表示那些最初并非设计为零样本策略迁移基准但可以适应为此的环境。有关各列的更详细描述，请参见正文。
- en: 4.1 Environments
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 环境
- en: Categorising Environments That Enable Generalisation
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分类可实现泛化的环境
- en: In [Table 1](#S4.T1 "In 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"),
    we list the available environments for testing ZSG in RL, as well as summarise
    each environment’s key properties. These environments all provide a non-singleton
    context set that can be used to create a variety of evaluation protocols. Choosing
    a specific evaluation protocol then produces a benchmark. We describe the meaning
    of the columns in [Table 1](#S4.T1 "In 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning") here.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表1](#S4.T1 "在4个基准测试中的零样本泛化 ‣ 深度强化学习中的零样本泛化调查")中，我们列出了测试ZSG在RL中的可用环境，并总结了每个环境的关键属性。这些环境都提供了一个非单一上下文集合，可以用来创建各种评估协议。选择特定的评估协议然后产生一个基准。我们在这里描述[表1](#S4.T1
    "在4个基准测试中的零样本泛化 ‣ 深度强化学习中的零样本泛化调查")中列的各列的含义。
- en: Style.
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 风格。
- en: This gives a rough high-level description of the kind of environment.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了环境的粗略高层次描述。
- en: Contexts.
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 上下文。
- en: This describes the context set. In the literature, there are two approaches
    to designing a context set, and the key difference between these approaches is
    whether the context-MDP creation is accessible and visible to the researcher.
    The first, which we refer to as Procedural Content Generation (PCG), relies on
    a single random seed to determine multiple choices during the context-MDP generation.
    Here the context set is the set of all supported random seeds. This is a black-box
    process in which the researcher only chooses a seed.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这描述了上下文集合。在文献中，有两种设计上下文集合的方法，这些方法之间的主要区别在于上下文-MDP创建是否对研究人员可访问和可见。第一种方法，我们称之为程序化内容生成（PCG），依赖于单个随机种子来确定上下文-MDP生成过程中的多个选择。在这里，上下文集合是所有支持的随机种子的集合。这是一个黑箱过程，其中研究人员只选择一个种子。
- en: The second approach provides more direct control over the factors of variation
    between context-MDPs, and we call these *Controllable* environments. The context
    set is generally a product of multiple factor spaces, some of which may be discrete
    (i.e. a choice between several colour schemes) and some continuous (i.e. a friction
    coefficient in a physical simulation). Borrowing from ? (?), a distinction between
    discrete factors of variation is whether they are cardinal (i.e. the choices are
    just a set with no additional structure) or ordinal (i.e. the set has additional
    structure through an ordering). Examples of cardinal factors include different
    game modes or visual distractions, and ordinal factors are commonly the number
    of entities of a certain type within the context-MDP. All continuous factors are
    effectively also ordinal factors, as continuity implies an ordering.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法提供了对上下文-MDP之间变异因素的更直接控制，我们称这些为*可控*环境。上下文集合通常是多个因素空间的乘积，其中一些可能是离散的（即在几个颜色方案之间选择）而一些是连续的（即物理仿真中的摩擦系数）。借鉴自？（？），离散变异因素的区别在于它们是基数的（即选择只是一个没有额外结构的集合）还是序数的（即集合通过排序具有额外结构）。基数因素的例子包括不同的游戏模式或视觉干扰，序数因素通常是上下文-MDP中某种类型实体的数量。所有连续因素实际上也是序数因素，因为连续性意味着排序。
- en: Previous literature has defined PCG as any process by which an algorithm produces
    MDPs given some input (?), which applies to both kinds of context sets we have
    described. Throughout the rest of this survey we use “PCG” to refer to black-box
    PCG, which uses a seed as input, and “controllable” to refer to environments where
    the context set directly changes the parameters of interest in the context-MDPs,
    which could also be seen as “white-box PCG”. We can understand (black-box) PCG
    settings as combinations of discrete and continuous factor spaces (i.e. controllable
    environments) where the choice of the value in each space is determined by the
    random generation process. However, only some environments make this more informative
    parametrisation of the context-MDPs available. In this table, we describe environments
    where this information is not easily controllable as PCG environments. See [Section 4.3](#S4.SS3.SSS0.Px4
    "The Downsides of Procedural Content Generation for Zero-shot Generalisation.
    ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")
    for a discussion of the downsides of purely PCG approaches.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以往文献将PCG定义为任何算法在给定一些输入（？）时生成MDPs的过程，这适用于我们描述的两种上下文集合。在本次调查的其余部分，我们使用“PCG”来指代黑箱PCG，它使用种子作为输入，并使用“可控”来指代环境，其中上下文集合直接改变上下文-MDP中的感兴趣参数，这也可以被视为“白箱PCG”。我们可以理解（黑箱）PCG设置为离散和连续因素空间的组合（即可控环境），其中每个空间中值的选择由随机生成过程决定。然而，只有一些环境使得这种对上下文-MDPs的参数化变得更具信息性。在此表中，我们将无法轻松控制这些信息的环境描述为PCG环境。有关纯PCG方法的缺点，请参见[第4.3节](#S4.SS3.SSS0.Px4
    "The Downsides of Procedural Content Generation for Zero-shot Generalisation.
    ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")。
- en: Variation.
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 变化。
- en: This describes what varies within the set of context MDPs. This could be state-space
    variation (the initial state distribution and hence implicitly the state space),
    dynamics variation (the transition function), visual variation (the observation
    function) or reward function variation. Where the reward varies, the policy often
    needs to be given some indication of the goal or reward, so that the set of contexts
    is solvable by a single policy (?).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这描述了在上下文MDP集合中变化的内容。这可能包括状态空间的变化（初始状态分布以及隐含的状态空间）、动态变化（过渡函数）、视觉变化（观察函数）或奖励函数变化。当奖励变化时，策略通常需要一些目标或奖励的指示，以便由单一策略解决上下文集合（？）。
- en: 4.1.1 Trends In Environments
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 环境中的趋势
- en: There are several trends and patterns shown in [Table 1](#S4.T1 "In 4 Benchmarks
    For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning"), which we draw the reader’s attention
    to here. We describe 55 environments in total and have aimed to be fairly exhaustive.³³3As
    such, if you think there are missing environments, please contact us using the
    details provided in the author list
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S4.T1 "In 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")中展示了几种趋势和模式，我们在这里引起读者的注意。我们总共描述了55个环境，并力求尽可能详尽。³³3因此，如果你认为缺少环境，请使用作者名单中提供的联系方式与我们联系。'
- en: There are a range of different Styles that these environments have, which is
    beneficial as ZSG methods should themselves be generally applicable across styles
    if possible. While numerically there is a focus on gridworlds (14, 25%) and continuous
    control (13, 24%) there are well-established benchmarks for arcade styles (?)
    and 3D environments (?). Looking at Context sets, we see that PCG is heavily used
    in ZSG environments, featuring in 21 (38%) environments. Many environments combine
    PCG components with controllable variation (?, ?, ?, ?, ?, ?, ?, ?, ?). Most environments
    have several different kinds of factors of variation within their context set.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这些环境有多种不同的风格，这很有益，因为ZSG方法如果可能的话，应该能广泛适用于各种风格。虽然在数值上重点关注网格世界（14, 25%）和连续控制（13,
    24%），但对于街机风格（？）和3D环境（？）已有成熟的基准。查看上下文集合，我们看到PCG在ZSG环境中被广泛使用，出现在21（38%）个环境中。许多环境将PCG组件与可控变化（？，？，？，？，？，？，？，？，？，？）结合在一起。大多数环境在其上下文集合中有几种不同的变化因素。
- en: There are a lot of differences between environments when looking at the Variation
    they use. Numerically, state variation is most common (42, 76%) followed by observation
    (29, 53%), and then reward (20, 36%) and dynamics (19, 35%). Most environments
    have multiple different types of variation (34, 62%), and while there are several
    environments targeted at just observation variation (10, 18%) or state variation
    (9, 16%), there is only a single environment with solely dynamics variation (RWRL,
    ?), and none with solely reward variation. State and Observation variations are
    often the easiest to engineer, especially with the aid of PCG. This is because
    changing the rendering effects of a simulator, or designing multiple ways the
    objects in a simulator could be arranged, is generally easier than designing a
    simulator engine that is parameterisable (for dynamics variation). Creating an
    environment for reward variation requires further design choices about how to
    specify the reward function or goal such that the environment satisfies the Principle
    Of Unchanged Optimality (?). PCG is often the only good way of generating a large
    diversity in state variation, and as such is often necessary to create highly
    varied environments. Only CausalWorld (?) enables easy testing of all forms of
    variation at once.⁴⁴4While CARL (?), MDP Playground (?), XLand (?) and TextWorld (?)
    are also categorised as containing all forms of variation, CARL is a collection
    of different environments, none of which have all variation types, XLand is not
    open-source, and MDP Playground and TextWorld would require significant work to
    construct a meaningful evaluation protocol which varies along all factors. Further,
    MDP Playground does not provide a method for describing the changed reward function
    to the agent, and there is uncertainty in how to interpret observation variation
    in text-based games like TextWorld.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察其使用的变化时，环境之间存在许多差异。在数值上，状态变化最为常见（42，76%），其次是观察变化（29，53%），然后是奖励变化（20，36%）和动态变化（19，35%）。大多数环境具有多种不同类型的变化（34，62%），虽然有几种环境仅针对观察变化（10，18%）或状态变化（9，16%），但仅有一个环境具有完全的动态变化（RWRL，？），且没有仅具有奖励变化的环境。状态和观察变化通常是最容易设计的，特别是在PCG的帮助下。这是因为改变模拟器的渲染效果，或设计模拟器中对象的多种排列方式，通常比设计一个可以参数化的模拟器引擎（用于动态变化）要容易。为奖励变化创建环境需要进一步的设计选择，以指定奖励函数或目标，以使环境符合不变最优性原则（？）。PCG通常是生成大量状态变化多样性的唯一有效方法，因此通常是创建高度多样化环境所必需的。只有CausalWorld（？）能够轻松地一次测试所有形式的变化。虽然CARL（？）、MDP
    Playground（？）、XLand（？）和TextWorld（？）也被归类为包含所有形式的变化，但CARL是不同环境的集合，其中没有任何一个环境具有所有变化类型，XLand不是开源的，而MDP
    Playground和TextWorld需要大量工作来构建一个在所有因素上都有变化的有意义的评估协议。此外，MDP Playground没有提供向代理描述变化后的奖励函数的方法，而在像TextWorld这样的文本游戏中，如何解释观察变化存在不确定性。
- en: 'There are several clusters that can be pointed out in the collection of benchmarks:
    There are several PCG state-varying gridworld environments (MiniGrid, BabyAI,
    Crafter, Rogue-gym, MarsExplorer, NLE, MiniHack; ?, ?, ?, ?, ?, ?, ?), non-PCG
    observation-varying continuous control environments (RoboSuite, DMC-Remastered,
    DMC-GB, DCS, KitchenShift, NaturalEnvs MuJoCo; ?, ?, ?, ?, ?, ?), and multi-task
    continuous control benchmarks which could be adapted to ZSG (CausalWorld, RLBench,
    Meta-world; ?, ?, ?).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在基准测试集合中，可以指出几个集群：有几个PCG状态变化的网格世界环境（MiniGrid、BabyAI、Crafter、Rogue-gym、MarsExplorer、NLE、MiniHack；？，？，？，？，？，？，？，），非PCG观察变化的连续控制环境（RoboSuite、DMC-Remastered、DMC-GB、DCS、KitchenShift、NaturalEnvs
    MuJoCo；？，？，？，？，？，？，），以及可以适应ZSG的多任务连续控制基准（CausalWorld、RLBench、Meta-world；？，？，？）。
- en: 4.2 Evaluation Protocols For Zsg
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 Zsg的评估协议
- en: As discussed, a benchmark is the combination of an environment and an evaluation
    protocol. Each environment supports a range of evaluating protocols determined
    by the context set, and often there are protocols recommended by the environment
    creators. In this section, we discuss the protocols and the differences between
    them. An evaluation protocol specifies the training and testing context sets,
    any restrictions on sampling from the training set during training, and the number
    of samples allowed from the training environment.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 正如讨论过的，基准测试是环境和评估协议的结合。每个环境支持由上下文设置决定的一系列评估协议，并且通常有环境创建者推荐的协议。在本节中，我们讨论这些协议及其之间的区别。评估协议指定了训练和测试的上下文集、训练期间从训练集抽样的任何限制，以及允许从训练环境中获得的样本数量。
- en: An important first attribute that varies between evaluation protocols is *context-efficiency*.
    This is analogous to sample efficiency, where only a certain number of samples
    are allowed during training, but instead, we place restrictions on the number
    of *contexts*. This ranges from a single context to a small number of contexts,
    to the entire context set.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 评估协议中一个重要的首要属性是*上下文效率*。这类似于样本效率，其中训练期间允许的样本数量有限，但我们对*上下文*的数量施加限制。这可以从单一上下文到少量上下文，再到整个上下文集。
- en: '![Refer to caption](img/125d42dad954d8c3b4c7020fc6797d6d.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/125d42dad954d8c3b4c7020fc6797d6d.png)'
- en: 'Figure 2: Visualisation of Evaluation Protocols for PCG Environments. A is
    a single training context, and the whole context set for testing. B uses a small
    collection of training contexts randomly sampled from the context set and the
    entire space for testing. C effectively reverses this, using the entire context
    set for training apart from several randomly sampled held-out contexts that are
    used for testing. The lack of axes indicates that these sets have no structure.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：PCG环境评估协议的可视化。A是一个单一的训练上下文，以及整个上下文集用于测试。B使用从上下文集中随机抽取的小集合训练上下文，以及整个空间用于测试。C有效地反转了这一点，使用整个上下文集进行训练，除了几个随机抽取的保留上下文用于测试。缺乏坐标轴表示这些集合没有结构。
- en: PCG Evaluation Protocols.
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PCG评估协议。
- en: In fact, in purely PCG environments, the only meaningful factor of variation
    between evaluation protocols is the context efficiency restriction. As we have
    no control over the factors of variation apart from sampling random seeds, the
    only choice we have is how many contexts to use for training. Further, the only
    meaningful testing context set is the full distribution, as taking a random sample
    from it (the only other option) would just be an approximation of the performance
    on the full distribution. This limitation of PCG environments is discussed further
    below ([Section 4.3](#S4.SS3.SSS0.Px4 "The Downsides of Procedural Content Generation
    for Zero-shot Generalisation. ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在纯PCG环境中，评估协议之间唯一有意义的变化因素是上下文效率限制。由于除了随机种子采样外，我们无法控制变化因素，我们唯一的选择是使用多少个上下文进行训练。此外，唯一有意义的测试上下文集是完整分布，因为从中抽取的随机样本（唯一的其他选择）只是对完整分布性能的近似。PCG环境的这一限制在下面进一步讨论（[第4.3节](#S4.SS3.SSS0.Px4
    "程序化内容生成对零样本泛化的缺点 ‣ 4.3讨论 ‣ 4个零样本泛化基准 ‣ 深度强化学习中的零样本泛化调查")）。
- en: 'This gives three classes of evaluation protocol for PCG environments, as determined
    by their training context set: A single context, a small set of contexts, or the
    full context set. These are visualised in [Fig. 2](#S4.F2 "In 4.2 Evaluation Protocols
    For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning") A, B and
    C respectively. There are not any examples of protocol A (for purely PCG environments),
    likely due to the high difficulty of such a challenge. For protocol B, while “a
    small set of contexts” is imprecise, the relevant point is that this set is meaningfully
    different from the full context set: it is possible to overfit to this set without
    getting good performance on the testing set. Examples of this protocol include
    both modes of OpenAI Procgen (?), RogueGym (?), some uses of JumpingFromPixels (?)
    and MarsExplorer (?).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了三种PCG环境的评估协议类别，根据它们的训练上下文集：单一上下文、少量上下文集或完整上下文集。这些在[图2](#S4.F2 "在4.2节中评估协议针对Zsg
    ‣ 4个零样本泛化基准 ‣ 深度强化学习中的零样本泛化调查")中分别以A、B和C进行可视化。没有协议A的示例（针对纯PCG环境），这可能是由于这种挑战的高难度。对于协议B，虽然“小集合上下文”不够精确，但相关点在于，这个集合与完整上下文集有显著区别：可能会过拟合这个集合，而在测试集上表现不佳。这个协议的示例包括OpenAI
    Procgen（？）、RogueGym（？）、JumpingFromPixels（？）和MarsExplorer（？）的两种模式。
- en: Protocol C is commonly used among PCG environments that are not explicitly targeted
    at ZSG (MiniGrid, NLE, MiniHack, Alchemy; ?, ?, ?, ?). The testing context set
    consists of seeds held out from the training set, and otherwise during training
    the full context set is used. This protocol effectively tests for more robust
    RL optimisation improvements but does not test for zero-shot generalisation beyond
    avoiding memorising. While this protocol only tests for ZSG in a weak sense, it
    still matches a wider variety of real-world deployment scenarios than the previous
    standard in RL, where the evaluation of the policy is performed on the environment
    it is trained in, and so we believe it should be the standard evaluation protocol
    in RL (not just in ZSG), and the previous standard should be considered a special
    case.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 协议 C 通常用于那些未明确针对 ZSG（MiniGrid、NLE、MiniHack、Alchemy；？，？，？，？）的 PCG 环境。测试上下文集由从训练集中保留的种子组成，在训练过程中则使用完整的上下文集。这个协议有效地测试了更稳健的
    RL 优化改进，但并没有测试超越避免记忆的零样本泛化。虽然这个协议只是以一种弱意义上测试 ZSG，但它仍然匹配了比 RL 中的先前标准更广泛的真实世界部署场景，因为在先前的标准中，策略的评估是在其训练环境中进行的，因此我们认为它应该是
    RL（不仅仅是 ZSG）的标准评估协议，而之前的标准应被视为特例。
- en: Controllable Environment Evaluation Protocols.
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可控环境评估协议。
- en: Many environments do not use only PCG and have factors of variation that can
    be controlled by the user of the environment. In these controllable environments,
    there is a much wider range of possible evaluation protocols.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 许多环境不仅使用 PCG，还具有可以由环境用户控制的变化因素。在这些可控环境中，可能的评估协议范围要广泛得多。
- en: The choice in PCG protocols – between a single context, a small set, or the
    entire range of contexts – transfers to the choice for each factor of variation
    in a controllable environment. For each factor, we can choose one of these options
    for the training context set, and then choose to sample either within or outside
    this range for the testing context set. The range of options is visualised in
    [Fig. 3](#S4.F3 "In Controllable Environment Evaluation Protocols. ‣ 4.2 Evaluation
    Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning").
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: PCG 协议中的选择——单一上下文、小集合或整个上下文范围——也转移到可控环境中的每个变化因素的选择上。对于每个因素，我们可以选择这些选项中的一个作为训练上下文集，然后选择在这个范围内或之外进行采样作为测试上下文集。选项的范围在[图
    3](#S4.F3 "在可控环境评估协议中。 ‣ 4.2 Zsg 的评估协议 ‣ 4 零样本泛化的基准 ‣ 深度强化学习中的零样本泛化调研")中进行了可视化。
- en: '![Refer to caption](img/1c74ee5601979dafc27d8b963e55f4d7.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1c74ee5601979dafc27d8b963e55f4d7.png)'
- en: 'Figure 3: Visualisation of Evaluation Protocols for Controllable Environments.
    Each diagram visualises one possible training context set (blue), and multiple
    possible testing context sets (all other colours). In A we choose the range for
    each factor of variation independently for the training distribution, resulting
    in a convex shape for this distribution. In this setting, possible testing distributions
    can either be interpolation (red), extrapolation along a single factor (either
    green square) or extrapolation along both factors (blue). In B and C the ranges
    for each factor of variation are linked together, resulting in a non-convex shape
    for the training distribution. This allows an additional type of generalisation
    to be tested, combinatorial interpolation (yellow), where the factors take values
    seen during training independently, but in unseen combinations. We continue to
    have the previous interpolation and extrapolation testing distributions. The difference
    from B to C is in the width of the training distribution in the axes along which
    we expect the agent to generalise. In C the policy will not be able to learn that
    the two factors can vary independently at all, making all forms of generalisation
    harder. Note that in actual environments and real-world settings it is likely
    this space will be higher than two dimensions and contain non-continuous and non-ordinal
    axes. The axes indicate that in this setting we have control over these factors
    of variation, in contrast to [Fig. 2](#S4.F2 "In 4.2 Evaluation Protocols For
    Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning ‣ A
    Survey of Zero-shot Generalisation in Deep Reinforcement Learning").'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：可控环境评估协议的可视化。每个图表可视化了一个可能的训练上下文集合（蓝色），以及多个可能的测试上下文集合（其他所有颜色）。在 A 中，我们独立选择每个变化因素的范围用于训练分布，从而产生了该分布的凸形状。在这种设置下，可能的测试分布可以是插值（红色）、沿单一因素的外推（绿色方块）或沿两个因素的外推（蓝色）。在
    B 和 C 中，每个变化因素的范围是相互关联的，从而使训练分布呈现非凸形状。这允许测试另一种类型的泛化——组合插值（黄色），其中因素在训练期间看到的值独立出现，但以未见过的组合形式出现。我们继续拥有之前的插值和外推测试分布。从
    B 到 C 的区别在于训练分布在我们期望代理泛化的轴上的宽度。在 C 中，策略将无法学会这两个因素可以完全独立变化，从而使所有形式的泛化更加困难。请注意，在实际环境和现实世界的设置中，这个空间可能高于二维，并包含非连续和非序数轴。这些轴表明，在这种设置下，我们对这些变化因素有控制权，这与[图
    2](#S4.F2 "在 4.2 Zsg 评估协议 ‣ 4 强化学习中的零样本泛化基准 ‣ 深度强化学习中零样本泛化的综述")形成对比。
- en: Making this choice for each factor independently gives us a convex training
    context set within the full context set ([Fig. 3](#S4.F3 "In Controllable Environment
    Evaluation Protocols. ‣ 4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning") A). For testing, each factor can then be either
    inside or outside this convex set (often respectively referred to as interpolation
    and extrapolation). The number of factors chosen to be extrapolating contributes
    to the difficulty of the evaluation protocol.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个因素独立地做出选择会给我们一个在完整上下文集合内的凸训练上下文集合（[图 3](#S4.F3 "在可控环境评估协议中。 ‣ 4.2 Zsg 评估协议
    ‣ 4 强化学习中的零样本泛化基准 ‣ 深度强化学习中零样本泛化的综述") A）。在测试时，每个因素可以在这个凸集合内或外（通常分别称为插值和外推）。选择外推的因素数量会影响评估协议的难度。
- en: 'However, if we create correlations or links between values of factors during
    training, we can get a non-convex training context set within the full context
    set ([Fig. 3](#S4.F3 "In Controllable Environment Evaluation Protocols. ‣ 4.2
    Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")
    B). Each possible testing context can either be within the training context set
    (fully interpolation), within the set formed by taking the convex hull of the
    non-convex training context set (combinatorial interpolation), or fully outside
    the convex hull (extrapolation). Combinatorial interpolation tests the ability
    of an agent to exhibit *systematicity*, a form of compositional ZSG discussed
    in [Section 3.1](#S3.SS1 "3.1 Background: Generalisation In Supervised Learning
    ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning"). For ordinal factors,
    we can also choose disjoint ranges, which allows us to test interpolation along
    a single axis (i.e. taking values between the two ranges). Note that when discussing
    convex hulls, this only applies to factors of variation that are continuous or
    discrete-ordinal; for cardinal factors of variation, the convex hull just includes
    those values sampled during training.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们在训练期间创建因子值之间的关联或链接，我们可以在完整的上下文集中得到一个非凸的训练上下文集（[图3](#S4.F3 "在可控环境评估协议中。
    ‣ 4.2 Zsg的评估协议 ‣ 4 零样本泛化的基准 ‣ 深度强化学习中的零样本泛化调查") B）。每一个可能的测试上下文可以在训练上下文集中（完全插值）、在由非凸训练上下文集的凸包形成的集合内（组合插值），或完全在凸包之外（外推）。组合插值测试了代理展示*系统性*的能力，这是一种在[第3.1节](#S3.SS1
    "3.1 背景：监督学习中的泛化 ‣ 3 强化学习中零样本泛化的形式化 ‣ 深度强化学习中的零样本泛化调查")中讨论的组成型ZSG。对于序数因子，我们还可以选择不相交的范围，这使我们能够沿单一轴进行插值（即在两个范围之间取值）。注意，当讨论凸包时，这仅适用于连续或离散序数的变化因子；对于基数变化因子，凸包仅包括在训练期间采样的值。
- en: For example, consider a policy trained in a CMDP where the context set consists
    of values for friction and gravity strength. During training, the environments
    instances have either a friction coefficient between 0.5 and 1 but gravity fixed
    at 1, or gravity strength ranging between 0.5 and 1 but friction fixed at 1 (the
    light blue line in [Fig. 3](#S4.F3 "In Controllable Environment Evaluation Protocols.
    ‣ 4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning") C). Testing contexts which take friction and gravity values within
    the training distribution are full interpolation (e.g. $(f=0.5,g=1),(f=1,g=1)$),
    contexts which take values for friction and gravity which have been seen independently
    but not in combination are combinatorial interpolation (e.g. $(f=0.5,g=0.5),(f=0.5,g=0.9)$,
    the yellow area), and contexts which take values for friction and gravity which
    are outside the seen ranges during training are full extrapolation (e.g. $(f=0.2,g=0.5),(f=1.1,g=1.5)$,
    either the dark blue or green areas).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个在CMDP中训练的策略，其中上下文集包含摩擦和重力强度的值。在训练期间，环境实例的摩擦系数在0.5到1之间，但重力固定为1，或者重力强度在0.5到1之间，但摩擦固定为1（[图3](#S4.F3
    "在可控环境评估协议中。 ‣ 4.2 Zsg的评估协议 ‣ 4 零样本泛化的基准 ‣ 深度强化学习中的零样本泛化调查") C）. 测试上下文中摩擦和重力值在训练分布内的为完全插值（例如 $(f=0.5,g=1),(f=1,g=1)$），摩擦和重力值分别见过但未组合的为组合插值（例如 $(f=0.5,g=0.5),(f=0.5,g=0.9)$，黄色区域），摩擦和重力值在训练期间未见过的为完全外推（例如 $(f=0.2,g=0.5),(f=1.1,g=1.5)$，深蓝色或绿色区域）。
- en: 'We can still consider the number of contexts within the training context set,
    which controls the density of the training context set, given its shape. When
    testing for extrapolation we can also vary the “width” of the training context
    set on the axis of variation along which extrapolation is being tested ([Fig. 3](#S4.F3
    "In Controllable Environment Evaluation Protocols. ‣ 4.2 Evaluation Protocols
    For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning") B vs C).
    These tests evaluate the agent’s ability to exhibit *productivity* ([Section 3.1](#S3.SS1
    "3.1 Background: Generalisation In Supervised Learning ‣ 3 Formalising Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")). Of course, ZSG will be easier if there is a
    wide diversity of values for this factor at training time, even if the values
    at test time are still outside this set. For example, if we are testing whether
    a policy can generalise to novel amounts of previously seen objects, then we should
    expect the policy to perform better if it has seen different amounts during training,
    as opposed to only having seen a single amount of the object during training.
    To expand on the friction and gravity example, during training the policy never
    sees gravity and friction varying together, which makes it much more difficult
    to generalise to the testing contexts. If gravity and friction did vary together
    during training then this would make generalisation easier.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以考虑训练上下文集中的上下文数量，这控制了训练上下文集的密度，取决于其形状。在测试推断时，我们还可以在变化轴上变化训练上下文集的“宽度” ([Fig.
    3](#S4.F3 "在可控环境评估协议中。 ‣ 4.2 ZSG评估协议 ‣ 4 强化学习中零样本泛化的基准 ‣ 深度强化学习中零样本泛化的调查") B vs
    C)。这些测试评估了代理展示*生产力*的能力 ([Section 3.1](#S3.SS1 "3.1 背景：监督学习中的泛化 ‣ 3 在强化学习中形式化零样本泛化
    ‣ 深度强化学习中零样本泛化的调查"))。当然，如果在训练时这个因素的值有广泛的多样性，即使测试时的值仍在此集合之外，ZSG也会更容易。例如，如果我们测试策略是否能够对先前见过的对象的新数量进行泛化，那么我们应该期望策略表现更好，如果在训练期间它见过不同数量的对象，而不是仅见过单一数量的对象。为了扩展摩擦力和重力的例子，在训练期间，策略从未见过重力和摩擦力一起变化，这使得在测试上下文中进行泛化变得更加困难。如果在训练期间重力和摩擦力确实一起变化，那么这将使泛化变得更容易。
- en: 'A notable point in this space is that of a single training context and a wide
    variety of testing contexts. This protocol tests for a strong form of ZSG, where
    the policy must be able to extrapolate to unseen contexts at test time. Because
    of the difficulty of this problem, benchmarks with this evaluation protocol focus
    on visual variation: the policy needs to be robust to different observation functions
    on the same underlying MDP (?, ?, ?). The protocol is often motivated by the sim-to-real
    problem, where we expect an agent trained in a single simulation to be robust
    to multiple visually different real-world settings at deployment time.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域中的一个显著点是单一训练上下文和各种测试上下文。该协议测试了一种强形式的零样本泛化（ZSG），其中策略必须能够在测试时推断出未见过的上下文。由于这个问题的困难，这种评估协议的基准测试侧重于视觉变化：策略需要在相同的基础马尔科夫决策过程（MDP）上对不同的观察函数具有鲁棒性（？，？，？）。该协议通常受到从模拟到现实问题的启发，我们期望在单一模拟中训练的代理能够在部署时对多个视觉上不同的现实世界环境具有鲁棒性。
- en: Beyond this single point, it is challenging to draw any more meaningful categorisation
    from the current array of evaluation protocols. Generally, each one is motivated
    by a specific problem setting or characteristic of human reasoning which we believe
    RL agents should be able to solve or have respectively.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个单一的点之外，从当前的评估协议数组中很难提取出更多有意义的分类。通常，每个协议都是由特定的问题设置或我们认为强化学习（RL）代理应当能够解决或具备的特征驱动的。
- en: 4.3 Discussion
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 讨论
- en: There are several comments, insights and conclusions that can be gained from
    surveying the breadth of ZSG benchmarks, which we raise here.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从调查ZSG基准的广度中可以获得若干评论、见解和结论，我们在此提出。
- en: Non-visual Generalisation.
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非视觉泛化。
- en: If testing for non-visual types of generalisation, then visually simple domains
    such as MiniHack (?) and NLE (?) should be used. These environments contain enough
    complexity to test for many types and strengths of non-visual generalisation but
    save on computation due to the lack of complex visual processing required. There
    are many real-world problem settings where no visual processing is required, such
    as systems control and recommender systems. Representation learning is still a
    problem in these non-visual domains, as many of them have a large variety of entities
    and scenarios which representations and hence policies need to generalise across.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果测试非视觉类型的泛化，那么应该使用视觉简单的领域，如MiniHack（？）和NLE（？）。这些环境包含足够的复杂性来测试许多类型和强度的非视觉泛化，但由于缺乏复杂的视觉处理要求，计算量较少。在许多不需要视觉处理的真实世界问题设置中，如系统控制和推荐系统，表示学习仍然是一个问题，因为它们中有许多实体和场景需要进行泛化。
- en: DeepMind Control Suite Variants.
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DeepMind 控制套件变体。
- en: 'A sub-category of ZSG benchmarks unto itself is the selection of DeepMind Control
    Suite (?) variants: DMC-Remastered, DMC-Generalisation Benchmark, Distracting
    Control Suite, Natural Environments (?, ?, ?, ?). All these environments focus
    on visual generalisation and sample efficiency, require learning continuous control
    policies from pixels and introduce visual distractors that the policy should be
    invariant to which are either available during training or only present at deployment.
    We believe that Distracting Control Suite (?) is the most fully-featured variant
    in this space, as it features the broadest set of variations, the hardest combinations
    of which are unsolvable by current methods.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ZSG基准的一个子类别是选择DeepMind控制套件的变体：DMC-重制版，DMC-泛化基准，分心控制套件，自然环境（？，？，？，？）。所有这些环境都关注视觉泛化和样本效率，需要从像素学习连续控制策略，并引入策略应对的视觉干扰，这些干扰可能在训练期间存在或仅在部署时出现。我们认为分心控制套件（？）是这个领域中功能最全面的变体，因为它具有最广泛的变化集，其中最难的组合当前方法无法解决。
- en: Unintentional Generalisation Benchmarks.
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无意间的泛化基准。
- en: Some environments listed in [Table 1](#S4.T1 "In 4 Benchmarks For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning") were not originally intended as ZSG benchmarks.
    For example, ? (?) presents three highly parameterisable versions of Atari games
    and uses them to perform post hoc analysis of agents trained on a single variant.
    Some environments are not targeted at zero-shot policy transfer (CausalWorld,
    RWRL, RLBench, Alchemy, Meta-world (?, ?, ?, ?, ?)), but could be adapted to such
    a scenario with a different evaluation protocol. More generally, all environments
    provide a context set, and many then propose specific evaluation protocols, but
    other protocols could be used as long as they were well-justified. This flexibility
    has downsides, as different methods can be evaluated on subtly different evaluation
    protocols which may favour one over another. We recommend being explicit when
    using these benchmarks in exactly which protocol is being used and comparing with
    evaluations of previous methods. Using a standard protocol aids reproducibility.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S4.T1 "在强化学习中的零样本泛化的4个基准 ‣ 深度强化学习中的零样本泛化调查")中列出的一些环境最初并非旨在作为ZSG基准。例如，？（？）展示了三个高度参数化的Atari游戏版本，并使用它们对在单一变体上训练的代理进行事后分析。一些环境并不针对零样本策略转移（CausalWorld，RWRL，RLBench，Alchemy，Meta-world（？，？，？，？，？）），但可以通过不同的评估协议调整为这种场景。更一般地说，所有环境提供一个上下文集，许多然后提出具体的评估协议，但只要这些协议有充分的理由，其他协议也可以使用。这种灵活性有缺点，因为不同的方法可能在细微不同的评估协议上进行评估，这些协议可能对某些方法有利。我们建议在使用这些基准时明确使用的协议，并与以前方法的评估进行比较。使用标准协议有助于重复性。'
- en: The Downsides of Procedural Content Generation for Zero-shot Generalisation.
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 程序化内容生成对于零样本泛化的缺点。
- en: Many environments make use of procedural content generation (PCG) for creating
    a variety of context-MDPs. In these environments, the context set is the set of
    random seeds used for the PCG and has no additional structure with which to control
    the variation between context-MDPs.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 许多环境利用程序化内容生成（PCG）来创建各种上下文MDP。在这些环境中，上下文集是用于PCG的随机种子集合，并且没有额外的结构来控制上下文MDP之间的变化。
- en: 'This means that while PCG is a useful tool for creating a large set of context-MDPs,
    there is a downside to purely PCG-based environments: the range of evaluation
    protocols supported by these environments is limited to different sizes of the
    training context set. Measuring zero-shot generalisation along specific factors
    of variation is impossible without significant effort either labelling generated
    levels or unravelling the PCG to expose the underlying parametrisation which captures
    these factors. Often more effort is required to enable setting these factors to
    specific values, as opposed to just revealing their values for generated levels.
    Hence, PCG benchmarks are testing for a “general” form of ZSG and RL optimisation,
    but do not enable more targeted evaluation of specific types of ZSG. This means
    making research progress on specific problems is difficult, as focusing on the
    specific bottleneck in isolation is hard.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着虽然 PCG 是创建大量上下文 MDP 的有用工具，但完全基于 PCG 的环境有一个缺点：这些环境支持的评估协议范围仅限于不同大小的训练上下文集。测量特定因素的零样本泛化是无法实现的，除非对生成的关卡进行大量标注或解开
    PCG 以暴露捕捉这些因素的底层参数化。通常，需要更多的努力来将这些因素设置为特定值，而不是仅仅揭示生成关卡的值。因此，PCG 基准测试测试的是“**通用**”形式的
    ZSG 和 RL 优化，但不支持更有针对性的特定类型 ZSG 评估。这意味着在特定问题上取得研究进展是困难的，因为单独关注特定瓶颈很难。
- en: An interesting compromise, which is struck by several environments, is to have
    some low-level portion of the environment procedurally generated, but still have
    many factors of variation under the control of the researcher. For example, Obstacle
    Tower (?) has procedurally generated level layouts, but the visual features (and
    to some extent the layout complexity) can be controlled. Another example is MiniHack
    (?), where entire MDPs can be specified from scratch in a rich description language,
    and PCG can fill in any components if required. These both enable more targeted
    types of experimentation. We believe this kind of combined PCG and controllable
    environment is the best approach for designing future environments; some usage
    of PCG will be necessary to generate sufficient variety in the environments (especially
    the state space), and if the control is fine-grained enough to enable precise
    scientific experimentation, then the environment will still be useful for disentangling
    progress in ZSG.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有趣的折衷方案是，在多个环境中，一部分低级环境是程序生成的，但仍有许多变因素由研究者控制。例如，Obstacle Tower (?) 具有程序生成的关卡布局，但视觉特征（以及在一定程度上的布局复杂性）可以被控制。另一个例子是
    MiniHack (?)，在该环境中，可以用丰富的描述语言从头指定整个 MDP，且 PCG 可以在需要时填充任何组件。这两种方法都使得实验类型更加有针对性。我们认为这种结合了
    PCG 和可控环境的方法是设计未来环境的**最佳**途径；使用 PCG 生成足够的环境多样性（尤其是状态空间）是必要的，如果控制足够细致以实现精确的科学实验，那么环境仍然可以用于**剥离**
    ZSG 的进展。
- en: Compositional Generalisation in Contextual MDPs.
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 上下文 MDP 中的组合泛化。
- en: 'Compositional generalisation is a key point of interest for many researchers
    (see [Section 3.1](#S3.SS1 "3.1 Background: Generalisation In Supervised Learning
    ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")). In *controllable*
    environments, different evaluation protocols enable us to test for some of the
    forms of compositional generalisation introduced in [Section 3.1](#S3.SS1 "3.1
    Background: Generalisation In Supervised Learning ‣ 3 Formalising Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning") (?): (1) Systematicity can be evaluated using a multidimensional context
    set, and testing on novel combinations of the context dimensions not seen at training
    time (combinatorial interpolation in [Fig. 3](#S4.F3 "In Controllable Environment
    Evaluation Protocols. ‣ 4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")). (2) Productivity can be evaluated with ordinal
    or continuous factors, measuring the ability to perform well in environment instances
    with context values beyond those seen at training time (either type of extrapolation
    in [Fig. 3](#S4.F3 "In Controllable Environment Evaluation Protocols. ‣ 4.2 Evaluation
    Protocols For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")).
    If dealing with CMDPs where the context space is partially language, as in language-conditioned
    RL (?), then the evaluations discussed by ? (?) can be directly applied to the
    language space. Crucially, a controllable environment with a structured context
    space is required to test these forms of compositional generalisation, and to
    ensure that the agent is seeing truly novel combinations at test time; this is
    difficult to validate in PCG environments like OpenAI Procgen (?) or NLE (?).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '组合概括是许多研究人员关注的关键点（参见 [第3.1节](#S3.SS1 "3.1 Background: Generalisation In Supervised
    Learning ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning ‣
    A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")）。在*可控*环境中，不同的评估协议使我们能够测试在
    [第3.1节](#S3.SS1 "3.1 Background: Generalisation In Supervised Learning ‣ 3 Formalising
    Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning") 中介绍的一些组合概括形式（？）：（1）系统性可以使用多维上下文集来评估，并对在训练时未见过的上下文维度的新组合进行测试（[图3](#S4.F3
    "In Controllable Environment Evaluation Protocols. ‣ 4.2 Evaluation Protocols
    For Zsg ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")中的组合插值）。
    （2）生产力可以通过序数或连续因素来评估，测量在具有超出训练时见过的上下文值的环境实例中表现良好的能力（[图3](#S4.F3 "In Controllable
    Environment Evaluation Protocols. ‣ 4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks
    For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning")中的任意类型外推）。如果处理上下文空间部分为语言的CMDPs，如语言条件化RL（？），那么上述讨论的评估（？）可以直接应用于语言空间。至关重要的是，需要一个具有结构化上下文空间的可控环境来测试这些组合概括形式，并确保代理在测试时看到真正的新组合；在如OpenAI
    Procgen（？）或NLE（？）这样的PCG环境中，这一点难以验证。'
- en: 'The other forms of compositional generalisation in (?) require additional structure
    not captured by the choices of evaluation protocol, and we describe what testing
    these forms could entail here: (3) substitutivity through the use of synonyms
    (in language) or equivalent objects and tools; (4) locality through comparing
    the interpretation of an agent given command A and command B separately vs. the
    combination of A + B and if those interpretations are different; and (5) overgeneralisation
    through how the agent responds to exceptions in language or rules of the environment.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其他形式的组合概括在（？）中需要额外的结构，而这些结构在选择的评估协议中并未被捕捉，我们在此描述了测试这些形式可能涉及的内容：（3）通过使用同义词（在语言中）或等效的对象和工具来进行的替代性；（4）通过比较在给定命令A和命令B分别与A
    + B的组合的解释，及这些解释是否不同来评估局部性；以及（5）通过代理对语言或环境规则中例外情况的反应来评估过度概括。
- en: What Generalisation Can We Expect?
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我们可以期待什么样的概括？
- en: 'In [Section 4.2](#S4.SS2 "4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For
    Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning") we discussed a variety of different possible
    evaluation protocols for different styles of ZSG benchmark. However, it is a different
    question in which protocols we can expect reasonable performance. For example,
    it is unreasonable to expect a standard RL policy trained *tabula rasa* on a single
    level of NLE (?) to generalise to dissimilar levels, as it might encounter entirely
    unseen entities or very out-of-distribution combinations of entities and level
    layouts. Each evaluation protocol measures a different kind of ZSG strength, and
    they hence form a kind of partial ordering where “easier” evaluation protocols
    come before “harder” protocols. We outline this ordering here:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4.2节](#S4.SS2 "4.2 Evaluation Protocols For Zsg ‣ 4 Benchmarks For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")中，我们讨论了各种可能的评估协议用于不同风格的 ZSG 基准。然而，哪种协议能期望合理的性能是另一个问题。例如，期望一个在
    NLE 单一层级上 *tabula rasa* 训练的标准 RL 策略能够对不相似的层级进行泛化是不现实的，因为它可能会遇到完全未见过的实体或非常不符合分布的实体和层级布局组合。每种评估协议测量的是不同类型的
    ZSG 强度，因此它们形成了一种部分排序，其中“更容易”的评估协议在“更难”的协议之前。我们在这里概述了这种排序：
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Increasing the number of samples can make an evaluation protocol easier, but
    often only to a point: more samples are unlikely to bring greater variety which
    is needed for zero-shot generalisation. Increasing the number of contexts (while
    keeping the shape of the context set the same) also makes an evaluation protocol
    easier. Even a small amount of additional variety can improve performance.'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增加样本数量可以使评估协议更简单，但通常只有到一定程度：更多样本不太可能带来更大的多样性，这对于零-shot 泛化是必要的。增加上下文数量（同时保持上下文集合的形状不变）也会使评估协议更简单。即使少量的额外多样性也能提高性能。
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The number of factors of variation which are extrapolating or combinatorially
    interpolating in the testing context set can also be varied. The more there are,
    the more difficult the evaluation protocol. Further, the width of the range of
    values that the extrapolating factors take at training time can vary. This is
    linked to the number of contexts but is also related to the variety available
    during training time along these axes of variation.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试上下文集合中变异因素的数量也可以变化。变异因素越多，评估协议就越困难。此外，训练时外推因素所取值的范围宽度也会有所不同。这与上下文数量有关，但也与这些变异轴在训练期间的可用多样性相关。
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Following ? (?), we consider the difficulty of interpolating and extrapolating
    along different types of factors of variation. Interpolation along ordinal axes
    is likely the easiest, followed by cardinal axes interpolation (which happens
    through unseen combinations of seen values for a cardinal axis combined with any
    other axis), and then extrapolation along ordinal axes. Finally, extrapolation
    along cardinal axes is the most difficult.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据 ? (? ) 的定义，我们考虑沿不同类型的变异因素进行插值和外推的难度。沿序数轴的插值可能是最简单的，其次是基数轴插值（通过未见过的基数轴值组合与任何其他轴的组合进行），然后是沿序数轴的外推。最后，沿基数轴的外推是最困难的。
- en: 'As the difficulty of an evaluation protocol increases, it becomes less likely
    that standard RL approaches will get good performance. In more difficult protocols
    which involve extrapolation of some form, zero-shot generalisation is unlikely
    to occur at all with standard RL methods, as there is no reason to expect a policy
    to generalise correctly to entirely unseen values. That does not mean that this
    type of generalisation is impossible: it just makes clear the fact that to achieve
    it, more than standard RL methods will be needed. That is, methods incorporating
    prior knowledge⁵⁵5[http://betr-rl.ml/2020/](http://betr-rl.ml/2020/) such as transfer
    from related environments (?); strong inductive biases (?) or assumptions about
    the variation; or utilising online adaptation (?, ?) will be necessary to produce
    policies that generalise in this way.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 随着评估协议难度的增加，标准 RL 方法取得良好性能的可能性减少。在更难的协议中，这些协议涉及某种形式的外推，标准 RL 方法几乎不可能实现零-shot
    泛化，因为没有理由期待策略能够正确泛化到完全未见过的值。这并不意味着这种泛化是不可能的：它只是明确表明，要实现它，超越标准 RL 方法的技术是必要的。也就是说，结合先验知识⁵⁵5[http://betr-rl.ml/2020/](http://betr-rl.ml/2020/)
    的方法，如来自相关环境的迁移 (?); 强 inductive 偏差 (?) 或对变异的假设; 或利用在线适应 (?, ?) 将是产生这种泛化策略所必需的。
- en: 5 Methods For Zero-shot Generalisation In Reinforcement Learning
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**零-shot 泛化的五种方法**在强化学习中。'
- en: We now classify methods that tackle ZSG in RL. The problem of ZSG occurs when
    the training and the testing context sets are not identical. There are many types
    of ZSG problems (as described in more detail in [Section 4](#S4 "4 Benchmarks
    For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning")), and hence there are many different
    styles of methods. We categorise the methods into those that try and increase
    the similarity between training and testing data and objective ([Section 5.1](#S5.SS1
    "5.1 Increasing Similarity Between Training And Testing ‣ 5 Methods For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")), those that explicitly aim to handle differences
    between training and testing environments ([Section 5.2](#S5.SS2 "5.2 Handling
    Differences Between Training And Testing ‣ 5 Methods For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")), and those that target RL-specific issues or optimisation improvements
    which aid ZSG performance ([Section 5.3](#S5.SS3 "5.3 RL-Specific Problems And
    Improvements ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将处理 ZSG 的方法进行分类。ZSG 问题出现在训练和测试上下文集不完全相同的情况。ZSG 问题有很多类型（如[第4节](#S4 "4 Benchmarks
    For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning")中详细描述），因此也有许多不同风格的方法。我们将这些方法分为以下几类：那些试图增加训练和测试数据及目标之间相似性的
    ([第5.1节](#S5.SS1 "5.1 Increasing Similarity Between Training And Testing ‣ 5 Methods
    For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning"))；那些明确旨在处理训练和测试环境之间差异的 ([第5.2节](#S5.SS2
    "5.2 Handling Differences Between Training And Testing ‣ 5 Methods For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"))；以及那些针对 RL 特定问题或优化改进以提升 ZSG 性能的 ([第5.3节](#S5.SS3
    "5.3 RL-Specific Problems And Improvements ‣ 5 Methods For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning"))。
- en: See [Fig. 4](#S5.F4 "In 5 Methods For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")
    for a diagram of this categorisation, and [Tables 2](#S5.T2 "In 5.4 Discussion
    ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning") and [3](#S5.T3 "Table
    3 ‣ 5.4 Discussion ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning") for a
    table classifying methods by their approach, the environment variation they were
    evaluated on, and whether they mostly change the environment, loss function or
    architecture. Performing this comprehensive classification enables us to see the
    under-explored areas within ZSG research, and we discuss future work for methods
    in [Section 6.6](#S6.SS6 "6.6 Future Work On Methods For Zero-shot Generalisation
    ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot Generalisation in Deep
    Reinforcement Learning").
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[图 4](#S5.F4 "在强化学习中零样本泛化的 5 种方法 ‣ 深度强化学习中零样本泛化调查")以查看这种分类的图示，以及[表 2](#S5.T2
    "在 5.4 讨论 ‣ 强化学习中零样本泛化的 5 种方法 ‣ 深度强化学习中零样本泛化调查")和[表 3](#S5.T3 "表 3 ‣ 5.4 讨论 ‣
    强化学习中零样本泛化的 5 种方法 ‣ 深度强化学习中零样本泛化调查")，这些表格对方法进行了分类，包括它们所采用的方法、评估环境的变化，以及它们主要改变环境、损失函数还是架构。进行这种全面分类使我们能够看到ZSG研究中未被充分探讨的领域，并在[第6.6节](#S6.SS6
    "6.6 零样本泛化方法的未来工作 ‣ 6 讨论与未来工作 ‣ 深度强化学习中零样本泛化调查")讨论了这些方法的未来工作。
- en: '![Refer to caption](img/ad2dcd1e6168c940251033fddd34905a.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad2dcd1e6168c940251033fddd34905a.png)'
- en: 'Figure 4: Categorisation of methods for tackling zero-shot generalisation in
    reinforcement learning'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：解决强化学习中零样本泛化问题的方法分类
- en: 5.1 Increasing Similarity Between Training And Testing
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 增加训练与测试之间的相似性
- en: All else being equal, the more similar the training and testing environments
    are, the smaller the generalisation gap and the higher the test time performance.
    This similarity can be increased by designing the training environment to be as
    close to the testing environment as possible. Assuming this has been done, in
    this section, we cover methods that make the data and objective being used to
    learn the policy during training closer to that which would be used if we were
    optimising on the testing environment.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他条件相同的情况下，训练环境和测试环境的相似性越高，泛化差距就越小，测试时间的表现就越好。这种相似性可以通过设计一个尽可能接近测试环境的训练环境来增加。假设已经做到这一点，本节将介绍使训练过程中使用的数据和目标更接近于优化测试环境时所使用的方法。
- en: 5.1.1 Data Augmentation and Domain Randomisation
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 数据增强和领域随机化
- en: Two natural ways to make training and testing data more similar are data augmentation
    (?) and domain randomisation (?, ?, ?). This is especially effective when the
    variation between the training and testing environments is known, as then a data
    augmentation or domain randomisation can be used which captures this variation.
    In practice there is only so far this approach can go, as stronger types of variation
    often cannot be captured by this simple method.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 使训练和测试数据更相似的两种自然方法是数据增强（？）和领域随机化（？，？，？）。当训练和测试环境之间的变化已知时，这尤其有效，因为可以使用捕获这些变化的数据增强或领域随机化。在实际操作中，这种方法的有效性有限，因为更强的变化类型往往无法通过这种简单方法捕捉到。
- en: Data augmentation (DA) can be viewed in two ways. First, the augmented data
    points are seen as additional data to train the model. This interpretation is
    what causes us to classify DA techniques as trying to increase the similarity
    between training and testing data. In the second view, DA can be used to enforce
    the learning of an invariance, by regularising the model to have the same output
    (or the same internal representations) for different augmented data points. In
    this view, DA is more with encoding inductive biases, which we cover in [Section 5.2.1](#S5.SS2.SSS1
    "5.2.1 Encoding Inductive Biases ‣ 5.2 Handling Differences Between Training And
    Testing ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning ‣ A
    Survey of Zero-shot Generalisation in Deep Reinforcement Learning"). We include
    all DA work in this section for clarity.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强（DA）可以从两个角度来看待。首先，增强的数据点被视为训练模型的额外数据。这种解释导致我们将DA技术分类为尝试增加训练和测试数据之间的相似性。在第二种观点中，DA可以用来强制学习不变性，通过对模型进行正则化，使其对不同增强的数据点有相同的输出（或相同的内部表示）。在这种观点中，DA更侧重于编码归纳偏差，我们在[第5.2.1节](#S5.SS2.SSS1
    "5.2.1 编码归纳偏差 ‣ 5.2 处理训练和测试之间的差异 ‣ 强化学习中的零样本泛化方法 ‣ 深度强化学习中零样本泛化的调查") 中对此进行了讨论。为了清晰起见，我们在本节中包含所有DA的工作。
- en: There are many examples of using DA in RL, although not all of them are targeted
    at ZSG performance. ? (?, UCB-DrAC) adapts the DA technique DrQ (?) to an actor-critic
    setting (?, PPO) and introduces a method for automatically picking the best augmentation
    during training. ? (?, Mixreg) adapts mixup (?) to the RL setting, which encourages
    the policy to be linear in its outputs with respect to mixtures of possible inputs. ? (?,
    PAADA) use adversarial DA combined with mixup. ? (?, RandFM) uses a randomised
    convolutional layer at the start of the network to improve robustness to a wide
    variety of visual inputs. ? (?, MixStyle) mixes style statistics across spatial
    dimensions in CNNs for increased data diversity. All these methods (?, ?, ?, ?, ?)
    show improved performance on CoinRun (?) or OpenAI Procgen (?) by improving both
    training and testing performance, and some also show gains on other benchmarks
    such as visually distracting DeepMind Control (DMC) variants. ? (?, SODA) uses
    similar augmentations as before but only to learn a more robust image encoder,
    while the policy is trained on non-augmented data, demonstrating good performance
    on DMC-GB (?). ? (?, RCAN) use DA to learn a visual mapping from any different
    observation back to a canonical observation of the same state, and then train
    a policy on this canonical observation. They show improved sim-to-real performance
    on a robotic grasping task.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中使用数据增强（DA）的例子很多，尽管并非所有例子都针对零样本泛化（ZSG）性能。? (?, UCB-DrAC) 将DA技术DrQ (?)
    适配到演员-评论家设置（?, PPO），并引入了一种在训练过程中自动选择最佳增强的方法。? (?, Mixreg) 将mixup (?) 适配到RL设置中，这鼓励策略在可能输入的混合上保持线性。? (?,
    PAADA) 结合对抗性DA与mixup。? (?, RandFM) 在网络开始处使用随机卷积层以提高对各种视觉输入的鲁棒性。? (?, MixStyle)
    在卷积神经网络（CNN）中跨空间维度混合风格统计量，以增加数据多样性。所有这些方法 (?, ?, ?, ?, ?) 通过改善训练和测试性能，在CoinRun
    (?) 或OpenAI Procgen (?) 上显示了改进的性能，某些方法还在其他基准上显示出提升，如视觉干扰的DeepMind Control (DMC)
    变体。? (?, SODA) 使用类似的增强方法，但仅用于学习更鲁棒的图像编码器，而策略在未增强的数据上进行训练，展示了在DMC-GB (?) 上的良好表现。? (?,
    RCAN) 使用DA学习从不同观察中映射回同一状态的标准观察，然后在该标准观察上训练策略。他们在机器人抓取任务中展示了改进的模拟到现实性能。
- en: '? (?, InDA,ExDA) show that the time at which the augmentations are applied
    is important for the performance: some augmentations help during training, whereas
    others only need to be applied to regularise the final policy. ? (?, SECANT) introduce
    a method for combining DA with policy distillation. As training on strong augmentations
    can hinder performance, they first train on weak augmentations to get an expert
    policy, which they then distil into a student policy trained with strong augmentations.
    ? (?, SVEA) argues that DA when naively applied increases the variance of Q-value
    targets, making learning less stable and efficient. They introduce adjustments
    to the standard data-augmentation protocol by only applying augmentations to specific
    components at specific points during the calculation of the loss function, evaluating
    performance on DMC-GB (?). These works show that in RL settings the choice of
    when to apply augmentations and what type of augmentations to apply is non-trivial,
    as the performance of the model during training impacts the final performance
    by changing the data the policy learns from.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ?（？，InDA,ExDA）显示了增强应用的时间对于性能的重要性：一些增强在训练过程中有帮助，而其他增强只需应用于正则化最终策略。?（？，SECANT）引入了一种将数据增强（DA）与策略蒸馏相结合的方法。由于在强增强上训练可能会阻碍性能，他们首先在弱增强上训练以获得专家策略，然后将其蒸馏为在强增强下训练的学生策略。?（？，SVEA）认为，当数据增强被天真地应用时，会增加Q值目标的方差，使得学习不稳定和低效。他们通过仅在损失函数计算的特定点对特定组件应用增强，对标准数据增强协议进行调整，在DMC-GB（？）上评估性能。这些工作表明，在强化学习设置中，应用增强的时间选择和增强类型的选择并非
    trivial，因为训练过程中的模型性能会通过改变策略学习的数据影响最终性能。
- en: 'Domain Randomisation (DR) is the practice of randomising the environment across
    a distribution of parametrisations, aiming for the testing environment to be covered
    by the distribution of environments trained on. Fundamentally DR is just the creation
    of a non-singleton training context set, and then randomly sampling from this
    set. ? (?), ? (?) and ? (?) introduced this idea in the setting of sim-to-real
    transfer in robotics. Much work has been done on different types of DR, so we
    cover just a sample here. ? (?) described Automatic Domain Randomisation: instead
    of sampling possible environment parametrisations uniformly at random, this method
    dynamically adjusts the distribution in response to the agent’s current performance. ? (?,
    Minimax DSAC) use adversarial training to learn the DR for improved robustness. ? (?,
    P2PDRL) improve DR though peer-to-peer distillation. ? (?, DDL) learns a world
    model in which to train a policy and then applies dropout to the recurrent network
    within the world model, effectively performing DR in imagination. Finally, as
    procedural content generation (?) is a method for generating a non-zero context
    set, it can be seen as a form of DR. Works on DR generally leverage the possibility
    of using a non-uniform context distribution, which possibly varies during training.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 域随机化（DR）是将环境在参数化分布上随机化的实践，旨在使测试环境被训练环境的分布覆盖。从根本上讲，DR 只是创建一个非单一训练上下文集，然后从该集中随机抽样。?（？）、?（？）和?（？）在机器人学中的模拟到现实转移设置中引入了这个概念。关于不同类型的
    DR 已经进行了大量工作，这里仅覆盖一些样本。?（？）描述了自动化域随机化：该方法不是均匀随机地采样可能的环境参数化，而是根据代理的当前表现动态调整分布。?（？，Minimax
    DSAC）使用对抗训练来学习 DR 以提高鲁棒性。?（？，P2PDRL）通过点对点蒸馏改进 DR。?（？，DDL）在其中训练策略的世界模型中学习，并对世界模型中的递归网络应用
    dropout，从而在想象中有效地执行 DR。最后，程序化内容生成（？）是一种生成非零上下文集的方法，可以看作是一种 DR 形式。DR 的工作通常利用使用非均匀上下文分布的可能性，该分布在训练过程中可能会有所变化。
- en: In both DA and DR approaches, as the training environment instances are increasingly
    augmented or randomised, optimisation becomes increasingly difficult, which often
    makes these methods much less sample-efficient. This is the motivation behind
    ?’s (?, ?, ?) work and other works, all of which train an RL policy on a non-randomised
    or only weakly randomised environment while using other techniques such as supervised
    or self-supervised learning to train a robust visual encoder.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DA 和 DR 方法中，随着训练环境实例的不断增强或随机化，优化变得越来越困难，这通常使得这些方法的样本效率大大降低。这是？（？，？，？）工作的动机和其他工作的动机，所有这些工作都在非随机化或仅弱随机化的环境中训练
    RL 策略，同时使用其他技术，如监督学习或自监督学习来训练一个鲁棒的视觉编码器。
- en: Finally, note that most of the DA techniques are focused on visual variation
    in the context set, as that is the easiest variation to produce useful augmentations
    for. Some DR work focuses on dynamics variation as a way of tackling the sim-to-real
    problem, where it is assumed that the dynamics will change between training (simulation)
    and testing (reality).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，大多数 DA 技术关注于上下文集中的视觉变化，因为这是最容易产生有用增强的变化。一些 DR 工作则关注于动态变化，作为解决模拟到现实问题的一种方式，其中假设动态会在训练（模拟）和测试（现实）之间发生变化。
- en: 5.1.2 Environment Generation
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 环境生成
- en: 'While DR and PCG produce context-MDPs within a pre-determined context set,
    it is normally assumed that all the context-MDPs are solvable. However, in some
    settings, it is unknown how to sample from the set of all *solvable* context-MDPs.
    For example, consider a simple gridworld maze environment, where the context set
    consists of all possible block layouts on the grid; some configuration of block
    placements will result in unsolvable mazes. Further, many block configurations
    are not useful for training: they may produce trivially easy mazes. To solve these
    problems we can *learn* to generate new levels (sample new contexts) on which
    to train the agent, such that we can be sure these context-MDPs are solvable and
    useful training instances. We want a distribution over context-MDPs which is closer
    to the testing context set, which likely has only solvable context-MDPs. This
    is known as environment generation.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DR 和 PCG 在预定的上下文集内生成上下文-MDP，但通常假设所有的上下文-MDP 都是可解的。然而，在某些设置中，不知道如何从所有*可解*上下文-MDP
    的集合中进行采样。例如，考虑一个简单的网格世界迷宫环境，其中上下文集包含网格上所有可能的块布局；某些块放置的配置将导致不可解的迷宫。此外，许多块配置对训练并没有用处：它们可能生成过于简单的迷宫。为了解决这些问题，我们可以*学习*生成新的关卡（采样新的上下文）来训练智能体，以确保这些上下文-MDP
    是可解的且有用的训练实例。我们希望得到一个上下文-MDP 的分布，这个分布更接近测试上下文集，该集可能只有可解的上下文-MDP。这被称为环境生成。
- en: ? (?) introduced Paired Open-Ended Trailblazer (POET), a method for jointly
    evolving context-MDPs and policies which solve those MDPs, aiming for a policy
    that can solve a wide variety of context-MDPs. They produce policies that solve
    unseen level instances reliably and perform better than training from scratch
    or a naive curriculum. ? (?) built on POET, introducing improvements to the open-ended
    algorithm including a measure of how novel generated context-MDPs are and a generic
    measure of how much a system exhibits open-ended innovation. These additions improve
    the diversity and complexity of context-MDPs generated.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 引入了配对开放式先驱（POET），这是一种联合演化上下文-MDP 和解决这些 MDP 的策略的方法，旨在获得一种能够解决各种上下文-MDP
    的策略。它们生成的策略能可靠地解决未见的关卡实例，并且表现优于从零开始训练或朴素的课程方法。? (?) 基于 POET，提出了对开放式算法的改进，包括如何衡量生成的上下文-MDP
    的新颖性和系统展现开放式创新的通用度量。这些新增内容提高了生成的上下文-MDP 的多样性和复杂性。
- en: ? (?) introduces the framework of Unsupervised Environment Design (UED), similar
    to POET, in which the task is to generate context-MDPs in an unsupervised manner,
    which are then used to train a policy. The aim is to improve ZSG to unseen tasks
    either within or outside the environment’s context-MDP space. Their method PAIRED
    outperforms standard DR and a method analogous to POET in the grid-world setting
    described above, as measured by the zero-shot generalisation performance to unseen
    levels. ? (?) extended the formal framework of UED, combining it with Prioritized
    Level Replay (?, PLR), and motivates understanding PLR as an environment generation
    algorithm. This combined method shows improved performance in terms of zero-shot
    generalisation to unseen out-of-distribution tasks in both gridworld mazes and
    2D car-racing tracks. We summarise PLR in the next section.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 引入了无监督环境设计（UED）的框架，类似于 POET，其中的任务是以无监督的方式生成上下文-MDP，然后用这些 MDP 来训练策略。目标是提高
    ZSG 对未见任务的适应能力，无论是在环境的上下文-MDP 空间内还是外。它们的方法 PAIRED 在前述的网格世界设置中优于标准的 DR 和类似于 POET
    的方法，依据是对未见关卡的零-shot 泛化性能。? (?) 扩展了 UED 的正式框架，将其与优先级关卡重放（?，PLR）结合，并鼓励将 PLR 理解为一种环境生成算法。这种组合方法在网格世界迷宫和
    2D 赛车赛道上都显示出了在零-shot 泛化到未见的分布外任务方面的改进。我们将在下一节总结 PLR。
- en: Environment generation and DR are both methods for adjusting the context distribution
    over some context set provided by the environment. Environment generation tends
    to learn this sampling procedure, and is targeted at environments where the context
    set is unstructured such that not all context-MDPs are solvable or useful for
    training, whereas DR work often uses hard-coded heuristics or non-parametric learning
    approaches to adjust the context distribution, and focuses on settings where the
    domains are all solvable but possibly have different difficulties or learning
    potentials. Both can often also be seen as a form of automatic curriculum learning
    (?), especially if the context distribution is changing during training and adapting
    to the agent’s performance.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 环境生成和DR都是调整环境提供的某些上下文集的上下文分布的方法。环境生成倾向于学习这种采样过程，并且目标是处理上下文集结构不规则的环境，以便并非所有上下文-MDPs
    都能解决或对训练有用，而DR工作通常使用硬编码的启发式或非参数学习方法来调整上下文分布，并专注于所有领域都能解决但可能有不同难度或学习潜力的设置。这两者也常常可以被视为自动课程学习的形式（?），特别是当上下文分布在训练过程中发生变化并适应代理的性能时。
- en: This area is very new and we expect there to be more research soon. However,
    it does require access to an environment where context-MDPs can be generated at
    a fairly fine level of detail. Environment generation methods can target ZSG over
    any kind of variation, as long as that kind of variation is present in the output
    space of the context-MDP generator. Current methods focus on state-space variation,
    as that is the most intuitive to formulate as a context set within which the generator
    can produce specific MDPs.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这一领域非常新，我们预计很快会有更多的研究。然而，它确实需要能够生成相当详细的上下文-MDPs 的环境。环境生成方法可以针对任何类型的变化，只要这种变化存在于上下文-MDP生成器的输出空间中。当前的方法专注于状态空间的变化，因为这是最直观的，能够被构造为生成器可以产生特定MDPs的上下文集。
- en: 5.1.3 Optimisation Objectives
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 优化目标
- en: It is sometimes possible to change our optimisation objective (explicitly or
    implicitly) to one which better aligns with testing performance.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 有时可以显式或隐式地改变我们的优化目标，使其更好地与测试性能对齐。
- en: Changing the distribution over which the training objective is calculated can
    be seen as implicitly changing the optimisation objective. An initial example
    in this area applied to improving ZSG is PLR (?), in which the sampling distribution
    over levels is changed to increase the learning efficiency and zero-shot generalisation
    of the trained policy. They show both increased training and testing performance
    on OpenAI Procgen, and the method effectively forms a rough curriculum over levels,
    enabling more sample-efficient learning while also ensuring that no context-MDP’s
    performance is too low.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 改变训练目标计算所用的分布可以看作是隐式地改变优化目标。这个领域的一个初步例子应用于提升ZSG的是PLR（?），其中改变了不同级别上的采样分布，以提高训练策略的学习效率和零样本泛化能力。他们展示了在OpenAI
    Procgen上的训练和测试性能的提升，并且该方法有效地形成了一个粗略的课程，使学习更加样本高效，同时确保没有上下文-MDP的性能过低。
- en: Methods for Robust RL (RRL) are also targeted at the ZSG problem, and work by
    changing the optimisation objective of the RL problem. These methods take a worst-case
    optimisation approach, maximising the minimum performance over a set of possible
    environment perturbations (which can be understood as different context-MDPs),
    and are focused on improving ZSG to unseen dynamics. ? (?) gave an overview and
    introduction to the field. ? (?, WR²L) optimised the worst-case performance using
    a Wasserstein ball around the transition function to define the perturbation set.
    ? (?, SRE-MPO) incorporates RLL into MPO (?) and shows improved performance on
    the RWRL benchmark (?). ? (?, RARL) also (implicitly) optimises a robust RL objective
    through the use of an adversary which is trained to pick the worst perturbations
    to the transition function. This can also be seen as an adversarial domain randomisation
    technique.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 强健的强化学习（RRL）方法也针对ZSG问题，通过改变强化学习问题的优化目标来工作。这些方法采用最坏情况优化方法，最大化在一组可能的环境扰动（可以理解为不同的上下文-MDPs）中的最小性能，并专注于提升对未知动态的ZSG。?
    (?) 对该领域进行了概述和介绍。? (?, WR²L) 通过在转移函数周围使用Wasserstein球体来定义扰动集，从而优化最坏情况性能。? (?, SRE-MPO)
    将RLL整合到MPO (?) 中，并在RWRL基准测试 (?) 上显示了改进的性能。? (?, RARL) 也通过使用一个对抗者（其被训练来选择对转移函数最坏的扰动）来（隐式地）优化一个强健的强化学习目标。这也可以看作是一种对抗性领域随机化技术。
- en: 5.2 Handling Differences Between Training And Testing
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 处理训练和测试之间的差异
- en: 'One way of conceptualising why policies do not transfer perfectly at test time
    is due to the differences between the two environments: the trained model will
    learn to rely on features during training that change in the testing environment
    and performance then suffers. In this section, we review methods that try and
    explicitly handle the possible differences between the features of the training
    and testing environments.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一种概念化的方式来解释为什么策略在测试时无法完美转移是由于两个环境之间的差异：训练中的模型会学习依赖于在测试环境中发生变化的特征，从而导致性能下降。在本节中，我们回顾了尝试显式处理训练和测试环境特征可能差异的方法。
- en: 5.2.1 Encoding Inductive Biases
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 编码归纳偏差
- en: 'If we know how features change between the training and testing context-MDPs,
    we can use inductive biases to encourage or ensure the model does not rely on
    features that we expect to change: The policy should only rely on features which
    will behave similarly in both the training and testing environments. For example,
    if we know colour varies between training and testing, and colour is irrelevant
    to the task, then we can remove colour from the visual input before processing.
    Simple changes like this tend not to be worthy of separate papers, but they are
    still important to consider in real-world problem scenarios.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道特征在训练和测试环境-MDPs 之间的变化情况，我们可以利用归纳偏差来鼓励或确保模型不依赖于我们期望会发生变化的特征：策略应该只依赖于在训练和测试环境中表现类似的特征。例如，如果我们知道颜色在训练和测试之间有所变化，并且颜色与任务无关，那么我们可以在处理之前从视觉输入中去除颜色。像这样的简单改变往往不会成为单独的论文，但在现实世界问题情境中仍然值得考虑。
- en: IDAAC (?) adds an adversarial regularisation term which encourages the internal
    representations of the policy not to be predictive of time within an episode.
    This invariance is useful for OpenAI Procgen (?), as timestep is irrelevant for
    the optimal policy but could be used to overfit to the training set of levels. ? (?,
    DARLA) uses $\beta$-VAEs (?) to encode the inductive bias of disentanglement into
    the representations of the policy, improving zero-shot performance on various
    visual variations. ? (?, NAP) incorporates a black-box shortest-path solver to
    improve ZSG performance in hard navigation problems. ? (?, ?) incorporate a relational
    inductive bias into the model architecture which aids in generalising along ordinal
    axes of variation, including extrapolation performance. ? (?, SchemaNetworks)
    use an object-oriented and entity-focused architecture, combined with structure-learning
    methods, to learn logical schema which can be used for backwards-chaining-based
    planning. These schemas generalise zero-shot to novel state spaces as long as
    the dynamics are consistent. ? (?, VAI) use unsupervised visual attention and
    keypoint detection methods to enforce a visual encoder to only encode information
    relevant to the foreground of the visual image, encoding the inductive bias that
    the foreground is the only part of the visual input that is important.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: IDAAC (?) 添加了一个对抗正则化项，鼓励策略的内部表示不要对一个回合中的时间进行预测。这种不变性对 OpenAI Procgen (?) 很有用，因为时间步对最优策略来说无关紧要，但可能会被用来过拟合到训练集的关卡。?
    (?, DARLA) 使用 $\beta$-VAEs (?) 将解耦的归纳偏差编码到策略的表示中，改善了对各种视觉变化的零样本性能。? (?, NAP) 引入了一个黑箱最短路径求解器，以提高在困难导航问题中的
    ZSG 性能。? (?, ?) 将关系归纳偏差融入模型架构中，有助于在序数变化轴上进行泛化，包括外推性能。? (?, SchemaNetworks) 使用面向对象和实体聚焦的架构，结合结构学习方法，学习可以用于基于反向链的规划的逻辑模式。这些模式可以对新的状态空间进行零样本泛化，只要动态是一致的。?
    (?, VAI) 使用无监督视觉注意力和关键点检测方法，强制视觉编码器仅编码与视觉图像前景相关的信息，编码前景是视觉输入中唯一重要部分的归纳偏差。
- en: '? (?) introduce AttentionAgent, which uses neuroevolution to optimise an architecture
    with a hard attention bottleneck, resulting in a network that only receives a
    fraction of the visual input. The key inductive bias here is that selective attention
    is beneficial for optimisation and ZSG. Their method generalises zero-shot to
    unseen backgrounds in CarRacing (?) and VizDoom (?, ?). ? (?, SensoryNeuron) build
    on AttentionAgent, adding an inductive bias of permutation invariance in the input
    space. They argue this is useful for improving ZSG, for a similar reason as before:
    the attention mechanism used for the permutation-invariant architecture encourages
    the agent to ignore parts of the input space that are irrelevant. ? (?, CRAR)
    use a modular architecture combining dynamics learning and value estimation both
    in a low-dimensional latent space, and show improved performance on a simple maze
    task.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 介绍了 AttentionAgent，它利用神经进化优化一个具有硬性注意力瓶颈的架构，从而得到一个只接收部分视觉输入的网络。这里的关键归纳偏差是选择性注意力对于优化和
    ZSG 有益。他们的方法在 CarRacing (?) 和 VizDoom (?, ?) 中将零-shot 泛化到未见过的背景。? (?, SensoryNeuron)
    基于 AttentionAgent，增加了输入空间中置换不变性的归纳偏差。他们认为这对于改善 ZSG 有用，原因与之前类似：用于置换不变架构的注意力机制鼓励代理忽略输入空间中不相关的部分。? (?,
    CRAR) 使用模块化架构，将动力学习和价值估计结合在低维潜在空间中，并在简单迷宫任务上显示出改进的性能。
- en: ? (?, SHIFTT) and ? (?, TransferLanfLfP) both use large pretrained models (?, ?)
    to encode natural language instructions for instruction following tasks, tackling
    reward function variation. They both show improved performance to novel instructions,
    leveraging the generalisation power of the large pretrained model. This can be
    seen as utilising domain knowledge to improve zero-shot generalisation to novel
    goal specifications by incorporating the inductive bias that all the goal specifications
    will be natural language.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?, SHIFTT) 和 ? (?, TransferLanfLfP) 都使用大型预训练模型 (?, ?) 来编码自然语言指令，以处理奖励函数的变化。他们都展示了对新指令的改进性能，利用了大型预训练模型的泛化能力。这可以视为利用领域知识来提高对新目标规范的零-shot
    泛化，通过结合所有目标规范都将是自然语言的归纳偏差。
- en: While methods in this area appear dissimilar, they all share the motivation
    of incorporating specific inductive biases into the RL algorithm. There are several
    ways of incorporating domain knowledge as an inductive bias. The architecture
    of the model can be changed to process the variation correctly. If the variation
    is one to which the policy should be invariant, it can either be removed entirely,
    or adversarial regularisation can be used to ensure the policy’s representations
    are invariant. More broadly, regularisation or auxiliary losses that encourage
    the policy to handle this variation correctly can be used. A recommendation to
    make this body of work more systematic is to use the additional types of structural
    assumptions discussed in [Section 3.6](#S3.SS6 "3.6 Additional Assumptions For
    More Feasible Generalisation ‣ 3 Formalising Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")
    as a starting point for developing algorithms that leverage those assumptions
    – many of the works discussed here rely on assumptions that can be classified
    in those introduced in [Section 3.6](#S3.SS6 "3.6 Additional Assumptions For More
    Feasible Generalisation ‣ 3 Formalising Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning").
    For a deeper discussion of inductive biases see ? (?), and for the original ideas
    surrounding inductive biases and No Free Lunch theorems see (?).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这一领域的方法看似不同，但它们都共享将特定归纳偏差融入 RL 算法的动机。将领域知识作为归纳偏差的方式有多种。可以改变模型的架构以正确处理变化。如果变化是策略应当不变的，可以完全移除它，或者使用对抗正则化来确保策略的表示是不可变的。更广泛地说，可以使用正则化或辅助损失，鼓励策略正确处理这些变化。使这一领域的工作更加系统化的建议是使用
    [第 3.6 节](#S3.SS6 "3.6 Additional Assumptions For More Feasible Generalisation
    ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning") 讨论的额外结构假设作为开发利用这些假设的算法的起点——许多在这里讨论的工作依赖于这些在
    [第 3.6 节](#S3.SS6 "3.6 Additional Assumptions For More Feasible Generalisation
    ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning") 中介绍的假设。有关归纳偏差的更深入讨论请参见
    ? (?), 关于归纳偏差和 No Free Lunch 定理的原始思想请参见 (?).
- en: 5.2.2 Regularisation and Simplicity
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 正则化与简洁性
- en: 'When we cannot encode a specific inductive bias, standard regularisation can
    be used. This is generally motivated by a paraphrased Occam’s razor: the simplest
    model will generalise the best. Task-agnostic regularisation encourages simpler
    models that rely on fewer features or less complex combinations of features. For
    example, L2 weight decay biases the network towards less complex features, dropout
    ensures the network cannot rely on specific combinations of features, and the
    information bottleneck ensures that only the most informative features are used.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不能编码特定的归纳偏差时，可以使用标准正则化。这通常由一种改写的奥卡姆剃刀原理驱动：最简单的模型将有最佳的泛化能力。任务无关的正则化鼓励依赖较少特征或特征组合较简单的模型。例如，L2
    权重衰减使网络偏向于使用较少复杂的特征，dropout 确保网络不能依赖特定的特征组合，而信息瓶颈确保只使用最有信息量的特征。
- en: ? (?) introduced CoinRun, and evaluated standard supervised learning regularisation
    techniques on the benchmark. They investigate data augmentation (a modified form
    of Cutout (?)), dropout, batch norm, L2 weight decay, policy entropy, and a combination
    of all techniques. All the techniques separately improve performance, and the
    combination improves performance further, although the gains of the combination
    is minimal over the individual methods, implying that many of these methods address
    similar causes of worse generalisation. Early stopping can be seen as a form of
    regularisation, and ? (?) shows that considering training iteration as a hyperparameter
    (effectively a form of early stopping) improves performance on some benchmarks.
    Almost all methods report performance at the end of training, as often early stopping
    would not be beneficial (as can be seen from the training and testing reward curves),
    but this is likely an attribute of the specific benchmarks being used, and in
    the future, we should keep early stopping in mind.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 引入了 CoinRun，并在基准测试中评估了标准的监督学习正则化技术。他们研究了数据增强（Cutout (?) 的一种改进形式）、dropout、批量归一化、L2
    权重衰减、策略熵以及所有技术的组合。所有技术单独使用时均能提高性能，而组合使用时性能进一步提升，尽管组合的增益相对于单独方法而言较小，这表明这些方法中的许多都解决了类似的泛化较差的原因。早停可以视为一种正则化形式，而
    ? (?) 显示将训练迭代次数视为超参数（实际上是一种早停形式）可以改善某些基准测试上的性能。几乎所有方法都在训练结束时报告性能，因为早停通常不会带来好处（如从训练和测试奖励曲线中可以看出），但这可能是所使用特定基准测试的特性，未来我们应当考虑早停。
- en: Several methods utilise information-theoretic regularisation techniques, building
    on the information bottleneck (?). ? (?, IBAC-SNI) and ? (?, IB-annealing) concurrently
    introduce methods that rely on the information bottleneck, along with other techniques
    to improve performance, demonstrating improved performance on OpenAI Procgen,
    and a variety of random mazes and continuous control tasks, respectively. ? (?,
    RPC) extend the motivation of the information bottleneck to the RL setting specifically,
    learning a dynamics model and policy which jointly minimises the information taken
    from the MDP by using information from previous states to predict future states.
    This results in policies using much less information than previously, which has
    benefits for robustness and zero-shot generalisation, although this method was
    not compared on standard ZSG benchmarks to other methods. ? (?, SMIRL) use surprise
    minimisation to improve the performance of the trained policy, although more rigorous
    benchmarking is needed to know whether this method has a positive effect.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 几种方法利用了信息论正则化技术，基于信息瓶颈（?）。? (?, IBAC-SNI) 和 ? (?, IB-annealing) 同时引入了依赖信息瓶颈的方法，以及其他技术以提升性能，在
    OpenAI Procgen 和各种随机迷宫及连续控制任务中分别展示了改进的性能。? (?, RPC) 将信息瓶颈的动机扩展到 RL 环境中，学习一个动态模型和策略，这两者共同最小化从
    MDP 中获取的信息，通过使用来自前一个状态的信息来预测未来状态。这导致策略使用的信息量大大减少，这对鲁棒性和零-shot 泛化有好处，尽管该方法未在标准
    ZSG 基准测试中与其他方法进行比较。? (?, SMIRL) 使用惊讶最小化来提高训练策略的性能，尽管需要更严格的基准测试来确认该方法是否有积极效果。
- en: '? (?) show that larger model sizes can lead to *implicit regularisation*: larger
    models, especially those with residual connections, generalise better, even when
    trained on the same number of training steps. This is also shown in (?, ?).'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 表明较大的模型尺寸可以导致*隐式正则化*：较大的模型，特别是那些具有残差连接的模型，即使在相同数量的训练步骤下，也能更好地泛化。这一点在 (?, ?)
    中也有展示。
- en: 5.2.3 Learning Invariances
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 学习不变性
- en: 'Sometimes we cannot rely on a specific inductive bias or standard regularisation.
    This is a very challenging setting for RL (and machine learning in general) to
    tackle, as there is a fundamental limit to performance due to a kind of “no free
    lunch” analogy: we cannot expect a policy to generalise to arbitrary contexts.
    However, several techniques can help, centred around the idea of using multiple
    training contexts to *learn* the invariances necessary to generalise to the testing
    contexts. If the factors of variation within the training contexts are the same
    as the factors of variation between the training and testing contexts, and the
    values that those factors take in testing are not far from those in training,
    then you can use that to learn these factors of variation and how to adapt or
    be invariant to them.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们不能依赖特定的归纳偏差或标准正则化。这对强化学习（以及一般机器学习）来说是一个非常具有挑战性的环境，因为由于一种“没有免费午餐”的类比，性能存在根本限制：我们不能指望一个策略能够泛化到任意环境。然而，几个技术可以提供帮助，这些技术围绕使用多个训练环境来*学习*泛化到测试环境所需的不变性。如果训练环境中的变化因素与训练和测试环境之间的变化因素相同，并且这些因素在测试中的值与训练中的值相差不远，那么你可以利用这一点来学习这些变化因素以及如何适应或对其保持不变。
- en: Much work draws on causal inference to learn invariant features from several
    training contexts. ? (?, ICP) assume a block MDP structure (?) and leverage that
    assumption to learn a state abstraction that is invariant to irrelevant features,
    which aids in generalisation performance. ? (?, DBC) use bisimulation metrics
    to learn a representation that is invariant to irrelevant visual features, and
    show that bisimulation metrics are linked to causal inference. ? (?, DBC-normed-IR-ID)
    improve DBC with a norm on the representation space and the use of intrinsic rewards
    and regularisation. ? (?, PSM) suggest limitations of bisimulation metrics and
    instead propose a policy similarity metric, where states are similar if the optimal
    policy has similar behaviour in that and future states. They use this metric,
    combined with a contrastive learning approach, to learn policies invariant to
    observation variation.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工作利用因果推断从多个训练环境中学习不变特征。？（？，ICP）假设了一个块 MDP 结构（？），并利用这一假设学习对无关特征不变的状态抽象，这有助于提高泛化性能。？（？，DBC）使用双模拟度量来学习对无关视觉特征不变的表示，并显示出双模拟度量与因果推断相关。？（？，DBC-normed-IR-ID）在表示空间上改进
    DBC，并使用内在奖励和正则化。？（？，PSM）提出了双模拟度量的局限性，而是提出了一种策略相似度度量，其中状态相似的条件是最佳策略在这些状态和未来状态中表现相似。他们结合对比学习方法，使用这一度量来学习对观测变化不变的策略。
- en: Several approaches use multiple contexts to learn an invariant representation,
    which is then assumed to generalise well to testing contexts. ? (?, IPO) apply
    ideas from Invariant Risk Minimization (?) to policy optimisation, learning a
    representation which enables jointly optimal action prediction across all domains,
    and show improved performance over PPO on several visual and dynamics variation
    environments. ? (?, IAPE) introduce the *Instance MDP*, an alternative formalism
    for the ZSG problem, and then motivate theoretically an approach to learn a collection
    of policies on subsets of the training domains, such that the aggregate policy
    is invariant to any individual context-specific features which would not generalise.
    They show improved performance on the CoinRun benchmark (?) compared to standard
    regularisation techniques. ? (?, LEEP) also produce an invariant policy across
    a collection of subsets of the training contexts but motivate this with Bayesian
    RL. Their approach learns separate policies on each subset and then optimistically
    aggregates them at test time, as opposed to IAPE which uses the aggregate policy
    during training for data collection and learns the collection of policies off-policy.
    While these approaches are similar there has been no direct comparison between
    them.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法使用多个上下文来学习不变的表示，然后假定这种表示在测试上下文中表现良好。? (?,?, IPO) 将不变风险最小化 (?) 的思想应用于策略优化，学习一种表示，使得在所有领域中能进行联合最优行动预测，并在多个视觉和动态变化环境中比
    PPO 展现了更好的性能。? (?,?, IAPE) 引入了 *实例 MDP*，一种 ZSG 问题的替代形式，然后从理论上动机出一种方法，在训练领域的子集上学习一组策略，使得综合策略对任何不具泛化能力的上下文特征不变。他们在
    CoinRun 基准测试 (?) 中表现出比标准正则化技术更好的性能。? (?,?, LEEP) 也在训练上下文的子集集合中产生了一个不变的策略，但使用贝叶斯强化学习来动机他们的方法。他们的方法在每个子集上学习单独的策略，然后在测试时乐观地将它们汇总，而
    IAPE 在训练期间使用汇总策略进行数据收集，并离策略地学习策略集合。尽管这些方法相似，但尚未对它们进行直接比较。
- en: Other approaches try to learn behaviourally similar representations with less
    theoretical motivation. ? (?, CSSC) use behavioural similarity (similarity of
    short future action sequences) to find positive and negative pairs for a contrastive
    learning objective. This auxiliary loss aids in zero-shot generalisation and sample
    efficiency in several OpenAI Procgen games. ? (?, CTRL) uses clustering methods
    and self-supervised learning to define an auxiliary task for representation learning
    based on behavioural similarity, showing improved performance on OpenAI Procgen.
    ? (?, DARL) uses adversarial learning to enforce the representations of different
    domains to be indistinguishable, improving performance in visually diverse testing
    contexts, even when only trained on simple training contexts. ? (?, DRIBO) uses
    contrastive learning combined with an information-theoretic objective to learn
    representations that only contain task-relevant information while being predictive
    of the future. They show improved performance on both visually diverse domains
    in DeepMind Control, and in OpenAI Procgen.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法尝试以行为相似的表示来学习，理论动机较少。? (?,?, CSSC) 利用行为相似性（短期未来行动序列的相似性）来找到正负样本对用于对比学习目标。这种辅助损失在多个
    OpenAI Procgen 游戏中有助于零样本泛化和样本效率。? (?,?, CTRL) 使用聚类方法和自监督学习定义了一个基于行为相似性的表示学习辅助任务，在
    OpenAI Procgen 上显示了改进的性能。? (?,?, DARL) 使用对抗学习来强制不同领域的表示不可区分，从而提高在视觉上多样的测试环境中的性能，即使仅在简单的训练环境中训练。?
    (?,?, DRIBO) 使用对比学习结合信息理论目标来学习只包含任务相关信息的表示，同时能够预测未来。他们在 DeepMind Control 中的视觉多样领域和
    OpenAI Procgen 中显示了改进的性能。
- en: 5.2.4 Adapting Online
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 在线适应
- en: A final way to handle differences between training and testing contexts is to
    adapt online to the testing contexts. [Definition 4](#Thmdefinition4 "Definition
    4 (Zero Shot Policy Transfer). ‣ 3.4 Training And Testing Contexts ‣ 3 Formalising
    Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning") allows that the policy can adjust online within
    a single episode, as the policy class includes non-Markovian policies (which can
    equivalently be viewed as adaptation procedures for producing markovian policies).
    This adaption has to happen within a single episode and for it to be useful, the
    adaptation will have to be rapid enough to be useful for improved performance
    within that episode. Most work on Meta RL, which is traditionally concerned with
    fast adaptation, assumes access to multiple training episodes in the testing CMDP,
    violating the zero-shot assumption. However, there are works which can adapt zero-shot.
    While not being exhaustive, we give a brief overview of this body of work here,
    focusing on key examples where zero-shot generalisation is evaluated.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 处理训练和测试上下文之间差异的最终方法是在线适应测试上下文。[定义 4](#Thmdefinition4 "定义 4（零-shot 策略转移）。 ‣ 3.4
    训练和测试上下文 ‣ 3 正式化强化学习中的零-shot 泛化 ‣ 深度强化学习中零-shot 泛化的调查") 允许策略在单一剧集内进行在线调整，因为策略类别包括非马尔可夫策略（可以等效地视为生成马尔可夫策略的适应过程）。这种适应必须在单一剧集内发生，为了有效，这种适应需要足够迅速，以便在该剧集中改进性能。大多数关于元强化学习的工作，传统上关注快速适应，假设可以在测试
    CMDP 中访问多个训练剧集，这违反了零-shot 假设。然而，也有一些工作能够零-shot 适应。虽然并不详尽，但我们在这里简要概述了这一领域的工作，重点介绍了在零-shot
    泛化中进行评估的关键例子。
- en: Many methods learn a context encoder or inference network, which then conditions
    either a policy or dynamics model for improved ZSG. The inference of this context
    at test-time can be seen as within-episode online adaptation. ? (?, UP-OSI) uses
    Online System Identification to infer the context, which then conditions a policy.
    ? (?, EVF) learns a context inference network end-to-end with a dynamics model
    and uses this to adapt to novel objects. ? (?, RMA) tackles the sim-to-real problem
    by training an agent using domain randomisation in simulation, and training a
    context inference model to condition the policy, similar to ?’s (?) UP-OSI but
    with learned context inference. ? (?, AugWM) take a similar idea but work in the
    offline RL setting, so first train a world model from offline data. They then
    perform domain randomisation of a specific form in the world model, and then use
    a hard-coded update rule (enabled by the specific form of domain randomisation)
    to determine the context to condition the policy, enabling it to adapt zero-shot
    to downstream tasks. These methods all tackle dynamics variation in continuous
    control or visual tasks. Often, these methods make use of domain randomisation
    approaches but aim to have a context-conditioned adaptive policy or model, rather
    than a policy invariant to all possible contexts.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方法学习上下文编码器或推理网络，然后用来调整策略或动态模型以提高零-shot 通用性。在测试时对这个上下文的推理可以看作是剧集内的在线适应。? (？，UP-OSI)
    使用在线系统识别来推断上下文，然后用来调整策略。? (？，EVF) 通过动态模型端到端地学习上下文推理网络，并利用这一点适应新对象。? (？，RMA) 通过在仿真中使用领域随机化训练智能体，并训练一个上下文推理模型以调整策略，从而解决了模拟到现实的问题，这类似于?
    的 (？) UP-OSI，但具有学习到的上下文推理。? (？，AugWM) 采用了类似的思路，但在离线强化学习设置中工作，因此首先从离线数据中训练世界模型。然后在世界模型中执行特定形式的领域随机化，再使用硬编码更新规则（由领域随机化的特定形式启用）来确定上下文以调整策略，使其能够零-shot
    适应下游任务。这些方法都处理了连续控制或视觉任务中的动态变化。通常，这些方法利用领域随机化方法，但旨在具有上下文条件的自适应策略或模型，而不是对所有可能的上下文都不变的策略。
- en: 'Some approaches use hard-coded adaptation techniques which do not rely on explicitly
    inferring a context. ? (?, TW-MCL) leverages a multi-headed architecture and multiple-choice
    learning to learn an ensemble of dynamics models that are selected from during
    deployment by choosing the one with the highest accuracy. ? (?, MOLe) tackles
    the online learning problem, keeping a continually updated and expanded collection
    of dynamics models which are chosen between using non-parametric task inference.
    In both these works the chosen model plans with model-predictive control, and
    they show improvements in continuous control tasks. ? (?, PAD) uses a self-supervised
    objective to update the internal representations of the policy during the test
    episode. They improve performance over standard baselines on visually varying
    DeepMind Control environments as well as CRLMaze (?) (a VizDoom (?) 3D navigation
    environment with visual variation), and on zero-shot generalisation to novel dynamics
    on both a real and simulated robot arm. (?, GrBAL,ReBAL) use meta-RL methods (MAML
    ? (?) and RL² ? (?)) to learn to quickly adapt a dynamics model using gradients
    or recurrence, and then use the adapted model to plan. In this case, it becomes
    more difficult to draw the line between learned and hard-coded update rules: The
    gradient update itself is hard-coded, but the initialisation is learned, and in
    the recurrent case the update is fully learned.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法使用硬编码的适应技术，这些技术不依赖于显式推断上下文。？（？，TW-MCL）利用多头架构和多选学习来学习一个动态模型的集合，在部署时通过选择准确度最高的模型来进行选择。？（？，MOLe）解决了在线学习问题，保持一个持续更新和扩展的动态模型集合，通过非参数任务推断进行选择。在这两项工作中，所选择的模型使用模型预测控制进行规划，并在连续控制任务中显示出改进。？（？，PAD）使用自监督目标在测试回合期间更新策略的内部表示。它们在视觉变化的DeepMind
    Control环境以及CRLMaze（？）（一个具有视觉变化的VizDoom（？）3D导航环境）和对真实及模拟机器人手臂的新动态的零样本泛化中，相比标准基准表现出改进。（？，GrBAL，ReBAL）使用元强化学习方法（MAML？（？）和RL²？（？））学习通过梯度或递归快速适应动态模型，然后使用适应后的模型进行规划。在这种情况下，区分学习的更新规则和硬编码的更新规则变得更加困难：梯度更新本身是硬编码的，但初始化是学习到的，在递归情况下，更新是完全学习到的。
- en: Finally, several methods meta-learn an adaptation function during training.
    RL² (?, ?) is a meta RL method where a recurrent network is used, the hidden state
    of which is not reset at episode boundaries, allowing it to learn and adapt within
    the recurrent state over multiple episodes. While often compared to methods that
    require multiple training episodes, this method can often adapt and perform well
    within a single episode, due to the optimisation approach and architecture. ? (?)
    introduce an extension to RL² called VariBAD based on bayesian RL, where the recurrent
    network learns to produce latent representations that are predictive of future
    rewards and previous transitions. This latent representation is used by the policy.
    ? (?, BOReL) adjusts VariBAD to be usable in an offline setting, improving zero-shot
    exploration using offline data. ? (?, HyperX) improves VariBAD with additional
    exploration bonuses to improve meta-exploration. ? (?) shows that these simple
    recurrent methods such as these, with carefully tuned implementations, can compete
    be improved further, often competing with more specialised algorithms, including
    in Robust RL and Meta-RL settings. ? (?, SNAIL) modelled fast adaptation as a
    sequence-to-sequence problem and learns an attention-based architecture which
    encodes sequences of experience to condition the policy. These methods all show
    improved performance on ZSG tasks, even though their main focus is on adaptation
    over longer time horizons.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，几种方法在训练过程中进行元学习以适应函数。RL²（？，？）是一种元强化学习方法，其中使用了递归网络，其隐藏状态在回合边界处不会重置，使其能够在多个回合中在递归状态内学习和适应。尽管这种方法经常与需要多个训练回合的方法进行比较，但由于优化方法和架构，该方法通常可以在单个回合内适应并表现良好。？（？）介绍了基于贝叶斯强化学习的RL²扩展，称为VariBAD，其中递归网络学习生成能够预测未来奖励和先前转移的潜在表示。这些潜在表示被策略使用。？（？，BOReL）调整VariBAD以便在离线环境中使用，通过离线数据改善零样本探索。？（？，HyperX）通过附加探索奖励改进VariBAD，以提高元探索。？（？）表明，这些简单的递归方法经过精心调整的实现可以进一步改进，通常与更多专业算法竞争，包括在稳健强化学习和元强化学习设置中。？（？，SNAIL）将快速适应建模为序列到序列问题，并学习基于注意力的架构，该架构对经验序列进行编码以调整策略。这些方法在ZSG任务上表现出改善，尽管它们的主要焦点是对更长时间范围的适应。
- en: 5.3 RL-Specific Problems And Improvements
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 RL特定问题及改进
- en: The motivations in the previous two sections are mostly equally applicable to
    supervised learning. However, on top of the problems of generalisation from supervised
    learning, RL has additional problems which inhibit zero-shot generalisation performance.
    In this section, we discuss methods targeted at these problems, and also discuss
    methods that improve ZSG purely through more effective optimisation on the training
    set in a way that does not overfit (at least empirically).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个部分中的动机大多同样适用于监督学习。然而，除了监督学习中的泛化问题外，RL 还有额外的问题，这些问题抑制了零-shot 泛化性能。在这一部分，我们讨论了针对这些问题的方法，同时也讨论了通过更有效的训练集优化（至少在经验上）来改进
    ZSG 的方法，这些方法不产生过拟合。
- en: 5.3.1 RL-specific Problems
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 RL 特有问题
- en: Optimisation in RL has additional issues on top of supervised learning, such
    as the non-stationarity of the data distribution, and the need to explore. These
    issues likely interact with generalisation in a non-trivial way. ? (?, ITER) shows
    that the non-stationarity of RL training means that policies learn features that
    do not generalise well, even if they achieve the same training performance. To
    address this, they introduce a method to iteratively distil the current policy
    network into a new policy network with reinitialised weights. This reduces the
    impact of non-stationarity on the new network, as it is being trained on a more
    stationary distribution. Other RL-specific optimisation issues likely interact
    with zero-shot generalisation either positively or negatively, and this area deserves
    further attention if we are to go beyond techniques copied and adjusted from supervised
    learning.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: RL 中的优化在监督学习的基础上有额外的问题，例如数据分布的非平稳性和探索的需求。这些问题很可能以非平凡的方式与泛化相互作用。? (?, ITER) 表明
    RL 训练的非平稳性意味着策略学习的特征泛化效果不好，即使它们达到了相同的训练性能。为了解决这个问题，他们引入了一种方法，通过将当前的策略网络迭代蒸馏到具有重新初始化权重的新策略网络中。这减少了非平稳性对新网络的影响，因为它是在更平稳的分布上进行训练的。其他
    RL 特有的优化问题可能与零-shot 泛化正面或负面互动，如果我们希望超越从监督学习中复制和调整的技术，这个领域值得进一步关注。
- en: 5.3.2 Better Optimisation without Overfitting
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 无过拟合的更好优化
- en: Several works improve ZSG by improving the training performance without overfitting. ? (?)
    introduce Phasic Policy Gradient (PPG), which adjusts the training regime and
    architecture of PPO such that the policy and value functions use entirely separate
    networks (rather than just separate heads), which allows the value head to be
    optimised for longer while not causing the policy to overfit. To recover the benefits
    of a joint representation, the value network is distilled into an auxiliary value
    head on the policy network. ? (?) build on PPG and introduce Decoupled Advantage
    Actor Critic (DAAC). They distil an advantage function calculated with GAE into
    the policy instead of a value function, which further ensures that the policy
    does not overfit to details that may be predictive of value function but not optimal
    action selection. They both show improved performance on OpenAI Procgen, demonstrating
    that value functions can be optimised more strongly than policies. ? (?, Sparse
    DVE) adjusts the architecture of the value function to allow for a multi-modal
    output, more closely modelling the true value function given just a visual input.
    This novel architecture combined with sparsity losses to ensure the value function
    has the desired properties reduces the variance of value function prediction and
    improves performance in terms of both return and navigation efficiency in OpenAI
    Procgen.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作通过提高训练性能而不产生过拟合来改进 ZSG。? (?) 介绍了相位策略梯度（PPG），该方法调整 PPO 的训练方案和架构，使得策略和价值函数使用完全独立的网络（而不仅仅是独立的头），这允许价值头在不导致策略过拟合的情况下进行更长时间的优化。为了恢复联合表示的好处，价值网络被蒸馏到策略网络上的辅助价值头中。? (?)
    在 PPG 的基础上引入了解耦优势演员评论员（DAAC）。他们将通过 GAE 计算的优势函数蒸馏到策略中，而不是价值函数，这进一步确保了策略不会过拟合于可能对价值函数有预测作用但不利于最佳动作选择的细节。他们都在
    OpenAI Procgen 上显示了改进的性能，证明了价值函数可以比策略更强地进行优化。? (?, 稀疏 DVE) 调整了价值函数的架构，以允许多模态输出，更准确地建模仅根据视觉输入的真实价值函数。这种新颖的架构结合了稀疏性损失以确保价值函数具有期望的属性，从而减少了价值函数预测的方差，并在
    OpenAI Procgen 中提高了回报和导航效率。
- en: Another angle on better optimisation is the use of model-based RL (MBRL). Very
    little work has applied MBRL to ZSG benchmarks, with ? (?) being an initial example.
    ? (?) apply MuZero Reanalyse (?, ?), a SOTA MBRL method, to OpenAI Procgen (?),
    showing much-improved performance over SOTA model-free methods at much lower sample
    complexity. This shows the potential of using MBRL to improve zero-shot generalisation
    to varying states and observations. The authors also apply MuZero to the meta-learning
    tasks in Meta-World (?), although the performance there is not as impressive,
    showing that generalising to new rewards (as is necessary for Meta-World) is not
    currently as amenable to MBRL approaches.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更好的优化角度是使用基于模型的RL (MBRL)。很少有工作将MBRL应用于ZSG基准，? (?) 是一个初步的例子。? (?) 将MuZero
    Reanalyse (?, ?) 应用到OpenAI Procgen (?), 显示出比SOTA无模型方法在样本复杂度上有显著提高。这展示了使用MBRL来改善对不同状态和观察的零样本泛化的潜力。作者还将MuZero应用于Meta-World
    (?) 中的元学习任务，尽管那里表现并不如人意，显示出泛化到新的奖励（如Meta-World中所需）目前对MBRL方法并不那么适合。
- en: While the methods described above do not target ZSG specifically, they improve
    test-time performance on ZSG benchmarks and so are included here. We hope the
    field will move towards benchmarks like Procgen being the standard for RL (and
    not just ZSG), such that in time this work is considered standard RL, rather the
    ZSG specifically.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述方法并不专门针对ZSG，但它们提高了在ZSG基准上的测试性能，因此在此包含。我们希望该领域会朝着像Procgen这样的基准成为RL的标准（而不仅仅是ZSG），以便最终这项工作被认为是标准RL，而不是仅仅是ZSG。
- en: 5.4 Discussion
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 讨论
- en: Having described existing methods for ZSG in RL, and categorised them in [Tables 2](#S5.T2
    "In 5.4 Discussion ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning") and [3](#S5.T3
    "Table 3 ‣ 5.4 Discussion ‣ 5 Methods For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"),
    we now draw some broader conclusions about the field, as well as discuss possible
    alternative classifications of methods.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 已描述ZSG在RL中的现有方法，并在[表2](#S5.T2 "在5.4讨论 ‣ 5种零样本泛化方法 ‣ 深度强化学习中零样本泛化的调查")和[表3](#S5.T3
    "表3 ‣ 5.4讨论 ‣ 5种零样本泛化方法 ‣ 深度强化学习中零样本泛化的调查")中对其进行了分类，我们现在得出一些更广泛的结论，并讨论可能的替代方法分类。
- en: '| Approach | Evaluation Variation |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 评估变异 |'
- en: '| --- | --- |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | Observation | State | Dynamics | Reward | All |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | 观察 | 状态 | 动态 | 奖励 | 全部 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Data Augmentation | *SODA (?)*, RCAN (?), *RandFM (?), InDA,ExDA (?), DrQ
    (?), SECANT (?), UCB-DrAC (?), PAADA (?), MixStyle (?), SVAE (?)* | *UCB-DrAC
    (?), PAADA (?)* |  |  |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 数据增强 | *SODA (?)*, RCAN (?), *RandFM (?), InDA,ExDA (?), DrQ (?), SECANT
    (?), UCB-DrAC (?), PAADA (?), MixStyle (?), SVAE (?)* | *UCB-DrAC (?), PAADA (?)*
    |  |  |  |'
- en: '| Domain Randomisation |  | MD-SAC (?) | P2PDRL (?), DR (?), CAD²RL (?), ADR
    (?), DDL (?) |  | PCG (?) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 领域随机化 |  | MD-SAC (?) | P2PDRL (?), DR (?), CAD²RL (?), ADR (?), DDL (?)
    |  | PCG (?) |'
- en: '| Environment Generation |  | POET (?), PAIRED (?), E-POET (?) |  |  |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 环境生成 |  | POET (?), PAIRED (?), E-POET (?) |  |  |  |'
- en: '| Optimisation Objectives |  | *PLR (?)* | *WR²L (?), RARL (?), (S)(R)(E)-MPO(?)*
    |  |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 优化目标 |  | *PLR (?)* | *WR²L (?), RARL (?), (S)(R)(E)-MPO(?)* |  |  |'
- en: '| Inductive Biases | AttentionAgent (?), VAI (?), SensoryNeuron (?), *DARLA
    (?)* | NAP (?), RelationalRL (?), SchemaNetworks (?), *IDAAC (?), CRAR (?)* |
    RelationalRL (?) | TransferLangLfP (?), SHIFTT (?) |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 归纳偏置 | AttentionAgent (?), VAI (?), SensoryNeuron (?), *DARLA (?)* | NAP
    (?), RelationalRL (?), SchemaNetworks (?), *IDAAC (?), CRAR (?)* | RelationalRL
    (?) | TransferLangLfP (?), SHIFTT (?) |  |'
- en: 'Table 2: A table categorising all methods for tackling the ZSG problem in RL,
    part 1 of 2\. The columns represent the type of variation (see [Table 1](#S4.T1
    "In 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")) the method is evaluated
    on, and rows represent the classification in [Fig. 4](#S5.F4 "In 5 Methods For
    Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"). Colours (and text styles) represent the main
    adjustment made by the method: Green normal-text methods mainly work by adjusting
    the training environment, Red monospace-text methods mainly work through adjusting
    the architecture, and *Blue italic* methods mainly work through adjusting the
    objective or loss function (including adding auxiliary losses). While changing
    the loss often requires an architectural adjustment, and often architectural changes
    require adjusted losses, we focus on the original motivation of the method.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：一个将所有针对 RL 中 ZSG 问题的方法分类的表格，第 1 部分，共 2 部。列代表方法评估的变化类型（见 [表 1](#S4.T1 "在
    4 个 RL 零样本泛化基准中 ‣ 深度强化学习中零样本泛化的调查")），行代表 [图 4](#S5.F4 "在 5 种 RL 零样本泛化方法中 ‣ 深度强化学习中零样本泛化的调查")
    的分类。颜色（和文本样式）代表方法的主要调整：绿色正常文本方法主要通过调整训练环境工作，红色等宽文本方法主要通过调整架构工作，而*蓝色斜体*方法主要通过调整目标或损失函数（包括添加辅助损失）工作。虽然改变损失通常需要架构调整，架构变化也经常需要调整损失，我们专注于方法的原始动机。
- en: '| Approach | Evaluation Variation |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 评估变化 |'
- en: '| --- | --- |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | Observation | State | Dynamics | Reward | All |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | 观察 | 状态 | 动态 | 奖励 | 全部 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Regularisation | Implicit Regularisation (?) | *SMIRL (?), Mixreg (?), IBAC-SNI
    (?)* | *RPC (?), IB-annealing (?)* |  | *L2,Dropout,etc. (?), EarlyStopping (?)*
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 正则化 | 隐式正则化 (?) | *SMIRL (?), Mixreg (?), IBAC-SNI (?)* | *RPC (?), IB-annealing
    (?)* |  | *L2, Dropout, 等 (?), EarlyStopping (?)* |'
- en: '| Learning Invariances | *DRIBO (?), DBC (?), DBC-normed-IR-ID (?)*, *DARL
    (?)*, *PSM (?), MISA (?)* | *IAPE (?), DRIBO (?), CTRL (?), CSSC (?), PSM (?),
    LEEP (?), IPO (?)* | *IPO (?)* |  |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 学习不变性 | *DRIBO (?), DBC (?), DBC-normed-IR-ID (?)*, *DARL (?)*, *PSM (?),
    MISA (?)* | *IAPE (?), DRIBO (?), CTRL (?), CSSC (?), PSM (?), LEEP (?), IPO (?)*
    | *IPO (?)* |  |  |'
- en: '| Fast Adaptation | PAD (?) | *EVF (?)*, SNAIL (?), *HyperX (?), VariBAD (?),
    RMA (?), RL² (?)* | *EVF (?)*, SNAIL (?), *HyperX (?), VariBAD (?)*, TW-MCL (?),
    *UP-OSI (?), BOReL (?), GrBAL,ReBAL (?), RMA (?), RL² (?)*, MOLe (?), AugWM (?),
    PAD (?) | SNAIL (?), *HyperX (?), BOReL (?), VariBAD (?), RL² (?)* |  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 快速适应 | PAD (?) | *EVF (?)*, SNAIL (?), *HyperX (?), VariBAD (?), RMA (?),
    RL² (?)* | *EVF (?)*, SNAIL (?), *HyperX (?), VariBAD (?)*, TW-MCL (?), *UP-OSI
    (?), BOReL (?), GrBAL, ReBAL (?), RMA (?), RL² (?)*, MOLe (?), AugWM (?), PAD
    (?) | SNAIL (?), *HyperX (?), BOReL (?), VariBAD (?), RL² (?)* |  |'
- en: '| RL-specific Problems | ITER (?) | ITER (?) |  |  |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| RL-specific Problems | ITER (?) | ITER (?) |  |  |  |'
- en: '| Better Optimisation | Sparse DVE (?), PPG (?), DAAC (?), MuZero++ (?) | Sparse
    DVE (?), PPG (?), DAAC (?), MuZero++ (?) |  | MuZero++ (?) |  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 更好的优化 | Sparse DVE (?), PPG (?), DAAC (?), MuZero++ (?) | Sparse DVE (?),
    PPG (?), DAAC (?), MuZero++ (?) |  | MuZero++ (?) |  |'
- en: 'Table 3: A table categorising all methods for tackling the ZSG problem in RL,
    part 2 of 2\. The columns represent the type of variation (see [Table 1](#S4.T1
    "In 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")) the method is evaluated
    on, and rows represent the classification in [Fig. 4](#S5.F4 "In 5 Methods For
    Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"). Colours (and text styles) represent the main
    adjustment made by the method: Green normal-text methods mainly work by adjusting
    the training environment, Red monospace-text methods mainly work through adjusting
    the architecture, and *Blue italic* methods mainly work through adjusting the
    objective or loss function (including adding auxiliary losses). While changing
    the loss often requires an architectural adjustment, and often architectural changes
    require adjusted losses, we focus on the original motivation of the method.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：一个表格对所有解决 RL 中 ZSG 问题的方法进行分类，这是第 2 部分，共 2 部。列表示了方法在不同类型的变体（参见 [表 1](#S4.T1
    "在 4 个零样本泛化基准中 ‣ 深度强化学习中的零样本泛化调查")）上的评估，而行表示了 [图 4](#S5.F4 "在 5 种零样本泛化方法中 ‣ 深度强化学习中的零样本泛化调查")中的分类。颜色（和文本样式）表示了方法的主要调整：绿色常规文本方法主要通过调整训练环境来工作，红色等宽文本方法主要通过调整架构来工作，而*蓝色斜体*方法主要通过调整目标或损失函数（包括添加辅助损失）来工作。虽然改变损失通常需要进行架构调整，且架构变化通常需要调整损失，但我们专注于方法的原始动机。
- en: Alternative Classifications.
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 替代分类。
- en: We have presented one possible classification of RL methods, but there are of
    course others. One alternative is to classify methods based on whether they change
    the architecture, environment or objective of the standard RL approach. This is
    useful from a low-level implementation perspective of what the differences are
    between approaches. This approach is not as useful for future researchers or practitioners
    who hope to choose a ZSG method for a concrete problem they are facing, as there
    is not a clear mapping between implementation details and whether a method will
    be effective for a specific problem. We do apply this classification through the
    colours in [Tables 2](#S5.T2 "In 5.4 Discussion ‣ 5 Methods For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning") and [3](#S5.T3 "Table 3 ‣ 5.4 Discussion ‣ 5 Methods For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"), to emphasise the current focus on adjusting
    the loss or objective function in current methods. Another approach would be to
    classify methods based on which benchmarks they attempt to solve, or what specific
    problem motivated their design. This goes too far in the other direction, grounding
    methods in exactly the benchmarks they tackle. While practitioners or researchers
    could try and see which benchmark is most similar to their problem, they might
    not understand which differences between benchmarks are most important, and hence
    choose a method that is not likely to succeed. This classification is also less
    useful in pointing out areas where there is less research being done. Our approach
    strikes a balance between these two approaches, describing the problem motivations
    and solution approaches at a high level which is useful for both practitioners
    and researchers in choosing methods and investigating future research directions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种可能的RL方法分类，但当然还有其他分类方法。另一种方法是基于是否改变标准RL方法的架构、环境或目标来分类。这对于从低级实现的角度了解方法之间的差异很有用。然而，对于未来希望为他们面临的具体问题选择ZSG方法的研究人员或从业者来说，这种方法并不那么有用，因为实现细节与方法是否对特定问题有效之间没有明确的对应关系。我们通过[表2](#S5.T2
    "在5.4讨论 ‣ 5种零样本泛化方法 ‣ 深度强化学习中的零样本泛化调查")和[表3](#S5.T3 "表3 ‣ 5.4讨论 ‣ 5种零样本泛化方法 ‣
    深度强化学习中的零样本泛化调查")中的颜色应用了这种分类，以强调当前方法在调整损失或目标函数上的关注点。另一种方法是基于方法尝试解决的基准测试或激发其设计的具体问题来分类。这种方法则完全把方法与它们解决的基准测试联系起来。虽然从业者或研究人员可以尝试查看哪些基准最类似于他们的问题，但他们可能无法理解基准之间哪些差异最为重要，从而选择一个可能不成功的方法。这种分类在指出研究较少的领域时也不那么有用。我们的方法在这两种方法之间达到了平衡，从高层次上描述了问题动机和解决方法，这对从业者和研究人员在选择方法和调查未来研究方向时都很有帮助。
- en: Strong Generalisation Requires Inductive Biases.
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 强大的泛化能力需要归纳偏差。
- en: As described in [Section 4.3](#S4.SS3.SSS0.Px6 "What Generalisation Can We Expect?
    ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"),
    there are hard ZSG problems involving combinatorial interpolation or extrapolation.
    We want to tackle these problems, as they will occur in real-world scenarios when
    we have limited contexts to train on, or we know the type of variation but cannot
    create context-MDPs in the deployment context-MDP set (e.g. due to limited simulators).
    To tackle these problems, we need stronger inductive biases targeted towards specific
    types of extrapolation, as there is unlikely to be a general-purpose algorithm
    that can handle all types of extrapolation (?). When doing research tackling extrapolative
    generalisation, researchers should be clear that they are introducing an inductive
    bias to help extrapolate in a specific way and be rigorous in analysing how this
    inductive bias helps. This involves also discussing in which situations the bias
    may hinder performance, for example in a different setting where extrapolation
    requires something else.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第4.3节](#S4.SS3.SSS0.Px6 "我们可以期待什么样的泛化？ ‣ 4.3 讨论 ‣ 4 零样本泛化基准 ‣ 深度强化学习中的零样本泛化调查")中所述，存在涉及组合插值或外推的困难零样本泛化问题。我们希望解决这些问题，因为在实际场景中，当我们只有有限的训练上下文时，或者我们知道变异类型但无法在部署上下文-MDP集中创建上下文-MDP（例如，由于模拟器有限）时，这些问题将会出现。为了解决这些问题，我们需要针对特定类型的外推制定更强的归纳偏置，因为不太可能有一种通用算法能够处理所有类型的外推。在进行外推泛化研究时，研究人员应该明确他们引入了归纳偏置以帮助以特定方式进行外推，并且在分析这种归纳偏置如何帮助时要严谨。这还涉及讨论在何种情况下这种偏置可能会妨碍性能，例如在外推需要其他条件的不同环境中。
- en: Going Beyond Supervised Learning as Inspiration.
  id: totrans-352
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超越监督学习作为灵感。
- en: Methods for improving generalisation from supervised learning have been a source
    of inspiration for many methods, particularly for visual variation. This is exactly
    the variation that happens in computer vision, and hence many methods from that
    field are applicable. However, non-visual forms of generalisation (i.e. dynamics,
    state and reward), while equally important are less studied. These challenges
    will be specific to RL and interact with other problems unique to RL such as the
    exploration-exploitation trade-off and the non-stationarity of the underlying
    data distribution. We hope to see more work in the area of non-visual ZSG, particularly
    when other hard RL problems are present.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 从监督学习中提高泛化能力的方法一直是许多方法的灵感来源，特别是在视觉变异方面。这正是计算机视觉中发生的变异，因此该领域的许多方法是适用的。然而，非视觉形式的泛化（即动态、状态和奖励），尽管同样重要，却研究较少。这些挑战将特定于强化学习，并与其他强化学习特有的问题如探索与利用权衡和底层数据分布的非平稳性相互作用。我们希望在非视觉零样本泛化领域看到更多的研究，特别是在其他困难的强化学习问题存在时。
- en: 6 Discussion And Future Work
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与未来工作
- en: In this section we highlight further points for discussion, building on those
    in [Section 4.3](#S4.SS3 "4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning") and [Section 5.4](#S5.SS4 "5.4 Discussion ‣ 5 Methods For Zero-shot
    Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"), including directions for future research on
    new methods, benchmarks, evaluation protocols and understanding.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们突出讨论了进一步的观点，基于[第4.3节](#S4.SS3 "4.3 讨论 ‣ 4 零样本泛化基准 ‣ 深度强化学习中的零样本泛化调查")和[第5.4节](#S5.SS4
    "5.4 讨论 ‣ 5 零样本泛化方法 ‣ 深度强化学习中的零样本泛化调查")中的内容，包括对未来研究的新方法、基准、评估协议和理解方向。
- en: 6.1 Generalisation Beyond Zero-Shot Policy Transfer
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 零样本策略转移之外的泛化
- en: This survey focuses on zero-shot policy transfer. We believe this problem setting
    is a reasonable one that captures many challenges relevant to deploying RL systems.
    However, there are many important scenarios where zero-shot generalisation is
    impossible, or the assumptions can be relaxed. We will want to move beyond zero-shot
    policy transfer if we are to use RL effectively in a wider variety of scenarios.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查聚焦于零样本策略转移。我们认为这个问题设置是合理的，它捕捉了许多与部署强化学习系统相关的挑战。然而，还有许多重要的场景中零样本泛化是不可能的，或者这些假设可以放宽。如果我们希望在更广泛的场景中有效使用强化学习，我们需要超越零样本策略转移。
- en: 'The most sensible way of relaxing the zero-shot assumption is to move into
    a continual RL (?, CRL) setting: future RL systems will likely be deployed in
    scenarios where the environment is constantly changing, such that the system needs
    to adapt continually to these changes. Making progress on this setting will require
    benchmarks, and we agree with ? (?) that there are not enough good benchmarks
    for CRL. Most benchmarks do not enable testing all the different attributes desired
    of CRL methods, although ?’s (?) CORA is a good first step. New benchmarks for
    CRL would also be excellent as benchmarks for ZSG, especially if these benchmarks
    introduce new environments. We expect that methods built for domain generalisation
    in RL might be suitable in CRL, as the continual learning setting can be conceptualised
    as one in which the domain that tasks are within changes over time. This is in
    contrast to benchmarks that evaluate generalisation to levels sampled from the
    same distribution as during training, such as OpenAI Procgen (?). Hence, we recommend
    work on building new CRL benchmarks, to make progress on CRL and ZSG together.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 放宽零-shot 假设的最合理方法是转向持续 RL（?，CRL）设置：未来的 RL 系统可能会在环境不断变化的场景中部署，从而需要系统不断适应这些变化。在这个设置中取得进展将需要基准测试，我们同意
    ?（?）的观点，即 CRL 的良好基准还不够多。大多数基准无法测试 CRL 方法所需的所有不同属性，尽管 ? 的（?）CORA 是一个好的第一步。新的 CRL
    基准也将是 ZSG 的优秀基准，尤其是如果这些基准引入了新的环境。我们预计为 RL 中的领域泛化构建的方法可能适用于 CRL，因为持续学习设置可以被概念化为任务所在领域随时间变化的情况。这与评估对从训练期间相同分布中采样的级别的泛化的基准（例如
    OpenAI Procgen (?)）形成对比。因此，我们建议在构建新的 CRL 基准上进行工作，以便在 CRL 和 ZSG 上共同取得进展。
- en: Coupled with the idea of CRL as a more realistic setting for generalisation
    in RL, we can take inspiration from how humans generalise and what they transfer
    when generalising, to go beyond transferring a single policy. While humans may
    not always be able to achieve good results zero-shot on a new task, if the task
    is related to previously seen tasks, they can reuse previous knowledge or skills
    to learn the new task faster. This broader notion of generalisation of objects
    other than a complete policy (e.g. skills or environment knowledge) will become
    more relevant when we start to build more powerful RL systems. Hierarchical and
    multi-task RL are related fields, and methods in these settings often learn subcomponents,
    modules or skills on source tasks (possibly in an unsupervised manner) which they
    can then use to increase learning speed and performance when transferred to novel
    tasks (?, ?). It is likely the capability to transfer components other than a
    single policy will be useful for future systems, and it would hence be beneficial
    to have benchmarks that enable us to test these kinds of capabilities. However,
    this is a challenging request to meet, as defining what these components are,
    and deciding on a performance metric for these subcomponent transfer benchmarks,
    are both difficult conceptual problems. We hope to see work in this area in the
    future.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 结合将 CRL 作为 RL 中更现实的泛化环境的想法，我们可以从人类如何泛化以及他们在泛化时转移的内容中获得灵感，以超越仅转移单一策略的范畴。虽然人类可能无法在新任务上做到零-shot
    的良好结果，但如果任务与之前见过的任务相关，他们可以重用以前的知识或技能，从而更快地学习新任务。当我们开始构建更强大的 RL 系统时，这种对除了完整策略之外的对象（例如技能或环境知识）的泛化的更广泛概念将变得更加相关。层次化和多任务
    RL 是相关领域，这些设置中的方法通常会在源任务上学习子组件、模块或技能（可能是以无监督的方式），然后在转移到新任务时使用这些子组件以提高学习速度和性能。除了单一策略的能力之外，转移其他组件的能力可能对未来的系统有用，因此具有能够测试这些能力的基准将是有益的。然而，这是一项难度很大的要求，因为定义这些组件是什么，以及为这些子组件转移基准决定性能指标，都是困难的概念问题。我们希望未来能看到这一领域的工作。
- en: A final assumption which is almost untouched in RL is that of a fixed action
    space between training and testing. Recently, ? (?) introduced a novel problem
    setting and framework centred around how to generalise to new actions in RL. They
    introduce several benchmarks for testing methods which generalise to new actions,
    and a method based on learning an action representation combined with an action-ranking
    network which acts as the policy at test time. There is very little work in this
    area, and we do not cover it in this survey, but it presents an interesting future
    direction for ZSG research.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中几乎未触及的最后一个假设是训练和测试之间的固定动作空间。最近，? (?) 提出了一个新颖的问题设置和框架，集中在如何在RL中泛化到新动作。他们引入了几个基准用于测试泛化到新动作的方法，以及一种基于学习动作表示结合动作排名网络的方法，该网络在测试时充当策略。这个领域的工作非常少，我们在本调查中没有涉及，但它为ZSG研究提供了一个有趣的未来方向。
- en: 6.2 Real World Reinforcement Learning Generalisation
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 现实世界强化学习泛化
- en: '? (?) propose 9 properties that are characteristic of real-world RL.⁶⁶6The
    9 properties are: limited samples; sensor, action and reward delays; high-dimensional
    state and action spaces; reasoning about constraints; partial observability; multi-objective
    or poorly specified rewards; low action latency; offline training; and explainable
    policies In thinking about the current set of ZSG benchmarks, these properties
    are relevant in two ways. First, when applying our methods to the real world,
    we will have to tackle these problems. Hence, it would be beneficial if ZSG benchmarks
    had these properties, such that we can ensure that our ZSG methods work in real-world
    environments.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ? (?) 提出了9个现实世界RL的特征。⁶⁶这9个特征是：样本有限；传感器、动作和奖励延迟；高维状态和动作空间；关于约束的推理；部分可观测性；多目标或指定不清的奖励；低动作延迟；离线训练；和可解释的策略。在考虑当前的ZSG基准时，这些特征在两个方面是相关的。首先，当将我们的方法应用于现实世界时，我们将不得不解决这些问题。因此，如果ZSG基准具备这些特征，将是有益的，以便我们可以确保我们的ZSG方法在现实世界环境中有效。
- en: 'Second, several properties are particularly relevant for zero-shot generalisation
    and the design of new benchmarks: (1) *high cost of new samples*, (2) *training
    from offline data* and (3) *underspecified or multi-objective reward functions*.
    We explain each of these and their relation to ZSG below.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，几个属性对于零样本泛化和新基准的设计尤其相关：（1）*新样本的高成本*，（2）*从离线数据中训练* 和（3）*未指定或多目标奖励函数*。我们将以下详细解释这些属性及其与ZSG的关系。
- en: Context Efficiency.
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 上下文效率。
- en: Addressing (1), it is likely that the high cost of new samples will also mean
    a high cost of new environment instances or contexts. This means we want methods
    that are *context efficient* as well as sample efficient, and hence we require
    benchmarks and evaluation protocols in which only a few contexts are allowed during
    training, rather than several 1000s. It is also worth investigating if there is
    an optimal trade-off (for different costs per sample and per context) between
    new training samples and new contexts. This line of work would revolve around
    different possible evaluation metrics based on how many contexts are needed to
    reach certain levels of generalisation performance. Further, there may be ways
    of actively selecting new contexts to maximise the generalisation performance
    while minimising the number of new contexts used, effectively a form of active
    learning. Evaluating context efficiency will be more computationally expensive,
    as models will need to be repeatedly trained on different numbers of training
    contexts, so work which figures out how to evaluate this property more efficiently
    is also beneficial.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 针对（1），新样本的高成本很可能也意味着新环境实例或上下文的高成本。这意味着我们需要既*上下文高效*又样本高效的方法，因此我们需要在训练过程中只允许使用少量上下文的基准和评估协议，而不是几千个。还值得调查是否存在在每个样本和上下文的不同成本下，新训练样本和新上下文之间的最佳权衡。这项工作将围绕基于需要多少上下文才能达到某些泛化性能水平的不同评估指标展开。此外，还可能存在主动选择新上下文以最大化泛化性能同时最小化使用的新上下文数量的方法，实际上是一种主动学习。评估上下文效率将更具计算成本，因为模型需要在不同数量的训练上下文上反复训练，因此找出如何更高效地评估这一属性的工作也是有益的。
- en: Sim-to-Real and Offline RL.
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模拟到现实和离线RL。
- en: 'To tackle (2), two options emerge. The first is relying on good simulations,
    and then tackling the *sim-to-real* problem, and the second is tackling the offline
    RL problem directly (?). These approaches might be more or less relevant or applicable
    depending on the scenario: for example, in many robotics applications, a simulator
    is available, whereas in healthcare settings it is likely learning from offline
    data is the only approach possible. Sim-to-real is a problem of domain generalisation.
    If this direction is most relevant, it implies we should focus on building environments
    that test for good domain generalisation. Existing work on sim-to-real does address
    this to some extent, but it would be beneficial to have a fully simulated benchmark
    for testing sim-to-real methods, as that enables faster research progress than
    requiring real robots. This is a difficult task and is prone to the possibility
    of overfitting to the simulation of the sim-to-real problem, but it would be useful
    as an initial environment for testing sim-to-real transfer.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决（2），出现了两个选项。第一个是依赖于良好的仿真，然后解决*从仿真到现实*的问题；第二个是直接解决离线强化学习问题（？）。这些方法的相关性或适用性可能因场景而异：例如，在许多机器人应用中，可以使用模拟器，而在医疗保健环境中，可能只能从离线数据中学习。从仿真到现实是领域泛化的问题。如果这个方向最为相关，意味着我们应该专注于构建测试良好领域泛化的环境。现有的从仿真到现实的工作在某种程度上确实涉及到这一点，但拥有一个完全模拟的基准来测试从仿真到现实的方法将会更有益，因为这可以比依赖真实机器人更快地推进研究。这是一项困难的任务，容易导致过拟合于从仿真到现实的问题，但作为测试从仿真到现实转移的初步环境，它会很有用。
- en: 'Offline RL is also a problem of generalisation: a key issue here is generalising
    to state-action pairs unseen in the training data, and most current methods tackle
    this by conservatively avoiding such pairs (?). If we had methods to reliably
    extrapolate to such pairs we could improve offline RL performance.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 离线强化学习（Offline RL）也是一个泛化问题：这里的关键问题是将未在训练数据中出现的状态-动作对进行泛化，目前大多数方法通过保守地避免这些对来解决这个问题（？）。如果我们有方法能可靠地推断这些对，我们可以提升离线强化学习的表现。
- en: 'As well as generalisation improving offline RL, it is likely that future RL
    deployment scenarios will need to tackle the combination of offline RL and ZSG:
    training policies offline that then generalise to new contexts unseen in the offline
    dataset. Current offline RL benchmarks (?, ?) do not measure generalisation in
    this way, but we believe they should enable us to tackle this combined problem:
    for example, training on offline data from 200 levels in OpenAI Procgen, and evaluating
    on the full distribution of levels. If tackling this is infeasible with current
    methods (as both offline RL and ZSG are hard problems), then a good compromise
    is to first work on the offline-online setting, where offline RL is used for pretraining,
    followed by online fine-tuning. Some work has been done in this area (?), but
    this does not tackle the ZSG problem specifically. Creating benchmarks for evaluating
    these approaches, where the emphasis is on reducing the length of the online fine-tuning
    stage and evaluating generalisation after fine-tuning, would move us towards truly
    offline RL generalisation while still being tractable with current methods.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 除了泛化提高离线强化学习外，未来的强化学习部署场景可能还需要解决离线强化学习和ZSG的结合：在离线数据集上训练策略，然后在未见过的新情境中进行泛化。当前的离线强化学习基准（？，？）没有以这种方式测量泛化，但我们认为它们应该使我们能够解决这个综合问题：例如，在OpenAI
    Procgen的200个级别的离线数据上进行训练，并在完整的级别分布上进行评估。如果使用当前的方法解决这个问题不可行（因为离线强化学习和ZSG都是困难的问题），那么一个好的折衷方法是首先在离线-在线设置上进行工作，其中离线强化学习用于预训练，然后进行在线微调。在这一领域已有一些工作（？），但这并未特别解决ZSG问题。创建评估这些方法的基准，重点在于减少在线微调阶段的长度，并在微调后评估泛化，将使我们在真正离线强化学习泛化方面更进一步，同时仍能用当前的方法解决。
- en: Reward Function Variation.
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 奖励函数变化。
- en: It is likely future RL systems will be goal- or task-conditioned, as it will
    be more efficient to train a general system to do several related tasks than to
    train a different system for each task. Here, as well as generalising to new dynamics
    and observations, the trained policies will need to generalise to unseen goals
    or tasks. The policy will need to be able to solve novel problem formulations,
    and hence have a more generic problem-solving ability. Benchmarks which address
    this capability are hence necessary for progress towards more general RL systems
    (see [Section 6.4](#S6.SS4 "6.4 Tackling Stronger Types Of Variation ‣ 6 Discussion
    And Future Work ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的强化学习系统可能会以目标或任务为条件，因为训练一个通用系统来完成几个相关任务比为每个任务训练一个不同的系统更高效。在这里，除了对新动态和观测的泛化外，训练后的策略还需要对未见的目标或任务进行泛化。策略需要能够解决新颖的问题表述，因此具有更通用的解决问题能力。因此，需要解决这种能力的基准测试，以便在朝着更通用的强化学习系统发展时取得进展（见[第6.4节](#S6.SS4
    "6.4 Tackling Stronger Types Of Variation ‣ 6 Discussion And Future Work ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")）。
- en: 'Related to challenges of generalisation surrounding reward functions, for many
    real-world problems designing good reward functions is very difficult. A promising
    approach is using inverse RL (?, ?, IRL) to learn a reward function from human
    demonstrations (?, ?, ?, ?), rather than hand-crafting reward functions for each
    task. This is often more time-efficient, as demonstrating a task is easier than
    specifying a reward function for it. There are two generalisation problems here:
    ensuring the learned reward function generalises to unseen context-MDPs during
    policy training, and ensuring the trained policy generalises to unseen context-MDPs
    at test time. The first is an IRL generalisation problem, and the second is the
    standard problem of generalisation we have considered here. Solving both will
    be important for ensuring that this approach to training agents is effective.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 与围绕奖励函数的一般化挑战相关的是，对于许多现实世界的问题，设计良好的奖励函数是非常困难的。一种有前景的方法是使用逆强化学习（？，？，IRL）从人类示范中学习奖励函数（？，？，？，？），而不是为每个任务手工设计奖励函数。这通常更具时间效率，因为演示任务比为其指定奖励函数更容易。这里存在两个一般化问题：确保学习到的奖励函数在策略训练过程中能在未见的上下文-MDPs中进行一般化，以及确保训练后的策略在测试时能在未见的上下文-MDPs中进行一般化。第一个是逆强化学习的一般化问题，第二个是我们在这里考虑的一般化的标准问题。解决这两个问题对于确保这种训练代理的方法有效非常重要。
- en: Work building benchmarks and methods to solve these problems would be valuable
    future work. Some of these directions could be addressed by combining new evaluation
    protocols with pre-existing environments to create new benchmarks, rather than
    requiring entirely new environments to be designed and created.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建基准测试和解决这些问题的方法方面的工作将是有价值的未来工作。这些方向中的一些可以通过将新的评估协议与现有环境结合来创建新的基准，而不是需要设计和创建完全新的环境来解决。
- en: 6.3 Multi-Dimensional Evaluation Of Generalisation
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 一般化的多维评估
- en: Generalisation performance is usually reported using a single scalar value of
    test-time performance. However, this does not give us much information with which
    to compare and choose between methods. While better performance on a benchmark,
    all else being equal, probably means that a method is more useful, it is usually
    not clear to what extent the ordering of methods on a benchmark’s leaderboard
    is representative of the hypothetical ordering of those methods on a real-world
    problem scenario for which we have to choose a method. To alleviate this, performance
    should be reported on multiple different testing context sets which evaluate different
    types of generalisation, and radar plots (such as those demonstrated by ?) can
    be used to compare methods. This will be more useful for comparing methods in
    a more realistic way, as well as for practitioners choosing between methods.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 一般化性能通常使用单一标量值的测试时间性能来报告。然而，这并不能提供足够的信息来比较和选择方法。虽然在基准测试上表现更好，其他条件相同，可能意味着方法更有用，但通常不清楚基准测试排行榜上的方法排序在真实问题场景中的代表性程度。为了缓解这一点，性能应在多个不同的测试上下文集上报告，这些上下文集评估不同类型的一般化，并且可以使用雷达图（例如由？演示的那些）来比较方法。这将更有助于以更现实的方式比较方法，以及帮助从业者在方法之间进行选择。
- en: Very few environments have the context set required for this type of evaluation,
    and even those that do would require additional work to create the specific testing
    context sets. Hence, we recommend that future benchmarks for ZSG are designed
    to enable this type of evaluation. This requires building environments with controllable
    context sets as well as PCG components ([Section 4.3](#S4.SS3.SSS0.Px4 "The Downsides
    of Procedural Content Generation for Zero-shot Generalisation. ‣ 4.3 Discussion
    ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")), as well as careful
    thought to create a variety of testing context sets, ensuring they match important
    types of generalisation. A first way of splitting up testing context sets might
    be by the type of variation between training and testing, as well as whether interpolation
    or extrapolation is required to generalise to that context set. There are likely
    many other ways, which may be domain-specific or general across many domains.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有环境具备进行这种评估所需的上下文集，即使有，也需要额外的工作来创建特定的测试上下文集。因此，我们建议未来的ZSG基准应设计为支持这种类型的评估。这需要构建具有可控上下文集以及PCG组件的环境（[第4.3节](#S4.SS3.SSS0.Px4
    "程序化内容生成对零-shot泛化的缺点。 ‣ 4.3 讨论 ‣ 4 零-shot泛化的基准 ‣ 深度强化学习中的零-shot泛化调查")），以及仔细思考创建多样化的测试上下文集，确保它们匹配重要的泛化类型。第一种划分测试上下文集的方式可能是根据训练和测试之间的变异类型，以及是否需要插值或外推以泛化到该上下文集。可能还有许多其他方式，这些方式可能是特定领域的，也可能在多个领域中通用。
- en: 6.4 Tackling Stronger Types Of Variation
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 解决更强的变异类型
- en: Many of the methods for ZSG tackle observation function or state space variation.
    Both of these (or at least the practical implementations of them used in the corresponding
    benchmarks) tend to produce environment families in which it is much simpler to
    verify (at least intuitively) The Principle Of Unchanged Optimality, meaning that
    tackling the ZSG problem is tractable; generalisation problems with this style
    of variation tend to be easier to solve. These two types of variation will appear
    in real-world scenarios, but the other types of variation are equally important
    and often harder to tackle.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 许多ZSG的方法解决了观察函数或状态空间变异。这两种变异（或至少是相应基准中使用的实际实现）往往会产生一些环境系列，在这些系列中验证（至少直观上）**不变最优性原则**会更简单，这意味着解决ZSG问题是可处理的；这种变异风格的泛化问题通常更容易解决。这两种变异类型会出现在现实世界场景中，但其他类型的变异同样重要，并且通常更难处理。
- en: 'Work on dynamics is mostly focused on two specific settings: sim-to-real transfer,
    and multi-agent environments. In sim-to-real transfer (?), there is always dynamics
    variation between the simulator and reality, and much work has focused on how
    to train policies in this setting. More generally, work on robotics and continuous
    control tends to address some forms of dynamics variation either in how the robot
    itself is controlled (e.g. due to degrading parts) or in the environment (e.g. different
    terrain). In multi-agent environments, if the other agents are considered part
    of the environment (for example in a single-agent training setting), then varying
    the other agents varies the dynamics of the environment (?). These both occur
    in the real world, but there are inevitably other forms of dynamics variation
    which are less well-studied. Investigating what these other forms of dynamics
    variation are and whether studying them would be useful is promising future work.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 动态学的研究主要集中在两个特定设置上：模拟到现实的转移和多智能体环境。在模拟到现实转移（？）中，模拟器和现实之间总是存在动态变异，许多工作集中在如何在这种设置下训练策略上。更一般来说，机器人学和连续控制的研究倾向于解决一些形式的动态变异，无论是在机器人自身的控制（例如由于部件的退化）还是在环境中（例如不同的地形）。在多智能体环境中，如果其他智能体被认为是环境的一部分（例如在单智能体训练设置中），那么变化其他智能体会改变环境的动态（？）。这两者都在现实世界中存在，但不可避免地还有其他形式的动态变异尚未得到充分研究。研究这些其他形式的动态变异是什么，以及研究它们是否有用，是有前途的未来工作。
- en: 'Tackling reward-function variation will be required to train general-purpose
    policies that can perform a variety of tasks, and generalise to unseen tasks without
    further training data (as discussed in [Section 6.2](#S6.SS2.SSS0.Px3 "Reward
    Function Variation. ‣ 6.2 Real World Reinforcement Learning Generalisation ‣ 6
    Discussion And Future Work ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")). This variation is more difficult to tackle, and it is often difficult
    or impossible to verify The Principle Of Unchanged Optimality (?). However, we
    must do work on research to tackle these problems, as otherwise RL approaches
    will be limited to less-ambitious problems or less-general applications. Further
    work building more benchmarks that enable testing for reward function variation,
    especially beyond simple styles of goal specification such as a target state,
    would be beneficial. Special attention needs to be paid to The Principle Of Unchanged
    Optimality (?) while building these benchmarks: current work tends to handle this
    by conditioning the policy on a goal or reward specification. Research on what
    approaches to goal specification are both tractable for policy optimisation and
    useful for real-world scenarios would be beneficial, as there is likely a trade-off
    between these two desirable attributes. ?’s (?, ?) work provide good examples
    of investigating natural language as goal specification, utilising pretrained
    models to improve ZSG, and we look forward to seeing more work in this area.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 解决奖励函数变化问题是训练能够执行多种任务的通用策略所必需的，这些策略可以泛化到未见过的任务，而无需进一步的训练数据（如[第6.2节](#S6.SS2.SSS0.Px3
    "奖励函数变化。 ‣ 6.2 现实世界强化学习泛化 ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零样本泛化调查")所讨论）。这种变化更难以应对，并且通常难以验证**不变最优性原理**（?）。然而，我们必须进行研究来解决这些问题，否则
    RL 方法将局限于较不雄心勃勃的问题或较少通用的应用。进一步建立更多基准测试以便测试奖励函数变化，特别是超越简单的目标状态指定风格，将是有益的。建立这些基准时需要特别关注**不变最优性原理**（?）：当前的研究往往通过根据目标或奖励规格对策略进行条件化来处理这一点。研究哪些目标规格方法既适用于策略优化又对实际场景有用将是有益的，因为这两种期望属性之间可能存在权衡。?
    的 (?, ?) 工作提供了研究自然语言作为目标规格的良好示例，利用预训练模型来改进 ZSG，我们期待看到该领域更多的研究。
- en: 6.5 Understanding Generalisation In Reinforcement Learning
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 理解强化学习中的泛化
- en: While beyond the scope of this survey, several works try to understand the problems
    underlying generalisation in RL. Works in this area include ?’s (?) results, which
    describe the notion of observational overfitting as one cause of the generalisation
    gap in RL; ? (?) analyses the relationship between gradient interference and generalisation
    in supervised and RL showing that temporal difference methods tend to have lower-interference
    training, which correlates with worse generalisation; ? (?) studies transient
    non-stationarity in RL and shows that it negatively impacts RL generalisation;
    and ? (?) investigates what environmental factors affect generalisation in an
    instruction-following task, finding for example that an egocentric viewpoint improves
    generalisation, as does a richer observation space.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然超出了本调查的范围，但已有若干研究尝试理解 RL 中泛化问题的根源。该领域的研究包括 ? 的 (?) 结果，描述了观察性过拟合这一现象作为 RL 泛化差距的原因之一；?
    (?) 分析了梯度干扰与监督学习和 RL 中泛化之间的关系，发现时间差分方法往往具有较低的干扰训练，这与较差的泛化相关；? (?) 研究了 RL 中的瞬态非平稳性，并表明它对
    RL 泛化有负面影响；? (?) 探讨了环境因素对指令跟随任务泛化的影响，例如发现以自我中心视角能改善泛化，丰富的观察空间也有类似效果。
- en: This research is barely scratching the surface of understanding why generalisation
    in RL in particular is a challenge, and there is much future work to be done.
    This will enable us to build better methods, and understand any theoretical limits
    to the diversity of tasks that an RL agent can solve given a limited number of
    training contexts. The precise empirical experimentation required for this kind
    of research is exactly that which is enabled by having tight control over the
    factors of variation in the environments being used, which reinforces the conclusion
    made in [Section 4.3](#S4.SS3.SSS0.Px4 "The Downsides of Procedural Content Generation
    for Zero-shot Generalisation. ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning") that purely PCG environments are unsuited for a study of generalisation
    in RL.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究仅仅触及了理解为什么RL中的泛化尤其具有挑战性的表面，还有许多未来的工作要做。这将使我们能够建立更好的方法，并理解RL代理在有限的训练上下文下能够解决的任务多样性的任何理论限制。这类研究所需的精确实证实验正是通过对使用环境中的变化因素进行严格控制来实现的，这进一步证明了在[第4.3节](#S4.SS3.SSS0.Px4
    "The Downsides of Procedural Content Generation for Zero-shot Generalisation.
    ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")中得出的结论，即纯粹的PCG环境不适合用于RL中的泛化研究。
- en: 6.6 Future Work On Methods For Zero-shot Generalisation
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 关于零-shot泛化方法的未来工作
- en: In this subsection, we summarise directions for future work on methods for ZSG,
    informed by [Section 5](#S5 "5 Methods For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning").
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们总结了基于[第5节](#S5 "5 Methods For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")的信息，关于ZSG方法的未来工作方向。
- en: As described in [Section 6.5](#S6.SS5 "6.5 Understanding Generalisation In Reinforcement
    Learning ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"), there are many RL-specific factors that interact
    with generalisation performance, often likely in a negative way. Examples of these
    factors include the non-stationarity of the data distribution used for training;
    bootstrapping and TD learning in general; and the need for exploration. Work to
    understand these factors and then build methods to tackle them as discussed in
    [Section 5.3.1](#S5.SS3.SSS1 "5.3.1 RL-specific Problems ‣ 5.3 RL-Specific Problems
    And Improvements ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning
    ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning") is a fruitful
    direction for future work.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第6.5节](#S6.SS5 "6.5 Understanding Generalisation In Reinforcement Learning
    ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot Generalisation in Deep
    Reinforcement Learning")所述，有许多与RL特定相关的因素会影响泛化性能，通常可能以负面的方式。例如，这些因素包括用于训练的数据分布的非平稳性；一般的自助采样和TD学习；以及探索的需求。理解这些因素并建立方法来应对它们的工作，如[第5.3.1节](#S5.SS3.SSS1
    "5.3.1 RL-specific Problems ‣ 5.3 RL-Specific Problems And Improvements ‣ 5 Methods
    For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning")所讨论，是未来工作的一个有益方向。
- en: We often have a context space that is unstructured or contains many unsolvable
    context-MDPs. Methods that enable more effective sampling from these context spaces
    can alleviate this. Several methods were covered in [Section 5.1.2](#S5.SS1.SSS2
    "5.1.2 Environment Generation ‣ 5.1 Increasing Similarity Between Training And
    Testing ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning ‣ A
    Survey of Zero-shot Generalisation in Deep Reinforcement Learning") but more work
    in this area, tackling more challenging and realistic environments with different
    types of variation, would be beneficial.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常常面临一个非结构化或包含许多不可解的上下文-MDP的上下文空间。能够更有效地从这些上下文空间中采样的方法可以缓解这一问题。[第5.1.2节](#S5.SS1.SSS2
    "5.1.2 Environment Generation ‣ 5.1 Increasing Similarity Between Training And
    Testing ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning ‣ A
    Survey of Zero-shot Generalisation in Deep Reinforcement Learning")中介绍了几种方法，但在这个领域的更多工作，解决更具挑战性和现实环境中的不同类型的变化，将是有益的。
- en: While much work has been done on meta RL, most work focuses on few-shot adaptation.
    However, work in this area could be adapted to tackle zero-shot policy transfer
    settings, if the environment has long episodes that require or enable online adaptation.
    Enabling the policy to learn and adapt online, and learning this adaptation, would
    likely improve performance. These approaches would also be more suited to tackling
    stronger forms of variation ([Section 6.4](#S6.SS4 "6.4 Tackling Stronger Types
    Of Variation ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")), as online adaptation may be necessary in these
    scenarios. Initial work in this area is described in [Section 5.2.4](#S5.SS2.SSS4
    "5.2.4 Adapting Online ‣ 5.2 Handling Differences Between Training And Testing
    ‣ 5 Methods For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning"), but much more research
    should be done.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在元强化学习（meta RL）方面已经做了大量工作，但大多数工作侧重于少样本适应。然而，这一领域的工作可以被改编来解决零样本策略迁移设置，如果环境具有长时间的
    episode，这些 episode 需要或允许在线适应。使策略能够在线学习和适应，并学习这种适应性，可能会提高性能。这些方法也更适合解决更强形式的变异（[第6.4节](#S6.SS4
    "6.4 处理更强类型的变异 ‣ 6 讨论与未来工作 ‣ 深度强化学习中零样本泛化的调查")），因为在线适应在这些场景中可能是必要的。该领域的初步工作在[第5.2.4节](#S5.SS2.SSS4
    "5.2.4 在线适应 ‣ 5.2 处理训练与测试之间的差异 ‣ 强化学习中的零样本泛化方法 ‣ 深度强化学习中零样本泛化的调查")中描述，但需要做更多的研究。
- en: There are several under-explored approaches that cut across the categorisation
    in this work. As shown in [Tables 2](#S5.T2 "In 5.4 Discussion ‣ 5 Methods For
    Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning") and [3](#S5.T3 "Table 3 ‣ 5.4 Discussion ‣ 5
    Methods For Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning"), most methods focus on changing
    the loss function or algorithmic approach. Architectural changes informed by inductive
    biases are less well studied, with notable examples coming from the work of ? (?, ?, ?, ?, ?).
    More work can be done on investigating different architectures, either taking
    inspiration from supervised learning or creating RL-specific architectures. These
    architectures could encode inductive biases in ways that are difficult to encode
    through the use of auxiliary losses or regularisation. A second under-explored
    area is model-based reinforcement learning (MBRL) for ZSG. Most methods surveyed
    here are model-free, with notable exceptions being the work of ? (?, ?, ?, ?).
    Learning a world model and combining it with planning methods can enable stronger
    forms of generalisation, especially to novel reward functions (if the reward function
    is available during planning). As long as the model generalises well, it could
    also enable generalisation to novel state and observation functions. World models
    which can adapt to changing dynamics will be more challenging, but ? (?) give
    an initial example. ? (?) is the first example investigating how well standard
    MBRL approaches generalise, and we look forward to seeing more work in this area.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了一些未被充分研究的方法，这些方法跨越了现有的分类。正如[表2](#S5.T2 "在5.4讨论 ‣ 强化学习中的零样本泛化方法 ‣ 深度强化学习中零样本泛化的调查")和[表3](#S5.T3
    "表3 ‣ 5.4讨论 ‣ 强化学习中的零样本泛化方法 ‣ 深度强化学习中零样本泛化的调查")所示，大多数方法侧重于改变损失函数或算法方法。基于归纳偏差的架构变化研究较少，值得注意的例子来自于?（?，?，?，?，?）。在研究不同架构方面还有更多的工作可以做，无论是借鉴监督学习的灵感还是创建特定于强化学习的架构。这些架构可以以难以通过使用辅助损失或正则化编码的方式来编码归纳偏差。第二个未被充分研究的领域是用于零样本泛化的基于模型的强化学习（MBRL）。这里调查的大多数方法是无模型的，值得注意的例外是?（?，?，?，?）。学习世界模型并将其与规划方法相结合可以实现更强的泛化，尤其是对新颖的奖励函数（如果在规划过程中奖励函数是可用的）。只要模型能够很好地泛化，它也可以实现对新颖状态和观察函数的泛化。能够适应变化动态的世界模型将更具挑战性，但?（?）提供了一个初步的例子。?（?）是第一个研究标准MBRL方法泛化能力的例子，我们期待看到该领域更多的工作。
- en: 7 Conclusion
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: The study of ZSG in RL is still new but is of vital importance if we want to
    develop applicable and usable RL solutions to real-world problems. In this survey
    we have aimed to clarify the terminology and formalism concerning ZSG in RL, bringing
    together disparate threads of research together in a unified framework. We presented
    a categorisation of benchmarks for ZSG, splitting the taxonomy into environments
    and evaluation protocols, and we categorised existing methods for tackling the
    wide variety of ZSG problems.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RL 中的 ZSG 研究仍然较新，但如果我们希望开发适用于现实问题的强化学习解决方案，它是至关重要的。在本调查中，我们旨在澄清有关 RL 中 ZSG
    的术语和形式化，将不同的研究方向汇聚在一个统一的框架中。我们提出了 ZSG 基准的分类，将分类法分为环境和评估协议，并对现有的 ZSG 问题解决方法进行了分类。
- en: Here we summarise the key takeaways of this survey (with pointers to the more
    in-depth discussion of the takeaway). The first two takeaways are concerned with
    the problem setting as a whole. The next four are focused on evaluating ZSG through
    benchmarks and metrics, and future work in these areas. The last two are focused
    on methods for tackling the ZSG problem.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们总结了本调查的关键要点（并附上了更深入讨论的指引）。前两个要点涉及整体问题设置。接下来的四个要点集中于通过基准和指标评估 ZSG，以及这些领域的未来工作。最后两个要点则集中于解决
    ZSG 问题的方法。
- en: •
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Zero-shot policy transfer is useful to study, even if in specific settings we
    may be able to relax the zero-shot assumption, as it provides base algorithms
    upon which domain-specific solutions can be built ([Section 3.7](#S3.SS7.SSS0.Px2
    "Motivating Zero-Shot Policy Transfer ‣ 3.7 Remarks And Discussion ‣ 3 Formalising
    Zero-shot Generalisation In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning")).
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零-shot 策略迁移在研究中非常有用，即使在特定环境中我们可能能够放宽零-shot 假设，因为它提供了基础算法，可以在此基础上构建领域特定的解决方案
    ([Section 3.7](#S3.SS7.SSS0.Px2 "激励零-shot 策略迁移 ‣ 3.7 备注与讨论 ‣ 3 形式化零-shot 泛化在强化学习中的应用
    ‣ 零-shot 泛化在深度强化学习中的调查"))。
- en: •
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: However, more work should be done to look beyond zero-shot policy transfer,
    particularly at continual reinforcement learning, as a way to get around the restriction
    of the principle of unchanged optimality ([Section 6.1](#S6.SS1 "6.1 Generalisation
    Beyond Zero-Shot Policy Transfer ‣ 6 Discussion And Future Work ‣ A Survey of
    Zero-shot Generalisation in Deep Reinforcement Learning")).
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，还需要更多的工作来超越零-shot 策略迁移，特别是考虑到持续强化学习，作为绕过不变最优性原则限制的一种方法 ([Section 6.1](#S6.SS1
    "6.1 超越零-shot 策略迁移的泛化 ‣ 6 讨论与未来工作 ‣ 零-shot 泛化在深度强化学习中的调查"))。
- en: •
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Purely black-box PCG environments are not useful for testing specific forms
    of generalisation and are most useful for ensuring robust improvements in standard
    RL algorithms. Combining PCG and controllable factors of variation is our recommended
    way to design new environments, having the best trade-off between high variety
    and the possibility of scientific experimentation ([Section 4.3](#S4.SS3.SSS0.Px4
    "The Downsides of Procedural Content Generation for Zero-shot Generalisation.
    ‣ 4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")).
    This also enables a more multidimensional approach to evaluating generalisation
    performance ([Section 6.3](#S6.SS3 "6.3 Multi-Dimensional Evaluation Of Generalisation
    ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot Generalisation in Deep
    Reinforcement Learning")), and specific experimentation aimed at improving our
    understanding of ZSG in RL ([Section 6.5](#S6.SS5 "6.5 Understanding Generalisation
    In Reinforcement Learning ‣ 6 Discussion And Future Work ‣ A Survey of Zero-shot
    Generalisation in Deep Reinforcement Learning")).
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 纯黑箱 PCG 环境不适合测试特定形式的泛化，最有用的是确保标准 RL 算法的稳健改进。结合 PCG 和可控的变化因素是我们推荐的设计新环境的方法，它在高变异性和科学实验的可能性之间取得了最佳平衡
    ([Section 4.3](#S4.SS3.SSS0.Px4 "程序化内容生成对零-shot 泛化的缺点 ‣ 4.3 讨论 ‣ 4 零-shot 泛化基准在强化学习中的应用
    ‣ 零-shot 泛化在深度强化学习中的调查"))。这也使得对泛化性能的评估可以采用更多维度的方法 ([Section 6.3](#S6.SS3 "6.3
    泛化的多维评估 ‣ 6 讨论与未来工作 ‣ 零-shot 泛化在深度强化学习中的调查"))，以及针对提升我们对 RL 中 ZSG 理解的具体实验 ([Section
    6.5](#S6.SS5 "6.5 理解强化学习中的泛化 ‣ 6 讨论与未来工作 ‣ 零-shot 泛化在深度强化学习中的调查"))。
- en: •
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For real-world scenarios, we have to consider both sample efficiency and context
    efficiency. Evaluating the performance of methods on different sizes of training
    context sets is a useful evaluation metric which gives us more information to
    choose between different methods ([Section 6.2](#S6.SS2.SSS0.Px1 "Context Efficiency.
    ‣ 6.2 Real World Reinforcement Learning Generalisation ‣ 6 Discussion And Future
    Work ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")).
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于现实世界场景，我们必须考虑样本效率和上下文效率。评估方法在不同大小的训练上下文集上的表现是一个有用的评估指标，它为我们选择不同方法提供了更多信息 ([第
    6.2 节](#S6.SS2.SSS0.Px1 "上下文效率。 ‣ 6.2 现实世界强化学习泛化 ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零-shot
    泛化调研"))。
- en: •
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Work on generalisation problems associated with offline RL is under-explored
    and would ensure that offline RL approaches are able to generalise effectively
    ([Section 6.2](#S6.SS2.SSS0.Px2 "Sim-to-Real and Offline RL. ‣ 6.2 Real World
    Reinforcement Learning Generalisation ‣ 6 Discussion And Future Work ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")).
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与离线 RL 相关的泛化问题仍未得到充分研究，这将确保离线 RL 方法能够有效泛化 ([第 6.2 节](#S6.SS2.SSS0.Px2 "从模拟到现实和离线
    RL。 ‣ 6.2 现实世界强化学习泛化 ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零-shot 泛化调研"))。
- en: •
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While observation-function and state-space variation are commonly studied, dynamics
    variation is only tackled in limited settings and reward-function variation is
    very under-studied. These stronger forms of variation are still likely to appear
    in real-world scenarios, and hence should be the focus of future research ([Section 6.4](#S6.SS4
    "6.4 Tackling Stronger Types Of Variation ‣ 6 Discussion And Future Work ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning")).
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管观察功能和状态空间变异常被研究，但动态变化仅在有限的环境中被处理，而奖励函数变化则非常少研究。这些更强的变异形式仍可能在现实世界场景中出现，因此应成为未来研究的重点
    ([第 6.4 节](#S6.SS4 "6.4 处理更强的变异类型 ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零-shot 泛化调研"))。
- en: •
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For stronger forms of ZSG, stronger inductive biases are necessary, and research
    should be up-front about what the inductive bias they are introducing is, how
    it tackles the specific benchmark they are tackling, and how general they expect
    that inductive bias to be ([Section 5.4](#S5.SS4.SSS0.Px2 "Strong Generalisation
    Requires Inductive Biases. ‣ 5.4 Discussion ‣ 5 Methods For Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning")).
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于更强的 ZSG 形式，需要更强的归纳偏差，研究应明确介绍他们引入的归纳偏差是什么，如何解决他们所面对的具体基准问题，以及他们期望这种归纳偏差的普遍性如何
    ([第 5.4 节](#S5.SS4.SSS0.Px2 "强泛化需要归纳偏差。 ‣ 5.4 讨论 ‣ 5 强化学习中的零-shot 泛化方法 ‣ 深度强化学习中的零-shot
    泛化调研"))。
- en: •
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: There is much underexplored future work in developing new methods for improved
    ZSG, such as model-based RL, new architectures, fast online adaptation, solving
    RL-specific generalisation problems, and environment generation ([Section 6.6](#S6.SS6
    "6.6 Future Work On Methods For Zero-shot Generalisation ‣ 6 Discussion And Future
    Work ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")).
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在开发改进 ZSG 的新方法方面仍有许多未深入探索的未来工作，例如基于模型的 RL、新架构、快速在线适应、解决 RL 特定的泛化问题以及环境生成 ([第
    6.6 节](#S6.SS6 "6.6 关于零-shot 泛化方法的未来工作 ‣ 6 讨论与未来工作 ‣ 深度强化学习中的零-shot 泛化调研"))。
- en: We hope that this survey will help clarify and unify work tackling the problem
    of zero-shot generalisation in RL, spurring further research in this area, and
    serve as a touch-point and reference for researchers and practitioners both inside
    and outside the field.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这项调研能够帮助澄清和统一解决 RL 中零-shot 泛化问题的工作，推动该领域的进一步研究，并作为领域内外研究人员和从业者的参考点和参考资料。
- en: Acknowledgements
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank (in alphabetical order) Flo Dorner, Jack Parker-Holder, Katja Hofmann,
    Laura Ruis, Maximilian Igl, Mikayel Samvelyan, Minqi Jiang, Nicklas Hansen, Roberta
    Raileanu, Yingchen Xi and Zhengyao Jiang for discussion and comments on drafts
    of this work. We also thank (alphabetically) André Biedenkapp, Chelsea Finn, Eliot
    Xing, Jessica Hamrick, Pablo Samuel Castro, Sirui Xie, Steve Hansen, Theresa Eimer,
    Vincent François-Lavet and Zhou Kaiyang for pointing out missing references or
    work for an updated version of this survey. Finally, we thank Frans Oliehoek and
    the other reviewers at JAIR for all their helpful criticism and advice, which
    resulted in a much-improved paper.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢（按字母顺序）Flo Dorner、Jack Parker-Holder、Katja Hofmann、Laura Ruis、Maximilian
    Igl、Mikayel Samvelyan、Minqi Jiang、Nicklas Hansen、Roberta Raileanu、Yingchen Xi
    和 Zhengyao Jiang 对本工作的草稿提供的讨论和评论。我们还感谢（按字母顺序）André Biedenkapp、Chelsea Finn、Eliot
    Xing、Jessica Hamrick、Pablo Samuel Castro、Sirui Xie、Steve Hansen、Theresa Eimer、Vincent
    François-Lavet 和 Zhou Kaiyang 指出缺失的参考文献或工作，以便更新本调查的版本。最后，我们感谢Frans Oliehoek 和
    JAIR 的其他审稿人，他们的有益批评和建议使论文得到了极大的改进。
- en: 'Author Contributions:'
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 作者贡献：
- en: 'Robert Kirk led the work, developed the formalism, benchmarks categorisation,
    methods categorisation, and discussion and future work, wrote the full manuscript
    of the survey, and wrote successive drafts with comments and feedback from the
    other authors. Amy Zhang wrote parts of [Sections 3.1](#S3.SS1 "3.1 Background:
    Generalisation In Supervised Learning ‣ 3 Formalising Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning"), [3.6](#S3.SS6 "3.6 Additional Assumptions For More Feasible Generalisation
    ‣ 3 Formalising Zero-shot Generalisation In Reinforcement Learning ‣ A Survey
    of Zero-shot Generalisation in Deep Reinforcement Learning"), [3.7](#S3.SS7 "3.7
    Remarks And Discussion ‣ 3 Formalising Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning"),
    [4.3](#S4.SS3 "4.3 Discussion ‣ 4 Benchmarks For Zero-shot Generalisation In Reinforcement
    Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement Learning")
    and [A](#S1a "A Other Structural Assumptions on MDPs ‣ A Survey of Zero-shot Generalisation
    in Deep Reinforcement Learning"), as well as providing improvements on the entire
    work through discussion and editing. Tim Rocktäschel and Edward Grefenstette advised
    Robert Kirk, providing discussion and feedback in developing the ideas behind
    the survey, and provided feedback and comments on the manuscript.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Robert Kirk 主导了这项工作，开发了形式主义、基准分类、方法分类，以及讨论和未来工作，撰写了调查的完整手稿，并根据其他作者的评论和反馈编写了后续草稿。Amy
    Zhang 撰写了[第 3.1 节](#S3.SS1 "3.1 背景：监督学习中的泛化 ‣ 3 强化学习中零样本泛化的形式化 ‣ 深度强化学习中零样本泛化的调查")、[3.6
    节](#S3.SS6 "3.6 更可行的泛化假设 ‣ 3 强化学习中零样本泛化的形式化 ‣ 深度强化学习中零样本泛化的调查")、[3.7 节](#S3.SS7
    "3.7 备注和讨论 ‣ 3 强化学习中零样本泛化的形式化 ‣ 深度强化学习中零样本泛化的调查")、[4.3 节](#S4.SS3 "4.3 讨论 ‣ 4
    零样本泛化基准 ‣ 深度强化学习中零样本泛化的调查") 和 [A 节](#S1a "A 其他 MDP 的结构假设 ‣ 深度强化学习中零样本泛化的调查") 的部分内容，并通过讨论和编辑改进了整个工作。Tim
    Rocktäschel 和 Edward Grefenstette 向 Robert Kirk 提供了建议，就调查背后的想法提供了讨论和反馈，并对手稿提出了反馈和评论。
- en: A Other Structural Assumptions on MDPs
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他 MDP 的结构假设
- en: 'Other forms of structured MDPs have been defined beyond the contextual MDP (?)
    and leveraged to develop algorithms that exploit those structural assumptions.
    One type that holds promise for generalisation is the factored MDP. A factored
    MDP assumes a state space described by a set of discrete variables, denoted $S:=\{S_{1},S_{2},\ldots,S_{n}\}$
    (?, ?). We follow the notation and definitions used by ? (?). The transition function
    $T$ has the following property:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上下文 MDP（?）之外，还定义了其他形式的结构化 MDP，并利用这些结构假设开发了相应的算法。一种有望用于推广的类型是分解 MDP。分解 MDP
    假设一个由离散变量集描述的状态空间，记作 $S:=\{S_{1},S_{2},\ldots,S_{n}\}$（?，?）。我们遵循 ?（?） 使用的符号和定义。转移函数
    $T$ 具有以下属性：
- en: Definition 6  (Factored transition functions).
  id: totrans-416
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 6（分解转移函数）。
- en: Given two states $s,s^{\prime}$ and action $a$ in a factored MDP $M$, the transition
    function satisfies a conditional independence condition
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个状态 $s,s^{\prime}$ 和一个动作 $a$ 在分解 MDP $M$ 中，转移函数满足条件独立性条件
- en: '|  | $T(s^{\prime}&#124;s,a)=\prod_{i}P(s^{\prime}_{i}&#124;s,a),$ |  | (3)
    |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  | $T(s^{\prime}&#124;s,a)=\prod_{i}P(s^{\prime}_{i}&#124;s,a),$ |  | (3)
    |'
- en: where $P(s^{\prime}_{i}|s,a)$ is the probability distribution for each factor
    $S_{i}$ conditioned on the previous state and action.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P(s^{\prime}_{i}|s,a)$ 是每个因子 $S_{i}$ 在给定先前状态和动作的条件下的概率分布。
- en: Parallels to causal graphs can be drawn (?), where a causal graph is a DAG,
    each vertex is a variable, and the directed edges represent causal relationships.
    We can rewrite the conditional independence assumption as
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 可以类比于因果图（？），其中因果图是一个DAG，每个顶点是一个变量，有向边表示因果关系。我们可以将条件独立性假设改写为
- en: '|  | $T(s^{\prime}&#124;s,a)=\prod_{i}P(s^{\prime}_{i}&#124;\text{PA}(s^{\prime}_{i}),a),$
    |  | (4) |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  | $T(s^{\prime}&#124;s,a)=\prod_{i}P(s^{\prime}_{i}&#124;\text{PA}(s^{\prime}_{i}),a),$
    |  | (4) |'
- en: 'where the probability of each factor $s_{i}$ only depends on its parent factors
    $\text{PA}(s_{i})$ from the previous time step. Ideally, these parents are only
    a subset of all factors so this representation results in a reduction in size
    from the original MDP. Further, this enforces that there are no synchronous edges
    between factors in the same time step. Critically, the rewards can also be factored
    in the following way:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个因子 $s_{i}$ 的概率仅依赖于其来自先前时间步的父因子 $\text{PA}(s_{i})$。理想情况下，这些父因子仅是所有因子的一个子集，因此这种表示方式会导致相对于原始MDP的大小减少。此外，这要求在同一时间步中因子之间没有同步边。关键是，奖励也可以以以下方式因子化：
- en: Definition 7  (Factored reward functions).
  id: totrans-423
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 7（因子化奖励函数）。
- en: Given two states $s,s^{\prime}$ and action $a$ in a factored MDP $M$, the reward
    function satisfies a conditional independence condition
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个状态 $s,s^{\prime}$ 和一个动作 $a$ 在一个因子化MDP $M$ 中，奖励函数满足条件独立性条件
- en: '|  | $\mathbb{E}\big{[}R(s,a)\big{]}=\sum_{i}\mathbb{E}\big{[}R_{i}(s_{i},a)\big{]},$
    |  | (5) |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}\big{[}R(s,a)\big{]}=\sum_{i}\mathbb{E}\big{[}R_{i}(s_{i},a)\big{]},$
    |  | (5) |'
- en: where $R_{i}(s_{i},a)$ is the reward function for each factor $S_{i}$.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R_{i}(s_{i},a)$ 是每个因子 $S_{i}$ 的奖励函数。
- en: 'One can think of this factored MDP framework as an extension of the single
    context-MDP, where the combination of one or more of these factors can be represented
    as the context, with the space of all possible combinations of factors being the
    context set. In the latter case, this formulation explicitly encodes how generalisation
    can be achieved to new contexts via *systematicity* ([Section 3.1](#S3.SS1 "3.1
    Background: Generalisation In Supervised Learning ‣ 3 Formalising Zero-shot Generalisation
    In Reinforcement Learning ‣ A Survey of Zero-shot Generalisation in Deep Reinforcement
    Learning"), ?): the policy will be trained on contexts taking some values within
    the set of possible factors, and then be tested on unseen combinations of seen
    factors.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将这种因子化的MDP框架视为单一上下文-MDP的扩展，其中这些因子之一或多个的组合可以被表示为上下文，所有可能因子组合的空间构成了上下文集。在后一种情况下，这种公式化明确地编码了如何通过*系统性*
    ([第3.1节](#S3.SS1 "3.1 背景：监督学习中的泛化 ‣ 强化学习中的零样本泛化的形式化 ‣ 深度强化学习中零样本泛化的调查"), ?)：策略将在上下文中训练，这些上下文取自可能因子集中的某些值，然后在未见过的因子组合上进行测试。
- en: A more restricted form of structured MDP is the relational MDP (?). A relational
    MDP is described by tuple $\langle C,F,A,D,T,R\rangle$. $C$ is the set of object
    types, $F$ is the set of fluent schemata that are arguments that modify each object
    type. $A$ is the set of action schemata that acts on objects, $D$ is the set of
    domain objects, each associated with a single type from $C$, and finally, $T$
    is the transition function and $R$ is the reward function. An additional assumption
    is that objects that are not acted upon do not change in a transition. Note that
    the relational MDP can be expanded into a factored MDP that does not assume the
    additional structure of the form of object types with invariant relations. While
    this form of MDP is a Planning Domain Definition Language (PDDL) and therefore
    lends itself well to planning algorithms, it is overly complex for learning algorithms.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更受限的结构化MDP形式是关系型MDP（？）。关系型MDP由元组 $\langle C,F,A,D,T,R\rangle$ 描述。$C$ 是对象类型的集合，$F$
    是修改每种对象类型的流畅模式的集合。$A$ 是作用于对象的动作模式集合，$D$ 是域对象的集合，每个对象与 $C$ 中的单个类型相关联，最后，$T$ 是转换函数，$R$
    是奖励函数。附加假设是，未被操作的对象在转换中不会改变。注意，关系型MDP 可以扩展为一个不假设额外对象类型与不变关系的结构的因子化MDP。虽然这种形式的MDP
    是规划领域定义语言（PDDL），因此非常适合规划算法，但对于学习算法来说过于复杂。
- en: Object-oriented MDPs (?) are a simpler form of relational MDPs that are less
    constrained, and therefore hold more promise for learning methods. Objects, fluents,
    and actions are defined in the same way as in relational MDPs, but all transition
    dynamics are determined by a set of Boolean transition terms which consist of
    a set of pre-defined relation terms between objects and object attributes. In
    spite of this simplification, it is still significantly constrained compared to
    CMDPs and can be difficult to use when describing complex systems.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 面向对象的MDP (?) 是比关系型MDP更简单的形式，约束更少，因此对学习方法更具前景。对象、流体和动作的定义与关系型MDP中的相同，但所有过渡动态都由一组布尔过渡项确定，这些项由一组预定义的对象及其属性之间的关系项组成。尽管简化了，但与CMDPs相比，这种方法仍然有显著约束，并且在描述复杂系统时可能很困难。
- en: A final example of a structured MDP is the Block MDP. Block MDPs (?) are described
    by a tuple $\langle\mathcal{S},\mathcal{A},\mathcal{X},p,q,R\rangle$ with a finite,
    unobservable state space $\mathcal{S}$ and possibly infinite, but observable space
    $\mathcal{X}$. $p$ denotes the latent transition distribution, $q$ is the (possibly
    stochastic) emission function and $R$ the reward function. This structured MDP
    is useful in rich observation environments where the given observation space is
    large, but a much smaller state space can be found that yields an equivalent MDP.
    This allows for improved exploration and sample complexity bounds that rely on
    the size of that latent state space rather than the given observation space.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化MDP的一个最终示例是Block MDP。Block MDPs (?) 由一个四元组$\langle\mathcal{S},\mathcal{A},\mathcal{X},p,q,R\rangle$描述，其中$\mathcal{S}$是有限且不可观察的状态空间，而$\mathcal{X}$可能是无限的但可观察的空间。$p$表示潜在的过渡分布，$q$是（可能是随机的）发射函数，$R$是奖励函数。这个结构化MDP在观察空间大的丰富观察环境中很有用，但可以找到一个更小的状态空间来获得等效的MDP。这使得基于潜在状态空间的改进探索和样本复杂度界限成为可能，而不是基于给定的观察空间。
- en: References
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdolmaleki et al. Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R.,
    Heess, N., and Riedmiller, M. A. (2018). Maximum a posteriori policy optimisation.
    In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
    BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdolmaleki等（Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess,
    N., 和Riedmiller, M. A. (2018)）。最大后验策略优化。在第六届国际学习表征会议（ICLR 2018），加拿大温哥华，2018年4月30日
    - 5月3日，会议录。OpenReview.net。
- en: Abdullah et al. Abdullah, M. A., Ren, H., Ammar, H. B., Milenkovic, V., Luo,
    R., Zhang, M., and Wang, J. (2019). Wasserstein Robust Reinforcement Learning..
    arXiv:1907.13196 [cs, stat]..
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdullah等（Abdullah, M. A., Ren, H., Ammar, H. B., Milenkovic, V., Luo, R., Zhang,
    M., 和Wang, J. (2019)）。Wasserstein鲁棒强化学习.. arXiv:1907.13196 [cs, stat]。
- en: 'Ada et al. Ada, S. E., Ugur, E., and Akin, H. L. (2021). Generalization in
    Transfer Learning.. arXiv:1909.01331 [cs, stat]., Comment: 23 pages, 36 figures.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ada等（Ada, S. E., Ugur, E., 和Akin, H. L. (2021)）。迁移学习中的泛化.. arXiv:1909.01331
    [cs, stat]。评论：23页，36张图。
- en: 'Agarwal et al. Agarwal, R., Machado, M. C., Castro, P. S., and Bellemare, M. G.
    (2021). Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement
    Learning.. arXiv:2101.05265 [cs, stat]., Comment: ICLR 2021 (Spotlight). Website:
    https://agarwl.github.io/pse.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal等（Agarwal, R., Machado, M. C., Castro, P. S., 和Bellemare, M. G. (2021)）。对比行为相似性嵌入在强化学习中的泛化..
    arXiv:2101.05265 [cs, stat]。评论：ICLR 2021（聚焦）。网站：https://agarwl.github.io/pse。
- en: 'Ahmed et al. Ahmed, O., Träuble, F., Goyal, A., Neitz, A., Bengio, Y., Schölkopf,
    B., Wüthrich, M., and Bauer, S. (2020). CausalWorld: A Robotic Manipulation Benchmark
    for Causal Structure and Transfer Learning.. arXiv:2010.04296 [cs, stat]., Comment:
    The first two authors contributed equally, the last two authors avised jointly.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed等（Ahmed, O., Träuble, F., Goyal, A., Neitz, A., Bengio, Y., Schölkopf,
    B., Wüthrich, M., 和Bauer, S. (2020)）。CausalWorld：一个用于因果结构和迁移学习的机器人操作基准.. arXiv:2010.04296
    [cs, stat]。评论：前两位作者贡献相等，最后两位作者共同建议。
- en: 'Albrecht and Stone Albrecht, S. V.,  and Stone, P. (2018). Autonomous agents
    modelling other agents: A comprehensive survey and open problems.. Artificial
    Intelligence..'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Albrecht 和 Stone（Albrecht, S. V., 和Stone, P. (2018)）。自主代理建模其他代理：综合调查和开放问题..
    人工智能..
- en: Amin et al. Amin, S., Gomrokchi, M., Satija, H., van Hoof, H., and Precup, D.
    (2021). A Survey of Exploration Methods in Reinforcement Learning.. arXiv:2109.00157
    [cs]..
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin等（Amin, S., Gomrokchi, M., Satija, H., van Hoof, H., 和Precup, D. (2021)）。强化学习中的探索方法调查..
    arXiv:2109.00157 [cs]。
- en: Anand et al. Anand, A., Walker, J., Li, Y., Vértes, E., Schrittwieser, J., Ozair,
    S., Weber, T., and Hamrick, J. B. (2021). Procedural Generalization by Planning
    with Self-Supervised World Models.. arXiv:2111.01587 [cs]..
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anand 等 Anand, A., Walker, J., Li, Y., Vértes, E., Schrittwieser, J., Ozair,
    S., Weber, T., 和 Hamrick, J. B. (2021). 通过自我监督世界模型进行程序性泛化。arXiv:2111.01587 [cs]。
- en: Arjovsky et al. Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2020).
    Invariant Risk Minimization.. arXiv:1907.02893 [cs, stat]..
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky 等 Arjovsky, M., Bottou, L., Gulrajani, I., 和 Lopez-Paz, D. (2020).
    不变风险最小化。arXiv:1907.02893 [cs, stat]。
- en: 'Arora and Doshi Arora, S.,  and Doshi, P. (2020). A Survey of Inverse Reinforcement
    Learning: Challenges, Methods and Progress.. arXiv:1806.06877 [cs, stat]..'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 和 Doshi Arora, S., 和 Doshi, P. (2020). 逆向强化学习的调查：挑战、方法与进展。arXiv:1806.06877
    [cs, stat]。
- en: 'Ball et al. Ball, P. J., Lu, C., Parker-Holder, J., and Roberts, S. (2021).
    Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single
    Offline Environment.. arXiv:2104.05632 [cs]., Comment: Accepted @ ICML 2021; Spotlight
    @ ICLR 2021 "Self-Supervision for Reinforcement Learning Workshop".'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ball 等 Ball, P. J., Lu, C., Parker-Holder, J., 和 Roberts, S. (2021). 增强的世界模型促进了从单一离线环境中的零样本动态泛化。arXiv:2104.05632
    [cs]。评论：被 ICML 2021 接受；ICLR 2021 “自我监督强化学习研讨会”特别推荐。
- en: Bapst et al. Bapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K. L.,
    Kohli, P., Battaglia, P. W., and Hamrick, J. B. (2019). Structured agents for
    physical construction. In Chaudhuri, K.,  and Salakhutdinov, R. (Eds.), Proceedings
    of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
    2019, Long Beach, California, USA, Vol. 97 of Proceedings of Machine Learning
    Research, pp. 464–474\. PMLR.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bapst 等 Bapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K. L., Kohli,
    P., Battaglia, P. W., 和 Hamrick, J. B. (2019). 用于物理构造的结构化代理。在 Chaudhuri, K., 和
    Salakhutdinov, R. (编辑), 第36届国际机器学习大会论文集，ICML 2019，2019年6月9-15日，加州洛杉矶，美国，机器学习研究论文集第97卷，第464–474页。PMLR。
- en: Battaglia et al. Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez,
    A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner,
    R., Gulcehre, C., Song, F., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A., Allen,
    K., Nash, C., Langston, V., Dyer, C., Heess, N., Wierstra, D., Kohli, P., Botvinick,
    M., Vinyals, O., Li, Y., and Pascanu, R. (2018). Relational inductive biases,
    deep learning, and graph networks.. arXiv:1806.01261 [cs, stat]..
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Battaglia 等 Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A.,
    Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner,
    R., Gulcehre, C., Song, F., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A., Allen,
    K., Nash, C., Langston, V., Dyer, C., Heess, N., Wierstra, D., Kohli, P., Botvinick,
    M., Vinyals, O., Li, Y., 和 Pascanu, R. (2018). 关系归纳偏置、深度学习与图网络。arXiv:1806.01261
    [cs, stat]。
- en: 'Bellemare et al. Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.
    (2013). The Arcade Learning Environment: An Evaluation Platform for General Agents..
    Journal of Artificial Intelligence Research..'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等 Bellemare, M. G., Naddaf, Y., Veness, J., 和 Bowling, M. (2013).
    游戏机学习环境：一个通用代理的评估平台。人工智能研究期刊。
- en: Bengio et al. Bengio, E., Pineau, J., and Precup, D. (2020). Interference and
    generalization in temporal difference learning. In Proceedings of the 37th International
    Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119
    of Proceedings of Machine Learning Research, pp. 767–777. PMLR.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等 Bengio, E., Pineau, J., 和 Precup, D. (2020). 时间差学习中的干扰与泛化。在第37届国际机器学习大会论文集，ICML
    2020，2020年7月13-18日，虚拟会议，机器学习研究论文集第119卷，第767–777页。PMLR。
- en: 'Benjamins et al. Benjamins, C., Eimer, T., Schubert, F., Biedenkapp, A., Rosenhahn,
    B., Hutter, F., and Lindauer, M. (2021). CARL: A Benchmark for Contextual and
    Adaptive Reinforcement Learning..'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benjamins 等 Benjamins, C., Eimer, T., Schubert, F., Biedenkapp, A., Rosenhahn,
    B., Hutter, F., 和 Lindauer, M. (2021). CARL：一个用于上下文和自适应强化学习的基准。
- en: 'Bertrán et al. Bertrán, M., Martínez, N., Phielipp, M., and Sapiro, G. (2020).
    Instance-based generalization in reinforcement learning. In Larochelle, H., Ranzato,
    M., Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertrán 等 Bertrán, M., Martínez, N., Phielipp, M., 和 Sapiro, G. (2020). 强化学习中的实例化泛化。在
    Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., 和 Lin, H. (编辑), 神经信息处理系统进展
    33：2020年神经信息处理系统年会论文集，NeurIPS 2020，2020年12月6-12日，虚拟会议。
- en: 'Biedenkapp et al. Biedenkapp, A., Bozkurt, H. F., Eimer, T., Hutter, F., and Lindauer,
    M. (2020). Dynamic Algorithm Configuration: Foundation of a New Meta-Algorithmic
    Framework.. ECAI 2020..'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biedenkapp 等人。Biedenkapp, A., Bozkurt, H. F., Eimer, T., Hutter, F., 和 Lindauer,
    M.（2020）。动态算法配置：新元算法框架的基础。ECAI 2020。
- en: Boutilier et al. Boutilier, C., Dearden, R., and Goldszmidt, M. (2000). Stochastic
    dynamic programming with factored representations.. Artificial Intelligence..
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boutilier 等人。Boutilier, C., Dearden, R., 和 Goldszmidt, M.（2000）。具有分解表示的随机动态规划。人工智能。
- en: Brockman et al. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman,
    J., Tang, J., and Zaremba, W. (2016). OpenAI Gym.. arXiv:1606.01540 [cs]..
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockman 等人。Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman,
    J., Tang, J., 和 Zaremba, W.（2016）。OpenAI Gym。arXiv:1606.01540 [cs]。
- en: 'Chen et al. Chen, A. S., Nair, S., and Finn, C. (2021). Learning Generalizable
    Robotic Reward Functions from "In-The-Wild" Human Videos.. arXiv:2103.16817 [cs].,
    Comment: https://sites.google.com/view/dvd-human-videos.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人。Chen, A. S., Nair, S., 和 Finn, C.（2021）。从“真实环境”中的人类视频学习可泛化的机器人奖励函数。arXiv:2103.16817
    [cs]，评论：https://sites.google.com/view/dvd-human-videos。
- en: 'Chen Chen, J. Z. (2020). Reinforcement Learning Generalization with Surprise
    Minimization.. arXiv:2004.12399 [cs]., Comment: Inductive biases, invariances
    and generalization in RL Workshop, ICML 2020.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen Chen, J. Z.（2020）。通过惊讶最小化进行强化学习泛化。arXiv:2004.12399 [cs]，评论：ICML 2020 的归纳偏差、不变性与泛化研讨会。
- en: Chen and Li Chen, S.,  and Li, Y. (2020). An Overview of Robust Reinforcement
    Learning. In 2020 IEEE International Conference on Networking, Sensing and Control
    (ICNSC), pp. 1–6.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Li Chen, S., 和 Li, Y.（2020）。鲁棒强化学习概述。在2020年IEEE国际网络、传感和控制会议（ICNSC），第1–6页。
- en: 'Chen et al. Chen, V., Gupta, A., and Marino, K. (2021). Ask Your Humans: Using
    Human Instructions to Improve Generalization in Reinforcement Learning.. arXiv:2011.00517
    [cs]..'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人。Chen, V., Gupta, A., 和 Marino, K.（2021）。询问你的“人类”：利用人类指令来提高强化学习中的泛化能力。arXiv:2011.00517
    [cs]。
- en: Chevalier-Boisvert Chevalier-Boisvert, M. (2021). Minimalistic Gridworld Environment
    (MiniGrid)..
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier-Boisvert Chevalier-Boisvert, M.（2021）。极简网格世界环境（MiniGrid）。
- en: 'Chevalier-Boisvert et al. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S.,
    Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. (2019). Babyai: A platform
    to study the sample efficiency of grounded language learning. In 7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019. OpenReview.net.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier-Boisvert 等人。Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems,
    L., Saharia, C., Nguyen, T. H., 和 Bengio, Y.（2019）。Babyai：一个用于研究有意义语言学习样本效率的平台。在第7届国际学习表征会议（ICLR
    2019），2019年5月6-9日，新奥尔良，美国。OpenReview.net。
- en: Cobbe et al. Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. (2020a). Leveraging
    procedural generation to benchmark reinforcement learning. In Proceedings of the
    37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,
    Virtual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 2048–2056.
    PMLR.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人。Cobbe, K., Hesse, C., Hilton, J., 和 Schulman, J.（2020a）。利用程序生成来基准测试强化学习。在第37届国际机器学习会议（ICML
    2020）论文集，2020年7月13-18日，虚拟会议，机器学习研究论文集第119卷，第2048–2056页。PMLR。
- en: Cobbe et al. Cobbe, K., Hilton, J., Klimov, O., and Schulman, J. (2020b). Phasic
    Policy Gradient.. arXiv:2009.04416 [cs, stat]..
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人。Cobbe, K., Hilton, J., Klimov, O., 和 Schulman, J.（2020b）。相位策略梯度。arXiv:2009.04416
    [cs, stat]。
- en: Cobbe et al. Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. (2019).
    Quantifying generalization in reinforcement learning. In Chaudhuri, K.,  and Salakhutdinov,
    R. (Eds.), Proceedings of the 36th International Conference on Machine Learning,
    ICML 2019, 9-15 June 2019, Long Beach, California, USA, Vol. 97 of Proceedings
    of Machine Learning Research, pp. 1282–1289\. PMLR.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人。Cobbe, K., Klimov, O., Hesse, C., Kim, T., 和 Schulman, J.（2019）。量化强化学习中的泛化能力。在
    Chaudhuri, K., 和 Salakhutdinov, R.（编辑），第36届国际机器学习会议（ICML 2019）论文集，2019年6月9-15日，加利福尼亚州长滩，美国，机器学习研究论文集第97卷，第1282–1289页。PMLR。
- en: 'Côté et al. Côté, M.-A., Kádár, Á., Yuan, X., Kybartas, B., Barnes, T., Fine,
    E., Moore, J., Tao, R. Y., Hausknecht, M., Asri, L. E., Adada, M., Tay, W., and Trischler,
    A. (2019). TextWorld: A Learning Environment for Text-based Games.. arXiv:1806.11532
    [cs, stat]., Comment: Presented at the Computer Games Workshop at IJCAI 2018,
    Stockholm.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Côté 等人。Côté, M.-A., Kádár, Á., Yuan, X., Kybartas, B., Barnes, T., Fine, E.,
    Moore, J., Tao, R. Y., Hausknecht, M., Asri, L. E., Adada, M., Tay, W., 和 Trischler,
    A.（2019）。TextWorld：一个用于文本游戏的学习环境。arXiv:1806.11532 [cs, stat]，评论：在IJCAI 2018计算机游戏研讨会上展示，斯德哥尔摩。
- en: Crosby et al. Crosby, M., Beyret, B., Shanahan, M., Hernández-Orallo, J., Cheke,
    L., and Halina, M. (2020). The Animal-AI Testbed and Competition. In Proceedings
    of the NeurIPS 2019 Competition and Demonstration Track, pp. 164–176\. PMLR.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crosby等人 Crosby, M., Beyret, B., Shanahan, M., Hernández-Orallo, J., Cheke,
    L., 和 Halina, M. (2020). 动物-AI测试平台与竞赛. 见于NeurIPS 2019竞赛与演示环节论文集，第164–176页\. PMLR.
- en: 'Dennis et al. Dennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell,
    S., Critch, A., and Levine, S. (2020). Emergent complexity and zero-shot transfer
    via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell,
    R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information Processing
    Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
    2020, December 6-12, 2020, virtual.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dennis等人 Dennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch,
    A., 和 Levine, S. (2020). 通过无监督环境设计的涌现复杂性和零样本迁移. 见于Larochelle, H., Ranzato, M.,
    Hadsell, R., Balcan, M., 和 Lin, H. (编), 神经信息处理系统进展33：2020年神经信息处理系统年度会议，NeurIPS
    2020，2020年12月6-12日，虚拟会议.
- en: 'Devlin et al. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
    BERT: Pre-training of deep bidirectional transformers for language understanding.
    In Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers), pp. 4171–4186, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin等人 Devlin, J., Chang, M.-W., Lee, K., 和 Toutanova, K. (2019). BERT：深度双向变换器的预训练用于语言理解.
    见于2019年北美计算语言学协会会议论文集：人类语言技术，第1卷（长短篇论文），第4171–4186页，明尼阿波利斯，明尼苏达州. 计算语言学协会.
- en: DeVries and Taylor DeVries, T.,  and Taylor, G. W. (2017). Improved Regularization
    of Convolutional Neural Networks with Cutout.. arXiv:1708.04552 [cs]..
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeVries和Taylor DeVries, T., 和 Taylor, G. W. (2017). 通过Cutout改进卷积神经网络的正则化.. arXiv:1708.04552
    [cs]..
- en: Diuk et al. Diuk, C., Cohen, A., and Littman, M. L. (2008). An object-oriented
    representation for efficient reinforcement learning. In Cohen, W. W., McCallum,
    A., and Roweis, S. T. (Eds.), Machine Learning, Proceedings of the Twenty-Fifth
    International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, Vol. 307
    of ACM International Conference Proceeding Series, pp. 240–247. ACM.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diuk等人 Diuk, C., Cohen, A., 和 Littman, M. L. (2008). 一种面向对象的高效强化学习表征. 见于Cohen,
    W. W., McCallum, A., 和 Roweis, S. T. (编), 机器学习，第二十五届国际会议论文集 (ICML 2008)，芬兰赫尔辛基，2008年6月5-9日，第307卷ACM国际会议论文集，第240–247页.
    ACM.
- en: Dorfman et al. Dorfman, R., Shenfeld, I., and Tamar, A. (2021). Offline Meta
    Learning of Exploration.. arXiv:2008.02598 [cs, stat]..
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorfman等人 Dorfman, R., Shenfeld, I., 和 Tamar, A. (2021). 离线元学习的探索.. arXiv:2008.02598
    [cs, stat]..
- en: 'Doshi-Velez and Konidaris Doshi-Velez, F.,  and Konidaris, G. D. (2016). Hidden
    parameter markov decision processes: A semiparametric regression approach for
    discovering latent task parametrizations. In Kambhampati, S. (Ed.), Proceedings
    of the Twenty-Fifth International Joint Conference on Artificial Intelligence,
    IJCAI 2016, New York, NY, USA, 9-15 July 2016, pp. 1432–1440\. IJCAI/AAAI Press.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doshi-Velez和Konidaris Doshi-Velez, F., 和 Konidaris, G. D. (2016). 隐含参数马尔可夫决策过程：一种用于发现潜在任务参数化的半参数回归方法.
    见于Kambhampati, S. (编), 第二十五届国际人工智能联合会议论文集，IJCAI 2016, 美国纽约，2016年7月9-15日，第1432–1440页\.
    IJCAI/AAAI Press.
- en: 'Dosovitskiy et al. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and Koltun,
    V. (2017). CARLA: An Open Urban Driving Simulator.. arXiv:1711.03938 [cs]., Comment:
    Published at the 1st Conference on Robot Learning (CoRL).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dosovitskiy等人 Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., 和 Koltun,
    V. (2017). CARLA: 一个开放的城市驾驶模拟器.. arXiv:1711.03938 [cs].，评论：发表于第1届机器人学习会议 (CoRL)。'
- en: Du et al. Du, S. S., Kakade, S. M., Wang, R., and Yang, L. F. (2020). Is a good
    representation sufficient for sample efficient reinforcement learning?. In 8th
    International Conference on Learning Representations, ICLR 2020, Addis Ababa,
    Ethiopia, April 26-30, 2020. OpenReview.net.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du等人 Du, S. S., Kakade, S. M., Wang, R., 和 Yang, L. F. (2020). 良好的表征是否足以实现样本高效的强化学习？.
    见于第8届国际学习表征会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日. OpenReview.net.
- en: Du et al. Du, S. S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudík, M., and Langford,
    J. (2019). Provably efficient RL with rich observations via latent state decoding.
    In Chaudhuri, K.,  and Salakhutdinov, R. (Eds.), Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA, Vol. 97 of Proceedings of Machine Learning Research, pp. 1665–1674\. PMLR.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du等人 Du, S. S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudík, M., 和 Langford,
    J. (2019). Provably efficient RL with rich observations via latent state decoding.
    见于 Chaudhuri, K., 和 Salakhutdinov, R. (主编), 第36届国际机器学习大会论文集, ICML 2019, 2019年6月9-15日,
    长滩，加利福尼亚，美国, 机器学习研究论文集第97卷, pp. 1665–1674\. PMLR.
- en: 'Duan et al. Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I.,
    and Abbeel, P. (2016). Rl $2̂$: Fast reinforcement learning via slow reinforcement
    learning.. arXiv preprint arXiv:1611.02779..'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duan等人 Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., 和
    Abbeel, P. (2016). Rl $2̂$: Fast reinforcement learning via slow reinforcement
    learning.. arXiv预印本 arXiv:1611.02779..'
- en: 'Dulac-Arnold et al. Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J.,
    Paduraru, C., Gowal, S., and Hester, T. (2021). An empirical investigation of
    the challenges of real-world reinforcement learning.. arXiv:2003.11881 [cs].,
    Comment: arXiv admin note: text overlap with arXiv:1904.12901.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dulac-Arnold等人 Dulac-Arnold, G., Levine, N., Mankowitz, D. J., Li, J., Paduraru,
    C., Gowal, S., 和 Hester, T. (2021). An empirical investigation of the challenges
    of real-world reinforcement learning.. arXiv:2003.11881 [cs]., 评论: arXiv管理员注:
    与arXiv:1904.12901文本重叠。'
- en: 'E. Todorov et al. E. Todorov, T. Erez, and Y. Tassa (2012). MuJoCo: A physics
    engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent
    Robots and Systems, pp. 5026–5033.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'E. Todorov等人 E. Todorov, T. Erez, 和 Y. Tassa (2012). MuJoCo: A physics engine
    for model-based control. 见于2012 IEEE/RSJ国际智能机器人与系统大会, pp. 5026–5033.'
- en: 'Eimer et al. Eimer, T., Biedenkapp, A., Reimer, M., Adriaensen, S., Hutter,
    F., and Lindauer, M. (2021). DACBench: A Benchmark Library for Dynamic Algorithm
    Configuration.. arXiv:2105.08541 [cs]., Comment: Accepted at IJCAI 2021.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Eimer等人 Eimer, T., Biedenkapp, A., Reimer, M., Adriaensen, S., Hutter, F.,
    和 Lindauer, M. (2021). DACBench: A Benchmark Library for Dynamic Algorithm Configuration..
    arXiv:2105.08541 [cs]., 评论: 被IJCAI 2021接受。'
- en: 'Eysenbach et al. Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. (2019).
    Diversity is all you need: Learning skills without a reward function. In 7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019. OpenReview.net.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Eysenbach等人 Eysenbach, B., Gupta, A., Ibarz, J., 和 Levine, S. (2019). Diversity
    is all you need: Learning skills without a reward function. 见于第7届国际学习表示会议, ICLR
    2019, 新奥尔良, LA, 美国, 2019年5月6-9日. OpenReview.net.'
- en: 'Eysenbach et al. Eysenbach, B., Salakhutdinov, R., and Levine, S. (2021). Robust
    Predictable Control.. arXiv:2109.03214 [cs]., Comment: Project site with videos
    and code: https://ben-eysenbach.github.io/rpc.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Eysenbach等人 Eysenbach, B., Salakhutdinov, R., 和 Levine, S. (2021). Robust Predictable
    Control.. arXiv:2109.03214 [cs]., 评论: 项目网站有视频和代码: [https://ben-eysenbach.github.io/rpc](https://ben-eysenbach.github.io/rpc)。'
- en: 'Fan and Li Fan, J.,  and Li, W. (2021). DRIBO: Robust Deep Reinforcement Learning
    via Multi-View Information Bottleneck.. arXiv:2102.13268 [cs]., Comment: 27 pages.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan和Li Fan, J., 和 Li, W. (2021). DRIBO: Robust Deep Reinforcement Learning
    via Multi-View Information Bottleneck.. arXiv:2102.13268 [cs]., 评论: 27页。'
- en: 'Fan et al. Fan, L., Wang, G., Huang, D.-A., Yu, Z., Fei-Fei, L., Zhu, Y., and Anandkumar,
    A. (2021). SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual
    Policies.. arXiv:2106.09678 [cs]., Comment: ICML 2021\. Website: https://linxifan.github.io/secant-site/.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan等人 Fan, L., Wang, G., Huang, D.-A., Yu, Z., Fei-Fei, L., Zhu, Y., 和 Anandkumar,
    A. (2021). SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual
    Policies.. arXiv:2106.09678 [cs]., 评论: ICML 2021\. 网站: [https://linxifan.github.io/secant-site/](https://linxifan.github.io/secant-site/)。'
- en: 'Farebrother et al. Farebrother, J., Machado, M. C., and Bowling, M. (2020).
    Generalization and Regularization in DQN.. arXiv:1810.00123 [cs, stat]., Comment:
    Earlier versions of this work were presented both at the NeurIPS’18 Deep Reinforcement
    Learning Workshop and the 4th Multidisciplinary Conference on Reinforcement Learning
    and Decision Making (RLDM’19).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Farebrother等人 Farebrother, J., Machado, M. C., 和 Bowling, M. (2020). Generalization
    and Regularization in DQN.. arXiv:1810.00123 [cs, stat]., 评论: 该工作的早期版本曾在NeurIPS’18深度强化学习研讨会和第4届多学科强化学习与决策会议（RLDM’19）上展示。'
- en: Filos et al. Filos, A., Tigkas, P., McAllister, R., Rhinehart, N., Levine, S.,
    and Gal, Y. (2020). Can autonomous vehicles identify, recover from, and adapt
    to distribution shifts?. In Proceedings of the 37th International Conference on
    Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings
    of Machine Learning Research, pp. 3145–3153. PMLR.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Filos等人，Filos, A., Tigkas, P., McAllister, R., Rhinehart, N., Levine, S., 和
    Gal, Y. (2020)。自动驾驶车辆能否识别、恢复和适应分布变化？在第37届国际机器学习大会（ICML 2020）论文集，2020年7月13-18日，虚拟会议，机器学习研究论文集第119卷，第3145-3153页。PMLR。
- en: Finn et al. Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning
    for fast adaptation of deep networks. In Precup, D.,  and Teh, Y. W. (Eds.), Proceedings
    of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,
    Australia, 6-11 August 2017, Vol. 70 of Proceedings of Machine Learning Research,
    pp. 1126–1135\. PMLR.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn等人，Finn, C., Abbeel, P., 和 Levine, S. (2017)。用于深度网络快速适应的模型无关元学习。在Precup,
    D. 和 Teh, Y. W. (编), 2017年国际机器学习大会（ICML 2017）论文集，悉尼，澳大利亚，新南威尔士州，2017年8月6-11日，机器学习研究论文集第70卷，第1126-1135页。PMLR。
- en: 'Fortunato et al. Fortunato, M., Tan, M., Faulkner, R., Hansen, S., Badia, A. P.,
    Buttimore, G., Deck, C., Leibo, J. Z., and Blundell, C. (2019). Generalization
    of reinforcement learners with working and episodic memory. In Wallach, H. M.,
    Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Garnett, R.
    (Eds.), Advances in Neural Information Processing Systems 32: Annual Conference
    on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
    Vancouver, BC, Canada, pp. 12448–12457.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortunato等人，Fortunato, M., Tan, M., Faulkner, R., Hansen, S., Badia, A. P.,
    Buttimore, G., Deck, C., Leibo, J. Z., 和 Blundell, C. (2019)。具有工作记忆和情节记忆的强化学习者的泛化。在Wallach,
    H. M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., 和 Garnett,
    R. (编), 神经信息处理系统第32届年会：2019年神经信息处理系统会议（NeurIPS 2019），2019年12月8-14日，加拿大温哥华，第12448-12457页。
- en: François-Lavet et al. François-Lavet, V., Bengio, Y., Precup, D., and Pineau,
    J. (2019). Combined reinforcement learning via abstract representations. In The
    Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First
    Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The
    Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI
    2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 3582–3589\. AAAI
    Press.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: François-Lavet等人，François-Lavet, V., Bengio, Y., Precup, D., 和 Pineau, J. (2019)。通过抽象表示进行的联合强化学习。在第三十三届AAAI人工智能会议（AAAI
    2019），第31届人工智能创新应用会议（IAAI 2019），第九届AAAI人工智能教育进展研讨会（EAAI 2019），美国夏威夷檀香山，2019年1月27日至2月1日，第3582-3589页。AAAI出版社。
- en: 'Fu et al. Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2021).
    D4RL: Datasets for Deep Data-Driven Reinforcement Learning.. arXiv:2004.07219
    [cs, stat]., Comment: Website available at https://sites.google.com/view/d4rl/home.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu等人，Fu, J., Kumar, A., Nachum, O., Tucker, G., 和 Levine, S. (2021)。D4RL：用于深度数据驱动强化学习的数据集。arXiv:2004.07219
    [cs, stat]。备注：网站访问链接为 https://sites.google.com/view/d4rl/home。
- en: Gamrian and Goldberg Gamrian, S.,  and Goldberg, Y. (2019). Transfer learning
    for related reinforcement learning tasks via image-to-image translation. In Chaudhuri,
    K.,  and Salakhutdinov, R. (Eds.), Proceedings of the 36th International Conference
    on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Vol. 97
    of Proceedings of Machine Learning Research, pp. 2063–2072\. PMLR.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamrian和Goldberg，Gamrian, S., 和 Goldberg, Y. (2019)。通过图像到图像的转换进行相关强化学习任务的迁移学习。在Chaudhuri,
    K. 和 Salakhutdinov, R. (编), 第36届国际机器学习大会（ICML 2019）论文集，2019年6月9-15日，美国加州长滩，机器学习研究论文集第97卷，第2063-2072页。PMLR。
- en: 'Ghosh et al. Ghosh, D., Rahme, J., Kumar, A., Zhang, A., Adams, R. P., and Levine,
    S. (2021). Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit
    Partial Observability.. arXiv:2107.06277 [cs, stat]., Comment: First two authors
    contributed equally.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghosh等人，Ghosh, D., Rahme, J., Kumar, A., Zhang, A., Adams, R. P., 和 Levine,
    S. (2021)。为什么RL中的泛化很困难：认知POMDPs和隐式部分可观测性。arXiv:2107.06277 [cs, stat]。备注：前两位作者贡献相等。
- en: 'Goel et al. Goel, S., Tatiya, G., Scheutz, M., and Sinapov, J. (2021). NovelGridworlds:
    A benchmark environment for detecting and adapting to novelties in open worlds..'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goel等人，Goel, S., Tatiya, G., Scheutz, M., 和 Sinapov, J. (2021)。NovelGridworlds：一个用于检测和适应开放世界新奇性的基准环境。
- en: 'Grigsby and Qi Grigsby, J.,  and Qi, Y. (2020). Measuring Visual Generalization
    in Continuous Control from Pixels.. arXiv:2010.06740 [cs]., Comment: A total of
    20 pages, 8 pages as the main text.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Grigsby, J., 和 Qi, Y. (2020). 从像素中测量连续控制的视觉泛化.. arXiv:2010.06740 [cs]., 评论:
    总共20页，8页为主要内容。'
- en: 'Gulcehre et al. Gulcehre, C., Wang, Z., Novikov, A., Paine, T. L., Colmenarejo,
    S. G., Zolna, K., Agarwal, R., Merel, J., Mankowitz, D., Paduraru, C., Dulac-Arnold,
    G., Li, J., Norouzi, M., Hoffman, M., Nachum, O., Tucker, G., Heess, N., and de
    Freitas, N. (2021). RL Unplugged: A Suite of Benchmarks for Offline Reinforcement
    Learning.. arXiv:2006.13888 [cs, stat]., Comment: NeurIPS paper. 21 pages including
    supplementary material, the github link for the datasets: https://github.com/deepmind/deepmind-research/rl_unplugged.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gulcehre, C., Wang, Z., Novikov, A., Paine, T. L., Colmenarejo, S. G., Zolna,
    K., Agarwal, R., Merel, J., Mankowitz, D., Paduraru, C., Dulac-Arnold, G., Li,
    J., Norouzi, M., Hoffman, M., Nachum, O., Tucker, G., Heess, N., 和 de Freitas,
    N. (2021). RL Unplugged: 离线强化学习的基准套件.. arXiv:2006.13888 [cs, stat]., 评论: NeurIPS
    论文。21页，包括补充材料，数据集的github链接: [https://github.com/deepmind/deepmind-research/rl_unplugged](https://github.com/deepmind/deepmind-research/rl_unplugged).'
- en: 'Hafner Hafner, D. (2021). Benchmarking the Spectrum of Agent Capabilities..
    arXiv:2109.06780 [cs]., Comment: Website: https://danijar.com/crafter.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hafner, D. (2021). 代理能力的谱评估.. arXiv:2109.06780 [cs]., 评论: 网站: [https://danijar.com/crafter](https://danijar.com/crafter).'
- en: Hallak et al. Hallak, A., Di Castro, D., and Mannor, S. (2015). Contextual Markov
    Decision Processes.. arXiv:1502.02259 [cs, stat]..
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hallak, A., Di Castro, D., 和 Mannor, S. (2015). 情境马尔可夫决策过程.. arXiv:1502.02259
    [cs, stat]..
- en: Han et al. Han, I., Park, D.-H., and Kim, K.-J. (2021). A New Open-Source Off-Road
    Environment for Benchmark Generalization of Autonomous Driving.. IEEE Access..
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han, I., Park, D.-H., 和 Kim, K.-J. (2021). 用于自动驾驶泛化基准的新开源越野环境.. IEEE Access..
- en: 'Hansen et al. Hansen, N., Jangir, R., Sun, Y., Alenyà, G., Abbeel, P., Efros,
    A. A., Pinto, L., and Wang, X. (2021a). Self-Supervised Policy Adaptation during
    Deployment.. arXiv:2007.04309 [cs, stat]., Comment: Website: https://nicklashansen.github.io/PAD/
    Code: https://github.com/nicklashansen/policy-adaptation-during-deployment ICLR
    2021.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hansen, N., Jangir, R., Sun, Y., Alenyà, G., Abbeel, P., Efros, A. A., Pinto,
    L., 和 Wang, X. (2021a). 部署期间的自监督策略适应.. arXiv:2007.04309 [cs, stat]., 评论: 网站: [https://nicklashansen.github.io/PAD/](https://nicklashansen.github.io/PAD/)
    代码: [https://github.com/nicklashansen/policy-adaptation-during-deployment](https://github.com/nicklashansen/policy-adaptation-during-deployment)
    ICLR 2021.'
- en: 'Hansen et al. Hansen, N., Su, H., and Wang, X. (2021b). Stabilizing Deep Q-Learning
    with ConvNets and Vision Transformers under Data Augmentation.. arXiv:2107.00644
    [cs]., Comment: Code and videos are available at https://nicklashansen.github.io/SVEA.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hansen, N., Su, H., 和 Wang, X. (2021b). 使用ConvNets和视觉变换器在数据增强下稳定深度Q学习.. arXiv:2107.00644
    [cs]., 评论: 代码和视频可在 [https://nicklashansen.github.io/SVEA](https://nicklashansen.github.io/SVEA)
    获取。'
- en: 'Hansen and Wang Hansen, N.,  and Wang, X. (2021). Generalization in Reinforcement
    Learning by Soft Data Augmentation.. arXiv:2011.13389 [cs]., Comment: Website:
    https://nicklashansen.github.io/SODA/ Code: https://github.com/nicklashansen/dmcontrol-generalization-benchmark.
    Presented at International Conference on Robotics and Automation (ICRA) 2021.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hansen, N., 和 Wang, X. (2021). 通过软数据增强实现强化学习的泛化.. arXiv:2011.13389 [cs]., 评论:
    网站: [https://nicklashansen.github.io/SODA/](https://nicklashansen.github.io/SODA/)
    代码: [https://github.com/nicklashansen/dmcontrol-generalization-benchmark](https://github.com/nicklashansen/dmcontrol-generalization-benchmark).
    在2021年国际机器人与自动化大会（ICRA）上展示。'
- en: Hao et al. Hao, B., Lattimore, T., Szepesvári, C., and Wang, M. (2021). Online
    sparse reinforcement learning. In Banerjee, A.,  and Fukumizu, K. (Eds.), The
    24th International Conference on Artificial Intelligence and Statistics, AISTATS
    2021, April 13-15, 2021, Virtual Event, Vol. 130 of Proceedings of Machine Learning
    Research, pp. 316–324\. PMLR.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao, B., Lattimore, T., Szepesvári, C., 和 Wang, M. (2021). 在线稀疏强化学习. 在 Banerjee,
    A., 和 Fukumizu, K. (编辑), 第24届人工智能与统计国际会议，AISTATS 2021，2021年4月13-15日，虚拟会议，机器学习研究会论文集第130卷，第316–324页。PMLR.
- en: 'Harries et al. Harries, L., Lee, S., Rzepecki, J., Hofmann, K., and Devlin,
    S. (2019). MazeExplorer: A Customisable 3D Benchmark for Assessing Generalisation
    in Reinforcement Learning. In 2019 IEEE Conference on Games (CoG), pp. 1–4.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Harries, L., Lee, S., Rzepecki, J., Hofmann, K., 和 Devlin, S. (2019). MazeExplorer:
    一个可定制的3D基准，用于评估强化学习中的泛化能力. 在2019年IEEE游戏大会（CoG），第1–4页。'
- en: 'Harrison et al. Harrison, J., Garg, A., Ivanovic, B., Zhu, Y., Savarese, S.,
    Fei-Fei, L., and Pavone, M. (2017). ADAPT: Zero-Shot Adaptive Policy Transfer
    for Stochastic Dynamical Systems.. arXiv:1707.04674 [cs]., Comment: International
    Symposium on Robotics Research (ISRR), 2017.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Harrison, J., Garg, A., Ivanovic, B., Zhu, Y., Savarese, S., Fei-Fei, L., 和
    Pavone, M. (2017). ADAPT: 针对随机动态系统的零样本自适应策略迁移.. arXiv:1707.04674 [cs]., 评论: 国际机器人研究研讨会（ISRR），2017.'
- en: 'Hausknecht et al. Hausknecht, M. J., Ammanabrolu, P., Côté, M., and Yuan, X.
    (2020). Interactive fiction games: A colossal adventure. In The Thirty-Fourth
    AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
    Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
    NY, USA, February 7-12, 2020, pp. 7903–7910\. AAAI Press.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hausknecht等人。Hausknecht, M. J., Ammanabrolu, P., Côté, M., 和 Yuan, X. (2020)。互动小说游戏：一场宏大的冒险。在第34届AAAI人工智能会议，AAAI
    2020，第32届创新人工智能应用会议，IAAI 2020，第十届AAAI教育进展研讨会，EAAI 2020，美国纽约，2020年2月7-12日，第7903–7910页。AAAI出版社。
- en: Henderson et al. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup,
    D., and Meger, D. (2018). Deep reinforcement learning that matters. In McIlraith,
    S. A.,  and Weinberger, K. Q. (Eds.), Proceedings of the Thirty-Second AAAI Conference
    on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial
    Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in
    Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
    2018, pp. 3207–3214\. AAAI Press.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson等人。Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., 和
    Meger, D. (2018)。深度强化学习的关键。在McIlraith, S. A., 和 Weinberger, K. Q. (编辑)，第32届AAAI人工智能会议（AAAI-18）、第30届创新人工智能应用会议（IAAI-18）和第八届AAAI教育进展研讨会（EAAI-18）论文集中，美国路易斯安那州新奥尔良，2018年2月2-7日，第3207–3214页。AAAI出版社。
- en: 'Higgins et al. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,
    Botvinick, M., Mohamed, S., and Lerchner, A. (2017a). beta-vae: Learning basic
    visual concepts with a constrained variational framework. In 5th International
    Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
    2017, Conference Track Proceedings. OpenReview.net.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Higgins等人。Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick,
    M., Mohamed, S., 和 Lerchner, A. (2017a)。beta-vae: 使用受限变分框架学习基本视觉概念。在第五届国际学习表征会议，ICLR
    2017，法国图盎，2017年4月24-26日，会议论文集。OpenReview.net。'
- en: 'Higgins et al. Higgins, I., Pal, A., Rusu, A. A., Matthey, L., Burgess, C.,
    Pritzel, A., Botvinick, M., Blundell, C., and Lerchner, A. (2017b). DARLA: improving
    zero-shot transfer in reinforcement learning. In Precup, D.,  and Teh, Y. W. (Eds.),
    Proceedings of the 34th International Conference on Machine Learning, ICML 2017,
    Sydney, NSW, Australia, 6-11 August 2017, Vol. 70 of Proceedings of Machine Learning
    Research, pp. 1480–1490\. PMLR.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Higgins等人。Higgins, I., Pal, A., Rusu, A. A., Matthey, L., Burgess, C., Pritzel,
    A., Botvinick, M., Blundell, C., 和 Lerchner, A. (2017b)。DARLA: 提升强化学习中的零样本迁移。在Precup,
    D., 和 Teh, Y. W. (编辑)，第34届国际机器学习会议，ICML 2017，澳大利亚新南威尔士州悉尼，2017年8月6-11日，机器学习研究论文集第70卷，第1480–1490页。PMLR。'
- en: Hill et al. Hill, F., Lampinen, A. K., Schneider, R., Clark, S., Botvinick,
    M., McClelland, J. L., and Santoro, A. (2020a). Environmental drivers of systematicity
    and generalization in a situated agent. In 8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill等人。Hill, F., Lampinen, A. K., Schneider, R., Clark, S., Botvinick, M., McClelland,
    J. L., 和 Santoro, A. (2020a)。情境代理中的系统性和泛化的环境驱动因素。第八届国际学习表征会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日。OpenReview.net。
- en: Hill et al. Hill, F., Mokra, S., Wong, N., and Harley, T. (2020b). Human Instruction-Following
    with Deep Reinforcement Learning via Transfer-Learning from Text.. arXiv:2005.09382
    [cs]..
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill等人。Hill, F., Mokra, S., Wong, N., 和 Harley, T. (2020b)。通过从文本中转移学习的深度强化学习进行人类指令跟随。arXiv:2005.09382
    [cs]。
- en: Hu et al. Hu, H., Lerer, A., Cui, B., Wu, D., Pineda, L., Brown, N., and Foerster,
    J. (2021). Off-Belief Learning.. arXiv:2103.04000 [cs]..
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等人。Hu, H., Lerer, A., Cui, B., Wu, D., Pineda, L., Brown, N., 和 Foerster,
    J. (2021)。离信念学习。arXiv:2103.04000 [cs]。
- en: Hu et al. Hu, H., Lerer, A., Peysakhovich, A., and Foerster, J. N. (2020). "other-play"
    for zero-shot coordination. In Proceedings of the 37th International Conference
    on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings
    of Machine Learning Research, pp. 4399–4410. PMLR.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等人。Hu, H., Lerer, A., Peysakhovich, A., 和 Foerster, J. N. (2020)。零样本协调的“其他玩法”。在第37届国际机器学习会议，ICML
    2020，2020年7月13-18日，虚拟会议，机器学习研究论文集第119卷，第4399–4410页。PMLR。
- en: 'Huang et al. Huang, B., Feng, F., Lu, C., Magliacane, S., and Zhang, K. (2021).
    AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning.. arXiv:2107.02729
    [cs, stat]..'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang等人。Huang, B., Feng, F., Lu, C., Magliacane, S., 和 Zhang, K. (2021)。AdaRL:
    在转移强化学习中应该、何处及如何适应。arXiv:2107.02729 [cs, stat]。'
- en: 'Hupkes et al. Hupkes, D., Dankers, V., Mul, M., and Bruni, E. (2020). Compositionality
    decomposed: How do neural networks generalise?.. arXiv:1908.08351 [cs, stat]..'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hupkes 等人 Hupkes, D., Dankers, V., Mul, M., 和 Bruni, E. (2020). 《组合性分解：神经网络如何进行泛化》。arXiv:1908.08351
    [cs, stat]。
- en: 'Igl et al. Igl, M., Ciosek, K., Li, Y., Tschiatschek, S., Zhang, C., Devlin,
    S., and Hofmann, K. (2019). Generalization in reinforcement learning with selective
    noise injection and information bottleneck. In Wallach, H. M., Larochelle, H.,
    Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Garnett, R. (Eds.), Advances
    in Neural Information Processing Systems 32: Annual Conference on Neural Information
    Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
    pp. 13956–13968.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Igl 等人 Igl, M., Ciosek, K., Li, Y., Tschiatschek, S., Zhang, C., Devlin, S.,
    和 Hofmann, K. (2019). 《通过选择性噪声注入和信息瓶颈在强化学习中的泛化》。在 Wallach, H. M., Larochelle,
    H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., 和 Garnett, R. (编辑), 《神经信息处理系统进展
    32：2019 年神经信息处理系统年会，NeurIPS 2019，2019 年 12 月 8-14 日，加拿大温哥华，第 13956–13968 页》。
- en: Igl et al. Igl, M., Farquhar, G., Luketina, J., Boehmer, W., and Whiteson, S.
    (2021). Transient Non-Stationarity and Generalisation in Deep Reinforcement Learning..
    arXiv:2006.05826 [cs, stat]..
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Igl 等人 Igl, M., Farquhar, G., Luketina, J., Boehmer, W., 和 Whiteson, S. (2021).
    《深度强化学习中的瞬态非平稳性与泛化》。arXiv:2006.05826 [cs, stat]。
- en: 'Irpan and Song Irpan, A.,  and Song, X. (2019). The Principle of Unchanged
    Optimality in Reinforcement Learning Generalization.. arXiv:1906.00336 [cs, stat].,
    Comment: Published at ICML 2019 Workshop "Understanding and Improving Generalization
    in Deep Learning".'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Irpan 和 Song Irpan, A., 和 Song, X. (2019). 《强化学习泛化中的不变最优性原则》。arXiv:1906.00336
    [cs, stat]。备注：发表于 ICML 2019 研讨会“理解与改进深度学习中的泛化”。
- en: Jain et al. Jain, A., Szot, A., and Lim, J. J. (2020). Generalization to new
    actions in reinforcement learning. In Proceedings of the 37th International Conference
    on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings
    of Machine Learning Research, pp. 4661–4672. PMLR.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等人 Jain, A., Szot, A., 和 Lim, J. J. (2020). 《强化学习中新动作的泛化》。在《第 37 届国际机器学习大会论文集，ICML
    2020，2020 年 7 月 13-18 日，虚拟会议，第 119 卷，机器学习研究论文集，第 4661–4672 页》。PMLR。
- en: 'James et al. James, S., Ma, Z., Arrojo, D. R., and Davison, A. J. (2019a).
    RLBench: The Robot Learning Benchmark & Learning Environment.. arXiv:1909.12271
    [cs]., Comment: Videos and code: https://sites.google.com/view/rlbench.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'James 等人 James, S., Ma, Z., Arrojo, D. R., 和 Davison, A. J. (2019a). 《RLBench:
    机器人学习基准和学习环境》。arXiv:1909.12271 [cs]。备注：视频和代码：[链接](https://sites.google.com/view/rlbench)。'
- en: 'James et al. James, S., Wohlhart, P., Kalakrishnan, M., Kalashnikov, D., Irpan,
    A., Ibarz, J., Levine, S., Hadsell, R., and Bousmalis, K. (2019b). Sim-to-real
    via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation
    networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR
    2019, Long Beach, CA, USA, June 16-20, 2019, pp. 12627–12637\. Computer Vision
    Foundation / IEEE.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: James 等人 James, S., Wohlhart, P., Kalakrishnan, M., Kalashnikov, D., Irpan,
    A., Ibarz, J., Levine, S., Hadsell, R., 和 Bousmalis, K. (2019b). 《通过模拟到模拟的数据高效机器人抓取：随机到经典适配网络》。在《IEEE
    计算机视觉与模式识别会议，CVPR 2019，2019 年 6 月 16-20 日，美国加州长滩，第 12627–12637 页》。计算机视觉基金会 / IEEE。
- en: 'Jiang et al. Jiang, M., Dennis, M., Parker-Holder, J., Foerster, J., Grefenstette,
    E., and Rocktäschel, T. (2021a). Replay-Guided Adversarial Environment Design..
    arXiv:2110.02439 [cs]., Comment: NeurIPS 2021.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 Jiang, M., Dennis, M., Parker-Holder, J., Foerster, J., Grefenstette,
    E., 和 Rocktäschel, T. (2021a). 《重放指导的对抗环境设计》。arXiv:2110.02439 [cs]。备注：NeurIPS
    2021。
- en: Jiang et al. Jiang, M., Grefenstette, E., and Rocktäschel, T. (2021b). Prioritized
    Level Replay.. arXiv:2010.03934 [cs]..
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 Jiang, M., Grefenstette, E., 和 Rocktäschel, T. (2021b). 《优先级级别重放》。arXiv:2010.03934
    [cs]。
- en: 'Jiang et al. Jiang, M., Luketina, J., Nardelli, N., Minervini, P., Torr, P. H.,
    Whiteson, S., and Rocktäschel, T. (2020). WordCraft: An environment for benchmarking
    commonsense agents. In Workshop on Language in Reinforcement Learning (LaRel).'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等人 Jiang, M., Luketina, J., Nardelli, N., Minervini, P., Torr, P. H.,
    Whiteson, S., 和 Rocktäschel, T. (2020). 《WordCraft: 用于基准测试常识代理的环境》。在《强化学习中的语言研讨会（LaRel）》中。'
- en: Johnson et al. Johnson, M., Hofmann, K., Hutton, T., and Bignell, D. (2016).
    The malmo platform for artificial intelligence experimentation. In Kambhampati,
    S. (Ed.), Proceedings of the Twenty-Fifth International Joint Conference on Artificial
    Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pp. 4246–4247\. IJCAI/AAAI
    Press.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等人 Johnson, M., Hofmann, K., Hutton, T., 和 Bignell, D. (2016). 《用于人工智能实验的
    Malmo 平台》。在 Kambhampati, S. (编辑), 《第二十五届国际联合人工智能大会论文集，IJCAI 2016，2016 年 7 月 9-15
    日，美国纽约，第 4246–4247 页》。IJCAI/AAAI 出版社。
- en: 'Juliani et al. Juliani, A., Khalifa, A., Berges, V., Harper, J., Teng, E.,
    Henry, H., Crespi, A., Togelius, J., and Lange, D. (2019). Obstacle tower: A generalization
    challenge in vision, control, and planning. In Kraus, S. (Ed.), Proceedings of
    the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI
    2019, Macao, China, August 10-16, 2019, pp. 2684–2691\. ijcai.org.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juliani 等人 Juliani, A., Khalifa, A., Berges, V., Harper, J., Teng, E., Henry,
    H., Crespi, A., Togelius, J., 和 Lange, D. (2019). 障碍塔：视觉、控制和规划中的泛化挑战。发表于 Kraus,
    S. (编), 第二十八届国际联合人工智能会议论文集，IJCAI 2019，中国澳门，2019 年 8 月 10-16 日，页码 2684–2691。ijcai.org。
- en: 'Kanagawa and Kaneko Kanagawa, Y.,  and Kaneko, T. (2019). Rogue-Gym: A New
    Challenge for Generalization in Reinforcement Learning.. arXiv:1904.08129 [cs,
    stat]., Comment: 8 pages, 14 figures, 4 tables, accepted to IEEE COG 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kanagawa 和 Kaneko Kanagawa, Y., 和 Kaneko, T. (2019). Rogue-Gym: 强化学习中泛化的新挑战。arXiv:1904.08129
    [cs, stat]。评论：8 页，14 张图，4 张表，已接受到 IEEE COG 2019。'
- en: 'Kansky et al. Kansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla,
    M., Lou, X., Dorfman, N., Sidor, S., Phoenix, D. S., and George, D. (2017). Schema
    networks: Zero-shot transfer with a generative causal model of intuitive physics.
    In Precup, D.,  and Teh, Y. W. (Eds.), Proceedings of the 34th International Conference
    on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, Vol. 70
    of Proceedings of Machine Learning Research, pp. 1809–1818\. PMLR.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kansky 等人 Kansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla,
    M., Lou, X., Dorfman, N., Sidor, S., Phoenix, D. S., 和 George, D. (2017). Schema
    网络：利用直观物理的生成因果模型进行零样本迁移。发表于 Precup, D., 和 Teh, Y. W. (编), 第 34 届国际机器学习会议论文集，ICML
    2017，澳大利亚悉尼，2017 年 8 月 6-11 日，机器学习研究论文集第 70 卷，页码 1809–1818。PMLR。
- en: Katz et al. Katz, G., Huang, D. A., Ibeling, D., Julian, K., Lazarus, C., Lim,
    R., Shah, P., Thakoor, S., Wu, H., Zeljić, A., Dill, D. L., Kochenderfer, M. J.,
    and Barrett, C. (2019). The Marabou Framework for Verification and Analysis of
    Deep Neural Networks. In Dillig, I.,  and Tasiran, S. (Eds.), Computer Aided Verification,
    Lecture Notes in Computer Science, pp. 443–452, Cham. Springer International Publishing.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katz 等人 Katz, G., Huang, D. A., Ibeling, D., Julian, K., Lazarus, C., Lim, R.,
    Shah, P., Thakoor, S., Wu, H., Zeljić, A., Dill, D. L., Kochenderfer, M. J., 和
    Barrett, C. (2019). Marabou 框架用于深度神经网络的验证与分析。发表于 Dillig, I., 和 Tasiran, S. (编),
    计算机辅助验证，计算机科学讲义，第 443–452 页，Cham。Springer 国际出版。
- en: Ke et al. Ke, N. R., Dasgupta, I., Chiappa, S., Goyal, A., Weber, T., Mitrovic,
    J., Hill, F., Chan, S. C. Y., Mozer, M. C., Rezende, D. J., and Kohli, P. (2021).
    Parametric Generalization for Benchmarking Reinforcement Learning Algorithms..
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等人 Ke, N. R., Dasgupta, I., Chiappa, S., Goyal, A., Weber, T., Mitrovic,
    J., Hill, F., Chan, S. C. Y., Mozer, M. C., Rezende, D. J., 和 Kohli, P. (2021).
    基准测试强化学习算法的参数泛化。
- en: 'Kemertas and Aumentado-Armstrong Kemertas, M.,  and Aumentado-Armstrong, T.
    (2021). Towards Robust Bisimulation Metric Learning.. arXiv:2110.14096 [cs].,
    Comment: Accepted to NeurIPS 2021.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kemertas 和 Aumentado-Armstrong Kemertas, M., 和 Aumentado-Armstrong, T. (2021).
    朝着稳健的双模拟度度量学习前进。arXiv:2110.14096 [cs]。评论：已接受到 NeurIPS 2021。
- en: 'Kempka et al. Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and Jaśkowski,
    W. (2016). ViZDoom: A Doom-based AI research platform for visual reinforcement
    learning. In IEEE Conference on Computational Intelligence and Games, pp. 341–348,
    Santorini, Greece. IEEE. The best paper award.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kempka 等人 Kempka, M., Wydmuch, M., Runc, G., Toczek, J., 和 Jaśkowski, W. (2016).
    ViZDoom: 基于 Doom 的视觉强化学习 AI 研究平台。发表于 IEEE 计算智能与游戏会议，页码 341–348，希腊圣托里尼。IEEE。最佳论文奖。'
- en: 'Keysers et al. Keysers, D., Schärli, N., Scales, N., Buisman, H., Furrer, D.,
    Kashubin, S., Momchev, N., Sinopalnikov, D., Stafiniak, L., Tihon, T., Tsarkov,
    D., Wang, X., van Zee, M., and Bousquet, O. (2020). Measuring compositional generalization:
    A comprehensive method on realistic data. In 8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keysers 等人 Keysers, D., Schärli, N., Scales, N., Buisman, H., Furrer, D., Kashubin,
    S., Momchev, N., Sinopalnikov, D., Stafiniak, L., Tihon, T., Tsarkov, D., Wang,
    X., van Zee, M., 和 Bousquet, O. (2020). 衡量组合泛化：一种针对真实数据的综合方法。发表于第 8 届国际学习表征会议，ICLR
    2020，埃塞俄比亚亚的斯亚贝巴，2020 年 4 月 26-30 日。OpenReview.net。
- en: 'Khetarpal et al. Khetarpal, K., Riemer, M., Rish, I., and Precup, D. (2020).
    Towards Continual Reinforcement Learning: A Review and Perspectives.. arXiv:2012.13490
    [cs]., Comment: Preprint, 52 pages, 8 figures.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khetarpal 等人 Khetarpal, K., Riemer, M., Rish, I., 和 Precup, D. (2020). 朝向持续强化学习：综述与展望。arXiv:2012.13490
    [cs]。评论：预印本，52 页，8 张图。
- en: Ko and Ok Ko, B.,  and Ok, J. (2021). Time Matters in Using Data Augmentation
    for Vision-based Deep Reinforcement Learning.. arXiv:2102.08581 [cs]..
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ko and Ok Ko, B., 和 Ok, J. (2021). Time Matters in Using Data Augmentation for
    Vision-based Deep Reinforcement Learning.. arXiv:2102.08581 [cs]..
- en: 'Kostrikov et al. Kostrikov, I., Yarats, D., and Fergus, R. (2021). Image Augmentation
    Is All You Need: Regularizing Deep Reinforcement Learning from Pixels.. arXiv:2004.13649
    [cs, eess, stat]..'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kostrikov et al. Kostrikov, I., Yarats, D., 和 Fergus, R. (2021). Image Augmentation
    Is All You Need: Regularizing Deep Reinforcement Learning from Pixels.. arXiv:2004.13649
    [cs, eess, stat]..'
- en: 'Koutras et al. Koutras, D. I., Kapoutsis, A. C., Amanatiadis, A. A., and Kosmatopoulos,
    E. B. (2021). MarsExplorer: Exploration of Unknown Terrains via Deep Reinforcement
    Learning and Procedurally Generated Environments.. arXiv:2107.09996 [cs]..'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koutras et al. Koutras, D. I., Kapoutsis, A. C., Amanatiadis, A. A., 和 Kosmatopoulos,
    E. B. (2021). MarsExplorer: Exploration of Unknown Terrains via Deep Reinforcement
    Learning and Procedurally Generated Environments.. arXiv:2107.09996 [cs]..'
- en: 'Kumar et al. Kumar, A., Fu, Z., Pathak, D., and Malik, J. (2021). RMA: Rapid
    Motor Adaptation for Legged Robots.. arXiv:2107.04034 [cs]., Comment: RSS 2021\.
    Webpage at https://ashish-kmr.github.io/rma-legged-robots/.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumar et al. Kumar, A., Fu, Z., Pathak, D., 和 Malik, J. (2021). RMA: Rapid
    Motor Adaptation for Legged Robots.. arXiv:2107.04034 [cs]., 评论: RSS 2021\. 网页地址
    https://ashish-kmr.github.io/rma-legged-robots/.'
- en: 'Kumar et al. Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative
    q-learning for offline reinforcement learning. In Larochelle, H., Ranzato, M.,
    Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information Processing
    Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
    2020, December 6-12, 2020, virtual.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumar et al. Kumar, A., Zhou, A., Tucker, G., 和 Levine, S. (2020). Conservative
    q-learning for offline reinforcement learning. 在 Larochelle, H., Ranzato, M.,
    Hadsell, R., Balcan, M., 和 Lin, H. (编辑), Advances in Neural Information Processing
    Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
    2020, December 6-12, 2020, virtual.'
- en: 'Küttler et al. Küttler, H., Nardelli, N., Miller, A. H., Raileanu, R., Selvatici,
    M., Grefenstette, E., and Rocktäschel, T. (2020). The nethack learning environment.
    In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Küttler et al. Küttler, H., Nardelli, N., Miller, A. H., Raileanu, R., Selvatici,
    M., Grefenstette, E., 和 Rocktäschel, T. (2020). The nethack learning environment.
    在 Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., 和 Lin, H. (编辑), Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.'
- en: 'Lee et al. Lee, K., Lee, K., Shin, J., and Lee, H. (2020). Network randomization:
    A simple technique for generalization in deep reinforcement learning. In 8th International
    Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
    26-30, 2020. OpenReview.net.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. Lee, K., Lee, K., Shin, J., 和 Lee, H. (2020). Network randomization:
    A simple technique for generalization in deep reinforcement learning. 在第8届国际学习表示会议，ICLR
    2020，埃塞俄比亚，亚的斯亚贝巴，2020年4月26-30日。OpenReview.net.'
- en: 'Levine et al. Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Offline
    Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems..
    arXiv:2005.01643 [cs, stat]..'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Levine et al. Levine, S., Kumar, A., Tucker, G., 和 Fu, J. (2020). Offline Reinforcement
    Learning: Tutorial, Review, and Perspectives on Open Problems.. arXiv:2005.01643
    [cs, stat]..'
- en: Li et al. Li, B., François-Lavet, V., Doan, T., and Pineau, J. (2021a). Domain
    Adversarial Reinforcement Learning.. arXiv:2102.07097 [cs]..
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. Li, B., François-Lavet, V., Doan, T., 和 Pineau, J. (2021a). Domain
    Adversarial Reinforcement Learning.. arXiv:2102.07097 [cs]..
- en: 'Li et al. Li, Q., Peng, Z., Xue, Z., Zhang, Q., and Zhou, B. (2021b). MetaDrive:
    Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning..'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. Li, Q., Peng, Z., Xue, Z., Zhang, Q., 和 Zhou, B. (2021b). MetaDrive:
    Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning..'
- en: Liu et al. Liu, G. T., Cheng, P.-J., and Lin, G. (2020). Cross-State Self-Constraint
    for Feature Generalization in Deep Reinforcement Learning..
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. Liu, G. T., Cheng, P.-J., 和 Lin, G. (2020). Cross-State Self-Constraint
    for Feature Generalization in Deep Reinforcement Learning..
- en: 'Lomonaco et al. Lomonaco, V., Desai, K., Culurciello, E., and Maltoni, D. (2020).
    Continual Reinforcement Learning in 3D Non-stationary Environments.. arXiv:1905.10112
    [cs, stat]., Comment: Accepted in the CLVision Workshop at CVPR2020: 13 pages,
    4 figures, 5 tables.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lomonaco et al. Lomonaco, V., Desai, K., Culurciello, E., 和 Maltoni, D. (2020).
    Continual Reinforcement Learning in 3D Non-stationary Environments.. arXiv:1905.10112
    [cs, stat]., 评论: 已接受于 CVPR2020 的 CLVision 研讨会: 13 pages, 4 figures, 5 tables.'
- en: 'Lu et al. Lu, X., Lee, K., Abbeel, P., and Tiomkin, S. (2020). Dynamics Generalization
    via Information Bottleneck in Deep Reinforcement Learning.. arXiv:2008.00614 [cs,
    stat]., Comment: 16 pages.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu et al. Lu, X., Lee, K., Abbeel, P., 和 Tiomkin, S. (2020). Dynamics Generalization
    via Information Bottleneck in Deep Reinforcement Learning.. arXiv:2008.00614 [cs,
    stat]., 评论: 16 pages.'
- en: Luketina et al. Luketina, J., Nardelli, N., Farquhar, G., Foerster, J. N., Andreas,
    J., Grefenstette, E., Whiteson, S., and Rocktäschel, T. (2019). A survey of reinforcement
    learning informed by natural language. In Kraus, S. (Ed.), Proceedings of the
    Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI
    2019, Macao, China, August 10-16, 2019, pp. 6309–6317\. ijcai.org.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luketina 等 Luketina, J., Nardelli, N., Farquhar, G., Foerster, J. N., Andreas,
    J., Grefenstette, E., Whiteson, S., 和 Rocktäschel, T. (2019)。**自然语言启发的强化学习调查**。在
    Kraus, S. (编)，《第二十八届国际联合人工智能会议论文集》，IJCAI 2019，澳门，中国，2019年8月10-16日，pp. 6309–6317。ijcai.org。
- en: 'Lynch and Sermanet Lynch, C.,  and Sermanet, P. (2021). Language Conditioned
    Imitation Learning over Unstructured Data.. arXiv:2005.07648 [cs]., Comment: Published
    at RSS 2021.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lynch 和 Sermanet Lynch, C., 和 Sermanet, P. (2021)。**基于语言条件的模仿学习与非结构化数据**。arXiv:2005.07648
    [cs]，评论：发表于 RSS 2021。
- en: 'Machado et al. Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J.,
    Hausknecht, M., and Bowling, M. (2017). Revisiting the Arcade Learning Environment:
    Evaluation Protocols and Open Problems for General Agents.. arXiv:1709.06009 [cs]..'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Machado 等 Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht,
    M., 和 Bowling, M. (2017)。**重新审视街机学习环境：评估协议和通用代理的开放问题**。arXiv:1709.06009 [cs]。
- en: 'Malik et al. Malik, D., Li, Y., and Ravikumar, P. (2021). When Is Generalizable
    Reinforcement Learning Tractable?.. arXiv:2101.00300 [cs, stat]., Comment: v2
    extends results to function approximation setting.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malik 等 Malik, D., Li, Y., 和 Ravikumar, P. (2021)。**何时可推广的强化学习是可处理的**？arXiv:2101.00300
    [cs, stat]，评论：v2 扩展了结果到函数近似设置。
- en: Mankowitz et al. Mankowitz, D. J., Levine, N., Jeong, R., Abdolmaleki, A., Springenberg,
    J. T., Shi, Y., Kay, J., Hester, T., Mann, T. A., and Riedmiller, M. A. (2020).
    Robust reinforcement learning for continuous control with model misspecification.
    In 8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mankowitz 等 Mankowitz, D. J., Levine, N., Jeong, R., Abdolmaleki, A., Springenberg,
    J. T., Shi, Y., Kay, J., Hester, T., Mann, T. A., 和 Riedmiller, M. A. (2020)。**用于连续控制的鲁棒强化学习与模型误设定**。在第八届国际学习表示会议，ICLR
    2020，亚的斯亚贝巴，埃塞俄比亚，2020年4月26-30日。OpenReview.net。
- en: Mausam Mausam, D. S. W. (2003). Solving relational MDPs with first-order machine
    learning. In IN PROC. ICAPS WORKSHOP ON PLANNING UNDER UNCERTAINTY AND INCOMPLETE
    INFORMATION. Citeseer.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mausam Mausam, D. S. W. (2003)。**利用一阶机器学习解决关系型 MDPs**。在 ICAPS 计划不确定性和信息不完全研讨会中。Citeseer。
- en: Mazoure et al. Mazoure, B., Ahmed, A. M., MacAlpine, P., Hjelm, R. D., and Kolobov,
    A. (2021). Cross-Trajectory Representation Learning for Zero-Shot Generalization
    in RL.. arXiv:2106.02193 [cs]..
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mazoure 等 Mazoure, B., Ahmed, A. M., MacAlpine, P., Hjelm, R. D., 和 Kolobov,
    A. (2021)。**用于零样本泛化的跨轨迹表示学习**。arXiv:2106.02193 [cs]。
- en: Milani et al. Milani, S., Topin, N., Veloso, M., and Fang, F. (2022). A Survey
    of Explainable Reinforcement Learning..
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Milani 等 Milani, S., Topin, N., Veloso, M., 和 Fang, F. (2022)。**可解释强化学习的调查**。
- en: Mishra et al. Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. (2018).
    A simple neural attentive meta-learner. In 6th International Conference on Learning
    Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
    Track Proceedings. OpenReview.net.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等 Mishra, N., Rohaninejad, M., Chen, X., 和 Abbeel, P. (2018)。**简单的神经关注元学习器**。在第六届国际学习表示会议，ICLR
    2018，温哥华，加拿大，2018年4月30日 - 5月3日，会议论文集。OpenReview.net。
- en: Morimoto and Doya Morimoto, J.,  and Doya, K. (2000). Robust reinforcement learning.
    In Leen, T. K., Dietterich, T. G., and Tresp, V. (Eds.), Advances in Neural Information
    Processing Systems 13, Papers from Neural Information Processing Systems (NIPS)
    2000, Denver, CO, USA, pp. 1061–1067\. MIT Press.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morimoto 和 Doya Morimoto, J.，和 Doya, K. (2000)。**鲁棒的强化学习**。在 Leen, T. K., Dietterich,
    T. G., 和 Tresp, V. (编)，《神经信息处理系统进展 13》，来自神经信息处理系统（NIPS）2000 的论文，丹佛，美国，pp. 1061–1067。MIT
    Press。
- en: 'Müller-Brockhausen et al. Müller-Brockhausen, M., Preuss, M., and Plaat, A.
    (2021). Procedural Content Generation: Better Benchmarks for Transfer Reinforcement
    Learning.. arXiv:2105.14780 [cs]..'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Müller-Brockhausen 等 Müller-Brockhausen, M., Preuss, M., 和 Plaat, A. (2021)。**程序化内容生成：更好的转移强化学习基准**。arXiv:2105.14780
    [cs]。
- en: Nagabandi et al. Nagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel,
    P., Levine, S., and Finn, C. (2019a). Learning to adapt in dynamic, real-world
    environments through meta-reinforcement learning. In 7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagabandi 等人，Nagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel, P.,
    Levine, S., 和 Finn, C. (2019a). 通过元强化学习在动态现实环境中学习适应。在第七届国际学习表征会议，ICLR 2019，美国路易斯安那州新奥尔良，2019年5月6-9日。OpenReview.net。
- en: 'Nagabandi et al. Nagabandi, A., Finn, C., and Levine, S. (2019b). Deep online
    learning via meta-learning: Continual adaptation for model-based RL. In 7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019. OpenReview.net.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagabandi 等人，Nagabandi, A., Finn, C., 和 Levine, S. (2019b). 通过元学习进行深度在线学习：基于模型的强化学习的持续适应。在第七届国际学习表征会议，ICLR
    2019，美国路易斯安那州新奥尔良，2019年5月6-9日。OpenReview.net。
- en: 'Nair et al. Nair, A., Gupta, A., Dalal, M., and Levine, S. (2021). AWAC: Accelerating
    Online Reinforcement Learning with Offline Datasets.. arXiv:2006.09359 [cs, stat].,
    Comment: 17 pages. Website: https://awacrl.github.io/.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nair 等人，Nair, A., Gupta, A., Dalal, M., 和 Levine, S. (2021). AWAC: 利用离线数据集加速在线强化学习。arXiv:2006.09359
    [cs, stat]。评论：17页。网站：https://awacrl.github.io/。'
- en: 'Narvekar et al. Narvekar, S., Peng, B., Leonetti, M., Sinapov, J., Taylor,
    M. E., and Stone, P. (2020). Curriculum Learning for Reinforcement Learning Domains:
    A Framework and Survey.. arXiv:2003.04960 [cs, stat]..'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narvekar 等人，Narvekar, S., Peng, B., Leonetti, M., Sinapov, J., Taylor, M. E.,
    和 Stone, P. (2020). 强化学习领域的课程学习：一个框架和调查。arXiv:2003.04960 [cs, stat]。
- en: Ng and Russell Ng, A. Y.,  and Russell, S. J. (2000). Algorithms for inverse
    reinforcement learning. In Langley, P. (Ed.), Proceedings of the Seventeenth International
    Conference on Machine Learning (ICML 2000), Stanford University, Stanford, CA,
    USA, June 29 - July 2, 2000, pp. 663–670. Morgan Kaufmann.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 和 Russell，Ng, A. Y., 和 Russell, S. J. (2000). 逆向强化学习算法。在 Langley, P. (编),
    第十七届国际机器学习会议论文集 (ICML 2000), 美国加州斯坦福大学, 2000年6月29日至7月2日，第663–670页。Morgan Kaufmann。
- en: Ni et al. Ni, T., Eysenbach, B., and Salakhutdinov, R. (2021). Recurrent Model-Free
    RL is a Strong Baseline for Many POMDPs.. arXiv:2110.05038 [cs]..
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni 等人，Ni, T., Eysenbach, B., 和 Salakhutdinov, R. (2021). 循环无模型强化学习是许多 POMDP
    的强基线。arXiv:2110.05038 [cs]。
- en: OpenAI et al. OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M.,
    McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., Schneider,
    J., Tezak, N., Tworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba, W., and Zhang,
    L. (2019a). Solving Rubik’s Cube with a Robot Hand.. arXiv:1910.07113 [cs, stat]..
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人，OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew,
    B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., Schneider, J.,
    Tezak, N., Tworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba, W., 和 Zhang,
    L. (2019a). 用机器人手解决魔方。arXiv:1910.07113 [cs, stat]。
- en: OpenAI et al. OpenAI, Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak,
    P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R.,
    Gray, S., Olsson, C., Pachocki, J., Petrov, M., Pinto, H. P. d. O., Raiman, J.,
    Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J.,
    Wolski, F., and Zhang, S. (2019b). Dota 2 with Large Scale Deep Reinforcement
    Learning.. arXiv:1912.06680 [cs, stat]..
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人，OpenAI, Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P.,
    Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray,
    S., Olsson, C., Pachocki, J., Petrov, M., Pinto, H. P. d. O., Raiman, J., Salimans,
    T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski,
    F., 和 Zhang, S. (2019b). 使用大规模深度强化学习的 Dota 2。arXiv:1912.06680 [cs, stat]。
- en: 'OpenAI OpenAI, O. (2016). OpenAI Gym: The DoomTakeCover-v0 environment. https://gym.openai.com/envs/DoomTakeCover-v0.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI，OpenAI, O. (2016). OpenAI Gym: The DoomTakeCover-v0 环境。https://gym.openai.com/envs/DoomTakeCover-v0。'
- en: Osband et al. Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E.,
    Saraiva, A., McKinney, K., Lattimore, T., Szepesvári, C., Singh, S., Roy, B. V.,
    Sutton, R. S., Silver, D., and van Hasselt, H. (2020). Behaviour suite for reinforcement
    learning. In 8th International Conference on Learning Representations, ICLR 2020,
    Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband 等人，Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva,
    A., McKinney, K., Lattimore, T., Szepesvári, C., Singh, S., Roy, B. V., Sutton,
    R. S., Silver, D., 和 van Hasselt, H. (2020). 强化学习行为套件。在第八届国际学习表征会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日。OpenReview.net。
- en: 'Osband and Roy Osband, I.,  and Roy, B. V. (2014). Near-optimal reinforcement
    learning in factored mdps. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence,
    N. D., and Weinberger, K. Q. (Eds.), Advances in Neural Information Processing
    Systems 27: Annual Conference on Neural Information Processing Systems 2014, December
    8-13 2014, Montreal, Quebec, Canada, pp. 604–612.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband 和 Roy。Osband, I., 和 Roy, B. V. (2014)。在因子MDP中的近似最优强化学习。见 Ghahramani,
    Z., Welling, M., Cortes, C., Lawrence, N. D., 和 Weinberger, K. Q. (编)，《神经信息处理系统27：2014年神经信息处理系统年会论文集》，2014年12月8-13日，加拿大蒙特利尔，页码604–612。
- en: 'Packer et al. Packer, C., Gao, K., Kos, J., Krähenbühl, P., Koltun, V., and Song,
    D. (2019). Assessing Generalization in Deep Reinforcement Learning.. arXiv:1810.12282
    [cs, stat]., Comment: 17 pages, 6 figures.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Packer 等人。Packer, C., Gao, K., Kos, J., Krähenbühl, P., Koltun, V., 和 Song,
    D. (2019)。评估深度强化学习中的泛化能力。arXiv:1810.12282 [cs, stat]。注：17页，6幅图。
- en: Peng et al. Peng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2018).
    Sim-to-Real Transfer of Robotic Control with Dynamics Randomization.. 2018 IEEE
    International Conference on Robotics and Automation (ICRA)..
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人。Peng, X. B., Andrychowicz, M., Zaremba, W., 和 Abbeel, P. (2018)。机器人控制的仿真到现实转移与动态随机化。2018
    IEEE国际机器人与自动化会议（ICRA）。
- en: Perez et al. Perez, C., Such, F., and Karaletsos, T. (2020). Generalized Hidden
    Parameter MDPs:Transferable Model-Based RL in a Handful of Trials.. Proceedings
    of the AAAI Conference on Artificial Intelligence..
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 等人。Perez, C., Such, F., 和 Karaletsos, T. (2020)。广义隐藏参数MDP：少量试验中的可转移模型基础强化学习。《AAAI人工智能会议论文集》。
- en: 'Perez-Liebana et al. Perez-Liebana, D., Liu, J., Khalifa, A., Gaina, R. D.,
    Togelius, J., and Lucas, S. M. (2019). General Video Game AI: A Multi-Track Framework
    for Evaluating Agents, Games and Content Generation Algorithms.. arXiv:1802.10363
    [cs]., Comment: 20 pages, 1 figure, accepted by IEEE ToG.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez-Liebana 等人。Perez-Liebana, D., Liu, J., Khalifa, A., Gaina, R. D., Togelius,
    J., 和 Lucas, S. M. (2019)。通用视频游戏人工智能：用于评估代理、游戏和内容生成算法的多轨框架。arXiv:1802.10363 [cs]。注：20页，1幅图，IEEE
    ToG 接受。
- en: Pinto et al. Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A. (2017).
    Robust adversarial reinforcement learning. In Precup, D.,  and Teh, Y. W. (Eds.),
    Proceedings of the 34th International Conference on Machine Learning, ICML 2017,
    Sydney, NSW, Australia, 6-11 August 2017, Vol. 70 of Proceedings of Machine Learning
    Research, pp. 2817–2826\. PMLR.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinto 等人。Pinto, L., Davidson, J., Sukthankar, R., 和 Gupta, A. (2017)。鲁棒对抗强化学习。见
    Precup, D., 和 Teh, Y. W. (编)，《第34届国际机器学习会议论文集》，ICML 2017，2017年8月6-11日，澳大利亚新南威尔士州悉尼，机器学习研究论文集第70卷，页码2817–2826。PMLR。
- en: 'Portelas et al. Portelas, R., Colas, C., Weng, L., Hofmann, K., and Oudeyer,
    P. (2020). Automatic curriculum learning for deep RL: A short survey. In Bessiere,
    C. (Ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial
    Intelligence, IJCAI 2020, pp. 4819–4825\. ijcai.org.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Portelas 等人。Portelas, R., Colas, C., Weng, L., Hofmann, K., 和 Oudeyer, P. (2020)。深度强化学习的自动化课程学习：简短综述。见
    Bessiere, C. (编)，《第二十九届国际人工智能联合会议论文集》，IJCAI 2020，页码4819–4825。ijcai.org。
- en: 'Powers et al. Powers, S., Xing, E., Kolve, E., Mottaghi, R., and Gupta, A.
    (2021). CORA: Benchmarks, Baselines, and Metrics as a Platform for Continual Reinforcement
    Learning Agents.. arXiv:2110.10067 [cs]., Comment: Repository available at https://github.com/AGI-Labs/continual_rl.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Powers 等人。Powers, S., Xing, E., Kolve, E., Mottaghi, R., 和 Gupta, A. (2021)。CORA：基准、基线和指标作为持续强化学习代理的平台。arXiv:2110.10067
    [cs]。注：可在 https://github.com/AGI-Labs/continual_rl 找到仓库。
- en: 'Racanière et al. Racanière, S., Weber, T., Reichert, D. P., Buesing, L., Guez,
    A., Rezende, D. J., Badia, A. P., Vinyals, O., Heess, N., Li, Y., Pascanu, R.,
    Battaglia, P. W., Hassabis, D., Silver, D., and Wierstra, D. (2017). Imagination-augmented
    agents for deep reinforcement learning. In Guyon, I., von Luxburg, U., Bengio,
    S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (Eds.),
    Advances in Neural Information Processing Systems 30: Annual Conference on Neural
    Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,
    pp. 5690–5701.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Racanière 等人。Racanière, S., Weber, T., Reichert, D. P., Buesing, L., Guez, A.,
    Rezende, D. J., Badia, A. P., Vinyals, O., Heess, N., Li, Y., Pascanu, R., Battaglia,
    P. W., Hassabis, D., Silver, D., 和 Wierstra, D. (2017)。用于深度强化学习的想象增强代理。见 Guyon,
    I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S.
    V. N., 和 Garnett, R. (编)，《神经信息处理系统30：2017年神经信息处理系统年会论文集》，2017年12月4-9日，美国加州长滩，页码5690–5701。
- en: Raileanu and Fergus Raileanu, R.,  and Fergus, R. (2021). Decoupling Value and
    Policy for Generalization in Reinforcement Learning.. arXiv:2102.10330 [cs]..
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raileanu 和 Fergus。Raileanu, R., 和 Fergus, R. (2021)。在强化学习中解耦值和策略以实现泛化。arXiv:2102.10330
    [cs]。
- en: Raileanu et al. Raileanu, R., Goldstein, M., Yarats, D., Kostrikov, I., and Fergus,
    R. (2021). Automatic Data Augmentation for Generalization in Deep Reinforcement
    Learning.. arXiv:2006.12862 [cs]..
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raileanu et al. Raileanu, R., Goldstein, M., Yarats, D., Kostrikov, I., 和 Fergus,
    R. (2021). 通过自动数据增强提高深度强化学习的泛化能力。arXiv:2006.12862 [cs]。
- en: 'Rajan et al. Rajan, R., Diaz, J. L. B., Guttikonda, S., Ferreira, F., Biedenkapp,
    A., von Hartz, J. O., and Hutter, F. (2021). MDP Playground: A Design and Debug
    Testbed for Reinforcement Learning.. arXiv:1909.07750 [cs, stat]., Comment: NeurIPS
    2021 Data and Benchmark Track submission (with slight formatting differences,
    most notably citation style).'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajan et al. Rajan, R., Diaz, J. L. B., Guttikonda, S., Ferreira, F., Biedenkapp,
    A., von Hartz, J. O., 和 Hutter, F. (2021). MDP Playground: 强化学习的设计和调试测试平台。arXiv:1909.07750
    [cs, stat]。注：NeurIPS 2021 数据和基准跟踪提交（格式略有不同，特别是引用样式）。'
- en: Ren et al. Ren, Y., Duan, J., Li, S. E., Guan, Y., and Sun, Q. (2020). Improving
    Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic..
    arXiv:2002.05502 [cs, stat]..
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. Ren, Y., Duan, J., Li, S. E., Guan, Y., 和 Sun, Q. (2020). 通过最小最大分布软演员-评论家提高强化学习的泛化能力。arXiv:2002.05502
    [cs, stat]。
- en: Risi and Togelius Risi, S.,  and Togelius, J. (2020). Increasing generality
    in machine learning through procedural content generation.. Nat Mach Intell..
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Risi and Togelius Risi, S., 和 Togelius, J. (2020). 通过程序化内容生成提高机器学习的普遍性。自然机器智能。
- en: 'Sadeghi and Levine Sadeghi, F.,  and Levine, S. (2017). CAD2RL: Real Single-Image
    Flight without a Single Real Image.. arXiv:1611.04201 [cs]., Comment: To appear
    at Robotics: Science and Systems Conference (R:SS), 2017\. Supplementary video:
    https://www.youtube.com/watch?v=nXBWmzFrj5s.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sadeghi and Levine Sadeghi, F., 和 Levine, S. (2017). CAD2RL: 无需真实图像的真实单幅图像飞行。arXiv:1611.04201
    [cs]。注：将在2017年机器人学：科学与系统会议（R:SS）上发表。补充视频： https://www.youtube.com/watch?v=nXBWmzFrj5s。'
- en: 'Samvelyan et al. Samvelyan, M., Kirk, R., Kurin, V., Parker-Holder, J., Jiang,
    M., Hambro, E., Petroni, F., Kuttler, H., Grefenstette, E., and Rocktäschel, T.
    (2021). MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research.
    In Thirty-Fifth Conference on Neural Information Processing Systems Datasets and
    Benchmarks Track (Round 1).'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Samvelyan et al. Samvelyan, M., Kirk, R., Kurin, V., Parker-Holder, J., Jiang,
    M., Hambro, E., Petroni, F., Kuttler, H., Grefenstette, E., 和 Rocktäschel, T.
    (2021). MiniHack the Planet: 开放式强化学习研究的沙盒。在第三十五届神经信息处理系统会议数据集和基准跟踪（第一轮）。'
- en: 'Schmeckpeper et al. Schmeckpeper, K., Rybkin, O., Daniilidis, K., Levine, S.,
    and Finn, C. (2020). Reinforcement Learning with Videos: Combining Offline Observations
    with Interaction.. arXiv:2011.06507 [cs]..'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmeckpeper et al. Schmeckpeper, K., Rybkin, O., Daniilidis, K., Levine, S.,
    和 Finn, C. (2020). 通过视频进行强化学习：将离线观察与互动相结合。arXiv:2011.06507 [cs]。
- en: Schölkopf Schölkopf, B. (2019). Causality for Machine Learning.. arXiv:1911.10500
    [cs, stat]..
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schölkopf Schölkopf, B. (2019). 机器学习中的因果关系。arXiv:1911.10500 [cs, stat]。
- en: Schrittwieser et al. Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan,
    K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T.,
    Lillicrap, T., and Silver, D. (2020). Mastering Atari, Go, chess and shogi by
    planning with a learned model.. Nature..
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schrittwieser et al. Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan,
    K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T.,
    Lillicrap, T., 和 Silver, D. (2020). 通过规划学习模型来掌握Atari、围棋、国际象棋和将棋。自然。
- en: Schrittwieser et al. Schrittwieser, J., Hubert, T., Mandhane, A., Barekatain,
    M., Antonoglou, I., and Silver, D. (2021). Online and Offline Reinforcement Learning
    by Planning with a Learned Model.. arXiv:2104.06294 [cs]..
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schrittwieser et al. Schrittwieser, J., Hubert, T., Mandhane, A., Barekatain,
    M., Antonoglou, I., 和 Silver, D. (2021). 通过规划学习模型进行在线和离线强化学习。arXiv:2104.06294
    [cs]。
- en: Schulman et al. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov,
    O. (2017). Proximal Policy Optimization Algorithms.. arXiv:1707.06347 [cs]..
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., 和 Klimov,
    O. (2017). 近端策略优化算法。arXiv:1707.06347 [cs]。
- en: 'Seo et al. Seo, Y., Lee, K., Clavera, I., Kurutach, T., Shin, J., and Abbeel,
    P. (2020). Trajectory-wise Multiple Choice Learning for Dynamics Generalization
    in Reinforcement Learning.. arXiv:2010.13303 [cs]., Comment: Accepted in NeurIPS2020.
    First two authors contributed equally, website: https://sites.google.com/view/trajectory-mcl
    code: https://github.com/younggyoseo/trajectory_mcl.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seo et al. Seo, Y., Lee, K., Clavera, I., Kurutach, T., Shin, J., 和 Abbeel,
    P. (2020). 强化学习中动态泛化的轨迹-wise 多项选择学习。arXiv:2010.13303 [cs]。注：被NeurIPS2020接受。前两位作者贡献相同，网站：
    https://sites.google.com/view/trajectory-mcl 代码： https://github.com/younggyoseo/trajectory_mcl。
- en: 'Sermanet et al. Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal,
    S., and Levine, S. (2018). Time-Contrastive Networks: Self-Supervised Learning
    from Video.. arXiv:1704.06888 [cs]..'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sermanet 等人 Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal,
    S., 和 Levine, S. (2018). 时间对比网络：从视频中自监督学习.. arXiv:1704.06888 [cs]..
- en: 'Sestini et al. Sestini, A., Kuhnle, A., and Bagdanov, A. D. (2020). Demonstration-efficient
    Inverse Reinforcement Learning in Procedurally Generated Environments.. arXiv:2012.02527
    [cs]., Comment: Presented at the AAAI-21 Workshop on Reinforcement Learning in
    Games.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sestini 等人 Sestini, A., Kuhnle, A., 和 Bagdanov, A. D. (2020). 演示高效的逆向强化学习在程序生成环境中的应用..
    arXiv:2012.02527 [cs]。, 注：在 AAAI-21 游戏中的强化学习研讨会上展示。
- en: Shapley Shapley, L. S. (1953). Stochastic Games*.. Proceedings of the National
    Academy of Sciences..
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley Shapley, L. S. (1953). 随机游戏*.. 国家科学院院刊..
- en: 'Shen et al. Shen, B., Xia, F., Li, C., Martín-Martín, R., Fan, L., Wang, G.,
    Pérez-D’Arpino, C., Buch, S., Srivastava, S., Tchapmi, L. P., Tchapmi, M. E.,
    Vainio, K., Wong, J., Fei-Fei, L., and Savarese, S. (2021). iGibson 1.0: A Simulation
    Environment for Interactive Tasks in Large Realistic Scenes.. arXiv:2012.02924
    [cs]..'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等人 Shen, B., Xia, F., Li, C., Martín-Martín, R., Fan, L., Wang, G., Pérez-D’Arpino,
    C., Buch, S., Srivastava, S., Tchapmi, L. P., Tchapmi, M. E., Vainio, K., Wong,
    J., Fei-Fei, L., 和 Savarese, S. (2021). iGibson 1.0: 大型逼真场景中交互任务的仿真环境.. arXiv:2012.02924
    [cs]..'
- en: Shorten and Khoshgoftaar Shorten, C.,  and Khoshgoftaar, T. M. (2019). A survey
    on Image Data Augmentation for Deep Learning.. J Big Data..
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shorten 和 Khoshgoftaar Shorten, C., 和 Khoshgoftaar, T. M. (2019). 深度学习中的图像数据增强调查..
    J Big Data..
- en: Singh and Zheng Singh, J.,  and Zheng, L. (2021). Sparse Attention Guided Dynamic
    Value Estimation for Single-Task Multi-Scene Reinforcement Learning.. arXiv:2102.07266
    [cs, stat]..
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 和 Zheng Singh, J., 和 Zheng, L. (2021). 稀疏注意力引导的动态值估计用于单任务多场景强化学习.. arXiv:2102.07266
    [cs, stat]..
- en: Sodhani et al. Sodhani, S., Meier, F., Pineau, J., and Zhang, A. (2022). Block
    Contextual MDPs for Continual Learning. In Proceedings of The 4th Annual Learning
    for Dynamics and Control Conference, pp. 608–623\. PMLR.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sodhani 等人 Sodhani, S., Meier, F., Pineau, J., 和 Zhang, A. (2022). 用于持续学习的块上下文
    MDPs. 在第 4 届动态与控制学习年度会议论文集中，页码 608–623。PMLR。
- en: 'Sonar et al. Sonar, A., Pacelli, V., and Majumdar, A. (2020). Invariant Policy
    Optimization: Towards Stronger Generalization in Reinforcement Learning.. arXiv:2006.01096
    [cs, stat]., Comment: 16 pages, 4 figures.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sonar 等人 Sonar, A., Pacelli, V., 和 Majumdar, A. (2020). 不变策略优化：迈向更强的强化学习泛化..
    arXiv:2006.01096 [cs, stat]。, 注：16 页，4 幅图。
- en: Song et al. Song, X., Jiang, Y., Tu, S., Du, Y., and Neyshabur, B. (2020). Observational
    overfitting in reinforcement learning. In 8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 Song, X., Jiang, Y., Tu, S., Du, Y., 和 Neyshabur, B. (2020). 强化学习中的观察过拟合.
    在第 8 届国际表示学习会议, ICLR 2020, 埃塞俄比亚亚的斯亚贝巴, 2020 年 4 月 26-30 日。OpenReview.net。
- en: 'Stone et al. Stone, A., Ramirez, O., Konolige, K., and Jonschkowski, R. (2021).
    The Distracting Control Suite – A Challenging Benchmark for Reinforcement Learning
    from Pixels.. arXiv:2101.02722 [cs]., Comment: Code available at https://github.com/google-research/google-research/tree/master/distracting_control.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stone 等人 Stone, A., Ramirez, O., Konolige, K., 和 Jonschkowski, R. (2021). 分散控制套件
    – 来自像素的强化学习挑战基准.. arXiv:2101.02722 [cs]。, 注：代码可在 [https://github.com/google-research/google-research/tree/master/distracting_control](https://github.com/google-research/google-research/tree/master/distracting_control)
    获取。
- en: Strehl et al. Strehl, A. L., Diuk, C., and Littman, M. L. (2007). Efficient
    structure learning in factored-state MDPs. In Proceedings of the 22nd National
    Conference on Artificial Intelligence - Volume 1, AAAI’07, pp. 645–650, Vancouver,
    British Columbia, Canada. AAAI Press.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strehl 等人 Strehl, A. L., Diuk, C., 和 Littman, M. L. (2007). 在因子状态 MDPs 中的高效结构学习.
    在第 22 届人工智能全国会议论文集 - 第 1 卷, AAAI’07, 页码 645–650, 加拿大不列颠哥伦比亚省温哥华。AAAI 出版社。
- en: 'Tachet et al. Tachet, R., Bachman, P., and van Seijen, H. (2020). Learning
    Invariances for Policy Generalization.. arXiv:1809.02591 [cs, stat]., Comment:
    7 pages, 1 figure.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tachet 等人 Tachet, R., Bachman, P., 和 van Seijen, H. (2020). 政策泛化的学习不变性.. arXiv:1809.02591
    [cs, stat]。, 注：7 页，1 幅图。
- en: 'Tang and Ha Tang, Y.,  and Ha, D. (2021). The Sensory Neuron as a Transformer:
    Permutation-Invariant Neural Networks for Reinforcement Learning.. arXiv:2109.02869
    [cs]..'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 和 Ha Tang, Y., 和 Ha, D. (2021). 感觉神经元作为变压器：用于强化学习的置换不变神经网络.. arXiv:2109.02869
    [cs]..
- en: 'Tang et al. Tang, Y., Nguyen, D., and Ha, D. (2020). Neuroevolution of Self-Interpretable
    Agents.. Proceedings of the 2020 Genetic and Evolutionary Computation Conference.,
    Comment: To appear at the Genetic and Evolutionary Computation Conference (GECCO
    2020) as a full paper.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 Tang, Y., Nguyen, D., 和 Ha, D. (2020). 自解释代理的神经进化.. 2020 年遗传与进化计算会议论文集.,
    评论：将作为完整论文出现在遗传与进化计算会议 (GECCO 2020)。
- en: 'Tassa et al. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D.
    d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T., and Riedmiller,
    M. (2018). DeepMind Control Suite.. arXiv:1801.00690 [cs]., Comment: 24 pages,
    7 figures, 2 tables.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tassa 等人 Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L.,
    Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T., 和 Riedmiller,
    M. (2018). DeepMind 控制套件.. arXiv:1801.00690 [cs]., 评论：24页，7图，2表。
- en: Team et al. Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck, C., Bauer,
    J., Sygnowski, J., Trebacz, M., Jaderberg, M., Mathieu, M., McAleese, N., Bradley-Schmieg,
    N., Wong, N., Porcel, N., Raileanu, R., Hughes-Fitt, S., Dalibard, V., and Czarnecki,
    W. M. (2021). Open-Ended Learning Leads to Generally Capable Agents.. arXiv:2107.12808
    [cs]..
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team 等人 Team, O. E. L., Stooke, A., Mahajan, A., Barros, C., Deck, C., Bauer,
    J., Sygnowski, J., Trebacz, M., Jaderberg, M., Mathieu, M., McAleese, N., Bradley-Schmieg,
    N., Wong, N., Porcel, N., Raileanu, R., Hughes-Fitt, S., Dalibard, V., 和 Czarnecki,
    W. M. (2021). 开放式学习导致普遍能力的代理.. arXiv:2107.12808 [cs]..
- en: Tishby and Zaslavsky Tishby, N.,  and Zaslavsky, N. (2015). Deep learning and
    the information bottleneck principle. In 2015 IEEE Information Theory Workshop
    (ITW), pp. 1–5.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tishby 和 Zaslavsky Tishby, N., 和 Zaslavsky, N. (2015). 深度学习与信息瓶颈原理. 在 2015 IEEE
    信息理论研讨会 (ITW) 中，第1–5页。
- en: 'Tobin et al. Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel,
    P. (2017). Domain Randomization for Transferring Deep Neural Networks from Simulation
    to the Real World.. arXiv:1703.06907 [cs]., Comment: 8 pages, 7 figures. Submitted
    to 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS
    2017).'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tobin 等人 Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., 和 Abbeel,
    P. (2017). 领域随机化：从模拟到现实世界的深度神经网络转移.. arXiv:1703.06907 [cs]., 评论：8页，7图。提交至 2017
    IEEE/RSJ 国际智能机器人与系统大会 (IROS 2017)。
- en: 'Tosch et al. Tosch, E., Clary, K., Foley, J., and Jensen, D. (2019). Toybox:
    A Suite of Environments for Experimental Evaluation of Deep Reinforcement Learning..
    arXiv:1905.02825 [cs, stat]..'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tosch 等人 Tosch, E., Clary, K., Foley, J., 和 Jensen, D. (2019). Toybox: 用于深度强化学习实验评估的环境套件..
    arXiv:1905.02825 [cs, stat]..'
- en: Vinyals et al. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik,
    A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan,
    D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J. P.,
    Jaderberg, M., Vezhnevets, A. S., Leblond, R., Pohlen, T., Dalibard, V., Budden,
    D., Sulsky, Y., Molloy, J., Paine, T. L., Gulcehre, C., Wang, Z., Pfaff, T., Wu,
    Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul, T., Lillicrap,
    T., Kavukcuoglu, K., Hassabis, D., Apps, C., and Silver, D. (2019). Grandmaster
    level in StarCraft II using multi-agent reinforcement learning.. Nature..
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等人 Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik,
    A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan,
    D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J. P.,
    Jaderberg, M., Vezhnevets, A. S., Leblond, R., Pohlen, T., Dalibard, V., Budden,
    D., Sulsky, Y., Molloy, J., Paine, T. L., Gulcehre, C., Wang, Z., Pfaff, T., Wu,
    Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul, T., Lillicrap,
    T., Kavukcuoglu, K., Hassabis, D., Apps, C., 和 Silver, D. (2019). 使用多智能体强化学习在星际争霸
    II 中达到大师级水平.. 自然..
- en: Vithayathil Varghese and Mahmoud Vithayathil Varghese, N.,  and Mahmoud, Q. H.
    (2020). A survey of multi-task deep reinforcement learning.. Electronics..
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vithayathil Varghese 和 Mahmoud Vithayathil Varghese, N., 和 Mahmoud, Q. H. (2020).
    多任务深度强化学习的综述.. 电子学..
- en: 'Vlastelica et al. Vlastelica, M., Rolínek, M., and Martius, G. (2021). Neuro-algorithmic
    Policies enable Fast Combinatorial Generalization.. arXiv:2102.07456 [cs]., Comment:
    15 pages.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vlastelica 等人 Vlastelica, M., Rolínek, M., 和 Martius, G. (2021). 神经算法策略实现快速组合泛化..
    arXiv:2102.07456 [cs]., 评论：15页。
- en: 'Wang et al. Wang, J. X., King, M., Porcel, N., Kurth-Nelson, Z., Zhu, T., Deck,
    C., Choy, P., Cassin, M., Reynolds, M., Song, F., Buttimore, G., Reichert, D. P.,
    Rabinowitz, N., Matthey, L., Hassabis, D., Lerchner, A., and Botvinick, M. (2021).
    Alchemy: A structured task distribution for meta-reinforcement learning.. arXiv:2102.02926
    [cs]., Comment: 16 pages, 9 figures.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 Wang, J. X., King, M., Porcel, N., Kurth-Nelson, Z., Zhu, T., Deck,
    C., Choy, P., Cassin, M., Reynolds, M., Song, F., Buttimore, G., Reichert, D.
    P., Rabinowitz, N., Matthey, L., Hassabis, D., Lerchner, A., 和 Botvinick, M. (2021).
    Alchemy：用于元强化学习的结构化任务分配.. arXiv:2102.02926 [cs]., 评论：16页，9图。
- en: 'Wang et al. Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo,
    J. Z., Munos, R., Blundell, C., Kumaran, D., and Botvinick, M. (2017). Learning
    to reinforcement learn.. arXiv:1611.05763 [cs, stat]., Comment: 17 pages, 7 figures,
    1 table.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z.,
    Munos, R., Blundell, C., Kumaran, D., 和 Botvinick, M. (2017). 学会强化学习.. arXiv:1611.05763
    [cs, stat]., 评论: 17 页, 7 图, 1 表。'
- en: 'Wang et al. Wang, K., Kang, B., Shao, J., and Feng, J. (2020). Improving Generalization
    in Reinforcement Learning with Mixture Regularization.. arXiv:2010.10814 [cs,
    stat]., Comment: NeurIPS 2020.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 Wang, K., Kang, B., Shao, J., 和 Feng, J. (2020). 通过混合正则化提高强化学习的泛化能力..
    arXiv:2010.10814 [cs, stat]., 评论: NeurIPS 2020。'
- en: 'Wang et al. Wang, R., Lehman, J., Clune, J., and Stanley, K. O. (2019). Paired
    Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse
    Learning Environments and Their Solutions.. arXiv:1901.01753 [cs]., Comment: 28
    pages, 9 figures.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 Wang, R., Lehman, J., Clune, J., 和 Stanley, K. O. (2019). 配对开放式开拓者
    (POET): 无休止地生成越来越复杂和多样的学习环境及其解决方案.. arXiv:1901.01753 [cs]., 评论: 28 页, 9 图。'
- en: 'Wang et al. Wang, R., Lehman, J., Rawal, A., Zhi, J., Li, Y., Clune, J., and Stanley,
    K. O. (2020). Enhanced POET: open-ended reinforcement learning through unbounded
    invention of learning challenges and their solutions. In Proceedings of the 37th
    International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual
    Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 9940–9951. PMLR.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 Wang, R., Lehman, J., Rawal, A., Zhi, J., Li, Y., Clune, J., 和 Stanley,
    K. O. (2020). 增强版 POET: 通过无界的学习挑战及其解决方案发明进行开放式强化学习。见第37届国际机器学习会议论文集，ICML 2020,
    2020年7月13-18日，虚拟事件, 机器学习研究论文集第119卷, pp. 9940–9951. PMLR。'
- en: 'Wang et al. Wang, X., Lian, L., and Yu, S. X. (2021). Unsupervised Visual Attention
    and Invariance for Reinforcement Learning.. arXiv:2104.02921 [cs]., Comment: Accepted
    at CVPR 2021.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 Wang, X., Lian, L., 和 Yu, S. X. (2021). 无监督视觉注意力和不变性在强化学习中的应用.. arXiv:2104.02921
    [cs]., 评论: 接受于 CVPR 2021。'
- en: 'Wellmer and Kwok Wellmer, Z.,  and Kwok, J. T. (2021). Dropout’s Dream Land:
    Generalization from Learned Simulators to Reality. In Oliver, N., Pérez-Cruz,
    F., Kramer, S., Read, J., and Lozano, J. A. (Eds.), Machine Learning and Knowledge
    Discovery in Databases. Research Track, Lecture Notes in Computer Science, pp. 255–270,
    Cham. Springer International Publishing.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wellmer 和 Kwok Wellmer, Z., 和 Kwok, J. T. (2021). Dropout 的梦想之地: 从学习的模拟器到现实的泛化。见
    Oliver, N., Pérez-Cruz, F., Kramer, S., Read, J., 和 Lozano, J. A. (编辑), 《机器学习与数据库中的知识发现》。研究轨道,
    计算机科学讲义, pp. 255–270, Cham. Springer 国际出版公司。'
- en: 'Wenke et al. Wenke, S., Saunders, D., Qiu, M., and Fleming, J. (2019). Reasoning
    and Generalization in RL: A Tool Use Perspective.. arXiv:1907.02050 [cs]., Comment:
    13 pages, 5 figures.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wenke 等人 Wenke, S., Saunders, D., Qiu, M., 和 Fleming, J. (2019). 强化学习中的推理与泛化:
    工具使用视角.. arXiv:1907.02050 [cs]., 评论: 13 页, 5 图。'
- en: Whiteson et al. Whiteson, S., Tanner, B., Taylor, M. E., and Stone, P. (2011).
    Protecting against evaluation overfitting in empirical reinforcement learning.
    In 2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning
    (ADPRL), pp. 120–127.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whiteson 等人 Whiteson, S., Tanner, B., Taylor, M. E., 和 Stone, P. (2011). 保护免受经验强化学习中评估过拟合的影响。见
    2011 IEEE 自适应动态规划与强化学习研讨会 (ADPRL), pp. 120–127。
- en: Wolpert and Macready Wolpert, D.,  and Macready, W. (1997). No free lunch theorems
    for optimization.. IEEE Transactions on Evolutionary Computation..
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolpert 和 Macready Wolpert, D., 和 Macready, W. (1997). 优化的无免费午餐定理.. IEEE 进化计算汇刊..
- en: 'Xie et al. Xie, S., Ma, X., Yu, P., Zhu, Y., Wu, Y. N., and Zhu, S.-C. (2021).
    HALMA: Humanlike Abstraction Learning Meets Affordance in Rapid Problem Solving..
    arXiv:2102.11344 [cs]..'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人 Xie, S., Ma, X., Yu, P., Zhu, Y., Wu, Y. N., 和 Zhu, S.-C. (2021). HALMA:
    人类般的抽象学习在快速问题解决中的作用.. arXiv:2102.11344 [cs]..'
- en: Xing et al. Xing, E., Gupta, A., Powers, S., and Dean, V. (2021a). Evaluating
    Generalization of Policy Learning Under Domain Shifts..
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing 等人 Xing, E., Gupta, A., Powers, S., 和 Dean, V. (2021a). 评估领域转移下的策略学习泛化能力..
- en: 'Xing et al. Xing, E., Gupta, A., Powers, S., and Dean, V. (2021b). KitchenShift:
    Evaluating Zero-Shot Generalization of Imitation-Based Policy Learning Under Domain
    Shifts. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and
    Applications.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xing 等人 Xing, E., Gupta, A., Powers, S., 和 Dean, V. (2021b). KitchenShift:
    评估模仿学习策略在领域转移下的零样本泛化能力。见 NeurIPS 2021 领域转移研讨会: 连接方法与应用。'
- en: 'Xue et al. Xue, C., Pinto, V., Gamage, C., Nikonova, E., Zhang, P., and Renz,
    J. (2021). Phy-Q: A Benchmark for Physical Reasoning.. arXiv:2108.13696 [cs].,
    Comment: For the associated website, see https://github.com/phy-q/benchmark.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue 等人。Xue, C., Pinto, V., Gamage, C., Nikonova, E., Zhang, P., 和 Renz, J. (2021)。Phy-Q：物理推理基准。arXiv:2108.13696
    [cs]。注释：有关关联网站，请参见 https://github.com/phy-q/benchmark。
- en: 'Yang et al. Yang, R., Xu, H., Wu, Y., and Wang, X. (2020a). Multi-Task Reinforcement
    Learning with Soft Modularization.. arXiv:2003.13661 [cs, stat]., Comment: Our
    project page: https://rchalyang.github.io/SoftModule.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人。Yang, R., Xu, H., Wu, Y., 和 Wang, X. (2020a)。具有软模块化的多任务强化学习。arXiv:2003.13661
    [cs, stat]。注释：我们的项目页面：https://rchalyang.github.io/SoftModule。
- en: 'Yang et al. Yang, Y., Cer, D., Ahmad, A., Guo, M., Law, J., Constant, N., Hernandez Abrego,
    G., Yuan, S., Tar, C., Sung, Y.-h., Strope, B., and Kurzweil, R. (2020b). Multilingual
    universal sentence encoder for semantic retrieval. In Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics: System Demonstrations,
    pp. 87–94, Online. Association for Computational Linguistics.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人。Yang, Y., Cer, D., Ahmad, A., Guo, M., Law, J., Constant, N., Hernandez
    Abrego, G., Yuan, S., Tar, C., Sung, Y.-h., Strope, B., 和 Kurzweil, R. (2020b)。用于语义检索的多语言通用句子编码器。发表于第58届计算语言学协会年会：系统演示，第87–94页，在线。计算语言学协会。
- en: 'Yen-Chen et al. Yen-Chen, L., Bauza, M., and Isola, P. (2019). Experience-Embedded
    Visual Foresight.. arXiv:1911.05071 [cs]., Comment: CoRL 2019\. Project website:
    http://yenchenlin.me/evf/.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yen-Chen 等人。Yen-Chen, L., Bauza, M., 和 Isola, P. (2019)。经验嵌入的视觉预测。arXiv:1911.05071
    [cs]。注释：CoRL 2019。项目网站：http://yenchenlin.me/evf/。
- en: 'Yu et al. Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine,
    S. (2019). Meta-world: A benchmark and evaluation for multi-task and meta reinforcement
    learning.. arXiv preprint arXiv:1910.10897..'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人。Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., 和 Levine,
    S. (2019)。Meta-world：多任务和元强化学习的基准和评估。arXiv 预印本 arXiv:1910.10897。
- en: 'Yu et al. Yu, W., Tan, J., Liu, C. K., and Turk, G. (2017). Preparing for the
    Unknown: Learning a Universal Policy with Online System Identification.. arXiv:1702.02453
    [cs]., Comment: Accepted as a conference paper at RSS 2017.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人。Yu, W., Tan, J., Liu, C. K., 和 Turk, G. (2017)。为未知做好准备：通过在线系统识别学习通用策略。arXiv:1702.02453
    [cs]。注释：被RSS 2017会议接受。
- en: Zambaldi et al. Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin,
    I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston,
    V., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. (2018). Relational
    Deep Reinforcement Learning.. arXiv:1806.01830 [cs, stat]..
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zambaldi 等人。Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin,
    I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston,
    V., Pascanu, R., Botvinick, M., Vinyals, O., 和 Battaglia, P. (2018)。关系深度强化学习。arXiv:1806.01830
    [cs, stat]。
- en: Zambaldi et al. Zambaldi, V. F., Raposo, D., Santoro, A., Bapst, V., Li, Y.,
    Babuschkin, I., Tuyls, K., Reichert, D. P., Lillicrap, T. P., Lockhart, E., Shanahan,
    M., Langston, V., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. W.
    (2019). Deep reinforcement learning with relational inductive biases. In 7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019. OpenReview.net.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zambaldi 等人。Zambaldi, V. F., Raposo, D., Santoro, A., Bapst, V., Li, Y., Babuschkin,
    I., Tuyls, K., Reichert, D. P., Lillicrap, T. P., Lockhart, E., Shanahan, M.,
    Langston, V., Pascanu, R., Botvinick, M., Vinyals, O., 和 Battaglia, P. W. (2019)。具有关系归纳偏置的深度强化学习。第7届国际表示学习大会，ICLR
    2019，美国路易斯安那州新奥尔良，2019年5月6-9日。OpenReview.net。
- en: 'Zhang et al. Zhang, A., Ballas, N., and Pineau, J. (2018). A Dissection of
    Overfitting and Generalization in Continuous Reinforcement Learning.. arXiv:1806.07937
    [cs, stat]., Comment: 20 pages, 16 figures.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人。Zhang, A., Ballas, N., 和 Pineau, J. (2018)。连续强化学习中过拟合和泛化的剖析。arXiv:1806.07937
    [cs, stat]。注释：20页，16幅图。
- en: Zhang et al. Zhang, A., Lyle, C., Sodhani, S., Filos, A., Kwiatkowska, M., Pineau,
    J., Gal, Y., and Precup, D. (2020). Invariant causal prediction for block mdps.
    In Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings of Machine Learning
    Research, pp. 11214–11224. PMLR.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人。Zhang, A., Lyle, C., Sodhani, S., Filos, A., Kwiatkowska, M., Pineau,
    J., Gal, Y., 和 Precup, D. (2020)。针对块 MDPs 的不变因果预测。发表于第37届国际机器学习大会，ICML 2020，2020年7月13-18日，虚拟会议，机器学习研究会议录第119卷，第11214–11224页。PMLR。
- en: 'Zhang et al. Zhang, A., McAllister, R., Calandra, R., Gal, Y., and Levine,
    S. (2021). Learning Invariant Representations for Reinforcement Learning without
    Reconstruction.. arXiv:2006.10742 [cs, stat]., Comment: Accepted as an oral at
    ICLR 2021.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人 Zhang, A., McAllister, R., Calandra, R., Gal, Y., 和 Levine, S. (2021).
    学习强化学习的不变表示而无需重建。arXiv:2006.10742 [cs, stat]。, 评论: 在ICLR 2021上作为口头报告接受。'
- en: 'Zhang et al. Zhang, A., Wu, Y., and Pineau, J. (2018a). Natural Environment
    Benchmarks for Reinforcement Learning.. arXiv:1811.06032 [cs, stat]., Comment:
    12 figures.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人 Zhang, A., Wu, Y., 和 Pineau, J. (2018a). 强化学习的自然环境基准。arXiv:1811.06032
    [cs, stat]。, 评论: 12幅图。'
- en: Zhang et al. Zhang, C., Vinyals, O., Munos, R., and Bengio, S. (2018b). A Study
    on Overfitting in Deep Reinforcement Learning.. arXiv:1804.06893 [cs, stat]..
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人 Zhang, C., Vinyals, O., Munos, R., 和 Bengio, S. (2018b). 深度强化学习中的过拟合研究。arXiv:1804.06893
    [cs, stat]。
- en: Zhang and Guo Zhang, H.,  and Guo, Y. (2021). Generalization of Reinforcement
    Learning with Policy-Aware Adversarial Data Augmentation.. arXiv:2106.15587 [cs]..
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Guo Zhang, H., 和 Guo, Y. (2021). 使用策略感知对抗数据增强的强化学习泛化。arXiv:2106.15587
    [cs]。
- en: 'Zhang et al. Zhang, H., Cissé, M., Dauphin, Y. N., and Lopez-Paz, D. (2018).
    mixup: Beyond empirical risk minimization. In 6th International Conference on
    Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
    2018, Conference Track Proceedings. OpenReview.net.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人 Zhang, H., Cissé, M., Dauphin, Y. N., 和 Lopez-Paz, D. (2018). mixup:
    超越经验风险最小化。第六届国际学习表征会议，ICLR 2018, 加拿大温哥华，2018年4月30日至5月3日，会议记录。OpenReview.net。'
- en: Zhao and Hospedales Zhao, C.,  and Hospedales, T. (2020). Robust Domain Randomised
    Reinforcement Learning through Peer-to-Peer Distillation.. arXiv:2012.04839 [cs]..
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Hospedales Zhao, C., 和 Hospedales, T. (2020). 通过点对点蒸馏进行鲁棒的领域随机化强化学习。arXiv:2012.04839
    [cs]。
- en: Zhao et al. Zhao, C., Sigaud, O., Stulp, F., and Hospedales, T. M. (2019). Investigating
    Generalisation in Continuous Deep Reinforcement Learning.. arXiv:1902.07015 [cs,
    stat]..
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人 Zhao, C., Sigaud, O., Stulp, F., 和Hospedales, T. M. (2019). 调查连续深度强化学习中的泛化能力。arXiv:1902.07015
    [cs, stat]。
- en: 'Zhao et al. Zhao, W., Queralta, J. P., and Westerlund, T. (2020). Sim-to-Real
    Transfer in Deep Reinforcement Learning for Robotics: A Survey. In 2020 IEEE Symposium
    Series on Computational Intelligence (SSCI), pp. 737–744.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao等人 Zhao, W., Queralta, J. P., 和 Westerlund, T. (2020). 机器人深度强化学习中的模拟到真实转移:
    综述。在2020年IEEE计算智能研讨会（SSCI），第737-744页。'
- en: 'Zhong et al. Zhong, V., Rocktäschel, T., and Grefenstette, E. (2021). RTFM:
    Generalising to Novel Environment Dynamics via Reading.. arXiv:1910.08210 [cs].,
    Comment: ICLR 2020; 17 pages, 13 figures.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong等人 Zhong, V., Rocktäschel, T., 和 Grefenstette, E. (2021). RTFM: 通过阅读泛化到新环境动态。arXiv:1910.08210
    [cs]。, 评论: ICLR 2020；17页，13幅图。'
- en: 'Zhou et al. Zhou, K., Liu, Z., Qiao, Y., Xiang, T., and Loy, C. C. (2022).
    Domain Generalization: A Survey.. IEEE Trans. Pattern Anal. Mach. Intell.., Comment:
    IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou等人 Zhou, K., Liu, Z., Qiao, Y., Xiang, T., 和 Loy, C. C. (2022). 领域泛化: 综述。IEEE
    Trans. Pattern Anal. Mach. Intell.。, 评论: IEEE图像分析和机器智能（TPAMI），2022年。'
- en: 'Zhou et al. Zhou, K., Yang, Y., Qiao, Y., and Xiang, T. (2021). Domain Generalization
    with MixStyle.. arXiv:2104.02008 [cs]., Comment: ICLR 2021; Code is available
    at https://github.com/KaiyangZhou/mixstyle-release.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou等人 Zhou, K., Yang, Y., Qiao, Y., 和 Xiang, T. (2021). 使用MixStyle进行领域泛化。arXiv:2104.02008
    [cs]。, 评论: ICLR 2021; 代码可在 https://github.com/KaiyangZhou/mixstyle-release 获取。'
- en: 'Zhu et al. Zhu, Y., Wong, J., Mandlekar, A., and Martín-Martín, R. (2020).
    Robosuite: A Modular Simulation Framework and Benchmark for Robot Learning.. arXiv:2009.12293
    [cs]., Comment: For more information, please visit https://robosuite.ai.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu等人 Zhu, Y., Wong, J., Mandlekar, A., 和 Martín-Martín, R. (2020). Robosuite:
    机器人学习的模块化仿真框架和基准。arXiv:2009.12293 [cs]。, 评论: 更多信息，请访问 https://robosuite.ai。'
- en: 'Zhu et al. Zhu, Z., Lin, K., and Zhou, J. (2021). Transfer Learning in Deep
    Reinforcement Learning: A Survey.. arXiv:2009.07888 [cs, stat]..'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu等人 Zhu, Z., Lin, K., 和 Zhou, J. (2021). 深度强化学习中的迁移学习: 综述。arXiv:2009.07888
    [cs, stat]。'
- en: 'Zintgraf et al. Zintgraf, L., Feng, L., Lu, C., Igl, M., Hartikainen, K., Hofmann,
    K., and Whiteson, S. (2021). Exploration in Approximate Hyper-State Space for
    Meta Reinforcement Learning.. arXiv:2010.01062 [cs, stat]., Comment: Published
    at the International Conference on Machine Learning (ICML) 2021.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zintgraf等人 Zintgraf, L., Feng, L., Lu, C., Igl, M., Hartikainen, K., Hofmann,
    K., 和 Whiteson, S. (2021). 元强化学习中近似超状态空间的探索。arXiv:2010.01062 [cs, stat]。, 评论:
    发表在2021年国际机器学习大会（ICML）。'
- en: 'Zintgraf et al. Zintgraf, L. M., Shiarlis, K., Igl, M., Schulze, S., Gal, Y.,
    Hofmann, K., and Whiteson, S. (2020). Varibad: A very good method for bayes-adaptive
    deep RL via meta-learning. In 8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zintgraf等人（Zintgraf, L. M., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann,
    K., 和 Whiteson, S.）（2020年）。**Varibad**：一种通过元学习实现贝叶斯自适应深度强化学习的非常好方法。在第8届国际学习表征会议（ICLR
    2020），2020年4月26-30日，埃塞俄比亚，亚的斯亚贝巴。OpenReview.net。
