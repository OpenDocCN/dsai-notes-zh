- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:30:52'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2408.01287] Deep Learning based Visually Rich Document Content Understanding:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01287](https://ar5iv.labs.arxiv.org/html/2408.01287)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning based Visually Rich Document Content Understanding: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yihao Ding [0000-0001-5065-6911](https://orcid.org/0000-0001-5065-6911 "ORCID
    identifier") The University of Melbourne, The University of SydneyAustralia , 
    Jean Lee [0000-0002-7457-028X](https://orcid.org/0000-0002-7457-028X "ORCID identifier")
    The University of SydneyAustralia  and  Soyeon Caren Han [0000-0002-1948-6819](https://orcid.org/0000-0002-1948-6819
    "ORCID identifier") The University of Melbourne, The University of SydneyAustralia(2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visually Rich Documents (VRDs) are essential in academia, finance, medical fields,
    and marketing due to their multimodal information content. Traditional methods
    for extracting information from VRDs depend on expert knowledge and manual labor,
    making them costly and inefficient. The advent of deep learning has revolutionized
    this process, introducing models that leverage multimodal information—vision,
    text, and layout—along with pretraining tasks to develop comprehensive document
    representations. These models have achieved state-of-the-art performance across
    various downstream tasks, significantly enhancing the efficiency and accuracy
    of information extraction from VRDs. In response to the growing demands and rapid
    developments in Visually Rich Document Understanding (VRDU), this paper provides
    a comprehensive review of deep learning-based VRDU frameworks. We systematically
    survey and analyze existing methods and benchmark datasets, categorizing them
    based on adopted strategies and downstream tasks. Furthermore, we compare different
    techniques used in VRDU models, focusing on feature representation and fusion,
    model architecture, and pretraining methods, while highlighting their strengths,
    limitations, and appropriate scenarios. Finally, we identify emerging trends and
    challenges in VRDU, offering insights into future research directions and practical
    applications. This survey aims to provide a thorough understanding of VRDU advancements,
    benefiting both academic and industrial sectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visually Rich Document Understanding, Key Information Extraction, Question
    Answering, Entity Linking, Multimodal^†^†copyright: acmcopyright^†^†journalyear:
    2024^†^†doi: XXXXXXX.XXXXXXX^†^†ccs: Information systems Information extraction'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1.1\. Backgrounds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visually Rich Documents (VRDs) are documents that contain a diverse mix of visual
    and textual elements designed to convey information in a comprehensive and visually
    engaging manner, which are ubiquitous in our daily lives and appear in domains
    such as finance, medicine, and academia. These documents integrate various types
    of visually rich elements, including paragraphs, tables, charts, diagrams, and
    photos. Both textual elements (such as paragraphs, lists, and captions) and visually
    rich elements (such as tables and figures), collectively referred to as document
    semantic entities, are essential for illustrating, explaining, and summarizing
    information. Common formats for VRDs include PDF (Portable Document Format), DOC/DOCX
    (Microsoft Word Document), and image files (JPEG, PNG). These documents are typically
    structured in a semi-structured or even unstructured manner, making their understanding
    and information extraction challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VRD Understanding (VRDU) field is dedicated to comprehending the structure
    of VRDs and extracting pertinent information, thus transforming unstructured or
    semi-structured content into a machine-readable format. VRDU mainly encompasses
    two tasks: Key Information Extraction (KIE) and Question Answering (QA). Key Information
    Extraction (KIE) aims to identify and extract values based on predefined keys.
    Depending on the model used, KIE can be approached as an entity retrieval task
    (KIE Task Formulation 1) or a sequence tagging task (Task Formulation 2), as shown
    in Figure [1](#S2.F1 "Figure 1 ‣ 2.1\. Task Definition ‣ 2\. Background ‣ Deep
    Learning based Visually Rich Document Content Understanding: A Survey"). Visually
    Rich Document Question Answering (VRD-QA) involves answering questions in natural
    language based on contextual information in VRDs. As explained in Figure [1](#S2.F1
    "Figure 1 ‣ 2.1\. Task Definition ‣ 2\. Background ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey"), the model must locate the answer
    within the document to respond to the input question. Beyond extractive methods,
    KIE and VRD-QA can also be formulated as generative tasks, where the model generates
    the desired value or answer auto-regressively based on the query or question and
    the document images. In addition to the above-mentioned tasks, another content
    understanding task is Entity Linking (EL), which is proposed to identify the semantic
    correlations between document entities (parent-child relations).'
  prefs: []
  type: TYPE_NORMAL
- en: To address visually rich document understanding tasks, heuristic methods (Watanabe
    et al., [1995](#bib.bib133); Seki et al., [2007](#bib.bib106); Rusinol et al.,
    [2013](#bib.bib104)) and statistical machine learning techniques (Oliveira and
    Viana, [2017](#bib.bib89)) have been used in domain-specific document applications,
    which require expert customization and are challenging to update. Recent advances
    in deep learning have introduced promising alternatives. LSTM and CNN-based models
    (Katti et al., [2018](#bib.bib51); Denk and Reisswig, [2019](#bib.bib22); Zhao
    et al., [2019](#bib.bib148)), feature-driven approaches (Yu et al., [2021](#bib.bib142);
    Zhang et al., [2020](#bib.bib145); Wang et al., [2021a](#bib.bib124)), and layout-aware
    pretrained frameworks (Xu et al., [2020a](#bib.bib136); Wang et al., [2022b](#bib.bib123);
    Hong et al., [2021](#bib.bib41)), along with visual-integrated pretrained frameworks
    (Huang et al., [2022](#bib.bib43)), have significantly improved document representation
    and achieved state-of-the-art performance in various downstream tasks. However,
    these models primarily focus on fine-grained features, such as grids and word/word
    pieces (textual tokens), and often struggle with computational complexity and
    the ability to capture global logical and layout correlations. In contrast, coarse-grained
    models, which operate at the entity level, address some limitations of fine-grained
    models but may omit detailed information, resulting in less comprehensive representations.
    To bridge these gaps, joint-grained frameworks (Li et al., [2021c](#bib.bib68);
    Yu et al., [2022](#bib.bib143); Bai et al., [2022](#bib.bib4)) and LLM-based frameworks
    (Luo et al., [2024](#bib.bib79); Liu et al., [2024b](#bib.bib71)) have been developed.
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid development and increasing demands in Visually Rich Document
    Understanding (VRDU), various model architectures, multimodal learning methods,
    and pretraining techniques have been introduced to consistently enhance the performance
    of specific or multiple VRDU tasks. A comprehensive review of VRDU frameworks
    based on deep learning is provided, systematically surveying and analyzing existing
    methods and benchmark datasets, which are categorized based on adopted strategies
    and downstream tasks. Additionally, different techniques used in VRDU models are
    compared, focusing on feature representation and fusion, model architecture, and
    pretraining methods, with their strengths, limitations, and appropriate scenarios
    highlighted. Finally, emerging trends and challenges in visually rich document
    content understanding are identified, offering insights into future research directions
    and practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper is devoted to surveying and analysing existing work on understanding
    deep learning-based document content. Papers on the following topics will be reviewed
    and summarized in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper focuses on document content understanding tasks that will be included
    in this paper, mainly including document key information extraction, visual question
    answering and entity linking. This paper will not summarise the models and datasets
    that only focus on document structure understanding tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A paper proposed a novel deep-learning VRDU model for addressing one or more
    Visually Rich Document Content Understanding (VRD-CU) tasks. Section [2.2.1](#S2.SS2.SSS1
    "2.2.1\. Traditional Approaches ‣ 2.2\. Development of Document Understanding
    ‣ 2\. Background ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey") will briefly describe typical document content understanding approaches
    in a heuristically or traditional machine learning way.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper compiles widely used VRDU benchmark datasets and recently proposed
    VRDU datasets from top-tier conferences and journals since 2019\. All included
    datasets are publicly available.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper focuses on the multimodal capabilities for achieving a holistic understanding
    of entire, multi-page documents, in contrast to previous document understanding
    surveys that primarily focus on isolated subtasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It should be noted that this article only investigates models and datasets designed
    for the entire document or the document page rather than the specific document
    components such as table, chart. Therefore, models and datasets proposed for table
    detection, table structure recognition, and chart or plot question answering will
    not be summarized in the main body of this survey.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3\. Related Surveys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several surveys offer a comprehensive overview of general document understanding
    tasks. These surveys primarily focus on document layout analysis (Subramani et al.,
    [2020](#bib.bib113)), table extraction (Liu et al., [2023](#bib.bib73)), named
    entity recognition (Ehrmann et al., [2023](#bib.bib31)), and document image analysis
    (Lombardi and Marinai, [2020](#bib.bib77)) across diverse document types, including
    invoices (Saout et al., [2024](#bib.bib105)) and historical document (Ehrmann
    et al., [2023](#bib.bib31); Lombardi and Marinai, [2020](#bib.bib77)). In particular,
    computer vision-based research focuses on scanned document analysis and structure
    understanding. While these studies have advanced document image analysis, they
    often focus on fragmented subtasks and fall short of providing a holistic understanding
    of an entire document on multiple pages.
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements in deep learning have fueled the emergence of VRDU tasks,
    which demand complex document content understanding capabilities. These tasks
    include key information extraction, question answering, and document entity linking.
    However, existing surveys have not adequately addressed the unique challenges
    and opportunities presented by VRDU, with a specific emphasis on deep learning-based
    multimodal approaches (Cui et al., [2021](#bib.bib20)). To bridge this gap, this
    survey aims to provide a comprehensive overview of VRDU frameworks and datasets,
    including multimodal feature extractions and fusions in both mono and multi-task
    VRD models.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4\. Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main contributions of this paper can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper provides a detailed review and systematic categorization of VRDU frameworks
    and benchmark datasets, organized based on adopted strategies and downstream tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It critically examines and compares different techniques used in VRDU models,
    focusing on feature representation and fusion, model architecture, and pretraining
    methods, highlighting their strengths, limitations, and appropriate scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper identifies emerging trends and challenges in visually rich document
    content understanding, offering insights into future research directions and practical
    applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.5\. Survey Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Section [1](#S1 "1\. Introduction ‣ Deep Learning based Visually Rich Document
    Content Understanding: A Survey") in this survey discusses the background of visually
    rich document understanding and emphasizes the aim, scope, and contributions of
    this paper. Section [2](#S2 "2\. Background ‣ Deep Learning based Visually Rich
    Document Content Understanding: A Survey") provides additional background knowledge,
    including definitions of the VRDU tasks covered in this research and the development
    of document understanding. Sections [3](#S3 "3\. Mono-Task Document Understanding
    Frameworks ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey") and [3](#S4.F3 "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models
    ‣ Deep Learning based Visually Rich Document Content Understanding: A Survey")
    review significant frameworks for mono-task and multi-task VRDU, respectively.
    Section [5](#S5 "5\. Visually Rich Document Content Understanding Datasets ‣ Deep
    Learning based Visually Rich Document Content Understanding: A Survey") compiles
    benchmark datasets for three VRDU subtasks: scanned receipts, forms, and single
    and multiple-page documents from diverse domains. Section [6](#S6 "6\. Critical
    Discussion ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey") critically examines and compares the techniques adopted by various
    models, highlighting their strengths and limitations. Finally, Section [7](#S7
    "7\. Conclusion ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey") summarizes the trends and future directions in this field.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Task Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the differences in the purpose and application scenarios, visually
    rich document understanding tasks are classified into three categories, including
    Key Information Extraction, Question Answering and Entity Linking.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13198d7fb8081e85dcc98e2000bc4af0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Visually rich document content understanding task clarifications.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key Information Extraction: refers to identifying and extracting the relevant
    information based on the given text queries. Distinct pre-defined queries can
    be defined based on the domain of targeting documents and practical demands. For
    example, the crucial information of scanned receipts contains ”Store Name”, ”Address”,
    ”Item” and ”Price”, while for the financial reports, ”Company Name”, ”Share Holder
    Name”, ”Number of Interests” may be the critical information need to be extracted.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question Answering: is a task of answering questions about a VRD by using natural
    languages. Based on the answer types, it can be divided into extractive VQA and
    abstractive VQA. The answer from extractive VQA is directly extracted from the
    target document, while abstractive VQA requires generative answers based on comprehensively
    understanding questions and related VRDs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Document Entity Linking: refers to identifying the semantic relations between
    document entities to construct the logical structure of the input document image.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2\. Development of Document Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.2.1\. Traditional Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rule-based methods, as highlighted in several studies (Watanabe et al., [1995](#bib.bib133);
    O’Gorman, [1993](#bib.bib88); Seki et al., [2007](#bib.bib106); Rusinol et al.,
    [2013](#bib.bib104)), have demonstrated high precision in domain-specific applications.
    However, these methods have several drawbacks: they are manually intensive, costly,
    and require expert intervention for tailored customization. Additionally, they
    are inflexible, often necessitating frequent manual updates, even for minor modifications.
    In response to these limitations, machine learning-based approaches have been
    proposed for document understanding. For instance, SVM-based methods have been
    utilized for layout understanding (Oliveira and Viana, [2017](#bib.bib89)), and
    TF-IDF techniques combined with hand-crafted features have been applied for extracting
    information from invoices. Furthermore, rule-based and statistical models have
    been used for entity extraction. Despite these advancements, machine learning
    methods rely heavily on human intervention and domain-specific expertise. They
    are time-consuming and often deliver suboptimal performance. Moreover, the majority
    of these methods typically depend on single-modality data, restricting them to
    either layout, text, or visual inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Single Modality-based Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the advancement of deep learning, deeper model architectures such as CNNs
    (Katti et al., [2018](#bib.bib51); Yang et al., [2017](#bib.bib139)) have emerged,
    and pretrained language (Devlin, [2018](#bib.bib23); Liu et al., [2019b](#bib.bib75))
    and vision models (Ren et al., [2015](#bib.bib103); He et al., [2017](#bib.bib39))
    are now commonly used as robust baselines for understanding VRD content and structure.
    Due to the multimodal nature of VRDs, which involves the integration of text,
    vision, and layout, researchers are increasingly focusing on leveraging this combined
    information to achieve significant improvements in various downstream VRDU tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. Cross Modality based Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Considering the multimodal nature of VRDs, many frameworks propose various methods
    to encode multimodal information, including text, vision, and layout, and fuse
    them effectively. Text and vision information is normally encoded by pretrained
    backbones such as BERT (Devlin, [2018](#bib.bib23)) or RoBERTa (Liu et al., [2019b](#bib.bib75))
    for textual features, Faster-RCNN (Ren et al., [2015](#bib.bib103)) or Mask-RCNN
    (He et al., [2017](#bib.bib39)) for visual features. For layout information, different
    encoding methods are introduced, including linear projection (Wang et al., [2020b](#bib.bib130)),
    2D positional encoding (Xu et al., [2020a](#bib.bib136)), and attention bias to
    allow the proposed models to be layout-sensitive. Different feature fusion methods
    are introduced including summing up (Yu et al., [2021](#bib.bib142)), concatenation
    (Lee et al., [2022b](#bib.bib57)), attention-based contextual learning (Majumder
    et al., [2020a](#bib.bib81)), and prompting (He et al., [2023](#bib.bib38)). However,
    most of those frameworks leverage implicit knowledge from pretrained backbones
    with a task-orientated shadow adapter for specific VRDU downstream tasks such
    as KIE (Lee et al., [2023](#bib.bib58); Wang and Shang, [2022](#bib.bib128); Chen
    et al., [2023](#bib.bib15); Cao et al., [2023b](#bib.bib12)) or EL (Zhang et al.,
    [2021](#bib.bib146); Hu et al., [2023](#bib.bib42); Carbonell et al., [2021](#bib.bib13)).
    Those frameworks tend to achieve delicate performance on specific tasks or document
    formats instead of acquiring a generalised model to represent documents comprehensively.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4\. Multimodal Pre-training Approches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by the success of BERT-style models (Devlin, [2018](#bib.bib23); Liu
    et al., [2019b](#bib.bib75)) in acquiring knowledge through self-supervised learning,
    pretrained document understanding models have emerged to harness self-supervised
    or supervised pretraining tasks from extensive document collections. LayoutLM
    (Xu et al., [2020a](#bib.bib136)), the first encoder-only VRDU model, utilizes
    self-supervised tasks, such as masked vision-language modelling, with text and
    layout information. Subsequent models have expanded on this by integrating layout
    information (Wang et al., [2022b](#bib.bib123); Tu et al., [2023](#bib.bib119);
    Li et al., [2021c](#bib.bib68)) and visual cues (Xu et al., [2020b](#bib.bib138);
    Huang et al., [2022](#bib.bib43)) through multimodal transformers. While encoder-only
    models have shown significant improvements on various benchmark datasets (Jaume
    et al., [2019](#bib.bib47); Park et al., [2019](#bib.bib93); Huang et al., [2019](#bib.bib44);
    Mathew et al., [2021](#bib.bib84); Harley et al., [2015](#bib.bib37)), they often
    require detailed annotations and are limited by fixed input lengths. To address
    these limitations, encoder-decoder frameworks (Tang et al., [2022](#bib.bib116);
    Kim et al., [2022](#bib.bib52); Davis et al., [2022](#bib.bib21)) and prompt-based
    methods for LLMs/MLLMs (Luo et al., [2024](#bib.bib79); Liu et al., [2024b](#bib.bib71))
    have been developed, enhancing layout awareness and performance in VRDU tasks.
    However, a significant gap remains in effectively applying these models in real-world
    scenarios with zero shot.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Mono-Task Document Understanding Frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visually rich document content understanding encompasses several independent
    downstream tasks tailored to different application scenarios and user demands.
    This section introduces methods focused on specific document understanding tasks.
    Many of these models aim to provide comprehensive document representation and
    integrate target-oriented modules or techniques to improve performance and efficiency
    across various VRDU tasks. The models for three VRDU downstream tasks—Key Information
    Extraction (KIE), Entity Linking (EL), and Visual Question Answering (VQA)—are
    introduced and summarized with insights into current trends.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: 'for tree= forked edges, grow’=0, draw, rounded corners, node options=align=center,,
    text width=3cm, s sep=6pt, calign=child edge, calign child=(n_children()+1)/2
    [Mono-task based models (Content Understanding), for tree=fill=lime!60 [Key Information
    Extraction, for tree=fill=cyan!40 [Feature-driven Models, for tree=fill=cyan!25
    [Chargrid(Katti et al., [2018](#bib.bib51)); CUTIE(Zhao et al., [2019](#bib.bib148));BERTgrid(Denk
    and Reisswig, [2019](#bib.bib22)); ACP(Palm et al., [2019](#bib.bib92));XYLayoutLM(Gu
    et al., [2022](#bib.bib34)), for tree=fill=cyan!10] ] [Joint-learning Frameworks,
    for tree=fill=cyan!25 [TRIE(Zhang et al., [2020](#bib.bib145)); VIES(Wang et al.,
    [2021a](#bib.bib124)), for tree=fill=cyan!10] ] [Relation-aware Models, for tree=fill=cyan!25
    [Majumder et al. (Majumder et al., [2020a](#bib.bib81)); Liu et al. (Liu et al.,
    [2019a](#bib.bib74)); PICK(Yu et al., [2021](#bib.bib142)); FormNet(Lee et al.,
    [2022b](#bib.bib57)); FormNetv2(Lee et al., [2023](#bib.bib58)), for tree=fill=cyan!10]
    ] [Few/Zero-shot Learning Models, for tree=fill=cyan!25 [LASER(Wang and Shang,
    [2022](#bib.bib128)); Chen et al.(Chen et al., [2023](#bib.bib15)); Cheng et al.(Cheng
    et al., [2020](#bib.bib18)); QueryForm(Wang et al., [2023a](#bib.bib131)), for
    tree=fill=cyan!10] ] [Prompt-based Frameworks, for tree=fill=cyan!25 [GenKIE(Cao
    et al., [2023b](#bib.bib12)); ICL-D3IE(He et al., [2023](#bib.bib38)); LMDX(Perot
    et al., [2023](#bib.bib95)), for tree=fill=cyan!10] ] ] [Entity Linking, for tree=fill=blue!40
    [Entity-level Linking, for tree=fill=blue!20 [DocStruct(Wang et al., [2020b](#bib.bib130));
    Zhang et al.(Zhang et al., [2021](#bib.bib146)); KVPFormer.(Hu et al., [2023](#bib.bib42)),
    for tree=fill=blue!10] ] [Token-level Linking, for tree=fill=blue!20 [Carbonell
    et al. (Carbonell et al., [2021](#bib.bib13)); SPADE (Hwang et al., [2021](#bib.bib45));DocTR(Liao
    et al., [2023](#bib.bib69)), for tree=fill=blue!10] ] ] [Visual Question Answering,
    for tree=fill=teal!40 [Single Page Frameworks, for tree=fill=teal!25 [Please refer
    Section [3](#S4.F3 "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep
    Learning based Visually Rich Document Content Understanding: A Survey"). , for
    tree=fill=teal!10] ] [Multi-Page Frameworks, for tree=fill=teal!25 [Hi-VT5(Tito
    et al., [2023](#bib.bib117)); GRAM(Blau et al., [2024](#bib.bib9)); Kang et al.
    (Kang et al., [2024](#bib.bib50)), for tree=fill=teal!10] ] ] ]'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2\. Mono-task visually rich document understanding models
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Key Information Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Key Information Extraction (KIE), a typical natural language processing task,
    refers to the task of identifying and extracting crucial pieces of information
    from textual data. Unlike typical name entity recognition methods for addressing
    plain text, VRDs contain visually rich entities like tables and charts, as well
    as spatial and logical layout arrangements to enhance the challenge of extracting
    crucial information. Although plain-text pretrained language models, such as BERT
    (Devlin, [2018](#bib.bib23)), RoBERTa (Liu et al., [2019b](#bib.bib75)), and ALBERT
    (Lan et al., [2019](#bib.bib56)), are widely used as solid baselines on many benchmark
    datasets, more recent works introduce layout-aware pretrained models such as LayoutLM
    families (Xu et al., [2020a](#bib.bib136), [b](#bib.bib138); Huang et al., [2022](#bib.bib43)),
    LiLT (Wang et al., [2022b](#bib.bib123)), Bros (Hong et al., [2021](#bib.bib41))
    to enhance the document representation by leveraging visual and layout information
    and achieving SoTA performance on several downstream tasks (see Section [3](#S4.F3
    "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey")). This section will mainly focus
    on models specifically proposed for the document KIE models or only evaluated
    on KIE benchmark datasets including FUNSD (Jaume et al., [2019](#bib.bib47)),
    CORD (Park et al., [2019](#bib.bib93)), SROIE (Huang et al., [2019](#bib.bib44)),
    XFUND (Xu et al., [2021](#bib.bib137)), etc. Based on innovative aspects, we categorise
    KIE frameworks into five types: Feature-driven models use multimodal cues for
    rich feature representation. Joint Learning frameworks integrate auxiliary tasks
    to enhance document representation. Relation-aware models leverage spatial or
    logical relations via graphs or masked attention mechanisms. Few/Zero-shot learning
    frameworks explore methods for extracting key information with minimal labelled
    data, often using transfer learning. Prompt-based frameworks use structured prompts
    to guide specific information extraction from pretrained models or LLM/MLLMs.
    These categories represent diverse approaches to improving Key Information Extraction
    (KIE) by leveraging specific model designs and learning strategies tailored to
    document understanding challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. Feature-driven Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the initial phase, certain Recurrent Neural Network (RNN)-based models (Lample
    et al., [2016](#bib.bib55)) were introduced, primarily focusing on key information
    extraction tasks from plain text. However, these approaches overlook the importance
    of visual cues and layout information. Hence, several multimodal frameworks with
    feature-driven designs have been proposed to generate more representative document
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: Chargrid (Katti et al., [2018](#bib.bib51)) first mentioned the significance
    of 2D structure for document KIE and designed a character-box-based chargrid to
    convert the textual and 2D layout structure into coloured visual cues feed into
    CNN. Instead of using fine-grained character information, CUTIE (Zhao et al.,
    [2019](#bib.bib148)) and BERTgrid (Denk and Reisswig, [2019](#bib.bib22)) utilise
    various word embedding methods with bounding box coordinates to persevere the
    layout structure. ACP (Palm et al., [2019](#bib.bib92)) leverages attention mechanism
    and multi-aspect features, including visual, semantic (both character and word)
    and spatial, into dilated CNN for capturing both short and long-term dependencies
    of each word piece.
  prefs: []
  type: TYPE_NORMAL
- en: Joint learning KIE frameworks are proposed to leverage multi-level features
    to mitigate the information gap between various focused tasks. TRIE (Zhang et al.,
    [2020](#bib.bib145)) firstly offers an end-to-end framework for simultaneously
    conducting Optical Character Recognition (OCR) and KIE. OCR module will generate
    multi-aspect features, including positional, visual and textual aspects. Adaptively
    trainable weighting mechanisms are adopted to generate the fused embedding, followed
    by a Bi-LSTM-based Entity Extraction Module to conduct the final prediction. VIES
    (Wang et al., [2021a](#bib.bib124)) is introduced to use the multi-level cues
    of vision, position and text to generate more comprehensive representations. Both
    token and segment (entity) level positional and visual features are acquired from
    text detection modules, and dual-level textual features are gathered by text recognition
    brunch, fused by a self-attention-based fusion module for sequence labelling.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Relation-aware Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of only leveraging multimodal information, leveraging the inherent spatial
    or logical relations between multi-level components may lead to more robust and
    comprehensive document representations. Majumder et al. (Majumder et al., [2020a](#bib.bib81))
    propose a model which starts by generating candidates for each field using type-specific
    detectors and key phrases. The neural model scores these candidates by learning
    dense representations that consider both textual content and spatial positioning,
    allowing for accurate information extraction across different document templates
    by focusing on the candidates’ relevance to the fields. Graph-based frameworks
    are increasingly favoured for modelling document elements’ spatial and logical
    relationships. This trend involves meticulously defining distinct graph structures
    and employing graph convolution techniques to incorporate these relationships
    into feature representations seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. (Liu et al., [2019a](#bib.bib74)) first utilise a graph convolution
    module on top of the BiLSTM-CRF for key information extraction. Each document
    is a fully connected graph where the node is the textual representation $T$ of
    the document entity $E_{i}\in\mathbb{E}$, and the edge is related to the spatial
    relation between $E_{i}$ and another entity $E_{j}$. It is defined as $e_{ij}=[x_{ij},y_{ij},\frac{w_{i}}{h_{i}},\frac{h_{j}}{h_{i}},\frac{w_{j}}{h_{i}}]$,
    where $x_{ij}$ and $y_{ij}$ are horizontal and vertical distance between $E_{i}$
    and $E_{j}$. The self-attention-based graph convolution is performed on node-edge-node
    triplets (concatenated nodes and edge embeddings) to learn contextually with all
    other nodes. PICK (Yu et al., [2021](#bib.bib142)) adopts a similar way to construct
    the document graph but utilising multimodal node representations, including transformer-encoded
    textual embedding and CNN-encoded visual embedding, as well as the edge embedding
    is updated to $e_{ij}=[x_{ij},y_{ij},\frac{w_{i}}{h_{i}},\frac{h_{j}}{h_{i}},\frac{w_{j}}{h_{i}},\frac{S_{j}}{S_{i}}]$
    where $S$ is the sentence length of corresponding entity. Additionally, to get
    the node embedding for the downstream tagging task, a soft adjacent matrix-based
    graph learning layer is applied to obtain the task-specific node representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'FormNet (Lee et al., [2022b](#bib.bib57)) introduces an end-to-end framework
    with a new attention mechanism, Rich Attention, and graph framework to make an
    order/distance sensitive long sequence transformer. In Rich Attention, the model
    introduces two order-based and pixel distance scores along the x/y axis. These
    are summed up with usual self-attention scores to enable the model order/distance
    awareness. Additionally, a Graph is applied to contextually learn the neighbouring
    token embedding before serialising the token sequence feeding into the transformer
    encoders. The graph nodes are text embedding of tokens, and the edges are relative
    positions between nodes. This could mitigate the imperfect serialization issue,
    enabling token representation capturing in more contexts. FormNetv2 (Lee et al.,
    [2023](#bib.bib58)) integrates image modality as the additional Graph edge feature
    to capture more visual cues ¹¹1Please refer to Section [6.1.2](#S6.SS1.SSS2 "6.1.2\.
    Visual Representation ‣ 6.1\. Feature Representation ‣ 6\. Critical Discussion
    ‣ Deep Learning based Visually Rich Document Content Understanding: A Survey")
    to get more details. To use contrastive loss to learn the multimodal graph representation,
    they first perform stochastic graph corruption to sample topology-corrupted and
    feature-corrupted graphs. Topology corruption randomly removes edges in the original
    graph, while feature corruption drops all modality features from nodes and edges.
    Then, the contrastive objective is to maximize agreement between a pair of tokens
    between corrupted and original graphs under a standard normalized temperature-scaled
    cross-entropy (Sohn, [2016](#bib.bib110)) loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3\. Few-shot Learning Frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Extracting key information from VRDs based on deep learning frameworks typically
    involves more manual work to annotate the training data. However, acquiring large-scale,
    high-quality annotations in urgent and labouring-limited application scenarios
    is challenging. Thus, few-shot or one-shot frameworks emerged to extract the key
    information with less labouring annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 'LASER (Wang and Shang, [2022](#bib.bib128)) leverage the architecture from
    LayoutReader (Wang et al., [2021b](#bib.bib129)) to reformulate the entity recognition
    task to sequence-labelling to generative model by embedding the entity type information
    (named label surface name) into the target sequence to enable the model label
    semantic-aware. LASER depends on a ”partially triangle” mask to use one encoder
    to encode source text and generate the prediction. The $n$ source text sequence,
    $\{t_{1},...,t_{n}\}$, is the text sequence by summing the relative word, spatial,
    and positional embedding, feeding into the $\mathcal{E}$ with full self-attention.
    The target sequence only attends previous tokens. The generative formulation is
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\displaystyle t_{i-1},[B],t_{i},...,t_{j},[E],e_{1},...,e_{k},[T],t_{j+1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $[B]$ and $[E]$ denote start and end of entity, $e_{i}$ are the label
    surface name; $[T]$ denotes the end of the label surface name. As label surface
    names and functional tokens do not exist, the learnable embeddings are applied
    to them to ensure the generative-progress layout-aware. A binary classification
    module is applied to classify the next generated token type of current generated
    token hidden states: from source or not, effectively controlling the generative
    series.'
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al. (Chen et al., [2023](#bib.bib15)) introduce a novel framework for
    entity-level, N-way soft-K-shot extracting key information from VRDs, focusing
    on the challenges of extracting rare or unseen entities from documents with few
    examples. It leverages a meta-learning approach (Snell et al., [2017](#bib.bib109);
    Oreshkin et al., [2018](#bib.bib91)), utilizing a hierarchical decoder and contrastive
    learning (ContrastProtoNet) for task personalization and improved adaptation to
    new entity types. Additionally, it introduces FewVEX, a dataset tailored for entity-level
    few-shot VDER, to facilitate research and benchmarking in this area. The framework’s
    effectiveness is demonstrated through significant improvements in robustness and
    performance over existing meta-learning baselines.
  prefs: []
  type: TYPE_NORMAL
- en: In the domain of one-shot scenarios, Cheng et al. (Cheng et al., [2020](#bib.bib18))
    presents a novel application of graphs. They utilize attention mechanisms to seamlessly
    transfer spatial relationships between static text regions ( key/landmark) and
    dynamic text regions (value/field) from support documents to query documents.
    This transfer enables the acquisition of label probability distributions for files.
    Additionally, a self-attention-based module is integrated to exploit relationships
    between field entities, facilitating the derivation of label transition probability
    distributions. Finally, belief propagation is harnessed for inference within a
    pairwise Conditional Random Field (CRF), resulting in an assessable end-to-end
    trainable pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4\. Prompt-learning Frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'QueryForm (Wang et al., [2023a](#bib.bib131)) introduce a query-based framework
    for zero-shot document key information extraction. A dual prompting-based mechanism,
    entity query (E-prompt) and schema query (S-prompt), is introduced for pre-training
    and transferring knowledge from large-scale weakly annotated pre-trained webpages
    to target domains. The pretraining target is highly aligned with fine-tuning to
    ensure the proposed framework can consistently make query-conditional predictions
    at both stages. The HTML tags acquire the E-prompt ($\bm{e_{p}}$) during pretraining,
    and the S-prompt ($\bm{\tilde{s}_{p}}$) is generated from webpage domains, while
    during the fine-tuning, $\bm{e_{p}}$ is predefined and S-prompt $\bm{s_{p}}$ is
    learnable vectors. Supposing $\bm{t}$ is the serialised text of the input document,
    the pre-training and fine-tuning target $\bm{\hat{y}}$ and $\bm{y}$ can be represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\displaystyle\bm{\hat{y}}=\mathcal{F}([\bm{s_{p}};\mathcal{E}[\bm{e_{p}},\bm{x}]]),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (3) |  | $\displaystyle\bm{y}=\mathcal{F}([\mathcal{E}[\bm{\tilde{s}_{p}};\bm{e_{p}},\bm{x}]])$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{E}$ and $\mathcal{F}$ in here are the feature encoder and the
    rest of the language model, respectively. The training objective is to minimise
    the cross-entropy loss between $\bm{\hat{y}}$ and $\bm{y}$.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt learning is a technique in natural language processing where models are
    guided by specific prompts to produce or interpret targeted responses. With the
    rise of large-scale models, prompt learning can leverage contextual representation
    and implicit knowledge to enhance performance on targeted tasks. Since most LLMs
    (OpenAI, [2023](#bib.bib90); Touvron et al., [2023](#bib.bib118)) and MLLMs (Liu
    et al., [2024a](#bib.bib72)) are trained on plain text or natural images, layout-aware
    prompting and in-context learning (Brown et al., [2020](#bib.bib10)) methods have
    been introduced to improve understanding in visually rich documents (VRD)
  prefs: []
  type: TYPE_NORMAL
- en: GenKIE (Cao et al., [2023b](#bib.bib12)) proposes an encoder-decoder-based multimodal
    KIE framework to leverage prompt to adapt various datasets and better leverage
    multimodal information. Following (Xu et al., [2020a](#bib.bib136), [b](#bib.bib138)),
    different encoding methods are adopted to acquire textual, layout and visual embeddings.
    Byte Pair Encoding is used as language pretrained backbones, and the OCR extracted
    document content is concatenated with predefined prompts split by the ”[SEP]”
    token between OCR tokens and each prompt. The 2D-positional encoding introduced
    by (Xu et al., [2020a](#bib.bib136)) is used to acquire the layout embedding of
    each OCR extracted token, and ResNet (He et al., [2016](#bib.bib40)) is used to
    extract the visual representations following (Wang et al., [2022c](#bib.bib125)).
    The multimodal representations are fed into an encoder to learn interactively
    between modalities. Prompts, inserted at the end of the encoder’s textual inputs,
    are either template-style or question-style. For the entity extraction task, the
    prompt specifies the target entity type, and the decoder generates the entity
    value (e.g., for ”Company is?”, the decoder outputs the company name). For the
    entity-labeling task, the prompt includes the value, and the decoder provides
    the entity type (e.g., for ”Es Kopi Rupa is [SEP]”, the decoder identifies the
    entity type).
  prefs: []
  type: TYPE_NORMAL
- en: ICL-D3IE (He et al., [2023](#bib.bib38)) is the first framework to employ LLMs
    with in-context learning to extract key information from VRDs using the iterative
    updated diverse demonstrates. Before designing the initial diverse demonstrations,
    the most similar $n$ training documents to the $n$ test samples need to be selected
    by calculating the cosine-similarity of document representations encoded by Sentence-BERT
    (Reimers and Gurevych, [2019](#bib.bib102)). Then, different types of demonstrations
    are introduced to integrate multiple-view context information into LLMs. Hard
    Demonstrations highlight the most challenging cases, are initially designed based
    on the incorrectly predicted cases from GPT-3 (Brown et al., [2020](#bib.bib10))
    predictions and are updated based on prediction results during the training process.
    Layout-aware demonstrations are created by selecting the adjacent hard segments
    to understand the positional relations. Formatting demonstrations are designed
    to guide LLMs in formatting the outputs for easy post-processing.
  prefs: []
  type: TYPE_NORMAL
- en: LMDX (Perot et al., [2023](#bib.bib95)) designs a pipeline to use arbitrary
    LLMs to extract singular, repeated and hierarchical entities from VRDs. The document
    images are first fed into off-the-shelf OCR tools and are divided into smaller
    document chunks to be processed by the LLMs with accessible input length. Then,
    prompts are generated under XML-like tags to control the LLM’s responses and mitigate
    hallucination. Document Representation is a prompt contains the chunk content
    with the coordinates of OCR lines to bring layout modality to LLMs, where a text
    segment extracted by OCR tools is represented ¡text¿ [$x_{centre}$, $y_{centre}$].
    After that, the task description and scheme representation prompts are designed
    to explain the task to accomplish and determine the output format. During inference,
    $N$ prompts with $K$ LLMs completions are generated to sample the correct answers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5\. Summary of Key Information Extraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several models like Chargrid (Katti et al., [2018](#bib.bib51)) and ACP (Palm
    et al., [2019](#bib.bib92)) have enhanced VRDU by integrating visual and textual
    information. Additionally, auxiliary tasks such as OCR, utilized by (Zhang et al.,
    [2020](#bib.bib145); Wang et al., [2021a](#bib.bib124)), aid in improving multimodal
    feature representations through joint training. However, these frameworks often
    rely on smaller, randomly initialized models, which produce less representative
    features compared to those generated by large-scale pre-trained models like LayoutLM
    (Xu et al., [2020a](#bib.bib136)) and SelfDoc (Li et al., [2021b](#bib.bib66)).
    Documents typically exhibit specific layouts and logical structures, which has
    prompted many models (Yu et al., [2021](#bib.bib142); Lee et al., [2022b](#bib.bib57),
    [2023](#bib.bib58)) to adopt graph-based approaches. These methods capture spatial
    and logical correlations among document elements, such as key-value pairs, leading
    to a more comprehensive document representation. While these frameworks have achieved
    improvements in document representation, their effectiveness hinges on having
    sufficient well-annotated training samples, which are time-consuming to acquire.
    This limitation has escalated the demand for few-shot (Wang and Shang, [2022](#bib.bib128))
    and zero-shot (Wang et al., [2023a](#bib.bib131)) frameworks, which leverage contrastive
    learning and innovative attention mechanisms. Additionally, prompt learning has
    been applied to distill implicit knowledge from large-scale layout-aware pre-trained
    models (Hong et al., [2021](#bib.bib41); Tu et al., [2023](#bib.bib119)) and large
    language models (LLMs/MLLMs) (He et al., [2023](#bib.bib38); Perot et al., [2023](#bib.bib95)).
    Despite these advances, a performance gap remains between well-fine-tuned models
    and few/zero-shot frameworks, highlighting the ongoing challenges in VRDU optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Document Entity Linking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Documents are normally structured hierarchically, where parent-child relations
    always exist in various documents, such as key-value pairs in forms and section
    paragraphs in reports or papers. Unlike most VRD key information extraction models,
    which focus on recognising the semantic entity categories in a sequence tagging
    task ignoring the relation between entities, linking the logical associations
    between document semantic entity pairs has recently been of greater interest.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Entity-Level Entity Linking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Document entity linking aims to identify the relation between document entities.
    Some frameworks use the known entity bounding boxes, ignoring the entity recognition
    step and mainly focusing on exploring the relation between input document entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'DocStruct (Wang et al., [2020b](#bib.bib130)) is the first entity linking framework
    by leveraging multimodal features of known document entities to predict the hierarchical
    structure between them. [CLS] token from a BERT-like pretrained language model
    is extracted for entity-level textual representation $T_{e}$, and an RNN is applied
    to encode the sequential RoI visual cues $V_{e}$ along the width of the ResNet-50
    feature maps. Then, $T_{e}$ is concatenated with layout feature $P_{e}$, which
    is a linear projected vector of entity bounding box coordinates, $[x1,y1,x2,y2,x3,y3,x4,y4]$.
    The final entity representation $E$ can be represented:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle\alpha=Sigmoid(W[T_{e},P_{e},V_{e}]+b),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (5) |  | $\displaystyle E=[T_{e};P_{e}]+\alpha V_{e},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ is used as a gate to control the influence of visual cues $V_{e}$
    following (Wang et al., [2019](#bib.bib126)). Then, for pair of entities $E_{i}$
    and $E_{j}$, the probability of an existing parent-child relationship is represented
    as $P_{i\rightarrow j}=E_{i}ME_{j}$ where M is the matrix of asymmetric parameters
    to ensure the asymmetric relation between $E_{i}$ and $E_{j}$. Negative sampling
    (Mikolov et al., [2013b](#bib.bib86)) is applied during training to handle data
    sparsity and balance problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhang et al. (Zhang et al., [2021](#bib.bib146)) propose an entity-linking
    framework, SERA (Semantic Entity Relation extraction As dependency parsing) by
    formulating the entity relation prediction as a dependency parsing problem. LayoutLM
    (Xu et al., [2020a](#bib.bib136)) is used to obtain textual representations at
    the entity level $T_{e}$ that are concatenated with the embedding of the linear
    projected entity label $L_{e}$ to obtain the final entity representation $E$,
    which could be formulated as $E=[T_{e},L_{e}]$. Various encoders, such as Vanilla
    Transformer, BiLSTM, and Graph, are adopted to learn the following contextual
    representations. Then, a Biaffine parser is adopted to calculate the score between
    $E_{i}$ and $E_{j}$. The biaffine parser score $p_{b}$ can be calculated by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\displaystyle h_{i}^{key}=\sigma(W^{key}E_{i}+b^{key}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (7) |  | $\displaystyle h_{j}^{value}=\sigma(W^{value}E_{j}+b^{value}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (8) |  | $\displaystyle p_{b}=h_{i}^{key}W_{b1}h_{j}^{value}+h_{i}^{key}W_{b2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $h_{i}^{key}$ and $h_{j}^{value}$ are the hidden states after linear projection.
    To leverage the layout information, a two-dim layout vector $l_{ij}$ is used to
    get the layout feature score $p_{l}=W_{l}l_{ij}+b_{l}$, where $l_{ij}$ is the
    minimum distance between two entities along with width and height direction respectively.
    Then, the final score $p=p_{b}+p_{l}$ is used to calculate loss using binary or
    multi-label classification.
  prefs: []
  type: TYPE_NORMAL
- en: KVPFormer (Hu et al., [2023](#bib.bib42)) formulate the entity linking to a
    question-answer problem by introducing a transformer-based encoder-decoder structure
    to leverage joint-grained information (token and entity) to predict the entity
    association. The textual representation $T$ of each document entity, $E$, is encoded
    by averaging the inner-token embedding from pretrained document understanding
    models, concatenating with the linear projected entity label embedding $l$, represented
    as $E=[T;l]$. Then, the entity representations will be fed into a Transformer
    encoder, which has a spatial compatibility attention bias into vanilla self-attention
    mechanism ²²2Please refer to . A binary classifier determines which entity is
    a key entity or not. All detected key entities will be treated as question representation
    and fed into a DETR-style (Carion et al., [2020](#bib.bib14)) decoder to predict
    corresponding answers in parallel. For each input question (key query), Top K
    answer candidates will be selected based on the $Sigmoid$ score and fed into a
    softmax layer to get the final prediction, named a coarse-to-fine answer prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Token-level Entity Linking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As acquiring entity information needs prior knowledge from manual annotation
    or layout analysing models, some works utilise serialised OCR-extracted tokens
    as inputs to extract the structured relations. However, as the logical relation
    links semantic entities, token-level frameworks must group tokens into entities
    before exploring their association.
  prefs: []
  type: TYPE_NORMAL
- en: Carbonell et al. (Carbonell et al., [2021](#bib.bib13)) introduces a framework
    comprising three modules for sequentially conducting token grouping, entity labelling
    and relation prediction. Firstly, each text token $t$ is represented by $[L_{t};T_{t}]$
    where $L_{t}=[x,y,w,h]$ is the coco format bounding box coordinates of $t$ and
    $T_{t}$ is the work/representations. All tokens are fed into a token grouping
    GNN, $\mathcal{G}_{group}$, as a node where the edge between nodes is determined
    by k-NN to avoid high consumption of fully connected GNN. The $\mathcal{G}_{group}$
    is trained on a link prediction task to predict the edge score between two nodes
    to group words, of which scores larger than a predefined threshold $\rho$. Then,
    the grouped words are fed into a Graph Attention Network (GAT) to use multi-head
    attention to aggregate words into entities and follow an MLP to conduct node classification
    to predict the category of each document entity. At last, another link prediction
    GNN, $\mathcal{G}_{link}$, is trained on edge classification based on aggregated
    entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'SPADE (Hwang et al., [2021](#bib.bib45)) formulate the token-level entity linking
    as a spatial dependency parsing task for serializing (ordering and grouping) tokens
    and predicting inter-group relation between grouped tokens. Firstly, a spatial
    text encoder is designed to make spatial-aware attention by introducing a relative
    spatial vector considering relative, physical and angle aspects. During this task,
    two binary matrices must be predicted $M_{g}$ for token grouping and inter-group
    linking $M_{l}$. The vertices comprised by a entity types $\mathbb{V}$ and sequence
    of tokens $\mathbb{T}$ the encoded entity category and token are represented as
    $c$ and $t$, respectively, and the relation score between vertices $v_{i}\rightarrow
    v_{j}$ can be calculated by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\displaystyle h_{i}=\begin{cases}c_{v_{i}},&amp;\text{for }v_{i}\in\mathbb{V},\\
    W_{h}t_{v_{i}},&amp;\text{otherwise}\end{cases},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (10) |  | $\displaystyle d=W_{d}v_{j},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (11) |  | $\displaystyle s_{0}=h_{i}^{T}W_{0}d,\ s_{1}=h_{i}^{T}W_{1}d.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The probability is acquired by $p_{ij}=\frac{\exp(s_{0,j})}{\exp(s_{0,j})+\exp(s_{1,j})}$.
    An adjustable threshold is set to construct the $M_{g}$ or $M_{l}$.
  prefs: []
  type: TYPE_NORMAL
- en: DocTR (Liao et al., [2023](#bib.bib69)) formulate the entity linking as an anchor
    word-based entity detection and association problem. Each document entity is represented
    by the anchor word to convert the entity extraction and linking to token-level
    tasks. It contains a Deformable DETR-based vision encoder to extract multi-scale
    visual feature extraction. A LayoutLM based language encoder is applied to encode
    token-level textual representations. The outputs from vision/language encoders
    are fed into the vision-language decoder with the language-conditional queries
    to conduct entity extraction and linking. The decoder queries are one-to-one mapping
    with language encoder inputs. The entity extraction task aims to predict whether
    the query underlying token level input is an anchor word and corresponding categories,
    while entity linking is acquired by
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several models like Chargrid (Katti et al., [2018](#bib.bib51)) and ACP (Palm
    et al., [2019](#bib.bib92)) have enhanced VRDU by integrating visual and textual
    information. Additionally, auxiliary tasks such as OCR, utilised by (Zhang et al.,
    [2020](#bib.bib145); Wang et al., [2021a](#bib.bib124)), aid in improving multimodal
    feature representations through joint training. However, these frameworks often
    rely on smaller, randomly initialised models, which produce less representative
    features compared to those generated by large-scale pretrained models like LayoutLM
    (Xu et al., [2020a](#bib.bib136)) and SelfDoc (Li et al., [2021b](#bib.bib66)).
    Documents typically exhibit specific layouts and logical structures, which has
    prompted many models (Yu et al., [2021](#bib.bib142); Lee et al., [2022b](#bib.bib57),
    [2023](#bib.bib58)) to adopt graph-based approaches. These methods capture spatial
    and logical correlations among document elements, such as key-value pairs, leading
    to a more comprehensive document representation. While these frameworks have achieved
    improvements in document representation, their effectiveness hinges on having
    sufficient well-annotated training samples, which are time-consuming to acquire.
    This limitation has escalated the demand for few-shot (Wang and Shang, [2022](#bib.bib128))
    and zero-shot (Wang et al., [2023a](#bib.bib131)) frameworks, which leverage contrastive
    learning and innovative attention mechanisms. Furthermore, prompt learning has
    been applied to distil implicit knowledge from large-scale layout-aware pretrained
    models (Hong et al., [2021](#bib.bib41); Tu et al., [2023](#bib.bib119)) and large
    language models (LLMs/MLLMs) (He et al., [2023](#bib.bib38); Perot et al., [2023](#bib.bib95)).
    Despite these advances, a performance gap remains between well-fine-tuned models
    and few/zero-shot frameworks, highlighting the ongoing challenges in VRDU optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. VRD Question Answering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike key information extraction, which targets specific details within document
    images, answering natural language questions involves interpreting more complex
    intentions and requires models to facilitate interactive understanding between
    the queries and document representations (Ding et al., [2022](#bib.bib24)). The
    introduction of DocVQA (Mathew et al., [2021](#bib.bib84)) marked a significant
    shift in focus from natural scene images to text-dense, layout-aware single-page
    document images, establishing a benchmark in the field. As advancements have continued,
    demands have recently emerged for models capable of addressing more complex, multi-page
    scenarios (Tito et al., [2023](#bib.bib117); Ding et al., [2023b](#bib.bib26),
    [2024a](#bib.bib27)). These emerging requirements highlight the need for models
    to process multimodal inputs and navigate through extensive documents, reflecting
    user inquiries’ evolving complexity and naturalness in document-based question-answering
    systems. This section will briefly review the SoTAs in single-page document VQA
    models and introduce some recently proposed multi-page document understanding
    solutions (Tito et al., [2023](#bib.bib117); Blau et al., [2024](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Single-page VRD-QA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similar to key information extraction from visually rich documents, single-page
    question answering (QA) often utilises classical pretrained language models (Devlin,
    [2018](#bib.bib23); Liu et al., [2019b](#bib.bib75)) as baselines. These models
    conduct span-based question answering to extract sequences of text tokens. Additionally,
    general domain visual language pretrained models (Li et al., [2019](#bib.bib65);
    Tan and Bansal, [2019](#bib.bib114); Kim et al., [2021](#bib.bib53)) are employed
    to identify the target document’s semantic entities (Ding et al., [2023b](#bib.bib26)).
    Beyond these plain text or general domain vision language, numerous layout-aware
    models specifically pretrained in the document domain (see Section [3](#S4.F3
    "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey")) have achieved state-of-the-art
    (SoTA) performance in single-page document tasks. This approach underscores the
    importance of integrating layout awareness in the preprocessing stages to enhance
    performance on these specific document-based tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Multi-page VRD-QA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As demand increases for retrieving answers from multi-page documents (Tito et al.,
    [2023](#bib.bib117); Ding et al., [2024a](#bib.bib27)), most SoTA models (Xu et al.,
    [2020a](#bib.bib136); Wang et al., [2022b](#bib.bib123); Hong et al., [2021](#bib.bib41)),
    which are typically designed for single-page inputs, face a significant challenge
    due to their input length limitation of 512 tokens. To overcome this limitation,
    recent innovations have introduced solutions such as transformers capable of handling
    longer sequences and page-locating modules. These advancements are specifically
    designed to address the requirements of multipage document question answering
    (QA), enabling more efficient processing.
  prefs: []
  type: TYPE_NORMAL
- en: Hi-VT5 (Tito et al., [2023](#bib.bib117)) proposes a multimodal hierarchical
    encoder-decoder architecture for multi-page generative question-answering. A T5-based
    multimodal transformer is applied to encode the single-page level information,
    including questions, OCR-extracted page content, image patches, and a set of page
    tokens. The question and OCR extracted token sequence is the following (Biten
    et al., [2022](#bib.bib6)) to acquire the layout-aware initial textual representations.
    Document Image Trasnfoer (DIT) (Li et al., [2022b](#bib.bib64)) is leveraged to
    acquire initial patch representations. Then, the concatenated question ($Q$) and
    page OCR tokens ($T$), image patch tokens ($I$), and randomly initialised page
    tokens ($P$) are fed into T5-based page encoder to enhance the multi-level and
    multimodal contextual learning contextually. The enhanced page token embeddings
    $P^{\prime}$ of each input document image are fed into a T5-based decoder to generate
    the predicted answer, as well auto-regressively, and the page located on the target
    answer must output the page number based on $P^{\prime}$. As T5 is not a layout-aware
    language model, masked language modelling is applied to predicted masked tokens
    by leveraging non-based visual and layout information to boost multimodal understanding.
    As T5 can accept input sequence lengths up to 20,480 tokens, it dramatically increases
    the standard VRDU pretrained models that apply Hi-VT5 to multi-page scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'GRAM (Blau et al., [2024](#bib.bib9)): proposes a framework to extend single-page
    models to tackle multi-page document VQA scenarios. Following the way of pretrained
    DocFormerv2 (Appalaraju et al., [2024](#bib.bib3)), the framework encodes single-page
    inputs, including questions, OCR-extracted content, and visual features. For multi-page
    scenarios, a slim global encoder follows each single-page encoder layer, allowing
    the new learnable page tokens to interact with other pages contextually. This
    enables page tokens to capture page-level information through self-attention and
    enhance document-level understanding with sparse attention. As the global layer
    is a new stream applied to the pretrained model, an attention bias is applied
    following ALiBi (Press et al., [2021](#bib.bib97)) to prevent the model from possibly
    disregarding the page tokens, enabling them to capture more fine-grained information
    from pretrained weights. During the decoding stage, unlike Hi-VT5, which only
    feeds page tokens to the decoder, GRAM uses all fine-trained information for the
    decoder. C-Former (Raffel et al., [2020](#bib.bib98)) is applied to alleviate
    the high computation consumption, which could revise the information across all
    pages and distil only important details.'
  prefs: []
  type: TYPE_NORMAL
- en: Kang et al. (Kang et al., [2024](#bib.bib50)) introduce a multi-page document
    VQA framework to use a scoring self-attention mechanism to select and identify
    the related pages for generating the answer to the input question. The training
    processes include single-page Document VQA training and then training a self-attention
    scoring module based on the frozen trained encoder to feed the most relevant page
    information into the decoder for answer generating. Pix2Struct (Lee et al., [2022a](#bib.bib59))
    is used as the single-page model fine-tuned on the DocVQA (Mathew et al., [2021](#bib.bib84))
    dataset. The output from the Pix2Struct is fed into the self-attention scoring
    module to extract the first token for predicting a question-page matching score.
    The page with the highest matching score will be fed into the decoder for answer
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Document Visual Question Answering (Document VQA) is a relatively new field,
    pioneered by DocVQA (Mathew et al., [2021](#bib.bib84)), which aims to generate
    or extract answers to natural language questions based on document images. This
    differs from key information extraction, which focuses on recognizing or extracting
    predefined key-value pairs. Document VQA requires a more comprehensive representation
    of the document and an understanding of correlations between the document and
    questions. Pretrained VRDU models (Xu et al., [2020b](#bib.bib138); Huang et al.,
    [2022](#bib.bib43)) demonstrate robust performance in single-page document understanding
    tasks. However, these models encounter challenges when applied to more typical
    and natural multi-page scenarios due to input length limitations. Recent solutions,
    such as (Tito et al., [2023](#bib.bib117); Blau et al., [2024](#bib.bib9)), mainly
    address these challenges by identifying the page where a possible answer may be
    located and then applying SoTA single-page techniques to retrieve the answer.
    Despite these advancements, real-world applications often present more complex
    situations, such as long-term dependencies and cross-page entity relationships,
    which still need further exploration in the document VQA field.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Multi-Task VRD Understanding Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: for tree= forked edges, grow’=0, draw, rounded corners, node options=align=center,,
    text width=2.7cm, s sep=6pt, calign=child edge, calign child=(n_children()+1)/2
    [Multi-task based models, for tree=fill=brown!45 [Fine-grained Pretrained Models
    (Encoder-only), for tree=fill=violet!40 [Layout-aware Pretrained Models, for tree=fill=violet!20
    [LayoutLM (Xu et al., [2020a](#bib.bib136)); BROS(Hong et al., [2021](#bib.bib41));
    LiLT (Wang et al., [2022b](#bib.bib123)); XDoc (Chen et al., [2022](#bib.bib16));
    LayoutMask (Tu et al., [2023](#bib.bib119)); StructuralLM (Xu et al., [2020a](#bib.bib136)),
    for tree=fill=violet!10] ] [Visual-integrated Pretrained Models, for tree=fill=violet!20
    [LayoutLMv2(Xu et al., [2020b](#bib.bib138)); LayoutXLM(Xu et al., [2021](#bib.bib137));
    DocFormer(Appalaraju et al., [2021](#bib.bib2)); LayoutLMv3(Huang et al., [2022](#bib.bib43)),
    for tree=fill=violet!10] ] ] [Coarse and Joint-grained Pretrained Models (Encoder-only),
    for tree=fill=pink!40 [Coarse-grained Models, for tree=fill=pink!20 [SelfDoc(Li
    et al., [2021b](#bib.bib66)); UniDoc(Gu et al., [2021](#bib.bib33))], for tree=fill=pink!10
    ] [Joint-grained Models, for tree=fill=pink!20 [StrucText(Li et al., [2021c](#bib.bib68));
    Fast-StrucText(Zhai et al., [2023](#bib.bib144)); MGDoc (Wang et al., [2022a](#bib.bib127));
    WUKONG-READER (Bai et al., [2022](#bib.bib4)); GeoLayoutLM (Luo et al., [2023](#bib.bib78)),
    for tree=fill=pink!10] ] ] [Encoder-Decoder Pretrained Frameworks, for tree=fill=red!40
    [OCR-dependent Encoder-Decoder Frameworks, for tree=fill=red!20 [TiLT(Li et al.,
    [2021b](#bib.bib66)); UDOP(Tang et al., [2022](#bib.bib116)); DocFormerv2(Appalaraju
    et al., [2024](#bib.bib3)); ViTLP(Mao et al., [2024](#bib.bib83)), for tree=fill=red!10]
    ] [OCR-independent Frameworks, for tree=fill=red!20 [Donut(Kim et al., [2022](#bib.bib52));
    Dessurt(Davis et al., [2022](#bib.bib21)); ReRum (Cao et al., [2023a](#bib.bib11));
    StructTextV2 (Yu et al., [2022](#bib.bib143)), for tree=fill=red!10] ] [LLM-based
    Frameworks, for tree=fill=red!20 [HRVDA(Liu et al., [2024b](#bib.bib71)); LayoutLLM(Luo
    et al., [2024](#bib.bib79)), for tree=fill=red!10] ] ] [Non-Pretrained Frameworks,
    for tree=fill=yellow!40 [CALM(Du et al., [2022](#bib.bib30)); LayoutGCN(Shi et al.,
    [2023](#bib.bib107)),for tree=fill=yellow!20 ] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. Multi-task visually rich document understanding frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Models designed for specific VRDU tasks often incorporate task-oriented techniques.
    Drawing inspiration from pretrained models in the vision (Dosovitskiy et al.,
    [2020](#bib.bib29); Ren et al., [2015](#bib.bib103)) and language (Devlin, [2018](#bib.bib23);
    Liu et al., [2019b](#bib.bib75)) domains, enhancing document representations may
    significantly improve the performance of various downstream tasks. Consequently,
    a range of models have been developed to extract knowledge from extensive document
    collections. These models employ different pretrained tasks based on their architecture,
    the modalities they process, and the granularity of the information they extract
    from documents. Moreover, some models introduce specialised techniques to boost
    the effectiveness of pretrained model representations, achieving better performance
    without heavy pretraining. This section explores various document understanding
    frameworks focused on enhancing document representation for robustness and comprehensiveness
    in addressing multiple VRDU downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Fine-grained Pretrained Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inspired by BERT-style pretrained models, many researchers have proposed effective
    methods to integrate layout and visual information into models, aiming to enhance
    the comprehensiveness of textual token representations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1\. Layout-aware Pretrained Language Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Understanding layout structure and spatial correlations between textual tokens
    can yield a more comprehensive representation of documents, going beyond what
    plain-text input offers. To this end, various methods have been proposed to encode
    layout features. These methods, combined with tailored pretraining tasks, enable
    models to capture layout-aware information better and effectively fuse textual
    and layout features.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutLM (Xu et al., [2020a](#bib.bib136)) is the first pretrained document
    understanding model by leveraging textual and layout information in the pretraining
    stage. BERT architecture is the backbone and 2-D positional embedding ³³3Please
    refer to Section xx for more detailed information on 2-D positional encoding with
    textual information is used to pretrain on IIT-CDIP Test Collection 1.0\. Two
    specific pretraining tasks are first introduced, named Masked Visual-Language
    Model (MVLM) and Multi-label Document Classification, to generate layout-aware
    textual representation and more comprehensive document representation, respectively.
    Like Masked Language Modeling adopted by most pretrained language models, MVLM
    randomly masks some input tokens but keeps the corresponding 2-D position embeddings
    to predict masked tokens to ensure the pretrained model is aware of the spatial
    relations between input tokens. MDC is a supervised pretraining task to predict
    the input document types (e.g. forms, exam papers, academic papers) that generate
    more comprehensive document-level representations. The fine-tuned LayoutLM could
    perform much better than textual-only frameworks on key information extraction
    (Jaume et al., [2019](#bib.bib47); Huang et al., [2019](#bib.bib44)) and document
    classification (Harley et al., [2015](#bib.bib37)).
  prefs: []
  type: TYPE_NORMAL
- en: BROS (Hong et al., [2021](#bib.bib41)) aims to propose a pretrained VRDU model
    to represent the continuous property of 2D space by introducing a new 2-D positional
    encoding and a textual-spatial correlation aware attention score to replace the
    vanilla self-attention. Supposing $[x1,y1,x2,y2,x3,y3,x4,y4]$ is the bounding
    box coordinates of input token $t$, $p^{1}=[\mathcal{F}_{sin}(x1)\oplus\mathcal{F}_{sin}(y1)],p^{1}\rightarrow\mathbb{R}^{d_{pos}}$.
    The final positional representation of $t$ is $pos_{t}=W_{p1}p_{1}+W_{p2}p_{2}+W_{p3}p_{3}+W_{p4}p_{4}$,
    where all $W_{p}\in\mathbb{R}^{2d_{pos}\times d}$. The new 2-D positional encoding
    intends to provide a more natural way to encode the continuous 2-D coordinates.
    Additionally, instead of simply summing textual and positional representations,
    a novel attention score is introduced to consider both textual and spatial features
    and their correlations. To calculate the attention score $\alpha_{ij}$ between
    $t_{i}$ and $t_{j}$, they consider correlation intra and inter-correlation between
    textual and positional modalities.
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $\displaystyle\alpha_{ij}=(W^{q_{t_{i}}}T_{i})^{\top}(W^{q_{t_{j}}}T_{j})+(W^{q_{t_{i}}}T_{i}\circ
    W^{pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{pos_{t_{j}}}pos_{t_{j}})+(W^{\prime pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{\prime
    pos_{t_{j}}}pos_{t_{j}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $(W^{q_{t_{i}}}T_{i})^{\top}(W^{q_{t_{j}}}T_{j})$ and $(W^{\prime pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{\prime
    pos_{t_{j}}}pos_{t_{j}})$ is the intra-modality attention scores of textual and
    positional modalities between two tokens. $(W^{q_{t_{i}}}T_{i}\circ W^{pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{pos_{t_{j}}}pos_{t_{j}})$
    is used to formulate the spatial dependency given the source semantic representation.
    Additionally, inspired by SpanBERT (Joshi et al., [2020](#bib.bib49)), they use
    an area-masked language model to mask tokens by random-sized rectangle regions.
  prefs: []
  type: TYPE_NORMAL
- en: StructuralLM (Li et al., [2021a](#bib.bib62)) is the first VRUD model using
    image patches (named ”cells” in the paper) to group input tokens for conducting
    various pretraining tasks. They use BERT as the backbone and generate multimodal
    representations $P$ of each patch, which will be used for two pretraining tasks,
    MVLM and Cell (Patch) Position Classification (CPC). The bounding box $(x0,y0,x1,y1)$
    of an image patch is encoded by the 2-D positional encoding introduced by LayoutLM
    and the $n$ tokens inside $p$ are represented as $\{t_{1},t_{2},\dots,t_{n}\}$
    which share the same 2-D positional encoding as for tokens belongs to one patch.
    A token $t_{i}$ could be represented as summing up token representation $T_{i}$,
    2-D positional encoding $pos_{t_{i}}^{2d}$ and 1-D position embedding $pos_{i}$.
    Two pretraining tasks are used to understand the patch-level spatial correlations,
    including MVLM and CPC. MVLM is similar to LayoutLM but uses patch-level layout
    embeddings instead of token-level. Another novel cell position classification
    task is applied to predict the area index of randomly selected tokens from $N$
    evenly split areas. Two pretraining tasks are performed simultaneously to capture
    the patch-level spatial dependencies between input tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'LiLT (Wang et al., [2022b](#bib.bib123)) introduces a language-independent
    layout Transformer for mono/multi-lingual document understanding. The text and
    layout information are first separately encoded and jointly learned during pretraining.
    In the fine-tuning stage, two modality representations are concatenated to perform
    downstream tasks. The input token representations follow BERT, where $j$-th token
    is $T_{j}=T_{j}+pos_{t_{j}}+pos_{t_{j}}^{2d}$. The layout representations are
    slightly different from LayoutLM, normalising the bbox coordinates in the $[0,1000]$
    and four linear layers encode the x-axis, y-axis, width and height features. The
    normalised bbox of a token $t$ is $[x0,y0,x1,y1,w,h]$ is encoded to get the final
    layout representation $L$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $\displaystyle L=W_{L}(W_{x}x0\oplus W_{y}x0\oplus W_{x}x1\oplus
    W_{y}y1\oplus W_{w}w\oplus W_{h}h)+pos_{L}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $W_{L}\in\mathbb{R}^{6d_{L}^{\prime}\times d_{L}}$ and $W_{x},W_{y},W_{w},W_{h}\in\mathbb{R}^{1\times
    d_{L}}$. The text and layout embedding are fed into two sub-models to generate
    high-level representations. A bi-directional attention complementation mechanism
    (BiACM) is proposed to augment the cross-modality interaction ⁴⁴4We provide a
    detailed explanation of BiACM in Section X. Three self-supervised learning methods
    are proposed to enable the model to understand the multimodal document representation,
    including MVLM and Key Point Location, which is similar to the CPC proposed by
    StructureLM to predict the area index of masked tokens based on the layout features.
    Additionally, a text and layout alignment task is proposed to enhance further
    the cross-modality representation to predict whether each pair is aligned. LiLT
    could achieve promising performance on multi-lingual document understanding benchmarks
    (Wang et al., [2021a](#bib.bib124); Xu et al., [2021](#bib.bib137)).
  prefs: []
  type: TYPE_NORMAL
- en: XDoc (Chen et al., [2022](#bib.bib16)) propose a unified framework to deal with
    text inputs from multi-format inputs, including plain text, document and web text.
    Various encoding methods are proposed to tackle diverse text formats. For plain
    text token $t_{pln}$, the token representation follows BERT to get $T_{pln}$.
    For document text token $t_{doc}$, they adopted similar strategies (Xu et al.,
    [2020a](#bib.bib136); Hong et al., [2021](#bib.bib41)) to get $T_{doc}$ by summing
    up initial token representation $E_{t_{doc}}$, 1-D position embedding $pos_{t_{doc}}$
    and 2-D position embedding $pos_{t_{doc}}^{2d}$. $pos_{t_{doc}}^{2d}$ uses different
    bbox formats $[x_{0},x_{1},y_{0},y_{1},w,h]$ following a Linear-ReLu-Linear adaptor
    to make the document format text embedding more representative. To encode web
    text inputs, they follow the way introduced by MarkupLM to encode the source file
    tags and subscript information to get the XPath embedding. The same structured
    adaptor is leveraged for better pretraining. All three text formats are pretrained
    by MLM and fine-tuned on plain text (Rajpurkar et al., [2016a](#bib.bib99); Wang
    et al., [2018](#bib.bib121)), document (Jaume et al., [2019](#bib.bib47); Mathew
    et al., [2021](#bib.bib84)), and web text (Chen et al., [2021](#bib.bib17)) benchmark
    datasets, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutMask (Tu et al., [2023](#bib.bib119)) aims to improve the text-layout
    interactions by using local 1D positional encoding and new pretraining tasks.
    OCR-dependent document understanding frameworks are suffered from improper reading
    orders. It uses LayoutLMv2 as the backbone but removes the visual module, and
    the local 1D positional encoding is adopted to replace the global 1D positional
    encoding, which only encodes the token ordered within each segment detected by
    OCR tools and always restarts with 1 from each individual segment. Moreover, the
    2D positional encoding is also using segment bbox instead of individual words.
    The first task pertains to traditional masked language modelling, but two different
    masking strategies are used. Firstly, they set the mask at the word level instead
    of masking the token itself to enable the model to capture more contextual information
    to predict the masked word. Additionally, to boost cross-segment understanding,
    the probability of masking the first and last word of a segment is higher than
    that of others. Another Masked Position Modelling is proposed to ask the model
    to predict the masked word-level 2D positions to promote the layout information
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. Visual Integrated Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Integrating visual cues alongside text and layout information during the pretraining
    stage can significantly enhance model capabilities, capturing more comprehensive
    document insights than text and layout alone. Various frameworks and pretraining
    tasks focusing solely on text and layout have been expanded to include visual-text
    matching tasks to strengthen cross-modality alignment. This integration enables
    models to interpret better the complex interplay of visual, textual, and layout
    features within documents. Existing models that utilise these comprehensive inputs
    can be categorised based on their approach to visual feature acquisition: feature
    map-based models, which generate comprehensive visual representations, and patch
    pixel-based models, which focus on granular visual details. This classification
    helps understand how different models leverage visual information to enhance document
    understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: LayoutLMv2 (Xu et al., [2020b](#bib.bib138)) is the first pretrained model integrating
    text, layout and visual aspects during the pretraining stage. A single framework
    multimodal transformer is applied to get the visual, text and layout features
    simultaneously, where each input textual and visual tokens are assigned a segment
    ID to distinguish the modality or semantic type. The textual and layout representations
    generally follow LayoutLM, except the linear projected segment ID, $seg_{t}$,
    embeddings are addicted to original textual embeddings. The input document image
    is firstly fed into a trainable visual encoder (based on ResNeXt-FPN), of which
    output is evenly split into $m$ flattened visual tokens, $v_{token}$ following
    a linear layer to project into the same dimension as textual embedding. The $j$-th
    visual token embedding is $V_{j}=W_{v_{j}}+pos_{j}+W_{seg_{v}}$. Moreover, to
    capture the relative position information of inter/intra-modality features, spatial-aware
    self-attention is introduced by adding bias terms of 1-D ($b^{1D}$) and 2-D ($b^{2D}$)
    relative position. The 1-D relative positional bias between input visual or textual
    tokens $t_{i}$ and $t_{j}$ is $b^{1D}=W_{b_{1D}}(j-i)$ and $b^{2D}=b^{2D}_{x}+b^{2D}_{y}$,
    where $b^{2D}_{x}=W_{b^{2D}_{x}}(x0_{i}-x0_{j})$, $b^{2D}_{y}=W_{b^{2D}_{y}}(y0_{i}-y0_{j})$.
    LayoutLMv2 is pretrained on MVLM and the other two new proposed tasks, Text-Image
    Alignment (TIA) and Text-Image Matching (TIM) ⁵⁵5Please refer to Section to check
    more detailed information about those two pretraining tasks. to capture more cross-modality
    alignment.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutXLM (Xu et al., [2021](#bib.bib137)) extends the LayoutLMv2 architecture
    to a multilingual setup where the MVLM is extended to Multilingual Mased Visual-Language
    Modeling pretrained on 22 million self-collected digital-born PDF files and 8
    million scanned English documents from IIT-CDIP processed by off-the-shelf PDF
    parsers.
  prefs: []
  type: TYPE_NORMAL
- en: DocFormer (Appalaraju et al., [2021](#bib.bib2)) is a multimodal transformer
    encoder-based architecture which also uses a trainable CNN-backbone (ResNet50)
    to extract the visual cues of input document image to get visual representations
    $\mathbb{V}\in\mathbb{R}^{(d\times N)}$, where $d=768$ is the transformer hidden
    state and $N=512$ is the number of visual tokens. The initial textual representations
    $\mathbb{T}$ are acquired by feeding the OCR extracted text with bbox into LayoutLM
    where $\mathbb{T}\in\mathbb{R}^{(d\times N)}$. 2D positional encoding is also
    used by using $W^{x}_{v},W^{y}_{v}$ and $W^{x}_{t},W^{y}_{t}$ to encode $x,y$
    axis, as well as, visual and textual aspects to acquire $pos^{2d}_{v}$ and $pos^{2d}_{t}$.
    More positional features are adopted such as width, height and relative distance
    between neighbours. Moreover, certain inductive biases are applied to get a new
    self-attention score $\alpha$ for paying more attention to local features. Supposing
    $\alpha_{ij}^{v}$ is attention score between visual feature $V_{i}$ and $V_{j}$,
    we could have $\alpha_{ij}^{v}=(W^{K}_{v}V_{j})^{\top}(W^{Q}_{v}V_{i})+(pos_{ij})^{\top}W^{Q}_{v}V_{i}+(pos_{ij})^{\top}W^{K}_{v}V_{j}+(W^{Q}_{s}pos_{ij}^{2d})^{\top}(W^{K}_{s}pos_{ij}^{2d})$,
    where $pos_{ij}$ is the 1D relative positional encoding. The same procedures are
    applied to textual features as well, and two modalities share the same $W^{Q}_{s}$
    and $W^{K}_{s}$ to help the model correlate features across modalities. The multi-modal
    token representations $\mathbb{M}$ of $l$ encoder layer is $\mathbb{M}_{l}=\mathbb{T}_{l}+\mathbb{V}_{l}$.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutLMv3 (Huang et al., [2022](#bib.bib43)) is the first pretrained VRDU model
    to encode visual features without using heavy CNN-backbones. LayoutLMv3 uses the
    exact same way as LayoutLMv2 to encode the textual and layout information except
    replacing the word-level bbox to segment-level for 2-D positional embedding. For
    visual feature representation, they follow ViT and ViLT to linearly project the
    evenly split image patches with learnable 1-D positional encoding to the transformer
    encoder. For the pretraining setting, they use masked language modelling to mask
    30% tokens drawn from a Poisson distribution. To augment the visual representation
    by contextually learning with multimodal features, a Masked Image Modelling (MIM)
    (Bao et al., [2021](#bib.bib5)) task is used to mask 40% image tokens with clockwise
    masking randomly. Each image token will be converted into discrete tokens following
    (Ramesh et al., [2021](#bib.bib101)), and a cross-entropy loss is adopted to reconstruct
    the masked image tokens. Moreover, to explicitly learn the correlation between
    text and image modalities, a Word-Patch Alignment (WPA) pretraining objective
    is applied. WPA is a binary classification task to predict whether the unmasked
    token-patch pair is ”aligned” or ”unaligned”.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Coarse and Joint-grained Pretrained Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-grained models achieve state-of-the-art performance on many downstream
    tasks but face challenges with input length limitations and capturing document
    image layout and logical arrangement. To address these issues, coarse-grained
    or joint-grained frameworks have been introduced. To mitigate these limitations,
    these frameworks leverage multimodal information from document semantic entities
    such as paragraphs, tables, and textlines.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Coarse-grained Frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SelfDoc (Li et al., [2021b](#bib.bib66)) is the first pretrained VRDU model
    to leverage coarse-grained document elements for various document understanding
    tasks. Unlike fine-grained models that rely on OCR-extracted text sequences, SelfDoc
    uses Faster-RCNN to extract Regions of Interest (RoIs) from document entities.
    This approach reduces the input sequence length for text-dense and long documents
    with improved time and space complexities. To acquire the initial multimodal representations,
    Faster-RCNN extracts visual embeddings from these RoIs, while Sentence-BERT (Reimers
    and Gurevych, [2019](#bib.bib102)) provides textual embeddings based on the OCR-extracted
    text of each entity. The textual and visual representations are fed into two single-modality
    BERT-like encoders to learn the intra-modality correlations between entities contextually.
    A cross-modality encoder is introduced to enhance inter-modality learning, mirroring
    the structure of single-modality encoders but incorporating cross-attention layers.
    It randomly masks entities in document images within either language or vision
    branches during pretraining. Furthermore, a modality-adaptive attention mechanism
    dynamically adjusts weights for visual and textual embeddings based on the input
    and task, creating robust entity representations for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: UniDoc (Gu et al., [2021](#bib.bib33)) is an entity-level document understanding
    model which contains a trainable image encoder with RoI-Align (He et al., [2017](#bib.bib39))
    to extract entity visual representation and novel cross-attention mechanisms to
    fuse multimodal information. The initial sentence and textual embeddings are acquired
    by RoI features and averaged word embeddings summing up with the linear projected
    bbox coordinates. To enable the trainable visual representation to predict meaningful
    visual cues effectively, product quantization (Jegou et al., [2010](#bib.bib48))
    is applied to discretise a RoI feature into a finite set of visual representations
    and mapped to a new embedding. Additionally, a novel Gated Cross-Attention is
    introduced to improve the interactive learning between various modalities. After
    a multi-head cross-attention to get the enhanced visual and textual representations,
    a gating mechanism is applied to dynamically weight the visual ($V$) and textual
    features ($T$) by feeding concatenated $[V:T]$ to a non-linear network to acquire
    a modality-aware attention bias $\beta_{v},\beta_{t}$ for visual and textual respectively.
    There are three pretraining tasks are conducted simultaneously to enhance multimodal
    feature representatives. Masked Sentence Modelling and Visual Contrastive Learning
    are required to predict the masked sentence representation and quantize visual
    representation, respectively. Another Vision-Language Alignment introduced by
    LayoutLMv2 (Xu et al., [2020b](#bib.bib138)) to enforce contextual learning between
    multi-modalities. But instead of using split image regions, UniDoc aligns the
    image and text to belonging to the same entity.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Joint-grained Frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'StrucText (Li et al., [2021c](#bib.bib68)) is a multimodal pretrained VRDU
    model using fine-grained textual and coarse-grained vision information to capture
    richer geometric and semantic information from different levels and modalities.
    The layout embedding of each text token and segment is encoded by $L=W_{l}[x0,y0,x1,y1,w,h],W_{1}\in\mathbb{R}^{6\times
    N}$. They follow token-level models to encode the sequence of tokens and use pretrained
    ResNet50 with FPN to extract the entity visual features. A segment-ID embedding
    is allocated to each token’s textual and entity visual representations to boost
    the alignment learning. Three self-supervised tasks are introduced to enhance
    inter-modality learning: MVLM, Segment length Prediction (SLP), and Paired Box
    Direction (PBD). As a new self-supervised learning task, SLP asks the model to
    predict the entity’s length to leverage the entity’s visual embeddings and textual
    information from the same segment ID. Another self-supervised learning task aims
    to learn more comprehensive pair-wise spatial correlations between entities by
    clarifying their spatial correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: Fast-StructText (Zhai et al., [2023](#bib.bib144)) is built on StrucText to
    improve model efficiency and enhance the feature expressiveness by introducing
    an hourglass transformer and Symmetry Cross-Attention mechanisms (SCA). Similar
    feature encoding methods to StructText are adopted, but the bbox formats of layout
    encoding are $[x0,y0,x1,y1]$. To encode the module and progressively reduce the
    redundancy tokens, an hourglass transformer is proposed consisting of several
    Merging blocks and Extension blocks to down-sampling and up-sampling the number
    of input tokens. Merging blocks suggest merging neighbouring $k$ tokens with the
    weighted 1D average pooling to shorten the sequence length. The extension block
    is required to transform the shortened sequence back to the entire length by simply
    applying a repeat up-sampling method. In addition, to enhance the interaction
    between textual and vision modalities, SCA consists of two dual cross-attention
    to handle text and visual features. Different self-supervised tasks are conducted
    simultaneously, including MVLM (Xu et al., [2020b](#bib.bib138); Li et al., [2021c](#bib.bib68)),
    Graph-based Token Relation (GTR), Sentence Order Prediction (SOP) and Text-Image
    Alignment (Huang et al., [2022](#bib.bib43)). For the two new proposed tasks,
    GTR is a task similar to Paired-Box Direction to predict the positional relations
    between paired entity visual features and SOP is used to predict whether two sentence
    pairs are normal-order adjacent to learning semantic knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: MGDoc (Wang et al., [2022a](#bib.bib127)) is the first multimodal multi-granular
    pretrained framework which introduces multi-granular attention and cross-modal
    attention to increase the inter-grained learning and better cross-modality fusion.
    For acquiring initial features of different modalities, pretrained language models
    (Reimers and Gurevych, [2019](#bib.bib102)) and vision encoders (He et al., [2016](#bib.bib40))
    are used to encode different-grained inputs from word to whole page. The encoded
    modalities are associated with positional features (Zhai et al., [2023](#bib.bib144))
    and modality-type embeddings (Li et al., [2021c](#bib.bib68)) to acquire the final
    input representation of various modalities. To encode the hierarchical relation
    between multi-grained features, including pages, entities and words, two attention
    biases are added to the original self-attention weights. A hierarchical bias is
    a binary value of 0 or 1 to examine the inside or outside relation between two
    inputs, while the relation bias is the relative positive between two bboxes. Apart
    from multi-grained learning, cross-attention is applied to fuse cross-modality
    information better. Three tasks are proposed during pretraining, including Mask
    Text Modeling (MTM), Mask Vision Modeling (MVM) and Multi-Granularity Modeling.
    MTM and MVM randomly mask the multi-grained textual and visual representations
    to predict the masked features under the mean absolute error. To boost the interactive
    understanding of multi-granularity, a token-entity linking prediction is proposed
    by computing the dot-product between token and entity features.
  prefs: []
  type: TYPE_NORMAL
- en: WUKONG-READER (Bai et al., [2022](#bib.bib4)) uses fine-grained level inputs
    but leverages coarse-grained level self-supervised tasks to enhance the fine-grained
    level information. The model inputs contain a document image and OCR extracted
    sequence of tokens with their bboxes. A Mask-RCNN is used as a visual backbone
    to acquire several visual tokens and extract visual features of textlines from
    a RoIHead layer. For textual information encoding, the first is layers of RoBERTa(Wei
    et al., [2020](#bib.bib134)) are applied to extract the token representation and
    sum up with additional features following (Xu et al., [2020b](#bib.bib138)). The
    concatenated visual and textual features are fed into a multimodal modal encoder
    based on the rest six layers of RoBERTa. Different pretraining tasks are proposed,
    including MLM, Textline-Region Contrastive Learning (TRC), Masked Region Modeling
    (MRM) and Textline Grid Matching (TCM). TRC aims to enhance cross-modality interactive
    learning at the entity (textline) level by following a text-vision-aligned contrastive
    learning approach (Yao et al., [2021](#bib.bib140)). To enhance the visual representation,
    MRM is applied to mask 15% of textlines randomly and to predict the masked visual
    embeddings following (Li et al., [2021b](#bib.bib66)). A text-grid alignment task
    is introduced to enhance layout understanding by dividing the document image into
    N-grids and predicting the tokens in 15% selected textlines belonging to which
    grid. The scaling parameters are applied to each loss of pretraining tasks.
  prefs: []
  type: TYPE_NORMAL
- en: GeoLayoutLM (Luo et al., [2023](#bib.bib78)) is a sophisticated multimodal framework
    that distinctively incorporates geometric information through specialised pretraining
    tasks and the development of innovative relation heads. Inspired by the dual-stream
    structure of METER and SelfDoc (Li et al., [2021b](#bib.bib66)), GeoLayoutLM features
    separate vision and text-layout modules coupled with interactive co-attention
    layers that enhance the integration of visual and textual data. The model introduces
    two advanced relation heads—the Coarse Relation Prediction (CRP) head and the
    Relation Feature Enhancement (RFE) head—which refine relation feature representation
    crucial for both pretraining and fine-tuning phases. The pretraining regimen includes
    tasks designed to understand geometric relationships, such as GeoPair, GeoMPair,
    and GeoTriplet, aiding the model in grasping the complex dynamics of document
    layouts. During fine-tuning, the model utilises pretrained parameters to optimise
    both semantic entity recognition and relation extraction tasks, employing a novel
    inference technique that enhances relation pair selection accuracy by focusing
    on the most probable relationships and minimizing variance among potential options.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Encoder-Decoder Pretrained Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the above encoder-only frameworks, researchers have proposed
    encoder-decoder pretrained models that often approach tasks like Key Information
    Extraction (KIE) or Visual Question Answering (VQA) in a generative style. Addressing
    limitations of OCR-dependent frameworks, such as accumulated OCR errors and incorrect
    reading orders, OCR-independent models have been introduced for end-to-end VRDU.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1\. OCR-dependent Encoder-Decoder Frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TILT (Powalski et al., [2021](#bib.bib96)) is a T5-based transformer encoder-decoder
    architecture enhanced with a relative spatial bias in the self-attention mechanism
    to acquire fine-grained token representations. It incorporates encoding methods
    that apply relative sequence input bias and capture horizontal and vertical distance
    biases in the attention scores. A U-Net-based framework is applied to extract
    the fixed-size feature maps fed into the encoder together. They follow T5 pretraining
    strategies on RVL-CIDP dataset (Harley et al., [2015](#bib.bib37)) but use a salient
    span masking scheme adopted by (Raffel et al., [2020](#bib.bib98); Guu et al.,
    [2020](#bib.bib36)). As the first encoder-decoder framework, TILT requires off-the-shelf
    OCR tools to acquire the textual token sequence. To conduct VRD content understanding
    end-to-end, some OCR-free frameworks are proposed to solve the limitations of
    OCR-dependent models.
  prefs: []
  type: TYPE_NORMAL
- en: UDOP (Tang et al., [2022](#bib.bib116)) proposes an encoder-decoder pre-trained
    document understanding framework that leverages a ViT-based model (Dosovitskiy
    et al., [2020](#bib.bib29)), following the principles of LayoutLMv3 (Huang et al.,
    [2022](#bib.bib43)), to process multimodal information. To enhance the comprehensiveness
    of textual token embeddings, the framework sums the text token embeddings with
    token-aligned image patch embeddings when the centre of the bounding box falls
    within the image patch. Additionally, the positional biases introduced by TILT
    are added, but ID positional encoding is not used. The decoding stage is designed
    to generate all vision, text and layout modalities consisting of a bi-directional
    Transformer text-layout decoder and an MAE vision decoder. Two decoders are cross-attended
    with each other. The pertaining tasks contain self-supervised on unlabeled documents
    to learn robust document representation and supervised pertaining tasks for fine-grained
    model supervision. Masked text layout and image reconstruction are used to predict
    the masked information using unmasked multimodal information. A layout moulding
    task is applied to predict the positions of groups of text tokens, and a visual
    text recognition task is proposed to identify text at the given location in the
    image. Supervised tasks utilise a training set of publicly available benchmark
    datasets of different downstream tasks, including document classification (Harley
    et al., [2015](#bib.bib37)), KIE (Stanisławek et al., [2021](#bib.bib112)), VQA
    (Tanaka et al., [2021](#bib.bib115); Mathew et al., [2021](#bib.bib84)) and layout
    analysing (Zhong et al., [2019](#bib.bib150)).
  prefs: []
  type: TYPE_NORMAL
- en: DocFormerv2 (Appalaraju et al., [2024](#bib.bib3)) is an encoder-decoder transformer
    architecture that uses multimodal (visual, textual and positional) to enhance
    the multimodal understanding and layout-aware language decoder to predict the
    predictions. The patched image pixels are fed into the convolutional and linear
    layers to get down-sampled patch embedding. The textual embeddings are acquired
    by linear projected token one-hot encoding. Both visual and textual embeddings
    are summed with the 2D-positional encoding (Xu et al., [2020a](#bib.bib136)) of
    Patches and linear project bbox embedding ($[x0,y0,x1,y1]$) (Wang et al., [2022b](#bib.bib123)),
    respectively. Two encoder-based Token to Line (T2L), Token to Grid (T2G), and
    one decoder-based self-supervised learning task, MLM, are proposed to enable multimodal
    feature interactive learning. T2L aims to improve the relative position understanding
    between tokens by predicting the number of textlines between two randomly selected
    tokens. For improving the layout and structure, understanding needs to split the
    image into $m\times n$ grids to predict the located grid number of each OCR token.
    For the decoder MLM, the spatial feature of each masked text token is masked as
    well, and the other setup follows T5 (Raffel et al., [2020](#bib.bib98)).
  prefs: []
  type: TYPE_NORMAL
- en: 'ViTLP (Mao et al., [2024](#bib.bib83)) introduces an encoder-decoder architecture
    for OCR and document understanding using a ViT-based vision encoder to obtain
    image patch representations, which are fed into a decoder to generate text and
    layout sequences autoregressively. A special ”[LOC]” token, encoding bounding
    box coordinates $[x0,y0,x1,y1]$, reduces the layout token sequence length. To
    control generation flow, ”[BOS]” and ”[CONT]” tokens are added, representing input
    sequences of two tokens, t1,t2, as ”[BOS], t1, [LOC], t2, [LOC]”. The decoder
    has hierarchical heads: the text head uses all tokens to generate the next text
    token, while the layout head uses ”[LOC]” tokens to predict bounding box coordinates.
    The ”[CONT]” token handles sequences of arbitrary length by continuing generation
    until ”[END]”, based on the prefix token ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2\. OCR-free Pretrained Frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Donut (Kim et al., [2022](#bib.bib52)) is the first OCR-free VRD understanding
    model to understand and extract key information from input document images. Donut
    contains a Swin Transformer-based visual encoder to encode the input document
    image into image patches, which are then fed into a BART-based (Lewis et al.,
    [2020](#bib.bib61)) decoder pretrained on multi-lingual scenarios. During model
    training, teacher forcing is applied, and in the test stage, inspired by GPT-3
    (Brown et al., [2020](#bib.bib10)), prompts with special identify tokens are fed
    into the model for different downstream tasks. The output token sequence contains
    the special tokens $<START\_*>$ and $<END\_*>$ to identify the type of tasks and
    struct predict entities. The wrongly structured entity will be treated as an empty
    prediction. The model is pretrained on next-token prediction on the IIT-CDIP dataset
    and a Synthetic Dataset, which can be interpreted as a pseudo-OCR task. Similar
    to Donut, Dessurt (Davis et al., [2022](#bib.bib21)) also proposes an encoder-decoder
    architecture but a different decoding process. Instead of using BART, the cross-attention
    used in Dessurt attends to all visual, query and previously generated textual
    information and is pretrained on more synthetic datasets with different font sizes
    and handwritten content.
  prefs: []
  type: TYPE_NORMAL
- en: 'ReRum (Cao et al., [2023a](#bib.bib11)) introduce an end-to-end architecture
    in which the decoding process focuses on local interest and visual cues. Like
    other OCR-free frameworks, a Swin-Transformer extracts image patch representations.
    The extracted visual features are fed into a transformer-based Query-Decoder to
    acquire the vision-enhanced query representation. The enhanced query representation
    with visual features is fed into a Text Decoder to generate the predicted text
    tokens auto-regressively. Notably, a Content-aware Token Merge technique is proposed
    to dynamically focus on more relevant foreground parts of visual features, which
    selects top-K visual tokens based on the averaged correlation scores between query
    and visual token representations. Additionally, the unselected visual tokens (called
    Background Area) contain rich global features that could be used to enhance the
    Top-K foreground visual tokens through basic attention mechanisms. Three pretraining
    tasks are applied: query to segmentation (Q2S), text to segmentation (T2S), and
    segmentation to text (S2T). Q2S follows DETR setup for a token generation but
    alters to an instance segmentation task to predict $N$ mask for the target text
    area to improve text detection ability. T2S acquire the Text Decoder output to
    conduct another instance segmentation task after feeding the image text snippet
    into the Text Decoder to boost the layout-aware textual information understanding.
    The last S2T is to use the outputs from the Text Decoder to generate the token
    like an OCR task auto-regressively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'StrucTextV2 (Yu et al., [2022](#bib.bib143)) is an end-to-end structure that
    uses image-only input to conduct several downstream tasks. It contains a CNN-based
    visual extractor with FPN strategies (Lin et al., [2017](#bib.bib70)) and follows
    ViT (Dosovitskiy et al., [2020](#bib.bib29)) to get linear projected flattened
    patch-level representations. The patch token embeddings serve as the input to
    the Transformer encoder to enhance the contextually semantic representations.
    Then, the lightweight fusion network is applied to generate the final representations
    and fed into two branches during pretraining: made language Modeling (MLM) and
    Masked Image Modeling (MIM). Instead of using text inputs when MLM is used by
    other models (Devlin, [2018](#bib.bib23)), a portion of the text regions are masked
    with RGB values $[255,255,255]$ randomly with a 2-layer MLP decoder to predict
    the masked token. MIM masks the rectangular text regions and predicts the RGB
    values of the missing pixels to improve the document representations. Except for
    the global average pooled FPN fused visual representations, the MLM-generated
    hidden state of each text region is concatenated and fed into a Fully Convolutional
    New York to get the regressed masked missing pixel values.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. LLM-based Frameworks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: HRVDA (Liu et al., [2024b](#bib.bib71)) aims to propose a MLLM accepting high-resolution
    image inputs to conduct fine-grained information extraction from VRDs. A swin-transformer
    (Liu et al., [2021](#bib.bib76)) is used to encode document images into image
    patch tokens. A pluggable content detector then identifies visual tokens that
    contain relevant document content information. Following this, a content filtering
    mechanism performs token pruning to remove irrelevant tokens. The remaining encoded
    visual tokens are processed through an MLP to ensure consistency with the LLM
    embedding space dimensions. These pruned tokens are then fused with instruction
    features, allowing further filtering of tokens irrelevant to the instructions.
    The final streamlined set of visual tokens and instructions is fed into the LLM,
    which generates the corresponding responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'LayoutLLM (Luo et al., [2024](#bib.bib79)) introduces an LLM/MLLM-based approach
    integrated with a pretrained document understanding model to address the challenges
    of applying LLMs to zero-shot document understanding tasks. The input document’s
    visual, textual, and layout information and any question text are encoded by a
    pretrained LayoutLMv3 (Huang et al., [2022](#bib.bib43)) encoder and projected
    into the same embedding space as the adopted LLM, Vicuna-7B-v1.5 (Zheng et al.,
    [2024](#bib.bib149)). The method incorporates layout-aware pretraining tasks at
    three levels: document-level (e.g., document summarization), region-level (e.g.,
    layout analysis), and segment-level (e.g., MVLM). These tasks enable the model
    to achieve comprehensive document understanding. Additionally, a novel module
    called LayoutCoT is designed to help LayoutLLM focus on question-relevant regions
    and generate accurate answers through intermediate steps. GPT-3.5-turbo (OpenAI,
    [2023](#bib.bib90)) is used to prepare the dataset for document summarization
    training and to construct LayoutCoT training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Non-Pretrained Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CALM (Du et al., [2022](#bib.bib30)) introduces a common-sense augment document
    understanding framework to understand the query and extrapolate answers not contained
    in the context of the input document image. They follow LayoutLMv2 (Xu et al.,
    [2020b](#bib.bib138)) to encode input document multimodal representations. The
    textual token embeddings are fed into a Document Purifier component to merge the
    tokens $\{t_{1},\dots,t_{n}\}$ belonging to one entity type $N$ to one Upper Layer
    token $\hat{c}$ by applying average pooling of $\hat{c}=AvePool(t_{1},\dots,t_{n})$.
    Each Upper Layer token is concatenated with the commence augmented on ConceptNet
    NumberBatch (Speer et al., [2017](#bib.bib111)) entity word vector $c^{\prime}$
    to get the final entity representation $c=concat(\hat{c},c^{\prime})$. A similar
    Question-Purifier is applied to use common-sense knowledge to enhance the question
    representation. Then, with the assistance of ConceptNet, relevant common-sense
    knowledge is recalled based on the common-sense representation of both documents
    and queries. By considering the predicted question-answer relationship, a final
    self-attentive graph convolutional network following (Zhu et al., [2021b](#bib.bib153))
    is proposed to address document reasoning tasks more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutGCN (Shi et al., [2023](#bib.bib107)) proposes a lightweight and effective
    model which contains a fully connected graph where text blocks are nodes and edges
    connect every two blocks. The model architecture includes a TextCNN-based (Kim,
    [2014](#bib.bib54)) encoder to encode N-gram textual embeddings, a linear trainable
    layout encoder to project the normalised bbox coordinates into hyperspace following
    other layout-aware models, and a visual encoder (CSP-Darknet (Wang et al., [2020a](#bib.bib122))
    for document image features). These features are integrated using a Graph Convolution
    Network (GCN) to capture relationships between nodes. The final node representation
    combines text, layout, and visual information, benefiting various VRDU tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'XYLayoutLM (Gu et al., [2022](#bib.bib34)) introduces an Augmented XY Cut module
    to correct the improper reading order generated by OCR engines and a Dialted Conditional
    Position Encoding module to handle the variable lengths of input text and image
    tokens. LayoutXLM (Xu et al., [2021](#bib.bib137)) ⁶⁶6Please refer to Section [4.1.2](#S4.SS1.SSS2
    "4.1.2\. Visual Integrated Models ‣ 4.1\. Fine-grained Pretrained Models ‣ 4\.
    Multi-Task VRD Understanding Models ‣ Deep Learning based Visually Rich Document
    Content Understanding: A Survey") to see a detailed description of the LayoutXLM
    model. is adopted as a basic framework to process multimodal inputs. The Augmented
    XY Cut module enhances traditional XY Cut (Nagy and Seth, [1984](#bib.bib87))
    by incorporating thresholds ($\lambda_{x},\lambda_{y}$) and a shift factor ($\theta$)
    to adjust box positions on the x and y axis based on randomly generated values.
    It recursively divides token boxes using valleys in horizontal and vertical projection
    profiles, forming clusters in descending order. Each cluster’s reading order is
    determined recursively, prioritizing divisions until no significant valleys remain,
    ensuring a proper reading order is derived from an XY Tree structure. DCPE (Dilated
    Conditional Position Encoding) addresses limitations of CPE (Chu et al., [2022](#bib.bib19))
    in multimodal tasks by separately processing textual and visual features. It employs
    1D convolutions for textual tokens to extract 1D local layouts, accommodating
    their inherent 1D relationships. Additionally, DCPE utilizes dilated convolutions
    (Yu and Koltun, [2015](#bib.bib141)) to capture long-range dependencies effectively
    without increasing model complexity, thereby enhancing performance in multimodal
    document understanding tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Summary of Multi-Task Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Various models are proposed to enhance document representations for VRDU tasks
    by leveraging pretrained language models to enrich text token sequences with layout
    information through positional encoding (Xu et al., [2020a](#bib.bib136)), attention
    mechanisms (Wang et al., [2022b](#bib.bib123)), and layout-aware tasks (Tu et al.,
    [2023](#bib.bib119)). However, VRDs contain rich visual details like font, texture,
    and colour and visually complex entities such as tables, charts, and photos. Many
    models (Xu et al., [2020b](#bib.bib138); Huang et al., [2022](#bib.bib43); Appalaraju
    et al., [2021](#bib.bib2)) integrate visual cues to enhance fine-grained document
    features, but their quadratic time and space complexity pose challenges for handling
    long sequences in multi-page document understanding (Ding et al., [2024a](#bib.bib27)).
    Fine-grained models excel but struggle with capturing layout and structural details
    from document images. Coarse-grained frameworks (Li et al., [2021b](#bib.bib66);
    Gu et al., [2021](#bib.bib33)) mitigate fine-grained limitations by leveraging
    entity-level multimodal information, yet compressing diverse entity aspects into
    a single dense vector risks losing information (Ding et al., [2024b](#bib.bib28)).
    Joint-grained frameworks (Li et al., [2021c](#bib.bib68); Zhai et al., [2023](#bib.bib144);
    Wang et al., [2022a](#bib.bib127); Bai et al., [2022](#bib.bib4); Luo et al.,
    [2023](#bib.bib78)) integrate multi-grained information to produce comprehensive
    representations. Non-pretrained models leverage external knowledge (Du et al.,
    [2022](#bib.bib30)) or lightweight networks (Shi et al., [2023](#bib.bib107))
    to rival large-scale pretrained frameworks’ performance. Most document understanding
    models (Xu et al., [2020a](#bib.bib136); Huang et al., [2022](#bib.bib43); Wang
    et al., [2022b](#bib.bib123); Li et al., [2021b](#bib.bib66)) rely on off-the-shelf
    OCR tools for text extraction, which can be susceptible to OCR quality issues
    and incorrect reading orders. OCR-free frameworks directly process document images
    to mitigate these limitations; However, these frameworks may exhibit sub-optimal
    performance compared to methods using established OCR tools with additional resource
    consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Visually Rich Document Content Understanding Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1\. Key Information Extraction and Entity Linking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Name Conf./J. Year Domain # Docs # Images # Keys MP. Language Metrics Format
    FUNSD ICDAR-w 2019 Multi-source N/A 199 4 N English F1 P.& H. SROIE ICDAR-c 2019
    Scanned Receipts N/A 973 4 N English F1* P. CORD Neurips-w 2019 Scanned Receipts
    N/A 1,000 54 N English F1 P. Payment-Invoice ACL 2020 Invoice Form N/A 14,237+595
    7 N English F1 D. Payment-Receipts ACL 2020 Scanned Receipts N/A 478 2 N English
    F1 P. Kleister-NDA ICDAR 2021 Private Agreements 540 3,229 4 Y English F1 D. Kleister-Charity
    ICDAR 2021 AFR 2,778 61,643 8 Y English F1 D.& P. EPHOIE AAAI 2021 Exam Paper
    N/A 1,494 10 N Chinese F1 P.& H. XFUND ACL 2022 Synthetic Forms N/A 1,393 4 N
    Multilingual F1 D.& P.& H. Form-NLU SIGIR 2023 Financial Form N/A 857 12 N English
    F1 D.& P.& H. VRDU-Regist. Form KDD 2023 Registration Form N/A 1,915 6 N English
    F1 D. VRDU-Ad-buy Form KDD 2023 Political Invoice Form N/A 641 9+1(5) N English
    F1 D.&P. DocILE ICDAR 2023 Invoice Form 6,680 106,680 55 Y English AP, CLEval
    D.&P.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Summary of visually rich document key information extraction datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Scanned Receipt Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SROIE (Huang et al., [2019](#bib.bib44)) is a widely used dataset for text
    localization, OCR, and key information extraction from scanned receipts, introduced
    in the ICDAR 2019 Challenge on ”Scanned Receipts OCR and Key Information Extraction”
    The Key Information Extraction (KIE) task focuses on four key types: Address,
    Date, Company, and Total, with corresponding values provided in the annotation
    file for each receipt. The F1 score for this task is calculated based on Mean
    Average Precision (MAP) and recall. Note that entity-level annotations are not
    provided in the official dataset, requiring the use of external tools to obtain
    the information for coarse-grained or joint-grained models.'
  prefs: []
  type: TYPE_NORMAL
- en: Payment-Receipts (Majumder et al., [2020b](#bib.bib82)) is a subset of SROIE,
    created by sampling up to 5 documents from each template in the original SROIE
    dataset. The template of each receipt is decided by the Company annotation. The
    target schema focuses on extracting only Date and Total. This subset is used to
    evaluate the model’s ability to handle unseen templates.
  prefs: []
  type: TYPE_NORMAL
- en: CORD (Park et al., [2019](#bib.bib93)) is a widely used dataset for post-OCR
    receipt understanding, featuring two-level labels annotated by crowdsourcing workers.
    It includes eight superclasses, such as Store, Payment, Menu, Subtotal, and Total,
    each with several subclasses. For example, Store contains subclasses like Name,
    Address, and Telephone. CORD provides both textline-level and word-level annotations
    for both fine-grained and coarse-grained frameworks, with some sensitive information
    blurred. All models are evaluated on the released first 1,000 samples.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Form-style Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FUNSD (Jaume et al., [2019](#bib.bib47)) is derived from the RVL-CDIP dataset
    (Harley et al., [2015](#bib.bib37)) by manually selecting 199 readable and diverse
    template form images. The dataset is annotated using the GuiZero library to provide
    both entity and word-level annotations, including manual text recognition. Semantic
    links indicate relationships between entities, such as Question-Answer or Header-Question
    pairs. Consequently, FUNSD supports key information extraction, OCR, and entity
    linking tasks.
  prefs: []
  type: TYPE_NORMAL
- en: XFUND (Xu et al., [2021](#bib.bib137)) is the first multilingual dataset following
    the FUNSD format. It collects form templates in seven languages (Chinese, Japanese,
    Spanish, French, Italian, German, and Portuguese) from the internet. Human annotators
    fill these templates with synthetic information by typing or handwriting, ensuring
    each template is used only once. The filled forms are then scanned into document
    images, processed with OCR, and annotated with key-value pairs. Each language
    has 199 annotated forms, supporting multilingual key information extraction and
    entity linking tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Payment-Invoice (Majumder et al., [2020b](#bib.bib82)) contains two corpora
    of invoices from different sources. The first corpus, Invoice 1, includes 14,273
    invoices from various vendors with different template styles, used for training
    and validation. The second corpus, Invoice 2, comprises 595 documents with distinct
    templates not found in Invoice 1, serving as the test set. Human annotators extract
    six required keys from each single-page invoice, such as Invoice Date, Total Amount,
    and Tax Amount. This dataset is suitable for evaluating generative-style models
    and fine-grained sequence labeling models. For coarse-grained models, additional
    text line or entity-level information can be extracted using off-the-shelf tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'VRDU-Registration Form (Wang et al., [2023b](#bib.bib132)) is a dataset of
    registration forms about foreign agents registering with the US government collected
    from the Federal Communications Commission. Commercial OCR tools extract the text
    content of the forms. Annotators draw bounding boxes around six unrepeated entities
    (each entity appears only once per document) per document: File Date, Foreign
    Principal Name, Registrant Name, Registration ID, Signer Name, and Signer Title.
    The dataset provides entity-level annotations, which can be easily preprocessed
    to acquire token-level annotations, supporting any granularity of Key Information
    Extraction (KIE) models.'
  prefs: []
  type: TYPE_NORMAL
- en: VRDU-Ad-buy Form (Wang et al., [2023b](#bib.bib132)) consists of 641 invoices
    or receipts signed between TV stations and campaign groups for political advertisements.
    It follows the same annotation procedure as the VRDU-Registration Form but involves
    a more complex schema. This includes nine unique entities (e.g., Advertiser, Agency,
    Contract ID), four repeated entities (e.g., Item Description, Sub Prices), and
    hierarchical entities (e.g., Line Item). Repeated entities may contain different
    values within a single document, while hierarchical entities comprise several
    repeated entities as components.
  prefs: []
  type: TYPE_NORMAL
- en: Form-NLU (Ding et al., [2023a](#bib.bib25)) is a visual-linguistics dataset
    designed to support researchers in interpreting specific designer intentions amidst
    various types of noise from different form carriers, including digital, printed,
    and handwritten forms. Fine-grained key-value pairs, such as Company Name, Previous
    Notice Date, and Previous Shares, are manually annotated. The training and validation
    set comprises 535 digital-born forms, with 76 reserved for validation. Additionally,
    three test sets are provided, containing 146 digital, 50 printed, and 50 handwritten
    form images, respectively. Form-NLU can be used to evaluate form layout analysis
    and Key Information Extraction (KIE) models of any granularity. With proper processing,
    it can also be used to evaluate entity linking frameworks, thanks to the well-annotated
    key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3\. Multi-page Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kleister-NDA (Stanisławek et al., [2021](#bib.bib112)) is a dataset collected
    from the Electronic Data Gathering, Analysis, and Retrieval System (EDGAR) focusing
    on Non-disclosure Agreements (NDAs). During preprocessing, the collected 540 HTML
    files are converted into digital multi-page PDF files (totalling 3,229 pages)
    using the Puppeteer library. Four key items, Effective Date, Party, Jurisdiction,
    and Term, are manually annotated by three annotators to extract the corresponding
    values. The NDA dataset is widely used by fine-grained level models but may require
    additional processing for frameworks with limited sequence length due to the multi-page
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Kleister-Charity (Stanisławek et al., [2021](#bib.bib112)) contains 2,778 annual
    financial reports from the Charity Commission, which lack strict formatting rules.
    The Charity Commission website provides eight key pieces of information, such
    as Postcode, Charity Name, and Report Date. Annotators manually correct minor
    errors to ensure accuracy. Compared to Kleister-NDA, the Charity dataset has longer
    document inputs, totalling 61,643 pages, requiring models to handle long sequence
    outputs. Both Charity and NDA datasets provide only key-value pair annotations,
    making them suitable for the generation and fine-grained sequence labelling tasks
    but requiring additional processing to acquire entity-level annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 'DocILE (Šimsa et al., [2023](#bib.bib108)) comprises three subsets: an annotated
    set of 6,680 real business documents, an unlabeled set of 932,000 real business
    documents for unsupervised pretraining, and a synthetic set of 100,000 documents
    generated with full task labels. Documents come from public sources like the UCSF
    Industry Documents Library and Public Inspection Files, with annotations for Key
    Information Localization and Extraction and Line Item Recognition. Synthetic documents
    were created using annotated templates and a rule-based synthesizer. DocILE provides
    entity-level annotations that can be easily post-processed to acquire token-level
    annotations.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Visual Question Answering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Name | Conf./J. | Year | Domain | Lang. | # Docs | # Images | # Questions
    | Answer Type | MP | Format | Metrics | Annotation |'
  prefs: []
  type: TYPE_TB
- en: '| DocVQA | WACV | 2021 | Industrial Reports | English | N/A | 12,767 | 50,000
    | Text | N | D./P./H. | ANLS | Human |'
  prefs: []
  type: TYPE_TB
- en: '| VisualMRC | AAAI | 2021 | Website | English | N/A | 10,197 | 30,562 | Text
    | N | D. | BLUE, etc | Human |'
  prefs: []
  type: TYPE_TB
- en: '| TAT-DQA | MM | 2022 | Financial Reports | English | 2,758 | 3,067 | 16,558
    | Text/RS-Gen. | Y | D. | EM, F1 | Human |'
  prefs: []
  type: TYPE_TB
- en: '| RDVQA | MM | 2022 | Data Analysis Report | N/A | 8,362 | 8,514 | 41,378 |
    Text | N | D. | ANLS, ACC | Human |'
  prefs: []
  type: TYPE_TB
- en: '| CS-DVQA | MM | 2022 | Industry Documents | English | N/A | 600 | 1,000 |
    Text and Nodes | N | D./P./H. | ANLS | Human |'
  prefs: []
  type: TYPE_TB
- en: '| PDFVQA-Task A | ECML-PKDD | 2023 | Academic Paper | English | N/A | 12,337
    | 81,085 | Num or Yes/No | N | D. | F1 | Template |'
  prefs: []
  type: TYPE_TB
- en: '| PDFVQA-Task B | ECML-PKDD | 2023 | Academic Paper | English | N/A | 12,337
    | 53,872 | Entity | N | D. | F1 | Template |'
  prefs: []
  type: TYPE_TB
- en: '| PDFVQA-Task C | ECML-PKDD | 2023 | Academic Paper | English | 1,147 | 12,337
    | 5,653 | Entity | Y | D. | EM | Template |'
  prefs: []
  type: TYPE_TB
- en: '| MPDocVQA | PR | 2023 | Industrial Reports | English | 6,000 | 48,000 | 46,000
    | Text | Y | D./P./H. | ANLS | Human |'
  prefs: []
  type: TYPE_TB
- en: '| DUDE | ICCV | 2023 | Cross-domain | English | 5,019 | 28,709 | 41,541 | Text,
    Yes/No | Y | D. | ANLS | Human |'
  prefs: []
  type: TYPE_TB
- en: '| MMVQA | IJCAI | 2024 | Academic Paper | English | 3,146 | 30,239 | 262,928
    | Entity | Y | D. | EM, PM, MR | LLM + Human |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. Summary of visually rich document question answering datasets
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Single Page VRD-QA Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DocVQA (Mathew et al., [2021](#bib.bib84)) is a pioneering dataset in document-based
    Visual Question Answering (VQA), sourced from the UCSF Industry Document Library.
    It comprises 50,000 manually generated questions framed on 12,767 document images,
    encompassing digital, printed, and handwritten formats. The dataset follows an
    extractive-style QA format similar to benchmarks like SQuAD (Rajpurkar et al.,
    [2016b](#bib.bib100)) and VQA (Biten et al., [2019b](#bib.bib8)). Evaluation typically
    involves fine-grained models such as LayoutLM variants (Xu et al., [2020a](#bib.bib136),
    [b](#bib.bib138); Huang et al., [2022](#bib.bib43)), LiLT (Wang et al., [2022b](#bib.bib123)),
    and generative models (Tang et al., [2022](#bib.bib116); Kim et al., [2022](#bib.bib52)),
    using metrics like Average Normalized Levenshtein Similarity (ANLS) (Biten et al.,
    [2019a](#bib.bib7)). However, it requires additional processing for coarse-grained
    models like SelfDoc (Li et al., [2021b](#bib.bib66)). CS-DVQA (Du et al., [2022](#bib.bib30))
    builds upon DocVQA by enhancing QA pairs to better reflect real-world requirements.
    It extracts 600 images from the DocVQA dataset and generates 1,000 QA pairs under
    human supervision. During question generation, it incorporates common-sense knowledge
    from real life, expanding answers beyond extractive in-line text to include question-related
    nodes (Nodes) sourced from ConceptNet (Speer et al., [2017](#bib.bib111)).
  prefs: []
  type: TYPE_NORMAL
- en: VisualMRC (Tanaka et al., [2021](#bib.bib115)) is compiled from website screenshots
    across 35 domains, carefully selected to exclude pages with handwritten content
    and to prefer pages containing short text (no more than 2 to 3 paragraphs). Unlike
    other datasets that might only provide question-answer annotations (Mathew et al.,
    [2021](#bib.bib84)) or automatically acquire document semantic entities (Ding
    et al., [2023b](#bib.bib26)), VisualMRC includes manually annotated layout structures
    with fine-grained semantic entity types such as Heading, Paragraph, Subtitle,
    Picture, and Caption. Question-answer pairs are generated through crowdsourcing.
    Consequently, VisualMRC is well-suited for evaluating both fine-grained and coarse-grained-based
    QA frameworks, providing a rich resource for assessing the effectiveness of models
    in understanding and interpreting detailed document layouts and semantic entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'PDFVQA-Task A and Task B (Ding et al., [2023b](#bib.bib26)) form part of the
    first document VQA dataset from PubMed Central, focusing on content and structural
    understanding. This dataset includes three tasks: two for single-page documents
    (Tasks A and B) and one for multi-page documents (Task C). Task A evaluates the
    structural and spatial relationships within document images, with answers being
    either counts or Yes/No. Task B focuses on extracting document entities based
    on their logical and spatial configurations. The PDFVQA dataset provides only
    coarse-grained, entity-level annotations, necessitating further processing for
    models that require fine-grained analysis. This setup is ideal for testing models’
    capabilities in understanding the logical and spatial structures of document images.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Multi-Page VRD-QA Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TAT-DQA (Zhu et al., [2022](#bib.bib151)), an extension of the TAT-QA (Zhu et al.,
    [2021a](#bib.bib152)) dataset, is developed with more complex natural document
    structures and an expanded set of manually corrected and generated question-answer
    pairs derived from business financial reports. Unlike other datasets that primarily
    focus on extractive or simple abstractive answers (such as counting or yes/no),
    TAT-DQA includes questions requiring arithmetic reasoning, where values must be
    extracted from tabular data and textual content for discrete calculations. This
    dataset adopts the evaluation metrics of TAT-QA, including Exact Matching and
    a numeracy-focused F1 score. These metrics are particularly tailored to assess
    the accuracy of arithmetic reasoning and data extraction capabilities of the models
    tested with TAT-DQA.
  prefs: []
  type: TYPE_NORMAL
- en: RDVQA dataset (Wu et al., [2022](#bib.bib135)) compiles a large collection of
    conversational chats and associated images from an E-commerce platform. It employs
    standard OCR and Named Entity Recognition (NER) techniques to extract text and
    redact sensitive information, ensuring privacy protection through masking. The
    dataset includes question-answer pairs within the images, which are manually verified
    to confirm image clarity and the presence of at least one question-answer pair
    per image. Although some documents span multiple pages, this dataset is structured
    such that it can be processed relatively easily by single-page VRD-QA models.
  prefs: []
  type: TYPE_NORMAL
- en: PDFVQA-Task C (Ding et al., [2023b](#bib.bib26)) is a distinct sub-task within
    the PDFVQA dataset that expands document VQA to encompass entire long documents,
    moving beyond the single-page focus of Tasks A and B. In Task C, to answer questions,
    the model often needs to retrieve information from multiple document entities.
    Thus, Task C employs Exact Matching for its ground truth annotations. Similar
    to Tasks A and B, additional processing is required for evaluating models at a
    fine-grained level.
  prefs: []
  type: TYPE_NORMAL
- en: MP-DocVQA (Tito et al., [2023](#bib.bib117)) extends the original DocVQA (Mathew
    et al., [2021](#bib.bib84)) dataset to accommodate multi-page document analysis.
    This version includes adjacent pages from the same documents, expanding the dataset
    from 12,767 to 64,057 document images. In adapting to a multi-page format, some
    questions inappropriate were removed. However, it’s important to note that while
    the dataset allows for questions across multiple pages, the answers remain confined
    to individual pages; there are no cross-page answers in the MP-DocVQA dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'DUDE (Van Landeghem et al., [2023](#bib.bib120)) is the first cross-domain,
    multi-page document VQA dataset, featuring a diverse collection of documents from
    various fields such as medical, legal, technical, and financial, and different
    document types including CVs, reports, and papers. It comprises 5,019 documents,
    28,709 document pages, and 41,541 manually annotated questions. Question types
    vary from extractive in-line text and Yes/No answers to multi-hop reasoning and
    structural understanding, similar to those in PDFVQA (Ding et al., [2023b](#bib.bib26)).
    To evaluate model performance, DUDE uses the ANLS metric (Mathew et al., [2021](#bib.bib84))
    for assessing answer prediction accuracy. Additionally, it employs two other metrics:
    Expected Calibration Error (Guo et al., [2017](#bib.bib35)) and Area-Under-Risk-Coverage-Curve
    (CURC) (Geifman and El-Yaniv, [2017](#bib.bib32); Jaeger et al., [2022](#bib.bib46))
    to gauge the overconfidence and miscalibration in document understanding models.
    These features make DUDE a comprehensive tool for evaluating cross-domain document
    understanding models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MMVQA (Ding et al., [2024a](#bib.bib27)) is a dataset sourced from PubMed Central,
    designed for the retrieval of multimodal semantic entities from multi-page documents.
    The questions are generated using ChatGPT (OpenAI, [2023](#bib.bib90)) and subsequently
    verified manually. Unlike other datasets that focus solely on in-line text or
    text-dense entities, MMVQA also considers entire tables and figures as potential
    answers to the given questions. This dataset introduces various evaluation metrics
    to cater to different application scenarios: Exact Matching and Partial Matching
    Accuracy assess the precision of responses, while Multi-label Recall evaluates
    how well the model identifies all relevant answers across the document. This diverse
    set of metrics makes MMVQA suitable for comprehensive performance evaluation in
    complex, multimodal document understanding tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Summary of VRDU Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The common characteristics of KIE and VRD-QA datasets are represented in Table [1](#S5.T1
    "Table 1 ‣ 5.1\. Key Information Extraction and Entity Linking ‣ 5\. Visually
    Rich Document Content Understanding Datasets ‣ Deep Learning based Visually Rich
    Document Content Understanding: A Survey") and Table [2](#S5.T2 "Table 2 ‣ 5.2\.
    Visual Question Answering ‣ 5\. Visually Rich Document Content Understanding Datasets
    ‣ Deep Learning based Visually Rich Document Content Understanding: A Survey").
    Most existing benchmark datasets for key information extraction are designed for
    single-page scenarios and predominantly cater to English documents. However, real-world
    applications often involve more complex multi-page forms. Even forms typically
    require input from multiple parties and contain multiple languages, such as customs
    or import/export declaration forms, presenting unique challenges not addressed
    by current datasets. Similarly, in the domain of VRD-QA, while multi-page VRD-QA
    has gained increased attention, the integration and exploration of multimodal
    information remain insufficiently developed. Specifically, structure and relation-aware
    VRD-QA, which is crucial for interpreting and understanding cross-page relationships,
    is still a largely unexplored area.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Critical Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [3](#S3 "3\. Mono-Task Document Understanding Frameworks ‣ Deep
    Learning based Visually Rich Document Content Understanding: A Survey") and Section [3](#S4.F3
    "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey"), the models proposed for handling
    both mono-task and multi-task Visually Rich Document Understanding (VRDU) are
    introduced. This section aims to provide a summary and analysis of relevant techniques,
    including their application scenarios, advantages, and disadvantages. Several
    key topics are covered: Feature Engineering, Cross-Modality Fusion, Model Architecture,
    and Pre-training Mechanisms. Additionally, with the rapid development of Large
    Language Models (LLMs), LLM-based frameworks and emerging trends in applying LLMs
    to VRDU are discussed.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Feature Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.1.1\. Textual Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Text in VRDs provides essential semantic context, crucial for understanding
    content and conducting various downstream tasks. Depending on the information
    granularity required by the framework and application scenarios, textual representation
    methods can generally be categorized at the word or entity level.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3615efd6ebdfd0e27c83fb988e2ebb51.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Fine-grained (word-level) and coarse-grained (entity-level) textual
    information encoding for VRDU frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Word-level Representations In Visually Rich Documents (VRDs), text sequences
    extracted by off-the-shelf OCR tools or PDF parsers (e.g., PDFMiner) can be encoded
    using word embedding methods such as Word2Vec (Mikolov et al., [2013a](#bib.bib85))
    and Glove (Pennington et al., [2014](#bib.bib94)). For more comprehensive textual
    embeddings, various BERT-style bi-directional pretrained transformer models like
    BERT (Devlin, [2018](#bib.bib23)) and RoBERTa (Liu et al., [2019b](#bib.bib75))
    are employed to generate context-aware word representations. As the visually rich
    and structurally complex nature of VRDs, layout-aware and visual-integrated fine-grained
    models have been developed, such as LayoutLM families (Xu et al., [2020a](#bib.bib136),
    [b](#bib.bib138); Huang et al., [2022](#bib.bib43); Xu et al., [2021](#bib.bib137)),
    LiLT (Wang et al., [2022b](#bib.bib123)). These models generate word representations
    that integrate multimodal information, combining text, visual cues, and layout
    structure to achieve SoTA performance on several downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Entity-level Representations To acquire a dense representation of a text sequence
    for performing entity-level VRDU tasks, various approaches are adopted. These
    include averaging word embeddings specific to an entity or leveraging the $<CLS>$
    token to encapsulate the entire sequence, including averaging the word embeddings
    belonging to an entity or using $<CLS>$ token to represent an entire sequence.
    SentenceBERT (Reimers and Gurevych, [2019](#bib.bib102)) is also often adopted
    to encode text sequences within document entities. However, a standardized approach
    for acquiring textual representations of entities is yet to be established, necessitating
    preliminary testing and validation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2\. Visual Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f0d9ed8e847bf4ebd3dae4af349868f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Commonly adopted visual information encoding approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual information provides layout, structural insights, and rich contextual
    clues, making it easier for humans to interpret and prioritize content and resulting
    in a more comprehensive reading experience. Based on the methods used for encoding
    visual information, we categorize them into two main types: CNN-based and Vision
    Transformer-based approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: CNN-based Vision Encoding methods involve first acquiring Region of Interest
    (RoI) bounding boxes and then applying RoI-pooling and RoI-Align on pretrained
    CNN backbones (e.g., Faster-RCNN or Mask-RCNN) to extract the region features.
    Many frameworks (Xu et al., [2020a](#bib.bib136); Li et al., [2021b](#bib.bib66);
    Luo et al., [2022](#bib.bib80); Ding et al., [2024a](#bib.bib27); Zhai et al.,
    [2023](#bib.bib144); Ding et al., [2023b](#bib.bib26); Gu et al., [2021](#bib.bib33))
    utilize word or entity-level RoIs to effectively extract visual features of target
    regions and leverage the implicit knowledge embedded in pretrained backbones.
    However, acquiring the bounding boxes of words or entities incurs additional costs.
    Therefore, several frameworks (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137);
    Bai et al., [2022](#bib.bib4); Appalaraju et al., [2021](#bib.bib2)) directly
    use image patch bounding RoIs to extract visual features and learn contextually
    with other modalities.
  prefs: []
  type: TYPE_NORMAL
- en: After acquiring the visual features, they are typically fed into a transformer
    framework to fuse multimodal information, which can create significant computational
    bottlenecks. Additionally, acquiring high-quality RoIs of words or entities requires
    supervised training. To address these challenges, LayoutLMv3, inspired by ViT
    (Dosovitskiy et al., [2020](#bib.bib29)), introduces a transformer-only framework.
    This approach applies a linear layer to project flattened patch pixels, which
    are then fed into a multimodal transformer to contextually learn with other modalities.
    This method reduces the number of parameters and simplifies the preprocessing
    steps, making it more efficient and adopted by many recent frameworks (Tang et al.,
    [2022](#bib.bib116); Appalaraju et al., [2024](#bib.bib3); Mao et al., [2024](#bib.bib83)).
    However, this encoding method cannot take advantage of implicit knowledge in pre-trained
    frameworks and typically requires extensive pre-trained.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3\. Layout Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b91737df9f5662d1e13f9a0bb111e77e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Commonly adopted layout information encoding approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Layout information is crucial for understanding document elements’ spatial arrangement,
    including words and entities. Enhance document representation by clarifying the
    spatial relationships between these elements, thereby aiding in the comprehension
    of the overall document structure. The coordinates of the bounding box (bbox)
    of the document elements serve as initial layout information. This layout information
    can then be encoded using methods such as positional encoding, linear projection,
    and spatial-aware attention bias.
  prefs: []
  type: TYPE_NORMAL
- en: 2D positional encoding, first introduced by LayoutLM (Xu et al., [2020a](#bib.bib136)),
    and widely adopted by many visually rich document understanding (VRDU) models,
    allows the model to be aware of the relative spatial positions within a document.
    In this approach, document elements are normalized and discretized into integer
    ranges, and two separate embedding layers are used to encode the x and y coordinates,
    respectively. Despite its widespread use in models like (Xu et al., [2020a](#bib.bib136),
    [b](#bib.bib138), [2021](#bib.bib137); Huang et al., [2022](#bib.bib43); Tu et al.,
    [2023](#bib.bib119); Wang et al., [2022b](#bib.bib123); Bai et al., [2022](#bib.bib4);
    Liao et al., [2023](#bib.bib69))this method encodes x and y coordinates individually,
    making it challenging to represent continuous 2D space and capture special correlations
    between document elements. Some models (Ding et al., [2023a](#bib.bib25); Wang
    et al., [2020b](#bib.bib130)) that follow the approach of LXMERT (Tan and Bansal,
    [2019](#bib.bib114)) utilize linear projection to update the x and y coordinates
    of the normalized bounding box coordinates simultaneously. To address the limitations
    of absolute positional encoding and incorporate relative positional correlations,
    other models introduce spatial-aware attention mechanisms (Hong et al., [2021](#bib.bib41);
    Appalaraju et al., [2021](#bib.bib2); Wang et al., [2022b](#bib.bib123)). These
    mechanisms enable vanilla self-attention to learn spatial dependencies effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Multi-modality Fusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f75b57ef7fd56929ca92731438775b3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. Commonly adopted multi-modality fusion methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'After acquiring multimodal representations, it is important to explore effective
    fusion methods to integrate textual, visual, and layout information. This integration
    improves document understanding and boosts performance on downstream tasks. The
    straightforward integration methods, as shown in Figure [7](#S6.F7 "Figure 7 ‣
    6.2\. Multi-modality Fusion ‣ 6\. Critical Discussion ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey"), include the additive and concatenation
    of the feature vectors. For example, additive integration sums layout information
    with corresponding textual or visual token representations (Xu et al., [2020a](#bib.bib136),
    [b](#bib.bib138); Huang et al., [2022](#bib.bib43); Li et al., [2021b](#bib.bib66)),
    while concatenation merges visual and textual features of document entities (e.g.
    tables) (Ding et al., [2024a](#bib.bib27), [2023b](#bib.bib26)). However, these
    methods require one-to-one correlations and alternative approaches are needed
    when such correlations are not available. Consequently, self-attention and cross-attention
    mechanisms are widely adopted to enhance each modality by learning inter- and
    intramodality contexts. These mechanisms are commonly used in frameworks (Xu et al.,
    [2020b](#bib.bib138); Huang et al., [2022](#bib.bib43); Tu et al., [2023](#bib.bib119);
    Bai et al., [2022](#bib.bib4); Xu et al., [2021](#bib.bib137)) that integrate
    patch-level visual embeddings with textual features for contextual learning. Novel
    self-attention (Appalaraju et al., [2021](#bib.bib2); Wang et al., [2022b](#bib.bib123))
    and cross-attention (Gu et al., [2021](#bib.bib33); Li et al., [2021b](#bib.bib66);
    Zhai et al., [2023](#bib.bib144)) methods have been proposed to fuse multimodal
    information more effectively. Apart from model-based fusion approaches, self-supervised
    and joint learning tasks are also effective for integrating multi-aspect features.
    Self-supervised pretraining tasks such as Masked Visual-Language Modeling (Wang
    et al., [2022b](#bib.bib123); Xu et al., [2020a](#bib.bib136), [b](#bib.bib138);
    Li et al., [2021a](#bib.bib62); Huang et al., [2022](#bib.bib43); Zhai et al.,
    [2023](#bib.bib144)), Text-Image Alignment (Appalaraju et al., [2021](#bib.bib2)),
    Text-Layout Pairing (Wang et al., [2022b](#bib.bib123)), and Text-Image Matching
    (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137)) can significantly enhance
    multimodal information fusion. These methods require large-scale pretraining to
    learn cross-modality semantic correlations. Joint learning methods, often used
    in OCR-free frameworks (Wang et al., [2021a](#bib.bib124); Kim et al., [2022](#bib.bib52);
    Davis et al., [2022](#bib.bib21); Yu et al., [2022](#bib.bib143)), design auxiliary
    text detection or recognition tasks to fuse textual and visual information. This
    approach reduces pre-processing during inference and addresses mis-ordering sequence
    issues. However, these methods generally underperform compared to OCR-dependent
    models and involve additional training costs.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.3.1\. Transformer in VRDU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Referring to the models introduced in Section [3](#S3 "3\. Mono-Task Document
    Understanding Frameworks ‣ Deep Learning based Visually Rich Document Content
    Understanding: A Survey") and Section [3](#S4.F3 "Figure 3 ‣ 4\. Multi-Task VRD
    Understanding Models ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey"), transformers have become extensively utilized in VRDU tasks, attaining
    state-of-the-art performance due to several key advantages. Firstly, the attention
    mechanism effectively captures long-range dependencies within the multimodal information,
    including text, vision, and layout. Furthermore, the inherent scalability of transformers
    improves self-supervised learning on large-scale datasets (e.g. IIT-CDIP (Lewis
    et al., [2006](#bib.bib60))), allowing them to handle diverse types and formats
    of documents, capturing more intricate and comprehensive features of the documents.
    Based on the transformer architecture used, VRDU models can be divided into two
    categories: encoder-only and encoder-decoder-based models. The first encoder-only
    model, LayoutLM (Xu et al., [2020a](#bib.bib136)), was inspired by BERT (Devlin,
    [2018](#bib.bib23)) and uses various pretraining tasks to allow the bidirectional
    transformer encoder to capture more textual and layout information. Following
    LayoutLM, more pretrained VRDU models with encoder-only, layout-aware (Wang et al.,
    [2022b](#bib.bib123); Hong et al., [2021](#bib.bib41); Chen et al., [2022](#bib.bib16);
    Wang et al., [2022b](#bib.bib123); Tu et al., [2023](#bib.bib119); Li et al.,
    [2021a](#bib.bib62)) or visual integrated (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137);
    Appalaraju et al., [2021](#bib.bib2); Huang et al., [2022](#bib.bib43)) have been
    proposed. These models are pretrained on various tasks to enhance their understanding
    of document structures. Encoder-only models demonstrate remarkable performance
    in sequence tagging and document classification tasks. However, they face challenges
    related to heavy annotation requirements and low readability, and they struggle
    with generative tasks such as abstractive question answering. Additionally, OCR
    errors can complicate the extraction of accurate information from input text.
    Furthermore, the fixed maximum input length of encoder-only frameworks limits
    their ability to handle long document inputs effectively. To overcome the limitations
    of encoder-only frameworks in generative QA and KIE, several encoder-decoder models
    (Powalski et al., [2021](#bib.bib96); Tang et al., [2022](#bib.bib116); Appalaraju
    et al., [2024](#bib.bib3); Mao et al., [2024](#bib.bib83)) have been developed,
    but they still depend on costly OCR tools which can introduce errors affecting
    performance. OCR-free frameworks (Kim et al., [2022](#bib.bib52); Davis et al.,
    [2022](#bib.bib21); Cao et al., [2023a](#bib.bib11); Yu et al., [2022](#bib.bib143))
    address this issue by using vision encoders and text decoders for end-to-end processing.
    For long documents, T5-based encoder-decoder models (Raffel et al., [2020](#bib.bib98))
    have been proposed to effectively handle multipage contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2\. CNNs in VRDU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In VRDU, CNNs are employed as the core framework for extracting feature maps
    from document images (Yang et al., [2017](#bib.bib139); Palm et al., [2019](#bib.bib92))
    and character grids (Katti et al., [2018](#bib.bib51)), benefiting from their
    strong local feature extraction capabilities. Joint-learning frameworks (Wang
    et al., [2021a](#bib.bib124), [a](#bib.bib124)) use CNNs as a backbone to integrate
    OCR and KIE tasks, combining visual and textual information through auxiliary
    tasks. Some OCR-free pretrained frameworks (Davis et al., [2022](#bib.bib21);
    Yu et al., [2022](#bib.bib143)) also utilize CNNs as vision encoders to extract
    visual feature maps. However, CNNs struggle to capture long-range dependencies
    due to their localized receptive fields. To address this, StrucTexTv2 (Yu et al.,
    [2022](#bib.bib143)) combines CNN-extracted feature maps with a transformer to
    capture global contextual information. Additionally, CNNs are commonly used to
    extract visual features from regions of interest (RoI) with RoI Align(Xu et al.,
    [2020a](#bib.bib136), [b](#bib.bib138), [2021](#bib.bib137); Huang et al., [2022](#bib.bib43);
    Tu et al., [2023](#bib.bib119); Wang et al., [2022b](#bib.bib123); Bai et al.,
    [2022](#bib.bib4); Liao et al., [2023](#bib.bib69)). Although CNNs can accurately
    capture region-specific visual cues and leverage pre-trained knowledge from general
    domains, they require extra processing to obtain RoI bounding boxes, unlike vision
    transformers (Huang et al., [2022](#bib.bib43); Tang et al., [2022](#bib.bib116);
    Appalaraju et al., [2024](#bib.bib3); Mao et al., [2024](#bib.bib83)), which operate
    directly on patch pixel values.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3\. Graphs in VRDU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VRDs feature complex spatial and logical structures. The spatial structure shows
    the layout and positional relationships, such as a Title above a Paragraph and
    a Caption near a Figure or Table. The logical structure denotes semantic and hierarchical
    connections, like a Title being the parent of a Paragraph and a Caption describing
    a related Table or Figure. Graph-based frameworks explicitly encode these relationships
    using node and edge representations; therefore, Graph Neural Networks (GNNs) are
    widely used in VRDU models (Lee et al., [2022b](#bib.bib57); Shi et al., [2023](#bib.bib107);
    Luo et al., [2022](#bib.bib80); Zhang et al., [2021](#bib.bib146)) to encode the
    spatial and logical representations. Although GNNs effectively capture domain-specific
    knowledge, they struggle with scalability and general domain knowledge pretraining.
    To address this, some frameworks (Li et al., [2023](#bib.bib67); Zhang et al.,
    [2022](#bib.bib147)) use attention masks or biases to mimic relationships between
    document elements, blending attention mechanisms with explicit relational modelling.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4\. Pretraining Mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By performing various pretraining tasks, a model can enhance its generalization
    ability through extensive datasets and prior training. Inspired by advances in
    pre-training language (Devlin, [2018](#bib.bib23); Brown et al., [2020](#bib.bib10))
    and vision models (He et al., [2016](#bib.bib40); Dosovitskiy et al., [2020](#bib.bib29)),
    numerous pretraining tasks for VRDU have been developed that are typically trained
    in large-scale document collections. This section will summarize the pretraining
    techniques and datasets commonly used for VRDU pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1\. Pretraining Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on the purpose and pretraining targets, the pretraining methods can be
    categorised into Masked Information Modeling, Cross-modality Learning, Mono-modality
    Augmentation, and Contrastive Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Masked Information Modelling (MIM) is first introduced by Masked Language Modelling
    in BERT (Devlin, [2018](#bib.bib23)), which randomly masks 15% workpiece tokens
    and requires the model to predict the masked tokens. Some models use multi-source
    inputs, such as XDoc (Chen et al., [2022](#bib.bib16)) and MarkupLM (Li et al.,
    [2022a](#bib.bib63)), which directly adopts Masked Language Modeling as a pretraining
    task on plain text or markdown text subsets. Some methods optimise MLM by changing
    masking wordpieces to whole words (Whole Word Masking by LayoutMask (Tu et al.,
    [2023](#bib.bib119))) or all tokens belong to one randomly generated text block,
    named Area-Masked Language Modelling (Hong et al., [2021](#bib.bib41)). Similar
    strategies can also be applied to mask entity-level textual representation, e.g.
    Masked Sentence Modelling (Gu et al., [2021](#bib.bib33)). Those language-targeted
    masked information modelling to improve the language understanding ability of
    VRDU models. Except for language-focused masking strategies, vision (Gu et al.,
    [2021](#bib.bib33); Yu et al., [2022](#bib.bib143)) and layout-focused (Tu et al.,
    [2023](#bib.bib119); Wang et al., [2022b](#bib.bib123)) strategies are also adopted.
    Moreover, masked information modelling is an effective method to boost cross-modality
    understanding. LayoutLM (Xu et al., [2020a](#bib.bib136)) introduces a Masked
    Visual-Language Modelling which allows using kept visual/layout information and
    contextual text content to predict the masked word-pieces, adopted by many VRDU
    pretrained models (Wang et al., [2022b](#bib.bib123); Xu et al., [2020b](#bib.bib138);
    Li et al., [2021a](#bib.bib62); Huang et al., [2022](#bib.bib43); Zhai et al.,
    [2023](#bib.bib144); Appalaraju et al., [2021](#bib.bib2)). Similarly, some visual
    token masked models leverage multimodal information to reconstruct the view tokens,
    e.g. Learn to Reconstruct (Appalaraju et al., [2021](#bib.bib2)), Masked Image
    Modeling (Huang et al., [2022](#bib.bib43)). Additionally, some models mask multimodal
    features simultaneously to conduct a cross-modality masking (Li et al., [2021b](#bib.bib66);
    Liao et al., [2023](#bib.bib69)). Masking Information Modelling may have limitations
    on bias in masking strategies, thus some frameworks (Tu et al., [2023](#bib.bib119))
    may try different masking ratios or strategies to improve the training effectiveness.
    Additionally, other common concerns about MIM also include training efficiency
    and lack of structural information. Thus, other pretraining methods are introduced
    to mitigate the limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-modality Aligning Although some Masked Information Modelling methods could
    effectively boost the cross-modality understanding, implicit contextual learning
    is limited to capturing explicit alignment between different modalities. Thus,
    few cross-modality aligning methods are introduced to enhance the modality interaction.
    To enhance the text-image interactive learning, Text-Image Alignment is adopted
    (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137)) which covers the image
    region of token lines to predict whether the image region of the target token
    line is covered or not. LayoutLMv3 (Huang et al., [2022](#bib.bib43)) expands
    from covering the image region only to image/text tokens to further enhance interactive
    learning. Vision-language alignment (Gu et al., [2021](#bib.bib33)) and Text-Image
    Matching (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137)) target to predict
    whether image-text features belong to the same region or not. DocFormer (Appalaraju
    et al., [2021](#bib.bib2)) tends to predict the text content of the paired images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Pretraining Techniques: Some pretraining tasks are introduced to further
    enhance the understanding of specific modalities. To enhance layout information
    understanding, StructuralLM (Li et al., [2021a](#bib.bib62)) and WOKONG-READER
    (Bai et al., [2022](#bib.bib4)) introduce Cell Position Classification and Textline
    Grid Matching to predict which located grids are of each cell or textline. Fast-StrucText
    (Zhai et al., [2023](#bib.bib144)) introduces a graph-based token relation method
    to predict a spatial correlation between token pairs. MarkupLM (Li et al., [2022a](#bib.bib63))
    leverage the benefits from markup files to predict the logical parent-child relation
    between nodes by introducing a Node Relation Prediction. Contrastive learning
    based strategies are adopted to conduct single (Gu et al., [2021](#bib.bib33))
    or cross-modality contrastive (Textline-Region Contrastive Learning) (Bai et al.,
    [2022](#bib.bib4)) learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2\. Pretraining Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To perform the aforementioned tasks, large-scale document collections are essential
    for conducting self-supervised learning. Different pretraining datasets are adopted
    by various models. The most widely used pretraining dataset is the IIT-CDIP Test
    Collection 1.0 (Lewis et al., [2006](#bib.bib60)), which contains more than 6
    million documents with over 11 million scanned document images. Since it contains
    a cross-domain and large number of unannotated documents, it is used by the majority
    of models (Li et al., [2022b](#bib.bib64); Hong et al., [2021](#bib.bib41); Kim
    et al., [2022](#bib.bib52); Xu et al., [2020a](#bib.bib136), [b](#bib.bib138);
    Huang et al., [2022](#bib.bib43); Xu et al., [2021](#bib.bib137); Bai et al.,
    [2022](#bib.bib4); Appalaraju et al., [2021](#bib.bib2); Liao et al., [2023](#bib.bib69);
    Wang et al., [2022b](#bib.bib123); Gu et al., [2021](#bib.bib33); Zhai et al.,
    [2023](#bib.bib144); Li et al., [2021a](#bib.bib62); Luo et al., [2023](#bib.bib78);
    Chen et al., [2022](#bib.bib16); Appalaraju et al., [2024](#bib.bib3)). As the
    original IIT-CDIP dataset provides the text content without layout information,
    off-the-shelf OCR tools are normally used to acquire the bounding box information
    of each document. Some models (Li et al., [2021c](#bib.bib68), [b](#bib.bib66);
    Wang et al., [2022a](#bib.bib127)) use relatively smaller pretraining datasets
    like RVL-CDIP (Harley et al., [2015](#bib.bib37)), which contains 400,000 evenly
    distributed documents in 16 types, to reduce the cost of pretraining. The multi-source
    model XDoc (Chen et al., [2022](#bib.bib16)) also leverages many plain text corpora
    for pretraining, such as BookCORPUS, CC-NEWS, OPENWEBTEXT, STORIES and HTML-sourced
    CommonCrawl datasets. To address multilingual scenarios, both LiLT (Wang et al.,
    [2022b](#bib.bib123)) and LayoutXLM (Xu et al., [2021](#bib.bib137)) follow the
    principles and policies of Common Crawl to gather large amounts of multilingual
    digitally-born PDF documents.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper comprehensively reviews deep learning-based models for visually rich
    document content understanding, encompassing both mono-task frameworks designed
    for specific VRDU downstream tasks and multi-task frameworks that support multiple
    VRDU downstream tasks. Beyond introducing the novelties of each model, the limitations
    of these frameworks are summarized at the end of each section, offering a thorough
    trend analysis. Additionally, this paper summarizes existing VRD content understanding
    datasets, pointing out future trends and demands for VRDU. To provide a systematic
    review, we critically discuss various techniques, highlighting their strengths
    and limitations. We believe that this survey offers a comprehensive overview of
    the development of visually rich document content understanding, catering to the
    needs of both the academic and industrial sectors.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appalaraju et al. (2021) Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,
    Yusheng Xie, and R Manmatha. 2021. Docformer: End-to-end transformer for document
    understanding. In *Proceedings of the IEEE/CVF international conference on computer
    vision*. 993–1003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Appalaraju et al. (2024) Srikar Appalaraju, Peng Tang, Qi Dong, Nishant Sankaran,
    Yichu Zhou, and R Manmatha. 2024. Docformerv2: Local features for document understanding.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 38\.
    709–718.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2022) Haoli Bai, Zhiguang Liu, Xiaojun Meng, Wentao Li, Shuang
    Liu, Nian Xie, Rongfu Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei, et al. 2022.
    Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding.
    *arXiv preprint arXiv:2212.09621* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. (2021) Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. 2021. BEiT:
    BERT Pre-Training of Image Transformers. In *International Conference on Learning
    Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biten et al. (2022) Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju,
    and R Manmatha. 2022. Latr: Layout-aware transformer for scene-text vqa. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*. 16548–16558.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biten et al. (2019a) Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,
    Marçal Rusinol, Minesh Mathew, CV Jawahar, Ernest Valveny, and Dimosthenis Karatzas.
    2019a. Icdar 2019 competition on scene text visual question answering. In *2019
    International Conference on Document Analysis and Recognition (ICDAR)*. IEEE,
    1563–1570.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biten et al. (2019b) Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,
    Marçal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. 2019b. Scene
    text visual question answering. In *Proceedings of the IEEE/CVF international
    conference on computer vision*. 4291–4301.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blau et al. (2024) Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz,
    Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, and Ron Litman. 2024. GRAM: Global
    Reasoning for Multi-Page VQA. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 15598–15607.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2023a) Haoyu Cao, Changcun Bao, Chaohu Liu, Huang Chen, Kun Yin,
    Hao Liu, Yinsong Liu, Deqiang Jiang, and Xing Sun. 2023a. Attention Where It Matters:
    Rethinking Visual Document Understanding with Selective Region Concentration.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*.
    19517–19527.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2023b) Panfeng Cao, Ye Wang, Qiang Zhang, and Zaiqiao Meng. 2023b.
    GenKIE: Robust Generative Multimodal Document Key Information Extraction. In *Findings
    of the Association for Computational Linguistics: EMNLP 2023*. 14702–14713.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carbonell et al. (2021) Manuel Carbonell, Pau Riba, Mauricio Villegas, Alicia
    Fornés, and Josep Lladós. 2021. Named entity recognition and relation extraction
    with graph neural networks in semi structured documents. In *2020 25th International
    Conference on Pattern Recognition (ICPR)*. IEEE, 9622–9627.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
    Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection
    with transformers. In *European conference on computer vision*. Springer, 213–229.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Jiayi Chen, Hanjun Dai, Bo Dai, Aidong Zhang, and Wei Wei.
    2023. On Task-personalized Multimodal Few-shot Learning for Visually-rich Document
    Entity Retrieval. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*. 9006–9025.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei.
    2022. XDoc: Unified Pre-training for Cross-Format Document Understanding. *arXiv
    preprint arXiv:2210.02849* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Xingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang Zhang,
    Ao Luo, Yuxuan Xiong, and Kai Yu. 2021. WebSRC: A Dataset for Web-Based Structural
    Reading Comprehension. In *Proceedings of the 2021 Conference on Empirical Methods
    in Natural Language Processing*. 4173–4185.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2020) Mengli Cheng, Minghui Qiu, Xing Shi, Jun Huang, and Wei
    Lin. 2020. One-shot text field labeling using attention and belief propagation
    for structure information extraction. In *Proceedings of the 28th ACM International
    Conference on Multimedia*. 340–348.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu et al. (2022) Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua
    Shen. 2022. Conditional Positional Encodings for Vision Transformers. In *The
    Eleventh International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cui et al. (2021) Lei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. 2021. Document
    ai: Benchmarks, models and applications. *arXiv preprint arXiv:2111.08609* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davis et al. (2022) Brian Davis, Bryan Morse, Brian Price, Chris Tensmeyer,
    Curtis Wigington, and Vlad Morariu. 2022. End-to-end document recognition and
    understanding with dessurt. In *European Conference on Computer Vision*. Springer,
    280–296.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denk and Reisswig (2019) Timo I Denk and Christian Reisswig. 2019. Bertgrid:
    Contextualized embedding for 2d document representation and understanding. *arXiv
    preprint arXiv:1909.04948* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin (2018) J Devlin. 2018. BERT: Pre-training of deep bidirectional transformers
    for language understanding.. In *Proceedings of NAACL-HLT*, Vol. 2019\. 4171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2022) Yihao Ding, Zhe Huang, Runlin Wang, YanHang Zhang, Xianru
    Chen, Yuzhong Ma, Hyunsuk Chung, and Soyeon Caren Han. 2022. V-Doc: Visual questions
    answers with Documents. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 21492–21498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2023a) Yihao Ding, Siqu Long, Jiabin Huang, Kaixuan Ren, Xingxiang
    Luo, Hyunsuk Chung, and Soyeon Caren Han. 2023a. Form-NLU: Dataset for the Form
    Natural Language Understanding. In *Proceedings of the 46th International ACM
    SIGIR Conference on Research and Development in Information Retrieval*. 2807–2816.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2023b) Yihao Ding, Siwen Luo, Hyunsuk Chung, and Soyeon Caren
    Han. 2023b. VQA: A New Dataset for Real-World VQA on PDF Documents. In *Joint
    European Conference on Machine Learning and Knowledge Discovery in Databases*.
    Springer, 585–601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2024a) Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and Soyeon Caren
    Han. 2024a. MVQA: A Dataset for Multimodal Information Retrieval in PDF-based
    Visual Question Answering. *arXiv preprint arXiv:2404.12720* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2024b) Yihao Ding, Lorenzo Vaiani, Caren Han, Jean Lee, Paolo
    Garza, Josiah Poon, and Luca Cagliero. 2024b. M3-VRD: Multimodal Multi-task Multi-teacher
    Visually-Rich Form Document Understanding. *arXiv preprint arXiv:2402.17983* (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An Image is Worth 16x16 Words:
    Transformers for Image Recognition at Scale. In *International Conference on Learning
    Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022) Qinyi Du, Qingqing Wang, Keqian Li, Jidong Tian, Liqiang Xiao,
    and Yaohui Jin. 2022. CALM: commen-sense knowledge augmentation for document image
    understanding. In *Proceedings of the 30th ACM International Conference on Multimedia*.
    3282–3290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ehrmann et al. (2023) Maud Ehrmann, Ahmed Hamdi, Elvys Linhares Pontes, Matteo
    Romanello, and Antoine Doucet. 2023. Named entity recognition and classification
    in historical documents: A survey. *Comput. Surveys* 56, 2 (2023), 1–47.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geifman and El-Yaniv (2017) Yonatan Geifman and Ran El-Yaniv. 2017. Selective
    classification for deep neural networks. *Advances in neural information processing
    systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2021) Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv
    Jain, Nikolaos Barmpalios, Ani Nenkova, and Tong Sun. 2021. Unidoc: Unified pretraining
    framework for document understanding. *Advances in Neural Information Processing
    Systems* 34 (2021), 39–50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2022) Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang,
    Ming Gu, and Liqing Zhang. 2022. Xylayoutlm: Towards layout-aware multimodal networks
    for visually-rich document understanding. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 4583–4592.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
    2017. On calibration of modern neural networks. In *International conference on
    machine learning*. PMLR, 1321–1330.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    conference on machine learning*. PMLR, 3929–3938.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harley et al. (2015) Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis.
    2015. Evaluation of deep convolutional nets for document image classification
    and retrieval. In *2015 13th International Conference on Document Analysis and
    Recognition (ICDAR)*. IEEE, 991–995.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2023) Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, and
    Heng Tao Shen. 2023. ICL-D3IE: In-context learning with diverse demonstrations
    updating for document information extraction. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*. 19485–19494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    2017. Mask r-cnn. In *Proceedings of the IEEE international conference on computer
    vision*. 2961–2969.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. (2021) Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun
    Nam, and Sungrae Park. 2021. BROS: A pre-trained language model for understanding
    texts in document. (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2023) Kai Hu, Zhuoyuan Wu, Zhuoyao Zhong, Weihong Lin, Lei Sun, and
    Qiang Huo. 2023. A question-answering approach to key value pair extraction from
    form-like document images. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 37\. 12899–12906.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2022) Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu
    Wei. 2022. Layoutlmv3: Pre-training for document ai with unified text and image
    masking. In *Proceedings of the 30th ACM International Conference on Multimedia*.
    4083–4091.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2019) Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis
    Karatzas, Shijian Lu, and CV Jawahar. 2019. Icdar2019 competition on scanned receipt
    ocr and information extraction. In *2019 International Conference on Document
    Analysis and Recognition (ICDAR)*. IEEE, 1516–1520.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hwang et al. (2021) Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang,
    and Minjoon Seo. 2021. Spatial Dependency Parsing for Semi-Structured Document
    Information Extraction. In *Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021*. 330–343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaeger et al. (2022) Paul F Jaeger, Carsten Tim Lüth, Lukas Klein, and Till J
    Bungert. 2022. A Call to Reflect on Evaluation Practices for Failure Detection
    in Image Classification. In *The Eleventh International Conference on Learning
    Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaume et al. (2019) Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe
    Thiran. 2019. Funsd: A dataset for form understanding in noisy scanned documents.
    In *2019 International Conference on Document Analysis and Recognition Workshops
    (ICDARW)*, Vol. 2\. IEEE, 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jegou et al. (2010) Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010.
    Product quantization for nearest neighbor search. *IEEE transactions on pattern
    analysis and machine intelligence* 33, 1 (2010), 117–128.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke
    Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing
    and predicting spans. *Transactions of the association for computational linguistics*
    8 (2020), 64–77.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2024) Lei Kang, Rubèn Tito, Ernest Valveny, and Dimosthenis Karatzas.
    2024. Multi-Page Document Visual Question Answering using Self-Attention Scoring
    Mechanism. *arXiv preprint arXiv:2404.19024* (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katti et al. (2018) Anoop R Katti, Christian Reisswig, Cordula Guder, Sebastian
    Brarda, Steffen Bickel, Johannes Höhne, and Jean Baptiste Faddoul. 2018. Chargrid:
    Towards Understanding 2D Documents. In *Proceedings of the 2018 Conference on
    Empirical Methods in Natural Language Processing*. 4459–4469.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2022) Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung
    Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.
    2022. Ocr-free document understanding transformer. In *Computer Vision–ECCV 2022:
    17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXVIII*. Springer, 498–517.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021) Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language
    transformer without convolution or region supervision. In *International conference
    on machine learning*. PMLR, 5583–5594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim (2014) Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, Alessandro Moschitti, Bo Pang, and Walter Daelemans (Eds.).
    Association for Computational Linguistics, Doha, Qatar, 1746–1751. [https://doi.org/10.3115/v1/D14-1181](https://doi.org/10.3115/v1/D14-1181)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lample et al. (2016) Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian,
    Kazuya Kawakami, and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition.
    In *Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*. 260–270.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised
    learning of language representations. *arXiv preprint arXiv:1909.11942* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2022b) Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot,
    Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, and Tomas Pfister.
    2022b. FormNet: Structural Encoding beyond Sequential Modeling in Form Document
    Information Extraction. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*. 3735–3754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2023) Chen-Yu Lee, Chun-Liang Li, Hao Zhang, Timothy Dozat, Vincent
    Perot, Guolong Su, Xiang Zhang, Kihyuk Sohn, Nikolai Glushnev, Renshen Wang, et al.
    2023. FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information
    Extraction. *arXiv preprint arXiv:2305.02549* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2022a) Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu
    Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina
    Toutanova. 2022a. Pix2Struct: Screenshot parsing as pretraining for visual language
    understanding. *arXiv preprint arXiv:2210.03347* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2006) David Lewis, Gady Agam, Shlomo Argamon, Ophir Frieder, David
    Grossman, and Jefferson Heard. 2006. Building a test collection for complex document
    information processing. In *Proceedings of the 29th annual international ACM SIGIR
    conference on Research and development in information retrieval*. 665–666.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
    BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and Comprehension. In *Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics*. 7871–7880.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021a) Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang,
    Fei Huang, and Luo Si. 2021a. StructuralLM: Structural Pre-training for Form Understanding.
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*. 6309–6318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022a) Junlong Li, Yiheng Xu, Lei Cui, and Furu Wei. 2022a. MarkupLM:
    Pre-training of Text and Markup Language for Visually Rich Document Understanding.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*. 6078–6087.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and
    Furu Wei. 2022b. Dit: Self-supervised pre-training for document image transformer.
    In *Proceedings of the 30th ACM International Conference on Multimedia*. 3530–3539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and
    Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and
    language. *arXiv preprint arXiv:1908.03557* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021b) Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong
    Zhao, Rajiv Jain, Varun Manjunatha, and Hongfu Liu. 2021b. Selfdoc: Self-supervised
    document representation learning. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*. 5652–5660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Qiwei Li, Zuchao Li, Xiantao Cai, Bo Du, and Hai Zhao. 2023.
    Enhancing Visually-Rich Document Understanding via Layout Structure Modeling.
    In *Proceedings of the 31st ACM International Conference on Multimedia*. 4513–4523.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021c) Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang,
    Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, and Errui Ding. 2021c. Structext: Structured
    text understanding with multi-modal transformers. In *Proceedings of the 29th
    ACM International Conference on Multimedia*. 1912–1920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2023) Haofu Liao, Aruni RoyChowdhury, Weijian Li, Ankan Bansal,
    Yuting Zhang, Zhuowen Tu, Ravi Kumar Satzoda, R Manmatha, and Vijay Mahadevan.
    2023. Doctr: Document transformer for structured information extraction in documents.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*.
    19584–19594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2017) Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath
    Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    2117–2125.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024b) Chaohu Liu, Kun Yin, Haoyu Cao, Xinghua Jiang, Xin Li, Yinsong
    Liu, Deqiang Jiang, Xing Sun, and Linli Xu. 2024b. Hrvda: High-resolution visual
    document assistant. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 15534–15545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2024a. Visual instruction tuning. *Advances in neural information processing systems*
    36 (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Jixiong Liu, Yoan Chabot, Raphaël Troncy, Viet-Phi Huynh,
    Thomas Labbé, and Pierre Monnin. 2023. From tabular data to knowledge graphs:
    A survey of semantic table interpretation tasks and methods. *Journal of Web Semantics*
    76 (2023), 100761.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao. 2019a.
    Graph Convolution for Multimodal Information Extraction from Visually Rich Documents.
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 2 (Industry
    Papers)*. 32–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,
    Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer
    using shifted windows. In *Proceedings of the IEEE/CVF international conference
    on computer vision*. 10012–10022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lombardi and Marinai (2020) Francesco Lombardi and Simone Marinai. 2020. Deep
    learning for historical document analysis and recognition—a survey. *Journal of
    Imaging* 6, 10 (2020), 110.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023) Chuwei Luo, Changxu Cheng, Qi Zheng, and Cong Yao. 2023.
    Geolayoutlm: Geometric pre-training for visual information extraction. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 7092–7101.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2024) Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and
    Cong Yao. 2024. LayoutLLM: Layout Instruction Tuning with Large Language Models
    for Document Understanding. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 15630–15640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2022) Siwen Luo, Yihao Ding, Siqu Long, Josiah Poon, and Soyeon Caren
    Han. 2022. Doc-GCN: Heterogeneous Graph Convolutional Networks for Document Layout
    Analysis. In *Proceedings of the 29th International Conference on Computational
    Linguistics*. 2906–2916.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Majumder et al. (2020a) Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep
    Tata, James Bradley Wendt, Qi Zhao, and Marc Najork. 2020a. Representation learning
    for information extraction from form-like documents. In *proceedings of the 58th
    annual meeting of the Association for Computational Linguistics*. 6495–6504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Majumder et al. (2020b) Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep
    Tata, James Bradley Wendt, Qi Zhao, and Marc Najork. 2020b. Representation learning
    for information extraction from form-like documents. In *proceedings of the 58th
    annual meeting of the Association for Computational Linguistics*. 6495–6504.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. (2024) Zhiming Mao, Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang,
    Qun Liu, and Kam-Fai Wong. 2024. Visually Guided Generative Text-Layout Pre-training
    for Document Intelligence. In *Proceedings of the 2024 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies (Volume 1: Long Papers)*. 4713–4730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathew et al. (2021) Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021.
    Docvqa: A dataset for vqa on document images. In *Proceedings of the IEEE/CVF
    winter conference on applications of computer vision*. 2200–2209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013a) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013a. Efficient estimation of word representations in vector space. *arXiv preprint
    arXiv:1301.3781* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013b) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013b. Distributed representations of words and phrases and their
    compositionality. *Advances in neural information processing systems* 26 (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagy and Seth (1984) George Nagy and Sharad C Seth. 1984. Hierarchical representation
    of optically scanned documents. (1984).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: O’Gorman (1993) Lawrence O’Gorman. 1993. The document spectrum for page layout
    analysis. *IEEE Transactions on pattern analysis and machine intelligence* 15,
    11 (1993), 1162–1173.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oliveira and Viana (2017) Dario Augusto Borges Oliveira and Matheus Palhares
    Viana. 2017. Fast CNN-based document layout analysis. In *2017 IEEE International
    Conference on Computer Vision Workshops (ICCVW)*. IEEE, 1173–1180.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2023) OpenAI. 2023. ChatGPT: A conversational agent. [https://www.openai.com/chatgpt](https://www.openai.com/chatgpt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oreshkin et al. (2018) Boris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste.
    2018. Tadam: Task dependent adaptive metric for improved few-shot learning. *Advances
    in neural information processing systems* 31 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Palm et al. (2019) Rasmus Berg Palm, Florian Laws, and Ole Winther. 2019. Attend,
    copy, parse end-to-end information extraction from documents. In *2019 International
    Conference on Document Analysis and Recognition (ICDAR)*. IEEE, 329–336.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2019) Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung
    Surh, Minjoon Seo, and Hwalsuk Lee. 2019. CORD: a consolidated receipt dataset
    for post-OCR parsing. In *Workshop on Document Intelligence at NeurIPS 2019*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D
    Manning. 2014. Glove: Global vectors for word representation. In *Proceedings
    of the 2014 conference on empirical methods in natural language processing (EMNLP)*.
    1532–1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perot et al. (2023) Vincent Perot, Kai Kang, Florian Luisier, Guolong Su, Xiaoyu
    Sun, Ramya Sree Boppana, Zilong Wang, Jiaqi Mu, Hao Zhang, and Nan Hua. 2023.
    LMDX: Language Model-based Document Information Extraction and Localization. *arXiv
    preprint arXiv:2309.10952* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Powalski et al. (2021) Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz,
    Tomasz Dwojak, Michał Pietruszka, and Gabriela Pałka. 2021. Going full-tilt boogie
    on document understanding with text-image-layout transformer. In *Document Analysis
    and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland,
    September 5–10, 2021, Proceedings, Part II 16*. Springer, 732–747.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Press et al. (2021) Ofir Press, Noah Smith, and Mike Lewis. 2021. Train Short,
    Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In
    *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research* 21, 140 (2020), 1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016a) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
    and Percy Liang. 2016a. SQuAD: 100,000+ Questions for Machine Comprehension of
    Text. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*. 2383–2392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016b) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
    and Percy Liang. 2016b. SQuAD: 100,000+ Questions for Machine Comprehension of
    Text. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*. 2383–2392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image
    generation. In *International conference on machine learning*. Pmlr, 8821–8831.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*. 3982–3992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster r-cnn: Towards real-time object detection with region proposal networks.
    *Advances in neural information processing systems* 28 (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rusinol et al. (2013) Marçal Rusinol, Tayeb Benkhelfallah, and Vincent Poulain dAndecy.
    2013. Field extraction from administrative documents by incremental structural
    templates. In *2013 12th International Conference on Document Analysis and Recognition*.
    IEEE, 1100–1104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saout et al. (2024) Thomas Saout, Frédéric Lardeux, and Frédéric Saubion. 2024.
    An Overview of Data Extraction From Invoices. *IEEE Access* (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seki et al. (2007) Minenobu Seki, Masakazu Fujio, Takeshi Nagasaki, Hiroshi
    Shinjo, and Katsumi Marukawa. 2007. Information management system using structure
    analysis of paper/electronic documents and its applications. In *Ninth International
    Conference on Document Analysis and Recognition (ICDAR 2007)*, Vol. 2\. IEEE,
    689–693.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023) Dengliang Shi, Siliang Liu, Jintao Du, and Huijia Zhu. 2023.
    Layoutgcn: A lightweight architecture for visually rich document understanding.
    In *International Conference on Document Analysis and Recognition*. Springer,
    149–165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Šimsa et al. (2023) Štěpán Šimsa, Milan Šulc, Michal Uřičář, Yash Patel, Ahmed
    Hamdi, Matěj Kocián, Matyáš Skalickỳ, Jiří Matas, Antoine Doucet, Mickaël Coustaty,
    et al. 2023. Docile benchmark for document information localization and extraction.
    In *International Conference on Document Analysis and Recognition*. Springer,
    147–166.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snell et al. (2017) Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical
    networks for few-shot learning. *Advances in neural information processing systems*
    30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn (2016) Kihyuk Sohn. 2016. Improved deep metric learning with multi-class
    n-pair loss objective. *Advances in neural information processing systems* 29
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Speer et al. (2017) Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet
    5.5: An open multilingual graph of general knowledge. In *Proceedings of the AAAI
    conference on artificial intelligence*, Vol. 31.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stanisławek et al. (2021) Tomasz Stanisławek, Filip Graliński, Anna Wróblewska,
    Dawid Lipiński, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and Przemysław
    Biecek. 2021. Kleister: key information extraction datasets involving long documents
    with complex layouts. In *Document Analysis and Recognition–ICDAR 2021: 16th International
    Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part I*.
    Springer, 564–579.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subramani et al. (2020) Nishant Subramani, Alexandre Matton, Malcolm Greaves,
    and Adrian Lam. 2020. A survey of deep learning approaches for ocr and document
    understanding. *arXiv preprint arXiv:2011.13534* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Bansal (2019) Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality
    Encoder Representations from Transformers. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*. 5100–5111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tanaka et al. (2021) Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021.
    Visualmrc: Machine reading comprehension on document images. In *Proceedings of
    the AAAI Conference on Artificial Intelligence*, Vol. 35\. 13878–13888.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2022) Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu,
    Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. 2022. Unifying Vision,
    Text, and Layout for Universal Document Processing. *arXiv preprint arXiv:2212.02623*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tito et al. (2023) Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2023.
    Hierarchical multimodal transformers for Multipage DocVQA. *Pattern Recognition*
    144 (2023), 109834.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. (2023) Yi Tu, Ya Guo, Huan Chen, and Jinyang Tang. 2023. LayoutMask:
    Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding.
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*. 15200–15212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Landeghem et al. (2023) Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann,
    Michał Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty,
    Bertrand Anckaert, Ernest Valveny, et al. 2023. Document understanding dataset
    and evaluation (DUDE). In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 19528–19540.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis
    Platform for Natural Language Understanding. In *International Conference on Learning
    Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang
    Chen, Jun-Wei Hsieh, and I-Hau Yeh. 2020a. CSPNet: A new backbone that can enhance
    learning capability of CNN. In *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition workshops*. 390–391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) Jiapeng Wang, Lianwen Jin, and Kai Ding. 2022b. LiLT: A
    Simple yet Effective Language-Independent Layout Transformer for Structured Document
    Understanding. In *Proceedings of the 60th Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*. 7747–7757.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin
    Zhang, Shuaitao Zhang, Qianying Wang, Yaqiang Wu, and Mingxiang Cai. 2021a. Towards
    robust visual information extraction in real world: new dataset and novel solution.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 35\.
    2738–2745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022c) Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang
    Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022c. Ofa: Unifying
    architectures, tasks, and modalities through a simple sequence-to-sequence learning
    framework. In *International Conference on Machine Learning*. PMLR, 23318–23340.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh,
    and Louis-Philippe Morency. 2019. Words can shift: Dynamically adjusting word
    representations using nonverbal behaviors. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 33\. 7216–7223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) Zilong Wang, Jiuxiang Gu, Chris Tensmeyer, Nikolaos Barmpalios,
    Ani Nenkova, Tong Sun, Jingbo Shang, and Vlad Morariu. 2022a. MGDoc: Pre-training
    with Multi-granular Hierarchy for Document Image Understanding. In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*. 3984–3993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Shang (2022) Zilong Wang and Jingbo Shang. 2022. Towards Few-shot
    Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework.
    In *Findings of the Association for Computational Linguistics: ACL 2022*. 4174–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu
    Wei. 2021b. LayoutReader: Pre-training of Text and Layout for Reading Order Detection.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*. 4735–4744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Zilong Wang, Mingjie Zhan, Xuebo Liu, and Ding Liang. 2020b.
    Docstruct: A multimodal method to extract hierarchy structure in document for
    general form understanding. *arXiv preprint arXiv:2010.11685* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Zifeng Wang, Zizhao Zhang, Jacob Devlin, Chen-Yu Lee, Guolong
    Su, Hao Zhang, Jennifer Dy, Vincent Perot, and Tomas Pfister. 2023a. QueryForm:
    A Simple Zero-shot Form Entity Query Framework. In *Findings of the Association
    for Computational Linguistics: ACL 2023*. 4146–4159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Zilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, and Sandeep
    Tata. 2023b. Vrdu: A benchmark for visually-rich document understanding. In *Proceedings
    of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. 5184–5193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watanabe et al. (1995) Toyohide Watanabe, Qin Luo, and Noboru Sugie. 1995. Layout
    recognition of multi-kinds of table-form documents. *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* 17, 4 (1995), 432–445.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2020) Mengxi Wei, Yifan He, and Qiong Zhang. 2020. Robust layout-aware
    IE for visually rich documents with pre-trained language models. In *Proceedings
    of the 43rd International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 2367–2376.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022) Xinya Wu, Duo Zheng, Ruonan Wang, Jiashen Sun, Minzhen Hu,
    Fangxiang Feng, Xiaojie Wang, Huixing Jiang, and Fan Yang. 2022. A region-based
    document VQA. In *Proceedings of the 30th ACM International Conference on Multimedia*.
    4909–4920.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020a) Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei,
    and Ming Zhou. 2020a. Layoutlm: Pre-training of text and layout for document image
    understanding. In *Proceedings of the 26th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. 1192–1200.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei
    Florencio, Cha Zhang, and Furu Wei. 2021. Layoutxlm: Multimodal pre-training for
    multilingual visually-rich document understanding. *arXiv preprint arXiv:2104.08836*
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020b) Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin
    Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, et al. 2020b. Layoutlmv2:
    Multi-modal pre-training for visually-rich document understanding. *arXiv preprint
    arXiv:2012.14740* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2017) Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel
    Kifer, and C Lee Giles. 2017. Learning to extract semantic structure from documents
    using multimodal fully convolutional neural networks. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 5315–5324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2021) Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu,
    Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. FILIP: Fine-grained
    Interactive Language-Image Pre-Training. In *International Conference on Learning
    Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu and Koltun (2015) Fisher Yu and Vladlen Koltun. 2015. Multi-scale context
    aggregation by dilated convolutions. *arXiv preprint arXiv:1511.07122* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021) Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and Rong Xiao.
    2021. PICK: processing key information extraction from documents using improved
    graph learning-convolutional networks. In *2020 25th International Conference
    on Pattern Recognition (ICPR)*. IEEE, 4363–4370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2022) Yuechen Yu, Yulin Li, Chengquan Zhang, Xiaoqiang Zhang, Zengyuan
    Guo, Xiameng Qin, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang. 2022. StrucTexTv2:
    Masked Visual-Textual Prediction for Document Image Pre-training. In *The Eleventh
    International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. (2023) Mingliang Zhai, Yulin Li, Xiameng Qin, Chen Yi, Qunyi Xie,
    Chengquan Zhang, Kun Yao, Yuwei Wu, and Yunde Jia. 2023. Fast-StrucTexT: an efficient
    hourglass transformer with modality-guided dynamic token merge for document understanding.
    In *Proceedings of the Thirty-Second International Joint Conference on Artificial
    Intelligence*. 5269–5277.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing
    Lu, Liang Qiao, Yi Niu, and Fei Wu. 2020. TRIE: end-to-end text reading and information
    extraction for document understanding. In *Proceedings of the 28th ACM International
    Conference on Multimedia*. 1413–1422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Yue Zhang, Zhang Bo, Rui Wang, Junjie Cao, Chen Li, and
    Zuyi Bao. 2021. Entity Relation Extraction as Dependency Parsing in Visually Rich
    Documents. In *Proceedings of the 2021 Conference on Empirical Methods in Natural
    Language Processing*. 2759–2768.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Zhenrong Zhang, Jiefeng Ma, Jun Du, Licheng Wang, and Jianshu
    Zhang. 2022. Multimodal pre-training based on graph attention network for document
    understanding. *IEEE Transactions on Multimedia* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019) Xiaohui Zhao, Endi Niu, Zhuo Wu, and Xiaoguang Wang. 2019.
    Cutie: Learning to understand documents with convolutional universal text information
    extractor. *arXiv preprint arXiv:1903.12363* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2024. Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural
    Information Processing Systems* 36 (2024).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2019) Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019.
    Publaynet: largest dataset ever for document layout analysis. In *2019 International
    Conference on Document Analysis and Recognition (ICDAR)*. IEEE, 1015–1022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2022) Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang,
    and Tat-Seng Chua. 2022. Towards complex document understanding by discrete reasoning.
    In *Proceedings of the 30th ACM International Conference on Multimedia*. 4857–4866.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2021a) Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo
    Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021a. TAT-QA: A Question Answering
    Benchmark on a Hybrid of Tabular and Textual Content in Finance. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*. 3277–3287.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2021b) Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, and
    Qi Wu. 2021b. Mucko: multi-layer cross-modal knowledge reasoning for fact-based
    visual question answering. In *Proceedings of the Twenty-Ninth International Conference
    on International Joint Conferences on Artificial Intelligence*. 1097–1103.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
