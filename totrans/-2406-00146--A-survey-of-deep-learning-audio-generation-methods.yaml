- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:31:54'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2406.00146] A survey of deep learning audio generation methods'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.00146](https://ar5iv.labs.arxiv.org/html/2406.00146)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A survey of deep learning audio generation methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matej Božić1, Marko Horvat2 Department of Applied Computing
  prefs: []
  type: TYPE_NORMAL
- en: University of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb,
    Croatia
  prefs: []
  type: TYPE_NORMAL
- en: 1matej.bozic@fer.hr 2marko.horvat3@fer.hr
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This article presents a review of typical techniques used in three distinct
    aspects of deep learning model development for audio generation. In the first
    part of the article, we provide an explanation of audio representations, beginning
    with the fundamental audio waveform. We then progress to the frequency domain,
    with an emphasis on the attributes of human hearing, and finally introduce a relatively
    recent development. The main part of the article focuses on explaining basic and
    extended deep learning architecture variants, along with their practical applications
    in the field of audio generation. The following architectures are addressed: 1)
    Autoencoders 2) Generative adversarial networks 3) Normalizing flows 4) Transformer
    networks 5) Diffusion models. Lastly, we will examine four distinct evaluation
    metrics that are commonly employed in audio generation. This article aims to offer
    novice readers and beginners in the field a comprehensive understanding of the
    current state of the art in audio generation methods as well as relevant studies
    that can be explored for future research.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, Audio representations, Audio generation, Generative models, Sound
    synthesis
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The trend towards deep learning in Computer Vision (CV) and Natural Language
    Processing (NLP) has also reached the field of audio generation [[1](#bib.bibx1)].
    Deep learning has allowed us to move away from the complexity of hand-crafted
    features towards simple representations by letting the depth of the model create
    more complex mappings. We define audio generation as any method whose outputs
    are audio and cannot be derived solely from the inputs. Even though tasks such
    as text-to-speech involve translation from the text domain to the speech domain,
    there are many unknowns, such as the speaker’s voice. This means that the models
    have to invent or generate information for the translation to work. There are
    many applications for audio generation. We can create human-sounding voice assistants,
    generate ambient sounds for games or movies based on the current visual input,
    create various music samples to help music producers with ideas or composition,
    and much more. The structure of the presented survey on deep learning audio generation
    methods is illustrated in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A survey
    of deep learning audio generation methods").
  prefs: []
  type: TYPE_NORMAL
- en: This article will mainly focus on deep learning methods, as the field seems
    to be developing in this direction. Nevertheless, section [III](#S3 "III Background
    ‣ A survey of deep learning audio generation methods") will examine the development
    of audio generation methods over the years, starting around the 1970s. We consider
    this section important because, just as deep learning methods have re-emerged,
    there may be a time when audio generation methods that are now obsolete become
    state-of-the-art again. The goal is to take a broad but shallow look at the field
    of audio generation. Some areas, such as text-to-speech, will be more heavily
    represented as they have received more attention, but an attempt has been made
    to include many different subfields. This article does not attempt to present
    all possible methods but only introduces the reader to some of the popular methods
    in the field. Each listing of works on a topic is sorted so that the most recent
    articles are at the end.
  prefs: []
  type: TYPE_NORMAL
- en: 'The article is structured as follows: section [II](#S2 "II Related work ‣ A
    survey of deep learning audio generation methods") presents previous work dealing
    with deep learning in audio, section [III](#S3 "III Background ‣ A survey of deep
    learning audio generation methods") gives a brief overview of previous audio generation
    methods in text-to-speech and music generation, section [IV](#S4 "IV Audio features
    ‣ A survey of deep learning audio generation methods") deals with the two most
    prominent features and a recent advancement, section [V](#S5 "V Architectures
    ‣ A survey of deep learning audio generation methods") discusses five deep learning
    architectures and some of their popular extensions, and finally, section [VI](#S6
    "VI Evaluation metrics ‣ A survey of deep learning audio generation methods")
    looks at measuring the performance of generation models, some of which are specific
    to audio generation, while others are more generally applicable.'
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: for tree= forked edges, grow’=0, draw, rounded corners , [Survey, rotate=90
    [[IV](#S4 "IV Audio features ‣ A survey of deep learning audio generation methods").
    Audio features, for tree=fill=red!45, section [Raw waveform, fill=red!30, subsection]
    [Mel-spectrogram, fill=red!30, subsection] [Neural codecs, fill=red!30, subsection]
    ] [[V](#S5 "V Architectures ‣ A survey of deep learning audio generation methods").
    Architectures, for tree=fill=brown!45, section [Auto-encoder, fill=brown!30, subsection]
    [Generative adversarial networks, fill=brown!30, subsection] [Normalizing flows,
    fill=brown!30, subsection] [Transformer networks, fill=brown!30, subsection] [Diffusion
    models, fill=brown!30, subsection] ] [[VI](#S6 "VI Evaluation metrics ‣ A survey
    of deep learning audio generation methods"). Evaluation metrics, for tree=fill=orange!45,
    section [Human evaluation, fill=orange!30, subsection] [Inception score, fill=orange!30,
    subsection] [Fréchet distance, fill=orange!30, subsection] [Kullback-Leibler divergence,
    fill=orange!30, subsection] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: The main sections of the survey.'
  prefs: []
  type: TYPE_NORMAL
- en: II Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will mention some of the works that are good sources for
    further research in the field of audio generation. Some of them investigate only
    a specific model architecture or sub-area, while others, like this work, show
    a broader view.
  prefs: []
  type: TYPE_NORMAL
- en: In [[2](#bib.bibx2)], deep learning discriminative and generative architectures
    are discussed, along with their applications in speech and music synthesis. The
    article covers discriminative neural networks such as Multi-Layer Perceptron (MLP),
    Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN), as well
    as generative neural networks like Variational Autoencoders (VAE) and Deep Belief
    Networks (DBN). They also describe generative adversarial networks (GAN), their
    flaws, and enhancement strategies (with Wasserstein GAN as a standout). The study
    mainly focuses on speech generation and doesn’t focus much on different hybrid
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, [[3](#bib.bibx3)] emphasizes other areas of modeling, including
    feature representations, loss functions, data, and evaluation methods. It also
    investigates a variety of additional application fields, including enhancement
    as well as those outside of audio generation, such as source separation, audio
    classification, and tagging. They describe various audio aspects that are not
    covered here, such as the mel frequency cepstral coefficients (MFCC) and the constant-Q
    spectrogram. They do not cover as many architectures, but they do provide domain-specific
    datasets and evaluation methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike previous works, [[4](#bib.bibx4)] attempts to comprehensively examine
    a specific field of audio generation. This study considers five dimensions of
    music generation: objective, representation, architecture, challenge, and strategy.
    It looks at a variety of representations, both domain-specific and more general.
    Explains the fundamentals of music theory, including notes, rhythm, and chords.
    Introduces various previously established architectures such as MLP, VAE, RNN,
    CNN, and GAN, as well as some new ones like the Restricted Boltzmann Machine (RBM).
    Finally, it discusses the many challenges of music generation and ways for overcoming
    them. The work is quite extensive; however, some sections may benefit from a more
    detailed explanation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5](#bib.bibx5)] is another work that explores the subject of music generation
    and includes music translation. It discusses data representation, generative neural
    networks, and two popular DNN-based synthesizers. It discusses the issue of long-term
    dependence and how conditioning might alleviate it. Explains the autoregressive
    (AR) and normalized flow (NF) models, as well as VAE and GAN.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[1](#bib.bibx1)] provides an overview of deep learning techniques for audio.
    It distinguishes architectures from meta-architectures. The architectures include
    MLP, CNN, Temporal Convolutional Networks (TCN), and RNN, while the meta-architectures
    are Auto-Encoders (AE), VAE, GAN, Encoder/Decoder, Attention Mechanism, and Transformers.
    Divides audio representations into three categories: time-frequency, waveform,
    and knowledge-driven. Time-frequency representations include the Short-Time Fourier
    Transform (STFT), MFCC, Log-Mel-Spectrogram (LMS), and Constant-Q-Transform (CQT).
    The article concludes with a list of applications for audio deep learning algorithms,
    including music content description, environmental sound description, and content
    processing. It also briefly discusses semi-supervised and self-supervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6](#bib.bibx6)] provides a comprehensive overview of TTS methods, including
    history. It explains the basic components of TTS systems, such as text analysis,
    acoustic models, and vocoders, and includes a list of models in each area. Finally,
    it discusses advanced methods for implementing TTS systems in certain use situations,
    such as Fast TTS, Low-Resource TTS, and Robust TTS.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7](#bib.bibx7)] discusses TTS, music generation, audiovisual multi-modal
    processing, and datasets. This effort differs from earlier ones in that it organizes
    relevant articles by category rather than explaining subjects in depth.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[8](#bib.bibx8)] is the closest work to this one. It follows a similar structure,
    starting with input representations including raw waveforms, spectrograms, acoustic
    characteristics, embeddings, and symbolic representations, followed by conditioning
    representations used to guide audio synthesis. Includes audio synthesis techniques
    such as AR, NF, GAN, and VAE. The article concludes with the following evaluation
    methods: perceptual evaluation, number of statistically different bins, inception
    score, distance-based measurements, spectral convergence, and log likelihood.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[9](#bib.bibx9)] provides an overview of transformer architectures used in
    the field of speech processing. The article provides a description of the transformer,
    a list of popular transformers for speech, and a literature review on its applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[10](#bib.bibx10)] surveys TTS and speech enhancement, with a focus on diffusion
    models. Although the emphasis is on diffusion models, they also discuss the stages
    of TTS, pioneering work, and specialized models for distinct speech enhancement
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[11](#bib.bibx11)] conducted a comprehensive survey of deep learning techniques
    in speech processing. It begins with speech features and traditional speech processing
    models. It addresses the following deep learning architectures: RNN, CNN, Transformer,
    Conformer, Sequence-to-Sequence models (Seq2seq), Reinforcement learning, Graph
    neural networks (GNN), and diffusion probabilistic networks. Explains supervised,
    unsupervised, semi-supervised, and self-directed speech representation learning.
    Finally, it discusses a variety of speech processing tasks, including neural speech
    synthesis, speech-to-speech translation, speech enhancement, audio super resolution,
    as well transfer learning techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: III Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main purpose of this section is to show how audio generation has developed
    over the years up to this point. Since audio generation is a broad field that
    encompasses many different areas, such as text-to-speech synthesis, voice conversion,
    speech enhancement,… [[12](#bib.bibx12)], we will only focus on two different
    areas of audio generation: text-to-speech synthesis and music generation. There
    is no particular reason for this choice, except that they are among the more popular
    ones. The trend we want to show is how domain-specific knowledge is shifting towards
    general-purpose methods and how feature engineering is turning into feature recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Text-to-Speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text-to-speech (TTS) is a task with numerous applications, ranging from phone
    assistants to GPS navigators. The desire to construct a machine that can communicate
    with a human has historically fueled growth in this subject. Conventional speech
    synthesis technologies include rule-based concatenative speech synthesis (CSS)
    and statistical parametric speech synthesis (SPSS) [[13](#bib.bibx13)]. CSS and
    SPSS, which employ speech data, may be considered corpus-based speech synthesis
    approaches [[14](#bib.bibx14)].
  prefs: []
  type: TYPE_NORMAL
- en: Until the late 1980s, the field was dominated by rule-based systems [[15](#bib.bibx15)].
    They were heavily reliant on domain expertise such as phonological theory, necessitating
    the collaboration of many experts to develop a comprehensive rule set that would
    generate speech parameters. There are numerous works like [[16](#bib.bibx16),
    [17](#bib.bibx17), [18](#bib.bibx18), [19](#bib.bibx19)].
  prefs: []
  type: TYPE_NORMAL
- en: 'CSS methods try to achieve the naturalness and intelligibility of speech by
    combining chunks of recorded speech. They can be divided into two categories:
    fixed inventory and unit-selection approaches [[13](#bib.bibx13)]. Fixed inventory
    uses only one instance of each concatenative unit, which goes through signal processing
    before being combined into a spoken word. An example of this might be [[20](#bib.bibx20)],
    which uses the diphone method of segment assembly. On the other hand, unit-selection
    employs a large number of concatenative units, which can result in a better match
    between adjacent units, potentially boosting speech quality. There are two fundamental
    concepts: target cost and concatenation cost. The target cost determines how well
    an element from a database of units fits the desired unit, whereas the concatenation
    cost indicates how well a pair of selected units combine. The goal is to minimize
    both costs for the entire sequence of units; this is commonly done using a Viterbi
    search [[13](#bib.bibx13)]. Although it is always possible to minimize costs,
    the resulting speech may still contain errors. This can arise owing to a lack
    of units to choose from, an issue that can be mitigated by increasing the database
    size. It sounds straightforward; however, doing so increases unit creation costs
    and search times due to the increased number of possible concatenations. All of
    this requires CSS techniques to pick between speech quality and synthesis speed.
    Works in this domain include [[21](#bib.bibx21)], where they employ non-uniform
    synthesis units, and [[22](#bib.bibx22)], which treats units of a unit-selection
    database as states in a state transition network.'
  prefs: []
  type: TYPE_NORMAL
- en: SPSS models speech parameters using statistical methods depending on the desired
    phoneme sequence. This differs from CSS techniques in that we are not maintaining
    natural, unaltered speech but rather teaching the model how to recreate it. In
    a typical SPSS system, this is done by first extracting parametric representations
    of speech and then modeling them using generative models, commonly by applying
    the maximum likelihood criterion [[23](#bib.bibx23)]. The primary advantage of
    SPSS over CSS is its ability to generalize to unknown data [[13](#bib.bibx13)].
    This enables us to adjust the model to generate different voice characteristics
    [[15](#bib.bibx15)]. It also requires orders of magnitude less memory because
    we use model parameters instead of a speech database. Although there are other
    SPSS techniques, the majority of research has centered on hidden Markov models
    (HMM) [[15](#bib.bibx15)].
  prefs: []
  type: TYPE_NORMAL
- en: Some HMM works include [[14](#bib.bibx14)], which considers not only the output
    probability of static and dynamic feature vectors but also the global variance
    (GV). [[24](#bib.bibx24)] directly models speech waveforms with a trajectory HMM.
    [[25](#bib.bibx25), [26](#bib.bibx26)] use decision-tree-based context clustering
    to represent spectrum, pitch, and HMM state duration simultaneously. Commonly
    used contexts include the current phoneme, preceding and succeeding phonemes,
    the position of the current syllable within the current word or phrase, etc. [[27](#bib.bibx27)].
  prefs: []
  type: TYPE_NORMAL
- en: The notion that the human speech system has a layered structure in its transformation
    of the linguistic level to the waveform level has stimulated the adoption of deep
    neural network speech synthesis [[28](#bib.bibx28)]. [[29](#bib.bibx29)] employs
    an artificial neural network alongside a rule-based method to model speech parameters.
    [[30](#bib.bibx30)] employs limited Boltzmann machines and deep belief networks
    to predict speech parameters for each HMM state. Some other methods worth noting
    are multi-layer perceptron [[31](#bib.bibx31), [32](#bib.bibx32), [28](#bib.bibx28),
    [33](#bib.bibx33), [34](#bib.bibx34)], time-delay neural network [[35](#bib.bibx35),
    [36](#bib.bibx36)], long short-term memory [[37](#bib.bibx37), [38](#bib.bibx38),
    [39](#bib.bibx39), [40](#bib.bibx40)], gated recurrent unit [[41](#bib.bibx41)],
    attention-based recurrent network [[42](#bib.bibx42)], and mixture density network
    [[43](#bib.bibx43), [41](#bib.bibx41)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The TTS system consists of four major components: the first converts text to
    a linguistic representation, the second determines the duration of each speech
    segment, the third converts the linguistic and timing representations into speech
    parameters, and the fourth is the vocoder, which generates the speech waveform
    based on the speech parameters [[35](#bib.bibx35)]. The majority of the works
    we presented focused on converting the linguistic representation into speech parameters,
    but there are also models focusing on, for example, grapheme-to-phoneme conversion
    [[44](#bib.bibx44), [45](#bib.bibx45)] to allow TTS without knowledge of linguistic
    features. Examples of vocoders include MLSA [[46](#bib.bibx46)], STRAIGHT [[47](#bib.bibx47)],
    and Vocaine [[48](#bib.bibx48)]. Finally, there have also been attempts to construct
    a fully end-to-end system, which means integrating text analysis and acoustic
    modeling into a single model [[42](#bib.bibx42)].'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Music generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Music has been a part of human life long before the invention of the electronic
    computer, and people have developed many guidelines for how beautifully sounded
    music should be made. For this reason alone, the discipline of music generation
    has placed a heavy emphasis on rule-based systems that use music theory to create
    logical rules. Unlike text, musical vocabulary is rather tiny, consisting of at
    most several hundred discrete note symbols [[49](#bib.bibx49)]. Music creation
    is classified into six categories: grammars, knowledge-based, markov chains, artificial
    neural networks, evolutionary methods, and self-similarity [[50](#bib.bibx50)].
    Specific methods include discrete nonlinear maps [[51](#bib.bibx51), [52](#bib.bibx52)],
    rule-based [[53](#bib.bibx53), [54](#bib.bibx54)], genetic algorithm [[55](#bib.bibx55),
    [56](#bib.bibx56), [57](#bib.bibx57), [58](#bib.bibx58)], recurrent neural network
    [[59](#bib.bibx59), [57](#bib.bibx57)], long short-term memory [[60](#bib.bibx60),
    [61](#bib.bibx61), [62](#bib.bibx62), [63](#bib.bibx63)], markov chain [[52](#bib.bibx52),
    [64](#bib.bibx64)], context-free grammars [[64](#bib.bibx64), [58](#bib.bibx58)],
    context-sensitive grammars [[65](#bib.bibx65), [66](#bib.bibx66)], cellular automaton
    [[67](#bib.bibx67)], random fields [[49](#bib.bibx49)], L-systems [[68](#bib.bibx68)],
    knowledge base [[69](#bib.bibx69)], and restricted Boltzmann machines [[70](#bib.bibx70)].
    Unlike language, music employs a significantly smaller number of acoustic features.
    These include MIDI representation [[56](#bib.bibx56), [62](#bib.bibx62)], encoded
    sheet music [[57](#bib.bibx57)], binary vector of an octave [[49](#bib.bibx49)],
    and piano roll [[70](#bib.bibx70), [62](#bib.bibx62)].'
  prefs: []
  type: TYPE_NORMAL
- en: IV Audio features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though there have been numerous audio features used throughout the history
    of audio generation. Here we will describe the two most popular features, the
    raw waveform and the log-mel spectrogram, but also mention features that have
    recently gained traction. Keep in mind that there are too many features to describe
    them all, especially if we take into account the many hand-crafted features that
    were created before the rise of deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Raw waveform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term "raw audio" typically refers to a waveform recorded using pulse code
    modulation (PCM) [[8](#bib.bibx8)]. In PCM, a continuous waveform is sampled at
    uniform intervals, known as the sampling frequency. According to the sampling
    principle, if a signal is sampled at regular intervals at a rate slightly higher
    than twice the highest signal frequency, then it will contain all of the original
    signal information [[71](#bib.bibx71)]. The average sample frequency for audio
    applications is 44.1 kHz [[8](#bib.bibx8)], hence we cannot hold frequencies equal
    to or greater than 22.05 kHz. Computers cannot store real numbers with absolute precision;
    thus, each sample value is approximated by assigning it an element from a set
    of finite values, a technique known as quantization [[8](#bib.bibx8)]. The most
    common quantization levels are kept in 8 bits (256 levels), 16 bits (65536 levels),
    and 24 bits (16.8 million levels) [[8](#bib.bibx8)].
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using raw audio waveforms is that they can be easily transformed
    into actual sound. In certain tasks, the disadvantages appear to outweigh the
    benefits, as raw waveforms are still not universally used. The issue is that raw
    audio synthesis at higher bit rates becomes problematic due to the sheer amount
    of states involved [[72](#bib.bibx72)]. For example, 24-bit audio signals have
    more than 16 million states. High sampling rates create exceptionally long sequences,
    making raw audio synthesis more challenging [[73](#bib.bibx73)]. $\mu$-law is
    frequently employed in speech generative models like WaveNet [[74](#bib.bibx74)]
    to compress integer values and sequence length. The method can quantize each timestep
    to 256 values and reconstruct high-quality audio [[73](#bib.bibx73)]. According
    to [[75](#bib.bibx75)], increased bit depth representation can lead to models
    learning undesirable aspects, such as the calm background of the surroundings.
    It should be emphasized that this issue was only observed in older publications
    and is not discussed in current ones.
  prefs: []
  type: TYPE_NORMAL
- en: The most common models that use raw waveforms as their representation of choice
    are text-to-speech models called vocoders. In section [III](#S3 "III Background
    ‣ A survey of deep learning audio generation methods"), we mentioned vocoders,
    which are used to translate mid-term representations, such as mel-spectrograms,
    to raw audio waveforms. Examples include WaveNet [[74](#bib.bibx74)], SampleRNN
    [[76](#bib.bibx76)], and Deep Voice 3 [[77](#bib.bibx77)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Mel-spectrogram
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can talk about mel-spectrograms, we must first understand the Short-Time
    Fourier Transform (STFT). To represent audio frequencies, we use a Discrete-Fourier
    Transform (DFT), which transforms the original waveform into a sum of weighted
    complex exponentials [[78](#bib.bibx78)]. The problem emerges when we attempt
    to analyze complex audio signals; because the content of most audio signals changes
    over time, we can’t use DFT to figure out how frequencies change. Instead, we
    use STFT to apply DFT to overlapping sections of the audio waveform [[8](#bib.bibx8)].
    Most techniques that use the STFT to represent audio consider just its amplitude
    [[1](#bib.bibx1)], which results in a lossy representation. By removing the phase
    of the STFT, we can arrange it in a time/frequency visual, creating a spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: A mel-spectrogram compresses the STFT in the frequency axis by projecting it
    onto a scale known as the mel-scale [[79](#bib.bibx79)]. The mel-scale divides
    the frequency range into a set of mel-frequency bands, with higher frequencies
    having lower resolution and lower frequencies having higher resolution [[11](#bib.bibx11)].
    The scale was inspired by the non-linear frequency perception of human hearing
    [[11](#bib.bibx11)]. Applying the logarithm to the amplitude results in the log-mel-spectrogram
    [[1](#bib.bibx1)]. Finally, using the discrete cosine transform yields the mel
    frequency cepstral coefficients (MFCC) [[3](#bib.bibx3)]. MFCC is a popular representation
    in speech applications [[13](#bib.bibx13)], but it was shown to be unnecessary
    with deep learning models [[3](#bib.bibx3), [1](#bib.bibx1)].
  prefs: []
  type: TYPE_NORMAL
- en: While representations such as the STFT and raw waveform are invertible, the
    spectrogram is not, so we must use some approach to approximate the missing values.
    The algorithms used for these were already mentioned in the previous section.
    In addition to neural-based vocoders, other algorithms include Griffin-Lim [[80](#bib.bibx80)],
    gradient-based inversion [[81](#bib.bibx81)], single-pass spectrogram inversion
    (SPSI) [[82](#bib.bibx82)], and phase gradient heap integration (PGHI) [[83](#bib.bibx83)].
    Mel-spectrograms have been frequently utilized as intermediate features in text-to-speech
    pipelines [[7](#bib.bibx7)]. Tacotron 1/2 [[84](#bib.bibx84), [85](#bib.bibx85)]
    and FastSpeech 1/2 [[86](#bib.bibx86), [87](#bib.bibx87)] are examples of such
    models. To better illustrate the compression of the mel-spectrogram, take, for
    example, a 5-minute video sampled at 44.1 kHz. With 16-bit depth, our raw waveform
    will take up ${\approx}25\text{MB}$, while the mel-spectrogram with a common configuration¹¹1Based
    on a small sample of articles observed using mel-spectrograms of 80 bins, 256
    hop size, 1024 window size, and 1024 points of Fourier transform takes up ${\approx}8\text{MB}$
    at the same bit depth. In a \citeyearchoi_ComparisonAudioSignal_2018 article,
    it was proven that mel-spectrogram is preferable over STFT because it achieves
    the same performance while having a more compact representation [[88](#bib.bibx88)].
    Given the field’s progress, it should be emphasized that only recurrent and convolutional
    models were examined in the article. Another advantage of the mel-spectrogram,
    and spectrograms in general, is that they can be displayed as images. Because
    it ignores phase information, it can be shown with one dimension being frequency
    and the other being time. This is useful since images have been widely employed
    in computer vision tasks, allowing us to borrow models for usage in the audio
    domain. There is a concern as spectrograms aren’t the same as images due to the
    different meaning of the axis. This does not appear to have a substantial effect
    since many works implement mel-spectrograms in their convolutional models [[1](#bib.bibx1)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Neural codecs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An audio codec is a signal processing technique that compresses an audio signal
    into discrete codes before using those codes to reconstruct the audio signal,
    which is not always possible with complete accuracy. A typical audio codec system
    consists of three components: an encoder, a quantizer, and a decoder. The function
    of each component is explained in section [V-A](#S5.SS1 "V-A Auto-encoders ‣ V
    Architectures ‣ A survey of deep learning audio generation methods"). The goal
    of an audio codec is to use as little information as possible to store or transmit
    an audio signal while ensuring that the decoded audio quality is not significantly
    reduced by eliminating redundant or irrelevant information from the audio signal.
    Traditionally, this is accomplished by changing the signal and trading off the
    quality of specific signal components that are less likely to influence the quality
    [[89](#bib.bibx89)]. Audio codecs have been utilized for a wide range of applications,
    including mobile and internet communication. There are numerous types of audio
    codecs; some are utilized in real-time applications like streaming, while others
    may be used for audio production. Whereas in streaming, latency is a larger concern,
    which means sacrificing quality for speed, in production, we want to retain as
    much detail as possible while maintaining a compact representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Audio codecs can be separated into two categories: waveform codecs and parametric
    codecs. Waveform codecs make little to no assumptions about the nature of audio,
    allowing them to work with any audio signal. This universality makes them well-suited
    for creating high-quality audio at low compression, but they tend to produce artifacts
    when operating at high compression [[90](#bib.bibx90)]. Furthermore, because they
    do not operate well in high compression, they tend to increase storage and transmission
    costs. In contrast to waveform codecs, parametric codecs make assumptions about
    the source audio being encoded and introduce strong priors in the form of a parametric
    model that characterizes the audio synthesis process. The goal is not to achieve
    a faithful reconstruction on a sample-by-sample basis but rather to generate audio
    that is perceptually comparable to the original [[90](#bib.bibx90)]. Parametric
    codecs offer great compression but suffer from low decoded audio quality and noise
    susceptibility [[91](#bib.bibx91)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the way to the neural codec, we first encountered hybrid codecs, which substituted
    some parametric codec modules with neural networks. This type of codec improves
    performance by leveraging neural networks’ adaptability. Following that came vocoder-based
    approaches, which could leverage previously introduced neural vocoders to reconstruct
    audio signals by conditioning them on parametric coder codes or quantized acoustic
    features. However, their performance and compression were still dependent on the
    handcrafted features received at the input [[92](#bib.bibx92)]. The observation
    that separating models into modules prevents them from functioning effectively
    has inspired end-to-end auto-encoders (E2E AE) that accept raw waveforms as input
    and output. A standard E2E AE is made up of four basic components: an encoder,
    a projector, a quantizer, and a decoder [[92](#bib.bibx92)]. The basic use case
    is to take the raw waveform and use the encoder to construct a representation
    with reduced temporal resolution, which is then projected into a multidimensional
    space by the projector component. To make the representations suitable for transmission
    and storage, we further quantize the projections into codes. These codes make
    up a lookup table, which is used at the other end by the decoder to transform
    the quantized representations back to a raw waveform. [[93](#bib.bibx93)] defines
    the neural codec as a kind of neural network model that converts audio waveform
    into compact representations with a codec encoder and reconstructs audio waveform
    from these representations with a codec decoder. The core idea is to use the audio
    codec to compress the speech or sound into a set of discrete tokens, and then
    the generation model is used to generate these tokens [[94](#bib.bibx94)]. They
    have been shown to allow for cross-modal tasks [[95](#bib.bibx95)].'
  prefs: []
  type: TYPE_NORMAL
- en: Neural codec methods include SoundStream [[90](#bib.bibx90)], EnCodec [[89](#bib.bibx89)],
    HiFi-Codec [[94](#bib.bibx94)], AudioDec [[92](#bib.bibx92)], and APCodec [[91](#bib.bibx91)].
    All the said methods use residual vector quantization (RVQ), while HiFi-Codec
    also introduced an extension called group-RVQ. VQ methods will be talked about
    in section [V-A](#S5.SS1 "V-A Auto-encoders ‣ V Architectures ‣ A survey of deep
    learning audio generation methods"). SoundStream [[90](#bib.bibx90)] is used by
    AudioLM, [[96](#bib.bibx96)], MusicLM [[97](#bib.bibx97)], SingSong [[98](#bib.bibx98)]
    and SoundStorm [[99](#bib.bibx99)], while EnCodec [[89](#bib.bibx89)] is used
    by VALL-E [[73](#bib.bibx73)], VALL-E X [[100](#bib.bibx100)], Speech-X [[101](#bib.bibx101)],
    and VioLA [[95](#bib.bibx95)].
  prefs: []
  type: TYPE_NORMAL
- en: Finally, despite the fact that neural codec approaches are relatively new, they
    have not been without criticism. [[93](#bib.bibx93)] noted that although RVQ can
    achieve acceptable reconstruction quality and low bitrate, they are meant for
    compression and transmission; therefore, they may not be suited as intermediate
    representations for audio production jobs. This is because the sequence of discrete
    tokens created by RVQ can be very long, approximately $N$ times longer when $N$
    residual quantifiers are utilized. Because language models cannot handle extremely
    long sequences, we will encounter inaccurate predictions of discrete tokens, resulting
    in word skipping, word repetition, or speech collapse issues while attempting
    to reconstruct the speech waveform from these tokens.
  prefs: []
  type: TYPE_NORMAL
- en: V Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the models become more advanced, they start utilizing many different architectures
    in unison, making it impossible to categorize them efficiently. Therefore, each
    subsection will contain models that fit into many subsections but have been divided
    up in the way the author thought made the most sense. Unlike the audio features,
    there are many different architectures. Here we will mention the architectures
    that have been most commonly used in the field of audio generation.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Auto-encoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The majority of this section was taken from works by [[102](#bib.bibx102), [103](#bib.bibx103)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The auto-encoder’s objective is to duplicate the input into the output. It
    consists of two parts: an encoder and a decoder. The intersection of the two components
    depicts a code that attempts to represent both the input and, by extension, the
    output. The encoder receives input data and changes it into a code that the decoder
    then uses to approximate the original input. If we allowed arbitrary values in
    the encoder and decoder, we would obtain no meaningful code because it would simply
    simulate an identity function. To obtain meaningful code, we constrain both the
    encoder and decoder, preventing them from just passing data through. We can accomplish
    this by, for example, restricting the dimensionality of the values in the model.
    The auto-encoder has the advantage of not requiring labeled data because it merely
    seeks to reconstruct the input, allowing for unsupervised learning. [[104](#bib.bibx104)]
    demonstrates a basic use case for a simple auto-encoder, feature extraction. While
    VITS [[105](#bib.bibx105)] connects two text-to-speech modules using VAE, enabling
    end-to-end learning in an adversarial setting. Figure [2a](#S5.F2.sf1 "In Figure
    2 ‣ V-B Generative adversarial networks ‣ V Architectures ‣ A survey of deep learning
    audio generation methods") depicts a simple auto-encoder setup using generic encoder
    and decoder components.'
  prefs: []
  type: TYPE_NORMAL
- en: As this article focuses on generation, we will now introduce one of the most
    popular forms of the auto-encoder, the Variational Auto-Encoder (VAE) [[1](#bib.bibx1)].
    VAE has been proposed to enable us to employ auto-encoders as generative models
    [[8](#bib.bibx8)]. The VAE components can be considered as a combination of two
    separately parameterized models, the recognition model and the generative model.
    The VAE’s success was mostly due to the choice of the Kullback-Leibler (KL) divergence
    as the loss function [[8](#bib.bibx8)]. KL will also be described in section [VI](#S6
    "VI Evaluation metrics ‣ A survey of deep learning audio generation methods").
    Unlike the auto-encoder, the VAE learns the parameters of a probability distribution
    rather than a compressed representation of the data [[5](#bib.bibx5)]. Modeling
    the probability distribution allows us to sample from the learned data distribution.
    The Gaussian distribution is typically used for its generality [[4](#bib.bibx4)].
    MusicVAE [[106](#bib.bibx106)] uses a hierarchical decoder in a recurrent VAE
    to avoid posterior collapse. BUTTER [[107](#bib.bibx107)] creates a unified multi-model
    representation learning model using VAEs. Music FaderNets [[108](#bib.bibx108)]
    introduces a Guassian Mixture VAE. RAVE [[109](#bib.bibx109)] employs multi-stage
    training, initially with representation learning and then with adversarial fine-tuning.
    Tango [[110](#bib.bibx110)] and Make-An-Audio 2 [[111](#bib.bibx111)] generate
    mel-spectrograms by using VAE in a diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: Vector-Quantized VAE (VQ-VAE) is an extension of VAE that places the latent
    representation in a discrete latent space. VQ-VAE changes the auto-encoder structure
    by introducing a new component called the codebook. The most significant change
    happens between the encoder and decoder, where the encoder’s output is used in
    a nearest neighbor lookup utilizing the codebook. In other words, the continuous
    value received from the encoder is quantized and mapped onto a discrete latent
    space that will be received by the decoder. VQ-VAE replaces the KL divergence
    loss with negative log likelihood, codebook, and commitment losses. One possible
    issue with the VQ-VAE is codebook collapse. This occurs when the model stops using
    a piece of the codebook, indicating that it is no longer at full capacity. It
    can result in decreased likelihoods and inadequate reconstruction [[75](#bib.bibx75)].
    [[75](#bib.bibx75)] proposes the argmax auto-encoder as an alternative to VQ-VAE
    for music generation. MelGAN [[112](#bib.bibx112)], VQVAE [[113](#bib.bibx113)],
    Jukebox [[114](#bib.bibx114)] with Hierarchical VQ-VAE, DiscreTalk [[115](#bib.bibx115)],
    FIGARO [[116](#bib.bibx116)], Diffsound [[117](#bib.bibx117)], and Im2Wav [[118](#bib.bibx118)]
    use VQ-VAE to compress the input to a lower-dimensional space. While Dance2Music-GAN
    [[119](#bib.bibx119)], SpeechT5 [[120](#bib.bibx120)], VQTTS [[121](#bib.bibx121)],
    and DelightfulTTS 2 [[122](#bib.bibx122)] only use the vector quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Residual Vector Quantization (RVQ) improves VAE by computing the residual after
    quantization and further quantizing it using a second codebook, a third, and so
    on [[89](#bib.bibx89)]. In other words, RVQ cascades $N$ layers of VQ where unquantized
    input vector is passed through a first VQ and quantization residuals are computed,
    then those residuals are iteratively quantized by an additional sequence of $N-1$
    vector quantizers [[90](#bib.bibx90)]. Section [IV-C](#S4.SS3 "IV-C Neural codecs
    ‣ IV Audio features ‣ A survey of deep learning audio generation methods") provides
    a list of models that employ RVQ.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Generative adversarial networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another prominent generating architecture is generative adversarial networks
    (GAN). It is made up of two models that serve different purposes: the generator
    and the discriminator. The generator’s function is to convert a random input vector
    to a data sample. The random vector is usually smaller because the generator mimics
    the decoder part of the auto-encoder [[1](#bib.bibx1)]. Unlike VAE, which imposes a
    distribution to generate realistic data, GAN utilizes a second network called
    the discriminator [[1](#bib.bibx1)]. It takes the generator’s output or a sample
    from the dataset and attempts to classify it as either real or fake. The generator
    is penalized based on the discriminator’s ability to tell the difference between
    real and fake. The opposite is also true: if the discriminator is unable to distinguish
    between the generator and the actual data points, it is penalized as well. In
    other words, the two neural networks face off in a two-player minimax game. According
    to [[123](#bib.bibx123)], the ideal outcome for network training is for the discriminator
    to be 50% certain whether the input is real or bogus. In practice, we train the
    generator through the discriminator by reducing the probability that the sample
    is fake, while the discriminator does the opposite for fake data and the same
    for real data. Figure [2b](#S5.F2.sf2 "In Figure 2 ‣ V-B Generative adversarial
    networks ‣ V Architectures ‣ A survey of deep learning audio generation methods")
    illustrates the generator taking a random vector input and the discriminator attempting
    to distinguish between real and fake samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b4fd019599e3eafbc0236c2e03b1d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7056c8bac75f0dd3eed0f57c7fe3fe58.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Two deep learning architectures that appear to have little in common
    until we look closer. The generator mimics the auto-encoder’s decoder, whereas
    the discriminator resembles the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: This basic setup allows us to generate samples resembling those in the dataset,
    but it doesn’t let us condition the generation. In other words, the random vector
    used in the generator does not match the semantic features of the data [[5](#bib.bibx5)].
    Many datasets contain additional information about each sample, such as the type
    of object in an image. It would be beneficial if we could use the additional information to
    condition the generator and generate from a subset of the learned outputs. Conditional
    GAN (cGAN) induces additional structure by including additional information into
    the generator and discriminator inputs. The generator adds the additional information
    to the random vector, whereas the discriminator adds it to the data to discriminate.
    Some of the works that utilize cGAN are MidiNet [[124](#bib.bibx124)], [[125](#bib.bibx125)],
    [[126](#bib.bibx126)], [[127](#bib.bibx127)], and V2RA-GAN [[128](#bib.bibx128)].
  prefs: []
  type: TYPE_NORMAL
- en: Common issues with GAN include mode collapse, unstable training, and a lack
    of an evaluation metric [[2](#bib.bibx2)]. Mode collapse occurs when the generator
    focuses exclusively on a few outputs that can trick the discriminator into thinking
    they are real. Even if the generator meets the discriminator requirements, we
    cannot use it to produce more than a few examples. This might happen because the
    discriminator is unable to force the generator to be diverse [[123](#bib.bibx123)].
    The Wasserstein GAN (WGAN) is a well-known variant for addressing this problem.
    WGAN shifts the discriminator’s job from distinguishing between real and forged
    data to computing the Wasserstein distance, commonly known as the Earth Mover’s
    distance. In addition, a modification to aid WGAN convergence has been proposed;
    it uses a gradient penalty rather than weight clipping and is known as WGAN-GP.
    WGAN was used by MuseGAN [[129](#bib.bibx129)], WaveGAN [[130](#bib.bibx130)],
    TiFGAN [[131](#bib.bibx131)], and Catch-A-Waveform [[132](#bib.bibx132)].
  prefs: []
  type: TYPE_NORMAL
- en: Another popular modification to the GAN architecture is the use of deep convolutional
    networks known as deep convolutional GANs (DCGAN). Unlike WGAN, DCGAN only requires
    a change to the model architecture, rather than the entire training procedure,
    for both the generator and discriminator. It aims to provide a stable learning
    environment in an unsupervised setting by applying a set of architectural constraints
    [[123](#bib.bibx123)]. DGAN is used in works such as MidiNet [[124](#bib.bibx124)],
    WaveGAN [[130](#bib.bibx130)], and TiFGAN [[131](#bib.bibx131)].
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it is worth noting a simple GAN extension designed to address the
    issue of vanishing gradients while simultaneously improving training stability.
    Least squares GAN (LSGAN) improves the quality of generated samples by altering
    the discriminator’s loss function. Unlike the regular GAN, LSGAN penalizes correctly
    classified samples much more, pulling them toward the decision boundary, which
    allows LSGAN to generate samples that are closer to the real data [[133](#bib.bibx133)].
    Papers using LSGAN include SEGAN [[134](#bib.bibx134)], [[135](#bib.bibx135)],
    HiFi-GAN [[136](#bib.bibx136)], Parallel WaveGAN [[137](#bib.bibx137)], Fre-GAN
    [[138](#bib.bibx138)], VITS [[105](#bib.bibx105)] and V2RA-GAN [[128](#bib.bibx128)].
  prefs: []
  type: TYPE_NORMAL
- en: There are many more modifications to GAN we haven’t mentioned, like the Cycle
    GAN [[139](#bib.bibx139)] or the Boundary-Equilibrium GAN [[140](#bib.bibx140)],
    as we tried to showcase the most prevalent modifications in the field of audio
    generation. Works like MelGAN [[112](#bib.bibx112)], GAAE [[141](#bib.bibx141)],
    GGAN [[142](#bib.bibx142)], SEANet [[143](#bib.bibx143)], EATS [[144](#bib.bibx144)],
    Dance2Music-GAN [[119](#bib.bibx119)] and Musika [[145](#bib.bibx145)] use yet
    another type of loss called the hinge loss.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’d like to mention works that were challenging to categorize. They
    are GANSynth [[146](#bib.bibx146)], GAN-TTS [[147](#bib.bibx147)], RegNet [[148](#bib.bibx148)],
    Audeo [[149](#bib.bibx149)], Multi-Band MelGAN [[150](#bib.bibx150)], Multi-Singer
    [[151](#bib.bibx151)] and DelightfulTTS 2 [[122](#bib.bibx122)].
  prefs: []
  type: TYPE_NORMAL
- en: V-C Normalizing flows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although VAE and GAN were frequently utilized in audio generation, neither allowed
    for an exact evaluation of the probability density of new points [[152](#bib.bibx152)].
    Normalizing Flows (NF) are a family of generative models with tractable distributions
    that enable exact density evaluation and sampling. The network is made up of two
    "flows" that move in opposite directions. One flow starts with a base density,
    which we call noise, and progresses to a more complex density. The opposing flow
    reverses the direction, transforming the complex density back into the base density.
    The movement from base to complex is known as the generating direction, whereas
    the reverse is known as the normalizing direction. The term normalizing flow refers
    to the notion that the normalizing direction makes a complex distribution more
    regular, or normal. The normal distribution is typically used for base density,
    which is another reason for the name. Similar to how we layer transformations in
    a deep neural network, we compose several simple functions to generate complex
    behavior. These functions cannot be chosen arbitrarily because the flow must be
    in both directions; hence, the functions chosen must be invertible. Using a characteristic
    of the invertible function composition, we can create a likelihood-based estimation
    of the parameters that we can apply to train the model. In this setup, data generation
    is simple; utilizing the generative flow, we can input a sample from the base distribution
    and generate the required complex distribution sample. It has been formally proven
    that if you can build an arbitrarily complex transformation, you will be able
    to generate any distribution under reasonable assumptions [[152](#bib.bibx152)].
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the nature of the function employed in the flow, there can be significant
    performance differences that impact training or inference time. Inverse Autoregressive
    Flows (IAF) are a type of NF model with a specialized function that allows for
    efficient synthesis. The transform is based on an autoregressive network, which
    means that the current output is only determined by the current and previous input
    values. The advantage of this transformation is that the generative flow may be
    computed in parallel, allowing efficient use of resources such as the GPU. Although
    IAF networks can be run in parallel during inference, training with maximum likelihood
    estimation requires sequential processing. To allow for parallel training, a probability
    density distillation strategy is used [[153](#bib.bibx153), [154](#bib.bibx154)].
    In this method, we try to transfer knowledge from an already-trained teacher to
    a student model. Parallel WaveNet [[153](#bib.bibx153)] and ClariNet [[154](#bib.bibx154)] are
    two IAF models that employ this method. On the other hand, WaveGlow [[155](#bib.bibx155)],
    FloWaveNet [[156](#bib.bibx156)], and Glow-TTS [[157](#bib.bibx157)] all utilize
    an affine coupling layer. Because this layer allows for both parallel training
    and inference, they can avoid the problems associated with the former. Other efforts
    that are worth mentioning are WaveNODE [[158](#bib.bibx158)], which uses continuous
    normalizing flow, and WaveFlow [[159](#bib.bibx159)], which uses a dilated 2-D
    convolutional architecture.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this section, we’d like to address a problem that can arise when
    using flow-based networks with audio data. Because audio is digitally stored in
    a discrete representation, training a continuous density model on discrete data
    might lead to poor model performance. [[160](#bib.bibx160)] presented audio dequantization
    methods that can be deployed in flow-based networks and improved audio generating
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Transformer networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The majority of the material discussed in this section comes from [[161](#bib.bibx161)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can discuss transformers, we need to talk about attention. Attention
    has three components: a query, keys, and values. We have a database of (key, value)
    pairs, and our goal is to locate all values that closely match their key with
    the query. In the transformer, we improve on this concept by introducing a new
    type of attention known as self-attention. Self-attention includes three new functions
    that accept input and have learnable parameters. The functions change the input
    into one of the three previously mentioned components: query, key, and value.
    The prefix self refers to the fact that we utilize the same input for both the
    database and the query. If we were to translate a sentence, we would expect the
    translation of the nth word to be determined not only by itself but also by the
    other words in the sentence. For this example, the query would be the nth word
    and the database would be the sentence itself; if the parameters were learned
    correctly, we would expect to see relevant values for translation as the output
    of self-attention. To boost the model’s capacity to capture both short- and long-term
    dependencies, we can concatenate numerous self-attention modules, each with its
    own set of parameters, resulting in multi-head self-attention. Furthermore, if
    we want to prevent the model from attending to future entries, we can use masked
    multi-head self-attention, which employs a mask to specify which future entries
    we wish to ignore.'
  prefs: []
  type: TYPE_NORMAL
- en: The second essential element of the transformer is positional encoding. Instead
    of processing a sequence one at a time, the self-attention in the transformer
    provides parallel computing. The consequence is that the sequence’s order is not
    preserved. The prevailing method for preserving order information is to feed the
    model with additional input associated with each token. These additional inputs
    are known as positional encodings. Position encoding can be absolute or relative,
    and it can be predefined or learned during training.
  prefs: []
  type: TYPE_NORMAL
- en: At last, the transformer, like an auto-encoder, has both an encoder and a decoder.
    Both the encoder and decoder are made up of a stack of identical layers, each
    with two sublayers. The first sublayer is multi-head self-attention, whereas the
    second is a feed-forward network. In addition, each identical layer contains a
    residual connection surrounding both sublayers that follows layer normalization.
    Unlike the encoder, the decoder employs both encoder-decoder attention and masked
    multi-head self-attention at the input. The encoder-decoder attention is a normal multi-head
    attention with queries from the decoder and (key, value) pairs from the encoder.
    Before the input is fed into the network, positional embedding is applied.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers were primarily intended for natural language processing but were
    then used for images with the vision transformer and lately for audio signals
    [[162](#bib.bibx162)]. They have revolutionized modern deep learning by providing
    the ability to model long-term sequences [[72](#bib.bibx72)]. On the other hand,
    transformers are generally referred to as data-hungry since they require a large
    amount of training data [[162](#bib.bibx162)]. The attention mechanism’s quadratic
    complexity makes it difficult to process long sequences [[72](#bib.bibx72)]. To
    use transformers with audio, we would convert signals into visual spectrograms
    and divide them into "patches" that are then treated as separate input tokens,
    analogous to text [[162](#bib.bibx162)]. There are many works that use the transformer
    architecture, including Music Transformer [[163](#bib.bibx163)], FastSpeech [[86](#bib.bibx86)],
    Wave2Midi2Wave [[164](#bib.bibx164)], [[165](#bib.bibx165)], RobuTrans [[166](#bib.bibx166)],
    Jukebox [[114](#bib.bibx114)], AlignTTS [[167](#bib.bibx167)], Multi-Track Music
    Machine [[168](#bib.bibx168)], JDI-T [[169](#bib.bibx169)], AdaSpeech [[170](#bib.bibx170)],
    FastPitch [[171](#bib.bibx171)], [[72](#bib.bibx72)], Controllable Music Transformer
    [[172](#bib.bibx172)], [[173](#bib.bibx173)], SpeechT5 [[120](#bib.bibx120)],
    CPS [[174](#bib.bibx174)], FastSpeech 2 [[87](#bib.bibx87)], FIGARO [[116](#bib.bibx116)],
    HAT [[175](#bib.bibx175)], ELMG [[176](#bib.bibx176)], AudioLM [[96](#bib.bibx96)],
    VALL-E [[73](#bib.bibx73)], MusicLM [[97](#bib.bibx97)], SingSong [[98](#bib.bibx98)],
    SPEAR-TTS [[177](#bib.bibx177)], AudioGen [[178](#bib.bibx178)], VALL-E X [[100](#bib.bibx100)],
    dGSLM [[179](#bib.bibx179)], VioLA [[95](#bib.bibx95)], MuseCoco [[180](#bib.bibx180)],
    Im2Wav [[118](#bib.bibx118)], AudioPaLM [[181](#bib.bibx181)], VampNet [[182](#bib.bibx182)],
    LM-VC [[183](#bib.bibx183)], UniAudio [[184](#bib.bibx184)], and MusicGen [[185](#bib.bibx185)].
  prefs: []
  type: TYPE_NORMAL
- en: LakhNES [[186](#bib.bibx186)] and REMI [[187](#bib.bibx187)] use Transformer-XL,
    an extension of the Transformer that can, in theory, encode arbitrary long contexts
    into fixed-length representations. This is accomplished by providing a recurrence
    mechanism [[188](#bib.bibx188)], wherein the preceding segment is cached for later
    usage as an expanded context for the subsequent segment. Furthermore, to support
    the recurrence mechanism, it introduces an expanded positional encoding scheme known
    as relative positional encoding, which keeps positional information coherent when
    reusing states. In addition to Transformer-XL, [[189](#bib.bibx189)] and [[190](#bib.bibx190)]
    presented Perceiver AR and Museformer as alternatives to tackle problems that
    require extended contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, another extension to the transformer has been successful for various
    speech tasks [[191](#bib.bibx191)]. Convolution-augmented Transformer (Conformer)
    extends the Transformer by incorporating convolution and self-attention between
    two feed-forward modules; this cascade of modules is a single Conformer block.
    It integrates a relative positional encoding scheme, a method adopted from the
    described Transformer-XL to improve generalization for diverse input lengths [[192](#bib.bibx192)].
    Papers utilizing the conformer are SpeechNet [[193](#bib.bibx193)], $\text{A}^{3}\text{T}$
    [[191](#bib.bibx191)], VQTTS [[121](#bib.bibx121)], [[194](#bib.bibx194)], and
    SoundStorm [[99](#bib.bibx99)].
  prefs: []
  type: TYPE_NORMAL
- en: V-E Diffusion models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diffusion models are generative models inspired by non-equilibrium thermodynamics
    [[11](#bib.bibx11)]. Diffusion models, like normalizing flows, consist of two
    processes: forward and reverse. The forward process converts the data to a conventional
    Gaussian distribution by constructing a Markov chain of diffusion steps with a
    predetermined noise schedule. The reverse method gradually reconstructs data samples
    from the noise using an interference noise schedule. Unlike other architectures
    that change the distribution of data, such as variational auto-encoders and normalizing
    flows, diffusion models maintain the dimensionality of the latent variables fixed.
    Because the dimensionality of the latent variables must be fixed during the iterative
    generation process, which can result in slow inference speed in high-dimensional
    spaces [[195](#bib.bibx195)]. A potential solution is to utilize a more compressed
    representation, such as a mel-spectrogram, instead of a short-time Fourier transform.
    Papers employing diffusion models include WaveGrad [[196](#bib.bibx196)], DiffWave
    [[197](#bib.bibx197)], Diff-TTS [[198](#bib.bibx198)], Grad-TTS [[199](#bib.bibx199)],
    DiffuSE [[200](#bib.bibx200)], FastDiff [[201](#bib.bibx201)], CDiffuSE [[202](#bib.bibx202)],
    Guided-TTS [[203](#bib.bibx203)], Guided-TTS 2 [[204](#bib.bibx204)], DiffSinger
    [[205](#bib.bibx205)], UNIVERSE [[206](#bib.bibx206)], Diffsound [[117](#bib.bibx117)],
    Noise2Music [[207](#bib.bibx207)], DiffAVA [[208](#bib.bibx208)], MeLoDy [[209](#bib.bibx209)],
    Tango [[110](#bib.bibx110)], SRTNet [[210](#bib.bibx210)], MusicLDM [[211](#bib.bibx211)],
    JEN-1 [[212](#bib.bibx212)], AudioLDM [[195](#bib.bibx195)], [[213](#bib.bibx213)],
    ERNIE-Music [[214](#bib.bibx214)], and Re-AudioLDM [[215](#bib.bibx215)].'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer and diffusion models were the most popular designs discussed in
    this article. As a result, in the final half of this chapter, we will list some
    works that use both the transformer and diffusion models. These works include
    [[216](#bib.bibx216)], Make-An-Audio 2 [[111](#bib.bibx111)], NaturalSpeech 2
    [[93](#bib.bibx93)], Grad-StyleSpeech [[217](#bib.bibx217)], Make-An-Audio [[218](#bib.bibx218)],
    AudioLDM 2 [[219](#bib.bibx219)], and Moûsai [[220](#bib.bibx220)].
  prefs: []
  type: TYPE_NORMAL
- en: VI Evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluations could be considered the most important piece of the puzzle. By
    introducing evaluations, we are able to quantify progress. We can compare, improve,
    and optimize our models, all thanks to evaluation metrics. We will not mention
    domain-specific evaluation metrics such as the character error rate used in text-to-speech
    or the perceptual evaluation of speech quality used in speech enhancement. There
    are many more widely used metrics that we will not mention in the following sections.
    Some of them are: Nearest neighbor comparisons, Number of statistically-different
    bins, Kernel Inception Distance, and CLIP score.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Human evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As humans, we are constantly exposed to various sorts of sounds, which provides
    us with a wealth of expertise when attempting to distinguish between real and
    manufactured audio. The audio-generating method we are seeking to construct is
    intended to trick people into thinking the sound is a recording rather than synthesis.
    As a result, who better to judge the success of such systems than the ones we’re
    attempting to fool? Human evaluation is the gold standard for assessing audio
    quality. The human ear is particularly sensitive to irregularities, which are
    disruptive for the listener [[146](#bib.bibx146)]. Intuitively, it is simple to
    label an audio sample as good or bad, real or fake, but it is much more challenging to
    document a procedure derived from our thinking that may be used to evaluate future
    audio samples. The human assessment is often carried out with a defined number
    of listeners who listen and rate the audio on a 1-5 Likert scale. This type of
    test is termed the Mean-Opinion Score (MOS) [[221](#bib.bibx221)]. While MOS is
    used to evaluate naturalness, similarity MOS is used to assess how similar the
    generated and real samples are [[222](#bib.bibx222)]. Another metric, known as
    the comparative MOS, can be used to compare two systems by subtracting their MOS
    values. We may also calculate it by providing listeners with two audio samples
    generated by different models and immediately judging which one is better [[165](#bib.bibx165)].
    [[153](#bib.bibx153)] discovered that preference scores from a paired comparison
    test, frequently referred to as the A/B test, were more reliable than the MOS
    score. Many alternative human evaluation metrics have been proposed for music;
    domain-specific metrics include melody, groove, consonance, coherence, and integrity.
    The biggest disadvantage of human evaluation is that the findings cannot be replicated exactly.
    This means that the concrete numbers in the evaluation are unimportant, and only
    the link between them is crucial. This stems from the inherent subjectivity of
    human evaluation as well as biases or predispositions for specific auditory features.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Inception score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Inception Score (IS) is a perceptual metric that correlates with human evaluation.
    The Inception score is calculated by applying a pretrained Inception classifier
    to the generative model’s output. The IS is defined as the mean Kullback-Leibler
    divergence between the conditional output class label distribution and the labels’
    marginal distribution. It evaluates the diversity and quality of the audio outputs
    and prefers generated samples that the model can confidently classify. With $N$
    samples, the measure ranges from $1$ to $N$. The IS is maximized when the classifier
    is confident in every classification of the generated sample and each label is
    predicted equally often [[130](#bib.bibx130)]. Normally, the Inception classifier
    is trained using the ImageNet dataset, which may not be compatible with audio
    spectrograms. This will cause the classifier to be unable to separate the data
    into meaningful categories, resulting in a low IS score. An extension to the IS
    called Modified Inception Score (mIS) measures the within-class diversity of samples
    in addition to the IS which favors sharp and clear samples [[197](#bib.bibx197)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Fréchet distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The inception score is based solely on the generated samples, not taking into
    consideration the real samples [[141](#bib.bibx141)]. The Fréchet Inception Distance
    (FID) score addresses this issue by comparing the generated samples with the real ones.
    The FID calculates the Fréchet distance between two distributions for both generated
    and real samples using distribution parameters taken from the intermediate layer
    of the pretrained Inception Network [[141](#bib.bibx141)]. The lower the FID score,
    the higher the perceived generation quality. It is frequently used to assess the
    fidelity of generated samples in the image generation domain [[117](#bib.bibx117)].
    This metric was found to correlate with perceptual quality and diversity on synthetic
    distributions [[146](#bib.bibx146)]. The Inception Network is trained on the ImageNet
    dataset, which is purpose-built for images, but this does not ensure it will function
    for spectrograms. It may be unable to classify the spectrograms into any meaningful
    categories, resulting in unsatisfactory results [[142](#bib.bibx142)].
  prefs: []
  type: TYPE_NORMAL
- en: Fréchet Audio Distance (FAD) is a perceptual metric adapted from the FID for
    the audio domain. Unlike reference-based metrics, FAD measures the distance between
    the generated audio distribution and the real audio distribution using a pretrained
    audio classifier that does not use reference audio samples. The VGGish model [[223](#bib.bibx223)]
    is used to extract the characteristics of both generated and real audio [[219](#bib.bibx219)].
    As with the FID, the lower the score, the better the audio fidelity. According
    to [[178](#bib.bibx178)], the FAD correlates well with human perceptions of audio
    quality. The FAD was found to be robust against noise, computationally efficient,
    consistent with human judgments, and sensitive to intra-class mode dropping [[79](#bib.bibx79)].
    Although FAD may indicate good audio quality, it does not necessarily indicate
    that the sample is desired or relevant [[97](#bib.bibx97)]. For instance, in text-to-speech
    applications, low-FAD audio may be generated that does not match the input text.
    According to [[147](#bib.bibx147)], the FAD measure is not appropriate for evaluating
    text-to-speech models since it was created for music. While according to [[214](#bib.bibx214)],
    calculating the similarity between real and generated samples does not account
    for sample quality. Another similar metric called the Fréchet DeepSpeech Distance
    (FDSD) also uses the Fréchet distance on audio features extracted by a speech
    recognition model [[8](#bib.bibx8)]. [[144](#bib.bibx144)] found the FDSD to be
    unreliable in their use case.
  prefs: []
  type: TYPE_NORMAL
- en: The last Fréchet metric that is important to discuss is the Fréchet Distance
    (FD). Unlike the FAD, which extracts features using the VGGish [[223](#bib.bibx223)]
    model, FD employs the PANN [[224](#bib.bibx224)] model. The model change enables
    the FD to use different audio representations as input. PANN [[224](#bib.bibx224)]
    uses mel-spectrogram as input, whereas VGGish [[223](#bib.bibx223)] uses raw waveform.
    FD evaluates audio quality using an audio embedding model to measure the similarity
    between the embedding space of generations and that of targets [[211](#bib.bibx211)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-D Kullback-Leibler divergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kullback-Leibler (KL) divergence is a reference-dependent metric that computes
    the divergence between the generated and reference audio distributions. It uses
    a pretrained classifier to obtain the probabilities of generated and reference
    samples and then calculates the KL divergence between the distributions. The probabilities
    are computed over the class predictions of the pretrained classifier. A low KL
    divergence score may indicate that a generated audio sample shares concepts with
    the given reference [[212](#bib.bibx212)]. In music, this could indicate that
    the created audio has similar acoustic characteristics [[97](#bib.bibx97)]. While
    the FAD measure is more related to human perception [[110](#bib.bibx110)], the
    KL measure reflects more on the broader audio concepts occurring in the sample
    [[178](#bib.bibx178)].
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of deep learning methods has significantly changed the field
    of audio generation. In this work, we have presented three important parts of
    building a deep learning model for the task of audio generation. For audio representation,
    we have presented two long-standing champions and a third up-and-comer. We explained
    five architectures and listed work that implements them in the field of audio
    generation. Finally, we presented the four most common evaluations in the works
    we examined. While the first three architectures mentioned above seem to have
    lost importance in recent years, the transformer and diffusion models seem to
    have taken their place. This could be due to the popularization of large language
    models such as ChatGPT or, in the case of the diffusion models, diffusion-based
    text-to-image generators.
  prefs: []
  type: TYPE_NORMAL
- en: With the ever-increasing computing power and availability of large databases,
    it looks like the age of deep learning has only just begun. Just as deep learning
    has allowed us to move from domain-dependent features and methods to a more universal
    solution, more recent work has attempted to move from a single task or purpose
    to a multi-task or even multi-modality model.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Geoffroy Peeters and Gaël Richard “Deep Learning for Audio and Music” In
    *Multi-Faceted Deep Learning* Cham: Springer International Publishing, 2021, pp.
    231–266 DOI: [10.1007/978-3-030-74478-6_10](https://dx.doi.org/10.1007/978-3-030-74478-6_10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Yuanjun Zhao, Xianjun Xia and Roberto Togneri “Applications of Deep Learning
    to Audio Generation” In *IEEE Circuits and Systems Magazine* 19.4, 2019, pp. 19–38
    DOI: [10.1109/MCAS.2019.2945210](https://dx.doi.org/10.1109/MCAS.2019.2945210)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Hendrik Purwins et al. “Deep Learning for Audio Signal Processing” In *IEEE
    Journal of Selected Topics in Signal Processing* 13.2, 2019, pp. 206–219 DOI:
    [10.1109/JSTSP.2019.2908700](https://dx.doi.org/10.1109/JSTSP.2019.2908700)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Jean-Pierre Briot, Gaëtan Hadjeres and François-David Pachet “Deep Learning
    Techniques for Music Generation – A Survey”, 2019 DOI: [10.48550/arXiv.1709.01620](https://dx.doi.org/10.48550/arXiv.1709.01620)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Huzaifah and L. Wyse “Deep Generative Models for Musical Audio Synthesis”,
    2020 arXiv: [http://arxiv.org/abs/2006.06426](http://arxiv.org/abs/2006.06426)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Xu Tan, Tao Qin, Frank Soong and Tie-Yan Liu “A Survey on Neural Speech
    Synthesis”, 2021 DOI: [10.48550/arXiv.2106.15561](https://dx.doi.org/10.48550/arXiv.2106.15561)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Zhaofeng Shi “A Survey on Audio Synthesis and Audio-Visual Multimodal Processing”,
    2021 DOI: [10.48550/arXiv.2108.00443](https://dx.doi.org/10.48550/arXiv.2108.00443)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Anastasia Natsiou and Seán O’Leary “Audio Representations for Deep Learning
    in Sound Synthesis: A Review” In *2021 IEEE/ACS 18th International Conference
    on Computer Systems and Applications (AICCSA)*, 2021, pp. 1–8 DOI: [10.1109/AICCSA53542.2021.9686838](https://dx.doi.org/10.1109/AICCSA53542.2021.9686838)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Siddique Latif et al. “Transformers in Speech Processing: A Survey”, 2023
    DOI: [10.48550/arXiv.2303.11607](https://dx.doi.org/10.48550/arXiv.2303.11607)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Chenshuang Zhang et al. “A Survey on Audio Diffusion Models: Text To Speech
    Synthesis and Enhancement in Generative AI”, 2023 DOI: [10.48550/arXiv.2303.13336](https://dx.doi.org/10.48550/arXiv.2303.13336)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Ambuj Mehrish et al. “A Review of Deep Learning Techniques for Speech
    Processing” In *Information Fusion* 99, 2023, pp. 101869 DOI: [10.1016/j.inffus.2023.101869](https://dx.doi.org/10.1016/j.inffus.2023.101869)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Zhen-Hua Ling et al. “Deep Learning for Acoustic Modeling in Parametric
    Speech Generation: A Systematic Review of Existing Techniques and Future Trends”
    In *IEEE Signal Processing Magazine* 32.3, 2015, pp. 35–52 DOI: [10.1109/MSP.2014.2359987](https://dx.doi.org/10.1109/MSP.2014.2359987)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] “Springer Handbook of Speech Processing”, Springer Handbooks Berlin, Heidelberg:
    Springer Berlin Heidelberg, 2008 DOI: [10.1007/978-3-540-49127-9](https://dx.doi.org/10.1007/978-3-540-49127-9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Tomoki Toda and Keiichi Tokuda “A Speech Parameter Generation Algorithm
    Considering Global Variance for HMM-Based Speech Synthesis” In *IEICE TRANSACTIONS
    on Information and Systems* E90-D.5 The Institute of Electronics, Information
    and Communication Engineers, 2007, pp. 816–824'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Paul Taylor “Text-to-Speech Synthesis” Cambridge: Cambridge University
    Press, 2009 DOI: [10.1017/CBO9780511816338](https://dx.doi.org/10.1017/CBO9780511816338)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] A.. Liberman et al. “Minimal Rules for Synthesizing Speech” In *The Journal
    of the Acoustical Society of America* 31.11, 1959, pp. 1490–1499 DOI: [10.1121/1.1907654](https://dx.doi.org/10.1121/1.1907654)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J.. Holmes, Ignatius G. Mattingly and J.. Shearme “Speech Synthesis by
    Rule” In *Language and Speech* 7.3 SAGE Publications Ltd, 1964, pp. 127–143 DOI:
    [10.1177/002383096400700301](https://dx.doi.org/10.1177/002383096400700301)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] C. Coker, N. Umeda and C. Browman “Automatic Synthesis from Ordinary English
    Text” In *IEEE Transactions on Audio and Electroacoustics* 21.3, 1973, pp. 293–298
    DOI: [10.1109/TAU.1973.1162458](https://dx.doi.org/10.1109/TAU.1973.1162458)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. Klatt “Structure of a Phonological Rule Component for a Synthesis-by-Rule
    Program” In *IEEE Transactions on Acoustics, Speech, and Signal Processing* 24.5,
    1976, pp. 391–398 DOI: [10.1109/TASSP.1976.1162847](https://dx.doi.org/10.1109/TASSP.1976.1162847)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] N. Dixon and H. Maxey “Terminal Analog Synthesis of Continuous Speech
    Using the Diphone Method of Segment Assembly” In *IEEE Transactions on Audio and
    Electroacoustics* 16.1, 1968, pp. 40–50 DOI: [10.1109/TAU.1968.1161948](https://dx.doi.org/10.1109/TAU.1968.1161948)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Sagisaka “Speech Synthesis by Rule Using an Optimal Selection of Non-Uniform
    Synthesis Units” IEEE Computer Society, 1988, pp. 679\bibrangessep680\bibrangessep681\bibrangessep682–679\bibrangessep680\bibrangessep681\bibrangessep682
    DOI: [10.1109/ICASSP.1988.196677](https://dx.doi.org/10.1109/ICASSP.1988.196677)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A.J. Hunt and A.W. Black “Unit Selection in a Concatenative Speech Synthesis
    System Using a Large Speech Database” In *1996 IEEE International Conference on
    Acoustics, Speech, and Signal Processing Conference Proceedings* 1, 1996, pp.
    373–376 vol. 1 DOI: [10.1109/ICASSP.1996.541110](https://dx.doi.org/10.1109/ICASSP.1996.541110)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Heiga Zen, Keiichi Tokuda and Alan W. Black “Statistical Parametric Speech
    Synthesis” In *Speech Communication* 51.11, 2009, pp. 1039–1064 DOI: [10.1016/j.specom.2009.04.004](https://dx.doi.org/10.1016/j.specom.2009.04.004)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Kazuhiro Nakamura, Kei Hashimoto, Yoshihiko Nankaku and Keiichi Tokuda
    “Integration of Spectral Feature Extraction and Modeling for HMM-Based Speech
    Synthesis” In *IEICE Transactions on Information and Systems* E97.D.6, 2014, pp.
    1438–1448 DOI: [10.1587/transinf.E97.D.1438](https://dx.doi.org/10.1587/transinf.E97.D.1438)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Takayoshi Yoshimura et al. “Simultaneous Modeling of Spectrum, Pitch and
    Duration in HMM-based Speech Synthesis” In *6th European Conference on Speech
    Communication and Technology (Eurospeech 1999)* ISCA, 1999, pp. 2347–2350 DOI:
    [10.21437/Eurospeech.1999-513](https://dx.doi.org/10.21437/Eurospeech.1999-513)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Keiichi Tokuda, Heiga Zen and Alan W. Black “An HMM-based Speech Synthesis
    System Applied to English” In *Proc. IEEE Workshop on Speech Synthesis, 2002*,
    2002 URL: [https://cir.nii.ac.jp/crid/1572543025105746048](https://cir.nii.ac.jp/crid/1572543025105746048)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Keiichi Tokuda et al. “Speech Synthesis Based on Hidden Markov Models”
    In *Proceedings of the IEEE* 101.5, 2013, pp. 1234–1252 DOI: [10.1109/JPROC.2013.2251852](https://dx.doi.org/10.1109/JPROC.2013.2251852)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Heiga Zen, Andrew Senior and Mike Schuster “Statistical Parametric Speech
    Synthesis Using Deep Neural Networks” In *2013 IEEE International Conference on
    Acoustics, Speech and Signal Processing*, 2013, pp. 7962–7966 DOI: [10.1109/ICASSP.2013.6639215](https://dx.doi.org/10.1109/ICASSP.2013.6639215)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Burniston and K.M. Curtis “A Hybrid Neural Network/Rule Based Architecture
    for Diphone Speech Synthesis” In *Proceedings of ICSIPNN ’94\. International Conference
    on Speech, Image Processing and Neural Networks*, 1994, pp. 323–326 vol.1 DOI:
    [10.1109/SIPNN.1994.344901](https://dx.doi.org/10.1109/SIPNN.1994.344901)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Zhen-Hua Ling, Li Deng and Dong Yu “Modeling Spectral Envelopes Using
    Restricted Boltzmann Machines and Deep Belief Networks for Statistical Parametric
    Speech Synthesis” In *IEEE Transactions on Audio, Speech, and Language Processing*
    21.10, 2013, pp. 2129–2139 DOI: [10.1109/TASL.2013.2269291](https://dx.doi.org/10.1109/TASL.2013.2269291)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] T. Weijters and J. Thole “Speech Synthesis with Artificial Neural Networks”
    In *IEEE International Conference on Neural Networks*, 1993, pp. 1764–1769 vol.3
    DOI: [10.1109/ICNN.1993.298824](https://dx.doi.org/10.1109/ICNN.1993.298824)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Heng Lu, Simon King and Oliver Watts “Combining a Vector Space Representation
    of Linguistic Context with a Deep Neural Network for Text-To-Speech Synthesis”
    In *8th ISCA Speech Synthesis Workshop*, 2013, pp. 261–265 URL: [https://www.research.ed.ac.uk/en/publications/combining-a-vector-space-representation-of-linguistic-context-wit](https://www.research.ed.ac.uk/en/publications/combining-a-vector-space-representation-of-linguistic-context-wit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Yuchen Fan, Yao Qian, Frank K. Soong and Lei He “Multi-Speaker Modeling
    and Speaker Adaptation for DNN-based TTS Synthesis” In *2015 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2015, pp. 4475–4479
    DOI: [10.1109/ICASSP.2015.7178817](https://dx.doi.org/10.1109/ICASSP.2015.7178817)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Keiichi Tokuday and Heiga Zen “Directly Modeling Speech Waveforms by Neural
    Networks for Statistical Parametric Speech Synthesis” In *2015 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2015, pp. 4215–4219
    DOI: [10.1109/ICASSP.2015.7178765](https://dx.doi.org/10.1109/ICASSP.2015.7178765)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Orhan Karaali, Gerald Corrigan and Ira Gerson “Speech Synthesis with Neural
    Networks”, 1998 arXiv: [http://arxiv.org/abs/cs/9811031](http://arxiv.org/abs/cs/9811031)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Orhan Karaali, Gerald Corrigan, Ira Gerson and Noel Massey “Text-To-Speech
    Conversion with Neural Networks: A Recurrent TDNN Approach”, 1998 DOI: [10.48550/arXiv.cs/9811032](https://dx.doi.org/10.48550/arXiv.cs/9811032)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Yuchen Fan, Yao Qian, Feng-Long Xie and Frank K. Soong “TTS Synthesis
    with Bidirectional LSTM Based Recurrent Neural Networks” In *Interspeech 2014*
    ISCA, 2014, pp. 1964–1968 DOI: [10.21437/Interspeech.2014-443](https://dx.doi.org/10.21437/Interspeech.2014-443)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Heiga Zen and Haşim Sak “Unidirectional Long Short-Term Memory Recurrent
    Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis” In
    *2015 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2015, pp. 4470–4474 DOI: [10.1109/ICASSP.2015.7178816](https://dx.doi.org/10.1109/ICASSP.2015.7178816)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Bo Li and Heiga Zen “Multi-Language Multi-Speaker Acoustic Modeling for
    LSTM-RNN Based Statistical Parametric Speech Synthesis” In *Interspeech 2016*
    ISCA, 2016, pp. 2468–2472 DOI: [10.21437/Interspeech.2016-172](https://dx.doi.org/10.21437/Interspeech.2016-172)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Keiichi Tokuda and Heiga Zen “Directly Modeling Voiced and Unvoiced Components
    in Speech Waveforms by Neural Networks” In *2016 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2016, pp. 5640–5644 DOI:
    [10.1109/ICASSP.2016.7472757](https://dx.doi.org/10.1109/ICASSP.2016.7472757)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Wenfu Wang, Shuang Xu and Bo Xu “Gating Recurrent Mixture Density Networks
    for Acoustic Modeling in Statistical Parametric Speech Synthesis” In *2016 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2016, pp. 5520–5524 DOI: [10.1109/ICASSP.2016.7472733](https://dx.doi.org/10.1109/ICASSP.2016.7472733)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Wenfu Wang, Shuang Xu and Bo Xu “First Step Towards End-to-End Parametric
    TTS Synthesis: Generating Spectral Parameters with Neural Attention” In *Interspeech
    2016* ISCA, 2016, pp. 2243–2247 DOI: [10.21437/Interspeech.2016-134](https://dx.doi.org/10.21437/Interspeech.2016-134)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Heiga Zen and Andrew Senior “Deep Mixture Density Networks for Acoustic
    Modeling in Statistical Parametric Speech Synthesis” In *2014 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2014, pp. 3844–3848
    DOI: [10.1109/ICASSP.2014.6854321](https://dx.doi.org/10.1109/ICASSP.2014.6854321)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Kanishka Rao, Fuchun Peng, Haşim Sak and Françoise Beaufays “Grapheme-to-Phoneme
    Conversion Using Long Short-Term Memory Recurrent Neural Networks” In *2015 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2015, pp. 4225–4229 DOI: [10.1109/ICASSP.2015.7178767](https://dx.doi.org/10.1109/ICASSP.2015.7178767)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Kaisheng Yao and Geoffrey Zweig “Sequence-to-Sequence Neural Net Models
    for Grapheme-to-Phoneme Conversion”, 2015 DOI: [10.48550/arXiv.1506.00196](https://dx.doi.org/10.48550/arXiv.1506.00196)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Imai “Cepstral Analysis Synthesis on the Mel Frequency Scale” In *ICASSP
    ’83\. IEEE International Conference on Acoustics, Speech, and Signal Processing*
    8, 1983, pp. 93–96 DOI: [10.1109/ICASSP.1983.1172250](https://dx.doi.org/10.1109/ICASSP.1983.1172250)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Hideki Kawahara, Ikuyo Masuda-Katsuse and Alain Cheveigné “Restructuring
    Speech Representations Using a Pitch-Adaptive Time–Frequency Smoothing and an
    Instantaneous-Frequency-Based F0 Extraction: Possible Role of a Repetitive Structure
    in Sounds1” In *Speech Communication* 27.3, 1999, pp. 187–207 DOI: [10.1016/S0167-6393(98)00085-5](https://dx.doi.org/10.1016/S0167-6393(98)00085-5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Yannis Agiomyrgiannakis “Vocaine the Vocoder and Applications in Speech
    Synthesis” In *2015 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, 2015, pp. 4230–4234 DOI: [10.1109/ICASSP.2015.7178768](https://dx.doi.org/10.1109/ICASSP.2015.7178768)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Victor Lavrenko and Jeremy Pickens “Polyphonic Music Modeling with Random
    Fields” In *Proceedings of the Eleventh ACM International Conference on Multimedia*,
    MULTIMEDIA ’03 New York, NY, USA: Association for Computing Machinery, 2003, pp.
    120–129 DOI: [10.1145/957013.957041](https://dx.doi.org/10.1145/957013.957041)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J.. Fernandez and F. Vico “AI Methods in Algorithmic Composition: A Comprehensive
    Survey” In *Journal of Artificial Intelligence Research* 48, 2013, pp. 513–582
    DOI: [10.1613/jair.3908](https://dx.doi.org/10.1613/jair.3908)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Jeff Pressing “Nonlinear Maps as Generators of Musical Design” In *Computer
    Music Journal* 12.2 The MIT Press, 1988, pp. 35–46 DOI: [10.2307/3679940](https://dx.doi.org/10.2307/3679940)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Charles Dodge and Thomas A. Jerse “Computer Music: Synthesis, Composition,
    and Performance” London: Macmillan, 1985'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Francesco Giomi and Marco Ligabue “Computational Generation and Study
    of Jazz Music” In *Interface* 20.1 Routledge, 1991, pp. 47–64 DOI: [10.1080/09298219108570576](https://dx.doi.org/10.1080/09298219108570576)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Charles Ames and Michael Domino “Cybernetic Composer: An Overview” In
    *Understanding Music with AI: Perspectives on Music Cognition* Cambridge, MA,
    USA: MIT Press, 1992, pp. 186–205'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] John Biles “GenJam: A Genetic Algorithm for Generating Jazz Solos” In
    *ICMC* 94 Ann Arbor, MI, 1994, pp. 131–137'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Artemis Moroni, Jônatas Manzolli, Fernando Von Zuben and Ricardo Gudwin
    “Vox Populi: An Interactive Evolutionary System for Algorithmic Music Composition”
    In *Leonardo Music Journal* 10, 2000, pp. 49–54 DOI: [10.1162/096112100570602](https://dx.doi.org/10.1162/096112100570602)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] C.-C.J. Chen and R. Miikkulainen “Creating Melodies with Evolving Recurrent
    Neural Networks” In *IJCNN’01\. International Joint Conference on Neural Networks.
    Proceedings (Cat. No.01CH37222)* 3, 2001, pp. 2241–2246 vol.3 DOI: [10.1109/IJCNN.2001.938515](https://dx.doi.org/10.1109/IJCNN.2001.938515)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Alfonso Ortega Puente, Rafael Sánchez Alfonso and Manuel Alfonseca Moreno
    “Automatic Composition of Music by Means of Grammatical Evolution” In *Proceedings
    of the 2002 Conference on APL: Array Processing Languages: Lore, Problems, and
    Applications*, APL ’02 New York, NY, USA: Association for Computing Machinery,
    2002, pp. 148–155 DOI: [10.1145/602231.602249](https://dx.doi.org/10.1145/602231.602249)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] MICHAEL C. MOZER “Neural Network Music Composition by Prediction: Exploring
    the Benefits of Psychoacoustic Constraints and Multi-scale Processing” In *Connection
    Science* 6.2-3 Taylor & Francis, 1994, pp. 247–280 DOI: [10.1080/09540099408915726](https://dx.doi.org/10.1080/09540099408915726)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Douglas Eck and Juergen Schmidhuber “A First Look at Music Composition
    Using Lstm Recurrent Neural Networks” In *Istituto Dalle Molle Di Studi Sull Intelligenza
    Artificiale* 103.4, 2002, pp. 48–56'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Hang Chu, Raquel Urtasun and Sanja Fidler “Song From PI: A Musically Plausible
    Network for Pop Music Generation”, 2016 DOI: [10.48550/arXiv.1611.03477](https://dx.doi.org/10.48550/arXiv.1611.03477)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Allen Huang and Raymond Wu “Deep Learning for Music”, 2016 DOI: [10.48550/arXiv.1606.04930](https://dx.doi.org/10.48550/arXiv.1606.04930)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Olof Mogren “C-RNN-GAN: Continuous Recurrent Neural Networks with Adversarial
    Training”, 2016 DOI: [10.48550/arXiv.1611.09904](https://dx.doi.org/10.48550/arXiv.1611.09904)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Kevin Jones “Compositional Applications of Stochastic Processes” In *Computer
    Music Journal* 5.2 The MIT Press, 1981, pp. 45–61 DOI: [10.2307/3679879](https://dx.doi.org/10.2307/3679879)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Kohonen “A Self-Learning Musical Grammar, or ’Associative Memory of the
    Second Kind”’ In *International 1989 Joint Conference on Neural Networks*, 1989,
    pp. 1–5 vol.1 DOI: [10.1109/IJCNN.1989.118552](https://dx.doi.org/10.1109/IJCNN.1989.118552)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Teuvo Kohonen, Pauli Laine, Kalev Tiits and Kari Torkkola “A Nonheuristic
    Automatic Composing Method” In *Music and Connectionism* The MIT Press, 1991,
    pp. 229–242 DOI: [10.7551/mitpress/4804.003.0020](https://dx.doi.org/10.7551/mitpress/4804.003.0020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Eduardo Reck Miranda “Granular Synthesis of Sounds by Means of a Cellular
    Automaton” In *Leonardo* 28.4 The MIT Press, 1995, pp. 297–300 DOI: [10.2307/1576193](https://dx.doi.org/10.2307/1576193)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Peter Worth and Susan Stepney “Growing Music: Musical Interpretations
    of L-Systems” In *Applications of Evolutionary Computing* Berlin, Heidelberg:
    Springer, 2005, pp. 545–550 DOI: [10.1007/978-3-540-32003-6_56](https://dx.doi.org/10.1007/978-3-540-32003-6_56)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Michael Chan, John Potter and Emery Schubert “Improving Algorithmic Music
    Composition with Machine Learning” In *9th International Conference on Music Perception
    and Cognition* Citeseer, 2006, pp. 1848–1854'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Nicolas Boulanger-Lewandowski, Yoshua Bengio and Pascal Vincent “Modeling
    Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic
    Music Generation and Transcription”, 2012 DOI: [10.48550/arXiv.1206.6392](https://dx.doi.org/10.48550/arXiv.1206.6392)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] H.. Black and J.. Edson “Pulse Code Modulation” In *Transactions of the
    American Institute of Electrical Engineers* 66.1, 1947, pp. 895–899 DOI: [10.1109/T-AIEE.1947.5059525](https://dx.doi.org/10.1109/T-AIEE.1947.5059525)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Prateek Verma and Chris Chafe “A Generative Model for Raw Audio Using
    Transformer Architectures” In *2021 24th International Conference on Digital Audio
    Effects (DAFx)*, 2021, pp. 230–237 DOI: [10.23919/DAFx51585.2021.9768298](https://dx.doi.org/10.23919/DAFx51585.2021.9768298)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Chengyi Wang et al. “Neural Codec Language Models Are Zero-Shot Text to
    Speech Synthesizers”, 2023 DOI: [10.48550/arXiv.2301.02111](https://dx.doi.org/10.48550/arXiv.2301.02111)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Aaron Oord et al. “WaveNet: A Generative Model for Raw Audio”, 2016 DOI:
    [10.48550/arXiv.1609.03499](https://dx.doi.org/10.48550/arXiv.1609.03499)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Sander Dieleman, Aaron Oord and Karen Simonyan “The Challenge of Realistic
    Music Generation: Modelling Raw Audio at Scale” In *Advances in Neural Information
    Processing Systems* 31 Curran Associates, Inc., 2018 URL: [https://proceedings.neurips.cc/paper/2018/hash/3e441eec3456b703a4fe741005f3981f-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/3e441eec3456b703a4fe741005f3981f-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Soroush Mehri et al. “SampleRNN: An Unconditional End-to-End Neural Audio
    Generation Model”, 2017 DOI: [10.48550/arXiv.1612.07837](https://dx.doi.org/10.48550/arXiv.1612.07837)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Wei Ping et al. “Deep Voice 3: Scaling Text-to-Speech with Convolutional
    Sequence Learning”, 2018 DOI: [10.48550/arXiv.1710.07654](https://dx.doi.org/10.48550/arXiv.1710.07654)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Paolo Prandoni and Martin Vetterli “Signal Processing for Communications”
    EPFL Press, 2008 GOOGLEBOOKS: tKUYSrBM4gYC'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Javier Nistal, Stefan Lattner and Gaël Richard “Comparing Representations
    for Audio Synthesis Using Generative Adversarial Networks” In *2020 28th European
    Signal Processing Conference (EUSIPCO)*, 2021, pp. 161–165 DOI: [10.23919/Eusipco47968.2020.9287799](https://dx.doi.org/10.23919/Eusipco47968.2020.9287799)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] D. Griffin and Jae Lim “Signal Estimation from Modified Short-Time Fourier
    Transform” In *IEEE Transactions on Acoustics, Speech, and Signal Processing*
    32.2, 1984, pp. 236–243 DOI: [10.1109/TASSP.1984.1164317](https://dx.doi.org/10.1109/TASSP.1984.1164317)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Rémi Decorsière, Peter L. Søndergaard, Ewen N. MacDonald and Torsten Dau
    “Inversion of Auditory Spectrograms, Traditional Spectrograms, and Other Envelope
    Representations” In *IEEE/ACM Transactions on Audio, Speech, and Language Processing*
    23.1, 2015, pp. 46–56 DOI: [10.1109/TASLP.2014.2367821](https://dx.doi.org/10.1109/TASLP.2014.2367821)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Gerald T. Beauregard, Mithila Harish and Lonce Wyse “Single Pass Spectrogram
    Inversion” In *2015 IEEE International Conference on Digital Signal Processing
    (DSP)*, 2015, pp. 427–431 DOI: [10.1109/ICDSP.2015.7251907](https://dx.doi.org/10.1109/ICDSP.2015.7251907)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Zdeněk Průša, Peter Balazs and Peter Lempel Søndergaard “A Noniterative
    Method for Reconstruction of Phase From STFT Magnitude” In *IEEE/ACM Transactions
    on Audio, Speech, and Language Processing* 25.5, 2017, pp. 1154–1164 DOI: [10.1109/TASLP.2017.2678166](https://dx.doi.org/10.1109/TASLP.2017.2678166)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Yuxuan Wang et al. “Tacotron: Towards End-to-End Speech Synthesis”, 2017
    DOI: [10.48550/arXiv.1703.10135](https://dx.doi.org/10.48550/arXiv.1703.10135)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Jonathan Shen et al. “Natural TTS Synthesis by Conditioning Wavenet on
    MEL Spectrogram Predictions” In *2018 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2018, pp. 4779–4783 DOI: [10.1109/ICASSP.2018.8461368](https://dx.doi.org/10.1109/ICASSP.2018.8461368)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Yi Ren et al. “FastSpeech: Fast, Robust and Controllable Text to Speech”
    In *Advances in Neural Information Processing Systems* 32 Curran Associates, Inc.,
    2019 URL: [https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Yi Ren et al. “FastSpeech 2: Fast and High-Quality End-to-End Text to
    Speech”, 2022 DOI: [10.48550/arXiv.2006.04558](https://dx.doi.org/10.48550/arXiv.2006.04558)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Keunwoo Choi, György Fazekas, Mark Sandler and Kyunghyun Cho “A Comparison
    of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging”
    In *2018 26th European Signal Processing Conference (EUSIPCO)*, 2018, pp. 1870–1874
    DOI: [10.23919/EUSIPCO.2018.8553106](https://dx.doi.org/10.23919/EUSIPCO.2018.8553106)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Alexandre Défossez, Jade Copet, Gabriel Synnaeve and Yossi Adi “High Fidelity
    Neural Audio Compression”, 2022 DOI: [10.48550/arXiv.2210.13438](https://dx.doi.org/10.48550/arXiv.2210.13438)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Neil Zeghidour et al. “SoundStream: An End-to-End Neural Audio Codec”
    In *IEEE/ACM Transactions on Audio, Speech, and Language Processing* 30, 2022,
    pp. 495–507 DOI: [10.1109/TASLP.2021.3129994](https://dx.doi.org/10.1109/TASLP.2021.3129994)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Yang Ai et al. “APCodec: A Neural Audio Codec with Parallel Amplitude
    and Phase Spectrum Encoding and Decoding”, 2024 arXiv: [http://arxiv.org/abs/2402.10533](http://arxiv.org/abs/2402.10533)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Yi-Chiao Wu, Israel D. Gebru, Dejan Marković and Alexander Richard “AudioDec:
    An Open-source Streaming High-fidelity Neural Audio Codec” In *ICASSP 2023 - 2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10096509](https://dx.doi.org/10.1109/ICASSP49357.2023.10096509)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Kai Shen et al. “NaturalSpeech 2: Latent Diffusion Models Are Natural
    and Zero-Shot Speech and Singing Synthesizers”, 2023 DOI: [10.48550/arXiv.2304.09116](https://dx.doi.org/10.48550/arXiv.2304.09116)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Dongchao Yang et al. “HiFi-Codec: Group-residual Vector Quantization for
    High Fidelity Audio Codec”, 2023 DOI: [10.48550/arXiv.2305.02765](https://dx.doi.org/10.48550/arXiv.2305.02765)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Tianrui Wang et al. “VioLA: Unified Codec Language Models for Speech Recognition,
    Synthesis, and Translation”, 2023 DOI: [10.48550/arXiv.2305.16107](https://dx.doi.org/10.48550/arXiv.2305.16107)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Zalán Borsos et al. “AudioLM: A Language Modeling Approach to Audio Generation”
    In *IEEE/ACM Transactions on Audio, Speech, and Language Processing* 31, 2023,
    pp. 2523–2533 DOI: [10.1109/TASLP.2023.3288409](https://dx.doi.org/10.1109/TASLP.2023.3288409)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Andrea Agostinelli et al. “MusicLM: Generating Music From Text”, 2023
    DOI: [10.48550/arXiv.2301.11325](https://dx.doi.org/10.48550/arXiv.2301.11325)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Chris Donahue et al. “SingSong: Generating Musical Accompaniments from
    Singing”, 2023 DOI: [10.48550/arXiv.2301.12662](https://dx.doi.org/10.48550/arXiv.2301.12662)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Zalán Borsos et al. “SoundStorm: Efficient Parallel Audio Generation”,
    2023 DOI: [10.48550/arXiv.2305.09636](https://dx.doi.org/10.48550/arXiv.2305.09636)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Ziqiang Zhang et al. “Speak Foreign Languages with Your Own Voice: Cross-Lingual
    Neural Codec Language Modeling”, 2023 DOI: [10.48550/arXiv.2303.03926](https://dx.doi.org/10.48550/arXiv.2303.03926)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Xiaofei Wang et al. “SpeechX: Neural Codec Language Model as a Versatile
    Speech Transformer”, 2023 DOI: [10.48550/arXiv.2308.06873](https://dx.doi.org/10.48550/arXiv.2308.06873)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Ian Goodfellow, Yoshua Bengio and Aaron Courville “Deep Learning” Cambridge,
    Mass: The MIT Press, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Diederik P. Kingma and Max Welling “An Introduction to Variational Autoencoders”
    In *Foundations and Trends® in Machine Learning* 12.4 Now Publishers, Inc., 2019,
    pp. 307–392 DOI: [10.1561/2200000056](https://dx.doi.org/10.1561/2200000056)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Hu-Cheng Lee, Chih-Yu Lin, Pin-Chun Hsu and Winston H. Hsu “Audio Feature
    Generation for Missing Modality Problem in Video Action Recognition” In *ICASSP
    2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2019, pp. 3956–3960 DOI: [10.1109/ICASSP.2019.8682513](https://dx.doi.org/10.1109/ICASSP.2019.8682513)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Jaehyeon Kim, Jungil Kong and Juhee Son “Conditional Variational Autoencoder
    with Adversarial Learning for End-to-End Text-to-Speech” In *Proceedings of the
    38th International Conference on Machine Learning* PMLR, 2021, pp. 5530–5540 URL:
    [https://proceedings.mlr.press/v139/kim21f.html](https://proceedings.mlr.press/v139/kim21f.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Adam Roberts et al. “A Hierarchical Latent Vector Model for Learning
    Long-Term Structure in Music” In *Proceedings of the 35th International Conference
    on Machine Learning* PMLR, 2018, pp. 4364–4373 URL: [https://proceedings.mlr.press/v80/roberts18a.html](https://proceedings.mlr.press/v80/roberts18a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Yixiao Zhang, Ziyu Wang, Dingsu Wang and Gus Xia “BUTTER: A Representation
    Learning Framework for Bi-directional Music-Sentence Retrieval and Generation”
    In *Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA)* Online:
    Association for Computational Linguistics, 2020, pp. 54–58 URL: [https://aclanthology.org/2020.nlp4musa-1.11](https://aclanthology.org/2020.nlp4musa-1.11)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Hao Hao Tan and Dorien Herremans “Music FaderNets: Controllable Music
    Generation Based On High-Level Features via Low-Level Feature Modelling”, 2020
    DOI: [10.48550/arXiv.2007.15474](https://dx.doi.org/10.48550/arXiv.2007.15474)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Antoine Caillon and Philippe Esling “RAVE: A Variational Autoencoder
    for Fast and High-Quality Neural Audio Synthesis”, 2021 DOI: [10.48550/arXiv.2111.05011](https://dx.doi.org/10.48550/arXiv.2111.05011)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish and Soujanya Poria
    “Text-to-Audio Generation Using Instruction-Tuned LLM and Latent Diffusion Model”,
    2023 DOI: [10.48550/arXiv.2304.13731](https://dx.doi.org/10.48550/arXiv.2304.13731)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Jiawei Huang et al. “Make-An-Audio 2: Temporal-Enhanced Text-to-Audio
    Generation”, 2023 DOI: [10.48550/arXiv.2305.18474](https://dx.doi.org/10.48550/arXiv.2305.18474)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Kundan Kumar et al. “MelGAN: Generative Adversarial Networks for Conditional
    Waveform Synthesis” In *Advances in Neural Information Processing Systems* 32
    Curran Associates, Inc., 2019 URL: [https://proceedings.neurips.cc/paper/2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Andros Tjandra et al. “VQVAE Unsupervised Unit Discovery and Multi-scale
    Code2Spec Inverter for Zerospeech Challenge 2019”, 2019 DOI: [10.48550/arXiv.1905.11449](https://dx.doi.org/10.48550/arXiv.1905.11449)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Prafulla Dhariwal et al. “Jukebox: A Generative Model for Music”, 2020
    DOI: [10.48550/arXiv.2005.00341](https://dx.doi.org/10.48550/arXiv.2005.00341)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Tomoki Hayashi and Shinji Watanabe “DiscreTalk: Text-to-Speech as a Machine
    Translation Problem”, 2020 DOI: [10.48550/arXiv.2005.05525](https://dx.doi.org/10.48550/arXiv.2005.05525)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Dimitri Rütte, Luca Biggio, Yannic Kilcher and Thomas Hofmann “FIGARO:
    Controllable Music Generation Using Learned and Expert Features”, 2022 URL: [https://openreview.net/forum?id=NyR8OZFHw6i](https://openreview.net/forum?id=NyR8OZFHw6i)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Dongchao Yang et al. “Diffsound: Discrete Diffusion Model for Text-to-Sound
    Generation” In *IEEE/ACM Transactions on Audio, Speech, and Language Processing*
    31, 2023, pp. 1720–1733 DOI: [10.1109/TASLP.2023.3268730](https://dx.doi.org/10.1109/TASLP.2023.3268730)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Roy Sheffer and Yossi Adi “I Hear Your True Colors: Image Guided Audio
    Generation” In *ICASSP 2023 - 2023 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10096023](https://dx.doi.org/10.1109/ICASSP49357.2023.10096023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Ye Zhu et al. “Quantized GAN for Complex Music Generation from Dance
    Videos” In *Computer Vision – ECCV 2022* Cham: Springer Nature Switzerland, 2022,
    pp. 182–199 DOI: [10.1007/978-3-031-19836-6_11](https://dx.doi.org/10.1007/978-3-031-19836-6_11)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Junyi Ao et al. “SpeechT5: Unified-Modal Encoder-Decoder Pre-Training
    for Spoken Language Processing”, 2022 DOI: [10.48550/arXiv.2110.07205](https://dx.doi.org/10.48550/arXiv.2110.07205)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Chenpeng Du, Yiwei Guo, Xie Chen and Kai Yu “VQTTS: High-Fidelity Text-to-Speech
    Synthesis with Self-Supervised VQ Acoustic Feature”, 2022 DOI: [10.48550/arXiv.2204.00768](https://dx.doi.org/10.48550/arXiv.2204.00768)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Yanqing Liu et al. “DelightfulTTS 2: End-to-End Speech Synthesis with
    Adversarial Vector-Quantized Auto-Encoders”, 2022 DOI: [10.48550/arXiv.2207.04646](https://dx.doi.org/10.48550/arXiv.2207.04646)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Norberto Torres-Reyes and Shahram Latifi “Audio Enhancement and Synthesis
    Using Generative Adversarial Networks: A Survey” In *International Journal of
    Computer Applications* 182.35, 2019, pp. 27–31 DOI: [10.5120/ijca2019918334](https://dx.doi.org/10.5120/ijca2019918334)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Li-Chia Yang, Szu-Yu Chou and Yi-Hsuan Yang “MidiNet: A Convolutional
    Generative Adversarial Network for Symbolic-domain Music Generation”, 2017 DOI:
    [10.48550/arXiv.1703.10847](https://dx.doi.org/10.48550/arXiv.1703.10847)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Daniel Michelsanti and Zheng-Hua Tan “Conditional Generative Adversarial
    Networks for Speech Enhancement and Noise-Robust Speaker Verification” In *Interspeech
    2017*, 2017, pp. 2008–2012 DOI: [10.21437/Interspeech.2017-1620](https://dx.doi.org/10.21437/Interspeech.2017-1620)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Lele Chen, Sudhanshu Srivastava, Zhiyao Duan and Chenliang Xu “Deep Cross-Modal
    Audio-Visual Generation” In *Proceedings of the on Thematic Workshops of ACM Multimedia
    2017*, Thematic Workshops ’17 New York, NY, USA: Association for Computing Machinery,
    2017, pp. 349–357 DOI: [10.1145/3126686.3126723](https://dx.doi.org/10.1145/3126686.3126723)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Paarth Neekhara et al. “Expediting TTS Synthesis with Adversarial Vocoding”,
    2019 DOI: [10.48550/arXiv.1904.07944](https://dx.doi.org/10.48550/arXiv.1904.07944)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Shiguang Liu, Sijia Li and Haonan Cheng “Towards an End-to-End Visual-to-Raw-Audio
    Generation With GAN” In *IEEE Transactions on Circuits and Systems for Video Technology*
    32.3, 2022, pp. 1299–1312 DOI: [10.1109/TCSVT.2021.3079897](https://dx.doi.org/10.1109/TCSVT.2021.3079897)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang and Yi-Hsuan Yang “MuseGAN:
    Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation
    and Accompaniment” In *Proceedings of the AAAI Conference on Artificial Intelligence*
    32.1, 2018 DOI: [10.1609/aaai.v32i1.11312](https://dx.doi.org/10.1609/aaai.v32i1.11312)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Chris Donahue, Julian McAuley and Miller Puckette “Adversarial Audio
    Synthesis”, 2019 DOI: [10.48550/arXiv.1802.04208](https://dx.doi.org/10.48550/arXiv.1802.04208)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Andrés Marafioti, Nathanaël Perraudin, Nicki Holighaus and Piotr Majdak
    “Adversarial Generation of Time-Frequency Features with Application in Audio Synthesis”
    In *Proceedings of the 36th International Conference on Machine Learning* PMLR,
    2019, pp. 4352–4362 URL: [https://proceedings.mlr.press/v97/marafioti19a.html](https://proceedings.mlr.press/v97/marafioti19a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Gal Greshler, Tamar Shaham and Tomer Michaeli “Catch-A-Waveform: Learning
    to Generate Audio from a Single Short Example” In *Advances in Neural Information
    Processing Systems* 34 Curran Associates, Inc., 2021, pp. 20916–20928 URL: [https://proceedings.neurips.cc/paper/2021/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Xudong Mao et al. “Least Squares Generative Adversarial Networks”, 2017
    DOI: [10.48550/arXiv.1611.04076](https://dx.doi.org/10.48550/arXiv.1611.04076)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Santiago Pascual, Antonio Bonafonte and Joan Serrà “SEGAN: Speech Enhancement
    Generative Adversarial Network”, 2017 DOI: [10.48550/arXiv.1703.09452](https://dx.doi.org/10.48550/arXiv.1703.09452)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Ryuichi Yamamoto, Eunwoo Song and Jae-Min Kim “Probability Density Distillation
    with Generative Adversarial Networks for High-Quality Parallel Waveform Generation”,
    2019 DOI: [10.48550/arXiv.1904.04472](https://dx.doi.org/10.48550/arXiv.1904.04472)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Jungil Kong, Jaehyeon Kim and Jaekyoung Bae “HiFi-GAN: Generative Adversarial
    Networks for Efficient and High Fidelity Speech Synthesis” In *Advances in Neural
    Information Processing Systems* 33 Curran Associates, Inc., 2020, pp. 17022–17033
    URL: [https://proceedings.neurips.cc/paper_files/paper/2020/hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2020/hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Ryuichi Yamamoto, Eunwoo Song and Jae-Min Kim “Parallel Wavegan: A Fast
    Waveform Generation Model Based on Generative Adversarial Networks with Multi-Resolution
    Spectrogram” In *ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2020, pp. 6199–6203 DOI: [10.1109/ICASSP40776.2020.9053795](https://dx.doi.org/10.1109/ICASSP40776.2020.9053795)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Ji-Hoon Kim, Sang-Hoon Lee, Ji-Hyun Lee and Seong-Whan Lee “Fre-GAN:
    Adversarial Frequency-consistent Audio Synthesis”, 2021 DOI: [10.48550/arXiv.2106.02297](https://dx.doi.org/10.48550/arXiv.2106.02297)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Wangli Hao, Zhaoxiang Zhang and He Guan “CMCGAN: A Uniform Framework
    for Cross-Modal Visual-Audio Mutual Generation” In *Proceedings of the AAAI Conference
    on Artificial Intelligence* 32.1, 2018 DOI: [10.1609/aaai.v32i1.12329](https://dx.doi.org/10.1609/aaai.v32i1.12329)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Jen-Yu Liu, Yu-Hua Chen, Yin-Cheng Yeh and Yi-Hsuan Yang “Unconditional
    Audio Generation with Generative Adversarial Networks and Cycle Regularization”
    In *Interspeech 2020*, 2020, pp. 1997–2001 DOI: [10.21437/Interspeech.2020-1137](https://dx.doi.org/10.21437/Interspeech.2020-1137)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Kazi Nazmul Haque, Rajib Rana and Björn W. Schuller “High-Fidelity Audio
    Generation and Representation Learning With Guided Adversarial Autoencoder” In
    *IEEE Access* 8, 2020, pp. 223509–223528 DOI: [10.1109/ACCESS.2020.3040797](https://dx.doi.org/10.1109/ACCESS.2020.3040797)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Kazi Nazmul Haque et al. “Guided Generative Adversarial Neural Network
    for Representation Learning and Audio Generation Using Fewer Labelled Audio Data”
    In *IEEE/ACM Transactions on Audio, Speech, and Language Processing* 29, 2021,
    pp. 2575–2590 DOI: [10.1109/TASLP.2021.3098764](https://dx.doi.org/10.1109/TASLP.2021.3098764)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas and Dominik Roblek “SEANet:
    A Multi-modal Speech Enhancement Network”, 2020 DOI: [10.48550/arXiv.2009.02095](https://dx.doi.org/10.48550/arXiv.2009.02095)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Jeff Donahue et al. “End-to-End Adversarial Text-to-Speech”, 2021 DOI:
    [10.48550/arXiv.2006.03575](https://dx.doi.org/10.48550/arXiv.2006.03575)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Marco Pasini and Jan Schlüter “Musika! Fast Infinite Waveform Music Generation”,
    2022 DOI: [10.48550/arXiv.2208.08706](https://dx.doi.org/10.48550/arXiv.2208.08706)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Jesse Engel et al. “GANSynth: Adversarial Neural Audio Synthesis”, 2019
    DOI: [10.48550/arXiv.1902.08710](https://dx.doi.org/10.48550/arXiv.1902.08710)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Mikołaj Bińkowski et al. “High Fidelity Speech Synthesis with Adversarial
    Networks”, 2019 DOI: [10.48550/arXiv.1909.11646](https://dx.doi.org/10.48550/arXiv.1909.11646)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Peihao Chen et al. “Generating Visually Aligned Sound From Videos” In
    *IEEE Transactions on Image Processing* 29, 2020, pp. 8292–8302 DOI: [10.1109/TIP.2020.3009820](https://dx.doi.org/10.1109/TIP.2020.3009820)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Kun Su, Xiulong Liu and Eli Shlizerman “Audeo: Audio Generation for a
    Silent Performance Video” In *Advances in Neural Information Processing Systems*
    33 Curran Associates, Inc., 2020, pp. 3325–3337 URL: [https://proceedings.neurips.cc/paper/2020/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Geng Yang et al. “Multi-Band Melgan: Faster Waveform Generation For High-Quality
    Text-To-Speech” In *2021 IEEE Spoken Language Technology Workshop (SLT)*, 2021,
    pp. 492–498 DOI: [10.1109/SLT48900.2021.9383551](https://dx.doi.org/10.1109/SLT48900.2021.9383551)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Rongjie Huang et al. “Multi-Singer: Fast Multi-Singer Singing Voice Vocoder
    With A Large-Scale Corpus” In *Proceedings of the 29th ACM International Conference
    on Multimedia*, MM ’21 New York, NY, USA: Association for Computing Machinery,
    2021, pp. 3945–3954 DOI: [10.1145/3474085.3475437](https://dx.doi.org/10.1145/3474085.3475437)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Ivan Kobyzev, Simon J.. Prince and Marcus A. Brubaker “Normalizing Flows:
    An Introduction and Review of Current Methods” In *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* 43.11, 2021, pp. 3964–3979 DOI: [10.1109/TPAMI.2020.2992934](https://dx.doi.org/10.1109/TPAMI.2020.2992934)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Aaron Oord et al. “Parallel WaveNet: Fast High-Fidelity Speech Synthesis”
    In *Proceedings of the 35th International Conference on Machine Learning* PMLR,
    2018, pp. 3918–3926 URL: [https://proceedings.mlr.press/v80/oord18a.html](https://proceedings.mlr.press/v80/oord18a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Wei Ping, Kainan Peng and Jitong Chen “ClariNet: Parallel Wave Generation
    in End-to-End Text-to-Speech”, 2019 DOI: [10.48550/arXiv.1807.07281](https://dx.doi.org/10.48550/arXiv.1807.07281)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Ryan Prenger, Rafael Valle and Bryan Catanzaro “Waveglow: A Flow-based
    Generative Network for Speech Synthesis” In *ICASSP 2019 - 2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2019, pp. 3617–3621
    DOI: [10.1109/ICASSP.2019.8683143](https://dx.doi.org/10.1109/ICASSP.2019.8683143)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Sungwon Kim et al. “FloWaveNet : A Generative Flow for Raw Audio”, 2019
    DOI: [10.48550/arXiv.1811.02155](https://dx.doi.org/10.48550/arXiv.1811.02155)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Jaehyeon Kim, Sungwon Kim, Jungil Kong and Sungroh Yoon “Glow-TTS: A
    Generative Flow for Text-to-Speech via Monotonic Alignment Search” In *Advances
    in Neural Information Processing Systems* 33 Curran Associates, Inc., 2020, pp.
    8067–8077 URL: [https://proceedings.neurips.cc/paper/2020/hash/5c3b99e8f92532e5ad1556e53ceea00c-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/5c3b99e8f92532e5ad1556e53ceea00c-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Hyeongju Kim et al. “WaveNODE: A Continuous Normalizing Flow for Speech
    Synthesis”, 2020 DOI: [10.48550/arXiv.2006.04598](https://dx.doi.org/10.48550/arXiv.2006.04598)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Wei Ping, Kainan Peng, Kexin Zhao and Zhao Song “WaveFlow: A Compact
    Flow-based Model for Raw Audio” In *Proceedings of the 37th International Conference
    on Machine Learning* PMLR, 2020, pp. 7706–7716 URL: [https://proceedings.mlr.press/v119/ping20a.html](https://proceedings.mlr.press/v119/ping20a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Hyun-Wook Yoon, Sang-Hoon Lee, Hyeong-Rae Noh and Seong-Whan Lee “Audio
    Dequantization for High Fidelity Audio Generation in Flow-based Neural Vocoder”,
    2020 DOI: [10.48550/arXiv.2008.06867](https://dx.doi.org/10.48550/arXiv.2008.06867)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Aston Zhang, Zachary C. Lipton, Mu Li and Alexander J. Smola “Dive into
    Deep Learning”, 2023 DOI: [10.48550/arXiv.2106.11342](https://dx.doi.org/10.48550/arXiv.2106.11342)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Khalid Zaman, Melike Sah, Cem Direkoglu and Masashi Unoki “A Survey of
    Audio Classification Using Deep Learning” In *IEEE Access* 11, 2023, pp. 106620–106649
    DOI: [10.1109/ACCESS.2023.3318015](https://dx.doi.org/10.1109/ACCESS.2023.3318015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Cheng-Zhi Anna Huang et al. “Music Transformer”, 2018 DOI: [10.48550/arXiv.1809.04281](https://dx.doi.org/10.48550/arXiv.1809.04281)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Curtis Hawthorne et al. “Enabling Factorized Piano Music Modeling and
    Generation with the MAESTRO Dataset”, 2019 DOI: [10.48550/arXiv.1810.12247](https://dx.doi.org/10.48550/arXiv.1810.12247)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Naihan Li et al. “Neural Speech Synthesis with Transformer Network” In
    *Proceedings of the AAAI Conference on Artificial Intelligence* 33.01, 2019, pp.
    6706–6713 DOI: [10.1609/aaai.v33i01.33016706](https://dx.doi.org/10.1609/aaai.v33i01.33016706)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Naihan Li et al. “RobuTrans: A Robust Transformer-Based Text-to-Speech
    Model” In *Proceedings of the AAAI Conference on Artificial Intelligence* 34.05,
    2020, pp. 8228–8235 DOI: [10.1609/aaai.v34i05.6337](https://dx.doi.org/10.1609/aaai.v34i05.6337)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Zhen Zeng et al. “Aligntts: Efficient Feed-Forward Text-to-Speech System
    Without Explicit Alignment” In *ICASSP 2020 - 2020 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2020, pp. 6714–6718 DOI:
    [10.1109/ICASSP40776.2020.9054119](https://dx.doi.org/10.1109/ICASSP40776.2020.9054119)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Jeff Ens and Philippe Pasquier “MMM : Exploring Conditional Multi-Track
    Music Generation with the Transformer”, 2020 DOI: [10.48550/arXiv.2008.06048](https://dx.doi.org/10.48550/arXiv.2008.06048)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Dan Lim et al. “JDI-T: Jointly Trained Duration Informed Transformer
    for Text-To-Speech without Explicit Alignment”, 2020 DOI: [10.48550/arXiv.2005.07799](https://dx.doi.org/10.48550/arXiv.2005.07799)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Mingjian Chen et al. “AdaSpeech: Adaptive Text to Speech for Custom Voice”,
    2021 DOI: [10.48550/arXiv.2103.00993](https://dx.doi.org/10.48550/arXiv.2103.00993)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Adrian Łańcucki “Fastpitch: Parallel Text-to-Speech with Pitch Prediction”
    In *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and
    Signal Processing (ICASSP)*, 2021, pp. 6588–6592 DOI: [10.1109/ICASSP39728.2021.9413889](https://dx.doi.org/10.1109/ICASSP39728.2021.9413889)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Shangzhe Di et al. “Video Background Music Generation with Controllable
    Music Transformer” In *Proceedings of the 29th ACM International Conference on
    Multimedia*, MM ’21 New York, NY, USA: Association for Computing Machinery, 2021,
    pp. 2037–2045 DOI: [10.1145/3474085.3475195](https://dx.doi.org/10.1145/3474085.3475195)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Ann Lee et al. “Direct Speech-to-Speech Translation with Discrete Units”,
    2022 DOI: [10.48550/arXiv.2107.05604](https://dx.doi.org/10.48550/arXiv.2107.05604)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Weipeng Wang et al. “CPS: Full-Song and Style-Conditioned Music Generation
    with Linear Transformer” In *2022 IEEE International Conference on Multimedia
    and Expo Workshops (ICMEW)*, 2022, pp. 1–6 DOI: [10.1109/ICMEW56448.2022.9859286](https://dx.doi.org/10.1109/ICMEW56448.2022.9859286)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Xueyao Zhang et al. “Structure-Enhanced Pop Music Generation via Harmony-Aware
    Learning” In *Proceedings of the 30th ACM International Conference on Multimedia*,
    MM ’22 New York, NY, USA: Association for Computing Machinery, 2022, pp. 1204–1213
    DOI: [10.1145/3503161.3548084](https://dx.doi.org/10.1145/3503161.3548084)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Chunhui Bao and Qianru Sun “Generating Music With Emotions” In *IEEE
    Transactions on Multimedia* 25, 2023, pp. 3602–3614 DOI: [10.1109/TMM.2022.3163543](https://dx.doi.org/10.1109/TMM.2022.3163543)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Eugene Kharitonov et al. “Speak, Read and Prompt: High-Fidelity Text-to-Speech
    with Minimal Supervision”, 2023 DOI: [10.48550/arXiv.2302.03540](https://dx.doi.org/10.48550/arXiv.2302.03540)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Felix Kreuk et al. “AudioGen: Textually Guided Audio Generation”, 2023
    DOI: [10.48550/arXiv.2209.15352](https://dx.doi.org/10.48550/arXiv.2209.15352)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Tu Anh Nguyen et al. “Generative Spoken Dialogue Language Modeling” In
    *Transactions of the Association for Computational Linguistics* 11, 2023, pp.
    250–266 DOI: [10.1162/tacl_a_00545](https://dx.doi.org/10.1162/tacl_a_00545)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Peiling Lu et al. “MuseCoco: Generating Symbolic Music from Text”, 2023
    DOI: [10.48550/arXiv.2306.00110](https://dx.doi.org/10.48550/arXiv.2306.00110)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Paul K. Rubenstein et al. “AudioPaLM: A Large Language Model That Can
    Speak and Listen”, 2023 DOI: [10.48550/arXiv.2306.12925](https://dx.doi.org/10.48550/arXiv.2306.12925)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar and Bryan Pardo “VampNet:
    Music Generation via Masked Acoustic Token Modeling”, 2023 DOI: [10.48550/arXiv.2307.04686](https://dx.doi.org/10.48550/arXiv.2307.04686)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Zhichao Wang et al. “LM-VC: Zero-shot Voice Conversion via Speech Generation
    Based on Language Models”, 2023 DOI: [10.48550/arXiv.2306.10521](https://dx.doi.org/10.48550/arXiv.2306.10521)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Dongchao Yang et al. “UniAudio: An Audio Foundation Model Toward Universal
    Audio Generation”, 2023 DOI: [10.48550/arXiv.2310.00704](https://dx.doi.org/10.48550/arXiv.2310.00704)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Jade Copet et al. “Simple and Controllable Music Generation” In *Advances
    in Neural Information Processing Systems* 36, 2023, pp. 47704–47720 URL: [https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Chris Donahue et al. “LakhNES: Improving Multi-Instrumental Music Generation
    with Cross-Domain Pre-Training”, 2019 DOI: [10.48550/arXiv.1907.04868](https://dx.doi.org/10.48550/arXiv.1907.04868)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Yu-Siang Huang and Yi-Hsuan Yang “Pop Music Transformer: Beat-based Modeling
    and Generation of Expressive Pop Piano Compositions” In *Proceedings of the 28th
    ACM International Conference on Multimedia*, MM ’20 New York, NY, USA: Association
    for Computing Machinery, 2020, pp. 1180–1188 DOI: [10.1145/3394171.3413671](https://dx.doi.org/10.1145/3394171.3413671)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Zihang Dai et al. “Transformer-XL: Attentive Language Models Beyond a
    Fixed-Length Context”, 2019 arXiv: [http://arxiv.org/abs/1901.02860](http://arxiv.org/abs/1901.02860)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Curtis Hawthorne et al. “General-Purpose, Long-Context Autoregressive
    Modeling with Perceiver AR” In *Proceedings of the 39th International Conference
    on Machine Learning* PMLR, 2022, pp. 8535–8558 URL: [https://proceedings.mlr.press/v162/hawthorne22a.html](https://proceedings.mlr.press/v162/hawthorne22a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Botao Yu et al. “Museformer: Transformer with Fine- and Coarse-Grained
    Attention for Music Generation” In *Advances in Neural Information Processing
    Systems* 35, 2022, pp. 1376–1388 URL: [https://proceedings.neurips.cc/paper_files/paper/2022/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2022/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] He Bai et al. “A$^3$T: Alignment-Aware Acoustic and Text Pretraining
    for Speech Synthesis and Editing” In *Proceedings of the 39th International Conference
    on Machine Learning* PMLR, 2022, pp. 1399–1411 URL: [https://proceedings.mlr.press/v162/bai22d.html](https://proceedings.mlr.press/v162/bai22d.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Anmol Gulati et al. “Conformer: Convolution-augmented Transformer for
    Speech Recognition”, 2020 arXiv: [http://arxiv.org/abs/2005.08100](http://arxiv.org/abs/2005.08100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Yi-Chen Chen et al. “SpeechNet: A Universal Modularized Model for Speech
    Processing Tasks”, 2021 DOI: [10.48550/arXiv.2105.03070](https://dx.doi.org/10.48550/arXiv.2105.03070)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Sravya Popuri et al. “Enhanced Direct Speech-to-Speech Translation Using
    Self-supervised Pre-training and Data Augmentation”, 2022 DOI: [10.48550/arXiv.2204.02967](https://dx.doi.org/10.48550/arXiv.2204.02967)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Haohe Liu et al. “AudioLDM: Text-to-Audio Generation with Latent Diffusion
    Models”, 2023 DOI: [10.48550/arXiv.2301.12503](https://dx.doi.org/10.48550/arXiv.2301.12503)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Nanxin Chen et al. “WaveGrad: Estimating Gradients for Waveform Generation”,
    2020 DOI: [10.48550/arXiv.2009.00713](https://dx.doi.org/10.48550/arXiv.2009.00713)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Zhifeng Kong et al. “DiffWave: A Versatile Diffusion Model for Audio
    Synthesis”, 2021 DOI: [10.48550/arXiv.2009.09761](https://dx.doi.org/10.48550/arXiv.2009.09761)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Myeonghun Jeong et al. “Diff-TTS: A Denoising Diffusion Model for Text-to-Speech”,
    2021 DOI: [10.48550/arXiv.2104.01409](https://dx.doi.org/10.48550/arXiv.2104.01409)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Vadim Popov et al. “Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech”
    In *Proceedings of the 38th International Conference on Machine Learning* PMLR,
    2021, pp. 8599–8608 URL: [https://proceedings.mlr.press/v139/popov21a.html](https://proceedings.mlr.press/v139/popov21a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Yen-Ju Lu, Yu Tsao and Shinji Watanabe “A Study on Speech Enhancement
    Based on Diffusion Probabilistic Model” In *2021 Asia-Pacific Signal and Information
    Processing Association Annual Summit and Conference (APSIPA ASC)*, 2021, pp. 659–666
    URL: [https://ieeexplore.ieee.org/abstract/document/9689602](https://ieeexplore.ieee.org/abstract/document/9689602)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Rongjie Huang et al. “FastDiff: A Fast Conditional Diffusion Model for
    High-Quality Speech Synthesis”, 2022 DOI: [10.48550/arXiv.2204.09934](https://dx.doi.org/10.48550/arXiv.2204.09934)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Yen-Ju Lu et al. “Conditional Diffusion Probabilistic Model for Speech
    Enhancement” In *ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2022, pp. 7402–7406 DOI: [10.1109/ICASSP43922.2022.9746901](https://dx.doi.org/10.1109/ICASSP43922.2022.9746901)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Heeseung Kim, Sungwon Kim and Sungroh Yoon “Guided-TTS: A Diffusion Model
    for Text-to-Speech via Classifier Guidance” In *Proceedings of the 39th International
    Conference on Machine Learning* PMLR, 2022, pp. 11119–11133 URL: [https://proceedings.mlr.press/v162/kim22d.html](https://proceedings.mlr.press/v162/kim22d.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Sungwon Kim, Heeseung Kim and Sungroh Yoon “Guided-TTS 2: A Diffusion
    Model for High-quality Adaptive Text-to-Speech with Untranscribed Data”, 2022
    DOI: [10.48550/arXiv.2205.15370](https://dx.doi.org/10.48550/arXiv.2205.15370)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Jinglin Liu et al. “DiffSinger: Singing Voice Synthesis via Shallow Diffusion
    Mechanism” In *Proceedings of the AAAI Conference on Artificial Intelligence*
    36.10, 2022, pp. 11020–11028 DOI: [10.1609/aaai.v36i10.21350](https://dx.doi.org/10.1609/aaai.v36i10.21350)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Joan Serrà et al. “Universal Speech Enhancement with Score-based Diffusion”,
    2022 DOI: [10.48550/arXiv.2206.03065](https://dx.doi.org/10.48550/arXiv.2206.03065)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Qingqing Huang et al. “Noise2Music: Text-conditioned Music Generation
    with Diffusion Models”, 2023 DOI: [10.48550/arXiv.2302.03917](https://dx.doi.org/10.48550/arXiv.2302.03917)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Shentong Mo, Jing Shi and Yapeng Tian “DiffAVA: Personalized Text-to-Audio
    Generation with Visual Alignment”, 2023 DOI: [10.48550/arXiv.2305.12903](https://dx.doi.org/10.48550/arXiv.2305.12903)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Max W.. Lam et al. “Efficient Neural Music Generation”, 2023 DOI: [10.48550/arXiv.2305.15719](https://dx.doi.org/10.48550/arXiv.2305.15719)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Zhibin Qiu et al. “SRTNET: Time Domain Speech Enhancement via Stochastic
    Refinement” In *ICASSP 2023 - 2023 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10095850](https://dx.doi.org/10.1109/ICASSP49357.2023.10095850)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Ke Chen et al. “MusicLDM: Enhancing Novelty in Text-to-Music Generation
    Using Beat-Synchronous Mixup Strategies”, 2023 DOI: [10.48550/arXiv.2308.01546](https://dx.doi.org/10.48550/arXiv.2308.01546)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Peike Li et al. “JEN-1: Text-Guided Universal Music Generation with Omnidirectional
    Diffusion Models”, 2023 DOI: [10.48550/arXiv.2308.04729](https://dx.doi.org/10.48550/arXiv.2308.04729)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Yatong Bai et al. “Accelerating Diffusion-Based Text-to-Audio Generation
    with Consistency Distillation”, 2023 DOI: [10.48550/arXiv.2309.10740](https://dx.doi.org/10.48550/arXiv.2309.10740)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Pengfei Zhu et al. “ERNIE-Music: Text-to-Waveform Music Generation with
    Diffusion Models”, 2023 DOI: [10.48550/arXiv.2302.04456](https://dx.doi.org/10.48550/arXiv.2302.04456)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Yi Yuan et al. “Retrieval-Augmented Text-to-Audio Generation”, 2024 DOI:
    [10.48550/arXiv.2309.08051](https://dx.doi.org/10.48550/arXiv.2309.08051)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Curtis Hawthorne et al. “Multi-Instrument Music Synthesis with Spectrogram
    Diffusion”, 2022 DOI: [10.48550/arXiv.2206.05408](https://dx.doi.org/10.48550/arXiv.2206.05408)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Minki Kang, Dongchan Min and Sung Ju Hwang “Grad-StyleSpeech: Any-Speaker
    Adaptive Text-to-Speech Synthesis with Diffusion Models” In *ICASSP 2023 - 2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10095515](https://dx.doi.org/10.1109/ICASSP49357.2023.10095515)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Rongjie Huang et al. “Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced
    Diffusion Models” In *Proceedings of the 40th International Conference on Machine
    Learning* PMLR, 2023, pp. 13916–13932 URL: [https://proceedings.mlr.press/v202/huang23i.html](https://proceedings.mlr.press/v202/huang23i.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Haohe Liu et al. “AudioLDM 2: Learning Holistic Audio Generation with
    Self-supervised Pretraining”, 2023 DOI: [10.48550/arXiv.2308.05734](https://dx.doi.org/10.48550/arXiv.2308.05734)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Flavio Schneider, Ojasv Kamal, Zhijing Jin and Bernhard Schölkopf “Mo\^usai:
    Text-to-Music Generation with Long-Context Latent Diffusion”, 2023 DOI: [10.48550/arXiv.2301.11757](https://dx.doi.org/10.48550/arXiv.2301.11757)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Soumi Maiti, Yifan Peng, Takaaki Saeki and Shinji Watanabe “Speechlmscore:
    Evaluating Speech Generation Using Speech Language Model” In *ICASSP 2023 - 2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10095710](https://dx.doi.org/10.1109/ICASSP49357.2023.10095710)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Sercan Arik et al. “Neural Voice Cloning with a Few Samples” In *Advances
    in Neural Information Processing Systems* 31 Curran Associates, Inc., 2018 URL:
    [https://proceedings.neurips.cc/paper_files/paper/2018/hash/4559912e7a94a9c32b09d894f2bc3c82-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2018/hash/4559912e7a94a9c32b09d894f2bc3c82-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Shawn Hershey et al. “CNN Architectures for Large-Scale Audio Classification”
    In *2017 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2017, pp. 131–135 DOI: [10.1109/ICASSP.2017.7952132](https://dx.doi.org/10.1109/ICASSP.2017.7952132)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Qiuqiang Kong et al. “PANNs: Large-Scale Pretrained Audio Neural Networks
    for Audio Pattern Recognition” In *IEEE/ACM Transactions on Audio, Speech, and
    Language Processing* 28, 2020, pp. 2880–2894 DOI: [10.1109/TASLP.2020.3030497](https://dx.doi.org/10.1109/TASLP.2020.3030497)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
