- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:06:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[1905.04149] A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1905.04149](https://ar5iv.labs.arxiv.org/html/1905.04149)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Accepted
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: October 2020
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xiang Zhang^(1,5), Lina Yao¹, Xianzhi Wang², Jessica Monaghan³, David McAlpine³,
    Yu Zhang⁴ ¹University of New South Wales, Australia
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: ²University of Technology Sydney, Australia
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: ³Macquarie university, Australia
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: ⁴Lehigh University, USA
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: ⁵Harvard University, USA [xiang_zhang@hms.harvard.edu,
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: lina.yao@unsw.edu.au, xianzhi.wang@uts.edu.au, {jessica.monaghan,david.mcalpine}@mq.edu.au,
    yuzi20@lehigh.edu](mailto:xiang_zhang@hms.harvard.edu,%20)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Brain signals refer to the biometric information collected from the human brain.
    The research on brain signals aims to discover the underlying neurological or
    physical status of the individuals by signal decoding. The emerging deep learning
    techniques have improved the study of brain signals significantly in recent years.
    In this work, we first present a taxonomy of non-invasive brain signals and the
    basics of deep learning algorithms. Then, we provide the frontiers of applying
    deep learning for non-invasive brain signals analysis, by summarizing a large
    number of recent publications. Moreover, upon the deep learning-powered brain
    signal studies, we report the potential real-world applications which benefit
    not only disabled people but also normal individuals. Finally, we discuss the
    opening challenges and future directions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '^†^†: J. Neural Eng.\ioptwocol'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brain signals measure the instinct biometric information from the human brain,
    which reflects the user’s passive or active mental state. Through precise brain
    signal decoding, we can recognize the underlying psychological and physical status
    of the user and further improve his/her life quality. Based on the signal collection,
    brain signals contain invasive signals and non-invasive signals. The former are
    acquired by electrodes deployed under the scalp while the latter are collected
    upon human scalp without electrodes being inserted. In this survey, we mainly
    consider non-invasive brain signals¹¹1Without specification, the brain signals
    mentioned in this work refer to non-invasive signals..
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 General Workflow
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1.1 General Workflow ‣ 1 Introduction ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers")
    shows the general paradigm of brain signal decoding, which receives brain signals
    and produces the user’s latent informatics. The workflow includes several key
    components: brain signal collection, signal preprocessing, feature extraction,
    classification, and data analysis. The brain signals are collected from humans
    and sent to the preprocessing component for denoising and enhancement. Then, the
    discriminating features are extracted from the processed signals and sent to the
    classifier for further analysis.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The collection methods differ from signal to signal. For example, EEG signals
    measure the voltage fluctuation resulting from ionic current within the neurons
    of the brain. Collecting EEG signals requires placing a series of electrodes on
    the scalp of the human head to record the electrical activity of the brain. Since
    the ionic current generated within the brain is measured at the scalp, obstacles
    (e.g., skull) greatly decrease the signal quality—the fidelity of the collected
    EEG signals, measured as Signal-to-Noise Ratio (SNR), is only approximately 5%
    of that of original brain signals [[1](#bib.bib1)]. The collection methods of
    more non-invasive signals can be found in Appendix [A](#A1 "Appendix A Non-invasive
    Brain Signals ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, brain signals are usually preprocessed before feature extraction
    to increase the SNR. The preprocessing component contains multiple steps such
    as signal cleaning (smoothing the noisy signals or resolving the inconsistencies),
    signal normalization (normalizing each channel of the signals along time-axis),
    signal enhancement (removing direct current), and signal reduction (presenting
    a reduced representation of the signal).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction refers to the process of extracting discriminating features
    from the input signals through domain knowledge. Traditional features are extracted
    from time-domain (e.g., variance, mean value, kurtosis), frequency-domain (e.g.,
    fast Fourier transform), and time-frequency domains (e.g., discrete wavelet transform).
    They will enrich distinguishable information regarding user intention. Feature
    extraction is highly dependent on the domain knowledge. For example, neuroscience
    knowledge is required to extract distinctive features from motor imagery EEG signals.
    Manual feature extraction is also time-consuming and difficult. Recently, deep
    learning provides a better option to automatically extract distinguishable features.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: The classification component refers to the machine learning algorithms that
    classify the extracted features into logical control signals recognizable by external
    devices. Deep learning algorithms are shown to be more powerful than traditional
    classifiers [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/de06841302b89e8bac0c8089d93e194b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Generally workflow of brain signal analysis. It is named as a Brain-Computer
    Interface if the classified signal are used to control smart equipment (dashed
    lines).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The classification results reflect the user’s psychological or physical status
    and can inspire further information analysis. This is widely used in real-world
    applications such as neurological disorder diagnosis, emotion measuring, and driving
    fatigue detection. Appropriate treatment, therapy, and precaution could be conducted
    based on the analysis results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'In specific, the system is called a Brain-Computer Interface (BCI) while the
    decoded brain signals are converted into digital commands to control the smart
    equipment and react with the user (dashed lines in Figure [1](#S1.F1 "Figure 1
    ‣ 1.1 General Workflow ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")). BCI²²2Apart from BCI, there
    are a number of similar terms to define the system that machines are directly
    controlled by human brain signals, like Brain-Machine Interface (BMI), Brain Interface
    (BI), Direct Brain Interface (DBI), Adaptive Brain Interface (ABI), and so on.
    systems interpret the human brain patterns into messages or commands to communicate
    with the outer world [[5](#bib.bib5)]. BCI is generally a closed-loop system with
    an external device (e.g., wheelchair and robotic arm), which can directly serve
    the user. In contrast, brain signal analysis doesn’t require a specific device
    as long as the analysis results can benefit society and individuals.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'In this survey, we summarize the state-of-the-art studies which adopt deep
    learning models: 1) for feature extraction only; 2) for classification only; 3)
    for both feature extraction and classification. The details will be introduced
    in Section [4](#S4 "4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers").
    Brain signal underpins many novel applications that are important to people’s
    daily life. For example, the brain signal-based user identification system, with
    high fake-resistance, allows normal people to enjoy enhanced entertainment and
    security [[6](#bib.bib6)]; for people with psychological/physical deceases or
    disabilities, brain signals enable them to control smart device such as wheelchairs,
    home appliances, and robots. We present a wide range of deep learning-based brain
    signal applications in Section [5](#S5 "5 Brain Signal-based Applications ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Why Deep Learning?
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although traditional brain signal system has made tremendous progress [[7](#bib.bib7),
    [8](#bib.bib8)], it still faces significant challenges. First, brain signals are
    easily corrupted by various biological (e.g., eye blinks, muscle artifacts, fatigue,
    and the concentration level) and environmental artifacts (e.g., noises) [[7](#bib.bib7)].
    Therefore, it is crucial to distill informative data from corrupted brain signals
    and build a robust system that works in different situations. Second, it faces
    the low SNR of non-stationary electrophysiological brain signals [[9](#bib.bib9)].
    The low SNR cannot be easily addressed by traditional preprocessing or feature
    extraction methods due to the time complexity of those method and the risk of
    information loss [[10](#bib.bib10)]. Third, feature extraction highly depends
    on human expertise in the specific domain. For example, it requires the basic
    biological knowledge to investigate sleep state through Electroencephalogram (EEG)
    signals. Human experience may help on certain aspects but fall insufficient in
    more general circumstances. An automatic feature extraction method is highly desirable.
    Moreover, most existing machine learning research focuses on static data and therefore,
    cannot classify rapidly changing brain signals accurately. For instance, the state-of-the-art
    classification accuracy for multi-class motor imagery EEG is generally below 80%
    [[11](#bib.bib11)]. It requires novel learning methods to deal with dynamical
    data streams in brain signal systems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Until now, deep learning has been applied extensively in brain signal applications
    and shown success in addressing the above challenges [[12](#bib.bib12), [13](#bib.bib13)].
    Deep learning has two advantages. First, it works directly on raw brain signals,
    thus avoiding the time-consuming preprocessing and feature extraction. Second,
    deep neural networks can capture both representative high-level features and latent
    dependencies through deep structures.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: The existing survey on brain signals in the last decade. The column
    ‘Comprehensiveness’ indicates whether the survey covers all subcategories of non-invasive
    brain signals or not. MI EEG refers to Motor Imagery EEG signals.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '| No. | Reference | Comprehensiveness | Signal | Deep Learning |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '&#124; Publication &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Time &#124;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '| Area |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| 2 | [[14](#bib.bib14)] | No | fMRI | Yes | 2018 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '&#124; Mental Disease &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Diagnosis &#124;'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '| 3 | [[11](#bib.bib11)] | Partial | EEG (MI EEG, P300) | No | 2007 | Classification
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| 4 | [[5](#bib.bib5)] | Partial | EEG (MI EEG, P300) | Partial | 2018 | Classification
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| 5 | [[15](#bib.bib15)] | Partial | EEG (ERD, P300, SSVEP, VEP, AEP) | No
    | 2007 |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| 6 | [[16](#bib.bib16)] | No | MRI, CT | Partial | 2017 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '&#124; Medical Image &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Analysis &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '| 7 | [[17](#bib.bib17)] | No | EEG |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '&#124; Yes &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| 8 | [[8](#bib.bib8)] | No | EEG | No | 2007 | Signal Processing |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| 9 | [[18](#bib.bib18)] | Partial | EEG | No | 2016 | BCI Applications |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| 10 | [[7](#bib.bib7)] | Yes |  | No | 2015 |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| 11 | [[19](#bib.bib19)] | No | EEG | Partial | 2018 |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| 12 | [[20](#bib.bib20)] | No | EEG, fMRI | No | 2015 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '&#124; Neurorehabilitation &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of Stroke &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '| 13 | [[21](#bib.bib21)] | No | MI EEG | No | 2015 |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| 14 | [[22](#bib.bib22)] | No | fMRI | No | 2014 |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| 15 | [[23](#bib.bib23)] | No | ERP (P300) | No | 2017 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '&#124; Applications &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of ERP” &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '| 16 | [[24](#bib.bib24)] | No | fMRI | Yes | 2018 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '&#124; Applications &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of fMRI &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '| 17 | [[25](#bib.bib25)] | No | ERP | No | 2017 | Classification |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| 18 | [[26](#bib.bib26)] | Partial | EEG | No | 2019 | Brain Biometrics |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| 19 | [[27](#bib.bib27)] | Partial | EEG | No | 2018 | BCI Paradigms |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| 20 | Current Study | Yes |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '&#124; EEG and the subcategories, &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; fNIRS, fMRI, MEG &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '| Yes |  |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: 1.3 Why this Survey is Necessary?
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct this survey for three reasons. First, there lacks a comprehensive
    survey on the non-invasive brain signals. Table [1](#S1.T1 "Table 1 ‣ 1.2 Why
    Deep Learning? ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers") shows a summary of the existing
    survey on brain signals. As our best knowledge, the limited existing surveys [[14](#bib.bib14),
    [24](#bib.bib24), [7](#bib.bib7), [11](#bib.bib11), [5](#bib.bib5), [8](#bib.bib8),
    [15](#bib.bib15)] only focus on partial EEG signals. For example, Lotte et al.
    [[11](#bib.bib11)] and Wang et al. [[18](#bib.bib18)] focus on general EEG without
    analyzing EEG subtypes; Cecotti et al. [[28](#bib.bib28)] focus on Event-Related
    Potentials (ERP); Haseer et al. [[29](#bib.bib29)] focus on functional near-infrared
    spectroscopy (fNIRS); Mason et al. [[15](#bib.bib15)] brief the neurological phenomenons
    like event-related desynchronization (ERD), P300, SSVEP, Visual Evoked Potentials
    (VEP), Auditory Evoked Potentials (AEP) but have not organized them systematically;
    Abdulkader et al. [[7](#bib.bib7)] present a topology of brain signals but have
    not mentioned spontaneous EEG and Rapid Serial Visual Presentation (RSVP); Lotte
    et al. [[5](#bib.bib5)] have not considered ERD and RSVP; VEP should be a subtype
    of ERP in [[8](#bib.bib8)]. Ahn et al. [[21](#bib.bib21)] review the performance
    variation in MI-EEG based BCI systems. Roy et al. [[17](#bib.bib17)] list some
    deep learning-based EEG studies but present little technical inspirations and
    have less analysis on deep learning algorithms, they also failed to investigate
    other non-invasive brain signals beyond EEG. In particular, compared to [[17](#bib.bib17)],
    this work provides a better introduction of deep learning including the basic
    concepts, algorithms, and popular models (Section [3](#S3 "3 Overview on Deep
    Learning Models ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers") and Appendix [B](#A2 "Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")). Moreover, this paper discusses
    the high-level guidelines in brain signal analysis in terms of the brain signal
    paradigms, the suitable deep learning frameworks and the promising real-world
    applications (Section [6](#S6 "6 Analysis and Guidelines ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Second, few research has investigated the association between deep learning
    ([[30](#bib.bib30), [31](#bib.bib31)]) and brain signals ([[32](#bib.bib32), [7](#bib.bib7),
    [11](#bib.bib11), [5](#bib.bib5), [8](#bib.bib8), [15](#bib.bib15)]). To the best
    of our knowledge, this paper is in the first batch of comprehensive survey on
    recent advances on deep learning-based brain signals. We also point out frontiers
    and promising directions in this area.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the existing surveys focus on specific areas or applications and lack
    an overview of broad scenarios. For example, Litjens et al. [[16](#bib.bib16)]
    summarize several deep neural network concepts aiming at medical image analysis;
    Soekadar et al. [[20](#bib.bib20)] review the BCI systems and machine learning
    methods for stroke-related motor paralysis based on Sensori-Motor Rhythms (SMR);
    Vieira et al. [[33](#bib.bib33)] investigate the application of brain signals
    on the neurological disorder and psychiatric.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，现有的调查集中于特定领域或应用，缺乏广泛场景的概述。例如，Litjens等人[[16](#bib.bib16)]总结了几个深度神经网络概念，旨在医学图像分析；Soekadar等人[[20](#bib.bib20)]回顾了基于感官运动节律（SMR）的脑机接口系统和用于中风相关运动瘫痪的机器学习方法；Vieira等人[[33](#bib.bib33)]调查了脑信号在神经疾病和精神病学中的应用。
- en: 1.4 Our Contributions
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 我们的贡献
- en: 'This survey can mainly benefit: 1) the researchers with computer science background
    who are interested in the brain signal research; 2) the biomedical/medical/neuroscience
    experts who want to adopt deep learning techniques to solve problems in basic
    science.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查主要受益于：1）对脑信号研究感兴趣的计算机科学背景研究人员；2）希望采用深度学习技术解决基础科学问题的生物医学/医学/神经科学专家。
- en: 'To our best knowledge, this survey is the first comprehensive survey of the
    recent advances and frontiers of deep learning-based brain signal analysis. To
    this end, we have summarized over 200 contributions, most of which were published
    in the last five years. We make several key contributions in this survey:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，本调查是关于基于深度学习的脑信号分析的最新进展和前沿的首次全面调查。为此，我们总结了200多项贡献，其中大多数发表于过去五年。我们在本调查中做出了一些关键贡献：
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review brain signals and deep learning techniques to help readers gain a
    comprehensive understanding of this area of research.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了脑信号和深度学习技术，帮助读者全面了解这一研究领域。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss the popular deep learning techniques and state-of-the-art models
    for brain signals, providing practical guidelines for choosing the suitable deep
    learning models given a specific subtype of signal.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了用于脑信号的流行深度学习技术和最先进的模型，提供了选择适当深度学习模型的实用指南，针对特定类型的信号。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review the applications of deep learning-based brain signal analysis and
    highlight some promising topics for future research.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了基于深度学习的脑信号分析的应用，并突出了未来研究的一些有前景的主题。
- en: 'The rest of this survey is structured as followed. Section [2](#S2 "2 Brain
    Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers") briefly introduces an taxonomy of brain signals
    in order to help the reader build a big picture in this field. Section [3](#S3
    "3 Overview on Deep Learning Models ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers") overviews the commonly used
    deep learning models to present the basic knowledge for researchers (e.g., neurological
    and biomedical scholars ) who are not familiar with deep learning. Section [4](#S4
    "4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") presents the state-of-the-art
    deep learning techniques for brain signals and Section [5](#S5 "5 Brain Signal-based
    Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers") discusses the applications related to brain signals.
    Section [6](#S6 "6 Analysis and Guidelines ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers") provides a detailed analysis
    and gives guidelines for choosing appropriate deep learning models based on the
    specific brain signal. Section [7](#S7 "7 Open Issues ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") points out the
    opening challenges and future directions. Finally, Section [8](#S8 "8 Conclusion
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") gives the concluding remarks.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 2 Brain Imaging Techniques
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5fa71297b1bb5047db5e16b541344b4f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The taxonomy of non-invasive brain signals. The dashed quadrilaterals
    (RAVP, SEP, SSAEP, and SSSEP) are not included in this survey because there is
    no existing work focusing on them involving deep learning algorithms. P300, which
    is a positive potential recorded approximately 300 ms after the onset of presented
    stimuli, is not listed in this signal tree because it is included by ERP (which
    refers to all the potentials after the presented stimuli). In this classification,
    other brain imaging technique beyond EEG (e.g., MEG and fNIRS) could also include
    visual/auditory tasks theoretically, but we omitted them since there is no existing
    work adopting deep learning on these tasks.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we present a brief introduction of typical non-invasive brain
    imaging techniques. More fundamental details about non-invasive brain signal (e.g.,
    concepts, characteristics, advantages, and drawbacks) are provided in Appendix [A](#A1
    "Appendix A Non-invasive Brain Signals ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep
    Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers")
    shows a taxonomy of non-invasive brain signals based on the signal collection
    method. Non-invasive signals divides into Electroencephalogram (EEG), Functional
    near-infrared spectroscopy (fNIRS), Functional magnetic resonance imaging (fMRI),
    and Magnetoencephalography (MEG) [[34](#bib.bib34)]. Table [2](#S2.T2 "Table 2
    ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers") summarizes the characteristics of
    various brain signals. In this survey, we mainly focus on EEG signals and its
    subcategories because they dominate the non-invasive signals. EEG monitors the
    voltage fluctuations generated by an electrical current within human neurons.
    The electrodes attached on scalp can measure various types of EEG signals, including
    spontaneous EEG [[35](#bib.bib35)] and evoked potentials (EP) [[36](#bib.bib36)].
    Depending on the scenario, spontaneous EEG further diverges into sleep EEG, motor
    imagery EEG, emotional EEG, mental disease EEG, and others. Similarly, EP divides
    into event-related potentials (ERP) [[28](#bib.bib28)] and steady-state evoked
    potentials (SSEP) [[37](#bib.bib37)] according to the frequency of external stimuli.
    Each potential contains visual-, auditory-, and somatosensory-potentials based
    on the external stimuli types.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the other non-invasive techniques, fNIRS produces functional neuroimages
    by employing near-infrared (NIR) light to measure the aggregation degree of oxygenated
    hemoglobin (Hb) and deoxygenated-hemoglobin (deoxy-Hb), both of which have higher
    absorbers of light than other head components such as skull and scalp [[38](#bib.bib38)];
    fMRI monitors brain activities by detecting the blood flow changes in brain areas
    [[14](#bib.bib14)]; MEG reflects brain activities via magnetic changes [[39](#bib.bib39)].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of non-invasive brain signals’ characteristics.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '| Signals | EEG | fNIRS | fMRI | MEG |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
- en: '| Spatial resolution | Low | Intermediate | High | Intermediate |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| Temporal resolution | High | Low | Low | High |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| Signal-to-Noise Ratio | Low | Low | Intermediate | Low |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| Portability | High | High | Low | Low |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| Cost | Low | Low | High | High |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| Characteristic | Electrical | Metabolic | Metabolic | Magnetic |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: 3 Overview on Deep Learning Models
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5eb5875c0916f2c4e288b7b31bf1c92.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Deep learning models. They can be divided into discriminative, representative,
    generative and hybrid models based on the algorithm functions. Discriminative
    models (Appendix [B.1](#A2.SS1 "B.1 Discriminative Deep Learning Models ‣ Appendix
    B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")) mainly include
    Multi-Layer Perceptron (MLP), Recurrent Neural Networks(RNN), and Convolutional
    Neural Networks (CNN). The two mainstreams of RNN are Long Short-Term Memory (LSTM)
    and Gated Recurrent Unit (GRU). Representative models (Appendix [B.2](#A2.SS2
    "B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain
    Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers")) can be divided into Authoencoder (AE), Restricted
    Boltzmann Machine (RBM), and Deep Belief Networks (DBN). D-AE denotes Deep-Autoencoder
    which refers to the Autoencoder with multiple hidden layers. Likewise, D-RBM denotes
    Deep-Restricted Boltzmann Machine with multiple hidden layers. Deep Belief Network
    can be composed of AE or RBM, therefore, we divided DBN into DBN-AE and DBN-RBM.
    Generative models (Appendix [B.3](#A2.SS3 "B.3 Generative Deep Learning Models
    ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")) that are commonly
    used in non-invasive brain signal analysis include Variational Autoencoder (VAE)
    and Generative Adversarial Networks (GAN).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary of deep learning model types'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '| Deep Learning | Input | Output | Function | Training method |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| Discriminative | Input data | Label | Feature extraction, Classification
    | Supervised |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| Representative | Input data | Representation | Feature extraction | Unsupervised
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Generative | Input data | New Sample | Generation, Reconstruction | Unsupervised
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Hybrid | Input data | – | – | – |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: 'In this section, we formally introduce the deep learning models including concepts,
    architectures, and techniques that are commonly used in the field of brain signal
    researches. Deep learning is a class of machine learning techniques that uses
    many layers of information-processing stages in hierarchical architectures for
    pattern classification and feature/representation learning [[31](#bib.bib31)].
    More detailed information about the deep learning techniques which are common-used
    in brain signal analysis can be find in Appendix [B](#A2 "Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning algorithms contain several subcategories based on the aim of
    the techniques (Figure [3](#S3.F3 "Figure 3 ‣ 3 Overview on Deep Learning Models
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers")):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discriminative deep learning models, which classify the input data into a pre-known
    label based on the adaptively learned discriminative features. Discriminative
    algorithms are able to learn distinctive features by non-linear transformation,
    and classification through probabilistic prediction³³3The classification function
    is achieved by the combination of a softmax layer and one-hot label encoding.
    The one-hot label encoding refers to encoding the label by the one-hot method,
    which is a group of bits among which the only valid combinations of values are
    those with a single high (1) bit and all the others low (0) bits. For instance,
    a set of labels 0, 1, 2, 3 can be encoded as (1, 0, 0, 0), (0, 1, 0, 0), (0, 0,
    1, 0), (0, 0, 0, 1).. Thus these algorithms can play the role of both feature
    extraction and classification (corresponding to Figure [1](#S1.F1 "Figure 1 ‣
    1.1 General Workflow ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")). Discriminative architectures
    mainly include Multi-Layer Perceptron (MLP) [[40](#bib.bib40)], Recurrent Neural
    Networks (RNN) [[41](#bib.bib41)], Convolutional Neural Networks (CNN) [[42](#bib.bib42)],
    along with their variations.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Representative deep learning models, which learn the pure and representative
    features from the input data. These algorithms only have the function of feature
    extraction (Figure [1](#S1.F1 "Figure 1 ‣ 1.1 General Workflow ‣ 1 Introduction
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers")) but cannot make classification. Commonly used deep learning
    algorithms for representation are Autoencoder (AE) [[43](#bib.bib43)], Restricted
    Boltzmann Machine (RBM) [[44](#bib.bib44)], Deep Belief Networks (DBN) [[45](#bib.bib45)],
    along with their variations.'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative deep learning models, which learn the joint probability distribution
    of the input data and the target label. In the brain signal scope, generative
    algorithms are mostly used to generate a batch of brain signals samples to enhance
    the training set. Generative models commonly used in brain signal analysis include
    Variational Autoencoder (VAE)⁴⁴4VAE is a variation of AE. However, they are working
    on different aspects. Therefore, we separately introduce AE and VAE. [[46](#bib.bib46)],
    Generative Adversarial Networks (GANs) [[47](#bib.bib47)], etc.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid deep learning models, which combine more than two deep learning models.
    For example, the typical hybrid deep learning model employs a representation algorithm
    for feature extraction and discriminative algorithms for classification.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The summary of the characteristics of each deep learning subcategories are
    listed in Table [3](#S3.T3 "Table 3 ‣ 3 Overview on Deep Learning Models ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers").
    Almost all the classification functions in neural networks are implemented by
    a softmax layer, which will not be regarded as an algorithmic component in this
    survey. For instance, a model combining a DBN and a softmax layer will still be
    regarded as a representative model instead of a hybrid model.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 4 State-of-The-Art DL Techniques for Brain Signals
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we thoroughly summarize the advanced studies on deep learning-based
    brain signals (Table [4](#S4.T4 "Table 4 ‣ 4.1.1 Spontaneous EEG ‣ 4.1 EEG ‣ 4
    State-of-The-Art DL Techniques for Brain Signals ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")). The hybrid models
    are divided into three parts: the combination of RNN and CNN, the combination
    of representative and discriminative models (denoted as ‘Repre + Discri’), and
    others hybrid models.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 EEG
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Due to the advantages of high portability and low price, EEG signals have attracted
    much attention. Most of the latest publications on non-invasive brain signals
    are related to EEG. In this section, we summarize two aspects of EEG signals:
    spontaneous EEG and evoked potentials. As implied by the name, the former are
    spontaneous and the latter requires outside stimuli.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Spontaneous EEG
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We present the deep learning models for spontaneous EEG according to the application
    scenarios as follows.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Sleep EEG. Sleep EEG is mainly used for recognizing the sleep stage and
    diagnosing sleep disorders or cultivating the healthy habit [[48](#bib.bib48),
    [49](#bib.bib49)]. According to Rechtschaffen and Kales (R&K) rules, the sleep
    stage includes wakefulness, non-REM (rapid eye movement) 1, non-REM 2, non-REM
    3, non-REM 4, and REM. The American Academy of Sleep Medicine (AASM) recommends
    segmentation of sleep in five stages: wakefulness, non-REM 1, non-REM 2, slow
    wave sleep (SWS), and REM. The non-REM 3 and non-REM 4 are combined into SWS since
    there is no clear distinction between them [[49](#bib.bib49)]. Generally, in the
    sleep stage analysis, the EEG signals are preprocessed by a filter which has various
    passband in different papers, but all notched at 50 Hz. The EEG signals are usually
    segmented into 30s windows.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: (i) Discriminative models. CNN are frequently used for sleep stage classification
    on single-channel EEG [[25](#bib.bib25), [50](#bib.bib50)]. For example, Viamala
    et al. [[51](#bib.bib51)] manually extracted the time-frequency features and achieved
    a classification accuracy of 86%. Others used RNN [[52](#bib.bib52)] and LSTM
    [[53](#bib.bib53)] based on various features from the frequency domain, correlation,
    and graph theoretical features.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: (ii) Representative models. Tan et al. [[54](#bib.bib54)] adopted a DBN-RBM
    algorithm to detect sleep spindle based on Power Spectral Density (PSD) features
    extracted from sleep EEG signals and achieved an F-1 of 92.78% on a local dataset.
    Zhang et al. [[49](#bib.bib49)] further combined DBN-RBM with three RBMs for sleep
    feature extraction.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: (iii) Hybrid models. Manzano et al. [[55](#bib.bib55)] presented a multi-view
    algorithm in order to predict sleep stage by combining CNN and MLP. The CNN was
    employed to receive the raw time-domain EEG oscillations while the MLP received
    the spectrum singles processed by the Short-Time Fourier Transform (STFT) among
    0.5-32 Hz. Fraiwan et al. [[56](#bib.bib56)] combined DBN with MLP for neonatal
    sleep state identification. Supratak et al. [[57](#bib.bib57)] proposed a model
    by combing a multi-view CNN and LSTM for automatic sleep stage scoring, in which
    the former was adopted to discover time-invariant dependencies while the latter
    (a bidirectional LSTM) was adopted the temporal features during the sleep. Dong
    et al. [[58](#bib.bib58)] proposed a hybrid deep learning model aiming at temporal
    sleep stage classification and took advantage of MLP for detecting hierarchical
    features along with LSTM for sequential information learning.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: (2) MI EEG. Deep learning models have shown the superior on the classification
    of Motor-Imagery (MI) EEG and real-motor EEG [[59](#bib.bib59), [60](#bib.bib60)].
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: (i) Discriminative models. Such models mostly use CNN to recognize MI EEG [[61](#bib.bib61)].
    Some are based on manually extracted features [[62](#bib.bib62), [63](#bib.bib63)].
    For instance, Lee et al. [[64](#bib.bib64)] and Zhang et al. [[65](#bib.bib65)]
    employed CNN and 2-D CNN, respectively, for classification; Zhang et al. [[65](#bib.bib65)]
    learned affective information from EEG signals to built a modified LSTM control
    smart home appliances. Others also used CNN for feature extraction [[66](#bib.bib66)].
    For example, Wang et al. [[67](#bib.bib67)] first used CNN to capture latent connections
    from MI-EEG signals and then applied weak classifiers to choose important features
    for the final classification; Hartmann et al. [[59](#bib.bib59)] investigated
    how CNN represented spectral features through the sequence of the MI EEG samples.
    MLP has also been applied for MI EEG recognition [[68](#bib.bib68)], which showed
    higher sensitivity to EEG phase features at earlier stages and higher sensitivity
    to EEG amplitude features at later stages.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: A summary of non-invasive brain signal studies based on deep learning
    models'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '| brain signals | Deep Learning Models |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| Discriminative Models | Representative Models | Generative Models | Hybrid
    Models |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| MLP | RNN | CNN | AE (D-AE) | RBM (D-RBM) | DBN | VAE | GAN | LSTM+CNN |
    Repre + Discri | Others |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| DBN-AE | DBN-RBM |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Non- invasive Signals | EEG | Spont- aneous EEG | Sleep EEG | [[69](#bib.bib69),
    [52](#bib.bib52)] | [[53](#bib.bib53), [52](#bib.bib52)] |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '&#124; [[51](#bib.bib51)],[[48](#bib.bib48)], &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[25](#bib.bib25), [50](#bib.bib50)], &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[70](#bib.bib70), [52](#bib.bib52)] &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | [[49](#bib.bib49), [54](#bib.bib54)] |  |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '&#124; [[57](#bib.bib57)] &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[52](#bib.bib52)] &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '| [[56](#bib.bib56)] |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '&#124; [[55](#bib.bib55)], &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[58](#bib.bib58)] &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '| MI EEG | [[71](#bib.bib71)],[[68](#bib.bib68)] |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '&#124; [[6](#bib.bib6)], &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[61](#bib.bib61), [65](#bib.bib65)] &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[64](#bib.bib64)], [[72](#bib.bib72)], &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[60](#bib.bib60)], &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[63](#bib.bib63)],[[73](#bib.bib73)], &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[59](#bib.bib59), [62](#bib.bib62)] &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[66](#bib.bib66)] &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[74](#bib.bib74), [75](#bib.bib75)] &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[76](#bib.bib76)] &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '|  | [[77](#bib.bib77)] |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '&#124; [[78](#bib.bib78), [79](#bib.bib79)], &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[80](#bib.bib80)] &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '| [[81](#bib.bib81)] |  | [[82](#bib.bib82), [10](#bib.bib10)] | [[4](#bib.bib4)],[[83](#bib.bib83)]
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '&#124; [[84](#bib.bib84), [85](#bib.bib85)], &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[67](#bib.bib67), [86](#bib.bib86)] &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[2](#bib.bib2)] &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Emotional &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EEG &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '| [[87](#bib.bib87)] | [[88](#bib.bib88)] |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '&#124; [[89](#bib.bib89)],[[90](#bib.bib90)], &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[91](#bib.bib91)], &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[92](#bib.bib92), [93](#bib.bib93)] &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[94](#bib.bib94)], &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[95](#bib.bib95)] &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[96](#bib.bib96), [97](#bib.bib97)] &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[98](#bib.bib98)] &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '| [[99](#bib.bib99)] |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '&#124; [[98](#bib.bib98)],[[99](#bib.bib99)], &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[100](#bib.bib100)],[[101](#bib.bib101)], &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[102](#bib.bib102), [103](#bib.bib103)] &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | [[104](#bib.bib104)] |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '&#124; [[105](#bib.bib105), [106](#bib.bib106)] &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[107](#bib.bib107)] &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '| [[108](#bib.bib108)] |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mental Disease &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EEG &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '| [[109](#bib.bib109)] | [[110](#bib.bib110)],[[111](#bib.bib111)] |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '&#124; [[112](#bib.bib112)],[[113](#bib.bib113)], &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[114](#bib.bib114)],[[115](#bib.bib115)], &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[116](#bib.bib116)],[[117](#bib.bib117)], &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[118](#bib.bib118), [119](#bib.bib119)] &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[120](#bib.bib120)] &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[121](#bib.bib121)],[[122](#bib.bib122)], &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[123](#bib.bib123)], &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[124](#bib.bib124)] &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '|  | [[125](#bib.bib125)] | [[126](#bib.bib126), [127](#bib.bib127)] |  |  |
    [[128](#bib.bib128)] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '&#124; [[129](#bib.bib129), [120](#bib.bib120)], &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[130](#bib.bib130), [131](#bib.bib131)] &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Data &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Augmentation &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  |  |  |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '&#124; [[132](#bib.bib132), [81](#bib.bib81)], &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[133](#bib.bib133)] &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[134](#bib.bib134)] &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| Others |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '&#124; [[135](#bib.bib135), [136](#bib.bib136)] &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[137](#bib.bib137)] &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '| [[138](#bib.bib138)] |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '&#124; [[139](#bib.bib139)], [[140](#bib.bib140)], &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[141](#bib.bib141)],[[142](#bib.bib142)], &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[138](#bib.bib138)],[[143](#bib.bib143)],[[144](#bib.bib144)], &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[145](#bib.bib145), [146](#bib.bib146)] &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[147](#bib.bib147), [147](#bib.bib147)] &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[148](#bib.bib148), [149](#bib.bib149)] &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '| [[150](#bib.bib150)],[[151](#bib.bib151)] |  | [[152](#bib.bib152)] |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '&#124; [[153](#bib.bib153)],[[152](#bib.bib152)], &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[154](#bib.bib154), [155](#bib.bib155)] &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | [[156](#bib.bib156), [147](#bib.bib147)] | [[157](#bib.bib157)],[[158](#bib.bib158),
    [159](#bib.bib159)] | [[160](#bib.bib160)] |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '|  |  | EP | ERP | VEP | [[161](#bib.bib161), [162](#bib.bib162)], | [[163](#bib.bib163),
    [134](#bib.bib134)] |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '&#124; [[163](#bib.bib163)],[[73](#bib.bib73)] &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[164](#bib.bib164), [147](#bib.bib147)], &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[147](#bib.bib147), [165](#bib.bib165)] &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[166](#bib.bib166)] &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '| [[167](#bib.bib167)] |  |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '&#124; [[168](#bib.bib168), [169](#bib.bib169)], &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[165](#bib.bib165)] &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '&#124; [[170](#bib.bib170)],[[171](#bib.bib171)], &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[172](#bib.bib172)] &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[97](#bib.bib97), [173](#bib.bib173)] &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[96](#bib.bib96)] &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| RSVP | [[174](#bib.bib174), [175](#bib.bib175)], |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '&#124; [[176](#bib.bib176)],[[177](#bib.bib177)], &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[178](#bib.bib178)],[[179](#bib.bib179)], &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[180](#bib.bib180), [181](#bib.bib181)], &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[182](#bib.bib182), [175](#bib.bib175)] &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[183](#bib.bib183), [184](#bib.bib184)] &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[185](#bib.bib185), [12](#bib.bib12)] &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | [[186](#bib.bib186)] |  |  |  |  | [[181](#bib.bib181), [175](#bib.bib175)]
    | [[12](#bib.bib12)] |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| AEP |  |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '&#124; [[187](#bib.bib187), [165](#bib.bib165)], &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[166](#bib.bib166), [188](#bib.bib188)] &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | [[165](#bib.bib165)] |  |  |  |  |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| SSEP | SSVEP | [[189](#bib.bib189)] | [[190](#bib.bib190)] |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '&#124; [[191](#bib.bib191)], [[192](#bib.bib192)], &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[190](#bib.bib190), [193](#bib.bib193)] &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[194](#bib.bib194)] &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | [[195](#bib.bib195)] | [[195](#bib.bib195)] |  |  | [[196](#bib.bib196)]
    | [[197](#bib.bib197)] |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| fNIRS |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '&#124; [[38](#bib.bib38)],[[198](#bib.bib198)], &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[199](#bib.bib199)],[[71](#bib.bib71)], &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[200](#bib.bib200)] &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '|  | [[198](#bib.bib198)] |  |  |  |  |  |  |  | [[201](#bib.bib201)] |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| fMRI | [[202](#bib.bib202), [203](#bib.bib203)] |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '&#124; [[204](#bib.bib204)],[[63](#bib.bib63)], &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[205](#bib.bib205)],[[206](#bib.bib206)], &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[117](#bib.bib117)],[[207](#bib.bib207)], &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[208](#bib.bib208), [194](#bib.bib194)] &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '| [[209](#bib.bib209)] |  | [[210](#bib.bib210)], |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '&#124; [[211](#bib.bib211)],[[210](#bib.bib210)],[[212](#bib.bib212), [213](#bib.bib213)]
    &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '&#124; [[214](#bib.bib214), [215](#bib.bib215)], &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[216](#bib.bib216), [203](#bib.bib203)] &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '|  | [[217](#bib.bib217)] |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| MEG |  |  | [[218](#bib.bib218)],[[204](#bib.bib204)] | [[219](#bib.bib219)]
    |  |  |  |  |  |  | [[220](#bib.bib220)] |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: (ii) Representative models. DBN is widely used as a basis for MI EEG classification
    for its high representative ability [[80](#bib.bib80), [79](#bib.bib79)]. For
    example, Ren et al. [[78](#bib.bib78)] applied a convolutional DBN based on RBM
    components, showing better feature representation than hand-crafted features.
    Li et al. [[77](#bib.bib77)] processed EEG signals with discrete wavelet transformation
    and then applied a DBN-AE based on denoising AE. Other models include the combination
    of AE model (for feature extraction) and a KNN classifier [[75](#bib.bib75)],
    the combination of Genetic Algorithm (for hyper-parameter tuning) and MLP (for
    classification) [[84](#bib.bib84)], the combination AE and XGBoost for multi-person
    scenarios [[76](#bib.bib76)], and the combination of LSTM and reinforcement learning
    for multi-modality signal classification [[85](#bib.bib85), [2](#bib.bib2)].
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: (iii) Hybrid models. Several studies proposed hybrid models for the recognition
    of MI EEG [[81](#bib.bib81)]. For example, Tabar et al. [[4](#bib.bib4)] extracted
    high-level representations from the time, frequency domain and location information
    of EEG signals using CNN and then used a DBN-AE with seven AEs as the classifier;
    Tan et al. [[82](#bib.bib82)] used a denoising AE for dimensional reduction, a
    multi-view CNN combined with RNN for discovering latent temporal and spatial information,
    and finally achieved an average accuracy of 72.22% on a public dataset.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '(3) Emotional EEG. The emotion of an individual can be evaluated in three aspects:
    valence, arousal, and dominance. The combination of the three aspects form emotions
    such as fear, sadness, and anger, which can be revealed by EEG signals.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: (i) Discriminative models. MLP are traditionally used [[137](#bib.bib137), [87](#bib.bib87)]
    while CNN and RNN are increasingly popular in EEG based emotion prediction [[89](#bib.bib89),
    [90](#bib.bib90)]. Typical CNN-based work in this category includes hierarchical
    CNN [[89](#bib.bib89), [92](#bib.bib92)] and augmenting the training set for CNN
    [[91](#bib.bib91)]. Li et al. [[89](#bib.bib89)] were the first to propose capturing
    the spatial dependencies among EEG channels via converting multi-channel EEG signals
    into a 2-D matrix. Besides, Talathi [[110](#bib.bib110)] used a discriminative
    deep learning model composed of GRU cells. Zhang et al. [[88](#bib.bib88)] proposed
    a spatial-temporal recurrent neural network, which employs a multi-directional
    RNN layer to discover long-range contextual cues and a bi-directional RNN layer
    to capture sequential features produced by the previous spatial RNN.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: (ii) Representative models. DBN, especially DBN-RBM, is widely used for the
    unsupervised representation ability in emotion recognition [[100](#bib.bib100),
    [106](#bib.bib106), [103](#bib.bib103)]. For instance, Xu et al. [[99](#bib.bib99),
    [101](#bib.bib101)] proposed a DBN-RBM algorithm with three RBMs and an RBM-AE
    to predict affective state; Zhao et al. [[126](#bib.bib126)] and Zheng et al.
    [[102](#bib.bib102)] cobmined DBN-RBM with SVM and Hidden Markov Model (HMM),
    respectively, addressing the same problem; Zheng et al. [[96](#bib.bib96), [97](#bib.bib97)]
    introduced a D-RBM with five hidden RBM layers to search the important frequency
    patterns and informative channels in affection recognition; Jia et al. [[98](#bib.bib98)]
    eliminated channels with high errors and then used D-RBM for affective state recognition
    based on representative features of the residual channels.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: The emotion is affected by many subjective and environmental factors (e.g.,
    gender and fatigue). Yan et al. [[95](#bib.bib95)] investigated the discrepancy
    of emotional patterns between men and women by proposing a novel model called
    Bimodal Deep AutoEncoder (BDAE) which received both EEG and eye movement features
    and shared the information in a fusion layer which connected with an SVM classifier.
    The results showed that the females have higher EEG signal diversity on the fearful
    emotion while males on sad emotion. Moreover, for women, the inter-subject differences
    in fear is more significant then other emotions [[95](#bib.bib95)]. To overcome
    the mismatched distribution among the samples collected from different subjects
    or different experimental sessions, Chai et al. [[94](#bib.bib94)] proposed an
    unsupervised domain adaptation technology which is called subspace alignment autoencoder
    (SAAE) by combing an AE and a subspace alignment solution. The proposed approach
    obtained a mean accuracy of 77.88% in person independent scenario.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: (iii) Hybrid models. One common-used hybrid model is a combination of RNN and
    MLP. For example, Alhagry et al. [[108](#bib.bib108)] employed an LSTM architecture
    for feature extraction from emotional EEG signals and the features are forwarded
    into an MLP for classification. Furthermore, Yin et al. [[107](#bib.bib107)] proposed
    a multi-view ensemble classifier to recognize individual emotions using multimodal
    physiological signals. The ensemble classifier contains several D-AEs with three
    hidden layers and a fusion structure. Each D-AE receives one physiological signal
    (e.g., EEG) and then sends the outputs of D-AE to a fusion structure which is
    composed of another D-AE. At last, an MLP classifier makes the prediction based
    on the mixed features. Kawde et al. [[105](#bib.bib105)] implemented an affect
    recognition system by combining a DBN-RBM for effective feature extraction and
    an MLP for classification.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: (4) Mental Disease EEG. A large number of researchers exploited EEG signals
    to diagnose neurological disorders, especially epileptic seizure [[109](#bib.bib109)].
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: (i) Discriminative models. The CNN is widely used in the automatic detection
    of epileptic seizure [[112](#bib.bib112), [114](#bib.bib114), [116](#bib.bib116),
    [93](#bib.bib93)]. For example, Johansen et al. [[118](#bib.bib118)] adopted CNN
    to work on the high-passed (1 Hz) EEG signals of epileptic spike and achieved
    an AUC of 94.7%. Acharya et al. [[113](#bib.bib113)] employed a CNN model with
    13 layers on depression detection, which was evaluated on a local dataset with
    30 subjects and achieved the accuracies of 93.5% and 96.0% based on the left-
    and right- hemisphere EEG signals, respectively. Morabito et al. [[115](#bib.bib115)]
    tried to exploit a CNN structure to extract suitable features of multi-channel
    EEG signals to classify Alzheimer’s Disease from the patients with Mild Cognitive
    Impairment and healthy control group. The EEG signals are filtered in bandpass
    ($0.1\sim 30$ Hz) and achieved an accuracy of around 82% for three-class classification.
    Eapid Eye Movement Behavior Disorder (RBD) may cause many mental disorder diseases
    like Parkinson’s disease (PD). Ruffini et al. [[111](#bib.bib111)] described an
    Echo State Networks (ESNs) model, a particular class of RNN, to distinguish RBD
    from healthy individuals. In some research, the discriminative model is only employed
    for feature extraction. For example, Ansari et al. [[119](#bib.bib119)] used CNN
    to extract the latent features and fed into a Random Forest classifier for the
    final seizure detection of neonatal babies. Chu et al. [[149](#bib.bib149)] combined
    CNN and a traditional classifier for schizophrenia recognition.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: (ii) Representative models. For disease detection, one commonly used method
    is adopting a representative model (e.g., DBN) followed by a softmax layer for
    classification [[127](#bib.bib127)]. Page et al. [[125](#bib.bib125)] adopted
    DBN-AE to extract informative features from seizure EEG signals. The extracted
    features were fed into a traditional logistic regression classifier for seizure
    detection. Al et al. [[131](#bib.bib131)] proposed a multi-view DBN-RBM structure
    to analyze EEG signals from depression patients. The proposed approach contains
    multiple input pathways, composed of two RBMs, while each corresponded to one
    EEG channel. All the input pathways would merge into a shared structure which
    is composed of another RBMs. Some papers would like to preprocess the EEG signals
    through dimensionality reduction methods such as PCA [[129](#bib.bib129)] while
    others prefer to directly fed the raw signals to the representative model [[122](#bib.bib122)].
    Lin et al. [[122](#bib.bib122)] proposed a sparse D-AE with three hidden layers
    to extract the representative features from epileptic EEG signals while Hosseini
    et al. [[129](#bib.bib129)] adopted a similar sparse D-AE with two hidden layers.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: (iii) Hybrid models. A popular hybrid method is a combination of RNN and CNN.
    Shah et al. [[128](#bib.bib128)] investigated the performance of CNN-LSTM on seizure
    detection after channel selection and the sensitivities range from 33% to 37%
    while false alarms ranges from 38% to 50%. Golmohammadi et al. [[130](#bib.bib130)]
    proposed a hybrid architecture for automatic interpretation of EEG by integrating
    both the temporal and spatial information. 2D and 1D CNNs capture the spatial
    features while LSTM networks capture the temporal features. The authors claimed
    a sensitivity of 30.83% and a specificity of 96.86% on the well-known TUH EEG
    seizure corpus. In the detection of early-stage Creutzfeldt-Jakob Disease (SJD),
    Morabito et al. [[123](#bib.bib123)] combined D-AE and MLP together. The EEG signals
    of SJD were first filtered by bandpass (0.5$\sim$70 Hz) and then fed into a D-AE
    with two hidden layers for feature representation. At last, the MLP classifier
    obtained the accuracy of 81$\sim$ 83% in a local dataset. Convolutional autoencoder,
    replacing the fully-connected layers in a standard AE by convolutional and de-convolutional
    layers, is applied to extract the seizure features in an unsupervised manner [[124](#bib.bib124)].
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: (5) Data augmentation. The generative models such as GAN could be used for data
    augmentation in brain signal classification [[132](#bib.bib132)]. Palazzo et al.
    [[133](#bib.bib133)] first demonstrated that the information contained in brainwaves
    are empowered to distinguish the visual object and then extracted more robust
    and distinguishable representations of EEG data using RNN. At last, they employed
    the GAN paradigm to train an image generator conditioned by the learned EEG representations,
    which could convert the EEG signals into images [[133](#bib.bib133)]. Kavasidis
    et al. [[134](#bib.bib134)] aiming at converting EEG signals into images. The
    EEG signals were collected when the subjects were observing images on a screen.
    An LSTM layer was employed to extract the latent features from the EEG signals,
    and the extracted features were regarded as the input of a GAN structure. The
    generator and the discriminator of the GAN were both composed of convolutional
    layers. The generator was supposed to generate an image based on the input EEG
    signals after the pre-training. Abdelfattach et al. [[132](#bib.bib132)] adopted
    a GAN on seizure data augmentation. The generator and discriminator are both composed
    of fully-connected layers. The authors demonstrated that GAN outperforms other
    generative models such as AE and VAE. After the augmentation, the classification
    accuracy increased dramatically from 48% to 82%.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: (6) Others. Some researches have explored a wide range of exciting topics. The
    first one is how EEG signals are affected by audio/visual stimuli. This differs
    from the potentials evoked by audio/visual stimulations because the stimuli in
    this phenomenon always exist instead of flicking in a particular frequency. Stober
    et al. [[188](#bib.bib188), [142](#bib.bib142)] claimed that the rhythm-evoked
    EEG signals are informative enough to distinguish the rhythm stimuli. The authors
    conducted an experiment where 13 participants were stimulated by 23 rhythmic stimuli,
    including 12 East African and 12 Western stimuli. For the 24-category classification,
    the proposed CNN achieved a mean accuracy of 24.4%. After that, the authors exploited
    convolutional AE for representation learning and CNN for recognition and achieved
    an accuracy of 27% for 12-class classification [[157](#bib.bib157)]. Sternin et
    al. [[148](#bib.bib148)] adopted CNN to capture discriminative features from the
    EEG oscillations to distinguish whether the subject was listening or imaging music.
    Similarly, Sarkar et al. [[165](#bib.bib165)] designed two deep learning models
    to recognize the EEG signals aroused by audio or visual stimuli. For this binary
    classification task, the proposed CNN and DBN-RBM with three RBMs achieved the
    accuracy of 91.63% and 91.75%, respectively. Furthermore, the spontaneous EEG
    could be used to distinguish the user’s mental state (logical versus emotional)
    [[172](#bib.bib172)].
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, some researchers focus on the impact on EEG of cognitive load [[138](#bib.bib138)]
    or physical workload [[221](#bib.bib221)]. Bashivan et al. [[159](#bib.bib159)]
    first extract informative features through wavelet entropy and band-specific power,
    which would be fed into a DBN-RBM for further refining. At last, an MLP is employed
    for cognitive load level recognition. The authors, in another work [[171](#bib.bib171)],
    also denoted to find the general features which are constant in inter-/intra-
    subjects scenarios under various mental load. Yin et al. [[150](#bib.bib150)]
    collected the EEG signals from different mental workload levels (e.g., high and
    low) for binary classification. The EEG signals are filtered by a low-pass filter,
    transformed to the frequency domain and be calculated the power spectral density
    (PSD). The extracted PSD features were fed into a denoising D-AE structure for
    future refining. They finally got an accuracy of 95.48%. Li et al. [[155](#bib.bib155)]
    worked on the recognition of mental fatigue level, including alert, slight fatigue,
    and severe fatigue.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, EEG based driver fatigue detection is an attractive area [[158](#bib.bib158),
    [151](#bib.bib151), [147](#bib.bib147)]. Huang et al. [[140](#bib.bib140)] designed
    a 3D CNN to predict the reaction time in drowsiness driving. This is meaningful
    to reduce traffic accident. Hajinoroozi et al. [[153](#bib.bib153)] adopted a
    DBN-RBM to handle the EEG signals which were processed by ICA. They achieved an
    accuracy of around 85% in binary classification (‘drowsy’ or ‘alert’). The strength
    of this paper is that it evaluated the DBN-RBM on three levels: time samples,
    channel epochs, and windowed samples. The experiments illustrated that the channel
    epoch level outperformed the other two levels. San et al. [[154](#bib.bib154)]
    combined deep learning models with a traditional classifier to detect driver fatigue.
    The model contains a DBN-RBM structure followed by an SVM classifier, which achieved
    the detection accuracy of 73.29%. Almogbel et al. [[145](#bib.bib145)] investigated
    the drivers’ mental state under different low workload levels. A proposed CNN
    is claimed to detect the driving workload directly based on the raw EEG signals.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: The research of the detection of eye state has shown exceeding accuracy. Narejo
    et al. [[152](#bib.bib152)] explored the detection of eye state (closed or open)
    based on EEG signals. They tried a DBN-RBM with three RBMs and a DBN-AE with three
    AEs and achieved a high accuracy of 98.9%. Reddy et al. [[136](#bib.bib136)] tried
    a simpler structure, MLP, and got a slightly lower accuracy of 97.5%.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, to make this survey more complete, we provide a brief introduction
    of Event-related desynchronization/synchronization (ERD/ERS). ERD/ERS refers to
    the phenomena that the magnitude and frequency distribution of the EEG signal
    power changes during a specific brain state [[36](#bib.bib36)]. In particular,
    ERD denotes the power decrease of ongoing EEG signals while ERS represents the
    power increase of EEG signals. This characteristic of ERD/ERS of brain signals
    can be used to detect the event which caused the EEG fluctuation. For example,
    [[222](#bib.bib222)] presents the ERD/ERS phenomena in motor cortex recorded during
    a motor-imagery task.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'ERD/ERS mainly appears in sensory, cognitive and motor procedures, which is
    not widely used in brain research due to the drawbacks like unstable accuracy
    cross subjects [[36](#bib.bib36)]. In most of the situations, the ERD/ERS is regarded
    as a specific feature of EEG powers for further analysis [[81](#bib.bib81), [4](#bib.bib4)].
    The task causes an ERD in the mu band (8-13 Hz) of EEG and an ERS in the beta
    band (13-30 Hz). In particular, the ERD/ERS were calculated as relative changes
    in power concerning baseline: $ERD/ERS=(P_{e}-P_{b})/P_{b}$, where $P_{e}$ denotes
    the signals power over one-second segment when the event occurring and $P_{b}$
    denotes the signal power in a one-second segment during baseline which is before
    the event [[71](#bib.bib71)]. Generally, the baseline refers to the rest state.
    For example, Sakhavi et al. calculated the ERD/ERS map and analyzed the different
    patterns among different tasks. The analysis demonstrated that the dynamic of
    energy should be considered because the static energy does not contains enough
    information [[86](#bib.bib86)].'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: There are several overlooked yet promising areas. Baltatzis et al. [[141](#bib.bib141)]
    adopted CNN to detect school bullying through the EEG when watching the specific
    video. They achieved 93.7% and 88.58% for binary and four-class classification.
    Khurana et al. [[223](#bib.bib223)] proposed deep dictionary learning that outperformed
    several deep learning methods. Volker et al. [[143](#bib.bib143)] evaluated the
    use of Deep CNN in flanker task, which achieved an averaging accuracy of 84.1%
    on the seen subject and 81.7 on the unseen subject. Zhang et al. [[160](#bib.bib160)]
    combined CNN and graph network to discover the latent information from the EEG
    signal.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Miranda-Correa et al. [[104](#bib.bib104)] proposed a cascaded framework by
    combing RNN and CNN to predict individuals’ affective level and personal factors
    (Big-five personality traits, mood, and social context). An experiment conducted
    by Putten et al. [[146](#bib.bib146)] attempted to identify the user’s gender
    based on their EEG signals. They employed a standard CNN algorithm and achieved
    the binary classification accuracy of 81% over a local dataset. The detection
    of emergency braking intention could help to reduce the responses time. Hernandez
    et al. [[144](#bib.bib144)] demonstrated that the driver’s EEG signals could distinguish
    braking intention and normal driving state. They combined a CNN algorithm which
    achieved the accuracy of 71.8% in binary classification. Behncke et al. [[139](#bib.bib139)]
    applied deep learning, a CNN model, in the context of robot assistive devices.
    They attempted to use CNN to improve the accuracy of decoding robot errors from
    EEG while the subject was watching the robot both during an object grasping and
    a pouring task.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Teo et al. [[135](#bib.bib135)] tried to combine the brain signal and recommender
    system, which predicted the user’s preference by EEG signals. There were sixteen
    participants took the experiments which collected the EEG signals when the subject
    was presented 60 bracelet-like objects as rotating visual stimuli (a 3D object).
    Then, an MLP algorithm was adopted to classify the user like or dislike the object.
    This exploration got the prediction accuracy of 63.99%. Some researchers have
    tried to explore a common framework which can be used for various brain signal
    paradigms. Lawhern et al. [[73](#bib.bib73)] introduced EEGNet based on a compact
    CNN and evaluated its robustness in various brain signal contexts [[73](#bib.bib73)].
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Evoked Potential
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, we introduce the latest researches on evoked potentials including ERP
    and SSEP.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: (1) ERP. In most situations, the ERP signals are analyzed through P300 phenomena.
    Meanwhile, almost all the studies on P300 are based on the scenario of ERP. Therefore,
    in this section, a majority of the P300 related publications are introduced in
    the subsection of VEP/AEP according to the scenario.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: (i) VEP. VEP is one of the most popular subcategories of ERP [[23](#bib.bib23),
    [224](#bib.bib224), [163](#bib.bib163)]. Ma et al. [[225](#bib.bib225)] worked
    on motion-onset VEP (mVEP) by extracting representative features through deep
    learning and adopted genetic algorithm combined with a multi-level sensing structure
    to compress the raw signals. The compressed signals were sent to a DBN-RBM algorithm
    to capture the more abstract high-level features. Maddula et al. [[170](#bib.bib170)]
    filtered the P300 signals with visual stimuli by a bandpass filter ($2\sim 35$
    Hz) and then fed into a proposed hybrid deep learning model for further analysis.
    The model includes a 2D CNN structure to capture the spatial features followed
    by an LSTM layer for temporal feature extraction. Liu et al. [[168](#bib.bib168)]
    combined a DBN-RBM representative model with an SVM classifier for concealed information
    test and achieved a high accuracy of 97.3% over a local dataset. Gao et al. [[167](#bib.bib167)]
    employed an AE model for feature extraction followed by an SVM classifier. In
    the experiment, each segment contains 150 points, which were divided into five
    time-steps, and each step had 30 points. This model achieved an accuracy of 88.1%
    over a local dataset. A wide range of P300 related studies is based on P300 speller
    [[173](#bib.bib173)], which allows the user to write characters. Cecotti et al.
    [[177](#bib.bib177)] tried to increase the P300 detection accuracy for more precise
    word-spelling. A new model was presented based on CNN, which including five low-level
    CNN classifiers with the different feature set, and the final high-level results
    are voted by the low-level classifiers. The highest accuracy reached 95.5% over
    the dataset II from the third BCI competition. Liu et al. [[164](#bib.bib164)]
    proposed a Batch Normalized Neural Network (BN³) which is a variant of CNN in
    P300 speller. The proposed method consists of six layers, and the batch normalization
    was operated in each batch. Kawasaki et al. [[162](#bib.bib162)] employed an MLP
    model to detect P300 segments from non-P300 segments and achieved the accuracy
    of 90.8%.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: (ii) AEP. A few works focused on the recognition of AEP. For example, Carabez
    et al. [[187](#bib.bib187)] proposed and tested 18 CNN structures to classify
    single-trial AEP signals. In the experiment, the volunteers were required to wear
    on an earphone which produces auditory stimulus designed based on the oddball
    paradigm. The experimental analysis demonstrated that the CNN frameworks, regardless
    of the number of convolutional layers, were effective to extract the temporal
    and spatial features and provided competitive results. The AEP signals are filtered
    by $0.1\sim 8$ Hz and downsampled from 256 Hz to 25 Hz. The experimental results
    showed that the downsampled data work better.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: (iii) RSVP. Among various VEP diagrams, RSVP has attracted much attention [[183](#bib.bib183)].
    In the analysis of RSVP, a number of discriminative deep learning models (e,g.,
    CNN [[177](#bib.bib177), [178](#bib.bib178), [182](#bib.bib182)] and MLP [[174](#bib.bib174)])
    has achieved a big success. A common preprocessing method used in RSVP signals
    is frequency filtering. The pass bands are generally ranged from $0.1\sim 50$
    Hz [[176](#bib.bib176), [185](#bib.bib185)]. Cecotti et al. [[12](#bib.bib12)]
    worked on the classification of ERP signals in RSVP scenario and proposed a modified
    CNN model for the detection of the specific target in RSVP. In the experiment,
    the images of faces and cars were regarded as target or non-target, respectively.
    The image presenting frequency is 2 Hz. In each session, the target probability
    was 10%. The proposed model offered an AUC of 86.1%. Hajinoroozi et al. [[179](#bib.bib179)]
    adopted a CNN model targeting the inter-subject and inter-task detection of RSVP.
    The experimental results showed that CNN worked good in cross-task but failed
    to get satisfying performance in the cross-subject scenario. Mao et al. [[175](#bib.bib175)]
    compared three different deep neural network algorithms in the prediction of whether
    the subject had seen the target or not. The MLP, CNN, and DBN models obtained
    the AUC of 81.7%, 79.6%, and 81.6%, respectively. The author also applied a CNN
    model to analyze the RSVP signals for person identification [[180](#bib.bib180)].
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: The representative deep learning models are also applied in RSVP. Vareka et
    al. [[186](#bib.bib186)] verified if deep learning performs well for single trial
    P300 classification. They conducted an RSVP experiment while the subjects were
    asked to recognize the target from non-target and distracters. Then a DBN-AE was
    implemented and compared with some non-deep learning algorithms. The DBN-AE was
    composed of five AEs while the hidden layer of the last AE only has two nodes
    which can be used for classification through softmax function. Finally, the proposed
    model achieved the accuracy of 69.2%. Manor et al. [[181](#bib.bib181)] applied
    two deep neural networks to deal with the RSVP signals after lowpass filtering
    ($0\sim 51$ Hz). Discriminative CNN achieved the accuracy of 85.06%. Meanwhile,
    the representative convolutional D-AE achieved the accuracy of 80.68%.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: (2) SSEP. Most of deep learning-based studies in SSEP area focus on SSVEP like
    [[191](#bib.bib191)]. SSVEP refers to brain oscillations evoked by the flickering
    visual stimuli, which generally produced from the parietal and occipital regions
    [[192](#bib.bib192)]. Attia et al. [[196](#bib.bib196)] aimed at finding an intermediate
    representation of SSVEP. A hybrid method combined CNN and RNN was proposed to
    capture the meaningful features from the time domain directly, which achieved
    the accuracy of 93.59%. Waytowich et al. [[192](#bib.bib192)] applied a compact
    CNN model to directly work on the raw SSVEP signals without any hand-crafted features.
    The reported cross subject mean accuracy was approximately 80%. Thomas et al.
    [[190](#bib.bib190)] first filter the raw SSVEP signals through a bandpass filter
    ($5\sim 48$ Hz) and then operated discrete FFT on consecutive 512 points. The
    processed data were classified by a CNN (69.03%) and an LSTM (66.89%) independently.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Perez et al. [[197](#bib.bib197)] adopted a representative model, a sparse AE,
    to extract the distinct features from the SSVEP from multi-frequency visual stimuli.
    The proposed model employed a softmax layer for the final classification and achieved
    the accuracy of 97.78%. Kulasingham et al. [[195](#bib.bib195)] classified SSVEP
    signals in the context of guilty knowledge test. The authors applied DBN-RBM and
    DBN-AE independently and achieved the accuracy of 86.9% and 86.01%, respectively.
    Hachem et al. [[189](#bib.bib189)] investigated the influence of fatigue on SSVEP
    through an MLP model during wheelchair navigation. The goal of this study was
    to seek the key parameters to switch between manual, semi-autonomous, and autonomous
    wheelchair command. Aznan et al. [[193](#bib.bib193)] explored the SSVEP classification,
    where the signals were collected through dry electrodes. The dry signals were
    more challenging for the lower SNR than standard EEG signals. This study applied
    a CNN discriminative model and achieved the highest accuracy of 96% over a local
    dataset.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 fNIRS
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to now, only a few of researchers paid attention on deep learning-based fNIRS.
    Naseer et al. [[38](#bib.bib38)] analyzed the difference between two mental tasks
    (mental arithmetic and rest) based on fNIRS signals. The authors manually extracted
    six features from the prefrontal cortex fNIRS and compared six different classifiers.
    The results demonstrated that the MLP with the accuracy of 96.3% outperformed
    all the traditional classifiers, including SVM, KNN, naive Bayes, etc. Huve et
    al. [[198](#bib.bib198)] classified the fNIRS signals, which were collected from
    the subjects during three mental states, including substractions, word generation,
    and rest. The employed MLP model achieved the accuracy of 66.48% based on the
    hand-crafted features (e.g., the concentration of OxyHb/DeoxyHb). After that,
    the authors study the mobile robot control through fNIRS signals and got the binary
    classification accuracy of 82% (offline) and 66% (online) [[199](#bib.bib199)].
    Chiarelli et al. [[71](#bib.bib71)] exploited the combination of fNIRS and EEG
    for left/right MI EEG classification. Sixteen features extracted from fNIRS signals
    (eight from OxyHb and eight from DeoxyHb) were fed into an MLP classifier with
    four hidden layers.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Hiroyasu et al. [[201](#bib.bib201)] attempted to detect
    the gender of the subject through their fNIRS signals. The authors employed a
    denoising D-AE with three hidden layers to extract distinctive features to be
    fed into an MLP classifier for gender detection. The model was evaluated over
    a local dataset and gained the average accuracy of 81%. In this study, the authors
    also pointed out that, compared with Positron Emission Tomography (PET) and fMRI,
    fNIRS has higher time resolution and more affordable [[201](#bib.bib201)].
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 fMRI
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, several deep learning methods have been applied to fMRI analysis,
    especially on the diagnosis of cognitive impairment [[14](#bib.bib14), [33](#bib.bib33)].
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: (1) Discriminative models. Among the discriminative models, CNN is a promising
    model to analyze fMRI [[206](#bib.bib206)]. For example, Havaei et al. built a
    segmentation approach for brain tumor based on fMRI with a novel CNN algorithm
    which can capture both the global features and the local features simultaneously
    [[205](#bib.bib205)]. The convolutional filters have different size. Thus, the
    small-size and large-size filter could exploit the local and global features,
    independently. Sarraf et al. [[226](#bib.bib226), [207](#bib.bib207)] applied
    deep CNN to recognize Alzheimer’s Disease based on fMRI and MRI data. Morenolopez
    et al. [[227](#bib.bib227)] employed a CNN model to deal with fMRI of brain tumor
    patients for three-class recognition (normal, edema, or active tumor). The model
    was evaluated over BRATS dataset and obtained the F1 score of 88%. Hosseini et
    al. [[117](#bib.bib117)] employed CNN for feature extraction. The extracted features
    were classified by SVM for the detection of an epileptic seizure.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Li et al. proposed a data completion method based on CNN. In particular,
    utilizing the information from fMRI data to complete PET, then train the classifier
    based on both fMRI and PET [[208](#bib.bib208)]. In the model, the input data
    of the proposed CNN is the fMRI patch, and the output is a PET patch. There are
    two convolutional layers with ten filters mapping the fMRI to PET. The experiments
    illustrated that the classifier trained by the combination of fMRI and PET (92.87%)
    outperformed the one trained by solo fMRI (91.92%) Moreover, Koyamada et al. used
    a nonlinear MLP to extract common features from different subjects. The model
    is evaluated over a dataset from the Human Connectome Project (HCP) [[202](#bib.bib202)].
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: (2) Representative models. A wide range of publications demonstrated the effectiveness
    of representative models in recognition of fMRI data [[213](#bib.bib213)]. Hu
    et al. [[217](#bib.bib217)] used demonstrated that deep learning outperforms other
    machine learning methods in the diagnosis of neurological disorders such as Alzheimer’s
    disease. Firstly, the fMRI images were converted to a matrix to represent the
    activity of 90 brain regions. Secondly, a correlation matrix is obtained by calculating
    the correlation between each pair of brain regions to represent the functional
    connectivity between different brain regions. Furthermore, a targeted AE is built
    to classify the correlation matrix, which is sensitive to AD. The proposed approach
    achieved an accuracy of 87.5%. Plis et al. [[211](#bib.bib211)] employed a DBN-RBM
    with three RBM components to extract the distinctive features from ICA processed
    fMRI and finally achieved an average F1 measure of above 90% over four public
    datasets. Suk et al. compared the effectiveness of DBN-RBM and DBN-AE on Alzheimer’s
    disease detection and the experimental results showed that the former obtained
    the accuracy of 95.4%, which is slightly lower than the latter (97.9%) [[210](#bib.bib210)].
    Suk et al. [[209](#bib.bib209)] applied a D-AE model to extract latent features
    from the resting-state fMRI data on the diagnosis of Mild Cognitive Impairment
    (MCI). The latent features are fed into a SVM classifier which achieved the accuracy
    of 72.58%. Ortiz et al. [[212](#bib.bib212)] proposed a multi-view DBN-RBM to
    receives the information of MRI and PET simultaneously. The learned representations
    were sent to several simple SVM classifiers which were ensembled to form a high-level
    stronger classifier by voting.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: (3) Generative models. The reconstruction of natural image (e.g., fMRI) has
    been attracted lots of attention [[215](#bib.bib215), [88](#bib.bib88), [203](#bib.bib203)].
    Seeliger et al. [[214](#bib.bib214)] proposed a deep convolutional GAN for reconstructing
    visual stimuli from fMRI, which aimed at training a generator to create an image
    similar to the visual stimuli. The generator contains four convolutional layers
    in order to convert the input fMRI to a natural image. Han et al. [[215](#bib.bib215)]
    focused on the generation of synthetic multi-sequence fMRI using GAN. The generated
    image can be used for data augmentation for better diagnostic accuracy or physician
    training to help better understand various diseases. The authors applied the existing
    Deep Convolutional GAN (DCGAN) [[228](#bib.bib228)] and Wasserstein GAN (WGAN)
    [[229](#bib.bib229)] and found that the former works better. Shen et al. [[203](#bib.bib203)]
    presented another image recovery approach by minimizing the distance between the
    real image and the image generated based on real fMRI.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 MEG
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Garg et al. [[218](#bib.bib218)] worked on the refining of MEG signals by removing
    the artifacts like eye-blinks and cardiac activity. The MEG singles were decomposed
    by ICA first and then classified by a 1-D CNN model. At last, the proposed approach
    achieved the sensitivity of 85% and specificity of 97% over a local dataset. Hasasneh
    et al. [[220](#bib.bib220)] also focused on artifacts detection (cardiac and ocular
    artifacts). The proposed approach uses CNN to capture temporal features and MLP
    to extract spatial information. Shu et al. [[219](#bib.bib219)] employed a sparse
    AE to learn the latent dependencies of MEG signals in the task of single word
    decoding. The results demonstrated that the proposed approach is advantageous
    for some subjects, although it did not produce an overall increase in decoding
    accuracy. Cichy et al. [[204](#bib.bib204)] applied a CNN model to recognize visual
    object based on MEG and fMRI signals.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 5 Brain Signal-based Applications
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning models have contributed to various of brain signal applications
    as summarized in Table [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based
    Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers"). The papers focused on signal classification without
    application background are not listed in this table. Therefore, the publication
    amounts in this table are less than in Table [4](#S4.T4 "Table 4 ‣ 4.1.1 Spontaneous
    EEG ‣ 4.1 EEG ‣ 4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers").'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Health Care
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the health care area, the deep learning-based brain signal systems mainly
    works on the detection and diagnosis of mental diseases such as sleep disorders,
    Alzheimer’s Disease, epileptic seizure, and other disorders. In the first place,
    for the sleep disorder detection, most studies are focused on the sleep stage
    detection based on sleep spontaneous EEG. In this situation, the researchers do
    not need to recruit patients with sleep disorder because the sleep EEG signals
    can be easily collected from healthy individuals. In terms of the algorithm, it
    can be observed from Table [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based
    Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers") that the DBN-RBM and CNN are widely adopted for feature
    selection and classification. Ruffini et al. [[111](#bib.bib111)] walk one step
    further by detecting the REM Behavior Disorder (RBD), which may cause neurodegenerative
    diseases such as Parkinson’s disease. They achieved an average accuracy of 85%
    in recognition of the RBD from healthy controls.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, fMRI is widely applied in the diagnosis of Alzheimer’s Disease. By
    taking advantage of the high spatial resolution of fMRI, the diagnosis achieved
    the accuracy of above 90% in several studies. Another reason that contributes
    to competitive performance is the binary classification scenario. Apart from that,
    there are several publications diagnose the AD based on spontaneous EEG [[115](#bib.bib115),
    [126](#bib.bib126)].
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Besides, the diagnosis of epileptic seizure attracted much attention. The seizure
    detection mainly based on spontaneous EEG. The popular deep learning models in
    this scenario contain the independent CNN and RNN, along with hybrid models combined
    RNN and CNN. Some models integrated the deep learning models for feature extraction
    and traditional classifier for detection [[127](#bib.bib127), [125](#bib.bib125)].
    For example, Yuan et al. [[121](#bib.bib121)] applied a D-AE in feature extraction
    followed by SVM for seizure diagnosis. Ullah et al. [[112](#bib.bib112)] adopted
    the voting for post-processing, which proposed several different CNN classifiers
    and predicted the final result by voting.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, there are a lot of other healthcare issues can be solved by brain
    signal research. The cardiac artifacts in MEG can be automatically detected by
    deep learning models[[218](#bib.bib218), [220](#bib.bib220)]. Several modified
    CNN structures are proposed to detect brain tumor based on fMRI from the public
    BRATS dataset [[205](#bib.bib205), [206](#bib.bib206)]. Researchers have demonstrated
    the effectiveness of deep learning models in the detection of a wide number of
    mental diseases such as depression [[113](#bib.bib113)], Interictal Epileptic
    Discharge (IED) [[230](#bib.bib230)], schizophrenia [[211](#bib.bib211)], Creutzfeldt-Jakob
    Disease (CJD) [[123](#bib.bib123)], and Mild Cognitive Impairment (MCI) [[209](#bib.bib209)].
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Smart Environment
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The smart environment is a promising application scenario for brain signals
    in the future. With the development of Internet of Things (IoT), an increasing
    number smart environment can be connected to brain signals. For example, the assisting
    robot can be used in smart home [[65](#bib.bib65), [2](#bib.bib2)], in which the
    robot can be controlled by brain signals of the individuals. Moreover, Behncke
    et al. [[139](#bib.bib139)] and Huve et al. [[199](#bib.bib199)] investigated
    the robot control problem based on the visual stimulated spontaneous EEG and fNIRS
    signals. The brain signal controlled exoskeleton could help the disabilities who
    damaged the motor system in sub-limb in walking and daily activities [[191](#bib.bib191)].
    In the future, the research on brain-controlled appliances may be beneficial to
    the elders or disabilities in smart home and smart hospital.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Communication
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the biggest advantages of brain signals, compared to other human-machine
    interface techniques, is that brain signal enables the patient who lost most motor
    abilities like speaking to communicate with the outer world. The deep learning
    technology improved the efficiency of brain signal based communications. One typical
    diagram which enables individual typing without any motor system is P300 speller,
    which can convert the user’s intent into text [[162](#bib.bib162)]. The powerful
    deep learning models empower the brain signal systems to recognize the P300 segment
    from the non-P300 segment while the former contains the communication information
    of the user [[166](#bib.bib166)]. In a higher level, the representative deep learning
    models can help to detect what character the user is focusing on and print it
    on the screen to chat with others [[166](#bib.bib166), [170](#bib.bib170), [164](#bib.bib164)].
    Additionally, Zhang et al. [[10](#bib.bib10)] proposed a hybrid model that combined
    RNN, CNN, and AE to extract the informative features from MI EEG to recognize
    what letter the user wants to speak.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/25a7736082517ef68571f8cff8a01610.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: (a) Brain signals
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/955382d97dccae46a0f708c662174439.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: (b) Deep learning models
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Illustration of the publications proportion for crucial brain signals
    and deep learning models.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of deep learning-based brain signal applications. The ‘local’
    dataset refers to private or not available dataset. The public datasets (along
    with download links) will be introduced in Section [5.9](#S5.SS9 "5.9 Benchmark
    Datasets ‣ 5 Brain Signal-based Applications ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers"). In the signals,
    S-EEG, MD EEG, and E-EEG separately denote sleep EEG, mental disease EEG, and
    emotional EEG. The single ‘EEG’ refers to the other subcategory of spontaneous
    EEG. In the models, RF and LR denote to random forest and logistic regression
    algorithms, respectively. In the performance column, ‘N/A’, ‘sen’, ‘spe’, ’aro’,
    ‘val’, ‘dom’, and ‘like’ denote not-found, sensitivity, specificity, arousal,
    valence, dominance, and liking, respectively. For each application scenario, the
    literature are sorted out by signal types and deep learning models.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '| Brain Signal Applications | Reference | Signals |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '&#124; Deep Learning &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Models &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Performance |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| Health Care | Sleep Quality Evaluation | Shahin et al. [[69](#bib.bib69)]
    | S-EEG | MLP |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '&#124; University &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hospital &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in Berlin &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.9 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| Biswai et al. [[52](#bib.bib52)] | S-EEG | RNN | Local | 0.8576 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| Ruffini et al. [[111](#bib.bib111)] | S-EEG | RNN | Local | 0.85 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| Vilamala et al. [[51](#bib.bib51)] | S-EEG | CNN | Sleep-EDF | 0.86 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| Tsinalis et al. [[25](#bib.bib25)] | S-EEG | CNN | Sleep-EDF | 0.82 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| Sors et al. [[50](#bib.bib50)] | S-EEG | CNN | SHHS | 0.87 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Chambon et al. [[48](#bib.bib48)] | S-EEG | Multi-view CNN | MASS session
    3 | N/A |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| Manzano et al. [[55](#bib.bib55)] | S-EEG | CNN + MLP | Sleep-EDF | 0.732
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| Fraiwan et al. [[56](#bib.bib56)] | S-EEG | DBN-AE + MLP | Local | 0.804
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| Tan et al. [[54](#bib.bib54)] | S-EEG | DBN-RBM | Local | 0.9278 (F1) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[49](#bib.bib49)] | S-EEG | DBN + voting | UCD | 0.9131 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| Fernandez et al. [[70](#bib.bib70)] | S-EEG | CNN | SHHS | 0.9 (F1) |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| Supratak et al. [[57](#bib.bib57)] | S-EEG | CNN + LSTM |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '&#124; MASS/ &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sleep-EDF &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.862/0.82 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| AD Detection | Morabito et al. [[115](#bib.bib115)] | MD EEG | CNN | Local
    | 0.82 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[126](#bib.bib126)] | MD EEG | DBN-RBM | Local | 0.92 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| Suk et al. [[210](#bib.bib210)] | fMRI |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-AE; &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DBN-RBM &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '| ADNI |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '&#124; 0.979; &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.954 &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '| Sarraf et al. [[207](#bib.bib207)] | fMRI | CNN | ADNI | 0.9685 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[208](#bib.bib208)] | fMRI | CNN + LR | ADNI | 0.9192 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| Hu et al. [[217](#bib.bib217)] | fMRI | D-AE + MLP | ADNI | 0.875 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| Ortiz et al. [[212](#bib.bib212)] | fMRI, PET |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-RBM &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + SVM &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '| ADNI | 0.9 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| Seizure Detection | Hosseini et al. [[120](#bib.bib120)] | EEG | CNN | Local
    | 0.96 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| Yuan et al. [[109](#bib.bib109)] | MD EEG | Attention-MLP | CHB-MIT | 0.9661
    |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| Tsiouris et al. [[53](#bib.bib53)] | MD EEG | LSTM | CHB-MIT | $>$0.99 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| Talathi et al. [[110](#bib.bib110)] | MD EEG | GRU | BUD | 0.996 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| Acharya et al. [[114](#bib.bib114)] | MD EEG | CNN | UBD | 0.8867 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| Schirmeister et al. [[116](#bib.bib116)] | MD EEG | CNN | TUH | 0.854 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| Hosseini et al. [[117](#bib.bib117)] | MD EEG | CNN | Local | N/A |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| Johansen et al. [[118](#bib.bib118)] | MD EEG | CNN | Local | 0.947 (AUC)
    |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| Ansari et al. [[119](#bib.bib119)] | MD EEG | CNN + RF | Local | 0.77 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| Ullah et al. [[112](#bib.bib112)] | MD EEG | CNN + voting | UBD | 0.954 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| Wen et al. [[124](#bib.bib124)] | MD EEG | AE | Local | 0.92 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| Lin et al.[[122](#bib.bib122)] | MD EEG | D-AE | UBD | 0.96 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| Yuan et al. [[121](#bib.bib121)] | MD EEG | D-AE + SVM | CHB-MIT | 0.95 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| Page et al. [[125](#bib.bib125)] | MD EEG | DBN-AE + LR | N/A | 0.8 $\sim$
    0.9 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| Turner et al. [[127](#bib.bib127)] | MD EEG |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-RBM &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + LR &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '| Local | N/A |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| Hosseini et al. [[129](#bib.bib129)] | MD EEG | D-AE + MLP | Local | 0.94
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| Golmohammadi et al. [[130](#bib.bib130)] | MD EEG | RNN+CNN | TUH |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '&#124; Sen: 0.3083; &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Spe: 0.9686 &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Shah et al. [[128](#bib.bib128)] | MD EEG | CNN+ LSTM | TUH |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '&#124; Sen: 0.39; &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Spe: 0.9037 &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers"). Summary of deep learning-based brain signal applications
    (Continued). IEF and CJD refer to Interictal Epileptic Discharge and Creutzfeldt-Jakob
    Disease, respectively.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '| Brain Signal Applications | Reference | Signals |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '&#124; Deep Learning &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Models &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Performance |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| Health Care | Others: |  |  |  |  |  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| IED | Antoniades et al. [[231](#bib.bib231)] | EEG | AE + CNN | Local | 0.68
    |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| CJD | Morabito et al. [[123](#bib.bib123)] | MD EEG | D-AE | Local | 0.81
    $\sim$ 0.83 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| Depression | Acharya et al. [[113](#bib.bib113)] | MD EEG | CNN | Local |
    0.935 $\sim$ 0.9596 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| Al et al. [[131](#bib.bib131)] | MD EEG |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-RBM &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + MLP &#124;'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '| Local | 0.695 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| Brain Tumor | Morenolopez et al. [[227](#bib.bib227)] | fMRI | CNN | BRATS
    | 0.88 (F1) |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| Shreyas et al. [[206](#bib.bib206)] | fMRI | CNN | BRATS | 0.83 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| Havaei et al. [[205](#bib.bib205)] | fMRI | Muli-scale CNN | BRATS | 0.88
    (F1) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| Schizophrenia | Plils et al. [[211](#bib.bib211)] | fMRI | DBN-RBM | Combined
    | 0.9 (F1) |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| Chu et al. [[149](#bib.bib149)] |  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN + RF &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + Voting &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '| Local | 0.816, 0.967, 0.992 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mild Cognitive &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Impairment (MCI) &#124;'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '| Suk et al. [[209](#bib.bib209)] | fMRI | AE + SVM | ADNI2 | 0.7258 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| Cardiac Detection | Garg [[218](#bib.bib218)] | MEG | CNN | Local |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '&#124; Sen: 0.85, &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Spe: 0.97 &#124;'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '| Hasasneh et al. [[220](#bib.bib220)] | MEG | CNN + MLP | Local | 0.944 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| Smart Environment | Robot Control | Behncke et al. [[139](#bib.bib139)] |
    EEG | CNN | Local | 0.75 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Smart &#124;'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Home &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhang et al. [[65](#bib.bib65)] | MI EEG | RNN | EEGMMI | 0.9553 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Exoskeleton &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Control &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '| Kwak et al. [[191](#bib.bib191)] | SSVEP | CNN | Local | 0.9403 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '|  | Huve et al. [[199](#bib.bib199)] | fNIRS | MLP | Local | 0.82 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| Communication | Zhang et al. [[10](#bib.bib10)] | MI EEG |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '&#124; LSTM+CNN &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +AE &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '| Local | 0.9452 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '| Kawasaki et al. [[162](#bib.bib162)] | VEP | MLP | Local | 0.908 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: '| Cecotti et al. [[166](#bib.bib166)] | VEP | CNN |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '&#124; The third BCI &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; competition, &#124;'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dataset II &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.945 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[164](#bib.bib164)] | VEP | CNN |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '&#124; The third BCI &#124;'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; competition, &#124;'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dataset II &#124;'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.92 $\sim$ 0.96 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| Cecotti et al. [[166](#bib.bib166)] | VEP | CNN + Voting |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '&#124; The third BCI &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; competition, &#124;'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dataset II &#124;'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.955 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| Maddula et al. [[170](#bib.bib170)] | VEP | RCNN | Local | 0.65$\sim$0.76
    |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: '| Security | Identification | Zhang et al. [[6](#bib.bib6)] | MI-EEG |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
- en: '&#124; Attention-based &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RNN &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '| EEGMMI + local | 0.9882 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '| Koike et al. [[161](#bib.bib161)] | VEP | MLP | Local | 0.976 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| Mao et al. [[180](#bib.bib180)] | RSVP | CNN | Local | 0.97 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: '| Authentication | Zhang et al. [[61](#bib.bib61)] | MI EEG | Hybrid | EEGMMI
    + local | 0.984 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
- en: '| Affective Computing | Frydenlund et al. [[87](#bib.bib87)] | E-EEG | MLP
    | DEAP | N/A |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[88](#bib.bib88)] | E-EEG | RNN | SEED | 0.895 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[201](#bib.bib201)] | E-EEG | CNN | SEED | 0.882 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[90](#bib.bib90)] | E-EEG | CNN | Local | 0.82 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[89](#bib.bib89)] | E-EEG |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
- en: '&#124; Hierarchical &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '| SEED | 0.882 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
- en: '| Chai et al. [[94](#bib.bib94)] | E-EEG | AE | SEED | 0.818 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[99](#bib.bib99)] | E-EEG |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-AE, &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DBN-RBM &#124;'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '| DEAP | $>$0.86 (F1) |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| Jia et al. [[98](#bib.bib98)] | E-EEG | DBN-RBM | DEAP |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '&#124; 0.8 $\sim$ &#124;'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.85 (AUC) &#124;'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[100](#bib.bib100)] | E-EEG | DBN-RBM | DEAP |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '&#124; Aro:0.642, &#124;'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Val:0.584, &#124;'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dom 0.658 &#124;'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '| Xu et al. [[101](#bib.bib101)] | E-EEG | DBN-RBM | DEAP |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '&#124; Aro:0.6984, &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Val:0.6688, &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lik: 0.7539 &#124;'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers"). Summary of deep learning-based brain signal applications
    (Continued).'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '| Brain Signal Applications | Reference | Signals |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
- en: '&#124; Deep Learning &#124;'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Models &#124;'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Performance |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| Affective Computing | Zheng et al. [[102](#bib.bib102)] | E-EEG |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-RBM &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + HMM &#124;'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '| Local | 0.8762 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[96](#bib.bib96), [97](#bib.bib97)] | E-EEG |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-RBM &#124;'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + MLP &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: '| SEED | 0.8608 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[106](#bib.bib106)] | E-EEG |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-RBM &#124;'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + MLP &#124;'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '| Local | 0.684 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: '| Yin et al. [[107](#bib.bib107)] | E-EEG |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
- en: '&#124; Multi-view D-AE &#124;'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + MLP &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '| DEAP |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
- en: '&#124; Aro: 0.7719; &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Val: 0.7617 &#124;'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: '| Mioranda et al. [[104](#bib.bib104)] | E-EEG | RNN + CNN | AMIGOS | ¡0.7
    |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
- en: '| Alhagry et al. [[108](#bib.bib108)] | E-EEG | LSTM + MLP | DEAP |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '&#124; Aro:0.8565, &#124;'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Val:0.8545, &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lik: 0.8799 &#124;'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[95](#bib.bib95)] |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
- en: '&#124; EEG &#124;'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '| AE |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
- en: '&#124; SEED, &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DEAP &#124;'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '| 0.9101, 0.8325 |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
- en: '| Kawde et al. [[105](#bib.bib105)] | EEG | DBN-RBM | DEAP |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
- en: '&#124; Aro: 0.7033; &#124;'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Val: 0.7828; &#124;'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dom: 0.7016 &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '| Drive Fatigue Detection | Hung et al. [[140](#bib.bib140), [140](#bib.bib140)]
    | EEG | CNN | Local | 0.572 (RMSE) |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
- en: '| Hung et al. [[140](#bib.bib140)] | EEG | CNN | Local |  |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
- en: '| Almogbel et al. [[145](#bib.bib145)] | EEG | CNN | Local | 0.9531 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
- en: '| Hajinoroozi et al. [[147](#bib.bib147), [147](#bib.bib147)] | EEG | CNN |
    Local | 0.8294 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
- en: '| Hajinoroozi et al. [[153](#bib.bib153)] | EEG | DBN-RBM | Local | 0.85 |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
- en: '| San et al. [[154](#bib.bib154)] | EEG | DBN-RBM + SVM | Local | 0.7392 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
- en: '| Chai et al. [[158](#bib.bib158)] | EEG | DBN + MLP | Local | 0.931 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
- en: '| Du et al. [[151](#bib.bib151)] |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
- en: '&#124; EEG &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: '| D-AE + SVM | Local | 0.094 (RMSE) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
- en: '| Hachem et al. [[189](#bib.bib189)] | SSVEP | MLP | Local | 0.75 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
- en: '| Mental Load Measurement | Yin et al. [[150](#bib.bib150)] | EEG | D-AE |
    Local | 0.9584 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: '| Bashivan et al. [[159](#bib.bib159)] | EEG | DBN-RBM | Local | 0.92 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[155](#bib.bib155)] | EEG | DBN-RBM | Local | 0.9886 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
- en: '| Bashivan et al. [[171](#bib.bib171)] | EEG | R-CNN | Local | 0.9111 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '| Bashivan et al. [[172](#bib.bib172)] | EEG | DBN + MLP | Local | N/A |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '| Naseer et al. [[38](#bib.bib38)] | fNIRS | MLP | Local | 0.963 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
- en: '| Hennrich et al. [[200](#bib.bib200)] | fNIRS | MLP | Local | 0.641 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
- en: '| Other Appli- -cations | School Bullying | Baltatzis et al. [[141](#bib.bib141)]
    | EEG | CNN | Local | 0.937 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
- en: '| Music Detection | Stober et al. [[142](#bib.bib142)] | EEG | CNN | Local
    | 0.776 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
- en: '| Stober et al. [[157](#bib.bib157)] | EEG | AE + CNN | Open MIIR | 0.27 for
    12-class |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
- en: '| Stober et al. [[188](#bib.bib188)] | EEG | CNN | Local | 0.244 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
- en: '| Sternin et al. [[148](#bib.bib148)] |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '&#124; EEG &#124;'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: '| CNN | Local | 0.75 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Number &#124;'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Choosing &#124;'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '| Waytowich et al. [[192](#bib.bib192)] | SSVEP | CNN | Local | 0.8 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| Visual Object Recognition | Cichy et al. [[204](#bib.bib204)] |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
- en: '&#124; fMRI, MEG &#124;'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: '| CNN | N/A | N/A |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
- en: '| Manor et al. [[176](#bib.bib176)] | RSVP | CNN | Local | 0.75 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
- en: '| Cecotti et al. [[177](#bib.bib177)] | RSVP | CNN | Local | 0.897 (AUC) |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
- en: '| Hajinoroozi et al. [[179](#bib.bib179)] | RSVP | CNN | Local | 0.7242 (AUC)
    |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
- en: '| Shamwell et al. [[185](#bib.bib185)] | RSVP | CNN | Local | 0.7252 (AUC)
    |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
- en: '| Perez et al. [[197](#bib.bib197)] | SSVEP | AE | Local | 0.9778 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Guilty &#124;'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge &#124;'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Test &#124;'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '| Kulasingham et al. [[195](#bib.bib195)] | SSVEP |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
- en: '&#124; DBN-RBM; &#124;'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DBN-AE &#124;'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: '| Local |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
- en: '&#124; 0.869; &#124;'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.8601 &#124;'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Concealed &#124;'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Information &#124;'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Test &#124;'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[168](#bib.bib168)] | EEG | DBN-RBM | Local | 0.973 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
- en: '| Flanker Task | Volker et al. [[143](#bib.bib143)] | EEG | CNN | Local | 0.841
    |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
- en: '| Eye State | Narejo et al. [[152](#bib.bib152)] | EEG | DBN-RBM | UCI | 0.989
    |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
- en: '| Reddy et al. [[136](#bib.bib136)] | EEG | MLP | Local | 0.975 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
- en: '| User Preference | Teo et al. [[135](#bib.bib135)] | EEG | MLP | Local | 0.6399
    |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Emergency &#124;'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Braking &#124;'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: '| Hernandez et al. [[144](#bib.bib144)] | EEG | CNN | Local | 0.718 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
- en: '| Gender Detection | Putten et al. [[146](#bib.bib146)] | EEG | CNN | Local
    | 0.81 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
- en: '| Hiroyasu et al. [[201](#bib.bib201)] | fNIRS | D-AE + MLP | Local | 0.81
    |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
- en: 5.4 Security
  id: totrans-610
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Brain signals can be used in security scenarios such as identification (or recognition)
    and authentication (or verification). The former conducts multi-class classification
    to recognize a person’s identity [[6](#bib.bib6)]. The latter conducts binary
    classification to decide whether a person is authorized [[61](#bib.bib61)].
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: The majority of the existing biometric identification/authentication systems
    rely on individuals’ intrinsic physiological features such as face, iris, retina,
    voice, and fingerprint [[6](#bib.bib6)]. They are vulnerable to various attacks
    based on anti-surveillance prosthetic masks, contact lenses, vocoder, and fingerprint
    films. EEG-based biometric person identification is a promising alternative given
    its highly resilient to spoofing attacks—individual’s EEG signals are virtually
    impossible for an imposter to mimic. Koike et al. [[161](#bib.bib161)] have adopted
    deep neural networks to identify the user’s ID based on the VEP signals; Mao et
    al. [[180](#bib.bib180)] applied CNN for person identification based on RSVP signals;
    Zhang et al. [[6](#bib.bib6)] proposed an attention-based LSTM model and evaluated
    it over both public and local datasets. EEG signals are also combined with gait
    information in a hybrid deep learning model for a dual-authentication system [[61](#bib.bib61)].
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Affective Computing
  id: totrans-613
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Affective states of a user provide critical information for many applications
    such as personalized information (e.g., multimedia content) retrieval or intelligent
    human-computer interface design [[99](#bib.bib99)]. Recent research illustrated
    that deep learning models can enhance the performance in affective computing.
    The most widely used circumplex model believe the emotions are distributed in
    two dimensions: arousal and valence. The arousal refers to the intensity of the
    emotional stimuli or how strong is the emotion. The valence refers to the relationship
    within the person who experiences the emotion. In some other models, the dominance
    and liking dimensions are deployed.'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: Some research [[89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)] attempts
    to classify users’ emotional state into two (positive/negative) or three categories
    (positive, neutral, and negative) based on EEG signals using deep learning algorithms
    such as CNN and its variants [[87](#bib.bib87)]. DBN-RBM is the most representative
    deep learning model to discover the concealed features from emotional spontaneous
    EEG [[99](#bib.bib99), [96](#bib.bib96)]. Xu et al. [[99](#bib.bib99)] applied
    DBN-RBM as feature extractors to classify affective states based on EEG.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: Further, some researchers aim to recognize the positive/negative state of each
    specific emotional dimension. For example, Yin et al. [[107](#bib.bib107)] employed
    an ensemble classifier of AE in order to recognize the user’s affection. Each
    AE uses three hidden layers to filter out noises and to derive stable physiological
    feature representations. The proposed model was evaluated over the benchmark,
    DEAP, and achieved the arousal of 77.19% and valence of 76.17%.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Driver Fatigue Detection
  id: totrans-617
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vehicle drivers’ ability to keep alert and maintain optimal performance will
    dramatically affect the traffic safety [[145](#bib.bib145)]. EEG signals have
    proven useful in evaluating the human’s cognitive state in different context.
    Generally, a driver is regarded as in an alert state if the reaction time is lower
    than 0.7 seconds and in fatigue state if it is higher than 2.1 seconds. Hajinoroozi
    et al. [[153](#bib.bib153)] considered the detection of driver’s fatigue from
    EEG signals by discovering the distinct features. They explored an approach based
    on DBN for dimension reduction.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: Detecting driver fatigue is crucial because the drowsiness of the driver may
    lead to disaster. Driver fatigue detection is feasible in practice. In the hardware
    aspect, the collection equipment of EEG singles is off-the-shelf and portable
    enough to be used in a car. Moreover, the price of an EEG headset is affordable
    for most people. In the algorithm aspect, deep learning models have enhanced the
    performance of fatigue detection. As we summarized, the EEG based driving drowsiness
    can be recognized with high accuracy (82% $\sim$ 95%).
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: Future scope of drive fatigue detection is in the self-driving scenario. As
    we know, in the most situation of self-driving (e.g., Automation level 3⁵⁵5https://en.wikipedia.org/wiki/Self-driving_car),
    the human driver is expected to respond appropriately to a request to intervene,
    which indicates that the driver should keep alert state. Therefore, we believe
    the application of brain signal-based drive fatigue detection will benefit the
    development of the self-driving car.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Mental Load Measurement
  id: totrans-621
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The EEG oscillations can be used to measure the mental workload level, which
    can sustain decision making and strategy development in the context of human-machine
    interaction [[150](#bib.bib150)]. Additionally, the appropriate mental workload
    is essential for maintaining human health and preventing accidents. For example,
    the abnormal mental workload of the human operator may result in performance degradation
    which could cause catastrophic accidents [[232](#bib.bib232)]. Evaluation of operator
    Mental Workload levels via ongoing EEG is quite promising in Human-Machine collaborative
    task environment to alarm the temporal operator performance degradation.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The summary of public dataset for brain signal studies. The ‘# Sub’,
    ‘# Cla’, and S-Rate denote the number of subject, number of class, and sampling
    rate, respectively. FM denote finger movement while BCI-C denote the BCI Competition.
    The ‘# channel‘ refers to the number of brain signal channels.'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: '| Brain Signals | Name Link | # Sub | # Cla | S-Rate | # Channel |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: '| EEG | Sleep EEG | Sleep-EDF⁶⁶6https://physionet.org/physiobank/database/sleep-edfx/:
    Telemetry | 22 | 6 | 100 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
- en: '&#124; 2 &#124;'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: '| Sleep-EDF: Cassette | 78 | 6 | 100, 1 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
- en: '&#124; 2 &#124;'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: '| MASS-1⁷⁷7https://massdb.herokuapp.com/en/ | 53 | 5 | 256 |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
- en: '&#124; 17 &#124;'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: '| MASS-2 | 19 | 6 | 256 |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
- en: '&#124; 19 &#124;'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: '| MASS-3 | 62 | 5 | 256 |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
- en: '&#124; 20 &#124;'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: '| MASS-4 | 40 | 6 | 256 |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
- en: '&#124; 4 &#124;'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: '| MASS-5 | 26 | 6 | 256 |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
- en: '&#124; 20 &#124;'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: '| SHHS⁸⁸footnotemark: 8 | 5804 | N/A | 125, 50 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
- en: '&#124; 2 &#124;'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: '| Seizure EEG | CHB-MIT⁹⁹footnotemark: 9 | 22 | 2 | 256 | 18 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
- en: '| TUH^(10)^(10)footnotemark: 10 | 315 | 2 | 200 | 19 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
- en: '| MI EEG | EEGMMI^(11)^(11)footnotemark: 11 | 109 | 4 | 160 | 64 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
- en: '| BCI-C II^(12)^(12)footnotemark: 12, Dataset III | 1 | 2 | 128 | 3 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
- en: '| BCI-C III, Dataset III a | 3 | 4 | 250 | 60 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
- en: '| BCI-C III, Dataset III b | 3 | 2 | 125 | 2 |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
- en: '| BCI-C III, Dataset IV a | 5 | 2 | 1000 | 118 |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
- en: '| BCI-C III, Dataset IV b | 1 | 2 | 1001 | 119 |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
- en: '| BCI-C III, Dataset IV c | 1 | 2 | 1002 | 120 |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
- en: '| BCI-C IV, Dataset I | 7 | 2 | 1000 | 64 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
- en: '| BCI-C IV, Dataset II a | 9 | 4 | 250 | 22 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
- en: '| BCI-C IV, Dataset II b | 9 | 2 | 250 | 3 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
- en: '| Emotional EEG | AMIGOS^(13)^(13)footnotemark: 13 | 40 | 4 | 128 | 14 |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
- en: '| SEED^(14)^(14)footnotemark: 14 | 15 | 3 | 200 | 62 |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
- en: '| DEAP^(15)^(15)footnotemark: 15 | 32 | 4 | 512 | 32 |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Others &#124;'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EEG &#124;'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: '| Open MIIR^(16)^(16)footnotemark: 16 | 10 | 12 | 512 | 64 |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
- en: '| VEP | BCI-C II, Dataset II b | 1 | 36 | 240 | 64 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
- en: '| BCI-C III, Dataset II | 2 | 26 | 240 | 64 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
- en: '| fMRI | ADNI^(17)^(17)footnotemark: 17 | 202 | 3 | N/A | N/A |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
- en: '| BRATS^(18)^(18)18https://physionet.org/pn3/shhpsgdb/2013 | 65 | 4 | N/A |
    N/A |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| MEG | BCI-C IV, Dataset III | 2 | 4 | 400 | 10 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '^(19)^(19)footnotetext: https://physionet.org/pn6/chbmit/^(20)^(20)footnotetext:
    https://www.isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml^(21)^(21)footnotetext:
    https://physionet.org/pn4/eegmmidb/^(22)^(22)footnotetext: http://www.bbci.de/competition/ii/^(23)^(23)footnotetext:
    http://www.eecs.qmul.ac.uk/mmv/datasets/amigos/readme.html^(24)^(24)footnotetext:
    http://bcmi.sjtu.edu.cn/ seed/download.html^(25)^(25)footnotetext: https://www.eecs.qmul.ac.uk/mmv/datasets/deap/^(26)^(26)footnotetext:
    https://owenlab.uwo.ca/research/the_openmiir_dataset.html^(27)^(27)footnotetext:
    http://adni.loni.usc.edu/data-samples/access-data/^(28)^(28)footnotetext: https://www.med.upenn.edu/sbia/brats2018/data.html'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: Several researchers have been paid attention to this topic. The mental workload
    can be measured from fNIRS signals or spontaneous EEG. Naseer et al. adopted a
    MLP algorithm for fNIRS-based binary mental task level classification (mental
    arithmetic and rest) [[38](#bib.bib38)]. The experiment results showed that the
    MLP outperformed the traditional classifiers like SVM, KNN, and achieved the highest
    accuracy of 96.3%. Bashivan et al. [[159](#bib.bib159)] presented a statistical
    approach, a DBN model, for the recognition of mental workload level based on single-trial
    EEG. Before the DBN, the authors manually extracted the wavelet entropy and band-specific
    power from three frequency bands (theta, alpha, and beta). At last, the experiments
    demonstrated the recognition of mental workload achieved an overall accuracy of
    92%. Zhang et al. [[156](#bib.bib156)] investigate the mental load measurement
    across multiple mental tasks via a recurrent-convolutional framework. The model
    simultaneously learns EEG features from the spatial, spectral, and temporal dimensions,
    which results in the accuracy of 88.9% in binary classification (high/low workload
    levels).
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 Other Applications
  id: totrans-675
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are plenty of interesting scenarios beyond the above where deep learning-based
    brain signals can apply, such as recommender system [[135](#bib.bib135)] and emergency
    braking [[144](#bib.bib144)]. One possible topic is the recognition of a visual
    object, which may be used in guilty knowledge test [[195](#bib.bib195)] and concealed
    information test [[168](#bib.bib168)]. The neurons of the participant will produce
    a pulse when he/she suddenly watch a similar object. Based on the theory, the
    visual target recognition is mainly used RSVP signals. Cecotti et al. [[177](#bib.bib177)]
    aimed to build a common model for target recognition, which can work for various
    subjects instead of a specific subject.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: Besides, researchers have investigated to distinguish the subject’s gender by
    the fNIRS [[201](#bib.bib201)] and spontaneous EEG [[146](#bib.bib146)]. Hiriyasu
    et al. [[201](#bib.bib201)] adopted deep learning to recognize the gender of the
    subject based on the cerebral blood flow. The experiment results suggested that
    the cerebral blood flow changes in different ways for male and female. Putten
    et al. [[146](#bib.bib146)] tried to discover the sex-specific information from
    the brain rhythms and adopted a CNN model to recognize the participant’s gender.
    This paper illustrated that fast beta activity (20 $\sim$25 Hz) is one of the
    most distinctive attributes.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: 5.9 Benchmark Datasets
  id: totrans-678
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have extensively explored the benchmark datasets usable for deep learning-based
    brain signals (Table [6](#S5.T6 "Table 6 ‣ 5.7 Mental Load Measurement ‣ 5 Brain
    Signal-based Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers")). We provide a bunch of public datasets
    with download links, which cover most brain signal types. In particular, BCI competition
    IV (BCI-C IV) contains five datasets via the same link. For better understanding,
    we present the number of subjects, the number of class (how many categories),
    sampling rate, and the number of channels of each dataset. In the ‘# Channel’
    column, the default channel is for EEG signals. Some datasets contain more biometric
    signals (e.g., ECG), but we only list the channels related to brain signals.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: 6 Analysis and Guidelines
  id: totrans-680
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we first analyze what is the most suitable deep learning models
    for each brain signal. Then, we summarize the popular deep learning models in
    brain signal research. At last, we investigate the brain signals in terms of application.
    We hope this survey could help our readers to select the most effective and efficient
    methods when dealing with brain signals. Please recall Table [4](#S4.T4 "Table
    4 ‣ 4.1.1 Spontaneous EEG ‣ 4.1 EEG ‣ 4 State-of-The-Art DL Techniques for Brain
    Signals ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") where we summarize the brain signals and the corresponding
    deep learning models of the state-of-the-art papers. Figure [4](#S5.F4 "Figure
    4 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") illustrated of
    the publications proportion for crucial brain signals and deep learning models.'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Brain Signal Acquisition
  id: totrans-682
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Among the non-invasive signals, the studies on EEG is far more than the sum
    of all the other brain signal paradigms (fNIRS, fMRI, and MEG). Furthermore, there
    are about 70% of the EEG papers pay attention to the spontaneous EEG (133 publications).
    For better understanding, we split the spontaneous EEG into several aspects: the
    sleep, the motor imagery, the emotional, the mental disease, the data augmentation,
    and others.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: First, the classification of the sleep EEG mainly depends on the discriminative
    and the hybrid models. Among the nineteen studies about sleep stage classification,
    there are six employed CNN and the modified CNN models independently while two
    papers adopted RNN models. There are three hybrid models built on the combination
    of CNN and RNN.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: Second, in terms of the research on MI EEG (30 publications), the independent
    CNN and CNN-based hybrid models are widely used. As for the representative models,
    DBN-RBM is often applied to capture the latent features from the MI EEG signals.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: Third, there are twenty-five publications related to spontaneous emotional EEG.
    More than half of them employed representative models (such as D-AE, D-RBM, especially
    DBN-RBM) for unsupervised feature learning. The most typical state recognition
    works recognize the user’s emotion as positive, neutral, or negative. Some researchers
    take a further step to classify the valence, and the arouse rate, which is more
    complex and challenging.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, the research on mental disease diagnosis is promising and attracting.
    The majority of the related research focuses on the detection of epileptic seizure
    and Alzheimer’s Disease. Since the detection is a binary classification problem
    which is rather easier than multi-class classification, many studies can achieve
    a high accuracy like above 90%. In this area, the standard CNN model and the D-AE
    are prevalent. One possible reason is that CNN and AE are the most well-known
    and effective deep learning models for classification and dimensionality reduction.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: Fifth, several publications pay attention to the GAN based data augmentation.
    At last, about thirty studies are investigating other spontaneous EEG such as
    driving fatigue, audio/visual stimuli impact, cognitive/mental load, and eye state
    detection. These studies extensively apply standard CNN models and variants.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, apart from spontaneous EEG, evoked potentials also attracted much
    attention. On the one hand, in ERP, VEP and the subcategory RSVP has drawn lots
    of investigations because visual stimuli, compared to other stimuli, is easier
    to be conducted and more applicable in the real world (e.g., P300 speller can
    be used for brain typing). For VEP (twenty-one publications), there are elven
    studies applied discriminative models, and six works adopted hybrid models. In
    terms of RSVP, the sole CNN dominates the algorithms. Apart from them, five papers
    focused on the analysis of AEP signals. On the other hand, among the steady-state
    related researches, only SSVEP has been studied by deep learning models. Most
    of them only applied discriminative models on the recognition of the target image.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, beyond the diverse EEG diagrams, a wide range of papers paid attention
    to fNIRS and fMRI. The fNIRS images are rarely studied by deep learning, and the
    major studies just employed the simple MLP models. We believe more attention should
    be paid to the research on fNIRS for the high portability and low cost. As for
    the fMRI, twenty-three papers proposed deep learning models to the classification.
    The CNN model is widely used for its outstanding performance in feature learning
    from images. There are also several papers interested in image reconstruction
    based on fMRI signals. One reason why fMRI is so hot is that several public datasets
    are available on the Internet, although the fMRI equipment is expensive. The MEG
    signals are mainly used in the medical area, which is insensitive to the deep
    learning algorithm. Thus, we only found very few studies on MEG. The sparse AE
    and CNN algorithms have a positive influence on the feature refining and classification
    of MEG.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Selection Criteria for Deep Learning Models
  id: totrans-691
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our investigation shows that discriminative models are most frequent in the
    summarized publications. This is reasonable at a high level because a large proportion
    of brain signal issues can be regarded as a classification problem. Another observation
    is that CNN and its variants are adopted in more than 70% of the discriminative
    models, for which we provide reasons as follows.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: First, the design of CNN is powerful enough to extract the latent discriminative
    features and spatial dependencies from the EEG signals for classification. As
    a result, CNN structures are adopted for classification in some studies while
    adopted for feature extraction in some other studies.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: Second, CNN has been achieved great success in some research areas (e.g., computer
    vision), which makes it extremely famous and feasible (public codes). Thus, the
    brain signal researchers have more chance to understand and apply CNN on their
    works.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, some brain signal diagrams (e.g., fMRI) are naturally formed as two-dimension
    images that are conducive to be processedg by CNN. Meanwhile, other 1-D signals
    (e.g., EEG) could be converted into 2-D images for further analysis by CNN. Here,
    we provide several methods converting 1-D EEG signals (with multiple channels)
    to the 2-D matrix: 1) convert each time-point^(19)^(19)19Time-point represents
    one sampling point. For example, we can have 100 time-points if the sampling rate
    is 100 Hz. to a 2-D image; 2) convert a segment into a 2-D matrix. In the first
    situation, suppose we have 32 channels, and we can collect 32 elements (each element
    corresponding to a channel) at each time-point. As described in [[89](#bib.bib89)],
    the collected 32 elements could be converted into a 2-D image based on the spatial
    position. In the second situation, suppose we have 32 channels, and the segment
    contains 100 time-points. The collected data can be arranged as a matrix with
    the shape of $[32,100]$ where each row and column refers to a specific channel
    and time-point, respectively.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, there are a lot of variants of CNN which are suitable for a wide range
    of brain signal scenarios. For example, the single-channel EEG signals can be
    processed by 1-D CNN. In terms of RNN, only about 20% of discriminative model-based
    papers adopted RNN, which is much less than we expected since RNN has demonstrated
    powerful in temporal feature learning. One possible reason for this phenomena
    is that processing a long sequence by RNN is time-consuming and the EEG signals
    are generally formed as a long sequence. For example, the sleep signals are usually
    sliced into segments with 30 seconds, which has 3000 time-points under 100 Hz
    sampling rate. For a sequence with 3000 elements, through our preliminary experiments,
    RNN takes more than 20 folds training time than CNN. Moreover, MLP is not popular
    due to its inferior effectiveness (e.g., non-linear ability) to the other algorithms
    its simple deep learning architecture.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: 'As for representative models, DBN, especially DBN-RBM, is the most popular
    model for feature extraction. DBN is widely used in brain signal for two reasons:
    1) it learns the generative parameters that reveal the relationship of variables
    in neighboring layers efficiently; 2) it makes it straightforward to calculate
    the values of latent variables in each hidden layer [[233](#bib.bib233)]. However,
    most works that employed the DBN-RBM model were published before 2016\. It can
    be inferred that the researchers prefer to use DBN for feature learning followed
    by a non-deep learning classifier before 2016; but recently, an increasing number
    of studies would like to adopt CNN or hybrid models for both feature learning
    and classification.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, generative models are rarely employed independently. The GAN- and
    VAE-based data augmentation and image reconstruction are mainly focused on fMRI
    and EEG signals. It is demonstrated that the trained classifier will achieve more
    competitive performance after data augmentation. Therefore, this is a promising
    research prospect in the future.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: Last but not the least, there are fifty-three publications proposed hybrid models
    for brain signal studies. Among them, the combinations of RNN and CNN take about
    one-fifth proportion. Since RNN and CNN are illustrated having excellent temporal
    and spatial feature extraction ability, it is natural to combine them for both
    temporal and spatial feature learning. Another type of hybrid models is the combination
    of representative and discriminative models. This is easy to understand because
    the former is employed for feature refining, and the latter is employed for classification.
    There are twenty-eight publications which almost covered all the brain signals
    proposed this type of hybrid deep learning models. The adopted representative
    models are mostly AE or DBN-RBM; at the meanwhile, the adopted discriminative
    models are mostly CNN. Apart from that, there are twelve papers proposed other
    hybrid models such as two discriminative models. For example, several studies
    proposed the combination of CNN and MLP where a CNN structure is used for extract
    spatial features and an MLP is used for classification.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Application Performance
  id: totrans-700
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to have a closer observation of the recent advances on deep learning-based
    brain signal analysis, we analyze the brain signal acquisition methods and the
    deep learning algorithms in terms of application performance. In some cases, various
    studies adopt the same deep architecture working on the same dataset but results
    in different performance, which maybe caused by the different pre-processing methods
    and hyper-parameter settings.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, the most appealing and hot field is that using brain signal analysis
    on health care area. For sleep quality evaluation, the dominate brain signals
    are spontaneous EEG which are measured while the patient is sleeping. The single
    RNN or CNN models seem have a good discriminative feature learning ability and
    lead to a comprehensive performance. Generally, most of the deep learning algorithms
    can achieve the accuracy of above 85% in the context of multiple sleep stage scenario.
    Upon this, the combined hybrid models (e.g., CNN integrates with LSTM) can only
    have incremental improvements.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: One key method to detect Alzheimer’s Disease is brain signal analysis by measuring
    the functions of specific brain regions. In detail, the diagnosis can be conducted
    by spontaneous EEG signals or fMRI images. For MD EEG, DBN is supposed to outperform
    CNN since the EEG signals contains more temporal instead of spatial information.
    As for the fMRI pictures, CNN have great advantages in the grid-arranged spatial
    information learning, which makes it obtain a very comprehensive classification
    accuracy (above 90%). As for epileptic seizure, the diagnosis are generally based
    on EEG signals. The single RNN classifier (e.g., LSTM or GRU) seems work better
    than its counterparts due to the excellent temporal dependency representing ability.
    Here, the complex hybrid models indeed outperform the single component. For example,
    [[130](#bib.bib130)] achieves a better specification than [[116](#bib.bib116)]
    on the same dataset because of combing with RNN. Most of the epileptic seizure
    detection models claim a rather high classification accuracy (above 95%). One
    possible reason is that the binary recognition scenario is much easier than multi-class
    classification.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: The brain signal-controlled smart environment only appear in a small number
    of publications. Among them, the brain signals are collected through very different
    methods. This is an emerging but promising field because it is easy to integrate
    with smart home and smart hospital to benefit the individuals whether healthy
    or disable. Another advantage of brain signals is bridging people’s inside and
    outer world by communication techniques. In this area, lots of investigations
    are focusing on the VEP signals because the visual evoked potential is obvious
    and easy to be detected. One important data source is from the third BCI competition.
    In addition, brain signal analysis can be widely implement in security systems
    since the brain signals are invisible and very hard to be mimicked. The characteristic
    of high fake-resistance enables brain signal a raising star in the identification/authentication
    in confidential scenarios. The drawbacks of brain signal-based security systems
    are the expensive equipment and inconvenient (e.g., the subject have to wear an
    EEG headset to monitor the brainwaves).
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: Affective computing has drawn much attention in recent years. The EEG signals
    have high temporal resolution and able to capture the quick-varying emotions.
    Therefore, almost all the studies are based on spontaneous EEG signals. The signals
    are gathered when the subject is watching video which is supposed to arouse the
    subject’s specific emotion. Another reason for this phenomenon is that there are
    several open-source EEG-based affecting analysis datasets (e.g., DEAP and SEED)
    which greatly promote the investigation in this area. The EEG-based affective
    computing contains two mainstreams. One of them focuses on developing powerful
    discriminative classifiers (such as hierarchical CNN) which are designed to perform
    feature extraction and classification in the same step. The other tries to learn
    the latent features through deep representative models (e.g., DBN-RBM) and then
    send the learned representations into a powerful classifier (such as HMM and MLP).
    It can be observed that the former models ([[88](#bib.bib88), [201](#bib.bib201)])
    seem outperform the latter methods ([[96](#bib.bib96)]) with a small margin on
    the SEED dataset.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: Drive fatigue detection can be easily integrated in the platforms such as self-driving
    vehicles. Nevertheless, there are only a few publications in this area due to
    the expensive experimental cost and the lack of accessible dataset. Moreover,
    there are a lot of interesting applications (e.g., guilty knowledge test and gender
    detection) have been explored by deep learning models.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: 7 Open Issues
  id: totrans-707
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although deep learning has lifted the performance of brain signal systems, technical
    and usability challenges remain. The technical challenges concern the classification
    ability in complex scenarios, and the usability challenges refer to limitations
    in large scale real-world deployment. In this section, we introduce these challenges
    and point out the possible solutions.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Explainable General Framework
  id: totrans-709
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Until now, we have introduced several types of brain signals (e.g., spontaneous
    EEG, ERP, fMRI) and deep learning models that have been applied for each type.
    One promising research direction for deep learning-based brain signal research
    is to develop a general framework that can handle various brain signals regardless
    of the number of channels used for signal collection, the sample dimensions (e.g.,
    1-D or 2-D sample), and stimulation types (e.g., visual or audio stimuli), etc.
    The general framework would require two key capabilities: the attention mechanism
    and the ability to capture latent feature. The former guarantees the framework
    can focus on the most valuable parts of input signals, and the latter enables
    the framework to capture the distinctive and informative features.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism can be implemented based on attention scores or by various
    machine learning algorithms such as reinforcement learning. The attention scores
    can be inferred from the input data and work as a weight to help the framework
    to pay attention to the parts with high attention scores. Reinforcement learning
    has shown to be able to find the most valuable part through a policy search [[85](#bib.bib85)].
    CNN is the most suitable structure for capturing features at various levels and
    ranges. In the future, CNN could be used as a fundamental feature learning tool
    and be integrated with suitable attention mechanisms to form a general classification
    framework.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: One additional direction we may consider is how to interpret the feature representation
    derived by the deep neural network, what is the intrinsic relationship between
    the learned features and the task-related neural pattern, or neuropathology of
    mental disorders. More and more people are realizing that interpretation could
    be even more important than prediction performance, since we usually just treat
    deep learning as a black box.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Subject-Independent Classification
  id: totrans-713
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Until now, most brain signal classification tasks focus on person-dependent
    scenarios, where the training samples and testing samples are collected from the
    identical individual. The future direction is to realize person-independent classification
    so that the testing data will never appear in the training set. High-performance
    person-independent classification is compulsory for the wide application of brain
    signals in the real world.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible solution to achieving this goal is to build a personalized model
    with transfer learning. A personalized affective model can adopt a transductive
    parameter transfer approach to construct individual classifiers and to learn a
    regression function that maps the relationship between data distribution and classifier
    parameters [[234](#bib.bib234)]. Another potential solution is mining the subject-independent
    component from the input data. The input data can be decomposed into two parts:
    a subject-dependent component, which depends on the subject and a subject-independent
    component, which is common for all subjects. A hybrid multi-task model can work
    on two tasks simultaneously, one focusing on person identification and the other
    on class recognition. A well-trained and converged model is supposed to extract
    the subject-independent features in the class recognition task.'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Semi-supervised and Unsupervised Classification
  id: totrans-716
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The performance of deep learning highly depends on the size of training data,
    which, however, requires expensive and time-consuming manual labeling to collect
    abundant class labels for a wide range of scenarios such as sleep EEG. While supervised
    learning requires both observations and labels for the training, unsupervised
    learning requires no labels, and semi-supervised learning only requires partial
    labels [[98](#bib.bib98)]. They are, therefore, more suitable for problems with
    little ground truth.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. proposed an Adversarial Variational Embedding (AVAE) framework
    that combines a VAE++ model (as a high-quality generative model) and semi-supervised
    GAN (as a posterior distribution learner) [[235](#bib.bib235)] for robust and
    effective semi-supervised learning. Jia et al. proposed a semi-supervised framework
    by leveraging the data distribution of unlabelled data to prompt the representation
    learning of labelled data [[98](#bib.bib98)].
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: 'Two methods may enhance the unsupervised learning: one is to employ crowd-sourcing
    to label the unlabeled observations; the other is to leverage unsupervised domain
    adaption learning to align the distribution of source brain signals and the distribution
    of target signals with a linear transformation.'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Online Implementation
  id: totrans-720
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the existing brain signal systems focus on offline procedure which means
    that the training and testing dataset are pre-collected and evaluated offline.
    However, in the real-world scenarios, the brain signal systems are supposed to
    receive live data stream and produce classification results in real time, which
    is still very challenging.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: For EEG signals, in the online system, compared to the offline procedure, the
    gathered live signals are more noisy and unstable due to lots of factors such
    as the less-concentrating of the subject [[236](#bib.bib236)] and the inherent
    destabilization of the equipment (e.g., fluctuating sampling rate). Through our
    empirical experiments, online brain signal systems generally perform a lower accuracy
    of 10% than their counterparts. One future scope of online implementation is to
    develop a batch of robust algorithms in order to handle the influence factors
    and discover the latent distinctive patterns underlying the noisy live brain signals.
    [[237](#bib.bib237)] implemented an EEG-based online system that achieves comparable
    performance, however, this work only investigates a very high-level target (i.e.,
    human attention). Discovering the latent invariant representations through covariance
    matrices of EEG signals can help to mitigate the influence of extinct perturbations
    [[238](#bib.bib238)]. Some post-processing methods (e.g., voting and aggregating)
    [[166](#bib.bib166), [149](#bib.bib149)] can help to improve the decoding performance
    by averaging the results from multiple continues samples. However, these methods
    will inevitably bring higher latency. Thus, the post-processing requires a trade-off
    between the high-accuracy and low-latency.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: For fNIRS and fMRI, the online evaluation is relatively less challenging since
    they have a rather low temporal resolution. The online images with less dynamic
    can be regarded as static images to some extent, which makes the online system
    approximating to the offline system. Furthermore, most fMRI and MEG signals are
    used to evaluate the user’s neurological status (e.g., detect the effects of tumor)
    which does not require an instantaneous response. Thus, they have less demand
    for a real-time monitoring system.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Hardware Portability
  id: totrans-724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Poor portability of hardware has been preventing brain signals from wide application
    in the real world. In most scenarios, users would like to use small, comfortable,
    or even wearable brain signal hardware to collect brain signals and to control
    appliances and assistant robots.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, there are three types of EEG collection equipment: the unportable,
    the portable headset, and ear-EEG sensors. The unportable equipment has high sampling
    frequency, channel numbers, and signal quality but is expensive. It is suitable
    for physical examination in a hospital. The portable headsets (e.g., Neurosky,
    Emotiv EPOC) have 1 $\sim$ 14 channels and 128$\sim$ 256 sampling rate but has
    inaccuracy readings and cause discomfort after long-time use. The ear-EEG sensors,
    which are attached to the outer eat, have gained increasing attention recently
    but remain mostly at the laboratory stage [[239](#bib.bib239)]. The ear-EEG sensors
    contain a series of electrodes which are placed in each ear canal and concha [[240](#bib.bib240)].
    The EEGrids, to the best of our knowledge, is the only commercial ear-EEG. It
    has multi-channel sensor arrays placed around the ear using an adhesive ^(20)^(20)20http://ceegrid.com/home/concept/
    and is even more expensive. A promising future direction is to improve the usability
    by developing a cheaper (e.g., lower than 200$) and more comfortable (e.g., can
    last longer than 3 hours without feeling uncomfortable) wireless ear-EEG equipment.'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  id: totrans-727
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we thoroughly summarize the recent advances in deep learning
    models for non-invasive brain signal analysis. Compared with traditional machine
    learning methods, deep learning not only enables to learn high-level features
    automatically from brain signals but also have less dependency on domain knowledge.
    We organize brain signals and dominant deep learning models, followed by discussing
    state-of-the-art deep learning techniques for brain signals. Moreover, we provide
    guidelines to help researchers to find the suitable deep learning algorithms for
    each category of brain signals. Finally, we overview deep learning-based brain
    signal applications and point out the open challenges and future directions.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-729
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: References
  id: totrans-730
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage, “Signal
    quality of simultaneously recorded invasive and non-invasive eeg,” *Neuroimage*,
    vol. 46, no. 3, pp. 708–716, 2009.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] X. Zhang, L. Yao, S. Zhang, S. Kanhere, M. Sheng, and Y. Liu, “Internet
    of things meets brain-computer interface: A unified deep learning framework for
    enabling human-thing cognitive interactivity,” *IEEE Internet of Things Journal*,
    2018.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] X. An, D. Kuang, X. Guo, Y. Zhao, and L. He, “A deep learning method for
    classification of eeg data based on motor imagery,” in *International Conference
    on Intelligent Computing*, 2014, pp. 203–210.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y. R. Tabar and U. Halici, “A novel deep learning approach for classification
    of eeg motor imagery signals,” *Journal of neural engineering*, vol. 14, no. 1,
    p. 016003, 2016.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] F. Lotte, L. Bougrain, A. Cichocki, M. Clerc, M. Congedo, A. Rakotomamonjy,
    and F. Yger, “A review of classification algorithms for eeg-based brain–computer
    interfaces: a 10 year update,” *Journal of neural engineering*, vol. 15, no. 3,
    p. 031005, 2018.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] X. Zhang, L. Yao, S. S. Kanhere, Y. Liu, T. Gu, and K. Chen, “Mindid: Person
    identification from brain waves through attention-based recurrent neural network,”
    *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    vol. 2, no. 3, p. 149, 2018.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. N. Abdulkader, A. Atia, and M.-S. M. Mostafa, “Brain computer interfacing:
    Applications and challenges,” *Egyptian Informatics Journal*, vol. 16, no. 2,
    pp. 213–230, 2015.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Bashashati, M. Fatourechi, R. K. Ward, and G. E. Birch, “A survey of
    signal processing algorithms in brain–computer interfaces based on electrical
    brain signals,” *Journal of Neural engineering*, vol. 4, no. 2, p. R32, 2007.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] W. Samek, K.-R. Müller, M. Kawanabe, and C. Vidaurre, “Brain-computer interfacing
    in discriminative and stationary subspaces,” in *Engineering in Medicine and Biology
    Society (EMBC), 2012 Annual International Conference of the IEEE*, 2012, pp. 2873–2876.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] X. Zhang, L. Yao, Q. Z. Sheng, S. S. Kanhere, T. Gu, and D. Zhang, “Converting
    your thoughts to texts: Enabling brain typing via deep feature learning of eeg
    signals,” in *2018 IEEE International Conference on Pervasive Computing and Communications
    (PerCom)*, 2018, pp. 1–10.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] F. Lotte, M. Congedo, A. Lécuyer, F. Lamarche, and B. Arnaldi, “A review
    of classification algorithms for eeg-based brain–computer interfaces,” *Journal
    of neural engineering*, vol. 4, no. 2, p. R1, 2007.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] H. Cecotti, M. P. Eckstein, and B. Giesbrecht, “Single-trial classification
    of event-related potentials in rapid serial visual presentation tasks using supervised
    spatial filtering,” *IEEE transactions on neural networks and learning systems*,
    vol. 25, no. 11, pp. 2030–2042, 2014.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Mahmud, M. S. Kaiser, A. Hussain, and S. Vassanelli, “Applications
    of deep learning and reinforcement learning to biological data,” *IEEE transactions
    on neural networks and learning systems*, vol. 29, no. 6, pp. 2063–2079, 2018.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. Wen, Z. Wei, Y. Zhou, G. Li, X. Zhang, and W. Han, “Deep learning methods
    to process fmri data and their application in the diagnosis of cognitive impairment:
    A brief overview and our opinion,” *Frontiers in neuroinformatics*, vol. 12, p. 23,
    2018.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Mason, A. Bashashati, M. Fatourechi, K. Navarro, and G. Birch, “A comprehensive
    survey of brain interface technology designs,” *Annals of biomedical engineering*,
    vol. 35, no. 2, pp. 137–169, 2007.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical image analysis*, vol. 42, pp. 60–88, 2017.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Roy, H. Banville, I. Albuquerque, A. Gramfort, T. H. Falk, and J. Faubert,
    “Deep learning-based electroencephalography analysis: a systematic review,” *Journal
    of neural engineering*, vol. 16, no. 5, p. 051001, 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Wang, G. Gong, N. Li, and Y. Ma, “A survey of the bci and its application
    prospect,” in *Theory, Methodology, Tools and Applications for Modeling and Simulation
    of Complex Systems*, 2016, pp. 102–111.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] F. Movahedi, J. L. Coyle, and E. Sejdić, “Deep belief networks for electroencephalography:
    A review of recent contributions and future outlooks,” *IEEE journal of biomedical
    and health informatics*, vol. 22, no. 3, pp. 642–652, 2018.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. R. Soekadar, N. Birbaumer, M. W. Slutzky, and L. G. Cohen, “Brain–machine
    interfaces in neurorehabilitation of stroke,” *Neurobiology of disease*, vol. 83,
    pp. 172–179, 2015.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. Ahn and S. C. Jun, “Performance variation in motor imagery brain–computer
    interface: a brief review,” *Journal of neuroscience methods*, vol. 243, pp. 103–110,
    2015.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] S. Ruiz, K. Buyukturkoglu, M. Rana, N. Birbaumer, and R. Sitaram, “Real-time
    fmri brain computer interfaces: self-regulation of single brain regions to networks,”
    *Biological psychology*, vol. 95, pp. 4–20, 2014.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Haider and R. Fazel-Rezai, “Application of p300 event-related potential
    in brain-computer interface,” in *Event-Related Potentials and Evoked Potentials*,
    2017.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Liu, Y. Pan, M. Li, Z. Chen, L. Tang, C. Lu, and J. Wang, “Applications
    of deep learning to mri images: a survey,” *Big Data Mining and Analytics*, vol. 1,
    no. 1, pp. 1–18, 2018.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] O. Tsinalis, P. M. Matthews, Y. Guo, and S. Zafeiriou, “Automatic sleep
    stage scoring with single-channel eeg using convolutional neural networks,” *arXiv
    preprint arXiv:1610.01683*, 2016.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Q. Gui, M. Ruiz-blondet, S. Laszlo, and Z. Jin, “A survey on brain biometrics,”
    *ACM Computing Surveys*, vol. 51, no. 112, 2019.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] R. Abiri, S. Borhani, E. W. Sellers, Y. Jiang, and X. Zhao, “A comprehensive
    review of eeg-based brain–computer interface paradigms,” *Journal of neural engineering*,
    vol. 16, no. 1, p. 011001, 2019.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] H. Cecotti and A. J. Ries, “Best practice for single-trial detection of
    event-related potentials: Application to brain-computer interfaces,” *International
    Journal of Psychophysiology*, vol. 111, pp. 156–169, 2017.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] N. Naseer and K.-S. Hong, “fnirs-based brain-computer interfaces: a review,”
    *Frontiers in human neuroscience*, vol. 9, p. 3, 2015.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] L. Deng, “A tutorial survey of architectures, algorithms, and applications
    for deep learning,” *APSIPA Transactions on Signal and Information Processing*,
    vol. 3, 2014.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M. Fatourechi, A. Bashashati, R. K. Ward, and G. E. Birch, “Emg and eog
    artifacts in brain computer interface systems: A survey,” *Clinical neurophysiology*,
    vol. 118, no. 3, pp. 480–494, 2007.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. Vieira, W. H. Pinaya, and A. Mechelli, “Using deep learning to investigate
    the neuroimaging correlates of psychiatric and neurological disorders: Methods
    and applications,” *Neuroscience & Biobehavioral Reviews*, vol. 74, pp. 58–75,
    2017.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] P. Aricò, G. Borghini, G. Di Flumeri, N. Sciaraffa, and F. Babiloni, “Passive
    bci beyond the lab: current trends and future directions,” *Physiological measurement*,
    vol. 39, no. 8, p. 08TR02, 2018.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] G. Pfurtscheller and C. Neuper, “Motor imagery and direct brain-computer
    communication,” *Proceedings of the IEEE*, vol. 89, no. 7, pp. 1123–1134, 2001.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] D. Huang, K. Qian, D.-Y. Fei, W. Jia, X. Chen, and O. Bai, “Electroencephalography
    (eeg)-based brain–computer interface (bci): A 2-d virtual wheelchair control based
    on event-related desynchronization/synchronization and state control,” *IEEE Transactions
    on Neural Systems and Rehabilitation Engineering*, vol. 20, no. 3, pp. 379–388,
    2012.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] D. Regan, “Steady-state evoked potentials,” *JOSA*, vol. 67, no. 11, pp.
    1475–1489, 1977.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] N. Naseer, N. K. Qureshi, F. M. Noori, and K.-S. Hong, “Analysis of different
    classification techniques for two-class functional near-infrared spectroscopy-based
    brain-computer interface,” *Computational intelligence and neuroscience*, vol.
    2016, 2016.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Singh, S. Jain, T. Ahuja, Y. Sharma, and N. Pathak, “Study for reduction
    of pollution level in diesel engines, petrol engines and generator sets by bio
    signal ring,” *International Journal of Advance Research and Innovation*, vol. 6,
    no. 3, pp. 175–181, 2018.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. K. Pal and S. Mitra, “Multilayer perceptron, fuzzy sets, and classification,”
    *IEEE Transactions on neural networks*, vol. 3, no. 5, pp. 683–697, 1992.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, and S. Khudanpur, “Recurrent
    neural network based language model,” in *Eleventh annual conference of the international
    speech communication association*, 2010.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran, “Deep
    convolutional neural networks for lvcsr,” in *2013 IEEE international conference
    on acoustics, speech and signal processing*, 2013, pp. 8614–8618.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] M. A. Kramer, “Nonlinear principal component analysis using autoassociative
    neural networks,” *AIChE journal*, vol. 37, no. 2, pp. 233–243, 1991.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
    data with neural networks,” *science*, vol. 313, no. 5786, pp. 504–507, 2006.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] G. E. Hinton, “Deep belief networks,” *Scholarpedia*, vol. 4, no. 5, p.
    5947, 2009.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] S. Chambon, M. N. Galtier, P. J. Arnal, G. Wainrib, and A. Gramfort, “A
    deep learning architecture for temporal sleep stage classification using multivariate
    and multimodal time series,” *IEEE Transactions on Neural Systems and Rehabilitation
    Engineering*, 2018.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Zhang, Y. Wu, J. Bai, and F. Chen, “Automatic sleep stage classification
    based on sparse deep belief net and combination of multiple classifiers,” *Transactions
    of the Institute of Measurement and Control*, vol. 38, no. 4, pp. 435–451, 2016.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Sors, S. Bonnet, S. Mirek, L. Vercueil, and J.-F. Payen, “A convolutional
    neural network for sleep stage scoring from raw single-channel eeg,” *Biomedical
    Signal Processing and Control*, vol. 42, pp. 107–114, 2018.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Vilamala, K. H. Madsen, and L. K. Hansen, “Neural networks for interpretable
    analysis of eeg sleep stage scoring,” in *International Workshop on Machine Learning
    for Signal Processing 2017*, 2017.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S. Biswal, J. Kulas, H. Sun, B. Goparaju, M. B. Westover, M. T. Bianchi,
    and J. Sun, “Sleepnet: automated sleep staging system via deep learning,” *arXiv
    preprint arXiv:1707.08262*, 2017.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] K. M. Tsiouris, V. C. Pezoulas, M. Zervakis, S. Konitsiotis, D. D. Koutsouris,
    and D. I. Fotiadis, “A long short-term memory deep learning network for the prediction
    of epileptic seizures using eeg signals,” *Computers in biology and medicine*,
    vol. 99, pp. 24–37, 2018.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] D. Tan, R. Zhao, J. Sun, and W. Qin, “Sleep spindle detection using deep
    learning: a validation study based on crowdsourcing,” in *Engineering in Medicine
    and Biology Society (EMBC), 2015 37th Annual International Conference of the IEEE*,
    2015, pp. 2828–2831.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Manzano, A. Guillén, I. Rojas, and L. J. Herrera, “Combination of eeg
    data time and frequency representations in deep networks for sleep stage classification,”
    in *International Conference on Intelligent Computing*, 2017, pp. 219–229.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] L. Fraiwan and K. Lweesy, “Neonatal sleep state identification using deep
    learning autoencoders,” in *Signal Processing & its Applications (CSPA), 2017
    IEEE 13th International Colloquium on*, 2017, pp. 228–231.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Supratak, H. Dong, C. Wu, and Y. Guo, “Deepsleepnet: a model for automatic
    sleep stage scoring based on raw single-channel eeg,” *IEEE Transactions on Neural
    Systems and Rehabilitation Engineering*, vol. 25, no. 11, pp. 1998–2008, 2017.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H. Dong, A. Supratak, W. Pan, C. Wu, P. M. Matthews, and Y. Guo, “Mixed
    neural network approach for temporal sleep stage classification,” *IEEE Transactions
    on Neural Systems and Rehabilitation Engineering*, vol. 26, no. 2, pp. 324–333,
    2018.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] K. G. Hartmann, R. T. Schirrmeister, and T. Ball, “Hierarchical internal
    representation of spectral features in deep convolutional networks trained for
    eeg decoding,” in *Brain-Computer Interface (BCI), 2018 6th International Conference
    on*, 2018, pp. 1–6.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] E. Nurse, B. S. Mashford, A. J. Yepes, I. Kiral-Kornek, S. Harrer, and
    D. R. Freestone, “Decoding eeg and lfp signals using deep learning: heading truenorth,”
    in *Proceedings of the ACM International Conference on Computing Frontiers*, 2016,
    pp. 259–266.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X. Zhang, L. Yao, K. Chen, X. Wang, Q. Sheng, and T. Gu, “Deepkey: An
    eeg and gait based dual-authentication system,” *ACM Transactions on Intelligent
    Systems and Technology (TIST)*, 2017.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] H. Yang, S. Sakhavi, K. K. Ang, and C. Guan, “On the use of convolutional
    neural networks and augmented csp features for multi-class motor imagery of eeg
    signals classification,” in *Engineering in Medicine and Biology Society (EMBC),
    2015 37th Annual International Conference of the IEEE*, 2015, pp. 2620–2623.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] L. Jingwei, C. Yin, and Z. Weidong, “Deep learning eeg response representation
    for brain computer interface,” in *Control Conference (CCC), 2015 34th Chinese*,
    2015, pp. 3518–3523.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] H. K. Lee and Y.-S. Choi, “A convolution neural networks scheme for classification
    of motor imagery eeg based on wavelet time-frequecy image,” in *International
    Conference on Information Networking (ICOIN) 2018*, 2018, pp. 906–909.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] X. Zhang, L. Yao, C. Huang, Q. Z. Sheng, and X. Wang, “Intent recognition
    in smart living through deep recurrent neural networks,” in *International Conference
    on Neural Information Processing*, 2017, pp. 748–758.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Z. Tang, C. Li, and S. Sun, “Single-trial eeg classification of motor
    imagery using deep convolutional neural networks,” *Optik-International Journal
    for Light and Electron Optics*, vol. 130, pp. 11–18, 2017.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Q. Wang, Y. Hu, and H. Chen, “Multi-channel eeg classification based on
    fast convolutional feature extraction,” in *International Symposium on Neural
    Networks*, 2017, pp. 533–540.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] I. Sturm, S. Lapuschkin, W. Samek, and K.-R. Müller, “Interpretable deep
    neural networks for single-trial eeg classification,” *Journal of neuroscience
    methods*, vol. 274, pp. 141–145, 2016.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. Shahin, B. Ahmed, S. T.-B. Hamida, F. L. Mulaffer, M. Glos, and T. Penzel,
    “Deep learning and insomnia: Assisting clinicians with their diagnosis,” *IEEE
    journal of biomedical and health informatics*, vol. 21, no. 6, pp. 1546–1553,
    2017.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] I. Fernández-Varela, D. Athanasakis, S. Parsons, E. Hernández-Pereira,
    and V. Moret-Bonillo, “Sleep staging with deep learning: a convolutional model,”
    in *Proceedings of the European Symposium on Artificial Neural Networks, Computational
    Intelligence and Machine Learning (ESANN 2018)*.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] A. M. Chiarelli, P. Croce, A. Merla, and F. Zappasodi, “Deep learning
    for hybrid eeg-fnirs brain–computer interface: application to motor imagery classification,”
    *Journal of neural engineering*, vol. 15, no. 3, p. 036028, 2018.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] T. Uktveris and V. Jusas, “Application of convolutional neural networks
    to four-class motor imagery classification problem,” *Information Technology And
    Control*, vol. 46, no. 2, pp. 260–273, 2017.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] V. Lawhern, A. Solon, N. Waytowich, S. M. Gordon, C. Hung, and B. J. Lance,
    “Eegnet: a compact convolutional neural network for eeg-based brain–computer interfaces,”
    *Journal of neural engineering*, 2018.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Li, Z. Struzik, L. Zhang, and A. Cichocki, “Feature learning from incomplete
    eeg with denoising autoencoder,” *Neurocomputing*, vol. 165, pp. 23–31, 2015.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S. Redkar, “Using deep learning for human computer interface via electroencephalography,”
    *IAES International Journal of Robotics and Automation*, vol. 4, no. 4, 2015.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Zhang, L. Yao, D. Zhang, X. Wang, Q. Z. Sheng, and T. Gu, “Multi-person
    brain activity recognition via comprehensive eeg signal analysis,” in *Proceedings
    of the 14th EAI International Conference on Mobile and Ubiquitous Systems: Computing,
    Networking and Services*, 2017, pp. 28–37.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] J. Li and A. Cichocki, “Deep learning of multifractal attributes from
    motor imagery induced eeg,” in *International Conference on Neural Information
    Processing*, 2014, pp. 503–510.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y. Ren and Y. Wu, “Convolutional deep belief networks for feature extraction
    of eeg signal,” in *International Joint Conference on Neural Networks (IJCNN)*,
    2014, pp. 2850–2853.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Kumar, A. Sharma, K. Mamun, and T. Tsunoda, “A deep learning approach
    for motor imagery eeg signal classification,” in *Computer Science and Engineering
    (APWC on CSE), 2016 3rd Asia-Pacific World Congress on*, 2016, pp. 34–39.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] N. Lu, T. Li, X. Ren, and H. Miao, “A deep learning scheme for motor imagery
    classification based on restricted boltzmann machines,” *IEEE transactions on
    neural systems and rehabilitation engineering*, vol. 25, no. 6, pp. 566–576, 2017.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] M. Dai, D. Zheng, R. Na, S. Wang, and S. Zhang, “Eeg classification of
    motor imagery using a novel deep learning framework,” *Sensors*, vol. 19, no. 3,
    p. 551, 2019.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C. Tan, F. Sun, W. Zhang, J. Chen, and C. Liu, “Multimodal classification
    with deep convolutional-recurrent neural networks for electroencephalography,”
    in *International Conference on Neural Information Processing*, 2017, pp. 767–776.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] L. Duan, M. Bao, J. Miao, Y. Xu, and J. Chen, “Classification based on
    multilayer extreme learning machine for motor imagery task from eeg signals,”
    *Procedia Computer Science*, vol. 88, pp. 176–184, 2016.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] E. S. Nurse, P. J. Karoly, D. B. Grayden, and D. R. Freestone, “A generalizable
    brain-computer interface (bci) using machine learning for feature discovery,”
    *PloS one*, vol. 10, no. 6, p. e0131328, 2015.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] X. Zhang, L. Yao, C. Huang, S. Wang, M. Tan, G. Long, and C. Wang, “Multi-modality
    sensor data classification with selective attention,” *International Joint Conferences
    on Artificial Intelligence (IJCAI)*, 2018.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] S. Sakhavi, C. Guan, and S. Yan, “Parallel convolutional-linear neural
    network for motor imagery classification,” in *Signal Processing Conference (EUSIPCO),
    2015 23rd European*, 2015, pp. 2736–2740.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Frydenlund and F. Rudzicz, “Emotional affect estimation using video
    and eeg data in deep neural networks,” in *Canadian Conference on Artificial Intelligence*,
    2015, pp. 273–280.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] T. Zhang, W. Zheng, Z. Cui, Y. Zong, and Y. Li, “Spatial-temporal recurrent
    neural network for emotion recognition,” *IEEE transactions on cybernetics*, no. 99,
    pp. 1–9, 2018.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] J. Li, Z. Zhang, and H. He, “Implementation of eeg emotion recognition
    system based on hierarchical convolutional neural networks,” in *International
    Conference on Brain Inspired Cognitive Systems*, 2016, pp. 22–33.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] W. Liu, H. Jiang, and Y. Lu, “Analyze eeg signals with convolutional neural
    network based on power spectrum feature selection,” *Proceedings of Science*,
    2017.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] F. Wang, S.-h. Zhong, J. Peng, J. Jiang, and Y. Liu, “Data augmentation
    for eeg-based emotion recognition with deep convolutional neural networks,” in
    *International Conference on Multimedia Modeling*, 2018, pp. 82–93.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Li, Z. Zhang, and H. He, “Hierarchical convolutional neural networks
    for eeg-based emotion recognition,” *Cognitive Computation*, pp. 1–13, 2017.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] K. Wang, Y. Zhao, Q. Xiong, M. Fan, G. Sun, L. Ma, and T. Liu, “Research
    on healthy anomaly detection model based on deep learning from multiple time-series
    physiological signals,” *Scientific Programming*, vol. 2016, 2016.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] X. Chai, Q. Wang, Y. Zhao, X. Liu, O. Bai, and Y. Li, “Unsupervised domain
    adaptation techniques based on auto-encoder for non-stationary eeg-based emotion
    recognition,” *Computers in biology and medicine*, vol. 79, pp. 205–214, 2016.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] W. Liu, W.-L. Zheng, and B.-L. Lu, “Emotion recognition using multimodal
    deep learning,” in *International Conference on Neural Information Processing*,
    2016, pp. 521–529.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] W.-L. Zheng and B.-L. Lu, “Investigating critical frequency bands and
    channels for eeg-based emotion recognition with deep neural networks,” *IEEE Transactions
    on Autonomous Mental Development*, vol. 7, no. 3, pp. 162–175, 2015.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] W.-L. Zheng, H.-T. Guo, and B.-L. Lu, “Revealing critical channels and
    frequency bands for emotion recognition from eeg with deep belief network,” in
    *Neural Engineering (NER), 2015 7th International IEEE/EMBS Conference on*, 2015,
    pp. 154–157.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Jia, K. Li, X. Li, and A. Zhang, “A novel semi-supervised deep learning
    framework for affective state recognition on eeg signals,” in *IEEE International
    Conference on Bioinformatics and Bioengineering (BIBE)*, 2014, pp. 30–37.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] H. Xu and K. N. Plataniotis, “Affective states classification using eeg
    and semi-supervised deep learning approaches,” in *Multimedia Signal Processing
    (MMSP), 2016 IEEE 18th International Workshop on*, 2016, pp. 1–6.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Li, P. Zhang, D. Song, G. Yu, Y. Hou, and B. Hu, “Eeg based emotion
    identification using unsupervised deep feature learning,” 2015.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] H. Xu and K. N. Plataniotis, “Eeg-based affect states classification
    using deep belief networks,” in *Digital Media Industry & Academic Forum (DMIAF)*,
    2016, pp. 148–153.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] W.-L. Zheng, J.-Y. Zhu, Y. Peng, and B.-L. Lu, “Eeg-based emotion classification
    using deep belief networks,” in *Multimedia and Expo (ICME), 2014 IEEE International
    Conference on*, 2014, pp. 1–6.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] K. Li, X. Li, Y. Zhang, and A. Zhang, “Affective state recognition from
    eeg with deep belief networks,” in *2013 IEEE International Conference on Bioinformatics
    and Biomedicine*, 2013, pp. 305–310.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J. A. Mioranda-Correa and I. Patras, “A multi-task cascaded network for
    prediction of affect, personality, mood and social context using eeg signals,”
    in *Automatic Face & Gesture Recognition (FG 2018), 2018 13th IEEE International
    Conference on*, 2018, pp. 373–380.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] P. Kawde and G. K. Verma, “Deep belief network based affect recognition
    from physiological signals,” in *Electrical, Computer and Electronics (UPCON),
    2017 4th IEEE Uttar Pradesh Section International Conference on*, 2017, pp. 587–592.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Y. Gao, H. J. Lee, and R. M. Mehmood, “Deep learninig of eeg signals
    for emotion recognition,” in *Multimedia & Expo Workshops (ICMEW), 2015 IEEE International
    Conference on*, 2015, pp. 1–5.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Z. Yin, M. Zhao, Y. Wang, J. Yang, and J. Zhang, “Recognition of emotions
    using multimodal physiological signals and an ensemble deep learning model,” *Computer
    methods and programs in biomedicine*, vol. 140, pp. 93–110, 2017.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] S. Alhagry, A. A. Fahmy, and R. A. El-Khoribi, “Emotion recognition based
    on eeg using lstm recurrent neural network,” *Emotion*, vol. 8, no. 10, 2017.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Y. Yuan, G. Xun, F. Ma, Q. Suo, H. Xue, K. Jia, and A. Zhang, “A novel
    channel-aware attention framework for multi-channel eeg seizure detection via
    multi-view deep learning,” in *International Conference on Biomedical & Health
    Informatics (BHI)*, 2018, pp. 206–209.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] S. S. Talathi, “Deep recurrent neural networks for seizure detection
    and early seizure detection systems,” *arXiv preprint arXiv:1706.03283*, 2017.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] G. Ruffini, D. Ibañez, M. Castellano, S. Dunne, and A. Soria-Frisch,
    “Eeg-driven rnn classification for prognosis of neurodegeneration in at-risk patients,”
    in *International Conference on Artificial Neural Networks*, 2016, pp. 306–313.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] I. Ullah, M. Hussain, H. Aboalsamh *et al.*, “An automated system for
    epilepsy detection using eeg brain signals based on deep learning approach,” *Expert
    Systems with Applications*, vol. 107, pp. 61–71, 2018.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] U. R. Acharya, S. L. Oh, Y. Hagiwara, J. H. Tan, H. Adeli, and D. P.
    Subha, “Automated eeg-based screening of depression using deep convolutional neural
    network,” *Computer methods and programs in biomedicine*, vol. 161, pp. 103–113,
    2018.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] U. R. Acharya, S. L. Oh, Y. Hagiwara, J. H. Tan, and H. Adeli, “Deep
    convolutional neural network for the automated detection and diagnosis of seizure
    using eeg signals,” *Computers in biology and medicine*, vol. 100, pp. 270–278,
    2018.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] F. C. Morabito, M. Campolo, C. Ieracitano, J. M. Ebadi, L. Bonanno, A. Bramanti,
    S. Desalvo, N. Mammone, and P. Bramanti, “Deep convolutional neural networks for
    classification of mild cognitive impaired and alzheimer’s disease patients from
    scalp eeg recordings,” in *Research and Technologies for Society and Industry
    Leveraging a better tomorrow (RTSI), 2016 IEEE 2nd International Forum on*, 2016,
    pp. 1–6.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball,
    “Deep learning with convolutional neural networks for decoding and visualization
    of eeg pathology,” in *Signal Processing in Medicine and Biology Symposium (SPMB),
    2017 IEEE*, 2017, pp. 1–7.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] M.-P. Hosseini, T. X. Tran, D. Pompili, K. Elisevich, and H. Soltanian-Zadeh,
    “Deep learning with edge computing for localization of epileptogenicity using
    multimodal rs-fmri and eeg big data,” in *Autonomic Computing (ICAC), 2017 IEEE
    International Conference on*, 2017, pp. 83–92.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] A. R. Johansen, J. Jin, T. Maszczyk, J. Dauwels, S. S. Cash, and M. B.
    Westover, “Epileptiform spike detection via convolutional neural networks,” in
    *IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2016, pp. 754–758.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] A. H. Ansari, P. J. Cherian, A. Caicedo, G. Naulaers, M. De Vos, and
    S. Van Huffel, “Neonatal seizure detection using deep convolutional neural networks,”
    *International journal of neural systems*, p. 1850011, 2018.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] M.-P. Hosseini, D. Pompili, K. Elisevich, and H. Soltanian-Zadeh, “Optimized
    deep learning for eeg big data and seizure prediction bci via internet of things,”
    *IEEE Transactions on Big Data*, vol. 3, no. 4, pp. 392–404, 2017.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Y. Yuan, G. Xun, K. Jia, and A. Zhang, “A novel wavelet-based model for
    eeg epileptic seizure detection using multi-context learning,” in *IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM)*, 2017, pp. 694–699.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Q. Lin, S.-q. Ye, X.-m. Huang, S.-y. Li, M.-z. Zhang, Y. Xue, and W.-S.
    Chen, “Classification of epileptic eeg signals with stacked sparse autoencoder
    based on deep learning,” in *International Conference on Intelligent Computing*,
    2016, pp. 802–810.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] F. C. Morabito, M. Campolo, N. Mammone, M. Versaci, S. Franceschetti,
    F. Tagliavini, V. Sofia, D. Fatuzzo, A. Gambardella, A. Labate *et al.*, “Deep
    learning representation from electroencephalography of early-stage creutzfeldt-jakob
    disease and features for differentiation from rapidly progressive dementia,” *International
    journal of neural systems*, vol. 27, no. 02, p. 1650039, 2017.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] T. Wen and Z. Zhang, “Deep convolution neural network and autoencoders-based
    unsupervised feature learning of eeg signals,” *IEEE Access*, vol. 6, pp. 25 399–25 410,
    2018.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] A. Page, J. Turner, T. Mohsenin, and T. Oates, “Comparing raw data and
    feature extraction for seizure detection with deep learning methods.” in *FLAIRS
    Conference*, 2014.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Y. Zhao and L. He, “Deep learning in the eeg diagnosis of alzheimer’s
    disease,” in *Asian Conference on Computer Vision*, 2014, pp. 340–353.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Turner, A. Page, T. Mohsenin, and T. Oates, “Deep belief networks
    used on high resolution multichannel electroencephalography data for seizure detection,”
    in *2014 AAAI Spring Symposium Series*, 2014.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] V. Shah, M. Golmohammadi, S. Ziyabari, E. Von Weltin, I. Obeid, and J. Picone,
    “Optimizing channel selection for seizure detection,” in *Signal Processing in
    Medicine and Biology Symposium (SPMB), 2017 IEEE*, 2017, pp. 1–5.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] M.-P. Hosseini, H. Soltanian-Zadeh, K. Elisevich, and D. Pompili, “Cloud-based
    deep learning of big eeg data for epileptic seizure prediction,” *arXiv preprint
    arXiv:1702.05192*, 2017.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] M. Golmohammadi, S. Ziyabari, V. Shah, S. L. de Diego, I. Obeid, and
    J. Picone, “Deep architectures for automated seizure detection in scalp eegs,”
    *arXiv preprint arXiv:1712.09776*, 2017.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] A. M. Al-kaysi, A. Al-Ani, and T. W. Boonstra, “A multichannel deep belief
    network for the classification of eeg data,” in *International Conference on Neural
    Information Processing*, 2015, pp. 38–45.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S. M. Abdelfattah, G. M. Abdelrahman, and M. Wang, “Augmenting the size
    of eeg datasets using generative adversarial networks,” in *2018 International
    Joint Conference on Neural Networks (IJCNN)*, 2018, pp. 1–6.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] S. Palazzo, C. Spampinato, I. Kavasidis, D. Giordano, and M. Shah, “Generative
    adversarial networks conditioned by brain signals,” in *Proceedings of the IEEE
    International Conference on Computer Vision*, 2017, pp. 3410–3418.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] P. S. S. C. G. D. S. M. Kavasidis, I., “Brain2image: Converting brain
    signals into images,” in *Proceedings of the 25th ACM international conference
    on Multimedia*, 2017, pp. 1809–1817.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] J. Teo, C. L. Hou, and J. Mountstephens, “Deep learning for eeg-based
    preference classification,” in *AIP Conference Proceedings*, vol. 1891, no. 1,
    2017, p. 020141.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] T. K. Reddy and L. Behera, “Online eye state recognition from eeg data
    using deep architectures,” in *Systems, Man, and Cybernetics (SMC), 2016 IEEE
    International Conference on*, 2016, pp. 000 712–000 717.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] A. J. Yepes, J. Tang, and B. S. Mashford, “Improving classification accuracy
    of feedforward neural networks for spiking neuromorphic chips,” *arXiv preprint
    arXiv:1705.07755*, 2017.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] J. Shang, W. Zhang, J. Xiong, and Q. Liu, “Cognitive load recognition
    using multi-channel complex network method,” in *International Symposium on Neural
    Networks*, 2017, pp. 466–474.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Behncke, R. T. Schirrmeister, W. Burgard, and T. Ball, “The signature
    of robot action success in eeg signals of a human observer: Decoding and visualization
    using deep convolutional neural networks,” in *International Conference on Brain-Computer
    Interface (BCI) 2018*, 2018, pp. 1–6.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Y.-C. Hung, Y.-K. Wang, M. Prasad, and C.-T. Lin, “Brain dynamic states
    analysis based on 3d convolutional neural network,” in *Systems, Man, and Cybernetics
    (SMC), 2017 IEEE International Conference on*, 2017, pp. 222–227.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] V. Baltatzis, K.-M. Bintsi, G. K. Apostolidis, and L. J. Hadjileontiadis,
    “Bullying incidences identification within an immersive environment using hd eeg-based
    analysis: A swarm decomposition and deep learning approach,” *Scientific reports*,
    vol. 7, no. 1, p. 17292, 2017.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] S. Stober, D. J. Cameron, and J. A. Grahn, “Classifying eeg recordings
    of rhythm perception.” in *ISMIR*, 2014, pp. 649–654.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] M. Völker, R. T. Schirrmeister, L. D. Fiederer, W. Burgard, and T. Ball,
    “Deep transfer learning for error decoding from non-invasive eeg,” in *Brain-Computer
    Interface (BCI), 2018 6th International Conference on*, 2018, pp. 1–6.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] L. G. Hernández, O. M. Mozos, J. M. Ferrández, and J. M. Antelis, “Eeg-based
    detection of braking intention under different car driving conditions,” *Frontiers
    in neuroinformatics*, vol. 12, 2018.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. A. Almogbel, A. H. Dang, and W. Kameyama, “Eeg-signals based cognitive
    workload detection of vehicle driver using deep learning,” in *Advanced Communication
    Technology (ICACT), 2018 20th International Conference on*, 2018, pp. 256–259.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] M. J. Putten, S. Olbrich, and M. Arns, “Predicting sex from brain rhythms
    with deep learning,” *Scientific reports*, vol. 8, no. 1, p. 3069, 2018.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M. Hajinoroozi, Z. Mao, and Y. Huang, “Prediction of driver’s drowsy
    and alert states from eeg signals with deep learning,” in *Computational Advances
    in Multi-Sensor Adaptive Processing (CAMSAP), 2015 IEEE 6th International Workshop
    on*, 2015, pp. 493–496.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] A. Sternin, S. Stober, J. Grahn, and A. Owen, “Tempo estimation from
    the eeg signal during perception and imagination of music,” in *International
    Symposium on Computer Music Multidisciplinary Research*, 2015.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] L. Chu, R. Qiu, H. Liu, Z. Ling, T. Zhang, and J. Wang, “Individual recognition
    in schizophrenia using deep learning methods with random forest and voting classifiers:
    Insights from resting state eeg streams,” *arXiv preprint arXiv:1707.03467*, 2017.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Z. Yin and J. Zhang, “Cross-session classification of mental workload
    levels using eeg and an adaptive deep learning model,” *Biomedical Signal Processing
    and Control*, vol. 33, pp. 30–47, 2017.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] L.-H. Du, W. Liu, W.-L. Zheng, and B.-L. Lu, “Detecting driving fatigue
    with multimodal deep learning,” in *Neural Engineering (NER), 2017 8th International
    IEEE/EMBS Conference on*, 2017, pp. 74–77.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] S. Narejo, E. Pasero, and F. Kulsoom, “Eeg based eye state classification
    using deep belief network and stacked autoencoder,” *International Journal of
    Electrical and Computer Engineering (IJECE)*, vol. 6, no. 6, pp. 3131–3141, 2016.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] M. Hajinoroozi, T.-P. Jung, C.-T. Lin, and Y. Huang, “Feature extraction
    with deep belief networks for driver’s cognitive states prediction from eeg data,”
    in *Signal and Information Processing (ChinaSIP), 2015 IEEE China Summit and International
    Conference on*, 2015, pp. 812–815.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] P. P. San, S. H. Ling, R. Chai, Y. Tran, A. Craig, and H. Nguyen, “Eeg-based
    driver fatigue detection using hybrid deep generic model,” in *Engineering in
    Medicine and Biology Society (EMBC), 2016 IEEE 38th Annual International Conference
    of the*, 2016, pp. 800–803.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] P. Li, W. Jiang, and F. Su, “Single-channel eeg-based mental fatigue
    detection based on deep belief network,” in *Engineering in Medicine and Biology
    Society (EMBC), 2016 IEEE 38th Annual International Conference of the*, 2016,
    pp. 367–370.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] P. Zhang, X. Wang, W. Zhang, and J. Chen, “Learning spatial–spectral–temporal
    eeg features with recurrent 3d convolutional neural networks for cross-task mental
    workload assessment,” *IEEE Transactions on neural systems and rehabilitation
    engineering*, vol. 27, no. 1, pp. 31–42, 2018.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] S. Stober, A. Sternin, A. M. Owen, and J. A. Grahn, “Deep feature learning
    for eeg recordings,” *arXiv preprint arXiv:1511.04306*, 2015.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] R. Chai, S. H. Ling, P. P. San, G. R. Naik, T. N. Nguyen, Y. Tran, A. Craig,
    and H. T. Nguyen, “Improving eeg-based driver fatigue classification using sparse-deep
    belief networks,” *Frontiers in neuroscience*, vol. 11, p. 103, 2017.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] P. Bashivan, M. Yeasin, and G. M. Bidelman, “Single trial prediction
    of normal and excessive cognitive load through eeg feature fusion,” in *Signal
    Processing in Medicine and Biology Symposium (SPMB), 2015 IEEE*, 2015, pp. 1–5.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] X. Zhang, L. Yao, C. Huang, S. S. Kanhere, and D. Zhang, “Brain2object:
    Printing your mind from brain signals with spatial correlation embedding,” *arXiv
    preprint arXiv:1810.02223*, 2018.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] T. Koike-Akino, R. Mahajan, T. K. Marks, Y. Wang, S. Watanabe, O. Tuzel,
    and P. Orlik, “High-accuracy user identification using eeg biometrics,” in *2016
    38th Annual International Conference of the IEEE Engineering in Medicine and Biology
    Society (EMBC)*, 2016, pp. 854–858.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] K. Kawasaki, T. Yoshikawa, and T. Furuhashi, “Visualizing extracted feature
    by deep learning in p300 discrimination task,” in *Soft Computing and Pattern
    Recognition (SoCPaR), 2015 7th International Conference of*, 2015, pp. 149–154.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] C. Spampinato, S. Palazzo, I. Kavasidis, D. Giordano, N. Souly, and M. Shah,
    “Deep learning human mind for automated visual classification,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2017, pp.
    6809–6817.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] M. Liu, W. Wu, Z. Gu, Z. Yu, F. Qi, and Y. Li, “Deep learning based on
    batch normalization for p300 signal detection,” *Neurocomputing*, vol. 275, pp.
    288–297, 2018.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] S. Sarkar, K. Reddy, A. Dorgan, C. Fidopiastis, and M. Giering, “Wearable
    eeg-based activity recognition in phm-related service environment via deep learning,”
    *Int. J. Progn. Health Manag*, vol. 7, pp. 1–10, 2016.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] H. Cecotti and A. Graser, “Convolutional neural networks for p300 detection
    with application to brain-computer interfaces,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 33, no. 3, pp. 433–445, 2011.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] W. Gao, J.-a. Guan, J. Gao, and D. Zhou, “Multi-ganglion ann based feature
    learning with application to p300-bci signal classification,” *Biomedical Signal
    Processing and Control*, vol. 18, pp. 127–137, 2015.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Q. Liu, X.-G. Zhao, Z.-G. Hou, and H.-G. Liu, “Deep belief networks for
    eeg-based concealed information test,” in *International Symposium on Neural Networks*,
    2017, pp. 498–506.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] T. Ma, H. Li, H. Yang, X. Lv, P. Li, T. Liu, D. Yao, and P. Xu, “The
    extraction of motion-onset vep bci features based on deep learning and compressed
    sensing,” *Journal of neuroscience methods*, vol. 275, pp. 80–92, 2017.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] R. Maddula, J. Stivers, M. Mousavi, S. Ravindran, and V. de Sa, “Deep
    recurrent convolutional neural networks for classifying p300 bci signals,” in
    *Proceedings of the 7th Graz Brain-Computer Interface Conference, Graz, Austria*,
    2017, pp. 18–22.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] P. Bashivan, I. Rish, M. Yeasin, and N. Codella, “Learning representations
    from eeg with deep recurrent-convolutional neural networks,” *ICLR*, 2016.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] P. Bashivan, I. Rish, and S. Heisig, “Mental state recognition via wearable
    eeg,” *arXiv preprint arXiv:1602.00985*, 2016.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] A. Shanbhag, A. P. Kholkar, S. Sawant, A. Vicente, S. Martires, and S. Patil,
    “P300 analysis using deep neural network,” in *2017 International Conference on
    Energy, Communication, Data Analytics and Soft Computing (ICECDS)*, 2017, pp.
    3142–3147.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Z. Mao, V. Lawhern, L. M. Merino, K. Ball, L. Deng, B. J. Lance, K. Robbins,
    and Y. Huang, “Classification of non-time-locked rapid serial visual presentation
    events for brain-computer interaction using deep learning,” in *Signal and Information
    Processing (ChinaSIP), 2014 IEEE China Summit & International Conference on*,
    2014, pp. 520–524.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Z. Mao, “Deep learning for rapid serial visual presentation event from
    electroencephalography signal,” Ph.D. dissertation, The University of Texas at
    San Antonio, 2016.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] R. Manor and A. B. Geva, “Convolutional neural network for multi-category
    rapid serial visual presentation bci,” *Frontiers in computational neuroscience*,
    vol. 9, p. 146, 2015.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] H. Cecotti, “Convolutional neural networks for event-related potential
    detection: impact of the architecture,” in *Engineering in Medicine and Biology
    Society (EMBC), 2017 39th Annual International Conference of the IEEE*, 2017,
    pp. 2031–2034.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] A. J. Solon, S. M. Gordon, B. Lance, and V. Lawhern, “Deep learning approaches
    for p300 classification in image triage: Applications to the nails task,” in *Proceedings
    of the 13th NTCIR Conference on Evaluation of Information Access Technologies,
    NTCIR-13, Tokyo, Japan*, 2017, pp. 5–8.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] M. Hajinoroozi, Z. Mao, Y.-P. Lin, and Y. Huang, “Deep transfer learning
    for cross-subject and cross-experiment prediction of image rapid serial visual
    presentation events from eeg data,” in *International Conference on Augmented
    Cognition*, 2017, pp. 45–55.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Z. Mao, W. X. Yao, and Y. Huang, “Eeg-based biometric identification
    with deep learning,” in *Neural Engineering (NER), 2017 8th International IEEE/EMBS
    Conference on*, 2017, pp. 609–612.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] R. Manor, L. Mishali, and A. B. Geva, “Multimodal neural network for
    rapid serial visual presentation brain computer interface,” *Frontiers in computational
    neuroscience*, vol. 10, p. 130, 2016.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Z. Lin, Y. Zeng, L. Tong, H. Zhang, C. Zhang, and B. Yan, “Method for
    enhancing single-trial p300 detection by introducing the complexity degree of
    image information in rapid serial visual presentation tasks,” *PloS one*, vol. 12,
    no. 12, p. e0184713, 2017.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] S. M. Gordon, M. Jaswa, A. J. Solon, and V. J. Lawhern, “Real world bci:
    cross-domain learning and practical applications,” in *Proceedings of the 2017
    ACM Workshop on An Application-oriented Approach to BCI out of the laboratory*,
    2017, pp. 25–28.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] J. Yoon, J. Lee, and M. Whang, “Spatial and time domain feature of erp
    speller system extracted via convolutional neural network,” *Computational intelligence
    and neuroscience*, vol. 2018, 2018.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] J. Shamwell, H. Lee, H. Kwon, A. R. Marathe, V. Lawhern, and W. Nothwang,
    “Single-trial eeg rsvp classification using convolutional neural networks,” in
    *Micro-and Nanotechnology Sensors, Systems, and Applications VIII*, vol. 9836,
    2016, p. 983622.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] L. Vařeka and P. Mautner, “Stacked autoencoders for the p300 component
    detection,” *Frontiers in neuroscience*, vol. 11, p. 302, 2017.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] E. Carabez, M. Sugi, I. Nambu, and Y. Wada, “Identifying single trial
    event-related potentials in an earphone-based auditory brain-computer interface,”
    *Applied Sciences*, vol. 7, no. 11, p. 1197, 2017.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] S. Stober, D. J. Cameron, and J. A. Grahn, “Using convolutional neural
    networks to recognize rhythm stimuli from electroencephalography recordings,”
    in *Advances in neural information processing systems*, 2014, pp. 1449–1457.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A. HACHEM, M. M. B. Khelifa, A. M. Alimi, P. Gorce, S. V. ARASU, S. BAULKANI,
    S. K. BISOY, P. K. PATTNAIK, S. RAVINDRAN, N. PALANISAMY *et al.*, “Effect of
    fatigue on ssvep during virtual wheelchair navigation,” *Journal of Theoretical
    and Applied Information Technology*, vol. 65, no. 1, 2014.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] J. Thomas, T. Maszczyk, N. Sinha, T. Kluge, and J. Dauwels, “Deep learning-based
    classification for brain-computer interfaces,” in *Systems, Man, and Cybernetics
    (SMC), 2017 IEEE International Conference on*, 2017, pp. 234–239.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] N.-S. Kwak, K.-R. Müller, and S.-W. Lee, “A convolutional neural network
    for steady state visual evoked potential classification under ambulatory environment,”
    *PloS one*, vol. 12, no. 2, p. e0172578, 2017.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] N. R. Waytowich, V. Lawhern, J. O. Garcia, J. Cummings, J. Faller, P. Sajda,
    and J. M. Vettel, “Compact convolutional neural networks for classification of
    asynchronous steady-state visual evoked potentials,” *arXiv preprint arXiv:1803.04566*,
    2018.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] N. K. N. Aznan, S. Bonner, J. D. Connolly, N. A. Moubayed, and T. P.
    Breckon, “On the classification of ssvep-based dry-eeg signals via convolutional
    neural networks,” *arXiv preprint arXiv:1805.04157*, 2018.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] T. Tu, J. Koss, and P. Sajda, “Relating deep neural network representations
    to eeg-fmri spatiotemporal dynamics in a perceptual decision-making task,” in
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    Workshops*, 2018, pp. 1985–1991.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Kulasingham, V. Vibujithan, and A. De Silva, “Deep belief networks
    and stacked autoencoders for the p300 guilty knowledge test,” in *Biomedical Engineering
    and Sciences (IECBES), 2016 IEEE EMBS Conference on*, 2016, pp. 127–132.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] M. Attia, I. Hettiarachchi, M. Hossny, and S. Nahavandi, “A time domain
    classification of steady-state visual evoked potentials using deep recurrent-convolutional
    neural networks,” in *Biomedical Imaging (ISBI 2018), 2018 IEEE 15th International
    Symposium on*, 2018, pp. 766–769.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] J. Pérez-Benítez, J. Pérez-Benítez, and J. Espina-Hernández, “Development
    of a brain computer interface interface using multi-frequency visual stimulation
    and deep neural networks,” in *Electronics, Communications and Computers (CONIELECOMP),
    2018 International Conference on*, 2018, pp. 18–24.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] G. Huve, K. Takahashi, and M. Hashimoto, “Brain activity recognition
    with a wearable fnirs using neural networks,” in *Mechatronics and Automation
    (ICMA), 2017 IEEE International Conference on*, 2017, pp. 1573–1578.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] ——, “Brain-computer interface using deep neural network and its application
    to mobile robot control,” in *Advanced Motion Control (AMC), 2018 IEEE 15th International
    Workshop on*, 2018, pp. 169–174.'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] J. Hennrich, C. Herff, D. Heger, and T. Schultz, “Investigating deep
    learning for fnirs based bci.” in *EMBC*, 2015, pp. 2844–2847.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] T. Hiroyasu, K. Hanawa, and U. Yamamoto, “Gender classification of subjects
    from cerebral blood flow changes using deep learning,” in *Computational Intelligence
    and Data Mining (CIDM), 2014 IEEE Symposium on*, 2014, pp. 229–233.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] S. Koyamada, Y. Shikauchi, K. Nakae, M. Koyama, and S. Ishii, “Deep learning
    of fmri big data: a novel approach to subject-transfer decoding,” *arXiv preprint
    arXiv:1502.00093*, 2015.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] G. Shen, T. Horikawa, K. Majima, and Y. Kamitani, “Deep image reconstruction
    from human brain activity,” *PLoS computational biology*, vol. 15, no. 1, p. e1006633,
    2019.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] R. M. Cichy, A. Khosla, D. Pantazis, A. Torralba, and A. Oliva, “Comparison
    of deep neural networks to spatio-temporal cortical dynamics of human visual object
    recognition reveals hierarchical correspondence,” *Scientific reports*, vol. 6,
    p. 27755, 2016.'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio,
    C. Pal, P.-M. Jodoin, and H. Larochelle, “Brain tumor segmentation with deep neural
    networks,” *Medical image analysis*, vol. 35, pp. 18–31, 2017.'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] V. Shreyas and V. Pankajakshan, “A deep learning architecture for brain
    tumor segmentation in mri images,” in *Multimedia Signal Processing (MMSP), 2017
    IEEE 19th International Workshop on*, 2017, pp. 1–6.'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] S. Sarraf and G. Tofighi, “Deep learning-based pipeline to recognize
    alzheimer’s disease using fmri data,” in *Future Technologies Conference (FTC)*,
    2016, pp. 816–820.'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] R. Li, W. Zhang, H.-I. Suk, L. Wang, J. Li, D. Shen, and S. Ji, “Deep
    learning based imaging data completion for improved brain disease diagnosis,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, 2014, pp. 305–312.'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] H.-I. Suk, C.-Y. Wee, S.-W. Lee, and D. Shen, “State-space model with
    deep learning for functional dynamics estimation in resting-state fmri,” *NeuroImage*,
    vol. 129, pp. 292–307, 2016.'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] H.-I. Suk, D. Shen, A. D. N. Initiative *et al.*, “Deep learning in diagnosis
    of brain disorders,” in *Recent Progress in Brain and Cognitive Engineering*,
    2015, pp. 203–213.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] S. M. Plis, D. R. Hjelm, R. Salakhutdinov, E. A. Allen, H. J. Bockholt,
    J. D. Long, H. J. Johnson, J. S. Paulsen, J. A. Turner, and V. D. Calhoun, “Deep
    learning for neuroimaging: a validation study,” *Frontiers in neuroscience*, vol. 8,
    p. 229, 2014.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] A. Ortiz, J. Munilla, J. M. Gorriz, and J. Ramirez, “Ensembles of deep
    learning architectures for the early diagnosis of the alzheimer’s disease,” *International
    journal of neural systems*, vol. 26, no. 07, p. 1650025, 2016.'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] N. F. M. Suhaimi, Z. Z. Htike, and N. K. A. M. Rashid, “Studies on classification
    of fmri data using deep learning approach,” 2015.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] K. Seeliger, U. Güçlü, L. Ambrogioni, Y. Güçlütürk, and M. Van Gerven,
    “Generative adversarial networks for reconstructing natural images from brain
    activity,” *NeuroImage*, vol. 181, pp. 775–785, 2018.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] C. Han, H. Hayashi, L. Rundo, R. Araki, W. Shimoda, S. Muramatsu, Y. Furukawa,
    G. Mauri, and H. Nakayama, “Gan-based synthetic brain mr image generation,” in
    *2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)*, 2018,
    pp. 734–738.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] P. Zhang, F. Wang, W. Xu, and Y. Li, “Multi-channel generative adversarial
    network for parallel magnetic resonance image reconstruction in k-space,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*, 2018,
    pp. 180–188.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] C. Hu, R. Ju, Y. Shen, P. Zhou, and Q. Li, “Clinical decision support
    for alzheimer’s disease based on deep learning and brain network,” in *Communications
    (ICC), 2016 IEEE International Conference on*, 2016, pp. 1–6.'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] P. Garg, E. Davenport, G. Murugesan, B. Wagner, C. Whitlow, J. Maldjian,
    and A. Montillo, “Automatic 1d convolutional neural network-based detection of
    artifacts in meg acquired without electrooculography or electrocardiography,”
    in *Pattern Recognition in Neuroimaging (PRNI), 2017 International Workshop on*,
    2017, pp. 1–4.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] M. Shu and A. Fyshe, “Sparse autoencoders for word decoding from magnetoencephalography,”
    in *Proceedings of the third NIPS Workshop on Machine Learning and Interpretation
    in NeuroImaging (MLINI)*, 2013.'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] A. Hasasneh, N. Kampel, P. Sripad, N. J. Shah, and J. Dammers, “Deep
    learning approach for automatic classification of ocular and cardiac artifacts
    in meg data,” *Journal of Engineering*, vol. 2018, 2018.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Y. Gordienko, S. Stirenko, Y. Kochura, O. Alienin, M. Novotarskiy, and
    N. Gordienko, “Deep learning for fatigue estimation on the basis of multimodal
    human-machine interactions,” *arXiv preprint arXiv:1801.06048*, 2017.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] G. Pfurtscheller and F. L. Da Silva, “Event-related eeg/meg synchronization
    and desynchronization: basic principles,” *Clinical neurophysiology*, vol. 110,
    no. 11, pp. 1842–1857, 1999.'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] P. Khurana, A. Majumdar, and R. Ward, “Class-wise deep dictionaries for
    eeg classification,” in *International Joint Conference on Neural Networks (IJCNN)*,
    2016, pp. 3556–3563.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] E. Yin, T. Zeyl, R. Saab, T. Chau, D. Hu, and Z. Zhou, “A hybrid brain–computer
    interface based on the fusion of p300 and ssvep scores,” *IEEE Transactions on
    Neural Systems and Rehabilitation Engineering*, vol. 23, no. 4, pp. 693–701, 2015.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] S. Min, B. Lee, and S. Yoon, “Deep learning in bioinformatics,” *Briefings
    in bioinformatics*, vol. 18, no. 5, pp. 851–869, 2017.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] S. Sarraf, G. Tofighi *et al.*, “Deepad: Alzheimer’s disease classification
    via deep convolutional neural networks using mri and fmri,” *bioRxiv*, p. 070441,
    2016.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] L. Marc Moreno, “Deep learning for brain tumor segmentation,” *Master
    diss. University of Colorado Colorado Springs.*, 2017.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” *International Conference
    on Learning Representations (ICLR)*, 2016.'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
    networks,” in *International Conference on Machine Learning (ICML)*, 2017, pp.
    214–223.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei, “Deep learning for
    epileptic intracranial eeg data,” in *Machine Learning for Signal Processing (MLSP),
    2016 IEEE 26th International Workshop on*, 2016, pp. 1–6.'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] A. Antoniades, L. Spyrou, D. Martin-Lopez, A. Valentin, G. Alarcon, S. Sanei,
    and C. C. Took, “Deep neural architectures for mapping scalp to intracranial eeg,”
    *International journal of neural systems*, p. 1850009, 2018.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] R. Parasuraman and Y. Jiang, “Individual differences in cognition, affect,
    and performance: Behavioral, neuroimaging, and molecular genetic approaches,”
    *Neuroimage*, vol. 59, no. 1, pp. 70–82, 2012.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] L. Deng, “Three classes of deep learning architectures and their applications:
    a tutorial survey,” *APSIPA transactions on signal and information processing*,
    2012.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] W.-L. Zheng and B.-L. Lu, “Personalizing eeg-based affective models with
    transfer learning,” in *Proceedings of the Twenty-Fifth International Joint Conference
    on Artificial Intelligence*, 2016, pp. 2732–2738.'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] X. Zhang, L. Yao, and F. Yuan, “Adversarial variational embedding for
    robust semi-supervised learning,” in *SIGKDD 2019*, 2019.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] S. Koyama, S. M. Chase, A. S. Whitford, M. Velliste, A. B. Schwartz,
    and R. E. Kass, “Comparison of brain–computer interface decoding algorithms in
    open-loop and closed-loop control,” *Journal of computational neuroscience*, vol. 29,
    no. 1-2, pp. 73–87, 2010.'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] S. Aliakbaryhosseinabadi, E. N. Kamavuako, N. Jiang, D. Farina, and N. Mrachacz-Kersting,
    “Online adaptive synchronous bci system with attention variations,” in *Brain-Computer
    Interface Research*, 2019, pp. 31–41.'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] E. K. Kalunga, S. Chevallier, Q. Barthélemy, K. Djouani, E. Monacelli,
    and Y. Hamam, “Online ssvep-based bci using riemannian geometry,” *Neurocomputing*,
    vol. 191, pp. 55–68, 2016.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] M. Pacharra, S. Debener, and E. Wascher, “Concealed around-the-ear eeg
    captures cognitive processing in a visual simon task,” *Frontiers in human neuroscience*,
    vol. 11, p. 290, 2017.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] K. B. Mikkelsen, S. L. Kappel, D. P. Mandic, and P. Kidmose, “Eeg recorded
    from the ear: Characterizing the ear-eeg method,” *Frontiers in neuroscience*,
    vol. 9, p. 438, 2015.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] S. B. Rutkove, “Introduction to volume conduction,” in *The clinical
    neurophysiology primer*, 2007, pp. 43–53.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] B. Burle, L. Spieser, C. Roger, L. Casini, T. Hasbroucq, and F. Vidal,
    “Spatial and temporal resolutions of eeg: Is it really black and white? a scalp
    current density view,” *International Journal of Psychophysiology*, vol. 97, no. 3,
    pp. 210–220, 2015.'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] J. Malmivuo, R. Plonsey *et al.*, *Bioelectromagnetism: principles and
    applications of bioelectric and biomagnetic fields*, 1995.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] M. Tortella-Feliu, A. Morillas-Romero, M. Balle, J. Llabrés, X. Bornas,
    and P. Putman, “Spontaneous eeg activity and spontaneous emotion regulation,”
    *International Journal of Psychophysiology*, vol. 94, no. 3, pp. 365–372, 2014.'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] A. Salek-Haddadi, K. Friston, L. Lemieux, and D. Fish, “Studying spontaneous
    eeg activity with fmri,” *Brain research reviews*, vol. 43, no. 1, pp. 110–133,
    2003.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] A. Ikeda and Y. Washizawa, “Spontaneous eeg classification using complex
    valued neural network,” in *International Conference on Neural Information Processing*,
    2019, pp. 495–503.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] A. M. Norcia, L. G. Appelbaum, J. M. Ales, B. R. Cottereau, and B. Rossion,
    “The steady-state visual evoked potential in vision research: a review,” *Journal
    of vision*, vol. 15, no. 6, pp. 4–4, 2015.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] S. Lees, N. Dayan, H. Cecotti, P. Mccullagh, L. Maguire, F. Lotte, and
    D. Coyle, “A review of rapid serial visual presentation-based brain–computer interfaces,”
    *Journal of neural engineering*, vol. 15, no. 2, p. 021001, 2018.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] K. H. Chiappa, *Evoked potentials in clinical medicine*, 1997.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] V. Mayya, B. Mainsah, and G. Reeves, “Information-theoretic analysis
    of refractory effects in the p300 speller,” *arXiv preprint arXiv:1701.03313\.
    Non-exclusive-distrib License. https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html*,
    2017.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] C. Guger, S. Daban, E. Sellers, C. Holzner, G. Krausz, R. Carabalona,
    F. Gramatica, and G. Edlinger, “How many people are able to control a p300-based
    brain–computer interface (bci)?” *Neuroscience letters*, vol. 462, no. 1, pp.
    94–98, 2009.'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] A. Belitski, J. Farquhar, and P. Desain, “P300 audio-visual speller,”
    *Journal of Neural Engineering*, vol. 8, no. 2, p. 025022, 2011.'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] M. Welvaert and Y. Rosseel, “On the definition of signal-to-noise ratio
    and contrast-to-noise ratio for fmri data,” *PloS one*, vol. 8, no. 11, 2013.'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] R. M. Cichy, A. Khosla, D. Pantazis, and A. Oliva, “Dynamics of scene
    representations in the human brain revealed by magnetoencephalography and deep
    neural networks,” *Neuroimage*, vol. 153, pp. 346–358, 2017.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” *arXiv preprint arXiv:1406.1078*, 2014.'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A
    review and new perspectives,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 35, no. 8, pp. 1798–1828, 2013.'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] P. O. Glauner, “Comparison of training methods for deep neural networks,”
    *arXiv preprint arXiv:1504.06825*, 2015.'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] G. St-Yves and T. Naselaris, “Generative adversarial networks conditioned
    on brain activity reconstruct seen images,” in *2018 IEEE International Conference
    on Systems, Man, and Cybernetics (SMC)*, 2018, pp. 1054–1061.'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Non-invasive Brain Signals
  id: totrans-990
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we present a detailed introduction of brain signals as shown in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"). Non-invasive brain imaging
    technique can be collected using electrical, magnetic or metabolic methods, which
    mainly include Electroencephalogram (EEG), Functional near-infrared spectroscopy
    (fNIRS), Functional magnetic resonance imaging (fMRI), and Magnetoencephalography
    (MEG).'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Electroencephalography (EEG)
  id: totrans-992
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Electroencephalography (EEG) is the most commonly used non-invasive technique
    for measuring brain activities. EEG monitors the voltage fluctuations generated
    by an electrical current within human neurons. Electrodes placed on the scalp
    measure the amplitude of EEG signals. EEG signals have a low spatial resolution
    due to the effect of volume conduction which refers to the complex effects of
    measuring electrical potentials a distance from the source generators [[241](#bib.bib241),
    [242](#bib.bib242)]. EEG electrode locations generally follow the international
    10-20 system [[243](#bib.bib243)]. The specific placement of electrodes is presented
    in Figure [5](#A1.F5 "Figure 5 ‣ A.1 Electroencephalography (EEG) ‣ Appendix A
    Non-invasive Brain Signals ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers") [[10](#bib.bib10)]. The EEG signals
    are collected while the subject is undertaking imagination task. Each line represents
    the signal stream collected from a single EEG electrode (also called ‘channel‘)
    over time.'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: The temporal resolution of EEG signals is much better than the spatial resolution.
    The ionic current changes rapidly, which offers a temporal resolution higher than
    $1000$ Hz. The SNR of EEG is generally very poor due to both objective and subjective
    factors. Objective factors include environmental noises, the obstruction of the
    skull and other tissues between cortex and scalp, and different stimulations.
    Subjective factors contain the subject’s mental stage, fatigue status, the variance
    among different subjects, and so on.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: EEG recording equipment can be installed in a cap-like headset. The EEG headset
    can be mounted on the user’s head to gather signals. Compared to other equipment
    used to measure brain signals, EEG headsets are portable and more accessible for
    most applications.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: 'The EEG signals collected from any typical EEG hardware have several non-overlapping
    frequency bands (Delta, Theta, Alpha, Beta, and Gamma) based on the strong intra-band
    correlation with a distinct behavioral state [[10](#bib.bib10)]. Each EEG pattern
    contains signals associated with particular brain information. Table [7](#A1.T7
    "Table 7 ‣ A.1 Electroencephalography (EEG) ‣ Appendix A Non-invasive Brain Signals
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") shows EEG frequency patterns and the corresponding characteristics.
    Here, the degree of awareness denotes the perception of individuals when presented
    with external stimuli.'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to other signals (e.g., fMRI, fNIRS, MEG), EEG has several important
    advantages: 1) the hardware has higher portability with much lower price; 2) the
    temporal resolution is very high (milliseconds level). Among other non-invasive
    techniques, only MEG has the same level of temporal resolution; 3) EEG is relatively
    tolerant of subject movement and artifacts, which can be minimized by existing
    signal processing methods; 4) the subject doesn’t need to be exposed to high-intensity
    ($>$1 Tesla) magnetic fields, therefore, EEG can serve subjects that have metal
    implants in their body (such as metal-containing pacemakers).'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
- en: 'As the most commonly used signals, there are a large number of sub-classes
    of EEG signals. In this section, we present a methodical introduction of EEG sub-class
    signals. As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Brain Imaging Techniques
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers"), we divided EEG signals into spontaneous EEG and evoked potentials.
    Evoked potentials can be split into event-related potentials and steady-state
    evoked potentials based on the frequency of the external stimuli [[7](#bib.bib7)].
    Each potential contains visual-, auditory-, and somatosensory- potentials based
    on the external stimuli types. The dashed quadrilaterals in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"), such as Intracortical, SEP,
    SSAEP, SSSEP, and RSAP, are not included in this survey because there are very
    few existing studies working on them with deep learning algorithms. We list these
    signals for systematic completeness.'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c25f2283966e3801da3e07bf21abf7f.png)'
  id: totrans-999
  prefs: []
  type: TYPE_IMG
- en: (a) EEG electrode locations
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a67e87a4d51d8d018757d6b9157cc1fc.png)'
  id: totrans-1001
  prefs: []
  type: TYPE_IMG
- en: (b) EEG signals
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: EEG electrode locations on scalp (10-20 system) and the gathered
    EEG signals [[10](#bib.bib10)]. The electrodes’ names are marked by their position:
    Fp (pre-frontal), F (frontal), T (temporal), P (parietal), O (occipital), and
    C ( central).'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: EEG patterns and corresponding characters. Awareness Degree denotes
    the degree of being aware of an external world. The awareness degree mentioned
    here is mainly defined in physiology instead of psychology.'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
- en: '| Patterns | Frequency ($Hz$) | Amplitude | Brain State | Awareness Degree
    | Produced Location |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
- en: '| Delta | 0.5-4 | Higher | Deep sleep pattern | Lower | Frontally and posteriorly
    |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
- en: '| Theta | 4-8 | High | Light sleep pattern | Low | Entorhinal cortex, hippocampus
    |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
- en: '| Alpha | 8-12 | Medium | Closing the eyes, relax state | Intermediate | Posterior
    regions of head |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
- en: '| Beta | 12-30 | Low | Active thinking, focus, high alert, anxious | High |
    Most evident frontally, motor areas |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
- en: '| Gamma | 30-100 | Lower | During cross-modal sensory processing | Higher |
    Somatosensory, auditory cortices |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
- en: A.1.1 Spontaneous EEG
  id: totrans-1012
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typically, when we talk about the term ‘EEG,’ we refer to spontaneous EEG which
    measures the brain signals under a specific state without external stimulation
    [[244](#bib.bib244), [245](#bib.bib245), [246](#bib.bib246)]. In particular, spontaneous
    EEG includes the EEG signals while the individual is sleeping, undertaking a mental
    task (e.g., counting), suffering brain disorders, undertaking motor imagery tasks,
    in a certain emotion, etc.
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
- en: The EEG signals recorded while a user stares at a color/shape/image belong to
    this category. While the subject is gazing at a specific image, the visual stimuli
    are steady without any change. This scenario differs from the visual stimuli in
    evoked potential, where the visual stimuli are changing at a specific frequency.
    Thus, we regard the image stimulation as a particular state and regard it as spontaneous
    EEG. Spontaneous EEG-based systems are challenging to train, due to the lower
    SNR and the larger variation across subjects [[35](#bib.bib35)].
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the gathering scenarios, the spontaneous EEG contains several
    subordinates: sleeping, motor imagery, emotional, mental disease and others.'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
- en: A.1.2 Evoked Potential (EP)
  id: totrans-1016
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Evoked Potentials (EP) or evoked responses refers to the EEG signals which
    are evoked by an external stimulus instead of spontaneously. An EP is time-locked
    to the external stimulus while the aforementioned spontaneous EEG is non-time-locked.
    In contrast to spontaneous EEG, EP generally has higher amplitude and lower frequency.
    As a result, the EP signals are more robust across subjects. According to the
    stimulation method, there exist two categories of EP: the Event-Related Potential
    (ERP) and the Steady State Evoked Potential (SSEP). ERP records the EEG signals
    in response to an isolated discrete stimulus event (or event change). To achieve
    this isolation, stimuli in an ERP experiment are typically separated from each
    other by a long inter-stimulus interval, allowing for the estimation of a stimulus-independent
    baseline reference [[247](#bib.bib247)]. The stimuli frequency of ERP is generally
    lower than 2 Hz. In contrast, SSEP is generated in response to a periodic stimulus
    at a fixed rate. The stimuli frequency of SSEP generally ranges within 3.5-75
    Hz.'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
- en: 'Event-related potential (ERP). There are three kinds of evoked potentials in
    extensive research and clinical use: Visual Evoked Potentials (VEP); Auditory
    Evoked Potentials (AEP); and Somatosensory Evoked Potentials (SEP) [[28](#bib.bib28)].
    The VEP signals are mainly on the occipital lobe, and the highest signal amplitudes
    are collected at the Calcarine sulcus.'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: 1) Visual Evoked Potentials (VEP). Visual Evoked Potentials are a specific category
    of ERP which is caused by visual stimulus (e.g., an alternating checkerboard pattern
    on a computer screen). VEP signals are hidden within the normal spontaneous EEG.
    To separate VEP signals from the background EEG readings, repetitive stimulation
    and time-locked signal-averaging techniques are generally employed.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
- en: Rapid Serial Visual Presentation (RSVP) [[248](#bib.bib248)] can be regarded
    as one kind of VEP. An RSVP diagram is commonly used to examine the temporal characteristics
    of attention. The subject is required to stare at a screen where a series of items
    (e.g., images) are presented one-by-one. There is a specific item (called the
    target) separates from the rest of the other items (called distracters). The subject
    knows which is the target before the RSVP experiment. For instance, the distracters
    can be a color change or letters among numbers. RSVP contains a static mode (the
    items appear on the screen and then disappear without moving) and a moving mode
    (the items appear on the screen, move to another place, and finally disappear).
    Nowadays, brain signal research mainly focuses on the static mode RSVP. Usually,
    the frequency of RSVP is 10Hz which means that each item will stay on the screen
    for 0.1 seconds.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
- en: 2) Auditory Evoked Potentials (AEP). Auditory Evoked Potentials are a specific
    subclass of ERP in which responses to auditory (sound) stimuli are recorded. AEP
    is mainly recorded from the scalp but originates at the brainstem or cortex. The
    most common AEP measured is the auditory brainstem response (ABR) which is generally
    employed to test the hearing ability of newborns and infants. In the brain signal
    area, AEP is mainly used in clinical tests for its accuracy and reliability in
    detecting unilateral loss [[249](#bib.bib249)]. Similar to RSVP, Rapid Serial
    Auditory Presentation (RSAP) refers to the experiments with rapid serial presentation
    of sound stimuli. The task for the subject is to recognize the target audio among
    the distracters.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
- en: 3) Somatosensory Evoked Potentials (SEP).^(21)^(21)21Generally, Somatosensory
    Evoked Potentials is abbreviated as SSEP or SEP. In this paper, we choose SEP
    as the abbreviation in case of the conflict with Steady-State Evoked Potentials
    (SSEP). Somatosensory Evoked Potentials are another commonly used subcategory
    of ERP which is elicited by electrical stimulation of the peripheral nerves. SEP
    signals conclude a series of amplitude deflection that can be elicited by virtually
    any sensory stimuli.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/630309cd8712b0b41e52e5dae5c8873a.png)'
  id: totrans-1023
  prefs: []
  type: TYPE_IMG
- en: (a) ERP components
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2d2854dac31d649e55a58c9924e818d.png)'
  id: totrans-1025
  prefs: []
  type: TYPE_IMG
- en: (b) P300 speller
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: P300 waves and visual-based P300 speller [[250](#bib.bib250)].'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
- en: 'P300. P300 (also called P3) is an important component in ERP [[251](#bib.bib251)].
    Here we introduce P300 signal separately since it is widely-used in brain signal
    analysis. Figure [6(a)](#A1.F6.sf1 "In Figure 6 ‣ A.1.2 Evoked Potential (EP)
    ‣ A.1 Electroencephalography (EEG) ‣ Appendix A Non-invasive Brain Signals ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers") shows the ERP signal fluctuation in the 500 ms after the stimuli
    onset. The waveform mainly concludes five components, P1, N1, P2, N2, and P3\.
    The capital character P/N represents positive/negative electrical potentials.
    The following number refers to the occurrence time of the specific potential.
    Thus, P300 denotes the positive potential of ERP waveform at approximately 300
    ms after the presented stimuli. Compared to other components, P300 has the highest
    amplitude and is easiest to detect. Thus, a large number of brain signal studies
    focus on P300 analysis. P300 is more of an informative feature instead of a type
    of brain signal (e.g., VEP). Therefore, we do no list P300 in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"). P300 can be analyzed in most
    of ERP signals such as VEP, AEP, SEP.'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, P300 can be elicited by rare, task-relevant events in an ‘oddball’
    paradigm (e.g., P300 speaker). In the oddball paradigm, the subject receives a
    series of stimuli where low-probability target items are mixed with high-probability
    non-target items. Visual and auditory stimuli are the most commonly used in the
    oddball paradigm. Figure [6(b)](#A1.F6.sf2 "In Figure 6 ‣ A.1.2 Evoked Potential
    (EP) ‣ A.1 Electroencephalography (EEG) ‣ Appendix A Non-invasive Brain Signals
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") shows an example of visual-based P300 speller which enables
    the subject the spell letters/numbers directly through brain signals [[250](#bib.bib250)].
    The 26 letters of the alphabet and the Arabic numbers are displayed on a computer
    screen which serves as the keyboard. The subject focuses attention successively
    on the characters they wish to spell. The computer detects the chosen character
    online in real time. This detection is achieved by repeatedly flashing rows and
    columns of the matrix. When the elements containing the selected characters are
    flashing, a P300 fluctuation is elicited. In the $6\times 6$ matrix screen, the
    rows and columns flash in mixed random order. The flash duration and interval
    among adjacent flashes are generally set as 100 ms [[252](#bib.bib252)]. The columns
    and rows flash separately. First, the columns flash six times with each column
    flashing one time. Second, the rows will flash for six times. After that, this
    paradigm repeats for several times (e.g., $N$ times). The P300 signals of the
    total $12N$ flash will be analyzed to output a single outcome (i.e., one letter/number).'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
- en: 'Steady State Evoked Potentials (SSEP). Steady State Evoked Potential is another
    subcategory of evoked potentials, which are periodic cortical responses evoked
    by certain repetitive stimuli with a constant frequency. It has been demonstrated
    that the brain oscillations generally maintain a steady level over time while
    the potentials are evoked by steady state stimuli (e.g., a flickering light with
    fixed frequency). Technically, SSEP is defined as a form of response to repetitive
    sensory stimulation in which the constituent frequency components of the response
    remain constant over time in both amplitude and phase [[37](#bib.bib37)]. Depending
    on the type of stimuli, SSEP divides into three subcategories: Steady-State Visually
    Evoked Potentials (SSVEP), Steady-State Auditory Evoked Potentials (SSAEP), and
    Steady-State Somatosensory Evoked Potentials (SSSEP). In the brain signal area,
    most studies are focused on visual evoked steady potentials, and only rarely do
    papers focus on auditory and somatosensory stimuli. Therefore, in this survey,
    we mainly introduce SSVEP rather than SSAEP and SSSEP.'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
- en: 'Commonly Used Visual-Related Potentials. Visual evoked potentials are the most
    common used potentials. Therefore, it is essential to distinguish the three different
    visual evoked potential paradigms: VEP, RSVP, SSVEP. Here, we theoretically introduce
    the characteristics of each paradigm and then give three demonstration videos
    to provide a better understanding. First, the frequencies are different: the frequency
    of VEP is less than 2Hz while the frequency of RSVP is around 10Hz, and the frequency
    of SSVEP ranges from 3.5 to 75Hz. Second, they have various presentation protocols.
    In the VEP paradigm, different visual patterns will be presented on the screen
    to check the user’s brain signals changes. For instance, in this video^(22)^(22)22https://www.youtube.com/watch?v=iUW_l5YAEEM,
    the image pattern is full of the screen and changes dramatically. In RSVP diagram,
    several items will be presented on a screen one-by-one. All the items are shown
    in the same place and share the same frequency. For example, the video^(23)^(23)23https://www.youtube.com/watch?v=5yddeRrd0hA&t=36s
    shows an RSVP scenario which is called speed reading. In SSVEP paradigm, several
    items will be presented on a screen at the same time while the items are shown
    at variant positions with different frequencies. For example, in this demonstration
    video^(24)^(24)24https://www.youtube.com/watch?v=t96rl1SFHlI, there are four circles
    distributed on the up, down, left, and right sides of a screen and the frequency
    of each item differs from each other.'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Functional Near-infrared Spectroscopy (fNIRS)
  id: totrans-1032
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functional near-infrared spectroscopy (fNIRS) is a non-invasive functional neuro-imaging
    technology using near-infrared (NIR) light [[38](#bib.bib38)]. In specific, fNIRS
    employs NIR light to measure the aggregation degree of oxygenated hemoglobin (Hb)
    and deoxygenated-hemoglobin (deoxy-Hb) because Hb and deoxy-Hb have higher absorbence
    of light than other head components such as the skull and scalp. fNIRS relies
    on blood-oxygen-level-dependent (BOLD) response or hemodynamic response to form
    a functional neuro-image. The BOLD response can detect the oxygenated or deoxygenated
    blood level in the brain blood. The relative levels reflect the blood flow and
    neural activation, where increased blood flow implies a higher metabolic demand
    caused by active neurons. For example, when the user is concentrating on a mental
    task, the prefrontal cortex neurons will be activated, and the BOLD response in
    the prefrontal cortex area will be stronger [[200](#bib.bib200)].
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
- en: 'Single or multiple emitter-detector pairs measure the Hb and deoxy-Hb: the
    emitter transmits NIR light through the blood vessels to the detector. Most existing
    studies use fNIRS technologies to measure the status of prefrontal and motor cortex.
    The former response to mental tasks and music/image imagery while the latter is
    a response to motor-related tasks (e.g., motor imagery). The monitored Hb and
    deoxy-Hb change slowly since the blood speed varies in a relatively slow ratio
    compared to electrical signals. Temporal resolution refers to the smallest time
    of neural activity reliably separated by the signal. The fNIRS has lower temporal
    resolution compared with electrical or magnetic signals. The spatial resolution
    depends on the number of emitter-detector pairs. In current studies, three emitters
    and eight detectors would suffice for adequately acquiring the prefrontal cortex
    signals; and six emitters and six detectors would suffice for covering the motor
    cortex area [[29](#bib.bib29)]. fNIRS has a drawback in that it cannot be used
    to measure cortical activity occurring deeper than 4cm in the brain, due to the
    limitations in light emitter power and spatial resolution.'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Functional Magnetic Resonance Imaging (fMRI)
  id: totrans-1035
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functional magnetic resonance imaging (fMRI) monitors brain activities by detecting
    changes associated with blood flow in brain areas [[14](#bib.bib14)]. Similar
    to fNIRS, fMRI relies on the BOLD response. The main differences between fNIRS
    and fMRI are as follows [[24](#bib.bib24)]. First, as the name implies, fMRI measures
    BOLD response through magnetic instead of optical methods. Hemoglobin differs
    in how it responds to magnetic fields, depending on whether it has a bound oxygen
    molecule. The magnetic fields are more sensitive to and are more easily distorted
    by deoxy-Hb than Hb molecules. Second, the magnetic fields have higher penetration
    than NIR light, which gives fMRI greater ability to capture information from deep
    parts of the brain than fNIRS. Third, fMRI has a higher spatial resolution than
    fNIRS since the latter’s spatial resolution is limited by the emitter-detector
    pairs. However, the temporal resolutions of fMRI and fNIRS are at an equal level
    because they both constrained by the blood flow speed.
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
- en: 'fMRI has several flaws compared to fNIRS: 1) fMRI requires an expensive scanner
    to generate magnetic fields; 2) the scanner is heavy and has poor portability.
    In order to measure the signal of interest, CNR (Contrast-to-Noise Ratio) has
    been investigated to measure the image quality of fMRI because researchers are
    more interested in the contrast between images rather than the raw images. So
    for fMRI data, using the CNR of the time series instead of (t)SNR is more preferred
    because CNR compares a measure of the activation fluctuations to the noise [[253](#bib.bib253)].'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Magnetoencephalography (MEG)
  id: totrans-1038
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Magnetoencephalography (MEG) is a functional neuroimaging technique for mapping
    brain activity by recording magnetic fields produced by electrical currents occurring
    naturally in the brain, using very sensitive magnetometers [[254](#bib.bib254)].
    The ionic currents of active neurons will create weak magnetic fields. The generated
    magnetic fields can be measured by magnetometers like SQUIDs (superconducting
    quantum interference devices). However, producing a detectable magnetic field
    requires massive (e.g., 50,000) active neurons with similar orientation. The source
    of the magnetic field measured by MEG is the pyramidal cells which are perpendicular
    to the cortex surface.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
- en: MEG has a relatively low spatial resolution since the signal quality highly
    depends on the measurement factors (e.g., brain area, neuron orientations, neuron
    depth). However, MEG can provide very high temporal resolution ($\geq$1000Hz)
    since MEG directly monitors the brain activity from the neuron level, which is
    in the same level of intracortical signals. MEG equipment is expensive and not
    portable which limits its real-world deployment.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Basic Deep Learning in Brain Signal Analysis
  id: totrans-1041
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this part, we will give relative detail introduction of various deep learning
    models for the reason that a part of the potential readers who are from non-computer
    area (e.g., biomedical) are not familiar to deep learning.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
- en: For simplification, we first define an operation $\mathcal{T}(\cdot)$ as
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{T}(\bm{x})=\bm{w}*\bm{x}+\bm{b}$ |  | (1) |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathcal{T}(\bm{x},\bm{x^{\prime}})=\bm{w}*\bm{x}+\bm{b}+\bm{w^{\prime}}*\bm{x^{\prime}}+\bm{b^{\prime}}$
    |  | (2) |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
- en: where $\bm{x}$ and $\bm{x^{\prime}}$ denote two variables while $\bm{w}$, $\bm{w^{\prime}}$,
    $\bm{b}$, and $\bm{b^{\prime}}$ denote the corresponding weights and basis.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Discriminative Deep Learning Models
  id: totrans-1047
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the main task of brain signal analysis is brain signal recognition, the
    discriminative deep learning models are the most popular and powerful algorithms.
    Suppose we have a dataset of brain signal samples $\{\mathbb{X},\mathbb{Y}\}$
    where $\mathbb{X}$ denotes the set of brain signal observations and $\mathbb{Y}$
    denotes the set of sample ground truth (i.e., labels). Suppose an specific sample-label
    pair $\{\bm{x}\in\mathbb{R}^{N},\bm{y}\in\mathbb{R}^{M}\}$ where $N$ and $M$ denote
    the dimension of observations and the number of sample categories, respectively.
    The aim of discriminative deep learning models is to learn a function with the
    mapping: $\bm{x}\rightarrow\bm{y}$. In short, the discriminative models receive
    the input data and output the corresponding category or label. All the discriminative
    models introduced in this section are supervised learning techniques which require
    the information of both the observations and the ground truth.'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
- en: B.1.1 Multi-Layer Perceptron (MLP)
  id: totrans-1049
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most basic neural network is fully-connected neural networks (Figure [7(a)](#A2.F7.sf1
    "In Figure 7 ‣ B.1.2 Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep
    Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers")) which only contains one hidden layer. The input layer receives
    the raw data or extracted features of brain signals while the output layer shows
    the classification results. The term ‘fully-connected’ denotes each node in a
    specific layer is connected with all the nodes in the previous and next layer.
    This network is too ‘shallow‘ and generally not regarded as ‘deep‘ neural networks.'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
- en: 'Multilayer Perceptron is the simplest and the most basic deep learning model.
    The key difference between MLP and the fully-connected neural network is that
    MLP has more than one hidden layers. All the nodes are fully-connected with the
    nodes of the adjacent layers but without connection with the other nodes of the
    same layer. MLP includes multiple hidden layers. As shown in Figure [7(b)](#A2.F7.sf2
    "In Figure 7 ‣ B.1.2 Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep
    Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers"), we take a structure with two hidden layers as an example to describe
    the data flow in MLP.'
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
- en: The input layer receives the observation $\bm{x}$ and feeds forward to the first
    hidden layer,
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{h1}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (3) |'
  id: totrans-1053
  prefs: []
  type: TYPE_TB
- en: where $\bm{x^{h1}}$ denotes the data flow in the first hidden layer and $\sigma$
    represents the non-linear activation function. There are several commonly used
    activation functions such as sigmoid/Logistic, Tanh, ReLU, we choose sigmoid activation
    function as an example in this section. Then, the data flow to the second hidden
    layer and the output layer,
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{h2}}=\sigma(\mathcal{T}(\bm{x^{h1}}))$ |  | (4) |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
- en: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (5) |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
- en: where $\bm{y^{\prime}}$ denotes the predict results in one-hot format. The error
    (i.e., loss) could be calculated based on the distance between $\bm{y^{\prime}}$
    and the ground truth $\bm{y}$. For instance, the Euclidean-distance based error
    can be calculated by
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $error=\left\&#124;\bm{y^{\prime}}-\bm{y}\right\&#124;_{2}$ |  | (6) |'
  id: totrans-1058
  prefs: []
  type: TYPE_TB
- en: where $\left\|\cdot\right\|_{2}$ denotes the Euclidean norm. Afterward, the
    error will be back-propagated and optimized by a suitable optimizer. The optimizer
    will adjust all the weights and basis in the model until the error converges.
    The most widely used loss functions includes cross-entropy, negative log likelihood,
    mean square estimation, etc. The most widely used optimizers include Adaptive
    moment estimation (Adam), Stochastic Gradient Descent (SGD), Adagrad (Adaptive
    subgradient method), etc.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
- en: 'Several terms may be easily confused with each other: Artificial Neural Network
    (ANN), Deep Neural Network (DNN), and MLP. These terms have no strict difference
    and often mixed in literature and commonly used as synonyms. Generally, ANN and
    DNN can be used to describe deep learning models overall, including not only fully-connected
    networks but also other networks (e.g., recurrent, convolutional networks), but
    MLP can only refer to fully-connected network. Additionally, ANN contains all
    the models of neural networks, can be either shallow (one hidden layer) or deep
    (multiple hidden layers) while DNN doesn’t cover shallow neural network [[30](#bib.bib30),
    [31](#bib.bib31)].'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
- en: B.1.2 Recurrent Neural Networks (RNN)
  id: totrans-1061
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recurrent Neural Network is a specific subclass of discriminative deep learning
    model which are designed to capture temporal dependencies among input data [[41](#bib.bib41)].
    Figure [8(a)](#A2.F8.sf1 "In Figure 8 ‣ B.1.2 Recurrent Neural Networks (RNN)
    ‣ B.1 Discriminative Deep Learning Models ‣ Appendix B Basic Deep Learning in
    Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers") describes the activity of a specific RNN node
    in the time domain. At each time ranges from $[1,t+1]$, the node receives an input
    $I$ (the subscript represents the specific time) and a hidden state $c$ from the
    previous time (except the first time). For instance, at time $t$ it receives not
    only the input $I_{t}$ but also the hidden state of the previous node $c_{t-1}$.
    The hidden state can be regarded as the ‘memory’ of the nodes which can help the
    RNN ‘remember’ the historical input.'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will report two typical RNN architectures which have attracted much
    attention and achieved great success: long short-term memory and gated recurrent
    units. They both follow the basic principles of RNN, and we will pay our attention
    to the complicated internal structures in each node. Since the structure is much
    more complicated than general neural nodes, we call it a ‘cell.’ Cells in RNN
    are equivalent to nodes in MLP.'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ae29fb752f390e9724b867b6c19e3ca.png)'
  id: totrans-1064
  prefs: []
  type: TYPE_IMG
- en: (a) Fully-connected neural network
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0a620f9b43ec7601181079a21d5b2780.png)'
  id: totrans-1066
  prefs: []
  type: TYPE_IMG
- en: (b) MLP
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Illustration of standard neural network and multilayer perceptron.
    (a) The basic structure of the fully-connected neural network. The input layer
    receives the raw data or extracted features of brain signals while the output
    layer shows the classification results. The term ‘fully-connected’ denotes each
    node in a specific layer is connected with all the nodes in the previous and next
    layer. (b) MLP could have multiple hidden layers, the more, the deeper. This is
    an example of MLP with two hidden layers, which is the simplest MLP model.'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
- en: 'Long Short-Term Memory (LSTM). Figure [9(a)](#A2.F9.sf1 "In Figure 9 ‣ B.1.2
    Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep Learning Models ‣ Appendix
    B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") shows the structure
    of a single LSTM cell at time $t$ [[255](#bib.bib255)]. The LSTM cell has three
    inputs ($I_{t}$, $O_{t-1}$, and $c_{t-1}$) and two outputs ($c_{t}$ and $O_{t}$).
    The operation is as follows:'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I_{t},O_{t-1},c_{t-1}\rightarrow c_{t},O_{t}$ |  | (7) |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
- en: '$I_{t}$ denotes the input value at time $t$, $O_{t-1}$ denotes the output at
    the previous time (i.e., time $t-1$), and $c_{t-1}$ denotes the hidden state at
    the previous time. $c_{t}$ and $O_{t}$ separately denote the hidden state and
    the output at time $t$. Therefore, we can observe that the output $O_{t}$ at time
    $t$ not only related to the input $I_{t}$ but also related to the information
    at the previous time. In this way, LSTM is empowered to remember the important
    information in the time domain. Moreover, the essential idea of LSTM is to control
    the memory of specific information. For this aim, LSTM cell adopts four gates:
    the input gate, forget gate, output gate, and input modulation gate. Each gate
    is a weight to control how much information can flow through this gate. For example,
    if the weight of the forget gate is zero, the LSTM cell would remember all the
    information passed from the previous time $t-1$; if the weight is one, the LSTM
    cell would remember nothing. The corresponding activation function determines
    the weight. The detailed data flow as follows:'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (8) |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
- en: '|  | $i=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (9) |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
- en: '|  | $o=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (10) |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
- en: '|  | $m=tanh(\mathcal{T}(I_{t},O_{t-1}))$ |  | (11) |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
- en: '|  | $c_{t}=f*c_{t-1}+i*m$ |  | (12) |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
- en: '|  | $h_{t}=o*tanh(c_{t})$ |  | (13) |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
- en: where $i$, $f$, $o$ and $m$ represent the input gate, forget gate, output gate
    and input modulation gate, respectively.
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
- en: 'Gated Recurrent Units (GRU). Another widely used RNN architecture is GRU [[256](#bib.bib256)].
    Similar to LSTM, GRU attempts to exploit the information from the past. GRU does
    not require hidden states, however, it receives temporal information only from
    the output of time $t-1$. Thus, as shown in Figure [9(b)](#A2.F9.sf2 "In Figure
    9 ‣ B.1.2 Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep Learning Models
    ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers"), GRU has two inputs
    ($I_{t}$ and $O_{t-1}$) and one output ($O_{t}$). The mapping can be described
    as:'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I_{t},O_{t-1}\rightarrow O_{t}$ |  | (14) |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
- en: 'GRU contains two gates: reset gate $r$ and update gate $z$. The former decides
    how to combine the input with previous memory. The latter decides how much of
    previous memory to keep around, which is similar to the forget gate of LSTM. The
    data flow as follows:'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (15) |'
  id: totrans-1082
  prefs: []
  type: TYPE_TB
- en: '|  | $r=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (16) |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
- en: '|  | $\bar{O_{t}}=tanh(\mathcal{T}(I_{t},r*O_{t-1}))$ |  | (17) |'
  id: totrans-1084
  prefs: []
  type: TYPE_TB
- en: '|  | $O_{t}=(1-z)*O_{t-1}+z*\bar{O_{t}}$ |  | (18) |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
- en: It can be observed that there’s an intermediate variable $\bar{O_{t}}$ which
    is similar to the hidden state of LSTM. However, $\bar{O_{t}}$ only works on this
    time point and unable to pass to the next time point.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
- en: We here give a brief comparison between LSTM and GRU since they are very similar.
    First, LSTM and GRU have comparable performance as studied by literature. For
    any specific task, it is recommended to try both of them to determine which provides
    better performance. Second, GRU is lightweight since it only has two gates and
    without the hidden state. Therefore, GRU is faster to train and requires few data
    for generalization. Third, in contrast, LSTM generally works better if the training
    dataset is big enough. The reason is that LSTM has better non-linearity than GRU
    since LSTM has two more control gates (input modulation gate and forget gate).
    As a result, LSTM, compared with GRU, is more powerful to discover the latent
    distinct information from large-level training dataset.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7921007345feb6ba5e1901eceb7781fa.png)'
  id: totrans-1088
  prefs: []
  type: TYPE_IMG
- en: (a) RNN
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59b5de172c0da4b202d49b63bbf4385e.png)'
  id: totrans-1090
  prefs: []
  type: TYPE_IMG
- en: (b) CNN
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Illustration of RNN and CNN models. (a) The recurrent procedure of
    the RNN model. This procedure describes the recurrent procedure of a specific
    node in time range $[1,t+1]$. The node at time $t$ receives two inputs variables
    ($I_{t}$ denotes the input at time $t$ and $c_{t-1}$ denotes the hidden state
    at time $t-1$) and exports two variables (the output $O_{t}$ and the hidden state
    $c_{t}$ at time $t$). (b) The paradigm of CNN model which includes two convolutional
    layers, two pooling layers, and one fully-connected layer.'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80598fb00b532542d4a7f3ad5f7879dd.png)'
  id: totrans-1093
  prefs: []
  type: TYPE_IMG
- en: (a) Structure of a LSTM cell.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af96975fd02bc925b90c4f51d86c3692.png)'
  id: totrans-1095
  prefs: []
  type: TYPE_IMG
- en: (b) Structure of a GRU cell.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Illustration of detailed LSTM and GRU cell structures. (a) LSTM cell
    receives three inputs ($I_{t}$ denotes the input at time $t$, $O_{t-1}$ denotes
    the output of previous time, and $c_{t-1}$ denotes the hidden state of the previous
    time) and exports two outputs (the output of this time $O_{t}$ and the hidden
    state of this time $c_{t}$). LSTM cell contains four gates in order to control
    the data flow, which are the input gate, output gate, forget gate, and input modulation
    gate. (b) GRU cell receives two inputs (the input of this time $I_{t}$ and the
    output of the previous time $O_{t-1}$) and exports its output $O_{t}$. GRU cell
    only contains two gates which are the reset gate and the update gate. Unlike the
    hidden state $c_{t}$ in LSTM cell, there is no transmittable hidden state in GRU
    cell except one intermediate variable $\bar{O_{t}}$.'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: B.1.3 Convolutional Neural Networks (CNN)
  id: totrans-1098
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolutional Neural Networks is one of the most popular deep learning models
    specialized in spatial information exploration [[42](#bib.bib42)]. This section
    will briefly introduce the working mechanism of CNN. CNN is widely used to discover
    the latent spatial information in applications such as image recognition, ubiquitous,
    and object searching due to their salient features such as regularized structure,
    good spatial locality, and translation invariance. In the area of brain signal,
    specifically, CNN is supposed to capture the distinctive dependencies among the
    patterns associated with different brain signals.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
- en: 'We present a standard CNN architecture as shown in Figure [8(b)](#A2.F8.sf2
    "In Figure 8 ‣ B.1.2 Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep
    Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers"). The CNN contains one input layer, two convolutional layers with
    each followed by a pooling layer, one fully-connected layer, and one output layer.
    The square patch in each layer shows the processing progress of a specific batch
    of input values. The key to the CNN is to reduce the input data into a form which
    is easier to recognize, with as little information loss as possible. CNN has three
    stacked layers: the convolutional Layer, pooling Layer, and fully-connected Layer.'
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolutional layer is the core block of CNN, which contains a set of filters
    to convolve the input data followed by a nonlinear transformation to extract the
    geographical features. In the deep learning implementation, there are several
    key hyper-parameters should be set in the convolutional layer, like the number
    of filters, the size of each filter, etc. The pooling layer generally follows
    the convolutional layer. The pooling layer aims to reduce the spatial size of
    the features progressively. In this way, it can help to decrease the number of
    parameters (e.g., weights and basis) and the computing burden. There are three
    kinds of pooling operation: max, min, average. Take max pooling for example. The
    pooling operation outputs the maximum value of the pooling area as a result. The
    hyper-parameters in the pooling layer includes the pooling operation, the size
    of the pooling area, the strides, etc. In the fully-connected layer, as in the
    basic neural network, the nodes have full connections to all activations in the
    previous layer.'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNN is the most popular deep learning model in brain signal research, which
    can be used to exploit the latent spatial dependencies among the input brain signals
    like fMRI image, spontaneous EEG, and so on. More details will be reported in
    Section [4](#S4 "4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers").'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Representative Deep Learning Models
  id: totrans-1103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term of representative deep learning refers to use deep neural network for
    representation learning. It aims to learn representations of input data that makes
    it easier to perform a downstream task (e.g., classification, generation, and
    clustering) [[257](#bib.bib257)].
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
- en: The essential blocks of representative deep learning models are autoencoders,
    and restricted Boltzmann machines^(25)^(25)25AE and RBM are generally regarded
    as kind of deep learning although they only have three and two layers, respectively..
    Deep Belief Networks are composed of AE or RBM. The representative models including
    AE, RBM^(26)^(26)26We regard AE, and RBMas representative methods as most researches
    in brain researches adopt them for feature representation., and DBN, are unsupervised
    learning methods. Thus, they can learn the representative features from only the
    input observations $\bm{x}$ without the ground truth $\bm{y}$. In short, representative
    models receive the input data and output a dense representation of the data. There
    are various definitions in different studies for several models (such as DBN,
    Deep RBM, and Deep AE), in this survey, we choose the most understandable definitions
    and will present them in detail in this section.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5db2c0be92218a8d853c16f52534eaca.png)'
  id: totrans-1106
  prefs: []
  type: TYPE_IMG
- en: (a) Autoencoder
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa7588817d9e760e24dee8e516981f64.png)'
  id: totrans-1108
  prefs: []
  type: TYPE_IMG
- en: (b) RBM
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ac17bd242ef2071bb0c12a3ad9ec18a.png)'
  id: totrans-1110
  prefs: []
  type: TYPE_IMG
- en: (c) Deep AE
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/35a17bbd2dc440865cff3fb6536cf42d.png)'
  id: totrans-1112
  prefs: []
  type: TYPE_IMG
- en: (d) Deep RBM
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Illustration of several standard representative deep learning models.
    (a) A basic autoencoder contains one hidden layer. The process from the input
    layer to the hidden layer is an encoder while the process from the hidden layer
    to the output layer is a decoder. (b) Restricted Boltzmann Machine, the encoder
    and the decoder share the same transformation weights. The input layer and the
    output layer are merged into the visible layer. (c) Deep AE with hidden layers.
    Generally, the number of hidden layers is odd, and the middle layer is the learned
    representative features. (d) Deep RBM has one visible layer and multiple hidden
    layers, the last layer is the encoded representation.'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
- en: B.2.1 Autoencoder (AE)
  id: totrans-1115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Figure [10(a)](#A2.F10.sf1 "In Figure 10 ‣ B.2 Representative Deep
    Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers"), A autoencoder is a neural network that has three layers: the
    input layer, the hidden layer, and the output layer [[43](#bib.bib43)]. It differs
    from the standard neural network, in that the AE is trained to reconstruct its
    inputs, which forces the hidden layer to try to learn good representations of
    the inputs.'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: The structure of AE contains two blocks. The first block is called the encoder,
    which embeds the observation to a latent representation (also called ‘code’),
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{h}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (19) |'
  id: totrans-1118
  prefs: []
  type: TYPE_TB
- en: where $\bm{x^{h}}$ represents the hidden layer. The second block is called the
    decoder, which decodes the representation into the original space,
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h}}))$ |  | (20) |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
- en: where $\bm{y^{\prime}}$ represents the output.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: AE forces $\bm{y^{\prime}}$ to be equal to the input $\bm{x}$ and calculates
    the error based on the distance between them. Thus, AE can compute the loss function
    only by $\bm{x}$ without the ground truth $\bm{y}$
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $error=\left\&#124;\bm{y^{\prime}}-\bm{x}\right\&#124;_{2}$ |  | (21)
    |'
  id: totrans-1123
  prefs: []
  type: TYPE_TB
- en: 'Compared to Equation [6](#A2.E6 "In B.1.1 Multi-Layer Perceptron (MLP) ‣ B.1
    Discriminative Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain
    Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers"), this equation does not involve the variable
    $\bm{y}$ because it takes the input $\bm{x}$ as the ground truth. This is why
    AE is able to perform unsupervised learning.'
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, one variant of AE is Deep-AE (D-AE) which has more than one hidden
    layer. We present the structure of D-AE with three hidden layers in Figure [10(c)](#A2.F10.sf3
    "In Figure 10 ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"). From the figure, we can observe
    that there is one more hidden layer in both the encoder and the decoder. The symmetrical
    structure ensures the smoothness of encoding and decoding procedure. Thus, D-AE
    generally has an odd number of hidden layers (e.g., $2n+1$) where the first $n$
    layers belong to the encoder, the $(n+1)$-th layer works as the code which belongs
    to both encoder and decoder, and the last $n$ layers belong to the decoder. The
    data flow of D-AE (Figure [10(c)](#A2.F10.sf3 "In Figure 10 ‣ B.2 Representative
    Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers")) can be represented as'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{h1}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (22) |'
  id: totrans-1126
  prefs: []
  type: TYPE_TB
- en: '|  | $\bm{x^{h2}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (23) |'
  id: totrans-1127
  prefs: []
  type: TYPE_TB
- en: where $\bm{x^{h2}}$ denotes the median hidden layer (the code). Then decode
    the hidden layer, we can get
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{h3}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (24) |'
  id: totrans-1129
  prefs: []
  type: TYPE_TB
- en: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h3}}))$ |  | (25) |'
  id: totrans-1130
  prefs: []
  type: TYPE_TB
- en: 'It is almost the same as AE except that D-AE has more hidden layers. Apart
    from D-AE, AE has many other variants like denoising autoencoder, sparse autoencoder,
    contractive AE, etc. Here we only introduce the D-AE because it is easily confused
    with the AE-based deep belief network. The key difference between them will be
    provided in Section [B.2.3](#A2.SS2.SSS3 "B.2.3 Deep Belief Networks (DBN) ‣ B.2
    Representative Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain
    Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers").'
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
- en: The core idea of AE and its variants is simple, which is that condensing the
    input data $\bm{x}$ into a code $\bm{x^{h}}$ (generally the code layer has lower
    dimension) and then reconstructing the data based on the code. If the reconstructed
    $\bm{y^{\prime}}$ can approximate to the input data $\bm{x}$, it can be demonstrated
    that the condensed code $\bm{x^{h}}$ carries enough information about $\bm{x}$,
    thus, we can regard $\bm{x^{h}}$ as a representation of the input data for future
    operation (e.g., classification).
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
- en: B.2.2 Restricted Boltzmann Machine (RBM)
  id: totrans-1133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Restricted Boltzmann Machine is a stochastic artificial neural network that
    can learn a probability distribution over its set of inputs [[44](#bib.bib44)].
    It contains two layers including one visible layer (input layer) and one hidden
    layer, as shown in Figure [10(b)](#A2.F10.sf2 "In Figure 10 ‣ B.2 Representative
    Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers"). From the figure, we can see that the connection lines between
    the two layers are bidirectional. RBM is a variant of Boltzmann Machine with stronger
    restriction of being without intra-layer connections. In a general Boltzmann machine,
    the nodes in the same hidden layer will connect. Similar to AE, the procedure
    of RBM also includes two steps. The first step condenses the input data from the
    original space to the hidden layer in a latent space. After that, the hidden layer
    is used to reconstruct the input data in an identical way. Compared to AE, RBM
    has a stronger constraint which is that the encoder weights and the decoder weights
    should be equal. We have'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{h}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (26) |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
- en: '|  | $\bm{x^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h}}))$ |  | (27) |'
  id: totrans-1136
  prefs: []
  type: TYPE_TB
- en: In the above two equations, the weights of $\mathcal{T}(\cdot)$ are the same.
    Then, the error for training can be calculated by
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $error=\left\&#124;\bm{x^{\prime}}-\bm{x}\right\&#124;_{2}$ |  | (28)
    |'
  id: totrans-1138
  prefs: []
  type: TYPE_TB
- en: 'We can observe from the Figure [10(d)](#A2.F10.sf4 "In Figure 10 ‣ B.2 Representative
    Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") that the Deep-RBM (D-RBM) is an RBM with multiple hidden layers.
    The input data from the visible layer firstly flow to the first hidden layer and
    then the second hidden layer. Then, the code will flow backward into the visible
    layer for reconstruction.'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/62ff570c8b7729be58be0726bf0b09c8.png)'
  id: totrans-1140
  prefs: []
  type: TYPE_IMG
- en: (a) DBN-AE
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce52725ee523a3224e0d6b7d1664d026.png)'
  id: totrans-1142
  prefs: []
  type: TYPE_IMG
- en: (b) DBN-RBM
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Illustration of deep belief networks. (a) DBN composed of autoencoders.
    DBN-AE contains multiple AE components (in this case, two AE), with the hidden
    layer of the previous AE working as the input layer of the next AE. The hidden
    layer of the last AE is the learned representation. (b) DBN composed of RBM. In
    this illustration, there are two RBM components with the hidden layer of the first
    RBM working as the visible layer of the second RBM. The last hidden layer is the
    encoded representation. While DBN-RBM and D-RBM (Figure [10(d)](#A2.F10.sf4 "In
    Figure 10 ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep Learning
    in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers")) have similar architecture, the former
    is trained greedily while the latter is trained jointly .'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: B.2.3 Deep Belief Networks (DBN)
  id: totrans-1145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Deep Belief Network (DBN) is a stack of simple networks, such as AEs or RBMs
    [[258](#bib.bib258)]. Thus, we divided DBN into DBN-AE (also called stacked AE)
    which is composed of AE and DBN-RBM (also called stacked RBM) which is composed
    of RBM.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [11(a)](#A2.F11.sf1 "In Figure 11 ‣ B.2.2 Restricted Boltzmann
    Machine (RBM) ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"), the DBN-AE contains two AE
    structures while the hidden layer of the first AE works as the input layer of
    the second AE. This diagram has two stages. In the first stage, the input data
    feed into the first AE follows the rules introduced in Section [B.2.1](#A2.SS2.SSS1
    "B.2.1 Autoencoder (AE) ‣ B.2 Representative Deep Learning Models ‣ Appendix B
    Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers"). The reconstruction
    error is calculated and back propagated to adjust the corresponding weights and
    basis. This iteration continues until the AE converges. We get the mapping,'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{1}}\rightarrow\bm{x^{h1}}$ |  | (29) |'
  id: totrans-1148
  prefs: []
  type: TYPE_TB
- en: Then, we move on to the second stage where the learned representative code in
    the hidden layer $\bm{x^{h1}}$ will be used as the input layer of the second AE,
    which is
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{2}}=\bm{x^{h1}}$ |  | (30) |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
- en: and then, after the second AE converges, we have
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x^{2}}\rightarrow\bm{x^{h2}}$ |  | (31) |'
  id: totrans-1152
  prefs: []
  type: TYPE_TB
- en: where $\bm{x^{h2}}$ denotes the hidden layer of the second AE, meanwhile, it
    is the final outcome of the DBN-AE.
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
- en: The core idea of AE is that of learning a representative code with lower dimensionality
    but containing most information of the input data. The idea behind DBN-AE is to
    learn a more representative and purer code.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the DBN-RBM is composed of several single RBM structures. Figure [11(b)](#A2.F11.sf2
    "In Figure 11 ‣ B.2.2 Restricted Boltzmann Machine (RBM) ‣ B.2 Representative
    Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") shows a DBN with two RBMs where the hidden layer of the first
    RBM is used as the visible layer of the second RBM.'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare the DBN-RBM (Figure [11(b)](#A2.F11.sf2 "In Figure 11 ‣ B.2.2 Restricted
    Boltzmann Machine (RBM) ‣ B.2 Representative Deep Learning Models ‣ Appendix B
    Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")) and D-RBM (Figure [10(d)](#A2.F10.sf4
    "In Figure 10 ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")). They almost have the same
    architecture. Moreover, DBN-AE (Figure [11(a)](#A2.F11.sf1 "In Figure 11 ‣ B.2.2
    Restricted Boltzmann Machine (RBM) ‣ B.2 Representative Deep Learning Models ‣
    Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")) and D-AE (Figure [10(c)](#A2.F10.sf3
    "In Figure 10 ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")) have similar architecture.
    The most important difference between the DBN and the deep AE/RBM is that the
    former is trained greedily while the latter is trained jointly. In particular,
    for the DBN, the first AE/RBM is trained first, after it converges, the second
    AE/RBM is trained[[44](#bib.bib44)]. For the deep AE/RBM, jointly training means
    that the whole structure is trained together, no matter how layers it has.'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8792751befd947fff62ea33681e09530.png)'
  id: totrans-1157
  prefs: []
  type: TYPE_IMG
- en: (a) Variational Autoencoder
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f89c94a6a7331d0f4a66aefe3ca2f5c5.png)'
  id: totrans-1159
  prefs: []
  type: TYPE_IMG
- en: (b) GAN
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: Illustration of generative deep learning models. (a) VAE contains
    two hidden layers. The first hidden layer is composed of two components: the expectation
    and the standard deviation, which are learned separately from the input layer.
    The second hidden layer represents the encoded information. $\epsilon$ denotes
    the standard normal distribution. (b) GAN mainly contain two crucial components:
    the generator and the discriminator network. The former receives a latent random
    variable to generate a fake brain signal while the latter receives both the real
    and the generated brain signals and attempts to determine if its generated or
    not. In the are of brain signals, GAN reconstructs or augments data instead of
    classification.'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Generative Deep Learning Models
  id: totrans-1162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative deep learning models are mainly used to generate training samples
    or data augmentation. In other words, generative deep learning models play a supporting
    role in the brain signal field to enhance the training data quality and quantity.
    After the data augmentation, the discriminative models will be employed for the
    classification. This procedure is created to improve the robustness and effectiveness
    of the trained deep learning networks, especially when the training data is limited.
    In short, the generative models receive the input data and output a batch of similar
    data. In this section, we will introduce two typical generative deep learning
    models: variational Autoencoder (VAE) and Generative Adversarial Networks (GAN).'
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: B.3.1 Variational Autoencoder (VAE)
  id: totrans-1164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Variational Autoencoder, proposed in 2013 [[46](#bib.bib46)], is an important
    variant of AE, and one of the most powerful generative algorithms. The standard
    AE and its other variants can be used for representation but fail in generation
    for the reason that the learned code (or representation) may not be continuous.
    Therefore, we cannot generate a random sample which is similar to the input sample.
    In other words, the standard AE does not allow interpolation. Thus, we can replicate
    the input sample but cannot generate a similar one. VAE has one fundamentally
    unique property that separates it from other AEs, and it is this property that
    makes VAE so useful for generative modeling: the latent spaces are designed to
    be continuous which allows easy random sampling and interpolation. Next, we will
    introduce how VAE works.'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the standard AE, VAE can be divided into an encoder and decoder where
    the former embeds the input data to a latent space and the latter transfers the
    data from the latent space to the original space. However, the learned representation
    in the latent space is forced to approximate a prior distribution $\bm{\bar{p(z)}}$
    which is generally set as Standard Gaussian distribution. Based on the reparameterization
    trick [[46](#bib.bib46)], the first hidden layer of VAE is designed to have two
    parts where one denotes the expectation $\bm{\mu}$ and another denotes the standard
    deviation $\bm{\sigma}$, thus we have
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\mu}=\sigma(\mathcal{T}(\bm{x}))$ |  | (32) |'
  id: totrans-1167
  prefs: []
  type: TYPE_TB
- en: '|  | $\bm{\sigma}=\sigma(\mathcal{T}(\bm{x}))$ |  | (33) |'
  id: totrans-1168
  prefs: []
  type: TYPE_TB
- en: Then, the latent code in the hidden layer is not directly calculated but sampled
    from a Gaussian distribution $\mathcal{N}(\bm{\mu},\bm{\sigma}^{2})$. The statistic
    code
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{z}=\bm{\mu}+\bm{\sigma}*\bm{\varepsilon}$ |  | (34) |'
  id: totrans-1170
  prefs: []
  type: TYPE_TB
- en: where $\bm{\varepsilon}\sim\mathcal{N}(\bm{0},\bm{I})$. The representation $\bm{z}$
    is forced to a prior distribution, and the distance $error_{KL}$ is measured by
    Kullback–Leibler divergence,
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $error_{KL}=D_{KL}(z,\bm{\bar{p(z)}})$ |  | (35) |'
  id: totrans-1172
  prefs: []
  type: TYPE_TB
- en: where $\bm{\bar{p(z)}}$ denotes the prior distribution. In the decoder, $\bm{z}$
    is decoded into the output $\bm{y}^{\prime}$,
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{z}))$ |  | (36) |'
  id: totrans-1174
  prefs: []
  type: TYPE_TB
- en: and the reconstruction error is
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $error_{recon}=\left\&#124;\bm{y^{\prime}}-\bm{x}\right\&#124;_{2}$ |  |
    (37) |'
  id: totrans-1176
  prefs: []
  type: TYPE_TB
- en: The overall error for VAE is combined by the DL divergence and the reconstruction
    error,
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $error=error_{KL}+error_{recon}$ |  | (38) |'
  id: totrans-1178
  prefs: []
  type: TYPE_TB
- en: The key point of VAE is that all the latent representations $\bm{z}$ are forced
    to obey the normal distribution. Thus, we can randomly sample a representation
    $\bm{z^{\prime}}\in\bm{\bar{p(z)}}$ from the prior distribution and then reconstruct
    a sample based on $\bm{z^{\prime}}$. This is why VAE is so powerful in generation.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: B.3.2 Generative Adversarial Networks (GAN)
  id: totrans-1180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generative Adversarial Networks [[47](#bib.bib47)] is proposed in 2014 and achieved
    great success in a wide range of research areas (e.g., computer vision and natural
    language processing). GAN is composed of two simultaneously trained neural networks
    with a generator and a discriminator. The generator captures the distribution
    of the input data, and the discriminator is used to estimate the probability that
    a sample came from the training data. The generator aims to generate fake samples
    while the discriminator aims to distinguish whether the sample is genuine. The
    functions of the generator and the discriminator are opposite; that’s why GAN
    is called ‘adversarial.’ After the convergence of both the generator and the discriminator,
    the discriminator ought to be unable to recognize the generated samples. Thus,
    the pre-trained generator can be used to create a batch of samples and use them
    for further operations such as as classification.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [12(b)](#A2.F12.sf2 "In Figure 12 ‣ B.2.3 Deep Belief Networks (DBN)
    ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep Learning in
    Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers") shows the procedure of a standard GAN. The
    generator receives a noise signal $\bm{s}$ which is randomly sampled from a multimodal
    Gaussian distribution and outputs the fake brain signals $\bm{x}_{F}$. The distributor
    receives the real brain signals $\bm{x}_{R}$ and the generated fake sample $\bm{x}_{F}$,
    and then it predicts whether the received sample is real or fake. The internal
    architecture of the generator and discriminator are designed depending on the
    data types and scenarios. For instance, we can build the GAN by convolutional
    layers on fMRI images since CNN has an excellent ability to extract spatial features.
    The discriminator and the generator are trained jointly. After the convergence,
    numerous brain signals $\bm{x}_{G}$ can be created by the generator. Thus, the
    training set is enlarged from $\bm{x}_{R}$ to $\{\bm{x}_{R},\bm{x}_{G}\}$ to train
    a more effective and robust classifier.'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Hybrid Model
  id: totrans-1183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hybrid deep learning models refers to models which are composed of at least
    two deep basic learning models where the basic model is a discriminative, representative,
    or generative deep learning model. Hybrid models comprise two subcategories based
    on their targets: classification-aimed (CA) hybrid models and the non-classification-aimed
    (NCA) hybrid models.'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the deep learning related studies in brain signal area are focused
    on the first category. Based on the existing literature, the representative and
    generative models are employed to enhance the discriminative models. The representative
    models can provide more informative and low dimensional features for the discrimination
    while the generative models can help to augment the training data quality and
    quantity which supply more information for the classification. The CA hybrid models
    can be further subdivided into: 1) several discriminative models combined to extract
    more distinctive and robust features (e.g., CNN+RNN); 2) representative model
    followed by a discriminative model (e.g., DBN+MLP); 3) generative + representative
    model followed by a discriminative model; 4) generative + representative model
    followed by a non-deep learning classifier. In which, a representative model followed
    by a non-deep learning classifier is regarded as a representative deep learning
    model.'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: A few NCA hybrid models aim for brain signal reconstruction. For example, St-yves
    et al. [[259](#bib.bib259)] adopted GAN to reconstruct visual stimuli based on
    fMRI images.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
