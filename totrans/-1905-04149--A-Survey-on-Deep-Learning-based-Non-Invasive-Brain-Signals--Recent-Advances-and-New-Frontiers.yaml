- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:06:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:06:22'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1905.04149] A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1905.04149] 一项基于深度学习的非侵入性大脑信号调查：近期进展与新前沿'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1905.04149](https://ar5iv.labs.arxiv.org/html/1905.04149)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1905.04149](https://ar5iv.labs.arxiv.org/html/1905.04149)
- en: Accepted
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已接受
- en: October 2020
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2020年10月
- en: 'A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一项基于深度学习的非侵入性大脑信号调查：近期进展与新前沿
- en: Xiang Zhang^(1,5), Lina Yao¹, Xianzhi Wang², Jessica Monaghan³, David McAlpine³,
    Yu Zhang⁴ ¹University of New South Wales, Australia
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 张翔^(1,5), 姚丽娜¹, 王显之², 杰西卡·莫纳汉³, 大卫·麦克阿尔平³, 张宇⁴ ¹新南威尔士大学，澳大利亚
- en: ²University of Technology Sydney, Australia
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²悉尼科技大学，澳大利亚
- en: ³Macquarie university, Australia
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ³麦考瑞大学，澳大利亚
- en: ⁴Lehigh University, USA
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴利哈伊大学，美国
- en: ⁵Harvard University, USA [xiang_zhang@hms.harvard.edu,
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵哈佛大学，美国 [xiang_zhang@hms.harvard.edu,
- en: lina.yao@unsw.edu.au, xianzhi.wang@uts.edu.au, {jessica.monaghan,david.mcalpine}@mq.edu.au,
    yuzi20@lehigh.edu](mailto:xiang_zhang@hms.harvard.edu,%20)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: lina.yao@unsw.edu.au, xianzhi.wang@uts.edu.au, {jessica.monaghan,david.mcalpine}@mq.edu.au,
    yuzi20@lehigh.edu](mailto:xiang_zhang@hms.harvard.edu,%20)
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Brain signals refer to the biometric information collected from the human brain.
    The research on brain signals aims to discover the underlying neurological or
    physical status of the individuals by signal decoding. The emerging deep learning
    techniques have improved the study of brain signals significantly in recent years.
    In this work, we first present a taxonomy of non-invasive brain signals and the
    basics of deep learning algorithms. Then, we provide the frontiers of applying
    deep learning for non-invasive brain signals analysis, by summarizing a large
    number of recent publications. Moreover, upon the deep learning-powered brain
    signal studies, we report the potential real-world applications which benefit
    not only disabled people but also normal individuals. Finally, we discuss the
    opening challenges and future directions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑信号指的是从人脑中收集的生物识别信息。对大脑信号的研究旨在通过信号解码发现个体潜在的神经或身体状态。近年来， emerging 深度学习技术显著改善了大脑信号的研究。在这项工作中，我们首先介绍了非侵入性大脑信号的分类以及深度学习算法的基础知识。然后，通过总结大量近期出版的文献，我们提供了应用深度学习进行非侵入性大脑信号分析的前沿进展。此外，基于深度学习驱动的大脑信号研究，我们报告了潜在的现实世界应用，这些应用不仅惠及残疾人，也惠及普通人。最后，我们讨论了开放挑战和未来方向。
- en: '^†^†: J. Neural Eng.\ioptwocol'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '^†^†: J. Neural Eng.\ioptwocol'
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Brain signals measure the instinct biometric information from the human brain,
    which reflects the user’s passive or active mental state. Through precise brain
    signal decoding, we can recognize the underlying psychological and physical status
    of the user and further improve his/her life quality. Based on the signal collection,
    brain signals contain invasive signals and non-invasive signals. The former are
    acquired by electrodes deployed under the scalp while the latter are collected
    upon human scalp without electrodes being inserted. In this survey, we mainly
    consider non-invasive brain signals¹¹1Without specification, the brain signals
    mentioned in this work refer to non-invasive signals..
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑信号测量的是来自人脑的本能生物识别信息，反映了用户的被动或主动心理状态。通过精确的大脑信号解码，我们可以识别用户潜在的心理和身体状态，并进一步改善其生活质量。根据信号采集方式，大脑信号包含侵入性信号和非侵入性信号。前者通过部署在头皮下的电极获取，而后者则是在不插入电极的情况下从头皮上收集。在本调查中，我们主要考虑非侵入性大脑信号¹¹1如未指定，本工作中提到的大脑信号指的是非侵入性信号。.
- en: 1.1 General Workflow
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 一般工作流程
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1.1 General Workflow ‣ 1 Introduction ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers")
    shows the general paradigm of brain signal decoding, which receives brain signals
    and produces the user’s latent informatics. The workflow includes several key
    components: brain signal collection, signal preprocessing, feature extraction,
    classification, and data analysis. The brain signals are collected from humans
    and sent to the preprocessing component for denoising and enhancement. Then, the
    discriminating features are extracted from the processed signals and sent to the
    classifier for further analysis.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S1.F1 "图 1 ‣ 1.1 一般工作流程 ‣ 1 引言 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿") 显示了脑信号解码的一般范式，该范式接收脑信号并生成用户的潜在信息。工作流程包括几个关键组件：脑信号采集、信号预处理、特征提取、分类和数据分析。脑信号从人类身上采集并发送到预处理组件进行去噪和增强。然后，从处理后的信号中提取区分特征，并送到分类器进行进一步分析。
- en: 'The collection methods differ from signal to signal. For example, EEG signals
    measure the voltage fluctuation resulting from ionic current within the neurons
    of the brain. Collecting EEG signals requires placing a series of electrodes on
    the scalp of the human head to record the electrical activity of the brain. Since
    the ionic current generated within the brain is measured at the scalp, obstacles
    (e.g., skull) greatly decrease the signal quality—the fidelity of the collected
    EEG signals, measured as Signal-to-Noise Ratio (SNR), is only approximately 5%
    of that of original brain signals [[1](#bib.bib1)]. The collection methods of
    more non-invasive signals can be found in Appendix [A](#A1 "Appendix A Non-invasive
    Brain Signals ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 收集方法因信号而异。例如，脑电信号测量的是脑内离子电流引起的电压波动。收集脑电信号需要在头皮上放置一系列电极以记录脑部的电活动。由于在头皮上测量的离子电流，障碍物（例如，颅骨）大大降低了信号质量——所收集的脑电信号的保真度，按信噪比（SNR）测量，仅为原脑信号的约5%[[1](#bib.bib1)]。更多非侵入性信号的收集方法见附录 [A](#A1
    "附录 A 非侵入性脑信号 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")。
- en: Therefore, brain signals are usually preprocessed before feature extraction
    to increase the SNR. The preprocessing component contains multiple steps such
    as signal cleaning (smoothing the noisy signals or resolving the inconsistencies),
    signal normalization (normalizing each channel of the signals along time-axis),
    signal enhancement (removing direct current), and signal reduction (presenting
    a reduced representation of the signal).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，脑信号通常在特征提取前进行预处理，以提高SNR。预处理组件包含多个步骤，例如信号清理（平滑噪声信号或解决不一致问题）、信号归一化（沿时间轴对每个通道的信号进行归一化）、信号增强（去除直流分量）和信号降维（呈现信号的简化表示）。
- en: Feature extraction refers to the process of extracting discriminating features
    from the input signals through domain knowledge. Traditional features are extracted
    from time-domain (e.g., variance, mean value, kurtosis), frequency-domain (e.g.,
    fast Fourier transform), and time-frequency domains (e.g., discrete wavelet transform).
    They will enrich distinguishable information regarding user intention. Feature
    extraction is highly dependent on the domain knowledge. For example, neuroscience
    knowledge is required to extract distinctive features from motor imagery EEG signals.
    Manual feature extraction is also time-consuming and difficult. Recently, deep
    learning provides a better option to automatically extract distinguishable features.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是指通过领域知识从输入信号中提取具有区分性的特征的过程。传统特征从时域（例如，方差、均值、峭度）、频域（例如，快速傅里叶变换）和时频域（例如，离散小波变换）中提取。这些特征将丰富有关用户意图的可区分信息。特征提取高度依赖于领域知识。例如，从运动意象脑电信号中提取特征需要神经科学知识。人工特征提取也耗时且困难。最近，深度学习提供了一种更好的自动提取可区分特征的选择。
- en: The classification component refers to the machine learning algorithms that
    classify the extracted features into logical control signals recognizable by external
    devices. Deep learning algorithms are shown to be more powerful than traditional
    classifiers [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 分类组件指的是将提取的特征分类为外部设备能够识别的逻辑控制信号的机器学习算法。深度学习算法被证明比传统分类器更强大[[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]。
- en: '![Refer to caption](img/de06841302b89e8bac0c8089d93e194b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/de06841302b89e8bac0c8089d93e194b.png)'
- en: 'Figure 1: Generally workflow of brain signal analysis. It is named as a Brain-Computer
    Interface if the classified signal are used to control smart equipment (dashed
    lines).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：脑信号分析的一般工作流程。如果分类信号用于控制智能设备（虚线），则称为脑-计算机接口。
- en: The classification results reflect the user’s psychological or physical status
    and can inspire further information analysis. This is widely used in real-world
    applications such as neurological disorder diagnosis, emotion measuring, and driving
    fatigue detection. Appropriate treatment, therapy, and precaution could be conducted
    based on the analysis results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分类结果反映了用户的心理或身体状态，并可以激发进一步的信息分析。这在实际应用中广泛使用，如神经疾病诊断、情绪测量和驾驶疲劳检测。可以根据分析结果进行适当的治疗、疗法和预防措施。
- en: 'In specific, the system is called a Brain-Computer Interface (BCI) while the
    decoded brain signals are converted into digital commands to control the smart
    equipment and react with the user (dashed lines in Figure [1](#S1.F1 "Figure 1
    ‣ 1.1 General Workflow ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")). BCI²²2Apart from BCI, there
    are a number of similar terms to define the system that machines are directly
    controlled by human brain signals, like Brain-Machine Interface (BMI), Brain Interface
    (BI), Direct Brain Interface (DBI), Adaptive Brain Interface (ABI), and so on.
    systems interpret the human brain patterns into messages or commands to communicate
    with the outer world [[5](#bib.bib5)]. BCI is generally a closed-loop system with
    an external device (e.g., wheelchair and robotic arm), which can directly serve
    the user. In contrast, brain signal analysis doesn’t require a specific device
    as long as the analysis results can benefit society and individuals.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，这个系统称为脑机接口（BCI），将解码后的脑信号转换为数字命令，以控制智能设备并与用户互动（图[1](#S1.F1 "Figure 1 ‣
    1.1 General Workflow ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers)中的虚线）。除了BCI，还有许多类似的术语来定义由人脑信号直接控制的系统，如脑-机器接口（BMI）、脑接口（BI）、直接脑接口（DBI）、自适应脑接口（ABI）等。系统将人脑模式解释为消息或命令，以与外界沟通[[5](#bib.bib5)]。BCI通常是一个闭环系统，配有外部设备（例如，轮椅和机器人手臂），可以直接为用户服务。相比之下，脑信号分析不需要特定设备，只要分析结果能对社会和个人有益即可。'
- en: 'In this survey, we summarize the state-of-the-art studies which adopt deep
    learning models: 1) for feature extraction only; 2) for classification only; 3)
    for both feature extraction and classification. The details will be introduced
    in Section [4](#S4 "4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers").
    Brain signal underpins many novel applications that are important to people’s
    daily life. For example, the brain signal-based user identification system, with
    high fake-resistance, allows normal people to enjoy enhanced entertainment and
    security [[6](#bib.bib6)]; for people with psychological/physical deceases or
    disabilities, brain signals enable them to control smart device such as wheelchairs,
    home appliances, and robots. We present a wide range of deep learning-based brain
    signal applications in Section [5](#S5 "5 Brain Signal-based Applications ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '在本次调查中，我们总结了采用深度学习模型的最先进研究：1）仅用于特征提取；2）仅用于分类；3）同时用于特征提取和分类。具体细节将在第[4](#S4 "4
    State-of-The-Art DL Techniques for Brain Signals ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")节中介绍。脑信号支撑了许多对人们日常生活重要的新应用。例如，基于脑信号的用户识别系统具有高抗伪造性，使普通人可以享受更好的娱乐和安全[[6](#bib.bib6)]；对于有心理/身体疾病或残疾的人，脑信号使他们能够控制智能设备，如轮椅、家用电器和机器人。我们在第[5](#S5
    "5 Brain Signal-based Applications ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")节中展示了广泛的深度学习基础的脑信号应用。'
- en: 1.2 Why Deep Learning?
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 为什么选择深度学习？
- en: Although traditional brain signal system has made tremendous progress [[7](#bib.bib7),
    [8](#bib.bib8)], it still faces significant challenges. First, brain signals are
    easily corrupted by various biological (e.g., eye blinks, muscle artifacts, fatigue,
    and the concentration level) and environmental artifacts (e.g., noises) [[7](#bib.bib7)].
    Therefore, it is crucial to distill informative data from corrupted brain signals
    and build a robust system that works in different situations. Second, it faces
    the low SNR of non-stationary electrophysiological brain signals [[9](#bib.bib9)].
    The low SNR cannot be easily addressed by traditional preprocessing or feature
    extraction methods due to the time complexity of those method and the risk of
    information loss [[10](#bib.bib10)]. Third, feature extraction highly depends
    on human expertise in the specific domain. For example, it requires the basic
    biological knowledge to investigate sleep state through Electroencephalogram (EEG)
    signals. Human experience may help on certain aspects but fall insufficient in
    more general circumstances. An automatic feature extraction method is highly desirable.
    Moreover, most existing machine learning research focuses on static data and therefore,
    cannot classify rapidly changing brain signals accurately. For instance, the state-of-the-art
    classification accuracy for multi-class motor imagery EEG is generally below 80%
    [[11](#bib.bib11)]. It requires novel learning methods to deal with dynamical
    data streams in brain signal systems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管传统的脑信号系统已经取得了巨大进展[[7](#bib.bib7), [8](#bib.bib8)]，但仍面临重大挑战。首先，脑信号容易受到各种生物（例如，眼睛眨动、肌肉伪影、疲劳和集中力）和环境伪影（例如，噪音）[[7](#bib.bib7)]的干扰。因此，从受污染的脑信号中提取有用的数据，并建立一个在不同情况下都能有效工作的强大系统是至关重要的。其次，它面临着非稳态电生理脑信号的低信噪比[[9](#bib.bib9)]。由于这些方法的时间复杂度和信息丢失的风险，传统的预处理或特征提取方法无法轻易解决低信噪比的问题[[10](#bib.bib10)]。第三，特征提取高度依赖于特定领域的人类专业知识。例如，通过脑电图（EEG）信号调查睡眠状态需要基本的生物学知识。人类经验可能在某些方面有所帮助，但在更一般的情况下可能不够充分。自动特征提取方法是非常需要的。此外，大多数现有的机器学习研究集中在静态数据上，因此无法准确分类快速变化的脑信号。例如，多类运动想象EEG的最先进分类准确率通常低于80%[[11](#bib.bib11)]。这需要新的学习方法来处理脑信号系统中的动态数据流。
- en: Until now, deep learning has been applied extensively in brain signal applications
    and shown success in addressing the above challenges [[12](#bib.bib12), [13](#bib.bib13)].
    Deep learning has two advantages. First, it works directly on raw brain signals,
    thus avoiding the time-consuming preprocessing and feature extraction. Second,
    deep neural networks can capture both representative high-level features and latent
    dependencies through deep structures.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，深度学习已经广泛应用于脑信号应用中，并在解决上述挑战方面取得了成功[[12](#bib.bib12), [13](#bib.bib13)]。深度学习有两个优点。首先，它直接处理原始脑信号，从而避免了耗时的预处理和特征提取。其次，深度神经网络可以通过深层结构捕捉代表性高层特征和潜在的依赖关系。
- en: 'Table 1: The existing survey on brain signals in the last decade. The column
    ‘Comprehensiveness’ indicates whether the survey covers all subcategories of non-invasive
    brain signals or not. MI EEG refers to Motor Imagery EEG signals.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：过去十年脑信号的现有调查。列‘综合性’指示调查是否覆盖所有非侵入性脑信号的子类别。MI EEG指的是运动想象EEG信号。
- en: '| No. | Reference | Comprehensiveness | Signal | Deep Learning |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 参考 | 综合性 | 信号 | 深度学习 |'
- en: '&#124; Publication &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出版 &#124;'
- en: '&#124; Time &#124;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间 &#124;'
- en: '| Area |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 领域 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2 | [[14](#bib.bib14)] | No | fMRI | Yes | 2018 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 2 | [[14](#bib.bib14)] | 否 | fMRI | 是 | 2018 |'
- en: '&#124; Mental Disease &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精神疾病 &#124;'
- en: '&#124; Diagnosis &#124;'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 诊断 &#124;'
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 3 | [[11](#bib.bib11)] | Partial | EEG (MI EEG, P300) | No | 2007 | Classification
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 3 | [[11](#bib.bib11)] | 部分 | EEG（MI EEG, P300） | 否 | 2007 | 分类 |'
- en: '| 4 | [[5](#bib.bib5)] | Partial | EEG (MI EEG, P300) | Partial | 2018 | Classification
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 4 | [[5](#bib.bib5)] | 部分 | EEG（MI EEG, P300） | 部分 | 2018 | 分类 |'
- en: '| 5 | [[15](#bib.bib15)] | Partial | EEG (ERD, P300, SSVEP, VEP, AEP) | No
    | 2007 |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 5 | [[15](#bib.bib15)] | 部分 | EEG（ERD, P300, SSVEP, VEP, AEP） | 否 | 2007
    |  |'
- en: '| 6 | [[16](#bib.bib16)] | No | MRI, CT | Partial | 2017 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 6 | [[16](#bib.bib16)] | 否 | MRI, CT | 部分 | 2017 |'
- en: '&#124; Medical Image &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医学影像 &#124;'
- en: '&#124; Analysis &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分析 &#124;'
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 7 | [[17](#bib.bib17)] | No | EEG |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 7 | [[17](#bib.bib17)] | 否 | EEG |'
- en: '&#124; Yes &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '| 2019 |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |  |'
- en: '| 8 | [[8](#bib.bib8)] | No | EEG | No | 2007 | Signal Processing |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 8 | [[8](#bib.bib8)] | 否 | EEG | 否 | 2007 | 信号处理 |'
- en: '| 9 | [[18](#bib.bib18)] | Partial | EEG | No | 2016 | BCI Applications |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 9 | [[18](#bib.bib18)] | 部分 | EEG | 否 | 2016 | BCI 应用 |'
- en: '| 10 | [[7](#bib.bib7)] | Yes |  | No | 2015 |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 10 | [[7](#bib.bib7)] | 是 |  | 否 | 2015 |  |'
- en: '| 11 | [[19](#bib.bib19)] | No | EEG | Partial | 2018 |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 11 | [[19](#bib.bib19)] | 否 | EEG | 部分 | 2018 |  |'
- en: '| 12 | [[20](#bib.bib20)] | No | EEG, fMRI | No | 2015 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 12 | [[20](#bib.bib20)] | 否 | EEG, fMRI | 否 | 2015 |'
- en: '&#124; Neurorehabilitation &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 神经康复 &#124;'
- en: '&#124; of Stroke &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中风的 &#124;'
- en: '|'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 13 | [[21](#bib.bib21)] | No | MI EEG | No | 2015 |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 13 | [[21](#bib.bib21)] | 否 | MI EEG | 否 | 2015 |  |'
- en: '| 14 | [[22](#bib.bib22)] | No | fMRI | No | 2014 |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 14 | [[22](#bib.bib22)] | 否 | fMRI | 否 | 2014 |  |'
- en: '| 15 | [[23](#bib.bib23)] | No | ERP (P300) | No | 2017 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 15 | [[23](#bib.bib23)] | 否 | ERP (P300) | 否 | 2017 |'
- en: '&#124; Applications &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用 &#124;'
- en: '&#124; of ERP” &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ERP 的 &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 16 | [[24](#bib.bib24)] | No | fMRI | Yes | 2018 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 16 | [[24](#bib.bib24)] | 否 | fMRI | 是 | 2018 |'
- en: '&#124; Applications &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用 &#124;'
- en: '&#124; of fMRI &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; fMRI 的 &#124;'
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 17 | [[25](#bib.bib25)] | No | ERP | No | 2017 | Classification |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 17 | [[25](#bib.bib25)] | 否 | ERP | 否 | 2017 | 分类 |'
- en: '| 18 | [[26](#bib.bib26)] | Partial | EEG | No | 2019 | Brain Biometrics |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 18 | [[26](#bib.bib26)] | 部分 | EEG | 否 | 2019 | 脑生物识别 |'
- en: '| 19 | [[27](#bib.bib27)] | Partial | EEG | No | 2018 | BCI Paradigms |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 19 | [[27](#bib.bib27)] | 部分 | EEG | 否 | 2018 | BCI范式 |'
- en: '| 20 | Current Study | Yes |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 当前研究 | 是 |'
- en: '&#124; EEG and the subcategories, &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EEG 及其子类别， &#124;'
- en: '&#124; fNIRS, fMRI, MEG &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; fNIRS, fMRI, MEG &#124;'
- en: '| Yes |  |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 是 |  |  |'
- en: 1.3 Why this Survey is Necessary?
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 为什么这个调查是必要的？
- en: 'We conduct this survey for three reasons. First, there lacks a comprehensive
    survey on the non-invasive brain signals. Table [1](#S1.T1 "Table 1 ‣ 1.2 Why
    Deep Learning? ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers") shows a summary of the existing
    survey on brain signals. As our best knowledge, the limited existing surveys [[14](#bib.bib14),
    [24](#bib.bib24), [7](#bib.bib7), [11](#bib.bib11), [5](#bib.bib5), [8](#bib.bib8),
    [15](#bib.bib15)] only focus on partial EEG signals. For example, Lotte et al.
    [[11](#bib.bib11)] and Wang et al. [[18](#bib.bib18)] focus on general EEG without
    analyzing EEG subtypes; Cecotti et al. [[28](#bib.bib28)] focus on Event-Related
    Potentials (ERP); Haseer et al. [[29](#bib.bib29)] focus on functional near-infrared
    spectroscopy (fNIRS); Mason et al. [[15](#bib.bib15)] brief the neurological phenomenons
    like event-related desynchronization (ERD), P300, SSVEP, Visual Evoked Potentials
    (VEP), Auditory Evoked Potentials (AEP) but have not organized them systematically;
    Abdulkader et al. [[7](#bib.bib7)] present a topology of brain signals but have
    not mentioned spontaneous EEG and Rapid Serial Visual Presentation (RSVP); Lotte
    et al. [[5](#bib.bib5)] have not considered ERD and RSVP; VEP should be a subtype
    of ERP in [[8](#bib.bib8)]. Ahn et al. [[21](#bib.bib21)] review the performance
    variation in MI-EEG based BCI systems. Roy et al. [[17](#bib.bib17)] list some
    deep learning-based EEG studies but present little technical inspirations and
    have less analysis on deep learning algorithms, they also failed to investigate
    other non-invasive brain signals beyond EEG. In particular, compared to [[17](#bib.bib17)],
    this work provides a better introduction of deep learning including the basic
    concepts, algorithms, and popular models (Section [3](#S3 "3 Overview on Deep
    Learning Models ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers") and Appendix [B](#A2 "Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")). Moreover, this paper discusses
    the high-level guidelines in brain signal analysis in terms of the brain signal
    paradigms, the suitable deep learning frameworks and the promising real-world
    applications (Section [6](#S6 "6 Analysis and Guidelines ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行这项调查有三个原因。首先，现有的关于非侵入性脑信号的综合调查尚缺乏。表[1](#S1.T1 "Table 1 ‣ 1.2 Why Deep Learning?
    ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers")展示了现有脑信号调查的总结。根据我们所知，有限的现有调查[[14](#bib.bib14),
    [24](#bib.bib24), [7](#bib.bib7), [11](#bib.bib11), [5](#bib.bib5), [8](#bib.bib8),
    [15](#bib.bib15)]仅关注部分脑电图（EEG）信号。例如，Lotte等人[[11](#bib.bib11)]和Wang等人[[18](#bib.bib18)]关注的是一般的EEG，而未分析EEG的亚型；Cecotti等人[[28](#bib.bib28)]关注事件相关电位（ERP）；Haseer等人[[29](#bib.bib29)]关注功能性近红外光谱（fNIRS）；Mason等人[[15](#bib.bib15)]简要介绍了神经现象，如事件相关去同步化（ERD）、P300、SSVEP、视觉诱发电位（VEP）、听觉诱发电位（AEP），但未进行系统整理；Abdulkader等人[[7](#bib.bib7)]展示了脑信号的拓扑结构，但未提及自发EEG和快速序列视觉呈现（RSVP）；Lotte等人[[5](#bib.bib5)]没有考虑ERD和RSVP；在[[8](#bib.bib8)]中，VEP应属于ERP的一个亚型。Ahn等人[[21](#bib.bib21)]回顾了基于MI-EEG的BCI系统中的性能变化。Roy等人[[17](#bib.bib17)]列出了一些基于深度学习的EEG研究，但技术启示较少，对深度学习算法的分析较少，也未研究EEG之外的其他非侵入性脑信号。特别是，与[[17](#bib.bib17)]相比，本文提供了更好的深度学习介绍，包括基本概念、算法和流行模型（第[3](#S3
    "3 Overview on Deep Learning Models ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")节和附录[B](#A2 "Appendix B Basic
    Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")）。此外，本文还讨论了脑信号分析中的高级指南，包括脑信号范式、合适的深度学习框架和有前景的实际应用（第[6](#S6
    "6 Analysis and Guidelines ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers")节）。'
- en: Second, few research has investigated the association between deep learning
    ([[30](#bib.bib30), [31](#bib.bib31)]) and brain signals ([[32](#bib.bib32), [7](#bib.bib7),
    [11](#bib.bib11), [5](#bib.bib5), [8](#bib.bib8), [15](#bib.bib15)]). To the best
    of our knowledge, this paper is in the first batch of comprehensive survey on
    recent advances on deep learning-based brain signals. We also point out frontiers
    and promising directions in this area.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，针对深度学习（[[30](#bib.bib30), [31](#bib.bib31)]）与脑信号（[[32](#bib.bib32), [7](#bib.bib7),
    [11](#bib.bib11), [5](#bib.bib5), [8](#bib.bib8), [15](#bib.bib15)]）之间的关联，已有的研究较少。根据我们所知，本文是关于基于深度学习的脑信号最近进展的首批综合调查之一。我们还指出了该领域的前沿和有前景的方向。
- en: Lastly, the existing surveys focus on specific areas or applications and lack
    an overview of broad scenarios. For example, Litjens et al. [[16](#bib.bib16)]
    summarize several deep neural network concepts aiming at medical image analysis;
    Soekadar et al. [[20](#bib.bib20)] review the BCI systems and machine learning
    methods for stroke-related motor paralysis based on Sensori-Motor Rhythms (SMR);
    Vieira et al. [[33](#bib.bib33)] investigate the application of brain signals
    on the neurological disorder and psychiatric.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，现有的调查集中于特定领域或应用，缺乏广泛场景的概述。例如，Litjens等人[[16](#bib.bib16)]总结了几个深度神经网络概念，旨在医学图像分析；Soekadar等人[[20](#bib.bib20)]回顾了基于感官运动节律（SMR）的脑机接口系统和用于中风相关运动瘫痪的机器学习方法；Vieira等人[[33](#bib.bib33)]调查了脑信号在神经疾病和精神病学中的应用。
- en: 1.4 Our Contributions
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 我们的贡献
- en: 'This survey can mainly benefit: 1) the researchers with computer science background
    who are interested in the brain signal research; 2) the biomedical/medical/neuroscience
    experts who want to adopt deep learning techniques to solve problems in basic
    science.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查主要受益于：1）对脑信号研究感兴趣的计算机科学背景研究人员；2）希望采用深度学习技术解决基础科学问题的生物医学/医学/神经科学专家。
- en: 'To our best knowledge, this survey is the first comprehensive survey of the
    recent advances and frontiers of deep learning-based brain signal analysis. To
    this end, we have summarized over 200 contributions, most of which were published
    in the last five years. We make several key contributions in this survey:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，本调查是关于基于深度学习的脑信号分析的最新进展和前沿的首次全面调查。为此，我们总结了200多项贡献，其中大多数发表于过去五年。我们在本调查中做出了一些关键贡献：
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review brain signals and deep learning techniques to help readers gain a
    comprehensive understanding of this area of research.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了脑信号和深度学习技术，帮助读者全面了解这一研究领域。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss the popular deep learning techniques and state-of-the-art models
    for brain signals, providing practical guidelines for choosing the suitable deep
    learning models given a specific subtype of signal.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了用于脑信号的流行深度学习技术和最先进的模型，提供了选择适当深度学习模型的实用指南，针对特定类型的信号。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review the applications of deep learning-based brain signal analysis and
    highlight some promising topics for future research.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了基于深度学习的脑信号分析的应用，并突出了未来研究的一些有前景的主题。
- en: 'The rest of this survey is structured as followed. Section [2](#S2 "2 Brain
    Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers") briefly introduces an taxonomy of brain signals
    in order to help the reader build a big picture in this field. Section [3](#S3
    "3 Overview on Deep Learning Models ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers") overviews the commonly used
    deep learning models to present the basic knowledge for researchers (e.g., neurological
    and biomedical scholars ) who are not familiar with deep learning. Section [4](#S4
    "4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") presents the state-of-the-art
    deep learning techniques for brain signals and Section [5](#S5 "5 Brain Signal-based
    Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers") discusses the applications related to brain signals.
    Section [6](#S6 "6 Analysis and Guidelines ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers") provides a detailed analysis
    and gives guidelines for choosing appropriate deep learning models based on the
    specific brain signal. Section [7](#S7 "7 Open Issues ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") points out the
    opening challenges and future directions. Finally, Section [8](#S8 "8 Conclusion
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") gives the concluding remarks.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分结构如下。第[2](#S2 "2 脑成像技术 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")节简要介绍了脑信号的分类，以帮助读者在该领域建立一个宏观的概念。第[3](#S3
    "3 深度学习模型概述 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")节概述了常用的深度学习模型，为不熟悉深度学习的研究人员（例如神经学和生物医学学者）提供基本知识。第[4](#S4
    "4 脑信号的最先进深度学习技术 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")节介绍了脑信号的最先进深度学习技术，第[5](#S5 "5 基于脑信号的应用
    ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")节讨论了与脑信号相关的应用。第[6](#S6 "6 分析与指南 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")节提供了详细的分析，并给出了基于特定脑信号选择适当深度学习模型的指南。第[7](#S7
    "7 开放问题 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")节指出了当前面临的挑战和未来方向。最后，第[8](#S8 "8 结论 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")节给出了总结性评论。
- en: 2 Brain Imaging Techniques
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 脑成像技术
- en: '![Refer to caption](img/5fa71297b1bb5047db5e16b541344b4f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5fa71297b1bb5047db5e16b541344b4f.png)'
- en: 'Figure 2: The taxonomy of non-invasive brain signals. The dashed quadrilaterals
    (RAVP, SEP, SSAEP, and SSSEP) are not included in this survey because there is
    no existing work focusing on them involving deep learning algorithms. P300, which
    is a positive potential recorded approximately 300 ms after the onset of presented
    stimuli, is not listed in this signal tree because it is included by ERP (which
    refers to all the potentials after the presented stimuli). In this classification,
    other brain imaging technique beyond EEG (e.g., MEG and fNIRS) could also include
    visual/auditory tasks theoretically, but we omitted them since there is no existing
    work adopting deep learning on these tasks.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：非侵入性脑信号的分类。虚线四边形（RAVP、SEP、SSAEP 和 SSSEP）不包括在本调查中，因为没有现有工作关注这些内容，并涉及深度学习算法。P300
    是一种在呈现刺激后约 300 毫秒记录的正向电位，在此信号树中未列出，因为它已被 ERP 包含（ERP 指的是所有呈现刺激后的电位）。在此分类中，除了 EEG
    之外的其他脑成像技术（例如 MEG 和 fNIRS）理论上也可以包括视觉/听觉任务，但我们省略了它们，因为尚无采用深度学习处理这些任务的现有工作。
- en: 'In this section, we present a brief introduction of typical non-invasive brain
    imaging techniques. More fundamental details about non-invasive brain signal (e.g.,
    concepts, characteristics, advantages, and drawbacks) are provided in Appendix [A](#A1
    "Appendix A Non-invasive Brain Signals ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们简要介绍了典型的非侵入性脑成像技术。关于非侵入性脑信号（例如概念、特征、优点和缺点）的更多基本细节请参见附录[A](#A1 "附录 A
    非侵入性脑信号 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")。
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep
    Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers")
    shows a taxonomy of non-invasive brain signals based on the signal collection
    method. Non-invasive signals divides into Electroencephalogram (EEG), Functional
    near-infrared spectroscopy (fNIRS), Functional magnetic resonance imaging (fMRI),
    and Magnetoencephalography (MEG) [[34](#bib.bib34)]. Table [2](#S2.T2 "Table 2
    ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers") summarizes the characteristics of
    various brain signals. In this survey, we mainly focus on EEG signals and its
    subcategories because they dominate the non-invasive signals. EEG monitors the
    voltage fluctuations generated by an electrical current within human neurons.
    The electrodes attached on scalp can measure various types of EEG signals, including
    spontaneous EEG [[35](#bib.bib35)] and evoked potentials (EP) [[36](#bib.bib36)].
    Depending on the scenario, spontaneous EEG further diverges into sleep EEG, motor
    imagery EEG, emotional EEG, mental disease EEG, and others. Similarly, EP divides
    into event-related potentials (ERP) [[28](#bib.bib28)] and steady-state evoked
    potentials (SSEP) [[37](#bib.bib37)] according to the frequency of external stimuli.
    Each potential contains visual-, auditory-, and somatosensory-potentials based
    on the external stimuli types.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S2.F2 "图 2 ‣ 2 脑成像技术 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿") 显示了基于信号采集方法的非侵入性脑信号分类。非侵入性信号分为脑电图（EEG）、功能性近红外光谱（fNIRS）、功能性磁共振成像（fMRI）和磁脑图（MEG）
    [[34](#bib.bib34)]。表 [2](#S2.T2 "表 2 ‣ 2 脑成像技术 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿") 总结了各种脑信号的特征。在本次调查中，我们主要关注
    EEG 信号及其子类别，因为它们主导了非侵入性信号。EEG 监测由人类神经元中的电流产生的电压波动。附着在头皮上的电极可以测量各种类型的 EEG 信号，包括自发
    EEG [[35](#bib.bib35)] 和诱发电位（EP） [[36](#bib.bib36)]。根据场景，自发 EEG 进一步分为睡眠 EEG、运动意象
    EEG、情感 EEG、心理疾病 EEG 等。同样，EP 根据外部刺激的频率分为事件相关电位（ERP） [[28](#bib.bib28)] 和稳态诱发电位（SSEP）
    [[37](#bib.bib37)]。每种电位根据外部刺激类型包含视觉、电听和躯体感觉电位。
- en: Regarding the other non-invasive techniques, fNIRS produces functional neuroimages
    by employing near-infrared (NIR) light to measure the aggregation degree of oxygenated
    hemoglobin (Hb) and deoxygenated-hemoglobin (deoxy-Hb), both of which have higher
    absorbers of light than other head components such as skull and scalp [[38](#bib.bib38)];
    fMRI monitors brain activities by detecting the blood flow changes in brain areas
    [[14](#bib.bib14)]; MEG reflects brain activities via magnetic changes [[39](#bib.bib39)].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 关于其他非侵入性技术，fNIRS 通过利用近红外（NIR）光来测量氧合血红蛋白（Hb）和去氧血红蛋白（deoxy-Hb）的聚集程度，从而生成功能性神经图像，这两者比其他头部成分（如颅骨和头皮）对光的吸收更强
    [[38](#bib.bib38)]；fMRI 通过检测脑区血流变化来监测脑活动 [[14](#bib.bib14)]；MEG 通过磁场变化反映脑活动 [[39](#bib.bib39)]。
- en: 'Table 2: Summary of non-invasive brain signals’ characteristics.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 非侵入性脑信号特征总结。'
- en: '| Signals | EEG | fNIRS | fMRI | MEG |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 信号 | EEG | fNIRS | fMRI | MEG |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Spatial resolution | Low | Intermediate | High | Intermediate |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 空间分辨率 | 低 | 中等 | 高 | 中等 |'
- en: '| Temporal resolution | High | Low | Low | High |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 时间分辨率 | 高 | 低 | 低 | 高 |'
- en: '| Signal-to-Noise Ratio | Low | Low | Intermediate | Low |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 信噪比 | 低 | 低 | 中等 | 低 |'
- en: '| Portability | High | High | Low | Low |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 可移植性 | 高 | 高 | 低 | 低 |'
- en: '| Cost | Low | Low | High | High |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 成本 | 低 | 低 | 高 | 高 |'
- en: '| Characteristic | Electrical | Metabolic | Metabolic | Magnetic |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 电气 | 代谢 | 代谢 | 磁性 |'
- en: 3 Overview on Deep Learning Models
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习模型概述
- en: '![Refer to caption](img/c5eb5875c0916f2c4e288b7b31bf1c92.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c5eb5875c0916f2c4e288b7b31bf1c92.png)'
- en: 'Figure 3: Deep learning models. They can be divided into discriminative, representative,
    generative and hybrid models based on the algorithm functions. Discriminative
    models (Appendix [B.1](#A2.SS1 "B.1 Discriminative Deep Learning Models ‣ Appendix
    B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")) mainly include
    Multi-Layer Perceptron (MLP), Recurrent Neural Networks(RNN), and Convolutional
    Neural Networks (CNN). The two mainstreams of RNN are Long Short-Term Memory (LSTM)
    and Gated Recurrent Unit (GRU). Representative models (Appendix [B.2](#A2.SS2
    "B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain
    Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers")) can be divided into Authoencoder (AE), Restricted
    Boltzmann Machine (RBM), and Deep Belief Networks (DBN). D-AE denotes Deep-Autoencoder
    which refers to the Autoencoder with multiple hidden layers. Likewise, D-RBM denotes
    Deep-Restricted Boltzmann Machine with multiple hidden layers. Deep Belief Network
    can be composed of AE or RBM, therefore, we divided DBN into DBN-AE and DBN-RBM.
    Generative models (Appendix [B.3](#A2.SS3 "B.3 Generative Deep Learning Models
    ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")) that are commonly
    used in non-invasive brain signal analysis include Variational Autoencoder (VAE)
    and Generative Adversarial Networks (GAN).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：深度学习模型。根据算法功能，它们可以分为判别式、代表性、生成式和混合型模型。判别式模型（附录 [B.1](#A2.SS1 "B.1 判别式深度学习模型
    ‣ 附录 B 基础深度学习在脑信号分析中的应用 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")）主要包括多层感知器（MLP）、递归神经网络（RNN）和卷积神经网络（CNN）。RNN的两个主流是长短期记忆网络（LSTM）和门控递归单元（GRU）。代表性模型（附录 [B.2](#A2.SS2
    "B.2 代表性深度学习模型 ‣ 附录 B 基础深度学习在脑信号分析中的应用 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")）可以分为自编码器（AE）、限制玻尔兹曼机（RBM）和深度信念网络（DBN）。D-AE表示深度自编码器，指的是具有多个隐藏层的自编码器。同样，D-RBM表示深度限制玻尔兹曼机，具有多个隐藏层。深度信念网络可以由AE或RBM组成，因此，我们将DBN分为DBN-AE和DBN-RBM。生成式模型（附录 [B.3](#A2.SS3
    "B.3 生成式深度学习模型 ‣ 附录 B 基础深度学习在脑信号分析中的应用 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")）在非侵入性脑信号分析中常用的包括变分自编码器（VAE）和生成对抗网络（GAN）。
- en: 'Table 3: Summary of deep learning model types'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：深度学习模型类型汇总
- en: '| Deep Learning | Input | Output | Function | Training method |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | 输入 | 输出 | 功能 | 训练方法 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Discriminative | Input data | Label | Feature extraction, Classification
    | Supervised |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 判别式 | 输入数据 | 标签 | 特征提取，分类 | 监督式 |'
- en: '| Representative | Input data | Representation | Feature extraction | Unsupervised
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 代表性 | 输入数据 | 表示 | 特征提取 | 无监督 |'
- en: '| Generative | Input data | New Sample | Generation, Reconstruction | Unsupervised
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 生成式 | 输入数据 | 新样本 | 生成，重建 | 无监督 |'
- en: '| Hybrid | Input data | – | – | – |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 混合型 | 输入数据 | – | – | – |'
- en: 'In this section, we formally introduce the deep learning models including concepts,
    architectures, and techniques that are commonly used in the field of brain signal
    researches. Deep learning is a class of machine learning techniques that uses
    many layers of information-processing stages in hierarchical architectures for
    pattern classification and feature/representation learning [[31](#bib.bib31)].
    More detailed information about the deep learning techniques which are common-used
    in brain signal analysis can be find in Appendix [B](#A2 "Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们正式介绍了深度学习模型，包括在脑信号研究领域常用的概念、架构和技术。深度学习是一类机器学习技术，它使用多层信息处理阶段的分层架构进行模式分类和特征/表示学习[[31](#bib.bib31)]。有关在脑信号分析中常用的深度学习技术的更详细信息，请参见附录[B](#A2
    "附录 B 基础深度学习在脑信号分析中的应用 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")。
- en: 'Deep learning algorithms contain several subcategories based on the aim of
    the techniques (Figure [3](#S3.F3 "Figure 3 ‣ 3 Overview on Deep Learning Models
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers")):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习算法根据技术目标的不同，包含多个子类别（见图[3](#S3.F3 "Figure 3 ‣ 3 Overview on Deep Learning
    Models ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers)")：'
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Discriminative deep learning models, which classify the input data into a pre-known
    label based on the adaptively learned discriminative features. Discriminative
    algorithms are able to learn distinctive features by non-linear transformation,
    and classification through probabilistic prediction³³3The classification function
    is achieved by the combination of a softmax layer and one-hot label encoding.
    The one-hot label encoding refers to encoding the label by the one-hot method,
    which is a group of bits among which the only valid combinations of values are
    those with a single high (1) bit and all the others low (0) bits. For instance,
    a set of labels 0, 1, 2, 3 can be encoded as (1, 0, 0, 0), (0, 1, 0, 0), (0, 0,
    1, 0), (0, 0, 0, 1).. Thus these algorithms can play the role of both feature
    extraction and classification (corresponding to Figure [1](#S1.F1 "Figure 1 ‣
    1.1 General Workflow ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")). Discriminative architectures
    mainly include Multi-Layer Perceptron (MLP) [[40](#bib.bib40)], Recurrent Neural
    Networks (RNN) [[41](#bib.bib41)], Convolutional Neural Networks (CNN) [[42](#bib.bib42)],
    along with their variations.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '判别深度学习模型，根据自适应学习的判别特征将输入数据分类为预知标签。判别算法能够通过非线性变换学习独特特征，并通过概率预测进行分类³³3分类功能通过softmax层和one-hot标签编码的组合实现。one-hot标签编码指的是通过one-hot方法对标签进行编码，即一组位中唯一有效的组合是只有一个高（1）位和其他位都低（0）的值。例如，标签0、1、2、3可以编码为（1，0，0，0），（0，1，0，0），（0，0，1，0），（0，0，0，1）..因此，这些算法可以同时发挥特征提取和分类的作用（对应图[1](#S1.F1
    "Figure 1 ‣ 1.1 General Workflow ‣ 1 Introduction ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")）。判别架构主要包括多层感知机（MLP）[[40](#bib.bib40)]，递归神经网络（RNN）[[41](#bib.bib41)]，卷积神经网络（CNN）[[42](#bib.bib42)]及其变体。'
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Representative deep learning models, which learn the pure and representative
    features from the input data. These algorithms only have the function of feature
    extraction (Figure [1](#S1.F1 "Figure 1 ‣ 1.1 General Workflow ‣ 1 Introduction
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers")) but cannot make classification. Commonly used deep learning
    algorithms for representation are Autoencoder (AE) [[43](#bib.bib43)], Restricted
    Boltzmann Machine (RBM) [[44](#bib.bib44)], Deep Belief Networks (DBN) [[45](#bib.bib45)],
    along with their variations.'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '代表性深度学习模型，从输入数据中学习纯粹且具有代表性的特征。这些算法只有特征提取功能（见图[1](#S1.F1 "Figure 1 ‣ 1.1 General
    Workflow ‣ 1 Introduction ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers)")，不能进行分类。常用于表示的深度学习算法有自编码器（AE）[[43](#bib.bib43)]，限制玻尔兹曼机（RBM）[[44](#bib.bib44)]，深度置信网络（DBN）[[45](#bib.bib45)]，及其变体。'
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generative deep learning models, which learn the joint probability distribution
    of the input data and the target label. In the brain signal scope, generative
    algorithms are mostly used to generate a batch of brain signals samples to enhance
    the training set. Generative models commonly used in brain signal analysis include
    Variational Autoencoder (VAE)⁴⁴4VAE is a variation of AE. However, they are working
    on different aspects. Therefore, we separately introduce AE and VAE. [[46](#bib.bib46)],
    Generative Adversarial Networks (GANs) [[47](#bib.bib47)], etc.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成深度学习模型，学习输入数据和目标标签的联合概率分布。在脑信号范围内，生成算法通常用于生成一批脑信号样本，以增强训练集。常用于脑信号分析的生成模型包括变分自编码器（VAE）⁴⁴4VAE是AE的变体，但它们工作于不同的方面。因此，我们分别介绍AE和VAE。[[46](#bib.bib46)]，生成对抗网络（GANs）[[47](#bib.bib47)]，等等。
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hybrid deep learning models, which combine more than two deep learning models.
    For example, the typical hybrid deep learning model employs a representation algorithm
    for feature extraction and discriminative algorithms for classification.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混合深度学习模型，结合了两种以上的深度学习模型。例如，典型的混合深度学习模型采用表示算法进行特征提取，使用判别算法进行分类。
- en: 'The summary of the characteristics of each deep learning subcategories are
    listed in Table [3](#S3.T3 "Table 3 ‣ 3 Overview on Deep Learning Models ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers").
    Almost all the classification functions in neural networks are implemented by
    a softmax layer, which will not be regarded as an algorithmic component in this
    survey. For instance, a model combining a DBN and a softmax layer will still be
    regarded as a representative model instead of a hybrid model.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '各种深度学习子类别的特征总结列在表[3](#S3.T3 "Table 3 ‣ 3 Overview on Deep Learning Models ‣
    A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers")中。几乎所有神经网络中的分类函数都是通过softmax层实现的，在本调查中不会将其视为算法组件。例如，结合DBN和softmax层的模型仍然会被视为代表性模型，而不是混合模型。'
- en: 4 State-of-The-Art DL Techniques for Brain Signals
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 前沿深度学习技术用于脑信号
- en: 'In this section, we thoroughly summarize the advanced studies on deep learning-based
    brain signals (Table [4](#S4.T4 "Table 4 ‣ 4.1.1 Spontaneous EEG ‣ 4.1 EEG ‣ 4
    State-of-The-Art DL Techniques for Brain Signals ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")). The hybrid models
    are divided into three parts: the combination of RNN and CNN, the combination
    of representative and discriminative models (denoted as ‘Repre + Discri’), and
    others hybrid models.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们详细总结了基于深度学习的脑信号的先进研究（表[4](#S4.T4 "Table 4 ‣ 4.1.1 Spontaneous EEG ‣
    4.1 EEG ‣ 4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey on Deep
    Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers")）。混合模型分为三部分：RNN和CNN的组合、代表性和区分模型的组合（记作‘Repre
    + Discri’），以及其他混合模型。'
- en: 4.1 EEG
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 脑电图
- en: 'Due to the advantages of high portability and low price, EEG signals have attracted
    much attention. Most of the latest publications on non-invasive brain signals
    are related to EEG. In this section, we summarize two aspects of EEG signals:
    spontaneous EEG and evoked potentials. As implied by the name, the former are
    spontaneous and the latter requires outside stimuli.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高便携性和低价格的优势，脑电图信号受到了广泛关注。最新的大多数非侵入性脑信号相关出版物都与脑电图有关。本节总结了脑电图信号的两个方面：自发性脑电图和诱发电位。顾名思义，自发性脑电图是自发的，而诱发电位则需要外部刺激。
- en: 4.1.1 Spontaneous EEG
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 自发性脑电图
- en: We present the deep learning models for spontaneous EEG according to the application
    scenarios as follows.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据应用场景介绍自发性脑电图的深度学习模型如下。
- en: '(1) Sleep EEG. Sleep EEG is mainly used for recognizing the sleep stage and
    diagnosing sleep disorders or cultivating the healthy habit [[48](#bib.bib48),
    [49](#bib.bib49)]. According to Rechtschaffen and Kales (R&K) rules, the sleep
    stage includes wakefulness, non-REM (rapid eye movement) 1, non-REM 2, non-REM
    3, non-REM 4, and REM. The American Academy of Sleep Medicine (AASM) recommends
    segmentation of sleep in five stages: wakefulness, non-REM 1, non-REM 2, slow
    wave sleep (SWS), and REM. The non-REM 3 and non-REM 4 are combined into SWS since
    there is no clear distinction between them [[49](#bib.bib49)]. Generally, in the
    sleep stage analysis, the EEG signals are preprocessed by a filter which has various
    passband in different papers, but all notched at 50 Hz. The EEG signals are usually
    segmented into 30s windows.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 睡眠脑电图。睡眠脑电图主要用于识别睡眠阶段和诊断睡眠障碍或培养健康习惯[[48](#bib.bib48), [49](#bib.bib49)]。根据Rechtschaffen和Kales
    (R&K)规则，睡眠阶段包括清醒、非快速眼动1、非快速眼动2、非快速眼动3、非快速眼动4和快速眼动。美国睡眠医学学会(AASM)建议将睡眠分为五个阶段：清醒、非快速眼动1、非快速眼动2、慢波睡眠(SWS)和快速眼动。非快速眼动3和非快速眼动4合并为SWS，因为它们之间没有明显的区别[[49](#bib.bib49)]。一般来说，在睡眠阶段分析中，脑电图信号会通过一个滤波器进行预处理，该滤波器在不同论文中有不同的通带，但都被滤除50
    Hz的干扰。脑电图信号通常被分割成30秒的窗口。
- en: (i) Discriminative models. CNN are frequently used for sleep stage classification
    on single-channel EEG [[25](#bib.bib25), [50](#bib.bib50)]. For example, Viamala
    et al. [[51](#bib.bib51)] manually extracted the time-frequency features and achieved
    a classification accuracy of 86%. Others used RNN [[52](#bib.bib52)] and LSTM
    [[53](#bib.bib53)] based on various features from the frequency domain, correlation,
    and graph theoretical features.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 区分模型。CNN经常用于单通道脑电图的睡眠阶段分类[[25](#bib.bib25), [50](#bib.bib50)]。例如，Viamala等人[[51](#bib.bib51)]手动提取了时频特征，并取得了86%的分类准确率。其他研究使用了基于频域、相关性和图理论特征的RNN[[52](#bib.bib52)]和LSTM[[53](#bib.bib53)]。
- en: (ii) Representative models. Tan et al. [[54](#bib.bib54)] adopted a DBN-RBM
    algorithm to detect sleep spindle based on Power Spectral Density (PSD) features
    extracted from sleep EEG signals and achieved an F-1 of 92.78% on a local dataset.
    Zhang et al. [[49](#bib.bib49)] further combined DBN-RBM with three RBMs for sleep
    feature extraction.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 表征模型。Tan等[[54](#bib.bib54)]采用DBN-RBM算法基于从睡眠EEG信号中提取的功率谱密度（PSD）特征检测睡眠纺锤体，在本地数据集上达到了92.78%的F-1分数。Zhang等[[49](#bib.bib49)]进一步将DBN-RBM与三种RBM结合用于睡眠特征提取。
- en: (iii) Hybrid models. Manzano et al. [[55](#bib.bib55)] presented a multi-view
    algorithm in order to predict sleep stage by combining CNN and MLP. The CNN was
    employed to receive the raw time-domain EEG oscillations while the MLP received
    the spectrum singles processed by the Short-Time Fourier Transform (STFT) among
    0.5-32 Hz. Fraiwan et al. [[56](#bib.bib56)] combined DBN with MLP for neonatal
    sleep state identification. Supratak et al. [[57](#bib.bib57)] proposed a model
    by combing a multi-view CNN and LSTM for automatic sleep stage scoring, in which
    the former was adopted to discover time-invariant dependencies while the latter
    (a bidirectional LSTM) was adopted the temporal features during the sleep. Dong
    et al. [[58](#bib.bib58)] proposed a hybrid deep learning model aiming at temporal
    sleep stage classification and took advantage of MLP for detecting hierarchical
    features along with LSTM for sequential information learning.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 混合模型。Manzano等[[55](#bib.bib55)]提出了一种多视角算法，通过结合CNN和MLP来预测睡眠阶段。CNN用于接收原始时间域EEG振荡，而MLP接收经短时傅里叶变换（STFT）处理的0.5-32
    Hz范围的频谱信号。Fraiwan等[[56](#bib.bib56)]将DBN与MLP结合用于新生儿睡眠状态识别。Supratak等[[57](#bib.bib57)]提出了一种通过结合多视角CNN和LSTM进行自动睡眠阶段评分的模型，其中前者用于发现时间不变的依赖关系，而后者（双向LSTM）用于处理睡眠中的时间特征。Dong等[[58](#bib.bib58)]提出了一种混合深度学习模型，旨在时间性睡眠阶段分类，利用MLP检测层次特征，并结合LSTM进行序列信息学习。
- en: (2) MI EEG. Deep learning models have shown the superior on the classification
    of Motor-Imagery (MI) EEG and real-motor EEG [[59](#bib.bib59), [60](#bib.bib60)].
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (2) MI EEG。深度学习模型在运动意象（MI）脑电图（EEG）和实际运动EEG的分类上表现优越[[59](#bib.bib59), [60](#bib.bib60)]。
- en: (i) Discriminative models. Such models mostly use CNN to recognize MI EEG [[61](#bib.bib61)].
    Some are based on manually extracted features [[62](#bib.bib62), [63](#bib.bib63)].
    For instance, Lee et al. [[64](#bib.bib64)] and Zhang et al. [[65](#bib.bib65)]
    employed CNN and 2-D CNN, respectively, for classification; Zhang et al. [[65](#bib.bib65)]
    learned affective information from EEG signals to built a modified LSTM control
    smart home appliances. Others also used CNN for feature extraction [[66](#bib.bib66)].
    For example, Wang et al. [[67](#bib.bib67)] first used CNN to capture latent connections
    from MI-EEG signals and then applied weak classifiers to choose important features
    for the final classification; Hartmann et al. [[59](#bib.bib59)] investigated
    how CNN represented spectral features through the sequence of the MI EEG samples.
    MLP has also been applied for MI EEG recognition [[68](#bib.bib68)], which showed
    higher sensitivity to EEG phase features at earlier stages and higher sensitivity
    to EEG amplitude features at later stages.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 判别模型。这类模型主要使用CNN来识别MI EEG[[61](#bib.bib61)]。一些模型基于手动提取的特征[[62](#bib.bib62),
    [63](#bib.bib63)]。例如，Lee等[[64](#bib.bib64)]和Zhang等[[65](#bib.bib65)]分别采用CNN和2-D
    CNN进行分类；Zhang等[[65](#bib.bib65)]从EEG信号中学习情感信息以构建改进的LSTM控制智能家电。其他模型也使用CNN进行特征提取[[66](#bib.bib66)]。例如，Wang等[[67](#bib.bib67)]首先使用CNN从MI-EEG信号中捕捉潜在的连接，然后应用弱分类器选择最终分类的重要特征；Hartmann等[[59](#bib.bib59)]研究了CNN如何通过MI
    EEG样本的序列表示谱特征。MLP也被应用于MI EEG识别[[68](#bib.bib68)]，在早期阶段对EEG相位特征的敏感性更高，而在后期阶段对EEG幅度特征的敏感性更高。
- en: 'Table 4: A summary of non-invasive brain signal studies based on deep learning
    models'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：基于深度学习模型的非侵入脑信号研究总结
- en: '| brain signals | Deep Learning Models |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 脑信号 | 深度学习模型 |'
- en: '| Discriminative Models | Representative Models | Generative Models | Hybrid
    Models |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 判别模型 | 表征模型 | 生成模型 | 混合模型 |'
- en: '| MLP | RNN | CNN | AE (D-AE) | RBM (D-RBM) | DBN | VAE | GAN | LSTM+CNN |
    Repre + Discri | Others |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MLP | RNN | CNN | AE (D-AE) | RBM (D-RBM) | DBN | VAE | GAN | LSTM+CNN |
    表征+判别 | 其他 |'
- en: '| DBN-AE | DBN-RBM |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| DBN-AE | DBN-RBM |'
- en: '| Non- invasive Signals | EEG | Spont- aneous EEG | Sleep EEG | [[69](#bib.bib69),
    [52](#bib.bib52)] | [[53](#bib.bib53), [52](#bib.bib52)] |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 非侵入信号 | EEG | 自发EEG | 睡眠EEG | [[69](#bib.bib69), [52](#bib.bib52)] | [[53](#bib.bib53),
    [52](#bib.bib52)] |'
- en: '&#124; [[51](#bib.bib51)],[[48](#bib.bib48)], &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[51](#bib.bib51)],[[48](#bib.bib48)], &#124;'
- en: '&#124; [[25](#bib.bib25), [50](#bib.bib50)], &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[25](#bib.bib25), [50](#bib.bib50)], &#124;'
- en: '&#124; [[70](#bib.bib70), [52](#bib.bib52)] &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[70](#bib.bib70), [52](#bib.bib52)] &#124;'
- en: '|  |  |  | [[49](#bib.bib49), [54](#bib.bib54)] |  |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | [[49](#bib.bib49), [54](#bib.bib54)] |  |  |'
- en: '&#124; [[57](#bib.bib57)] &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[57](#bib.bib57)] &#124;'
- en: '&#124; [[52](#bib.bib52)] &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[52](#bib.bib52)] &#124;'
- en: '| [[56](#bib.bib56)] |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| [[56](#bib.bib56)] |'
- en: '&#124; [[55](#bib.bib55)], &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[55](#bib.bib55)], &#124;'
- en: '&#124; [[58](#bib.bib58)] &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[58](#bib.bib58)] &#124;'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MI EEG | [[71](#bib.bib71)],[[68](#bib.bib68)] |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| MI EEG | [[71](#bib.bib71)],[[68](#bib.bib68)] |'
- en: '&#124; [[6](#bib.bib6)], &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[6](#bib.bib6)], &#124;'
- en: '&#124; [[61](#bib.bib61), [65](#bib.bib65)] &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[61](#bib.bib61), [65](#bib.bib65)] &#124;'
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[64](#bib.bib64)], [[72](#bib.bib72)], &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[64](#bib.bib64)], [[72](#bib.bib72)], &#124;'
- en: '&#124; [[60](#bib.bib60)], &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[60](#bib.bib60)], &#124;'
- en: '&#124; [[63](#bib.bib63)],[[73](#bib.bib73)], &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[63](#bib.bib63)],[[73](#bib.bib73)], &#124;'
- en: '&#124; [[59](#bib.bib59), [62](#bib.bib62)] &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[59](#bib.bib59), [62](#bib.bib62)] &#124;'
- en: '&#124; [[66](#bib.bib66)] &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[66](#bib.bib66)] &#124;'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[74](#bib.bib74), [75](#bib.bib75)] &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[74](#bib.bib74), [75](#bib.bib75)] &#124;'
- en: '&#124; [[76](#bib.bib76)] &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[76](#bib.bib76)] &#124;'
- en: '|  | [[77](#bib.bib77)] |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | [[77](#bib.bib77)] |'
- en: '&#124; [[78](#bib.bib78), [79](#bib.bib79)], &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[78](#bib.bib78), [79](#bib.bib79)], &#124;'
- en: '&#124; [[80](#bib.bib80)] &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[80](#bib.bib80)] &#124;'
- en: '| [[81](#bib.bib81)] |  | [[82](#bib.bib82), [10](#bib.bib10)] | [[4](#bib.bib4)],[[83](#bib.bib83)]
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| [[81](#bib.bib81)] |  | [[82](#bib.bib82), [10](#bib.bib10)] | [[4](#bib.bib4)],[[83](#bib.bib83)]
    |'
- en: '&#124; [[84](#bib.bib84), [85](#bib.bib85)], &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[84](#bib.bib84), [85](#bib.bib85)], &#124;'
- en: '&#124; [[67](#bib.bib67), [86](#bib.bib86)] &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[67](#bib.bib67), [86](#bib.bib86)] &#124;'
- en: '&#124; [[2](#bib.bib2)] &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[2](#bib.bib2)] &#124;'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Emotional &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 情感 &#124;'
- en: '&#124; EEG &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EEG &#124;'
- en: '| [[87](#bib.bib87)] | [[88](#bib.bib88)] |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| [[87](#bib.bib87)] | [[88](#bib.bib88)] |'
- en: '&#124; [[89](#bib.bib89)],[[90](#bib.bib90)], &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[89](#bib.bib89)],[[90](#bib.bib90)], &#124;'
- en: '&#124; [[91](#bib.bib91)], &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[91](#bib.bib91)], &#124;'
- en: '&#124; [[92](#bib.bib92), [93](#bib.bib93)] &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[92](#bib.bib92), [93](#bib.bib93)] &#124;'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[94](#bib.bib94)], &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[94](#bib.bib94)], &#124;'
- en: '&#124; [[95](#bib.bib95)] &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[95](#bib.bib95)] &#124;'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[96](#bib.bib96), [97](#bib.bib97)] &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[96](#bib.bib96), [97](#bib.bib97)] &#124;'
- en: '&#124; [[98](#bib.bib98)] &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[98](#bib.bib98)] &#124;'
- en: '| [[99](#bib.bib99)] |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| [[99](#bib.bib99)] |'
- en: '&#124; [[98](#bib.bib98)],[[99](#bib.bib99)], &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[98](#bib.bib98)],[[99](#bib.bib99)], &#124;'
- en: '&#124; [[100](#bib.bib100)],[[101](#bib.bib101)], &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[100](#bib.bib100)],[[101](#bib.bib101)], &#124;'
- en: '&#124; [[102](#bib.bib102), [103](#bib.bib103)] &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[102](#bib.bib102), [103](#bib.bib103)] &#124;'
- en: '|  |  | [[104](#bib.bib104)] |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [[104](#bib.bib104)] |'
- en: '&#124; [[105](#bib.bib105), [106](#bib.bib106)] &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[105](#bib.bib105), [106](#bib.bib106)] &#124;'
- en: '&#124; [[107](#bib.bib107)] &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[107](#bib.bib107)] &#124;'
- en: '| [[108](#bib.bib108)] |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| [[108](#bib.bib108)] |'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mental Disease &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精神疾病 &#124;'
- en: '&#124; EEG &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EEG &#124;'
- en: '| [[109](#bib.bib109)] | [[110](#bib.bib110)],[[111](#bib.bib111)] |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| [[109](#bib.bib109)] | [[110](#bib.bib110)],[[111](#bib.bib111)] |'
- en: '&#124; [[112](#bib.bib112)],[[113](#bib.bib113)], &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[112](#bib.bib112)],[[113](#bib.bib113)], &#124;'
- en: '&#124; [[114](#bib.bib114)],[[115](#bib.bib115)], &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[114](#bib.bib114)],[[115](#bib.bib115)], &#124;'
- en: '&#124; [[116](#bib.bib116)],[[117](#bib.bib117)], &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[116](#bib.bib116)],[[117](#bib.bib117)], &#124;'
- en: '&#124; [[118](#bib.bib118), [119](#bib.bib119)] &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[118](#bib.bib118), [119](#bib.bib119)] &#124;'
- en: '&#124; [[120](#bib.bib120)] &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[120](#bib.bib120)] &#124;'
- en: '|'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[121](#bib.bib121)],[[122](#bib.bib122)], &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[121](#bib.bib121)],[[122](#bib.bib122)], &#124;'
- en: '&#124; [[123](#bib.bib123)], &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[123](#bib.bib123)], &#124;'
- en: '&#124; [[124](#bib.bib124)] &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[124](#bib.bib124)] &#124;'
- en: '|  | [[125](#bib.bib125)] | [[126](#bib.bib126), [127](#bib.bib127)] |  |  |
    [[128](#bib.bib128)] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | [[125](#bib.bib125)] | [[126](#bib.bib126), [127](#bib.bib127)] |  |  |
    [[128](#bib.bib128)] |'
- en: '&#124; [[129](#bib.bib129), [120](#bib.bib120)], &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[129](#bib.bib129), [120](#bib.bib120)], &#124;'
- en: '&#124; [[130](#bib.bib130), [131](#bib.bib131)] &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[130](#bib.bib130), [131](#bib.bib131)] &#124;'
- en: '|  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Data &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据 &#124;'
- en: '&#124; Augmentation &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强 &#124;'
- en: '|  |  |  |  |  |  |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |'
- en: '&#124; [[132](#bib.bib132), [81](#bib.bib81)], &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[132](#bib.bib132), [81](#bib.bib81)], &#124;'
- en: '&#124; [[133](#bib.bib133)] &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[133](#bib.bib133)] &#124;'
- en: '&#124; [[134](#bib.bib134)] &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[134](#bib.bib134)] &#124;'
- en: '|  |  |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| Others |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 其他 |'
- en: '&#124; [[135](#bib.bib135), [136](#bib.bib136)] &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[135](#bib.bib135), [136](#bib.bib136)] &#124;'
- en: '&#124; [[137](#bib.bib137)] &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[137](#bib.bib137)] &#124;'
- en: '| [[138](#bib.bib138)] |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] |'
- en: '&#124; [[139](#bib.bib139)], [[140](#bib.bib140)], &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[139](#bib.bib139)], [[140](#bib.bib140)], &#124;'
- en: '&#124; [[141](#bib.bib141)],[[142](#bib.bib142)], &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[141](#bib.bib141)],[[142](#bib.bib142)], &#124;'
- en: '&#124; [[138](#bib.bib138)],[[143](#bib.bib143)],[[144](#bib.bib144)], &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[138](#bib.bib138)],[[143](#bib.bib143)],[[144](#bib.bib144)], &#124;'
- en: '&#124; [[145](#bib.bib145), [146](#bib.bib146)] &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[145](#bib.bib145), [146](#bib.bib146)] &#124;'
- en: '&#124; [[147](#bib.bib147), [147](#bib.bib147)] &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[147](#bib.bib147), [147](#bib.bib147)] &#124;'
- en: '&#124; [[148](#bib.bib148), [149](#bib.bib149)] &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[148](#bib.bib148), [149](#bib.bib149)] &#124;'
- en: '| [[150](#bib.bib150)],[[151](#bib.bib151)] |  | [[152](#bib.bib152)] |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| [[150](#bib.bib150)],[[151](#bib.bib151)] |  | [[152](#bib.bib152)] |'
- en: '&#124; [[153](#bib.bib153)],[[152](#bib.bib152)], &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[153](#bib.bib153)],[[152](#bib.bib152)], &#124;'
- en: '&#124; [[154](#bib.bib154), [155](#bib.bib155)] &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[154](#bib.bib154), [155](#bib.bib155)] &#124;'
- en: '|  |  | [[156](#bib.bib156), [147](#bib.bib147)] | [[157](#bib.bib157)],[[158](#bib.bib158),
    [159](#bib.bib159)] | [[160](#bib.bib160)] |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [[156](#bib.bib156), [147](#bib.bib147)] | [[157](#bib.bib157)],[[158](#bib.bib158),
    [159](#bib.bib159)] | [[160](#bib.bib160)] |'
- en: '|  |  | EP | ERP | VEP | [[161](#bib.bib161), [162](#bib.bib162)], | [[163](#bib.bib163),
    [134](#bib.bib134)] |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  |  | EP | ERP | VEP | [[161](#bib.bib161), [162](#bib.bib162)], | [[163](#bib.bib163),
    [134](#bib.bib134)] |'
- en: '&#124; [[163](#bib.bib163)],[[73](#bib.bib73)] &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[163](#bib.bib163)],[[73](#bib.bib73)] &#124;'
- en: '&#124; [[164](#bib.bib164), [147](#bib.bib147)], &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[164](#bib.bib164), [147](#bib.bib147)], &#124;'
- en: '&#124; [[147](#bib.bib147), [165](#bib.bib165)] &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[147](#bib.bib147), [165](#bib.bib165)] &#124;'
- en: '&#124; [[166](#bib.bib166)] &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[166](#bib.bib166)] &#124;'
- en: '| [[167](#bib.bib167)] |  |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| [[167](#bib.bib167)] |  |  |'
- en: '&#124; [[168](#bib.bib168), [169](#bib.bib169)], &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[168](#bib.bib168), [169](#bib.bib169)], &#124;'
- en: '&#124; [[165](#bib.bib165)] &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[165](#bib.bib165)] &#124;'
- en: '|  |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; [[170](#bib.bib170)],[[171](#bib.bib171)], &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[170](#bib.bib170)],[[171](#bib.bib171)], &#124;'
- en: '&#124; [[172](#bib.bib172)] &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[172](#bib.bib172)] &#124;'
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[97](#bib.bib97), [173](#bib.bib173)] &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[97](#bib.bib97), [173](#bib.bib173)] &#124;'
- en: '&#124; [[96](#bib.bib96)] &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[96](#bib.bib96)] &#124;'
- en: '|  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| RSVP | [[174](#bib.bib174), [175](#bib.bib175)], |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| RSVP | [[174](#bib.bib174), [175](#bib.bib175)], |  |'
- en: '&#124; [[176](#bib.bib176)],[[177](#bib.bib177)], &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[176](#bib.bib176)],[[177](#bib.bib177)], &#124;'
- en: '&#124; [[178](#bib.bib178)],[[179](#bib.bib179)], &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[178](#bib.bib178)],[[179](#bib.bib179)], &#124;'
- en: '&#124; [[180](#bib.bib180), [181](#bib.bib181)], &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[180](#bib.bib180), [181](#bib.bib181)], &#124;'
- en: '&#124; [[182](#bib.bib182), [175](#bib.bib175)] &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[182](#bib.bib182), [175](#bib.bib175)] &#124;'
- en: '&#124; [[183](#bib.bib183), [184](#bib.bib184)] &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[183](#bib.bib183), [184](#bib.bib184)] &#124;'
- en: '&#124; [[185](#bib.bib185), [12](#bib.bib12)] &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[185](#bib.bib185), [12](#bib.bib12)] &#124;'
- en: '|  |  | [[186](#bib.bib186)] |  |  |  |  | [[181](#bib.bib181), [175](#bib.bib175)]
    | [[12](#bib.bib12)] |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [[186](#bib.bib186)] |  |  |  |  | [[181](#bib.bib181), [175](#bib.bib175)]
    | [[12](#bib.bib12)] |'
- en: '| AEP |  |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| AEP |  |  |'
- en: '&#124; [[187](#bib.bib187), [165](#bib.bib165)], &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[187](#bib.bib187), [165](#bib.bib165)], &#124;'
- en: '&#124; [[166](#bib.bib166), [188](#bib.bib188)] &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[166](#bib.bib166), [188](#bib.bib188)] &#124;'
- en: '|  |  |  | [[165](#bib.bib165)] |  |  |  |  |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | [[165](#bib.bib165)] |  |  |  |  |  |'
- en: '| SSEP | SSVEP | [[189](#bib.bib189)] | [[190](#bib.bib190)] |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| SSEP | SSVEP | [[189](#bib.bib189)] | [[190](#bib.bib190)] |'
- en: '&#124; [[191](#bib.bib191)], [[192](#bib.bib192)], &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[191](#bib.bib191)], [[192](#bib.bib192)], &#124;'
- en: '&#124; [[190](#bib.bib190), [193](#bib.bib193)] &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[190](#bib.bib190), [193](#bib.bib193)] &#124;'
- en: '&#124; [[194](#bib.bib194)] &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[194](#bib.bib194)] &#124;'
- en: '|  |  | [[195](#bib.bib195)] | [[195](#bib.bib195)] |  |  | [[196](#bib.bib196)]
    | [[197](#bib.bib197)] |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [[195](#bib.bib195)] | [[195](#bib.bib195)] |  |  | [[196](#bib.bib196)]
    | [[197](#bib.bib197)] |  |'
- en: '| fNIRS |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| fNIRS |'
- en: '&#124; [[38](#bib.bib38)],[[198](#bib.bib198)], &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[38](#bib.bib38)],[[198](#bib.bib198)], &#124;'
- en: '&#124; [[199](#bib.bib199)],[[71](#bib.bib71)], &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[199](#bib.bib199)],[[71](#bib.bib71)], &#124;'
- en: '&#124; [[200](#bib.bib200)] &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[200](#bib.bib200)] &#124;'
- en: '|  | [[198](#bib.bib198)] |  |  |  |  |  |  |  | [[201](#bib.bib201)] |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | [[198](#bib.bib198)] |  |  |  |  |  |  |  | [[201](#bib.bib201)] |  |'
- en: '| fMRI | [[202](#bib.bib202), [203](#bib.bib203)] |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| fMRI | [[202](#bib.bib202), [203](#bib.bib203)] |  |'
- en: '&#124; [[204](#bib.bib204)],[[63](#bib.bib63)], &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[204](#bib.bib204)],[[63](#bib.bib63)], &#124;'
- en: '&#124; [[205](#bib.bib205)],[[206](#bib.bib206)], &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[205](#bib.bib205)],[[206](#bib.bib206)], &#124;'
- en: '&#124; [[117](#bib.bib117)],[[207](#bib.bib207)], &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[117](#bib.bib117)],[[207](#bib.bib207)], &#124;'
- en: '&#124; [[208](#bib.bib208), [194](#bib.bib194)] &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[208](#bib.bib208), [194](#bib.bib194)] &#124;'
- en: '| [[209](#bib.bib209)] |  | [[210](#bib.bib210)], |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| [[209](#bib.bib209)] |  | [[210](#bib.bib210)], |'
- en: '&#124; [[211](#bib.bib211)],[[210](#bib.bib210)],[[212](#bib.bib212), [213](#bib.bib213)]
    &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[211](#bib.bib211)],[[210](#bib.bib210)],[[212](#bib.bib212), [213](#bib.bib213)]
    &#124;'
- en: '|  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; [[214](#bib.bib214), [215](#bib.bib215)], &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[214](#bib.bib214), [215](#bib.bib215)], &#124;'
- en: '&#124; [[216](#bib.bib216), [203](#bib.bib203)] &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[216](#bib.bib216), [203](#bib.bib203)] &#124;'
- en: '|  | [[217](#bib.bib217)] |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | [[217](#bib.bib217)] |  |'
- en: '| MEG |  |  | [[218](#bib.bib218)],[[204](#bib.bib204)] | [[219](#bib.bib219)]
    |  |  |  |  |  |  | [[220](#bib.bib220)] |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| MEG |  |  | [[218](#bib.bib218)],[[204](#bib.bib204)] | [[219](#bib.bib219)]
    |  |  |  |  |  |  | [[220](#bib.bib220)] |  |'
- en: (ii) Representative models. DBN is widely used as a basis for MI EEG classification
    for its high representative ability [[80](#bib.bib80), [79](#bib.bib79)]. For
    example, Ren et al. [[78](#bib.bib78)] applied a convolutional DBN based on RBM
    components, showing better feature representation than hand-crafted features.
    Li et al. [[77](#bib.bib77)] processed EEG signals with discrete wavelet transformation
    and then applied a DBN-AE based on denoising AE. Other models include the combination
    of AE model (for feature extraction) and a KNN classifier [[75](#bib.bib75)],
    the combination of Genetic Algorithm (for hyper-parameter tuning) and MLP (for
    classification) [[84](#bib.bib84)], the combination AE and XGBoost for multi-person
    scenarios [[76](#bib.bib76)], and the combination of LSTM and reinforcement learning
    for multi-modality signal classification [[85](#bib.bib85), [2](#bib.bib2)].
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 代表性模型。深度置信网络（DBN）因其高代表性能力而被广泛用于运动想象（MI）脑电图分类 [[80](#bib.bib80), [79](#bib.bib79)]。例如，Ren等
    [[78](#bib.bib78)] 应用了一种基于RBM组件的卷积DBN，显示出比手工提取特征更好的特征表示。Li等 [[77](#bib.bib77)]
    使用离散小波变换处理脑电图信号，然后应用基于去噪自编码器（AE）的DBN-AE。其他模型包括AE模型（用于特征提取）和KNN分类器的组合 [[75](#bib.bib75)]，遗传算法（用于超参数调整）和MLP（用于分类）的组合
    [[84](#bib.bib84)]，AE和XGBoost的组合用于多人场景 [[76](#bib.bib76)]，以及LSTM和强化学习的组合用于多模态信号分类
    [[85](#bib.bib85), [2](#bib.bib2)]。
- en: (iii) Hybrid models. Several studies proposed hybrid models for the recognition
    of MI EEG [[81](#bib.bib81)]. For example, Tabar et al. [[4](#bib.bib4)] extracted
    high-level representations from the time, frequency domain and location information
    of EEG signals using CNN and then used a DBN-AE with seven AEs as the classifier;
    Tan et al. [[82](#bib.bib82)] used a denoising AE for dimensional reduction, a
    multi-view CNN combined with RNN for discovering latent temporal and spatial information,
    and finally achieved an average accuracy of 72.22% on a public dataset.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 混合模型。一些研究提出了用于识别运动想象（MI）脑电图的混合模型 [[81](#bib.bib81)]。例如，Tabar等 [[4](#bib.bib4)]
    从脑电图信号的时间、频率域和位置数据中提取高级表示，使用CNN后再使用具有七个AE的DBN-AE作为分类器；Tan等 [[82](#bib.bib82)]
    使用去噪AE进行维度缩减，结合RNN的多视角CNN来发现潜在的时间和空间信息，最终在一个公开数据集上达到了72.22%的平均准确率。
- en: '(3) Emotional EEG. The emotion of an individual can be evaluated in three aspects:
    valence, arousal, and dominance. The combination of the three aspects form emotions
    such as fear, sadness, and anger, which can be revealed by EEG signals.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 情感脑电图。个体的情感可以从三个方面进行评估：情感效价、唤醒度和支配感。这三个方面的结合形成了恐惧、悲伤和愤怒等情感，这些情感可以通过脑电图信号揭示出来。
- en: (i) Discriminative models. MLP are traditionally used [[137](#bib.bib137), [87](#bib.bib87)]
    while CNN and RNN are increasingly popular in EEG based emotion prediction [[89](#bib.bib89),
    [90](#bib.bib90)]. Typical CNN-based work in this category includes hierarchical
    CNN [[89](#bib.bib89), [92](#bib.bib92)] and augmenting the training set for CNN
    [[91](#bib.bib91)]. Li et al. [[89](#bib.bib89)] were the first to propose capturing
    the spatial dependencies among EEG channels via converting multi-channel EEG signals
    into a 2-D matrix. Besides, Talathi [[110](#bib.bib110)] used a discriminative
    deep learning model composed of GRU cells. Zhang et al. [[88](#bib.bib88)] proposed
    a spatial-temporal recurrent neural network, which employs a multi-directional
    RNN layer to discover long-range contextual cues and a bi-directional RNN layer
    to capture sequential features produced by the previous spatial RNN.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 判别模型。传统上使用MLP [[137](#bib.bib137), [87](#bib.bib87)]，而CNN和RNN在基于脑电图的情感预测中越来越受欢迎
    [[89](#bib.bib89), [90](#bib.bib90)]。该类别中的典型CNN基础工作包括分层CNN [[89](#bib.bib89),
    [92](#bib.bib92)] 和增强CNN训练集 [[91](#bib.bib91)]。Li等 [[89](#bib.bib89)] 首次提出通过将多通道脑电图信号转换为二维矩阵来捕捉脑电图通道之间的空间依赖。此外，Talathi
    [[110](#bib.bib110)] 使用了一种由GRU单元组成的判别深度学习模型。Zhang等 [[88](#bib.bib88)] 提出了一个时空递归神经网络，该网络使用多方向RNN层来发现长程上下文线索，以及一个双向RNN层来捕捉由前一空间RNN产生的序列特征。
- en: (ii) Representative models. DBN, especially DBN-RBM, is widely used for the
    unsupervised representation ability in emotion recognition [[100](#bib.bib100),
    [106](#bib.bib106), [103](#bib.bib103)]. For instance, Xu et al. [[99](#bib.bib99),
    [101](#bib.bib101)] proposed a DBN-RBM algorithm with three RBMs and an RBM-AE
    to predict affective state; Zhao et al. [[126](#bib.bib126)] and Zheng et al.
    [[102](#bib.bib102)] cobmined DBN-RBM with SVM and Hidden Markov Model (HMM),
    respectively, addressing the same problem; Zheng et al. [[96](#bib.bib96), [97](#bib.bib97)]
    introduced a D-RBM with five hidden RBM layers to search the important frequency
    patterns and informative channels in affection recognition; Jia et al. [[98](#bib.bib98)]
    eliminated channels with high errors and then used D-RBM for affective state recognition
    based on representative features of the residual channels.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 代表性模型。DBN，特别是 DBN-RBM，因其在情感识别中的无监督表示能力而被广泛使用 [[100](#bib.bib100), [106](#bib.bib106),
    [103](#bib.bib103)]。例如，Xu 等人 [[99](#bib.bib99), [101](#bib.bib101)] 提出了一个具有三层
    RBM 和一个 RBM-AE 的 DBN-RBM 算法来预测情感状态；Zhao 等人 [[126](#bib.bib126)] 和 Zheng 等人 [[102](#bib.bib102)]
    分别将 DBN-RBM 与 SVM 和隐马尔可夫模型（HMM）相结合，解决了相同的问题；Zheng 等人 [[96](#bib.bib96), [97](#bib.bib97)]
    引入了一个具有五层隐藏 RBM 层的 D-RBM 来寻找情感识别中的重要频率模式和信息通道；Jia 等人 [[98](#bib.bib98)] 排除了具有高错误的通道，然后基于残余通道的代表性特征使用
    D-RBM 进行情感状态识别。
- en: The emotion is affected by many subjective and environmental factors (e.g.,
    gender and fatigue). Yan et al. [[95](#bib.bib95)] investigated the discrepancy
    of emotional patterns between men and women by proposing a novel model called
    Bimodal Deep AutoEncoder (BDAE) which received both EEG and eye movement features
    and shared the information in a fusion layer which connected with an SVM classifier.
    The results showed that the females have higher EEG signal diversity on the fearful
    emotion while males on sad emotion. Moreover, for women, the inter-subject differences
    in fear is more significant then other emotions [[95](#bib.bib95)]. To overcome
    the mismatched distribution among the samples collected from different subjects
    or different experimental sessions, Chai et al. [[94](#bib.bib94)] proposed an
    unsupervised domain adaptation technology which is called subspace alignment autoencoder
    (SAAE) by combing an AE and a subspace alignment solution. The proposed approach
    obtained a mean accuracy of 77.88% in person independent scenario.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 情感受许多主观和环境因素（例如，性别和疲劳）的影响。Yan 等人 [[95](#bib.bib95)] 通过提出一种名为双模态深度自编码器（Bimodal
    Deep AutoEncoder, BDAE）的新模型，调查了男性和女性情感模式的差异，该模型同时接收 EEG 和眼动特征，并在一个与 SVM 分类器连接的融合层中共享信息。结果显示，女性在恐惧情绪上的
    EEG 信号多样性更高，而男性在悲伤情绪上的 EEG 信号多样性更高。此外，对于女性来说，恐惧的被试间差异比其他情绪更显著 [[95](#bib.bib95)]。为了克服从不同被试或不同实验会话中收集样本之间的不匹配分布，Chai
    等人 [[94](#bib.bib94)] 提出了一个无监督领域适应技术，称为子空间对齐自编码器（subspace alignment autoencoder,
    SAAE），通过结合 AE 和子空间对齐解决方案。该方法在人员独立场景中取得了 77.88% 的平均准确率。
- en: (iii) Hybrid models. One common-used hybrid model is a combination of RNN and
    MLP. For example, Alhagry et al. [[108](#bib.bib108)] employed an LSTM architecture
    for feature extraction from emotional EEG signals and the features are forwarded
    into an MLP for classification. Furthermore, Yin et al. [[107](#bib.bib107)] proposed
    a multi-view ensemble classifier to recognize individual emotions using multimodal
    physiological signals. The ensemble classifier contains several D-AEs with three
    hidden layers and a fusion structure. Each D-AE receives one physiological signal
    (e.g., EEG) and then sends the outputs of D-AE to a fusion structure which is
    composed of another D-AE. At last, an MLP classifier makes the prediction based
    on the mixed features. Kawde et al. [[105](#bib.bib105)] implemented an affect
    recognition system by combining a DBN-RBM for effective feature extraction and
    an MLP for classification.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 混合模型。一种常见的混合模型是 RNN 和 MLP 的组合。例如，Alhagry 等人 [[108](#bib.bib108)] 使用了一个
    LSTM 架构从情感 EEG 信号中提取特征，并将这些特征传递给 MLP 进行分类。此外，Yin 等人 [[107](#bib.bib107)] 提出了一个多视角集成分类器，使用多模态生理信号来识别个体情感。该集成分类器包含多个具有三层隐藏层的
    D-AE 和一个融合结构。每个 D-AE 接收一个生理信号（例如 EEG），然后将 D-AE 的输出发送到由另一个 D-AE 组成的融合结构。最后，一个 MLP
    分类器根据混合特征进行预测。Kawde 等人 [[105](#bib.bib105)] 通过将 DBN-RBM 用于有效特征提取并将 MLP 用于分类来实现了一个情感识别系统。
- en: (4) Mental Disease EEG. A large number of researchers exploited EEG signals
    to diagnose neurological disorders, especially epileptic seizure [[109](#bib.bib109)].
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 精神疾病 EEG。大量研究人员利用 EEG 信号诊断神经系统疾病，尤其是癫痫发作[[109](#bib.bib109)]。
- en: (i) Discriminative models. The CNN is widely used in the automatic detection
    of epileptic seizure [[112](#bib.bib112), [114](#bib.bib114), [116](#bib.bib116),
    [93](#bib.bib93)]. For example, Johansen et al. [[118](#bib.bib118)] adopted CNN
    to work on the high-passed (1 Hz) EEG signals of epileptic spike and achieved
    an AUC of 94.7%. Acharya et al. [[113](#bib.bib113)] employed a CNN model with
    13 layers on depression detection, which was evaluated on a local dataset with
    30 subjects and achieved the accuracies of 93.5% and 96.0% based on the left-
    and right- hemisphere EEG signals, respectively. Morabito et al. [[115](#bib.bib115)]
    tried to exploit a CNN structure to extract suitable features of multi-channel
    EEG signals to classify Alzheimer’s Disease from the patients with Mild Cognitive
    Impairment and healthy control group. The EEG signals are filtered in bandpass
    ($0.1\sim 30$ Hz) and achieved an accuracy of around 82% for three-class classification.
    Eapid Eye Movement Behavior Disorder (RBD) may cause many mental disorder diseases
    like Parkinson’s disease (PD). Ruffini et al. [[111](#bib.bib111)] described an
    Echo State Networks (ESNs) model, a particular class of RNN, to distinguish RBD
    from healthy individuals. In some research, the discriminative model is only employed
    for feature extraction. For example, Ansari et al. [[119](#bib.bib119)] used CNN
    to extract the latent features and fed into a Random Forest classifier for the
    final seizure detection of neonatal babies. Chu et al. [[149](#bib.bib149)] combined
    CNN and a traditional classifier for schizophrenia recognition.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 区分模型。CNN 广泛用于自动检测癫痫发作[[112](#bib.bib112), [114](#bib.bib114), [116](#bib.bib116),
    [93](#bib.bib93)]。例如，Johansen 等人[[118](#bib.bib118)] 采用 CNN 对高通（1 Hz）癫痫尖峰 EEG
    信号进行处理，达到了 94.7% 的 AUC。Acharya 等人[[113](#bib.bib113)] 在抑郁症检测中使用了具有 13 层的 CNN 模型，该模型在包含
    30 名受试者的本地数据集上进行评估，基于左半球和右半球 EEG 信号分别达到了 93.5% 和 96.0% 的准确率。Morabito 等人[[115](#bib.bib115)]
    尝试利用 CNN 结构从多通道 EEG 信号中提取适合的特征，以区分阿尔茨海默病患者、轻度认知障碍患者和健康对照组。EEG 信号经过带通滤波（$0.1\sim
    30$ Hz），三类分类的准确率约为 82%。快速眼动行为障碍（RBD）可能导致许多精神障碍疾病，如帕金森病（PD）。Ruffini 等人[[111](#bib.bib111)]
    描述了一种回声状态网络（ESNs）模型，这是一类特殊的 RNN，用于区分 RBD 和健康个体。在一些研究中，区分模型仅用于特征提取。例如，Ansari 等人[[119](#bib.bib119)]
    使用 CNN 提取潜在特征，并将其输入随机森林分类器，以最终检测新生儿的癫痫发作。Chu 等人[[149](#bib.bib149)] 将 CNN 和传统分类器结合用于精神分裂症识别。
- en: (ii) Representative models. For disease detection, one commonly used method
    is adopting a representative model (e.g., DBN) followed by a softmax layer for
    classification [[127](#bib.bib127)]. Page et al. [[125](#bib.bib125)] adopted
    DBN-AE to extract informative features from seizure EEG signals. The extracted
    features were fed into a traditional logistic regression classifier for seizure
    detection. Al et al. [[131](#bib.bib131)] proposed a multi-view DBN-RBM structure
    to analyze EEG signals from depression patients. The proposed approach contains
    multiple input pathways, composed of two RBMs, while each corresponded to one
    EEG channel. All the input pathways would merge into a shared structure which
    is composed of another RBMs. Some papers would like to preprocess the EEG signals
    through dimensionality reduction methods such as PCA [[129](#bib.bib129)] while
    others prefer to directly fed the raw signals to the representative model [[122](#bib.bib122)].
    Lin et al. [[122](#bib.bib122)] proposed a sparse D-AE with three hidden layers
    to extract the representative features from epileptic EEG signals while Hosseini
    et al. [[129](#bib.bib129)] adopted a similar sparse D-AE with two hidden layers.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 代表性模型。在疾病检测中，一种常用的方法是采用代表性模型（例如 DBN），然后通过 softmax 层进行分类[[127](#bib.bib127)]。Page
    等人[[125](#bib.bib125)] 采用 DBN-AE 从癫痫 EEG 信号中提取信息特征。这些提取的特征被输入传统的逻辑回归分类器进行癫痫发作检测。Al
    等人[[131](#bib.bib131)] 提出了一个多视角 DBN-RBM 结构，用于分析抑郁症患者的 EEG 信号。该方法包含多个输入通路，由两个 RBM
    组成，每个通路对应一个 EEG 通道。所有输入通路将合并到一个共享结构中，该结构由另一个 RBM 组成。一些论文喜欢通过降维方法（如 PCA）对 EEG 信号进行预处理[[129](#bib.bib129)]，而其他人则更倾向于直接将原始信号输入代表性模型[[122](#bib.bib122)]。Lin
    等人[[122](#bib.bib122)] 提出了一个具有三个隐藏层的稀疏 D-AE，从癫痫 EEG 信号中提取代表性特征，而 Hosseini 等人[[129](#bib.bib129)]
    采用了一个类似的具有两个隐藏层的稀疏 D-AE。
- en: (iii) Hybrid models. A popular hybrid method is a combination of RNN and CNN.
    Shah et al. [[128](#bib.bib128)] investigated the performance of CNN-LSTM on seizure
    detection after channel selection and the sensitivities range from 33% to 37%
    while false alarms ranges from 38% to 50%. Golmohammadi et al. [[130](#bib.bib130)]
    proposed a hybrid architecture for automatic interpretation of EEG by integrating
    both the temporal and spatial information. 2D and 1D CNNs capture the spatial
    features while LSTM networks capture the temporal features. The authors claimed
    a sensitivity of 30.83% and a specificity of 96.86% on the well-known TUH EEG
    seizure corpus. In the detection of early-stage Creutzfeldt-Jakob Disease (SJD),
    Morabito et al. [[123](#bib.bib123)] combined D-AE and MLP together. The EEG signals
    of SJD were first filtered by bandpass (0.5$\sim$70 Hz) and then fed into a D-AE
    with two hidden layers for feature representation. At last, the MLP classifier
    obtained the accuracy of 81$\sim$ 83% in a local dataset. Convolutional autoencoder,
    replacing the fully-connected layers in a standard AE by convolutional and de-convolutional
    layers, is applied to extract the seizure features in an unsupervised manner [[124](#bib.bib124)].
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 混合模型。一种流行的混合方法是将 RNN 和 CNN 结合起来。Shah 等人 [[128](#bib.bib128)] 研究了 CNN-LSTM
    在通道选择后的癫痫检测性能，其灵敏度范围为 33% 到 37%，而误报率范围为 38% 到 50%。Golmohammadi 等人 [[130](#bib.bib130)]
    提出了一个用于自动解读 EEG 的混合架构，通过整合时间和空间信息来进行处理。2D 和 1D CNN 捕捉空间特征，而 LSTM 网络捕捉时间特征。作者声称，在著名的
    TUH EEG 癫痫语料库中，其灵敏度为 30.83%，特异性为 96.86%。在早期 Creutzfeldt-Jakob 病 (SJD) 的检测中，Morabito
    等人 [[123](#bib.bib123)] 将 D-AE 和 MLP 结合在一起。SJD 的 EEG 信号首先通过带通滤波（0.5$\sim$70 Hz）进行过滤，然后输入到一个具有两个隐藏层的
    D-AE 进行特征表示。最后，MLP 分类器在本地数据集中获得了 81$\sim$83% 的准确率。卷积自编码器通过用卷积和反卷积层替代标准 AE 中的全连接层，以无监督的方式提取癫痫特征
    [[124](#bib.bib124)]。
- en: (5) Data augmentation. The generative models such as GAN could be used for data
    augmentation in brain signal classification [[132](#bib.bib132)]. Palazzo et al.
    [[133](#bib.bib133)] first demonstrated that the information contained in brainwaves
    are empowered to distinguish the visual object and then extracted more robust
    and distinguishable representations of EEG data using RNN. At last, they employed
    the GAN paradigm to train an image generator conditioned by the learned EEG representations,
    which could convert the EEG signals into images [[133](#bib.bib133)]. Kavasidis
    et al. [[134](#bib.bib134)] aiming at converting EEG signals into images. The
    EEG signals were collected when the subjects were observing images on a screen.
    An LSTM layer was employed to extract the latent features from the EEG signals,
    and the extracted features were regarded as the input of a GAN structure. The
    generator and the discriminator of the GAN were both composed of convolutional
    layers. The generator was supposed to generate an image based on the input EEG
    signals after the pre-training. Abdelfattach et al. [[132](#bib.bib132)] adopted
    a GAN on seizure data augmentation. The generator and discriminator are both composed
    of fully-connected layers. The authors demonstrated that GAN outperforms other
    generative models such as AE and VAE. After the augmentation, the classification
    accuracy increased dramatically from 48% to 82%.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 数据增强。生成模型如 GAN 可以用于脑信号分类中的数据增强 [[132](#bib.bib132)]。Palazzo 等人 [[133](#bib.bib133)]
    首次证明了脑波中包含的信息能够区分视觉对象，并利用 RNN 提取了更强大和可区分的 EEG 数据表示。最后，他们采用了 GAN 模式训练一个以学习到的 EEG
    表示为条件的图像生成器，该生成器能够将 EEG 信号转换成图像 [[133](#bib.bib133)]。Kavasidis 等人 [[134](#bib.bib134)]
    旨在将 EEG 信号转换成图像。在受试者在屏幕上观察图像时收集 EEG 信号。采用 LSTM 层从 EEG 信号中提取潜在特征，并将提取的特征视为 GAN
    结构的输入。GAN 的生成器和鉴别器都由卷积层组成。生成器在预训练后被期望根据输入的 EEG 信号生成图像。Abdelfattach 等人 [[132](#bib.bib132)]
    采用 GAN 进行癫痫数据增强。生成器和鉴别器均由全连接层组成。作者证明了 GAN 的表现优于其他生成模型，如 AE 和 VAE。经过增强后，分类准确率从
    48% 大幅提高到 82%。
- en: (6) Others. Some researches have explored a wide range of exciting topics. The
    first one is how EEG signals are affected by audio/visual stimuli. This differs
    from the potentials evoked by audio/visual stimulations because the stimuli in
    this phenomenon always exist instead of flicking in a particular frequency. Stober
    et al. [[188](#bib.bib188), [142](#bib.bib142)] claimed that the rhythm-evoked
    EEG signals are informative enough to distinguish the rhythm stimuli. The authors
    conducted an experiment where 13 participants were stimulated by 23 rhythmic stimuli,
    including 12 East African and 12 Western stimuli. For the 24-category classification,
    the proposed CNN achieved a mean accuracy of 24.4%. After that, the authors exploited
    convolutional AE for representation learning and CNN for recognition and achieved
    an accuracy of 27% for 12-class classification [[157](#bib.bib157)]. Sternin et
    al. [[148](#bib.bib148)] adopted CNN to capture discriminative features from the
    EEG oscillations to distinguish whether the subject was listening or imaging music.
    Similarly, Sarkar et al. [[165](#bib.bib165)] designed two deep learning models
    to recognize the EEG signals aroused by audio or visual stimuli. For this binary
    classification task, the proposed CNN and DBN-RBM with three RBMs achieved the
    accuracy of 91.63% and 91.75%, respectively. Furthermore, the spontaneous EEG
    could be used to distinguish the user’s mental state (logical versus emotional)
    [[172](#bib.bib172)].
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 其他。一些研究探索了广泛的有趣主题。第一个是EEG信号如何受到音频/视觉刺激的影响。这不同于由音频/视觉刺激引起的电位，因为这种现象中的刺激总是存在的，而不是以特定频率闪烁。Stober等人[[188](#bib.bib188),
    [142](#bib.bib142)]声称，节奏引起的EEG信号足够信息化，可以区分节奏刺激。作者进行了一个实验，其中13名参与者接受了23种节奏刺激，包括12种东非刺激和12种西方刺激。在24类别分类中，提出的CNN达到了24.4%的平均准确率。之后，作者利用卷积AE进行表示学习，并用CNN进行识别，对于12类分类达到了27%的准确率[[157](#bib.bib157)]。Sternin等人[[148](#bib.bib148)]采用CNN从EEG振荡中捕捉辨别特征，以区分被试是否在听音乐或想象音乐。类似地，Sarkar等人[[165](#bib.bib165)]设计了两个深度学习模型，以识别由音频或视觉刺激引起的EEG信号。对于这项二分类任务，提出的CNN和具有三个RBM的DBN-RBM分别达到了91.63%和91.75%的准确率。此外，自发EEG还可以用于区分用户的心理状态（逻辑与情感）[[172](#bib.bib172)]。
- en: Moreover, some researchers focus on the impact on EEG of cognitive load [[138](#bib.bib138)]
    or physical workload [[221](#bib.bib221)]. Bashivan et al. [[159](#bib.bib159)]
    first extract informative features through wavelet entropy and band-specific power,
    which would be fed into a DBN-RBM for further refining. At last, an MLP is employed
    for cognitive load level recognition. The authors, in another work [[171](#bib.bib171)],
    also denoted to find the general features which are constant in inter-/intra-
    subjects scenarios under various mental load. Yin et al. [[150](#bib.bib150)]
    collected the EEG signals from different mental workload levels (e.g., high and
    low) for binary classification. The EEG signals are filtered by a low-pass filter,
    transformed to the frequency domain and be calculated the power spectral density
    (PSD). The extracted PSD features were fed into a denoising D-AE structure for
    future refining. They finally got an accuracy of 95.48%. Li et al. [[155](#bib.bib155)]
    worked on the recognition of mental fatigue level, including alert, slight fatigue,
    and severe fatigue.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究者关注认知负荷[[138](#bib.bib138)]或物理负荷[[221](#bib.bib221)]对脑电图（EEG）的影响。Bashivan等人[[159](#bib.bib159)]首先通过小波熵和带特定功率提取信息特征，这些特征将被输入到DBN-RBM中进一步精炼。最后，使用MLP进行认知负荷水平识别。作者在另一项工作中[[171](#bib.bib171)]也致力于寻找在各种心理负荷下在被试间/被试内场景中恒定的一般特征。Yin等人[[150](#bib.bib150)]收集了来自不同心理负荷水平（例如高和低）的EEG信号进行二分类。EEG信号经过低通滤波器滤波，转换到频域，并计算功率谱密度（PSD）。提取的PSD特征被输入到去噪D-AE结构中进行后续精炼。他们最终获得了95.48%的准确率。Li等人[[155](#bib.bib155)]研究了心理疲劳水平的识别，包括警觉、轻度疲劳和严重疲劳。
- en: 'In addition, EEG based driver fatigue detection is an attractive area [[158](#bib.bib158),
    [151](#bib.bib151), [147](#bib.bib147)]. Huang et al. [[140](#bib.bib140)] designed
    a 3D CNN to predict the reaction time in drowsiness driving. This is meaningful
    to reduce traffic accident. Hajinoroozi et al. [[153](#bib.bib153)] adopted a
    DBN-RBM to handle the EEG signals which were processed by ICA. They achieved an
    accuracy of around 85% in binary classification (‘drowsy’ or ‘alert’). The strength
    of this paper is that it evaluated the DBN-RBM on three levels: time samples,
    channel epochs, and windowed samples. The experiments illustrated that the channel
    epoch level outperformed the other two levels. San et al. [[154](#bib.bib154)]
    combined deep learning models with a traditional classifier to detect driver fatigue.
    The model contains a DBN-RBM structure followed by an SVM classifier, which achieved
    the detection accuracy of 73.29%. Almogbel et al. [[145](#bib.bib145)] investigated
    the drivers’ mental state under different low workload levels. A proposed CNN
    is claimed to detect the driving workload directly based on the raw EEG signals.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于 EEG 的驾驶员疲劳检测是一个具有吸引力的领域 [[158](#bib.bib158), [151](#bib.bib151), [147](#bib.bib147)]。Huang
    等人 [[140](#bib.bib140)] 设计了一个 3D CNN 来预测疲劳驾驶中的反应时间。这对于减少交通事故具有重要意义。Hajinoroozi
    等人 [[153](#bib.bib153)] 采用了 DBN-RBM 处理经过 ICA 处理的 EEG 信号。他们在二分类（‘疲劳’或‘警觉’）中达到了约
    85% 的准确率。本文的亮点在于它在三个层次上评估了 DBN-RBM：时间样本、通道时段和窗口样本。实验表明，通道时段层次优于其他两个层次。San 等人 [[154](#bib.bib154)]
    将深度学习模型与传统分类器结合起来进行驾驶员疲劳检测。该模型包含一个 DBN-RBM 结构，后跟一个 SVM 分类器，检测准确率达到了 73.29%。Almogbel
    等人 [[145](#bib.bib145)] 研究了不同低工作负荷水平下驾驶员的心理状态。提出的 CNN 被声称能够直接基于原始 EEG 信号检测驾驶工作负荷。
- en: The research of the detection of eye state has shown exceeding accuracy. Narejo
    et al. [[152](#bib.bib152)] explored the detection of eye state (closed or open)
    based on EEG signals. They tried a DBN-RBM with three RBMs and a DBN-AE with three
    AEs and achieved a high accuracy of 98.9%. Reddy et al. [[136](#bib.bib136)] tried
    a simpler structure, MLP, and got a slightly lower accuracy of 97.5%.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对眼睛状态检测的研究显示出了超高的准确性。Narejo 等人 [[152](#bib.bib152)] 探索了基于 EEG 信号的眼睛状态（闭合或张开）检测。他们尝试了一个具有三个
    RBMs 的 DBN-RBM 和一个具有三个 AEs 的 DBN-AE，并达到了 98.9% 的高准确率。Reddy 等人 [[136](#bib.bib136)]
    尝试了更简单的结构 MLP，得到了略低的 97.5% 的准确率。
- en: Furthermore, to make this survey more complete, we provide a brief introduction
    of Event-related desynchronization/synchronization (ERD/ERS). ERD/ERS refers to
    the phenomena that the magnitude and frequency distribution of the EEG signal
    power changes during a specific brain state [[36](#bib.bib36)]. In particular,
    ERD denotes the power decrease of ongoing EEG signals while ERS represents the
    power increase of EEG signals. This characteristic of ERD/ERS of brain signals
    can be used to detect the event which caused the EEG fluctuation. For example,
    [[222](#bib.bib222)] presents the ERD/ERS phenomena in motor cortex recorded during
    a motor-imagery task.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使本综述更加完整，我们提供了事件相关去同步化/同步化（ERD/ERS）的简要介绍。ERD/ERS 指的是在特定脑状态下 EEG 信号功率的幅度和频率分布发生变化的现象
    [[36](#bib.bib36)]。特别地，ERD 表示正在进行的 EEG 信号的功率减少，而 ERS 则代表 EEG 信号的功率增加。这种 ERD/ERS
    脑信号的特性可以用来检测引起 EEG 波动的事件。例如，[[222](#bib.bib222)] 介绍了在运动想象任务中记录的运动皮层的 ERD/ERS 现象。
- en: 'ERD/ERS mainly appears in sensory, cognitive and motor procedures, which is
    not widely used in brain research due to the drawbacks like unstable accuracy
    cross subjects [[36](#bib.bib36)]. In most of the situations, the ERD/ERS is regarded
    as a specific feature of EEG powers for further analysis [[81](#bib.bib81), [4](#bib.bib4)].
    The task causes an ERD in the mu band (8-13 Hz) of EEG and an ERS in the beta
    band (13-30 Hz). In particular, the ERD/ERS were calculated as relative changes
    in power concerning baseline: $ERD/ERS=(P_{e}-P_{b})/P_{b}$, where $P_{e}$ denotes
    the signals power over one-second segment when the event occurring and $P_{b}$
    denotes the signal power in a one-second segment during baseline which is before
    the event [[71](#bib.bib71)]. Generally, the baseline refers to the rest state.
    For example, Sakhavi et al. calculated the ERD/ERS map and analyzed the different
    patterns among different tasks. The analysis demonstrated that the dynamic of
    energy should be considered because the static energy does not contains enough
    information [[86](#bib.bib86)].'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ERD/ERS 主要出现在感觉、认知和运动过程中，由于存在如跨个体准确性不稳定等缺点，因此在脑研究中的应用不广泛 [[36](#bib.bib36)]。在大多数情况下，ERD/ERS
    被视为 EEG 功率的特定特征，用于进一步分析 [[81](#bib.bib81), [4](#bib.bib4)]。该任务在 EEG 的 mu 波段 (8-13
    Hz) 引起 ERD，在 beta 波段 (13-30 Hz) 引起 ERS。特别地，ERD/ERS 被计算为相对于基线的功率相对变化：$ERD/ERS=(P_{e}-P_{b})/P_{b}$，其中
    $P_{e}$ 表示事件发生时一秒段内的信号功率，$P_{b}$ 表示事件前基线期间一秒段内的信号功率 [[71](#bib.bib71)]。一般来说，基线指的是休息状态。例如，Sakhavi
    等人计算了 ERD/ERS 图并分析了不同任务之间的不同模式。分析表明，应考虑能量的动态，因为静态能量并未包含足够的信息 [[86](#bib.bib86)]。
- en: There are several overlooked yet promising areas. Baltatzis et al. [[141](#bib.bib141)]
    adopted CNN to detect school bullying through the EEG when watching the specific
    video. They achieved 93.7% and 88.58% for binary and four-class classification.
    Khurana et al. [[223](#bib.bib223)] proposed deep dictionary learning that outperformed
    several deep learning methods. Volker et al. [[143](#bib.bib143)] evaluated the
    use of Deep CNN in flanker task, which achieved an averaging accuracy of 84.1%
    on the seen subject and 81.7 on the unseen subject. Zhang et al. [[160](#bib.bib160)]
    combined CNN and graph network to discover the latent information from the EEG
    signal.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个被忽视但前景广阔的领域。Baltatzis 等人 [[141](#bib.bib141)] 采用 CNN 通过观看特定视频时的 EEG 来检测校园欺凌。他们在二分类和四分类任务中分别取得了
    93.7% 和 88.58% 的准确率。Khurana 等人 [[223](#bib.bib223)] 提出了深度字典学习，这种方法超越了几种深度学习方法。Volker
    等人 [[143](#bib.bib143)] 评估了深度 CNN 在侧翼任务中的应用，达到了在已见对象上的平均准确率 84.1% 和在未见对象上的 81.7%。Zhang
    等人 [[160](#bib.bib160)] 将 CNN 和图网络结合，以从 EEG 信号中发现潜在信息。
- en: Miranda-Correa et al. [[104](#bib.bib104)] proposed a cascaded framework by
    combing RNN and CNN to predict individuals’ affective level and personal factors
    (Big-five personality traits, mood, and social context). An experiment conducted
    by Putten et al. [[146](#bib.bib146)] attempted to identify the user’s gender
    based on their EEG signals. They employed a standard CNN algorithm and achieved
    the binary classification accuracy of 81% over a local dataset. The detection
    of emergency braking intention could help to reduce the responses time. Hernandez
    et al. [[144](#bib.bib144)] demonstrated that the driver’s EEG signals could distinguish
    braking intention and normal driving state. They combined a CNN algorithm which
    achieved the accuracy of 71.8% in binary classification. Behncke et al. [[139](#bib.bib139)]
    applied deep learning, a CNN model, in the context of robot assistive devices.
    They attempted to use CNN to improve the accuracy of decoding robot errors from
    EEG while the subject was watching the robot both during an object grasping and
    a pouring task.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Miranda-Correa 等人 [[104](#bib.bib104)] 提出了一个级联框架，通过结合 RNN 和 CNN 来预测个人的情感水平和个人因素（大五人格特质、情绪和社会背景）。Putten
    等人 [[146](#bib.bib146)] 进行了一项实验，尝试基于 EEG 信号识别用户的性别。他们采用了标准的 CNN 算法，在本地数据集上实现了
    81% 的二分类准确率。检测紧急制动意图可以帮助减少反应时间。Hernandez 等人 [[144](#bib.bib144)] 证明了驾驶员的 EEG 信号可以区分制动意图和正常驾驶状态。他们结合了一个
    CNN 算法，达到了 71.8% 的二分类准确率。Behncke 等人 [[139](#bib.bib139)] 在机器人辅助设备的背景下应用了深度学习的
    CNN 模型。他们尝试使用 CNN 改善在物体抓取和倒水任务中观看机器人时从 EEG 中解码机器人错误的准确性。
- en: Teo et al. [[135](#bib.bib135)] tried to combine the brain signal and recommender
    system, which predicted the user’s preference by EEG signals. There were sixteen
    participants took the experiments which collected the EEG signals when the subject
    was presented 60 bracelet-like objects as rotating visual stimuli (a 3D object).
    Then, an MLP algorithm was adopted to classify the user like or dislike the object.
    This exploration got the prediction accuracy of 63.99%. Some researchers have
    tried to explore a common framework which can be used for various brain signal
    paradigms. Lawhern et al. [[73](#bib.bib73)] introduced EEGNet based on a compact
    CNN and evaluated its robustness in various brain signal contexts [[73](#bib.bib73)].
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Teo 等人 [[135](#bib.bib135)] 尝试将脑信号与推荐系统结合，通过 EEG 信号预测用户的偏好。共有十六名参与者参与了实验，当实验对象看到
    60 个类似手镯的物体作为旋转视觉刺激（一个 3D 物体）时，收集了 EEG 信号。然后，采用 MLP 算法来分类用户喜欢或不喜欢这些物体。这项探索获得了
    63.99% 的预测准确率。一些研究人员尝试探索一种通用框架，可用于各种脑信号范式。Lawhern 等人 [[73](#bib.bib73)] 介绍了基于紧凑
    CNN 的 EEGNet，并评估了其在各种脑信号背景下的鲁棒性 [[73](#bib.bib73)]。
- en: 4.1.2 Evoked Potential
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 诱发电位
- en: Next, we introduce the latest researches on evoked potentials including ERP
    and SSEP.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍包括 ERP 和 SSEP 在内的最新诱发电位研究。
- en: (1) ERP. In most situations, the ERP signals are analyzed through P300 phenomena.
    Meanwhile, almost all the studies on P300 are based on the scenario of ERP. Therefore,
    in this section, a majority of the P300 related publications are introduced in
    the subsection of VEP/AEP according to the scenario.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: (1) ERP。在大多数情况下，ERP 信号通过 P300 现象进行分析。同时，几乎所有关于 P300 的研究都基于 ERP 的情境。因此，在这一部分，大多数与
    P300 相关的出版物都在 VEP/AEP 的子部分中介绍。
- en: (i) VEP. VEP is one of the most popular subcategories of ERP [[23](#bib.bib23),
    [224](#bib.bib224), [163](#bib.bib163)]. Ma et al. [[225](#bib.bib225)] worked
    on motion-onset VEP (mVEP) by extracting representative features through deep
    learning and adopted genetic algorithm combined with a multi-level sensing structure
    to compress the raw signals. The compressed signals were sent to a DBN-RBM algorithm
    to capture the more abstract high-level features. Maddula et al. [[170](#bib.bib170)]
    filtered the P300 signals with visual stimuli by a bandpass filter ($2\sim 35$
    Hz) and then fed into a proposed hybrid deep learning model for further analysis.
    The model includes a 2D CNN structure to capture the spatial features followed
    by an LSTM layer for temporal feature extraction. Liu et al. [[168](#bib.bib168)]
    combined a DBN-RBM representative model with an SVM classifier for concealed information
    test and achieved a high accuracy of 97.3% over a local dataset. Gao et al. [[167](#bib.bib167)]
    employed an AE model for feature extraction followed by an SVM classifier. In
    the experiment, each segment contains 150 points, which were divided into five
    time-steps, and each step had 30 points. This model achieved an accuracy of 88.1%
    over a local dataset. A wide range of P300 related studies is based on P300 speller
    [[173](#bib.bib173)], which allows the user to write characters. Cecotti et al.
    [[177](#bib.bib177)] tried to increase the P300 detection accuracy for more precise
    word-spelling. A new model was presented based on CNN, which including five low-level
    CNN classifiers with the different feature set, and the final high-level results
    are voted by the low-level classifiers. The highest accuracy reached 95.5% over
    the dataset II from the third BCI competition. Liu et al. [[164](#bib.bib164)]
    proposed a Batch Normalized Neural Network (BN³) which is a variant of CNN in
    P300 speller. The proposed method consists of six layers, and the batch normalization
    was operated in each batch. Kawasaki et al. [[162](#bib.bib162)] employed an MLP
    model to detect P300 segments from non-P300 segments and achieved the accuracy
    of 90.8%.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: (i) VEP。VEP是ERP的最受欢迎的子类别之一[[23](#bib.bib23), [224](#bib.bib224), [163](#bib.bib163)]。Ma等人[[225](#bib.bib225)]通过深度学习提取代表性特征，并采用结合多级传感结构的遗传算法来压缩原始信号，研究了运动启动VEP（mVEP）。压缩后的信号被送入DBN-RBM算法，以捕捉更抽象的高级特征。Maddula等人[[170](#bib.bib170)]使用带通滤波器（$2\sim
    35$ Hz）滤波视觉刺激的P300信号，然后输入到提出的混合深度学习模型中进行进一步分析。该模型包括一个2D CNN结构来捕捉空间特征，随后是一个LSTM层用于时间特征提取。Liu等人[[168](#bib.bib168)]将DBN-RBM代表性模型与SVM分类器结合用于隐藏信息测试，并在本地数据集上实现了97.3%的高准确率。Gao等人[[167](#bib.bib167)]采用AE模型进行特征提取，随后使用SVM分类器。在实验中，每个片段包含150个点，这些点被分成五个时间步骤，每个步骤有30个点。该模型在本地数据集上的准确率达到88.1%。大量P300相关研究基于P300拼写器[[173](#bib.bib173)]，该拼写器允许用户书写字符。Cecotti等人[[177](#bib.bib177)]尝试提高P300检测准确率，以便更精确地拼写单词。提出了一种基于CNN的新模型，包括五个不同特征集的低级CNN分类器，最终的高级结果由低级分类器投票决定。最高准确率达到了来自第三届BCI竞赛数据集II的95.5%。Liu等人[[164](#bib.bib164)]提出了一种批量归一化神经网络（BN³），这是P300拼写器中CNN的一个变体。该方法包含六层，并且在每个批次中进行了批量归一化。Kawasaki等人[[162](#bib.bib162)]使用MLP模型检测P300片段与非P300片段，并实现了90.8%的准确率。
- en: (ii) AEP. A few works focused on the recognition of AEP. For example, Carabez
    et al. [[187](#bib.bib187)] proposed and tested 18 CNN structures to classify
    single-trial AEP signals. In the experiment, the volunteers were required to wear
    on an earphone which produces auditory stimulus designed based on the oddball
    paradigm. The experimental analysis demonstrated that the CNN frameworks, regardless
    of the number of convolutional layers, were effective to extract the temporal
    and spatial features and provided competitive results. The AEP signals are filtered
    by $0.1\sim 8$ Hz and downsampled from 256 Hz to 25 Hz. The experimental results
    showed that the downsampled data work better.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) AEP。一些研究集中在AEP的识别上。例如，Carabez等人[[187](#bib.bib187)] 提出了并测试了18种CNN结构来分类单次试验的AEP信号。在实验中，志愿者需佩戴耳机，该耳机产生基于奇偶范式设计的听觉刺激。实验分析表明，无论卷积层的数量如何，CNN框架都能有效提取时间和空间特征，并提供了具有竞争力的结果。AEP信号经过$0.1\sim
    8$ Hz的滤波，并从256 Hz下采样到25 Hz。实验结果表明，下采样的数据效果更好。
- en: (iii) RSVP. Among various VEP diagrams, RSVP has attracted much attention [[183](#bib.bib183)].
    In the analysis of RSVP, a number of discriminative deep learning models (e,g.,
    CNN [[177](#bib.bib177), [178](#bib.bib178), [182](#bib.bib182)] and MLP [[174](#bib.bib174)])
    has achieved a big success. A common preprocessing method used in RSVP signals
    is frequency filtering. The pass bands are generally ranged from $0.1\sim 50$
    Hz [[176](#bib.bib176), [185](#bib.bib185)]. Cecotti et al. [[12](#bib.bib12)]
    worked on the classification of ERP signals in RSVP scenario and proposed a modified
    CNN model for the detection of the specific target in RSVP. In the experiment,
    the images of faces and cars were regarded as target or non-target, respectively.
    The image presenting frequency is 2 Hz. In each session, the target probability
    was 10%. The proposed model offered an AUC of 86.1%. Hajinoroozi et al. [[179](#bib.bib179)]
    adopted a CNN model targeting the inter-subject and inter-task detection of RSVP.
    The experimental results showed that CNN worked good in cross-task but failed
    to get satisfying performance in the cross-subject scenario. Mao et al. [[175](#bib.bib175)]
    compared three different deep neural network algorithms in the prediction of whether
    the subject had seen the target or not. The MLP, CNN, and DBN models obtained
    the AUC of 81.7%, 79.6%, and 81.6%, respectively. The author also applied a CNN
    model to analyze the RSVP signals for person identification [[180](#bib.bib180)].
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) RSVP。在各种 VEP 图中，RSVP 引起了广泛关注 [[183](#bib.bib183)]。在 RSVP 的分析中，许多判别性深度学习模型（例如
    CNN [[177](#bib.bib177), [178](#bib.bib178), [182](#bib.bib182)] 和 MLP [[174](#bib.bib174)]）取得了巨大成功。RSVP
    信号中常用的预处理方法是频率滤波。通带范围通常从 $0.1\sim 50$ Hz [[176](#bib.bib176), [185](#bib.bib185)]。Cecotti
    等人 [[12](#bib.bib12)] 研究了 RSVP 情境中的 ERP 信号分类，并提出了一种改进的 CNN 模型用于检测 RSVP 中的特定目标。在实验中，面孔和汽车的图像分别被视为目标或非目标。图像呈现频率为
    2 Hz。在每个实验中，目标概率为 10%。提出的模型提供了 86.1% 的 AUC。Hajinoroozi 等人 [[179](#bib.bib179)]
    采用了一个 CNN 模型来检测 RSVP 的受试者间和任务间的变化。实验结果显示，CNN 在任务间的表现良好，但在受试者间的表现不尽如人意。Mao 等人 [[175](#bib.bib175)]
    比较了三种不同的深度神经网络算法在预测受试者是否见过目标的表现。MLP、CNN 和 DBN 模型的 AUC 分别为 81.7%、79.6% 和 81.6%。作者还应用了一个
    CNN 模型来分析 RSVP 信号以进行个人识别 [[180](#bib.bib180)]。
- en: The representative deep learning models are also applied in RSVP. Vareka et
    al. [[186](#bib.bib186)] verified if deep learning performs well for single trial
    P300 classification. They conducted an RSVP experiment while the subjects were
    asked to recognize the target from non-target and distracters. Then a DBN-AE was
    implemented and compared with some non-deep learning algorithms. The DBN-AE was
    composed of five AEs while the hidden layer of the last AE only has two nodes
    which can be used for classification through softmax function. Finally, the proposed
    model achieved the accuracy of 69.2%. Manor et al. [[181](#bib.bib181)] applied
    two deep neural networks to deal with the RSVP signals after lowpass filtering
    ($0\sim 51$ Hz). Discriminative CNN achieved the accuracy of 85.06%. Meanwhile,
    the representative convolutional D-AE achieved the accuracy of 80.68%.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性的深度学习模型也应用于 RSVP。Vareka 等人 [[186](#bib.bib186)] 验证了深度学习在单次试验 P300 分类中的表现。他们进行了一个
    RSVP 实验，要求受试者从非目标和干扰物中识别目标。然后实施了一个 DBN-AE 模型，并与一些非深度学习算法进行了比较。DBN-AE 由五个 AE 组成，而最后一个
    AE 的隐藏层只有两个节点，这些节点可以通过 softmax 函数用于分类。最终，提出的模型达到了 69.2% 的准确率。Manor 等人 [[181](#bib.bib181)]
    在低通滤波（$0\sim 51$ Hz）后应用了两个深度神经网络处理 RSVP 信号。判别性 CNN 达到了 85.06% 的准确率。同时，代表性的卷积 D-AE
    达到了 80.68% 的准确率。
- en: (2) SSEP. Most of deep learning-based studies in SSEP area focus on SSVEP like
    [[191](#bib.bib191)]. SSVEP refers to brain oscillations evoked by the flickering
    visual stimuli, which generally produced from the parietal and occipital regions
    [[192](#bib.bib192)]. Attia et al. [[196](#bib.bib196)] aimed at finding an intermediate
    representation of SSVEP. A hybrid method combined CNN and RNN was proposed to
    capture the meaningful features from the time domain directly, which achieved
    the accuracy of 93.59%. Waytowich et al. [[192](#bib.bib192)] applied a compact
    CNN model to directly work on the raw SSVEP signals without any hand-crafted features.
    The reported cross subject mean accuracy was approximately 80%. Thomas et al.
    [[190](#bib.bib190)] first filter the raw SSVEP signals through a bandpass filter
    ($5\sim 48$ Hz) and then operated discrete FFT on consecutive 512 points. The
    processed data were classified by a CNN (69.03%) and an LSTM (66.89%) independently.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: (2) SSEP。大多数基于深度学习的 SSEP 领域研究关注于 SSVEP，如 [[191](#bib.bib191)]。SSVEP 是由闪烁视觉刺激引发的大脑振荡，通常来自顶叶和枕叶区域
    [[192](#bib.bib192)]。Attia 等人 [[196](#bib.bib196)] 旨在寻找 SSVEP 的中间表示。提出了一种结合 CNN
    和 RNN 的混合方法，直接从时间域捕捉有意义的特征，准确率达到了 93.59%。Waytowich 等人 [[192](#bib.bib192)] 应用了一种紧凑的
    CNN 模型，直接处理原始 SSVEP 信号，而无需任何手工特征。报告的跨主体平均准确率约为 80%。Thomas 等人 [[190](#bib.bib190)]
    首先通过带通滤波器（$5\sim 48$ Hz）滤波原始 SSVEP 信号，然后对连续的 512 个点进行离散 FFT。处理后的数据分别由 CNN（69.03%）和
    LSTM（66.89%）进行分类。
- en: Perez et al. [[197](#bib.bib197)] adopted a representative model, a sparse AE,
    to extract the distinct features from the SSVEP from multi-frequency visual stimuli.
    The proposed model employed a softmax layer for the final classification and achieved
    the accuracy of 97.78%. Kulasingham et al. [[195](#bib.bib195)] classified SSVEP
    signals in the context of guilty knowledge test. The authors applied DBN-RBM and
    DBN-AE independently and achieved the accuracy of 86.9% and 86.01%, respectively.
    Hachem et al. [[189](#bib.bib189)] investigated the influence of fatigue on SSVEP
    through an MLP model during wheelchair navigation. The goal of this study was
    to seek the key parameters to switch between manual, semi-autonomous, and autonomous
    wheelchair command. Aznan et al. [[193](#bib.bib193)] explored the SSVEP classification,
    where the signals were collected through dry electrodes. The dry signals were
    more challenging for the lower SNR than standard EEG signals. This study applied
    a CNN discriminative model and achieved the highest accuracy of 96% over a local
    dataset.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Perez 等人 [[197](#bib.bib197)] 采用了一个代表性模型，即稀疏自编码器（AE），从多频视觉刺激的SSVEP中提取出独特的特征。该模型使用了一个
    softmax 层进行最终分类，并达到了 97.78% 的准确率。Kulasingham 等人 [[195](#bib.bib195)] 在有罪知识测试的背景下对
    SSVEP 信号进行了分类。作者分别应用了 DBN-RBM 和 DBN-AE，并达到了 86.9% 和 86.01% 的准确率。Hachem 等人 [[189](#bib.bib189)]
    通过在轮椅导航过程中使用 MLP 模型，研究了疲劳对 SSVEP 的影响。本研究的目标是寻找在手动、半自主和自主轮椅指令之间切换的关键参数。Aznan 等人
    [[193](#bib.bib193)] 探索了 SSVEP 分类，其中信号通过干电极收集。干信号相比标准 EEG 信号，具有较低 SNR，处理难度更大。本研究应用了
    CNN 判别模型，并在本地数据集上取得了 96% 的最高准确率。
- en: 4.2 fNIRS
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 fNIRS
- en: Up to now, only a few of researchers paid attention on deep learning-based fNIRS.
    Naseer et al. [[38](#bib.bib38)] analyzed the difference between two mental tasks
    (mental arithmetic and rest) based on fNIRS signals. The authors manually extracted
    six features from the prefrontal cortex fNIRS and compared six different classifiers.
    The results demonstrated that the MLP with the accuracy of 96.3% outperformed
    all the traditional classifiers, including SVM, KNN, naive Bayes, etc. Huve et
    al. [[198](#bib.bib198)] classified the fNIRS signals, which were collected from
    the subjects during three mental states, including substractions, word generation,
    and rest. The employed MLP model achieved the accuracy of 66.48% based on the
    hand-crafted features (e.g., the concentration of OxyHb/DeoxyHb). After that,
    the authors study the mobile robot control through fNIRS signals and got the binary
    classification accuracy of 82% (offline) and 66% (online) [[199](#bib.bib199)].
    Chiarelli et al. [[71](#bib.bib71)] exploited the combination of fNIRS and EEG
    for left/right MI EEG classification. Sixteen features extracted from fNIRS signals
    (eight from OxyHb and eight from DeoxyHb) were fed into an MLP classifier with
    four hidden layers.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，仅有少数研究者关注基于深度学习的功能性近红外光谱（fNIRS）。Naseer等人[[38](#bib.bib38)]分析了基于fNIRS信号的两种心理任务（心理算术和休息）之间的差异。作者手动从前额叶fNIRS中提取了六个特征，并比较了六种不同的分类器。结果表明，具有96.3%准确率的MLP优于所有传统分类器，包括SVM、KNN、朴素贝叶斯等。Huve等人[[198](#bib.bib198)]对在三种心理状态下（包括减法、词生成和休息）收集的fNIRS信号进行了分类。所用的MLP模型在基于手工特征（例如OxyHb/DeoxyHb的浓度）下达到了66.48%的准确率。之后，作者研究了通过fNIRS信号进行的移动机器人控制，得到了82%（离线）和66%（在线）的二分类准确率[[199](#bib.bib199)]。Chiarelli等人[[71](#bib.bib71)]利用fNIRS和EEG的组合进行左右脑动觉（MI）EEG分类。从fNIRS信号中提取的十六个特征（八个来自OxyHb，八个来自DeoxyHb）被输入到具有四个隐藏层的MLP分类器中。
- en: On the other hand, Hiroyasu et al. [[201](#bib.bib201)] attempted to detect
    the gender of the subject through their fNIRS signals. The authors employed a
    denoising D-AE with three hidden layers to extract distinctive features to be
    fed into an MLP classifier for gender detection. The model was evaluated over
    a local dataset and gained the average accuracy of 81%. In this study, the authors
    also pointed out that, compared with Positron Emission Tomography (PET) and fMRI,
    fNIRS has higher time resolution and more affordable [[201](#bib.bib201)].
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Hiroyasu等人[[201](#bib.bib201)]尝试通过fNIRS信号检测受试者的性别。作者采用了具有三个隐藏层的去噪自编码器（D-AE）来提取独特特征，并输入到MLP分类器中进行性别检测。该模型在本地数据集上评估，获得了81%的平均准确率。在这项研究中，作者还指出，与正电子发射断层扫描（PET）和fMRI相比，fNIRS具有更高的时间分辨率和更高的性价比[[201](#bib.bib201)]。
- en: 4.3 fMRI
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 fMRI
- en: Recently, several deep learning methods have been applied to fMRI analysis,
    especially on the diagnosis of cognitive impairment [[14](#bib.bib14), [33](#bib.bib33)].
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，几种深度学习方法已被应用于fMRI分析，特别是在认知障碍的诊断方面[[14](#bib.bib14), [33](#bib.bib33)]。
- en: (1) Discriminative models. Among the discriminative models, CNN is a promising
    model to analyze fMRI [[206](#bib.bib206)]. For example, Havaei et al. built a
    segmentation approach for brain tumor based on fMRI with a novel CNN algorithm
    which can capture both the global features and the local features simultaneously
    [[205](#bib.bib205)]. The convolutional filters have different size. Thus, the
    small-size and large-size filter could exploit the local and global features,
    independently. Sarraf et al. [[226](#bib.bib226), [207](#bib.bib207)] applied
    deep CNN to recognize Alzheimer’s Disease based on fMRI and MRI data. Morenolopez
    et al. [[227](#bib.bib227)] employed a CNN model to deal with fMRI of brain tumor
    patients for three-class recognition (normal, edema, or active tumor). The model
    was evaluated over BRATS dataset and obtained the F1 score of 88%. Hosseini et
    al. [[117](#bib.bib117)] employed CNN for feature extraction. The extracted features
    were classified by SVM for the detection of an epileptic seizure.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: （1）判别模型。在判别模型中，CNN是一个有前景的模型，用于分析fMRI[[206](#bib.bib206)]。例如，Havaei等人基于fMRI建立了一种脑肿瘤分割方法，使用了一种新颖的CNN算法，能够同时捕捉全局特征和局部特征[[205](#bib.bib205)]。卷积滤波器的大小不同。因此，小尺寸和大尺寸的滤波器可以分别利用局部和全局特征。Sarraf等人[[226](#bib.bib226),
    [207](#bib.bib207)]应用深度CNN来识别基于fMRI和MRI数据的阿尔茨海默病。Morenolopez等人[[227](#bib.bib227)]使用CNN模型处理脑肿瘤患者的fMRI数据进行三类识别（正常、浮肿或活动性肿瘤）。该模型在BRATS数据集上评估，获得了88%的F1分数。Hosseini等人[[117](#bib.bib117)]使用CNN进行特征提取。提取的特征通过SVM进行分类，以检测癫痫发作。
- en: Furthermore, Li et al. proposed a data completion method based on CNN. In particular,
    utilizing the information from fMRI data to complete PET, then train the classifier
    based on both fMRI and PET [[208](#bib.bib208)]. In the model, the input data
    of the proposed CNN is the fMRI patch, and the output is a PET patch. There are
    two convolutional layers with ten filters mapping the fMRI to PET. The experiments
    illustrated that the classifier trained by the combination of fMRI and PET (92.87%)
    outperformed the one trained by solo fMRI (91.92%) Moreover, Koyamada et al. used
    a nonlinear MLP to extract common features from different subjects. The model
    is evaluated over a dataset from the Human Connectome Project (HCP) [[202](#bib.bib202)].
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Li 等人提出了一种基于 CNN 的数据补全方法。特别是，利用 fMRI 数据的信息来补全 PET 数据，然后基于 fMRI 和 PET 数据训练分类器[[208](#bib.bib208)]。在模型中，所提议的
    CNN 输入数据是 fMRI 补丁，输出是 PET 补丁。该模型有两层卷积层，每层有十个滤波器，将 fMRI 映射到 PET。实验表明，使用 fMRI 和
    PET 组合训练的分类器（92.87%）优于仅使用 fMRI 训练的分类器（91.92%）。此外，Koyamada 等人使用非线性 MLP 从不同受试者中提取共同特征。该模型在来自人类连接组项目（HCP）的数据集上进行评估[[202](#bib.bib202)]。
- en: (2) Representative models. A wide range of publications demonstrated the effectiveness
    of representative models in recognition of fMRI data [[213](#bib.bib213)]. Hu
    et al. [[217](#bib.bib217)] used demonstrated that deep learning outperforms other
    machine learning methods in the diagnosis of neurological disorders such as Alzheimer’s
    disease. Firstly, the fMRI images were converted to a matrix to represent the
    activity of 90 brain regions. Secondly, a correlation matrix is obtained by calculating
    the correlation between each pair of brain regions to represent the functional
    connectivity between different brain regions. Furthermore, a targeted AE is built
    to classify the correlation matrix, which is sensitive to AD. The proposed approach
    achieved an accuracy of 87.5%. Plis et al. [[211](#bib.bib211)] employed a DBN-RBM
    with three RBM components to extract the distinctive features from ICA processed
    fMRI and finally achieved an average F1 measure of above 90% over four public
    datasets. Suk et al. compared the effectiveness of DBN-RBM and DBN-AE on Alzheimer’s
    disease detection and the experimental results showed that the former obtained
    the accuracy of 95.4%, which is slightly lower than the latter (97.9%) [[210](#bib.bib210)].
    Suk et al. [[209](#bib.bib209)] applied a D-AE model to extract latent features
    from the resting-state fMRI data on the diagnosis of Mild Cognitive Impairment
    (MCI). The latent features are fed into a SVM classifier which achieved the accuracy
    of 72.58%. Ortiz et al. [[212](#bib.bib212)] proposed a multi-view DBN-RBM to
    receives the information of MRI and PET simultaneously. The learned representations
    were sent to several simple SVM classifiers which were ensembled to form a high-level
    stronger classifier by voting.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 代表性模型。大量出版物展示了代表性模型在 fMRI 数据识别中的有效性[[213](#bib.bib213)]。Hu 等人[[217](#bib.bib217)]展示了深度学习在诊断阿尔茨海默病等神经系统疾病中优于其他机器学习方法。首先，将
    fMRI 图像转换为矩阵，以表示 90 个脑区的活动。其次，通过计算每对脑区之间的相关性来获得相关矩阵，以表示不同脑区之间的功能连接。此外，构建了一个针对
    AD 敏感的 AE 来对相关矩阵进行分类。所提方法的准确率为 87.5%。Plis 等人[[211](#bib.bib211)]采用了具有三个 RBM 组件的
    DBN-RBM，从经过 ICA 处理的 fMRI 中提取出独特特征，最终在四个公共数据集上获得了超过 90% 的平均 F1 评分。Suk 等人比较了 DBN-RBM
    和 DBN-AE 在阿尔茨海默病检测中的效果，实验结果显示前者的准确率为 95.4%，略低于后者（97.9%）[[210](#bib.bib210)]。Suk
    等人[[209](#bib.bib209)]应用 D-AE 模型从静息态 fMRI 数据中提取潜在特征用于轻度认知障碍（MCI）的诊断。这些潜在特征被输入到一个
    SVM 分类器中，准确率为 72.58%。Ortiz 等人[[212](#bib.bib212)] 提出了一个多视图 DBN-RBM 同时接收 MRI 和
    PET 信息。学习到的表示被发送到几个简单的 SVM 分类器，这些分类器通过投票组合成一个更强的高级分类器。
- en: (3) Generative models. The reconstruction of natural image (e.g., fMRI) has
    been attracted lots of attention [[215](#bib.bib215), [88](#bib.bib88), [203](#bib.bib203)].
    Seeliger et al. [[214](#bib.bib214)] proposed a deep convolutional GAN for reconstructing
    visual stimuli from fMRI, which aimed at training a generator to create an image
    similar to the visual stimuli. The generator contains four convolutional layers
    in order to convert the input fMRI to a natural image. Han et al. [[215](#bib.bib215)]
    focused on the generation of synthetic multi-sequence fMRI using GAN. The generated
    image can be used for data augmentation for better diagnostic accuracy or physician
    training to help better understand various diseases. The authors applied the existing
    Deep Convolutional GAN (DCGAN) [[228](#bib.bib228)] and Wasserstein GAN (WGAN)
    [[229](#bib.bib229)] and found that the former works better. Shen et al. [[203](#bib.bib203)]
    presented another image recovery approach by minimizing the distance between the
    real image and the image generated based on real fMRI.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 生成模型。自然图像（例如，fMRI）的重建引起了大量关注 [[215](#bib.bib215), [88](#bib.bib88), [203](#bib.bib203)]。Seeliger
    等人 [[214](#bib.bib214)] 提出了一个深度卷积 GAN，用于从 fMRI 重建视觉刺激，旨在训练一个生成器来创建与视觉刺激相似的图像。该生成器包含四个卷积层，以将输入的
    fMRI 转换为自然图像。Han 等人 [[215](#bib.bib215)] 专注于使用 GAN 生成合成的多序列 fMRI。生成的图像可用于数据增强，以提高诊断准确性或医学培训，帮助更好地理解各种疾病。作者应用了现有的深度卷积
    GAN (DCGAN) [[228](#bib.bib228)] 和 Wasserstein GAN (WGAN) [[229](#bib.bib229)]，发现前者效果更好。Shen
    等人 [[203](#bib.bib203)] 提出了另一种图像恢复方法，通过最小化真实图像与基于真实 fMRI 生成的图像之间的距离来实现。
- en: 4.4 MEG
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 MEG
- en: Garg et al. [[218](#bib.bib218)] worked on the refining of MEG signals by removing
    the artifacts like eye-blinks and cardiac activity. The MEG singles were decomposed
    by ICA first and then classified by a 1-D CNN model. At last, the proposed approach
    achieved the sensitivity of 85% and specificity of 97% over a local dataset. Hasasneh
    et al. [[220](#bib.bib220)] also focused on artifacts detection (cardiac and ocular
    artifacts). The proposed approach uses CNN to capture temporal features and MLP
    to extract spatial information. Shu et al. [[219](#bib.bib219)] employed a sparse
    AE to learn the latent dependencies of MEG signals in the task of single word
    decoding. The results demonstrated that the proposed approach is advantageous
    for some subjects, although it did not produce an overall increase in decoding
    accuracy. Cichy et al. [[204](#bib.bib204)] applied a CNN model to recognize visual
    object based on MEG and fMRI signals.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Garg 等人 [[218](#bib.bib218)] 通过去除眼睛眨动和心脏活动等伪影来改进 MEG 信号。MEG 信号首先通过 ICA 分解，然后由
    1-D CNN 模型分类。最后，该方法在本地数据集上达到了 85% 的灵敏度和 97% 的特异性。Hasasneh 等人 [[220](#bib.bib220)]
    也专注于伪影检测（心脏和眼部伪影）。提出的方法使用 CNN 捕捉时间特征，使用 MLP 提取空间信息。Shu 等人 [[219](#bib.bib219)]
    采用稀疏 AE 来学习 MEG 信号的潜在依赖关系，以进行单词解码。结果表明，尽管该方法并未整体提高解码准确性，但对某些受试者具有优势。Cichy 等人 [[204](#bib.bib204)]
    应用 CNN 模型基于 MEG 和 fMRI 信号识别视觉物体。
- en: 5 Brain Signal-based Applications
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 脑信号基础应用
- en: 'Deep learning models have contributed to various of brain signal applications
    as summarized in Table [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based
    Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers"). The papers focused on signal classification without
    application background are not listed in this table. Therefore, the publication
    amounts in this table are less than in Table [4](#S4.T4 "Table 4 ‣ 4.1.1 Spontaneous
    EEG ‣ 4.1 EEG ‣ 4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers").'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习模型在各种脑信号应用中做出了贡献，如表 [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based
    Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers") 总结。专注于信号分类而没有应用背景的论文未列入此表。因此，本表中的出版数量少于表 [4](#S4.T4
    "Table 4 ‣ 4.1.1 Spontaneous EEG ‣ 4.1 EEG ‣ 4 State-of-The-Art DL Techniques
    for Brain Signals ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers")。'
- en: 5.1 Health Care
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 健康护理
- en: 'In the health care area, the deep learning-based brain signal systems mainly
    works on the detection and diagnosis of mental diseases such as sleep disorders,
    Alzheimer’s Disease, epileptic seizure, and other disorders. In the first place,
    for the sleep disorder detection, most studies are focused on the sleep stage
    detection based on sleep spontaneous EEG. In this situation, the researchers do
    not need to recruit patients with sleep disorder because the sleep EEG signals
    can be easily collected from healthy individuals. In terms of the algorithm, it
    can be observed from Table [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based
    Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent
    Advances and New Frontiers") that the DBN-RBM and CNN are widely adopted for feature
    selection and classification. Ruffini et al. [[111](#bib.bib111)] walk one step
    further by detecting the REM Behavior Disorder (RBD), which may cause neurodegenerative
    diseases such as Parkinson’s disease. They achieved an average accuracy of 85%
    in recognition of the RBD from healthy controls.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '在医疗保健领域，基于深度学习的脑信号系统主要用于检测和诊断精神疾病，如睡眠障碍、阿尔茨海默病、癫痫发作及其他障碍。首先，在睡眠障碍检测方面，大多数研究集中在基于睡眠自发EEG的睡眠阶段检测。在这种情况下，研究人员无需招募睡眠障碍患者，因为可以很容易地从健康个体那里收集睡眠EEG信号。在算法方面，从表[5](#S5.T5
    "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications ‣ A Survey on
    Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers")中可以观察到，DBN-RBM和CNN被广泛采用用于特征选择和分类。Ruffini等人[[111](#bib.bib111)]进一步检测了快速眼动行为障碍（RBD），这可能导致如帕金森病等神经退行性疾病。他们在从健康对照中识别RBD时取得了85%的平均准确率。'
- en: Moreover, fMRI is widely applied in the diagnosis of Alzheimer’s Disease. By
    taking advantage of the high spatial resolution of fMRI, the diagnosis achieved
    the accuracy of above 90% in several studies. Another reason that contributes
    to competitive performance is the binary classification scenario. Apart from that,
    there are several publications diagnose the AD based on spontaneous EEG [[115](#bib.bib115),
    [126](#bib.bib126)].
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，fMRI在阿尔茨海默病的诊断中被广泛应用。通过利用fMRI的高空间分辨率，多项研究在诊断中实现了90%以上的准确率。另一个有助于竞争性能的原因是二分类场景。除此之外，还有几篇出版物基于自发EEG诊断AD[[115](#bib.bib115),
    [126](#bib.bib126)]。
- en: Besides, the diagnosis of epileptic seizure attracted much attention. The seizure
    detection mainly based on spontaneous EEG. The popular deep learning models in
    this scenario contain the independent CNN and RNN, along with hybrid models combined
    RNN and CNN. Some models integrated the deep learning models for feature extraction
    and traditional classifier for detection [[127](#bib.bib127), [125](#bib.bib125)].
    For example, Yuan et al. [[121](#bib.bib121)] applied a D-AE in feature extraction
    followed by SVM for seizure diagnosis. Ullah et al. [[112](#bib.bib112)] adopted
    the voting for post-processing, which proposed several different CNN classifiers
    and predicted the final result by voting.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，癫痫发作的诊断也引起了广泛关注。癫痫发作检测主要基于自发EEG。在这种情况下，流行的深度学习模型包括独立的CNN和RNN，以及结合了RNN和CNN的混合模型。一些模型将深度学习模型用于特征提取，而传统分类器用于检测[[127](#bib.bib127),
    [125](#bib.bib125)]。例如，Yuan等人[[121](#bib.bib121)]应用了D-AE进行特征提取，然后使用SVM进行发作诊断。Ullah等人[[112](#bib.bib112)]采用投票进行后处理，提出了几种不同的CNN分类器，并通过投票预测最终结果。
- en: Furthermore, there are a lot of other healthcare issues can be solved by brain
    signal research. The cardiac artifacts in MEG can be automatically detected by
    deep learning models[[218](#bib.bib218), [220](#bib.bib220)]. Several modified
    CNN structures are proposed to detect brain tumor based on fMRI from the public
    BRATS dataset [[205](#bib.bib205), [206](#bib.bib206)]. Researchers have demonstrated
    the effectiveness of deep learning models in the detection of a wide number of
    mental diseases such as depression [[113](#bib.bib113)], Interictal Epileptic
    Discharge (IED) [[230](#bib.bib230)], schizophrenia [[211](#bib.bib211)], Creutzfeldt-Jakob
    Disease (CJD) [[123](#bib.bib123)], and Mild Cognitive Impairment (MCI) [[209](#bib.bib209)].
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有许多其他的医疗问题可以通过脑信号研究来解决。心脏伪影在MEG中的自动检测可以通过深度学习模型来实现[[218](#bib.bib218), [220](#bib.bib220)]。提出了几种修改过的CNN结构，用于基于来自公开BRATS数据集的fMRI来检测脑肿瘤[[205](#bib.bib205),
    [206](#bib.bib206)]。研究人员已经证明，深度学习模型在检测各种精神疾病方面是有效的，如抑郁症[[113](#bib.bib113)]、间歇性癫痫放电（IED）[[230](#bib.bib230)]、精神分裂症[[211](#bib.bib211)]、克鲁茨费尔特-雅各布病（CJD）[[123](#bib.bib123)]和轻度认知障碍（MCI）[[209](#bib.bib209)]。
- en: 5.2 Smart Environment
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 智能环境
- en: The smart environment is a promising application scenario for brain signals
    in the future. With the development of Internet of Things (IoT), an increasing
    number smart environment can be connected to brain signals. For example, the assisting
    robot can be used in smart home [[65](#bib.bib65), [2](#bib.bib2)], in which the
    robot can be controlled by brain signals of the individuals. Moreover, Behncke
    et al. [[139](#bib.bib139)] and Huve et al. [[199](#bib.bib199)] investigated
    the robot control problem based on the visual stimulated spontaneous EEG and fNIRS
    signals. The brain signal controlled exoskeleton could help the disabilities who
    damaged the motor system in sub-limb in walking and daily activities [[191](#bib.bib191)].
    In the future, the research on brain-controlled appliances may be beneficial to
    the elders or disabilities in smart home and smart hospital.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 智能环境是未来脑信号的一个有前景的应用场景。随着物联网（IoT）的发展，越来越多的智能环境可以连接到脑信号。例如，辅助机器人可以用于智能家居 [[65](#bib.bib65),
    [2](#bib.bib2)]，在其中机器人可以通过个人的脑信号进行控制。此外，Behncke等人 [[139](#bib.bib139)] 和Huve等人
    [[199](#bib.bib199)] 研究了基于视觉刺激自发EEG和fNIRS信号的机器人控制问题。脑信号控制的外骨骼可以帮助那些因下肢受损的残疾人进行行走和日常活动
    [[191](#bib.bib191)]。未来，脑控设备的研究可能对智能家居和智能医院中的老年人或残疾人有所帮助。
- en: 5.3 Communication
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 通信
- en: One of the biggest advantages of brain signals, compared to other human-machine
    interface techniques, is that brain signal enables the patient who lost most motor
    abilities like speaking to communicate with the outer world. The deep learning
    technology improved the efficiency of brain signal based communications. One typical
    diagram which enables individual typing without any motor system is P300 speller,
    which can convert the user’s intent into text [[162](#bib.bib162)]. The powerful
    deep learning models empower the brain signal systems to recognize the P300 segment
    from the non-P300 segment while the former contains the communication information
    of the user [[166](#bib.bib166)]. In a higher level, the representative deep learning
    models can help to detect what character the user is focusing on and print it
    on the screen to chat with others [[166](#bib.bib166), [170](#bib.bib170), [164](#bib.bib164)].
    Additionally, Zhang et al. [[10](#bib.bib10)] proposed a hybrid model that combined
    RNN, CNN, and AE to extract the informative features from MI EEG to recognize
    what letter the user wants to speak.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他人机接口技术相比，脑信号的最大优势之一在于它能够使失去大部分运动能力如说话的患者与外界沟通。深度学习技术提高了基于脑信号的通信效率。一个典型的图示是P300拼写器，它能够将用户的意图转化为文本
    [[162](#bib.bib162)]。强大的深度学习模型使脑信号系统能够从非P300段中识别P300段，而前者包含用户的通信信息 [[166](#bib.bib166)]。在更高层次上，代表性的深度学习模型可以帮助检测用户关注的字符，并在屏幕上打印出来与他人聊天
    [[166](#bib.bib166), [170](#bib.bib170), [164](#bib.bib164)]。此外，张等人 [[10](#bib.bib10)]
    提出了一个混合模型，将RNN、CNN和AE结合起来，从MI EEG中提取信息特征，以识别用户想要说的字母。
- en: '![Refer to caption](img/25a7736082517ef68571f8cff8a01610.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25a7736082517ef68571f8cff8a01610.png)'
- en: (a) Brain signals
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 脑信号
- en: '![Refer to caption](img/955382d97dccae46a0f708c662174439.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/955382d97dccae46a0f708c662174439.png)'
- en: (b) Deep learning models
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 深度学习模型
- en: 'Figure 4: Illustration of the publications proportion for crucial brain signals
    and deep learning models.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：关键脑信号和深度学习模型的出版比例示意图。
- en: 'Table 5: Summary of deep learning-based brain signal applications. The ‘local’
    dataset refers to private or not available dataset. The public datasets (along
    with download links) will be introduced in Section [5.9](#S5.SS9 "5.9 Benchmark
    Datasets ‣ 5 Brain Signal-based Applications ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers"). In the signals,
    S-EEG, MD EEG, and E-EEG separately denote sleep EEG, mental disease EEG, and
    emotional EEG. The single ‘EEG’ refers to the other subcategory of spontaneous
    EEG. In the models, RF and LR denote to random forest and logistic regression
    algorithms, respectively. In the performance column, ‘N/A’, ‘sen’, ‘spe’, ’aro’,
    ‘val’, ‘dom’, and ‘like’ denote not-found, sensitivity, specificity, arousal,
    valence, dominance, and liking, respectively. For each application scenario, the
    literature are sorted out by signal types and deep learning models.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 基于深度学习的脑信号应用总结。‘local’ 数据集指的是私有或不可用的数据集。公共数据集（及其下载链接）将在第[5.9](#S5.SS9
    "5.9 Benchmark Datasets ‣ 5 Brain Signal-based Applications ‣ A Survey on Deep
    Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers")节中介绍。在信号中，S-EEG、MD
    EEG 和 E-EEG 分别表示睡眠 EEG、精神疾病 EEG 和情感 EEG。单一的‘EEG’指的是自发 EEG 的其他子类别。在模型中，RF 和 LR
    分别指随机森林和逻辑回归算法。在性能列中，‘N/A’、‘sen’、‘spe’、‘aro’、‘val’、‘dom’ 和 ‘like’ 分别表示未找到、灵敏度、特异性、唤醒、情感、支配和喜好。对于每个应用场景，文献按信号类型和深度学习模型进行排序。'
- en: '| Brain Signal Applications | Reference | Signals |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 脑信号应用 | 参考 | 信号 |'
- en: '&#124; Deep Learning &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度学习 &#124;'
- en: '&#124; Models &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '| Dataset | Performance |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 性能 |'
- en: '| Health Care | Sleep Quality Evaluation | Shahin et al. [[69](#bib.bib69)]
    | S-EEG | MLP |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 健康护理 | 睡眠质量评估 | Shahin 等 [[69](#bib.bib69)] | S-EEG | MLP |'
- en: '&#124; University &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大学 &#124;'
- en: '&#124; Hospital &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医院 &#124;'
- en: '&#124; in Berlin &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 柏林 &#124;'
- en: '| 0.9 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 |'
- en: '| Biswai et al. [[52](#bib.bib52)] | S-EEG | RNN | Local | 0.8576 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Biswai 等 [[52](#bib.bib52)] | S-EEG | RNN | Local | 0.8576 |'
- en: '| Ruffini et al. [[111](#bib.bib111)] | S-EEG | RNN | Local | 0.85 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Ruffini 等 [[111](#bib.bib111)] | S-EEG | RNN | Local | 0.85 |'
- en: '| Vilamala et al. [[51](#bib.bib51)] | S-EEG | CNN | Sleep-EDF | 0.86 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Vilamala 等 [[51](#bib.bib51)] | S-EEG | CNN | Sleep-EDF | 0.86 |'
- en: '| Tsinalis et al. [[25](#bib.bib25)] | S-EEG | CNN | Sleep-EDF | 0.82 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Tsinalis 等 [[25](#bib.bib25)] | S-EEG | CNN | Sleep-EDF | 0.82 |'
- en: '| Sors et al. [[50](#bib.bib50)] | S-EEG | CNN | SHHS | 0.87 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| Sors 等 [[50](#bib.bib50)] | S-EEG | CNN | SHHS | 0.87 |'
- en: '| Chambon et al. [[48](#bib.bib48)] | S-EEG | Multi-view CNN | MASS session
    3 | N/A |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Chambon 等 [[48](#bib.bib48)] | S-EEG | 多视角 CNN | MASS session 3 | N/A |'
- en: '| Manzano et al. [[55](#bib.bib55)] | S-EEG | CNN + MLP | Sleep-EDF | 0.732
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Manzano 等 [[55](#bib.bib55)] | S-EEG | CNN + MLP | Sleep-EDF | 0.732 |'
- en: '| Fraiwan et al. [[56](#bib.bib56)] | S-EEG | DBN-AE + MLP | Local | 0.804
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Fraiwan 等 [[56](#bib.bib56)] | S-EEG | DBN-AE + MLP | Local | 0.804 |'
- en: '| Tan et al. [[54](#bib.bib54)] | S-EEG | DBN-RBM | Local | 0.9278 (F1) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Tan 等 [[54](#bib.bib54)] | S-EEG | DBN-RBM | Local | 0.9278 (F1) |'
- en: '| Zhang et al. [[49](#bib.bib49)] | S-EEG | DBN + voting | UCD | 0.9131 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等 [[49](#bib.bib49)] | S-EEG | DBN + 投票 | UCD | 0.9131 |'
- en: '| Fernandez et al. [[70](#bib.bib70)] | S-EEG | CNN | SHHS | 0.9 (F1) |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Fernandez 等 [[70](#bib.bib70)] | S-EEG | CNN | SHHS | 0.9 (F1) |'
- en: '| Supratak et al. [[57](#bib.bib57)] | S-EEG | CNN + LSTM |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Supratak 等 [[57](#bib.bib57)] | S-EEG | CNN + LSTM |'
- en: '&#124; MASS/ &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MASS/ &#124;'
- en: '&#124; Sleep-EDF &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Sleep-EDF &#124;'
- en: '| 0.862/0.82 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 0.862/0.82 |'
- en: '| AD Detection | Morabito et al. [[115](#bib.bib115)] | MD EEG | CNN | Local
    | 0.82 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| AD 检测 | Morabito 等 [[115](#bib.bib115)] | MD EEG | CNN | Local | 0.82 |'
- en: '| Zhao et al. [[126](#bib.bib126)] | MD EEG | DBN-RBM | Local | 0.92 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| Zhao 等 [[126](#bib.bib126)] | MD EEG | DBN-RBM | Local | 0.92 |'
- en: '| Suk et al. [[210](#bib.bib210)] | fMRI |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Suk 等 [[210](#bib.bib210)] | fMRI |'
- en: '&#124; DBN-AE; &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-AE; &#124;'
- en: '&#124; DBN-RBM &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM &#124;'
- en: '| ADNI |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| ADNI |'
- en: '&#124; 0.979; &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.979; &#124;'
- en: '&#124; 0.954 &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.954 &#124;'
- en: '|'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sarraf et al. [[207](#bib.bib207)] | fMRI | CNN | ADNI | 0.9685 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| Sarraf 等 [[207](#bib.bib207)] | fMRI | CNN | ADNI | 0.9685 |'
- en: '| Li et al. [[208](#bib.bib208)] | fMRI | CNN + LR | ADNI | 0.9192 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| Li 等 [[208](#bib.bib208)] | fMRI | CNN + LR | ADNI | 0.9192 |'
- en: '| Hu et al. [[217](#bib.bib217)] | fMRI | D-AE + MLP | ADNI | 0.875 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Hu 等 [[217](#bib.bib217)] | fMRI | D-AE + MLP | ADNI | 0.875 |'
- en: '| Ortiz et al. [[212](#bib.bib212)] | fMRI, PET |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| Ortiz 等 [[212](#bib.bib212)] | fMRI, PET |'
- en: '&#124; DBN-RBM &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM &#124;'
- en: '&#124; + SVM &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + SVM &#124;'
- en: '| ADNI | 0.9 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| ADNI | 0.9 |'
- en: '| Seizure Detection | Hosseini et al. [[120](#bib.bib120)] | EEG | CNN | Local
    | 0.96 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 癫痫检测 | Hosseini 等 [[120](#bib.bib120)] | EEG | CNN | Local | 0.96 |'
- en: '| Yuan et al. [[109](#bib.bib109)] | MD EEG | Attention-MLP | CHB-MIT | 0.9661
    |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| Yuan 等 [[109](#bib.bib109)] | MD EEG | 注意力-MLP | CHB-MIT | 0.9661 |'
- en: '| Tsiouris et al. [[53](#bib.bib53)] | MD EEG | LSTM | CHB-MIT | $>$0.99 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| Tsiouris 等 [[53](#bib.bib53)] | MD EEG | LSTM | CHB-MIT | $>$0.99 |'
- en: '| Talathi et al. [[110](#bib.bib110)] | MD EEG | GRU | BUD | 0.996 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Talathi 等 [[110](#bib.bib110)] | MD EEG | GRU | BUD | 0.996 |'
- en: '| Acharya et al. [[114](#bib.bib114)] | MD EEG | CNN | UBD | 0.8867 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| Acharya 等 [[114](#bib.bib114)] | MD EEG | CNN | UBD | 0.8867 |'
- en: '| Schirmeister et al. [[116](#bib.bib116)] | MD EEG | CNN | TUH | 0.854 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| Schirmeister 等 [[116](#bib.bib116)] | MD EEG | CNN | TUH | 0.854 |'
- en: '| Hosseini et al. [[117](#bib.bib117)] | MD EEG | CNN | Local | N/A |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| Hosseini 等 [[117](#bib.bib117)] | MD EEG | CNN | 本地 | 不适用 |'
- en: '| Johansen et al. [[118](#bib.bib118)] | MD EEG | CNN | Local | 0.947 (AUC)
    |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| Johansen 等 [[118](#bib.bib118)] | MD EEG | CNN | 本地 | 0.947 (AUC) |'
- en: '| Ansari et al. [[119](#bib.bib119)] | MD EEG | CNN + RF | Local | 0.77 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| Ansari 等 [[119](#bib.bib119)] | MD EEG | CNN + RF | 本地 | 0.77 |'
- en: '| Ullah et al. [[112](#bib.bib112)] | MD EEG | CNN + voting | UBD | 0.954 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Ullah 等 [[112](#bib.bib112)] | MD EEG | CNN + 投票 | UBD | 0.954 |'
- en: '| Wen et al. [[124](#bib.bib124)] | MD EEG | AE | Local | 0.92 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| Wen 等 [[124](#bib.bib124)] | MD EEG | AE | 本地 | 0.92 |'
- en: '| Lin et al.[[122](#bib.bib122)] | MD EEG | D-AE | UBD | 0.96 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| Lin 等[[122](#bib.bib122)] | MD EEG | D-AE | UBD | 0.96 |'
- en: '| Yuan et al. [[121](#bib.bib121)] | MD EEG | D-AE + SVM | CHB-MIT | 0.95 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Yuan 等 [[121](#bib.bib121)] | MD EEG | D-AE + SVM | CHB-MIT | 0.95 |'
- en: '| Page et al. [[125](#bib.bib125)] | MD EEG | DBN-AE + LR | N/A | 0.8 $\sim$
    0.9 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| Page 等 [[125](#bib.bib125)] | MD EEG | DBN-AE + LR | 不适用 | 0.8 $\sim$ 0.9
    |'
- en: '| Turner et al. [[127](#bib.bib127)] | MD EEG |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| Turner 等 [[127](#bib.bib127)] | MD EEG |'
- en: '&#124; DBN-RBM &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM &#124;'
- en: '&#124; + LR &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + LR &#124;'
- en: '| Local | N/A |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 不适用 |'
- en: '| Hosseini et al. [[129](#bib.bib129)] | MD EEG | D-AE + MLP | Local | 0.94
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| Hosseini 等 [[129](#bib.bib129)] | MD EEG | D-AE + MLP | 本地 | 0.94 |'
- en: '| Golmohammadi et al. [[130](#bib.bib130)] | MD EEG | RNN+CNN | TUH |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| Golmohammadi 等 [[130](#bib.bib130)] | MD EEG | RNN+CNN | TUH |'
- en: '&#124; Sen: 0.3083; &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 灵敏度: 0.3083; &#124;'
- en: '&#124; Spe: 0.9686 &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度: 0.9686 &#124;'
- en: '|'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | Shah et al. [[128](#bib.bib128)] | MD EEG | CNN+ LSTM | TUH |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | Shah 等 [[128](#bib.bib128)] | MD EEG | CNN+ LSTM | TUH |'
- en: '&#124; Sen: 0.39; &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 灵敏度: 0.39; &#124;'
- en: '&#124; Spe: 0.9037 &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度: 0.9037 &#124;'
- en: '|'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers"). Summary of deep learning-based brain signal applications
    (Continued). IEF and CJD refer to Interictal Epileptic Discharge and Creutzfeldt-Jakob
    Disease, respectively.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#S5.T5 "表 5 ‣ 5.3 通信 ‣ 5 脑信号基础应用 ‣ 基于深度学习的非侵入性脑信号调查：近期进展与新前沿")。深度学习脑信号应用的总结（续）。IEF
    和 CJD 分别指间歇性癫痫放电和克雅二氏病。
- en: '| Brain Signal Applications | Reference | Signals |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 脑信号应用 | 参考 | 信号 |'
- en: '&#124; Deep Learning &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度学习 &#124;'
- en: '&#124; Models &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '| Dataset | Performance |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 性能 |'
- en: '| Health Care | Others: |  |  |  |  |  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 健康护理 | 其他: |  |  |  |  |  |'
- en: '| IED | Antoniades et al. [[231](#bib.bib231)] | EEG | AE + CNN | Local | 0.68
    |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| IED | Antoniades 等 [[231](#bib.bib231)] | EEG | AE + CNN | 本地 | 0.68 |'
- en: '| CJD | Morabito et al. [[123](#bib.bib123)] | MD EEG | D-AE | Local | 0.81
    $\sim$ 0.83 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| CJD | Morabito 等 [[123](#bib.bib123)] | MD EEG | D-AE | 本地 | 0.81 $\sim$
    0.83 |'
- en: '| Depression | Acharya et al. [[113](#bib.bib113)] | MD EEG | CNN | Local |
    0.935 $\sim$ 0.9596 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 抑郁症 | Acharya 等 [[113](#bib.bib113)] | MD EEG | CNN | 本地 | 0.935 $\sim$ 0.9596
    |'
- en: '| Al et al. [[131](#bib.bib131)] | MD EEG |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| Al 等 [[131](#bib.bib131)] | MD EEG |'
- en: '&#124; DBN-RBM &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM &#124;'
- en: '&#124; + MLP &#124;'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + MLP &#124;'
- en: '| Local | 0.695 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 0.695 |'
- en: '| Brain Tumor | Morenolopez et al. [[227](#bib.bib227)] | fMRI | CNN | BRATS
    | 0.88 (F1) |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 脑肿瘤 | Morenolopez 等 [[227](#bib.bib227)] | fMRI | CNN | BRATS | 0.88 (F1)
    |'
- en: '| Shreyas et al. [[206](#bib.bib206)] | fMRI | CNN | BRATS | 0.83 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| Shreyas 等 [[206](#bib.bib206)] | fMRI | CNN | BRATS | 0.83 |'
- en: '| Havaei et al. [[205](#bib.bib205)] | fMRI | Muli-scale CNN | BRATS | 0.88
    (F1) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| Havaei 等 [[205](#bib.bib205)] | fMRI | 多尺度 CNN | BRATS | 0.88 (F1) |'
- en: '| Schizophrenia | Plils et al. [[211](#bib.bib211)] | fMRI | DBN-RBM | Combined
    | 0.9 (F1) |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 精神分裂症 | Plils 等 [[211](#bib.bib211)] | fMRI | DBN-RBM | 组合 | 0.9 (F1) |'
- en: '| Chu et al. [[149](#bib.bib149)] |  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| Chu 等 [[149](#bib.bib149)] |  |'
- en: '&#124; CNN + RF &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN + RF &#124;'
- en: '&#124; + Voting &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 投票 &#124;'
- en: '| Local | 0.816, 0.967, 0.992 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 0.816, 0.967, 0.992 |'
- en: '|'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mild Cognitive &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轻度认知 &#124;'
- en: '&#124; Impairment (MCI) &#124;'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 损伤 (MCI) &#124;'
- en: '| Suk et al. [[209](#bib.bib209)] | fMRI | AE + SVM | ADNI2 | 0.7258 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| Suk 等 [[209](#bib.bib209)] | fMRI | AE + SVM | ADNI2 | 0.7258 |'
- en: '| Cardiac Detection | Garg [[218](#bib.bib218)] | MEG | CNN | Local |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 心脏检测 | Garg [[218](#bib.bib218)] | MEG | CNN | 本地 |'
- en: '&#124; Sen: 0.85, &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 灵敏度: 0.85, &#124;'
- en: '&#124; Spe: 0.97 &#124;'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度: 0.97 &#124;'
- en: '|'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Hasasneh et al. [[220](#bib.bib220)] | MEG | CNN + MLP | Local | 0.944 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| Hasasneh 等 [[220](#bib.bib220)] | MEG | CNN + MLP | 本地 | 0.944 |'
- en: '| Smart Environment | Robot Control | Behncke et al. [[139](#bib.bib139)] |
    EEG | CNN | Local | 0.75 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 智能环境 | 机器人控制 | 贝恩克等人 [[139](#bib.bib139)] | EEG | CNN | 本地 | 0.75 |'
- en: '|'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Smart &#124;'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 智能 &#124;'
- en: '&#124; Home &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 家庭 &#124;'
- en: '| Zhang et al. [[65](#bib.bib65)] | MI EEG | RNN | EEGMMI | 0.9553 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 张等人 [[65](#bib.bib65)] | MI EEG | RNN | EEGMMI | 0.9553 |'
- en: '|'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Exoskeleton &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外骨骼 &#124;'
- en: '&#124; Control &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 控制 &#124;'
- en: '| Kwak et al. [[191](#bib.bib191)] | SSVEP | CNN | Local | 0.9403 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 郭等人 [[191](#bib.bib191)] | SSVEP | CNN | 本地 | 0.9403 |'
- en: '|  | Huve et al. [[199](#bib.bib199)] | fNIRS | MLP | Local | 0.82 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  | 胡维等人 [[199](#bib.bib199)] | fNIRS | MLP | 本地 | 0.82 |'
- en: '| Communication | Zhang et al. [[10](#bib.bib10)] | MI EEG |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 通信 | 张等人 [[10](#bib.bib10)] | MI EEG |'
- en: '&#124; LSTM+CNN &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LSTM+CNN &#124;'
- en: '&#124; +AE &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; +AE &#124;'
- en: '| Local | 0.9452 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 0.9452 |'
- en: '| Kawasaki et al. [[162](#bib.bib162)] | VEP | MLP | Local | 0.908 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 川崎等人 [[162](#bib.bib162)] | VEP | MLP | 本地 | 0.908 |'
- en: '| Cecotti et al. [[166](#bib.bib166)] | VEP | CNN |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 切科蒂等人 [[166](#bib.bib166)] | VEP | CNN |'
- en: '&#124; The third BCI &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 第三代BCI &#124;'
- en: '&#124; competition, &#124;'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 竞赛, &#124;'
- en: '&#124; Dataset II &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集 II &#124;'
- en: '| 0.945 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 0.945 |'
- en: '| Liu et al. [[164](#bib.bib164)] | VEP | CNN |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 刘等人 [[164](#bib.bib164)] | VEP | CNN |'
- en: '&#124; The third BCI &#124;'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 第三代BCI &#124;'
- en: '&#124; competition, &#124;'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 竞赛, &#124;'
- en: '&#124; Dataset II &#124;'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集 II &#124;'
- en: '| 0.92 $\sim$ 0.96 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 0.92 $\sim$ 0.96 |'
- en: '| Cecotti et al. [[166](#bib.bib166)] | VEP | CNN + Voting |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 切科蒂等人 [[166](#bib.bib166)] | VEP | CNN + 投票 |'
- en: '&#124; The third BCI &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 第三代BCI &#124;'
- en: '&#124; competition, &#124;'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 竞赛, &#124;'
- en: '&#124; Dataset II &#124;'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集 II &#124;'
- en: '| 0.955 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 0.955 |'
- en: '| Maddula et al. [[170](#bib.bib170)] | VEP | RCNN | Local | 0.65$\sim$0.76
    |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 马杜拉等人 [[170](#bib.bib170)] | VEP | RCNN | 本地 | 0.65$\sim$0.76 |'
- en: '| Security | Identification | Zhang et al. [[6](#bib.bib6)] | MI-EEG |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 安全 | 识别 | 张等人 [[6](#bib.bib6)] | MI-EEG |'
- en: '&#124; Attention-based &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于注意力的 &#124;'
- en: '&#124; RNN &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RNN &#124;'
- en: '| EEGMMI + local | 0.9882 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| EEGMMI + 本地 | 0.9882 |'
- en: '| Koike et al. [[161](#bib.bib161)] | VEP | MLP | Local | 0.976 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 小池等人 [[161](#bib.bib161)] | VEP | MLP | 本地 | 0.976 |'
- en: '| Mao et al. [[180](#bib.bib180)] | RSVP | CNN | Local | 0.97 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 毛等人 [[180](#bib.bib180)] | RSVP | CNN | 本地 | 0.97 |'
- en: '| Authentication | Zhang et al. [[61](#bib.bib61)] | MI EEG | Hybrid | EEGMMI
    + local | 0.984 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 身份认证 | 张等人 [[61](#bib.bib61)] | MI EEG | 混合 | EEGMMI + 本地 | 0.984 |'
- en: '| Affective Computing | Frydenlund et al. [[87](#bib.bib87)] | E-EEG | MLP
    | DEAP | N/A |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 情感计算 | 弗里登伦等人 [[87](#bib.bib87)] | E-EEG | MLP | DEAP | N/A |'
- en: '| Zhang et al. [[88](#bib.bib88)] | E-EEG | RNN | SEED | 0.895 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 张等人 [[88](#bib.bib88)] | E-EEG | RNN | SEED | 0.895 |'
- en: '| Li et al. [[201](#bib.bib201)] | E-EEG | CNN | SEED | 0.882 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 李等人 [[201](#bib.bib201)] | E-EEG | CNN | SEED | 0.882 |'
- en: '| Liu et al. [[90](#bib.bib90)] | E-EEG | CNN | Local | 0.82 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 刘等人 [[90](#bib.bib90)] | E-EEG | CNN | 本地 | 0.82 |'
- en: '| Li et al. [[89](#bib.bib89)] | E-EEG |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 李等人 [[89](#bib.bib89)] | E-EEG |'
- en: '&#124; Hierarchical &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 层次结构 &#124;'
- en: '&#124; CNN &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN &#124;'
- en: '| SEED | 0.882 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| SEED | 0.882 |'
- en: '| Chai et al. [[94](#bib.bib94)] | E-EEG | AE | SEED | 0.818 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| Chai等人 [[94](#bib.bib94)] | E-EEG | AE | SEED | 0.818 |'
- en: '| Xu et al. [[99](#bib.bib99)] | E-EEG |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| 徐等人 [[99](#bib.bib99)] | E-EEG |'
- en: '&#124; DBN-AE, &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-AE, &#124;'
- en: '&#124; DBN-RBM &#124;'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM &#124;'
- en: '| DEAP | $>$0.86 (F1) |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| DEAP | $>$0.86 (F1) |'
- en: '| Jia et al. [[98](#bib.bib98)] | E-EEG | DBN-RBM | DEAP |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 贾等人 [[98](#bib.bib98)] | E-EEG | DBN-RBM | DEAP |'
- en: '&#124; 0.8 $\sim$ &#124;'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.8 $\sim$ &#124;'
- en: '&#124; 0.85 (AUC) &#124;'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.85 (AUC) &#124;'
- en: '|'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Li et al. [[100](#bib.bib100)] | E-EEG | DBN-RBM | DEAP |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| 李等人 [[100](#bib.bib100)] | E-EEG | DBN-RBM | DEAP |'
- en: '&#124; Aro:0.642, &#124;'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Aro:0.642, &#124;'
- en: '&#124; Val:0.584, &#124;'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Val:0.584, &#124;'
- en: '&#124; Dom 0.658 &#124;'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Dom 0.658 &#124;'
- en: '|'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Xu et al. [[101](#bib.bib101)] | E-EEG | DBN-RBM | DEAP |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 徐等人 [[101](#bib.bib101)] | E-EEG | DBN-RBM | DEAP |'
- en: '&#124; Aro:0.6984, &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Aro:0.6984, &#124;'
- en: '&#124; Val:0.6688, &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Val:0.6688, &#124;'
- en: '&#124; Lik: 0.7539 &#124;'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Lik: 0.7539 &#124;'
- en: '|'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table [5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers"). Summary of deep learning-based brain signal applications
    (Continued).'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S5.T5 "Table 5 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers")。深度学习基础脑信号应用的总结（续）。 '
- en: '| Brain Signal Applications | Reference | Signals |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 脑信号应用 | 参考 | 信号 |'
- en: '&#124; Deep Learning &#124;'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度学习 &#124;'
- en: '&#124; Models &#124;'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '| Dataset | Performance |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 性能 |'
- en: '| Affective Computing | Zheng et al. [[102](#bib.bib102)] | E-EEG |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 情感计算 | 郑等人 [[102](#bib.bib102)] | E-EEG |'
- en: '&#124; DBN-RBM &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM &#124;'
- en: '&#124; + HMM &#124;'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + HMM &#124;'
- en: '| Local | 0.8762 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 0.8762 |'
- en: '| Zhang et al. [[96](#bib.bib96), [97](#bib.bib97)] | E-EEG |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 张等人 [[96](#bib.bib96), [97](#bib.bib97)] | E-EEG |'
- en: '&#124; DBN-RBM &#124;'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM &#124;'
- en: '&#124; + MLP &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + MLP &#124;'
- en: '| SEED | 0.8608 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| SEED | 0.8608 |'
- en: '| Gao et al. [[106](#bib.bib106)] | E-EEG |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| Gao 等人 [[106](#bib.bib106)] | E-EEG |'
- en: '&#124; DBN-RBM &#124;'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM &#124;'
- en: '&#124; + MLP &#124;'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + MLP &#124;'
- en: '| Local | 0.684 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 0.684 |'
- en: '| Yin et al. [[107](#bib.bib107)] | E-EEG |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| Yin 等人 [[107](#bib.bib107)] | E-EEG |'
- en: '&#124; Multi-view D-AE &#124;'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多视角 D-AE &#124;'
- en: '&#124; + MLP &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + MLP &#124;'
- en: '| DEAP |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| DEAP |'
- en: '&#124; Aro: 0.7719; &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Aro: 0.7719; &#124;'
- en: '&#124; Val: 0.7617 &#124;'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Val: 0.7617 &#124;'
- en: '|'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Mioranda et al. [[104](#bib.bib104)] | E-EEG | RNN + CNN | AMIGOS | ¡0.7
    |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| Mioranda 等人 [[104](#bib.bib104)] | E-EEG | RNN + CNN | AMIGOS | ¡0.7 |'
- en: '| Alhagry et al. [[108](#bib.bib108)] | E-EEG | LSTM + MLP | DEAP |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| Alhagry 等人 [[108](#bib.bib108)] | E-EEG | LSTM + MLP | DEAP |'
- en: '&#124; Aro:0.8565, &#124;'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Aro:0.8565, &#124;'
- en: '&#124; Val:0.8545, &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Val:0.8545, &#124;'
- en: '&#124; Lik: 0.8799 &#124;'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Lik: 0.8799 &#124;'
- en: '|'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Liu et al. [[95](#bib.bib95)] |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 [[95](#bib.bib95)] |'
- en: '&#124; EEG &#124;'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EEG &#124;'
- en: '| AE |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| AE |'
- en: '&#124; SEED, &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SEED, &#124;'
- en: '&#124; DEAP &#124;'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DEAP &#124;'
- en: '| 0.9101, 0.8325 |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 0.9101, 0.8325 |'
- en: '| Kawde et al. [[105](#bib.bib105)] | EEG | DBN-RBM | DEAP |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| Kawde 等人 [[105](#bib.bib105)] | EEG | DBN-RBM | DEAP |'
- en: '&#124; Aro: 0.7033; &#124;'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Aro: 0.7033; &#124;'
- en: '&#124; Val: 0.7828; &#124;'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Val: 0.7828; &#124;'
- en: '&#124; Dom: 0.7016 &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Dom: 0.7016 &#124;'
- en: '|'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Drive Fatigue Detection | Hung et al. [[140](#bib.bib140), [140](#bib.bib140)]
    | EEG | CNN | Local | 0.572 (RMSE) |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 驾驶疲劳检测 | Hung 等人 [[140](#bib.bib140), [140](#bib.bib140)] | EEG | CNN | 本地
    | 0.572 (RMSE) |'
- en: '| Hung et al. [[140](#bib.bib140)] | EEG | CNN | Local |  |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| Hung 等人 [[140](#bib.bib140)] | EEG | CNN | 本地 |  |'
- en: '| Almogbel et al. [[145](#bib.bib145)] | EEG | CNN | Local | 0.9531 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| Almogbel 等人 [[145](#bib.bib145)] | EEG | CNN | 本地 | 0.9531 |'
- en: '| Hajinoroozi et al. [[147](#bib.bib147), [147](#bib.bib147)] | EEG | CNN |
    Local | 0.8294 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| Hajinoroozi 等人 [[147](#bib.bib147), [147](#bib.bib147)] | EEG | CNN | 本地
    | 0.8294 |'
- en: '| Hajinoroozi et al. [[153](#bib.bib153)] | EEG | DBN-RBM | Local | 0.85 |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| Hajinoroozi 等人 [[153](#bib.bib153)] | EEG | DBN-RBM | 本地 | 0.85 |'
- en: '| San et al. [[154](#bib.bib154)] | EEG | DBN-RBM + SVM | Local | 0.7392 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| San 等人 [[154](#bib.bib154)] | EEG | DBN-RBM + SVM | 本地 | 0.7392 |'
- en: '| Chai et al. [[158](#bib.bib158)] | EEG | DBN + MLP | Local | 0.931 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| Chai 等人 [[158](#bib.bib158)] | EEG | DBN + MLP | 本地 | 0.931 |'
- en: '| Du et al. [[151](#bib.bib151)] |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| Du 等人 [[151](#bib.bib151)] |'
- en: '&#124; EEG &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EEG &#124;'
- en: '| D-AE + SVM | Local | 0.094 (RMSE) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| D-AE + SVM | 本地 | 0.094 (RMSE) |'
- en: '| Hachem et al. [[189](#bib.bib189)] | SSVEP | MLP | Local | 0.75 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| Hachem 等人 [[189](#bib.bib189)] | SSVEP | MLP | 本地 | 0.75 |'
- en: '| Mental Load Measurement | Yin et al. [[150](#bib.bib150)] | EEG | D-AE |
    Local | 0.9584 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 心理负荷测量 | Yin 等人 [[150](#bib.bib150)] | EEG | D-AE | 本地 | 0.9584 |'
- en: '| Bashivan et al. [[159](#bib.bib159)] | EEG | DBN-RBM | Local | 0.92 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| Bashivan 等人 [[159](#bib.bib159)] | EEG | DBN-RBM | 本地 | 0.92 |'
- en: '| Li et al. [[155](#bib.bib155)] | EEG | DBN-RBM | Local | 0.9886 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 [[155](#bib.bib155)] | EEG | DBN-RBM | 本地 | 0.9886 |'
- en: '| Bashivan et al. [[171](#bib.bib171)] | EEG | R-CNN | Local | 0.9111 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| Bashivan 等人 [[171](#bib.bib171)] | EEG | R-CNN | 本地 | 0.9111 |'
- en: '| Bashivan et al. [[172](#bib.bib172)] | EEG | DBN + MLP | Local | N/A |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| Bashivan 等人 [[172](#bib.bib172)] | EEG | DBN + MLP | 本地 | 不适用 |'
- en: '| Naseer et al. [[38](#bib.bib38)] | fNIRS | MLP | Local | 0.963 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| Naseer 等人 [[38](#bib.bib38)] | fNIRS | MLP | 本地 | 0.963 |'
- en: '| Hennrich et al. [[200](#bib.bib200)] | fNIRS | MLP | Local | 0.641 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| Hennrich 等人 [[200](#bib.bib200)] | fNIRS | MLP | 本地 | 0.641 |'
- en: '| Other Appli- -cations | School Bullying | Baltatzis et al. [[141](#bib.bib141)]
    | EEG | CNN | Local | 0.937 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| 其他应用 | 校园欺凌 | Baltatzis 等人 [[141](#bib.bib141)] | EEG | CNN | 本地 | 0.937
    |'
- en: '| Music Detection | Stober et al. [[142](#bib.bib142)] | EEG | CNN | Local
    | 0.776 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| 音乐检测 | Stober 等人 [[142](#bib.bib142)] | EEG | CNN | 本地 | 0.776 |'
- en: '| Stober et al. [[157](#bib.bib157)] | EEG | AE + CNN | Open MIIR | 0.27 for
    12-class |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| Stober 等人 [[157](#bib.bib157)] | EEG | AE + CNN | 开放 MIIR | 0.27（12类） |'
- en: '| Stober et al. [[188](#bib.bib188)] | EEG | CNN | Local | 0.244 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| Stober 等人 [[188](#bib.bib188)] | EEG | CNN | 本地 | 0.244 |'
- en: '| Sternin et al. [[148](#bib.bib148)] |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| Sternin 等人 [[148](#bib.bib148)] |'
- en: '&#124; EEG &#124;'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EEG &#124;'
- en: '| CNN | Local | 0.75 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 本地 | 0.75 |'
- en: '|'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number &#124;'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数量 &#124;'
- en: '&#124; Choosing &#124;'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 选择 &#124;'
- en: '| Waytowich et al. [[192](#bib.bib192)] | SSVEP | CNN | Local | 0.8 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| Waytowich 等人 [[192](#bib.bib192)] | SSVEP | CNN | 本地 | 0.8 |'
- en: '| Visual Object Recognition | Cichy et al. [[204](#bib.bib204)] |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 视觉对象识别 | Cichy 等人 [[204](#bib.bib204)] |'
- en: '&#124; fMRI, MEG &#124;'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; fMRI, MEG &#124;'
- en: '| CNN | N/A | N/A |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 不适用 | 不适用 |'
- en: '| Manor et al. [[176](#bib.bib176)] | RSVP | CNN | Local | 0.75 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| Manor 等人 [[176](#bib.bib176)] | RSVP | CNN | 本地 | 0.75 |'
- en: '| Cecotti et al. [[177](#bib.bib177)] | RSVP | CNN | Local | 0.897 (AUC) |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| Cecotti 等人 [[177](#bib.bib177)] | RSVP | CNN | 本地 | 0.897 (AUC) |'
- en: '| Hajinoroozi et al. [[179](#bib.bib179)] | RSVP | CNN | Local | 0.7242 (AUC)
    |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| Hajinoroozi 等人 [[179](#bib.bib179)] | RSVP | CNN | 本地 | 0.7242 (AUC) |'
- en: '| Shamwell et al. [[185](#bib.bib185)] | RSVP | CNN | Local | 0.7252 (AUC)
    |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| Shamwell 等人 [[185](#bib.bib185)] | RSVP | CNN | 本地 | 0.7252 (AUC) |'
- en: '| Perez et al. [[197](#bib.bib197)] | SSVEP | AE | Local | 0.9778 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| Perez 等人 [[197](#bib.bib197)] | SSVEP | AE | Local | 0.9778 |'
- en: '|'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Guilty &#124;'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 罪犯 &#124;'
- en: '&#124; Knowledge &#124;'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识 &#124;'
- en: '&#124; Test &#124;'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试 &#124;'
- en: '| Kulasingham et al. [[195](#bib.bib195)] | SSVEP |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| Kulasingham 等人 [[195](#bib.bib195)] | SSVEP |'
- en: '&#124; DBN-RBM; &#124;'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-RBM; &#124;'
- en: '&#124; DBN-AE &#124;'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBN-AE &#124;'
- en: '| Local |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| Local |'
- en: '&#124; 0.869; &#124;'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.869; &#124;'
- en: '&#124; 0.8601 &#124;'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.8601 &#124;'
- en: '|'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Concealed &#124;'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 隐藏 &#124;'
- en: '&#124; Information &#124;'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息 &#124;'
- en: '&#124; Test &#124;'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试 &#124;'
- en: '| Liu et al. [[168](#bib.bib168)] | EEG | DBN-RBM | Local | 0.973 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 [[168](#bib.bib168)] | EEG | DBN-RBM | Local | 0.973 |'
- en: '| Flanker Task | Volker et al. [[143](#bib.bib143)] | EEG | CNN | Local | 0.841
    |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| Flanker 任务 | Volker 等人 [[143](#bib.bib143)] | EEG | CNN | Local | 0.841 |'
- en: '| Eye State | Narejo et al. [[152](#bib.bib152)] | EEG | DBN-RBM | UCI | 0.989
    |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| 眼睛状态 | Narejo 等人 [[152](#bib.bib152)] | EEG | DBN-RBM | UCI | 0.989 |'
- en: '| Reddy et al. [[136](#bib.bib136)] | EEG | MLP | Local | 0.975 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| Reddy 等人 [[136](#bib.bib136)] | EEG | MLP | Local | 0.975 |'
- en: '| User Preference | Teo et al. [[135](#bib.bib135)] | EEG | MLP | Local | 0.6399
    |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| 用户偏好 | Teo 等人 [[135](#bib.bib135)] | EEG | MLP | Local | 0.6399 |'
- en: '|'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Emergency &#124;'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 紧急 &#124;'
- en: '&#124; Braking &#124;'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 刹车 &#124;'
- en: '| Hernandez et al. [[144](#bib.bib144)] | EEG | CNN | Local | 0.718 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| Hernandez 等人 [[144](#bib.bib144)] | EEG | CNN | Local | 0.718 |'
- en: '| Gender Detection | Putten et al. [[146](#bib.bib146)] | EEG | CNN | Local
    | 0.81 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| 性别检测 | Putten 等人 [[146](#bib.bib146)] | EEG | CNN | Local | 0.81 |'
- en: '| Hiroyasu et al. [[201](#bib.bib201)] | fNIRS | D-AE + MLP | Local | 0.81
    |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| Hiroyasu 等人 [[201](#bib.bib201)] | fNIRS | D-AE + MLP | Local | 0.81 |'
- en: 5.4 Security
  id: totrans-610
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 安全
- en: Brain signals can be used in security scenarios such as identification (or recognition)
    and authentication (or verification). The former conducts multi-class classification
    to recognize a person’s identity [[6](#bib.bib6)]. The latter conducts binary
    classification to decide whether a person is authorized [[61](#bib.bib61)].
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 脑信号可用于安全场景，如身份识别（或识别）和认证（或验证）。前者进行多类分类以识别一个人的身份 [[6](#bib.bib6)]。后者进行二分类以决定一个人是否被授权
    [[61](#bib.bib61)]。
- en: The majority of the existing biometric identification/authentication systems
    rely on individuals’ intrinsic physiological features such as face, iris, retina,
    voice, and fingerprint [[6](#bib.bib6)]. They are vulnerable to various attacks
    based on anti-surveillance prosthetic masks, contact lenses, vocoder, and fingerprint
    films. EEG-based biometric person identification is a promising alternative given
    its highly resilient to spoofing attacks—individual’s EEG signals are virtually
    impossible for an imposter to mimic. Koike et al. [[161](#bib.bib161)] have adopted
    deep neural networks to identify the user’s ID based on the VEP signals; Mao et
    al. [[180](#bib.bib180)] applied CNN for person identification based on RSVP signals;
    Zhang et al. [[6](#bib.bib6)] proposed an attention-based LSTM model and evaluated
    it over both public and local datasets. EEG signals are also combined with gait
    information in a hybrid deep learning model for a dual-authentication system [[61](#bib.bib61)].
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数生物识别/认证系统依赖于个人的内在生理特征，如面部、虹膜、视网膜、声音和指纹 [[6](#bib.bib6)]。这些系统容易受到基于抗监视假肢面具、隐形眼镜、声码器和指纹膜的各种攻击。基于
    EEG 的生物识别身份识别是一个有前景的替代方案，因为它对伪造攻击高度抗干扰——个人的 EEG 信号几乎不可能被伪装者模仿。Koike 等人 [[161](#bib.bib161)]
    采用深度神经网络根据 VEP 信号识别用户 ID；Mao 等人 [[180](#bib.bib180)] 使用 CNN 进行基于 RSVP 信号的身份识别；Zhang
    等人 [[6](#bib.bib6)] 提出了一个基于注意力的 LSTM 模型，并在公共和本地数据集上进行了评估。EEG 信号还与步态信息结合，在混合深度学习模型中用于双重认证系统
    [[61](#bib.bib61)]。
- en: 5.5 Affective Computing
  id: totrans-613
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 情感计算
- en: 'Affective states of a user provide critical information for many applications
    such as personalized information (e.g., multimedia content) retrieval or intelligent
    human-computer interface design [[99](#bib.bib99)]. Recent research illustrated
    that deep learning models can enhance the performance in affective computing.
    The most widely used circumplex model believe the emotions are distributed in
    two dimensions: arousal and valence. The arousal refers to the intensity of the
    emotional stimuli or how strong is the emotion. The valence refers to the relationship
    within the person who experiences the emotion. In some other models, the dominance
    and liking dimensions are deployed.'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 用户的情感状态为许多应用程序提供了关键信息，如个性化信息（例如，多媒体内容）检索或智能人机界面设计[[99](#bib.bib99)]。最近的研究表明，深度学习模型可以提高情感计算的性能。最广泛使用的圆形模型认为情感分布在两个维度上：唤醒度和愉悦度。唤醒度指的是情感刺激的强度或情感的强烈程度。愉悦度指的是经历情感的个人内在关系。在一些其他模型中，部署了支配和喜好维度。
- en: Some research [[89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)] attempts
    to classify users’ emotional state into two (positive/negative) or three categories
    (positive, neutral, and negative) based on EEG signals using deep learning algorithms
    such as CNN and its variants [[87](#bib.bib87)]. DBN-RBM is the most representative
    deep learning model to discover the concealed features from emotional spontaneous
    EEG [[99](#bib.bib99), [96](#bib.bib96)]. Xu et al. [[99](#bib.bib99)] applied
    DBN-RBM as feature extractors to classify affective states based on EEG.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)]尝试基于EEG信号使用深度学习算法（如CNN及其变体）将用户的情感状态分类为两个（积极/消极）或三个类别（积极、中性和消极）[[87](#bib.bib87)]。DBN-RBM是最具代表性的深度学习模型，用于发现情感自发EEG中的隐含特征[[99](#bib.bib99),
    [96](#bib.bib96)]。Xu等人[[99](#bib.bib99)]将DBN-RBM作为特征提取器，用于基于EEG分类情感状态。
- en: Further, some researchers aim to recognize the positive/negative state of each
    specific emotional dimension. For example, Yin et al. [[107](#bib.bib107)] employed
    an ensemble classifier of AE in order to recognize the user’s affection. Each
    AE uses three hidden layers to filter out noises and to derive stable physiological
    feature representations. The proposed model was evaluated over the benchmark,
    DEAP, and achieved the arousal of 77.19% and valence of 76.17%.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究人员旨在识别每个特定情感维度的积极/消极状态。例如，Yin等人[[107](#bib.bib107)]采用了AE的集成分类器以识别用户的情感。每个AE使用三个隐藏层来过滤噪声并获得稳定的生理特征表示。所提出的模型在基准测试DEAP上进行评估，达到了77.19%的唤醒度和76.17%的愉悦度。
- en: 5.6 Driver Fatigue Detection
  id: totrans-617
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 驾驶员疲劳检测
- en: Vehicle drivers’ ability to keep alert and maintain optimal performance will
    dramatically affect the traffic safety [[145](#bib.bib145)]. EEG signals have
    proven useful in evaluating the human’s cognitive state in different context.
    Generally, a driver is regarded as in an alert state if the reaction time is lower
    than 0.7 seconds and in fatigue state if it is higher than 2.1 seconds. Hajinoroozi
    et al. [[153](#bib.bib153)] considered the detection of driver’s fatigue from
    EEG signals by discovering the distinct features. They explored an approach based
    on DBN for dimension reduction.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆驾驶员保持警觉和维持最佳表现的能力将极大地影响交通安全[[145](#bib.bib145)]。EEG信号在不同背景下评估人类认知状态方面已被证明有效。通常，如果反应时间低于0.7秒，则认为驾驶员处于警觉状态；如果高于2.1秒，则认为处于疲劳状态。Hajinoroozi等人[[153](#bib.bib153)]通过发现EEG信号中的独特特征来考虑驾驶员疲劳的检测。他们探索了一种基于DBN的降维方法。
- en: Detecting driver fatigue is crucial because the drowsiness of the driver may
    lead to disaster. Driver fatigue detection is feasible in practice. In the hardware
    aspect, the collection equipment of EEG singles is off-the-shelf and portable
    enough to be used in a car. Moreover, the price of an EEG headset is affordable
    for most people. In the algorithm aspect, deep learning models have enhanced the
    performance of fatigue detection. As we summarized, the EEG based driving drowsiness
    can be recognized with high accuracy (82% $\sim$ 95%).
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 检测驾驶员疲劳至关重要，因为驾驶员的困倦可能导致灾难。驾驶员疲劳检测在实践中是可行的。在硬件方面，EEG信号采集设备现成且足够便携，可以用于汽车中。此外，EEG耳机的价格对大多数人来说是负担得起的。在算法方面，深度学习模型提升了疲劳检测的性能。正如我们总结的那样，基于EEG的驾驶困倦可以以高准确率（82%
    $\sim$ 95%）被识别。
- en: Future scope of drive fatigue detection is in the self-driving scenario. As
    we know, in the most situation of self-driving (e.g., Automation level 3⁵⁵5https://en.wikipedia.org/wiki/Self-driving_car),
    the human driver is expected to respond appropriately to a request to intervene,
    which indicates that the driver should keep alert state. Therefore, we believe
    the application of brain signal-based drive fatigue detection will benefit the
    development of the self-driving car.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 驾驶疲劳检测的未来方向是在自动驾驶场景中。如我们所知，在大多数自动驾驶情况下（例如，自动化等级 3⁵⁵5https://en.wikipedia.org/wiki/Self-driving_car），预计人类驾驶员应对干预请求做出适当响应，这表明驾驶员应保持警觉状态。因此，我们相信基于脑信号的驾驶疲劳检测应用将有助于自动驾驶汽车的发展。
- en: 5.7 Mental Load Measurement
  id: totrans-621
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 心理负荷测量
- en: The EEG oscillations can be used to measure the mental workload level, which
    can sustain decision making and strategy development in the context of human-machine
    interaction [[150](#bib.bib150)]. Additionally, the appropriate mental workload
    is essential for maintaining human health and preventing accidents. For example,
    the abnormal mental workload of the human operator may result in performance degradation
    which could cause catastrophic accidents [[232](#bib.bib232)]. Evaluation of operator
    Mental Workload levels via ongoing EEG is quite promising in Human-Machine collaborative
    task environment to alarm the temporal operator performance degradation.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 脑电图（EEG）振荡可以用来测量心理负荷水平，这对维持决策制定和策略发展在人机交互的背景下至关重要[[150](#bib.bib150)]。此外，适当的心理负荷对于维持人类健康和预防事故也是必不可少的。例如，人类操作员的异常心理负荷可能导致性能下降，从而引发灾难性事故[[232](#bib.bib232)]。通过持续脑电图评估操作员的心理负荷水平，在人机协作任务环境中对警报操作员性能暂时下降的前景非常有希望。
- en: 'Table 6: The summary of public dataset for brain signal studies. The ‘# Sub’,
    ‘# Cla’, and S-Rate denote the number of subject, number of class, and sampling
    rate, respectively. FM denote finger movement while BCI-C denote the BCI Competition.
    The ‘# channel‘ refers to the number of brain signal channels.'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '表6: 脑信号研究的公共数据集总结。‘# Sub’，‘# Cla’ 和 S-Rate 分别表示受试者数量、类别数量和采样率。FM 代表手指运动，而 BCI-C
    代表 BCI 竞赛。‘# channel’ 指的是脑信号通道的数量。'
- en: '| Brain Signals | Name Link | # Sub | # Cla | S-Rate | # Channel |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| 脑信号 | 名称 链接 | # Sub | # Cla | S-Rate | # Channel |'
- en: '| EEG | Sleep EEG | Sleep-EDF⁶⁶6https://physionet.org/physiobank/database/sleep-edfx/:
    Telemetry | 22 | 6 | 100 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| 脑电图 | 睡眠脑电图 | Sleep-EDF⁶⁶6https://physionet.org/physiobank/database/sleep-edfx/:
    遥测 | 22 | 6 | 100 |'
- en: '&#124; 2 &#124;'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 &#124;'
- en: '|'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sleep-EDF: Cassette | 78 | 6 | 100, 1 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| Sleep-EDF: Cassette | 78 | 6 | 100, 1 |'
- en: '&#124; 2 &#124;'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 &#124;'
- en: '|'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MASS-1⁷⁷7https://massdb.herokuapp.com/en/ | 53 | 5 | 256 |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| MASS-1⁷⁷7https://massdb.herokuapp.com/en/ | 53 | 5 | 256 |'
- en: '&#124; 17 &#124;'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 17 &#124;'
- en: '|'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MASS-2 | 19 | 6 | 256 |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| MASS-2 | 19 | 6 | 256 |'
- en: '&#124; 19 &#124;'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 19 &#124;'
- en: '|'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MASS-3 | 62 | 5 | 256 |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| MASS-3 | 62 | 5 | 256 |'
- en: '&#124; 20 &#124;'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 20 &#124;'
- en: '|'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MASS-4 | 40 | 6 | 256 |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| MASS-4 | 40 | 6 | 256 |'
- en: '&#124; 4 &#124;'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4 &#124;'
- en: '|'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MASS-5 | 26 | 6 | 256 |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| MASS-5 | 26 | 6 | 256 |'
- en: '&#124; 20 &#124;'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 20 &#124;'
- en: '|'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SHHS⁸⁸footnotemark: 8 | 5804 | N/A | 125, 50 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| SHHS⁸⁸footnotemark: 8 | 5804 | 不适用 | 125, 50 |'
- en: '&#124; 2 &#124;'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 &#124;'
- en: '|'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Seizure EEG | CHB-MIT⁹⁹footnotemark: 9 | 22 | 2 | 256 | 18 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| 癫痫脑电图 | CHB-MIT⁹⁹footnotemark: 9 | 22 | 2 | 256 | 18 |'
- en: '| TUH^(10)^(10)footnotemark: 10 | 315 | 2 | 200 | 19 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| TUH^(10)^(10)footnotemark: 10 | 315 | 2 | 200 | 19 |'
- en: '| MI EEG | EEGMMI^(11)^(11)footnotemark: 11 | 109 | 4 | 160 | 64 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| MI 脑电图 | EEGMMI^(11)^(11)footnotemark: 11 | 109 | 4 | 160 | 64 |'
- en: '| BCI-C II^(12)^(12)footnotemark: 12, Dataset III | 1 | 2 | 128 | 3 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C II^(12)^(12)footnotemark: 12, Dataset III | 1 | 2 | 128 | 3 |'
- en: '| BCI-C III, Dataset III a | 3 | 4 | 250 | 60 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C III, Dataset III a | 3 | 4 | 250 | 60 |'
- en: '| BCI-C III, Dataset III b | 3 | 2 | 125 | 2 |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C III, Dataset III b | 3 | 2 | 125 | 2 |'
- en: '| BCI-C III, Dataset IV a | 5 | 2 | 1000 | 118 |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C III, Dataset IV a | 5 | 2 | 1000 | 118 |'
- en: '| BCI-C III, Dataset IV b | 1 | 2 | 1001 | 119 |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C III, Dataset IV b | 1 | 2 | 1001 | 119 |'
- en: '| BCI-C III, Dataset IV c | 1 | 2 | 1002 | 120 |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C III, Dataset IV c | 1 | 2 | 1002 | 120 |'
- en: '| BCI-C IV, Dataset I | 7 | 2 | 1000 | 64 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C IV, Dataset I | 7 | 2 | 1000 | 64 |'
- en: '| BCI-C IV, Dataset II a | 9 | 4 | 250 | 22 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C IV, Dataset II a | 9 | 4 | 250 | 22 |'
- en: '| BCI-C IV, Dataset II b | 9 | 2 | 250 | 3 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C IV, Dataset II b | 9 | 2 | 250 | 3 |'
- en: '| Emotional EEG | AMIGOS^(13)^(13)footnotemark: 13 | 40 | 4 | 128 | 14 |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| 情感脑电图 | AMIGOS^(13)^(13)footnotemark: 13 | 40 | 4 | 128 | 14 |'
- en: '| SEED^(14)^(14)footnotemark: 14 | 15 | 3 | 200 | 62 |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| SEED^(14)^(14)footnotemark: 14 | 15 | 3 | 200 | 62 |'
- en: '| DEAP^(15)^(15)footnotemark: 15 | 32 | 4 | 512 | 32 |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| DEAP^(15)^(15)footnotemark: 15 | 32 | 4 | 512 | 32 |'
- en: '|'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Others &#124;'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其他 &#124;'
- en: '&#124; EEG &#124;'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 脑电图 &#124;'
- en: '| Open MIIR^(16)^(16)footnotemark: 16 | 10 | 12 | 512 | 64 |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| Open MIIR^(16)^(16)footnotemark: 16 | 10 | 12 | 512 | 64 |'
- en: '| VEP | BCI-C II, Dataset II b | 1 | 36 | 240 | 64 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| VEP | BCI-C II, 数据集 II b | 1 | 36 | 240 | 64 |'
- en: '| BCI-C III, Dataset II | 2 | 26 | 240 | 64 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| BCI-C III, 数据集 II | 2 | 26 | 240 | 64 |'
- en: '| fMRI | ADNI^(17)^(17)footnotemark: 17 | 202 | 3 | N/A | N/A |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| fMRI | ADNI^(17)^(17)脚注：17 | 202 | 3 | N/A | N/A |'
- en: '| BRATS^(18)^(18)18https://physionet.org/pn3/shhpsgdb/2013 | 65 | 4 | N/A |
    N/A |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| BRATS^(18)^(18)18https://physionet.org/pn3/shhpsgdb/2013 | 65 | 4 | N/A |
    N/A |'
- en: '| MEG | BCI-C IV, Dataset III | 2 | 4 | 400 | 10 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| MEG | BCI-C IV, 数据集 III | 2 | 4 | 400 | 10 |'
- en: '^(19)^(19)footnotetext: https://physionet.org/pn6/chbmit/^(20)^(20)footnotetext:
    https://www.isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml^(21)^(21)footnotetext:
    https://physionet.org/pn4/eegmmidb/^(22)^(22)footnotetext: http://www.bbci.de/competition/ii/^(23)^(23)footnotetext:
    http://www.eecs.qmul.ac.uk/mmv/datasets/amigos/readme.html^(24)^(24)footnotetext:
    http://bcmi.sjtu.edu.cn/ seed/download.html^(25)^(25)footnotetext: https://www.eecs.qmul.ac.uk/mmv/datasets/deap/^(26)^(26)footnotetext:
    https://owenlab.uwo.ca/research/the_openmiir_dataset.html^(27)^(27)footnotetext:
    http://adni.loni.usc.edu/data-samples/access-data/^(28)^(28)footnotetext: https://www.med.upenn.edu/sbia/brats2018/data.html'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: ^(19)^(19)脚注： https://physionet.org/pn6/chbmit/^(20)^(20)脚注： https://www.isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml^(21)^(21)脚注：
    https://physionet.org/pn4/eegmmidb/^(22)^(22)脚注： http://www.bbci.de/competition/ii/^(23)^(23)脚注：
    http://www.eecs.qmul.ac.uk/mmv/datasets/amigos/readme.html^(24)^(24)脚注： http://bcmi.sjtu.edu.cn/
    seed/download.html^(25)^(25)脚注： https://www.eecs.qmul.ac.uk/mmv/datasets/deap/^(26)^(26)脚注：
    https://owenlab.uwo.ca/research/the_openmiir_dataset.html^(27)^(27)脚注： http://adni.loni.usc.edu/data-samples/access-data/^(28)^(28)脚注：
    https://www.med.upenn.edu/sbia/brats2018/data.html
- en: Several researchers have been paid attention to this topic. The mental workload
    can be measured from fNIRS signals or spontaneous EEG. Naseer et al. adopted a
    MLP algorithm for fNIRS-based binary mental task level classification (mental
    arithmetic and rest) [[38](#bib.bib38)]. The experiment results showed that the
    MLP outperformed the traditional classifiers like SVM, KNN, and achieved the highest
    accuracy of 96.3%. Bashivan et al. [[159](#bib.bib159)] presented a statistical
    approach, a DBN model, for the recognition of mental workload level based on single-trial
    EEG. Before the DBN, the authors manually extracted the wavelet entropy and band-specific
    power from three frequency bands (theta, alpha, and beta). At last, the experiments
    demonstrated the recognition of mental workload achieved an overall accuracy of
    92%. Zhang et al. [[156](#bib.bib156)] investigate the mental load measurement
    across multiple mental tasks via a recurrent-convolutional framework. The model
    simultaneously learns EEG features from the spatial, spectral, and temporal dimensions,
    which results in the accuracy of 88.9% in binary classification (high/low workload
    levels).
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究者已经关注了这个话题。心理负荷可以通过 fNIRS 信号或自发 EEG 进行测量。Naseer 等人采用了一种 MLP 算法来进行基于 fNIRS
    的二元心理任务级别分类（心理算术和休息）[[38](#bib.bib38)]。实验结果表明，MLP 优于传统分类器如 SVM、KNN，达到了最高 96.3%
    的准确率。Bashivan 等人 [[159](#bib.bib159)] 提出了一个统计方法，即 DBN 模型，用于基于单次试验 EEG 的心理负荷水平识别。在
    DBN 之前，作者手动从三个频段（θ 波、α 波和 β 波）中提取了小波熵和带特定功率。最后，实验表明心理负荷的识别达到了整体 92% 的准确率。Zhang
    等人 [[156](#bib.bib156)] 通过递归卷积框架研究了多个心理任务下的心理负荷测量。该模型同时从空间、频谱和时间维度学习 EEG 特征，二元分类（高/低负荷水平）的准确率达到了
    88.9%。
- en: 5.8 Other Applications
  id: totrans-675
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.8 其他应用
- en: There are plenty of interesting scenarios beyond the above where deep learning-based
    brain signals can apply, such as recommender system [[135](#bib.bib135)] and emergency
    braking [[144](#bib.bib144)]. One possible topic is the recognition of a visual
    object, which may be used in guilty knowledge test [[195](#bib.bib195)] and concealed
    information test [[168](#bib.bib168)]. The neurons of the participant will produce
    a pulse when he/she suddenly watch a similar object. Based on the theory, the
    visual target recognition is mainly used RSVP signals. Cecotti et al. [[177](#bib.bib177)]
    aimed to build a common model for target recognition, which can work for various
    subjects instead of a specific subject.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 除上述情况外，还有许多有趣的场景可以应用基于深度学习的脑信号，例如推荐系统 [[135](#bib.bib135)] 和紧急制动 [[144](#bib.bib144)]。一个可能的主题是视觉对象的识别，这可能用于罪恶知识测试
    [[195](#bib.bib195)] 和隐秘信息测试 [[168](#bib.bib168)]。当参与者突然看到一个类似的对象时，神经元会产生一个脉冲。基于这一理论，视觉目标识别主要使用
    RSVP 信号。Cecotti 等人 [[177](#bib.bib177)] 旨在建立一个通用的目标识别模型，可以适用于不同的受试者，而不是特定的受试者。
- en: Besides, researchers have investigated to distinguish the subject’s gender by
    the fNIRS [[201](#bib.bib201)] and spontaneous EEG [[146](#bib.bib146)]. Hiriyasu
    et al. [[201](#bib.bib201)] adopted deep learning to recognize the gender of the
    subject based on the cerebral blood flow. The experiment results suggested that
    the cerebral blood flow changes in different ways for male and female. Putten
    et al. [[146](#bib.bib146)] tried to discover the sex-specific information from
    the brain rhythms and adopted a CNN model to recognize the participant’s gender.
    This paper illustrated that fast beta activity (20 $\sim$25 Hz) is one of the
    most distinctive attributes.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员已经通过 fNIRS [[201](#bib.bib201)] 和自发 EEG [[146](#bib.bib146)] 研究区分受试者的性别。Hiriyasu
    等人 [[201](#bib.bib201)] 采用深度学习来识别基于脑血流的受试者性别。实验结果表明，男性和女性的脑血流变化方式不同。Putten 等人
    [[146](#bib.bib146)] 尝试从脑电节律中发现性别特异性信息，并采用 CNN 模型来识别参与者的性别。本文表明，快速 beta 活动 (20
    $\sim$25 Hz) 是最具辨别性的特征之一。
- en: 5.9 Benchmark Datasets
  id: totrans-678
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.9 基准数据集
- en: 'We have extensively explored the benchmark datasets usable for deep learning-based
    brain signals (Table [6](#S5.T6 "Table 6 ‣ 5.7 Mental Load Measurement ‣ 5 Brain
    Signal-based Applications ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers")). We provide a bunch of public datasets
    with download links, which cover most brain signal types. In particular, BCI competition
    IV (BCI-C IV) contains five datasets via the same link. For better understanding,
    we present the number of subjects, the number of class (how many categories),
    sampling rate, and the number of channels of each dataset. In the ‘# Channel’
    column, the default channel is for EEG signals. Some datasets contain more biometric
    signals (e.g., ECG), but we only list the channels related to brain signals.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '我们广泛探讨了适用于基于深度学习的脑信号的基准数据集（表 [6](#S5.T6 "Table 6 ‣ 5.7 Mental Load Measurement
    ‣ 5 Brain Signal-based Applications ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")）。我们提供了一些公共数据集的下载链接，涵盖了大多数脑信号类型。特别是，BCI
    竞赛 IV (BCI-C IV) 通过相同的链接包含五个数据集。为了更好地理解，我们展示了每个数据集的受试者数量、类别数量（多少类）、采样率和通道数量。在‘#
    Channel’列中，默认通道用于 EEG 信号。一些数据集包含更多的生物特征信号（例如 ECG），但我们仅列出了与脑信号相关的通道。'
- en: 6 Analysis and Guidelines
  id: totrans-680
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 分析与指导
- en: 'In this section, we first analyze what is the most suitable deep learning models
    for each brain signal. Then, we summarize the popular deep learning models in
    brain signal research. At last, we investigate the brain signals in terms of application.
    We hope this survey could help our readers to select the most effective and efficient
    methods when dealing with brain signals. Please recall Table [4](#S4.T4 "Table
    4 ‣ 4.1.1 Spontaneous EEG ‣ 4.1 EEG ‣ 4 State-of-The-Art DL Techniques for Brain
    Signals ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") where we summarize the brain signals and the corresponding
    deep learning models of the state-of-the-art papers. Figure [4](#S5.F4 "Figure
    4 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") illustrated of
    the publications proportion for crucial brain signals and deep learning models.'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们首先分析最适合每种脑信号的深度学习模型。然后，我们总结了脑信号研究中流行的深度学习模型。最后，我们从应用的角度探讨脑信号。我们希望这项调查能够帮助读者在处理脑信号时选择最有效、最有效的方法。请参考表 [4](#S4.T4
    "Table 4 ‣ 4.1.1 Spontaneous EEG ‣ 4.1 EEG ‣ 4 State-of-The-Art DL Techniques
    for Brain Signals ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers")，其中总结了脑信号及其对应的最先进论文中的深度学习模型。图 [4](#S5.F4 "Figure
    4 ‣ 5.3 Communication ‣ 5 Brain Signal-based Applications ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") 说明了关键脑信号和深度学习模型的出版比例。'
- en: 6.1 Brain Signal Acquisition
  id: totrans-682
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 脑信号获取
- en: 'Among the non-invasive signals, the studies on EEG is far more than the sum
    of all the other brain signal paradigms (fNIRS, fMRI, and MEG). Furthermore, there
    are about 70% of the EEG papers pay attention to the spontaneous EEG (133 publications).
    For better understanding, we split the spontaneous EEG into several aspects: the
    sleep, the motor imagery, the emotional, the mental disease, the data augmentation,
    and others.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 在非侵入性信号中，对 EEG 的研究远远超过了所有其他脑信号范式（fNIRS、fMRI 和 MEG）的总和。此外，大约 70% 的 EEG 论文关注自发
    EEG（133 篇出版物）。为了更好地理解，我们将自发 EEG 分为几个方面：睡眠、运动想象、情感、心理疾病、数据增强等。
- en: First, the classification of the sleep EEG mainly depends on the discriminative
    and the hybrid models. Among the nineteen studies about sleep stage classification,
    there are six employed CNN and the modified CNN models independently while two
    papers adopted RNN models. There are three hybrid models built on the combination
    of CNN and RNN.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，睡眠 EEG 的分类主要依赖于判别模型和混合模型。在关于睡眠阶段分类的十九项研究中，有六项独立使用了 CNN 和改进的 CNN 模型，而两篇论文采用了
    RNN 模型。还有三种混合模型是基于 CNN 和 RNN 的组合构建的。
- en: Second, in terms of the research on MI EEG (30 publications), the independent
    CNN and CNN-based hybrid models are widely used. As for the representative models,
    DBN-RBM is often applied to capture the latent features from the MI EEG signals.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，就 MI EEG 的研究（30篇出版物）而言，独立 CNN 和基于 CNN 的混合模型被广泛使用。至于代表性模型，DBN-RBM 通常用于捕获 MI
    EEG 信号中的潜在特征。
- en: Third, there are twenty-five publications related to spontaneous emotional EEG.
    More than half of them employed representative models (such as D-AE, D-RBM, especially
    DBN-RBM) for unsupervised feature learning. The most typical state recognition
    works recognize the user’s emotion as positive, neutral, or negative. Some researchers
    take a further step to classify the valence, and the arouse rate, which is more
    complex and challenging.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，有二十五篇与自发情绪脑电图（EEG）相关的出版物。其中一半以上使用了代表性模型（如 D-AE、D-RBM，特别是 DBN-RBM）进行无监督特征学习。最典型的状态识别工作将用户的情绪识别为积极、中性或消极。有些研究者进一步将情绪分类为情感价值和唤醒率，这更为复杂且具有挑战性。
- en: Fourth, the research on mental disease diagnosis is promising and attracting.
    The majority of the related research focuses on the detection of epileptic seizure
    and Alzheimer’s Disease. Since the detection is a binary classification problem
    which is rather easier than multi-class classification, many studies can achieve
    a high accuracy like above 90%. In this area, the standard CNN model and the D-AE
    are prevalent. One possible reason is that CNN and AE are the most well-known
    and effective deep learning models for classification and dimensionality reduction.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，精神疾病诊断的研究前景广阔且引人注目。大多数相关研究集中在癫痫发作和阿尔茨海默病的检测上。由于检测是一个二分类问题，比多分类问题容易得多，因此许多研究可以达到超过90%的高准确率。在这个领域，标准
    CNN 模型和 D-AE 被广泛使用。一个可能的原因是 CNN 和 AE 是最知名和有效的深度学习模型，用于分类和降维。
- en: Fifth, several publications pay attention to the GAN based data augmentation.
    At last, about thirty studies are investigating other spontaneous EEG such as
    driving fatigue, audio/visual stimuli impact, cognitive/mental load, and eye state
    detection. These studies extensively apply standard CNN models and variants.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 第五，一些出版物关注基于 GAN 的数据增强。最后，大约三十项研究正在调查其他自发 EEG，例如驾驶疲劳、音频/视觉刺激影响、认知/心理负荷和眼睛状态检测。这些研究广泛应用了标准
    CNN 模型及其变体。
- en: Moreover, apart from spontaneous EEG, evoked potentials also attracted much
    attention. On the one hand, in ERP, VEP and the subcategory RSVP has drawn lots
    of investigations because visual stimuli, compared to other stimuli, is easier
    to be conducted and more applicable in the real world (e.g., P300 speller can
    be used for brain typing). For VEP (twenty-one publications), there are elven
    studies applied discriminative models, and six works adopted hybrid models. In
    terms of RSVP, the sole CNN dominates the algorithms. Apart from them, five papers
    focused on the analysis of AEP signals. On the other hand, among the steady-state
    related researches, only SSVEP has been studied by deep learning models. Most
    of them only applied discriminative models on the recognition of the target image.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了自发 EEG，诱发电位也引起了很多关注。一方面，在 ERP、VEP 和子类别 RSVP 中，由于视觉刺激相比于其他刺激更易于实施且在现实世界中更具适用性（例如
    P300 拼写器可用于脑部打字），因此得到了大量研究。在 VEP（21篇出版物）中，有十一项研究应用了判别模型，六项研究采用了混合模型。在 RSVP 中，单一的
    CNN 主导了算法。除此之外，还有五篇论文关注 AEP 信号的分析。另一方面，在稳态相关的研究中，只有 SSVEP 被深度学习模型研究。大多数研究仅在目标图像的识别中应用了判别模型。
- en: Furthermore, beyond the diverse EEG diagrams, a wide range of papers paid attention
    to fNIRS and fMRI. The fNIRS images are rarely studied by deep learning, and the
    major studies just employed the simple MLP models. We believe more attention should
    be paid to the research on fNIRS for the high portability and low cost. As for
    the fMRI, twenty-three papers proposed deep learning models to the classification.
    The CNN model is widely used for its outstanding performance in feature learning
    from images. There are also several papers interested in image reconstruction
    based on fMRI signals. One reason why fMRI is so hot is that several public datasets
    are available on the Internet, although the fMRI equipment is expensive. The MEG
    signals are mainly used in the medical area, which is insensitive to the deep
    learning algorithm. Thus, we only found very few studies on MEG. The sparse AE
    and CNN algorithms have a positive influence on the feature refining and classification
    of MEG.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了多样的 EEG 图表外，还有许多论文关注 fNIRS 和 fMRI。fNIRS 图像很少被深度学习研究，主要的研究仅使用了简单的 MLP 模型。我们认为应更多关注
    fNIRS 的研究，因为它具有高度的可移植性和低成本。至于 fMRI，有二十三篇论文提出了用于分类的深度学习模型。CNN 模型因其在图像特征学习中的出色表现而被广泛使用。还有几篇论文关注基于
    fMRI 信号的图像重建。fMRI 之所以如此热门的一个原因是，虽然 fMRI 设备昂贵，但有几个公开数据集可以在互联网上获得。MEG 信号主要用于医学领域，对深度学习算法不敏感。因此，我们只找到很少的关于
    MEG 的研究。稀疏 AE 和 CNN 算法对 MEG 的特征提炼和分类有积极影响。
- en: 6.2 Selection Criteria for Deep Learning Models
  id: totrans-691
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 深度学习模型的选择标准
- en: Our investigation shows that discriminative models are most frequent in the
    summarized publications. This is reasonable at a high level because a large proportion
    of brain signal issues can be regarded as a classification problem. Another observation
    is that CNN and its variants are adopted in more than 70% of the discriminative
    models, for which we provide reasons as follows.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查显示，分类模型在总结的文献中最为频繁。这在较高层次上是合理的，因为大脑信号问题中的大部分可以视为分类问题。另一个观察结果是，CNN 及其变体被采用于超过
    70% 的分类模型，我们提供如下原因。
- en: First, the design of CNN is powerful enough to extract the latent discriminative
    features and spatial dependencies from the EEG signals for classification. As
    a result, CNN structures are adopted for classification in some studies while
    adopted for feature extraction in some other studies.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，CNN 的设计足够强大，能够从 EEG 信号中提取潜在的区分特征和空间依赖关系进行分类。因此，CNN 结构在一些研究中被用于分类，而在另一些研究中则被用于特征提取。
- en: Second, CNN has been achieved great success in some research areas (e.g., computer
    vision), which makes it extremely famous and feasible (public codes). Thus, the
    brain signal researchers have more chance to understand and apply CNN on their
    works.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，CNN 在某些研究领域（例如计算机视觉）取得了巨大成功，这使其极为著名且可行（公开代码）。因此，大脑信号研究人员有更多机会了解和应用 CNN 于他们的研究中。
- en: 'Third, some brain signal diagrams (e.g., fMRI) are naturally formed as two-dimension
    images that are conducive to be processedg by CNN. Meanwhile, other 1-D signals
    (e.g., EEG) could be converted into 2-D images for further analysis by CNN. Here,
    we provide several methods converting 1-D EEG signals (with multiple channels)
    to the 2-D matrix: 1) convert each time-point^(19)^(19)19Time-point represents
    one sampling point. For example, we can have 100 time-points if the sampling rate
    is 100 Hz. to a 2-D image; 2) convert a segment into a 2-D matrix. In the first
    situation, suppose we have 32 channels, and we can collect 32 elements (each element
    corresponding to a channel) at each time-point. As described in [[89](#bib.bib89)],
    the collected 32 elements could be converted into a 2-D image based on the spatial
    position. In the second situation, suppose we have 32 channels, and the segment
    contains 100 time-points. The collected data can be arranged as a matrix with
    the shape of $[32,100]$ where each row and column refers to a specific channel
    and time-point, respectively.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，一些脑信号图（例如fMRI）自然形成二维图像，这有利于用CNN进行处理。同时，其他1-D信号（例如EEG）可以转换为2-D图像，进一步通过CNN分析。这里，我们提供几种将1-D
    EEG信号（具有多个通道）转换为2-D矩阵的方法：1）将每个时间点^(19)^(19)19时间点表示一个采样点。例如，如果采样率是100 Hz，我们可以有100个时间点。转换为2-D图像；2）将一个片段转换为2-D矩阵。在第一种情况下，假设我们有32个通道，我们可以在每个时间点收集32个元素（每个元素对应一个通道）。如[[89](#bib.bib89)]所述，收集的32个元素可以根据空间位置转换为2-D图像。在第二种情况下，假设我们有32个通道，片段包含100个时间点。收集的数据可以排列成一个形状为$[32,100]$的矩阵，其中每行和每列分别指代特定的通道和时间点。
- en: Fourth, there are a lot of variants of CNN which are suitable for a wide range
    of brain signal scenarios. For example, the single-channel EEG signals can be
    processed by 1-D CNN. In terms of RNN, only about 20% of discriminative model-based
    papers adopted RNN, which is much less than we expected since RNN has demonstrated
    powerful in temporal feature learning. One possible reason for this phenomena
    is that processing a long sequence by RNN is time-consuming and the EEG signals
    are generally formed as a long sequence. For example, the sleep signals are usually
    sliced into segments with 30 seconds, which has 3000 time-points under 100 Hz
    sampling rate. For a sequence with 3000 elements, through our preliminary experiments,
    RNN takes more than 20 folds training time than CNN. Moreover, MLP is not popular
    due to its inferior effectiveness (e.g., non-linear ability) to the other algorithms
    its simple deep learning architecture.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，CNN有许多变体，适用于各种脑信号场景。例如，单通道EEG信号可以通过1-D CNN进行处理。至于RNN，只有约20%的基于判别模型的论文采用了RNN，这比我们预期的要少，因为RNN在时间特征学习中表现出强大的能力。这种现象的一个可能原因是处理长序列的RNN耗时较长，而EEG信号通常形成长序列。例如，睡眠信号通常被切分为30秒的片段，在100
    Hz采样率下有3000个时间点。对于3000个元素的序列，通过我们的初步实验，RNN的训练时间比CNN多20倍以上。此外，由于其效果（例如非线性能力）不如其他算法，MLP因其简单的深度学习架构而不受欢迎。
- en: 'As for representative models, DBN, especially DBN-RBM, is the most popular
    model for feature extraction. DBN is widely used in brain signal for two reasons:
    1) it learns the generative parameters that reveal the relationship of variables
    in neighboring layers efficiently; 2) it makes it straightforward to calculate
    the values of latent variables in each hidden layer [[233](#bib.bib233)]. However,
    most works that employed the DBN-RBM model were published before 2016\. It can
    be inferred that the researchers prefer to use DBN for feature learning followed
    by a non-deep learning classifier before 2016; but recently, an increasing number
    of studies would like to adopt CNN or hybrid models for both feature learning
    and classification.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 关于代表性模型，DBN，特别是DBN-RBM，是最受欢迎的特征提取模型。DBN在脑信号中广泛使用有两个原因：1）它高效地学习生成参数，揭示相邻层变量之间的关系；2）它使得计算每个隐藏层的潜在变量的值变得简单[[233](#bib.bib233)]。然而，大多数采用DBN-RBM模型的工作发表在2016年之前。因此，可以推测，在2016年之前，研究人员倾向于使用DBN进行特征学习，然后使用非深度学习分类器；但最近，越来越多的研究倾向于采用CNN或混合模型进行特征学习和分类。
- en: Moreover, generative models are rarely employed independently. The GAN- and
    VAE-based data augmentation and image reconstruction are mainly focused on fMRI
    and EEG signals. It is demonstrated that the trained classifier will achieve more
    competitive performance after data augmentation. Therefore, this is a promising
    research prospect in the future.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，生成模型很少单独使用。基于GAN和VAE的数据增强和图像重建主要集中在fMRI和EEG信号上。研究表明，经过数据增强的训练分类器将实现更具竞争力的性能。因此，这是未来一个有前景的研究方向。
- en: Last but not the least, there are fifty-three publications proposed hybrid models
    for brain signal studies. Among them, the combinations of RNN and CNN take about
    one-fifth proportion. Since RNN and CNN are illustrated having excellent temporal
    and spatial feature extraction ability, it is natural to combine them for both
    temporal and spatial feature learning. Another type of hybrid models is the combination
    of representative and discriminative models. This is easy to understand because
    the former is employed for feature refining, and the latter is employed for classification.
    There are twenty-eight publications which almost covered all the brain signals
    proposed this type of hybrid deep learning models. The adopted representative
    models are mostly AE or DBN-RBM; at the meanwhile, the adopted discriminative
    models are mostly CNN. Apart from that, there are twelve papers proposed other
    hybrid models such as two discriminative models. For example, several studies
    proposed the combination of CNN and MLP where a CNN structure is used for extract
    spatial features and an MLP is used for classification.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，有五十三篇文献提出了用于脑信号研究的混合模型。其中，RNN和CNN的组合约占五分之一。由于RNN和CNN被证明具有出色的时间和空间特征提取能力，因此将它们结合用于时间和空间特征学习是很自然的。另一种类型的混合模型是代表性模型和辨别模型的组合。这很容易理解，因为前者用于特征细化，而后者用于分类。有二十八篇文献几乎覆盖了所有脑信号，并提出了这种类型的混合深度学习模型。采用的代表性模型大多是AE或DBN-RBM，而辨别模型大多是CNN。除此之外，还有十二篇论文提出了其他混合模型，如两个辨别模型。例如，一些研究提出了CNN和MLP的组合，其中CNN结构用于提取空间特征，MLP用于分类。
- en: 6.3 Application Performance
  id: totrans-700
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 应用性能
- en: In order to have a closer observation of the recent advances on deep learning-based
    brain signal analysis, we analyze the brain signal acquisition methods and the
    deep learning algorithms in terms of application performance. In some cases, various
    studies adopt the same deep architecture working on the same dataset but results
    in different performance, which maybe caused by the different pre-processing methods
    and hyper-parameter settings.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更密切地观察基于深度学习的脑信号分析的最新进展，我们分析了脑信号采集方法和深度学习算法在应用性能方面的表现。在某些情况下，各种研究采用相同的深度架构在相同的数据集上工作，但结果却不同，这可能是由于不同的预处理方法和超参数设置造成的。
- en: To begin with, the most appealing and hot field is that using brain signal analysis
    on health care area. For sleep quality evaluation, the dominate brain signals
    are spontaneous EEG which are measured while the patient is sleeping. The single
    RNN or CNN models seem have a good discriminative feature learning ability and
    lead to a comprehensive performance. Generally, most of the deep learning algorithms
    can achieve the accuracy of above 85% in the context of multiple sleep stage scenario.
    Upon this, the combined hybrid models (e.g., CNN integrates with LSTM) can only
    have incremental improvements.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，最具吸引力和热门的领域是将脑信号分析应用于健康护理领域。在睡眠质量评估中，主要的脑信号是自发的EEG，这些信号是在患者睡觉时测量的。单一的RNN或CNN模型似乎具有良好的特征辨别学习能力，并能带来全面的性能。一般来说，大多数深度学习算法在多种睡眠阶段的情况下可以达到85%以上的准确率。在此基础上，结合的混合模型（例如，CNN与LSTM的集成）只能带来增量的改进。
- en: One key method to detect Alzheimer’s Disease is brain signal analysis by measuring
    the functions of specific brain regions. In detail, the diagnosis can be conducted
    by spontaneous EEG signals or fMRI images. For MD EEG, DBN is supposed to outperform
    CNN since the EEG signals contains more temporal instead of spatial information.
    As for the fMRI pictures, CNN have great advantages in the grid-arranged spatial
    information learning, which makes it obtain a very comprehensive classification
    accuracy (above 90%). As for epileptic seizure, the diagnosis are generally based
    on EEG signals. The single RNN classifier (e.g., LSTM or GRU) seems work better
    than its counterparts due to the excellent temporal dependency representing ability.
    Here, the complex hybrid models indeed outperform the single component. For example,
    [[130](#bib.bib130)] achieves a better specification than [[116](#bib.bib116)]
    on the same dataset because of combing with RNN. Most of the epileptic seizure
    detection models claim a rather high classification accuracy (above 95%). One
    possible reason is that the binary recognition scenario is much easier than multi-class
    classification.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 检测阿尔茨海默病的一个关键方法是通过分析大脑信号来测量特定大脑区域的功能。具体来说，诊断可以通过自发EEG信号或fMRI图像来进行。对于MD EEG，DBN应该优于CNN，因为EEG信号包含更多的时间信息而不是空间信息。至于fMRI图像，CNN在网格排列的空间信息学习中具有很大优势，这使其获得了非常全面的分类准确率（超过90%）。至于癫痫发作，诊断通常基于EEG信号。单一的RNN分类器（例如LSTM或GRU）似乎比其对手效果更好，因为它具有出色的时间依赖性表示能力。在这里，复杂的混合模型确实优于单一组件。例如，[[130](#bib.bib130)]在相同数据集上比[[116](#bib.bib116)]取得了更好的性能，因为结合了RNN。大多数癫痫发作检测模型声称具有相当高的分类准确率（超过95%）。一个可能的原因是二分类场景比多分类更容易。
- en: The brain signal-controlled smart environment only appear in a small number
    of publications. Among them, the brain signals are collected through very different
    methods. This is an emerging but promising field because it is easy to integrate
    with smart home and smart hospital to benefit the individuals whether healthy
    or disable. Another advantage of brain signals is bridging people’s inside and
    outer world by communication techniques. In this area, lots of investigations
    are focusing on the VEP signals because the visual evoked potential is obvious
    and easy to be detected. One important data source is from the third BCI competition.
    In addition, brain signal analysis can be widely implement in security systems
    since the brain signals are invisible and very hard to be mimicked. The characteristic
    of high fake-resistance enables brain signal a raising star in the identification/authentication
    in confidential scenarios. The drawbacks of brain signal-based security systems
    are the expensive equipment and inconvenient (e.g., the subject have to wear an
    EEG headset to monitor the brainwaves).
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑信号控制的智能环境只出现在少数几篇出版物中。其中，大脑信号通过非常不同的方法进行收集。这是一个新兴但充满前景的领域，因为它易于与智能家居和智能医院集成，以便为健康或残疾的个人带来好处。大脑信号的另一个优势是通过通信技术架起了人们的内在世界与外部世界之间的桥梁。在这个领域，许多研究集中在VEP信号上，因为视觉诱发电位明显且易于检测。一个重要的数据来源来自第三届BCI竞赛。此外，大脑信号分析可以广泛应用于安全系统，因为大脑信号是不可见的且很难被模仿。高伪造抗性特性使得大脑信号在机密场景中的身份识别/认证中成为一颗新星。基于大脑信号的安全系统的缺点是设备昂贵且不方便（例如，受试者必须佩戴EEG头盔以监测脑电波）。
- en: Affective computing has drawn much attention in recent years. The EEG signals
    have high temporal resolution and able to capture the quick-varying emotions.
    Therefore, almost all the studies are based on spontaneous EEG signals. The signals
    are gathered when the subject is watching video which is supposed to arouse the
    subject’s specific emotion. Another reason for this phenomenon is that there are
    several open-source EEG-based affecting analysis datasets (e.g., DEAP and SEED)
    which greatly promote the investigation in this area. The EEG-based affective
    computing contains two mainstreams. One of them focuses on developing powerful
    discriminative classifiers (such as hierarchical CNN) which are designed to perform
    feature extraction and classification in the same step. The other tries to learn
    the latent features through deep representative models (e.g., DBN-RBM) and then
    send the learned representations into a powerful classifier (such as HMM and MLP).
    It can be observed that the former models ([[88](#bib.bib88), [201](#bib.bib201)])
    seem outperform the latter methods ([[96](#bib.bib96)]) with a small margin on
    the SEED dataset.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 情感计算近年来引起了广泛关注。EEG信号具有高时间分辨率，能够捕捉快速变化的情绪。因此，几乎所有的研究都基于自发EEG信号。这些信号是在被试观看可能激发其特定情绪的视频时收集的。另一个原因是有多个开源的基于EEG的情感分析数据集（例如DEAP和SEED），这些数据集极大地促进了这一领域的研究。基于EEG的情感计算包括两个主流方向。其一侧重于开发强大的判别分类器（如分层CNN），这些分类器设计用于在同一步骤中进行特征提取和分类。另一种方法则尝试通过深度表征模型（如DBN-RBM）学习潜在特征，然后将学习到的表征输入到强大的分类器（如HMM和MLP）中。可以观察到，前者的模型（[[88](#bib.bib88),
    [201](#bib.bib201)]) 在SEED数据集上表现略优于后者的方法（[[96](#bib.bib96)])。
- en: Drive fatigue detection can be easily integrated in the platforms such as self-driving
    vehicles. Nevertheless, there are only a few publications in this area due to
    the expensive experimental cost and the lack of accessible dataset. Moreover,
    there are a lot of interesting applications (e.g., guilty knowledge test and gender
    detection) have been explored by deep learning models.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 驾驶疲劳检测可以很容易地集成到如自动驾驶车辆等平台中。然而，由于实验成本高昂和缺乏可用数据集，这一领域的出版物仍然很少。此外，深度学习模型还探索了许多有趣的应用（例如，罪恶知识测试和性别检测）。
- en: 7 Open Issues
  id: totrans-707
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 开放问题
- en: Although deep learning has lifted the performance of brain signal systems, technical
    and usability challenges remain. The technical challenges concern the classification
    ability in complex scenarios, and the usability challenges refer to limitations
    in large scale real-world deployment. In this section, we introduce these challenges
    and point out the possible solutions.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习提升了脑信号系统的性能，但仍然存在技术和可用性挑战。技术挑战涉及在复杂场景中的分类能力，而可用性挑战则指大规模实际部署中的限制。在这一部分，我们介绍了这些挑战并指出了可能的解决方案。
- en: 7.1 Explainable General Framework
  id: totrans-709
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 可解释的通用框架
- en: 'Until now, we have introduced several types of brain signals (e.g., spontaneous
    EEG, ERP, fMRI) and deep learning models that have been applied for each type.
    One promising research direction for deep learning-based brain signal research
    is to develop a general framework that can handle various brain signals regardless
    of the number of channels used for signal collection, the sample dimensions (e.g.,
    1-D or 2-D sample), and stimulation types (e.g., visual or audio stimuli), etc.
    The general framework would require two key capabilities: the attention mechanism
    and the ability to capture latent feature. The former guarantees the framework
    can focus on the most valuable parts of input signals, and the latter enables
    the framework to capture the distinctive and informative features.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们介绍了几种类型的脑信号（如自发EEG、ERP、fMRI）以及针对每种类型应用的深度学习模型。深度学习基础的脑信号研究的一个有前景的研究方向是开发一个通用框架，可以处理各种脑信号，无论是用于信号采集的通道数量、样本维度（例如，1-D或2-D样本）还是刺激类型（例如，视觉或听觉刺激）。该通用框架需要具备两个关键能力：注意机制和捕捉潜在特征的能力。前者保证框架能够专注于输入信号中最有价值的部分，而后者则使框架能够捕捉独特和有用的特征。
- en: The attention mechanism can be implemented based on attention scores or by various
    machine learning algorithms such as reinforcement learning. The attention scores
    can be inferred from the input data and work as a weight to help the framework
    to pay attention to the parts with high attention scores. Reinforcement learning
    has shown to be able to find the most valuable part through a policy search [[85](#bib.bib85)].
    CNN is the most suitable structure for capturing features at various levels and
    ranges. In the future, CNN could be used as a fundamental feature learning tool
    and be integrated with suitable attention mechanisms to form a general classification
    framework.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制可以基于注意力分数或通过各种机器学习算法（如强化学习）来实现。注意力分数可以从输入数据中推断出来，并作为权重帮助框架关注高注意力分数的部分。强化学习已经证明能够通过策略搜索找到最有价值的部分[[85](#bib.bib85)]。CNN是捕捉各种层次和范围特征的最合适结构。未来，CNN可以作为基础特征学习工具，并与适当的注意力机制结合，形成通用分类框架。
- en: One additional direction we may consider is how to interpret the feature representation
    derived by the deep neural network, what is the intrinsic relationship between
    the learned features and the task-related neural pattern, or neuropathology of
    mental disorders. More and more people are realizing that interpretation could
    be even more important than prediction performance, since we usually just treat
    deep learning as a black box.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能考虑的另一个方向是如何解释深度神经网络得出的特征表示，学习的特征与任务相关的神经模式或心理障碍的内在关系是什么。越来越多的人意识到解释可能比预测性能更为重要，因为我们通常只是将深度学习视为一个黑箱。
- en: 7.2 Subject-Independent Classification
  id: totrans-713
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 主题无关分类
- en: Until now, most brain signal classification tasks focus on person-dependent
    scenarios, where the training samples and testing samples are collected from the
    identical individual. The future direction is to realize person-independent classification
    so that the testing data will never appear in the training set. High-performance
    person-independent classification is compulsory for the wide application of brain
    signals in the real world.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，大多数脑信号分类任务都专注于与个人相关的场景，其中训练样本和测试样本来自同一人。未来的方向是实现与个人无关的分类，以便测试数据永远不会出现在训练集中。高性能的个人无关分类对于脑信号在现实世界中的广泛应用是必不可少的。
- en: 'One possible solution to achieving this goal is to build a personalized model
    with transfer learning. A personalized affective model can adopt a transductive
    parameter transfer approach to construct individual classifiers and to learn a
    regression function that maps the relationship between data distribution and classifier
    parameters [[234](#bib.bib234)]. Another potential solution is mining the subject-independent
    component from the input data. The input data can be decomposed into two parts:
    a subject-dependent component, which depends on the subject and a subject-independent
    component, which is common for all subjects. A hybrid multi-task model can work
    on two tasks simultaneously, one focusing on person identification and the other
    on class recognition. A well-trained and converged model is supposed to extract
    the subject-independent features in the class recognition task.'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标的一个可能解决方案是使用迁移学习构建个性化模型。个性化情感模型可以采用传导性参数迁移方法来构建个体分类器，并学习一个回归函数，将数据分布与分类器参数之间的关系进行映射[[234](#bib.bib234)]。另一个潜在解决方案是从输入数据中挖掘主题无关的成分。输入数据可以被分解为两个部分：一个是主题相关的成分，依赖于被试，另一个是主题无关的成分，对所有被试都是通用的。一个混合多任务模型可以同时处理两个任务，一个专注于个人识别，另一个专注于类别识别。一个经过良好训练并收敛的模型应该能在类别识别任务中提取主题无关的特征。
- en: 7.3 Semi-supervised and Unsupervised Classification
  id: totrans-716
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 半监督与无监督分类
- en: The performance of deep learning highly depends on the size of training data,
    which, however, requires expensive and time-consuming manual labeling to collect
    abundant class labels for a wide range of scenarios such as sleep EEG. While supervised
    learning requires both observations and labels for the training, unsupervised
    learning requires no labels, and semi-supervised learning only requires partial
    labels [[98](#bib.bib98)]. They are, therefore, more suitable for problems with
    little ground truth.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的性能高度依赖于训练数据的规模，而这需要昂贵且耗时的手动标记来收集大量类别标签，以适应诸如睡眠EEG等广泛场景。虽然监督学习需要观测值和标签用于训练，但无监督学习不需要标签，半监督学习仅需要部分标签[[98](#bib.bib98)]。因此，它们更适合于真实标签较少的问题。
- en: Zhang et al. proposed an Adversarial Variational Embedding (AVAE) framework
    that combines a VAE++ model (as a high-quality generative model) and semi-supervised
    GAN (as a posterior distribution learner) [[235](#bib.bib235)] for robust and
    effective semi-supervised learning. Jia et al. proposed a semi-supervised framework
    by leveraging the data distribution of unlabelled data to prompt the representation
    learning of labelled data [[98](#bib.bib98)].
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人提出了一个对抗变分嵌入（AVAE）框架，该框架结合了VAE++模型（作为高质量生成模型）和半监督GAN（作为后验分布学习者）[[235](#bib.bib235)]，用于鲁棒和有效的半监督学习。贾等人提出了一种通过利用未标记数据的数据分布来促使已标记数据的表示学习的半监督框架[[98](#bib.bib98)]。
- en: 'Two methods may enhance the unsupervised learning: one is to employ crowd-sourcing
    to label the unlabeled observations; the other is to leverage unsupervised domain
    adaption learning to align the distribution of source brain signals and the distribution
    of target signals with a linear transformation.'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法可能会增强无监督学习：一种是利用众包来标记未标记的观测值；另一种是利用无监督领域适应学习，通过线性变换对齐源脑信号的分布和目标信号的分布。
- en: 7.4 Online Implementation
  id: totrans-720
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 在线实施
- en: Most of the existing brain signal systems focus on offline procedure which means
    that the training and testing dataset are pre-collected and evaluated offline.
    However, in the real-world scenarios, the brain signal systems are supposed to
    receive live data stream and produce classification results in real time, which
    is still very challenging.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数脑信号系统集中于离线程序，这意味着训练和测试数据集是预先收集并离线评估的。然而，在实际场景中，脑信号系统应该接收实时数据流并实时生成分类结果，这仍然是非常具有挑战性的。
- en: For EEG signals, in the online system, compared to the offline procedure, the
    gathered live signals are more noisy and unstable due to lots of factors such
    as the less-concentrating of the subject [[236](#bib.bib236)] and the inherent
    destabilization of the equipment (e.g., fluctuating sampling rate). Through our
    empirical experiments, online brain signal systems generally perform a lower accuracy
    of 10% than their counterparts. One future scope of online implementation is to
    develop a batch of robust algorithms in order to handle the influence factors
    and discover the latent distinctive patterns underlying the noisy live brain signals.
    [[237](#bib.bib237)] implemented an EEG-based online system that achieves comparable
    performance, however, this work only investigates a very high-level target (i.e.,
    human attention). Discovering the latent invariant representations through covariance
    matrices of EEG signals can help to mitigate the influence of extinct perturbations
    [[238](#bib.bib238)]. Some post-processing methods (e.g., voting and aggregating)
    [[166](#bib.bib166), [149](#bib.bib149)] can help to improve the decoding performance
    by averaging the results from multiple continues samples. However, these methods
    will inevitably bring higher latency. Thus, the post-processing requires a trade-off
    between the high-accuracy and low-latency.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 对于脑电图（EEG）信号，在在线系统中，与离线程序相比，由于许多因素，例如受试者的注意力不集中[[236](#bib.bib236)]和设备固有的不稳定性（例如，采样率波动），采集到的实时信号更加嘈杂和不稳定。通过我们的实证实验，在线脑信号系统的准确性通常比其离线系统低10%。未来在线实施的一个方向是开发一批鲁棒的算法，以处理这些影响因素，并发现潜在的独特模式，以揭示嘈杂实时脑信号中的潜在特征。[[237](#bib.bib237)]
    实现了一个基于EEG的在线系统，该系统表现出类似的性能，但这项工作仅研究了一个非常高层次的目标（即人类注意力）。通过EEG信号的协方差矩阵发现潜在的不变表示可以帮助减轻过时扰动的影响[[238](#bib.bib238)]。一些后处理方法（例如投票和聚合）[[166](#bib.bib166),
    [149](#bib.bib149)]可以通过对多个连续样本的结果进行平均来改善解码性能。然而，这些方法不可避免地会带来更高的延迟。因此，后处理需要在高准确性和低延迟之间进行权衡。
- en: For fNIRS and fMRI, the online evaluation is relatively less challenging since
    they have a rather low temporal resolution. The online images with less dynamic
    can be regarded as static images to some extent, which makes the online system
    approximating to the offline system. Furthermore, most fMRI and MEG signals are
    used to evaluate the user’s neurological status (e.g., detect the effects of tumor)
    which does not require an instantaneous response. Thus, they have less demand
    for a real-time monitoring system.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 对于fNIRS和fMRI，在线评估相对不那么具有挑战性，因为它们的时间分辨率较低。在线图像的动态较少，在某种程度上可以视为静态图像，这使得在线系统接近于离线系统。此外，大多数fMRI和MEG信号用于评估用户的神经状态（例如，检测肿瘤的影响），这不需要即时响应。因此，它们对实时监测系统的需求较低。
- en: 7.5 Hardware Portability
  id: totrans-724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 硬件便携性
- en: Poor portability of hardware has been preventing brain signals from wide application
    in the real world. In most scenarios, users would like to use small, comfortable,
    or even wearable brain signal hardware to collect brain signals and to control
    appliances and assistant robots.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件的便携性差阻碍了脑信号在实际中的广泛应用。在大多数情况下，用户希望使用小型、舒适甚至可穿戴的脑信号硬件来采集脑信号，并控制电器和辅助机器人。
- en: 'Currently, there are three types of EEG collection equipment: the unportable,
    the portable headset, and ear-EEG sensors. The unportable equipment has high sampling
    frequency, channel numbers, and signal quality but is expensive. It is suitable
    for physical examination in a hospital. The portable headsets (e.g., Neurosky,
    Emotiv EPOC) have 1 $\sim$ 14 channels and 128$\sim$ 256 sampling rate but has
    inaccuracy readings and cause discomfort after long-time use. The ear-EEG sensors,
    which are attached to the outer eat, have gained increasing attention recently
    but remain mostly at the laboratory stage [[239](#bib.bib239)]. The ear-EEG sensors
    contain a series of electrodes which are placed in each ear canal and concha [[240](#bib.bib240)].
    The EEGrids, to the best of our knowledge, is the only commercial ear-EEG. It
    has multi-channel sensor arrays placed around the ear using an adhesive ^(20)^(20)20http://ceegrid.com/home/concept/
    and is even more expensive. A promising future direction is to improve the usability
    by developing a cheaper (e.g., lower than 200$) and more comfortable (e.g., can
    last longer than 3 hours without feeling uncomfortable) wireless ear-EEG equipment.'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有三种类型的EEG采集设备：不可移动的、便携式头戴设备和耳EEG传感器。不可移动的设备具有高采样频率、通道数量和信号质量，但价格昂贵。它适合用于医院的体检。便携式头戴设备（例如，Neurosky、Emotiv
    EPOC）具有1 $\sim$ 14个通道和128$\sim$ 256的采样率，但读数不准确且长时间使用后会导致不适。耳EEG传感器，附着在外耳上，最近受到越来越多的关注，但大多仍处于实验室阶段[[239](#bib.bib239)]。耳EEG传感器包含一系列电极，放置在每个耳道和耳轮[[240](#bib.bib240)]。据我们了解，EEGrids是唯一的商业耳EEG。它具有多通道传感器阵列，贴在耳部周围，使用粘合剂^(20)^(20)20http://ceegrid.com/home/concept/，价格更高。一个有前景的未来方向是通过开发更便宜（例如，低于200美元）且更舒适（例如，可以持续超过3小时而不会感到不适）的无线耳EEG设备来提高可用性。
- en: 8 Conclusion
  id: totrans-727
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this paper, we thoroughly summarize the recent advances in deep learning
    models for non-invasive brain signal analysis. Compared with traditional machine
    learning methods, deep learning not only enables to learn high-level features
    automatically from brain signals but also have less dependency on domain knowledge.
    We organize brain signals and dominant deep learning models, followed by discussing
    state-of-the-art deep learning techniques for brain signals. Moreover, we provide
    guidelines to help researchers to find the suitable deep learning algorithms for
    each category of brain signals. Finally, we overview deep learning-based brain
    signal applications and point out the open challenges and future directions.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们全面总结了深度学习模型在非侵入性脑信号分析中的最新进展。与传统的机器学习方法相比，深度学习不仅可以自动从脑信号中学习高级特征，还对领域知识的依赖较小。我们组织了脑信号和主流的深度学习模型，接着讨论了用于脑信号的最先进的深度学习技术。此外，我们提供了指导方针，帮助研究人员找到适合每种类别脑信号的深度学习算法。最后，我们概述了基于深度学习的脑信号应用，并指出了开放挑战和未来方向。
- en: References
  id: totrans-729
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: References
  id: totrans-730
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage, “Signal
    quality of simultaneously recorded invasive and non-invasive eeg,” *Neuroimage*,
    vol. 46, no. 3, pp. 708–716, 2009.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] T. Ball, M. Kern, I. Mutschler, A. Aertsen, 和 A. Schulze-Bonhage, “同时记录的侵入性和非侵入性EEG信号质量,”
    *Neuroimage*, vol. 46, no. 3, pp. 708–716, 2009.'
- en: '[2] X. Zhang, L. Yao, S. Zhang, S. Kanhere, M. Sheng, and Y. Liu, “Internet
    of things meets brain-computer interface: A unified deep learning framework for
    enabling human-thing cognitive interactivity,” *IEEE Internet of Things Journal*,
    2018.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] X. Zhang, L. Yao, S. Zhang, S. Kanhere, M. Sheng, 和 Y. Liu，“物联网与脑-计算机接口的结合：启用人-物认知互动的统一深度学习框架”，*IEEE
    物联网杂志*，2018 年。'
- en: '[3] X. An, D. Kuang, X. Guo, Y. Zhao, and L. He, “A deep learning method for
    classification of eeg data based on motor imagery,” in *International Conference
    on Intelligent Computing*, 2014, pp. 203–210.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. An, D. Kuang, X. Guo, Y. Zhao, 和 L. He，“一种基于运动意象的脑电图数据分类深度学习方法”，在*智能计算国际会议*，2014
    年，第 203–210 页。'
- en: '[4] Y. R. Tabar and U. Halici, “A novel deep learning approach for classification
    of eeg motor imagery signals,” *Journal of neural engineering*, vol. 14, no. 1,
    p. 016003, 2016.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. R. Tabar 和 U. Halici，“一种新颖的深度学习方法用于脑电图运动意象信号的分类”，*神经工程学杂志*，第 14 卷，第
    1 期，第 016003 页，2016 年。'
- en: '[5] F. Lotte, L. Bougrain, A. Cichocki, M. Clerc, M. Congedo, A. Rakotomamonjy,
    and F. Yger, “A review of classification algorithms for eeg-based brain–computer
    interfaces: a 10 year update,” *Journal of neural engineering*, vol. 15, no. 3,
    p. 031005, 2018.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] F. Lotte, L. Bougrain, A. Cichocki, M. Clerc, M. Congedo, A. Rakotomamonjy,
    和 F. Yger，“基于脑电图的脑-计算机接口分类算法综述：十年更新”，*神经工程学杂志*，第 15 卷，第 3 期，第 031005 页，2018 年。'
- en: '[6] X. Zhang, L. Yao, S. S. Kanhere, Y. Liu, T. Gu, and K. Chen, “Mindid: Person
    identification from brain waves through attention-based recurrent neural network,”
    *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    vol. 2, no. 3, p. 149, 2018.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] X. Zhang, L. Yao, S. S. Kanhere, Y. Liu, T. Gu, 和 K. Chen，“Mindid：通过基于注意力的递归神经网络从脑波中识别个人”，*ACM
    交互式、移动、可穿戴及普适计算技术会议论文集*，第 2 卷，第 3 期，第 149 页，2018 年。'
- en: '[7] S. N. Abdulkader, A. Atia, and M.-S. M. Mostafa, “Brain computer interfacing:
    Applications and challenges,” *Egyptian Informatics Journal*, vol. 16, no. 2,
    pp. 213–230, 2015.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] S. N. Abdulkader, A. Atia, 和 M.-S. M. Mostafa，“脑-计算机接口：应用与挑战”，*埃及信息学杂志*，第
    16 卷，第 2 期，第 213–230 页，2015 年。'
- en: '[8] A. Bashashati, M. Fatourechi, R. K. Ward, and G. E. Birch, “A survey of
    signal processing algorithms in brain–computer interfaces based on electrical
    brain signals,” *Journal of Neural engineering*, vol. 4, no. 2, p. R32, 2007.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Bashashati, M. Fatourechi, R. K. Ward, 和 G. E. Birch，“基于电脑信号的脑-计算机接口中的信号处理算法调查”，*神经工程学杂志*，第
    4 卷，第 2 期，第 R32 页，2007 年。'
- en: '[9] W. Samek, K.-R. Müller, M. Kawanabe, and C. Vidaurre, “Brain-computer interfacing
    in discriminative and stationary subspaces,” in *Engineering in Medicine and Biology
    Society (EMBC), 2012 Annual International Conference of the IEEE*, 2012, pp. 2873–2876.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] W. Samek, K.-R. Müller, M. Kawanabe, 和 C. Vidaurre，“在判别和稳定子空间中的脑-计算机接口”，在*医学与生物学工程学会（EMBC），2012
    年 IEEE 年度国际会议*，2012 年，第 2873–2876 页。'
- en: '[10] X. Zhang, L. Yao, Q. Z. Sheng, S. S. Kanhere, T. Gu, and D. Zhang, “Converting
    your thoughts to texts: Enabling brain typing via deep feature learning of eeg
    signals,” in *2018 IEEE International Conference on Pervasive Computing and Communications
    (PerCom)*, 2018, pp. 1–10.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] X. Zhang, L. Yao, Q. Z. Sheng, S. S. Kanhere, T. Gu, 和 D. Zhang，“将你的思想转化为文本：通过脑电图信号的深度特征学习实现脑输入”，在*2018
    IEEE 全球计算与通信大会（PerCom）*，2018 年，第 1–10 页。'
- en: '[11] F. Lotte, M. Congedo, A. Lécuyer, F. Lamarche, and B. Arnaldi, “A review
    of classification algorithms for eeg-based brain–computer interfaces,” *Journal
    of neural engineering*, vol. 4, no. 2, p. R1, 2007.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] F. Lotte, M. Congedo, A. Lécuyer, F. Lamarche, 和 B. Arnaldi，“基于脑电图的脑-计算机接口分类算法综述”，*神经工程学杂志*，第
    4 卷，第 2 期，第 R1 页，2007 年。'
- en: '[12] H. Cecotti, M. P. Eckstein, and B. Giesbrecht, “Single-trial classification
    of event-related potentials in rapid serial visual presentation tasks using supervised
    spatial filtering,” *IEEE transactions on neural networks and learning systems*,
    vol. 25, no. 11, pp. 2030–2042, 2014.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] H. Cecotti, M. P. Eckstein, 和 B. Giesbrecht，“使用监督空间滤波器在快速序列视觉呈现任务中对事件相关电位进行单次试验分类”，*IEEE
    神经网络与学习系统汇刊*，第 25 卷，第 11 期，第 2030–2042 页，2014 年。'
- en: '[13] M. Mahmud, M. S. Kaiser, A. Hussain, and S. Vassanelli, “Applications
    of deep learning and reinforcement learning to biological data,” *IEEE transactions
    on neural networks and learning systems*, vol. 29, no. 6, pp. 2063–2079, 2018.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Mahmud, M. S. Kaiser, A. Hussain, 和 S. Vassanelli，“深度学习和强化学习在生物数据中的应用”，*IEEE
    神经网络与学习系统汇刊*，第 29 卷，第 6 期，第 2063–2079 页，2018 年。'
- en: '[14] D. Wen, Z. Wei, Y. Zhou, G. Li, X. Zhang, and W. Han, “Deep learning methods
    to process fmri data and their application in the diagnosis of cognitive impairment:
    A brief overview and our opinion,” *Frontiers in neuroinformatics*, vol. 12, p. 23,
    2018.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] D. Wen, Z. Wei, Y. Zhou, G. Li, X. Zhang, 和 W. Han，“处理fMRI数据的深度学习方法及其在认知障碍诊断中的应用：简要概述及我们的观点”，*神经信息学前沿*，第12卷，页码23，2018年。'
- en: '[15] S. Mason, A. Bashashati, M. Fatourechi, K. Navarro, and G. Birch, “A comprehensive
    survey of brain interface technology designs,” *Annals of biomedical engineering*,
    vol. 35, no. 2, pp. 137–169, 2007.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Mason, A. Bashashati, M. Fatourechi, K. Navarro, 和 G. Birch，“脑接口技术设计的全面综述”，*生物医学工程年鉴*，第35卷，第2期，页码137–169，2007年。'
- en: '[16] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, “A survey on deep learning
    in medical image analysis,” *Medical image analysis*, vol. 42, pp. 60–88, 2017.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, 和 C. I. Sánchez，“医疗图像分析中的深度学习综述”，*医疗图像分析*，第42卷，页码60–88，2017年。'
- en: '[17] Y. Roy, H. Banville, I. Albuquerque, A. Gramfort, T. H. Falk, and J. Faubert,
    “Deep learning-based electroencephalography analysis: a systematic review,” *Journal
    of neural engineering*, vol. 16, no. 5, p. 051001, 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Roy, H. Banville, I. Albuquerque, A. Gramfort, T. H. Falk, 和 J. Faubert，“基于深度学习的脑电图分析：系统评价”，*神经工程学杂志*，第16卷，第5期，页码051001，2019年。'
- en: '[18] X. Wang, G. Gong, N. Li, and Y. Ma, “A survey of the bci and its application
    prospect,” in *Theory, Methodology, Tools and Applications for Modeling and Simulation
    of Complex Systems*, 2016, pp. 102–111.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] X. Wang, G. Gong, N. Li, 和 Y. Ma，“脑机接口及其应用前景综述”，收录于*复杂系统建模与仿真的理论、方法、工具与应用*，2016年，页码102–111。'
- en: '[19] F. Movahedi, J. L. Coyle, and E. Sejdić, “Deep belief networks for electroencephalography:
    A review of recent contributions and future outlooks,” *IEEE journal of biomedical
    and health informatics*, vol. 22, no. 3, pp. 642–652, 2018.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] F. Movahedi, J. L. Coyle, 和 E. Sejdić，“用于脑电图的深度信念网络：近期贡献与未来展望的回顾”，*IEEE生物医学与健康信息学期刊*，第22卷，第3期，页码642–652，2018年。'
- en: '[20] S. R. Soekadar, N. Birbaumer, M. W. Slutzky, and L. G. Cohen, “Brain–machine
    interfaces in neurorehabilitation of stroke,” *Neurobiology of disease*, vol. 83,
    pp. 172–179, 2015.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. R. Soekadar, N. Birbaumer, M. W. Slutzky, 和 L. G. Cohen，“脑–机接口在中风神经康复中的应用”，*疾病神经生物学*，第83卷，页码172–179，2015年。'
- en: '[21] M. Ahn and S. C. Jun, “Performance variation in motor imagery brain–computer
    interface: a brief review,” *Journal of neuroscience methods*, vol. 243, pp. 103–110,
    2015.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] M. Ahn 和 S. C. Jun，“运动想象脑–机接口中的性能变异：简要回顾”，*神经科学方法期刊*，第243卷，页码103–110，2015年。'
- en: '[22] S. Ruiz, K. Buyukturkoglu, M. Rana, N. Birbaumer, and R. Sitaram, “Real-time
    fmri brain computer interfaces: self-regulation of single brain regions to networks,”
    *Biological psychology*, vol. 95, pp. 4–20, 2014.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] S. Ruiz, K. Buyukturkoglu, M. Rana, N. Birbaumer, 和 R. Sitaram，“实时fMRI脑机接口：从单一脑区到网络的自我调节”，*生物心理学*，第95卷，页码4–20，2014年。'
- en: '[23] A. Haider and R. Fazel-Rezai, “Application of p300 event-related potential
    in brain-computer interface,” in *Event-Related Potentials and Evoked Potentials*,
    2017.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Haider 和 R. Fazel-Rezai，“P300事件相关电位在脑–机接口中的应用”，收录于*事件相关电位与诱发电位*，2017年。'
- en: '[24] J. Liu, Y. Pan, M. Li, Z. Chen, L. Tang, C. Lu, and J. Wang, “Applications
    of deep learning to mri images: a survey,” *Big Data Mining and Analytics*, vol. 1,
    no. 1, pp. 1–18, 2018.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Liu, Y. Pan, M. Li, Z. Chen, L. Tang, C. Lu, 和 J. Wang，“深度学习在MRI图像中的应用：综述”，*大数据挖掘与分析*，第1卷，第1期，页码1–18，2018年。'
- en: '[25] O. Tsinalis, P. M. Matthews, Y. Guo, and S. Zafeiriou, “Automatic sleep
    stage scoring with single-channel eeg using convolutional neural networks,” *arXiv
    preprint arXiv:1610.01683*, 2016.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] O. Tsinalis, P. M. Matthews, Y. Guo, 和 S. Zafeiriou，“使用卷积神经网络进行单通道脑电图自动睡眠阶段评分”，*arXiv预印本arXiv:1610.01683*，2016年。'
- en: '[26] Q. Gui, M. Ruiz-blondet, S. Laszlo, and Z. Jin, “A survey on brain biometrics,”
    *ACM Computing Surveys*, vol. 51, no. 112, 2019.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Q. Gui, M. Ruiz-blondet, S. Laszlo, 和 Z. Jin，“脑生物识别技术综述”，*ACM计算机调查*，第51卷，第112期，2019年。'
- en: '[27] R. Abiri, S. Borhani, E. W. Sellers, Y. Jiang, and X. Zhao, “A comprehensive
    review of eeg-based brain–computer interface paradigms,” *Journal of neural engineering*,
    vol. 16, no. 1, p. 011001, 2019.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] R. Abiri, S. Borhani, E. W. Sellers, Y. Jiang, 和 X. Zhao，“基于脑电图的脑–机接口范式综述”，*神经工程学杂志*，第16卷，第1期，页码011001，2019年。'
- en: '[28] H. Cecotti and A. J. Ries, “Best practice for single-trial detection of
    event-related potentials: Application to brain-computer interfaces,” *International
    Journal of Psychophysiology*, vol. 111, pp. 156–169, 2017.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] H. Cecotti 和 A. J. Ries, “单次试验事件相关电位的最佳实践：应用于脑机接口，” *国际心理生理学期刊*，第111卷，第156–169页，2017年。'
- en: '[29] N. Naseer and K.-S. Hong, “fnirs-based brain-computer interfaces: a review,”
    *Frontiers in human neuroscience*, vol. 9, p. 3, 2015.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] N. Naseer 和 K.-S. Hong, “基于fnirs的脑机接口：综述，” *人类神经科学前沿*，第9卷，第3页，2015年。'
- en: '[30] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Schmidhuber, “神经网络中的深度学习：概述，” *神经网络*，第61卷，第85–117页，2015年。'
- en: '[31] L. Deng, “A tutorial survey of architectures, algorithms, and applications
    for deep learning,” *APSIPA Transactions on Signal and Information Processing*,
    vol. 3, 2014.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] L. Deng, “深度学习的架构、算法和应用教程综述，” *APSIPA信号与信息处理学报*，第3卷，2014年。'
- en: '[32] M. Fatourechi, A. Bashashati, R. K. Ward, and G. E. Birch, “Emg and eog
    artifacts in brain computer interface systems: A survey,” *Clinical neurophysiology*,
    vol. 118, no. 3, pp. 480–494, 2007.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Fatourechi, A. Bashashati, R. K. Ward, 和 G. E. Birch, “脑机接口系统中的肌电图和眼电图伪影：综述，”
    *临床神经生理学*，第118卷，第3期，第480–494页，2007年。'
- en: '[33] S. Vieira, W. H. Pinaya, and A. Mechelli, “Using deep learning to investigate
    the neuroimaging correlates of psychiatric and neurological disorders: Methods
    and applications,” *Neuroscience & Biobehavioral Reviews*, vol. 74, pp. 58–75,
    2017.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Vieira, W. H. Pinaya, 和 A. Mechelli, “利用深度学习调查精神疾病和神经疾病的神经影像学相关性：方法与应用，”
    *神经科学与生物行为评论*，第74卷，第58–75页，2017年。'
- en: '[34] P. Aricò, G. Borghini, G. Di Flumeri, N. Sciaraffa, and F. Babiloni, “Passive
    bci beyond the lab: current trends and future directions,” *Physiological measurement*,
    vol. 39, no. 8, p. 08TR02, 2018.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] P. Aricò, G. Borghini, G. Di Flumeri, N. Sciaraffa, 和 F. Babiloni, “实验室之外的被动bci：当前趋势与未来方向，”
    *生理测量*，第39卷，第8期，第08TR02页，2018年。'
- en: '[35] G. Pfurtscheller and C. Neuper, “Motor imagery and direct brain-computer
    communication,” *Proceedings of the IEEE*, vol. 89, no. 7, pp. 1123–1134, 2001.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] G. Pfurtscheller 和 C. Neuper, “运动想象与直接脑机通信，” *IEEE学报*，第89卷，第7期，第1123–1134页，2001年。'
- en: '[36] D. Huang, K. Qian, D.-Y. Fei, W. Jia, X. Chen, and O. Bai, “Electroencephalography
    (eeg)-based brain–computer interface (bci): A 2-d virtual wheelchair control based
    on event-related desynchronization/synchronization and state control,” *IEEE Transactions
    on Neural Systems and Rehabilitation Engineering*, vol. 20, no. 3, pp. 379–388,
    2012.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Huang, K. Qian, D.-Y. Fei, W. Jia, X. Chen, 和 O. Bai, “基于脑电图（eeg）的脑-机接口（bci）：基于事件相关去同步/同步和状态控制的2D虚拟轮椅控制，”
    *IEEE神经系统与康复工程学报*，第20卷，第3期，第379–388页，2012年。'
- en: '[37] D. Regan, “Steady-state evoked potentials,” *JOSA*, vol. 67, no. 11, pp.
    1475–1489, 1977.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] D. Regan, “稳态诱发电位，” *光学学会杂志*，第67卷，第11期，第1475–1489页，1977年。'
- en: '[38] N. Naseer, N. K. Qureshi, F. M. Noori, and K.-S. Hong, “Analysis of different
    classification techniques for two-class functional near-infrared spectroscopy-based
    brain-computer interface,” *Computational intelligence and neuroscience*, vol.
    2016, 2016.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] N. Naseer, N. K. Qureshi, F. M. Noori, 和 K.-S. Hong, “对两类功能性近红外光谱脑机接口的不同分类技术的分析，”
    *计算智能与神经科学*，第2016卷，2016年。'
- en: '[39] S. Singh, S. Jain, T. Ahuja, Y. Sharma, and N. Pathak, “Study for reduction
    of pollution level in diesel engines, petrol engines and generator sets by bio
    signal ring,” *International Journal of Advance Research and Innovation*, vol. 6,
    no. 3, pp. 175–181, 2018.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Singh, S. Jain, T. Ahuja, Y. Sharma, 和 N. Pathak, “通过生物信号环减少柴油发动机、汽油发动机和发电机组的污染水平的研究，”
    *国际先进研究与创新期刊*，第6卷，第3期，第175–181页，2018年。'
- en: '[40] S. K. Pal and S. Mitra, “Multilayer perceptron, fuzzy sets, and classification,”
    *IEEE Transactions on neural networks*, vol. 3, no. 5, pp. 683–697, 1992.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. K. Pal 和 S. Mitra, “多层感知器、模糊集合与分类，” *IEEE神经网络学报*，第3卷，第5期，第683–697页，1992年。'
- en: '[41] T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, and S. Khudanpur, “Recurrent
    neural network based language model,” in *Eleventh annual conference of the international
    speech communication association*, 2010.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, 和 S. Khudanpur, “基于递归神经网络的语言模型，”
    载于 *第十一届国际语音通信协会年会*，2010年。'
- en: '[42] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran, “Deep
    convolutional neural networks for lvcsr,” in *2013 IEEE international conference
    on acoustics, speech and signal processing*, 2013, pp. 8614–8618.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, 和 B. Ramabhadran，“用于大词汇连续语音识别的深度卷积神经网络”，收录于*《2013年IEEE国际声学、语音与信号处理会议》*，2013年，第8614–8618页。'
- en: '[43] M. A. Kramer, “Nonlinear principal component analysis using autoassociative
    neural networks,” *AIChE journal*, vol. 37, no. 2, pp. 233–243, 1991.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. A. Kramer，“使用自编码神经网络的非线性主成分分析”，*《AIChE期刊》*，第37卷，第2期，第233–243页，1991年。'
- en: '[44] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
    data with neural networks,” *science*, vol. 313, no. 5786, pp. 504–507, 2006.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] G. E. Hinton 和 R. R. Salakhutdinov，“使用神经网络降低数据维度”，*《科学》*，第313卷，第5786期，第504–507页，2006年。'
- en: '[45] G. E. Hinton, “Deep belief networks,” *Scholarpedia*, vol. 4, no. 5, p.
    5947, 2009.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] G. E. Hinton，“深度置信网络”，*《学者百科》*，第4卷，第5期，第5947页，2009年。'
- en: '[46] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] D. P. Kingma 和 M. Welling，“自编码变分贝叶斯”，*《arXiv预印本 arXiv:1312.6114》*，2013年。'
- en: '[47] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络”，收录于*《神经信息处理系统进展》*，2014年，第2672–2680页。'
- en: '[48] S. Chambon, M. N. Galtier, P. J. Arnal, G. Wainrib, and A. Gramfort, “A
    deep learning architecture for temporal sleep stage classification using multivariate
    and multimodal time series,” *IEEE Transactions on Neural Systems and Rehabilitation
    Engineering*, 2018.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] S. Chambon, M. N. Galtier, P. J. Arnal, G. Wainrib, 和 A. Gramfort，“用于时间序列睡眠阶段分类的深度学习架构”，*《IEEE神经系统与康复工程交易》*，2018年。'
- en: '[49] J. Zhang, Y. Wu, J. Bai, and F. Chen, “Automatic sleep stage classification
    based on sparse deep belief net and combination of multiple classifiers,” *Transactions
    of the Institute of Measurement and Control*, vol. 38, no. 4, pp. 435–451, 2016.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Zhang, Y. Wu, J. Bai, 和 F. Chen，“基于稀疏深度置信网络和多分类器组合的自动睡眠阶段分类”，*《测量与控制学会交易》*，第38卷，第4期，第435–451页，2016年。'
- en: '[50] A. Sors, S. Bonnet, S. Mirek, L. Vercueil, and J.-F. Payen, “A convolutional
    neural network for sleep stage scoring from raw single-channel eeg,” *Biomedical
    Signal Processing and Control*, vol. 42, pp. 107–114, 2018.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] A. Sors, S. Bonnet, S. Mirek, L. Vercueil, 和 J.-F. Payen，“基于原始单通道EEG的睡眠阶段评分卷积神经网络”，*《生物医学信号处理与控制》*，第42卷，第107–114页，2018年。'
- en: '[51] A. Vilamala, K. H. Madsen, and L. K. Hansen, “Neural networks for interpretable
    analysis of eeg sleep stage scoring,” in *International Workshop on Machine Learning
    for Signal Processing 2017*, 2017.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Vilamala, K. H. Madsen, 和 L. K. Hansen，“用于EEG睡眠阶段评分的神经网络可解释分析”，收录于*《2017年机器学习信号处理国际研讨会》*，2017年。'
- en: '[52] S. Biswal, J. Kulas, H. Sun, B. Goparaju, M. B. Westover, M. T. Bianchi,
    and J. Sun, “Sleepnet: automated sleep staging system via deep learning,” *arXiv
    preprint arXiv:1707.08262*, 2017.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Biswal, J. Kulas, H. Sun, B. Goparaju, M. B. Westover, M. T. Bianchi,
    和 J. Sun，“Sleepnet：通过深度学习实现的自动化睡眠分期系统”，*《arXiv预印本 arXiv:1707.08262》*，2017年。'
- en: '[53] K. M. Tsiouris, V. C. Pezoulas, M. Zervakis, S. Konitsiotis, D. D. Koutsouris,
    and D. I. Fotiadis, “A long short-term memory deep learning network for the prediction
    of epileptic seizures using eeg signals,” *Computers in biology and medicine*,
    vol. 99, pp. 24–37, 2018.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] K. M. Tsiouris, V. C. Pezoulas, M. Zervakis, S. Konitsiotis, D. D. Koutsouris,
    和 D. I. Fotiadis，“用于癫痫发作预测的长短期记忆深度学习网络，基于EEG信号”，*《生物医学与医学计算机》*，第99卷，第24–37页，2018年。'
- en: '[54] D. Tan, R. Zhao, J. Sun, and W. Qin, “Sleep spindle detection using deep
    learning: a validation study based on crowdsourcing,” in *Engineering in Medicine
    and Biology Society (EMBC), 2015 37th Annual International Conference of the IEEE*,
    2015, pp. 2828–2831.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] D. Tan, R. Zhao, J. Sun, 和 W. Qin，“使用深度学习进行睡眠纺锤体检测：基于众包的验证研究”，收录于*《医学与生物工程学会（EMBC），2015年第37届IEEE年度国际会议》*，2015年，第2828–2831页。'
- en: '[55] M. Manzano, A. Guillén, I. Rojas, and L. J. Herrera, “Combination of eeg
    data time and frequency representations in deep networks for sleep stage classification,”
    in *International Conference on Intelligent Computing*, 2017, pp. 219–229.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M. Manzano, A. Guillén, I. Rojas, 和 L. J. Herrera，“在深度网络中结合EEG数据的时间和频率表示进行睡眠阶段分类”，收录于*《国际智能计算会议》*，2017年，第219–229页。'
- en: '[56] L. Fraiwan and K. Lweesy, “Neonatal sleep state identification using deep
    learning autoencoders,” in *Signal Processing & its Applications (CSPA), 2017
    IEEE 13th International Colloquium on*, 2017, pp. 228–231.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] L. Fraiwan 和 K. Lweesy, “利用深度学习自编码器进行新生儿睡眠状态识别，” 在 *信号处理与其应用 (CSPA)，2017
    IEEE 第十三届国际讨论会*，2017 年，第 228–231 页。'
- en: '[57] A. Supratak, H. Dong, C. Wu, and Y. Guo, “Deepsleepnet: a model for automatic
    sleep stage scoring based on raw single-channel eeg,” *IEEE Transactions on Neural
    Systems and Rehabilitation Engineering*, vol. 25, no. 11, pp. 1998–2008, 2017.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Supratak, H. Dong, C. Wu, 和 Y. Guo, “Deepsleepnet: 一种基于原始单通道 EEG 的自动睡眠阶段评分模型，”
    *IEEE 神经系统与康复工程交易*，第 25 卷，第 11 期，第 1998–2008 页，2017 年。'
- en: '[58] H. Dong, A. Supratak, W. Pan, C. Wu, P. M. Matthews, and Y. Guo, “Mixed
    neural network approach for temporal sleep stage classification,” *IEEE Transactions
    on Neural Systems and Rehabilitation Engineering*, vol. 26, no. 2, pp. 324–333,
    2018.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Dong, A. Supratak, W. Pan, C. Wu, P. M. Matthews, 和 Y. Guo, “基于混合神经网络的时间睡眠阶段分类，”
    *IEEE 神经系统与康复工程交易*，第 26 卷，第 2 期，第 324–333 页，2018 年。'
- en: '[59] K. G. Hartmann, R. T. Schirrmeister, and T. Ball, “Hierarchical internal
    representation of spectral features in deep convolutional networks trained for
    eeg decoding,” in *Brain-Computer Interface (BCI), 2018 6th International Conference
    on*, 2018, pp. 1–6.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] K. G. Hartmann, R. T. Schirrmeister, 和 T. Ball, “深度卷积网络中用于 EEG 解码的谱特征的层次内部表示，”
    在 *脑机接口 (BCI)，2018 第六届国际会议*，2018 年，第 1–6 页。'
- en: '[60] E. Nurse, B. S. Mashford, A. J. Yepes, I. Kiral-Kornek, S. Harrer, and
    D. R. Freestone, “Decoding eeg and lfp signals using deep learning: heading truenorth,”
    in *Proceedings of the ACM International Conference on Computing Frontiers*, 2016,
    pp. 259–266.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] E. Nurse, B. S. Mashford, A. J. Yepes, I. Kiral-Kornek, S. Harrer, 和 D.
    R. Freestone, “利用深度学习解码 EEG 和 LFP 信号：迎向真北，” 在 *ACM 国际计算前沿会议论文集*，2016 年，第 259–266
    页。'
- en: '[61] X. Zhang, L. Yao, K. Chen, X. Wang, Q. Sheng, and T. Gu, “Deepkey: An
    eeg and gait based dual-authentication system,” *ACM Transactions on Intelligent
    Systems and Technology (TIST)*, 2017.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] X. Zhang, L. Yao, K. Chen, X. Wang, Q. Sheng, 和 T. Gu, “Deepkey: 基于 EEG
    和步态的双重认证系统，” *ACM 智能系统与技术事务 (TIST)*，2017 年。'
- en: '[62] H. Yang, S. Sakhavi, K. K. Ang, and C. Guan, “On the use of convolutional
    neural networks and augmented csp features for multi-class motor imagery of eeg
    signals classification,” in *Engineering in Medicine and Biology Society (EMBC),
    2015 37th Annual International Conference of the IEEE*, 2015, pp. 2620–2623.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] H. Yang, S. Sakhavi, K. K. Ang, 和 C. Guan, “卷积神经网络和增强 CSP 特征在多类运动想象 EEG
    信号分类中的应用，” 在 *医学与生物工程学会 (EMBC)，2015 第三十七届 IEEE 年度国际会议*，2015 年，第 2620–2623 页。'
- en: '[63] L. Jingwei, C. Yin, and Z. Weidong, “Deep learning eeg response representation
    for brain computer interface,” in *Control Conference (CCC), 2015 34th Chinese*,
    2015, pp. 3518–3523.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] L. Jingwei, C. Yin, 和 Z. Weidong, “用于脑机接口的深度学习 EEG 响应表示，” 在 *控制会议 (CCC)，2015
    第三十四届中国*，2015 年，第 3518–3523 页。'
- en: '[64] H. K. Lee and Y.-S. Choi, “A convolution neural networks scheme for classification
    of motor imagery eeg based on wavelet time-frequecy image,” in *International
    Conference on Information Networking (ICOIN) 2018*, 2018, pp. 906–909.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] H. K. Lee 和 Y.-S. Choi, “基于小波时频图像的运动想象 EEG 分类的卷积神经网络方案，” 在 *信息网络国际会议 (ICOIN)
    2018*，2018 年，第 906–909 页。'
- en: '[65] X. Zhang, L. Yao, C. Huang, Q. Z. Sheng, and X. Wang, “Intent recognition
    in smart living through deep recurrent neural networks,” in *International Conference
    on Neural Information Processing*, 2017, pp. 748–758.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] X. Zhang, L. Yao, C. Huang, Q. Z. Sheng, 和 X. Wang, “通过深度递归神经网络实现智能生活中的意图识别，”
    在 *神经信息处理国际会议*，2017 年，第 748–758 页。'
- en: '[66] Z. Tang, C. Li, and S. Sun, “Single-trial eeg classification of motor
    imagery using deep convolutional neural networks,” *Optik-International Journal
    for Light and Electron Optics*, vol. 130, pp. 11–18, 2017.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Z. Tang, C. Li, 和 S. Sun, “使用深度卷积神经网络对运动想象的单次试验 EEG 分类，” *Optik-国际光学与电子光学杂志*，第
    130 卷，第 11–18 页，2017 年。'
- en: '[67] Q. Wang, Y. Hu, and H. Chen, “Multi-channel eeg classification based on
    fast convolutional feature extraction,” in *International Symposium on Neural
    Networks*, 2017, pp. 533–540.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Q. Wang, Y. Hu, 和 H. Chen, “基于快速卷积特征提取的多通道 EEG 分类，” 在 *神经网络国际研讨会*，2017
    年，第 533–540 页。'
- en: '[68] I. Sturm, S. Lapuschkin, W. Samek, and K.-R. Müller, “Interpretable deep
    neural networks for single-trial eeg classification,” *Journal of neuroscience
    methods*, vol. 274, pp. 141–145, 2016.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] I. Sturm, S. Lapuschkin, W. Samek, 和 K.-R. Müller, “用于单次试验 EEG 分类的可解释深度神经网络，”
    *神经科学方法杂志*，第 274 卷，第 141–145 页，2016 年。'
- en: '[69] M. Shahin, B. Ahmed, S. T.-B. Hamida, F. L. Mulaffer, M. Glos, and T. Penzel,
    “Deep learning and insomnia: Assisting clinicians with their diagnosis,” *IEEE
    journal of biomedical and health informatics*, vol. 21, no. 6, pp. 1546–1553,
    2017.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. Shahin, B. Ahmed, S. T.-B. Hamida, F. L. Mulaffer, M. Glos, 和 T. Penzel，“深度学习与失眠：协助临床医生诊断，”*IEEE
    生物医学与健康信息学期刊*，第21卷，第6期，pp. 1546–1553，2017。'
- en: '[70] I. Fernández-Varela, D. Athanasakis, S. Parsons, E. Hernández-Pereira,
    and V. Moret-Bonillo, “Sleep staging with deep learning: a convolutional model,”
    in *Proceedings of the European Symposium on Artificial Neural Networks, Computational
    Intelligence and Machine Learning (ESANN 2018)*.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] I. Fernández-Varela, D. Athanasakis, S. Parsons, E. Hernández-Pereira,
    和 V. Moret-Bonillo，“使用深度学习进行睡眠分期：一种卷积模型，”收录于*欧洲人工神经网络、计算智能与机器学习研讨会（ESANN 2018）论文集*。'
- en: '[71] A. M. Chiarelli, P. Croce, A. Merla, and F. Zappasodi, “Deep learning
    for hybrid eeg-fnirs brain–computer interface: application to motor imagery classification,”
    *Journal of neural engineering*, vol. 15, no. 3, p. 036028, 2018.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] A. M. Chiarelli, P. Croce, A. Merla, 和 F. Zappasodi，“用于混合 EEG-fNIRS 脑-计算机接口的深度学习：应用于动作意象分类，”*神经工程期刊*，第15卷，第3期，p.
    036028，2018。'
- en: '[72] T. Uktveris and V. Jusas, “Application of convolutional neural networks
    to four-class motor imagery classification problem,” *Information Technology And
    Control*, vol. 46, no. 2, pp. 260–273, 2017.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] T. Uktveris 和 V. Jusas，“将卷积神经网络应用于四类动作意象分类问题，”*信息技术与控制*，第46卷，第2期，pp. 260–273，2017。'
- en: '[73] V. Lawhern, A. Solon, N. Waytowich, S. M. Gordon, C. Hung, and B. J. Lance,
    “Eegnet: a compact convolutional neural network for eeg-based brain–computer interfaces,”
    *Journal of neural engineering*, 2018.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] V. Lawhern, A. Solon, N. Waytowich, S. M. Gordon, C. Hung, 和 B. J. Lance，“EEGNet：用于基于
    EEG 的脑-计算机接口的紧凑卷积神经网络，”*神经工程期刊*，2018。'
- en: '[74] J. Li, Z. Struzik, L. Zhang, and A. Cichocki, “Feature learning from incomplete
    eeg with denoising autoencoder,” *Neurocomputing*, vol. 165, pp. 23–31, 2015.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Li, Z. Struzik, L. Zhang, 和 A. Cichocki，“使用去噪自编码器从不完整的 EEG 数据中学习特征，”*神经计算*，第165卷，pp.
    23–31，2015。'
- en: '[75] S. Redkar, “Using deep learning for human computer interface via electroencephalography,”
    *IAES International Journal of Robotics and Automation*, vol. 4, no. 4, 2015.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. Redkar，“通过脑电图使用深度学习进行人机界面研究，”*IAES 国际机器人与自动化期刊*，第4卷，第4期，2015。'
- en: '[76] X. Zhang, L. Yao, D. Zhang, X. Wang, Q. Z. Sheng, and T. Gu, “Multi-person
    brain activity recognition via comprehensive eeg signal analysis,” in *Proceedings
    of the 14th EAI International Conference on Mobile and Ubiquitous Systems: Computing,
    Networking and Services*, 2017, pp. 28–37.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] X. Zhang, L. Yao, D. Zhang, X. Wang, Q. Z. Sheng, 和 T. Gu，“通过综合 EEG 信号分析进行多人的脑活动识别，”收录于*第十四届
    EAI 国际移动与普适系统会议：计算、网络与服务论文集*，2017，pp. 28–37。'
- en: '[77] J. Li and A. Cichocki, “Deep learning of multifractal attributes from
    motor imagery induced eeg,” in *International Conference on Neural Information
    Processing*, 2014, pp. 503–510.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] J. Li 和 A. Cichocki，“从动作意象诱发的 EEG 中深度学习多重分形属性，”收录于*国际神经信息处理会议*，2014，pp.
    503–510。'
- en: '[78] Y. Ren and Y. Wu, “Convolutional deep belief networks for feature extraction
    of eeg signal,” in *International Joint Conference on Neural Networks (IJCNN)*,
    2014, pp. 2850–2853.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. Ren 和 Y. Wu，“卷积深度置信网络用于 EEG 信号的特征提取，”收录于*国际神经网络联合会议（IJCNN）*，2014，pp.
    2850–2853。'
- en: '[79] S. Kumar, A. Sharma, K. Mamun, and T. Tsunoda, “A deep learning approach
    for motor imagery eeg signal classification,” in *Computer Science and Engineering
    (APWC on CSE), 2016 3rd Asia-Pacific World Congress on*, 2016, pp. 34–39.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. Kumar, A. Sharma, K. Mamun, 和 T. Tsunoda，“用于动作意象 EEG 信号分类的深度学习方法，”收录于*计算机科学与工程（APWC
    on CSE），2016 年亚太世界大会*，2016，pp. 34–39。'
- en: '[80] N. Lu, T. Li, X. Ren, and H. Miao, “A deep learning scheme for motor imagery
    classification based on restricted boltzmann machines,” *IEEE transactions on
    neural systems and rehabilitation engineering*, vol. 25, no. 6, pp. 566–576, 2017.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] N. Lu, T. Li, X. Ren, 和 H. Miao，“基于限制玻尔兹曼机的动作意象分类深度学习方案，”*IEEE 神经系统与康复工程交易*，第25卷，第6期，pp.
    566–576，2017。'
- en: '[81] M. Dai, D. Zheng, R. Na, S. Wang, and S. Zhang, “Eeg classification of
    motor imagery using a novel deep learning framework,” *Sensors*, vol. 19, no. 3,
    p. 551, 2019.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Dai, D. Zheng, R. Na, S. Wang, 和 S. Zhang，“使用新颖的深度学习框架进行 EEG 动作意象分类，”*传感器*，第19卷，第3期，p.
    551，2019。'
- en: '[82] C. Tan, F. Sun, W. Zhang, J. Chen, and C. Liu, “Multimodal classification
    with deep convolutional-recurrent neural networks for electroencephalography,”
    in *International Conference on Neural Information Processing*, 2017, pp. 767–776.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C. Tan, F. Sun, W. Zhang, J. Chen, 和 C. Liu，“基于深度卷积-递归神经网络的多模态分类用于脑电图”，见于
    *国际神经信息处理大会*，2017，页 767–776。'
- en: '[83] L. Duan, M. Bao, J. Miao, Y. Xu, and J. Chen, “Classification based on
    multilayer extreme learning machine for motor imagery task from eeg signals,”
    *Procedia Computer Science*, vol. 88, pp. 176–184, 2016.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] L. Duan, M. Bao, J. Miao, Y. Xu, 和 J. Chen，“基于多层极限学习机的脑电信号运动想象任务分类”，*计算机科学学报*，第
    88 卷，页 176–184，2016。'
- en: '[84] E. S. Nurse, P. J. Karoly, D. B. Grayden, and D. R. Freestone, “A generalizable
    brain-computer interface (bci) using machine learning for feature discovery,”
    *PloS one*, vol. 10, no. 6, p. e0131328, 2015.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] E. S. Nurse, P. J. Karoly, D. B. Grayden, 和 D. R. Freestone，“一种通用的脑机接口（BCI）利用机器学习进行特征发现”，*PloS
    一*，第 10 卷，第 6 期，页 e0131328，2015。'
- en: '[85] X. Zhang, L. Yao, C. Huang, S. Wang, M. Tan, G. Long, and C. Wang, “Multi-modality
    sensor data classification with selective attention,” *International Joint Conferences
    on Artificial Intelligence (IJCAI)*, 2018.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] X. Zhang, L. Yao, C. Huang, S. Wang, M. Tan, G. Long, 和 C. Wang，“具有选择性注意的多模态传感器数据分类”，*国际人工智能联合会议
    (IJCAI)*，2018。'
- en: '[86] S. Sakhavi, C. Guan, and S. Yan, “Parallel convolutional-linear neural
    network for motor imagery classification,” in *Signal Processing Conference (EUSIPCO),
    2015 23rd European*, 2015, pp. 2736–2740.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] S. Sakhavi, C. Guan, 和 S. Yan，“用于运动想象分类的并行卷积-线性神经网络”，见于 *信号处理会议 (EUSIPCO),
    2015 第23届欧洲*，2015，页 2736–2740。'
- en: '[87] A. Frydenlund and F. Rudzicz, “Emotional affect estimation using video
    and eeg data in deep neural networks,” in *Canadian Conference on Artificial Intelligence*,
    2015, pp. 273–280.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] A. Frydenlund 和 F. Rudzicz，“使用视频和脑电图数据在深度神经网络中估计情感影响”，见于 *加拿大人工智能大会*，2015，页
    273–280。'
- en: '[88] T. Zhang, W. Zheng, Z. Cui, Y. Zong, and Y. Li, “Spatial-temporal recurrent
    neural network for emotion recognition,” *IEEE transactions on cybernetics*, no. 99,
    pp. 1–9, 2018.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] T. Zhang, W. Zheng, Z. Cui, Y. Zong, 和 Y. Li，“用于情感识别的时空递归神经网络”，*IEEE 网络控制论*，第
    99 期，页 1–9，2018。'
- en: '[89] J. Li, Z. Zhang, and H. He, “Implementation of eeg emotion recognition
    system based on hierarchical convolutional neural networks,” in *International
    Conference on Brain Inspired Cognitive Systems*, 2016, pp. 22–33.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Li, Z. Zhang, 和 H. He，“基于层次卷积神经网络的脑电图情感识别系统的实现”，见于 *国际脑启发认知系统大会*，2016，页
    22–33。'
- en: '[90] W. Liu, H. Jiang, and Y. Lu, “Analyze eeg signals with convolutional neural
    network based on power spectrum feature selection,” *Proceedings of Science*,
    2017.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] W. Liu, H. Jiang, 和 Y. Lu，“基于功率谱特征选择的卷积神经网络脑电信号分析”，*科学论文集*，2017。'
- en: '[91] F. Wang, S.-h. Zhong, J. Peng, J. Jiang, and Y. Liu, “Data augmentation
    for eeg-based emotion recognition with deep convolutional neural networks,” in
    *International Conference on Multimedia Modeling*, 2018, pp. 82–93.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] F. Wang, S.-h. Zhong, J. Peng, J. Jiang, 和 Y. Liu，“基于深度卷积神经网络的脑电图情感识别的数据增强”，见于
    *国际多媒体建模大会*，2018，页 82–93。'
- en: '[92] J. Li, Z. Zhang, and H. He, “Hierarchical convolutional neural networks
    for eeg-based emotion recognition,” *Cognitive Computation*, pp. 1–13, 2017.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Li, Z. Zhang, 和 H. He，“基于脑电图的情感识别的层次卷积神经网络”，*认知计算*，页 1–13，2017。'
- en: '[93] K. Wang, Y. Zhao, Q. Xiong, M. Fan, G. Sun, L. Ma, and T. Liu, “Research
    on healthy anomaly detection model based on deep learning from multiple time-series
    physiological signals,” *Scientific Programming*, vol. 2016, 2016.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] K. Wang, Y. Zhao, Q. Xiong, M. Fan, G. Sun, L. Ma, 和 T. Liu，“基于深度学习的多时间序列生理信号的健康异常检测模型研究”，*科学编程*，第
    2016 卷，2016。'
- en: '[94] X. Chai, Q. Wang, Y. Zhao, X. Liu, O. Bai, and Y. Li, “Unsupervised domain
    adaptation techniques based on auto-encoder for non-stationary eeg-based emotion
    recognition,” *Computers in biology and medicine*, vol. 79, pp. 205–214, 2016.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] X. Chai, Q. Wang, Y. Zhao, X. Liu, O. Bai, 和 Y. Li，“基于自编码器的无监督领域适应技术用于非平稳脑电图情感识别”，*生物医学计算机*，第
    79 卷，页 205–214，2016。'
- en: '[95] W. Liu, W.-L. Zheng, and B.-L. Lu, “Emotion recognition using multimodal
    deep learning,” in *International Conference on Neural Information Processing*,
    2016, pp. 521–529.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] W. Liu, W.-L. Zheng, 和 B.-L. Lu，“使用多模态深度学习进行情感识别”，见于 *国际神经信息处理大会*，2016，页
    521–529。'
- en: '[96] W.-L. Zheng and B.-L. Lu, “Investigating critical frequency bands and
    channels for eeg-based emotion recognition with deep neural networks,” *IEEE Transactions
    on Autonomous Mental Development*, vol. 7, no. 3, pp. 162–175, 2015.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] W.-L. Zheng 和 B.-L. Lu, “使用深度神经网络研究基于 EEG 的情感识别的关键频率带和通道，” *IEEE 自主心理发展期刊*，第
    7 卷，第 3 期，第 162–175 页，2015年。'
- en: '[97] W.-L. Zheng, H.-T. Guo, and B.-L. Lu, “Revealing critical channels and
    frequency bands for emotion recognition from eeg with deep belief network,” in
    *Neural Engineering (NER), 2015 7th International IEEE/EMBS Conference on*, 2015,
    pp. 154–157.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] W.-L. Zheng, H.-T. Guo, 和 B.-L. Lu, “揭示使用深度信念网络从 EEG 中识别情感的关键通道和频率带，”
    在 *神经工程 (NER), 2015 第七届国际 IEEE/EMBS 会议*，2015年，第 154–157 页。'
- en: '[98] X. Jia, K. Li, X. Li, and A. Zhang, “A novel semi-supervised deep learning
    framework for affective state recognition on eeg signals,” in *IEEE International
    Conference on Bioinformatics and Bioengineering (BIBE)*, 2014, pp. 30–37.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] X. Jia, K. Li, X. Li, 和 A. Zhang, “用于 EEG 信号情感状态识别的新型半监督深度学习框架，” 在 *IEEE
    国际生物信息学与生物工程会议 (BIBE)*，2014年，第 30–37 页。'
- en: '[99] H. Xu and K. N. Plataniotis, “Affective states classification using eeg
    and semi-supervised deep learning approaches,” in *Multimedia Signal Processing
    (MMSP), 2016 IEEE 18th International Workshop on*, 2016, pp. 1–6.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] H. Xu 和 K. N. Plataniotis, “使用 EEG 和半监督深度学习方法的情感状态分类，” 在 *多媒体信号处理 (MMSP),
    2016 IEEE 第十八届国际研讨会*，2016年，第 1–6 页。'
- en: '[100] X. Li, P. Zhang, D. Song, G. Yu, Y. Hou, and B. Hu, “Eeg based emotion
    identification using unsupervised deep feature learning,” 2015.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Li, P. Zhang, D. Song, G. Yu, Y. Hou, 和 B. Hu, “基于 EEG 的情感识别使用无监督深度特征学习，”
    2015年。'
- en: '[101] H. Xu and K. N. Plataniotis, “Eeg-based affect states classification
    using deep belief networks,” in *Digital Media Industry & Academic Forum (DMIAF)*,
    2016, pp. 148–153.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] H. Xu 和 K. N. Plataniotis, “基于 EEG 的情感状态分类使用深度信念网络，” 在 *数字媒体产业与学术论坛 (DMIAF)*，2016年，第
    148–153 页。'
- en: '[102] W.-L. Zheng, J.-Y. Zhu, Y. Peng, and B.-L. Lu, “Eeg-based emotion classification
    using deep belief networks,” in *Multimedia and Expo (ICME), 2014 IEEE International
    Conference on*, 2014, pp. 1–6.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] W.-L. Zheng, J.-Y. Zhu, Y. Peng, 和 B.-L. Lu, “基于 EEG 的情感分类使用深度信念网络，”
    在 *多媒体与展览 (ICME), 2014 IEEE 国际会议*，2014年，第 1–6 页。'
- en: '[103] K. Li, X. Li, Y. Zhang, and A. Zhang, “Affective state recognition from
    eeg with deep belief networks,” in *2013 IEEE International Conference on Bioinformatics
    and Biomedicine*, 2013, pp. 305–310.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] K. Li, X. Li, Y. Zhang, 和 A. Zhang, “使用深度信念网络的 EEG 情感状态识别，” 在 *2013 IEEE
    国际生物信息学与生物医学会议*，2013年，第 305–310 页。'
- en: '[104] J. A. Mioranda-Correa and I. Patras, “A multi-task cascaded network for
    prediction of affect, personality, mood and social context using eeg signals,”
    in *Automatic Face & Gesture Recognition (FG 2018), 2018 13th IEEE International
    Conference on*, 2018, pp. 373–380.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. A. Mioranda-Correa 和 I. Patras, “用于预测情感、个性、情绪和社会背景的多任务级联网络，” 在 *自动面部与手势识别
    (FG 2018), 2018 第十三届 IEEE 国际会议*，2018年，第 373–380 页。'
- en: '[105] P. Kawde and G. K. Verma, “Deep belief network based affect recognition
    from physiological signals,” in *Electrical, Computer and Electronics (UPCON),
    2017 4th IEEE Uttar Pradesh Section International Conference on*, 2017, pp. 587–592.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] P. Kawde 和 G. K. Verma, “基于深度信念网络的生理信号情感识别，” 在 *电气、计算机与电子 (UPCON), 2017
    第四届 IEEE 北方邦分部国际会议*，2017年，第 587–592 页。'
- en: '[106] Y. Gao, H. J. Lee, and R. M. Mehmood, “Deep learninig of eeg signals
    for emotion recognition,” in *Multimedia & Expo Workshops (ICMEW), 2015 IEEE International
    Conference on*, 2015, pp. 1–5.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Gao, H. J. Lee, 和 R. M. Mehmood, “情感识别的 EEG 信号深度学习，” 在 *Multimedia
    & Expo Workshops (ICMEW), 2015 IEEE 国际会议*，2015年，第 1–5 页。'
- en: '[107] Z. Yin, M. Zhao, Y. Wang, J. Yang, and J. Zhang, “Recognition of emotions
    using multimodal physiological signals and an ensemble deep learning model,” *Computer
    methods and programs in biomedicine*, vol. 140, pp. 93–110, 2017.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Z. Yin, M. Zhao, Y. Wang, J. Yang, 和 J. Zhang, “使用多模态生理信号和集成深度学习模型的情感识别，”
    *计算机方法与生物医学程序*，第 140 卷，第 93–110 页，2017年。'
- en: '[108] S. Alhagry, A. A. Fahmy, and R. A. El-Khoribi, “Emotion recognition based
    on eeg using lstm recurrent neural network,” *Emotion*, vol. 8, no. 10, 2017.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] S. Alhagry, A. A. Fahmy, 和 R. A. El-Khoribi, “基于 EEG 的情感识别使用 LSTM 循环神经网络，”
    *情感*，第 8 卷，第 10 期，2017年。'
- en: '[109] Y. Yuan, G. Xun, F. Ma, Q. Suo, H. Xue, K. Jia, and A. Zhang, “A novel
    channel-aware attention framework for multi-channel eeg seizure detection via
    multi-view deep learning,” in *International Conference on Biomedical & Health
    Informatics (BHI)*, 2018, pp. 206–209.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Y. Yuan, G. Xun, F. Ma, Q. Suo, H. Xue, K. Jia, 和 A. Zhang，"一种新颖的通道感知注意力框架用于多通道
    EEG 癫痫检测，采用多视图深度学习"，发表于 *国际生物医学与健康信息学会议（BHI）*，2018，页码 206–209。'
- en: '[110] S. S. Talathi, “Deep recurrent neural networks for seizure detection
    and early seizure detection systems,” *arXiv preprint arXiv:1706.03283*, 2017.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] S. S. Talathi，"用于癫痫检测和早期癫痫检测系统的深度递归神经网络"，*arXiv 预印本 arXiv:1706.03283*，2017。'
- en: '[111] G. Ruffini, D. Ibañez, M. Castellano, S. Dunne, and A. Soria-Frisch,
    “Eeg-driven rnn classification for prognosis of neurodegeneration in at-risk patients,”
    in *International Conference on Artificial Neural Networks*, 2016, pp. 306–313.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] G. Ruffini, D. Ibañez, M. Castellano, S. Dunne, 和 A. Soria-Frisch，"基于
    EEG 的 RNN 分类用于评估有风险患者的神经退行性病"，发表于 *国际人工神经网络会议*，2016，页码 306–313。'
- en: '[112] I. Ullah, M. Hussain, H. Aboalsamh *et al.*, “An automated system for
    epilepsy detection using eeg brain signals based on deep learning approach,” *Expert
    Systems with Applications*, vol. 107, pp. 61–71, 2018.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] I. Ullah, M. Hussain, H. Aboalsamh *等*，"基于深度学习方法的 EEG 脑信号自动癫痫检测系统"，*应用专家系统*，卷
    107，页码 61–71，2018。'
- en: '[113] U. R. Acharya, S. L. Oh, Y. Hagiwara, J. H. Tan, H. Adeli, and D. P.
    Subha, “Automated eeg-based screening of depression using deep convolutional neural
    network,” *Computer methods and programs in biomedicine*, vol. 161, pp. 103–113,
    2018.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] U. R. Acharya, S. L. Oh, Y. Hagiwara, J. H. Tan, H. Adeli, 和 D. P. Subha，"基于深度卷积神经网络的自动化
    EEG 抑郁症筛查"，*生物医学计算方法与程序*，卷 161，页码 103–113，2018。'
- en: '[114] U. R. Acharya, S. L. Oh, Y. Hagiwara, J. H. Tan, and H. Adeli, “Deep
    convolutional neural network for the automated detection and diagnosis of seizure
    using eeg signals,” *Computers in biology and medicine*, vol. 100, pp. 270–278,
    2018.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] U. R. Acharya, S. L. Oh, Y. Hagiwara, J. H. Tan, 和 H. Adeli，"基于 EEG 信号的自动癫痫检测和诊断的深度卷积神经网络"，*生物医学与医学计算机*，卷
    100，页码 270–278，2018。'
- en: '[115] F. C. Morabito, M. Campolo, C. Ieracitano, J. M. Ebadi, L. Bonanno, A. Bramanti,
    S. Desalvo, N. Mammone, and P. Bramanti, “Deep convolutional neural networks for
    classification of mild cognitive impaired and alzheimer’s disease patients from
    scalp eeg recordings,” in *Research and Technologies for Society and Industry
    Leveraging a better tomorrow (RTSI), 2016 IEEE 2nd International Forum on*, 2016,
    pp. 1–6.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] F. C. Morabito, M. Campolo, C. Ieracitano, J. M. Ebadi, L. Bonanno, A.
    Bramanti, S. Desalvo, N. Mammone, 和 P. Bramanti，"用于从头皮 EEG 记录中分类轻度认知障碍和阿尔茨海默病患者的深度卷积神经网络"，发表于
    *研究与技术服务于社会和工业，展望更美好的明天（RTSI），2016 年 IEEE 第二届国际论坛*，2016，页码 1–6。'
- en: '[116] R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball,
    “Deep learning with convolutional neural networks for decoding and visualization
    of eeg pathology,” in *Signal Processing in Medicine and Biology Symposium (SPMB),
    2017 IEEE*, 2017, pp. 1–7.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, 和 T. Ball，"利用卷积神经网络进行
    EEG 病理解码和可视化的深度学习"，发表于 *医学和生物学信号处理研讨会（SPMB），2017 年 IEEE*，2017，页码 1–7。'
- en: '[117] M.-P. Hosseini, T. X. Tran, D. Pompili, K. Elisevich, and H. Soltanian-Zadeh,
    “Deep learning with edge computing for localization of epileptogenicity using
    multimodal rs-fmri and eeg big data,” in *Autonomic Computing (ICAC), 2017 IEEE
    International Conference on*, 2017, pp. 83–92.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] M.-P. Hosseini, T. X. Tran, D. Pompili, K. Elisevich, 和 H. Soltanian-Zadeh，"结合边缘计算的深度学习用于利用多模态
    rs-fMRI 和 EEG 大数据定位癫痫原性"，发表于 *自适应计算（ICAC），2017 年 IEEE 国际会议*，2017，页码 83–92。'
- en: '[118] A. R. Johansen, J. Jin, T. Maszczyk, J. Dauwels, S. S. Cash, and M. B.
    Westover, “Epileptiform spike detection via convolutional neural networks,” in
    *IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2016, pp. 754–758.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] A. R. Johansen, J. Jin, T. Maszczyk, J. Dauwels, S. S. Cash, 和 M. B.
    Westover，"通过卷积神经网络检测癫痫样尖峰"，发表于 *IEEE 国际声学、语音和信号处理会议（ICASSP）*，2016，页码 754–758。'
- en: '[119] A. H. Ansari, P. J. Cherian, A. Caicedo, G. Naulaers, M. De Vos, and
    S. Van Huffel, “Neonatal seizure detection using deep convolutional neural networks,”
    *International journal of neural systems*, p. 1850011, 2018.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. H. Ansari, P. J. Cherian, A. Caicedo, G. Naulaers, M. De Vos, 和 S.
    Van Huffel，"利用深度卷积神经网络进行新生儿癫痫检测"，*国际神经系统期刊*，页码 1850011，2018。'
- en: '[120] M.-P. Hosseini, D. Pompili, K. Elisevich, and H. Soltanian-Zadeh, “Optimized
    deep learning for eeg big data and seizure prediction bci via internet of things,”
    *IEEE Transactions on Big Data*, vol. 3, no. 4, pp. 392–404, 2017.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] M.-P. Hosseini, D. Pompili, K. Elisevich 和 H. Soltanian-Zadeh, “针对 EEG
    大数据和通过物联网进行癫痫发作预测的优化深度学习，” *IEEE 大数据汇刊*，第3卷，第4期，第392–404页，2017年。'
- en: '[121] Y. Yuan, G. Xun, K. Jia, and A. Zhang, “A novel wavelet-based model for
    eeg epileptic seizure detection using multi-context learning,” in *IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM)*, 2017, pp. 694–699.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y. Yuan, G. Xun, K. Jia 和 A. Zhang, “一种基于小波的新型模型，用于多上下文学习的 EEG 癫痫发作检测，”
    *IEEE 国际生物信息学与生物医学会议 (BIBM)*，2017年，第694–699页。'
- en: '[122] Q. Lin, S.-q. Ye, X.-m. Huang, S.-y. Li, M.-z. Zhang, Y. Xue, and W.-S.
    Chen, “Classification of epileptic eeg signals with stacked sparse autoencoder
    based on deep learning,” in *International Conference on Intelligent Computing*,
    2016, pp. 802–810.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Q. Lin, S.-q. Ye, X.-m. Huang, S.-y. Li, M.-z. Zhang, Y. Xue 和 W.-S.
    Chen, “基于深度学习的堆叠稀疏自编码器在癫痫 EEG 信号分类中的应用，” *国际智能计算会议*，2016年，第802–810页。'
- en: '[123] F. C. Morabito, M. Campolo, N. Mammone, M. Versaci, S. Franceschetti,
    F. Tagliavini, V. Sofia, D. Fatuzzo, A. Gambardella, A. Labate *et al.*, “Deep
    learning representation from electroencephalography of early-stage creutzfeldt-jakob
    disease and features for differentiation from rapidly progressive dementia,” *International
    journal of neural systems*, vol. 27, no. 02, p. 1650039, 2017.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] F. C. Morabito, M. Campolo, N. Mammone, M. Versaci, S. Franceschetti,
    F. Tagliavini, V. Sofia, D. Fatuzzo, A. Gambardella, A. Labate *等*，“早期克雅二氏病的脑电图深度学习表示及与快速进展型痴呆的区分特征，”
    *国际神经系统期刊*，第27卷，第02期，第1650039页，2017年。'
- en: '[124] T. Wen and Z. Zhang, “Deep convolution neural network and autoencoders-based
    unsupervised feature learning of eeg signals,” *IEEE Access*, vol. 6, pp. 25 399–25 410,
    2018.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] T. Wen 和 Z. Zhang, “基于深度卷积神经网络和自编码器的 EEG 信号无监督特征学习，” *IEEE 访问*，第6卷，第25,399–25,410页，2018年。'
- en: '[125] A. Page, J. Turner, T. Mohsenin, and T. Oates, “Comparing raw data and
    feature extraction for seizure detection with deep learning methods.” in *FLAIRS
    Conference*, 2014.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] A. Page, J. Turner, T. Mohsenin 和 T. Oates, “比较原始数据与特征提取在深度学习方法下的癫痫检测。”
    *FLAIRS 会议*，2014年。'
- en: '[126] Y. Zhao and L. He, “Deep learning in the eeg diagnosis of alzheimer’s
    disease,” in *Asian Conference on Computer Vision*, 2014, pp. 340–353.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Y. Zhao 和 L. He, “深度学习在阿尔茨海默病 EEG 诊断中的应用，” *亚洲计算机视觉会议*，2014年，第340–353页。'
- en: '[127] J. Turner, A. Page, T. Mohsenin, and T. Oates, “Deep belief networks
    used on high resolution multichannel electroencephalography data for seizure detection,”
    in *2014 AAAI Spring Symposium Series*, 2014.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Turner, A. Page, T. Mohsenin 和 T. Oates, “在高分辨率多通道脑电图数据上使用深度置信网络进行癫痫发作检测，”
    *2014 AAAI 春季研讨会系列*，2014年。'
- en: '[128] V. Shah, M. Golmohammadi, S. Ziyabari, E. Von Weltin, I. Obeid, and J. Picone,
    “Optimizing channel selection for seizure detection,” in *Signal Processing in
    Medicine and Biology Symposium (SPMB), 2017 IEEE*, 2017, pp. 1–5.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] V. Shah, M. Golmohammadi, S. Ziyabari, E. Von Weltin, I. Obeid 和 J. Picone,
    “优化癫痫发作检测的通道选择，” *2017 IEEE 医学与生物学信号处理研讨会 (SPMB)*，2017年，第1–5页。'
- en: '[129] M.-P. Hosseini, H. Soltanian-Zadeh, K. Elisevich, and D. Pompili, “Cloud-based
    deep learning of big eeg data for epileptic seizure prediction,” *arXiv preprint
    arXiv:1702.05192*, 2017.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] M.-P. Hosseini, H. Soltanian-Zadeh, K. Elisevich 和 D. Pompili, “基于云的深度学习大规模
    EEG 数据用于癫痫发作预测，” *arXiv 预印本 arXiv:1702.05192*，2017年。'
- en: '[130] M. Golmohammadi, S. Ziyabari, V. Shah, S. L. de Diego, I. Obeid, and
    J. Picone, “Deep architectures for automated seizure detection in scalp eegs,”
    *arXiv preprint arXiv:1712.09776*, 2017.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] M. Golmohammadi, S. Ziyabari, V. Shah, S. L. de Diego, I. Obeid 和 J.
    Picone, “用于头皮 EEG 自动化癫痫发作检测的深度结构，” *arXiv 预印本 arXiv:1712.09776*，2017年。'
- en: '[131] A. M. Al-kaysi, A. Al-Ani, and T. W. Boonstra, “A multichannel deep belief
    network for the classification of eeg data,” in *International Conference on Neural
    Information Processing*, 2015, pp. 38–45.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] A. M. Al-kaysi, A. Al-Ani 和 T. W. Boonstra, “用于 EEG 数据分类的多通道深度置信网络，”
    *国际神经信息处理会议*，2015年，第38–45页。'
- en: '[132] S. M. Abdelfattah, G. M. Abdelrahman, and M. Wang, “Augmenting the size
    of eeg datasets using generative adversarial networks,” in *2018 International
    Joint Conference on Neural Networks (IJCNN)*, 2018, pp. 1–6.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] S. M. Abdelfattah, G. M. Abdelrahman 和 M. Wang, “利用生成对抗网络扩增 EEG 数据集的大小，”
    *2018 国际联合神经网络会议 (IJCNN)*，2018年，第1–6页。'
- en: '[133] S. Palazzo, C. Spampinato, I. Kavasidis, D. Giordano, and M. Shah, “Generative
    adversarial networks conditioned by brain signals,” in *Proceedings of the IEEE
    International Conference on Computer Vision*, 2017, pp. 3410–3418.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] S. Palazzo, C. Spampinato, I. Kavasidis, D. Giordano, 和 M. Shah，“由脑信号条件生成对抗网络，”
    在 *IEEE国际计算机视觉会议论文集*，2017年，第3410–3418页。'
- en: '[134] P. S. S. C. G. D. S. M. Kavasidis, I., “Brain2image: Converting brain
    signals into images,” in *Proceedings of the 25th ACM international conference
    on Multimedia*, 2017, pp. 1809–1817.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] P. S. S. C. G. D. S. M. Kavasidis, I.，“Brain2image：将脑信号转换为图像，” 在 *第25届ACM国际多媒体会议论文集*，2017年，第1809–1817页。'
- en: '[135] J. Teo, C. L. Hou, and J. Mountstephens, “Deep learning for eeg-based
    preference classification,” in *AIP Conference Proceedings*, vol. 1891, no. 1,
    2017, p. 020141.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] J. Teo, C. L. Hou, 和 J. Mountstephens，“基于脑电图的偏好分类的深度学习，” 在 *AIP会议论文集*，第1891卷，第1期，2017年，第020141页。'
- en: '[136] T. K. Reddy and L. Behera, “Online eye state recognition from eeg data
    using deep architectures,” in *Systems, Man, and Cybernetics (SMC), 2016 IEEE
    International Conference on*, 2016, pp. 000 712–000 717.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] T. K. Reddy 和 L. Behera，“利用深度架构从脑电图数据中在线识别眼状态，” 在 *系统，人机与控制 (SMC)，2016年IEEE国际会议*，2016年，第000
    712–000 717页。'
- en: '[137] A. J. Yepes, J. Tang, and B. S. Mashford, “Improving classification accuracy
    of feedforward neural networks for spiking neuromorphic chips,” *arXiv preprint
    arXiv:1705.07755*, 2017.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. J. Yepes, J. Tang, 和 B. S. Mashford，“提高前馈神经网络对脉冲神经形态芯片的分类准确性，” *arXiv预印本
    arXiv:1705.07755*，2017年。'
- en: '[138] J. Shang, W. Zhang, J. Xiong, and Q. Liu, “Cognitive load recognition
    using multi-channel complex network method,” in *International Symposium on Neural
    Networks*, 2017, pp. 466–474.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] J. Shang, W. Zhang, J. Xiong, 和 Q. Liu，“使用多通道复杂网络方法的认知负荷识别，” 在 *国际神经网络研讨会*，2017年，第466–474页。'
- en: '[139] J. Behncke, R. T. Schirrmeister, W. Burgard, and T. Ball, “The signature
    of robot action success in eeg signals of a human observer: Decoding and visualization
    using deep convolutional neural networks,” in *International Conference on Brain-Computer
    Interface (BCI) 2018*, 2018, pp. 1–6.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] J. Behncke, R. T. Schirrmeister, W. Burgard, 和 T. Ball，“机器人动作成功的脑电图信号特征：使用深度卷积神经网络解码和可视化，”
    在 *2018年脑-计算机接口 (BCI) 国际会议*，2018年，第1–6页。'
- en: '[140] Y.-C. Hung, Y.-K. Wang, M. Prasad, and C.-T. Lin, “Brain dynamic states
    analysis based on 3d convolutional neural network,” in *Systems, Man, and Cybernetics
    (SMC), 2017 IEEE International Conference on*, 2017, pp. 222–227.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Y.-C. Hung, Y.-K. Wang, M. Prasad, 和 C.-T. Lin，“基于3D卷积神经网络的脑动态状态分析，”
    在 *系统，人机与控制 (SMC)，2017年IEEE国际会议*，2017年，第222–227页。'
- en: '[141] V. Baltatzis, K.-M. Bintsi, G. K. Apostolidis, and L. J. Hadjileontiadis,
    “Bullying incidences identification within an immersive environment using hd eeg-based
    analysis: A swarm decomposition and deep learning approach,” *Scientific reports*,
    vol. 7, no. 1, p. 17292, 2017.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] V. Baltatzis, K.-M. Bintsi, G. K. Apostolidis, 和 L. J. Hadjileontiadis，“在沉浸式环境中利用高清脑电图分析识别欺凌事件：一种群体分解和深度学习方法，”
    *科学报告*，第7卷，第1期，第17292页，2017年。'
- en: '[142] S. Stober, D. J. Cameron, and J. A. Grahn, “Classifying eeg recordings
    of rhythm perception.” in *ISMIR*, 2014, pp. 649–654.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] S. Stober, D. J. Cameron, 和 J. A. Grahn，“节奏感知脑电图记录的分类，” 在 *ISMIR*，2014年，第649–654页。'
- en: '[143] M. Völker, R. T. Schirrmeister, L. D. Fiederer, W. Burgard, and T. Ball,
    “Deep transfer learning for error decoding from non-invasive eeg,” in *Brain-Computer
    Interface (BCI), 2018 6th International Conference on*, 2018, pp. 1–6.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] M. Völker, R. T. Schirrmeister, L. D. Fiederer, W. Burgard, 和 T. Ball，“用于非侵入性脑电图的错误解码的深度迁移学习，”
    在 *脑-计算机接口 (BCI) 2018年第六届国际会议*，2018年，第1–6页。'
- en: '[144] L. G. Hernández, O. M. Mozos, J. M. Ferrández, and J. M. Antelis, “Eeg-based
    detection of braking intention under different car driving conditions,” *Frontiers
    in neuroinformatics*, vol. 12, 2018.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] L. G. Hernández, O. M. Mozos, J. M. Ferrández, 和 J. M. Antelis，“基于脑电图的制动意图检测在不同驾驶条件下的研究，”
    *神经信息学前沿*，第12卷，2018年。'
- en: '[145] M. A. Almogbel, A. H. Dang, and W. Kameyama, “Eeg-signals based cognitive
    workload detection of vehicle driver using deep learning,” in *Advanced Communication
    Technology (ICACT), 2018 20th International Conference on*, 2018, pp. 256–259.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. A. Almogbel, A. H. Dang, 和 W. Kameyama，“基于脑电图信号的车辆驾驶员认知负荷检测的深度学习，”
    在 *高级通信技术 (ICACT)，2018年第20届国际会议*，2018年，第256–259页。'
- en: '[146] M. J. Putten, S. Olbrich, and M. Arns, “Predicting sex from brain rhythms
    with deep learning,” *Scientific reports*, vol. 8, no. 1, p. 3069, 2018.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] M. J. Putten, S. Olbrich, 和 M. Arns，“通过深度学习预测脑节律中的性别，” *科学报告*，第8卷，第1期，第3069页，2018年。'
- en: '[147] M. Hajinoroozi, Z. Mao, and Y. Huang, “Prediction of driver’s drowsy
    and alert states from eeg signals with deep learning,” in *Computational Advances
    in Multi-Sensor Adaptive Processing (CAMSAP), 2015 IEEE 6th International Workshop
    on*, 2015, pp. 493–496.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] M. Hajinoroozi, Z. Mao, 和 Y. Huang，“利用深度学习从 EEG 信号中预测驾驶员的困倦和警觉状态，”发表于
    *计算多传感器自适应处理进展（CAMSAP），2015 IEEE 第六届国际研讨会*，2015年，第493–496页。'
- en: '[148] A. Sternin, S. Stober, J. Grahn, and A. Owen, “Tempo estimation from
    the eeg signal during perception and imagination of music,” in *International
    Symposium on Computer Music Multidisciplinary Research*, 2015.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Sternin, S. Stober, J. Grahn, 和 A. Owen，“从 EEG 信号中估计节拍在音乐感知和想象中的表现，”发表于
    *国际计算机音乐多学科研究研讨会*，2015年。'
- en: '[149] L. Chu, R. Qiu, H. Liu, Z. Ling, T. Zhang, and J. Wang, “Individual recognition
    in schizophrenia using deep learning methods with random forest and voting classifiers:
    Insights from resting state eeg streams,” *arXiv preprint arXiv:1707.03467*, 2017.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] L. Chu, R. Qiu, H. Liu, Z. Ling, T. Zhang, 和 J. Wang，“使用深度学习方法结合随机森林和投票分类器在精神分裂症中的个体识别：基于静息状态
    EEG 流的见解，”*arXiv 预印本 arXiv:1707.03467*，2017年。'
- en: '[150] Z. Yin and J. Zhang, “Cross-session classification of mental workload
    levels using eeg and an adaptive deep learning model,” *Biomedical Signal Processing
    and Control*, vol. 33, pp. 30–47, 2017.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Z. Yin 和 J. Zhang，“使用 EEG 和自适应深度学习模型进行跨会话心理负荷水平分类，”*生物医学信号处理与控制*，第33卷，第30–47页，2017年。'
- en: '[151] L.-H. Du, W. Liu, W.-L. Zheng, and B.-L. Lu, “Detecting driving fatigue
    with multimodal deep learning,” in *Neural Engineering (NER), 2017 8th International
    IEEE/EMBS Conference on*, 2017, pp. 74–77.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] L.-H. Du, W. Liu, W.-L. Zheng, 和 B.-L. Lu，“通过多模态深度学习检测驾驶疲劳，”发表于 *神经工程（NER），2017
    第八届国际 IEEE/EMBS 会议*，2017年，第74–77页。'
- en: '[152] S. Narejo, E. Pasero, and F. Kulsoom, “Eeg based eye state classification
    using deep belief network and stacked autoencoder,” *International Journal of
    Electrical and Computer Engineering (IJECE)*, vol. 6, no. 6, pp. 3131–3141, 2016.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] S. Narejo, E. Pasero, 和 F. Kulsoom，“基于 EEG 的眼睛状态分类，使用深度信念网络和堆叠自编码器，”*国际电气与计算机工程学报（IJECE）*，第6卷，第6期，第3131–3141页，2016年。'
- en: '[153] M. Hajinoroozi, T.-P. Jung, C.-T. Lin, and Y. Huang, “Feature extraction
    with deep belief networks for driver’s cognitive states prediction from eeg data,”
    in *Signal and Information Processing (ChinaSIP), 2015 IEEE China Summit and International
    Conference on*, 2015, pp. 812–815.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] M. Hajinoroozi, T.-P. Jung, C.-T. Lin, 和 Y. Huang，“利用深度信念网络进行特征提取，以预测驾驶员的认知状态，”发表于
    *信号与信息处理（ChinaSIP），2015 IEEE 中国峰会与国际会议*，2015年，第812–815页。'
- en: '[154] P. P. San, S. H. Ling, R. Chai, Y. Tran, A. Craig, and H. Nguyen, “Eeg-based
    driver fatigue detection using hybrid deep generic model,” in *Engineering in
    Medicine and Biology Society (EMBC), 2016 IEEE 38th Annual International Conference
    of the*, 2016, pp. 800–803.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] P. P. San, S. H. Ling, R. Chai, Y. Tran, A. Craig, 和 H. Nguyen，“基于 EEG
    的驾驶员疲劳检测，使用混合深度通用模型，”发表于 *医学与生物学工程学会（EMBC），2016 IEEE 第38届国际年会*，2016年，第800–803页。'
- en: '[155] P. Li, W. Jiang, and F. Su, “Single-channel eeg-based mental fatigue
    detection based on deep belief network,” in *Engineering in Medicine and Biology
    Society (EMBC), 2016 IEEE 38th Annual International Conference of the*, 2016,
    pp. 367–370.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] P. Li, W. Jiang, 和 F. Su，“基于单通道 EEG 的精神疲劳检测，基于深度信念网络，”发表于 *医学与生物学工程学会（EMBC），2016
    IEEE 第38届国际年会*，2016年，第367–370页。'
- en: '[156] P. Zhang, X. Wang, W. Zhang, and J. Chen, “Learning spatial–spectral–temporal
    eeg features with recurrent 3d convolutional neural networks for cross-task mental
    workload assessment,” *IEEE Transactions on neural systems and rehabilitation
    engineering*, vol. 27, no. 1, pp. 31–42, 2018.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] P. Zhang, X. Wang, W. Zhang, 和 J. Chen，“利用递归 3D 卷积神经网络学习空间–光谱–时间 EEG
    特征，以评估跨任务的心理负荷，”*IEEE 神经系统与康复工程学报*，第27卷，第1期，第31–42页，2018年。'
- en: '[157] S. Stober, A. Sternin, A. M. Owen, and J. A. Grahn, “Deep feature learning
    for eeg recordings,” *arXiv preprint arXiv:1511.04306*, 2015.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] S. Stober, A. Sternin, A. M. Owen, 和 J. A. Grahn，“用于 EEG 记录的深度特征学习，”*arXiv
    预印本 arXiv:1511.04306*，2015年。'
- en: '[158] R. Chai, S. H. Ling, P. P. San, G. R. Naik, T. N. Nguyen, Y. Tran, A. Craig,
    and H. T. Nguyen, “Improving eeg-based driver fatigue classification using sparse-deep
    belief networks,” *Frontiers in neuroscience*, vol. 11, p. 103, 2017.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] R. Chai, S. H. Ling, P. P. San, G. R. Naik, T. N. Nguyen, Y. Tran, A.
    Craig, 和 H. T. Nguyen，“使用稀疏深度信念网络改善基于 EEG 的驾驶员疲劳分类，”*神经科学前沿*，第11卷，第103页，2017年。'
- en: '[159] P. Bashivan, M. Yeasin, and G. M. Bidelman, “Single trial prediction
    of normal and excessive cognitive load through eeg feature fusion,” in *Signal
    Processing in Medicine and Biology Symposium (SPMB), 2015 IEEE*, 2015, pp. 1–5.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] P. Bashivan, M. Yeasin, 和 G. M. Bidelman, “通过脑电图特征融合预测正常和过度的认知负荷，” 载于
    *医学与生物学信号处理研讨会（SPMB），2015年IEEE*，2015年，第1–5页。'
- en: '[160] X. Zhang, L. Yao, C. Huang, S. S. Kanhere, and D. Zhang, “Brain2object:
    Printing your mind from brain signals with spatial correlation embedding,” *arXiv
    preprint arXiv:1810.02223*, 2018.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] X. Zhang, L. Yao, C. Huang, S. S. Kanhere, 和 D. Zhang, “Brain2object：从脑信号中打印你的思想，结合空间相关嵌入，”
    *arXiv预印本arXiv:1810.02223*，2018年。'
- en: '[161] T. Koike-Akino, R. Mahajan, T. K. Marks, Y. Wang, S. Watanabe, O. Tuzel,
    and P. Orlik, “High-accuracy user identification using eeg biometrics,” in *2016
    38th Annual International Conference of the IEEE Engineering in Medicine and Biology
    Society (EMBC)*, 2016, pp. 854–858.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] T. Koike-Akino, R. Mahajan, T. K. Marks, Y. Wang, S. Watanabe, O. Tuzel,
    和 P. Orlik, “使用脑电图生物特征进行高精度用户识别，” 载于 *2016年第38届IEEE医学与生物工程学会年会（EMBC）*，2016年，第854–858页。'
- en: '[162] K. Kawasaki, T. Yoshikawa, and T. Furuhashi, “Visualizing extracted feature
    by deep learning in p300 discrimination task,” in *Soft Computing and Pattern
    Recognition (SoCPaR), 2015 7th International Conference of*, 2015, pp. 149–154.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] K. Kawasaki, T. Yoshikawa, 和 T. Furuhashi, “通过深度学习可视化P300区分任务中的提取特征，”
    载于 *软计算与模式识别（SoCPaR），2015年第七届国际会议*，2015年，第149–154页。'
- en: '[163] C. Spampinato, S. Palazzo, I. Kavasidis, D. Giordano, N. Souly, and M. Shah,
    “Deep learning human mind for automated visual classification,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2017, pp.
    6809–6817.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] C. Spampinato, S. Palazzo, I. Kavasidis, D. Giordano, N. Souly, 和 M. Shah,
    “用于自动化视觉分类的深度学习人脑，” 载于 *IEEE计算机视觉与模式识别会议论文集*，2017年，第6809–6817页。'
- en: '[164] M. Liu, W. Wu, Z. Gu, Z. Yu, F. Qi, and Y. Li, “Deep learning based on
    batch normalization for p300 signal detection,” *Neurocomputing*, vol. 275, pp.
    288–297, 2018.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] M. Liu, W. Wu, Z. Gu, Z. Yu, F. Qi, 和 Y. Li, “基于批量归一化的深度学习在P300信号检测中的应用，”
    *神经计算*，第275卷，第288–297页，2018年。'
- en: '[165] S. Sarkar, K. Reddy, A. Dorgan, C. Fidopiastis, and M. Giering, “Wearable
    eeg-based activity recognition in phm-related service environment via deep learning,”
    *Int. J. Progn. Health Manag*, vol. 7, pp. 1–10, 2016.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. Sarkar, K. Reddy, A. Dorgan, C. Fidopiastis, 和 M. Giering, “通过深度学习在PHM相关服务环境中的可穿戴脑电图活动识别，”
    *预测健康管理国际期刊*，第7卷，第1–10页，2016年。'
- en: '[166] H. Cecotti and A. Graser, “Convolutional neural networks for p300 detection
    with application to brain-computer interfaces,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 33, no. 3, pp. 433–445, 2011.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] H. Cecotti 和 A. Graser, “用于P300检测的卷积神经网络及其在脑-机接口中的应用，” *IEEE模式分析与机器智能汇刊*，第33卷，第3期，第433–445页，2011年。'
- en: '[167] W. Gao, J.-a. Guan, J. Gao, and D. Zhou, “Multi-ganglion ann based feature
    learning with application to p300-bci signal classification,” *Biomedical Signal
    Processing and Control*, vol. 18, pp. 127–137, 2015.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] W. Gao, J.-a. Guan, J. Gao, 和 D. Zhou, “基于多神经节的特征学习及其在P300-BCI信号分类中的应用，”
    *生物医学信号处理与控制*，第18卷，第127–137页，2015年。'
- en: '[168] Q. Liu, X.-G. Zhao, Z.-G. Hou, and H.-G. Liu, “Deep belief networks for
    eeg-based concealed information test,” in *International Symposium on Neural Networks*,
    2017, pp. 498–506.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Q. Liu, X.-G. Zhao, Z.-G. Hou, 和 H.-G. Liu, “用于脑电图基础上隐匿信息测试的深度信念网络，”
    载于 *国际神经网络研讨会*，2017年，第498–506页。'
- en: '[169] T. Ma, H. Li, H. Yang, X. Lv, P. Li, T. Liu, D. Yao, and P. Xu, “The
    extraction of motion-onset vep bci features based on deep learning and compressed
    sensing,” *Journal of neuroscience methods*, vol. 275, pp. 80–92, 2017.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] T. Ma, H. Li, H. Yang, X. Lv, P. Li, T. Liu, D. Yao, 和 P. Xu, “基于深度学习和压缩感知的运动起始视觉诱发电位BCI特征提取，”
    *神经科学方法期刊*，第275卷，第80–92页，2017年。'
- en: '[170] R. Maddula, J. Stivers, M. Mousavi, S. Ravindran, and V. de Sa, “Deep
    recurrent convolutional neural networks for classifying p300 bci signals,” in
    *Proceedings of the 7th Graz Brain-Computer Interface Conference, Graz, Austria*,
    2017, pp. 18–22.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] R. Maddula, J. Stivers, M. Mousavi, S. Ravindran, 和 V. de Sa, “用于分类P300
    BCI信号的深度递归卷积神经网络，” 载于 *第七届格拉茨脑-机接口会议论文集，奥地利格拉茨*，2017年，第18–22页。'
- en: '[171] P. Bashivan, I. Rish, M. Yeasin, and N. Codella, “Learning representations
    from eeg with deep recurrent-convolutional neural networks,” *ICLR*, 2016.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] P. Bashivan, I. Rish, M. Yeasin, 和 N. Codella, “通过深度递归卷积神经网络学习脑电图的表示，”
    *ICLR*，2016年。'
- en: '[172] P. Bashivan, I. Rish, and S. Heisig, “Mental state recognition via wearable
    eeg,” *arXiv preprint arXiv:1602.00985*, 2016.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] P. Bashivan, I. Rish, 和 S. Heisig, “通过可穿戴脑电图识别心理状态，” *arXiv预印本arXiv:1602.00985*，2016年。'
- en: '[173] A. Shanbhag, A. P. Kholkar, S. Sawant, A. Vicente, S. Martires, and S. Patil,
    “P300 analysis using deep neural network,” in *2017 International Conference on
    Energy, Communication, Data Analytics and Soft Computing (ICECDS)*, 2017, pp.
    3142–3147.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] A. Shanbhag, A. P. Kholkar, S. Sawant, A. Vicente, S. Martires, 和 S.
    Patil，“利用深度神经网络的P300分析”，发表于*2017国际能源、通信、数据分析与软计算会议（ICECDS）*，2017年，第3142–3147页。'
- en: '[174] Z. Mao, V. Lawhern, L. M. Merino, K. Ball, L. Deng, B. J. Lance, K. Robbins,
    and Y. Huang, “Classification of non-time-locked rapid serial visual presentation
    events for brain-computer interaction using deep learning,” in *Signal and Information
    Processing (ChinaSIP), 2014 IEEE China Summit & International Conference on*,
    2014, pp. 520–524.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Z. Mao, V. Lawhern, L. M. Merino, K. Ball, L. Deng, B. J. Lance, K. Robbins,
    和 Y. Huang，“基于深度学习的非时间锁定快速串行视觉呈现事件的脑机交互分类”，发表于*信号与信息处理（ChinaSIP），2014年IEEE中国峰会及国际会议*，2014年，第520–524页。'
- en: '[175] Z. Mao, “Deep learning for rapid serial visual presentation event from
    electroencephalography signal,” Ph.D. dissertation, The University of Texas at
    San Antonio, 2016.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Z. Mao，“基于脑电图信号的快速串行视觉呈现事件的深度学习”，博士学位论文，德克萨斯大学圣安东尼奥分校，2016年。'
- en: '[176] R. Manor and A. B. Geva, “Convolutional neural network for multi-category
    rapid serial visual presentation bci,” *Frontiers in computational neuroscience*,
    vol. 9, p. 146, 2015.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] R. Manor 和 A. B. Geva，“用于多类别快速串行视觉呈现脑机接口的卷积神经网络”，*计算神经科学前沿*，第9卷，第146页，2015年。'
- en: '[177] H. Cecotti, “Convolutional neural networks for event-related potential
    detection: impact of the architecture,” in *Engineering in Medicine and Biology
    Society (EMBC), 2017 39th Annual International Conference of the IEEE*, 2017,
    pp. 2031–2034.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] H. Cecotti，“用于事件相关电位检测的卷积神经网络：架构的影响”，发表于*医学与生物工程学会（EMBC），2017年第39届IEEE国际年会*，2017年，第2031–2034页。'
- en: '[178] A. J. Solon, S. M. Gordon, B. Lance, and V. Lawhern, “Deep learning approaches
    for p300 classification in image triage: Applications to the nails task,” in *Proceedings
    of the 13th NTCIR Conference on Evaluation of Information Access Technologies,
    NTCIR-13, Tokyo, Japan*, 2017, pp. 5–8.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] A. J. Solon, S. M. Gordon, B. Lance, 和 V. Lawhern，“用于图像筛选的P300分类的深度学习方法：在指甲任务中的应用”，发表于*第13届NTCIR信息获取技术评估会议论文集，NTCIR-13，东京，日本*，2017年，第5–8页。'
- en: '[179] M. Hajinoroozi, Z. Mao, Y.-P. Lin, and Y. Huang, “Deep transfer learning
    for cross-subject and cross-experiment prediction of image rapid serial visual
    presentation events from eeg data,” in *International Conference on Augmented
    Cognition*, 2017, pp. 45–55.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] M. Hajinoroozi, Z. Mao, Y.-P. Lin, 和 Y. Huang，“用于从脑电图数据中预测图像快速串行视觉呈现事件的深度迁移学习”，发表于*增强认知国际会议*，2017年，第45–55页。'
- en: '[180] Z. Mao, W. X. Yao, and Y. Huang, “Eeg-based biometric identification
    with deep learning,” in *Neural Engineering (NER), 2017 8th International IEEE/EMBS
    Conference on*, 2017, pp. 609–612.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Z. Mao, W. X. Yao, 和 Y. Huang，“基于脑电图的深度学习生物识别”，发表于*神经工程（NER），第8届国际IEEE/EMBS会议*，2017年，第609–612页。'
- en: '[181] R. Manor, L. Mishali, and A. B. Geva, “Multimodal neural network for
    rapid serial visual presentation brain computer interface,” *Frontiers in computational
    neuroscience*, vol. 10, p. 130, 2016.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] R. Manor, L. Mishali, 和 A. B. Geva，“用于快速串行视觉呈现脑机接口的多模态神经网络”，*计算神经科学前沿*，第10卷，第130页，2016年。'
- en: '[182] Z. Lin, Y. Zeng, L. Tong, H. Zhang, C. Zhang, and B. Yan, “Method for
    enhancing single-trial p300 detection by introducing the complexity degree of
    image information in rapid serial visual presentation tasks,” *PloS one*, vol. 12,
    no. 12, p. e0184713, 2017.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Z. Lin, Y. Zeng, L. Tong, H. Zhang, C. Zhang, 和 B. Yan，“通过引入图像信息的复杂度提高单次试验P300检测的的方法”，*PloS
    one*，第12卷，第12期，第e0184713页，2017年。'
- en: '[183] S. M. Gordon, M. Jaswa, A. J. Solon, and V. J. Lawhern, “Real world bci:
    cross-domain learning and practical applications,” in *Proceedings of the 2017
    ACM Workshop on An Application-oriented Approach to BCI out of the laboratory*,
    2017, pp. 25–28.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. M. Gordon, M. Jaswa, A. J. Solon, 和 V. J. Lawhern，“现实世界中的脑机接口：跨领域学习和实际应用”，发表于*2017年ACM脑机接口应用导向研讨会论文集*，2017年，第25–28页。'
- en: '[184] J. Yoon, J. Lee, and M. Whang, “Spatial and time domain feature of erp
    speller system extracted via convolutional neural network,” *Computational intelligence
    and neuroscience*, vol. 2018, 2018.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Yoon, J. Lee, 和 M. Whang，“通过卷积神经网络提取的ERP拼写系统的空间和时间域特征”，*计算智能与神经科学*，第2018卷，2018年。'
- en: '[185] J. Shamwell, H. Lee, H. Kwon, A. R. Marathe, V. Lawhern, and W. Nothwang,
    “Single-trial eeg rsvp classification using convolutional neural networks,” in
    *Micro-and Nanotechnology Sensors, Systems, and Applications VIII*, vol. 9836,
    2016, p. 983622.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] J. Shamwell, H. Lee, H. Kwon, A. R. Marathe, V. Lawhern, 和 W. Nothwang，“使用卷积神经网络进行单次试验脑电RSVP分类，”
    在 *微纳技术传感器、系统和应用第八卷*，第9836卷，2016年，第983622页。'
- en: '[186] L. Vařeka and P. Mautner, “Stacked autoencoders for the p300 component
    detection,” *Frontiers in neuroscience*, vol. 11, p. 302, 2017.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] L. Vařeka 和 P. Mautner，“用于P300组件检测的堆叠自编码器，” *神经科学前沿*，第11卷，第302页，2017年。'
- en: '[187] E. Carabez, M. Sugi, I. Nambu, and Y. Wada, “Identifying single trial
    event-related potentials in an earphone-based auditory brain-computer interface,”
    *Applied Sciences*, vol. 7, no. 11, p. 1197, 2017.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] E. Carabez, M. Sugi, I. Nambu, 和 Y. Wada，“耳机基础上的听觉脑机接口中的单次试验事件相关电位识别，”
    *应用科学*，第7卷，第11期，第1197页，2017年。'
- en: '[188] S. Stober, D. J. Cameron, and J. A. Grahn, “Using convolutional neural
    networks to recognize rhythm stimuli from electroencephalography recordings,”
    in *Advances in neural information processing systems*, 2014, pp. 1449–1457.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] S. Stober, D. J. Cameron, 和 J. A. Grahn，“使用卷积神经网络从脑电图记录中识别节奏刺激，” 在 *神经信息处理系统进展*，2014年，第1449–1457页。'
- en: '[189] A. HACHEM, M. M. B. Khelifa, A. M. Alimi, P. Gorce, S. V. ARASU, S. BAULKANI,
    S. K. BISOY, P. K. PATTNAIK, S. RAVINDRAN, N. PALANISAMY *et al.*, “Effect of
    fatigue on ssvep during virtual wheelchair navigation,” *Journal of Theoretical
    and Applied Information Technology*, vol. 65, no. 1, 2014.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] A. HACHEM, M. M. B. Khelifa, A. M. Alimi, P. Gorce, S. V. ARASU, S. BAULKANI,
    S. K. BISOY, P. K. PATTNAIK, S. RAVINDRAN, N. PALANISAMY *等*，“疲劳对虚拟轮椅导航期间稳态视觉诱发电位的影响，”
    *理论与应用信息技术杂志*，第65卷，第1期，2014年。'
- en: '[190] J. Thomas, T. Maszczyk, N. Sinha, T. Kluge, and J. Dauwels, “Deep learning-based
    classification for brain-computer interfaces,” in *Systems, Man, and Cybernetics
    (SMC), 2017 IEEE International Conference on*, 2017, pp. 234–239.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] J. Thomas, T. Maszczyk, N. Sinha, T. Kluge, 和 J. Dauwels，“基于深度学习的脑机接口分类，”
    在 *系统、人工智能与控制（SMC），2017 IEEE国际会议*，2017年，第234–239页。'
- en: '[191] N.-S. Kwak, K.-R. Müller, and S.-W. Lee, “A convolutional neural network
    for steady state visual evoked potential classification under ambulatory environment,”
    *PloS one*, vol. 12, no. 2, p. e0172578, 2017.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] N.-S. Kwak, K.-R. Müller, 和 S.-W. Lee，“用于步态环境下稳态视觉诱发电位分类的卷积神经网络，” *PloS
    one*，第12卷，第2期，第e0172578页，2017年。'
- en: '[192] N. R. Waytowich, V. Lawhern, J. O. Garcia, J. Cummings, J. Faller, P. Sajda,
    and J. M. Vettel, “Compact convolutional neural networks for classification of
    asynchronous steady-state visual evoked potentials,” *arXiv preprint arXiv:1803.04566*,
    2018.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] N. R. Waytowich, V. Lawhern, J. O. Garcia, J. Cummings, J. Faller, P.
    Sajda, 和 J. M. Vettel，“用于异步稳态视觉诱发电位分类的紧凑卷积神经网络，” *arXiv预印本 arXiv:1803.04566*，2018年。'
- en: '[193] N. K. N. Aznan, S. Bonner, J. D. Connolly, N. A. Moubayed, and T. P.
    Breckon, “On the classification of ssvep-based dry-eeg signals via convolutional
    neural networks,” *arXiv preprint arXiv:1805.04157*, 2018.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] N. K. N. Aznan, S. Bonner, J. D. Connolly, N. A. Moubayed, 和 T. P. Breckon，“通过卷积神经网络对基于稳态视觉诱发电位的干电极脑电信号进行分类，”
    *arXiv预印本 arXiv:1805.04157*，2018年。'
- en: '[194] T. Tu, J. Koss, and P. Sajda, “Relating deep neural network representations
    to eeg-fmri spatiotemporal dynamics in a perceptual decision-making task,” in
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    Workshops*, 2018, pp. 1985–1991.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] T. Tu, J. Koss, 和 P. Sajda，“将深度神经网络表示与脑电-功能磁共振成像时空动态相关联，用于感知决策任务，” 在
    *IEEE计算机视觉与模式识别研讨会论文集*，2018年，第1985–1991页。'
- en: '[195] J. Kulasingham, V. Vibujithan, and A. De Silva, “Deep belief networks
    and stacked autoencoders for the p300 guilty knowledge test,” in *Biomedical Engineering
    and Sciences (IECBES), 2016 IEEE EMBS Conference on*, 2016, pp. 127–132.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] J. Kulasingham, V. Vibujithan, 和 A. De Silva，“用于P300罪恶知识测试的深度信念网络和堆叠自编码器，”
    在 *生物医学工程与科学（IECBES），2016 IEEE EMBS会议*，2016年，第127–132页。'
- en: '[196] M. Attia, I. Hettiarachchi, M. Hossny, and S. Nahavandi, “A time domain
    classification of steady-state visual evoked potentials using deep recurrent-convolutional
    neural networks,” in *Biomedical Imaging (ISBI 2018), 2018 IEEE 15th International
    Symposium on*, 2018, pp. 766–769.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] M. Attia, I. Hettiarachchi, M. Hossny, 和 S. Nahavandi，“使用深度递归卷积神经网络对稳态视觉诱发电位进行时间域分类，”
    在 *生物医学成像（ISBI 2018），2018 IEEE第15届国际研讨会*，2018年，第766–769页。'
- en: '[197] J. Pérez-Benítez, J. Pérez-Benítez, and J. Espina-Hernández, “Development
    of a brain computer interface interface using multi-frequency visual stimulation
    and deep neural networks,” in *Electronics, Communications and Computers (CONIELECOMP),
    2018 International Conference on*, 2018, pp. 18–24.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] J. Pérez-Benítez, J. Pérez-Benítez, 和 J. Espina-Hernández, “利用多频视觉刺激和深度神经网络开发脑计算机接口，”
    在 *电子、通信与计算机（CONIELECOMP），2018年国际会议*，2018, 页码 18–24。'
- en: '[198] G. Huve, K. Takahashi, and M. Hashimoto, “Brain activity recognition
    with a wearable fnirs using neural networks,” in *Mechatronics and Automation
    (ICMA), 2017 IEEE International Conference on*, 2017, pp. 1573–1578.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] G. Huve, K. Takahashi, 和 M. Hashimoto, “使用可穿戴fnirs进行脑活动识别，采用神经网络，” 在
    *机电一体化与自动化（ICMA），2017年IEEE国际会议*，2017, 页码 1573–1578。'
- en: '[199] ——, “Brain-computer interface using deep neural network and its application
    to mobile robot control,” in *Advanced Motion Control (AMC), 2018 IEEE 15th International
    Workshop on*, 2018, pp. 169–174.'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] ——，“利用深度神经网络的脑-计算机接口及其在移动机器人控制中的应用，” 在 *高级运动控制（AMC），2018年IEEE第15届国际研讨会*，2018,
    页码 169–174。'
- en: '[200] J. Hennrich, C. Herff, D. Heger, and T. Schultz, “Investigating deep
    learning for fnirs based bci.” in *EMBC*, 2015, pp. 2844–2847.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] J. Hennrich, C. Herff, D. Heger, 和 T. Schultz, “探讨用于fnirs的深度学习，” 在 *EMBC*，2015,
    页码 2844–2847。'
- en: '[201] T. Hiroyasu, K. Hanawa, and U. Yamamoto, “Gender classification of subjects
    from cerebral blood flow changes using deep learning,” in *Computational Intelligence
    and Data Mining (CIDM), 2014 IEEE Symposium on*, 2014, pp. 229–233.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] T. Hiroyasu, K. Hanawa, 和 U. Yamamoto, “利用深度学习从脑血流变化中分类性别，” 在 *计算智能与数据挖掘（CIDM），2014年IEEE研讨会*，2014,
    页码 229–233。'
- en: '[202] S. Koyamada, Y. Shikauchi, K. Nakae, M. Koyama, and S. Ishii, “Deep learning
    of fmri big data: a novel approach to subject-transfer decoding,” *arXiv preprint
    arXiv:1502.00093*, 2015.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] S. Koyamada, Y. Shikauchi, K. Nakae, M. Koyama, 和 S. Ishii, “fmri大数据的深度学习：一种新颖的主体转移解码方法，”
    *arXiv预印本arXiv:1502.00093*, 2015。'
- en: '[203] G. Shen, T. Horikawa, K. Majima, and Y. Kamitani, “Deep image reconstruction
    from human brain activity,” *PLoS computational biology*, vol. 15, no. 1, p. e1006633,
    2019.'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] G. Shen, T. Horikawa, K. Majima, 和 Y. Kamitani, “基于人脑活动的深度图像重建，” *PLoS计算生物学*,
    vol. 15, no. 1, 页码 e1006633, 2019。'
- en: '[204] R. M. Cichy, A. Khosla, D. Pantazis, A. Torralba, and A. Oliva, “Comparison
    of deep neural networks to spatio-temporal cortical dynamics of human visual object
    recognition reveals hierarchical correspondence,” *Scientific reports*, vol. 6,
    p. 27755, 2016.'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] R. M. Cichy, A. Khosla, D. Pantazis, A. Torralba, 和 A. Oliva, “深度神经网络与人类视觉物体识别的时空皮层动态比较揭示了层次对应关系，”
    *科学报告*, vol. 6, 页码 27755, 2016。'
- en: '[205] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio,
    C. Pal, P.-M. Jodoin, and H. Larochelle, “Brain tumor segmentation with deep neural
    networks,” *Medical image analysis*, vol. 35, pp. 18–31, 2017.'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio,
    C. Pal, P.-M. Jodoin, 和 H. Larochelle, “利用深度神经网络进行脑肿瘤分割，” *医学图像分析*, vol. 35, 页码
    18–31, 2017。'
- en: '[206] V. Shreyas and V. Pankajakshan, “A deep learning architecture for brain
    tumor segmentation in mri images,” in *Multimedia Signal Processing (MMSP), 2017
    IEEE 19th International Workshop on*, 2017, pp. 1–6.'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] V. Shreyas 和 V. Pankajakshan, “用于MRI图像中脑肿瘤分割的深度学习架构，” 在 *多媒体信号处理（MMSP），2017年IEEE第19届国际研讨会*，2017,
    页码 1–6。'
- en: '[207] S. Sarraf and G. Tofighi, “Deep learning-based pipeline to recognize
    alzheimer’s disease using fmri data,” in *Future Technologies Conference (FTC)*,
    2016, pp. 816–820.'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] S. Sarraf 和 G. Tofighi, “基于深度学习的管道来识别阿尔茨海默病，使用fmri数据，” 在 *未来技术会议（FTC）*，2016,
    页码 816–820。'
- en: '[208] R. Li, W. Zhang, H.-I. Suk, L. Wang, J. Li, D. Shen, and S. Ji, “Deep
    learning based imaging data completion for improved brain disease diagnosis,”
    in *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, 2014, pp. 305–312.'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] R. Li, W. Zhang, H.-I. Suk, L. Wang, J. Li, D. Shen, 和 S. Ji, “基于深度学习的影像数据补全以改善脑疾病诊断，”
    在 *国际医学图像计算与计算机辅助干预会议*，2014, 页码 305–312。'
- en: '[209] H.-I. Suk, C.-Y. Wee, S.-W. Lee, and D. Shen, “State-space model with
    deep learning for functional dynamics estimation in resting-state fmri,” *NeuroImage*,
    vol. 129, pp. 292–307, 2016.'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] H.-I. Suk, C.-Y. Wee, S.-W. Lee, 和 D. Shen, “用于静息态fmri功能动态估计的深度学习状态空间模型，”
    *神经影像*, vol. 129, 页码 292–307, 2016。'
- en: '[210] H.-I. Suk, D. Shen, A. D. N. Initiative *et al.*, “Deep learning in diagnosis
    of brain disorders,” in *Recent Progress in Brain and Cognitive Engineering*,
    2015, pp. 203–213.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] H.-I. Suk, D. Shen, A. D. N. Initiative *等*，“脑部疾病诊断中的深度学习，” 在 *脑与认知工程领域的最新进展*，2015,
    页码 203–213。'
- en: '[211] S. M. Plis, D. R. Hjelm, R. Salakhutdinov, E. A. Allen, H. J. Bockholt,
    J. D. Long, H. J. Johnson, J. S. Paulsen, J. A. Turner, and V. D. Calhoun, “Deep
    learning for neuroimaging: a validation study,” *Frontiers in neuroscience*, vol. 8,
    p. 229, 2014.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] S. M. Plis、D. R. Hjelm、R. Salakhutdinov、E. A. Allen、H. J. Bockholt、J.
    D. Long、H. J. Johnson、J. S. Paulsen、J. A. Turner 和 V. D. Calhoun，“神经影像学中的深度学习：验证研究”，*Frontiers
    in neuroscience*，第8卷，第229页，2014年。'
- en: '[212] A. Ortiz, J. Munilla, J. M. Gorriz, and J. Ramirez, “Ensembles of deep
    learning architectures for the early diagnosis of the alzheimer’s disease,” *International
    journal of neural systems*, vol. 26, no. 07, p. 1650025, 2016.'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] A. Ortiz、J. Munilla、J. M. Gorriz 和 J. Ramirez，“用于早期诊断阿尔茨海默病的深度学习架构集成”，*International
    journal of neural systems*，第26卷，第07期，第1650025页，2016年。'
- en: '[213] N. F. M. Suhaimi, Z. Z. Htike, and N. K. A. M. Rashid, “Studies on classification
    of fmri data using deep learning approach,” 2015.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] N. F. M. Suhaimi、Z. Z. Htike 和 N. K. A. M. Rashid，“使用深度学习方法对 fMRI 数据进行分类的研究”，2015年。'
- en: '[214] K. Seeliger, U. Güçlü, L. Ambrogioni, Y. Güçlütürk, and M. Van Gerven,
    “Generative adversarial networks for reconstructing natural images from brain
    activity,” *NeuroImage*, vol. 181, pp. 775–785, 2018.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] K. Seeliger、U. Güçlü、L. Ambrogioni、Y. Güçlütürk 和 M. Van Gerven，“用于从脑活动重建自然图像的生成对抗网络”，*NeuroImage*，第181卷，第775–785页，2018年。'
- en: '[215] C. Han, H. Hayashi, L. Rundo, R. Araki, W. Shimoda, S. Muramatsu, Y. Furukawa,
    G. Mauri, and H. Nakayama, “Gan-based synthetic brain mr image generation,” in
    *2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)*, 2018,
    pp. 734–738.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] C. Han、H. Hayashi、L. Rundo、R. Araki、W. Shimoda、S. Muramatsu、Y. Furukawa、G.
    Mauri 和 H. Nakayama，“基于 GAN 的合成脑 MR 图像生成”，在 *2018 IEEE 第15届生物医学成像国际研讨会（ISBI 2018）*，2018年，第734–738页。'
- en: '[216] P. Zhang, F. Wang, W. Xu, and Y. Li, “Multi-channel generative adversarial
    network for parallel magnetic resonance image reconstruction in k-space,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*, 2018,
    pp. 180–188.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] P. Zhang、F. Wang、W. Xu 和 Y. Li，“用于 k 空间中平行磁共振图像重建的多通道生成对抗网络”，在 *医学图像计算与计算机辅助干预国际会议*，2018年，第180–188页。'
- en: '[217] C. Hu, R. Ju, Y. Shen, P. Zhou, and Q. Li, “Clinical decision support
    for alzheimer’s disease based on deep learning and brain network,” in *Communications
    (ICC), 2016 IEEE International Conference on*, 2016, pp. 1–6.'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] C. Hu、R. Ju、Y. Shen、P. Zhou 和 Q. Li，“基于深度学习和脑网络的阿尔茨海默病临床决策支持”，在 *通信（ICC），2016
    IEEE 国际会议*，2016年，第1–6页。'
- en: '[218] P. Garg, E. Davenport, G. Murugesan, B. Wagner, C. Whitlow, J. Maldjian,
    and A. Montillo, “Automatic 1d convolutional neural network-based detection of
    artifacts in meg acquired without electrooculography or electrocardiography,”
    in *Pattern Recognition in Neuroimaging (PRNI), 2017 International Workshop on*,
    2017, pp. 1–4.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] P. Garg、E. Davenport、G. Murugesan、B. Wagner、C. Whitlow、J. Maldjian 和
    A. Montillo，“自动 1D 卷积神经网络基于 MEG 伪影检测，无需眼电图或心电图”，在 *神经影像中的模式识别（PRNI），2017 国际研讨会*，2017年，第1–4页。'
- en: '[219] M. Shu and A. Fyshe, “Sparse autoencoders for word decoding from magnetoencephalography,”
    in *Proceedings of the third NIPS Workshop on Machine Learning and Interpretation
    in NeuroImaging (MLINI)*, 2013.'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] M. Shu 和 A. Fyshe，“稀疏自编码器用于从磁脑电图中解码单词”，在 *第三届 NIPS 神经影像中的机器学习与解释研讨会（MLINI）*，2013年。'
- en: '[220] A. Hasasneh, N. Kampel, P. Sripad, N. J. Shah, and J. Dammers, “Deep
    learning approach for automatic classification of ocular and cardiac artifacts
    in meg data,” *Journal of Engineering*, vol. 2018, 2018.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] A. Hasasneh、N. Kampel、P. Sripad、N. J. Shah 和 J. Dammers，“用于自动分类 MEG 数据中眼动和心脏伪影的深度学习方法”，*Journal
    of Engineering*，第2018卷，2018年。'
- en: '[221] Y. Gordienko, S. Stirenko, Y. Kochura, O. Alienin, M. Novotarskiy, and
    N. Gordienko, “Deep learning for fatigue estimation on the basis of multimodal
    human-machine interactions,” *arXiv preprint arXiv:1801.06048*, 2017.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Y. Gordienko、S. Stirenko、Y. Kochura、O. Alienin、M. Novotarskiy 和 N. Gordienko，“基于多模态人机交互的疲劳估计深度学习”，*arXiv
    预印本 arXiv:1801.06048*，2017年。'
- en: '[222] G. Pfurtscheller and F. L. Da Silva, “Event-related eeg/meg synchronization
    and desynchronization: basic principles,” *Clinical neurophysiology*, vol. 110,
    no. 11, pp. 1842–1857, 1999.'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] G. Pfurtscheller 和 F. L. Da Silva，“事件相关 EEG/MEG 同步与去同步：基本原理”，*Clinical
    neurophysiology*，第110卷，第11期，第1842–1857页，1999年。'
- en: '[223] P. Khurana, A. Majumdar, and R. Ward, “Class-wise deep dictionaries for
    eeg classification,” in *International Joint Conference on Neural Networks (IJCNN)*,
    2016, pp. 3556–3563.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] P. Khurana、A. Majumdar 和 R. Ward，“用于 EEG 分类的类别深度字典”，在 *国际神经网络联合会议（IJCNN）*，2016年，第3556–3563页。'
- en: '[224] E. Yin, T. Zeyl, R. Saab, T. Chau, D. Hu, and Z. Zhou, “A hybrid brain–computer
    interface based on the fusion of p300 and ssvep scores,” *IEEE Transactions on
    Neural Systems and Rehabilitation Engineering*, vol. 23, no. 4, pp. 693–701, 2015.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] E. Yin, T. Zeyl, R. Saab, T. Chau, D. Hu, 和 Z. Zhou，“基于 p300 和 ssvep
    评分融合的混合脑–计算机接口”，*IEEE 神经系统与康复工程学报*，第23卷，第4期，第693–701页，2015年。'
- en: '[225] S. Min, B. Lee, and S. Yoon, “Deep learning in bioinformatics,” *Briefings
    in bioinformatics*, vol. 18, no. 5, pp. 851–869, 2017.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] S. Min, B. Lee, 和 S. Yoon，“生物信息学中的深度学习”，*生物信息学简报*，第18卷，第5期，第851–869页，2017年。'
- en: '[226] S. Sarraf, G. Tofighi *et al.*, “Deepad: Alzheimer’s disease classification
    via deep convolutional neural networks using mri and fmri,” *bioRxiv*, p. 070441,
    2016.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] S. Sarraf, G. Tofighi *等*，“Deepad：通过使用 MRI 和 fMRI 的深度卷积神经网络对阿尔茨海默病进行分类”，*bioRxiv*，第070441页，2016年。'
- en: '[227] L. Marc Moreno, “Deep learning for brain tumor segmentation,” *Master
    diss. University of Colorado Colorado Springs.*, 2017.'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] L. Marc Moreno，“用于脑肿瘤分割的深度学习”，*硕士论文，科罗拉多大学科罗拉多泉分校*，2017年。'
- en: '[228] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” *International Conference
    on Learning Representations (ICLR)*, 2016.'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] A. Radford, L. Metz, 和 S. Chintala，“使用深度卷积生成对抗网络进行无监督表示学习”，*国际学习表征会议
    (ICLR)*，2016年。'
- en: '[229] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
    networks,” in *International Conference on Machine Learning (ICML)*, 2017, pp.
    214–223.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] M. Arjovsky, S. Chintala, 和 L. Bottou，“Wasserstein 生成对抗网络”，发表于 *国际机器学习会议
    (ICML)*，2017年，第214–223页。'
- en: '[230] A. Antoniades, L. Spyrou, C. C. Took, and S. Sanei, “Deep learning for
    epileptic intracranial eeg data,” in *Machine Learning for Signal Processing (MLSP),
    2016 IEEE 26th International Workshop on*, 2016, pp. 1–6.'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] A. Antoniades, L. Spyrou, C. C. Took, 和 S. Sanei，“用于癫痫颅内 EEG 数据的深度学习”，发表于
    *2016 IEEE 第26届国际信号处理工作坊 (MLSP)*，2016年，第1–6页。'
- en: '[231] A. Antoniades, L. Spyrou, D. Martin-Lopez, A. Valentin, G. Alarcon, S. Sanei,
    and C. C. Took, “Deep neural architectures for mapping scalp to intracranial eeg,”
    *International journal of neural systems*, p. 1850009, 2018.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] A. Antoniades, L. Spyrou, D. Martin-Lopez, A. Valentin, G. Alarcon, S.
    Sanei, 和 C. C. Took，“用于将头皮 EEG 映射到颅内 EEG 的深度神经架构”，*国际神经系统杂志*，第1850009页，2018年。'
- en: '[232] R. Parasuraman and Y. Jiang, “Individual differences in cognition, affect,
    and performance: Behavioral, neuroimaging, and molecular genetic approaches,”
    *Neuroimage*, vol. 59, no. 1, pp. 70–82, 2012.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] R. Parasuraman 和 Y. Jiang，“认知、情感和表现中的个体差异：行为学、神经成像和分子遗传学方法”，*神经影像*，第59卷，第1期，第70–82页，2012年。'
- en: '[233] L. Deng, “Three classes of deep learning architectures and their applications:
    a tutorial survey,” *APSIPA transactions on signal and information processing*,
    2012.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] L. Deng，“深度学习架构的三种类别及其应用：教程综述”，*APSIPA 信号与信息处理学报*，2012年。'
- en: '[234] W.-L. Zheng and B.-L. Lu, “Personalizing eeg-based affective models with
    transfer learning,” in *Proceedings of the Twenty-Fifth International Joint Conference
    on Artificial Intelligence*, 2016, pp. 2732–2738.'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] W.-L. Zheng 和 B.-L. Lu，“使用迁移学习个性化 EEG 基于情感模型”，发表于 *第二十五届国际人工智能联合会议论文集*，2016年，第2732–2738页。'
- en: '[235] X. Zhang, L. Yao, and F. Yuan, “Adversarial variational embedding for
    robust semi-supervised learning,” in *SIGKDD 2019*, 2019.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] X. Zhang, L. Yao, 和 F. Yuan，“用于鲁棒半监督学习的对抗变分嵌入”，发表于 *SIGKDD 2019*，2019年。'
- en: '[236] S. Koyama, S. M. Chase, A. S. Whitford, M. Velliste, A. B. Schwartz,
    and R. E. Kass, “Comparison of brain–computer interface decoding algorithms in
    open-loop and closed-loop control,” *Journal of computational neuroscience*, vol. 29,
    no. 1-2, pp. 73–87, 2010.'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] S. Koyama, S. M. Chase, A. S. Whitford, M. Velliste, A. B. Schwartz,
    和 R. E. Kass，“在开环和闭环控制中比较脑机接口解码算法”，*计算神经科学杂志*，第29卷，第1-2期，第73–87页，2010年。'
- en: '[237] S. Aliakbaryhosseinabadi, E. N. Kamavuako, N. Jiang, D. Farina, and N. Mrachacz-Kersting,
    “Online adaptive synchronous bci system with attention variations,” in *Brain-Computer
    Interface Research*, 2019, pp. 31–41.'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] S. Aliakbaryhosseinabadi, E. N. Kamavuako, N. Jiang, D. Farina, 和 N.
    Mrachacz-Kersting，“具有注意力变化的在线自适应同步脑机接口系统”，发表于 *脑机接口研究*，2019年，第31–41页。'
- en: '[238] E. K. Kalunga, S. Chevallier, Q. Barthélemy, K. Djouani, E. Monacelli,
    and Y. Hamam, “Online ssvep-based bci using riemannian geometry,” *Neurocomputing*,
    vol. 191, pp. 55–68, 2016.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] E. K. Kalunga, S. Chevallier, Q. Barthélemy, K. Djouani, E. Monacelli,
    和 Y. Hamam，“基于 Riemannian 几何的在线 ssvep 传感器脑机接口”，*神经计算*，第191卷，第55–68页，2016年。'
- en: '[239] M. Pacharra, S. Debener, and E. Wascher, “Concealed around-the-ear eeg
    captures cognitive processing in a visual simon task,” *Frontiers in human neuroscience*,
    vol. 11, p. 290, 2017.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] M. Pacharra, S. Debener, 和 E. Wascher，“耳后隐藏 EEG 捕捉视觉 Simon 任务中的认知处理，”
    *前沿人类神经科学*，第 11 卷，第 290 页，2017 年。'
- en: '[240] K. B. Mikkelsen, S. L. Kappel, D. P. Mandic, and P. Kidmose, “Eeg recorded
    from the ear: Characterizing the ear-eeg method,” *Frontiers in neuroscience*,
    vol. 9, p. 438, 2015.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] K. B. Mikkelsen, S. L. Kappel, D. P. Mandic, 和 P. Kidmose，“从耳朵记录的 EEG：耳
    EEG 方法的特征，” *前沿神经科学*，第 9 卷，第 438 页，2015 年。'
- en: '[241] S. B. Rutkove, “Introduction to volume conduction,” in *The clinical
    neurophysiology primer*, 2007, pp. 43–53.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] S. B. Rutkove，“体积导电介绍，” 在 *临床神经生理学初学者*，2007 年，第 43–53 页。'
- en: '[242] B. Burle, L. Spieser, C. Roger, L. Casini, T. Hasbroucq, and F. Vidal,
    “Spatial and temporal resolutions of eeg: Is it really black and white? a scalp
    current density view,” *International Journal of Psychophysiology*, vol. 97, no. 3,
    pp. 210–220, 2015.'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] B. Burle, L. Spieser, C. Roger, L. Casini, T. Hasbroucq, 和 F. Vidal，“EEG
    的空间和时间分辨率：真的非黑即白吗？头皮电流密度视角，” *国际心理生理学杂志*，第 97 卷，第 3 期，第 210–220 页，2015 年。'
- en: '[243] J. Malmivuo, R. Plonsey *et al.*, *Bioelectromagnetism: principles and
    applications of bioelectric and biomagnetic fields*, 1995.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] J. Malmivuo, R. Plonsey *等*，*生物电磁学：生物电和生物磁场的原理与应用*，1995 年。'
- en: '[244] M. Tortella-Feliu, A. Morillas-Romero, M. Balle, J. Llabrés, X. Bornas,
    and P. Putman, “Spontaneous eeg activity and spontaneous emotion regulation,”
    *International Journal of Psychophysiology*, vol. 94, no. 3, pp. 365–372, 2014.'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] M. Tortella-Feliu, A. Morillas-Romero, M. Balle, J. Llabrés, X. Bornas,
    和 P. Putman，“自发 EEG 活动和自发情绪调节，” *国际心理生理学杂志*，第 94 卷，第 3 期，第 365–372 页，2014 年。'
- en: '[245] A. Salek-Haddadi, K. Friston, L. Lemieux, and D. Fish, “Studying spontaneous
    eeg activity with fmri,” *Brain research reviews*, vol. 43, no. 1, pp. 110–133,
    2003.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] A. Salek-Haddadi, K. Friston, L. Lemieux, 和 D. Fish，“通过 FMRI 研究自发 EEG
    活动，” *脑研究评论*，第 43 卷，第 1 期，第 110–133 页，2003 年。'
- en: '[246] A. Ikeda and Y. Washizawa, “Spontaneous eeg classification using complex
    valued neural network,” in *International Conference on Neural Information Processing*,
    2019, pp. 495–503.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] A. Ikeda 和 Y. Washizawa，“使用复杂值神经网络进行自发 EEG 分类，” 在 *国际神经信息处理会议*，2019 年，第
    495–503 页。'
- en: '[247] A. M. Norcia, L. G. Appelbaum, J. M. Ales, B. R. Cottereau, and B. Rossion,
    “The steady-state visual evoked potential in vision research: a review,” *Journal
    of vision*, vol. 15, no. 6, pp. 4–4, 2015.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] A. M. Norcia, L. G. Appelbaum, J. M. Ales, B. R. Cottereau, 和 B. Rossion，“视觉研究中的稳态视觉诱发电位：综述，”
    *视觉杂志*，第 15 卷，第 6 期，第 4–4 页，2015 年。'
- en: '[248] S. Lees, N. Dayan, H. Cecotti, P. Mccullagh, L. Maguire, F. Lotte, and
    D. Coyle, “A review of rapid serial visual presentation-based brain–computer interfaces,”
    *Journal of neural engineering*, vol. 15, no. 2, p. 021001, 2018.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] S. Lees, N. Dayan, H. Cecotti, P. Mccullagh, L. Maguire, F. Lotte, 和
    D. Coyle，“基于快速串行视觉展示的脑机接口综述，” *神经工程杂志*，第 15 卷，第 2 期，第 021001 页，2018 年。'
- en: '[249] K. H. Chiappa, *Evoked potentials in clinical medicine*, 1997.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] K. H. Chiappa，*临床医学中的诱发电位*，1997 年。'
- en: '[250] V. Mayya, B. Mainsah, and G. Reeves, “Information-theoretic analysis
    of refractory effects in the p300 speller,” *arXiv preprint arXiv:1701.03313\.
    Non-exclusive-distrib License. https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html*,
    2017.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] V. Mayya, B. Mainsah, 和 G. Reeves，“P300 拼写器中的耐药效应的信息论分析，” *arXiv 预印本
    arXiv:1701.03313\. 非独占分发许可。 https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html*，2017
    年。'
- en: '[251] C. Guger, S. Daban, E. Sellers, C. Holzner, G. Krausz, R. Carabalona,
    F. Gramatica, and G. Edlinger, “How many people are able to control a p300-based
    brain–computer interface (bci)?” *Neuroscience letters*, vol. 462, no. 1, pp.
    94–98, 2009.'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] C. Guger, S. Daban, E. Sellers, C. Holzner, G. Krausz, R. Carabalona,
    F. Gramatica, 和 G. Edlinger，“有多少人能够控制基于 P300 的脑机接口 (BCI)？” *神经科学快报*，第 462 卷，第
    1 期，第 94–98 页，2009 年。'
- en: '[252] A. Belitski, J. Farquhar, and P. Desain, “P300 audio-visual speller,”
    *Journal of Neural Engineering*, vol. 8, no. 2, p. 025022, 2011.'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] A. Belitski, J. Farquhar, 和 P. Desain，“P300 视听拼写器，” *神经工程杂志*，第 8 卷，第
    2 期，第 025022 页，2011 年。'
- en: '[253] M. Welvaert and Y. Rosseel, “On the definition of signal-to-noise ratio
    and contrast-to-noise ratio for fmri data,” *PloS one*, vol. 8, no. 11, 2013.'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] M. Welvaert 和 Y. Rosseel，“关于 FMRI 数据的信噪比和对比噪声比的定义，” *PloS one*，第 8 卷，第
    11 期，2013 年。'
- en: '[254] R. M. Cichy, A. Khosla, D. Pantazis, and A. Oliva, “Dynamics of scene
    representations in the human brain revealed by magnetoencephalography and deep
    neural networks,” *Neuroimage*, vol. 153, pp. 346–358, 2017.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] R. M. Cichy, A. Khosla, D. Pantazis 和 A. Oliva, “通过脑磁图和深度神经网络揭示的人脑场景表征动态，”
    *神经影像*, 第 153 卷，页码 346–358, 2017。'
- en: '[255] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] S. Hochreiter 和 J. Schmidhuber, “长短期记忆，” *神经计算*, 第 9 卷，第 8 期，页码 1735–1780,
    1997。'
- en: '[256] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” *arXiv preprint arXiv:1406.1078*, 2014.'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H.
    Schwenk, 和 Y. Bengio, “使用 RNN 编码器-解码器学习短语表示以进行统计机器翻译，” *arXiv 预印本 arXiv:1406.1078*,
    2014。'
- en: '[257] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A
    review and new perspectives,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 35, no. 8, pp. 1798–1828, 2013.'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Y. Bengio, A. Courville 和 P. Vincent, “表示学习：综述与新视角，” *IEEE 模式分析与机器智能汇刊*,
    第 35 卷，第 8 期，页码 1798–1828, 2013。'
- en: '[258] P. O. Glauner, “Comparison of training methods for deep neural networks,”
    *arXiv preprint arXiv:1504.06825*, 2015.'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] P. O. Glauner, “深度神经网络训练方法比较，” *arXiv 预印本 arXiv:1504.06825*, 2015。'
- en: '[259] G. St-Yves and T. Naselaris, “Generative adversarial networks conditioned
    on brain activity reconstruct seen images,” in *2018 IEEE International Conference
    on Systems, Man, and Cybernetics (SMC)*, 2018, pp. 1054–1061.'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] G. St-Yves 和 T. Naselaris, “以脑活动为条件的生成对抗网络重建已见图像，” 见 *2018 IEEE 国际系统、人类与控制会议
    (SMC)*, 2018, 页码 1054–1061。'
- en: Appendix A Non-invasive Brain Signals
  id: totrans-990
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 非侵入性脑信号
- en: 'Here, we present a detailed introduction of brain signals as shown in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"). Non-invasive brain imaging
    technique can be collected using electrical, magnetic or metabolic methods, which
    mainly include Electroencephalogram (EEG), Functional near-infrared spectroscopy
    (fNIRS), Functional magnetic resonance imaging (fMRI), and Magnetoencephalography
    (MEG).'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们详细介绍了脑信号，如图[2](#S2.F2 "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers")所示。非侵入性脑成像技术可以使用电学、磁学或代谢方法收集，主要包括脑电图
    (EEG)、功能性近红外光谱 (fNIRS)、功能性磁共振成像 (fMRI) 和脑磁图 (MEG)。'
- en: A.1 Electroencephalography (EEG)
  id: totrans-992
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 脑电图 (EEG)
- en: 'Electroencephalography (EEG) is the most commonly used non-invasive technique
    for measuring brain activities. EEG monitors the voltage fluctuations generated
    by an electrical current within human neurons. Electrodes placed on the scalp
    measure the amplitude of EEG signals. EEG signals have a low spatial resolution
    due to the effect of volume conduction which refers to the complex effects of
    measuring electrical potentials a distance from the source generators [[241](#bib.bib241),
    [242](#bib.bib242)]. EEG electrode locations generally follow the international
    10-20 system [[243](#bib.bib243)]. The specific placement of electrodes is presented
    in Figure [5](#A1.F5 "Figure 5 ‣ A.1 Electroencephalography (EEG) ‣ Appendix A
    Non-invasive Brain Signals ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers") [[10](#bib.bib10)]. The EEG signals
    are collected while the subject is undertaking imagination task. Each line represents
    the signal stream collected from a single EEG electrode (also called ‘channel‘)
    over time.'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: '脑电图 (EEG) 是测量脑部活动最常用的非侵入性技术。EEG 监测由人类神经元内电流生成的电压波动。放置在头皮上的电极测量 EEG 信号的振幅。由于体积导电效应（即从源发生器远距离测量电位的复杂效应），EEG
    信号具有较低的空间分辨率[[241](#bib.bib241), [242](#bib.bib242)]。EEG 电极位置通常遵循国际 10-20 系统[[243](#bib.bib243)]。电极的具体放置如图[5](#A1.F5
    "Figure 5 ‣ A.1 Electroencephalography (EEG) ‣ Appendix A Non-invasive Brain Signals
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") [[10](#bib.bib10)]所示。EEG 信号在受试者进行想象任务时收集。每条线代表从单个 EEG 电极（也称为“通道”）随时间收集的信号流。'
- en: The temporal resolution of EEG signals is much better than the spatial resolution.
    The ionic current changes rapidly, which offers a temporal resolution higher than
    $1000$ Hz. The SNR of EEG is generally very poor due to both objective and subjective
    factors. Objective factors include environmental noises, the obstruction of the
    skull and other tissues between cortex and scalp, and different stimulations.
    Subjective factors contain the subject’s mental stage, fatigue status, the variance
    among different subjects, and so on.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: EEG信号的时间分辨率远优于空间分辨率。离子电流变化迅速，这提供了高于$1000$ Hz的时间分辨率。由于客观和主观因素，EEG的信噪比通常非常差。客观因素包括环境噪音、头骨和皮肤之间的其他组织的阻碍以及不同的刺激。主观因素包括受试者的心理状态、疲劳状况、不同受试者之间的差异等。
- en: EEG recording equipment can be installed in a cap-like headset. The EEG headset
    can be mounted on the user’s head to gather signals. Compared to other equipment
    used to measure brain signals, EEG headsets are portable and more accessible for
    most applications.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: EEG记录设备可以安装在类似头盔的耳机上。EEG耳机可以安装在用户的头部以收集信号。与其他用于测量脑信号的设备相比，EEG耳机更便携，适用于大多数应用。
- en: 'The EEG signals collected from any typical EEG hardware have several non-overlapping
    frequency bands (Delta, Theta, Alpha, Beta, and Gamma) based on the strong intra-band
    correlation with a distinct behavioral state [[10](#bib.bib10)]. Each EEG pattern
    contains signals associated with particular brain information. Table [7](#A1.T7
    "Table 7 ‣ A.1 Electroencephalography (EEG) ‣ Appendix A Non-invasive Brain Signals
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") shows EEG frequency patterns and the corresponding characteristics.
    Here, the degree of awareness denotes the perception of individuals when presented
    with external stimuli.'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: '从任何典型的EEG硬件收集到的EEG信号有几个不重叠的频带（Delta、Theta、Alpha、Beta 和 Gamma），基于与特定行为状态的强内部频带相关性[[10](#bib.bib10)]。每个EEG模式包含与特定大脑信息相关的信号。表[7](#A1.T7
    "Table 7 ‣ A.1 Electroencephalography (EEG) ‣ Appendix A Non-invasive Brain Signals
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers")显示了EEG频率模式及其相应特征。在这里，意识的程度表示个体在面对外部刺激时的感知。'
- en: 'Compared to other signals (e.g., fMRI, fNIRS, MEG), EEG has several important
    advantages: 1) the hardware has higher portability with much lower price; 2) the
    temporal resolution is very high (milliseconds level). Among other non-invasive
    techniques, only MEG has the same level of temporal resolution; 3) EEG is relatively
    tolerant of subject movement and artifacts, which can be minimized by existing
    signal processing methods; 4) the subject doesn’t need to be exposed to high-intensity
    ($>$1 Tesla) magnetic fields, therefore, EEG can serve subjects that have metal
    implants in their body (such as metal-containing pacemakers).'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他信号（如fMRI、fNIRS、MEG）相比，EEG有几个重要的优势：1）硬件具有更高的便携性且价格更低；2）时间分辨率非常高（毫秒级）。在其他非侵入性技术中，只有MEG具有相同水平的时间分辨率；3）EEG对受试者运动和伪影的耐受性相对较高，现有的信号处理方法可以最小化这些伪影；4）受试者不需要暴露于高强度（$>$1
    Tesla）磁场，因此，EEG可以用于有金属植入物的受试者（如含金属的心脏起搏器）。
- en: 'As the most commonly used signals, there are a large number of sub-classes
    of EEG signals. In this section, we present a methodical introduction of EEG sub-class
    signals. As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Brain Imaging Techniques
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers"), we divided EEG signals into spontaneous EEG and evoked potentials.
    Evoked potentials can be split into event-related potentials and steady-state
    evoked potentials based on the frequency of the external stimuli [[7](#bib.bib7)].
    Each potential contains visual-, auditory-, and somatosensory- potentials based
    on the external stimuli types. The dashed quadrilaterals in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"), such as Intracortical, SEP,
    SSAEP, SSSEP, and RSAP, are not included in this survey because there are very
    few existing studies working on them with deep learning algorithms. We list these
    signals for systematic completeness.'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最常用的信号，有大量 EEG 信号的子类。在这一部分，我们介绍了 EEG 子类信号的方法性介绍。如图 [2](#S2.F2 "图 2 ‣ 2 大脑成像技术
    ‣ 基于深度学习的非侵入性脑信号：近期进展与新前沿") 所示，我们将 EEG 信号分为自发 EEG 和诱发电位。诱发电位可以根据外部刺激的频率分为事件相关电位和稳态诱发电位
    [[7](#bib.bib7)]。每种电位包含基于外部刺激类型的视觉、听觉和体感电位。图 [2](#S2.F2 "图 2 ‣ 2 大脑成像技术 ‣ 基于深度学习的非侵入性脑信号：近期进展与新前沿")
    中的虚线四边形，如皮层内，SEP，SSAEP，SSSEP 和 RSAP，不包括在本调查中，因为关于这些信号的深度学习算法研究非常少。我们列出这些信号以确保系统的完整性。
- en: '![Refer to caption](img/1c25f2283966e3801da3e07bf21abf7f.png)'
  id: totrans-999
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1c25f2283966e3801da3e07bf21abf7f.png)'
- en: (a) EEG electrode locations
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: (a) EEG 电极位置
- en: '![Refer to caption](img/a67e87a4d51d8d018757d6b9157cc1fc.png)'
  id: totrans-1001
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a67e87a4d51d8d018757d6b9157cc1fc.png)'
- en: (b) EEG signals
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: (b) EEG 信号
- en: 'Figure 5: EEG electrode locations on scalp (10-20 system) and the gathered
    EEG signals [[10](#bib.bib10)]. The electrodes’ names are marked by their position:
    Fp (pre-frontal), F (frontal), T (temporal), P (parietal), O (occipital), and
    C ( central).'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 头皮上的 EEG 电极位置（10-20 系统）和收集到的 EEG 信号 [[10](#bib.bib10)]。电极的名称由其位置标记：Fp（前额），F（额），T（颞），P（顶），O（枕），和
    C（中央）。'
- en: 'Table 7: EEG patterns and corresponding characters. Awareness Degree denotes
    the degree of being aware of an external world. The awareness degree mentioned
    here is mainly defined in physiology instead of psychology.'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: EEG 模式及其对应特征。觉察程度表示对外部世界的意识程度。此处提到的觉察程度主要定义在生理学而非心理学上。'
- en: '| Patterns | Frequency ($Hz$) | Amplitude | Brain State | Awareness Degree
    | Produced Location |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 频率 ($Hz$) | 振幅 | 大脑状态 | 觉察程度 | 产生位置 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Delta | 0.5-4 | Higher | Deep sleep pattern | Lower | Frontally and posteriorly
    |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '| Delta | 0.5-4 | 较高 | 深睡眠模式 | 更低 | 前部和后部 |'
- en: '| Theta | 4-8 | High | Light sleep pattern | Low | Entorhinal cortex, hippocampus
    |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
  zh: '| Theta | 4-8 | 高 | 浅睡眠模式 | 低 | 内嗅皮层，海马体 |'
- en: '| Alpha | 8-12 | Medium | Closing the eyes, relax state | Intermediate | Posterior
    regions of head |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
  zh: '| Alpha | 8-12 | 中等 | 闭眼，放松状态 | 中等 | 头部后部区域 |'
- en: '| Beta | 12-30 | Low | Active thinking, focus, high alert, anxious | High |
    Most evident frontally, motor areas |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '| Beta | 12-30 | 低 | 活跃思维，集中，高警觉，焦虑 | 高 | 前额和运动区域最为明显 |'
- en: '| Gamma | 30-100 | Lower | During cross-modal sensory processing | Higher |
    Somatosensory, auditory cortices |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
  zh: '| Gamma | 30-100 | 较低 | 跨模态感官处理期间 | 较高 | 体感，听觉皮层 |'
- en: A.1.1 Spontaneous EEG
  id: totrans-1012
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 自发 EEG
- en: Typically, when we talk about the term ‘EEG,’ we refer to spontaneous EEG which
    measures the brain signals under a specific state without external stimulation
    [[244](#bib.bib244), [245](#bib.bib245), [246](#bib.bib246)]. In particular, spontaneous
    EEG includes the EEG signals while the individual is sleeping, undertaking a mental
    task (e.g., counting), suffering brain disorders, undertaking motor imagery tasks,
    in a certain emotion, etc.
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们谈论“EEG”这个术语时，我们指的是自发 EEG，它在没有外部刺激的特定状态下测量大脑信号 [[244](#bib.bib244), [245](#bib.bib245),
    [246](#bib.bib246)]。特别地，自发 EEG 包括个体在睡眠、进行心理任务（例如计数）、遭受脑部疾病、进行运动想象任务、处于某种情绪等状态下的
    EEG 信号。
- en: The EEG signals recorded while a user stares at a color/shape/image belong to
    this category. While the subject is gazing at a specific image, the visual stimuli
    are steady without any change. This scenario differs from the visual stimuli in
    evoked potential, where the visual stimuli are changing at a specific frequency.
    Thus, we regard the image stimulation as a particular state and regard it as spontaneous
    EEG. Spontaneous EEG-based systems are challenging to train, due to the lower
    SNR and the larger variation across subjects [[35](#bib.bib35)].
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户注视颜色/形状/图像时记录的脑电图信号属于这一类别。当受试者注视特定图像时，视觉刺激保持不变。这种情况不同于诱发电位中的视觉刺激，在诱发电位中，视觉刺激以特定频率变化。因此，我们将图像刺激视为一种特定状态，并将其视为自发脑电图。基于自发脑电图的系统训练具有挑战性，因为其信噪比较低且受试者间变化较大[[35](#bib.bib35)]。
- en: 'According to the gathering scenarios, the spontaneous EEG contains several
    subordinates: sleeping, motor imagery, emotional, mental disease and others.'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 根据采集场景，自发脑电图包含几个子类别：睡眠、运动想象、情绪、心理疾病等。
- en: A.1.2 Evoked Potential (EP)
  id: totrans-1016
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 诱发电位（EP）
- en: 'Evoked Potentials (EP) or evoked responses refers to the EEG signals which
    are evoked by an external stimulus instead of spontaneously. An EP is time-locked
    to the external stimulus while the aforementioned spontaneous EEG is non-time-locked.
    In contrast to spontaneous EEG, EP generally has higher amplitude and lower frequency.
    As a result, the EP signals are more robust across subjects. According to the
    stimulation method, there exist two categories of EP: the Event-Related Potential
    (ERP) and the Steady State Evoked Potential (SSEP). ERP records the EEG signals
    in response to an isolated discrete stimulus event (or event change). To achieve
    this isolation, stimuli in an ERP experiment are typically separated from each
    other by a long inter-stimulus interval, allowing for the estimation of a stimulus-independent
    baseline reference [[247](#bib.bib247)]. The stimuli frequency of ERP is generally
    lower than 2 Hz. In contrast, SSEP is generated in response to a periodic stimulus
    at a fixed rate. The stimuli frequency of SSEP generally ranges within 3.5-75
    Hz.'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: 诱发电位（EP）或诱发反应指的是由外部刺激引发的脑电图信号，而不是自发产生的。EP与外部刺激时间同步，而前述自发脑电图则是非时间同步的。与自发脑电图相比，EP通常具有更高的振幅和更低的频率。因此，EP信号在不同受试者间更具鲁棒性。根据刺激方法，EP分为两类：事件相关电位（ERP）和稳态诱发电位（SSEP）。ERP记录对孤立的离散刺激事件（或事件变化）的脑电图信号。为了实现这种隔离，ERP实验中的刺激通常由较长的刺激间隔隔开，从而允许估算一个与刺激无关的基线参考[[247](#bib.bib247)]。ERP的刺激频率通常低于2
    Hz。相对而言，SSEP是对以固定速率的周期性刺激产生的。SSEP的刺激频率一般在3.5-75 Hz之间。
- en: 'Event-related potential (ERP). There are three kinds of evoked potentials in
    extensive research and clinical use: Visual Evoked Potentials (VEP); Auditory
    Evoked Potentials (AEP); and Somatosensory Evoked Potentials (SEP) [[28](#bib.bib28)].
    The VEP signals are mainly on the occipital lobe, and the highest signal amplitudes
    are collected at the Calcarine sulcus.'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 事件相关电位（ERP）。在广泛的研究和临床应用中，有三种类型的诱发电位：视觉诱发电位（VEP）；听觉诱发电位（AEP）；和躯体感觉诱发电位（SEP）[[28](#bib.bib28)]。VEP信号主要位于枕叶，最高的信号振幅集中在角回。
- en: 1) Visual Evoked Potentials (VEP). Visual Evoked Potentials are a specific category
    of ERP which is caused by visual stimulus (e.g., an alternating checkerboard pattern
    on a computer screen). VEP signals are hidden within the normal spontaneous EEG.
    To separate VEP signals from the background EEG readings, repetitive stimulation
    and time-locked signal-averaging techniques are generally employed.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 视觉诱发电位（VEP）。视觉诱发电位是由视觉刺激（例如计算机屏幕上的交替棋盘图案）引起的一类特定ERP。VEP信号隐藏在正常的自发脑电图中。为了将VEP信号与背景脑电图读数分离，通常采用重复刺激和时间锁定信号平均技术。
- en: Rapid Serial Visual Presentation (RSVP) [[248](#bib.bib248)] can be regarded
    as one kind of VEP. An RSVP diagram is commonly used to examine the temporal characteristics
    of attention. The subject is required to stare at a screen where a series of items
    (e.g., images) are presented one-by-one. There is a specific item (called the
    target) separates from the rest of the other items (called distracters). The subject
    knows which is the target before the RSVP experiment. For instance, the distracters
    can be a color change or letters among numbers. RSVP contains a static mode (the
    items appear on the screen and then disappear without moving) and a moving mode
    (the items appear on the screen, move to another place, and finally disappear).
    Nowadays, brain signal research mainly focuses on the static mode RSVP. Usually,
    the frequency of RSVP is 10Hz which means that each item will stay on the screen
    for 0.1 seconds.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 快速串行视觉呈现 (RSVP) [[248](#bib.bib248)] 可视为一种视觉诱发电位 (VEP)。RSVP 图通常用于检查注意力的时间特性。受试者需要注视屏幕，上面会逐一呈现一系列项目（例如，图像）。有一个特定的项目（称为目标）与其他项目（称为干扰项）分开。受试者在
    RSVP 实验前知道哪个是目标。例如，干扰项可以是数字中的颜色变化或字母。RSVP 包括静态模式（项目出现在屏幕上然后消失而不移动）和移动模式（项目出现在屏幕上，移动到另一个地方，最后消失）。现在，大脑信号研究主要集中在静态模式
    RSVP 上。通常，RSVP 的频率为 10Hz，这意味着每个项目在屏幕上停留 0.1 秒。
- en: 2) Auditory Evoked Potentials (AEP). Auditory Evoked Potentials are a specific
    subclass of ERP in which responses to auditory (sound) stimuli are recorded. AEP
    is mainly recorded from the scalp but originates at the brainstem or cortex. The
    most common AEP measured is the auditory brainstem response (ABR) which is generally
    employed to test the hearing ability of newborns and infants. In the brain signal
    area, AEP is mainly used in clinical tests for its accuracy and reliability in
    detecting unilateral loss [[249](#bib.bib249)]. Similar to RSVP, Rapid Serial
    Auditory Presentation (RSAP) refers to the experiments with rapid serial presentation
    of sound stimuli. The task for the subject is to recognize the target audio among
    the distracters.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 听觉诱发电位 (AEP)。听觉诱发电位是 ERP 的一个特定子类，记录对听觉（声音）刺激的反应。AEP 主要从头皮上记录，但源自脑干或皮层。最常测量的
    AEP 是听觉脑干反应 (ABR)，通常用于测试新生儿和婴儿的听力。在大脑信号领域，AEP 主要用于临床测试，因其在检测单侧听力丧失方面的准确性和可靠性 [[249](#bib.bib249)]。类似于
    RSVP，快速串行听觉呈现 (RSAP) 指的是快速串行呈现声音刺激的实验。受试者的任务是识别目标声音中的干扰项。
- en: 3) Somatosensory Evoked Potentials (SEP).^(21)^(21)21Generally, Somatosensory
    Evoked Potentials is abbreviated as SSEP or SEP. In this paper, we choose SEP
    as the abbreviation in case of the conflict with Steady-State Evoked Potentials
    (SSEP). Somatosensory Evoked Potentials are another commonly used subcategory
    of ERP which is elicited by electrical stimulation of the peripheral nerves. SEP
    signals conclude a series of amplitude deflection that can be elicited by virtually
    any sensory stimuli.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 躯体感觉诱发电位 (SEP)。^(21)^(21)21 通常，躯体感觉诱发电位简称为 SSEP 或 SEP。本文选择 SEP 作为缩写，以避免与稳态诱发电位
    (SSEP) 冲突。躯体感觉诱发电位是 ERP 的另一个常用子类别，通过对外周神经的电刺激诱发。SEP 信号包括一系列幅度偏移，可以被几乎任何感官刺激所引发。
- en: '![Refer to caption](img/630309cd8712b0b41e52e5dae5c8873a.png)'
  id: totrans-1023
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/630309cd8712b0b41e52e5dae5c8873a.png)'
- en: (a) ERP components
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ERP 组件
- en: '![Refer to caption](img/e2d2854dac31d649e55a58c9924e818d.png)'
  id: totrans-1025
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e2d2854dac31d649e55a58c9924e818d.png)'
- en: (b) P300 speller
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: (b) P300 拼写器
- en: 'Figure 6: P300 waves and visual-based P300 speller [[250](#bib.bib250)].'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：P300 波形和基于视觉的 P300 拼写器 [[250](#bib.bib250)]。
- en: 'P300. P300 (also called P3) is an important component in ERP [[251](#bib.bib251)].
    Here we introduce P300 signal separately since it is widely-used in brain signal
    analysis. Figure [6(a)](#A1.F6.sf1 "In Figure 6 ‣ A.1.2 Evoked Potential (EP)
    ‣ A.1 Electroencephalography (EEG) ‣ Appendix A Non-invasive Brain Signals ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers") shows the ERP signal fluctuation in the 500 ms after the stimuli
    onset. The waveform mainly concludes five components, P1, N1, P2, N2, and P3\.
    The capital character P/N represents positive/negative electrical potentials.
    The following number refers to the occurrence time of the specific potential.
    Thus, P300 denotes the positive potential of ERP waveform at approximately 300
    ms after the presented stimuli. Compared to other components, P300 has the highest
    amplitude and is easiest to detect. Thus, a large number of brain signal studies
    focus on P300 analysis. P300 is more of an informative feature instead of a type
    of brain signal (e.g., VEP). Therefore, we do no list P300 in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Brain Imaging Techniques ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"). P300 can be analyzed in most
    of ERP signals such as VEP, AEP, SEP.'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: P300。P300（也称为P3）是ERP的重要组成部分[[251](#bib.bib251)]。由于它在脑信号分析中广泛应用，这里单独介绍P300信号。图[6(a)](#A1.F6.sf1
    "在图6 ‣ A.1.2 诱发电位 (EP) ‣ A.1 脑电图 (EEG) ‣ 附录A 非侵入性脑信号 ‣ 基于深度学习的非侵入性脑信号调查：最新进展与新前沿")展示了刺激开始后500毫秒内的ERP信号波动。该波形主要包括五个成分：P1、N1、P2、N2和P3。大写字母P/N代表正/负电位。接下来的数字指的是特定电位的出现时间。因此，P300表示刺激呈现后约300毫秒的ERP波形中的正电位。与其他成分相比，P300具有最高的振幅且最易检测。因此，大量脑信号研究集中于P300分析。P300更多的是一个信息性特征，而不是一种脑信号（例如，VEP）。因此，我们在图[2](#S2.F2
    "图2 ‣ 2 脑成像技术 ‣ 基于深度学习的非侵入性脑信号调查：最新进展与新前沿")中没有列出P300。P300可以在大多数ERP信号中进行分析，例如VEP、AEP、SEP。
- en: 'In practice, P300 can be elicited by rare, task-relevant events in an ‘oddball’
    paradigm (e.g., P300 speaker). In the oddball paradigm, the subject receives a
    series of stimuli where low-probability target items are mixed with high-probability
    non-target items. Visual and auditory stimuli are the most commonly used in the
    oddball paradigm. Figure [6(b)](#A1.F6.sf2 "In Figure 6 ‣ A.1.2 Evoked Potential
    (EP) ‣ A.1 Electroencephalography (EEG) ‣ Appendix A Non-invasive Brain Signals
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") shows an example of visual-based P300 speller which enables
    the subject the spell letters/numbers directly through brain signals [[250](#bib.bib250)].
    The 26 letters of the alphabet and the Arabic numbers are displayed on a computer
    screen which serves as the keyboard. The subject focuses attention successively
    on the characters they wish to spell. The computer detects the chosen character
    online in real time. This detection is achieved by repeatedly flashing rows and
    columns of the matrix. When the elements containing the selected characters are
    flashing, a P300 fluctuation is elicited. In the $6\times 6$ matrix screen, the
    rows and columns flash in mixed random order. The flash duration and interval
    among adjacent flashes are generally set as 100 ms [[252](#bib.bib252)]. The columns
    and rows flash separately. First, the columns flash six times with each column
    flashing one time. Second, the rows will flash for six times. After that, this
    paradigm repeats for several times (e.g., $N$ times). The P300 signals of the
    total $12N$ flash will be analyzed to output a single outcome (i.e., one letter/number).'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，P300可以通过‘异常’范式中的稀有任务相关事件引发（例如，P300拼写器）。在异常范式中，受试者接收到一系列刺激，其中低概率的目标项与高概率的非目标项混合。视觉和听觉刺激是异常范式中最常用的刺激。图[6(b)](#A1.F6.sf2
    "在图6 ‣ A.1.2 诱发电位 (EP) ‣ A.1 脑电图 (EEG) ‣ 附录A 非侵入式脑信号 ‣ 基于深度学习的非侵入式脑信号调查：近期进展与新前沿")展示了一个视觉基础的P300拼写器示例，该示例使受试者可以通过脑信号直接拼写字母/数字[[250](#bib.bib250)]。计算机屏幕上显示了26个字母和阿拉伯数字，作为键盘。受试者依次将注意力集中在他们希望拼写的字符上。计算机实时检测所选择的字符。这种检测通过反复闪烁矩阵的行和列来实现。当包含所选字符的元素闪烁时，会引发P300波动。在$6\times
    6$矩阵屏幕中，行和列以混合随机顺序闪烁。闪烁持续时间和相邻闪烁之间的间隔通常设置为100毫秒[[252](#bib.bib252)]。列和行分别闪烁。首先，列闪烁六次，每列闪烁一次。然后，行闪烁六次。之后，这个范式会重复几次（例如，$N$次）。对总共$12N$次闪烁的P300信号进行分析，以输出一个结果（即一个字母/数字）。
- en: 'Steady State Evoked Potentials (SSEP). Steady State Evoked Potential is another
    subcategory of evoked potentials, which are periodic cortical responses evoked
    by certain repetitive stimuli with a constant frequency. It has been demonstrated
    that the brain oscillations generally maintain a steady level over time while
    the potentials are evoked by steady state stimuli (e.g., a flickering light with
    fixed frequency). Technically, SSEP is defined as a form of response to repetitive
    sensory stimulation in which the constituent frequency components of the response
    remain constant over time in both amplitude and phase [[37](#bib.bib37)]. Depending
    on the type of stimuli, SSEP divides into three subcategories: Steady-State Visually
    Evoked Potentials (SSVEP), Steady-State Auditory Evoked Potentials (SSAEP), and
    Steady-State Somatosensory Evoked Potentials (SSSEP). In the brain signal area,
    most studies are focused on visual evoked steady potentials, and only rarely do
    papers focus on auditory and somatosensory stimuli. Therefore, in this survey,
    we mainly introduce SSVEP rather than SSAEP and SSSEP.'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 稳态诱发电位（SSEP）。稳态诱发电位是诱发电位的另一子类别，是由特定重复刺激以恒定频率引发的周期性皮层反应。研究表明，大脑振荡在稳态刺激（例如，固定频率的闪烁光）引发的电位中通常保持稳定水平。从技术上讲，SSEP被定义为对重复感官刺激的一种响应形式，其中响应的组成频率分量在幅度和相位上随时间保持不变[[37](#bib.bib37)]。根据刺激类型，SSEP分为三个子类别：稳态视觉诱发电位（SSVEP）、稳态听觉诱发电位（SSAEP）和稳态体感诱发电位（SSSEP）。在脑信号领域，大多数研究集中在视觉诱发稳态电位上，听觉和体感刺激的研究则很少。因此，在这项调查中，我们主要介绍SSVEP，而非SSAEP和SSSEP。
- en: 'Commonly Used Visual-Related Potentials. Visual evoked potentials are the most
    common used potentials. Therefore, it is essential to distinguish the three different
    visual evoked potential paradigms: VEP, RSVP, SSVEP. Here, we theoretically introduce
    the characteristics of each paradigm and then give three demonstration videos
    to provide a better understanding. First, the frequencies are different: the frequency
    of VEP is less than 2Hz while the frequency of RSVP is around 10Hz, and the frequency
    of SSVEP ranges from 3.5 to 75Hz. Second, they have various presentation protocols.
    In the VEP paradigm, different visual patterns will be presented on the screen
    to check the user’s brain signals changes. For instance, in this video^(22)^(22)22https://www.youtube.com/watch?v=iUW_l5YAEEM,
    the image pattern is full of the screen and changes dramatically. In RSVP diagram,
    several items will be presented on a screen one-by-one. All the items are shown
    in the same place and share the same frequency. For example, the video^(23)^(23)23https://www.youtube.com/watch?v=5yddeRrd0hA&t=36s
    shows an RSVP scenario which is called speed reading. In SSVEP paradigm, several
    items will be presented on a screen at the same time while the items are shown
    at variant positions with different frequencies. For example, in this demonstration
    video^(24)^(24)24https://www.youtube.com/watch?v=t96rl1SFHlI, there are four circles
    distributed on the up, down, left, and right sides of a screen and the frequency
    of each item differs from each other.'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的视觉相关电位。视觉诱发电位是最常用的电位。因此，区分三种不同的视觉诱发电位范式：VEP、RSVP、SSVEP是必要的。在这里，我们理论上介绍每个范式的特点，然后提供三个演示视频以便更好地理解。首先，频率不同：VEP的频率低于2Hz，而RSVP的频率约为10Hz，SSVEP的频率范围为3.5到75Hz。其次，它们具有不同的呈现协议。在VEP范式中，将在屏幕上呈现不同的视觉图案以检查用户脑信号的变化。例如，在这个视频^(22)^(22)22https://www.youtube.com/watch?v=iUW_l5YAEEM中，图像图案充满了屏幕并且变化剧烈。在RSVP图中，几个项目将一个接一个地呈现在屏幕上。所有项目都显示在相同的位置，并共享相同的频率。例如，视频^(23)^(23)23https://www.youtube.com/watch?v=5yddeRrd0hA&t=36s展示了一个被称为速读的RSVP场景。在SSVEP范式中，多个项目将同时呈现在屏幕上，同时这些项目显示在不同的位置，具有不同的频率。例如，在这个演示视频^(24)^(24)24https://www.youtube.com/watch?v=t96rl1SFHlI中，四个圆圈分布在屏幕的上下左右四个位置，每个项目的频率彼此不同。
- en: A.2 Functional Near-infrared Spectroscopy (fNIRS)
  id: totrans-1032
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 功能性近红外光谱（fNIRS）
- en: Functional near-infrared spectroscopy (fNIRS) is a non-invasive functional neuro-imaging
    technology using near-infrared (NIR) light [[38](#bib.bib38)]. In specific, fNIRS
    employs NIR light to measure the aggregation degree of oxygenated hemoglobin (Hb)
    and deoxygenated-hemoglobin (deoxy-Hb) because Hb and deoxy-Hb have higher absorbence
    of light than other head components such as the skull and scalp. fNIRS relies
    on blood-oxygen-level-dependent (BOLD) response or hemodynamic response to form
    a functional neuro-image. The BOLD response can detect the oxygenated or deoxygenated
    blood level in the brain blood. The relative levels reflect the blood flow and
    neural activation, where increased blood flow implies a higher metabolic demand
    caused by active neurons. For example, when the user is concentrating on a mental
    task, the prefrontal cortex neurons will be activated, and the BOLD response in
    the prefrontal cortex area will be stronger [[200](#bib.bib200)].
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: 功能性近红外光谱（fNIRS）是一种使用近红外（NIR）光的非侵入性功能性神经成像技术[[38](#bib.bib38)]。具体而言，fNIRS利用NIR光测量氧合血红蛋白（Hb）和去氧血红蛋白（deoxy-Hb）的聚集程度，因为Hb和deoxy-Hb对光的吸收高于其他头部成分，如颅骨和头皮。fNIRS依赖于血氧水平依赖（BOLD）反应或血流动力学反应来形成功能性神经图像。BOLD反应可以检测大脑血液中的氧合或去氧血液水平。相对水平反映了血流和神经激活，其中血流增加意味着活跃神经元导致的代谢需求增加。例如，当用户集中注意力进行思维任务时，前额叶皮层神经元将被激活，前额叶皮层区域的BOLD反应将更强[[200](#bib.bib200)]。
- en: 'Single or multiple emitter-detector pairs measure the Hb and deoxy-Hb: the
    emitter transmits NIR light through the blood vessels to the detector. Most existing
    studies use fNIRS technologies to measure the status of prefrontal and motor cortex.
    The former response to mental tasks and music/image imagery while the latter is
    a response to motor-related tasks (e.g., motor imagery). The monitored Hb and
    deoxy-Hb change slowly since the blood speed varies in a relatively slow ratio
    compared to electrical signals. Temporal resolution refers to the smallest time
    of neural activity reliably separated by the signal. The fNIRS has lower temporal
    resolution compared with electrical or magnetic signals. The spatial resolution
    depends on the number of emitter-detector pairs. In current studies, three emitters
    and eight detectors would suffice for adequately acquiring the prefrontal cortex
    signals; and six emitters and six detectors would suffice for covering the motor
    cortex area [[29](#bib.bib29)]. fNIRS has a drawback in that it cannot be used
    to measure cortical activity occurring deeper than 4cm in the brain, due to the
    limitations in light emitter power and spatial resolution.'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 单个或多个发射器-探测器对测量 Hb 和脱氧 Hb：发射器通过血管向探测器传输近红外（NIR）光。大多数现有研究使用功能性近红外光谱（fNIRS）技术来测量前额叶和运动皮层的状态。前者对心理任务和音乐/图像想象做出反应，而后者对与运动相关的任务（例如运动想象）做出反应。监测的
    Hb 和脱氧 Hb 变化较慢，因为血流速度与电信号相比变化较慢。时间分辨率指的是信号可靠区分的最小神经活动时间。fNIRS 的时间分辨率低于电信号或磁信号。空间分辨率取决于发射器-探测器对的数量。在当前研究中，三个发射器和八个探测器足以充分获取前额叶皮层信号；六个发射器和六个探测器足以覆盖运动皮层区域
    [[29](#bib.bib29)]。fNIRS 的一个缺点是它无法测量大脑深于 4 厘米的皮层活动，这是由于光发射器功率和空间分辨率的限制。
- en: A.3 Functional Magnetic Resonance Imaging (fMRI)
  id: totrans-1035
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 功能性磁共振成像（fMRI）
- en: Functional magnetic resonance imaging (fMRI) monitors brain activities by detecting
    changes associated with blood flow in brain areas [[14](#bib.bib14)]. Similar
    to fNIRS, fMRI relies on the BOLD response. The main differences between fNIRS
    and fMRI are as follows [[24](#bib.bib24)]. First, as the name implies, fMRI measures
    BOLD response through magnetic instead of optical methods. Hemoglobin differs
    in how it responds to magnetic fields, depending on whether it has a bound oxygen
    molecule. The magnetic fields are more sensitive to and are more easily distorted
    by deoxy-Hb than Hb molecules. Second, the magnetic fields have higher penetration
    than NIR light, which gives fMRI greater ability to capture information from deep
    parts of the brain than fNIRS. Third, fMRI has a higher spatial resolution than
    fNIRS since the latter’s spatial resolution is limited by the emitter-detector
    pairs. However, the temporal resolutions of fMRI and fNIRS are at an equal level
    because they both constrained by the blood flow speed.
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 功能性磁共振成像（fMRI）通过检测与大脑区域血流相关的变化来监测大脑活动 [[14](#bib.bib14)]。与 fNIRS 类似，fMRI 也依赖于
    BOLD 响应。fNIRS 和 fMRI 之间的主要区别如下 [[24](#bib.bib24)]。首先，顾名思义，fMRI 通过磁性而非光学方法测量 BOLD
    响应。血红蛋白对磁场的响应有所不同，具体取决于是否有结合的氧分子。磁场对脱氧 Hb 比对 Hb 分子更为敏感，且更容易被扭曲。其次，磁场的穿透力高于 NIR
    光，这使得 fMRI 比 fNIRS 更能捕捉大脑深部的信息。第三，由于 fNIRS 的空间分辨率受限于发射器-探测器对，fMRI 的空间分辨率高于 fNIRS。然而，fMRI
    和 fNIRS 的时间分辨率水平相等，因为它们都受到血流速度的限制。
- en: 'fMRI has several flaws compared to fNIRS: 1) fMRI requires an expensive scanner
    to generate magnetic fields; 2) the scanner is heavy and has poor portability.
    In order to measure the signal of interest, CNR (Contrast-to-Noise Ratio) has
    been investigated to measure the image quality of fMRI because researchers are
    more interested in the contrast between images rather than the raw images. So
    for fMRI data, using the CNR of the time series instead of (t)SNR is more preferred
    because CNR compares a measure of the activation fluctuations to the noise [[253](#bib.bib253)].'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于 fNIRS，fMRI 存在几个缺陷：1) fMRI 需要昂贵的扫描仪来生成磁场；2) 扫描仪重且便携性差。为了测量感兴趣的信号，已经研究了 CNR（对比噪声比）来衡量
    fMRI 的图像质量，因为研究人员更关注图像之间的对比度而不是原始图像。因此，对于 fMRI 数据，更倾向于使用时间序列的 CNR 而不是 (t)SNR，因为
    CNR 比较激活波动的度量与噪声 [[253](#bib.bib253)]。
- en: A.4 Magnetoencephalography (MEG)
  id: totrans-1038
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 磁脑电图（MEG）
- en: Magnetoencephalography (MEG) is a functional neuroimaging technique for mapping
    brain activity by recording magnetic fields produced by electrical currents occurring
    naturally in the brain, using very sensitive magnetometers [[254](#bib.bib254)].
    The ionic currents of active neurons will create weak magnetic fields. The generated
    magnetic fields can be measured by magnetometers like SQUIDs (superconducting
    quantum interference devices). However, producing a detectable magnetic field
    requires massive (e.g., 50,000) active neurons with similar orientation. The source
    of the magnetic field measured by MEG is the pyramidal cells which are perpendicular
    to the cortex surface.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 磁脑电图（MEG）是一种功能性神经影像学技术，通过记录大脑中自然发生的电流产生的磁场来绘制大脑活动图像，使用非常灵敏的磁力计 [[254](#bib.bib254)]。活跃神经元的离子电流会产生微弱的磁场。这些生成的磁场可以通过像
    SQUID（超导量子干涉装置）这样的磁力计来测量。然而，产生可检测的磁场需要大量（例如 50,000 个）具有相似方向的活跃神经元。MEG 测量的磁场源是垂直于皮层表面的锥体细胞。
- en: MEG has a relatively low spatial resolution since the signal quality highly
    depends on the measurement factors (e.g., brain area, neuron orientations, neuron
    depth). However, MEG can provide very high temporal resolution ($\geq$1000Hz)
    since MEG directly monitors the brain activity from the neuron level, which is
    in the same level of intracortical signals. MEG equipment is expensive and not
    portable which limits its real-world deployment.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: MEG 的空间分辨率相对较低，因为信号质量高度依赖于测量因素（例如，大脑区域、神经元方向、神经元深度）。然而，MEG 能提供非常高的时间分辨率（$\geq$1000Hz），因为
    MEG 直接从神经元层面监测大脑活动，这与皮层内信号处于相同层次。MEG 设备昂贵且不便携，这限制了其在实际应用中的部署。
- en: Appendix B Basic Deep Learning in Brain Signal Analysis
  id: totrans-1041
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 大脑信号分析中的基础深度学习
- en: In this part, we will give relative detail introduction of various deep learning
    models for the reason that a part of the potential readers who are from non-computer
    area (e.g., biomedical) are not familiar to deep learning.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将详细介绍各种深度学习模型，因为一部分潜在读者（如生物医学领域的读者）对深度学习不太熟悉。
- en: For simplification, we first define an operation $\mathcal{T}(\cdot)$ as
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们首先定义一个操作 $\mathcal{T}(\cdot)$ 为
- en: '|  | $\mathcal{T}(\bm{x})=\bm{w}*\bm{x}+\bm{b}$ |  | (1) |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{T}(\bm{x})=\bm{w}*\bm{x}+\bm{b}$ |  | (1) |'
- en: '|  | $\mathcal{T}(\bm{x},\bm{x^{\prime}})=\bm{w}*\bm{x}+\bm{b}+\bm{w^{\prime}}*\bm{x^{\prime}}+\bm{b^{\prime}}$
    |  | (2) |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{T}(\bm{x},\bm{x^{\prime}})=\bm{w}*\bm{x}+\bm{b}+\bm{w^{\prime}}*\bm{x^{\prime}}+\bm{b^{\prime}}$
    |  | (2) |'
- en: where $\bm{x}$ and $\bm{x^{\prime}}$ denote two variables while $\bm{w}$, $\bm{w^{\prime}}$,
    $\bm{b}$, and $\bm{b^{\prime}}$ denote the corresponding weights and basis.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{x}$ 和 $\bm{x^{\prime}}$ 表示两个变量，而 $\bm{w}$、$\bm{w^{\prime}}$、$\bm{b}$
    和 $\bm{b^{\prime}}$ 表示相应的权重和基。
- en: B.1 Discriminative Deep Learning Models
  id: totrans-1047
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 判别性深度学习模型
- en: 'Since the main task of brain signal analysis is brain signal recognition, the
    discriminative deep learning models are the most popular and powerful algorithms.
    Suppose we have a dataset of brain signal samples $\{\mathbb{X},\mathbb{Y}\}$
    where $\mathbb{X}$ denotes the set of brain signal observations and $\mathbb{Y}$
    denotes the set of sample ground truth (i.e., labels). Suppose an specific sample-label
    pair $\{\bm{x}\in\mathbb{R}^{N},\bm{y}\in\mathbb{R}^{M}\}$ where $N$ and $M$ denote
    the dimension of observations and the number of sample categories, respectively.
    The aim of discriminative deep learning models is to learn a function with the
    mapping: $\bm{x}\rightarrow\bm{y}$. In short, the discriminative models receive
    the input data and output the corresponding category or label. All the discriminative
    models introduced in this section are supervised learning techniques which require
    the information of both the observations and the ground truth.'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: 由于脑信号分析的主要任务是脑信号识别，因此判别性深度学习模型是最流行和强大的算法。假设我们有一个脑信号样本数据集 $\{\mathbb{X},\mathbb{Y}\}$，其中
    $\mathbb{X}$ 表示脑信号观测值的集合，$\mathbb{Y}$ 表示样本真实标签的集合（即标签）。假设一个具体的样本-标签对 $\{\bm{x}\in\mathbb{R}^{N},\bm{y}\in\mathbb{R}^{M}\}$，其中
    $N$ 和 $M$ 分别表示观测值的维度和样本类别的数量。判别性深度学习模型的目标是学习一个映射函数：$\bm{x}\rightarrow\bm{y}$。简而言之，判别性模型接收输入数据并输出相应的类别或标签。本节介绍的所有判别性模型都是监督学习技术，需要观测值和真实标签的信息。
- en: B.1.1 Multi-Layer Perceptron (MLP)
  id: totrans-1049
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.1 多层感知器（MLP）
- en: 'The most basic neural network is fully-connected neural networks (Figure [7(a)](#A2.F7.sf1
    "In Figure 7 ‣ B.1.2 Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep
    Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers")) which only contains one hidden layer. The input layer receives
    the raw data or extracted features of brain signals while the output layer shows
    the classification results. The term ‘fully-connected’ denotes each node in a
    specific layer is connected with all the nodes in the previous and next layer.
    This network is too ‘shallow‘ and generally not regarded as ‘deep‘ neural networks.'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的神经网络是全连接神经网络（图 [7(a)](#A2.F7.sf1 "图 7 ‣ B.1.2 循环神经网络 (RNN) ‣ B.1 判别深度学习模型
    ‣ 附录 B 基本深度学习在脑信号分析中的应用 ‣ 基于深度学习的非侵入性脑信号：近期进展与新前沿")），它只包含一个隐藏层。输入层接收原始数据或提取的脑信号特征，而输出层显示分类结果。‘全连接’这个术语表示特定层中的每个节点都与前一层和下一层中的所有节点连接。该网络过于‘浅层’，通常不被认为是‘深层’神经网络。
- en: 'Multilayer Perceptron is the simplest and the most basic deep learning model.
    The key difference between MLP and the fully-connected neural network is that
    MLP has more than one hidden layers. All the nodes are fully-connected with the
    nodes of the adjacent layers but without connection with the other nodes of the
    same layer. MLP includes multiple hidden layers. As shown in Figure [7(b)](#A2.F7.sf2
    "In Figure 7 ‣ B.1.2 Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep
    Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers"), we take a structure with two hidden layers as an example to describe
    the data flow in MLP.'
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机是最简单、最基本的深度学习模型。MLP 和全连接神经网络之间的主要区别在于 MLP 拥有多个隐藏层。所有节点都与相邻层的节点完全连接，但与同一层的其他节点没有连接。MLP
    包含多个隐藏层。如图 [7(b)](#A2.F7.sf2 "图 7 ‣ B.1.2 循环神经网络 (RNN) ‣ B.1 判别深度学习模型 ‣ 附录 B 基本深度学习在脑信号分析中的应用
    ‣ 基于深度学习的非侵入性脑信号：近期进展与新前沿") 所示，我们以具有两个隐藏层的结构为例，描述 MLP 中的数据流。
- en: The input layer receives the observation $\bm{x}$ and feeds forward to the first
    hidden layer,
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层接收观测 $\bm{x}$ 并向前传递到第一个隐藏层，
- en: '|  | $\bm{x^{h1}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (3) |'
  id: totrans-1053
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{h1}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (3) |'
- en: where $\bm{x^{h1}}$ denotes the data flow in the first hidden layer and $\sigma$
    represents the non-linear activation function. There are several commonly used
    activation functions such as sigmoid/Logistic, Tanh, ReLU, we choose sigmoid activation
    function as an example in this section. Then, the data flow to the second hidden
    layer and the output layer,
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{x^{h1}}$ 表示第一个隐藏层中的数据流，$\sigma$ 代表非线性激活函数。有几种常用的激活函数，如 sigmoid/Logistic、Tanh、ReLU，本节选择
    sigmoid 激活函数作为示例。然后，数据流向第二个隐藏层和输出层，
- en: '|  | $\bm{x^{h2}}=\sigma(\mathcal{T}(\bm{x^{h1}}))$ |  | (4) |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{h2}}=\sigma(\mathcal{T}(\bm{x^{h1}}))$ |  | (4) |'
- en: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (5) |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (5) |'
- en: where $\bm{y^{\prime}}$ denotes the predict results in one-hot format. The error
    (i.e., loss) could be calculated based on the distance between $\bm{y^{\prime}}$
    and the ground truth $\bm{y}$. For instance, the Euclidean-distance based error
    can be calculated by
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{y^{\prime}}$ 表示一热编码格式的预测结果。误差（即损失）可以根据 $\bm{y^{\prime}}$ 和真实值 $\bm{y}$
    之间的距离进行计算。例如，可以通过以下方式计算基于欧几里得距离的误差：
- en: '|  | $error=\left\&#124;\bm{y^{\prime}}-\bm{y}\right\&#124;_{2}$ |  | (6) |'
  id: totrans-1058
  prefs: []
  type: TYPE_TB
  zh: '|  | $error=\left\|\bm{y^{\prime}}-\bm{y}\right\|_{2}$ |  | (6) |'
- en: where $\left\|\cdot\right\|_{2}$ denotes the Euclidean norm. Afterward, the
    error will be back-propagated and optimized by a suitable optimizer. The optimizer
    will adjust all the weights and basis in the model until the error converges.
    The most widely used loss functions includes cross-entropy, negative log likelihood,
    mean square estimation, etc. The most widely used optimizers include Adaptive
    moment estimation (Adam), Stochastic Gradient Descent (SGD), Adagrad (Adaptive
    subgradient method), etc.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\left\|\cdot\right\|_{2}$ 表示欧几里得范数。随后，误差将通过适当的优化器进行反向传播和优化。优化器将调整模型中的所有权重和基准，直到误差收敛。最常用的损失函数包括交叉熵、负对数似然、均方估计等。最常用的优化器包括自适应矩估计（Adam）、随机梯度下降（SGD）、Adagrad（自适应子梯度方法）等。
- en: 'Several terms may be easily confused with each other: Artificial Neural Network
    (ANN), Deep Neural Network (DNN), and MLP. These terms have no strict difference
    and often mixed in literature and commonly used as synonyms. Generally, ANN and
    DNN can be used to describe deep learning models overall, including not only fully-connected
    networks but also other networks (e.g., recurrent, convolutional networks), but
    MLP can only refer to fully-connected network. Additionally, ANN contains all
    the models of neural networks, can be either shallow (one hidden layer) or deep
    (multiple hidden layers) while DNN doesn’t cover shallow neural network [[30](#bib.bib30),
    [31](#bib.bib31)].'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 一些术语可能容易混淆：人工神经网络（ANN）、深度神经网络（DNN）和 MLP。这些术语没有严格的区别，文献中常常混用，并且通常用作同义词。通常，ANN
    和 DNN 可以用来描述深度学习模型的整体，包括不仅限于完全连接网络，还有其他网络（例如，递归网络、卷积网络），但 MLP 仅指完全连接网络。此外，ANN
    包含所有神经网络模型，可以是浅层的（一个隐藏层）或深层的（多个隐藏层），而 DNN 不包括浅层神经网络 [[30](#bib.bib30), [31](#bib.bib31)]。
- en: B.1.2 Recurrent Neural Networks (RNN)
  id: totrans-1061
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.2 循环神经网络（RNN）
- en: 'Recurrent Neural Network is a specific subclass of discriminative deep learning
    model which are designed to capture temporal dependencies among input data [[41](#bib.bib41)].
    Figure [8(a)](#A2.F8.sf1 "In Figure 8 ‣ B.1.2 Recurrent Neural Networks (RNN)
    ‣ B.1 Discriminative Deep Learning Models ‣ Appendix B Basic Deep Learning in
    Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers") describes the activity of a specific RNN node
    in the time domain. At each time ranges from $[1,t+1]$, the node receives an input
    $I$ (the subscript represents the specific time) and a hidden state $c$ from the
    previous time (except the first time). For instance, at time $t$ it receives not
    only the input $I_{t}$ but also the hidden state of the previous node $c_{t-1}$.
    The hidden state can be regarded as the ‘memory’ of the nodes which can help the
    RNN ‘remember’ the historical input.'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是一个特定的判别深度学习模型子类，旨在捕捉输入数据之间的时间依赖关系 [[41](#bib.bib41)]。图 [8(a)](#A2.F8.sf1
    "图 8 ‣ B.1.2 循环神经网络（RNN） ‣ B.1 判别深度学习模型 ‣ 附录 B 脑信号分析中的基础深度学习 ‣ 基于深度学习的非侵入性脑信号：最新进展与新前沿")
    描述了特定 RNN 节点在时间域的活动。在每个时间范围 $[1,t+1]$ 内，该节点接收一个输入 $I$（下标表示具体时间）和来自前一个时间的隐藏状态 $c$（除了第一次）。例如，在时间
    $t$ 时，它不仅接收输入 $I_{t}$，还接收前一个节点的隐藏状态 $c_{t-1}$。隐藏状态可以看作是节点的“记忆”，它可以帮助 RNN “记住”历史输入。
- en: 'Next, we will report two typical RNN architectures which have attracted much
    attention and achieved great success: long short-term memory and gated recurrent
    units. They both follow the basic principles of RNN, and we will pay our attention
    to the complicated internal structures in each node. Since the structure is much
    more complicated than general neural nodes, we call it a ‘cell.’ Cells in RNN
    are equivalent to nodes in MLP.'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将报告两种典型的 RNN 架构，这两种架构引起了广泛关注并取得了巨大成功：长短期记忆网络和门控递归单元。它们都遵循 RNN 的基本原理，我们将关注每个节点内部复杂的结构。由于这些结构比一般的神经节点复杂得多，我们称之为“单元”。RNN
    中的单元相当于 MLP 中的节点。
- en: '![Refer to caption](img/4ae29fb752f390e9724b867b6c19e3ca.png)'
  id: totrans-1064
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4ae29fb752f390e9724b867b6c19e3ca.png)'
- en: (a) Fully-connected neural network
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 完全连接神经网络
- en: '![Refer to caption](img/0a620f9b43ec7601181079a21d5b2780.png)'
  id: totrans-1066
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0a620f9b43ec7601181079a21d5b2780.png)'
- en: (b) MLP
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MLP
- en: 'Figure 7: Illustration of standard neural network and multilayer perceptron.
    (a) The basic structure of the fully-connected neural network. The input layer
    receives the raw data or extracted features of brain signals while the output
    layer shows the classification results. The term ‘fully-connected’ denotes each
    node in a specific layer is connected with all the nodes in the previous and next
    layer. (b) MLP could have multiple hidden layers, the more, the deeper. This is
    an example of MLP with two hidden layers, which is the simplest MLP model.'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：标准神经网络和多层感知器的示意图。 (a) 完全连接神经网络的基本结构。输入层接收原始数据或提取的脑信号特征，而输出层显示分类结果。术语“完全连接”表示特定层中的每个节点都与上一层和下一层的所有节点连接。
    (b) MLP 可以有多个隐藏层，层数越多，网络越深。这是一个具有两个隐藏层的 MLP 示例，它是最简单的 MLP 模型。
- en: 'Long Short-Term Memory (LSTM). Figure [9(a)](#A2.F9.sf1 "In Figure 9 ‣ B.1.2
    Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep Learning Models ‣ Appendix
    B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers") shows the structure
    of a single LSTM cell at time $t$ [[255](#bib.bib255)]. The LSTM cell has three
    inputs ($I_{t}$, $O_{t-1}$, and $c_{t-1}$) and two outputs ($c_{t}$ and $O_{t}$).
    The operation is as follows:'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）。图 [9(a)](#A2.F9.sf1 "在图 9 ‣ B.1.2 递归神经网络 (RNN) ‣ B.1 判别深度学习模型 ‣
    附录 B 基本的脑信号深度学习 ‣ 关于基于深度学习的非侵入性脑信号的调查：最新进展和新前沿") 显示了时间 $t$ 时单个 LSTM 单元的结构 [[255](#bib.bib255)]。LSTM
    单元具有三个输入（$I_{t}$、$O_{t-1}$ 和 $c_{t-1}$）和两个输出（$c_{t}$ 和 $O_{t}$）。操作如下：
- en: '|  | $I_{t},O_{t-1},c_{t-1}\rightarrow c_{t},O_{t}$ |  | (7) |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{t},O_{t-1},c_{t-1}\rightarrow c_{t},O_{t}$ |  | (7) |'
- en: '$I_{t}$ denotes the input value at time $t$, $O_{t-1}$ denotes the output at
    the previous time (i.e., time $t-1$), and $c_{t-1}$ denotes the hidden state at
    the previous time. $c_{t}$ and $O_{t}$ separately denote the hidden state and
    the output at time $t$. Therefore, we can observe that the output $O_{t}$ at time
    $t$ not only related to the input $I_{t}$ but also related to the information
    at the previous time. In this way, LSTM is empowered to remember the important
    information in the time domain. Moreover, the essential idea of LSTM is to control
    the memory of specific information. For this aim, LSTM cell adopts four gates:
    the input gate, forget gate, output gate, and input modulation gate. Each gate
    is a weight to control how much information can flow through this gate. For example,
    if the weight of the forget gate is zero, the LSTM cell would remember all the
    information passed from the previous time $t-1$; if the weight is one, the LSTM
    cell would remember nothing. The corresponding activation function determines
    the weight. The detailed data flow as follows:'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: $I_{t}$ 表示时间 $t$ 的输入值，$O_{t-1}$ 表示前一时间（即时间 $t-1$）的输出，$c_{t-1}$ 表示前一时间的隐藏状态。$c_{t}$
    和 $O_{t}$ 分别表示时间 $t$ 的隐藏状态和输出。因此，我们可以观察到时间 $t$ 的输出 $O_{t}$ 不仅与输入 $I_{t}$ 相关，还与前一时间的信息相关。这样，LSTM
    能够在时间域内记住重要信息。此外，LSTM 的基本思想是控制特定信息的记忆。为此，LSTM 单元采用了四个门：输入门、遗忘门、输出门和输入调制门。每个门是一个权重，用于控制多少信息可以通过该门。例如，如果遗忘门的权重为零，LSTM
    单元将记住从前一时间 $t-1$ 传递来的所有信息；如果权重为一，LSTM 单元将什么都不记住。相应的激活函数决定了权重。详细的数据流如下：
- en: '|  | $f=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (8) |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
  zh: '|  | $f=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (8) |'
- en: '|  | $i=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (9) |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '|  | $i=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (9) |'
- en: '|  | $o=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (10) |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
  zh: '|  | $o=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (10) |'
- en: '|  | $m=tanh(\mathcal{T}(I_{t},O_{t-1}))$ |  | (11) |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
  zh: '|  | $m=tanh(\mathcal{T}(I_{t},O_{t-1}))$ |  | (11) |'
- en: '|  | $c_{t}=f*c_{t-1}+i*m$ |  | (12) |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{t}=f*c_{t-1}+i*m$ |  | (12) |'
- en: '|  | $h_{t}=o*tanh(c_{t})$ |  | (13) |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{t}=o*tanh(c_{t})$ |  | (13) |'
- en: where $i$, $f$, $o$ and $m$ represent the input gate, forget gate, output gate
    and input modulation gate, respectively.
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $i$、$f$、$o$ 和 $m$ 分别表示输入门、遗忘门、输出门和输入调制门。
- en: 'Gated Recurrent Units (GRU). Another widely used RNN architecture is GRU [[256](#bib.bib256)].
    Similar to LSTM, GRU attempts to exploit the information from the past. GRU does
    not require hidden states, however, it receives temporal information only from
    the output of time $t-1$. Thus, as shown in Figure [9(b)](#A2.F9.sf2 "In Figure
    9 ‣ B.1.2 Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep Learning Models
    ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers"), GRU has two inputs
    ($I_{t}$ and $O_{t-1}$) and one output ($O_{t}$). The mapping can be described
    as:'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 门控递归单元（GRU）。另一种广泛使用的 RNN 结构是 GRU [[256](#bib.bib256)]。类似于 LSTM，GRU 尝试利用来自过去的信息。GRU
    不需要隐藏状态，但它仅从时间 $t-1$ 的输出中接收时间信息。因此，如图 [9(b)](#A2.F9.sf2 "在图 9 ‣ B.1.2 递归神经网络 (RNN)
    ‣ B.1 判别深度学习模型 ‣ 附录 B 基本的脑信号深度学习 ‣ 关于基于深度学习的非侵入性脑信号的调查：最新进展和新前沿") 所示，GRU 具有两个输入（$I_{t}$
    和 $O_{t-1}$）和一个输出（$O_{t}$）。这个映射可以描述为：
- en: '|  | $I_{t},O_{t-1}\rightarrow O_{t}$ |  | (14) |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{t},O_{t-1}\rightarrow O_{t}$ |  | (14) |'
- en: 'GRU contains two gates: reset gate $r$ and update gate $z$. The former decides
    how to combine the input with previous memory. The latter decides how much of
    previous memory to keep around, which is similar to the forget gate of LSTM. The
    data flow as follows:'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: GRU包含两个门：重置门$r$和更新门$z$。前者决定如何将输入与先前的记忆结合，后者决定保留多少先前的记忆，这类似于LSTM的遗忘门。数据流如下：
- en: '|  | $z=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (15) |'
  id: totrans-1082
  prefs: []
  type: TYPE_TB
  zh: '|  | $z=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (15) |'
- en: '|  | $r=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (16) |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
  zh: '|  | $r=\sigma(\mathcal{T}(I_{t},O_{t-1}))$ |  | (16) |'
- en: '|  | $\bar{O_{t}}=tanh(\mathcal{T}(I_{t},r*O_{t-1}))$ |  | (17) |'
  id: totrans-1084
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{O_{t}}=tanh(\mathcal{T}(I_{t},r*O_{t-1}))$ |  | (17) |'
- en: '|  | $O_{t}=(1-z)*O_{t-1}+z*\bar{O_{t}}$ |  | (18) |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
  zh: '|  | $O_{t}=(1-z)*O_{t-1}+z*\bar{O_{t}}$ |  | (18) |'
- en: It can be observed that there’s an intermediate variable $\bar{O_{t}}$ which
    is similar to the hidden state of LSTM. However, $\bar{O_{t}}$ only works on this
    time point and unable to pass to the next time point.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到有一个中间变量$\bar{O_{t}}$，它类似于LSTM的隐藏状态。然而，$\bar{O_{t}}$仅在这个时间点起作用，无法传递到下一个时间点。
- en: We here give a brief comparison between LSTM and GRU since they are very similar.
    First, LSTM and GRU have comparable performance as studied by literature. For
    any specific task, it is recommended to try both of them to determine which provides
    better performance. Second, GRU is lightweight since it only has two gates and
    without the hidden state. Therefore, GRU is faster to train and requires few data
    for generalization. Third, in contrast, LSTM generally works better if the training
    dataset is big enough. The reason is that LSTM has better non-linearity than GRU
    since LSTM has two more control gates (input modulation gate and forget gate).
    As a result, LSTM, compared with GRU, is more powerful to discover the latent
    distinct information from large-level training dataset.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里简要比较LSTM和GRU，因为它们非常相似。首先，文献研究表明LSTM和GRU的性能相当。对于任何特定任务，建议尝试两者以确定哪个提供更好的性能。其次，由于GRU只有两个门且没有隐藏状态，因此它比较轻量。GRU训练速度较快，对数据的要求也较少。第三，相比之下，如果训练数据集足够大，LSTM通常表现更好。这是因为LSTM具有比GRU更好的非线性，因为LSTM有两个额外的控制门（输入调制门和遗忘门）。因此，LSTM相较于GRU，更能从大规模训练数据集中发现潜在的不同信息。
- en: '![Refer to caption](img/7921007345feb6ba5e1901eceb7781fa.png)'
  id: totrans-1088
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7921007345feb6ba5e1901eceb7781fa.png)'
- en: (a) RNN
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RNN
- en: '![Refer to caption](img/59b5de172c0da4b202d49b63bbf4385e.png)'
  id: totrans-1090
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/59b5de172c0da4b202d49b63bbf4385e.png)'
- en: (b) CNN
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: (b) CNN
- en: 'Figure 8: Illustration of RNN and CNN models. (a) The recurrent procedure of
    the RNN model. This procedure describes the recurrent procedure of a specific
    node in time range $[1,t+1]$. The node at time $t$ receives two inputs variables
    ($I_{t}$ denotes the input at time $t$ and $c_{t-1}$ denotes the hidden state
    at time $t-1$) and exports two variables (the output $O_{t}$ and the hidden state
    $c_{t}$ at time $t$). (b) The paradigm of CNN model which includes two convolutional
    layers, two pooling layers, and one fully-connected layer.'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：RNN和CNN模型的示意图。 (a) RNN模型的递归过程。该过程描述了时间范围$[1,t+1]$中某一特定节点的递归过程。时间$t$的节点接收两个输入变量（$I_{t}$表示时间$t$的输入，$c_{t-1}$表示时间$t-1$的隐藏状态），并输出两个变量（输出$O_{t}$和时间$t$的隐藏状态$c_{t}$）。
    (b) CNN模型的范式，包括两个卷积层、两个池化层和一个全连接层。
- en: '![Refer to caption](img/80598fb00b532542d4a7f3ad5f7879dd.png)'
  id: totrans-1093
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/80598fb00b532542d4a7f3ad5f7879dd.png)'
- en: (a) Structure of a LSTM cell.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LSTM单元的结构。
- en: '![Refer to caption](img/af96975fd02bc925b90c4f51d86c3692.png)'
  id: totrans-1095
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/af96975fd02bc925b90c4f51d86c3692.png)'
- en: (b) Structure of a GRU cell.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GRU单元的结构。
- en: 'Figure 9: Illustration of detailed LSTM and GRU cell structures. (a) LSTM cell
    receives three inputs ($I_{t}$ denotes the input at time $t$, $O_{t-1}$ denotes
    the output of previous time, and $c_{t-1}$ denotes the hidden state of the previous
    time) and exports two outputs (the output of this time $O_{t}$ and the hidden
    state of this time $c_{t}$). LSTM cell contains four gates in order to control
    the data flow, which are the input gate, output gate, forget gate, and input modulation
    gate. (b) GRU cell receives two inputs (the input of this time $I_{t}$ and the
    output of the previous time $O_{t-1}$) and exports its output $O_{t}$. GRU cell
    only contains two gates which are the reset gate and the update gate. Unlike the
    hidden state $c_{t}$ in LSTM cell, there is no transmittable hidden state in GRU
    cell except one intermediate variable $\bar{O_{t}}$.'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：详细的LSTM和GRU单元结构示意图。(a) LSTM单元接收三个输入（$I_{t}$表示时间$t$的输入，$O_{t-1}$表示前一时间的输出，$c_{t-1}$表示前一时间的隐藏状态）并输出两个结果（当前时间的输出$O_{t}$和当前时间的隐藏状态$c_{t}$）。LSTM单元包含四个门以控制数据流，包括输入门、输出门、遗忘门和输入调节门。(b)
    GRU单元接收两个输入（当前时间的输入$I_{t}$和前一时间的输出$O_{t-1}$）并输出其结果$O_{t}$。GRU单元仅包含两个门，即重置门和更新门。与LSTM单元中的隐藏状态$c_{t}$不同，GRU单元没有可传递的隐藏状态，只有一个中间变量$\bar{O_{t}}$。
- en: B.1.3 Convolutional Neural Networks (CNN)
  id: totrans-1098
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.3 卷积神经网络 (CNN)
- en: Convolutional Neural Networks is one of the most popular deep learning models
    specialized in spatial information exploration [[42](#bib.bib42)]. This section
    will briefly introduce the working mechanism of CNN. CNN is widely used to discover
    the latent spatial information in applications such as image recognition, ubiquitous,
    and object searching due to their salient features such as regularized structure,
    good spatial locality, and translation invariance. In the area of brain signal,
    specifically, CNN is supposed to capture the distinctive dependencies among the
    patterns associated with different brain signals.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是专注于空间信息探索的最流行的深度学习模型之一[[42](#bib.bib42)]。本节将简要介绍CNN的工作机制。CNN因其突出特性，如规整结构、良好的空间局部性和翻译不变性，广泛应用于图像识别、普遍存在和物体搜索等领域。特别是在脑信号领域，CNN旨在捕捉与不同脑信号相关的模式之间的独特依赖关系。
- en: 'We present a standard CNN architecture as shown in Figure [8(b)](#A2.F8.sf2
    "In Figure 8 ‣ B.1.2 Recurrent Neural Networks (RNN) ‣ B.1 Discriminative Deep
    Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers"). The CNN contains one input layer, two convolutional layers with
    each followed by a pooling layer, one fully-connected layer, and one output layer.
    The square patch in each layer shows the processing progress of a specific batch
    of input values. The key to the CNN is to reduce the input data into a form which
    is easier to recognize, with as little information loss as possible. CNN has three
    stacked layers: the convolutional Layer, pooling Layer, and fully-connected Layer.'
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了一个标准的CNN架构，如图[8(b)](#A2.F8.sf2 "在图8 ‣ B.1.2 循环神经网络 (RNN) ‣ B.1 判别性深度学习模型
    ‣ 附录B 基本深度学习在脑信号分析中的应用 ‣ 深度学习基础的非侵入性脑信号：最新进展与新前沿")所示。CNN包含一个输入层、两个卷积层，每个卷积层后面跟一个池化层、一个全连接层和一个输出层。每层中的方形补丁显示了特定输入值批次的处理进度。CNN的关键是将输入数据减少到更易于识别的形式，同时尽可能减少信息丢失。CNN具有三个堆叠层：卷积层、池化层和全连接层。
- en: 'The convolutional layer is the core block of CNN, which contains a set of filters
    to convolve the input data followed by a nonlinear transformation to extract the
    geographical features. In the deep learning implementation, there are several
    key hyper-parameters should be set in the convolutional layer, like the number
    of filters, the size of each filter, etc. The pooling layer generally follows
    the convolutional layer. The pooling layer aims to reduce the spatial size of
    the features progressively. In this way, it can help to decrease the number of
    parameters (e.g., weights and basis) and the computing burden. There are three
    kinds of pooling operation: max, min, average. Take max pooling for example. The
    pooling operation outputs the maximum value of the pooling area as a result. The
    hyper-parameters in the pooling layer includes the pooling operation, the size
    of the pooling area, the strides, etc. In the fully-connected layer, as in the
    basic neural network, the nodes have full connections to all activations in the
    previous layer.'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是 CNN 的核心模块，它包含一组滤波器，用于对输入数据进行卷积，然后进行非线性变换以提取地理特征。在深度学习实现中，卷积层中有几个关键的超参数需要设置，如滤波器的数量、每个滤波器的大小等。池化层通常紧随卷积层之后。池化层旨在逐步减少特征的空间大小。这样，它有助于减少参数（例如权重和基）和计算负担。池化操作有三种类型：最大池化、最小池化、平均池化。以最大池化为例。池化操作将池化区域的最大值作为结果输出。池化层中的超参数包括池化操作、池化区域的大小、步幅等。在全连接层中，与基本神经网络一样，节点与前一层的所有激活都有完全连接。
- en: 'The CNN is the most popular deep learning model in brain signal research, which
    can be used to exploit the latent spatial dependencies among the input brain signals
    like fMRI image, spontaneous EEG, and so on. More details will be reported in
    Section [4](#S4 "4 State-of-The-Art DL Techniques for Brain Signals ‣ A Survey
    on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and New Frontiers").'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是大脑信号研究中最流行的深度学习模型，它可以用来挖掘输入大脑信号（如 fMRI 图像、自发 EEG 等）之间的潜在空间依赖关系。更多详细信息将在第
    [4](#S4 "4 先进的脑信号深度学习技术 ‣ 基于深度学习的非侵入性脑信号：近期进展和新前沿") 节中报告。
- en: B.2 Representative Deep Learning Models
  id: totrans-1103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 代表性的深度学习模型
- en: The term of representative deep learning refers to use deep neural network for
    representation learning. It aims to learn representations of input data that makes
    it easier to perform a downstream task (e.g., classification, generation, and
    clustering) [[257](#bib.bib257)].
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性深度学习一词指的是使用深度神经网络进行表示学习。其目的是学习输入数据的表示，从而使得执行下游任务（如分类、生成和聚类）更容易 [[257](#bib.bib257)]。
- en: The essential blocks of representative deep learning models are autoencoders,
    and restricted Boltzmann machines^(25)^(25)25AE and RBM are generally regarded
    as kind of deep learning although they only have three and two layers, respectively..
    Deep Belief Networks are composed of AE or RBM. The representative models including
    AE, RBM^(26)^(26)26We regard AE, and RBMas representative methods as most researches
    in brain researches adopt them for feature representation., and DBN, are unsupervised
    learning methods. Thus, they can learn the representative features from only the
    input observations $\bm{x}$ without the ground truth $\bm{y}$. In short, representative
    models receive the input data and output a dense representation of the data. There
    are various definitions in different studies for several models (such as DBN,
    Deep RBM, and Deep AE), in this survey, we choose the most understandable definitions
    and will present them in detail in this section.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性深度学习模型的基本模块是自编码器和限制玻尔兹曼机^(25)^(25)25AE 和 RBM 通常被视为深度学习的一种，即使它们分别只有三层和两层。深度置信网络由
    AE 或 RBM 组成。包括 AE、RBM^(26)^(26)26我们将 AE 和 RBM 视为代表性方法，因为大多数脑研究采用它们进行特征表示。和 DBN
    在内的代表性模型都是无监督学习方法。因此，它们可以仅通过输入观测 $\bm{x}$ 学习代表性特征，而无需真实值 $\bm{y}$。简而言之，代表性模型接收输入数据并输出数据的密集表示。不同研究对几种模型（如
    DBN、深度 RBM 和深度 AE）有不同的定义，在本次调查中，我们选择了最易于理解的定义，并将在本节中详细介绍。
- en: '![Refer to caption](img/5db2c0be92218a8d853c16f52534eaca.png)'
  id: totrans-1106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5db2c0be92218a8d853c16f52534eaca.png)'
- en: (a) Autoencoder
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 自编码器
- en: '![Refer to caption](img/fa7588817d9e760e24dee8e516981f64.png)'
  id: totrans-1108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa7588817d9e760e24dee8e516981f64.png)'
- en: (b) RBM
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: (b) RBM
- en: '![Refer to caption](img/0ac17bd242ef2071bb0c12a3ad9ec18a.png)'
  id: totrans-1110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0ac17bd242ef2071bb0c12a3ad9ec18a.png)'
- en: (c) Deep AE
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 深度 AE
- en: '![Refer to caption](img/35a17bbd2dc440865cff3fb6536cf42d.png)'
  id: totrans-1112
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/35a17bbd2dc440865cff3fb6536cf42d.png)'
- en: (d) Deep RBM
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 深度 RBM
- en: 'Figure 10: Illustration of several standard representative deep learning models.
    (a) A basic autoencoder contains one hidden layer. The process from the input
    layer to the hidden layer is an encoder while the process from the hidden layer
    to the output layer is a decoder. (b) Restricted Boltzmann Machine, the encoder
    and the decoder share the same transformation weights. The input layer and the
    output layer are merged into the visible layer. (c) Deep AE with hidden layers.
    Generally, the number of hidden layers is odd, and the middle layer is the learned
    representative features. (d) Deep RBM has one visible layer and multiple hidden
    layers, the last layer is the encoded representation.'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：几种标准代表性的深度学习模型示意图。 (a) 基本自编码器包含一个隐藏层。从输入层到隐藏层的过程是编码器，而从隐藏层到输出层的过程是解码器。
    (b) 受限玻尔兹曼机，编码器和解码器共享相同的变换权重。输入层和输出层合并成可见层。 (c) 具有隐藏层的深度 AE。通常，隐藏层的数量是奇数，中间层是学习到的代表特征。
    (d) 深度 RBM 具有一个可见层和多个隐藏层，最后一层是编码表示。
- en: B.2.1 Autoencoder (AE)
  id: totrans-1115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2.1 自编码器（AE）
- en: 'As shown in Figure [10(a)](#A2.F10.sf1 "In Figure 10 ‣ B.2 Representative Deep
    Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A
    Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances and
    New Frontiers"), A autoencoder is a neural network that has three layers: the
    input layer, the hidden layer, and the output layer [[43](#bib.bib43)]. It differs
    from the standard neural network, in that the AE is trained to reconstruct its
    inputs, which forces the hidden layer to try to learn good representations of
    the inputs.'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [10(a)](#A2.F10.sf1 "图 10 ‣ B.2 代表性深度学习模型 ‣ 附录 B 大脑信号分析中的基础深度学习 ‣ 基于深度学习的非侵入性脑信号：近期进展与新前沿")
    所示，自编码器是一个具有三层的神经网络：输入层、隐藏层和输出层 [[43](#bib.bib43)]。它与标准神经网络的不同之处在于 AE 经过训练以重构其输入，这迫使隐藏层尝试学习输入的良好表示。
- en: The structure of AE contains two blocks. The first block is called the encoder,
    which embeds the observation to a latent representation (also called ‘code’),
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: AE 的结构包含两个块。第一个块称为编码器，它将观测嵌入到潜在表示中（也称为‘代码’），
- en: '|  | $\bm{x^{h}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (19) |'
  id: totrans-1118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{h}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (19) |'
- en: where $\bm{x^{h}}$ represents the hidden layer. The second block is called the
    decoder, which decodes the representation into the original space,
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{x^{h}}$ 表示隐藏层。第二个块称为解码器，它将表示解码回原始空间，
- en: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h}}))$ |  | (20) |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h}}))$ |  | (20) |'
- en: where $\bm{y^{\prime}}$ represents the output.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{y^{\prime}}$ 表示输出。
- en: AE forces $\bm{y^{\prime}}$ to be equal to the input $\bm{x}$ and calculates
    the error based on the distance between them. Thus, AE can compute the loss function
    only by $\bm{x}$ without the ground truth $\bm{y}$
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: AE 强制 $\bm{y^{\prime}}$ 等于输入 $\bm{x}$，并根据它们之间的距离计算误差。因此，AE 只需通过 $\bm{x}$ 计算损失函数，而不需要真实值
    $\bm{y}$
- en: '|  | $error=\left\&#124;\bm{y^{\prime}}-\bm{x}\right\&#124;_{2}$ |  | (21)
    |'
  id: totrans-1123
  prefs: []
  type: TYPE_TB
  zh: '|  | $error=\left\&#124;\bm{y^{\prime}}-\bm{x}\right\&#124;_{2}$ |  | (21)
    |'
- en: 'Compared to Equation [6](#A2.E6 "In B.1.1 Multi-Layer Perceptron (MLP) ‣ B.1
    Discriminative Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain
    Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers"), this equation does not involve the variable
    $\bm{y}$ because it takes the input $\bm{x}$ as the ground truth. This is why
    AE is able to perform unsupervised learning.'
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 与方程 [6](#A2.E6 "在 B.1.1 多层感知器 (MLP) ‣ B.1 判别深度学习模型 ‣ 附录 B 大脑信号分析中的基础深度学习 ‣ 基于深度学习的非侵入性脑信号：近期进展与新前沿")
    相比，该方程不涉及变量 $\bm{y}$，因为它将输入 $\bm{x}$ 作为真实值。这就是 AE 能够进行无监督学习的原因。
- en: 'Naturally, one variant of AE is Deep-AE (D-AE) which has more than one hidden
    layer. We present the structure of D-AE with three hidden layers in Figure [10(c)](#A2.F10.sf3
    "In Figure 10 ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"). From the figure, we can observe
    that there is one more hidden layer in both the encoder and the decoder. The symmetrical
    structure ensures the smoothness of encoding and decoding procedure. Thus, D-AE
    generally has an odd number of hidden layers (e.g., $2n+1$) where the first $n$
    layers belong to the encoder, the $(n+1)$-th layer works as the code which belongs
    to both encoder and decoder, and the last $n$ layers belong to the decoder. The
    data flow of D-AE (Figure [10(c)](#A2.F10.sf3 "In Figure 10 ‣ B.2 Representative
    Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers")) can be represented as'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，AE的一种变体是深度AE（D-AE），它有多个隐藏层。我们在图[10(c)](#A2.F10.sf3 "图10 ‣ B.2 代表性深度学习模型
    ‣ 附录B 脑信号分析中的基本深度学习 ‣ 基于深度学习的非侵入式脑信号调查：最新进展和新前沿")中展示了具有三个隐藏层的D-AE的结构。从图中可以观察到，编码器和解码器中各多出了一层隐藏层。对称的结构确保了编码和解码过程的顺畅。因此，D-AE通常具有奇数个隐藏层（例如，$2n+1$），其中前$n$层属于编码器，第$(n+1)$层作为代码，既属于编码器也属于解码器，最后$n$层属于解码器。D-AE的数据流（图[10(c)](#A2.F10.sf3
    "图10 ‣ B.2 代表性深度学习模型 ‣ 附录B 脑信号分析中的基本深度学习 ‣ 基于深度学习的非侵入式脑信号调查：最新进展和新前沿")）可以表示为
- en: '|  | $\bm{x^{h1}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (22) |'
  id: totrans-1126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{h1}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (22) |'
- en: '|  | $\bm{x^{h2}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (23) |'
  id: totrans-1127
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{h2}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (23) |'
- en: where $\bm{x^{h2}}$ denotes the median hidden layer (the code). Then decode
    the hidden layer, we can get
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{x^{h2}}$表示中间隐藏层（即代码）。然后解码隐藏层，我们可以得到
- en: '|  | $\bm{x^{h3}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (24) |'
  id: totrans-1129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{h3}}=\sigma(\mathcal{T}(\bm{x^{h2}}))$ |  | (24) |'
- en: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h3}}))$ |  | (25) |'
  id: totrans-1130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h3}}))$ |  | (25) |'
- en: 'It is almost the same as AE except that D-AE has more hidden layers. Apart
    from D-AE, AE has many other variants like denoising autoencoder, sparse autoencoder,
    contractive AE, etc. Here we only introduce the D-AE because it is easily confused
    with the AE-based deep belief network. The key difference between them will be
    provided in Section [B.2.3](#A2.SS2.SSS3 "B.2.3 Deep Belief Networks (DBN) ‣ B.2
    Representative Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain
    Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers").'
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: 它与AE几乎相同，只是D-AE有更多的隐藏层。除了D-AE，AE还有许多其他变体，如去噪自编码器、稀疏自编码器、收缩AE等。在这里我们仅介绍D-AE，因为它容易与基于AE的深度置信网络混淆。它们之间的主要区别将在[B.2.3](#A2.SS2.SSS3
    "B.2.3 深度置信网络（DBN） ‣ B.2 代表性深度学习模型 ‣ 附录B 脑信号分析中的基本深度学习 ‣ 基于深度学习的非侵入式脑信号调查：最新进展和新前沿")节中提供。
- en: The core idea of AE and its variants is simple, which is that condensing the
    input data $\bm{x}$ into a code $\bm{x^{h}}$ (generally the code layer has lower
    dimension) and then reconstructing the data based on the code. If the reconstructed
    $\bm{y^{\prime}}$ can approximate to the input data $\bm{x}$, it can be demonstrated
    that the condensed code $\bm{x^{h}}$ carries enough information about $\bm{x}$,
    thus, we can regard $\bm{x^{h}}$ as a representation of the input data for future
    operation (e.g., classification).
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: AE及其变体的核心思想很简单，即将输入数据$\bm{x}$压缩成一个代码$\bm{x^{h}}$（通常代码层具有较低的维度），然后基于该代码重构数据。如果重构的$\bm{y^{\prime}}$能够逼近输入数据$\bm{x}$，则可以证明压缩的代码$\bm{x^{h}}$包含了关于$\bm{x}$的足够信息，因此我们可以将$\bm{x^{h}}$视为输入数据在未来操作（例如分类）中的表示。
- en: B.2.2 Restricted Boltzmann Machine (RBM)
  id: totrans-1133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2.2 受限玻尔兹曼机（RBM）
- en: 'Restricted Boltzmann Machine is a stochastic artificial neural network that
    can learn a probability distribution over its set of inputs [[44](#bib.bib44)].
    It contains two layers including one visible layer (input layer) and one hidden
    layer, as shown in Figure [10(b)](#A2.F10.sf2 "In Figure 10 ‣ B.2 Representative
    Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers"). From the figure, we can see that the connection lines between
    the two layers are bidirectional. RBM is a variant of Boltzmann Machine with stronger
    restriction of being without intra-layer connections. In a general Boltzmann machine,
    the nodes in the same hidden layer will connect. Similar to AE, the procedure
    of RBM also includes two steps. The first step condenses the input data from the
    original space to the hidden layer in a latent space. After that, the hidden layer
    is used to reconstruct the input data in an identical way. Compared to AE, RBM
    has a stronger constraint which is that the encoder weights and the decoder weights
    should be equal. We have'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机（Restricted Boltzmann Machine）是一种随机人工神经网络，可以学习其输入集合的概率分布 [[44](#bib.bib44)]。它包含两个层次，包括一个可见层（输入层）和一个隐藏层，如图 [10(b)](#A2.F10.sf2
    "在图10 ‣ B.2 代表性深度学习模型 ‣ 附录B 基本的脑信号深度学习 ‣ 基于深度学习的非侵入性脑信号：最新进展和新前沿")所示。从图中可以看到，两层之间的连接线是双向的。RBM
    是玻尔兹曼机的一种变体，具有更强的限制，即没有层内连接。在一般的玻尔兹曼机中，隐藏层中的节点会相互连接。与AE类似，RBM的过程也包括两个步骤。第一步是将输入数据从原始空间压缩到隐藏层的潜在空间中。之后，隐藏层用于以相同的方式重构输入数据。与AE相比，RBM具有更强的约束，即编码器权重和解码器权重应该相等。我们有
- en: '|  | $\bm{x^{h}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (26) |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{h}}=\sigma(\mathcal{T}(\bm{x}))$ |  | (26) |'
- en: '|  | $\bm{x^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h}}))$ |  | (27) |'
  id: totrans-1136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{\prime}}=\sigma(\mathcal{T}(\bm{x^{h}}))$ |  | (27) |'
- en: In the above two equations, the weights of $\mathcal{T}(\cdot)$ are the same.
    Then, the error for training can be calculated by
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两个方程中，$\mathcal{T}(\cdot)$ 的权重是相同的。然后，训练误差可以通过以下公式计算：
- en: '|  | $error=\left\&#124;\bm{x^{\prime}}-\bm{x}\right\&#124;_{2}$ |  | (28)
    |'
  id: totrans-1138
  prefs: []
  type: TYPE_TB
  zh: '|  | $error=\left\&#124;\bm{x^{\prime}}-\bm{x}\right\&#124;_{2}$ |  | (28)
    |'
- en: 'We can observe from the Figure [10(d)](#A2.F10.sf4 "In Figure 10 ‣ B.2 Representative
    Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") that the Deep-RBM (D-RBM) is an RBM with multiple hidden layers.
    The input data from the visible layer firstly flow to the first hidden layer and
    then the second hidden layer. Then, the code will flow backward into the visible
    layer for reconstruction.'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 [10(d)](#A2.F10.sf4 "在图10 ‣ B.2 代表性深度学习模型 ‣ 附录B 基本的脑信号深度学习 ‣ 基于深度学习的非侵入性脑信号：最新进展和新前沿")中可以观察到，Deep-RBM
    (D-RBM) 是一种具有多隐藏层的RBM。输入数据从可见层首先流向第一个隐藏层，然后流向第二个隐藏层。接着，编码会向后流回可见层进行重构。
- en: '![Refer to caption](img/62ff570c8b7729be58be0726bf0b09c8.png)'
  id: totrans-1140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/62ff570c8b7729be58be0726bf0b09c8.png)'
- en: (a) DBN-AE
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: (a) DBN-AE
- en: '![Refer to caption](img/ce52725ee523a3224e0d6b7d1664d026.png)'
  id: totrans-1142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ce52725ee523a3224e0d6b7d1664d026.png)'
- en: (b) DBN-RBM
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: (b) DBN-RBM
- en: 'Figure 11: Illustration of deep belief networks. (a) DBN composed of autoencoders.
    DBN-AE contains multiple AE components (in this case, two AE), with the hidden
    layer of the previous AE working as the input layer of the next AE. The hidden
    layer of the last AE is the learned representation. (b) DBN composed of RBM. In
    this illustration, there are two RBM components with the hidden layer of the first
    RBM working as the visible layer of the second RBM. The last hidden layer is the
    encoded representation. While DBN-RBM and D-RBM (Figure [10(d)](#A2.F10.sf4 "In
    Figure 10 ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep Learning
    in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain
    Signals: Recent Advances and New Frontiers")) have similar architecture, the former
    is trained greedily while the latter is trained jointly .'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：深度置信网络示意图。 (a) 由自动编码器组成的 DBN。DBN-AE 包含多个 AE 组件（在这种情况下是两个 AE），前一个 AE 的隐藏层作为下一个
    AE 的输入层。最后一个 AE 的隐藏层是学习到的表示。 (b) 由 RBM 组成的 DBN。在此示例中，有两个 RBM 组件，第一个 RBM 的隐藏层作为第二个
    RBM 的可见层。最后的隐藏层是编码表示。虽然 DBN-RBM 和 D-RBM（图 [10(d)](#A2.F10.sf4 "在图 10 ‣ B.2 代表性深度学习模型
    ‣ 附录 B 基本深度学习在脑信号分析中的应用 ‣ 基于深度学习的非侵入性脑信号调查：最新进展与新前沿")）具有类似的架构，但前者是贪婪训练的，而后者是联合训练的。
- en: B.2.3 Deep Belief Networks (DBN)
  id: totrans-1145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2.3 深度置信网络（DBN）
- en: A Deep Belief Network (DBN) is a stack of simple networks, such as AEs or RBMs
    [[258](#bib.bib258)]. Thus, we divided DBN into DBN-AE (also called stacked AE)
    which is composed of AE and DBN-RBM (also called stacked RBM) which is composed
    of RBM.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 深度置信网络（DBN）是由简单网络（如自动编码器或限制玻尔兹曼机）堆叠而成的[[258](#bib.bib258)]。因此，我们将 DBN 分为 DBN-AE（也称为堆叠
    AE），由 AE 组成，以及 DBN-RBM（也称为堆叠 RBM），由 RBM 组成。
- en: 'As shown in Figure [11(a)](#A2.F11.sf1 "In Figure 11 ‣ B.2.2 Restricted Boltzmann
    Machine (RBM) ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers"), the DBN-AE contains two AE
    structures while the hidden layer of the first AE works as the input layer of
    the second AE. This diagram has two stages. In the first stage, the input data
    feed into the first AE follows the rules introduced in Section [B.2.1](#A2.SS2.SSS1
    "B.2.1 Autoencoder (AE) ‣ B.2 Representative Deep Learning Models ‣ Appendix B
    Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers"). The reconstruction
    error is calculated and back propagated to adjust the corresponding weights and
    basis. This iteration continues until the AE converges. We get the mapping,'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [11(a)](#A2.F11.sf1 "在图 11 ‣ B.2.2 限制玻尔兹曼机（RBM） ‣ B.2 代表性深度学习模型 ‣ 附录 B 基本深度学习在脑信号分析中的应用
    ‣ 基于深度学习的非侵入性脑信号调查：最新进展与新前沿") 所示，DBN-AE 包含两个 AE 结构，其中第一个 AE 的隐藏层作为第二个 AE 的输入层。此图有两个阶段。在第一阶段，输入数据喂入第一个
    AE 按照 [B.2.1](#A2.SS2.SSS1 "B.2.1 自动编码器（AE） ‣ B.2 代表性深度学习模型 ‣ 附录 B 基本深度学习在脑信号分析中的应用
    ‣ 基于深度学习的非侵入性脑信号调查：最新进展与新前沿") 节中介绍的规则进行处理。计算重建误差并反向传播以调整相应的权重和基准。这一迭代过程持续进行，直到
    AE 收敛。我们得到映射，
- en: '|  | $\bm{x^{1}}\rightarrow\bm{x^{h1}}$ |  | (29) |'
  id: totrans-1148
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{1}}\rightarrow\bm{x^{h1}}$ |  | (29) |'
- en: Then, we move on to the second stage where the learned representative code in
    the hidden layer $\bm{x^{h1}}$ will be used as the input layer of the second AE,
    which is
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入第二阶段，此时，隐藏层 $\bm{x^{h1}}$ 中学习到的代表性代码将作为第二个自动编码器的输入层，即
- en: '|  | $\bm{x^{2}}=\bm{x^{h1}}$ |  | (30) |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{2}}=\bm{x^{h1}}$ |  | (30) |'
- en: and then, after the second AE converges, we have
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，第二个自动编码器收敛后，我们得到
- en: '|  | $\bm{x^{2}}\rightarrow\bm{x^{h2}}$ |  | (31) |'
  id: totrans-1152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x^{2}}\rightarrow\bm{x^{h2}}$ |  | (31) |'
- en: where $\bm{x^{h2}}$ denotes the hidden layer of the second AE, meanwhile, it
    is the final outcome of the DBN-AE.
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\bm{x^{h2}}$ 表示第二个自动编码器的隐藏层，同时也是 DBN-AE 的最终结果。
- en: The core idea of AE is that of learning a representative code with lower dimensionality
    but containing most information of the input data. The idea behind DBN-AE is to
    learn a more representative and purer code.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: AE 的核心思想是学习一个具有更低维度但包含输入数据大部分信息的代表性代码。DBN-AE 的思想是学习一个更具代表性和更纯粹的代码。
- en: 'Similarly, the DBN-RBM is composed of several single RBM structures. Figure [11(b)](#A2.F11.sf2
    "In Figure 11 ‣ B.2.2 Restricted Boltzmann Machine (RBM) ‣ B.2 Representative
    Deep Learning Models ‣ Appendix B Basic Deep Learning in Brain Signal Analysis
    ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers") shows a DBN with two RBMs where the hidden layer of the first
    RBM is used as the visible layer of the second RBM.'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，DBN-RBM 由几个单一的 RBM 结构组成。图 [11(b)](#A2.F11.sf2 "图 11 ‣ B.2.2 限制玻尔兹曼机 (RBM)
    ‣ B.2 代表性深度学习模型 ‣ 附录 B 大脑信号分析中的基本深度学习 ‣ 基于深度学习的非侵入性脑信号调查：近期进展与新前沿") 显示了一个具有两个
    RBM 的 DBN，其中第一个 RBM 的隐藏层用作第二个 RBM 的可见层。
- en: 'Compare the DBN-RBM (Figure [11(b)](#A2.F11.sf2 "In Figure 11 ‣ B.2.2 Restricted
    Boltzmann Machine (RBM) ‣ B.2 Representative Deep Learning Models ‣ Appendix B
    Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")) and D-RBM (Figure [10(d)](#A2.F10.sf4
    "In Figure 10 ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")). They almost have the same
    architecture. Moreover, DBN-AE (Figure [11(a)](#A2.F11.sf1 "In Figure 11 ‣ B.2.2
    Restricted Boltzmann Machine (RBM) ‣ B.2 Representative Deep Learning Models ‣
    Appendix B Basic Deep Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based
    Non-Invasive Brain Signals: Recent Advances and New Frontiers")) and D-AE (Figure [10(c)](#A2.F10.sf3
    "In Figure 10 ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep
    Learning in Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive
    Brain Signals: Recent Advances and New Frontiers")) have similar architecture.
    The most important difference between the DBN and the deep AE/RBM is that the
    former is trained greedily while the latter is trained jointly. In particular,
    for the DBN, the first AE/RBM is trained first, after it converges, the second
    AE/RBM is trained[[44](#bib.bib44)]. For the deep AE/RBM, jointly training means
    that the whole structure is trained together, no matter how layers it has.'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 DBN-RBM（图 [11(b)](#A2.F11.sf2 "图 11 ‣ B.2.2 限制玻尔兹曼机 (RBM) ‣ B.2 代表性深度学习模型
    ‣ 附录 B 大脑信号分析中的基本深度学习 ‣ 基于深度学习的非侵入性脑信号调查：近期进展与新前沿")）和 D-RBM（图 [10(d)](#A2.F10.sf4
    "图 10 ‣ B.2 代表性深度学习模型 ‣ 附录 B 大脑信号分析中的基本深度学习 ‣ 基于深度学习的非侵入性脑信号调查：近期进展与新前沿")）。它们几乎具有相同的架构。此外，DBN-AE（图 [11(a)](#A2.F11.sf1
    "图 11 ‣ B.2.2 限制玻尔兹曼机 (RBM) ‣ B.2 代表性深度学习模型 ‣ 附录 B 大脑信号分析中的基本深度学习 ‣ 基于深度学习的非侵入性脑信号调查：近期进展与新前沿")）和
    D-AE（图 [10(c)](#A2.F10.sf3 "图 10 ‣ B.2 代表性深度学习模型 ‣ 附录 B 大脑信号分析中的基本深度学习 ‣ 基于深度学习的非侵入性脑信号调查：近期进展与新前沿")）具有相似的架构。DBN
    和深度 AE/RBM 之间最重要的区别在于前者是贪婪地训练的，而后者是联合训练的。特别是对于 DBN，首先训练第一个 AE/RBM，待其收敛后，再训练第二个
    AE/RBM[[44](#bib.bib44)]。对于深度 AE/RBM，联合训练意味着整个结构一起训练，无论其层数多少。
- en: '![Refer to caption](img/8792751befd947fff62ea33681e09530.png)'
  id: totrans-1157
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8792751befd947fff62ea33681e09530.png)'
- en: (a) Variational Autoencoder
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 变分自编码器
- en: '![Refer to caption](img/f89c94a6a7331d0f4a66aefe3ca2f5c5.png)'
  id: totrans-1159
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f89c94a6a7331d0f4a66aefe3ca2f5c5.png)'
- en: (b) GAN
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GAN
- en: 'Figure 12: Illustration of generative deep learning models. (a) VAE contains
    two hidden layers. The first hidden layer is composed of two components: the expectation
    and the standard deviation, which are learned separately from the input layer.
    The second hidden layer represents the encoded information. $\epsilon$ denotes
    the standard normal distribution. (b) GAN mainly contain two crucial components:
    the generator and the discriminator network. The former receives a latent random
    variable to generate a fake brain signal while the latter receives both the real
    and the generated brain signals and attempts to determine if its generated or
    not. In the are of brain signals, GAN reconstructs or augments data instead of
    classification.'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：生成式深度学习模型的示意图。 (a) VAE 包含两个隐藏层。第一个隐藏层由两个组件组成：期望值和标准差，这些组件从输入层中分别学习。第二个隐藏层表示编码后的信息。$\epsilon$
    表示标准正态分布。 (b) GAN 主要包含两个关键组件：生成器和判别器网络。生成器接收一个潜在的随机变量以生成伪造的大脑信号，而判别器则接收真实和生成的大脑信号，并尝试判断其是否为生成的。在大脑信号领域，GAN
    重建或扩充数据，而不是进行分类。
- en: B.3 Generative Deep Learning Models
  id: totrans-1162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 生成式深度学习模型
- en: 'Generative deep learning models are mainly used to generate training samples
    or data augmentation. In other words, generative deep learning models play a supporting
    role in the brain signal field to enhance the training data quality and quantity.
    After the data augmentation, the discriminative models will be employed for the
    classification. This procedure is created to improve the robustness and effectiveness
    of the trained deep learning networks, especially when the training data is limited.
    In short, the generative models receive the input data and output a batch of similar
    data. In this section, we will introduce two typical generative deep learning
    models: variational Autoencoder (VAE) and Generative Adversarial Networks (GAN).'
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式深度学习模型主要用于生成训练样本或数据增强。换句话说，生成式深度学习模型在脑信号领域中发挥辅助作用，以提高训练数据的质量和数量。经过数据增强后，将使用判别模型进行分类。这个过程旨在提高训练深度学习网络的鲁棒性和有效性，特别是当训练数据有限时。简而言之，生成模型接收输入数据并输出一批相似的数据。在本节中，我们将介绍两种典型的生成式深度学习模型：变分自编码器（VAE）和生成对抗网络（GAN）。
- en: B.3.1 Variational Autoencoder (VAE)
  id: totrans-1164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.1 变分自编码器（VAE）
- en: 'Variational Autoencoder, proposed in 2013 [[46](#bib.bib46)], is an important
    variant of AE, and one of the most powerful generative algorithms. The standard
    AE and its other variants can be used for representation but fail in generation
    for the reason that the learned code (or representation) may not be continuous.
    Therefore, we cannot generate a random sample which is similar to the input sample.
    In other words, the standard AE does not allow interpolation. Thus, we can replicate
    the input sample but cannot generate a similar one. VAE has one fundamentally
    unique property that separates it from other AEs, and it is this property that
    makes VAE so useful for generative modeling: the latent spaces are designed to
    be continuous which allows easy random sampling and interpolation. Next, we will
    introduce how VAE works.'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE），于 2013 年提出 [[46](#bib.bib46)]，是 AE 的一个重要变种，也是最强大的生成算法之一。标准 AE 及其其他变种可用于表示，但在生成方面存在不足，因为学到的编码（或表示）可能不连续。因此，我们无法生成与输入样本相似的随机样本。换句话说，标准
    AE 不允许插值。因此，我们可以复制输入样本，但不能生成类似样本。VAE 具有一种根本独特的属性，将其与其他 AE 区分开来，这种属性使 VAE 在生成建模中非常有用：潜在空间被设计为连续的，这使得随机采样和插值变得容易。接下来，我们将介绍
    VAE 的工作原理。
- en: Similar to the standard AE, VAE can be divided into an encoder and decoder where
    the former embeds the input data to a latent space and the latter transfers the
    data from the latent space to the original space. However, the learned representation
    in the latent space is forced to approximate a prior distribution $\bm{\bar{p(z)}}$
    which is generally set as Standard Gaussian distribution. Based on the reparameterization
    trick [[46](#bib.bib46)], the first hidden layer of VAE is designed to have two
    parts where one denotes the expectation $\bm{\mu}$ and another denotes the standard
    deviation $\bm{\sigma}$, thus we have
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于标准自编码器（AE），变分自编码器（VAE）可以分为编码器和解码器，其中前者将输入数据嵌入到潜在空间，后者则将数据从潜在空间转换回原始空间。然而，在潜在空间中学到的表示被强制接近一个先验分布
    $\bm{\bar{p(z)}}$，通常设为标准高斯分布。基于重参数化技巧 [[46](#bib.bib46)]，VAE 的第一隐藏层被设计为具有两个部分，其中一个表示期望
    $\bm{\mu}$，另一个表示标准差 $\bm{\sigma}$，因此我们有
- en: '|  | $\bm{\mu}=\sigma(\mathcal{T}(\bm{x}))$ |  | (32) |'
  id: totrans-1167
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\mu}=\sigma(\mathcal{T}(\bm{x}))$ |  | (32) |'
- en: '|  | $\bm{\sigma}=\sigma(\mathcal{T}(\bm{x}))$ |  | (33) |'
  id: totrans-1168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\sigma}=\sigma(\mathcal{T}(\bm{x}))$ |  | (33) |'
- en: Then, the latent code in the hidden layer is not directly calculated but sampled
    from a Gaussian distribution $\mathcal{N}(\bm{\mu},\bm{\sigma}^{2})$. The statistic
    code
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，隐藏层中的潜在编码不是直接计算的，而是从高斯分布 $\mathcal{N}(\bm{\mu},\bm{\sigma}^{2})$ 中采样。统计编码
- en: '|  | $\bm{z}=\bm{\mu}+\bm{\sigma}*\bm{\varepsilon}$ |  | (34) |'
  id: totrans-1170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{z}=\bm{\mu}+\bm{\sigma}*\bm{\varepsilon}$ |  | (34) |'
- en: where $\bm{\varepsilon}\sim\mathcal{N}(\bm{0},\bm{I})$. The representation $\bm{z}$
    is forced to a prior distribution, and the distance $error_{KL}$ is measured by
    Kullback–Leibler divergence,
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{\varepsilon}\sim\mathcal{N}(\bm{0},\bm{I})$。表示 $\bm{z}$ 被强制为先验分布，距离
    $error_{KL}$ 由 Kullback–Leibler 散度测量，
- en: '|  | $error_{KL}=D_{KL}(z,\bm{\bar{p(z)}})$ |  | (35) |'
  id: totrans-1172
  prefs: []
  type: TYPE_TB
  zh: '|  | $error_{KL}=D_{KL}(z,\bm{\bar{p(z)}})$ |  | (35) |'
- en: where $\bm{\bar{p(z)}}$ denotes the prior distribution. In the decoder, $\bm{z}$
    is decoded into the output $\bm{y}^{\prime}$,
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{\bar{p(z)}}$ 表示先验分布。在解码器中，$\bm{z}$ 被解码为输出 $\bm{y}^{\prime}$，
- en: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{z}))$ |  | (36) |'
  id: totrans-1174
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{y^{\prime}}=\sigma(\mathcal{T}(\bm{z}))$ |  | (36) |'
- en: and the reconstruction error is
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 重构误差为
- en: '|  | $error_{recon}=\left\&#124;\bm{y^{\prime}}-\bm{x}\right\&#124;_{2}$ |  |
    (37) |'
  id: totrans-1176
  prefs: []
  type: TYPE_TB
  zh: '|  | $error_{recon}=\left\&#124;\bm{y^{\prime}}-\bm{x}\right\&#124;_{2}$ |  |
    (37) |'
- en: The overall error for VAE is combined by the DL divergence and the reconstruction
    error,
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 的总体误差由 DL 发散和重构误差组合而成，
- en: '|  | $error=error_{KL}+error_{recon}$ |  | (38) |'
  id: totrans-1178
  prefs: []
  type: TYPE_TB
  zh: '|  | $error=error_{KL}+error_{recon}$ |  | (38) |'
- en: The key point of VAE is that all the latent representations $\bm{z}$ are forced
    to obey the normal distribution. Thus, we can randomly sample a representation
    $\bm{z^{\prime}}\in\bm{\bar{p(z)}}$ from the prior distribution and then reconstruct
    a sample based on $\bm{z^{\prime}}$. This is why VAE is so powerful in generation.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 的关键点在于所有潜在表示 $\bm{z}$ 都被强制遵循正态分布。因此，我们可以从先验分布中随机采样一个表示 $\bm{z^{\prime}}\in\bm{\bar{p(z)}}$，然后基于
    $\bm{z^{\prime}}$ 重构一个样本。这就是为什么 VAE 在生成任务中如此强大的原因。
- en: B.3.2 Generative Adversarial Networks (GAN)
  id: totrans-1180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.2 生成对抗网络 (GAN)
- en: Generative Adversarial Networks [[47](#bib.bib47)] is proposed in 2014 and achieved
    great success in a wide range of research areas (e.g., computer vision and natural
    language processing). GAN is composed of two simultaneously trained neural networks
    with a generator and a discriminator. The generator captures the distribution
    of the input data, and the discriminator is used to estimate the probability that
    a sample came from the training data. The generator aims to generate fake samples
    while the discriminator aims to distinguish whether the sample is genuine. The
    functions of the generator and the discriminator are opposite; that’s why GAN
    is called ‘adversarial.’ After the convergence of both the generator and the discriminator,
    the discriminator ought to be unable to recognize the generated samples. Thus,
    the pre-trained generator can be used to create a batch of samples and use them
    for further operations such as as classification.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络 [[47](#bib.bib47)] 于 2014 年提出，并在广泛的研究领域（例如计算机视觉和自然语言处理）取得了巨大成功。GAN 由两个同时训练的神经网络组成，一个生成器和一个判别器。生成器捕捉输入数据的分布，判别器用于估计样本来自训练数据的概率。生成器的目标是生成伪造样本，而判别器的目标是区分样本是否真实。生成器和判别器的功能是相反的；这就是为什么
    GAN 被称为‘对抗性’。在生成器和判别器都收敛后，判别器应该无法识别生成的样本。因此，预训练的生成器可以用来创建一批样本，并将其用于进一步操作，例如分类。
- en: 'Figure [12(b)](#A2.F12.sf2 "In Figure 12 ‣ B.2.3 Deep Belief Networks (DBN)
    ‣ B.2 Representative Deep Learning Models ‣ Appendix B Basic Deep Learning in
    Brain Signal Analysis ‣ A Survey on Deep Learning-based Non-Invasive Brain Signals:
    Recent Advances and New Frontiers") shows the procedure of a standard GAN. The
    generator receives a noise signal $\bm{s}$ which is randomly sampled from a multimodal
    Gaussian distribution and outputs the fake brain signals $\bm{x}_{F}$. The distributor
    receives the real brain signals $\bm{x}_{R}$ and the generated fake sample $\bm{x}_{F}$,
    and then it predicts whether the received sample is real or fake. The internal
    architecture of the generator and discriminator are designed depending on the
    data types and scenarios. For instance, we can build the GAN by convolutional
    layers on fMRI images since CNN has an excellent ability to extract spatial features.
    The discriminator and the generator are trained jointly. After the convergence,
    numerous brain signals $\bm{x}_{G}$ can be created by the generator. Thus, the
    training set is enlarged from $\bm{x}_{R}$ to $\{\bm{x}_{R},\bm{x}_{G}\}$ to train
    a more effective and robust classifier.'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12(b)](#A2.F12.sf2 "图 12 ‣ B.2.3 深度信念网络 (DBN) ‣ B.2 代表性深度学习模型 ‣ 附录 B 脑信号分析中的基本深度学习
    ‣ 基于深度学习的非侵入性脑信号调查：近期进展和新前沿") 显示了标准 GAN 的过程。生成器接收一个噪声信号 $\bm{s}$，该信号是从多模态高斯分布中随机采样的，并输出伪造的脑信号
    $\bm{x}_{F}$。分发器接收真实脑信号 $\bm{x}_{R}$ 和生成的伪造样本 $\bm{x}_{F}$，然后预测接收到的样本是真实的还是伪造的。生成器和判别器的内部架构设计取决于数据类型和场景。例如，我们可以通过在
    fMRI 图像上使用卷积层来构建 GAN，因为 CNN 在提取空间特征方面具有很好的能力。判别器和生成器是联合训练的。收敛后，生成器可以创建大量脑信号 $\bm{x}_{G}$。因此，训练集从
    $\bm{x}_{R}$ 扩展到 $\{\bm{x}_{R},\bm{x}_{G}\}$ 以训练更有效和更强大的分类器。
- en: B.4 Hybrid Model
  id: totrans-1183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 混合模型
- en: 'Hybrid deep learning models refers to models which are composed of at least
    two deep basic learning models where the basic model is a discriminative, representative,
    or generative deep learning model. Hybrid models comprise two subcategories based
    on their targets: classification-aimed (CA) hybrid models and the non-classification-aimed
    (NCA) hybrid models.'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 混合深度学习模型指的是由至少两个深度基础学习模型组成的模型，其中基础模型是判别性、代表性或生成性深度学习模型。混合模型根据其目标分为两个子类别：以分类为目标的（CA）混合模型和非分类目标的（NCA）混合模型。
- en: 'Most of the deep learning related studies in brain signal area are focused
    on the first category. Based on the existing literature, the representative and
    generative models are employed to enhance the discriminative models. The representative
    models can provide more informative and low dimensional features for the discrimination
    while the generative models can help to augment the training data quality and
    quantity which supply more information for the classification. The CA hybrid models
    can be further subdivided into: 1) several discriminative models combined to extract
    more distinctive and robust features (e.g., CNN+RNN); 2) representative model
    followed by a discriminative model (e.g., DBN+MLP); 3) generative + representative
    model followed by a discriminative model; 4) generative + representative model
    followed by a non-deep learning classifier. In which, a representative model followed
    by a non-deep learning classifier is regarded as a representative deep learning
    model.'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数与脑信号相关的深度学习研究集中在第一类。根据现有文献，代表性模型和生成模型被用来增强判别模型。代表性模型可以提供更多信息量和低维特征以便于判别，而生成模型可以帮助提高训练数据的质量和数量，从而为分类提供更多信息。CA混合模型可以进一步细分为：1）多个判别模型结合以提取更具辨别力和鲁棒性的特征（例如，CNN+RNN）；2）代表性模型后接判别模型（例如，DBN+MLP）；3）生成+代表性模型后接判别模型；4）生成+代表性模型后接非深度学习分类器。其中，代表性模型后接非深度学习分类器被视为代表性深度学习模型。
- en: A few NCA hybrid models aim for brain signal reconstruction. For example, St-yves
    et al. [[259](#bib.bib259)] adopted GAN to reconstruct visual stimuli based on
    fMRI images.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 一些NCA混合模型旨在进行脑信号重建。例如，St-yves等人[[259](#bib.bib259)]采用GAN基于fMRI图像重建视觉刺激。
