- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:33:02'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2404.18961] 2.1.1\. Feature Selection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18961](https://ar5iv.labs.arxiv.org/html/2404.18961)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Model Name | Origin | Year | Type | Matrix Regularizer | Vector Formalization
    |'
  prefs: []
  type: TYPE_TB
- en: '| Regularized MTL | KDD | evgeniou2004regularized | Group regularization |
    Frobenius norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}-\frac{1}{T}\sum_{t=1}^{T}\boldsymbol{w}^{t}\&#124;}^{2}_{2}+\lambda_{2}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}\&#124;}^{2}_{2}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Learning Multiple Tasks with Kernel Methods | JMLR | \citeyearevgeniou2005learning
    | Priori Sharing | Adaptive penalty | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{t=1}^{T}{\boldsymbol{w}^{t}}^{\top}\boldsymbol{V}^{+}\boldsymbol{w}^{t},$  s.t. $\boldsymbol{V}\in\boldsymbol{S}_{+}^{D},$
    $\boldsymbol{V}\in\boldsymbol{S}^{D}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Alternating structure optimization | JMLR | \citeyearando2005framework |
    Decomposition | Frobenius norm | $\min\limits_{\{\boldsymbol{W},\boldsymbol{V}\},\Theta}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{w}^{t}+\Theta^{\top}\boldsymbol{v}^{t})-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}$,  s.t. $\Theta\Theta^{\top}=\boldsymbol{I}_{h\times
    h}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-task feature selection | Tech. Rep.¹ | \citeyearobozinski2006multi
    | Group-sparse learning | $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-task Lasso | Thesis² | \citeyearzhang2006a | Group-sparse learning
    | $\ell_{\infty,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{\infty}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-task feature learning | NeurIPS | \citeyearargyriou2006multi | Group-sparse
    learning, feature learning | $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{U},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;({\boldsymbol{X}^{(t)}}\boldsymbol{U})\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda(\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2})^{2}$,  s.t. $\boldsymbol{U}\in\boldsymbol{O}^{D}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Convex multi-task feature learning | Mach. Lea. | \citeyearargyriou2008convex
    | Feature learning | Adaptive penalty | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{t=1}^{T}{\boldsymbol{w}^{t}}^{\top}\boldsymbol{V}^{+}\boldsymbol{w}^{t},$  s.t. $\boldsymbol{V}\in\boldsymbol{S}_{+}^{D},$
    tr$(\boldsymbol{V})\leq 1$, col$(\boldsymbol{W})\subseteq$col$(\boldsymbol{V})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Low rank MTL | ICML | \citeyearji2009accelerated | Low-rank learning | Trace
    norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\&#124;\boldsymbol{W}\&#124;_{*}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Convex ASO | ICML | \citeyearchen2009convex | — | — | $\min\limits_{\boldsymbol{U},\Theta}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{u}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda\eta(1-\eta)\text{tr}(\boldsymbol{U}^{\top}(\eta\boldsymbol{I}+\Theta^{\top}\Theta)^{-1}\boldsymbol{U}),~{}~{}s.t.~{}\Theta\Theta^{\top}=\boldsymbol{I}_{h\times
    h}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Dirty block-sparse model | NeurIPS | \citeyearjalali2010dirty | Group-sparse
    learning, decomposition | $\ell_{\infty,1}$ norm $+$ $\ell_{1,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{s}^{t}+\boldsymbol{b}^{t})-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}{\&#124;\boldsymbol{s}_{d}\&#124;}_{1}+\lambda_{2}\sum_{d=1}^{D}{\&#124;\boldsymbol{b}_{d}\&#124;}_{\infty}$,  s.t. $\boldsymbol{W}=\boldsymbol{S}+\boldsymbol{B}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sparse multi-task Lasso | NeurIPS | \citeyearlee2010adaptive | Group-sparse
    learning | Weighted $\ell_{2,1}$ norm $+$ weighted $\ell_{1,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\rho_{d}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}+\lambda_{2}\sum_{d=1}^{D}\theta_{d}{\&#124;\boldsymbol{w}_{d}\&#124;}_{1}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-6 |  |  |  | Weighted $\ell_{2,1}$ norm $+$ weighted $\ell_{1,1}$
    norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\rho_{d}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}+\lambda_{2}\sum_{d=1}^{D}\theta_{d}{\&#124;\boldsymbol{w}_{d}\&#124;}_{1}+\log
    Z(\boldsymbol{\rho},\boldsymbol{\theta})$, |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptive multi-task Lasso | NeurIPS | \citeyearlee2010adaptive | Group-sparse
    learning | $+$ adaptive penalty | $P(\boldsymbol{W}&#124;\boldsymbol{\rho},\boldsymbol{\theta})=\frac{1}{Z(\boldsymbol{\rho},\boldsymbol{\theta})}\prod_{d=1}^{D}\prod_{t=1}^{T}\exp(-\theta_{d}\lvert
    w_{n,t}\rvert)\times\prod_{d=1}^{D}\exp(-\rho_{d}\&#124;\mathbf{w}_{d}\&#124;_{2})$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | $\min\limits_{\mathbf{M}_{0},\ldots,\mathbf{M}_{T}}\gamma_{0}\&#124;\mathbf{M}_{0}-\mathbf{I}\&#124;_{F}^{2}+\sum\nolimits_{t=1}^{T}\left[\gamma_{t}\&#124;\mathbf{M}_{t}\&#124;_{F}^{2}+\sum\nolimits_{(i,j)\in
    J_{t},j\neq i}d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{j})+\sum\nolimits_{(i,j,k)\in
    S_{t}}\xi_{ijk}\right]$ |'
  prefs: []
  type: TYPE_TB
- en: '| Large margin multi-task metric learning | NeurIPS | \citeyearparameswaran2010large
    | Priori Sharing | Frobenius norm | s.t. $\forall t,\forall(i,j,k)\in S_{t}\colon\quad
    d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{k})-d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{j})\geq
    1-\xi_{ijk};\xi_{ijk}\geq 0;\mathbf{M}_{0},\mathbf{M}_{1},\ldots,\mathbf{M}_{T}\geq
    0$ |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical multitask structured output learning | NeurIPS | \citeyeargornitz2011hierarchical
    | Priori Sharing | Frobenius norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{1}{2}\sum_{t=1}^{T}&#124;&#124;\boldsymbol{w}&#124;&#124;_{2}^{2}-\lambda\boldsymbol{w}^{T}\boldsymbol{w}_{p}$,
    where $p$ is the parent node. |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | low-rank learning |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Robust MTL | KDD | \citeyearchen2011integrating | Decomposition, group-sparse
    learning, | Trace norm + $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{l}^{t}+\boldsymbol{s}^{t})-\boldsymbol{y}^{(t)}\&#124;_{2}^{2}+\lambda_{1}\&#124;\boldsymbol{L}\&#124;_{*}+\lambda_{2}\sum_{t=1}^{T}\&#124;\boldsymbol{s}_{t}\&#124;_{2}$,  s.t. $\boldsymbol{W}=\boldsymbol{L}+\boldsymbol{S}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Temporal group Lasso | KDD | \citeyearzhou2011multi | Group-sparse learning
    | Frobenius norm + $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}+\lambda_{2}\sum_{t=1}^{T-1}\&#124;\boldsymbol{w}^{t}-\boldsymbol{w}^{t+1}\&#124;_{2}^{2}+\lambda_{3}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Clustered MTL | NeurIPS | \citeyearzhou2011clustered | task clustering |
    Clustering penalty + $\ell_{2,2}$ norm | $\min\limits_{\boldsymbol{W},\boldsymbol{F}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda_{1}(\text{tr}(\boldsymbol{W}^{\top}\boldsymbol{W})-\text{tr}(\boldsymbol{F}^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{F}))+\lambda_{2}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}\&#124;}^{2}_{2},$
    |'
  prefs: []
  type: TYPE_TB
- en: '| $~{}~{}\text{s.t.}~{}\boldsymbol{F}_{t,j}=1/\sqrt{n_{j}}~{}\text{if}~{}t\in\mathcal{C}_{j}~{}\text{otherwise}~{}0,$
    $t=1,\cdots,T$, where $n_{j}$ is the #task in the $j$-th cluster $\mathbf{\mathcal{C}}_{j}$.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Decomposition, sparse learning, |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Sparse and low rank MTL | TKDD | \citeyearchen2012learning | low-rank learning
    | $\ell_{1,1}$ norm + trace norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}\&#124;\boldsymbol{p}_{d}\&#124;_{1}$,  s.t. $\boldsymbol{W}=\boldsymbol{P}+\boldsymbol{Q},\&#124;\boldsymbol{Q}\&#124;_{*}\leq\tau$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Convex fused sparse group Lasso | KDD | \citeyearzhou2012modeling | Group-sparse
    learning | $\ell_{1,1}$ norm $+$ $\ell_{2,1}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{1}+\lambda_{2}\sum_{t=1}^{T-1}\&#124;\boldsymbol{w}^{t}-\boldsymbol{w}^{t+1}\&#124;_{1}+\lambda_{3}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptive multi-task elastic-net | SDM | \citeyearchen2012adaptive | Group-sparse
    learning | $\ell_{2,1}$ norm $+$ Frobenius norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}+\lambda_{2}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-level Lasso | ICML | \citeyearlozano2012multi | Decomposition, sparse
    learning | $\ell_{1,1}$ norm + adaptive penalty | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\theta_{d}+\lambda_{2}\sum_{d=1}^{D}\&#124;\boldsymbol{\boldsymbol{\gamma}}_{d}\&#124;_{1}$,  s.t. $\boldsymbol{W}=\vec{\boldsymbol{\theta}}\boldsymbol{\Lambda}\boldsymbol{\Gamma},\vec{\boldsymbol{\theta}}\geq\boldsymbol{0}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Robust multi-task feature learning | KDD | \citeyeargong2012robust | Decomposition,
    group-sparse learning | $\ell_{2,1}$ norm + $\ell_{1,2}$ norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{p}_{d}\&#124;_{2}+\lambda_{2}\sqrt{\sum_{d=1}^{D}\&#124;\boldsymbol{q}_{d}\&#124;_{1}^{2}}$,  s.t. $\boldsymbol{W}=\boldsymbol{P}+\boldsymbol{Q}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-stage multi-task feature learning | NeurIPS | \citeyeargong2012multi
    | Sparse learning | Capped $\ell_{1}$ norm \citepzhang2010analysis | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{R}\min\{\&#124;\boldsymbol{w}_{d}\&#124;_{1},\tau\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Convex formulation for MTL | IJCAI | \citeyearzhang2012convex | Priori sharing
    | Clustering penalty | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{\lambda_{1}}{2}$tr$(\boldsymbol{W}\boldsymbol{W}^{T})+\frac{\lambda_{2}}{2}$tr$(\boldsymbol{W}\boldsymbol{\Omega}^{-1}\boldsymbol{W}^{T})$  s.t. $\boldsymbol{\Omega}\in\boldsymbol{S}_{+}^{D}$,
    tr$\boldsymbol{\Omega}=1$ |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-linear multi-task learning | ICML | \citeyearromera2013multilinear
    | Low-rank learning | Overlapped tensor trace norm | $\min\limits_{\boldsymbol{\mathcal{W}}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{k=1}^{N}\&#124;\boldsymbol{W}_{(k)}\&#124;_{*}$
    where $\boldsymbol{W}_{(k)}$ is the mode-$k$ unfolding of tensor $\boldsymbol{\mathcal{W}}\in\mathbb{R}^{D\times
    I_{2}\times\cdots\times I_{N}}$. |'
  prefs: []
  type: TYPE_TB
- en: '| Regularization approach to learn MTL | TKDD | \citeyearzhang2014regularization
    | Priori sharing | Clustering penalty + $\ell_{2,2}$ norm | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{\lambda}{2}\sum_{t=1}^{T}&#124;&#124;\boldsymbol{w}^{t}&#124;&#124;_{2}^{2}+$tr$(\boldsymbol{W}\boldsymbol{\Omega}^{-1}\boldsymbol{W}^{T})+d$ln$\boldsymbol{\Omega}$  s.t. $\boldsymbol{\Omega}\in\boldsymbol{S}_{+}^{D}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-linear multi-task learning | NeurIPS | \citeyearwimalawarne2014multitask
    | Low-rank learning | Scaled latent tensor trace norm | $\min\limits_{\boldsymbol{\mathcal{W}}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\inf_{\boldsymbol{\mathcal{W}}^{(1)}+\cdots+\boldsymbol{\mathcal{W}}^{(N)}=\boldsymbol{\mathcal{W}}}\lambda\sum_{k=1}^{N}I_{k}^{-1/2}\&#124;\boldsymbol{W}_{(k)}^{(k)}\&#124;_{*}$
    where $\boldsymbol{\mathcal{W}}\in\mathbb{R}^{D\times I_{2}\times\cdots\times
    I_{N}}$ is a tensor. |'
  prefs: []
  type: TYPE_TB
- en: '| Task Tree model | KDD | \citeyearhan2015learning | task clustering | $\ell_{2,2}$
    norm | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\sum_{h=1}^{H}\boldsymbol{w}_{h}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\sum_{h=1}^{H}\lambda_{h}\sum_{i<j}^{T}\&#124;\boldsymbol{w}_{h}^{i}-\boldsymbol{w}_{h}^{j}\&#124;^{2}_{2},\text{s.t.}&#124;\boldsymbol{w}_{h-1}^{i}-\boldsymbol{w}_{h-1}^{j}&#124;\succeq&#124;\boldsymbol{w}_{h}^{i}-\boldsymbol{w}_{h}^{j}&#124;,\forall
    h\geq 2,\forall i<j$ |'
  prefs: []
  type: TYPE_TB
- en: '| Reduced rank multi-stage MTL | AAAI | \citeyearhan2016multi | Low-rank learning
    | Capped trace norm \citepsun2013robust | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{r=1}^{R}\min\{\sigma_{r}(\boldsymbol{W}),\tau\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This work is published in Technical Report, the Department of Statistics, UC
    Berkeley.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This work is published in Jian Zhang’s Ph.D. Thesis, CMU Technical Report CMU-LTI-06-006,
    2006.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.1.1\. Feature Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The high-dimensional scaling \citepnegahban2008joint where the number of model
    weights is much larger than that of the observations/features, i.e., $D\gg N$,
    arises in many real-world problems, leading it costly and arduous to seek effective
    predictor variables. Sparse learning with an $\ell_{1}$ regularizer that aims
    to identify a structure characterized by a reduced number of non-zero elements.
    This parsimonious solution ensures the retention and selection of the most effective
    and efficient subset of features tailored to the target task\citeptibshirani1996regression.
    In MTL, Assumption LABEL:assump:parameter underpins the development of all sparse
    learning models. Under the settings of sparse learning, this assumption posits
    that similar sparsity patterns in model parameters suggest the relatedness between
    tasks. As a result, sparsity patterns subtly represent task relatedness, underscoring
    a subset of common features derived from these limited samples. More benefits
    and efficacy of employing sparsity in MTL have been thoroughly assessed and discussed
    in
  prefs: []
  type: TYPE_NORMAL
- en: Conversion to HTML had a Fatal error and exited abruptly. This document may
    be truncated or damaged.
  prefs: []
  type: TYPE_NORMAL
