- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:32:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2405.10357] RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10357](https://ar5iv.labs.arxiv.org/html/2405.10357)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1]\fnmChenyang \surGe'
  prefs: []
  type: TYPE_NORMAL
- en: '[1]\orgdivInstitute of Artificial Intelligence and Robotics, \orgnameXi’an
    Jiaotong University, \orgaddress\streetNo.28, West Xianning Road, \cityXi’an,
    \postcode710049, \stateShaanxi, \countryChina'
  prefs: []
  type: TYPE_NORMAL
- en: 2]\orgdivDepartment of Computer Science and Engineering (DISI), \orgnameUniversity
    of Bologna, \orgaddress\streetViale Risorgimento, 2, \cityBologna, \postcode40136,
    \stateEmilia-Romagna, \countryItaly
  prefs: []
  type: TYPE_NORMAL
- en: ³\orgdivAdvanced Research Center on Electronic System (ARCES), \orgnameUniversity
    of Bologna, \orgaddress\streetVia Vincenzo Toffano, 2/2, \cityBologna, \postcode40125,
    \stateEmilia-Romagna, \countryItaly
  prefs: []
  type: TYPE_NORMAL
- en: 'RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: \fnmXin \surQiao    \fnmMatteo \surPoggi    \fnmPengchao \surDeng    \fnmHao
    \surWei    [cyge@mail.xjtu.edu.cn](mailto:cyge@mail.xjtu.edu.cn)    \fnmStefano
    \surMattoccia * [
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Integrating an RGB camera into a ToF imaging system has become a significant
    technique for perceiving the real world. The RGB guided ToF imaging system is
    crucial to several applications, including face anti-spoofing, saliency detection,
    and trajectory prediction. Depending on the distance of the working range, the
    implementation schemes of the RGB guided ToF imaging systems are different. Specifically,
    ToF sensors with a uniform field of illumination, which can output dense depth
    but have low resolution, are typically used for close-range measurements. In contrast,
    LiDARs, which emit laser pulses and can only capture sparse depth, are usually
    employed for long-range detection. In the two cases, depth quality improvement
    for RGB guided ToF imaging corresponds to two sub-tasks: guided depth super-resolution
    and guided depth completion. In light of the recent significant boost to the field
    provided by deep learning, this paper comprehensively reviews the works related
    to RGB guided ToF imaging, including network structures, learning strategies,
    evaluation metrics, benchmark datasets, and objective functions. Besides, we present
    quantitative comparisons of state-of-the-art methods on widely used benchmark
    datasets. Finally, we discuss future trends and the challenges in real applications
    for further research.'
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: RGB-guided ToF imaging, Deep learning, Depth super-resolution, Depth completion,
    Multimodal fusion
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The real world is three-dimensional, yet usually recorded with 2D vision sensors
    such as color cameras. To faithfully portray it, a diffuse trend is to employ
    a depth camera during collection, widely used nowadays in downstream tasks related
    to computer vision and pattern recognition, such as face anti-spoofing [[86](#bib.bib86),
    [19](#bib.bib19)], saliency detection [[16](#bib.bib16)], autonomous driving [[47](#bib.bib47),
    [162](#bib.bib162)] and virtual/augmented reality (VR/AR) [[54](#bib.bib54), [64](#bib.bib64)].
    Among depth cameras, time-of-flight (ToF) technology emerges as the preferred
    choice for many applications thanks to its compact structure, high precision,
    and low cost. Recently, the iPad Pro and iPhone Pro/Pro Max from Apple use LiDAR
    based on direct ToF (dToF) technology, often adopted in fields such as biometrics,
    photography, games, modeling, virtual reality, and augmented reality. The ToF-only
    imaging system is usually preferred when cost efficiency, energy efficiency, fast
    real-time, or privacy sensitivity are the primary considerations.
  prefs: []
  type: TYPE_NORMAL
- en: While ToF-only cameras might be the preferred choice in scenarios where depth
    sensing is paramount, some drawbacks still limit their performance in applications
    such as scene understanding, mixed reality, human-computer interaction, and facial
    recognition, where accuracy, versatility, and image content cues play a crucial
    role. In particular, the resolution of existing consumer-grade ToF cameras is
    about 300K pixels, with the LiDAR sensors – again based on the time of flight
    principle – used, for instance, by autonomous vehicles capturing even sparser
    measurements. In contrast, the resolution of RGB cameras used in mobile phones
    (e.g., Huawei P60) can reach dozens of MegaPixels, i.e., orders of magnitude higher,
    yet at a much lower cost. One solution is to directly enhance the low-quality
    (LQ) ToF depth map, i.e., to upsample [[127](#bib.bib127)] or densify [[153](#bib.bib153)]
    it, relying on hand-crafted or neural network-learned priors. However, these priors
    are likely to cause the predicted High-Resolution (HR) depth to contain blurred
    edges and other undesired artifacts. To face these issues, RGBD cameras integrate
    an HR RGB imager and a ToF sensor. Then, under the guidance of HR color images,
    sparse or Low-Resolution (LR) depth maps can be densified or super-solved by exploiting
    the structural details available from the high-resolution image content.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practical applications, ToF cameras based on different technical routes
    are deployed in different scenarios, mainly divided into the following three categories:
    1) for short-ranging acquisition in applications like face/gesture recognition,
    typically using an indirect ToF (iToF) camera with a uniform field of illumination,
    obtaining a dense depth map, yet at a low resolution; 2) for long-range measurements,
    as in the case of LiDAR sensors emitting laser pulses, which produces a scattered
    point cloud of the sensed scene: when coupled with a color camera, the sparsity
    of the depth map obtained by projecting the point cloud on the image plane depends
    on the resolution of the image, the higher, the sparser – i.e., containing only
    $5\%$ of pixels encoding valid depth measurements [[104](#bib.bib104)] on $\sim
    0.3$Mpx images; 3) for medium-ranging, as in the case of the ToF cameras used
    in augmented reality and automated guided vehicle robots, either iToF or dToF
    can be used. However, even when iToF is used for short-ranging, depth values around
    the corners or edges may be missing.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/afb9385ea4a1d7c0e8a12d566749cb47.png) |'
  prefs: []
  type: TYPE_TB
- en: '| \begin{overpic}[width=203.80193pt]{x1.pdf} \put(62.0,18.0){\color[rgb]{1,0,0}\line(2,0){12.0}}
    \put(62.0,1.0){\color[rgb]{1,0,0}\line(2,0){12.0}} \put(62.0,1.0){\color[rgb]{1,0,0}\line(0,2){17.0}}
    \put(74.0,1.0){\color[rgb]{1,0,0}\line(0,2){17.0}} \end{overpic} |'
  prefs: []
  type: TYPE_TB
- en: '| 375$\times$1242 |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/013bc4fdcabf6596cfcb777209621a65.png) | ![Refer to
    caption](img/81595a0deffdaac750e1b352a825fe7d.png) | ![Refer to caption](img/ada06ad6eb9ad2efbcc35ea7e78d66e9.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 375$\times$1242 | $\times 0.5$ | $\times 0.25$ |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/dcc951b7651079dab30c042d1587ee69.png) | ![Refer to
    caption](img/a6c42ea4d02244353fc0249ec8e703da.png) | ![Refer to caption](img/ad08bec223554d03e671e7d7b5ac3ecc.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 375$\times$1242 | $\times 2$ | $\times 4$ |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 1: An example of LiDAR point cloud from the KITTI dataset reprojected
    on images at different resolutions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the three cases, improving the quality of original depth maps translates
    into 1) upsampling the original, spatial resolution through Guided Depth Super-Resolution
    (GDSR) [[33](#bib.bib33), [50](#bib.bib50)], or 2) densifying the sparse depth
    map through Guided Depth Completion (GDC) [[153](#bib.bib153), [13](#bib.bib13)].
    The goal of GDSR is to utilize RGB images to increase pixel density for low-resolution
    depth and provide richer visual details, while GDC aims to fill in missing depth
    values in sparse depth maps obtained when using laser pulses, as well as increase
    the overall depth map resolution – as the sparsity of the depth map depends on
    the resolution of the image over which the sensed point cloud is projected (the
    lower the image resolution, the denser the projected depth map; the higher the
    image resolution, the sparser the depth map). In Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), we show an example of LiDAR point cloud from the KITTI dataset reprojected
    at the original resolution used in the dataset (375$\times$1242) as well as both
    at lower and higher resolution (respectively $\times$0.5, $\times$0.25, $\times$2
    and $\times$4). We can observe how assuming different camera resolutions produces
    depth maps whose density changes sensibly, with fewer holes present at lower resolutions
    and much sparser depth points at higher resolutions. This evidence confirms that
    the density of a depth map obtained from LiDAR scans is a function of the resolution
    at which we wish to obtain a depth map and that, to some extent, the completion
    process has analogies with the super-resolution task. The key difference with
    GDSR is that the laser spot emitted by LiDAR in GDC is tiny and typically cannot
    cover the scene corresponding to one pixel in the sensor. Therefore, GDC usually
    requires perceiving a larger neighborhood (e.g., $7\times 7$) than GDSR in shallow
    feature extraction of the input to estimate depth values in areas where the information
    is missing. Generally, RGB guided ToF imaging systems involve one of the two sub-tasks,
    depending on the use case and the depth sensing technologies. Consequently, different
    solutions must be deployed depending on the specific sub-task. Nonetheless, both
    tasks aim to yield high-quality depth maps with the guidance of RGB information,
    that is, transfer high-frequency structural information and fine-grained features
    from HR color images to LR/sparse depth maps while avoiding texture-copying artifacts.
    Furthermore, it is worth noting that the model design of the two tasks exhibits
    intriguing similarities, which inspired us to analyze them together as one topic
    for investigation, and their specific classification will be elaborated upon in
    Sec. [2.4](#S2.SS4 "2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF
    Imaging System: A Survey of Deep Learning-based Methods").'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/3bd63fa25c14f90e8856bc33a5e57e99.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 2: Timeline showing the advances in RGB guided ToF Imaging. Methods
    are marked in blue or purple, depending on the learning paradigm being unsupervised
    or supervised.'
  prefs: []
  type: TYPE_NORMAL
- en: At first, traditional methods extract meaningful features from color images
    through guided filtering-based [[116](#bib.bib116), [49](#bib.bib49), [87](#bib.bib87)]
    or optimization-based [[23](#bib.bib23), [33](#bib.bib33), [102](#bib.bib102)]
    algorithms. These hand-crafted approaches rely on the assumption that depth discontinuities
    statistically co-occur in correspondence with RGB image edges. In real-world cases,
    however, this prior may fail to extract informative features and introduce artifacts.
    For instance, in the presence of rich texture, these methods convey essential
    structural details but also transfer the appearance of RGB images to depth maps
    as a side effect. Therefore, this task remains challenging as it requires properly
    distinguishing between high-frequency image information corresponding to discontinuities
    in the depth map and those not. Over the past decades, solutions for enhancing
    RGB guided ToF imaging have shifted from model-driven to data-driven paradigms [[20](#bib.bib20),
    [168](#bib.bib168)], in particular thanks to deep learning methods [[10](#bib.bib10),
    [208](#bib.bib208), [196](#bib.bib196)] that have rapidly advanced the field.
    Even with a simple network structure, such as a few stacked convolutional layers [[153](#bib.bib153)]
    or the vanilla encoder-decoder [[78](#bib.bib78)] architecture, it is straightforward
    to recover HR/dense depth maps with much higher accuracy compared to what obtained
    through hand-crafted methods. Consequently, RGB guided ToF imaging via deep learning
    received increasing research interest from academia and industry, motivating us
    to survey recent progress in this field.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, given the substantial development of deep-learning solutions in
    the last decade, this survey aims to provide a complete overview of solutions
    designed for enhancing RGB guided ToF imaging in both use cases. Although we are
    aware of previous review papers on guided depth super-resolution [[199](#bib.bib199)]
    and depth completion [[174](#bib.bib174), [55](#bib.bib55)] treated as standalone
    tasks, we highlight the lack of interconnection between the two, despite the shared
    insights they bring to the community of researchers working with RGB guided ToF
    sensors, which we aim to envelop in a comprehensive survey. Purposely, we will
    review the literature concerning deep learning frameworks developed to enhance
    the depth maps perceived from this family of devices. We will study and classify
    them according to several aspects, such as the framework design, the learning
    paradigm, and the objective functions minimized during training. Moreover, we
    introduce the reader to the standard datasets used as benchmarks for evaluating
    existing methodologies, reporting the performance of state-of-the-art methods
    as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'We outline our major contributions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, we are the first to provide an in-depth investigation
    for RGB guided ToF imaging through deep learning, including guided depth super-resolution
    and guided depth completion as the main tasks to be dealt with according to the
    specific use case.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each subtask, this paper reviews recent deep learning-based methods under
    several aspects to illustrate the motivations behind their design, contributions,
    and performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss the challenges and future trends of RGB guided ToF imaging.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of this survey is structured as follows. In Section [2](#S2 "2 Preliminaries
    and Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), we describe preliminaries for RGB guided ToF imaging. Section. [3](#S3
    "3 Guided Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of
    Deep Learning-based Methods") and [4](#S4 "4 Guided Depth Completion ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods") present deep learning-based
    methods for GDSR and GDC, respectively. In Section [5](#S5 "5 Benchmark Datasets
    and objective functions ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), we introduce the datasets and loss functions. Section [6](#S6 "6 Evaluation
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") provides
    comparison results of some recent methods. Finally, we discuss future trends and
    challenges in Section [7](#S7 "7 Discussion and future trends ‣ RGB Guided ToF
    Imaging System: A Survey of Deep Learning-based Methods"). Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods") draws a timeline and reports for each year from 2017 to 2023, any method
    we will review in this paper, and the publication venue.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries and Taxonomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first introduce the foundation of RGB guided ToF imaging,
    then we present GDSR and GDC as the two main research trends aimed at enhancing
    ToF depth maps and the commonly used objective functions in both.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Principles of ToF Imaging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we describe the imaging principles of iToF and dToF, respectively. In
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and
    Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods"),
    we provide a schematic diagram of the principles commonly used by iToF and dToF.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/aabebd51f0f9f392f453e2c43b2a431a.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 3: iToF and dToF working principles. The former computes distances based
    on the phase shift between emitted and reflected light; the latter measures the
    time required for a laser light to bounce over an object.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Indirect ToF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Indirect ToF (iToF) determines distance by capturing reflected light and calculating
    the phase delay between emitted and reflected light. Currently, most of the off-the-shelf
    ToF cameras emit signals $s(t)$ using sinusoidal amplitude modulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s(t)=s_{1}\cos(\omega t)+s_{0}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Characteristics of different types of LiDAR.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Technology | Advantages | Disadvantages |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mechanical | Rotating mirrors or prisms | High accuracy, long range | Large,
    expensive |'
  prefs: []
  type: TYPE_TB
- en: '| MEMS | Micromirrors | Small, cheap | Lower FoV, range |'
  prefs: []
  type: TYPE_TB
- en: '| Flash | Arrays of lasers firing simultaneously | Fast, high resolution |
    Short range |'
  prefs: []
  type: TYPE_TB
- en: '| OPA | Optical phased arrays | Small, cheap, versatile | Still in development
    |'
  prefs: []
  type: TYPE_TB
- en: 'Since the baseline between the ToF transmitter and the receiver is so small
    (usually only a few millimeters), they can generally be considered coaxial. Assuming
    that the target is static, and the signal is only reflected once before being
    received by the sensor, the reflected signal of a single pixel measured at a specific
    fixed frequency can be expressed as a function of time $t$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $r(t)=\frac{\rho}{c^{2}\tau_{0}^{2}}[s_{1}\cos(\omega t-2\omega\tau_{0})+s_{0}]+e_{0}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\rho$ is the surface reflectivity of the target, $\omega$ denotes angular
    frequency, $\tau_{0}$ is the delay of signal reflection from the target back to
    a pixel at the speed of light $c\approx 3\times 10^{8}$m/s, and $e_{0}$ is the
    offset caused by ambient light. According to [[51](#bib.bib51)], Eq. [1](#S2.E1
    "In 2.1.1 Indirect ToF ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and Taxonomy
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") can
    be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle A_{1}=\frac{\rho s_{1}}{c^{2}\tau_{0}^{2}},\quad A_{0}=\frac{\rho
    s_{0}}{c^{2}\tau_{0}^{2}}+e_{0}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle r(t)=A_{1}\cos(\omega t-2\omega\tau_{0})+A_{0}$ |  | (4)
    |'
  prefs: []
  type: TYPE_TB
- en: 'The received signal remains sinusoidal, but the phase, which contains scene
    depth information, is changed. After a high-frequency periodic signal $B_{1}\cos(\omega
    t-\phi)$ modulates the incident signal, the modulated signal can be expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\tilde{I}_{\phi,\omega}(t)=$ | $\displaystyle r(t)B_{1}\cos(\omega
    t-\phi)$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{A_{1}B_{1}\cos(\phi-2\omega\tau_{0})}{2}+$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\frac{A_{1}B_{1}\cos(2\omega t+\phi+2\omega\tau_{0})}{2}+$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle A_{0}B_{1}\cos(\omega t-\phi)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Considering the quantum efficiency of ToF sensors, the sensor exposure time
    $T$ set to capture a sufficient number of photons is usually much larger than
    $\pi c/\omega$. During the exposure, the measurement is equivalent to integrating
    over the time range $T$, so the period terms in Eq. [5](#S2.E5 "In 2.1.1 Indirect
    ToF ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods") all vanish in the
    calculation, and the result can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle I_{\phi,\omega}$ | $\displaystyle=\int_{-T/2}^{T/2}\tilde{i}_{\phi,\omega}(t)dt$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\approx A_{1}B_{1}\cos(\phi-2\omega\tau_{0})=A_{1}g_{\phi,\omega}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $A_{1}$ is the scene response that represents the intensity of the encoded
    corresponding pixel at time $\tau_{0}$ after the laser transmitter sends a light
    pulse, $g_{\phi,\omega}=B_{1}\cos(\phi-2\omega\tau_{0})$ is the camera function,
    and $I_{\phi,\omega}$ is the raw measurement with correlation at phase $\phi$,
    angular frequency $\omega$, i.e. ToF raw data. If a single modulation frequency
    $f_{m}$ is used to collect multiple ($K\geq 2$) raw measurements in one cycle,
    the distance between the scene and camera can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d=\frac{c}{2\omega}\arctan(\frac{\sin{\boldmath{\phi}}\cdot\boldmath{I}_{\phi,\omega}}{\cos{\boldmath{\phi}}\cdot\boldmath{I}_{\phi,\omega}})\\
    $ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: In practice, if the modulation frequency $f_{m}=\frac{\omega}{2\pi}$ is fixed,
    ToF cameras usually adopt the four-phase method to sample ToF raw data, i.e.,
    sampling four times in one cycle, with a phase step of $90^{\circ}$. For ease
    of reading, the four raw data can be abbreviated as $I_{i}=I_{\frac{\pi}{2\omega}i},i=0,1,2,3$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle A=\frac{\sqrt{(I_{3}-I_{1})^{2}+(I_{0}-I_{2})^{2}}}{2}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle B=\frac{I_{0}+I_{1}+I_{2}+I_{3}}{4}$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\phi=\arctan{\frac{I_{3}-I_{1}}{I_{0}-I_{2}}}$ |  | (10)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle d=\frac{c}{2\pi}(\frac{\phi}{2f}+N)$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $N$ is an integer that represents the number of phase wraps. Based on
    the above principle, some works[[144](#bib.bib144), [15](#bib.bib15), [46](#bib.bib46),
    [76](#bib.bib76)] directly process the raw signal of the iToF, thus improving
    the depth quality.
  prefs: []
  type: TYPE_NORMAL
- en: At present, most iToF for consumer use sensors based on complementary metal-oxide
    semiconductor (CMOS) or charge-coupled device (CCD), with a resolution of $320\times
    240$ (QVGA) or $640\times 480$ (VGA) pixels. However, the resolution is still
    significantly lower than that of an RGB image using the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3618d16c14ee7b2e61537fdf4366b67e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Principle of mechanical LiDAR
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0bdfd02a168a418cad2fc027ae91d98d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Principle of MEMS LiDAR
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5baab47ef44cd04b5f6427f222d7fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Principle of flash LiDAR
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0e57a51a48429bd07644a532ba42422.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Principle of OPA LiDAR
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Schematic diagram of LiDAR systems with different imaging principles.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Direct ToF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DToF is a method that measures the time it takes for laser light to be fired
    directly at an object and returned to the sensor. The moment the laser is fired,
    the electronic clock is activated. Then, the pulse bounces off the target and
    is partially picked up by the photodetector. By measuring the time lapse $\Delta
    T$, the distance $d$ between the outgoing light pulse and the reflected light
    pulse is calculated with
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d=\frac{c\Delta T}{2}\\ $ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: To achieve higher sensitivity and lower power consumption, dToF usually adopts
    single photon avalanche diodes (SPADs) instead of Avalanche Photodiodes (APDs)
    as photodetectors. Due to limitations in the size of the chip and its auxiliary
    circuits, the pixel count of SPAD arrays is much lower than that of iToF.
  prefs: []
  type: TYPE_NORMAL
- en: 'LiDAR, an acronym for Light Detection and Ranging, despite being based on the
    same working principle as dToF, is frequently listed as a distinct depth sensing
    technology due to some of its peculiar design and setup. For instance, conventional
    LiDAR continuously scans the environment utilizing mechanical components (e.g.,
    spinning mirrors). Another type is MEMS LiDAR, which uses micromirrors controlled
    by micro-electromechanical systems (MEMS) to scan the environment. This type of
    LiDAR is smaller and cheaper than mechanical LiDARs but has lower FoV and range.
    As a still developing technology, OPA LiDARs use optical phased arrays (OPAs)
    to steer the laser beam electronically, and they have the potential to be small,
    cheap, and versatile. Different from the techniques mentioned above, global shutter
    flash LiDARs, also referred to as flash LiDARs or ToF LiDARs, use arrays of lasers
    firing simultaneously to create a 3D image of the surrounding environment, just
    like iToF, thereby improving frame rate and imaging efficiency for applications
    such as autonomous driving. This technology enables the integration of dToF into
    consumer electronic devices. We provide a comparison summarizing the different
    types of LiDAR in Table. [1](#S2.T1 "Table 1 ‣ 2.1.1 Indirect ToF ‣ 2.1 Principles
    of ToF Imaging ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging System:
    A Survey of Deep Learning-based Methods") and the schematic diagram of each method
    in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.1 Indirect ToF ‣ 2.1 Principles of ToF Imaging
    ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep
    Learning-based Methods").'
  prefs: []
  type: TYPE_NORMAL
- en: In essence, LiDAR still employs the dToF principle, and the two terms of dToF
    and LiDAR are even interchangeable in many scenarios (e.g., Apple refers to the
    dToF sensor in iPad Pro and iPhone 12 Pro as a LiDAR scanner). However, LiDAR
    technology is rapidly evolving, and new state-of-the-art LiDARs are constantly
    being developed, thus potentially having more applications in various scenarios.
    For instance, the InnovizOne MEMS LiDAR, manufactured by EDOM technology, achieves
    an angular resolution of up to $0.1^{\circ}\times 0.1^{\circ}$, producing much
    denser point clouds than before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Characteristics of iToF and dToF.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | iToF | dToF |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Principle |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Using phase shift between emitted &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and reflected light to determine distance. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Using stop watch method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to calculate time lapse. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Detector type |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PMDs: $6\sim 100\mu m$ pixel size &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SPADs / APDs |'
  prefs: []
  type: TYPE_TB
- en: '| Depth calculation | In-pixel calculation | Histogram analysis |'
  prefs: []
  type: TYPE_TB
- en: '| Performance | Long integration time | Fast acquisition |'
  prefs: []
  type: TYPE_TB
- en: '| Range | Short-medium | Longer |'
  prefs: []
  type: TYPE_TB
- en: '| Range ambiguity | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Pixel count | Large | Smaller |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | Linearly related to distance | Higher |'
  prefs: []
  type: TYPE_TB
- en: '| Power consumption | Low | Higher |'
  prefs: []
  type: TYPE_TB
- en: '| Portability | Generally compact | Traditionally larger |'
  prefs: []
  type: TYPE_TB
- en: '| Cost | Medium | Higher |'
  prefs: []
  type: TYPE_TB
- en: 2.1.3 Comparative Analysis of dToF and iToF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here, we provide their characteristics in Table [2](#S2.T2 "Table 2 ‣ 2.1.2
    Direct ToF ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and Taxonomy ‣ RGB
    Guided ToF Imaging System: A Survey of Deep Learning-based Methods"). Besides,
    we conduct a comparative analysis on the following aspects.'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy. iToF cameras typically provide accurate distance measurements within
    a specific range. Its measurement error is theoretically positively correlated
    with the target distance. Currently, the accuracy of mass-produced iToF cameras
    can be controlled within $1\%$, e.g., Helios2 ToF 3D Camera with Sony’s IMX556
    DepthSense ToF has an accuracy of less than 5mm and a precision of less than 2mm
    at 1m camera distance. At the same time, dToF (LiDAR) can achieve millimeter-level
    accuracy even over longer distances. Therefore. for short-range applications,
    both iToF and dToF can achieve comparable accuracy, although they may fall short
    of structured light cameras. However, a key advantage of LiDAR is that its accuracy
    substantially remains constant regardless of distance, as discussed in Sec 2.1.3\.
    This attribute positions LiDAR measurements as reliably accurate in long-range
    applications, distinguishing them from other technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Range and Power consumption. iToF cameras are generally suitable for short to
    medium-range applications. The effective range may vary but typically within a
    few to tens of meters, depending on the specific design and implementation. Because
    of these characteristics, ToF cameras are often designed to be power-efficient
    and, therefore, suitable for consumer-grade devices. In contrast, LiDAR systems
    can achieve longer ranges, making them suitable for applications like autonomous
    vehicles and long-range mapping. Consequently, they are commonly employed in scenarios
    where power consumption is less critical.
  prefs: []
  type: TYPE_NORMAL
- en: Portability. iToF cameras are inherently compact and lightweight, making them
    suitable for integration into portable devices such as smartphones, tablets, and
    wearable gadgets. Meanwhile, LiDAR, traditionally larger, is undergoing miniaturization
    trends, especially with the development of solid-state LiDAR, making it increasingly
    applicable in portable and handheld devices.
  prefs: []
  type: TYPE_NORMAL
- en: Applications. Due to the properties peculiar to each, the two families of sensors
    are usually involved in different downstream applications. Specifically, iToF
    sensors are the best fit for AR/VR experiences, which mainly focus on narrow,
    close-range environments or, in general, for any applications running in a constrained
    space (e.g., bin picking with a robotic arm). On the contrary, dToF is preferable
    in contexts requiring long-range perception, e.g., the automotive one with ADAS
    or autonomous driving systems. It is worth noting that there is an overlap in
    the operating ranges of iToF and dToF, i.e., both can be used for applications
    such as AR/VR (e.g., Kinect v2 with iToF and iPad Pro with dToF) when performing
    medium-range sensing.
  prefs: []
  type: TYPE_NORMAL
- en: Though both imaging systems currently excel in their respective application
    domains, advancements in technology are steadily bridging the gap between their
    performance metrics, leading to a growing overlap in their practical uses.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although ToF cameras have many advantages, they also bring some inherent limitations,
    such as multipath interference [[45](#bib.bib45), [46](#bib.bib46)], flying pixels [[119](#bib.bib119)]
    and wiggling errors[[59](#bib.bib59)]. Hand-crafted models can deal with some
    issues, e.g., depth correction, but others require additional cues, such as those
    available in RGB images.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of RGB guided ToF imaging is to recover a high-quality depth map $Z_{hq}$
    from the acquired low-quality depth map $Z_{lq}:\Omega_{Z}\subset\Omega\mapsto\mathbb{R}_{+}$
    and high-resolution color image $I:\Omega\subset\mathbb{R}^{2}\mapsto\mathbb{R}^{3}_{+}$.
    Depending on the setup of ranging systems, low-quality depth maps can be either
    dense, yet at low resolution, or sparse. Besides, we assume all LQ depth maps
    and the corresponding color images are correctly aligned. This is possible if
    the RGB-D system is calibrated, i.e., the relative pose between the two color
    and depth cameras is known. If a deep neural network $\Phi_{\gamma}$ with parameters
    $\gamma$ is deployed for RGB guided ToF imaging, the task can be modeled as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{Z}_{hq}=\Phi_{\gamma}(Z_{lq},I)$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{Z}_{hq}$ is the prediction of latent HR depth map.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network parameters $\gamma$ are updated during training of $\Phi$, by solving
    the following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\gamma}=\mathop{\arg\min}\limits_{\gamma}\mathcal{L}(Z_{hq},\hat{Z}_{hq})$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}$ is an objective function, usually minimizing the distance
    between the prediction and the ground truth depth.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As for most computer vision tasks, for RGB guided ToF imaging, it is necessary
    to employ appropriate evaluation metrics. The most commonly used measures for
    both the two sub-tasks, i.e., GDSR and GDC, are root mean squared error (RMSE)
    and mean absolute error (MAE):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathrm{RMSE}(mm)=\sqrt{\frac{1}{N}\sum_{p\in N}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}}$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle N\mathrm{MAE}(mm)=\frac{1}{N}\sum_{p\in N}&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: with $p$ being a single pixel in the depth map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics for GDSR. In addition to the above metrics, mean squared error (MSE)
    is frequently utilized, which has a similar role to RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{MSE}=\frac{1}{N}\sum_{p\in N}&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;^{2}$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'Furthermore, with a short-range ToF sensor, GDSR focuses on recovering depth
    maps with desirable details. As a result, peak signal-to-noise ratio (PNSR) and
    structural similarity index (SSIM) are also sometimes used to assess the quality
    of depth maps, which are defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathrm{PSNR}=10log_{10}(\frac{Z_{max}^{2}}{\mathrm{MSE}})$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathrm{SSIM}(p,q)=\frac{(2\mu_{p}\mu_{q}+C_{1})(2\sigma_{xy}+C_{2})}{(\mu_{p}^{2}+\mu_{q}^{2}+C_{1})(\sigma_{p}^{2}+\sigma_{q}^{2}+C_{2})}$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: with $Z_{max}$ being the maximum depth value; $q$ is a neighbor of pixel $p$;
    $\mu_{p}$ and $\mu_{q}$ denote the mean values of HQ depth maps and the corresponding
    ground truth, respectively; $\sigma_{p}^{2}$ and $\sigma_{q}^{2}$ are the variance;
    $C_{1}$ and $C_{2}$ are constants used to maintain the stability of the division.
    The smaller the pixel value difference between the two depth maps, the higher
    the PSNR.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for GDC. Sparse depth measurements are often captured from a long-range
    ToF LiDAR, so there are several task-specific metrics for GDC, including RMSE
    of the inverse depth (iRMSE), MAE of the inverse depth (iMAE), which are defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathrm{iRMSE}(\frac{1}{km})=\sqrt{\frac{1}{n}\sum_{n\in
    N}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}}$ |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathrm{iMAE}(\frac{1}{km})=\frac{1}{N}\sum_{p\in N}&#124;\frac{1}{Z_{hq}^{p}}-\frac{1}{\hat{Z}_{hq}^{p}}&#124;$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: When the evaluation of deep models is performed on indoor datasets, e.g., NYU-v2 [[138](#bib.bib138)],
    mean absolute relative error (REL) and thresholded accuracy are more popular
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathrm{REL}(mm)=\frac{1}{N}\sum_{n\in N}\frac{&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;}{Z_{hq}^{p}}$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\sigma=\max(\frac{Z_{hq}^{p}}{\hat{Z}_{hq}^{p}},\frac{\hat{Z}_{hq}^{p}}{Z_{hq}^{p}})<th$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: with $th$ being a given threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Taxonomy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/268cb138edff4951bd613eecdc47604b.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Taxonomy of RGB guided ToF imaging techniques. We identify 5 and
    6 main categories for GDSR and GDC methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5](#S2.F5 "Figure 5 ‣ 2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy ‣
    RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") anticipates
    the proposed taxonomy for methods we will survey in this paper. On the one hand,
    from the figure, we can already highlight some common trends in GDSR and GDC tasks.
    Specifically, we can identify the category of Dual-branch Frameworks in both,
    i.e., those deploying two main branches for processing RGB and depth cues separately,
    as well as the category of Auxiliary-based Methods – those exploiting multi-task
    learning to improve the accuracy of the predicted depth map – Fusion-focused Methods
    which mainly study the fusion strategy for combining color and depth features,
    and the Unsupervised Methods not requiring ground-truth labels at training time.
    On the other hand, we identify some trends that are peculiar to one of the two
    tasks: specifically, for GDSR, we have Hybrid Methods, focusing on obtaining explainable
    models to run the super-resolution process; for GDC, Affinity-based Methods developed
    are characterized by the used of Spatial Propagation Networks (SPNs) which learn
    affinity matrices for propagating depth across neighboring pixels, and Uncertainty-guided
    Methods explicitly involve uncertainty modeling to improve the results. Each category
    will be introduced and discussed in detail in Sections[3](#S3 "3 Guided Depth
    Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods") and [4](#S4 "4 Guided Depth Completion ‣ RGB Guided ToF Imaging System:
    A Survey of Deep Learning-based Methods"), for GDSR and GDC, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Guided Depth Super-Resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present and discuss recent deep learning-based approaches.
    Initially, researchers exploited general architectures with two branches to extract
    features from RGB and depth separately, which are fused in a naive manner, i.e.,
    by means of concatenation or summation. Numerous novel architectures and fusion
    schemes have been proposed to boost network accuracy. From the perspective of
    data requirements, depth super-resolution methods are divided into supervised
    and unsupervised learning. The difference between the two paradigms is that the
    former requires labeled input data, while the latter does not. For what concerns
    supervised learning, we classify existing works into three categories, depending
    on the main contribution they bring to the literature, in terms of (i) Dual-branch
    framework; (ii) optimization-inspired architecture; (iii) auxiliary learning;
    (iv) multi-modal fusion scheme. For unsupervised learning, the emphasis is on
    the fusion strategy and loss functions. These five categories are listed in the
    top branch of Fig. [5](#S2.F5 "Figure 5 ‣ 2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods").'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised methods are supplemented by labeled data so that the task can be
    viewed as a regression problem. Thus, deep models aim to learn depth predictions
    as close as possible to the ground truth depth labels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Dual-branch Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the depth sensor captures the same scene as the RGB camera, the obtained
    two images are geometrically similar and complementary. Therefore, how to extract
    meaningful features from the two modalities through designed algorithms is the
    key to depth super-resolution. Intuitively, [[79](#bib.bib79)] design a deep joint
    filter (DJFR) which contains two sub-networks for extracting depth and RGB features,
    respectively. After concatenating the two streams, the fused information is decoded
    by another sub-network and outputs the predicted depth, as seen in Fig.[6](#S3.F6
    "Figure 6 ‣ 3.1 Dual-branch Framework ‣ 3 Guided Depth Super-Resolution ‣ RGB
    Guided ToF Imaging System: A Survey of Deep Learning-based Methods").'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-scale information is crucial for many low-level vision tasks, including
    GDSR, which can recover images from different scales through rich hierarchical
    features. The multi-scale guided convolutional network (MSG-Net) [[58](#bib.bib58)]
    is the first work to super-solve depth maps using multi-scale guidance, which
    progressively enhances depth details from high- to low-level. Inspired by this
    idea, numerous methods utilizing multi-scale guidance have been proposed and attained
    significantly improved performance. For instance, a novel deep network for depth
    map super-resolution (DepthSR) proposed by[[44](#bib.bib44)] leverages a residual
    U-Net architecture[[129](#bib.bib129)], where hierarchical guidance is introduced
    at each scale, to recover HR depth maps. [[209](#bib.bib209)] design a multi-scale
    architecture with a guide similar to MSG-Net[[58](#bib.bib58)] where dense layers[[57](#bib.bib57)]
    are employed to revisit the features from all higher levels at a given scale.
    Also, guided by RGB images, the multi-scale fusion residual network for GDSR (MFR-SR)
    proposed by [[211](#bib.bib211)] progressively upsamples depth maps in multiple
    scales with global and local residual learning. [[77](#bib.bib77)] employ a multi-scale
    strategy with the proposed symmetric unit (SU) as the basic component of the network.
    With SU, it can effectively deal with textureless and edge features in depth images.
    Unlike previous works, [[210](#bib.bib210)] propose a two-stream multi-scale network,
    MIG-net, including three branches, i.e., intensity, depth, and gradient branches.
    At each scale, the depth and gradient features are iteratively refined by the
    guide. [[197](#bib.bib197)] introduce an attention-based hierarchical multi-modal
    fusion network (AHMF) where a bi-directional hierarchical feature collaboration
    module is proposed to make full use of multi-scale features. In this module, features
    from different levels are aggregated so that low and high level spatial information
    can collaborate to improve each other. [[160](#bib.bib160)] present a novel method
    that exploits pyramid structure to capture multi-scale features so that HR depth
    maps can be progressively recovered. [[187](#bib.bib187)] propose a structure
    flow-guided framework for GDSR, whose core is to learn a structure flow map to
    guide structure representation transfer. In this method, a flow-enhance pyramid
    edge attention network is introduced to learn multi-scale features for sharp edge
    reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/10637d920f41856ab3f49e4b9febaf0e.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 6: DJFR architecture [[79](#bib.bib79)] belonging to dual-branch framework.
    It employs a typical dual-branch auto-encoder framework, where the guide and source
    features are extracted from the two sub-networks in the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a coarse-to-fine scheme is another common solution for GDSR, consisting
    of building a multi-stage network to enhance depth maps progressively. The definitions
    of the multi-scale strategy and the coarse-to-fine scheme may overlap in some
    approaches. Thus, we distinguish them according to the focus of the works. [[167](#bib.bib167)]
    propose a coarse-to-fine neural network with two stages. In the coarse stage,
    the model learns large filter kernels to obtain less accurate results, further
    refined by smaller filter kernels in the fine stage. In [[208](#bib.bib208)],
    the filtering and refinement of depth-guided intensity features and intensity-guided
    depth features are iteratively conducted in multiple stages through the proposed
    depth-guided affine transformation. The network can reduce artifacts produced
    by distribution gaps between depth maps and the corresponding guide. [[183](#bib.bib183)]
    introduce a multi-branch aggregation network with multiple stages to reconstruct
    sharp depth boundaries. Specifically, they design three parallel branches – the
    reconstruction, the color, and the multi-scale branches – in each stage. [[141](#bib.bib141)]
    develop a coarse-to-fine framework consisting of several sub-modules that can
    gradually extract the high-frequency features from depth maps. Each sub-module
    employs a channel attention strategy to obtain informative features. Besides,
    the framework further improves the depth quality through Total Generalized Variation
    (TGV) term and input loss. In [[50](#bib.bib50)], a high-frequency guidance branch
    is developed to adaptively extract the HF information and reduce the LF components.
    The HF components are then fused with depth features by means of concatenation.
    [[186](#bib.bib186)] propose a recurrent structure attention guidance framework
    (RSAG) that utilizes both multi-scale and multi-stage strategies, shown in Fig.[7](#S3.F7
    "Figure 7 ‣ 3.1 Dual-branch Framework ‣ 3 Guided Depth Super-Resolution ‣ RGB
    Guided ToF Imaging System: A Survey of Deep Learning-based Methods"). In this
    framework, a deep contrastive network with multi-scale filters is designed to
    adaptively separate HF and LF features.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/740fb5aabd80468f6a0f90245d520419.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 7: RSAG architecture [[186](#bib.bib186)] belonging to dual-branch framework.
    It progressively refines the depth using a coarse-to-fine strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of deploying spatially-invariant kernels in vanilla convolutional neural
    networks (CNNs), some methods design learnable kernels to boost the GDSR performance.
    [[68](#bib.bib68)] propose a deformable network that provides a custom convolution
    kernel for each pixel by computing neighborhood weights. The kernel weights can
    be obtained not only by calculating the pixels in regular positions (e.g., 8-neighborhood),
    but also by calculating its sub-pixels after interpolation. [[161](#bib.bib161)]
    introduce a continuous depth representation whose core is the proposed geometric
    spatial aggregator (GSA). The GSA comprises two parts: (1) the geometric encoder
    using the scale-modulated distance field to build the correlation between pixels
    and (2) the learnable kernel learning typical texture pattern processing prior.
    Thanks to the continuous representation, the model can super-solve depth at an
    arbitrary scale. [[198](#bib.bib198)] design a kernel generation network that
    can cope with inconsistent structures between RGB and depth.'
  prefs: []
  type: TYPE_NORMAL
- en: To increase the interpretability, [[149](#bib.bib149)] regards GDSR as a neural
    implicit interpolation problem, which can map continuous coordinates of LR depth
    and HR color images into latent codes. Meanwhile, they learn the interpolation
    weights to establish a unified framework to yield the interpolation weights and
    values. To promote the deployment of dToF cameras in mobile devices, [[80](#bib.bib80)]
    perform depth estimation from a lightweight ToF sensor and RGB image (DELTAR).
    In this work, PointNet[[117](#bib.bib117)] and Efficient B5[[148](#bib.bib148)]
    are used to extract depth and RGB features, respectively, fused at the decoder
    stage through the proposed transformer-based fusion module.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Hybrid Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As neural networks are usually described as black-box models, some researchers
    combine optimization methods to obtain explainable frameworks. [[126](#bib.bib126)]
    present a novel network consisting of two sub-networks, i.e., a fully-convolutional
    network and a primal-dual network, with the first yielding HR depth maps and weights
    and the latter producing the final results with a non-local variational method.
  prefs: []
  type: TYPE_NORMAL
- en: As an optimization-inspired network, a weighted analysis sparse representation
    (WASR) model [[40](#bib.bib40)] is designed for GDSR. It leverages a neural network
    to learn filter parameterizations and non-linear functions to build more flexible
    stage-wise operations. Inspired by the multi-modal convolutional sparse coding
    model[[139](#bib.bib139)], [[21](#bib.bib21)] develop a network that can adaptively
    separate the shared features between different modalities from the unique features
    existing in a single modality. [[108](#bib.bib108)] propose a deep unfolding network
    to efficiently compute the convolutional sparse coding with the guide.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/318f6ccc33ae80571167f0d5d15dfad5.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 8: LGR architecture [[18](#bib.bib18)] belonging to optimization-inspired
    architecture. The optimization layer is plugged into the end of the network to
    predict the high-resolution depth.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[196](#bib.bib196)] introduce discrete cosine transform (DCT) into the proposed
    neural network to make the model explainable. This model utilizes DCT to tackle
    the optimization issue for GDSR by reconstructing the multi-channel HR depth features.
    Another advantage of using DCT is that it can make network design easier because
    it does not need to learn the mapping function between LR/HR image pairs, as it
    explicitly models it. [[18](#bib.bib18)] propose a novel architecture learning
    graph regularization (LGR) with a neural network to perform GDSR, as shown in
    Fig.[8](#S3.F8 "Figure 8 ‣ 3.2 Hybrid Architecture ‣ 3 Guided Depth Super-Resolution
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods"). First,
    a network extracts features from the source and the guide. A graph constructed
    using such features is then sent to a differentiable optimization layer to regularize
    it. Combining the advantages of model-based and learning-based methods, [[200](#bib.bib200)]
    propose a memory-augmented deep unfolding network in which local implicit prior
    and global implicit prior are introduced based on a maximal posterior view. To
    further prevent information loss, long short-term unit (LSTM) is employed in the
    persistent memory mechanism. [[110](#bib.bib110)] propose integrating guided anisotropic
    diffusion into a CNN to enhance depth discontinuities in the depth images. It
    has a fixed inference time and memory footprint, given a scale factor.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Auxiliary-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Auxiliary learning, also known as multi-task learning, aims to find or design
    auxiliary tasks that can improve the performance of one or several primary tasks.
    Driven by its recent progress, several works attempt to exploit auxiliary task
    learning to upsample depth maps. [[145](#bib.bib145)] propose a knowledge distillation
    approach where depth estimation is employed as an auxiliary task during training
    to improve the results. As a representative method shown in Fig.[9](#S3.F9 "Figure
    9 ‣ 3.3 Auxiliary-based Methods ‣ 3 Guided Depth Super-Resolution ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods"), BridgeNet [[151](#bib.bib151)]
    explores a paradigm where the association between monocular depth estimate (MDE)
    and depth super-resolution (DSR) is exploited deeply. To make the two tasks work
    together, they design two auto-encoders, namely DSRNet and MDENet, with information
    interaction at the encoder stage. In MDENet, a high-frequency attention bridge
    is proposed to capture HF information which can guide depth upsampling in the
    other task. In contrast, the content guidance is provided by DSRNet to guide monocular
    depth estimates. Considering the distinct learning curves of the two sub-tasks,
    they optimize the two sub-networks separately.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to auxiliary learning, some researchers consider using auxiliary
    guidance from color images to improve depth quality. Based on the importance of
    edge information in depth upsampling, [[165](#bib.bib165)] propose an edge-guided
    depth upsampling framework that leverages edge maps as the guide to recover HR
    depth maps. For training, the ground truth edge map is computed by the Canny operator
    on the corresponding HR depth map.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/6e53721dfc4da3ae9e8eda42ea86a096.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 9: BridgeNet architecture [[151](#bib.bib151)] belonging to auxiliary-based
    methods. The BridgeNet consists of a monocular depth estimation subnetwork and
    a guided depth upsampling subnetwork.'
  prefs: []
  type: TYPE_NORMAL
- en: Differently from most existing GDSR methods, [[155](#bib.bib155)] attempt to
    gauge the upsampled depth quality through renderings of surface normal maps. More
    specifically, a visual appearance-based loss is proposed, which can assist a baseline
    network in yielding more visually pleasing results.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Fusion-focused Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'How to effectively fuse information from different modalities is critical to
    obtain high-quality depth maps. Intuitively, we can perform multi-modal fusion
    with a simple operation, such as concatenation [[165](#bib.bib165), [149](#bib.bib149),
    [18](#bib.bib18)]. However, these fusion schemes can not selectively transfer
    HF features from the guide to the target, and may even result in texture-copying
    artifacts. Therefore, researchers design different fusion schemes to alleviate
    the issue, mainly grouped into three categories: early, late, and multi-level
    fusion. [[77](#bib.bib77)] present a correlation-controlled color guidance block
    (block) to fuse the multi-modal information. [[197](#bib.bib197)] design a multi-modal
    attention-based fusion strategy, including a feature enhancement block and a feature
    recalibration block, with the former paying more attention to meaningful features
    extracted from depth maps and color images and the latter aiming at rescaling
    multi-modal features. This strategy can effectively avoid the texture-copying
    effect in the final prediction. [[196](#bib.bib196)] deploy an enhanced spatial
    attention block to transfer meaningful structural information from RGB to depth.
    [[186](#bib.bib186)] develop a recurrent structure attention block, where the
    latest depth estimate and its corresponding HR color image are taken as the input,
    to obtain useful HF features of the color image. [[160](#bib.bib160)] propose
    a multi-perspective cross-guided fusion filter block, to improve depth features
    gradually by fusing the structure details in RGB images. In this block, spatial
    representations from various views are learned to capture depth salient structures
    further. Moreover, a color-depth cross-attention module is employed to achieve
    edge preservation. [[198](#bib.bib198)] propose a multi-scale guided filtering
    module to refine the depth map in a coarse-to-fine manner. To obtain stable depth
    predictions, [[147](#bib.bib147)] conduct cross-modal fusion in a multi-frame
    manner, i.e., dToF depth video super-resolution (DVSR) and histogram video super-resolution
    (HVSR), to upsample LR depth maps. [[120](#bib.bib120)] propose an Adaptive Feature
    Fusion Module (AFFM), which enables the recovery of fine details from the HR guide
    and the LR depth maps. [[195](#bib.bib195)] propose a Spherical Space feature
    Decomposition network (SSDNet), which projects encoded features onto the spherical
    space. This strategy allows for separating and aligning domain-shared and domain-private
    features, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Unsupervised Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since collecting training data paired with annotations is labor-intensive and
    time-consuming, researchers seek to solve the issue by leveraging self-supervised
    learning, allowing training models without labels. In [[103](#bib.bib103)], GDSR
    is regarded as a pixel-wise mapping from the source to the target, implemented
    as a multi-layer perceptron. This formulation can be trained in a fully unsupervised
    manner with the constraint of having the LR source. [[135](#bib.bib135)] present
    a single-pair method that can upsample depth maps even when the input pairs are
    misaligned. During training, patches cropped from the input pairs are utilized
    as pseudo-label data to enable weakly supervised learning. To improve generalization,
    [[25](#bib.bib25)] propose a mutual modulation super-resolution model (MMSR),
    where a cross-modality modulation strategy using adaptive filters transfers meaningful
    features from one modality to the other, as shown in Fig.[10](#S3.F10 "Figure
    10 ‣ 3.5 Unsupervised Methods ‣ 3 Guided Depth Super-Resolution ‣ RGB Guided ToF
    Imaging System: A Survey of Deep Learning-based Methods"). In this mutual modulation,
    the spatial relationship between the corresponding pixels of the two modalities
    is fully exploited. Moreover, a cycle consistency loss is adopted to enforce the
    target faithful to the source. Differently from previous unsupervised methods
    for GDSR, [[136](#bib.bib136)] introduce a pretext task into this field. Through
    the proposed scene structure guidance network, explicit structure features of
    color images can be obtained, which, together with the corresponding LR depth
    maps, serve as the input of a baseline network to produce high-resolution depth
    maps. [[159](#bib.bib159)] construct an adversarial network where the dependency
    between RGB images and depth maps is fully exploited to enhance the depth. Further,
    the optimal transport theory is introduced into the framework to boost depth enhancement
    performance. [[121](#bib.bib121)] exploit a contrast learning scheme to extract
    unique features from the guidance, which can boost the GDSR performance based
    on a baseline network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude this section, Tab. [3](#S3.T3 "Table 3 ‣ 3.5 Unsupervised Methods
    ‣ 3 Guided Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of
    Deep Learning-based Methods") collects any of the methods discussed so far, divided
    into five categories. For each method, the venue and year are reported, together
    with a short description of the key idea behind it.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/db9b5829210e76e477be63807f8903b4.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 10: MMSR framework [[25](#bib.bib25)] belonging to unsupervised methods.
    The MMSR adopts the cycle consistency loss to train the network in a self-supervised
    manner.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paradigm | Method | Reference | Key Idea |'
  prefs: []
  type: TYPE_TB
- en: '| Dual-branch Framework | DJFR [[79](#bib.bib79)] | TPAMI-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Introduce two sub-networks for extracting features from the target and
    guidance images. Following the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; concatenation operation, the fused information is fed into another sub-network
    for the predicted depth. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DMSG [[58](#bib.bib58)] | ECCV-2016 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; First work to super-solve depth maps using multi-scale guidance, which
    progressively enhances depth details from &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; high- to low-level. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DepthSR [[44](#bib.bib44)] | TIP-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Exploit a residual U-Net structure, where hierarchical guidance is used
    at each scale, to recover HR depth maps. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MGD [[209](#bib.bib209)] | TCSVT-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a multi-scale architecture based on dense layers that aim to
    revisit the features from all higher levels at a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; given scale. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MFR-SR [[211](#bib.bib211)] | TCSVT-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Combine global and local residual learning to upsamples depth maps from
    coarse to fine via multi-scale frequency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; synthesis. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MIG-net [[210](#bib.bib210)] | TMM-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Introduce a two-stream multi-scale network to extract features in the
    image and gradient domains, where the depth &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; features and gradient features are alternatively complemented with each
    other. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SFG [[187](#bib.bib187)] | AAAI-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a structure flow-guided framework for GDSR, whose key point
    is to learn a structure flow map to guide &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; structure representation transferring. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CCFN [[167](#bib.bib167)] | TIP-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a coarse-to-fine neural network with two stages. In the first
    stage, the model learns large filter kernels to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; obtain coarse results, which are further refined by smaller filter kernels
    in the second stage. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CGN [[208](#bib.bib208)] | TMM-2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; With the proposed depth-guided affine transformation, the intensity-guided
    depth feature filtering and refinement &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; are carried out iteratively in multiple stages. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| PMBANet [[183](#bib.bib183)] | TIP-2020 | Introduce a multi-branch aggregation
    network with multiple stages to reconstruct sharp depth boundaries. |'
  prefs: []
  type: TYPE_TB
- en: '| IRLF [[141](#bib.bib141)] | CVPR-2020 | Develop a coarse-to-fine framework
    consisting of several sub-modules with channel attention strategies. |'
  prefs: []
  type: TYPE_TB
- en: '| DKN [[68](#bib.bib68)] | IJCV-2021 | Design a deformable kernel network that
    obtains a kernel for each pixel based on the neighborhood weights. |'
  prefs: []
  type: TYPE_TB
- en: '| FDSR [[50](#bib.bib50)] | CVPR-2021 | Explore high-frequency (HF) information
    via an HF guidance branch, where the HF information is fused with depth features
    to improve the performance. |'
  prefs: []
  type: TYPE_TB
- en: '| GeoDSR [[161](#bib.bib161)] | AAAI-2023 | Introduce a continuous depth representation
    to effectively implement scale-continuous and spatial-continuous upsampling in
    guided depth super-resolution. |'
  prefs: []
  type: TYPE_TB
- en: '| DAGF [[198](#bib.bib198)] | TNNLS-2023 | Design a kernel generation network
    that can cope with inconsistent structures between RGB and depth, where a multi-scale
    guided filtering module is proposed to refine the depth map in a coarse-to-fine
    manner. |'
  prefs: []
  type: TYPE_TB
- en: '| DCSR [[164](#bib.bib164)] | Display-2023 | Proposes a depth map continuous
    SR framework that can achieve resolution adaptation at arbitrary SR ratios. |'
  prefs: []
  type: TYPE_TB
- en: '| PAC [[143](#bib.bib143)] | CVPR-2019 | Present a pixel-adaptive convolution
    operation for deep joint image upsampling. |'
  prefs: []
  type: TYPE_TB
- en: '| DELTAR [[80](#bib.bib80)] | ECCV-2022 | Propose a transformer-based architecture
    with two branches, which can extract and fuse depth and RGB features efficiently.
    Besides, a cross-modal calibration is performed to align RGB and depth. |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid Model | DPDN [[126](#bib.bib126)] | BMVC-2016 | Use two sub-networks.
    one based on a fully-convolutional structure produces HR depth maps and weights.
    Another uses a non-local variational method to get final results. |'
  prefs: []
  type: TYPE_TB
- en: '| DG-CMM [[40](#bib.bib40)] | TPAMI-2019 | Propose a weighted analysis sparse
    representation (WASR) model and a neural network to learn filter parameterizations
    and non-linear functions to build flexible stage-wise operations. |'
  prefs: []
  type: TYPE_TB
- en: '| CUNet [[21](#bib.bib21)] | TPAMI-2020 | Develop a multi-modal convolutional
    sparse coding (MCSC) models to solve the general multi-modal image restoration
    and multi-modal image fusion problems. |'
  prefs: []
  type: TYPE_TB
- en: '| LMCSC [[108](#bib.bib108)] | TIP-2020 | Propose a deep unfolding network
    using guided image SR that exploits information from two modalities. |'
  prefs: []
  type: TYPE_TB
- en: '| LGR [[18](#bib.bib18)] | CVPR-2022 | Propose a novel architecture combining
    a neural network and graph-based optimization to perform GDSR. |'
  prefs: []
  type: TYPE_TB
- en: '| DCTNet [[196](#bib.bib196)] | CVPR-2022 | Utilizes discrete cosine transform
    (DCT) to tackle the optimization issue for GDSR by reconstructing the multi-channel
    HR depth features. |'
  prefs: []
  type: TYPE_TB
- en: '| DaDa [[110](#bib.bib110)] | CVPR-2023 | Suggest incorporating guided anisotropic
    diffusion into a CNN to improve depth discontinuities in depth images. Given a
    scale factor, the inference time and memory demands of the method are fixed. |'
  prefs: []
  type: TYPE_TB
- en: '| MADUNet [[200](#bib.bib200)] | IJCV-2023 | Propose a memory-augmented deep
    unfolding network where the introduction of local and global implicit priors is
    based on a maximal a posterior view. |'
  prefs: []
  type: TYPE_TB
- en: '| Auxiliary-based | CTKT [[145](#bib.bib145)] | CVPR-2021 | Propose a knowledge
    distillation method that uses the depth estimate auxiliary task during training
    for better results. |'
  prefs: []
  type: TYPE_TB
- en: '| BridgeNet [[151](#bib.bib151)] | ACM MM-2021 | Explore a paradigm where the
    association between monocular depth estimate (MDE) and depth super-resolution
    are used to advance the performance of depth map super-resolution. |'
  prefs: []
  type: TYPE_TB
- en: '| DSR-N [[165](#bib.bib165)] | PR-2020 | Propose a depth upsampling framework
    that leverages edge information as the guide to recover HR depth maps. |'
  prefs: []
  type: TYPE_TB
- en: '| PDSR [[155](#bib.bib155)] | ICCV-2019 | Attempt to measure the upsampled
    depth quality using renderings of surface normal maps. |'
  prefs: []
  type: TYPE_TB
- en: '| PDR-Net [[93](#bib.bib93)] | Neurocomputing-2022 | Proposes a progressive
    depth reconstruction network to further enhance the performance of DMSR. |'
  prefs: []
  type: TYPE_TB
- en: '| SVLRM [[24](#bib.bib24)] | TPAMI-2021 | Propose a new joint filtering method
    based on a spatially variant linear representation model where the target image
    is linearly represented by the guidance image. |'
  prefs: []
  type: TYPE_TB
- en: '| Fusion-focused | JIIF [[149](#bib.bib149)] | MM-2021 | Considers GDSR to
    be a neural implicit interpolation problem that can convert latent codes from
    continuous coordinates in LR depth and HR color images. |'
  prefs: []
  type: TYPE_TB
- en: '| MSSN [[77](#bib.bib77)] | PR-2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Present a multi-scale strategy with the proposed symmetric unit as the
    basic component of the network, which can &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; effectively deal with textureless and edge features in depth images.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| AHMF [[197](#bib.bib197)] | TIP-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a hierarchical network based on a bi-directional hierarchical
    feature collaboration module, where low- and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; high-level spatial information can work together to improve each other.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| RSAG [[186](#bib.bib186)] | AAAI-2023 | Propose a recurrent structure attention
    guidance framework that employs both multi-scale and multi-stage strategies. A
    deep contrastive network with multi-scale filters is used to separate HF and LF
    features. |'
  prefs: []
  type: TYPE_TB
- en: '| JGF [[160](#bib.bib160)] | PR-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Present a novel method that uses pyramid structure to capture multi-scale
    features, allowing HR depth maps to be &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; recovered progressively. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DVSR [[147](#bib.bib147)] | CVPR-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a multi-frame scheme to upsample LR dToF videos with corresponding
    HR RGB sequences. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SSDNet [[195](#bib.bib195)] | ICCV-2023 | Propose a spherical contrast refinement
    module to enhance depth details from RGB features. |'
  prefs: []
  type: TYPE_TB
- en: '| DSR-EI [[120](#bib.bib120)] | CVIU-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose AFFM to adaptively fuse discriminative cross-modality features.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Unsupervised | P2P [[103](#bib.bib103)] | ICCV-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; GDSR is regarded as a pixel-wise mapping from source to target and implemented
    as a multi-layer perceptron. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CMSR [[135](#bib.bib135)] | CVPR-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Present a single-pair method that can upsample depth maps even when
    the input pairs are misaligned, where &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; patches cropped from the input pairs are utilized as pseudo-label data
    to enable weakly supervised learning. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MMSR [[25](#bib.bib25)] | ECCV-2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; A cross-modality modulation strategy using adaptive filters is developed
    to transfer significant features from one &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modality to the other in a proposed mutual modulation super-resolution
    model. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DEDE [[159](#bib.bib159)] | TIP-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Design an adversarial network where the relationship between depth maps
    and RGB images are fully exploited &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to improve the depth. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SSGnet [[136](#bib.bib136)] | AAAI-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Explicit structure features of color images can be extracted using the
    suggested scene structure guidance network, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and these features—along with the corresponding LR depth maps—serve
    as the input of a base network. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CMPNet [[121](#bib.bib121)] | NN-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Develop an autoencoder-based framework with contrastive and reconstruction
    losses &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to reduce information redundancy. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: List of Guided Depth Super-Resolution methods, divided according to
    the five outlined categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Guided Depth Completion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pivotal works on depth completion [[153](#bib.bib153), [13](#bib.bib13)] only
    take depth maps as input. However, due to the sparsity of depth maps obtained
    from long-range LiDAR measurements, a substantial amount of details, such as object
    boundaries, are lost. Thus, restoring these details is difficult on densified
    depth maps, even using deep learning. Therefore, researchers started exploiting
    additional modalities capturing the same scene to guide the completion of sparse
    depth maps. In practice, color images can provide rich texture, scene structure,
    and object details, thus allowing for the extraction of sufficient cues to enhance
    depth completion. Besides, color images can also offer various auxiliary cues,
    such as semantics, monocular depth, and surface normal. Similar to GDSR, we divide
    GDC into two categories: supervised and unsupervised methods. Supervised methods
    can be further classified into dual-branch architectures, spatial propagation
    networks, auxiliary-based methods, and uncertainty guidance. These six categories
    are listed in the bottom branch of Fig. [5](#S2.F5 "Figure 5 ‣ 2.4 Taxonomy ‣
    2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep
    Learning-based Methods").'
  prefs: []
  type: TYPE_NORMAL
- en: Most existing approaches employ supervised learning for this task, which typically
    constructs image pairs from dense maps and their sparse version for training.
    With dense depth information being used as annotation, deep neural networks are
    trained to solve a regression problem, as done for GDSR.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Dual-branch Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The dual-branch architecture is common in GDC. It utilizes two encoders to
    extract features from RGB and sparse depth maps, and then the cross-modal information
    is sent to a single decoder after aggregation. In this latter, a designed information
    fusion strategy can also be incorporated. Thus, two folds must be focused on:
    effective representation learning and information fusion.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[60](#bib.bib60)] employ a dual-branch architecture based on a modified version
    of the neural architecture search network (NASNet)[[206](#bib.bib206)]. First,
    the features from RGB images and depth maps are extracted using two NASNet encoders.
    Then, the intermediate features are fused in a late fusion scheme through channel-wise
    concatenation and fed into a decoder, yielding the final dense estimate. In this
    framework, validity masks are demonstrated to fail further to improve performance
    for GDC in large neural networks. To address the issue of model performance degradation
    caused by blurry guidance in the color images and unclear structures in the depth
    maps, [[180](#bib.bib180)] exploit a repetitive design based on the hourglass
    network[[150](#bib.bib150)]. To achieve multi-scale training, [[74](#bib.bib74)]
    introduce the cascaded architecture into the hourglass network, extended by [[30](#bib.bib30)]
    with denser interaction between the two modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the two-branch framework proposed by [[154](#bib.bib154)], global and local
    information are extracted with two encoders based on ERFNet [[128](#bib.bib128)],
    respectively. Notably, the local branch uses two U-nets to reconstruct sharp edges
    and structural details. Finally, the predictions are weighted by the outputs of
    each branch. [[56](#bib.bib56)] propose a network that can perform precise and
    efficient depth completion (PENet), as depicted in Fig.[11](#S4.F11 "Figure 11
    ‣ 4.1 Dual-branch Architecture ‣ 4 Guided Depth Completion ‣ RGB Guided ToF Imaging
    System: A Survey of Deep Learning-based Methods"). It gradually enhances the accuracy
    of depth prediction in a coarse-to-fine manner. The first branch, named the color-dominant
    branch, takes a color image and a sparse depth map as input to produce a dense
    depth map that inherits sharp edges and structural details in the color image.
    The second branch, i.e., the depth dominant branch, employs a similar network,
    whose inputs are the previous dense and sparse depth map, to output another dense
    map. Finally, the two depth maps are fused using the strategy proposed by FusionNet [[154](#bib.bib154)].
    Notably, they concatenate the 3D position maps to the convolutional layer, forming
    a geometric convolutional layer, to lift the performance further. For indoor depth
    completion, [[63](#bib.bib63)] use a dual-branch structure to achieve an efficient
    solution with low power consumption.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/5e1dd05cb0cfa07b316768aea00333dd.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 11: PENet [[56](#bib.bib56)] framework belonging to dual-branch architectures.
    It consists of two branches that can respectively produce depth estimates from
    color-dominant and depth-dominant information.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a dense pseudo-depth map as a coarse reference for the final prediction,
    [[39](#bib.bib39)] can output stable and accurate dense depth maps. Different
    from the vanilla point-estimate methods with a dual-branch architecture, [[182](#bib.bib182)]
    develop a conditional prior network (CPN) to calculate a posterior probability
    over the depth of each pixel, combined with a likelihood term using sparse measurements.
    The standard convolution operation applies a kernel to the pixel grid in an image,
    which, however, cannot handle sparse depth maps with uneven distribution of depth
    values. To address this issue, [[194](#bib.bib194)] propose to leverage graph
    propagation to extract spatial contexts. Considering the variability of the graph
    structure, it applies the co-attention module[[100](#bib.bib100)] on the guided
    graph propagation for efficient multi-modal representation extraction. Combining
    traditional methods with a dual-branch deep network, [[175](#bib.bib175)] achieve
    promising results. In the most recent works, [[8](#bib.bib8)] employ two parallel
    Unet-shape networks to extract RGB and depth features at different scales. Then,
    feature fusion is performed in the decoding stage through the proposed attention-guided
    skip connection (AG-SC) module, yielding the desired results. In contrast, [[185](#bib.bib185)]
    propose a network named PointDC that leverages 2D and 3D information to conduct
    robust depth completion.
  prefs: []
  type: TYPE_NORMAL
- en: Guided image filtering. In GDC, deep-guided image filtering can be viewed as
    a type of dual-branch architecture that utilizes features from RGB images to predict
    a kernel. This kernel is often learned by a deep network and applied to depth
    for feature fusion or dense depth prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by classical guided image filtering [[152](#bib.bib152), [49](#bib.bib49)],
    [[150](#bib.bib150)] propose a guided network consisting of a GuideNet and a DepthNet
    to recover dense depth maps. Inside the GuideNet, content-dependent and spatial-variant
    convolution kernels are predicted to capture geometric structural features in
    color images consistent with real-world scenes. Furthermore, a convolution factorization
    is introduced into the method to reduce computational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: '[[89](#bib.bib89)] present a two-stage model where depth interpolation and
    refinement are performed sequentially. Specifically, they first interpolate the
    sparse depth map via the proposed differentiable kernel regression layer, proving
    more effective than hand-designed filters. Then, a residual network based on U-net
    is adopted to refine the coarse depth map.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Affinity-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the form of a generic matrix, the affinity matrix expresses the pairwise
    proximity information between a set of data points. Since the spatial propagation
    network (SPN) for learning the affinity matrix was first employed for depth completion [[95](#bib.bib95)],
    many significant efforts have been made to improve it, yielding impressive results.
    In this pioneering work, the affinity matrix, which can establish associations
    between any two pixels over a whole image, is composed of the spatial transformation
    matrix. Given a hidden representation $H^{s}$ at iteration $t$, the spatial propagation
    with affinity matrix $w$ can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{H}_{p,q}^{t+1}=(I-d_{t})\mathbf{H}_{p,q}^{t}+\sum_{p,q\in N_{p,q}}w_{p,q}^{i,j}\mathbf{H}_{i,j}^{t}$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: where $(p,q)$ and $(i,j)$ are the positions of reference and neighbor pixels,
    respectively, $N_{p,q}$ denote the neighborhood of $(p,q)$, and $d_{t}=1-\sum_{i,j\in
    N_{p,q}}w_{p,q}^{i,j}$.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/f86bf6dd18c9b6e5891ba34381be0bb3.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 12: CSPN framework [[11](#bib.bib11)] belonging to affinity-based methods.
    At each step, CSPN propagates a local region in all directions at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since spatial propagation is only performed along a certain direction at a
    time, the time cost of this method, which is run in sequence, is high. In contrast,
    [[11](#bib.bib11)] formulate the task as anisotropic diffusion filtering[[166](#bib.bib166),
    [94](#bib.bib94)], and implement it through a convolutional network named CSPN,
    depicted in Fig. [12](#S4.F12 "Figure 12 ‣ 4.2 Affinity-based Methods ‣ 4 Guided
    Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"). In detail, they first employ an Unet-based network that inputs an RGB
    image and a sparse depth map, and outputs a blurred depth map and an affinity
    matrix. Then, the propagation is carried out with recurrent convolutional operation.
    However, both approaches fix the receptive field in the propagation, which may
    yield suboptimal results for the task. To solve the problem, some methods utilizing
    flexible propagation schemes, such as CSPN++ [[10](#bib.bib10)], non-local SPN
    (NLSPN) [[113](#bib.bib113)], deformable SPN (DSPN) [[178](#bib.bib178)] and dynamic
    SPN (DySPN)[[84](#bib.bib84)], are proposed.'
  prefs: []
  type: TYPE_NORMAL
- en: To further improve the model performance for depth completion, [[10](#bib.bib10)]
    propose CSPN++, which includes two variants, i.e., context-aware CSPN (CA-CSPN)
    and resource-aware CSPN (RA-CSPN). By converting two constants in CSPN – the number
    of iterations and kernel size – into variables, CA-CSPN can adaptively assemble
    results from different steps in the propagation. However, in practice, the computational
    complexity is too high to be deployed in real applications. To speed up the algorithm,
    RA-CSPN picks the optimal kernel size and the number of iterations for each pixel
    depending on the learned hyper-parameters. With the improvements, the method can
    dynamically allocate the context and computing resources required by each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike CSPN++, NLSPN [[113](#bib.bib113)] is proposed with a two-stage network
    based on non-local neighbors instead of a fixed receptive field. In the first
    stage, a U-net is employed to produce a coarse depth, a confidence map, and non-local
    neighbors prediction with their raw affinities. Then, non-local spatial propagation
    computed by deformable convolutions[[204](#bib.bib204)] is performed iteratively.
    In this manner, irrelevant features are avoided, while only relevant non-local
    neighborhood features are emphasized. Similar to NLSPN[[113](#bib.bib113)], DSPN [[178](#bib.bib178)]
    also conducts spatial propagation with deformable convolution, which can offer
    an adaptive receptive field according to the dependencies between pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Later, motivated by dynamic convolution [[9](#bib.bib9)], [[84](#bib.bib84)]
    develop a non-linear propagation model, named dynamic spatial propagation network
    (DySPN), where the affinity matrix is decoupled into parts depending on the distance.
    For neighborhoods with different distances, the affinity is assigned different
    weights to improve CSPN. Moreover, they propose a diffusion suppression operation,
    which adaptively stops iterations on specific pixels as directed by the attention
    matrix to avoid over-smoothing issues. According to different computational budgets,
    they design three variants of adaptive affinity matrix to achieve a balance between
    performance and complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '[[98](#bib.bib98)] integrate SPN and graph neural networks into one framework,
    allowing to learn both neighboring and long-range features. To adapt to practical
    applications, [[134](#bib.bib134), [17](#bib.bib17)] propose novel approaches
    that can achieve good generalization. [[191](#bib.bib191)] adopt the non-local
    spatial propagation network for improving the depth quality after obtaining an
    initial densified depth map. As for the backbone that produces an initial depth,
    they propose a novel single-branch network, which leverages both the advantages
    of CNNs and transformers. In the network, they design a joint convolutional attention
    and transformer (JCAT) block that contains two paths – the transformer layer and
    the convolutional attention layer – and achieves more efficient performance than
    pure transformer-based methods. To learn the neighborhood affinity, [[73](#bib.bib73)]
    propose a network that leverages multi-scale and local features. [[163](#bib.bib163)]
    propose a long-short range recurrent updating (LRRU) network that leverages an
    iterative scheme and employs fewer parameters to achieve state-of-the-art performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Auxiliary-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As described in Sec. [3.3](#S3.SS3 "3.3 Auxiliary-based Methods ‣ 3 Guided
    Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), auxiliary learning employs several related tasks to improve the performance
    of the main task [[192](#bib.bib192)]. Many methods using auxiliary learning also
    emerge in GDC. [[2](#bib.bib2)] propose a multi-task framework, which allows conducting
    monocular depth estimation and sparse depth completion. This network is implemented
    as two cascaded sub-networks based on encoder-encoder architecture, with the first
    yielding a sparse depth map from a color image and the second estimating a dense
    depth map, taking the previous sparse depth as input. To obtain sharp depth boundaries
    in the final depth maps, they adopt adversarial training on both synthetic and
    real-world datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/8223cfc70cc3df5e0a359b9c49ea923e.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 13: MNASnet framework [[60](#bib.bib60)] belonging to auxiliary-based
    methods. It can perform depth completion and semantic segmentation simultaneously
    with minor changes in the last layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to monocular depth estimation, semantic segmentation also contributes
    to the performance improvement of depth completion. [[60](#bib.bib60)] perform
    multi-task learning, namely depth completion and semantic segmentation as shown
    in Fig. [13](#S4.F13 "Figure 13 ‣ 4.3 Auxiliary-based Methods ‣ 4 Guided Depth
    Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods"),
    by designing different heads at the end of a common network, e.g., NASNet [[206](#bib.bib206)].
    When depth density varies, the method can still fuse features across modalities
    effectively. Unlike the previous method, which does not utilize results from the
    auxiliary task, [[188](#bib.bib188)] use semantic maps to improve the accuracy
    of depth completion. In order to integrate semantic segmentation and depth completion
    into a single framework, they build multi-task generative adversarial networks
    in combination with a semantic-guided smoothness loss, which is inspired by[[193](#bib.bib193)].
    In addition to semantic segmentation and depth completion, edge detection, which
    serves as a bridge between the two tasks, is introduced into the simultaneous
    semantic segmentation and depth completion multi-task network (SSDNet) [[207](#bib.bib207)].
    Based on the multi-task learning of semantic segmentation and depth completion,
    [[52](#bib.bib52)] utilize boundary class labeling to adjust the effects of adjacent
    pixels. In[[81](#bib.bib81)], the authors perform multiple perception tasks jointly,
    including 2D and 3D object detection, ground estimation and depth completion,
    to obtain better representations. Although this work is aimed at object detection,
    it demonstrates that the selected tasks are complementary, so helpful cues provided
    by the other tasks can also be applied to depth completion. Motivated by [[101](#bib.bib101)],
    [[125](#bib.bib125)] design an auxiliary task to generate depth contours while
    outputting dense depth maps at the end of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Researchers leverage some further auxiliary information to enhance the precision
    of densified depth maps. Through a modified auto-encoder framework, [[190](#bib.bib190),
    [123](#bib.bib123)] propose to estimate surface normal maps from color images
    to improve the depth accuracy. In[[123](#bib.bib123)], the surface normals are
    regarded as intermediate representations since they can result in higher errors
    in distance measures. Considering the limitation of surface normals, the dense
    depth map estimated by the color branch is also considered in the generation of
    the final dense depth. Instead of generating a surface normal from a color map,
    [[88](#bib.bib88)] take the normal map produced by sparse depth maps as an intermediate
    constraint during training. [[177](#bib.bib177)] introduce a unified two-stage
    framework based on the assumption that natural scenes can be represented by piecewise
    planes. They build a diffusion module that incorporates the geometric constraints
    between depth and surface normals. This module is differentiable, allowing the
    diffusion conductance to be adjusted flexibly depending on the similarity in the
    guide. Inspired by [[154](#bib.bib154), [56](#bib.bib56)], [[111](#bib.bib111)]
    added a semantic-guided branch to highlight depth discontinuities.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using surface normals as auxiliary information, some works utilize
    a point cloud to enhance model performance. In order to cope with unpredictable
    real-world environments, [[62](#bib.bib62)] propose a point-cloud-centric method
    based on the proposed attentive bilateral convolutional layer. This 3D convolution
    layer consists of four steps, i.e., splat, convolve, attention, and slice, which
    can directly perform convolution operations on point clouds and focus on capturing
    helpful features for GDC.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, depth completion as auxiliary information can also improve the performance
    of other tasks. For instance, [[6](#bib.bib6), [172](#bib.bib172)] integrate depth
    completion networks into object detection models to achieve more accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Fusion-focused Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to GDSR, fusion strategies are also crucial for GDC. Compared with straightforward
    fusion methods, such as concatenation [[29](#bib.bib29), [205](#bib.bib205)] or
    summation[[137](#bib.bib137), [130](#bib.bib130)], researchers design various
    complicated and effective methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[[72](#bib.bib72)] propose a multi-scale network that exchanges information
    with the attention mechanism at each encoder scale. [[179](#bib.bib179)] employ
    a similar strategy to fuse cross-modality features at multiple scales. [[194](#bib.bib194)]
    conduct multi-modal information fusion via the proposed symmetric gated fusion
    strategy consisting of two paths, each aiming to modulate the current modality
    features with adaptively extracted features from the other modality. Using this
    strategy, features can interact across modalities, thus allowing the model to
    obtain accurate depth estimates. [[90](#bib.bib90)] develop a novel coarse-to-fine
    framework that emphasizes the fusion of cross-modal information. It introduces
    a channel shuffle extraction operation to blend features from different modalities.
    Next, they propose an energy-based fusion operation, which selects feature values
    with higher regional energy to fuse the features effectively. [[66](#bib.bib66)]
    propose a multi-modal deep aggregation network (MDANet), containing multiple connection
    and aggregation pathways to implement a deeper fusion of the two modalities. [[91](#bib.bib91)]
    design a multi-feature channel shuffle extraction, which utilizes the channel
    shuffle operation to fuse features from color images and depth maps in the encoding
    stage. Then, a multi-level weighted combination leveraging multi-scale features
    is introduced into the decoding process to enhance the information fusion effectiveness
    further.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/18e0dfab6b405a94e97401113e0a7a16.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 14: The framework of FusionNet [[154](#bib.bib154)] belonging to the
    category of fusion strategy. This method fuses global and local features based
    on the confidence maps in a late fusion manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Uncertainty Guidance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the works surveyed so far do not take into account the reliability of
    the sparse depth when designing algorithms. Recently, two categories of uncertainty,
    i.e., aleatoric and epistemic uncertainty, have been considered in GDC with Bayesian
    deep learning. Aleatoric or stochastic uncertainty represents inherent randomness
    in each observation of the same experiment. In the task of GDC, aleatoric uncertainty
    refers to the random noise in raw LiDAR data and the distribution of sparse pixels.
    Epistemic or systematic uncertainty is often caused by models ignoring certain
    practical factors. Hence, the models for GDC are usually focused on handling aleatoric
    uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: '[[28](#bib.bib28), [29](#bib.bib29)] introduce a novel framework with a proposed
    algebraically-constrained normalized convolution layer, where confidence can be
    computed and propagated to subsequent layers. Here, confidence is used as auxiliary
    information to improve the model performance. Furthermore, they propose a loss
    function by maximizing the final confidence to constrain the training. Later,
    [[27](#bib.bib27)] estimate the confidence of the input depth with the normalized
    convolution layer in an unsupervised manner, which can significantly improve the
    accuracy of depth prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [14](#S4.F14 "Figure 14 ‣ 4.4 Fusion-focused Methods ‣ 4 Guided
    Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), [[154](#bib.bib154)] estimate confidence maps in the global and local
    branches, respectively. Based on the learned confidence maps calculated by the
    softmax function in the last layer, the model can pay more attention to reliable
    pixels. Sharing a similar idea, [[123](#bib.bib123)] output confidence maps for
    occlusion handling and integrate depth predictions from color images and normal
    as well. [[177](#bib.bib177)] obtain confidence maps through a decoder to suppress
    noise propagation in depth maps.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[205](#bib.bib205)] provide a novel uncertainty formulation with Jeffrey’s
    prior, which can improve the robustness of the model to noise or invalid pixels.
    In the first stage, this proposed multi-scale joint prediction model outputs coarse
    depth and uncertainty maps, which are then sent to the uncertainty attention residual
    learning network for depth refinement. Through their architecture and loss function,
    outliers and uneven distribution of pixels are well handled. Additionally, [[115](#bib.bib115)]
    attempt to employ the proposed uncertainty-aware sampling algorithm to identify
    the reliability of pixels, which is beneficial for the subsequent adaptive prediction
    of the depth range for each pixel.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/4c6687c20aea14f2bb84be8db0272e6b.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 15: VIO [[169](#bib.bib169)] framework belonging to unsupervised methods.
    Combining the four losses of photometric consistency, sparse depth consistency,
    pose consistency and local smoothness enables training the network in a fully
    unsupervised manner.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paradigm | Method | Reference | Key Idea |'
  prefs: []
  type: TYPE_TB
- en: '| Dual-branch | MNASNet [[60](#bib.bib60)] | 3DV-2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Leverage fused features from dual streams to yield the final dense estimation
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; via a decoder. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| GuideNet [[150](#bib.bib150)] | TIP-2020 | Propose a hourglass network for
    depth completion. |'
  prefs: []
  type: TYPE_TB
- en: '| KernelNet  [[89](#bib.bib89)] | TIP-2021 | Present a two-stage model where
    depth interpolation and refinement are performed sequentially. |'
  prefs: []
  type: TYPE_TB
- en: '| RigNet [[180](#bib.bib180)] | ECCV-2022 | Exploit a repetitive design based
    on the hourglass network. |'
  prefs: []
  type: TYPE_TB
- en: '| MSG-CHN [[74](#bib.bib74)] | WACV-2020 | Introduce the cascaded architecture
    into the hourglass network. |'
  prefs: []
  type: TYPE_TB
- en: '| CDCNet  [[30](#bib.bib30)] | BMVC-2022 |'
  prefs: []
  type: TYPE_TB
- en: '| Penet [[56](#bib.bib56)] | ICRA-2021 | Enhance the accuracy of depth prediction
    in a coarse-to-fine manner. |'
  prefs: []
  type: TYPE_TB
- en: '| FusionNet [[154](#bib.bib154)] | MVA-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Exploit confidence maps in the global and local branches to achieve
    a guided- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; base method. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DenseLiDAR [[39](#bib.bib39)] | RAL-2021 | Use a dense pseudo-depth map as
    a coarse reference for the final prediction. |'
  prefs: []
  type: TYPE_TB
- en: '| SDDC [[175](#bib.bib175)] | Visual Computer-2023 | Propose the semantic-guided
    branch to achieve better prediction results. |'
  prefs: []
  type: TYPE_TB
- en: '| QNN [[63](#bib.bib63)] | CVPR-2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Adopt a dual-branch structure to achieve an efficient solution with
    low power &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; consumption. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ACMNet [[194](#bib.bib194)] | TIP-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Conduct multi-modal information fusion via the proposed symmetric gated
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; fusion strategy. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| PointDC [[185](#bib.bib185)] | ICCV-2023 | Propose a dual-branch network
    to extract 2d and sparse 3D feature point cloud for depth completion. |'
  prefs: []
  type: TYPE_TB
- en: '| AGGNet [[8](#bib.bib8)] | ICCV-2023 | Design a dual-branch autoencoder with
    attention mechenism to obtain dense depth maps. |'
  prefs: []
  type: TYPE_TB
- en: '| Affinity-based | SPN [[95](#bib.bib95)] | NeurIPS-2017 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Exploit the spatial propagation network to learning affinity matrix
    for depth &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; completion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CSPN [[11](#bib.bib11)] | ECCV-2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Formulate the task as anisotropic diffusion filtering, and implement
    it through a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; deep network. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CSPN++ [[10](#bib.bib10)] | AAAI-2020 | Propose a new structure including
    context-aware and resource-aware CSPNs. |'
  prefs: []
  type: TYPE_TB
- en: '| NLSPN [[113](#bib.bib113)] | ECCV-2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a two-stage network that is based on non-local neighbors instead
    of a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; fixed receptive field. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DSPN [[178](#bib.bib178)] | ICIP-2020 | Conduct spatial propagation with
    deformable convolution. |'
  prefs: []
  type: TYPE_TB
- en: '| DySPN [[84](#bib.bib84)] | AAAI-2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Develop a non-linear propagation model, where the affinity matrix is
    decoupled &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; into parts depending on the distance. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| GraphCSPN [[98](#bib.bib98)] | ECCV-2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Integrate SPN and graph neural networks to learn both neighboring and
    long- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; range features. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ssgp [[134](#bib.bib134)] | WACV-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a unified architecture to perform sparse-to-dense interpolation
    in &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; different domains. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SpAgNet [[17](#bib.bib17)] | WACV-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a depth completion method that is independent of their distribution
    and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; density. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CompletionFormer [[191](#bib.bib191)] | CVPR-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Leverage the non-local spatial propagation network to improve the depth
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; quality after obtaining initial depth with convolutions and transformers.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DenseLConv [[73](#bib.bib73)] | IROS-2022 | Propose a depth completion method
    that is independent of their distribution and density. |'
  prefs: []
  type: TYPE_TB
- en: '| LRRU [[163](#bib.bib163)] | ICCV-2023 | Propose a lightweight network to
    learn spatially-variant kernels and perform updates iteratively. |'
  prefs: []
  type: TYPE_TB
- en: '| Auxiliary-based | DCMDE [[2](#bib.bib2)] | 3DV-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a multi-task framework conducting monocular depth estimation
    and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; sparse depth completion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multitask GANs [[188](#bib.bib188)] | TNNLS-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Present a multi-task network using semantic images to improve the accuracy
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of depth completion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SSDNet  [[207](#bib.bib207)] | Sensors-2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Introduce edge detection to serve as a bridge between semantic segmentation
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and depth completion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MMF  [[81](#bib.bib81)] | CVPR-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Combine 2D and 3D object detection, ground estimation, and depth completion
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to obtain better representations. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DPS  [[52](#bib.bib52)] | RAL-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Present a method for estimating a dense depth from a sparse LIDAR point
    cloud &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and an image sequence. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SIUNet  [[125](#bib.bib125)] | WACV-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a network regarding depth contours as auxiliary information
    to obtain &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; dense depth maps. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DDC  [[190](#bib.bib190)] | CVPR-2018 | Propose to estimate surface normal
    maps from color images to improve the depth accuracy. |'
  prefs: []
  type: TYPE_TB
- en: '| Deeplidar  [[123](#bib.bib123)] | CVPR-2019 |'
  prefs: []
  type: TYPE_TB
- en: '| NNNet  [[88](#bib.bib88)] | IEEE Access-2022 |'
  prefs: []
  type: TYPE_TB
- en: '| DNC  [[177](#bib.bib177)] | ICCV-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Confidence maps are obtained through a decoder to suppress noise propagation
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in depth maps. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SemAttNet  [[111](#bib.bib111)] | IEEE Access-2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Add a semantic-guided branch for depth completion to highlight depth
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; discontinuities. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ABCD  [[62](#bib.bib62)] | RAL-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a point-cloud-centric method that is based on the proposed attentive
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; bilateral convolutional layer. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: List of Guided Depth Completion methods, divided according to the
    first three outlined categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paradigm | Method | Reference | Key Idea |'
  prefs: []
  type: TYPE_TB
- en: '| Fusion-focused | CGM  [[89](#bib.bib89)] | IEEE Access-2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Present a two-stage model where depth interpolation and refinement are
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; performed sequentially. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| RSIC  [[179](#bib.bib179)] | IEEE Access-2020 | Employ a similar strategy
    to fuse cross-modality features at multiple scales. |'
  prefs: []
  type: TYPE_TB
- en: '| FCFR-Net  [[90](#bib.bib90)] | AAAI-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Develop a novel coarse-to-fine framework that emphasizes the fusion
    of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cross-modal information. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MDANet  [[66](#bib.bib66)] | ICRA-2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a multi-modal deep aggregation block that consists of multiple
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; connection and aggregation pathways for deeper fusion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MFF-Net  [[91](#bib.bib91)] | RAL-2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a multi-feature channel shuffle extraction and a decoding process
    with &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-scale features. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Uncertainty-guided | MS-Net  [[29](#bib.bib29)] | TPAMI-2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Introduce a novel framework with a proposed algebraically-constrained
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; normalized convolution layer. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| pNCNN  [[27](#bib.bib27)] | CVPR-2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Estimate the confidence of the input depth with the normalized convolution
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; layer in an unsupervised manner. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| MJPM  [[205](#bib.bib205)] | AAAI-2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Provide Jeffrey’s prior uncertainty formulation that can increase the
    model’s &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; resistance to noise and invalid pixel input. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| FusionNet [[168](#bib.bib168)] | RAL-2021 | Leverage synthetic datasets to
    alleviate the lack of real image pairs. |'
  prefs: []
  type: TYPE_TB
- en: '| PADNet  [[115](#bib.bib115)] | ACM MM-2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Employ the proposed uncertainty-aware sampling algorithm to identify
    the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reliability of pixels, which is beneficial for prediction of the depth
    range. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Unsupervised | SfMLearner [[201](#bib.bib201)] | CVPR-2017 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Single-view depth prediction and multi-view camera pose estimation from
    two &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; independent networks. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SSDC [[105](#bib.bib105)] | ICRA-2019 | Propose learning a mapping from the
    sparse input to the dense prediction. |'
  prefs: []
  type: TYPE_TB
- en: '| lsf [[124](#bib.bib124)] | WACV-2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Propose a least squares fitting model for the self-supervised learning
    benchmark &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; network. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| VIO [[169](#bib.bib169)] | ICRA-2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Use a piecewise planar scaffolding of a scene as supervision to achieve
    self- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; supervised learning. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| KBNet [[170](#bib.bib170)] | ICCV-2021 | Exploit the proposed calibrated
    backprojection layers to improve the baselines. |'
  prefs: []
  type: TYPE_TB
- en: '| Monitored Distillation [[96](#bib.bib96)] | ECCV-2022 | Introduce knowledge
    distillation into depth completion. |'
  prefs: []
  type: TYPE_TB
- en: '| CostDCNet [[65](#bib.bib65)] | ECCV-2022 | Propose a two-branch network based
    on the cost volume-based depth estimation. |'
  prefs: []
  type: TYPE_TB
- en: '| Struct-MDC [[61](#bib.bib61)] | RAL-2022 | First perform mesh depth construction
    by leveraging point and line features. |'
  prefs: []
  type: TYPE_TB
- en: '| DDP [[182](#bib.bib182)] | CVPR-2019 | Develop a conditional prior network
    to calculate a posterior probability over the depth of each pixel. |'
  prefs: []
  type: TYPE_TB
- en: '| AGG-CVCNet [[176](#bib.bib176)] | ACM MM-2022 | Propose a novel adjacent
    geometry guided training loss. |'
  prefs: []
  type: TYPE_TB
- en: '| MSC [[189](#bib.bib189)] | Remote Sensing-2022 | Introduce multi-modal spatio-temporal
    consistency constraints to train models. |'
  prefs: []
  type: TYPE_TB
- en: '| LeoVR [[75](#bib.bib75)] | MobiSys-2022 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: List of Guided Depth Completion methods, divided according to the
    last three outlined categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Unsupervised Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practice, dense ground truth depth maps are hard to obtain due to the limitations
    of LiDAR imaging principles. Even a high-end LiDAR can only generate at most $30\%$
    valid pixels of the whole depth map[[153](#bib.bib153)]. Thus, some methods attempt
    to address the problem with self-supervised learning, among which the representative
    classes build supervisory signals using monocular sequences or synchronized stereo
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in Fig. [15](#S4.F15 "Figure 15 ‣ 4.5 Uncertainty Guidance ‣ 4
    Guided Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), [[201](#bib.bib201)] design a self-supervised framework in which only
    color and depth sequences must be provided instead of sparse/dense image pairs.
    The method produces single-view depth prediction and multi-view camera pose estimation
    from two independent networks. This principle is brought to GDC by [[105](#bib.bib105)]
    introducing three loss functions, i.e., sparse depth loss, photometric loss, and
    smoothness loss. Under this framework, the performance even outperforms some methods
    with semi-dense annotations. To increase the robustness to noise and outliers,
    [[124](#bib.bib124)] substituted the proposed least squares fitting model for
    the last convolutional layer of the self-supervised learning benchmark network[[38](#bib.bib38)].
    [[169](#bib.bib169)] propose to use a piecewise planar scaffolding of a scene
    as supervision to implement self-supervised learning. Using a similar structure,
    [[170](#bib.bib170)] leverage the proposed calibrated back-projection layers to
    improve the baselines, while [[96](#bib.bib96)] introduce knowledge distillation
    into depth completion. [[65](#bib.bib65)] propose a two-branch network inspired
    by cost volume-based depth estimation, which can obtain informative 2D and 3D
    representations from cross-modality input with the lightweight structure design.
    In [[61](#bib.bib61)], they first perform mesh depth construction by leveraging
    point and line features, which, together with the color image, are fed into a
    mesh depth refinement module. Finally, the depth prediction can be obtained through
    the calibrated backprojection network (KBNet) [[170](#bib.bib170)]. While most
    networks are trained on real datasets, [[168](#bib.bib168), [99](#bib.bib99)]
    train their models on synthetic datasets, alleviating the lack of real image pairs
    with dense labels.'
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, stereo pairs also play an important role. Among the
    self-supervised stereo methods, most use photometric consistency [[37](#bib.bib37),
    [182](#bib.bib182), [137](#bib.bib137)] to infer geometry and obtain promising
    results. In contrast, [[176](#bib.bib176)] propose a novel adjacent geometry guided
    training loss to confine depth maps of low-confident regions by high-confident
    labels. [[189](#bib.bib189), [75](#bib.bib75)] introduce multi-modal spatiotemporal
    consistency constraints to train models, which enables the model to better adapt
    to real-world environments, such as dark and low-texture objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude this section, Tabs. [4](#S4.T4 "Table 4 ‣ 4.5 Uncertainty Guidance
    ‣ 4 Guided Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep
    Learning-based Methods") and [5](#S4.T5 "Table 5 ‣ 4.5 Uncertainty Guidance ‣
    4 Guided Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods") collect the methods discussed so far, divided into six categories. For
    each method, the venue and year are reported, together with a short description
    of the key idea behind it.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Benchmark Datasets and objective functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduce existing datasets relevant to our survey and the most common loss
    functions used to train the methods we surveyed before.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section introduces public datasets for RGB guided ToF imaging; they are
    divided into low-resolution and sparse depth according to the image characteristics
    collected by ToF cameras.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Dataset | Ref. | Year | Sensor Name | Capture condition | Modalities |
    Images |'
  prefs: []
  type: TYPE_TB
- en: '| LR Depth | ToFMark |  [[33](#bib.bib33)] | 2013 | PMD Nano | Indoor | Color,
    Depth | 3 Images |'
  prefs: []
  type: TYPE_TB
- en: '| Lu |  [[102](#bib.bib102)] | 2014 | ASUS Xiton Pro | Indoor | Color, Depth
    | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| SUN RGBD |  [[140](#bib.bib140)] | 2015 | Kinect v2 | Indoor | Color, Depth
    | 2860 |'
  prefs: []
  type: TYPE_TB
- en: '| DIML |  [[12](#bib.bib12)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Kinect v2, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ZED Stereo &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Indoor, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Outdoor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Color, Depth | $>$200 Scenes |'
  prefs: []
  type: TYPE_TB
- en: '| RGBDD |  [[50](#bib.bib50)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Huawei P30 Pro, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Helios &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Indoor, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Outdoor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Color, Depth | 4811 Images |'
  prefs: []
  type: TYPE_TB
- en: '| Middlebury |  [[133](#bib.bib133)]  [[53](#bib.bib53)]  [[131](#bib.bib131)]
    | 2003 2007 2014 | Structured Light Structured Light Stereo Camera | Indoor |
    Color,Depth | 32 Images |'
  prefs: []
  type: TYPE_TB
- en: '| NYUv2 |  [[138](#bib.bib138)] | 2012 | Kinect V1 | Indoor | Color, Depth
    | 1449 Images |'
  prefs: []
  type: TYPE_TB
- en: '| MPI Sintel Depth |  [[5](#bib.bib5)] | 2012 | Blender |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Indoor, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Outdoor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Color, Depth &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Optical Flow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1064 Images |'
  prefs: []
  type: TYPE_TB
- en: '| ToF-FT3D |  [[158](#bib.bib158)] | 2022 | synthetic | – | Color, Depth |
    6250 Views |'
  prefs: []
  type: TYPE_TB
- en: '| Sparse Depth | KITTI |  [[153](#bib.bib153)] | 2017 | Velodyne, Stereo Camera
    | Outdoor | Color, Depth | 94k Frames |'
  prefs: []
  type: TYPE_TB
- en: '| DenseLivox |  [[184](#bib.bib184)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Livox LiDAR, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RealSense d435i &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Indoor, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Outdoor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Color, Depth | 6 Scenes |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI-360 |  [[82](#bib.bib82)] | 2022 | Velodyne, Stereo Camera | Outdoor
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Color, Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GPS, IMU &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 150 Scenes |'
  prefs: []
  type: TYPE_TB
- en: '| Gibson |  [[173](#bib.bib173)] | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; NaVis, Matterport Camera, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DotProduct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Outdoor | Color, Depth | 572 Scenes |'
  prefs: []
  type: TYPE_TB
- en: '| Virtual KITTI |  [[35](#bib.bib35)] | 2016 | Synthetic | Outdoor | Color,Depth
    | 50 Videos |'
  prefs: []
  type: TYPE_TB
- en: '| Leddar Pixset |  [[22](#bib.bib22)] | 2021 | Leddar Pixell LiDAR | Outdoor
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Color, Depth &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; IMU, Radar &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 29k Frames |'
  prefs: []
  type: TYPE_TB
- en: '| Waymo Perception |  [[146](#bib.bib146)] | 2020 | LiDAR | Outdoor | Color,
    Depth | 1150 Scenes |'
  prefs: []
  type: TYPE_TB
- en: '| DDAD |  [[42](#bib.bib42)] | 2020 | Luminar-H2 LiDAR | Outdoor | Color, Depth
    | 150 Scenes |'
  prefs: []
  type: TYPE_TB
- en: '| Near-Collision Set |  [[107](#bib.bib107)] | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; LiDAR (N/A), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Stereo Camera &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Indoor | Color, Depth | 13658 Sequences |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: RGB guided ToF Datasets, divided according to the depth map properties.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Low-resolution depth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we mainly introduce datasets composed of low-resolution images collected
    by RGB guided ToF cameras, which can be employed for depth super-resolution and
    depth completion through certain sampling strategies. We also introduce datasets
    that are commonly used in this field, such as Middlebury [[133](#bib.bib133),
    [53](#bib.bib53), [132](#bib.bib132), [131](#bib.bib131)], NYU v2 [[138](#bib.bib138)],
    MPI Sintel Depth [[5](#bib.bib5)] and ToF-FT3D[[158](#bib.bib158)], although being
    collected by other sensors or synthesized.
  prefs: []
  type: TYPE_NORMAL
- en: 'ToFMark [[33](#bib.bib33)]: This dataset captures 3 real-world scenes, i.e.,
    Books, Shark and Devil, by PMD Nano ToF camera.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LU [[102](#bib.bib102)]: This dataset consists of 6 RGBD images captured by
    the ASUS Xtion Pro camera, which usually serves as validation or test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SUN RGBD [[140](#bib.bib140)]: SUN RGBD dataset contains 10 335 indoor image
    pairs acquired by four different sensors. This dataset is often applied to scene
    understanding tasks. Additionally, this dataset can be used to investigate cross-sensor
    biases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DIML [[12](#bib.bib12)]: DIML is a large-scale RGBD database containing 2M
    RGBD frames. In order to obtain high-precision depth maps, they use Kinect v2
    to capture indoor scenes and ZED stereo camera to capture outdoor scenes. In outdoor
    scenes, confidence maps of disparity are also provided.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RGBDD [[50](#bib.bib50)]: RGBDD establishes the first depth map SR dataset,
    which can reflect the correspondence between real LR and HR depth maps. This dataset
    consists of 4811 images capturing various scenes, e.g., human body, stuffed dolls,
    toys and plants. Notably, LR and HR depth maps are collected by Huawei P30 Pro
    and Helios ToF camera, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Middlebury [[133](#bib.bib133), [53](#bib.bib53), [132](#bib.bib132), [131](#bib.bib131)]:
    The Middlebury dataset is a widely used dataset in the field of GDSR, and consists
    of four sub-datasets from different years. Middlebury 2003[[133](#bib.bib133)],
    Middlebury 2005 [[53](#bib.bib53)], Middlebury 2006 [[132](#bib.bib132)], and
    Middlebury 2012 [[131](#bib.bib131)] are acquired by the stereo camera under indoor
    condition. These datasets use stereo image pairs to generate depth maps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NYUv2 [[138](#bib.bib138)]: This dataset uses Microsoft Kinect v1 camera to
    capture 1449 images. Most previous works take the first 1000 RGBD images as the
    training set, and the rest of the 449 images as the test set. [[18](#bib.bib18),
    [110](#bib.bib110)] randomly selected 849 images for training, 300 for validation,
    and 300 for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MPI Sintel Depth [[5](#bib.bib5)]: This dataset originates from an animated
    short film for flow evaluation. It provides synthetic video sequences, including
    35 naturalistic scenes, for which depth maps are provided.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ToF-FT3D [[158](#bib.bib158)]: ToF-FT3D, also known as ToF-FlyingThings3D,
    is a synthetic dataset which captures 6250 different views using Blender. Similar
    to the FlyingThings3D[[109](#bib.bib109)] for optical flow estimation, it consists
    of objects that fly along randomized 3D trajectories. Besides, this dataset provides
    ToF amplitude, ToF depth and RGB images with a resolution of $640\times 480$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Sparse depth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In practical applications, sparse depth maps are usually produced by LiDAR and
    only contain about $4\%$ valid pixels[[153](#bib.bib153)]. Under some extreme
    conditions, even less than $1\%$ of the pixels are effectively measured[[43](#bib.bib43),
    [41](#bib.bib41)]. Although sparse depth maps can be obtained from dense depth
    maps with sampling strategies, here we focus on datasets collected by LiDAR.
  prefs: []
  type: TYPE_NORMAL
- en: 'KITTI [[153](#bib.bib153)]: KITTI is a dataset initially gathered by the Karlsruhe
    Institute of Technology (KIT) and Toyota Technological Institute at Chicago (TTI-C).
    As one of the most important datasets in the field of autonomous driving, it captures
    more than 93K depth maps with corresponding raw LiDAR scans and RGB images. The
    LiDAR and two color cameras used for data acquisition are Velodyne HDL-64E and
    Point Grey Flea 2 (FL2-14S3C-C), respectively. Based on the KITTI raw dataset,
    [[153](#bib.bib153)] remove noise and artifacts in the scenes and aggregates multiple
    LiDAR scans over time to obtain denser ground truth depths, making this dataset
    suitable for GDC. There are 86 000, 7 000, and 1 000 image pairs for training,
    validation, and evaluation, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Virtual KITTI [[35](#bib.bib35)]: This dataset is a photo-realistic synthetic
    video dataset that contains 50 videos with a total of 21 260 frames. Scenes in
    the dataset are generated with the Unity game engine and a real-to-virtual cloning
    method. It can be applied to a variety of tasks, such as object detection, object
    tracking, instance semantic segmentation, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'KITTI-360 [[82](#bib.bib82)]: KITTI-360 is a newly collected large-scale dataset
    with rich sensory information and full annotations. This dataset captures over
    320k images and 100k laser scans, including annotated static and dynamic 3D scene
    elements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gibson [[173](#bib.bib173)]: Gibson contains 572 full buildings with 1447 floors
    covering a total area of 211k $m^{2}$. In addition to depth maps and the corresponding
    color maps, it provides normal maps and semantic object annotations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DenseLivox [[184](#bib.bib184)]: This dataset collects the images with a Livox
    Horizon LiDAR and Intel RalSense D435i camera. It contains dense, accurate depth
    as ground truth, with up to $88.3\%$ valid pixel coverage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Leddar Pixset [[22](#bib.bib22)]: Leddar Pixset is the first full-waveform
    flash LiDAR dataset for autonomous driving. It contains 29 000 frames in 97 sequences
    of various environments, weather conditions, and periods, annotated with more
    than 1.3 million 3D bounding boxes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Waymo Perception [[146](#bib.bib146)]: This dataset is collected by five LiDAR
    sensors and five high-resolution pinhole cameras, all synchronized and calibrated.
    It captures 1150 scenes, including urban and suburban areas, each lasting 20 seconds.
    Typically, 1000 scenes are used for training and 150 for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DDAD [[42](#bib.bib42)]: DDAD is a new benchmark dataset for autonomous driving
    from Toyota Research Institute, which consists of monocular videos and accurate
    ground-truth depth. It has a long measurable distance, which can reach up to 250m.
    There are 150 scenes with 12 650 individual image pairs in the training set, 50
    scenes with 3 950 image pairs in the validation set, and 3 080 images in the test
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Near-Collision Set [[107](#bib.bib107)]: Near-Collision Set is a large-scale,
    real-world dataset for near-collision prediction, collected with a stereo camera
    and LiDAR sensor. In this dataset, color images, accurate depth, and human pose
    annotations are provided. It contains 13 658 egocentric video snippets of humans
    navigating in indoor hallways.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude, Tab. [6](#S5.T6 "Table 6 ‣ 5.1 Datasets ‣ 5 Benchmark Datasets
    and objective functions ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods") lists the main datasets we discussed so far, categorizing them according
    to the input depth map being LR or sparse, as well as reporting the year of collection,
    the sensor used, the capture environment, the sensed modalities and the number
    of images.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Objective Functions.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notably, the performance of a trained model is directly related to the choice
    of loss functions, as they estimate the corrections to be applied to weights in
    the network during training. In this section, we discuss general loss functions
    for RGB guided ToF imaging, along with loss functions that are frequently employed
    for the two sub-tasks of GDSR and GDC, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 General losses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In RGB guided ToF imaging, the most frequently utilized losses are mean absolute
    error (MAE), mean square error (MSE), and root mean square error (RMSE). MAE is
    used in many works [[60](#bib.bib60), [137](#bib.bib137), [151](#bib.bib151),
    [149](#bib.bib149)], also referred to as $l_{1}$ loss
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{MAE}=\frac{1}{N}\sum_{p\in N}\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel_{1}$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: where $\parallel\cdot\parallel_{1}$ means the $l_{1}$ norm and $N$ denotes the
    total number of valid pixels in depth maps. Some methods[[106](#bib.bib106), [196](#bib.bib196)]
    adopted MSE ($l_{2}$ loss) which is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{MSE}=\frac{1}{N}\sum_{p\in N}\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel_{2}$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\parallel\cdot\parallel_{2}$ represents the $l_{2}$ norm. Another common
    quantity is RMSE, i.e., the square root of MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{RMSE}=\sqrt{\mathrm{MSE}}$ |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: 'As a perceptual metric for image quality assessment, the Structural Similarity
    Index (SSIM) is based on the visible structures and is defined as shown in Eq.
    [19](#S2.E19 "In 2.3 Evaluation metrics ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Losses for GDSR.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To achieve more robust training, a few approaches [[92](#bib.bib92)] use the
    Charbonnier loss[[7](#bib.bib7)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{cha}=\sqrt{\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel^{2}+\epsilon^{2}}$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\epsilon$ denotes a constant that ensures the penalty is non-zero. Sometimes,
    Peak Signal-to-Noise Ratio (PNSR) is also utilized for GDSR. PSNR can be formulated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{PSNR}=10log_{10}(\frac{Z_{max}^{2}}{\mathrm{MSE}})$ |  | (29)
    |'
  prefs: []
  type: TYPE_TB
- en: where $Z_{max}$ is the maximum depth value. The smaller the pixel value difference
    between the two depth maps, the higher the PSNR.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Losses for GDC.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Depth loss. During training, since MAE treats all errors equally and MSE emphasizes
    the outliers, Huber loss combines the advantages of both:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{huber}=\begin{cases}\frac{1}{N}\sum_{p\in N}\frac{1}{2}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2},&amp;&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;\leq
    1\\ \frac{1}{N}\sum_{p\in N}(&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;-\frac{1}{2}),&amp;&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;>1\end{cases}$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: 'where $|\cdot|$ is the operator for absolute value. In[[154](#bib.bib154)],
    another loss named Focal-MSE, inspired by [[83](#bib.bib83)], is proposed and
    proved to be better than MSE, which is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{focal}=\frac{1}{N}\sum_{p\in N}(1+\frac{\mathrm{epoch}}{20}&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;)\cdot(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}$
    |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: Uncertainty-driven loss. Considering the uneven distribution in the captured
    depth maps from LiDAR, the uncertainty-driven loss is introduced[[177](#bib.bib177),
    [27](#bib.bib27), [205](#bib.bib205)] to focus on more reliable pixels. In their
    works, GDC is defined as maximizing the posterior probability. The final optimized
    loss with Jeffrey’s prior[[34](#bib.bib34)] can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{ud}=\frac{1}{N}\sum(e^{-s_{n}}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}+2s_{p})$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: where $s_{n}$ stands for prediction uncertainty at the $p^{th}$ pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial loss. For method adopting a generative adversarial learning strategy[[67](#bib.bib67),
    [157](#bib.bib157), [112](#bib.bib112)] to perform depth completion, the adversarial
    loss is needed to discriminate between real and fake images, which can be written
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{adv}\!=\!\min_{G}\max_{D}\mathbb{E}[\log D(Z_{hq})]\!+\!\mathbb{E}[\log(1-D(G(Z_{lq},I)))]$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: where $G(\cdot)$ and $D(\cdot)$ denote the generator and discriminator, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normal constraint. To further improve depth accuracy, surface normal constraints
    have been exploited in several works[[190](#bib.bib190), [71](#bib.bib71), [177](#bib.bib177)].
    [[190](#bib.bib190), [71](#bib.bib71)] compute the depth-normal consistency loss
    between the predicted normal $R(p)$ at pixel $p$ and the estimated normal $v(p,q)$
    from the depth map, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{n1}=\sum_{p,q\in N}\parallel<v(p,q),R(p)>\parallel^{2}$ |  | (34)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $q$ is a neighbour of pixel $p$, and $<\!\!\cdot\!\!>$ denotes the inner
    product. To enable models to be trained in an end-to-end fashion, [[177](#bib.bib177)]
    propose a negative cosine loss inspired by[[26](#bib.bib26)], which is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{n2}=-\frac{1}{N}\sum_{p\in N}R(p)\cdot\hat{R}(q)$ |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{R}(\cdot)$ is the computed normal vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Smoothness term. Based on the assumption of local smoothness, the smoothness
    term is introduced in many works[[105](#bib.bib105), [137](#bib.bib137)] to avoid
    local optimal solutions, penalizing gradients along $x-$ and $y-$ directions.
    In order to maintain the discontinuities in the final depth maps, some methods[[169](#bib.bib169),
    [170](#bib.bib170), [14](#bib.bib14), [142](#bib.bib142), [130](#bib.bib130),
    [168](#bib.bib168)] weight the gradients maps according to the local properties
    in RGB images:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{sm}=\frac{1}{N}\sum_{p\in N}\lambda_{x}(p)&#124;\partial_{x}\hat{Z}_{hq}^{p}&#124;+\lambda_{y}(p)&#124;\partial_{y}\hat{Z}_{hq}^{p}&#124;$
    |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{x}(p)=e^{-|\partial_{x}I^{p}|}$ and $\lambda_{y}(p)=e^{-|\partial_{y}I^{p}|}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Photometric consistency. In unsupervised methods for GDC, photometric consistency
    based on the assumption of local smoothness is usually used to refine depth estimation.
    Given a reference image $I_{t}$ and its neighboring frame $I_{\tau}$ along the
    temporal dimension where $\tau\in T=\{t-1,t+1\}$, the goal is to estimate the
    difference between $I_{t}$ and its reconstructed version from $I_{\tau}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{I}_{\tau}(x,\hat{Z}_{hq})=I_{\tau}(\pi g_{\tau t}K^{-1}\overline{x}\hat{Z}_{hq}(x))$
    |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: where $\overline{x}=[x^{T}1]^{T}$ are the homogeneous coordinates $x\subset\Omega$,
    $\pi$ denotes the perspective projection, $g_{\tau t}$ is the relative pose of
    the camera from time $t$ to $\tau$, $K$ represents the camera intrinsic matrix,
    and $\hat{Z}_{hq}(x)$ denotes the predicted depth of $x$. The relative pose can
    be either known or estimated by a network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The photometric consistency is constructed by the combination of $L1$ loss
    and SSIM on the photometric reprojection as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle l_{ph}=$ | $\displaystyle\frac{1}{&#124;\Omega&#124;}\sum_{\tau\in
    T}\sum_{x\in\Omega}\omega_{co}&#124;I_{t}(x)-\hat{I}_{\tau}(x)&#124;+$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\omega_{st}(1-\mathrm{SSIM}(I_{t}(x),\hat{I}_{\tau}(x)))$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: where $\omega_{co}$ and $\omega_{st}$ are weights to balance the two terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pose consistency. Given a pair of images ${I_{t},I_{\tau}}$ as input, a pose
    network predicts the relative pose $g_{\tau t}\in SE(3)$. When the input of the
    image sequence is reversed, we expect to obtain $g_{t}\tau$. The pose consistency
    loss is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $l_{pc}=\parallel\log(g_{\tau t}\cdot g_{t\tau})\parallel^{2}_{2}$ |  |
    (39) |'
  prefs: []
  type: TYPE_TB
- en: where $\log:SE(3)\to se(3)$ is the logarithmic map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Others. Finally, we review other commonly used loss functions in Tab.[7](#S5.T7
    "Table 7 ‣ 5.2.3 Losses for GDC. ‣ 5.2 Objective Functions. ‣ 5 Benchmark Datasets
    and objective functions ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), including depth loss, structural loss, and others.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Types | Notation | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Depth loss | $l_{berhu}$ | Berhu loss is a reversion of Huber loss $l_{huber}$.
    |'
  prefs: []
  type: TYPE_TB
- en: '| $l_{ce}$ | Cross-entropy is to measure the validaty of pixels. |'
  prefs: []
  type: TYPE_TB
- en: '| $l_{cyc}$ | Two $l_{1}$ losses for cycle consistency. |'
  prefs: []
  type: TYPE_TB
- en: '| Structural loss | $l_{grad}$ | Gradient between the prediction and GT without
    weights. |'
  prefs: []
  type: TYPE_TB
- en: '| Others | $l_{cpn}$ | RMSE between the prediction and its reconstruction from
    CPN. |'
  prefs: []
  type: TYPE_TB
- en: '| $l_{cos}$ | Cosine similarity measures the similarity between the prediction
    and GT. |'
  prefs: []
  type: TYPE_TB
- en: '| $l_{tp}$ | MAE between the initial depth and the prediction. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Main loss terms deployed for GDSR and GDC.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we compare the performance of the state-of-the-art GDSR and
    GDC approaches on widely used benchmarks. We first present the quantitative results
    for the two categories of RGB guided ToF imaging. Then, the pros and cons of the
    methods are analyzed. Notably, we focus more on the most recent methods for RGB
    guided ToF imaging.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Comparison of GDSR methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We select representative GDSR methods for the quantitative comparison on Middlebury,
    NYUv2, RGBDD, and DIML datasets with scaling factors of $4\times,8\times$, and
    $16\times$. For all the datasets, we follow the recent literature [[149](#bib.bib149),
    [196](#bib.bib196), [186](#bib.bib186), [198](#bib.bib198)] to assess the depth
    quality according to the RMSE metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tab. [8](#S6.T8 "Table 8 ‣ 6.1 Comparison of GDSR methods ‣ 6 Evaluation ‣
    RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") shows
    the comparison of the state-of-the-art GDSR methods. In this experiment, bicubic
    interpolation is set as the baseline method. From this table, we can see that
    although the early deep learning methods[[58](#bib.bib58), [78](#bib.bib78)] are
    designed with simple structures, they show more powerful performance than traditional
    methods. Using an encoder-decoder architecture, DJFR[[79](#bib.bib79)], DepthSR [[44](#bib.bib44)]
    and PAC[[143](#bib.bib143)] design two sub-networks that extract RGB and depth
    features respectively, and attain better performance compared to models built
    by several conventional CNN layers. In order to improve the representation capability
    of neural networks, PMBANet[[183](#bib.bib183)] design a complex structure and
    further boost the GDSR performance. Using a lightweight structure, FDSR[[50](#bib.bib50)]
    can still produce accurate results. As an alternative solution, the kernel prediction
    network (KPN) employed by DKN[[68](#bib.bib68)] and FDKN also achieves promising
    results. Further, [[198](#bib.bib198)] employ the KPN to handle inconsistent structures
    between RGB and depth, which improves generalization to real scenes. CUNet[[21](#bib.bib21)]
    develops an optimization-based method and yields results comparable to KPNs. Inspired
    by these optimization-based methods, DCTNet[[196](#bib.bib196)] and LGR[[18](#bib.bib18)]
    attain better performance. LGR[[18](#bib.bib18)], in particular, achieves the
    state-of-the-art on Middlebury and DIML datasets. Unlike most methods that can
    only perform upsampling at fixed integer scales, GeoDSR [[161](#bib.bib161)] learns
    a continuous representation and achieves strong performance in arbitrary scale
    depth super-resolution on the Middlebury dataset. With an effective multimodal
    feature fusion strategy, DSR-EI[[120](#bib.bib120)] attains the best results.
    However, the disadvantage is that it brings a large computational burden.'
  prefs: []
  type: TYPE_NORMAL
- en: From another perspective, most state-of-the-art methods adopt multi-scale or
    coarse-to-fine strategies, such as SFG [[187](#bib.bib187)], JGF [[160](#bib.bib160)],
    and RSAG [[186](#bib.bib186)]. On the one hand, these strategies have achieved
    impressive results in other tasks, showing their effectiveness in learning feature
    representations. On the other hand, some methods, such as DCTNet [[196](#bib.bib196)],
    LGR [[18](#bib.bib18)], DADA [[110](#bib.bib110)] and SSDNet[[195](#bib.bib195)],
    combine classic optimization algorithms with modern CNNs and demonstrate strong
    performance. Nonetheless, these models cannot be directly applied to many real
    scenarios, especially on mobile devices. Hence, efficient and effective approaches
    need to be further investigated.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Middlebury | NYUv2 | RGBDD | DIML |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Methods | $4\times$ | $8\times$ | $16\times$ | $4\times$ | $8\times$ | $16\times$
    | $4\times$ | $8\times$ | $16\times$ | $4\times$ | $8\times$ | $16\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bicubic | 2.28 | 3.96 | 6.37 | 4.28 | 7.14 | 11.58 | 2.75 | 4.47 | 6.98 |
    1.92 | 3.20 | 5.14 |'
  prefs: []
  type: TYPE_TB
- en: '| GF [[48](#bib.bib48)] | 2.49 | 3.98 | 6.08 | 5.05 | 6.97 | 11.1 | 2.72 |
    4.02 | 6.68 | 2.72 | 3.40 | 5.13 |'
  prefs: []
  type: TYPE_TB
- en: '| DMSG [[58](#bib.bib58)] | 2.11 | 3.74 | 6.03 | 3.02 | 5.38 | 9.17 | 1.80
    | 3.04 | 5.10 | 1.39 | 2.34 | 4.02 |'
  prefs: []
  type: TYPE_TB
- en: '| DJF [[78](#bib.bib78)] | 1.68 | 3.24 | 5.62 | 2.80 | 5.33 | 9.46 | 1.72 |
    2.96 | 5.26 | 1.39 | 2.49 | 4.27 |'
  prefs: []
  type: TYPE_TB
- en: '| DJFR [[79](#bib.bib79)] | 1.32 | 3.19 | 5.57 | 2.38 | 4.94 | 9.18 | 1.50
    | 2.72 | 5.05 | 1.27 | 2.34 | 4.13 |'
  prefs: []
  type: TYPE_TB
- en: '| DepthSR [[44](#bib.bib44)] | 2.08 | 3.26 | 5.78 | 3.49 | 5.70 | 9.76 | 1.82
    | 2.85 | 4.60 | 1.40 | 2.23 | 3.75 |'
  prefs: []
  type: TYPE_TB
- en: '| PAC [[143](#bib.bib143)] | 1.32 | 2.62 | 4.58 | 1.89 | 3.33 | 6.78 | 1.25
    | 1.98 | 3.49 | 1.27 | 2.03 | 3.45 |'
  prefs: []
  type: TYPE_TB
- en: '| CUNet [[21](#bib.bib21)] | 1.10 | 2.17 | 4.33 | 1.92 | 3.70 | 6.78 | 1.18
    | 1.95 | 3.45 | 1.18 | 1.88 | 3.25 |'
  prefs: []
  type: TYPE_TB
- en: '| DKN [[68](#bib.bib68)] | 1.23 | 2.12 | 4.24 | 1.62 | 3.26 | 6.51 | 1.30 |
    1.96 | 3.42 | 1.27 | 1.86 | 3.22 |'
  prefs: []
  type: TYPE_TB
- en: '| FDKN [[68](#bib.bib68)] | 1.08 | 2.17 | 4.50 | 1.86 | 3.58 | 6.96 | 1.18
    | 1.91 | 3.41 | 1.13 | 1.84 | 3.29 |'
  prefs: []
  type: TYPE_TB
- en: '| PMBANet [[183](#bib.bib183)] | 1.11 | 2.18 | 3.25 | 1.06 | 2.28 | 4.98 |
    1.21 | 1.90 | 3.33 | 1.10 | 1.72 | 3.11 |'
  prefs: []
  type: TYPE_TB
- en: '| FDSR [[50](#bib.bib50)] | 1.13 | 2.08 | 4.39 | 1.61 | 3.18 | 5.86 | 1.16
    | 1.82 | 3.06 | 1.10 | 1.71 | 2.87 |'
  prefs: []
  type: TYPE_TB
- en: '| JIIF [[149](#bib.bib149)] | 1.09 | 1.82 | 3.31 | 1.37 | 2.76 | 5.27 | 1.15
    | 1.77 | 2.79 | 1.17 | 1.79 | 2.86 |'
  prefs: []
  type: TYPE_TB
- en: '| SVLRM [[24](#bib.bib24)] | 1.11 | 2.13 | 4.34 | 1.51 | 3.21 | 6.98 | 1.22
    | 1.88 | 3.55 | 1.19 | 1.93 | 3.49 |'
  prefs: []
  type: TYPE_TB
- en: '| AHMF [[197](#bib.bib197)] | 1.07 | 1.63 | 3.14 | 1.40 | 2.89 | 5.64 | 1.10
    | 1.73 | 3.04 | 1.10 | 1.70 | 2.83 |'
  prefs: []
  type: TYPE_TB
- en: '| DCTNet [[196](#bib.bib196)] | 1.10 | 2.05 | 4.19 | 1.59 | 3.16 | 5.84 | 1.08
    | 1.74 | 3.05 | 1.07 | 1.74 | 3.09 |'
  prefs: []
  type: TYPE_TB
- en: '| LGR [[18](#bib.bib18)] | 1.11 | 2.12 | 4.43 | 1.79 | 3.17 | 6.02 | 1.30 |
    1.83 | 3.12 | 1.25 | 1.79 | 3.03 |'
  prefs: []
  type: TYPE_TB
- en: '| RSAG [[186](#bib.bib186)] | – | – | – | 1.23 | 2.51 | 5.27 | 1.14 | 1.75
    | 2.96 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| SFG [[187](#bib.bib187)] | – | – | – | 1.45 | 2.84 | 5.56 | – | – | – | –
    | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| DAGF [[198](#bib.bib198)] | 1.15 | 1.80 | 3.70 | 1.36 | 2.87 | 6.06 | 1.17
    | 1.75 | 3.10 | 1.15 | 1.76 | 3.16 |'
  prefs: []
  type: TYPE_TB
- en: '| JGF [[160](#bib.bib160)] | – | – | – | 1.57 | 2.48 | 4.11 | 1.25 | 1.85 |
    3.03 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| DADA [[110](#bib.bib110)] | 1.02 | 1.72 | 3.16 | 1.55 | 2.88 | 5.34 | 1.12
    | 1.70 | 2.89 | 1.06 | 1.62 | 2.64 |'
  prefs: []
  type: TYPE_TB
- en: '| GeoDSR [[161](#bib.bib161)] | 1.04 | 1.68 | 3.10 | 1.42 | 2.62 | 4.86 | 1.10
    | 1.69 | 2.84 | 1.07 | 1.62 | 2.70 |'
  prefs: []
  type: TYPE_TB
- en: '| SSDNet [[195](#bib.bib195)] | 1.02 | 1.91 | 4.02 | 1.60 | 3.14 | 5.86 | 1.04
    | 1.72 | 2.92 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| DSR-EI [[120](#bib.bib120)] | 0.97 | 1.53 | 2.32 | 1.21 | 2.46 | 4.95 | 0.91
    | 1.37 | 2.10 | 0.69 | 1.19 | 1.96 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: GDSR – Results on Middlebury, NYUv2, RGBDD and DIML datasets. The
    lower the RMSE, the better. Best results in bold, second-best are underlined.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Comparison of GDC methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section gives the quantitative results achieved by several representative
    methods on the two benchmark datasets, i.e., KITTI driving scenes and NYUv2 indoor
    scenes datasets, as shown in Table [9](#S6.T9 "Table 9 ‣ 6.2 Comparison of GDC
    methods ‣ 6 Evaluation ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"). For the KITTI dataset, we use RMSE, MAE, iRMSE, and iMAE metrics to
    evaluate model performance. The results on NYUv2 are measured using RMSE and REL
    metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: From the table, we can see that unsupervised models have been making continuous
    progress. Even so, their performance still lags significantly behind their supervised
    counterparts. By using uncertainty as auxiliary information for depth, MS-Net
     [[29](#bib.bib29)], pNCNN  [[27](#bib.bib27)], MJPM  [[205](#bib.bib205)] and
    PADNet[[115](#bib.bib115)] can filter out noisy input and invalid pixels, thus
    achieving better performance than versions without using uncertainty. As another
    auxiliary-based method, multi-task learning has been adopted by various methods
    to improve generalization performance. The results produced by Multitask GANs[[188](#bib.bib188)]
    and SIUNet[[125](#bib.bib125)] demonstrate the effectiveness of this kind of approach.
    Networks using dual-branch structure[[150](#bib.bib150), [89](#bib.bib89), [194](#bib.bib194),
    [39](#bib.bib39), [8](#bib.bib8), [185](#bib.bib185)] also attain good performance.
    These methods fully leverage multi-scale features to recover the details of depth
    maps progressively. Based on the framework, RigNet[[180](#bib.bib180)] obtains
    the best results on NYUv2.
  prefs: []
  type: TYPE_NORMAL
- en: We also analyze the performance of the affinity-based methods. The affinity
    matrix produced by these methods, such as NLSPN [[113](#bib.bib113)] and GraphCSPN[[98](#bib.bib98)],
    are shown to improve depth accuracy through spatial propagation consistently.
    In addition to spatial propagation, DySPN [[84](#bib.bib84)] employs the attention
    mechanism to achieve excellent results. Furthermore, CompletionFormer[[191](#bib.bib191)]
    combines CNN with a recently popular and dominant technique, Vision Transformer,
    to achieve better performance in depth completion. In the most recent work, LRRU[[163](#bib.bib163)]
    outperform other state-of-the-art approaches by learning spatially-variant kernels
    and updating iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Loss Function | Learning | KITTI Dataset | NYUv2 Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RMSE | MAE | iRMSE | iMAE | RMSE | REL |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DDP [[182](#bib.bib182)] | $l_{1}+l_{cpn}+l_{SSIM}$ | Un | 1263.19 | 343.46
    | 3.58 | 1.32 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| VIO [[169](#bib.bib169)] | $l_{ph}+l_{1}+l_{pc}+l_{sm}$ | Un | 1169.97 |
    299.41 | 3.56 | 1.20 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| lsf [[124](#bib.bib124)] | $l_{1}+l_{SSIM}$ | Un | 885.00 | 225.20 | 3.40
    | – | 0.134 | – |'
  prefs: []
  type: TYPE_TB
- en: '| KBNet [[170](#bib.bib170)] | $l_{ph}+l_{1}+l_{sm}$ | Un | 1069.47 | 256.76
    | 2.95 | 1.02 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| CSPN [[11](#bib.bib11)] | $l_{2}$ | Su | 1019.64 | 279.46 | 2.93 | 1.15 |
    0.117 | 0.016 |'
  prefs: []
  type: TYPE_TB
- en: '| MS-Net  [[29](#bib.bib29)] | $l_{1}$ | Su | 859.22 | 207.77 | 2.52 | 0.92
    | 0.117 | 0.016 |'
  prefs: []
  type: TYPE_TB
- en: '| Deeplidar  [[123](#bib.bib123)] | $l_{2}+l_{n2}$ | Su | 758.38 | 226.50 |
    2.56 | 1.15 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| pNCNN  [[27](#bib.bib27)] | $l_{2}$ | Su | 988.57 | 228.53 | 2.71 | 1.00
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| NLSPN [[113](#bib.bib113)] | $l_{1}$ or $l_{2}$ | Su | 741.68 | 199.59 |
    1.99 | 0.84 | 0.092 | 0.012 |'
  prefs: []
  type: TYPE_TB
- en: '| GuideNet [[150](#bib.bib150)] | $l_{2}$ | Su | 736.24 | 218.83 | 2.25 | 0.99
    | 0.101 | 0.015 |'
  prefs: []
  type: TYPE_TB
- en: '| KernelNet  [[89](#bib.bib89)] | $l_{1}+l_{ce}+l_{grad}$ | Su | 785.06 | 218.60
    | 2.11 | 0.92 | 0.111 | 0.015 |'
  prefs: []
  type: TYPE_TB
- en: '| ACMNet [[194](#bib.bib194)] | $l_{2}+l_{sm}$ | Su | 744.91 | 206.09 | 2.08
    | 0.90 | 0.105 | 0.015 |'
  prefs: []
  type: TYPE_TB
- en: '| DenseLiDAR [[39](#bib.bib39)] | $l_{2}+l_{grad}+l_{SSIM}$ | Su | 755.41 |
    214.13 | 2.25 | 0.96 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| RigNet [[180](#bib.bib180)] | $l_{2}$ | Su | 712.66 | 203.35 | 2.08 | 0.90
    | 0.090 | 0.013 |'
  prefs: []
  type: TYPE_TB
- en: '| Ssgp [[134](#bib.bib134)] | $l_{2}$ | Su | 833.00 | 204.00 | – | – | – |
    – |'
  prefs: []
  type: TYPE_TB
- en: '| DenseLConv [[73](#bib.bib73)] | $l_{2}$ | Su | 729.88 | 210.06 | 2.10 | 0.93
    | 0.099 | 0.015 |'
  prefs: []
  type: TYPE_TB
- en: '| GraphCSPN [[98](#bib.bib98)] | $l_{1}$ | Su | 738.41 | 199.31 | 1.96 | 0.84
    | 0.090 | 0.012 |'
  prefs: []
  type: TYPE_TB
- en: '| SpAgNet [[17](#bib.bib17)] | $l_{1}$ | Su | 844.79 | 218.39 | - | - | 0.114
    | 0.015 |'
  prefs: []
  type: TYPE_TB
- en: '| Multitask GANs [[188](#bib.bib188)] | $l_{adv}+l_{2}+l_{cyc}$ | Su | 746.96
    | 267.71 | 2.24 | 1.10 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SIUNet  [[125](#bib.bib125)] | $l_{1}$ | Su | 1026.61 | 227.28 | 2.73 | 0.96
    | 0.138 | 0.015 |'
  prefs: []
  type: TYPE_TB
- en: '| FCFR-Net  [[90](#bib.bib90)] | $l_{1}+l_{2}$ | Su | 735.81 | 217.15 | 2.20
    | 0.98 | 0.106 | 0.015 |'
  prefs: []
  type: TYPE_TB
- en: '| MFF-Net  [[91](#bib.bib91)] | $l_{1}+l_{2}$ | Su | 719.85 | 208.11 | 2.21
    | 0.94 | 0.100 | 0.015 |'
  prefs: []
  type: TYPE_TB
- en: '| MJPM  [[205](#bib.bib205)] | $l_{1}+l_{2}$ | Su | 795.43 | 190.88 | 1.98
    | 0.83 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PADNet  [[115](#bib.bib115)] | $l_{1}+l_{focal}$ | Su | 746.19 | 197.99 |
    1.96 | 0.85 | 0.094 | 0.012 |'
  prefs: []
  type: TYPE_TB
- en: '| DySPN [[84](#bib.bib84)] | $l_{1}+l_{2}$ | Su | 709.12 | 192.71 | 1.88 |
    0.82 | 0.090 | 0.012 |'
  prefs: []
  type: TYPE_TB
- en: '| CompletionFormer [[191](#bib.bib191)] | $l_{1}+l_{2}$ | Su | 708.87 | 203.45
    | 2.01 | 0.84 | 0.090 | 0.012 |'
  prefs: []
  type: TYPE_TB
- en: '| AGGNet [[8](#bib.bib8)] | $l_{2}+l_{huber}$ | Su | – | – | – | – | 0.092
    | 0.014 |'
  prefs: []
  type: TYPE_TB
- en: '| PointDC [[185](#bib.bib185)] | $l_{1}+l+2+l_{grad}$ | Su | 736.07 | 201.87
    | 1.97 | 0.87 | 0.089 | 0.012 |'
  prefs: []
  type: TYPE_TB
- en: '| LRRU [[163](#bib.bib163)] |  | Su | 696.51 | 189.96 | 1.87 | 0.81 | 0.091
    | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: GDC – Results on KITTI and NYU-v2 datasets. Un and Su indicate unsupervised
    and supervised models. The lower the RMSE, the better. Best results in bold, second-best
    are underlined.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion and future trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although some reviewed methods have achieved promising results on RGB guided
    ToF imaging, some issues hinder their practical application. Here, we mainly describe
    the challenges and future trends from the following four aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Unsupervised methods with lightweight structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described before, most methods perform GDSR and GDC after training with a
    supervised paradigm. However, labeled data is usually difficult to collect. In
    addition, the performance of mass-produced sensors generally fluctuates to a certain
    extent, and it is unrealistic to fine-tune the model on data collected from any
    existing sensor. Therefore, unsupervised learning methods need to be developed
    without ground truth depth. In particular, state-of-the-art unsupervised learning
    methods cannot achieve the performance of supervised learning methods, so there
    is still much room for improvement in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, these methods often require many parameters and much computation
    to achieve better results, which makes the models unable to run on many devices
    at high speed, especially widely used mobile devices. While transferring the information
    to the cloud for processing is one solution to this problem, it raises thorny
    privacy issues. Thus, efficiently recovering high-quality depth is necessary and
    remains an open challenge. One recent example is SeaFormer [[156](#bib.bib156)],
    which proposes a versatile mobile-friendly backbone based on the vision transformer.
    Developing a simple yet efficient unsupervised model for RGB guided ToF imaging
    is challenging but highly desirable.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Cross-domain generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another issue concerns the ability of networks to generalize to new domains.
    This topic has been mostly overlooked in the literature despite its relevance
    for the widespread diffusion of depth estimation in practical applications. Purposely,
    [[3](#bib.bib3)] proposed a method to improve cross-domain generalization for
    depth completion by exploiting the virtual projection pattern paradigm proposed
    in [[4](#bib.bib4)] and casting depth completion as a correspondence problem using
    a virtual stereo setup. Processing properly hallucinated fictitious stereo pairs
    with a robust stereo network that possibly exploits the RGB image context, such
    as RAFT [[85](#bib.bib85)], leads to more robust cross-domain generalization than
    state-of-the-art depth completion networks.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 ToF with varifocal lens color camera
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RGB cameras in many devices, such as mobile phones and drones, have variable
    focal lengths, while ToF cameras usually employ fixed focal length lenses. Different
    focal lengths correspond to different depths of field and angles of view. When
    a ToF camera captures a distant object, boundaries are usually blurred. If the
    color image autofocuses on the object, it can provide rich structural details
    and transfer to the depth map. Nonetheless, it also brings a problem that the
    scales of the images from the two modalities may differ. In this case, the RGB
    image and the depth map cannot be directly aligned by the registration technique,
    i.e., a new calibration is required. Hence, further investigation needs to be
    conducted on how to align two scale-inconsistent images, i.e., the RGB image and
    the depth map, and simultaneously transfer the scene geometry in the RGB image
    to the depth map.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Spot-ToF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main disadvantage of ToF cameras using floodlight projection, currently
    the most commonly used technique in mobile devices, is their close operating distance.
    The scheme proposed by [[1](#bib.bib1)] can significantly extend the measurable
    distance of the ToF camera, but the long acquisition time of each image limits
    the practical application. On the other hand, direct ToF (d-ToF) can capture distant
    objects, whereas the resolution of the sensors is much lower than that of indirect
    ToF (i-ToF) with tens of thousands of pixels, e.g., $8\times 8$ or $24\times 24$.
    LiDAR also faces the same problem, and its manufacturing cost is too expensive,
    making it currently unable to be used as a consumer-grade camera. [[36](#bib.bib36)]
    provide an alternative to alleviate the issue. Specifically, they replaced the
    optical diffuser in the ToF camera projector with a diffraction optical element
    (DOE), which is used to evenly distribute the incident light into thousands of
    laser beams. In this way, each laser beam can have a measurable distance greater
    than that of floodlight illumination. However, this approach faces two challenges.
    First, a laser beam forms a spot in the sensor, corresponding to several pixels.
    The uneven intensity distribution of the spot may lead to inconsistencies in the
    measurement, so it is necessary to develop new depth correction methods to obtain
    an accurate depth map. Second, the captured images are sparse (similar to LiDAR
    but at a lower cost), so effective and efficient algorithms need to be developed
    to estimate dense depth under this setting.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Under-display RGBD camera
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In recent years, in pursuit of a better visual experience, full-screen devices
    have attracted the attention of industry and academia. This fact also brings a
    major challenge: images captured by under-display cameras are severely degraded,
    requiring effective strategies for restoration. Most of the existing work is to
    study the image restoration of under-display RGB cameras (UDC), while few works
    focus on under-display ToF (UD-ToF) depth restoration.'
  prefs: []
  type: TYPE_NORMAL
- en: Under-display camera. Recently, smartphones with full-screen displays, eliminating
    the need for bezels, have become a new product trend and motivated manufacturers
    to design a brand-new imaging system, the Under-Display Camera. Placing a display
    in front of the camera lens can increase the screen-to-body ratio for a better
    user experience. However, the broad commercial manufacturing of UDC is prevented
    by poor imaging quality due to the inevitable degradations, such as blurs, noise,
    diffraction artifacts, color shifts, etc.
  prefs: []
  type: TYPE_NORMAL
- en: UDC image restoration has been studied in a few literary works. As the pioneered
    of this work, [[203](#bib.bib203)] devise a Monitor Camera Imaging System (MCIS)
    to collect paired data and offer two practical solutions for UDC image restoration,
    including a deconvolution-based Wierner Filter pipeline and the data-driven method.[[181](#bib.bib181)]
    redesign the display layout to improve the quality of recovered images. To address
    spatially variant blur and noise,[[70](#bib.bib70)] develop a controllable image
    restoration algorithm, which performs well on both a monitor-based aligned dataset[[202](#bib.bib202)]
    and a real-world dataset.[[32](#bib.bib32)] propose a domain-knowledge-based network
    to restore the UDC images. However, this work requires a point-spread function
    (PSF) as a prior, thus failing to work well when the PSF is unavailable. To remedy
    this,[[69](#bib.bib69)] design an effective two-branched network for UDC image
    restoration from the perspective of low-frequency and high-frequency information
    learning. The UDC image restoration methods need paired image datasets that perform
    poor generalization capability for real degradations. The reason behind this phenomenon
    is the domain discrepancy between synthesized data and real-captured data. To
    alleviate this problem,[[31](#bib.bib31)] employ collected non-aligned data for
    UDC image restoration and improve the robustness of the restoration network in
    real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Under-display ToF. Currently, only one work[[118](#bib.bib118)] has investigated
    image restoration with a ToF camera placed under a display. In this paper, they
    propose a cascaded network to restore the depth in a coarse-to-fine manner. Differently
    from previous methods, in the first stage, they design a complex-valued neural
    network to recover the ToF raw measurements. In the second stage, they refine
    the depth based on the proposed multi-scale depth enhancement block. Moreover,
    to enable the training, they introduce real and synthetic datasets for UD-ToF
    imaging, respectively. Specifically, the large-scale synthetic dataset is created
    by analyzing noise in magnitude and phase.
  prefs: []
  type: TYPE_NORMAL
- en: If a color camera is located under a display with a ToF camera, there is greater
    potential for obtaining high-quality depth maps. However, the design of models
    to jointly compensate for multi-modal image degradation is still challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Hybrid models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Typically, RGBD-based imaging can be addressed by either conventional or deep-learning
    methods, each of which possesses advantages and disadvantages. A major advantage
    of conventional methods is that, through well-interpretable hand-crafted models,
    the predicted depth can be guaranteed to be loyal to the source. However, designing
    a model with excellent feature representation capabilities is usually extremely
    challenging. In contrast, deep-learning methods can perform very well on a specific
    task with sufficient and representative data for training. On the other hand,
    when there is insufficient data or a biased distribution, these trained models
    may not achieve the desired results during testing. Therefore, combining the advantages
    of both to design hybrid models is one of the future trends in RGB guided ToF
    camera imaging. In particular, the hand-crafted prior, complemented by the powerful
    feature representation capability of neural networks, can make it easier to achieve
    a lightweight approach.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7 Online alignment of RGB and depth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous work, RGB and depth maps were usually assumed to have been well-aligned
    in spatial by calibration operations. However, in practice, the intrinsic ${c_{x},c_{y}}$
    and extrinsic parameters ${t_{x},t_{y}}$ change when the camera is deployed on
    a device or subjected to vibration, which requires online weakly calibration for
    the RGB guided ToF camera. On the other hand, cross-modal online alignment in
    the time dimension is also necessary for practical applications, as different
    sensors may not achieve completely accurate synchronization in sensing the environment.
    Especially when the acquisition is out of sync during motion, the imaging quality
    of the RGBD camera will be seriously degraded. Although some previous efforts[[122](#bib.bib122)]
    have been devoted to cross-modal correspondence matching, the algorithms consume
    a long inference time, so temporally and spatially efficient alignment methods
    remain to be further explored.
  prefs: []
  type: TYPE_NORMAL
- en: 7.8 Lightweight ToF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To promote the application of ToF cameras in various consumer electronic devices,
    lightweight ToF cameras [[80](#bib.bib80), [97](#bib.bib97)] were launched on
    the market and have received the favor of many manufacturers for their low power
    consumption, low cost, and compact structure. Unlike common ToFs, lightweight
    ToF cameras generate measurements as depth distributions rather than specific
    values. Specifically, this type of camera can be used in many fields, such as
    AR/VR and obstacle avoidance. Nonetheless, the lightweight design inevitably brings
    drawbacks such as extremely low resolution (e.g., $8\times 8$), which leads to
    a degradation in the depth quality and cannot support applications requiring high-quality
    depth. Therefore, using other modal information, such as RGB or IR, to improve
    the resolution of lightweight ToF depth becomes one of the future trends.
  prefs: []
  type: TYPE_NORMAL
- en: 7.9 Multi-frame processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, most algorithms deal with RGB guided ToF imaging in a single-frame
    fashion, using one degraded depth map as input to infer the corresponding high-quality
    output. However, the format we usually capture with cameras is a data stream (video)
    rather than a single image. Therefore, it is crucial to consider the temporal
    correlation of data and output stable depth maps. There have been some works [[171](#bib.bib171),
    [114](#bib.bib114)] employing multi-frame processing technology for image denoising,
    enhancement, and super-resolution, but only a few works[[147](#bib.bib147)] have
    applied it to ToF depth improvement. Based on the above analysis, fusing ToF depth
    and RGB images with multi-frame processing technology is a promising direction.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we have presented a survey of RGB guided ToF imaging methods
    based on deep learning. Our review covers preliminaries, evaluation metrics, network
    design, learning protocols, benchmark datasets, and objective functions. According
    to the measurable distance and the purpose of the ToF camera, we roughly divide
    the problem faced into two categories: guided depth super-resolution and guided
    depth completion. Moreover, we collect a quantitative comparison of the surveyed
    methods on widely used benchmarks and analyze their performance and respective
    characteristics. Finally, we summarize the current challenges in practical applications
    as well as promising future trends.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is supported by the National Natural Science Foundation of China (No.
    62088102, No. 62376208), Fundamental Research Funds for the Central Universities
    (No. xzy022023107), China Telecom Group Corporation-Xi’an Jiaotong University
    Jointly Established Intelligent Cloud Network Science and Education Integration
    Innovation Research Institute (No. 20221279-ZKT03).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \bibcommenthead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achar \BOthers. [\APACyear2017] \APACinsertmetastarachar2017epipolar{APACrefauthors}Achar,
    S., Bartels, J.R., Whittaker, W.L., Kutulakos, K.N.\BCBL Narasimhan, S.G. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleEpipolar time-of-flight imaging Epipolar time-of-flight imaging.\BBCQ
    \APACjournalVolNumPagesACM Transactions on Graphics (ToG)3641–8, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Atapour-Abarghouei \BBA Breckon [\APACyear2019] \APACinsertmetastaratapour2019complete{APACrefauthors}Atapour-Abarghouei,
    A.\BCBT \BBA Breckon, T.P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleTo complete
    or to estimate, that is the question: A multi-task approach to depth completion
    and monocular depth estimation To complete or to estimate, that is the question:
    A multi-task approach to depth completion and monocular depth estimation.\BBCQ
    \APACrefbtitle2019 International Conference on 3D Vision (3DV) 2019 international
    conference on 3d vision (3dv) (\BPGS183–193). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bartolomei \BOthers. [\APACyear2024] \APACinsertmetastarBartolomei_2024_3DV{APACrefauthors}Bartolomei,
    L., Poggi, M., Conti, A., Tosi, F.\BCBL Mattoccia, S. \APACrefYearMonthDay2024March.
    \BBOQ\APACrefatitleRevisiting depth completion from a stereo matching perspective
    for cross-domain generalization Revisiting depth completion from a stereo matching
    perspective for cross-domain generalization.\BBCQ \APACrefbtitleInternational
    Conference on 3D Vision 2024 (3DV 2024). International conference on 3d vision
    2024 (3dv 2024). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bartolomei \BOthers. [\APACyear2023] \APACinsertmetastarBartolomei_2023_ICCV{APACrefauthors}Bartolomei,
    L., Poggi, M., Tosi, F., Conti, A.\BCBL Mattoccia, S. \APACrefYearMonthDay2023October.
    \BBOQ\APACrefatitleActive Stereo Without Pattern Projector Active stereo without
    pattern projector.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV) Proceedings of the ieee/cvf international
    conference on computer vision (iccv) (\BPG18470-18482). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Butler \BOthers. [\APACyear2012] \APACinsertmetastarbutler2012naturalistic{APACrefauthors}Butler,
    D.J., Wulff, J., Stanley, G.B.\BCBL Black, M.J. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleA
    naturalistic open source movie for optical flow evaluation A naturalistic open
    source movie for optical flow evaluation.\BBCQ \APACrefbtitleComputer Vision–ECCV
    2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13,
    2012, Proceedings, Part VI 12 Computer vision–eccv 2012: 12th european conference
    on computer vision, florence, italy, october 7-13, 2012, proceedings, part vi
    12 (\BPGS611–625). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carranza-García \BOthers. [\APACyear2022] \APACinsertmetastarcarranza2022object{APACrefauthors}Carranza-García,
    M., Galán-Sales, F.J., Luna-Romera, J.M.\BCBL Riquelme, J.C. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleObject detection using depth completion and camera-LiDAR fusion
    for autonomous driving Object detection using depth completion and camera-lidar
    fusion for autonomous driving.\BBCQ \APACjournalVolNumPagesIntegrated Computer-Aided
    Engineering293241–258, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charbonnier \BOthers. [\APACyear1994] \APACinsertmetastarcharbonnier1994two{APACrefauthors}Charbonnier,
    P., Blanc-Feraud, L., Aubert, G.\BCBL Barlaud, M. \APACrefYearMonthDay1994. \BBOQ\APACrefatitleTwo
    deterministic half-quadratic regularization algorithms for computed imaging Two
    deterministic half-quadratic regularization algorithms for computed imaging.\BBCQ
    \APACrefbtitleProceedings of 1st international conference on image processing
    Proceedings of 1st international conference on image processing (\BVOL2, \BPGS168–172).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'D. Chen \BOthers. [\APACyear2023] \APACinsertmetastarchen2023agg{APACrefauthors}Chen,
    D., Huang, T., Song, Z., Deng, S.\BCBL Jia, T. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleAGG-Net:
    Attention Guided Gated-convolutional Network for Depth Image Completion Agg-net:
    Attention guided gated-convolutional network for depth image completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF International Conference on Computer
    Vision Proceedings of the ieee/cvf international conference on computer vision (\BPGS8853–8862).
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y. Chen \BOthers. [\APACyear2020] \APACinsertmetastarchen2020dynamic{APACrefauthors}Chen,
    Y., Dai, X., Liu, M., Chen, D., Yuan, L.\BCBL Liu, Z. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleDynamic convolution: Attention over convolution kernels Dynamic
    convolution: Attention over convolution kernels.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF conference on computer vision and pattern recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS11030–11039).
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng \BOthers. [\APACyear2020] \APACinsertmetastarcheng2020cspn++{APACrefauthors}Cheng,
    X., Wang, P., Guan, C.\BCBL Yang, R. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleCspn++:
    Learning context and resource aware convolutional spatial propagation networks
    for depth completion Cspn++: Learning context and resource aware convolutional
    spatial propagation networks for depth completion.\BBCQ \APACrefbtitleProceedings
    of the AAAI Conference on Artificial Intelligence Proceedings of the aaai conference
    on artificial intelligence (\BVOL34, \BPGS10615–10622). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng \BOthers. [\APACyear2018] \APACinsertmetastarcheng2018depth{APACrefauthors}Cheng,
    X., Wang, P.\BCBL Yang, R. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleDepth
    estimation via affinity learned with convolutional spatial propagation network
    Depth estimation via affinity learned with convolutional spatial propagation network.\BBCQ
    \APACrefbtitleProceedings of the European Conference on Computer Vision (ECCV)
    Proceedings of the european conference on computer vision (eccv) (\BPGS103–119).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho \BOthers. [\APACyear2021] \APACinsertmetastarcho2021deep{APACrefauthors}Cho,
    J., Min, D., Kim, Y.\BCBL Sohn, K. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDeep
    monocular depth estimation leveraging a large-scale outdoor stereo dataset Deep
    monocular depth estimation leveraging a large-scale outdoor stereo dataset.\BBCQ
    \APACjournalVolNumPagesExpert Systems with Applications178114877, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chodosh \BOthers. [\APACyear2019] \APACinsertmetastarchodosh2019deep{APACrefauthors}Chodosh,
    N., Wang, C.\BCBL Lucey, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeep
    convolutional compressed sensing for lidar depth completion Deep convolutional
    compressed sensing for lidar depth completion.\BBCQ \APACrefbtitleComputer Vision–ACCV
    2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2–6,
    2018, Revised Selected Papers, Part I 14 Computer vision–accv 2018: 14th asian
    conference on computer vision, perth, australia, december 2–6, 2018, revised selected
    papers, part i 14 (\BPGS499–513). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi \BOthers. [\APACyear2021] \APACinsertmetastarchoi2021selfdeco{APACrefauthors}Choi,
    J., Jung, D., Lee, Y., Kim, D., Manocha, D.\BCBL Lee, D. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleSelfdeco: Self-supervised monocular depth completion in challenging
    indoor environments Selfdeco: Self-supervised monocular depth completion in challenging
    indoor environments.\BBCQ \APACrefbtitle2021 IEEE International Conference on
    Robotics and Automation (ICRA) 2021 ieee international conference on robotics
    and automation (icra) (\BPGS467–474). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chugunov \BOthers. [\APACyear2021] \APACinsertmetastarchugunov2021mask{APACrefauthors}Chugunov,
    I., Baek, S\BHBIH., Fu, Q., Heidrich, W.\BCBL Heide, F. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMask-tof: Learning microlens masks for flying pixel correction
    in time-of-flight imaging Mask-tof: Learning microlens masks for flying pixel
    correction in time-of-flight imaging.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS9116–9126). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cong \BOthers. [\APACyear2018] \APACinsertmetastarcong2018hscs{APACrefauthors}Cong,
    R., Lei, J., Fu, H., Huang, Q., Cao, X.\BCBL Ling, N. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitleHSCS: Hierarchical sparsity based co-saliency detection for
    RGBD images Hscs: Hierarchical sparsity based co-saliency detection for rgbd images.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Multimedia2171660–1671, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conti \BOthers. [\APACyear2023] \APACinsertmetastarconti2023sparsity{APACrefauthors}Conti,
    A., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSparsity
    Agnostic Depth Completion Sparsity agnostic depth completion.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision Proceedings
    of the ieee/cvf winter conference on applications of computer vision (\BPGS5871–5880).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Lutio \BOthers. [\APACyear2022] \APACinsertmetastarde2022learning{APACrefauthors}De Lutio,
    R., Becker, A., D’Aronco, S., Russo, S., Wegner, J.D.\BCBL Schindler, K. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleLearning graph regularisation for guided super-resolution Learning
    graph regularisation for guided super-resolution.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS1979–1988).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P. Deng \BOthers. [\APACyear2022] \APACinsertmetastardeng2022multi{APACrefauthors}Deng,
    P., Ge, C., Qiao, X.\BCBL Wei, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleMulti-stream
    Face Anti-spoofing System Using 3D Information Multi-stream face anti-spoofing
    system using 3d information.\BBCQ \APACrefbtitle2022 IEEE International Conference
    on Consumer Electronics (ICCE) 2022 ieee international conference on consumer
    electronics (icce) (\BPGS1–6). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X. Deng \BBA Dragotti [\APACyear2019] \APACinsertmetastardeng2019coupled{APACrefauthors}Deng,
    X.\BCBT \BBA Dragotti, P.L. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleCoupled
    ista network for multi-modal image super-resolution Coupled ista network for multi-modal
    image super-resolution.\BBCQ \APACrefbtitleICASSP 2019-2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP) Icassp 2019-2019
    ieee international conference on acoustics, speech and signal processing (icassp) (\BPGS1862–1866).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X. Deng \BBA Dragotti [\APACyear2020] \APACinsertmetastardeng2020deep{APACrefauthors}Deng,
    X.\BCBT \BBA Dragotti, P.L. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDeep
    convolutional neural network for multi-modal image restoration and fusion Deep
    convolutional neural network for multi-modal image restoration and fusion.\BBCQ
    \APACjournalVolNumPagesIEEE transactions on pattern analysis and machine intelligence43103333–3348,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Déziel \BOthers. [\APACyear2021] \APACinsertmetastardeziel2021pixset{APACrefauthors}Déziel,
    J\BHBIL., Merriaux, P., Tremblay, F., Lessard, D., Plourde, D., Stanguennec, J.\BDBLOlivier,
    P. \APACrefYearMonthDay2021. \BBOQ\APACrefatitlePixset: An opportunity for 3D
    computer vision to go beyond point clouds with a full-waveform LiDAR dataset Pixset:
    An opportunity for 3d computer vision to go beyond point clouds with a full-waveform
    lidar dataset.\BBCQ \APACrefbtitle2021 IEEE International Intelligent Transportation
    Systems Conference (ITSC) 2021 ieee international intelligent transportation systems
    conference (itsc) (\BPGS2987–2993). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diebel \BBA Thrun [\APACyear2005] \APACinsertmetastardiebel2005application{APACrefauthors}Diebel,
    J.\BCBT \BBA Thrun, S. \APACrefYearMonthDay2005. \BBOQ\APACrefatitleAn application
    of markov random fields to range sensing An application of markov random fields
    to range sensing.\BBCQ \APACjournalVolNumPagesAdvances in neural information processing
    systems18, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Dong \BOthers. [\APACyear2021] \APACinsertmetastardong2021learning{APACrefauthors}Dong,
    J., Pan, J., Ren, J.S., Lin, L., Tang, J.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleLearning spatially variant linear representation models for
    joint filtering Learning spatially variant linear representation models for joint
    filtering.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Pattern Analysis and
    Machine Intelligence44118355–8370, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'X. Dong \BOthers. [\APACyear2022] \APACinsertmetastardong2022learning{APACrefauthors}Dong,
    X., Yokoya, N., Wang, L.\BCBL Uezato, T. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleLearning
    Mutual Modulation for Self-supervised Cross-Modal Super-Resolution Learning mutual
    modulation for self-supervised cross-modal super-resolution.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XIX Computer vision–eccv 2022: 17th european conference, tel
    aviv, israel, october 23–27, 2022, proceedings, part xix (\BPGS1–18). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigen \BBA Fergus [\APACyear2015] \APACinsertmetastareigen2015predicting{APACrefauthors}Eigen,
    D.\BCBT \BBA Fergus, R. \APACrefYearMonthDay2015. \BBOQ\APACrefatitlePredicting
    depth, surface normals and semantic labels with a common multi-scale convolutional
    architecture Predicting depth, surface normals and semantic labels with a common
    multi-scale convolutional architecture.\BBCQ \APACrefbtitleProceedings of the
    IEEE International Conference on Computer Vision Proceedings of the ieee international
    conference on computer vision (\BPGS2650–2658). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eldesokey \BOthers. [\APACyear2020] \APACinsertmetastareldesokey2020uncertainty{APACrefauthors}Eldesokey,
    A., Felsberg, M., Holmquist, K.\BCBL Persson, M. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleUncertainty-aware
    cnns for depth completion: Uncertainty from beginning to end Uncertainty-aware
    cnns for depth completion: Uncertainty from beginning to end.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS12014–12023).
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eldesokey \BOthers. [\APACyear2018] \APACinsertmetastareldesokey2018propagating{APACrefauthors}Eldesokey,
    A., Felsberg, M.\BCBL Khan, F.S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitlePropagating
    confidences through cnns for sparse data regression Propagating confidences through
    cnns for sparse data regression.\BBCQ \APACjournalVolNumPagesarXiv preprint arXiv:1805.11913,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eldesokey \BOthers. [\APACyear2019] \APACinsertmetastareldesokey2019confidence{APACrefauthors}Eldesokey,
    A., Felsberg, M.\BCBL Khan, F.S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleConfidence
    propagation through cnns for guided sparse depth regression Confidence propagation
    through cnns for guided sparse depth regression.\BBCQ \APACjournalVolNumPagesIEEE
    transactions on pattern analysis and machine intelligence42102423–2436, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan \BOthers. [\APACyear2022] \APACinsertmetastarFan_2022_BMVC{APACrefauthors}Fan,
    R., Li, Z., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleA
    Cascade Dense Connection Fusion Network for Depth Completion A cascade dense connection
    fusion network for depth completion.\BBCQ \APACrefbtitle33rd British Machine Vision
    Conference 2022, BMVC 2022, London, UK, November 21-24, 2022\. 33rd british machine
    vision conference 2022, BMVC 2022, london, uk, november 21-24, 2022. \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng \BOthers. [\APACyear2023] \APACinsertmetastarfeng2023generating{APACrefauthors}Feng,
    R., Li, C., Chen, H., Li, S., Gu, J.\BCBL Loy, C.C. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleGenerating Aligned Pseudo-Supervision from Non-Aligned Data
    for Image Restoration in Under-Display Camera Generating aligned pseudo-supervision
    from non-aligned data for image restoration in under-display camera.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2304.06019, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng \BOthers. [\APACyear2021] \APACinsertmetastarfeng2021removing{APACrefauthors}Feng,
    R., Li, C., Chen, H., Li, S., Loy, C.C.\BCBL Gu, J. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleRemoving diffraction image artifacts in under-display camera
    via dynamic skip connection network Removing diffraction image artifacts in under-display
    camera via dynamic skip connection network.\BBCQ \APACrefbtitleProceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS662–671).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferstl \BOthers. [\APACyear2013] \APACinsertmetastarferstl2013image{APACrefauthors}Ferstl,
    D., Reinbacher, C., Ranftl, R., Rüther, M.\BCBL Bischof, H. \APACrefYearMonthDay2013.
    \BBOQ\APACrefatitleImage guided depth upsampling using anisotropic total generalized
    variation Image guided depth upsampling using anisotropic total generalized variation.\BBCQ
    \APACrefbtitleProceedings of the IEEE international conference on computer vision
    Proceedings of the ieee international conference on computer vision (\BPGS993–1000).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figueiredo [\APACyear2001] \APACinsertmetastarfigueiredo2001adaptive{APACrefauthors}Figueiredo,
    M. \APACrefYearMonthDay2001. \BBOQ\APACrefatitleAdaptive sparseness using Jeffreys
    prior Adaptive sparseness using jeffreys prior.\BBCQ \APACjournalVolNumPagesAdvances
    in neural information processing systems14, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaidon \BOthers. [\APACyear2016] \APACinsertmetastargaidon2016virtual{APACrefauthors}Gaidon,
    A., Wang, Q., Cabon, Y.\BCBL Vig, E. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleVirtual
    worlds as proxy for multi-object tracking analysis Virtual worlds as proxy for
    multi-object tracking analysis.\BBCQ \APACrefbtitleProceedings of the IEEE conference
    on computer vision and pattern recognition Proceedings of the ieee conference
    on computer vision and pattern recognition (\BPGS4340–4349). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge \BOthers. [\APACyear2021] \APACinsertmetastarge2021tof{APACrefauthors}Ge,
    C., Qiao, X., Huimin, Y., Zhou, Y.\BCBL Deng, P. \APACrefYearMonthDay2021\APACmonth10 12.
    \APACrefbtitleTOF depth sensor based on laser speckle projection and distance
    measuring method thereof. Tof depth sensor based on laser speckle projection and
    distance measuring method thereof. \APACaddressPublisherGoogle Patents. \APACrefnoteUS
    Patent 11,143,880 \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Godard \BOthers. [\APACyear2017] \APACinsertmetastargodard2017unsupervised{APACrefauthors}Godard,
    C., Mac Aodha, O.\BCBL Brostow, G.J. \APACrefYearMonthDay2017. \BBOQ\APACrefatitleUnsupervised
    monocular depth estimation with left-right consistency Unsupervised monocular
    depth estimation with left-right consistency.\BBCQ \APACrefbtitleProceedings of
    the IEEE conference on computer vision and pattern recognition Proceedings of
    the ieee conference on computer vision and pattern recognition (\BPGS270–279).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Godard \BOthers. [\APACyear2019] \APACinsertmetastargodard2019digging{APACrefauthors}Godard,
    C., Mac Aodha, O., Firman, M.\BCBL Brostow, G.J. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDigging
    into self-supervised monocular depth estimation Digging into self-supervised monocular
    depth estimation.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF international
    conference on computer vision Proceedings of the ieee/cvf international conference
    on computer vision (\BPGS3828–3838). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'J. Gu \BOthers. [\APACyear2021] \APACinsertmetastargu2021denselidar{APACrefauthors}Gu,
    J., Xiang, Z., Ye, Y.\BCBL Wang, L. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDenseLiDAR:
    A real-time pseudo dense depth guided depth completion network Denselidar: A real-time
    pseudo dense depth guided depth completion network.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters621808–1815, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Gu \BOthers. [\APACyear2019] \APACinsertmetastargu2019learned{APACrefauthors}Gu,
    S., Guo, S., Zuo, W., Chen, Y., Timofte, R., Van Gool, L.\BCBL Zhang, L. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleLearned dynamic guidance for depth image reconstruction Learned
    dynamic guidance for depth image reconstruction.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence42102437–2452, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guizilini \BOthers. [\APACyear2021] \APACinsertmetastarguizilini2021sparse{APACrefauthors}Guizilini,
    V., Ambrus, R., Burgard, W.\BCBL Gaidon, A. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSparse
    auxiliary networks for unified monocular depth prediction and completion Sparse
    auxiliary networks for unified monocular depth prediction and completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS11078–11088). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guizilini, Ambrus\BCBL \BOthers. [\APACyear2020] \APACinsertmetastarguizilini20203d{APACrefauthors}Guizilini,
    V., Ambrus, R., Pillai, S., Raventos, A.\BCBL Gaidon, A. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitle3d packing for self-supervised monocular depth estimation 3d
    packing for self-supervised monocular depth estimation.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF conference on computer vision and pattern recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS2485–2494).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guizilini, Li\BCBL \BOthers. [\APACyear2020] \APACinsertmetastarguizilini2020robust{APACrefauthors}Guizilini,
    V., Li, J., Ambrus, R., Pillai, S.\BCBL Gaidon, A. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleRobust
    semi-supervised monocular depth estimation with reprojected distances Robust semi-supervised
    monocular depth estimation with reprojected distances.\BBCQ \APACrefbtitleConference
    on robot learning Conference on robot learning (\BPGS503–512). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo \BOthers. [\APACyear2019] \APACinsertmetastarguo2018hierarchical{APACrefauthors}Guo,
    C., Li, C., Guo, J., Cong, R., Fu, H.\BCBL Han, P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleHierarchical
    Features Driven Residual Learning for Depth Map Super-Resolution Hierarchical
    features driven residual learning for depth map super-resolution.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Image Processing2852545-2557, {APACrefDOI}  [https://doi.org/10.1109/TIP.2018.2887029](https://doi.org/10.1109/TIP.2018.2887029)
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta \BOthers. [\APACyear2015] \APACinsertmetastargupta2015phasor{APACrefauthors}Gupta,
    M., Nayar, S.K., Hullin, M.B.\BCBL Martin, J. \APACrefYearMonthDay2015. \BBOQ\APACrefatitlePhasor
    imaging: A generalization of correlation-based time-of-flight imaging Phasor imaging:
    A generalization of correlation-based time-of-flight imaging.\BBCQ \APACjournalVolNumPagesACM
    Transactions on Graphics (ToG)3451–18, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gutierrez-Barragan \BOthers. [\APACyear2021] \APACinsertmetastargutierrez2021itof2dtof{APACrefauthors}Gutierrez-Barragan,
    F., Chen, H., Gupta, M., Velten, A.\BCBL Gu, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleitof2dtof:
    A robust and flexible representation for data-driven time-of-flight imaging itof2dtof:
    A robust and flexible representation for data-driven time-of-flight imaging.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Computational Imaging71205–1214, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Häne \BOthers. [\APACyear2017] \APACinsertmetastarhane20173d{APACrefauthors}Häne,
    C., Heng, L., Lee, G.H., Fraundorfer, F., Furgale, P., Sattler, T.\BCBL Pollefeys,
    M. \APACrefYearMonthDay2017. \BBOQ\APACrefatitle3D visual perception for self-driving
    cars using a multi-camera system: Calibration, mapping, localization, and obstacle
    detection 3d visual perception for self-driving cars using a multi-camera system:
    Calibration, mapping, localization, and obstacle detection.\BBCQ \APACjournalVolNumPagesImage
    and Vision Computing6814–27, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K. He \BOthers. [\APACyear2010] \APACinsertmetastarhe2010guided{APACrefauthors}He,
    K., Sun, J.\BCBL Tang, X. \APACrefYearMonthDay2010. \BBOQ\APACrefatitleGuided
    image filtering Guided image filtering.\BBCQ \APACrefbtitleEuropean conference
    on computer vision European conference on computer vision (\BPGS1–14). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K. He \BOthers. [\APACyear2012] \APACinsertmetastarhe2012guided{APACrefauthors}He,
    K., Sun, J.\BCBL Tang, X. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleGuided
    image filtering Guided image filtering.\BBCQ \APACjournalVolNumPagesIEEE transactions
    on pattern analysis and machine intelligence3561397–1409, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L. He \BOthers. [\APACyear2021] \APACinsertmetastarhe2021towards{APACrefauthors}He,
    L., Zhu, H., Li, F., Bai, H., Cong, R., Zhang, C.\BDBLZhao, Y. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleTowards fast and accurate real-world depth super-resolution:
    Benchmark dataset and baseline Towards fast and accurate real-world depth super-resolution:
    Benchmark dataset and baseline.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS9229–9238). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heide \BOthers. [\APACyear2015] \APACinsertmetastarheide2015doppler{APACrefauthors}Heide,
    F., Heidrich, W., Hullin, M.\BCBL Wetzstein, G. \APACrefYearMonthDay2015. \BBOQ\APACrefatitleDoppler
    time-of-flight imaging Doppler time-of-flight imaging.\BBCQ \APACjournalVolNumPagesACM
    Transactions on Graphics (ToG)3441–11, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hirata \BOthers. [\APACyear2019] \APACinsertmetastarhirata2019real{APACrefauthors}Hirata,
    A., Ishikawa, R., Roxas, M.\BCBL Oishi, T. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleReal-time
    dense depth estimation using semantically-guided LIDAR data propagation and motion
    stereo Real-time dense depth estimation using semantically-guided lidar data propagation
    and motion stereo.\BBCQ \APACjournalVolNumPagesIEEE Robotics and Automation Letters443806–3811,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hirschmuller \BBA Scharstein [\APACyear2007] \APACinsertmetastarhirschmuller2007evaluation{APACrefauthors}Hirschmuller,
    H.\BCBT \BBA Scharstein, D. \APACrefYearMonthDay2007. \BBOQ\APACrefatitleEvaluation
    of cost functions for stereo matching Evaluation of cost functions for stereo
    matching.\BBCQ \APACrefbtitle2007 IEEE Conference on Computer Vision and Pattern
    Recognition 2007 ieee conference on computer vision and pattern recognition (\BPGS1–8).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holynski \BBA Kopf [\APACyear2018] \APACinsertmetastarholynski2018fast{APACrefauthors}Holynski,
    A.\BCBT \BBA Kopf, J. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleFast depth
    densification for occlusion-aware augmented reality Fast depth densification for
    occlusion-aware augmented reality.\BBCQ \APACjournalVolNumPagesACM Transactions
    on Graphics (ToG)3761–11, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'J. Hu \BOthers. [\APACyear2022] \APACinsertmetastarsurvey_completion_2{APACrefauthors}Hu,
    J., Bao, C., Ozay, M., Fan, C., Gao, Q., Liu, H.\BCBL Lam, T.L. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleDeep Depth Completion from Extremely Sparse Data: A Survey
    Deep depth completion from extremely sparse data: A survey.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence1-20, {APACrefDOI}  [https://doi.org/10.1109/TPAMI.2022.3229090](https://doi.org/10.1109/TPAMI.2022.3229090)
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'M. Hu \BOthers. [\APACyear2021] \APACinsertmetastarhu2021penet{APACrefauthors}Hu,
    M., Wang, S., Li, B., Ning, S., Fan, L.\BCBL Gong, X. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitlePenet: Towards precise and efficient image guided depth completion
    Penet: Towards precise and efficient image guided depth completion.\BBCQ \APACrefbtitle2021
    IEEE International Conference on Robotics and Automation (ICRA) 2021 ieee international
    conference on robotics and automation (icra) (\BPGS13656–13662). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang \BOthers. [\APACyear2017] \APACinsertmetastarhuang2017densely{APACrefauthors}Huang,
    G., Liu, Z., Van Der Maaten, L.\BCBL Weinberger, K.Q. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleDensely connected convolutional networks Densely connected
    convolutional networks.\BBCQ \APACrefbtitleProceedings of the IEEE conference
    on computer vision and pattern recognition Proceedings of the ieee conference
    on computer vision and pattern recognition (\BPGS4700–4708). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hui \BOthers. [\APACyear2016] \APACinsertmetastarhui2016depth{APACrefauthors}Hui,
    T\BHBIW., Loy, C.C.\BCBL Tang, X. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleDepth
    map super-resolution by deep multi-scale guidance Depth map super-resolution by
    deep multi-scale guidance.\BBCQ \APACrefbtitleComputer Vision–ECCV 2016: 14th
    European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,
    Part III 14 Computer vision–eccv 2016: 14th european conference, amsterdam, the
    netherlands, october 11-14, 2016, proceedings, part iii 14 (\BPGS353–369). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hussmann \BOthers. [\APACyear2013] \APACinsertmetastarhussmann2013modulation{APACrefauthors}Hussmann,
    S., Knoll, F.\BCBL Edeler, T. \APACrefYearMonthDay2013. \BBOQ\APACrefatitleModulation
    method including noise model for minimizing the wiggling error of TOF cameras
    Modulation method including noise model for minimizing the wiggling error of tof
    cameras.\BBCQ \APACjournalVolNumPagesIEEE transactions on instrumentation and
    measurement6351127–1136, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaritz \BOthers. [\APACyear2018] \APACinsertmetastarjaritz2018sparse{APACrefauthors}Jaritz,
    M., De Charette, R., Wirbel, E., Perrotton, X.\BCBL Nashashibi, F. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitleSparse and dense data with cnns: Depth completion and semantic
    segmentation Sparse and dense data with cnns: Depth completion and semantic segmentation.\BBCQ
    \APACrefbtitle2018 International Conference on 3D Vision (3DV) 2018 international
    conference on 3d vision (3dv) (\BPGS52–60). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'J. Jeon \BOthers. [\APACyear2022] \APACinsertmetastarjeon2022struct{APACrefauthors}Jeon,
    J., Lim, H., Seo, D\BHBIU.\BCBL Myung, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleStruct-MDC:
    Mesh-refined unsupervised depth completion leveraging structural regularities
    from visual SLAM Struct-mdc: Mesh-refined unsupervised depth completion leveraging
    structural regularities from visual slam.\BBCQ \APACjournalVolNumPagesIEEE Robotics
    and Automation Letters736391–6398, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y. Jeon \BOthers. [\APACyear2021] \APACinsertmetastarjeon2021abcd{APACrefauthors}Jeon,
    Y., Kim, H.\BCBL Seo, S\BHBIW. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleABCD:
    Attentive bilateral convolutional network for robust depth completion Abcd: Attentive
    bilateral convolutional network for robust depth completion.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters7181–87, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang \BOthers. [\APACyear2022] \APACinsertmetastarjiang2022low{APACrefauthors}Jiang,
    X., Cambareri, V., Agresti, G., Ugwu, C.I., Simonetto, A., Cardinaux, F.\BCBL
    Zanuttigh, P. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleA Low Memory Footprint
    Quantized Neural Network for Depth Completion of Very Sparse Time-of-Flight Depth
    Maps A low memory footprint quantized neural network for depth completion of very
    sparse time-of-flight depth maps.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS2687–2696). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalia \BOthers. [\APACyear2019] \APACinsertmetastarkalia2019real{APACrefauthors}Kalia,
    M., Navab, N.\BCBL Salcudean, T. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleA
    real-time interactive augmented reality depth estimation technique for surgical
    robotics A real-time interactive augmented reality depth estimation technique
    for surgical robotics.\BBCQ \APACrefbtitle2019 International Conference on Robotics
    and Automation (ICRA) 2019 international conference on robotics and automation
    (icra) (\BPGS8291–8297). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kam \BOthers. [\APACyear2022] \APACinsertmetastarkam2022costdcnet{APACrefauthors}Kam,
    J., Kim, J., Kim, S., Park, J.\BCBL Lee, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleCostDCNet:
    Cost Volume Based Depth Completion for a Single RGB-D Image Costdcnet: Cost volume
    based depth completion for a single rgb-d image.\BBCQ \APACrefbtitleComputer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part II Computer vision–eccv 2022: 17th european conference, tel aviv, israel,
    october 23–27, 2022, proceedings, part ii (\BPGS257–274). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ke \BOthers. [\APACyear2021] \APACinsertmetastarke2021mdanet{APACrefauthors}Ke,
    Y., Li, K., Yang, W., Xu, Z., Hao, D., Huang, L.\BCBL Wang, G. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMDANet: Multi-Modal Deep Aggregation Network for Depth Completion
    Mdanet: Multi-modal deep aggregation network for depth completion.\BBCQ \APACrefbtitle2021
    IEEE International Conference on Robotics and Automation (ICRA) 2021 ieee international
    conference on robotics and automation (icra) (\BPGS4288–4294). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khan \BOthers. [\APACyear2021] \APACinsertmetastarkhan2021sparse{APACrefauthors}Khan,
    M.F.F., Troncoso Aldas, N.D., Kumar, A., Advani, S.\BCBL Narayanan, V. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleSparse to dense depth completion using a generative adversarial
    network with intelligent sampling strategies Sparse to dense depth completion
    using a generative adversarial network with intelligent sampling strategies.\BBCQ
    \APACrefbtitleProceedings of the 29th ACM International Conference on Multimedia
    Proceedings of the 29th acm international conference on multimedia (\BPGS5528–5536).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim \BOthers. [\APACyear2021] \APACinsertmetastarkim2021deformable{APACrefauthors}Kim,
    B., Ponce, J.\BCBL Ham, B. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDeformable
    kernel networks for joint image filtering Deformable kernel networks for joint
    image filtering.\BBCQ \APACjournalVolNumPagesInternational Journal of Computer
    Vision1292579–600, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koh \BOthers. [\APACyear2022] \APACinsertmetastarkoh2022bnudc{APACrefauthors}Koh,
    J., Lee, J.\BCBL Yoon, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleBnudc:
    A two-branched deep neural network for restoring images from under-display cameras
    Bnudc: A two-branched deep neural network for restoring images from under-display
    cameras.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Proceedings of the ieee/cvf conference on computer
    vision and pattern recognition (\BPGS1950–1959). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon \BOthers. [\APACyear2021] \APACinsertmetastarkwon2021controllable{APACrefauthors}Kwon,
    K., Kang, E., Lee, S., Lee, S\BHBIJ., Lee, H\BHBIE., Yoo, B.\BCBL Han, J\BHBIJ.
    \APACrefYearMonthDay2021. \BBOQ\APACrefatitleControllable image restoration for
    under-display camera in smartphones Controllable image restoration for under-display
    camera in smartphones.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS2073–2082). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: B\BHBIU. Lee \BOthers. [\APACyear2019] \APACinsertmetastarlee2019depth{APACrefauthors}Lee,
    B\BHBIU., Jeon, H\BHBIG., Im, S.\BCBL Kweon, I.S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDepth
    completion with deep geometry and context guidance Depth completion with deep
    geometry and context guidance.\BBCQ \APACrefbtitle2019 International Conference
    on Robotics and Automation (ICRA) 2019 international conference on robotics and
    automation (icra) (\BPGS3281–3287). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Lee \BOthers. [\APACyear2020] \APACinsertmetastarlee2020deep{APACrefauthors}Lee,
    S., Lee, J., Kim, D.\BCBL Kim, J. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDeep
    architecture with cross guidance between single image and sparse lidar data for
    depth completion Deep architecture with cross guidance between single image and
    sparse lidar data for depth completion.\BBCQ \APACjournalVolNumPagesIEEE Access879801–79810,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Lee \BOthers. [\APACyear2022] \APACinsertmetastarlee2022multi{APACrefauthors}Lee,
    S., Yi, E., Lee, J.\BCBL Kim, J. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleMulti-Scaled
    and Densely Connected Locally Convolutional Layers for Depth Completion Multi-scaled
    and densely connected locally convolutional layers for depth completion.\BBCQ
    \APACrefbtitle2022 IEEE/RSJ International Conference on Intelligent Robots and
    Systems (IROS) 2022 ieee/rsj international conference on intelligent robots and
    systems (iros) (\BPGS8360–8367). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A. Li \BOthers. [\APACyear2020] \APACinsertmetastarli2020multi{APACrefauthors}Li,
    A., Yuan, Z., Ling, Y., Chi, W., Zhang, C.\BCBL \BOthersPeriod. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleA multi-scale guided cascade hourglass network for depth completion
    A multi-scale guided cascade hourglass network for depth completion.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision Proceedings
    of the ieee/cvf winter conference on applications of computer vision (\BPGS32–40).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'D. Li \BOthers. [\APACyear2022] \APACinsertmetastarli2022motion{APACrefauthors}Li,
    D., Xu, J., Yang, Z., Zhang, Q., Ma, Q., Zhang, L.\BCBL Chen, P. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleMotion inspires notion: self-supervised visual-LiDAR fusion
    for environment depth estimation Motion inspires notion: self-supervised visual-lidar
    fusion for environment depth estimation.\BBCQ \APACrefbtitleProceedings of the
    20th Annual International Conference on Mobile Systems, Applications and Services
    Proceedings of the 20th annual international conference on mobile systems, applications
    and services (\BPGS114–127). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Li \BOthers. [\APACyear2022] \APACinsertmetastarli2022fisher{APACrefauthors}Li,
    J., Yue, T., Zhao, S.\BCBL Hu, X. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleFisher
    information guidance for learned time-of-flight imaging Fisher information guidance
    for learned time-of-flight imaging.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS16334–16343). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T. Li \BOthers. [\APACyear2020] \APACinsertmetastarli2020depth{APACrefauthors}Li,
    T., Lin, H., Dong, X.\BCBL Zhang, X. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDepth
    image super-resolution using correlation-controlled color guidance and multi-scale
    symmetric network Depth image super-resolution using correlation-controlled color
    guidance and multi-scale symmetric network.\BBCQ \APACjournalVolNumPagesPattern
    Recognition107107513, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Li \BOthers. [\APACyear2016] \APACinsertmetastarli2016deep{APACrefauthors}Li,
    Y., Huang, J\BHBIB., Ahuja, N.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleDeep
    joint image filtering Deep joint image filtering.\BBCQ \APACrefbtitleEuropean
    conference on computer vision European conference on computer vision (\BPGS154–169).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Li \BOthers. [\APACyear2019] \APACinsertmetastarli2019joint{APACrefauthors}Li,
    Y., Huang, J\BHBIB., Ahuja, N.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleJoint
    image filtering with deep convolutional networks Joint image filtering with deep
    convolutional networks.\BBCQ \APACjournalVolNumPagesIEEE transactions on pattern
    analysis and machine intelligence4181909–1923, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y. Li \BOthers. [\APACyear2022] \APACinsertmetastarli2022deltar{APACrefauthors}Li,
    Y., Liu, X., Dong, W., Zhou, H., Bao, H., Zhang, G.\BDBLCui, Z. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleDeltar: Depth estimation from a light-weight tof sensor and
    rgb image Deltar: Depth estimation from a light-weight tof sensor and rgb image.\BBCQ
    \APACrefbtitleEuropean Conference on Computer Vision European conference on computer
    vision (\BPGS619–636). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang \BOthers. [\APACyear2019] \APACinsertmetastarliang2019multi{APACrefauthors}Liang,
    M., Yang, B., Chen, Y., Hu, R.\BCBL Urtasun, R. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleMulti-task
    multi-sensor fusion for 3d object detection Multi-task multi-sensor fusion for
    3d object detection.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS7345–7353). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao \BOthers. [\APACyear2022] \APACinsertmetastarliao2022kitti{APACrefauthors}Liao,
    Y., Xie, J.\BCBL Geiger, A. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleKITTI-360:
    A novel dataset and benchmarks for urban scene understanding in 2d and 3d Kitti-360:
    A novel dataset and benchmarks for urban scene understanding in 2d and 3d.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Pattern Analysis and Machine Intelligence,
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T\BHBIY. Lin \BOthers. [\APACyear2017] \APACinsertmetastarlin2017focal{APACrefauthors}Lin,
    T\BHBIY., Goyal, P., Girshick, R., He, K.\BCBL Dollár, P. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleFocal loss for dense object detection Focal loss for dense
    object detection.\BBCQ \APACrefbtitleProceedings of the IEEE international conference
    on computer vision Proceedings of the ieee international conference on computer
    vision (\BPGS2980–2988). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Lin \BOthers. [\APACyear2022] \APACinsertmetastarlin2022dynamic{APACrefauthors}Lin,
    Y., Cheng, T., Zhong, Q., Zhou, W.\BCBL Yang, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleDynamic
    spatial propagation network for depth completion Dynamic spatial propagation network
    for depth completion.\BBCQ \APACrefbtitleProceedings of the AAAI Conference on
    Artificial Intelligence Proceedings of the aaai conference on artificial intelligence (\BVOL36,
    \BPGS1638–1646). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lipson \BOthers. [\APACyear2021] \APACinsertmetastarRAFT_STEREO{APACrefauthors}Lipson,
    L., Teed, Z.\BCBL Deng, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleRAFT-Stereo:
    Multilevel Recurrent Field Transforms for Stereo Matching Raft-stereo: Multilevel
    recurrent field transforms for stereo matching.\BBCQ \APACrefbtitleInternational
    Conference on 3D Vision (3DV). International conference on 3d vision (3dv). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A. Liu \BOthers. [\APACyear2021] \APACinsertmetastarliu2021casia{APACrefauthors}Liu,
    A., Tan, Z., Wan, J., Escalera, S., Guo, G.\BCBL Li, S.Z. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleCasia-surf cefa: A benchmark for multi-modal cross-ethnicity
    face anti-spoofing Casia-surf cefa: A benchmark for multi-modal cross-ethnicity
    face anti-spoofing.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision Proceedings of the ieee/cvf winter conference
    on applications of computer vision (\BPGS1179–1187). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Liu \BOthers. [\APACyear2012] \APACinsertmetastarliu2012guided{APACrefauthors}Liu,
    J., Gong, X.\BCBL Liu, J. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleGuided
    inpainting and filtering for kinect depth maps Guided inpainting and filtering
    for kinect depth maps.\BBCQ \APACrefbtitleProceedings of the 21st International
    Conference on Pattern Recognition (ICPR2012) Proceedings of the 21st international
    conference on pattern recognition (icpr2012) (\BPGS2055–2058). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'J. Liu \BBA Jung [\APACyear2022] \APACinsertmetastarliu2022nnnet{APACrefauthors}Liu,
    J.\BCBT \BBA Jung, C. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleNNNet: New
    Normal Guided Depth Completion From Sparse LiDAR Data and Single Color Image Nnnet:
    New normal guided depth completion from sparse lidar data and single color image.\BBCQ
    \APACjournalVolNumPagesIEEE Access10114252–114261, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L. Liu, Liao\BCBL \BOthers. [\APACyear2021] \APACinsertmetastarliu2021learning{APACrefauthors}Liu,
    L., Liao, Y., Wang, Y., Geiger, A.\BCBL Liu, Y. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleLearning
    steering kernels for guided depth completion Learning steering kernels for guided
    depth completion.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image Processing302850–2861,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L. Liu, Song\BCBL \BOthers. [\APACyear2021] \APACinsertmetastarliu2021fcfr{APACrefauthors}Liu,
    L., Song, X., Lyu, X., Diao, J., Wang, M., Liu, Y.\BCBL Zhang, L. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleFcfr-net: Feature fusion based coarse-to-fine residual learning
    for depth completion Fcfr-net: Feature fusion based coarse-to-fine residual learning
    for depth completion.\BBCQ \APACrefbtitleProceedings of the AAAI conference on
    artificial intelligence Proceedings of the aaai conference on artificial intelligence (\BVOL35,
    \BPGS2136–2144). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L. Liu \BOthers. [\APACyear2023] \APACinsertmetastarliu2023mff{APACrefauthors}Liu,
    L., Song, X., Sun, J., Lyu, X., Li, L., Liu, Y.\BCBL Zhang, L. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleMFF-Net: Towards Efficient Monocular Depth Completion with
    Multi-modal Feature Fusion Mff-net: Towards efficient monocular depth completion
    with multi-modal feature fusion.\BBCQ \APACjournalVolNumPagesIEEE Robotics and
    Automation Letters, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P. Liu \BOthers. [\APACyear2021] \APACinsertmetastarliu2021deformable{APACrefauthors}Liu,
    P., Zhang, Z., Meng, Z.\BCBL Gao, N. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDeformable
    Enhancement and Adaptive Fusion for Depth Map Super-Resolution Deformable enhancement
    and adaptive fusion for depth map super-resolution.\BBCQ \APACjournalVolNumPagesIEEE
    Signal Processing Letters29204–208, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022pdr{APACrefauthors}Liu,
    P., Zhang, Z., Meng, Z., Gao, N.\BCBL Wang, C. \APACrefYearMonthDay2022. \BBOQ\APACrefatitlePDR-Net:
    Progressive depth reconstruction network for color guided depth map super-resolution
    Pdr-net: Progressive depth reconstruction network for color guided depth map super-resolution.\BBCQ
    \APACjournalVolNumPagesNeurocomputing47975–88, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'R. Liu \BOthers. [\APACyear2016] \APACinsertmetastarliu2016learning{APACrefauthors}Liu,
    R., Zhong, G., Cao, J., Lin, Z., Shan, S.\BCBL Luo, Z. \APACrefYearMonthDay2016.
    \BBOQ\APACrefatitleLearning to diffuse: A new perspective to design pdes for visual
    analysis Learning to diffuse: A new perspective to design pdes for visual analysis.\BBCQ
    \APACjournalVolNumPagesIEEE transactions on pattern analysis and machine intelligence38122457–2471,
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Liu \BOthers. [\APACyear2017] \APACinsertmetastarliu2017learning{APACrefauthors}Liu,
    S., De Mello, S., Gu, J., Zhong, G., Yang, M\BHBIH.\BCBL Kautz, J. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleLearning affinity via spatial propagation networks Learning
    affinity via spatial propagation networks.\BBCQ \APACjournalVolNumPagesAdvances
    in Neural Information Processing Systems30, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'T.Y. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022monitored{APACrefauthors}Liu,
    T.Y., Agrawal, P., Chen, A., Hong, B\BHBIW.\BCBL Wong, A. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleMonitored distillation for positive congruent depth completion
    Monitored distillation for positive congruent depth completion.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part II Computer vision–eccv 2022: 17th european conference, tel
    aviv, israel, october 23–27, 2022, proceedings, part ii (\BPGS35–53). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X. Liu \BOthers. [\APACyear2023] \APACinsertmetastarliu2023multi{APACrefauthors}Liu,
    X., Li, Y., Teng, Y., Bao, H., Zhang, G., Zhang, Y.\BCBL Cui, Z. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleMulti-modal neural radiance field for monocular dense slam
    with a light-weight tof sensor Multi-modal neural radiance field for monocular
    dense slam with a light-weight tof sensor.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF International Conference on Computer Vision Proceedings of the ieee/cvf
    international conference on computer vision (\BPGS1–11). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'X. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022graphcspn{APACrefauthors}Liu,
    X., Shao, X., Wang, B., Li, Y.\BCBL Wang, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleGraphCSPN:
    Geometry-Aware Depth Completion via Dynamic GCNs Graphcspn: Geometry-aware depth
    completion via dynamic gcns.\BBCQ \APACrefbtitleComputer Vision–ECCV 2022: 17th
    European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
    XXXIII Computer vision–eccv 2022: 17th european conference, tel aviv, israel,
    october 23–27, 2022, proceedings, part xxxiii (\BPGS90–107). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lopez-Rodriguez \BOthers. [\APACyear2020] \APACinsertmetastarlopez2020project{APACrefauthors}Lopez-Rodriguez,
    A., Busam, B.\BCBL Mikolajczyk, K. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleProject
    to adapt: Domain adaptation for depth completion from noisy and sparse sensor
    data Project to adapt: Domain adaptation for depth completion from noisy and sparse
    sensor data.\BBCQ \APACrefbtitleProceedings of the Asian Conference on Computer
    Vision. Proceedings of the asian conference on computer vision. \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Lu \BOthers. [\APACyear2016] \APACinsertmetastarlu2016hierarchical{APACrefauthors}Lu,
    J., Yang, J., Batra, D.\BCBL Parikh, D. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleHierarchical
    question-image co-attention for visual question answering Hierarchical question-image
    co-attention for visual question answering.\BBCQ \APACjournalVolNumPagesAdvances
    in neural information processing systems29, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K. Lu \BOthers. [\APACyear2020] \APACinsertmetastarlu2020depth{APACrefauthors}Lu,
    K., Barnes, N., Anwar, S.\BCBL Zheng, L. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleFrom
    depth what can you see? Depth completion via auxiliary image reconstruction From
    depth what can you see? depth completion via auxiliary image reconstruction.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF conference on computer vision and pattern
    recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS11306–11315). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Lu \BOthers. [\APACyear2014] \APACinsertmetastarlu2014depth{APACrefauthors}Lu,
    S., Ren, X.\BCBL Liu, F. \APACrefYearMonthDay2014. \BBOQ\APACrefatitleDepth enhancement
    via low-rank matrix completion Depth enhancement via low-rank matrix completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE conference on computer vision and pattern
    recognition Proceedings of the ieee conference on computer vision and pattern
    recognition (\BPGS3390–3397). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lutio \BOthers. [\APACyear2019] \APACinsertmetastarlutio2019guided{APACrefauthors}Lutio,
    R.d., D’aronco, S., Wegner, J.D.\BCBL Schindler, K. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleGuided super-resolution as pixel-to-pixel transformation Guided
    super-resolution as pixel-to-pixel transformation.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF International Conference on Computer Vision Proceedings of the
    ieee/cvf international conference on computer vision (\BPGS8829–8837). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma \BOthers. [\APACyear2018] \APACinsertmetastarMa2018SelfSupervisedSS{APACrefauthors}Ma,
    F., Cavalheiro, G.V.\BCBL Karaman, S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleSelf-Supervised
    Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera
    Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and
    monocular camera.\BBCQ \APACjournalVolNumPages2019 International Conference on
    Robotics and Automation (ICRA)3288-3295, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma \BOthers. [\APACyear2019] \APACinsertmetastarma2019self{APACrefauthors}Ma,
    F., Cavalheiro, G.V.\BCBL Karaman, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleSelf-supervised
    sparse-to-dense: Self-supervised depth completion from lidar and monocular camera
    Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and
    monocular camera.\BBCQ \APACrefbtitle2019 International Conference on Robotics
    and Automation (ICRA) 2019 international conference on robotics and automation
    (icra) (\BPGS3288–3295). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma \BBA Karaman [\APACyear2018] \APACinsertmetastarma2018sparse{APACrefauthors}Ma,
    F.\BCBT \BBA Karaman, S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleSparse-to-dense:
    Depth prediction from sparse depth samples and a single image Sparse-to-dense:
    Depth prediction from sparse depth samples and a single image.\BBCQ \APACrefbtitle2018
    IEEE international conference on robotics and automation (ICRA) 2018 ieee international
    conference on robotics and automation (icra) (\BPGS4796–4803). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manglik \BOthers. [\APACyear2019] \APACinsertmetastarmanglik2019forecasting{APACrefauthors}Manglik,
    A., Weng, X., Ohn-Bar, E.\BCBL Kitanil, K.M. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleForecasting
    time-to-collision from monocular video: Feasibility, dataset, and challenges Forecasting
    time-to-collision from monocular video: Feasibility, dataset, and challenges.\BBCQ
    \APACrefbtitle2019 IEEE/RSJ International Conference on Intelligent Robots and
    Systems (IROS) 2019 ieee/rsj international conference on intelligent robots and
    systems (iros) (\BPGS8081–8088). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marivani \BOthers. [\APACyear2020] \APACinsertmetastarmarivani2020multimodal{APACrefauthors}Marivani,
    I., Tsiligianni, E., Cornelis, B.\BCBL Deligiannis, N. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleMultimodal deep unfolding for guided image super-resolution
    Multimodal deep unfolding for guided image super-resolution.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Image Processing298443–8456, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mayer \BOthers. [\APACyear2016] \APACinsertmetastarmayer2016large{APACrefauthors}Mayer,
    N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A.\BCBL Brox,
    T. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleA large dataset to train convolutional
    networks for disparity, optical flow, and scene flow estimation A large dataset
    to train convolutional networks for disparity, optical flow, and scene flow estimation.\BBCQ
    \APACrefbtitleProceedings of the IEEE conference on computer vision and pattern
    recognition Proceedings of the ieee conference on computer vision and pattern
    recognition (\BPGS4040–4048). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metzger \BOthers. [\APACyear2022] \APACinsertmetastarmetzger2022guided{APACrefauthors}Metzger,
    N., Daudt, R.C.\BCBL Schindler, K. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleGuided
    Depth Super-Resolution by Deep Anisotropic Diffusion Guided depth super-resolution
    by deep anisotropic diffusion.\BBCQ \APACjournalVolNumPagesarXiv preprint arXiv:2211.11592,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nazir \BOthers. [\APACyear2022] \APACinsertmetastarnazir2022semattnet{APACrefauthors}Nazir,
    D., Pagani, A., Liwicki, M., Stricker, D.\BCBL Afzal, M.Z. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleSemAttNet: Toward Attention-Based Semantic Aware Guided Depth
    Completion Semattnet: Toward attention-based semantic aware guided depth completion.\BBCQ
    \APACjournalVolNumPagesIEEE Access10120781–120791, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen \BBA Yoo [\APACyear2022] \APACinsertmetastarnguyen2022patchgan{APACrefauthors}Nguyen,
    T.\BCBT \BBA Yoo, M. \APACrefYearMonthDay2022. \BBOQ\APACrefatitlePatchGAN-Based
    Depth Completion in Autonomous Vehicle Patchgan-based depth completion in autonomous
    vehicle.\BBCQ \APACrefbtitle2022 International Conference on Information Networking
    (ICOIN) 2022 international conference on information networking (icoin) (\BPGS498–501).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park \BOthers. [\APACyear2020] \APACinsertmetastarpark2020non{APACrefauthors}Park,
    J., Joo, K., Hu, Z., Liu, C\BHBIK.\BCBL So Kweon, I. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleNon-local spatial propagation network for depth completion
    Non-local spatial propagation network for depth completion.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part XIII 16 Computer vision–eccv 2020: 16th european conference, glasgow, uk,
    august 23–28, 2020, proceedings, part xiii 16 (\BPGS120–136). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patil \BOthers. [\APACyear2022] \APACinsertmetastarpatil2022multi{APACrefauthors}Patil,
    P.W., Dudhane, A., Chaudhary, S.\BCBL Murala, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleMulti-frame
    based adversarial learning approach for video surveillance Multi-frame based adversarial
    learning approach for video surveillance.\BBCQ \APACjournalVolNumPagesPattern
    Recognition122108350, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng \BOthers. [\APACyear2022] \APACinsertmetastarpeng2022pixelwise{APACrefauthors}Peng,
    R., Zhang, T., Li, B.\BCBL Wang, Y. \APACrefYearMonthDay2022. \BBOQ\APACrefatitlePixelwise
    Adaptive Discretization with Uncertainty Sampling for Depth Completion Pixelwise
    adaptive discretization with uncertainty sampling for depth completion.\BBCQ \APACrefbtitleProceedings
    of the 30th ACM International Conference on Multimedia Proceedings of the 30th
    acm international conference on multimedia (\BPGS3926–3935). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petschnigg \BOthers. [\APACyear2004] \APACinsertmetastarpetschnigg2004digital{APACrefauthors}Petschnigg,
    G., Szeliski, R., Agrawala, M., Cohen, M., Hoppe, H.\BCBL Toyama, K. \APACrefYearMonthDay2004.
    \BBOQ\APACrefatitleDigital photography with flash and no-flash image pairs Digital
    photography with flash and no-flash image pairs.\BBCQ \APACjournalVolNumPagesACM
    transactions on graphics (TOG)233664–672, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi \BOthers. [\APACyear2017] \APACinsertmetastarqi2017pointnet{APACrefauthors}Qi,
    C.R., Su, H., Mo, K.\BCBL Guibas, L.J. \APACrefYearMonthDay2017. \BBOQ\APACrefatitlePointnet:
    Deep learning on point sets for 3d classification and segmentation Pointnet: Deep
    learning on point sets for 3d classification and segmentation.\BBCQ \APACrefbtitleProceedings
    of the IEEE conference on computer vision and pattern recognition Proceedings
    of the ieee conference on computer vision and pattern recognition (\BPGS652–660).
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao, Ge, Deng\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2022depth{APACrefauthors}Qiao,
    X., Ge, C., Deng, P., Wei, H., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleDepth Restoration in Under-Display Time-of-Flight Imaging Depth
    restoration in under-display time-of-flight imaging.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence4555668-5683, {APACrefDOI}  [https://doi.org/10.1109/TPAMI.2022.3209905](https://doi.org/10.1109/TPAMI.2022.3209905)
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao \BOthers. [\APACyear2020] \APACinsertmetastarqiao2020valid{APACrefauthors}Qiao,
    X., Ge, C., Yao, H., Deng, P.\BCBL Zhou, Y. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleValid
    depth data extraction and correction for time-of-flight camera Valid depth data
    extraction and correction for time-of-flight camera.\BBCQ \APACrefbtitleTwelfth
    International Conference on Machine Vision (ICMV 2019) Twelfth international conference
    on machine vision (icmv 2019) (\BVOL11433, \BPGS696–703). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao, Ge, Zhang\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2023depth{APACrefauthors}Qiao,
    X., Ge, C., Zhang, Y., Zhou, Y., Tosi, F., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleDepth super-resolution from explicit and implicit high-frequency
    features Depth super-resolution from explicit and implicit high-frequency features.\BBCQ
    \APACjournalVolNumPagesComputer Vision and Image Understanding237103841, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao, Ge, Zhao\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2023self{APACrefauthors}Qiao,
    X., Ge, C., Zhao, C., Tosi, F., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleSelf-supervised depth super-resolution with contrastive multiview
    pre-training Self-supervised depth super-resolution with contrastive multiview
    pre-training.\BBCQ \APACjournalVolNumPagesNeural Networks168223–236, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D. Qiu \BOthers. [\APACyear2019] \APACinsertmetastarqiu2019deep{APACrefauthors}Qiu,
    D., Pang, J., Sun, W.\BCBL Yang, C. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeep
    end-to-end alignment and refinement for time-of-flight RGB-D module Deep end-to-end
    alignment and refinement for time-of-flight rgb-d module.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF International Conference on Computer Vision Proceedings of the
    ieee/cvf international conference on computer vision (\BPGS9994–10003). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'J. Qiu \BOthers. [\APACyear2019] \APACinsertmetastarqiu2019deeplidar{APACrefauthors}Qiu,
    J., Cui, Z., Zhang, Y., Zhang, X., Liu, S., Zeng, B.\BCBL Pollefeys, M. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDeeplidar: Deep surface normal guided depth prediction for
    outdoor scene from sparse lidar data and single color image Deeplidar: Deep surface
    normal guided depth prediction for outdoor scene from sparse lidar data and single
    color image.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Proceedings of the ieee/cvf conference on computer
    vision and pattern recognition (\BPGS3313–3322). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qu \BOthers. [\APACyear2020] \APACinsertmetastarqu2020depth{APACrefauthors}Qu,
    C., Nguyen, T.\BCBL Taylor, C. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDepth
    completion via deep basis fitting Depth completion via deep basis fitting.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision Proceedings of the ieee/cvf winter conference on applications
    of computer vision (\BPGS71–80). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramesh \BOthers. [\APACyear2023] \APACinsertmetastarramesh2023siunet{APACrefauthors}Ramesh,
    A.N., Giovanneschi, F.\BCBL González-Huici, M.A. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSIUNet:
    Sparsity Invariant U-Net for Edge-Aware Depth Completion Siunet: Sparsity invariant
    u-net for edge-aware depth completion.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision Proceedings of the ieee/cvf
    winter conference on applications of computer vision (\BPGS5818–5827). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riegler, Ferstl\BCBL \BOthers. [\APACyear2016] \APACinsertmetastarriegler2016deep{APACrefauthors}Riegler,
    G., Ferstl, D., Rüther, M.\BCBL Horst, B. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleA
    Deep Primal-Dual Network for Guided Depth Super-Resolution A deep primal-dual
    network for guided depth super-resolution.\BBCQ \APACrefbtitleBritish Machine
    Vision Conference. British machine vision conference. \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riegler, Rüther\BCBL \BBA Bischof [\APACyear2016] \APACinsertmetastarriegler2016atgv{APACrefauthors}Riegler,
    G., Rüther, M.\BCBL Bischof, H. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleAtgv-net:
    Accurate depth super-resolution Atgv-net: Accurate depth super-resolution.\BBCQ
    \APACrefbtitleComputer Vision–ECCV 2016: 14th European Conference, Amsterdam,
    The Netherlands, October 11-14, 2016, Proceedings, Part III 14 Computer vision–eccv
    2016: 14th european conference, amsterdam, the netherlands, october 11-14, 2016,
    proceedings, part iii 14 (\BPGS268–284). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Romera \BOthers. [\APACyear2017] \APACinsertmetastarromera2017erfnet{APACrefauthors}Romera,
    E., Alvarez, J.M., Bergasa, L.M.\BCBL Arroyo, R. \APACrefYearMonthDay2017. \BBOQ\APACrefatitleErfnet:
    Efficient residual factorized convnet for real-time semantic segmentation Erfnet:
    Efficient residual factorized convnet for real-time semantic segmentation.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Intelligent Transportation Systems191263–272,
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger \BOthers. [\APACyear2015] \APACinsertmetastarronneberger2015u{APACrefauthors}Ronneberger,
    O., Fischer, P.\BCBL Brox, T. \APACrefYearMonthDay2015. \BBOQ\APACrefatitleU-net:
    Convolutional networks for biomedical image segmentation U-net: Convolutional
    networks for biomedical image segmentation.\BBCQ \APACrefbtitleMedical Image Computing
    and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
    Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 Medical image computing
    and computer-assisted intervention–miccai 2015: 18th international conference,
    munich, germany, october 5-9, 2015, proceedings, part iii 18 (\BPGS234–241). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ryu \BOthers. [\APACyear2021] \APACinsertmetastarryu2021scanline{APACrefauthors}Ryu,
    K., Lee, K\BHBIi., Cho, J.\BCBL Yoon, K\BHBIJ. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleScanline
    resolution-invariant depth completion using a single image and sparse LiDAR point
    cloud Scanline resolution-invariant depth completion using a single image and
    sparse lidar point cloud.\BBCQ \APACjournalVolNumPagesIEEE Robotics and Automation
    Letters646961–6968, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scharstein \BOthers. [\APACyear2014] \APACinsertmetastarscharstein2014high{APACrefauthors}Scharstein,
    D., Hirschmüller, H., Kitajima, Y., Krathwohl, G., Nešić, N., Wang, X.\BCBL Westling,
    P. \APACrefYearMonthDay2014. \BBOQ\APACrefatitleHigh-resolution stereo datasets
    with subpixel-accurate ground truth High-resolution stereo datasets with subpixel-accurate
    ground truth.\BBCQ \APACrefbtitlePattern Recognition: 36th German Conference,
    GCPR 2014, Münster, Germany, September 2-5, 2014, Proceedings 36 Pattern recognition:
    36th german conference, gcpr 2014, münster, germany, september 2-5, 2014, proceedings
    36 (\BPGS31–42). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scharstein \BBA Pal [\APACyear2007] \APACinsertmetastarscharstein2007learning{APACrefauthors}Scharstein,
    D.\BCBT \BBA Pal, C. \APACrefYearMonthDay2007. \BBOQ\APACrefatitleLearning conditional
    random fields for stereo Learning conditional random fields for stereo.\BBCQ \APACrefbtitle2007
    IEEE Conference on Computer Vision and Pattern Recognition 2007 ieee conference
    on computer vision and pattern recognition (\BPGS1–8). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scharstein \BBA Szeliski [\APACyear2003] \APACinsertmetastarscharstein2003high{APACrefauthors}Scharstein,
    D.\BCBT \BBA Szeliski, R. \APACrefYearMonthDay2003. \BBOQ\APACrefatitleHigh-accuracy
    stereo depth maps using structured light High-accuracy stereo depth maps using
    structured light.\BBCQ \APACrefbtitle2003 IEEE Computer Society Conference on
    Computer Vision and Pattern Recognition, 2003\. Proceedings. 2003 ieee computer
    society conference on computer vision and pattern recognition, 2003\. proceedings. (\BVOL1,
    \BPGSI–I). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schuster \BOthers. [\APACyear2021] \APACinsertmetastarschuster2021ssgp{APACrefauthors}Schuster,
    R., Wasenmuller, O., Unger, C.\BCBL Stricker, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSsgp:
    Sparse spatial guided propagation for robust and generic interpolation Ssgp: Sparse
    spatial guided propagation for robust and generic interpolation.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision Proceedings
    of the ieee/cvf winter conference on applications of computer vision (\BPGS197–206).
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shacht \BOthers. [\APACyear2021] \APACinsertmetastarshacht2021single{APACrefauthors}Shacht,
    G., Danon, D., Fogel, S.\BCBL Cohen-Or, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSingle
    pair cross-modality super resolution Single pair cross-modality super resolution.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS6378–6387). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shin \BOthers. [\APACyear2023] \APACinsertmetastarshin2023task{APACrefauthors}Shin,
    J., Shin, S.\BCBL Jeon, H\BHBIG. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleTask-specific
    Scene Structure Representations Task-specific scene structure representations.\BBCQ
    \APACjournalVolNumPagesarXiv preprint arXiv:2301.00555, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shivakumar \BOthers. [\APACyear2019] \APACinsertmetastarshivakumar2019dfusenet{APACrefauthors}Shivakumar,
    S.S., Nguyen, T., Miller, I.D., Chen, S.W., Kumar, V.\BCBL Taylor, C.J. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDfusenet: Deep fusion of rgb and sparse depth information for
    image guided dense depth completion Dfusenet: Deep fusion of rgb and sparse depth
    information for image guided dense depth completion.\BBCQ \APACrefbtitle2019 IEEE
    Intelligent Transportation Systems Conference (ITSC) 2019 ieee intelligent transportation
    systems conference (itsc) (\BPGS13–20). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silberman \BOthers. [\APACyear2012] \APACinsertmetastarsilberman2012indoor{APACrefauthors}Silberman,
    N., Hoiem, D., Kohli, P.\BCBL Fergus, R. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleIndoor
    segmentation and support inference from rgbd images Indoor segmentation and support
    inference from rgbd images.\BBCQ \APACrefbtitleEuropean conference on computer
    vision European conference on computer vision (\BPGS746–760). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P. Song \BOthers. [\APACyear2019] \APACinsertmetastarsong2019multimodal{APACrefauthors}Song,
    P., Deng, X., Mota, J.F., Deligiannis, N., Dragotti, P.L.\BCBL Rodrigues, M.R.
    \APACrefYearMonthDay2019. \BBOQ\APACrefatitleMultimodal image super-resolution
    via joint sparse representations induced by coupled dictionaries Multimodal image
    super-resolution via joint sparse representations induced by coupled dictionaries.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Computational Imaging657–72, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'S. Song \BOthers. [\APACyear2015] \APACinsertmetastarsong2015sun{APACrefauthors}Song,
    S., Lichtenberg, S.P.\BCBL Xiao, J. \APACrefYearMonthDay2015. \BBOQ\APACrefatitleSun
    rgb-d: A rgb-d scene understanding benchmark suite Sun rgb-d: A rgb-d scene understanding
    benchmark suite.\BBCQ \APACrefbtitleProceedings of the IEEE conference on computer
    vision and pattern recognition Proceedings of the ieee conference on computer
    vision and pattern recognition (\BPGS567–576). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X. Song \BOthers. [\APACyear2020] \APACinsertmetastarsong2020channel{APACrefauthors}Song,
    X., Dai, Y., Zhou, D., Liu, L., Li, W., Li, H.\BCBL Yang, R. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleChannel attention based iterative residual learning for depth
    map super-resolution Channel attention based iterative residual learning for depth
    map super-resolution.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS5631–5640). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z. Song \BOthers. [\APACyear2021] \APACinsertmetastarsong2021self{APACrefauthors}Song,
    Z., Lu, J., Yao, Y.\BCBL Zhang, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSelf-supervised
    depth completion from direct visual-LiDAR odometry in autonomous driving Self-supervised
    depth completion from direct visual-lidar odometry in autonomous driving.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Intelligent Transportation Systems23811654–11665,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H. Su \BOthers. [\APACyear2019] \APACinsertmetastarsu2019pixel{APACrefauthors}Su,
    H., Jampani, V., Sun, D., Gallo, O., Learned-Miller, E.\BCBL Kautz, J. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitlePixel-adaptive convolutional neural networks Pixel-adaptive
    convolutional neural networks.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS11166–11175). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Su \BOthers. [\APACyear2018] \APACinsertmetastarsu2018deep{APACrefauthors}Su,
    S., Heide, F., Wetzstein, G.\BCBL Heidrich, W. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleDeep
    end-to-end time-of-flight imaging Deep end-to-end time-of-flight imaging.\BBCQ
    \APACrefbtitleProceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee conference on computer vision and pattern
    recognition (\BPGS6383–6392). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: B. Sun \BOthers. [\APACyear2021] \APACinsertmetastarsun2021learning{APACrefauthors}Sun,
    B., Ye, X., Li, B., Li, H., Wang, Z.\BCBL Xu, R. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleLearning
    scene structure guidance via cross-task knowledge transfer for single depth super-resolution
    Learning scene structure guidance via cross-task knowledge transfer for single
    depth super-resolution.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS7792–7801). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P. Sun \BOthers. [\APACyear2020] \APACinsertmetastarsun2020scalability{APACrefauthors}Sun,
    P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P.\BDBLothers
    \APACrefYearMonthDay2020. \BBOQ\APACrefatitleScalability in perception for autonomous
    driving: Waymo open dataset Scalability in perception for autonomous driving:
    Waymo open dataset.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF conference
    on computer vision and pattern recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS2446–2454). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z. Sun \BOthers. [\APACyear2023] \APACinsertmetastarsun2023consistent{APACrefauthors}Sun,
    Z., Ye, W., Xiong, J., Choe, G., Wang, J., Su, S.\BCBL Ranjan, R. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleConsistent Direct Time-of-Flight Video Depth Super-Resolution
    Consistent direct time-of-flight video depth super-resolution.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS5075–5085).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan \BBA Le [\APACyear2019] \APACinsertmetastartan2019efficientnet{APACrefauthors}Tan,
    M.\BCBT \BBA Le, Q. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleEfficientnet:
    Rethinking model scaling for convolutional neural networks Efficientnet: Rethinking
    model scaling for convolutional neural networks.\BBCQ \APACrefbtitleInternational
    conference on machine learning International conference on machine learning (\BPGS6105–6114).
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Tang \BOthers. [\APACyear2021] \APACinsertmetastartang2021joint{APACrefauthors}Tang,
    J., Chen, X.\BCBL Zeng, G. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleJoint
    implicit image function for guided depth super-resolution Joint implicit image
    function for guided depth super-resolution.\BBCQ \APACrefbtitleProceedings of
    the 29th ACM International Conference on Multimedia Proceedings of the 29th acm
    international conference on multimedia (\BPGS4390–4399). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Tang \BOthers. [\APACyear2020] \APACinsertmetastartang2020learning{APACrefauthors}Tang,
    J., Tian, F\BHBIP., Feng, W., Li, J.\BCBL Tan, P. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleLearning
    guided convolutional network for depth completion Learning guided convolutional
    network for depth completion.\BBCQ \APACjournalVolNumPagesIEEE Transactions on
    Image Processing301116–1129, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Q. Tang \BOthers. [\APACyear2021] \APACinsertmetastartang2021bridgenet{APACrefauthors}Tang,
    Q., Cong, R., Sheng, R., He, L., Zhang, D., Zhao, Y.\BCBL Kwong, S. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleBridgenet: A joint learning network of depth map super-resolution
    and monocular depth estimation Bridgenet: A joint learning network of depth map
    super-resolution and monocular depth estimation.\BBCQ \APACrefbtitleProceedings
    of the 29th ACM International Conference on Multimedia Proceedings of the 29th
    acm international conference on multimedia (\BPGS2148–2157). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tomasi \BBA Manduchi [\APACyear1998] \APACinsertmetastartomasi1998bilateral{APACrefauthors}Tomasi,
    C.\BCBT \BBA Manduchi, R. \APACrefYearMonthDay1998. \BBOQ\APACrefatitleBilateral
    filtering for gray and color images Bilateral filtering for gray and color images.\BBCQ
    \APACrefbtitleSixth international conference on computer vision (IEEE Cat. No.
    98CH36271) Sixth international conference on computer vision (ieee cat. no. 98ch36271) (\BPGS839–846).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uhrig \BOthers. [\APACyear2017] \APACinsertmetastaruhrig2017sparsity{APACrefauthors}Uhrig,
    J., Schneider, N., Schneider, L., Franke, U., Brox, T.\BCBL Geiger, A. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleSparsity invariant cnns Sparsity invariant cnns.\BBCQ \APACrefbtitle2017
    international conference on 3D Vision (3DV) 2017 international conference on 3d
    vision (3dv) (\BPGS11–20). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Gansbeke \BOthers. [\APACyear2019] \APACinsertmetastarvan2019sparse{APACrefauthors}Van Gansbeke,
    W., Neven, D., De Brabandere, B.\BCBL Van Gool, L. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleSparse
    and noisy lidar completion with rgb guidance and uncertainty Sparse and noisy
    lidar completion with rgb guidance and uncertainty.\BBCQ \APACrefbtitle2019 16th
    international conference on machine vision applications (MVA) 2019 16th international
    conference on machine vision applications (mva) (\BPGS1–6). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voynov \BOthers. [\APACyear2019] \APACinsertmetastarvoynov2019perceptual{APACrefauthors}Voynov,
    O., Artemov, A., Egiazarian, V., Notchenko, A., Bobrovskikh, G., Burnaev, E.\BCBL
    Zorin, D. \APACrefYearMonthDay2019. \BBOQ\APACrefatitlePerceptual deep depth super-resolution
    Perceptual deep depth super-resolution.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF International Conference on Computer Vision Proceedings of the ieee/cvf
    international conference on computer vision (\BPGS5653–5663). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan \BOthers. [\APACyear2023] \APACinsertmetastarwan2023seaformer{APACrefauthors}Wan,
    Q., Huang, Z., Lu, J., Yu, G.\BCBL Zhang, L. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSeaFormer:
    Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation Seaformer:
    Squeeze-enhanced axial transformer for mobile semantic segmentation.\BBCQ \APACrefbtitleInternational
    Conference on Learning Representations (ICLR). International conference on learning
    representations (iclr). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H. Wang \BOthers. [\APACyear2022] \APACinsertmetastarwang2022rgb{APACrefauthors}Wang,
    H., Wang, M., Che, Z., Xu, Z., Qiao, X., Qi, M.\BDBLTang, J. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleRgb-depth fusion gan for indoor depth completion Rgb-depth
    fusion gan for indoor depth completion.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings of
    the ieee/cvf conference on computer vision and pattern recognition (\BPGS6209–6218).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Wang \BOthers. [\APACyear2022] \APACinsertmetastarwang2022self{APACrefauthors}Wang,
    J., Liu, P.\BCBL Wen, F. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleSelf-Supervised
    Learning for RGB-Guided Depth Enhancement by Exploiting the Dependency Between
    RGB and Depth Self-supervised learning for rgb-guided depth enhancement by exploiting
    the dependency between rgb and depth.\BBCQ \APACjournalVolNumPagesIEEE Transactions
    on Image Processing32159–174, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Wang \BOthers. [\APACyear2023] \APACinsertmetastarwang2023self{APACrefauthors}Wang,
    J., Liu, P.\BCBL Wen, F. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSelf-Supervised
    Learning for RGB-Guided Depth Enhancement by Exploiting the Dependency Between
    RGB and Depth Self-supervised learning for rgb-guided depth enhancement by exploiting
    the dependency between rgb and depth.\BBCQ \APACjournalVolNumPagesIEEE Transactions
    on Image Processing32159-174, {APACrefDOI}  [https://doi.org/10.1109/TIP.2022.3226419](https://doi.org/10.1109/TIP.2022.3226419)
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K. Wang \BOthers. [\APACyear2023] \APACinsertmetastarwang2023joint{APACrefauthors}Wang,
    K., Zhao, L., Zhang, J., Zhang, J., Wang, A.\BCBL Bai, H. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleJoint depth map super-resolution method via deep hybrid-cross
    guidance filter Joint depth map super-resolution method via deep hybrid-cross
    guidance filter.\BBCQ \APACjournalVolNumPagesPattern Recognition136109260, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X. Wang \BOthers. [\APACyear2022] \APACinsertmetastarwang2022learning{APACrefauthors}Wang,
    X., Chen, X., Ni, B., Tong, Z.\BCBL Wang, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleLearning
    Continuous Depth Representation via Geometric Spatial Aggregator Learning continuous
    depth representation via geometric spatial aggregator.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2212.03499, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y. Wang \BOthers. [\APACyear2019] \APACinsertmetastarwang2019pseudo{APACrefauthors}Wang,
    Y., Chao, W\BHBIL., Garg, D., Hariharan, B., Campbell, M.\BCBL Weinberger, K.Q.
    \APACrefYearMonthDay2019. \BBOQ\APACrefatitlePseudo-lidar from visual depth estimation:
    Bridging the gap in 3d object detection for autonomous driving Pseudo-lidar from
    visual depth estimation: Bridging the gap in 3d object detection for autonomous
    driving.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Proceedings of the ieee/cvf conference on computer
    vision and pattern recognition (\BPGS8445–8453). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y. Wang, Li\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarLRRU_ICCV_2023{APACrefauthors}Wang,
    Y., Li, B., Zhang, G., Liu, Q., Tao, G.\BCBL Dai, Y. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleLRRU: Long-short Range Recurrent Updating Networks for Depth
    Completion Lrru: Long-short range recurrent updating networks for depth completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE International Conference on Computer Vision
    (ICCV) Proceedings of the ieee international conference on computer vision (iccv) (\BPG9422-9432).
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Wang, Yang\BCBL \BBA Yue [\APACyear2023] \APACinsertmetastarwang2023depth{APACrefauthors}Wang,
    Y., Yang, J.\BCBL Yue, H. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleDepth map
    continuous super-resolution with local implicit guidance function Depth map continuous
    super-resolution with local implicit guidance function.\BBCQ \APACjournalVolNumPagesDisplays78102418,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z. Wang \BOthers. [\APACyear2020] \APACinsertmetastarwang2020depth{APACrefauthors}Wang,
    Z., Ye, X., Sun, B., Yang, J., Xu, R.\BCBL Li, H. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDepth
    upsampling based on deep edge-aware learning Depth upsampling based on deep edge-aware
    learning.\BBCQ \APACjournalVolNumPagesPattern Recognition103107274, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weickert \BOthers. [\APACyear1998] \APACinsertmetastarweickert1998anisotropic{APACrefauthors}Weickert,
    J.\BCBT \BOthersPeriod. \APACrefYear1998. \APACrefbtitleAnisotropic diffusion
    in image processing Anisotropic diffusion in image processing (\BVOL1). \APACaddressPublisherTeubner
    Stuttgart. \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen \BOthers. [\APACyear2019] \APACinsertmetastarwen2018deep{APACrefauthors}Wen,
    Y., Sheng, B., Li, P., Lin, W.\BCBL Feng, D.D. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeep
    Color Guided Coarse-to-Fine Convolutional Network Cascade for Depth Image Super-Resolution
    Deep color guided coarse-to-fine convolutional network cascade for depth image
    super-resolution.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image Processing282994-1006,
    {APACrefDOI}  [https://doi.org/10.1109/TIP.2018.2874285](https://doi.org/10.1109/TIP.2018.2874285)
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong \BOthers. [\APACyear2021] \APACinsertmetastarwong2021learning{APACrefauthors}Wong,
    A., Cicek, S.\BCBL Soatto, S. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleLearning
    topology from synthetic data for unsupervised depth completion Learning topology
    from synthetic data for unsupervised depth completion.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters621495–1502, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong \BOthers. [\APACyear2020] \APACinsertmetastarwong2020unsupervised{APACrefauthors}Wong,
    A., Fei, X., Tsuei, S.\BCBL Soatto, S. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleUnsupervised
    depth completion from visual inertial odometry Unsupervised depth completion from
    visual inertial odometry.\BBCQ \APACjournalVolNumPagesIEEE Robotics and Automation
    Letters521899–1906, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong \BBA Soatto [\APACyear2021] \APACinsertmetastarwong2021unsupervised{APACrefauthors}Wong,
    A.\BCBT \BBA Soatto, S. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleUnsupervised
    depth completion with calibrated backprojection layers Unsupervised depth completion
    with calibrated backprojection layers.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    International Conference on Computer Vision Proceedings of the ieee/cvf international
    conference on computer vision (\BPGS12747–12756). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wronski \BOthers. [\APACyear2019] \APACinsertmetastarwronski2019handheld{APACrefauthors}Wronski,
    B., Garcia-Dorado, I., Ernst, M., Kelly, D., Krainin, M., Liang, C\BHBIK.\BDBLMilanfar,
    P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleHandheld multi-frame super-resolution
    Handheld multi-frame super-resolution.\BBCQ \APACjournalVolNumPagesACM Transactions
    on Graphics (ToG)3841–18, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu \BOthers. [\APACyear2022] \APACinsertmetastarwu2022sparse{APACrefauthors}Wu,
    X., Peng, L., Yang, H., Xie, L., Huang, C., Deng, C.\BDBLCai, D. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleSparse fuse dense: Towards high quality 3d detection with depth
    completion Sparse fuse dense: Towards high quality 3d detection with depth completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS5418–5427). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia \BOthers. [\APACyear2018] \APACinsertmetastarxia2018gibson{APACrefauthors}Xia,
    F., Zamir, A.R., He, Z., Sax, A., Malik, J.\BCBL Savarese, S. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitleGibson env: Real-world perception for embodied agents Gibson
    env: Real-world perception for embodied agents.\BBCQ \APACrefbtitleProceedings
    of the IEEE conference on computer vision and pattern recognition Proceedings
    of the ieee conference on computer vision and pattern recognition (\BPGS9068–9079).
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie \BOthers. [\APACyear2022] \APACinsertmetastarsurvey_completion_1{APACrefauthors}Xie,
    Z., Yu, X., Gao, X., Li, K.\BCBL Shen, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleRecent
    Advances in Conventional and Deep Learning-Based Depth Completion: A Survey Recent
    advances in conventional and deep learning-based depth completion: A survey.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Neural Networks and Learning Systems1-21,
    {APACrefDOI}  [https://doi.org/10.1109/TNNLS.2022.3201534](https://doi.org/10.1109/TNNLS.2022.3201534)
    \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: J. Xu \BOthers. [\APACyear2023] \APACinsertmetastarxu2023real{APACrefauthors}Xu,
    J., Zhu, Y., Wang, W.\BCBL Liu, G. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleA
    real-time semi-dense depth-guided depth completion network A real-time semi-dense
    depth-guided depth completion network.\BBCQ \APACjournalVolNumPagesThe Visual
    Computer1–11, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L. Xu \BOthers. [\APACyear2022] \APACinsertmetastarxu2022self{APACrefauthors}Xu,
    L., Guan, T., Wang, Y., Luo, Y., Chen, Z., Liu, W.\BCBL Yang, W. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleSelf-Supervised Multi-view Stereo via Adjacent Geometry Guided
    Volume Completion Self-supervised multi-view stereo via adjacent geometry guided
    volume completion.\BBCQ \APACrefbtitleProceedings of the 30th ACM International
    Conference on Multimedia Proceedings of the 30th acm international conference
    on multimedia (\BPGS2202–2210). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Xu \BOthers. [\APACyear2019] \APACinsertmetastarxu2019depth{APACrefauthors}Xu,
    Y., Zhu, X., Shi, J., Zhang, G., Bao, H.\BCBL Li, H. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDepth completion from sparse lidar data with depth-normal constraints
    Depth completion from sparse lidar data with depth-normal constraints.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF International Conference on Computer Vision Proceedings of the
    ieee/cvf international conference on computer vision (\BPGS2811–2820). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z. Xu \BOthers. [\APACyear2020] \APACinsertmetastarxu2020deformable{APACrefauthors}Xu,
    Z., Yin, H.\BCBL Yao, J. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDeformable
    spatial propagation networks for depth completion Deformable spatial propagation
    networks for depth completion.\BBCQ \APACrefbtitle2020 IEEE International Conference
    on Image Processing (ICIP) 2020 ieee international conference on image processing
    (icip) (\BPGS913–917). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L. Yan \BOthers. [\APACyear2020] \APACinsertmetastaryan2020revisiting{APACrefauthors}Yan,
    L., Liu, K.\BCBL Belyaev, E. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleRevisiting
    sparsity invariant convolution: A network for image guided depth completion Revisiting
    sparsity invariant convolution: A network for image guided depth completion.\BBCQ
    \APACjournalVolNumPagesIEEE Access8126323–126332, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Z. Yan \BOthers. [\APACyear2022] \APACinsertmetastaryan2022rignet{APACrefauthors}Yan,
    Z., Wang, K., Li, X., Zhang, Z., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleRigNet: Repetitive image guided network for depth completion
    Rignet: Repetitive image guided network for depth completion.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXVII Computer vision–eccv 2022: 17th european conference, tel
    aviv, israel, october 23–27, 2022, proceedings, part xxvii (\BPGS214–230). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A. Yang \BBA Sankaranarayanan [\APACyear2021] \APACinsertmetastaryang2021designing{APACrefauthors}Yang,
    A.\BCBT \BBA Sankaranarayanan, A.C. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDesigning
    display pixel layouts for under-panel cameras Designing display pixel layouts
    for under-panel cameras.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Pattern
    Analysis and Machine Intelligence4372245–2256, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Yang \BOthers. [\APACyear2019] \APACinsertmetastaryang2019dense{APACrefauthors}Yang,
    Y., Wong, A.\BCBL Soatto, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDense
    depth posterior (ddp) from single image and sparse range Dense depth posterior
    (ddp) from single image and sparse range.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings of
    the ieee/cvf conference on computer vision and pattern recognition (\BPGS3353–3362).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye \BOthers. [\APACyear2020] \APACinsertmetastarye2020pmbanet{APACrefauthors}Ye,
    X., Sun, B., Wang, Z., Yang, J., Xu, R., Li, H.\BCBL Li, B. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitlePMBANet: Progressive multi-branch aggregation network for scene
    depth super-resolution Pmbanet: Progressive multi-branch aggregation network for
    scene depth super-resolution.\BBCQ \APACjournalVolNumPagesIEEE Transactions on
    Image Processing297427–7442, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q. Yu \BOthers. [\APACyear2021] \APACinsertmetastaryu2021grayscale{APACrefauthors}Yu,
    Q., Chu, L., Wu, Q.\BCBL Pei, L. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleGrayscale
    and normal guided depth completion with a low-cost lidar Grayscale and normal
    guided depth completion with a low-cost lidar.\BBCQ \APACrefbtitle2021 IEEE International
    Conference on Image Processing (ICIP) 2021 ieee international conference on image
    processing (icip) (\BPGS979–983). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z. Yu \BOthers. [\APACyear2023] \APACinsertmetastaryu2023aggregating{APACrefauthors}Yu,
    Z., Sheng, Z., Zhou, Z., Luo, L., Cao, S\BHBIY., Gu, H.\BDBLShen, H\BHBIL. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleAggregating Feature Point Cloud for Depth Completion Aggregating
    feature point cloud for depth completion.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF International Conference on Computer Vision Proceedings of the ieee/cvf
    international conference on computer vision (\BPGS8732–8743). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan \BOthers. [\APACyear2023\APACexlab\BCnt1] \APACinsertmetastaryuan2023recurrent{APACrefauthors}Yuan,
    J., Jiang, H., Li, X., Qian, J., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2023\BCnt1.
    \BBOQ\APACrefatitleRecurrent Structure Attention Guidance for Depth Super-Resolution
    Recurrent structure attention guidance for depth super-resolution.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2301.13419, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan \BOthers. [\APACyear2023\APACexlab\BCnt2] \APACinsertmetastaryuan2023structure{APACrefauthors}Yuan,
    J., Jiang, H., Li, X., Qian, J., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2023\BCnt2.
    \BBOQ\APACrefatitleStructure Flow-Guided Network for Real Depth Super-Resolution
    Structure flow-guided network for real depth super-resolution.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2301.13416, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C. Zhang \BOthers. [\APACyear2021] \APACinsertmetastarzhang2021multitask{APACrefauthors}Zhang,
    C., Tang, Y., Zhao, C., Sun, Q., Ye, Z.\BCBL Kurths, J. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMultitask gans for semantic segmentation and depth completion
    with cycle consistency Multitask gans for semantic segmentation and depth completion
    with cycle consistency.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Neural
    Networks and Learning Systems32125404–5415, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q. Zhang \BOthers. [\APACyear2022] \APACinsertmetastarzhang2022self{APACrefauthors}Zhang,
    Q., Chen, X., Wang, X., Han, J., Zhang, Y.\BCBL Yue, J. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleSelf-Supervised Depth Completion Based on Multi-Modal Spatio-Temporal
    Consistency Self-supervised depth completion based on multi-modal spatio-temporal
    consistency.\BBCQ \APACjournalVolNumPagesRemote Sensing151135, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Zhang \BBA Funkhouser [\APACyear2018] \APACinsertmetastarzhang2018deep{APACrefauthors}Zhang,
    Y.\BCBT \BBA Funkhouser, T. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleDeep
    depth completion of a single rgb-d image Deep depth completion of a single rgb-d
    image.\BBCQ \APACrefbtitleProceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition Proceedings of the ieee conference on computer vision
    and pattern recognition (\BPGS175–185). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y. Zhang \BOthers. [\APACyear2023] \APACinsertmetastarZhang2023CompletionFormer{APACrefauthors}Zhang,
    Y., Guo, X., Poggi, M., Zhu, Z., Huang, G.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleCompletionFormer: Depth Completion with Convolutions and Vision
    Transformers Completionformer: Depth completion with convolutions and vision transformers.\BBCQ
    \APACrefbtitleCVPR. Cvpr. \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Zhang \BBA Yang [\APACyear2021] \APACinsertmetastarzhang2021survey{APACrefauthors}Zhang,
    Y.\BCBT \BBA Yang, Q. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleA survey on
    multi-task learning A survey on multi-task learning.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Knowledge and Data Engineering34125586–5609, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C. Zhao \BOthers. [\APACyear2020] \APACinsertmetastarzhao2020masked{APACrefauthors}Zhao,
    C., Yen, G.G., Sun, Q., Zhang, C.\BCBL Tang, Y. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleMasked
    GAN for unsupervised depth and pose prediction with scale consistency Masked gan
    for unsupervised depth and pose prediction with scale consistency.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Neural Networks and Learning Systems32125392–5403, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Zhao \BOthers. [\APACyear2021] \APACinsertmetastarzhao2021adaptive{APACrefauthors}Zhao,
    S., Gong, M., Fu, H.\BCBL Tao, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleAdaptive
    context-aware multi-modal network for depth completion Adaptive context-aware
    multi-modal network for depth completion.\BBCQ \APACjournalVolNumPagesIEEE Transactions
    on Image Processing305264–5276, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z. Zhao \BOthers. [\APACyear2023] \APACinsertmetastarzhao2023spherical{APACrefauthors}Zhao,
    Z., Zhang, J., Gu, X., Tan, C., Xu, S., Zhang, Y.\BDBLVan Gool, L. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleSpherical space feature decomposition for guided depth map
    super-resolution Spherical space feature decomposition for guided depth map super-resolution.\BBCQ
    \APACjournalVolNumPagesProceedings of the IEEE International Conference on Computer
    Vision (ICCV)12547-12558, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z. Zhao \BOthers. [\APACyear2022] \APACinsertmetastarzhao2022discrete{APACrefauthors}Zhao,
    Z., Zhang, J., Xu, S., Lin, Z.\BCBL Pfister, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleDiscrete
    cosine transform network for guided depth map super-resolution Discrete cosine
    transform network for guided depth map super-resolution.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS5697–5707).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong \BOthers. [\APACyear2021] \APACinsertmetastarzhong2021high{APACrefauthors}Zhong,
    Z., Liu, X., Jiang, J., Zhao, D., Chen, Z.\BCBL Ji, X. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleHigh-resolution depth maps imaging via attention-based hierarchical
    multi-modal fusion High-resolution depth maps imaging via attention-based hierarchical
    multi-modal fusion.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image Processing31648–663,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhong \BOthers. [\APACyear2023\APACexlab\BCnt1] \APACinsertmetastarzhong2023deep{APACrefauthors}Zhong,
    Z., Liu, X., Jiang, J., Zhao, D.\BCBL Ji, X. \APACrefYearMonthDay2023\BCnt1. \BBOQ\APACrefatitleDeep
    attentional guided image filtering Deep attentional guided image filtering.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Neural Networks and Learning Systems,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong \BOthers. [\APACyear2023\APACexlab\BCnt2] \APACinsertmetastarzhong2023guided{APACrefauthors}Zhong,
    Z., Liu, X., Jiang, J., Zhao, D.\BCBL Ji, X. \APACrefYearMonthDay2023\BCnt2. \BBOQ\APACrefatitleGuided
    Depth Map Super-resolution: A Survey Guided depth map super-resolution: A survey.\BBCQ
    \APACjournalVolNumPagesACM Computing Surveys, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: M. Zhou \BOthers. [\APACyear2023] \APACinsertmetastarzhou2023memory{APACrefauthors}Zhou,
    M., Yan, K., Pan, J., Ren, W., Xie, Q.\BCBL Cao, X. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleMemory-augmented deep unfolding network for guided image super-resolution
    Memory-augmented deep unfolding network for guided image super-resolution.\BBCQ
    \APACjournalVolNumPagesInternational Journal of Computer Vision1311215–242, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T. Zhou \BOthers. [\APACyear2017] \APACinsertmetastarzhou2017unsupervised{APACrefauthors}Zhou,
    T., Brown, M., Snavely, N.\BCBL Lowe, D.G. \APACrefYearMonthDay2017. \BBOQ\APACrefatitleUnsupervised
    learning of depth and ego-motion from video Unsupervised learning of depth and
    ego-motion from video.\BBCQ \APACrefbtitleProceedings of the IEEE conference on
    computer vision and pattern recognition Proceedings of the ieee conference on
    computer vision and pattern recognition (\BPGS1851–1858). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y. Zhou \BOthers. [\APACyear2020] \APACinsertmetastarzhou2020udc{APACrefauthors}Zhou,
    Y., Kwan, M., Tolentino, K., Emerton, N., Lim, S., Large, T.\BDBLothers \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleUDC 2020 challenge on image restoration of under-display camera:
    Methods and results Udc 2020 challenge on image restoration of under-display camera:
    Methods and results.\BBCQ \APACrefbtitleComputer Vision–ECCV 2020 Workshops: Glasgow,
    UK, August 23–28, 2020, Proceedings, Part V 16 Computer vision–eccv 2020 workshops:
    Glasgow, uk, august 23–28, 2020, proceedings, part v 16 (\BPGS337–351). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Zhou \BOthers. [\APACyear2021] \APACinsertmetastarzhou2021image{APACrefauthors}Zhou,
    Y., Ren, D., Emerton, N., Lim, S.\BCBL Large, T. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleImage
    restoration for under-display camera Image restoration for under-display camera.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS9179–9188). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'X. Zhu \BOthers. [\APACyear2019] \APACinsertmetastarzhu2019deformable{APACrefauthors}Zhu,
    X., Hu, H., Lin, S.\BCBL Dai, J. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeformable
    convnets v2: More deformable, better results Deformable convnets v2: More deformable,
    better results.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF conference on computer
    vision and pattern recognition Proceedings of the ieee/cvf conference on computer
    vision and pattern recognition (\BPGS9308–9316). \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. Zhu \BOthers. [\APACyear2022] \APACinsertmetastarzhu2022robust{APACrefauthors}Zhu,
    Y., Dong, W., Li, L., Wu, J., Li, X.\BCBL Shi, G. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleRobust
    depth completion with uncertainty-driven loss functions Robust depth completion
    with uncertainty-driven loss functions.\BBCQ \APACrefbtitleProceedings of the
    AAAI Conference on Artificial Intelligence Proceedings of the aaai conference
    on artificial intelligence (\BVOL36, \BPGS3626–3634). \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph \BOthers. [\APACyear2018] \APACinsertmetastarzoph2018learning{APACrefauthors}Zoph,
    B., Vasudevan, V., Shlens, J.\BCBL Le, Q.V. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleLearning
    transferable architectures for scalable image recognition Learning transferable
    architectures for scalable image recognition.\BBCQ \APACrefbtitleProceedings of
    the IEEE conference on computer vision and pattern recognition Proceedings of
    the ieee conference on computer vision and pattern recognition (\BPGS8697–8710).
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou \BOthers. [\APACyear2020] \APACinsertmetastarzou2020simultaneous{APACrefauthors}Zou,
    N., Xiang, Z., Chen, Y., Chen, S.\BCBL Qiao, C. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleSimultaneous
    semantic segmentation and depth completion with constraint of boundary Simultaneous
    semantic segmentation and depth completion with constraint of boundary.\BBCQ \APACjournalVolNumPagesSensors203635,
    \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zuo \BOthers. [\APACyear2020] \APACinsertmetastarzuo2020frequency{APACrefauthors}Zuo,
    Y., Fang, Y., An, P., Shang, X.\BCBL Yang, J. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleFrequency-dependent
    depth map enhancement via iterative depth-guided affine transformation and intensity-guided
    refinement Frequency-dependent depth map enhancement via iterative depth-guided
    affine transformation and intensity-guided refinement.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Multimedia23772–783, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zuo, Fang\BCBL \BOthers. [\APACyear2019] \APACinsertmetastarzuo2019depth{APACrefauthors}Zuo,
    Y., Fang, Y., Yang, Y., Shang, X.\BCBL Wu, Q. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDepth
    map enhancement by revisiting multi-scale intensity guidance within coarse-to-fine
    stages Depth map enhancement by revisiting multi-scale intensity guidance within
    coarse-to-fine stages.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Circuits
    and Systems for Video Technology30124676–4687, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zuo \BOthers. [\APACyear2021] \APACinsertmetastarzuo2021mig{APACrefauthors}Zuo,
    Y., Wang, H., Fang, Y., Huang, X., Shang, X.\BCBL Wu, Q. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMIG-net: Multi-scale network alternatively guided by intensity
    and gradient features for depth map super-resolution Mig-net: Multi-scale network
    alternatively guided by intensity and gradient features for depth map super-resolution.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Multimedia243506–3519, \PrintBackRefs\CurrentBib'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zuo, Wu\BCBL \BOthers. [\APACyear2019] \APACinsertmetastarzuo2019multi{APACrefauthors}Zuo,
    Y., Wu, Q., Fang, Y., An, P., Huang, L.\BCBL Chen, Z. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleMulti-scale frequency reconstruction for guided depth map super-resolution
    via deep residual network Multi-scale frequency reconstruction for guided depth
    map super-resolution via deep residual network.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Circuits and Systems for Video Technology302297–306, \PrintBackRefs\CurrentBib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
