- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:59:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2009.08920] Deep Learning for 3D Point Cloud Understanding: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2009.08920] 深度学习在3D点云理解中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.08920](https://ar5iv.labs.arxiv.org/html/2009.08920)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2009.08920](https://ar5iv.labs.arxiv.org/html/2009.08920)
- en: 'Deep Learning for 3D Point Cloud Understanding: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在3D点云理解中的应用：综述
- en: Haoming Lu¹, Humphrey Shi^(2,1)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Haoming Lu¹, Humphrey Shi^(2,1)
- en: ¹University of Illinois at Urbana-Champaign, ²University of Oregon
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹伊利诺伊大学厄尔巴纳-香槟分校，²俄勒冈大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The development of practical applications, such as autonomous driving and robotics,
    has brought increasing attention to 3D point cloud understanding. While deep learning
    has achieved remarkable success on image-based tasks, there are many unique challenges
    faced by deep neural networks in processing massive, unstructured and noisy 3D
    points. To demonstrate the latest progress of deep learning for 3D point cloud
    understanding, this paper summarizes recent remarkable research contributions
    in this area from several different directions (classification, segmentation,
    detection, tracking, flow estimation, registration, augmentation and completion),
    together with commonly used datasets, metrics and state-of-the-art performances.
    More information regarding this survey can be found at: [https://github.com/SHI-Labs/3D-Point-Cloud-Learning](https://github.com/SHI-Labs/3D-Point-Cloud-Learning).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用的发展，例如自动驾驶和机器人技术，越来越多地引起了对3D点云理解的关注。虽然深度学习在基于图像的任务中取得了显著成功，但深度神经网络在处理大量、无结构和噪声3D点时面临许多独特的挑战。为了展示深度学习在3D点云理解方面的最新进展，本文总结了该领域最近的显著研究贡献，包括多个不同方向（分类、分割、检测、跟踪、流估计、配准、增强和补全），以及常用的数据集、指标和最新性能。有关此调查的更多信息，请访问：[https://github.com/SHI-Labs/3D-Point-Cloud-Learning](https://github.com/SHI-Labs/3D-Point-Cloud-Learning)。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Deep learning has shown outstanding performance in a wide range of computer
    vision tasks in the past years, especially image tasks. Meanwhile, in many practical
    applications, such as autonomous vehicles (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning for 3D Point Cloud Understanding: A Survey") shows a point cloud
    collected by an autonomous vehicle), we need more information than only images
    to obtain a better sense of the environment. 3D data from lidar or RGB-D cameras
    are considered to be a good supplement here. These devices generate 3D geometric
    data in the form of point clouds. With the growing demand from industry, utilization
    of point clouds with deep learning models is becoming a research hotspot recently.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，深度学习在广泛的计算机视觉任务中表现出色，特别是在图像任务方面。与此同时，在许多实际应用中，例如自动驾驶车辆（图 [1](#S1.F1 "图
    1 ‣ 1 引言 ‣ 深度学习在3D点云理解中的应用：综述") 展示了由自动驾驶车辆收集的点云），我们需要比仅仅图像更多的信息来获得对环境的更好理解。来自激光雷达或RGB-D相机的3D数据被认为是一个很好的补充。这些设备生成以点云形式呈现的3D几何数据。随着工业需求的增加，使用深度学习模型处理点云最近成为了研究热点。
- en: '![Refer to caption](img/d24ccbcc55aeb03598d5b8c45e08e872.png)![Refer to caption](img/6818d18ae59b9f5035168e10fb90b9c7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d24ccbcc55aeb03598d5b8c45e08e872.png)![参考说明](img/6818d18ae59b9f5035168e10fb90b9c7.png)'
- en: 'Figure 1: Point cloud data collected from outdoor scene, shown from two distinct
    angles.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：从户外场景中收集的点云数据，从两个不同的角度展示。
- en: 'In constrast to image data, point clouds do not directly contain spatial structure,
    and deep models on point clouds must therefore solve three main problems: (1)
    how to find a representation of high information density from a sparse point cloud,
    (2) how to build a network satisfying necessary restrictions like size-variance
    and permutation-invariance, (3) how to process large volumes of data with lower
    time and computing resource consumption. PointNet [[79](#bib.bib79)] is one of
    the representative early attempts to design a novel deep network for comsumption
    of unordered 3D point sets by taking advantage of MLP and T-Net. PointNet, together
    with its improved version PointNet++ [[80](#bib.bib80)], inspired a lot of follow-up
    works.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像数据相比，点云不直接包含空间结构，因此点云上的深度模型必须解决三个主要问题：（1）如何从稀疏的点云中找到高信息密度的表示，（2）如何建立满足必要限制条件（如大小不变性和排列不变性）的网络，（3）如何以较低的时间和计算资源消耗处理大量数据。PointNet
    [[79](#bib.bib79)] 是早期尝试设计新型深度网络来处理无序3D点集的代表性作品之一，它利用了MLP和T-Net。PointNet及其改进版PointNet++
    [[80](#bib.bib80)] 启发了许多后续研究工作。
- en: 'Fundamental tasks in images, such as classification, segmentation and object
    detection also exist in point clouds. Most solutions to these problems benefit
    from research findings on the image side, while adequate adaptions are inevitable
    to suit the characteristics of 3D data. In this paper, recent works on point clouds
    are divided into the following categories: classification, segmentation, detection,
    matching and registration, augmentation, completion and reconstruction. Detailed
    descriptions of each category will be provided in the following sections.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的基本任务，如分类、分割和物体检测，在点云中也存在。大多数解决这些问题的方案都受益于图像领域的研究成果，同时适应3D数据特性的调整是不可避免的。本文将最近的点云研究分为以下几类：分类、分割、检测、匹配和配准、增强、补全和重建。每一类的详细描述将在接下来的章节中提供。
- en: A growing number of datasets are available for different tasks on point clouds.
    ShapeNet [[5](#bib.bib5)] and ModelNet [[113](#bib.bib113)] are two early datasets
    consisting of clean 3D models. These early datasets suffer from the lack of generalization.
    However, it is necessary to consider disturbance including noise and missing points
    to develop robust models. With that in mind, datasets such as ScanNet [[10](#bib.bib10)]
    and KITTI [[22](#bib.bib22)] are then created from scans of the actual environment.
    Datasets designed for autonomous vehicle tasks, like nuScenes [[4](#bib.bib4)]
    and Lyft [[44](#bib.bib44)], are further generalized by involving various environments
    at different times. Currently, ever more datasets are being proposed in order
    to meet the increasing demands of distinct niches.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的数据集数量不断增加，满足不同的点云任务需求。ShapeNet [[5](#bib.bib5)] 和 ModelNet [[113](#bib.bib113)]
    是两个早期的数据集，包含干净的3D模型。这些早期数据集存在泛化不足的问题。然而，开发稳健的模型时必须考虑噪声和缺失点等干扰因素。考虑到这一点，ScanNet
    [[10](#bib.bib10)] 和 KITTI [[22](#bib.bib22)] 等数据集随后从实际环境的扫描中创建。设计用于自动驾驶任务的数据集，如nuScenes
    [[4](#bib.bib4)] 和 Lyft [[44](#bib.bib44)]，通过涉及不同时间的各种环境进一步进行泛化。目前，为满足不同领域日益增长的需求，越来越多的数据集被提出。
- en: The structure of this paper is as follows. Section 2 introduces existing 3D
    datasets and corresponding metrics for different tasks. Section 3 includes a survey
    of 3D shape classification methods. Section 4 reviews methods for 3D semantic
    segmentation and instance segmentation. Section 5 presents a survey of methods
    for 3D object detection and its derivative task. Section 6 introduces recent progress
    in 3D point cloud matching and registration. Section 7 provides a review of methods
    to improve data quality. Finally, section 8 concludes the paper.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构如下。第2节介绍了现有的3D数据集和针对不同任务的相应指标。第3节包括了3D形状分类方法的综述。第4节回顾了3D语义分割和实例分割的方法。第5节介绍了3D物体检测及其衍生任务的方法综述。第6节介绍了3D点云匹配和配准的最新进展。第7节提供了提高数据质量的方法综述。最后，第8节对本文进行了总结。
- en: 2 Datasets and metrics
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据集和指标
- en: Datasets are of great importance in deep learning methods for 3D point cloud
    data. First, well-designed datasets provide convictive evaluation and comparison
    among different algorithms. Second, datasets with richer content and metadata
    help define more complicated tasks and raise new research topics. In this section,
    we will briefly introduce some most commonly used datasets and evaluation metrics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在3D点云数据的深度学习方法中具有重要意义。首先，设计良好的数据集提供了不同算法之间的有说服力的评估和比较。其次，具有丰富内容和元数据的数据集有助于定义更复杂的任务并提出新的研究课题。在本节中，我们将简要介绍一些常用的数据集和评估指标。
- en: 'Table 1: Commonly used 3D point cloud datasets in recent works'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：近期工作中常用的3D点云数据集
- en: '| Dataset | Task | Classes | Scale | Feature | Year |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 任务 | 类别 | 规模 | 特征 | 年份 |'
- en: '| ShapeNet [[5](#bib.bib5)] | Classification | 55 | 51300 models | The categories
    are selected according to WordNet [[70](#bib.bib70)] synset. | 2015 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet [[5](#bib.bib5)] | 分类 | 55 | 51300模型 | 类别根据WordNet [[70](#bib.bib70)]
    词汇集选择。 | 2015 |'
- en: '| ModelNet40 [[113](#bib.bib113)] | Classification | 40 | 12311 models | The
    models are collected with online search engines by querying for each established
    object category. | 2015 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet40 [[113](#bib.bib113)] | 分类 | 40 | 12311模型 | 模型通过在线搜索引擎按照每个既定物体类别查询收集。
    | 2015 |'
- en: '| S3DIS [[1](#bib.bib1)] | Segmentation | 12 | 215 million points | Points
    are collected in 5 large-scale indoor scenes from 3 different buildings. | 2016
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| S3DIS [[1](#bib.bib1)] | 分割 | 12 | 2.15亿点 | 点数据收集于来自3栋不同建筑的5个大规模室内场景中。 |
    2016 |'
- en: '| Semantic3D [[30](#bib.bib30)] | Segmentation | 8 | 4 billion points | Hand-labelled
    from a range of diverse urban scenes. | 2017 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Semantic3D [[30](#bib.bib30)] | 分割 | 8 | 40亿点 | 从各种城市场景中手动标记。 | 2017 |'
- en: '| ScanNet [[10](#bib.bib10)] | Segmentation | 20 | 2.5 million frames | Collected
    with a scalable RGB-D capture system with automated surface reconstruction and
    crowdsourced semantic annotation. | 2017 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet [[10](#bib.bib10)] | 分割 | 20 | 250万帧 | 使用具有自动表面重建和众包语义注释的可扩展RGB-D捕捉系统收集。
    | 2017 |'
- en: '| KITTI [[22](#bib.bib22), [23](#bib.bib23), [21](#bib.bib21), [69](#bib.bib69)]
    | Detection Tracking | 3 | 80256 objects | Captured by a standard station wagon
    equipped with two cameras, a Velodyne laser scanner and a GPS localization system
    driving in different outdoor scenes. | 2012 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| KITTI [[22](#bib.bib22), [23](#bib.bib23), [21](#bib.bib21), [69](#bib.bib69)]
    | 检测跟踪 | 3 | 80256个对象 | 通过配备两台摄像头、一台Velodyne激光扫描仪和一套GPS定位系统的标准旅行车在不同户外场景中捕获。 |
    2012 |'
- en: '| nuScenes [[4](#bib.bib4)] | Detection Tracking | 23 | 1.4M objects | Captured
    with full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS); 1000 scenes
    of 20s each. | 2019 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| nuScenes [[4](#bib.bib4)] | 检测跟踪 | 23 | 1.4M个对象 | 使用全套传感器（1x LIDAR，5x RADAR，6x摄像头，IMU，GPS）捕获；1000个场景，每个场景20秒。
    | 2019 |'
- en: '| Waymo Open Dataset [[95](#bib.bib95)] | Detection Tracking | 4 | 12.6M objects
    with tracking ID | Captured with 1 mid-range lidar, 4 short-range lidars and 5
    cameras (front and sides); 1,950 segments of 20s each, collected at 10Hz. | 2019
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Waymo Open Dataset [[95](#bib.bib95)] | 检测跟踪 | 4 | 12.6M带跟踪ID的对象 | 通过1个中等范围激光雷达、4个短距离激光雷达和5个摄像头（前置和侧面）捕获；共1950段，每段20秒，以10Hz频率收集。
    | 2019 |'
- en: 2.1 Datasets
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据集
- en: 'Table [1](#S2.T1 "Table 1 ‣ 2 Datasets and metrics ‣ Deep Learning for 3D Point
    Cloud Understanding: A Survey") shows the most commonly used 3D point cloud datasets
    for three matured tasks (classification, segmentation and detection), which will
    be mentioned often in the following sections. We will also introduce each of them
    with more details.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S2.T1 "表1 ‣ 2 数据集和指标 ‣ 深度学习在3D点云理解中的应用：综述")展示了用于三种成熟任务（分类、分割和检测）中最常用的3D点云数据集，这些数据集将在以下章节中经常提到。我们还将详细介绍每个数据集。
- en: ShapeNet ShapeNet [[5](#bib.bib5)] is a rich-annotated dataset with 51300 3D
    models in 55 categories. It consists of several subsets. ShapeNetSem, which is
    one of the subsets, contains 12000 models spread over a broader set of 270 categories.
    This dataset, together with ModelNet40 [[113](#bib.bib113)], are relatively clean
    and small, so they are usually used to evaluate the capacity of backbones before
    applied to more complicated tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ShapeNet ShapeNet [[5](#bib.bib5)] 是一个注释丰富的数据集，包含55类中的51300个3D模型。它由几个子集组成。ShapeNetSem是其中一个子集，包含12000个模型，分布在更广泛的270个类别中。这个数据集与ModelNet40
    [[113](#bib.bib113)] 一起，由于其相对干净和小型，通常用于在应用于更复杂任务之前评估骨干网的能力。
- en: 'ModelNet40 The ModelNet [[113](#bib.bib113)] project provides three benchmarks:
    ModelNet10, ModelNet40 and Aligned40\. The ModelNet40 benchmark, where “40” indicates
    the number of classes, is the most widely used. To find the most common object
    categories, the statistics obtained from the SUN database [[115](#bib.bib115)]
    are utilized. After establishing the vocabulary, 3D CAD models are collected with
    online search engines and verified by human workers.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ModelNet项目提供了三个基准：ModelNet10、ModelNet40和Aligned40，其中“40”表示类别数量，ModelNet40基准是使用最广泛的。为了找到最常见的对象类别，利用了从SUN数据库获得的统计数据。建立词汇表后，通过在线搜索引擎收集3D
    CAD模型，并由人工验证。
- en: S3DIS The Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset is composed
    of 5 large-scale indoor scenes from three buildings to hold diverse in architectural
    style and appearance. The point clouds are automatically generated without manual
    intervention. 12 semantic elements including structural elements (floor, wall,
    etc.) and common furniture are detected.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: S3DIS斯坦福大规模3D室内空间（S3DIS）数据集由来自三座建筑的5个大规模室内场景组成，这些场景在建筑风格和外观上各不相同。点云是自动生成的，无需人工干预。检测到了包括结构元素（地板、墙壁等）和常见家具在内的12种语义元素。
- en: Semantic3D Semantic3D [[30](#bib.bib30)] is the largest 3D point cloud dataset
    for outdoor scene segmentation so far. It contains over 4 billion points collected
    from around 110000$m^{2}$ area with a static lidar. The natural of outdoor scene,
    such as the unevenly distribution of points and massive occlusions, makes the
    dataset challenging.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Semantic3D Semantic3D [[30](#bib.bib30)] 是目前最大的户外场景分割 3D 点云数据集。它包含了从约 110000$m^{2}$
    区域收集的超过 40 亿个点，这些点是用静态激光雷达收集的。户外场景的自然属性，如点的分布不均和大量遮挡，使得数据集具有挑战性。
- en: ScanNet ScanNet [[10](#bib.bib10)] is a video dataset consists of 2.5 million
    frames from more than 1000 scans, annotated with camera poses, surface reconstructions
    and instance-level semantic segmentation. The dataset provides benchmarks for
    mutiple 3D scene understanding tasks, such as classification, semantic voxel labeling
    and CAD model retrieval.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ScanNet ScanNet [[10](#bib.bib10)] 是一个视频数据集，包含来自 1000 多个扫描的 250 万帧，注释了相机姿态、表面重建和实例级语义分割。该数据集提供了多种
    3D 场景理解任务的基准，如分类、语义体素标记和 CAD 模型检索。
- en: 'KITTI The KITTI [[22](#bib.bib22), [23](#bib.bib23), [21](#bib.bib21), [69](#bib.bib69)]
    vision benchmark suite is among the most famous benchmarks with 3D data. It covers
    benchmarks for 3D object detection, tracking and scene flow estimation. The multi-view
    data are captured with an autonomous driving platform with two high-resolution
    color and gray cameras, a Velodyne laser scanner and a GPS localization system.
    Only three kinds of objects which are important to autonomous driving are labelled:
    cars, pedestrians and cyclists.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: KITTI KITTI [[22](#bib.bib22), [23](#bib.bib23), [21](#bib.bib21), [69](#bib.bib69)]
    视觉基准套件是最著名的 3D 数据基准之一。它涵盖了 3D 目标检测、跟踪和场景流估计的基准。多视角数据是通过配备两个高分辨率彩色和灰度摄像头、一个 Velodyne
    激光扫描仪和一个 GPS 定位系统的自动驾驶平台捕获的。仅对三种对自动驾驶重要的对象进行标注：汽车、行人和骑自行车者。
- en: Other datasets There are some other datasets of high quality but not widely
    used, such as Oakland [[71](#bib.bib71)], iQmulus [[97](#bib.bib97)] and Paris-Lille-3D
    [[85](#bib.bib85)]. 3DMatch [[131](#bib.bib131)] pushed the research in 3D matching
    and registration, which is a less popular direction in the past period. Recently,
    the rising demand from industry of autonomous driving has spawned several large-scale
    road-based datasets, represented by nuScenes [[4](#bib.bib4)], Lyft Level 5 [[44](#bib.bib44)]
    and Waymo Open Dataset [[95](#bib.bib95)]. They proposed complicated challenges
    requiring to leverage multi-view data and related metadata. The development of
    datasets is helping reduce the gap between research and practical applications.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据集 还有一些高质量但不广泛使用的数据集，如 Oakland [[71](#bib.bib71)]、iQmulus [[97](#bib.bib97)]
    和 Paris-Lille-3D [[85](#bib.bib85)]。3DMatch [[131](#bib.bib131)] 推动了 3D 匹配和注册的研究，这在过去是一项不太流行的方向。最近，来自自动驾驶行业的需求上升催生了几个大规模的道路数据集，如
    nuScenes [[4](#bib.bib4)]、Lyft Level 5 [[44](#bib.bib44)] 和 Waymo Open Dataset
    [[95](#bib.bib95)]。它们提出了复杂的挑战，需要利用多视角数据和相关元数据。数据集的发展有助于缩小研究与实际应用之间的差距。
- en: 2.2 Metrics
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 指标
- en: The comparison between different algorithms requires certain metrics. It is
    important to design and select appropriate metrics. Well-designed metrics can
    provide valid evaluation of different models, while unreasonable metrics might
    lead to incorrect conclusions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不同算法之间的比较需要某些指标。设计和选择适当的指标非常重要。设计良好的指标可以提供对不同模型的有效评估，而不合理的指标可能导致错误的结论。
- en: 'Table [2](#S2.T2 "Table 2 ‣ 2.2 Metrics ‣ 2 Datasets and metrics ‣ Deep Learning
    for 3D Point Cloud Understanding: A Survey") lists widely used metrics in different
    tasks. For classification methods, overall accuracy and mean accuracy are most
    frequently used. Segmentation models can be analyzed by accuracy or (m)IoU. In
    detection tasks, the result are usually evaluated region-wise, so (m)IoU, accuracy,
    precision and recall could apply. MOTA and MOTP are specially designed for object
    tracking modelts, while EPE is for scene for estimation. ROC curves, which is
    the derivative of precision and recall, help evaluate the performance of 3D match
    and registration models. Besides, visualization is always an effective supplement
    of numbers.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [2](#S2.T2 "Table 2 ‣ 2.2 Metrics ‣ 2 Datasets and metrics ‣ Deep Learning
    for 3D Point Cloud Understanding: A Survey") 列出了不同任务中广泛使用的指标。对于分类方法，整体准确率和平均准确率是最常用的。分割模型可以通过准确率或
    (m)IoU 进行分析。在检测任务中，结果通常是按区域评估的，因此 (m)IoU、准确率、精确度和召回率都可以应用。MOTA 和 MOTP 专门为目标跟踪模型设计，而
    EPE 用于场景估计。ROC 曲线是精确度和召回率的导数，有助于评估 3D 匹配和注册模型的性能。此外，数据可视化始终是数字的有效补充。'
- en: 'Table 2: Commonly used metrics for different tasks. In this table, $N$ denotes
    the number of samples, $C$ denotes the number of categories, $IDS$ denotes the
    number of identity switches, $I_{i,j}$ denotes the number of points that are from
    ground truth class/instance $i$ and labelled as $j$, $TP/TN/FP/FN$ stands for
    the number of true positives, true negatives, false positives and false negatives
    respectively. Higher metrics indicate better results if not specified otherwise.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同任务的常用度量标准。在此表中，$N$ 表示样本数量，$C$ 表示类别数量，$IDS$ 表示身份切换的数量，$I_{i,j}$ 表示来自真实类别/实例
    $i$ 并标记为 $j$ 的点数，$TP/TN/FP/FN$ 分别表示真阳性、真阴性、假阳性和假阴性的数量。较高的度量值表示结果更好，除非另有说明。
- en: '| Metric | Formula | Explanation |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 度量 | 公式 | 解释 |'
- en: '| --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Accuracy | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$ | Accuracy indicates how
    many predictions are correct over all predictions. “Overall accuracy (OA)” indicates
    the accuracy on the entire dataset. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$ | 准确率表示所有预测中有多少预测是正确的。“整体准确率（OA）”表示整个数据集上的准确率。
    |'
- en: '| mACC | $mACC=\frac{1}{C}\sum_{c=1}^{C}Accuracy_{c}$ | The mean of accuracy
    on different categories, useful when the categories are imbalanced. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 平均准确率 | $mACC=\frac{1}{C}\sum_{c=1}^{C}Accuracy_{c}$ | 不同类别准确率的均值，在类别不平衡时特别有用。'
- en: '| Precision | $Precision=\frac{TP}{TP+FP}$ | The ratio of correct predictions
    over all predictions. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | $Precision=\frac{TP}{TP+FP}$ | 正确预测占所有预测的比例。 |'
- en: '| Recall | $Recall=\frac{TP}{TP+FN}$ | The ratio of correct predictions over
    positive samples in the ground truth. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | $Recall=\frac{TP}{TP+FN}$ | 正确预测占真实正样本的比例。 |'
- en: '| F1-Score | $F_{1}=2\times\frac{Precision\cdot Recall}{Precision+Recall}$
    | The harmonic mean of precision and recall. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| F1 分数 | $F_{1}=2\times\frac{Precision\cdot Recall}{Precision+Recall}$ | 精度和召回率的调和平均值。
    |'
- en: '| IoU | $IoU_{i}=\frac{I_{i,i}}{\sum_{c=1}^{C}(I_{i,c}+I_{c,i})-I_{i,i}}$ |
    Intersection over Union (of class/instance $i$). The intersection and union are
    calculated between the prediction and the ground truth. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| IoU | $IoU_{i}=\frac{I_{i,i}}{\sum_{c=1}^{C}(I_{i,c}+I_{c,i})-I_{i,i}}$ |
    类别/实例 $i$ 的交并比。交集和并集是在预测和真实值之间计算的。 |'
- en: '| mIoU | $mIoU=\frac{1}{C}\sum_{c=1}^{C}IoU_{i}$ | The mean of IoU on all classes/instances.
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 平均 IoU | $mIoU=\frac{1}{C}\sum_{c=1}^{C}IoU_{i}$ | 所有类别/实例的 IoU 的均值。 |'
- en: '| MOTA | $MOTA=1-\frac{FN+FP+IDS}{TP+FN}$ | Multi-object tracking accuracy
    (MOTA) synthesizes 3 error sources: false positives, missed targets and identity
    switches, and the number of ground truth (as $TP+FN$) is used for normalization.
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 多目标跟踪准确率 | $MOTA=1-\frac{FN+FP+IDS}{TP+FN}$ | 多目标跟踪准确率（MOTA）综合了 3 种误差来源：假阳性、漏检目标和身份切换，且使用真实值数量（$TP+FN$）进行归一化。
    |'
- en: '| MOTP | $MOTP=\frac{\sum_{i,t}e_{i,t}}{\sum_{t}d_{t}}$ | Multi-object tracking
    precision (MOTP) indicates the precision of localization. $d_{t}$ denotes the
    number of matches at time $t$, and $e_{i,t}$ denotes the error of the $i$-th pair
    at time $t$. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 多目标跟踪精度 | $MOTP=\frac{\sum_{i,t}e_{i,t}}{\sum_{t}d_{t}}$ | 多目标跟踪精度（MOTP）表示定位精度。$d_{t}$
    表示时间 $t$ 上的匹配数量，而 $e_{i,t}$ 表示时间 $t$ 上第 $i$ 对的误差。 |'
- en: '| EPE | $EPE=&#124;&#124;\hat{sf}-sf&#124;&#124;_{2}$ | End point error (EPE)
    is used in scene flow estimation, also referred as EPE2D/EPE3D for 2D/3D data
    respectively. $\hat{sf}$ denotes the predicted scene flow vector while $sf$ denotes
    the ground truth. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 端点误差 | $EPE=&#124;&#124;\hat{sf}-sf&#124;&#124;_{2}$ | 端点误差（EPE）用于场景流估计，也称为
    EPE2D/EPE3D，分别用于 2D/3D 数据。$\hat{sf}$ 表示预测的场景流向量，而 $sf$ 表示真实值。 |'
- en: 3 Classification
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 分类
- en: 3.1 Overview
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: 'Classification on point clouds is commonly known as 3D shape classification.
    Similar to image classification models, models on 3D shape classification usually
    first generate a global embedding with an aggregation encoder, then pass the embedding
    through several fully connected layers to obtain the final result. Most 3D shape
    classification methods are tested with clean 3D models (as in Figure [2](#S3.F2
    "Figure 2 ‣ 3.1 Overview ‣ 3 Classification ‣ Deep Learning for 3D Point Cloud
    Understanding: A Survey")). Based on the point cloud aggregation method, classification
    models can be generally divided into two categories: projection-based methods
    and point-based methods.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '点云分类通常称为 3D 形状分类。类似于图像分类模型，3D 形状分类模型通常首先通过聚合编码器生成全局嵌入，然后将嵌入传递通过多个全连接层以获得最终结果。大多数
    3D 形状分类方法在干净的 3D 模型上进行测试（如图 [2](#S3.F2 "Figure 2 ‣ 3.1 Overview ‣ 3 Classification
    ‣ Deep Learning for 3D Point Cloud Understanding: A Survey")）。根据点云聚合方法，分类模型通常可以分为两类：基于投影的方法和基于点的方法。'
- en: '![Refer to caption](img/31178a158fb9f3782b1b869281d42623.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/31178a158fb9f3782b1b869281d42623.png)'
- en: 'Figure 2: 3D models from ShapeNet [[5](#bib.bib5)]. ShapeNet contains large-scale
    3D models with manually verified annotation.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：来自ShapeNet的3D模型 [[5](#bib.bib5)]。ShapeNet包含了大规模的3D模型和人工验证的注释。
- en: 3.2 Projection-based Methods
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于投影的方法
- en: Projection-based methods project the unstructured 3D point clouds into specific
    presupposed modality (e.g. voxels, pillars), and extract features from the target
    format, which allows them to benefit from the previous research findings in the
    corresponding direction.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于投影的方法将无结构的3D点云投射到特定的预设模态（例如体素、柱状体），并从目标格式中提取特征，这使得它们能够受益于在相应方向上的前期研究成果。
- en: 3.2.1 Multi-view representation
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 多视角表示
- en: MvCNN [[94](#bib.bib94)] is a method based on a multi-view representation of
    point clouds. A 3D point cloud is represented by a group of 2D images by rendering
    snapshots from different angles. Each image in the group will be passed through
    a CNN to extract view-based features, pooled across views and passed through another
    CNN to build a compact descriptor. While MVCNN does not distinguish different
    views, it is helpful to consider the relationship among views. GVCNN [[20](#bib.bib20)]
    is a method that takes advantage of this relationship. By quantifying the discrimination
    of views, we are able to divided the set of views into groups based on their discrimination
    scores. The view descriptors will be passed through intra-group pooling and cross-group
    fusion for prediction. Aside from the models mentioned above, [[128](#bib.bib128)]
    and [[124](#bib.bib124)] also improve the recognition accuracy with multi-view
    representation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MvCNN [[94](#bib.bib94)] 是一种基于点云多视角表示的方法。3D点云通过从不同角度渲染快照的2D图像组进行表示。组中的每个图像将通过CNN提取视角特征，视角特征在视角间汇聚，然后通过另一个CNN构建紧凑描述符。虽然MVCNN不区分不同的视角，但考虑视角之间的关系是有帮助的。GVCNN
    [[20](#bib.bib20)] 是一种利用这种关系的方法。通过量化视角的区分度，我们能够根据视角的区分分数将视角集合划分为多个组。视角描述符将通过组内汇聚和组间融合进行预测。除了上述模型，[[128](#bib.bib128)]
    和 [[124](#bib.bib124)] 也通过多视角表示提高了识别准确性。
- en: 3.2.2 Volumetric representation
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 体积表示
- en: VoxNet [[67](#bib.bib67)] is an early method using the volumetric representation.
    In this method, each point $(x,y,z)$ is projected into a corresponding discrete
    voxel point $(i,j,k)$. Each point cloud will be mapped into an occupancy grid
    of $32\times 32\times 32$ voxels, and the grid will then be passed through two
    3D convolutional layers to obtain the final representation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: VoxNet [[67](#bib.bib67)] 是一种早期使用体积表示的方法。在这种方法中，每个点 $(x,y,z)$ 被投射到一个对应的离散体素点
    $(i,j,k)$。每个点云将被映射到一个 $32\times 32\times 32$ 体素的占用网格中，然后该网格将通过两个3D卷积层获得最终表示。
- en: VoxNet simply uses adaption of CNN layers for the prediction head, which leads
    to potential loss of detailed spatial information. 3D ShapeNet [[113](#bib.bib113)]
    proposed a belief-based deep convolutional network to learn the distribution of
    point clouds in different 3D shapes. In this method, 3D shapes are represented
    by the probability distributions of binary variables on grids.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: VoxNet仅仅对CNN层进行了适应以用于预测头，这可能导致详细空间信息的潜在丧失。3D ShapeNet [[113](#bib.bib113)] 提出了一个基于信念的深度卷积网络，以学习点云在不同3D形状中的分布。在这种方法中，3D形状由网格上二元变量的概率分布表示。
- en: While volumetric methods already achieve satisfactory performance, most suffer
    from the cubic growth of computation complexity and memory footprint, hence the
    resolution of the grid is strictly limited. OctNet [[84](#bib.bib84)] improved
    the efficiency by introducing a hybrid grid-octree structure to hierarchically
    partition point clouds. A point cloud is represented by several octrees along
    a regular grid, each octree is encoded as a bit string, and features are generated
    through naive arithmetic. Inspired by OctNet, OCNN [[99](#bib.bib99)] then proposed
    a method that introduces 3D-CNNs to extract features from octrees.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然体积方法已经取得了令人满意的性能，但大多数方法都受到计算复杂度和内存占用立方增长的影响，因此网格的分辨率受到严格限制。OctNet [[84](#bib.bib84)]
    通过引入混合网格-八叉树结构来分层分割点云，从而提高了效率。一个点云由几个八叉树和一个规则网格表示，每个八叉树被编码为一个位字符串，通过简单的算术生成特征。受到OctNet的启发，OCNN
    [[99](#bib.bib99)] 随后提出了一种方法，采用3D-CNN从八叉树中提取特征。
- en: Methods based on volumetric representations as mentioned above are naturally
    coarse as only a small fraction of voxels are non-empty and the detailed context
    inside each voxel is hardly collected. The balance between resolution and computation
    is difficult to achieve in practice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述基于体积表示的方法由于只有一小部分体素是非空的，且每个体素内部的详细上下文很难收集，因此自然较为粗糙。在实际操作中，很难在分辨率和计算之间实现平衡。
- en: 3.2.3 Basis point set
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 基础点集
- en: BPS [[75](#bib.bib75)] proposed a new approach that breaks the convention that
    point clouds, even with various sizes, are usually projected onto a grid of same
    size. In BPS, input points are first normalized into a unit ball, then a group
    of points is randomly sampled to make up a basis point set (BPS). The sampled
    BPS is constant for all point clouds in a dataset. For a given point cloud $X$,
    each point $x_{i}$ is represented by the Euclidean distance between itself and
    its nearest neighbor in BPS. By passing such representation through the last two
    fully connected layers of PointNet, the model achieves performance similar to
    that of the original PointNet design.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: BPS [[75](#bib.bib75)] 提出了一个新的方法，打破了点云（即使具有不同大小）通常被投影到相同大小网格上的常规。在 BPS 中，输入点首先被标准化为单位球体，然后随机采样一组点来构成基础点集（BPS）。所采样的
    BPS 对数据集中所有点云是恒定的。对于给定的点云 $X$，每个点 $x_{i}$ 由其自身与 BPS 中最近邻点之间的欧几里得距离表示。通过将这种表示传递通过
    PointNet 的最后两层全连接层，模型达到了类似于原始 PointNet 设计的性能。
- en: 3.3 Point-based Methods
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于点的方法
- en: Compared with projection-based methods that aggregate points from a spatial
    neighborhood, point-based methods attempt to learn features from individual points.
    Most of recent work focuses on this direction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于投影的方法（通过空间邻域汇总点）相比，基于点的方法试图从单个点中学习特征。最近的大部分工作都集中在这个方向上。
- en: 3.3.1 MLP networks
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 MLP 网络
- en: PointNet [[79](#bib.bib79)] is a famous architecture that takes advantage of
    multi-layer perceptrons (MLPs). The input (an $n\times 3$ 2D tensor) is first
    multiplied by an affine transformation matrix predicted by a mini-network (T-Net)
    to hold invariance under geometric transformations. The point set is then passed
    through a group of MLPs followed by another joint alignment network, and a max-pooling
    layer to obtain the final global feature. This backbone can be used for both classification
    and segmentation prediction. For classification, the global feature is passed
    through an MLP for output scores. For segmentation, the concatenations of the
    global feature and different levels of intermediate features from each point are
    passed through an MLP for the classification result of each point. Conventional
    CNNs take features at different scales by a stack of convolutional layers; inspired
    by that, PointNet++ [[80](#bib.bib80)] is proposed. In this work, the local region
    of a point $x$ is defined as the points within a sphere centered at $x$. One set
    abstraction level here contains a sampling layer, a grouping layer to identify
    local regions and a PointNet layer. Stacking such set abstraction levels allows
    us to extract features hierarchically as CNNs for image tasks do.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet [[79](#bib.bib79)] 是一个著名的架构，利用了多层感知机（MLPs）。输入（一个 $n\times 3$ 的 2D 张量）首先乘以由一个小型网络（T-Net）预测的仿射变换矩阵，以保持在几何变换下的不变性。然后，点集经过一组
    MLPs，接着是另一个联合对齐网络，最后经过一个最大池化层以获得最终的全局特征。该骨干网络可用于分类和分割预测。对于分类，全局特征通过 MLP 得到输出分数。对于分割，全球特征和来自每个点的不同层级的中间特征的连接通过
    MLP 进行处理，以获得每个点的分类结果。传统的 CNN 通过一系列卷积层来处理不同尺度的特征；受此启发，提出了 PointNet++ [[80](#bib.bib80)]。在这项工作中，点
    $x$ 的局部区域被定义为以 $x$ 为中心的球体内的点。一个集合抽象层包含一个采样层、一个分组层用于识别局部区域和一个 PointNet 层。堆叠这样的集合抽象层允许我们像
    CNNs 处理图像任务一样分层提取特征。
- en: The simple implementation and promising performance of PointNet [[79](#bib.bib79)]
    and PointNet++ [[80](#bib.bib80)] inspired a lot of follow-up work. PointWeb [[136](#bib.bib136)]
    is adapted from PointNet++ and improves quality of features by introducing Adaptive
    Feature Adjustment (AFA) to make use of context information of local neighborhoods.
    In addition, SRN [[16](#bib.bib16)] proposed Structural Relation Network (SRN)
    to equip PointNet++, and obtained better performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet [[79](#bib.bib79)] 和 PointNet++ [[80](#bib.bib80)] 的简单实现和良好性能激发了大量后续工作。PointWeb
    [[136](#bib.bib136)] 是从 PointNet++ 改编而来，通过引入自适应特征调整（AFA）来利用局部邻域的上下文信息，从而提高了特征质量。此外，SRN
    [[16](#bib.bib16)] 提出了结构关系网络（SRN）来增强 PointNet++，并取得了更好的性能。
- en: 3.3.2 Convolutional networks
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 卷积网络
- en: Convolution kernels on 2D data can be extended to work on 3D point cloud data.
    As mentioned before, VoxNet [[67](#bib.bib67)] is an early work that directly
    takes advantage of 3D convolution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2D 数据上的卷积核可以扩展到 3D 点云数据上。如前所述，VoxNet [[67](#bib.bib67)] 是一个早期的工作，直接利用 3D 卷积。
- en: A-CNN [[46](#bib.bib46)] proposed another way to apply convolution on point
    clouds. In order to prevent redundant information from overlapped local regions
    (the same group of neighboring points might be repeatedly included in regions
    at different scales), A-CNN proposed a ring-based scheme instead of spheres. To
    convolve points within a ring, points are projected on a tangent plane at a query
    point $q_{i}$, then ordered in clockwise or counter-clockwise direction by making
    use of cross product and dot product, and eventually a 1-D convolution kernel
    will be applied to the ordered sequence. The output feature can be used for both
    classification and segmentation as in PointNet.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: A-CNN [[46](#bib.bib46)] 提出了另一种在点云上应用卷积的方法。为了防止重叠局部区域中冗余信息（同一组邻域点可能在不同尺度的区域中重复出现），A-CNN
    提出了基于环的方案，而不是球体。为了在环内进行卷积，将点投影到查询点 $q_{i}$ 的切平面上，然后通过交叉积和点积按顺时针或逆时针方向排序，最终对排序序列应用
    1-D 卷积核。输出特征可用于分类和分割，类似于 PointNet。
- en: RS-CNN [[62](#bib.bib62)] is another convolutional network based on relation-shape
    convolution. An RS-Conv kernel takes a neighborhood around a certain point as
    its input, and learns the mapping from naive relations (e.g. Euclidean distance,
    relative position) to high-level relations among points, and encodes the spatial
    structure within the neighborhood with the learned mapping.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: RS-CNN [[62](#bib.bib62)] 是另一个基于关系-形状卷积的卷积网络。RS-Conv 卷积核以某个点周围的邻域为输入，学习从原始关系（例如欧几里得距离、相对位置）到点之间高层次关系的映射，并使用学习到的映射对邻域内的空间结构进行编码。
- en: In PointConv [[112](#bib.bib112)], the convolution operation is defined as finding
    a Monte Carlo estimation of the hidden continuous 3D convolution w.r.t. an importance
    sampling. The process is composed with a weighting function and a density function,
    implemented by MLP layers and a kernelized density estimation. Furthermore, the
    3D convolution is reduced into matrix multiplication and 2D convolution for memory
    and computational efficiency and easy deployment. A similar idea is used in MCCNN
    [[34](#bib.bib34)], where convolution is replaced by a Monte Carlo estimation
    based on the density function of the sample.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PointConv [[112](#bib.bib112)] 中，卷积操作被定义为对重要性采样下的隐藏连续 3D 卷积进行蒙特卡洛估计。该过程由加权函数和密度函数组成，通过
    MLP 层和核密度估计实现。此外，为了提高内存和计算效率以及便于部署，3D 卷积被简化为矩阵乘法和 2D 卷积。MCCNN [[34](#bib.bib34)]
    中使用了类似的思路，其中卷积被基于样本密度函数的蒙特卡洛估计所替代。
- en: Geo-CNN [[49](#bib.bib49)] proposed another way to model the geometric relationship
    among neighborhood points. By taking six orthogonal bases, the space will be separated
    into eight quadrants, and all vectors in a specific quadrant can be composed by
    three of the bases. Features are extracted independently along each direction
    with corresponding direction-associated weight matrices, and are aggregated based
    on the angle between the geometric vector and the bases. The feature of some specific
    point at the current layer is the sum of features of the given point and its neighboring
    edge features from the previous layer.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Geo-CNN [[49](#bib.bib49)] 提出了另一种建模邻域点几何关系的方法。通过选择六个正交基，将空间分成八个象限，特定象限中的所有向量可以由三个基组成。特征沿每个方向独立提取，使用相应的方向相关权重矩阵，并基于几何向量与基之间的角度进行聚合。当前层某些特定点的特征是该点和其邻边特征在上一层的特征之和。
- en: In SFCNN [[83](#bib.bib83)], the input point cloud is projected onto regular
    icosahedral lattices with discrete sphere coordinates, hence convolution can be
    implemented by maxpooling and convolution on the concatenated features from vertices
    of spherical lattices and their neighbors. SFCNN holds rotation invariance and
    is robust to perturbations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SFCNN [[83](#bib.bib83)] 中，输入的点云被投影到具有离散球面坐标的规则二十面体格点上，因此可以通过在球面格点的顶点及其邻居的特征拼接上实施最大池化和卷积。SFCNN
    具有旋转不变性，并且对扰动具有鲁棒性。
- en: 3.3.3 Graph networks
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 图网络
- en: Graph networks consider a point cloud as a graph and the vertices of the graph
    as the points, and edges are generated based on the neighbors of each point. Features
    will be learned in spatial or spectral domains.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图网络将点云视为图形，将图形的顶点视为点，并根据每个点的邻居生成边。特征将在空间或谱域中学习。
- en: ECC [[93](#bib.bib93)] first proposed the idea of considering each point as
    a vertex of the graph and connected edges between pairs of points that are “neighbors”.
    Then, edge conditioned convolution (ECC) is applied with a filter generating network
    such as MLP. Neighborhood information is aggregated by maxpooling and coarsened
    graph will be generated with VoxelGrid [[86](#bib.bib86)] algorithm. After that,
    DGCNN [[105](#bib.bib105)] uses a MLP to implement EdgeConv, followed by channel-wise
    symmetric aggregation on edge features from the neighborhood of each point, which
    allows the graph to be dynamically updated after each layer of the network.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ECC [[93](#bib.bib93)] 首次提出将每个点视为图形的一个顶点，并在“邻居”点对之间连接边的想法。然后，应用边条件卷积（ECC）与如
    MLP 的滤波器生成网络。通过最大池化聚合邻域信息，粗化图将使用 VoxelGrid [[86](#bib.bib86)] 算法生成。之后，DGCNN [[105](#bib.bib105)]
    使用 MLP 实现 EdgeConv，并对每个点邻域中的边特征进行通道级对称聚合，这使得图在网络的每一层之后可以动态更新。
- en: Inspired by DGCNN, Hassani and Haley [[31](#bib.bib31)] proposed an unsupervised
    multi-task approach to learn shape features. The approach consists of an encoder
    and an decoder, where the encoder is constructed from multi-scale graphs, and
    the decoder is constructed for three unsupervised tasks (clustering, self-supervised
    classification and reconstruction) trained by a joint loss.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 受 DGCNN 启发，Hassani 和 Haley [[31](#bib.bib31)] 提出了一个无监督的多任务方法来学习形状特征。该方法由编码器和解码器组成，其中编码器由多尺度图构建，解码器则用于三种无监督任务（聚类、自监督分类和重建），通过联合损失进行训练。
- en: ClusterNet [[6](#bib.bib6)] uses rigorously rotation-invariant (RRI) module
    to generate rotation-invariant features from each point, and an unsupervised agglomerative
    hierarchical clustering method to construct hierarchical structures of a point
    cloud. Features of sub-clusters at each level are first learned with an EdgeConv
    block, then aggregated by maxpooling.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterNet [[6](#bib.bib6)] 使用严格的旋转不变（RRI）模块从每个点生成旋转不变特征，并使用无监督的聚合层次聚类方法构建点云的层次结构。每一级子簇的特征首先通过
    EdgeConv 模块学习，然后通过最大池化进行聚合。
- en: 3.3.4 Other networks
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 其他网络
- en: Aside from OctNet [[84](#bib.bib84)], which uses octrees on voxel grids to hierarchically
    extract features from point clouds, Kd-Net [[45](#bib.bib45)] makes use of K-d
    trees to build a bottom-up encoder. Leaf node representations are normalized 3D
    coordinates (by setting the center of mass as origin and rescaled to $[-1,1]^{3}$),
    and non-leaf node representations are calculated from its children nodes with
    MLP. The parameters of MLPs are shared within each level of the tree. Moreover,
    3DContextNet [[131](#bib.bib131)] proposed another method based on K-d trees.
    While non-leaf representations are still computed with MLP from its children,
    the aggregation at each level is more complicated for considering both local cues
    and global cues. The local cues concern points in the corresponding local region,
    and the global cues concern the relationship between current position and all
    positions in the input feature map. The representation at the root will be used
    for prediction.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用八叉树在体素网格上分层提取点云特征的 OctNet [[84](#bib.bib84)] 外，Kd-Net [[45](#bib.bib45)]
    利用 K-d 树构建自下而上的编码器。叶节点表示被规范化为 3D 坐标（通过将质心设为原点并重新缩放到 $[-1,1]^{3}$），非叶节点表示则通过 MLP
    从其子节点计算。MLP 的参数在树的每一层中共享。此外，3DContextNet [[131](#bib.bib131)] 提出了另一种基于 K-d 树的方法。虽然非叶节点表示仍然通过
    MLP 从其子节点计算，但每一层的聚合更复杂，以考虑局部线索和全局线索。局部线索关注对应局部区域中的点，全局线索关注当前位置信息与输入特征图中所有位置之间的关系。根节点的表示将用于预测。
- en: RCNet [[111](#bib.bib111)] introduced RNN to point cloud embedding. The ambient
    space is first partitioned into parallel beams, each beam is then fed into a shared
    RNN, and the output subregional features are considered as a 2D feature map and
    processed by a 2D CNN.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: RCNet [[111](#bib.bib111)] 引入了 RNN 到点云嵌入。首先将环境空间划分为平行光束，然后将每个光束输入共享的 RNN，输出的子区域特征被视为
    2D 特征图并由 2D CNN 处理。
- en: SO-Net [[56](#bib.bib56)] is a method based on the self-organized map (SOM).
    A SOM is a low-dimensional (two-dimensional in the paper) representation of the
    input point cloud, initialized by a proper guess (dispersing nodes uniformly in
    a unit ball), and trained with unsupervised competitive learning. A k-nearest-neighbor
    set is searched over the SOM for each point, and the normalized KNN set is then
    passed through a series of fully connected layers to generate individual point
    features. The point features are used to generate node features by maxpooling
    according to the association in KNN search, and the node features are passed through
    another series of fully connected layers and aggregated into a global representation
    of the input point cloud.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: SO-Net [[56](#bib.bib56)] 是一种基于自组织映射（SOM）的方法。SOM 是对输入点云的低维（论文中为二维）表示，通过适当的猜测（在单位球内均匀分布节点）初始化，并通过无监督竞争学习进行训练。为每个点在
    SOM 上搜索一个 k 最近邻集，然后将标准化的 KNN 集通过一系列全连接层生成单个点特征。这些点特征用于通过根据 KNN 搜索中的关联进行最大池化来生成节点特征，节点特征再通过另一系列全连接层处理，并聚合成输入点云的全局表示。
- en: 3.4 Experiments
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 实验
- en: 'Different methods choose to test their models on various datasets. In order
    to obtain a better comparison among methods, we select datasets that most methods
    are tested on, and list the experiment results for them in Table [3](#S3.T3 "Table
    3 ‣ 3.4 Experiments ‣ 3 Classification ‣ Deep Learning for 3D Point Cloud Understanding:
    A Survey").'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '不同的方法选择在各种数据集上测试他们的模型。为了获得方法间的更好比较，我们选择了大多数方法进行测试的数据集，并在表[3](#S3.T3 "Table
    3 ‣ 3.4 Experiments ‣ 3 Classification ‣ Deep Learning for 3D Point Cloud Understanding:
    A Survey")中列出了它们的实验结果。'
- en: 'Table 3: Experiment results on ModelNet40 classification benchmark. “OA” stands
    for overall accuracy and “mACC” stands for mean accuracy.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：ModelNet40 分类基准测试的实验结果。“OA”代表整体准确率，“mACC”代表平均准确率。
- en: '| Methods | ModelNet40(OA) | ModelNet40(mAcc) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ModelNet40(OA) | ModelNet40(mAcc) |'
- en: '| --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| PointNet [[79](#bib.bib79)] | 89.2% | 86.2% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| PointNet [[79](#bib.bib79)] | 89.2% | 86.2% |'
- en: '| PointNet++ [[80](#bib.bib80)] | 90.7% | 90.7% |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| PointNet++ [[80](#bib.bib80)] | 90.7% | 90.7% |'
- en: '| PointWeb [[136](#bib.bib136)] | 92.3% | 89.4% |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| PointWeb [[136](#bib.bib136)] | 92.3% | 89.4% |'
- en: '| SRN [[16](#bib.bib16)] | 91.5% | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SRN [[16](#bib.bib16)] | 91.5% | - |'
- en: '| Pointwise-CNN [[37](#bib.bib37)] | 86.1% | 81.4% |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Pointwise-CNN [[37](#bib.bib37)] | 86.1% | 81.4% |'
- en: '| PointConv [[112](#bib.bib112)] | 92.5% | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| PointConv [[112](#bib.bib112)] | 92.5% | - |'
- en: '| RS-CNN [[62](#bib.bib62)] | 92.6% | - |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| RS-CNN [[62](#bib.bib62)] | 92.6% | - |'
- en: '| GeoCNN [[49](#bib.bib49)] | 93.4% | 91.1% |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| GeoCNN [[49](#bib.bib49)] | 93.4% | 91.1% |'
- en: '| A-CNN [[46](#bib.bib46)] | 92.6% | 90.3% |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| A-CNN [[46](#bib.bib46)] | 92.6% | 90.3% |'
- en: '| Hassani and Haley [[31](#bib.bib31)] | 89.1% | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Hassani and Haley [[31](#bib.bib31)] | 89.1% | - |'
- en: '| ECC [[93](#bib.bib93)] | 87.4% | 83.2% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ECC [[93](#bib.bib93)] | 87.4% | 83.2% |'
- en: '| SFCNN [[83](#bib.bib83)] | 91.4% | - |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SFCNN [[83](#bib.bib83)] | 91.4% | - |'
- en: '| DGCNN [[105](#bib.bib105)] | 92.2% | 90.2% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN [[105](#bib.bib105)] | 92.2% | 90.2% |'
- en: '| ClusterNet [[6](#bib.bib6)] | 87.1% | - |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ClusterNet [[6](#bib.bib6)] | 87.1% | - |'
- en: '| BPS [[75](#bib.bib75)] | 91.6% | - |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| BPS [[75](#bib.bib75)] | 91.6% | - |'
- en: '| KD-Net [[45](#bib.bib45)] | 91.8% | 88.5% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| KD-Net [[45](#bib.bib45)] | 91.8% | 88.5% |'
- en: '| 3DContextNet[[131](#bib.bib131)] | 91.1% | - |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 3DContextNet [[131](#bib.bib131)] | 91.1% | - |'
- en: '| RCNet [[111](#bib.bib111)] | 91.6% | - |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| RCNet [[111](#bib.bib111)] | 91.6% | - |'
- en: '| SO-Net [[56](#bib.bib56)] | 90.9% | 87.3% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| SO-Net [[56](#bib.bib56)] | 90.9% | 87.3% |'
- en: 4 Segmentation
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 分割
- en: 4.1 Overview
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 概述
- en: '3D segmentation intends to label each individual point, which requires the
    model to collect both global context and detailed local information at each point.
    Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Overview ‣ 4 Segmentation ‣ Deep Learning for
    3D Point Cloud Understanding: A Survey") shows some examples from S3DIS [[1](#bib.bib1)]
    dataset. There are two main tasks in 3D segmentation: semantic segmentation and
    instance segmentation.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '3D 分割旨在标记每个单独的点，这要求模型在每个点处收集全局上下文和详细的局部信息。图[3](#S4.F3 "Figure 3 ‣ 4.1 Overview
    ‣ 4 Segmentation ‣ Deep Learning for 3D Point Cloud Understanding: A Survey")展示了一些来自
    S3DIS [[1](#bib.bib1)] 数据集的示例。3D 分割主要有两个任务：语义分割和实例分割。'
- en: Since a large number of classification models are able to achieve very high
    performance on popular benchmarks, they tend to test their backbone on segmentation
    datasets to prove the novel contribution and generalization ability. We will not
    reintroduce these models if they have been mentioned above. There are also some
    models that benefit from the jointly training on multiple tasks, and we will discuss
    these methods later in section 3.4.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大量分类模型能够在流行基准上实现非常高的性能，它们往往会在分割数据集上测试其骨干网络，以证明其新颖的贡献和泛化能力。如果这些模型在上述内容中已被提及，我们将不再重新介绍。此外，还有一些模型受益于多任务的联合训练，我们将在
    3.4 节中讨论这些方法。
- en: '![Refer to caption](img/53778104d72437877161902a4744742d.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/53778104d72437877161902a4744742d.png)'
- en: 'Figure 3: Stanford Large-Scale 3D Indoor Spaces Dataset [[1](#bib.bib1)] (S3DIS).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：斯坦福大规模 3D 室内空间数据集 [[1](#bib.bib1)] (S3DIS)。
- en: 4.2 Semantic Segmentation
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 语义分割
- en: Similar to 3D shape classification models, based on how the raw point cloud
    is organized, semantic segmentation methods can be generally divided into projection-based
    methods and point-based methods.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 3D 形状分类模型，根据原始点云的组织方式，语义分割方法一般可以分为基于投影的方法和基于点的方法。
- en: 4.2.1 Projection-based methods
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 基于投影的方法
- en: Huang and You [[38](#bib.bib38)] project the input point cloud into occupancy
    voxels, which are then fed into a 3D convolutional network to generate voxel-level
    labels. All points within a voxel are assigned with the same semantic label as
    the voxel. ScanComplete [[12](#bib.bib12)] utilizes fully convolutional networks
    to adapt to different input data sizes, and deploys a coarse-to-fine strategy
    to improve the resolution of predictions hierarchically. VV-Net [[68](#bib.bib68)]
    also transfers unordered points into regular voxel grids as the first step. After
    that, the local geometry information of each voxel will be encoded with a kernel-based
    interpolated variational auto-encoder (VAE). In each voxel, a radial basis function
    (RBF) is computed to generate a local continuous representation to deal with sparse
    distributions of points.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Huang 和 You [[38](#bib.bib38)] 将输入点云投影到占据体素中，然后将其输入到 3D 卷积网络中以生成体素级标签。体素内的所有点都被分配与体素相同的语义标签。ScanComplete
    [[12](#bib.bib12)] 利用全卷积网络适应不同的输入数据大小，并部署粗到精策略来逐层提高预测分辨率。VV-Net [[68](#bib.bib68)]
    也将无序点转换为规则体素网格作为第一步。之后，每个体素的局部几何信息将通过基于核的插值变分自编码器 (VAE) 进行编码。在每个体素中，计算一个径向基函数
    (RBF) 以生成一个局部连续表示，以处理点的稀疏分布。
- en: F. Jaremo-Lawin et al. [[55](#bib.bib55)] proposed a multi-view method that
    first projects a 3D cloud to 2D planes from multiple camera views, then pixel-wise
    scores on synthetic images are predicted with a multi-stream FCN, and the final
    labels are obtained by fusing scores over different views. PolarNet [[135](#bib.bib135)],
    however, proposed a polar BEV representation. By implicitly aligning attention
    with the long-tailed distribution, this representation reduces the imbalance of
    points across grid cells along the radial axis.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: F. Jaremo-Lawin 等人 [[55](#bib.bib55)] 提出了一个多视角方法，该方法首先将 3D 点云从多个相机视角投影到 2D 平面上，然后使用多流
    FCN 预测合成图像上的像素级分数，最终通过融合不同视角的分数来获得标签。然而，PolarNet [[135](#bib.bib135)] 提出了一个极坐标
    BEV 表示。通过隐式对齐注意力与长尾分布，该表示减少了沿径向轴网格单元中点的不平衡。
- en: Some other methods leverage scans in multiple modalities. 3DMV [[11](#bib.bib11)]
    proposed a joint 3D-multi-view network that combines features from RGB images
    and point cloud. Features are extracted with a 3D CNN stream and a group of 2D
    streams respectively. MVPNet [[42](#bib.bib42)] proposed another aggregation to
    fuse features (from images and point cloud) in 3D canonical space with a point-based
    network.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些方法利用了多种模态的扫描。3DMV [[11](#bib.bib11)] 提出了一个联合 3D-多视角网络，该网络结合了 RGB 图像和点云的特征。特征分别通过
    3D CNN 流和一组 2D 流提取。MVPNet [[42](#bib.bib42)] 提出了另一种聚合方式，将图像和点云的特征在 3D 标准空间中与基于点的网络融合。
- en: 4.2.2 Point-based methods
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 基于点的方法
- en: First of all, PointNet [[79](#bib.bib79)] and PointNet++ [[80](#bib.bib80)]
    can predict semantic labels with corresponding prediction branches attached. Engelmann
    et al. [[18](#bib.bib18)] proposed a method to define neighborhoods in both world
    space and feature space with k-means clustering and KNN. A pairwise distance loss
    and centroid loss are introduced to feature learning based on the assumption that
    points with the same semantic label are supposed to be closer. PointWeb [[136](#bib.bib136)],
    as mentioned in classification, can also be adapted to predict segmentation labels.
    PVCNN [[63](#bib.bib63)] proposed a comprehensive method that leverages both point
    and voxel representation to obtain memory and computation efficiency simultaneously.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，PointNet [[79](#bib.bib79)] 和 PointNet++ [[80](#bib.bib80)] 可以通过附加的预测分支来预测语义标签。Engelmann
    等人 [[18](#bib.bib18)] 提出了通过 k-means 聚类和 KNN 在世界空间和特征空间中定义邻域的方法。引入了成对距离损失和中心点损失，以特征学习为基础，假设具有相同语义标签的点应当更接近。PointWeb
    [[136](#bib.bib136)]，如分类中提到的，也可以适用于预测分割标签。PVCNN [[63](#bib.bib63)] 提出了一种综合方法，利用点和体素表示同时获得内存和计算效率。
- en: Some extensions of the convolution operator are introduced for feature extraction
    on point cloud. PCCN [[100](#bib.bib100)] introduces parametric continuous convolutional
    layers. These layers are parameterized by MLPs and span full continuous vector
    spaces. The generalization allows models to learn over any data structure where
    the support relationship is computable. Pointwise-CNN [[37](#bib.bib37)] introduced
    a point-wise convolution where the neighbor points are projected into kernel cells
    and convolved with corresponding kernel weights. Engelmann et al. [[17](#bib.bib17)]
    proposed Dilated Point Convolution (DPC) to aggregate dilated neighbor features,
    instead of the conventional k-nearest neighbors.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一些卷积算子的扩展被引入用于点云的特征提取。PCCN [[100](#bib.bib100)] 引入了参数化的连续卷积层。这些层由 MLPs 参数化，并跨越整个连续向量空间。该泛化允许模型在任何可计算支持关系的数据结构上进行学习。Pointwise-CNN
    [[37](#bib.bib37)] 引入了一种逐点卷积，其中邻点被投影到核单元中，并与相应的核权重进行卷积。Engelmann 等人 [[17](#bib.bib17)]
    提出了膨胀点卷积（DPC），以聚合膨胀的邻域特征，而不是传统的 k-最近邻。
- en: Graph networks are also used in some segmentation models to obtain the underlying
    geometric structures of the input point clouds. SPG [[52](#bib.bib52)] introduced
    a structure called superpoint graph (SPG) to capture the organization of point
    clouds. The idea is further extended in [[51](#bib.bib51)], which introduces a
    oversegmentation (into pure superpoints) of the input point cloud. Aside from
    that, Graph Attention Convolution [[98](#bib.bib98)] (GAC) is proposed to learn
    relevant features from local neighborhoods selectively. By dynamically assigning
    attention weights to different neighbor points and different feature channels
    based on their spatial positions and feature differences, the model is able to
    learn discriminative features from the most relevant part of the neighbor point
    sets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图网络也在一些分割模型中使用，以获得输入点云的基础几何结构。SPG [[52](#bib.bib52)] 引入了一种称为超点图（SPG）的结构来捕捉点云的组织。这个思想在
    [[51](#bib.bib51)] 中得到了进一步扩展，引入了对输入点云的过分割（成纯超点）。此外，图注意力卷积 [[98](#bib.bib98)]（GAC）被提出以选择性地从局部邻域中学习相关特征。通过根据邻点的空间位置和特征差异动态分配注意力权重，模型能够从邻点集合中最相关的部分学习辨别特征。
- en: Compared with projection-based methods, point-based methods usually require
    more computation and therefore have more trouble dealing with large-scale data.
    Tatarchenko et al. [[96](#bib.bib96)] introduced tangent convolutions to solve
    this. A fully-convolutional network is designed based on the tangent convolution
    and successfully improved the performance on large-scale point clouds. RandLA-Net
    [[36](#bib.bib36)] attempted to reduce computation by replace conventional complex
    point sampling approaches with random sampling. And to avoid random sampling from
    discarding crucial information, a novel feature aggregation module is introduced
    to enlarge receptive fields of each point.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于基于投影的方法，基于点的方法通常需要更多的计算，因此在处理大规模数据时更具挑战性。Tatarchenko 等人 [[96](#bib.bib96)]
    引入了切线卷积来解决这个问题。设计了一个基于切线卷积的全卷积网络，成功提高了在大规模点云上的性能。RandLA-Net [[36](#bib.bib36)]
    试图通过用随机采样替代传统复杂的点采样方法来减少计算量。为了避免随机采样丢失关键的信息，引入了一种新颖的特征聚合模块，以扩大每个点的感受野。
- en: Based on the fact that the production of point-level labels is labor-intensive
    and time-consuming, some methods explored weakly supervised segmentation. Xu and
    Lee [[118](#bib.bib118)] proposed a weakly supervised approach which only requires
    a small fraction of points to be labelled at training stage. By learning gradient
    approximation and smoothness constraints in geometry and color, competitive results
    can be obtained with as few as 10% points labelled. On the other hand, Wei et
    al. [[108](#bib.bib108)] introduced a multi-path region mining module, which can
    provide pseudo point-level labels by a classification network over weak labels.
    The segmentation network is then trained with these pseudo labels in a fully supervised
    manner.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 基于点级标签生成劳动密集且耗时的事实，一些方法探索了弱监督分割。Xu 和 Lee [[118](#bib.bib118)] 提出了一个弱监督方法，该方法只需要在训练阶段对小部分点进行标记。通过学习几何和颜色中的梯度近似和光滑性约束，可以使用仅标记
    10% 的点获得具有竞争力的结果。另一方面，Wei 等人 [[108](#bib.bib108)] 介绍了一个多路径区域挖掘模块，该模块可以通过一个分类网络对弱标签提供伪点级标签。然后，分割网络通过这些伪标签以完全监督的方式进行训练。
- en: 4.3 Instance Segmentation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实例分割
- en: 'Instance segmentation, compared with semantic segmentation, requires distinguishing
    points with same semantic meaning, which makes the task more challenging. In this
    section, instance segmentation methods are further divided into two categories:
    proposal-based methods and proposal-free methods.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于语义分割，实例分割需要区分具有相同语义的点，这使得任务更加具有挑战性。在这一部分，实例分割方法进一步分为两类：基于提议的方法和无提议的方法。
- en: 4.3.1 Proposal-based methods
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 基于提议的方法
- en: Proposal-based instance segmentation methods can be considered as the combination
    of object detection and mask prediction. 3D-SIS [[35](#bib.bib35)] is a fully
    convolutional network for 3D semantic instance segmentation where geometry and
    color signals are fused. For each image, 2D features for each pixel are extracted
    by a series of 2D convolutional layers, and then backprojected to the associated
    3D voxel grids. The geometry and color features are passed through a series of
    3D convolutional layers respectively and concatenated into a global semantic feature
    map. Then a 3D-RPN and a 3D-RoI layer are applied to generate bounding boxes,
    instance masks and object labels. Generative Shape Proposal Network (GSPN) [[126](#bib.bib126)]
    generates proposals by reconstructing shapes from the scene instead of directly
    regresses bounding boxes. The generated proposals are refined with a region-based
    PointNet (R-PointNet), and the labels are determined with a point-wise binary
    mask prediction over all class labels. 3D-BoNet [[124](#bib.bib124)] is a single-stage
    method that adapts PointNet++ [[80](#bib.bib80)] as backbone network to global
    features and local features at each point. Two prediction branches follow to generate
    instance-level bounding box and point-level mask respectively. Zhang el al. [[134](#bib.bib134)]
    proposed a method for large-scale outdoor point clouds. The point cloud is first
    encoded into a high-resolution BEV representation augmented by KNN, and features
    are then extracted by voxel feature encoding (VFE) layers and self-attention blocks.
    For each grid, a horizontal object center and its height limit are predicted,
    objects that are closed enough will be merged, and eventually these constraints
    will be leveraged to generate instance prediction.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提议的实例分割方法可以被视为目标检测和掩模预测的结合。3D-SIS [[35](#bib.bib35)] 是一个用于 3D 语义实例分割的全卷积网络，其中几何和颜色信号被融合。对于每张图像，通过一系列
    2D 卷积层提取每个像素的 2D 特征，然后将这些特征反向投影到相关的 3D 体素网格上。几何特征和颜色特征分别通过一系列 3D 卷积层，然后拼接成一个全局语义特征图。接着，应用
    3D-RPN 和 3D-RoI 层生成边界框、实例掩模和目标标签。生成形状提议网络 (GSPN) [[126](#bib.bib126)] 通过从场景中重建形状而不是直接回归边界框来生成提议。生成的提议通过基于区域的
    PointNet (R-PointNet) 进行细化，标签则通过对所有类别标签进行逐点二值掩模预测来确定。3D-BoNet [[124](#bib.bib124)]
    是一个单阶段方法，它将 PointNet++ [[80](#bib.bib80)] 作为骨干网络来处理每个点的全局特征和局部特征。随后，两个预测分支分别生成实例级边界框和点级掩模。张等人
    [[134](#bib.bib134)] 提出了一个针对大规模户外点云的方法。点云首先被编码成高分辨率的 BEV 表示，并通过 KNN 进行增强，然后通过体素特征编码
    (VFE) 层和自注意力块提取特征。对于每个网格，预测一个水平的目标中心及其高度限制，相互足够接近的物体将被合并，最终利用这些约束生成实例预测。
- en: 4.3.2 Proposal-free methods
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 无提议的方法
- en: Proposal-free methods tend to generate instance-level label based on semantic
    segmentation by algorithms like clustering. Similarity Group Proposal Network
    (SGPN) [[101](#bib.bib101)] is a representative work that learns a feature and
    semantic map for each point, and a similarity matrix to estimate the similarity
    between pairs of features. A heuristic non-maximal suppression method follows
    to merge points into instances. Lahoud et al. [[48](#bib.bib48)] adopted multi-task
    metric learning to (1) learn a feature embedding such that voxels with the same
    instance label are close and those with different labels are separated in the
    feature space and (2) predict the shape of instance at each voxel. Instance boundaries
    are estimated with mean-shift clustering and NMS.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 无提议的方法倾向于通过类似聚类的算法基于语义分割生成实例级标签。Similarity Group Proposal Network (SGPN) [[101](#bib.bib101)]
    是一种代表性工作，它为每个点学习特征和语义图，并通过相似性矩阵来估计特征对之间的相似性。然后采用启发式的非极大值抑制方法将点合并成实例。Lahoud 等人
    [[48](#bib.bib48)] 采用了多任务度量学习来（1）学习特征嵌入，使得具有相同实例标签的体素在特征空间中彼此接近，而具有不同标签的体素被分开，以及（2）预测每个体素上的实例形状。实例边界通过均值漂移聚类和
    NMS 进行估计。
- en: Zhang et al. [[133](#bib.bib133)] introduced a probabilistic embedding to encode
    point clouds. The embedding is implemented with multivariate Gaussian distribution,
    and the Bhattacharyya kernel is adopted to esimate the similarity between points.
    Proposal-free methods do not suffer from the computational complexity of region-proposal
    layers; however, it is usually difficult for them to produce discriminative object
    boundaries from clustering.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人 [[133](#bib.bib133)] 引入了概率嵌入来编码点云。该嵌入通过多变量高斯分布实现，采用 Bhattacharyya 核来估计点之间的相似性。无提议的方法不会遭受区域提议层的计算复杂性；然而，它们通常很难从聚类中产生具有辨别力的物体边界。
- en: There are also several instance segmentation methods based on projection. SqueezeSeg
    [[109](#bib.bib109)] is one of the pioneer works in this direction. In this method,
    points are first projected onto a sphere for a grid-based representation. The
    transformed representation is of size $H\times W\times C$, where in practice $H$=64
    is the number of vertical channels of lidar, $W$ is manually picked to be 512,
    and $C$ equals to 5 (3 dimensional coordinates + intensity measurement + range).
    The representation is then fed through a conventional 2D CNN and a conditional
    random field (CRF) for refined segmentation results. This method is afterwards
    improved by SqueezeSegv2 [[110](#bib.bib110)] with a context aggregation module
    and a domain adaptation pipeline.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 基于投影的实例分割方法也有几种。SqueezeSeg [[109](#bib.bib109)] 是这个方向的开创性工作之一。在这种方法中，点首先被投影到一个球面上以进行网格化表示。变换后的表示的大小为
    $H\times W\times C$，其中实际中 $H$=64 是激光雷达的垂直通道数量，$W$ 手动选定为 512，而 $C$ 等于 5（3 维坐标 +
    强度测量 + 距离）。然后，该表示通过常规的 2D CNN 和条件随机场（CRF）进行细化分割结果。之后，该方法通过 SqueezeSegv2 [[110](#bib.bib110)]
    得到改进，添加了上下文聚合模块和领域适应管道。
- en: The idea of projection-based methods is further explored by Lyu et al. [[66](#bib.bib66)].
    Inspired by graph drawing algorithms, they proposed a hierarchical approximate
    algorithm to project point clouds into image representations with abundant local
    geometric information preserved. The segmentation will then be generated by a
    multi-scale U-Net from the image representation. With this innovative projection
    algorithm, the method obtained significant improvement.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Lyu 等人 [[66](#bib.bib66)] 进一步探讨了基于投影的方法。受图形绘制算法的启发，他们提出了一种分层近似算法，将点云投影到图像表示中，同时保留了丰富的局部几何信息。然后，通过从图像表示中生成的多尺度
    U-Net 来进行分割。借助这种创新的投影算法，该方法获得了显著的改进。
- en: PointGroup [[43](#bib.bib43)] proposed a bottom-up framework with two prediction
    branches. For each point, its semantic label and relative offset to its respective
    instance centroid are predicted. The offset branch helps better grouping of points
    into objects as well as separation of objects with the same semantic label. During
    the clustering stage, both original positions and shifted positions are considered,
    the association of these two results turns out to have a better performance. Along
    with NMS based on the newly designed ScoreNet, this method outperforms other works
    of the day by a great margin.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: PointGroup [[43](#bib.bib43)] 提出了一个自下而上的框架，其中包含两个预测分支。对于每个点，预测其语义标签及相对于其实例质心的相对偏移。偏移分支有助于将点更好地分组到对象中，并且能够分离具有相同语义标签的对象。在聚类阶段，考虑了原始位置和偏移位置，这两种结果的关联表现出更好的性能。结合基于新设计的ScoreNet的NMS，这种方法比当时的其他工作有很大优势。
- en: 4.4 Joint Training
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 联合训练
- en: As mentioned above, some recent works jointly address more than one problems
    to better realized the power of models. The unsupervised multi-task approach proposed
    by Hassani and Haley [[31](#bib.bib31)] is an example in which clustering, self-supervised
    classification and reconstruction are jointly trained. The two tasks under segmentation,
    semantic segmentation and instance segmentation, are also proven to likely benefit
    from simultaneous training.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，一些最新的工作联合解决多个问题，以更好地实现模型的强大功能。Hassani和Haley [[31](#bib.bib31)] 提出的无监督多任务方法是一个例子，其中聚类、自监督分类和重建被联合训练。语义分割和实例分割这两个分割任务也被证明可能从同时训练中受益。
- en: 'There are two naive ways to solve semantic segmentation and instance segmentation
    at the same time: (1) solve semantic segmentation first, run instance segmentation
    on points of certain labels based on the result of semantic segmentation, (2)
    solve instance segmentation first, and directly assign semantic labels with instance
    labels. These two step-wise paradigms highly depend on the output quality of the
    first step, and are not able to make full use of the shared information between
    two tasks.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 同时解决语义分割和实例分割有两种简单的方法：（1）首先解决语义分割，然后在语义分割结果的基础上对某些标签的点进行实例分割，（2）首先解决实例分割，然后直接用实例标签分配语义标签。这两种逐步范式高度依赖于第一步的输出质量，无法充分利用两个任务之间的共享信息。
- en: JSIS3D [[74](#bib.bib74)] develops a pointwise network that predicts the semantic
    label of each point and high-dimensional embeddings at the same time. After these
    steps, instances of the same class will have similar embeddings, then a multi-value
    conditional random field model is applied to synthesize semantic and instance
    labels, formulating the problem as jointly optimizing labels in the field model.
    ASIS [[103](#bib.bib103)] is another method that makes the two tasks benefit from
    each other. Specifically, instance segmentation benefits from semantic segmentation
    by learning semantic-aware instance embedding at point level, while semantic features
    of the point set from the same instance will be fused together to generate accurate
    semantic predictions for every point.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: JSIS3D [[74](#bib.bib74)] 开发了一种点对点网络，该网络同时预测每个点的语义标签和高维嵌入。经过这些步骤后，同一类别的实例将具有相似的嵌入，然后应用多值条件随机场模型来合成语义和实例标签，将问题表述为在场模型中共同优化标签。ASIS
    [[103](#bib.bib103)] 是另一种让这两个任务相互受益的方法。具体而言，实例分割通过在点级别学习语义感知实例嵌入来受益于语义分割，同时来自同一实例的点集的语义特征将被融合在一起，以生成每个点的准确语义预测。
- en: 4.5 Experiments
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 实验
- en: 'We select the benchmarks on which most methods are tested, S3DIS[[1](#bib.bib1)],
    to compare the performance of different methods. The performances are summarized
    in Table [4](#S4.T4 "Table 4 ‣ 4.5 Experiments ‣ 4 Segmentation ‣ Deep Learning
    for 3D Point Cloud Understanding: A Survey").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '我们选择了大多数方法测试的基准，S3DIS[[1](#bib.bib1)]，以比较不同方法的性能。这些性能总结在表格[4](#S4.T4 "Table
    4 ‣ 4.5 Experiments ‣ 4 Segmentation ‣ Deep Learning for 3D Point Cloud Understanding:
    A Survey")中。'
- en: 'Table 4: Experiment results on on semantic segmentation in S3DIS benchmark.
    Only results that are reported in the original papers are listed, those reported
    as a reference by other papers are excluded because they are sometimes conflicting.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表格4：S3DIS基准上语义分割的实验结果。仅列出了原始论文中报告的结果，那些被其他论文作为参考报告的结果被排除，因为它们有时会产生冲突。
- en: '| Methods | Area5(mACC) | Area5(mIoU) | 6-fold(mACC) | 6-fold(mIoU) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Area5(mACC) | Area5(mIoU) | 6-fold(mACC) | 6-fold(mIoU) |'
- en: '| PointCNN [[58](#bib.bib58)] | 63.9 | 57.3 | 75.6 | 65.4 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| PointCNN [[58](#bib.bib58)] | 63.9 | 57.3 | 75.6 | 65.4 |'
- en: '| PointWeb [[136](#bib.bib136)] | 66.6 | 60.3 | 76.2 | 66.7 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| PointWeb [[136](#bib.bib136)] | 66.6 | 60.3 | 76.2 | 66.7 |'
- en: '| A-CNN [[112](#bib.bib112)] | - | - | - | 62.9 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| A-CNN [[112](#bib.bib112)] | - | - | - | 62.9 |'
- en: '| DGCNN [[105](#bib.bib105)] | - | - | - | 56.1 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN [[105](#bib.bib105)] | - | - | - | 56.1 |'
- en: '| VV-Net [[68](#bib.bib68)] | - | - | 82.2 | 78.2 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| VV-Net [[68](#bib.bib68)] | - | - | 82.2 | 78.2 |'
- en: '| PCCN [[100](#bib.bib100)] | - | 58.3 | - | - |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| PCCN [[100](#bib.bib100)] | - | 58.3 | - | - |'
- en: '| GAC [[98](#bib.bib98)] | - | 62.9 | - | - |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| GAC [[98](#bib.bib98)] | - | 62.9 | - | - |'
- en: '| DPC [[17](#bib.bib17)] | 68.4 | 61.3 | - | - |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| DPC [[17](#bib.bib17)] | 68.4 | 61.3 | - | - |'
- en: '| SSP+SPG [[51](#bib.bib51)] | - | - | 78.3 | 68.4 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| SSP+SPG [[51](#bib.bib51)] | - | - | 78.3 | 68.4 |'
- en: '| JSIS3D [[74](#bib.bib74)] | - | - | 78.6 | - |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| JSIS3D [[74](#bib.bib74)] | - | - | 78.6 | - |'
- en: '| ASIS [[103](#bib.bib103)] | 60.9 | 53.4 | 70.1 | 59.3 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ASIS [[103](#bib.bib103)] | 60.9 | 53.4 | 70.1 | 59.3 |'
- en: '| Xu and Lee [[118](#bib.bib118)] | - | 48.0 | - | - |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Xu 和 Lee [[118](#bib.bib118)] | - | 48.0 | - | - |'
- en: '| RandLA-Net [[36](#bib.bib36)] | - | - | 82.0 | 70.0 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| RandLA-Net [[36](#bib.bib36)] | - | - | 82.0 | 70.0 |'
- en: '| Tatarchenko et al. [[96](#bib.bib96)] | 62.2 | 52.8 | - | - |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Tatarchenko 等人 [[96](#bib.bib96)] | 62.2 | 52.8 | - | - |'
- en: 5 Detection, Tracking and Flow Estimation
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 检测、跟踪和流动估计
- en: 5.1 Overview
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 概述
- en: 'Object detection is a recent research hotspot as the basis of many practical
    applications. It aims to locate all the objects in the given scene. 3D object
    detection methods can be generally divided into three categories: multi-view methods,
    projection-based methods and point-based methods. Figure 4.1 shows an example
    of 3D object detection on multiple (lidar and camera) views. Aside from image
    object detection models, the exclusive characteristics of point cloud data provide
    more potential of optimization. Also, since 3D object tracking and scene flow
    estimation are two derivative tasks that highly depend on object detection, they
    will be discussed together in this section.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测作为许多实际应用的基础，已成为近期研究的热点。其目标是定位给定场景中的所有物体。3D 物体检测方法通常可以分为三类：多视角方法、基于投影的方法和基于点的方法。图
    4.1 显示了一个在多个（激光雷达和摄像头）视角下进行的 3D 物体检测的示例。除了图像物体检测模型外，点云数据的独特特征提供了更多的优化潜力。此外，由于
    3D 物体跟踪和场景流估计是两个高度依赖于物体检测的衍生任务，因此在本节中将一起讨论这两个任务。
- en: '![Refer to caption](img/f57b0ace156d53be1ac72c2d49edf78c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f57b0ace156d53be1ac72c2d49edf78c.png)'
- en: 'Figure 4: An outdoor scene from nuScenes [[4](#bib.bib4)], annotations in multi-view
    (lidar/camera) are provided.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：来自 nuScenes [[4](#bib.bib4)] 的户外场景，提供了多视角（激光雷达/摄像头）注释。
- en: 5.2 Object Detection
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 物体检测
- en: 5.2.1 Projection-based methods
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 基于投影的方法
- en: The success of convolutional neural networks in image object detection inspired
    attempts to apply 3D CNN on projected point cloud data. VoxelNet [[138](#bib.bib138)]
    proposed an approach that applies random sampling to the point set within each
    voxel, and passes them through a novel voxel feature encoding (VFE) layer based
    on PointNet [[79](#bib.bib79)] and PointNet++ [[80](#bib.bib80)] to extract point-wise
    features. A region proposal network is used to produce detection results. Similar
    to classification models with volumetric representation, VoxelNet runs at a relatively
    low speed due to the sparsity of voxels and 3D convolutions. SECOND [[119](#bib.bib119)]
    then proposed an improvement in inference efficiency by taking advantage of sparse
    convolution network.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络在图像物体检测中的成功激发了在投影点云数据上应用 3D CNN 的尝试。VoxelNet [[138](#bib.bib138)] 提出了一个方法，该方法对每个体素中的点集进行随机采样，并通过基于
    PointNet [[79](#bib.bib79)] 和 PointNet++ [[80](#bib.bib80)] 的新型体素特征编码（VFE）层提取逐点特征。区域提议网络用于生成检测结果。类似于具有体积表示的分类模型，由于体素的稀疏性和
    3D 卷积，VoxelNet 的运行速度相对较慢。SECOND [[119](#bib.bib119)] 随后通过利用稀疏卷积网络提出了提高推理效率的方法。
- en: PointPillars [[53](#bib.bib53)] utilizes point cloud data in another way. Points
    are organized in vertical columns (called Pillars), and the features of pillars
    are extracted with PointNet to generate a pseudo image. The pseudo image is then
    considered as the input of a 2D object detection pipeline to predict 3D bounding
    boxes. PointPillars is more accurate than previous fusion approaches, and it is
    capable of real-time applications with a running speed of 62 FPS. Wang et al.
    [[104](#bib.bib104)] further proposed another anchor-free bounding box prediction
    based on a cylindrical projection into multi-view features.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: PointPillars [[53](#bib.bib53)] 以另一种方式利用点云数据。点被组织成垂直列（称为Pillars），然后使用PointNet提取Pillars的特征以生成伪图像。伪图像随后被作为2D目标检测流程的输入，以预测3D边界框。PointPillars比之前的融合方法更准确，并且以62
    FPS的运行速度能够进行实时应用。Wang等人 [[104](#bib.bib104)] 进一步提出了另一种基于圆柱投影到多视角特征的无锚框边界框预测方法。
- en: Projection-based methods suffer from spatial information loss inevitably. Aside
    from using point-based networks instead, He et al. [[32](#bib.bib32)] proposed
    a structure-aware method to mitigate the problem. The convolutional layers are
    explicitly supervised to contain structural information by an auxiliary network.
    The auxiliary network converts the convolutional features from the backbone network
    to point-level representations and is jointly optimized. After the training process
    is finished, the auxiliary network can be detached to speed up the inference.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 基于投影的方法不可避免地存在空间信息丢失的问题。除了使用基于点的网络之外，He等人 [[32](#bib.bib32)] 提出了一个结构感知方法来缓解这个问题。卷积层通过辅助网络显式地监督以包含结构信息。辅助网络将来自骨干网络的卷积特征转换为点级表示，并进行联合优化。训练过程完成后，辅助网络可以被拆除以加速推理过程。
- en: 5.2.2 Point-based methods
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 基于点的方法
- en: Most point-based methods attempt to minimize information loss during feature
    extraction, and they are the group with the best performance so far. STD [[123](#bib.bib123)]
    introduced the idea of using sphere anchors for proposal generation, which achieves
    a high recall with significantly less computation than previous methods. Each
    proposal is passed through a PointsPool layer that converts proposal features
    from sparse expression to compact representation, and is robust under transformation.
    In addition to the regular regression branch, STD has another IoU branch to replace
    the role of classification score in NMS.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于点的方法尝试在特征提取过程中最小化信息损失，它们是迄今为止表现最佳的一组方法。STD [[123](#bib.bib123)] 引入了使用球形锚点进行提案生成的概念，该方法在计算量显著减少的情况下实现了高召回率。每个提案通过一个PointsPool层，该层将提案特征从稀疏表达转换为紧凑表示，并且在变换下具有鲁棒性。除了常规的回归分支外，STD还具有另一个IoU分支，以替代NMS中的分类分数角色。
- en: Some methods use foreground-background classification to improve the quality
    of proposals. PointRCNN [[90](#bib.bib90)] is such a framework, in which points
    are directly segmented to screen out foreground points, while semantic features
    and spatial features are then fused to produce high-quality 3D boxes. Compared
    with multi-view methods above, segmentation-based methods perform better for complicated
    scenes and occluded objects.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法使用前景-背景分类来提高提案的质量。PointRCNN [[90](#bib.bib90)] 就是这样一个框架，其中点被直接分割以筛选出前景点，然后融合语义特征和空间特征以生成高质量的3D框。与上述多视角方法相比，基于分割的方法在复杂场景和遮挡物体上的表现更好。
- en: Furthermore, Qi et al. proposed VoteNet [[77](#bib.bib77)]. A group of points
    are sampled as seeds, and each seed independently generates a vote for potential
    center points of objects in the point cloud with the help of PointNet++ [[80](#bib.bib80)].
    By taking advantage of voting, VoteNet outperforms previous approaches on two
    large indoor benchmarks. However, as the center point prediction of virtual center
    points is not as stable, the method performs less satisfactorily in wild scenes.
    As a follow-up work, ImVoteNet [[76](#bib.bib76)] inherited the idea of VoteNet
    and achieved prominent improvement by fusing 3D votes with 2D votes from images.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Qi等人提出了VoteNet [[77](#bib.bib77)]。一组点被采样作为种子，每个种子利用PointNet++ [[80](#bib.bib80)]
    独立生成对点云中物体潜在中心点的投票。通过利用投票机制，VoteNet在两个大型室内基准测试上超越了以前的方法。然而，由于虚拟中心点的中心点预测不够稳定，该方法在野外场景中的表现不尽如人意。作为后续工作，ImVoteNet
    [[76](#bib.bib76)] 继承了VoteNet的思想，通过将3D投票与来自图像的2D投票融合，取得了显著的改进。
- en: There are also attempts that consider domain knowledge as an auxiliary to enhance
    features. Associate-3Ddet [[15](#bib.bib15)] introduced the idea of perceptual-to-conceptual
    association. To enrich perception features that might be incomplete due to occlusion
    or sparsity, a perceptual-to-conceptual module is proposed to generate class-wise
    conceptual models from the dataset. The perception and conceptual features will
    be associated for feature enhancement.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 也有尝试将领域知识作为辅助以增强特征。Associate-3Ddet [[15](#bib.bib15)] 引入了感知到概念的关联思想。为了丰富由于遮挡或稀疏性可能不完整的感知特征，提出了一种感知到概念的模块，从数据集中生成类别概念模型。感知特征和概念特征将被关联以增强特征。
- en: Yang et al. [[122](#bib.bib122)] proposed a point-based anchor-free method 3DSSD.
    This method attempts to reduce computation by abandoning the upsampling layers
    (e.g. feature propagation layers in [[123](#bib.bib123)]) and refinement stages
    that are widely used in previous point-based methods. Previous set abstraction
    layers for downsampling only leverage furthest-point-sampling based on Euclidean
    distance (D-FPS), instances with a small number of interior points are easily
    lost under this strategy. In this case, removing upsampling layers could lead
    to huge performance drop. 3DSSD proposed F-FPS, a new sampling strategy based
    on feature distances, to preserve more foreground points for instances. The fusion
    of F-FPS and D-FPS, together with the candidate generation layer and 3D center-ness
    assignment in the prediction head, help this method outperform previous single-stage
    methods with a considerable margin.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Yang 等人 [[122](#bib.bib122)] 提出了基于点的无锚点方法 3DSSD。该方法尝试通过放弃以前点基方法中广泛使用的上采样层（例如
    [[123](#bib.bib123)] 中的特征传播层）和细化阶段来减少计算。以前的集抽象层用于下采样，仅利用基于欧几里得距离的最远点采样（D-FPS），在这种策略下，具有较少内部点的实例容易丢失。在这种情况下，去除上采样层可能会导致性能大幅下降。3DSSD
    提出了 F-FPS，一种基于特征距离的新采样策略，以保留更多前景点。F-FPS 和 D-FPS 的融合，加上候选生成层和预测头中的 3D 中心度分配，使该方法在性能上明显超过以前的单阶段方法。
- en: Graph neural networks have also been introduced to 3D object detection for its
    ability to accommodate intrinsic characteristics of point clouds like sparsity.
    PointRGCN [[130](#bib.bib130)] is an early work that introduce graph-based representation
    for 3D vehicle detection refinement. After that, HGNet [[7](#bib.bib7)] introduces
    a hierarchical graph network based on shape-attentive graph convolution (SA-GConv).
    By capturing object shapes with relative geometric information and reasoning on
    proposals, the method obtained a significant improvement on previous results.
    Besides, Point-GNN [[91](#bib.bib91)] proposed a single-shot method based on graph
    neural networks. It first builds a fixed radius near-neighbors graph over the
    input point cloud. Then, the category and the bounding box of affiliation are
    predicted with the point graph. Finally, a box merging and scoring operation is
    used to obtain accurate combination of detection results from multiple vertices.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络也被引入到 3D 目标检测中，以适应点云的内在特征，如稀疏性。PointRGCN [[130](#bib.bib130)] 是一项早期工作，引入了基于图的表示用于
    3D 车辆检测的细化。之后，HGNet [[7](#bib.bib7)] 引入了一种基于形状注意图卷积（SA-GConv）的层次图网络。通过捕捉具有相对几何信息的物体形状并对提议进行推理，该方法在以前的结果上获得了显著改进。此外，Point-GNN
    [[91](#bib.bib91)] 提出了基于图神经网络的单次方法。它首先在输入点云上构建一个固定半径的邻居图。然后，利用点图预测类别和隶属的边界框。最后，使用框合并和评分操作来获得多个顶点的检测结果的准确组合。
- en: 5.2.3 Multi-view methods
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 多视角方法
- en: MV3D [[8](#bib.bib8)] is a pioneering method in multi-view object detection
    methods on point clouds. In this approach, candidate boxes are generated from
    BEV map and projected into feature maps of multiple views (RGB images, lidar data,
    etc.), then the region-wise features extracted from different views are combined
    to produce the final oriented 3D bounding boxes. While this approach achieves
    satisfactory performance, much like many other early multi-view methods, its running
    speed is too slow for practical use.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: MV3D [[8](#bib.bib8)] 是点云上多视角目标检测方法中的开创性方法。在这种方法中，从 BEV 图生成候选框，并投影到多个视角的特征图（RGB
    图像、激光雷达数据等）中，然后将从不同视角提取的区域特征结合起来生成最终的定向 3D 边界框。尽管这种方法实现了令人满意的性能，但与许多其他早期多视角方法类似，其运行速度对于实际应用来说仍然过于缓慢。
- en: Attempts to improve multi-view methods generally take one of two directions.
    First, we could find a more efficient way to fuse information from different views.
    Liang et al. [[59](#bib.bib59)] use continuous convolutions to effectively fuse
    feature maps from images and lidar at different resolutions. Image features for
    each point in BEV space are utilized to generate a dense BEV feature map by bi-linear
    interpolation with projections of image features within the BEV plane. Experiments
    show that dense BEV feature maps perform better than discrete image feature maps
    and sparse point cloud feature maps. Second, many methods propose innovative feature
    extraction approaches to obtain representations of input data with higher robustness.
    SCANet [[64](#bib.bib64)] introduced a Spatial Channel Attention (SCA) module
    to make use of multi-scale contextual information. The SCA module captures useful
    features from the global and multi-scale context of given scene, while an Extension
    Spatial Unsample (ESU) module helps combine multi-scale low-level features to
    generate high-level features with rich spatial information, which then leads to
    accurate 3D object proposals. In RT3D [[132](#bib.bib132)], the majority of convolution
    operations prior to the RoI pooling module are removed. With such optimization,
    RoI convolutions only need to be performed once for all proposals, accelerating
    the method to run at 11.1 FPS, which is five times faster than MV3D [[8](#bib.bib8)].
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 改进多视角方法的尝试通常有两个方向。首先，我们可以找到一种更有效的方式来融合来自不同视角的信息。Liang 等人 [[59](#bib.bib59)]
    使用连续卷积来有效融合来自图像和激光雷达的不同分辨率的特征图。利用 BEV 空间中每个点的图像特征，通过与 BEV 平面中图像特征的投影进行双线性插值，生成密集的
    BEV 特征图。实验表明，密集 BEV 特征图比离散图像特征图和稀疏点云特征图表现更好。其次，许多方法提出了创新的特征提取方法，以获得更高鲁棒性的输入数据表示。SCANet
    [[64](#bib.bib64)] 引入了一个空间通道注意力 (SCA) 模块，以利用多尺度上下文信息。SCA 模块从给定场景的全局和多尺度上下文中捕获有用特征，同时，扩展空间下采样
    (ESU) 模块有助于结合多尺度低级特征，生成具有丰富空间信息的高级特征，从而带来准确的3D物体提案。在 RT3D [[132](#bib.bib132)]
    中，RoI 池化模块之前的大多数卷积操作被移除。通过这种优化，RoI 卷积仅需对所有提案执行一次，从而将方法的运行速度加快至 11.1 FPS，比 MV3D
    [[8](#bib.bib8)] 快五倍。
- en: Another approach to detect 3D objects is to generate candidate regions on 2D
    plane with 2D object detectors, then extract a 3D frustum proposal for each 2D
    candidate region. In F-PointNets [[78](#bib.bib78)], each 2D region generates
    a frustum proposal, and the features of each 3D frustum are learned with PointNet
    [[79](#bib.bib79)] or PointNet++ [[80](#bib.bib80)] and used for 3D bounding box
    estimation. PointFusion [[117](#bib.bib117)] uses both 2D image region and corresponding
    frustum points for more accurate 3D box regression. A fusion network is proposed
    to directly predict corner locations of boxes by fusing image features and global
    features from point clouds.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 检测3D物体的另一种方法是通过2D物体检测器在2D平面上生成候选区域，然后为每个2D候选区域提取一个3D截锥体提案。在 F-PointNets [[78](#bib.bib78)]中，每个2D区域生成一个截锥体提案，每个3D截锥体的特征通过
    PointNet [[79](#bib.bib79)] 或 PointNet++ [[80](#bib.bib80)] 学习，并用于3D边界框估计。PointFusion
    [[117](#bib.bib117)] 使用2D图像区域和相应的截锥体点以实现更准确的3D框回归。提出了一种融合网络，通过融合图像特征和来自点云的全局特征来直接预测框的角点位置。
- en: 5.3 Object Tracking
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 目标跟踪
- en: Object tracking targets estimating the location of a certain object in subsequent
    frames given its state in the first frame. The success of Siamese networks [[2](#bib.bib2)]
    in 2D image object tracking inspired 3D object tracking, and Giancola et al. [[24](#bib.bib24)]
    extend Siamese networks to 3D. In this method, candidates are first generated
    by a Kalman filter, then passed through an encoding model to generate compact
    representations with shape regularization, and match the detected objects by cosine
    similarity. Zarzar et al. [[130](#bib.bib130)] proposed another method that captures
    target objects more efficiently by leveraging a 2D Siamese network to detect coarse
    object candidates on BEV representation. The coarse candidates are then refined
    by cosine similarity in the 3D Siamese network.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 目标跟踪的目标是在给定第一帧中的状态的情况下，估计随后的帧中某个物体的位置。Siamese 网络 [[2](#bib.bib2)] 在2D图像目标跟踪中的成功激发了3D目标跟踪，Giancola
    等人 [[24](#bib.bib24)] 将 Siamese 网络扩展到 3D。在这种方法中，候选物体首先由卡尔曼滤波器生成，然后通过编码模型生成具有形状正则化的紧凑表示，并通过余弦相似度匹配检测到的物体。Zarzar
    等人 [[130](#bib.bib130)] 提出了一种方法，通过利用2D Siamese 网络在 BEV 表示上检测粗略的物体候选区域，更有效地捕获目标物体。粗略候选区域随后通过3D
    Siamese 网络中的余弦相似度进行精细化。
- en: Chiu et al. [[9](#bib.bib9)] introduced the Kalman filter to encode the hidden
    states of objects. The state of an object is represented by a tuple of 11 variables,
    including position, orientation, size and speed. A Kalman filter is adopted to
    predict the object in next frame based on previous information, and a greedy algorithm
    is used for data association with Mahalanobis distance.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Chiu 等人 [[9](#bib.bib9)] 引入了卡尔曼滤波器来编码对象的隐藏状态。对象的状态由 11 个变量的元组表示，包括位置、方向、大小和速度。卡尔曼滤波器用于基于先前的信息预测下一帧中的对象，并使用贪婪算法进行与马氏距离的数据关联。
- en: Besides, Qi et al. [[81](#bib.bib81)] proposed P2B, a point-to-box method for
    3D object tracking. It divides the task into two parts. The first part is target-specific
    feature augmentation, seeds from the template and the search area are generated
    with a PointNet++ backbone, and the search area seeds will be enriched with target
    clues from the template. The second is target proposal and verification, candidate
    target centers are regressed and seed-wise targetness is evaluated for joint target
    proposal and verification.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Qi 等人 [[81](#bib.bib81)] 提出了 P2B，一种用于 3D 目标跟踪的点到框方法。它将任务分为两个部分。第一部分是目标特定的特征增强，使用
    PointNet++ 骨干网络生成来自模板和搜索区域的种子，并用来自模板的目标线索丰富搜索区域的种子。第二部分是目标提议和验证，对候选目标中心进行回归，并评估种子级目标性以进行联合目标提议和验证。
- en: 5.4 Scene Flow Estimation
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 场景流估计
- en: Similar to optical flow estimation on images, 3D scene flow estimation works
    on a sequence of point clouds. FlowNet3D [[60](#bib.bib60)] is a representative
    work that directly estimates scene flows from pairs of consecutive point clouds.
    The flow embedding layer is used to learn point-level features and motion features.
    The experiment results of FlowNet3D shows that it performs less than satisfactorily
    in non-static scenes, and the angles of predicted motion vectors sometimes significantly
    differ from the ground truth. FlowNet3D++ [[107](#bib.bib107)] is proposed to
    fix these issues by introducing a cosine distance loss in angles, and a point-to-plane
    distance loss to improve accuracy in dynamic scenes. HPLFlowNet [[28](#bib.bib28)],
    on the other hand, proposed a series of bilateral convolutional layers to fuse
    information from two consecutive frames and restore structural information from
    unconstructed point clouds.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于图像上的光流估计，3D场景流估计处理的是一系列点云。FlowNet3D [[60](#bib.bib60)] 是一个代表性工作，它直接从连续点云对中估计场景流。流嵌入层用于学习点级特征和运动特征。FlowNet3D
    的实验结果表明，在非静态场景中表现不尽如人意，预测的运动向量角度有时与实际值有显著差异。FlowNet3D++ [[107](#bib.bib107)] 通过引入角度的余弦距离损失和点到平面距离损失来解决这些问题，以提高动态场景中的准确性。另一方面，HPLFlowNet
    [[28](#bib.bib28)] 提出了一系列双边卷积层，用于融合来自两个连续帧的信息，并从未构建的点云中恢复结构信息。
- en: In addition, MeteorNet [[61](#bib.bib61)] introduced direct grouping and chained-flow
    grouping to group temporal neighbors, and adopted information aggregation over
    neighbor points to generate representation for dynamic scenes. Derived from recurrent
    models in images, Fan and Yang [[19](#bib.bib19)] proposed PointRNN, PointGRU
    and PointLSTM to encode dynamic point clouds by capturing both spatial and temporary
    information.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MeteorNet [[61](#bib.bib61)] 引入了直接分组和链式流分组以分组时间邻居，并采用邻居点上的信息聚合来生成动态场景的表示。Fan
    和 Yang [[19](#bib.bib19)] 受到图像中递归模型的启发，提出了 PointRNN、PointGRU 和 PointLSTM，通过捕捉空间和时间信息来编码动态点云。
- en: 5.5 Experiments
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 实验
- en: 'KITTI [[22](#bib.bib22), [23](#bib.bib23), [21](#bib.bib21), [69](#bib.bib69)]
    is one of the most popular benchmarks for many computer vision tasks, including
    those in images, point clouds, and multi-views. By taking advantage of autonomous
    driving platforms, KITTI provides raw data of real-world scenes, and allows evaluation
    on multiple tasks. Table [5](#S5.T5 "Table 5 ‣ 5.5 Experiments ‣ 5 Detection,
    Tracking and Flow Estimation ‣ Deep Learning for 3D Point Cloud Understanding:
    A Survey") shows experimental results of different methods on KITTI. Some methods,
    such as VoteNet [[14](#bib.bib14)], which does not provide detailed test results
    on KITTI, are not listed.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 'KITTI [[22](#bib.bib22), [23](#bib.bib23), [21](#bib.bib21), [69](#bib.bib69)]
    是许多计算机视觉任务中最受欢迎的基准之一，包括图像、点云和多视图任务。借助自动驾驶平台，KITTI 提供了真实场景的原始数据，并允许在多个任务上进行评估。表
    [5](#S5.T5 "Table 5 ‣ 5.5 Experiments ‣ 5 Detection, Tracking and Flow Estimation
    ‣ Deep Learning for 3D Point Cloud Understanding: A Survey") 显示了不同方法在 KITTI 上的实验结果。一些方法，例如
    VoteNet [[14](#bib.bib14)]，由于未提供详细的测试结果，因此未列出。'
- en: 'Table 5: Experiment results on KITTI 3D detection benchmark, E/M/H stands for
    easy/medium/hard samples.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：KITTI 3D检测基准上的实验结果，E/M/H 代表简单/中等/困难样本。
- en: '| Method | Category | Speed | Car | Pedestrians | Cyclists |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类别 | 速度 | 车辆 | 行人 | 骑行者 |'
- en: '| E | M | H | E | M | H | E | M | H |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| E | M | H | E | M | H | E | M | H |'
- en: '| MV3D [[8](#bib.bib8)] | multi-view | 2.8 | 74.8 | 63.6 | 54.0 | - | - | -
    | - | - | - |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| MV3D [[8](#bib.bib8)] | multi-view | 2.8 | 74.8 | 63.6 | 54.0 | - | - | -
    | - | - | - |'
- en: '| AVOD [[47](#bib.bib47)] | multi-view | 12.5 | 89.8 | 85.0 | 78.3 | 42.6 |
    33.6 | 30.1 | 64.1 | 48.1 | 42.4 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| AVOD [[47](#bib.bib47)] | multi-view | 12.5 | 89.8 | 85.0 | 78.3 | 42.6 |
    33.6 | 30.1 | 64.1 | 48.1 | 42.4 |'
- en: '| SCANet [[64](#bib.bib64)] | multi-view | 12.5 | 76.4 | 66.5 | 60.2 | - |
    - | - | - | - | - |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| SCANet [[64](#bib.bib64)] | multi-view | 12.5 | 76.4 | 66.5 | 60.2 | - |
    - | - | - | - | - |'
- en: '| PIXOR [[120](#bib.bib120)] | projection | 28.6 | 84.0 | 80.0 | 74.3 | - |
    - | - | - | - | - |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| PIXOR [[120](#bib.bib120)] | projection | 28.6 | 84.0 | 80.0 | 74.3 | - |
    - | - | - | - | - |'
- en: '| VoxelNet [[138](#bib.bib138)] | projection | 2.0 | 77.5 | 65.1 | 57.7 | 39.5
    | 33.7 | 31.5 | 61.2 | 48.4 | 44.4 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| VoxelNet [[138](#bib.bib138)] | projection | 2.0 | 77.5 | 65.1 | 57.7 | 39.5
    | 33.7 | 31.5 | 61.2 | 48.4 | 44.4 |'
- en: '| SECOND [[119](#bib.bib119)] | projection | 26.3 | 83.3 | 72.6 | 65.8 | 49.0
    | 38.8 | 34.9 | 71.3 | 52.1 | 45.8 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| SECOND [[119](#bib.bib119)] | projection | 26.3 | 83.3 | 72.6 | 65.8 | 49.0
    | 38.8 | 34.9 | 71.3 | 52.1 | 45.8 |'
- en: '| PointPillars [[53](#bib.bib53)] | projection | 62.0 | 82.6 | 74.3 | 69.0
    | 54.5 | 41.2 | 38.9 | 77.1 | 85.7 | 52.0 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| PointPillars [[53](#bib.bib53)] | projection | 62.0 | 82.6 | 74.3 | 69.0
    | 54.5 | 41.2 | 38.9 | 77.1 | 85.7 | 52.0 |'
- en: '| PointRCNN [[90](#bib.bib90)] | point | 10.0 | 87.0 | 75.6 | 70.7 | 48.0 |
    39.4 | 36.0 | 75.0 | 58.8 | 52.5 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| PointRCNN [[90](#bib.bib90)] | point | 10.0 | 87.0 | 75.6 | 70.7 | 48.0 |
    39.4 | 36.0 | 75.0 | 58.8 | 52.5 |'
- en: '| PointRGCN [[130](#bib.bib130)] | point | 3.8 | 86.0 | 95.6 | 70.7 | - | -
    | - | - | - | - |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| PointRGCN [[130](#bib.bib130)] | point | 3.8 | 86.0 | 95.6 | 70.7 | - | -
    | - | - | - | - |'
- en: '| STD [[123](#bib.bib123)] | point | 12.5 | 88.0 | 79.7 | 75.1 | 53.3 | 42.5
    | 38.3 | 78.7 | 61.6 | 55.3 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| STD [[123](#bib.bib123)] | point | 12.5 | 88.0 | 79.7 | 75.1 | 53.3 | 42.5
    | 38.3 | 78.7 | 61.6 | 55.3 |'
- en: '| Point-GNN [[91](#bib.bib91)] | point | - | 88.3 | 79.5 | 72.3 | 52.0 | 43.8
    | 40.1 | 78.6 | 63.5 | 57.0 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Point-GNN [[91](#bib.bib91)] | point | - | 88.3 | 79.5 | 72.3 | 52.0 | 43.8
    | 40.1 | 78.6 | 63.5 | 57.0 |'
- en: '| PV-RCNN [[89](#bib.bib89)] | point | - | 90.2 | 81.4 | 76.8 | 52.1 | 43.3
    | 40.3 | 78.6 | 63.7 | 57.7 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| PV-RCNN [[89](#bib.bib89)] | point | - | 90.2 | 81.4 | 76.8 | 52.1 | 43.3
    | 40.3 | 78.6 | 63.7 | 57.7 |'
- en: '| 3DSSD [[122](#bib.bib122)] | point | 26.3 | 88.4 | 79.6 | 74.6 | 54.6 | 44.3
    | 40.2 | 82.5 | 64.1 | 56.9 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 3DSSD [[122](#bib.bib122)] | point | 26.3 | 88.4 | 79.6 | 74.6 | 54.6 | 44.3
    | 40.2 | 82.5 | 64.1 | 56.9 |'
- en: 6 Registration
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 配准
- en: 6.1 Overview
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 概述
- en: In some scenarios like autopilot, it is of great value to find the relationship
    between point cloud data of the same scene collected in different ways. These
    data might be collected from different angles, or at different times. 3D point
    cloud registration (sometimes also called matching) attempts to align two or more
    different point clouds by estimating the transformation between them. It is a
    challenging problem affected by a lot of factors including noise, outliers and
    nonrigid spatial transformation.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景中，如自动驾驶，发现同一场景在不同方式下收集的点云数据之间的关系具有重要价值。这些数据可能从不同角度或在不同时间收集。3D点云配准（有时也称为匹配）试图通过估计它们之间的变换来对齐两个或多个不同的点云。这是一个受许多因素影响的挑战性问题，包括噪声、离群点和非刚性空间变换。
- en: 6.2 Traditional Methods
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 传统方法
- en: 'The Iterative Closest Point (ICP) algorithm [[3](#bib.bib3)] is a pioneering
    work that solves 3D point set registration. The basic pipeline of ICP and its
    variants is as follows: (1) Sample a point set $P$ from the source point cloud.
    (2) Compute the closest point set $Q$ from the target point cloud. (3) Calculate
    the registration (transformation) with $P$ and $Q$. (4) Apply the registration,
    and if the error is above some threshold, go back to step (2), otherwise terminate.
    A global refinement step is usually required for better performance. The performance
    of ICP highly depends on the quality of initialization and whether the input point
    clouds are clean. Generalized-ICP [[88](#bib.bib88)] and Go-ICP [[121](#bib.bib121)]
    are two representative follow-up works that mitigate the problems of ICP in different
    ways.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代最近点（ICP）**算法 [[3](#bib.bib3)] 是解决3D点集配准的开创性工作。ICP及其变体的基本流程如下：(1) 从源点云中采样一个点集$P$。(2)
    从目标点云中计算最近的点集$Q$。(3) 使用$P$和$Q$计算配准（变换）。(4) 应用配准，如果误差超过某个阈值，则返回步骤(2)，否则终止。通常需要全局精化步骤以获得更好的性能。ICP的性能高度依赖于初始化质量和输入点云的清洁程度。**广义-ICP**
    [[88](#bib.bib88)] 和 **Go-ICP** [[121](#bib.bib121)] 是两种代表性的后续工作，它们以不同方式缓解了ICP的问题。'
- en: Coherent Point Drift (CPD) algorithm [[72](#bib.bib72)] considers the alignment
    as a problem of probability density estimation. Concretely, the algorithm consider
    the first point set as the Gaussian mixture model centroids, and the transformation
    is estimated by maximizing the likelihood in fitting them to the second point
    set. The movement of these centroids are forced to be coherent to preserve the
    topological structure.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Coherent Point Drift (CPD) 算法 [[72](#bib.bib72)] 将对齐视为概率密度估计问题。具体来说，该算法将第一个点集视为高斯混合模型的中心点，并通过最大化将其拟合到第二个点集的可能性来估计变换。这些中心点的移动被强制保持一致，以保持拓扑结构。
- en: Robust Point Matching (RPM) [[27](#bib.bib27)] is another influential point
    matching algorithm. The algorithm starts with soft assignments of the point correspondences,
    and these soft assignments will get hardened through deterministic annealing.
    RPM is generally more robust than ICP, but still sensitive to initialization and
    noise.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Robust Point Matching (RPM) [[27](#bib.bib27)] 是另一种影响力大的点匹配算法。该算法从点对应关系的软分配开始，这些软分配通过确定性退火过程变硬。RPM
    通常比 ICP 更加鲁棒，但仍然对初始化和噪声敏感。
- en: Iglesias et al. [[41](#bib.bib41)] focused on the registration of several point
    clouds to a global coordinate system. In other words, with the original set of
    $n$ points, we want to find the correspondences between (subsets of) the original
    set and $m$ local coordinate systems respectively. Iglesias et al. consider the
    problem as a Semidefinite Program (SDP), and attempt to analyze it with the application
    of Lagrangian duality.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Iglesias 等人 [[41](#bib.bib41)] 关注于将多个点云配准到一个全局坐标系统。换句话说，给定原始的 $n$ 个点集，我们希望找到原始点集的（子集）与
    $m$ 个局部坐标系统之间的对应关系。Iglesias 等人将问题视为半正定规划（SDP），并尝试通过拉格朗日对偶分析该问题。
- en: 6.3 Learning-based Methods
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 基于学习的方法
- en: 'DeepVCP [[65](#bib.bib65)] is the first end-to-end learning-based framework
    in point cloud registration. Given the source and target point cloud, PointNet++
    [[80](#bib.bib80)] is applied to extract local features. A point weighting layer
    then helps select a set of $N$ keypoints, after which $N\times C$ candidates from
    the target point cloud are selected and passed through a deep feature embedding
    operation together with keypoints from the source. Finally, a corresponding point
    generation layer takes the embeddings and generates the final result. Two losses
    are incurred: (1) the Euclidean distance between the estimated corresponding points
    and ground truth under the ground truth transformation, and (2) the distance between
    the target under the estimated transformation and ground truth. These losses are
    combined to consider both global geometric information and local similarity.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: DeepVCP [[65](#bib.bib65)] 是第一个基于端到端学习的点云配准框架。给定源点云和目标点云，使用 PointNet++ [[80](#bib.bib80)]
    提取局部特征。然后，通过一个点权重层帮助选择一组 $N$ 个关键点，之后从目标点云中选择 $N\times C$ 个候选点，并将其与源点的关键点一起通过深度特征嵌入操作。最后，一个对应点生成层将这些嵌入生成最终结果。损失包括：（1）估计的对应点在真实变换下与真实点之间的欧几里得距离，以及（2）在估计变换下目标点与真实点之间的距离。这些损失结合考虑了全局几何信息和局部相似性。
- en: 3DSmoothNet [[26](#bib.bib26)] is proposed to perform 3D point cloud matching
    with a compact learned local feature descriptor. Given two raw point clouds as
    input, the model first computes the local reference frame (LRF) of the neighborhood
    around the randomly sampled interest points. Then the neighborhoods are transformed
    into canonical representations and voxelized by Gaussian smoothing, and the local
    feature of each point is then generated by 3DSmoothNet. The features will then
    be utilized by a RANSAC approach to produce registration results. The proposed
    smooth density value (SDV) voxelization outperforms traditional binary-occupancy
    grids by reducing the impact of boundary effects and noise, and provides greater
    compactness. Following 3DSmoothNet, Gojcic et al. [[25](#bib.bib25)] proposed
    another method that formulates conventional two-stage approaches in an end-to-end
    structure. Earlier methods solve the problem in two steps, the pairwise alignment
    and the globally consistent refinement, by jointly learning both parts. Gojcic
    et al.’s method outperforms previous ones with higher accuracy and less computational
    complexity.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 3DSmoothNet [[26](#bib.bib26)] 提出了一个使用紧凑学习局部特征描述符进行3D点云匹配的方法。给定两个原始点云作为输入，模型首先计算随机采样兴趣点周围的局部参考框架（LRF）。然后，邻域被转换成标准化表示，并通过高斯平滑进行体素化，接着由3DSmoothNet生成每个点的局部特征。然后，特征将通过
    RANSAC 方法用于生成配准结果。提出的平滑密度值（SDV）体素化通过减少边界效应和噪声的影响，且提供了更高的紧凑性，优于传统的二进制占用网格。在3DSmoothNet之后，Gojcic
    等人 [[25](#bib.bib25)] 提出了另一种方法，该方法将传统的两阶段方法公式化为端到端结构。早期方法通过联合学习配对对齐和全局一致性细化两步来解决问题。Gojcic
    等人的方法在准确性更高、计算复杂性更低方面优于之前的方法。
- en: RPM-Net [[125](#bib.bib125)] inherits the idea of RPM [[27](#bib.bib27)] algorithm,
    and takes advantage of deep learning to enhance robustness against noise, outliers
    and bad initialization. In this method, the initialization assignments are generated
    based on hybrid features from a network instead of spatial distances between points.
    The parameters of annealing is predicted by a secondary network, and a modified
    Chamfer distance is introduced to evaluate the quality of registration. This method
    outperforms previous methods no matter the input is clean, noisy, or even partially
    visible.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: RPM-Net [[125](#bib.bib125)] 继承了 RPM [[27](#bib.bib27)] 算法的思想，并利用深度学习增强对噪声、异常值和糟糕初始化的鲁棒性。在这种方法中，初始化分配是基于网络的混合特征生成的，而不是点之间的空间距离。退火的参数由一个次级网络预测，并引入了修改的
    Chamfer 距离来评估配准的质量。无论输入是干净的、嘈杂的，还是部分可见的，这种方法都优于之前的方法。
- en: 7 Augmentation and Completion
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 增强与完成
- en: 7.1 Overview
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 概述
- en: 'Point clouds collected by lidar, especially those from outdoor scenes, suffer
    from different kinds of quality issues like noise, outliers, and missing points.
    Many attempts have been made to improve the quality of raw point clouds by completing
    missing points, removing outliers and so on. The motivation and implementation
    vary a lot among different approaches; in this paper, we divide them into two
    categories: discriminative models and generative models.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 激光雷达收集的点云，特别是来自户外场景的点云，存在不同类型的质量问题，如噪声、异常值和缺失点。已经有很多尝试来通过完成缺失点、去除异常值等方法来提高原始点云的质量。这些方法的动机和实现方式差异很大；在本文中，我们将其分为两类：判别模型和生成模型。
- en: 7.2 Discriminative Methods
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 判别方法
- en: Noise in point clouds collected from outdoor scenes is naturally inevitable.
    To prevent noise from influencing the encoding of point clouds, some denoising
    methods shall be applied in pre-processing. Conventional methods include local
    surface fitting, neighborhood averaging and guessing the underlying noise model.
    PointCleanNet [[82](#bib.bib82)] proposed a data-driven method to remove outliers
    and reduce noise. With a deep neural network adapted from PCPNet[[29](#bib.bib29)],
    the model first classifies outliers and discards them, then estimates a correction
    projection that projects noise to original surfaces.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从户外场景中收集的点云中噪声是自然不可避免的。为了防止噪声影响点云的编码，预处理时需应用一些去噪方法。传统方法包括局部表面拟合、邻域平均和猜测潜在噪声模型。PointCleanNet
    [[82](#bib.bib82)] 提出了一个数据驱动的方法来去除异常值和减少噪声。该模型使用了从 PCPNet [[29](#bib.bib29)] 适配的深度神经网络，首先对异常值进行分类并将其丢弃，然后估算一个修正投影，将噪声投射到原始表面上。
- en: Hermosilla et al. [[33](#bib.bib33)] proposed Total Denoising that achieved
    unsupervised denoising of 3D point clouds without additional data. The unsupervised
    image denoisers are usually built based on the assumption that the value of a
    noisy pixel follows a distribution around a clean pixel value. Under this assumption,
    the original clean value can be recovered by learning the parameters of the random
    distribution. However, such an idea cannot be directly extended to point clouds
    because there are multiple formats of noise in point clouds, such as a global
    position deviation where no reliable reference point exists. Total Denoising introduces
    a spatial prior term that finds the closest of all possible modes on a manifold.
    The model achieves competitive performance against supervised models.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Hermosilla 等人 [[33](#bib.bib33)] 提出了 Total Denoising，这是一种在没有额外数据的情况下实现 3D 点云无监督去噪的方法。无监督图像去噪器通常基于假设噪声像素的值遵循围绕干净像素值的分布。在这一假设下，可以通过学习随机分布的参数来恢复原始的干净值。然而，这一思路不能直接扩展到点云，因为点云中存在多种噪声格式，例如不存在可靠参考点的全局位置偏差。Total
    Denoising 引入了一个空间先验项，该项在流形上找到所有可能模式中最接近的模式。该模型在与监督模型的比较中表现出竞争力。
- en: While a lot of models benefit from rich information in dense point clouds, some
    others are suffering from the low efficiency with large amounts of points. Conventional
    downsampling approaches usually have to risk dropping critical points. Nezhadarya
    et al. [[73](#bib.bib73)] proposed the critical points layer (CPL) that learns
    to reduce the number of points while preserving the important ones. The layer
    is deterministic, order-agnostic and also efficient by avoiding neighbor search.
    Aside from that, SampleNet [[54](#bib.bib54)] proposed a differentiable relaxation
    of point sampling by approximating points after sampling as a mixture of original
    points. The method has been tested as a front to networks on various tasks, and
    obtains decent performance with only a small fraction of the raw input point cloud.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多模型受益于密集点云中的丰富信息，但也有一些模型在处理大量点时效率较低。传统的降采样方法通常需要冒着丢失关键点的风险。Nezhadarya 等人
    [[73](#bib.bib73)] 提出了关键点层（CPL），该层学习在保留重要点的同时减少点的数量。该层是确定性的、与顺序无关的，并且通过避免邻域搜索而高效。除此之外，SampleNet
    [[54](#bib.bib54)] 提出了点采样的可微松弛，通过将采样后的点近似为原始点的混合来实现。这种方法已经在各种任务中作为网络前端进行了测试，并且仅用少量的原始输入点云就获得了不错的性能。
- en: 7.3 Generative Methods
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 生成方法
- en: Generative adversarial networks are widely studied for 2D images and CNNs, as
    they help locate the potential defects of networks by generating false samples.
    While typical applications of point cloud models, such as autonomous driving,
    consider safety as a critical concern, it is helpful to study how current deep
    neural networks on point clouds are affected by false samples.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络在 2D 图像和 CNNs 中被广泛研究，因为它们通过生成虚假样本帮助定位网络的潜在缺陷。虽然点云模型的典型应用，如自动驾驶，将安全性视为关键问题，但研究当前深度神经网络如何受虚假样本影响仍然是有益的。
- en: 'Xiang et al. [[114](#bib.bib114)] proposed several algorithms to generate adversarial
    point clouds against PointNet. The adversarial algorithms work in two ways: point
    perturbation and point generation. Perturbation is implemented by shifting existing
    points negligibly, and generation is implemented by either adding some independent
    and scattered points or a small number of point clusters with predefined shapes.
    Shu et al. [[92](#bib.bib92)] proposed tree-GAN, a tree-structured graph convolution
    network. By performing graph convolution within a tree, the model takes advantage
    of ancestor information to enrich the capacity of features. Along with the development
    of adversarial networks, DUP-Net [[137](#bib.bib137)] is proposed to defend 3D
    adversarial models. The model contains a statistical outlier removal (SOR) module
    as denoiser and a data-driven upsampling network as upsampler.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Xiang 等人 [[114](#bib.bib114)] 提出了几种算法来生成对抗性点云以对抗 PointNet。这些对抗算法有两种方式：点扰动和点生成。扰动通过轻微移动现有点来实现，生成则通过添加一些独立的散点或少量具有预定义形状的点簇来实现。Shu
    等人 [[92](#bib.bib92)] 提出了树状生成对抗网络（tree-GAN），这是一种树结构图卷积网络。通过在树内执行图卷积，模型利用祖先信息来丰富特征的能力。随着对抗网络的发展，DUP-Net
    [[137](#bib.bib137)] 被提出以防御 3D 对抗模型。该模型包含一个统计离群点移除（SOR）模块作为去噪器和一个数据驱动的上采样网络作为上采样器。
- en: Aside from adversarial generation, generative models are also used for point
    cloud upsampling. There are generally two motivations to upsample a point cloud.
    The first is to reduce the sparseness and irregularity of data, and the second
    is to restore missing points due to occlusion.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对抗生成，生成模型也被用于点云上采样。上采样点云的动机一般有两个。第一个是减少数据的稀疏性和不规则性，第二个是恢复由于遮挡而缺失的点。
- en: For the first aim, PU-Net [[127](#bib.bib127)] proposed upsampling in the feature
    space. For each point, multi-level features are extracted and expanded via a multi-branch
    convolution unit; after that, the expanded feature is split into multiple features
    and reconstructed to upsample the input set. Inspired by image super-resolution
    models, Wang et al. [[106](#bib.bib106)] proposed a cascade of patch-based upsampling
    networks, learning different levels of details at different steps, where at each
    step the network focuses only on a local patch from the output of the previous
    step. The architecture is able to upsample a sparse input point set to a dense
    set with rich details. Hui et al. [[40](#bib.bib40)] also proposed a learning-based
    deconvolution network that generates multi-resolution point clouds based on low-resolution
    input with bilateral interpolation performed in both the spatial and feature spaces.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个目标，PU-Net [[127](#bib.bib127)] 提出了在特征空间进行上采样。对于每个点，提取多级特征并通过多分支卷积单元扩展；之后，将扩展的特征拆分为多个特征，并重建以上采样输入集。受图像超分辨率模型的启发，Wang等人
    [[106](#bib.bib106)] 提出了一个基于补丁的上采样网络级联，在不同步骤学习不同级别的细节，其中每一步网络只关注来自前一步输出的局部补丁。该架构能够将稀疏输入点集上采样为具有丰富细节的密集点集。Hui等人
    [[40](#bib.bib40)] 还提出了一种基于学习的反卷积网络，它基于低分辨率输入生成多分辨率点云，并在空间和特征空间中进行双边插值。
- en: Meanwhile, early methods in completion, such as [[13](#bib.bib13)], tend to
    voxelize the input point cloud at the very beginning. PCN [[129](#bib.bib129)]
    was the first framework to work on raw point clouds and in a coarse-to-fine fashion.
    Wang et al. [[102](#bib.bib102)] improved the results with a two-step reconstruction
    design. Besides, Huang et al. [[39](#bib.bib39)] proposed PF-Net that preserves
    the spatial structure of the original incomplete point cloud, and predicts the
    missing points hierarchically a multi-scale generating network. GRNet[[116](#bib.bib116)],
    on the other hand, proposed a gridding-based which retrieve structural context
    by performing cubic feature sampling per grid, and complete the output with ”Gridding
    Reverse” layers and MLPs.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，早期的点云补全方法，如[[13](#bib.bib13)]，倾向于在一开始就对输入点云进行体素化处理。PCN [[129](#bib.bib129)]
    是第一个在原始点云上工作的框架，并采用粗到细的方式。Wang等人 [[102](#bib.bib102)] 通过两步重建设计改进了结果。此外，Huang等人
    [[39](#bib.bib39)] 提出了PF-Net，它保留了原始不完整点云的空间结构，并通过多尺度生成网络分层预测缺失点。另一方面，GRNet[[116](#bib.bib116)]
    提出了基于网格的结构，通过对每个网格进行立方体特征采样来检索结构上下文，并通过“网格反向”层和MLP完成输出。
- en: Lan et al. [[50](#bib.bib50)] proposed a probabilistic approach to optimize
    outliers by applying EM algorithm with Cauchy-Uniform mixture model to suppress
    potential outliers. More generally, PU-GAN [[57](#bib.bib57)] proposed a data-driven
    generative adversarial network to learn point distributions from the data and
    upsample points over patches on the surfaces of objects. Furthermore, RL-GAN-Net
    [[87](#bib.bib87)] uses a reinforcement learning (RL) agent to provide fast and
    reliable control of a generative adversarial network. By first training the GAN
    on the dimension-reduced latent space representation, and then finding the correct
    input to generate the representation that fits the current input form the uncompleted
    point cloud with a RL agent, the framework is able to convert noisy, partial point
    cloud into a completed shape in real time.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Lan等人 [[50](#bib.bib50)] 提出了一个概率方法，通过应用带有Cauchy-Uniform混合模型的EM算法来优化离群点，以抑制潜在的离群点。更一般来说，PU-GAN
    [[57](#bib.bib57)] 提出了一个数据驱动的生成对抗网络，从数据中学习点分布，并在物体表面的补丁上进行点云上采样。此外，RL-GAN-Net
    [[87](#bib.bib87)] 使用强化学习 (RL) 代理来提供生成对抗网络的快速和可靠的控制。通过首先在维度减少的潜在空间表示上训练GAN，然后使用RL代理找到生成适合当前输入的表示的正确输入，该框架能够实时将噪声、部分点云转换为完整的形状。
- en: 8 Conclusion
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this paper, we reviewed milestones and recent progress on various problems
    in 3D point clouds. With the expectation of practical applications like autonomous
    driving, point cloud understanding has received increasing attention lately. In
    3D shape classification, point-based models have achieved satisfactory performance
    on recognized benchmarks. Methods developed from image tasks, such as two-stage
    detector and the Siamese architecture, are widely introduced in 3D segmentation,
    object detection and other derivative tasks. Specific deep learning frameworks
    are proposed to match point clouds of the same scene from multiple scans, and
    generative networks are adapted to improve the quality of point cloud data with
    noise and missing points. Deep learning methods with proper adaption have been
    proven to efficiently help overcome the unique challenges in point cloud data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了 3D 点云中各种问题的里程碑和近期进展。由于对自动驾驶等实际应用的期望，点云理解最近受到越来越多的关注。在 3D 形状分类中，基于点的模型在公认的基准测试中取得了令人满意的表现。源于图像任务的方法，如两阶段检测器和孪生网络结构，被广泛应用于
    3D 分割、物体检测和其他衍生任务中。提出了特定的深度学习框架来匹配来自多个扫描的相同场景的点云，生成网络也被改编以提高带噪声和缺失点的点云数据的质量。经过适当调整的深度学习方法已被证明能有效帮助克服点云数据中的独特挑战。
- en: References
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin
    Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1534–1543, 2016.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin
    Fischer 和 Silvio Savarese。大规模室内空间的 3D 语义解析。发表于 IEEE 计算机视觉与模式识别会议论文集，页码 1534–1543，2016
    年。'
- en: '[2] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS
    Torr. Fully-convolutional siamese networks for object tracking. In European Conference
    on Computer Vision, pages 850–865. Springer, 2016.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi 和 Philip
    HS Torr。用于物体跟踪的全卷积孪生网络。发表于欧洲计算机视觉会议，页码 850–865。Springer，2016 年。'
- en: '[3] Paul J Besl and Neil D McKay. Method for registration of 3-D shapes. In
    Sensor Fusion IV: Control Paradigms and Data Structures, volume 1611, pages 586–606\.
    International Society for Optics and Photonics, 1992.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Paul J Besl 和 Neil D McKay。3D 形状配准的方法。发表于传感器融合 IV：控制范式和数据结构，第 1611 卷，页码
    586–606。国际光学和光子学学会，1992 年。'
- en: '[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
    Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes:
    A multimodal dataset for autonomous driving. arXiv preprint arXiv:1903.11027,
    2019.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
    Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan 和 Oscar Beijbom。nuScenes：用于自动驾驶的多模态数据集。arXiv
    预印本 arXiv:1903.11027，2019 年。'
- en: '[5] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing
    Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong
    Xiao, Li Yi, and Fisher Yu. ShapeNet: An information-rich 3D model repository.
    Technical Report arXiv:1512.03012 [cs.GR], 2015.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing
    Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong
    Xiao, Li Yi 和 Fisher Yu。ShapeNet：一个信息丰富的 3D 模型库。技术报告 arXiv:1512.03012 [cs.GR]，2015
    年。'
- en: '[6] Chao Chen, Guanbin Li, Ruijia Xu, Tianshui Chen, Meng Wang, and Liang Lin.
    ClusterNet: Deep hierarchical cluster network with rigorously rotation-invariant
    representation for point cloud analysis. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 4994–5002, 2019.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Chao Chen, Guanbin Li, Ruijia Xu, Tianshui Chen, Meng Wang 和 Liang Lin。ClusterNet：用于点云分析的深度分层聚类网络，具有严格的旋转不变表示。发表于
    IEEE 计算机视觉与模式识别会议论文集，页码 4994–5002，2019 年。'
- en: '[7] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z. Chen, and Jian
    Wu. A hierarchical graph network for 3d object detection on point clouds. In IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z. Chen 和 Jian
    Wu。用于点云上 3D 物体检测的分层图网络。发表于 IEEE/CVF 计算机视觉与模式识别会议（CVPR），2020 年。'
- en: '[8] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3D object
    detection network for autonomous driving. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 1907–1915, 2017.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li 和 Tian Xia。用于自动驾驶的多视角 3D 物体检测网络。发表于
    IEEE 计算机视觉与模式识别会议论文集，页码 1907–1915，2017 年。'
- en: '[9] Hsu-kuang Chiu, Antonio Prioletti, Jie Li, and Jeannette Bohg. Probabilistic
    3D multi-object tracking for autonomous driving. arXiv preprint arXiv:2001.05673,
    2020.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 许广秋、安东尼奥·普里奥莱蒂、李杰和珍妮特·博赫。用于自动驾驶的概率性 3D 多目标跟踪。arXiv 预印本 arXiv:2001.05673，2020年。'
- en: '[10] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser,
    and Matthias Nießner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes.
    In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 安吉拉·戴、安吉尔·X·张、马诺利斯·萨瓦、马切伊·哈尔伯、托马斯·芬克豪瑟和马蒂亚斯·尼斯纳。ScanNet：丰富标注的室内场景 3D 重建。发表于《计算机视觉与模式识别会议（CVPR）》，IEEE，2017年。'
- en: '[11] Angela Dai and Matthias Nießner. 3DMV: Joint 3D-multi-view prediction
    for 3D semantic scene segmentation. In Proceedings of the European Conference
    on Computer Vision (ECCV), pages 452–468, 2018.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 安吉拉·戴和马蒂亚斯·尼斯纳。3DMV：用于 3D 语义场景分割的联合 3D-多视图预测。发表于《欧洲计算机视觉会议论文集（ECCV）》，第
    452–468 页，2018年。'
- en: '[12] Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, Jürgen Sturm,
    and Matthias Nießner. ScanComplete: Large-scale scene completion and semantic
    segmentation for 3D scans. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 4578–4587, 2018.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 安吉拉·戴、丹尼尔·里奇、马丁·博克洛、斯科特·里德、于尔根·施图尔姆和马蒂亚斯·尼斯纳。ScanComplete：大规模场景完成和 3D
    扫描的语义分割。发表于《IEEE计算机视觉与模式识别会议论文集》，第 4578–4587 页，2018年。'
- en: '[13] Angela Dai, Charles Ruizhongtai Qi, and Matthias Nießner. Shape completion
    using 3d-encoder-predictor cnns and shape synthesis. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 5868–5877, 2017.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 安吉拉·戴、查尔斯·瑞宗泰·齐和马蒂亚斯·尼斯纳。形状完成使用 3D 编码器预测 CNN 和形状合成。发表于《IEEE计算机视觉与模式识别会议论文集》，第
    5868–5877 页，2017年。'
- en: '[14] Zhipeng Ding, Xu Han, and Marc Niethammer. VoteNet: A deep learning label
    fusion method for multi-atlas segmentation. In International Conference on Medical
    Image Computing and Computer-Assisted Intervention, pages 202–210\. Springer,
    2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 丁志鹏、韩旭和马克·尼特哈默。VoteNet：一种用于多图谱分割的深度学习标签融合方法。发表于《医学图像计算与计算机辅助干预国际会议》，第
    202–210 页，施普林格，2019年。'
- en: '[15] Liang Du, Xiaoqing Ye, Xiao Tan, Jianfeng Feng, Zhenbo Xu, Errui Ding,
    and Shilei Wen. Associate-3ddet: Perceptual-to-conceptual association for 3d point
    cloud object detection. In IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), 2020.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 梁都、叶晓青、谭晓、冯建峰、徐振博、丁尔瑞和温世磊。Associate-3ddet：3D 点云物体检测的感知到概念的关联。发表于《IEEE/CVF计算机视觉与模式识别会议（CVPR）》，2020年。'
- en: '[16] Yueqi Duan, Yu Zheng, Jiwen Lu, Jie Zhou, and Qi Tian. Structural relational
    reasoning of point clouds. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 949–958, 2019.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 岳琪·段、郑宇、吕际文、周杰和田启。点云的结构关系推理。发表于《IEEE计算机视觉与模式识别会议论文集》，第 949–958 页，2019年。'
- en: '[17] Francis Engelmann, Theodora Kontogianni, and Bastian Leibe. Dilated point
    convolutions: On the receptive field of point convolutions. arXiv preprint arXiv:1907.12046,
    2019.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 弗朗西斯·恩格尔曼、西奥多拉·孔托吉安尼和巴斯蒂安·莱贝。扩张点卷积：关于点卷积的感受野。arXiv 预印本 arXiv:1907.12046，2019年。'
- en: '[18] Francis Engelmann, Theodora Kontogianni, Jonas Schult, and Bastian Leibe.
    Know what your neighbors do: 3D semantic segmentation of point clouds. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 0–0, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 弗朗西斯·恩格尔曼、西奥多拉·孔托吉安尼、约纳斯·舒尔特和巴斯蒂安·莱贝。了解你的邻居在做什么：点云的 3D 语义分割。发表于《欧洲计算机视觉会议论文集（ECCV）》，第
    0–0 页，2018年。'
- en: '[19] Hehe Fan and Yi Yang. PointRNN: Point recurrent neural network for moving
    point cloud processing. arXiv preprint arXiv:1910.08287, 2019.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 樊和禾和杨毅。PointRNN：用于移动点云处理的点递归神经网络。arXiv 预印本 arXiv:1910.08287，2019年。'
- en: '[20] Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue Gao. GVCNN:
    Group-view convolutional neural networks for 3D shape recognition. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 264–272,
    2018.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 冯一凡、张自召、赵喜宾、季戎戎和高跃。GVCNN：用于 3D 形状识别的组视图卷积神经网络。发表于《IEEE计算机视觉与模式识别会议论文集》，第
    264–272 页，2018年。'
- en: '[21] Jannik Fritsch, Tobias Kuehnl, and Andreas Geiger. A new performance measure
    and evaluation benchmark for road detection algorithms. In International Conference
    on Intelligent Transportation Systems (ITSC), 2013.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 贾尼克·弗里奇、托比亚斯·库恩尔和安德reas·盖格。用于道路检测算法的新性能度量和评估基准。发表于《智能交通系统国际会议（ITSC）》，2013年。'
- en: '[22] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision
    meets robotics: The KITTI dataset. International Journal of Robotics Research
    (IJRR), 2013.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Andreas Geiger、Philip Lenz、Christoph Stiller 和 Raquel Urtasun。视觉与机器人技术的融合：KITTI数据集。《国际机器人研究杂志》（IJRR），2013年。'
- en: '[23] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous
    driving? The KITTI vision benchmark suite. In Conference on Computer Vision and
    Pattern Recognition (CVPR), 2012.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Andreas Geiger、Philip Lenz 和 Raquel Urtasun。我们准备好自动驾驶了吗？KITTI视觉基准套件。在计算机视觉与模式识别会议（CVPR），2012年。'
- en: '[24] Silvio Giancola, Jesus Zarzar, and Bernard Ghanem. Leveraging shape completion
    for 3D siamese tracking. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 1359–1368, 2019.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Silvio Giancola、Jesus Zarzar 和 Bernard Ghanem。利用形状补全进行3D Siamese跟踪。在《IEEE计算机视觉与模式识别会议论文集》中，第1359–1368页，2019年。'
- en: '[25] Zan Gojcic, Caifa Zhou, Jan D. Wegner, Leonidas J. Guibas, and Tolga Birdal.
    Learning multiview 3d point cloud registration. In IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Zan Gojcic、Caifa Zhou、Jan D. Wegner、Leonidas J. Guibas 和 Tolga Birdal。学习多视角3D点云配准。在IEEE/CVF计算机视觉与模式识别会议（CVPR），2020年。'
- en: '[26] Zan Gojcic, Caifa Zhou, Jan D Wegner, and Andreas Wieser. The perfect
    match: 3D point cloud matching with smoothed densities. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 5545–5554, 2019.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Zan Gojcic、Caifa Zhou、Jan D Wegner 和 Andreas Wieser。完美匹配：具有平滑密度的3D点云匹配。在《IEEE计算机视觉与模式识别会议论文集》中，第5545–5554页，2019年。'
- en: '[27] Steven Gold, Anand Rangarajan, Chien-Ping Lu, Suguna Pappu, and Eric Mjolsness.
    New algorithms for 2d and 3d point matching: Pose estimation and correspondence.
    Pattern recognition, 31(8):1019–1031, 1998.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Steven Gold、Anand Rangarajan、Chien-Ping Lu、Suguna Pappu 和 Eric Mjolsness。二维和三维点匹配的新算法：姿态估计和对应关系。《模式识别》，31(8):1019–1031，1998年。'
- en: '[28] Xiuye Gu, Yijie Wang, Chongruo Wu, Yong Jae Lee, and Panqu Wang. HPLFlowNet:
    Hierarchical permutohedral lattice flownet for scene flow estimation on large-scale
    point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 3254–3263, 2019.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Xiuye Gu、Yijie Wang、Chongruo Wu、Yong Jae Lee 和 Panqu Wang。HPLFlowNet：用于大规模点云场景流估计的分层置换格子流网络。在《IEEE计算机视觉与模式识别会议论文集》中，第3254–3263页，2019年。'
- en: '[29] Paul Guerrero, Yanir Kleiman, Maks Ovsjanikov, and Niloy J Mitra. PCPNet:
    Learning local shape properties from raw point clouds. In Computer Graphics Forum,
    volume 37, pages 75–85\. Wiley Online Library, 2018.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Paul Guerrero、Yanir Kleiman、Maks Ovsjanikov 和 Niloy J Mitra。PCPNet：从原始点云中学习局部形状属性。在《计算机图形学论坛》中，第37卷，第75–85页。Wiley在线图书馆，2018年。'
- en: '[30] Timo Hackel, N. Savinov, L. Ladicky, Jan D. Wegner, K. Schindler, and
    M. Pollefeys. SEMANTIC3D.NET: A new large-scale point cloud classification benchmark.
    In ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information
    Sciences, volume IV-1-W1, pages 91–98, 2017.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Timo Hackel、N. Savinov、L. Ladicky、Jan D. Wegner、K. Schindler 和 M. Pollefeys。SEMANTIC3D.NET：一种新的大规模点云分类基准。在ISPRS摄影测量、遥感与空间信息科学年刊，第IV-1-W1卷，第91–98页，2017年。'
- en: '[31] Kaveh Hassani and Mike Haley. Unsupervised multi-task feature learning
    on point clouds. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 8160–8171, 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Kaveh Hassani 和 Mike Haley。点云上的无监督多任务特征学习。在《IEEE国际计算机视觉会议论文集》中，第8160–8171页，2019年。'
- en: '[32] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang.
    Structure aware single-stage 3d object detection from point cloud. In The IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Chenhang He、Hui Zeng、Jianqiang Huang、Xian-Sheng Hua 和 Lei Zhang。结构感知单阶段3D目标检测从点云中。在IEEE/CVF计算机视觉与模式识别会议（CVPR），2020年。'
- en: '[33] Pedro Hermosilla, Tobias Ritschel, and Timo Ropinski. Total denoising:
    Unsupervised learning of 3D point cloud cleaning. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 52–60, 2019.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Pedro Hermosilla、Tobias Ritschel 和 Timo Ropinski。全面去噪：无监督学习3D点云清理。在《IEEE国际计算机视觉会议论文集》中，第52–60页，2019年。'
- en: '[34] Pedro Hermosilla, Tobias Ritschel, Pere-Pau Vázquez, Àlvar Vinacua, and
    Timo Ropinski. Monte Carlo convolution for learning on non-uniformly sampled point
    clouds. ACM Transactions on Graphics (TOG), 37(6):1–12, 2018.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Pedro Hermosilla、Tobias Ritschel、Pere-Pau Vázquez、Àlvar Vinacua 和 Timo
    Ropinski。蒙特卡罗卷积用于学习非均匀采样的点云。《ACM图形学汇刊》（TOG），37(6):1–12，2018年。'
- en: '[35] Ji Hou, Angela Dai, and Matthias Nießner. 3D-SIS: 3D semantic instance
    segmentation of RGB-D scans. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 4421–4430, 2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ji Hou、Angela Dai 和 Matthias Nießner. 3D-SIS：RGB-D 扫描的 3D 语义实例分割。在 IEEE
    计算机视觉与模式识别大会论文集，页码4421–4430，2019年。'
- en: '[36] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang,
    Niki Trigoni, and Andrew Markham. Randla-net: Efficient semantic segmentation
    of large-scale point clouds. In IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Qingyong Hu、Bo Yang、Linhai Xie、Stefano Rosa、Yulan Guo、Zhihua Wang、Niki
    Trigoni 和 Andrew Markham. RandLA-Net：大规模点云的高效语义分割。在 IEEE/CVF 计算机视觉与模式识别大会（CVPR），2020年。'
- en: '[37] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Pointwise convolutional
    neural networks. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 984–993, 2018.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Binh-Son Hua、Minh-Khoi Tran 和 Sai-Kit Yeung. 点卷积神经网络。在 IEEE 计算机视觉与模式识别大会论文集，页码984–993，2018年。'
- en: '[38] Jing Huang and Suya You. Point cloud labeling using 3D convolutional neural
    network. In 2016 23rd International Conference on Pattern Recognition (ICPR),
    pages 2670–2675\. IEEE, 2016.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Jing Huang 和 Suya You. 使用 3D 卷积神经网络进行点云标注。在 2016年第23届国际模式识别大会（ICPR），页码2670–2675。IEEE，2016年。'
- en: '[39] Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le. Pf-net: Point
    fractal network for 3d point cloud completion. In IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Zitian Huang、Yikuan Yu、Jiawen Xu、Feng Ni 和 Xinyi Le. Pf-net：用于 3D 点云完成的点分形网络。在
    IEEE/CVF 计算机视觉与模式识别大会（CVPR），2020年。'
- en: '[40] Le Hui, Rui Xu, Jin Xie, Jianjun Qian, and Jian Yang. Progressive point
    cloud deconvolution generation network. In The European Conference on Computer
    Vision (ECCV), 2020.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Le Hui、Rui Xu、Jin Xie、Jianjun Qian 和 Jian Yang. 渐进式点云反卷积生成网络。在欧洲计算机视觉会议（ECCV），2020年。'
- en: '[41] Jose Pedro Iglesias, Carl Olsson, and Fredrik Kahl. Global optimality
    for point set registration using semidefinite programming. In The IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), June 2020.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jose Pedro Iglesias、Carl Olsson 和 Fredrik Kahl. 使用半正定编程进行点集配准的全局最优性。在
    IEEE/CVF 计算机视觉与模式识别大会（CVPR），2020年6月。'
- en: '[42] Maximilian Jaritz, Jiayuan Gu, and Hao Su. Multi-view pointnet for 3D
    scene understanding. In Proceedings of the IEEE International Conference on Computer
    Vision Workshops, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Maximilian Jaritz、Jiayuan Gu 和 Hao Su. 多视角 PointNet 用于 3D 场景理解。在 IEEE
    国际计算机视觉研讨会论文集，2019年。'
- en: '[43] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya
    Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Li Jiang、Hengshuang Zhao、Shaoshuai Shi、Shu Liu、Chi-Wing Fu 和 Jiaya Jia.
    PointGroup：用于 3D 实例分割的双集点分组。在 IEEE/CVF 计算机视觉与模式识别大会（CVPR），2020年。'
- en: '[44] R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira,
    M. Yuan, B. Low, A. Jain, P. Ondruska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova,
    C. Tao, L. Platinsky, W. Jiang, and V. Shet. Lyft Level 5 AV Dataset 2019. [https://level5.lyft.com/dataset/](https://level5.lyft.com/dataset/),
    2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Kesten、M. Usman、J. Houston、T. Pandya、K. Nadhamuni、A. Ferreira、M. Yuan、B.
    Low、A. Jain、P. Ondruska、S. Omari、S. Shah、A. Kulkarni、A. Kazakova、C. Tao、L. Platinsky、W.
    Jiang 和 V. Shet. Lyft Level 5 AV 数据集 2019。[https://level5.lyft.com/dataset/](https://level5.lyft.com/dataset/)，2019年。'
- en: '[45] Roman Klokov and Victor Lempitsky. Escape from cells: Deep kd-networks
    for the recognition of 3D point cloud models. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 863–872, 2017.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Roman Klokov 和 Victor Lempitsky. 逃离单元：用于 3D 点云模型识别的深度 kd 网络。在 IEEE 国际计算机视觉会议论文集，页码863–872，2017年。'
- en: '[46] Artem Komarichev, Zichun Zhong, and Jing Hua. A-CNN: Annularly convolutional
    neural networks on point clouds. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 7421–7430, 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Artem Komarichev、Zichun Zhong 和 Jing Hua. A-CNN：点云上的环形卷积神经网络。在 IEEE 计算机视觉与模式识别大会论文集，页码7421–7430，2019年。'
- en: '[47] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L Waslander.
    Joint 3D proposal generation and object detection from view aggregation. In 2018
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
    1–8\. IEEE, 2018.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Jason Ku、Melissa Mozifian、Jungwook Lee、Ali Harakeh 和 Steven L Waslander.
    从视图聚合中联合生成 3D 提议和物体检测。在 2018 年 IEEE/RSJ 智能机器人与系统国际会议（IROS），页码1–8。IEEE，2018年。'
- en: '[48] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3D instance
    segmentation via multi-task metric learning. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 9256–9266, 2019.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Jean Lahoud, Bernard Ghanem, Marc Pollefeys 和 Martin R Oswald. 通过多任务度量学习进行3D实例分割。发表于IEEE国际计算机视觉大会论文集，页码9256–9266，2019年。'
- en: '[49] Shiyi Lan, Ruichi Yu, Gang Yu, and Larry S Davis. Modeling local geometric
    structure of 3D point clouds using Geo-CNN. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 998–1008, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Shiyi Lan, Ruichi Yu, Gang Yu 和 Larry S Davis. 使用Geo-CNN建模3D点云的局部几何结构。发表于IEEE计算机视觉与模式识别大会论文集，页码998–1008，2019年。'
- en: '[50] Ziquan Lan, Zi Jian Yew, and Gim Hee Lee. Robust point cloud based reconstruction
    of large-scale outdoor scenes. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 9690–9698, 2019.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Ziquan Lan, Zi Jian Yew 和 Gim Hee Lee. 基于点云的大规模户外场景鲁棒重建。发表于IEEE计算机视觉与模式识别大会论文集，页码9690–9698，2019年。'
- en: '[51] Loic Landrieu and Mohamed Boussaha. Point cloud oversegmentation with
    graph-structured deep metric learning. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 7440–7449, 2019.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Loic Landrieu 和 Mohamed Boussaha. 基于图结构深度度量学习的点云过分割。发表于IEEE计算机视觉与模式识别大会论文集，页码7440–7449，2019年。'
- en: '[52] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic
    segmentation with superpoint graphs. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 4558–4567, 2018.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Loic Landrieu 和 Martin Simonovsky. 大规模点云语义分割与超点图。发表于IEEE计算机视觉与模式识别大会论文集，页码4558–4567，2018年。'
- en: '[53] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and
    Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 12697–12705, 2019.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang 和 Oscar
    Beijbom. PointPillars: 用于从点云中进行物体检测的快速编码器。发表于IEEE计算机视觉与模式识别大会论文集，页码12697–12705，2019年。'
- en: '[54] Itai Lang, Asaf Manor, and Shai Avidan. Samplenet: Differentiable point
    cloud sampling. In IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), 2020.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Itai Lang, Asaf Manor 和 Shai Avidan. Samplenet: 可微分点云采样。发表于IEEE/CVF计算机视觉与模式识别大会（CVPR），2020年。'
- en: '[55] Felix Järemo Lawin, Martin Danelljan, Patrik Tosteberg, Goutam Bhat, Fahad Shahbaz
    Khan, and Michael Felsberg. Deep projective 3D semantic segmentation. In International
    Conference on Computer Analysis of Images and Patterns, pages 95–107\. Springer,
    2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Felix Järemo Lawin, Martin Danelljan, Patrik Tosteberg, Goutam Bhat, Fahad
    Shahbaz Khan 和 Michael Felsberg. 深度投影3D语义分割。发表于国际图像与模式分析会议，页码95–107。Springer，2017年。'
- en: '[56] Jiaxin Li, Ben M Chen, and Gim Hee Lee. SO-Net: Self-organizing network
    for point cloud analysis. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 9397–9406, 2018.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Jiaxin Li, Ben M Chen 和 Gim Hee Lee. SO-Net: 用于点云分析的自组织网络。发表于IEEE计算机视觉与模式识别大会论文集，页码9397–9406，2018年。'
- en: '[57] Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng.
    PU-GAN: A point cloud upsampling adversarial network. In Proceedings of the IEEE
    International Conference on Computer Vision, pages 7203–7212, 2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or 和 Pheng-Ann Heng.
    PU-GAN: 点云上采样对抗网络。发表于IEEE国际计算机视觉大会论文集，页码7203–7212，2019年。'
- en: '[58] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen.
    PointCNN: Convolution on x-transformed points. In Advances in Neural Information
    Processing Systems, pages 820–830, 2018.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di 和 Baoquan Chen. PointCNN:
    在x变换点上进行卷积。发表于神经信息处理系统进展，页码820–830，2018年。'
- en: '[59] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun. Deep continuous
    fusion for multi-sensor 3D object detection. In Proceedings of the European Conference
    on Computer Vision (ECCV), pages 641–656, 2018.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Ming Liang, Bin Yang, Shenlong Wang 和 Raquel Urtasun. 深度连续融合用于多传感器3D物体检测。发表于欧洲计算机视觉大会（ECCV）论文集，页码641–656，2018年。'
- en: '[60] Xingyu Liu, Charles R Qi, and Leonidas J Guibas. FlowNet3D: Learning scene
    flow in 3D point clouds. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 529–537, 2019.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Xingyu Liu, Charles R Qi 和 Leonidas J Guibas. FlowNet3D: 学习3D点云中的场景流。发表于IEEE计算机视觉与模式识别大会论文集，页码529–537，2019年。'
- en: '[61] Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. MeteorNet: Deep learning
    on dynamic 3D point cloud sequences. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 9246–9255, 2019.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 兴宇刘，梦源严和Jeannette Bohg。MeteorNet：动态3D点云序列上的深度学习。在IEEE国际计算机视觉会议论文集中，页面9246–9255，2019年。'
- en: '[62] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape
    convolutional neural network for point cloud analysis. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 8895–8904, 2019.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 勇成柳，彬帆，世明向和春红潘。关系形状卷积神经网络用于点云分析。在IEEE计算机视觉和模式识别会议论文集中，页面8895–8904，2019年。'
- en: '[63] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for
    efficient 3d deep learning. In Advances in Neural Information Processing Systems,
    pages 965–975, 2019.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Zhijian Liu，Haotian Tang，Yujun Lin和Song Han。用于高效3D深度学习的点-体素CNN。在神经信息处理系统的进展中，页面965–975，2019年。'
- en: '[64] Haihua Lu, Xuesong Chen, Guiying Zhang, Qiuhao Zhou, Yanbo Ma, and Yong
    Zhao. SCANet: Spatial-channel attention network for 3D object detection. In ICASSP
    2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), pages 1992–1996\. IEEE, 2019.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 华华·陆，学松·陈，贵英·张，球昊·周，彦波·马和勇·赵。SCANet：用于3D物体检测的空间-通道注意力网络。在ICASSP 2019-2019年IEEE国际声学、语音和信号处理会议（ICASSP）上，页面1992–1996\.
    IEEE，2019年。'
- en: '[65] Weixin Lu, Guowei Wan, Yao Zhou, Xiangyu Fu, Pengfei Yuan, and Shiyu Song.
    DeepVCP: An end-to-end deep neural network for point cloud registration. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 12–21, 2019.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 卫欣陆，国伟万，耀州姜，翔宇傅，鹏飞袁和诗宇宋。DeepVCP：用于点云配准的端到端深度神经网络。在IEEE国际计算机视觉会议论文集中，页面12–21，2019年。'
- en: '[66] Yecheng Lyu, Xinming Huang, and Ziming Zhang. Learning to segment 3d point
    clouds in 2d image space. In The IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), 2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 乐成吕，新明黄和子明张。学会在2D图像空间中分割3D点云。在计算机视觉和模式识别（CVPR）IEEE/CVF会议上，2020年。'
- en: '[67] Daniel Maturana and Sebastian Scherer. VoxNet: A 3D convolutional neural
    network for real-time object recognition. In 2015 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS), pages 922–928\. IEEE, 2015.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Daniel Maturana和Sebastian Scherer。VoxNet：用于实时物体识别的3D卷积神经网络。在2015年IEEE/RSJ国际智能机器人和系统（IROS）会议上，页面922–928\.
    IEEE，2015年。'
- en: '[68] Hsien-Yu Meng, Lin Gao, Yu-Kun Lai, and Dinesh Manocha. VV-Net: Voxel
    vae net with group convolutions for point cloud segmentation. In Proceedings of
    the IEEE International Conference on Computer Vision, pages 8500–8508, 2019.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] 孝运孟，林高，玉坤赖和Dinesh Manocha。VV-Net：用于点云分割的体素VAE网络与组卷积。在IEEE国际计算机视觉会议论文集中，页面8500–8508，2019年。'
- en: '[69] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles.
    In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Moritz Menze和Andreas Geiger。自动驾驶车辆的目标场景流。在计算机视觉和模式识别（CVPR）会议上，2015年。'
- en: '[70] George A Miller. Wordnet: a lexical database for english. Communications
    of the ACM, 38(11):39–41, 1995.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] George A Miller。Wordnet：英语词汇数据库。ACM通信杂志，38(11)：39–41，1995年。'
- en: '[71] Daniel Munoz, J Andrew Bagnell, Nicolas Vandapel, and Martial Hebert.
    Contextual classification with functional max-margin markov networks. In 2009
    IEEE Conference on Computer Vision and Pattern Recognition, pages 975–982\. IEEE,
    2009.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Daniel Munoz，J Andrew Bagnell，Nicolas Vandapel和Martial Hebert。具有函数最大边际马尔可夫网络的上下文分类。在2009年IEEE计算机视觉和模式识别会议上，页面975–982\.
    IEEE，2009年。'
- en: '[72] Andriy Myronenko and Xubo Song. Point set registration: Coherent point
    drift. IEEE transactions on pattern analysis and machine intelligence, 32(12):2262–2275,
    2010.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Andriy Myronenko和Xubo Song。点集配准：一致点漂移。IEEE模式分析与机器智能交易，32(12)：2262–2275，2010年。'
- en: '[73] Ehsan Nezhadarya, Ehsan Taghavi, Ryan Razani, Bingbing Liu, and Jun Luo.
    Adaptive hierarchical down-sampling for point cloud classification. In IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Ehsan Nezhadarya，Ehsan Taghavi，Ryan Razani，Bingbing Liu和Jun Luo。用于点云分类的自适应分层下采样。在IEEE/CVF计算机视觉和模式识别（CVPR）会议上，2020年。'
- en: '[74] Quang-Hieu Pham, Thanh Nguyen, Binh-Son Hua, Gemma Roig, and Sai-Kit Yeung.
    JSIS3D: Joint semantic-instance segmentation of 3D point clouds with multi-task
    pointwise networks and multi-value conditional random fields. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 8827–8836,
    2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Quang-Hieu Pham、Thanh Nguyen、Binh-Son Hua、Gemma Roig 和 Sai-Kit Yeung。JSIS3D：通过多任务点位网络和多值条件随机场对3D点云进行联合语义-实例分割。在《IEEE计算机视觉与模式识别会议论文集》中，第8827–8836页，2019年。'
- en: '[75] Sergey Prokudin, Christoph Lassner, and Javier Romero. Efficient learning
    on point clouds with basis point sets. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 4332–4341, 2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Sergey Prokudin、Christoph Lassner 和 Javier Romero。利用基点集在点云上进行高效学习。在《IEEE/CVF国际计算机视觉会议论文集》中，第4332–4341页，2019年。'
- en: '[76] Charles R. Qi, Xinlei Chen, Or Litany, and Leonidas J. Guibas. Imvotenet:
    Boosting 3d object detection in point clouds with image votes. In IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Charles R. Qi、Xinlei Chen、Or Litany 和 Leonidas J. Guibas。Imvotenet：通过图像投票提升点云中的3D物体检测。在《IEEE/CVF计算机视觉与模式识别会议（CVPR）》中，2020年。'
- en: '[77] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough
    voting for 3d object detection in point clouds. In Proceedings of the IEEE International
    Conference on Computer Vision, 2019.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Charles R Qi、Or Litany、Kaiming He 和 Leonidas J Guibas。深度霍夫投票用于点云中的3D物体检测。在《IEEE国际计算机视觉会议论文集》中，2019年。'
- en: '[78] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum
    PointNets for 3D object detection from RGB-D data. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 918–927, 2018.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Charles R Qi、Wei Liu、Chenxia Wu、Hao Su 和 Leonidas J Guibas。Frustum PointNets：从RGB-D数据中进行3D物体检测。在《IEEE计算机视觉与模式识别会议论文集》中，第918–927页，2018年。'
- en: '[79] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. PointNet: Deep
    learning on point sets for 3D classification and segmentation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 652–660,
    2017.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Charles R Qi、Hao Su、Kaichun Mo 和 Leonidas J Guibas。PointNet：用于3D分类和分割的点集深度学习。在《IEEE计算机视觉与模式识别会议论文集》中，第652–660页，2017年。'
- en: '[80] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. PointNet++:
    Deep hierarchical feature learning on point sets in a metric space. In Advances
    in Neural Information Processing Systems, pages 5099–5108, 2017.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Charles Ruizhongtai Qi、Li Yi、Hao Su 和 Leonidas J Guibas。PointNet++：在度量空间中对点集进行深度层次特征学习。在《神经信息处理系统进展》中，第5099–5108页，2017年。'
- en: '[81] Haozhe Qi, Chen Feng, Zhiguo Cao, Feng Zhao, and Yang Xiao. P2b: Point-to-box
    network for 3d object tracking in point clouds. In IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Haozhe Qi、Chen Feng、Zhiguo Cao、Feng Zhao 和 Yang Xiao。P2b：用于点云中的3D物体跟踪的点到框网络。在《IEEE/CVF计算机视觉与模式识别会议（CVPR）》中，2020年。'
- en: '[82] Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guerrero, Niloy J Mitra,
    and Maks Ovsjanikov. PointCleanNet: Learning to denoise and remove outliers from
    dense point clouds. In Computer Graphics Forum, volume 39, pages 185–203, 2020.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Marie-Julie Rakotosaona、Vittorio La Barbera、Paul Guerrero、Niloy J Mitra
    和 Maks Ovsjanikov。PointCleanNet：学习从密集点云中去噪声和移除离群点。在《计算机图形论坛》中，第39卷，第185–203页，2020年。'
- en: '[83] Yongming Rao, Jiwen Lu, and Jie Zhou. Spherical fractal convolutional
    neural networks for point cloud recognition. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 452–460, 2019.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Yongming Rao、Jiwen Lu 和 Jie Zhou。用于点云识别的球形分形卷积神经网络。在《IEEE计算机视觉与模式识别会议论文集》中，第452–460页，2019年。'
- en: '[84] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. OctNet: Learning
    deep 3D representations at high resolutions. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 3577–3586, 2017.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Gernot Riegler、Ali Osman Ulusoy 和 Andreas Geiger。OctNet：在高分辨率下学习深度3D表示。在《IEEE计算机视觉与模式识别会议论文集》中，第3577–3586页，2017年。'
- en: '[85] Xavier Roynard, Jean-Emmanuel Deschaud, and François Goulette. Paris-lille-3d:
    A large and high-quality ground-truth urban point cloud dataset for automatic
    segmentation and classification. The International Journal of Robotics Research,
    37(6):545–557, 2018.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Xavier Roynard、Jean-Emmanuel Deschaud 和 François Goulette。Paris-lille-3d：用于自动分割和分类的大型高质量城市点云数据集。《国际机器人研究杂志》，37(6)：545–557，2018年。'
- en: '[86] Radu Bogdan Rusu and Steve Cousins. 3D is here: Point cloud library (PCL).
    In 2011 IEEE International Conference on Robotics and Automation, pages 1–4\.
    IEEE, 2011.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Radu Bogdan Rusu 和 Steve Cousins. 3D 已到来：点云库 (PCL)。在 2011 年 IEEE 国际机器人与自动化会议中，第
    1–4 页。IEEE, 2011 年。'
- en: '[87] Muhammad Sarmad, Hyunjoo Jenny Lee, and Young Min Kim. RL-GAN-Net: A reinforcement
    learning agent controlled gan network for real-time point cloud shape completion.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5898–5907, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Muhammad Sarmad, Hyunjoo Jenny Lee, 和 Young Min Kim. RL-GAN-Net: 一种强化学习代理控制的
    GAN 网络，用于实时点云形状完成。在《IEEE计算机视觉与模式识别会议论文集》中，第 5898–5907 页，2019 年。'
- en: '[88] Aleksandr Segal, Dirk Haehnel, and Sebastian Thrun. Generalized-icp. In
    Robotics: science and systems, volume 2, page 435\. Seattle, WA, 2009.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Aleksandr Segal, Dirk Haehnel, 和 Sebastian Thrun. 泛化-icp。在《机器人学：科学与系统》中，第
    2 卷，第 435 页。西雅图, WA, 2009 年。'
- en: '[89] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang
    Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object
    detection. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), 2020.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang
    Wang, 和 Hongsheng Li. Pv-rcnn: 用于 3D 物体检测的点-体素特征集抽象。在《IEEE/CVF计算机视觉与模式识别会议》(CVPR)，2020
    年。'
- en: '[90] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. PointRCNN: 3D object proposal
    generation and detection from point cloud. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 770–779, 2019.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Shaoshuai Shi, Xiaogang Wang, 和 Hongsheng Li. PointRCNN: 从点云中生成和检测 3D
    物体提议。在《IEEE计算机视觉与模式识别会议论文集》中，第 770–779 页，2019 年。'
- en: '[91] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object
    detection in a point cloud. In IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), 2020.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Weijing Shi 和 Raj Rajkumar. Point-gnn: 用于点云中 3D 物体检测的图神经网络。在 IEEE/CVF
    计算机视觉与模式识别会议 (CVPR)，2020 年。'
- en: '[92] Dong Wook Shu, Sung Woo Park, and Junseok Kwon. 3D point cloud generative
    adversarial network based on tree structured graph convolutions. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 3859–3868, 2019.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Dong Wook Shu, Sung Woo Park, 和 Junseok Kwon. 基于树结构图卷积的 3D 点云生成对抗网络。在《IEEE国际计算机视觉会议论文集》中，第
    3859–3868 页，2019 年。'
- en: '[93] Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters
    in convolutional neural networks on graphs. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 3693–3702, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Martin Simonovsky 和 Nikos Komodakis. 图卷积神经网络中的动态边条件滤波器。在《IEEE计算机视觉与模式识别会议论文集》中，第
    3693–3702 页，2017 年。'
- en: '[94] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller.
    Multi-view convolutional neural networks for 3d shape recognition. In Proceedings
    of the IEEE international conference on computer vision, pages 945–953, 2015.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Hang Su, Subhransu Maji, Evangelos Kalogerakis, 和 Erik Learned-Miller.
    多视角卷积神经网络用于 3D 形状识别。在《IEEE国际计算机视觉会议论文集》中，第 945–953 页，2015 年。'
- en: '[95] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
    Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability
    in perception for autonomous driving: Waymo open dataset. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2446–2454,
    2020.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
    Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine 等. 自动驾驶中的感知可扩展性：Waymo
    开放数据集。在《IEEE/CVF计算机视觉与模式识别会议论文集》中，第 2446–2454 页，2020 年。'
- en: '[96] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent
    convolutions for dense prediction in 3D. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 3887–3896, 2018.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, 和 Qian-Yi Zhou. 用于密集预测的切线卷积。在《IEEE计算机视觉与模式识别会议论文集》中，第
    3887–3896 页，2018 年。'
- en: '[97] Bruno Vallet, Mathieu Brédif, Andrés Serna, Beatriz Marcotegui, and Nicolas
    Paparoditis. Terramobilita/iqmulus urban point cloud analysis benchmark. Computers
    & Graphics, 49:126–133, 2015.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Bruno Vallet, Mathieu Brédif, Andrés Serna, Beatriz Marcotegui, 和 Nicolas
    Paparoditis. Terramobilita/iqmulus 城市点云分析基准。计算机与图形学, 49:126–133, 2015 年。'
- en: '[98] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph
    attention convolution for point cloud semantic segmentation. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 10296–10305,
    2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, 和 Jie Shan. 用于点云语义分割的图注意卷积。在《IEEE计算机视觉与模式识别会议论文集》中，第
    10296–10305 页，2019 年。'
- en: '[99] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-CNN:
    Octree-based convolutional neural networks for 3D shape analysis. ACM Transactions
    on Graphics (TOG), 36(4):1–11, 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] 汪鹏帅，刘洋，郭宇晓，孙春雨和童鑫. O-CNN：基于八叉树的卷积神经网络用于3D形状分析。ACM计算机图形学汇刊（TOG），36(4):1–11，2017年。'
- en: '[100] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, and Raquel Urtasun.
    Deep parametric continuous convolutional neural networks. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 2589–2597, 2018.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] 王深龙，苏思远，马伟琇，Andrei Pokrovsky 和 Raquel Urtasun. 深度参数化连续卷积神经网络。发表于IEEE计算机视觉与模式识别会议，页码2589–2597，2018年。'
- en: '[101] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. SGPN: Similarity
    group proposal network for 3D point cloud instance segmentation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2569–2578,
    2018.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] 王伟跃，Ronald Yu，黄强贵 和 Ulrich Neumann. SGPN：用于3D点云实例分割的相似性组提议网络。发表于IEEE计算机视觉与模式识别会议，页码2569–2578，2018年。'
- en: '[102] Xiaogang Wang, Marcelo H. Ang Jr. , and Gim Hee Lee. Cascaded refinement
    network for point cloud completion. In IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), 2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] 王小刚，Marcelo H. Ang Jr. 和 Gim Hee Lee. 用于点云补全的级联精化网络。发表于IEEE/CVF计算机视觉与模式识别会议（CVPR），2020年。'
- en: '[103] Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, and Jiaya Jia. Associatively
    segmenting instances and semantics in point clouds. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 4096–4105, 2019.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] 王新龙，刘舒，沈晓勇，沈春华 和 贾佳雅. 在点云中关联分割实例和语义。发表于IEEE计算机视觉与模式识别会议，页码4096–4105，2019年。'
- en: '[104] Yue Wang, Alireza Fathi, Abhijit Kundu, David A. Ross, Caroline Pantofaru,
    Thomas A. Funkhouser, and Justin M. Solomon. Pillar-based object detection for
    autonomous driving. In The European Conference on Computer Vision (ECCV), 2020.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] 王跃，Alireza Fathi，Abhijit Kundu，David A. Ross，Caroline Pantofaru，Thomas
    A. Funkhouser 和 Justin M. Solomon. 基于柱的物体检测用于自动驾驶。发表于欧洲计算机视觉会议（ECCV），2020年。'
- en: '[105] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein,
    and Justin M Solomon. Dynamic graph CNN for learning on point clouds. ACM Transactions
    on Graphics (TOG), 38(5):1–12, 2019.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] 王跃，孙永彬，刘紫薇，Sanjay E Sarma，Michael M Bronstein 和 Justin M Solomon. 动态图CNN用于点云学习。ACM计算机图形学汇刊（TOG），38(5):1–12，2019年。'
- en: '[106] Yifan Wang, Shihao Wu, Hui Huang, Daniel Cohen-Or, and Olga Sorkine-Hornung.
    Patch-based progressive 3D point set upsampling. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 5958–5967, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] 王义凡，吴世豪，黄慧，Daniel Cohen-Or 和 Olga Sorkine-Hornung. 基于补丁的渐进式3D点集上采样。发表于IEEE计算机视觉与模式识别会议，页码5958–5967，2019年。'
- en: '[107] Zirui Wang, Shuda Li, Henry Howard-Jenkins, Victor Prisacariu, and Min
    Chen. FlowNet3D++: Geometric losses for deep scene flow estimation. In The IEEE
    Winter Conference on Applications of Computer Vision, pages 91–98, 2020.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] 王子瑞，李树达，Henry Howard-Jenkins，Victor Prisacariu 和 陈敏. FlowNet3D++：用于深度场景流估计的几何损失。发表于IEEE冬季计算机视觉应用会议，页码91–98，2020年。'
- en: '[108] Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Tzu-Yi Hung, and Lihua Xie.
    Multi-path region mining for weakly supervised 3d semantic segmentation on point
    clouds. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
    2020.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] 魏家成，林国盛，Kim-Hui Yap，洪子毅 和 谢立华. 基于多路径区域挖掘的弱监督3D语义分割。发表于IEEE/CVF计算机视觉与模式识别会议（CVPR），2020年。'
- en: '[109] Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer. Squeezeseg: Convolutional
    neural nets with recurrent crf for real-time road-object segmentation from 3d
    lidar point cloud. In 2018 IEEE International Conference on Robotics and Automation
    (ICRA), pages 1887–1893\. IEEE, 2018.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] 吴必臣，Alvin Wan，岳向宇和Kurt Keutzer. Squeezeseg：具有递归CRF的卷积神经网络，用于实时道路物体分割，基于3D激光雷达点云。发表于2018年IEEE国际机器人与自动化会议（ICRA），页码1887–1893。IEEE，2018年。'
- en: '[110] Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, and Kurt Keutzer.
    Squeezesegv2: Improved model structure and unsupervised domain adaptation for
    road-object segmentation from a lidar point cloud. In 2019 International Conference
    on Robotics and Automation (ICRA), pages 4376–4382\. IEEE, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] 吴必臣，周轩宇，赵思成，岳向宇 和 Kurt Keutzer. Squeezesegv2：改进的模型结构和无监督领域适应，用于激光雷达点云的道路物体分割。发表于2019年国际机器人与自动化会议（ICRA），页码4376–4382。IEEE，2019年。'
- en: '[111] Pengxiang Wu, Chao Chen, Jingru Yi, and Dimitris Metaxas. Point cloud
    processing via recurrent set encoding. In Proceedings of the AAAI Conference on
    Artificial Intelligence, volume 33, pages 5441–5449, 2019.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] 吴鹏翔、陈超、易靖如和迪米特里斯·梅塔克萨斯。通过递归集合编码处理点云。发表于AAAI人工智能会议论文集，第33卷，页码5441–5449，2019年。'
- en: '[112] Wenxuan Wu, Zhongang Qi, and Li Fuxin. PointConv: Deep convolutional
    networks on 3D point clouds. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 9621–9630, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] 吴文轩、齐忠昂和李福欣。PointConv: 深度卷积网络在3D点云上的应用。发表于IEEE计算机视觉与模式识别会议论文集，页码9621–9630，2019年。'
- en: '[113] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou
    Tang, and Jianxiong Xiao. 3D ShapeNets: A deep representation for volumetric shapes.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1912–1920, 2015.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] 吴志荣、宋书然、阿迪提亚·科斯拉、余飞、张凌光、唐晓鸥和肖建雄。3D ShapeNets: 体积形状的深度表示。发表于IEEE计算机视觉与模式识别会议论文集，页码1912–1920，2015年。'
- en: '[114] Chong Xiang, Charles R Qi, and Bo Li. Generating 3D adversarial point
    clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 9136–9144, 2019.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] 项聪、查尔斯·R·齐和李博。生成3D对抗点云。发表于IEEE计算机视觉与模式识别会议论文集，页码9136–9144，2019年。'
- en: '[115] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio
    Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010
    IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages
    3485–3492\. IEEE, 2010.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] 肖建雄、詹姆斯·海斯、克里斯塔·A·埃辛格、奥德·奥利瓦和安东尼奥·托拉尔巴。Sun数据库: 从修道院到动物园的大规模场景识别。发表于2010
    IEEE计算机学会计算机视觉与模式识别会议，页码3485–3492。IEEE，2010年。'
- en: '[116] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao, Shengping Zhang,
    and Wenxiu Sun. Grnet: Gridding residual network for dense point cloud completion.
    In ECCV, 2020.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] 谢浩哲、姚洪勋、周尚晨、毛家耿、张盛平和孙文秀。Grnet: 用于密集点云补全的网格残差网络。发表于ECCV，2020年。'
- en: '[117] Danfei Xu, Dragomir Anguelov, and Ashesh Jain. PointFusion: Deep sensor
    fusion for 3D bounding box estimation. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 244–253, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] 许丹飞、德拉戈米尔·安格洛夫和阿什什·贾因。PointFusion: 深度传感器融合用于3D边界框估计。发表于IEEE计算机视觉与模式识别会议论文集，页码244–253，2018年。'
- en: '[118] Xun Xu and Gim Hee Lee. Weakly supervised semantic point cloud segmentation:
    Towards 10x fewer labels. In IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] 许勋和李金赫。弱监督语义点云分割: 朝着10倍更少的标签。发表于IEEE/CVF计算机视觉与模式识别会议（CVPR），2020年。'
- en: '[119] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional
    detection. Sensors, 18(10):3337, 2018.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] 燕燕、毛宇星和李博。Second: 稀疏嵌入卷积检测。传感器，18(10):3337，2018年。'
- en: '[120] Bin Yang, Wenjie Luo, and Raquel Urtasun. PIXOR: Real-time 3D object
    detection from point clouds. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 7652–7660, 2018.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] 杨斌、罗文杰和拉奎尔·乌尔塔孙。PIXOR: 实时3D物体检测从点云中。发表于IEEE计算机视觉与模式识别会议论文集，页码7652–7660，2018年。'
- en: '[121] Jiaolong Yang, Hongdong Li, Dylan Campbell, and Yunde Jia. Go-icp: A
    globally optimal solution to 3d icp point-set registration. IEEE transactions
    on pattern analysis and machine intelligence, 38(11):2241–2254, 2015.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] 杨家龙、李宏东、迪伦·坎贝尔和贾云德。Go-icp: 3D ICP点集配准的全局最优解。IEEE模式分析与机器智能学报，38(11):2241–2254，2015年。'
- en: '[122] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d
    single stage object detector. In IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] 杨泽瞳、孙亚楠、刘舒和贾佳雅。3dssd: 基于点的3D单阶段物体检测器。发表于IEEE/CVF计算机视觉与模式识别会议（CVPR），2020年。'
- en: '[123] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. STD: Sparse-to-dense
    3D object detector for point cloud. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1951–1960, 2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] 杨泽瞳、孙亚楠、刘舒、沈晓勇和贾佳雅。STD: 稀疏到密集的3D物体检测器用于点云。发表于IEEE国际计算机视觉会议论文集，页码1951–1960，2019年。'
- en: '[124] Ze Yang and Liwei Wang. Learning relationships for multi-view 3D object
    recognition. In Proceedings of the IEEE International Conference on Computer Vision,
    pages 7505–7514, 2019.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] 杨泽和王立伟。用于多视角3D物体识别的关系学习。发表于IEEE国际计算机视觉会议论文集，页码7505–7514，2019年。'
- en: '[125] Zi Jian Yew and Gim Hee Lee. Rpm-net: Robust point matching using learned
    features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pages 11824–11833, 2020.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Zi Jian Yew 和 Gim Hee Lee。rpm-net：使用学习特征的鲁棒点匹配。在 IEEE/CVF 计算机视觉与模式识别会议论文集中，页码
    11824–11833，2020 年。'
- en: '[126] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. GSPN:
    Generative shape proposal network for 3D instance segmentation in point cloud.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 3947–3956, 2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Li Yi、Wang Zhao、He Wang、Minhyuk Sung 和 Leonidas J Guibas。GSPN：用于点云中 3D
    实例分割的生成形状提议网络。在 IEEE 计算机视觉与模式识别会议论文集中，页码 3947–3956，2019 年。'
- en: '[127] Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng.
    PU-Net: Point cloud upsampling network. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 2790–2799, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Lequan Yu、Xianzhi Li、Chi-Wing Fu、Daniel Cohen-Or 和 Pheng-Ann Heng。PU-Net：点云上采样网络。在
    IEEE 计算机视觉与模式识别会议论文集中，页码 2790–2799，2018 年。'
- en: '[128] Tan Yu, Jingjing Meng, and Junsong Yuan. Multi-view harmonized bilinear
    network for 3D object recognition. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 186–194, 2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Tan Yu、Jingjing Meng 和 Junsong Yuan。用于 3D 对象识别的多视角和谐双线性网络。在 IEEE 计算机视觉与模式识别会议论文集中，页码
    186–194，2018 年。'
- en: '[129] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and Martial Hebert.
    Pcn: Point completion network. In 2018 International Conference on 3D Vision (3DV),
    pages 728–737\. IEEE, 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Wentao Yuan、Tejas Khot、David Held、Christoph Mertz 和 Martial Hebert。Pcn：点完成网络。在
    2018 年国际 3D 视觉会议（3DV）论文集中，页码 728–737，IEEE，2018 年。'
- en: '[130] Jesus Zarzar, Silvio Giancola, and Bernard Ghanem. PointRGCN: Graph convolution
    networks for 3D vehicles detection refinement. arXiv preprint arXiv:1911.12236,
    2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Jesus Zarzar、Silvio Giancola 和 Bernard Ghanem。PointRGCN：用于 3D 车辆检测精细化的图卷积网络。arXiv
    预印本 arXiv:1911.12236，2019 年。'
- en: '[131] Wei Zeng and Theo Gevers. 3DContextNet: Kd tree guided hierarchical learning
    of point clouds using local and global contextual cues. In Proceedings of the
    European Conference on Computer Vision (ECCV), 2018.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Wei Zeng 和 Theo Gevers。3DContextNet：使用局部和全局上下文线索的 Kd 树引导的点云分层学习。在欧洲计算机视觉会议（ECCV）论文集中，2018
    年。'
- en: '[132] Yiming Zeng, Yu Hu, Shice Liu, Jing Ye, Yinhe Han, Xiaowei Li, and Ninghui
    Sun. RT3D: Real-time 3-D vehicle detection in lidar point cloud for autonomous
    driving. IEEE Robotics and Automation Letters, 3(4):3434–3440, 2018.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Yiming Zeng、Yu Hu、Shice Liu、Jing Ye、Yinhe Han、Xiaowei Li 和 Ninghui Sun。RT3D：实时
    3D 车辆检测用于自动驾驶的激光雷达点云。IEEE 机器人与自动化快报，3(4)：3434–3440，2018 年。'
- en: '[133] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
    embeddings. arXiv preprint arXiv:1912.00145, 2019.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Biao Zhang 和 Peter Wonka。使用概率嵌入的点云实例分割。arXiv 预印本 arXiv:1912.00145，2019
    年。'
- en: '[134] Feihu Zhang, Chenye Guan, Jin Fang, Song Bai, Ruigang Yang, Philip Torr,
    and Victor Prisacariu. Instance segmentation of lidar point clouds. ICRA, Cited
    by, 4(1), 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Feihu Zhang、Chenye Guan、Jin Fang、Song Bai、Ruigang Yang、Philip Torr 和
    Victor Prisacariu。激光雷达点云的实例分割。ICRA，引用，4(1)，2020 年。'
- en: '[135] Yang Zhang, Zixiang Zhou, Philip David, Xiangyu Yue, Zerong Xi, Boqing
    Gong, and Hassan Foroosh. Polarnet: An improved grid representation for online
    lidar point clouds semantic segmentation. In The IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), 2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Yang Zhang、Zixiang Zhou、Philip David、Xiangyu Yue、Zerong Xi、Boqing Gong
    和 Hassan Foroosh。Polarnet：一种改进的网格表示用于在线激光雷达点云的语义分割。在 IEEE/CVF 计算机视觉与模式识别会议（CVPR）上，2020
    年。'
- en: '[136] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia. PointWeb: Enhancing
    local neighborhood features for point cloud processing. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 5565–5573, 2019.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Hengshuang Zhao、Li Jiang、Chi-Wing Fu 和 Jiaya Jia。PointWeb：增强点云处理的局部邻域特征。在
    IEEE 计算机视觉与模式识别会议论文集中，页码 5565–5573，2019 年。'
- en: '[137] Hang Zhou, Kejiang Chen, Weiming Zhang, Han Fang, Wenbo Zhou, and Nenghai
    Yu. DUP-Net: Denoiser and upsampler network for 3D adversarial point clouds defense.
    In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1961–1970\.
    IEEE, 2019.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Hang Zhou、Kejiang Chen、Weiming Zhang、Han Fang、Wenbo Zhou 和 Nenghai Yu。DUP-Net：用于
    3D 对抗点云防御的去噪和上采样网络。在 2019 年 IEEE/CVF 国际计算机视觉会议（ICCV）上，页码 1961–1970，IEEE，2019 年。'
- en: '[138] Yin Zhou and Oncel Tuzel. VoxelNet: End-to-end learning for point cloud
    based 3D object detection. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 4490–4499, 2018.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] 尹周和翁策尔·图泽尔。VoxelNet：基于点云的 3D 对象检测的端到端学习。载于《IEEE 计算机视觉与模式识别会议论文集》，第 4490–4499
    页，2018 年。'
