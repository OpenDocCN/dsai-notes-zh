- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:47:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:47:56
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2202.08235] Data Augmentation for Deep Graph Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2202.08235] 深度图学习的数据增强：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2202.08235](https://ar5iv.labs.arxiv.org/html/2202.08235)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2202.08235](https://ar5iv.labs.arxiv.org/html/2202.08235)
- en: \WarningFilter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \WarningFilter
- en: '*natbibCitation \WarningFilter*BibTex'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*natbibCitation \WarningFilter*BibTex'
- en: 'Data Augmentation for Deep Graph Learning:'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度图学习的数据增强：
- en: A Survey
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一项综述
- en: Kaize Ding¹    Zhe Xu²    Hanghang Tong²    Huan Liu¹
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kaize Ding¹    Zhe Xu²    Hanghang Tong²    Huan Liu¹
- en: kaize.ding@asu.edu, zhexu3@illinois.edu, htong@illinois.edu, huanliu@asu.edu
    ¹Arizona State University ²University of Illinois Urbana-Champaign
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: kaize.ding@asu.edu, zhexu3@illinois.edu, htong@illinois.edu, huanliu@asu.edu
    ¹亚利桑那州立大学 ²伊利诺伊大学厄本那-香槟分校
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Graph neural networks, a powerful deep learning tool to model graph-structured
    data, have demonstrated remarkable performance on numerous graph learning tasks.
    To address the data noise and data scarcity issues in deep graph learning, the
    research on graph data augmentation has intensified lately. However, conventional
    data augmentation methods can hardly handle graph-structured data which is defined
    in non-Euclidean space with multi-modality. In this survey, we formally formulate
    the problem of graph data augmentation and further review the representative techniques
    and their applications in different deep graph learning problems. Specifically,
    we first propose a taxonomy for graph data augmentation techniques and then provide
    a structured review by categorizing the related work based on the augmented information
    modalities. Moreover, we summarize the applications of graph data augmentation
    in two representative problems in data-centric deep graph learning: (1) reliable
    graph learning which focuses on enhancing the utility of input graph as well as
    the model capacity via graph data augmentation; and (2) low-resource graph learning
    which targets on enlarging the labeled training data scale through graph data
    augmentation. For each problem, we also provide a hierarchical problem taxonomy
    and review the existing literature related to graph data augmentation. Finally,
    we point out promising research directions and the challenges in future research.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络作为一种强大的深度学习工具，用于建模图结构数据，在众多图学习任务中展现了卓越的性能。为了应对深度图学习中的数据噪声和数据稀缺问题，图数据增强的研究最近得到了加强。然而，传统的数据增强方法很难处理定义在非欧几里得空间中的多模态图结构数据。在这项综述中，我们正式提出了图数据增强的问题，并进一步回顾了代表性技术及其在不同深度图学习问题中的应用。具体而言，我们首先提出了图数据增强技术的分类法，然后通过根据增强信息模态对相关工作进行结构化回顾。此外，我们总结了图数据增强在数据中心深度图学习中的两个代表性问题中的应用：(1)
    可靠的图学习，重点是通过图数据增强来提升输入图的效用以及模型能力；(2) 低资源图学习，旨在通过图数据增强扩大标注训练数据的规模。对于每个问题，我们还提供了一个层次化的问题分类法，并回顾了与图数据增强相关的现有文献。最后，我们指出了未来研究中有前景的研究方向和挑战。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Graphs have been widely used for modeling a plethora of structured or relational
    systems, such as social networks, knowledge graphs, and academic graphs, where
    nodes represent the entities and edges denote the relations between entities.
    As a powerful deep learning tool to distill the knowledge behind graph-structured
    data, graph neural networks (GNNs) which generally follow a recursive message-passing
    scheme, have drawn a surge of research interest lately. Owing to its state-of-the-art
    performance, deep graph learning (DGL) nowadays has achieved remarkable success
    in a wide spectrum of graph analytical tasks [[20](#bib.bib20), [102](#bib.bib102),
    [22](#bib.bib22)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图已经广泛用于建模大量结构化或关系系统，如社交网络、知识图谱和学术图谱，其中节点代表实体，边表示实体之间的关系。作为一种强大的深度学习工具，用于提炼图结构数据背后的知识，图神经网络（GNNs）通常遵循递归消息传递机制，最近引起了大量的研究兴趣。由于其最先进的性能，深度图学习（DGL）如今在广泛的图分析任务中取得了显著的成功 [[20](#bib.bib20),
    [102](#bib.bib102), [22](#bib.bib22)]。
- en: 'Despite the superb power of GNNs, their effectiveness in DGL largely depends
    on high-quality input training graph(s) and ground-truth labels. The performance
    of GNNs tends to be delicate on real-world graphs, mainly because of their incapability
    of handling the following challenges: (1) On the one hand, prevailing DGL models
    are predominantly designed for the supervised or semi-supervised setting where
    sufficient ground-truth labels are available [[23](#bib.bib23), [21](#bib.bib21)].
    Considering the fact that data labeling on graphs is always time-consuming, labor-intensive,
    and rarely complete, the overreliance on labeled data poses great challenges to
    DGL models in real-world scenarios. Meanwhile, there are increasingly more tasks
    and domain-specific applications that are low-resource, having a paucity of labeled
    training examples. When ground-truth labels are extremely scarce, DGL models may
    easily overfit and be hard to generalize, losing their efficacy in solving various
    downstream DGL tasks [[90](#bib.bib90), [21](#bib.bib21)]. (2) On the other hand,
    real-world graphs are usually extracted from complex interaction systems which
    inevitably contain redundant, erroneous, or missing features and connections.
    In addition, the noxious manipulations from adversaries as well as the inherent
    limitations of GNNs such as the oversmoothing issue [[67](#bib.bib67)] also bring
    additional challenges to the success of reliable DGL. Directly training GNN-based
    models on such inferior graphs that are not clean and consistent with the properties
    of GNNs might lead to serious performance degradation [[18](#bib.bib18)].'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GNN的强大性能非常出色，但它们在DGL中的有效性在很大程度上依赖于高质量的输入训练图和真实标签。GNN的性能在现实世界图上往往较为敏感，主要因为它们无法处理以下挑战：（1）一方面，现有的DGL模型主要设计用于有足够真实标签的监督或半监督设置[[23](#bib.bib23),
    [21](#bib.bib21)]。考虑到图数据标注通常耗时、劳动密集且几乎不完整，对标注数据的过度依赖给现实世界场景中的DGL模型带来了巨大挑战。同时，越来越多的任务和领域特定应用是低资源的，标注训练样本稀缺。当真实标签极度稀少时，DGL模型可能容易过拟合，难以泛化，从而失去在各种下游DGL任务中的有效性[[90](#bib.bib90),
    [21](#bib.bib21)]。 （2）另一方面，现实世界图通常从复杂的交互系统中提取，这些系统不可避免地包含冗余、错误或缺失的特征和连接。此外，来自对手的有害操控以及GNN的固有局限性，如过平滑问题[[67](#bib.bib67)]，也为可靠DGL的成功带来了额外挑战。直接在这些不干净且不符合GNN特性的劣质图上训练基于GNN的模型可能导致严重的性能下降[[18](#bib.bib18)]。
- en: To improve the sufficiency and quality of training data, data augmentation is
    proposed as an effective tool to augment the given input data by either slightly
    modifying existing data instances or generating synthetic instances from existing
    ones. The importance of data augmentation has been well recognized in the computer
    vision [[89](#bib.bib89)] and natural language processing domains [[145](#bib.bib145)]
    in the past few years. More recently, data augmentation techniques have also been
    explored in the graph domain to push forward the performance boundary of DGL and
    demonstrated promising results [[145](#bib.bib145), [84](#bib.bib84)]. Apart from
    conventional image or text data, graph-structured data is known to be far more
    complicated with heterogeneous information modalities and complex graph properties,
    yielding a broader design space as well as the additional challenges for graph
    data augmentation (GraphDA). Though this line of research has been actively conducted
    lately, the problem of GraphDA has not been well formulated and researchers commonly
    adopt GraphDA techniques (e.g., edge perturbation, feature masking) arbitrarily
    without clear preference. Hence, it poses great challenge for researchers to grasp
    the design principles behind and further leverage them to solve specific DGL problems.
    Therefore, a timely and systematic review of GraphDA is of great benefit to be
    aware of existing research in this field and what the challenges are for conducting
    future research.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高训练数据的充分性和质量，提出了数据增强作为有效工具，通过稍微修改现有数据实例或从现有数据生成合成实例来扩充给定的输入数据。数据增强在计算机视觉 [[89](#bib.bib89)]和自然语言处理领域 [[145](#bib.bib145)]中已被广泛认可。最近，数据增强技术在图领域也得到了探索，以推动
    DGL 的性能边界，并展示了有前景的结果 [[145](#bib.bib145), [84](#bib.bib84)]。除了传统的图像或文本数据，图结构数据因其异质信息模式和复杂的图属性而更加复杂，带来了更广泛的设计空间以及图数据增强（GraphDA）的额外挑战。尽管这一研究方向最近得到了积极开展，但
    GraphDA 问题尚未得到很好地表述，研究人员通常任意采用 GraphDA 技术（例如，边缘扰动、特征遮蔽），而没有明确的偏好。因此，这对研究人员把握背后的设计原则并进一步利用它们解决特定
    DGL 问题构成了很大挑战。因此，及时和系统地回顾 GraphDA 对了解该领域的现有研究和未来研究的挑战大有裨益。
- en: 'Contributions. In this work, we present a forward-looking and up-to-date survey
    for GraphDA and its applications in solving data-centric DGL problems. In summary,
    our major contributions are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献。在这项工作中，我们呈现了关于 GraphDA 及其在解决数据中心 DGL 问题中应用的前瞻性和最新的综述。总之，我们的主要贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first survey for GraphDA. We provide
    a formal formulation for this emerging research area and review the related recent
    advances, which can facilitate the understanding of important issues to promote
    future research.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是 GraphDA 的首次综述。我们为这一新兴研究领域提供了正式的表述，并回顾了相关的最新进展，这有助于理解重要问题以促进未来的研究。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present a comprehensive taxonomy of GraphDA which categorizes the existing
    techniques in terms of the target augmentation modality (i.e., feature-oriented,
    structure-oriented, and label-oriented) and provides a clear design space for
    developing and customizing new GraphDA methods for different DGL problems.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了一个全面的 GraphDA 分类法，该分类法根据目标增强方式（即，特征导向、结构导向和标签导向）对现有技术进行分类，并为开发和定制针对不同 DGL
    问题的新 GraphDA 方法提供了清晰的设计空间。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss the applications of GraphDA in solving two major research problems
    in data-centric DGL, i.e., optimal graph learning and low-resource graph learning,
    and review the prevalent learning paradigms for solving specific sub-problems.
    We also outline the open issues and promising future directions in this area.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了 GraphDA 在解决数据中心 DGL 的两个主要研究问题中的应用，即优化图学习和低资源图学习，并回顾了解决特定子问题的普遍学习范式。我们还概述了该领域的开放问题和有前景的未来方向。
- en: Connection to Existing Surveys. Although there are a few survey papers [[72](#bib.bib72),
    [113](#bib.bib113), [119](#bib.bib119)] have related content to GraphDA, those
    surveys predominately focus on a single DGL problem such as graph self-supervised
    learning or graph adversarial defense. Other data-centric DGL problems related
    to GraphDA are largely overlooked and the corresponding GraphDA techniques are
    rarely discussed. In contrast, our survey includes a detailed and systematic review
    of GraphDA techniques and their corresponding applications for solving two most
    representative data-centric DGL problems. Meanwhile, we also discuss a list of
    under-explored directions in GraphDA, which can shed great light on future DGL
    research.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有调查的关联。尽管有一些调查论文 [[72](#bib.bib72), [113](#bib.bib113), [119](#bib.bib119)]
    与GraphDA相关，这些调查主要集中在单一的DGL问题上，如图自监督学习或图对抗防御。与GraphDA相关的其他数据中心DGL问题被大多忽视，相应的GraphDA技术也很少讨论。相比之下，我们的调查包括了对GraphDA技术及其在解决两个最具代表性的数据中心DGL问题中的应用的详细和系统的回顾。同时，我们还讨论了一些GraphDA中未充分探索的方向，这些方向可以为未来的DGL研究提供重要的启示。
- en: 'Structure Overview. This survey is structured as follows. Section [2](#S2 "2
    Preliminaries ‣ Data Augmentation for Deep Graph Learning: A Survey") first gives
    background on graph neural networks (GNNs) and deep graph learning. Then in Section
    [3](#S3 "3 Techniques of Graph Data Augmentation ‣ Data Augmentation for Deep
    Graph Learning: A Survey") we provide a comprehensive taxonomy of GraphDA techniques
    based on the focused augmentation modality of the input graph(s). In the following
    two sections, we describe the applications of GraphDA techniques for solving two
    data-centric DGL problems, i.e., Low-resource Graph Learning (Section [4](#S4
    "4 Graph Data Augmentation for Low-Resource Graph Learning ‣ Data Augmentation
    for Deep Graph Learning: A Survey")) and Reliable Graph Learning (Section [5](#S5
    "5 Graph Data Augmentation for Reliable Graph Learning ‣ Data Augmentation for
    Deep Graph Learning: A Survey")). Specifically, Section [4](#S4 "4 Graph Data
    Augmentation for Low-Resource Graph Learning ‣ Data Augmentation for Deep Graph
    Learning: A Survey") includes the techniques based on GraphDA for solving graph
    self-supervised learning and graph semi-supervised learning. Section [5](#S5 "5
    Graph Data Augmentation for Reliable Graph Learning ‣ Data Augmentation for Deep
    Graph Learning: A Survey") covers the content of improving the robustness, expressiveness,
    and scalability of DGL models from the data augmentation perspective. Within each
    subsection, we introduce the methods grouped by their related GraphDA techniques.
    Finally, Section [6](#S6 "6 Future Directions ‣ Data Augmentation for Deep Graph
    Learning: A Survey") discusses challenges and future directions in GraphDA.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '结构概述。本文的结构如下。第 [2](#S2 "2 Preliminaries ‣ Data Augmentation for Deep Graph
    Learning: A Survey") 节首先介绍图神经网络（GNNs）和深度图学习的背景。接着在第 [3](#S3 "3 Techniques of Graph
    Data Augmentation ‣ Data Augmentation for Deep Graph Learning: A Survey") 节，我们提供了一个基于输入图（s）的重点增强模式的GraphDA技术的综合分类。在接下来的两个章节中，我们描述了GraphDA技术在解决两个数据中心的深度图学习（DGL）问题中的应用，即低资源图学习（第
    [4](#S4 "4 Graph Data Augmentation for Low-Resource Graph Learning ‣ Data Augmentation
    for Deep Graph Learning: A Survey") 节）和可靠图学习（第 [5](#S5 "5 Graph Data Augmentation
    for Reliable Graph Learning ‣ Data Augmentation for Deep Graph Learning: A Survey")
    节）。具体来说，第 [4](#S4 "4 Graph Data Augmentation for Low-Resource Graph Learning ‣
    Data Augmentation for Deep Graph Learning: A Survey") 节包括了基于GraphDA的图自监督学习和图半监督学习的技术。第
    [5](#S5 "5 Graph Data Augmentation for Reliable Graph Learning ‣ Data Augmentation
    for Deep Graph Learning: A Survey") 节涵盖了从数据增强的角度提升DGL模型的鲁棒性、表达能力和可扩展性的内容。在每个子节中，我们介绍了按相关GraphDA技术分组的方法。最后，第
    [6](#S6 "6 Future Directions ‣ Data Augmentation for Deep Graph Learning: A Survey")
    节讨论了GraphDA中的挑战和未来方向。'
- en: 2 Preliminaries
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言
- en: 2.1 Notations and Definitions
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 符号和定义
- en: We briefly introduce the main symbols and notations used throughout this paper.
    We use bold uppercase letters for matrices (e.g., $\mathbf{X}$), bold lowercase
    letters for vectors (e.g., $\mathbf{v}$), lowercase and uppercase letters for
    scalars (e.g., $d$, $n$), and calligraphic letters for sets (e.g., $\mathcal{N}$).
    We use $\mathbf{X}[i,j]$ to represent the entry of matrix $\mathbf{X}$ at the
    $i$-th row and the $j$-th column, $\mathbf{X}[i,:]$ to represent the $i$-th row
    of matrix $\mathbf{X}$, and $\mathbf{X}[:,j]$ to represent the $j$-th column of
    matrix $\mathbf{X}$. Similarly, $\mathbf{v}[i]$ denotes the $i$-th entry of vector
    $\mathbf{v}$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要介绍了本文中使用的主要符号和符号约定。我们使用粗体大写字母表示矩阵（例如$\mathbf{X}$），粗体小写字母表示向量（例如$\mathbf{v}$），小写和大写字母表示标量（例如$d$，$n$），花体字母表示集合（例如$\mathcal{N}$）。我们使用$\mathbf{X}[i,j]$表示矩阵$\mathbf{X}$在第$i$行和第$j$列的条目，$\mathbf{X}[i,:]$表示矩阵$\mathbf{X}$的第$i$行，$\mathbf{X}[:,j]$表示矩阵$\mathbf{X}$的第$j$列。类似地，$\mathbf{v}[i]$表示向量$\mathbf{v}$的第$i$个条目。
- en: For the general purpose, we focus on undirected attributed graphs in this survey.
    A graph can be represented as $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$ where
    $\mathcal{V}$ denotes the node set $\{v_{i}\}_{i=1}^{n}$ and $\mathcal{E}$ denotes
    the edge set $\{e_{i}\}_{i=1}^{m}$. In matrix form, it can also be represented
    as $\mathcal{G}=(\mathbf{A},\mathbf{X})$, where $\mathbf{A}\in\mathbb{R}^{n\times
    n}$ denotes the adjacency matrix and $\mathbf{X}\in\mathbb{R}^{n\times d}$ denotes
    the node feature matrix. Here $n$ is the number of nodes, $m$ is the number of
    edges, and $d$ is the feature dimension. For supervised tasks, a part of the node
    labels $\mathbf{y}\in\mathbb{R}^{n}$, the edge labels $\mathbf{Y}\in\mathbb{R}^{n\times
    n}$, and the graph label $y$ are provided. For tasks with multiple graphs (e.g.,
    graph-level tasks), we appropriately use subscripts to describe graphs and corresponding
    components. For example, $\mathbf{A}_{i}$ denotes the adjacency matrix of the
    $i$-th graph $\mathcal{G}_{i}$.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 出于一般目的，我们在本次调查中聚焦于无向带属性图。一个图可以表示为$\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$，其中$\mathcal{V}$表示节点集$\{v_{i}\}_{i=1}^{n}$，$\mathcal{E}$表示边集$\{e_{i}\}_{i=1}^{m}$。在矩阵形式下，它也可以表示为$\mathcal{G}=(\mathbf{A},\mathbf{X})$，其中$\mathbf{A}\in\mathbb{R}^{n\times
    n}$表示邻接矩阵，$\mathbf{X}\in\mathbb{R}^{n\times d}$表示节点特征矩阵。这里$n$是节点数量，$m$是边的数量，$d$是特征维度。对于监督任务，提供了部分节点标签$\mathbf{y}\in\mathbb{R}^{n}$、边标签$\mathbf{Y}\in\mathbb{R}^{n\times
    n}$以及图标签$y$。对于多个图的任务（例如，图级任务），我们适当地使用下标来描述图及其相应的组件。例如，$\mathbf{A}_{i}$表示第$i$个图$\mathcal{G}_{i}$的邻接矩阵。
- en: 2.2 Graph Neural Networks
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 图神经网络
- en: Graph neural networks (GNNs) [[116](#bib.bib116)][[139](#bib.bib139)] are the
    extension of the neural network models [[3](#bib.bib3)] onto graph data. They
    show great flexibility and strong expressiveness to extract representations of
    various graph components and are becoming the core modules of broad graph learning
    tasks. In this subsection, we will briefly introduce the general message passing
    formulas [[33](#bib.bib33)] of the mainstream GNNs as they are widely adopted
    by the works remaining of this paper.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）[[116](#bib.bib116)][[139](#bib.bib139)]是神经网络模型[[3](#bib.bib3)]在图数据上的扩展。它们在提取各种图组件的表示方面表现出极大的灵活性和强大的表达能力，并且正成为广泛图学习任务的核心模块。在这一小节中，我们将简要介绍主流GNNs的一般消息传递公式[[33](#bib.bib33)]，因为它们被本论文剩余部分的工作广泛采用。
- en: 'The message passing framework of GNNs can be mathematically presented as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs的消息传递框架可以用数学形式表示如下：
- en: '|  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\mathbf{m}_{i}^{t+1}$ | $\displaystyle=\sum_{j\in N(i)}\texttt{Message}(\mathbf{h}_{i}^{t},\mathbf{h}_{j}^{t}),$
    |  | (1a) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{m}_{i}^{t+1}$ | $\displaystyle=\sum_{j\in N(i)}\texttt{Message}(\mathbf{h}_{i}^{t},\mathbf{h}_{j}^{t}),$
    |  | (1a) |'
- en: '|  | $\displaystyle\mathbf{h}_{i}^{t+1}$ | $\displaystyle=\texttt{Update}(\mathbf{h}_{i}^{t},\mathbf{m}_{i}^{t+1}),$
    |  | (1b) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}_{i}^{t+1}$ | $\displaystyle=\texttt{Update}(\mathbf{h}_{i}^{t},\mathbf{m}_{i}^{t+1}),$
    |  | (1b) |'
- en: 'where $\mathbf{h}_{i}^{t}$ denotes the representation of the node $i$ at $t$-th
    layer and $\mathbf{m}_{i}^{t+1}$ denotes the message aggregates on the node $i$
    at the $(t+1)$-th layer. The initial node representation is the node feature (i.e.,
    $\mathbf{h}_{i}^{0}=\mathbf{X}[i]$) and the node neighborhood can be represented
    by the adjacency matrix (i.e., $\mathbf{A}[i,j]=1$ is equivalent to $j\in N(i)$).
    For graph-level tasks, graph representation $\mathbf{h}_{G}$ can be obtained through
    a Readout function as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{h}_{i}^{t}$ 表示节点 $i$ 在第 $t$ 层的表示，$\mathbf{m}_{i}^{t+1}$ 表示节点 $i$
    在第 $(t+1)$ 层的消息聚合。初始节点表示是节点特征（即 $\mathbf{h}_{i}^{0}=\mathbf{X}[i]$），节点邻域可以通过邻接矩阵表示（即
    $\mathbf{A}[i,j]=1$ 等价于 $j\in N(i)$）。对于图级任务，图表示 $\mathbf{h}_{G}$ 可以通过如下 Readout
    函数获得：
- en: '|  | $\mathbf{h}_{G}=\texttt{Readout}(\mathbf{H}),$ |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{G}=\texttt{Readout}(\mathbf{H}),$ |  | (2) |'
- en: where $\mathbf{H}$ can be the node representations from the final layer or any
    intermediate layer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{H}$ 可以是来自最终层或任何中间层的节点表示。
- en: 2.3 Deep Graph Learning Tasks
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 深度图学习任务
- en: In this subsection, we introduce several mainstream DGL tasks on which GraphDA
    techniques are widely used. We categorize tasks according to their objective graph
    components (i.e., node, edge, and graph).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们介绍了几种主流的 DGL 任务，GraphDA 技术在这些任务上得到了广泛应用。我们根据任务的目标图组件（即节点、边和图）对任务进行分类。
- en: 'Node-level DGL Tasks. Node-level DGL tasks aim to find a mapping $p_{\phi}$
    from the given graphs to node properties by minimizing a utility loss $L_{\texttt{util}}$
    as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 节点级 DGL 任务。节点级 DGL 任务旨在通过最小化效用损失 $L_{\texttt{util}}$ 来找到从给定图到节点属性的映射 $p_{\phi}$，具体如下：
- en: '|  | $\phi^{*}=\arg\min_{\phi}L_{\texttt{util}}\Big{(}p_{\phi}(\mathcal{G}),\mathbf{y}\Big{)},$
    |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi^{*}=\arg\min_{\phi}L_{\texttt{util}}\Big{(}p_{\phi}(\mathcal{G}),\mathbf{y}\Big{)},$
    |  |'
- en: whose typical example is *semi-supervised node classification*. Given the labels
    of partial nodes for training, the goal is to predict the labels of (a part of)
    unlabelled nodes. A classic implementation of a node classifier is a node encoder
    (e.g., GNNs) working with a multi-class classifier (e.g., an MLP).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的例子是*半监督节点分类*。给定部分节点的标签进行训练，目标是预测（部分）未标记节点的标签。节点分类器的经典实现是一个节点编码器（例如，GNN）与多类别分类器（例如，MLP）一起工作。
- en: 'Edge-level DGL Tasks. Edge-level DGL tasks focus on finding a mapping $p_{\phi}$
    from the given graphs to edge properties which can be presented as below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 边级 DGL 任务。边级 DGL 任务专注于找到从给定图到边属性的映射，其表示如下：
- en: '|  | $\phi^{*}=\arg\min_{\phi}L_{\texttt{util}}\Big{(}p_{\phi}(\mathcal{G}),\mathbf{Y}\Big{)}.$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi^{*}=\arg\min_{\phi}L_{\texttt{util}}\Big{(}p_{\phi}(\mathcal{G}),\mathbf{Y}\Big{)}.$
    |  |'
- en: Taking *link prediction* as an example, its goal is to discriminate if there
    is an edge between specified node pair. A common implementation is a binary GNN
    classifier whose input is the edge embeddings (e.g., the aggregation of node embeddings
    of the head and tail nodes).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以*链接预测*为例，其目标是判断指定的节点对之间是否存在边。常见的实现是一个二元 GNN 分类器，其输入是边嵌入（例如，头节点和尾节点的节点嵌入的聚合）。
- en: 'Graph-level DGL Tasks. A graph-level task considers every graph as a data sample
    and infers the property of graph(s) by a mapping $p_{\phi}$. Its mathematical
    formulation is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图级 DGL 任务。图级任务将每个图视为一个数据样本，通过映射 $p_{\phi}$ 推断图的属性。其数学公式如下：
- en: '|  | $\phi^{*}=\arg\min_{\phi}L_{\texttt{util}}\Big{(}\{p_{\phi}(\mathcal{G}_{i})\},\{y_{i}\}\Big{)}.$
    |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi^{*}=\arg\min_{\phi}L_{\texttt{util}}\Big{(}\{p_{\phi}(\mathcal{G}_{i})\},\{y_{i}\}\Big{)}.$
    |  |'
- en: For instance, in the *graph classification* task, some labeled graphs are provided
    and the goal is to predict the labels of graphs of interest. A general solution
    is to aggregate the node embeddings into a graph embedding via a readout function
    and feed the graph embeddings into a classifier.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*图分类*任务中，提供了一些标记的图，目标是预测感兴趣图的标签。一个通用的解决方案是通过读出函数将节点嵌入聚合成图嵌入，并将图嵌入输入到分类器中。
- en: 3 Techniques of Graph Data Augmentation
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图数据增强的 3 种技术
- en: 'The goal of graph data augmentation (GraphDA) is to find a transformation function
    $f_{\theta}(\cdot):\mathcal{G}\rightarrow\tilde{\mathcal{G}}$ to generate augmented
    graph(s) $\{\tilde{\mathcal{G}}_{i}=(\tilde{\mathbf{A}}_{i},\tilde{\mathbf{X}}_{i})\}$
    that can enrich or enhance the preserved information from the given graph(s).
    In terms of whether the parameter $\theta$ can be updated or not during the learning
    process, most, if not all, of the GraphDA methods can be classified to: non-learnable
    and learnable methods. If the augmentation method is non-learnable, we can simply
    omit the parameter $\theta$ for brevity.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据增强（GraphDA）的目标是找到一个变换函数 $f_{\theta}(\cdot):\mathcal{G}\rightarrow\tilde{\mathcal{G}}$
    来生成增强后的图 $\{\tilde{\mathcal{G}}_{i}=(\tilde{\mathbf{A}}_{i},\tilde{\mathbf{X}}_{i})\}$，以丰富或增强从给定图中保留的信息。根据参数
    $\theta$ 在学习过程中是否可以更新，大多数（如果不是全部的话）GraphDA 方法可以分为：不可学习和可学习的方法。如果增强方法是不可学习的，我们可以为简便起见省略参数
    $\theta$。
- en: 'In general, as the ultimate goal of GraphDA is to improve the GNN model performance
    on downstream learning tasks, we need to consider them together during the learning
    process. We denote the augmentation loss as $L_{\textrm{aug}}$ whose goal is to
    regularize the augmented graph(s) to be close to the given graph(s), and denote
    the utility loss that measures the GNN performance on specific downstream tasks
    as $L_{\textrm{utility}}$. In terms of training strategies, most, if not all,
    of the learnable GraphDA methods can be divided into three categories: (1) decoupled
    training, (2) joint training, and (3) bi-level optimization. The workflow of each
    scheme is shown in Figure [1](#S3.F1 "Figure 1 ‣ 3 Techniques of Graph Data Augmentation
    ‣ Data Augmentation for Deep Graph Learning: A Survey") and the details can be
    found as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，由于 GraphDA 的最终目标是提高 GNN 模型在下游学习任务中的性能，我们需要在学习过程中将它们一起考虑。我们将增强损失表示为 $L_{\textrm{aug}}$，其目标是使增强图接近给定图，并将衡量
    GNN 在特定下游任务上表现的效用损失表示为 $L_{\textrm{utility}}$。在训练策略方面，大多数（如果不是全部的话）可学习的 GraphDA
    方法可以分为三类：（1）解耦训练，（2）联合训练，和（3）双层优化。每种方案的工作流程见图 [1](#S3.F1 "图 1 ‣ 图数据增强的三种技术 ‣ 深度图学习的数据增强：综述")，具体细节如下：
- en: 'Decoupled Training (DT). In this training scheme, the augmenter $f_{\theta}$
    and the predictor $p_{\phi}$ are independently trained in a two-stage paradigm.
    Specifically, the augmenter is first learned with augmentation loss $L_{\texttt{aug}}$.
    After that, the prediction model $p_{\theta}$ is trained on the augmented graph
    under the supervision of specific downstream tasks (i.e., $L_{\textrm{util}}$).
    The learning process can be formulated as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦训练（DT）。在这种训练方案中，增强器 $f_{\theta}$ 和预测器 $p_{\phi}$ 在两阶段范式中独立训练。具体来说，首先通过增强损失
    $L_{\texttt{aug}}$ 学习增强器。之后，在特定下游任务的监督下（即 $L_{\textrm{util}}$），对增强图进行预测模型 $p_{\theta}$
    的训练。学习过程可以表示为：
- en: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=\arg\min_{\theta}\ L_{\texttt{aug}}\Big{(}\{\mathcal{G}_{i}\},\{f_{\theta}(\mathcal{G}_{i})\}\Big{)},$
    |  | (3) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=\arg\min_{\theta}\ L_{\texttt{aug}}\Big{(}\{\mathcal{G}_{i}\},\{f_{\theta}(\mathcal{G}_{i})\}\Big{)},$
    |  | (3) |'
- en: '|  | $\displaystyle\phi^{*}$ | $\displaystyle=\arg\min_{\phi}L_{\texttt{util}}\Big{(}p_{\phi},\{f_{\theta^{*}}(\mathcal{G}_{i})\}\Big{)}.$
    |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi^{*}$ | $\displaystyle=\arg\min_{\phi}L_{\texttt{util}}\Big{(}p_{\phi},\{f_{\theta^{*}}(\mathcal{G}_{i})\}\Big{)}.$
    |  |'
- en: 'Joint Training (JT). In the joint training scheme, the augmenter $f_{\theta}$
    and the predictor $p_{\phi}$ are jointly trained with the augmentation loss $L_{\textrm{aug}}$
    and utility loss $L_{\textrm{util}}$. This learning process can be also considered
    as multi-task learning, which can be expressed as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 联合训练（JT）。在联合训练方案中，增强器 $f_{\theta}$ 和预测器 $p_{\phi}$ 通过增强损失 $L_{\texttt{aug}}$
    和效用损失 $L_{\texttt{util}}$ 共同训练。这个学习过程也可以被视为多任务学习，可以表示为：
- en: '|  | $\displaystyle\theta^{*},\phi^{*}=\arg\min_{\theta,\phi}$ | $\displaystyle
    L_{\texttt{aug}}\Big{(}\{\mathcal{G}_{i}\},\{f_{\theta}(\mathcal{G}_{i})\}\Big{)}$
    |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{*},\phi^{*}=\arg\min_{\theta,\phi}$ | $\displaystyle
    L_{\texttt{aug}}\Big{(}\{\mathcal{G}_{i}\},\{f_{\theta}(\mathcal{G}_{i})\}\Big{)}$
    |  | (4) |'
- en: '|  | $\displaystyle+$ | $\displaystyle L_{\texttt{util}}\Big{(}p_{\phi},\{f_{\theta}(\mathcal{G}_{i})\}\Big{)}.$
    |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+$ | $\displaystyle L_{\texttt{util}}\Big{(}p_{\phi},\{f_{\theta}(\mathcal{G}_{i})\}\Big{)}.$
    |  |'
- en: 'Bi-level Optimization (BO). Another training scheme of GraphDA for DGL is bi-level
    optimization. Different from joint training, the augmenter $f_{\theta}$ and the
    predictor $p_{\phi}$ are alternatively updated with the augmentation loss $L_{\textrm{aug}}$
    and utility loss $L_{\textrm{util}}$. As shown in Figure [1](#S3.F1 "Figure 1
    ‣ 3 Techniques of Graph Data Augmentation ‣ Data Augmentation for Deep Graph Learning:
    A Survey") (c), the update of the augmenter is based on the optimal updated predictor,
    which implies a bi-level optimization problem with $\theta$ as the upper-level
    variable and $\phi$ as the lower-level variable:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 双层优化（BO）。另一种GraphDA训练方案是双层优化。与联合训练不同，增强器 $f_{\theta}$ 和预测器 $p_{\phi}$ 是交替更新的，分别使用增强损失
    $L_{\textrm{aug}}$ 和效用损失 $L_{\textrm{util}}$。如图 [1](#S3.F1 "图 1 ‣ 3 种图数据增强技术 ‣
    深度图学习的数据增强：综述") (c) 所示，增强器的更新是基于最优的更新预测器，这意味着一个双层优化问题，其中 $\theta$ 是上层变量，$\phi$
    是下层变量：
- en: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=\arg\min_{\theta}\ L_{\texttt{aug}}\Big{(}\{\mathcal{G}_{i}\},\{f_{\theta}(\mathcal{G}_{i})\},p_{\phi^{*}(\theta)}\Big{)},$
    |  | (5) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=\arg\min_{\theta}\ L_{\texttt{aug}}\Big{(}\{\mathcal{G}_{i}\},\{f_{\theta}(\mathcal{G}_{i})\},p_{\phi^{*}(\theta)}\Big{)},$
    |  | (5) |'
- en: '|  | $\displaystyle s.t.\quad\phi^{*}(\theta)$ | $\displaystyle=\arg\min_{\phi}L_{\texttt{util}}\Big{(}p_{\phi},\{f_{\theta}(\mathcal{G}_{i})\}\Big{)}.$
    |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s.t.\quad\phi^{*}(\theta)$ | $\displaystyle=\arg\min_{\phi}L_{\texttt{util}}\Big{(}p_{\phi},\{f_{\theta}(\mathcal{G}_{i})\}\Big{)}.$
    |  |'
- en: 'In the following subsections, we provide a systematic taxonomy to cover mainstream
    GraphDA techniques. Since graphs commonly consist of multiple information modalities,
    GraphDA techniques can be naturally divided into three categories based on the
    augmentation modality, including: *feature-oriented*, *structure-oriented*, and
    *label-oriented* techniques. Specifically, we summarize the commonly used techniques
    in terms of each augmentation modality and clearly illustrate their augmentation
    strategies. Figure [2](#S3.F2 "Figure 2 ‣ 3 Techniques of Graph Data Augmentation
    ‣ Data Augmentation for Deep Graph Learning: A Survey") (left) summarizes our
    proposed taxonomy for GraphDA techniques.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们提供了一个系统的分类方法，以涵盖主流的GraphDA技术。由于图数据通常由多种信息模态组成，GraphDA技术可以根据增强模态自然地分为三类，包括：*特征导向*、*结构导向*和*标签导向*技术。具体而言，我们总结了每种增强模态中常用的技术，并清晰地阐述了它们的增强策略。图
    [2](#S3.F2 "图 2 ‣ 3 种图数据增强技术 ‣ 深度图学习的数据增强：综述") (左) 总结了我们提出的GraphDA技术分类方法。
- en: '![Refer to caption](img/fc91d8b5d328a1ba4baeca54bd219b1f.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fc91d8b5d328a1ba4baeca54bd219b1f.png)'
- en: (a) Decoupled Training (DT)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 解耦训练（DT）
- en: '![Refer to caption](img/432b80362f4de6c02a2dba4fa53d81a4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/432b80362f4de6c02a2dba4fa53d81a4.png)'
- en: (b) Joint Training (JT)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 联合训练（JT）
- en: '![Refer to caption](img/b50b468da23bce1439346d4a8d970e58.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b50b468da23bce1439346d4a8d970e58.png)'
- en: (c) Bi-level Optimization (BO)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 双层优化（BO）
- en: 'Figure 1: Learnable GraphDA training paradigms.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：可学习的图数据增强（GraphDA）训练范式。
- en: '![Refer to caption](img/1f671cbd2bf1d31d4e4aac00c527da0b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1f671cbd2bf1d31d4e4aac00c527da0b.png)'
- en: 'Figure 2: Proposed taxonomy of Graph Data Augmentation (GraphDA) techniques
    and applications.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：图数据增强（GraphDA）技术及应用的分类。
- en: 3.1 Structure-oriented Augmentations
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 面向结构的增强
- en: Different from i.i.d. data, graphs are inherently relational where the connections
    (i.e., edges) between data instances (i.e., nodes) are unique and essential for
    understanding and analyzing graphs. Given an input graph $\mathcal{G}=(\mathbf{A},\mathbf{X})$,
    a structure-oriented GraphDA operation focuses on augmenting the adjacency matrix
    $\mathbf{A}$ of the input graph. We summarize the representative ones as follows.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与独立同分布（i.i.d.）数据不同，图数据本质上是关系性的，其中数据实例（即节点）之间的连接（即边）对于理解和分析图数据至关重要。给定一个输入图 $\mathcal{G}=(\mathbf{A},\mathbf{X})$，面向结构的图数据增强操作主要关注于增强输入图的邻接矩阵
    $\mathbf{A}$。我们总结了以下代表性的增强方法。
- en: 'Edge Perturbation. Perturbing the given graph structure, e.g., randomly adding
    or dropping edges, is a widely adopted GraphDA method in different DGL tasks [[99](#bib.bib99),
    [128](#bib.bib128), [127](#bib.bib127), [149](#bib.bib149)]. Mathematically edge
    perturbation keeps the original node order and rewrites a part of the entries
    in the given adjacency matrices, which can be defined as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 边扰动。扰动给定的图结构，例如，随机添加或删除边，是不同 DGL 任务中广泛采用的 GraphDA 方法 [[99](#bib.bib99), [128](#bib.bib128),
    [127](#bib.bib127), [149](#bib.bib149)]。在数学上，边扰动保持原始节点顺序，并重写给定邻接矩阵中的部分条目，可以定义如下：
- en: '|  | $\mathbf{\tilde{A}}=\mathbf{A}\oplus\mathbf{C},$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\tilde{A}}=\mathbf{A}\oplus\mathbf{C},$ |  | (6) |'
- en: where $\mathbf{C}$ is the corruption matrix and $\oplus$ denotes the XOR (exclusive
    OR) operation. Commonly, the corruption matrix $\mathbf{C}$ is obtained by sampling,
    i.i.d., from a prior distribution, and $\mathbf{C}_{ij}$ determines whether to
    corrupt the adjacency matrix at position (i, j). For example, assuming a given
    corruption rate $\rho$, we may define the corruption matrix as $\mathbf{C}_{ij}\sim
    Bernoulli(\rho)$, whose elements in $\mathbf{C}$ are set to 1 individually with
    a probability $\rho$ and 0 with a probability $1-\rho$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{C}$ 是腐蚀矩阵，$\oplus$ 表示 XOR（异或）操作。通常，腐蚀矩阵 $\mathbf{C}$ 通过从先验分布中独立同分布地采样得到，$\mathbf{C}_{ij}$
    确定是否在位置 (i, j) 处扰动邻接矩阵。例如，假设给定腐蚀率 $\rho$，我们可以定义腐蚀矩阵为 $\mathbf{C}_{ij}\sim Bernoulli(\rho)$，其元素在
    $\mathbf{C}$ 中以概率 $\rho$ 单独设置为 1，以概率 $1-\rho$ 设置为 0。
- en: Graph Rewiring. Though sharing the same basic operation with edge perturbation,
    graph rewiring has an opposite augmentation objective, which is to improve the
    utility of the input graph by rewiring the edges. Instead of perturbing the input
    graph structure by randomly adding/dropping edges, graph rewiring is commonly
    guided by the learning objective of the downstream task, and the corruption matrix
    $\mathbf{C}$ is learned or predicted through a specific module.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图重连。尽管与边扰动共享相同的基本操作，但图重连具有相反的增强目标，即通过重新连线边来提高输入图的效用。与通过随机添加/删除边来扰动输入图结构不同，图重连通常由下游任务的学习目标指导，并且腐蚀矩阵
    $\mathbf{C}$ 通过特定模块进行学习或预测。
- en: 'Graph Diffusion. As another effective structure-wise augmentation strategy
    for improving the graph utility, graph diffusion generates an augmented graph
    by exploiting the global structure knowledge of the input graph. In certain cases,
    it is also considered as a graph rewiring method [[98](#bib.bib98)]. Specifically,
    graph diffusion injects the global topological information into the given graph
    structure by connecting nodes with their indirectly connected neighbors with calculated
    weights. A generalized graph diffusion operation can be formulated as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图扩散。作为另一种有效的结构增强策略以提高图的效用，图扩散通过利用输入图的全局结构知识生成增强图。在某些情况下，它也被视为一种图重连方法 [[98](#bib.bib98)]。具体而言，图扩散通过将全局拓扑信息注入给定图结构，将节点与其间接连接的邻居以计算出的权重连接起来。一个广义的图扩散操作可以表述为：
- en: '|  | $\mathbf{\tilde{A}}=\sum^{\infty}_{k=0}\gamma_{k}\mathbf{T}^{k},$ |  |
    (7) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\tilde{A}}=\sum^{\infty}_{k=0}\gamma_{k}\mathbf{T}^{k},$ |  |
    (7) |'
- en: 'where $\mathbf{T}\in\mathbb{R}^{N\times N}$ is the generalized transition matrix
    derived from the adjacency matrix $\mathbf{A}$ and $\gamma_{k}$ is the weighting
    coefficient that determines the ratio of global-local information. Imposing $\sum_{k=0}^{\infty}\gamma_{k}=1,\gamma_{k}\in[0,1]$
    and $\lambda_{i}\in[0,1]$ where $\lambda_{i}$ are eigenvalues of $\mathbf{T}$,
    guarantees convergence. Two popular examples of graph diffusion are personalized
    PageRank (PPR) [[80](#bib.bib80)] (i.e., $\gamma_{k}=\alpha(1-\alpha)^{k}$) and
    the heat kernel [[62](#bib.bib62)] (i.e., $\gamma_{k}=e^{-t}\frac{t^{k}}{k!}$).
    where $\alpha$ denotes teleport probability in a random walk and $t$ is diffusion
    time. Closed-form solutions to heat kernel and PPR diffusion are formulated in
    Eq. ([8](#S3.E8 "In 3.1 Structure-oriented Augmentations ‣ 3 Techniques of Graph
    Data Augmentation ‣ Data Augmentation for Deep Graph Learning: A Survey")) and
    ([9](#S3.E9 "In 3.1 Structure-oriented Augmentations ‣ 3 Techniques of Graph Data
    Augmentation ‣ Data Augmentation for Deep Graph Learning: A Survey")), respectively:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{T}\in\mathbb{R}^{N\times N}$ 是由邻接矩阵 $\mathbf{A}$ 推导出的广义转移矩阵，$\gamma_{k}$
    是权重系数，决定全局-局部信息的比例。施加约束 $\sum_{k=0}^{\infty}\gamma_{k}=1,\gamma_{k}\in[0,1]$ 以及
    $\lambda_{i}\in[0,1]$，其中 $\lambda_{i}$ 是 $\mathbf{T}$ 的特征值，保证收敛。图扩散的两个流行示例是个性化
    PageRank (PPR) [[80](#bib.bib80)]（即，$\gamma_{k}=\alpha(1-\alpha)^{k}$）和热核 [[62](#bib.bib62)]（即，$\gamma_{k}=e^{-t}\frac{t^{k}}{k!}$），其中
    $\alpha$ 表示随机游走中的跳跃概率，$t$ 是扩散时间。 热核和 PPR 扩散的闭式解分别表示为：
- en: '|  | $\displaystyle\mathbf{\tilde{A}}^{\text{heat}}$ | $\displaystyle=e^{-(\mathbf{I}-\mathbf{T})t},$
    |  | (8) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{\tilde{A}}^{\text{heat}}$ | $\displaystyle=e^{-(\mathbf{I}-\mathbf{T})t},$
    |  | (8) |'
- en: '|  | $\displaystyle\mathbf{\tilde{A}}^{\text{PPR}}$ | $\displaystyle=\alpha(\mathbf{I}-(1-\alpha)\mathbf{T})^{-1}.$
    |  | (9) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{\tilde{A}}^{\text{PPR}}$ | $\displaystyle=\alpha(\mathbf{I}-(1-\alpha)\mathbf{T})^{-1}.$
    |  | (9) |'
- en: 'Graph Sampling. Graph sampling or subgraph sampling is a commonly used data
    augmentation technique for graphs. It can be used for different purposes, such
    as scaling up GNNs [[36](#bib.bib36)], and creating augmented views [[84](#bib.bib84)],
    to name a few. The augmented graph is obtained via a sampler $\textsc{Sample}(\mathcal{G})$
    which can be vertex-based sampling [[50](#bib.bib50)], edge-based sampling [[147](#bib.bib147)],
    traversal-based sampling [[84](#bib.bib84)], and other advanced methods such as
    Metropolis-Hastings sampling [[81](#bib.bib81)]. For all the above graph samplers,
    they commonly return a connected subgraph induced from the sampled nodes. Mathematically,
    graph sampling can be represented as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图采样。图采样或子图采样是图数据增强的常用技术，可用于不同目的，比如扩展图神经网络 [[36](#bib.bib36)]，创建增强视图 [[84](#bib.bib84)]
    等。通过采样器 $\textsc{Sample}(\mathcal{G})$ 获取增强图，采样器可以采用基于顶点的采样 [[50](#bib.bib50)]，基于边的采样
    [[147](#bib.bib147)]，基于遍历的采样 [[84](#bib.bib84)]，以及其他高级方法，如 Metropolis-Hastings
    采样 [[81](#bib.bib81)]。对于上述所有图采样器，它们通常返回从采样节点诱导出的连接子图。从数学上讲，图采样可以表示为：
- en: '|  | $\tilde{\mathcal{G}}=\{\tilde{\mathbf{A}},\tilde{\mathbf{X}}\}=\{\mathbf{A}[idx,idx],\mathbf{X}[idx,:]\},$
    |  | (10) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\mathcal{G}}=\{\tilde{\mathbf{A}},\tilde{\mathbf{X}}\}=\{\mathbf{A}[idx,idx],\mathbf{X}[idx,:]\},$
    |  | (10) |'
- en: where $idx$ is a list of index to select the given elements (i.e., rows and
    columns) from $\mathbf{A}$ and $\mathbf{X}$. In general, the goal of graph sampling
    is to find augmented graph instances from the input graphs that best preserve
    desired properties by keeping a portion of nodes and their underlying linkages.
    For example, in the *graph sparsification* [[147](#bib.bib147), [75](#bib.bib75)]
    problem, researchers aim to sample the subgraph which preserves as much task-relevant
    information as possible.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $idx$ 是要从 $\mathbf{A}$ 和 $\mathbf{X}$ 中选择的给定元素（即行和列）的索引列表。通常，图采样的目标是从输入图中找到最能保留所需属性的增强图实例，通过保留部分节点及其基础链接。例如，在*图稀疏化*
    [[147](#bib.bib147), [75](#bib.bib75)]问题中，研究人员旨在采样保留尽可能多任务相关信息的子图。
- en: 'Node Dropping. In the literature, node dropping is also known as node masking.
    Specifically, a set of nodes $\hat{\mathcal{V}}=\{v_{i}\in\mathcal{V}\}$ will
    be dropped from the input graph, together with their associated edges $\hat{\mathcal{E}}=\{e_{i}\in\mathcal{E}\}$.
    This augmentation can be formulated as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 节点丢弃。在文献中，节点丢弃也被称为节点遮蔽。具体而言，将从输入图中丢弃一组节点 $\hat{\mathcal{V}}=\{v_{i}\in\mathcal{V}\}$，连同其相关的边
    $\hat{\mathcal{E}}=\{e_{i}\in\mathcal{E}\}$。这种增强可以表述为：
- en: '|  | $\mathbf{\tilde{A}}=\{\mathcal{V}\setminus\hat{\mathcal{V}},\mathcal{E}\setminus\hat{\mathcal{E}}\}.$
    |  | (11) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\tilde{A}}=\{\mathcal{V}\setminus\hat{\mathcal{V}},\mathcal{E}\setminus\hat{\mathcal{E}}\}.$
    |  | (11) |'
- en: Note that for attributed graphs, the corresponding node features will also be
    dropped at the same time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于属性图，相应的节点特征也会同时被丢弃。
- en: 'Node Insertion. Node insertion is commonly used to improve the message-passing
    or connectivity on the input graph by inserting virtual node(s) [[33](#bib.bib33)].
    Specifically, node insertion adds an extra set of nodes $\hat{\mathcal{V}}=\{v_{i}\}$
    and a set of edges $\hat{\mathcal{E}}=\{e_{i}\}$ between $\hat{\mathcal{V}}$ and
    $\mathcal{V}$ to the original node set $\mathcal{V}$ and the edge set $\mathcal{E}$,
    respectively:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 节点插入。节点插入通常用于通过插入虚拟节点来改善输入图上的消息传递或连通性 [[33](#bib.bib33)]。具体来说，节点插入会在原节点集合 $\mathcal{V}$
    和边集合 $\mathcal{E}$ 中分别添加一组额外的节点 $\hat{\mathcal{V}}=\{v_{i}\}$ 和一组边 $\hat{\mathcal{E}}=\{e_{i}\}$：
- en: '|  | $\mathbf{\tilde{A}}=\{\mathcal{V}\cup\hat{\mathcal{V}},\mathcal{E}\cup\hat{\mathcal{E}}\}.$
    |  | (12) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\tilde{A}}=\{\mathcal{V}\cup\hat{\mathcal{V}},\mathcal{E}\cup\hat{\mathcal{E}}\}.$
    |  | (12) |'
- en: Since node insertion also requires adding additional edges in the new graph,
    this GraphDA operation is highly related to graph rewiring. Note that for attributed
    graphs, the corresponding node features also need to be initialized, e.g., using
    the mean average of all the connected node features.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于节点插入还需要在新图中添加额外的边，因此这一 GraphDA 操作与图的重连性密切相关。注意，对于属性图，相应的节点特征也需要初始化，例如，使用所有连接节点特征的平均值。
- en: 'Graph Generation. Graph generation is commonly used as a GraphDA strategy for
    improving the scales of training graphs graph-level DGL tasks, e.g., graph classification.
    Most graph generation methods are expected to automatically learn from observed
    graphs. Generally, the graph generation process can be presented as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图生成。图生成通常作为 GraphDA 策略用于改善训练图的规模，特别是图级 DGL 任务，例如图分类。大多数图生成方法预计会从观察到的图中自动学习。一般来说，图生成过程可以表示如下：
- en: '|  | $\tilde{\mathcal{G}}\sim D_{\theta}(\mathcal{G}&#124;\{\mathcal{G}_{i}\}),$
    |  | (13) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\mathcal{G}}\sim D_{\theta}(\mathcal{G}&#124;\{\mathcal{G}_{i}\}),$
    |  | (13) |'
- en: where $\{\mathcal{G}_{i}\}$ is the set of observed graphs and here the augmentation
    function $D_{\theta}$ is a graph generation distribution parameterized by $\theta$.
    Notice that some techniques such as *graph coarsening* [[8](#bib.bib8)] and *graph
    condensation* [[55](#bib.bib55)] whose goals are to generate a new graph from
    the initial large graph can also be categorized into this augmentation operation.
    We also consider edge mixing between two (or mode) graphs [[35](#bib.bib35)] as
    one instantiation of graph generation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\{\mathcal{G}_{i}\}$ 是观察到的图的集合，这里增强函数 $D_{\theta}$ 是由 $\theta$ 参数化的图生成分布。注意，一些技术，如
    *图粗化* [[8](#bib.bib8)] 和 *图压缩* [[55](#bib.bib55)]，其目标是从初始大图生成新图，也可以归类为这种增强操作。我们还将两张（或多张）图之间的边混合
    [[35](#bib.bib35)] 视为图生成的一种实例。
- en: 3.2 Feature-oriented Augmentations
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 特征导向的增强
- en: In this subsection, we review the feature-oriented GraphDA techniques. Generally,
    given an input graph $\mathcal{G}=(\mathbf{A},\mathbf{X})$, a feature-oriented
    GraphDA operation focuses on performing transformation on the node feature matrix
    $\mathbf{X}$. Notably, we also consider those methods performing augmentations
    on the latent feature representations $\mathbf{H}$ as feature-oriented augmentation
    methods.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们回顾了以特征为导向的 GraphDA 技术。一般来说，给定一个输入图 $\mathcal{G}=(\mathbf{A},\mathbf{X})$，特征导向的
    GraphDA 操作专注于对节点特征矩阵 $\mathbf{X}$ 进行变换。特别地，我们还考虑那些在潜在特征表示 $\mathbf{H}$ 上进行增强的方法作为特征导向的增强方法。
- en: 'Feature Corruption. This GraphDA method aims at adding noises to either the
    original node features [[27](#bib.bib27)] or learned feature representations [[123](#bib.bib123)].
    For simplicity, here we use $\mathbf{x}_{i}$ to represent the original node features
    or learned feature representations:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 特征损坏。此 GraphDA 方法旨在向原始节点特征 [[27](#bib.bib27)] 或学习到的特征表示 [[123](#bib.bib123)]
    添加噪声。为了简便起见，这里我们用 $\mathbf{x}_{i}$ 表示原始节点特征或学习到的特征表示：
- en: '|  | $\mathbf{\tilde{x}}_{i}=\mathbf{x}_{i}+\mathbf{r}_{i},$ |  | (14) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\tilde{x}}_{i}=\mathbf{x}_{i}+\mathbf{r}_{i},$ |  | (14) |'
- en: where $\mathbf{r}_{i}$ denotes the added feature noise. Note that the feature
    noise could be either randomly added [[99](#bib.bib99)] or learned in an adversarial
    training fashion [[27](#bib.bib27), [123](#bib.bib123)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{r}_{i}$ 表示添加的特征噪声。注意，特征噪声可以是随机添加的 [[99](#bib.bib99)] 或以对抗训练方式学习的
    [[27](#bib.bib27), [123](#bib.bib123)]。
- en: 'Feature Shuffling. By randomly changing the contextual information through
    switching rows and columns in the feature matrix, The input feature matrix $\mathbf{X}$
    is corrupted to yield augmentations. This method can be formulated as:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 特征洗牌。通过随机更改特征矩阵中的上下文信息，方法是交换行和列，从而破坏输入特征矩阵 $\mathbf{X}$ 以产生增强。这种方法可以表示为：
- en: '|  | $\mathbf{\tilde{X}}=\mathbf{P}_{r}\mathbf{X}\mathbf{P}_{c},$ |  | (15)
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\tilde{X}}=\mathbf{P}_{r}\mathbf{X}\mathbf{P}_{c},$ |  | (15)
    |'
- en: where $\mathbf{P}_{r}$ and $\mathbf{P}_{c}$ are row-wise permutation matrix
    and column-wise permutation matrix, respectively. have exactly one entry of $1$
    in each row and each column and $0$ elsewhere.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{P}_{r}$ 和 $\mathbf{P}_{c}$ 分别是按行排列的置换矩阵和按列排列的置换矩阵。它们在每一行和每一列中都有一个
    $1$ 的条目，其余位置为 $0$。
- en: 'Feature Masking. The core operation of feature masking is to set a part of
    the entries in the node feature matrix $\mathbf{X}$ to $0$, which can be formulated
    as:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 特征掩码。特征掩码的核心操作是将节点特征矩阵 $\mathbf{X}$ 中的一部分条目设置为 $0$，可以表示为：
- en: '|  | $\mathbf{\tilde{X}}=\mathbf{X}\odot\mathbf{M}$ |  | (16) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\tilde{X}}=\mathbf{X}\odot\mathbf{M}$ |  | (16) |'
- en: where $\mathbf{M}$ is the masking matrix that $\mathbf{M}_{i,j}=0$ if the $j$-th
    element of vector $i$ is masked/dropped, otherwise $\mathbf{M}_{i,j}=1$. The masking
    matrix $\mathbf{M}$ is commonly generated by the Bernoulli distribution [[128](#bib.bib128),
    [127](#bib.bib127), [96](#bib.bib96)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{M}$ 是掩码矩阵，如果向量 $i$ 的第 $j$ 个元素被掩盖/丢弃，则 $\mathbf{M}_{i,j}=0$，否则 $\mathbf{M}_{i,j}=1$。掩码矩阵
    $\mathbf{M}$ 通常通过伯努利分布生成 [[128](#bib.bib128), [127](#bib.bib127), [96](#bib.bib96)]。
- en: 'Feature Addition. Since the input graph usually lacks informative features
    in real-world scenarios, Feature Addition can be used to (1) initiate node features
    on plain graphs to smoothly incorporate into DGL models (e.g., GNNs) and (2) supplement
    additional graph knowledge that are hard to be captured by GNN models. A straightforward
    way is to encode proximate/topological information (e.g., node index or node properties)
    into a feature vector and concatenate with the original node features. In general,
    Feature Addition can be expressed as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 特征添加。由于实际场景中的输入图通常缺乏有用的特征，特征添加可以用于（1）在简单图上初始化节点特征，以便顺利融入 DGL 模型（例如 GNNs），以及（2）补充
    GNN 模型难以捕获的额外图知识。一种简单的方法是将近似/拓扑信息（例如节点索引或节点属性）编码到特征向量中，并与原始节点特征连接。一般来说，特征添加可以表示为：
- en: '|  | $\tilde{\mathbf{x}}_{i}=[\hat{\mathbf{x}}_{i}&#124;&#124;\mathbf{x}_{i}],$
    |  | (17) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\mathbf{x}}_{i}=[\hat{\mathbf{x}}_{i}&#124;&#124;\mathbf{x}_{i}],$
    |  | (17) |'
- en: where $||$ denotes the concatenation operation and $\mathbf{x}_{i}$ could be
    an empty vector if the input graph is plain.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $||$ 表示连接操作，$\mathbf{x}_{i}$ 如果输入图是简单的，则可以是一个空向量。
- en: 'Feature Rewriting. Considering the fact that the given node features are commonly
    noisy and incomplete, recovering the clean and complete node features can directly
    improve the performance of the DGL models. Generally, feature rewriting can be
    expressed as:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重写。考虑到给定的节点特征通常是嘈杂和不完整的，恢复干净且完整的节点特征可以直接提高 DGL 模型的性能。一般来说，特征重写可以表示为：
- en: '|  | $\tilde{\mathbf{x}}=\alpha\mathbf{x}_{i}+\beta\mathbf{b}_{i},$ |  | (18)
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\mathbf{x}}=\alpha\mathbf{x}_{i}+\beta\mathbf{b}_{i},$ |  | (18)
    |'
- en: where $\alpha$ and $\beta$ are two controlling parameters and $\mathbf{b}_{i}$
    is a feature vector computed in a heuristic or learnable way. For example, Wang
    et al. propose feature replacement [[108](#bib.bib108)] that rewrites node features
    with its neighbors’ features. While Xu et al. [[121](#bib.bib121)] apply gradient
    descent-based optimizers to rewrite the node features as parameters.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 和 $\beta$ 是两个控制参数，$\mathbf{b}_{i}$ 是以启发式或可学习的方式计算的特征向量。例如，Wang 等
    [[108](#bib.bib108)] 提出了特征替换 [[108](#bib.bib108)]，用邻居的特征重写节点特征。而 Xu 等 [[121](#bib.bib121)]
    则应用基于梯度下降的优化器来将节点特征重写为参数。
- en: 'Feature Mixing. Based on the features of nodes in the input graph, feature
    mixing can be used to obtain the node features of a synthetic node:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 特征混合。基于输入图中节点的特征，特征混合可用于获得合成节点的节点特征：
- en: '|  | $\tilde{\mathbf{x}}=\lambda\mathbf{x}_{i}+(1-\lambda)\mathbf{x}_{j},$
    |  | (19) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\mathbf{x}}=\lambda\mathbf{x}_{i}+(1-\lambda)\mathbf{x}_{j},$
    |  | (19) |'
- en: where $\lambda$ is the mixing coefficient that controls the proportion of information
    from $\mathbf{x}_{i}$ and $\mathbf{x}_{j}$. Notably, feature mixing can also be
    performed on the intermediate representations learned from two training samples
    (i.e., Manifold Mixup [[100](#bib.bib100)]).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是混合系数，控制来自 $\mathbf{x}_{i}$ 和 $\mathbf{x}_{j}$ 的信息比例。值得注意的是，特征混合也可以在从两个训练样本中学习到的中间表示上进行（即，Manifold
    Mixup [[100](#bib.bib100)]）。
- en: 'Feature Propagation. Based on Graph Diffusion, Feature Propagation propagates
    the node features along the graph structure. It is an interpolation method that
    has also been widely used to augment the node features of the input graph. Mathematically:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 特征传播。基于图扩散，特征传播沿图结构传播节点特征。这是一种插值方法，也被广泛用于增强输入图的节点特征。从数学上讲：
- en: '|  | $\mathbf{\tilde{X}}=\mathbf{\tilde{A}}\mathbf{X},$ |  | (20) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\tilde{X}}=\mathbf{\tilde{A}}\mathbf{X},$ |  | (20) |'
- en: where $\mathbf{\tilde{A}}$ is the new adjacency matrix obtained through different
    graph diffusion methods.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\tilde{A}}$ 是通过不同图扩散方法获得的新邻接矩阵。
- en: 3.3 Label-oriented Augmentations
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 面向标签的增强
- en: Due to the expensive data labeling cost on graphs, label-oriented GraphDA is
    an important line of work to directly enrich the limited labeled training data.
    Commonly, there are two groups of strategies.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图上的数据标记成本高昂，面向标签的 GraphDA 是直接丰富有限标记训练数据的重要研究方向。通常，有两组策略。
- en: Pseudo-Labeling. Pseudo-labeling is a semi-supervised learning mechanism that
    aims to obtain one (or several) augmented labeled set(s), based on their most
    confident predictions on unlabeled data. Its learning process starts with a base
    teacher model trained on the labeled set $\mathcal{D}^{L}$, and then the teacher
    model is applied to the unlabeled data $\mathcal{D}^{U}$ to obtain pseudo labels
    (hard or soft) of unlabeled data. Finally, a subset of unlabeled data $\mathcal{D}^{P}$
    will be used to augment the training data, and the combined data $\mathcal{D}^{L}\cup\mathcal{D}^{P}$
    can be used to train a student model. In this sense, the label signals can be
    “propagated” to the unlabeled data samples via the learned teacher model. Note
    that this learning process could go through multiple rounds until converges by
    iteratively updating the teacher model with the current student model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 伪标签。伪标签是一种半监督学习机制，旨在基于对未标记数据的最自信预测来获得一个（或多个）增强的标记集。其学习过程从一个在标记集 $\mathcal{D}^{L}$
    上训练的基本教师模型开始，然后将教师模型应用于未标记数据 $\mathcal{D}^{U}$，以获得未标记数据的伪标签（硬标签或软标签）。最后，将未标记数据的子集
    $\mathcal{D}^{P}$ 用于增强训练数据，合并的数据 $\mathcal{D}^{L}\cup\mathcal{D}^{P}$ 可以用于训练学生模型。从这个意义上讲，标签信号可以通过学习到的教师模型“传播”到未标记的数据样本。注意，这一学习过程可能会经历多轮，直到通过迭代更新教师模型与当前学生模型相结合而收敛。
- en: 'Label Mixing. In order to enlarge the scale of training samples, we can directly
    interpolate the training samples based on the labeled examples. Generally, Mixup [[136](#bib.bib136)]
    constructs virtual training samples via feature mixing and label mixing:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 标签混合。为了扩大训练样本的规模，我们可以直接基于标记样本进行训练样本的插值。通常，Mixup [[136](#bib.bib136)] 通过特征混合和标签混合构建虚拟训练样本：
- en: '|  | $\displaystyle\tilde{\mathbf{x}}$ | $\displaystyle=\lambda\mathbf{x}_{i}+(1-\lambda)\mathbf{x}_{j},$
    |  | (21) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{\mathbf{x}}$ | $\displaystyle=\lambda\mathbf{x}_{i}+(1-\lambda)\mathbf{x}_{j},$
    |  | (21) |'
- en: '|  | $\displaystyle\tilde{\mathbf{y}}$ | $\displaystyle=\lambda\mathbf{y}_{i}+(1-\lambda)\mathbf{y}_{j},$
    |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{\mathbf{y}}$ | $\displaystyle=\lambda\mathbf{y}_{i}+(1-\lambda)\mathbf{y}_{j},$
    |  |'
- en: where $(\mathbf{x}_{i},\mathbf{y}_{i})$ and $(\mathbf{x}_{j},\mathbf{x}_{j})$
    are two labeled samples randomly sampled from the training set, and $\lambda\in[0,1]$.
    In this way, Mixup methods extend the training distribution by incorporating the
    prior knowledge that linear interpolations of feature vectors should lead to linear
    interpolations of the associated target labels.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\mathbf{x}_{i},\mathbf{y}_{i})$ 和 $(\mathbf{x}_{j},\mathbf{x}_{j})$ 是从训练集中随机抽取的两个标记样本，而
    $\lambda\in[0,1]$。通过这种方式，Mixup 方法通过结合线性插值特征向量的先验知识来扩展训练分布，这些知识表明特征向量的线性插值应当导致相关目标标签的线性插值。
- en: 'Table 1: Summary of representative GraphDA works for graph self-supervised
    learning. DT, JT, BO stand for decoupled training, joint training, and bi-level
    optimization, respectively'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 代表性 GraphDA 图自监督学习工作的总结。DT, JT, BO 分别代表解耦训练、联合训练和双层优化'
- en: '| Topic | Name | Ref. | Year | Venue | Task Level | Augmented Data Modality
    | Learnable | Augmentation Technique |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Topic | Name | Ref. | Year | Venue | Task Level | Augmented Data Modality
    | Learnable | Augmentation Technique |'
- en: '| Structure | Feature | Label |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Structure | Feature | Label |'
- en: '| Graph Generative Modeling | GraphMAE | [[43](#bib.bib43)] | 2022 | KDD |
    node & graph | ✓ |  |  |  | feature masking |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Graph Generative Modeling | GraphMAE | [[43](#bib.bib43)] | 2022 | KDD |
    节点 & 图 | ✓ |  |  |  | 特征遮蔽 |'
- en: '| GMAE | [[10](#bib.bib10)] | 2022 | arXiv | noded & graph | ✓ |  |  |  | node
    dropping |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GMAE | [[10](#bib.bib10)] | 2022 | arXiv | 节点 & 图 | ✓ |  |  |  | 节点丢弃 |'
- en: '| MGAE | [[95](#bib.bib95)] | 2022 | arXiv | node & edge | ✓ |  |  |  | edge
    perturbation (dropping) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MGAE | [[95](#bib.bib95)] | 2022 | arXiv | 节点 & 边 | ✓ |  |  |  | 边扰动（丢弃）
    |'
- en: '| MGM | [[77](#bib.bib77)] | 2021 | Nature Communications | graph |  | ✓ |  |  |
    feature corruption |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MGM | [[77](#bib.bib77)] | 2021 | Nature Communications | 图 |  | ✓ |  |  |
    特征破坏 |'
- en: '| GPT-GNN | [[44](#bib.bib44)] | 2020 | KDD | node | ✓ | ✓ |  |  | edge perturbation
    (dropping) feature masking |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| GPT-GNN | [[44](#bib.bib44)] | 2020 | KDD | 节点 | ✓ | ✓ |  |  | 边扰动（丢弃） 特征遮蔽
    |'
- en: '| MTL | [[129](#bib.bib129)] | 2020 | ICML | node |  | ✓ | ✓ | ✓(JT) | pseudo-labeling/feature
    corruption |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| MTL | [[129](#bib.bib129)] | 2020 | ICML | 节点 |  | ✓ | ✓ | ✓(JT) | 伪标签/特征破坏
    |'
- en: '| GraphBert | [[137](#bib.bib137)] | 2020 | arXiv | node | ✓ | ✓ |  |  | graph
    sampling |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| GraphBert | [[137](#bib.bib137)] | 2020 | arXiv | 节点 | ✓ | ✓ |  |  | 图采样
    |'
- en: '| Pre-train | [[45](#bib.bib45)] | 2019 | arXiv | node & edge & graph | ✓ |  |
    ✓ |  | edge perturbation/pseudo-labeling |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Pre-train | [[45](#bib.bib45)] | 2019 | arXiv | 节点 & 边 & 图 | ✓ |  | ✓ |  |
    边扰动/伪标签 |'
- en: '| Graph Contrastive Learning | S³-CL | [[24](#bib.bib24)] | 2022 | arXiv |
    node |  | ✓ | ✓ | ✓(JT) | feature propagation/pseudo labeling |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Graph Contrastive Learning | S³-CL | [[24](#bib.bib24)] | 2022 | arXiv |
    节点 |  | ✓ | ✓ | ✓(JT) | 特征传播/伪标签 |'
- en: '| SUGRL | [[78](#bib.bib78)] | 2022 | AAAI | node | ✓ | ✓ |  |  | feature shuffling/graph
    sampling |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| SUGRL | [[78](#bib.bib78)] | 2022 | AAAI | 节点 | ✓ | ✓ |  |  | 特征打乱/图采样 |'
- en: '| LG2AR | [[39](#bib.bib39)] | 2022 | arXiv | node & graph | ✓ | ✓ |  | ✓(BO)
    | node dropping/edge perturbation graph sampling/feature corruption |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| LG2AR | [[39](#bib.bib39)] | 2022 | arXiv | 节点 & 图 | ✓ | ✓ |  | ✓(BO) | 节点丢弃/边扰动
    图采样/特征破坏 |'
- en: '| BGRL | [[96](#bib.bib96)] | 2022 | ICLR | node | ✓ | ✓ |  |  | edge perturbation
    (dropping) feature masking |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| BGRL | [[96](#bib.bib96)] | 2022 | ICLR | 节点 | ✓ | ✓ |  |  | 边扰动（丢弃） 特征遮蔽
    |'
- en: '| ARIEL | [[28](#bib.bib28)] | 2022 | TheWebConf | node | ✓ | ✓ |  | ✓(JT)
    | feature corruption/edge perturbation |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| ARIEL | [[28](#bib.bib28)] | 2022 | TheWebConf | 节点 | ✓ | ✓ |  | ✓(JT) |
    特征破坏/边扰动 |'
- en: '| AD-GCL | [[93](#bib.bib93)] | 2021 | NeurIPS | graph | ✓ |  |  | ✓(JT) |
    edge perturbation (dropping) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| AD-GCL | [[93](#bib.bib93)] | 2021 | NeurIPS | 图 | ✓ |  |  | ✓(JT) | 边扰动（丢弃）
    |'
- en: '| JOAO | [[127](#bib.bib127)] | 2021 | ICML | graph | ✓ | ✓ |  | ✓(BO) | node
    dropping/edge perturbation graph sampling/feature corruption |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| JOAO | [[127](#bib.bib127)] | 2021 | ICML | 图 | ✓ | ✓ |  | ✓(BO) | 节点丢弃/边扰动
    图采样/特征破坏 |'
- en: '| GCA | [[149](#bib.bib149)] | 2021 | TheWebConf | node | ✓ | ✓ |  |  | edge
    perturbation (dropping) feature masking |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| GCA | [[149](#bib.bib149)] | 2021 | TheWebConf | 节点 | ✓ | ✓ |  |  | 边扰动（丢弃）
    特征遮蔽 |'
- en: '| MERIT | [[52](#bib.bib52)] | 2021 | IJCAI | graph | ✓ | ✓ |  |  | graph diffusion/edge
    perturbation graph sampling/feature masking |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| MERIT | [[52](#bib.bib52)] | 2021 | IJCAI | 图 | ✓ | ✓ |  |  | 图扩散/边扰动 图采样/特征遮蔽
    |'
- en: '| CSSL | [[133](#bib.bib133)] | 2021 | AAAI | graph | ✓ |  |  |  | node dropping/node
    insertion edge perturbation |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| CSSL | [[133](#bib.bib133)] | 2021 | AAAI | 图 | ✓ |  |  |  | 节点丢弃/节点插入 边扰动
    |'
- en: '| GraphCL | [[128](#bib.bib128)] | 2020 | NeurIPS | graph | ✓ | ✓ |  |  | node
    dropping/edge perturbation graph sampling/feature corruption |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| GraphCL | [[128](#bib.bib128)] | 2020 | NeurIPS | 图 | ✓ | ✓ |  |  | 节点丢弃/边扰动
    图采样/特征破坏 |'
- en: '| GCC | [[84](#bib.bib84)] | 2020 | KDD | graph | ✓ |  |  |  | graph sampling
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| GCC | [[84](#bib.bib84)] | 2020 | KDD | 图 | ✓ |  |  |  | 图采样 |'
- en: '| SUBGCON | [[50](#bib.bib50)] | 2020 | ICDM | node | ✓ |  |  |  | graph sampling
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| SUBGCON | [[50](#bib.bib50)] | 2020 | ICDM | 节点 | ✓ |  |  |  | 图采样 |'
- en: '| MVGRL | [[38](#bib.bib38)] | 2020 | ICML | node & graph | ✓ |  |  |  | graph
    diffusion/graph sampling |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| MVGRL | [[38](#bib.bib38)] | 2020 | ICML | 节点 & 图 | ✓ |  |  |  | 图扩散/图采样
    |'
- en: '| GRACE | [[148](#bib.bib148)] | 2020 | arXiv | node | ✓ | ✓ |  |  | edge perturbation
    (dropping) feature masking |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| GRACE | [[148](#bib.bib148)] | 2020 | arXiv | 节点 | ✓ | ✓ |  |  | 边缘扰动（丢弃）特征遮蔽
    |'
- en: '| DGI | [[99](#bib.bib99)] | 2019 | ICLR | node |  | ✓ |  |  | feature corruption
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| DGI | [[99](#bib.bib99)] | 2019 | ICLR | 节点 |  | ✓ |  |  | 特征腐蚀 |'
- en: 4 Graph Data Augmentation for
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 图数据增强
- en: Low-Resource Graph Learning
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 低资源图学习
- en: 'The shortage of ground-truth labels has been a longstanding and notorious problem
    for learning effective DGL. To advance the research on Low-resource Graph Learning,
    GraphDA has been actively investigated and shows promising results. In this section,
    we discuss the applications of GraphDA for solving both Graph Self-Supervised
    Learning and Graph Semi-Supervised Learning. We summarize the representative works
    in Table [1](#S3.T1 "Table 1 ‣ 3.3 Label-oriented Augmentations ‣ 3 Techniques
    of Graph Data Augmentation ‣ Data Augmentation for Deep Graph Learning: A Survey")
    and Table [2](#S4.T2 "Table 2 ‣ 4.1 Graph Self-Supervised Learning ‣ 4 Graph Data
    Augmentation for Low-Resource Graph Learning ‣ Data Augmentation for Deep Graph
    Learning: A Survey"), respectively.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 真实标签的短缺一直是学习有效深度图学习（DGL）的长期存在且臭名昭著的问题。为了推动低资源图学习的研究，GraphDA 已被积极研究并显示出有希望的结果。在本节中，我们讨论了
    GraphDA 在解决图自监督学习和图半监督学习中的应用。我们在表 [1](#S3.T1 "表 1 ‣ 3.3 标签导向增强 ‣ 3 图数据增强技术 ‣ 深度图学习的数据增强：综述")
    和表 [2](#S4.T2 "表 2 ‣ 4.1 图自监督学习 ‣ 4 低资源图学习的图数据增强 ‣ 深度图学习的数据增强：综述") 中总结了代表性的工作。
- en: '![Refer to caption](img/3cbe7abd437b365e10c8c288b6b44038.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3cbe7abd437b365e10c8c288b6b44038.png)'
- en: (a) Graph Generative Modeling
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 图生成建模
- en: '![Refer to caption](img/8680133397ef1920fab9d59745a5bc94.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8680133397ef1920fab9d59745a5bc94.png)'
- en: (b) Graph Contrastive Learning
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 图对比学习
- en: 'Figure 3: Comparison between the workflows of Graph Generative Modeling and
    Graph Contrastive Learning.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：图生成建模与图对比学习的工作流程比较。
- en: 4.1 Graph Self-Supervised Learning
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 图自监督学习
- en: 'Graph Generative Modeling. Recently data augmentation has been widely used
    for graph self-supervised learning (SSL). Based on the idea of Graph AutoEncoders
    (GAE) [[58](#bib.bib58), [20](#bib.bib20)], graph generative modeling methods
    perform data augmentation on the input graphs through either edge perturbation,
    feature masking, or node dropping (masking) then learn the node representations
    by reconstructing feature or/and structure information from the augmented graphs
    (as shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4 Graph Data Augmentation for Low-Resource
    Graph Learning ‣ Data Augmentation for Deep Graph Learning: A Survey") (a)). For
    example, Denoising Link Reconstruction [[45](#bib.bib45)] randomly drops existing
    edges to obtain the perturbed graph and tries to recover the discarded connections
    with a pairwise similarity-based decoder trained by the cross-entropy loss. Given
    a graph with its nodes and edges randomly masked, GPT-GNN [[44](#bib.bib44)] generates
    one masked node and its related edges jointly and optimizes the likelihood of
    the node and edges generated in the current iteration. GraphBert [[137](#bib.bib137)]
    samples linkless subgraphs and pre-trains a graph transformer model via node feature
    reconstruction and graph structure recovery. You et al. [[129](#bib.bib129)] define
    the graph completion pretext task which aims to recover the masked feature of
    target nodes based on their neighbors’ features and connections. MGM [[77](#bib.bib77)]
    tries to reconstruct the masked node/edge features for learning the GNNs for molecule
    generation. GMAE [[10](#bib.bib10)] and GraphMAE [[43](#bib.bib43)] first randomly
    mask the features of some nodes and then their decoders reconstruct the original
    features of the masked nodes, while MGAE [[95](#bib.bib95)] tries to reconstruct
    the masked edges based on the augmented graphs.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '图生成建模。最近，数据增强被广泛应用于图自监督学习（SSL）。基于图自动编码器（GAE）的思想[[58](#bib.bib58), [20](#bib.bib20)]，图生成建模方法通过边扰动、特征掩蔽或节点丢弃（掩蔽）对输入图进行数据增强，然后通过从增强图中重建特征或/和结构信息来学习节点表示（如图[3](#S4.F3
    "Figure 3 ‣ 4 Graph Data Augmentation for Low-Resource Graph Learning ‣ Data Augmentation
    for Deep Graph Learning: A Survey")（a）所示）。例如，去噪链接重建[[45](#bib.bib45)] 随机丢弃现有边以获得扰动图，并尝试通过交叉熵损失训练的基于成对相似度的解码器恢复丢弃的连接。给定一个节点和边随机被掩蔽的图，GPT-GNN[[44](#bib.bib44)]
    生成一个掩蔽节点及其相关边，并优化当前迭代中生成的节点和边的可能性。GraphBert[[137](#bib.bib137)] 通过节点特征重建和图结构恢复对无链接子图进行采样，并预训练图转换器模型。You
    等人[[129](#bib.bib129)] 定义了图完成预任务，旨在根据目标节点的邻居特征和连接恢复掩蔽特征。MGM[[77](#bib.bib77)]
    尝试重建掩蔽节点/边特征，以便为分子生成学习GNN。GMAE[[10](#bib.bib10)] 和GraphMAE[[43](#bib.bib43)] 首先随机掩蔽一些节点的特征，然后它们的解码器重建掩蔽节点的原始特征，而MGAE[[95](#bib.bib95)]
    尝试根据增强图重建掩蔽边。'
- en: 'Graph Contrastive Learning. Motivated by the recent breakthroughs in contrastive
    visual feature learning, data augmentation has also been widely used for Graph
    Contrastive Learning (GCL). As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ 4 Graph
    Data Augmentation for Low-Resource Graph Learning ‣ Data Augmentation for Deep
    Graph Learning: A Survey") (b), GCL methods try to generate augmented examples
    from the input and view two augmented examples from the same original sample as
    a positive pair, while those from different original samples are negative pairs.
    By applying contrastive learning loss, the positive pairs will be pulled together
    and the negative pairs will be pushed away in the latent space. Therefore, GraphDA
    plays an essential role in GCL. As a pioneering work, DGI [[99](#bib.bib99)] applies
    feature shuffling and edge perturbation to obtain the negative pairs of graphs,
    then a contrastive objective is proposed to maximize the mutual information between
    node embeddings and a global summary embedding. Another popular GraphDA method
    for GCL is graph sampling. As an example, GCC [[84](#bib.bib84)] proposes to sample
    subgraphs as contrastive instances to pre-train the graph encoder, which can be
    used for different downstream tasks with either freezing or full fine-tuning strategy.
    SUBGCON [[50](#bib.bib50)] also utilizes a subgraph sampler based on Personalized
    PageRank to sample the augmented subgraph and perform contrastive learning between
    the representations of the central node and the sampled subgraph.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '图对比学习。受到最近对比视觉特征学习突破的激励，数据增强也被广泛应用于图对比学习（GCL）。如图 [3](#S4.F3 "Figure 3 ‣ 4 Graph
    Data Augmentation for Low-Resource Graph Learning ‣ Data Augmentation for Deep
    Graph Learning: A Survey") (b) 所示，GCL 方法试图从输入中生成增强样本，并将来自相同原始样本的两个增强样本视为正对，而来自不同原始样本的则为负对。通过应用对比学习损失，正对会被拉近，而负对会被推远在潜在空间中。因此，GraphDA
    在 GCL 中发挥着至关重要的作用。作为开创性工作，DGI [[99](#bib.bib99)] 采用特征洗牌和边扰动来获得图的负对，然后提出了一个对比目标以最大化节点嵌入与全局摘要嵌入之间的互信息。另一个流行的
    GraphDA 方法是图采样。例如，GCC [[84](#bib.bib84)] 提出了将子图采样作为对比实例来预训练图编码器，该编码器可以用于不同的下游任务，无论是冻结还是完全微调策略。SUBGCON [[50](#bib.bib50)]
    还利用基于个性化 PageRank 的子图采样器来采样增强的子图，并在中心节点的表示与采样子图之间进行对比学习。'
- en: 'It is noteworthy that many GCL methods usually leverage combinations of augmentation
    strategies. For instance, MVGRL [[38](#bib.bib38)] first augments the input graph
    via graph diffusion. Afterward, two graph views are generated by subgraph sampling
    and the model learns to contrast node representations to global representations
    across the two views. GRACE [[148](#bib.bib148)] adopts two augmentation strategies,
    i.e., edge perturbation (dropping) and feature masking, to generate augmented
    views of graph data. It jointly considers both intra-view and inter-view negative
    pairs for the contrastive learning objectives. gCooL [[69](#bib.bib69)] uses the
    same augmentations and considers the community information in GCL. GraphCL [[128](#bib.bib128)]
    considers four GraphDA operations: node dropping, edge perturbation, feature masking,
    and subgraph sampling, while MERIT [[52](#bib.bib52)] leverages graph diffusion,
    edge perturbation (dropping), subgraph sampling, and feature masking to generate
    augmented graphs. CSSL [[133](#bib.bib133)] augments graphs with the edge perturbation
    (dropping) and node dropping. Recent work SUGRL [[78](#bib.bib78)] leverages feature
    shuffling and graph sampling, S³-CL uses feature propagation and pseudo labeling
    to efficiently perform GCL.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，许多 GCL 方法通常利用增强策略的组合。例如，MVGRL [[38](#bib.bib38)] 首先通过图扩散来增强输入图。之后，通过子图采样生成两个图视图，模型学习对比这两个视图中的节点表示与全局表示。GRACE [[148](#bib.bib148)]
    采用两种增强策略，即边扰动（丢弃）和特征掩蔽，以生成图数据的增强视图。它同时考虑了视图内和视图间的负对进行对比学习目标。gCooL [[69](#bib.bib69)]
    使用相同的增强方式，并在 GCL 中考虑社区信息。GraphCL [[128](#bib.bib128)] 考虑了四种 GraphDA 操作：节点丢弃、边扰动、特征掩蔽和子图采样，而
    MERIT [[52](#bib.bib52)] 利用图扩散、边扰动（丢弃）、子图采样和特征掩蔽来生成增强图。CSSL [[133](#bib.bib133)]
    通过边扰动（丢弃）和节点丢弃来增强图。最近的工作 SUGRL [[78](#bib.bib78)] 利用特征洗牌和图采样，S³-CL 使用特征传播和伪标签来高效执行
    GCL。
- en: 'Different from the aforementioned pre-defined GraphDA strategies, another line
    of research proposes to perform data augmentation on graphs in a learnable manner.
    Specifically, Zhu et al. [[149](#bib.bib149)] propose a joint, adaptive data augmentation
    scheme based on edge perturbation (dropping) and feature masking to provide diverse
    contexts for nodes in different views, so as to boost optimization of the contrastive
    objective. GASSL [[123](#bib.bib123)] is an adversarial self-supervised learning
    framework for learning unsupervised representations of graph data without any
    handcrafted views. It learns to perform feature corruption on either the input
    or latent space. AD-GCL [[93](#bib.bib93)] enables GNNs to avoid capturing redundant
    information during the training by optimizing edge perturbation (dropping) strategy
    used in GCL in an adversarial fashion. To automatically select optimal augmentation
    combinations for the given graph dataset, JOAO [[127](#bib.bib127)] and LG2AR [[39](#bib.bib39)]
    are proposed to automatically select augmentations from a given pool of augmentation
    types: {node dropping, subgraph sampling, edge perturbation, feature masking}.
    It is worth mentioning that there are increasingly more works in GCL, we will
    not cover all of them due to the space limit.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述预定义的GraphDA策略不同，另一类研究提出在图上以可学习的方式执行数据增强。具体来说，Zhu等人[[149](#bib.bib149)]提出了一种基于边缘扰动（丢弃）和特征掩蔽的联合、自适应数据增强方案，以为不同视图中的节点提供多样化的上下文，从而提升对比目标的优化。GASSL
    [[123](#bib.bib123)]是一个对抗性自监督学习框架，用于学习图数据的无监督表示而无需任何手工定义的视图。它学习在输入或潜在空间上执行特征损坏。AD-GCL
    [[93](#bib.bib93)]使得GNN在训练过程中通过对抗性地优化GCL中使用的边缘扰动（丢弃）策略，从而避免捕获冗余信息。为了自动选择给定图数据集的最佳增强组合，JOAO
    [[127](#bib.bib127)]和LG2AR [[39](#bib.bib39)]被提出以从给定的增强类型池中自动选择增强：{节点丢弃、子图采样、边缘扰动、特征掩蔽}。值得一提的是，GCL领域的相关工作越来越多，由于篇幅限制，我们不会覆盖所有这些工作。
- en: 'Table 2: Summary of representative GraphDA works for graph semi-supervised
    learning. DT, JT, BO stand for decoupled training, joint training, and bi-level
    optimization, respectively'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：图半监督学习的代表性GraphDA工作的总结。DT、JT、BO分别代表解耦训练、联合训练和双层优化。
- en: '| Topic | Name | Ref. | Year | Venue | Task Level | Augmented Data Modality
    | Learnable | Augmentation Technique |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 名称 | 参考文献 | 年份 | 会议 | 任务级别 | 增强数据模态 | 可学习 | 增强技术 |'
- en: '| Structure | Feature | Label |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 特征 | 标签 |'
- en: '| Graph Consistency Training | NodeAug | [[108](#bib.bib108)] | 2020 | KDD
    | node | ✓ | ✓ | ✓ |  | feature rewriting/graph rewiring pseudo labeling |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 图一致性训练 | NodeAug | [[108](#bib.bib108)] | 2020 | KDD | 节点 | ✓ | ✓ | ✓ |  |
    特征重写/图重连伪标签化 |'
- en: '| GRAND | [[30](#bib.bib30)] | 2020 | NeurIPS | node |  | ✓ | ✓ |  | node dropping/feature
    propagation pseudo labeling |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| GRAND | [[30](#bib.bib30)] | 2020 | NeurIPS | 节点 |  | ✓ | ✓ |  | 节点丢弃/特征传播伪标签化
    |'
- en: '| SCR | [[135](#bib.bib135)] | 2021 | arXiv | node | ✓ |  |  |  | graph rewiring
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| SCR | [[135](#bib.bib135)] | 2021 | arXiv | 节点 | ✓ |  |  |  | 图重连 |'
- en: '| MH-Aug | [[81](#bib.bib81)] | 2021 | NeurIPS | node | ✓ |  | ✓ |  | graph
    sampling |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| MH-Aug | [[81](#bib.bib81)] | 2021 | NeurIPS | 节点 | ✓ |  | ✓ |  | 图采样 |'
- en: '| NASA | [[5](#bib.bib5)] | 2022 | AAAI | node | ✓ |  | ✓ |  | edge perturbation
    (dropping) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| NASA | [[5](#bib.bib5)] | 2022 | AAAI | 节点 | ✓ |  | ✓ |  | 边缘扰动（丢弃） |'
- en: '| Graph Self-/Co-Training | Meta-PN | [[21](#bib.bib21)] | 2022 | AAAI | node
    |  |  | ✓ | ✓(BO) | pseudo-labeling |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 图自/协同训练 | Meta-PN | [[21](#bib.bib21)] | 2022 | AAAI | 节点 |  |  | ✓ | ✓(BO)
    | 伪标签化 |'
- en: '| NRGNN | [[18](#bib.bib18)] | 2021 | KDD | node | ✓ |  | ✓ | ✓(DT) | graph
    rewiring/pseudo-labeling |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| NRGNN | [[18](#bib.bib18)] | 2021 | KDD | 节点 | ✓ |  | ✓ | ✓(DT) | 图重连/伪标签化
    |'
- en: '| PTA | [[25](#bib.bib25)] | 2021 | TheWebConf | node |  |  | ✓ |  | pseudo-labeling
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| PTA | [[25](#bib.bib25)] | 2021 | TheWebConf | 节点 |  |  | ✓ |  | 伪标签化 |'
- en: '| CGCN | [[47](#bib.bib47)] | 2020 | AAAI | node |  |  | ✓ | ✓(JT) | pseudo-labeling
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| CGCN | [[47](#bib.bib47)] | 2020 | AAAI | 节点 |  |  | ✓ | ✓(JT) | 伪标签化 |'
- en: '| M3S | [[90](#bib.bib90)] | 2020 | AAAI | node |  |  | ✓ | ✓(JT) | pseudo-labeling
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| M3S | [[90](#bib.bib90)] | 2020 | AAAI | 节点 |  |  | ✓ | ✓(JT) | 伪标签化 |'
- en: '| Co-GCN | [[68](#bib.bib68)] | 2020 | AAAI | node | ✓ |  |  | ✓(DT) | feature
    masking/pseudo labeling |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Co-GCN | [[68](#bib.bib68)] | 2020 | AAAI | 节点 | ✓ |  |  | ✓(DT) | 特征掩蔽/伪标签化
    |'
- en: '| ST-GCNs | [[67](#bib.bib67)] | 2018 | AAAI | node |  |  | ✓ | ✓(DT) | pseudo-labeling
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ST-GCNs | [[67](#bib.bib67)] | 2018 | AAAI | 节点 |  |  | ✓ | ✓(DT) | 伪标签化
    |'
- en: '| Graph Data Interpolation | G-Transplant | [[82](#bib.bib82)] | 2022 | AAAI
    | graph | ✓ | ✓ | ✓ | ✓(DT) | label mixing/graph rewiring |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 图数据插值 | G-Transplant | [[82](#bib.bib82)] | 2022 | AAAI | 图 | ✓ | ✓ | ✓ |
    ✓(DT) | 标签混合/图重连 |'
- en: '| G-Mixup | [[107](#bib.bib107)] | 2021 | TheWebConf | graph |  | ✓ | ✓ | ✓(DT)
    | graph generation/label mixing |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| G-Mixup | [[107](#bib.bib107)] | 2021 | TheWebConf | 图形 |  | ✓ | ✓ | ✓(DT)
    | 图形生成/标签混合 |'
- en: '| GraphMix | [[101](#bib.bib101)] | 2021 | AAAI | node |  | ✓ | ✓ |  | feature
    mixing/label mixing/pseudo labeling |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| GraphMix | [[101](#bib.bib101)] | 2021 | AAAI | 节点 |  | ✓ | ✓ |  | 特征混合/标签混合/伪标签
    |'
- en: '| ifMixup | [[35](#bib.bib35)] | 2021 | arXiv | graph | ✓ | ✓ | ✓ |  | feature
    mixing/label mixing/graph generation |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ifMixup | [[35](#bib.bib35)] | 2021 | arXiv | 图形 | ✓ | ✓ | ✓ |  | 特征混合/标签混合/图形生成
    |'
- en: '| Graph Imbalanced Training | GATSMOTE | [[73](#bib.bib73)] | 2022 | Mathematics
    | node | ✓ | ✓ | ✓ | ✓(JT) | feature rewriting/node insertion/graph rewiring |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Graph Imbalanced Training | GATSMOTE | [[73](#bib.bib73)] | 2022 | Mathematics
    | 节点 | ✓ | ✓ | ✓ | ✓(JT) | 特征重写/节点插入/图形重连 |'
- en: '| GNN-CL | [[69](#bib.bib69)] | 2022 | SDM | node | ✓ | ✓ | ✓ | ✓(JT) | feature
    mixing/node insertion/graph rewiring |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| GNN-CL | [[69](#bib.bib69)] | 2022 | SDM | 节点 | ✓ | ✓ | ✓ | ✓(JT) | 特征混合/节点插入/图形重连
    |'
- en: '| GraphSMOTE | [[146](#bib.bib146)] | 2021 | WSDM | node | ✓ | ✓ | ✓ | ✓(JT)
    | feature mixing/node insertion/graph rewiring |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| GraphSMOTE | [[146](#bib.bib146)] | 2021 | WSDM | 节点 | ✓ | ✓ | ✓ | ✓(JT)
    | 特征混合/节点插入/图形重连 |'
- en: '| DPGNN | [[106](#bib.bib106)] | 2021 | arXiv | node |  |  | ✓ |  | pseudo-labelling
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| DPGNN | [[106](#bib.bib106)] | 2021 | arXiv | 节点 |  |  | ✓ |  | 伪标签 |'
- en: '| D-GCN | [[104](#bib.bib104)] | 2021 | ICCSAE | node | ✓ |  |  |  | graph
    rewiring |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| D-GCN | [[104](#bib.bib104)] | 2021 | ICCSAE | 节点 | ✓ |  |  |  | 图形重连 |'
- en: '| GraphMixup | [[112](#bib.bib112)] | 2021 | arXiv | node | ✓ | ✓ | ✓ | ✓(BO)
    | feature mixing/label mixing/graph rewiring |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| GraphMixup | [[112](#bib.bib112)] | 2021 | arXiv | 节点 | ✓ | ✓ | ✓ | ✓(BO)
    | 特征混合/标签混合/图形重连 |'
- en: 4.2 Graph Semi-Supervised Learning
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 图形半监督学习
- en: Graph Consistency Training. Similar to the idea of contrastive learning, Consistency
    Training [[118](#bib.bib118)] leverages unlabeled data to improve model performance
    by enforcing the consistency across predictions learned from different stochastic
    augmentations of the same input. As one effective semi-supervised learning paradigm,
    consistency training has also been explored in learning graph neural networks
    under low-resource settings. For example, NodeAug [[108](#bib.bib108)] uses local
    structure-wise augmentation operations (i.e., feature corruption, and edge perturbation),
    and minimizes the KL-divergence between the node representations learned from
    the original graph and augmented graph. GRAND [[30](#bib.bib30)] creates multiple
    different augmented graphs with node dropping and feature masking, followed by
    feature propagation. Then the consistency loss is applied to minimize the distances
    of the representations learned from the augmented graphs. Following GRAND, Zhang
    et al. [[135](#bib.bib135)] propose to use two edge perturbation methods – DropEdge [[85](#bib.bib85)]
    or DropNode [[30](#bib.bib30)] as the augmentation strategies and leverage a mean-teacher
    consistency regularization to guide the training of the GNN model by calculating
    a consistency loss between the student and teacher models. To enable consistency
    training on large-scale graphs, Hawkins et al. [[40](#bib.bib40)] propose to use
    graph sampling to generate different neighborhood subgraph expansions and ensemble
    the predictions to provide pseudo labels. To avoid the detrimental effects of
    arbitrary augmentations, Park et al. [[81](#bib.bib81)] propose a graph sampling
    method MH-Aug that uses the Metropolis-Hastings algorithm to obtain the augmented
    samples of the input graph, and adopt consistency training to better utilize the
    unlabeled data. Following the idea of graph rewiring, NASA [[5](#bib.bib5)] generates
    graph augmentations with high consistency and diversity by replacing immediate
    neighbors with remote neighbors and enforcing the predictions of augmented neighbors
    to be consistent.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图一致性训练。类似于对比学习的理念，一致性训练[[118](#bib.bib118)] 利用未标记的数据，通过强制不同随机增强的相同输入的预测一致性来提高模型性能。作为一种有效的半监督学习范式，一致性训练在低资源设置下的图神经网络学习中也得到了探索。例如，NodeAug[[108](#bib.bib108)]
    使用局部结构增强操作（即特征破坏和边扰动），并最小化从原始图和增强图中学到的节点表示之间的KL散度。 GRAND[[30](#bib.bib30)] 创建了多个不同的增强图，采用节点丢弃和特征掩蔽，然后进行特征传播。接着，应用一致性损失来最小化从增强图中学到的表示之间的距离。继
    GRAND 之后，Zhang 等人[[135](#bib.bib135)] 提出了使用两种边扰动方法—DropEdge[[85](#bib.bib85)]
    或 DropNode[[30](#bib.bib30)] 作为增强策略，并利用均值教师一致性正则化来指导 GNN 模型的训练，通过计算学生模型和教师模型之间的一致性损失。为了在大规模图上实现一致性训练，Hawkins
    等人[[40](#bib.bib40)] 提出了使用图采样生成不同的邻域子图扩展，并将预测结果进行集成以提供伪标签。为了避免任意增强的有害效果，Park 等人[[81](#bib.bib81)]
    提出了图采样方法 MH-Aug，该方法使用 Metropolis-Hastings 算法获得输入图的增强样本，并采用一致性训练以更好地利用未标记数据。继图重连的理念之后，NASA[[5](#bib.bib5)]
    通过用远程邻居替换直接邻居并强制增强邻居的预测一致性，生成高一致性和多样性的图增强。
- en: Graph Self-/Co-Training. To address the data scarcity issue, one effective solution
    is to leverage the unlabeled data to augment the limited labeled data. Following
    the idea of pseudo labeling, self-training [[124](#bib.bib124)] imputes the labels
    on unlabeled data based on a teacher model trained with limited labeled data,
    and it has become a prevailing paradigm to solve the problem of semi-supervised
    node classification when training data is limited. Among those methods, Li et
    al. [[67](#bib.bib67)] first combine GCNs with self-training to expand supervision
    signals. CGCN [[47](#bib.bib47)] generates pseudo labels by combining variational
    graph auto-encoder with Gaussian mixture models. Furthermore, M3S [[90](#bib.bib90)]
    propose the multi-stage self-training and utilize a clustering method to eliminate
    the pseudo labels that may be incorrect. Similar ideas can also be found in [[18](#bib.bib18)].
    In addition, recent research [[25](#bib.bib25), [21](#bib.bib21)] adopt label
    propagation as the teacher model to generate pseudo labels that encode valuable
    global structure knowledge.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图自我/协同训练。为了解决数据稀缺问题，一种有效的解决方案是利用未标记的数据来扩充有限的标记数据。遵循伪标签的理念，自我训练 [[124](#bib.bib124)]
    基于一个使用有限标记数据训练的教师模型对未标记数据进行标记，这已成为解决训练数据有限时半监督节点分类问题的流行范式。在这些方法中，Li 等人 [[67](#bib.bib67)]
    首次将GCNs与自我训练结合，以扩展监督信号。CGCN [[47](#bib.bib47)] 通过结合变分图自编码器和高斯混合模型生成伪标签。此外，M3S [[90](#bib.bib90)]
    提出了多阶段自我训练，并利用聚类方法来消除可能不正确的伪标签。类似的思想也可以在 [[18](#bib.bib18)] 中找到。此外，最近的研究 [[25](#bib.bib25),
    [21](#bib.bib21)] 采用标签传播作为教师模型，以生成编码有价值的全球结构知识的伪标签。
- en: Similar to Self-training, Co-training [[4](#bib.bib4)] has also been investigated
    for augmenting the training set with unlabeled data. It learns two classifiers
    with initial labeled data on the two views respectively and lets them label unlabeled
    data for each other to augment the training data. Li et al. [[68](#bib.bib68)]
    develop a novel multi-view semi-supervised learning method Co-GCN based on feature
    masking, which unifies GCN and co-training into one framework.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于自我训练，协同训练 [[4](#bib.bib4)] 也被研究用于用未标记数据扩充训练集。它分别在两个视图上使用初始标记数据学习两个分类器，并让它们为彼此标记未标记的数据以扩充训练数据。Li
    等人 [[68](#bib.bib68)] 开发了一种基于特征掩码的创新性多视图半监督学习方法 Co-GCN，将 GCN 和协同训练统一到一个框架中。
- en: Graph Data Interpolation. Another way of obtaining extra training examples is
    to use interpolation-based data augmentation strategy, such as Mixup [[136](#bib.bib136)]
    to generate synthetic training examples (i.e., node insertion) based on feature
    mixing and label mixing. While unlike images or natural sentences, graphs have
    arbitrary structure, it remains a non-trivial task to identify the meaningful
    connections between the original nodes and synthetic nodes. Also, due to the cascading
    effect of graph data, even simply adding an edge into a graph can dramatically
    change the graph semantic meanings. To circumvent those challenges, Manifold Mixup [[100](#bib.bib100)]
    has been applied to graph data interpolation. Specifically, GraphMix [[101](#bib.bib101)]
    trains a fully connected network (FCN) jointly with the GNN via parameter sharing,
    and the FCN is learned based on Manifold Mixup and pseudo-labeling, which can
    effectively train GNNs for semi-supervised node classification. Similarly, Wang
    et al. [[107](#bib.bib107)] also follow the idea of Manifold Mixup and interpolate
    the input features of both nodes and graphs in the embedding space. Those methods
    leverage a simple way to avoid dealing with the arbitrary structure in the input
    space for mixing a node or graph pair, through mixing the graph representation
    learned from GNNs.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据插值。获取额外训练样本的另一种方法是使用基于插值的数据增强策略，例如 Mixup [[136](#bib.bib136)]，通过特征混合和标签混合生成合成训练样本（即节点插入）。不同于图像或自然句子，图具有任意结构，识别原始节点和合成节点之间有意义的连接仍然是一个复杂的任务。此外，由于图数据的级联效应，即使是简单地向图中添加一条边，也会显著改变图的语义含义。为了规避这些挑战，Manifold
    Mixup [[100](#bib.bib100)] 已被应用于图数据插值。具体来说，GraphMix [[101](#bib.bib101)] 通过参数共享与GNN联合训练一个全连接网络（FCN），并基于Manifold
    Mixup和伪标签学习FCN，这可以有效地训练GNN进行半监督节点分类。类似地，Wang 等人 [[107](#bib.bib107)] 也遵循Manifold
    Mixup的思想，并在嵌入空间中插值节点和图的输入特征。这些方法利用一种简单的方法来避免处理输入空间中任意结构的节点或图对，通过混合从GNN中学习到的图表示。
- en: For the input-level graph data interpolation, ifMixup [[35](#bib.bib35)] targets
    the Manifold Intrusion issue (mixing graph pairs may naturally create graphs with
    identical structure but with conflict labels) and first interpolates both the
    node features and the edges of the input pair based on feature mixing and graph
    generation. Graph Transplant [[82](#bib.bib82)] is another input-level graph interpolation
    method that leverages graph rewiring to mix two dissimilar-structured graphs by
    replacing the destination subgraph with the source subgraph while preserving the
    local structure. $G$-Mixup [[37](#bib.bib37)] first estimates the graphon of each
    class and then mixup between the graphons and perform graph generation to generate
    interpolated graphs, which improves the generalizability and robustness of GNNs
    for semi-supervised graph classification.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入级图数据插值，如果**Mixup**[[35](#bib.bib35)] 针对流形入侵问题（混合图对可能自然地创建具有相同结构但标签冲突的图）并首先基于特征混合和图生成对输入对的节点特征和边进行插值。**Graph
    Transplant**[[82](#bib.bib82)] 是另一种输入级图插值方法，通过图重连利用图重构，将目标子图替换为源子图，同时保持局部结构，从而混合两种不同结构的图。**$G$-Mixup**[[37](#bib.bib37)]
    首先估计每个类别的图谱，然后在图谱之间进行混合，并执行图生成以生成插值图，这提高了 GNN 在半监督图分类中的泛化能力和鲁棒性。
- en: Graph Imbalanced Training. The class distribution of graph data is inherently
    imbalanced which follows the power-law distribution. As an example, on the benchmark
    Pubmed dataset, nodes are labeled into three classes while the minority class
    only contains $5.25\%$ of the total nodes. Such highly imbalanced data will lead
    to the suboptimal performance of downstream tasks especially classification tasks
    and one of the effective solutions is to augment the minority to alleviate the
    imbalance. To counter this problem, node insertion has been proven as an effective
    solution to augment the minority class. Meanwhile, feature mixing and graph rewiring
    are also needed for enable graph imbalanced training. For instance, GraphSMOTE [[146](#bib.bib146)]
    augments the minority class by mixing up the minority nodes and leverages an edge
    generator to predict neighbor information for those synthetic nodes. GraphMixup [[112](#bib.bib112)]
    first performs interpolation on a node from one target minority class with its
    nearest neighbors, then adopts an edge prediction module to predict the connections
    between generated nodes and existing nodes. Following this idea, GATSMOTE [[73](#bib.bib73)]
    and GNN-CL [[69](#bib.bib69)] adopts an attention mechanism to generate the edges
    between the synthetic nodes and the original nodes. On the label level, DPGNN [[106](#bib.bib106)]
    conducts pseudo labeling via label propagation to enrich the training samples
    from the minority class. However, many challenges in this topic are still under-explored.
    For example, if the amount of labeled minority nodes is extremely small, such
    as few-shot or even one-shot per class, how to transfer knowledge from the majority
    classes to augment the minority classes is worth studying.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图不平衡训练。图数据的类别分布本质上是不平衡的，遵循幂律分布。例如，在基准 Pubmed 数据集上，节点被标记为三类，而少数类仅包含总节点的 $5.25\%$。这种高度不平衡的数据将导致下游任务特别是分类任务的次优性能，其中一个有效的解决方案是增强少数类以缓解不平衡。为了解决这个问题，节点插入已被证明是增强少数类的有效解决方案。同时，特征混合和图重连也需要用于实现图不平衡训练。例如，**GraphSMOTE**[[146](#bib.bib146)]
    通过混合少数节点并利用边生成器预测这些合成节点的邻居信息来增强少数类。**GraphMixup**[[112](#bib.bib112)] 首先对来自目标少数类的一个节点与其最近邻进行插值，然后采用边预测模块来预测生成节点和现有节点之间的连接。沿着这个思路，**GATSMOTE**[[73](#bib.bib73)]
    和 **GNN-CL**[[69](#bib.bib69)] 采用了注意力机制来生成合成节点与原始节点之间的边。在标签级别，**DPGNN**[[106](#bib.bib106)]
    通过标签传播进行伪标注，以丰富来自少数类的训练样本。然而，这个主题中的许多挑战仍然没有得到深入探索。例如，如果标记的少数节点数量极少，如每类仅有少量或甚至一例，如何从多数类转移知识以增强少数类仍值得研究。
- en: 'Table 3: Summary of representative GraphDA works for reliable graph learning.
    DT, JT, BO stand for decoupled training, joint training, and bi-level optimization,
    respectively.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 代表性 GraphDA 工作的可靠图学习总结。DT、JT、BO 分别代表解耦训练、联合训练和双层优化。'
- en: '| Topic | Name | Ref. | Year | Venue | Task Level | Augmented Data Modality
    | Learnable | Augmentation Technique |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 名称 | 参考文献 | 年份 | 会议 | 任务级别 | 增强数据模态 | 可学习 | 增强技术 |'
- en: '| Structure | Feature | Label |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 结构 | 特征 | 标签 |'
- en: '| Graph Structure Learning | DGM | [[57](#bib.bib57)] | 2022 | TPAMI | node
    | ✓ | ✓ |  | ✓(JT) | feature rewriting/graph rewiring |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 图结构学习 | DGM | [[57](#bib.bib57)] | 2022 | TPAMI | 节点 | ✓ | ✓ |  | ✓(JT) |
    特征重写/图重构 |'
- en: '| GEN | [[103](#bib.bib103)] | 2021 | TheWebConf | node | ✓ |  |  | ✓(JT) |
    graph rewiring |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| GEN | [[103](#bib.bib103)] | 2021 | TheWebConf | 节点 | ✓ |  |  | ✓(JT) | 图重构
    |'
- en: '| PTDNet | [[75](#bib.bib75)] | 2021 | WSDM | node & edge | ✓ |  |  | ✓(JT)
    | graph sampling |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| PTDNet | [[75](#bib.bib75)] | 2021 | WSDM | 节点 & 边 | ✓ |  |  | ✓(JT) | 图采样
    |'
- en: '| GAUG | [[145](#bib.bib145)] | 2021 | AAAI | node | ✓ |  |  | ✓(DT) | graph
    rewiring |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| GAUG | [[145](#bib.bib145)] | 2021 | AAAI | 节点 | ✓ |  |  | ✓(DT) | 图重构 |'
- en: '| HGSL | [[143](#bib.bib143)] | 2021 | AAAI | node | ✓ |  |  | ✓(JT) | graph
    rewiring |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| HGSL | [[143](#bib.bib143)] | 2021 | AAAI | 节点 | ✓ |  |  | ✓(JT) | 图重构 |'
- en: '| IDGL | [[16](#bib.bib16)] | 2020 | NeurIPS | node & graph | ✓ |  |  | ✓(JT)
    | graph rewiring |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| IDGL | [[16](#bib.bib16)] | 2020 | NeurIPS | 节点 & 图 | ✓ |  |  | ✓(JT) | 图重构
    |'
- en: '| NeuralSparse | [[147](#bib.bib147)] | 2020 | ICML | node | ✓ |  |  | ✓(JT)
    | graph sampling |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| NeuralSparse | [[147](#bib.bib147)] | 2020 | ICML | 节点 | ✓ |  |  | ✓(JT)
    | 图采样 |'
- en: '| TO-GNN | [[122](#bib.bib122)] | 2019 | IJCAI | node | ✓ |  |  | ✓(JT) | graph
    rewiring |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| TO-GNN | [[122](#bib.bib122)] | 2019 | IJCAI | 节点 | ✓ |  |  | ✓(JT) | 图重构
    |'
- en: '| LDS | [[31](#bib.bib31)] | 2019 | ICML | node | ✓ |  |  | ✓(BO) | graph rewiring
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| LDS | [[31](#bib.bib31)] | 2019 | ICML | 节点 | ✓ |  |  | ✓(BO) | 图重构 |'
- en: '| PG-LEARN | [[115](#bib.bib115)] | 2018 | CIKM | node | ✓ |  |  | ✓(JT) |
    graph rewiring |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| PG-LEARN | [[115](#bib.bib115)] | 2018 | CIKM | 节点 | ✓ |  |  | ✓(JT) | 图重构
    |'
- en: '| Graph Feature Denoising | HGCA | [[41](#bib.bib41)] | 2022 | TNNLS | node
    |  | ✓ |  | ✓(JT) | feature addition |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 图特征去噪 | HGCA | [[41](#bib.bib41)] | 2022 | TNNLS | 节点 |  | ✓ |  | ✓(JT) |
    特征添加 |'
- en: '| AirGNN | [[71](#bib.bib71)] | 2021 | NeurIPS | node |  | ✓ |  | ✓(JT) | feature
    rewriting |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| AirGNN | [[71](#bib.bib71)] | 2021 | NeurIPS | 节点 |  | ✓ |  | ✓(JT) | 特征重写
    |'
- en: '| GCNMF | [[94](#bib.bib94)] | 2021 | FGCS | node & edge |  | ✓ |  | ✓(JT)
    | feature addition |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| GCNMF | [[94](#bib.bib94)] | 2021 | FGCS | 节点 & 边 |  | ✓ |  | ✓(JT) | 特征添加
    |'
- en: '| HGNN-AC | [[51](#bib.bib51)] | 2021 | TheWebConf | node |  | ✓ |  | ✓(JT)
    | feature addition |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| HGNN-AC | [[51](#bib.bib51)] | 2021 | TheWebConf | 节点 |  | ✓ |  | ✓(JT) |
    特征添加 |'
- en: '| SAT | [[15](#bib.bib15)] | 2020 | TPAMI | node & edge |  | ✓ |  | ✓(JT) |
    feature addition |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| SAT | [[15](#bib.bib15)] | 2020 | TPAMI | 节点 & 边 |  | ✓ |  | ✓(JT) | 特征添加
    |'
- en: '| Graph Adversarial Defense | Gasoline | [[121](#bib.bib121)] | 2022 | TheWebConf
    | node | ✓ | ✓ |  | ✓(BO) | feature rewriting/graph rewiring |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 图对抗防御 | Gasoline | [[121](#bib.bib121)] | 2022 | TheWebConf | 节点 | ✓ | ✓
    |  | ✓(BO) | 特征重写/图重构 |'
- en: '| Pro-GNN | [[53](#bib.bib53)] | 2020 | KDD | node | ✓ |  |  | ✓(JT) | graph
    rewiring |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Pro-GNN | [[53](#bib.bib53)] | 2020 | KDD | 节点 | ✓ |  |  | ✓(JT) | 图重构 |'
- en: '| GIB-N | [[114](#bib.bib114)] | 2020 | NeurIPS | node | ✓ |  |  | ✓(JT) |
    graph rewiring |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| GIB-N | [[114](#bib.bib114)] | 2020 | NeurIPS | 节点 | ✓ |  |  | ✓(JT) | 图重构
    |'
- en: '| G-SVD | [[26](#bib.bib26)] | 2020 | WSDM | node | ✓ |  |  | ✓(DT) | graph
    rewiring |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| G-SVD | [[26](#bib.bib26)] | 2020 | WSDM | 节点 | ✓ |  |  | ✓(DT) | 图重构 |'
- en: '| RoGNN | [[109](#bib.bib109)] | 2020 | WCSP | node | ✓ |  |  | ✓(JT) | graph
    rewiring |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| RoGNN | [[109](#bib.bib109)] | 2020 | WCSP | 节点 | ✓ |  |  | ✓(JT) | 图重构 |'
- en: '| GNNGuard | [[140](#bib.bib140)] | 2020 | NeurIPS | node | ✓ |  |  | ✓(JT)
    | graph rewiring |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| GNNGuard | [[140](#bib.bib140)] | 2020 | NeurIPS | 节点 | ✓ |  |  | ✓(JT) |
    图重构 |'
- en: '| Flag | [[63](#bib.bib63)] | 2020 | arXiv | node & edge & graph |  | ✓ |  |
    ✓(JT) | feature rewriting |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Flag | [[63](#bib.bib63)] | 2020 | arXiv | 节点 & 边 & 图 |  | ✓ |  | ✓(JT) |
    特征重写 |'
- en: '| G-Jaccard | [[111](#bib.bib111)] | 2019 | IJCAI | node | ✓ |  |  |  | graph
    rewiring |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| G-Jaccard | [[111](#bib.bib111)] | 2019 | IJCAI | 节点 | ✓ |  |  |  | 图重构 |'
- en: '| GraphAT | [[27](#bib.bib27)] | 2019 | TKDE | node |  | ✓ |  | ✓(JT) | feature
    rewriting |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| GraphAT | [[27](#bib.bib27)] | 2019 | TKDE | 节点 |  | ✓ |  | ✓(JT) | 特征重写
    |'
- en: '| Boosting GNN Expressiveness | LAGNN | [[70](#bib.bib70)] | 2022 | ICML |
    node & edge & graph |  | ✓ |  | ✓(JT) | feature addition |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 提升GNN表达能力 | LAGNN | [[70](#bib.bib70)] | 2022 | ICML | 节点 & 边 & 图 |  | ✓
    |  | ✓(JT) | 特征添加 |'
- en: '| rGINs | [[88](#bib.bib88)] | 2021 | SDM | graph |  | ✓ |  |  | feature addition
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| rGINs | [[88](#bib.bib88)] | 2021 | SDM | 图 |  | ✓ |  |  | 特征添加 |'
- en: '| GSN | [[7](#bib.bib7)] | 2021 | TPAMI | graph |  | ✓ |  |  | feature addition
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| GSN | [[7](#bib.bib7)] | 2021 | TPAMI | 图 |  | ✓ |  |  | 特征添加 |'
- en: '| NGNN | [[138](#bib.bib138)] | 2021 | NeurIPS | graph | ✓ |  |  |  | graph
    sampling |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| NGNN | [[138](#bib.bib138)] | 2021 | NeurIPS | 图 | ✓ |  |  |  | 图采样 |'
- en: '| ID-GNN | [[126](#bib.bib126)] | 2021 | AAAI | node & edge & graph | ✓ | ✓
    |  |  | graph sampling/feature addition |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ID-GNN | [[126](#bib.bib126)] | 2021 | AAAI | 节点 & 边 & 图 | ✓ | ✓ |  |  |
    图采样/特征添加 |'
- en: '| Distance Encoding | [[66](#bib.bib66)] | 2020 | NeurIPS | node & edge & graph
    |  | ✓ |  |  | feature addition |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 距离编码 | [[66](#bib.bib66)] | 2020 | NeurIPS | 节点 & 边 & 图 |  | ✓ |  |  | 特征添加
    |'
- en: '| Master Node | [[33](#bib.bib33)] | 2017 | ICML | graph | ✓ |  |  |  | node
    insertion |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Master Node | [[33](#bib.bib33)] | 2017 | ICML | 图 | ✓ |  |  |  | 节点插入 |'
- en: '| Alleviating Over-Smoothing/ Squashing | SDRF | [[98](#bib.bib98)] | 2022
    | ICLR | node | ✓ |  |  |  | graph rewiring |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 减轻过平滑/压缩 | SDRF | [[98](#bib.bib98)] | 2022 | ICLR | 节点 | ✓ |  |  |  | 图重连
    |'
- en: '| ADC | [[142](#bib.bib142)] | 2021 | NeurIPS | node | ✓ |  |  | ✓(BO) | graph
    diffusion |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| ADC | [[142](#bib.bib142)] | 2021 | NeurIPS | 节点 | ✓ |  |  | ✓(BO) | 图扩散
    |'
- en: '| SHADOW-GNN | [[131](#bib.bib131)] | 2021 | NeurIPS | node & edge | ✓ |  |  |  |
    graph sampling |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| SHADOW-GNN | [[131](#bib.bib131)] | 2021 | NeurIPS | 节点 & 边 | ✓ |  |  |  |
    图采样 |'
- en: '| DropEdge | [[85](#bib.bib85)] | 2020 | ICLR | node | ✓ |  |  |  | edge perturbation
    (dropping) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| DropEdge | [[85](#bib.bib85)] | 2020 | ICLR | 节点 | ✓ |  |  |  | 边缘扰动（丢弃）
    |'
- en: '| AdaEdge | [[9](#bib.bib9)] | 2020 | AAAI | node | ✓ |  |  | ✓(JT) | graph
    rewiring |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| AdaEdge | [[9](#bib.bib9)] | 2020 | AAAI | 节点 | ✓ |  |  | ✓(JT) | 图重连 |'
- en: '| GDC | [[61](#bib.bib61)] | 2019 | NeurIPS | node | ✓ |  |  |  | graph diffusion
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| GDC | [[61](#bib.bib61)] | 2019 | NeurIPS | 节点 | ✓ |  |  |  | 图扩散 |'
- en: '| Scalable GNN Training | GCOND | [[55](#bib.bib55)] | 2022 | ICLR | node |
    ✓ | ✓ | ✓ | ✓(BO) | graph generation |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展GNN训练 | GCOND | [[55](#bib.bib55)] | 2022 | ICLR | 节点 | ✓ | ✓ | ✓ | ✓(BO)
    | 图生成 |'
- en: '| DosCond | [[54](#bib.bib54)] | 2022 | KDD | node & graph | ✓ | ✓ | ✓ | ✓(BO)
    | graph generation |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| DosCond | [[54](#bib.bib54)] | 2022 | KDD | 节点 & 图 | ✓ | ✓ | ✓ | ✓(BO) |
    图生成 |'
- en: '| GOREN | [[8](#bib.bib8)] | 2021 | ICLR | node | ✓ | ✓ | ✓ | ✓(DT) | graph
    generation |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| GOREN | [[8](#bib.bib8)] | 2021 | ICLR | 节点 | ✓ | ✓ | ✓ | ✓(DT) | 图生成 |'
- en: '| SpectralGC | [[56](#bib.bib56)] | 2020 | AISTATS | graph | ✓ |  |  | ✓(DT)
    | graph generation |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| SpectralGC | [[56](#bib.bib56)] | 2020 | AISTATS | 图 | ✓ |  |  | ✓(DT) |
    图生成 |'
- en: '| SIGN | [[86](#bib.bib86)] | 2020 | arXiv | node | ✓ |  |  |  | graph diffusion
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| SIGN | [[86](#bib.bib86)] | 2020 | arXiv | 节点 | ✓ |  |  |  | 图扩散 |'
- en: '| PPRGO | [[6](#bib.bib6)] | 2020 | KDD | node | ✓ |  |  |  | graph diffusion
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| PPRGO | [[6](#bib.bib6)] | 2020 | KDD | 节点 | ✓ |  |  |  | 图扩散 |'
- en: '| GBP | [[14](#bib.bib14)] | 2020 | NeurIPS | node | ✓ |  |  |  | graph diffusion
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 英镑 | [[14](#bib.bib14)] | 2020 | NeurIPS | 节点 | ✓ |  |  |  | 图扩散 |'
- en: '| GraphSAINT | [[132](#bib.bib132)] | 2020 | ICLR | node | ✓ |  |  |  | graph
    sampling |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAINT | [[132](#bib.bib132)] | 2020 | ICLR | 节点 | ✓ |  |  |  | 图采样 |'
- en: '| LADIES | [[150](#bib.bib150)] | 2019 | NeurIPS | node | ✓ |  |  |  | graph
    sampling |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| LADIES | [[150](#bib.bib150)] | 2019 | NeurIPS | 节点 | ✓ |  |  |  | 图采样 |'
- en: '| Cluster-GCN | [[17](#bib.bib17)] | 2019 | KDD | node | ✓ |  |  | ✓(DT) |
    graph rewiring |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Cluster-GCN | [[17](#bib.bib17)] | 2019 | KDD | 节点 | ✓ |  |  | ✓(DT) | 图重连
    |'
- en: '| Fast-GCN | [[11](#bib.bib11)] | 2018 | ICLR | node | ✓ |  |  |  | graph sampling
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Fast-GCN | [[11](#bib.bib11)] | 2018 | ICLR | 节点 | ✓ |  |  |  | 图采样 |'
- en: '| GraphSAGE | [[36](#bib.bib36)] | 2017 | NIPS | node | ✓ |  |  |  | graph
    sampling |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE | [[36](#bib.bib36)] | 2017 | NIPS | 节点 | ✓ |  |  |  | 图采样 |'
- en: 5 Graph Data Augmentation for
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 图数据增强用于
- en: Reliable Graph Learning
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠的图学习
- en: 'One main goal of GraphDA is to achieve Reliable Graph Learning in the real-world
    scenarios by augmenting the input graph(s). Specifically, in this work we focus
    on improving the robustness, expressiveness, and scalability of DGL models via
    GraphDA for different challenging learning scenarios. We summarize the representative
    works in Table [3](#S4.T3 "Table 3 ‣ 4.2 Graph Semi-Supervised Learning ‣ 4 Graph
    Data Augmentation for Low-Resource Graph Learning ‣ Data Augmentation for Deep
    Graph Learning: A Survey").'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: GraphDA的一个主要目标是通过增强输入图来实现现实世界中的可靠图学习。具体而言，在这项工作中，我们重点关注通过GraphDA提高DGL模型在不同挑战性学习场景下的鲁棒性、表现力和可扩展性。我们在表[3](#S4.T3
    "表 3 ‣ 4.2 图半监督学习 ‣ 4 低资源图学习的数据增强 ‣ 深度图学习的数据增强：一项调查")中总结了代表性的工作。
- en: Graph Structure Learning. Due to various reasons such as fake connections [[42](#bib.bib42)],
    over-personalized users [[13](#bib.bib13)] and construction heuristics, the given
    graph structure is not optimal for downstream graph learning tasks. Graph structure
    learning is proposed as a solution for the above challenge. From a general sense,
    the core technique used for structure learning is graph rewiring.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图结构学习。由于虚假连接[[42](#bib.bib42)]、过度个性化用户[[13](#bib.bib13)]和构建启发式等各种原因，给定的图结构对于下游图学习任务并不是最优的。图结构学习被提出作为解决上述挑战的一种方法。从一般意义上讲，结构学习的核心技术是图重连。
- en: A series of methods rewire the given graphs following various node similarity
    metrics. In most cases, such metrics are learned from the given graph topology.
    For example, GAUG [[145](#bib.bib145)] and IDGL [[16](#bib.bib16)] train edge
    predictors based on learned node embeddings. Besides, from the optimization perspective,
    it is feasible to directly incorporate the graph data (e.g., adjacency matrix)
    itself as a part of the optimization variables. Based on that, the graph rewiring
    process is essentially guided by the optimization objective. TO-GNN [[122](#bib.bib122)]
    is a representative work whose loss functions include smoothness-related regularizations
    and the update of graphs is gradient descent-based. In addition, instead of optimizing
    the graph itself, an interesting idea is to optimize the graph-related distributions
    (e.g., graph generation distributions and edge dropping distributions). After
    that, graph rewiring can be conducted by sampling from those distributions. A
    representative work is LDS [[31](#bib.bib31)] which assumes that every edge is
    sampled from an independent Bernoulli distribution. Other representative works
    in this line include Bayesian-GCNN [[141](#bib.bib141)], GEN [[103](#bib.bib103)],
    NeuralSparse [[147](#bib.bib147)], and PTDNet [[75](#bib.bib75)].
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列方法通过各种节点相似度度量来重排给定的图。在大多数情况下，这些度量是从给定的图拓扑中学习得来的。例如，GAUG [[145](#bib.bib145)]
    和 IDGL [[16](#bib.bib16)] 基于学习到的节点嵌入训练边预测器。此外，从优化的角度来看，可以将图数据（例如，邻接矩阵）本身直接纳入优化变量的一部分。在此基础上，图重排过程本质上由优化目标指导。TO-GNN [[122](#bib.bib122)]
    是一项代表性工作，其损失函数包括平滑性相关的正则化，图的更新基于梯度下降。此外，与其优化图本身，不如优化与图相关的分布（例如，图生成分布和边丢弃分布）更有趣。之后，可以通过从这些分布中采样来进行图重排。一项代表性工作是
    LDS [[31](#bib.bib31)]，它假设每条边都从独立的伯努利分布中采样。其他此类工作的代表包括 Bayesian-GCNN [[141](#bib.bib141)]、GEN [[103](#bib.bib103)]、NeuralSparse [[147](#bib.bib147)]
    和 PTDNet [[75](#bib.bib75)]。
- en: Graph Feature Denoising. Compared to structure denoising, the research on graph
    feature denoising has received less attention. In general, most of the work is
    developed based on feature rewriting. For instance, AirGNN [[71](#bib.bib71)]
    regularizes the $l_{21}$ norm between the input node features and convoluted node
    features such that the model is more tolerant against abnormal features. To handle
    missing node features, a special case of suboptimal initial node features, feature
    propagation [[87](#bib.bib87)] diffuses the features from observed nodes to neighbors
    whose features are missing based on the heat diffusion equation; in other words,
    it imputes the missing node features with aggregated features from the neighborhood
    of the target nodes. GCNMF [[94](#bib.bib94)] explicitly formulate the missing
    node features by Gaussian mixture models whose parameters are inferred from the
    downstream tasks. An effort named SAT [[15](#bib.bib15)] reconstructs the missing
    features through the feature distribution, which is inferred from the topology
    distribution. To handle missing features on heterogeneous information networks,
    HGNN-AC [[51](#bib.bib51)] imputes missing features from neighbor nodes’ topology-based
    node embedding, while HGCA [[41](#bib.bib41)] designs a feature augmenter which
    is trained by maximizing the agreement between the augmented node embedding and
    the actual node embedding.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图特征去噪。与结构去噪相比，图特征去噪的研究受到的关注较少。一般来说，大多数工作都是基于特征重写进行的。例如，AirGNN [[71](#bib.bib71)]
    通过正则化输入节点特征和卷积节点特征之间的 $l_{21}$ 范数，使模型对异常特征更具容忍性。为了处理缺失的节点特征，一种特殊情况是次优的初始节点特征，特征传播 [[87](#bib.bib87)]
    基于热扩散方程将观察到的节点的特征扩散到特征缺失的邻居节点；换句话说，它用目标节点邻域的聚合特征来填补缺失的节点特征。GCNMF [[94](#bib.bib94)]
    明确地通过高斯混合模型来制定缺失的节点特征，其参数从下游任务中推断出来。一项名为 SAT [[15](#bib.bib15)] 的工作通过特征分布重建缺失的特征，该分布是从拓扑分布中推断出来的。为了处理异质信息网络上的缺失特征，HGNN-AC [[51](#bib.bib51)]
    从邻居节点的基于拓扑的节点嵌入中填补缺失的特征，而 HGCA [[41](#bib.bib41)] 设计了一种特征增强器，通过最大化增强节点嵌入与实际节点嵌入之间的一致性来进行训练。
- en: Graph Adversarial Defense. Aside from the noise introduced in the data collection
    phase, GNNs are fragile against adversarial attacks on graph structure and features [[91](#bib.bib91)].
    Naturally, conducting GraphDA to recover and enhance (a part of) the poisoned
    graphs is effective to alleviate the performance degradation. In this section,
    we discuss recent advances in defending graph adversarial attacks via GraphDA.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图对抗防御。除了数据收集阶段引入的噪声外，GNN对图结构和特征的对抗攻击非常脆弱[[91](#bib.bib91)]。自然地，进行GraphDA以恢复和增强（部分）中毒图是有效的，可以缓解性能下降。在这一部分，我们讨论了通过GraphDA防御图对抗攻击的最新进展。
- en: Using prior knowledge about benign graphs (e.g., feature smoothness) to guide
    graph rewiring is effective to defend graph adversarial attacks. For instance,
    works such as G-SVD [[26](#bib.bib26)] and DefenseVGAE [[134](#bib.bib134)] restore
    poisoned graphs into their reconstructed low-rank graphs which shows great empirical
    effectiveness again adversarial attacks. In addition,G-Jaccard [[111](#bib.bib111)]
    and GNNGuard [[140](#bib.bib140)] prune links whose head and tail nodes’ feature
    similarity (or embedding similarity) is lower than a predefined threshold to eliminate
    potentially malicious edges. Alternatively, the graph prior knowledge can be realized
    by setting explicit regularization terms. Pro-GNN [[53](#bib.bib53)] includes
    the topology sparsity and feature smoothness regularization terms into the optimization
    objective which can guide the graph rewiring. Besides, the supervision signals
    from downstream tasks can implicitly reflect the uncontaminated topology and feature
    distribution. For instance, Gasoline [[121](#bib.bib121)] calibrates the given
    graph based on the evaluation performance (e.g., classification loss) of the validation
    nodes. GIB-N [[114](#bib.bib114)] adopts the information bottleneck objective [[97](#bib.bib97)]
    which maximizes the mutual information between node labels and node embeddings
    and uses this objective function to guide the graph rewiring.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用关于良性图的先验知识（例如，特征平滑性）来指导图重连对于防御图对抗攻击是有效的。例如，G-SVD[[26](#bib.bib26)]和DefenseVGAE[[134](#bib.bib134)]等工作将中毒图恢复为其重构的低秩图，这在应对对抗攻击方面表现出了极好的经验效果。此外，G-Jaccard[[111](#bib.bib111)]和GNNGuard[[140](#bib.bib140)]通过修剪头尾节点特征相似性（或嵌入相似性）低于预定义阈值的链接，来消除潜在的恶意边。或者，可以通过设置显式正则化项来实现图的先验知识。Pro-GNN[[53](#bib.bib53)]将拓扑稀疏性和特征平滑性正则化项纳入优化目标中，从而指导图的重连。此外，来自下游任务的监督信号可以隐含地反映未污染的拓扑和特征分布。例如，Gasoline[[121](#bib.bib121)]基于验证节点的评估性能（例如，分类损失）来校准给定图。GIB-N[[114](#bib.bib114)]采用信息瓶颈目标[[97](#bib.bib97)]，最大化节点标签和节点嵌入之间的互信息，并使用该目标函数来指导图的重连。
- en: It is worth noting that an established defense strategy named adversarial training [[34](#bib.bib34)]
    is grafted onto the graph data. Examples include a family of graph (virtual) adversarial
    training [[63](#bib.bib63), [27](#bib.bib27), [19](#bib.bib19), [105](#bib.bib105)].
    Their core idea is to adversarially conduct edge perturbation and feature corruption
    to generate various challenging graph data samples which maximize the classification
    loss. Then those samples are included in the training of downstream GNN models
    which can improve the robustness of GNNs against adversarial attacks.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，一个名为对抗训练的已建立防御策略[[34](#bib.bib34)]被应用于图数据上。示例包括一系列图（虚拟）对抗训练[[63](#bib.bib63),
    [27](#bib.bib27), [19](#bib.bib19), [105](#bib.bib105)]。它们的核心思想是通过对抗性地进行边缘扰动和特征破坏，生成各种具有挑战性的图数据样本，从而最大化分类损失。然后，这些样本被纳入下游GNN模型的训练中，从而提高GNN对对抗攻击的鲁棒性。
- en: '![Refer to caption](img/0ce07f7251ab1795da5020ce8cb517c2.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0ce07f7251ab1795da5020ce8cb517c2.png)'
- en: 'Figure 4: 1-WL test fails to distinguish the decalin graph (left) and the bicyclopentyl
    graph (right).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 1-WL测试无法区分脱氢环己烷图（左）和双环戊烯图（右）。'
- en: 'Boosting GNN Expressive Power. Studies about the expressiveness of GNNs show
    that various mainstream GNNs cannot be more powerful than the 1-Weisfeiler-Lehman
    (WL) isomorphism test to distinguish graphs and subgraphs [[79](#bib.bib79), [120](#bib.bib120)].
    E.g., Figure [4](#S5.F4 "Figure 4 ‣ 5 Graph Data Augmentation for Reliable Graph
    Learning ‣ Data Augmentation for Deep Graph Learning: A Survey") shows a pair
    of common examples that cannot be distinguished by the 1-WL test due to the common
    rooted trees. In addition, due to the power-law distribution of node degree, many
    nodes with few neighbors are hard to be represented properly. To break such an
    inherent limitation of GNNs, augmenting the given graphs is a feasible solution.
    One line of research adopts the feature addition technique to augment the original
    node/edge features. For example, Distance Encoding [[66](#bib.bib66)] augments
    the given node features with distance measures between node pairs (or aggregated
    distance measures) to yield stronger expressiveness than 1-WL test-based GNNs.
    A recent study from Sato et al. [[88](#bib.bib88)] reveals that adding random
    features into the existing node features can boost the expressiveness of off-the-shelf
    GNNs (e.g., discriminate triangle structure). GSN [[7](#bib.bib7)] and a variant
    of ID-GNN [[126](#bib.bib126)] take a step further and augment the node features
    with the count of various motifs (e.g., cycles). Recently, LAGNN [[70](#bib.bib70)]
    adopts feature addition by a trained feature generator from which nodes with few
    neighbors can benefit greatly.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 提升GNN的表达能力。关于GNN表达能力的研究表明，各种主流GNN无法比1-威斯费勒-莱曼（WL）同构测试更强大，用于区分图和子图[[79](#bib.bib79)、[120](#bib.bib120)]。例如，图[4](#S5.F4
    "图 4 ‣ 5 图数据增强用于可靠的图学习 ‣ 深度图学习的数据增强：综述")展示了一对由于共同的根树而无法通过1-WL测试区分的常见示例。此外，由于节点度的幂律分布，许多邻居很少的节点难以被正确表示。为了打破这种GNN的固有限制，增强给定图是一种可行的解决方案。一项研究采用了特征增加技术来增强原始的节点/边特征。例如，距离编码[[66](#bib.bib66)]通过节点对之间的距离度量（或聚合距离度量）增强给定的节点特征，表现出比基于1-WL测试的GNN更强的表达能力。最近Sato等人的研究[[88](#bib.bib88)]揭示，将随机特征添加到现有的节点特征中可以提升现成GNN的表达能力（例如，区分三角形结构）。GSN[[7](#bib.bib7)]和ID-GNN的一个变体[[126](#bib.bib126)]进一步通过各种图案（例如循环）的计数来增强节点特征。最近，LAGNN[[70](#bib.bib70)]通过训练的特征生成器进行特征增加，从中节点邻居较少的节点可以大大受益。
- en: Another line of work argues that the limitation of the 1-WL test (and 1-WL test-based
    GNNs) is related to the structure of rooted trees. Based on that, graph sampling
    technique can enhance the GNN expressiveness. Specifically, NGNN [[138](#bib.bib138)]
    claims that node representations from sampled rooted subgraphs are more expressive
    than those from rooted trees. Interestingly, ID-GNN [[126](#bib.bib126)] shares
    a similar insight with NGNN [[138](#bib.bib138)] and also samples an ego network
    for every node for computing the node embedding.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作认为，1-WL测试（和基于1-WL测试的GNN）的限制与根树的结构有关。基于此，图采样技术可以增强GNN的表达能力。具体而言，NGNN[[138](#bib.bib138)]声称，从采样的根子图中的节点表示比从根树中获得的表示更具表达力。有趣的是，ID-GNN[[126](#bib.bib126)]与NGNN[[138](#bib.bib138)]有类似的见解，也为每个节点采样一个自我网络以计算节点嵌入。
- en: As the node representations are obtained through the aggregation of node features
    within the receptive fields, to enhance the propagation of messages for long distance,
    a straightforward way is to apply node insertion by inserting virtual nodes (i.e.,
    super nodes or master nodes) [[33](#bib.bib33), [65](#bib.bib65), [48](#bib.bib48),
    [83](#bib.bib83)] into the graphs which are connected with all the existing nodes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 由于节点表示是通过感受野内节点特征的聚合获得的，为了增强长距离消息传播的效果，一种简单的方法是通过在图中插入虚拟节点（即超级节点或主节点）[[33](#bib.bib33)、[65](#bib.bib65)、[48](#bib.bib48)、[83](#bib.bib83)]来实现，这些虚拟节点与所有现有节点相连接。
- en: Alleviating Over-Smoothing/Squashing of GNNs. Due to the inherent design limit
    of GNNs, as the depth of the model increases, the representations of different
    nodes in the graph eventually become indistinguishable with iterative message
    passing. The so-called over-smoothing phenomenon generally leads to failure in
    the graph learning tasks [[67](#bib.bib67)]. To counter the over-smoothing issue,
    GraphDA, especially graph rewiring methods have been shown as an effective solution.
    For example, DropEdge [[85](#bib.bib85)] randomly removes graph edges during message
    passing to alleviate over-smoothing. TADropEdge [[32](#bib.bib32)] exploits graph
    structural information to compute edge weights for edge dropping, such that the
    augmented subgraphs can avoid the arbitrary data augmentation issue in DropEdge.
    AdaEdge [[9](#bib.bib9)] iteratively adds or removes edges to the graph topology
    based on the classification results (from a GNNs-based classifier) and trains
    GNN classifiers on the updated graphs to overcome the over-smoothing issue. SHADOW-GNN [[131](#bib.bib131)]
    samples informative subgraphs centered on each node and then builds a deep GNN
    operating on subgraphs instead of the whole graph to decouple the depth and scope
    of GNNs.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 减轻GNNs的过度平滑/挤压。由于GNNs固有的设计限制，随着模型深度的增加，图中不同节点的表示最终在迭代消息传递中变得不可区分。所谓的过度平滑现象通常会导致图学习任务的失败 [[67](#bib.bib67)]。为了应对过度平滑问题，GraphDA，尤其是图重连方法被证明是一种有效的解决方案。例如，DropEdge [[85](#bib.bib85)]在消息传递过程中随机删除图边，以减轻过度平滑。TADropEdge [[32](#bib.bib32)]利用图结构信息计算边权重进行边删除，从而避免了DropEdge中的任意数据增强问题。AdaEdge [[9](#bib.bib9)]根据分类结果（来自基于GNN的分类器）迭代地添加或删除图拓扑中的边，并在更新后的图上训练GNN分类器以克服过度平滑问题。SHADOW-GNN [[131](#bib.bib131)]以每个节点为中心抽样信息子图，然后构建一个在子图上操作的深度GNN，而不是在整个图上，以解耦GNN的深度和范围。
- en: Note that over-smoothing is mostly demonstrated in tasks that depend mostly
    on short-range information, while GNNs are also ineffective for capturing long-range
    node interactions. A recent study [[1](#bib.bib1)] points out that the distortion
    of information flowing from distant nodes (i.e., over-squashing) is the main factor
    limiting the efficiency of message passing for tasks relying on long-distance
    interactions. As the layer depth increases, the number of nodes in each node’s
    receptive field grows exponentially and leads to the over-squashing issue. To
    overcome over-squashing, graph diffusion and graph rewiring are two effective
    choices. Graph Diffusion Convolution (GDC) [[61](#bib.bib61)] constructs a new
    graph based on a generalized form of graph diffusion, which can be further used
    to enlarge a larger neighborhood for message-passing. Adaptive Diffusion Convolution
    (ADC) [[142](#bib.bib142)] supports learning the optimal neighborhood from the
    data automatically to eliminate the manual search process of the optimal propagation
    neighborhood in GDC. Alon & Yahav [[1](#bib.bib1)] propose to rewire the graph
    to build a fully-adjacent GNN layer for reducing the bottleneck. Similarly, Stochastic
    Discrete Ricci Flow (SDRF) [[98](#bib.bib98)] is a curvature-based method for
    graph rewiring to mitigate the over-squashing issue.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到过度平滑主要在依赖短期信息的任务中表现出来，而GNNs在捕捉长期节点交互方面也不有效。最近的一项研究 [[1](#bib.bib1)]指出，来自远端节点的信息扭曲（即过度挤压）是限制基于长距离交互任务的消息传递效率的主要因素。随着层深度的增加，每个节点的感受野中的节点数量呈指数增长，从而导致过度挤压问题。为了解决过度挤压，图扩散和图重连是两个有效的选择。图扩散卷积（GDC） [[61](#bib.bib61)]基于图扩散的广义形式构建一个新图，进一步用于扩大消息传递的邻域。自适应扩散卷积（ADC） [[142](#bib.bib142)]支持从数据中自动学习最佳邻域，以消除GDC中手动搜索最佳传播邻域的过程。Alon
    & Yahav [[1](#bib.bib1)]建议重连图以建立一个完全相邻的GNN层，从而减少瓶颈。同样，随机离散Ricci流（SDRF） [[98](#bib.bib98)]是一种基于曲率的图重连方法，用于缓解过度挤压问题。
- en: Scalable Graph Training. Scalability is always a crucial challenge for GNNs
    which hinders the applicability of a broad class of GNN models over large real-world
    graphs. GraphDA techniques, such as graph sampling, graph diffusion, and graph
    generation play an important role to speed up the training and inferring of GNNs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展图训练。可扩展性始终是GNNs面临的关键挑战，这阻碍了广泛GNN模型在大型现实世界图上的应用。GraphDA技术，如图采样、图扩散和图生成，在加速GNNs的训练和推理方面发挥着重要作用。
- en: Based on the idea of graph sampling, GraphSAGE [[36](#bib.bib36)] uniformly
    samples the neighbors of the target nodes to enable GNN inductive learning on
    large graphs. FastGCN [[11](#bib.bib11)] shares a similar idea with GraphSAGE
    but follows the idea of importance sampling to sample vertices in every layer.
    To overcome the redundant sampling and sparse connection problem brought by the
    GraphSAGE [[36](#bib.bib36)] and FastGCN [[11](#bib.bib11)] respectively, Zou
    et al. [[150](#bib.bib150)] propose LADIES which is a layer-dependent sampling
    strategy. In addition, GraphSAINT [[132](#bib.bib132)] is another sampling-based
    method that applies various sampling methods (e.g., based on random walk) to improve
    the scalability of training rather than modifying the vanilla GCN model [[59](#bib.bib59)].
    Similar to sampling methods, graph partition is also effective to lower the memory
    requirement. For example, Cluster-GCN [[17](#bib.bib17)] partitions the given
    graph into clusters and conducts convolution operation within every cluster to
    avoid heavy neighborhood search and sampling.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图采样的思想，GraphSAGE [[36](#bib.bib36)] 均匀地采样目标节点的邻居，从而使 GNN 在大图上进行归纳学习。FastGCN [[11](#bib.bib11)]
    具有与 GraphSAGE 类似的思想，但采用重要性采样的方法在每一层采样顶点。为了克服 GraphSAGE [[36](#bib.bib36)] 和 FastGCN [[11](#bib.bib11)]
    各自带来的冗余采样和稀疏连接问题，Zou 等人 [[150](#bib.bib150)] 提出了 LADIES，这是一种层依赖的采样策略。此外，GraphSAINT [[132](#bib.bib132)]
    是另一种基于采样的方法，它应用了各种采样方法（例如，基于随机游走）来提高训练的可扩展性，而不是修改原始的 GCN 模型 [[59](#bib.bib59)]。类似于采样方法，图划分也有效降低内存需求。例如，Cluster-GCN [[17](#bib.bib17)]
    将给定图划分为多个簇，并在每个簇内进行卷积操作，以避免繁重的邻域搜索和采样。
- en: Another line of GraphDA research on scaling the training of GNNs adopts a decoupled
    design that trains MLP on the augmented graphs via graph diffusion  [[12](#bib.bib12),
    [110](#bib.bib110), [60](#bib.bib60), [86](#bib.bib86)]. Given the above model
    design, the graph diffusion matrix can be pre-computed, which makes the training
    of MLP more efficient. A set of representative works adopt PageRank [[80](#bib.bib80)]
    and its variant (e.g., personalized PageRank [[49](#bib.bib49)]) to infer the
    diffusion matrix such as SGC [[110](#bib.bib110)] and APPNP [[60](#bib.bib60)].
    Similarly, PPRGo [[6](#bib.bib6)] adopts the Forward Push algorithm [[2](#bib.bib2)]
    to approximate the personalized PageRank matrix for efficiency. GBP [[14](#bib.bib14)]
    revisits the core idea of PPRGo and further speeds up the computation of the generalized
    PageRank matrix by the Bidirectional Propagation algorithm. In addition, a very
    recent work named GRAND+ [[29](#bib.bib29)] also approximates the graph diffusion
    matrix to scale the training of graph random neural networks [[30](#bib.bib30)]
    in the consistency training context.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: GraphDA 在扩展 GNN 训练方面的另一条研究路线采用了解耦设计，通过图扩散 [[12](#bib.bib12), [110](#bib.bib110),
    [60](#bib.bib60), [86](#bib.bib86)] 在扩展图上训练 MLP。根据上述模型设计，图扩散矩阵可以预计算，这使得 MLP 的训练更加高效。一组代表性工作采用
    PageRank [[80](#bib.bib80)] 及其变体（例如，个性化 PageRank [[49](#bib.bib49)]）来推断扩散矩阵，如
    SGC [[110](#bib.bib110)] 和 APPNP [[60](#bib.bib60)]。类似地，PPRGo [[6](#bib.bib6)]
    采用 Forward Push 算法 [[2](#bib.bib2)] 来近似个性化 PageRank 矩阵，以提高效率。GBP [[14](#bib.bib14)]
    重新审视了 PPRGo 的核心思想，并通过双向传播算法进一步加速广义 PageRank 矩阵的计算。此外，最近有一项名为 GRAND+ [[29](#bib.bib29)]
    的工作也在一致性训练背景下近似图扩散矩阵，以扩展图随机神经网络 [[30](#bib.bib30)] 的训练。
- en: In addition, works on “graph condensation” and “graph coarsening” try to shrink
    the given graph $\mathcal{G}$ by graph generation so that the generated graph
    $\tilde{\mathcal{G}}$ can be handled by GNNs [[46](#bib.bib46)]. Their core idea
    is to minimize a ‘quantity of interest’ between the input graph $\mathcal{G}$
    and augmented graph $\tilde{\mathcal{G}}$. For example, Jin et al. [[56](#bib.bib56)]
    propose to minimize the spectral distance between $\mathcal{G}$ and $\tilde{\mathcal{G}}$.
    GOREN [[8](#bib.bib8)] aims to keep Laplace operators from $\mathcal{G}$ and $\tilde{\mathcal{G}}$
    comparable. GCOND [[55](#bib.bib55)] and DosCond [[54](#bib.bib54)] minimize the
    difference of training gradients on $\mathcal{G}$ and $\tilde{\mathcal{G}}$.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，关于“图浓缩”和“图粗化”的研究尝试通过图生成来缩小给定图 $\mathcal{G}$，使得生成的图 $\tilde{\mathcal{G}}$
    可以被 GNNs [[46](#bib.bib46)] 处理。他们的核心思想是最小化输入图 $\mathcal{G}$ 和扩展图 $\tilde{\mathcal{G}}$
    之间的“感兴趣量”。例如，Jin 等人 [[56](#bib.bib56)] 提出了最小化 $\mathcal{G}$ 和 $\tilde{\mathcal{G}}$
    之间的谱距离。GOREN [[8](#bib.bib8)] 旨在使 $\mathcal{G}$ 和 $\tilde{\mathcal{G}}$ 的拉普拉斯算子保持可比性。GCOND [[55](#bib.bib55)]
    和 DosCond [[54](#bib.bib54)] 最小化 $\mathcal{G}$ 和 $\tilde{\mathcal{G}}$ 上训练梯度的差异。
- en: 6 Future Directions
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来方向
- en: GraphDA is an emerging and fast-developing field. Although substantial progress
    has been achieved, many challenges remain under-explored. In this section, we
    discuss some promising research directions as follows.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: GraphDA 是一个新兴且快速发展的领域。尽管已经取得了 substantial 进展，但仍然存在许多未被充分探讨的挑战。在本节中，我们讨论了一些有前景的研究方向。
- en: Data Augmentation beyond Simple Graphs. Most of the aforementioned works develop
    augmentation strategies on homophilic (i.e., assortative) graphs where edges tend
    to connect nodes with the same properties (e.g., labels, features). However, heterophily
    (i.e., disassortativity) also exists commonly in networks such as heterosexual
    dating networks. Many existing augmentation approaches [[92](#bib.bib92)] on heterophilic
    graphs focus on improving the assortativity of the given graphs or dropping/deweighting
    the existing disassortative edges [[125](#bib.bib125)]. The augmentation of node
    features, labels, and non-existing edges of heterophilic graphs remains understudied.
    Besides, existing GraphDA efforts are mainly developed for either plain or attributed
    graphs, while principled augmentation approaches for other types of graphs (e.g.,
    heterogeneous graphs, hypergraphs, multiplex graphs, dynamic graphs) remain largely
    unexplored. Those complex graphs provide broader design space for augmentation
    but also challenge the effectiveness of existing GraphDA methods greatly, which
    is vital to explore in the future.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 超越简单图的 数据增强。上述大多数研究在同质性（即，按属性分组的）图上开发了增强策略，其中边缘往往连接具有相同属性（例如标签、特征）的节点。然而，异质性（即，非同质性）在网络中也很常见，如异性恋交友网络。许多现有的增强方法[[92](#bib.bib92)]
    针对异质性图关注于改善给定图的同质性或删除/减轻现有的异质性边缘[[125](#bib.bib125)]。对异质性图的节点特征、标签和不存在的边的增强仍然没有得到充分研究。此外，现有的
    GraphDA 努力主要针对简单图或属性图，而针对其他类型图（例如异构图、超图、多层图、动态图）的原则性增强方法仍然大多未被探索。这些复杂图提供了更广泛的增强设计空间，但也极大地挑战了现有的
    GraphDA 方法，这是未来需要探索的重要领域。
- en: Automated and Generalizable Graph Data Augmentation. In general, the effectiveness
    of DGL problems hinges on adhoc graph data augmentations, which have to be manually
    picked per dataset, by either rules of thumb or trial-and-errors. For example,
    researchers observe that different data augmentations affect downstream tasks
    differently across datasets, which suggests that searching over augmentation functions
    is crucial for graph self-supervised learning [[127](#bib.bib127)]. Nevertheless,
    evaluating representations derived from multiple augmentation functions without
    direct access to ground truth labels makes this problem challenging. Hence, it
    is necessary to develop automated data augmentation solutions to adaptively customize
    augmentation strategies for each graph dataset [[76](#bib.bib76)]. Meanwhile,
    considering that different graphs usually have distinct graph properties, developing
    generalizable data augmentation methods without learning from scratch for each
    domain is also a promising direction to improve the practical usage of GraphDA
    methods.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化和通用化的图数据增强。一般来说，DGL 问题的有效性依赖于临时的图数据增强，这些增强方法必须根据数据集手动选择，可以通过经验法则或试错法进行。例如，研究人员观察到不同的数据增强在不同数据集上对下游任务的影响不同，这表明搜索增强函数对于图的自监督学习至关重要[[127](#bib.bib127)]。然而，在没有直接访问真实标签的情况下评估从多个增强函数派生出的表示，使得这个问题变得具有挑战性。因此，有必要开发自动化的数据增强解决方案，以便为每个图数据集自适应地定制增强策略[[76](#bib.bib76)]。同时，考虑到不同的图通常具有不同的图属性，开发通用的数据增强方法，而不是为每个领域从头学习，也是改善
    GraphDA 方法实际应用的一个有前途的方向。
- en: Semantic-Preserving Graph Data Augmentation. Designing effective data augmentation
    for graphs is challenging due to their non-Euclidean nature and the dependencies
    between data samples. Most graph data augmentation methods adopt arbitrary augmentations
    on the input graph, which may unexpectedly change both structural and semantic
    patterns of the graph [[81](#bib.bib81)]. For example, dropping a carbon atom
    from the phenyl ring of aspirin breaks the aromatic system and results in a alkene
    chain, which is an entirely different chemical compound. Hence, proposing a label-consistent/semantic-preserving
    GraphDA method is of necessity. To this end, recent studies [[64](#bib.bib64),
    [117](#bib.bib117), [130](#bib.bib130)] perform data augmentation on the latent
    space to avoid the perturbation on the semantics.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 语义保持的图数据增强。由于图的非欧几里得特性和数据样本之间的依赖关系，设计有效的图数据增强是具有挑战性的。大多数图数据增强方法在输入图上采用任意增强，这可能会意外改变图的结构和语义模式[[81](#bib.bib81)]。例如，从阿司匹林的苯环中去掉一个碳原子会破坏芳香系统，导致形成一个烯烃链，这是一个完全不同的化学化合物。因此，提出一个标签一致/语义保持的GraphDA方法是必要的。为此，最近的研究[[64](#bib.bib64),
    [117](#bib.bib117), [130](#bib.bib130)] 在潜在空间上执行数据增强，以避免对语义的扰动。
- en: Graph Data Augmentation for Trustworthy DGL. Despite the success of DGL, how
    to ensure various DGL algorithms behave in a socially responsible manner and meet
    regulatory compliance requirements becomes an emerging problem, especially in
    risk-sensitive applications. In fact, GraphDA can be an effective tool to achieve
    Trustworthy GML, especially on fairness, causality, explainability of DGL algorithms.
    For example, counterfactural graph data augmentation [[74](#bib.bib74), [144](#bib.bib144)]
    has been used to explain the behavior of GNNs in the literature. Moreover, data
    augmentation itself is typically performed in an adhoc manner with little understanding
    of the underlying theoretical principles. Existing work on GraphDA is mainly surface-level,
    and rarely investigates the theoretical underpinnings and principles. Overall,
    there indeed appears to be a lack of research on interpret why exactly GraphDA
    works.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 可信赖DGL的图数据增强。尽管DGL取得了成功，但如何确保各种DGL算法在社会责任方面表现良好并符合监管合规要求，特别是在风险敏感的应用中，成为了一个新兴问题。实际上，GraphDA可以成为实现可信赖GML的有效工具，特别是在DGL算法的公平性、因果关系和可解释性方面。例如，文献中已经使用了反事实图数据增强[[74](#bib.bib74),
    [144](#bib.bib144)] 来解释GNN的行为。此外，数据增强本身通常以临时的方式进行，对其基础理论原则的理解甚少。现有的GraphDA研究主要是表面层面的，很少探讨其理论基础和原则。总体而言，确实存在对GraphDA具体如何有效的解释研究不足的情况。
- en: 7 Conclusion
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we present a forward-looking and structured survey of graph data
    augmentation (GraphDA). To inspect the nature of GraphDA, we give a formal formulation
    and a taxonomy to facilitate the understanding of this emerging research problem.
    Specifically, we frame GraphDA methods into three categories according to the
    target augmentation modalities, i.e., feature-wise, structure-wise, and label-wise
    augmentations. We further review the application of GraphDA methods to address
    two data-centric DGL problems (i.e., low-resource graph learning and reliable
    graph learning) and discuss the prevailing GraphDA-based algorithms. Finally,
    we outline current challenges as well as opportunities for future research in
    this field.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们呈现了一个前瞻性且结构化的图数据增强（GraphDA）综述。为了检视GraphDA的本质，我们给出了正式的定义和分类法，以便于理解这一新兴研究问题。具体来说，我们将GraphDA方法框定为三类，依据目标增强方式，即特征级、结构级和标签级增强。我们进一步回顾了GraphDA方法在解决两个以数据为中心的DGL问题（即低资源图学习和可靠图学习）中的应用，并讨论了流行的基于GraphDA的算法。最后，我们概述了当前面临的挑战以及该领域未来研究的机会。
- en: Acknowledgments
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by NSF (No. 1947135, 2106825, 2229461, and 2134079),
    the NSF Program on Fairness in AI in collaboration with Amazon (No. 1939725),
    DARPA (No. HR001121C0165), ARO (No. W911NF2110088), ONR (No. N00014-21-1-4002),
    NIFA (No. 2020-67021-32799, and W911NF2110030), and ARL (No. W911NF2020124).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了NSF（编号1947135、2106825、2229461和2134079）、与Amazon合作的NSF公平性AI项目（编号1939725）、DARPA（编号HR001121C0165）、ARO（编号W911NF2110088）、ONR（编号N00014-21-1-4002）、NIFA（编号2020-67021-32799和W911NF2110030）以及ARL（编号W911NF2020124）的资助支持。
- en: References
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] U. Alon and E. Yahav. On the bottleneck of graph neural networks and its
    practical implications. In ICLR, 2021.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] U. Alon 和 E. Yahav. 关于图神经网络的瓶颈及其实际影响。发表于ICLR，2021年。'
- en: '[2] R. Andersen, F. Chung, and K. Lang. Local graph partitioning using pagerank
    vectors. In FOCS, 2006.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] R. Andersen, F. Chung, 和 K. Lang. 使用 PageRank 向量的局部图划分。见 FOCS，2006。'
- en: '[3] C. M. Bishop. Neural networks and their applications. Review of scientific
    instruments, 1994.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. M. Bishop. 神经网络及其应用。科学仪器评论，1994。'
- en: '[4] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training.
    In COLT, 1998.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Blum 和 T. Mitchell. 结合标记数据和未标记数据的共同训练。见 COLT，1998。'
- en: '[5] D. Bo, B. Hu, X. Wang, Z. Zhang, C. Shi, and J. Zhou. Regularizing graph
    neural networks via consistency-diversity graph augmentations. In AAAI, 2022.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. Bo, B. Hu, X. Wang, Z. Zhang, C. Shi, 和 J. Zhou. 通过一致性-多样性图增强来正则化图神经网络。见
    AAAI，2022。'
- en: '[6] A. Bojchevski, J. Klicpera, B. Perozzi, A. Kapoor, M. Blais, B. Rózemberczki,
    M. Lukasik, and S. Günnemann. Scaling graph neural networks with approximate pagerank.
    In KDD, 2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Bojchevski, J. Klicpera, B. Perozzi, A. Kapoor, M. Blais, B. Rózemberczki,
    M. Lukasik, 和 S. Günnemann. 使用近似 PageRank 扩展图神经网络。见 KDD，2020。'
- en: '[7] G. Bouritsas, F. Frasca, S. P. Zafeiriou, and M. Bronstein. Improving graph
    neural network expressivity via subgraph isomorphism counting. TPAMI, 2022.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Bouritsas, F. Frasca, S. P. Zafeiriou, 和 M. Bronstein. 通过子图同构计数提升图神经网络的表达能力。见
    TPAMI，2022。'
- en: '[8] C. Cai, D. Wang, and Y. Wang. Graph coarsening with neural networks. In
    ICLR, 2021.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. Cai, D. Wang, 和 Y. Wang. 使用神经网络的图粗化。见 ICLR，2021。'
- en: '[9] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, and X. Sun. Measuring and relieving
    the over-smoothing problem for graph neural networks from the topological view.
    In AAAI, 2020.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, 和 X. Sun. 从拓扑视角衡量和缓解图神经网络的过平滑问题。见
    AAAI，2020。'
- en: '[10] H. Chen, S. Zhang, and G. Xu. Graph masked autoencoder. arXiv preprint
    arXiv:2202.08391, 2022.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] H. Chen, S. Zhang, 和 G. Xu. 图掩码自编码器。arXiv 预印本 arXiv:2202.08391，2022。'
- en: '[11] J. Chen, T. Ma, and C. Xiao. Fastgcn: Fast learning with graph convolutional
    networks via importance sampling. In ICLR, 2018.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Chen, T. Ma, 和 C. Xiao. FastGCN：通过重要性采样加速图卷积网络学习。见 ICLR，2018。'
- en: '[12] L. Chen, Z. Chen, and J. Bruna. On graph neural networks versus graph-augmented
    mlps. In ICLR, 2020.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L. Chen, Z. Chen, 和 J. Bruna. 关于图神经网络与图增强 MLP 的比较。见 ICLR，2020。'
- en: '[13] L. Chen, Y. Yao, F. Xu, M. Xu, and H. Tong. Trading personalization for
    accuracy: Data debugging in collaborative filtering. NeurIPS, 2020.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] L. Chen, Y. Yao, F. Xu, M. Xu, 和 H. Tong. 交换个性化以换取准确性：协同过滤中的数据调试。见 NeurIPS，2020。'
- en: '[14] M. Chen, Z. Wei, B. Ding, Y. Li, Y. Yuan, X. Du, and J.-R. Wen. Scalable
    graph neural networks via bidirectional propagation. NeurIPS, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Chen, Z. Wei, B. Ding, Y. Li, Y. Yuan, X. Du, 和 J.-R. Wen. 通过双向传播扩展图神经网络。见
    NeurIPS，2020。'
- en: '[15] X. Chen, S. Chen, J. Yao, H. Zheng, Y. Zhang, and I. W. Tsang. Learning
    on attribute-missing graphs. TPAMI, 2020.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] X. Chen, S. Chen, J. Yao, H. Zheng, Y. Zhang, 和 I. W. Tsang. 在属性缺失图上的学习。见
    TPAMI，2020。'
- en: '[16] Y. Chen, L. Wu, and M. Zaki. Iterative deep graph learning for graph neural
    networks: Better and robust node embeddings. NeurIPS, 2020.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. Chen, L. Wu, 和 M. Zaki. 用于图神经网络的迭代深度图学习：更好且鲁棒的节点嵌入。见 NeurIPS，2020。'
- en: '[17] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn:
    An efficient algorithm for training deep and large graph convolutional networks.
    In KDD, 2019.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, 和 C.-J. Hsieh. Cluster-GCN：用于训练深度和大规模图卷积网络的高效算法。见
    KDD，2019。'
- en: '[18] E. Dai, C. Aggarwal, and S. Wang. Nrgnn: Learning a label noise-resistant
    graph neural network on sparsely and noisily labeled graphs. In KDD, 2021.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] E. Dai, C. Aggarwal, 和 S. Wang. NR-GNN：在稀疏和噪声标签图上的抗标签噪声图神经网络学习。见 KDD，2021。'
- en: '[19] Z. Deng, Y. Dong, and J. Zhu. Batch virtual adversarial training for graph
    convolutional networks. arXiv preprint arXiv:1902.09192, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Z. Deng, Y. Dong, 和 J. Zhu. 用于图卷积网络的批量虚拟对抗训练。arXiv 预印本 arXiv:1902.09192，2019。'
- en: '[20] K. Ding, J. Li, R. Bhanushali, and H. Liu. Deep anomaly detection on attributed
    networks. In SDM, 2019.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Ding, J. Li, R. Bhanushali, 和 H. Liu. 在属性网络上的深度异常检测。见 SDM，2019。'
- en: '[21] K. Ding, J. Wang, J. Caverlee, and H. Liu. Meta propagation networks for
    graph few-shot semi-supervised learning. In AAAI, 2022.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] K. Ding, J. Wang, J. Caverlee, 和 H. Liu. 用于图少样本半监督学习的元传播网络。见 AAAI，2022。'
- en: '[22] K. Ding, J. Wang, J. Li, D. Li, and H. Liu. Be more with less: Hypergraph
    attention networks for inductive text classification. In EMNLP, 2020.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] K. Ding, J. Wang, J. Li, D. Li, 和 H. Liu. 用更少的资源获得更多：用于归纳文本分类的超图注意力网络。见
    EMNLP，2020。'
- en: '[23] K. Ding, J. Wang, J. Li, K. Shu, C. Liu, and H. Liu. Graph prototypical
    networks for few-shot learning on attributed networks. In CIKM, 2020.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] K. Ding, J. Wang, J. Li, K. Shu, C. Liu, 和 H. Liu. 用于少样本学习的图原型网络。见 CIKM，2020。'
- en: '[24] K. Ding, Y. Wang, Y. Yang, and H. Liu. Eliciting structural and semantic
    global knowledge in unsupervised graph contrastive learning. In AAAI, 2023.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] K. Ding, Y. Wang, Y. Yang, 和 H. Liu. 在无监督图对比学习中引导结构和语义的全球知识。在 AAAI, 2023。'
- en: '[25] H. Dong, J. Chen, F. Feng, X. He, S. Bi, Z. Ding, and P. Cui. On the equivalence
    of decoupled graph convolution network and label propagation. In TheWebConf, 2021.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] H. Dong, J. Chen, F. Feng, X. He, S. Bi, Z. Ding, 和 P. Cui. 关于解耦图卷积网络和标签传播的等价性。在
    TheWebConf, 2021。'
- en: '[26] N. Entezari, S. A. Al-Sayouri, A. Darvishzadeh, and E. E. Papalexakis.
    All you need is low (rank) defending against adversarial attacks on graphs. In
    WSDM, 2020.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] N. Entezari, S. A. Al-Sayouri, A. Darvishzadeh, 和 E. E. Papalexakis. 你需要的只是低（秩）来防御图上的对抗攻击。在
    WSDM, 2020。'
- en: '[27] F. Feng, X. He, J. Tang, and T.-S. Chua. Graph adversarial training: Dynamically
    regularizing based on graph structure. TKDE, 2019.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] F. Feng, X. He, J. Tang, 和 T.-S. Chua. 图对抗训练: 基于图结构动态正则化。TKDE, 2019。'
- en: '[28] S. Feng, B. Jing, Y. Zhu, and H. Tong. Adversarial graph contrastive learning
    with information regularization. In TheWebConf, 2022.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Feng, B. Jing, Y. Zhu, 和 H. Tong. 带有信息正则化的对抗性图对比学习。在 TheWebConf, 2022。'
- en: '[29] W. Feng, Y. Dong, T. Huang, Z. Yin, X. Cheng, E. Kharlamov, and J. Tang.
    Grand+: Scalable graph random neural networks. In TheWebConf, 2022.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] W. Feng, Y. Dong, T. Huang, Z. Yin, X. Cheng, E. Kharlamov, 和 J. Tang.
    Grand+: 可扩展图随机神经网络。在 TheWebConf, 2022。'
- en: '[30] W. Feng, J. Zhang, Y. Dong, Y. Han, H. Luan, Q. Xu, Q. Yang, E. Kharlamov,
    and J. Tang. Graph random neural networks for semi-supervised learning on graphs.
    In NeurIPS, 2020.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] W. Feng, J. Zhang, Y. Dong, Y. Han, H. Luan, Q. Xu, Q. Yang, E. Kharlamov,
    和 J. Tang. 图随机神经网络用于图上的半监督学习。在 NeurIPS, 2020。'
- en: '[31] L. Franceschi, M. Niepert, M. Pontil, and X. He. Learning discrete structures
    for graph neural networks. In ICML, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] L. Franceschi, M. Niepert, M. Pontil, 和 X. He. 为图神经网络学习离散结构。在 ICML, 2019。'
- en: '[32] Z. Gao, S. Bhattacharya, L. Zhang, R. S. Blum, A. Ribeiro, and B. M. Sadler.
    Training robust graph neural networks with topology adaptive edge dropping. arXiv
    preprint arXiv:2106.02892, 2021.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Z. Gao, S. Bhattacharya, L. Zhang, R. S. Blum, A. Ribeiro, 和 B. M. Sadler.
    通过拓扑自适应边丢弃训练鲁棒的图神经网络。arXiv 预印本 arXiv:2106.02892, 2021。'
- en: '[33] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl.
    Neural message passing for quantum chemistry. In ICML, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, 和 G. E. Dahl. 用于量子化学的神经消息传递。在
    ICML, 2017。'
- en: '[34] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing
    adversarial examples. In ICLR, 2015.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] I. J. Goodfellow, J. Shlens, 和 C. Szegedy. 解释和利用对抗性示例。在 ICLR, 2015。'
- en: '[35] H. Guo and Y. Mao. ifmixup: Towards intrusion-free graph mixup for graph
    classification. arXiv preprint arXiv:2110.09344, 2021.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] H. Guo 和 Y. Mao. ifmixup: 朝着无侵入图 mixup 的图分类方向发展。arXiv 预印本 arXiv:2110.09344,
    2021。'
- en: '[36] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning
    on large graphs. arXiv preprint arXiv:1706.02216, 2017.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] W. L. Hamilton, R. Ying, 和 J. Leskovec. 大图上的归纳表示学习。arXiv 预印本 arXiv:1706.02216,
    2017。'
- en: '[37] X. Han, Z. Jiang, N. Liu, and X. Hu. G-mixup: Graph data augmentation
    for graph classification. In ICML, 2022.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] X. Han, Z. Jiang, N. Liu, 和 X. Hu. G-mixup: 图数据增强用于图分类。在 ICML, 2022。'
- en: '[38] K. Hassani and A. H. Khasahmadi. Contrastive multi-view representation
    learning on graphs. In ICML, 2020.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] K. Hassani 和 A. H. Khasahmadi. 图上的对比多视角表示学习。在 ICML, 2020。'
- en: '[39] K. Hassani and A. H. Khasahmadi. Learning graph augmentations to learn
    graph representations. arXiv preprint arXiv:2201.09830, 2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. Hassani 和 A. H. Khasahmadi. 学习图增强以学习图表示。arXiv 预印本 arXiv:2201.09830,
    2022。'
- en: '[40] C. Hawkins, V. N. Ioannidis, S. Adeshina, and G. Karypis. Scalable consistency
    training for graph neural networks via self-ensemble self-distillation. arXiv
    preprint arXiv:2110.06290, 2021.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C. Hawkins, V. N. Ioannidis, S. Adeshina, 和 G. Karypis. 通过自集成自蒸馏进行图神经网络的可扩展一致性训练。arXiv
    预印本 arXiv:2110.06290, 2021。'
- en: '[41] D. He, C. Liang, C. Huo, Z. Feng, D. Jin, L. Yang, and W. Zhang. Analyzing
    heterogeneous networks with missing attributes by unsupervised contrastive learning.
    TNNLS, 2022.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] D. He, C. Liang, C. Huo, Z. Feng, D. Jin, L. Yang, 和 W. Zhang. 通过无监督对比学习分析具有缺失属性的异构网络。TNNLS,
    2022。'
- en: '[42] B. Hooi, H. A. Song, A. Beutel, N. Shah, K. Shin, and C. Faloutsos. Fraudar:
    Bounding graph fraud in the face of camouflage. In KDD, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] B. Hooi, H. A. Song, A. Beutel, N. Shah, K. Shin, 和 C. Faloutsos. Fraudar:
    面对伪装的图欺诈边界。在 KDD, 2016。'
- en: '[43] Z. Hou, X. Liu, Y. Dong, C. Wang, J. Tang, et al. Graphmae: Self-supervised
    masked graph autoencoders. In KDD, 2022.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Z. Hou, X. Liu, Y. Dong, C. Wang, J. Tang, 等等. Graphmae: 自监督掩码图自编码器。在
    KDD, 2022。'
- en: '[44] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun. Gpt-gnn: Generative
    pre-training of graph neural networks. In KDD, 2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, 和 Y. Sun. Gpt-gnn：图神经网络的生成预训练。发表于KDD,
    2020。'
- en: '[45] Z. Hu, C. Fan, T. Chen, K.-W. Chang, and Y. Sun. Pre-training graph neural
    networks for generic structural feature extraction. arXiv preprint arXiv:1905.13728,
    2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Z. Hu, C. Fan, T. Chen, K.-W. Chang, 和 Y. Sun. 用于通用结构特征提取的图神经网络预训练。arXiv预印本
    arXiv:1905.13728, 2019。'
- en: '[46] Z. Huang, S. Zhang, C. Xi, T. Liu, and M. Zhou. Scaling up graph neural
    networks via graph coarsening. In KDD, 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Z. Huang, S. Zhang, C. Xi, T. Liu, 和 M. Zhou. 通过图粗化扩大图神经网络。发表于KDD, 2021。'
- en: '[47] B. Hui, P. Zhu, and Q. Hu. Collaborative graph convolutional networks:
    Unsupervised learning meets semi-supervised learning. In AAAI, 2020.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] B. Hui, P. Zhu, 和 Q. Hu. 协作图卷积网络：无监督学习与半监督学习的结合。发表于AAAI, 2020。'
- en: '[48] K. Ishiguro, S.-i. Maeda, and M. Koyama. Graph warp module: an auxiliary
    module for boosting the power of graph neural networks in molecular graph analysis.
    arXiv preprint arXiv:1902.01020, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] K. Ishiguro, S.-i. Maeda, 和 M. Koyama. 图变形模块：用于提升图神经网络在分子图分析中的能力的辅助模块。arXiv预印本
    arXiv:1902.01020, 2019。'
- en: '[49] G. Jeh and J. Widom. Scaling personalized web search. In WWW, 2003.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] G. Jeh 和 J. Widom. 扩展个性化网页搜索。发表于WWW, 2003。'
- en: '[50] Y. Jiao, Y. Xiong, J. Zhang, Y. Zhang, T. Zhang, and Y. Zhu. Sub-graph
    contrast for scalable self-supervised graph representation learning. In ICDM,
    2020.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. Jiao, Y. Xiong, J. Zhang, Y. Zhang, T. Zhang, 和 Y. Zhu. 用于可扩展自监督图表示学习的子图对比。发表于ICDM,
    2020。'
- en: '[51] D. Jin, C. Huo, C. Liang, and L. Yang. Heterogeneous graph neural network
    via attribute completion. In TheWebConf, 2021.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] D. Jin, C. Huo, C. Liang, 和 L. Yang. 通过属性补全的异构图神经网络。发表于TheWebConf, 2021。'
- en: '[52] M. Jin, Y. Zheng, Y.-F. Li, C. Gong, C. Zhou, and S. Pan. Multi-scale
    contrastive siamese networks for self-supervised graph representation learning.
    In IJCAI, 2021.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. Jin, Y. Zheng, Y.-F. Li, C. Gong, C. Zhou, 和 S. Pan. 用于自监督图表示学习的多尺度对比Siamese网络。发表于IJCAI,
    2021。'
- en: '[53] W. Jin, Y. Ma, X. Liu, X. Tang, S. Wang, and J. Tang. Graph structure
    learning for robust graph neural networks. In KDD, 2020.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] W. Jin, Y. Ma, X. Liu, X. Tang, S. Wang, 和 J. Tang. 为鲁棒图神经网络进行图结构学习。发表于KDD,
    2020。'
- en: '[54] W. Jin, X. Tang, H. Jiang, Z. Li, D. Zhang, J. Tang, and B. Yin. Condensing
    graphs via one-step gradient matching. In KDD, 2022.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] W. Jin, X. Tang, H. Jiang, Z. Li, D. Zhang, J. Tang, 和 B. Yin. 通过一步梯度匹配进行图压缩。发表于KDD,
    2022。'
- en: '[55] W. Jin, L. Zhao, S. Zhang, Y. Liu, J. Tang, and N. Shah. Graph condensation
    for graph neural networks. In ICLR, 2022.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] W. Jin, L. Zhao, S. Zhang, Y. Liu, J. Tang, 和 N. Shah. 图神经网络的图压缩。发表于ICLR,
    2022。'
- en: '[56] Y. Jin, A. Loukas, and J. JaJa. Graph coarsening with preserved spectral
    properties. In AISTATS, 2020.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Jin, A. Loukas, 和 J. JaJa. 保持谱特性的图粗化。发表于AISTATS, 2020。'
- en: '[57] A. Kazi, L. Cosmo, S.-A. Ahmadi, N. Navab, and M. Bronstein. Differentiable
    graph module (dgm) for graph convolutional networks. TPAMI, 2022.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Kazi, L. Cosmo, S.-A. Ahmadi, N. Navab, 和 M. Bronstein. 图卷积网络的可微图模块（DGM）。TPAMI,
    2022。'
- en: '[58] T. N. Kipf and M. Welling. Variational graph auto-encoders. arXiv preprint
    arXiv:1611.07308, 2016.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] T. N. Kipf 和 M. Welling. 变分图自编码器。arXiv预印本 arXiv:1611.07308, 2016。'
- en: '[59] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional
    networks. In ICLR, 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] T. N. Kipf 和 M. Welling. 使用图卷积网络进行半监督分类。发表于ICLR, 2017。'
- en: '[60] J. Klicpera, A. Bojchevski, and S. Günnemann. Predict then propagate:
    Graph neural networks meet personalized pagerank. In ICLR, 2018.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Klicpera, A. Bojchevski, 和 S. Günnemann. 预测然后传播：图神经网络与个性化PageRank的结合。发表于ICLR,
    2018。'
- en: '[61] J. Klicpera, S. Weißenberger, and S. Günnemann. Diffusion improves graph
    learning. In NeurIPS, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Klicpera, S. Weißenberger, 和 S. Günnemann. 扩散改进图学习。发表于NeurIPS, 2019。'
- en: '[62] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete
    structures. In ICML, 2002.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] R. I. Kondor 和 J. Lafferty. 图和其他离散结构上的扩散核。发表于ICML, 2002。'
- en: '[63] K. Kong, G. Li, M. Ding, Z. Wu, C. Zhu, B. Ghanem, G. Taylor, and T. Goldstein.
    Flag: Adversarial data augmentation for graph neural networks. arXiv preprint
    arXiv:2010.09891, 2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] K. Kong, G. Li, M. Ding, Z. Wu, C. Zhu, B. Ghanem, G. Taylor, 和 T. Goldstein.
    FLAG：图神经网络的对抗性数据增强。arXiv预印本 arXiv:2010.09891, 2020。'
- en: '[64] N. Lee, J. Lee, and C. Park. Augmentation-free self-supervised learning
    on graphs. In AAAI, 2022.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] N. Lee, J. Lee, 和 C. Park. 无需增强的自监督图学习。发表于AAAI, 2022。'
- en: '[65] J. Li, D. Cai, and X. He. Learning graph-level representation for drug
    discovery. arXiv preprint arXiv:1709.03741, 2017.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] J. Li, D. Cai, 和 X. He. 为药物发现学习图级表示。arXiv预印本 arXiv:1709.03741, 2017。'
- en: '[66] P. Li, Y. Wang, H. Wang, and J. Leskovec. Distance encoding: Design provably
    more powerful neural networks for graph representation learning. NeurIPS, 2020.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] P. Li, Y. Wang, H. Wang 和 J. Leskovec。距离编码：设计具有理论上更强大能力的图表示学习神经网络。NeurIPS，2020。'
- en: '[67] Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph convolutional
    networks for semi-supervised learning. In AAAI, 2018.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Q. Li, Z. Han 和 X.-M. Wu。对图卷积网络进行更深入的理解，以实现半监督学习。在 AAAI，2018。'
- en: '[68] S. Li, W.-T. Li, and W. Wang. Co-gcn for multi-view semi-supervised learning.
    In AAAI, 2020.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] S. Li, W.-T. Li 和 W. Wang。Co-gcn 用于多视角半监督学习。在 AAAI，2020。'
- en: '[69] X. Li, L. Wen, Y. Deng, F. Feng, X. Hu, L. Wang, and Z. Fan. Graph neural
    network with curriculum learning for imbalanced node classification. arXiv preprint
    arXiv:2202.02529, 2022.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] X. Li, L. Wen, Y. Deng, F. Feng, X. Hu, L. Wang 和 Z. Fan。带有课程学习的图神经网络用于不平衡节点分类。arXiv
    预印本 arXiv:2202.02529，2022。'
- en: '[70] S. Liu, H. Dong, L. Li, T. Xu, Y. Rong, P. Zhao, J. Huang, and D. Wu.
    Local augmentation for graph neural networks. ICML, 2022.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Liu, H. Dong, L. Li, T. Xu, Y. Rong, P. Zhao, J. Huang 和 D. Wu。图神经网络的局部增强。ICML，2022。'
- en: '[71] X. Liu, J. Ding, W. Jin, H. Xu, Y. Ma, Z. Liu, and J. Tang. Graph neural
    networks with adaptive residual. NeurIPS, 2021.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] X. Liu, J. Ding, W. Jin, H. Xu, Y. Ma, Z. Liu 和 J. Tang。具有自适应残差的图神经网络。NeurIPS，2021。'
- en: '[72] Y. Liu, M. Jin, S. Pan, C. Zhou, Y. Zheng, F. Xia, and P. Yu. Graph self-supervised
    learning: A survey. TKDE, 2022.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Liu, M. Jin, S. Pan, C. Zhou, Y. Zheng, F. Xia 和 P. Yu。图自监督学习：综述。TKDE，2022。'
- en: '[73] Y. Liu, Z. Zhang, Y. Liu, and Y. Zhu. Gatsmote: Improving imbalanced node
    classification on graphs via attention and homophily. Mathematics, 2022.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Liu, Z. Zhang, Y. Liu 和 Y. Zhu。Gatsmote：通过注意力和同质性改善图中的不平衡节点分类。Mathematics，2022。'
- en: '[74] A. Lucic, M. A. Ter Hoeve, G. Tolomei, M. De Rijke, and F. Silvestri.
    Cf-gnnexplainer: Counterfactual explanations for graph neural networks. In AISTATS,
    2022.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] A. Lucic, M. A. Ter Hoeve, G. Tolomei, M. De Rijke 和 F. Silvestri。Cf-gnnexplainer：图神经网络的反事实解释。在
    AISTATS，2022。'
- en: '[75] D. Luo, W. Cheng, W. Yu, B. Zong, J. Ni, H. Chen, and X. Zhang. Learning
    to drop: Robust graph neural network via topological denoising. In WSDM, 2021.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] D. Luo, W. Cheng, W. Yu, B. Zong, J. Ni, H. Chen 和 X. Zhang。学习丢弃：通过拓扑去噪的鲁棒图神经网络。在
    WSDM，2021。'
- en: '[76] Y. Luo, M. McThrow, W. Y. Au, T. Komikado, K. Uchino, K. Maruhash, and
    S. Ji. Automated data augmentations for graph classification. arXiv preprint arXiv:2202.13248,
    2022.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Luo, M. McThrow, W. Y. Au, T. Komikado, K. Uchino, K. Maruhash 和 S.
    Ji。图分类的自动数据增强。arXiv 预印本 arXiv:2202.13248，2022。'
- en: '[77] O. Mahmood, E. Mansimov, R. Bonneau, and K. Cho. Masked graph modeling
    for molecule generation. Nature communications, 2021.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] O. Mahmood, E. Mansimov, R. Bonneau 和 K. Cho。用于分子生成的掩码图建模。Nature communications，2021。'
- en: '[78] Y. Mo, L. Peng, J. Xu, X. Shi, and X. Zhu. Simple unsupervised graph representation
    learning. In AAAI, 2022.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. Mo, L. Peng, J. Xu, X. Shi 和 X. Zhu。简单无监督图表示学习。在 AAAI，2022。'
- en: '[79] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan,
    and M. Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
    In AAAI, 2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan
    和 M. Grohe。Weisfeiler 和 Leman 变得神经化：高阶图神经网络。在 AAAI，2019。'
- en: '[80] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking:
    Bringing order to the web. Technical report, Stanford InfoLab, 1999.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] L. Page, S. Brin, R. Motwani 和 T. Winograd。PageRank 引用排名：为网络带来秩序。技术报告，斯坦福
    InfoLab，1999。'
- en: '[81] H. Park, S. Lee, S. Kim, J. Park, J. Jeong, K.-M. Kim, J.-W. Ha, and H. J.
    Kim. Metropolis-hastings data augmentation for graph neural networks. NeurIPS,
    2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H. Park, S. Lee, S. Kim, J. Park, J. Jeong, K.-M. Kim, J.-W. Ha 和 H. J.
    Kim。Metropolis-Hastings 数据增强用于图神经网络。NeurIPS，2021。'
- en: '[82] J. Park, H. Shim, and E. Yang. Graph transplant: Node saliency-guided
    graph mixup with local structure preservation. In AAAI, 2022.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Park, H. Shim 和 E. Yang。图移植：节点显著性引导的图混合与局部结构保留。在 AAAI，2022。'
- en: '[83] T. Pham, T. Tran, H. Dam, and S. Venkatesh. Graph classification via deep
    learning with virtual nodes. arXiv preprint arXiv:1708.04357, 2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] T. Pham, T. Tran, H. Dam 和 S. Venkatesh。通过虚拟节点的深度学习进行图分类。arXiv 预印本 arXiv:1708.04357，2017。'
- en: '[84] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang, and J. Tang.
    Gcc: Graph contrastive coding for graph neural network pre-training. In KDD, 2020.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang 和 J. Tang。Gcc：图对比编码用于图神经网络预训练。在
    KDD，2020。'
- en: '[85] Y. Rong, W. Huang, T. Xu, and J. Huang. Dropedge: Towards deep graph convolutional
    networks on node classification. In ICLR, 2019.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Rong, W. Huang, T. Xu 和 J. Huang。Dropedge：针对节点分类的深度图卷积网络。在 ICLR，2019。'
- en: '[86] E. Rossi, F. Frasca, B. Chamberlain, D. Eynard, M. Bronstein, and F. Monti.
    Sign: Scalable inception graph neural networks. arXiv preprint arXiv:2004.11198,
    2020.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] E. Rossi、F. Frasca、B. Chamberlain、D. Eynard、M. Bronstein 和 F. Monti。Sign：可扩展的图神经网络。arXiv
    预印本 arXiv:2004.11198，2020 年。'
- en: '[87] E. Rossi, H. Kenlay, M. I. Gorinova, B. P. Chamberlain, X. Dong, and M. Bronstein.
    On the unreasonable effectiveness of feature propagation in learning on graphs
    with missing node features. arXiv preprint arXiv:2111.12128, 2021.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] E. Rossi、H. Kenlay、M. I. Gorinova、B. P. Chamberlain、X. Dong 和 M. Bronstein。在缺失节点特征的图上学习时特征传播的非凡有效性。arXiv
    预印本 arXiv:2111.12128，2021 年。'
- en: '[88] R. Sato, M. Yamada, and H. Kashima. Random features strengthen graph neural
    networks. In SDM, 2021.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] R. Sato、M. Yamada 和 H. Kashima。随机特征增强图神经网络。发表于 SDM，2021 年。'
- en: '[89] C. Shorten and T. M. Khoshgoftaar. A survey on image data augmentation
    for deep learning. Journal of Big Data, 2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] C. Shorten 和 T. M. Khoshgoftaar。深度学习中的图像数据增强调查。《大数据期刊》，2019 年。'
- en: '[90] K. Sun, Z. Lin, and Z. Zhu. Multi-stage self-supervised learning for graph
    convolutional networks on graphs with few labeled nodes. In AAAI, 2020.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] K. Sun、Z. Lin 和 Z. Zhu。图卷积网络的多阶段自监督学习，针对标签节点稀少的图。发表于 AAAI，2020 年。'
- en: '[91] L. Sun, Y. Dou, C. Yang, J. Wang, P. S. Yu, L. He, and B. Li. Adversarial
    attack and defense on graph data: A survey. arXiv preprint arXiv:1812.10528, 2018.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] L. Sun、Y. Dou、C. Yang、J. Wang、P. S. Yu、L. He 和 B. Li。图数据上的对抗攻击与防御：调查。arXiv
    预印本 arXiv:1812.10528，2018 年。'
- en: '[92] S. Suresh, V. Budde, J. Neville, P. Li, and J. Ma. Breaking the limit
    of graph neural networks by improving the assortativity of graphs with local mixing
    patterns. In KDD, 2021.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Suresh、V. Budde、J. Neville、P. Li 和 J. Ma。通过改进图的局部混合模式突破图神经网络的限制。发表于
    KDD，2021 年。'
- en: '[93] S. Suresh, P. Li, C. Hao, and J. Neville. Adversarial graph augmentation
    to improve graph contrastive learning. In NeurIPS, 2021.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Suresh、P. Li、C. Hao 和 J. Neville。对抗性图增强以改进图对比学习。发表于 NeurIPS，2021 年。'
- en: '[94] H. Taguchi, X. Liu, and T. Murata. Graph convolutional networks for graphs
    containing missing features. FGCS, 2021.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] H. Taguchi、X. Liu 和 T. Murata。用于包含缺失特征的图的图卷积网络。FGCS，2021 年。'
- en: '[95] Q. Tan, N. Liu, X. Huang, R. Chen, S.-H. Choi, and X. Hu. Mgae: Masked
    autoencoders for self-supervised learning on graphs. arXiv preprint arXiv:2201.02534,
    2022.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Q. Tan、N. Liu、X. Huang、R. Chen、S.-H. Choi 和 X. Hu。Mgae：图上的自监督学习的掩蔽自编码器。arXiv
    预印本 arXiv:2201.02534，2022 年。'
- en: '[96] S. Thakoor, C. Tallec, M. G. Azar, M. Azabou, E. L. Dyer, R. Munos, P. Veličković,
    and M. Valko. Large-scale representation learning on graphs via bootstrapping.
    In ICLR, 2021.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] S. Thakoor、C. Tallec、M. G. Azar、M. Azabou、E. L. Dyer、R. Munos、P. Veličković
    和 M. Valko。通过自举进行大规模图表示学习。发表于 ICLR，2021 年。'
- en: '[97] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method.
    arXiv preprint physics/0004057, 2000.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] N. Tishby、F. C. Pereira 和 W. Bialek。信息瓶颈方法。arXiv 预印本 physics/0004057，2000
    年。'
- en: '[98] J. Topping, F. Di Giovanni, B. P. Chamberlain, X. Dong, and M. M. Bronstein.
    Understanding over-squashing and bottlenecks on graphs via curvature. In ICLR,
    2022.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Topping、F. Di Giovanni、B. P. Chamberlain、X. Dong 和 M. M. Bronstein。通过曲率理解图上的过度挤压和瓶颈。发表于
    ICLR，2022 年。'
- en: '[99] P. Velickovic, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D.
    Hjelm. Deep graph infomax. ICLR, 2019.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] P. Velickovic、W. Fedus、W. L. Hamilton、P. Liò、Y. Bengio 和 R. D. Hjelm。深度图信息最大化。发表于
    ICLR，2019 年。'
- en: '[100] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    and Y. Bengio. Manifold mixup: Better representations by interpolating hidden
    states. In ICML, 2019.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] V. Verma、A. Lamb、C. Beckham、A. Najafi、I. Mitliagkas、D. Lopez-Paz 和 Y.
    Bengio。流形混合：通过插值隐藏状态获得更好的表示。发表于 ICML，2019 年。'
- en: '[101] V. Verma, M. Qu, K. Kawaguchi, A. Lamb, Y. Bengio, J. Kannala, and J. Tang.
    Graphmix: Improved training of gnns for semi-supervised learning. In AAAI, 2021.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] V. Verma、M. Qu、K. Kawaguchi、A. Lamb、Y. Bengio、J. Kannala 和 J. Tang。Graphmix：改进
    GNNs 的半监督学习训练。发表于 AAAI，2021 年。'
- en: '[102] J. Wang, K. Ding, L. Hong, H. Liu, and J. Caverlee. Next-item recommendation
    with sequential hypergraphs. In SIGIR, 2020.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Wang、K. Ding、L. Hong、H. Liu 和 J. Caverlee。使用序列超图进行下一项推荐。发表于 SIGIR，2020
    年。'
- en: '[103] R. Wang, S. Mou, X. Wang, W. Xiao, Q. Ju, C. Shi, and X. Xie. Graph structure
    estimation neural networks. In TheWebConf, 2021.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] R. Wang、S. Mou、X. Wang、W. Xiao、Q. Ju、C. Shi 和 X. Xie。图结构估计神经网络。发表于 TheWebConf，2021
    年。'
- en: '[104] X. Wang and J. Chen. A dual-branch graph convolutional network on imbalanced
    node classification. In CSAE, 2021.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] X. Wang 和 J. Chen。在不平衡节点分类上的双分支图卷积网络。发表于 CSAE，2021 年。'
- en: '[105] X. Wang, X. Liu, and C.-J. Hsieh. Graphdefense: Towards robust graph
    convolutional networks. arXiv preprint arXiv:1911.04429, 2019.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] X. Wang、X. Liu 和 C.-J. Hsieh。Graphdefense：朝着稳健的图卷积网络迈进。arXiv 预印本 arXiv:1911.04429，2019
    年。'
- en: '[106] Y. Wang, C. Aggarwal, and T. Derr. Distance-wise prototypical graph neural
    network in node imbalance classification. arXiv preprint arXiv:2110.12035, 2021.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Wang, C. Aggarwal, 和 T. Derr. 节点不平衡分类中的距离原型图神经网络。arXiv 预印本 arXiv:2110.12035,
    2021。'
- en: '[107] Y. Wang, W. Wang, Y. Liang, Y. Cai, and B. Hooi. Mixup for node and graph
    classification. In TheWebConf, 2021.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Y. Wang, W. Wang, Y. Liang, Y. Cai, 和 B. Hooi. Mixup 用于节点和图分类。发表于 TheWebConf,
    2021。'
- en: '[108] Y. Wang, W. Wang, Y. Liang, Y. Cai, J. Liu, and B. Hooi. Nodeaug: Semi-supervised
    node classification with data augmentation. In KDD, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Y. Wang, W. Wang, Y. Liang, Y. Cai, J. Liu, 和 B. Hooi. Nodeaug：带有数据增强的半监督节点分类。发表于
    KDD, 2020。'
- en: '[109] X. Wei, Y. Li, X. Qin, X. Xu, X. Li, and M. Liu. From decoupling to reconstruction:
    A robust graph neural network against topology attacks. In WCSP, 2020.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] X. Wei, Y. Li, X. Qin, X. Xu, X. Li, 和 M. Liu. 从解耦到重建：针对拓扑攻击的鲁棒图神经网络。发表于
    WCSP, 2020。'
- en: '[110] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger. Simplifying
    graph convolutional networks. In ICML, 2019.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, 和 K. Weinberger. 简化图卷积网络。发表于
    ICML, 2019。'
- en: '[111] H. Wu, C. Wang, Y. Tyshetskiy, A. Docherty, K. Lu, and L. Zhu. Adversarial
    examples for graph data: deep insights into attack and defense. In IJCAI, 2019.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] H. Wu, C. Wang, Y. Tyshetskiy, A. Docherty, K. Lu, 和 L. Zhu. 图数据中的对抗样本：攻击与防御的深刻洞察。发表于
    IJCAI, 2019。'
- en: '[112] L. Wu, H. Lin, Z. Gao, C. Tan, S. Li, et al. Graphmixup: Improving class-imbalanced
    node classification on graphs by self-supervised context prediction. arXiv preprint
    arXiv:2106.11133, 2021.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] L. Wu, H. Lin, Z. Gao, C. Tan, S. Li 等. Graphmixup：通过自监督上下文预测改善图上的类别不平衡节点分类。arXiv
    预印本 arXiv:2106.11133, 2021。'
- en: '[113] L. Wu, H. Lin, C. Tan, Z. Gao, and S. Z. Li. Self-supervised learning
    on graphs: Contrastive, generative, or predictive. TKDE, 2021.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] L. Wu, H. Lin, C. Tan, Z. Gao, 和 S. Z. Li. 图上的自监督学习：对比性、生成性或预测性。TKDE,
    2021。'
- en: '[114] T. Wu, H. Ren, P. Li, and J. Leskovec. Graph information bottleneck.
    In NeurIPS, 2020.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] T. Wu, H. Ren, P. Li, 和 J. Leskovec. 图信息瓶颈。发表于 NeurIPS, 2020。'
- en: '[115] X. Wu, L. Zhao, and L. Akoglu. A quest for structure: jointly learning
    the graph structure and semi-supervised classification. In CIKM, 2018.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] X. Wu, L. Zhao, 和 L. Akoglu. 结构探寻：联合学习图结构和半监督分类。发表于 CIKM, 2018。'
- en: '[116] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip. A comprehensive
    survey on graph neural networks. TNNLS, 2020.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, 和 S. Y. Philip. 图神经网络的全面综述。TNNLS,
    2020。'
- en: '[117] J. Xia, L. Wu, J. Chen, B. Hu, and S. Z. Li. Simgrace: A simple framework
    for graph contrastive learning without data augmentation. In TheWebConf, 2022.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] J. Xia, L. Wu, J. Chen, B. Hu, 和 S. Z. Li. Simgrace：一个无需数据增强的图对比学习简单框架。发表于
    TheWebConf, 2022。'
- en: '[118] Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le. Unsupervised data augmentation
    for consistency training. In NeurIPS, 2020.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Q. Xie, Z. Dai, E. Hovy, T. Luong, 和 Q. Le. 无监督数据增强用于一致性训练。发表于 NeurIPS,
    2020。'
- en: '[119] Y. Xie, Z. Xu, J. Zhang, Z. Wang, and S. Ji. Self-supervised learning
    of graph neural networks: A unified review. TPAMI, 2022.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Y. Xie, Z. Xu, J. Zhang, Z. Wang, 和 S. Ji. 图神经网络的自监督学习：一个统一的综述。TPAMI,
    2022。'
- en: '[120] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural
    networks? In ICLR, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] K. Xu, W. Hu, J. Leskovec, 和 S. Jegelka. 图神经网络的能力有多强？发表于 ICLR, 2018。'
- en: '[121] Z. Xu, B. Du, and H. Tong. Graph sanitation with application to node
    classification. arXiv preprint arXiv:2105.09384, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Z. Xu, B. Du, 和 H. Tong. 图清洗及其在节点分类中的应用。arXiv 预印本 arXiv:2105.09384, 2021。'
- en: '[122] L. Yang, Z. Kang, X. Cao, D. Jin, B. Yang, and Y. Guo. Topology optimization
    based graph convolutional network. In IJCAI, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] L. Yang, Z. Kang, X. Cao, D. Jin, B. Yang, 和 Y. Guo. 基于拓扑优化的图卷积网络。发表于
    IJCAI, 2019。'
- en: '[123] L. Yang, L. Zhang, and W. Yang. Graph adversarial self-supervised learning.
    In NeurIPS, 2021.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] L. Yang, L. Zhang, 和 W. Yang. 图对抗自监督学习。发表于 NeurIPS, 2021。'
- en: '[124] D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised
    methods. In ACL, 1995.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] D. Yarowsky. 无监督词义消歧技术可与监督方法相媲美。发表于 ACL, 1995。'
- en: '[125] Y. Ye and S. Ji. Sparse graph attention networks. TKDE, 2021.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Y. Ye 和 S. Ji. 稀疏图注意网络。TKDE, 2021。'
- en: '[126] J. You, J. M. Gomes-Selman, R. Ying, and J. Leskovec. Identity-aware
    graph neural networks. In AAAI, 2021.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. You, J. M. Gomes-Selman, R. Ying, 和 J. Leskovec. 识别感知图神经网络。发表于 AAAI,
    2021。'
- en: '[127] Y. You, T. Chen, Y. Shen, and Z. Wang. Graph contrastive learning automated.
    In ICML, 2021.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Y. You, T. Chen, Y. Shen, 和 Z. Wang. 自动化图对比学习。发表于 ICML, 2021。'
- en: '[128] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen. Graph contrastive
    learning with augmentations. In NeurIPS, 2020.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, 和 Y. Shen. 带有增强的图对比学习。发表于
    NeurIPS, 2020。'
- en: '[129] Y. You, T. Chen, Z. Wang, and Y. Shen. When does self-supervision help
    graph convolutional networks? In ICML, 2020.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. 游, T. 陈, Z. 王, 和 Y. 沈. 自监督何时有助于图卷积网络？ 在 ICML, 2020.'
- en: '[130] H. Yue, C. Zhang, C. Zhang, and H. Liu. Label-invariant augmentation
    for semi-supervised graph classification. In NeurIPS, 2022.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] H. 岳, C. 张, C. 张, 和 H. 刘. 用于半监督图分类的标签不变增强。 在 NeurIPS, 2022.'
- en: '[131] H. Zeng, M. Zhang, Y. Xia, A. Srivastava, A. Malevich, R. Kannan, V. Prasanna,
    L. Jin, and R. Chen. Decoupling the depth and scope of graph neural networks.
    NeurIPS, 2021.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] H. 曾, M. 张, Y. 夏, A. 斯里瓦斯塔瓦, A. 马列维奇, R. 坎南, V. 普拉萨纳, L. 金, 和 R. 陈. 解耦图神经网络的深度和范围。NeurIPS,
    2021.'
- en: '[132] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna. Graphsaint:
    Graph sampling based inductive learning method. In ICLR, 2019.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] H. 曾, H. 周, A. 斯里瓦斯塔瓦, R. 坎南, 和 V. 普拉萨纳. Graphsaint: 基于图采样的归纳学习方法。 在
    ICLR, 2019.'
- en: '[133] J. Zeng and P. Xie. Contrastive self-supervised learning for graph classification.
    In AAAI, 2021.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. 曾 和 P. 谢. 图分类的对比自监督学习。 在 AAAI, 2021.'
- en: '[134] A. Zhang and J. Ma. Defensevgae: Defending against adversarial attacks
    on graph data via a variational graph autoencoder. arXiv preprint arXiv:2006.08900,
    2020.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] A. 张 和 J. 马. Defensevgae: 通过变分图自编码器防御图数据上的对抗攻击。arXiv 预印本 arXiv:2006.08900,
    2020.'
- en: '[135] C. Zhang, Y. He, Y. Cen, Z. Hou, and J. Tang. Improving the training
    of graph neural networks with consistency regularization. arXiv preprint arXiv:2112.04319,
    2021.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] C. 张, Y. 何, Y. 岑, Z. 侯, 和 J. 唐. 通过一致性正则化改善图神经网络的训练。arXiv 预印本 arXiv:2112.04319,
    2021.'
- en: '[136] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical
    risk minimization. In ICLR, 2018.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. 张, M. 西塞, Y. N. 多芬, 和 D. 洛佩斯-帕斯. mixup: 超越经验风险最小化。 在 ICLR, 2018.'
- en: '[137] J. Zhang, H. Zhang, C. Xia, and L. Sun. Graph-bert: Only attention is
    needed for learning graph representations. arXiv preprint arXiv:2001.05140, 2020.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. 张, H. 张, C. 夏, 和 L. 孙. Graph-bert: 仅需注意力来学习图表示。arXiv 预印本 arXiv:2001.05140,
    2020.'
- en: '[138] M. Zhang and P. Li. Nested graph neural networks. NeurIPS, 2021.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] M. 张 和 P. 李. 嵌套图神经网络。NeurIPS, 2021.'
- en: '[139] S. Zhang, H. Tong, J. Xu, and R. Maciejewski. Graph convolutional networks:
    a comprehensive review. Computational Social Networks, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S. 张, H. 童, J. 许, 和 R. 马切耶夫斯基. 图卷积网络：全面综述。计算社会网络, 2019.'
- en: '[140] X. Zhang and M. Zitnik. Gnnguard: Defending graph neural networks against
    adversarial attacks. NeurIPS, 2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] X. 张和 M. Zitnik. Gnnguard: 保护图神经网络免受对抗攻击。NeurIPS, 2020.'
- en: '[141] Y. Zhang, S. Pal, M. Coates, and D. Ustebay. Bayesian graph convolutional
    neural networks for semi-supervised classification. In AAAI, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. 张, S. 帕尔, M. 科茨, 和 D. 乌斯特巴伊. 用于半监督分类的贝叶斯图卷积神经网络。 在 AAAI, 2019.'
- en: '[142] J. Zhao, Y. Dong, M. Ding, E. Kharlamov, and J. Tang. Adaptive diffusion
    in graph neural networks. In NeurIPS, 2021.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. 赵, Y. 董, M. 丁, E. 哈拉莫夫, 和 J. 唐. 图神经网络中的自适应扩散。 在 NeurIPS, 2021.'
- en: '[143] J. Zhao, X. Wang, C. Shi, B. Hu, G. Song, and Y. Ye. Heterogeneous graph
    structure learning for graph neural networks. In AAAI, 2021.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. 赵, X. 王, C. 石, B. 胡, G. 宋, 和 Y. 叶. 用于图神经网络的异构图结构学习。 在 AAAI, 2021.'
- en: '[144] T. Zhao, G. Liu, D. Wang, W. Yu, and M. Jiang. Learning from counterfactual
    links for link prediction. In ICML, 2022.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] T. 赵, G. 刘, D. 王, W. 于, 和 M. 姜. 从反事实链接中学习以进行链接预测。 在 ICML, 2022.'
- en: '[145] T. Zhao, Y. Liu, L. Neves, O. Woodford, M. Jiang, and N. Shah. Data augmentation
    for graph neural networks. In AAAI, 2021.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] T. 赵, Y. 刘, L. 内维斯, O. 伍德福德, M. 姜, 和 N. 沙赫. 图神经网络的数据增强。 在 AAAI, 2021.'
- en: '[146] T. Zhao, X. Zhang, and S. Wang. Graphsmote: Imbalanced node classification
    on graphs with graph neural networks. In WSDM, 2021.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] T. 赵, X. 张, 和 S. 王. Graphsmote: 基于图神经网络的图上不平衡节点分类。 在 WSDM, 2021.'
- en: '[147] C. Zheng, B. Zong, W. Cheng, D. Song, J. Ni, W. Yu, H. Chen, and W. Wang.
    Robust graph representation learning via neural sparsification. In ICML, 2020.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] C. 郑, B. 宗, W. 程, D. 宋, J. 倪, W. 于, H. 陈, 和 W. 王. 通过神经稀疏化的鲁棒图表示学习。 在
    ICML, 2020.'
- en: '[148] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang. Deep graph contrastive
    representation learning. arXiv preprint arXiv:2006.04131, 2020.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Y. 朱, Y. 许, F. 于, Q. 刘, S. 吴, 和 L. 王. 深度图对比表示学习。arXiv 预印本 arXiv:2006.04131,
    2020.'
- en: '[149] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang. Graph contrastive learning
    with adaptive augmentation. In TheWebConf, 2021.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. 朱, Y. 许, F. 于, Q. 刘, S. 吴, 和 L. 王. 带有自适应增强的图对比学习。 在 TheWebConf, 2021.'
- en: '[150] D. Zou, Z. Hu, Y. Wang, S. Jiang, Y. Sun, and Q. Gu. Layer-dependent
    importance sampling for training deep and large graph convolutional networks.
    NeurIPS, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] D. Zou, Z. Hu, Y. Wang, S. Jiang, Y. Sun, 和 Q. Gu. 针对训练深度和大规模图卷积网络的层依赖重要性采样。NeurIPS,
    2019.'
