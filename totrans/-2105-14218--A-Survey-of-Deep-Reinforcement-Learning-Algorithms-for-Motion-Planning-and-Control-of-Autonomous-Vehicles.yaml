- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:54:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[2105.14218] A Survey of Deep Reinforcement Learning Algorithms for Motion
    Planning and Control of Autonomous Vehicles'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2105.14218](https://ar5iv.labs.arxiv.org/html/2105.14218)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Reinforcement Learning Algorithms for
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Motion Planning and Control of Autonomous Vehicles
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Fei Ye¹, Shen Zhang², Pin Wang³, and Ching-Yao Chan³ ¹ F. Ye is with TuSimple
    Inc., 9191 Towne Centre Dr. Ste 600, San Diego, CA 92122, USA. fei.ye@tusimple.ai.²
    S. Zhang is with Georgia Institute of Technology, Atlanta, GA 30332, USA. shenzhang@gatech.edu.³
    P. Wang and C. Chan are with California PATH, University of California, Berkeley,
    Richmond, CA 94804, USA. {pin_wang, cychan}@berkeley.edu.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this survey, we systematically summarize the current literature on studies
    that apply reinforcement learning (RL) to the motion planning and control of autonomous
    vehicles. Many existing contributions can be attributed to the pipeline approach,
    which consists of many hand-crafted modules, each with a functionality selected
    for the ease of human interpretation. However, this approach does not automatically
    guarantee maximal performance due to the lack of a system-level optimization.
    Therefore, this paper also presents a growing trend of work that falls into the
    end-to-end approach, which typically offers better performance and smaller system
    scales. However, their performance also suffers from the lack of expert data and
    generalization issues. Finally, the remaining challenges applying deep RL algorithms
    on autonomous driving are summarized, and future research directions are also
    presented to tackle these challenges.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automated and semi-automated vehicles are gaining popularity in assisting our
    daily transportation. There is a considerable amount of studies in the past decade
    focusing on autonomous driving applications [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]. Specifically, a large number
    of research activities based on deep learning have been conducted for advanced
    driving assistance systems (ADAS) and automated driving applications, aiming to
    automate as much of the driving task as possible. Supervised learning approaches
    rely heavily on large amounts of labeled data to be able to generalize and it
    is basically trained on each task in isolation. However, obtaining a big amount
    of data for each individual task in autonomous driving is costly and time-consuming.
    Moreover, it requires massive human labor to label such data and still may not
    cover all the complex situations in the real-world driving.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, reinforcement learning (RL) algorithms have been extensively
    applied to vehicle decision making and control problems [[5](#bib.bib5), [6](#bib.bib6)].
    Specifically, RL is able to learn in a trial-and-error way and does not require
    explicit human labeling or supervision on each data sample. Instead, it needs
    a well-defined reward function to receive reward signals in its learning process.
    Additionally, there is a wide variety of deep RL algorithms and high flexibility
    in the implementation level, such as the state space, the action space, and the
    reward function, etc.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，强化学习（RL）算法已广泛应用于车辆决策制定和控制问题 [[5](#bib.bib5), [6](#bib.bib6)]。具体而言，RL能够通过试错方式进行学习，并且不需要对每个数据样本进行显式的人工标记或监督。相反，它需要一个定义良好的奖励函数来在学习过程中接收奖励信号。此外，深度强化学习算法种类繁多，实施层面具有高度的灵活性，如状态空间、动作空间和奖励函数等。
- en: In general, existing work applying deep RL to the motion planning and control
    of autonomous vehicles can be divided into the hierarchical (pipeline) approach
    and the end-to-end approach [[7](#bib.bib7)], as demonstrated in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles"). Specifically, the pipeline
    approach can be typically categorized into different modules such as perception,
    decision making, motion planning, low-level control, and so on. In this paper
    we’ll review and examine papers that apply deep RL algorithms to specifically
    accomplish functionalities of motion planning and vehicle control. Each of these
    modules is engineered manually, and the interfaces between the modules are typically
    implemented by hands. However, this modular distribution is obviously targeted
    for the convenience of human interpretation, rather than the highest attainable
    system performance. For example, if there is a pipeline system with parts that
    can improve with data, and with parts that they don’t, then those parts that do
    not improve with data will eventually become the bottleneck.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，将深度强化学习应用于自动驾驶车辆的运动规划和控制的现有工作可以分为分层（管道）方法和端到端方法 [[7](#bib.bib7)]，如图 [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles") 所示。具体来说，管道方法通常可以分为不同的模块，如感知、决策制定、运动规划、低级控制等。在本文中，我们将回顾和研究那些应用深度强化学习算法以实现运动规划和车辆控制功能的论文。这些模块中的每一个都是手动设计的，模块之间的接口通常是人工实现的。然而，这种模块化分布显然是为了方便人类解释，而不是为了达到最高的系统性能。例如，如果一个管道系统中有些部分可以通过数据改进，而有些则不能，那么那些无法通过数据改进的部分最终将成为瓶颈。
- en: 'Recently, more efforts have been devoted to the second approach enabled by
    deep RL, which is the end-to-end approach that can optimize all those modules
    of abstractions to map sensory input to control commands with the minimal number
    of processing steps. There are mainly two reasons why we might want to apply end-to-end
    techniques to autonomous driving: 1) better performance; and 2) smaller system
    scales [[8](#bib.bib8)]. Better performance will result because the internal components
    can be self-optimized to maximize overall system performance, instead of optimizing
    human-selected intermediate criteria. Smaller networks are possible because the
    system learns to solve problems with minimal processing steps.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，更多的努力被投入到由深度强化学习（deep RL）支持的第二种方法，即端到端的方法。该方法可以优化所有这些抽象模块，将感官输入映射到控制命令，同时减少处理步骤。我们可能希望将端到端技术应用于自动驾驶的主要原因有两个：1）更好的性能；2）更小的系统规模
    [[8](#bib.bib8)]。更好的性能是因为内部组件可以自我优化以最大化整体系统性能，而不是优化人工选择的中间标准。较小的网络是可能的，因为系统学会以最少的处理步骤解决问题。
- en: '![Refer to caption](img/bd29d3f5a41a484d56dde93bae649c90.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bd29d3f5a41a484d56dde93bae649c90.png)'
- en: 'Figure 1: An illustration of the pipeline and the end-to-end approach for motion
    planning and control of autonomous vehicles, figure adapted from [[2](#bib.bib2)].'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：运动规划和控制自动驾驶车辆的管道和端到端方法的示意图，图源自 [[2](#bib.bib2)]。
- en: In this context, this paper seeks to summarize existing work that explains how
    deep RL algorithms, when combined together with deep neural network representations,
    can generalize and perform automated vehicle decision making and control. The
    rest of the paper is organized as follows. In Sections II and III, we introduce
    how different deep RL algorithms can be leveraged to accomplish behavioral decision
    making, motion planning and control modules in the pipeline approach. Next, in
    Section IV, we take a deep dive into end-to-end learning methods for both real-world
    applications and the sim-to-real approach. Section V summarizes existing challenges
    and future work directions in applying deep RL to autonomous vehicles. Finally,
    a conclusion is provided in Section VI.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一背景下，本文旨在总结现有的工作，解释当深度强化学习算法与深度神经网络表示结合时，如何能够泛化并执行自动驾驶车辆的决策和控制。本文的其余部分组织如下。在第二和第三部分，我们介绍了如何利用不同的深度强化学习算法来实现管道方法中的行为决策、运动规划和控制模块。接下来，在第四部分，我们深入探讨了针对实际应用和从模拟到实际的方法的端到端学习方法。第五部分总结了在将深度强化学习应用于自主驾驶车辆中的现有挑战和未来工作方向。最后，第六部分提供了结论。
- en: II Deep Reinforcement Learning for Decision Making and Motion Planning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度强化学习在决策和运动规划中的应用
- en: In this section, we focus on recent advances in behavioral decision-making and
    motion planning for autonomous vehicles based on deep RL. As shown in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles"), the typical pipelines
    of an autonomous driving system process a stream of observation from vehicle on-board
    sensors with high level routing plans to the executable control output such as
    steering angles, accelerations and braking actions. Typically, a hierarchical
    structure in the design of planning systems for autonomous driving is usually
    desired, since driving is naturally hierarchical as higher level decisions are
    made on discrete state transitions and lower level executions are performed in
    continuous state space. Behavior layer is a decision making system that determines
    the transition of discrete state of mid-level driving behaviors such as lane changing,
    car-following, turning left/right, etc. When the behavior decision is made, the
    motion planning system is responsible for providing a safe, comfortable, and dynamically
    feasible continuous trajectory to achieve the selected driving behavior from the
    decision making system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们重点关注基于深度强化学习的自主驾驶车辆在行为决策和运动规划方面的最新进展。如图[1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ A Survey of Deep Reinforcement Learning Algorithms for Motion Planning and Control
    of Autonomous Vehicles")所示，自主驾驶系统的典型流程处理来自车载传感器的观测流，并将其高层路由计划转化为可执行的控制输出，如转向角、加速度和制动动作。通常，在自主驾驶规划系统的设计中，期望采用层次结构，因为驾驶自然是层次性的，高层决策基于离散状态转换，而低层执行则在连续状态空间中进行。行为层是一个决策系统，它确定中层驾驶行为（如变道、跟车、左转/右转等）的离散状态转换。当行为决策做出时，运动规划系统负责提供一个安全、舒适且动态可行的连续轨迹，以实现决策系统所选择的驾驶行为。
- en: Deep reinforcement learning has shown great success in the area of vehicle behavioral
    decision makings, especially in the highway scenarios and urban intersections.
    To reduce the sample complexity, some of the studies choose to adopt the mid-level
    inputs from the perception system processes and extract the vehicle state and
    relative states of surrounding vehicles as inputs. Hoel et al [[9](#bib.bib9)]
    trained a Deep Q-Network (DQN) in a simulation environment to issue driving behavioral
    commands (e.g. change lanes to the right/left, cruise on the current lane and
    etc.) and compared the different neural network structures’ effects on the agent
    performance. To migrate the concerns of safety performance of a trained RL agent,
    it is common to add rule-based safety constraints that can verify unsafe actions
    before they are executed. In [[10](#bib.bib10)], a prediction model combined with
    Deep Q-Network is proposed given the outputs from the perception system to label
    the unsafe behavioral decisions in unprotected turn scenarios. Some studies [[11](#bib.bib11),
    [12](#bib.bib12)] alternatively trained a lane change decision making system based
    on DQN for behavioral level decision-making and uses an underlying rule-based
    layer to verify the safety of a planned trajectory before it is executed by the
    vehicle control system. In [[13](#bib.bib13)], a hierarchical RL based architecture
    is presented to combine lane change decisions and motion planning together. Specifically,
    a deep Q-network (DQN) is trained to decide when to conduct the maneuver based
    on safety considerations, while a deep Q-learning framework with quadratic approximator
    is designed to complete the maneuver in longitudinal direction. On the other hand,
    some of the studies choose to learn from human demonstrations via imitation learning
    and introduce perturbation to discourage undesirable behavior [[14](#bib.bib14)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: More recently, actor-critic policy-based RL methods are introduced in autonomous
    vehicle decision making and motion planning. Compared to value-based RL methods
    such as Deep Q-Networks that approximate the value function using neural networks
    in an off-policy way, the primary advantage of actor-critic policy-based RL method
    is that they can directly compute actions from the policy gradient rather than
    optimizing from the value function, while remaining stable during function approximations.
    On the other hand, the merit of the critic is to supply the actor with the knowledge
    of performance in low variance. Actor-Critic algorithm has been successfully applied
    in both discrete behavior decision makings and continuous motion planning tasks
    [[15](#bib.bib15), [16](#bib.bib16)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: III Deep Reinforcement Learning for Vehicle Control
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the pipeline approach, the perception layer such as CNN is typically used
    to process and recognize objects in digital images. Afterwards, deep RL algorithms
    can be applied to understand and learn intelligent actions and controls. Many
    papers are devoted to accomplishing low-level vehicle control with deep RL methods,
    such as lane keeping [[17](#bib.bib17), [18](#bib.bib18)], lateral control [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21)], longitudinal control [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)],
    or both [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: For lane-keeping, a DDPG model was implemented in continuous state and action
    space in [[17](#bib.bib17)] to guide its training and apply it to solve the lane
    keeping (steering control) problem in self-driving or autonomous driving. It is
    shown that the proposed method can help speed up RL training remarkably for the
    lane keeping task as compared to the RL algorithm without exploiting the state-action
    permissibility-based guidance and other baselines that employ constrained action
    space exploration strategies. To improve efficiency and reduce failures in autonomous
    vehicles, two different algorithms, namely the robust adversarial RL and neural
    fictitious self play are proposed in [[18](#bib.bib18)], and compares performance
    on lane keeping and lane changing scenarios. The results exhibit improved driving
    efficiency while effectively reducing collision rates compared to baseline control
    policies produced by traditional RL methods.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the vehicle lateral control, to address the high computational complexity
    of model predictive control (MPC) for real-time implementation, a fast integrated
    planning and control framework is proposed in [[19](#bib.bib19)] that combines
    a driving policy layer and an execution layer. Several example driving scenarios
    demonstrated that the performance of the policy layer can be improved quickly
    and continuously online. In [[20](#bib.bib20)], DDPG was implemented to formulate
    the lane change behavior with continuous action in a model-free dynamic driving
    environment, and the reward function takes the position deviation status and the
    maneuvering time into account. Eventually, the RL agent is trained to smoothly
    and stably change to the target lane with a success rate of 100% under diverse
    driving situations in simulation. In [[21](#bib.bib21)], a vision-based lateral
    control system was broken into a perception module and a control module, which
    is based on deep RL to make a control decision based on features coming from the
    perception module. The trained RL controller in visual TORCS outperforms the linear
    quadratic regulator (LQR) controller and model MPC controller on different tracks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: IV End-to-End Deep Reinforcement Learning for Autonomous Driving
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous two sections summarize existing work that fall into the pipeline
    approach, which consists of many hand-crafted modules for the ease of human interpretation,
    such as perception, motion planning, decision making, low-level control, and so
    on. However, this pipeline approach does not guarantee maximum system performance
    if some parts of modules cannot improve with data, which will eventually become
    the bottleneck.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the end-to-end approach is expected to have better performance
    and smaller systems for autonomous driving. With end-to-end training, it is not
    possible to make a clean break among different modules of a system, such as which
    parts of the network mainly act as feature extractors and which ones serve as
    controllers. However, it is able to optimize all those modules of abstractions
    simultaneously with a reduced number of processing steps, which indicates that
    the abstractions will be optimally and automatically adapted for the task that
    needs to be solved, instead of optimizing human-selected intermediate criteria.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Joint Optimization in Real-World Applications
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A very early attempt that successfully steered a car on public roads using the
    end-to-end approach can be found in Pomerleau’s pioneering work in 1989 [[32](#bib.bib32)],
    in which controlling an autonomous land vehicle was trained using a neural network
    system. In the early 2000’s, LeCun et al. built a small off-road robot that uses
    an end-to-end learning system to avoid obstacles solely from visual input [[33](#bib.bib33)].
    The robot named DAVE was trained on images sampled from human driving videos,
    paired with the corresponding steering command $1/r$. While DAVE demonstrated
    the potential of end-to-end learning, its performance was not reliable enough
    to provide a complete alternative to more modular methods of off-road driving,
    as its mean distance between crashes was around 20 meters in complex environments.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: With the evolution of hardware computation capabilities and the advancement
    of modern deep learning algorithms, in 2016, researchers at NVIDIA proposed an
    end-to-end learning based on CNN that learns the entire processing pipeline needed
    to directly steer an automobile [[34](#bib.bib34)]. Training data was collected
    from less than a hundred hours of expert driving on a wide variety of roads, paired
    with time-synchronized steering commands generated by the human driver. The sampled
    images are fed into a CNN consisting of five convolutional layers with three fully
    connected layers, which then computes the proposed steering command. The weights
    of the CNN are optimized by minimizing the mean squared error between the proposed
    command and the benchmark command for that image. This milestone also empirically
    validated that CNNs can “learn the entire task of lane and road following without
    manual decomposition into road or lane marking detection, semantic abstraction,
    path planning, and control” [[34](#bib.bib34)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Direct deep supervised learning usually requires a large amount of data to learn
    a generic driving policy and the ground truth labeling for training. For example,
    [[35](#bib.bib35)] proposes a novel FCN-LSTM architecture to leverage both previous
    vehicle states and current visual observations using a long short-term memory
    temporal encoder with a fully convolutional visual encoder. Meanwhile, this work
    also released the Berkeley DeepDrive Video dataset (BDDV) for learning driving
    models and used the human driving behavior as the ground truth label for training.
    As such, human labeling can be costly and time-consuming. Furthermore, the policy
    is trained to mimic human behavior in a certain scenarios and it is hard to cover
    all the real-world scenarios.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 直接的深度监督学习通常需要大量数据来学习通用驾驶策略以及训练的地面真相标签。例如，[[35](#bib.bib35)] 提出了一个新颖的 FCN-LSTM
    架构，通过使用带有完全卷积视觉编码器的长短期记忆时间编码器来利用之前的车辆状态和当前的视觉观察。同时，这项工作还发布了伯克利深度驾驶视频数据集（BDDV），用于学习驾驶模型，并使用人类驾驶行为作为训练的地面真相标签。因此，人类标注可能会很昂贵且耗时。此外，该策略被训练以模仿人类行为在特定场景下的表现，且很难涵盖所有实际场景。
- en: By contrast, a full end-to-end reinforcement learning approaches learn policy
    in a trail-and-error way, and would not require such supervision. As presented
    in Sections II and III, deep RL algorithms have been widely employed as independent
    motion planning or control modules for autonomous vehicles. If extended, they
    can be also trained together with convolutional layers to constitute an end-to-end
    approach via joint optimization. For instance, using a single monocular image
    as input, the actor-critic algorithm was adopted in [[36](#bib.bib36)] to learn
    a policy for lane following in a handful of training episodes, which is trained
    to maximize the reward of distance that an agent can travel before intervention
    by a safety driver. Similarly, an asynchronous actor critic (A3C) framework established
    in [[37](#bib.bib37)] was used to learn vehicle control, and a thorough evaluation
    was conducted on unseen tracks and using legal speed limits. This work also demonstrated
    good domain adaptation capability when testing the proposed control commands on
    real videos.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，完全的端到端强化学习方法以试错的方式学习策略，不需要这种监督。如第二和第三节所述，深度强化学习算法已被广泛应用于自主车辆的独立运动规划或控制模块。如果扩展，它们也可以与卷积层一起训练，通过联合优化构成端到端方法。例如，使用单一的单目图像作为输入，在[[36](#bib.bib36)]中采用了演员-评论家算法来学习车道跟随策略，通过少量训练回合训练，以最大化在安全驾驶员介入之前，代理可以行驶的距离的奖励。类似地，[[37](#bib.bib37)]中建立的异步演员评论家（A3C）框架用于学习车辆控制，并对未见过的赛道和法律速度限制进行了全面评估。这项工作还展示了在实际视频上测试所提控制命令时的良好领域适应能力。
- en: Besides the aforementioned end-to-end on-road driving tasks that need to generalize
    to a larger domain and contend with moving objects such as cars and pedestrians,
    an end-to-end learning system for agile, off-road autonomous driving using only
    low-cost on-board sensors is presented in [[38](#bib.bib38)]. Compared with paved
    roads, the surface of our dirt tracks are constantly evolving and highly stochastic.
    As a result, to successfully perform high-speed driving in our task, high-frequency
    decision and execution of both steering and throttle commands are required. By
    imitating an optimal controller, a deep neural network control policy was successfully
    trained to map raw, high-dimensional observations to continuous steering and throttle
    commands.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前述的需要泛化到更大领域并应对移动物体（如汽车和行人）的端到端公路驾驶任务外，[[38](#bib.bib38)] 提出了一个仅使用低成本车载传感器的敏捷越野自主驾驶端到端学习系统。与铺装道路相比，我们的泥土赛道表面不断变化且高度随机。因此，为了成功进行高速驾驶，任务中需要对转向和油门命令进行高频决策和执行。通过模仿最优控制器，成功训练了一个深度神经网络控制策略，将原始的高维观察映射到连续的转向和油门命令上。
- en: IV-B Simulation
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 模拟
- en: Despite the remarkable advancement of end-to-end learning in real-world applications,
    there are still many challenges that hinders their on-road deployment in full
    autonomy. Specifically, many of these challenges arise from the fact that rarely
    are these vehicles trained or tested at all possible scenarios (including corner
    cases). However, manually creating these cases and collecting data in the real
    world can be a process that is expensive and often impractical.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a very promising approach is to leverage simulation to gain experience
    in these situations. In simulation, the cost of putting an obstacle on the road
    or simulating a car crash is negligible, but it still offers valuable experiences,
    whilst in a real-world scenario, such unexpected events can lead to severe financial
    losses and even threaten the lives of pedestrians or people in neighboring vehicles.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e3d6a389e71bbb9e200def71da61c143.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Carla environments: Hard Rainy in Town 1 [[39](#bib.bib39)].'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: While simulation has long been an essential part of testing autonomous driving
    systems, such as Carcraft of Waymo [[40](#bib.bib40)] and Drive Constellation
    of NVIDIA [[41](#bib.bib41)], only recently has simulation been applied to building
    and training end-to-end self-driving vehicles on driving simulation platforms,
    including TORCS [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)], CARLA [[48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [39](#bib.bib39)], Unity [[52](#bib.bib52)],
    WRC6 [[53](#bib.bib53), [37](#bib.bib37)], and Vdrift [[54](#bib.bib54)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: As a highly portable multi platform car racing simulation, TORCS [[55](#bib.bib55)]
    can be used as an ordinary car racing game, but also as a research platform. To
    our knowledge, the first attempt to control vehicles on TORCS using end-to-end
    technique is presented in [[42](#bib.bib42)] for an RL competition [[56](#bib.bib56)]
    using vision from the driver’s perspective, which is later processed with CNN
    and RNN. Since then, many different deep RL algorithms have been applied to drive
    a vehicle end-to-end on this platform, such as deep deterministic policy gradient
    (DDPG) [[43](#bib.bib43), [46](#bib.bib46), [47](#bib.bib47)], SafeDAgger [[44](#bib.bib44)],
    and A3C [[45](#bib.bib45)], etc. Specifically, [[43](#bib.bib43)] is the original
    paper that proposed the DDPG algorithm to handle complex state and action spaces
    in continuous domain, which is extremely useful to generate continuous action
    spaces that can adapt to the complex real-world driving scenarios.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Both CARLA [[57](#bib.bib57)] and Unity [[58](#bib.bib58)] can be used to efficiently
    create more realistic simulation environments that are rich in sensory and physical
    complexity. Their major difference lies in CARLA being an explicit driving simulation
    while the basic Unity being a more generic engine, and does not describe a very
    specific realization. CARLA has been developed to support development, training,
    and validation of autonomous driving systems, which also provides an interface
    allowing an RL agent to control a vehicle and interact with a dynamic environment.
    An illustration of the CARLA environment with Hard Rainy in Town 1 is presented
    in Fig. [2](#S4.F2 "Figure 2 ‣ IV-B Simulation ‣ IV End-to-End Deep Reinforcement
    Learning for Autonomous Driving ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles"). Since a vehicle trained
    end-to-end to imitate an expert cannot be controlled at test time (i.e., cannot
    take a specific turn at an upcoming intersection), a condition imitation learning
    approach was proposed in [[48](#bib.bib48)] to enable the learned driving policy
    to behave as a chauffeur that handles sensorimotor coordination but also continues
    to respond to navigational commands. To alleviate the the inefficiency of exploring
    large continuous action spaces that often prohibits the use of classical RL in
    challenging real driving tasks, [[49](#bib.bib49)] proposed a controllable imitative
    RL based on DDPG to explore over a reasonably constrained action space guided
    by encoded experiences that imitate human demonstrations. Extensive experiments
    on CARLA demonstrate its superior performance in terms of the percentage of successfully
    completed episodes and good generalization capability in unseen environments.
    Similarly, to achieve a better robustness of the agent learning strategies when
    acting in complex and unstable environments, an advantage actor-critic algorithm
    was implemented in [[39](#bib.bib39)] with multi-step returns.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: CARLA [[57](#bib.bib57)] 和 Unity [[58](#bib.bib58)] 都可以用来高效地创建更具现实感的模拟环境，这些环境在感知和物理复杂性方面都很丰富。它们的主要区别在于，CARLA
    是一个显式的驾驶模拟器，而基本的 Unity 是一个更通用的引擎，并没有描述一个非常具体的实现。CARLA 已经开发以支持自动驾驶系统的开发、训练和验证，它还提供了一个接口，允许
    RL 代理控制车辆并与动态环境互动。图 [2](#S4.F2 "Figure 2 ‣ IV-B Simulation ‣ IV End-to-End Deep
    Reinforcement Learning for Autonomous Driving ‣ A Survey of Deep Reinforcement
    Learning Algorithms for Motion Planning and Control of Autonomous Vehicles") 展示了带有强降雨的
    CARLA 环境。由于训练的车辆在测试时无法控制（即，无法在即将到来的交叉路口做出特定转弯），在 [[48](#bib.bib48)] 中提出了一种条件模仿学习方法，使得学习到的驾驶策略可以像司机一样处理感觉运动协调，同时继续响应导航命令。为了缓解探索大型连续动作空间的低效性，这种低效性通常阻碍了经典
    RL 在具有挑战性的实际驾驶任务中的使用，[[49](#bib.bib49)] 提出了基于 DDPG 的可控模仿 RL，通过模仿人类演示的编码经验在合理约束的动作空间内进行探索。大量的
    CARLA 实验表明，其在成功完成任务的百分比和未见环境中的良好泛化能力方面表现优越。类似地，为了提高代理在复杂和不稳定环境中的学习策略的鲁棒性，[[39](#bib.bib39)]
    中实现了一种带有多步回报的优势演员-评论员算法。
- en: '![Refer to caption](img/104869a258d9372a656bc094f246e108.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/104869a258d9372a656bc094f246e108.png)'
- en: 'Figure 3: An autonomous driving model using combined deep imitation learning
    and model-based reinforcement learning [[51](#bib.bib51)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 一个结合深度模仿学习和基于模型的强化学习的自动驾驶模型 [[51](#bib.bib51)]。'
- en: Another innovative deep RL algorithm that is worth mentioning is introduced
    by Rhinehart et al. in [[50](#bib.bib50)] and [[51](#bib.bib51)]. Specifically,
    the proposed imitative model, as depicted in Fig. [3](#S4.F3 "Figure 3 ‣ IV-B
    Simulation ‣ IV End-to-End Deep Reinforcement Learning for Autonomous Driving
    ‣ A Survey of Deep Reinforcement Learning Algorithms for Motion Planning and Control
    of Autonomous Vehicles"), combines imitation learning with model-based RL to develop
    probabilistic models to overcome the difficulty of specifying appropriate reward
    functions that are crucial to evoke desirable behaviors. Relying on LiDAR data
    inputs, the effectiveness of the proposed imitative model to predict expert-like
    vehicle trajectories is validated using CARLA, but without including other vehicles
    and pedestrians.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A Summary of Different Deep RL Formulations on the Motion Planning
    of Autonomous Vehicles.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Algorithm | State $s_{t}\in S$ | Action $a_{t}\in A$ | Reward $R$ / Loss
    $\ell$ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| Motion Planning (Pipeline) |  |  |  |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| Hoel et al. (2018) [[9](#bib.bib9)] | DQN | Vehicle & relative states | $A=\{\mathrm{Lane~{}change,Acc.,Brake}\}$
    | $R\{\mathrm{+:Efficiency,Comfort,Safety}\}$ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| Isele et al. (2018) [[10](#bib.bib10)] | Classical RL | Local states | $A=\{\mathrm{Safe~{}driving~{}decision}\}$
    | $R\{\mathrm{+:Safety}\}$ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| Bansal et al. (2018) [[14](#bib.bib14)] | Imitation learning | Traffic light/dynamic
    object states | $A=\{\mathrm{Driving~{}trajectory}\}$ | 9 training losses |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. (2019) [[11](#bib.bib11)] | DQN | Vehicle states | $A=\{\mathrm{Lane~{}change~{}decision}\}$
    | $R\{\mathrm{+:Speed;-:Collision,Invalid~{}Decision}\}$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| Mirchevska et al. (2018) [[12](#bib.bib12)] | DQN | Vehicle & relative states
    | $A=\{\mathrm{Lane~{}change~{}decision}\}$ | $R\{\mathrm{+:Speed}\}$ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| Ferdowsi et al. (2018) [[59](#bib.bib59)] | Adversarial RL | Vehicle states
    | $A=\{\mathrm{Optimal~{}safe~{}action}\}$ | $R\{\mathrm{+:Safety(distance)}\}$
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| Ye et al. (2019) [[15](#bib.bib15)] | DDPG | Vehicle & relative states |
    $A=\{\mathrm{Lane~{}change~{}decision}\}$ | $R\{\mathrm{+:Efficiency,Comfort,Safety}\}$
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Shi et al. (2019) [[13](#bib.bib13)] | DQN | Vehicle relative states | $A=\{\mathrm{Lane~{}change~{}decision}\}$
    | Hand-Crafted $R(s_{t},a_{t})$ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| Nishi et al. (2019) [[60](#bib.bib60)] | MPDM & pAC | Vehicle relative states
    | $A=\{\mathrm{Lane~{}merging~{}trajectory}\}$ | Hand-Crafted $R(s_{t},a_{t})$
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| You et al. (2019) [[61](#bib.bib61)] | Deep inverse RL | Grid-form states
    | $A=\{\mathrm{Optimal~{}driving~{}strategy}\}$ | Hand-Crafted $R(s_{t},a_{t})$
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| Bouton et al. (2019) [[62](#bib.bib62)] | Q-learning | Vehicle & agent states
    | $A=\{\mathrm{Strategic~{}maneuvers}\}$ | $R\{\mathrm{+:Goal,Efficiency;-:Collision}\}$
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| Ye et al. (2020) [[16](#bib.bib16)] | PPO | Vehicle & relative states | $A=\{\mathrm{Lane~{}change~{}decision,Car~{}following}\}$
    | $R\{\mathrm{+:Efficiency,Comfort,Safety}\}$ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| End-to-End |  |  |  |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| LeCun et al. (2005) [[33](#bib.bib33)] | Imitation learning | Camera image
    | $A=\{a&#124;a\in[-\max Left,+\max Right]\}$ | $\ell^{2}~{}\mathrm{loss}$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| Lillicrap et al. (2015) [[43](#bib.bib43)] | DDPG | Simulator image | $A=\{\mathrm{Steering,Acc.,Brake}\}$
    | $R\{\mathrm{+1:Direction;-1:Collision}\}$ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| Bojarski et al. (2016) [[34](#bib.bib34)] | Imitation learning | Camera image
    | $A=\{1/r&#124;r\in\mathrm{\{turning~{}radius\}}\}$ | $\ell^{2}~{}\mathrm{loss}$
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| Vitelli et al. (2016) [[54](#bib.bib54)] | DQN | Images and estimated states
    | $A=\{\mathrm{Steering,Acc.,Brake}\}$ | Hand-Crafted $R(s_{t},a_{t})$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. (2016) [[35](#bib.bib35)] | FCN-LSTM | Visual and sensor states
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '&#124; Disc. $A=\{\text{Straight, Stop, Left, Right}\}$ &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Continuous $A=\left\{\vec{v}&#124;\vec{v}\in\mathbb{R}^{2}\right\}$
    &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '| Cross entropy loss |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. (2016) [[44](#bib.bib44)] | SafeDAgger | Camera image | $A=\{\mathrm{Steering,Acc.,Brake}\}$
    | $R\{\mathrm{+1:No~{}crash}\}$ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| Perot et al. (2017) [[53](#bib.bib53)] | A3C | Game states | $A=\{\mathrm{Steering,Acc.,Brake}\}$
    | $R=v(\cos\theta-d)$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| Codevilla et al. (2017) [[48](#bib.bib48)] | Imitation learning | Images
    & driver internal states | $A=\{\mathrm{Steering,Acc.}\}$ | $\ell^{2}~{}\mathrm{loss}$
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| Pan, Cheng et al. (2017) [[38](#bib.bib38)] | Imitation learning | Camera
    image & vehicle speed | $A=\{\mathrm{Steering,Throttle}\}$ | Steering/Throttle
    loss |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| Pan, You et al. (2017) [[45](#bib.bib45)] | A3C | Virtual world images |
    $A=\{\mathrm{Steering,Acc.,Brake}\}$ | $R=\{+v(\cos\alpha-d),-\mathrm{Collision}\}$
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. (2018) [[46](#bib.bib46)] | DDPG | LiDAR sensor & camera image
    | $A=\{\mathrm{Steering,Acc.,Brake}\}$ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '&#124; $R_{t}=V_{x}\cos(\theta)-\alpha V_{x}\sin(\theta)$ &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $~{}~{}~{}~{}~{}~{}-\gamma&#124;Pos&#124;-\beta V_{x}&#124;Pos&#124;$
    &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '| Kendall et al. (2018) [[36](#bib.bib36)] | DDPG | Camera image | $A=\{\mathrm{Steering,Speed}\}$
    | Distance travelled without infraction |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| Jaritz et al. (2018) [[37](#bib.bib37)] | A3C | Input image | $A=\{\mathrm{Steering,Acc.,Brake,Hand~{}brake}\}$
    | $R\{\mathrm{+1:On~{}track;In~{}lane}\}$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| Klose et al. (2018) [[52](#bib.bib52)] | DQN | Raw sensor input | $A=\{\mathrm{Acc.,Brake}\}$
    | $R=\sum_{i=1}^{3}\exp\left(-0.5\cdot\left[{x_{t}^{i}}/{\theta_{i}}\right)^{2}\right]$
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. (2018) [[49](#bib.bib49)] | Imitative RL | Human driving videos
    | $A=\{\mathrm{Steering,Acc.,Brake}\}$ | $R\{\mathrm{+:Speed;-1:Collision,Steering}\}$
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| Rhinehart et al. (2018) [[50](#bib.bib50)] | Imitative model | LiDAR image
    | $A=\{\mathrm{Driving~{}way~{}points}\}$ | Probabilistic inference objectives
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| Min et al. (2019) [[63](#bib.bib63)] | DDPG | Raw sensor input | $A=\{\mathrm{Lane~{}change,Acc.,Brake}\}$
    | $R\{\mathrm{+1:Speed;-1:Collision}\}$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| Chhor et al. (2019) [[47](#bib.bib47)] | DDPG | Raw sensor input | $A=\{\mathrm{Steering,Acc.,Brake,Hand~{}brake}\}$
    | $R=V\cos\theta-V\sin\theta-V&#124;trackPos&#124;$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Chhor 等（2019）[[47](#bib.bib47)] | DDPG | 原始传感器输入 | $A=\{\mathrm{Steering,Acc.,Brake,Hand~{}brake}\}$
    | $R=V\cos\theta-V\sin\theta-V&#124;trackPos&#124;$ |'
- en: '| Jaafra et al. (2019) [[39](#bib.bib39)] | A2C | Camera image | $A=\{\mathrm{Steering,Throttle,Break}\}$
    | $R\{\mathrm{+1:Closing~{}goals;-1:Not~{}in~{}lane}\}$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Jaafra 等（2019）[[39](#bib.bib39)] | A2C | 摄像头图像 | $A=\{\mathrm{Steering,Throttle,Break}\}$
    | $R\{\mathrm{+1:Closing~{}goals;-1:Not~{}in~{}lane}\}$ |'
- en: '| Rhinehart et al. (2019) [[51](#bib.bib51)] | Imitative model | LiDAR image
    | $A=\{\mathrm{Driving~{}way~{}points}\}$ | Probabilistic inference objectives
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Rhinehart 等（2019）[[51](#bib.bib51)] | 模仿模型 | LiDAR 图像 | $A=\{\mathrm{Driving~{}way~{}points}\}$
    | 概率推断目标 |'
- en: IV-C The Sim-to-Real Approach
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C Sim-to-Real 方法
- en: By leveraging the simulated driving scenarios and experiences obtained via simulation,
    the sim-to-real approach is a good alternative to train an end-to-end driving
    policy without using real data, as shown in Fig. [4](#S4.F4 "Figure 4 ‣ IV-C The
    Sim-to-Real Approach ‣ IV End-to-End Deep Reinforcement Learning for Autonomous
    Driving ‣ A Survey of Deep Reinforcement Learning Algorithms for Motion Planning
    and Control of Autonomous Vehicles"). This field has received a lot of attention
    during the recent years, and it has been shown in [[64](#bib.bib64)] that just
    by randomizing the simulator very carefully, a policy can be trained to fly drones
    indoors using the simulated data without having to do very careful system identification.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用模拟的驾驶场景和通过模拟获得的经验，sim-to-real 方法是训练端到端驾驶策略而无需使用真实数据的一个良好替代方案，如图 [4](#S4.F4
    "Figure 4 ‣ IV-C The Sim-to-Real Approach ‣ IV End-to-End Deep Reinforcement Learning
    for Autonomous Driving ‣ A Survey of Deep Reinforcement Learning Algorithms for
    Motion Planning and Control of Autonomous Vehicles") 所示。近年来，该领域引起了大量关注，并且[[64](#bib.bib64)]中显示，通过非常仔细地随机化模拟器，政策可以通过模拟数据训练出能够在室内飞行无人机的策略，而不需要进行非常仔细的系统识别。
- en: Specifically, the simulator can facilitate the training of deep neural networks
    by generating abundant labeled data in many corner cases to extract task-relevant
    features and acquire good state representations, and the learned knowledge is
    expected to promote faster learning and better performance in real world scenarios.
    Therefore, the sim-to-real transfer is an important area of research that can
    adapt the learned knowledge from vehicle-traffic simulations to the real-world
    environment for decision making, planning and control.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，模拟器可以通过生成大量标记数据来促进深度神经网络的训练，以处理许多边角案例，从中提取任务相关特征并获得良好的状态表示，所学知识有望促进现实世界场景中的更快学习和更好性能。因此，sim-to-real
    转移是一个重要的研究领域，可以将从车辆交通模拟中获得的知识适应于现实世界环境，以进行决策、规划和控制。
- en: However, driving autonomously in the urban environment consists of multiple
    tasks that involve complex and uncertain driving behaviors and interactions with
    the surrounding traffic. Besides some typical tasks that are shared with highway
    driving, such as lane keeping, lane changing, overtaking, and car following, driving
    in the urban environment also includes taking left-turns, complying with road
    signs and traffic lights, keeping an eye on lower-speed pedestrians and bicyclists,
    etc. While each of these specific tasks can be separately modeled in the simulator
    to train the autonomous driving policy with an outstanding performance, the knowledge
    transfer from the simulator to the real-world scenario would be more challenging
    due to the large number and complexity of tasks that further intensifies the difference
    between the source domain (simulator) and the target domain (real-world). Moreover,
    there are other technical challenges such as real-world visual signal noises,
    and the training environment in a car driving simulator is often significantly
    different from real-world driving in terms of their visual appearance [[45](#bib.bib45)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在城市环境中自主驾驶涉及多个任务，这些任务包含复杂且不确定的驾驶行为以及与周围交通的互动。除了与高速公路驾驶共享的一些典型任务，如保持车道、变换车道、超车和跟车外，城市环境驾驶还包括左转、遵守交通标志和信号灯、留意低速行驶的行人和骑自行车的人等。虽然这些具体任务中的每一个都可以在模拟器中单独建模，以训练出具有优异性能的自主驾驶策略，但由于任务的数量和复杂性，模拟器与现实世界场景之间的差异进一步加剧，知识迁移将更加具有挑战性。此外，还有其他技术挑战，例如现实世界中的视觉信号噪声，以及汽车驾驶模拟器中的训练环境在视觉外观上通常与现实世界驾驶大相径庭[[45](#bib.bib45)]。
- en: Due to the aforementioned challenges, while many research work applied some
    variant of deep RL algorithms on the simulation platform, only a handful of them
    attempted to transfer the knowledge learned from the simulator to real-world applications,
    i.e., [[37](#bib.bib37)]. In fact, [[37](#bib.bib37)] claimed itself to be “the
    first time of a deep RL driving”, which is trained using the simulator, “is shown
    working on real images”, and foresees simulation based RL can be used as initialization
    strategy for networks used in real applications. However, it is also reported
    in [[65](#bib.bib65)] that “end-to-end models trained solely in CARLA were unable
    to transfer to the real world”, so some domain randomization and domain adaptation
    methods are needed for bridging the sim-to-real gap in simulators.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述挑战，虽然许多研究工作在模拟平台上应用了某种变体的深度 RL 算法，但只有少数几项尝试将从模拟器中学到的知识转移到现实应用中，即 [[37](#bib.bib37)]。实际上，[[37](#bib.bib37)]
    自称为“深度 RL 驾驶首次使用模拟器训练并在真实图像上展示”，并预见到基于模拟的 RL 可以作为实际应用中网络的初始化策略。然而，[[65](#bib.bib65)]
    报告称“仅在 CARLA 中训练的端到端模型无法转移到现实世界”，因此需要一些领域随机化和领域适应方法来弥合模拟与现实之间的差距。
- en: '![Refer to caption](img/e9595cb56ad4b4040bca7d54e6d5f665.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9595cb56ad4b4040bca7d54e6d5f665.png)'
- en: 'Figure 4: Training and deployment of policies from Sim-to-Real transfer [[65](#bib.bib65)].'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：从 Sim-to-Real 转移中的策略训练和部署 [[65](#bib.bib65)]。
- en: IV-D Summary
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 总结
- en: For the purpose of providing a comprehensive and intuitive comparison among
    different deep RL formulations on the motion planning and control of autonomous
    vehicles, TABLE [I](#S4.T1 "TABLE I ‣ IV-B Simulation ‣ IV End-to-End Deep Reinforcement
    Learning for Autonomous Driving ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles") summarizes the specific
    algorithm, the state space, the action space, and the specific reward/loss design
    of selective papers included in this review.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供不同深度 RL 公式在自主车辆运动规划和控制方面的全面且直观的比较，TABLE [I](#S4.T1 "TABLE I ‣ IV-B Simulation
    ‣ IV End-to-End Deep Reinforcement Learning for Autonomous Driving ‣ A Survey
    of Deep Reinforcement Learning Algorithms for Motion Planning and Control of Autonomous
    Vehicles") 总结了本次综述中所包含的特定算法、状态空间、动作空间和具体的奖励/损失设计。
- en: V Challenges and Future Work Directions
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 挑战和未来工作方向
- en: V-A Challenges
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 挑战
- en: V-A1 The Pipeline Approach
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 管道方法
- en: The choice of hand-crafted abstractions (features) for each module can limit
    the performance of the entire system. Once designed, these abstractions will have
    limited capacity to improve, and those parts that do not improve with data will
    eventually become the bottleneck. For instance, if the perception system becomes
    better, but the planner doesn’t get any better to utilize those benefits, eventually
    the planner will be the bottleneck. Ultimately we don’t actually know how to accurately
    construct the correct abstractions for the real world, and eventually these will
    get us into trouble.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个模块选择手工制作的抽象（特征）可能会限制整个系统的性能。一旦设计完成，这些抽象的改进能力会有限，那些无法随着数据改进的部分最终会成为瓶颈。例如，如果感知系统变得更好，但规划系统没有相应改进来利用这些好处，最终规划系统将成为瓶颈。*最终*，我们实际上并不知道如何准确地构建适合现实世界的正确抽象，*最终*这些抽象会给我们带来麻烦。
- en: One potential approach to tackle this challenge is to train all those layers
    of abstractions end-to-end, which means that the abstractions were optimally adapted
    for the task that needs to be solved.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的方法是对所有这些抽象层进行端到端训练，这意味着这些抽象被最优地适应于需要解决的任务。
- en: V-A2 The Sim-to-Real Approach
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 Sim-to-Real 方法
- en: Generally speaking, training with data from only the human driver is not adequate,
    and collecting a sufficient amount of data from every possible driving condition
    can be extremely expensive and dangerous. While the sim-to-real approach can help
    people get away with no real-world data at all, this approach might not be sufficient
    to solve the problem, even though it is still an excellent way to get the network
    and parameters initialized. This is because instead of designing each of these
    pipeline modules by hand, we’ll still have to design our simulator by hand. In
    the end, the simulator will become the bottleneck due to challenges for developing
    a sufficiently realistic simulator with diverse environments [[66](#bib.bib66)],
    including the modeling of any interacting traffic participant with realistic dynamics.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，仅用人类驾驶员的数据进行训练是不够的，收集每一种可能驾驶条件下的足够数据可能非常昂贵且危险。虽然从模拟到现实的方法可以帮助人们完全不使用真实世界数据，这种方法可能仍然不足以解决问题，即使它仍然是初始化网络和参数的极好方式。这是因为我们不仅要手动设计这些管道模块，还需要手动设计我们的模拟器。最终，由于开发一个足够真实的具有多样环境的模拟器的挑战，模拟器将成为瓶颈[[66](#bib.bib66)]，包括对任何具有真实动态的交互交通参与者的建模。
- en: V-B Future Work Directions
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 未来工作方向
- en: Despite the exciting progress of applied deep RL algorithms in autonomous driving
    tasks, these approaches are mostly trained for one task at a time, and each new
    task requires training a new agent, which is data-inefficient and fails to exploit
    the learned properties of similar tasks. In contrast, humans have the ability
    to not only learn complex tasks, but they can also adapt rapidly to new or evolving
    situations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管应用深度强化学习算法在自动驾驶任务中取得了令人兴奋的进展，这些方法大多是为一个任务训练一次，每个新任务都需要训练一个新的代理，这在数据上效率低下，并且无法利用相似任务的学习特性。相比之下，人类不仅能够学习复杂任务，还能快速适应新的或不断变化的情况。
- en: Therefore, some methods have been proposed to increase the generalization of
    deep RL algorithms in a variety of driving tasks. On the one hand, randomizing
    the simulator environment has been applied to generalize the trained policies
    [[67](#bib.bib67)]. On the other hand, transferring the knowledge accumulated
    from past experience through continual learning and meta learning has recently
    been explored to facilitate the generalization in both reinforcement learning
    [[68](#bib.bib68)] and imitation learning [[69](#bib.bib69)]. The ability to continuously
    learn and adapt quickly is essential to achieving real-world automated driving,
    which motivates further studies to introduce transfer learning and meta learning
    concepts into deep RL in the realm of autonomous driving.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已经提出了一些方法来提高深度强化学习算法在各种驾驶任务中的泛化能力。一方面，已应用随机化模拟器环境来泛化训练策略[[67](#bib.bib67)]。另一方面，最近探索了通过持续学习和元学习转移过去经验积累的知识，以促进强化学习[[68](#bib.bib68)]和模仿学习[[69](#bib.bib69)]中的泛化。能够持续学习并迅速适应对实现现实世界自动驾驶至关重要，这激励了进一步研究将迁移学习和元学习概念引入自动驾驶领域的深度强化学习中。
- en: VI Conclusions
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this paper, a systematic review is conducted on the existing literature employing
    deep RL algorithms on motion planning and control of autonomous vehicles, which
    is a field that has spurred the interest of industry and academia over the past
    five years. Both pipeline and end-to-end approaches have been extensively discussed.
    It has been demonstrated that, despite the fact that deep RL algorithms require
    an extended period and a large dataset to train, they can effectively interact
    with the environment in a trial-and-error way and does not require explicit human
    labeling or supervision on each data sample, making them promising candidates
    to accomplish autonomous driving in real-world applications.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本文系统地回顾了现有文献中采用深度强化学习算法对自动驾驶车辆的运动规划和控制的研究，这是在过去五年中引起了业界和学术界关注的一个领域。管道和端到端的方法都已被广泛讨论。研究表明，尽管深度强化学习算法需要较长时间和大量数据集进行训练，但它们能够以试错的方式有效地与环境互动，并且不需要对每个数据样本进行显式的人类标注或监督，这使得它们成为在实际应用中实现自动驾驶的有希望的候选者。
- en: References
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] D. González, J. Pérez, V. Milanés, and F. Nashashibi, “A review of motion
    planning techniques for automated vehicles,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 17, no. 4, pp. 1135–1145, 2015.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] D. González, J. Pérez, V. Milanés, 和 F. Nashashibi, “自动化车辆的运动规划技术综述，” *IEEE智能交通系统汇刊*,
    第17卷，第4期，第1135-1145页，2015年。'
- en: '[2] B. Paden, M. Čáp, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of
    motion planning and control techniques for self-driving urban vehicles,” *IEEE
    Transactions on intelligent vehicles*, vol. 1, no. 1, pp. 33–55, 2016.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and decision-making
    for autonomous vehicles,” *Annual Review of Control, Robotics, and Autonomous
    Systems*, 2018.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, “A survey of autonomous
    driving: common practices and emerging technologies,” *arXiv preprint arXiv:1906.05113*,
    2019.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] V. Talpaert, I. Sobh, B. R. Kiran, P. Mannion, S. Yogamani, A. El-Sallab,
    and P. Perez, “Exploring applications of deep reinforcement learning for real-world
    autonomous driving systems,” *arXiv preprint arXiv:1901.01536*, 2019.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani,
    and P. Pérez, “Deep reinforcement learning for autonomous driving: A survey,”
    *arXiv preprint arXiv:2002.00444*, 2020.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. Aradi, “Survey of deep reinforcement learning for motion planning of
    autonomous vehicles,” *IEEE Transactions on Intelligent Transportation Systems*,
    2020.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Levine, “Imitation, prediction, and model-based reinforcement learning
    for autonomous driving,” [Online]. Available: [https://slideslive.com/38917941/imitation-prediction-and-modelbased-reinforcement-learning-for-autonomous-driving](https://slideslive.com/38917941/imitation-prediction-and-modelbased-reinforcement-learning-for-autonomous-driving),
    Accessed: Oct. 2019.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] C.-J. Hoel, K. Wolff, and L. Laine, “Automated speed and lane change decision
    making using deep reinforcement learning,” in *2018 21st International Conference
    on Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 2148–2155.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] D. Isele, A. Nakhaei, and K. Fujimura, “Safe reinforcement learning on
    autonomous vehicles,” in *2018 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*, Oct 2018, pp. 1–6.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Wang, Q. Zhang, D. Zhao, and Y. Chen, “Lane change decision-making
    through deep reinforcement learning with rule-based constraints,” in *2019 International
    Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2019, pp. 1–6.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] B. Mirchevska, C. Pek, M. Werling, M. Althoff, and J. Boedecker, “High-level
    decision making for safe and reasonable autonomous lane changing using reinforcement
    learning,” in *2018 21st International Conference on Intelligent Transportation
    Systems (ITSC)*.   IEEE, 2018, pp. 2156–2162.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] T. Shi, P. Wang, X. Cheng, and C.-Y. Chan, “Driving decision and control
    for autonomous lane change based on deep reinforcement learning,” *arXiv preprint
    arXiv:1904.10171*, 2019.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to drive
    by imitating the best and synthesizing the worst,” *arXiv preprint arXiv:1812.03079*,
    2018.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Ye, X. Zhang, and J. Sun, “Automated vehicle’s behavior decision making
    using deep reinforcement learning and high-fidelity simulation environment,” *Transportation
    Research Part C: Emerging Technologies*, vol. 107, pp. 155–170, 2019.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] F. Ye, X. Cheng, P. Wang, and C.-Y. Chan, “Automated lane change strategy
    using proximal policy optimization-based deep reinforcement learning,” *arXiv
    preprint arXiv:2002.02667*, 2020.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Mazumder, B. Liu, S. Wang, Y. Zhu, L. Liu, and J. Li, “Action permissibility
    in deep reinforcement learning and application to autonomous driving,” *KDD’18
    Deep Learning Day*, 2018.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Ma, K. Driggs-Campbell, and M. J. Kochenderfer, “Improved robustness
    and safety for autonomous vehicle control with adversarial reinforcement learning,”
    in *2018 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2018, pp. 1665–1671.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L. Sun, C. Peng, W. Zhan, and M. Tomizuka, “A fast integrated planning
    and control framework for autonomous driving via imitation learning,” in *ASME
    2018 Dynamic Systems and Control Conference*.   American Society of Mechanical
    Engineers Digital Collection, 2017.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] P. Wang, H. Li, and C.-Y. Chan, “Continuous control for automated lane
    change behavior based on deep deterministic policy gradient algorithm,” in *2019
    IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2019, pp. 1454–1460.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. Li, D. Zhao, Q. Zhang, and Y. Chen, “Reinforcement learning and deep
    learning based lateral control for autonomous driving [application notes],” *IEEE
    Computational Intelligence Magazine*, vol. 14, no. 2, pp. 83–98, 2019.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] O. Pietquin, F. Tango, and R. Aras, “Batch reinforcement learning for
    optimizing longitudinal driving assistance strategies,” in *2011 IEEE Symposium
    on Computational Intelligence in Vehicles and Transportation Systems (CIVTS) Proceedings*.   IEEE,
    2011, pp. 73–79.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Lefevre, A. Carvalho, and F. Borrelli, “A learning-based framework
    for velocity control in autonomous driving,” *IEEE Transactions on Automation
    Science and Engineering*, vol. 13, no. 1, pp. 32–42, 2015.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Schultz and V. Sokolov, “Deep reinforcement learning for dynamic urban
    transportation problems,” *arXiv preprint arXiv:1806.05310*, 2018.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] M. Zhu, X. Wang, and Y. Wang, “Human-like autonomous car-following model
    with deep reinforcement learning,” *Transportation research part C: emerging technologies*,
    vol. 97, pp. 348–368, 2018.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] G. Hartmann, Z. Shiller, and A. Azaria, “Deep reinforcement learning for
    time optimal velocity control using prior knowledge,” in *2019 IEEE 31st International
    Conference on Tools with Artificial Intelligence (ICTAI)*.   IEEE, 2019, pp. 186–193.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Lin, J. McPhee, and N. L. Azad, “Longitudinal dynamic versus kinematic
    models for car-following control using deep reinforcement learning,” in *2019
    IEEE Intelligent Transportation Systems Conference (ITSC)*.   IEEE, 2019, pp.
    1504–1510.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement learning
    for urban autonomous driving,” in *2019 IEEE Intelligent Transportation Systems
    Conference (ITSC)*.   IEEE, 2019, pp. 2765–2771.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” *arXiv preprint arXiv:1610.03295*, 2016.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] X. He, D. Xu, H. Zhao, M. Moze, F. Aioun, and F. Guillemard, “A human-like
    trajectory planning method by learning from naturalistic driving data,” in *2018
    IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2018, pp. 339–346.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] P. Wang, H. Li, and C.-Y. Chan, “Quadratic q-network for learning continuous
    control for autonomous vehicles,” *arXiv preprint arXiv:1912.00074*, 2019.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    in *Advances in neural information processing systems*, 1989, pp. 305–313.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Y. Lecun, E. Cosatto, J. Ben, U. Muller, and B. Flepp, “Dave: Autonomous
    off-road vehicle control using end-to-end learning,” *DARPA-IPTO Final Report*,
    2004.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *et al.*, “End to end learning for
    self-driving cars,” *arXiv preprint arXiv:1604.07316*, 2016.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-end learning of driving
    models from large-scale video datasets,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 2174–2182.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam,
    A. Bewley, and A. Shah, “Learning to drive in a day,” in *2019 International Conference
    on Robotics and Automation (ICRA)*, 2019, pp. 8248–8254.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Jaritz, R. de Charette, M. Toromanoff, E. Perot, and F. Nashashibi,
    “End-to-end race driving with deep reinforcement learning,” in *2018 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2018, pp. 2070–2075.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and B. Boots,
    “Learning deep neural network control policies for agile off-road autonomous driving,”
    in *The NIPS Deep Rienforcement Learning Symposium*, 2017.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Jaafra, J. L. Laurent, A. Deruyver, and M. S. Naceur, “Robust reinforcement
    learning for autonomous driving,” *APIA*, p. 52, 2019.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. C. Madrigal, “Inside waymo’s secret world for training self-driving
    cars,” [Online]. Available: [https://www.businessinsider.com/waymo-engineer-explains-why-testing-self-driving-cars-virtually-is-critical-2018-8](https://www.businessinsider.com/waymo-engineer-explains-why-testing-self-driving-cars-virtually-is-critical-2018-8),
    Accessed: Mar. 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] NVIDIA, “Nvidia drive constellation – virtual reality autonomous vehicle
    simulator,” [Online]. Available: [https://www.nvidia.com/en-us/self-driving-cars/drive-constellation/](https://www.nvidia.com/en-us/self-driving-cars/drive-constellation/),
    Accessed: Mar. 2020.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Koutník, G. Cuccu, J. Schmidhuber, and F. Gomez, “Evolving large-scale
    neural networks for vision-based reinforcement learning,” in *Proceedings of the
    15th annual conference on Genetic and evolutionary computation*, 2013, pp. 1061–1068.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] J. Zhang and K. Cho, “Query-efficient imitation learning for end-to-end
    autonomous driving,” *arXiv preprint arXiv:1605.06450*, 2016.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] X. Pan, Y. You, Z. Wang, and C. Lu, “Virtual to real reinforcement learning
    for autonomous driving,” *arXiv preprint arXiv:1704.03952*, 2017.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Wang, D. Jia, and X. Weng, “Deep reinforcement learning for autonomous
    driving,” *arXiv preprint arXiv:1811.11329*, 2018.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] G. Chhor, S. Pandey, and V. Patel, “Robust deep reinforcement learning
    for autonomous driving,” 2018.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] F. Codevilla, M. Miiller, A. López, V. Koltun, and A. Dosovitskiy, “End-to-end
    driving via conditional imitation learning,” in *2018 IEEE International Conference
    on Robotics and Automation (ICRA)*, 2018, pp. 1–9.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. Liang, T. Wang, L. Yang, and E. Xing, “Cirl: Controllable imitative
    reinforcement learning for vision-based self-driving,” in *Proceedings of the
    European Conference on Computer Vision (ECCV)*, 2018, pp. 584–599.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Rhinehart, R. McAllister, and S. Levine, “Deep imitative models for
    flexible inference, planning, and control,” *arXiv preprint arXiv:1810.06544*,
    2018.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] N. Rhinehart, R. McAllister, K. Kitani, and S. Levine, “Precog: Prediction
    conditioned on goals in visual multi-agent settings,” in *Proceedings of the IEEE
    International Conference on Computer Vision*, 2019, pp. 2821–2830.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] P. Klose and R. Mester, “Simulated autonomous driving in a realistic driving
    environment using deep reinforcement learning and a deterministic finite state
    machine,” in *Proceedings of the 2nd International Conference on Applications
    of Intelligent Systems*, 2019, pp. 1–6.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] E. Perot, M. Jaritz, M. Toromanoff, and R. De Charette, “End-to-end driving
    in a realistic racing game with deep reinforcement learning,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*,
    2017, pp. 3–4.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. Vitelli and A. Nayebi, “Carma: A deep reinforcement learning approach
    to autonomous driving,” Tech. rep. Stanford University, Tech. Rep., 2016.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] TORCS, “The open racing car simulator,” [Online]. Available: [http://torcs.sourceforge.net/](http://torcs.sourceforge.net/),
    Accessed: Mar. 2020.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] D. Loiacono, P. L. Lanzi, J. Togelius, E. Onieva, D. A. Pelta, M. V. Butz,
    T. D. Lönneker, L. Cardamone, D. Perez, Y. Sáez, M. Preuss, and J. Quadflieg,
    “The 2009 simulated car racing championship,” *IEEE Trans. Comput. Intell. AI
    in Games*, vol. 2, no. 2, pp. 131–147, June 2010.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA:
    An open urban driving simulator,” in *Proceedings of the 1st Annual Conference
    on Robot Learning*, 2017, pp. 1–16.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Unity, “Easily build autonomous vehicle training environments,” [Online].
    Available: [https://unity.com/solutions/automotive-transportation/autonomous-vehicle-training](https://unity.com/solutions/automotive-transportation/autonomous-vehicle-training),
    Accessed: Mar. 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, “Robust deep reinforcement
    learning for security and safety in autonomous vehicle systems,” in *2018 21st
    International Conference on Intelligent Transportation Systems (ITSC)*.   IEEE,
    2018, pp. 307–312.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. Nishi, P. Doshi, and D. Prokhorov, “Merging in congested freeway traffic
    using multipolicy decision making and passive actor-critic learning,” *IEEE Transactions
    on Intelligent Vehicles*, vol. 4, no. 2, pp. 287–297, 2019.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C. You, J. Lu, D. Filev, and P. Tsiotras, “Advanced planning for autonomous
    vehicles using reinforcement learning and deep inverse reinforcement learning,”
    *Robotics and Autonomous Systems*, vol. 114, pp. 1–18, 2019.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] M. Bouton, J. Karlsson, A. Nakhaei, K. Fujimura, M. J. Kochenderfer, and
    J. Tumova, “Reinforcement learning with probabilistic guarantees for autonomous
    driving,” *arXiv preprint arXiv:1904.07189*, 2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] K. Min, H. Kim, and K. Huh, “Deep distributional reinforcement learning
    based high-level driving policy determination,” *IEEE Transactions on Intelligent
    Vehicles*, vol. 4, no. 3, pp. 416–424, Sep. 2019.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] F. Sadeghi and S. Levine, “Cad2rl: Real single-image flight without a
    single real image,” *arXiv preprint arXiv:1611.04201*, 2016.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Amini, I. Gilitschenski, J. Phillips, J. Moseyko, R. Banerjee, S. Karaman,
    and D. Rus, “Learning robust control policies for end-to-end autonomous driving
    from data-driven simulation,” *IEEE Robotics and Automation Letters*, vol. 5,
    no. 2, pp. 1143–1150, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in deep
    reinforcement learning for robotics: a survey,” in *2020 IEEE Symposium Series
    on Computational Intelligence (SSCI)*, 2020, pp. 737–744.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
    randomization for transferring deep neural networks from simulation to the real
    world,” in *2017 IEEE/RSJ international conference on intelligent robots and systems
    (IROS)*.   IEEE, 2017, pp. 23–30.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] F. Ye, P. Wang, C.-Y. Chan, and J. Zhang, “Meta reinforcement learning-based
    lane change strategy for autonomous vehicles,” *arXiv preprint arXiv:2008.12451*,
    2020.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. E. Sallab, M. Saeed, O. A. Tawab, and M. Abdou, “Meta learning framework
    for automated driving,” 2017.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
