- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:54:55'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2105.14218] A Survey of Deep Reinforcement Learning Algorithms for Motion
    Planning and Control of Autonomous Vehicles'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2105.14218](https://ar5iv.labs.arxiv.org/html/2105.14218)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Reinforcement Learning Algorithms for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Motion Planning and Control of Autonomous Vehicles
  prefs: []
  type: TYPE_NORMAL
- en: Fei Ye¹, Shen Zhang², Pin Wang³, and Ching-Yao Chan³ ¹ F. Ye is with TuSimple
    Inc., 9191 Towne Centre Dr. Ste 600, San Diego, CA 92122, USA. fei.ye@tusimple.ai.²
    S. Zhang is with Georgia Institute of Technology, Atlanta, GA 30332, USA. shenzhang@gatech.edu.³
    P. Wang and C. Chan are with California PATH, University of California, Berkeley,
    Richmond, CA 94804, USA. {pin_wang, cychan}@berkeley.edu.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this survey, we systematically summarize the current literature on studies
    that apply reinforcement learning (RL) to the motion planning and control of autonomous
    vehicles. Many existing contributions can be attributed to the pipeline approach,
    which consists of many hand-crafted modules, each with a functionality selected
    for the ease of human interpretation. However, this approach does not automatically
    guarantee maximal performance due to the lack of a system-level optimization.
    Therefore, this paper also presents a growing trend of work that falls into the
    end-to-end approach, which typically offers better performance and smaller system
    scales. However, their performance also suffers from the lack of expert data and
    generalization issues. Finally, the remaining challenges applying deep RL algorithms
    on autonomous driving are summarized, and future research directions are also
    presented to tackle these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automated and semi-automated vehicles are gaining popularity in assisting our
    daily transportation. There is a considerable amount of studies in the past decade
    focusing on autonomous driving applications [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]. Specifically, a large number
    of research activities based on deep learning have been conducted for advanced
    driving assistance systems (ADAS) and automated driving applications, aiming to
    automate as much of the driving task as possible. Supervised learning approaches
    rely heavily on large amounts of labeled data to be able to generalize and it
    is basically trained on each task in isolation. However, obtaining a big amount
    of data for each individual task in autonomous driving is costly and time-consuming.
    Moreover, it requires massive human labor to label such data and still may not
    cover all the complex situations in the real-world driving.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, reinforcement learning (RL) algorithms have been extensively
    applied to vehicle decision making and control problems [[5](#bib.bib5), [6](#bib.bib6)].
    Specifically, RL is able to learn in a trial-and-error way and does not require
    explicit human labeling or supervision on each data sample. Instead, it needs
    a well-defined reward function to receive reward signals in its learning process.
    Additionally, there is a wide variety of deep RL algorithms and high flexibility
    in the implementation level, such as the state space, the action space, and the
    reward function, etc.
  prefs: []
  type: TYPE_NORMAL
- en: In general, existing work applying deep RL to the motion planning and control
    of autonomous vehicles can be divided into the hierarchical (pipeline) approach
    and the end-to-end approach [[7](#bib.bib7)], as demonstrated in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles"). Specifically, the pipeline
    approach can be typically categorized into different modules such as perception,
    decision making, motion planning, low-level control, and so on. In this paper
    we’ll review and examine papers that apply deep RL algorithms to specifically
    accomplish functionalities of motion planning and vehicle control. Each of these
    modules is engineered manually, and the interfaces between the modules are typically
    implemented by hands. However, this modular distribution is obviously targeted
    for the convenience of human interpretation, rather than the highest attainable
    system performance. For example, if there is a pipeline system with parts that
    can improve with data, and with parts that they don’t, then those parts that do
    not improve with data will eventually become the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, more efforts have been devoted to the second approach enabled by
    deep RL, which is the end-to-end approach that can optimize all those modules
    of abstractions to map sensory input to control commands with the minimal number
    of processing steps. There are mainly two reasons why we might want to apply end-to-end
    techniques to autonomous driving: 1) better performance; and 2) smaller system
    scales [[8](#bib.bib8)]. Better performance will result because the internal components
    can be self-optimized to maximize overall system performance, instead of optimizing
    human-selected intermediate criteria. Smaller networks are possible because the
    system learns to solve problems with minimal processing steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd29d3f5a41a484d56dde93bae649c90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An illustration of the pipeline and the end-to-end approach for motion
    planning and control of autonomous vehicles, figure adapted from [[2](#bib.bib2)].'
  prefs: []
  type: TYPE_NORMAL
- en: In this context, this paper seeks to summarize existing work that explains how
    deep RL algorithms, when combined together with deep neural network representations,
    can generalize and perform automated vehicle decision making and control. The
    rest of the paper is organized as follows. In Sections II and III, we introduce
    how different deep RL algorithms can be leveraged to accomplish behavioral decision
    making, motion planning and control modules in the pipeline approach. Next, in
    Section IV, we take a deep dive into end-to-end learning methods for both real-world
    applications and the sim-to-real approach. Section V summarizes existing challenges
    and future work directions in applying deep RL to autonomous vehicles. Finally,
    a conclusion is provided in Section VI.
  prefs: []
  type: TYPE_NORMAL
- en: II Deep Reinforcement Learning for Decision Making and Motion Planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we focus on recent advances in behavioral decision-making and
    motion planning for autonomous vehicles based on deep RL. As shown in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles"), the typical pipelines
    of an autonomous driving system process a stream of observation from vehicle on-board
    sensors with high level routing plans to the executable control output such as
    steering angles, accelerations and braking actions. Typically, a hierarchical
    structure in the design of planning systems for autonomous driving is usually
    desired, since driving is naturally hierarchical as higher level decisions are
    made on discrete state transitions and lower level executions are performed in
    continuous state space. Behavior layer is a decision making system that determines
    the transition of discrete state of mid-level driving behaviors such as lane changing,
    car-following, turning left/right, etc. When the behavior decision is made, the
    motion planning system is responsible for providing a safe, comfortable, and dynamically
    feasible continuous trajectory to achieve the selected driving behavior from the
    decision making system.
  prefs: []
  type: TYPE_NORMAL
- en: Deep reinforcement learning has shown great success in the area of vehicle behavioral
    decision makings, especially in the highway scenarios and urban intersections.
    To reduce the sample complexity, some of the studies choose to adopt the mid-level
    inputs from the perception system processes and extract the vehicle state and
    relative states of surrounding vehicles as inputs. Hoel et al [[9](#bib.bib9)]
    trained a Deep Q-Network (DQN) in a simulation environment to issue driving behavioral
    commands (e.g. change lanes to the right/left, cruise on the current lane and
    etc.) and compared the different neural network structures’ effects on the agent
    performance. To migrate the concerns of safety performance of a trained RL agent,
    it is common to add rule-based safety constraints that can verify unsafe actions
    before they are executed. In [[10](#bib.bib10)], a prediction model combined with
    Deep Q-Network is proposed given the outputs from the perception system to label
    the unsafe behavioral decisions in unprotected turn scenarios. Some studies [[11](#bib.bib11),
    [12](#bib.bib12)] alternatively trained a lane change decision making system based
    on DQN for behavioral level decision-making and uses an underlying rule-based
    layer to verify the safety of a planned trajectory before it is executed by the
    vehicle control system. In [[13](#bib.bib13)], a hierarchical RL based architecture
    is presented to combine lane change decisions and motion planning together. Specifically,
    a deep Q-network (DQN) is trained to decide when to conduct the maneuver based
    on safety considerations, while a deep Q-learning framework with quadratic approximator
    is designed to complete the maneuver in longitudinal direction. On the other hand,
    some of the studies choose to learn from human demonstrations via imitation learning
    and introduce perturbation to discourage undesirable behavior [[14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: More recently, actor-critic policy-based RL methods are introduced in autonomous
    vehicle decision making and motion planning. Compared to value-based RL methods
    such as Deep Q-Networks that approximate the value function using neural networks
    in an off-policy way, the primary advantage of actor-critic policy-based RL method
    is that they can directly compute actions from the policy gradient rather than
    optimizing from the value function, while remaining stable during function approximations.
    On the other hand, the merit of the critic is to supply the actor with the knowledge
    of performance in low variance. Actor-Critic algorithm has been successfully applied
    in both discrete behavior decision makings and continuous motion planning tasks
    [[15](#bib.bib15), [16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: III Deep Reinforcement Learning for Vehicle Control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the pipeline approach, the perception layer such as CNN is typically used
    to process and recognize objects in digital images. Afterwards, deep RL algorithms
    can be applied to understand and learn intelligent actions and controls. Many
    papers are devoted to accomplishing low-level vehicle control with deep RL methods,
    such as lane keeping [[17](#bib.bib17), [18](#bib.bib18)], lateral control [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21)], longitudinal control [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)],
    or both [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)].
  prefs: []
  type: TYPE_NORMAL
- en: For lane-keeping, a DDPG model was implemented in continuous state and action
    space in [[17](#bib.bib17)] to guide its training and apply it to solve the lane
    keeping (steering control) problem in self-driving or autonomous driving. It is
    shown that the proposed method can help speed up RL training remarkably for the
    lane keeping task as compared to the RL algorithm without exploiting the state-action
    permissibility-based guidance and other baselines that employ constrained action
    space exploration strategies. To improve efficiency and reduce failures in autonomous
    vehicles, two different algorithms, namely the robust adversarial RL and neural
    fictitious self play are proposed in [[18](#bib.bib18)], and compares performance
    on lane keeping and lane changing scenarios. The results exhibit improved driving
    efficiency while effectively reducing collision rates compared to baseline control
    policies produced by traditional RL methods.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the vehicle lateral control, to address the high computational complexity
    of model predictive control (MPC) for real-time implementation, a fast integrated
    planning and control framework is proposed in [[19](#bib.bib19)] that combines
    a driving policy layer and an execution layer. Several example driving scenarios
    demonstrated that the performance of the policy layer can be improved quickly
    and continuously online. In [[20](#bib.bib20)], DDPG was implemented to formulate
    the lane change behavior with continuous action in a model-free dynamic driving
    environment, and the reward function takes the position deviation status and the
    maneuvering time into account. Eventually, the RL agent is trained to smoothly
    and stably change to the target lane with a success rate of 100% under diverse
    driving situations in simulation. In [[21](#bib.bib21)], a vision-based lateral
    control system was broken into a perception module and a control module, which
    is based on deep RL to make a control decision based on features coming from the
    perception module. The trained RL controller in visual TORCS outperforms the linear
    quadratic regulator (LQR) controller and model MPC controller on different tracks.
  prefs: []
  type: TYPE_NORMAL
- en: IV End-to-End Deep Reinforcement Learning for Autonomous Driving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous two sections summarize existing work that fall into the pipeline
    approach, which consists of many hand-crafted modules for the ease of human interpretation,
    such as perception, motion planning, decision making, low-level control, and so
    on. However, this pipeline approach does not guarantee maximum system performance
    if some parts of modules cannot improve with data, which will eventually become
    the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the end-to-end approach is expected to have better performance
    and smaller systems for autonomous driving. With end-to-end training, it is not
    possible to make a clean break among different modules of a system, such as which
    parts of the network mainly act as feature extractors and which ones serve as
    controllers. However, it is able to optimize all those modules of abstractions
    simultaneously with a reduced number of processing steps, which indicates that
    the abstractions will be optimally and automatically adapted for the task that
    needs to be solved, instead of optimizing human-selected intermediate criteria.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Joint Optimization in Real-World Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A very early attempt that successfully steered a car on public roads using the
    end-to-end approach can be found in Pomerleau’s pioneering work in 1989 [[32](#bib.bib32)],
    in which controlling an autonomous land vehicle was trained using a neural network
    system. In the early 2000’s, LeCun et al. built a small off-road robot that uses
    an end-to-end learning system to avoid obstacles solely from visual input [[33](#bib.bib33)].
    The robot named DAVE was trained on images sampled from human driving videos,
    paired with the corresponding steering command $1/r$. While DAVE demonstrated
    the potential of end-to-end learning, its performance was not reliable enough
    to provide a complete alternative to more modular methods of off-road driving,
    as its mean distance between crashes was around 20 meters in complex environments.
  prefs: []
  type: TYPE_NORMAL
- en: With the evolution of hardware computation capabilities and the advancement
    of modern deep learning algorithms, in 2016, researchers at NVIDIA proposed an
    end-to-end learning based on CNN that learns the entire processing pipeline needed
    to directly steer an automobile [[34](#bib.bib34)]. Training data was collected
    from less than a hundred hours of expert driving on a wide variety of roads, paired
    with time-synchronized steering commands generated by the human driver. The sampled
    images are fed into a CNN consisting of five convolutional layers with three fully
    connected layers, which then computes the proposed steering command. The weights
    of the CNN are optimized by minimizing the mean squared error between the proposed
    command and the benchmark command for that image. This milestone also empirically
    validated that CNNs can “learn the entire task of lane and road following without
    manual decomposition into road or lane marking detection, semantic abstraction,
    path planning, and control” [[34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: Direct deep supervised learning usually requires a large amount of data to learn
    a generic driving policy and the ground truth labeling for training. For example,
    [[35](#bib.bib35)] proposes a novel FCN-LSTM architecture to leverage both previous
    vehicle states and current visual observations using a long short-term memory
    temporal encoder with a fully convolutional visual encoder. Meanwhile, this work
    also released the Berkeley DeepDrive Video dataset (BDDV) for learning driving
    models and used the human driving behavior as the ground truth label for training.
    As such, human labeling can be costly and time-consuming. Furthermore, the policy
    is trained to mimic human behavior in a certain scenarios and it is hard to cover
    all the real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, a full end-to-end reinforcement learning approaches learn policy
    in a trail-and-error way, and would not require such supervision. As presented
    in Sections II and III, deep RL algorithms have been widely employed as independent
    motion planning or control modules for autonomous vehicles. If extended, they
    can be also trained together with convolutional layers to constitute an end-to-end
    approach via joint optimization. For instance, using a single monocular image
    as input, the actor-critic algorithm was adopted in [[36](#bib.bib36)] to learn
    a policy for lane following in a handful of training episodes, which is trained
    to maximize the reward of distance that an agent can travel before intervention
    by a safety driver. Similarly, an asynchronous actor critic (A3C) framework established
    in [[37](#bib.bib37)] was used to learn vehicle control, and a thorough evaluation
    was conducted on unseen tracks and using legal speed limits. This work also demonstrated
    good domain adaptation capability when testing the proposed control commands on
    real videos.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the aforementioned end-to-end on-road driving tasks that need to generalize
    to a larger domain and contend with moving objects such as cars and pedestrians,
    an end-to-end learning system for agile, off-road autonomous driving using only
    low-cost on-board sensors is presented in [[38](#bib.bib38)]. Compared with paved
    roads, the surface of our dirt tracks are constantly evolving and highly stochastic.
    As a result, to successfully perform high-speed driving in our task, high-frequency
    decision and execution of both steering and throttle commands are required. By
    imitating an optimal controller, a deep neural network control policy was successfully
    trained to map raw, high-dimensional observations to continuous steering and throttle
    commands.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Simulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the remarkable advancement of end-to-end learning in real-world applications,
    there are still many challenges that hinders their on-road deployment in full
    autonomy. Specifically, many of these challenges arise from the fact that rarely
    are these vehicles trained or tested at all possible scenarios (including corner
    cases). However, manually creating these cases and collecting data in the real
    world can be a process that is expensive and often impractical.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a very promising approach is to leverage simulation to gain experience
    in these situations. In simulation, the cost of putting an obstacle on the road
    or simulating a car crash is negligible, but it still offers valuable experiences,
    whilst in a real-world scenario, such unexpected events can lead to severe financial
    losses and even threaten the lives of pedestrians or people in neighboring vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e3d6a389e71bbb9e200def71da61c143.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Carla environments: Hard Rainy in Town 1 [[39](#bib.bib39)].'
  prefs: []
  type: TYPE_NORMAL
- en: While simulation has long been an essential part of testing autonomous driving
    systems, such as Carcraft of Waymo [[40](#bib.bib40)] and Drive Constellation
    of NVIDIA [[41](#bib.bib41)], only recently has simulation been applied to building
    and training end-to-end self-driving vehicles on driving simulation platforms,
    including TORCS [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)], CARLA [[48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [39](#bib.bib39)], Unity [[52](#bib.bib52)],
    WRC6 [[53](#bib.bib53), [37](#bib.bib37)], and Vdrift [[54](#bib.bib54)].
  prefs: []
  type: TYPE_NORMAL
- en: As a highly portable multi platform car racing simulation, TORCS [[55](#bib.bib55)]
    can be used as an ordinary car racing game, but also as a research platform. To
    our knowledge, the first attempt to control vehicles on TORCS using end-to-end
    technique is presented in [[42](#bib.bib42)] for an RL competition [[56](#bib.bib56)]
    using vision from the driver’s perspective, which is later processed with CNN
    and RNN. Since then, many different deep RL algorithms have been applied to drive
    a vehicle end-to-end on this platform, such as deep deterministic policy gradient
    (DDPG) [[43](#bib.bib43), [46](#bib.bib46), [47](#bib.bib47)], SafeDAgger [[44](#bib.bib44)],
    and A3C [[45](#bib.bib45)], etc. Specifically, [[43](#bib.bib43)] is the original
    paper that proposed the DDPG algorithm to handle complex state and action spaces
    in continuous domain, which is extremely useful to generate continuous action
    spaces that can adapt to the complex real-world driving scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Both CARLA [[57](#bib.bib57)] and Unity [[58](#bib.bib58)] can be used to efficiently
    create more realistic simulation environments that are rich in sensory and physical
    complexity. Their major difference lies in CARLA being an explicit driving simulation
    while the basic Unity being a more generic engine, and does not describe a very
    specific realization. CARLA has been developed to support development, training,
    and validation of autonomous driving systems, which also provides an interface
    allowing an RL agent to control a vehicle and interact with a dynamic environment.
    An illustration of the CARLA environment with Hard Rainy in Town 1 is presented
    in Fig. [2](#S4.F2 "Figure 2 ‣ IV-B Simulation ‣ IV End-to-End Deep Reinforcement
    Learning for Autonomous Driving ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles"). Since a vehicle trained
    end-to-end to imitate an expert cannot be controlled at test time (i.e., cannot
    take a specific turn at an upcoming intersection), a condition imitation learning
    approach was proposed in [[48](#bib.bib48)] to enable the learned driving policy
    to behave as a chauffeur that handles sensorimotor coordination but also continues
    to respond to navigational commands. To alleviate the the inefficiency of exploring
    large continuous action spaces that often prohibits the use of classical RL in
    challenging real driving tasks, [[49](#bib.bib49)] proposed a controllable imitative
    RL based on DDPG to explore over a reasonably constrained action space guided
    by encoded experiences that imitate human demonstrations. Extensive experiments
    on CARLA demonstrate its superior performance in terms of the percentage of successfully
    completed episodes and good generalization capability in unseen environments.
    Similarly, to achieve a better robustness of the agent learning strategies when
    acting in complex and unstable environments, an advantage actor-critic algorithm
    was implemented in [[39](#bib.bib39)] with multi-step returns.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/104869a258d9372a656bc094f246e108.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An autonomous driving model using combined deep imitation learning
    and model-based reinforcement learning [[51](#bib.bib51)].'
  prefs: []
  type: TYPE_NORMAL
- en: Another innovative deep RL algorithm that is worth mentioning is introduced
    by Rhinehart et al. in [[50](#bib.bib50)] and [[51](#bib.bib51)]. Specifically,
    the proposed imitative model, as depicted in Fig. [3](#S4.F3 "Figure 3 ‣ IV-B
    Simulation ‣ IV End-to-End Deep Reinforcement Learning for Autonomous Driving
    ‣ A Survey of Deep Reinforcement Learning Algorithms for Motion Planning and Control
    of Autonomous Vehicles"), combines imitation learning with model-based RL to develop
    probabilistic models to overcome the difficulty of specifying appropriate reward
    functions that are crucial to evoke desirable behaviors. Relying on LiDAR data
    inputs, the effectiveness of the proposed imitative model to predict expert-like
    vehicle trajectories is validated using CARLA, but without including other vehicles
    and pedestrians.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A Summary of Different Deep RL Formulations on the Motion Planning
    of Autonomous Vehicles.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Algorithm | State $s_{t}\in S$ | Action $a_{t}\in A$ | Reward $R$ / Loss
    $\ell$ |'
  prefs: []
  type: TYPE_TB
- en: '| Motion Planning (Pipeline) |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hoel et al. (2018) [[9](#bib.bib9)] | DQN | Vehicle & relative states | $A=\{\mathrm{Lane~{}change,Acc.,Brake}\}$
    | $R\{\mathrm{+:Efficiency,Comfort,Safety}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Isele et al. (2018) [[10](#bib.bib10)] | Classical RL | Local states | $A=\{\mathrm{Safe~{}driving~{}decision}\}$
    | $R\{\mathrm{+:Safety}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Bansal et al. (2018) [[14](#bib.bib14)] | Imitation learning | Traffic light/dynamic
    object states | $A=\{\mathrm{Driving~{}trajectory}\}$ | 9 training losses |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. (2019) [[11](#bib.bib11)] | DQN | Vehicle states | $A=\{\mathrm{Lane~{}change~{}decision}\}$
    | $R\{\mathrm{+:Speed;-:Collision,Invalid~{}Decision}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mirchevska et al. (2018) [[12](#bib.bib12)] | DQN | Vehicle & relative states
    | $A=\{\mathrm{Lane~{}change~{}decision}\}$ | $R\{\mathrm{+:Speed}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Ferdowsi et al. (2018) [[59](#bib.bib59)] | Adversarial RL | Vehicle states
    | $A=\{\mathrm{Optimal~{}safe~{}action}\}$ | $R\{\mathrm{+:Safety(distance)}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ye et al. (2019) [[15](#bib.bib15)] | DDPG | Vehicle & relative states |
    $A=\{\mathrm{Lane~{}change~{}decision}\}$ | $R\{\mathrm{+:Efficiency,Comfort,Safety}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Shi et al. (2019) [[13](#bib.bib13)] | DQN | Vehicle relative states | $A=\{\mathrm{Lane~{}change~{}decision}\}$
    | Hand-Crafted $R(s_{t},a_{t})$ |'
  prefs: []
  type: TYPE_TB
- en: '| Nishi et al. (2019) [[60](#bib.bib60)] | MPDM & pAC | Vehicle relative states
    | $A=\{\mathrm{Lane~{}merging~{}trajectory}\}$ | Hand-Crafted $R(s_{t},a_{t})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| You et al. (2019) [[61](#bib.bib61)] | Deep inverse RL | Grid-form states
    | $A=\{\mathrm{Optimal~{}driving~{}strategy}\}$ | Hand-Crafted $R(s_{t},a_{t})$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bouton et al. (2019) [[62](#bib.bib62)] | Q-learning | Vehicle & agent states
    | $A=\{\mathrm{Strategic~{}maneuvers}\}$ | $R\{\mathrm{+:Goal,Efficiency;-:Collision}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ye et al. (2020) [[16](#bib.bib16)] | PPO | Vehicle & relative states | $A=\{\mathrm{Lane~{}change~{}decision,Car~{}following}\}$
    | $R\{\mathrm{+:Efficiency,Comfort,Safety}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| End-to-End |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| LeCun et al. (2005) [[33](#bib.bib33)] | Imitation learning | Camera image
    | $A=\{a&#124;a\in[-\max Left,+\max Right]\}$ | $\ell^{2}~{}\mathrm{loss}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Lillicrap et al. (2015) [[43](#bib.bib43)] | DDPG | Simulator image | $A=\{\mathrm{Steering,Acc.,Brake}\}$
    | $R\{\mathrm{+1:Direction;-1:Collision}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Bojarski et al. (2016) [[34](#bib.bib34)] | Imitation learning | Camera image
    | $A=\{1/r&#124;r\in\mathrm{\{turning~{}radius\}}\}$ | $\ell^{2}~{}\mathrm{loss}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vitelli et al. (2016) [[54](#bib.bib54)] | DQN | Images and estimated states
    | $A=\{\mathrm{Steering,Acc.,Brake}\}$ | Hand-Crafted $R(s_{t},a_{t})$ |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. (2016) [[35](#bib.bib35)] | FCN-LSTM | Visual and sensor states
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Disc. $A=\{\text{Straight, Stop, Left, Right}\}$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Continuous $A=\left\{\vec{v}&#124;\vec{v}\in\mathbb{R}^{2}\right\}$
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cross entropy loss |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. (2016) [[44](#bib.bib44)] | SafeDAgger | Camera image | $A=\{\mathrm{Steering,Acc.,Brake}\}$
    | $R\{\mathrm{+1:No~{}crash}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Perot et al. (2017) [[53](#bib.bib53)] | A3C | Game states | $A=\{\mathrm{Steering,Acc.,Brake}\}$
    | $R=v(\cos\theta-d)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Codevilla et al. (2017) [[48](#bib.bib48)] | Imitation learning | Images
    & driver internal states | $A=\{\mathrm{Steering,Acc.}\}$ | $\ell^{2}~{}\mathrm{loss}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pan, Cheng et al. (2017) [[38](#bib.bib38)] | Imitation learning | Camera
    image & vehicle speed | $A=\{\mathrm{Steering,Throttle}\}$ | Steering/Throttle
    loss |'
  prefs: []
  type: TYPE_TB
- en: '| Pan, You et al. (2017) [[45](#bib.bib45)] | A3C | Virtual world images |
    $A=\{\mathrm{Steering,Acc.,Brake}\}$ | $R=\{+v(\cos\alpha-d),-\mathrm{Collision}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. (2018) [[46](#bib.bib46)] | DDPG | LiDAR sensor & camera image
    | $A=\{\mathrm{Steering,Acc.,Brake}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $R_{t}=V_{x}\cos(\theta)-\alpha V_{x}\sin(\theta)$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $~{}~{}~{}~{}~{}~{}-\gamma&#124;Pos&#124;-\beta V_{x}&#124;Pos&#124;$
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Kendall et al. (2018) [[36](#bib.bib36)] | DDPG | Camera image | $A=\{\mathrm{Steering,Speed}\}$
    | Distance travelled without infraction |'
  prefs: []
  type: TYPE_TB
- en: '| Jaritz et al. (2018) [[37](#bib.bib37)] | A3C | Input image | $A=\{\mathrm{Steering,Acc.,Brake,Hand~{}brake}\}$
    | $R\{\mathrm{+1:On~{}track;In~{}lane}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Klose et al. (2018) [[52](#bib.bib52)] | DQN | Raw sensor input | $A=\{\mathrm{Acc.,Brake}\}$
    | $R=\sum_{i=1}^{3}\exp\left(-0.5\cdot\left[{x_{t}^{i}}/{\theta_{i}}\right)^{2}\right]$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. (2018) [[49](#bib.bib49)] | Imitative RL | Human driving videos
    | $A=\{\mathrm{Steering,Acc.,Brake}\}$ | $R\{\mathrm{+:Speed;-1:Collision,Steering}\}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rhinehart et al. (2018) [[50](#bib.bib50)] | Imitative model | LiDAR image
    | $A=\{\mathrm{Driving~{}way~{}points}\}$ | Probabilistic inference objectives
    |'
  prefs: []
  type: TYPE_TB
- en: '| Min et al. (2019) [[63](#bib.bib63)] | DDPG | Raw sensor input | $A=\{\mathrm{Lane~{}change,Acc.,Brake}\}$
    | $R\{\mathrm{+1:Speed;-1:Collision}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Chhor et al. (2019) [[47](#bib.bib47)] | DDPG | Raw sensor input | $A=\{\mathrm{Steering,Acc.,Brake,Hand~{}brake}\}$
    | $R=V\cos\theta-V\sin\theta-V&#124;trackPos&#124;$ |'
  prefs: []
  type: TYPE_TB
- en: '| Jaafra et al. (2019) [[39](#bib.bib39)] | A2C | Camera image | $A=\{\mathrm{Steering,Throttle,Break}\}$
    | $R\{\mathrm{+1:Closing~{}goals;-1:Not~{}in~{}lane}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Rhinehart et al. (2019) [[51](#bib.bib51)] | Imitative model | LiDAR image
    | $A=\{\mathrm{Driving~{}way~{}points}\}$ | Probabilistic inference objectives
    |'
  prefs: []
  type: TYPE_TB
- en: IV-C The Sim-to-Real Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By leveraging the simulated driving scenarios and experiences obtained via simulation,
    the sim-to-real approach is a good alternative to train an end-to-end driving
    policy without using real data, as shown in Fig. [4](#S4.F4 "Figure 4 ‣ IV-C The
    Sim-to-Real Approach ‣ IV End-to-End Deep Reinforcement Learning for Autonomous
    Driving ‣ A Survey of Deep Reinforcement Learning Algorithms for Motion Planning
    and Control of Autonomous Vehicles"). This field has received a lot of attention
    during the recent years, and it has been shown in [[64](#bib.bib64)] that just
    by randomizing the simulator very carefully, a policy can be trained to fly drones
    indoors using the simulated data without having to do very careful system identification.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the simulator can facilitate the training of deep neural networks
    by generating abundant labeled data in many corner cases to extract task-relevant
    features and acquire good state representations, and the learned knowledge is
    expected to promote faster learning and better performance in real world scenarios.
    Therefore, the sim-to-real transfer is an important area of research that can
    adapt the learned knowledge from vehicle-traffic simulations to the real-world
    environment for decision making, planning and control.
  prefs: []
  type: TYPE_NORMAL
- en: However, driving autonomously in the urban environment consists of multiple
    tasks that involve complex and uncertain driving behaviors and interactions with
    the surrounding traffic. Besides some typical tasks that are shared with highway
    driving, such as lane keeping, lane changing, overtaking, and car following, driving
    in the urban environment also includes taking left-turns, complying with road
    signs and traffic lights, keeping an eye on lower-speed pedestrians and bicyclists,
    etc. While each of these specific tasks can be separately modeled in the simulator
    to train the autonomous driving policy with an outstanding performance, the knowledge
    transfer from the simulator to the real-world scenario would be more challenging
    due to the large number and complexity of tasks that further intensifies the difference
    between the source domain (simulator) and the target domain (real-world). Moreover,
    there are other technical challenges such as real-world visual signal noises,
    and the training environment in a car driving simulator is often significantly
    different from real-world driving in terms of their visual appearance [[45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: Due to the aforementioned challenges, while many research work applied some
    variant of deep RL algorithms on the simulation platform, only a handful of them
    attempted to transfer the knowledge learned from the simulator to real-world applications,
    i.e., [[37](#bib.bib37)]. In fact, [[37](#bib.bib37)] claimed itself to be “the
    first time of a deep RL driving”, which is trained using the simulator, “is shown
    working on real images”, and foresees simulation based RL can be used as initialization
    strategy for networks used in real applications. However, it is also reported
    in [[65](#bib.bib65)] that “end-to-end models trained solely in CARLA were unable
    to transfer to the real world”, so some domain randomization and domain adaptation
    methods are needed for bridging the sim-to-real gap in simulators.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9595cb56ad4b4040bca7d54e6d5f665.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Training and deployment of policies from Sim-to-Real transfer [[65](#bib.bib65)].'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the purpose of providing a comprehensive and intuitive comparison among
    different deep RL formulations on the motion planning and control of autonomous
    vehicles, TABLE [I](#S4.T1 "TABLE I ‣ IV-B Simulation ‣ IV End-to-End Deep Reinforcement
    Learning for Autonomous Driving ‣ A Survey of Deep Reinforcement Learning Algorithms
    for Motion Planning and Control of Autonomous Vehicles") summarizes the specific
    algorithm, the state space, the action space, and the specific reward/loss design
    of selective papers included in this review.
  prefs: []
  type: TYPE_NORMAL
- en: V Challenges and Future Work Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V-A Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-A1 The Pipeline Approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The choice of hand-crafted abstractions (features) for each module can limit
    the performance of the entire system. Once designed, these abstractions will have
    limited capacity to improve, and those parts that do not improve with data will
    eventually become the bottleneck. For instance, if the perception system becomes
    better, but the planner doesn’t get any better to utilize those benefits, eventually
    the planner will be the bottleneck. Ultimately we don’t actually know how to accurately
    construct the correct abstractions for the real world, and eventually these will
    get us into trouble.
  prefs: []
  type: TYPE_NORMAL
- en: One potential approach to tackle this challenge is to train all those layers
    of abstractions end-to-end, which means that the abstractions were optimally adapted
    for the task that needs to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 The Sim-to-Real Approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally speaking, training with data from only the human driver is not adequate,
    and collecting a sufficient amount of data from every possible driving condition
    can be extremely expensive and dangerous. While the sim-to-real approach can help
    people get away with no real-world data at all, this approach might not be sufficient
    to solve the problem, even though it is still an excellent way to get the network
    and parameters initialized. This is because instead of designing each of these
    pipeline modules by hand, we’ll still have to design our simulator by hand. In
    the end, the simulator will become the bottleneck due to challenges for developing
    a sufficiently realistic simulator with diverse environments [[66](#bib.bib66)],
    including the modeling of any interacting traffic participant with realistic dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Future Work Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the exciting progress of applied deep RL algorithms in autonomous driving
    tasks, these approaches are mostly trained for one task at a time, and each new
    task requires training a new agent, which is data-inefficient and fails to exploit
    the learned properties of similar tasks. In contrast, humans have the ability
    to not only learn complex tasks, but they can also adapt rapidly to new or evolving
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, some methods have been proposed to increase the generalization of
    deep RL algorithms in a variety of driving tasks. On the one hand, randomizing
    the simulator environment has been applied to generalize the trained policies
    [[67](#bib.bib67)]. On the other hand, transferring the knowledge accumulated
    from past experience through continual learning and meta learning has recently
    been explored to facilitate the generalization in both reinforcement learning
    [[68](#bib.bib68)] and imitation learning [[69](#bib.bib69)]. The ability to continuously
    learn and adapt quickly is essential to achieving real-world automated driving,
    which motivates further studies to introduce transfer learning and meta learning
    concepts into deep RL in the realm of autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, a systematic review is conducted on the existing literature employing
    deep RL algorithms on motion planning and control of autonomous vehicles, which
    is a field that has spurred the interest of industry and academia over the past
    five years. Both pipeline and end-to-end approaches have been extensively discussed.
    It has been demonstrated that, despite the fact that deep RL algorithms require
    an extended period and a large dataset to train, they can effectively interact
    with the environment in a trial-and-error way and does not require explicit human
    labeling or supervision on each data sample, making them promising candidates
    to accomplish autonomous driving in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] D. González, J. Pérez, V. Milanés, and F. Nashashibi, “A review of motion
    planning techniques for automated vehicles,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 17, no. 4, pp. 1135–1145, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] B. Paden, M. Čáp, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of
    motion planning and control techniques for self-driving urban vehicles,” *IEEE
    Transactions on intelligent vehicles*, vol. 1, no. 1, pp. 33–55, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and decision-making
    for autonomous vehicles,” *Annual Review of Control, Robotics, and Autonomous
    Systems*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, “A survey of autonomous
    driving: common practices and emerging technologies,” *arXiv preprint arXiv:1906.05113*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] V. Talpaert, I. Sobh, B. R. Kiran, P. Mannion, S. Yogamani, A. El-Sallab,
    and P. Perez, “Exploring applications of deep reinforcement learning for real-world
    autonomous driving systems,” *arXiv preprint arXiv:1901.01536*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani,
    and P. Pérez, “Deep reinforcement learning for autonomous driving: A survey,”
    *arXiv preprint arXiv:2002.00444*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. Aradi, “Survey of deep reinforcement learning for motion planning of
    autonomous vehicles,” *IEEE Transactions on Intelligent Transportation Systems*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] S. Levine, “Imitation, prediction, and model-based reinforcement learning
    for autonomous driving,” [Online]. Available: [https://slideslive.com/38917941/imitation-prediction-and-modelbased-reinforcement-learning-for-autonomous-driving](https://slideslive.com/38917941/imitation-prediction-and-modelbased-reinforcement-learning-for-autonomous-driving),
    Accessed: Oct. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] C.-J. Hoel, K. Wolff, and L. Laine, “Automated speed and lane change decision
    making using deep reinforcement learning,” in *2018 21st International Conference
    on Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 2148–2155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] D. Isele, A. Nakhaei, and K. Fujimura, “Safe reinforcement learning on
    autonomous vehicles,” in *2018 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*, Oct 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Wang, Q. Zhang, D. Zhao, and Y. Chen, “Lane change decision-making
    through deep reinforcement learning with rule-based constraints,” in *2019 International
    Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2019, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] B. Mirchevska, C. Pek, M. Werling, M. Althoff, and J. Boedecker, “High-level
    decision making for safe and reasonable autonomous lane changing using reinforcement
    learning,” in *2018 21st International Conference on Intelligent Transportation
    Systems (ITSC)*.   IEEE, 2018, pp. 2156–2162.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] T. Shi, P. Wang, X. Cheng, and C.-Y. Chan, “Driving decision and control
    for autonomous lane change based on deep reinforcement learning,” *arXiv preprint
    arXiv:1904.10171*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to drive
    by imitating the best and synthesizing the worst,” *arXiv preprint arXiv:1812.03079*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Ye, X. Zhang, and J. Sun, “Automated vehicle’s behavior decision making
    using deep reinforcement learning and high-fidelity simulation environment,” *Transportation
    Research Part C: Emerging Technologies*, vol. 107, pp. 155–170, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] F. Ye, X. Cheng, P. Wang, and C.-Y. Chan, “Automated lane change strategy
    using proximal policy optimization-based deep reinforcement learning,” *arXiv
    preprint arXiv:2002.02667*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Mazumder, B. Liu, S. Wang, Y. Zhu, L. Liu, and J. Li, “Action permissibility
    in deep reinforcement learning and application to autonomous driving,” *KDD’18
    Deep Learning Day*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Ma, K. Driggs-Campbell, and M. J. Kochenderfer, “Improved robustness
    and safety for autonomous vehicle control with adversarial reinforcement learning,”
    in *2018 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2018, pp. 1665–1671.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L. Sun, C. Peng, W. Zhan, and M. Tomizuka, “A fast integrated planning
    and control framework for autonomous driving via imitation learning,” in *ASME
    2018 Dynamic Systems and Control Conference*.   American Society of Mechanical
    Engineers Digital Collection, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] P. Wang, H. Li, and C.-Y. Chan, “Continuous control for automated lane
    change behavior based on deep deterministic policy gradient algorithm,” in *2019
    IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2019, pp. 1454–1460.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. Li, D. Zhao, Q. Zhang, and Y. Chen, “Reinforcement learning and deep
    learning based lateral control for autonomous driving [application notes],” *IEEE
    Computational Intelligence Magazine*, vol. 14, no. 2, pp. 83–98, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] O. Pietquin, F. Tango, and R. Aras, “Batch reinforcement learning for
    optimizing longitudinal driving assistance strategies,” in *2011 IEEE Symposium
    on Computational Intelligence in Vehicles and Transportation Systems (CIVTS) Proceedings*.   IEEE,
    2011, pp. 73–79.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Lefevre, A. Carvalho, and F. Borrelli, “A learning-based framework
    for velocity control in autonomous driving,” *IEEE Transactions on Automation
    Science and Engineering*, vol. 13, no. 1, pp. 32–42, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Schultz and V. Sokolov, “Deep reinforcement learning for dynamic urban
    transportation problems,” *arXiv preprint arXiv:1806.05310*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] M. Zhu, X. Wang, and Y. Wang, “Human-like autonomous car-following model
    with deep reinforcement learning,” *Transportation research part C: emerging technologies*,
    vol. 97, pp. 348–368, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] G. Hartmann, Z. Shiller, and A. Azaria, “Deep reinforcement learning for
    time optimal velocity control using prior knowledge,” in *2019 IEEE 31st International
    Conference on Tools with Artificial Intelligence (ICTAI)*.   IEEE, 2019, pp. 186–193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Lin, J. McPhee, and N. L. Azad, “Longitudinal dynamic versus kinematic
    models for car-following control using deep reinforcement learning,” in *2019
    IEEE Intelligent Transportation Systems Conference (ITSC)*.   IEEE, 2019, pp.
    1504–1510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement learning
    for urban autonomous driving,” in *2019 IEEE Intelligent Transportation Systems
    Conference (ITSC)*.   IEEE, 2019, pp. 2765–2771.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” *arXiv preprint arXiv:1610.03295*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] X. He, D. Xu, H. Zhao, M. Moze, F. Aioun, and F. Guillemard, “A human-like
    trajectory planning method by learning from naturalistic driving data,” in *2018
    IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2018, pp. 339–346.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] P. Wang, H. Li, and C.-Y. Chan, “Quadratic q-network for learning continuous
    control for autonomous vehicles,” *arXiv preprint arXiv:1912.00074*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    in *Advances in neural information processing systems*, 1989, pp. 305–313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Y. Lecun, E. Cosatto, J. Ben, U. Muller, and B. Flepp, “Dave: Autonomous
    off-road vehicle control using end-to-end learning,” *DARPA-IPTO Final Report*,
    2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *et al.*, “End to end learning for
    self-driving cars,” *arXiv preprint arXiv:1604.07316*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-end learning of driving
    models from large-scale video datasets,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 2174–2182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam,
    A. Bewley, and A. Shah, “Learning to drive in a day,” in *2019 International Conference
    on Robotics and Automation (ICRA)*, 2019, pp. 8248–8254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Jaritz, R. de Charette, M. Toromanoff, E. Perot, and F. Nashashibi,
    “End-to-end race driving with deep reinforcement learning,” in *2018 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2018, pp. 2070–2075.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and B. Boots,
    “Learning deep neural network control policies for agile off-road autonomous driving,”
    in *The NIPS Deep Rienforcement Learning Symposium*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Jaafra, J. L. Laurent, A. Deruyver, and M. S. Naceur, “Robust reinforcement
    learning for autonomous driving,” *APIA*, p. 52, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. C. Madrigal, “Inside waymo’s secret world for training self-driving
    cars,” [Online]. Available: [https://www.businessinsider.com/waymo-engineer-explains-why-testing-self-driving-cars-virtually-is-critical-2018-8](https://www.businessinsider.com/waymo-engineer-explains-why-testing-self-driving-cars-virtually-is-critical-2018-8),
    Accessed: Mar. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] NVIDIA, “Nvidia drive constellation – virtual reality autonomous vehicle
    simulator,” [Online]. Available: [https://www.nvidia.com/en-us/self-driving-cars/drive-constellation/](https://www.nvidia.com/en-us/self-driving-cars/drive-constellation/),
    Accessed: Mar. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Koutník, G. Cuccu, J. Schmidhuber, and F. Gomez, “Evolving large-scale
    neural networks for vision-based reinforcement learning,” in *Proceedings of the
    15th annual conference on Genetic and evolutionary computation*, 2013, pp. 1061–1068.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] J. Zhang and K. Cho, “Query-efficient imitation learning for end-to-end
    autonomous driving,” *arXiv preprint arXiv:1605.06450*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] X. Pan, Y. You, Z. Wang, and C. Lu, “Virtual to real reinforcement learning
    for autonomous driving,” *arXiv preprint arXiv:1704.03952*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Wang, D. Jia, and X. Weng, “Deep reinforcement learning for autonomous
    driving,” *arXiv preprint arXiv:1811.11329*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] G. Chhor, S. Pandey, and V. Patel, “Robust deep reinforcement learning
    for autonomous driving,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] F. Codevilla, M. Miiller, A. López, V. Koltun, and A. Dosovitskiy, “End-to-end
    driving via conditional imitation learning,” in *2018 IEEE International Conference
    on Robotics and Automation (ICRA)*, 2018, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. Liang, T. Wang, L. Yang, and E. Xing, “Cirl: Controllable imitative
    reinforcement learning for vision-based self-driving,” in *Proceedings of the
    European Conference on Computer Vision (ECCV)*, 2018, pp. 584–599.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Rhinehart, R. McAllister, and S. Levine, “Deep imitative models for
    flexible inference, planning, and control,” *arXiv preprint arXiv:1810.06544*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] N. Rhinehart, R. McAllister, K. Kitani, and S. Levine, “Precog: Prediction
    conditioned on goals in visual multi-agent settings,” in *Proceedings of the IEEE
    International Conference on Computer Vision*, 2019, pp. 2821–2830.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] P. Klose and R. Mester, “Simulated autonomous driving in a realistic driving
    environment using deep reinforcement learning and a deterministic finite state
    machine,” in *Proceedings of the 2nd International Conference on Applications
    of Intelligent Systems*, 2019, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] E. Perot, M. Jaritz, M. Toromanoff, and R. De Charette, “End-to-end driving
    in a realistic racing game with deep reinforcement learning,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*,
    2017, pp. 3–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. Vitelli and A. Nayebi, “Carma: A deep reinforcement learning approach
    to autonomous driving,” Tech. rep. Stanford University, Tech. Rep., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] TORCS, “The open racing car simulator,” [Online]. Available: [http://torcs.sourceforge.net/](http://torcs.sourceforge.net/),
    Accessed: Mar. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] D. Loiacono, P. L. Lanzi, J. Togelius, E. Onieva, D. A. Pelta, M. V. Butz,
    T. D. Lönneker, L. Cardamone, D. Perez, Y. Sáez, M. Preuss, and J. Quadflieg,
    “The 2009 simulated car racing championship,” *IEEE Trans. Comput. Intell. AI
    in Games*, vol. 2, no. 2, pp. 131–147, June 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA:
    An open urban driving simulator,” in *Proceedings of the 1st Annual Conference
    on Robot Learning*, 2017, pp. 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Unity, “Easily build autonomous vehicle training environments,” [Online].
    Available: [https://unity.com/solutions/automotive-transportation/autonomous-vehicle-training](https://unity.com/solutions/automotive-transportation/autonomous-vehicle-training),
    Accessed: Mar. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, “Robust deep reinforcement
    learning for security and safety in autonomous vehicle systems,” in *2018 21st
    International Conference on Intelligent Transportation Systems (ITSC)*.   IEEE,
    2018, pp. 307–312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. Nishi, P. Doshi, and D. Prokhorov, “Merging in congested freeway traffic
    using multipolicy decision making and passive actor-critic learning,” *IEEE Transactions
    on Intelligent Vehicles*, vol. 4, no. 2, pp. 287–297, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C. You, J. Lu, D. Filev, and P. Tsiotras, “Advanced planning for autonomous
    vehicles using reinforcement learning and deep inverse reinforcement learning,”
    *Robotics and Autonomous Systems*, vol. 114, pp. 1–18, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] M. Bouton, J. Karlsson, A. Nakhaei, K. Fujimura, M. J. Kochenderfer, and
    J. Tumova, “Reinforcement learning with probabilistic guarantees for autonomous
    driving,” *arXiv preprint arXiv:1904.07189*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] K. Min, H. Kim, and K. Huh, “Deep distributional reinforcement learning
    based high-level driving policy determination,” *IEEE Transactions on Intelligent
    Vehicles*, vol. 4, no. 3, pp. 416–424, Sep. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] F. Sadeghi and S. Levine, “Cad2rl: Real single-image flight without a
    single real image,” *arXiv preprint arXiv:1611.04201*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Amini, I. Gilitschenski, J. Phillips, J. Moseyko, R. Banerjee, S. Karaman,
    and D. Rus, “Learning robust control policies for end-to-end autonomous driving
    from data-driven simulation,” *IEEE Robotics and Automation Letters*, vol. 5,
    no. 2, pp. 1143–1150, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in deep
    reinforcement learning for robotics: a survey,” in *2020 IEEE Symposium Series
    on Computational Intelligence (SSCI)*, 2020, pp. 737–744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
    randomization for transferring deep neural networks from simulation to the real
    world,” in *2017 IEEE/RSJ international conference on intelligent robots and systems
    (IROS)*.   IEEE, 2017, pp. 23–30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] F. Ye, P. Wang, C.-Y. Chan, and J. Zhang, “Meta reinforcement learning-based
    lane change strategy for autonomous vehicles,” *arXiv preprint arXiv:2008.12451*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. E. Sallab, M. Saeed, O. A. Tawab, and M. Abdou, “Meta learning framework
    for automated driving,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
