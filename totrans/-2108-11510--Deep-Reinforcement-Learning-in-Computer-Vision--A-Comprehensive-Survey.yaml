- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:51:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.11510] Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.11510] 计算机视觉中的深度强化学习：综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.11510](https://ar5iv.labs.arxiv.org/html/2108.11510)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2108.11510](https://ar5iv.labs.arxiv.org/html/2108.11510)
- en: 'Deep Reinforcement Learning in Computer Vision:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉中的深度强化学习：
- en: A Comprehensive Survey
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 综合调查
- en: Ngan Le^(∗∗)    Vidhiwar Singh Rathour^∗    Kashu Yamazaki^∗    Khoa Luu   
    Marios Savvides
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Ngan Le^(∗∗)    Vidhiwar Singh Rathour^∗    Kashu Yamazaki^∗    Khoa Luu   
    Marios Savvides
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep reinforcement learning augments the reinforcement learning framework and
    utilizes the powerful representation of deep neural networks. Recent works have
    demonstrated the remarkable successes of deep reinforcement learning in various
    domains including finance, medicine, healthcare, video games, robotics, and computer
    vision. In this work, we provide a detailed review of recent and state-of-the-art
    research advances of deep reinforcement learning in computer vision. We start
    with *comprehending the theories* of deep learning, reinforcement learning, and
    deep reinforcement learning. We then *propose a categorization* of deep reinforcement
    learning methodologies and *discuss their advantages and limitations*. In particular,
    we divide deep reinforcement learning into *seven main categories* according to
    their applications in computer vision, i.e. (i) landmark localization (ii) object
    detection; (iii) object tracking; (iv) registration on both 2D image and 3D image
    volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other
    applications. Each of these categories is further analyzed with reinforcement
    learning techniques, network design, and performance. Moreover, we provide a comprehensive
    analysis of the existing publicly available datasets and examine source code availability.
    Finally, we present some open issues and discuss future research directions on
    deep reinforcement learning in computer vision.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习扩展了强化学习框架，并利用了深度神经网络的强大表示能力。最近的研究展示了深度强化学习在金融、医学、医疗保健、视频游戏、机器人技术和计算机视觉等多个领域的显著成功。在这项工作中，我们提供了对计算机视觉中深度强化学习的最新和最前沿研究进展的详细回顾。我们从*理解理论*开始，包括深度学习、强化学习和深度强化学习。接着我们*提出了一种分类*的深度强化学习方法，并*讨论了它们的优点和局限性*。特别是，我们根据在计算机视觉中的应用，将深度强化学习分为*七个主要类别*，即
    (i) 地标定位 (ii) 物体检测 (iii) 物体跟踪 (iv) 2D 图像和 3D 图像体数据的配准 (v) 图像分割 (vi) 视频分析；以及 (vii)
    其他应用。每个类别都进一步分析了强化学习技术、网络设计和性能。此外，我们提供了对现有公开数据集的全面分析，并检查了源代码的可用性。最后，我们提出了一些未解的问题，并讨论了计算机视觉中深度强化学习的未来研究方向。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Reinforcement learning (RL) is a machine learning technique for learning a sequence
    of actions in an interactive environment by trial and error that maximizes the
    expected reward [[351](#bib.bib351)]. Deep Reinforcement Learning (DRL) is the
    combination of Reinforcement Learning and Deep Learning (DL) and it has become
    one of the most intriguing areas of artificial intelligence today. DRL can solve
    a wide range of complex real-world decision-making problems with human-like intelligence
    that were previously intractable. DRL was selected by [[316](#bib.bib316)], [[106](#bib.bib106)]
    as one of ten breakthrough techniques in 2013 and 2017, respectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种通过试错学习在交互环境中学习一系列动作的机器学习技术，其目标是最大化期望奖励 [[351](#bib.bib351)]。深度强化学习（DRL）是强化学习与深度学习（DL）的结合，已成为当前人工智能领域最引人注目的领域之一。DRL
    能够解决广泛的复杂现实世界决策问题，这些问题以前是无法处理的，并具有类似人类的智能。DRL 被[[316](#bib.bib316)]和[[106](#bib.bib106)]分别选为2013年和2017年的十项突破性技术之一。
- en: The past years have witnessed the rapid development of DRL thanks to its amazing
    achievement in solving challenging decision-making problems in the real world.
    DRL has been successfully applied into many domains including games, robotics,
    autonomous driving, healthcare, natural language processing, and computer vision.
    In contrast to supervised learning which requires large labeled training data,
    DRL samples training data from an environment. This opens up many machine learning
    applications where big labeled training data does not exist.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年见证了深度强化学习（DRL）的迅速发展，得益于其在解决现实世界复杂决策问题上的惊人成就。DRL已经成功应用于许多领域，包括游戏、机器人技术、自动驾驶、医疗保健、自然语言处理和计算机视觉。与需要大量标记训练数据的监督学习不同，DRL从环境中采样训练数据。这为那些没有大量标记训练数据的机器学习应用打开了许多可能性。
- en: Far from supervised learning, DRL-based approaches focus on solving sequential
    decision-making problems. They aim at deciding, based on a set of experiences
    collected by interacting with the environment, the sequence of actions in an uncertain
    environment to achieve some targets. Different from supervised learning where
    the feedback is available after each system action, it is simply a scalar value
    that may be delayed in time in the DRL framework. For example, the success or
    failure of the entire system is reflected after a sequence of actions. Furthermore,
    the supervised learning model is updated based on the loss/error of the output
    and there is no mechanism to get the correct value when it is wrong. This is addressed
    by policy gradients in DRL by assigning gradients without a differentiable loss
    function. This aims at teaching a model to try things out randomly and learn to
    do correct things more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，基于DRL的方法专注于解决顺序决策问题。它们旨在根据通过与环境互动收集的一系列经验，决定在不确定环境中的行动序列以实现某些目标。不同于监督学习，其中反馈在每次系统动作后都可以获得，DRL框架中的反馈通常只是一个可能会延迟的标量值。例如，整个系统的成功或失败是在一系列动作之后才会反映出来。此外，监督学习模型是基于输出的损失/错误进行更新的，并且当输出错误时没有机制来获取正确值。DRL中的策略梯度通过在没有可微分损失函数的情况下分配梯度来解决这个问题。这旨在教会模型随机尝试，并学会更多地做正确的事情。
- en: Many survey papers in the field of DRL including [[13](#bib.bib13)] [[97](#bib.bib97)]
    [[414](#bib.bib414)] have been introduced recently. While [[13](#bib.bib13)] covers
    central algorithms in DRL, [[97](#bib.bib97)] provides an introduction to DRL
    models, algorithms, and techniques, where particular focus is the aspects related
    to generalization and how DRL can be used for practical applications. Recently,
    [[414](#bib.bib414)] introduces a survey, which discusses the broad applications
    of RL techniques in healthcare domains ranging from dynamic treatment regimes
    in chronic diseases and critical care, an automated medical diagnosis from both
    unstructured and structured clinical data, to many other control or scheduling
    domains that have infiltrated many aspects of a healthcare system. Different from
    the previous work, our survey focuses on how to implement DRL in various computer
    vision applications such as landmark detection, object detection, object tracking,
    image registration, image segmentation, and video analysis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近介绍了许多关于DRL领域的综述论文，包括[[13](#bib.bib13)] [[97](#bib.bib97)] [[414](#bib.bib414)]。虽然[[13](#bib.bib13)]涵盖了DRL中的核心算法，[[97](#bib.bib97)]提供了对DRL模型、算法和技术的介绍，特别关注与泛化相关的方面以及DRL如何用于实际应用。最近，[[414](#bib.bib414)]介绍了一项综述，讨论了RL技术在医疗保健领域的广泛应用，从慢性疾病和重症监护中的动态治疗方案，到从非结构化和结构化临床数据中自动医疗诊断，再到渗透到医疗保健系统各个方面的许多控制或调度领域。与之前的工作不同，我们的综述专注于如何在各种计算机视觉应用中实现DRL，如地标检测、物体检测、物体跟踪、图像配准、图像分割和视频分析。
- en: 'Our goal is to provide our readers good knowledge about the principle of RL/DRL
    and thorough coverage of the latest examples of how DRL is used for solving computer
    vision tasks. We structure the rest of the paper as follows: we first introduce
    fundamentals of Deep Learning (DL) in section [2](#S2 "2 Introduction to Deep
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    including Multi-Layer Perceptron (MLP), Autoencoder, Deep Belief Network, Convolutional
    Neural Networks (CNNs), Recurrent Neural Networks (RNNs). Then, we present the
    theories of RL in section [3](#S3 "3 Basics of Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), which starts with the Markov
    Decision Process (MDP) and continues with value function and Q-function. In the
    end of section [3](#S3 "3 Basics of Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), we introduce various techniques
    in RL under two categories of model-based and model-free RL. Next, we introduce
    DRL in section [4](#S4 "4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey") with main techniques in
    both value-based methods, policy gradient methods, and actor-critic methods under
    model-based and model-free categories. The application of DRL in computer vision
    will then be introduced in sections [5](#S5 "5 DRL in Landmark Detection ‣ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey"), [6](#S6 "6
    DRL in Object Detection ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey"), [7](#S7 "7 DRL in Object Tracking ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey"), [8](#S8 "8 DRL in Image Registration ‣ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey"), [9](#S9 "9
    DRL in Image Segmentation ‣ Deep Reinforcement Learning in Computer Vision: A
    Comprehensive Survey"), [10](#S10 "10 DRL in Video Analysis ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), [11](#S11 "11 Others Applications
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") corresponding
    respectively to DRL in landmark detection, DRL in object detection, DRL in object
    tracking, DRL in image registration, DRL in image segmentation, DRL in video analysis
    and other applications of DRL. Each application category first starts with a problem
    introduction and then state-of-the-art approaches in the field are discussed and
    compared through a summary table. We are going to discuss some future perspectives
    in section [12](#S12 "12 Future Perspectives ‣ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey") including challenges of DRL in computer
    vision and the recent advanced techniques.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的目标是为读者提供关于RL/DRL原理的良好知识，并全面覆盖DRL在计算机视觉任务中应用的最新示例。我们将本文的其余部分结构化如下：我们首先在第[2](#S2
    "2 Introduction to Deep Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")节介绍深度学习（DL）的基础知识，包括多层感知机（MLP）、自编码器、深度信念网络、卷积神经网络（CNNs）、递归神经网络（RNNs）。然后，我们在第[3](#S3
    "3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey")节介绍RL的理论，从马尔可夫决策过程（MDP）开始，继续讲解价值函数和Q函数。在第[3](#S3
    "3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey")节的最后，我们介绍了RL中两类技术：基于模型的和无模型的RL。接下来，我们在第[4](#S4
    "4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey")节介绍DRL，重点讲解价值方法、策略梯度方法和演员-评论家方法在基于模型和无模型类别中的主要技术。然后，第[5](#S5
    "5 DRL in Landmark Detection ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")、第[6](#S6 "6 DRL in Object Detection ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey")、第[7](#S7 "7 DRL in Object
    Tracking ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")、第[8](#S8
    "8 DRL in Image Registration ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")、第[9](#S9 "9 DRL in Image Segmentation ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey")、第[10](#S10 "10 DRL in Video
    Analysis ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")、第[11](#S11
    "11 Others Applications ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")节分别介绍了DRL在地标检测、目标检测、目标跟踪、图像配准、图像分割、视频分析和DRL其他应用中的应用。每个应用类别首先介绍问题，然后讨论和比较该领域的最新方法，通过总结表格呈现。我们将在第[12](#S12
    "12 Future Perspectives ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")节讨论一些未来的展望，包括DRL在计算机视觉中的挑战和近期先进技术。'
- en: 2 Introduction to Deep Learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习简介
- en: 2.1 Multi-Layer Perceptron (MLP)
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多层感知机（MLP）
- en: 'Deep learning models, in simple words, are large and deep artificial neural
    networks. Let us consider the simplest possible neural network which is called
    ”neuron” as illustrated in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1 Multi-Layer Perceptron
    (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey"). A computational model of a single neuron is
    called a perceptron which consists of one or more inputs, a processor, and a single
    output.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '简单来说，深度学习模型是大型且深层的人工神经网络。我们以最简单的神经网络为例，这种网络被称为“神经元”，如图 [1](#S2.F1 "Figure 1
    ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey") 所示。一个神经元的计算模型被称为感知器，它由一个或多个输入、一个处理器和一个输出组成。'
- en: '![Refer to caption](img/ae3f08d40dc14c10f862a38b2763122e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ae3f08d40dc14c10f862a38b2763122e.png)'
- en: 'Figure 1: An example of one neuron which takes input $\textbf{x}=[x_{1},x_{2},x_{3}]$,
    the intercept term $+1$ as bias, and the output o.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个神经元的示例，它以 $\textbf{x}=[x_{1},x_{2},x_{3}]$ 作为输入，以截距项 $+1$ 作为偏置，并计算输出 o。
- en: '![Refer to caption](img/3c8e28551e381f4fb936359fe3a47ebf.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3c8e28551e381f4fb936359fe3a47ebf.png)'
- en: 'Figure 2: An example of multi-layer perceptron network (MLP)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：多层感知器网络 (MLP) 的示例
- en: '![Refer to caption](img/ffb334467464b76c1b885be0d35835d8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ffb334467464b76c1b885be0d35835d8.png)'
- en: 'Figure 3: An illustration of various DL architectures. (a): Autoencoder (AE);
    (b): Deep Belief Network; (c): Convolutional Neural Network (CNN); (d): Recurrent
    Neural Network (RNN).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：各种深度学习架构的示意图。（a）：自编码器 (AE)；（b）：深度置信网络；（c）：卷积神经网络 (CNN)；（d）：递归神经网络 (RNN)。
- en: 'In this example, the neuron is a computational unit that takes $\textbf{x}=[x_{0},x_{1},x_{2}]$
    as input, the intercept term $+1$ as bias b, and the output o. The goal of this
    simple network is to learn a function $f:\mathrm{R^{N}}\rightarrow\mathrm{R^{M}}$
    where $N$ is the number of dimensions for input x and $M$ is the number of dimensions
    for output which is computed as $\textbf{o}=f(\textbf{x},\theta)$, where $\theta$
    is a set of weights and are known as weights $\theta=\{w_{i}\}$. Mathematically,
    the output o of a one neuron is defined as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，神经元是一个计算单元，它以 $\textbf{x}=[x_{0},x_{1},x_{2}]$ 作为输入，以截距项 $+1$ 作为偏置 b，计算输出
    o。这个简单网络的目标是学习一个函数 $f:\mathrm{R^{N}}\rightarrow\mathrm{R^{M}}$，其中 $N$ 是输入 x 的维度数量，$M$
    是输出的维度数量，计算公式为 $\textbf{o}=f(\textbf{x},\theta)$，其中 $\theta$ 是一组权重，称为权重 $\theta=\{w_{i}\}$。在数学上，一个神经元的输出
    o 定义为：
- en: '|  | $\textbf{o}=f(\textbf{x},\theta)=\sigma\left(\sum_{i=1}^{N}{w_{i}x_{i}+b}\right)=\sigma(\textbf{W}^{T}\textbf{x}+b)$
    |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{o}=f(\textbf{x},\theta)=\sigma\left(\sum_{i=1}^{N}{w_{i}x_{i}+b}\right)=\sigma(\textbf{W}^{T}\textbf{x}+b)$
    |  | (1) |'
- en: 'In this equation, $\sigma$ is the point-wise non-linear activation function.
    The common non-linear activation functions for hidden units are hyperbolic tangent
    (Tanh), sigmoid, softmax, ReLU, and LeakyReLU. A typical multi-layer perception
    (MLP) neural network is composed of one input layer, one output layer, and many
    hidden layers. Each layer may contain many units. In this network, x is the input
    layer, o is the output layer. The middle layer is called the hidden layer. In
    Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction
    to Deep Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(b), MLP contains 3 units of the input layer, 3 units of the hidden layer,
    and 1 unit of the output layer.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个方程中，$\sigma$ 是逐点非线性激活函数。隐藏单元的常见非线性激活函数有双曲正切（Tanh）、sigmoid、softmax、ReLU 和
    LeakyReLU。一个典型的多层感知器（MLP）神经网络由一个输入层、一个输出层和多个隐藏层组成。每一层可能包含多个单元。在这个网络中，x 是输入层，o
    是输出层。中间层称为隐藏层。在图 [2](#S2.F2 "Figure 2 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction
    to Deep Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(b) 中，MLP 包含 3 个输入层单元、3 个隐藏层单元和 1 个输出层单元。'
- en: In general, we consider a MLP neural network with $L$ hidden layers of units,
    one layer of input units and one layer of output units. The number of input units
    is $N$, output units is $M$, and units in hidden layer $l^{th}$ is $N^{l}$. The
    weight of the $j^{th}$ unit in layer $l^{th}$ and the $i^{th}$ unit in layer $(l+1)^{th}$
    is denoted by $w_{ij}^{l}$. The activation of the $i^{th}$ unit in layer $l^{th}$
    is $\textbf{h}_{i}^{l}$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们考虑一个具有 $L$ 层隐藏单元的 MLP 神经网络，一个输入单元层和一个输出单元层。输入单元的数量是 $N$，输出单元的数量是 $M$，第
    $l^{th}$ 层的隐藏单元数量是 $N^{l}$。第 $l^{th}$ 层的第 $j^{th}$ 单元和 $(l+1)^{th}$ 层的第 $i^{th}$
    单元之间的权重用 $w_{ij}^{l}$ 表示。第 $l^{th}$ 层的第 $i^{th}$ 单元的激活值为 $\textbf{h}_{i}^{l}$。
- en: '![Refer to caption](img/873522a85978bea2ad1c0c5014fc2f0d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/873522a85978bea2ad1c0c5014fc2f0d.png)'
- en: 'Figure 4: Architecture of a typical convolutional network for image classification
    containing three basic layers: convolution layer, pooling layer and fully connected
    layer'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：典型卷积网络的架构用于图像分类，包含三个基本层：卷积层、池化层和全连接层
- en: 2.2 Autoencoder
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 自编码器
- en: 'Autoencoder is an unsupervised algorithm used for representation learning,
    such as feature selection or dimension reduction. A gentle introduction to Variational
    Autoencoder (VAE) is given in [[11](#bib.bib11)] and VAE framework is illustrated
    in Fig.[3](#S2.F3 "Figure 3 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction
    to Deep Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(a). In general, VAE aims to learn a parametric latent variable model
    by maximizing the marginal log-likelihood of the training data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '自编码器是一种无监督算法，用于表示学习，如特征选择或维度减少。有关变分自编码器（VAE）的温和介绍见[[11](#bib.bib11)]，VAE框架在图[3](#S2.F3
    "Figure 3 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction to Deep Learning
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")(a)中有所说明。一般来说，VAE旨在通过最大化训练数据的边际对数似然来学习一个参数化的潜变量模型。'
- en: 2.3 Deep Belief Network
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 深度置信网络
- en: 'Deep Belief Network (DBN) and Deep Autoencoder are two common unsupervised
    approaches that have been used to initialize the network instead of random initialization.
    While Deep Autoencoder is based on Autoencoder, Deep Belief Networks is based
    on Restricted Boltzmann Machine (RBM), which contains a layer of input data and
    a layer of hidden units that learn to represent features that capture high-order
    correlations in the data as illustrated in Fig.[3](#S2.F3 "Figure 3 ‣ 2.1 Multi-Layer
    Perceptron (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey")(b).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '深度置信网络（DBN）和深度自编码器是两种常见的无监督学习方法，它们被用来初始化网络而不是随机初始化。虽然深度自编码器基于自编码器，深度置信网络则基于限制玻尔兹曼机（RBM），RBM包含一个输入数据层和一个隐含单元层，隐含单元层学习表示能够捕捉数据中高阶相关性的特征，如图[3](#S2.F3
    "Figure 3 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction to Deep Learning
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")(b)所示。'
- en: 2.4 Convolutional Neural Networks (CNN)
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 卷积神经网络（CNN）
- en: 'Convolutional Neural Network (CNN) [[204](#bib.bib204)] [[203](#bib.bib203)]
    is a special case of fully connected MLP that implements weight sharing for processing
    data. CNN uses the spatial correlation of the signal to utilize the architecture
    in a more sensible way. Their architecture, somewhat inspired by the biological
    visual system, possesses two key properties that make them extremely useful for
    image applications: spatially shared weights and spatial pooling. These kinds
    of networks learn features that are shift-invariant, i.e., filters that are useful
    across the entire image (due to the fact that image statistics are stationary).
    The pooling layers are responsible for reducing the sensitivity of the output
    to slight input shifts and distortions, and increasing the reception field for
    next layers. Since 2012, one of the most notable results in Deep Learning is the
    use of CNN to obtain a remarkable improvement in object recognition in ImageNet
    classification challenge [[72](#bib.bib72)] [[187](#bib.bib187)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）[[204](#bib.bib204)] [[203](#bib.bib203)] 是一种特殊的全连接MLP，它实现了权重共享以处理数据。CNN利用信号的空间相关性，以更合理的方式利用架构。其架构在一定程度上受到生物视觉系统的启发，具有两个关键属性，使其在图像应用中极为有用：空间共享权重和空间池化。这些网络学习的特征是平移不变的，即在整个图像中都有效的滤波器（因为图像统计是静态的）。池化层负责减少输出对输入轻微位移和失真的敏感性，并增加下一层的接受域。自2012年以来，深度学习领域最显著的成果之一是使用CNN在ImageNet分类挑战中取得了显著的目标识别改进[[72](#bib.bib72)]
    [[187](#bib.bib187)]。
- en: 'A typical CNN is composed of multiple stages, as shown in Fig. [3](#S2.F3 "Figure
    3 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey")(c). The output
    of each stage is made of a set of 2D arrays called feature maps. Each feature
    map is the outcome of one convolutional (and an optional pooling) filter applied
    over the full image. A point-wise non-linear activation function is applied after
    each convolution. In its more general form, a CNN can be written as'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 CNN 由多个阶段组成，如图 [3](#S2.F3 "图 3 ‣ 2.1 多层感知器 (MLP) ‣ 2 深度学习介绍 ‣ 计算机视觉中的深度强化学习：全面综述")(c)
    所示。每个阶段的输出由一组称为特征图的 2D 数组组成。每个特征图是一个卷积（及可选的池化）滤波器应用于整个图像的结果。每次卷积后都会应用一个逐点非线性激活函数。在其更一般的形式下，CNN
    可以写作
- en: '|  | <math   alttext="\begin{split}\textbf{h}^{0}=&amp;\textbf{x}\\ \textbf{h}^{l}=&amp;pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{split}\textbf{h}^{0}=&amp;\textbf{x}\\ \textbf{h}^{l}=&amp;pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\'
- en: \textbf{o}=&amp;\textbf{h}^{L}\\
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \textbf{o}=&amp;\textbf{h}^{L}\\
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd columnalign="right" ><mrow ><msup ><mtext >h</mtext><mn  >0</mn></msup><mo
    >=</mo></mrow></mtd><mtd columnalign="left" ><mtext >x</mtext></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><msup ><mtext >h</mtext><mi >l</mi></msup><mo
    >=</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mrow ><mrow ><mrow ><mi  >p</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><msup ><mi  >l</mi><mi >l</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub
    ><mi >σ</mi><mi >l</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mrow ><msup ><mtext >w</mtext><mi >l</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msup ><mtext >h</mtext><mrow ><mi >l</mi><mo
    >−</mo><mn >1</mn></mrow></msup></mrow><mo >+</mo><msup ><mtext >b</mtext><mi
    >l</mi></msup></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo><mrow ><mo rspace="0.167em"  >∀</mo><mi >l</mi></mrow></mrow><mo >∈</mo><mn
    >1</mn></mrow><mo >,</mo><mrow ><mn >2</mn><mo >,</mo><mrow ><mi mathvariant="normal"  >…</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >L</mi></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mtext >o</mtext><mo >=</mo></mrow></mtd><mtd
    columnalign="left" ><msup ><mtext >h</mtext><mi >L</mi></msup></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><cn
    type="integer" >0</cn></apply><apply ><ci ><mtext >x</mtext></ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><ci >𝑙</ci></apply></apply></apply><apply
    ><apply  ><ci >𝑝</ci><ci >𝑜</ci><ci  >𝑜</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑙</ci><ci >𝑙</ci></apply><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜎</ci><ci >𝑙</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    ><mtext >w</mtext></ci><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    ><mtext >h</mtext></ci><apply ><ci >𝑙</ci><cn type="integer" >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci ><mtext >b</mtext></ci><ci
    >𝑙</ci></apply></apply></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >formulae-sequence</csymbol><apply ><apply ><csymbol cd="latexml" >for-all</csymbol><ci
    >𝑙</ci></apply><list ><cn type="integer" >1</cn><cn type="integer" >2</cn></list></apply><apply
    ><apply  ><ci >…</ci><ci >𝐿</ci><ci ><mtext >o</mtext></ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><ci >𝐿</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\textbf{h}^{0}=&\textbf{x}\\ \textbf{h}^{l}=&pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\ \textbf{o}=&\textbf{h}^{L}\\ \end{split}</annotation></semantics></math>
    |  | (2) |
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: where $\textbf{w}^{l},\textbf{b}^{l}$ are trainable parameters as in MLPs at
    layer $l^{th}$. $\textbf{x}\in\mathrm{R}^{c\times h\times w}$ is vectorized from
    an input image with $c$ being the color channels, $h$ the image height and $w$
    the image width. $\textbf{o}\in\mathrm{R}^{n\times h^{\prime}\times w^{\prime}}$
    is vectorized from an array of dimension $h^{\prime}\times w^{\prime}$ of output
    vector (of dimension $n$). $pool^{l}$ is a (optional) pooling function at layer
    $l^{th}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{w}^{l},\textbf{b}^{l}$ 是像MLPs中层 $l^{th}$ 的可训练参数。$\textbf{x}\in\mathrm{R}^{c\times
    h\times w}$ 是从输入图像向量化得到的，其中 $c$ 为颜色通道，$h$ 为图像高度，$w$ 为图像宽度。$\textbf{o}\in\mathrm{R}^{n\times
    h^{\prime}\times w^{\prime}}$ 是从输出向量（维度为 $n$）的 $h^{\prime}\times w^{\prime}$ 的数组向量化得到的。$pool^{l}$
    是层 $l^{th}$ 的（可选）池化函数。
- en: Compared to traditional machine learning methods, CNN has achieved state-of-the-art
    performance in many domains including image understanding, video analysis and
    audio/speech recognition. In image understanding [[404](#bib.bib404)], [[426](#bib.bib426)],
    CNN outperforms human capacities [[39](#bib.bib39)]. Video analysis [[422](#bib.bib422)],
    [[217](#bib.bib217)] is another application that turns the CNN model from a detector
    [[374](#bib.bib374)] into a tracker [[94](#bib.bib94)]. As a special case of image
    segmentation [[194](#bib.bib194)], [[193](#bib.bib193)], saliency detection is
    another computer vision application that uses CNN [[381](#bib.bib381)], [[213](#bib.bib213)].
    In addition to the previous applications, pose estimation [[290](#bib.bib290)],
    [[362](#bib.bib362)] is another interesting research that uses CNN to estimate
    human-body pose. Action recognition in both still images and videos is a special
    case of recognition and is a challenging problem. [[110](#bib.bib110)] utilizes
    CNN-based representation of contextual information in which the most representative
    secondary region within a large number of object proposal regions, together with
    the contextual features, is used to describe the primary region. CNN-based action
    recognition in video sequences is reviewed in [[420](#bib.bib420)]. Text detection
    and recognition using CNN is the next step of optical character recognition (OCR)
    [[406](#bib.bib406)] and word spotting [[160](#bib.bib160)]. Not only in computer
    vision, CNN has been successfully applied into other domains such as speech recognition
    and speech synthesis [[274](#bib.bib274)], [[283](#bib.bib283)], biometrics [[242](#bib.bib242)],
    [[85](#bib.bib85)], [[281](#bib.bib281)], [[350](#bib.bib350)],[[304](#bib.bib304)],
    [[261](#bib.bib261)], biomedical [[191](#bib.bib191)], [[342](#bib.bib342)], [[192](#bib.bib192)],
    [[411](#bib.bib411)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习方法相比，CNN 在包括图像理解、视频分析和音频/语音识别等许多领域中取得了最先进的性能。在图像理解中 [[404](#bib.bib404)],
    [[426](#bib.bib426)], CNN 超越了人类的能力 [[39](#bib.bib39)]。视频分析 [[422](#bib.bib422)],
    [[217](#bib.bib217)] 是另一个应用，它将 CNN 模型从检测器 [[374](#bib.bib374)] 转变为跟踪器 [[94](#bib.bib94)]。作为图像分割
    [[194](#bib.bib194)], [[193](#bib.bib193)] 的特殊情况，显著性检测是另一个利用 CNN 的计算机视觉应用 [[381](#bib.bib381)],
    [[213](#bib.bib213)]。除了之前的应用，姿态估计 [[290](#bib.bib290)], [[362](#bib.bib362)] 是另一个有趣的研究，利用
    CNN 估计人体姿势。静态图像和视频中的动作识别是识别的特殊情况，并且是一个具有挑战性的问题。[[110](#bib.bib110)] 利用基于 CNN 的上下文信息表示，其中在大量目标提议区域中最具代表性的次区域与上下文特征一起，用于描述主要区域。基于
    CNN 的视频序列动作识别的综述见 [[420](#bib.bib420)]。使用 CNN 的文本检测和识别是光学字符识别（OCR） [[406](#bib.bib406)]
    和词汇检测 [[160](#bib.bib160)] 的下一步。CNN 不仅在计算机视觉中取得成功，还成功应用于语音识别和语音合成 [[274](#bib.bib274)],
    [[283](#bib.bib283)], 生物识别 [[242](#bib.bib242)], [[85](#bib.bib85)], [[281](#bib.bib281)],
    [[350](#bib.bib350)], [[304](#bib.bib304)], [[261](#bib.bib261)], 生物医学 [[191](#bib.bib191)],
    [[342](#bib.bib342)], [[192](#bib.bib192)], [[411](#bib.bib411)]。
- en: 2.5 Recurrent Neural Networks (RNN)
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 循环神经网络（RNN）
- en: 'RNN is an extremely powerful sequence model and was introduced in the early
    1990s [[172](#bib.bib172)]. A typical RNN contains three parts, namely, sequential
    input data ($\textbf{x}_{t}$), hidden state ($\textbf{h}_{t}$) and sequential
    output data ($\textbf{y}_{t}$) as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Multi-Layer
    Perceptron (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey")(d).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是一种极为强大的序列模型，早在1990年代初期就被引入 [[172](#bib.bib172)]。一个典型的 RNN 包含三个部分，即序列输入数据（$\textbf{x}_{t}$）、隐藏状态（$\textbf{h}_{t}$）和序列输出数据（$\textbf{y}_{t}$），如图
    [3](#S2.F3 "图 3 ‣ 2.1 多层感知器 (MLP) ‣ 2 深度学习简介 ‣ 计算机视觉中的深度强化学习：全面调查")(d) 所示。
- en: RNN makes use of sequential information and performs the same task for every
    element of a sequence where the output is dependent on the previous computations.
    The activation of the hidden states at time-step $t$ is computed as a function
    $f$ of the current input symbol $\bf{x}_{t}$ and the previous hidden states $\bf{h}_{t-1}$.
    The output at time $t$ is calculated as a function $g$ of the current hidden state
    $\bf{h}_{t}$ as follows
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 利用序列信息，并对序列的每个元素执行相同的任务，其中输出依赖于之前的计算。时间步 $t$ 的隐藏状态激活作为当前输入符号 $\bf{x}_{t}$
    和之前隐藏状态 $\bf{h}_{t-1}$ 的函数 $f$ 来计算。时间 $t$ 的输出作为当前隐藏状态 $\bf{h}_{t}$ 的函数 $g$ 计算如下
- en: '|  | $\begin{split}\textbf{{h}}_{t}&amp;=f(\textbf{Ux}_{t}+\textbf{Wh}_{t-1})\\
    \textbf{y}_{t}&amp;=g(\textbf{Vh}_{t})\end{split}$ |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\textbf{{h}}_{t}&=f(\textbf{Ux}_{t}+\textbf{Wh}_{t-1})\\
    \textbf{y}_{t}&=g(\textbf{Vh}_{t})\end{split}$ |  | (3) |'
- en: where $\bf{U}$ is the input-to-hidden weight matrix, $\bf{W}$ is the state-to-state
    recurrent weight matrix, $\bf{V}$ is the hidden-to-output weight matrix. $f$ is
    usually a logistic sigmoid function or a hyperbolic tangent function and $g$ is
    defined as a softmax function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bf{U}$ 是输入到隐藏的权重矩阵，$\bf{W}$ 是状态到状态的递归权重矩阵，$\bf{V}$ 是隐藏到输出的权重矩阵。$f$ 通常是逻辑
    sigmoid 函数或双曲正切函数，$g$ 定义为 softmax 函数。
- en: Most works on RNN have made use of the method of backpropagation through time
    (BPTT) [[318](#bib.bib318)] to train the parameter set $(\bf{U},\bf{V},\bf{W})$
    and propagate error backward through time. In classic backpropagation, the error
    or loss function is defined as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于 RNN 的研究使用了通过时间的反向传播（BPTT）[[318](#bib.bib318)] 方法来训练参数集 $(\bf{U},\bf{V},\bf{W})$
    并将误差向后传播。在经典的反向传播中，误差或损失函数定义为
- en: '|  | $E(\textbf{y''},\textbf{y})=\sum_{t}{&#124;&#124;\textbf{y''}_{t}-\textbf{y}_{t}&#124;&#124;^{2}}$
    |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(\textbf{y''},\textbf{y})=\sum_{t}{\|\textbf{y''}_{t}-\textbf{y}_{t}\|^{2}}$
    |  | (4) |'
- en: where $\textbf{y}_{t}$ is the prediction and $\textbf{y'}_{t}$ is the labeled
    groundtruth.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{y}_{t}$ 是预测值，$\textbf{y'}_{t}$ 是标记的真实值。
- en: For a specific weight W, the update rule for gradient descent is defined as
    $\textbf{W}^{new}=\textbf{W}-\gamma\frac{\partial E}{\partial\textbf{W}}$, where
    $\gamma$ is the learning rate. In RNN model, the gradients of the error with respect
    to our parameters U, V and W are learned using Stochastic Gradient Descent (SGD)
    and chain rule of differentiation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的权重 W，梯度下降的更新规则定义为 $\textbf{W}^{new}=\textbf{W}-\gamma\frac{\partial E}{\partial\textbf{W}}$，其中
    $\gamma$ 是学习率。在 RNN 模型中，使用随机梯度下降（SGD）和链式法则来学习误差相对于参数 U、V 和 W 的梯度。
- en: The difficulty of training RNN to capture long-term dependencies has been studied
    in [[26](#bib.bib26)]. To address the issue of learning long-term dependencies,
    Hochreiter and Schmidhuber [[139](#bib.bib139)] proposed Long Short-Term Memory
    (LSTM), which can maintain a separate memory cell inside it that updates and exposes
    its content only when deemed necessary. Recently, a Gated Recurrent Unit (GRU)
    was proposed by [[51](#bib.bib51)] to make each recurrent unit adaptively capture
    dependencies of different time scales. Like the LSTM unit, the GRU has gating
    units that modulate the flow of information inside the unit but without having
    separate memory cells.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 RNN 以捕捉长期依赖性的难度已在[[26](#bib.bib26)]中研究过。为了应对学习长期依赖性的问题，Hochreiter 和 Schmidhuber
    [[139](#bib.bib139)] 提出了长短期记忆（LSTM），它可以在内部维持一个独立的记忆单元，仅在必要时更新并暴露其内容。最近，[[51](#bib.bib51)]
    提出了门控循环单元（GRU），使每个循环单元能够自适应地捕捉不同时间尺度的依赖性。与 LSTM 单元类似，GRU 具有门控单元来调节信息在单元内部的流动，但没有独立的记忆单元。
- en: Several variants of RNN have been later introduced and successfully applied
    to wide variety of tasks, such as natural language processing [[257](#bib.bib257)],
    [[214](#bib.bib214)], speech recognition [[115](#bib.bib115)], [[54](#bib.bib54)],
    machine translation [[175](#bib.bib175)], [[241](#bib.bib241)], question answering
    [[138](#bib.bib138)], image captioning [[247](#bib.bib247)], [[78](#bib.bib78)],
    and many more.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 后来介绍了几种 RNN 的变体，并成功应用于各种任务，如自然语言处理 [[257](#bib.bib257)]、[[214](#bib.bib214)]、语音识别
    [[115](#bib.bib115)]、[[54](#bib.bib54)]、机器翻译 [[175](#bib.bib175)]、[[241](#bib.bib241)]、问答
    [[138](#bib.bib138)]、图像描述 [[247](#bib.bib247)]、[[78](#bib.bib78)] 等。
- en: 3 Basics of Reinforcement Learning
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 强化学习基础
- en: This section serves as a brief introduction to the theoretical models and techniques
    in RL. In order to provide a quick overview of what constitutes the main components
    of RL methods, some fundamental concepts and major theoretical problems are also
    clarified. RL is a kind of machine learning method where agents learn the optimal
    policy by trial and error. Unlike supervised learning, the feedback is available
    after each system action, it is simply a scalar value that may be delayed in time
    in RL framework, for example, the success or failure of the entire system is reflected
    after a sequence of actions. Furthermore, the supervised learning model is updated
    based on the loss/error of the output and there is no mechanism to get the correct
    value when it is wrong. This is addressed by policy gradients in RL by assigning
    gradients without a differentiable loss function which aims at teaching a model
    to try things out randomly and learn to do correct things more.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了 RL 中的理论模型和技术。为了提供 RL 方法主要组成部分的快速概述，某些基本概念和主要理论问题也得到澄清。RL 是一种机器学习方法，其中代理通过试错学习最优策略。与监督学习不同，反馈在每次系统动作后可用，在
    RL 框架中，反馈只是一个可能会延迟的标量值，例如，整个系统的成功或失败是在一系列动作后体现的。此外，监督学习模型是基于输出的损失/错误进行更新的，并且没有机制在出错时获取正确的值。RL
    中的策略梯度通过分配梯度而不使用可微分的损失函数来解决这个问题，这旨在教导模型随机尝试，并学习做正确的事情更多。
- en: Inspired by behavioral psychology, RL was proposed to address the sequential
    decision-making problems which exist in many applications such as games, robotics,
    healthcare, smart grids, stock, autonomous driving, etc. Unlike supervised learning
    where the data is given, an artificial agent collects experiences (data) by interacting
    with its environment in RL framework. Such experience is then gathered to optimize
    the cumulative rewards/utilities.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 受行为心理学启发，RL 被提出以解决许多应用中存在的序列决策问题，如游戏、机器人、医疗保健、智能电网、股票、自动驾驶等。与给定数据的监督学习不同，人工代理通过与环境交互在
    RL 框架中收集经验（数据）。然后收集这些经验以优化累积奖励/效用。
- en: 'In this section, we focus on how the RL problem can be formalized as an agent
    that can make decisions in an environment to optimize some objectives presented
    under reward functions. Some key aspects of RL are: (i) Address the sequential
    decision making; (ii) There is no supervisor, only a reward presented as scalar
    number; and (iii) The feedback is highly delayed. Markov Decision Process (MDP)
    is a framework that has commonly been used to solve most RL problems with discrete
    actions, thus we will first discuss MDP in this section. We then introduce value
    function and how to categorize RL into model-based or model-free methods. At the
    end of this section, we discuss some challenges in RL.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们重点讨论 RL 问题如何被形式化为一个能够在环境中做出决策以优化在奖励函数下呈现的一些目标的代理。RL 的一些关键方面是：(i) 解决序列决策问题；(ii)
    没有监督者，只有以标量数字呈现的奖励；(iii) 反馈高度延迟。马尔可夫决策过程（MDP）是一个框架，通常用于解决大多数具有离散动作的 RL 问题，因此我们将首先讨论
    MDP。在本节结束时，我们介绍值函数以及如何将 RL 分类为基于模型或无模型的方法。最后，我们讨论 RL 中的一些挑战。
- en: '![Refer to caption](img/878eca4dbadcd3f5807e57db9d9a5ff4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/878eca4dbadcd3f5807e57db9d9a5ff4.png)'
- en: 'Figure 5: An illustration of agent-environment interaction in RL'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：RL 中代理与环境交互的示意图
- en: 3.1 Markov Decision Process
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 马尔可夫决策过程
- en: 'The standard theory of RL is defined by a Markov Decision Process (MDP), which
    is an extension of the Markov process (also known as the Markov chain). Mathematically,
    the Markov process is a discrete-time stochastic process whose conditional probability
    distribution of the future states only depends upon the present state and it provides
    a framework to model decision-making situations. An MDP is typically defined by
    five elements as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: RL 的标准理论由马尔可夫决策过程（MDP）定义，它是马尔可夫过程（也称为马尔可夫链）的扩展。从数学上讲，马尔可夫过程是一种离散时间随机过程，其未来状态的条件概率分布仅依赖于当前状态，并且它提供了建模决策情境的框架。一个
    MDP 通常由以下五个元素定义：
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$S$: a set of state or observation space of an environment. $s_{0}$ is starting
    state.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S$：环境的状态或观察空间集合。$s_{0}$ 是起始状态。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mathcal{A}$: set of actions the agent can choose.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{A}$：代理可以选择的动作集。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$T$: a transition probability function $T(s_{t+1}|s_{t},a_{t})$, specifying
    the probability that the environment will transition to state $s_{t+1}\in S$ if
    the agent takes action $a_{t}\in\mathcal{A}$ in state $s_{t}\in S$.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$T$: 转移概率函数 $T(s_{t+1}|s_{t},a_{t})$，指定如果代理在状态 $s_{t}\in S$ 下采取动作 $a_{t}\in\mathcal{A}$，环境将以概率转移到状态
    $s_{t+1}\in S$。'
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$R$: a reward function where $r_{t+1}=R(s_{t},s_{t+1})$ is a reward received
    for taking action $a_{t}$ at state $s_{t}$ and transfer to the next state $s_{t+1}$.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$R$: 奖励函数，其中 $r_{t+1}=R(s_{t},s_{t+1})$ 是在状态 $s_{t}$ 下采取动作 $a_{t}$ 并转移到下一个状态
    $s_{t+1}$ 时获得的奖励。'
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\gamma$: a discount factor.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\gamma$: 折扣因子。'
- en: 'Considering MDP($S$, $\mathcal{A}$, $\gamma$, $T$, $R$), the agent chooses
    an action $a_{t}$ according to the policy $\pi(a_{t}|s_{t})$ at state $s_{t}$.
    Notably, agent’s algorithm for choosing action $a$ given current state $s$, which
    in general can be viewed as distribution $\pi(a|s)$, is called a policy (strategy).
    The environment receives the action, produces a reward $r_{t+1}$ and transfers
    to the next state $s_{t+1}$ according to the transition probability $T(s_{t+1}|s_{t},a_{t})$.
    The process continues until the agent reaches a terminal state or a maximum time
    step. In RL framework, the tuple $(s_{t},a_{t},r_{t+1},s_{t+1})$ is called transition.
    Several sequential transitions are usually referred to as roll-out. Full sequence
    $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$ is called a trajectory. Theoretically,
    trajectory is infinitely long, but the episodic property holds in most practical
    cases. One trajectory of some finite length $\tau$ is called an episode. For given
    MDP and policy $\pi$, the probability of observing $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$
    is called trajectory distribution and is denoted as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 MDP($S$, $\mathcal{A}$, $\gamma$, $T$, $R$)，代理根据策略 $\pi(a_{t}|s_{t})$ 在状态
    $s_{t}$ 下选择动作 $a_{t}$。值得注意的是，代理在给定当前状态 $s$ 时选择动作 $a$ 的算法，通常可以视为分布 $\pi(a|s)$，称为策略（策略）。环境接收到动作，产生奖励
    $r_{t+1}$ 并根据转移概率 $T(s_{t+1}|s_{t},a_{t})$ 转移到下一个状态 $s_{t+1}$。该过程继续，直到代理到达终止状态或达到最大时间步。在强化学习框架中，元组
    $(s_{t},a_{t},r_{t+1},s_{t+1})$ 称为转移。多个顺序转移通常被称为回滚。完整的序列 $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$
    被称为轨迹。理论上，轨迹是无限长的，但在大多数实际情况下具有 episodic 属性。某个有限长度 $\tau$ 的轨迹称为一个回合。对于给定的 MDP 和策略
    $\pi$，观察到 $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$ 的概率称为轨迹分布，记作：
- en: '|  | $\mathcal{T}_{\pi}=\prod_{t}{\pi(a_{t}&#124;s_{t})T(s_{t+1}&#124;s_{t},a_{t})}$
    |  | (5) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{T}_{\pi}=\prod_{t}{\pi(a_{t}|s_{t})T(s_{t+1}|s_{t},a_{t})}$
    |  | (5) |'
- en: 'The objective of RL is to find the optimal policy $\pi^{*}$ for the agent that
    maximizes the cumulative reward, which is called return. For every episode, the
    return is defined as the weighted sum of immediate rewards:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是为代理找到最优策略 $\pi^{*}$，该策略最大化累积奖励，这被称为回报。对于每个回合，回报被定义为即时奖励的加权和：
- en: '|  | $\mathcal{R}=\sum_{t=0}^{\tau-1}{\gamma^{t}r_{t+1}}$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}=\sum_{t=0}^{\tau-1}{\gamma^{t}r_{t+1}}$ |  | (6) |'
- en: 'Because the policy induces a trajectory distribution, the expected reward maximization
    can be written as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因为策略会引发一个轨迹分布，所以期望奖励最大化可以写作：
- en: '|  | $\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}{r_{t+1}}\rightarrow\max_{\pi}$
    |  | (7) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}{r_{t+1}}\rightarrow\max_{\pi}$
    |  | (7) |'
- en: 'Thus, given MDP and policy $\pi$, the discounted expected reward is defined:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定 MDP 和策略 $\pi$，折扣期望奖励被定义为：
- en: '|  | $\mathcal{G}(\pi)=\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}\gamma^{t}{r_{t+1}}$
    |  | (8) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}(\pi)=\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}\gamma^{t}{r_{t+1}}$
    |  | (8) |'
- en: The goal of RL is to find an optimal policy $\pi^{*}$, which maximizes the discounted
    expected reward, i.e. $\mathcal{G}(\pi)\rightarrow\max_{\pi}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是找到一个最优策略 $\pi^{*}$，使折扣期望奖励最大化，即 $\mathcal{G}(\pi)\rightarrow\max_{\pi}$。
- en: 3.2 Value and Q- functions
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 价值函数和 Q-函数
- en: 'The value function is applied to evaluate how good it is for an agent to utilize
    policy $\pi$ to visit state $s$. The concept of ”good” is defined in terms of
    expected return, i.e. future rewards that can be expected to receive in the future
    and it depends on what actions it will take. Mathematically, the value is the
    expectation of return, and value approximation is obtained by Bellman expectation
    equation as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数用于评估代理利用策略 $\pi$ 访问状态 $s$ 的好坏。这里的“好”是通过期望回报来定义的，即可以预期在未来收到的奖励，它取决于将采取的行动。数学上，价值是回报的期望值，价值的近似通过
    Bellman 期望方程得到，如下所示：
- en: '|  | $\begin{split}V^{\pi}(s_{t})=\mathbb{E}[r_{t+1}+\gamma V^{\pi}(s_{t+1})]\end{split}$
    |  | (9) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '$V^{\pi}(s_{t})$ is also known as state-value function, and the expectation
    term can be expanded as a product of policy, transition probability, and return
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}V^{\pi}(s_{t})=\sum_{a_{t}\in\mathcal{A}}{\pi(a_{t}&#124;s_{t})}\sum_{s_{t+1}\in
    S}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1})]}\end{split}$
    |  | (10) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: 'This equation is called the Bellman equation. When the agent always selects
    the action according to the optimal policy $\pi^{*}$ that maximizes the value,
    the Bellman equation can be expressed as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}V^{*}(s_{t})=&amp;\max_{a_{t}}\sum_{s_{t+1}\in S}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma
    V^{*}(s_{t+1})]}\\ \overset{\Delta}{=}&amp;\max_{a_{t}}Q^{*}(s_{t},a_{t})\end{split}$
    |  | (11) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: 'However, obtaining optimal value function $V^{*}$ does not provide enough information
    to reconstruct some optimal policy $\pi^{*}$ because the real-world environment
    is complicated. Thus, a quality function (Q-function) is also called the action-value
    function under policy $\pi$. The Q-function is used to estimate how good it is
    for an agent to perform a particular action ($a_{t}$) in a state ($s_{t}$) with
    a policy $\pi$ and it is introduced as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q^{\pi}(s_{t},a_{t})=\sum_{s_{t+1}}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma
    V^{\pi}(s_{t+1})]}$ |  | (12) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: Unlike value function which specifies the goodness of a state, a Q-function
    specifies the goodness of action in a state.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Category
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In general, RL can be divided into either model-free or model-based methods.
    Here, ”model” is defined by the two quantity: transition probability function
    $T(s_{t+1}|s_{t},a_{t})$ and the reward function $R(s_{t},s_{t+1})$.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Model-based RL
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Model-based RL is an approach that uses a learnt model, i.e. $T(s_{t+1}|s_{t},a_{t})$
    and reward function $R(s_{t},s_{t+1})$ to predict the future action. There are
    four main model-based techniques as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Value Function: The objective of value function methods is to obtain the best
    policy by maximizing the value functions in each state. A value function of a
    RL problem can be defined as in Eq.[10](#S3.E10 "In 3.2 Value and Q- functions
    ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey") and the optimal state-value function is given
    in Eq.[11](#S3.E11 "In 3.2 Value and Q- functions ‣ 3 Basics of Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    which are known as Bellman equations. Some common approaches in this group are
    Differential Dynamic Programming [[208](#bib.bib208)], [[266](#bib.bib266)], Temporal
    Difference Learning [[249](#bib.bib249)], Policy Iteration [[334](#bib.bib334)]
    and Monte Carlo [[137](#bib.bib137)].'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transition Models: Transition models decide how to map from a state s, taking
    action a to the next state (s’) and it strongly affects the performance of model-based
    RL algorithms. Based on whether predicting the future state s’ is based on the
    probability distribution of a random variable or not, there are two main approaches
    in this group: stochastic and deterministic. Some common methods for deterministic
    models are decision trees [[280](#bib.bib280)] and linear regression [[265](#bib.bib265)].
    Some common methods for stochastic models are Gaussian processes [[71](#bib.bib71)],
    [[1](#bib.bib1)], [[12](#bib.bib12)], Expectation-Maximization [[59](#bib.bib59)]
    and dynamic Bayesian networks [[280](#bib.bib280)].'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移模型：转移模型决定了如何从状态s，采取动作a到下一个状态（s’），它对基于模型的强化学习算法的性能有着重要影响。根据预测未来状态s’是否基于随机变量的概率分布，这个组别主要有两种方法：随机和确定性。一些常见的确定性模型方法有决策树
    [[280](#bib.bib280)] 和线性回归 [[265](#bib.bib265)]。一些常见的随机模型方法有高斯过程 [[71](#bib.bib71)]、[[1](#bib.bib1)]、[[12](#bib.bib12)]、期望最大化
    [[59](#bib.bib59)] 和动态贝叶斯网络 [[280](#bib.bib280)]。
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Policy Search: Policy search approach directly searches for the optimal policy
    by modifying its parameters, whereas the value function methods indirectly find
    the actions that maximize the value function at each state. Some of the popular
    approaches in this group are: gradient-based [[87](#bib.bib87)], [[267](#bib.bib267)],
    information theory [[1](#bib.bib1)], [[189](#bib.bib189)] and sampling based [[21](#bib.bib21)].'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 策略搜索：策略搜索方法通过修改参数直接搜索最优策略，而价值函数方法则间接找到在每个状态下最大化价值函数的动作。该组别中的一些流行方法有：基于梯度的 [[87](#bib.bib87)]、[[267](#bib.bib267)]、信息理论
    [[1](#bib.bib1)]、[[189](#bib.bib189)] 和基于采样的 [[21](#bib.bib21)]。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Return Functions: Return functions decide how to aggregate rewards or punishments
    over an episode. They affect both the convergence and the feasibility of the model.
    There are two main approaches in this group: discounted returns functions [[21](#bib.bib21)],
    [[75](#bib.bib75)], [[393](#bib.bib393)] and averaged returns functions [[34](#bib.bib34)],
    [[3](#bib.bib3)]. Between the two approaches, the former is the most popular which
    represents the uncertainty about future rewards. While small discount factors
    provide faster convergence, its solution may not be optimal.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回报函数：回报函数决定了如何在一个回合中汇总奖励或惩罚。它们影响模型的收敛性和可行性。这个组别主要有两种方法：折扣回报函数 [[21](#bib.bib21)]、[[75](#bib.bib75)]、[[393](#bib.bib393)]
    和平均回报函数 [[34](#bib.bib34)]、[[3](#bib.bib3)]。在这两种方法中，前者更为流行，它表示对未来奖励的不确定性。虽然小的折扣因子提供了更快的收敛速度，但其解可能并不是最优的。
- en: 'In practice, transition and reward functions are rarely known and hard to model.
    The comparative performance among all model-based techniques is reported in [[385](#bib.bib385)]
    with over 18 benchmarking environments including noisy environments. The Fig.[6](#S3.F6
    "Figure 6 ‣ 3.3.2 Model-free methods ‣ 3.3 Category ‣ 3 Basics of Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    summarizes different model-based RL approaches.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '实际中，转移和奖励函数很少被知道且难以建模。所有基于模型的技术的比较性能已在 [[385](#bib.bib385)] 中报告，包括超过18个基准环境（包括噪声环境）。图[6](#S3.F6
    "Figure 6 ‣ 3.3.2 Model-free methods ‣ 3.3 Category ‣ 3 Basics of Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    总结了不同的基于模型的强化学习方法。'
- en: 3.3.2 Model-free methods
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 无模型方法
- en: Learning through the experience gained from interactions with the environment,
    i.e. model-free method tries to estimate the t. discrete problems transition probability
    function and the reward function from the experience to exploit them in acquisition
    of policy. Policy gradient and value-based algorithms are popularly used in model-free
    methods.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与环境互动获得的经验进行学习，即无模型方法试图从经验中估计离散问题的转移概率函数和奖励函数，以利用它们来获取策略。策略梯度和基于价值的算法在无模型方法中被广泛使用。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The policy gradient methods: In this approach, RL task is considered as optimization
    with stochastic first-order optimization. Policy gradient methods directly optimize
    the discounted expected reward, i.e. $\mathcal{G}(\pi)\rightarrow\max_{\pi}$ to
    obtains the optimal policy $\pi^{*}$ without any additional information about
    MDP. To do so, approximate estimations of the gradient with respect to policy
    parameters are used. Take [[392](#bib.bib392)] as an example, policy gradient
    parameterizes the policy and updates parameters $\theta$,'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 策略梯度方法：在这种方法中，强化学习任务被视为带有随机一阶优化的优化问题。策略梯度方法直接优化折扣期望回报，即 $\mathcal{G}(\pi)\rightarrow\max_{\pi}$
    以获得最优策略 $\pi^{*}$，而无需任何有关 MDP 的额外信息。为此，使用相对于策略参数的梯度的近似估计。以 [[392](#bib.bib392)]
    为例，策略梯度参数化策略并更新参数 $\theta$，
- en: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\mathcal{R}}$
    |  | (13) |'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\mathcal{R}}$
    |  | (13) |'
- en: 'where $\mathcal{R}$ is the total accumulated return and defined in Eq. [6](#S3.E6
    "In 3.1 Markov Decision Process ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). Common used policies are
    Gibbs policies [[20](#bib.bib20)], [[352](#bib.bib352)] and Gaussian policies
    [[294](#bib.bib294)]. Gibbs policies are used in discrete problems whereas Gaussian
    policies are used in continuous problems.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{R}$ 是总累计回报，如公式 [6](#S3.E6 "在 3.1 马尔可夫决策过程 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：全面调查")
    所定义。常用的策略有 Gibbs 策略 [[20](#bib.bib20)]、[[352](#bib.bib352)] 和高斯策略 [[294](#bib.bib294)]。Gibbs
    策略用于离散问题，而高斯策略用于连续问题。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Value-based methods: In this approach, the optimal policy $\pi^{*}$ is implicitly
    conducted by gaining an approximation of optimal Q-function $Q^{*}(s,a)$. In value-based
    methods, agents update the value function to learn suitable policy while policy-based
    RL agents learn the policy directly. To do that, Q-learning is a typical value-based
    method. The update rule of Q-learning with learning rate $\lambda$ is defined
    as:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于价值的方法：在这种方法中，通过获得最优 Q 函数 $Q^{*}(s,a)$ 的近似来隐式地实现最优策略 $\pi^{*}$。在基于价值的方法中，智能体更新价值函数以学习合适的策略，而基于策略的强化学习智能体直接学习策略。为此，Q
    学习是典型的基于价值的方法。Q 学习的更新规则与学习率 $\lambda$ 定义为：
- en: '|  | $Q(s_{t},a_{t})=Q(s_{t},a_{t})+\lambda\delta_{t}$ |  | (14) |'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})=Q(s_{t},a_{t})+\lambda\delta_{t}$ |  | (14) |'
- en: where $\delta_{t}=R(s_{t},s_{t+1})+\gamma\text{arg}\max_{a}{Q(s_{t+1},a)-Q(s_{t},a)}$
    is the temporal difference (TD) error.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\delta_{t}=R(s_{t},s_{t+1})+\gamma\text{arg}\max_{a}{Q(s_{t+1},a)-Q(s_{t},a)}$
    是时序差分（TD）误差。
- en: Target at self-play Chess, [[394](#bib.bib394)] investigates inasmuch it is
    possible to leverage the qualitative feedback for learning an evaluation function
    for the game. [[319](#bib.bib319)] provides the comparison of learning of linear
    evaluation functions between using preference learning and using least-squares
    temporal difference learning, from samples of game trajectories. The value-based
    methods depend on a specific, optimal policy, thus it is hard for transfer learning.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以自我博弈象棋为目标，[[394](#bib.bib394)] 研究了是否可以利用定性反馈来学习游戏的评估函数。[[319](#bib.bib319)]
    提供了使用偏好学习和使用最小二乘时序差分学习在游戏轨迹样本中学习线性评估函数的比较。基于价值的方法依赖于特定的最优策略，因此转移学习较为困难。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Actor-critic is an improvement of policy gradient with an value-based critic
    $\Gamma$, thus, Eq.[13](#S3.E13 "In 1st item ‣ 3.3.2 Model-free methods ‣ 3.3
    Category ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey") is rewritten as:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Actor-critic 是一种改进的策略梯度方法，具有基于价值的评论员 $\Gamma$，因此，公式 [13](#S3.E13 "在第 1 项 ‣ 3.3.2
    无模型方法 ‣ 3.3 类别 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：全面调查") 被重写为：
- en: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\Gamma_{t}}$
    |  | (15) |'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\Gamma_{t}}$
    |  | (15) |'
- en: The critic function $\Gamma$ can be defined as $Q^{\pi}(s_{t},a_{t})$ or $Q^{\pi}(s_{t},a_{t})-V^{\pi}_{t}$
    or $R[s_{t-1},s_{t}]+V^{\pi}_{t+1}-V^{\pi}_{t}$
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评论员函数 $\Gamma$ 可以定义为 $Q^{\pi}(s_{t},a_{t})$ 或 $Q^{\pi}(s_{t},a_{t})-V^{\pi}_{t}$
    或 $R[s_{t-1},s_{t}]+V^{\pi}_{t+1}-V^{\pi}_{t}$
- en: 'Actor-critic methods are combinations of actor-only methods and critic-only
    methods. Thus, actor-critic methods have been commonly used RL. Depend on reward
    setting, there are two groups of actor-critic methods, namely discounted return
    [[282](#bib.bib282)], [[30](#bib.bib30)] and average return [[289](#bib.bib289)],
    [[31](#bib.bib31)]. The comparison between model-based and model-free methods
    is given in Table [1](#S3.T1 "Table 1 ‣ 3.3.2 Model-free methods ‣ 3.3 Category
    ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic方法是仅有演员方法和仅有评论员方法的组合。因此，actor-critic方法通常用于RL。根据奖励设置，有两组actor-critic方法，即折扣回报[[282](#bib.bib282)]、[[30](#bib.bib30)]和平均回报[[289](#bib.bib289)]、[[31](#bib.bib31)]。基于模型和无模型方法的比较见表[1](#S3.T1
    "表 1 ‣ 3.3.2 无模型方法 ‣ 3.3 类别 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：全面综述")。
- en: '{forest}'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: '[Model-based RL [ Value Functions [ Differential Dynamic Programming [[208](#bib.bib208)],
    [[266](#bib.bib266)] ] [ Temporal Difference Learning [[249](#bib.bib249)] ] [
    Policy Iteration [[334](#bib.bib334)] ] [ Monte Carlo [[137](#bib.bib137)] ] ]
    [ Transition Models [ Deterministic models [ Decision trees [[280](#bib.bib280)]
    ] [ Linear regression [[265](#bib.bib265)] ] ] [ Stochastic models [ Gaussian
    processes [[71](#bib.bib71)], [[1](#bib.bib1)], [[12](#bib.bib12)] ] [ Expectation-Maximization
    [[59](#bib.bib59)] ] [ Dynamic Bayesian networks [[280](#bib.bib280)] ] ] ] [
    Policy Search [ Gradient-based [[87](#bib.bib87)], [[267](#bib.bib267)] ] [ Information
    theory [[1](#bib.bib1)], [[189](#bib.bib189)] ] [ Sampling based [[21](#bib.bib21)]
    ] ] [ Return Functions [ Discounted returns functions [[21](#bib.bib21)], [[75](#bib.bib75)],
    [[393](#bib.bib393)] ] [ Averaged returns functions [[34](#bib.bib34)], [[3](#bib.bib3)]
    ] ] ] ] ]'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[基于模型的RL [ 价值函数 [ 微分动态规划 [[208](#bib.bib208)], [[266](#bib.bib266)] ] [ 时序差分学习
    [[249](#bib.bib249)] ] [ 策略迭代 [[334](#bib.bib334)] ] [ 蒙特卡罗 [[137](#bib.bib137)]
    ] ] [ 转移模型 [ 确定性模型 [ 决策树 [[280](#bib.bib280)] ] [ 线性回归 [[265](#bib.bib265)] ]
    ] [ 随机模型 [ 高斯过程 [[71](#bib.bib71)], [[1](#bib.bib1)], [[12](#bib.bib12)] ] [ 期望最大化
    [[59](#bib.bib59)] ] [ 动态贝叶斯网络 [[280](#bib.bib280)] ] ] ] [ 策略搜索 [ 基于梯度 [[87](#bib.bib87)],
    [[267](#bib.bib267)] ] [ 信息理论 [[1](#bib.bib1)], [[189](#bib.bib189)] ] [ 基于采样
    [[21](#bib.bib21)] ] ] [ 回报函数 [ 折扣回报函数 [[21](#bib.bib21)], [[75](#bib.bib75)],
    [[393](#bib.bib393)] ] [ 平均回报函数 [[34](#bib.bib34)], [[3](#bib.bib3)] ] ] ] ] ]'
- en: 'Figure 6: Summarization of model-based RL approaches'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于模型的RL方法总结
- en: 'Table 1: Comparison between model-based RL and model-free RL'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：基于模型的RL和无模型RL的比较
- en: '| Factors | Model-based RL | Model-free RL |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 因素 | 基于模型的RL | 无模型RL |'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number of iterations between &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迭代次数之间 &#124;'
- en: '&#124; agent and environment &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代理和环境 &#124;'
- en: '| Small | Big |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 小 | 大 |'
- en: '| Convergence | Fast | Slow |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 收敛性 | 快 | 慢 |'
- en: '| Prior knowledge of transitions | Yes | No |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 过渡的先验知识 | 是 | 否 |'
- en: '| Flexibility |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 灵活性 |'
- en: '&#124; Strongly depend on &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 强烈依赖于 &#124;'
- en: '&#124; a learnt model &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个学习的模型 &#124;'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Adjust based &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于调整 &#124;'
- en: '&#124; on trials and errors &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于试验和错误 &#124;'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 4 Introduction to Deep Reinforcement Learning
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度强化学习简介
- en: DRL, which was proposed as a combination of RL and DL, has achieved rapid developments,
    thanks to the rich context representation of DL. Under DRL, the aforementioned
    value and policy can be expressed by neural networks which allow dealing with
    a continuous state or action that was hard for a table representation. Similar
    to RL, DRL can be categorized into model-based algorithms and model-free algorithms
    which will be introduced in this section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: DRL作为RL和DL的结合体得到了迅速的发展，这得益于DL的丰富上下文表示。在DRL中，前述的价值和策略可以通过神经网络表达，从而处理以前难以用表格表示的连续状态或动作。类似于RL，DRL可以分为基于模型的算法和无模型算法，本节将介绍这两类算法。
- en: 4.1 Model-Free Algorithms
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 无模型算法
- en: There are two approaches, namely, Value-based DRL methods and Policy gradient
    DRL methods to implement model-free algorithms.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法，即基于价值的DRL方法和策略梯度DRL方法，用于实现无模型算法。
- en: 4.1.1 Value-based DRL methods
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 基于价值的DRL方法
- en: 'Deep Q-Learning Network (DQN): Deep Q-learning [[264](#bib.bib264)] (DQN) is
    the most famous DRL model which learns policies directly from high-dimensional
    inputs by CNNs. In DQN, input is raw pixels and output is a quality function to
    estimate future rewards as given in Fig.[7](#S4.F7 "Figure 7 ‣ 4.1.1 Value-based
    DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    Take regression problem as an instance. Let $y$ denote the target of our regression
    task, the regression with input $(s,a)$, target $y(s,a)$ and the MSE loss function
    is as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '深度 Q 学习网络 (DQN)：深度 Q 学习[[264](#bib.bib264)](DQN)是最著名的深度强化学习模型，它通过 CNN 直接从高维输入中学习策略。在
    DQN 中，输入是原始像素，输出是一个质量函数，用于估计未来的奖励，如图[7](#S4.F7 "Figure 7 ‣ 4.1.1 Value-based DRL
    methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement Learning
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")中所示。以回归问题为例。让
    $y$ 表示我们回归任务的目标，对于输入 $(s,a)$ 的回归，目标 $y(s,a)$ 和均方误差损失函数为：'
- en: '|  | <math   alttext="\begin{split}\mathcal{L^{DQN}}&amp;=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &amp;=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\mathcal{L^{DQN}}&amp;=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &amp;=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\ '
- en: y(s_{t},a_{t})&amp;=R(s_{t},s_{t+1})+\gamma\max_{a_{t+1}}Q^{*}(s_{t_{1}},a_{t+1},\theta_{t})\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="right"  ><msup ><mi >ℒ</mi><mrow ><mi >𝒟</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >𝒬</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >𝒩</mi></mrow></msup></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mrow ><mi >ℒ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi  >y</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo><mrow ><msup ><mi  >Q</mi><mo >∗</mo></msup><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >θ</mi><mi >t</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><msup ><mrow ><mo stretchy="false"
    >‖</mo><mrow ><mrow ><mi  >y</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >−</mo><mrow ><msup ><mi >Q</mi><mo >∗</mo></msup><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >θ</mi><mi >t</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >2</mn></msup></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >y</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mrow ><mrow  ><mi >R</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo
    stretchy="false" >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >γ</mi><mo lspace="0.167em"
    rspace="0em" >​</mo><mrow ><munder ><mi >max</mi><msub ><mi >a</mi><mrow ><mi
    >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"  >⁡</mo><msup
    ><mi >Q</mi><mo >∗</mo></msup></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><msub ><mi >t</mi><mn >1</mn></msub></msub><mo
    >,</mo><msub ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo
    >,</mo><msub ><mi  >θ</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℒ</ci><apply ><ci  >𝒟</ci><ci >𝒬</ci><ci >𝒩</ci></apply></apply><apply ><ci >ℒ</ci><interval
    closure="open" ><apply  ><ci >𝑦</ci><interval closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci  >𝑡</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply></interval></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑄</ci></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑡</ci></apply></vector></apply></interval></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="latexml" >norm</csymbol><apply ><apply ><ci  >𝑦</ci><interval closure="open"  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply></interval></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑄</ci></apply><vector
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑡</ci></apply></vector></apply></apply></apply><cn
    type="integer"  >2</cn></apply><ci >𝑦</ci><interval closure="open"  ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply></interval></apply></apply><apply
    ><apply ><apply  ><ci >𝑅</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >𝛾</ci><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑄</ci></apply></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><ci >𝑡</ci></apply></vector></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{L^{DQN}}&=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\ y(s_{t},a_{t})&=R(s_{t},s_{t+1})+\gamma\max_{a_{t+1}}Q^{*}(s_{t_{1}},a_{t+1},\theta_{t})\end{split}</annotation></semantics></math>
    |  | (16) |
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Where $\theta$ is vector of parameters, $\theta\in\mathbb{R}^{|S||R|}$ and $s_{t+1}$
    is a sample from $T(s_{t+1}|s_{t},a_{t})$ with input of $(s_{t},a_{t})$.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing the loss function yields a gradient descent step formula to update
    $\theta$ as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{t+1}=\theta_{t}-\alpha_{t}\frac{\mathcal{\partial L^{DQN}}}{\partial\theta}$
    |  | (17) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/72d78e811fd5b99100e03f51dd288fd9.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Network structure of Deep Q-Network (DQN), where Q-values Q(s,a)
    are generated for all actions for a given state.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Double DQN: In DQN, the values of $Q^{*}$ in many domains were leading to overestimation
    because of $max$. In Eq.[16](#S4.E16 "In 4.1.1 Value-based DRL methods ‣ 4.1 Model-Free
    Algorithms ‣ 4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), $y(s,a)=R(s,s^{\prime})+\gamma\max_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta)$
    shifts Q-value estimation towards either to the actions with high reward or to
    the actions with overestimating approximation error. Double DQN [[370](#bib.bib370)]
    is an improvement of DQN that combines double Q-learning [[130](#bib.bib130)]
    with DQN and it aims at reducing observed overestimation with better performance.
    The idea of Double DQN is based on separating action selection and action evaluation
    using its own approximation of $Q^{*}$ as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\max_{a_{t+1}}Q^{*}(s_{t+1},a_{t+1};\theta)=Q^{*}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}$
    |  | (18) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: Thus
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=R(s_{t},s_{t+1})+\gamma Q^{*}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})$
    |  | (19) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: 'The easiest and most expensive implementation of double DQN is to run two independent
    DQNs as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}y_{1}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{1}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{2}(s_{t+1},a_{t+1};\theta_{2});\theta_{1})\\'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: y_{2}=R(s_{t},s_{t+1})+\\
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: \gamma Q^{*}_{2}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{1}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><msub  ><mi >y</mi><mn >1</mn></msub><mo >=</mo><mrow
    ><mrow  ><mi >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi  >s</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >γ</mi><mo
    lspace="0em" rspace="0em" >​</mo><msubsup ><mi  >Q</mi><mn >1</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><munder accentunder="true"
    ><mrow  ><mi >arg</mi><mo lspace="0.167em"  >⁡</mo><mi >max</mi></mrow><msub ><mi
    >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"
    rspace="0em"  >​</mo><msubsup ><mi >Q</mi><mn >2</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi
    >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >;</mo><msub
    ><mi  >θ</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >;</mo><msub ><mi  >θ</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msub ><mi  >y</mi><mn >2</mn></msub><mo >=</mo><mrow
    ><mrow ><mi  >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi  >s</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >γ</mi><mo
    lspace="0em" rspace="0em" >​</mo><msubsup ><mi  >Q</mi><mn >2</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><munder accentunder="true"
    ><mrow  ><mi >arg</mi><mo lspace="0.167em"  >⁡</mo><mi >max</mi></mrow><msub ><mi
    >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"
    rspace="0em"  >​</mo><msubsup ><mi >Q</mi><mn >1</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi
    >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >;</mo><msub
    ><mi  >θ</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >;</mo><msub ><mi  >θ</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑦</ci><cn type="integer" >1</cn></apply><apply ><apply ><ci  >𝑅</ci><interval
    closure="open"  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><apply
    ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >𝛾</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝑄</ci></apply><cn type="integer" >1</cn></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><apply ><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑄</ci></apply><cn type="integer"  >2</cn></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><cn type="integer" >2</cn></apply></vector></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜃</ci><cn type="integer" >1</cn></apply></vector><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑦</ci><cn type="integer" >2</cn></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci >𝑅</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >𝛾</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝑄</ci></apply><cn type="integer" >2</cn></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><apply ><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑄</ci></apply><cn type="integer"  >1</cn></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><cn type="integer" >1</cn></apply></vector></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜃</ci><cn type="integer" >2</cn></apply></vector></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}y_{1}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{1}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{2}(s_{t+1},a_{t+1};\theta_{2});\theta_{1})\\
    y_{2}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{2}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{1}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}</annotation></semantics></math>
    |  | (20) |
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Dueling DQN: In DQN, when the agent visits an unfavorable state, instead of
    lowering its value $V^{*}$, it remembers only low pay-off by updating $Q^{*}$.
    In order to address this limitation, Dueling DQN [[390](#bib.bib390)] incorporates
    approximation of $V^{*}$ explicitly in a computational graph by introducing an
    advantage function as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对偶DQN：在DQN中，当代理访问一个不利状态时，不是降低其值$V^{*}$，而是通过更新$Q^{*}$只记住低回报。为了应对这一限制，对偶DQN[[390](#bib.bib390)]通过在计算图中显式地引入优势函数来近似$V^{*}$，如下所示：
- en: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (21) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (21) |'
- en: 'Therefore, we can reformulate Q-value: $Q^{*}(s,a)=A^{*}(s,a)+V^{*}(s)$. This
    implies that after DL the feature map is decomposed into two parts corresponding
    to $V^{*}(v)$ and $A^{*}(s,a)$ as illustrated in Fig.[8](#S4.F8 "Figure 8 ‣ 4.1.1
    Value-based DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    This can be implemented by splitting the fully connected layers in the DQN architecture
    to compute the advantage and state value functions separately, then combining
    them back into a single Q-function. An interesting result has shown that Dueling
    DQN obtains better performance if it is formulated as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以重新表述Q值：$Q^{*}(s,a)=A^{*}(s,a)+V^{*}(s)$。这意味着在深度学习后，特征图被分解为两个部分，分别对应$V^{*}(v)$和$A^{*}(s,a)$，如图[8](#S4.F8
    "图 8 ‣ 4.1.1 基于价值的深度强化学习方法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介 ‣ 深度强化学习在计算机视觉中的应用：综合调查")所示。这可以通过将DQN架构中的全连接层分开来计算优势函数和状态值函数，然后将它们组合回一个单一的Q函数来实现。有趣的结果表明，如果将对偶DQN表述为：
- en: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\max_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  | (22) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\max_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  | (22) |'
- en: In practical implementation, averaging instead of maximum is used, i.e.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现中，使用的是平均而不是最大值，即
- en: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\text{mean}_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\text{mean}_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  |'
- en: Furthermore, to address the limitation of memory and imperfect information at
    each decision point, Deep Recurrent Q-Network (DRQN) [[131](#bib.bib131)] employed
    RNNs into DQN by replacing the first fully-connected layer with an RNN. Multi-step
    DQN [[68](#bib.bib68)] is one of the most popular improvements of DQN by substituting
    one-step approximation with N-steps.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了应对在每个决策点的记忆限制和不完全信息，深度递归Q网络（DRQN）[[131](#bib.bib131)]通过用RNN替换第一个全连接层将RNN引入DQN。多步DQN
    [[68](#bib.bib68)]是DQN最受欢迎的改进之一，通过用N步替代一步近似。
- en: '![Refer to caption](img/e873b6469a95ee4e6160354d4267e4e2.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/e873b6469a95ee4e6160354d4267e4e2.png)'
- en: 'Figure 8: Network structure of Dueling DQN, where value function $V(s)$ and
    advantage function $A(s,a)$ are combined to predict Q-values $Q(s,a)$ for all
    actions for a given state.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：对偶DQN的网络结构，其中价值函数$V(s)$和优势函数$A(s,a)$被组合以预测给定状态下所有动作的Q值$Q(s,a)$。
- en: 4.1.2 Policy gradient DRL methods
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 策策渐进深度强化学习方法
- en: 'Policy Gradient Theorem: Different from value-based DRL methods, policy gradient
    DRL optimizes the policy directly by optimizing the following objective function
    which is defined as a function of $\theta$.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 策策渐进定理：与基于价值的深度强化学习方法不同，策略梯度深度强化学习通过优化以下目标函数直接优化策略，该函数被定义为$\theta$的函数。
- en: '|  | $\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=1}{\gamma^{t-1}R(s_{t-1},s_{t})}\rightarrow\max_{\theta}$
    |  | (23) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=1}{\gamma^{t-1}R(s_{t-1},s_{t})}\rightarrow\max_{\theta}$
    |  | (23) |'
- en: 'For any MDP and differentiable policy $\pi_{\theta}$, the gradient of objective
    Eq.[23](#S4.E23 "In 4.1.2 Policy gradient DRL methods ‣ 4.1 Model-Free Algorithms
    ‣ 4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey") is defined by policy gradient theorem
    [[353](#bib.bib353)] as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何MDP和可微分策略$\pi_{\theta}$，目标方程 Eq.[23](#S4.E23 "在4.1.2 策策渐进深度强化学习方法 ‣ 4.1
    无模型算法 ‣ 4 深度强化学习简介 ‣ 深度强化学习在计算机视觉中的应用：综合调查")的梯度由策略梯度定理[[353](#bib.bib353)]定义如下：
- en: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=0}{\gamma^{t}Q^{\pi}(s_{t},a_{t})\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})}$
    |  | (24) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=0}{\gamma^{t}Q^{\pi}(s_{t},a_{t})\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}|s_{t})}$
    |  | (24) |'
- en: 'REINFORCE: REINFORCE was introduced by [[392](#bib.bib392)] to approximately
    calculate the gradient in Eq.[24](#S4.E24 "In 4.1.2 Policy gradient DRL methods
    ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement Learning ‣
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") by using
    Monte-Carlo estimation. In REINFORCE approximate estimator, Eq.[24](#S4.E24 "In
    4.1.2 Policy gradient DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey") is reformulated as:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'REINFORCE: REINFORCE 是由 [[392](#bib.bib392)] 引入的，用于通过蒙特卡洛估计近似计算 Eq.[24](#S4.E24
    "在 4.1.2 策略梯度 DRL 方法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介 ‣ 计算机视觉中的深度强化学习：全面综述") 中的梯度。在 REINFORCE
    近似估计器中，Eq.[24](#S4.E24 "在 4.1.2 策略梯度 DRL 方法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介 ‣ 计算机视觉中的深度强化学习：全面综述")
    被重新表述为：'
- en: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)\approx\sum_{\mathcal{T}}^{N}\sum_{t=0}{\gamma^{t}\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})(\sum_{t^{\prime}=t}{\gamma^{t^{\prime}-t}R(s_{t^{\prime}},s_{t^{\prime}+1})})}$
    |  | (25) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)\approx\sum_{\mathcal{T}}^{N}\sum_{t=0}{\gamma^{t}\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})(\sum_{t^{\prime}=t}{\gamma^{t^{\prime}-t}R(s_{t^{\prime}},s_{t^{\prime}+1})})}$
    |  | (25) |'
- en: 'where $\mathcal{T}$ is trajectory distribution and defined in Eq.[5](#S3.E5
    "In 3.1 Markov Decision Process ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). Theoretically, REINFORCE
    can be straightforwardly applied into any parametric $\pi_{theta}(a|s)$. However,
    it is impractical to use because of its time-consuming nature for convergence
    and local optimums problem. Based on the observation that the convergence rate
    of stochastic gradient descent directly depends on the variance of gradient estimation,
    the variance reduction technique was proposed to address naive REINFORCE’s limitations
    by adding a term that reduces the variance without affecting the expectation.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{T}$ 是轨迹分布，在 Eq.[5](#S3.E5 "在 3.1 马尔可夫决策过程 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：全面综述")
    中定义。从理论上讲，REINFORCE 可以直接应用于任何参数化的 $\pi_{\theta}(a|s)$。然而，由于收敛的时间消耗和局部最优问题，它在实际使用中并不实用。基于随机梯度下降的收敛速度直接依赖于梯度估计的方差的观察，提出了方差减少技术来解决原始
    REINFORCE 的局限性，通过添加一个项来减少方差而不影响期望值。
- en: 4.1.3 Actor-Critic DRL algorithm
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 Actor-Critic DRL 算法
- en: 'Both value-based and policy gradient algorithms have their own pros and cons,
    i.e. policy gradient methods are better for continuous and stochastic environments,
    and have a faster convergence whereas, value-based methods are more sample efficient
    and steady. Lately, actor-critic [[182](#bib.bib182)] [[262](#bib.bib262)] was
    born to take advantage from both value-based and policy gradient while limiting
    their drawbacks. Actor-critic architecture computes the policy gradient using
    a value-based critic function to estimate expected future reward. The principal
    idea of actor-critic is to divide the model into two parts: (i) computing an action
    based on a state and (ii) producing the Q values of the action. As given in Fig.[9](#S4.F9
    "Figure 9 ‣ 4.1.3 Actor-Critic DRL algorithm ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey"), the actor takes as input the state $s_{t}$ and outputs
    the best action $a_{t}$. It essentially controls how the agent behaves by learning
    the optimal policy (policy-based). The critic, on the other hand, evaluates the
    action by computing the value function (value-based). The most basic actor-critic
    method (beyond the tabular case) is naive policy gradients (REINFORCE). The relationship
    between actor-critic is similar to kid-mom. The kid (actor) explores the environment
    around him/her with new actions i.e. tough fire, hit a wall, climb a tree, etc
    while the mom (critic) watches the kid and criticizes/compliments him/her. The
    kid then adjusts his/her behavior based on what his/her mom told. When the kids
    get older, he/she can realize which action is bad/good.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的方法和策略梯度算法各有优缺点，即策略梯度方法在连续和随机环境中表现更好，且收敛速度更快，而基于价值的方法则在样本效率和稳定性上更为出色。最近，演员-评论家算法
    [[182](#bib.bib182)] [[262](#bib.bib262)] 的出现融合了基于价值和策略梯度方法的优点，同时限制了它们的缺点。演员-评论家架构通过使用基于价值的评论家函数来估计期望的未来奖励，从而计算策略梯度。演员-评论家的主要思想是将模型分为两个部分：（i）基于状态计算动作和（ii）生成动作的
    Q 值。如图 [9](#S4.F9 "图 9 ‣ 4.1.3 演员-评论家 DRL 算法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介 ‣ 计算机视觉中的深度强化学习：全面综述")
    所示，演员以状态 $s_{t}$ 为输入，输出最佳动作 $a_{t}$。它本质上通过学习最优策略（基于策略）来控制代理的行为。而评论家则通过计算价值函数（基于价值）来评估动作。最基本的演员-评论家方法（超出表格情况）是朴素策略梯度（REINFORCE）。演员-评论家的关系类似于孩子和母亲。孩子（演员）通过新的动作探索周围环境，即经历困难的火灾、撞墙、爬树等，而母亲（评论家）则观察孩子并对其进行批评或赞扬。孩子随后根据母亲的反馈调整自己的行为。当孩子长大后，他/她能够意识到哪些动作是好或坏的。
- en: '![Refer to caption](img/8fdb0e2aceef831039655bd509b14e01.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8fdb0e2aceef831039655bd509b14e01.png)'
- en: 'Figure 9: Flowchart showing the structure of actor critic algorithm.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：显示演员-评论家算法结构的流程图。
- en: '![Refer to caption](img/13da43e1b6315b61af979883bf93af15.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/13da43e1b6315b61af979883bf93af15.png)'
- en: 'Figure 10: An illustration of Actor-Critic algorithm in two cases: sharing
    parameters (a) and not sharing parameters (b).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：演员-评论家算法在两种情况下的示意图：共享参数 (a) 和不共享参数 (b)。
- en: 'Advantage Actor-Critic (A2C) Advantage Actor-Critic (A2C) [[263](#bib.bib263)]
    consist of two neural networks i.e. actor network $\pi_{\theta}(a_{t}|s_{t})$
    representing for policy and critic network $V^{\pi}_{\omega}$ with parameters
    $\omega$ approximately estimating actor’s performance. In order to determine how
    much better, it is to take a specific action compared to the average, an advantage
    value is defined as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 优势演员-评论家（A2C）优势演员-评论家（A2C） [[263](#bib.bib263)] 包含两个神经网络，即代表策略的演员网络 $\pi_{\theta}(a_{t}|s_{t})$
    和带有参数 $\omega$ 的评论家网络 $V^{\pi}_{\omega}$，后者大致估计演员的表现。为了确定采取特定动作相对于平均水平有多大优势，定义了优势值为：
- en: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (26) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (26) |'
- en: 'Instead of constructing two neural networks for both the Q value and the V
    value, using the Bellman optimization equation, we can rewrite the advantage function
    as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与其为 Q 值和 V 值分别构建两个神经网络，不如使用贝尔曼优化方程，我们可以将优势函数重写为：
- en: '|  | $A^{\pi}(s_{t},a_{t})=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})-V^{\pi}_{\omega}(s_{t})$
    |  | (27) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})-V^{\pi}_{\omega}(s_{t})$
    |  | (27) |'
- en: 'For given policy $\pi$, its value function can be obtained using point iteration
    for solving:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的策略 $\pi$，其价值函数可以通过点迭代来求解：
- en: '|  | $V^{\pi}(s_{t})=\mathbb{E}_{a_{t}\sim\pi(a_{t}&#124;s_{t})}\mathbb{E}_{s_{t+1}\sim
    T(s_{t+1}&#124;a_{t},s_{t})}(R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1}))$ |  | (28)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $V^{\pi}(s_{t})=\mathbb{E}_{a_{t}\sim\pi(a_{t}&#124;s_{t})}\mathbb{E}_{s_{t+1}\sim
    T(s_{t+1}&#124;a_{t},s_{t})}(R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1}))$ |  | (28)
    |'
- en: 'Similar to DQN, on each update a target is computed using current approximation:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 DQN，在每次更新时，使用当前近似值计算目标：
- en: '|  | $y=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})$ |  | (29) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})$ |  | (29) |'
- en: 'At time step t, the A2C algorithm can be implemented as following steps:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 t，A2C 算法可以按以下步骤实施：
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 1: Compute advantage function using Eq.[27](#S4.E27 "In 4.1.3 Actor-Critic
    DRL algorithm ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一步：使用 Eq.[27](#S4.E27 "在 4.1.3 Actor-Critic DRL 算法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介
    ‣ 深度强化学习在计算机视觉中的应用：综合调查") 计算优势函数。
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 2: Compute target using Eq.[29](#S4.E29 "In 4.1.3 Actor-Critic DRL algorithm
    ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement Learning ‣
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二步：使用 Eq.[29](#S4.E29 "在 4.1.3 Actor-Critic DRL 算法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介
    ‣ 深度强化学习在计算机视觉中的应用：综合调查") 计算目标。
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 3: Compute critic loss with MSE loss: $\mathcal{L}=\frac{1}{B}\sum_{T}||y-V^{\pi}(s_{t}))||^{2}$,
    where $B$ is batch size and $V^{\pi}(s_{t})$ is defined in Eq.[28](#S4.E28 "In
    4.1.3 Actor-Critic DRL algorithm ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey").'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三步：用 MSE 损失计算评论家损失：$\mathcal{L}=\frac{1}{B}\sum_{T}||y-V^{\pi}(s_{t}))||^{2}$，其中
    $B$ 是批量大小，$V^{\pi}(s_{t})$ 在 Eq.[28](#S4.E28 "在 4.1.3 Actor-Critic DRL 算法 ‣ 4.1
    无模型算法 ‣ 4 深度强化学习简介 ‣ 深度强化学习在计算机视觉中的应用：综合调查") 中定义。
- en: •
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 4: Compute critic gradient: $\bigtriangledown^{critic}=\frac{\partial\mathcal{L}}{\partial\omega}$.'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第四步：计算评论家梯度：$\bigtriangledown^{critic}=\frac{\partial\mathcal{L}}{\partial\omega}$。
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 5: Compute actor gradient: $\bigtriangledown^{actor}=\frac{1}{B}\sum_{T}{\bigtriangledown_{\theta}\text{log}\pi(a_{t}|s_{t})A^{\pi}(s_{t},a_{t})}$'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第五步：计算演员梯度：$\bigtriangledown^{actor}=\frac{1}{B}\sum_{T}{\bigtriangledown_{\theta}\text{log}\pi(a_{t}|s_{t})A^{\pi}(s_{t},a_{t})}$
- en: 'Asynchronous Advantage Actor Critic (A3C) Besides A2C, there is another strategy
    to implement an Actor-Critic agent. Asynchronous Advantage Actor-Critic (A3C)
    [[263](#bib.bib263)] approach does not use experience replay because this requires
    a lot of memory. Instead, A3C asynchronously executes different agents in parallel
    on multiple instances of the environment. Each worker (copy of the network) will
    update the global network asynchronously. Because of the asynchronous nature of
    A3C, some workers (copy of the agents) will work with older values of the parameters.
    Thus the aggregating update will not be optimal. On the other hand, A2C synchronously
    updates the global network. A2C waits until all workers finished their training
    and calculated their gradients to average them, to update the global network.
    In order to update the entire network, A2C waits for each actor to finish their
    segment of experience before updating the global parameters. As a consequence,
    the training will be more cohesive and faster. Different from A3C, each worker
    in A2C has the same set of weights since and A2C updates all their workers at
    the same time. In short, A2C is an alternative to the synchronous version of the
    A3C. In A2C, it waits for each actor to finish its segment of experience before
    updating, averaging over all of the actors. In a practical experiment, this implementation
    is more effectively uses GPUs due to larger batch sizes. The structure of an actor-critic
    algorithm can be divided into two types depending on parameter sharing as illustrated
    in Fig.[10](#S4.F10 "Figure 10 ‣ 4.1.3 Actor-Critic DRL algorithm ‣ 4.1 Model-Free
    Algorithms ‣ 4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '异步优势演员-评论家（A3C）除了 A2C，还有另一种策略来实现一个演员-评论家代理。异步优势演员-评论家（A3C）[[263](#bib.bib263)]
    方法不使用经验回放，因为这需要大量的内存。相反，A3C 异步地在多个环境实例上并行执行不同的代理。每个工作者（网络的副本）将异步更新全局网络。由于 A3C
    的异步特性，一些工作者（代理的副本）将使用旧的参数值。因此，聚合更新将不是最优的。另一方面，A2C 同步更新全局网络。A2C 等待所有工作者完成他们的训练并计算其梯度以进行平均，然后更新全局网络。为了更新整个网络，A2C
    等待每个演员完成其经验段后再更新全局参数。因此，训练将更具凝聚力且更快。与 A3C 不同的是，A2C 中的每个工作者都有相同的权重，因为 A2C 同时更新所有工作者。简而言之，A2C
    是 A3C 同步版本的替代方案。在 A2C 中，它等待每个演员完成其经验段后进行更新，对所有演员进行平均。在实际实验中，由于批量大小较大，这种实现更有效地使用了
    GPU。演员-评论家算法的结构可以根据参数共享分为两种类型，如图[10](#S4.F10 "Figure 10 ‣ 4.1.3 Actor-Critic DRL
    algorithm ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement Learning
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") 所示。'
- en: In order to overcome the limitation of speed, GA3C [[16](#bib.bib16)] was proposed
    and it achieved a significant speedup compared to the original CPU implementation.
    To more effectively train A3C, [[141](#bib.bib141)] proposed FFE which forces
    random exploration at the right time during a training episode, that can lead
    to improved training performance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服速度的限制，GA3C [[16](#bib.bib16)] 被提出，并且与原始的 CPU 实现相比，实现了显著的加速。为了更有效地训练 A3C，[[141](#bib.bib141)]
    提出了 FFE，这种方法在训练过程中强制在合适的时间进行随机探索，这可以提高训练性能。
- en: 4.2 Model-Based Algorithms
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于模型的算法
- en: 'We have discussed so far model-free methods including the value-based approach
    and policy gradient approach. In this section, we focus on the model-based approach,
    that deals with the dynamics of the environment by learning a transition model
    that allows for simulation of the environment without interacting with the environment
    directly. In contrast to model-free approaches, model-based approaches are learned
    from experience by a function approximation. Theoretically, no specific prior
    knowledge is required in model-based RL/DRL but incorporating prior knowledge
    can help faster convergence and better-trained model, speed up training time as
    well as the number of training samples. While using raw data with pixel, it is
    difficult for model-based RL to work on high dimensional and dynamic environments.
    This is addressed in DRL by embedding the high-dimensional observations into a
    lower-dimensional space using autoencoders [[95](#bib.bib95)]. Many DRL approaches
    have been based on scaling up prior work in RL to high-dimensional problems. A
    good overview of model-based RL for high-dimensional problems can be found in
    [[297](#bib.bib297)] which partition model-based DRL into three categories: explicit
    planning on given transitions, explicit planning on learned transitions, and end-to-end
    learning of both planning and transitions. In general, DRL targets training DNNs
    to approximate the optimal policy $\pi^{*}$ together with optimal value functions
    $V^{*}$ and $Q^{*}$. In the following, we will cover the most common model-based
    DRL approaches including value function and policy search methods.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了包括基于价值的方法和策略梯度方法的无模型方法。在本节中，我们重点介绍基于模型的方法，它通过学习一个转移模型来处理环境的动态，该模型允许在不直接与环境互动的情况下模拟环境。与无模型方法不同，基于模型的方法通过函数近似从经验中学习。理论上，基于模型的
    RL/DRL 不需要特定的先验知识，但结合先验知识可以帮助更快的收敛和更好的训练模型，缩短训练时间以及减少训练样本的数量。虽然使用原始像素数据，基于模型的
    RL 在高维和动态环境中难以工作。DRL 通过使用自编码器 [[95](#bib.bib95)] 将高维观察嵌入到低维空间来解决这一问题。许多 DRL 方法已基于将
    RL 先前工作扩展到高维问题。关于高维问题的基于模型的 RL 的良好概述可以在 [[297](#bib.bib297)] 中找到，该文献将基于模型的 DRL
    分为三个类别：给定转移上的显式规划、学习转移上的显式规划以及规划和转移的端到端学习。一般来说，DRL 目标是训练 DNNs 来近似最优策略 $\pi^{*}$
    以及最优价值函数 $V^{*}$ 和 $Q^{*}$。接下来，我们将介绍最常见的基于模型的 DRL 方法，包括价值函数和策略搜索方法。
- en: 4.2.1 Value function
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 价值函数
- en: 'We start this category with DQN [[264](#bib.bib264)] which has been successfully
    applied to classic Atari and illustrated in Fig.[7](#S4.F7 "Figure 7 ‣ 4.1.1 Value-based
    DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    DQN uses CNNs to deal with high dimensional state space like pixels, to approximate
    the Q-value function.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从 DQN [[264](#bib.bib264)] 开始，这一方法已成功应用于经典的 Atari 游戏，并在图 [7](#S4.F7 "Figure
    7 ‣ 4.1.1 Value-based DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey") 中展示。DQN 使用 CNNs 来处理像素等高维状态空间，以近似 Q 值函数。'
- en: Monte Carlo tree search (MCTS) MCTS [[62](#bib.bib62)] is one of the most popular
    methods to look-ahead search and it is combined with a DNN-based transition model
    to build a model-based DRL in [[9](#bib.bib9)]. In this work, the learned transition
    model predicts the next frame and the rewards one step ahead using the input of
    the last four frames of the agent’s first-person-view image and the current action.
    This model is then used by the Monte Carlo tree search algorithm to plan the best
    sequence of actions for the agent to perform.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索（MCTS）[[62](#bib.bib62)] 是一种最受欢迎的前瞻搜索方法，它与基于 DNN 的转移模型结合，用于构建 [[9](#bib.bib9)]
    中的模型基 DRL。在这项工作中，学习的转移模型预测下一帧以及一步之遥的奖励，使用的是代理的第一人称视角图像的最后四帧和当前动作作为输入。然后，蒙特卡洛树搜索算法使用该模型规划代理执行的最佳动作序列。
- en: Value-Targeted Regression (UCRL-VTR) Alex, et al. proposed model-based DRL for
    regret minimization [[167](#bib.bib167)]. In their work, a set of models, that
    are ‘consistent’ with the data collected, is constructed at each episode. The
    consistency is defined as the total squared error, whereas the value function
    is determined by solving the optimistic planning problem with the constructed
    set of models
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Policy search
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Policy search methods aim to directly find policies by means of gradient-free
    or gradient-based methods.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) ME-TRPO [[190](#bib.bib190)]
    is mainly based on Trust Region Policy Optimization (TRPO) [[327](#bib.bib327)]
    which imposes a trust region constraint on the policy to further stabilize learning.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Model-Based Meta-Policy-Optimization (MB-MPO) MB-MPO [[58](#bib.bib58)] addresses
    the performance limitation of model-based DRL compared against model-free DRL
    when learning dynamics models. MB-MPO learns an ensemble of dynamics models, a
    policy that can quickly adapt to any model in the ensemble with one policy gradient
    step. As a result, the learned policy exhibits less model bias without the need
    to behave conservatively.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of both model-based and model-free DRL algorithms is given in Table
    [2](#S4.T2 "Table 2 ‣ 4.2.2 Policy search ‣ 4.2 Model-Based Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey"). In this Table, we also categorized DRL techniques into
    either on-policy or off-policy. In on-policy RL, it allows the use of older samples
    (collected using the older policies) in the calculation. The policy $\pi^{k}$
    is updated with data collected by $\pi^{k}$ itself. In off-policy RL, the data
    is assumed to be composed of different policies $\pi^{0},\pi^{0},...,\pi^{k}$.
    Each policy has its own data collection, then the data collected from $\pi^{0}$,
    $\pi^{1}$, …, $\pi^{k}$ is used to train $\pi^{k+1}$.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of model-based and model-free DRL algorithms consisting of
    value-based and policy gradient methods.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '| DRL Algorithms | Description | Category |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| DQN [[264](#bib.bib264)] | Deep Q Network | Value-based Off-policy |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| Double DQN [[370](#bib.bib370)] | Double Deep Q Network | Value-based Off-policy
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| Dueling DQN [[390](#bib.bib390)] | Dueling Deep Q Network | Value-based Off-policy
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| MCTS [[9](#bib.bib9)] | Monte Carlo tree search | Value-based On-policy |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| UCRL-VTR[[167](#bib.bib167)] | optimistic planning problem | Value-based
    Off-policy |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| DDPG [[223](#bib.bib223)] | DQN with Deterministic Policy Gradient | Policy
    gradient Off-policy |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| TRPO [[327](#bib.bib327)] | Trust Region Policy Optimization | Policy gradient
    On-policy |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| PPO [[328](#bib.bib328)] | Proximal Policy Optimization | Policy gradient
    On-policy |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| ME-TRPO [[190](#bib.bib190)] | Model-Ensemble Trust-Region Policy Optimization
    | Policy gradient On-policy |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| MB-MPO [[58](#bib.bib58)] | Model-Based Meta- Policy-Optimization | Policy
    gradient On-policy |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| MB-MPO [[58](#bib.bib58)] | 基于模型的元策略优化 | 策略梯度在线策略 |'
- en: '| A3C [[263](#bib.bib263)] | Asynchronous Advantage Actor Critic | Actor Critic
    On-Policy |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| A3C [[263](#bib.bib263)] | 异步优势演员评论家 | 演员评论家在线策略 |'
- en: '| A2C [[263](#bib.bib263)] | Advantage Actor Critic | Actor Critic On-Policy
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| A2C [[263](#bib.bib263)] | 优势演员评论家 | 演员评论家在线策略 |'
- en: 4.3 Good practices
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 良好实践
- en: Inspired by Deep Q-learning [[264](#bib.bib264)], we discuss some useful techniques
    that are used during training an agent in DRL framework in practices.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 受到深度Q学习的启发 [[264](#bib.bib264)]，我们讨论了一些在实际DRL框架中训练代理时使用的有用技术。
- en: Experience replay Experience replay [[417](#bib.bib417)] is a useful part of
    off-policy learning and is often used while training an agent in RL framework.
    By getting rid of as much information as possible from past experiences, it removes
    the correlations in training data and reduces the oscillation of the learning
    procedure. As a result, it enables agents to remember and re-use past experiences
    sometimes in many weights updates which increases data efficiency.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放 经验回放 [[417](#bib.bib417)] 是离策略学习中的一个有用部分，通常在RL框架中训练代理时使用。通过尽可能多地消除过去经验中的信息，它去除了训练数据中的相关性，减少了学习过程的波动。因此，它使代理能够记住并重新利用过去的经验，这在许多权重更新中增加了数据效率。
- en: Minibatch learning Minibatch learning is a common technique that is used together
    with experience replay. Minibatch allows learning more than one training sample
    at each step, thus, it makes the learning process robust to outliers and noise.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量学习 小批量学习是一种常用技术，通常与经验回放一起使用。小批量学习允许在每一步学习多个训练样本，从而使学习过程对异常值和噪声具有鲁棒性。
- en: 'Target Q-network freezing As described in [[264](#bib.bib264)], two networks
    are used for the training process. In target Q-network freezing: one network interacts
    with the environment and another network plays the role of a target network. The
    first network is used to generate target Q-values that are used to calculate losses.
    The weights of the second network i.e. target network are fixed and slowly updated
    to the first network [[224](#bib.bib224)].'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 目标Q网络冻结 如[[264](#bib.bib264)]所述，训练过程中使用了两个网络。在目标Q网络冻结中：一个网络与环境交互，另一个网络则扮演目标网络的角色。第一个网络用于生成目标Q值，这些Q值用于计算损失。第二个网络，即目标网络的权重是固定的，并且缓慢更新为第一个网络的权重
    [[224](#bib.bib224)]。
- en: Reward clipping A reward is the scalar number provided by the environment and
    it aims at optimizing the network. To keep the rewards in a reasonable scale and
    to ensure proper learning, they are clipped to a specific range (-1 ,1). Here
    1 refers to as positive reinforcement or reward and -1 is referred to as negative
    reinforcement or punishment.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励剪辑 奖励是由环境提供的标量数值，旨在优化网络。为了保持奖励在合理的范围内并确保适当的学习，奖励被剪辑到特定范围（-1，1）。这里1表示正向强化或奖励，-1表示负向强化或惩罚。
- en: Model-based v.s. model-free approach Whether the model-free or model-based approaches
    is chosen mainly depends on the model architecture i.e. policy and value function.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法与无模型的方法 选择基于模型的方法还是无模型的方法主要取决于模型架构，即策略和价值函数。
- en: 5 DRL in Landmark Detection
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 DRL在地标检测中的应用
- en: Autonomous landmark detection has gained more and more attention in the past
    few years. One of the main reasons for this increased inclination is the rise
    of automation for evaluating data. The motivation behind using an algorithm for
    landmarking instead of a person is that manual annotation is a time-consuming
    tedious task and is prone to errors. Many efforts have been made for the automation
    of this task. Most of the works that were published for this task using a machine
    learning algorithm to solve the problem. [[64](#bib.bib64)] proposed a regression
    forest-based method for detecting landmark in a full-body CT scan. Although the
    method was fast it was less accurate when dealing with large organs. [[101](#bib.bib101)]
    extended the work of [[64](#bib.bib64)] by adding statistical shape priors that
    were derived from segmentation masks with cascade regression.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: In order to address the limitations of previous works on anatomy detection,
    [[105](#bib.bib105)] reformulated the detection problem as a behavior learning
    task for an artificial agent using MDP. By using the capabilities of DRL and scale-space
    theory [[226](#bib.bib226)], the optimal search strategies for finding anatomical
    structures are learned based on the image information at multiple scales. In their
    approach, the search starts at the coarsest scale level for capturing global context
    and continues to finer scales for capturing more local information. In their RL
    configuration, the state of the agent at time $t$, $s_{t}=I(\vec{p}_{t})$ is defined
    as an axis-aligned box of image intensities extracted from the image $I$ and centered
    at the voxel-position $\vec{p}_{t}$ in image space. An action $a_{t}$ allows the
    agent to move from any voxel position $\vec{p}_{t}$ to an adjacent voxel position
    $\vec{p}_{t+1}$. The reward function represents distance-based feedback, which
    is positive if the agent gets closer to the target structure and negative otherwise.
    In this work, a CNN is used to extract deep semantic features. The search starts
    with the coarsest scale level $M-1$, the algorithm tries to maximize the reward
    which is the change in distance between ground truth and predicted landmark location
    before and after the action of moving the scale window across the image. Upon
    convergence, the scale level is changed to $M-2$ and the search continued from
    the convergence point at scale level $M-1$. The process is repeated on the following
    scales until convergence on the finest scale. The authors performed experiments
    on 3D CT scans and obtained an average accuracy increase of 20-30$\%$ and lower
    distance error than the other techniques such as SADNN [[104](#bib.bib104)] and
    3D-DL [[427](#bib.bib427)]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Focus on anatomical landmark localization in 3D fetal US images, [[10](#bib.bib10)]
    proposed and demonstrated use cases of several different Deep Q-Network RL models
    to train agents that can precisely localize target landmarks in medical scans.
    In their work, they formulate the landmark detection problem as an MDP of a goal-oriented
    agent, where an artificial agent is learned to make a sequence of decisions towards
    the target point of interest. At each time step, the agent should decide which
    direction it has to move to find the target landmark. These sequential actions
    form a learned policy forming a path between the starting point and the target
    landmark. This sequential decision-making process is approximated under RL. In
    this RL configuration, the environment is defined as a 3D input image, action
    $A$ is a set of six actions $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$ corresponding
    to three directions, the state $s$ is defined as a 3D region of interest (ROI)
    centered around the target landmark and the reward is chosen as the difference
    between the two Euclidean distances: the previous step and current step. This
    reward signifies whether the agent is moving closer to or further away from the
    desired target location. In this work, they also proposed a novel fixed- and multi-scale
    optimal path search strategy with hierarchical action steps for agent-based landmark
    localization frameworks.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Whereas pure policy or value-based methods have been widely used to solve RL-based
    localization problems, [[7](#bib.bib7)] adopts an actor-critic [[262](#bib.bib262)]
    based direct policy search method framed in a temporal difference learning approach.
    In their work, the state is defined as a function of the agent-position which
    allows the agent at any position to observe an $m\times m\times 3$ block of surrounding
    voxels. Similar to other previous work, the action space is $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$.
    The reward is chosen as a simple binary reward function, where a positive reward
    is given if an action leads the agent closer to the target landmark, and a negative
    reward is given otherwise. Far apart from the previous work, their approach proposes
    a non-linear policy function approximator represented by an MLP whereas the value
    function approximator is presented by another MLP stacked on top of the same CNN
    from the policy net. Both policy (actor) and value (critic) networks are updated
    by actor-critic learning. To improve the learning, they introduce a partial policy-based
    RL to enable solving the large problem of localization by learning the optimal
    policy on smaller partial domains. The objective of the partial policy is to obtain
    multiple simple policies on the projections of the actual action space, where
    the projected policies can reconstruct the policy on the original action space.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Based on the hypothesis that the position of all anatomical landmarks is interdependent
    and non-random within the human anatomy and this is necessary as the localization
    of different landmarks requires learning partly heterogeneous policies, [[377](#bib.bib377)]
    concluded that one landmark can help to deduce the location of others. For collective
    gain, the agents share their accumulated knowledge during training. In their approach,
    the state is defined as RoI centered around the location of the agent. The reward
    function is defined as the relative improvement in Euclidean distance between
    their location at time $t$ and the target landmark location. Each agent is considered
    as Partially Observable Markov Decision Process (POMDP) [[107](#bib.bib107)] and
    calculates its individual reward as their policies are disjoint. In order to reduce
    the computational load in locating multiple landmarks and increase accuracy through
    anatomical interdependence, they propose a collaborative multi-agent landmark
    detection framework (Collab-DQN) where DQN is built upon a CNN. The backbone CNN
    is shared across all agents while the policy-making fully connected layers are
    separate for each agent.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Comparing various DRL-based landmark detection methods. The first
    group on Single Landmark Detection (SLD) and the second group for Multiple Landmark
    Detection (MLD)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Performance
    | Datasets and source code |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| SLD [[105](#bib.bib105)] | 2017 | DQN | 6 action: 2 per axis | State: an
    axis-aligned box centered at the voxel-position. Action: move from $\vec{p}_{t}$
    to $\vec{p}_{t+1}$. Reward: distance-based feedback | Average accuracy increase
    20-30%. Lower distance error than other techniques such as SADNN [[104](#bib.bib104)]
    and 3D-DL [[427](#bib.bib427)] | 3D CT Scan |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| SLD [[10](#bib.bib10)] | 2019 | DQN, DDQN, Duel DQN and Duel DDQN | 6 action:
    2 per axis | Environment: 3D input image. State: 3D RoI centered around the target
    landmark. Reward: Euclidean distance between predicted points and groundtruth
    points. | Duel DQN performs the best on Right Cerebellum (FS), Left Cerebellum
    (FS, MS) Duel DDQN is the best on Right Cerebellum (MS) DQN performs the best
    on Cavum Septum Pellucidum(FS, MS) | Fetal head, ultrasound scans [[219](#bib.bib219)].
    [Available code](https://github.com/amiralansary/rl-medical) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| SLD [[7](#bib.bib7)] | 2019 | Actor- Critic -based Partial -Policy RL | 6
    action: 2 per axis | State: a function of the agent-position. Reward: binary reward
    function. policy function: MLP. value function: MLP | Faster and better convergence,
    outperforms than other conventional actor-critic and Q-learning | CT volumes:
    Aortic valve. CT volumes: LAA seed-point. MR images: Vertebra centers [[42](#bib.bib42)].
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| MLD [[377](#bib.bib377)] | 2019 | Collab DQN | 6 action: 2 per axis | State:
    RoI centred around the agent. Reward: relative improvement in Euclidean distance.
    Each Agent is a POMDP has its own reward. Collab-DQN: reduce the computational
    load | Colab DQN got better results than supervised CNN and DQN | Brain MRI landmark
    [[158](#bib.bib158)], Cardiac MRI landmark [[70](#bib.bib70)], Fetal brain landmark
    [[10](#bib.bib10)]. [Available code](https://github.com/thanosvlo/MARL-for-Anatomical-Landmark-Detection)
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| MLD [[161](#bib.bib161)] | 2020 | DQN | 6 action 2 per axis | State: 3D image
    patch. Reward: Euclidean distance and $\in[-1,1]$. Backbone CNN is share among
    agents Each agent has it own Fully connected layer | Detection error increased
    as the degree of missing information increased Performance is affected by the
    choice of landmarks | 3D Head MR images |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: 'Different from the previous works on RL-based landmark detection, which detect
    a single landmark,[[161](#bib.bib161)] proposed a multiple landmark detection
    approach to better time-efficient and more robust to missing data. In their approach,
    each landmark is guided by one agent. The MDP is models as follows: The state
    is defined as a 3D image patch. The reward, clipped in [-1, +1], is defined as
    the difference in the Euclidean distance between the landmark predicted in the
    previous time step and the target, and in the landmark predicted in the current
    time step and the target. The action space is defined as in other previous works
    i.e. there are 6 actions $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$ in the action
    space. To enable the agents to share the information learned by detecting one
    landmark for use in detecting other landmarks, hard parameter sharing from multi-task
    learning is used. In this work, the backbone network is shared among agents and
    each agent has its own fully connected layer.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Table LABEL:tab:landmark summarizes and compares all approaches for DRL in
    landmark detection, and a basic implementation of landmark detection using DRL
    has been shown in Fig. [11](#S5.F11 "Figure 11 ‣ 5 DRL in Landmark Detection ‣
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure illustrates a general implementation of landmark detection with the help
    of DRL, where the state is the Region of interest (ROI) around the current landmark
    location cropped from the image, The actions performed by the DRL agent are responsible
    for shifting the ROI across the image forming a new state and the reward corresponds
    to the improvement in euclidean distance between ground truth and predicted landmark
    location with iterations as used by [[105](#bib.bib105)],[[7](#bib.bib7)],[[10](#bib.bib10)],[[377](#bib.bib377)],[[161](#bib.bib161)].'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9972fc9734de5c35a7ada06c3f6700c.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: DRL implementation for landmark detection, The red point corresponds
    to the current landmark location and Red box is the Region of Interest (ROI) centered
    around the landmark, the actions of DRL agent shift the ROI across the image to
    maximize the reward corresponding to the improvement in distance between the ground
    truth and predicted landmark location.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 6 DRL in Object Detection
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object detection is a task that requires the algorithm to find bounding boxes
    for all objects in a given image. Many attempts have been made towards object
    detection. A method for bounding box prediction for object detection was proposed
    by [[109](#bib.bib109)], in which the task was performed by extracting region
    proposals from an image and then feeding each of them to a CNN to classify each
    region. An improvement to this technique was proposed by [[108](#bib.bib108)],
    where they used the feature from the CNN to propose region proposals instead of
    the image itself, this resulted in fast detection. Further improvement was proposed
    by [[309](#bib.bib309)], where the authors proposed using a region proposal network
    (RPN) to identify the region of interest, resulting in much faster detection.
    Other attempts including focal loss [[225](#bib.bib225)] and Fast YOLO [[332](#bib.bib332)]
    have been proposed to address the imbalanced data problem in object detection
    with focal loss [[225](#bib.bib225)], and perform object detection in video on
    embedded devices in a real-time manner [[332](#bib.bib332)].
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Considering MDP as the framework for solving the problem, [[43](#bib.bib43)]
    used DRL for active object localization. The authors considered 8 different actions
    (up, down, left, right, bigger, smaller, fatter, taller) to improve the fit of
    the bounding box around the object and additional action to trigger the goal state.
    They used a tuple of feature vector and history of actions for state and change
    in IOU across actions as a reward.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: An improvement to [[43](#bib.bib43)] was proposed by [[25](#bib.bib25)], where
    the authors used a hierarchical approach for object detection by treating the
    problem of object detection as an MDP. In their method, the agent was responsible
    to find a region of interest in the image and then reducing the region of interest
    to find smaller regions from the previously selected region and hence forming
    a hierarchy. For the reward function, they used the change in Intersection over
    union (IOU) across the actions and used DQN as the agent. As described in their
    paper, two networks namely, Image-zooms and Pool45-crops with VGG-16 [[340](#bib.bib340)]
    backbone were used to extract the feature information that formed the state for
    DQN along with a memory vector of the last four actions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a sequential search strategy, [[251](#bib.bib251)] proposed a method
    for object detection using DRL. The authors trained the model with a set of image
    regions where at each time step the agent returned fixate actions that specified
    a location in image for actor to explore next and the terminal state was specified
    by $done$ action. The state consisted of a tuple three elements: the observed
    region history $H_{t}$, selected evidence region history $E_{t}$ and fixate history
    $F_{t}$. The $fixate$ action was also a tuple of three elements: $fixate$ action,
    index of evidence region $e_{t}$ and image coordinate of next fixate $z_{t}$.
    The $done$ action consisted of: $done$ action, index of region representing the
    detected output $b_{t}$ and the detection confidence $c_{t}$. The authors defined
    the reward function that was sensitive to the detection location, the confidence
    at the final state and incurs a penalty for each region evaluation.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'To map the inter-dependencies among the different objects, [[170](#bib.bib170)]
    proposed a tree-structured RL agent (Tree-RL) for object localization by considering
    the problem as an MDP. The authors in their implementation considered actions
    of two types: translation and scaling, where the scaling consisted of five actions
    whereas translation consisted of eight actions. In the specified work, the authors
    used the state as a concatenation of the feature vector of the current window,
    feature vector of the whole image, and history of taken actions. The feature vector
    were extracted from an ImageNet [[72](#bib.bib72)] [[320](#bib.bib320)] trained
    VGG-16 [[340](#bib.bib340)] model and for reward the change in IOU across an action
    was used. Tree-RL utilized a top-down tress search starting from the whole image
    where each window recursively takes the best action from each action group which
    further gives two new windows. This process is repeated recursively to find the
    object.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: The task of breast lesion detection is a challenging yet very important task
    in the medical imaging field. A DRL method for active lesion detection in the
    breast was proposed by [[246](#bib.bib246)], where the authors formulated the
    problem as an MDP. In their formulation, a total of nine actions consisting of
    6 translation actions, 2 scaling actions, and 1 trigger action were used. In the
    specified work, the change in dice coefficient across an action was used as the
    reward for scaling and translation actions, and for trigger action, the reward
    was $+\eta$ for dice coefficient greater than $r_{w}$ and $-\eta$ otherwise, where
    $\eta$ and $r_{w}$ were the hyperparameters chosen by the authors. For network
    structure, ResNet [[133](#bib.bib133)] was used as the backbone and DQN as the
    agent.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Different from the previous methods, [[386](#bib.bib386)] proposed a method
    for multitask learning using DRL for object localization. The authors considered
    the problem as an MDP where the agent was responsible to perform a series of transformations
    on the bounding box using a series of actions. Utilizing an RL framework the states
    consisted of feature vector and historical actions concatenated together, and
    a total of 8 actions for Bounding box transformation (left, right, up, down, bigger,
    smaller, fatter, and taller) were used. For reward the authors used the change
    in IOU between actions, the reward being 0 for an increase in IOU and -1 otherwise.
    For terminal action, however, the reward was 8 for IOU greater than 0.5 and -8
    otherwise. The authors in the paper used DQN with multitask learning for localization
    and divided terminal action and 8 transformation actions into two networks and
    trained them together.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: An improvement for the Region proposal networks that greedily select the ROIs
    was proposed by [[295](#bib.bib295)], where they used RL for the task. The authors
    in this paper used a two-stage detector similar to Fast and Faster R-CNN But used
    RL for the decision-making Process. For the reward, they used the normalized change
    in Intersection over Union (IOU).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Instead of learning a policy from a large set of data, [[15](#bib.bib15)] proposed
    a method for bounding box refinement (BAR) using RL. In the paper, once the authors
    have an inaccurate bounding box that is predicted by some algorithm they use the
    BAR algorithm to predict a series of actions for refinement of a bounding box.
    They considered a total of 8 actions (up, down, left, right, wider, taller, fatter,
    thinner) for bounding box transformation and considered the problem as a sequential
    decision-making problem (SDMP). They proposed an offline method called BAR-DRL
    and an online method called BAR-CB where training is done on every image. In BAR-DRL
    the authors trained a DQN over the states which consisted of features extracted
    from ResNet50 [[133](#bib.bib133)] [[354](#bib.bib354)] pretrained on ImageNet
    [[72](#bib.bib72)] [[320](#bib.bib320)] and a history vector of 10 actions. The
    Reward for BAR-DRL was 1 if the IOU increase after action and -3 otherwise. For
    BAR-CB they adapted the LinUCB [[216](#bib.bib216)] algorithm for an episodic
    scenario and considered The Histogram of Oriented Gradients (HOG) for the state
    to capture the outline and edges of the object of interest. The actions in the
    online method (BAR-CB) were the same as the offline method and the reward was
    1 for increasing IOU and 0 otherwise. For both the implementations, the authors
    considered $\beta$ as terminal IOU.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: An improvement to sequential search strategy by [[251](#bib.bib251)] was proposed
    by [[367](#bib.bib367)], where they used a framework consisting of two modules,
    Coarse and fine level search. According to the authors, this method is efficient
    for object detection in large images (dimensions larger than 3000 pixels). The
    authors first performed a course level search on a large image to find a set of
    patches that are used by fine level search to find sub-patches. Both fine and
    coarse levels were conducted using a two-step episodic MDP, where The policy network
    was responsible for returning the probability distribution of all actions. In
    the paper, the authors considered the actions to be the binary action array (0,1)
    where 1 means that the agent would consider acquiring sub-patches for that particular
    patch. The authors in their implementation considered a number of patches and
    sub-patches as 16 and 4 respectively and used the linear combination of $R_{acc}$
    (detection recall) and $R_{cost}$ which combines image acquisition cost and run-time
    performance reward.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparing various DRL-based object detection methods'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and source code |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| Active Object Localization [[43](#bib.bib43)] | 2015 | DQN | 8 actions: up,
    down, left, right, bigger, smaller, fatter, taller | States: feature vector of
    observed region and action history. Reward: Change in IOU. | 5 layer pretrained
    CNN | Higher mAP as compared to methods that did not use region proposals like
    MultiBox [[89](#bib.bib89)], RegionLets [[433](#bib.bib433)], DetNet [[356](#bib.bib356)],
    and second best mAP as compared to R-CNN [[109](#bib.bib109)] | Pascal VOC-2007
    [[90](#bib.bib90)], 2012 [[91](#bib.bib91)] Image Dataset. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical Object Detection [[25](#bib.bib25)] | 2016 | DQN | 5 actions:
    1 action per image quarter and 1 at the center | States: current region and memory
    vector using Image-zooms and Pool45-crops. Reward: change in IOU. | VGG-16 [[340](#bib.bib340)]
    | Objects detected with very few region proposals per image | Pascal VOC-2007
    Image Dataset [[90](#bib.bib90)]. [Available Code](https://github.com/imatge-upc/detection-2016-nipsws)
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| Visual Object Detection [[251](#bib.bib251)] | 2016 | Policy sampling and
    state transition algorithm | 2 actions: fixate and done, where each is a tuple
    of three. | States: Observed region history, evidence region history and fixate
    history. Reward: sensitive to detection location | Deep NN [[187](#bib.bib187)]
    | Comparable mAP and lower run time as compared to other methods such as to exhaustive
    sliding window search(SW), exhaustive search over the CPMC and region proposal
    set(RP) [[112](#bib.bib112)]  [[366](#bib.bib366)] | Pascal VOC 2012 Object detection
    challenge [[91](#bib.bib91)]. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| Tree-Structured Sequential Object Localization (Tree-RL) [[170](#bib.bib170)]
    | 2016 | DQN | 13 actions: 8 translation, 5 scaling. | States: Feature vector
    of current region, and whole image. Reward: change in IOU. | CNN trained on ImageNet
    [[72](#bib.bib72)]  [[320](#bib.bib320)] | Tree-RL with faster R-CNN outperformed
    RPN with fast R-CNN [[108](#bib.bib108)] in terms of AP and comparable results
    to Faster R-CNN [[309](#bib.bib309)] | Pascal VOC 2007 [[90](#bib.bib90)] and
    2012 [[91](#bib.bib91)]. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| Active Breast Lesion Detection [[246](#bib.bib246)] | 2017 | DQN | 9 actions:
    6 translation, 2 scaling, 1 trigger | States: feature vector of current region,
    Reward: improvement in localization. | ResNet [[133](#bib.bib133)] | Comparable
    true positive and false positive proportions as compared to SL [[253](#bib.bib253)]
    and Ms-C [[116](#bib.bib116)], but with lesser mean inference time. | DCE-MRI
    and T1-weighted anatomical dataset [[253](#bib.bib253)] |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| Multitask object localization [[386](#bib.bib386)] | 2018 | DQN | 8 actions:
    left, right, up, down, bigger, smaller, fatter and taller | States: feature vector,
    historical actions. Reward: change in IOU. different network for transformation
    actions and terminal actions. | Pretrained VGG-16 [[340](#bib.bib340)] with ImageNet
    [[72](#bib.bib72)]  [[320](#bib.bib320)] | Better mAP as compared to MultiBox
    [[89](#bib.bib89)], Caicedo et al. [[43](#bib.bib43)] and second best to R-CNN
    [[109](#bib.bib109)]. | Pascal VOC-2007 Image Dataset [[90](#bib.bib90)]. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| Bounding-Box Automated Refinement [[15](#bib.bib15)] | 2020 | DQN | 8 actions:
    up, down, left, right, bigger, smaller, fatter, taller | Offline and online implementation
    States: feature vector for offline (BAR-DRL), HOG for online (BAR-CB). Reward:
    change in IOU | ResNet50 [[133](#bib.bib133)] | Better final IOU for boxes generated
    by methods such as RetinaNet [[225](#bib.bib225)]. | Pascal VOC-2007 [[90](#bib.bib90)],
    2012 [[91](#bib.bib91)] Image Dataset. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| Efficient Object Detection in Large Images [[367](#bib.bib367)] | 2020 |
    DQN | binary action array: where 1 means that the agent would consider acquiring
    sub-patches for that particular patch | Course CPNet and fine FPNet level search.
    States: selected region. Reward: detection recall image acquisition cost. Policy:
    REINFORCE [[351](#bib.bib351)] | ResNet32 [[133](#bib.bib133)] for policy network.
    and YOLOv3 [[306](#bib.bib306)] with DarkNet-53 for Object detector | Higher mAP
    and lower run time as compared to other methods such as [[99](#bib.bib99)]. |
    Caltech Pedestrian dataset (CPD) [[77](#bib.bib77)] [Available Code](https://github.com/uzkent/EfficientObjectDetection)
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| Organ Localization in CT [[275](#bib.bib275)] | 2020 | DQN | 11 actions:
    6 translation, 2 scaling, 3 deformation | States: region inside the Bounding box.
    Reward: change in IOU. | Architecture similar to [[10](#bib.bib10)] | Lower distance
    error for organ localization and run time as compared to other methods such as
    3D-RCNN [[409](#bib.bib409)] and CNNs [[152](#bib.bib152)] | CT scans from the
    VISCERAL dataset [[171](#bib.bib171)] |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| Monocular 3D Object Detection [[231](#bib.bib231)] | 2020 | DQN [[264](#bib.bib264)]
    | 15 actions, each modifies the 3D bounding box in a specific parameter | State:
    3D bounding box parameters, 2D image of object cropped by 2D its detected bounding
    box. Reward: accuracy improvement after applying an action. | ResNet-101 [[133](#bib.bib133)]
    | Higher average precision (AP) compared to [[268](#bib.bib268)], [[302](#bib.bib302)],
    [[210](#bib.bib210)] and [[35](#bib.bib35)] | KITTI [[102](#bib.bib102)] |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: Localization of organs in CT scans is an important pre-processing requirement
    for taking the images of an organ, planning radiotherapy, etc. A DRL method for
    organ localization was proposed by [[275](#bib.bib275)], where the problem was
    formulated as an MDP. In the implementation, the agent was responsible for predicting
    a 3D bounding box around the organ. The authors used the last 4 states as input
    to the agent to stabilize the search and the action space consists of Eleven actions,
    6 for the position of the bounding box, 2 for zoom in and zoom out the action,
    and last 3 for height, width, and depth. For Reward, they used the change the
    in Intersection over union (IOU) across an action.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Monocular 3D object detection is a problem where 3D bounding boxes of objects
    are required to be detected from a single 2D image. Even the sampling-based method
    is the SOTA approach, it has a huge flaw, in which most of the samples it generates
    do not overlap with the groundtruth. To leverage that method, [[231](#bib.bib231)]
    introduced Reinforced Axial Refinement Network (RARN) for monocular 3D object
    detection by utilizing an RL model to iteratively refining the sampled bounding
    box to be more overlapped with the groundtruth bounding box. Given a state having
    the coordinates of the 3D bounding box and image patch of the image, the model
    predicts an action out of a set of 15 actions to refine one of the bounding box
    coordinates in a direction at every timestep, the model is trained by DQN method
    with the immediate reward is the improvement in detection accuracy between every
    pair of timesteps. The whole pipeline, namely RAR-Net, was evaluated on the real-world
    KITTI dataset [[102](#bib.bib102)] and achieved state-of-the-art performance.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'All these methods have been summarised and compared in Table LABEL:tab:obs,
    and a basic implementation of object detection using DRL has been shown in Fig.
    [12](#S6.F12 "Figure 12 ‣ 6 DRL in Object Detection ‣ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey"). The figure illustrates a general
    implementation of object detection using DRL, where the state is an image segment
    cropped using a bounding box produced by some other algorithm or previous iteration
    of DRL, actions predicted by the DRL agent predict a series of bounding box transformation
    to fit the object better, hence forming a new state and Reward is the improvement
    in Intersection over union (IOU) with iterations as used by [[43](#bib.bib43)],[[25](#bib.bib25)],[[15](#bib.bib15)],[[386](#bib.bib386)],[[170](#bib.bib170)],[[275](#bib.bib275)].'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58a1d5d89631a83968289981f8c75065.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: DRL implementation for object detection. The red box corresponds
    to the initial bounding box which for t=0 is predicted by some other algorithm
    or the transformed bounding box by previous iterations of DRL using the actions
    to maximize the improvement in IOU.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 7 DRL in Object Tracking
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Real-time object tracking has a large number of applications in the field of
    autonomous driving, robotics, security, and even in sports where the umpire needs
    accurate estimation of ball movement to make decisions. Object tracking can be
    divided into two main categories: Single object tracking (SOT) and Multiple object
    tracking (MOT).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Many attempts have been made for both SOT and MOT. SOT can be divided into two
    types, active and passive. In passive tracking it is assumed that the object that
    is being tracked is always in the camera frame, hence camera movement is not required.
    In active tracking, however, the decision to move the camera frame is required
    so that the object is always in the frame. Passive tracking has been performed
    by [[397](#bib.bib397)], [[146](#bib.bib146)], where [[146](#bib.bib146)] performed
    tracking for both single and multiple objects. The authors of these papers proposed
    various solutions to overcome common problems such as a change in lighting and
    occlusion. Active tracking is a little bit harder as compared to a passive one
    because additional decisions are required for camera movement. Some efforts towards
    active tracking include [[74](#bib.bib74)] [[270](#bib.bib270)] [[178](#bib.bib178)].
    These solutions treat object detection and object tracking as two separate tasks
    and tend to fail when there is background noise.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: An end-to-end active object tracker using DRL was proposed by [[240](#bib.bib240)],
    where the authors used CNNs along with an LSTM [[139](#bib.bib139)] in their implementation.
    They used the actor-critic algorithm [[262](#bib.bib262)] to calculate the probability
    distribution of different actions and the value of state and used the object orientation
    and distance from the camera to calculate rewards. For experiments, the authors
    used VizDoom and Unreal Engine as the environment.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Another end-to-end method for SOT using sequential search strategy and DRL was
    proposed by [[418](#bib.bib418)]. The method included using an RNN along with
    REINFORCE [[392](#bib.bib392)] algorithm to train the network. The authors used
    a function $f(W_{0})$ that takes in $S_{t}$ and frame as input, where $S_{t}$
    is the object location for the first frame and is zero elsewhere. The output is
    fed to an LSTM module [[139](#bib.bib139)] with past hidden state $h_{t}$. The
    authors calculated the reward function by using insertion over union (IoU) and
    the difference between the average and max.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'A deformable face tracking method that could predict bounding box along with
    facial landmarks in real-time was proposed by [[118](#bib.bib118)]. The dual-agent
    DRL method (DADRL) mentioned in the paper consisted of two agents: a tracking
    and an alignment agent. The problem of object tracking was formulated as an MDP
    where state consisted of image regions extracted by the bounding box and a total
    of 8 actions (left, right, up, down, scale-up, scale down, stop and continue)
    were used, where first six consists of movement actions used by tracking agent
    and last two for alignment agent. The tracking agent is responsible for changing
    the current observable region and the alignment agent determines whether the iteration
    should be terminated. For the tracking agent, the reward corresponded to the misalignment
    descent and for the alignment agent the reward was $+\eta$ for misalignment less
    than the threshold and $-\eta$ otherwise. The DADRL implementation also consisted
    of communicated message channels beside the tracking agent and the alignment agent.
    The tracking agent consisted of a VGG-M [[340](#bib.bib340)] backbone followed
    by a one-layer Q-Network and the alignment agent was designed as a combination
    of a stacked hourglass network with a confidence network. The two communicated
    message channels were encoded by a deconvolution layer and an LSTM unit [[139](#bib.bib139)]
    respectively.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual object tracking when dealing with deformations and abrupt changes can
    be a challenging task. A DRL method for object tracking with iterative shift was
    proposed by [[308](#bib.bib308)]. The approach (DRL-IS) consisted of three networks:
    The actor network, the prediction network, and the critic network, where all three
    networks shared the same CNN and a fully connected layer. Given the initial frame
    and bounding box, the cropped frame is fed to the CNNs to extract the features
    to be used as a state by the networks. The actions included continue, stop and
    update, stop and ignore, and restart. For continue, the bounding boxes are adjusted
    according to the output of the prediction network, for stop and update the iteration
    is stopped and the appearance feature of the target is updated according to the
    prediction network, for stop and ignore the updating of target appearance feature
    is ignored and restart means that the target is lost and the algorithm needs to
    start from the initial bounding box. The authors of the paper used reward as 1
    for change in IoU greater than the threshold, 0 for change in IOU between + and
    - threshold, and -1 otherwise.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Considering the performance of actor-critic framework for various applications,
    [[45](#bib.bib45)] proposed an actor-critic [[262](#bib.bib262)] framework for
    real-time object tracking. The authors of the paper used a pre-processing function
    to obtain an image patch using the bounding box that is fed into the network to
    find the bounding box location in subsequent frames. For actions the authors used
    $\triangle x$ for relative horizontal translation, $\triangle y$ for relative
    vertical translation, and $\triangle s$ for relative scale change, and for a reward
    they used 1 for IoU greater than a threshold and -1 otherwise. They proposed offline
    training and online tracking, where for offline training a pre-trained VGG-M [[340](#bib.bib340)]
    was used as a backbone, and the actor-critic network was trained using the DDPG
    approach [[224](#bib.bib224)].
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'An improvement to [[45](#bib.bib45)] for SOT was proposed by [[84](#bib.bib84)],
    where a visual tracker was formulated using DRL and an expert demonstrator. The
    authors treated the problem as an MDP, where the state consists of two consecutive
    frames that have been cropped using the bounding box corresponding to the former
    frame and used a scaling factor to control the offset while cropping. The actions
    consisted of four elements: $\triangle x$ for relative horizontal translation,
    $\triangle y$ for relative vertical translation, $\triangle w$ for width scaling,
    and $\triangle h$ for height scaling, and the reward was calculated by considering
    whether the IoU is greater than a threshold or not. For the agent architecture
    the authors used a ResNet-18 [[133](#bib.bib133)] as backbone followed by an LSTM
    unit [[391](#bib.bib391)][[139](#bib.bib139)] to encode past information, and
    performed training based on the on-policy A3C framework [[262](#bib.bib262)].'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'In MOT the algorithm is responsible to track trajectories of multiple objects
    in the given video. Many attempts have been made with MOT including [[53](#bib.bib53)],
    [[55](#bib.bib55)] and [[143](#bib.bib143)]. However, MOT is a challenging task
    because of environmental constraints such as crowding or object overlapping. MOT
    can be divided into two main techniques: Offline [[53](#bib.bib53)] and Online
    [[55](#bib.bib55)] [[143](#bib.bib143)]. In offline batch, tracking is done using
    a small batch to obtain tracklets and later all these are connected to obtain
    a complete trajectory. The online method includes using present and past frames
    to calculate the trajectory. Some common methods include Kalman filtering [[177](#bib.bib177)],
    Particle Filtering [[284](#bib.bib284)] or Markov decision [[401](#bib.bib401)].
    These techniques however are prone to errors due to environmental constraints.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the constraints of MOT by previous methods, [[401](#bib.bib401)]
    proposed a method for MOT where the problem was approached as an MDP. The authors
    tracked each object in the frame through the Markov decision process, where each
    object has four states consisting: Active, Tracked, Lost, and Inactive. Object
    detection is the active state and when the object is in the lost state for a sufficient
    amount of time it is considered Inactive, which is the terminal state. The reward
    function in the implementation was learned through data by inverse RL problem
    [[279](#bib.bib279)].'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Previous approaches for MOT follow a tracking by detection technique that is
    prone to errors. An improvement was proposed by [[307](#bib.bib307)], where detection
    and tracking of the objects were carried out simultaneously. The authors used
    a collaborative Q-Network to track trajectories of multiple objects, given the
    initial position of an object the algorithm tracked the trajectory of that object
    in all subsequent frames. For actions the authors used $\triangle x$ for relative
    horizontal translation, $\triangle y$ for relative vertical translation, $\triangle
    w$ for width scaling, and $\triangle h$ for height scaling, and the reward consisted
    of values 1,0,-1 based on the IoU.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method for MOT was proposed by [[168](#bib.bib168)], where the authors
    used LSTM [[139](#bib.bib139)] and DRL to approach the problem of multi-object
    tracking. The method described in the paper used three basic components: a YOLO
    V2 [[260](#bib.bib260)] object detector, many single object trackers, and a data
    association module. Firstly the YOLO V2 object detector is used to find objects
    in a frame, then each detected object goes through the agent which consists of
    CNN followed by an LSTM to encode past information for the object. The state consisted
    of the image patch and history of past 10 actions, where six actions (right, left,
    up, down, scale-up, scale down) were used for bounding box movement across the
    frame with a stop action for the terminal state. To provide reinforcement to the
    agent the reward was 1 if the IOU is greater than a threshold and 0 otherwise.
    In their experiments, the authors used VGG-16 [[340](#bib.bib340)] for CNN backbone
    and performed experiments on MOT benchmark [[201](#bib.bib201)] for people tracking.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparing various DRL-based object tracking methods. The First group
    for Single object tracking (SOT) and the second group for multi-object tracking
    (MOT)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and Source code |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| End to end active object tracking [[240](#bib.bib240)] | 2017 | Actor-Critic
    (a3c) [[262](#bib.bib262)] | 6 actions: turn left, turn right, turn left and move
    forward, turn right and move forward, move forward, no-op | Environment: virtual
    environment. Reward: calculated using object orientation and position. Tracking
    Using LSTM [[139](#bib.bib139)] | ConvNet-LSTM | Higher accumulated reward and
    episode length as compared to methods like MIL [[17](#bib.bib17)], Meanshift [[60](#bib.bib60)],
    KCF [[134](#bib.bib134)]. | ViZDoom [[176](#bib.bib176)], Unreal Engine |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| DRL for object tracking [[418](#bib.bib418)] | 2017 | DRLT | None | State:
    feature vector, Reward: change in IOU use of LSTM [[139](#bib.bib139)] and REINFORCE
    [[392](#bib.bib392)] | YOLO network [[305](#bib.bib305)] | Higher area under curve
    (success rate Vs overlap threshold), precision and speed (fps) as compared to
    STUCK [[126](#bib.bib126)] and DLT [[384](#bib.bib384)]. | Object tracking benchmark
    [[397](#bib.bib397)]. [Available Code](https://github.com/fgabel/Deep-Reinforcement-Learning-for-Visual-Object-Tracking-in-Videos)
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| Dual-agent deformable face tracker [[118](#bib.bib118)] | 2018 | DQN | 8
    actions: left, right, up, down, scale up, scale down, stop and continue. | States:
    image region using Bounding box. Reward: distance error. Facial landmark detection
    and tracking using LSTM [[139](#bib.bib139)] | VGG-M [[340](#bib.bib340)] | Lower
    normalized point to point error for landmarks and higher success rate for facial
    tracking as compared to ICCR [[187](#bib.bib187)], MDM [[336](#bib.bib336)], Xiao
    et al [[32](#bib.bib32)], etc. | Large-scale face tracking dataset, the 300-VW
    test set [[336](#bib.bib336)] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| Tracking with iterative shift [[308](#bib.bib308)] | 2018 | Actor-critic
    [[262](#bib.bib262)] | 4 actions: continue, stop and update, stop and ignore and
    restart | States: image region using bounding box. Reward: change in IOU. Three
    networks: actor, critic and prediction network | 3 Layer CNN and FC layer | Higher
    area under curve for success rate Vs overlap threshold and precision Vs location
    error threshold as compared to CREST [[345](#bib.bib345)], ADNet [[416](#bib.bib416)],
    MDNet [[273](#bib.bib273)], HCFT [[243](#bib.bib243)], SINT [[358](#bib.bib358)],
    DeepSRDCF [[67](#bib.bib67)], and HDT [[301](#bib.bib301)] | OTB-2015 [[398](#bib.bib398)],
    Temple-Color [[220](#bib.bib220)], and VOT-2016 Dataset [[186](#bib.bib186)] |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| Tracking with actor-critic [[45](#bib.bib45)] | 2018 | Actor-critic [[262](#bib.bib262)]
    | 3 actions: $\triangle x$, $\triangle y$ and $\triangle s$ | States: image region
    using bounding box. Reward: IOU greater then threshold. Offline training, online
    tracking | VGG-M [[340](#bib.bib340)] | Higher average precision score then PTAV
    [[93](#bib.bib93)], CFNet [[368](#bib.bib368)], ACFN [[52](#bib.bib52)], SiameFC
    [[29](#bib.bib29)], ECO-HC [[67](#bib.bib67)], etc. | OTB-2013 [[397](#bib.bib397)],
    OTB-2015 [[398](#bib.bib398)] and VOT-2016 dataset [[186](#bib.bib186)] [Available
    Code](https://github.com/bychen515/ACT) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| Visual tracking and expert demonstrator [[84](#bib.bib84)] | 2019 | Actor-critic
    (a3c) [[262](#bib.bib262)] | 4 actions: $\triangle x$, $\triangle y$,$\triangle
    w$ and $\triangle h$ | States: image region using bounding box. Reward: change
    in IOU. SOT using LSTM [[391](#bib.bib391)][[139](#bib.bib139)] | ResNet-18 [[133](#bib.bib133)]
    | Comparable success and precision scores as compared to LADCF [[408](#bib.bib408)],
    SiamRPN [[209](#bib.bib209)] and ECO [[66](#bib.bib66)] | GOT-10k [[148](#bib.bib148)],
    LaSOT [[92](#bib.bib92)], UAV123 [[269](#bib.bib269)], OTB-100 [[397](#bib.bib397)],
    VOT-2018 [[185](#bib.bib185)] and VOT-2019. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| Object tracking by decision making [[401](#bib.bib401)] | 2015 | TLD Tracker
    [[174](#bib.bib174)] | 7 actions: corresponding to moving the object between states
    such as Active, tracked, lost and Inactive | States: 4 states: Active, tracked,
    lost and Inactive. Reward: inverse RL problem [[279](#bib.bib279)] | None | Comparable
    multiple object tracking accuracy (MOTA) and multiple object tracking precision
    (MOTP) [[28](#bib.bib28)] as compared to DPNMS [[296](#bib.bib296)], TCODAL [[18](#bib.bib18)],
    SegTrack [[259](#bib.bib259)], MotiCon [[200](#bib.bib200)], etc | M0T15 dataset
    [[201](#bib.bib201)] [Available Code](https://github.com/yuxng/MDP_Tracking) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| Collaborative multi object tracker [[307](#bib.bib307)] | 2018 | DQN | 4
    actions: $\triangle x$, $\triangle y$, $\triangle w$ and $\triangle h$ | States:
    image region using bounding box. Reward: IOU greater then threshold. 2 networks:
    prediction and decision network | 3 Layer CNN and FC Layer | Comparable multiple
    object tracking accuracy (MOTA) and multiple object tracking precision (MOTP)
    [[28](#bib.bib28)] as compared to SCEA [[143](#bib.bib143)], MDP [[401](#bib.bib401)],
    CDADDALpb [[19](#bib.bib19)], AMIR15 [[321](#bib.bib321)] | MOT15 [[201](#bib.bib201)]
    and MOT16 [[258](#bib.bib258)] datasets |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| Multi object tracking in video [[168](#bib.bib168)] | 2018 | DQN | 6 actions:
    right, left, up, down, scale up, scale down | States: image region using bounding
    box. Reward: IOU greater then threshold. Detection using YOLO-V2 [[260](#bib.bib260)]
    for detector and LSTM [[139](#bib.bib139)] . | VGG-16 [[340](#bib.bib340)] | Comparable
    if not better multiple object tracking accuracy (MOTA) and multiple object tracking
    precision (MOTP) [[28](#bib.bib28)] as compared to RNN-LSTM [[201](#bib.bib201)],
    LP-SSVM [[401](#bib.bib401)], MDPSubCNN [[199](#bib.bib199)], and SiameseCNN [[123](#bib.bib123)]
    | MOT15 Dataset [[201](#bib.bib201)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| Multi agent multi object tracker [[169](#bib.bib169)] | 2019 | DQN | 9 actions:
    move right, move left, move up, move down, scale up, scale down, fatter, taller
    and stop | States: image region using bounding box. Reward: IOU greater then threshold.
    YOLO-V3 [[306](#bib.bib306)] for detection and LSTM [[139](#bib.bib139)]. | VGG-16
    [[340](#bib.bib340)] | Higher running time, and comparable if not better multiple
    object tracking accuracy (MOTA) and multiple object tracking precision (MOTP)
    [[28](#bib.bib28)] as compared to RNN-LSTM [[201](#bib.bib201)], LP-SSVM [[401](#bib.bib401)],
    MDPSubCNN [[199](#bib.bib199)], and SiameseCNN [[123](#bib.bib123)] | MOT15 challenge
    benchmark [[201](#bib.bib201)]. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: To address the problems in existing tracking methods such as varying numbers
    of targets, non-real-time tracking, etc, [[169](#bib.bib169)] proposed a multi-object
    tracking algorithm based on a multi-agent DRL tracker (MADRL). In their object
    tracking pipeline the authors used YOLO-V3 [[306](#bib.bib306)] as object detector,
    where multiple detections produced by YOLO-V3 were filtered using the IOU and
    the selected results were used as multiple agents in multiple agent detector.
    The input agents were fed into a pre-trained VGG-16 [[340](#bib.bib340)] followed
    by an LSTM unit [[139](#bib.bib139)] that could share information across agents
    and return the actions encoded in a 9-dimensional vector( move right, move left,
    move up, move down, scale-up, scale down, aspect ratio change fatter, aspect ratio
    change taller and stop), also a reward function similar to [[168](#bib.bib168)]
    was used.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Various works in the field of object tracking have been summarized in Table
    LABEL:tab:track, and a basic implementation of object tracking using DRL has been
    shown in Fig. [13](#S7.F13 "Figure 13 ‣ 7 DRL in Object Tracking ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). The figure illustrates
    a general implementation of object tracking in videos using DRL, where the state
    consists of two consecutive frames $(F_{t},F_{t+1})$ with a bounding box for the
    first frame produced by another algorithm for the first iteration or by the previous
    iterations of DRL agent. The actions corresponds to the moving the bounding on
    the image to fit the object in frame $F_{t+1}$, hence forming a new state with
    frame $F_{t+1}$ and frame $F_{t+2}$ along with the bounding box for frame $F_{t+1}$
    predicted by previous iteration and reward corresponds to whether IOU is greater
    then a given threshold as used by [[118](#bib.bib118)],[[308](#bib.bib308)],[[45](#bib.bib45)],
    [[84](#bib.bib84)],[[307](#bib.bib307)],[[168](#bib.bib168)],[[169](#bib.bib169)].'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2136103d0d64b38351fdb4369ef5e52.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: DRL implementation for object tracking. Here the state consists
    of two consecutive frames with bounding box locations for the first frame that
    is predicted by some object detection algorithm or by the previous iteration of
    DRL, the actions move the bounding box present in the first frame to fit the object
    in the second frame to maximize the reward which is the whether the IOU is greater
    than a given threshold or not.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 8 DRL in Image Registration
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image registration is a very useful step that is performed on 3D medical images
    for the alignment of two or more images. The goal of 3D medical image registration
    is to find a correlation between two images from either different patients or
    the same patients at different times, where the images can be Computed Tomography
    (CT), Magnetic Resonance Imaging (MRI), or Positron Emission Tomography (PET).
    In the process, the images are brought to the same coordinate system and aligned
    with each other. The reason for image registration being a challenging task is
    the fact that the two images used may have a different coordinate system, scale,
    or resolution.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Many attempts have been made toward automated image registration. A multi-resolution
    strategy with local optimizers to perform 2D or 3D image registration was performed
    by [[359](#bib.bib359)]. However, multi-resolution tends to fail with different
    field of views. Heuristic semi-global optimization schemes were proposed to solve
    this problem and used by [[252](#bib.bib252)] through simulated annealing and
    through genetic algorithm [[317](#bib.bib317)], However, their cost of computation
    was very high. A CNN-based approach to this problem was suggested by [[256](#bib.bib256)],
    and [[79](#bib.bib79)] proposed an optical flow method between 2D RGB images.
    A descriptor learned through a CNN was proposed by [[395](#bib.bib395)], where
    the authors encoded the posture and identity of a 3D object using the 2D image.
    Although all of these formulations produce satisfactory results yet, the methods
    could not be applied directly to 3D medical images.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the problems faced by previous methods, [[238](#bib.bib238)] proposed
    a method for improving probabilistic image registration via RL and uncertainty
    evaluation. The method involved predicting a regression function that predicts
    registration error from a set of features by using regression random forests (RRF)
    [[37](#bib.bib37)] method for training. The authors performed experiments on 3D
    MRI images and obtained an accuracy improvement of up to 25%.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Previous image registration methods are often customized to a specific problem
    and are sensitive to image quality and artifacts. To overcome these problems,
    [[221](#bib.bib221)] proposed a robust method using DRL. The authors considered
    the problem as an MDP where the goal is to find a set of transformations to be
    performed on the floating image to register it on the reference image. They used
    the gamma value for future reward decay and used the change in L2 Norm between
    the predicted transformation and ground truth transformation to calculate the
    reward. The authors also used a hierarchical approach to solve the problem with
    varying FOVs and resolutions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Comparing various DRL-based image registration methods.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| Image registration using uncertainity evaluation [[238](#bib.bib238)] | 2013
    | DQN | Not specified | Probabilistic model using regression random forests (RRF)
    [[37](#bib.bib37)] | Not specified | Higher final Dice score (DSC) as compared
    to other methods like random seed selection and grid-based seed selection | 3D
    MRI images from LONI Probabilistic Brain Atlas (LPBA40) [Dataset](http://www.loni.ucla.edu/)
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| Robust Image registration [[221](#bib.bib221)] | 2017 | DQN | 12 actions:
    corresponding to different transformations | States: current transformation. Reward:
    distance error. | 5 Conv3D layers and 3 FC layers | Better success rate then ITK
    [[153](#bib.bib153)], Quasi-global [[255](#bib.bib255)] and Semantic registration[[277](#bib.bib277)]
    | Abdominal spine CT and CBCT dataset, Cardiac CT and CBCT |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| Multimodal image registration [[244](#bib.bib244)] | 2017 | Duel-DQN Double-DQN
    | Actions update the transformations on floating image | States: cropped 3D image.
    Duel-DQN for value estimation and Double DQN for updating weights. | Batch normalization
    followed by 5 Conv3D and 3 Maxpool layers | Lower Euclidean distance error as
    compared to methods like Hausdorff, ICP, DQN [[264](#bib.bib264)], Dueling [[390](#bib.bib390)],
    etc. | Thorax and Abdomen (ABD) dataset |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| Robust non-rigid agent-based registration [[184](#bib.bib184)] | 2017 | DQN
    | 2n actions for n dimensional $\theta$ vector | States: fixed and moving image.
    Reward: change in transformation error. With Statistical deformation model and
    fuzzy action control. | Multi layer CNN, pooling and FC layers. | Higher Mean
    Dice score and lower Hausdorff distance as compared to methods like LCC-Demons
    [[237](#bib.bib237)] and Elastix [[180](#bib.bib180)]. | MICCAI challenge PROMISE12
    [[227](#bib.bib227)] |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| Robust Multimodal registration [[349](#bib.bib349)] | 2018 | Actor-Critic
    (a3c) [[262](#bib.bib262)] | 8 actions: for different transformations | States:
    fixed and moving image. Reward: Distance error. Monte-carlo method with LSTM [[139](#bib.bib139)].
    | Multi layer CNN and FC layer | Comparable if not lower target registration error
    [[96](#bib.bib96)] as compared to methods like SIFT [[239](#bib.bib239)], Elastix
    [[180](#bib.bib180)], Pure SL, RL-matrix, RL-LME, etc. | CT and MR images |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: A multi-modal method for image registration was proposed by [[244](#bib.bib244)],
    where the authors used DRL for alignment of depth data with medical images. In
    the specified work Duel DQN was used as the agent for estimating the state value
    and the advantage function, and the cropped 3D image tensor of both data modalities
    was considered as the state. The algorithm’s goal was to estimate a transformation
    function that could align moving images to a fixed image by maximizing a similarity
    function between the fixed and moving image. A large number of convolution and
    pooling layer were used to extract high-level contextual information, batch normalization
    and concatenation of feature vector from last convolution layer with action history
    vector was used to solve the problem of oscillation and closed loops, and Double
    DQN architecture for updating the network weights was used.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Previous methods for image registration fail to cope with large deformations
    and variability in appearance. To overcome these issues [[184](#bib.bib184)] proposed
    a robust non-rigid agent-based method for image registration. The method involves
    finding a spatial transformation $T_{\theta}$ that can map the fixed image with
    the floating image using actions at each time step, that is responsible for optimizing
    $\theta$. If the $\theta$ is a d dimensional vector then there will be 2d possible
    actions. In this work, a DQN was used as an agent for value estimation, along
    with a reward that corresponded to the change in $\theta$ distance between ground
    truth and predicted transformations across an action.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: An improvement to the previous methods was proposed by [[349](#bib.bib349)],
    where the authors used a recurrent network with RL to solve the problem. Similar
    to [[221](#bib.bib221)], they considered the two images as a reference/fixed and
    floating/moving, and the algorithm was responsible for predicting transformation
    on the moving image to register it on a fixed image. In the specified work an
    LSTM [[139](#bib.bib139)] was used to encode past hidden states, Actor-critic
    [[262](#bib.bib262)] for policy estimation, and a reward function corresponding
    to distance between ground truth and transformed predicted landmarks were used.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Various methods in the field of Image registration have been summarized and
    compared in Table LABEL:tab:reg, and a basic implementation of image registration
    using DRL has been shown in Fig. [14](#S8.F14 "Figure 14 ‣ 8 DRL in Image Registration
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure illustrates a general implementation of image registration using DRL where
    the state consists of a fixed and floating image. The DRL agent predicts actions
    in form of a set of transformations on a floating image to register it onto the
    fixed image hence forming a new state and accepts reward in form of improvement
    in distance error between ground truth and predicted transformations with iterations
    as described by [[349](#bib.bib349)],[[184](#bib.bib184)],[[221](#bib.bib221)].'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d75a1dbe18c7a1d15a804e3770b2961a.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: DRL implementation for image registration. The state consists of
    fixed and floating image and the actions in form of transformations are performed
    on the floating image so as to maximize reward by minimizing distance between
    the ground truth and predicted transformations.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 9 DRL in Image Segmentation
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image segmentation is one of the most extensively performed tasks in computer
    vision, where the algorithm is responsible for labeling each pixel position as
    foreground or background corresponding to the object being segmented in the image.
    Image segmentation has a wide variety of applications in medical, robotics, weather,
    etc. One of the earlier attempts with image segmentation includes [[125](#bib.bib125)].
    With the improvement in detection techniques and introduction of CNN, new methods
    are introduced every year for image segmentation. Mask R-CNN [[132](#bib.bib132)]
    extended the work by Faster R-CNN [[309](#bib.bib309)] by adding a segmentation
    layer after the Bounding box has been predicted. Some earlier works include [[109](#bib.bib109)],
    [[127](#bib.bib127)], [[128](#bib.bib128)] etc. Most of these works give promising
    results in image segmentation. However, due to the supervised nature of CNN and
    R-CNN, these algorithms need a large amount of data. In fields like medical, the
    data is sometimes not readily available hence we needed a way to train algorithms
    to perform a given task when there are data constraints. Luckily RL tends to shine
    when the data is not available in a large quantity.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: One of the first methods for Image segmentation through RL was proposed by [[324](#bib.bib324)],
    where the authors proposed an RL framework for medical image segmentation. In
    their work, they used a Q-Matrix, where the actions were responsible for adjusting
    the threshold values to predict the mask and the reward was the normalized change
    in quality measure between action steps. [[325](#bib.bib325)] also used a similar
    technique of Tabular method.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the constraints of the previous method for segmentation, [[310](#bib.bib310)]
    proposed a method for indoor semantic segmentation through RL. In their paper,
    the authors proposed a sequential strategy using RL to combine binary object masks
    of different objects into a single multi-object segmentation mask. They formulated
    the binary mask in a Conditional Random Field Framework (CRF), and used a logistic
    regression version of AdaBoost [[140](#bib.bib140)] for classification. The authors
    considered the problem of adding multiple binary segmentation into one as an MDP,
    where the state consisted of a list of probability distributions of different
    objects in an image, and the actions correspond to the selection of object/background
    segmentation for a particular object in the sequential semantic segmentation.
    In the RL framework, the reward was considered in terms of pixel-wise frequency
    weighted Jaccard Index computed over the set of actions taken at any stage of
    an episode.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Interactive segmentation is the task of producing an interactive mask for objects
    in an image. Most of the previous works in this field greatly depend on the distribution
    of inputs which is user-dependent and hence produce inadequate results. An improvement
    was proposed by [[343](#bib.bib343)], where the authors proposed SeedNet, an automatic
    seed generation method for robust interactive segmentation through RL. With the
    image and initial seed points, the algorithm is capable of generating additional
    seed points and image segmentation results. The implementation included Random
    Walk (RW) [[114](#bib.bib114)] as the segmentation algorithm and DQN for value
    estimation by considering the problem as an MDP. They used the current binary
    segmentation mask and image features as the state, the actions corresponded to
    selecting seed points in a sparse matrix of size $20\times 20$(800 different actions
    were possible), and the reward consisted of the change in IOU across an action.
    In addition, the authors used an exponential IOU model to capture changes in IOU
    values more accurately.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Most of the previous work for image segmentation fail to produce satisfactory
    results when it comes to 3D medical data. An attempt on 3D medical image segmentation
    was done by [[222](#bib.bib222)], where the authors proposed an iteratively-refined
    interactive multi-agent method for 3D medical image segmentation. They proposed
    a method to refine an initial course segmentation produced by some segmentation
    methods using RL, where the state consisted of the image, previous segmentation
    probability, and user hint map. The actions corresponded to adjusting the segmentation
    probability for refinement of segmentation, and a relative cross-entropy gain-based
    reward to update the model in a constrained direction was used. In simple words,
    it is the relative improvement of previous segmentation to the current one. The
    authors utilized an asynchronous advantage actor-critic algorithm for determining
    the policy and value of the state.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Comparing various DRL-based image segmentation methods'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| Semantic Segmentation for indoor scenes[[310](#bib.bib310)] | 2016 | DQN
    | 2 actions per object: object, background | States: current probability distribution.
    Reward: pixel-wise frequency weighted Jaccard index. Conditional Random Field
    Framework (CRF) and logistic regression version of AdaBoost [[140](#bib.bib140)]
    for classification. | Not Specified | Pixel-wise percentage jaccard index comparable
    to Gupta-L [[121](#bib.bib121)] and Gupta-P [[120](#bib.bib120)]. | NYUD V2 dataset
    [[338](#bib.bib338)] |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| SeedNet [[343](#bib.bib343)] | 2018 | DQN, Double-DQN, Duel-DQN | 800 actions:
    2 per pixel | States: image features and segmentation mask. Reward: change in
    IOU. Random Walk (RW) [[114](#bib.bib114)] for segmentation algorithm. | Multi
    layer CNN | Better IOU then methods like FCN [[236](#bib.bib236)] and iFCN [[407](#bib.bib407)].
    | MSRA10K saliency dataset [[49](#bib.bib49)] |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| Iteratively refined multi agent segmentation [[222](#bib.bib222)] | 2020
    | Actor-critic (a3c) [[262](#bib.bib262)] | 1 action per voxel for adjusting segmentation
    probability | States: 3D image segmentation probability and hint map. Reward:
    cross entropy gain based framework. | R-net [[378](#bib.bib378)] | Better performance
    then methods like MinCut [[183](#bib.bib183)], DeepIGeoS (R-Net) [[378](#bib.bib378)]
    and InterCNN [[36](#bib.bib36)]. | BraTS 2015[[254](#bib.bib254)], MM-WHS [[432](#bib.bib432)]
    and NCI-ISBI 2013 Challenge [[33](#bib.bib33)] |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| Multi-step medical image segmentation [[360](#bib.bib360)] | 2020 | Actor-critic
    (a3c) [[262](#bib.bib262)] | Actions control the position and shape of brush stroke
    to modify segmentation | States: image, segmentation mask and time step. Reward:
    change in distance error. Policy: DPG [[339](#bib.bib339)]. | ResNet18 [[133](#bib.bib133)]
    | Higher Mean Dice score and lower Hausdorff distance then methods like Grab-Cut
    [[315](#bib.bib315)], PSPNet [[425](#bib.bib425)], FCN [[236](#bib.bib236)], U-Net
    [[313](#bib.bib313)], etc. | Prostate MR image dataset (PROMISE12, ISBI2013) and
    retinal fundus image dataset (REFUGE challenge dataset [[285](#bib.bib285)]) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| Anomaly Detection in Images [[56](#bib.bib56)] | 2020 | REINFORCE [[392](#bib.bib392)]
    | 9 actions, 8 for directions to shift center of the extracted patch to, the last
    action is to switch to a random new image | Environment: input image to the model.
    State: observed patch from the image centered by predicted center of interest.
    | None | Superior performance in [[27](#bib.bib27)] and [[337](#bib.bib337)] on
    all metrics e.g. precision, recall and F1 when compared with U-Net [[313](#bib.bib313)]
    and baseline unsupervised method in [[27](#bib.bib27)] but only wins on recall
    in [[44](#bib.bib44)] | MVTec AD [[27](#bib.bib27)], NanoTWICE [[44](#bib.bib44)],
    CrackForest [[337](#bib.bib337)] |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: Further improvement in the results of medical image segmentation was proposed
    by [[360](#bib.bib360)]. The authors proposed a method for multi-step medical
    image segmentation using RL, where they used a deep deterministic policy gradient
    method (DDPG) based on actor-critic framework [[262](#bib.bib262)] and similar
    to Deterministic policy gradient (DPG) [[339](#bib.bib339)]. The authors used
    ResNet18 [[133](#bib.bib133)] as backbone for actor and critic network along with
    batch normalisation [[157](#bib.bib157)] and weight normalization with Translated
    ReLU [[400](#bib.bib400)]. In their MDP formulation, the state consisted of the
    image along with the current segmentation mask and step-index, and the reward
    corresponded to the change in mean squared error between the predicted segmentation
    and ground truth across an action. According to the paper the action was defined
    to control the position and shape of brush stroke used to modify the segmentation.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: An example in image segmentation outside the medical field is [[56](#bib.bib56)]
    proposing to tackle the problem of anomalies detection and segmentation in images
    (i.e. damaged pins of an IC chip, small tears in woven fabric). [[56](#bib.bib56)]
    utilizes an additional module to attend only on a specific patch of the image
    centered by a predicted center instead of the whole image, this module helps a
    lot in reducing the imbalance between normal regions and abnormal locations. Given
    an image, this module, namely Neural Batch Sampling (NBS), starts from a random
    initiated center and recurrently moves that center by eight directions to the
    abnormal location in the image if it exists, and it has an additional action to
    stop moving the center when it has already converged to the anomaly location or
    there is not any anomaly can be observed. The NBS module is trained by REINFORCE
    algorithm [[392](#bib.bib392)] and the whole model is evaluated on multiple datasets
    e.g. MVTec AD [[27](#bib.bib27)], NanoTWICE [[44](#bib.bib44)], CrackForest [[337](#bib.bib337)].
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Various works in the fields of Image segmentation have been summarised and
    compared in Table LABEL:tab:seg, and a basic implementation of image segmentation
    using DRL has been shown in Fig. [15](#S9.F15 "Figure 15 ‣ 9 DRL in Image Segmentation
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure shows a general implementation of image segmentation using DRL. The states
    consist of the image along with user hint (landmarks or segmentation mask by the
    other algorithm) for the first iteration or segmentation mask by the previous
    iteration. The actions are responsible for labeling each pixel as foreground and
    background and reward corresponds to an improvement in IOU with iterations as
    used by [[343](#bib.bib343)],[[222](#bib.bib222)].'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/47751bb3b691f2ca58b0fd46cd4d3310.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: DRL implementation for Image segmentation. The state consists of
    the image to be segmented along with a user hint for t=0 or the segmentation mask
    by the previous iterations. The DRL agent performs actions by labeling each pixel
    as foreground and background to maximize the improvement in IOU over the iterations.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 10 DRL in Video Analysis
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object segmentation in videos is a very useful yet challenging task in computer
    vision field. Video object segmentation task focuses on labelling each pixel for
    each frame as foreground or background. Previous works in the field of video object
    segmentation can be divided into three main methods. unsupervised [[288](#bib.bib288)][[402](#bib.bib402)],
    weakly supervised [[48](#bib.bib48)][[163](#bib.bib163)] [[419](#bib.bib419)]
    and semi-supervised [[41](#bib.bib41)] [[164](#bib.bib164)][[292](#bib.bib292)].
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: A DRL-based framework for video object segmentation was proposed by [[323](#bib.bib323)],
    where the authors divided the image into a group of sub-images and then used the
    algorithm on each of the sub-image. They proposed a group of actions that can
    perform to change the local values inside each sub-image and the agent received
    reward based on the change in the quality of segmented object inside each sub-image
    across an action. In the proposed method deep belief network (DBN) [[47](#bib.bib47)]
    was used for approximating the Q-values.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Surgical gesture recognition is a very important yet challenging task in the
    computer vision field. It is useful in assessing surgical skills and for efficient
    training of surgeons. A DRL method for surgical gesture classification and segmentation
    was proposed by [[228](#bib.bib228)]. The proposed method could work on features
    extracted by video frames or kinematic data frames collected by some means along
    with the ground truth labels. The problem of classification and segmentation was
    considered as an MDP, where the state was a concatenation of TCN [[195](#bib.bib195)][[199](#bib.bib199)]
    features of the current frame, 2 future frames a specified number of frames later,
    transition probability of each gesture computed from a statistical language model
    [[311](#bib.bib311)] and a one-hot encoded vector for gesture classes. The actions
    could be divided into two sub-actions, One to decide optimal step size and one
    for choosing gesture class, and the reward was adopted in a way that encouraging
    the agent to adopt a larger step and also penalizes the agent for errors caused
    by the action. The authors used Trust Region Policy Optimization (TRPO) [[326](#bib.bib326)]
    for training the policy and a spacial CNN [[196](#bib.bib196)] to extract features.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Earlier approaches for video object segmentation required a large number of
    actions to complete the task. An Improvement was proposed by [[124](#bib.bib124)],
    where authors used an RL method for object segmentation in videos. They proposed
    a reinforcement cutting-agent learning framework, where the cutting-agent consists
    of a cutting-policy network (CPN) and a cutting-execution network (CEN). The CPN
    learns to predict the object-context box pair, while CEN learns to predict the
    mask based on the inferred object-context box pair. The authors used MDP to solve
    the problem in a semi-supervised fashion. For the state of CPN the authors used
    the input frame information, the action history, and the segmentation mask provided
    in the first frame. The output boxes by CPN were input for the CEN. The actions
    for CPN network included 4 translation actions (Up, Down, Left, Right), 4 scaling
    action (Horizontal shrink, Vertical shrink, Horizontal zoom, Vertical zoom), and
    1 terminal action (Stop), and the reward corresponded to the change in IOU across
    an action. For the network architecture, a Fully-Convolutional DenseNet56 [[166](#bib.bib166)]
    was used as a backbone along with DQN as the agent for CPN and down-sampling followed
    by up-sampling architecture for CEN.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised video object segmentation is an intuitive task in the computer
    vision field. A DRL method for this task was proposed by [[111](#bib.bib111)],
    where the authors proposed a motion-oriented unsupervised method for image segmentation
    in videos (MOREL). They proposed a two-step process to achieve the task in which
    first a representation of input is learned to understand all moving objects through
    unsupervised video object segmentation, Then the weights are transferred to the
    RL framework to jointly train segmentation network along with policy and value
    function. The first part of the method takes two consecutive frames as input and
    predicts a number of segmentation masks, corresponding object translations, and
    camera translations. They used a modified version of actor-critic [[262](#bib.bib262)][[329](#bib.bib329)][[371](#bib.bib371)]
    for the network of first step. Following the unsupervised fashion, the authors
    used the approach similar to [[375](#bib.bib375)] and trained the network to interpolate
    between consecutive frames and used the masks and translations to estimate the
    optical flow using the method that was proposed in Spatial Transformer Networks
    [[159](#bib.bib159)]. They also used structural dissimilarity (DSSIM) [[388](#bib.bib388)]
    to calculate reconstruction loss and actor-critic [[262](#bib.bib262)] algorithm
    to learn policy in the second step.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'A DRL method for dynamic semantic face video segmentation was proposed by [[387](#bib.bib387)],
    where Deep Feature Flow [[431](#bib.bib431)] was utilized as the feature propagation
    framework and RL was used for an efficient and effective scheduling policy. The
    method involved dividing frames into key ($I_{k}$) and non-key ($I_{i}$), and
    using the last key frame features for performing segmentation of non-key frame.
    The actions made by the policy network corresponded to categorizing a frame as
    $I_{k}$ or $I_{i}$ and the state consisted of deviation information and expert
    information, where the deviation information described the difference between
    current $I_{i}$ and last $I_{k}$ and expert information encapsulated the key decision
    history. The authors utilized FlowNet2-s model [[156](#bib.bib156)] as an optical
    flow estimation function, and divided the network into feature extraction module
    and task-specific module. After policy network which consisted of one convolution
    layer, 4 fully connected layers and 2 concatenated channels consisting of KAR
    (Key all ratio: Ratio between key frame and every other frame in decision history)
    and LKD (Last key distance: Distance between current and last key frame) predicted
    the action, If the current frame is categorized as key frame the feature extraction
    module produced the frame features and task-specific module predicted the segmentation,
    However if the frame is categorized as a non-key frame the features from the last
    key frame along with the optical flow was used by the task-specific module to
    predict the segmentation. The authors proposed two types of reward functions,
    The first reward function was calculated by considering the difference between
    the IOU for key and non-key actions. The second reward function was proposed for
    a situation when ground truth was not available and was calculated by considering
    the accuracy score between segmentation for key and non-key actions.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparing various methods associated with video. First group for video
    object segmentation, second group for action recognition and third group for video
    summarisation'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and Source code |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| Object segmentation in videos[[323](#bib.bib323)] | 2016 | Deep Belief Network
    [[47](#bib.bib47)] | Actions changed local values in sub-images | States: sub-images.
    Reward: quality of segmentation. | Not specified | Not specified | Not specified
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| Surgical gesture segmentation and classification [[228](#bib.bib228)] | 2018
    | Trust Region Policy Optimization (TRPO) [[326](#bib.bib326)] | 2 types: optimal
    step size and gesture class | States: TCN [[[195](#bib.bib195)], [[199](#bib.bib199)]]
    and future frames. Reward: encourage larger steps and minimize action errors.
    Statistical language model [[311](#bib.bib311)] for gesture probability. | Spacial
    CNN [[196](#bib.bib196)] | Comparable accuracy, and higher edit and F1 scores
    as compared to methods like SD-SDL [[331](#bib.bib331)], Bidir LSTM [[76](#bib.bib76)],
    LC-SC-CRF [[197](#bib.bib197)], Seg-ST-CNN [[196](#bib.bib196)], TCN [[198](#bib.bib198)],
    etc | JIGSAWS [[[6](#bib.bib6)], [[100](#bib.bib100)]] benchmark dataset [Available
    Code](https://github.com/Finspire13/RL-Surgical-Gesture-Segmentation) |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Cutting agent for video object segmentation [[124](#bib.bib124)] | 2018 |
    DQN | 8 actions: 4 translation actions (Up, Down, Left, Right), 4 scaling action
    (Horizontal shrink, Vertical shrink, Horizontal zoom, Vertical zoom) and 1 terminal
    action (Stop) | States: input frame, action history and segmentation mask. Reward:
    change in IOU. cutting-policy network for box-context pair and cutting-execution
    network for mask generation | DenseNet [[166](#bib.bib166)] | Higher mean region
    similarity, counter accuracy and temporal stability [[293](#bib.bib293)] as compared
    to methods like MSK [[292](#bib.bib292)], ARP [[173](#bib.bib173)], CTN [[165](#bib.bib165)],
    VPN [[164](#bib.bib164)], etc. | DAVIS dataset [[293](#bib.bib293)] and the YouTube
    Objects dataset [[162](#bib.bib162)], [[300](#bib.bib300)] |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised video object segmentation (MOREL) [[111](#bib.bib111)] | 2018
    | Actor-critic (a2c) [[262](#bib.bib262)] | Not specified | States: consecutive
    frames. Two step process with optical flow using Spatial Transformer Networks
    [[159](#bib.bib159)] and reconstruction loss using structural dissimilarity [[388](#bib.bib388)].
    | Multi-layer CNN | Higher total episodic reward as compared to methods that used
    actor-critic without MOREL | 59 Atari games. [Available Code](https://github.com/vik-goel/MOREL)
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| Face video segmentation [[387](#bib.bib387)] | 2020 | Not specified | 2 actions:
    categorising a frame as a key or a non-key | States: deviation information which
    described the difference between current non-key and last key decision, and expert
    information which encapsulated the key decision history. Reward: improvement in
    mean IOU/accuracy score between segmentation of key and non-key frames | Multi-layer
    CNN | Higher mean IOU then other methods like DVSNet [[410](#bib.bib410)], DFF
    [[431](#bib.bib431)]. | 300VW dataset [[336](#bib.bib336)] and Cityscape dataset
    [[61](#bib.bib61)] |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| Multi-agent Video Object Segmentation [[373](#bib.bib373)] | 2020 | DQN |
    Actions of 2 types: movement actions (up, down, left and right) and set action
    (action to place location prior at a random location on the patch) | States: input
    frame, optical flow [[156](#bib.bib156)] from previous frame and action history.
    Reward: clicks generated by gamification. Down-sampling and up-sampling similar
    to U-Net [[313](#bib.bib313)] | DenseNet [[147](#bib.bib147)] | Higher mean region
    similarity and contour accuracy [[293](#bib.bib293)] as compared to semi-supervised
    methods such as SeamSeg [[14](#bib.bib14)], BSVS [[248](#bib.bib248)], VSOF [[363](#bib.bib363)],
    OSVOS [[41](#bib.bib41)] and weakly-supervised methods such as GVOS [[346](#bib.bib346)],
    Spftn [[419](#bib.bib419)] | DAVIS-17 dataset [[293](#bib.bib293)] |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| Skeleton-based Action Recognition [[357](#bib.bib357)] | 2018 | DQN | 3 actions:
    shifting to left, staying the same and shifting to right | States: Global video
    information and selected frames. Reward: change in categorical probability. 2
    step network (FDNet) to filter frames and GCNN for action labels | Multi-layer
    CNN | Higher cross subject and cross view metrics for NTU+RGBD dataset [[333](#bib.bib333)],
    and higher accuracy for SYSU-3D [[145](#bib.bib145)] and UT-Kinect Dataset [[399](#bib.bib399)]
    when compared with other methods like Dynamic Skeletons [[145](#bib.bib145)],
    HBRNN-L [[81](#bib.bib81)], Part-aware LSTM [[333](#bib.bib333)], LieNet-3Blocks
    [[151](#bib.bib151)], Two-Stream CNN [[211](#bib.bib211)], etc. | NTU+RGBD [[333](#bib.bib333)],
    SYSU-3D [[145](#bib.bib145)] and UT-Kinect Dataset [[399](#bib.bib399)] |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| Video summarisation [[429](#bib.bib429)] | 2018 | DQN | 2 actions: selecting
    and rejecting the frame | tates: bidirectional LSTM [[150](#bib.bib150)] produced
    states by input frame features. Reward: Diversity-Representativeness Reward Function.
    | GoogLeNet [[355](#bib.bib355)] | Higher F-score [[421](#bib.bib421)] as compared
    to methods like Uniform sampling, K-medoids, Dictionary selection [[88](#bib.bib88)],
    Video-MMR [[218](#bib.bib218)], Vsumm [[69](#bib.bib69)], etc. | TVSum [[344](#bib.bib344)]
    and SumMe [[122](#bib.bib122)]. [Available Code](https://github.com/KaiyangZhou/pytorch-vsumm-reinforce)
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| Video summarization [[430](#bib.bib430)] | 2018 | Duel DQN Double DQN | 2
    actions: selecting and rejecting the frame | States: sequence of frames Reward:
    Diversity-Representativeness Reward Function 2 stage implementation: classification
    and summarisation network using bidirectional GRU network and LSTM [[150](#bib.bib150)]
    | GoogLeNet [[355](#bib.bib355)] | Higher F-score [[421](#bib.bib421)] as compared
    to methods like Dictionary selection [[88](#bib.bib88)], GAN [[245](#bib.bib245)],
    DR-DSN [[429](#bib.bib429)], Backprop-Grad [[287](#bib.bib287)], etc in most cases.
    | TVSum [[344](#bib.bib344)] and CoSum [[57](#bib.bib57)] datasets. [Available
    Code](https://github.com/KaiyangZhou) |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| Video summarization in Ultrasound [[233](#bib.bib233)] | 2020 | Not specified
    | 2 actions: selecting and rejecting the frame | States: frame latent scores Reward:
    $R_{det}$, $R_{rep}$ and $R_{div}$ bidirectional LSTM [[150](#bib.bib150)] and
    Kernel temporal segmentation [[298](#bib.bib298)] | Not specified | Higher F1-scores
    in supervised and unsupervised fashion as compared to methods like FCSN [[312](#bib.bib312)]
    and DR-DSN [[429](#bib.bib429)]. | Fetal Ultrasound [[179](#bib.bib179)] |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: Video object segmentation using human-provided location priors have been capable
    of producing promising results. An RL method for this task was proposed by [[373](#bib.bib373)],
    in which the authors proposed MASK-RL, a multiagent RL framework for object segmentation
    in videos. They proposed a weakly supervised method where the location priors
    were provided by the user in form of clicks using gamification (Web game to collect
    location priors by different users) to support the segmentation and used a Gaussian
    filter to emphasize the areas. The segmentation network is fed a 12 channel input
    tensor that contained a sequence of video frames and their corresponding location
    priors (3 $\times$ 3 color channels + three gray-scale images). The authors used
    a fully convoluted DenseNet [[147](#bib.bib147)] with down-sampling and up-sampling
    similar to U-Net [[313](#bib.bib313)] and an LSTM [[139](#bib.bib139)] for the
    segmentation network. For the RL method, the actor takes a series of steps over
    a frame divided into a grid of equal size patches and makes the decision whether
    there is an object in the patch or not. In their MDP formulation the states consisted
    of the input frame, optical flow (computed by [[156](#bib.bib156)]) from the previous
    frame, patch from the previous iteration, and the episode location history, the
    actions consisted of movement actions (up, down, left and right) and set action
    (action to place location prior at a random location on the patch), and two types
    of rewards one for set actions and one for movement actions were used. The reward
    was calculated using the clicks generated by the game player.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Action recognition is an important task in the computer vision field which
    focuses on categorizing the action that is being performed in the video frame.
    To address the problem a deep progressive RL (DPRL) method for action recognition
    in skeleton-based videos was proposed by [[357](#bib.bib357)]. The authors proposed
    a method that distills the most informative frames and discards ambiguous frames
    by considering the quality of the frame and the relationship of the frame with
    the complete video along with a graph-based structure to map the human body in
    form of joints and vertices. DPRL was utilized to filter out informative frames
    in a video and graph-based CNNs were used to learn the spatial dependency between
    the joints. The approach consisted of two sub-networks, a frame distillation network
    (FDNet) to filter a fixed number of frames from input sequence using DPRL and
    GCNN to recognize the action labels using output in form of a graphical structure
    by the FDNet. The authors modeled the problem as an MDP where the state consisted
    of the concatenation of two tensors $F$ and $M$, where $F$ consisted of global
    information about the video and $M$ consisted of the frames that were filtered,
    The actions which correspond to the output of FDNet were divided into three types:
    shifting to left, staying the same and shifting to the right, and the reward function
    corresponded to the change in probability of categorizing the video equal to the
    ground truth clipped it between [-1 and 1] and is provided by GCNN to FDNet.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Video summarization is a useful yet difficult task in the computer vision field
    that involves predicting the object or the task that is being performed in a video.
    A DRL method for unsupervised video summarisation was proposed by [[429](#bib.bib429)],
    in which the authors proposed a Diversity-Representativeness reward system and
    a deep summarisation network (DSN) which was capable of predicting a probability
    for each video frame that specified the likeliness of selecting the frame and
    then take actions to form video summaries. They used an encode-decoder framework
    for the DSN where GoogLeNet [[355](#bib.bib355)] pre-trained on ImageNet [[320](#bib.bib320)]
    [[72](#bib.bib72)] was used as an encoder and a bidirectional RNNs (BiRNNs) topped
    with a fully connected (FC) layer was used as a decoder. The authors modeled the
    problem as an MDP where the action corresponded to the task of selecting or rejecting
    a frame. They proposed a novel Diversity-Representativeness Reward Function in
    their implementation, where diversity reward corresponded to the degree of dissimilarity
    among the selected frames in feature space, and representativeness reward measured
    how well the generated summary can represent the original video. For the RNN unit
    they used an LSTM [[139](#bib.bib139)] to capture long-term video dependencies
    and used REINFORCE algorithm for training the policy function.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd162a8f5e9bde527aab23443aacd365.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: DRL implementation for video summarization. For state a sequence
    of consecutive frames are used and the DRL agent decided whether to include the
    frame in the summary set that is used to predict video summary.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'An improvement to [[429](#bib.bib429)] was proposed by [[430](#bib.bib430)],
    where the summarisation network was implemented using Deep Q-learning (DQSN),
    and a trained classification network was used to provide a reward for training
    the DQSN. The approach included using (Bi-GRU) bidirectional recurrent networks
    with a gated recurrent unit (GRU) [[50](#bib.bib50)] for both classification and
    summarisation network. The authors first trained the classification network using
    a supervised classification loss and then used the classification network with
    fixed weights for the classification of summaries generated by the summarisation
    network. The summarisation network included an MDP-based framework in which states
    consisted of a sequence of video frames and actions reflected the task of either
    keeping the frame or discarding it. They used a structure similar to Duel-DQN
    where value function and advantage function are trained together. In their implementation,
    the authors considered 3 different rewards: Global Recognisability reward using
    the classification network with +1 as reward and -5 as punishment, Local Relative
    Importance Reward for rewarding the action of accepting or rejecting a frame by
    summarisation network, and an Unsupervised Reward that is computed globally using
    the unsupervised diversity-representativeness (DR) reward proposed in [[429](#bib.bib429)].
    The authors trained both the networks using the features generated by GoogLeNet
    [[355](#bib.bib355)] pre-trained on ImageNet [[72](#bib.bib72)].'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: A method for video summarization in Ultrasound using DRL was proposed by [[233](#bib.bib233)],
    in which the authors proposed a deep summarisation network in an encoder-decoder
    fashion and used a bidirectional LSTM (Bi-LSTM) [[150](#bib.bib150)] for sequential
    modeling. In their implementation, the encoder-decoder convolution network extracted
    features from video frames and fed them into the Bi-LSTM. The RL network accepted
    states in form of latent scores from Bi-LSTM and produced actions, where the actions
    consist of the task of including or discarding the video frame inside the summary
    set that is used to produce video summaries. The authors used three different
    rewards $R_{det}$, $R_{rep}$ and $R_{div}$ where $R_{det}$ evaluated the likelihood
    of a frame being a standard diagnostic plane, $R_{rep}$ defined the representativeness
    reward and $R_{div}$ was the diversity reward that evaluated the quality of the
    selected summary. They used Kernel temporal segmentation (KTS) [[298](#bib.bib298)]
    for video summary generalization.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Various works associated with video analysis have been summarised and compared
    in Table LABEL:tab:vid and a basic implementation of video summarization using
    DRL has been shown in Fig. [16](#S10.F16 "Figure 16 ‣ 10 DRL in Video Analysis
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"), where
    the states consist of a sequence of video frames. The DRL agent performs actions
    to include or discard a frame from the summary set that is later used by the summarization
    network to predict video summary. Each research paper propose their own reward
    function for this application, for example [[429](#bib.bib429)] and [[430](#bib.bib430)]
    used diversity representativeness reward function and [[233](#bib.bib233)] used
    a combination of various reward functions.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 11 Others Applications
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Object manipulation refers to the task of handling and manipulating an object
    using a robot. A method for deformable object manipulation using RL was proposed
    by [[250](#bib.bib250)], where the authors used a modified version of Deep Deterministic
    Policy Gradients (DDPG) [[224](#bib.bib224)]. They used the simulator Pybullet
    [[63](#bib.bib63)] for the environment where the observation consisted of a $84\times
    84\times 3$ image, the state consists of joint angles and gripper positions and
    action of four dimensions: first three for velocity and lasts for gripper velocity
    was used. The authors used sparse reward for the task that returns the reward
    at the completion of the task. They used the algorithm to perform tasks such as
    folding and hanging cloth and got a success rate of up to 90%.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Visual perception-based control refers to the task of controlling robotic systems
    using a visual input. A virtual to real method for control using semantic segmentation
    was proposed by [[142](#bib.bib142)], in which the authors combined various modules
    such as, Perception module, control policy module, and a visual guidance module
    to perform the task. For the perception module, the authors directly used models
    such as DeepLab [[46](#bib.bib46)] and ICNet [[424](#bib.bib424)], pre-trained
    on ADE20K [[428](#bib.bib428)] and Cityscape [[61](#bib.bib61)], and used the
    output of these model as the state for the control policy module. They implemented
    the control policy module using the actor-critic [[262](#bib.bib262)] framework,
    where the action consisted of forward, turn right, and turn left. In their implementation,
    a reward of 0.001 is given at each time step. They used the Unity3D engine for
    the environment and got higher success and lower collision rate than other implementations
    such as ResNet-A3C and Depth-A3C.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Automatic tracing of structures such as axons and blood vessels is an important
    yet challenging task in the field of biomedical imaging. A DRL method for sub-pixel
    neural tracking was proposed by [[65](#bib.bib65)], where the authors used 2D
    grey-scale images as the environment. They considered a full resolution 11px $\times$
    11px window and a 21px $\times$ 21px window down-scaled to 11px $\times$ 11px
    as state and the actions were responsible for moving the position of agent in
    2D space using continuous control for sub-pixel tracking because axons can be
    smaller then a pixel. The authors used a reward that was calculated using the
    average integral of intensity between the agent’s current and next location, and
    the agent was penalized if it does not move or changes directions more than once.
    They used an Actor-critic [[262](#bib.bib262)] framework to estimate value and
    policy functions.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: An RL method for automatic diagnosis of acute appendicitis in abdominal CT images
    was proposed by [[8](#bib.bib8)], in which the authors used RL to find the location
    of the appendix and then used a CNN classifier to find the likelihood of Acute
    Appendicitis, finally they defined a region of low-entropy (RLE) using the spatial
    representation of output scores to obtain optimal diagnosis scores. The authors
    considered the problem of appendix localization as an MDP, where the state consisted
    of a $50\times 50\times 50$ volume around the predicted appendix location, 6 actions
    (2 per axis) were used and the reward consisted of the change in distance between
    the predicted appendix location and actual appendix location across an action.
    They utilized an Actor-critic [[262](#bib.bib262)] framework to estimate policy
    and value functions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Comparing various other methods besides landmark detection, object
    detection, object tracking, image registration, image segmentation, video analysis,
    that is associated with DRL'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets Source code |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| Object manipulation [[250](#bib.bib250)] | 2018 | Rainbow DDPG | 4 actions:
    3 for velocity 1 for gripper velocity | State: joint angle and gripper position.
    Reward: at the end of task. | Multi layer CNN | Success rate up to 90% | Pybullet
    [[63](#bib.bib63)]. [Code](https://github.com/JanMatas/Rainbow_ddpg) |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| Visual based control [[142](#bib.bib142)] | 2018 | Actor-critic (a3c) [[262](#bib.bib262)]
    | 3 actions: forward, turn right and turn left | State: output by backbones. Reward:
    0.001 at each time-step. | DeepLab [[46](#bib.bib46)] and ICNet [[424](#bib.bib424)]
    | Higher success and lower collision rate then ResNet-A3C and Depth-A3C | Unity3D
    engine |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| Automatic tracing [[65](#bib.bib65)] | 2019 | Actor-critic [[262](#bib.bib262)]
    | 4 actions | State: 11px $\times$ 11px window. Reward: average integral of intensity
    between the agent’s current and next location. | Multi layer CNN | Comparable
    convergence $\%$ and average error as compared to other methods like Vaa3D software
    [[291](#bib.bib291)] and APP2 neuron tracer [[403](#bib.bib403)] | Synthetic and
    microscopy dataset [[24](#bib.bib24)] |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| Automatic diagnosis (RLE) [[8](#bib.bib8)] | 2019 | Actor-critic [[262](#bib.bib262)]
    | 6 actions: 2 per axis | State: $50\times 50\times 50$ volume. Reward: change
    in distance error. | Fully connected CNN | Higher sensitivity and specificity
    as compared to only CNN classifier and CNN classifier with RL without RLE. | Abdominal
    CT Scans |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| Learning to paint [[149](#bib.bib149)] | 2019 | Actor-critic with DDPG |
    Actions control the stoke parameter: location, shape, color and transparency |
    State: Reference image, Drawing canvas and time step. Reward: change in discriminator
    score (calculated by WGAN-GP [[117](#bib.bib117)] across an action. GANs [[113](#bib.bib113)]
    to improve image quality | ResNet18 [[133](#bib.bib133)] | Able to replicate the
    original images to a large extent, and better resemblance to the original image
    as compared to SPIRAL [[98](#bib.bib98)] with same number of brush strokes. |
    MNIST [[202](#bib.bib202)], SVHN [[276](#bib.bib276)], CelebA [[235](#bib.bib235)]
    and ImageNet [[320](#bib.bib320)]. [Code](https://github.com/hzwer/ICCV2019-LearningToPaint)
    |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| Guiding medical robots [[129](#bib.bib129)] | 2020 | Double-DQN, Duel-DQN
    | 5 actions: up, down, left, right and stop | State: probe position. Reward: Move
    closer: 0.05, Move away: -0.1, Correct stop: 1.0, Incorrect stop: -0.25. | ResNet18
    [[133](#bib.bib133)] | Higher % of policy correctness and reachability as compared
    to CNN Classifier, where MS-DQN showed the best results | Ultrasound Images [Dataset](https://github.com/hhase/sacrumdata-set).
    [Code](https://github.com/hhase/spinal-navigation-rl) |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| Crowd counting [[230](#bib.bib230)] | 2020 | DQN | 9 actions: -10, -5, -2,
    -1, +1, +2, +5, +10 and end | State: weight vector $W_{t}$ and image feature vector
    $FV_{I}$. Reward: Intermediate reward and ending reward | VGG16 [[340](#bib.bib340)]
    | Lower/comparable mean squared error (MSE) and mean absolute error (MAE) as compared
    to other methods like DRSAN [[232](#bib.bib232)], PGCNet [[412](#bib.bib412)],
    MBTTBF [[341](#bib.bib341)], S-DCNet [[405](#bib.bib405)], CAN [[234](#bib.bib234)],
    etc. | The ShanghaiTech (SHT) Dataset [[423](#bib.bib423)], The UCFCC50 Dataset
    [[154](#bib.bib154)] and The UCF-QNRF Dataset [[155](#bib.bib155)]. [Code](https://github.com/poppinace/libranet)
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| Automated Exposure bracketing [[389](#bib.bib389)] | 2020 | Not Specified
    | selecting optimal bracketing from candidates | State: quality of generated HDR
    image. Reward: improvement in peak signal to noise ratio | AlexNet [[188](#bib.bib188)]
    | Higher peak signal to noise ratio as compared to other methods like Barakat
    [[22](#bib.bib22)], Pourreza-Shahri [[299](#bib.bib299)], Beek [[369](#bib.bib369)],
    etc. | Proposed benchmark dataset. [Code/data](https://github.com/wzhouxiff/EBSNetMEFNet)
    |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| Urban Autonomous driving [[361](#bib.bib361)] | 2020 | Rainbow-IQN | 36 or
    108 actions: ($9\times 4$) or ($27\times 4$), 9/27 steering and 4 throttle | State:
    environment variables like traffic light, pedestrians, position with respect to
    center lane. Reward: generated by CARLA waypoint API | Resnet18 [[133](#bib.bib133)]
    | Won the 2019 camera only CARLA challenge [[314](#bib.bib314)]. | CARLA urban
    driving simulator [[314](#bib.bib314)] [Code](https://github.com/valeoai/learningbycheating)
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| Mitigating bias in Facial Recognition [[382](#bib.bib382)] | 2020 | DQN |
    3 actions:(Margin adjustment) staying the same, shifting to a larger value and
    shifting to a smaller value | State: the race group, current adaptive margin and
    bias between the race group and Caucasians. Reward: change in the sum of inter-class
    and intra-class bias | Multi-layer CNN | Proposed algorithm had higher verification
    accuracy as compared to other methods such as CosFace [[379](#bib.bib379)] and
    ArcFace [[73](#bib.bib73)]. | RFW [[383](#bib.bib383)] and proposed novel datasets:
    BUPT-Globalface and BUPT-Balancedface [Data](http://www.whdeng.cn/RFW/index.html)
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| Attention mechanism to improve CNN performance [[212](#bib.bib212)] | 2020
    | DQN [[264](#bib.bib264)] | Actions are weights for every location or channel
    in the feature map. | State: Feature map at each intermediate layer of model.
    Reward: predicted by a LSTM model. | ResNet-101 [[133](#bib.bib133)] | Improves
    the performances of [[144](#bib.bib144)], [[205](#bib.bib205)] and [[396](#bib.bib396)],
    which attend on feature channel, spatial-channel and style, respectively | ImageNet
    [[72](#bib.bib72)] |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/70ccfd4e0c164cd86be06ea3df464348.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: A general DRL implementation for agent movement with visual inputs.
    The state is provided by the environment based on which the agent performs movement
    actions to get a new state and a reward from the environment.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Painting using an algorithm is a fantastic yet challenging task in the computer
    vision field. An automated painting method was proposed by [[149](#bib.bib149)],
    where the authors introduced a model-based DRL technique for this task. The specified
    work involved using a neural renderer in DRL, where the agent was responsible
    for making a decision about the position and color of each stroke, and making
    long-term decisions to organize those strokes into a visual masterpiece. In this
    work, GANs [[113](#bib.bib113)] were employed to improve image quality at pixel-level
    and DDPG [[224](#bib.bib224)] was utilized for determining the policy. The authors
    formulated the problem as an MDP, where the state consisted of three parts: the
    target image $I$, the canvas on which actions (paint strokes) are performed $C_{t}$,
    and the time step. The actions corresponding to a set of parameters that controlled
    the position, shape, color, and transparency of strokes, and for reward the WGAN
    with gradient penalty (WGAN-GP) [[117](#bib.bib117)] was used to calculate the
    discriminator score between the target image $I$ and the canvas $C_{t}$, and the
    change in discriminator score across an action (time-step) was used as the reward.
    The agent that predicted the stroke parameters was trained in actor-critic [[262](#bib.bib262)]
    fashion with backbone similar to Resnet18 [[133](#bib.bib133)], and the stroke
    parameters by the actor were used by the neural renderer network to predict paint
    strokes. The network structure of the neural renderer and discriminator consisted
    of multiple convolutions and fully connected blocks.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'A method for guiding medical robots using Ultrasound images with the help of
    DRL was proposed by [[129](#bib.bib129)]. The authors treated the problem as an
    MDP where the agent takes the Ultrasound images as input and estimates the state
    hence the problem became Partially observable MDP (POMDP). They used Double-DQN
    and Duel-DQN for estimating Q-Values and ResNet18 [[133](#bib.bib133)] backbone
    for extracting feature to be used by the algorithm along with Prioritized Replay
    Memory. In their implementation the action space consisted of 8 actions (up, down,
    left, right, and stop), probe position as compared to the sacrum was used as the
    state and the reward was calculated by considering the agent position as compared
    to the target (Move closer: 0.05, Move away: -0.1, Correct stop: 1.0, Incorrect
    stop: -0.25). In their implementation, the authors proposed various architectures
    such as V-DQN, M-DQN, and MS-DQN for the task and performed experimentation on
    Ultrasound images.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'Crowd counting is considered a tricky task in computer vision and is even trickier
    for humans. A DRL method for crowd counting was proposed by [[230](#bib.bib230)],
    where the authors used sequential decision making to approach the task through
    RL. In the specified work, the authors proposed a DQN agent (LibraNet) based on
    the motivation of a weighing scale. In their implementation crowd counting was
    modeled using a weighing scale where the agent was responsible for adding weights
    on one side of the scale sequentially to balance the crowded image on the other
    side. The problem of adding weights on one side of the pan for balancing was formulated
    as an MDP, where state consisted weight vector $W_{t}$ and image feature vector
    $FV_{I}$, and the actions space was defined similar to scale weighing and money
    system [[372](#bib.bib372)] containing values $(-10,-5,-2,-1,+1,+2,+5,+10,end)$.
    For reinforcing the agent two different rewards: ending reward and intermediate
    reward were utilized, where ending reward (following [[43](#bib.bib43)]) was calculated
    by comparing the absolute value error between the ground-truth count and the accumulated
    value with the error tolerance, and three counting specific rewards: force ending
    reward, guiding reward and squeezing reward were calculated for the intermediate
    rewards.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Exposure bracketing is a method used in digital photography, where one scene
    is captured using multiple exposures for getting a high dynamic range (HDR) image.
    An RL method for automated bracketing selection was proposed by [[389](#bib.bib389)].
    For flexible automated bracketing selection, an exposure bracketing selection
    network (EBSNet) was proposed for selecting optimal exposure bracketing and a
    multi-exposure fusion network (MEFNet) for generating an HDR image from selected
    exposure bracketing which consisted of 3 images. Since there is no ground truth
    for the exposure bracketing selection procedure, an RL scheme was utilized to
    train the agent (EBSNet). The authors also introduced a novel dataset consisting
    of a single auto-exposure image that was used as input to the EBSNet, 10 images
    with varying exposures from which EBSNet generated probability distribution for
    120 possible candidate exposure bracketing ($C^{3}_{10}$) and a reference HDR
    image. The reward for EBSNet was defined as the difference between peak signal-to-noise
    ratio between generated and reference HDR for the current and previous iteration,
    and the MEFNet was trained by minimizing the Charbonnier loss [[23](#bib.bib23)].
    For performing the action of bracketing selection ESBNet consisted of a semantic
    branch using AlexNet [[188](#bib.bib188)] for feature extraction, an illumination
    branch to understand the global and local illuminations by calculating a histogram
    of input and feeding it to CNN layers, and a policy module to generate a probability
    distribution for the candidate exposure bracketing from semantic and illumination
    branches. The neural network for MEFNet was derived from HDRNet [[103](#bib.bib103)].
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous driving in an urban environment is a challenging task, because of
    a large number of environmental variables and constraints. A DRL approach to this
    problem was proposed by [[361](#bib.bib361)]. In their implementation, the authors
    proposed an end-to-end model-free RL method, where they introduced a novel technique
    called Implicit Affordances. For the environment, the CARLA Simulator [[80](#bib.bib80)]
    was utilized, which provided the observations and the training reward was obtained
    by using the CARLA waypoint API. In the novel implicit affordances technique the
    training was broken into two phases, The first phase included using a Resnet18
    [[133](#bib.bib133)] encoder to predict the state of various environment variables
    such as traffic light, pedestrians, position with respect to the center lane,
    etc., and the output features were used as a state for the RL agent, For which
    a modified version of Rainbow-IQN Ape-X [[136](#bib.bib136)] was used. CARLA simulator
    accepts actions in form of continuous steering and throttle values, so to make
    it work with Rainbow-IQN which supports discrete actions, the authors sampled
    steering values into 9 or 27 discrete values and throttle into 4 discrete values
    (including braking), making a total of 36($9\times 4$) or 108($27\times 4$) actions.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Racial discrimination has been one of the hottest topics of the 21st century.
    To mitigate racial discrimination in facial recognition, [[382](#bib.bib382)]
    proposed a facial recognition method using skewness-aware RL. According to the
    authors, the reason for racial bias in facial recognition algorithms can be either
    due to the data or due to the algorithm, so the authors provided two ethnicity-aware
    datasets, BUPT-Globalface and BUPT-Balancedface along with an RL based race balanced
    network (RL-RBN). In their implementation, the authors formulated an MDP for adaptive
    margin policy learning where the state consisted of three parts: the race group
    (0: Indian, 1: Asian, 2: African), current adaptive margin, and bias or the skewness
    between the race group and Caucasians. A DQN was used as a policy network that
    performed three actions (staying the same, shifting to a larger value, and shifting
    to a smaller value) to change the adaptive margin, and accepted reward in form
    of change in the sum of inter-class and intra-class bias.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms are currently gaining popularity because of their powerful
    ability in eliminating uninformative parts of the input to leverage the other
    parts having a more useful information. Recently, attention mechanism has been
    integrated into typical CNN models at every individual layer to strengthen the
    intermediate outputs of each layer, in turn improving the final predictions for
    recognition in images. This model is usually trained with a weakly supervised
    method, however, this optimization method may lead to sub-optimal weights in the
    attention module. Hence, [[212](#bib.bib212)] proposed to train attention module
    by deep Q-learning with an LSTM model is trained to predict the reward, the whole
    process is called Deep REinforced Attention Learning (DREAL).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Various works specified here have been summarised and compared in Table LABEL:tab:oth
    and general implementation of a DRL method to control an agents movement in an
    environment has been shown in fig [17](#S11.F17 "Figure 17 ‣ 11 Others Applications
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") where
    state consists of an image frame provided by the environment, the DRL agent predicts
    actions to move the agent in the environment providing next state and the reward
    is provided by the environment, for example, [[142](#bib.bib142)].'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 12 Future Perspectives
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 12.1 Challenge Discussion
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DRL is a powerful framework, which has been successfully applied to various
    computer vision applications including landmark detection, object detection, object
    tracking, image registration, image segmentation, video analysis, and other computer
    vision applications. DRL has also demonstrated to be an effective alternative
    for solving difficult optimization problems, including tuning parameters, selecting
    augmentation strategies, and neural architecture search (NAS). However, most approaches,
    that we have reviewed, assume a stationary environment, from which observations
    are made. Take landmark detection as an instance, the environment takes into account
    the image itself, and each state is defined as an image patch consisting of the
    landmark location. In such a case, the environment is known while the RL/DRL framework
    naturally accommodates a dynamic environment, that is the environment itself evolves
    with the state and action. Realizing the full potential of DRL for computer vision
    requires solving several challenges. In this section, we would like to discuss
    the challenges of DRL in computer vision for real-world systems.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward function: In most real-world applications, it is hard to define a specified
    reward function because it requires the knowledge from different domains that
    may not always be available. Thus, the intermediate rewards at each time step
    are not always easily computed. Furthermore, a reward function with too long delay
    will make training difficult. In contrast, assigning a reward for each action
    requires careful and manual human design.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Continuous state and action space: Training an RL system on a continuous state
    and action space is challenging because most RL algorithms, i.e. Q learning, can
    only deal with discrete states and discrete action space. To address this limitation,
    most existing works discretize the continuous state and action space.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'High-dimensional state and action space: Training Q-function on a high-dimensional
    action space is challenging. For this reason, existing works use low-dimensional
    parameterization, whose dimensions are typically less than 10 with an exception
    [[184](#bib.bib184)] that uses 15-D and 25-D to model 2D and 3D registration,
    respectively.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Environment is complicated: Almost all real-world systems, where we would want
    to deploy DRL/RL, are partially observable and non-stationary. Currently, the
    approaches we have reviewed assume a stationary environment, from which observations
    are made. However, the DRL/RL framework naturally accommodates dynamic environment,
    that is the environment itself evolves with the state and action. Furthermore,
    those systems are often stochastic and noisy (action delay, sensor and action
    noise) as compared to most simulated environments.'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training data requirement: RL/DRL requires a large amount of training data
    or expert demonstrations. Large-scale datasets with annotations are expensive
    and hard to come by.'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: More details of challenges that embody difficulties to deploy RL/DRL in the
    real world are discussed in [[82](#bib.bib82)]. In this work, they designed a
    set of experiments and analyzed their effects on common RL agents. Open-sourcing
    an environmental suite, [realworldrl-suite](https://github.com/google-research/realworldrl_suite)
    [[83](#bib.bib83)] is provided in this work as well.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 DRL Recent Advances
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some advanced DRL approaches such as Inverse DRL, Multi-agent DRL, Meta DRL,
    and imitation learning are worth the attention and may promote new insights for
    many machine learning and computer vision tasks.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inverse DRL: DRL has been successfully applied into domains where the reward
    function is clearly defined. However, this is limited in real-world applications
    because it requires knowledge from different domains that may not always be available.
    Inverse DRL is one of the special cases of imitation learning. An example is autonomous
    driving, the reward function should be based on all factors such as driver’s behavior,
    gas consumption, time, speed, safety, driving quality, etc. In real-world scenario,
    it is exhausting and hard to control all these factors. Different from DRL, inverse
    DRL [[278](#bib.bib278)], [[4](#bib.bib4)], [[413](#bib.bib413)], [[86](#bib.bib86)]
    a specific form of imitation learning [[286](#bib.bib286)], infers the reward
    function of an agent, given its policy or observed behavior, thereby avoiding
    a manual specification of its reward function. In the same problem of autonomous
    driving, inverse RL first uses a dataset collected from the human-generated driving
    and then approximates the reward function. Inverse RL has been successfully applied
    to many domains [[4](#bib.bib4)]. Recently, to analyze complex human movement
    and control high-dimensional robot systems, [[215](#bib.bib215)] proposed an online
    inverse RL algorithm. [[2](#bib.bib2)] combined both RL and Inverse RL to address
    planning problems in autonomous driving.'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-Agent DRL: Most of the successful DRL applications such as game[[38](#bib.bib38)],
    [[376](#bib.bib376)], robotics[[181](#bib.bib181)], and autonomous driving [[335](#bib.bib335)],
    stock trading [[206](#bib.bib206)], social science [[207](#bib.bib207)], etc.,
    involve multiple players that requires a model with multi-agent. Take autonomous
    driving as an instance, multi-agent DRL addresses the sequential decision-making
    problem which involves many autonomous agents, each of which aims to optimize
    its own utility return by interacting with the environment and other agents [[40](#bib.bib40)].
    Learning in a multi-agent scenario is more difficult than a single-agent scenario
    because non-stationarity [[135](#bib.bib135)], multi-dimensionality [[40](#bib.bib40)],
    credit assignment [[5](#bib.bib5)], etc., depend on the multi-agent DRL approach
    of either fully cooperative or fully competitive. The agents can either collaborate
    to optimize a long-term utility or compete so that the utility is summed to zero.
    Recent work on Multi-Agent RL pays attention to learning new criteria or new setup
    [[348](#bib.bib348)].'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta DRL: As aforementioned, DRL algorithms consume large amounts of experience
    in order to learn an individual task and are unable to generalize the learned
    policy to newer problems. To alleviate the data challenge, Meta-RL algorithms
    [[330](#bib.bib330)], [[380](#bib.bib380)] are studied to enable agents to learn
    new skills from small amounts of experience. Recently, there is a research interest
    in meta RL [[271](#bib.bib271)], [[119](#bib.bib119)], [[322](#bib.bib322)], [[303](#bib.bib303)],
    [[229](#bib.bib229)], each using a different approach. For benchmarking and evaluation
    of meta RL algorithms, [[415](#bib.bib415)] presented Meta-world, which is an
    open-source simulator consisting of 50 distinct robotic manipulation tasks.'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Imitation Learning: Imitation learning is close to learning from demonstrations
    which aims at training a policy to mimic an expert’s behavior given the samples
    collected from that expert. Imitation learning is also considered as an alternative
    to RL/DRL to solve sequential decision-making problems. Besides inverse DRL, an
    imitation learning approach as aforementioned, behavior cloning is another imitation
    learning approach to train policy under supervised learning manner. Bradly et
    al. [[347](#bib.bib347)] presented a method for unsupervised third-person imitation
    learning to observe how other humans perform and infer the task. Building on top
    of Deep Deterministic Policy Gradients and Hindsight Experience Replay, Nair et
    al. [[272](#bib.bib272)] proposed behavior cloning Loss to increase imitating
    the demonstrations. Besides Q-learning, Generative Adversarial Imitation Learning
    [[364](#bib.bib364)] proposes P-GAIL that integrates imitation learning into the
    policy gradient framework. P-GAIL considers both smoothness and causal entropy
    in policy update by utilizing Deep P-Network [[365](#bib.bib365)].'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning (DRL) is nowadays the most popular technique for
    an artificial agent to learn closely optimal strategies by experiences. This paper
    aims to provide a state-of-the-art comprehensive survey of DRL applications to
    a variety of decision-making problems in the area of computer vision. In this
    work, we firstly provided a structured summarization of the theoretical foundations
    in Deep Learning (DL) including AutoEncoder (AE), Multi-Layer Perceptron (MLP),
    Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN). We then
    continued to introduce key techniques in RL research including model-based methods
    (value functions, transaction models, policy search, return functions) and model-free
    methods (value-based, policy-based, and actor-critic). Main techniques in DRL
    were thirdly presented under two categories of model-based and model-free approaches.
    We fourthly surveyed the broad-ranging applications of DRL methods in solving
    problems affecting areas of computer vision, from landmark detection, object detection,
    object tracking, image registration, image segmentation, video analysis, and many
    other applications in the computer vision area. We finally discussed several challenges
    ahead of us in order to realize the full potential of DRL for computer vision.
    Some latest advanced DRL techniques were included in the last discussion.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Model-based contextual policy search for data-efficient generalization
    of robot skills. Artificial Intelligence, 247:415 – 439, 2017.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Advanced planning for autonomous vehicles using reinforcement learning
    and deep inverse reinforcement learning. Robotics and Autonomous Systems, 114:1
    – 18, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Pieter Abbeel, Adam Coates, and Andrew Y. Ng. Autonomous helicopter aerobatics
    through apprenticeship learning. The International Journal of Robotics Research,
    29(13):1608–1639, 2010.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement
    learning. In Proceedings of the Twenty-First International Conference on Machine
    Learning, pages 1–8\. Association for Computing Machinery, 2004.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Adrian K. Agogino and Kagan Tumer. Unifying temporal and structural credit
    assignment problems. In Proceedings of the Third International Joint Conference
    on Autonomous Agents and Multiagent Systems - Volume 2, page 980–987\. IEEE Computer
    Society, 2004.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Narges Ahmidi, Lingling Tao, Shahin Sefati, Yixin Gao, Colin Lea, Benjamin Bejar
    Haro, Luca Zappella, Sanjeev Khudanpur, René Vidal, and Gregory D Hager. A dataset
    and benchmarks for segmentation and recognition of gestures in robotic surgery.
    IEEE Transactions on Biomedical Engineering, 64(9):2025–2041, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Walid Abdullah Al and Il Dong Yun. Partial policy-based reinforcement learning
    for anatomical landmark localization in 3d medical images. IEEE transactions on
    medical imaging, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Walid Abdullah Al, Il Dong Yun, and Kyong Joon Lee. Reinforcement learning-based
    automatic diagnosis of acute appendicitis in abdominal ct. arXiv preprint arXiv:1909.00617,
    2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Stephan Alaniz. Deep reinforcement learning with model learning and monte
    carlo tree search in minecraft. In Conference on Reinforcement Learning and Decision
    Making, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Amir Alansary, Ozan Oktay, Yuanwei Li, Loic Le Folgoc, Benjamin Hou, Ghislain
    Vaillant, Konstantinos Kamnitsas, Athanasios Vlontzos, Ben Glocker, Bernhard Kainz,
    et al. Evaluating reinforcement learning agents for anatomical landmark detection.
    Medical image analysis, 53:156–164, 2019.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection
    using reconstruction probability. Special Lecture on IE, 2(1):1–18, 2015.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] O. Andersson, F. Heintz, and P. Doherty. Model-based reinforcement learning
    in continuous environments using real-time constrained optimization. In AAAI,
    2015.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony
    Bharath. A brief survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866,
    2017.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] S Avinash Ramakanth and R Venkatesh Babu. Seamseg: Video object segmentation
    using patch seams. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 376–383, 2014.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Morgane Ayle, Jimmy Tekli, Julia El-Zini, Boulos El-Asmar, and Mariette
    Awad. Bar-a reinforcement learning agent for bounding-box automated refinement.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan
    Kautz. GA3C: gpu-based A3C for deep reinforcement learning. CoRR, abs/1611.06256,
    2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. Visual tracking with
    online multiple instance learning. In 2009 IEEE conference on computer vision
    and pattern recognition, pages 983–990\. IEEE, 2009.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Seung-Hwan Bae and Kuk-Jin Yoon. Robust online multi-object tracking based
    on tracklet confidence and online discriminative appearance learning. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 1218–1225,
    2014.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Seung-Hwan Bae and Kuk-Jin Yoon. Confidence-based data association and
    discriminative deep appearance learning for robust online multi-object tracking.
    IEEE transactions on pattern analysis and machine intelligence, 40(3):595–610,
    2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J. Bagnell. Learning decision: Robustness, uncertainty, and approximation.
    04 2012.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. A. Bagnell and J. G. Schneider. Autonomous helicopter control using
    reinforcement learning policy search methods. In Proceedings 2001 ICRA. IEEE International
    Conference on Robotics and Automation (Cat. No.01CH37164), volume 2, pages 1615–1620,
    2001.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Neil Barakat, A Nicholas Hone, and Thomas E Darcie. Minimal-bracketing
    sets for high-dynamic-range image capture. IEEE Transactions on Image Processing,
    17(10):1864–1875, 2008.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Jonathan T Barron. A general and adaptive robust loss function. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4331–4339,
    2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Cher Bass, Pyry Helkkula, Vincenzo De Paola, Claudia Clopath, and Anil Anthony
    Bharath. Detection of axonal synapses in 3d two-photon images. PloS one, 12(9):e0183309,
    2017.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Miriam Bellver, Xavier Giró-i Nieto, Ferran Marqués, and Jordi Torres.
    Hierarchical object detection with deep reinforcement learning. arXiv preprint
    arXiv:1611.03718, 2016.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term
    dependencies with gradient descent is difficult. IEEE Trans. Neural Networks,
    5(2):157–166, 1994.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad — a comprehensive
    real-world dataset for unsupervised anomaly detection. In 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 9584–9592, 2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking
    performance: the clear mot metrics. EURASIP Journal on Image and Video Processing,
    2008:1–10, 2008.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and
    Philip HS Torr. Fully-convolutional siamese networks for object tracking. In European
    conference on computer vision, pages 850–865. Springer, 2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Shalabh Bhatnagar. An actor–critic algorithm with function approximation
    for discounted cost constrained markov decision processes. Systems & Control Letters,
    59(12):760–766, 2010.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Shalabh Bhatnagar, Richard S. Sutton, Mohammad Ghavamzadeh, and Mark Lee.
    Natural actorâ-critic algorithms. Automatica, 45(11):2471 – 2482, 2009.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Michael J Black and Yaser Yacoob. Tracking and recognizing rigid and non-rigid
    facial motions using local parametric models of image motion. In Proceedings of
    IEEE international conference on computer vision, pages 374–381\. IEEE, 1995.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] N Bloch, A Madabhushi, H Huisman, J Freymann, J Kirby, M Grauer, A Enquobahrie,
    C Jaffe, L Clarke, and K Farahani. Nci-isbi 2013 challenge: automated segmentation
    of prostate structures. The Cancer Imaging Archive, 370, 2015.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Boedecker, J. T. Springenberg, J. Wülfing, and M. Riedmiller. Approximate
    real-time optimal control based on sparse gaussian process models. In 2014 IEEE
    Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),
    pages 1–8, 2014.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d region proposal
    network for object detection. In Proceedings of the IEEE International Conference
    on Computer Vision, Seoul, South Korea, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative interaction
    training for segmentation editing networks. In International Workshop on Machine
    Learning in Medical Imaging, pages 363–370\. Springer, 2018.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science,
    365(6456):885–890, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Antoine Buetti-Dinh, Vanni Galli, SÃ¶ren Bellenberg, Olga Ilie, Malte
    Herold, Stephan Christel, Mariia Boretska, Igor V. Pivkin, Paul Wilmes, Wolfgang
    Sand, Mario Vera, and Mark Dopson. Deep neural networks outperform human expert’s
    capacity in characterizing bioleaching bacterial biofilm composition. Biotechnology
    Reports, 22:e00321, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of
    multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics,
    Part C (Applications and Reviews), 38(2):156–172, 2008.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixé,
    Daniel Cremers, and Luc Van Gool. One-shot video object segmentation. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 221–230,
    2017.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Yunliang Cai, Said Osman, Manas Sharma, Mark Landis, and Shuo Li. Multi-modality
    vertebra recognition in arbitrary views using 3d deformable hierarchical model.
    IEEE transactions on medical imaging, 34(8):1676–1693, 2015.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Juan C Caicedo and Svetlana Lazebnik. Active object localization with
    deep reinforcement learning. In Proceedings of the IEEE international conference
    on computer vision, pages 2488–2496, 2015.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] D. Carrera, F. Manganini, G. Boracchi, and E. Lanzarone. Defect detection
    in sem images of nanofibrous materials. IEEE Transactions on Industrial Informatics,
    13(2):551–561, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and Huchuan Lu. Real-time’actor-critic’tracking.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 318–334,
    2018.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
    Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets,
    atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis
    and machine intelligence, 40(4):834–848, 2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Yushi Chen, Xing Zhao, and Xiuping Jia. Spectral–spatial classification
    of hyperspectral data based on deep belief network. IEEE Journal of Selected Topics
    in Applied Earth Observations and Remote Sensing, 8(6):2381–2392, 2015.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-Hsuan Yang. Segflow:
    Joint learning for video object segmentation and optical flow. In Proceedings
    of the IEEE international conference on computer vision, pages 686–695, 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min
    Hu. Global contrast based salient region detection. IEEE transactions on pattern
    analysis and machine intelligence, 37(3):569–582, 2014.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using rnn encoder-decoder for statistical machine translation. arXiv preprint
    arXiv:1406.1078, 2014.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares,
    Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
    for statistical machine translation. CoRR, abs/1406.1078, 2014.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris,
    and Jin Young Choi. Attentional correlation filter network for adaptive visual
    tracking. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pages 4807–4816, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Wongun Choi. Near-online multi-target tracking with aggregated local flow
    descriptor. In Proceedings of the IEEE international conference on computer vision,
    pages 3029–3037, 2015.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, and Yoshua
    Bengio. Attention-based models for speech recognition. CoRR, abs/1506.07503, 2015.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai
    Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal
    attention mechanism. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 4836–4845, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Wen-Hsuan Chu and Kris M. Kitani. Neural batch sampling with reinforcement
    learning for semi-supervised anomaly detection. In European Conference on Computer
    Vision, pages 751–766, 2020.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Wen-Sheng Chu, Yale Song, and Alejandro Jaimes. Video co-summarization:
    Video summarization by visual co-occurrence. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 3584–3592, 2015.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim
    Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy
    optimization. CoRR, abs/1809.05214, 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Adam Coates, Pieter Abbeel, and Andrew Y. Ng. Apprenticeship learning
    for helicopter control. Commun. ACM, 52(7):97–105, July 2009.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Dorin Comaniciu, Visvanathan Ramesh, and Peter Meer. Real-time tracking
    of non-rigid objects using mean shift. In Proceedings IEEE Conference on Computer
    Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662), volume 2, pages
    142–149\. IEEE, 2000.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
    Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset
    for semantic urban scene understanding. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 3213–3223, 2016.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Rémi Coulom. Efficient selectivity and backup operators in monte-carlo
    tree search. In Proceedings of the 5th International Conference on Computers and
    Games, page 72–83, 2006.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation
    for games, robotics and machine learning. 2016.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Antonio Criminisi, Jamie Shotton, Duncan Robertson, and Ender Konukoglu.
    Regression forests for efficient anatomy detection and localization in ct studies.
    In International MICCAI Workshop on Medical Computer Vision, pages 106–117\. Springer,
    2010.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Tianhong Dai, Magda Dubois, Kai Arulkumaran, Jonathan Campbell, Cher Bass,
    Benjamin Billot, Fatmatulzehra Uslu, Vincenzo De Paola, Claudia Clopath, and Anil Anthony
    Bharath. Deep reinforcement learning for subpixel neural tracking. In International
    Conference on Medical Imaging with Deep Learning, pages 130–150, 2019.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg.
    Eco: Efficient convolution operators for tracking. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 6638–6646, 2017.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and Michael Felsberg.
    Learning spatially regularized correlation filters for visual tracking. In Proceedings
    of the IEEE international conference on computer vision, pages 4310–4318, 2015.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Kristopher De Asis, J Fernando Hernandez-Garcia, G Zacharias Holland,
    and Richard S Sutton. Multi-step reinforcement learning: A unifying algorithm.
    In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Sandra Eliza Fontes De Avila, Ana Paula Brandão Lopes, Antonio da Luz Jr,
    and Arnaldo de Albuquerque Araújo. Vsumm: A mechanism designed to produce static
    video summaries and a novel evaluation method. Pattern Recognition Letters, 32(1):56–68,
    2011.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Antonio de Marvao, Timothy JW Dawes, Wenzhe Shi, Christopher Minas, Niall G
    Keenan, Tamara Diamond, Giuliana Durighel, Giovanni Montana, Daniel Rueckert,
    Stuart A Cook, et al. Population-based studies of myocardial hypertrophy: high
    resolution cardiovascular magnetic resonance atlases improve statistical power.
    Journal of cardiovascular magnetic resonance, 16(1):16, 2014.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. P. Deisenroth, P. Englert, J. Peters, and D. Fox. Multi-task policy
    search for robotics. In 2014 IEEE International Conference on Robotics and Automation
    (ICRA), pages 3876–3881, 2014.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
    Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on
    computer vision and pattern recognition, pages 248–255\. Ieee, 2009.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface:
    Additive angular margin loss for deep face recognition. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 4690–4699, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Joachim Denzler and Dietrich WR Paulus. Active motion detection and object
    tracking. In Proceedings of 1st International Conference on Image Processing,
    volume 3, pages 635–639\. IEEE, 1994.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] B. Depraetere, M. Liu, G. Pinte, I. Grondman, and R. BabuÅ¡ka. Comparison
    of model-free and model-based methods for time optimal hit control of a badminton
    robot. Mechatronics, 24(8):1021 – 1030, 2014.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S Swaroop Vedula,
    Gyusung I Lee, Mija R Lee, and Gregory D Hager. Recognizing surgical activities
    with recurrent neural networks. In International conference on medical image computing
    and computer-assisted intervention, pages 551–558\. Springer, 2016.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Piotr Dollár, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian
    detection: A benchmark. In 2009 IEEE Conference on Computer Vision and Pattern
    Recognition, pages 304–311\. IEEE, 2009.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach,
    Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional
    networks for visual recognition and description. CoRR, abs/1411.4389, 2014.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
    Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
    Learning optical flow with convolutional networks. In Proceedings of the IEEE
    international conference on computer vision, pages 2758–2766, 2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
    Koltun. Carla: An open urban driving simulator. arXiv preprint arXiv:1711.03938,
    2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Yong Du, Wei Wang, and Liang Wang. Hierarchical recurrent neural network
    for skeleton based action recognition. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 1110–1118, 2015.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin
    Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges
    of real-world reinforcement learning, 2020.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin
    Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges
    of real-world reinforcement learning. 2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Matteo Dunnhofer, Niki Martinel, Gian Luca Foresti, and Christian Micheloni.
    Visual tracking by means of deep reinforcement learning and an expert demonstrator.
    In Proceedings of the IEEE International Conference on Computer Vision Workshops,
    pages 0–0, 2019.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Chi Nhan Duong, Kha Gia Quach, Ibsa Jalata, Ngan Le, and Khoa Luu. Mobiface:
    A lightweight deep learning face recognition on mobile devices. In 2019 IEEE 10th
    International Conference on Biometrics Theory, Applications and Systems (BTAS),
    pages 1–6\. IEEE, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Le, Marios Savvides,
    and Tien D. Bui. Learning from longitudinal face demonstration–where tractable
    deep modeling meets inverse reinforcement learning. 127(6–7), 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. El-Fakdi and M. Carreras. Policy gradient based reinforcement learning
    for real autonomous underwater cable tracking. In 2008 IEEE/RSJ International
    Conference on Intelligent Robots and Systems, pages 3635–3640, 2008.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Ehsan Elhamifar, Guillermo Sapiro, and Rene Vidal. See all by looking
    at a few: Sparse modeling for finding representative objects. In 2012 IEEE conference
    on computer vision and pattern recognition, pages 1600–1607\. IEEE, 2012.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov.
    Scalable object detection using deep neural networks. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 2147–2154, 2014.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and
    Andrew Zisserman. The pascal visual object classes challenge 2007 (voc2007) results.
    2007.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Mark Everingham and John Winn. The pascal visual object classes challenge
    2012 (voc2012) development kit. Pattern Analysis, Statistical Modelling and Computational
    Learning, Tech. Rep, 8, 2011.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai,
    Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale
    single object tracking. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 5374–5383, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Heng Fan and Haibin Ling. Parallel tracking and verifying: A framework
    for real-time and high accuracy visual tracking. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 5486–5494, 2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Jialue Fan, Wei Xu, Ying Wu, and Yihong Gong. Human tracking using convolutional
    neural networks. IEEE Transactions on Neural Networks, 21(10):1610–1623, 2010.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and
    Pieter Abbeel. Deep spatial autoencoders for visuomotor learning. In Danica Kragic,
    Antonio Bicchi, and Alessandro De Luca, editors, 2016 IEEE International Conference
    on Robotics and Automation, ICRA 2016, Stockholm, Sweden, May 16-21, 2016, pages
    512–519.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J Michael Fitzpatrick and Jay B West. The distribution of target registration
    error in rigid-body point-based registration. IEEE transactions on medical imaging,
    20(9):917–927, 2001.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare,
    and Joelle Pineau. An introduction to deep reinforcement learning. arXiv preprint
    arXiv:1811.12560, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Eslami, and Oriol
    Vinyals. Synthesizing programs for images using reinforced adversarial learning.
    arXiv preprint arXiv:1804.01118, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and Larry S Davis. Dynamic
    zoom-in network for fast object detection in large images. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 6926–6935, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Yixin Gao, S Swaroop Vedula, Carol E Reiley, Narges Ahmidi, Balakrishnan
    Varadarajan, Henry C Lin, Lingling Tao, Luca Zappella, Benjamın Béjar, David D
    Yuh, et al. Jhu-isi gesture and skill assessment working set (jigsaws): A surgical
    activity dataset for human motion modeling. In Miccai workshop: M2cai, volume 3,
    page 3, 2014.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Romane Gauriau, Rémi Cuingnet, David Lesage, and Isabelle Bloch. Multi-organ
    localization combining global-to-local regression and confidence maps. In International
    Conference on Medical Image Computing and Computer-Assisted Intervention, pages
    337–344\. Springer, 2014.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving?
    the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and
    Pattern Recognition, pages 3354–3361, 2012.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Michaël Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W Hasinoff, and
    Frédo Durand. Deep bilateral learning for real-time image enhancement. ACM Transactions
    on Graphics (TOG), 36(4):1–12, 2017.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Florin C Ghesu, Edward Krubasik, Bogdan Georgescu, Vivek Singh, Yefeng
    Zheng, Joachim Hornegger, and Dorin Comaniciu. Marginal space deep learning: efficient
    architecture for volumetric image parsing. IEEE transactions on medical imaging,
    35(5):1217–1228, 2016.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Florin-Cristian Ghesu, Bogdan Georgescu, Yefeng Zheng, Sasa Grbic, Andreas
    Maier, Joachim Hornegger, and Dorin Comaniciu. Multi-scale deep reinforcement
    learning for real-time 3d-landmark detection in ct scans. IEEE transactions on
    pattern analysis and machine intelligence, 41(1):176–189, 2017.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] M Giles. Mit technology review. Google researchers have reportedly achieved”
    quantum supremacy” URL: https:/www.technologyreview. com/f, 614416, 2017.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Justin Girard and M Reza Emami. Concurrent markov decision processes
    for robot team learning. Engineering applications of artificial intelligence,
    39:223–234, 2015.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference
    on computer vision, pages 1440–1448, 2015.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich
    feature hierarchies for accurate object detection and semantic segmentation. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 580–587, 2014.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Georgia Gkioxari, Ross Girshick, and Jitendra Malik. Contextual action
    recognition with r* cnn. In Proceedings of the IEEE international conference on
    computer vision, pages 1080–1088, 2015.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Vikash Goel, Jameson Weng, and Pascal Poupart. Unsupervised video object
    segmentation for deep reinforcement learning. In Advances in Neural Information
    Processing Systems, pages 5683–5694, 2018.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Abel Gonzalez-Garcia, Alexander Vezhnevets, and Vittorio Ferrari. An
    active search strategy for efficient object class detection. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 3022–3031,
    2015.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Advances in neural information processing systems, pages 2672–2680, 2014.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Leo Grady. Random walks for image segmentation. IEEE transactions on
    pattern analysis and machine intelligence, 28(11):1768–1783, 2006.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition
    with deep recurrent neural networks. CoRR, abs/1303.5778, 2013.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Albert Gubern-Mérida, Robert Martí, Jaime Melendez, Jakob L Hauth, Ritse M
    Mann, Nico Karssemeijer, and Bram Platel. Automated localization of breast cancer
    in dce-mri. Medical image analysis, 20(1):265–274, 2015.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
    Aaron C Courville. Improved training of wasserstein gans. In Advances in neural
    information processing systems, pages 5767–5777, 2017.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Minghao Guo, Jiwen Lu, and Jie Zhou. Dual-agent deep reinforcement learning
    for deformable face tracking. In Proceedings of the European Conference on Computer
    Vision (ECCV), pages 768–783, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey
    Levine. Meta-reinforcement learning of structured exploration strategies. In Advances
    in Neural Information Processing Systems, pages 5302–5311, 2018.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization
    and recognition of indoor scenes from rgb-d images. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 564–571, 2013.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Saurabh Gupta, Ross Girshick, Pablo Arbeláez, and Jitendra Malik. Learning
    rich features from rgb-d images for object detection and segmentation. In European
    conference on computer vision, pages 345–360. Springer, 2014.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool.
    Creating summaries from user videos. In European conference on computer vision,
    pages 505–520. Springer, 2014.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Seyed Hamid Rezatofighi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony
    Dick, and Ian Reid. Joint probabilistic data association revisited. In Proceedings
    of the IEEE international conference on computer vision, pages 3047–3055, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Junwei Han, Le Yang, Dingwen Zhang, Xiaojun Chang, and Xiaodan Liang.
    Reinforcement cutting-agent learning for video object segmentation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9080–9089,
    2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Robert M Haralick and Linda G Shapiro. Image segmentation techniques.
    Computer vision, graphics, and image processing, 29(1):100–132, 1985.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Sam Hare, Stuart Golodetz, Amir Saffari, Vibhav Vineet, Ming-Ming Cheng,
    Stephen L Hicks, and Philip HS Torr. Struck: Structured output tracking with kernels.
    IEEE transactions on pattern analysis and machine intelligence, 38(10):2096–2109,
    2015.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Jitendra Malik.
    Simultaneous detection and segmentation. In European Conference on Computer Vision,
    pages 297–312. Springer, 2014.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Jitendra Malik.
    Hypercolumns for object segmentation and fine-grained localization. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 447–456,
    2015.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Hannes Hase, Mohammad Farid Azampour, Maria Tirindelli, Magdalini Paschali,
    Walter Simson, Emad Fatemizadeh, and Nassir Navab. Ultrasound-guided robotic navigation
    with deep reinforcement learning. arXiv preprint arXiv:2003.13321, 2020.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Hado V Hasselt. Double q-learning. In Advances in neural information
    processing systems, pages 2613–2621, 2010.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Matthew J. Hausknecht and Peter Stone. Deep recurrent q-learning for
    partially observable mdps. CoRR, abs/1507.06527, 2015.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn.
    In Proceedings of the IEEE international conference on computer vision, pages
    2961–2969, 2017.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual
    learning for image recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 770–778, 2016.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] João F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed
    tracking with kernelized correlation filters. IEEE transactions on pattern analysis
    and machine intelligence, 37(3):583–596, 2014.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz
    de Cote. A survey of learning in multiagent environments: Dealing with non-stationarity.
    CoRR, abs/1707.09183, 2017.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski,
    Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow:
    Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298,
    2017.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Todd Hester, Michael Quinlan, and Peter Stone. A real-time model-based
    reinforcement learning architecture for robot control. CoRR, abs/1105.1749, 2011.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks
    principle: Reading children’s books with explicit memory representations. CoRR,
    abs/1511.02301, 2015.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recovering surface layout
    from an image. International Journal of Computer Vision, 75(1):151–172, 2007.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] James B. Holliday and Ngan T.H. Le. Follow then forage exploration: Improving
    asynchronous advantage actor critic. International Conference on Soft Computing,
    Artificial Intelligence and Applications (SAI 2020), pages 107–118, 2020.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang
    Chang, Hsuan-Kung Yang, Brian Hsi-Lin Ho, Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching
    Hsiao, et al. Virtual-to-real: Learning to control in visual semantic segmentation.
    arXiv preprint arXiv:1802.00285, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Ju Hong Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang, and Kuk-Jin Yoon. Online
    multi-object tracking via structural constraint event aggregation. In Proceedings
    of the IEEE Conference on computer vision and pattern recognition, pages 1392–1400,
    2016.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In 2018
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7132–7141,
    2018.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. Jointly
    learning heterogeneous features for rgb-d activity recognition. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 5344–5352,
    2015.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Weiming Hu, Xi Li, Wenhan Luo, Xiaoqin Zhang, Stephen Maybank, and Zhongfei
    Zhang. Single and multiple object tracking using log-euclidean riemannian subspace
    and block-division appearance model. IEEE transactions on pattern analysis and
    machine intelligence, 34(12):2420–2440, 2012.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
    Densely connected convolutional networks. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 4700–4708, 2017.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity
    benchmark for generic object tracking in the wild. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 2019.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Zhewei Huang, Wen Heng, and Shuchang Zhou. Learning to paint with model-based
    deep reinforcement learning. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 8709–8718, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for
    sequence tagging. arXiv preprint arXiv:1508.01991, 2015.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning
    on lie groups for skeleton-based action recognition. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 6099–6108, 2017.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Gabriel Efrain Humpire-Mamani, Arnaud Arindra Adiyoso Setio, Bram van
    Ginneken, and Colin Jacobs. Efficient organ localization using multi-label convolutional
    neural networks in thorax-abdomen ct scans. Physics in Medicine & Biology, 63(8):085003,
    2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Luis Ibanez, Will Schroeder, Lydia Ng, and Josh Cates. The itk software
    guide: updated for itk version 2.4, 2005.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. Multi-source
    multi-scale counting in extremely dense crowd images. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 2547–2554, 2013.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed,
    Nasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map estimation
    and localization in dense crowds. In Proceedings of the European Conference on
    Computer Vision (ECCV), pages 532–546, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy,
    and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2462–2470, 2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167,
    2015.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Clifford R Jack Jr, Matt A Bernstein, Nick C Fox, Paul Thompson, Gene
    Alexander, Danielle Harvey, Bret Borowski, Paula J Britson, Jennifer L. Whitwell,
    Chadwick Ward, et al. The alzheimer’s disease neuroimaging initiative (adni):
    Mri methods. Journal of Magnetic Resonance Imaging: An Official Journal of the
    International Society for Magnetic Resonance in Medicine, 27(4):685–691, 2008.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer
    networks. In Advances in neural information processing systems, pages 2017–2025,
    2015.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Deep features for
    text spotting. In European conference on computer vision, pages 512–528. Springer,
    2014.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Arjit Jain, Alexander Powers, and Hans J Johnson. Robust automatic multiple
    landmark detection. In 2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI), pages 1178–1182\. IEEE, 2020.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Suyog Dutt Jain and Kristen Grauman. Supervoxel-consistent foreground
    propagation in video. In European conference on computer vision, pages 656–671.
    Springer, 2014.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusionseg: Learning to
    combine motion and appearance for fully automatic segmentation of generic objects
    in videos. In 2017 IEEE conference on computer vision and pattern recognition
    (CVPR), pages 2117–2126\. IEEE, 2017.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Varun Jampani, Raghudeep Gadde, and Peter V Gehler. Video propagation
    networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 451–461, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Won-Dong Jang and Chang-Su Kim. Online video object segmentation via
    convolutional trident network. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 5849–5858, 2017.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Simon Jégou, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua
    Bengio. The one hundred layers tiramisu: Fully convolutional densenets for semantic
    segmentation. In Proceedings of the IEEE conference on computer vision and pattern
    recognition workshops, pages 11–19, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang. Model-based reinforcement
    learning with value-targeted regression. In Proceedings of the 2nd Conference
    on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning
    Research, pages 666–686, The Cloud, 10–11 Jun 2020.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Ming-xin Jiang, Chao Deng, Zhi-geng Pan, Lan-fang Wang, and Xing Sun.
    Multiobject tracking in videos based on lstm and deep reinforcement learning.
    Complexity, 2018, 2018.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Mingxin Jiang, Tao Hai, Zhigeng Pan, Haiyan Wang, Yinjie Jia, and Chao
    Deng. Multi-agent deep reinforcement learning for multi-object tracker. IEEE Access,
    7:32400–32407, 2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Lu, and Shuicheng
    Yan. Tree-structured reinforcement learning for sequential object localization.
    In Advances in Neural Information Processing Systems, pages 127–135, 2016.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Oscar Jimenez-del Toro, Henning Müller, Markus Krenn, Katharina Gruenberg,
    Abdel Aziz Taha, Marianne Winterstein, Ivan Eggel, Antonio Foncubierta-Rodríguez,
    Orcun Goksel, András Jakab, et al. Cloud-based evaluation of anatomical structure
    segmentation and landmark detection algorithms: Visceral anatomy benchmarks. IEEE
    transactions on medical imaging, 35(11):2459–2475, 2016.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] V Craig Jordan. Long-term adjuvant tamoxifen therapy for breast cancer.
    Breast cancer research and treatment, 15(3):125–136, 1990.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Yeong Jun Koh and Chang-Su Kim. Primary object segmentation in videos
    based on region augmentation and reduction. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 3442–3450, 2017.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. Tracking-learning-detection.
    IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422,
    2011.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models.
    Association for Computational Linguistics, October 2013.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech
    Jaśkowski. Vizdoom: A doom-based ai research platform for visual reinforcement
    learning. In 2016 IEEE Conference on Computational Intelligence and Games (CIG),
    pages 1–8\. IEEE, 2016.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Du Yong Kim and Moongu Jeon. Data fusion of radar and image measurements
    for multi-object tracking via kalman filtering. Information Sciences, 278:641–652,
    2014.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Kye Kyung Kim, Soo Hyun Cho, Hae Jin Kim, and Jae Yeon Lee. Detecting
    and tracking moving object using an active camera. In The 7th International Conference
    on Advanced Communication Technology, 2005, ICACT 2005., volume 2, pages 817–820\.
    IEEE, 2005.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Donna Kirwan. Nhs fetal anomaly screening programme. National Standards
    and Guidance for England, 18(0), 2010.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Stefan Klein, Marius Staring, Keelin Murphy, Max A Viergever, and Josien PW
    Pluim. Elastix: a toolbox for intensity-based medical image registration. IEEE
    transactions on medical imaging, 29(1):196–205, 2009.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning
    in robotics: A survey. The International Journal of Robotics Research, 32(11):1238–1274,
    2013.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances
    in neural information processing systems, pages 1008–1014, 2000.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected
    crfs with gaussian edge potentials. In Advances in neural information processing
    systems, pages 109–117, 2011.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Julian Krebs, Tommaso Mansi, Hervé Delingette, Li Zhang, Florin C Ghesu,
    Shun Miao, Andreas K Maier, Nicholas Ayache, Rui Liao, and Ali Kamen. Robust non-rigid
    registration through agent-based action learning. In International Conference
    on Medical Image Computing and Computer-Assisted Intervention, pages 344–352\.
    Springer, 2017.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder,
    Luka Cehovin Zajc, Tomas Vojir, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey,
    et al. The sixth visual object tracking vot2018 challenge results. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 0–0, 2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin,
    Gustavo Fernandez, Tomas Vojir, Gustav Hager, Georg Nebehay, and Roman Pflugfelder.
    The visual object tracking vot2015 challenge results. In Proceedings of the IEEE
    international conference on computer vision workshops, pages 1–23, 2015.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems, pages 1097–1105, 2012.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. Communications of the ACM, 60(6):84–90,
    2017.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A. Kupcsik, M. Deisenroth, Jan Peters, and G. Neumann. Data-efficient
    generalization of robot skills with contextual policy search. In AAAI, 2013.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel.
    Model-ensemble trust-region policy optimization. 02 2018.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Ngan Le, Trung Le, Kashu Yamazaki, Toan Duc Bui, Khoa Luu, and Marios
    Savides. Offset curves loss for imbalanced problem in medical segmentation. arXiv
    preprint arXiv:2012.02463, 2020.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Ngan Le, Kashu Yamazaki, Dat Truong, Kha Gia Quach, and Marios Savvides.
    A multi-task contextual atrous residual network for brain tumor detection & segmentation.
    arXiv preprint arXiv:2012.02073, 2020.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] T Hoang Ngan Le, Chi Nhan Duong, Ligong Han, Khoa Luu, Kha Gia Quach,
    and Marios Savvides. Deep contextual recurrent residual networks for scene labeling.
    Pattern Recognition, 80:32–41, 2018.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] T Hoang Ngan Le, Kha Gia Quach, Khoa Luu, Chi Nhan Duong, and Marios
    Savvides. Reformulating level sets as deep recurrent neural network approach to
    semantic segmentation. IEEE Transactions on Image Processing, 27(5):2393–2407,
    2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D
    Hager. Temporal convolutional networks for action segmentation and detection.
    In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 156–165, 2017.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Colin Lea, Austin Reiter, René Vidal, and Gregory D Hager. Segmental
    spatiotemporal cnns for fine-grained action segmentation. In European Conference
    on Computer Vision, pages 36–52. Springer, 2016.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Colin Lea, René Vidal, and Gregory D Hager. Learning convolutional action
    primitives for fine-grained action recognition. In 2016 IEEE international conference
    on robotics and automation (ICRA), pages 1642–1649\. IEEE, 2016.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional
    networks: A unified approach to action segmentation. In European Conference on
    Computer Vision, pages 47–54. Springer, 2016.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Laura Leal-Taixé, Cristian Canton-Ferrer, and Konrad Schindler. Learning
    by tracking: Siamese cnn for robust target association. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 33–40,
    2016.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Laura Leal-Taixé, Michele Fenzi, Alina Kuznetsova, Bodo Rosenhahn, and
    Silvio Savarese. Learning an image-based motion context for multiple people tracking.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 3542–3549, 2014.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Laura Leal-Taixé, Anton Milan, Ian Reid, Stefan Roth, and Konrad Schindler.
    Motchallenge 2015: Towards a benchmark for multi-target tracking. arXiv preprint
    arXiv:1504.01942, 2015.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
    com/exdb/mnist/, 1998.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Yann LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efficient
    backprop. In Neural networks: Tricks of the trade, pages 9–50\. Springer, 1998.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework
    for back-propagation. In Proceedings of the 1988 connectionist models summer school,
    pages 21–28\. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Hyunjae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm: A style-based recalibration
    module for convolutional neural networks. pages 1854–1862, 10 2019.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Jae Won Lee, Jonghun Park, Jangmin O, Jongwoo Lee, and Euyseok Hong.
    A multiagent approach to q-learning for daily stock trading. Trans. Sys. Man Cyber.
    Part A, 37(6):864–877, November 2007.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Joel Z. Leibo, Vinícius Flores Zambaldi, Marc Lanctot, Janusz Marecki,
    and Thore Graepel. Multi-agent reinforcement learning in sequential social dilemmas.
    CoRR, abs/1702.03037, 2017.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Sergey Levine and Vladlen Koltun. Learning complex neural network policies
    with trajectory optimization. In Proceedings of the 31st International Conference
    on Machine Learning, pages 829–837, 2014.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance
    visual tracking with siamese region proposal network. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 8971–8980, 2018.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiaogang Wang. GS3D:
    an efficient 3d object detection framework for autonomous driving. In IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
    16-20, 2019, pages 1019–1028. Computer Vision Foundation / IEEE, 2019.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu. Co-occurrence feature
    learning from skeleton data for action recognition and detection with hierarchical
    aggregation. arXiv preprint arXiv:1804.06055, 2018.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Duo Li and Qifeng Chen. Deep reinforced attention learning for quality-aware
    visual recognition. In European Conference on Computer Vision, pages 493–509,
    2020.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features.
    arXiv preprint arXiv:1503.08663, 2015.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder
    for paragraphs and documents. CoRR, abs/1506.01057, 2015.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] K. Li, M. Rath, and J. W. Burdick. Inverse reinforcement learning via
    function approximation for clinical motion analysis. In 2018 IEEE International
    Conference on Robotics and Automation (ICRA), pages 610–617, 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit
    approach to personalized news article recommendation. In Proceedings of the 19th
    international conference on World wide web, pages 661–670, 2010.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep visual tracking:
    Review and experimental comparison. Pattern Recognition, 76:323–338, 2018.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Yingbo Li and Bernard Merialdo. Multi-video summarization based on video-mmr.
    In 11th International Workshop on Image Analysis for Multimedia Interactive Services
    WIAMIS 10, pages 1–4\. IEEE, 2010.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Yuanwei Li, Amir Alansary, Juan J Cerrolaza, Bishesh Khanal, Matthew
    Sinclair, Jacqueline Matthew, Chandni Gupta, Caroline Knight, Bernhard Kainz,
    and Daniel Rueckert. Fast multiple landmark localisation using a patch-based iterative
    network. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 563–571\. Springer, 2018.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding color information
    for visual tracking: Algorithms and benchmark. IEEE Transactions on Image Processing,
    24(12):5630–5644, 2015.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Rui Liao, Shun Miao, Pierre de Tournemire, Sasa Grbic, Ali Kamen, Tommaso
    Mansi, and Dorin Comaniciu. An artificial agent for robust image registration.
    In Thirty-First AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang,
    Yanfeng Wang, and Ya Zhang. Iteratively-refined interactive 3d medical image segmentation
    with multi-agent reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 9394–9402, 2020.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Timothy P. Lillicrap, Jonathan J. Hunt, Alexand er Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv e-prints, page arXiv:1509.02971, September
    2015.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
    Focal loss for dense object detection. In Proceedings of the IEEE international
    conference on computer vision, pages 2980–2988, 2017.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Tony Lindeberg. Scale-space theory in computer vision, volume 256. Springer
    Science & Business Media, 2013.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Geert Litjens, Robert Toth, Wendy van de Ven, Caroline Hoeks, Sjoerd
    Kerkstra, Bram van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang
    Zhang, et al. Evaluation of prostate segmentation algorithms for mri: the promise12
    challenge. Medical image analysis, 18(2):359–373, 2014.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Daochang Liu and Tingting Jiang. Deep reinforcement learning for surgical
    gesture segmentation and classification. In International conference on medical
    image computing and computer-assisted intervention, pages 247–255\. Springer,
    2018.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Hao Liu, Richard Socher, and Caiming Xiong. Taming maml: Efficient unbiased
    meta-reinforcement learning. In International Conference on Machine Learning,
    pages 4061–4071, 2019.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua
    Shen. Weighing counts: Sequential crowd counting by reinforcement learning. 2020.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Lijie Liu, Chufan Wu, Jiwen Lu, Lingxi Xie, Jie Zhou, and Qi Tian. Reinforced
    axial refinement network for monocular 3d object detection. In European Conference
    on Computer Vision ECCV, pages 540–556, 2020.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and Liang Lin. Crowd
    counting using deep recurrent spatial-aware network. arXiv preprint arXiv:1807.00601,
    2018.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Tianrui Liu, Qingjie Meng, Athanasios Vlontzos, Jeremy Tan, Daniel Rueckert,
    and Bernhard Kainz. Ultrasound video summarization using deep reinforcement learning.
    arXiv preprint arXiv:2005.09531, 2020.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5099–5108, 2019.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face
    attributes in the wild. In Proceedings of the IEEE international conference on
    computer vision, pages 3730–3738, 2015.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Marco Lorenzi, Nicholas Ayache, Giovanni B Frisoni, Xavier Pennec, Alzheimer’s
    Disease Neuroimaging Initiative (ADNI, et al. Lcc-demons: a robust and accurate
    symmetric diffeomorphic registration algorithm. NeuroImage, 81:470–483, 2013.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Tayebeh Lotfi, Lisa Tang, Shawn Andrews, and Ghassan Hamarneh. Improving
    probabilistic image registration via reinforcement learning and uncertainty evaluation.
    In International Workshop on Machine Learning in Medical Imaging, pages 187–194\.
    Springer, 2013.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] David G Lowe. Distinctive image features from scale-invariant keypoints.
    International journal of computer vision, 60(2):91–110, 2004.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, and Yizhou
    Wang. End-to-end active object tracking via reinforcement learning. arXiv preprint
    arXiv:1705.10561, 2017.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech
    Zaremba. Addressing the rare word problem in neural machine translation. CoRR,
    abs/1410.8206, 2014.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Khoa Luu, Chenchen Zhu, Chandrasekhar Bhagavatula, T Hoang Ngan Le, and
    Marios Savvides. A deep learning approach to joint face detection and segmentation.
    In Advances in Face Detection and Facial Image Analysis, pages 1–12\. Springer,
    2016.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang. Hierarchical
    convolutional features for visual tracking. In Proceedings of the IEEE international
    conference on computer vision, pages 3074–3082, 2015.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Kai Ma, Jiangping Wang, Vivek Singh, Birgi Tamersoy, Yao-Jen Chang, Andreas
    Wimmer, and Terrence Chen. Multimodal image registration with deep context reinforcement
    learning. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 240–248\. Springer, 2017.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video
    summarization with adversarial lstm networks. In Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition, pages 202–211, 2017.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] Gabriel Maicas, Gustavo Carneiro, Andrew P Bradley, Jacinto C Nascimento,
    and Ian Reid. Deep reinforcement learning for active breast lesion detection from
    dce-mri. In International conference on medical image computing and computer-assisted
    intervention, pages 665–673\. Springer, 2017.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille. Deep captioning
    with multimodal recurrent neural networks (m-rnn). CoRR, abs/1412.6632, 2014.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Nicolas Märki, Federico Perazzi, Oliver Wang, and Alexander Sorkine-Hornung.
    Bilateral space video segmentation. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 743–751, 2016.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] T. Martinez-Marin and T. Duckett. Fast reinforcement learning for vision-guided
    mobile robots. In Proceedings of the 2005 IEEE International Conference on Robotics
    and Automation, pages 4170–4175, 2005.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforcement
    learning for deformable object manipulation. arXiv preprint arXiv:1806.07851,
    2018.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] Stefan Mathe, Aleksis Pirinen, and Cristian Sminchisescu. Reinforcement
    learning for visual object detection. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 2894–2902, 2016.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] George K Matsopoulos, Nicolaos A Mouravliansky, Konstantinos K Delibasis,
    and Konstantina S Nikita. Automatic retinal image registration scheme using global
    optimization techniques. IEEE Transactions on Information Technology in Biomedicine,
    3(1):47–60, 1999.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Darryl McClymont, Andrew Mehnert, Adnan Trakic, Dominic Kennedy, and
    Stuart Crozier. Fully automatic lesion segmentation in breast mri using mean-shift
    and graph-cuts on a region adjacency graph. Journal of Magnetic Resonance Imaging,
    39(4):795–804, 2014.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer,
    Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom,
    Roland Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats).
    IEEE transactions on medical imaging, 34(10):1993–2024, 2014.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Shun Miao, Rui Liao, Marcus Pfister, Li Zhang, and Vincent Ordy. System
    and method for 3-d/3-d registration between non-contrast-enhanced cbct and contrast-enhanced
    ct for abdominal aortic aneurysm stenting. In International Conference on Medical
    Image Computing and Computer-Assisted Intervention, pages 380–387\. Springer,
    2013.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Shun Miao, Z Jane Wang, and Rui Liao. A cnn regression approach for real-time
    2d/3d registration. IEEE transactions on medical imaging, 35(5):1352–1363, 2016.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Tomas Mikolov, Stefan Kombrink, Lukás Burget, Jan Cernocký, and Sanjeev
    Khudanpur. Extensions of recurrent neural network language model. In ICASSP, pages
    5528–5531, 2011.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Anton Milan, Laura Leal-Taixé, Ian Reid, Stefan Roth, and Konrad Schindler.
    Mot16: A benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831,
    2016.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Anton Milan, Laura Leal-Taixé, Konrad Schindler, and Ian Reid. Joint
    tracking and segmentation of multiple targets. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 5397–5406, 2015.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] Anton Milan, S Hamid Rezatofighi, Anthony Dick, Ian Reid, and Konrad
    Schindler. Online multi-target tracking using recurrent neural networks. In Thirty-First
    AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Shervin Minaee, AmirAli Abdolrashidi, Hang Su, Mohammed Bennamoun, and
    David Zhang. Biometric recognition using deep learning: A survey. CoRR, abs/1912.00271,
    2019.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937, 2016.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In Proceedings of The 33rd International Conference
    on Machine Learning, pages 1928–1937, 20–22 Jun 2016.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel
    Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
    Georg Ostrovski, et al. Human-level control through deep reinforcement learning.
    Nature, 518(7540):529–533, 2015.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] I. Mordatch, N. Mishra, C. Eppner, and P. Abbeel. Combining model-based
    policy search with online model learning for control of physical humanoids. In
    2016 IEEE International Conference on Robotics and Automation (ICRA), pages 242–248,
    2016.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] J. Morimoto, G. Zeglin, and C. G. Atkeson. Minimax differential dynamic
    programming: application to a biped walking robot. In Proceedings 2003 IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),
    volume 2, pages 1927–1932, 2003.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Jun Morimoto and Christopher G. Atkeson. Nonparametric representation
    of an approximated poincaré map for learning biped locomotion. In Autonomous Robots,
    page 131–144, 2009.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] A. Mousavian, D. Anguelov, J. Flynn, and J. Košecká. 3d bounding box
    estimation using deep learning and geometry. In 2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 5632–5640, 2017.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator
    for uav tracking. In European conference on computer vision, pages 445–461. Springer,
    2016.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Don Murray and Anup Basu. Motion tracking with an active camera. IEEE
    transactions on pattern analysis and machine intelligence, 16(5):449–459, 1994.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter
    Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world
    environments through meta-reinforcement learning. arXiv preprint arXiv:1803.11347,
    2018.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming
    exploration in reinforcement learning with demonstrations. In 2018 IEEE International
    Conference on Robotics and Automation (ICRA), pages 6292–6299, 2018.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural
    networks for visual tracking. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 4293–4302, 2016.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] Ali Bou Nassif, Ismail Shahin, Imtinan Attili, Mohammad Azzeh, and Khaled
    Shaalan. Speech recognition using deep neural networks: A systematic review. IEEE
    Access, 7:19143–19165, 2019.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, Jan C Peeken,
    Stephanie E Combs, and Bjoern H Menze. Deep reinforcement learning for organ localization
    in ct. arXiv preprint arXiv:2005.04974, 2020.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and
    Andrew Y Ng. Reading digits in natural images with unsupervised feature learning.
    2011.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] Dominik Neumann, Saša Grbić, Matthias John, Nassir Navab, Joachim Hornegger,
    and Razvan Ionasec. Probabilistic sparse matching for robust 3d/3d fusion in minimally
    invasive surgery. IEEE transactions on medical imaging, 34(1):49–60, 2014.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement
    learning. In Proceedings of the Seventeenth International Conference on Machine
    Learning, ICML ’00, page 663–670, San Francisco, CA, USA, 2000. Morgan Kaufmann
    Publishers Inc.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement
    learning. In Icml, volume 1, page 2, 2000.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] Trung Thanh Nguyen, Zhuoru Li, Tomi Silander, and Tze-Yun Leong. Online
    feature selection for model-based reinforcement learning. In Proceedings of the
    30th International Conference on International Conference on Machine Learning
    - Volume 28, page I–498–I–506, 2013.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, Ngan Le, and Marios Savvides.
    Temporal non-volume preserving approach to facial age-progression and age-invariant
    face recognition. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 3735–3743, 2017.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] C. Niedzwiedz, I. Elhanany, Zhenzhen Liu, and S. Livingston. A consolidated
    actor-critic model with function approximation for high-dimensional pomdps. In
    AAAI 2008Workshop for Advancement in POMDP Solvers, 2008.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Yishuang Ning, Sheng He, Zhiyong Wu, Chunxiao Xing, and Liang-Jie Zhang.
    A review of deep learning based speech synthesis. Applied Sciences, 9(19), 2019.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Kenji Okuma, Ali Taleghani, Nando De Freitas, James J Little, and David G
    Lowe. A boosted particle filter: Multitarget detection and tracking. In European
    conference on computer vision, pages 28–39. Springer, 2004.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] José Ignacio Orlando, Huazhu Fu, João Barbosa Breda, Karel van Keer,
    Deepti R Bathula, Andrés Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim,
    JoonHo Lee, et al. Refuge challenge: A unified framework for evaluating automated
    methods for glaucoma assessment from fundus photographs. Medical image analysis,
    59:101570, 2020.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters.
    2018.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Rameswar Panda, Abir Das, Ziyan Wu, Jan Ernst, and Amit K Roy-Chowdhury.
    Weakly supervised summarization of web videos. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 3657–3666, 2017.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Anestis Papazoglou and Vittorio Ferrari. Fast object segmentation in
    unconstrained video. In Proceedings of the IEEE international conference on computer
    vision, pages 1777–1784, 2013.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] I. C. Paschalidis, K. Li, and R. Moazzez Estanjini. An actor-critic method
    using least squares temporal difference learning. In Proceedings of the 48h IEEE
    Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control
    Conference, pages 2564–2569, 2009.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Massimiliano Patacchiola and Angelo Cangelosi. Head pose estimation in
    the wild using convolutional neural networks and adaptive gradient methods. Pattern
    Recognition, 71:132–143, 2017.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Hanchuan Peng, Zongcai Ruan, Fuhui Long, Julie H Simpson, and Eugene W
    Myers. V3d enables real-time 3d visualization and quantitative analysis of large-scale
    biological image data sets. Nature biotechnology, 28(4):348–353, 2010.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and
    Alexander Sorkine-Hornung. Learning video object segmentation from static images.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2663–2672, 2017.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus
    Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology
    for video object segmentation. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 724–732, 2016.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills
    with policy gradients. Neural Networks, 21(4):682 – 697, 2008.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] Aleksis Pirinen and Cristian Sminchisescu. Deep reinforcement learning
    of region proposal networks for object detection. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 6945–6954, 2018.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Hamed Pirsiavash, Deva Ramanan, and Charless C Fowlkes. Globally-optimal
    greedy algorithms for tracking a variable number of objects. In CVPR 2011, pages
    1201–1208\. IEEE, 2011.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] Aske Plaat, Walter Kosters, and Mike Preuss. Deep model-based reinforcement
    learning for high-dimensional problems, a survey, 2020.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid.
    Category-specific video summarization. In European conference on computer vision,
    pages 540–555. Springer, 2014.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] Reza Pourreza-Shahri and Nasser Kehtarnavaz. Exposure bracketing via
    automatic exposure selection. In 2015 IEEE International Conference on Image Processing
    (ICIP), pages 320–323\. IEEE, 2015.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid,
    and Vittorio Ferrari. Learning object class detectors from weakly annotated video.
    In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3282–3289\.
    IEEE, 2012.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao, Qingming Huang, Jongwoo
    Lim, and Ming-Hsuan Yang. Hedged deep tracking. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 4303–4311, 2016.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] Zengyi Qin, Jinglu Wang, and Yan Lu. Monogrnet: A geometric reasoning
    network for monocular 3d object localization. Proceedings of the AAAI Conference
    on Artificial Intelligence, 33(01):8851–8858, Jul. 2019.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.
    Efficient off-policy meta-reinforcement learning via probabilistic context variables.
    In International conference on machine learning, pages 5331–5340, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] Vidhiwar Singh Rathour, Kashu Yamakazi, and T Le. Roughness index and
    roughness distance for benchmarking medical segmentation. arXiv preprint arXiv:2103.12350,
    2021.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only
    look once: Unified, real-time object detection. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 779–788, 2016.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian, and Jie Zhou. Collaborative
    deep reinforcement learning for multi-object tracking. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 586–602, 2018.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] Liangliang Ren, Xin Yuan, Jiwen Lu, Ming Yang, and Jie Zhou. Deep reinforcement
    learning with iterative shift for visual tracking. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 684–700, 2018.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. In Advances
    in neural information processing systems, pages 91–99, 2015.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] Md Reza, Jana Kosecka, et al. Reinforcement learning for semantic segmentation
    in indoor scenes. arXiv preprint arXiv:1606.01178, 2016.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Alexander Richard and Juergen Gall. Temporal action detection using a
    statistical language model. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 3131–3140, 2016.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] Mrigank Rochan, Linwei Ye, and Yang Wang. Video summarization using fully
    convolutional sequence networks. In Proceedings of the European Conference on
    Computer Vision (ECCV), pages 347–363, 2018.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional
    networks for biomedical image segmentation. In International Conference on Medical
    image computing and computer-assisted intervention, pages 234–241\. Springer,
    2015.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] German Ros, Vladfen Koltun, Felipe Codevilla, and Antonio Lopez. The
    carla autonomous driving challenge, 2019.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. ” grabcut” interactive
    foreground extraction using iterated graph cuts. ACM transactions on graphics
    (TOG), 23(3):309–314, 2004.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] David Rotman. Mit technology review. Retrieved from Meet the Man with
    a Cheap and Easy Plan to Stop Global Warming: http://www. technologyreview. com/featuredstor
    y/511016/a-cheap-and-easy-plan-to-stop-globalwarming, 2013.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] J-M Rouet, J-J Jacq, and Christian Roux. Genetic algorithms for a robust
    3-d mr-ct registration. IEEE transactions on information technology in biomedicine,
    4(2):126–136, 2000.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] David E Rumelhart. The architecture of mind: A connectionist approach.
    Mind readings, pages 207–238, 1998.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] T. P. Runarsson and S. M. Lucas. Imitating play from game trajectories:
    Temporal difference learning versus preference learning. In 2012 IEEE Conference
    on Computational Intelligence and Games (CIG), pages 79–82, 2012.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
    Imagenet large scale visual recognition challenge. International Journal of Computer
    Vision, 115(3):211–252, 2015.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Tracking the untrackable:
    Learning to track multiple cues with long-term dependencies. In Proceedings of
    the IEEE International Conference on Computer Vision, pages 300–311, 2017.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] Steindór Sæmundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta reinforcement
    learning with latent variable gaussian processes. arXiv preprint arXiv:1803.07551,
    2018.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] Farhang Sahba. Deep reinforcement learning for object segmentation in
    video sequences. In 2016 International Conference on Computational Science and
    Computational Intelligence (CSCI), pages 857–860\. IEEE, 2016.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] Farhang Sahba, Hamid R Tizhoosh, and Magdy MA Salama. A reinforcement
    learning framework for medical image segmentation. In The 2006 IEEE International
    Joint Conference on Neural Network Proceedings, pages 511–517\. IEEE, 2006.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] Farhang Sahba, Hamid R Tizhoosh, and Magdy MMA Salama. Application of
    opposition-based reinforcement learning in image segmentation. In 2007 IEEE Symposium
    on Computational Intelligence in Image and Signal Processing, pages 246–251\.
    IEEE, 2007.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
    Moritz. Trust region policy optimization. In International conference on machine
    learning, pages 1889–1897, 2015.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and
    Pieter Abbeel. Trust Region Policy Optimization. arXiv e-prints, February 2015.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal Policy Optimization Algorithms. arXiv e-prints, July 2017.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning.
    Neural Networks, 16(1):5–9, 2003.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] Shahin Sefati, Noah J Cowan, and René Vidal. Learning shared, discriminative
    dictionaries for surgical gesture segmentation and classification. In MICCAI Workshop:
    M2CAI, volume 4, 2015.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] Mohammad Javad Shafiee, Brendan Chywl, Francis Li, and Alexander Wong.
    Fast yolo: A fast you only look once system for real-time embedded object detection
    in video. arXiv preprint arXiv:1709.05943, 2017.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: A
    large scale dataset for 3d human activity analysis. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 1010–1019, 2016.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] M. R. Shaker, Shigang Yue, and T. Duckett. Vision-based reinforcement
    learning using approximate policy iteration. In 2009 International Conference
    on Advanced Robotics, pages 1–6, 2009.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent,
    reinforcement learning for autonomous driving. CoRR, abs/1610.03295, 2016.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] Jie Shen, Stefanos Zafeiriou, Grigoris G Chrysos, Jean Kossaifi, Georgios
    Tzimiropoulos, and Maja Pantic. The first facial landmark tracking in-the-wild
    challenge: Benchmark and results. In Proceedings of the IEEE international conference
    on computer vision workshops, pages 50–58, 2015.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] Y. Shi, L. Cui, Z. Qi, F. Meng, and Z. Chen. Automatic road crack detection
    using random structured forests. IEEE Transactions on Intelligent Transportation
    Systems, 17(12):3434–3445, 2016.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor
    segmentation and support inference from rgbd images. In European conference on
    computer vision, pages 746–760. Springer, 2012.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
    and Martin Riedmiller. Deterministic policy gradient algorithms. 2014.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks
    for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] Vishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom
    feature fusion for crowd counting. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1002–1012, 2019.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan,
    and Balázs Gulyás. 3d deep learning on medical images: A review, 2020.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] Gwangmo Song, Heesoo Myeong, and Kyoung Mu Lee. Seednet: Automatic seed
    generation with deep reinforcement learning for robust interactive segmentation.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 1760–1768, 2018.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum:
    Summarizing web videos using titles. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 5179–5187, 2015.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson WH Lau, and Ming-Hsuan
    Yang. Crest: Convolutional residual learning for visual tracking. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 2555–2564, 2017.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] Concetto Spampinato, Simone Palazzo, and Daniela Giordano. Gamifying
    video object segmentation. IEEE transactions on pattern analysis and machine intelligence,
    39(10):1942–1958, 2016.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation
    learning. CoRR, abs/1703.01703, 2017.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary
    mean-field games. page 251–259\. International Foundation for Autonomous Agents
    and Multiagent Systems, 2019.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] Shanhui Sun, Jing Hu, Mingqing Yao, Jinrong Hu, Xiaodong Yang, Qi Song,
    and Xi Wu. Robust multimodal image registration using deep recurrent reinforcement
    learning. In Asian Conference on Computer Vision, pages 511–526. Springer, 2018.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] Kalaivani Sundararajan and Damon L. Woodard. Deep learning for biometrics:
    A survey. 51(3), 2018.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
    Policy gradient methods for reinforcement learning with function approximation.
    In Proceedings of the 12th International Conference on Neural Information Processing
    Systems, NIPS’99, page 1057–1063, 1999.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay
    Mansour. Policy gradient methods for reinforcement learning with function approximation.
    In Advances in Neural Information Processing Systems 12, pages 1057–1063\. 2000.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In Thirty-first AAAI conference on artificial intelligence, 2017.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1–9, 2015.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks
    for object detection. In Advances in neural information processing systems, pages
    2553–2561, 2013.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep progressive
    reinforcement learning for skeleton-based action recognition. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 5323–5332,
    2018.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders. Siamese instance
    search for tracking. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1420–1429, 2016.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] Philippe Thévenaz and Michael Unser. Optimization of mutual information
    for multiresolution image registration. IEEE transactions on image processing,
    9(12):2083–2099, 2000.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] Zhiqiang Tian, Xiangyu Si, Yaoyue Zheng, Zhang Chen, and Xiaojian Li.
    Multi-step medical image segmentation based on reinforcement learning. JOURNAL
    OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING, 2020.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free
    reinforcement learning for urban driving using implicit affordances. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7153–7162,
    2020.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation
    via deep neural networks. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1653–1660, 2014.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J Black. Video segmentation
    via object flow. In Proceedings of the IEEE conference on computer vision and
    pattern recognition, pages 3899–3908, 2016.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] Y. Tsurumine, Y. Cui, K. Yamazaki, and T. Matsubara. Generative adversarial
    imitation learning with deep p-network for robotic cloth manipulation. In 2019
    IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids), pages 274–280,
    2019.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] Yoshihisa Tsurumine, Yunduan Cui, Eiji Uchibe, and Takamitsu Matsubara.
    Deep reinforcement learning with smooth policy update: Application to robotic
    cloth manipulation. Robotics and Autonomous Systems, 112:72 – 83, 2019.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM
    Smeulders. Selective search for object recognition. International journal of computer
    vision, 104(2):154–171, 2013.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] Burak Uzkent, Christopher Yeh, and Stefano Ermon. Efficient object detection
    in large images using deep reinforcement learning. In The IEEE Winter Conference
    on Applications of Computer Vision, pages 1824–1833, 2020.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, and Philip HS
    Torr. End-to-end representation learning for correlation filter based tracking.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 2805–2813, 2017.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] Peter van Beek. Improved image selection for stack-based hdr imaging.
    arXiv preprint arXiv:1806.07420, 2018.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning
    with Double Q-learning. arXiv e-prints, page arXiv:1509.06461, September 2015.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning
    with double q-learning. In Thirtieth AAAI conference on artificial intelligence,
    2016.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] Leo Van Hove. Optimal denominations for coins and bank notes: in defense
    of the principle of least effort. Journal of Money, Credit and Banking, pages
    1015–1021, 2001.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] Giuseppe Vecchio, Simone Palazzo, Daniela Giordano, Francesco Rundo,
    and Concetto Spampinato. Mask-rl: Multiagent video object segmentation framework
    through reinforcement learning. IEEE Transactions on Neural Networks and Learning
    Systems, 2020.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] Kashu Yamakazi Akihiro Sugimoto Viet-Khoa Vo-Ho, Ngan T.H. Le and Triet
    Tran. Agent-environment network for temporal action proposal generation. In International
    Conference on Acoustics, Speech and Signal Processing. 2021.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar,
    and Katerina Fragkiadaki. Sfm-net: Learning of structure and motion from video.
    arXiv preprint arXiv:1704.07804, 2017.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max
    Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard
    Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk
    Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets,
    James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang,
    Tobias Pfaff, Toby Pohlen, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver
    Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis,
    and David Silver. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II.
    [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/),
    2019.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] Athanasios Vlontzos, Amir Alansary, Konstantinos Kamnitsas, Daniel Rueckert,
    and Bernhard Kainz. Multiple landmark detection using multi-agent reinforcement
    learning. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 262–270\. Springer, 2019.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] Guotai Wang, Maria A Zuluaga, Wenqi Li, Rosalind Pratt, Premal A Patel,
    Michael Aertsen, Tom Doel, Anna L David, Jan Deprest, Sébastien Ourselin, et al.
    Deepigeos: a deep interactive geodesic framework for medical image segmentation.
    IEEE transactions on pattern analysis and machine intelligence, 41(7):1559–1572,
    2018.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou,
    Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5265–5274, 2018.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z.
    Leibo, Rémi Munos, Charles Blundell, Dharshan Kumaran, and Matthew Botvinick.
    Learning to reinforcement learn. CoRR, abs/1611.05763, 2016.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] Lijun Wang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Deep networks
    for saliency detection via local estimation and global search. In Computer Vision
    and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3183–3192\. IEEE,
    2015.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] Mei Wang and Weihong Deng. Mitigating bias in face recognition using
    skewness-aware reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 9322–9331, 2020.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial
    faces in the wild: Reducing racial bias by information maximization adaptation
    network. In Proceedings of the IEEE International Conference on Computer Vision,
    pages 692–702, 2019.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] Naiyan Wang and Dit-Yan Yeung. Learning a deep compact image representation
    for visual tracking. In Advances in neural information processing systems, pages
    809–817, 2013.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric
    Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking
    model-based reinforcement learning. CoRR, abs/1907.02057, 2019.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] Yan Wang, Lei Zhang, Lituan Wang, and Zizhou Wang. Multitask learning
    for object localization with deep reinforcement learning. IEEE Transactions on
    Cognitive and Developmental Systems, 11(4):573–580, 2018.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, and Maja
    Pantic. Dynamic face video segmentation via reinforcement learning. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6959–6969,
    2020.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image
    quality assessment: from error visibility to structural similarity. IEEE transactions
    on image processing, 13(4):600–612, 2004.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] Zhouxia Wang, Jiawei Zhang, Mude Lin, Jiong Wang, Ping Luo, and Jimmy
    Ren. Learning a reinforced agent for flexible exposure bracketing selection. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1820–1828, 2020.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot,
    and Nando De Freitas. Dueling network architectures for deep reinforcement learning.
    arXiv preprint arXiv:1511.06581, 2015.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] Wayne A Wickelgren. The long and the short of memory. Psychological Bulletin,
    80(6):425, 1973.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] Ronald J Williams. Simple statistical gradient-following algorithms for
    connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] Aaron Wilson, Alan Fern, and Prasad Tadepalli. Using trajectory data
    to improve bayesian optimization for reinforcement learning. Journal of Machine
    Learning Research, 15(8):253–282, 2014.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] C. Wirth and J. Fürnkranz. On learning from game annotations. IEEE Transactions
    on Computational Intelligence and AI in Games, 7(3):304–316, 2015.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] Paul Wohlhart and Vincent Lepetit. Learning descriptors for object recognition
    and 3d pose estimation. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 3109–3118, 2015.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional
    block attention module. In European Conference on Computer Vision, pages 3–19,
    2018.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2411–2418, 2013.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 37(9):1834–1848, 2015.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] Lu Xia, Chia-Chih Chen, and Jake K Aggarwal. View invariant human action
    recognition using histograms of 3d joints. In 2012 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition Workshops, pages 20–27\. IEEE, 2012.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] Sitao Xiang and Hao Li. On the effects of batch and weight normalization
    in generative adversarial networks. arXiv preprint arXiv:1704.03971, 2017.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track: Online
    multi-object tracking by decision making. In Proceedings of the IEEE international
    conference on computer vision, pages 4705–4713, 2015.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] Fanyi Xiao and Yong Jae Lee. Track and segment: An iterative unsupervised
    approach for video object proposals. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 933–942, 2016.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] Hang Xiao and Hanchuan Peng. App2: automatic tracing of 3d neuron morphology
    based on hierarchical pruning of a gray-weighted image distance-tree. Bioinformatics,
    29(11):1448–1454, 2013.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training
    with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 10687–10698, 2020.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, and Chunhua
    Shen. From open set to closed set: Counting objects by spatial divide-and-conquer.
    In Proceedings of the IEEE International Conference on Computer Vision, pages
    8362–8371, 2019.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] Hailiang Xu and Feng Su. Robust seed localization and growing with deep
    convolutional features for scene text detection. In Proceedings of the 5th ACM
    on International Conference on Multimedia Retrieval, pages 387–394\. ACM, 2015.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang. Deep
    interactive object selection. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 373–381, 2016.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, and Josef Kittler. Learning
    adaptive discriminative correlation filters via temporal consistency preserving
    spatial feature selection for robust visual object tracking. IEEE Transactions
    on Image Processing, 28(11):5596–5609, 2019.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] Xuanang Xu, Fugen Zhou, Bo Liu, Dongshan Fu, and Xiangzhi Bai. Efficient
    multiple organ localization in ct image using 3d region proposal network. IEEE
    transactions on medical imaging, 38(8):1885–1898, 2019.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, and Chun-Yi Lee. Dynamic video
    segmentation network. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 6556–6565, 2018.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[411] Kashu Yamazaki, Vidhiwar Singh Rathour, and T Le. Invertible residual
    network with regularization for effective medical image segmentation. arXiv preprint
    arXiv:2103.09042, 2021.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[412] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei
    Wen, and Errui Ding. Perspective-guided convolution networks for crowd counting.
    In Proceedings of the IEEE International Conference on Computer Vision, pages
    952–961, 2019.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[413] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory
    Zelinsky, Dimitris Samaras, and Minh Hoai. Predicting goal-directed human attention
    using inverse reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), June 2020.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[414] Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare:
    a survey. arXiv preprint arXiv:1908.08796, 2019.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[415] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman,
    Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task
    and meta reinforcement learning. In Conference on Robot Learning, pages 1094–1100,
    2020.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[416] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, and Jin Young Choi.
    Action-decision networks for visual tracking with deep reinforcement learning.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2711–2720, 2017.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[417] Daochen Zha, Kwei-Herng Lai, Kaixiong Zhou, and Xia Hu. Experience replay
    optimization. arXiv preprint arXiv:1906.08387, 2019.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[418] Da Zhang, Hamid Maei, Xin Wang, and Yuan-Fang Wang. Deep reinforcement
    learning for visual object tracking in videos. arXiv preprint arXiv:1701.08936,
    2017.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[419] Dingwen Zhang, Le Yang, Deyu Meng, Dong Xu, and Junwei Han. Spftn: A
    self-paced fine-tuning network for segmenting objects in weakly labelled videos.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4429–4437, 2017.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[420] Jing Zhang, Wanqing Li, Philip O Ogunbona, Pichao Wang, and Chang Tang.
    Rgb-d-based action recognition datasets: A survey. Pattern Recognition, 60:86–105,
    2016.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[421] Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman. Video summarization
    with long short-term memory. In European conference on computer vision, pages
    766–782. Springer, 2016.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[422] Pengyu Zhang, Dong Wang, and Huchuan Lu. Multi-modal visual tracking:
    Review and experimental comparison, 2020.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[423] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image
    crowd counting via multi-column convolutional neural network. In Proceedings of
    the IEEE conference on computer vision and pattern recognition, pages 589–597,
    2016.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[424] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya
    Jia. Icnet for real-time semantic segmentation on high-resolution images. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 405–420, 2018.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[425] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya
    Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2881–2890, 2017.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[426] Zhong-Qiu Zhao, Shou-Tao Xu, Dian Liu, Wei-Dong Tian, and Zhi-Da Jiang.
    A review of image set classification. Neurocomputing, 335:251–260, 2019.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[427] Yefeng Zheng, David Liu, Bogdan Georgescu, Hien Nguyen, and Dorin Comaniciu.
    3d deep learning for efficient and robust landmark detection in volumetric data.
    In International Conference on Medical Image Computing and Computer-Assisted Intervention,
    pages 565–572\. Springer, 2015.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[428] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and
    Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 633–641, 2017.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[429] Kaiyang Zhou, Yu Qiao, and Tao Xiang. Deep reinforcement learning for
    unsupervised video summarization with diversity-representativeness reward. In
    Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[430] Kaiyang Zhou, Tao Xiang, and Andrea Cavallaro. Video summarisation by
    classification with deep reinforcement learning. arXiv preprint arXiv:1807.03089,
    2018.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[431] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature
    flow for video recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2349–2358, 2017.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[432] Xiahai Zhuang and Juan Shen. Multi-scale patch and multi-modality atlases
    for whole heart segmentation of mri. Medical image analysis, 31:77–87, 2016.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[433] Will Y Zou, Xiaoyu Wang, Miao Sun, and Yuanqing Lin. Generic object detection
    with dense neural patterns and regionlets. arXiv preprint arXiv:1404.4316, 2014.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
