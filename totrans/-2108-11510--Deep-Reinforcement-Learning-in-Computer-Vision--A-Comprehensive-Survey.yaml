- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:51:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:51:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.11510] Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.11510] è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šç»¼åˆè°ƒæŸ¥'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2108.11510](https://ar5iv.labs.arxiv.org/html/2108.11510)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2108.11510](https://ar5iv.labs.arxiv.org/html/2108.11510)
- en: 'Deep Reinforcement Learning in Computer Vision:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼š
- en: A Comprehensive Survey
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç»¼åˆè°ƒæŸ¥
- en: Ngan Le^(âˆ—âˆ—) â€ƒâ€ƒ Vidhiwar Singh Rathour^âˆ— â€ƒâ€ƒ Kashu Yamazaki^âˆ— â€ƒâ€ƒ Khoa Luu â€ƒâ€ƒ
    Marios Savvides
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Ngan Le^(âˆ—âˆ—) â€ƒâ€ƒ Vidhiwar Singh Rathour^âˆ— â€ƒâ€ƒ Kashu Yamazaki^âˆ— â€ƒâ€ƒ Khoa Luu â€ƒâ€ƒ
    Marios Savvides
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Deep reinforcement learning augments the reinforcement learning framework and
    utilizes the powerful representation of deep neural networks. Recent works have
    demonstrated the remarkable successes of deep reinforcement learning in various
    domains including finance, medicine, healthcare, video games, robotics, and computer
    vision. In this work, we provide a detailed review of recent and state-of-the-art
    research advances of deep reinforcement learning in computer vision. We start
    with *comprehending the theories* of deep learning, reinforcement learning, and
    deep reinforcement learning. We then *propose a categorization* of deep reinforcement
    learning methodologies and *discuss their advantages and limitations*. In particular,
    we divide deep reinforcement learning into *seven main categories* according to
    their applications in computer vision, i.e. (i) landmark localization (ii) object
    detection; (iii) object tracking; (iv) registration on both 2D image and 3D image
    volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other
    applications. Each of these categories is further analyzed with reinforcement
    learning techniques, network design, and performance. Moreover, we provide a comprehensive
    analysis of the existing publicly available datasets and examine source code availability.
    Finally, we present some open issues and discuss future research directions on
    deep reinforcement learning in computer vision.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å¼ºåŒ–å­¦ä¹ æ‰©å±•äº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œçš„å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨é‡‘èã€åŒ»å­¦ã€åŒ»ç–—ä¿å¥ã€è§†é¢‘æ¸¸æˆã€æœºå™¨äººæŠ€æœ¯å’Œè®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªé¢†åŸŸçš„æ˜¾è‘—æˆåŠŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹è®¡ç®—æœºè§†è§‰ä¸­æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æœ€æ–°å’Œæœ€å‰æ²¿ç ”ç©¶è¿›å±•çš„è¯¦ç»†å›é¡¾ã€‚æˆ‘ä»¬ä»*ç†è§£ç†è®º*å¼€å§‹ï¼ŒåŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‚æ¥ç€æˆ‘ä»¬*æå‡ºäº†ä¸€ç§åˆ†ç±»*çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¹¶*è®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§*ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æ ¹æ®åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ï¼Œå°†æ·±åº¦å¼ºåŒ–å­¦ä¹ åˆ†ä¸º*ä¸ƒä¸ªä¸»è¦ç±»åˆ«*ï¼Œå³
    (i) åœ°æ ‡å®šä½ (ii) ç‰©ä½“æ£€æµ‹ (iii) ç‰©ä½“è·Ÿè¸ª (iv) 2D å›¾åƒå’Œ 3D å›¾åƒä½“æ•°æ®çš„é…å‡† (v) å›¾åƒåˆ†å‰² (vi) è§†é¢‘åˆ†æï¼›ä»¥åŠ (vii)
    å…¶ä»–åº”ç”¨ã€‚æ¯ä¸ªç±»åˆ«éƒ½è¿›ä¸€æ­¥åˆ†æäº†å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ã€ç½‘ç»œè®¾è®¡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹ç°æœ‰å…¬å¼€æ•°æ®é›†çš„å…¨é¢åˆ†æï¼Œå¹¶æ£€æŸ¥äº†æºä»£ç çš„å¯ç”¨æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€äº›æœªè§£çš„é—®é¢˜ï¼Œå¹¶è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ä¸­æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: Reinforcement learning (RL) is a machine learning technique for learning a sequence
    of actions in an interactive environment by trial and error that maximizes the
    expected reward [[351](#bib.bib351)]. Deep Reinforcement Learning (DRL) is the
    combination of Reinforcement Learning and Deep Learning (DL) and it has become
    one of the most intriguing areas of artificial intelligence today. DRL can solve
    a wide range of complex real-world decision-making problems with human-like intelligence
    that were previously intractable. DRL was selected by [[316](#bib.bib316)], [[106](#bib.bib106)]
    as one of ten breakthrough techniques in 2013 and 2017, respectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§é€šè¿‡è¯•é”™å­¦ä¹ åœ¨äº¤äº’ç¯å¢ƒä¸­å­¦ä¹ ä¸€ç³»åˆ—åŠ¨ä½œçš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå…¶ç›®æ ‡æ˜¯æœ€å¤§åŒ–æœŸæœ›å¥–åŠ± [[351](#bib.bib351)]ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„ç»“åˆï¼Œå·²æˆä¸ºå½“å‰äººå·¥æ™ºèƒ½é¢†åŸŸæœ€å¼•äººæ³¨ç›®çš„é¢†åŸŸä¹‹ä¸€ã€‚DRL
    èƒ½å¤Ÿè§£å†³å¹¿æ³›çš„å¤æ‚ç°å®ä¸–ç•Œå†³ç­–é—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä»¥å‰æ˜¯æ— æ³•å¤„ç†çš„ï¼Œå¹¶å…·æœ‰ç±»ä¼¼äººç±»çš„æ™ºèƒ½ã€‚DRL è¢«[[316](#bib.bib316)]å’Œ[[106](#bib.bib106)]åˆ†åˆ«é€‰ä¸º2013å¹´å’Œ2017å¹´çš„åé¡¹çªç ´æ€§æŠ€æœ¯ä¹‹ä¸€ã€‚
- en: The past years have witnessed the rapid development of DRL thanks to its amazing
    achievement in solving challenging decision-making problems in the real world.
    DRL has been successfully applied into many domains including games, robotics,
    autonomous driving, healthcare, natural language processing, and computer vision.
    In contrast to supervised learning which requires large labeled training data,
    DRL samples training data from an environment. This opens up many machine learning
    applications where big labeled training data does not exist.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡å»å‡ å¹´è§è¯äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„è¿…é€Ÿå‘å±•ï¼Œå¾—ç›Šäºå…¶åœ¨è§£å†³ç°å®ä¸–ç•Œå¤æ‚å†³ç­–é—®é¢˜ä¸Šçš„æƒŠäººæˆå°±ã€‚DRLå·²ç»æˆåŠŸåº”ç”¨äºè®¸å¤šé¢†åŸŸï¼ŒåŒ…æ‹¬æ¸¸æˆã€æœºå™¨äººæŠ€æœ¯ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—ä¿å¥ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ã€‚ä¸éœ€è¦å¤§é‡æ ‡è®°è®­ç»ƒæ•°æ®çš„ç›‘ç£å­¦ä¹ ä¸åŒï¼ŒDRLä»ç¯å¢ƒä¸­é‡‡æ ·è®­ç»ƒæ•°æ®ã€‚è¿™ä¸ºé‚£äº›æ²¡æœ‰å¤§é‡æ ‡è®°è®­ç»ƒæ•°æ®çš„æœºå™¨å­¦ä¹ åº”ç”¨æ‰“å¼€äº†è®¸å¤šå¯èƒ½æ€§ã€‚
- en: Far from supervised learning, DRL-based approaches focus on solving sequential
    decision-making problems. They aim at deciding, based on a set of experiences
    collected by interacting with the environment, the sequence of actions in an uncertain
    environment to achieve some targets. Different from supervised learning where
    the feedback is available after each system action, it is simply a scalar value
    that may be delayed in time in the DRL framework. For example, the success or
    failure of the entire system is reflected after a sequence of actions. Furthermore,
    the supervised learning model is updated based on the loss/error of the output
    and there is no mechanism to get the correct value when it is wrong. This is addressed
    by policy gradients in DRL by assigning gradients without a differentiable loss
    function. This aims at teaching a model to try things out randomly and learn to
    do correct things more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç›‘ç£å­¦ä¹ ä¸åŒï¼ŒåŸºäºDRLçš„æ–¹æ³•ä¸“æ³¨äºè§£å†³é¡ºåºå†³ç­–é—®é¢˜ã€‚å®ƒä»¬æ—¨åœ¨æ ¹æ®é€šè¿‡ä¸ç¯å¢ƒäº’åŠ¨æ”¶é›†çš„ä¸€ç³»åˆ—ç»éªŒï¼Œå†³å®šåœ¨ä¸ç¡®å®šç¯å¢ƒä¸­çš„è¡ŒåŠ¨åºåˆ—ä»¥å®ç°æŸäº›ç›®æ ‡ã€‚ä¸åŒäºç›‘ç£å­¦ä¹ ï¼Œå…¶ä¸­åé¦ˆåœ¨æ¯æ¬¡ç³»ç»ŸåŠ¨ä½œåéƒ½å¯ä»¥è·å¾—ï¼ŒDRLæ¡†æ¶ä¸­çš„åé¦ˆé€šå¸¸åªæ˜¯ä¸€ä¸ªå¯èƒ½ä¼šå»¶è¿Ÿçš„æ ‡é‡å€¼ã€‚ä¾‹å¦‚ï¼Œæ•´ä¸ªç³»ç»Ÿçš„æˆåŠŸæˆ–å¤±è´¥æ˜¯åœ¨ä¸€ç³»åˆ—åŠ¨ä½œä¹‹åæ‰ä¼šåæ˜ å‡ºæ¥ã€‚æ­¤å¤–ï¼Œç›‘ç£å­¦ä¹ æ¨¡å‹æ˜¯åŸºäºè¾“å‡ºçš„æŸå¤±/é”™è¯¯è¿›è¡Œæ›´æ–°çš„ï¼Œå¹¶ä¸”å½“è¾“å‡ºé”™è¯¯æ—¶æ²¡æœ‰æœºåˆ¶æ¥è·å–æ­£ç¡®å€¼ã€‚DRLä¸­çš„ç­–ç•¥æ¢¯åº¦é€šè¿‡åœ¨æ²¡æœ‰å¯å¾®åˆ†æŸå¤±å‡½æ•°çš„æƒ…å†µä¸‹åˆ†é…æ¢¯åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™æ—¨åœ¨æ•™ä¼šæ¨¡å‹éšæœºå°è¯•ï¼Œå¹¶å­¦ä¼šæ›´å¤šåœ°åšæ­£ç¡®çš„äº‹æƒ…ã€‚
- en: Many survey papers in the field of DRL including [[13](#bib.bib13)] [[97](#bib.bib97)]
    [[414](#bib.bib414)] have been introduced recently. While [[13](#bib.bib13)] covers
    central algorithms in DRL, [[97](#bib.bib97)] provides an introduction to DRL
    models, algorithms, and techniques, where particular focus is the aspects related
    to generalization and how DRL can be used for practical applications. Recently,
    [[414](#bib.bib414)] introduces a survey, which discusses the broad applications
    of RL techniques in healthcare domains ranging from dynamic treatment regimes
    in chronic diseases and critical care, an automated medical diagnosis from both
    unstructured and structured clinical data, to many other control or scheduling
    domains that have infiltrated many aspects of a healthcare system. Different from
    the previous work, our survey focuses on how to implement DRL in various computer
    vision applications such as landmark detection, object detection, object tracking,
    image registration, image segmentation, and video analysis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ä»‹ç»äº†è®¸å¤šå…³äºDRLé¢†åŸŸçš„ç»¼è¿°è®ºæ–‡ï¼ŒåŒ…æ‹¬[[13](#bib.bib13)] [[97](#bib.bib97)] [[414](#bib.bib414)]ã€‚è™½ç„¶[[13](#bib.bib13)]æ¶µç›–äº†DRLä¸­çš„æ ¸å¿ƒç®—æ³•ï¼Œ[[97](#bib.bib97)]æä¾›äº†å¯¹DRLæ¨¡å‹ã€ç®—æ³•å’ŒæŠ€æœ¯çš„ä»‹ç»ï¼Œç‰¹åˆ«å…³æ³¨ä¸æ³›åŒ–ç›¸å…³çš„æ–¹é¢ä»¥åŠDRLå¦‚ä½•ç”¨äºå®é™…åº”ç”¨ã€‚æœ€è¿‘ï¼Œ[[414](#bib.bib414)]ä»‹ç»äº†ä¸€é¡¹ç»¼è¿°ï¼Œè®¨è®ºäº†RLæŠ€æœ¯åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œä»æ…¢æ€§ç–¾ç—…å’Œé‡ç—‡ç›‘æŠ¤ä¸­çš„åŠ¨æ€æ²»ç–—æ–¹æ¡ˆï¼Œåˆ°ä»éç»“æ„åŒ–å’Œç»“æ„åŒ–ä¸´åºŠæ•°æ®ä¸­è‡ªåŠ¨åŒ»ç–—è¯Šæ–­ï¼Œå†åˆ°æ¸—é€åˆ°åŒ»ç–—ä¿å¥ç³»ç»Ÿå„ä¸ªæ–¹é¢çš„è®¸å¤šæ§åˆ¶æˆ–è°ƒåº¦é¢†åŸŸã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„ç»¼è¿°ä¸“æ³¨äºå¦‚ä½•åœ¨å„ç§è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­å®ç°DRLï¼Œå¦‚åœ°æ ‡æ£€æµ‹ã€ç‰©ä½“æ£€æµ‹ã€ç‰©ä½“è·Ÿè¸ªã€å›¾åƒé…å‡†ã€å›¾åƒåˆ†å‰²å’Œè§†é¢‘åˆ†æã€‚
- en: 'Our goal is to provide our readers good knowledge about the principle of RL/DRL
    and thorough coverage of the latest examples of how DRL is used for solving computer
    vision tasks. We structure the rest of the paper as follows: we first introduce
    fundamentals of Deep Learning (DL) in section [2](#S2 "2 Introduction to Deep
    Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    including Multi-Layer Perceptron (MLP), Autoencoder, Deep Belief Network, Convolutional
    Neural Networks (CNNs), Recurrent Neural Networks (RNNs). Then, we present the
    theories of RL in section [3](#S3 "3 Basics of Reinforcement Learning â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), which starts with the Markov
    Decision Process (MDP) and continues with value function and Q-function. In the
    end of section [3](#S3 "3 Basics of Reinforcement Learning â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), we introduce various techniques
    in RL under two categories of model-based and model-free RL. Next, we introduce
    DRL in section [4](#S4 "4 Introduction to Deep Reinforcement Learning â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey") with main techniques in
    both value-based methods, policy gradient methods, and actor-critic methods under
    model-based and model-free categories. The application of DRL in computer vision
    will then be introduced in sections [5](#S5 "5 DRL in Landmark Detection â€£ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey"), [6](#S6 "6
    DRL in Object Detection â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey"), [7](#S7 "7 DRL in Object Tracking â€£ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey"), [8](#S8 "8 DRL in Image Registration â€£ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey"), [9](#S9 "9
    DRL in Image Segmentation â€£ Deep Reinforcement Learning in Computer Vision: A
    Comprehensive Survey"), [10](#S10 "10 DRL in Video Analysis â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), [11](#S11 "11 Others Applications
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") corresponding
    respectively to DRL in landmark detection, DRL in object detection, DRL in object
    tracking, DRL in image registration, DRL in image segmentation, DRL in video analysis
    and other applications of DRL. Each application category first starts with a problem
    introduction and then state-of-the-art approaches in the field are discussed and
    compared through a summary table. We are going to discuss some future perspectives
    in section [12](#S12 "12 Future Perspectives â€£ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey") including challenges of DRL in computer
    vision and the recent advanced techniques.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºè¯»è€…æä¾›å…³äºRL/DRLåŸç†çš„è‰¯å¥½çŸ¥è¯†ï¼Œå¹¶å…¨é¢è¦†ç›–DRLåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­åº”ç”¨çš„æœ€æ–°ç¤ºä¾‹ã€‚æˆ‘ä»¬å°†æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ç»“æ„åŒ–å¦‚ä¸‹ï¼šæˆ‘ä»¬é¦–å…ˆåœ¨ç¬¬[2](#S2
    "2 Introduction to Deep Learning â€£ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")èŠ‚ä»‹ç»æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€è‡ªç¼–ç å™¨ã€æ·±åº¦ä¿¡å¿µç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨ç¬¬[3](#S3
    "3 Basics of Reinforcement Learning â€£ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey")èŠ‚ä»‹ç»RLçš„ç†è®ºï¼Œä»é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å¼€å§‹ï¼Œç»§ç»­è®²è§£ä»·å€¼å‡½æ•°å’ŒQå‡½æ•°ã€‚åœ¨ç¬¬[3](#S3
    "3 Basics of Reinforcement Learning â€£ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey")èŠ‚çš„æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†RLä¸­ä¸¤ç±»æŠ€æœ¯ï¼šåŸºäºæ¨¡å‹çš„å’Œæ— æ¨¡å‹çš„RLã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åœ¨ç¬¬[4](#S4
    "4 Introduction to Deep Reinforcement Learning â€£ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey")èŠ‚ä»‹ç»DRLï¼Œé‡ç‚¹è®²è§£ä»·å€¼æ–¹æ³•ã€ç­–ç•¥æ¢¯åº¦æ–¹æ³•å’Œæ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•åœ¨åŸºäºæ¨¡å‹å’Œæ— æ¨¡å‹ç±»åˆ«ä¸­çš„ä¸»è¦æŠ€æœ¯ã€‚ç„¶åï¼Œç¬¬[5](#S5
    "5 DRL in Landmark Detection â€£ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")ã€ç¬¬[6](#S6 "6 DRL in Object Detection â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey")ã€ç¬¬[7](#S7 "7 DRL in Object
    Tracking â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")ã€ç¬¬[8](#S8
    "8 DRL in Image Registration â€£ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")ã€ç¬¬[9](#S9 "9 DRL in Image Segmentation â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey")ã€ç¬¬[10](#S10 "10 DRL in Video
    Analysis â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")ã€ç¬¬[11](#S11
    "11 Others Applications â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")èŠ‚åˆ†åˆ«ä»‹ç»äº†DRLåœ¨åœ°æ ‡æ£€æµ‹ã€ç›®æ ‡æ£€æµ‹ã€ç›®æ ‡è·Ÿè¸ªã€å›¾åƒé…å‡†ã€å›¾åƒåˆ†å‰²ã€è§†é¢‘åˆ†æå’ŒDRLå…¶ä»–åº”ç”¨ä¸­çš„åº”ç”¨ã€‚æ¯ä¸ªåº”ç”¨ç±»åˆ«é¦–å…ˆä»‹ç»é—®é¢˜ï¼Œç„¶åè®¨è®ºå’Œæ¯”è¾ƒè¯¥é¢†åŸŸçš„æœ€æ–°æ–¹æ³•ï¼Œé€šè¿‡æ€»ç»“è¡¨æ ¼å‘ˆç°ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬[12](#S12
    "12 Future Perspectives â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")èŠ‚è®¨è®ºä¸€äº›æœªæ¥çš„å±•æœ›ï¼ŒåŒ…æ‹¬DRLåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„æŒ‘æˆ˜å’Œè¿‘æœŸå…ˆè¿›æŠ€æœ¯ã€‚'
- en: 2 Introduction to Deep Learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 æ·±åº¦å­¦ä¹ ç®€ä»‹
- en: 2.1 Multi-Layer Perceptron (MLP)
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰
- en: 'Deep learning models, in simple words, are large and deep artificial neural
    networks. Let us consider the simplest possible neural network which is called
    â€neuronâ€ as illustrated in Fig. [1](#S2.F1 "Figure 1 â€£ 2.1 Multi-Layer Perceptron
    (MLP) â€£ 2 Introduction to Deep Learning â€£ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey"). A computational model of a single neuron is
    called a perceptron which consists of one or more inputs, a processor, and a single
    output.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç®€å•æ¥è¯´ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¯å¤§å‹ä¸”æ·±å±‚çš„äººå·¥ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬ä»¥æœ€ç®€å•çš„ç¥ç»ç½‘ç»œä¸ºä¾‹ï¼Œè¿™ç§ç½‘ç»œè¢«ç§°ä¸ºâ€œç¥ç»å…ƒâ€ï¼Œå¦‚å›¾ [1](#S2.F1 "Figure 1
    â€£ 2.1 Multi-Layer Perceptron (MLP) â€£ 2 Introduction to Deep Learning â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey") æ‰€ç¤ºã€‚ä¸€ä¸ªç¥ç»å…ƒçš„è®¡ç®—æ¨¡å‹è¢«ç§°ä¸ºæ„ŸçŸ¥å™¨ï¼Œå®ƒç”±ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥ã€ä¸€ä¸ªå¤„ç†å™¨å’Œä¸€ä¸ªè¾“å‡ºç»„æˆã€‚'
- en: '![Refer to caption](img/ae3f08d40dc14c10f862a38b2763122e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/ae3f08d40dc14c10f862a38b2763122e.png)'
- en: 'Figure 1: An example of one neuron which takes input $\textbf{x}=[x_{1},x_{2},x_{3}]$,
    the intercept term $+1$ as bias, and the output o.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šä¸€ä¸ªç¥ç»å…ƒçš„ç¤ºä¾‹ï¼Œå®ƒä»¥ $\textbf{x}=[x_{1},x_{2},x_{3}]$ ä½œä¸ºè¾“å…¥ï¼Œä»¥æˆªè·é¡¹ $+1$ ä½œä¸ºåç½®ï¼Œå¹¶è®¡ç®—è¾“å‡º oã€‚
- en: '![Refer to caption](img/3c8e28551e381f4fb936359fe3a47ebf.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/3c8e28551e381f4fb936359fe3a47ebf.png)'
- en: 'Figure 2: An example of multi-layer perceptron network (MLP)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šå¤šå±‚æ„ŸçŸ¥å™¨ç½‘ç»œ (MLP) çš„ç¤ºä¾‹
- en: '![Refer to caption](img/ffb334467464b76c1b885be0d35835d8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/ffb334467464b76c1b885be0d35835d8.png)'
- en: 'Figure 3: An illustration of various DL architectures. (a): Autoencoder (AE);
    (b): Deep Belief Network; (c): Convolutional Neural Network (CNN); (d): Recurrent
    Neural Network (RNN).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šå„ç§æ·±åº¦å­¦ä¹ æ¶æ„çš„ç¤ºæ„å›¾ã€‚ï¼ˆaï¼‰ï¼šè‡ªç¼–ç å™¨ (AE)ï¼›ï¼ˆbï¼‰ï¼šæ·±åº¦ç½®ä¿¡ç½‘ç»œï¼›ï¼ˆcï¼‰ï¼šå·ç§¯ç¥ç»ç½‘ç»œ (CNN)ï¼›ï¼ˆdï¼‰ï¼šé€’å½’ç¥ç»ç½‘ç»œ (RNN)ã€‚
- en: 'In this example, the neuron is a computational unit that takes $\textbf{x}=[x_{0},x_{1},x_{2}]$
    as input, the intercept term $+1$ as bias b, and the output o. The goal of this
    simple network is to learn a function $f:\mathrm{R^{N}}\rightarrow\mathrm{R^{M}}$
    where $N$ is the number of dimensions for input x and $M$ is the number of dimensions
    for output which is computed as $\textbf{o}=f(\textbf{x},\theta)$, where $\theta$
    is a set of weights and are known as weights $\theta=\{w_{i}\}$. Mathematically,
    the output o of a one neuron is defined as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œç¥ç»å…ƒæ˜¯ä¸€ä¸ªè®¡ç®—å•å…ƒï¼Œå®ƒä»¥ $\textbf{x}=[x_{0},x_{1},x_{2}]$ ä½œä¸ºè¾“å…¥ï¼Œä»¥æˆªè·é¡¹ $+1$ ä½œä¸ºåç½® bï¼Œè®¡ç®—è¾“å‡º
    oã€‚è¿™ä¸ªç®€å•ç½‘ç»œçš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªå‡½æ•° $f:\mathrm{R^{N}}\rightarrow\mathrm{R^{M}}$ï¼Œå…¶ä¸­ $N$ æ˜¯è¾“å…¥ x çš„ç»´åº¦æ•°é‡ï¼Œ$M$
    æ˜¯è¾“å‡ºçš„ç»´åº¦æ•°é‡ï¼Œè®¡ç®—å…¬å¼ä¸º $\textbf{o}=f(\textbf{x},\theta)$ï¼Œå…¶ä¸­ $\theta$ æ˜¯ä¸€ç»„æƒé‡ï¼Œç§°ä¸ºæƒé‡ $\theta=\{w_{i}\}$ã€‚åœ¨æ•°å­¦ä¸Šï¼Œä¸€ä¸ªç¥ç»å…ƒçš„è¾“å‡º
    o å®šä¹‰ä¸ºï¼š
- en: '|  | $\textbf{o}=f(\textbf{x},\theta)=\sigma\left(\sum_{i=1}^{N}{w_{i}x_{i}+b}\right)=\sigma(\textbf{W}^{T}\textbf{x}+b)$
    |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{o}=f(\textbf{x},\theta)=\sigma\left(\sum_{i=1}^{N}{w_{i}x_{i}+b}\right)=\sigma(\textbf{W}^{T}\textbf{x}+b)$
    |  | (1) |'
- en: 'In this equation, $\sigma$ is the point-wise non-linear activation function.
    The common non-linear activation functions for hidden units are hyperbolic tangent
    (Tanh), sigmoid, softmax, ReLU, and LeakyReLU. A typical multi-layer perception
    (MLP) neural network is composed of one input layer, one output layer, and many
    hidden layers. Each layer may contain many units. In this network, x is the input
    layer, o is the output layer. The middle layer is called the hidden layer. In
    Fig. [2](#S2.F2 "Figure 2 â€£ 2.1 Multi-Layer Perceptron (MLP) â€£ 2 Introduction
    to Deep Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(b), MLP contains 3 units of the input layer, 3 units of the hidden layer,
    and 1 unit of the output layer.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿™ä¸ªæ–¹ç¨‹ä¸­ï¼Œ$\sigma$ æ˜¯é€ç‚¹éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚éšè—å•å…ƒçš„å¸¸è§éçº¿æ€§æ¿€æ´»å‡½æ•°æœ‰åŒæ›²æ­£åˆ‡ï¼ˆTanhï¼‰ã€sigmoidã€softmaxã€ReLU å’Œ
    LeakyReLUã€‚ä¸€ä¸ªå…¸å‹çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ç¥ç»ç½‘ç»œç”±ä¸€ä¸ªè¾“å…¥å±‚ã€ä¸€ä¸ªè¾“å‡ºå±‚å’Œå¤šä¸ªéšè—å±‚ç»„æˆã€‚æ¯ä¸€å±‚å¯èƒ½åŒ…å«å¤šä¸ªå•å…ƒã€‚åœ¨è¿™ä¸ªç½‘ç»œä¸­ï¼Œx æ˜¯è¾“å…¥å±‚ï¼Œo
    æ˜¯è¾“å‡ºå±‚ã€‚ä¸­é—´å±‚ç§°ä¸ºéšè—å±‚ã€‚åœ¨å›¾ [2](#S2.F2 "Figure 2 â€£ 2.1 Multi-Layer Perceptron (MLP) â€£ 2 Introduction
    to Deep Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(b) ä¸­ï¼ŒMLP åŒ…å« 3 ä¸ªè¾“å…¥å±‚å•å…ƒã€3 ä¸ªéšè—å±‚å•å…ƒå’Œ 1 ä¸ªè¾“å‡ºå±‚å•å…ƒã€‚'
- en: In general, we consider a MLP neural network with $L$ hidden layers of units,
    one layer of input units and one layer of output units. The number of input units
    is $N$, output units is $M$, and units in hidden layer $l^{th}$ is $N^{l}$. The
    weight of the $j^{th}$ unit in layer $l^{th}$ and the $i^{th}$ unit in layer $(l+1)^{th}$
    is denoted by $w_{ij}^{l}$. The activation of the $i^{th}$ unit in layer $l^{th}$
    is $\textbf{h}_{i}^{l}$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå…·æœ‰ $L$ å±‚éšè—å•å…ƒçš„ MLP ç¥ç»ç½‘ç»œï¼Œä¸€ä¸ªè¾“å…¥å•å…ƒå±‚å’Œä¸€ä¸ªè¾“å‡ºå•å…ƒå±‚ã€‚è¾“å…¥å•å…ƒçš„æ•°é‡æ˜¯ $N$ï¼Œè¾“å‡ºå•å…ƒçš„æ•°é‡æ˜¯ $M$ï¼Œç¬¬
    $l^{th}$ å±‚çš„éšè—å•å…ƒæ•°é‡æ˜¯ $N^{l}$ã€‚ç¬¬ $l^{th}$ å±‚çš„ç¬¬ $j^{th}$ å•å…ƒå’Œ $(l+1)^{th}$ å±‚çš„ç¬¬ $i^{th}$
    å•å…ƒä¹‹é—´çš„æƒé‡ç”¨ $w_{ij}^{l}$ è¡¨ç¤ºã€‚ç¬¬ $l^{th}$ å±‚çš„ç¬¬ $i^{th}$ å•å…ƒçš„æ¿€æ´»å€¼ä¸º $\textbf{h}_{i}^{l}$ã€‚
- en: '![Refer to caption](img/873522a85978bea2ad1c0c5014fc2f0d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/873522a85978bea2ad1c0c5014fc2f0d.png)'
- en: 'Figure 4: Architecture of a typical convolutional network for image classification
    containing three basic layers: convolution layer, pooling layer and fully connected
    layer'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šå…¸å‹å·ç§¯ç½‘ç»œçš„æ¶æ„ç”¨äºå›¾åƒåˆ†ç±»ï¼ŒåŒ…å«ä¸‰ä¸ªåŸºæœ¬å±‚ï¼šå·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚
- en: 2.2 Autoencoder
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 è‡ªç¼–ç å™¨
- en: 'Autoencoder is an unsupervised algorithm used for representation learning,
    such as feature selection or dimension reduction. A gentle introduction to Variational
    Autoencoder (VAE) is given in [[11](#bib.bib11)] and VAE framework is illustrated
    in Fig.[3](#S2.F3 "Figure 3 â€£ 2.1 Multi-Layer Perceptron (MLP) â€£ 2 Introduction
    to Deep Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(a). In general, VAE aims to learn a parametric latent variable model
    by maximizing the marginal log-likelihood of the training data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'è‡ªç¼–ç å™¨æ˜¯ä¸€ç§æ— ç›‘ç£ç®—æ³•ï¼Œç”¨äºè¡¨ç¤ºå­¦ä¹ ï¼Œå¦‚ç‰¹å¾é€‰æ‹©æˆ–ç»´åº¦å‡å°‘ã€‚æœ‰å…³å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„æ¸©å’Œä»‹ç»è§[[11](#bib.bib11)]ï¼ŒVAEæ¡†æ¶åœ¨å›¾[3](#S2.F3
    "Figure 3 â€£ 2.1 Multi-Layer Perceptron (MLP) â€£ 2 Introduction to Deep Learning
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")(a)ä¸­æœ‰æ‰€è¯´æ˜ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒVAEæ—¨åœ¨é€šè¿‡æœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„è¾¹é™…å¯¹æ•°ä¼¼ç„¶æ¥å­¦ä¹ ä¸€ä¸ªå‚æ•°åŒ–çš„æ½œå˜é‡æ¨¡å‹ã€‚'
- en: 2.3 Deep Belief Network
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 æ·±åº¦ç½®ä¿¡ç½‘ç»œ
- en: 'Deep Belief Network (DBN) and Deep Autoencoder are two common unsupervised
    approaches that have been used to initialize the network instead of random initialization.
    While Deep Autoencoder is based on Autoencoder, Deep Belief Networks is based
    on Restricted Boltzmann Machine (RBM), which contains a layer of input data and
    a layer of hidden units that learn to represent features that capture high-order
    correlations in the data as illustrated in Fig.[3](#S2.F3 "Figure 3 â€£ 2.1 Multi-Layer
    Perceptron (MLP) â€£ 2 Introduction to Deep Learning â€£ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey")(b).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ·±åº¦ç½®ä¿¡ç½‘ç»œï¼ˆDBNï¼‰å’Œæ·±åº¦è‡ªç¼–ç å™¨æ˜¯ä¸¤ç§å¸¸è§çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå®ƒä»¬è¢«ç”¨æ¥åˆå§‹åŒ–ç½‘ç»œè€Œä¸æ˜¯éšæœºåˆå§‹åŒ–ã€‚è™½ç„¶æ·±åº¦è‡ªç¼–ç å™¨åŸºäºè‡ªç¼–ç å™¨ï¼Œæ·±åº¦ç½®ä¿¡ç½‘ç»œåˆ™åŸºäºé™åˆ¶ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ï¼ŒRBMåŒ…å«ä¸€ä¸ªè¾“å…¥æ•°æ®å±‚å’Œä¸€ä¸ªéšå«å•å…ƒå±‚ï¼Œéšå«å•å…ƒå±‚å­¦ä¹ è¡¨ç¤ºèƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­é«˜é˜¶ç›¸å…³æ€§çš„ç‰¹å¾ï¼Œå¦‚å›¾[3](#S2.F3
    "Figure 3 â€£ 2.1 Multi-Layer Perceptron (MLP) â€£ 2 Introduction to Deep Learning
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")(b)æ‰€ç¤ºã€‚'
- en: 2.4 Convolutional Neural Networks (CNN)
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
- en: 'Convolutional Neural Network (CNN) [[204](#bib.bib204)] [[203](#bib.bib203)]
    is a special case of fully connected MLP that implements weight sharing for processing
    data. CNN uses the spatial correlation of the signal to utilize the architecture
    in a more sensible way. Their architecture, somewhat inspired by the biological
    visual system, possesses two key properties that make them extremely useful for
    image applications: spatially shared weights and spatial pooling. These kinds
    of networks learn features that are shift-invariant, i.e., filters that are useful
    across the entire image (due to the fact that image statistics are stationary).
    The pooling layers are responsible for reducing the sensitivity of the output
    to slight input shifts and distortions, and increasing the reception field for
    next layers. Since 2012, one of the most notable results in Deep Learning is the
    use of CNN to obtain a remarkable improvement in object recognition in ImageNet
    classification challenge [[72](#bib.bib72)] [[187](#bib.bib187)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰[[204](#bib.bib204)] [[203](#bib.bib203)] æ˜¯ä¸€ç§ç‰¹æ®Šçš„å…¨è¿æ¥MLPï¼Œå®ƒå®ç°äº†æƒé‡å…±äº«ä»¥å¤„ç†æ•°æ®ã€‚CNNåˆ©ç”¨ä¿¡å·çš„ç©ºé—´ç›¸å…³æ€§ï¼Œä»¥æ›´åˆç†çš„æ–¹å¼åˆ©ç”¨æ¶æ„ã€‚å…¶æ¶æ„åœ¨ä¸€å®šç¨‹åº¦ä¸Šå—åˆ°ç”Ÿç‰©è§†è§‰ç³»ç»Ÿçš„å¯å‘ï¼Œå…·æœ‰ä¸¤ä¸ªå…³é”®å±æ€§ï¼Œä½¿å…¶åœ¨å›¾åƒåº”ç”¨ä¸­æä¸ºæœ‰ç”¨ï¼šç©ºé—´å…±äº«æƒé‡å’Œç©ºé—´æ± åŒ–ã€‚è¿™äº›ç½‘ç»œå­¦ä¹ çš„ç‰¹å¾æ˜¯å¹³ç§»ä¸å˜çš„ï¼Œå³åœ¨æ•´ä¸ªå›¾åƒä¸­éƒ½æœ‰æ•ˆçš„æ»¤æ³¢å™¨ï¼ˆå› ä¸ºå›¾åƒç»Ÿè®¡æ˜¯é™æ€çš„ï¼‰ã€‚æ± åŒ–å±‚è´Ÿè´£å‡å°‘è¾“å‡ºå¯¹è¾“å…¥è½»å¾®ä½ç§»å’Œå¤±çœŸçš„æ•æ„Ÿæ€§ï¼Œå¹¶å¢åŠ ä¸‹ä¸€å±‚çš„æ¥å—åŸŸã€‚è‡ª2012å¹´ä»¥æ¥ï¼Œæ·±åº¦å­¦ä¹ é¢†åŸŸæœ€æ˜¾è‘—çš„æˆæœä¹‹ä¸€æ˜¯ä½¿ç”¨CNNåœ¨ImageNetåˆ†ç±»æŒ‘æˆ˜ä¸­å–å¾—äº†æ˜¾è‘—çš„ç›®æ ‡è¯†åˆ«æ”¹è¿›[[72](#bib.bib72)]
    [[187](#bib.bib187)]ã€‚
- en: 'A typical CNN is composed of multiple stages, as shown in Fig. [3](#S2.F3 "Figure
    3 â€£ 2.1 Multi-Layer Perceptron (MLP) â€£ 2 Introduction to Deep Learning â€£ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey")(c). The output
    of each stage is made of a set of 2D arrays called feature maps. Each feature
    map is the outcome of one convolutional (and an optional pooling) filter applied
    over the full image. A point-wise non-linear activation function is applied after
    each convolution. In its more general form, a CNN can be written as'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…¸å‹çš„ CNN ç”±å¤šä¸ªé˜¶æ®µç»„æˆï¼Œå¦‚å›¾ [3](#S2.F3 "å›¾ 3 â€£ 2.1 å¤šå±‚æ„ŸçŸ¥å™¨ (MLP) â€£ 2 æ·±åº¦å­¦ä¹ ä»‹ç» â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢ç»¼è¿°")(c)
    æ‰€ç¤ºã€‚æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºç”±ä¸€ç»„ç§°ä¸ºç‰¹å¾å›¾çš„ 2D æ•°ç»„ç»„æˆã€‚æ¯ä¸ªç‰¹å¾å›¾æ˜¯ä¸€ä¸ªå·ç§¯ï¼ˆåŠå¯é€‰çš„æ± åŒ–ï¼‰æ»¤æ³¢å™¨åº”ç”¨äºæ•´ä¸ªå›¾åƒçš„ç»“æœã€‚æ¯æ¬¡å·ç§¯åéƒ½ä¼šåº”ç”¨ä¸€ä¸ªé€ç‚¹éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚åœ¨å…¶æ›´ä¸€èˆ¬çš„å½¢å¼ä¸‹ï¼ŒCNN
    å¯ä»¥å†™ä½œ
- en: '|  | <math   alttext="\begin{split}\textbf{h}^{0}=&amp;\textbf{x}\\ \textbf{h}^{l}=&amp;pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{split}\textbf{h}^{0}=&amp;\textbf{x}\\ \textbf{h}^{l}=&amp;pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\'
- en: \textbf{o}=&amp;\textbf{h}^{L}\\
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \textbf{o}=&amp;\textbf{h}^{L}\\
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd columnalign="right" ><mrow ><msup ><mtext >h</mtext><mn  >0</mn></msup><mo
    >=</mo></mrow></mtd><mtd columnalign="left" ><mtext >x</mtext></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><msup ><mtext >h</mtext><mi >l</mi></msup><mo
    >=</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mrow ><mrow ><mrow ><mi  >p</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><msup ><mi  >l</mi><mi >l</mi></msup><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub
    ><mi >Ïƒ</mi><mi >l</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mrow ><msup ><mtext >w</mtext><mi >l</mi></msup><mo
    lspace="0em" rspace="0em" >â€‹</mo><msup ><mtext >h</mtext><mrow ><mi >l</mi><mo
    >âˆ’</mo><mn >1</mn></mrow></msup></mrow><mo >+</mo><msup ><mtext >b</mtext><mi
    >l</mi></msup></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo><mrow ><mo rspace="0.167em"  >âˆ€</mo><mi >l</mi></mrow></mrow><mo >âˆˆ</mo><mn
    >1</mn></mrow><mo >,</mo><mrow ><mn >2</mn><mo >,</mo><mrow ><mi mathvariant="normal"  >â€¦</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >L</mi></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mtext >o</mtext><mo >=</mo></mrow></mtd><mtd
    columnalign="left" ><msup ><mtext >h</mtext><mi >L</mi></msup></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><cn
    type="integer" >0</cn></apply><apply ><ci ><mtext >x</mtext></ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><ci >ğ‘™</ci></apply></apply></apply><apply
    ><apply  ><ci >ğ‘</ci><ci >ğ‘œ</ci><ci  >ğ‘œ</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğ‘™</ci><ci >ğ‘™</ci></apply><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğœ</ci><ci >ğ‘™</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    ><mtext >w</mtext></ci><ci >ğ‘™</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    ><mtext >h</mtext></ci><apply ><ci >ğ‘™</ci><cn type="integer" >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci ><mtext >b</mtext></ci><ci
    >ğ‘™</ci></apply></apply></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >formulae-sequence</csymbol><apply ><apply ><csymbol cd="latexml" >for-all</csymbol><ci
    >ğ‘™</ci></apply><list ><cn type="integer" >1</cn><cn type="integer" >2</cn></list></apply><apply
    ><apply  ><ci >â€¦</ci><ci >ğ¿</ci><ci ><mtext >o</mtext></ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><ci >ğ¿</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\textbf{h}^{0}=&\textbf{x}\\ \textbf{h}^{l}=&pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\ \textbf{o}=&\textbf{h}^{L}\\ \end{split}</annotation></semantics></math>
    |  | (2) |
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: where $\textbf{w}^{l},\textbf{b}^{l}$ are trainable parameters as in MLPs at
    layer $l^{th}$. $\textbf{x}\in\mathrm{R}^{c\times h\times w}$ is vectorized from
    an input image with $c$ being the color channels, $h$ the image height and $w$
    the image width. $\textbf{o}\in\mathrm{R}^{n\times h^{\prime}\times w^{\prime}}$
    is vectorized from an array of dimension $h^{\prime}\times w^{\prime}$ of output
    vector (of dimension $n$). $pool^{l}$ is a (optional) pooling function at layer
    $l^{th}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\textbf{w}^{l},\textbf{b}^{l}$ æ˜¯åƒMLPsä¸­å±‚ $l^{th}$ çš„å¯è®­ç»ƒå‚æ•°ã€‚$\textbf{x}\in\mathrm{R}^{c\times
    h\times w}$ æ˜¯ä»è¾“å…¥å›¾åƒå‘é‡åŒ–å¾—åˆ°çš„ï¼Œå…¶ä¸­ $c$ ä¸ºé¢œè‰²é€šé“ï¼Œ$h$ ä¸ºå›¾åƒé«˜åº¦ï¼Œ$w$ ä¸ºå›¾åƒå®½åº¦ã€‚$\textbf{o}\in\mathrm{R}^{n\times
    h^{\prime}\times w^{\prime}}$ æ˜¯ä»è¾“å‡ºå‘é‡ï¼ˆç»´åº¦ä¸º $n$ï¼‰çš„ $h^{\prime}\times w^{\prime}$ çš„æ•°ç»„å‘é‡åŒ–å¾—åˆ°çš„ã€‚$pool^{l}$
    æ˜¯å±‚ $l^{th}$ çš„ï¼ˆå¯é€‰ï¼‰æ± åŒ–å‡½æ•°ã€‚
- en: Compared to traditional machine learning methods, CNN has achieved state-of-the-art
    performance in many domains including image understanding, video analysis and
    audio/speech recognition. In image understanding [[404](#bib.bib404)], [[426](#bib.bib426)],
    CNN outperforms human capacities [[39](#bib.bib39)]. Video analysis [[422](#bib.bib422)],
    [[217](#bib.bib217)] is another application that turns the CNN model from a detector
    [[374](#bib.bib374)] into a tracker [[94](#bib.bib94)]. As a special case of image
    segmentation [[194](#bib.bib194)], [[193](#bib.bib193)], saliency detection is
    another computer vision application that uses CNN [[381](#bib.bib381)], [[213](#bib.bib213)].
    In addition to the previous applications, pose estimation [[290](#bib.bib290)],
    [[362](#bib.bib362)] is another interesting research that uses CNN to estimate
    human-body pose. Action recognition in both still images and videos is a special
    case of recognition and is a challenging problem. [[110](#bib.bib110)] utilizes
    CNN-based representation of contextual information in which the most representative
    secondary region within a large number of object proposal regions, together with
    the contextual features, is used to describe the primary region. CNN-based action
    recognition in video sequences is reviewed in [[420](#bib.bib420)]. Text detection
    and recognition using CNN is the next step of optical character recognition (OCR)
    [[406](#bib.bib406)] and word spotting [[160](#bib.bib160)]. Not only in computer
    vision, CNN has been successfully applied into other domains such as speech recognition
    and speech synthesis [[274](#bib.bib274)], [[283](#bib.bib283)], biometrics [[242](#bib.bib242)],
    [[85](#bib.bib85)], [[281](#bib.bib281)], [[350](#bib.bib350)],[[304](#bib.bib304)],
    [[261](#bib.bib261)], biomedical [[191](#bib.bib191)], [[342](#bib.bib342)], [[192](#bib.bib192)],
    [[411](#bib.bib411)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCNN åœ¨åŒ…æ‹¬å›¾åƒç†è§£ã€è§†é¢‘åˆ†æå’ŒéŸ³é¢‘/è¯­éŸ³è¯†åˆ«ç­‰è®¸å¤šé¢†åŸŸä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å›¾åƒç†è§£ä¸­ [[404](#bib.bib404)],
    [[426](#bib.bib426)], CNN è¶…è¶Šäº†äººç±»çš„èƒ½åŠ› [[39](#bib.bib39)]ã€‚è§†é¢‘åˆ†æ [[422](#bib.bib422)],
    [[217](#bib.bib217)] æ˜¯å¦ä¸€ä¸ªåº”ç”¨ï¼Œå®ƒå°† CNN æ¨¡å‹ä»æ£€æµ‹å™¨ [[374](#bib.bib374)] è½¬å˜ä¸ºè·Ÿè¸ªå™¨ [[94](#bib.bib94)]ã€‚ä½œä¸ºå›¾åƒåˆ†å‰²
    [[194](#bib.bib194)], [[193](#bib.bib193)] çš„ç‰¹æ®Šæƒ…å†µï¼Œæ˜¾è‘—æ€§æ£€æµ‹æ˜¯å¦ä¸€ä¸ªåˆ©ç”¨ CNN çš„è®¡ç®—æœºè§†è§‰åº”ç”¨ [[381](#bib.bib381)],
    [[213](#bib.bib213)]ã€‚é™¤äº†ä¹‹å‰çš„åº”ç”¨ï¼Œå§¿æ€ä¼°è®¡ [[290](#bib.bib290)], [[362](#bib.bib362)] æ˜¯å¦ä¸€ä¸ªæœ‰è¶£çš„ç ”ç©¶ï¼Œåˆ©ç”¨
    CNN ä¼°è®¡äººä½“å§¿åŠ¿ã€‚é™æ€å›¾åƒå’Œè§†é¢‘ä¸­çš„åŠ¨ä½œè¯†åˆ«æ˜¯è¯†åˆ«çš„ç‰¹æ®Šæƒ…å†µï¼Œå¹¶ä¸”æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚[[110](#bib.bib110)] åˆ©ç”¨åŸºäº CNN çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¡¨ç¤ºï¼Œå…¶ä¸­åœ¨å¤§é‡ç›®æ ‡æè®®åŒºåŸŸä¸­æœ€å…·ä»£è¡¨æ€§çš„æ¬¡åŒºåŸŸä¸ä¸Šä¸‹æ–‡ç‰¹å¾ä¸€èµ·ï¼Œç”¨äºæè¿°ä¸»è¦åŒºåŸŸã€‚åŸºäº
    CNN çš„è§†é¢‘åºåˆ—åŠ¨ä½œè¯†åˆ«çš„ç»¼è¿°è§ [[420](#bib.bib420)]ã€‚ä½¿ç”¨ CNN çš„æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«æ˜¯å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ [[406](#bib.bib406)]
    å’Œè¯æ±‡æ£€æµ‹ [[160](#bib.bib160)] çš„ä¸‹ä¸€æ­¥ã€‚CNN ä¸ä»…åœ¨è®¡ç®—æœºè§†è§‰ä¸­å–å¾—æˆåŠŸï¼Œè¿˜æˆåŠŸåº”ç”¨äºè¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆ [[274](#bib.bib274)],
    [[283](#bib.bib283)], ç”Ÿç‰©è¯†åˆ« [[242](#bib.bib242)], [[85](#bib.bib85)], [[281](#bib.bib281)],
    [[350](#bib.bib350)], [[304](#bib.bib304)], [[261](#bib.bib261)], ç”Ÿç‰©åŒ»å­¦ [[191](#bib.bib191)],
    [[342](#bib.bib342)], [[192](#bib.bib192)], [[411](#bib.bib411)]ã€‚
- en: 2.5 Recurrent Neural Networks (RNN)
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰
- en: 'RNN is an extremely powerful sequence model and was introduced in the early
    1990s [[172](#bib.bib172)]. A typical RNN contains three parts, namely, sequential
    input data ($\textbf{x}_{t}$), hidden state ($\textbf{h}_{t}$) and sequential
    output data ($\textbf{y}_{t}$) as shown in Fig. [3](#S2.F3 "Figure 3 â€£ 2.1 Multi-Layer
    Perceptron (MLP) â€£ 2 Introduction to Deep Learning â€£ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey")(d).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RNN æ˜¯ä¸€ç§æä¸ºå¼ºå¤§çš„åºåˆ—æ¨¡å‹ï¼Œæ—©åœ¨1990å¹´ä»£åˆæœŸå°±è¢«å¼•å…¥ [[172](#bib.bib172)]ã€‚ä¸€ä¸ªå…¸å‹çš„ RNN åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼Œå³åºåˆ—è¾“å…¥æ•°æ®ï¼ˆ$\textbf{x}_{t}$ï¼‰ã€éšè—çŠ¶æ€ï¼ˆ$\textbf{h}_{t}$ï¼‰å’Œåºåˆ—è¾“å‡ºæ•°æ®ï¼ˆ$\textbf{y}_{t}$ï¼‰ï¼Œå¦‚å›¾
    [3](#S2.F3 "å›¾ 3 â€£ 2.1 å¤šå±‚æ„ŸçŸ¥å™¨ (MLP) â€£ 2 æ·±åº¦å­¦ä¹ ç®€ä»‹ â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢è°ƒæŸ¥")(d) æ‰€ç¤ºã€‚
- en: RNN makes use of sequential information and performs the same task for every
    element of a sequence where the output is dependent on the previous computations.
    The activation of the hidden states at time-step $t$ is computed as a function
    $f$ of the current input symbol $\bf{x}_{t}$ and the previous hidden states $\bf{h}_{t-1}$.
    The output at time $t$ is calculated as a function $g$ of the current hidden state
    $\bf{h}_{t}$ as follows
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RNN åˆ©ç”¨åºåˆ—ä¿¡æ¯ï¼Œå¹¶å¯¹åºåˆ—çš„æ¯ä¸ªå…ƒç´ æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡ï¼Œå…¶ä¸­è¾“å‡ºä¾èµ–äºä¹‹å‰çš„è®¡ç®—ã€‚æ—¶é—´æ­¥ $t$ çš„éšè—çŠ¶æ€æ¿€æ´»ä½œä¸ºå½“å‰è¾“å…¥ç¬¦å· $\bf{x}_{t}$
    å’Œä¹‹å‰éšè—çŠ¶æ€ $\bf{h}_{t-1}$ çš„å‡½æ•° $f$ æ¥è®¡ç®—ã€‚æ—¶é—´ $t$ çš„è¾“å‡ºä½œä¸ºå½“å‰éšè—çŠ¶æ€ $\bf{h}_{t}$ çš„å‡½æ•° $g$ è®¡ç®—å¦‚ä¸‹
- en: '|  | $\begin{split}\textbf{{h}}_{t}&amp;=f(\textbf{Ux}_{t}+\textbf{Wh}_{t-1})\\
    \textbf{y}_{t}&amp;=g(\textbf{Vh}_{t})\end{split}$ |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\textbf{{h}}_{t}&=f(\textbf{Ux}_{t}+\textbf{Wh}_{t-1})\\
    \textbf{y}_{t}&=g(\textbf{Vh}_{t})\end{split}$ |  | (3) |'
- en: where $\bf{U}$ is the input-to-hidden weight matrix, $\bf{W}$ is the state-to-state
    recurrent weight matrix, $\bf{V}$ is the hidden-to-output weight matrix. $f$ is
    usually a logistic sigmoid function or a hyperbolic tangent function and $g$ is
    defined as a softmax function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\bf{U}$ æ˜¯è¾“å…¥åˆ°éšè—çš„æƒé‡çŸ©é˜µï¼Œ$\bf{W}$ æ˜¯çŠ¶æ€åˆ°çŠ¶æ€çš„é€’å½’æƒé‡çŸ©é˜µï¼Œ$\bf{V}$ æ˜¯éšè—åˆ°è¾“å‡ºçš„æƒé‡çŸ©é˜µã€‚$f$ é€šå¸¸æ˜¯é€»è¾‘
    sigmoid å‡½æ•°æˆ–åŒæ›²æ­£åˆ‡å‡½æ•°ï¼Œ$g$ å®šä¹‰ä¸º softmax å‡½æ•°ã€‚
- en: Most works on RNN have made use of the method of backpropagation through time
    (BPTT) [[318](#bib.bib318)] to train the parameter set $(\bf{U},\bf{V},\bf{W})$
    and propagate error backward through time. In classic backpropagation, the error
    or loss function is defined as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°å…³äº RNN çš„ç ”ç©¶ä½¿ç”¨äº†é€šè¿‡æ—¶é—´çš„åå‘ä¼ æ’­ï¼ˆBPTTï¼‰[[318](#bib.bib318)] æ–¹æ³•æ¥è®­ç»ƒå‚æ•°é›† $(\bf{U},\bf{V},\bf{W})$
    å¹¶å°†è¯¯å·®å‘åä¼ æ’­ã€‚åœ¨ç»å…¸çš„åå‘ä¼ æ’­ä¸­ï¼Œè¯¯å·®æˆ–æŸå¤±å‡½æ•°å®šä¹‰ä¸º
- en: '|  | $E(\textbf{y''},\textbf{y})=\sum_{t}{&#124;&#124;\textbf{y''}_{t}-\textbf{y}_{t}&#124;&#124;^{2}}$
    |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(\textbf{y''},\textbf{y})=\sum_{t}{\|\textbf{y''}_{t}-\textbf{y}_{t}\|^{2}}$
    |  | (4) |'
- en: where $\textbf{y}_{t}$ is the prediction and $\textbf{y'}_{t}$ is the labeled
    groundtruth.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\textbf{y}_{t}$ æ˜¯é¢„æµ‹å€¼ï¼Œ$\textbf{y'}_{t}$ æ˜¯æ ‡è®°çš„çœŸå®å€¼ã€‚
- en: For a specific weight W, the update rule for gradient descent is defined as
    $\textbf{W}^{new}=\textbf{W}-\gamma\frac{\partial E}{\partial\textbf{W}}$, where
    $\gamma$ is the learning rate. In RNN model, the gradients of the error with respect
    to our parameters U, V and W are learned using Stochastic Gradient Descent (SGD)
    and chain rule of differentiation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç‰¹å®šçš„æƒé‡ Wï¼Œæ¢¯åº¦ä¸‹é™çš„æ›´æ–°è§„åˆ™å®šä¹‰ä¸º $\textbf{W}^{new}=\textbf{W}-\gamma\frac{\partial E}{\partial\textbf{W}}$ï¼Œå…¶ä¸­
    $\gamma$ æ˜¯å­¦ä¹ ç‡ã€‚åœ¨ RNN æ¨¡å‹ä¸­ï¼Œä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰å’Œé“¾å¼æ³•åˆ™æ¥å­¦ä¹ è¯¯å·®ç›¸å¯¹äºå‚æ•° Uã€V å’Œ W çš„æ¢¯åº¦ã€‚
- en: The difficulty of training RNN to capture long-term dependencies has been studied
    in [[26](#bib.bib26)]. To address the issue of learning long-term dependencies,
    Hochreiter and Schmidhuber [[139](#bib.bib139)] proposed Long Short-Term Memory
    (LSTM), which can maintain a separate memory cell inside it that updates and exposes
    its content only when deemed necessary. Recently, a Gated Recurrent Unit (GRU)
    was proposed by [[51](#bib.bib51)] to make each recurrent unit adaptively capture
    dependencies of different time scales. Like the LSTM unit, the GRU has gating
    units that modulate the flow of information inside the unit but without having
    separate memory cells.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒ RNN ä»¥æ•æ‰é•¿æœŸä¾èµ–æ€§çš„éš¾åº¦å·²åœ¨[[26](#bib.bib26)]ä¸­ç ”ç©¶è¿‡ã€‚ä¸ºäº†åº”å¯¹å­¦ä¹ é•¿æœŸä¾èµ–æ€§çš„é—®é¢˜ï¼ŒHochreiter å’Œ Schmidhuber
    [[139](#bib.bib139)] æå‡ºäº†é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ï¼Œå®ƒå¯ä»¥åœ¨å†…éƒ¨ç»´æŒä¸€ä¸ªç‹¬ç«‹çš„è®°å¿†å•å…ƒï¼Œä»…åœ¨å¿…è¦æ—¶æ›´æ–°å¹¶æš´éœ²å…¶å†…å®¹ã€‚æœ€è¿‘ï¼Œ[[51](#bib.bib51)]
    æå‡ºäº†é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ï¼Œä½¿æ¯ä¸ªå¾ªç¯å•å…ƒèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ•æ‰ä¸åŒæ—¶é—´å°ºåº¦çš„ä¾èµ–æ€§ã€‚ä¸ LSTM å•å…ƒç±»ä¼¼ï¼ŒGRU å…·æœ‰é—¨æ§å•å…ƒæ¥è°ƒèŠ‚ä¿¡æ¯åœ¨å•å…ƒå†…éƒ¨çš„æµåŠ¨ï¼Œä½†æ²¡æœ‰ç‹¬ç«‹çš„è®°å¿†å•å…ƒã€‚
- en: Several variants of RNN have been later introduced and successfully applied
    to wide variety of tasks, such as natural language processing [[257](#bib.bib257)],
    [[214](#bib.bib214)], speech recognition [[115](#bib.bib115)], [[54](#bib.bib54)],
    machine translation [[175](#bib.bib175)], [[241](#bib.bib241)], question answering
    [[138](#bib.bib138)], image captioning [[247](#bib.bib247)], [[78](#bib.bib78)],
    and many more.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åæ¥ä»‹ç»äº†å‡ ç§ RNN çš„å˜ä½“ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç† [[257](#bib.bib257)]ã€[[214](#bib.bib214)]ã€è¯­éŸ³è¯†åˆ«
    [[115](#bib.bib115)]ã€[[54](#bib.bib54)]ã€æœºå™¨ç¿»è¯‘ [[175](#bib.bib175)]ã€[[241](#bib.bib241)]ã€é—®ç­”
    [[138](#bib.bib138)]ã€å›¾åƒæè¿° [[247](#bib.bib247)]ã€[[78](#bib.bib78)] ç­‰ã€‚
- en: 3 Basics of Reinforcement Learning
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 å¼ºåŒ–å­¦ä¹ åŸºç¡€
- en: This section serves as a brief introduction to the theoretical models and techniques
    in RL. In order to provide a quick overview of what constitutes the main components
    of RL methods, some fundamental concepts and major theoretical problems are also
    clarified. RL is a kind of machine learning method where agents learn the optimal
    policy by trial and error. Unlike supervised learning, the feedback is available
    after each system action, it is simply a scalar value that may be delayed in time
    in RL framework, for example, the success or failure of the entire system is reflected
    after a sequence of actions. Furthermore, the supervised learning model is updated
    based on the loss/error of the output and there is no mechanism to get the correct
    value when it is wrong. This is addressed by policy gradients in RL by assigning
    gradients without a differentiable loss function which aims at teaching a model
    to try things out randomly and learn to do correct things more.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ç®€è¦ä»‹ç»äº† RL ä¸­çš„ç†è®ºæ¨¡å‹å’ŒæŠ€æœ¯ã€‚ä¸ºäº†æä¾› RL æ–¹æ³•ä¸»è¦ç»„æˆéƒ¨åˆ†çš„å¿«é€Ÿæ¦‚è¿°ï¼ŒæŸäº›åŸºæœ¬æ¦‚å¿µå’Œä¸»è¦ç†è®ºé—®é¢˜ä¹Ÿå¾—åˆ°æ¾„æ¸…ã€‚RL æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå…¶ä¸­ä»£ç†é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚ä¸ç›‘ç£å­¦ä¹ ä¸åŒï¼Œåé¦ˆåœ¨æ¯æ¬¡ç³»ç»ŸåŠ¨ä½œåå¯ç”¨ï¼Œåœ¨
    RL æ¡†æ¶ä¸­ï¼Œåé¦ˆåªæ˜¯ä¸€ä¸ªå¯èƒ½ä¼šå»¶è¿Ÿçš„æ ‡é‡å€¼ï¼Œä¾‹å¦‚ï¼Œæ•´ä¸ªç³»ç»Ÿçš„æˆåŠŸæˆ–å¤±è´¥æ˜¯åœ¨ä¸€ç³»åˆ—åŠ¨ä½œåä½“ç°çš„ã€‚æ­¤å¤–ï¼Œç›‘ç£å­¦ä¹ æ¨¡å‹æ˜¯åŸºäºè¾“å‡ºçš„æŸå¤±/é”™è¯¯è¿›è¡Œæ›´æ–°çš„ï¼Œå¹¶ä¸”æ²¡æœ‰æœºåˆ¶åœ¨å‡ºé”™æ—¶è·å–æ­£ç¡®çš„å€¼ã€‚RL
    ä¸­çš„ç­–ç•¥æ¢¯åº¦é€šè¿‡åˆ†é…æ¢¯åº¦è€Œä¸ä½¿ç”¨å¯å¾®åˆ†çš„æŸå¤±å‡½æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™æ—¨åœ¨æ•™å¯¼æ¨¡å‹éšæœºå°è¯•ï¼Œå¹¶å­¦ä¹ åšæ­£ç¡®çš„äº‹æƒ…æ›´å¤šã€‚
- en: Inspired by behavioral psychology, RL was proposed to address the sequential
    decision-making problems which exist in many applications such as games, robotics,
    healthcare, smart grids, stock, autonomous driving, etc. Unlike supervised learning
    where the data is given, an artificial agent collects experiences (data) by interacting
    with its environment in RL framework. Such experience is then gathered to optimize
    the cumulative rewards/utilities.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å—è¡Œä¸ºå¿ƒç†å­¦å¯å‘ï¼ŒRL è¢«æå‡ºä»¥è§£å†³è®¸å¤šåº”ç”¨ä¸­å­˜åœ¨çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œå¦‚æ¸¸æˆã€æœºå™¨äººã€åŒ»ç–—ä¿å¥ã€æ™ºèƒ½ç”µç½‘ã€è‚¡ç¥¨ã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚ä¸ç»™å®šæ•°æ®çš„ç›‘ç£å­¦ä¹ ä¸åŒï¼Œäººå·¥ä»£ç†é€šè¿‡ä¸ç¯å¢ƒäº¤äº’åœ¨
    RL æ¡†æ¶ä¸­æ”¶é›†ç»éªŒï¼ˆæ•°æ®ï¼‰ã€‚ç„¶åæ”¶é›†è¿™äº›ç»éªŒä»¥ä¼˜åŒ–ç´¯ç§¯å¥–åŠ±/æ•ˆç”¨ã€‚
- en: 'In this section, we focus on how the RL problem can be formalized as an agent
    that can make decisions in an environment to optimize some objectives presented
    under reward functions. Some key aspects of RL are: (i) Address the sequential
    decision making; (ii) There is no supervisor, only a reward presented as scalar
    number; and (iii) The feedback is highly delayed. Markov Decision Process (MDP)
    is a framework that has commonly been used to solve most RL problems with discrete
    actions, thus we will first discuss MDP in this section. We then introduce value
    function and how to categorize RL into model-based or model-free methods. At the
    end of this section, we discuss some challenges in RL.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹è®¨è®º RL é—®é¢˜å¦‚ä½•è¢«å½¢å¼åŒ–ä¸ºä¸€ä¸ªèƒ½å¤Ÿåœ¨ç¯å¢ƒä¸­åšå‡ºå†³ç­–ä»¥ä¼˜åŒ–åœ¨å¥–åŠ±å‡½æ•°ä¸‹å‘ˆç°çš„ä¸€äº›ç›®æ ‡çš„ä»£ç†ã€‚RL çš„ä¸€äº›å…³é”®æ–¹é¢æ˜¯ï¼š(i) è§£å†³åºåˆ—å†³ç­–é—®é¢˜ï¼›(ii)
    æ²¡æœ‰ç›‘ç£è€…ï¼Œåªæœ‰ä»¥æ ‡é‡æ•°å­—å‘ˆç°çš„å¥–åŠ±ï¼›(iii) åé¦ˆé«˜åº¦å»¶è¿Ÿã€‚é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šå¸¸ç”¨äºè§£å†³å¤§å¤šæ•°å…·æœ‰ç¦»æ•£åŠ¨ä½œçš„ RL é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†é¦–å…ˆè®¨è®º
    MDPã€‚åœ¨æœ¬èŠ‚ç»“æŸæ—¶ï¼Œæˆ‘ä»¬ä»‹ç»å€¼å‡½æ•°ä»¥åŠå¦‚ä½•å°† RL åˆ†ç±»ä¸ºåŸºäºæ¨¡å‹æˆ–æ— æ¨¡å‹çš„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®º RL ä¸­çš„ä¸€äº›æŒ‘æˆ˜ã€‚
- en: '![Refer to caption](img/878eca4dbadcd3f5807e57db9d9a5ff4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/878eca4dbadcd3f5807e57db9d9a5ff4.png)'
- en: 'Figure 5: An illustration of agent-environment interaction in RL'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šRL ä¸­ä»£ç†ä¸ç¯å¢ƒäº¤äº’çš„ç¤ºæ„å›¾
- en: 3.1 Markov Decision Process
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹
- en: 'The standard theory of RL is defined by a Markov Decision Process (MDP), which
    is an extension of the Markov process (also known as the Markov chain). Mathematically,
    the Markov process is a discrete-time stochastic process whose conditional probability
    distribution of the future states only depends upon the present state and it provides
    a framework to model decision-making situations. An MDP is typically defined by
    five elements as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: RL çš„æ ‡å‡†ç†è®ºç”±é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å®šä¹‰ï¼Œå®ƒæ˜¯é©¬å°”å¯å¤«è¿‡ç¨‹ï¼ˆä¹Ÿç§°ä¸ºé©¬å°”å¯å¤«é“¾ï¼‰çš„æ‰©å±•ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œé©¬å°”å¯å¤«è¿‡ç¨‹æ˜¯ä¸€ç§ç¦»æ•£æ—¶é—´éšæœºè¿‡ç¨‹ï¼Œå…¶æœªæ¥çŠ¶æ€çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒä»…ä¾èµ–äºå½“å‰çŠ¶æ€ï¼Œå¹¶ä¸”å®ƒæä¾›äº†å»ºæ¨¡å†³ç­–æƒ…å¢ƒçš„æ¡†æ¶ã€‚ä¸€ä¸ª
    MDP é€šå¸¸ç”±ä»¥ä¸‹äº”ä¸ªå…ƒç´ å®šä¹‰ï¼š
- en: â€¢
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$S$: a set of state or observation space of an environment. $s_{0}$ is starting
    state.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S$ï¼šç¯å¢ƒçš„çŠ¶æ€æˆ–è§‚å¯Ÿç©ºé—´é›†åˆã€‚$s_{0}$ æ˜¯èµ·å§‹çŠ¶æ€ã€‚
- en: â€¢
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$\mathcal{A}$: set of actions the agent can choose.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{A}$ï¼šä»£ç†å¯ä»¥é€‰æ‹©çš„åŠ¨ä½œé›†ã€‚
- en: â€¢
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$T$: a transition probability function $T(s_{t+1}|s_{t},a_{t})$, specifying
    the probability that the environment will transition to state $s_{t+1}\in S$ if
    the agent takes action $a_{t}\in\mathcal{A}$ in state $s_{t}\in S$.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$T$: è½¬ç§»æ¦‚ç‡å‡½æ•° $T(s_{t+1}|s_{t},a_{t})$ï¼ŒæŒ‡å®šå¦‚æœä»£ç†åœ¨çŠ¶æ€ $s_{t}\in S$ ä¸‹é‡‡å–åŠ¨ä½œ $a_{t}\in\mathcal{A}$ï¼Œç¯å¢ƒå°†ä»¥æ¦‚ç‡è½¬ç§»åˆ°çŠ¶æ€
    $s_{t+1}\in S$ã€‚'
- en: â€¢
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$R$: a reward function where $r_{t+1}=R(s_{t},s_{t+1})$ is a reward received
    for taking action $a_{t}$ at state $s_{t}$ and transfer to the next state $s_{t+1}$.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$R$: å¥–åŠ±å‡½æ•°ï¼Œå…¶ä¸­ $r_{t+1}=R(s_{t},s_{t+1})$ æ˜¯åœ¨çŠ¶æ€ $s_{t}$ ä¸‹é‡‡å–åŠ¨ä½œ $a_{t}$ å¹¶è½¬ç§»åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€
    $s_{t+1}$ æ—¶è·å¾—çš„å¥–åŠ±ã€‚'
- en: â€¢
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$\gamma$: a discount factor.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\gamma$: æŠ˜æ‰£å› å­ã€‚'
- en: 'Considering MDP($S$, $\mathcal{A}$, $\gamma$, $T$, $R$), the agent chooses
    an action $a_{t}$ according to the policy $\pi(a_{t}|s_{t})$ at state $s_{t}$.
    Notably, agentâ€™s algorithm for choosing action $a$ given current state $s$, which
    in general can be viewed as distribution $\pi(a|s)$, is called a policy (strategy).
    The environment receives the action, produces a reward $r_{t+1}$ and transfers
    to the next state $s_{t+1}$ according to the transition probability $T(s_{t+1}|s_{t},a_{t})$.
    The process continues until the agent reaches a terminal state or a maximum time
    step. In RL framework, the tuple $(s_{t},a_{t},r_{t+1},s_{t+1})$ is called transition.
    Several sequential transitions are usually referred to as roll-out. Full sequence
    $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$ is called a trajectory. Theoretically,
    trajectory is infinitely long, but the episodic property holds in most practical
    cases. One trajectory of some finite length $\tau$ is called an episode. For given
    MDP and policy $\pi$, the probability of observing $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$
    is called trajectory distribution and is denoted as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ MDP($S$, $\mathcal{A}$, $\gamma$, $T$, $R$)ï¼Œä»£ç†æ ¹æ®ç­–ç•¥ $\pi(a_{t}|s_{t})$ åœ¨çŠ¶æ€
    $s_{t}$ ä¸‹é€‰æ‹©åŠ¨ä½œ $a_{t}$ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»£ç†åœ¨ç»™å®šå½“å‰çŠ¶æ€ $s$ æ—¶é€‰æ‹©åŠ¨ä½œ $a$ çš„ç®—æ³•ï¼Œé€šå¸¸å¯ä»¥è§†ä¸ºåˆ†å¸ƒ $\pi(a|s)$ï¼Œç§°ä¸ºç­–ç•¥ï¼ˆç­–ç•¥ï¼‰ã€‚ç¯å¢ƒæ¥æ”¶åˆ°åŠ¨ä½œï¼Œäº§ç”Ÿå¥–åŠ±
    $r_{t+1}$ å¹¶æ ¹æ®è½¬ç§»æ¦‚ç‡ $T(s_{t+1}|s_{t},a_{t})$ è½¬ç§»åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ $s_{t+1}$ã€‚è¯¥è¿‡ç¨‹ç»§ç»­ï¼Œç›´åˆ°ä»£ç†åˆ°è¾¾ç»ˆæ­¢çŠ¶æ€æˆ–è¾¾åˆ°æœ€å¤§æ—¶é—´æ­¥ã€‚åœ¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼Œå…ƒç»„
    $(s_{t},a_{t},r_{t+1},s_{t+1})$ ç§°ä¸ºè½¬ç§»ã€‚å¤šä¸ªé¡ºåºè½¬ç§»é€šå¸¸è¢«ç§°ä¸ºå›æ»šã€‚å®Œæ•´çš„åºåˆ— $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$
    è¢«ç§°ä¸ºè½¨è¿¹ã€‚ç†è®ºä¸Šï¼Œè½¨è¿¹æ˜¯æ— é™é•¿çš„ï¼Œä½†åœ¨å¤§å¤šæ•°å®é™…æƒ…å†µä¸‹å…·æœ‰ episodic å±æ€§ã€‚æŸä¸ªæœ‰é™é•¿åº¦ $\tau$ çš„è½¨è¿¹ç§°ä¸ºä¸€ä¸ªå›åˆã€‚å¯¹äºç»™å®šçš„ MDP å’Œç­–ç•¥
    $\pi$ï¼Œè§‚å¯Ÿåˆ° $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$ çš„æ¦‚ç‡ç§°ä¸ºè½¨è¿¹åˆ†å¸ƒï¼Œè®°ä½œï¼š
- en: '|  | $\mathcal{T}_{\pi}=\prod_{t}{\pi(a_{t}&#124;s_{t})T(s_{t+1}&#124;s_{t},a_{t})}$
    |  | (5) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{T}_{\pi}=\prod_{t}{\pi(a_{t}|s_{t})T(s_{t+1}|s_{t},a_{t})}$
    |  | (5) |'
- en: 'The objective of RL is to find the optimal policy $\pi^{*}$ for the agent that
    maximizes the cumulative reward, which is called return. For every episode, the
    return is defined as the weighted sum of immediate rewards:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¸ºä»£ç†æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ $\pi^{*}$ï¼Œè¯¥ç­–ç•¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œè¿™è¢«ç§°ä¸ºå›æŠ¥ã€‚å¯¹äºæ¯ä¸ªå›åˆï¼Œå›æŠ¥è¢«å®šä¹‰ä¸ºå³æ—¶å¥–åŠ±çš„åŠ æƒå’Œï¼š
- en: '|  | $\mathcal{R}=\sum_{t=0}^{\tau-1}{\gamma^{t}r_{t+1}}$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}=\sum_{t=0}^{\tau-1}{\gamma^{t}r_{t+1}}$ |  | (6) |'
- en: 'Because the policy induces a trajectory distribution, the expected reward maximization
    can be written as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºç­–ç•¥ä¼šå¼•å‘ä¸€ä¸ªè½¨è¿¹åˆ†å¸ƒï¼Œæ‰€ä»¥æœŸæœ›å¥–åŠ±æœ€å¤§åŒ–å¯ä»¥å†™ä½œï¼š
- en: '|  | $\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}{r_{t+1}}\rightarrow\max_{\pi}$
    |  | (7) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}{r_{t+1}}\rightarrow\max_{\pi}$
    |  | (7) |'
- en: 'Thus, given MDP and policy $\pi$, the discounted expected reward is defined:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œç»™å®š MDP å’Œç­–ç•¥ $\pi$ï¼ŒæŠ˜æ‰£æœŸæœ›å¥–åŠ±è¢«å®šä¹‰ä¸ºï¼š
- en: '|  | $\mathcal{G}(\pi)=\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}\gamma^{t}{r_{t+1}}$
    |  | (8) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}(\pi)=\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}\gamma^{t}{r_{t+1}}$
    |  | (8) |'
- en: The goal of RL is to find an optimal policy $\pi^{*}$, which maximizes the discounted
    expected reward, i.e. $\mathcal{G}(\pi)\rightarrow\max_{\pi}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ $\pi^{*}$ï¼Œä½¿æŠ˜æ‰£æœŸæœ›å¥–åŠ±æœ€å¤§åŒ–ï¼Œå³ $\mathcal{G}(\pi)\rightarrow\max_{\pi}$ã€‚
- en: 3.2 Value and Q- functions
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 ä»·å€¼å‡½æ•°å’Œ Q-å‡½æ•°
- en: 'The value function is applied to evaluate how good it is for an agent to utilize
    policy $\pi$ to visit state $s$. The concept of â€goodâ€ is defined in terms of
    expected return, i.e. future rewards that can be expected to receive in the future
    and it depends on what actions it will take. Mathematically, the value is the
    expectation of return, and value approximation is obtained by Bellman expectation
    equation as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä»·å€¼å‡½æ•°ç”¨äºè¯„ä¼°ä»£ç†åˆ©ç”¨ç­–ç•¥ $\pi$ è®¿é—®çŠ¶æ€ $s$ çš„å¥½åã€‚è¿™é‡Œçš„â€œå¥½â€æ˜¯é€šè¿‡æœŸæœ›å›æŠ¥æ¥å®šä¹‰çš„ï¼Œå³å¯ä»¥é¢„æœŸåœ¨æœªæ¥æ”¶åˆ°çš„å¥–åŠ±ï¼Œå®ƒå–å†³äºå°†é‡‡å–çš„è¡ŒåŠ¨ã€‚æ•°å­¦ä¸Šï¼Œä»·å€¼æ˜¯å›æŠ¥çš„æœŸæœ›å€¼ï¼Œä»·å€¼çš„è¿‘ä¼¼é€šè¿‡
    Bellman æœŸæœ›æ–¹ç¨‹å¾—åˆ°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '|  | $\begin{split}V^{\pi}(s_{t})=\mathbb{E}[r_{t+1}+\gamma V^{\pi}(s_{t+1})]\end{split}$
    |  | (9) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '$V^{\pi}(s_{t})$ is also known as state-value function, and the expectation
    term can be expanded as a product of policy, transition probability, and return
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}V^{\pi}(s_{t})=\sum_{a_{t}\in\mathcal{A}}{\pi(a_{t}&#124;s_{t})}\sum_{s_{t+1}\in
    S}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1})]}\end{split}$
    |  | (10) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: 'This equation is called the Bellman equation. When the agent always selects
    the action according to the optimal policy $\pi^{*}$ that maximizes the value,
    the Bellman equation can be expressed as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}V^{*}(s_{t})=&amp;\max_{a_{t}}\sum_{s_{t+1}\in S}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma
    V^{*}(s_{t+1})]}\\ \overset{\Delta}{=}&amp;\max_{a_{t}}Q^{*}(s_{t},a_{t})\end{split}$
    |  | (11) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: 'However, obtaining optimal value function $V^{*}$ does not provide enough information
    to reconstruct some optimal policy $\pi^{*}$ because the real-world environment
    is complicated. Thus, a quality function (Q-function) is also called the action-value
    function under policy $\pi$. The Q-function is used to estimate how good it is
    for an agent to perform a particular action ($a_{t}$) in a state ($s_{t}$) with
    a policy $\pi$ and it is introduced as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q^{\pi}(s_{t},a_{t})=\sum_{s_{t+1}}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma
    V^{\pi}(s_{t+1})]}$ |  | (12) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: Unlike value function which specifies the goodness of a state, a Q-function
    specifies the goodness of action in a state.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Category
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In general, RL can be divided into either model-free or model-based methods.
    Here, â€modelâ€ is defined by the two quantity: transition probability function
    $T(s_{t+1}|s_{t},a_{t})$ and the reward function $R(s_{t},s_{t+1})$.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Model-based RL
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Model-based RL is an approach that uses a learnt model, i.e. $T(s_{t+1}|s_{t},a_{t})$
    and reward function $R(s_{t},s_{t+1})$ to predict the future action. There are
    four main model-based techniques as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Value Function: The objective of value function methods is to obtain the best
    policy by maximizing the value functions in each state. A value function of a
    RL problem can be defined as in Eq.[10](#S3.E10 "In 3.2 Value and Q- functions
    â€£ 3 Basics of Reinforcement Learning â€£ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey") and the optimal state-value function is given
    in Eq.[11](#S3.E11 "In 3.2 Value and Q- functions â€£ 3 Basics of Reinforcement
    Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    which are known as Bellman equations. Some common approaches in this group are
    Differential Dynamic Programming [[208](#bib.bib208)], [[266](#bib.bib266)], Temporal
    Difference Learning [[249](#bib.bib249)], Policy Iteration [[334](#bib.bib334)]
    and Monte Carlo [[137](#bib.bib137)].'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transition Models: Transition models decide how to map from a state s, taking
    action a to the next state (sâ€™) and it strongly affects the performance of model-based
    RL algorithms. Based on whether predicting the future state sâ€™ is based on the
    probability distribution of a random variable or not, there are two main approaches
    in this group: stochastic and deterministic. Some common methods for deterministic
    models are decision trees [[280](#bib.bib280)] and linear regression [[265](#bib.bib265)].
    Some common methods for stochastic models are Gaussian processes [[71](#bib.bib71)],
    [[1](#bib.bib1)], [[12](#bib.bib12)], Expectation-Maximization [[59](#bib.bib59)]
    and dynamic Bayesian networks [[280](#bib.bib280)].'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è½¬ç§»æ¨¡å‹ï¼šè½¬ç§»æ¨¡å‹å†³å®šäº†å¦‚ä½•ä»çŠ¶æ€sï¼Œé‡‡å–åŠ¨ä½œaåˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼ˆsâ€™ï¼‰ï¼Œå®ƒå¯¹åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½æœ‰ç€é‡è¦å½±å“ã€‚æ ¹æ®é¢„æµ‹æœªæ¥çŠ¶æ€sâ€™æ˜¯å¦åŸºäºéšæœºå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¸ªç»„åˆ«ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•ï¼šéšæœºå’Œç¡®å®šæ€§ã€‚ä¸€äº›å¸¸è§çš„ç¡®å®šæ€§æ¨¡å‹æ–¹æ³•æœ‰å†³ç­–æ ‘
    [[280](#bib.bib280)] å’Œçº¿æ€§å›å½’ [[265](#bib.bib265)]ã€‚ä¸€äº›å¸¸è§çš„éšæœºæ¨¡å‹æ–¹æ³•æœ‰é«˜æ–¯è¿‡ç¨‹ [[71](#bib.bib71)]ã€[[1](#bib.bib1)]ã€[[12](#bib.bib12)]ã€æœŸæœ›æœ€å¤§åŒ–
    [[59](#bib.bib59)] å’ŒåŠ¨æ€è´å¶æ–¯ç½‘ç»œ [[280](#bib.bib280)]ã€‚
- en: â€¢
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Policy Search: Policy search approach directly searches for the optimal policy
    by modifying its parameters, whereas the value function methods indirectly find
    the actions that maximize the value function at each state. Some of the popular
    approaches in this group are: gradient-based [[87](#bib.bib87)], [[267](#bib.bib267)],
    information theory [[1](#bib.bib1)], [[189](#bib.bib189)] and sampling based [[21](#bib.bib21)].'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç­–ç•¥æœç´¢ï¼šç­–ç•¥æœç´¢æ–¹æ³•é€šè¿‡ä¿®æ”¹å‚æ•°ç›´æ¥æœç´¢æœ€ä¼˜ç­–ç•¥ï¼Œè€Œä»·å€¼å‡½æ•°æ–¹æ³•åˆ™é—´æ¥æ‰¾åˆ°åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹æœ€å¤§åŒ–ä»·å€¼å‡½æ•°çš„åŠ¨ä½œã€‚è¯¥ç»„åˆ«ä¸­çš„ä¸€äº›æµè¡Œæ–¹æ³•æœ‰ï¼šåŸºäºæ¢¯åº¦çš„ [[87](#bib.bib87)]ã€[[267](#bib.bib267)]ã€ä¿¡æ¯ç†è®º
    [[1](#bib.bib1)]ã€[[189](#bib.bib189)] å’ŒåŸºäºé‡‡æ ·çš„ [[21](#bib.bib21)]ã€‚
- en: â€¢
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Return Functions: Return functions decide how to aggregate rewards or punishments
    over an episode. They affect both the convergence and the feasibility of the model.
    There are two main approaches in this group: discounted returns functions [[21](#bib.bib21)],
    [[75](#bib.bib75)], [[393](#bib.bib393)] and averaged returns functions [[34](#bib.bib34)],
    [[3](#bib.bib3)]. Between the two approaches, the former is the most popular which
    represents the uncertainty about future rewards. While small discount factors
    provide faster convergence, its solution may not be optimal.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å›æŠ¥å‡½æ•°ï¼šå›æŠ¥å‡½æ•°å†³å®šäº†å¦‚ä½•åœ¨ä¸€ä¸ªå›åˆä¸­æ±‡æ€»å¥–åŠ±æˆ–æƒ©ç½šã€‚å®ƒä»¬å½±å“æ¨¡å‹çš„æ”¶æ•›æ€§å’Œå¯è¡Œæ€§ã€‚è¿™ä¸ªç»„åˆ«ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•ï¼šæŠ˜æ‰£å›æŠ¥å‡½æ•° [[21](#bib.bib21)]ã€[[75](#bib.bib75)]ã€[[393](#bib.bib393)]
    å’Œå¹³å‡å›æŠ¥å‡½æ•° [[34](#bib.bib34)]ã€[[3](#bib.bib3)]ã€‚åœ¨è¿™ä¸¤ç§æ–¹æ³•ä¸­ï¼Œå‰è€…æ›´ä¸ºæµè¡Œï¼Œå®ƒè¡¨ç¤ºå¯¹æœªæ¥å¥–åŠ±çš„ä¸ç¡®å®šæ€§ã€‚è™½ç„¶å°çš„æŠ˜æ‰£å› å­æä¾›äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œä½†å…¶è§£å¯èƒ½å¹¶ä¸æ˜¯æœ€ä¼˜çš„ã€‚
- en: 'In practice, transition and reward functions are rarely known and hard to model.
    The comparative performance among all model-based techniques is reported in [[385](#bib.bib385)]
    with over 18 benchmarking environments including noisy environments. The Fig.[6](#S3.F6
    "Figure 6 â€£ 3.3.2 Model-free methods â€£ 3.3 Category â€£ 3 Basics of Reinforcement
    Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    summarizes different model-based RL approaches.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'å®é™…ä¸­ï¼Œè½¬ç§»å’Œå¥–åŠ±å‡½æ•°å¾ˆå°‘è¢«çŸ¥é“ä¸”éš¾ä»¥å»ºæ¨¡ã€‚æ‰€æœ‰åŸºäºæ¨¡å‹çš„æŠ€æœ¯çš„æ¯”è¾ƒæ€§èƒ½å·²åœ¨ [[385](#bib.bib385)] ä¸­æŠ¥å‘Šï¼ŒåŒ…æ‹¬è¶…è¿‡18ä¸ªåŸºå‡†ç¯å¢ƒï¼ˆåŒ…æ‹¬å™ªå£°ç¯å¢ƒï¼‰ã€‚å›¾[6](#S3.F6
    "Figure 6 â€£ 3.3.2 Model-free methods â€£ 3.3 Category â€£ 3 Basics of Reinforcement
    Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    æ€»ç»“äº†ä¸åŒçš„åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚'
- en: 3.3.2 Model-free methods
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 æ— æ¨¡å‹æ–¹æ³•
- en: Learning through the experience gained from interactions with the environment,
    i.e. model-free method tries to estimate the t. discrete problems transition probability
    function and the reward function from the experience to exploit them in acquisition
    of policy. Policy gradient and value-based algorithms are popularly used in model-free
    methods.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸ç¯å¢ƒäº’åŠ¨è·å¾—çš„ç»éªŒè¿›è¡Œå­¦ä¹ ï¼Œå³æ— æ¨¡å‹æ–¹æ³•è¯•å›¾ä»ç»éªŒä¸­ä¼°è®¡ç¦»æ•£é—®é¢˜çš„è½¬ç§»æ¦‚ç‡å‡½æ•°å’Œå¥–åŠ±å‡½æ•°ï¼Œä»¥åˆ©ç”¨å®ƒä»¬æ¥è·å–ç­–ç•¥ã€‚ç­–ç•¥æ¢¯åº¦å’ŒåŸºäºä»·å€¼çš„ç®—æ³•åœ¨æ— æ¨¡å‹æ–¹æ³•ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚
- en: â€¢
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'The policy gradient methods: In this approach, RL task is considered as optimization
    with stochastic first-order optimization. Policy gradient methods directly optimize
    the discounted expected reward, i.e. $\mathcal{G}(\pi)\rightarrow\max_{\pi}$ to
    obtains the optimal policy $\pi^{*}$ without any additional information about
    MDP. To do so, approximate estimations of the gradient with respect to policy
    parameters are used. Take [[392](#bib.bib392)] as an example, policy gradient
    parameterizes the policy and updates parameters $\theta$,'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼šåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡è¢«è§†ä¸ºå¸¦æœ‰éšæœºä¸€é˜¶ä¼˜åŒ–çš„ä¼˜åŒ–é—®é¢˜ã€‚ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç›´æ¥ä¼˜åŒ–æŠ˜æ‰£æœŸæœ›å›æŠ¥ï¼Œå³ $\mathcal{G}(\pi)\rightarrow\max_{\pi}$
    ä»¥è·å¾—æœ€ä¼˜ç­–ç•¥ $\pi^{*}$ï¼Œè€Œæ— éœ€ä»»ä½•æœ‰å…³ MDP çš„é¢å¤–ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨ç›¸å¯¹äºç­–ç•¥å‚æ•°çš„æ¢¯åº¦çš„è¿‘ä¼¼ä¼°è®¡ã€‚ä»¥ [[392](#bib.bib392)]
    ä¸ºä¾‹ï¼Œç­–ç•¥æ¢¯åº¦å‚æ•°åŒ–ç­–ç•¥å¹¶æ›´æ–°å‚æ•° $\theta$ï¼Œ
- en: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\mathcal{R}}$
    |  | (13) |'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\mathcal{R}}$
    |  | (13) |'
- en: 'where $\mathcal{R}$ is the total accumulated return and defined in Eq. [6](#S3.E6
    "In 3.1 Markov Decision Process â€£ 3 Basics of Reinforcement Learning â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). Common used policies are
    Gibbs policies [[20](#bib.bib20)], [[352](#bib.bib352)] and Gaussian policies
    [[294](#bib.bib294)]. Gibbs policies are used in discrete problems whereas Gaussian
    policies are used in continuous problems.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathcal{R}$ æ˜¯æ€»ç´¯è®¡å›æŠ¥ï¼Œå¦‚å…¬å¼ [6](#S3.E6 "åœ¨ 3.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ â€£ 3 å¼ºåŒ–å­¦ä¹ åŸºç¡€ â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢è°ƒæŸ¥")
    æ‰€å®šä¹‰ã€‚å¸¸ç”¨çš„ç­–ç•¥æœ‰ Gibbs ç­–ç•¥ [[20](#bib.bib20)]ã€[[352](#bib.bib352)] å’Œé«˜æ–¯ç­–ç•¥ [[294](#bib.bib294)]ã€‚Gibbs
    ç­–ç•¥ç”¨äºç¦»æ•£é—®é¢˜ï¼Œè€Œé«˜æ–¯ç­–ç•¥ç”¨äºè¿ç»­é—®é¢˜ã€‚
- en: â€¢
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Value-based methods: In this approach, the optimal policy $\pi^{*}$ is implicitly
    conducted by gaining an approximation of optimal Q-function $Q^{*}(s,a)$. In value-based
    methods, agents update the value function to learn suitable policy while policy-based
    RL agents learn the policy directly. To do that, Q-learning is a typical value-based
    method. The update rule of Q-learning with learning rate $\lambda$ is defined
    as:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºäºä»·å€¼çš„æ–¹æ³•ï¼šåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œé€šè¿‡è·å¾—æœ€ä¼˜ Q å‡½æ•° $Q^{*}(s,a)$ çš„è¿‘ä¼¼æ¥éšå¼åœ°å®ç°æœ€ä¼˜ç­–ç•¥ $\pi^{*}$ã€‚åœ¨åŸºäºä»·å€¼çš„æ–¹æ³•ä¸­ï¼Œæ™ºèƒ½ä½“æ›´æ–°ä»·å€¼å‡½æ•°ä»¥å­¦ä¹ åˆé€‚çš„ç­–ç•¥ï¼Œè€ŒåŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ç›´æ¥å­¦ä¹ ç­–ç•¥ã€‚ä¸ºæ­¤ï¼ŒQ
    å­¦ä¹ æ˜¯å…¸å‹çš„åŸºäºä»·å€¼çš„æ–¹æ³•ã€‚Q å­¦ä¹ çš„æ›´æ–°è§„åˆ™ä¸å­¦ä¹ ç‡ $\lambda$ å®šä¹‰ä¸ºï¼š
- en: '|  | $Q(s_{t},a_{t})=Q(s_{t},a_{t})+\lambda\delta_{t}$ |  | (14) |'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})=Q(s_{t},a_{t})+\lambda\delta_{t}$ |  | (14) |'
- en: where $\delta_{t}=R(s_{t},s_{t+1})+\gamma\text{arg}\max_{a}{Q(s_{t+1},a)-Q(s_{t},a)}$
    is the temporal difference (TD) error.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\delta_{t}=R(s_{t},s_{t+1})+\gamma\text{arg}\max_{a}{Q(s_{t+1},a)-Q(s_{t},a)}$
    æ˜¯æ—¶åºå·®åˆ†ï¼ˆTDï¼‰è¯¯å·®ã€‚
- en: Target at self-play Chess, [[394](#bib.bib394)] investigates inasmuch it is
    possible to leverage the qualitative feedback for learning an evaluation function
    for the game. [[319](#bib.bib319)] provides the comparison of learning of linear
    evaluation functions between using preference learning and using least-squares
    temporal difference learning, from samples of game trajectories. The value-based
    methods depend on a specific, optimal policy, thus it is hard for transfer learning.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»¥è‡ªæˆ‘åšå¼ˆè±¡æ£‹ä¸ºç›®æ ‡ï¼Œ[[394](#bib.bib394)] ç ”ç©¶äº†æ˜¯å¦å¯ä»¥åˆ©ç”¨å®šæ€§åé¦ˆæ¥å­¦ä¹ æ¸¸æˆçš„è¯„ä¼°å‡½æ•°ã€‚[[319](#bib.bib319)]
    æä¾›äº†ä½¿ç”¨åå¥½å­¦ä¹ å’Œä½¿ç”¨æœ€å°äºŒä¹˜æ—¶åºå·®åˆ†å­¦ä¹ åœ¨æ¸¸æˆè½¨è¿¹æ ·æœ¬ä¸­å­¦ä¹ çº¿æ€§è¯„ä¼°å‡½æ•°çš„æ¯”è¾ƒã€‚åŸºäºä»·å€¼çš„æ–¹æ³•ä¾èµ–äºç‰¹å®šçš„æœ€ä¼˜ç­–ç•¥ï¼Œå› æ­¤è½¬ç§»å­¦ä¹ è¾ƒä¸ºå›°éš¾ã€‚
- en: â€¢
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Actor-critic is an improvement of policy gradient with an value-based critic
    $\Gamma$, thus, Eq.[13](#S3.E13 "In 1st item â€£ 3.3.2 Model-free methods â€£ 3.3
    Category â€£ 3 Basics of Reinforcement Learning â€£ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey") is rewritten as:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Actor-critic æ˜¯ä¸€ç§æ”¹è¿›çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå…·æœ‰åŸºäºä»·å€¼çš„è¯„è®ºå‘˜ $\Gamma$ï¼Œå› æ­¤ï¼Œå…¬å¼ [13](#S3.E13 "åœ¨ç¬¬ 1 é¡¹ â€£ 3.3.2
    æ— æ¨¡å‹æ–¹æ³• â€£ 3.3 ç±»åˆ« â€£ 3 å¼ºåŒ–å­¦ä¹ åŸºç¡€ â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢è°ƒæŸ¥") è¢«é‡å†™ä¸ºï¼š
- en: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\Gamma_{t}}$
    |  | (15) |'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\Gamma_{t}}$
    |  | (15) |'
- en: The critic function $\Gamma$ can be defined as $Q^{\pi}(s_{t},a_{t})$ or $Q^{\pi}(s_{t},a_{t})-V^{\pi}_{t}$
    or $R[s_{t-1},s_{t}]+V^{\pi}_{t+1}-V^{\pi}_{t}$
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¯„è®ºå‘˜å‡½æ•° $\Gamma$ å¯ä»¥å®šä¹‰ä¸º $Q^{\pi}(s_{t},a_{t})$ æˆ– $Q^{\pi}(s_{t},a_{t})-V^{\pi}_{t}$
    æˆ– $R[s_{t-1},s_{t}]+V^{\pi}_{t+1}-V^{\pi}_{t}$
- en: 'Actor-critic methods are combinations of actor-only methods and critic-only
    methods. Thus, actor-critic methods have been commonly used RL. Depend on reward
    setting, there are two groups of actor-critic methods, namely discounted return
    [[282](#bib.bib282)], [[30](#bib.bib30)] and average return [[289](#bib.bib289)],
    [[31](#bib.bib31)]. The comparison between model-based and model-free methods
    is given in Table [1](#S3.T1 "Table 1 â€£ 3.3.2 Model-free methods â€£ 3.3 Category
    â€£ 3 Basics of Reinforcement Learning â€£ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-criticæ–¹æ³•æ˜¯ä»…æœ‰æ¼”å‘˜æ–¹æ³•å’Œä»…æœ‰è¯„è®ºå‘˜æ–¹æ³•çš„ç»„åˆã€‚å› æ­¤ï¼Œactor-criticæ–¹æ³•é€šå¸¸ç”¨äºRLã€‚æ ¹æ®å¥–åŠ±è®¾ç½®ï¼Œæœ‰ä¸¤ç»„actor-criticæ–¹æ³•ï¼Œå³æŠ˜æ‰£å›æŠ¥[[282](#bib.bib282)]ã€[[30](#bib.bib30)]å’Œå¹³å‡å›æŠ¥[[289](#bib.bib289)]ã€[[31](#bib.bib31)]ã€‚åŸºäºæ¨¡å‹å’Œæ— æ¨¡å‹æ–¹æ³•çš„æ¯”è¾ƒè§è¡¨[1](#S3.T1
    "è¡¨ 1 â€£ 3.3.2 æ— æ¨¡å‹æ–¹æ³• â€£ 3.3 ç±»åˆ« â€£ 3 å¼ºåŒ–å­¦ä¹ åŸºç¡€ â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢ç»¼è¿°")ã€‚
- en: '{forest}'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: '[Model-based RL [ Value Functions [ Differential Dynamic Programming [[208](#bib.bib208)],
    [[266](#bib.bib266)] ] [ Temporal Difference Learning [[249](#bib.bib249)] ] [
    Policy Iteration [[334](#bib.bib334)] ] [ Monte Carlo [[137](#bib.bib137)] ] ]
    [ Transition Models [ Deterministic models [ Decision trees [[280](#bib.bib280)]
    ] [ Linear regression [[265](#bib.bib265)] ] ] [ Stochastic models [ Gaussian
    processes [[71](#bib.bib71)], [[1](#bib.bib1)], [[12](#bib.bib12)] ] [ Expectation-Maximization
    [[59](#bib.bib59)] ] [ Dynamic Bayesian networks [[280](#bib.bib280)] ] ] ] [
    Policy Search [ Gradient-based [[87](#bib.bib87)], [[267](#bib.bib267)] ] [ Information
    theory [[1](#bib.bib1)], [[189](#bib.bib189)] ] [ Sampling based [[21](#bib.bib21)]
    ] ] [ Return Functions [ Discounted returns functions [[21](#bib.bib21)], [[75](#bib.bib75)],
    [[393](#bib.bib393)] ] [ Averaged returns functions [[34](#bib.bib34)], [[3](#bib.bib3)]
    ] ] ] ] ]'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[åŸºäºæ¨¡å‹çš„RL [ ä»·å€¼å‡½æ•° [ å¾®åˆ†åŠ¨æ€è§„åˆ’ [[208](#bib.bib208)], [[266](#bib.bib266)] ] [ æ—¶åºå·®åˆ†å­¦ä¹ 
    [[249](#bib.bib249)] ] [ ç­–ç•¥è¿­ä»£ [[334](#bib.bib334)] ] [ è’™ç‰¹å¡ç½— [[137](#bib.bib137)]
    ] ] [ è½¬ç§»æ¨¡å‹ [ ç¡®å®šæ€§æ¨¡å‹ [ å†³ç­–æ ‘ [[280](#bib.bib280)] ] [ çº¿æ€§å›å½’ [[265](#bib.bib265)] ]
    ] [ éšæœºæ¨¡å‹ [ é«˜æ–¯è¿‡ç¨‹ [[71](#bib.bib71)], [[1](#bib.bib1)], [[12](#bib.bib12)] ] [ æœŸæœ›æœ€å¤§åŒ–
    [[59](#bib.bib59)] ] [ åŠ¨æ€è´å¶æ–¯ç½‘ç»œ [[280](#bib.bib280)] ] ] ] [ ç­–ç•¥æœç´¢ [ åŸºäºæ¢¯åº¦ [[87](#bib.bib87)],
    [[267](#bib.bib267)] ] [ ä¿¡æ¯ç†è®º [[1](#bib.bib1)], [[189](#bib.bib189)] ] [ åŸºäºé‡‡æ ·
    [[21](#bib.bib21)] ] ] [ å›æŠ¥å‡½æ•° [ æŠ˜æ‰£å›æŠ¥å‡½æ•° [[21](#bib.bib21)], [[75](#bib.bib75)],
    [[393](#bib.bib393)] ] [ å¹³å‡å›æŠ¥å‡½æ•° [[34](#bib.bib34)], [[3](#bib.bib3)] ] ] ] ] ]'
- en: 'Figure 6: Summarization of model-based RL approaches'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šåŸºäºæ¨¡å‹çš„RLæ–¹æ³•æ€»ç»“
- en: 'Table 1: Comparison between model-based RL and model-free RL'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1ï¼šåŸºäºæ¨¡å‹çš„RLå’Œæ— æ¨¡å‹RLçš„æ¯”è¾ƒ
- en: '| Factors | Model-based RL | Model-free RL |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| å› ç´  | åŸºäºæ¨¡å‹çš„RL | æ— æ¨¡å‹RL |'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number of iterations between &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; è¿­ä»£æ¬¡æ•°ä¹‹é—´ &#124;'
- en: '&#124; agent and environment &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ä»£ç†å’Œç¯å¢ƒ &#124;'
- en: '| Small | Big |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| å° | å¤§ |'
- en: '| Convergence | Fast | Slow |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| æ”¶æ•›æ€§ | å¿« | æ…¢ |'
- en: '| Prior knowledge of transitions | Yes | No |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| è¿‡æ¸¡çš„å…ˆéªŒçŸ¥è¯† | æ˜¯ | å¦ |'
- en: '| Flexibility |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| çµæ´»æ€§ |'
- en: '&#124; Strongly depend on &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å¼ºçƒˆä¾èµ–äº &#124;'
- en: '&#124; a learnt model &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ä¸€ä¸ªå­¦ä¹ çš„æ¨¡å‹ &#124;'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Adjust based &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; åŸºäºè°ƒæ•´ &#124;'
- en: '&#124; on trials and errors &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; åŸºäºè¯•éªŒå’Œé”™è¯¯ &#124;'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 4 Introduction to Deep Reinforcement Learning
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹
- en: DRL, which was proposed as a combination of RL and DL, has achieved rapid developments,
    thanks to the rich context representation of DL. Under DRL, the aforementioned
    value and policy can be expressed by neural networks which allow dealing with
    a continuous state or action that was hard for a table representation. Similar
    to RL, DRL can be categorized into model-based algorithms and model-free algorithms
    which will be introduced in this section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: DRLä½œä¸ºRLå’ŒDLçš„ç»“åˆä½“å¾—åˆ°äº†è¿…é€Ÿçš„å‘å±•ï¼Œè¿™å¾—ç›ŠäºDLçš„ä¸°å¯Œä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚åœ¨DRLä¸­ï¼Œå‰è¿°çš„ä»·å€¼å’Œç­–ç•¥å¯ä»¥é€šè¿‡ç¥ç»ç½‘ç»œè¡¨è¾¾ï¼Œä»è€Œå¤„ç†ä»¥å‰éš¾ä»¥ç”¨è¡¨æ ¼è¡¨ç¤ºçš„è¿ç»­çŠ¶æ€æˆ–åŠ¨ä½œã€‚ç±»ä¼¼äºRLï¼ŒDRLå¯ä»¥åˆ†ä¸ºåŸºäºæ¨¡å‹çš„ç®—æ³•å’Œæ— æ¨¡å‹ç®—æ³•ï¼Œæœ¬èŠ‚å°†ä»‹ç»è¿™ä¸¤ç±»ç®—æ³•ã€‚
- en: 4.1 Model-Free Algorithms
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 æ— æ¨¡å‹ç®—æ³•
- en: There are two approaches, namely, Value-based DRL methods and Policy gradient
    DRL methods to implement model-free algorithms.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§æ–¹æ³•ï¼Œå³åŸºäºä»·å€¼çš„DRLæ–¹æ³•å’Œç­–ç•¥æ¢¯åº¦DRLæ–¹æ³•ï¼Œç”¨äºå®ç°æ— æ¨¡å‹ç®—æ³•ã€‚
- en: 4.1.1 Value-based DRL methods
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 åŸºäºä»·å€¼çš„DRLæ–¹æ³•
- en: 'Deep Q-Learning Network (DQN): Deep Q-learning [[264](#bib.bib264)] (DQN) is
    the most famous DRL model which learns policies directly from high-dimensional
    inputs by CNNs. In DQN, input is raw pixels and output is a quality function to
    estimate future rewards as given in Fig.[7](#S4.F7 "Figure 7 â€£ 4.1.1 Value-based
    DRL methods â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction to Deep Reinforcement
    Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    Take regression problem as an instance. Let $y$ denote the target of our regression
    task, the regression with input $(s,a)$, target $y(s,a)$ and the MSE loss function
    is as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ·±åº¦ Q å­¦ä¹ ç½‘ç»œ (DQN)ï¼šæ·±åº¦ Q å­¦ä¹ [[264](#bib.bib264)](DQN)æ˜¯æœ€è‘—åçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œå®ƒé€šè¿‡ CNN ç›´æ¥ä»é«˜ç»´è¾“å…¥ä¸­å­¦ä¹ ç­–ç•¥ã€‚åœ¨
    DQN ä¸­ï¼Œè¾“å…¥æ˜¯åŸå§‹åƒç´ ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªè´¨é‡å‡½æ•°ï¼Œç”¨äºä¼°è®¡æœªæ¥çš„å¥–åŠ±ï¼Œå¦‚å›¾[7](#S4.F7 "Figure 7 â€£ 4.1.1 Value-based DRL
    methods â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction to Deep Reinforcement Learning
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")ä¸­æ‰€ç¤ºã€‚ä»¥å›å½’é—®é¢˜ä¸ºä¾‹ã€‚è®©
    $y$ è¡¨ç¤ºæˆ‘ä»¬å›å½’ä»»åŠ¡çš„ç›®æ ‡ï¼Œå¯¹äºè¾“å…¥ $(s,a)$ çš„å›å½’ï¼Œç›®æ ‡ $y(s,a)$ å’Œå‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ä¸ºï¼š'
- en: '|  | <math   alttext="\begin{split}\mathcal{L^{DQN}}&amp;=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &amp;=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\mathcal{L^{DQN}}&amp;=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &amp;=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\ '
- en: y(s_{t},a_{t})&amp;=R(s_{t},s_{t+1})+\gamma\max_{a_{t+1}}Q^{*}(s_{t_{1}},a_{t+1},\theta_{t})\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="right"  ><msup ><mi >â„’</mi><mrow ><mi >ğ’Ÿ</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi >ğ’¬</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >ğ’©</mi></mrow></msup></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mrow ><mi >â„’</mi><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi  >y</mi><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo><mrow ><msup ><mi  >Q</mi><mo >âˆ—</mo></msup><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >Î¸</mi><mi >t</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><msup ><mrow ><mo stretchy="false"
    >â€–</mo><mrow ><mrow ><mi  >y</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo
    stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >âˆ’</mo><mrow ><msup ><mi >Q</mi><mo >âˆ—</mo></msup><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >Î¸</mi><mi >t</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >â€–</mo></mrow><mn
    >2</mn></msup></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >y</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mrow ><mrow  ><mi >R</mi><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo
    stretchy="false" >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >Î³</mi><mo lspace="0.167em"
    rspace="0em" >â€‹</mo><mrow ><munder ><mi >max</mi><msub ><mi >a</mi><mrow ><mi
    >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"  >â¡</mo><msup
    ><mi >Q</mi><mo >âˆ—</mo></msup></mrow><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><msub ><mi >t</mi><mn >1</mn></msub></msub><mo
    >,</mo><msub ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo
    >,</mo><msub ><mi  >Î¸</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >â„’</ci><apply ><ci  >ğ’Ÿ</ci><ci >ğ’¬</ci><ci >ğ’©</ci></apply></apply><apply ><ci >â„’</ci><interval
    closure="open" ><apply  ><ci >ğ‘¦</ci><interval closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci  >ğ‘¡</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘¡</ci></apply></interval></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘„</ci></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘¡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğœƒ</ci><ci >ğ‘¡</ci></apply></vector></apply></interval></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="latexml" >norm</csymbol><apply ><apply ><ci  >ğ‘¦</ci><interval closure="open"  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¡</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘¡</ci></apply></interval></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ğ‘„</ci></apply><vector
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘¡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğœƒ</ci><ci >ğ‘¡</ci></apply></vector></apply></apply></apply><cn
    type="integer"  >2</cn></apply><ci >ğ‘¦</ci><interval closure="open"  ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¡</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘¡</ci></apply></interval></apply></apply><apply
    ><apply ><apply  ><ci >ğ‘…</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¡</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ </ci><apply ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >ğ›¾</ci><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><apply ><ci >ğ‘¡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >ğ‘„</ci></apply></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ </ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘</ci><apply ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğœƒ</ci><ci >ğ‘¡</ci></apply></vector></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{L^{DQN}}&=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\ y(s_{t},a_{t})&=R(s_{t},s_{t+1})+\gamma\max_{a_{t+1}}Q^{*}(s_{t_{1}},a_{t+1},\theta_{t})\end{split}</annotation></semantics></math>
    |  | (16) |
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Where $\theta$ is vector of parameters, $\theta\in\mathbb{R}^{|S||R|}$ and $s_{t+1}$
    is a sample from $T(s_{t+1}|s_{t},a_{t})$ with input of $(s_{t},a_{t})$.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing the loss function yields a gradient descent step formula to update
    $\theta$ as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{t+1}=\theta_{t}-\alpha_{t}\frac{\mathcal{\partial L^{DQN}}}{\partial\theta}$
    |  | (17) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/72d78e811fd5b99100e03f51dd288fd9.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Network structure of Deep Q-Network (DQN), where Q-values Q(s,a)
    are generated for all actions for a given state.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Double DQN: In DQN, the values of $Q^{*}$ in many domains were leading to overestimation
    because of $max$. In Eq.[16](#S4.E16 "In 4.1.1 Value-based DRL methods â€£ 4.1 Model-Free
    Algorithms â€£ 4 Introduction to Deep Reinforcement Learning â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), $y(s,a)=R(s,s^{\prime})+\gamma\max_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta)$
    shifts Q-value estimation towards either to the actions with high reward or to
    the actions with overestimating approximation error. Double DQN [[370](#bib.bib370)]
    is an improvement of DQN that combines double Q-learning [[130](#bib.bib130)]
    with DQN and it aims at reducing observed overestimation with better performance.
    The idea of Double DQN is based on separating action selection and action evaluation
    using its own approximation of $Q^{*}$ as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\max_{a_{t+1}}Q^{*}(s_{t+1},a_{t+1};\theta)=Q^{*}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}$
    |  | (18) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: Thus
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=R(s_{t},s_{t+1})+\gamma Q^{*}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})$
    |  | (19) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: 'The easiest and most expensive implementation of double DQN is to run two independent
    DQNs as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}y_{1}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{1}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{2}(s_{t+1},a_{t+1};\theta_{2});\theta_{1})\\'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: y_{2}=R(s_{t},s_{t+1})+\\
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: \gamma Q^{*}_{2}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{1}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><msub  ><mi >y</mi><mn >1</mn></msub><mo >=</mo><mrow
    ><mrow  ><mi >R</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi  >s</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >Î³</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><msubsup ><mi  >Q</mi><mn >1</mn><mo >âˆ—</mo></msubsup><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><munder accentunder="true"
    ><mrow  ><mi >arg</mi><mo lspace="0.167em"  >â¡</mo><mi >max</mi></mrow><msub ><mi
    >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"
    rspace="0em"  >â€‹</mo><msubsup ><mi >Q</mi><mn >2</mn><mo >âˆ—</mo></msubsup><mo
    lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi
    >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >;</mo><msub
    ><mi  >Î¸</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >;</mo><msub ><mi  >Î¸</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msub ><mi  >y</mi><mn >2</mn></msub><mo >=</mo><mrow
    ><mrow ><mi  >R</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi  >s</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >Î³</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><msubsup ><mi  >Q</mi><mn >2</mn><mo >âˆ—</mo></msubsup><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><munder accentunder="true"
    ><mrow  ><mi >arg</mi><mo lspace="0.167em"  >â¡</mo><mi >max</mi></mrow><msub ><mi
    >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"
    rspace="0em"  >â€‹</mo><msubsup ><mi >Q</mi><mn >1</mn><mo >âˆ—</mo></msubsup><mo
    lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi
    >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >;</mo><msub
    ><mi  >Î¸</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >;</mo><msub ><mi  >Î¸</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘¦</ci><cn type="integer" >1</cn></apply><apply ><apply ><ci  >ğ‘…</ci><interval
    closure="open"  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci
    >ğ‘¡</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><apply
    ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >ğ›¾</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >ğ‘„</ci></apply><cn type="integer" >1</cn></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><apply ><ci
    >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><apply ><ci >ğ‘¡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğ‘„</ci></apply><cn type="integer"  >2</cn></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ </ci><apply ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><apply ><ci >ğ‘¡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğœƒ</ci><cn type="integer" >2</cn></apply></vector></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğœƒ</ci><cn type="integer" >1</cn></apply></vector><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘¦</ci><cn type="integer" >2</cn></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci >ğ‘…</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¡</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ </ci><apply ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >ğ›¾</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >ğ‘„</ci></apply><cn type="integer" >2</cn></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><apply ><ci
    >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><apply ><ci >ğ‘¡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğ‘„</ci></apply><cn type="integer"  >1</cn></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ </ci><apply ><ci >ğ‘¡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><apply ><ci >ğ‘¡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğœƒ</ci><cn type="integer" >1</cn></apply></vector></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğœƒ</ci><cn type="integer" >2</cn></apply></vector></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}y_{1}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{1}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{2}(s_{t+1},a_{t+1};\theta_{2});\theta_{1})\\
    y_{2}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{2}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{1}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}</annotation></semantics></math>
    |  | (20) |
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Dueling DQN: In DQN, when the agent visits an unfavorable state, instead of
    lowering its value $V^{*}$, it remembers only low pay-off by updating $Q^{*}$.
    In order to address this limitation, Dueling DQN [[390](#bib.bib390)] incorporates
    approximation of $V^{*}$ explicitly in a computational graph by introducing an
    advantage function as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å¶DQNï¼šåœ¨DQNä¸­ï¼Œå½“ä»£ç†è®¿é—®ä¸€ä¸ªä¸åˆ©çŠ¶æ€æ—¶ï¼Œä¸æ˜¯é™ä½å…¶å€¼$V^{*}$ï¼Œè€Œæ˜¯é€šè¿‡æ›´æ–°$Q^{*}$åªè®°ä½ä½å›æŠ¥ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é™åˆ¶ï¼Œå¯¹å¶DQN[[390](#bib.bib390)]é€šè¿‡åœ¨è®¡ç®—å›¾ä¸­æ˜¾å¼åœ°å¼•å…¥ä¼˜åŠ¿å‡½æ•°æ¥è¿‘ä¼¼$V^{*}$ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (21) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (21) |'
- en: 'Therefore, we can reformulate Q-value: $Q^{*}(s,a)=A^{*}(s,a)+V^{*}(s)$. This
    implies that after DL the feature map is decomposed into two parts corresponding
    to $V^{*}(v)$ and $A^{*}(s,a)$ as illustrated in Fig.[8](#S4.F8 "Figure 8 â€£ 4.1.1
    Value-based DRL methods â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction to Deep Reinforcement
    Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    This can be implemented by splitting the fully connected layers in the DQN architecture
    to compute the advantage and state value functions separately, then combining
    them back into a single Q-function. An interesting result has shown that Dueling
    DQN obtains better performance if it is formulated as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°è¡¨è¿°Qå€¼ï¼š$Q^{*}(s,a)=A^{*}(s,a)+V^{*}(s)$ã€‚è¿™æ„å‘³ç€åœ¨æ·±åº¦å­¦ä¹ åï¼Œç‰¹å¾å›¾è¢«åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«å¯¹åº”$V^{*}(v)$å’Œ$A^{*}(s,a)$ï¼Œå¦‚å›¾[8](#S4.F8
    "å›¾ 8 â€£ 4.1.1 åŸºäºä»·å€¼çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³• â€£ 4.1 æ— æ¨¡å‹ç®—æ³• â€£ 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹ â€£ æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ï¼šç»¼åˆè°ƒæŸ¥")æ‰€ç¤ºã€‚è¿™å¯ä»¥é€šè¿‡å°†DQNæ¶æ„ä¸­çš„å…¨è¿æ¥å±‚åˆ†å¼€æ¥è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’ŒçŠ¶æ€å€¼å‡½æ•°ï¼Œç„¶åå°†å®ƒä»¬ç»„åˆå›ä¸€ä¸ªå•ä¸€çš„Qå‡½æ•°æ¥å®ç°ã€‚æœ‰è¶£çš„ç»“æœè¡¨æ˜ï¼Œå¦‚æœå°†å¯¹å¶DQNè¡¨è¿°ä¸ºï¼š
- en: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\max_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  | (22) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\max_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  | (22) |'
- en: In practical implementation, averaging instead of maximum is used, i.e.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…å®ç°ä¸­ï¼Œä½¿ç”¨çš„æ˜¯å¹³å‡è€Œä¸æ˜¯æœ€å¤§å€¼ï¼Œå³
- en: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\text{mean}_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\text{mean}_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  |'
- en: Furthermore, to address the limitation of memory and imperfect information at
    each decision point, Deep Recurrent Q-Network (DRQN) [[131](#bib.bib131)] employed
    RNNs into DQN by replacing the first fully-connected layer with an RNN. Multi-step
    DQN [[68](#bib.bib68)] is one of the most popular improvements of DQN by substituting
    one-step approximation with N-steps.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä¸ºäº†åº”å¯¹åœ¨æ¯ä¸ªå†³ç­–ç‚¹çš„è®°å¿†é™åˆ¶å’Œä¸å®Œå…¨ä¿¡æ¯ï¼Œæ·±åº¦é€’å½’Qç½‘ç»œï¼ˆDRQNï¼‰[[131](#bib.bib131)]é€šè¿‡ç”¨RNNæ›¿æ¢ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚å°†RNNå¼•å…¥DQNã€‚å¤šæ­¥DQN
    [[68](#bib.bib68)]æ˜¯DQNæœ€å—æ¬¢è¿çš„æ”¹è¿›ä¹‹ä¸€ï¼Œé€šè¿‡ç”¨Næ­¥æ›¿ä»£ä¸€æ­¥è¿‘ä¼¼ã€‚
- en: '![Refer to caption](img/e873b6469a95ee4e6160354d4267e4e2.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒå›¾ä¾‹](img/e873b6469a95ee4e6160354d4267e4e2.png)'
- en: 'Figure 8: Network structure of Dueling DQN, where value function $V(s)$ and
    advantage function $A(s,a)$ are combined to predict Q-values $Q(s,a)$ for all
    actions for a given state.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8ï¼šå¯¹å¶DQNçš„ç½‘ç»œç»“æ„ï¼Œå…¶ä¸­ä»·å€¼å‡½æ•°$V(s)$å’Œä¼˜åŠ¿å‡½æ•°$A(s,a)$è¢«ç»„åˆä»¥é¢„æµ‹ç»™å®šçŠ¶æ€ä¸‹æ‰€æœ‰åŠ¨ä½œçš„Qå€¼$Q(s,a)$ã€‚
- en: 4.1.2 Policy gradient DRL methods
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 ç­–ç­–æ¸è¿›æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•
- en: 'Policy Gradient Theorem: Different from value-based DRL methods, policy gradient
    DRL optimizes the policy directly by optimizing the following objective function
    which is defined as a function of $\theta$.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç­–æ¸è¿›å®šç†ï¼šä¸åŸºäºä»·å€¼çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼Œç­–ç•¥æ¢¯åº¦æ·±åº¦å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¼˜åŒ–ä»¥ä¸‹ç›®æ ‡å‡½æ•°ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥å‡½æ•°è¢«å®šä¹‰ä¸º$\theta$çš„å‡½æ•°ã€‚
- en: '|  | $\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=1}{\gamma^{t-1}R(s_{t-1},s_{t})}\rightarrow\max_{\theta}$
    |  | (23) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=1}{\gamma^{t-1}R(s_{t-1},s_{t})}\rightarrow\max_{\theta}$
    |  | (23) |'
- en: 'For any MDP and differentiable policy $\pi_{\theta}$, the gradient of objective
    Eq.[23](#S4.E23 "In 4.1.2 Policy gradient DRL methods â€£ 4.1 Model-Free Algorithms
    â€£ 4 Introduction to Deep Reinforcement Learning â€£ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey") is defined by policy gradient theorem
    [[353](#bib.bib353)] as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»»ä½•MDPå’Œå¯å¾®åˆ†ç­–ç•¥$\pi_{\theta}$ï¼Œç›®æ ‡æ–¹ç¨‹ Eq.[23](#S4.E23 "åœ¨4.1.2 ç­–ç­–æ¸è¿›æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³• â€£ 4.1
    æ— æ¨¡å‹ç®—æ³• â€£ 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹ â€£ æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ï¼šç»¼åˆè°ƒæŸ¥")çš„æ¢¯åº¦ç”±ç­–ç•¥æ¢¯åº¦å®šç†[[353](#bib.bib353)]å®šä¹‰å¦‚ä¸‹ï¼š
- en: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=0}{\gamma^{t}Q^{\pi}(s_{t},a_{t})\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})}$
    |  | (24) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=0}{\gamma^{t}Q^{\pi}(s_{t},a_{t})\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}|s_{t})}$
    |  | (24) |'
- en: 'REINFORCE: REINFORCE was introduced by [[392](#bib.bib392)] to approximately
    calculate the gradient in Eq.[24](#S4.E24 "In 4.1.2 Policy gradient DRL methods
    â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction to Deep Reinforcement Learning â€£
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") by using
    Monte-Carlo estimation. In REINFORCE approximate estimator, Eq.[24](#S4.E24 "In
    4.1.2 Policy gradient DRL methods â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction
    to Deep Reinforcement Learning â€£ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey") is reformulated as:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'REINFORCE: REINFORCE æ˜¯ç”± [[392](#bib.bib392)] å¼•å…¥çš„ï¼Œç”¨äºé€šè¿‡è’™ç‰¹å¡æ´›ä¼°è®¡è¿‘ä¼¼è®¡ç®— Eq.[24](#S4.E24
    "åœ¨ 4.1.2 ç­–ç•¥æ¢¯åº¦ DRL æ–¹æ³• â€£ 4.1 æ— æ¨¡å‹ç®—æ³• â€£ 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹ â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢ç»¼è¿°") ä¸­çš„æ¢¯åº¦ã€‚åœ¨ REINFORCE
    è¿‘ä¼¼ä¼°è®¡å™¨ä¸­ï¼ŒEq.[24](#S4.E24 "åœ¨ 4.1.2 ç­–ç•¥æ¢¯åº¦ DRL æ–¹æ³• â€£ 4.1 æ— æ¨¡å‹ç®—æ³• â€£ 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹ â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢ç»¼è¿°")
    è¢«é‡æ–°è¡¨è¿°ä¸ºï¼š'
- en: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)\approx\sum_{\mathcal{T}}^{N}\sum_{t=0}{\gamma^{t}\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})(\sum_{t^{\prime}=t}{\gamma^{t^{\prime}-t}R(s_{t^{\prime}},s_{t^{\prime}+1})})}$
    |  | (25) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)\approx\sum_{\mathcal{T}}^{N}\sum_{t=0}{\gamma^{t}\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})(\sum_{t^{\prime}=t}{\gamma^{t^{\prime}-t}R(s_{t^{\prime}},s_{t^{\prime}+1})})}$
    |  | (25) |'
- en: 'where $\mathcal{T}$ is trajectory distribution and defined in Eq.[5](#S3.E5
    "In 3.1 Markov Decision Process â€£ 3 Basics of Reinforcement Learning â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). Theoretically, REINFORCE
    can be straightforwardly applied into any parametric $\pi_{theta}(a|s)$. However,
    it is impractical to use because of its time-consuming nature for convergence
    and local optimums problem. Based on the observation that the convergence rate
    of stochastic gradient descent directly depends on the variance of gradient estimation,
    the variance reduction technique was proposed to address naive REINFORCEâ€™s limitations
    by adding a term that reduces the variance without affecting the expectation.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathcal{T}$ æ˜¯è½¨è¿¹åˆ†å¸ƒï¼Œåœ¨ Eq.[5](#S3.E5 "åœ¨ 3.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ â€£ 3 å¼ºåŒ–å­¦ä¹ åŸºç¡€ â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢ç»¼è¿°")
    ä¸­å®šä¹‰ã€‚ä»ç†è®ºä¸Šè®²ï¼ŒREINFORCE å¯ä»¥ç›´æ¥åº”ç”¨äºä»»ä½•å‚æ•°åŒ–çš„ $\pi_{\theta}(a|s)$ã€‚ç„¶è€Œï¼Œç”±äºæ”¶æ•›çš„æ—¶é—´æ¶ˆè€—å’Œå±€éƒ¨æœ€ä¼˜é—®é¢˜ï¼Œå®ƒåœ¨å®é™…ä½¿ç”¨ä¸­å¹¶ä¸å®ç”¨ã€‚åŸºäºéšæœºæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›é€Ÿåº¦ç›´æ¥ä¾èµ–äºæ¢¯åº¦ä¼°è®¡çš„æ–¹å·®çš„è§‚å¯Ÿï¼Œæå‡ºäº†æ–¹å·®å‡å°‘æŠ€æœ¯æ¥è§£å†³åŸå§‹
    REINFORCE çš„å±€é™æ€§ï¼Œé€šè¿‡æ·»åŠ ä¸€ä¸ªé¡¹æ¥å‡å°‘æ–¹å·®è€Œä¸å½±å“æœŸæœ›å€¼ã€‚
- en: 4.1.3 Actor-Critic DRL algorithm
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 Actor-Critic DRL ç®—æ³•
- en: 'Both value-based and policy gradient algorithms have their own pros and cons,
    i.e. policy gradient methods are better for continuous and stochastic environments,
    and have a faster convergence whereas, value-based methods are more sample efficient
    and steady. Lately, actor-critic [[182](#bib.bib182)] [[262](#bib.bib262)] was
    born to take advantage from both value-based and policy gradient while limiting
    their drawbacks. Actor-critic architecture computes the policy gradient using
    a value-based critic function to estimate expected future reward. The principal
    idea of actor-critic is to divide the model into two parts: (i) computing an action
    based on a state and (ii) producing the Q values of the action. As given in Fig.[9](#S4.F9
    "Figure 9 â€£ 4.1.3 Actor-Critic DRL algorithm â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction
    to Deep Reinforcement Learning â€£ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey"), the actor takes as input the state $s_{t}$ and outputs
    the best action $a_{t}$. It essentially controls how the agent behaves by learning
    the optimal policy (policy-based). The critic, on the other hand, evaluates the
    action by computing the value function (value-based). The most basic actor-critic
    method (beyond the tabular case) is naive policy gradients (REINFORCE). The relationship
    between actor-critic is similar to kid-mom. The kid (actor) explores the environment
    around him/her with new actions i.e. tough fire, hit a wall, climb a tree, etc
    while the mom (critic) watches the kid and criticizes/compliments him/her. The
    kid then adjusts his/her behavior based on what his/her mom told. When the kids
    get older, he/she can realize which action is bad/good.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºä»·å€¼çš„æ–¹æ³•å’Œç­–ç•¥æ¢¯åº¦ç®—æ³•å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œå³ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨è¿ç»­å’Œéšæœºç¯å¢ƒä¸­è¡¨ç°æ›´å¥½ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œè€ŒåŸºäºä»·å€¼çš„æ–¹æ³•åˆ™åœ¨æ ·æœ¬æ•ˆç‡å’Œç¨³å®šæ€§ä¸Šæ›´ä¸ºå‡ºè‰²ã€‚æœ€è¿‘ï¼Œæ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•
    [[182](#bib.bib182)] [[262](#bib.bib262)] çš„å‡ºç°èåˆäº†åŸºäºä»·å€¼å’Œç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ä¼˜ç‚¹ï¼ŒåŒæ—¶é™åˆ¶äº†å®ƒä»¬çš„ç¼ºç‚¹ã€‚æ¼”å‘˜-è¯„è®ºå®¶æ¶æ„é€šè¿‡ä½¿ç”¨åŸºäºä»·å€¼çš„è¯„è®ºå®¶å‡½æ•°æ¥ä¼°è®¡æœŸæœ›çš„æœªæ¥å¥–åŠ±ï¼Œä»è€Œè®¡ç®—ç­–ç•¥æ¢¯åº¦ã€‚æ¼”å‘˜-è¯„è®ºå®¶çš„ä¸»è¦æ€æƒ³æ˜¯å°†æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šï¼ˆiï¼‰åŸºäºçŠ¶æ€è®¡ç®—åŠ¨ä½œå’Œï¼ˆiiï¼‰ç”ŸæˆåŠ¨ä½œçš„
    Q å€¼ã€‚å¦‚å›¾ [9](#S4.F9 "å›¾ 9 â€£ 4.1.3 æ¼”å‘˜-è¯„è®ºå®¶ DRL ç®—æ³• â€£ 4.1 æ— æ¨¡å‹ç®—æ³• â€£ 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹ â€£ è®¡ç®—æœºè§†è§‰ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šå…¨é¢ç»¼è¿°")
    æ‰€ç¤ºï¼Œæ¼”å‘˜ä»¥çŠ¶æ€ $s_{t}$ ä¸ºè¾“å…¥ï¼Œè¾“å‡ºæœ€ä½³åŠ¨ä½œ $a_{t}$ã€‚å®ƒæœ¬è´¨ä¸Šé€šè¿‡å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼ˆåŸºäºç­–ç•¥ï¼‰æ¥æ§åˆ¶ä»£ç†çš„è¡Œä¸ºã€‚è€Œè¯„è®ºå®¶åˆ™é€šè¿‡è®¡ç®—ä»·å€¼å‡½æ•°ï¼ˆåŸºäºä»·å€¼ï¼‰æ¥è¯„ä¼°åŠ¨ä½œã€‚æœ€åŸºæœ¬çš„æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼ˆè¶…å‡ºè¡¨æ ¼æƒ…å†µï¼‰æ˜¯æœ´ç´ ç­–ç•¥æ¢¯åº¦ï¼ˆREINFORCEï¼‰ã€‚æ¼”å‘˜-è¯„è®ºå®¶çš„å…³ç³»ç±»ä¼¼äºå­©å­å’Œæ¯äº²ã€‚å­©å­ï¼ˆæ¼”å‘˜ï¼‰é€šè¿‡æ–°çš„åŠ¨ä½œæ¢ç´¢å‘¨å›´ç¯å¢ƒï¼Œå³ç»å†å›°éš¾çš„ç«ç¾ã€æ’å¢™ã€çˆ¬æ ‘ç­‰ï¼Œè€Œæ¯äº²ï¼ˆè¯„è®ºå®¶ï¼‰åˆ™è§‚å¯Ÿå­©å­å¹¶å¯¹å…¶è¿›è¡Œæ‰¹è¯„æˆ–èµæ‰¬ã€‚å­©å­éšåæ ¹æ®æ¯äº²çš„åé¦ˆè°ƒæ•´è‡ªå·±çš„è¡Œä¸ºã€‚å½“å­©å­é•¿å¤§åï¼Œä»–/å¥¹èƒ½å¤Ÿæ„è¯†åˆ°å“ªäº›åŠ¨ä½œæ˜¯å¥½æˆ–åçš„ã€‚
- en: '![Refer to caption](img/8fdb0e2aceef831039655bd509b14e01.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/8fdb0e2aceef831039655bd509b14e01.png)'
- en: 'Figure 9: Flowchart showing the structure of actor critic algorithm.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9ï¼šæ˜¾ç¤ºæ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•ç»“æ„çš„æµç¨‹å›¾ã€‚
- en: '![Refer to caption](img/13da43e1b6315b61af979883bf93af15.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/13da43e1b6315b61af979883bf93af15.png)'
- en: 'Figure 10: An illustration of Actor-Critic algorithm in two cases: sharing
    parameters (a) and not sharing parameters (b).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10ï¼šæ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•åœ¨ä¸¤ç§æƒ…å†µä¸‹çš„ç¤ºæ„å›¾ï¼šå…±äº«å‚æ•° (a) å’Œä¸å…±äº«å‚æ•° (b)ã€‚
- en: 'Advantage Actor-Critic (A2C) Advantage Actor-Critic (A2C) [[263](#bib.bib263)]
    consist of two neural networks i.e. actor network $\pi_{\theta}(a_{t}|s_{t})$
    representing for policy and critic network $V^{\pi}_{\omega}$ with parameters
    $\omega$ approximately estimating actorâ€™s performance. In order to determine how
    much better, it is to take a specific action compared to the average, an advantage
    value is defined as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆA2Cï¼‰ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆA2Cï¼‰ [[263](#bib.bib263)] åŒ…å«ä¸¤ä¸ªç¥ç»ç½‘ç»œï¼Œå³ä»£è¡¨ç­–ç•¥çš„æ¼”å‘˜ç½‘ç»œ $\pi_{\theta}(a_{t}|s_{t})$
    å’Œå¸¦æœ‰å‚æ•° $\omega$ çš„è¯„è®ºå®¶ç½‘ç»œ $V^{\pi}_{\omega}$ï¼Œåè€…å¤§è‡´ä¼°è®¡æ¼”å‘˜çš„è¡¨ç°ã€‚ä¸ºäº†ç¡®å®šé‡‡å–ç‰¹å®šåŠ¨ä½œç›¸å¯¹äºå¹³å‡æ°´å¹³æœ‰å¤šå¤§ä¼˜åŠ¿ï¼Œå®šä¹‰äº†ä¼˜åŠ¿å€¼ä¸ºï¼š
- en: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (26) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (26) |'
- en: 'Instead of constructing two neural networks for both the Q value and the V
    value, using the Bellman optimization equation, we can rewrite the advantage function
    as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä¸º Q å€¼å’Œ V å€¼åˆ†åˆ«æ„å»ºä¸¤ä¸ªç¥ç»ç½‘ç»œï¼Œä¸å¦‚ä½¿ç”¨è´å°”æ›¼ä¼˜åŒ–æ–¹ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¼˜åŠ¿å‡½æ•°é‡å†™ä¸ºï¼š
- en: '|  | $A^{\pi}(s_{t},a_{t})=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})-V^{\pi}_{\omega}(s_{t})$
    |  | (27) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})-V^{\pi}_{\omega}(s_{t})$
    |  | (27) |'
- en: 'For given policy $\pi$, its value function can be obtained using point iteration
    for solving:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç»™å®šçš„ç­–ç•¥ $\pi$ï¼Œå…¶ä»·å€¼å‡½æ•°å¯ä»¥é€šè¿‡ç‚¹è¿­ä»£æ¥æ±‚è§£ï¼š
- en: '|  | $V^{\pi}(s_{t})=\mathbb{E}_{a_{t}\sim\pi(a_{t}&#124;s_{t})}\mathbb{E}_{s_{t+1}\sim
    T(s_{t+1}&#124;a_{t},s_{t})}(R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1}))$ |  | (28)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $V^{\pi}(s_{t})=\mathbb{E}_{a_{t}\sim\pi(a_{t}&#124;s_{t})}\mathbb{E}_{s_{t+1}\sim
    T(s_{t+1}&#124;a_{t},s_{t})}(R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1}))$ |  | (28)
    |'
- en: 'Similar to DQN, on each update a target is computed using current approximation:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº DQNï¼Œåœ¨æ¯æ¬¡æ›´æ–°æ—¶ï¼Œä½¿ç”¨å½“å‰è¿‘ä¼¼å€¼è®¡ç®—ç›®æ ‡ï¼š
- en: '|  | $y=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})$ |  | (29) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})$ |  | (29) |'
- en: 'At time step t, the A2C algorithm can be implemented as following steps:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ—¶é—´æ­¥ tï¼ŒA2C ç®—æ³•å¯ä»¥æŒ‰ä»¥ä¸‹æ­¥éª¤å®æ–½ï¼š
- en: â€¢
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Step 1: Compute advantage function using Eq.[27](#S4.E27 "In 4.1.3 Actor-Critic
    DRL algorithm â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction to Deep Reinforcement
    Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ Eq.[27](#S4.E27 "åœ¨ 4.1.3 Actor-Critic DRL ç®—æ³• â€£ 4.1 æ— æ¨¡å‹ç®—æ³• â€£ 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹
    â€£ æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ï¼šç»¼åˆè°ƒæŸ¥") è®¡ç®—ä¼˜åŠ¿å‡½æ•°ã€‚
- en: â€¢
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Step 2: Compute target using Eq.[29](#S4.E29 "In 4.1.3 Actor-Critic DRL algorithm
    â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction to Deep Reinforcement Learning â€£
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ Eq.[29](#S4.E29 "åœ¨ 4.1.3 Actor-Critic DRL ç®—æ³• â€£ 4.1 æ— æ¨¡å‹ç®—æ³• â€£ 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹
    â€£ æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ï¼šç»¼åˆè°ƒæŸ¥") è®¡ç®—ç›®æ ‡ã€‚
- en: â€¢
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Step 3: Compute critic loss with MSE loss: $\mathcal{L}=\frac{1}{B}\sum_{T}||y-V^{\pi}(s_{t}))||^{2}$,
    where $B$ is batch size and $V^{\pi}(s_{t})$ is defined in Eq.[28](#S4.E28 "In
    4.1.3 Actor-Critic DRL algorithm â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction
    to Deep Reinforcement Learning â€£ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey").'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥ï¼šç”¨ MSE æŸå¤±è®¡ç®—è¯„è®ºå®¶æŸå¤±ï¼š$\mathcal{L}=\frac{1}{B}\sum_{T}||y-V^{\pi}(s_{t}))||^{2}$ï¼Œå…¶ä¸­
    $B$ æ˜¯æ‰¹é‡å¤§å°ï¼Œ$V^{\pi}(s_{t})$ åœ¨ Eq.[28](#S4.E28 "åœ¨ 4.1.3 Actor-Critic DRL ç®—æ³• â€£ 4.1
    æ— æ¨¡å‹ç®—æ³• â€£ 4 æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®€ä»‹ â€£ æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨ï¼šç»¼åˆè°ƒæŸ¥") ä¸­å®šä¹‰ã€‚
- en: â€¢
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Step 4: Compute critic gradient: $\bigtriangledown^{critic}=\frac{\partial\mathcal{L}}{\partial\omega}$.'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬å››æ­¥ï¼šè®¡ç®—è¯„è®ºå®¶æ¢¯åº¦ï¼š$\bigtriangledown^{critic}=\frac{\partial\mathcal{L}}{\partial\omega}$ã€‚
- en: â€¢
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Step 5: Compute actor gradient: $\bigtriangledown^{actor}=\frac{1}{B}\sum_{T}{\bigtriangledown_{\theta}\text{log}\pi(a_{t}|s_{t})A^{\pi}(s_{t},a_{t})}$'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¬¬äº”æ­¥ï¼šè®¡ç®—æ¼”å‘˜æ¢¯åº¦ï¼š$\bigtriangledown^{actor}=\frac{1}{B}\sum_{T}{\bigtriangledown_{\theta}\text{log}\pi(a_{t}|s_{t})A^{\pi}(s_{t},a_{t})}$
- en: 'Asynchronous Advantage Actor Critic (A3C) Besides A2C, there is another strategy
    to implement an Actor-Critic agent. Asynchronous Advantage Actor-Critic (A3C)
    [[263](#bib.bib263)] approach does not use experience replay because this requires
    a lot of memory. Instead, A3C asynchronously executes different agents in parallel
    on multiple instances of the environment. Each worker (copy of the network) will
    update the global network asynchronously. Because of the asynchronous nature of
    A3C, some workers (copy of the agents) will work with older values of the parameters.
    Thus the aggregating update will not be optimal. On the other hand, A2C synchronously
    updates the global network. A2C waits until all workers finished their training
    and calculated their gradients to average them, to update the global network.
    In order to update the entire network, A2C waits for each actor to finish their
    segment of experience before updating the global parameters. As a consequence,
    the training will be more cohesive and faster. Different from A3C, each worker
    in A2C has the same set of weights since and A2C updates all their workers at
    the same time. In short, A2C is an alternative to the synchronous version of the
    A3C. In A2C, it waits for each actor to finish its segment of experience before
    updating, averaging over all of the actors. In a practical experiment, this implementation
    is more effectively uses GPUs due to larger batch sizes. The structure of an actor-critic
    algorithm can be divided into two types depending on parameter sharing as illustrated
    in Fig.[10](#S4.F10 "Figure 10 â€£ 4.1.3 Actor-Critic DRL algorithm â€£ 4.1 Model-Free
    Algorithms â€£ 4 Introduction to Deep Reinforcement Learning â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆA3Cï¼‰é™¤äº† A2Cï¼Œè¿˜æœ‰å¦ä¸€ç§ç­–ç•¥æ¥å®ç°ä¸€ä¸ªæ¼”å‘˜-è¯„è®ºå®¶ä»£ç†ã€‚å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆA3Cï¼‰[[263](#bib.bib263)]
    æ–¹æ³•ä¸ä½¿ç”¨ç»éªŒå›æ”¾ï¼Œå› ä¸ºè¿™éœ€è¦å¤§é‡çš„å†…å­˜ã€‚ç›¸åï¼ŒA3C å¼‚æ­¥åœ°åœ¨å¤šä¸ªç¯å¢ƒå®ä¾‹ä¸Šå¹¶è¡Œæ‰§è¡Œä¸åŒçš„ä»£ç†ã€‚æ¯ä¸ªå·¥ä½œè€…ï¼ˆç½‘ç»œçš„å‰¯æœ¬ï¼‰å°†å¼‚æ­¥æ›´æ–°å…¨å±€ç½‘ç»œã€‚ç”±äº A3C
    çš„å¼‚æ­¥ç‰¹æ€§ï¼Œä¸€äº›å·¥ä½œè€…ï¼ˆä»£ç†çš„å‰¯æœ¬ï¼‰å°†ä½¿ç”¨æ—§çš„å‚æ•°å€¼ã€‚å› æ­¤ï¼Œèšåˆæ›´æ–°å°†ä¸æ˜¯æœ€ä¼˜çš„ã€‚å¦ä¸€æ–¹é¢ï¼ŒA2C åŒæ­¥æ›´æ–°å…¨å±€ç½‘ç»œã€‚A2C ç­‰å¾…æ‰€æœ‰å·¥ä½œè€…å®Œæˆä»–ä»¬çš„è®­ç»ƒå¹¶è®¡ç®—å…¶æ¢¯åº¦ä»¥è¿›è¡Œå¹³å‡ï¼Œç„¶åæ›´æ–°å…¨å±€ç½‘ç»œã€‚ä¸ºäº†æ›´æ–°æ•´ä¸ªç½‘ç»œï¼ŒA2C
    ç­‰å¾…æ¯ä¸ªæ¼”å‘˜å®Œæˆå…¶ç»éªŒæ®µåå†æ›´æ–°å…¨å±€å‚æ•°ã€‚å› æ­¤ï¼Œè®­ç»ƒå°†æ›´å…·å‡èšåŠ›ä¸”æ›´å¿«ã€‚ä¸ A3C ä¸åŒçš„æ˜¯ï¼ŒA2C ä¸­çš„æ¯ä¸ªå·¥ä½œè€…éƒ½æœ‰ç›¸åŒçš„æƒé‡ï¼Œå› ä¸º A2C åŒæ—¶æ›´æ–°æ‰€æœ‰å·¥ä½œè€…ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒA2C
    æ˜¯ A3C åŒæ­¥ç‰ˆæœ¬çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨ A2C ä¸­ï¼Œå®ƒç­‰å¾…æ¯ä¸ªæ¼”å‘˜å®Œæˆå…¶ç»éªŒæ®µåè¿›è¡Œæ›´æ–°ï¼Œå¯¹æ‰€æœ‰æ¼”å‘˜è¿›è¡Œå¹³å‡ã€‚åœ¨å®é™…å®éªŒä¸­ï¼Œç”±äºæ‰¹é‡å¤§å°è¾ƒå¤§ï¼Œè¿™ç§å®ç°æ›´æœ‰æ•ˆåœ°ä½¿ç”¨äº†
    GPUã€‚æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•çš„ç»“æ„å¯ä»¥æ ¹æ®å‚æ•°å…±äº«åˆ†ä¸ºä¸¤ç§ç±»å‹ï¼Œå¦‚å›¾[10](#S4.F10 "Figure 10 â€£ 4.1.3 Actor-Critic DRL
    algorithm â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction to Deep Reinforcement Learning
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") æ‰€ç¤ºã€‚'
- en: In order to overcome the limitation of speed, GA3C [[16](#bib.bib16)] was proposed
    and it achieved a significant speedup compared to the original CPU implementation.
    To more effectively train A3C, [[141](#bib.bib141)] proposed FFE which forces
    random exploration at the right time during a training episode, that can lead
    to improved training performance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…‹æœé€Ÿåº¦çš„é™åˆ¶ï¼ŒGA3C [[16](#bib.bib16)] è¢«æå‡ºï¼Œå¹¶ä¸”ä¸åŸå§‹çš„ CPU å®ç°ç›¸æ¯”ï¼Œå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°è®­ç»ƒ A3Cï¼Œ[[141](#bib.bib141)]
    æå‡ºäº† FFEï¼Œè¿™ç§æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼ºåˆ¶åœ¨åˆé€‚çš„æ—¶é—´è¿›è¡Œéšæœºæ¢ç´¢ï¼Œè¿™å¯ä»¥æé«˜è®­ç»ƒæ€§èƒ½ã€‚
- en: 4.2 Model-Based Algorithms
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 åŸºäºæ¨¡å‹çš„ç®—æ³•
- en: 'We have discussed so far model-free methods including the value-based approach
    and policy gradient approach. In this section, we focus on the model-based approach,
    that deals with the dynamics of the environment by learning a transition model
    that allows for simulation of the environment without interacting with the environment
    directly. In contrast to model-free approaches, model-based approaches are learned
    from experience by a function approximation. Theoretically, no specific prior
    knowledge is required in model-based RL/DRL but incorporating prior knowledge
    can help faster convergence and better-trained model, speed up training time as
    well as the number of training samples. While using raw data with pixel, it is
    difficult for model-based RL to work on high dimensional and dynamic environments.
    This is addressed in DRL by embedding the high-dimensional observations into a
    lower-dimensional space using autoencoders [[95](#bib.bib95)]. Many DRL approaches
    have been based on scaling up prior work in RL to high-dimensional problems. A
    good overview of model-based RL for high-dimensional problems can be found in
    [[297](#bib.bib297)] which partition model-based DRL into three categories: explicit
    planning on given transitions, explicit planning on learned transitions, and end-to-end
    learning of both planning and transitions. In general, DRL targets training DNNs
    to approximate the optimal policy $\pi^{*}$ together with optimal value functions
    $V^{*}$ and $Q^{*}$. In the following, we will cover the most common model-based
    DRL approaches including value function and policy search methods.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è®¨è®ºäº†åŒ…æ‹¬åŸºäºä»·å€¼çš„æ–¹æ³•å’Œç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„æ— æ¨¡å‹æ–¹æ³•ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»åŸºäºæ¨¡å‹çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å­¦ä¹ ä¸€ä¸ªè½¬ç§»æ¨¡å‹æ¥å¤„ç†ç¯å¢ƒçš„åŠ¨æ€ï¼Œè¯¥æ¨¡å‹å…è®¸åœ¨ä¸ç›´æ¥ä¸ç¯å¢ƒäº’åŠ¨çš„æƒ…å†µä¸‹æ¨¡æ‹Ÿç¯å¢ƒã€‚ä¸æ— æ¨¡å‹æ–¹æ³•ä¸åŒï¼ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•é€šè¿‡å‡½æ•°è¿‘ä¼¼ä»ç»éªŒä¸­å­¦ä¹ ã€‚ç†è®ºä¸Šï¼ŒåŸºäºæ¨¡å‹çš„
    RL/DRL ä¸éœ€è¦ç‰¹å®šçš„å…ˆéªŒçŸ¥è¯†ï¼Œä½†ç»“åˆå…ˆéªŒçŸ¥è¯†å¯ä»¥å¸®åŠ©æ›´å¿«çš„æ”¶æ•›å’Œæ›´å¥½çš„è®­ç»ƒæ¨¡å‹ï¼Œç¼©çŸ­è®­ç»ƒæ—¶é—´ä»¥åŠå‡å°‘è®­ç»ƒæ ·æœ¬çš„æ•°é‡ã€‚è™½ç„¶ä½¿ç”¨åŸå§‹åƒç´ æ•°æ®ï¼ŒåŸºäºæ¨¡å‹çš„
    RL åœ¨é«˜ç»´å’ŒåŠ¨æ€ç¯å¢ƒä¸­éš¾ä»¥å·¥ä½œã€‚DRL é€šè¿‡ä½¿ç”¨è‡ªç¼–ç å™¨ [[95](#bib.bib95)] å°†é«˜ç»´è§‚å¯ŸåµŒå…¥åˆ°ä½ç»´ç©ºé—´æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è®¸å¤š DRL æ–¹æ³•å·²åŸºäºå°†
    RL å…ˆå‰å·¥ä½œæ‰©å±•åˆ°é«˜ç»´é—®é¢˜ã€‚å…³äºé«˜ç»´é—®é¢˜çš„åŸºäºæ¨¡å‹çš„ RL çš„è‰¯å¥½æ¦‚è¿°å¯ä»¥åœ¨ [[297](#bib.bib297)] ä¸­æ‰¾åˆ°ï¼Œè¯¥æ–‡çŒ®å°†åŸºäºæ¨¡å‹çš„ DRL
    åˆ†ä¸ºä¸‰ä¸ªç±»åˆ«ï¼šç»™å®šè½¬ç§»ä¸Šçš„æ˜¾å¼è§„åˆ’ã€å­¦ä¹ è½¬ç§»ä¸Šçš„æ˜¾å¼è§„åˆ’ä»¥åŠè§„åˆ’å’Œè½¬ç§»çš„ç«¯åˆ°ç«¯å­¦ä¹ ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒDRL ç›®æ ‡æ˜¯è®­ç»ƒ DNNs æ¥è¿‘ä¼¼æœ€ä¼˜ç­–ç•¥ $\pi^{*}$
    ä»¥åŠæœ€ä¼˜ä»·å€¼å‡½æ•° $V^{*}$ å’Œ $Q^{*}$ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»‹ç»æœ€å¸¸è§çš„åŸºäºæ¨¡å‹çš„ DRL æ–¹æ³•ï¼ŒåŒ…æ‹¬ä»·å€¼å‡½æ•°å’Œç­–ç•¥æœç´¢æ–¹æ³•ã€‚
- en: 4.2.1 Value function
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 ä»·å€¼å‡½æ•°
- en: 'We start this category with DQN [[264](#bib.bib264)] which has been successfully
    applied to classic Atari and illustrated in Fig.[7](#S4.F7 "Figure 7 â€£ 4.1.1 Value-based
    DRL methods â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction to Deep Reinforcement
    Learning â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    DQN uses CNNs to deal with high dimensional state space like pixels, to approximate
    the Q-value function.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬ä» DQN [[264](#bib.bib264)] å¼€å§‹ï¼Œè¿™ä¸€æ–¹æ³•å·²æˆåŠŸåº”ç”¨äºç»å…¸çš„ Atari æ¸¸æˆï¼Œå¹¶åœ¨å›¾ [7](#S4.F7 "Figure
    7 â€£ 4.1.1 Value-based DRL methods â€£ 4.1 Model-Free Algorithms â€£ 4 Introduction
    to Deep Reinforcement Learning â€£ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey") ä¸­å±•ç¤ºã€‚DQN ä½¿ç”¨ CNNs æ¥å¤„ç†åƒç´ ç­‰é«˜ç»´çŠ¶æ€ç©ºé—´ï¼Œä»¥è¿‘ä¼¼ Q å€¼å‡½æ•°ã€‚'
- en: Monte Carlo tree search (MCTS) MCTS [[62](#bib.bib62)] is one of the most popular
    methods to look-ahead search and it is combined with a DNN-based transition model
    to build a model-based DRL in [[9](#bib.bib9)]. In this work, the learned transition
    model predicts the next frame and the rewards one step ahead using the input of
    the last four frames of the agentâ€™s first-person-view image and the current action.
    This model is then used by the Monte Carlo tree search algorithm to plan the best
    sequence of actions for the agent to perform.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰[[62](#bib.bib62)] æ˜¯ä¸€ç§æœ€å—æ¬¢è¿çš„å‰ç»æœç´¢æ–¹æ³•ï¼Œå®ƒä¸åŸºäº DNN çš„è½¬ç§»æ¨¡å‹ç»“åˆï¼Œç”¨äºæ„å»º [[9](#bib.bib9)]
    ä¸­çš„æ¨¡å‹åŸº DRLã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œå­¦ä¹ çš„è½¬ç§»æ¨¡å‹é¢„æµ‹ä¸‹ä¸€å¸§ä»¥åŠä¸€æ­¥ä¹‹é¥çš„å¥–åŠ±ï¼Œä½¿ç”¨çš„æ˜¯ä»£ç†çš„ç¬¬ä¸€äººç§°è§†è§’å›¾åƒçš„æœ€åå››å¸§å’Œå½“å‰åŠ¨ä½œä½œä¸ºè¾“å…¥ã€‚ç„¶åï¼Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•ä½¿ç”¨è¯¥æ¨¡å‹è§„åˆ’ä»£ç†æ‰§è¡Œçš„æœ€ä½³åŠ¨ä½œåºåˆ—ã€‚
- en: Value-Targeted Regression (UCRL-VTR) Alex, et al. proposed model-based DRL for
    regret minimization [[167](#bib.bib167)]. In their work, a set of models, that
    are â€˜consistentâ€™ with the data collected, is constructed at each episode. The
    consistency is defined as the total squared error, whereas the value function
    is determined by solving the optimistic planning problem with the constructed
    set of models
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Policy search
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Policy search methods aim to directly find policies by means of gradient-free
    or gradient-based methods.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) ME-TRPO [[190](#bib.bib190)]
    is mainly based on Trust Region Policy Optimization (TRPO) [[327](#bib.bib327)]
    which imposes a trust region constraint on the policy to further stabilize learning.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Model-Based Meta-Policy-Optimization (MB-MPO) MB-MPO [[58](#bib.bib58)] addresses
    the performance limitation of model-based DRL compared against model-free DRL
    when learning dynamics models. MB-MPO learns an ensemble of dynamics models, a
    policy that can quickly adapt to any model in the ensemble with one policy gradient
    step. As a result, the learned policy exhibits less model bias without the need
    to behave conservatively.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of both model-based and model-free DRL algorithms is given in Table
    [2](#S4.T2 "Table 2 â€£ 4.2.2 Policy search â€£ 4.2 Model-Based Algorithms â€£ 4 Introduction
    to Deep Reinforcement Learning â€£ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey"). In this Table, we also categorized DRL techniques into
    either on-policy or off-policy. In on-policy RL, it allows the use of older samples
    (collected using the older policies) in the calculation. The policy $\pi^{k}$
    is updated with data collected by $\pi^{k}$ itself. In off-policy RL, the data
    is assumed to be composed of different policies $\pi^{0},\pi^{0},...,\pi^{k}$.
    Each policy has its own data collection, then the data collected from $\pi^{0}$,
    $\pi^{1}$, â€¦, $\pi^{k}$ is used to train $\pi^{k+1}$.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of model-based and model-free DRL algorithms consisting of
    value-based and policy gradient methods.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '| DRL Algorithms | Description | Category |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| DQN [[264](#bib.bib264)] | Deep Q Network | Value-based Off-policy |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| Double DQN [[370](#bib.bib370)] | Double Deep Q Network | Value-based Off-policy
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| Dueling DQN [[390](#bib.bib390)] | Dueling Deep Q Network | Value-based Off-policy
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| MCTS [[9](#bib.bib9)] | Monte Carlo tree search | Value-based On-policy |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| UCRL-VTR[[167](#bib.bib167)] | optimistic planning problem | Value-based
    Off-policy |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| DDPG [[223](#bib.bib223)] | DQN with Deterministic Policy Gradient | Policy
    gradient Off-policy |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| TRPO [[327](#bib.bib327)] | Trust Region Policy Optimization | Policy gradient
    On-policy |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| PPO [[328](#bib.bib328)] | Proximal Policy Optimization | Policy gradient
    On-policy |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| ME-TRPO [[190](#bib.bib190)] | Model-Ensemble Trust-Region Policy Optimization
    | Policy gradient On-policy |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| MB-MPO [[58](#bib.bib58)] | Model-Based Meta- Policy-Optimization | Policy
    gradient On-policy |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| MB-MPO [[58](#bib.bib58)] | åŸºäºæ¨¡å‹çš„å…ƒç­–ç•¥ä¼˜åŒ– | ç­–ç•¥æ¢¯åº¦åœ¨çº¿ç­–ç•¥ |'
- en: '| A3C [[263](#bib.bib263)] | Asynchronous Advantage Actor Critic | Actor Critic
    On-Policy |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| A3C [[263](#bib.bib263)] | å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå®¶ | æ¼”å‘˜è¯„è®ºå®¶åœ¨çº¿ç­–ç•¥ |'
- en: '| A2C [[263](#bib.bib263)] | Advantage Actor Critic | Actor Critic On-Policy
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| A2C [[263](#bib.bib263)] | ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå®¶ | æ¼”å‘˜è¯„è®ºå®¶åœ¨çº¿ç­–ç•¥ |'
- en: 4.3 Good practices
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 è‰¯å¥½å®è·µ
- en: Inspired by Deep Q-learning [[264](#bib.bib264)], we discuss some useful techniques
    that are used during training an agent in DRL framework in practices.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å—åˆ°æ·±åº¦Qå­¦ä¹ çš„å¯å‘ [[264](#bib.bib264)]ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸€äº›åœ¨å®é™…DRLæ¡†æ¶ä¸­è®­ç»ƒä»£ç†æ—¶ä½¿ç”¨çš„æœ‰ç”¨æŠ€æœ¯ã€‚
- en: Experience replay Experience replay [[417](#bib.bib417)] is a useful part of
    off-policy learning and is often used while training an agent in RL framework.
    By getting rid of as much information as possible from past experiences, it removes
    the correlations in training data and reduces the oscillation of the learning
    procedure. As a result, it enables agents to remember and re-use past experiences
    sometimes in many weights updates which increases data efficiency.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ç»éªŒå›æ”¾ ç»éªŒå›æ”¾ [[417](#bib.bib417)] æ˜¯ç¦»ç­–ç•¥å­¦ä¹ ä¸­çš„ä¸€ä¸ªæœ‰ç”¨éƒ¨åˆ†ï¼Œé€šå¸¸åœ¨RLæ¡†æ¶ä¸­è®­ç»ƒä»£ç†æ—¶ä½¿ç”¨ã€‚é€šè¿‡å°½å¯èƒ½å¤šåœ°æ¶ˆé™¤è¿‡å»ç»éªŒä¸­çš„ä¿¡æ¯ï¼Œå®ƒå»é™¤äº†è®­ç»ƒæ•°æ®ä¸­çš„ç›¸å…³æ€§ï¼Œå‡å°‘äº†å­¦ä¹ è¿‡ç¨‹çš„æ³¢åŠ¨ã€‚å› æ­¤ï¼Œå®ƒä½¿ä»£ç†èƒ½å¤Ÿè®°ä½å¹¶é‡æ–°åˆ©ç”¨è¿‡å»çš„ç»éªŒï¼Œè¿™åœ¨è®¸å¤šæƒé‡æ›´æ–°ä¸­å¢åŠ äº†æ•°æ®æ•ˆç‡ã€‚
- en: Minibatch learning Minibatch learning is a common technique that is used together
    with experience replay. Minibatch allows learning more than one training sample
    at each step, thus, it makes the learning process robust to outliers and noise.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: å°æ‰¹é‡å­¦ä¹  å°æ‰¹é‡å­¦ä¹ æ˜¯ä¸€ç§å¸¸ç”¨æŠ€æœ¯ï¼Œé€šå¸¸ä¸ç»éªŒå›æ”¾ä¸€èµ·ä½¿ç”¨ã€‚å°æ‰¹é‡å­¦ä¹ å…è®¸åœ¨æ¯ä¸€æ­¥å­¦ä¹ å¤šä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä»è€Œä½¿å­¦ä¹ è¿‡ç¨‹å¯¹å¼‚å¸¸å€¼å’Œå™ªå£°å…·æœ‰é²æ£’æ€§ã€‚
- en: 'Target Q-network freezing As described in [[264](#bib.bib264)], two networks
    are used for the training process. In target Q-network freezing: one network interacts
    with the environment and another network plays the role of a target network. The
    first network is used to generate target Q-values that are used to calculate losses.
    The weights of the second network i.e. target network are fixed and slowly updated
    to the first network [[224](#bib.bib224)].'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡Qç½‘ç»œå†»ç»“ å¦‚[[264](#bib.bib264)]æ‰€è¿°ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†ä¸¤ä¸ªç½‘ç»œã€‚åœ¨ç›®æ ‡Qç½‘ç»œå†»ç»“ä¸­ï¼šä¸€ä¸ªç½‘ç»œä¸ç¯å¢ƒäº¤äº’ï¼Œå¦ä¸€ä¸ªç½‘ç»œåˆ™æ‰®æ¼”ç›®æ ‡ç½‘ç»œçš„è§’è‰²ã€‚ç¬¬ä¸€ä¸ªç½‘ç»œç”¨äºç”Ÿæˆç›®æ ‡Qå€¼ï¼Œè¿™äº›Qå€¼ç”¨äºè®¡ç®—æŸå¤±ã€‚ç¬¬äºŒä¸ªç½‘ç»œï¼Œå³ç›®æ ‡ç½‘ç»œçš„æƒé‡æ˜¯å›ºå®šçš„ï¼Œå¹¶ä¸”ç¼“æ…¢æ›´æ–°ä¸ºç¬¬ä¸€ä¸ªç½‘ç»œçš„æƒé‡
    [[224](#bib.bib224)]ã€‚
- en: Reward clipping A reward is the scalar number provided by the environment and
    it aims at optimizing the network. To keep the rewards in a reasonable scale and
    to ensure proper learning, they are clipped to a specific range (-1 ,1). Here
    1 refers to as positive reinforcement or reward and -1 is referred to as negative
    reinforcement or punishment.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å‰ªè¾‘ å¥–åŠ±æ˜¯ç”±ç¯å¢ƒæä¾›çš„æ ‡é‡æ•°å€¼ï¼Œæ—¨åœ¨ä¼˜åŒ–ç½‘ç»œã€‚ä¸ºäº†ä¿æŒå¥–åŠ±åœ¨åˆç†çš„èŒƒå›´å†…å¹¶ç¡®ä¿é€‚å½“çš„å­¦ä¹ ï¼Œå¥–åŠ±è¢«å‰ªè¾‘åˆ°ç‰¹å®šèŒƒå›´ï¼ˆ-1ï¼Œ1ï¼‰ã€‚è¿™é‡Œ1è¡¨ç¤ºæ­£å‘å¼ºåŒ–æˆ–å¥–åŠ±ï¼Œ-1è¡¨ç¤ºè´Ÿå‘å¼ºåŒ–æˆ–æƒ©ç½šã€‚
- en: Model-based v.s. model-free approach Whether the model-free or model-based approaches
    is chosen mainly depends on the model architecture i.e. policy and value function.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¨¡å‹çš„æ–¹æ³•ä¸æ— æ¨¡å‹çš„æ–¹æ³• é€‰æ‹©åŸºäºæ¨¡å‹çš„æ–¹æ³•è¿˜æ˜¯æ— æ¨¡å‹çš„æ–¹æ³•ä¸»è¦å–å†³äºæ¨¡å‹æ¶æ„ï¼Œå³ç­–ç•¥å’Œä»·å€¼å‡½æ•°ã€‚
- en: 5 DRL in Landmark Detection
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 DRLåœ¨åœ°æ ‡æ£€æµ‹ä¸­çš„åº”ç”¨
- en: Autonomous landmark detection has gained more and more attention in the past
    few years. One of the main reasons for this increased inclination is the rise
    of automation for evaluating data. The motivation behind using an algorithm for
    landmarking instead of a person is that manual annotation is a time-consuming
    tedious task and is prone to errors. Many efforts have been made for the automation
    of this task. Most of the works that were published for this task using a machine
    learning algorithm to solve the problem. [[64](#bib.bib64)] proposed a regression
    forest-based method for detecting landmark in a full-body CT scan. Although the
    method was fast it was less accurate when dealing with large organs. [[101](#bib.bib101)]
    extended the work of [[64](#bib.bib64)] by adding statistical shape priors that
    were derived from segmentation masks with cascade regression.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: In order to address the limitations of previous works on anatomy detection,
    [[105](#bib.bib105)] reformulated the detection problem as a behavior learning
    task for an artificial agent using MDP. By using the capabilities of DRL and scale-space
    theory [[226](#bib.bib226)], the optimal search strategies for finding anatomical
    structures are learned based on the image information at multiple scales. In their
    approach, the search starts at the coarsest scale level for capturing global context
    and continues to finer scales for capturing more local information. In their RL
    configuration, the state of the agent at time $t$, $s_{t}=I(\vec{p}_{t})$ is defined
    as an axis-aligned box of image intensities extracted from the image $I$ and centered
    at the voxel-position $\vec{p}_{t}$ in image space. An action $a_{t}$ allows the
    agent to move from any voxel position $\vec{p}_{t}$ to an adjacent voxel position
    $\vec{p}_{t+1}$. The reward function represents distance-based feedback, which
    is positive if the agent gets closer to the target structure and negative otherwise.
    In this work, a CNN is used to extract deep semantic features. The search starts
    with the coarsest scale level $M-1$, the algorithm tries to maximize the reward
    which is the change in distance between ground truth and predicted landmark location
    before and after the action of moving the scale window across the image. Upon
    convergence, the scale level is changed to $M-2$ and the search continued from
    the convergence point at scale level $M-1$. The process is repeated on the following
    scales until convergence on the finest scale. The authors performed experiments
    on 3D CT scans and obtained an average accuracy increase of 20-30$\%$ and lower
    distance error than the other techniques such as SADNN [[104](#bib.bib104)] and
    3D-DL [[427](#bib.bib427)]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Focus on anatomical landmark localization in 3D fetal US images, [[10](#bib.bib10)]
    proposed and demonstrated use cases of several different Deep Q-Network RL models
    to train agents that can precisely localize target landmarks in medical scans.
    In their work, they formulate the landmark detection problem as an MDP of a goal-oriented
    agent, where an artificial agent is learned to make a sequence of decisions towards
    the target point of interest. At each time step, the agent should decide which
    direction it has to move to find the target landmark. These sequential actions
    form a learned policy forming a path between the starting point and the target
    landmark. This sequential decision-making process is approximated under RL. In
    this RL configuration, the environment is defined as a 3D input image, action
    $A$ is a set of six actions $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$ corresponding
    to three directions, the state $s$ is defined as a 3D region of interest (ROI)
    centered around the target landmark and the reward is chosen as the difference
    between the two Euclidean distances: the previous step and current step. This
    reward signifies whether the agent is moving closer to or further away from the
    desired target location. In this work, they also proposed a novel fixed- and multi-scale
    optimal path search strategy with hierarchical action steps for agent-based landmark
    localization frameworks.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Whereas pure policy or value-based methods have been widely used to solve RL-based
    localization problems, [[7](#bib.bib7)] adopts an actor-critic [[262](#bib.bib262)]
    based direct policy search method framed in a temporal difference learning approach.
    In their work, the state is defined as a function of the agent-position which
    allows the agent at any position to observe an $m\times m\times 3$ block of surrounding
    voxels. Similar to other previous work, the action space is $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$.
    The reward is chosen as a simple binary reward function, where a positive reward
    is given if an action leads the agent closer to the target landmark, and a negative
    reward is given otherwise. Far apart from the previous work, their approach proposes
    a non-linear policy function approximator represented by an MLP whereas the value
    function approximator is presented by another MLP stacked on top of the same CNN
    from the policy net. Both policy (actor) and value (critic) networks are updated
    by actor-critic learning. To improve the learning, they introduce a partial policy-based
    RL to enable solving the large problem of localization by learning the optimal
    policy on smaller partial domains. The objective of the partial policy is to obtain
    multiple simple policies on the projections of the actual action space, where
    the projected policies can reconstruct the policy on the original action space.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Based on the hypothesis that the position of all anatomical landmarks is interdependent
    and non-random within the human anatomy and this is necessary as the localization
    of different landmarks requires learning partly heterogeneous policies, [[377](#bib.bib377)]
    concluded that one landmark can help to deduce the location of others. For collective
    gain, the agents share their accumulated knowledge during training. In their approach,
    the state is defined as RoI centered around the location of the agent. The reward
    function is defined as the relative improvement in Euclidean distance between
    their location at time $t$ and the target landmark location. Each agent is considered
    as Partially Observable Markov Decision Process (POMDP) [[107](#bib.bib107)] and
    calculates its individual reward as their policies are disjoint. In order to reduce
    the computational load in locating multiple landmarks and increase accuracy through
    anatomical interdependence, they propose a collaborative multi-agent landmark
    detection framework (Collab-DQN) where DQN is built upon a CNN. The backbone CNN
    is shared across all agents while the policy-making fully connected layers are
    separate for each agent.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Comparing various DRL-based landmark detection methods. The first
    group on Single Landmark Detection (SLD) and the second group for Multiple Landmark
    Detection (MLD)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Performance
    | Datasets and source code |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| SLD [[105](#bib.bib105)] | 2017 | DQN | 6 action: 2 per axis | State: an
    axis-aligned box centered at the voxel-position. Action: move from $\vec{p}_{t}$
    to $\vec{p}_{t+1}$. Reward: distance-based feedback | Average accuracy increase
    20-30%. Lower distance error than other techniques such as SADNN [[104](#bib.bib104)]
    and 3D-DL [[427](#bib.bib427)] | 3D CT Scan |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| SLD [[10](#bib.bib10)] | 2019 | DQN, DDQN, Duel DQN and Duel DDQN | 6 action:
    2 per axis | Environment: 3D input image. State: 3D RoI centered around the target
    landmark. Reward: Euclidean distance between predicted points and groundtruth
    points. | Duel DQN performs the best on Right Cerebellum (FS), Left Cerebellum
    (FS, MS) Duel DDQN is the best on Right Cerebellum (MS) DQN performs the best
    on Cavum Septum Pellucidum(FS, MS) | Fetal head, ultrasound scans [[219](#bib.bib219)].
    [Available code](https://github.com/amiralansary/rl-medical) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| SLD [[7](#bib.bib7)] | 2019 | Actor- Critic -based Partial -Policy RL | 6
    action: 2 per axis | State: a function of the agent-position. Reward: binary reward
    function. policy function: MLP. value function: MLP | Faster and better convergence,
    outperforms than other conventional actor-critic and Q-learning | CT volumes:
    Aortic valve. CT volumes: LAA seed-point. MR images: Vertebra centers [[42](#bib.bib42)].
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| MLD [[377](#bib.bib377)] | 2019 | Collab DQN | 6 action: 2 per axis | State:
    RoI centred around the agent. Reward: relative improvement in Euclidean distance.
    Each Agent is a POMDP has its own reward. Collab-DQN: reduce the computational
    load | Colab DQN got better results than supervised CNN and DQN | Brain MRI landmark
    [[158](#bib.bib158)], Cardiac MRI landmark [[70](#bib.bib70)], Fetal brain landmark
    [[10](#bib.bib10)]. [Available code](https://github.com/thanosvlo/MARL-for-Anatomical-Landmark-Detection)
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| MLD [[161](#bib.bib161)] | 2020 | DQN | 6 action 2 per axis | State: 3D image
    patch. Reward: Euclidean distance and $\in[-1,1]$. Backbone CNN is share among
    agents Each agent has it own Fully connected layer | Detection error increased
    as the degree of missing information increased Performance is affected by the
    choice of landmarks | 3D Head MR images |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: 'Different from the previous works on RL-based landmark detection, which detect
    a single landmark,[[161](#bib.bib161)] proposed a multiple landmark detection
    approach to better time-efficient and more robust to missing data. In their approach,
    each landmark is guided by one agent. The MDP is models as follows: The state
    is defined as a 3D image patch. The reward, clipped in [-1, +1], is defined as
    the difference in the Euclidean distance between the landmark predicted in the
    previous time step and the target, and in the landmark predicted in the current
    time step and the target. The action space is defined as in other previous works
    i.e. there are 6 actions $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$ in the action
    space. To enable the agents to share the information learned by detecting one
    landmark for use in detecting other landmarks, hard parameter sharing from multi-task
    learning is used. In this work, the backbone network is shared among agents and
    each agent has its own fully connected layer.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Table LABEL:tab:landmark summarizes and compares all approaches for DRL in
    landmark detection, and a basic implementation of landmark detection using DRL
    has been shown in Fig. [11](#S5.F11 "Figure 11 â€£ 5 DRL in Landmark Detection â€£
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure illustrates a general implementation of landmark detection with the help
    of DRL, where the state is the Region of interest (ROI) around the current landmark
    location cropped from the image, The actions performed by the DRL agent are responsible
    for shifting the ROI across the image forming a new state and the reward corresponds
    to the improvement in euclidean distance between ground truth and predicted landmark
    location with iterations as used by [[105](#bib.bib105)],[[7](#bib.bib7)],[[10](#bib.bib10)],[[377](#bib.bib377)],[[161](#bib.bib161)].'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9972fc9734de5c35a7ada06c3f6700c.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: DRL implementation for landmark detection, The red point corresponds
    to the current landmark location and Red box is the Region of Interest (ROI) centered
    around the landmark, the actions of DRL agent shift the ROI across the image to
    maximize the reward corresponding to the improvement in distance between the ground
    truth and predicted landmark location.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 6 DRL in Object Detection
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object detection is a task that requires the algorithm to find bounding boxes
    for all objects in a given image. Many attempts have been made towards object
    detection. A method for bounding box prediction for object detection was proposed
    by [[109](#bib.bib109)], in which the task was performed by extracting region
    proposals from an image and then feeding each of them to a CNN to classify each
    region. An improvement to this technique was proposed by [[108](#bib.bib108)],
    where they used the feature from the CNN to propose region proposals instead of
    the image itself, this resulted in fast detection. Further improvement was proposed
    by [[309](#bib.bib309)], where the authors proposed using a region proposal network
    (RPN) to identify the region of interest, resulting in much faster detection.
    Other attempts including focal loss [[225](#bib.bib225)] and Fast YOLO [[332](#bib.bib332)]
    have been proposed to address the imbalanced data problem in object detection
    with focal loss [[225](#bib.bib225)], and perform object detection in video on
    embedded devices in a real-time manner [[332](#bib.bib332)].
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Considering MDP as the framework for solving the problem, [[43](#bib.bib43)]
    used DRL for active object localization. The authors considered 8 different actions
    (up, down, left, right, bigger, smaller, fatter, taller) to improve the fit of
    the bounding box around the object and additional action to trigger the goal state.
    They used a tuple of feature vector and history of actions for state and change
    in IOU across actions as a reward.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: An improvement to [[43](#bib.bib43)] was proposed by [[25](#bib.bib25)], where
    the authors used a hierarchical approach for object detection by treating the
    problem of object detection as an MDP. In their method, the agent was responsible
    to find a region of interest in the image and then reducing the region of interest
    to find smaller regions from the previously selected region and hence forming
    a hierarchy. For the reward function, they used the change in Intersection over
    union (IOU) across the actions and used DQN as the agent. As described in their
    paper, two networks namely, Image-zooms and Pool45-crops with VGG-16 [[340](#bib.bib340)]
    backbone were used to extract the feature information that formed the state for
    DQN along with a memory vector of the last four actions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a sequential search strategy, [[251](#bib.bib251)] proposed a method
    for object detection using DRL. The authors trained the model with a set of image
    regions where at each time step the agent returned fixate actions that specified
    a location in image for actor to explore next and the terminal state was specified
    by $done$ action. The state consisted of a tuple three elements: the observed
    region history $H_{t}$, selected evidence region history $E_{t}$ and fixate history
    $F_{t}$. The $fixate$ action was also a tuple of three elements: $fixate$ action,
    index of evidence region $e_{t}$ and image coordinate of next fixate $z_{t}$.
    The $done$ action consisted of: $done$ action, index of region representing the
    detected output $b_{t}$ and the detection confidence $c_{t}$. The authors defined
    the reward function that was sensitive to the detection location, the confidence
    at the final state and incurs a penalty for each region evaluation.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'To map the inter-dependencies among the different objects, [[170](#bib.bib170)]
    proposed a tree-structured RL agent (Tree-RL) for object localization by considering
    the problem as an MDP. The authors in their implementation considered actions
    of two types: translation and scaling, where the scaling consisted of five actions
    whereas translation consisted of eight actions. In the specified work, the authors
    used the state as a concatenation of the feature vector of the current window,
    feature vector of the whole image, and history of taken actions. The feature vector
    were extracted from an ImageNet [[72](#bib.bib72)] [[320](#bib.bib320)] trained
    VGG-16 [[340](#bib.bib340)] model and for reward the change in IOU across an action
    was used. Tree-RL utilized a top-down tress search starting from the whole image
    where each window recursively takes the best action from each action group which
    further gives two new windows. This process is repeated recursively to find the
    object.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: The task of breast lesion detection is a challenging yet very important task
    in the medical imaging field. A DRL method for active lesion detection in the
    breast was proposed by [[246](#bib.bib246)], where the authors formulated the
    problem as an MDP. In their formulation, a total of nine actions consisting of
    6 translation actions, 2 scaling actions, and 1 trigger action were used. In the
    specified work, the change in dice coefficient across an action was used as the
    reward for scaling and translation actions, and for trigger action, the reward
    was $+\eta$ for dice coefficient greater than $r_{w}$ and $-\eta$ otherwise, where
    $\eta$ and $r_{w}$ were the hyperparameters chosen by the authors. For network
    structure, ResNet [[133](#bib.bib133)] was used as the backbone and DQN as the
    agent.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Different from the previous methods, [[386](#bib.bib386)] proposed a method
    for multitask learning using DRL for object localization. The authors considered
    the problem as an MDP where the agent was responsible to perform a series of transformations
    on the bounding box using a series of actions. Utilizing an RL framework the states
    consisted of feature vector and historical actions concatenated together, and
    a total of 8 actions for Bounding box transformation (left, right, up, down, bigger,
    smaller, fatter, and taller) were used. For reward the authors used the change
    in IOU between actions, the reward being 0 for an increase in IOU and -1 otherwise.
    For terminal action, however, the reward was 8 for IOU greater than 0.5 and -8
    otherwise. The authors in the paper used DQN with multitask learning for localization
    and divided terminal action and 8 transformation actions into two networks and
    trained them together.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: An improvement for the Region proposal networks that greedily select the ROIs
    was proposed by [[295](#bib.bib295)], where they used RL for the task. The authors
    in this paper used a two-stage detector similar to Fast and Faster R-CNN But used
    RL for the decision-making Process. For the reward, they used the normalized change
    in Intersection over Union (IOU).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Instead of learning a policy from a large set of data, [[15](#bib.bib15)] proposed
    a method for bounding box refinement (BAR) using RL. In the paper, once the authors
    have an inaccurate bounding box that is predicted by some algorithm they use the
    BAR algorithm to predict a series of actions for refinement of a bounding box.
    They considered a total of 8 actions (up, down, left, right, wider, taller, fatter,
    thinner) for bounding box transformation and considered the problem as a sequential
    decision-making problem (SDMP). They proposed an offline method called BAR-DRL
    and an online method called BAR-CB where training is done on every image. In BAR-DRL
    the authors trained a DQN over the states which consisted of features extracted
    from ResNet50 [[133](#bib.bib133)] [[354](#bib.bib354)] pretrained on ImageNet
    [[72](#bib.bib72)] [[320](#bib.bib320)] and a history vector of 10 actions. The
    Reward for BAR-DRL was 1 if the IOU increase after action and -3 otherwise. For
    BAR-CB they adapted the LinUCB [[216](#bib.bib216)] algorithm for an episodic
    scenario and considered The Histogram of Oriented Gradients (HOG) for the state
    to capture the outline and edges of the object of interest. The actions in the
    online method (BAR-CB) were the same as the offline method and the reward was
    1 for increasing IOU and 0 otherwise. For both the implementations, the authors
    considered $\beta$ as terminal IOU.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: An improvement to sequential search strategy by [[251](#bib.bib251)] was proposed
    by [[367](#bib.bib367)], where they used a framework consisting of two modules,
    Coarse and fine level search. According to the authors, this method is efficient
    for object detection in large images (dimensions larger than 3000 pixels). The
    authors first performed a course level search on a large image to find a set of
    patches that are used by fine level search to find sub-patches. Both fine and
    coarse levels were conducted using a two-step episodic MDP, where The policy network
    was responsible for returning the probability distribution of all actions. In
    the paper, the authors considered the actions to be the binary action array (0,1)
    where 1 means that the agent would consider acquiring sub-patches for that particular
    patch. The authors in their implementation considered a number of patches and
    sub-patches as 16 and 4 respectively and used the linear combination of $R_{acc}$
    (detection recall) and $R_{cost}$ which combines image acquisition cost and run-time
    performance reward.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparing various DRL-based object detection methods'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and source code |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| Active Object Localization [[43](#bib.bib43)] | 2015 | DQN | 8 actions: up,
    down, left, right, bigger, smaller, fatter, taller | States: feature vector of
    observed region and action history. Reward: Change in IOU. | 5 layer pretrained
    CNN | Higher mAP as compared to methods that did not use region proposals like
    MultiBox [[89](#bib.bib89)], RegionLets [[433](#bib.bib433)], DetNet [[356](#bib.bib356)],
    and second best mAP as compared to R-CNN [[109](#bib.bib109)] | Pascal VOC-2007
    [[90](#bib.bib90)], 2012 [[91](#bib.bib91)] Image Dataset. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical Object Detection [[25](#bib.bib25)] | 2016 | DQN | 5 actions:
    1 action per image quarter and 1 at the center | States: current region and memory
    vector using Image-zooms and Pool45-crops. Reward: change in IOU. | VGG-16 [[340](#bib.bib340)]
    | Objects detected with very few region proposals per image | Pascal VOC-2007
    Image Dataset [[90](#bib.bib90)]. [Available Code](https://github.com/imatge-upc/detection-2016-nipsws)
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| Visual Object Detection [[251](#bib.bib251)] | 2016 | Policy sampling and
    state transition algorithm | 2 actions: fixate and done, where each is a tuple
    of three. | States: Observed region history, evidence region history and fixate
    history. Reward: sensitive to detection location | Deep NN [[187](#bib.bib187)]
    | Comparable mAP and lower run time as compared to other methods such as to exhaustive
    sliding window search(SW), exhaustive search over the CPMC and region proposal
    set(RP) [[112](#bib.bib112)]  [[366](#bib.bib366)] | Pascal VOC 2012 Object detection
    challenge [[91](#bib.bib91)]. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| Tree-Structured Sequential Object Localization (Tree-RL) [[170](#bib.bib170)]
    | 2016 | DQN | 13 actions: 8 translation, 5 scaling. | States: Feature vector
    of current region, and whole image. Reward: change in IOU. | CNN trained on ImageNet
    [[72](#bib.bib72)]  [[320](#bib.bib320)] | Tree-RL with faster R-CNN outperformed
    RPN with fast R-CNN [[108](#bib.bib108)] in terms of AP and comparable results
    to Faster R-CNN [[309](#bib.bib309)] | Pascal VOC 2007 [[90](#bib.bib90)] and
    2012 [[91](#bib.bib91)]. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| Active Breast Lesion Detection [[246](#bib.bib246)] | 2017 | DQN | 9 actions:
    6 translation, 2 scaling, 1 trigger | States: feature vector of current region,
    Reward: improvement in localization. | ResNet [[133](#bib.bib133)] | Comparable
    true positive and false positive proportions as compared to SL [[253](#bib.bib253)]
    and Ms-C [[116](#bib.bib116)], but with lesser mean inference time. | DCE-MRI
    and T1-weighted anatomical dataset [[253](#bib.bib253)] |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| Multitask object localization [[386](#bib.bib386)] | 2018 | DQN | 8 actions:
    left, right, up, down, bigger, smaller, fatter and taller | States: feature vector,
    historical actions. Reward: change in IOU. different network for transformation
    actions and terminal actions. | Pretrained VGG-16 [[340](#bib.bib340)] with ImageNet
    [[72](#bib.bib72)]  [[320](#bib.bib320)] | Better mAP as compared to MultiBox
    [[89](#bib.bib89)], Caicedo et al. [[43](#bib.bib43)] and second best to R-CNN
    [[109](#bib.bib109)]. | Pascal VOC-2007 Image Dataset [[90](#bib.bib90)]. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| Bounding-Box Automated Refinement [[15](#bib.bib15)] | 2020 | DQN | 8 actions:
    up, down, left, right, bigger, smaller, fatter, taller | Offline and online implementation
    States: feature vector for offline (BAR-DRL), HOG for online (BAR-CB). Reward:
    change in IOU | ResNet50 [[133](#bib.bib133)] | Better final IOU for boxes generated
    by methods such as RetinaNet [[225](#bib.bib225)]. | Pascal VOC-2007 [[90](#bib.bib90)],
    2012 [[91](#bib.bib91)] Image Dataset. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| Efficient Object Detection in Large Images [[367](#bib.bib367)] | 2020 |
    DQN | binary action array: where 1 means that the agent would consider acquiring
    sub-patches for that particular patch | Course CPNet and fine FPNet level search.
    States: selected region. Reward: detection recall image acquisition cost. Policy:
    REINFORCE [[351](#bib.bib351)] | ResNet32 [[133](#bib.bib133)] for policy network.
    and YOLOv3 [[306](#bib.bib306)] with DarkNet-53 for Object detector | Higher mAP
    and lower run time as compared to other methods such as [[99](#bib.bib99)]. |
    Caltech Pedestrian dataset (CPD) [[77](#bib.bib77)] [Available Code](https://github.com/uzkent/EfficientObjectDetection)
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| Organ Localization in CT [[275](#bib.bib275)] | 2020 | DQN | 11 actions:
    6 translation, 2 scaling, 3 deformation | States: region inside the Bounding box.
    Reward: change in IOU. | Architecture similar to [[10](#bib.bib10)] | Lower distance
    error for organ localization and run time as compared to other methods such as
    3D-RCNN [[409](#bib.bib409)] and CNNs [[152](#bib.bib152)] | CT scans from the
    VISCERAL dataset [[171](#bib.bib171)] |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| Monocular 3D Object Detection [[231](#bib.bib231)] | 2020 | DQN [[264](#bib.bib264)]
    | 15 actions, each modifies the 3D bounding box in a specific parameter | State:
    3D bounding box parameters, 2D image of object cropped by 2D its detected bounding
    box. Reward: accuracy improvement after applying an action. | ResNet-101 [[133](#bib.bib133)]
    | Higher average precision (AP) compared to [[268](#bib.bib268)], [[302](#bib.bib302)],
    [[210](#bib.bib210)] and [[35](#bib.bib35)] | KITTI [[102](#bib.bib102)] |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: Localization of organs in CT scans is an important pre-processing requirement
    for taking the images of an organ, planning radiotherapy, etc. A DRL method for
    organ localization was proposed by [[275](#bib.bib275)], where the problem was
    formulated as an MDP. In the implementation, the agent was responsible for predicting
    a 3D bounding box around the organ. The authors used the last 4 states as input
    to the agent to stabilize the search and the action space consists of Eleven actions,
    6 for the position of the bounding box, 2 for zoom in and zoom out the action,
    and last 3 for height, width, and depth. For Reward, they used the change the
    in Intersection over union (IOU) across an action.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Monocular 3D object detection is a problem where 3D bounding boxes of objects
    are required to be detected from a single 2D image. Even the sampling-based method
    is the SOTA approach, it has a huge flaw, in which most of the samples it generates
    do not overlap with the groundtruth. To leverage that method, [[231](#bib.bib231)]
    introduced Reinforced Axial Refinement Network (RARN) for monocular 3D object
    detection by utilizing an RL model to iteratively refining the sampled bounding
    box to be more overlapped with the groundtruth bounding box. Given a state having
    the coordinates of the 3D bounding box and image patch of the image, the model
    predicts an action out of a set of 15 actions to refine one of the bounding box
    coordinates in a direction at every timestep, the model is trained by DQN method
    with the immediate reward is the improvement in detection accuracy between every
    pair of timesteps. The whole pipeline, namely RAR-Net, was evaluated on the real-world
    KITTI dataset [[102](#bib.bib102)] and achieved state-of-the-art performance.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'All these methods have been summarised and compared in Table LABEL:tab:obs,
    and a basic implementation of object detection using DRL has been shown in Fig.
    [12](#S6.F12 "Figure 12 â€£ 6 DRL in Object Detection â€£ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey"). The figure illustrates a general
    implementation of object detection using DRL, where the state is an image segment
    cropped using a bounding box produced by some other algorithm or previous iteration
    of DRL, actions predicted by the DRL agent predict a series of bounding box transformation
    to fit the object better, hence forming a new state and Reward is the improvement
    in Intersection over union (IOU) with iterations as used by [[43](#bib.bib43)],[[25](#bib.bib25)],[[15](#bib.bib15)],[[386](#bib.bib386)],[[170](#bib.bib170)],[[275](#bib.bib275)].'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58a1d5d89631a83968289981f8c75065.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: DRL implementation for object detection. The red box corresponds
    to the initial bounding box which for t=0 is predicted by some other algorithm
    or the transformed bounding box by previous iterations of DRL using the actions
    to maximize the improvement in IOU.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 7 DRL in Object Tracking
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Real-time object tracking has a large number of applications in the field of
    autonomous driving, robotics, security, and even in sports where the umpire needs
    accurate estimation of ball movement to make decisions. Object tracking can be
    divided into two main categories: Single object tracking (SOT) and Multiple object
    tracking (MOT).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Many attempts have been made for both SOT and MOT. SOT can be divided into two
    types, active and passive. In passive tracking it is assumed that the object that
    is being tracked is always in the camera frame, hence camera movement is not required.
    In active tracking, however, the decision to move the camera frame is required
    so that the object is always in the frame. Passive tracking has been performed
    by [[397](#bib.bib397)], [[146](#bib.bib146)], where [[146](#bib.bib146)] performed
    tracking for both single and multiple objects. The authors of these papers proposed
    various solutions to overcome common problems such as a change in lighting and
    occlusion. Active tracking is a little bit harder as compared to a passive one
    because additional decisions are required for camera movement. Some efforts towards
    active tracking include [[74](#bib.bib74)] [[270](#bib.bib270)] [[178](#bib.bib178)].
    These solutions treat object detection and object tracking as two separate tasks
    and tend to fail when there is background noise.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: An end-to-end active object tracker using DRL was proposed by [[240](#bib.bib240)],
    where the authors used CNNs along with an LSTM [[139](#bib.bib139)] in their implementation.
    They used the actor-critic algorithm [[262](#bib.bib262)] to calculate the probability
    distribution of different actions and the value of state and used the object orientation
    and distance from the camera to calculate rewards. For experiments, the authors
    used VizDoom and Unreal Engine as the environment.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Another end-to-end method for SOT using sequential search strategy and DRL was
    proposed by [[418](#bib.bib418)]. The method included using an RNN along with
    REINFORCE [[392](#bib.bib392)] algorithm to train the network. The authors used
    a function $f(W_{0})$ that takes in $S_{t}$ and frame as input, where $S_{t}$
    is the object location for the first frame and is zero elsewhere. The output is
    fed to an LSTM module [[139](#bib.bib139)] with past hidden state $h_{t}$. The
    authors calculated the reward function by using insertion over union (IoU) and
    the difference between the average and max.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'A deformable face tracking method that could predict bounding box along with
    facial landmarks in real-time was proposed by [[118](#bib.bib118)]. The dual-agent
    DRL method (DADRL) mentioned in the paper consisted of two agents: a tracking
    and an alignment agent. The problem of object tracking was formulated as an MDP
    where state consisted of image regions extracted by the bounding box and a total
    of 8 actions (left, right, up, down, scale-up, scale down, stop and continue)
    were used, where first six consists of movement actions used by tracking agent
    and last two for alignment agent. The tracking agent is responsible for changing
    the current observable region and the alignment agent determines whether the iteration
    should be terminated. For the tracking agent, the reward corresponded to the misalignment
    descent and for the alignment agent the reward was $+\eta$ for misalignment less
    than the threshold and $-\eta$ otherwise. The DADRL implementation also consisted
    of communicated message channels beside the tracking agent and the alignment agent.
    The tracking agent consisted of a VGG-M [[340](#bib.bib340)] backbone followed
    by a one-layer Q-Network and the alignment agent was designed as a combination
    of a stacked hourglass network with a confidence network. The two communicated
    message channels were encoded by a deconvolution layer and an LSTM unit [[139](#bib.bib139)]
    respectively.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual object tracking when dealing with deformations and abrupt changes can
    be a challenging task. A DRL method for object tracking with iterative shift was
    proposed by [[308](#bib.bib308)]. The approach (DRL-IS) consisted of three networks:
    The actor network, the prediction network, and the critic network, where all three
    networks shared the same CNN and a fully connected layer. Given the initial frame
    and bounding box, the cropped frame is fed to the CNNs to extract the features
    to be used as a state by the networks. The actions included continue, stop and
    update, stop and ignore, and restart. For continue, the bounding boxes are adjusted
    according to the output of the prediction network, for stop and update the iteration
    is stopped and the appearance feature of the target is updated according to the
    prediction network, for stop and ignore the updating of target appearance feature
    is ignored and restart means that the target is lost and the algorithm needs to
    start from the initial bounding box. The authors of the paper used reward as 1
    for change in IoU greater than the threshold, 0 for change in IOU between + and
    - threshold, and -1 otherwise.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Considering the performance of actor-critic framework for various applications,
    [[45](#bib.bib45)] proposed an actor-critic [[262](#bib.bib262)] framework for
    real-time object tracking. The authors of the paper used a pre-processing function
    to obtain an image patch using the bounding box that is fed into the network to
    find the bounding box location in subsequent frames. For actions the authors used
    $\triangle x$ for relative horizontal translation, $\triangle y$ for relative
    vertical translation, and $\triangle s$ for relative scale change, and for a reward
    they used 1 for IoU greater than a threshold and -1 otherwise. They proposed offline
    training and online tracking, where for offline training a pre-trained VGG-M [[340](#bib.bib340)]
    was used as a backbone, and the actor-critic network was trained using the DDPG
    approach [[224](#bib.bib224)].
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'An improvement to [[45](#bib.bib45)] for SOT was proposed by [[84](#bib.bib84)],
    where a visual tracker was formulated using DRL and an expert demonstrator. The
    authors treated the problem as an MDP, where the state consists of two consecutive
    frames that have been cropped using the bounding box corresponding to the former
    frame and used a scaling factor to control the offset while cropping. The actions
    consisted of four elements: $\triangle x$ for relative horizontal translation,
    $\triangle y$ for relative vertical translation, $\triangle w$ for width scaling,
    and $\triangle h$ for height scaling, and the reward was calculated by considering
    whether the IoU is greater than a threshold or not. For the agent architecture
    the authors used a ResNet-18 [[133](#bib.bib133)] as backbone followed by an LSTM
    unit [[391](#bib.bib391)][[139](#bib.bib139)] to encode past information, and
    performed training based on the on-policy A3C framework [[262](#bib.bib262)].'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'In MOT the algorithm is responsible to track trajectories of multiple objects
    in the given video. Many attempts have been made with MOT including [[53](#bib.bib53)],
    [[55](#bib.bib55)] and [[143](#bib.bib143)]. However, MOT is a challenging task
    because of environmental constraints such as crowding or object overlapping. MOT
    can be divided into two main techniques: Offline [[53](#bib.bib53)] and Online
    [[55](#bib.bib55)] [[143](#bib.bib143)]. In offline batch, tracking is done using
    a small batch to obtain tracklets and later all these are connected to obtain
    a complete trajectory. The online method includes using present and past frames
    to calculate the trajectory. Some common methods include Kalman filtering [[177](#bib.bib177)],
    Particle Filtering [[284](#bib.bib284)] or Markov decision [[401](#bib.bib401)].
    These techniques however are prone to errors due to environmental constraints.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the constraints of MOT by previous methods, [[401](#bib.bib401)]
    proposed a method for MOT where the problem was approached as an MDP. The authors
    tracked each object in the frame through the Markov decision process, where each
    object has four states consisting: Active, Tracked, Lost, and Inactive. Object
    detection is the active state and when the object is in the lost state for a sufficient
    amount of time it is considered Inactive, which is the terminal state. The reward
    function in the implementation was learned through data by inverse RL problem
    [[279](#bib.bib279)].'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Previous approaches for MOT follow a tracking by detection technique that is
    prone to errors. An improvement was proposed by [[307](#bib.bib307)], where detection
    and tracking of the objects were carried out simultaneously. The authors used
    a collaborative Q-Network to track trajectories of multiple objects, given the
    initial position of an object the algorithm tracked the trajectory of that object
    in all subsequent frames. For actions the authors used $\triangle x$ for relative
    horizontal translation, $\triangle y$ for relative vertical translation, $\triangle
    w$ for width scaling, and $\triangle h$ for height scaling, and the reward consisted
    of values 1,0,-1 based on the IoU.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method for MOT was proposed by [[168](#bib.bib168)], where the authors
    used LSTM [[139](#bib.bib139)] and DRL to approach the problem of multi-object
    tracking. The method described in the paper used three basic components: a YOLO
    V2 [[260](#bib.bib260)] object detector, many single object trackers, and a data
    association module. Firstly the YOLO V2 object detector is used to find objects
    in a frame, then each detected object goes through the agent which consists of
    CNN followed by an LSTM to encode past information for the object. The state consisted
    of the image patch and history of past 10 actions, where six actions (right, left,
    up, down, scale-up, scale down) were used for bounding box movement across the
    frame with a stop action for the terminal state. To provide reinforcement to the
    agent the reward was 1 if the IOU is greater than a threshold and 0 otherwise.
    In their experiments, the authors used VGG-16 [[340](#bib.bib340)] for CNN backbone
    and performed experiments on MOT benchmark [[201](#bib.bib201)] for people tracking.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparing various DRL-based object tracking methods. The First group
    for Single object tracking (SOT) and the second group for multi-object tracking
    (MOT)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and Source code |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| End to end active object tracking [[240](#bib.bib240)] | 2017 | Actor-Critic
    (a3c) [[262](#bib.bib262)] | 6 actions: turn left, turn right, turn left and move
    forward, turn right and move forward, move forward, no-op | Environment: virtual
    environment. Reward: calculated using object orientation and position. Tracking
    Using LSTM [[139](#bib.bib139)] | ConvNet-LSTM | Higher accumulated reward and
    episode length as compared to methods like MIL [[17](#bib.bib17)], Meanshift [[60](#bib.bib60)],
    KCF [[134](#bib.bib134)]. | ViZDoom [[176](#bib.bib176)], Unreal Engine |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| DRL for object tracking [[418](#bib.bib418)] | 2017 | DRLT | None | State:
    feature vector, Reward: change in IOU use of LSTM [[139](#bib.bib139)] and REINFORCE
    [[392](#bib.bib392)] | YOLO network [[305](#bib.bib305)] | Higher area under curve
    (success rate Vs overlap threshold), precision and speed (fps) as compared to
    STUCK [[126](#bib.bib126)] and DLT [[384](#bib.bib384)]. | Object tracking benchmark
    [[397](#bib.bib397)]. [Available Code](https://github.com/fgabel/Deep-Reinforcement-Learning-for-Visual-Object-Tracking-in-Videos)
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| Dual-agent deformable face tracker [[118](#bib.bib118)] | 2018 | DQN | 8
    actions: left, right, up, down, scale up, scale down, stop and continue. | States:
    image region using Bounding box. Reward: distance error. Facial landmark detection
    and tracking using LSTM [[139](#bib.bib139)] | VGG-M [[340](#bib.bib340)] | Lower
    normalized point to point error for landmarks and higher success rate for facial
    tracking as compared to ICCR [[187](#bib.bib187)], MDM [[336](#bib.bib336)], Xiao
    et al [[32](#bib.bib32)], etc. | Large-scale face tracking dataset, the 300-VW
    test set [[336](#bib.bib336)] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| Tracking with iterative shift [[308](#bib.bib308)] | 2018 | Actor-critic
    [[262](#bib.bib262)] | 4 actions: continue, stop and update, stop and ignore and
    restart | States: image region using bounding box. Reward: change in IOU. Three
    networks: actor, critic and prediction network | 3 Layer CNN and FC layer | Higher
    area under curve for success rate Vs overlap threshold and precision Vs location
    error threshold as compared to CREST [[345](#bib.bib345)], ADNet [[416](#bib.bib416)],
    MDNet [[273](#bib.bib273)], HCFT [[243](#bib.bib243)], SINT [[358](#bib.bib358)],
    DeepSRDCF [[67](#bib.bib67)], and HDT [[301](#bib.bib301)] | OTB-2015 [[398](#bib.bib398)],
    Temple-Color [[220](#bib.bib220)], and VOT-2016 Dataset [[186](#bib.bib186)] |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| Tracking with actor-critic [[45](#bib.bib45)] | 2018 | Actor-critic [[262](#bib.bib262)]
    | 3 actions: $\triangle x$, $\triangle y$ and $\triangle s$ | States: image region
    using bounding box. Reward: IOU greater then threshold. Offline training, online
    tracking | VGG-M [[340](#bib.bib340)] | Higher average precision score then PTAV
    [[93](#bib.bib93)], CFNet [[368](#bib.bib368)], ACFN [[52](#bib.bib52)], SiameFC
    [[29](#bib.bib29)], ECO-HC [[67](#bib.bib67)], etc. | OTB-2013 [[397](#bib.bib397)],
    OTB-2015 [[398](#bib.bib398)] and VOT-2016 dataset [[186](#bib.bib186)] [Available
    Code](https://github.com/bychen515/ACT) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| Visual tracking and expert demonstrator [[84](#bib.bib84)] | 2019 | Actor-critic
    (a3c) [[262](#bib.bib262)] | 4 actions: $\triangle x$, $\triangle y$,$\triangle
    w$ and $\triangle h$ | States: image region using bounding box. Reward: change
    in IOU. SOT using LSTM [[391](#bib.bib391)][[139](#bib.bib139)] | ResNet-18 [[133](#bib.bib133)]
    | Comparable success and precision scores as compared to LADCF [[408](#bib.bib408)],
    SiamRPN [[209](#bib.bib209)] and ECO [[66](#bib.bib66)] | GOT-10k [[148](#bib.bib148)],
    LaSOT [[92](#bib.bib92)], UAV123 [[269](#bib.bib269)], OTB-100 [[397](#bib.bib397)],
    VOT-2018 [[185](#bib.bib185)] and VOT-2019. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| Object tracking by decision making [[401](#bib.bib401)] | 2015 | TLD Tracker
    [[174](#bib.bib174)] | 7 actions: corresponding to moving the object between states
    such as Active, tracked, lost and Inactive | States: 4 states: Active, tracked,
    lost and Inactive. Reward: inverse RL problem [[279](#bib.bib279)] | None | Comparable
    multiple object tracking accuracy (MOTA) and multiple object tracking precision
    (MOTP) [[28](#bib.bib28)] as compared to DPNMS [[296](#bib.bib296)], TCODAL [[18](#bib.bib18)],
    SegTrack [[259](#bib.bib259)], MotiCon [[200](#bib.bib200)], etc | M0T15 dataset
    [[201](#bib.bib201)] [Available Code](https://github.com/yuxng/MDP_Tracking) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| Collaborative multi object tracker [[307](#bib.bib307)] | 2018 | DQN | 4
    actions: $\triangle x$, $\triangle y$, $\triangle w$ and $\triangle h$ | States:
    image region using bounding box. Reward: IOU greater then threshold. 2 networks:
    prediction and decision network | 3 Layer CNN and FC Layer | Comparable multiple
    object tracking accuracy (MOTA) and multiple object tracking precision (MOTP)
    [[28](#bib.bib28)] as compared to SCEA [[143](#bib.bib143)], MDP [[401](#bib.bib401)],
    CDADDALpb [[19](#bib.bib19)], AMIR15 [[321](#bib.bib321)] | MOT15 [[201](#bib.bib201)]
    and MOT16 [[258](#bib.bib258)] datasets |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| Multi object tracking in video [[168](#bib.bib168)] | 2018 | DQN | 6 actions:
    right, left, up, down, scale up, scale down | States: image region using bounding
    box. Reward: IOU greater then threshold. Detection using YOLO-V2 [[260](#bib.bib260)]
    for detector and LSTM [[139](#bib.bib139)] . | VGG-16 [[340](#bib.bib340)] | Comparable
    if not better multiple object tracking accuracy (MOTA) and multiple object tracking
    precision (MOTP) [[28](#bib.bib28)] as compared to RNN-LSTM [[201](#bib.bib201)],
    LP-SSVM [[401](#bib.bib401)], MDPSubCNN [[199](#bib.bib199)], and SiameseCNN [[123](#bib.bib123)]
    | MOT15 Dataset [[201](#bib.bib201)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| Multi agent multi object tracker [[169](#bib.bib169)] | 2019 | DQN | 9 actions:
    move right, move left, move up, move down, scale up, scale down, fatter, taller
    and stop | States: image region using bounding box. Reward: IOU greater then threshold.
    YOLO-V3 [[306](#bib.bib306)] for detection and LSTM [[139](#bib.bib139)]. | VGG-16
    [[340](#bib.bib340)] | Higher running time, and comparable if not better multiple
    object tracking accuracy (MOTA) and multiple object tracking precision (MOTP)
    [[28](#bib.bib28)] as compared to RNN-LSTM [[201](#bib.bib201)], LP-SSVM [[401](#bib.bib401)],
    MDPSubCNN [[199](#bib.bib199)], and SiameseCNN [[123](#bib.bib123)] | MOT15 challenge
    benchmark [[201](#bib.bib201)]. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: To address the problems in existing tracking methods such as varying numbers
    of targets, non-real-time tracking, etc, [[169](#bib.bib169)] proposed a multi-object
    tracking algorithm based on a multi-agent DRL tracker (MADRL). In their object
    tracking pipeline the authors used YOLO-V3 [[306](#bib.bib306)] as object detector,
    where multiple detections produced by YOLO-V3 were filtered using the IOU and
    the selected results were used as multiple agents in multiple agent detector.
    The input agents were fed into a pre-trained VGG-16 [[340](#bib.bib340)] followed
    by an LSTM unit [[139](#bib.bib139)] that could share information across agents
    and return the actions encoded in a 9-dimensional vector( move right, move left,
    move up, move down, scale-up, scale down, aspect ratio change fatter, aspect ratio
    change taller and stop), also a reward function similar to [[168](#bib.bib168)]
    was used.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Various works in the field of object tracking have been summarized in Table
    LABEL:tab:track, and a basic implementation of object tracking using DRL has been
    shown in Fig. [13](#S7.F13 "Figure 13 â€£ 7 DRL in Object Tracking â€£ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). The figure illustrates
    a general implementation of object tracking in videos using DRL, where the state
    consists of two consecutive frames $(F_{t},F_{t+1})$ with a bounding box for the
    first frame produced by another algorithm for the first iteration or by the previous
    iterations of DRL agent. The actions corresponds to the moving the bounding on
    the image to fit the object in frame $F_{t+1}$, hence forming a new state with
    frame $F_{t+1}$ and frame $F_{t+2}$ along with the bounding box for frame $F_{t+1}$
    predicted by previous iteration and reward corresponds to whether IOU is greater
    then a given threshold as used by [[118](#bib.bib118)],[[308](#bib.bib308)],[[45](#bib.bib45)],
    [[84](#bib.bib84)],[[307](#bib.bib307)],[[168](#bib.bib168)],[[169](#bib.bib169)].'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2136103d0d64b38351fdb4369ef5e52.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: DRL implementation for object tracking. Here the state consists
    of two consecutive frames with bounding box locations for the first frame that
    is predicted by some object detection algorithm or by the previous iteration of
    DRL, the actions move the bounding box present in the first frame to fit the object
    in the second frame to maximize the reward which is the whether the IOU is greater
    than a given threshold or not.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 8 DRL in Image Registration
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image registration is a very useful step that is performed on 3D medical images
    for the alignment of two or more images. The goal of 3D medical image registration
    is to find a correlation between two images from either different patients or
    the same patients at different times, where the images can be Computed Tomography
    (CT), Magnetic Resonance Imaging (MRI), or Positron Emission Tomography (PET).
    In the process, the images are brought to the same coordinate system and aligned
    with each other. The reason for image registration being a challenging task is
    the fact that the two images used may have a different coordinate system, scale,
    or resolution.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Many attempts have been made toward automated image registration. A multi-resolution
    strategy with local optimizers to perform 2D or 3D image registration was performed
    by [[359](#bib.bib359)]. However, multi-resolution tends to fail with different
    field of views. Heuristic semi-global optimization schemes were proposed to solve
    this problem and used by [[252](#bib.bib252)] through simulated annealing and
    through genetic algorithm [[317](#bib.bib317)], However, their cost of computation
    was very high. A CNN-based approach to this problem was suggested by [[256](#bib.bib256)],
    and [[79](#bib.bib79)] proposed an optical flow method between 2D RGB images.
    A descriptor learned through a CNN was proposed by [[395](#bib.bib395)], where
    the authors encoded the posture and identity of a 3D object using the 2D image.
    Although all of these formulations produce satisfactory results yet, the methods
    could not be applied directly to 3D medical images.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the problems faced by previous methods, [[238](#bib.bib238)] proposed
    a method for improving probabilistic image registration via RL and uncertainty
    evaluation. The method involved predicting a regression function that predicts
    registration error from a set of features by using regression random forests (RRF)
    [[37](#bib.bib37)] method for training. The authors performed experiments on 3D
    MRI images and obtained an accuracy improvement of up to 25%.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Previous image registration methods are often customized to a specific problem
    and are sensitive to image quality and artifacts. To overcome these problems,
    [[221](#bib.bib221)] proposed a robust method using DRL. The authors considered
    the problem as an MDP where the goal is to find a set of transformations to be
    performed on the floating image to register it on the reference image. They used
    the gamma value for future reward decay and used the change in L2 Norm between
    the predicted transformation and ground truth transformation to calculate the
    reward. The authors also used a hierarchical approach to solve the problem with
    varying FOVs and resolutions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Comparing various DRL-based image registration methods.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| Image registration using uncertainity evaluation [[238](#bib.bib238)] | 2013
    | DQN | Not specified | Probabilistic model using regression random forests (RRF)
    [[37](#bib.bib37)] | Not specified | Higher final Dice score (DSC) as compared
    to other methods like random seed selection and grid-based seed selection | 3D
    MRI images from LONI Probabilistic Brain Atlas (LPBA40) [Dataset](http://www.loni.ucla.edu/)
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| Robust Image registration [[221](#bib.bib221)] | 2017 | DQN | 12 actions:
    corresponding to different transformations | States: current transformation. Reward:
    distance error. | 5 Conv3D layers and 3 FC layers | Better success rate then ITK
    [[153](#bib.bib153)], Quasi-global [[255](#bib.bib255)] and Semantic registration[[277](#bib.bib277)]
    | Abdominal spine CT and CBCT dataset, Cardiac CT and CBCT |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| Multimodal image registration [[244](#bib.bib244)] | 2017 | Duel-DQN Double-DQN
    | Actions update the transformations on floating image | States: cropped 3D image.
    Duel-DQN for value estimation and Double DQN for updating weights. | Batch normalization
    followed by 5 Conv3D and 3 Maxpool layers | Lower Euclidean distance error as
    compared to methods like Hausdorff, ICP, DQN [[264](#bib.bib264)], Dueling [[390](#bib.bib390)],
    etc. | Thorax and Abdomen (ABD) dataset |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| Robust non-rigid agent-based registration [[184](#bib.bib184)] | 2017 | DQN
    | 2n actions for n dimensional $\theta$ vector | States: fixed and moving image.
    Reward: change in transformation error. With Statistical deformation model and
    fuzzy action control. | Multi layer CNN, pooling and FC layers. | Higher Mean
    Dice score and lower Hausdorff distance as compared to methods like LCC-Demons
    [[237](#bib.bib237)] and Elastix [[180](#bib.bib180)]. | MICCAI challenge PROMISE12
    [[227](#bib.bib227)] |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| Robust Multimodal registration [[349](#bib.bib349)] | 2018 | Actor-Critic
    (a3c) [[262](#bib.bib262)] | 8 actions: for different transformations | States:
    fixed and moving image. Reward: Distance error. Monte-carlo method with LSTM [[139](#bib.bib139)].
    | Multi layer CNN and FC layer | Comparable if not lower target registration error
    [[96](#bib.bib96)] as compared to methods like SIFT [[239](#bib.bib239)], Elastix
    [[180](#bib.bib180)], Pure SL, RL-matrix, RL-LME, etc. | CT and MR images |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: A multi-modal method for image registration was proposed by [[244](#bib.bib244)],
    where the authors used DRL for alignment of depth data with medical images. In
    the specified work Duel DQN was used as the agent for estimating the state value
    and the advantage function, and the cropped 3D image tensor of both data modalities
    was considered as the state. The algorithmâ€™s goal was to estimate a transformation
    function that could align moving images to a fixed image by maximizing a similarity
    function between the fixed and moving image. A large number of convolution and
    pooling layer were used to extract high-level contextual information, batch normalization
    and concatenation of feature vector from last convolution layer with action history
    vector was used to solve the problem of oscillation and closed loops, and Double
    DQN architecture for updating the network weights was used.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Previous methods for image registration fail to cope with large deformations
    and variability in appearance. To overcome these issues [[184](#bib.bib184)] proposed
    a robust non-rigid agent-based method for image registration. The method involves
    finding a spatial transformation $T_{\theta}$ that can map the fixed image with
    the floating image using actions at each time step, that is responsible for optimizing
    $\theta$. If the $\theta$ is a d dimensional vector then there will be 2d possible
    actions. In this work, a DQN was used as an agent for value estimation, along
    with a reward that corresponded to the change in $\theta$ distance between ground
    truth and predicted transformations across an action.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: An improvement to the previous methods was proposed by [[349](#bib.bib349)],
    where the authors used a recurrent network with RL to solve the problem. Similar
    to [[221](#bib.bib221)], they considered the two images as a reference/fixed and
    floating/moving, and the algorithm was responsible for predicting transformation
    on the moving image to register it on a fixed image. In the specified work an
    LSTM [[139](#bib.bib139)] was used to encode past hidden states, Actor-critic
    [[262](#bib.bib262)] for policy estimation, and a reward function corresponding
    to distance between ground truth and transformed predicted landmarks were used.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Various methods in the field of Image registration have been summarized and
    compared in Table LABEL:tab:reg, and a basic implementation of image registration
    using DRL has been shown in Fig. [14](#S8.F14 "Figure 14 â€£ 8 DRL in Image Registration
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure illustrates a general implementation of image registration using DRL where
    the state consists of a fixed and floating image. The DRL agent predicts actions
    in form of a set of transformations on a floating image to register it onto the
    fixed image hence forming a new state and accepts reward in form of improvement
    in distance error between ground truth and predicted transformations with iterations
    as described by [[349](#bib.bib349)],[[184](#bib.bib184)],[[221](#bib.bib221)].'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d75a1dbe18c7a1d15a804e3770b2961a.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: DRL implementation for image registration. The state consists of
    fixed and floating image and the actions in form of transformations are performed
    on the floating image so as to maximize reward by minimizing distance between
    the ground truth and predicted transformations.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 9 DRL in Image Segmentation
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image segmentation is one of the most extensively performed tasks in computer
    vision, where the algorithm is responsible for labeling each pixel position as
    foreground or background corresponding to the object being segmented in the image.
    Image segmentation has a wide variety of applications in medical, robotics, weather,
    etc. One of the earlier attempts with image segmentation includes [[125](#bib.bib125)].
    With the improvement in detection techniques and introduction of CNN, new methods
    are introduced every year for image segmentation. Mask R-CNN [[132](#bib.bib132)]
    extended the work by Faster R-CNN [[309](#bib.bib309)] by adding a segmentation
    layer after the Bounding box has been predicted. Some earlier works include [[109](#bib.bib109)],
    [[127](#bib.bib127)], [[128](#bib.bib128)] etc. Most of these works give promising
    results in image segmentation. However, due to the supervised nature of CNN and
    R-CNN, these algorithms need a large amount of data. In fields like medical, the
    data is sometimes not readily available hence we needed a way to train algorithms
    to perform a given task when there are data constraints. Luckily RL tends to shine
    when the data is not available in a large quantity.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: One of the first methods for Image segmentation through RL was proposed by [[324](#bib.bib324)],
    where the authors proposed an RL framework for medical image segmentation. In
    their work, they used a Q-Matrix, where the actions were responsible for adjusting
    the threshold values to predict the mask and the reward was the normalized change
    in quality measure between action steps. [[325](#bib.bib325)] also used a similar
    technique of Tabular method.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the constraints of the previous method for segmentation, [[310](#bib.bib310)]
    proposed a method for indoor semantic segmentation through RL. In their paper,
    the authors proposed a sequential strategy using RL to combine binary object masks
    of different objects into a single multi-object segmentation mask. They formulated
    the binary mask in a Conditional Random Field Framework (CRF), and used a logistic
    regression version of AdaBoost [[140](#bib.bib140)] for classification. The authors
    considered the problem of adding multiple binary segmentation into one as an MDP,
    where the state consisted of a list of probability distributions of different
    objects in an image, and the actions correspond to the selection of object/background
    segmentation for a particular object in the sequential semantic segmentation.
    In the RL framework, the reward was considered in terms of pixel-wise frequency
    weighted Jaccard Index computed over the set of actions taken at any stage of
    an episode.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Interactive segmentation is the task of producing an interactive mask for objects
    in an image. Most of the previous works in this field greatly depend on the distribution
    of inputs which is user-dependent and hence produce inadequate results. An improvement
    was proposed by [[343](#bib.bib343)], where the authors proposed SeedNet, an automatic
    seed generation method for robust interactive segmentation through RL. With the
    image and initial seed points, the algorithm is capable of generating additional
    seed points and image segmentation results. The implementation included Random
    Walk (RW) [[114](#bib.bib114)] as the segmentation algorithm and DQN for value
    estimation by considering the problem as an MDP. They used the current binary
    segmentation mask and image features as the state, the actions corresponded to
    selecting seed points in a sparse matrix of size $20\times 20$(800 different actions
    were possible), and the reward consisted of the change in IOU across an action.
    In addition, the authors used an exponential IOU model to capture changes in IOU
    values more accurately.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Most of the previous work for image segmentation fail to produce satisfactory
    results when it comes to 3D medical data. An attempt on 3D medical image segmentation
    was done by [[222](#bib.bib222)], where the authors proposed an iteratively-refined
    interactive multi-agent method for 3D medical image segmentation. They proposed
    a method to refine an initial course segmentation produced by some segmentation
    methods using RL, where the state consisted of the image, previous segmentation
    probability, and user hint map. The actions corresponded to adjusting the segmentation
    probability for refinement of segmentation, and a relative cross-entropy gain-based
    reward to update the model in a constrained direction was used. In simple words,
    it is the relative improvement of previous segmentation to the current one. The
    authors utilized an asynchronous advantage actor-critic algorithm for determining
    the policy and value of the state.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Comparing various DRL-based image segmentation methods'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| Semantic Segmentation for indoor scenes[[310](#bib.bib310)] | 2016 | DQN
    | 2 actions per object: object, background | States: current probability distribution.
    Reward: pixel-wise frequency weighted Jaccard index. Conditional Random Field
    Framework (CRF) and logistic regression version of AdaBoost [[140](#bib.bib140)]
    for classification. | Not Specified | Pixel-wise percentage jaccard index comparable
    to Gupta-L [[121](#bib.bib121)] and Gupta-P [[120](#bib.bib120)]. | NYUD V2 dataset
    [[338](#bib.bib338)] |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| SeedNet [[343](#bib.bib343)] | 2018 | DQN, Double-DQN, Duel-DQN | 800 actions:
    2 per pixel | States: image features and segmentation mask. Reward: change in
    IOU. Random Walk (RW) [[114](#bib.bib114)] for segmentation algorithm. | Multi
    layer CNN | Better IOU then methods like FCN [[236](#bib.bib236)] and iFCN [[407](#bib.bib407)].
    | MSRA10K saliency dataset [[49](#bib.bib49)] |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| Iteratively refined multi agent segmentation [[222](#bib.bib222)] | 2020
    | Actor-critic (a3c) [[262](#bib.bib262)] | 1 action per voxel for adjusting segmentation
    probability | States: 3D image segmentation probability and hint map. Reward:
    cross entropy gain based framework. | R-net [[378](#bib.bib378)] | Better performance
    then methods like MinCut [[183](#bib.bib183)], DeepIGeoS (R-Net) [[378](#bib.bib378)]
    and InterCNN [[36](#bib.bib36)]. | BraTS 2015[[254](#bib.bib254)], MM-WHS [[432](#bib.bib432)]
    and NCI-ISBI 2013 Challenge [[33](#bib.bib33)] |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| Multi-step medical image segmentation [[360](#bib.bib360)] | 2020 | Actor-critic
    (a3c) [[262](#bib.bib262)] | Actions control the position and shape of brush stroke
    to modify segmentation | States: image, segmentation mask and time step. Reward:
    change in distance error. Policy: DPG [[339](#bib.bib339)]. | ResNet18 [[133](#bib.bib133)]
    | Higher Mean Dice score and lower Hausdorff distance then methods like Grab-Cut
    [[315](#bib.bib315)], PSPNet [[425](#bib.bib425)], FCN [[236](#bib.bib236)], U-Net
    [[313](#bib.bib313)], etc. | Prostate MR image dataset (PROMISE12, ISBI2013) and
    retinal fundus image dataset (REFUGE challenge dataset [[285](#bib.bib285)]) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| Anomaly Detection in Images [[56](#bib.bib56)] | 2020 | REINFORCE [[392](#bib.bib392)]
    | 9 actions, 8 for directions to shift center of the extracted patch to, the last
    action is to switch to a random new image | Environment: input image to the model.
    State: observed patch from the image centered by predicted center of interest.
    | None | Superior performance in [[27](#bib.bib27)] and [[337](#bib.bib337)] on
    all metrics e.g. precision, recall and F1 when compared with U-Net [[313](#bib.bib313)]
    and baseline unsupervised method in [[27](#bib.bib27)] but only wins on recall
    in [[44](#bib.bib44)] | MVTec AD [[27](#bib.bib27)], NanoTWICE [[44](#bib.bib44)],
    CrackForest [[337](#bib.bib337)] |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: Further improvement in the results of medical image segmentation was proposed
    by [[360](#bib.bib360)]. The authors proposed a method for multi-step medical
    image segmentation using RL, where they used a deep deterministic policy gradient
    method (DDPG) based on actor-critic framework [[262](#bib.bib262)] and similar
    to Deterministic policy gradient (DPG) [[339](#bib.bib339)]. The authors used
    ResNet18 [[133](#bib.bib133)] as backbone for actor and critic network along with
    batch normalisation [[157](#bib.bib157)] and weight normalization with Translated
    ReLU [[400](#bib.bib400)]. In their MDP formulation, the state consisted of the
    image along with the current segmentation mask and step-index, and the reward
    corresponded to the change in mean squared error between the predicted segmentation
    and ground truth across an action. According to the paper the action was defined
    to control the position and shape of brush stroke used to modify the segmentation.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: An example in image segmentation outside the medical field is [[56](#bib.bib56)]
    proposing to tackle the problem of anomalies detection and segmentation in images
    (i.e. damaged pins of an IC chip, small tears in woven fabric). [[56](#bib.bib56)]
    utilizes an additional module to attend only on a specific patch of the image
    centered by a predicted center instead of the whole image, this module helps a
    lot in reducing the imbalance between normal regions and abnormal locations. Given
    an image, this module, namely Neural Batch Sampling (NBS), starts from a random
    initiated center and recurrently moves that center by eight directions to the
    abnormal location in the image if it exists, and it has an additional action to
    stop moving the center when it has already converged to the anomaly location or
    there is not any anomaly can be observed. The NBS module is trained by REINFORCE
    algorithm [[392](#bib.bib392)] and the whole model is evaluated on multiple datasets
    e.g. MVTec AD [[27](#bib.bib27)], NanoTWICE [[44](#bib.bib44)], CrackForest [[337](#bib.bib337)].
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Various works in the fields of Image segmentation have been summarised and
    compared in Table LABEL:tab:seg, and a basic implementation of image segmentation
    using DRL has been shown in Fig. [15](#S9.F15 "Figure 15 â€£ 9 DRL in Image Segmentation
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure shows a general implementation of image segmentation using DRL. The states
    consist of the image along with user hint (landmarks or segmentation mask by the
    other algorithm) for the first iteration or segmentation mask by the previous
    iteration. The actions are responsible for labeling each pixel as foreground and
    background and reward corresponds to an improvement in IOU with iterations as
    used by [[343](#bib.bib343)],[[222](#bib.bib222)].'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/47751bb3b691f2ca58b0fd46cd4d3310.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: DRL implementation for Image segmentation. The state consists of
    the image to be segmented along with a user hint for t=0 or the segmentation mask
    by the previous iterations. The DRL agent performs actions by labeling each pixel
    as foreground and background to maximize the improvement in IOU over the iterations.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 10 DRL in Video Analysis
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object segmentation in videos is a very useful yet challenging task in computer
    vision field. Video object segmentation task focuses on labelling each pixel for
    each frame as foreground or background. Previous works in the field of video object
    segmentation can be divided into three main methods. unsupervised [[288](#bib.bib288)][[402](#bib.bib402)],
    weakly supervised [[48](#bib.bib48)][[163](#bib.bib163)] [[419](#bib.bib419)]
    and semi-supervised [[41](#bib.bib41)] [[164](#bib.bib164)][[292](#bib.bib292)].
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: A DRL-based framework for video object segmentation was proposed by [[323](#bib.bib323)],
    where the authors divided the image into a group of sub-images and then used the
    algorithm on each of the sub-image. They proposed a group of actions that can
    perform to change the local values inside each sub-image and the agent received
    reward based on the change in the quality of segmented object inside each sub-image
    across an action. In the proposed method deep belief network (DBN) [[47](#bib.bib47)]
    was used for approximating the Q-values.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Surgical gesture recognition is a very important yet challenging task in the
    computer vision field. It is useful in assessing surgical skills and for efficient
    training of surgeons. A DRL method for surgical gesture classification and segmentation
    was proposed by [[228](#bib.bib228)]. The proposed method could work on features
    extracted by video frames or kinematic data frames collected by some means along
    with the ground truth labels. The problem of classification and segmentation was
    considered as an MDP, where the state was a concatenation of TCN [[195](#bib.bib195)][[199](#bib.bib199)]
    features of the current frame, 2 future frames a specified number of frames later,
    transition probability of each gesture computed from a statistical language model
    [[311](#bib.bib311)] and a one-hot encoded vector for gesture classes. The actions
    could be divided into two sub-actions, One to decide optimal step size and one
    for choosing gesture class, and the reward was adopted in a way that encouraging
    the agent to adopt a larger step and also penalizes the agent for errors caused
    by the action. The authors used Trust Region Policy Optimization (TRPO) [[326](#bib.bib326)]
    for training the policy and a spacial CNN [[196](#bib.bib196)] to extract features.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Earlier approaches for video object segmentation required a large number of
    actions to complete the task. An Improvement was proposed by [[124](#bib.bib124)],
    where authors used an RL method for object segmentation in videos. They proposed
    a reinforcement cutting-agent learning framework, where the cutting-agent consists
    of a cutting-policy network (CPN) and a cutting-execution network (CEN). The CPN
    learns to predict the object-context box pair, while CEN learns to predict the
    mask based on the inferred object-context box pair. The authors used MDP to solve
    the problem in a semi-supervised fashion. For the state of CPN the authors used
    the input frame information, the action history, and the segmentation mask provided
    in the first frame. The output boxes by CPN were input for the CEN. The actions
    for CPN network included 4 translation actions (Up, Down, Left, Right), 4 scaling
    action (Horizontal shrink, Vertical shrink, Horizontal zoom, Vertical zoom), and
    1 terminal action (Stop), and the reward corresponded to the change in IOU across
    an action. For the network architecture, a Fully-Convolutional DenseNet56 [[166](#bib.bib166)]
    was used as a backbone along with DQN as the agent for CPN and down-sampling followed
    by up-sampling architecture for CEN.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised video object segmentation is an intuitive task in the computer
    vision field. A DRL method for this task was proposed by [[111](#bib.bib111)],
    where the authors proposed a motion-oriented unsupervised method for image segmentation
    in videos (MOREL). They proposed a two-step process to achieve the task in which
    first a representation of input is learned to understand all moving objects through
    unsupervised video object segmentation, Then the weights are transferred to the
    RL framework to jointly train segmentation network along with policy and value
    function. The first part of the method takes two consecutive frames as input and
    predicts a number of segmentation masks, corresponding object translations, and
    camera translations. They used a modified version of actor-critic [[262](#bib.bib262)][[329](#bib.bib329)][[371](#bib.bib371)]
    for the network of first step. Following the unsupervised fashion, the authors
    used the approach similar to [[375](#bib.bib375)] and trained the network to interpolate
    between consecutive frames and used the masks and translations to estimate the
    optical flow using the method that was proposed in Spatial Transformer Networks
    [[159](#bib.bib159)]. They also used structural dissimilarity (DSSIM) [[388](#bib.bib388)]
    to calculate reconstruction loss and actor-critic [[262](#bib.bib262)] algorithm
    to learn policy in the second step.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'A DRL method for dynamic semantic face video segmentation was proposed by [[387](#bib.bib387)],
    where Deep Feature Flow [[431](#bib.bib431)] was utilized as the feature propagation
    framework and RL was used for an efficient and effective scheduling policy. The
    method involved dividing frames into key ($I_{k}$) and non-key ($I_{i}$), and
    using the last key frame features for performing segmentation of non-key frame.
    The actions made by the policy network corresponded to categorizing a frame as
    $I_{k}$ or $I_{i}$ and the state consisted of deviation information and expert
    information, where the deviation information described the difference between
    current $I_{i}$ and last $I_{k}$ and expert information encapsulated the key decision
    history. The authors utilized FlowNet2-s model [[156](#bib.bib156)] as an optical
    flow estimation function, and divided the network into feature extraction module
    and task-specific module. After policy network which consisted of one convolution
    layer, 4 fully connected layers and 2 concatenated channels consisting of KAR
    (Key all ratio: Ratio between key frame and every other frame in decision history)
    and LKD (Last key distance: Distance between current and last key frame) predicted
    the action, If the current frame is categorized as key frame the feature extraction
    module produced the frame features and task-specific module predicted the segmentation,
    However if the frame is categorized as a non-key frame the features from the last
    key frame along with the optical flow was used by the task-specific module to
    predict the segmentation. The authors proposed two types of reward functions,
    The first reward function was calculated by considering the difference between
    the IOU for key and non-key actions. The second reward function was proposed for
    a situation when ground truth was not available and was calculated by considering
    the accuracy score between segmentation for key and non-key actions.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparing various methods associated with video. First group for video
    object segmentation, second group for action recognition and third group for video
    summarisation'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and Source code |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| Object segmentation in videos[[323](#bib.bib323)] | 2016 | Deep Belief Network
    [[47](#bib.bib47)] | Actions changed local values in sub-images | States: sub-images.
    Reward: quality of segmentation. | Not specified | Not specified | Not specified
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| Surgical gesture segmentation and classification [[228](#bib.bib228)] | 2018
    | Trust Region Policy Optimization (TRPO) [[326](#bib.bib326)] | 2 types: optimal
    step size and gesture class | States: TCN [[[195](#bib.bib195)], [[199](#bib.bib199)]]
    and future frames. Reward: encourage larger steps and minimize action errors.
    Statistical language model [[311](#bib.bib311)] for gesture probability. | Spacial
    CNN [[196](#bib.bib196)] | Comparable accuracy, and higher edit and F1 scores
    as compared to methods like SD-SDL [[331](#bib.bib331)], Bidir LSTM [[76](#bib.bib76)],
    LC-SC-CRF [[197](#bib.bib197)], Seg-ST-CNN [[196](#bib.bib196)], TCN [[198](#bib.bib198)],
    etc | JIGSAWS [[[6](#bib.bib6)], [[100](#bib.bib100)]] benchmark dataset [Available
    Code](https://github.com/Finspire13/RL-Surgical-Gesture-Segmentation) |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Cutting agent for video object segmentation [[124](#bib.bib124)] | 2018 |
    DQN | 8 actions: 4 translation actions (Up, Down, Left, Right), 4 scaling action
    (Horizontal shrink, Vertical shrink, Horizontal zoom, Vertical zoom) and 1 terminal
    action (Stop) | States: input frame, action history and segmentation mask. Reward:
    change in IOU. cutting-policy network for box-context pair and cutting-execution
    network for mask generation | DenseNet [[166](#bib.bib166)] | Higher mean region
    similarity, counter accuracy and temporal stability [[293](#bib.bib293)] as compared
    to methods like MSK [[292](#bib.bib292)], ARP [[173](#bib.bib173)], CTN [[165](#bib.bib165)],
    VPN [[164](#bib.bib164)], etc. | DAVIS dataset [[293](#bib.bib293)] and the YouTube
    Objects dataset [[162](#bib.bib162)], [[300](#bib.bib300)] |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised video object segmentation (MOREL) [[111](#bib.bib111)] | 2018
    | Actor-critic (a2c) [[262](#bib.bib262)] | Not specified | States: consecutive
    frames. Two step process with optical flow using Spatial Transformer Networks
    [[159](#bib.bib159)] and reconstruction loss using structural dissimilarity [[388](#bib.bib388)].
    | Multi-layer CNN | Higher total episodic reward as compared to methods that used
    actor-critic without MOREL | 59 Atari games. [Available Code](https://github.com/vik-goel/MOREL)
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| Face video segmentation [[387](#bib.bib387)] | 2020 | Not specified | 2 actions:
    categorising a frame as a key or a non-key | States: deviation information which
    described the difference between current non-key and last key decision, and expert
    information which encapsulated the key decision history. Reward: improvement in
    mean IOU/accuracy score between segmentation of key and non-key frames | Multi-layer
    CNN | Higher mean IOU then other methods like DVSNet [[410](#bib.bib410)], DFF
    [[431](#bib.bib431)]. | 300VW dataset [[336](#bib.bib336)] and Cityscape dataset
    [[61](#bib.bib61)] |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| Multi-agent Video Object Segmentation [[373](#bib.bib373)] | 2020 | DQN |
    Actions of 2 types: movement actions (up, down, left and right) and set action
    (action to place location prior at a random location on the patch) | States: input
    frame, optical flow [[156](#bib.bib156)] from previous frame and action history.
    Reward: clicks generated by gamification. Down-sampling and up-sampling similar
    to U-Net [[313](#bib.bib313)] | DenseNet [[147](#bib.bib147)] | Higher mean region
    similarity and contour accuracy [[293](#bib.bib293)] as compared to semi-supervised
    methods such as SeamSeg [[14](#bib.bib14)], BSVS [[248](#bib.bib248)], VSOF [[363](#bib.bib363)],
    OSVOS [[41](#bib.bib41)] and weakly-supervised methods such as GVOS [[346](#bib.bib346)],
    Spftn [[419](#bib.bib419)] | DAVIS-17 dataset [[293](#bib.bib293)] |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| Skeleton-based Action Recognition [[357](#bib.bib357)] | 2018 | DQN | 3 actions:
    shifting to left, staying the same and shifting to right | States: Global video
    information and selected frames. Reward: change in categorical probability. 2
    step network (FDNet) to filter frames and GCNN for action labels | Multi-layer
    CNN | Higher cross subject and cross view metrics for NTU+RGBD dataset [[333](#bib.bib333)],
    and higher accuracy for SYSU-3D [[145](#bib.bib145)] and UT-Kinect Dataset [[399](#bib.bib399)]
    when compared with other methods like Dynamic Skeletons [[145](#bib.bib145)],
    HBRNN-L [[81](#bib.bib81)], Part-aware LSTM [[333](#bib.bib333)], LieNet-3Blocks
    [[151](#bib.bib151)], Two-Stream CNN [[211](#bib.bib211)], etc. | NTU+RGBD [[333](#bib.bib333)],
    SYSU-3D [[145](#bib.bib145)] and UT-Kinect Dataset [[399](#bib.bib399)] |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| Video summarisation [[429](#bib.bib429)] | 2018 | DQN | 2 actions: selecting
    and rejecting the frame | tates: bidirectional LSTM [[150](#bib.bib150)] produced
    states by input frame features. Reward: Diversity-Representativeness Reward Function.
    | GoogLeNet [[355](#bib.bib355)] | Higher F-score [[421](#bib.bib421)] as compared
    to methods like Uniform sampling, K-medoids, Dictionary selection [[88](#bib.bib88)],
    Video-MMR [[218](#bib.bib218)], Vsumm [[69](#bib.bib69)], etc. | TVSum [[344](#bib.bib344)]
    and SumMe [[122](#bib.bib122)]. [Available Code](https://github.com/KaiyangZhou/pytorch-vsumm-reinforce)
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| Video summarization [[430](#bib.bib430)] | 2018 | Duel DQN Double DQN | 2
    actions: selecting and rejecting the frame | States: sequence of frames Reward:
    Diversity-Representativeness Reward Function 2 stage implementation: classification
    and summarisation network using bidirectional GRU network and LSTM [[150](#bib.bib150)]
    | GoogLeNet [[355](#bib.bib355)] | Higher F-score [[421](#bib.bib421)] as compared
    to methods like Dictionary selection [[88](#bib.bib88)], GAN [[245](#bib.bib245)],
    DR-DSN [[429](#bib.bib429)], Backprop-Grad [[287](#bib.bib287)], etc in most cases.
    | TVSum [[344](#bib.bib344)] and CoSum [[57](#bib.bib57)] datasets. [Available
    Code](https://github.com/KaiyangZhou) |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| Video summarization in Ultrasound [[233](#bib.bib233)] | 2020 | Not specified
    | 2 actions: selecting and rejecting the frame | States: frame latent scores Reward:
    $R_{det}$, $R_{rep}$ and $R_{div}$ bidirectional LSTM [[150](#bib.bib150)] and
    Kernel temporal segmentation [[298](#bib.bib298)] | Not specified | Higher F1-scores
    in supervised and unsupervised fashion as compared to methods like FCSN [[312](#bib.bib312)]
    and DR-DSN [[429](#bib.bib429)]. | Fetal Ultrasound [[179](#bib.bib179)] |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: Video object segmentation using human-provided location priors have been capable
    of producing promising results. An RL method for this task was proposed by [[373](#bib.bib373)],
    in which the authors proposed MASK-RL, a multiagent RL framework for object segmentation
    in videos. They proposed a weakly supervised method where the location priors
    were provided by the user in form of clicks using gamification (Web game to collect
    location priors by different users) to support the segmentation and used a Gaussian
    filter to emphasize the areas. The segmentation network is fed a 12 channel input
    tensor that contained a sequence of video frames and their corresponding location
    priors (3 $\times$ 3 color channels + three gray-scale images). The authors used
    a fully convoluted DenseNet [[147](#bib.bib147)] with down-sampling and up-sampling
    similar to U-Net [[313](#bib.bib313)] and an LSTM [[139](#bib.bib139)] for the
    segmentation network. For the RL method, the actor takes a series of steps over
    a frame divided into a grid of equal size patches and makes the decision whether
    there is an object in the patch or not. In their MDP formulation the states consisted
    of the input frame, optical flow (computed by [[156](#bib.bib156)]) from the previous
    frame, patch from the previous iteration, and the episode location history, the
    actions consisted of movement actions (up, down, left and right) and set action
    (action to place location prior at a random location on the patch), and two types
    of rewards one for set actions and one for movement actions were used. The reward
    was calculated using the clicks generated by the game player.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Action recognition is an important task in the computer vision field which
    focuses on categorizing the action that is being performed in the video frame.
    To address the problem a deep progressive RL (DPRL) method for action recognition
    in skeleton-based videos was proposed by [[357](#bib.bib357)]. The authors proposed
    a method that distills the most informative frames and discards ambiguous frames
    by considering the quality of the frame and the relationship of the frame with
    the complete video along with a graph-based structure to map the human body in
    form of joints and vertices. DPRL was utilized to filter out informative frames
    in a video and graph-based CNNs were used to learn the spatial dependency between
    the joints. The approach consisted of two sub-networks, a frame distillation network
    (FDNet) to filter a fixed number of frames from input sequence using DPRL and
    GCNN to recognize the action labels using output in form of a graphical structure
    by the FDNet. The authors modeled the problem as an MDP where the state consisted
    of the concatenation of two tensors $F$ and $M$, where $F$ consisted of global
    information about the video and $M$ consisted of the frames that were filtered,
    The actions which correspond to the output of FDNet were divided into three types:
    shifting to left, staying the same and shifting to the right, and the reward function
    corresponded to the change in probability of categorizing the video equal to the
    ground truth clipped it between [-1 and 1] and is provided by GCNN to FDNet.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Video summarization is a useful yet difficult task in the computer vision field
    that involves predicting the object or the task that is being performed in a video.
    A DRL method for unsupervised video summarisation was proposed by [[429](#bib.bib429)],
    in which the authors proposed a Diversity-Representativeness reward system and
    a deep summarisation network (DSN) which was capable of predicting a probability
    for each video frame that specified the likeliness of selecting the frame and
    then take actions to form video summaries. They used an encode-decoder framework
    for the DSN where GoogLeNet [[355](#bib.bib355)] pre-trained on ImageNet [[320](#bib.bib320)]
    [[72](#bib.bib72)] was used as an encoder and a bidirectional RNNs (BiRNNs) topped
    with a fully connected (FC) layer was used as a decoder. The authors modeled the
    problem as an MDP where the action corresponded to the task of selecting or rejecting
    a frame. They proposed a novel Diversity-Representativeness Reward Function in
    their implementation, where diversity reward corresponded to the degree of dissimilarity
    among the selected frames in feature space, and representativeness reward measured
    how well the generated summary can represent the original video. For the RNN unit
    they used an LSTM [[139](#bib.bib139)] to capture long-term video dependencies
    and used REINFORCE algorithm for training the policy function.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd162a8f5e9bde527aab23443aacd365.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: DRL implementation for video summarization. For state a sequence
    of consecutive frames are used and the DRL agent decided whether to include the
    frame in the summary set that is used to predict video summary.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'An improvement to [[429](#bib.bib429)] was proposed by [[430](#bib.bib430)],
    where the summarisation network was implemented using Deep Q-learning (DQSN),
    and a trained classification network was used to provide a reward for training
    the DQSN. The approach included using (Bi-GRU) bidirectional recurrent networks
    with a gated recurrent unit (GRU) [[50](#bib.bib50)] for both classification and
    summarisation network. The authors first trained the classification network using
    a supervised classification loss and then used the classification network with
    fixed weights for the classification of summaries generated by the summarisation
    network. The summarisation network included an MDP-based framework in which states
    consisted of a sequence of video frames and actions reflected the task of either
    keeping the frame or discarding it. They used a structure similar to Duel-DQN
    where value function and advantage function are trained together. In their implementation,
    the authors considered 3 different rewards: Global Recognisability reward using
    the classification network with +1 as reward and -5 as punishment, Local Relative
    Importance Reward for rewarding the action of accepting or rejecting a frame by
    summarisation network, and an Unsupervised Reward that is computed globally using
    the unsupervised diversity-representativeness (DR) reward proposed in [[429](#bib.bib429)].
    The authors trained both the networks using the features generated by GoogLeNet
    [[355](#bib.bib355)] pre-trained on ImageNet [[72](#bib.bib72)].'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: A method for video summarization in Ultrasound using DRL was proposed by [[233](#bib.bib233)],
    in which the authors proposed a deep summarisation network in an encoder-decoder
    fashion and used a bidirectional LSTM (Bi-LSTM) [[150](#bib.bib150)] for sequential
    modeling. In their implementation, the encoder-decoder convolution network extracted
    features from video frames and fed them into the Bi-LSTM. The RL network accepted
    states in form of latent scores from Bi-LSTM and produced actions, where the actions
    consist of the task of including or discarding the video frame inside the summary
    set that is used to produce video summaries. The authors used three different
    rewards $R_{det}$, $R_{rep}$ and $R_{div}$ where $R_{det}$ evaluated the likelihood
    of a frame being a standard diagnostic plane, $R_{rep}$ defined the representativeness
    reward and $R_{div}$ was the diversity reward that evaluated the quality of the
    selected summary. They used Kernel temporal segmentation (KTS) [[298](#bib.bib298)]
    for video summary generalization.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Various works associated with video analysis have been summarised and compared
    in Table LABEL:tab:vid and a basic implementation of video summarization using
    DRL has been shown in Fig. [16](#S10.F16 "Figure 16 â€£ 10 DRL in Video Analysis
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"), where
    the states consist of a sequence of video frames. The DRL agent performs actions
    to include or discard a frame from the summary set that is later used by the summarization
    network to predict video summary. Each research paper propose their own reward
    function for this application, for example [[429](#bib.bib429)] and [[430](#bib.bib430)]
    used diversity representativeness reward function and [[233](#bib.bib233)] used
    a combination of various reward functions.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 11 Others Applications
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Object manipulation refers to the task of handling and manipulating an object
    using a robot. A method for deformable object manipulation using RL was proposed
    by [[250](#bib.bib250)], where the authors used a modified version of Deep Deterministic
    Policy Gradients (DDPG) [[224](#bib.bib224)]. They used the simulator Pybullet
    [[63](#bib.bib63)] for the environment where the observation consisted of a $84\times
    84\times 3$ image, the state consists of joint angles and gripper positions and
    action of four dimensions: first three for velocity and lasts for gripper velocity
    was used. The authors used sparse reward for the task that returns the reward
    at the completion of the task. They used the algorithm to perform tasks such as
    folding and hanging cloth and got a success rate of up to 90%.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Visual perception-based control refers to the task of controlling robotic systems
    using a visual input. A virtual to real method for control using semantic segmentation
    was proposed by [[142](#bib.bib142)], in which the authors combined various modules
    such as, Perception module, control policy module, and a visual guidance module
    to perform the task. For the perception module, the authors directly used models
    such as DeepLab [[46](#bib.bib46)] and ICNet [[424](#bib.bib424)], pre-trained
    on ADE20K [[428](#bib.bib428)] and Cityscape [[61](#bib.bib61)], and used the
    output of these model as the state for the control policy module. They implemented
    the control policy module using the actor-critic [[262](#bib.bib262)] framework,
    where the action consisted of forward, turn right, and turn left. In their implementation,
    a reward of 0.001 is given at each time step. They used the Unity3D engine for
    the environment and got higher success and lower collision rate than other implementations
    such as ResNet-A3C and Depth-A3C.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Automatic tracing of structures such as axons and blood vessels is an important
    yet challenging task in the field of biomedical imaging. A DRL method for sub-pixel
    neural tracking was proposed by [[65](#bib.bib65)], where the authors used 2D
    grey-scale images as the environment. They considered a full resolution 11px $\times$
    11px window and a 21px $\times$ 21px window down-scaled to 11px $\times$ 11px
    as state and the actions were responsible for moving the position of agent in
    2D space using continuous control for sub-pixel tracking because axons can be
    smaller then a pixel. The authors used a reward that was calculated using the
    average integral of intensity between the agentâ€™s current and next location, and
    the agent was penalized if it does not move or changes directions more than once.
    They used an Actor-critic [[262](#bib.bib262)] framework to estimate value and
    policy functions.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: An RL method for automatic diagnosis of acute appendicitis in abdominal CT images
    was proposed by [[8](#bib.bib8)], in which the authors used RL to find the location
    of the appendix and then used a CNN classifier to find the likelihood of Acute
    Appendicitis, finally they defined a region of low-entropy (RLE) using the spatial
    representation of output scores to obtain optimal diagnosis scores. The authors
    considered the problem of appendix localization as an MDP, where the state consisted
    of a $50\times 50\times 50$ volume around the predicted appendix location, 6 actions
    (2 per axis) were used and the reward consisted of the change in distance between
    the predicted appendix location and actual appendix location across an action.
    They utilized an Actor-critic [[262](#bib.bib262)] framework to estimate policy
    and value functions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Comparing various other methods besides landmark detection, object
    detection, object tracking, image registration, image segmentation, video analysis,
    that is associated with DRL'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets Source code |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| Object manipulation [[250](#bib.bib250)] | 2018 | Rainbow DDPG | 4 actions:
    3 for velocity 1 for gripper velocity | State: joint angle and gripper position.
    Reward: at the end of task. | Multi layer CNN | Success rate up to 90% | Pybullet
    [[63](#bib.bib63)]. [Code](https://github.com/JanMatas/Rainbow_ddpg) |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| Visual based control [[142](#bib.bib142)] | 2018 | Actor-critic (a3c) [[262](#bib.bib262)]
    | 3 actions: forward, turn right and turn left | State: output by backbones. Reward:
    0.001 at each time-step. | DeepLab [[46](#bib.bib46)] and ICNet [[424](#bib.bib424)]
    | Higher success and lower collision rate then ResNet-A3C and Depth-A3C | Unity3D
    engine |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| Automatic tracing [[65](#bib.bib65)] | 2019 | Actor-critic [[262](#bib.bib262)]
    | 4 actions | State: 11px $\times$ 11px window. Reward: average integral of intensity
    between the agentâ€™s current and next location. | Multi layer CNN | Comparable
    convergence $\%$ and average error as compared to other methods like Vaa3D software
    [[291](#bib.bib291)] and APP2 neuron tracer [[403](#bib.bib403)] | Synthetic and
    microscopy dataset [[24](#bib.bib24)] |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| Automatic diagnosis (RLE) [[8](#bib.bib8)] | 2019 | Actor-critic [[262](#bib.bib262)]
    | 6 actions: 2 per axis | State: $50\times 50\times 50$ volume. Reward: change
    in distance error. | Fully connected CNN | Higher sensitivity and specificity
    as compared to only CNN classifier and CNN classifier with RL without RLE. | Abdominal
    CT Scans |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| Learning to paint [[149](#bib.bib149)] | 2019 | Actor-critic with DDPG |
    Actions control the stoke parameter: location, shape, color and transparency |
    State: Reference image, Drawing canvas and time step. Reward: change in discriminator
    score (calculated by WGAN-GP [[117](#bib.bib117)] across an action. GANs [[113](#bib.bib113)]
    to improve image quality | ResNet18 [[133](#bib.bib133)] | Able to replicate the
    original images to a large extent, and better resemblance to the original image
    as compared to SPIRAL [[98](#bib.bib98)] with same number of brush strokes. |
    MNIST [[202](#bib.bib202)], SVHN [[276](#bib.bib276)], CelebA [[235](#bib.bib235)]
    and ImageNet [[320](#bib.bib320)]. [Code](https://github.com/hzwer/ICCV2019-LearningToPaint)
    |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| Guiding medical robots [[129](#bib.bib129)] | 2020 | Double-DQN, Duel-DQN
    | 5 actions: up, down, left, right and stop | State: probe position. Reward: Move
    closer: 0.05, Move away: -0.1, Correct stop: 1.0, Incorrect stop: -0.25. | ResNet18
    [[133](#bib.bib133)] | Higher % of policy correctness and reachability as compared
    to CNN Classifier, where MS-DQN showed the best results | Ultrasound Images [Dataset](https://github.com/hhase/sacrumdata-set).
    [Code](https://github.com/hhase/spinal-navigation-rl) |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| Crowd counting [[230](#bib.bib230)] | 2020 | DQN | 9 actions: -10, -5, -2,
    -1, +1, +2, +5, +10 and end | State: weight vector $W_{t}$ and image feature vector
    $FV_{I}$. Reward: Intermediate reward and ending reward | VGG16 [[340](#bib.bib340)]
    | Lower/comparable mean squared error (MSE) and mean absolute error (MAE) as compared
    to other methods like DRSAN [[232](#bib.bib232)], PGCNet [[412](#bib.bib412)],
    MBTTBF [[341](#bib.bib341)], S-DCNet [[405](#bib.bib405)], CAN [[234](#bib.bib234)],
    etc. | The ShanghaiTech (SHT) Dataset [[423](#bib.bib423)], The UCFCC50 Dataset
    [[154](#bib.bib154)] and The UCF-QNRF Dataset [[155](#bib.bib155)]. [Code](https://github.com/poppinace/libranet)
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| Automated Exposure bracketing [[389](#bib.bib389)] | 2020 | Not Specified
    | selecting optimal bracketing from candidates | State: quality of generated HDR
    image. Reward: improvement in peak signal to noise ratio | AlexNet [[188](#bib.bib188)]
    | Higher peak signal to noise ratio as compared to other methods like Barakat
    [[22](#bib.bib22)], Pourreza-Shahri [[299](#bib.bib299)], Beek [[369](#bib.bib369)],
    etc. | Proposed benchmark dataset. [Code/data](https://github.com/wzhouxiff/EBSNetMEFNet)
    |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| Urban Autonomous driving [[361](#bib.bib361)] | 2020 | Rainbow-IQN | 36 or
    108 actions: ($9\times 4$) or ($27\times 4$), 9/27 steering and 4 throttle | State:
    environment variables like traffic light, pedestrians, position with respect to
    center lane. Reward: generated by CARLA waypoint API | Resnet18 [[133](#bib.bib133)]
    | Won the 2019 camera only CARLA challenge [[314](#bib.bib314)]. | CARLA urban
    driving simulator [[314](#bib.bib314)] [Code](https://github.com/valeoai/learningbycheating)
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| Mitigating bias in Facial Recognition [[382](#bib.bib382)] | 2020 | DQN |
    3 actions:(Margin adjustment) staying the same, shifting to a larger value and
    shifting to a smaller value | State: the race group, current adaptive margin and
    bias between the race group and Caucasians. Reward: change in the sum of inter-class
    and intra-class bias | Multi-layer CNN | Proposed algorithm had higher verification
    accuracy as compared to other methods such as CosFace [[379](#bib.bib379)] and
    ArcFace [[73](#bib.bib73)]. | RFW [[383](#bib.bib383)] and proposed novel datasets:
    BUPT-Globalface and BUPT-Balancedface [Data](http://www.whdeng.cn/RFW/index.html)
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| Attention mechanism to improve CNN performance [[212](#bib.bib212)] | 2020
    | DQN [[264](#bib.bib264)] | Actions are weights for every location or channel
    in the feature map. | State: Feature map at each intermediate layer of model.
    Reward: predicted by a LSTM model. | ResNet-101 [[133](#bib.bib133)] | Improves
    the performances of [[144](#bib.bib144)], [[205](#bib.bib205)] and [[396](#bib.bib396)],
    which attend on feature channel, spatial-channel and style, respectively | ImageNet
    [[72](#bib.bib72)] |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/70ccfd4e0c164cd86be06ea3df464348.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: A general DRL implementation for agent movement with visual inputs.
    The state is provided by the environment based on which the agent performs movement
    actions to get a new state and a reward from the environment.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Painting using an algorithm is a fantastic yet challenging task in the computer
    vision field. An automated painting method was proposed by [[149](#bib.bib149)],
    where the authors introduced a model-based DRL technique for this task. The specified
    work involved using a neural renderer in DRL, where the agent was responsible
    for making a decision about the position and color of each stroke, and making
    long-term decisions to organize those strokes into a visual masterpiece. In this
    work, GANs [[113](#bib.bib113)] were employed to improve image quality at pixel-level
    and DDPG [[224](#bib.bib224)] was utilized for determining the policy. The authors
    formulated the problem as an MDP, where the state consisted of three parts: the
    target image $I$, the canvas on which actions (paint strokes) are performed $C_{t}$,
    and the time step. The actions corresponding to a set of parameters that controlled
    the position, shape, color, and transparency of strokes, and for reward the WGAN
    with gradient penalty (WGAN-GP) [[117](#bib.bib117)] was used to calculate the
    discriminator score between the target image $I$ and the canvas $C_{t}$, and the
    change in discriminator score across an action (time-step) was used as the reward.
    The agent that predicted the stroke parameters was trained in actor-critic [[262](#bib.bib262)]
    fashion with backbone similar to Resnet18 [[133](#bib.bib133)], and the stroke
    parameters by the actor were used by the neural renderer network to predict paint
    strokes. The network structure of the neural renderer and discriminator consisted
    of multiple convolutions and fully connected blocks.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'A method for guiding medical robots using Ultrasound images with the help of
    DRL was proposed by [[129](#bib.bib129)]. The authors treated the problem as an
    MDP where the agent takes the Ultrasound images as input and estimates the state
    hence the problem became Partially observable MDP (POMDP). They used Double-DQN
    and Duel-DQN for estimating Q-Values and ResNet18 [[133](#bib.bib133)] backbone
    for extracting feature to be used by the algorithm along with Prioritized Replay
    Memory. In their implementation the action space consisted of 8 actions (up, down,
    left, right, and stop), probe position as compared to the sacrum was used as the
    state and the reward was calculated by considering the agent position as compared
    to the target (Move closer: 0.05, Move away: -0.1, Correct stop: 1.0, Incorrect
    stop: -0.25). In their implementation, the authors proposed various architectures
    such as V-DQN, M-DQN, and MS-DQN for the task and performed experimentation on
    Ultrasound images.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'Crowd counting is considered a tricky task in computer vision and is even trickier
    for humans. A DRL method for crowd counting was proposed by [[230](#bib.bib230)],
    where the authors used sequential decision making to approach the task through
    RL. In the specified work, the authors proposed a DQN agent (LibraNet) based on
    the motivation of a weighing scale. In their implementation crowd counting was
    modeled using a weighing scale where the agent was responsible for adding weights
    on one side of the scale sequentially to balance the crowded image on the other
    side. The problem of adding weights on one side of the pan for balancing was formulated
    as an MDP, where state consisted weight vector $W_{t}$ and image feature vector
    $FV_{I}$, and the actions space was defined similar to scale weighing and money
    system [[372](#bib.bib372)] containing values $(-10,-5,-2,-1,+1,+2,+5,+10,end)$.
    For reinforcing the agent two different rewards: ending reward and intermediate
    reward were utilized, where ending reward (following [[43](#bib.bib43)]) was calculated
    by comparing the absolute value error between the ground-truth count and the accumulated
    value with the error tolerance, and three counting specific rewards: force ending
    reward, guiding reward and squeezing reward were calculated for the intermediate
    rewards.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Exposure bracketing is a method used in digital photography, where one scene
    is captured using multiple exposures for getting a high dynamic range (HDR) image.
    An RL method for automated bracketing selection was proposed by [[389](#bib.bib389)].
    For flexible automated bracketing selection, an exposure bracketing selection
    network (EBSNet) was proposed for selecting optimal exposure bracketing and a
    multi-exposure fusion network (MEFNet) for generating an HDR image from selected
    exposure bracketing which consisted of 3 images. Since there is no ground truth
    for the exposure bracketing selection procedure, an RL scheme was utilized to
    train the agent (EBSNet). The authors also introduced a novel dataset consisting
    of a single auto-exposure image that was used as input to the EBSNet, 10 images
    with varying exposures from which EBSNet generated probability distribution for
    120 possible candidate exposure bracketing ($C^{3}_{10}$) and a reference HDR
    image. The reward for EBSNet was defined as the difference between peak signal-to-noise
    ratio between generated and reference HDR for the current and previous iteration,
    and the MEFNet was trained by minimizing the Charbonnier loss [[23](#bib.bib23)].
    For performing the action of bracketing selection ESBNet consisted of a semantic
    branch using AlexNet [[188](#bib.bib188)] for feature extraction, an illumination
    branch to understand the global and local illuminations by calculating a histogram
    of input and feeding it to CNN layers, and a policy module to generate a probability
    distribution for the candidate exposure bracketing from semantic and illumination
    branches. The neural network for MEFNet was derived from HDRNet [[103](#bib.bib103)].
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous driving in an urban environment is a challenging task, because of
    a large number of environmental variables and constraints. A DRL approach to this
    problem was proposed by [[361](#bib.bib361)]. In their implementation, the authors
    proposed an end-to-end model-free RL method, where they introduced a novel technique
    called Implicit Affordances. For the environment, the CARLA Simulator [[80](#bib.bib80)]
    was utilized, which provided the observations and the training reward was obtained
    by using the CARLA waypoint API. In the novel implicit affordances technique the
    training was broken into two phases, The first phase included using a Resnet18
    [[133](#bib.bib133)] encoder to predict the state of various environment variables
    such as traffic light, pedestrians, position with respect to the center lane,
    etc., and the output features were used as a state for the RL agent, For which
    a modified version of Rainbow-IQN Ape-X [[136](#bib.bib136)] was used. CARLA simulator
    accepts actions in form of continuous steering and throttle values, so to make
    it work with Rainbow-IQN which supports discrete actions, the authors sampled
    steering values into 9 or 27 discrete values and throttle into 4 discrete values
    (including braking), making a total of 36($9\times 4$) or 108($27\times 4$) actions.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Racial discrimination has been one of the hottest topics of the 21st century.
    To mitigate racial discrimination in facial recognition, [[382](#bib.bib382)]
    proposed a facial recognition method using skewness-aware RL. According to the
    authors, the reason for racial bias in facial recognition algorithms can be either
    due to the data or due to the algorithm, so the authors provided two ethnicity-aware
    datasets, BUPT-Globalface and BUPT-Balancedface along with an RL based race balanced
    network (RL-RBN). In their implementation, the authors formulated an MDP for adaptive
    margin policy learning where the state consisted of three parts: the race group
    (0: Indian, 1: Asian, 2: African), current adaptive margin, and bias or the skewness
    between the race group and Caucasians. A DQN was used as a policy network that
    performed three actions (staying the same, shifting to a larger value, and shifting
    to a smaller value) to change the adaptive margin, and accepted reward in form
    of change in the sum of inter-class and intra-class bias.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms are currently gaining popularity because of their powerful
    ability in eliminating uninformative parts of the input to leverage the other
    parts having a more useful information. Recently, attention mechanism has been
    integrated into typical CNN models at every individual layer to strengthen the
    intermediate outputs of each layer, in turn improving the final predictions for
    recognition in images. This model is usually trained with a weakly supervised
    method, however, this optimization method may lead to sub-optimal weights in the
    attention module. Hence, [[212](#bib.bib212)] proposed to train attention module
    by deep Q-learning with an LSTM model is trained to predict the reward, the whole
    process is called Deep REinforced Attention Learning (DREAL).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Various works specified here have been summarised and compared in Table LABEL:tab:oth
    and general implementation of a DRL method to control an agents movement in an
    environment has been shown in fig [17](#S11.F17 "Figure 17 â€£ 11 Others Applications
    â€£ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") where
    state consists of an image frame provided by the environment, the DRL agent predicts
    actions to move the agent in the environment providing next state and the reward
    is provided by the environment, for example, [[142](#bib.bib142)].'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 12 Future Perspectives
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 12.1 Challenge Discussion
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DRL is a powerful framework, which has been successfully applied to various
    computer vision applications including landmark detection, object detection, object
    tracking, image registration, image segmentation, video analysis, and other computer
    vision applications. DRL has also demonstrated to be an effective alternative
    for solving difficult optimization problems, including tuning parameters, selecting
    augmentation strategies, and neural architecture search (NAS). However, most approaches,
    that we have reviewed, assume a stationary environment, from which observations
    are made. Take landmark detection as an instance, the environment takes into account
    the image itself, and each state is defined as an image patch consisting of the
    landmark location. In such a case, the environment is known while the RL/DRL framework
    naturally accommodates a dynamic environment, that is the environment itself evolves
    with the state and action. Realizing the full potential of DRL for computer vision
    requires solving several challenges. In this section, we would like to discuss
    the challenges of DRL in computer vision for real-world systems.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward function: In most real-world applications, it is hard to define a specified
    reward function because it requires the knowledge from different domains that
    may not always be available. Thus, the intermediate rewards at each time step
    are not always easily computed. Furthermore, a reward function with too long delay
    will make training difficult. In contrast, assigning a reward for each action
    requires careful and manual human design.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Continuous state and action space: Training an RL system on a continuous state
    and action space is challenging because most RL algorithms, i.e. Q learning, can
    only deal with discrete states and discrete action space. To address this limitation,
    most existing works discretize the continuous state and action space.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'High-dimensional state and action space: Training Q-function on a high-dimensional
    action space is challenging. For this reason, existing works use low-dimensional
    parameterization, whose dimensions are typically less than 10 with an exception
    [[184](#bib.bib184)] that uses 15-D and 25-D to model 2D and 3D registration,
    respectively.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Environment is complicated: Almost all real-world systems, where we would want
    to deploy DRL/RL, are partially observable and non-stationary. Currently, the
    approaches we have reviewed assume a stationary environment, from which observations
    are made. However, the DRL/RL framework naturally accommodates dynamic environment,
    that is the environment itself evolves with the state and action. Furthermore,
    those systems are often stochastic and noisy (action delay, sensor and action
    noise) as compared to most simulated environments.'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training data requirement: RL/DRL requires a large amount of training data
    or expert demonstrations. Large-scale datasets with annotations are expensive
    and hard to come by.'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: More details of challenges that embody difficulties to deploy RL/DRL in the
    real world are discussed in [[82](#bib.bib82)]. In this work, they designed a
    set of experiments and analyzed their effects on common RL agents. Open-sourcing
    an environmental suite, [realworldrl-suite](https://github.com/google-research/realworldrl_suite)
    [[83](#bib.bib83)] is provided in this work as well.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 DRL Recent Advances
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some advanced DRL approaches such as Inverse DRL, Multi-agent DRL, Meta DRL,
    and imitation learning are worth the attention and may promote new insights for
    many machine learning and computer vision tasks.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inverse DRL: DRL has been successfully applied into domains where the reward
    function is clearly defined. However, this is limited in real-world applications
    because it requires knowledge from different domains that may not always be available.
    Inverse DRL is one of the special cases of imitation learning. An example is autonomous
    driving, the reward function should be based on all factors such as driverâ€™s behavior,
    gas consumption, time, speed, safety, driving quality, etc. In real-world scenario,
    it is exhausting and hard to control all these factors. Different from DRL, inverse
    DRL [[278](#bib.bib278)], [[4](#bib.bib4)], [[413](#bib.bib413)], [[86](#bib.bib86)]
    a specific form of imitation learning [[286](#bib.bib286)], infers the reward
    function of an agent, given its policy or observed behavior, thereby avoiding
    a manual specification of its reward function. In the same problem of autonomous
    driving, inverse RL first uses a dataset collected from the human-generated driving
    and then approximates the reward function. Inverse RL has been successfully applied
    to many domains [[4](#bib.bib4)]. Recently, to analyze complex human movement
    and control high-dimensional robot systems, [[215](#bib.bib215)] proposed an online
    inverse RL algorithm. [[2](#bib.bib2)] combined both RL and Inverse RL to address
    planning problems in autonomous driving.'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-Agent DRL: Most of the successful DRL applications such as game[[38](#bib.bib38)],
    [[376](#bib.bib376)], robotics[[181](#bib.bib181)], and autonomous driving [[335](#bib.bib335)],
    stock trading [[206](#bib.bib206)], social science [[207](#bib.bib207)], etc.,
    involve multiple players that requires a model with multi-agent. Take autonomous
    driving as an instance, multi-agent DRL addresses the sequential decision-making
    problem which involves many autonomous agents, each of which aims to optimize
    its own utility return by interacting with the environment and other agents [[40](#bib.bib40)].
    Learning in a multi-agent scenario is more difficult than a single-agent scenario
    because non-stationarity [[135](#bib.bib135)], multi-dimensionality [[40](#bib.bib40)],
    credit assignment [[5](#bib.bib5)], etc., depend on the multi-agent DRL approach
    of either fully cooperative or fully competitive. The agents can either collaborate
    to optimize a long-term utility or compete so that the utility is summed to zero.
    Recent work on Multi-Agent RL pays attention to learning new criteria or new setup
    [[348](#bib.bib348)].'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta DRL: As aforementioned, DRL algorithms consume large amounts of experience
    in order to learn an individual task and are unable to generalize the learned
    policy to newer problems. To alleviate the data challenge, Meta-RL algorithms
    [[330](#bib.bib330)], [[380](#bib.bib380)] are studied to enable agents to learn
    new skills from small amounts of experience. Recently, there is a research interest
    in meta RLÂ [[271](#bib.bib271)], [[119](#bib.bib119)], [[322](#bib.bib322)], [[303](#bib.bib303)],
    [[229](#bib.bib229)], each using a different approach. For benchmarking and evaluation
    of meta RL algorithms, [[415](#bib.bib415)] presented Meta-world, which is an
    open-source simulator consisting of 50 distinct robotic manipulation tasks.'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Imitation Learning: Imitation learning is close to learning from demonstrations
    which aims at training a policy to mimic an expertâ€™s behavior given the samples
    collected from that expert. Imitation learning is also considered as an alternative
    to RL/DRL to solve sequential decision-making problems. Besides inverse DRL, an
    imitation learning approach as aforementioned, behavior cloning is another imitation
    learning approach to train policy under supervised learning manner. Bradly et
    al. [[347](#bib.bib347)] presented a method for unsupervised third-person imitation
    learning to observe how other humans perform and infer the task. Building on top
    of Deep Deterministic Policy Gradients and Hindsight Experience Replay, Nair et
    al. [[272](#bib.bib272)] proposed behavior cloning Loss to increase imitating
    the demonstrations. Besides Q-learning, Generative Adversarial Imitation Learning
    [[364](#bib.bib364)] proposes P-GAIL that integrates imitation learning into the
    policy gradient framework. P-GAIL considers both smoothness and causal entropy
    in policy update by utilizing Deep P-Network [[365](#bib.bib365)].'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning (DRL) is nowadays the most popular technique for
    an artificial agent to learn closely optimal strategies by experiences. This paper
    aims to provide a state-of-the-art comprehensive survey of DRL applications to
    a variety of decision-making problems in the area of computer vision. In this
    work, we firstly provided a structured summarization of the theoretical foundations
    in Deep Learning (DL) including AutoEncoder (AE), Multi-Layer Perceptron (MLP),
    Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN). We then
    continued to introduce key techniques in RL research including model-based methods
    (value functions, transaction models, policy search, return functions) and model-free
    methods (value-based, policy-based, and actor-critic). Main techniques in DRL
    were thirdly presented under two categories of model-based and model-free approaches.
    We fourthly surveyed the broad-ranging applications of DRL methods in solving
    problems affecting areas of computer vision, from landmark detection, object detection,
    object tracking, image registration, image segmentation, video analysis, and many
    other applications in the computer vision area. We finally discussed several challenges
    ahead of us in order to realize the full potential of DRL for computer vision.
    Some latest advanced DRL techniques were included in the last discussion.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Model-based contextual policy search for data-efficient generalization
    of robot skills. Artificial Intelligence, 247:415 â€“ 439, 2017.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Advanced planning for autonomous vehicles using reinforcement learning
    and deep inverse reinforcement learning. Robotics and Autonomous Systems, 114:1
    â€“ 18, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Pieter Abbeel, Adam Coates, and AndrewÂ Y. Ng. Autonomous helicopter aerobatics
    through apprenticeship learning. The International Journal of Robotics Research,
    29(13):1608â€“1639, 2010.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Pieter Abbeel and AndrewÂ Y. Ng. Apprenticeship learning via inverse reinforcement
    learning. In Proceedings of the Twenty-First International Conference on Machine
    Learning, pages 1â€“8\. Association for Computing Machinery, 2004.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] AdrianÂ K. Agogino and Kagan Tumer. Unifying temporal and structural credit
    assignment problems. In Proceedings of the Third International Joint Conference
    on Autonomous Agents and Multiagent Systems - Volume 2, page 980â€“987\. IEEE Computer
    Society, 2004.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Narges Ahmidi, Lingling Tao, Shahin Sefati, Yixin Gao, Colin Lea, BenjaminÂ Bejar
    Haro, Luca Zappella, Sanjeev Khudanpur, RenÃ© Vidal, and GregoryÂ D Hager. A dataset
    and benchmarks for segmentation and recognition of gestures in robotic surgery.
    IEEE Transactions on Biomedical Engineering, 64(9):2025â€“2041, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] WalidÂ Abdullah Al and IlÂ Dong Yun. Partial policy-based reinforcement learning
    for anatomical landmark localization in 3d medical images. IEEE transactions on
    medical imaging, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] WalidÂ Abdullah Al, IlÂ Dong Yun, and KyongÂ Joon Lee. Reinforcement learning-based
    automatic diagnosis of acute appendicitis in abdominal ct. arXiv preprint arXiv:1909.00617,
    2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Stephan Alaniz. Deep reinforcement learning with model learning and monte
    carlo tree search in minecraft. In Conference on Reinforcement Learning and Decision
    Making, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Amir Alansary, Ozan Oktay, Yuanwei Li, Loic LeÂ Folgoc, Benjamin Hou, Ghislain
    Vaillant, Konstantinos Kamnitsas, Athanasios Vlontzos, Ben Glocker, Bernhard Kainz,
    etÂ al. Evaluating reinforcement learning agents for anatomical landmark detection.
    Medical image analysis, 53:156â€“164, 2019.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection
    using reconstruction probability. Special Lecture on IE, 2(1):1â€“18, 2015.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] O.Â Andersson, F.Â Heintz, and P.Â Doherty. Model-based reinforcement learning
    in continuous environments using real-time constrained optimization. In AAAI,
    2015.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Kai Arulkumaran, MarcÂ Peter Deisenroth, Miles Brundage, and AnilÂ Anthony
    Bharath. A brief survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866,
    2017.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] SÂ AvinashÂ Ramakanth and RÂ VenkateshÂ Babu. Seamseg: Video object segmentation
    using patch seams. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 376â€“383, 2014.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Morgane Ayle, Jimmy Tekli, Julia El-Zini, Boulos El-Asmar, and Mariette
    Awad. Bar-a reinforcement learning agent for bounding-box automated refinement.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan
    Kautz. GA3C: gpu-based A3C for deep reinforcement learning. CoRR, abs/1611.06256,
    2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. Visual tracking with
    online multiple instance learning. In 2009 IEEE conference on computer vision
    and pattern recognition, pages 983â€“990\. IEEE, 2009.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Seung-Hwan Bae and Kuk-Jin Yoon. Robust online multi-object tracking based
    on tracklet confidence and online discriminative appearance learning. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 1218â€“1225,
    2014.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Seung-Hwan Bae and Kuk-Jin Yoon. Confidence-based data association and
    discriminative deep appearance learning for robust online multi-object tracking.
    IEEE transactions on pattern analysis and machine intelligence, 40(3):595â€“610,
    2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J.Â Bagnell. Learning decision: Robustness, uncertainty, and approximation.
    04 2012.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J.Â A. Bagnell and J.Â G. Schneider. Autonomous helicopter control using
    reinforcement learning policy search methods. In Proceedings 2001 ICRA. IEEE International
    Conference on Robotics and Automation (Cat. No.01CH37164), volumeÂ 2, pages 1615â€“1620,
    2001.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Neil Barakat, AÂ Nicholas Hone, and ThomasÂ E Darcie. Minimal-bracketing
    sets for high-dynamic-range image capture. IEEE Transactions on Image Processing,
    17(10):1864â€“1875, 2008.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] JonathanÂ T Barron. A general and adaptive robust loss function. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4331â€“4339,
    2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Cher Bass, Pyry Helkkula, Vincenzo DeÂ Paola, Claudia Clopath, and AnilÂ Anthony
    Bharath. Detection of axonal synapses in 3d two-photon images. PloS one, 12(9):e0183309,
    2017.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Miriam Bellver, Xavier GirÃ³-i Nieto, Ferran MarquÃ©s, and Jordi Torres.
    Hierarchical object detection with deep reinforcement learning. arXiv preprint
    arXiv:1611.03718, 2016.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term
    dependencies with gradient descent is difficult. IEEE Trans. Neural Networks,
    5(2):157â€“166, 1994.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] P.Â Bergmann, M.Â Fauser, D.Â Sattlegger, and C.Â Steger. Mvtec ad â€” a comprehensive
    real-world dataset for unsupervised anomaly detection. In 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 9584â€“9592, 2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking
    performance: the clear mot metrics. EURASIP Journal on Image and Video Processing,
    2008:1â€“10, 2008.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Luca Bertinetto, Jack Valmadre, JoaoÂ F Henriques, Andrea Vedaldi, and
    PhilipÂ HS Torr. Fully-convolutional siamese networks for object tracking. In European
    conference on computer vision, pages 850â€“865. Springer, 2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Shalabh Bhatnagar. An actorâ€“critic algorithm with function approximation
    for discounted cost constrained markov decision processes. Systems & Control Letters,
    59(12):760â€“766, 2010.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Shalabh Bhatnagar, RichardÂ S. Sutton, Mohammad Ghavamzadeh, and Mark Lee.
    Natural actorÃ¢-critic algorithms. Automatica, 45(11):2471 â€“ 2482, 2009.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] MichaelÂ J Black and Yaser Yacoob. Tracking and recognizing rigid and non-rigid
    facial motions using local parametric models of image motion. In Proceedings of
    IEEE international conference on computer vision, pages 374â€“381\. IEEE, 1995.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] NÂ Bloch, AÂ Madabhushi, HÂ Huisman, JÂ Freymann, JÂ Kirby, MÂ Grauer, AÂ Enquobahrie,
    CÂ Jaffe, LÂ Clarke, and KÂ Farahani. Nci-isbi 2013 challenge: automated segmentation
    of prostate structures. The Cancer Imaging Archive, 370, 2015.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J.Â Boedecker, J.Â T. Springenberg, J.Â WÃ¼lfing, and M.Â Riedmiller. Approximate
    real-time optimal control based on sparse gaussian process models. In 2014 IEEE
    Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),
    pages 1â€“8, 2014.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d region proposal
    network for object detection. In Proceedings of the IEEE International Conference
    on Computer Vision, Seoul, South Korea, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative interaction
    training for segmentation editing networks. In International Workshop on Machine
    Learning in Medical Imaging, pages 363â€“370\. Springer, 2018.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Leo Breiman. Bagging predictors. Machine learning, 24(2):123â€“140, 1996.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science,
    365(6456):885â€“890, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Antoine Buetti-Dinh, Vanni Galli, SÃƒÂ¶ren Bellenberg, Olga Ilie, Malte
    Herold, Stephan Christel, Mariia Boretska, IgorÂ V. Pivkin, Paul Wilmes, Wolfgang
    Sand, Mario Vera, and Mark Dopson. Deep neural networks outperform human expertâ€™s
    capacity in characterizing bioleaching bacterial biofilm composition. Biotechnology
    Reports, 22:e00321, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] L.Â Busoniu, R.Â Babuska, and B.Â De Schutter. A comprehensive survey of
    multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics,
    Part C (Applications and Reviews), 38(2):156â€“172, 2008.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-TaixÃ©,
    Daniel Cremers, and Luc VanÂ Gool. One-shot video object segmentation. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 221â€“230,
    2017.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Yunliang Cai, Said Osman, Manas Sharma, Mark Landis, and Shuo Li. Multi-modality
    vertebra recognition in arbitrary views using 3d deformable hierarchical model.
    IEEE transactions on medical imaging, 34(8):1676â€“1693, 2015.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] JuanÂ C Caicedo and Svetlana Lazebnik. Active object localization with
    deep reinforcement learning. In Proceedings of the IEEE international conference
    on computer vision, pages 2488â€“2496, 2015.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] D.Â Carrera, F.Â Manganini, G.Â Boracchi, and E.Â Lanzarone. Defect detection
    in sem images of nanofibrous materials. IEEE Transactions on Industrial Informatics,
    13(2):551â€“561, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and Huchuan Lu. Real-timeâ€™actor-criticâ€™tracking.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 318â€“334,
    2018.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
    AlanÂ L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets,
    atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis
    and machine intelligence, 40(4):834â€“848, 2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Yushi Chen, Xing Zhao, and Xiuping Jia. Spectralâ€“spatial classification
    of hyperspectral data based on deep belief network. IEEE Journal of Selected Topics
    in Applied Earth Observations and Remote Sensing, 8(6):2381â€“2392, 2015.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-Hsuan Yang. Segflow:
    Joint learning for video object segmentation and optical flow. In Proceedings
    of the IEEE international conference on computer vision, pages 686â€“695, 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Ming-Ming Cheng, NiloyÂ J Mitra, Xiaolei Huang, PhilipÂ HS Torr, and Shi-Min
    Hu. Global contrast based salient region detection. IEEE transactions on pattern
    analysis and machine intelligence, 37(3):569â€“582, 2014.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Kyunghyun Cho, Bart VanÂ MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using rnn encoder-decoder for statistical machine translation. arXiv preprint
    arXiv:1406.1078, 2014.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Kyunghyun Cho, Bart van Merrienboer, Ã‡aglar GÃ¼lÃ§ehre, Fethi Bougares,
    Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
    for statistical machine translation. CoRR, abs/1406.1078, 2014.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Jongwon Choi, Hyung JinÂ Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris,
    and Jin YoungÂ Choi. Attentional correlation filter network for adaptive visual
    tracking. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pages 4807â€“4816, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Wongun Choi. Near-online multi-target tracking with aggregated local flow
    descriptor. In Proceedings of the IEEE international conference on computer vision,
    pages 3029â€“3037, 2015.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, and Yoshua
    Bengio. Attention-based models for speech recognition. CoRR, abs/1506.07503, 2015.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] QiÂ Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai
    Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal
    attention mechanism. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 4836â€“4845, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Wen-Hsuan Chu and KrisÂ M. Kitani. Neural batch sampling with reinforcement
    learning for semi-supervised anomaly detection. In European Conference on Computer
    Vision, pages 751â€“766, 2020.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Wen-Sheng Chu, Yale Song, and Alejandro Jaimes. Video co-summarization:
    Video summarization by visual co-occurrence. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 3584â€“3592, 2015.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim
    Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy
    optimization. CoRR, abs/1809.05214, 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Adam Coates, Pieter Abbeel, and AndrewÂ Y. Ng. Apprenticeship learning
    for helicopter control. Commun. ACM, 52(7):97â€“105, July 2009.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Dorin Comaniciu, Visvanathan Ramesh, and Peter Meer. Real-time tracking
    of non-rigid objects using mean shift. In Proceedings IEEE Conference on Computer
    Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662), volumeÂ 2, pages
    142â€“149\. IEEE, 2000.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
    Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset
    for semantic urban scene understanding. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 3213â€“3223, 2016.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] RÃ©mi Coulom. Efficient selectivity and backup operators in monte-carlo
    tree search. In Proceedings of the 5th International Conference on Computers and
    Games, page 72â€“83, 2006.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation
    for games, robotics and machine learning. 2016.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Antonio Criminisi, Jamie Shotton, Duncan Robertson, and Ender Konukoglu.
    Regression forests for efficient anatomy detection and localization in ct studies.
    In International MICCAI Workshop on Medical Computer Vision, pages 106â€“117\. Springer,
    2010.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Tianhong Dai, Magda Dubois, Kai Arulkumaran, Jonathan Campbell, Cher Bass,
    Benjamin Billot, Fatmatulzehra Uslu, Vincenzo DeÂ Paola, Claudia Clopath, and AnilÂ Anthony
    Bharath. Deep reinforcement learning for subpixel neural tracking. In International
    Conference on Medical Imaging with Deep Learning, pages 130â€“150, 2019.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Martin Danelljan, Goutam Bhat, Fahad ShahbazÂ Khan, and Michael Felsberg.
    Eco: Efficient convolution operators for tracking. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 6638â€“6646, 2017.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Martin Danelljan, Gustav Hager, Fahad ShahbazÂ Khan, and Michael Felsberg.
    Learning spatially regularized correlation filters for visual tracking. In Proceedings
    of the IEEE international conference on computer vision, pages 4310â€“4318, 2015.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Kristopher DeÂ Asis, JÂ Fernando Hernandez-Garcia, GÂ Zacharias Holland,
    and RichardÂ S Sutton. Multi-step reinforcement learning: A unifying algorithm.
    In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Sandra ElizaÂ Fontes DeÂ Avila, Ana PaulaÂ BrandÃ£o Lopes, Antonio daÂ LuzÂ Jr,
    and Arnaldo deÂ AlbuquerqueÂ AraÃºjo. Vsumm: A mechanism designed to produce static
    video summaries and a novel evaluation method. Pattern Recognition Letters, 32(1):56â€“68,
    2011.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Antonio deÂ Marvao, TimothyÂ JW Dawes, Wenzhe Shi, Christopher Minas, NiallÂ G
    Keenan, Tamara Diamond, Giuliana Durighel, Giovanni Montana, Daniel Rueckert,
    StuartÂ A Cook, etÂ al. Population-based studies of myocardial hypertrophy: high
    resolution cardiovascular magnetic resonance atlases improve statistical power.
    Journal of cardiovascular magnetic resonance, 16(1):16, 2014.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M.Â P. Deisenroth, P.Â Englert, J.Â Peters, and D.Â Fox. Multi-task policy
    search for robotics. In 2014 IEEE International Conference on Robotics and Automation
    (ICRA), pages 3876â€“3881, 2014.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiÂ Fei-Fei.
    Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on
    computer vision and pattern recognition, pages 248â€“255\. Ieee, 2009.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface:
    Additive angular margin loss for deep face recognition. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 4690â€“4699, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Joachim Denzler and DietrichÂ WR Paulus. Active motion detection and object
    tracking. In Proceedings of 1st International Conference on Image Processing,
    volumeÂ 3, pages 635â€“639\. IEEE, 1994.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] B.Â Depraetere, M.Â Liu, G.Â Pinte, I.Â Grondman, and R.Â BabuÃ…Â¡ka. Comparison
    of model-free and model-based methods for time optimal hit control of a badminton
    robot. Mechatronics, 24(8):1021 â€“ 1030, 2014.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, SÂ Swaroop Vedula,
    GyusungÂ I Lee, MijaÂ R Lee, and GregoryÂ D Hager. Recognizing surgical activities
    with recurrent neural networks. In International conference on medical image computing
    and computer-assisted intervention, pages 551â€“558\. Springer, 2016.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Piotr DollÃ¡r, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian
    detection: A benchmark. In 2009 IEEE Conference on Computer Vision and Pattern
    Recognition, pages 304â€“311\. IEEE, 2009.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Jeff Donahue, LisaÂ Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach,
    Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional
    networks for visual recognition and description. CoRR, abs/1411.4389, 2014.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
    Vladimir Golkov, Patrick Van DerÂ Smagt, Daniel Cremers, and Thomas Brox. Flownet:
    Learning optical flow with convolutional networks. In Proceedings of the IEEE
    international conference on computer vision, pages 2758â€“2766, 2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
    Koltun. Carla: An open urban driving simulator. arXiv preprint arXiv:1711.03938,
    2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Yong Du, Wei Wang, and Liang Wang. Hierarchical recurrent neural network
    for skeleton based action recognition. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 1110â€“1118, 2015.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Gabriel Dulac-Arnold, Nir Levine, DanielÂ J. Mankowitz, Jerry Li, Cosmin
    Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges
    of real-world reinforcement learning, 2020.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Gabriel Dulac-Arnold, Nir Levine, DanielÂ J. Mankowitz, Jerry Li, Cosmin
    Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges
    of real-world reinforcement learning. 2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Matteo Dunnhofer, Niki Martinel, Gian LucaÂ Foresti, and Christian Micheloni.
    Visual tracking by means of deep reinforcement learning and an expert demonstrator.
    In Proceedings of the IEEE International Conference on Computer Vision Workshops,
    pages 0â€“0, 2019.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] ChiÂ Nhan Duong, KhaÂ Gia Quach, Ibsa Jalata, Ngan Le, and Khoa Luu. Mobiface:
    A lightweight deep learning face recognition on mobile devices. In 2019 IEEE 10th
    International Conference on Biometrics Theory, Applications and Systems (BTAS),
    pages 1â€“6\. IEEE, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] ChiÂ Nhan Duong, KhaÂ Gia Quach, Khoa Luu, T.Â Hoang Le, Marios Savvides,
    and TienÂ D. Bui. Learning from longitudinal face demonstrationâ€“where tractable
    deep modeling meets inverse reinforcement learning. 127(6â€“7), 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A.Â El-Fakdi and M.Â Carreras. Policy gradient based reinforcement learning
    for real autonomous underwater cable tracking. In 2008 IEEE/RSJ International
    Conference on Intelligent Robots and Systems, pages 3635â€“3640, 2008.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Ehsan Elhamifar, Guillermo Sapiro, and Rene Vidal. See all by looking
    at a few: Sparse modeling for finding representative objects. In 2012 IEEE conference
    on computer vision and pattern recognition, pages 1600â€“1607\. IEEE, 2012.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov.
    Scalable object detection using deep neural networks. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 2147â€“2154, 2014.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Mark Everingham, Luc VanÂ Gool, ChristopherÂ KI Williams, John Winn, and
    Andrew Zisserman. The pascal visual object classes challenge 2007 (voc2007) results.
    2007.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Mark Everingham and John Winn. The pascal visual object classes challenge
    2012 (voc2012) development kit. Pattern Analysis, Statistical Modelling and Computational
    Learning, Tech. Rep, 8, 2011.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Heng Fan, Liting Lin, Fan Yang, Peng Chu, GeÂ Deng, Sijia Yu, Hexin Bai,
    Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale
    single object tracking. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 5374â€“5383, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Heng Fan and Haibin Ling. Parallel tracking and verifying: A framework
    for real-time and high accuracy visual tracking. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 5486â€“5494, 2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Jialue Fan, Wei Xu, Ying Wu, and Yihong Gong. Human tracking using convolutional
    neural networks. IEEE Transactions on Neural Networks, 21(10):1610â€“1623, 2010.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Chelsea Finn, XinÂ Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and
    Pieter Abbeel. Deep spatial autoencoders for visuomotor learning. In Danica Kragic,
    Antonio Bicchi, and AlessandroÂ De Luca, editors, 2016 IEEE International Conference
    on Robotics and Automation, ICRA 2016, Stockholm, Sweden, May 16-21, 2016, pages
    512â€“519.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] JÂ Michael Fitzpatrick and JayÂ B West. The distribution of target registration
    error in rigid-body point-based registration. IEEE transactions on medical imaging,
    20(9):917â€“927, 2001.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Vincent FranÃ§ois-Lavet, Peter Henderson, Riashat Islam, MarcÂ G Bellemare,
    and Joelle Pineau. An introduction to deep reinforcement learning. arXiv preprint
    arXiv:1811.12560, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SMÂ Eslami, and Oriol
    Vinyals. Synthesizing programs for images using reinforced adversarial learning.
    arXiv preprint arXiv:1804.01118, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Mingfei Gao, Ruichi Yu, Ang Li, VladÂ I Morariu, and LarryÂ S Davis. Dynamic
    zoom-in network for fast object detection in large images. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 6926â€“6935, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Yixin Gao, SÂ Swaroop Vedula, CarolÂ E Reiley, Narges Ahmidi, Balakrishnan
    Varadarajan, HenryÂ C Lin, Lingling Tao, Luca Zappella, BenjamÄ±n BÃ©jar, DavidÂ D
    Yuh, etÂ al. Jhu-isi gesture and skill assessment working set (jigsaws): A surgical
    activity dataset for human motion modeling. In Miccai workshop: M2cai, volumeÂ 3,
    pageÂ 3, 2014.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Romane Gauriau, RÃ©mi Cuingnet, David Lesage, and Isabelle Bloch. Multi-organ
    localization combining global-to-local regression and confidence maps. In International
    Conference on Medical Image Computing and Computer-Assisted Intervention, pages
    337â€“344\. Springer, 2014.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A.Â Geiger, P.Â Lenz, and R.Â Urtasun. Are we ready for autonomous driving?
    the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and
    Pattern Recognition, pages 3354â€“3361, 2012.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] MichaÃ«l Gharbi, Jiawen Chen, JonathanÂ T Barron, SamuelÂ W Hasinoff, and
    FrÃ©do Durand. Deep bilateral learning for real-time image enhancement. ACM Transactions
    on Graphics (TOG), 36(4):1â€“12, 2017.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] FlorinÂ C Ghesu, Edward Krubasik, Bogdan Georgescu, Vivek Singh, Yefeng
    Zheng, Joachim Hornegger, and Dorin Comaniciu. Marginal space deep learning: efficient
    architecture for volumetric image parsing. IEEE transactions on medical imaging,
    35(5):1217â€“1228, 2016.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Florin-Cristian Ghesu, Bogdan Georgescu, Yefeng Zheng, Sasa Grbic, Andreas
    Maier, Joachim Hornegger, and Dorin Comaniciu. Multi-scale deep reinforcement
    learning for real-time 3d-landmark detection in ct scans. IEEE transactions on
    pattern analysis and machine intelligence, 41(1):176â€“189, 2017.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] MÂ Giles. Mit technology review. Google researchers have reportedly achievedâ€
    quantum supremacyâ€ URL: https:/www.technologyreview. com/f, 614416, 2017.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Justin Girard and MÂ Reza Emami. Concurrent markov decision processes
    for robot team learning. Engineering applications of artificial intelligence,
    39:223â€“234, 2015.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference
    on computer vision, pages 1440â€“1448, 2015.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich
    feature hierarchies for accurate object detection and semantic segmentation. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 580â€“587, 2014.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Georgia Gkioxari, Ross Girshick, and Jitendra Malik. Contextual action
    recognition with r* cnn. In Proceedings of the IEEE international conference on
    computer vision, pages 1080â€“1088, 2015.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Vikash Goel, Jameson Weng, and Pascal Poupart. Unsupervised video object
    segmentation for deep reinforcement learning. In Advances in Neural Information
    Processing Systems, pages 5683â€“5694, 2018.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Abel Gonzalez-Garcia, Alexander Vezhnevets, and Vittorio Ferrari. An
    active search strategy for efficient object class detection. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 3022â€“3031,
    2015.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Advances in neural information processing systems, pages 2672â€“2680, 2014.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Leo Grady. Random walks for image segmentation. IEEE transactions on
    pattern analysis and machine intelligence, 28(11):1768â€“1783, 2006.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Alex Graves, Abdel-rahman Mohamed, and GeoffreyÂ E. Hinton. Speech recognition
    with deep recurrent neural networks. CoRR, abs/1303.5778, 2013.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Albert Gubern-MÃ©rida, Robert MartÃ­, Jaime Melendez, JakobÂ L Hauth, RitseÂ M
    Mann, Nico Karssemeijer, and Bram Platel. Automated localization of breast cancer
    in dce-mri. Medical image analysis, 20(1):265â€“274, 2015.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
    AaronÂ C Courville. Improved training of wasserstein gans. In Advances in neural
    information processing systems, pages 5767â€“5777, 2017.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Minghao Guo, Jiwen Lu, and Jie Zhou. Dual-agent deep reinforcement learning
    for deformable face tracking. In Proceedings of the European Conference on Computer
    Vision (ECCV), pages 768â€“783, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey
    Levine. Meta-reinforcement learning of structured exploration strategies. In Advances
    in Neural Information Processing Systems, pages 5302â€“5311, 2018.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization
    and recognition of indoor scenes from rgb-d images. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 564â€“571, 2013.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Saurabh Gupta, Ross Girshick, Pablo ArbelÃ¡ez, and Jitendra Malik. Learning
    rich features from rgb-d images for object detection and segmentation. In European
    conference on computer vision, pages 345â€“360. Springer, 2014.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc VanÂ Gool.
    Creating summaries from user videos. In European conference on computer vision,
    pages 505â€“520. Springer, 2014.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Seyed HamidÂ Rezatofighi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony
    Dick, and Ian Reid. Joint probabilistic data association revisited. In Proceedings
    of the IEEE international conference on computer vision, pages 3047â€“3055, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Junwei Han, LeÂ Yang, Dingwen Zhang, Xiaojun Chang, and Xiaodan Liang.
    Reinforcement cutting-agent learning for video object segmentation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9080â€“9089,
    2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] RobertÂ M Haralick and LindaÂ G Shapiro. Image segmentation techniques.
    Computer vision, graphics, and image processing, 29(1):100â€“132, 1985.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Sam Hare, Stuart Golodetz, Amir Saffari, Vibhav Vineet, Ming-Ming Cheng,
    StephenÂ L Hicks, and PhilipÂ HS Torr. Struck: Structured output tracking with kernels.
    IEEE transactions on pattern analysis and machine intelligence, 38(10):2096â€“2109,
    2015.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Bharath Hariharan, Pablo ArbelÃ¡ez, Ross Girshick, and Jitendra Malik.
    Simultaneous detection and segmentation. In European Conference on Computer Vision,
    pages 297â€“312. Springer, 2014.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Bharath Hariharan, Pablo ArbelÃ¡ez, Ross Girshick, and Jitendra Malik.
    Hypercolumns for object segmentation and fine-grained localization. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 447â€“456,
    2015.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Hannes Hase, MohammadÂ Farid Azampour, Maria Tirindelli, Magdalini Paschali,
    Walter Simson, Emad Fatemizadeh, and Nassir Navab. Ultrasound-guided robotic navigation
    with deep reinforcement learning. arXiv preprint arXiv:2003.13321, 2020.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] HadoÂ V Hasselt. Double q-learning. In Advances in neural information
    processing systems, pages 2613â€“2621, 2010.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] MatthewÂ J. Hausknecht and Peter Stone. Deep recurrent q-learning for
    partially observable mdps. CoRR, abs/1507.06527, 2015.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. Mask r-cnn.
    In Proceedings of the IEEE international conference on computer vision, pages
    2961â€“2969, 2017.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual
    learning for image recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 770â€“778, 2016.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] JoÃ£oÂ F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed
    tracking with kernelized correlation filters. IEEE transactions on pattern analysis
    and machine intelligence, 37(3):583â€“596, 2014.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and EnriqueÂ Munoz
    deÂ Cote. A survey of learning in multiagent environments: Dealing with non-stationarity.
    CoRR, abs/1707.09183, 2017.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Matteo Hessel, Joseph Modayil, Hado VanÂ Hasselt, Tom Schaul, Georg Ostrovski,
    Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow:
    Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298,
    2017.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Todd Hester, Michael Quinlan, and Peter Stone. A real-time model-based
    reinforcement learning architecture for robot control. CoRR, abs/1105.1749, 2011.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks
    principle: Reading childrenâ€™s books with explicit memory representations. CoRR,
    abs/1511.02301, 2015.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735â€“1780, 1997.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Derek Hoiem, AlexeiÂ A Efros, and Martial Hebert. Recovering surface layout
    from an image. International Journal of Computer Vision, 75(1):151â€“172, 2007.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] JamesÂ B. Holliday and NganÂ T.H. Le. Follow then forage exploration: Improving
    asynchronous advantage actor critic. International Conference on Soft Computing,
    Artificial Intelligence and Applications (SAI 2020), pages 107â€“118, 2020.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang
    Chang, Hsuan-Kung Yang, Brian Hsi-Lin Ho, Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching
    Hsiao, etÂ al. Virtual-to-real: Learning to control in visual semantic segmentation.
    arXiv preprint arXiv:1802.00285, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] JuÂ HongÂ Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang, and Kuk-Jin Yoon. Online
    multi-object tracking via structural constraint event aggregation. In Proceedings
    of the IEEE Conference on computer vision and pattern recognition, pages 1392â€“1400,
    2016.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J.Â Hu, L.Â Shen, and G.Â Sun. Squeeze-and-excitation networks. In 2018
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7132â€“7141,
    2018.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. Jointly
    learning heterogeneous features for rgb-d activity recognition. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 5344â€“5352,
    2015.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Weiming Hu, XiÂ Li, Wenhan Luo, Xiaoqin Zhang, Stephen Maybank, and Zhongfei
    Zhang. Single and multiple object tracking using log-euclidean riemannian subspace
    and block-division appearance model. IEEE transactions on pattern analysis and
    machine intelligence, 34(12):2420â€“2440, 2012.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Gao Huang, Zhuang Liu, Laurens Van DerÂ Maaten, and KilianÂ Q Weinberger.
    Densely connected convolutional networks. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 4700â€“4708, 2017.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity
    benchmark for generic object tracking in the wild. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 2019.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Zhewei Huang, Wen Heng, and Shuchang Zhou. Learning to paint with model-based
    deep reinforcement learning. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 8709â€“8718, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for
    sequence tagging. arXiv preprint arXiv:1508.01991, 2015.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc VanÂ Gool. Deep learning
    on lie groups for skeleton-based action recognition. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 6099â€“6108, 2017.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] GabrielÂ Efrain Humpire-Mamani, Arnaud ArindraÂ Adiyoso Setio, Bram van
    Ginneken, and Colin Jacobs. Efficient organ localization using multi-label convolutional
    neural networks in thorax-abdomen ct scans. Physics in Medicine & Biology, 63(8):085003,
    2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Luis Ibanez, Will Schroeder, Lydia Ng, and Josh Cates. The itk software
    guide: updated for itk version 2.4, 2005.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. Multi-source
    multi-scale counting in extremely dense crowd images. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 2547â€“2554, 2013.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed,
    Nasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map estimation
    and localization in dense crowds. In Proceedings of the European Conference on
    Computer Vision (ECCV), pages 532â€“546, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy,
    and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2462â€“2470, 2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167,
    2015.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] CliffordÂ R JackÂ Jr, MattÂ A Bernstein, NickÂ C Fox, Paul Thompson, Gene
    Alexander, Danielle Harvey, Bret Borowski, PaulaÂ J Britson, Jennifer L.Â Whitwell,
    Chadwick Ward, etÂ al. The alzheimerâ€™s disease neuroimaging initiative (adni):
    Mri methods. Journal of Magnetic Resonance Imaging: An Official Journal of the
    International Society for Magnetic Resonance in Medicine, 27(4):685â€“691, 2008.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Max Jaderberg, Karen Simonyan, Andrew Zisserman, etÂ al. Spatial transformer
    networks. In Advances in neural information processing systems, pages 2017â€“2025,
    2015.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Deep features for
    text spotting. In European conference on computer vision, pages 512â€“528. Springer,
    2014.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Arjit Jain, Alexander Powers, and HansÂ J Johnson. Robust automatic multiple
    landmark detection. In 2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI), pages 1178â€“1182\. IEEE, 2020.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] SuyogÂ Dutt Jain and Kristen Grauman. Supervoxel-consistent foreground
    propagation in video. In European conference on computer vision, pages 656â€“671.
    Springer, 2014.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] SuyogÂ Dutt Jain, BoÂ Xiong, and Kristen Grauman. Fusionseg: Learning to
    combine motion and appearance for fully automatic segmentation of generic objects
    in videos. In 2017 IEEE conference on computer vision and pattern recognition
    (CVPR), pages 2117â€“2126\. IEEE, 2017.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Varun Jampani, Raghudeep Gadde, and PeterÂ V Gehler. Video propagation
    networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 451â€“461, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Won-Dong Jang and Chang-Su Kim. Online video object segmentation via
    convolutional trident network. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 5849â€“5858, 2017.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Simon JÃ©gou, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua
    Bengio. The one hundred layers tiramisu: Fully convolutional densenets for semantic
    segmentation. In Proceedings of the IEEE conference on computer vision and pattern
    recognition workshops, pages 11â€“19, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang. Model-based reinforcement
    learning with value-targeted regression. In Proceedings of the 2nd Conference
    on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning
    Research, pages 666â€“686, The Cloud, 10â€“11 Jun 2020.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Ming-xin Jiang, Chao Deng, Zhi-geng Pan, Lan-fang Wang, and Xing Sun.
    Multiobject tracking in videos based on lstm and deep reinforcement learning.
    Complexity, 2018, 2018.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Mingxin Jiang, Tao Hai, Zhigeng Pan, Haiyan Wang, Yinjie Jia, and Chao
    Deng. Multi-agent deep reinforcement learning for multi-object tracker. IEEE Access,
    7:32400â€“32407, 2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Lu, and Shuicheng
    Yan. Tree-structured reinforcement learning for sequential object localization.
    In Advances in Neural Information Processing Systems, pages 127â€“135, 2016.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Oscar Jimenez-del Toro, Henning MÃ¼ller, Markus Krenn, Katharina Gruenberg,
    AbdelÂ Aziz Taha, Marianne Winterstein, Ivan Eggel, Antonio Foncubierta-RodrÃ­guez,
    Orcun Goksel, AndrÃ¡s Jakab, etÂ al. Cloud-based evaluation of anatomical structure
    segmentation and landmark detection algorithms: Visceral anatomy benchmarks. IEEE
    transactions on medical imaging, 35(11):2459â€“2475, 2016.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] VÂ Craig Jordan. Long-term adjuvant tamoxifen therapy for breast cancer.
    Breast cancer research and treatment, 15(3):125â€“136, 1990.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Yeong JunÂ Koh and Chang-Su Kim. Primary object segmentation in videos
    based on region augmentation and reduction. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 3442â€“3450, 2017.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. Tracking-learning-detection.
    IEEE transactions on pattern analysis and machine intelligence, 34(7):1409â€“1422,
    2011.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models.
    Association for Computational Linguistics, October 2013.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] MichaÅ‚ Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech
    JaÅ›kowski. Vizdoom: A doom-based ai research platform for visual reinforcement
    learning. In 2016 IEEE Conference on Computational Intelligence and Games (CIG),
    pages 1â€“8\. IEEE, 2016.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] DuÂ Yong Kim and Moongu Jeon. Data fusion of radar and image measurements
    for multi-object tracking via kalman filtering. Information Sciences, 278:641â€“652,
    2014.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] KyeÂ Kyung Kim, SooÂ Hyun Cho, HaeÂ Jin Kim, and JaeÂ Yeon Lee. Detecting
    and tracking moving object using an active camera. In The 7th International Conference
    on Advanced Communication Technology, 2005, ICACT 2005., volumeÂ 2, pages 817â€“820\.
    IEEE, 2005.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Donna Kirwan. Nhs fetal anomaly screening programme. National Standards
    and Guidance for England, 18(0), 2010.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Stefan Klein, Marius Staring, Keelin Murphy, MaxÂ A Viergever, and JosienÂ PW
    Pluim. Elastix: a toolbox for intensity-based medical image registration. IEEE
    transactions on medical imaging, 29(1):196â€“205, 2009.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Jens Kober, J.Â Andrew Bagnell, and Jan Peters. Reinforcement learning
    in robotics: A survey. The International Journal of Robotics Research, 32(11):1238â€“1274,
    2013.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] VijayÂ R Konda and JohnÂ N Tsitsiklis. Actor-critic algorithms. In Advances
    in neural information processing systems, pages 1008â€“1014, 2000.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Philipp KrÃ¤henbÃ¼hl and Vladlen Koltun. Efficient inference in fully connected
    crfs with gaussian edge potentials. In Advances in neural information processing
    systems, pages 109â€“117, 2011.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Julian Krebs, Tommaso Mansi, HervÃ© Delingette, LiÂ Zhang, FlorinÂ C Ghesu,
    Shun Miao, AndreasÂ K Maier, Nicholas Ayache, Rui Liao, and Ali Kamen. Robust non-rigid
    registration through agent-based action learning. In International Conference
    on Medical Image Computing and Computer-Assisted Intervention, pages 344â€“352\.
    Springer, 2017.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder,
    Luka CehovinÂ Zajc, Tomas Vojir, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey,
    etÂ al. The sixth visual object tracking vot2018 challenge results. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 0â€“0, 2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin,
    Gustavo Fernandez, Tomas Vojir, Gustav Hager, Georg Nebehay, and Roman Pflugfelder.
    The visual object tracking vot2015 challenge results. In Proceedings of the IEEE
    international conference on computer vision workshops, pages 1â€“23, 2015.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Alex Krizhevsky, Ilya Sutskever, and GeoffreyÂ E Hinton. Imagenet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems, pages 1097â€“1105, 2012.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Alex Krizhevsky, Ilya Sutskever, and GeoffreyÂ E Hinton. Imagenet classification
    with deep convolutional neural networks. Communications of the ACM, 60(6):84â€“90,
    2017.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A.Â Kupcsik, M.Â Deisenroth, Jan Peters, and G.Â Neumann. Data-efficient
    generalization of robot skills with contextual policy search. In AAAI, 2013.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel.
    Model-ensemble trust-region policy optimization. 02 2018.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Ngan Le, Trung Le, Kashu Yamazaki, ToanÂ Duc Bui, Khoa Luu, and Marios
    Savides. Offset curves loss for imbalanced problem in medical segmentation. arXiv
    preprint arXiv:2012.02463, 2020.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Ngan Le, Kashu Yamazaki, Dat Truong, KhaÂ Gia Quach, and Marios Savvides.
    A multi-task contextual atrous residual network for brain tumor detection & segmentation.
    arXiv preprint arXiv:2012.02073, 2020.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] TÂ HoangÂ Ngan Le, ChiÂ Nhan Duong, Ligong Han, Khoa Luu, KhaÂ Gia Quach,
    and Marios Savvides. Deep contextual recurrent residual networks for scene labeling.
    Pattern Recognition, 80:32â€“41, 2018.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] TÂ HoangÂ Ngan Le, KhaÂ Gia Quach, Khoa Luu, ChiÂ Nhan Duong, and Marios
    Savvides. Reformulating level sets as deep recurrent neural network approach to
    semantic segmentation. IEEE Transactions on Image Processing, 27(5):2393â€“2407,
    2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Colin Lea, MichaelÂ D Flynn, Rene Vidal, Austin Reiter, and GregoryÂ D
    Hager. Temporal convolutional networks for action segmentation and detection.
    In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 156â€“165, 2017.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Colin Lea, Austin Reiter, RenÃ© Vidal, and GregoryÂ D Hager. Segmental
    spatiotemporal cnns for fine-grained action segmentation. In European Conference
    on Computer Vision, pages 36â€“52. Springer, 2016.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Colin Lea, RenÃ© Vidal, and GregoryÂ D Hager. Learning convolutional action
    primitives for fine-grained action recognition. In 2016 IEEE international conference
    on robotics and automation (ICRA), pages 1642â€“1649\. IEEE, 2016.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Colin Lea, Rene Vidal, Austin Reiter, and GregoryÂ D Hager. Temporal convolutional
    networks: A unified approach to action segmentation. In European Conference on
    Computer Vision, pages 47â€“54. Springer, 2016.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Laura Leal-TaixÃ©, Cristian Canton-Ferrer, and Konrad Schindler. Learning
    by tracking: Siamese cnn for robust target association. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 33â€“40,
    2016.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Laura Leal-TaixÃ©, Michele Fenzi, Alina Kuznetsova, Bodo Rosenhahn, and
    Silvio Savarese. Learning an image-based motion context for multiple people tracking.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 3542â€“3549, 2014.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Laura Leal-TaixÃ©, Anton Milan, Ian Reid, Stefan Roth, and Konrad Schindler.
    Motchallenge 2015: Towards a benchmark for multi-target tracking. arXiv preprint
    arXiv:1504.01942, 2015.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
    com/exdb/mnist/, 1998.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Yann LeCun, LÃ©on Bottou, GenevieveÂ B Orr, and Klaus-Robert MÃ¼ller. Efficient
    backprop. In Neural networks: Tricks of the trade, pages 9â€“50\. Springer, 1998.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Yann LeCun, DÂ Touresky, GÂ Hinton, and TÂ Sejnowski. A theoretical framework
    for back-propagation. In Proceedings of the 1988 connectionist models summer school,
    pages 21â€“28\. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Hyunjae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm: A style-based recalibration
    module for convolutional neural networks. pages 1854â€“1862, 10 2019.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] JaeÂ Won Lee, Jonghun Park, Jangmin O, Jongwoo Lee, and Euyseok Hong.
    A multiagent approach to q-learning for daily stock trading. Trans. Sys. Man Cyber.
    Part A, 37(6):864â€“877, November 2007.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] JoelÂ Z. Leibo, VinÃ­ciusÂ Flores Zambaldi, Marc Lanctot, Janusz Marecki,
    and Thore Graepel. Multi-agent reinforcement learning in sequential social dilemmas.
    CoRR, abs/1702.03037, 2017.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Sergey Levine and Vladlen Koltun. Learning complex neural network policies
    with trajectory optimization. In Proceedings of the 31st International Conference
    on Machine Learning, pages 829â€“837, 2014.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] BoÂ Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance
    visual tracking with siamese region proposal network. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 8971â€“8980, 2018.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Buyu Li, Wanli Ouyang, LuÂ Sheng, Xingyu Zeng, and Xiaogang Wang. GS3D:
    an efficient 3d object detection framework for autonomous driving. In IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
    16-20, 2019, pages 1019â€“1028. Computer Vision Foundation / IEEE, 2019.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Chao Li, Qiaoyong Zhong, DiÂ Xie, and Shiliang Pu. Co-occurrence feature
    learning from skeleton data for action recognition and detection with hierarchical
    aggregation. arXiv preprint arXiv:1804.06055, 2018.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Duo Li and Qifeng Chen. Deep reinforced attention learning for quality-aware
    visual recognition. In European Conference on Computer Vision, pages 493â€“509,
    2020.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features.
    arXiv preprint arXiv:1503.08663, 2015.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder
    for paragraphs and documents. CoRR, abs/1506.01057, 2015.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] K.Â Li, M.Â Rath, and J.Â W. Burdick. Inverse reinforcement learning via
    function approximation for clinical motion analysis. In 2018 IEEE International
    Conference on Robotics and Automation (ICRA), pages 610â€“617, 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Lihong Li, Wei Chu, John Langford, and RobertÂ E Schapire. A contextual-bandit
    approach to personalized news article recommendation. In Proceedings of the 19th
    international conference on World wide web, pages 661â€“670, 2010.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep visual tracking:
    Review and experimental comparison. Pattern Recognition, 76:323â€“338, 2018.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Yingbo Li and Bernard Merialdo. Multi-video summarization based on video-mmr.
    In 11th International Workshop on Image Analysis for Multimedia Interactive Services
    WIAMIS 10, pages 1â€“4\. IEEE, 2010.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Yuanwei Li, Amir Alansary, JuanÂ J Cerrolaza, Bishesh Khanal, Matthew
    Sinclair, Jacqueline Matthew, Chandni Gupta, Caroline Knight, Bernhard Kainz,
    and Daniel Rueckert. Fast multiple landmark localisation using a patch-based iterative
    network. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 563â€“571\. Springer, 2018.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding color information
    for visual tracking: Algorithms and benchmark. IEEE Transactions on Image Processing,
    24(12):5630â€“5644, 2015.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Rui Liao, Shun Miao, Pierre deÂ Tournemire, Sasa Grbic, Ali Kamen, Tommaso
    Mansi, and Dorin Comaniciu. An artificial agent for robust image registration.
    In Thirty-First AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, BoÂ Jin, Xiaoyun Zhang,
    Yanfeng Wang, and YaÂ Zhang. Iteratively-refined interactive 3d medical image segmentation
    with multi-agent reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 9394â€“9402, 2020.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] TimothyÂ P. Lillicrap, JonathanÂ J. Hunt, AlexandÂ er Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv e-prints, page arXiv:1509.02971, September
    2015.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] TimothyÂ P Lillicrap, JonathanÂ J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r.
    Focal loss for dense object detection. In Proceedings of the IEEE international
    conference on computer vision, pages 2980â€“2988, 2017.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Tony Lindeberg. Scale-space theory in computer vision, volume 256. Springer
    Science & Business Media, 2013.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Geert Litjens, Robert Toth, Wendy vanÂ de Ven, Caroline Hoeks, Sjoerd
    Kerkstra, Bram van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang
    Zhang, etÂ al. Evaluation of prostate segmentation algorithms for mri: the promise12
    challenge. Medical image analysis, 18(2):359â€“373, 2014.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Daochang Liu and Tingting Jiang. Deep reinforcement learning for surgical
    gesture segmentation and classification. In International conference on medical
    image computing and computer-assisted intervention, pages 247â€“255\. Springer,
    2018.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Hao Liu, Richard Socher, and Caiming Xiong. Taming maml: Efficient unbiased
    meta-reinforcement learning. In International Conference on Machine Learning,
    pages 4061â€“4071, 2019.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua
    Shen. Weighing counts: Sequential crowd counting by reinforcement learning. 2020.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Lijie Liu, Chufan Wu, Jiwen Lu, Lingxi Xie, Jie Zhou, and QiÂ Tian. Reinforced
    axial refinement network for monocular 3d object detection. In European Conference
    on Computer Vision ECCV, pages 540â€“556, 2020.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and Liang Lin. Crowd
    counting using deep recurrent spatial-aware network. arXiv preprint arXiv:1807.00601,
    2018.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Tianrui Liu, Qingjie Meng, Athanasios Vlontzos, Jeremy Tan, Daniel Rueckert,
    and Bernhard Kainz. Ultrasound video summarization using deep reinforcement learning.
    arXiv preprint arXiv:2005.09531, 2020.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5099â€“5108, 2019.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face
    attributes in the wild. In Proceedings of the IEEE international conference on
    computer vision, pages 3730â€“3738, 2015.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431â€“3440, 2015.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Marco Lorenzi, Nicholas Ayache, GiovanniÂ B Frisoni, Xavier Pennec, Alzheimerâ€™s
    Disease NeuroimagingÂ Initiative (ADNI, etÂ al. Lcc-demons: a robust and accurate
    symmetric diffeomorphic registration algorithm. NeuroImage, 81:470â€“483, 2013.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Tayebeh Lotfi, Lisa Tang, Shawn Andrews, and Ghassan Hamarneh. Improving
    probabilistic image registration via reinforcement learning and uncertainty evaluation.
    In International Workshop on Machine Learning in Medical Imaging, pages 187â€“194\.
    Springer, 2013.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] DavidÂ G Lowe. Distinctive image features from scale-invariant keypoints.
    International journal of computer vision, 60(2):91â€“110, 2004.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, and Yizhou
    Wang. End-to-end active object tracking via reinforcement learning. arXiv preprint
    arXiv:1705.10561, 2017.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Thang Luong, Ilya Sutskever, QuocÂ V. Le, Oriol Vinyals, and Wojciech
    Zaremba. Addressing the rare word problem in neural machine translation. CoRR,
    abs/1410.8206, 2014.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Khoa Luu, Chenchen Zhu, Chandrasekhar Bhagavatula, TÂ HoangÂ Ngan Le, and
    Marios Savvides. A deep learning approach to joint face detection and segmentation.
    In Advances in Face Detection and Facial Image Analysis, pages 1â€“12\. Springer,
    2016.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang. Hierarchical
    convolutional features for visual tracking. In Proceedings of the IEEE international
    conference on computer vision, pages 3074â€“3082, 2015.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Kai Ma, Jiangping Wang, Vivek Singh, Birgi Tamersoy, Yao-Jen Chang, Andreas
    Wimmer, and Terrence Chen. Multimodal image registration with deep context reinforcement
    learning. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 240â€“248\. Springer, 2017.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video
    summarization with adversarial lstm networks. In Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition, pages 202â€“211, 2017.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] Gabriel Maicas, Gustavo Carneiro, AndrewÂ P Bradley, JacintoÂ C Nascimento,
    and Ian Reid. Deep reinforcement learning for active breast lesion detection from
    dce-mri. In International conference on medical image computing and computer-assisted
    intervention, pages 665â€“673\. Springer, 2017.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Junhua Mao, Wei Xu, YiÂ Yang, Jiang Wang, and AlanÂ L. Yuille. Deep captioning
    with multimodal recurrent neural networks (m-rnn). CoRR, abs/1412.6632, 2014.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Nicolas MÃ¤rki, Federico Perazzi, Oliver Wang, and Alexander Sorkine-Hornung.
    Bilateral space video segmentation. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 743â€“751, 2016.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] T.Â Martinez-Marin and T.Â Duckett. Fast reinforcement learning for vision-guided
    mobile robots. In Proceedings of the 2005 IEEE International Conference on Robotics
    and Automation, pages 4170â€“4175, 2005.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Jan Matas, Stephen James, and AndrewÂ J Davison. Sim-to-real reinforcement
    learning for deformable object manipulation. arXiv preprint arXiv:1806.07851,
    2018.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] Stefan Mathe, Aleksis Pirinen, and Cristian Sminchisescu. Reinforcement
    learning for visual object detection. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 2894â€“2902, 2016.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] GeorgeÂ K Matsopoulos, NicolaosÂ A Mouravliansky, KonstantinosÂ K Delibasis,
    and KonstantinaÂ S Nikita. Automatic retinal image registration scheme using global
    optimization techniques. IEEE Transactions on Information Technology in Biomedicine,
    3(1):47â€“60, 1999.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Darryl McClymont, Andrew Mehnert, Adnan Trakic, Dominic Kennedy, and
    Stuart Crozier. Fully automatic lesion segmentation in breast mri using mean-shift
    and graph-cuts on a region adjacency graph. Journal of Magnetic Resonance Imaging,
    39(4):795â€“804, 2014.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] BjoernÂ H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer,
    Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom,
    Roland Wiest, etÂ al. The multimodal brain tumor image segmentation benchmark (brats).
    IEEE transactions on medical imaging, 34(10):1993â€“2024, 2014.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Shun Miao, Rui Liao, Marcus Pfister, LiÂ Zhang, and Vincent Ordy. System
    and method for 3-d/3-d registration between non-contrast-enhanced cbct and contrast-enhanced
    ct for abdominal aortic aneurysm stenting. In International Conference on Medical
    Image Computing and Computer-Assisted Intervention, pages 380â€“387\. Springer,
    2013.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Shun Miao, ZÂ Jane Wang, and Rui Liao. A cnn regression approach for real-time
    2d/3d registration. IEEE transactions on medical imaging, 35(5):1352â€“1363, 2016.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Tomas Mikolov, Stefan Kombrink, LukÃ¡s Burget, Jan CernockÃ½, and Sanjeev
    Khudanpur. Extensions of recurrent neural network language model. In ICASSP, pages
    5528â€“5531, 2011.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Anton Milan, Laura Leal-TaixÃ©, Ian Reid, Stefan Roth, and Konrad Schindler.
    Mot16: A benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831,
    2016.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Anton Milan, Laura Leal-TaixÃ©, Konrad Schindler, and Ian Reid. Joint
    tracking and segmentation of multiple targets. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 5397â€“5406, 2015.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] Anton Milan, SÂ Hamid Rezatofighi, Anthony Dick, Ian Reid, and Konrad
    Schindler. Online multi-target tracking using recurrent neural networks. In Thirty-First
    AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Shervin Minaee, AmirAli Abdolrashidi, Hang Su, Mohammed Bennamoun, and
    David Zhang. Biometric recognition using deep learning: A survey. CoRR, abs/1912.00271,
    2019.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Volodymyr Mnih, AdriaÂ Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In International conference on machine learning,
    pages 1928â€“1937, 2016.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] Volodymyr Mnih, AdriaÂ Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In Proceedings of The 33rd International Conference
    on Machine Learning, pages 1928â€“1937, 20â€“22 Jun 2016.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, AndreiÂ A Rusu, Joel
    Veness, MarcÂ G Bellemare, Alex Graves, Martin Riedmiller, AndreasÂ K Fidjeland,
    Georg Ostrovski, etÂ al. Human-level control through deep reinforcement learning.
    Nature, 518(7540):529â€“533, 2015.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] I.Â Mordatch, N.Â Mishra, C.Â Eppner, and P.Â Abbeel. Combining model-based
    policy search with online model learning for control of physical humanoids. In
    2016 IEEE International Conference on Robotics and Automation (ICRA), pages 242â€“248,
    2016.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] J.Â Morimoto, G.Â Zeglin, and C.Â G. Atkeson. Minimax differential dynamic
    programming: application to a biped walking robot. In Proceedings 2003 IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),
    volumeÂ 2, pages 1927â€“1932, 2003.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Jun Morimoto and ChristopherÂ G. Atkeson. Nonparametric representation
    of an approximated poincarÃ© map for learning biped locomotion. In Autonomous Robots,
    page 131â€“144, 2009.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] A.Â Mousavian, D.Â Anguelov, J.Â Flynn, and J.Â KoÅ¡eckÃ¡. 3d bounding box
    estimation using deep learning and geometry. In 2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 5632â€“5640, 2017.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator
    for uav tracking. In European conference on computer vision, pages 445â€“461. Springer,
    2016.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Don Murray and Anup Basu. Motion tracking with an active camera. IEEE
    transactions on pattern analysis and machine intelligence, 16(5):449â€“459, 1994.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Anusha Nagabandi, Ignasi Clavera, Simin Liu, RonaldÂ S Fearing, Pieter
    Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world
    environments through meta-reinforcement learning. arXiv preprint arXiv:1803.11347,
    2018.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] A.Â Nair, B.Â McGrew, M.Â Andrychowicz, W.Â Zaremba, and P.Â Abbeel. Overcoming
    exploration in reinforcement learning with demonstrations. In 2018 IEEE International
    Conference on Robotics and Automation (ICRA), pages 6292â€“6299, 2018.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural
    networks for visual tracking. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 4293â€“4302, 2016.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] AliÂ Bou Nassif, Ismail Shahin, Imtinan Attili, Mohammad Azzeh, and Khaled
    Shaalan. Speech recognition using deep neural networks: A systematic review. IEEE
    Access, 7:19143â€“19165, 2019.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, JanÂ C Peeken,
    StephanieÂ E Combs, and BjoernÂ H Menze. Deep reinforcement learning for organ localization
    in ct. arXiv preprint arXiv:2005.04974, 2020.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, BoÂ Wu, and
    AndrewÂ Y Ng. Reading digits in natural images with unsupervised feature learning.
    2011.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] Dominik Neumann, SaÅ¡a GrbiÄ‡, Matthias John, Nassir Navab, Joachim Hornegger,
    and Razvan Ionasec. Probabilistic sparse matching for robust 3d/3d fusion in minimally
    invasive surgery. IEEE transactions on medical imaging, 34(1):49â€“60, 2014.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] AndrewÂ Y. Ng and StuartÂ J. Russell. Algorithms for inverse reinforcement
    learning. In Proceedings of the Seventeenth International Conference on Machine
    Learning, ICML â€™00, page 663â€“670, San Francisco, CA, USA, 2000. Morgan Kaufmann
    Publishers Inc.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] AndrewÂ Y Ng, StuartÂ J Russell, etÂ al. Algorithms for inverse reinforcement
    learning. In Icml, volumeÂ 1, pageÂ 2, 2000.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] TrungÂ Thanh Nguyen, Zhuoru Li, Tomi Silander, and Tze-Yun Leong. Online
    feature selection for model-based reinforcement learning. In Proceedings of the
    30th International Conference on International Conference on Machine Learning
    - Volume 28, page Iâ€“498â€“Iâ€“506, 2013.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Chi NhanÂ Duong, Kha GiaÂ Quach, Khoa Luu, Ngan Le, and Marios Savvides.
    Temporal non-volume preserving approach to facial age-progression and age-invariant
    face recognition. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 3735â€“3743, 2017.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] C.Â Niedzwiedz, I.Â Elhanany, Zhenzhen Liu, and S.Â Livingston. A consolidated
    actor-critic model with function approximation for high-dimensional pomdps. In
    AAAI 2008Workshop for Advancement in POMDP Solvers, 2008.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Yishuang Ning, Sheng He, Zhiyong Wu, Chunxiao Xing, and Liang-Jie Zhang.
    A review of deep learning based speech synthesis. Applied Sciences, 9(19), 2019.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Kenji Okuma, Ali Taleghani, Nando DeÂ Freitas, JamesÂ J Little, and DavidÂ G
    Lowe. A boosted particle filter: Multitarget detection and tracking. In European
    conference on computer vision, pages 28â€“39. Springer, 2004.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] JosÃ©Â Ignacio Orlando, Huazhu Fu, JoÃ£oÂ Barbosa Breda, Karel van Keer,
    DeeptiÂ R Bathula, AndrÃ©s Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim,
    JoonHo Lee, etÂ al. Refuge challenge: A unified framework for evaluating automated
    methods for glaucoma assessment from fundus photographs. Medical image analysis,
    59:101570, 2020.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] T.Â Osa, J.Â Pajarinen, G.Â Neumann, J.Â A. Bagnell, P.Â Abbeel, and J.Â Peters.
    2018.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Rameswar Panda, Abir Das, Ziyan Wu, Jan Ernst, and AmitÂ K Roy-Chowdhury.
    Weakly supervised summarization of web videos. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 3657â€“3666, 2017.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Anestis Papazoglou and Vittorio Ferrari. Fast object segmentation in
    unconstrained video. In Proceedings of the IEEE international conference on computer
    vision, pages 1777â€“1784, 2013.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] I.Â C. Paschalidis, K.Â Li, and R.Â Moazzez Estanjini. An actor-critic method
    using least squares temporal difference learning. In Proceedings of the 48h IEEE
    Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control
    Conference, pages 2564â€“2569, 2009.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Massimiliano Patacchiola and Angelo Cangelosi. Head pose estimation in
    the wild using convolutional neural networks and adaptive gradient methods. Pattern
    Recognition, 71:132â€“143, 2017.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Hanchuan Peng, Zongcai Ruan, Fuhui Long, JulieÂ H Simpson, and EugeneÂ W
    Myers. V3d enables real-time 3d visualization and quantitative analysis of large-scale
    biological image data sets. Nature biotechnology, 28(4):348â€“353, 2010.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and
    Alexander Sorkine-Hornung. Learning video object segmentation from static images.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2663â€“2672, 2017.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc VanÂ Gool, Markus
    Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology
    for video object segmentation. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 724â€“732, 2016.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills
    with policy gradients. Neural Networks, 21(4):682 â€“ 697, 2008.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] Aleksis Pirinen and Cristian Sminchisescu. Deep reinforcement learning
    of region proposal networks for object detection. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 6945â€“6954, 2018.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Hamed Pirsiavash, Deva Ramanan, and CharlessÂ C Fowlkes. Globally-optimal
    greedy algorithms for tracking a variable number of objects. In CVPR 2011, pages
    1201â€“1208\. IEEE, 2011.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] Aske Plaat, Walter Kosters, and Mike Preuss. Deep model-based reinforcement
    learning for high-dimensional problems, a survey, 2020.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid.
    Category-specific video summarization. In European conference on computer vision,
    pages 540â€“555. Springer, 2014.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] Reza Pourreza-Shahri and Nasser Kehtarnavaz. Exposure bracketing via
    automatic exposure selection. In 2015 IEEE International Conference on Image Processing
    (ICIP), pages 320â€“323\. IEEE, 2015.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid,
    and Vittorio Ferrari. Learning object class detectors from weakly annotated video.
    In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3282â€“3289\.
    IEEE, 2012.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao, Qingming Huang, Jongwoo
    Lim, and Ming-Hsuan Yang. Hedged deep tracking. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 4303â€“4311, 2016.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] Zengyi Qin, Jinglu Wang, and Yan Lu. Monogrnet: A geometric reasoning
    network for monocular 3d object localization. Proceedings of the AAAI Conference
    on Artificial Intelligence, 33(01):8851â€“8858, Jul. 2019.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.
    Efficient off-policy meta-reinforcement learning via probabilistic context variables.
    In International conference on machine learning, pages 5331â€“5340, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] VidhiwarÂ Singh Rathour, Kashu Yamakazi, and TÂ Le. Roughness index and
    roughness distance for benchmarking medical segmentation. arXiv preprint arXiv:2103.12350,
    2021.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only
    look once: Unified, real-time object detection. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 779â€“788, 2016.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] Liangliang Ren, Jiwen Lu, Zifeng Wang, QiÂ Tian, and Jie Zhou. Collaborative
    deep reinforcement learning for multi-object tracking. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 586â€“602, 2018.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] Liangliang Ren, Xin Yuan, Jiwen Lu, Ming Yang, and Jie Zhou. Deep reinforcement
    learning with iterative shift for visual tracking. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 684â€“700, 2018.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. In Advances
    in neural information processing systems, pages 91â€“99, 2015.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] MdÂ Reza, Jana Kosecka, etÂ al. Reinforcement learning for semantic segmentation
    in indoor scenes. arXiv preprint arXiv:1606.01178, 2016.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Alexander Richard and Juergen Gall. Temporal action detection using a
    statistical language model. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 3131â€“3140, 2016.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] Mrigank Rochan, Linwei Ye, and Yang Wang. Video summarization using fully
    convolutional sequence networks. In Proceedings of the European Conference on
    Computer Vision (ECCV), pages 347â€“363, 2018.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional
    networks for biomedical image segmentation. In International Conference on Medical
    image computing and computer-assisted intervention, pages 234â€“241\. Springer,
    2015.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] German Ros, Vladfen Koltun, Felipe Codevilla, and Antonio Lopez. The
    carla autonomous driving challenge, 2019.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. â€ grabcutâ€ interactive
    foreground extraction using iterated graph cuts. ACM transactions on graphics
    (TOG), 23(3):309â€“314, 2004.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] David Rotman. Mit technology review. Retrieved from Meet the Man with
    a Cheap and Easy Plan to Stop Global Warming: http://www. technologyreview. com/featuredstor
    y/511016/a-cheap-and-easy-plan-to-stop-globalwarming, 2013.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] J-M Rouet, J-J Jacq, and Christian Roux. Genetic algorithms for a robust
    3-d mr-ct registration. IEEE transactions on information technology in biomedicine,
    4(2):126â€“136, 2000.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] DavidÂ E Rumelhart. The architecture of mind: A connectionist approach.
    Mind readings, pages 207â€“238, 1998.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] T.Â P. Runarsson and S.Â M. Lucas. Imitating play from game trajectories:
    Temporal difference learning versus preference learning. In 2012 IEEE Conference
    on Computational Intelligence and Games (CIG), pages 79â€“82, 2012.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, etÂ al.
    Imagenet large scale visual recognition challenge. International Journal of Computer
    Vision, 115(3):211â€“252, 2015.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Tracking the untrackable:
    Learning to track multiple cues with long-term dependencies. In Proceedings of
    the IEEE International Conference on Computer Vision, pages 300â€“311, 2017.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] SteindÃ³r SÃ¦mundsson, Katja Hofmann, and MarcÂ Peter Deisenroth. Meta reinforcement
    learning with latent variable gaussian processes. arXiv preprint arXiv:1803.07551,
    2018.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] Farhang Sahba. Deep reinforcement learning for object segmentation in
    video sequences. In 2016 International Conference on Computational Science and
    Computational Intelligence (CSCI), pages 857â€“860\. IEEE, 2016.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] Farhang Sahba, HamidÂ R Tizhoosh, and MagdyÂ MA Salama. A reinforcement
    learning framework for medical image segmentation. In The 2006 IEEE International
    Joint Conference on Neural Network Proceedings, pages 511â€“517\. IEEE, 2006.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] Farhang Sahba, HamidÂ R Tizhoosh, and MagdyÂ MMA Salama. Application of
    opposition-based reinforcement learning in image segmentation. In 2007 IEEE Symposium
    on Computational Intelligence in Image and Signal Processing, pages 246â€“251\.
    IEEE, 2007.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
    Moritz. Trust region policy optimization. In International conference on machine
    learning, pages 1889â€“1897, 2015.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] John Schulman, Sergey Levine, Philipp Moritz, MichaelÂ I. Jordan, and
    Pieter Abbeel. Trust Region Policy Optimization. arXiv e-prints, February 2015.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal Policy Optimization Algorithms. arXiv e-prints, July 2017.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning.
    Neural Networks, 16(1):5â€“9, 2003.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] Shahin Sefati, NoahÂ J Cowan, and RenÃ© Vidal. Learning shared, discriminative
    dictionaries for surgical gesture segmentation and classification. In MICCAI Workshop:
    M2CAI, volumeÂ 4, 2015.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] MohammadÂ Javad Shafiee, Brendan Chywl, Francis Li, and Alexander Wong.
    Fast yolo: A fast you only look once system for real-time embedded object detection
    in video. arXiv preprint arXiv:1709.05943, 2017.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: A
    large scale dataset for 3d human activity analysis. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 1010â€“1019, 2016.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] M.Â R. Shaker, Shigang Yue, and T.Â Duckett. Vision-based reinforcement
    learning using approximate policy iteration. In 2009 International Conference
    on Advanced Robotics, pages 1â€“6, 2009.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent,
    reinforcement learning for autonomous driving. CoRR, abs/1610.03295, 2016.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] Jie Shen, Stefanos Zafeiriou, GrigorisÂ G Chrysos, Jean Kossaifi, Georgios
    Tzimiropoulos, and Maja Pantic. The first facial landmark tracking in-the-wild
    challenge: Benchmark and results. In Proceedings of the IEEE international conference
    on computer vision workshops, pages 50â€“58, 2015.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] Y.Â Shi, L.Â Cui, Z.Â Qi, F.Â Meng, and Z.Â Chen. Automatic road crack detection
    using random structured forests. IEEE Transactions on Intelligent Transportation
    Systems, 17(12):3434â€“3445, 2016.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor
    segmentation and support inference from rgbd images. In European conference on
    computer vision, pages 746â€“760. Springer, 2012.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
    and Martin Riedmiller. Deterministic policy gradient algorithms. 2014.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks
    for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] VishwanathÂ A Sindagi and VishalÂ M Patel. Multi-level bottom-top and top-bottom
    feature fusion for crowd counting. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1002â€“1012, 2019.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] SatyaÂ P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan,
    and BalÃ¡zs GulyÃ¡s. 3d deep learning on medical images: A review, 2020.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] Gwangmo Song, Heesoo Myeong, and Kyoung MuÂ Lee. Seednet: Automatic seed
    generation with deep reinforcement learning for robust interactive segmentation.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 1760â€“1768, 2018.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum:
    Summarizing web videos using titles. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 5179â€“5187, 2015.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, RynsonÂ WH Lau, and Ming-Hsuan
    Yang. Crest: Convolutional residual learning for visual tracking. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 2555â€“2564, 2017.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] Concetto Spampinato, Simone Palazzo, and Daniela Giordano. Gamifying
    video object segmentation. IEEE transactions on pattern analysis and machine intelligence,
    39(10):1942â€“1958, 2016.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] BradlyÂ C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation
    learning. CoRR, abs/1703.01703, 2017.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary
    mean-field games. page 251â€“259\. International Foundation for Autonomous Agents
    and Multiagent Systems, 2019.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] Shanhui Sun, Jing Hu, Mingqing Yao, Jinrong Hu, Xiaodong Yang, QiÂ Song,
    and XiÂ Wu. Robust multimodal image registration using deep recurrent reinforcement
    learning. In Asian Conference on Computer Vision, pages 511â€“526. Springer, 2018.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] Kalaivani Sundararajan and DamonÂ L. Woodard. Deep learning for biometrics:
    A survey. 51(3), 2018.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] RichardÂ S Sutton and AndrewÂ G Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] RichardÂ S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
    Policy gradient methods for reinforcement learning with function approximation.
    In Proceedings of the 12th International Conference on Neural Information Processing
    Systems, NIPSâ€™99, page 1057â€“1063, 1999.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] RichardÂ S Sutton, DavidÂ A. McAllester, SatinderÂ P. Singh, and Yishay
    Mansour. Policy gradient methods for reinforcement learning with function approximation.
    In Advances in Neural Information Processing Systems 12, pages 1057â€“1063\. 2000.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and AlexanderÂ A Alemi.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In Thirty-first AAAI conference on artificial intelligence, 2017.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1â€“9, 2015.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks
    for object detection. In Advances in neural information processing systems, pages
    2553â€“2561, 2013.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] Yansong Tang, YiÂ Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep progressive
    reinforcement learning for skeleton-based action recognition. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 5323â€“5332,
    2018.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] Ran Tao, Efstratios Gavves, and ArnoldÂ WM Smeulders. Siamese instance
    search for tracking. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1420â€“1429, 2016.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] Philippe ThÃ©venaz and Michael Unser. Optimization of mutual information
    for multiresolution image registration. IEEE transactions on image processing,
    9(12):2083â€“2099, 2000.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] Zhiqiang Tian, Xiangyu Si, Yaoyue Zheng, Zhang Chen, and Xiaojian Li.
    Multi-step medical image segmentation based on reinforcement learning. JOURNAL
    OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING, 2020.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free
    reinforcement learning for urban driving using implicit affordances. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7153â€“7162,
    2020.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation
    via deep neural networks. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1653â€“1660, 2014.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] Yi-Hsuan Tsai, Ming-Hsuan Yang, and MichaelÂ J Black. Video segmentation
    via object flow. In Proceedings of the IEEE conference on computer vision and
    pattern recognition, pages 3899â€“3908, 2016.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] Y.Â Tsurumine, Y.Â Cui, K.Â Yamazaki, and T.Â Matsubara. Generative adversarial
    imitation learning with deep p-network for robotic cloth manipulation. In 2019
    IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids), pages 274â€“280,
    2019.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] Yoshihisa Tsurumine, Yunduan Cui, Eiji Uchibe, and Takamitsu Matsubara.
    Deep reinforcement learning with smooth policy update: Application to robotic
    cloth manipulation. Robotics and Autonomous Systems, 112:72 â€“ 83, 2019.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] JasperÂ RR Uijlings, KoenÂ EA Van DeÂ Sande, Theo Gevers, and ArnoldÂ WM
    Smeulders. Selective search for object recognition. International journal of computer
    vision, 104(2):154â€“171, 2013.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] Burak Uzkent, Christopher Yeh, and Stefano Ermon. Efficient object detection
    in large images using deep reinforcement learning. In The IEEE Winter Conference
    on Applications of Computer Vision, pages 1824â€“1833, 2020.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, and PhilipÂ HS
    Torr. End-to-end representation learning for correlation filter based tracking.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 2805â€“2813, 2017.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] Peter van Beek. Improved image selection for stack-based hdr imaging.
    arXiv preprint arXiv:1806.07420, 2018.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning
    with Double Q-learning. arXiv e-prints, page arXiv:1509.06461, September 2015.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] Hado VanÂ Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning
    with double q-learning. In Thirtieth AAAI conference on artificial intelligence,
    2016.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] Leo VanÂ Hove. Optimal denominations for coins and bank notes: in defense
    of the principle of least effort. Journal of Money, Credit and Banking, pages
    1015â€“1021, 2001.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] Giuseppe Vecchio, Simone Palazzo, Daniela Giordano, Francesco Rundo,
    and Concetto Spampinato. Mask-rl: Multiagent video object segmentation framework
    through reinforcement learning. IEEE Transactions on Neural Networks and Learning
    Systems, 2020.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] Kashu Yamakazi AkihiroÂ Sugimoto Viet-Khoa Vo-Ho, Ngan T.H.Â Le and Triet
    Tran. Agent-environment network for temporal action proposal generation. In International
    Conference on Acoustics, Speech and Signal Processing. 2021.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar,
    and Katerina Fragkiadaki. Sfm-net: Learning of structure and motion from video.
    arXiv preprint arXiv:1704.07804, 2017.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max
    Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard
    Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk
    Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets,
    James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang,
    Tobias Pfaff, Toby Pohlen, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver
    Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis,
    and David Silver. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II.
    [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/),
    2019.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] Athanasios Vlontzos, Amir Alansary, Konstantinos Kamnitsas, Daniel Rueckert,
    and Bernhard Kainz. Multiple landmark detection using multi-agent reinforcement
    learning. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 262â€“270\. Springer, 2019.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] Guotai Wang, MariaÂ A Zuluaga, Wenqi Li, Rosalind Pratt, PremalÂ A Patel,
    Michael Aertsen, Tom Doel, AnnaÂ L David, Jan Deprest, SÃ©bastien Ourselin, etÂ al.
    Deepigeos: a deep interactive geodesic framework for medical image segmentation.
    IEEE transactions on pattern analysis and machine intelligence, 41(7):1559â€“1572,
    2018.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou,
    Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5265â€“5274, 2018.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] JaneÂ X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, JoelÂ Z.
    Leibo, RÃ©mi Munos, Charles Blundell, Dharshan Kumaran, and Matthew Botvinick.
    Learning to reinforcement learn. CoRR, abs/1611.05763, 2016.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] Lijun Wang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Deep networks
    for saliency detection via local estimation and global search. In Computer Vision
    and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3183â€“3192\. IEEE,
    2015.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] Mei Wang and Weihong Deng. Mitigating bias in face recognition using
    skewness-aware reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 9322â€“9331, 2020.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial
    faces in the wild: Reducing racial bias by information maximization adaptation
    network. In Proceedings of the IEEE International Conference on Computer Vision,
    pages 692â€“702, 2019.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] Naiyan Wang and Dit-Yan Yeung. Learning a deep compact image representation
    for visual tracking. In Advances in neural information processing systems, pages
    809â€“817, 2013.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric
    Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking
    model-based reinforcement learning. CoRR, abs/1907.02057, 2019.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] Yan Wang, Lei Zhang, Lituan Wang, and Zizhou Wang. Multitask learning
    for object localization with deep reinforcement learning. IEEE Transactions on
    Cognitive and Developmental Systems, 11(4):573â€“580, 2018.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, and Maja
    Pantic. Dynamic face video segmentation via reinforcement learning. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6959â€“6969,
    2020.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] Zhou Wang, AlanÂ C Bovik, HamidÂ R Sheikh, and EeroÂ P Simoncelli. Image
    quality assessment: from error visibility to structural similarity. IEEE transactions
    on image processing, 13(4):600â€“612, 2004.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] Zhouxia Wang, Jiawei Zhang, Mude Lin, Jiong Wang, Ping Luo, and Jimmy
    Ren. Learning a reinforced agent for flexible exposure bracketing selection. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1820â€“1828, 2020.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado VanÂ Hasselt, Marc Lanctot,
    and Nando DeÂ Freitas. Dueling network architectures for deep reinforcement learning.
    arXiv preprint arXiv:1511.06581, 2015.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] WayneÂ A Wickelgren. The long and the short of memory. Psychological Bulletin,
    80(6):425, 1973.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] RonaldÂ J Williams. Simple statistical gradient-following algorithms for
    connectionist reinforcement learning. Machine learning, 8(3-4):229â€“256, 1992.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] Aaron Wilson, Alan Fern, and Prasad Tadepalli. Using trajectory data
    to improve bayesian optimization for reinforcement learning. Journal of Machine
    Learning Research, 15(8):253â€“282, 2014.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] C.Â Wirth and J.Â FÃ¼rnkranz. On learning from game annotations. IEEE Transactions
    on Computational Intelligence and AI in Games, 7(3):304â€“316, 2015.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] Paul Wohlhart and Vincent Lepetit. Learning descriptors for object recognition
    and 3d pose estimation. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 3109â€“3118, 2015.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and InÂ So Kweon. Cbam: Convolutional
    block attention module. In European Conference on Computer Vision, pages 3â€“19,
    2018.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] YiÂ Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2411â€“2418, 2013.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] YiÂ Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 37(9):1834â€“1848, 2015.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] LuÂ Xia, Chia-Chih Chen, and JakeÂ K Aggarwal. View invariant human action
    recognition using histograms of 3d joints. In 2012 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition Workshops, pages 20â€“27\. IEEE, 2012.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] Sitao Xiang and Hao Li. On the effects of batch and weight normalization
    in generative adversarial networks. arXiv preprint arXiv:1704.03971, 2017.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] YuÂ Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track: Online
    multi-object tracking by decision making. In Proceedings of the IEEE international
    conference on computer vision, pages 4705â€“4713, 2015.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] Fanyi Xiao and Yong JaeÂ Lee. Track and segment: An iterative unsupervised
    approach for video object proposals. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 933â€“942, 2016.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] Hang Xiao and Hanchuan Peng. App2: automatic tracing of 3d neuron morphology
    based on hierarchical pruning of a gray-weighted image distance-tree. Bioinformatics,
    29(11):1448â€“1454, 2013.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and QuocÂ V Le. Self-training
    with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 10687â€“10698, 2020.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, and Chunhua
    Shen. From open set to closed set: Counting objects by spatial divide-and-conquer.
    In Proceedings of the IEEE International Conference on Computer Vision, pages
    8362â€“8371, 2019.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] Hailiang Xu and Feng Su. Robust seed localization and growing with deep
    convolutional features for scene text detection. In Proceedings of the 5th ACM
    on International Conference on Multimedia Retrieval, pages 387â€“394\. ACM, 2015.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and ThomasÂ S Huang. Deep
    interactive object selection. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 373â€“381, 2016.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, and Josef Kittler. Learning
    adaptive discriminative correlation filters via temporal consistency preserving
    spatial feature selection for robust visual object tracking. IEEE Transactions
    on Image Processing, 28(11):5596â€“5609, 2019.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] Xuanang Xu, Fugen Zhou, BoÂ Liu, Dongshan Fu, and Xiangzhi Bai. Efficient
    multiple organ localization in ct image using 3d region proposal network. IEEE
    transactions on medical imaging, 38(8):1885â€“1898, 2019.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, and Chun-Yi Lee. Dynamic video
    segmentation network. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 6556â€“6565, 2018.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[411] Kashu Yamazaki, VidhiwarÂ Singh Rathour, and TÂ Le. Invertible residual
    network with regularization for effective medical image segmentation. arXiv preprint
    arXiv:2103.09042, 2021.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[412] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei
    Wen, and Errui Ding. Perspective-guided convolution networks for crowd counting.
    In Proceedings of the IEEE International Conference on Computer Vision, pages
    952â€“961, 2019.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[413] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory
    Zelinsky, Dimitris Samaras, and Minh Hoai. Predicting goal-directed human attention
    using inverse reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), June 2020.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[414] Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare:
    a survey. arXiv preprint arXiv:1908.08796, 2019.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[415] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman,
    Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task
    and meta reinforcement learning. In Conference on Robot Learning, pages 1094â€“1100,
    2020.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[416] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, and Jin YoungÂ Choi.
    Action-decision networks for visual tracking with deep reinforcement learning.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2711â€“2720, 2017.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[417] Daochen Zha, Kwei-Herng Lai, Kaixiong Zhou, and Xia Hu. Experience replay
    optimization. arXiv preprint arXiv:1906.08387, 2019.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[418] DaÂ Zhang, Hamid Maei, Xin Wang, and Yuan-Fang Wang. Deep reinforcement
    learning for visual object tracking in videos. arXiv preprint arXiv:1701.08936,
    2017.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[419] Dingwen Zhang, LeÂ Yang, Deyu Meng, Dong Xu, and Junwei Han. Spftn: A
    self-paced fine-tuning network for segmenting objects in weakly labelled videos.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4429â€“4437, 2017.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[420] Jing Zhang, Wanqing Li, PhilipÂ O Ogunbona, Pichao Wang, and Chang Tang.
    Rgb-d-based action recognition datasets: A survey. Pattern Recognition, 60:86â€“105,
    2016.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[421] KeÂ Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman. Video summarization
    with long short-term memory. In European conference on computer vision, pages
    766â€“782. Springer, 2016.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[422] Pengyu Zhang, Dong Wang, and Huchuan Lu. Multi-modal visual tracking:
    Review and experimental comparison, 2020.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[423] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and YiÂ Ma. Single-image
    crowd counting via multi-column convolutional neural network. In Proceedings of
    the IEEE conference on computer vision and pattern recognition, pages 589â€“597,
    2016.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[424] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya
    Jia. Icnet for real-time semantic segmentation on high-resolution images. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 405â€“420, 2018.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[425] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya
    Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2881â€“2890, 2017.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[426] Zhong-Qiu Zhao, Shou-Tao Xu, Dian Liu, Wei-Dong Tian, and Zhi-Da Jiang.
    A review of image set classification. Neurocomputing, 335:251â€“260, 2019.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[427] Yefeng Zheng, David Liu, Bogdan Georgescu, Hien Nguyen, and Dorin Comaniciu.
    3d deep learning for efficient and robust landmark detection in volumetric data.
    In International Conference on Medical Image Computing and Computer-Assisted Intervention,
    pages 565â€“572\. Springer, 2015.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[428] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and
    Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 633â€“641, 2017.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[429] Kaiyang Zhou, YuÂ Qiao, and Tao Xiang. Deep reinforcement learning for
    unsupervised video summarization with diversity-representativeness reward. In
    Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[430] Kaiyang Zhou, Tao Xiang, and Andrea Cavallaro. Video summarisation by
    classification with deep reinforcement learning. arXiv preprint arXiv:1807.03089,
    2018.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[431] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, LuÂ Yuan, and Yichen Wei. Deep feature
    flow for video recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2349â€“2358, 2017.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[432] Xiahai Zhuang and Juan Shen. Multi-scale patch and multi-modality atlases
    for whole heart segmentation of mri. Medical image analysis, 31:77â€“87, 2016.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[433] WillÂ Y Zou, Xiaoyu Wang, Miao Sun, and Yuanqing Lin. Generic object detection
    with dense neural patterns and regionlets. arXiv preprint arXiv:1404.4316, 2014.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
