- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:51:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.11510] Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.11510] 计算机视觉中的深度强化学习：综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.11510](https://ar5iv.labs.arxiv.org/html/2108.11510)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2108.11510](https://ar5iv.labs.arxiv.org/html/2108.11510)
- en: 'Deep Reinforcement Learning in Computer Vision:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉中的深度强化学习：
- en: A Comprehensive Survey
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 综合调查
- en: Ngan Le^(∗∗)    Vidhiwar Singh Rathour^∗    Kashu Yamazaki^∗    Khoa Luu   
    Marios Savvides
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Ngan Le^(∗∗)    Vidhiwar Singh Rathour^∗    Kashu Yamazaki^∗    Khoa Luu   
    Marios Savvides
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep reinforcement learning augments the reinforcement learning framework and
    utilizes the powerful representation of deep neural networks. Recent works have
    demonstrated the remarkable successes of deep reinforcement learning in various
    domains including finance, medicine, healthcare, video games, robotics, and computer
    vision. In this work, we provide a detailed review of recent and state-of-the-art
    research advances of deep reinforcement learning in computer vision. We start
    with *comprehending the theories* of deep learning, reinforcement learning, and
    deep reinforcement learning. We then *propose a categorization* of deep reinforcement
    learning methodologies and *discuss their advantages and limitations*. In particular,
    we divide deep reinforcement learning into *seven main categories* according to
    their applications in computer vision, i.e. (i) landmark localization (ii) object
    detection; (iii) object tracking; (iv) registration on both 2D image and 3D image
    volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other
    applications. Each of these categories is further analyzed with reinforcement
    learning techniques, network design, and performance. Moreover, we provide a comprehensive
    analysis of the existing publicly available datasets and examine source code availability.
    Finally, we present some open issues and discuss future research directions on
    deep reinforcement learning in computer vision.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习扩展了强化学习框架，并利用了深度神经网络的强大表示能力。最近的研究展示了深度强化学习在金融、医学、医疗保健、视频游戏、机器人技术和计算机视觉等多个领域的显著成功。在这项工作中，我们提供了对计算机视觉中深度强化学习的最新和最前沿研究进展的详细回顾。我们从*理解理论*开始，包括深度学习、强化学习和深度强化学习。接着我们*提出了一种分类*的深度强化学习方法，并*讨论了它们的优点和局限性*。特别是，我们根据在计算机视觉中的应用，将深度强化学习分为*七个主要类别*，即
    (i) 地标定位 (ii) 物体检测 (iii) 物体跟踪 (iv) 2D 图像和 3D 图像体数据的配准 (v) 图像分割 (vi) 视频分析；以及 (vii)
    其他应用。每个类别都进一步分析了强化学习技术、网络设计和性能。此外，我们提供了对现有公开数据集的全面分析，并检查了源代码的可用性。最后，我们提出了一些未解的问题，并讨论了计算机视觉中深度强化学习的未来研究方向。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Reinforcement learning (RL) is a machine learning technique for learning a sequence
    of actions in an interactive environment by trial and error that maximizes the
    expected reward [[351](#bib.bib351)]. Deep Reinforcement Learning (DRL) is the
    combination of Reinforcement Learning and Deep Learning (DL) and it has become
    one of the most intriguing areas of artificial intelligence today. DRL can solve
    a wide range of complex real-world decision-making problems with human-like intelligence
    that were previously intractable. DRL was selected by [[316](#bib.bib316)], [[106](#bib.bib106)]
    as one of ten breakthrough techniques in 2013 and 2017, respectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种通过试错学习在交互环境中学习一系列动作的机器学习技术，其目标是最大化期望奖励 [[351](#bib.bib351)]。深度强化学习（DRL）是强化学习与深度学习（DL）的结合，已成为当前人工智能领域最引人注目的领域之一。DRL
    能够解决广泛的复杂现实世界决策问题，这些问题以前是无法处理的，并具有类似人类的智能。DRL 被[[316](#bib.bib316)]和[[106](#bib.bib106)]分别选为2013年和2017年的十项突破性技术之一。
- en: The past years have witnessed the rapid development of DRL thanks to its amazing
    achievement in solving challenging decision-making problems in the real world.
    DRL has been successfully applied into many domains including games, robotics,
    autonomous driving, healthcare, natural language processing, and computer vision.
    In contrast to supervised learning which requires large labeled training data,
    DRL samples training data from an environment. This opens up many machine learning
    applications where big labeled training data does not exist.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年见证了深度强化学习（DRL）的迅速发展，得益于其在解决现实世界复杂决策问题上的惊人成就。DRL已经成功应用于许多领域，包括游戏、机器人技术、自动驾驶、医疗保健、自然语言处理和计算机视觉。与需要大量标记训练数据的监督学习不同，DRL从环境中采样训练数据。这为那些没有大量标记训练数据的机器学习应用打开了许多可能性。
- en: Far from supervised learning, DRL-based approaches focus on solving sequential
    decision-making problems. They aim at deciding, based on a set of experiences
    collected by interacting with the environment, the sequence of actions in an uncertain
    environment to achieve some targets. Different from supervised learning where
    the feedback is available after each system action, it is simply a scalar value
    that may be delayed in time in the DRL framework. For example, the success or
    failure of the entire system is reflected after a sequence of actions. Furthermore,
    the supervised learning model is updated based on the loss/error of the output
    and there is no mechanism to get the correct value when it is wrong. This is addressed
    by policy gradients in DRL by assigning gradients without a differentiable loss
    function. This aims at teaching a model to try things out randomly and learn to
    do correct things more.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，基于DRL的方法专注于解决顺序决策问题。它们旨在根据通过与环境互动收集的一系列经验，决定在不确定环境中的行动序列以实现某些目标。不同于监督学习，其中反馈在每次系统动作后都可以获得，DRL框架中的反馈通常只是一个可能会延迟的标量值。例如，整个系统的成功或失败是在一系列动作之后才会反映出来。此外，监督学习模型是基于输出的损失/错误进行更新的，并且当输出错误时没有机制来获取正确值。DRL中的策略梯度通过在没有可微分损失函数的情况下分配梯度来解决这个问题。这旨在教会模型随机尝试，并学会更多地做正确的事情。
- en: Many survey papers in the field of DRL including [[13](#bib.bib13)] [[97](#bib.bib97)]
    [[414](#bib.bib414)] have been introduced recently. While [[13](#bib.bib13)] covers
    central algorithms in DRL, [[97](#bib.bib97)] provides an introduction to DRL
    models, algorithms, and techniques, where particular focus is the aspects related
    to generalization and how DRL can be used for practical applications. Recently,
    [[414](#bib.bib414)] introduces a survey, which discusses the broad applications
    of RL techniques in healthcare domains ranging from dynamic treatment regimes
    in chronic diseases and critical care, an automated medical diagnosis from both
    unstructured and structured clinical data, to many other control or scheduling
    domains that have infiltrated many aspects of a healthcare system. Different from
    the previous work, our survey focuses on how to implement DRL in various computer
    vision applications such as landmark detection, object detection, object tracking,
    image registration, image segmentation, and video analysis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近介绍了许多关于DRL领域的综述论文，包括[[13](#bib.bib13)] [[97](#bib.bib97)] [[414](#bib.bib414)]。虽然[[13](#bib.bib13)]涵盖了DRL中的核心算法，[[97](#bib.bib97)]提供了对DRL模型、算法和技术的介绍，特别关注与泛化相关的方面以及DRL如何用于实际应用。最近，[[414](#bib.bib414)]介绍了一项综述，讨论了RL技术在医疗保健领域的广泛应用，从慢性疾病和重症监护中的动态治疗方案，到从非结构化和结构化临床数据中自动医疗诊断，再到渗透到医疗保健系统各个方面的许多控制或调度领域。与之前的工作不同，我们的综述专注于如何在各种计算机视觉应用中实现DRL，如地标检测、物体检测、物体跟踪、图像配准、图像分割和视频分析。
- en: 'Our goal is to provide our readers good knowledge about the principle of RL/DRL
    and thorough coverage of the latest examples of how DRL is used for solving computer
    vision tasks. We structure the rest of the paper as follows: we first introduce
    fundamentals of Deep Learning (DL) in section [2](#S2 "2 Introduction to Deep
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    including Multi-Layer Perceptron (MLP), Autoencoder, Deep Belief Network, Convolutional
    Neural Networks (CNNs), Recurrent Neural Networks (RNNs). Then, we present the
    theories of RL in section [3](#S3 "3 Basics of Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), which starts with the Markov
    Decision Process (MDP) and continues with value function and Q-function. In the
    end of section [3](#S3 "3 Basics of Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), we introduce various techniques
    in RL under two categories of model-based and model-free RL. Next, we introduce
    DRL in section [4](#S4 "4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey") with main techniques in
    both value-based methods, policy gradient methods, and actor-critic methods under
    model-based and model-free categories. The application of DRL in computer vision
    will then be introduced in sections [5](#S5 "5 DRL in Landmark Detection ‣ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey"), [6](#S6 "6
    DRL in Object Detection ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey"), [7](#S7 "7 DRL in Object Tracking ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey"), [8](#S8 "8 DRL in Image Registration ‣ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey"), [9](#S9 "9
    DRL in Image Segmentation ‣ Deep Reinforcement Learning in Computer Vision: A
    Comprehensive Survey"), [10](#S10 "10 DRL in Video Analysis ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), [11](#S11 "11 Others Applications
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") corresponding
    respectively to DRL in landmark detection, DRL in object detection, DRL in object
    tracking, DRL in image registration, DRL in image segmentation, DRL in video analysis
    and other applications of DRL. Each application category first starts with a problem
    introduction and then state-of-the-art approaches in the field are discussed and
    compared through a summary table. We are going to discuss some future perspectives
    in section [12](#S12 "12 Future Perspectives ‣ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey") including challenges of DRL in computer
    vision and the recent advanced techniques.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的目标是为读者提供关于RL/DRL原理的良好知识，并全面覆盖DRL在计算机视觉任务中应用的最新示例。我们将本文的其余部分结构化如下：我们首先在第[2](#S2
    "2 Introduction to Deep Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")节介绍深度学习（DL）的基础知识，包括多层感知机（MLP）、自编码器、深度信念网络、卷积神经网络（CNNs）、递归神经网络（RNNs）。然后，我们在第[3](#S3
    "3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey")节介绍RL的理论，从马尔可夫决策过程（MDP）开始，继续讲解价值函数和Q函数。在第[3](#S3
    "3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey")节的最后，我们介绍了RL中两类技术：基于模型的和无模型的RL。接下来，我们在第[4](#S4
    "4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey")节介绍DRL，重点讲解价值方法、策略梯度方法和演员-评论家方法在基于模型和无模型类别中的主要技术。然后，第[5](#S5
    "5 DRL in Landmark Detection ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")、第[6](#S6 "6 DRL in Object Detection ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey")、第[7](#S7 "7 DRL in Object
    Tracking ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")、第[8](#S8
    "8 DRL in Image Registration ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey")、第[9](#S9 "9 DRL in Image Segmentation ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey")、第[10](#S10 "10 DRL in Video
    Analysis ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")、第[11](#S11
    "11 Others Applications ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")节分别介绍了DRL在地标检测、目标检测、目标跟踪、图像配准、图像分割、视频分析和DRL其他应用中的应用。每个应用类别首先介绍问题，然后讨论和比较该领域的最新方法，通过总结表格呈现。我们将在第[12](#S12
    "12 Future Perspectives ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")节讨论一些未来的展望，包括DRL在计算机视觉中的挑战和近期先进技术。'
- en: 2 Introduction to Deep Learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习简介
- en: 2.1 Multi-Layer Perceptron (MLP)
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多层感知机（MLP）
- en: 'Deep learning models, in simple words, are large and deep artificial neural
    networks. Let us consider the simplest possible neural network which is called
    ”neuron” as illustrated in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1 Multi-Layer Perceptron
    (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey"). A computational model of a single neuron is
    called a perceptron which consists of one or more inputs, a processor, and a single
    output.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '简单来说，深度学习模型是大型且深层的人工神经网络。我们以最简单的神经网络为例，这种网络被称为“神经元”，如图 [1](#S2.F1 "Figure 1
    ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey") 所示。一个神经元的计算模型被称为感知器，它由一个或多个输入、一个处理器和一个输出组成。'
- en: '![Refer to caption](img/ae3f08d40dc14c10f862a38b2763122e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ae3f08d40dc14c10f862a38b2763122e.png)'
- en: 'Figure 1: An example of one neuron which takes input $\textbf{x}=[x_{1},x_{2},x_{3}]$,
    the intercept term $+1$ as bias, and the output o.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个神经元的示例，它以 $\textbf{x}=[x_{1},x_{2},x_{3}]$ 作为输入，以截距项 $+1$ 作为偏置，并计算输出 o。
- en: '![Refer to caption](img/3c8e28551e381f4fb936359fe3a47ebf.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3c8e28551e381f4fb936359fe3a47ebf.png)'
- en: 'Figure 2: An example of multi-layer perceptron network (MLP)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：多层感知器网络 (MLP) 的示例
- en: '![Refer to caption](img/ffb334467464b76c1b885be0d35835d8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ffb334467464b76c1b885be0d35835d8.png)'
- en: 'Figure 3: An illustration of various DL architectures. (a): Autoencoder (AE);
    (b): Deep Belief Network; (c): Convolutional Neural Network (CNN); (d): Recurrent
    Neural Network (RNN).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：各种深度学习架构的示意图。（a）：自编码器 (AE)；（b）：深度置信网络；（c）：卷积神经网络 (CNN)；（d）：递归神经网络 (RNN)。
- en: 'In this example, the neuron is a computational unit that takes $\textbf{x}=[x_{0},x_{1},x_{2}]$
    as input, the intercept term $+1$ as bias b, and the output o. The goal of this
    simple network is to learn a function $f:\mathrm{R^{N}}\rightarrow\mathrm{R^{M}}$
    where $N$ is the number of dimensions for input x and $M$ is the number of dimensions
    for output which is computed as $\textbf{o}=f(\textbf{x},\theta)$, where $\theta$
    is a set of weights and are known as weights $\theta=\{w_{i}\}$. Mathematically,
    the output o of a one neuron is defined as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，神经元是一个计算单元，它以 $\textbf{x}=[x_{0},x_{1},x_{2}]$ 作为输入，以截距项 $+1$ 作为偏置 b，计算输出
    o。这个简单网络的目标是学习一个函数 $f:\mathrm{R^{N}}\rightarrow\mathrm{R^{M}}$，其中 $N$ 是输入 x 的维度数量，$M$
    是输出的维度数量，计算公式为 $\textbf{o}=f(\textbf{x},\theta)$，其中 $\theta$ 是一组权重，称为权重 $\theta=\{w_{i}\}$。在数学上，一个神经元的输出
    o 定义为：
- en: '|  | $\textbf{o}=f(\textbf{x},\theta)=\sigma\left(\sum_{i=1}^{N}{w_{i}x_{i}+b}\right)=\sigma(\textbf{W}^{T}\textbf{x}+b)$
    |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{o}=f(\textbf{x},\theta)=\sigma\left(\sum_{i=1}^{N}{w_{i}x_{i}+b}\right)=\sigma(\textbf{W}^{T}\textbf{x}+b)$
    |  | (1) |'
- en: 'In this equation, $\sigma$ is the point-wise non-linear activation function.
    The common non-linear activation functions for hidden units are hyperbolic tangent
    (Tanh), sigmoid, softmax, ReLU, and LeakyReLU. A typical multi-layer perception
    (MLP) neural network is composed of one input layer, one output layer, and many
    hidden layers. Each layer may contain many units. In this network, x is the input
    layer, o is the output layer. The middle layer is called the hidden layer. In
    Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction
    to Deep Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(b), MLP contains 3 units of the input layer, 3 units of the hidden layer,
    and 1 unit of the output layer.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个方程中，$\sigma$ 是逐点非线性激活函数。隐藏单元的常见非线性激活函数有双曲正切（Tanh）、sigmoid、softmax、ReLU 和
    LeakyReLU。一个典型的多层感知器（MLP）神经网络由一个输入层、一个输出层和多个隐藏层组成。每一层可能包含多个单元。在这个网络中，x 是输入层，o
    是输出层。中间层称为隐藏层。在图 [2](#S2.F2 "Figure 2 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction
    to Deep Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(b) 中，MLP 包含 3 个输入层单元、3 个隐藏层单元和 1 个输出层单元。'
- en: In general, we consider a MLP neural network with $L$ hidden layers of units,
    one layer of input units and one layer of output units. The number of input units
    is $N$, output units is $M$, and units in hidden layer $l^{th}$ is $N^{l}$. The
    weight of the $j^{th}$ unit in layer $l^{th}$ and the $i^{th}$ unit in layer $(l+1)^{th}$
    is denoted by $w_{ij}^{l}$. The activation of the $i^{th}$ unit in layer $l^{th}$
    is $\textbf{h}_{i}^{l}$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们考虑一个具有 $L$ 层隐藏单元的 MLP 神经网络，一个输入单元层和一个输出单元层。输入单元的数量是 $N$，输出单元的数量是 $M$，第
    $l^{th}$ 层的隐藏单元数量是 $N^{l}$。第 $l^{th}$ 层的第 $j^{th}$ 单元和 $(l+1)^{th}$ 层的第 $i^{th}$
    单元之间的权重用 $w_{ij}^{l}$ 表示。第 $l^{th}$ 层的第 $i^{th}$ 单元的激活值为 $\textbf{h}_{i}^{l}$。
- en: '![Refer to caption](img/873522a85978bea2ad1c0c5014fc2f0d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/873522a85978bea2ad1c0c5014fc2f0d.png)'
- en: 'Figure 4: Architecture of a typical convolutional network for image classification
    containing three basic layers: convolution layer, pooling layer and fully connected
    layer'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：典型卷积网络的架构用于图像分类，包含三个基本层：卷积层、池化层和全连接层
- en: 2.2 Autoencoder
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 自编码器
- en: 'Autoencoder is an unsupervised algorithm used for representation learning,
    such as feature selection or dimension reduction. A gentle introduction to Variational
    Autoencoder (VAE) is given in [[11](#bib.bib11)] and VAE framework is illustrated
    in Fig.[3](#S2.F3 "Figure 3 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction
    to Deep Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")(a). In general, VAE aims to learn a parametric latent variable model
    by maximizing the marginal log-likelihood of the training data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '自编码器是一种无监督算法，用于表示学习，如特征选择或维度减少。有关变分自编码器（VAE）的温和介绍见[[11](#bib.bib11)]，VAE框架在图[3](#S2.F3
    "Figure 3 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction to Deep Learning
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")(a)中有所说明。一般来说，VAE旨在通过最大化训练数据的边际对数似然来学习一个参数化的潜变量模型。'
- en: 2.3 Deep Belief Network
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 深度置信网络
- en: 'Deep Belief Network (DBN) and Deep Autoencoder are two common unsupervised
    approaches that have been used to initialize the network instead of random initialization.
    While Deep Autoencoder is based on Autoencoder, Deep Belief Networks is based
    on Restricted Boltzmann Machine (RBM), which contains a layer of input data and
    a layer of hidden units that learn to represent features that capture high-order
    correlations in the data as illustrated in Fig.[3](#S2.F3 "Figure 3 ‣ 2.1 Multi-Layer
    Perceptron (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey")(b).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '深度置信网络（DBN）和深度自编码器是两种常见的无监督学习方法，它们被用来初始化网络而不是随机初始化。虽然深度自编码器基于自编码器，深度置信网络则基于限制玻尔兹曼机（RBM），RBM包含一个输入数据层和一个隐含单元层，隐含单元层学习表示能够捕捉数据中高阶相关性的特征，如图[3](#S2.F3
    "Figure 3 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction to Deep Learning
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")(b)所示。'
- en: 2.4 Convolutional Neural Networks (CNN)
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 卷积神经网络（CNN）
- en: 'Convolutional Neural Network (CNN) [[204](#bib.bib204)] [[203](#bib.bib203)]
    is a special case of fully connected MLP that implements weight sharing for processing
    data. CNN uses the spatial correlation of the signal to utilize the architecture
    in a more sensible way. Their architecture, somewhat inspired by the biological
    visual system, possesses two key properties that make them extremely useful for
    image applications: spatially shared weights and spatial pooling. These kinds
    of networks learn features that are shift-invariant, i.e., filters that are useful
    across the entire image (due to the fact that image statistics are stationary).
    The pooling layers are responsible for reducing the sensitivity of the output
    to slight input shifts and distortions, and increasing the reception field for
    next layers. Since 2012, one of the most notable results in Deep Learning is the
    use of CNN to obtain a remarkable improvement in object recognition in ImageNet
    classification challenge [[72](#bib.bib72)] [[187](#bib.bib187)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）[[204](#bib.bib204)] [[203](#bib.bib203)] 是一种特殊的全连接MLP，它实现了权重共享以处理数据。CNN利用信号的空间相关性，以更合理的方式利用架构。其架构在一定程度上受到生物视觉系统的启发，具有两个关键属性，使其在图像应用中极为有用：空间共享权重和空间池化。这些网络学习的特征是平移不变的，即在整个图像中都有效的滤波器（因为图像统计是静态的）。池化层负责减少输出对输入轻微位移和失真的敏感性，并增加下一层的接受域。自2012年以来，深度学习领域最显著的成果之一是使用CNN在ImageNet分类挑战中取得了显著的目标识别改进[[72](#bib.bib72)]
    [[187](#bib.bib187)]。
- en: 'A typical CNN is composed of multiple stages, as shown in Fig. [3](#S2.F3 "Figure
    3 ‣ 2.1 Multi-Layer Perceptron (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep
    Reinforcement Learning in Computer Vision: A Comprehensive Survey")(c). The output
    of each stage is made of a set of 2D arrays called feature maps. Each feature
    map is the outcome of one convolutional (and an optional pooling) filter applied
    over the full image. A point-wise non-linear activation function is applied after
    each convolution. In its more general form, a CNN can be written as'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 CNN 由多个阶段组成，如图 [3](#S2.F3 "图 3 ‣ 2.1 多层感知器 (MLP) ‣ 2 深度学习介绍 ‣ 计算机视觉中的深度强化学习：全面综述")(c)
    所示。每个阶段的输出由一组称为特征图的 2D 数组组成。每个特征图是一个卷积（及可选的池化）滤波器应用于整个图像的结果。每次卷积后都会应用一个逐点非线性激活函数。在其更一般的形式下，CNN
    可以写作
- en: '|  | <math   alttext="\begin{split}\textbf{h}^{0}=&amp;\textbf{x}\\ \textbf{h}^{l}=&amp;pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{split}\textbf{h}^{0}=&amp;\textbf{x}\\ \textbf{h}^{l}=&amp;pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\'
- en: \textbf{o}=&amp;\textbf{h}^{L}\\
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \textbf{o}=&amp;\textbf{h}^{L}\\
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd columnalign="right" ><mrow ><msup ><mtext >h</mtext><mn  >0</mn></msup><mo
    >=</mo></mrow></mtd><mtd columnalign="left" ><mtext >x</mtext></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><msup ><mtext >h</mtext><mi >l</mi></msup><mo
    >=</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mrow ><mrow ><mrow ><mi  >p</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><msup ><mi  >l</mi><mi >l</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub
    ><mi >σ</mi><mi >l</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mrow ><msup ><mtext >w</mtext><mi >l</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msup ><mtext >h</mtext><mrow ><mi >l</mi><mo
    >−</mo><mn >1</mn></mrow></msup></mrow><mo >+</mo><msup ><mtext >b</mtext><mi
    >l</mi></msup></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo><mrow ><mo rspace="0.167em"  >∀</mo><mi >l</mi></mrow></mrow><mo >∈</mo><mn
    >1</mn></mrow><mo >,</mo><mrow ><mn >2</mn><mo >,</mo><mrow ><mi mathvariant="normal"  >…</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >L</mi></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mtext >o</mtext><mo >=</mo></mrow></mtd><mtd
    columnalign="left" ><msup ><mtext >h</mtext><mi >L</mi></msup></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><cn
    type="integer" >0</cn></apply><apply ><ci ><mtext >x</mtext></ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><ci >𝑙</ci></apply></apply></apply><apply
    ><apply  ><ci >𝑝</ci><ci >𝑜</ci><ci  >𝑜</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑙</ci><ci >𝑙</ci></apply><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜎</ci><ci >𝑙</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    ><mtext >w</mtext></ci><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    ><mtext >h</mtext></ci><apply ><ci >𝑙</ci><cn type="integer" >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci ><mtext >b</mtext></ci><ci
    >𝑙</ci></apply></apply></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >formulae-sequence</csymbol><apply ><apply ><csymbol cd="latexml" >for-all</csymbol><ci
    >𝑙</ci></apply><list ><cn type="integer" >1</cn><cn type="integer" >2</cn></list></apply><apply
    ><apply  ><ci >…</ci><ci >𝐿</ci><ci ><mtext >o</mtext></ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><ci >𝐿</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\textbf{h}^{0}=&\textbf{x}\\ \textbf{h}^{l}=&pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\ \textbf{o}=&\textbf{h}^{L}\\ \end{split}</annotation></semantics></math>
    |  | (2) |
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd columnalign="right" ><mrow ><msup ><mtext >h</mtext><mn  >0</mn></msup><mo
    >=</mo></mrow></mtd><mtd columnalign="left" ><mtext >x</mtext></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><msup ><mtext >h</mtext><mi >l</mi></msup><mo
    >=</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mrow ><mrow ><mrow ><mi  >p</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><msup ><mi  >l</mi><mi >l</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub
    ><mi >σ</mi><mi >l</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mrow ><msup ><mtext >w</mtext><mi >l</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msup ><mtext >h</mtext><mrow ><mi >l</mi><mo
    >−</mo><mn >1</mn></mrow></msup></mrow><mo >+</mo><msup ><mtext >b</mtext><mi
    >l</mi></msup></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo><mrow ><mo rspace="0.167em"  >∀</mo><mi >l</mi></mrow></mrow><mo >∈</mo><mn
    >1</mn></mrow><mo >,</mo><mrow ><mn >2</mn><mo >,</mo><mrow ><mi mathvariant="normal"  >…</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >L</mi></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mtext >o</mtext><mo >=</mo></mrow></mtd><mtd
    columnalign="left" ><msup ><mtext >h</mtext><mi >L</mi></msup></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><cn
    type="integer" >0</cn></apply><apply ><ci ><mtext >x</mtext></ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><ci >𝑙</ci></apply></apply></apply><apply
    ><apply  ><ci >𝑝</ci><ci >𝑜</ci><ci  >𝑜</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑙</ci><ci >𝑙</ci></apply><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜎</ci><ci >𝑙</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    ><mtext >w</mtext></ci><ci >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    ><mtext >h</mtext></ci><apply ><ci >𝑙</ci><cn type="integer" >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci ><mtext >b</mtext></ci><ci
    >𝑙</ci></apply></apply></apply></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >formulae-sequence</csymbol><apply ><apply ><csymbol cd="latexml" >for-all</csymbol><ci
    >𝑙</ci></apply><list ><cn type="integer" >1</cn><cn type="integer" >2</cn></list></apply><apply
    ><apply  ><ci >…</ci><ci >𝐿</ci><ci ><mtext >o</mtext></ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci ><mtext >h</mtext></ci><ci >𝐿</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\textbf{h}^{0}=&\textbf{x}\\ \textbf{h}^{l}=&pool^{l}(\sigma_{l}(\textbf{w}^{l}\textbf{h}^{l-1}+\textbf{b}^{l})),\forall
    l\in{1,2,...L}\\ \textbf{o}=&\textbf{h}^{L}\\ \end{split}</annotation></semantics></math>
    |  | (2) |
- en: where $\textbf{w}^{l},\textbf{b}^{l}$ are trainable parameters as in MLPs at
    layer $l^{th}$. $\textbf{x}\in\mathrm{R}^{c\times h\times w}$ is vectorized from
    an input image with $c$ being the color channels, $h$ the image height and $w$
    the image width. $\textbf{o}\in\mathrm{R}^{n\times h^{\prime}\times w^{\prime}}$
    is vectorized from an array of dimension $h^{\prime}\times w^{\prime}$ of output
    vector (of dimension $n$). $pool^{l}$ is a (optional) pooling function at layer
    $l^{th}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{w}^{l},\textbf{b}^{l}$ 是像MLPs中层 $l^{th}$ 的可训练参数。$\textbf{x}\in\mathrm{R}^{c\times
    h\times w}$ 是从输入图像向量化得到的，其中 $c$ 为颜色通道，$h$ 为图像高度，$w$ 为图像宽度。$\textbf{o}\in\mathrm{R}^{n\times
    h^{\prime}\times w^{\prime}}$ 是从输出向量（维度为 $n$）的 $h^{\prime}\times w^{\prime}$ 的数组向量化得到的。$pool^{l}$
    是层 $l^{th}$ 的（可选）池化函数。
- en: Compared to traditional machine learning methods, CNN has achieved state-of-the-art
    performance in many domains including image understanding, video analysis and
    audio/speech recognition. In image understanding [[404](#bib.bib404)], [[426](#bib.bib426)],
    CNN outperforms human capacities [[39](#bib.bib39)]. Video analysis [[422](#bib.bib422)],
    [[217](#bib.bib217)] is another application that turns the CNN model from a detector
    [[374](#bib.bib374)] into a tracker [[94](#bib.bib94)]. As a special case of image
    segmentation [[194](#bib.bib194)], [[193](#bib.bib193)], saliency detection is
    another computer vision application that uses CNN [[381](#bib.bib381)], [[213](#bib.bib213)].
    In addition to the previous applications, pose estimation [[290](#bib.bib290)],
    [[362](#bib.bib362)] is another interesting research that uses CNN to estimate
    human-body pose. Action recognition in both still images and videos is a special
    case of recognition and is a challenging problem. [[110](#bib.bib110)] utilizes
    CNN-based representation of contextual information in which the most representative
    secondary region within a large number of object proposal regions, together with
    the contextual features, is used to describe the primary region. CNN-based action
    recognition in video sequences is reviewed in [[420](#bib.bib420)]. Text detection
    and recognition using CNN is the next step of optical character recognition (OCR)
    [[406](#bib.bib406)] and word spotting [[160](#bib.bib160)]. Not only in computer
    vision, CNN has been successfully applied into other domains such as speech recognition
    and speech synthesis [[274](#bib.bib274)], [[283](#bib.bib283)], biometrics [[242](#bib.bib242)],
    [[85](#bib.bib85)], [[281](#bib.bib281)], [[350](#bib.bib350)],[[304](#bib.bib304)],
    [[261](#bib.bib261)], biomedical [[191](#bib.bib191)], [[342](#bib.bib342)], [[192](#bib.bib192)],
    [[411](#bib.bib411)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习方法相比，CNN 在包括图像理解、视频分析和音频/语音识别等许多领域中取得了最先进的性能。在图像理解中 [[404](#bib.bib404)],
    [[426](#bib.bib426)], CNN 超越了人类的能力 [[39](#bib.bib39)]。视频分析 [[422](#bib.bib422)],
    [[217](#bib.bib217)] 是另一个应用，它将 CNN 模型从检测器 [[374](#bib.bib374)] 转变为跟踪器 [[94](#bib.bib94)]。作为图像分割
    [[194](#bib.bib194)], [[193](#bib.bib193)] 的特殊情况，显著性检测是另一个利用 CNN 的计算机视觉应用 [[381](#bib.bib381)],
    [[213](#bib.bib213)]。除了之前的应用，姿态估计 [[290](#bib.bib290)], [[362](#bib.bib362)] 是另一个有趣的研究，利用
    CNN 估计人体姿势。静态图像和视频中的动作识别是识别的特殊情况，并且是一个具有挑战性的问题。[[110](#bib.bib110)] 利用基于 CNN 的上下文信息表示，其中在大量目标提议区域中最具代表性的次区域与上下文特征一起，用于描述主要区域。基于
    CNN 的视频序列动作识别的综述见 [[420](#bib.bib420)]。使用 CNN 的文本检测和识别是光学字符识别（OCR） [[406](#bib.bib406)]
    和词汇检测 [[160](#bib.bib160)] 的下一步。CNN 不仅在计算机视觉中取得成功，还成功应用于语音识别和语音合成 [[274](#bib.bib274)],
    [[283](#bib.bib283)], 生物识别 [[242](#bib.bib242)], [[85](#bib.bib85)], [[281](#bib.bib281)],
    [[350](#bib.bib350)], [[304](#bib.bib304)], [[261](#bib.bib261)], 生物医学 [[191](#bib.bib191)],
    [[342](#bib.bib342)], [[192](#bib.bib192)], [[411](#bib.bib411)]。
- en: 2.5 Recurrent Neural Networks (RNN)
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 循环神经网络（RNN）
- en: 'RNN is an extremely powerful sequence model and was introduced in the early
    1990s [[172](#bib.bib172)]. A typical RNN contains three parts, namely, sequential
    input data ($\textbf{x}_{t}$), hidden state ($\textbf{h}_{t}$) and sequential
    output data ($\textbf{y}_{t}$) as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Multi-Layer
    Perceptron (MLP) ‣ 2 Introduction to Deep Learning ‣ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey")(d).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是一种极为强大的序列模型，早在1990年代初期就被引入 [[172](#bib.bib172)]。一个典型的 RNN 包含三个部分，即序列输入数据（$\textbf{x}_{t}$）、隐藏状态（$\textbf{h}_{t}$）和序列输出数据（$\textbf{y}_{t}$），如图
    [3](#S2.F3 "图 3 ‣ 2.1 多层感知器 (MLP) ‣ 2 深度学习简介 ‣ 计算机视觉中的深度强化学习：全面调查")(d) 所示。
- en: RNN makes use of sequential information and performs the same task for every
    element of a sequence where the output is dependent on the previous computations.
    The activation of the hidden states at time-step $t$ is computed as a function
    $f$ of the current input symbol $\bf{x}_{t}$ and the previous hidden states $\bf{h}_{t-1}$.
    The output at time $t$ is calculated as a function $g$ of the current hidden state
    $\bf{h}_{t}$ as follows
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 利用序列信息，并对序列的每个元素执行相同的任务，其中输出依赖于之前的计算。时间步 $t$ 的隐藏状态激活作为当前输入符号 $\bf{x}_{t}$
    和之前隐藏状态 $\bf{h}_{t-1}$ 的函数 $f$ 来计算。时间 $t$ 的输出作为当前隐藏状态 $\bf{h}_{t}$ 的函数 $g$ 计算如下
- en: '|  | $\begin{split}\textbf{{h}}_{t}&amp;=f(\textbf{Ux}_{t}+\textbf{Wh}_{t-1})\\
    \textbf{y}_{t}&amp;=g(\textbf{Vh}_{t})\end{split}$ |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\textbf{{h}}_{t}&=f(\textbf{Ux}_{t}+\textbf{Wh}_{t-1})\\
    \textbf{y}_{t}&=g(\textbf{Vh}_{t})\end{split}$ |  | (3) |'
- en: where $\bf{U}$ is the input-to-hidden weight matrix, $\bf{W}$ is the state-to-state
    recurrent weight matrix, $\bf{V}$ is the hidden-to-output weight matrix. $f$ is
    usually a logistic sigmoid function or a hyperbolic tangent function and $g$ is
    defined as a softmax function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bf{U}$ 是输入到隐藏的权重矩阵，$\bf{W}$ 是状态到状态的递归权重矩阵，$\bf{V}$ 是隐藏到输出的权重矩阵。$f$ 通常是逻辑
    sigmoid 函数或双曲正切函数，$g$ 定义为 softmax 函数。
- en: Most works on RNN have made use of the method of backpropagation through time
    (BPTT) [[318](#bib.bib318)] to train the parameter set $(\bf{U},\bf{V},\bf{W})$
    and propagate error backward through time. In classic backpropagation, the error
    or loss function is defined as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于 RNN 的研究使用了通过时间的反向传播（BPTT）[[318](#bib.bib318)] 方法来训练参数集 $(\bf{U},\bf{V},\bf{W})$
    并将误差向后传播。在经典的反向传播中，误差或损失函数定义为
- en: '|  | $E(\textbf{y''},\textbf{y})=\sum_{t}{&#124;&#124;\textbf{y''}_{t}-\textbf{y}_{t}&#124;&#124;^{2}}$
    |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(\textbf{y''},\textbf{y})=\sum_{t}{\|\textbf{y''}_{t}-\textbf{y}_{t}\|^{2}}$
    |  | (4) |'
- en: where $\textbf{y}_{t}$ is the prediction and $\textbf{y'}_{t}$ is the labeled
    groundtruth.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{y}_{t}$ 是预测值，$\textbf{y'}_{t}$ 是标记的真实值。
- en: For a specific weight W, the update rule for gradient descent is defined as
    $\textbf{W}^{new}=\textbf{W}-\gamma\frac{\partial E}{\partial\textbf{W}}$, where
    $\gamma$ is the learning rate. In RNN model, the gradients of the error with respect
    to our parameters U, V and W are learned using Stochastic Gradient Descent (SGD)
    and chain rule of differentiation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的权重 W，梯度下降的更新规则定义为 $\textbf{W}^{new}=\textbf{W}-\gamma\frac{\partial E}{\partial\textbf{W}}$，其中
    $\gamma$ 是学习率。在 RNN 模型中，使用随机梯度下降（SGD）和链式法则来学习误差相对于参数 U、V 和 W 的梯度。
- en: The difficulty of training RNN to capture long-term dependencies has been studied
    in [[26](#bib.bib26)]. To address the issue of learning long-term dependencies,
    Hochreiter and Schmidhuber [[139](#bib.bib139)] proposed Long Short-Term Memory
    (LSTM), which can maintain a separate memory cell inside it that updates and exposes
    its content only when deemed necessary. Recently, a Gated Recurrent Unit (GRU)
    was proposed by [[51](#bib.bib51)] to make each recurrent unit adaptively capture
    dependencies of different time scales. Like the LSTM unit, the GRU has gating
    units that modulate the flow of information inside the unit but without having
    separate memory cells.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 RNN 以捕捉长期依赖性的难度已在[[26](#bib.bib26)]中研究过。为了应对学习长期依赖性的问题，Hochreiter 和 Schmidhuber
    [[139](#bib.bib139)] 提出了长短期记忆（LSTM），它可以在内部维持一个独立的记忆单元，仅在必要时更新并暴露其内容。最近，[[51](#bib.bib51)]
    提出了门控循环单元（GRU），使每个循环单元能够自适应地捕捉不同时间尺度的依赖性。与 LSTM 单元类似，GRU 具有门控单元来调节信息在单元内部的流动，但没有独立的记忆单元。
- en: Several variants of RNN have been later introduced and successfully applied
    to wide variety of tasks, such as natural language processing [[257](#bib.bib257)],
    [[214](#bib.bib214)], speech recognition [[115](#bib.bib115)], [[54](#bib.bib54)],
    machine translation [[175](#bib.bib175)], [[241](#bib.bib241)], question answering
    [[138](#bib.bib138)], image captioning [[247](#bib.bib247)], [[78](#bib.bib78)],
    and many more.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 后来介绍了几种 RNN 的变体，并成功应用于各种任务，如自然语言处理 [[257](#bib.bib257)]、[[214](#bib.bib214)]、语音识别
    [[115](#bib.bib115)]、[[54](#bib.bib54)]、机器翻译 [[175](#bib.bib175)]、[[241](#bib.bib241)]、问答
    [[138](#bib.bib138)]、图像描述 [[247](#bib.bib247)]、[[78](#bib.bib78)] 等。
- en: 3 Basics of Reinforcement Learning
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 强化学习基础
- en: This section serves as a brief introduction to the theoretical models and techniques
    in RL. In order to provide a quick overview of what constitutes the main components
    of RL methods, some fundamental concepts and major theoretical problems are also
    clarified. RL is a kind of machine learning method where agents learn the optimal
    policy by trial and error. Unlike supervised learning, the feedback is available
    after each system action, it is simply a scalar value that may be delayed in time
    in RL framework, for example, the success or failure of the entire system is reflected
    after a sequence of actions. Furthermore, the supervised learning model is updated
    based on the loss/error of the output and there is no mechanism to get the correct
    value when it is wrong. This is addressed by policy gradients in RL by assigning
    gradients without a differentiable loss function which aims at teaching a model
    to try things out randomly and learn to do correct things more.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要介绍了 RL 中的理论模型和技术。为了提供 RL 方法主要组成部分的快速概述，某些基本概念和主要理论问题也得到澄清。RL 是一种机器学习方法，其中代理通过试错学习最优策略。与监督学习不同，反馈在每次系统动作后可用，在
    RL 框架中，反馈只是一个可能会延迟的标量值，例如，整个系统的成功或失败是在一系列动作后体现的。此外，监督学习模型是基于输出的损失/错误进行更新的，并且没有机制在出错时获取正确的值。RL
    中的策略梯度通过分配梯度而不使用可微分的损失函数来解决这个问题，这旨在教导模型随机尝试，并学习做正确的事情更多。
- en: Inspired by behavioral psychology, RL was proposed to address the sequential
    decision-making problems which exist in many applications such as games, robotics,
    healthcare, smart grids, stock, autonomous driving, etc. Unlike supervised learning
    where the data is given, an artificial agent collects experiences (data) by interacting
    with its environment in RL framework. Such experience is then gathered to optimize
    the cumulative rewards/utilities.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 受行为心理学启发，RL 被提出以解决许多应用中存在的序列决策问题，如游戏、机器人、医疗保健、智能电网、股票、自动驾驶等。与给定数据的监督学习不同，人工代理通过与环境交互在
    RL 框架中收集经验（数据）。然后收集这些经验以优化累积奖励/效用。
- en: 'In this section, we focus on how the RL problem can be formalized as an agent
    that can make decisions in an environment to optimize some objectives presented
    under reward functions. Some key aspects of RL are: (i) Address the sequential
    decision making; (ii) There is no supervisor, only a reward presented as scalar
    number; and (iii) The feedback is highly delayed. Markov Decision Process (MDP)
    is a framework that has commonly been used to solve most RL problems with discrete
    actions, thus we will first discuss MDP in this section. We then introduce value
    function and how to categorize RL into model-based or model-free methods. At the
    end of this section, we discuss some challenges in RL.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们重点讨论 RL 问题如何被形式化为一个能够在环境中做出决策以优化在奖励函数下呈现的一些目标的代理。RL 的一些关键方面是：(i) 解决序列决策问题；(ii)
    没有监督者，只有以标量数字呈现的奖励；(iii) 反馈高度延迟。马尔可夫决策过程（MDP）是一个框架，通常用于解决大多数具有离散动作的 RL 问题，因此我们将首先讨论
    MDP。在本节结束时，我们介绍值函数以及如何将 RL 分类为基于模型或无模型的方法。最后，我们讨论 RL 中的一些挑战。
- en: '![Refer to caption](img/878eca4dbadcd3f5807e57db9d9a5ff4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/878eca4dbadcd3f5807e57db9d9a5ff4.png)'
- en: 'Figure 5: An illustration of agent-environment interaction in RL'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：RL 中代理与环境交互的示意图
- en: 3.1 Markov Decision Process
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 马尔可夫决策过程
- en: 'The standard theory of RL is defined by a Markov Decision Process (MDP), which
    is an extension of the Markov process (also known as the Markov chain). Mathematically,
    the Markov process is a discrete-time stochastic process whose conditional probability
    distribution of the future states only depends upon the present state and it provides
    a framework to model decision-making situations. An MDP is typically defined by
    five elements as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: RL 的标准理论由马尔可夫决策过程（MDP）定义，它是马尔可夫过程（也称为马尔可夫链）的扩展。从数学上讲，马尔可夫过程是一种离散时间随机过程，其未来状态的条件概率分布仅依赖于当前状态，并且它提供了建模决策情境的框架。一个
    MDP 通常由以下五个元素定义：
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$S$: a set of state or observation space of an environment. $s_{0}$ is starting
    state.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S$：环境的状态或观察空间集合。$s_{0}$ 是起始状态。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mathcal{A}$: set of actions the agent can choose.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{A}$：代理可以选择的动作集。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$T$: a transition probability function $T(s_{t+1}|s_{t},a_{t})$, specifying
    the probability that the environment will transition to state $s_{t+1}\in S$ if
    the agent takes action $a_{t}\in\mathcal{A}$ in state $s_{t}\in S$.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$T$: 转移概率函数 $T(s_{t+1}|s_{t},a_{t})$，指定如果代理在状态 $s_{t}\in S$ 下采取动作 $a_{t}\in\mathcal{A}$，环境将以概率转移到状态
    $s_{t+1}\in S$。'
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$R$: a reward function where $r_{t+1}=R(s_{t},s_{t+1})$ is a reward received
    for taking action $a_{t}$ at state $s_{t}$ and transfer to the next state $s_{t+1}$.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$R$: 奖励函数，其中 $r_{t+1}=R(s_{t},s_{t+1})$ 是在状态 $s_{t}$ 下采取动作 $a_{t}$ 并转移到下一个状态
    $s_{t+1}$ 时获得的奖励。'
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\gamma$: a discount factor.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\gamma$: 折扣因子。'
- en: 'Considering MDP($S$, $\mathcal{A}$, $\gamma$, $T$, $R$), the agent chooses
    an action $a_{t}$ according to the policy $\pi(a_{t}|s_{t})$ at state $s_{t}$.
    Notably, agent’s algorithm for choosing action $a$ given current state $s$, which
    in general can be viewed as distribution $\pi(a|s)$, is called a policy (strategy).
    The environment receives the action, produces a reward $r_{t+1}$ and transfers
    to the next state $s_{t+1}$ according to the transition probability $T(s_{t+1}|s_{t},a_{t})$.
    The process continues until the agent reaches a terminal state or a maximum time
    step. In RL framework, the tuple $(s_{t},a_{t},r_{t+1},s_{t+1})$ is called transition.
    Several sequential transitions are usually referred to as roll-out. Full sequence
    $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$ is called a trajectory. Theoretically,
    trajectory is infinitely long, but the episodic property holds in most practical
    cases. One trajectory of some finite length $\tau$ is called an episode. For given
    MDP and policy $\pi$, the probability of observing $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$
    is called trajectory distribution and is denoted as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 MDP($S$, $\mathcal{A}$, $\gamma$, $T$, $R$)，代理根据策略 $\pi(a_{t}|s_{t})$ 在状态
    $s_{t}$ 下选择动作 $a_{t}$。值得注意的是，代理在给定当前状态 $s$ 时选择动作 $a$ 的算法，通常可以视为分布 $\pi(a|s)$，称为策略（策略）。环境接收到动作，产生奖励
    $r_{t+1}$ 并根据转移概率 $T(s_{t+1}|s_{t},a_{t})$ 转移到下一个状态 $s_{t+1}$。该过程继续，直到代理到达终止状态或达到最大时间步。在强化学习框架中，元组
    $(s_{t},a_{t},r_{t+1},s_{t+1})$ 称为转移。多个顺序转移通常被称为回滚。完整的序列 $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$
    被称为轨迹。理论上，轨迹是无限长的，但在大多数实际情况下具有 episodic 属性。某个有限长度 $\tau$ 的轨迹称为一个回合。对于给定的 MDP 和策略
    $\pi$，观察到 $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...)$ 的概率称为轨迹分布，记作：
- en: '|  | $\mathcal{T}_{\pi}=\prod_{t}{\pi(a_{t}&#124;s_{t})T(s_{t+1}&#124;s_{t},a_{t})}$
    |  | (5) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{T}_{\pi}=\prod_{t}{\pi(a_{t}|s_{t})T(s_{t+1}|s_{t},a_{t})}$
    |  | (5) |'
- en: 'The objective of RL is to find the optimal policy $\pi^{*}$ for the agent that
    maximizes the cumulative reward, which is called return. For every episode, the
    return is defined as the weighted sum of immediate rewards:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是为代理找到最优策略 $\pi^{*}$，该策略最大化累积奖励，这被称为回报。对于每个回合，回报被定义为即时奖励的加权和：
- en: '|  | $\mathcal{R}=\sum_{t=0}^{\tau-1}{\gamma^{t}r_{t+1}}$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}=\sum_{t=0}^{\tau-1}{\gamma^{t}r_{t+1}}$ |  | (6) |'
- en: 'Because the policy induces a trajectory distribution, the expected reward maximization
    can be written as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因为策略会引发一个轨迹分布，所以期望奖励最大化可以写作：
- en: '|  | $\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}{r_{t+1}}\rightarrow\max_{\pi}$
    |  | (7) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}{r_{t+1}}\rightarrow\max_{\pi}$
    |  | (7) |'
- en: 'Thus, given MDP and policy $\pi$, the discounted expected reward is defined:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定 MDP 和策略 $\pi$，折扣期望奖励被定义为：
- en: '|  | $\mathcal{G}(\pi)=\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}\gamma^{t}{r_{t+1}}$
    |  | (8) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}(\pi)=\mathbb{E}_{{\mathcal{T}_{\pi}}}\sum_{t=0}^{\tau-1}\gamma^{t}{r_{t+1}}$
    |  | (8) |'
- en: The goal of RL is to find an optimal policy $\pi^{*}$, which maximizes the discounted
    expected reward, i.e. $\mathcal{G}(\pi)\rightarrow\max_{\pi}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是找到一个最优策略 $\pi^{*}$，使折扣期望奖励最大化，即 $\mathcal{G}(\pi)\rightarrow\max_{\pi}$。
- en: 3.2 Value and Q- functions
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 价值函数和 Q-函数
- en: 'The value function is applied to evaluate how good it is for an agent to utilize
    policy $\pi$ to visit state $s$. The concept of ”good” is defined in terms of
    expected return, i.e. future rewards that can be expected to receive in the future
    and it depends on what actions it will take. Mathematically, the value is the
    expectation of return, and value approximation is obtained by Bellman expectation
    equation as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数用于评估代理利用策略 $\pi$ 访问状态 $s$ 的好坏。这里的“好”是通过期望回报来定义的，即可以预期在未来收到的奖励，它取决于将采取的行动。数学上，价值是回报的期望值，价值的近似通过
    Bellman 期望方程得到，如下所示：
- en: '|  | $\begin{split}V^{\pi}(s_{t})=\mathbb{E}[r_{t+1}+\gamma V^{\pi}(s_{t+1})]\end{split}$
    |  | (9) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}V^{\pi}(s_{t})=\mathbb{E}[r_{t+1}+\gamma V^{\pi}(s_{t+1})]\end{split}$
    |  | (9) |'
- en: '$V^{\pi}(s_{t})$ is also known as state-value function, and the expectation
    term can be expanded as a product of policy, transition probability, and return
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: $V^{\pi}(s_{t})$ 也称为状态价值函数，期望项可以展开为策略、转移概率和回报的乘积，如下所示：
- en: '|  | $\begin{split}V^{\pi}(s_{t})=\sum_{a_{t}\in\mathcal{A}}{\pi(a_{t}&#124;s_{t})}\sum_{s_{t+1}\in
    S}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1})]}\end{split}$
    |  | (10) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}V^{\pi}(s_{t})=\sum_{a_{t}\in\mathcal{A}}{\pi(a_{t}|s_{t})}\sum_{s_{t+1}\in
    S}{T(s_{t+1}|s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1})]}\end{split}$
    |  | (10) |'
- en: 'This equation is called the Bellman equation. When the agent always selects
    the action according to the optimal policy $\pi^{*}$ that maximizes the value,
    the Bellman equation can be expressed as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程被称为贝尔曼方程。当智能体总是按照最大化价值的最优策略 $\pi^{*}$ 选择动作时，贝尔曼方程可以表示如下：
- en: '|  | $\begin{split}V^{*}(s_{t})=&amp;\max_{a_{t}}\sum_{s_{t+1}\in S}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma
    V^{*}(s_{t+1})]}\\ \overset{\Delta}{=}&amp;\max_{a_{t}}Q^{*}(s_{t},a_{t})\end{split}$
    |  | (11) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}V^{*}(s_{t})=&\max_{a_{t}}\sum_{s_{t+1}\in S}{T(s_{t+1}|s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma
    V^{*}(s_{t+1})]}\\ \overset{\Delta}{=}&\max_{a_{t}}Q^{*}(s_{t},a_{t})\end{split}$
    |  | (11) |'
- en: 'However, obtaining optimal value function $V^{*}$ does not provide enough information
    to reconstruct some optimal policy $\pi^{*}$ because the real-world environment
    is complicated. Thus, a quality function (Q-function) is also called the action-value
    function under policy $\pi$. The Q-function is used to estimate how good it is
    for an agent to perform a particular action ($a_{t}$) in a state ($s_{t}$) with
    a policy $\pi$ and it is introduced as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，获取最优价值函数 $V^{*}$ 并不能提供足够的信息来重建某些最优策略 $\pi^{*}$，因为现实世界环境复杂。因此，质量函数（Q-函数）也称为策略
    $\pi$ 下的动作价值函数。Q-函数用于估计在策略 $\pi$ 下，智能体在某状态 ($s_{t}$) 执行动作 ($a_{t}$) 的好坏，它的引入如下：
- en: '|  | $Q^{\pi}(s_{t},a_{t})=\sum_{s_{t+1}}{T(s_{t+1}&#124;s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma
    V^{\pi}(s_{t+1})]}$ |  | (12) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{\pi}(s_{t},a_{t})=\sum_{s_{t+1}}{T(s_{t+1}|s_{t},a_{t})[R(s_{t},s_{t+1})+\gamma
    V^{\pi}(s_{t+1})]}$ |  | (12) |'
- en: Unlike value function which specifies the goodness of a state, a Q-function
    specifies the goodness of action in a state.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与指定状态优劣的价值函数不同，Q-函数指定了在某状态下动作的优劣。
- en: 3.3 Category
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 类别
- en: 'In general, RL can be divided into either model-free or model-based methods.
    Here, ”model” is defined by the two quantity: transition probability function
    $T(s_{t+1}|s_{t},a_{t})$ and the reward function $R(s_{t},s_{t+1})$.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，强化学习可以分为基于模型的方法或无模型的方法。在这里，“模型”由两个量定义：转移概率函数 $T(s_{t+1}|s_{t},a_{t})$ 和奖励函数
    $R(s_{t},s_{t+1})$。
- en: 3.3.1 Model-based RL
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 基于模型的强化学习
- en: 'Model-based RL is an approach that uses a learnt model, i.e. $T(s_{t+1}|s_{t},a_{t})$
    and reward function $R(s_{t},s_{t+1})$ to predict the future action. There are
    four main model-based techniques as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的强化学习是一种使用学习到的模型，即 $T(s_{t+1}|s_{t},a_{t})$ 和奖励函数 $R(s_{t},s_{t+1})$ 来预测未来动作的方法。主要有四种基于模型的技术如下：
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Value Function: The objective of value function methods is to obtain the best
    policy by maximizing the value functions in each state. A value function of a
    RL problem can be defined as in Eq.[10](#S3.E10 "In 3.2 Value and Q- functions
    ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey") and the optimal state-value function is given
    in Eq.[11](#S3.E11 "In 3.2 Value and Q- functions ‣ 3 Basics of Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    which are known as Bellman equations. Some common approaches in this group are
    Differential Dynamic Programming [[208](#bib.bib208)], [[266](#bib.bib266)], Temporal
    Difference Learning [[249](#bib.bib249)], Policy Iteration [[334](#bib.bib334)]
    and Monte Carlo [[137](#bib.bib137)].'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 价值函数：价值函数方法的目标是通过最大化每个状态下的价值函数来获得最佳策略。强化学习问题的价值函数可以定义为公式[10](#S3.E10 "在 3.2
    价值和 Q-函数 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：综合调查")，而最优的状态价值函数在公式[11](#S3.E11 "在 3.2 价值和
    Q-函数 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：综合调查")中给出，这些被称为贝尔曼方程。该组中的一些常见方法包括：微分动态规划 [[208](#bib.bib208)],
    [[266](#bib.bib266)], 时间差分学习 [[249](#bib.bib249)], 策略迭代 [[334](#bib.bib334)] 和蒙特卡洛
    [[137](#bib.bib137)]。
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transition Models: Transition models decide how to map from a state s, taking
    action a to the next state (s’) and it strongly affects the performance of model-based
    RL algorithms. Based on whether predicting the future state s’ is based on the
    probability distribution of a random variable or not, there are two main approaches
    in this group: stochastic and deterministic. Some common methods for deterministic
    models are decision trees [[280](#bib.bib280)] and linear regression [[265](#bib.bib265)].
    Some common methods for stochastic models are Gaussian processes [[71](#bib.bib71)],
    [[1](#bib.bib1)], [[12](#bib.bib12)], Expectation-Maximization [[59](#bib.bib59)]
    and dynamic Bayesian networks [[280](#bib.bib280)].'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移模型：转移模型决定了如何从状态s，采取动作a到下一个状态（s’），它对基于模型的强化学习算法的性能有着重要影响。根据预测未来状态s’是否基于随机变量的概率分布，这个组别主要有两种方法：随机和确定性。一些常见的确定性模型方法有决策树
    [[280](#bib.bib280)] 和线性回归 [[265](#bib.bib265)]。一些常见的随机模型方法有高斯过程 [[71](#bib.bib71)]、[[1](#bib.bib1)]、[[12](#bib.bib12)]、期望最大化
    [[59](#bib.bib59)] 和动态贝叶斯网络 [[280](#bib.bib280)]。
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Policy Search: Policy search approach directly searches for the optimal policy
    by modifying its parameters, whereas the value function methods indirectly find
    the actions that maximize the value function at each state. Some of the popular
    approaches in this group are: gradient-based [[87](#bib.bib87)], [[267](#bib.bib267)],
    information theory [[1](#bib.bib1)], [[189](#bib.bib189)] and sampling based [[21](#bib.bib21)].'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 策略搜索：策略搜索方法通过修改参数直接搜索最优策略，而价值函数方法则间接找到在每个状态下最大化价值函数的动作。该组别中的一些流行方法有：基于梯度的 [[87](#bib.bib87)]、[[267](#bib.bib267)]、信息理论
    [[1](#bib.bib1)]、[[189](#bib.bib189)] 和基于采样的 [[21](#bib.bib21)]。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Return Functions: Return functions decide how to aggregate rewards or punishments
    over an episode. They affect both the convergence and the feasibility of the model.
    There are two main approaches in this group: discounted returns functions [[21](#bib.bib21)],
    [[75](#bib.bib75)], [[393](#bib.bib393)] and averaged returns functions [[34](#bib.bib34)],
    [[3](#bib.bib3)]. Between the two approaches, the former is the most popular which
    represents the uncertainty about future rewards. While small discount factors
    provide faster convergence, its solution may not be optimal.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回报函数：回报函数决定了如何在一个回合中汇总奖励或惩罚。它们影响模型的收敛性和可行性。这个组别主要有两种方法：折扣回报函数 [[21](#bib.bib21)]、[[75](#bib.bib75)]、[[393](#bib.bib393)]
    和平均回报函数 [[34](#bib.bib34)]、[[3](#bib.bib3)]。在这两种方法中，前者更为流行，它表示对未来奖励的不确定性。虽然小的折扣因子提供了更快的收敛速度，但其解可能并不是最优的。
- en: 'In practice, transition and reward functions are rarely known and hard to model.
    The comparative performance among all model-based techniques is reported in [[385](#bib.bib385)]
    with over 18 benchmarking environments including noisy environments. The Fig.[6](#S3.F6
    "Figure 6 ‣ 3.3.2 Model-free methods ‣ 3.3 Category ‣ 3 Basics of Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    summarizes different model-based RL approaches.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '实际中，转移和奖励函数很少被知道且难以建模。所有基于模型的技术的比较性能已在 [[385](#bib.bib385)] 中报告，包括超过18个基准环境（包括噪声环境）。图[6](#S3.F6
    "Figure 6 ‣ 3.3.2 Model-free methods ‣ 3.3 Category ‣ 3 Basics of Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")
    总结了不同的基于模型的强化学习方法。'
- en: 3.3.2 Model-free methods
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 无模型方法
- en: Learning through the experience gained from interactions with the environment,
    i.e. model-free method tries to estimate the t. discrete problems transition probability
    function and the reward function from the experience to exploit them in acquisition
    of policy. Policy gradient and value-based algorithms are popularly used in model-free
    methods.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与环境互动获得的经验进行学习，即无模型方法试图从经验中估计离散问题的转移概率函数和奖励函数，以利用它们来获取策略。策略梯度和基于价值的算法在无模型方法中被广泛使用。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The policy gradient methods: In this approach, RL task is considered as optimization
    with stochastic first-order optimization. Policy gradient methods directly optimize
    the discounted expected reward, i.e. $\mathcal{G}(\pi)\rightarrow\max_{\pi}$ to
    obtains the optimal policy $\pi^{*}$ without any additional information about
    MDP. To do so, approximate estimations of the gradient with respect to policy
    parameters are used. Take [[392](#bib.bib392)] as an example, policy gradient
    parameterizes the policy and updates parameters $\theta$,'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 策略梯度方法：在这种方法中，强化学习任务被视为带有随机一阶优化的优化问题。策略梯度方法直接优化折扣期望回报，即 $\mathcal{G}(\pi)\rightarrow\max_{\pi}$
    以获得最优策略 $\pi^{*}$，而无需任何有关 MDP 的额外信息。为此，使用相对于策略参数的梯度的近似估计。以 [[392](#bib.bib392)]
    为例，策略梯度参数化策略并更新参数 $\theta$，
- en: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\mathcal{R}}$
    |  | (13) |'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\mathcal{R}}$
    |  | (13) |'
- en: 'where $\mathcal{R}$ is the total accumulated return and defined in Eq. [6](#S3.E6
    "In 3.1 Markov Decision Process ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). Common used policies are
    Gibbs policies [[20](#bib.bib20)], [[352](#bib.bib352)] and Gaussian policies
    [[294](#bib.bib294)]. Gibbs policies are used in discrete problems whereas Gaussian
    policies are used in continuous problems.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{R}$ 是总累计回报，如公式 [6](#S3.E6 "在 3.1 马尔可夫决策过程 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：全面调查")
    所定义。常用的策略有 Gibbs 策略 [[20](#bib.bib20)]、[[352](#bib.bib352)] 和高斯策略 [[294](#bib.bib294)]。Gibbs
    策略用于离散问题，而高斯策略用于连续问题。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Value-based methods: In this approach, the optimal policy $\pi^{*}$ is implicitly
    conducted by gaining an approximation of optimal Q-function $Q^{*}(s,a)$. In value-based
    methods, agents update the value function to learn suitable policy while policy-based
    RL agents learn the policy directly. To do that, Q-learning is a typical value-based
    method. The update rule of Q-learning with learning rate $\lambda$ is defined
    as:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于价值的方法：在这种方法中，通过获得最优 Q 函数 $Q^{*}(s,a)$ 的近似来隐式地实现最优策略 $\pi^{*}$。在基于价值的方法中，智能体更新价值函数以学习合适的策略，而基于策略的强化学习智能体直接学习策略。为此，Q
    学习是典型的基于价值的方法。Q 学习的更新规则与学习率 $\lambda$ 定义为：
- en: '|  | $Q(s_{t},a_{t})=Q(s_{t},a_{t})+\lambda\delta_{t}$ |  | (14) |'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})=Q(s_{t},a_{t})+\lambda\delta_{t}$ |  | (14) |'
- en: where $\delta_{t}=R(s_{t},s_{t+1})+\gamma\text{arg}\max_{a}{Q(s_{t+1},a)-Q(s_{t},a)}$
    is the temporal difference (TD) error.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\delta_{t}=R(s_{t},s_{t+1})+\gamma\text{arg}\max_{a}{Q(s_{t+1},a)-Q(s_{t},a)}$
    是时序差分（TD）误差。
- en: Target at self-play Chess, [[394](#bib.bib394)] investigates inasmuch it is
    possible to leverage the qualitative feedback for learning an evaluation function
    for the game. [[319](#bib.bib319)] provides the comparison of learning of linear
    evaluation functions between using preference learning and using least-squares
    temporal difference learning, from samples of game trajectories. The value-based
    methods depend on a specific, optimal policy, thus it is hard for transfer learning.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以自我博弈象棋为目标，[[394](#bib.bib394)] 研究了是否可以利用定性反馈来学习游戏的评估函数。[[319](#bib.bib319)]
    提供了使用偏好学习和使用最小二乘时序差分学习在游戏轨迹样本中学习线性评估函数的比较。基于价值的方法依赖于特定的最优策略，因此转移学习较为困难。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Actor-critic is an improvement of policy gradient with an value-based critic
    $\Gamma$, thus, Eq.[13](#S3.E13 "In 1st item ‣ 3.3.2 Model-free methods ‣ 3.3
    Category ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in
    Computer Vision: A Comprehensive Survey") is rewritten as:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Actor-critic 是一种改进的策略梯度方法，具有基于价值的评论员 $\Gamma$，因此，公式 [13](#S3.E13 "在第 1 项 ‣ 3.3.2
    无模型方法 ‣ 3.3 类别 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：全面调查") 被重写为：
- en: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\Gamma_{t}}$
    |  | (15) |'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{G}^{\theta}(\pi)=\mathbb{E}_{{\mathcal{T}_{\phi}}}\sum_{t=0}{log(\pi_{\theta}(a_{t}&#124;s_{t}))\gamma^{t}\Gamma_{t}}$
    |  | (15) |'
- en: The critic function $\Gamma$ can be defined as $Q^{\pi}(s_{t},a_{t})$ or $Q^{\pi}(s_{t},a_{t})-V^{\pi}_{t}$
    or $R[s_{t-1},s_{t}]+V^{\pi}_{t+1}-V^{\pi}_{t}$
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评论员函数 $\Gamma$ 可以定义为 $Q^{\pi}(s_{t},a_{t})$ 或 $Q^{\pi}(s_{t},a_{t})-V^{\pi}_{t}$
    或 $R[s_{t-1},s_{t}]+V^{\pi}_{t+1}-V^{\pi}_{t}$
- en: 'Actor-critic methods are combinations of actor-only methods and critic-only
    methods. Thus, actor-critic methods have been commonly used RL. Depend on reward
    setting, there are two groups of actor-critic methods, namely discounted return
    [[282](#bib.bib282)], [[30](#bib.bib30)] and average return [[289](#bib.bib289)],
    [[31](#bib.bib31)]. The comparison between model-based and model-free methods
    is given in Table [1](#S3.T1 "Table 1 ‣ 3.3.2 Model-free methods ‣ 3.3 Category
    ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic方法是仅有演员方法和仅有评论员方法的组合。因此，actor-critic方法通常用于RL。根据奖励设置，有两组actor-critic方法，即折扣回报[[282](#bib.bib282)]、[[30](#bib.bib30)]和平均回报[[289](#bib.bib289)]、[[31](#bib.bib31)]。基于模型和无模型方法的比较见表[1](#S3.T1
    "表 1 ‣ 3.3.2 无模型方法 ‣ 3.3 类别 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：全面综述")。
- en: '{forest}'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: '[Model-based RL [ Value Functions [ Differential Dynamic Programming [[208](#bib.bib208)],
    [[266](#bib.bib266)] ] [ Temporal Difference Learning [[249](#bib.bib249)] ] [
    Policy Iteration [[334](#bib.bib334)] ] [ Monte Carlo [[137](#bib.bib137)] ] ]
    [ Transition Models [ Deterministic models [ Decision trees [[280](#bib.bib280)]
    ] [ Linear regression [[265](#bib.bib265)] ] ] [ Stochastic models [ Gaussian
    processes [[71](#bib.bib71)], [[1](#bib.bib1)], [[12](#bib.bib12)] ] [ Expectation-Maximization
    [[59](#bib.bib59)] ] [ Dynamic Bayesian networks [[280](#bib.bib280)] ] ] ] [
    Policy Search [ Gradient-based [[87](#bib.bib87)], [[267](#bib.bib267)] ] [ Information
    theory [[1](#bib.bib1)], [[189](#bib.bib189)] ] [ Sampling based [[21](#bib.bib21)]
    ] ] [ Return Functions [ Discounted returns functions [[21](#bib.bib21)], [[75](#bib.bib75)],
    [[393](#bib.bib393)] ] [ Averaged returns functions [[34](#bib.bib34)], [[3](#bib.bib3)]
    ] ] ] ] ]'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[基于模型的RL [ 价值函数 [ 微分动态规划 [[208](#bib.bib208)], [[266](#bib.bib266)] ] [ 时序差分学习
    [[249](#bib.bib249)] ] [ 策略迭代 [[334](#bib.bib334)] ] [ 蒙特卡罗 [[137](#bib.bib137)]
    ] ] [ 转移模型 [ 确定性模型 [ 决策树 [[280](#bib.bib280)] ] [ 线性回归 [[265](#bib.bib265)] ]
    ] [ 随机模型 [ 高斯过程 [[71](#bib.bib71)], [[1](#bib.bib1)], [[12](#bib.bib12)] ] [ 期望最大化
    [[59](#bib.bib59)] ] [ 动态贝叶斯网络 [[280](#bib.bib280)] ] ] ] [ 策略搜索 [ 基于梯度 [[87](#bib.bib87)],
    [[267](#bib.bib267)] ] [ 信息理论 [[1](#bib.bib1)], [[189](#bib.bib189)] ] [ 基于采样
    [[21](#bib.bib21)] ] ] [ 回报函数 [ 折扣回报函数 [[21](#bib.bib21)], [[75](#bib.bib75)],
    [[393](#bib.bib393)] ] [ 平均回报函数 [[34](#bib.bib34)], [[3](#bib.bib3)] ] ] ] ] ]'
- en: 'Figure 6: Summarization of model-based RL approaches'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于模型的RL方法总结
- en: 'Table 1: Comparison between model-based RL and model-free RL'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：基于模型的RL和无模型RL的比较
- en: '| Factors | Model-based RL | Model-free RL |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 因素 | 基于模型的RL | 无模型RL |'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number of iterations between &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 迭代次数之间 &#124;'
- en: '&#124; agent and environment &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代理和环境 &#124;'
- en: '| Small | Big |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 小 | 大 |'
- en: '| Convergence | Fast | Slow |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 收敛性 | 快 | 慢 |'
- en: '| Prior knowledge of transitions | Yes | No |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 过渡的先验知识 | 是 | 否 |'
- en: '| Flexibility |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 灵活性 |'
- en: '&#124; Strongly depend on &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 强烈依赖于 &#124;'
- en: '&#124; a learnt model &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个学习的模型 &#124;'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Adjust based &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于调整 &#124;'
- en: '&#124; on trials and errors &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于试验和错误 &#124;'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 4 Introduction to Deep Reinforcement Learning
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度强化学习简介
- en: DRL, which was proposed as a combination of RL and DL, has achieved rapid developments,
    thanks to the rich context representation of DL. Under DRL, the aforementioned
    value and policy can be expressed by neural networks which allow dealing with
    a continuous state or action that was hard for a table representation. Similar
    to RL, DRL can be categorized into model-based algorithms and model-free algorithms
    which will be introduced in this section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: DRL作为RL和DL的结合体得到了迅速的发展，这得益于DL的丰富上下文表示。在DRL中，前述的价值和策略可以通过神经网络表达，从而处理以前难以用表格表示的连续状态或动作。类似于RL，DRL可以分为基于模型的算法和无模型算法，本节将介绍这两类算法。
- en: 4.1 Model-Free Algorithms
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 无模型算法
- en: There are two approaches, namely, Value-based DRL methods and Policy gradient
    DRL methods to implement model-free algorithms.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法，即基于价值的DRL方法和策略梯度DRL方法，用于实现无模型算法。
- en: 4.1.1 Value-based DRL methods
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 基于价值的DRL方法
- en: 'Deep Q-Learning Network (DQN): Deep Q-learning [[264](#bib.bib264)] (DQN) is
    the most famous DRL model which learns policies directly from high-dimensional
    inputs by CNNs. In DQN, input is raw pixels and output is a quality function to
    estimate future rewards as given in Fig.[7](#S4.F7 "Figure 7 ‣ 4.1.1 Value-based
    DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    Take regression problem as an instance. Let $y$ denote the target of our regression
    task, the regression with input $(s,a)$, target $y(s,a)$ and the MSE loss function
    is as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '深度 Q 学习网络 (DQN)：深度 Q 学习[[264](#bib.bib264)](DQN)是最著名的深度强化学习模型，它通过 CNN 直接从高维输入中学习策略。在
    DQN 中，输入是原始像素，输出是一个质量函数，用于估计未来的奖励，如图[7](#S4.F7 "Figure 7 ‣ 4.1.1 Value-based DRL
    methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement Learning
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey")中所示。以回归问题为例。让
    $y$ 表示我们回归任务的目标，对于输入 $(s,a)$ 的回归，目标 $y(s,a)$ 和均方误差损失函数为：'
- en: '|  | <math   alttext="\begin{split}\mathcal{L^{DQN}}&amp;=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &amp;=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\mathcal{L^{DQN}}&amp;=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &amp;=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\ '
- en: y(s_{t},a_{t})&amp;=R(s_{t},s_{t+1})+\gamma\max_{a_{t+1}}Q^{*}(s_{t_{1}},a_{t+1},\theta_{t})\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="right"  ><msup ><mi >ℒ</mi><mrow ><mi >𝒟</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >𝒬</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >𝒩</mi></mrow></msup></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mrow ><mi >ℒ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi  >y</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo><mrow ><msup ><mi  >Q</mi><mo >∗</mo></msup><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >θ</mi><mi >t</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><msup ><mrow ><mo stretchy="false"
    >‖</mo><mrow ><mrow ><mi  >y</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >−</mo><mrow ><msup ><mi >Q</mi><mo >∗</mo></msup><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi >t</mi></msub><mo >,</mo><msub
    ><mi  >a</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >θ</mi><mi >t</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >2</mn></msup></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >y</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >a</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mrow ><mrow  ><mi >R</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mi
    >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo
    stretchy="false" >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >γ</mi><mo lspace="0.167em"
    rspace="0em" >​</mo><mrow ><munder ><mi >max</mi><msub ><mi >a</mi><mrow ><mi
    >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"  >⁡</mo><msup
    ><mi >Q</mi><mo >∗</mo></msup></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><msub ><mi >t</mi><mn >1</mn></msub></msub><mo
    >,</mo><msub ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo
    >,</mo><msub ><mi  >θ</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℒ</ci><apply ><ci  >𝒟</ci><ci >𝒬</ci><ci >𝒩</ci></apply></apply><apply ><ci >ℒ</ci><interval
    closure="open" ><apply  ><ci >𝑦</ci><interval closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci  >𝑡</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply></interval></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑄</ci></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑡</ci></apply></vector></apply></interval></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="latexml" >norm</csymbol><apply ><apply ><ci  >𝑦</ci><interval closure="open"  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply></interval></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑄</ci></apply><vector
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑡</ci></apply></vector></apply></apply></apply><cn
    type="integer"  >2</cn></apply><ci >𝑦</ci><interval closure="open"  ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><ci >𝑡</ci></apply></interval></apply></apply><apply
    ><apply ><apply  ><ci >𝑅</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >𝛾</ci><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑄</ci></apply></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><ci >𝑡</ci></apply></vector></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{L^{DQN}}&=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &=&#124;&#124;y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})&#124;&#124;^{2}\\ y(s_{t},a_{t})&=R(s_{t},s_{t+1})+\gamma\max_{a_{t+1}}Q^{*}(s_{t_{1}},a_{t+1},\theta_{t})\end{split}</annotation></semantics></math>
    |  | (16) |
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L^{DQN}}$=\mathcal{L}(y(s_{t},a_{t}),Q^{*}(s_{t},a_{t},\theta_{t}))\\
    &= ||y(s_{t},a_{t})-Q^{*}(s_{t},a_{t},\theta_{t})||^{2}\\ y(s_{t},a_{t})=R(s_{t},s_{t+1})+\gamma\max_{a_{t+1}}Q^{*}(s_{t_{1}},a_{t+1},\theta_{t})$
    |  | (16) |
- en: Where $\theta$ is vector of parameters, $\theta\in\mathbb{R}^{|S||R|}$ and $s_{t+1}$
    is a sample from $T(s_{t+1}|s_{t},a_{t})$ with input of $(s_{t},a_{t})$.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta$是参数向量，$\theta\in\mathbb{R}^{|S||R|}$，$s_{t+1}$是从$T(s_{t+1}|s_{t},a_{t})$中抽样得到的，以$(s_{t},a_{t})$为输入。
- en: 'Minimizing the loss function yields a gradient descent step formula to update
    $\theta$ as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化损失函数得到一个梯度下降步骤公式，以更新$\theta$如下：
- en: '|  | $\theta_{t+1}=\theta_{t}-\alpha_{t}\frac{\mathcal{\partial L^{DQN}}}{\partial\theta}$
    |  | (17) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{t+1}=\theta_{t}-\alpha_{t}\frac{\mathcal{\partial L^{DQN}}}{\partial\theta}$
    |  | (17) |'
- en: '![Refer to caption](img/72d78e811fd5b99100e03f51dd288fd9.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/72d78e811fd5b99100e03f51dd288fd9.png)'
- en: 'Figure 7: Network structure of Deep Q-Network (DQN), where Q-values Q(s,a)
    are generated for all actions for a given state.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：深度Q网络（DQN）的网络结构，其中Q值Q(s,a)为给定状态生成所有动作的值。
- en: 'Double DQN: In DQN, the values of $Q^{*}$ in many domains were leading to overestimation
    because of $max$. In Eq.[16](#S4.E16 "In 4.1.1 Value-based DRL methods ‣ 4.1 Model-Free
    Algorithms ‣ 4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"), $y(s,a)=R(s,s^{\prime})+\gamma\max_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta)$
    shifts Q-value estimation towards either to the actions with high reward or to
    the actions with overestimating approximation error. Double DQN [[370](#bib.bib370)]
    is an improvement of DQN that combines double Q-learning [[130](#bib.bib130)]
    with DQN and it aims at reducing observed overestimation with better performance.
    The idea of Double DQN is based on separating action selection and action evaluation
    using its own approximation of $Q^{*}$ as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 双重DQN：在DQN中，许多领域的$Q^{*}$值由于$max$的存在而导致过高估计。在公式[16](#S4.E16 "在4.1.1值基于DRL方法 ‣
    4.1无模型算法 ‣ 4 深度强化学习简介 ‣ 深度强化学习在计算机视觉中的综合调查")中，$y(s,a)=R(s,s^{\prime})+\gamma\max_{a^{\prime}}Q^{*}(s^{\prime},a^{\prime},\theta)$使Q值估计偏向于高奖励的动作或过高估计的近似误差的动作。双重DQN
    [[370](#bib.bib370)] 是DQN的改进，结合了双重Q学习 [[130](#bib.bib130)] 和DQN，旨在通过更好的性能减少观察到的过高估计。双重DQN的思想是基于使用其自己的$Q^{*}$近似来分离动作选择和动作评估，如下：
- en: '|  | $\begin{split}\max_{a_{t+1}}Q^{*}(s_{t+1},a_{t+1};\theta)=Q^{*}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}$
    |  | (18) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\max_{a_{t+1}}Q^{*}(s_{t+1},a_{t+1};\theta)=Q^{*}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}$
    |  | (18) |'
- en: Thus
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '|  | $y=R(s_{t},s_{t+1})+\gamma Q^{*}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})$
    |  | (19) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=R(s_{t},s_{t+1})+\gamma Q^{*}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})$
    |  | (19) |'
- en: 'The easiest and most expensive implementation of double DQN is to run two independent
    DQNs as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 实现双重DQN的最简单且最昂贵的方法是运行两个独立的DQN，如下：
- en: '|  | <math   alttext="\begin{split}y_{1}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{1}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{2}(s_{t+1},a_{t+1};\theta_{2});\theta_{1})\\'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}y_{1}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{1}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{2}(s_{t+1},a_{t+1};\theta_{2});\theta_{1})\\'
- en: y_{2}=R(s_{t},s_{t+1})+\\
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: y_{2}=R(s_{t},s_{t+1})+\\
- en: \gamma Q^{*}_{2}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{1}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><msub  ><mi >y</mi><mn >1</mn></msub><mo >=</mo><mrow
    ><mrow  ><mi >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi  >s</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >γ</mi><mo
    lspace="0em" rspace="0em" >​</mo><msubsup ><mi  >Q</mi><mn >1</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><munder accentunder="true"
    ><mrow  ><mi >arg</mi><mo lspace="0.167em"  >⁡</mo><mi >max</mi></mrow><msub ><mi
    >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"
    rspace="0em"  >​</mo><msubsup ><mi >Q</mi><mn >2</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi
    >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >;</mo><msub
    ><mi  >θ</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >;</mo><msub ><mi  >θ</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msub ><mi  >y</mi><mn >2</mn></msub><mo >=</mo><mrow
    ><mrow ><mi  >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi  >s</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >γ</mi><mo
    lspace="0em" rspace="0em" >​</mo><msubsup ><mi  >Q</mi><mn >2</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><munder accentunder="true"
    ><mrow  ><mi >arg</mi><mo lspace="0.167em"  >⁡</mo><mi >max</mi></mrow><msub ><mi
    >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"
    rspace="0em"  >​</mo><msubsup ><mi >Q</mi><mn >1</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi
    >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >;</mo><msub
    ><mi  >θ</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >;</mo><msub ><mi  >θ</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑦</ci><cn type="integer" >1</cn></apply><apply ><apply ><ci  >𝑅</ci><interval
    closure="open"  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><apply
    ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >𝛾</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝑄</ci></apply><cn type="integer" >1</cn></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><apply ><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑄</ci></apply><cn type="integer"  >2</cn></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><cn type="integer" >2</cn></apply></vector></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜃</ci><cn type="integer" >1</cn></apply></vector><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑦</ci><cn type="integer" >2</cn></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci >𝑅</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >𝛾</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝑄</ci></apply><cn type="integer" >2</cn></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><apply ><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑄</ci></apply><cn type="integer"  >1</cn></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><cn type="integer" >1</cn></apply></vector></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜃</ci><cn type="integer" >2</cn></apply></vector></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}y_{1}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{1}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{2}(s_{t+1},a_{t+1};\theta_{2});\theta_{1})\\
    y_{2}=R(s_{t},s_{t+1})+\\ \gamma Q^{*}_{2}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{1}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}</annotation></semantics></math>
    |  | (20) |
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: \(\gamma Q^{*}_{2}(s_{t+1},\underset{a_{t+1}}{\arg\max}Q^{*}_{1}(s_{t+1},a_{t+1};\theta_{1});\theta_{2})\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><msub  ><mi >y</mi><mn >1</mn></msub><mo >=</mo><mrow
    ><mrow  ><mi >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi  >s</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >γ</mi><mo
    lspace="0em" rspace="0em" >​</mo><msubsup ><mi  >Q</mi><mn >1</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><munder accentunder="true"
    ><mrow  ><mi >arg</mi><mo lspace="0.167em"  >⁡</mo><mi >max</mi></mrow><msub ><mi
    >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"
    rspace="0em"  >​</mo><msubsup ><mi >Q</mi><mn >2</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi
    >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >;</mo><msub
    ><mi  >θ</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >;</mo><msub ><mi  >θ</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msub ><mi  >y</mi><mn >2</mn></msub><mo >=</mo><mrow
    ><mrow ><mi  >R</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi  >s</mi><mi >t</mi></msub><mo >,</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow ><mi >γ</mi><mo
    lspace="0em" rspace="0em" >​</mo><msubsup ><mi  >Q</mi><mn >2</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><munder accentunder="true"
    ><mrow  ><mi >arg</mi><mo lspace="0.167em"  >⁡</mo><mi >max</mi></mrow><msub ><mi
    >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub></munder><mo lspace="0.167em"
    rspace="0em"  >​</mo><msubsup ><mi >Q</mi><mn >1</mn><mo >∗</mo></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><msub ><mi
    >s</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><msub
    ><mi  >a</mi><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >;</mo><msub
    ><mi  >θ</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >;</mo><msub ><mi  >θ</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑦</ci><cn type="integer" >1</cn></apply><apply ><apply ><ci  >𝑅</ci><interval
    closure="open"  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci
    >𝑡</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><apply
    ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></interval></apply><apply
    ><ci >𝛾</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝑄</ci></apply><cn type="integer" >1</cn></apply><vector
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><apply ><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><apply  ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑄</ci></apply><cn type="integer"  >2</cn></apply><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑎</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><cn type="integer" >2</cn></apply></vector></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜃</ci><cn type="integer" >1</cn></apply></vector><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑦</ci><cn type="integer" >2</cn></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci >𝑅</ci><interval closure="open"  ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><ci >𝑡</ci></apply><apply ><csymbol cd="ambiguous"
    >
- en: 'Dueling DQN: In DQN, when the agent visits an unfavorable state, instead of
    lowering its value $V^{*}$, it remembers only low pay-off by updating $Q^{*}$.
    In order to address this limitation, Dueling DQN [[390](#bib.bib390)] incorporates
    approximation of $V^{*}$ explicitly in a computational graph by introducing an
    advantage function as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对偶DQN：在DQN中，当代理访问一个不利状态时，不是降低其值$V^{*}$，而是通过更新$Q^{*}$只记住低回报。为了应对这一限制，对偶DQN[[390](#bib.bib390)]通过在计算图中显式地引入优势函数来近似$V^{*}$，如下所示：
- en: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (21) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (21) |'
- en: 'Therefore, we can reformulate Q-value: $Q^{*}(s,a)=A^{*}(s,a)+V^{*}(s)$. This
    implies that after DL the feature map is decomposed into two parts corresponding
    to $V^{*}(v)$ and $A^{*}(s,a)$ as illustrated in Fig.[8](#S4.F8 "Figure 8 ‣ 4.1.1
    Value-based DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    This can be implemented by splitting the fully connected layers in the DQN architecture
    to compute the advantage and state value functions separately, then combining
    them back into a single Q-function. An interesting result has shown that Dueling
    DQN obtains better performance if it is formulated as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以重新表述Q值：$Q^{*}(s,a)=A^{*}(s,a)+V^{*}(s)$。这意味着在深度学习后，特征图被分解为两个部分，分别对应$V^{*}(v)$和$A^{*}(s,a)$，如图[8](#S4.F8
    "图 8 ‣ 4.1.1 基于价值的深度强化学习方法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介 ‣ 深度强化学习在计算机视觉中的应用：综合调查")所示。这可以通过将DQN架构中的全连接层分开来计算优势函数和状态值函数，然后将它们组合回一个单一的Q函数来实现。有趣的结果表明，如果将对偶DQN表述为：
- en: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\max_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  | (22) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\max_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  | (22) |'
- en: In practical implementation, averaging instead of maximum is used, i.e.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现中，使用的是平均而不是最大值，即
- en: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\text{mean}_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s_{t},a_{t})=V^{*}(s_{t})+A^{*}(s_{t},a_{t})-\text{mean}_{a_{t+1}}A^{*}(s_{t},a_{t+1})$
    |  |'
- en: Furthermore, to address the limitation of memory and imperfect information at
    each decision point, Deep Recurrent Q-Network (DRQN) [[131](#bib.bib131)] employed
    RNNs into DQN by replacing the first fully-connected layer with an RNN. Multi-step
    DQN [[68](#bib.bib68)] is one of the most popular improvements of DQN by substituting
    one-step approximation with N-steps.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了应对在每个决策点的记忆限制和不完全信息，深度递归Q网络（DRQN）[[131](#bib.bib131)]通过用RNN替换第一个全连接层将RNN引入DQN。多步DQN
    [[68](#bib.bib68)]是DQN最受欢迎的改进之一，通过用N步替代一步近似。
- en: '![Refer to caption](img/e873b6469a95ee4e6160354d4267e4e2.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/e873b6469a95ee4e6160354d4267e4e2.png)'
- en: 'Figure 8: Network structure of Dueling DQN, where value function $V(s)$ and
    advantage function $A(s,a)$ are combined to predict Q-values $Q(s,a)$ for all
    actions for a given state.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：对偶DQN的网络结构，其中价值函数$V(s)$和优势函数$A(s,a)$被组合以预测给定状态下所有动作的Q值$Q(s,a)$。
- en: 4.1.2 Policy gradient DRL methods
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 策策渐进深度强化学习方法
- en: 'Policy Gradient Theorem: Different from value-based DRL methods, policy gradient
    DRL optimizes the policy directly by optimizing the following objective function
    which is defined as a function of $\theta$.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 策策渐进定理：与基于价值的深度强化学习方法不同，策略梯度深度强化学习通过优化以下目标函数直接优化策略，该函数被定义为$\theta$的函数。
- en: '|  | $\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=1}{\gamma^{t-1}R(s_{t-1},s_{t})}\rightarrow\max_{\theta}$
    |  | (23) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=1}{\gamma^{t-1}R(s_{t-1},s_{t})}\rightarrow\max_{\theta}$
    |  | (23) |'
- en: 'For any MDP and differentiable policy $\pi_{\theta}$, the gradient of objective
    Eq.[23](#S4.E23 "In 4.1.2 Policy gradient DRL methods ‣ 4.1 Model-Free Algorithms
    ‣ 4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey") is defined by policy gradient theorem
    [[353](#bib.bib353)] as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何MDP和可微分策略$\pi_{\theta}$，目标方程 Eq.[23](#S4.E23 "在4.1.2 策策渐进深度强化学习方法 ‣ 4.1
    无模型算法 ‣ 4 深度强化学习简介 ‣ 深度强化学习在计算机视觉中的应用：综合调查")的梯度由策略梯度定理[[353](#bib.bib353)]定义如下：
- en: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=0}{\gamma^{t}Q^{\pi}(s_{t},a_{t})\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})}$
    |  | (24) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)=\mathbb{E}_{\mathcal{T}\sim\pi_{\theta}}\sum_{t=0}{\gamma^{t}Q^{\pi}(s_{t},a_{t})\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}|s_{t})}$
    |  | (24) |'
- en: 'REINFORCE: REINFORCE was introduced by [[392](#bib.bib392)] to approximately
    calculate the gradient in Eq.[24](#S4.E24 "In 4.1.2 Policy gradient DRL methods
    ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement Learning ‣
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") by using
    Monte-Carlo estimation. In REINFORCE approximate estimator, Eq.[24](#S4.E24 "In
    4.1.2 Policy gradient DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey") is reformulated as:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'REINFORCE: REINFORCE 是由 [[392](#bib.bib392)] 引入的，用于通过蒙特卡洛估计近似计算 Eq.[24](#S4.E24
    "在 4.1.2 策略梯度 DRL 方法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介 ‣ 计算机视觉中的深度强化学习：全面综述") 中的梯度。在 REINFORCE
    近似估计器中，Eq.[24](#S4.E24 "在 4.1.2 策略梯度 DRL 方法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介 ‣ 计算机视觉中的深度强化学习：全面综述")
    被重新表述为：'
- en: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)\approx\sum_{\mathcal{T}}^{N}\sum_{t=0}{\gamma^{t}\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})(\sum_{t^{\prime}=t}{\gamma^{t^{\prime}-t}R(s_{t^{\prime}},s_{t^{\prime}+1})})}$
    |  | (25) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}\mathcal{G}(\theta)\approx\sum_{\mathcal{T}}^{N}\sum_{t=0}{\gamma^{t}\bigtriangledown_{\theta}\text{log}\pi_{\theta}(a_{t}&#124;s_{t})(\sum_{t^{\prime}=t}{\gamma^{t^{\prime}-t}R(s_{t^{\prime}},s_{t^{\prime}+1})})}$
    |  | (25) |'
- en: 'where $\mathcal{T}$ is trajectory distribution and defined in Eq.[5](#S3.E5
    "In 3.1 Markov Decision Process ‣ 3 Basics of Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). Theoretically, REINFORCE
    can be straightforwardly applied into any parametric $\pi_{theta}(a|s)$. However,
    it is impractical to use because of its time-consuming nature for convergence
    and local optimums problem. Based on the observation that the convergence rate
    of stochastic gradient descent directly depends on the variance of gradient estimation,
    the variance reduction technique was proposed to address naive REINFORCE’s limitations
    by adding a term that reduces the variance without affecting the expectation.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{T}$ 是轨迹分布，在 Eq.[5](#S3.E5 "在 3.1 马尔可夫决策过程 ‣ 3 强化学习基础 ‣ 计算机视觉中的深度强化学习：全面综述")
    中定义。从理论上讲，REINFORCE 可以直接应用于任何参数化的 $\pi_{\theta}(a|s)$。然而，由于收敛的时间消耗和局部最优问题，它在实际使用中并不实用。基于随机梯度下降的收敛速度直接依赖于梯度估计的方差的观察，提出了方差减少技术来解决原始
    REINFORCE 的局限性，通过添加一个项来减少方差而不影响期望值。
- en: 4.1.3 Actor-Critic DRL algorithm
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 Actor-Critic DRL 算法
- en: 'Both value-based and policy gradient algorithms have their own pros and cons,
    i.e. policy gradient methods are better for continuous and stochastic environments,
    and have a faster convergence whereas, value-based methods are more sample efficient
    and steady. Lately, actor-critic [[182](#bib.bib182)] [[262](#bib.bib262)] was
    born to take advantage from both value-based and policy gradient while limiting
    their drawbacks. Actor-critic architecture computes the policy gradient using
    a value-based critic function to estimate expected future reward. The principal
    idea of actor-critic is to divide the model into two parts: (i) computing an action
    based on a state and (ii) producing the Q values of the action. As given in Fig.[9](#S4.F9
    "Figure 9 ‣ 4.1.3 Actor-Critic DRL algorithm ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey"), the actor takes as input the state $s_{t}$ and outputs
    the best action $a_{t}$. It essentially controls how the agent behaves by learning
    the optimal policy (policy-based). The critic, on the other hand, evaluates the
    action by computing the value function (value-based). The most basic actor-critic
    method (beyond the tabular case) is naive policy gradients (REINFORCE). The relationship
    between actor-critic is similar to kid-mom. The kid (actor) explores the environment
    around him/her with new actions i.e. tough fire, hit a wall, climb a tree, etc
    while the mom (critic) watches the kid and criticizes/compliments him/her. The
    kid then adjusts his/her behavior based on what his/her mom told. When the kids
    get older, he/she can realize which action is bad/good.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的方法和策略梯度算法各有优缺点，即策略梯度方法在连续和随机环境中表现更好，且收敛速度更快，而基于价值的方法则在样本效率和稳定性上更为出色。最近，演员-评论家算法
    [[182](#bib.bib182)] [[262](#bib.bib262)] 的出现融合了基于价值和策略梯度方法的优点，同时限制了它们的缺点。演员-评论家架构通过使用基于价值的评论家函数来估计期望的未来奖励，从而计算策略梯度。演员-评论家的主要思想是将模型分为两个部分：（i）基于状态计算动作和（ii）生成动作的
    Q 值。如图 [9](#S4.F9 "图 9 ‣ 4.1.3 演员-评论家 DRL 算法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介 ‣ 计算机视觉中的深度强化学习：全面综述")
    所示，演员以状态 $s_{t}$ 为输入，输出最佳动作 $a_{t}$。它本质上通过学习最优策略（基于策略）来控制代理的行为。而评论家则通过计算价值函数（基于价值）来评估动作。最基本的演员-评论家方法（超出表格情况）是朴素策略梯度（REINFORCE）。演员-评论家的关系类似于孩子和母亲。孩子（演员）通过新的动作探索周围环境，即经历困难的火灾、撞墙、爬树等，而母亲（评论家）则观察孩子并对其进行批评或赞扬。孩子随后根据母亲的反馈调整自己的行为。当孩子长大后，他/她能够意识到哪些动作是好或坏的。
- en: '![Refer to caption](img/8fdb0e2aceef831039655bd509b14e01.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8fdb0e2aceef831039655bd509b14e01.png)'
- en: 'Figure 9: Flowchart showing the structure of actor critic algorithm.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：显示演员-评论家算法结构的流程图。
- en: '![Refer to caption](img/13da43e1b6315b61af979883bf93af15.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/13da43e1b6315b61af979883bf93af15.png)'
- en: 'Figure 10: An illustration of Actor-Critic algorithm in two cases: sharing
    parameters (a) and not sharing parameters (b).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：演员-评论家算法在两种情况下的示意图：共享参数 (a) 和不共享参数 (b)。
- en: 'Advantage Actor-Critic (A2C) Advantage Actor-Critic (A2C) [[263](#bib.bib263)]
    consist of two neural networks i.e. actor network $\pi_{\theta}(a_{t}|s_{t})$
    representing for policy and critic network $V^{\pi}_{\omega}$ with parameters
    $\omega$ approximately estimating actor’s performance. In order to determine how
    much better, it is to take a specific action compared to the average, an advantage
    value is defined as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 优势演员-评论家（A2C）优势演员-评论家（A2C） [[263](#bib.bib263)] 包含两个神经网络，即代表策略的演员网络 $\pi_{\theta}(a_{t}|s_{t})$
    和带有参数 $\omega$ 的评论家网络 $V^{\pi}_{\omega}$，后者大致估计演员的表现。为了确定采取特定动作相对于平均水平有多大优势，定义了优势值为：
- en: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (26) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$ |  | (26) |'
- en: 'Instead of constructing two neural networks for both the Q value and the V
    value, using the Bellman optimization equation, we can rewrite the advantage function
    as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与其为 Q 值和 V 值分别构建两个神经网络，不如使用贝尔曼优化方程，我们可以将优势函数重写为：
- en: '|  | $A^{\pi}(s_{t},a_{t})=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})-V^{\pi}_{\omega}(s_{t})$
    |  | (27) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\pi}(s_{t},a_{t})=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})-V^{\pi}_{\omega}(s_{t})$
    |  | (27) |'
- en: 'For given policy $\pi$, its value function can be obtained using point iteration
    for solving:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的策略 $\pi$，其价值函数可以通过点迭代来求解：
- en: '|  | $V^{\pi}(s_{t})=\mathbb{E}_{a_{t}\sim\pi(a_{t}&#124;s_{t})}\mathbb{E}_{s_{t+1}\sim
    T(s_{t+1}&#124;a_{t},s_{t})}(R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1}))$ |  | (28)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $V^{\pi}(s_{t})=\mathbb{E}_{a_{t}\sim\pi(a_{t}&#124;s_{t})}\mathbb{E}_{s_{t+1}\sim
    T(s_{t+1}&#124;a_{t},s_{t})}(R(s_{t},s_{t+1})+\gamma V^{\pi}(s_{t+1}))$ |  | (28)
    |'
- en: 'Similar to DQN, on each update a target is computed using current approximation:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 DQN，在每次更新时，使用当前近似值计算目标：
- en: '|  | $y=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})$ |  | (29) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=R(s_{t},s_{t+1})+\gamma V^{\pi}_{\omega}(s_{t+1})$ |  | (29) |'
- en: 'At time step t, the A2C algorithm can be implemented as following steps:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 t，A2C 算法可以按以下步骤实施：
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 1: Compute advantage function using Eq.[27](#S4.E27 "In 4.1.3 Actor-Critic
    DRL algorithm ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一步：使用 Eq.[27](#S4.E27 "在 4.1.3 Actor-Critic DRL 算法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介
    ‣ 深度强化学习在计算机视觉中的应用：综合调查") 计算优势函数。
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 2: Compute target using Eq.[29](#S4.E29 "In 4.1.3 Actor-Critic DRL algorithm
    ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement Learning ‣
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二步：使用 Eq.[29](#S4.E29 "在 4.1.3 Actor-Critic DRL 算法 ‣ 4.1 无模型算法 ‣ 4 深度强化学习简介
    ‣ 深度强化学习在计算机视觉中的应用：综合调查") 计算目标。
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 3: Compute critic loss with MSE loss: $\mathcal{L}=\frac{1}{B}\sum_{T}||y-V^{\pi}(s_{t}))||^{2}$,
    where $B$ is batch size and $V^{\pi}(s_{t})$ is defined in Eq.[28](#S4.E28 "In
    4.1.3 Actor-Critic DRL algorithm ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey").'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三步：用 MSE 损失计算评论家损失：$\mathcal{L}=\frac{1}{B}\sum_{T}||y-V^{\pi}(s_{t}))||^{2}$，其中
    $B$ 是批量大小，$V^{\pi}(s_{t})$ 在 Eq.[28](#S4.E28 "在 4.1.3 Actor-Critic DRL 算法 ‣ 4.1
    无模型算法 ‣ 4 深度强化学习简介 ‣ 深度强化学习在计算机视觉中的应用：综合调查") 中定义。
- en: •
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 4: Compute critic gradient: $\bigtriangledown^{critic}=\frac{\partial\mathcal{L}}{\partial\omega}$.'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第四步：计算评论家梯度：$\bigtriangledown^{critic}=\frac{\partial\mathcal{L}}{\partial\omega}$。
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 5: Compute actor gradient: $\bigtriangledown^{actor}=\frac{1}{B}\sum_{T}{\bigtriangledown_{\theta}\text{log}\pi(a_{t}|s_{t})A^{\pi}(s_{t},a_{t})}$'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第五步：计算演员梯度：$\bigtriangledown^{actor}=\frac{1}{B}\sum_{T}{\bigtriangledown_{\theta}\text{log}\pi(a_{t}|s_{t})A^{\pi}(s_{t},a_{t})}$
- en: 'Asynchronous Advantage Actor Critic (A3C) Besides A2C, there is another strategy
    to implement an Actor-Critic agent. Asynchronous Advantage Actor-Critic (A3C)
    [[263](#bib.bib263)] approach does not use experience replay because this requires
    a lot of memory. Instead, A3C asynchronously executes different agents in parallel
    on multiple instances of the environment. Each worker (copy of the network) will
    update the global network asynchronously. Because of the asynchronous nature of
    A3C, some workers (copy of the agents) will work with older values of the parameters.
    Thus the aggregating update will not be optimal. On the other hand, A2C synchronously
    updates the global network. A2C waits until all workers finished their training
    and calculated their gradients to average them, to update the global network.
    In order to update the entire network, A2C waits for each actor to finish their
    segment of experience before updating the global parameters. As a consequence,
    the training will be more cohesive and faster. Different from A3C, each worker
    in A2C has the same set of weights since and A2C updates all their workers at
    the same time. In short, A2C is an alternative to the synchronous version of the
    A3C. In A2C, it waits for each actor to finish its segment of experience before
    updating, averaging over all of the actors. In a practical experiment, this implementation
    is more effectively uses GPUs due to larger batch sizes. The structure of an actor-critic
    algorithm can be divided into two types depending on parameter sharing as illustrated
    in Fig.[10](#S4.F10 "Figure 10 ‣ 4.1.3 Actor-Critic DRL algorithm ‣ 4.1 Model-Free
    Algorithms ‣ 4 Introduction to Deep Reinforcement Learning ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey").'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '异步优势演员-评论家（A3C）除了 A2C，还有另一种策略来实现一个演员-评论家代理。异步优势演员-评论家（A3C）[[263](#bib.bib263)]
    方法不使用经验回放，因为这需要大量的内存。相反，A3C 异步地在多个环境实例上并行执行不同的代理。每个工作者（网络的副本）将异步更新全局网络。由于 A3C
    的异步特性，一些工作者（代理的副本）将使用旧的参数值。因此，聚合更新将不是最优的。另一方面，A2C 同步更新全局网络。A2C 等待所有工作者完成他们的训练并计算其梯度以进行平均，然后更新全局网络。为了更新整个网络，A2C
    等待每个演员完成其经验段后再更新全局参数。因此，训练将更具凝聚力且更快。与 A3C 不同的是，A2C 中的每个工作者都有相同的权重，因为 A2C 同时更新所有工作者。简而言之，A2C
    是 A3C 同步版本的替代方案。在 A2C 中，它等待每个演员完成其经验段后进行更新，对所有演员进行平均。在实际实验中，由于批量大小较大，这种实现更有效地使用了
    GPU。演员-评论家算法的结构可以根据参数共享分为两种类型，如图[10](#S4.F10 "Figure 10 ‣ 4.1.3 Actor-Critic DRL
    algorithm ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement Learning
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") 所示。'
- en: In order to overcome the limitation of speed, GA3C [[16](#bib.bib16)] was proposed
    and it achieved a significant speedup compared to the original CPU implementation.
    To more effectively train A3C, [[141](#bib.bib141)] proposed FFE which forces
    random exploration at the right time during a training episode, that can lead
    to improved training performance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服速度的限制，GA3C [[16](#bib.bib16)] 被提出，并且与原始的 CPU 实现相比，实现了显著的加速。为了更有效地训练 A3C，[[141](#bib.bib141)]
    提出了 FFE，这种方法在训练过程中强制在合适的时间进行随机探索，这可以提高训练性能。
- en: 4.2 Model-Based Algorithms
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于模型的算法
- en: 'We have discussed so far model-free methods including the value-based approach
    and policy gradient approach. In this section, we focus on the model-based approach,
    that deals with the dynamics of the environment by learning a transition model
    that allows for simulation of the environment without interacting with the environment
    directly. In contrast to model-free approaches, model-based approaches are learned
    from experience by a function approximation. Theoretically, no specific prior
    knowledge is required in model-based RL/DRL but incorporating prior knowledge
    can help faster convergence and better-trained model, speed up training time as
    well as the number of training samples. While using raw data with pixel, it is
    difficult for model-based RL to work on high dimensional and dynamic environments.
    This is addressed in DRL by embedding the high-dimensional observations into a
    lower-dimensional space using autoencoders [[95](#bib.bib95)]. Many DRL approaches
    have been based on scaling up prior work in RL to high-dimensional problems. A
    good overview of model-based RL for high-dimensional problems can be found in
    [[297](#bib.bib297)] which partition model-based DRL into three categories: explicit
    planning on given transitions, explicit planning on learned transitions, and end-to-end
    learning of both planning and transitions. In general, DRL targets training DNNs
    to approximate the optimal policy $\pi^{*}$ together with optimal value functions
    $V^{*}$ and $Q^{*}$. In the following, we will cover the most common model-based
    DRL approaches including value function and policy search methods.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了包括基于价值的方法和策略梯度方法的无模型方法。在本节中，我们重点介绍基于模型的方法，它通过学习一个转移模型来处理环境的动态，该模型允许在不直接与环境互动的情况下模拟环境。与无模型方法不同，基于模型的方法通过函数近似从经验中学习。理论上，基于模型的
    RL/DRL 不需要特定的先验知识，但结合先验知识可以帮助更快的收敛和更好的训练模型，缩短训练时间以及减少训练样本的数量。虽然使用原始像素数据，基于模型的
    RL 在高维和动态环境中难以工作。DRL 通过使用自编码器 [[95](#bib.bib95)] 将高维观察嵌入到低维空间来解决这一问题。许多 DRL 方法已基于将
    RL 先前工作扩展到高维问题。关于高维问题的基于模型的 RL 的良好概述可以在 [[297](#bib.bib297)] 中找到，该文献将基于模型的 DRL
    分为三个类别：给定转移上的显式规划、学习转移上的显式规划以及规划和转移的端到端学习。一般来说，DRL 目标是训练 DNNs 来近似最优策略 $\pi^{*}$
    以及最优价值函数 $V^{*}$ 和 $Q^{*}$。接下来，我们将介绍最常见的基于模型的 DRL 方法，包括价值函数和策略搜索方法。
- en: 4.2.1 Value function
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 价值函数
- en: 'We start this category with DQN [[264](#bib.bib264)] which has been successfully
    applied to classic Atari and illustrated in Fig.[7](#S4.F7 "Figure 7 ‣ 4.1.1 Value-based
    DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction to Deep Reinforcement
    Learning ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey").
    DQN uses CNNs to deal with high dimensional state space like pixels, to approximate
    the Q-value function.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从 DQN [[264](#bib.bib264)] 开始，这一方法已成功应用于经典的 Atari 游戏，并在图 [7](#S4.F7 "Figure
    7 ‣ 4.1.1 Value-based DRL methods ‣ 4.1 Model-Free Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey") 中展示。DQN 使用 CNNs 来处理像素等高维状态空间，以近似 Q 值函数。'
- en: Monte Carlo tree search (MCTS) MCTS [[62](#bib.bib62)] is one of the most popular
    methods to look-ahead search and it is combined with a DNN-based transition model
    to build a model-based DRL in [[9](#bib.bib9)]. In this work, the learned transition
    model predicts the next frame and the rewards one step ahead using the input of
    the last four frames of the agent’s first-person-view image and the current action.
    This model is then used by the Monte Carlo tree search algorithm to plan the best
    sequence of actions for the agent to perform.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索（MCTS）[[62](#bib.bib62)] 是一种最受欢迎的前瞻搜索方法，它与基于 DNN 的转移模型结合，用于构建 [[9](#bib.bib9)]
    中的模型基 DRL。在这项工作中，学习的转移模型预测下一帧以及一步之遥的奖励，使用的是代理的第一人称视角图像的最后四帧和当前动作作为输入。然后，蒙特卡洛树搜索算法使用该模型规划代理执行的最佳动作序列。
- en: Value-Targeted Regression (UCRL-VTR) Alex, et al. proposed model-based DRL for
    regret minimization [[167](#bib.bib167)]. In their work, a set of models, that
    are ‘consistent’ with the data collected, is constructed at each episode. The
    consistency is defined as the total squared error, whereas the value function
    is determined by solving the optimistic planning problem with the constructed
    set of models
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 值目标回归（UCRL-VTR）Alex 等人提出了用于后悔最小化的基于模型的 DRL [[167](#bib.bib167)]。在他们的工作中，在每一集时构建一组与收集的数据“一致”的模型。这里的一致性定义为总的平方误差，而值函数通过解决带有构建模型集的问题来确定。
- en: 4.2.2 Policy search
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 策略搜索
- en: Policy search methods aim to directly find policies by means of gradient-free
    or gradient-based methods.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 策略搜索方法旨在通过无梯度或基于梯度的方法直接寻找策略。
- en: Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) ME-TRPO [[190](#bib.bib190)]
    is mainly based on Trust Region Policy Optimization (TRPO) [[327](#bib.bib327)]
    which imposes a trust region constraint on the policy to further stabilize learning.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 模型集成信任区域策略优化（ME-TRPO）ME-TRPO [[190](#bib.bib190)] 主要基于信任区域策略优化（TRPO）[[327](#bib.bib327)]，该方法对策略施加信任区域约束，以进一步稳定学习。
- en: Model-Based Meta-Policy-Optimization (MB-MPO) MB-MPO [[58](#bib.bib58)] addresses
    the performance limitation of model-based DRL compared against model-free DRL
    when learning dynamics models. MB-MPO learns an ensemble of dynamics models, a
    policy that can quickly adapt to any model in the ensemble with one policy gradient
    step. As a result, the learned policy exhibits less model bias without the need
    to behave conservatively.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的元策略优化（MB-MPO）MB-MPO [[58](#bib.bib58)] 解决了模型基于DRL在学习动态模型时，相比于模型自由DRL的性能限制。MB-MPO
    学习一组动态模型，一个策略可以通过一个策略梯度步骤快速适应该集合中的任何模型。结果是，所学的策略表现出较少的模型偏差，无需采取保守的行为。
- en: 'A summary of both model-based and model-free DRL algorithms is given in Table
    [2](#S4.T2 "Table 2 ‣ 4.2.2 Policy search ‣ 4.2 Model-Based Algorithms ‣ 4 Introduction
    to Deep Reinforcement Learning ‣ Deep Reinforcement Learning in Computer Vision:
    A Comprehensive Survey"). In this Table, we also categorized DRL techniques into
    either on-policy or off-policy. In on-policy RL, it allows the use of older samples
    (collected using the older policies) in the calculation. The policy $\pi^{k}$
    is updated with data collected by $\pi^{k}$ itself. In off-policy RL, the data
    is assumed to be composed of different policies $\pi^{0},\pi^{0},...,\pi^{k}$.
    Each policy has its own data collection, then the data collected from $\pi^{0}$,
    $\pi^{1}$, …, $\pi^{k}$ is used to train $\pi^{k+1}$.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S4.T2 "表 2 ‣ 4.2.2 策略搜索 ‣ 4.2 基于模型的算法 ‣ 4 深度强化学习导论 ‣ 深度强化学习在计算机视觉中的综合调查")
    给出了模型基于和无模型 DRL 算法的总结。在该表中，我们还将 DRL 技术分为 on-policy 或 off-policy。在 on-policy RL
    中，它允许使用较旧的样本（使用较旧策略收集）进行计算。策略 $\pi^{k}$ 使用由 $\pi^{k}$ 自身收集的数据进行更新。在 off-policy
    RL 中，数据被假定为由不同的策略 $\pi^{0},\pi^{1},...,\pi^{k}$ 组成。每个策略有自己的数据收集，然后使用从 $\pi^{0}$、$\pi^{1}$、…、$\pi^{k}$
    收集的数据来训练 $\pi^{k+1}$。
- en: 'Table 2: Summary of model-based and model-free DRL algorithms consisting of
    value-based and policy gradient methods.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于模型和无模型的 DRL 算法的总结，包括值基于和策略梯度方法。
- en: '| DRL Algorithms | Description | Category |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| DRL 算法 | 描述 | 类别 |'
- en: '| --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| DQN [[264](#bib.bib264)] | Deep Q Network | Value-based Off-policy |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| DQN [[264](#bib.bib264)] | 深度 Q 网络 | 值基于 Off-policy |'
- en: '| Double DQN [[370](#bib.bib370)] | Double Deep Q Network | Value-based Off-policy
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 双重 DQN [[370](#bib.bib370)] | 双重深度 Q 网络 | 值基于 Off-policy |'
- en: '| Dueling DQN [[390](#bib.bib390)] | Dueling Deep Q Network | Value-based Off-policy
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 对抗 DQN [[390](#bib.bib390)] | 对抗深度 Q 网络 | 值基于 Off-policy |'
- en: '| MCTS [[9](#bib.bib9)] | Monte Carlo tree search | Value-based On-policy |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| MCTS [[9](#bib.bib9)] | 蒙特卡罗树搜索 | 值基于 On-policy |'
- en: '| UCRL-VTR[[167](#bib.bib167)] | optimistic planning problem | Value-based
    Off-policy |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| UCRL-VTR[[167](#bib.bib167)] | 乐观规划问题 | 值基于 Off-policy |'
- en: '| DDPG [[223](#bib.bib223)] | DQN with Deterministic Policy Gradient | Policy
    gradient Off-policy |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| DDPG [[223](#bib.bib223)] | 带有确定性策略梯度的 DQN | 策略梯度 Off-policy |'
- en: '| TRPO [[327](#bib.bib327)] | Trust Region Policy Optimization | Policy gradient
    On-policy |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| TRPO [[327](#bib.bib327)] | 信任区域策略优化 | 策略梯度 On-policy |'
- en: '| PPO [[328](#bib.bib328)] | Proximal Policy Optimization | Policy gradient
    On-policy |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| PPO [[328](#bib.bib328)] | 近端策略优化 | 策略梯度 On-policy |'
- en: '| ME-TRPO [[190](#bib.bib190)] | Model-Ensemble Trust-Region Policy Optimization
    | Policy gradient On-policy |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| ME-TRPO [[190](#bib.bib190)] | 模型集成信任区域策略优化 | 策略梯度 On-policy |'
- en: '| MB-MPO [[58](#bib.bib58)] | Model-Based Meta- Policy-Optimization | Policy
    gradient On-policy |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| MB-MPO [[58](#bib.bib58)] | 基于模型的元策略优化 | 策略梯度在线策略 |'
- en: '| A3C [[263](#bib.bib263)] | Asynchronous Advantage Actor Critic | Actor Critic
    On-Policy |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| A3C [[263](#bib.bib263)] | 异步优势演员评论家 | 演员评论家在线策略 |'
- en: '| A2C [[263](#bib.bib263)] | Advantage Actor Critic | Actor Critic On-Policy
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| A2C [[263](#bib.bib263)] | 优势演员评论家 | 演员评论家在线策略 |'
- en: 4.3 Good practices
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 良好实践
- en: Inspired by Deep Q-learning [[264](#bib.bib264)], we discuss some useful techniques
    that are used during training an agent in DRL framework in practices.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 受到深度Q学习的启发 [[264](#bib.bib264)]，我们讨论了一些在实际DRL框架中训练代理时使用的有用技术。
- en: Experience replay Experience replay [[417](#bib.bib417)] is a useful part of
    off-policy learning and is often used while training an agent in RL framework.
    By getting rid of as much information as possible from past experiences, it removes
    the correlations in training data and reduces the oscillation of the learning
    procedure. As a result, it enables agents to remember and re-use past experiences
    sometimes in many weights updates which increases data efficiency.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放 经验回放 [[417](#bib.bib417)] 是离策略学习中的一个有用部分，通常在RL框架中训练代理时使用。通过尽可能多地消除过去经验中的信息，它去除了训练数据中的相关性，减少了学习过程的波动。因此，它使代理能够记住并重新利用过去的经验，这在许多权重更新中增加了数据效率。
- en: Minibatch learning Minibatch learning is a common technique that is used together
    with experience replay. Minibatch allows learning more than one training sample
    at each step, thus, it makes the learning process robust to outliers and noise.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量学习 小批量学习是一种常用技术，通常与经验回放一起使用。小批量学习允许在每一步学习多个训练样本，从而使学习过程对异常值和噪声具有鲁棒性。
- en: 'Target Q-network freezing As described in [[264](#bib.bib264)], two networks
    are used for the training process. In target Q-network freezing: one network interacts
    with the environment and another network plays the role of a target network. The
    first network is used to generate target Q-values that are used to calculate losses.
    The weights of the second network i.e. target network are fixed and slowly updated
    to the first network [[224](#bib.bib224)].'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 目标Q网络冻结 如[[264](#bib.bib264)]所述，训练过程中使用了两个网络。在目标Q网络冻结中：一个网络与环境交互，另一个网络则扮演目标网络的角色。第一个网络用于生成目标Q值，这些Q值用于计算损失。第二个网络，即目标网络的权重是固定的，并且缓慢更新为第一个网络的权重
    [[224](#bib.bib224)]。
- en: Reward clipping A reward is the scalar number provided by the environment and
    it aims at optimizing the network. To keep the rewards in a reasonable scale and
    to ensure proper learning, they are clipped to a specific range (-1 ,1). Here
    1 refers to as positive reinforcement or reward and -1 is referred to as negative
    reinforcement or punishment.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励剪辑 奖励是由环境提供的标量数值，旨在优化网络。为了保持奖励在合理的范围内并确保适当的学习，奖励被剪辑到特定范围（-1，1）。这里1表示正向强化或奖励，-1表示负向强化或惩罚。
- en: Model-based v.s. model-free approach Whether the model-free or model-based approaches
    is chosen mainly depends on the model architecture i.e. policy and value function.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法与无模型的方法 选择基于模型的方法还是无模型的方法主要取决于模型架构，即策略和价值函数。
- en: 5 DRL in Landmark Detection
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 DRL在地标检测中的应用
- en: Autonomous landmark detection has gained more and more attention in the past
    few years. One of the main reasons for this increased inclination is the rise
    of automation for evaluating data. The motivation behind using an algorithm for
    landmarking instead of a person is that manual annotation is a time-consuming
    tedious task and is prone to errors. Many efforts have been made for the automation
    of this task. Most of the works that were published for this task using a machine
    learning algorithm to solve the problem. [[64](#bib.bib64)] proposed a regression
    forest-based method for detecting landmark in a full-body CT scan. Although the
    method was fast it was less accurate when dealing with large organs. [[101](#bib.bib101)]
    extended the work of [[64](#bib.bib64)] by adding statistical shape priors that
    were derived from segmentation masks with cascade regression.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 自主地标检测在过去几年中受到了越来越多的关注。这种关注度增加的主要原因之一是自动化数据评估的兴起。使用算法进行地标检测而非人工标注的动机在于，人工标注是一项耗时且乏味的任务，并且容易出错。为实现这一任务的自动化，已经付出了许多努力。大多数发布的相关工作采用机器学习算法来解决这个问题。[[64](#bib.bib64)]
    提出了基于回归森林的方法来检测全身CT扫描中的地标。尽管该方法较快，但在处理大器官时准确性较低。[[101](#bib.bib101)] 通过增加从分割掩膜中派生的统计形状先验，并结合级联回归，扩展了[[64](#bib.bib64)]
    的工作。
- en: In order to address the limitations of previous works on anatomy detection,
    [[105](#bib.bib105)] reformulated the detection problem as a behavior learning
    task for an artificial agent using MDP. By using the capabilities of DRL and scale-space
    theory [[226](#bib.bib226)], the optimal search strategies for finding anatomical
    structures are learned based on the image information at multiple scales. In their
    approach, the search starts at the coarsest scale level for capturing global context
    and continues to finer scales for capturing more local information. In their RL
    configuration, the state of the agent at time $t$, $s_{t}=I(\vec{p}_{t})$ is defined
    as an axis-aligned box of image intensities extracted from the image $I$ and centered
    at the voxel-position $\vec{p}_{t}$ in image space. An action $a_{t}$ allows the
    agent to move from any voxel position $\vec{p}_{t}$ to an adjacent voxel position
    $\vec{p}_{t+1}$. The reward function represents distance-based feedback, which
    is positive if the agent gets closer to the target structure and negative otherwise.
    In this work, a CNN is used to extract deep semantic features. The search starts
    with the coarsest scale level $M-1$, the algorithm tries to maximize the reward
    which is the change in distance between ground truth and predicted landmark location
    before and after the action of moving the scale window across the image. Upon
    convergence, the scale level is changed to $M-2$ and the search continued from
    the convergence point at scale level $M-1$. The process is repeated on the following
    scales until convergence on the finest scale. The authors performed experiments
    on 3D CT scans and obtained an average accuracy increase of 20-30$\%$ and lower
    distance error than the other techniques such as SADNN [[104](#bib.bib104)] and
    3D-DL [[427](#bib.bib427)]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决先前解剖检测工作中的局限性，[[105](#bib.bib105)] 将检测问题重新表述为一个使用MDP的人工智能代理的行为学习任务。通过利用DRL和尺度空间理论[[226](#bib.bib226)]，学习基于多尺度图像信息的解剖结构的最佳搜索策略。在他们的方法中，搜索从最粗尺度级别开始，以捕捉全局背景，然后继续在更精细的尺度上捕捉更多局部信息。在他们的强化学习配置中，代理在时间$t$的状态$s_{t}=I(\vec{p}_{t})$被定义为从图像$I$中提取的、以图像空间中体素位置$\vec{p}_{t}$为中心的图像强度轴对齐框。一个动作$a_{t}$允许代理从任意体素位置$\vec{p}_{t}$移动到相邻的体素位置$\vec{p}_{t+1}$。奖励函数表示基于距离的反馈，如果代理接近目标结构，则为正，否则为负。在这项工作中，使用CNN提取深层语义特征。搜索从最粗尺度级别$M-1$开始，算法尝试最大化奖励，即在移动尺度窗口跨越图像前后，真实位置和预测地标位置之间距离的变化。收敛后，尺度级别更改为$M-2$，搜索从尺度级别$M-1$的收敛点继续。这个过程在接下来的尺度上重复，直到在最细尺度上收敛。作者在3D
    CT扫描上进行了实验，获得了20-30$\%$的平均准确度提升，以及比其他技术如SADNN[[104](#bib.bib104)]和3D-DL[[427](#bib.bib427)]更低的距离误差。
- en: 'Focus on anatomical landmark localization in 3D fetal US images, [[10](#bib.bib10)]
    proposed and demonstrated use cases of several different Deep Q-Network RL models
    to train agents that can precisely localize target landmarks in medical scans.
    In their work, they formulate the landmark detection problem as an MDP of a goal-oriented
    agent, where an artificial agent is learned to make a sequence of decisions towards
    the target point of interest. At each time step, the agent should decide which
    direction it has to move to find the target landmark. These sequential actions
    form a learned policy forming a path between the starting point and the target
    landmark. This sequential decision-making process is approximated under RL. In
    this RL configuration, the environment is defined as a 3D input image, action
    $A$ is a set of six actions $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$ corresponding
    to three directions, the state $s$ is defined as a 3D region of interest (ROI)
    centered around the target landmark and the reward is chosen as the difference
    between the two Euclidean distances: the previous step and current step. This
    reward signifies whether the agent is moving closer to or further away from the
    desired target location. In this work, they also proposed a novel fixed- and multi-scale
    optimal path search strategy with hierarchical action steps for agent-based landmark
    localization frameworks.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 关注于3D胎儿超声图像中的解剖标志物定位，[[10](#bib.bib10)] 提出了并演示了几种不同深度Q网络RL模型的使用案例，以训练能够精确定位医学扫描中目标地标的代理。在他们的工作中，他们将地标检测问题表述为一个目标导向的MDP，其中一个人工代理被训练去做出一系列决策以达到感兴趣的目标点。在每个时间步，代理应决定它需要移动的方向以找到目标地标。这些连续的动作形成了一个学习到的策略，从起始点到目标地标形成了一条路径。这个连续决策过程在RL下被逼近。在这种RL配置中，环境被定义为3D输入图像，动作
    $A$ 是一组六个动作 $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$ 对应于三个方向，状态 $s$ 被定义为围绕目标地标的3D兴趣区域（ROI），奖励被选为两个欧几里得距离之间的差值：上一步和当前步骤。这个奖励表示代理是离期望目标位置更近还是更远。在这项工作中，他们还提出了一种新颖的固定和多尺度最优路径搜索策略，具有层次化动作步骤，用于基于代理的地标定位框架。
- en: Whereas pure policy or value-based methods have been widely used to solve RL-based
    localization problems, [[7](#bib.bib7)] adopts an actor-critic [[262](#bib.bib262)]
    based direct policy search method framed in a temporal difference learning approach.
    In their work, the state is defined as a function of the agent-position which
    allows the agent at any position to observe an $m\times m\times 3$ block of surrounding
    voxels. Similar to other previous work, the action space is $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$.
    The reward is chosen as a simple binary reward function, where a positive reward
    is given if an action leads the agent closer to the target landmark, and a negative
    reward is given otherwise. Far apart from the previous work, their approach proposes
    a non-linear policy function approximator represented by an MLP whereas the value
    function approximator is presented by another MLP stacked on top of the same CNN
    from the policy net. Both policy (actor) and value (critic) networks are updated
    by actor-critic learning. To improve the learning, they introduce a partial policy-based
    RL to enable solving the large problem of localization by learning the optimal
    policy on smaller partial domains. The objective of the partial policy is to obtain
    multiple simple policies on the projections of the actual action space, where
    the projected policies can reconstruct the policy on the original action space.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管纯粹的策略或基于价值的方法已被广泛用于解决基于RL的定位问题，[[7](#bib.bib7)] 采用了一种基于演员-评论家的[[262](#bib.bib262)]
    直接策略搜索方法，并将其框架设定在时间差分学习方法中。在他们的工作中，状态被定义为代理位置的函数，这允许代理在任何位置观察到一个 $m\times m\times
    3$ 的周围体素块。类似于其他先前的工作，动作空间为 $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$。奖励被选为一个简单的二元奖励函数，如果一个动作使代理更接近目标地标，则给予正奖励，否则给予负奖励。与先前的工作大相径庭的是，他们的方法提出了一种由MLP表示的非线性策略函数逼近器，而价值函数逼近器则由另一个MLP表示，该MLP堆叠在策略网络的相同CNN上。策略（演员）和价值（评论家）网络通过演员-评论家学习进行更新。为了改进学习，他们引入了部分基于策略的RL，以通过在较小的部分领域中学习最佳策略来解决大规模定位问题。部分策略的目标是在实际动作空间的投影上获得多个简单策略，其中投影策略可以重建原始动作空间上的策略。
- en: Based on the hypothesis that the position of all anatomical landmarks is interdependent
    and non-random within the human anatomy and this is necessary as the localization
    of different landmarks requires learning partly heterogeneous policies, [[377](#bib.bib377)]
    concluded that one landmark can help to deduce the location of others. For collective
    gain, the agents share their accumulated knowledge during training. In their approach,
    the state is defined as RoI centered around the location of the agent. The reward
    function is defined as the relative improvement in Euclidean distance between
    their location at time $t$ and the target landmark location. Each agent is considered
    as Partially Observable Markov Decision Process (POMDP) [[107](#bib.bib107)] and
    calculates its individual reward as their policies are disjoint. In order to reduce
    the computational load in locating multiple landmarks and increase accuracy through
    anatomical interdependence, they propose a collaborative multi-agent landmark
    detection framework (Collab-DQN) where DQN is built upon a CNN. The backbone CNN
    is shared across all agents while the policy-making fully connected layers are
    separate for each agent.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 基于所有解剖地标的位置在人体解剖结构中是相互依赖且非随机的这一假设，并且由于不同地标的定位需要学习部分异质策略，[[377](#bib.bib377)]
    认为一个地标可以帮助推测其他地标的位置。为了集体利益，代理在训练过程中共享他们积累的知识。在他们的方法中，状态定义为围绕代理位置的RoI。奖励函数定义为其在时间$t$的当前位置与目标地标位置之间的欧几里得距离的相对改进。每个代理被视为部分可观测马尔可夫决策过程（POMDP）[[107](#bib.bib107)]，并根据其策略不相交来计算其个人奖励。为了减少定位多个地标的计算负担并通过解剖学相互依赖提高准确性，他们提出了一种协作式多代理地标检测框架（Collab-DQN），其中DQN建立在CNN之上。所有代理共享主干CNN，而策略制定的全连接层对每个代理是分开的。
- en: 'Table 3: Comparing various DRL-based landmark detection methods. The first
    group on Single Landmark Detection (SLD) and the second group for Multiple Landmark
    Detection (MLD)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：比较各种基于DRL的地标检测方法。第一组为单一地标检测（SLD），第二组为多地标检测（MLD）
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Performance
    | Datasets and source code |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 训练技术 | 动作 | 备注 | 性能 | 数据集和源代码 |'
- en: '| SLD [[105](#bib.bib105)] | 2017 | DQN | 6 action: 2 per axis | State: an
    axis-aligned box centered at the voxel-position. Action: move from $\vec{p}_{t}$
    to $\vec{p}_{t+1}$. Reward: distance-based feedback | Average accuracy increase
    20-30%. Lower distance error than other techniques such as SADNN [[104](#bib.bib104)]
    and 3D-DL [[427](#bib.bib427)] | 3D CT Scan |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| SLD [[105](#bib.bib105)] | 2017 | DQN | 6个动作：每个轴2个 | 状态：以体素位置为中心的轴对齐框。动作：从$\vec{p}_{t}$移动到$\vec{p}_{t+1}$。奖励：基于距离的反馈
    | 平均准确度提高20-30%。比其他技术（如SADNN [[104](#bib.bib104)]和3D-DL [[427](#bib.bib427)]）具有更低的距离误差
    | 3D CT扫描 |'
- en: '| SLD [[10](#bib.bib10)] | 2019 | DQN, DDQN, Duel DQN and Duel DDQN | 6 action:
    2 per axis | Environment: 3D input image. State: 3D RoI centered around the target
    landmark. Reward: Euclidean distance between predicted points and groundtruth
    points. | Duel DQN performs the best on Right Cerebellum (FS), Left Cerebellum
    (FS, MS) Duel DDQN is the best on Right Cerebellum (MS) DQN performs the best
    on Cavum Septum Pellucidum(FS, MS) | Fetal head, ultrasound scans [[219](#bib.bib219)].
    [Available code](https://github.com/amiralansary/rl-medical) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| SLD [[10](#bib.bib10)] | 2019 | DQN, DDQN, Duel DQN 和 Duel DDQN | 6个动作：每个轴2个
    | 环境：3D输入图像。状态：围绕目标地标的3D RoI。奖励：预测点与真实点之间的欧几里得距离。 | Duel DQN 在右侧小脑（FS）、左侧小脑（FS、MS）表现最佳；Duel
    DDQN 在右侧小脑（MS）表现最佳；DQN 在间隔膜窦（FS、MS）表现最佳 | 胎头，超声扫描 [[219](#bib.bib219)]。 [可用代码](https://github.com/amiralansary/rl-medical)
    |'
- en: '| SLD [[7](#bib.bib7)] | 2019 | Actor- Critic -based Partial -Policy RL | 6
    action: 2 per axis | State: a function of the agent-position. Reward: binary reward
    function. policy function: MLP. value function: MLP | Faster and better convergence,
    outperforms than other conventional actor-critic and Q-learning | CT volumes:
    Aortic valve. CT volumes: LAA seed-point. MR images: Vertebra centers [[42](#bib.bib42)].
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| SLD [[7](#bib.bib7)] | 2019 | 基于Actor-Critic的部分策略RL | 6个动作：每个轴2个 | 状态：代理位置的函数。奖励：二进制奖励函数。策略函数：MLP。价值函数：MLP
    | 更快和更好的收敛，优于其他传统的actor-critic和Q学习 | CT体积：主动脉瓣。CT体积：LAA种子点。MR图像：椎骨中心 [[42](#bib.bib42)]。
    |'
- en: '| MLD [[377](#bib.bib377)] | 2019 | Collab DQN | 6 action: 2 per axis | State:
    RoI centred around the agent. Reward: relative improvement in Euclidean distance.
    Each Agent is a POMDP has its own reward. Collab-DQN: reduce the computational
    load | Colab DQN got better results than supervised CNN and DQN | Brain MRI landmark
    [[158](#bib.bib158)], Cardiac MRI landmark [[70](#bib.bib70)], Fetal brain landmark
    [[10](#bib.bib10)]. [Available code](https://github.com/thanosvlo/MARL-for-Anatomical-Landmark-Detection)
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| MLD [[377](#bib.bib377)] | 2019 | Collab DQN | 6 个动作：每轴 2 个 | 状态：以代理为中心的
    ROI。奖励：欧几里得距离的相对改善。每个代理是一个 POMDP，有自己的奖励。Collab-DQN：减少计算负荷 | Collab DQN 比监督 CNN
    和 DQN 得到更好的结果 | 大脑 MRI 地标 [[158](#bib.bib158)]，心脏 MRI 地标 [[70](#bib.bib70)]，胎儿大脑地标
    [[10](#bib.bib10)]。 [可用代码](https://github.com/thanosvlo/MARL-for-Anatomical-Landmark-Detection)
    |'
- en: '| MLD [[161](#bib.bib161)] | 2020 | DQN | 6 action 2 per axis | State: 3D image
    patch. Reward: Euclidean distance and $\in[-1,1]$. Backbone CNN is share among
    agents Each agent has it own Fully connected layer | Detection error increased
    as the degree of missing information increased Performance is affected by the
    choice of landmarks | 3D Head MR images |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| MLD [[161](#bib.bib161)] | 2020 | DQN | 每轴 6 个动作 2 个 | 状态：3D 图像补丁。奖励：欧几里得距离和
    $\in[-1,1]$。骨干 CNN 在代理之间共享，每个代理有其自己的全连接层 | 随着缺失信息的增加，检测错误增加。性能受地标选择的影响 | 3D 头部
    MR 图像 |'
- en: 'Different from the previous works on RL-based landmark detection, which detect
    a single landmark,[[161](#bib.bib161)] proposed a multiple landmark detection
    approach to better time-efficient and more robust to missing data. In their approach,
    each landmark is guided by one agent. The MDP is models as follows: The state
    is defined as a 3D image patch. The reward, clipped in [-1, +1], is defined as
    the difference in the Euclidean distance between the landmark predicted in the
    previous time step and the target, and in the landmark predicted in the current
    time step and the target. The action space is defined as in other previous works
    i.e. there are 6 actions $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$ in the action
    space. To enable the agents to share the information learned by detecting one
    landmark for use in detecting other landmarks, hard parameter sharing from multi-task
    learning is used. In this work, the backbone network is shared among agents and
    each agent has its own fully connected layer.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于之前基于强化学习的地标检测工作，这些工作主要检测单一地标，[[161](#bib.bib161)] 提出了多地标检测方法，以提高时间效率和对缺失数据的鲁棒性。在他们的方法中，每个地标由一个代理引导。MDP
    的模型如下：状态定义为 3D 图像补丁。奖励值范围在[-1, +1]之间，定义为上一个时间步预测的地标与目标之间的欧几里得距离和当前时间步预测的地标与目标之间的欧几里得距离之间的差异。动作空间定义如其他之前的工作，即动作空间中有
    6 个动作 $a_{x}+,a_{x}-,a_{y}+,a_{y}-,a_{z}+,a_{z}-$。为了使代理能够共享通过检测一个地标获得的信息，以用于检测其他地标，使用了多任务学习中的硬参数共享。在这项工作中，骨干网络在各代理之间共享，每个代理都有自己的全连接层。
- en: 'Table LABEL:tab:landmark summarizes and compares all approaches for DRL in
    landmark detection, and a basic implementation of landmark detection using DRL
    has been shown in Fig. [11](#S5.F11 "Figure 11 ‣ 5 DRL in Landmark Detection ‣
    Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure illustrates a general implementation of landmark detection with the help
    of DRL, where the state is the Region of interest (ROI) around the current landmark
    location cropped from the image, The actions performed by the DRL agent are responsible
    for shifting the ROI across the image forming a new state and the reward corresponds
    to the improvement in euclidean distance between ground truth and predicted landmark
    location with iterations as used by [[105](#bib.bib105)],[[7](#bib.bib7)],[[10](#bib.bib10)],[[377](#bib.bib377)],[[161](#bib.bib161)].'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '表 LABEL:tab:landmark 总结并比较了所有用于地标检测的 DRL 方法，并且图 [11](#S5.F11 "Figure 11 ‣ 5
    DRL in Landmark Detection ‣ Deep Reinforcement Learning in Computer Vision: A
    Comprehensive Survey") 显示了使用 DRL 的地标检测的基本实现。该图展示了借助 DRL 进行地标检测的一般实现，其中状态是从图像中裁剪出的当前地标位置周围的兴趣区域
    (ROI)，DRL 代理执行的动作负责在图像中移动 ROI 形成新的状态，而奖励对应于迭代中真实值与预测地标位置之间欧几里得距离的改善，如 [[105](#bib.bib105)],
    [[7](#bib.bib7)], [[10](#bib.bib10)], [[377](#bib.bib377)], [[161](#bib.bib161)]
    使用的那样。'
- en: '![Refer to caption](img/d9972fc9734de5c35a7ada06c3f6700c.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d9972fc9734de5c35a7ada06c3f6700c.png)'
- en: 'Figure 11: DRL implementation for landmark detection, The red point corresponds
    to the current landmark location and Red box is the Region of Interest (ROI) centered
    around the landmark, the actions of DRL agent shift the ROI across the image to
    maximize the reward corresponding to the improvement in distance between the ground
    truth and predicted landmark location.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：用于地标检测的DRL实现，红点对应当前地标位置，红色框是以地标为中心的兴趣区域（ROI），DRL代理的动作将ROI在图像上移动，以最大化对应于真实位置和预测地标位置之间距离改进的奖励。
- en: 6 DRL in Object Detection
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 DRL在对象检测中的应用
- en: Object detection is a task that requires the algorithm to find bounding boxes
    for all objects in a given image. Many attempts have been made towards object
    detection. A method for bounding box prediction for object detection was proposed
    by [[109](#bib.bib109)], in which the task was performed by extracting region
    proposals from an image and then feeding each of them to a CNN to classify each
    region. An improvement to this technique was proposed by [[108](#bib.bib108)],
    where they used the feature from the CNN to propose region proposals instead of
    the image itself, this resulted in fast detection. Further improvement was proposed
    by [[309](#bib.bib309)], where the authors proposed using a region proposal network
    (RPN) to identify the region of interest, resulting in much faster detection.
    Other attempts including focal loss [[225](#bib.bib225)] and Fast YOLO [[332](#bib.bib332)]
    have been proposed to address the imbalanced data problem in object detection
    with focal loss [[225](#bib.bib225)], and perform object detection in video on
    embedded devices in a real-time manner [[332](#bib.bib332)].
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测是一个要求算法在给定图像中找到所有对象的边界框的任务。许多尝试已被提出用于对象检测。[[109](#bib.bib109)] 提出了一个边界框预测方法，其中通过从图像中提取区域提案，然后将每个提案输入到CNN中以分类每个区域。[[108](#bib.bib108)]
    对此技术进行了改进，他们使用CNN的特征来提出区域提案，而不是直接使用图像，从而实现了快速检测。[[309](#bib.bib309)] 进一步改进，作者提出使用区域提案网络（RPN）来识别兴趣区域，从而实现了更快的检测。其他尝试，包括焦点损失[[225](#bib.bib225)]
    和快速YOLO[[332](#bib.bib332)]，已被提出以解决对象检测中的数据不平衡问题，焦点损失[[225](#bib.bib225)] 处理对象检测中的数据不平衡问题，而快速YOLO[[332](#bib.bib332)]
    实现了在嵌入式设备上实时执行视频中的对象检测。
- en: Considering MDP as the framework for solving the problem, [[43](#bib.bib43)]
    used DRL for active object localization. The authors considered 8 different actions
    (up, down, left, right, bigger, smaller, fatter, taller) to improve the fit of
    the bounding box around the object and additional action to trigger the goal state.
    They used a tuple of feature vector and history of actions for state and change
    in IOU across actions as a reward.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑将MDP作为解决问题的框架，[[43](#bib.bib43)] 使用DRL进行主动对象定位。作者考虑了8种不同的动作（向上、向下、向左、向右、放大、缩小、更胖、更高）以改善围绕对象的边界框拟合，并增加了一个触发目标状态的动作。他们使用了特征向量和动作历史的元组作为状态，并将动作间的IOU变化作为奖励。
- en: An improvement to [[43](#bib.bib43)] was proposed by [[25](#bib.bib25)], where
    the authors used a hierarchical approach for object detection by treating the
    problem of object detection as an MDP. In their method, the agent was responsible
    to find a region of interest in the image and then reducing the region of interest
    to find smaller regions from the previously selected region and hence forming
    a hierarchy. For the reward function, they used the change in Intersection over
    union (IOU) across the actions and used DQN as the agent. As described in their
    paper, two networks namely, Image-zooms and Pool45-crops with VGG-16 [[340](#bib.bib340)]
    backbone were used to extract the feature information that formed the state for
    DQN along with a memory vector of the last four actions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对[[43](#bib.bib43)]的改进由[[25](#bib.bib25)] 提出，作者通过将对象检测问题视为MDP，使用了分层方法进行对象检测。在他们的方法中，代理负责在图像中找到一个兴趣区域，然后将该兴趣区域缩小以从之前选择的区域中找到更小的区域，从而形成层次结构。对于奖励函数，他们使用了动作间的交并比（IOU）变化，并使用了DQN作为代理。如他们的论文中所述，使用了两个网络，即Image-zooms和Pool45-crops，具有VGG-16
    [[340](#bib.bib340)] 主干，以提取形成DQN状态的特征信息以及最后四个动作的记忆向量。
- en: 'Using a sequential search strategy, [[251](#bib.bib251)] proposed a method
    for object detection using DRL. The authors trained the model with a set of image
    regions where at each time step the agent returned fixate actions that specified
    a location in image for actor to explore next and the terminal state was specified
    by $done$ action. The state consisted of a tuple three elements: the observed
    region history $H_{t}$, selected evidence region history $E_{t}$ and fixate history
    $F_{t}$. The $fixate$ action was also a tuple of three elements: $fixate$ action,
    index of evidence region $e_{t}$ and image coordinate of next fixate $z_{t}$.
    The $done$ action consisted of: $done$ action, index of region representing the
    detected output $b_{t}$ and the detection confidence $c_{t}$. The authors defined
    the reward function that was sensitive to the detection location, the confidence
    at the final state and incurs a penalty for each region evaluation.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用顺序搜索策略，[[251](#bib.bib251)] 提出了一个利用 DRL 的对象检测方法。作者用一组图像区域训练了模型，在每个时间步，代理返回指定图像中位置的固定动作以供演员下一个探索，而终止状态由
    $done$ 动作指定。状态包括一个三元组：观察区域历史 $H_{t}$、选择的证据区域历史 $E_{t}$ 和固定历史 $F_{t}$。$fixate$
    动作也是一个三元组：$fixate$ 动作、证据区域的索引 $e_{t}$ 和下一个固定点的图像坐标 $z_{t}$。$done$ 动作包括：$done$
    动作、表示检测输出的区域索引 $b_{t}$ 和检测置信度 $c_{t}$。作者定义了一个对检测位置、最终状态下的置信度敏感的奖励函数，并对每个区域评估施加了惩罚。
- en: 'To map the inter-dependencies among the different objects, [[170](#bib.bib170)]
    proposed a tree-structured RL agent (Tree-RL) for object localization by considering
    the problem as an MDP. The authors in their implementation considered actions
    of two types: translation and scaling, where the scaling consisted of five actions
    whereas translation consisted of eight actions. In the specified work, the authors
    used the state as a concatenation of the feature vector of the current window,
    feature vector of the whole image, and history of taken actions. The feature vector
    were extracted from an ImageNet [[72](#bib.bib72)] [[320](#bib.bib320)] trained
    VGG-16 [[340](#bib.bib340)] model and for reward the change in IOU across an action
    was used. Tree-RL utilized a top-down tress search starting from the whole image
    where each window recursively takes the best action from each action group which
    further gives two new windows. This process is repeated recursively to find the
    object.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了映射不同对象之间的相互依赖关系，[[170](#bib.bib170)] 提出了一个树结构的 RL 代理（Tree-RL），通过将问题视为 MDP
    进行对象定位。在其实现中，作者考虑了两种类型的动作：平移和缩放，其中缩放包括五个动作，而平移包括八个动作。在指定的工作中，作者使用状态作为当前窗口的特征向量、整张图像的特征向量和已采取动作的历史的拼接。特征向量是从一个在
    ImageNet [[72](#bib.bib72)] [[320](#bib.bib320)] 上训练的 VGG-16 [[340](#bib.bib340)]
    模型中提取的，而奖励则是基于动作间 IOU 的变化。Tree-RL 采用从整张图像开始的自上而下的树搜索，每个窗口递归地从每个动作组中选择最佳动作，从而生成两个新的窗口。这个过程会递归地重复，以找到对象。
- en: The task of breast lesion detection is a challenging yet very important task
    in the medical imaging field. A DRL method for active lesion detection in the
    breast was proposed by [[246](#bib.bib246)], where the authors formulated the
    problem as an MDP. In their formulation, a total of nine actions consisting of
    6 translation actions, 2 scaling actions, and 1 trigger action were used. In the
    specified work, the change in dice coefficient across an action was used as the
    reward for scaling and translation actions, and for trigger action, the reward
    was $+\eta$ for dice coefficient greater than $r_{w}$ and $-\eta$ otherwise, where
    $\eta$ and $r_{w}$ were the hyperparameters chosen by the authors. For network
    structure, ResNet [[133](#bib.bib133)] was used as the backbone and DQN as the
    agent.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺病变检测任务是医学影像领域中的一个具有挑战性但非常重要的任务。[[246](#bib.bib246)] 提出了一个用于乳腺主动病变检测的 DRL 方法，作者将问题公式化为
    MDP。在其公式中，共使用了九个动作，包括 6 个平移动作、2 个缩放动作和 1 个触发动作。在指定的工作中，动作间的 dice 系数变化被用作平移和缩放动作的奖励，而对于触发动作，当
    dice 系数大于 $r_{w}$ 时奖励为 $+\eta$，否则为 $-\eta$，其中 $\eta$ 和 $r_{w}$ 是作者选择的超参数。对于网络结构，使用了
    ResNet [[133](#bib.bib133)] 作为骨干网络，并使用 DQN 作为代理。
- en: Different from the previous methods, [[386](#bib.bib386)] proposed a method
    for multitask learning using DRL for object localization. The authors considered
    the problem as an MDP where the agent was responsible to perform a series of transformations
    on the bounding box using a series of actions. Utilizing an RL framework the states
    consisted of feature vector and historical actions concatenated together, and
    a total of 8 actions for Bounding box transformation (left, right, up, down, bigger,
    smaller, fatter, and taller) were used. For reward the authors used the change
    in IOU between actions, the reward being 0 for an increase in IOU and -1 otherwise.
    For terminal action, however, the reward was 8 for IOU greater than 0.5 and -8
    otherwise. The authors in the paper used DQN with multitask learning for localization
    and divided terminal action and 8 transformation actions into two networks and
    trained them together.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法不同，[[386](#bib.bib386)] 提出了一个使用深度强化学习（DRL）进行物体定位的多任务学习方法。作者将问题视为马尔可夫决策过程（MDP），其中代理负责对边界框执行一系列变换，使用一系列动作。利用
    RL 框架，状态包括特征向量和历史动作的拼接，共使用了 8 种边界框变换动作（左、右、上、下、更大、更小、更胖、更高）。奖励方面，作者使用了动作间 IOU
    的变化，IOU 增加时奖励为 0，否则为 -1。然而，对于终端动作，当 IOU 大于 0.5 时奖励为 8，否则为 -8。文中作者使用了 DQN 与多任务学习进行定位，将终端动作和
    8 种变换动作分为两个网络，并一同训练。
- en: An improvement for the Region proposal networks that greedily select the ROIs
    was proposed by [[295](#bib.bib295)], where they used RL for the task. The authors
    in this paper used a two-stage detector similar to Fast and Faster R-CNN But used
    RL for the decision-making Process. For the reward, they used the normalized change
    in Intersection over Union (IOU).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于贪婪选择区域兴趣点（ROIs）的区域提议网络，[[295](#bib.bib295)] 提出了改进方法，他们使用了强化学习（RL）来处理这个任务。本文作者使用了类似于
    Fast 和 Faster R-CNN 的两阶段检测器，但在决策过程中使用了 RL。奖励方面，他们使用了交并比（IOU）的归一化变化。
- en: Instead of learning a policy from a large set of data, [[15](#bib.bib15)] proposed
    a method for bounding box refinement (BAR) using RL. In the paper, once the authors
    have an inaccurate bounding box that is predicted by some algorithm they use the
    BAR algorithm to predict a series of actions for refinement of a bounding box.
    They considered a total of 8 actions (up, down, left, right, wider, taller, fatter,
    thinner) for bounding box transformation and considered the problem as a sequential
    decision-making problem (SDMP). They proposed an offline method called BAR-DRL
    and an online method called BAR-CB where training is done on every image. In BAR-DRL
    the authors trained a DQN over the states which consisted of features extracted
    from ResNet50 [[133](#bib.bib133)] [[354](#bib.bib354)] pretrained on ImageNet
    [[72](#bib.bib72)] [[320](#bib.bib320)] and a history vector of 10 actions. The
    Reward for BAR-DRL was 1 if the IOU increase after action and -3 otherwise. For
    BAR-CB they adapted the LinUCB [[216](#bib.bib216)] algorithm for an episodic
    scenario and considered The Histogram of Oriented Gradients (HOG) for the state
    to capture the outline and edges of the object of interest. The actions in the
    online method (BAR-CB) were the same as the offline method and the reward was
    1 for increasing IOU and 0 otherwise. For both the implementations, the authors
    considered $\beta$ as terminal IOU.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[[15](#bib.bib15)] 提出了一个使用 RL 的边界框细化（BAR）方法，而不是从大量数据中学习策略。文中提到，一旦作者通过某算法预测出不准确的边界框，他们使用
    BAR 算法预测一系列动作以细化边界框。他们考虑了 8 种边界框变换动作（上、下、左、右、更宽、更高、更胖、更瘦），并将问题视为一个顺序决策问题（SDMP）。他们提出了一种离线方法
    BAR-DRL 和一种在线方法 BAR-CB，训练在每张图像上进行。在 BAR-DRL 中，作者对包含从 ResNet50 [[133](#bib.bib133)]
    [[354](#bib.bib354)] 提取的特征和 10 个动作的历史向量的状态训练了 DQN。BAR-DRL 的奖励为 1（如果 IOU 在动作后增加），否则为
    -3。对于 BAR-CB，他们将 LinUCB [[216](#bib.bib216)] 算法应用于回合场景，并考虑了方向梯度直方图（HOG）作为状态来捕捉目标物体的轮廓和边缘。在线方法（BAR-CB）中的动作与离线方法相同，奖励为
    1（如果 IOU 增加），否则为 0。对于这两种实现，作者将 $\beta$ 视为终端 IOU。'
- en: An improvement to sequential search strategy by [[251](#bib.bib251)] was proposed
    by [[367](#bib.bib367)], where they used a framework consisting of two modules,
    Coarse and fine level search. According to the authors, this method is efficient
    for object detection in large images (dimensions larger than 3000 pixels). The
    authors first performed a course level search on a large image to find a set of
    patches that are used by fine level search to find sub-patches. Both fine and
    coarse levels were conducted using a two-step episodic MDP, where The policy network
    was responsible for returning the probability distribution of all actions. In
    the paper, the authors considered the actions to be the binary action array (0,1)
    where 1 means that the agent would consider acquiring sub-patches for that particular
    patch. The authors in their implementation considered a number of patches and
    sub-patches as 16 and 4 respectively and used the linear combination of $R_{acc}$
    (detection recall) and $R_{cost}$ which combines image acquisition cost and run-time
    performance reward.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[[251](#bib.bib251)]中提出的顺序搜索策略的改进，由[[367](#bib.bib367)]提出了一种新方法，该方法使用了一个由粗略搜索和精细搜索两个模块组成的框架。根据作者的说法，这种方法在处理大图像（尺寸大于3000像素）中的物体检测时非常高效。作者首先对大图像进行粗略搜索，以找到一组用于精细搜索的补丁。精细和粗略搜索均使用了两步式的情景MDP，其中策略网络负责返回所有动作的概率分布。在论文中，作者将动作定义为二元动作数组（0,1），其中1表示代理将考虑为特定补丁获取子补丁。在他们的实现中，作者将补丁和子补丁的数量分别设置为16和4，并使用了$R_{acc}$（检测召回率）和$R_{cost}$的线性组合，其中$R_{cost}$结合了图像获取成本和运行时性能奖励。
- en: 'Table 4: Comparing various DRL-based object detection methods'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：比较各种基于DRL的物体检测方法
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and source code |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 训练技术 | 动作 | 备注 | 主干网络 | 性能 | 数据集和源代码 |'
- en: '| Active Object Localization [[43](#bib.bib43)] | 2015 | DQN | 8 actions: up,
    down, left, right, bigger, smaller, fatter, taller | States: feature vector of
    observed region and action history. Reward: Change in IOU. | 5 layer pretrained
    CNN | Higher mAP as compared to methods that did not use region proposals like
    MultiBox [[89](#bib.bib89)], RegionLets [[433](#bib.bib433)], DetNet [[356](#bib.bib356)],
    and second best mAP as compared to R-CNN [[109](#bib.bib109)] | Pascal VOC-2007
    [[90](#bib.bib90)], 2012 [[91](#bib.bib91)] Image Dataset. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 主动物体定位 [[43](#bib.bib43)] | 2015 | DQN | 8个动作：上、下、左、右、更大、更小、更胖、更高 | 状态：观察区域的特征向量和动作历史。奖励：IOU变化。
    | 5层预训练CNN | 与未使用区域提议的方法（如MultiBox [[89](#bib.bib89)]、RegionLets [[433](#bib.bib433)]、DetNet
    [[356](#bib.bib356)]）相比，mAP更高，与R-CNN [[109](#bib.bib109)]相比，mAP排名第二 | Pascal VOC-2007
    [[90](#bib.bib90)]，2012 [[91](#bib.bib91)] 图像数据集。 |'
- en: '| Hierarchical Object Detection [[25](#bib.bib25)] | 2016 | DQN | 5 actions:
    1 action per image quarter and 1 at the center | States: current region and memory
    vector using Image-zooms and Pool45-crops. Reward: change in IOU. | VGG-16 [[340](#bib.bib340)]
    | Objects detected with very few region proposals per image | Pascal VOC-2007
    Image Dataset [[90](#bib.bib90)]. [Available Code](https://github.com/imatge-upc/detection-2016-nipsws)
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 层次化物体检测 [[25](#bib.bib25)] | 2016 | DQN | 5个动作：每张图像的四分之一区域一个动作，中心区域一个动作 |
    状态：当前区域和使用Image-zooms及Pool45-crops的记忆向量。奖励：IOU变化。 | VGG-16 [[340](#bib.bib340)]
    | 每张图像检测到的物体非常少的区域提议 | Pascal VOC-2007图像数据集 [[90](#bib.bib90)]。 [可用代码](https://github.com/imatge-upc/detection-2016-nipsws)
    |'
- en: '| Visual Object Detection [[251](#bib.bib251)] | 2016 | Policy sampling and
    state transition algorithm | 2 actions: fixate and done, where each is a tuple
    of three. | States: Observed region history, evidence region history and fixate
    history. Reward: sensitive to detection location | Deep NN [[187](#bib.bib187)]
    | Comparable mAP and lower run time as compared to other methods such as to exhaustive
    sliding window search(SW), exhaustive search over the CPMC and region proposal
    set(RP) [[112](#bib.bib112)]  [[366](#bib.bib366)] | Pascal VOC 2012 Object detection
    challenge [[91](#bib.bib91)]. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 视觉物体检测 [[251](#bib.bib251)] | 2016 | 策略采样和状态转换算法 | 2个动作：固定和完成，每个动作是一个三元组
    | 状态：观察区域历史、证据区域历史和固定历史。奖励：对检测位置敏感 | 深度NN [[187](#bib.bib187)] | 与其他方法（如穷举滑动窗口搜索（SW）、对CPMC和区域提议集（RP）进行穷举搜索
    [[112](#bib.bib112)]  [[366](#bib.bib366)]）相比，mAP相当且运行时间较低 | Pascal VOC 2012物体检测挑战
    [[91](#bib.bib91)]。'
- en: '| Tree-Structured Sequential Object Localization (Tree-RL) [[170](#bib.bib170)]
    | 2016 | DQN | 13 actions: 8 translation, 5 scaling. | States: Feature vector
    of current region, and whole image. Reward: change in IOU. | CNN trained on ImageNet
    [[72](#bib.bib72)]  [[320](#bib.bib320)] | Tree-RL with faster R-CNN outperformed
    RPN with fast R-CNN [[108](#bib.bib108)] in terms of AP and comparable results
    to Faster R-CNN [[309](#bib.bib309)] | Pascal VOC 2007 [[90](#bib.bib90)] and
    2012 [[91](#bib.bib91)]. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 树状序列目标定位 (Tree-RL) [[170](#bib.bib170)] | 2016 | DQN | 13 个动作：8 个平移，5 个缩放。
    | 状态：当前区域和整个图像的特征向量。奖励：IOU 的变化。 | 在 ImageNet [[72](#bib.bib72)] [[320](#bib.bib320)]
    上训练的 CNN | Tree-RL 与 Faster R-CNN 结合，相比于使用 Fast R-CNN [[108](#bib.bib108)] 的 RPN，在
    AP 上表现更好，并与 Faster R-CNN [[309](#bib.bib309)] 结果相当 | Pascal VOC 2007 [[90](#bib.bib90)]
    和 2012 [[91](#bib.bib91)]。 |'
- en: '| Active Breast Lesion Detection [[246](#bib.bib246)] | 2017 | DQN | 9 actions:
    6 translation, 2 scaling, 1 trigger | States: feature vector of current region,
    Reward: improvement in localization. | ResNet [[133](#bib.bib133)] | Comparable
    true positive and false positive proportions as compared to SL [[253](#bib.bib253)]
    and Ms-C [[116](#bib.bib116)], but with lesser mean inference time. | DCE-MRI
    and T1-weighted anatomical dataset [[253](#bib.bib253)] |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 主动乳腺病变检测 [[246](#bib.bib246)] | 2017 | DQN | 9 个动作：6 个平移，2 个缩放，1 个触发 | 状态：当前区域的特征向量，奖励：定位改进。
    | ResNet [[133](#bib.bib133)] | 与 SL [[253](#bib.bib253)] 和 Ms-C [[116](#bib.bib116)]
    相比，具有相似的真实正例和假正例比例，但推断时间更短。 | DCE-MRI 和 T1 加权解剖数据集 [[253](#bib.bib253)] |'
- en: '| Multitask object localization [[386](#bib.bib386)] | 2018 | DQN | 8 actions:
    left, right, up, down, bigger, smaller, fatter and taller | States: feature vector,
    historical actions. Reward: change in IOU. different network for transformation
    actions and terminal actions. | Pretrained VGG-16 [[340](#bib.bib340)] with ImageNet
    [[72](#bib.bib72)]  [[320](#bib.bib320)] | Better mAP as compared to MultiBox
    [[89](#bib.bib89)], Caicedo et al. [[43](#bib.bib43)] and second best to R-CNN
    [[109](#bib.bib109)]. | Pascal VOC-2007 Image Dataset [[90](#bib.bib90)]. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 多任务目标定位 [[386](#bib.bib386)] | 2018 | DQN | 8 个动作：左、右、上、下、变大、变小、更胖和更高 | 状态：特征向量、历史动作。奖励：IOU
    的变化。不同的网络用于变换动作和终端动作。 | 使用 ImageNet [[72](#bib.bib72)] 预训练的 VGG-16 [[340](#bib.bib340)]
    [[320](#bib.bib320)] | 与 MultiBox [[89](#bib.bib89)]、Caicedo 等 [[43](#bib.bib43)]
    相比，具有更好的 mAP，次于 R-CNN [[109](#bib.bib109)]。 | Pascal VOC-2007 图像数据集 [[90](#bib.bib90)]。
    |'
- en: '| Bounding-Box Automated Refinement [[15](#bib.bib15)] | 2020 | DQN | 8 actions:
    up, down, left, right, bigger, smaller, fatter, taller | Offline and online implementation
    States: feature vector for offline (BAR-DRL), HOG for online (BAR-CB). Reward:
    change in IOU | ResNet50 [[133](#bib.bib133)] | Better final IOU for boxes generated
    by methods such as RetinaNet [[225](#bib.bib225)]. | Pascal VOC-2007 [[90](#bib.bib90)],
    2012 [[91](#bib.bib91)] Image Dataset. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 边界框自动精炼 [[15](#bib.bib15)] | 2020 | DQN | 8 个动作：上、下、左、右、变大、变小、更胖、更高 | 离线和在线实现状态：离线（BAR-DRL）的特征向量，在线（BAR-CB）的
    HOG。奖励：IOU 的变化 | ResNet50 [[133](#bib.bib133)] | 与 RetinaNet [[225](#bib.bib225)]
    等方法生成的框相比，最终 IOU 更好。 | Pascal VOC-2007 [[90](#bib.bib90)]，2012 [[91](#bib.bib91)]
    图像数据集。 |'
- en: '| Efficient Object Detection in Large Images [[367](#bib.bib367)] | 2020 |
    DQN | binary action array: where 1 means that the agent would consider acquiring
    sub-patches for that particular patch | Course CPNet and fine FPNet level search.
    States: selected region. Reward: detection recall image acquisition cost. Policy:
    REINFORCE [[351](#bib.bib351)] | ResNet32 [[133](#bib.bib133)] for policy network.
    and YOLOv3 [[306](#bib.bib306)] with DarkNet-53 for Object detector | Higher mAP
    and lower run time as compared to other methods such as [[99](#bib.bib99)]. |
    Caltech Pedestrian dataset (CPD) [[77](#bib.bib77)] [Available Code](https://github.com/uzkent/EfficientObjectDetection)
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 大图像中的高效目标检测 [[367](#bib.bib367)] | 2020 | DQN | 二进制动作数组：其中 1 表示智能体会考虑获取特定补丁的子补丁
    | 课程 CPNet 和细化 FPNet 层级搜索。状态：选择的区域。奖励：检测召回图像获取成本。策略：REINFORCE [[351](#bib.bib351)]
    | 策略网络使用 ResNet32 [[133](#bib.bib133)]，目标检测器使用 YOLOv3 [[306](#bib.bib306)] 和 DarkNet-53
    | 相比于其他方法，如 [[99](#bib.bib99)]，具有更高的 mAP 和更低的运行时间。 | Caltech 行人数据集 (CPD) [[77](#bib.bib77)]
    [可用代码](https://github.com/uzkent/EfficientObjectDetection) |'
- en: '| Organ Localization in CT [[275](#bib.bib275)] | 2020 | DQN | 11 actions:
    6 translation, 2 scaling, 3 deformation | States: region inside the Bounding box.
    Reward: change in IOU. | Architecture similar to [[10](#bib.bib10)] | Lower distance
    error for organ localization and run time as compared to other methods such as
    3D-RCNN [[409](#bib.bib409)] and CNNs [[152](#bib.bib152)] | CT scans from the
    VISCERAL dataset [[171](#bib.bib171)] |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| CT中器官定位 [[275](#bib.bib275)] | 2020 | DQN | 11种动作：6种平移，2种缩放，3种变形 | 状态：边界框内的区域。奖励：IOU的变化。
    | 架构类似于 [[10](#bib.bib10)] | 相较于其他方法如3D-RCNN [[409](#bib.bib409)] 和 CNNs [[152](#bib.bib152)]，器官定位的距离误差和运行时间更低
    | 来自VISCERAL数据集的CT扫描 [[171](#bib.bib171)] |'
- en: '| Monocular 3D Object Detection [[231](#bib.bib231)] | 2020 | DQN [[264](#bib.bib264)]
    | 15 actions, each modifies the 3D bounding box in a specific parameter | State:
    3D bounding box parameters, 2D image of object cropped by 2D its detected bounding
    box. Reward: accuracy improvement after applying an action. | ResNet-101 [[133](#bib.bib133)]
    | Higher average precision (AP) compared to [[268](#bib.bib268)], [[302](#bib.bib302)],
    [[210](#bib.bib210)] and [[35](#bib.bib35)] | KITTI [[102](#bib.bib102)] |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 单目3D目标检测 [[231](#bib.bib231)] | 2020 | DQN [[264](#bib.bib264)] | 15种动作，每种动作修改3D边界框的一个特定参数
    | 状态：3D边界框参数，2D图像由2D检测边界框裁剪出的对象。奖励：应用动作后的准确性提升。 | ResNet-101 [[133](#bib.bib133)]
    | 相比于 [[268](#bib.bib268)]、[[302](#bib.bib302)]、[[210](#bib.bib210)] 和 [[35](#bib.bib35)]，平均精度（AP）更高
    | KITTI [[102](#bib.bib102)] |'
- en: Localization of organs in CT scans is an important pre-processing requirement
    for taking the images of an organ, planning radiotherapy, etc. A DRL method for
    organ localization was proposed by [[275](#bib.bib275)], where the problem was
    formulated as an MDP. In the implementation, the agent was responsible for predicting
    a 3D bounding box around the organ. The authors used the last 4 states as input
    to the agent to stabilize the search and the action space consists of Eleven actions,
    6 for the position of the bounding box, 2 for zoom in and zoom out the action,
    and last 3 for height, width, and depth. For Reward, they used the change the
    in Intersection over union (IOU) across an action.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: CT扫描中器官的定位是获取器官图像、计划放射治疗等的重要预处理要求。[[275](#bib.bib275)] 提出了用于器官定位的DRL方法，其中问题被形式化为MDP。在实现中，智能体负责预测围绕器官的3D边界框。作者使用了最后4个状态作为智能体的输入，以稳定搜索，动作空间包括11种动作：6种用于边界框的位置，2种用于放大和缩小，最后3种用于高度、宽度和深度。奖励方面，他们使用了跨动作的交并比（IOU）的变化。
- en: Monocular 3D object detection is a problem where 3D bounding boxes of objects
    are required to be detected from a single 2D image. Even the sampling-based method
    is the SOTA approach, it has a huge flaw, in which most of the samples it generates
    do not overlap with the groundtruth. To leverage that method, [[231](#bib.bib231)]
    introduced Reinforced Axial Refinement Network (RARN) for monocular 3D object
    detection by utilizing an RL model to iteratively refining the sampled bounding
    box to be more overlapped with the groundtruth bounding box. Given a state having
    the coordinates of the 3D bounding box and image patch of the image, the model
    predicts an action out of a set of 15 actions to refine one of the bounding box
    coordinates in a direction at every timestep, the model is trained by DQN method
    with the immediate reward is the improvement in detection accuracy between every
    pair of timesteps. The whole pipeline, namely RAR-Net, was evaluated on the real-world
    KITTI dataset [[102](#bib.bib102)] and achieved state-of-the-art performance.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 单目3D目标检测是一个需要从单个2D图像中检测对象的3D边界框的问题。尽管基于采样的方法是SOTA方法，但它有一个巨大的缺陷，即它生成的大多数样本与真实标签不重叠。为了解决这个问题，[[231](#bib.bib231)]
    引入了强化轴向细化网络（RARN），通过利用强化学习模型迭代地细化采样的边界框，使其与真实边界框的重叠度更高。在给定一个状态，该状态具有3D边界框的坐标和图像的图像补丁时，模型在每个时间步中从15个动作中预测一个动作，以在一个方向上细化边界框坐标，模型通过DQN方法训练，其即时奖励是每对时间步之间检测准确度的改善。整个管道，即RAR-Net，在真实世界的KITTI数据集
    [[102](#bib.bib102)] 上进行了评估，并取得了最先进的性能。
- en: 'All these methods have been summarised and compared in Table LABEL:tab:obs,
    and a basic implementation of object detection using DRL has been shown in Fig.
    [12](#S6.F12 "Figure 12 ‣ 6 DRL in Object Detection ‣ Deep Reinforcement Learning
    in Computer Vision: A Comprehensive Survey"). The figure illustrates a general
    implementation of object detection using DRL, where the state is an image segment
    cropped using a bounding box produced by some other algorithm or previous iteration
    of DRL, actions predicted by the DRL agent predict a series of bounding box transformation
    to fit the object better, hence forming a new state and Reward is the improvement
    in Intersection over union (IOU) with iterations as used by [[43](#bib.bib43)],[[25](#bib.bib25)],[[15](#bib.bib15)],[[386](#bib.bib386)],[[170](#bib.bib170)],[[275](#bib.bib275)].'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法都已在表 LABEL:tab:obs 中总结并比较，并且使用 DRL 的物体检测的基本实现已展示在图 [12](#S6.F12 "图 12
    ‣ 6 DRL 在物体检测中的应用 ‣ 深度强化学习在计算机视觉中的综合调查")。该图展示了使用 DRL 进行物体检测的一般实现，其中状态是使用由其他算法或
    DRL 先前迭代产生的边界框裁剪的图像段，DRL 代理预测的动作预测一系列边界框变换，以更好地适应物体，从而形成新的状态，奖励是随着迭代而提高的交并比（IOU），参考了[[43](#bib.bib43)],
    [[25](#bib.bib25)], [[15](#bib.bib15)], [[386](#bib.bib386)], [[170](#bib.bib170)],
    [[275](#bib.bib275)]。
- en: '![Refer to caption](img/58a1d5d89631a83968289981f8c75065.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/58a1d5d89631a83968289981f8c75065.png)'
- en: 'Figure 12: DRL implementation for object detection. The red box corresponds
    to the initial bounding box which for t=0 is predicted by some other algorithm
    or the transformed bounding box by previous iterations of DRL using the actions
    to maximize the improvement in IOU.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：DRL 在物体检测中的实现。红色框对应于初始边界框，对于 t=0，由其他算法预测或由 DRL 先前迭代使用动作变换得到的边界框，以最大化 IOU
    的改进。
- en: 7 DRL in Object Tracking
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 DRL 在物体跟踪中的应用
- en: 'Real-time object tracking has a large number of applications in the field of
    autonomous driving, robotics, security, and even in sports where the umpire needs
    accurate estimation of ball movement to make decisions. Object tracking can be
    divided into two main categories: Single object tracking (SOT) and Multiple object
    tracking (MOT).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 实时物体跟踪在自动驾驶、机器人技术、安全监控以及运动场景中有着广泛的应用，比如裁判需要准确估计球的运动以做出决策。物体跟踪可以分为两大类：单目标跟踪（SOT）和多目标跟踪（MOT）。
- en: Many attempts have been made for both SOT and MOT. SOT can be divided into two
    types, active and passive. In passive tracking it is assumed that the object that
    is being tracked is always in the camera frame, hence camera movement is not required.
    In active tracking, however, the decision to move the camera frame is required
    so that the object is always in the frame. Passive tracking has been performed
    by [[397](#bib.bib397)], [[146](#bib.bib146)], where [[146](#bib.bib146)] performed
    tracking for both single and multiple objects. The authors of these papers proposed
    various solutions to overcome common problems such as a change in lighting and
    occlusion. Active tracking is a little bit harder as compared to a passive one
    because additional decisions are required for camera movement. Some efforts towards
    active tracking include [[74](#bib.bib74)] [[270](#bib.bib270)] [[178](#bib.bib178)].
    These solutions treat object detection and object tracking as two separate tasks
    and tend to fail when there is background noise.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SOT 和 MOT 都做出了许多尝试。SOT 可以分为两种类型，主动和被动。在被动跟踪中，假设被跟踪的物体始终在摄像头视野中，因此不需要移动摄像头。而在主动跟踪中，则需要决定是否移动摄像头，以确保物体始终在视野中。被动跟踪已经由
    [[397](#bib.bib397)], [[146](#bib.bib146)] 执行，其中 [[146](#bib.bib146)] 进行了单目标和多目标的跟踪。这些论文的作者提出了各种解决方案来克服常见问题，如光照变化和遮挡。与被动跟踪相比，主动跟踪稍微困难一些，因为需要额外的摄像头移动决策。针对主动跟踪的一些努力包括
    [[74](#bib.bib74)] [[270](#bib.bib270)] [[178](#bib.bib178)]。这些解决方案将物体检测和物体跟踪视为两个独立的任务，当存在背景噪声时往往会失败。
- en: An end-to-end active object tracker using DRL was proposed by [[240](#bib.bib240)],
    where the authors used CNNs along with an LSTM [[139](#bib.bib139)] in their implementation.
    They used the actor-critic algorithm [[262](#bib.bib262)] to calculate the probability
    distribution of different actions and the value of state and used the object orientation
    and distance from the camera to calculate rewards. For experiments, the authors
    used VizDoom and Unreal Engine as the environment.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了一个使用深度强化学习（DRL）的端到端主动物体跟踪器[[240](#bib.bib240)]，其中作者在实现中使用了卷积神经网络（CNNs）和长短期记忆（LSTM）
    [[139](#bib.bib139)]。他们使用了演员-评论家算法 [[262](#bib.bib262)] 来计算不同动作的概率分布和状态的价值，并利用物体的方向和与相机的距离来计算奖励。实验中，作者使用了VizDoom和Unreal
    Engine作为环境。
- en: Another end-to-end method for SOT using sequential search strategy and DRL was
    proposed by [[418](#bib.bib418)]. The method included using an RNN along with
    REINFORCE [[392](#bib.bib392)] algorithm to train the network. The authors used
    a function $f(W_{0})$ that takes in $S_{t}$ and frame as input, where $S_{t}$
    is the object location for the first frame and is zero elsewhere. The output is
    fed to an LSTM module [[139](#bib.bib139)] with past hidden state $h_{t}$. The
    authors calculated the reward function by using insertion over union (IoU) and
    the difference between the average and max.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 使用序列搜索策略和深度强化学习（DRL）提出了一种用于SOT的端到端方法[[418](#bib.bib418)]。该方法包括使用递归神经网络（RNN）和REINFORCE
    [[392](#bib.bib392)] 算法来训练网络。作者使用了一个函数 $f(W_{0})$，它以 $S_{t}$ 和帧作为输入，其中 $S_{t}$
    是第一帧的目标位置，其余位置为零。输出结果被送入具有过去隐藏状态 $h_{t}$ 的长短期记忆（LSTM）模块 [[139](#bib.bib139)]。作者通过使用插入与并集（IoU）以及平均值和最大值之间的差异来计算奖励函数。
- en: 'A deformable face tracking method that could predict bounding box along with
    facial landmarks in real-time was proposed by [[118](#bib.bib118)]. The dual-agent
    DRL method (DADRL) mentioned in the paper consisted of two agents: a tracking
    and an alignment agent. The problem of object tracking was formulated as an MDP
    where state consisted of image regions extracted by the bounding box and a total
    of 8 actions (left, right, up, down, scale-up, scale down, stop and continue)
    were used, where first six consists of movement actions used by tracking agent
    and last two for alignment agent. The tracking agent is responsible for changing
    the current observable region and the alignment agent determines whether the iteration
    should be terminated. For the tracking agent, the reward corresponded to the misalignment
    descent and for the alignment agent the reward was $+\eta$ for misalignment less
    than the threshold and $-\eta$ otherwise. The DADRL implementation also consisted
    of communicated message channels beside the tracking agent and the alignment agent.
    The tracking agent consisted of a VGG-M [[340](#bib.bib340)] backbone followed
    by a one-layer Q-Network and the alignment agent was designed as a combination
    of a stacked hourglass network with a confidence network. The two communicated
    message channels were encoded by a deconvolution layer and an LSTM unit [[139](#bib.bib139)]
    respectively.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了一个可实时预测边界框及面部地标的可变形面部跟踪方法[[118](#bib.bib118)]。论文中提到的双代理深度强化学习（DADRL）方法由两个代理组成：跟踪代理和对齐代理。物体跟踪问题被构建为一个马尔可夫决策过程（MDP），状态由通过边界框提取的图像区域组成，总共使用了8个动作（左、右、上、下、放大、缩小、停止和继续），其中前六个是跟踪代理使用的移动动作，最后两个是对齐代理使用的动作。跟踪代理负责更改当前可观察区域，而对齐代理决定是否终止迭代。对于跟踪代理，奖励与对齐误差下降相关，对于对齐代理，当对齐误差小于阈值时奖励为
    $+\eta$，否则为 $-\eta$。DADRL 实现还包括了除了跟踪代理和对齐代理之外的通信消息通道。跟踪代理包括一个VGG-M [[340](#bib.bib340)]
    主干，后接一层Q-网络，而对齐代理设计为堆叠的沙漏网络与置信度网络的组合。这两个通信消息通道分别通过反卷积层和LSTM单元 [[139](#bib.bib139)]
    进行编码。
- en: 'Visual object tracking when dealing with deformations and abrupt changes can
    be a challenging task. A DRL method for object tracking with iterative shift was
    proposed by [[308](#bib.bib308)]. The approach (DRL-IS) consisted of three networks:
    The actor network, the prediction network, and the critic network, where all three
    networks shared the same CNN and a fully connected layer. Given the initial frame
    and bounding box, the cropped frame is fed to the CNNs to extract the features
    to be used as a state by the networks. The actions included continue, stop and
    update, stop and ignore, and restart. For continue, the bounding boxes are adjusted
    according to the output of the prediction network, for stop and update the iteration
    is stopped and the appearance feature of the target is updated according to the
    prediction network, for stop and ignore the updating of target appearance feature
    is ignored and restart means that the target is lost and the algorithm needs to
    start from the initial bounding box. The authors of the paper used reward as 1
    for change in IoU greater than the threshold, 0 for change in IOU between + and
    - threshold, and -1 otherwise.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 处理变形和突发变化时的视觉对象跟踪可能是一项具有挑战性的任务。[[308](#bib.bib308)] 提出了一个用于对象跟踪的迭代移位 DRL 方法。该方法（DRL-IS）由三个网络组成：演员网络、预测网络和评论网络，所有三个网络共享相同的
    CNN 和一个全连接层。给定初始帧和边界框，将裁剪后的帧输入 CNN，以提取用于网络状态的特征。动作包括继续、停止并更新、停止并忽略以及重启。对于继续，边界框根据预测网络的输出进行调整；对于停止并更新，迭代被停止，并根据预测网络更新目标的外观特征；对于停止并忽略，忽略目标外观特征的更新；重启意味着目标丢失，算法需要从初始边界框重新开始。论文的作者将奖励设置为：IoU
    变化大于阈值为 1，IoU 变化在阈值的 + 和 - 之间为 0，否则为 -1。
- en: Considering the performance of actor-critic framework for various applications,
    [[45](#bib.bib45)] proposed an actor-critic [[262](#bib.bib262)] framework for
    real-time object tracking. The authors of the paper used a pre-processing function
    to obtain an image patch using the bounding box that is fed into the network to
    find the bounding box location in subsequent frames. For actions the authors used
    $\triangle x$ for relative horizontal translation, $\triangle y$ for relative
    vertical translation, and $\triangle s$ for relative scale change, and for a reward
    they used 1 for IoU greater than a threshold and -1 otherwise. They proposed offline
    training and online tracking, where for offline training a pre-trained VGG-M [[340](#bib.bib340)]
    was used as a backbone, and the actor-critic network was trained using the DDPG
    approach [[224](#bib.bib224)].
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 针对演员-评论框架在各种应用中的表现，[[45](#bib.bib45)] 提出了一个用于实时对象跟踪的演员-评论 [[262](#bib.bib262)]
    框架。论文的作者使用预处理函数通过边界框获取图像补丁，将其输入网络以在后续帧中找到边界框位置。对于动作，作者使用了 $\triangle x$ 表示相对水平位移，$\triangle
    y$ 表示相对垂直位移，以及 $\triangle s$ 表示相对尺度变化，奖励则使用 IoU 大于阈值为 1，否则为 -1。他们提出了离线训练和在线跟踪，其中离线训练使用了预训练的
    VGG-M [[340](#bib.bib340)] 作为主干，并使用 DDPG 方法 [[224](#bib.bib224)] 对演员-评论网络进行训练。
- en: 'An improvement to [[45](#bib.bib45)] for SOT was proposed by [[84](#bib.bib84)],
    where a visual tracker was formulated using DRL and an expert demonstrator. The
    authors treated the problem as an MDP, where the state consists of two consecutive
    frames that have been cropped using the bounding box corresponding to the former
    frame and used a scaling factor to control the offset while cropping. The actions
    consisted of four elements: $\triangle x$ for relative horizontal translation,
    $\triangle y$ for relative vertical translation, $\triangle w$ for width scaling,
    and $\triangle h$ for height scaling, and the reward was calculated by considering
    whether the IoU is greater than a threshold or not. For the agent architecture
    the authors used a ResNet-18 [[133](#bib.bib133)] as backbone followed by an LSTM
    unit [[391](#bib.bib391)][[139](#bib.bib139)] to encode past information, and
    performed training based on the on-policy A3C framework [[262](#bib.bib262)].'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 对 [[45](#bib.bib45)] 的 SOT 改进由 [[84](#bib.bib84)] 提出，其中使用 DRL 和专家演示者制定了一个视觉跟踪器。作者将问题视为一个
    MDP，其中状态由使用前一个帧的边界框裁剪得到的两个连续帧组成，并使用缩放因子控制裁剪时的偏移。动作包括四个元素：$\triangle x$ 表示相对水平位移，$\triangle
    y$ 表示相对垂直位移，$\triangle w$ 表示宽度缩放，$\triangle h$ 表示高度缩放，奖励通过判断 IoU 是否大于阈值来计算。对于代理架构，作者使用了
    ResNet-18 [[133](#bib.bib133)] 作为主干，后接 LSTM 单元 [[391](#bib.bib391)][[139](#bib.bib139)]
    编码过去的信息，并基于 on-policy A3C 框架 [[262](#bib.bib262)] 进行训练。
- en: 'In MOT the algorithm is responsible to track trajectories of multiple objects
    in the given video. Many attempts have been made with MOT including [[53](#bib.bib53)],
    [[55](#bib.bib55)] and [[143](#bib.bib143)]. However, MOT is a challenging task
    because of environmental constraints such as crowding or object overlapping. MOT
    can be divided into two main techniques: Offline [[53](#bib.bib53)] and Online
    [[55](#bib.bib55)] [[143](#bib.bib143)]. In offline batch, tracking is done using
    a small batch to obtain tracklets and later all these are connected to obtain
    a complete trajectory. The online method includes using present and past frames
    to calculate the trajectory. Some common methods include Kalman filtering [[177](#bib.bib177)],
    Particle Filtering [[284](#bib.bib284)] or Markov decision [[401](#bib.bib401)].
    These techniques however are prone to errors due to environmental constraints.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在MOT（多目标跟踪）中，算法负责跟踪给定视频中多个物体的轨迹。MOT经历了许多尝试，包括[[53](#bib.bib53)]、[[55](#bib.bib55)]和[[143](#bib.bib143)]。然而，由于环境约束，如拥挤或物体重叠，MOT是一个具有挑战性的任务。MOT可以分为两种主要技术：离线[[53](#bib.bib53)]和在线[[55](#bib.bib55)]
    [[143](#bib.bib143)]。在离线批处理模式下，跟踪通过使用小批量数据来获取轨迹片段，然后将这些片段连接起来以获得完整轨迹。在线方法包括使用当前帧和过去帧来计算轨迹。一些常见的方法包括卡尔曼滤波[[177](#bib.bib177)]、粒子滤波[[284](#bib.bib284)]或马尔可夫决策[[401](#bib.bib401)]。然而，这些技术由于环境约束容易产生错误。
- en: 'To overcome the constraints of MOT by previous methods, [[401](#bib.bib401)]
    proposed a method for MOT where the problem was approached as an MDP. The authors
    tracked each object in the frame through the Markov decision process, where each
    object has four states consisting: Active, Tracked, Lost, and Inactive. Object
    detection is the active state and when the object is in the lost state for a sufficient
    amount of time it is considered Inactive, which is the terminal state. The reward
    function in the implementation was learned through data by inverse RL problem
    [[279](#bib.bib279)].'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服以前方法中MOT的限制，[[401](#bib.bib401)]提出了一种将问题视为MDP（马尔可夫决策过程）的MOT方法。作者通过马尔可夫决策过程跟踪每一帧中的物体，每个物体有四个状态：活动、跟踪、丢失和非活动。物体检测处于活动状态，当物体在丢失状态下持续一定时间后被认为是非活动状态，即终止状态。实现中的奖励函数通过逆向强化学习问题[[279](#bib.bib279)]从数据中学习得到。
- en: Previous approaches for MOT follow a tracking by detection technique that is
    prone to errors. An improvement was proposed by [[307](#bib.bib307)], where detection
    and tracking of the objects were carried out simultaneously. The authors used
    a collaborative Q-Network to track trajectories of multiple objects, given the
    initial position of an object the algorithm tracked the trajectory of that object
    in all subsequent frames. For actions the authors used $\triangle x$ for relative
    horizontal translation, $\triangle y$ for relative vertical translation, $\triangle
    w$ for width scaling, and $\triangle h$ for height scaling, and the reward consisted
    of values 1,0,-1 based on the IoU.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的MOT方法采用了容易出错的检测跟踪技术。[[307](#bib.bib307)]提出了一种改进方案，其中物体的检测和跟踪同时进行。作者使用了协作Q-网络来跟踪多个物体的轨迹，给定物体的初始位置，算法跟踪该物体在所有后续帧中的轨迹。对于动作，作者使用了$\triangle
    x$表示相对水平平移，$\triangle y$表示相对垂直平移，$\triangle w$表示宽度缩放，$\triangle h$表示高度缩放，奖励值由基于IoU的1、0和-1组成。
- en: 'Another method for MOT was proposed by [[168](#bib.bib168)], where the authors
    used LSTM [[139](#bib.bib139)] and DRL to approach the problem of multi-object
    tracking. The method described in the paper used three basic components: a YOLO
    V2 [[260](#bib.bib260)] object detector, many single object trackers, and a data
    association module. Firstly the YOLO V2 object detector is used to find objects
    in a frame, then each detected object goes through the agent which consists of
    CNN followed by an LSTM to encode past information for the object. The state consisted
    of the image patch and history of past 10 actions, where six actions (right, left,
    up, down, scale-up, scale down) were used for bounding box movement across the
    frame with a stop action for the terminal state. To provide reinforcement to the
    agent the reward was 1 if the IOU is greater than a threshold and 0 otherwise.
    In their experiments, the authors used VGG-16 [[340](#bib.bib340)] for CNN backbone
    and performed experiments on MOT benchmark [[201](#bib.bib201)] for people tracking.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于 MOT 的方法由 [[168](#bib.bib168)] 提出，作者使用了 LSTM [[139](#bib.bib139)] 和 DRL
    来解决多目标跟踪问题。文中描述的方法使用了三个基本组件：一个 YOLO V2 [[260](#bib.bib260)] 物体检测器、多个单目标跟踪器以及一个数据关联模块。首先，YOLO
    V2 物体检测器用于在帧中找到物体，然后每个检测到的物体经过由 CNN 组成的代理，接着是 LSTM，用于对物体的过去信息进行编码。状态包括图像块和过去 10
    次动作的历史，其中六个动作（右、左、上、下、放大、缩小）用于在帧中移动边界框，终态则有一个停止动作。为了给代理提供强化，如果 IOU 大于阈值，奖励为 1，否则为
    0。在他们的实验中，作者使用了 VGG-16 [[340](#bib.bib340)] 作为 CNN 主干，并在 MOT 基准 [[201](#bib.bib201)]
    上进行了人群跟踪实验。
- en: 'Table 5: Comparing various DRL-based object tracking methods. The First group
    for Single object tracking (SOT) and the second group for multi-object tracking
    (MOT)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：比较各种基于 DRL 的物体跟踪方法。第一组为单目标跟踪（SOT），第二组为多目标跟踪（MOT）。
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and Source code |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 训练技术 | 动作 | 备注 | 主干网络 | 性能 | 数据集和源代码 |'
- en: '| End to end active object tracking [[240](#bib.bib240)] | 2017 | Actor-Critic
    (a3c) [[262](#bib.bib262)] | 6 actions: turn left, turn right, turn left and move
    forward, turn right and move forward, move forward, no-op | Environment: virtual
    environment. Reward: calculated using object orientation and position. Tracking
    Using LSTM [[139](#bib.bib139)] | ConvNet-LSTM | Higher accumulated reward and
    episode length as compared to methods like MIL [[17](#bib.bib17)], Meanshift [[60](#bib.bib60)],
    KCF [[134](#bib.bib134)]. | ViZDoom [[176](#bib.bib176)], Unreal Engine |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 端到端主动物体跟踪 [[240](#bib.bib240)] | 2017 | Actor-Critic (a3c) [[262](#bib.bib262)]
    | 6 个动作：向左转、向右转、向左转并前进、向右转并前进、前进、无操作 | 环境：虚拟环境。奖励：根据物体方向和位置计算。使用 LSTM [[139](#bib.bib139)]
    进行跟踪 | ConvNet-LSTM | 相较于 MIL [[17](#bib.bib17)]、Meanshift [[60](#bib.bib60)]、KCF
    [[134](#bib.bib134)] 等方法，获得了更高的累计奖励和回合长度。 | ViZDoom [[176](#bib.bib176)]，虚幻引擎
    |'
- en: '| DRL for object tracking [[418](#bib.bib418)] | 2017 | DRLT | None | State:
    feature vector, Reward: change in IOU use of LSTM [[139](#bib.bib139)] and REINFORCE
    [[392](#bib.bib392)] | YOLO network [[305](#bib.bib305)] | Higher area under curve
    (success rate Vs overlap threshold), precision and speed (fps) as compared to
    STUCK [[126](#bib.bib126)] and DLT [[384](#bib.bib384)]. | Object tracking benchmark
    [[397](#bib.bib397)]. [Available Code](https://github.com/fgabel/Deep-Reinforcement-Learning-for-Visual-Object-Tracking-in-Videos)
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 用于物体跟踪的 DRL [[418](#bib.bib418)] | 2017 | DRLT | 无 | 状态：特征向量，奖励：IOU 变化，使用
    LSTM [[139](#bib.bib139)] 和 REINFORCE [[392](#bib.bib392)] | YOLO 网络 [[305](#bib.bib305)]
    | 相较于 STUCK [[126](#bib.bib126)] 和 DLT [[384](#bib.bib384)]，具有更高的曲线下面积（成功率 vs
    重叠阈值）、精度和速度（fps）。 | 物体跟踪基准 [[397](#bib.bib397)]。 [代码可用](https://github.com/fgabel/Deep-Reinforcement-Learning-for-Visual-Object-Tracking-in-Videos)
    |'
- en: '| Dual-agent deformable face tracker [[118](#bib.bib118)] | 2018 | DQN | 8
    actions: left, right, up, down, scale up, scale down, stop and continue. | States:
    image region using Bounding box. Reward: distance error. Facial landmark detection
    and tracking using LSTM [[139](#bib.bib139)] | VGG-M [[340](#bib.bib340)] | Lower
    normalized point to point error for landmarks and higher success rate for facial
    tracking as compared to ICCR [[187](#bib.bib187)], MDM [[336](#bib.bib336)], Xiao
    et al [[32](#bib.bib32)], etc. | Large-scale face tracking dataset, the 300-VW
    test set [[336](#bib.bib336)] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 双代理可变形面部跟踪器 [[118](#bib.bib118)] | 2018 | DQN | 8 种动作：左、右、上、下、放大、缩小、停止和继续。
    | 状态：使用边界框的图像区域。奖励：距离误差。使用 LSTM [[139](#bib.bib139)] 进行面部标志检测和跟踪 | VGG-M [[340](#bib.bib340)]
    | 相较于 ICCR [[187](#bib.bib187)]、MDM [[336](#bib.bib336)]、Xiao 等 [[32](#bib.bib32)]，标志点的归一化点对点误差更低，面部跟踪成功率更高
    | 大规模面部跟踪数据集，300-VW 测试集 [[336](#bib.bib336)] |'
- en: '| Tracking with iterative shift [[308](#bib.bib308)] | 2018 | Actor-critic
    [[262](#bib.bib262)] | 4 actions: continue, stop and update, stop and ignore and
    restart | States: image region using bounding box. Reward: change in IOU. Three
    networks: actor, critic and prediction network | 3 Layer CNN and FC layer | Higher
    area under curve for success rate Vs overlap threshold and precision Vs location
    error threshold as compared to CREST [[345](#bib.bib345)], ADNet [[416](#bib.bib416)],
    MDNet [[273](#bib.bib273)], HCFT [[243](#bib.bib243)], SINT [[358](#bib.bib358)],
    DeepSRDCF [[67](#bib.bib67)], and HDT [[301](#bib.bib301)] | OTB-2015 [[398](#bib.bib398)],
    Temple-Color [[220](#bib.bib220)], and VOT-2016 Dataset [[186](#bib.bib186)] |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 迭代移动跟踪 [[308](#bib.bib308)] | 2018 | Actor-critic [[262](#bib.bib262)] |
    4 种动作：继续、停止并更新、停止并忽略、重启 | 状态：使用边界框的图像区域。奖励：IOU 的变化。三个网络：actor、critic 和 prediction
    网络 | 3 层 CNN 和 FC 层 | 相较于 CREST [[345](#bib.bib345)]、ADNet [[416](#bib.bib416)]、MDNet
    [[273](#bib.bib273)]、HCFT [[243](#bib.bib243)]、SINT [[358](#bib.bib358)]、DeepSRDCF
    [[67](#bib.bib67)] 和 HDT [[301](#bib.bib301)]，成功率与重叠阈值的曲线下面积更高，以及精度与位置误差阈值的曲线下面积更高
    | OTB-2015 [[398](#bib.bib398)]、Temple-Color [[220](#bib.bib220)] 和 VOT-2016 数据集
    [[186](#bib.bib186)] |'
- en: '| Tracking with actor-critic [[45](#bib.bib45)] | 2018 | Actor-critic [[262](#bib.bib262)]
    | 3 actions: $\triangle x$, $\triangle y$ and $\triangle s$ | States: image region
    using bounding box. Reward: IOU greater then threshold. Offline training, online
    tracking | VGG-M [[340](#bib.bib340)] | Higher average precision score then PTAV
    [[93](#bib.bib93)], CFNet [[368](#bib.bib368)], ACFN [[52](#bib.bib52)], SiameFC
    [[29](#bib.bib29)], ECO-HC [[67](#bib.bib67)], etc. | OTB-2013 [[397](#bib.bib397)],
    OTB-2015 [[398](#bib.bib398)] and VOT-2016 dataset [[186](#bib.bib186)] [Available
    Code](https://github.com/bychen515/ACT) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 使用 actor-critic 的跟踪 [[45](#bib.bib45)] | 2018 | Actor-critic [[262](#bib.bib262)]
    | 3 种动作：$\triangle x$、$\triangle y$ 和 $\triangle s$ | 状态：使用边界框的图像区域。奖励：IOU 大于阈值。离线训练，在线跟踪
    | VGG-M [[340](#bib.bib340)] | 相较于 PTAV [[93](#bib.bib93)]、CFNet [[368](#bib.bib368)]、ACFN
    [[52](#bib.bib52)]、SiameFC [[29](#bib.bib29)]、ECO-HC [[67](#bib.bib67)] 等，平均精度分数更高
    | OTB-2013 [[397](#bib.bib397)]、OTB-2015 [[398](#bib.bib398)] 和 VOT-2016 数据集 [[186](#bib.bib186)]
    [可用代码](https://github.com/bychen515/ACT) |'
- en: '| Visual tracking and expert demonstrator [[84](#bib.bib84)] | 2019 | Actor-critic
    (a3c) [[262](#bib.bib262)] | 4 actions: $\triangle x$, $\triangle y$,$\triangle
    w$ and $\triangle h$ | States: image region using bounding box. Reward: change
    in IOU. SOT using LSTM [[391](#bib.bib391)][[139](#bib.bib139)] | ResNet-18 [[133](#bib.bib133)]
    | Comparable success and precision scores as compared to LADCF [[408](#bib.bib408)],
    SiamRPN [[209](#bib.bib209)] and ECO [[66](#bib.bib66)] | GOT-10k [[148](#bib.bib148)],
    LaSOT [[92](#bib.bib92)], UAV123 [[269](#bib.bib269)], OTB-100 [[397](#bib.bib397)],
    VOT-2018 [[185](#bib.bib185)] and VOT-2019. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 视觉跟踪和专家演示 [[84](#bib.bib84)] | 2019 | Actor-critic (a3c) [[262](#bib.bib262)]
    | 4 种动作：$\triangle x$、$\triangle y$、$\triangle w$ 和 $\triangle h$ | 状态：使用边界框的图像区域。奖励：IOU
    的变化。SOT 使用 LSTM [[391](#bib.bib391)][[139](#bib.bib139)] | ResNet-18 [[133](#bib.bib133)]
    | 相较于 LADCF [[408](#bib.bib408)]、SiamRPN [[209](#bib.bib209)] 和 ECO [[66](#bib.bib66)]，成功率和精度得分相当
    | GOT-10k [[148](#bib.bib148)]、LaSOT [[92](#bib.bib92)]、UAV123 [[269](#bib.bib269)]、OTB-100
    [[397](#bib.bib397)]、VOT-2018 [[185](#bib.bib185)] 和 VOT-2019 |'
- en: '| Object tracking by decision making [[401](#bib.bib401)] | 2015 | TLD Tracker
    [[174](#bib.bib174)] | 7 actions: corresponding to moving the object between states
    such as Active, tracked, lost and Inactive | States: 4 states: Active, tracked,
    lost and Inactive. Reward: inverse RL problem [[279](#bib.bib279)] | None | Comparable
    multiple object tracking accuracy (MOTA) and multiple object tracking precision
    (MOTP) [[28](#bib.bib28)] as compared to DPNMS [[296](#bib.bib296)], TCODAL [[18](#bib.bib18)],
    SegTrack [[259](#bib.bib259)], MotiCon [[200](#bib.bib200)], etc | M0T15 dataset
    [[201](#bib.bib201)] [Available Code](https://github.com/yuxng/MDP_Tracking) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 决策制定的目标跟踪 [[401](#bib.bib401)] | 2015 | TLD Tracker [[174](#bib.bib174)]
    | 7 种动作：对应于在活动、跟踪、丢失和非活动等状态之间移动目标 | 状态：4 种状态：活动、跟踪、丢失和非活动。奖励：逆向强化学习问题 [[279](#bib.bib279)]
    | 无 | 与 DPNMS [[296](#bib.bib296)]、TCODAL [[18](#bib.bib18)]、SegTrack [[259](#bib.bib259)]、MotiCon
    [[200](#bib.bib200)] 等相比，可比较的多目标跟踪准确率 (MOTA) 和多目标跟踪精度 (MOTP) [[28](#bib.bib28)]
    | M0T15 数据集 [[201](#bib.bib201)] [可用代码](https://github.com/yuxng/MDP_Tracking)
    |'
- en: '| Collaborative multi object tracker [[307](#bib.bib307)] | 2018 | DQN | 4
    actions: $\triangle x$, $\triangle y$, $\triangle w$ and $\triangle h$ | States:
    image region using bounding box. Reward: IOU greater then threshold. 2 networks:
    prediction and decision network | 3 Layer CNN and FC Layer | Comparable multiple
    object tracking accuracy (MOTA) and multiple object tracking precision (MOTP)
    [[28](#bib.bib28)] as compared to SCEA [[143](#bib.bib143)], MDP [[401](#bib.bib401)],
    CDADDALpb [[19](#bib.bib19)], AMIR15 [[321](#bib.bib321)] | MOT15 [[201](#bib.bib201)]
    and MOT16 [[258](#bib.bib258)] datasets |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 协作多目标跟踪器 [[307](#bib.bib307)] | 2018 | DQN | 4 种动作：$\triangle x$、$\triangle
    y$、$\triangle w$ 和 $\triangle h$ | 状态：使用边界框的图像区域。奖励：IOU 大于阈值。2 个网络：预测网络和决策网络 |
    3 层 CNN 和全连接层 | 与 SCEA [[143](#bib.bib143)]、MDP [[401](#bib.bib401)]、CDADDALpb
    [[19](#bib.bib19)]、AMIR15 [[321](#bib.bib321)] 相比，可比较的多目标跟踪准确率 (MOTA) 和多目标跟踪精度
    (MOTP) [[28](#bib.bib28)] | MOT15 [[201](#bib.bib201)] 和 MOT16 [[258](#bib.bib258)]
    数据集 |'
- en: '| Multi object tracking in video [[168](#bib.bib168)] | 2018 | DQN | 6 actions:
    right, left, up, down, scale up, scale down | States: image region using bounding
    box. Reward: IOU greater then threshold. Detection using YOLO-V2 [[260](#bib.bib260)]
    for detector and LSTM [[139](#bib.bib139)] . | VGG-16 [[340](#bib.bib340)] | Comparable
    if not better multiple object tracking accuracy (MOTA) and multiple object tracking
    precision (MOTP) [[28](#bib.bib28)] as compared to RNN-LSTM [[201](#bib.bib201)],
    LP-SSVM [[401](#bib.bib401)], MDPSubCNN [[199](#bib.bib199)], and SiameseCNN [[123](#bib.bib123)]
    | MOT15 Dataset [[201](#bib.bib201)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 视频中的多目标跟踪 [[168](#bib.bib168)] | 2018 | DQN | 6 种动作：右、左、上、下、放大、缩小 | 状态：使用边界框的图像区域。奖励：IOU
    大于阈值。使用 YOLO-V2 [[260](#bib.bib260)] 进行检测器和 LSTM [[139](#bib.bib139)]。 | VGG-16
    [[340](#bib.bib340)] | 与 RNN-LSTM [[201](#bib.bib201)]、LP-SSVM [[401](#bib.bib401)]、MDPSubCNN
    [[199](#bib.bib199)] 和 SiameseCNN [[123](#bib.bib123)] 相比，多目标跟踪准确率 (MOTA) 和多目标跟踪精度
    (MOTP) [[28](#bib.bib28)] 可比或更好 | MOT15 数据集 [[201](#bib.bib201)] |'
- en: '| Multi agent multi object tracker [[169](#bib.bib169)] | 2019 | DQN | 9 actions:
    move right, move left, move up, move down, scale up, scale down, fatter, taller
    and stop | States: image region using bounding box. Reward: IOU greater then threshold.
    YOLO-V3 [[306](#bib.bib306)] for detection and LSTM [[139](#bib.bib139)]. | VGG-16
    [[340](#bib.bib340)] | Higher running time, and comparable if not better multiple
    object tracking accuracy (MOTA) and multiple object tracking precision (MOTP)
    [[28](#bib.bib28)] as compared to RNN-LSTM [[201](#bib.bib201)], LP-SSVM [[401](#bib.bib401)],
    MDPSubCNN [[199](#bib.bib199)], and SiameseCNN [[123](#bib.bib123)] | MOT15 challenge
    benchmark [[201](#bib.bib201)]. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 多智能体多目标跟踪器 [[169](#bib.bib169)] | 2019 | DQN | 9 种动作：向右移动、向左移动、向上移动、向下移动、放大、缩小、变胖、变高和停止
    | 状态：使用边界框的图像区域。奖励：IOU 大于阈值。YOLO-V3 [[306](#bib.bib306)] 用于检测，LSTM [[139](#bib.bib139)]。
    | VGG-16 [[340](#bib.bib340)] | 更高的运行时间，与 RNN-LSTM [[201](#bib.bib201)]、LP-SSVM
    [[401](#bib.bib401)]、MDPSubCNN [[199](#bib.bib199)] 和 SiameseCNN [[123](#bib.bib123)]
    相比，多目标跟踪准确率 (MOTA) 和多目标跟踪精度 (MOTP) [[28](#bib.bib28)] 可比或更好 | MOT15 挑战基准 [[201](#bib.bib201)]。
    |'
- en: To address the problems in existing tracking methods such as varying numbers
    of targets, non-real-time tracking, etc, [[169](#bib.bib169)] proposed a multi-object
    tracking algorithm based on a multi-agent DRL tracker (MADRL). In their object
    tracking pipeline the authors used YOLO-V3 [[306](#bib.bib306)] as object detector,
    where multiple detections produced by YOLO-V3 were filtered using the IOU and
    the selected results were used as multiple agents in multiple agent detector.
    The input agents were fed into a pre-trained VGG-16 [[340](#bib.bib340)] followed
    by an LSTM unit [[139](#bib.bib139)] that could share information across agents
    and return the actions encoded in a 9-dimensional vector( move right, move left,
    move up, move down, scale-up, scale down, aspect ratio change fatter, aspect ratio
    change taller and stop), also a reward function similar to [[168](#bib.bib168)]
    was used.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决现有跟踪方法中的目标数量变化、非实时跟踪等问题，[[169](#bib.bib169)] 提出了基于多代理 DRL 跟踪器（MADRL）的多物体跟踪算法。在他们的物体跟踪流程中，作者使用了
    YOLO-V3 [[306](#bib.bib306)] 作为物体检测器，其中 YOLO-V3 产生的多个检测结果通过 IOU 进行过滤，选出的结果作为多个代理输入到多代理检测器中。输入的代理被输入到预训练的
    VGG-16 [[340](#bib.bib340)] 中，接着是一个 LSTM 单元 [[139](#bib.bib139)]，该单元可以在代理之间共享信息，并返回编码为
    9 维向量（向右移动、向左移动、向上移动、向下移动、放大、缩小、纵横比变胖、纵横比变高和停止）的动作，此外，还使用了类似于 [[168](#bib.bib168)]
    的奖励函数。
- en: 'Various works in the field of object tracking have been summarized in Table
    LABEL:tab:track, and a basic implementation of object tracking using DRL has been
    shown in Fig. [13](#S7.F13 "Figure 13 ‣ 7 DRL in Object Tracking ‣ Deep Reinforcement
    Learning in Computer Vision: A Comprehensive Survey"). The figure illustrates
    a general implementation of object tracking in videos using DRL, where the state
    consists of two consecutive frames $(F_{t},F_{t+1})$ with a bounding box for the
    first frame produced by another algorithm for the first iteration or by the previous
    iterations of DRL agent. The actions corresponds to the moving the bounding on
    the image to fit the object in frame $F_{t+1}$, hence forming a new state with
    frame $F_{t+1}$ and frame $F_{t+2}$ along with the bounding box for frame $F_{t+1}$
    predicted by previous iteration and reward corresponds to whether IOU is greater
    then a given threshold as used by [[118](#bib.bib118)],[[308](#bib.bib308)],[[45](#bib.bib45)],
    [[84](#bib.bib84)],[[307](#bib.bib307)],[[168](#bib.bib168)],[[169](#bib.bib169)].'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 LABEL:tab:track 中总结了物体跟踪领域的各种工作，并且在图 [13](#S7.F13 "Figure 13 ‣ 7 DRL in
    Object Tracking ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey") 中展示了使用 DRL 的物体跟踪基本实现。该图展示了在视频中使用 DRL 的物体跟踪的通用实现，其中状态由两个连续帧 $(F_{t},F_{t+1})$
    组成，第一帧的边界框由另一个算法或 DRL 代理的前几次迭代生成。动作对应于在图像上移动边界框以适应帧 $F_{t+1}$ 中的物体，从而形成新的状态，包括帧
    $F_{t+1}$ 和帧 $F_{t+2}$ 以及由前一次迭代预测的帧 $F_{t+1}$ 的边界框，奖励则对应于 IOU 是否大于给定的阈值，如 [[118](#bib.bib118)],
    [[308](#bib.bib308)], [[45](#bib.bib45)], [[84](#bib.bib84)], [[307](#bib.bib307)],
    [[168](#bib.bib168)], [[169](#bib.bib169)] 所用。'
- en: '![Refer to caption](img/f2136103d0d64b38351fdb4369ef5e52.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f2136103d0d64b38351fdb4369ef5e52.png)'
- en: 'Figure 13: DRL implementation for object tracking. Here the state consists
    of two consecutive frames with bounding box locations for the first frame that
    is predicted by some object detection algorithm or by the previous iteration of
    DRL, the actions move the bounding box present in the first frame to fit the object
    in the second frame to maximize the reward which is the whether the IOU is greater
    than a given threshold or not.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：物体跟踪的 DRL 实现。这里的状态由两个连续的帧组成，第一帧的边界框位置由某些物体检测算法或 DRL 的前几次迭代预测，动作是将第一帧中存在的边界框移动以适应第二帧中的物体，从而最大化奖励，即
    IOU 是否大于给定的阈值。
- en: 8 DRL in Image Registration
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 DRL 在图像配准中的应用
- en: Image registration is a very useful step that is performed on 3D medical images
    for the alignment of two or more images. The goal of 3D medical image registration
    is to find a correlation between two images from either different patients or
    the same patients at different times, where the images can be Computed Tomography
    (CT), Magnetic Resonance Imaging (MRI), or Positron Emission Tomography (PET).
    In the process, the images are brought to the same coordinate system and aligned
    with each other. The reason for image registration being a challenging task is
    the fact that the two images used may have a different coordinate system, scale,
    or resolution.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图像配准是对3D医学图像进行的一个非常有用的步骤，用于对齐两个或多个图像。3D医学图像配准的目标是找到不同患者或相同患者在不同时间的两幅图像之间的相关性，这些图像可以是计算机断层扫描（CT）、磁共振成像（MRI）或正电子发射断层扫描（PET）。在此过程中，图像被带到相同的坐标系统中，并彼此对齐。图像配准成为一个具有挑战性的任务的原因在于所使用的两幅图像可能具有不同的坐标系统、比例或分辨率。
- en: Many attempts have been made toward automated image registration. A multi-resolution
    strategy with local optimizers to perform 2D or 3D image registration was performed
    by [[359](#bib.bib359)]. However, multi-resolution tends to fail with different
    field of views. Heuristic semi-global optimization schemes were proposed to solve
    this problem and used by [[252](#bib.bib252)] through simulated annealing and
    through genetic algorithm [[317](#bib.bib317)], However, their cost of computation
    was very high. A CNN-based approach to this problem was suggested by [[256](#bib.bib256)],
    and [[79](#bib.bib79)] proposed an optical flow method between 2D RGB images.
    A descriptor learned through a CNN was proposed by [[395](#bib.bib395)], where
    the authors encoded the posture and identity of a 3D object using the 2D image.
    Although all of these formulations produce satisfactory results yet, the methods
    could not be applied directly to 3D medical images.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 许多尝试已用于自动图像配准。[[359](#bib.bib359)] 进行了一个使用局部优化器的多分辨率策略来执行2D或3D图像配准。然而，多分辨率在不同视场下往往会失败。为了解决这个问题，[[252](#bib.bib252)]
    提出了启发式半全局优化方案，并通过模拟退火和遗传算法 [[317](#bib.bib317)] 使用了这些方案。然而，它们的计算成本非常高。[[256](#bib.bib256)]
    提出了基于CNN的方法，并且 [[79](#bib.bib79)] 提出了一个2D RGB图像之间的光流方法。[[395](#bib.bib395)] 提出了通过CNN学习的描述符，作者使用2D图像编码了3D对象的姿态和身份。尽管所有这些方法都产生了令人满意的结果，但这些方法不能直接应用于3D医学图像。
- en: To overcome the problems faced by previous methods, [[238](#bib.bib238)] proposed
    a method for improving probabilistic image registration via RL and uncertainty
    evaluation. The method involved predicting a regression function that predicts
    registration error from a set of features by using regression random forests (RRF)
    [[37](#bib.bib37)] method for training. The authors performed experiments on 3D
    MRI images and obtained an accuracy improvement of up to 25%.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服以前方法面临的问题，[[238](#bib.bib238)] 提出了通过RL和不确定性评估来改进概率图像配准的方法。该方法涉及预测回归函数，该函数通过回归随机森林（RRF）[[37](#bib.bib37)]
    方法从特征集中预测配准误差。作者在3D MRI图像上进行了实验，并获得了高达25%的准确性提升。
- en: Previous image registration methods are often customized to a specific problem
    and are sensitive to image quality and artifacts. To overcome these problems,
    [[221](#bib.bib221)] proposed a robust method using DRL. The authors considered
    the problem as an MDP where the goal is to find a set of transformations to be
    performed on the floating image to register it on the reference image. They used
    the gamma value for future reward decay and used the change in L2 Norm between
    the predicted transformation and ground truth transformation to calculate the
    reward. The authors also used a hierarchical approach to solve the problem with
    varying FOVs and resolutions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的图像配准方法通常针对特定问题进行定制，对图像质量和伪影敏感。为了克服这些问题，[[221](#bib.bib221)] 提出了一个使用DRL的鲁棒方法。作者将问题视为一个MDP，其目标是找到一组变换，以便对浮动图像进行配准到参考图像上。他们使用了未来奖励衰减的gamma值，并使用预测变换与实际变换之间L2范数的变化来计算奖励。作者还使用了层次化方法来解决不同FOV和分辨率的问题。
- en: 'Table 6: Comparing various DRL-based image registration methods.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：比较各种基于DRL的图像配准方法。
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 训练技术 | 动作 | 备注 | 主干网络 | 性能 | 数据集 |'
- en: '| Image registration using uncertainity evaluation [[238](#bib.bib238)] | 2013
    | DQN | Not specified | Probabilistic model using regression random forests (RRF)
    [[37](#bib.bib37)] | Not specified | Higher final Dice score (DSC) as compared
    to other methods like random seed selection and grid-based seed selection | 3D
    MRI images from LONI Probabilistic Brain Atlas (LPBA40) [Dataset](http://www.loni.ucla.edu/)
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 使用不确定性评估的图像配准 [[238](#bib.bib238)] | 2013 | DQN | 未指定 | 使用回归随机森林（RRF）的概率模型
    [[37](#bib.bib37)] | 未指定 | 相较于随机种子选择和基于网格的种子选择等其他方法，具有更高的最终 Dice 分数（DSC） | 来自
    LONI 概率脑图谱（LPBA40）的 3D MRI 图像 [数据集](http://www.loni.ucla.edu/) |'
- en: '| Robust Image registration [[221](#bib.bib221)] | 2017 | DQN | 12 actions:
    corresponding to different transformations | States: current transformation. Reward:
    distance error. | 5 Conv3D layers and 3 FC layers | Better success rate then ITK
    [[153](#bib.bib153)], Quasi-global [[255](#bib.bib255)] and Semantic registration[[277](#bib.bib277)]
    | Abdominal spine CT and CBCT dataset, Cardiac CT and CBCT |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 鲁棒图像配准 [[221](#bib.bib221)] | 2017 | DQN | 12 个行动：对应于不同的变换 | 状态：当前变换。奖励：距离误差。
    | 5 层 Conv3D 和 3 层全连接层 | 相较于 ITK [[153](#bib.bib153)]、准全局 [[255](#bib.bib255)]
    和语义配准 [[277](#bib.bib277)]，成功率更高 | 腹部脊柱 CT 和 CBCT 数据集，心脏 CT 和 CBCT |'
- en: '| Multimodal image registration [[244](#bib.bib244)] | 2017 | Duel-DQN Double-DQN
    | Actions update the transformations on floating image | States: cropped 3D image.
    Duel-DQN for value estimation and Double DQN for updating weights. | Batch normalization
    followed by 5 Conv3D and 3 Maxpool layers | Lower Euclidean distance error as
    compared to methods like Hausdorff, ICP, DQN [[264](#bib.bib264)], Dueling [[390](#bib.bib390)],
    etc. | Thorax and Abdomen (ABD) dataset |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 多模态图像配准 [[244](#bib.bib244)] | 2017 | Duel-DQN Double-DQN | 行动更新浮动图像上的变换
    | 状态：裁剪后的 3D 图像。Duel-DQN 用于价值估计，Double DQN 用于更新权重。 | 批量归一化，随后是 5 层 Conv3D 和 3
    层 Maxpool | 相较于 Hausdorff、ICP、DQN [[264](#bib.bib264)]、Dueling [[390](#bib.bib390)]
    等方法，欧几里得距离误差更低 | 胸部和腹部（ABD）数据集 |'
- en: '| Robust non-rigid agent-based registration [[184](#bib.bib184)] | 2017 | DQN
    | 2n actions for n dimensional $\theta$ vector | States: fixed and moving image.
    Reward: change in transformation error. With Statistical deformation model and
    fuzzy action control. | Multi layer CNN, pooling and FC layers. | Higher Mean
    Dice score and lower Hausdorff distance as compared to methods like LCC-Demons
    [[237](#bib.bib237)] and Elastix [[180](#bib.bib180)]. | MICCAI challenge PROMISE12
    [[227](#bib.bib227)] |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 鲁棒的非刚性基于代理的配准 [[184](#bib.bib184)] | 2017 | DQN | 对于 n 维 $\theta$ 向量的 2n
    个行动 | 状态：固定图像和移动图像。奖励：变换误差的变化。具有统计形变模型和模糊行动控制。 | 多层 CNN、池化和全连接层。 | 相较于 LCC-Demons
    [[237](#bib.bib237)] 和 Elastix [[180](#bib.bib180)] 等方法，具有更高的平均 Dice 分数和更低的 Hausdorff
    距离。 | MICCAI 挑战 PROMISE12 [[227](#bib.bib227)] |'
- en: '| Robust Multimodal registration [[349](#bib.bib349)] | 2018 | Actor-Critic
    (a3c) [[262](#bib.bib262)] | 8 actions: for different transformations | States:
    fixed and moving image. Reward: Distance error. Monte-carlo method with LSTM [[139](#bib.bib139)].
    | Multi layer CNN and FC layer | Comparable if not lower target registration error
    [[96](#bib.bib96)] as compared to methods like SIFT [[239](#bib.bib239)], Elastix
    [[180](#bib.bib180)], Pure SL, RL-matrix, RL-LME, etc. | CT and MR images |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 鲁棒的多模态配准 [[349](#bib.bib349)] | 2018 | Actor-Critic (a3c) [[262](#bib.bib262)]
    | 8 个行动：用于不同的变换 | 状态：固定图像和移动图像。奖励：距离误差。使用 Monte-carlo 方法与 LSTM [[139](#bib.bib139)]。
    | 多层 CNN 和全连接层 | 相较于 SIFT [[239](#bib.bib239)]、Elastix [[180](#bib.bib180)]、Pure
    SL、RL-matrix、RL-LME 等方法，目标配准误差具有可比性或更低 | CT 和 MR 图像 |'
- en: A multi-modal method for image registration was proposed by [[244](#bib.bib244)],
    where the authors used DRL for alignment of depth data with medical images. In
    the specified work Duel DQN was used as the agent for estimating the state value
    and the advantage function, and the cropped 3D image tensor of both data modalities
    was considered as the state. The algorithm’s goal was to estimate a transformation
    function that could align moving images to a fixed image by maximizing a similarity
    function between the fixed and moving image. A large number of convolution and
    pooling layer were used to extract high-level contextual information, batch normalization
    and concatenation of feature vector from last convolution layer with action history
    vector was used to solve the problem of oscillation and closed loops, and Double
    DQN architecture for updating the network weights was used.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[[244](#bib.bib244)]提出了一种用于图像配准的多模态方法，作者使用深度强化学习（DRL）对深度数据与医学图像进行对齐。在指定的工作中，使用了Duel
    DQN作为估计状态值和优势函数的代理，并将两种数据模态的裁剪3D图像张量视为状态。算法的目标是通过最大化固定图像与移动图像之间的相似性函数来估计一个变换函数，使得移动图像与固定图像对齐。使用了大量卷积层和池化层来提取高级上下文信息，批量归一化和将最后一层卷积层的特征向量与动作历史向量连接起来，用于解决振荡和闭环问题，并使用了Double
    DQN架构来更新网络权重。'
- en: Previous methods for image registration fail to cope with large deformations
    and variability in appearance. To overcome these issues [[184](#bib.bib184)] proposed
    a robust non-rigid agent-based method for image registration. The method involves
    finding a spatial transformation $T_{\theta}$ that can map the fixed image with
    the floating image using actions at each time step, that is responsible for optimizing
    $\theta$. If the $\theta$ is a d dimensional vector then there will be 2d possible
    actions. In this work, a DQN was used as an agent for value estimation, along
    with a reward that corresponded to the change in $\theta$ distance between ground
    truth and predicted transformations across an action.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的图像配准方法无法应对大变形和外观变化。为了克服这些问题，[[184](#bib.bib184)]提出了一种鲁棒的非刚性基于代理的图像配准方法。该方法涉及找到一个空间变换$T_{\theta}$，通过在每个时间步骤执行动作来映射固定图像与浮动图像，即负责优化$\theta$。如果$\theta$是一个d维向量，则将有2d个可能的动作。在这项工作中，使用了DQN作为值估计的代理，并使用了与$\theta$变化距离（即真实值和预测变换之间的距离）相关的奖励。
- en: An improvement to the previous methods was proposed by [[349](#bib.bib349)],
    where the authors used a recurrent network with RL to solve the problem. Similar
    to [[221](#bib.bib221)], they considered the two images as a reference/fixed and
    floating/moving, and the algorithm was responsible for predicting transformation
    on the moving image to register it on a fixed image. In the specified work an
    LSTM [[139](#bib.bib139)] was used to encode past hidden states, Actor-critic
    [[262](#bib.bib262)] for policy estimation, and a reward function corresponding
    to distance between ground truth and transformed predicted landmarks were used.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对先前方法的改进由[[349](#bib.bib349)]提出，作者使用了带有强化学习（RL）的递归网络来解决问题。类似于[[221](#bib.bib221)]，他们将两张图像视为参考/固定图像和浮动/移动图像，算法负责预测移动图像上的变换以将其注册到固定图像上。在指定的工作中，使用了LSTM
    [[139](#bib.bib139)]来编码过去的隐藏状态，Actor-critic [[262](#bib.bib262)]用于策略估计，并使用了与真实值和变换预测地标之间距离相对应的奖励函数。
- en: 'Various methods in the field of Image registration have been summarized and
    compared in Table LABEL:tab:reg, and a basic implementation of image registration
    using DRL has been shown in Fig. [14](#S8.F14 "Figure 14 ‣ 8 DRL in Image Registration
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure illustrates a general implementation of image registration using DRL where
    the state consists of a fixed and floating image. The DRL agent predicts actions
    in form of a set of transformations on a floating image to register it onto the
    fixed image hence forming a new state and accepts reward in form of improvement
    in distance error between ground truth and predicted transformations with iterations
    as described by [[349](#bib.bib349)],[[184](#bib.bib184)],[[221](#bib.bib221)].'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '表LABEL:tab:reg总结并比较了图像配准领域的各种方法，并在图[14](#S8.F14 "Figure 14 ‣ 8 DRL in Image
    Registration ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive
    Survey")中展示了使用DRL的图像配准的基本实现。该图展示了使用DRL的图像配准的一般实现，其中状态由固定图像和浮动图像组成。DRL代理在浮动图像上预测一组变换作为动作，以将其配准到固定图像上，从而形成新的状态，并接受基于地面真实值和预测变换之间距离误差改进的奖励，如[[349](#bib.bib349)]、[[184](#bib.bib184)]、[[221](#bib.bib221)]所述。'
- en: '![Refer to caption](img/d75a1dbe18c7a1d15a804e3770b2961a.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d75a1dbe18c7a1d15a804e3770b2961a.png)'
- en: 'Figure 14: DRL implementation for image registration. The state consists of
    fixed and floating image and the actions in form of transformations are performed
    on the floating image so as to maximize reward by minimizing distance between
    the ground truth and predicted transformations.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：图像配准的DRL实现。状态由固定图像和浮动图像组成，变换形式的动作在浮动图像上执行，以通过最小化地面真实值和预测变换之间的距离来最大化奖励。
- en: 9 DRL in Image Segmentation
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 DRL在图像分割中的应用
- en: Image segmentation is one of the most extensively performed tasks in computer
    vision, where the algorithm is responsible for labeling each pixel position as
    foreground or background corresponding to the object being segmented in the image.
    Image segmentation has a wide variety of applications in medical, robotics, weather,
    etc. One of the earlier attempts with image segmentation includes [[125](#bib.bib125)].
    With the improvement in detection techniques and introduction of CNN, new methods
    are introduced every year for image segmentation. Mask R-CNN [[132](#bib.bib132)]
    extended the work by Faster R-CNN [[309](#bib.bib309)] by adding a segmentation
    layer after the Bounding box has been predicted. Some earlier works include [[109](#bib.bib109)],
    [[127](#bib.bib127)], [[128](#bib.bib128)] etc. Most of these works give promising
    results in image segmentation. However, due to the supervised nature of CNN and
    R-CNN, these algorithms need a large amount of data. In fields like medical, the
    data is sometimes not readily available hence we needed a way to train algorithms
    to perform a given task when there are data constraints. Luckily RL tends to shine
    when the data is not available in a large quantity.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是计算机视觉中最广泛执行的任务之一，其中算法负责将每个像素位置标记为前景或背景，与图像中被分割的对象相对应。图像分割在医学、机器人、天气等领域有着广泛的应用。图像分割的早期尝试之一包括[[125](#bib.bib125)]。随着检测技术的改进和CNN的引入，每年都有新的图像分割方法问世。Mask
    R-CNN [[132](#bib.bib132)] 通过在边界框预测之后添加分割层，扩展了Faster R-CNN [[309](#bib.bib309)]
    的工作。一些早期的工作包括[[109](#bib.bib109)]、[[127](#bib.bib127)]、[[128](#bib.bib128)]等。这些工作大多数在图像分割中取得了有希望的结果。然而，由于CNN和R-CNN的监督性质，这些算法需要大量的数据。在医学等领域，数据有时并不容易获得，因此我们需要一种方法来训练算法，在数据受限的情况下执行特定任务。幸运的是，当数据量不大时，RL表现得尤为出色。
- en: One of the first methods for Image segmentation through RL was proposed by [[324](#bib.bib324)],
    where the authors proposed an RL framework for medical image segmentation. In
    their work, they used a Q-Matrix, where the actions were responsible for adjusting
    the threshold values to predict the mask and the reward was the normalized change
    in quality measure between action steps. [[325](#bib.bib325)] also used a similar
    technique of Tabular method.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 通过RL进行图像分割的首个方法之一由[[324](#bib.bib324)]提出，作者为医学图像分割提出了一个RL框架。在他们的工作中，他们使用了Q矩阵，其中动作负责调整阈值以预测掩码，而奖励是动作步骤之间质量度量的标准化变化。[[325](#bib.bib325)]也使用了类似的表格方法。
- en: To overcome the constraints of the previous method for segmentation, [[310](#bib.bib310)]
    proposed a method for indoor semantic segmentation through RL. In their paper,
    the authors proposed a sequential strategy using RL to combine binary object masks
    of different objects into a single multi-object segmentation mask. They formulated
    the binary mask in a Conditional Random Field Framework (CRF), and used a logistic
    regression version of AdaBoost [[140](#bib.bib140)] for classification. The authors
    considered the problem of adding multiple binary segmentation into one as an MDP,
    where the state consisted of a list of probability distributions of different
    objects in an image, and the actions correspond to the selection of object/background
    segmentation for a particular object in the sequential semantic segmentation.
    In the RL framework, the reward was considered in terms of pixel-wise frequency
    weighted Jaccard Index computed over the set of actions taken at any stage of
    an episode.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服之前分割方法的限制，[[310](#bib.bib310)] 提出了通过 RL 实现室内语义分割的方法。在他们的论文中，作者提出了一种使用 RL
    的顺序策略，将不同对象的二进制掩码组合成一个多对象分割掩码。他们在条件随机场框架（CRF）中对二进制掩码进行了公式化，并使用了 AdaBoost 的逻辑回归版本
    [[140](#bib.bib140)] 进行分类。作者将多个二进制分割合并为一个的问题视为 MDP，其中状态由图像中不同对象的概率分布列表组成，动作对应于顺序语义分割中某一对象的对象/背景分割选择。在
    RL 框架中，奖励被考虑为在任何阶段的动作集合上计算的像素频率加权 Jaccard 指数。
- en: Interactive segmentation is the task of producing an interactive mask for objects
    in an image. Most of the previous works in this field greatly depend on the distribution
    of inputs which is user-dependent and hence produce inadequate results. An improvement
    was proposed by [[343](#bib.bib343)], where the authors proposed SeedNet, an automatic
    seed generation method for robust interactive segmentation through RL. With the
    image and initial seed points, the algorithm is capable of generating additional
    seed points and image segmentation results. The implementation included Random
    Walk (RW) [[114](#bib.bib114)] as the segmentation algorithm and DQN for value
    estimation by considering the problem as an MDP. They used the current binary
    segmentation mask and image features as the state, the actions corresponded to
    selecting seed points in a sparse matrix of size $20\times 20$(800 different actions
    were possible), and the reward consisted of the change in IOU across an action.
    In addition, the authors used an exponential IOU model to capture changes in IOU
    values more accurately.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式分割是为图像中的对象生成交互式掩码的任务。这个领域的大多数先前工作严重依赖于用户依赖的输入分布，因此产生了不充分的结果。[[343](#bib.bib343)]
    提出了改进，其中作者提出了 SeedNet，一种通过 RL 进行鲁棒交互式分割的自动种子生成方法。通过图像和初始种子点，该算法能够生成额外的种子点和图像分割结果。该实现包括使用
    Random Walk (RW) [[114](#bib.bib114)] 作为分割算法，并通过将问题视为 MDP 使用 DQN 进行价值估计。他们使用当前的二进制分割掩码和图像特征作为状态，动作对应于在大小为
    $20\times 20$ 的稀疏矩阵中选择种子点（有 800 种不同的动作可能），奖励由 IOU 在动作中的变化组成。此外，作者使用了指数 IOU 模型以更准确地捕捉
    IOU 值的变化。
- en: Most of the previous work for image segmentation fail to produce satisfactory
    results when it comes to 3D medical data. An attempt on 3D medical image segmentation
    was done by [[222](#bib.bib222)], where the authors proposed an iteratively-refined
    interactive multi-agent method for 3D medical image segmentation. They proposed
    a method to refine an initial course segmentation produced by some segmentation
    methods using RL, where the state consisted of the image, previous segmentation
    probability, and user hint map. The actions corresponded to adjusting the segmentation
    probability for refinement of segmentation, and a relative cross-entropy gain-based
    reward to update the model in a constrained direction was used. In simple words,
    it is the relative improvement of previous segmentation to the current one. The
    authors utilized an asynchronous advantage actor-critic algorithm for determining
    the policy and value of the state.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数以前的图像分割工作在处理3D医学数据时未能产生令人满意的结果。[[222](#bib.bib222)]尝试了3D医学图像分割，提出了一种迭代精化的交互式多智能体方法用于3D医学图像分割。他们提出了一种方法，通过RL精化由某些分割方法产生的初始粗略分割，其中状态包括图像、之前的分割概率和用户提示图。动作对应于调整分割概率以精化分割，并使用基于相对交叉熵增益的奖励来以受限的方向更新模型。简单来说，就是以前分割相对于当前分割的相对改进。作者使用异步优势演员-评论家算法来确定状态的策略和值。
- en: 'Table 7: Comparing various DRL-based image segmentation methods'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：比较各种基于DRL的图像分割方法
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 训练技术 | 动作 | 备注 | 主干网络 | 性能 | 数据集 |'
- en: '| Semantic Segmentation for indoor scenes[[310](#bib.bib310)] | 2016 | DQN
    | 2 actions per object: object, background | States: current probability distribution.
    Reward: pixel-wise frequency weighted Jaccard index. Conditional Random Field
    Framework (CRF) and logistic regression version of AdaBoost [[140](#bib.bib140)]
    for classification. | Not Specified | Pixel-wise percentage jaccard index comparable
    to Gupta-L [[121](#bib.bib121)] and Gupta-P [[120](#bib.bib120)]. | NYUD V2 dataset
    [[338](#bib.bib338)] |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 室内场景的语义分割[[310](#bib.bib310)] | 2016 | DQN | 每个对象2个动作：对象，背景 | 状态：当前概率分布。奖励：像素级别频率加权Jaccard指数。分类的条件随机场框架（CRF）和AdaBoost
    [[140](#bib.bib140)]的逻辑回归版本。 | 未指定 | 像素级Jaccard指数与Gupta-L [[121](#bib.bib121)]和Gupta-P
    [[120](#bib.bib120)]相当。 | NYUD V2数据集 [[338](#bib.bib338)] |'
- en: '| SeedNet [[343](#bib.bib343)] | 2018 | DQN, Double-DQN, Duel-DQN | 800 actions:
    2 per pixel | States: image features and segmentation mask. Reward: change in
    IOU. Random Walk (RW) [[114](#bib.bib114)] for segmentation algorithm. | Multi
    layer CNN | Better IOU then methods like FCN [[236](#bib.bib236)] and iFCN [[407](#bib.bib407)].
    | MSRA10K saliency dataset [[49](#bib.bib49)] |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| SeedNet [[343](#bib.bib343)] | 2018 | DQN、Double-DQN、Duel-DQN | 800个动作：每个像素2个动作
    | 状态：图像特征和分割掩模。奖励：IOU变化。用于分割算法的随机游走（RW） [[114](#bib.bib114)]。 | 多层CNN | 比FCN [[236](#bib.bib236)]和iFCN
    [[407](#bib.bib407)]等方法表现更好。 | MSRA10K显著性数据集 [[49](#bib.bib49)] |'
- en: '| Iteratively refined multi agent segmentation [[222](#bib.bib222)] | 2020
    | Actor-critic (a3c) [[262](#bib.bib262)] | 1 action per voxel for adjusting segmentation
    probability | States: 3D image segmentation probability and hint map. Reward:
    cross entropy gain based framework. | R-net [[378](#bib.bib378)] | Better performance
    then methods like MinCut [[183](#bib.bib183)], DeepIGeoS (R-Net) [[378](#bib.bib378)]
    and InterCNN [[36](#bib.bib36)]. | BraTS 2015[[254](#bib.bib254)], MM-WHS [[432](#bib.bib432)]
    and NCI-ISBI 2013 Challenge [[33](#bib.bib33)] |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 迭代精化的多智能体分割 [[222](#bib.bib222)] | 2020 | Actor-critic (a3c) [[262](#bib.bib262)]
    | 每个体素一个动作用于调整分割概率 | 状态：3D图像分割概率和提示图。奖励：基于交叉熵增益的框架。 | R-net [[378](#bib.bib378)]
    | 比MinCut [[183](#bib.bib183)]、DeepIGeoS (R-Net) [[378](#bib.bib378)]和InterCNN
    [[36](#bib.bib36)]等方法表现更好。 | BraTS 2015[[254](#bib.bib254)]、MM-WHS [[432](#bib.bib432)]和NCI-ISBI
    2013挑战赛 [[33](#bib.bib33)] |'
- en: '| Multi-step medical image segmentation [[360](#bib.bib360)] | 2020 | Actor-critic
    (a3c) [[262](#bib.bib262)] | Actions control the position and shape of brush stroke
    to modify segmentation | States: image, segmentation mask and time step. Reward:
    change in distance error. Policy: DPG [[339](#bib.bib339)]. | ResNet18 [[133](#bib.bib133)]
    | Higher Mean Dice score and lower Hausdorff distance then methods like Grab-Cut
    [[315](#bib.bib315)], PSPNet [[425](#bib.bib425)], FCN [[236](#bib.bib236)], U-Net
    [[313](#bib.bib313)], etc. | Prostate MR image dataset (PROMISE12, ISBI2013) and
    retinal fundus image dataset (REFUGE challenge dataset [[285](#bib.bib285)]) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 多步骤医学图像分割 [[360](#bib.bib360)] | 2020 | Actor-critic (a3c) [[262](#bib.bib262)]
    | 动作控制刷子笔画的位置和形状以修改分割 | 状态：图像、分割掩码和时间步。奖励：距离误差的变化。策略：DPG [[339](#bib.bib339)]。
    | ResNet18 [[133](#bib.bib133)] | 相较于 Grab-Cut [[315](#bib.bib315)]、PSPNet [[425](#bib.bib425)]、FCN
    [[236](#bib.bib236)]、U-Net [[313](#bib.bib313)] 等方法，具有更高的平均 Dice 分数和更低的 Hausdorff
    距离 | 前列腺 MR 图像数据集 (PROMISE12, ISBI2013) 和视网膜眼底图像数据集 (REFUGE 挑战数据集 [[285](#bib.bib285)])
    |'
- en: '| Anomaly Detection in Images [[56](#bib.bib56)] | 2020 | REINFORCE [[392](#bib.bib392)]
    | 9 actions, 8 for directions to shift center of the extracted patch to, the last
    action is to switch to a random new image | Environment: input image to the model.
    State: observed patch from the image centered by predicted center of interest.
    | None | Superior performance in [[27](#bib.bib27)] and [[337](#bib.bib337)] on
    all metrics e.g. precision, recall and F1 when compared with U-Net [[313](#bib.bib313)]
    and baseline unsupervised method in [[27](#bib.bib27)] but only wins on recall
    in [[44](#bib.bib44)] | MVTec AD [[27](#bib.bib27)], NanoTWICE [[44](#bib.bib44)],
    CrackForest [[337](#bib.bib337)] |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 图像中的异常检测 [[56](#bib.bib56)] | 2020 | REINFORCE [[392](#bib.bib392)] | 9 个动作，其中
    8 个用于将提取的补丁的中心位置移动到其他方向，最后一个动作是切换到随机的新图像 | 环境：模型的输入图像。状态：由预测的兴趣中心对齐的图像中观察到的补丁。
    | 无 | 与 U-Net [[313](#bib.bib313)] 和 [[27](#bib.bib27)] 中的基线无监督方法相比，在所有指标（如精度、召回率和
    F1）上表现优越，但在 [[44](#bib.bib44)] 中仅在召回率上获胜 | MVTec AD [[27](#bib.bib27)]、NanoTWICE
    [[44](#bib.bib44)]、CrackForest [[337](#bib.bib337)] |'
- en: Further improvement in the results of medical image segmentation was proposed
    by [[360](#bib.bib360)]. The authors proposed a method for multi-step medical
    image segmentation using RL, where they used a deep deterministic policy gradient
    method (DDPG) based on actor-critic framework [[262](#bib.bib262)] and similar
    to Deterministic policy gradient (DPG) [[339](#bib.bib339)]. The authors used
    ResNet18 [[133](#bib.bib133)] as backbone for actor and critic network along with
    batch normalisation [[157](#bib.bib157)] and weight normalization with Translated
    ReLU [[400](#bib.bib400)]. In their MDP formulation, the state consisted of the
    image along with the current segmentation mask and step-index, and the reward
    corresponded to the change in mean squared error between the predicted segmentation
    and ground truth across an action. According to the paper the action was defined
    to control the position and shape of brush stroke used to modify the segmentation.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分割结果的进一步改进由 [[360](#bib.bib360)] 提出。作者提出了一种使用 RL 的多步骤医学图像分割方法，其中采用了基于 actor-critic
    框架的深度确定性策略梯度方法 (DDPG) [[262](#bib.bib262)]，类似于确定性策略梯度 (DPG) [[339](#bib.bib339)]。作者使用
    ResNet18 [[133](#bib.bib133)] 作为 actor 和 critic 网络的骨干，并结合批量归一化 [[157](#bib.bib157)]
    和带有翻译 ReLU [[400](#bib.bib400)] 的权重归一化。在他们的 MDP 公式中，状态由图像、当前分割掩码和步骤索引组成，奖励对应于在一个动作中预测的分割和真实值之间均方误差的变化。根据论文，动作被定义为控制用于修改分割的刷子笔画的位置和形状。
- en: An example in image segmentation outside the medical field is [[56](#bib.bib56)]
    proposing to tackle the problem of anomalies detection and segmentation in images
    (i.e. damaged pins of an IC chip, small tears in woven fabric). [[56](#bib.bib56)]
    utilizes an additional module to attend only on a specific patch of the image
    centered by a predicted center instead of the whole image, this module helps a
    lot in reducing the imbalance between normal regions and abnormal locations. Given
    an image, this module, namely Neural Batch Sampling (NBS), starts from a random
    initiated center and recurrently moves that center by eight directions to the
    abnormal location in the image if it exists, and it has an additional action to
    stop moving the center when it has already converged to the anomaly location or
    there is not any anomaly can be observed. The NBS module is trained by REINFORCE
    algorithm [[392](#bib.bib392)] and the whole model is evaluated on multiple datasets
    e.g. MVTec AD [[27](#bib.bib27)], NanoTWICE [[44](#bib.bib44)], CrackForest [[337](#bib.bib337)].
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学领域之外的图像分割示例是 [[56](#bib.bib56)] 提出的，该示例旨在解决图像中的异常检测和分割问题（例如，集成电路芯片的损坏引脚、织物中的小裂缝）。
    [[56](#bib.bib56)] 利用附加模块仅关注由预测中心标记的特定图像区域，而不是整个图像，这个模块有助于减少正常区域和异常位置之间的不平衡。给定一张图像，该模块，即神经批量采样（NBS），从随机初始化的中心开始，并通过八个方向反复移动该中心，直到找到图像中的异常位置（如果存在），并具有一个额外的动作，当中心已经收敛到异常位置或无法观察到异常时，停止移动中心。NBS
    模块由 REINFORCE 算法 [[392](#bib.bib392)] 训练，整个模型在多个数据集上进行评估，例如 MVTec AD [[27](#bib.bib27)]、NanoTWICE
    [[44](#bib.bib44)]、CrackForest [[337](#bib.bib337)]。
- en: 'Various works in the fields of Image segmentation have been summarised and
    compared in Table LABEL:tab:seg, and a basic implementation of image segmentation
    using DRL has been shown in Fig. [15](#S9.F15 "Figure 15 ‣ 9 DRL in Image Segmentation
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"). The
    figure shows a general implementation of image segmentation using DRL. The states
    consist of the image along with user hint (landmarks or segmentation mask by the
    other algorithm) for the first iteration or segmentation mask by the previous
    iteration. The actions are responsible for labeling each pixel as foreground and
    background and reward corresponds to an improvement in IOU with iterations as
    used by [[343](#bib.bib343)],[[222](#bib.bib222)].'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图表 LABEL:tab:seg 中总结并比较了图像分割领域的各种工作，并在图 [15](#S9.F15 "图 15 ‣ 9 图像分割中的深度强化学习
    ‣ 计算机视觉中的深度强化学习：全面调查") 中展示了使用深度强化学习的图像分割的基本实现。该图展示了使用深度强化学习进行图像分割的一般实现。状态包括图像及用户提示（由其他算法提供的地标或分割掩码）用于第一次迭代或前一次迭代的分割掩码。动作负责将每个像素标记为前景或背景，奖励与每次迭代中的IOU改进相关，参考
    [[343](#bib.bib343)], [[222](#bib.bib222)]。
- en: '![Refer to caption](img/47751bb3b691f2ca58b0fd46cd4d3310.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/47751bb3b691f2ca58b0fd46cd4d3310.png)'
- en: 'Figure 15: DRL implementation for Image segmentation. The state consists of
    the image to be segmented along with a user hint for t=0 or the segmentation mask
    by the previous iterations. The DRL agent performs actions by labeling each pixel
    as foreground and background to maximize the improvement in IOU over the iterations.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：图像分割的深度强化学习实现。状态包括待分割的图像及t=0时的用户提示或前几次迭代的分割掩码。深度强化学习代理通过将每个像素标记为前景或背景来执行动作，以最大化IOU在迭代过程中的改进。
- en: 10 DRL in Video Analysis
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 视频分析中的深度强化学习
- en: Object segmentation in videos is a very useful yet challenging task in computer
    vision field. Video object segmentation task focuses on labelling each pixel for
    each frame as foreground or background. Previous works in the field of video object
    segmentation can be divided into three main methods. unsupervised [[288](#bib.bib288)][[402](#bib.bib402)],
    weakly supervised [[48](#bib.bib48)][[163](#bib.bib163)] [[419](#bib.bib419)]
    and semi-supervised [[41](#bib.bib41)] [[164](#bib.bib164)][[292](#bib.bib292)].
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 视频中的目标分割是计算机视觉领域中一个非常有用但具有挑战性的任务。视频目标分割任务侧重于为每一帧标记每个像素为前景或背景。视频目标分割领域的先前工作可以分为三种主要方法：无监督
    [[288](#bib.bib288)][[402](#bib.bib402)]、弱监督 [[48](#bib.bib48)][[163](#bib.bib163)][[419](#bib.bib419)]
    和半监督 [[41](#bib.bib41)][[164](#bib.bib164)][[292](#bib.bib292)]。
- en: A DRL-based framework for video object segmentation was proposed by [[323](#bib.bib323)],
    where the authors divided the image into a group of sub-images and then used the
    algorithm on each of the sub-image. They proposed a group of actions that can
    perform to change the local values inside each sub-image and the agent received
    reward based on the change in the quality of segmented object inside each sub-image
    across an action. In the proposed method deep belief network (DBN) [[47](#bib.bib47)]
    was used for approximating the Q-values.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基于深度强化学习（DRL）的框架用于视频目标分割，该框架由[[323](#bib.bib323)]提出，其中作者将图像划分为一组子图像，然后在每个子图像上应用算法。他们提出了一组可以用来改变每个子图像内局部值的动作，并且代理根据每个子图像内分割对象质量的变化来获得奖励。在提出的方法中，使用了深度信念网络（DBN）[[47](#bib.bib47)]来近似Q值。
- en: Surgical gesture recognition is a very important yet challenging task in the
    computer vision field. It is useful in assessing surgical skills and for efficient
    training of surgeons. A DRL method for surgical gesture classification and segmentation
    was proposed by [[228](#bib.bib228)]. The proposed method could work on features
    extracted by video frames or kinematic data frames collected by some means along
    with the ground truth labels. The problem of classification and segmentation was
    considered as an MDP, where the state was a concatenation of TCN [[195](#bib.bib195)][[199](#bib.bib199)]
    features of the current frame, 2 future frames a specified number of frames later,
    transition probability of each gesture computed from a statistical language model
    [[311](#bib.bib311)] and a one-hot encoded vector for gesture classes. The actions
    could be divided into two sub-actions, One to decide optimal step size and one
    for choosing gesture class, and the reward was adopted in a way that encouraging
    the agent to adopt a larger step and also penalizes the agent for errors caused
    by the action. The authors used Trust Region Policy Optimization (TRPO) [[326](#bib.bib326)]
    for training the policy and a spacial CNN [[196](#bib.bib196)] to extract features.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 外科手势识别是计算机视觉领域中一个非常重要且具有挑战性的任务。它在评估外科技能和高效培训外科医生方面非常有用。[[228](#bib.bib228)]提出了一种用于外科手势分类和分割的DRL方法。该方法可以处理由视频帧或通过某种手段收集的运动学数据帧提取的特征，以及真实标签。分类和分割问题被视为一个马尔可夫决策过程（MDP），其中状态是当前帧的TCN
    [[195](#bib.bib195)][[199](#bib.bib199)]特征的拼接，2个未来帧，特定数量帧后的转移概率，以及用于手势类别的一热编码向量。动作可以分为两个子动作，一个用于决定最佳步长，另一个用于选择手势类别，奖励的设计方式是鼓励代理采取较大的步长，同时对因动作导致的错误进行惩罚。作者使用了信任区域策略优化（TRPO）[[326](#bib.bib326)]来训练策略，并使用空间卷积神经网络（CNN）[[196](#bib.bib196)]来提取特征。
- en: Earlier approaches for video object segmentation required a large number of
    actions to complete the task. An Improvement was proposed by [[124](#bib.bib124)],
    where authors used an RL method for object segmentation in videos. They proposed
    a reinforcement cutting-agent learning framework, where the cutting-agent consists
    of a cutting-policy network (CPN) and a cutting-execution network (CEN). The CPN
    learns to predict the object-context box pair, while CEN learns to predict the
    mask based on the inferred object-context box pair. The authors used MDP to solve
    the problem in a semi-supervised fashion. For the state of CPN the authors used
    the input frame information, the action history, and the segmentation mask provided
    in the first frame. The output boxes by CPN were input for the CEN. The actions
    for CPN network included 4 translation actions (Up, Down, Left, Right), 4 scaling
    action (Horizontal shrink, Vertical shrink, Horizontal zoom, Vertical zoom), and
    1 terminal action (Stop), and the reward corresponded to the change in IOU across
    an action. For the network architecture, a Fully-Convolutional DenseNet56 [[166](#bib.bib166)]
    was used as a backbone along with DQN as the agent for CPN and down-sampling followed
    by up-sampling architecture for CEN.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的视频对象分割方法需要大量的操作才能完成任务。[[124](#bib.bib124)] 提出了改进方案，作者们使用了一种 RL 方法进行视频中的对象分割。他们提出了一个强化切割代理学习框架，其中切割代理包括一个切割策略网络（CPN）和一个切割执行网络（CEN）。CPN
    学习预测对象-上下文框对，而 CEN 学习根据推断出的对象-上下文框对预测掩模。作者们使用 MDP 以半监督的方式解决问题。对于 CPN 的状态，作者们使用了输入帧信息、动作历史和第一帧提供的分割掩模。CPN
    输出的框作为 CEN 的输入。CPN 网络的动作包括 4 个平移动作（上、下、左、右）、4 个缩放动作（水平收缩、垂直收缩、水平放大、垂直放大）和 1 个终止动作（停止），奖励与每个动作的
    IOU 变化对应。对于网络架构，使用了 Fully-Convolutional DenseNet56 [[166](#bib.bib166)] 作为主干网络，以及
    DQN 作为 CPN 的代理，并为 CEN 使用了下采样后跟上采样的架构。
- en: Unsupervised video object segmentation is an intuitive task in the computer
    vision field. A DRL method for this task was proposed by [[111](#bib.bib111)],
    where the authors proposed a motion-oriented unsupervised method for image segmentation
    in videos (MOREL). They proposed a two-step process to achieve the task in which
    first a representation of input is learned to understand all moving objects through
    unsupervised video object segmentation, Then the weights are transferred to the
    RL framework to jointly train segmentation network along with policy and value
    function. The first part of the method takes two consecutive frames as input and
    predicts a number of segmentation masks, corresponding object translations, and
    camera translations. They used a modified version of actor-critic [[262](#bib.bib262)][[329](#bib.bib329)][[371](#bib.bib371)]
    for the network of first step. Following the unsupervised fashion, the authors
    used the approach similar to [[375](#bib.bib375)] and trained the network to interpolate
    between consecutive frames and used the masks and translations to estimate the
    optical flow using the method that was proposed in Spatial Transformer Networks
    [[159](#bib.bib159)]. They also used structural dissimilarity (DSSIM) [[388](#bib.bib388)]
    to calculate reconstruction loss and actor-critic [[262](#bib.bib262)] algorithm
    to learn policy in the second step.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督视频对象分割是计算机视觉领域中的一个直观任务。[[111](#bib.bib111)] 提出了一个 DRL 方法来解决这个任务，作者们提出了一种面向运动的无监督方法来进行视频中的图像分割（MOREL）。他们提出了一个两步流程来实现任务：首先，通过无监督视频对象分割学习输入的表示，以了解所有移动对象；然后，将权重转移到
    RL 框架中，以联合训练分割网络以及策略和价值函数。该方法的第一部分以两个连续的帧作为输入，预测多个分割掩模、相应的对象平移和相机平移。他们使用了修改版的
    actor-critic [[262](#bib.bib262)][[329](#bib.bib329)][[371](#bib.bib371)] 作为第一步的网络。遵循无监督的方式，作者们使用类似于
    [[375](#bib.bib375)] 的方法，训练网络在连续帧之间进行插值，并使用掩模和平移来估计光流，使用的是 Spatial Transformer
    Networks [[159](#bib.bib159)] 中提出的方法。他们还使用了结构性不相似度（DSSIM） [[388](#bib.bib388)]
    来计算重建损失，并使用 actor-critic [[262](#bib.bib262)] 算法来学习第二步的策略。
- en: 'A DRL method for dynamic semantic face video segmentation was proposed by [[387](#bib.bib387)],
    where Deep Feature Flow [[431](#bib.bib431)] was utilized as the feature propagation
    framework and RL was used for an efficient and effective scheduling policy. The
    method involved dividing frames into key ($I_{k}$) and non-key ($I_{i}$), and
    using the last key frame features for performing segmentation of non-key frame.
    The actions made by the policy network corresponded to categorizing a frame as
    $I_{k}$ or $I_{i}$ and the state consisted of deviation information and expert
    information, where the deviation information described the difference between
    current $I_{i}$ and last $I_{k}$ and expert information encapsulated the key decision
    history. The authors utilized FlowNet2-s model [[156](#bib.bib156)] as an optical
    flow estimation function, and divided the network into feature extraction module
    and task-specific module. After policy network which consisted of one convolution
    layer, 4 fully connected layers and 2 concatenated channels consisting of KAR
    (Key all ratio: Ratio between key frame and every other frame in decision history)
    and LKD (Last key distance: Distance between current and last key frame) predicted
    the action, If the current frame is categorized as key frame the feature extraction
    module produced the frame features and task-specific module predicted the segmentation,
    However if the frame is categorized as a non-key frame the features from the last
    key frame along with the optical flow was used by the task-specific module to
    predict the segmentation. The authors proposed two types of reward functions,
    The first reward function was calculated by considering the difference between
    the IOU for key and non-key actions. The second reward function was proposed for
    a situation when ground truth was not available and was calculated by considering
    the accuracy score between segmentation for key and non-key actions.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[[387](#bib.bib387)] 提出了一种用于动态语义面部视频分割的DRL方法，其中利用了 Deep Feature Flow [[431](#bib.bib431)]
    作为特征传播框架，RL 被用来制定高效而有效的调度策略。该方法涉及将帧划分为关键帧 ($I_{k}$) 和非关键帧 ($I_{i}$)，并使用最后一个关键帧的特征来进行非关键帧的分割。策略网络所做的动作对应于将帧分类为
    $I_{k}$ 或 $I_{i}$，状态包括偏差信息和专家信息，其中偏差信息描述了当前 $I_{i}$ 和上一个 $I_{k}$ 之间的差异，专家信息则包含了关键决策历史。作者利用
    FlowNet2-s 模型 [[156](#bib.bib156)] 作为光流估计函数，将网络分为特征提取模块和任务特定模块。在由一个卷积层、4 个全连接层和
    2 个连接通道（包括 KAR（关键全比率：决策历史中关键帧与其他帧的比例）和 LKD（上一个关键距离：当前和上一个关键帧之间的距离））组成的策略网络预测动作后，如果当前帧被分类为关键帧，特征提取模块会生成帧特征，任务特定模块则预测分割。然而，如果帧被分类为非关键帧，任务特定模块则使用来自最后一个关键帧的特征和光流来预测分割。作者提出了两种类型的奖励函数，第一个奖励函数通过考虑关键动作和非关键动作的
    IOU 差异来计算。第二个奖励函数则针对没有地面真实值的情况提出，通过考虑关键动作和非关键动作的分割准确度评分来计算。'
- en: 'Table 8: Comparing various methods associated with video. First group for video
    object segmentation, second group for action recognition and third group for video
    summarisation'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：比较与视频相关的各种方法。第一组用于视频目标分割，第二组用于动作识别，第三组用于视频摘要。
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets and Source code |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 训练技术 | 动作 | 备注 | 主干网络 | 性能 | 数据集和源代码 |'
- en: '| Object segmentation in videos[[323](#bib.bib323)] | 2016 | Deep Belief Network
    [[47](#bib.bib47)] | Actions changed local values in sub-images | States: sub-images.
    Reward: quality of segmentation. | Not specified | Not specified | Not specified
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 视频中的目标分割[[323](#bib.bib323)] | 2016 | 深度置信网络 [[47](#bib.bib47)] | 动作改变子图中的局部值
    | 状态：子图。奖励：分割质量。 | 未指定 | 未指定 | 未指定 |'
- en: '| Surgical gesture segmentation and classification [[228](#bib.bib228)] | 2018
    | Trust Region Policy Optimization (TRPO) [[326](#bib.bib326)] | 2 types: optimal
    step size and gesture class | States: TCN [[[195](#bib.bib195)], [[199](#bib.bib199)]]
    and future frames. Reward: encourage larger steps and minimize action errors.
    Statistical language model [[311](#bib.bib311)] for gesture probability. | Spacial
    CNN [[196](#bib.bib196)] | Comparable accuracy, and higher edit and F1 scores
    as compared to methods like SD-SDL [[331](#bib.bib331)], Bidir LSTM [[76](#bib.bib76)],
    LC-SC-CRF [[197](#bib.bib197)], Seg-ST-CNN [[196](#bib.bib196)], TCN [[198](#bib.bib198)],
    etc | JIGSAWS [[[6](#bib.bib6)], [[100](#bib.bib100)]] benchmark dataset [Available
    Code](https://github.com/Finspire13/RL-Surgical-Gesture-Segmentation) |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 外科手势分割与分类 [[228](#bib.bib228)] | 2018 | 信任区域策略优化 (TRPO) [[326](#bib.bib326)]
    | 2 种类型：最优步长和手势类别 | 状态：TCN [[[195](#bib.bib195)], [[199](#bib.bib199)]] 和未来帧。奖励：鼓励更大的步长并最小化动作误差。手势概率的统计语言模型
    [[311](#bib.bib311)]。 | 空间 CNN [[196](#bib.bib196)] | 相较于 SD-SDL [[331](#bib.bib331)]、Bidir
    LSTM [[76](#bib.bib76)]、LC-SC-CRF [[197](#bib.bib197)]、Seg-ST-CNN [[196](#bib.bib196)]、TCN
    [[198](#bib.bib198)] 等方法，具有可比的准确性以及更高的编辑和 F1 分数 | JIGSAWS [[[6](#bib.bib6)], [[100](#bib.bib100)]]
    基准数据集 [可用代码](https://github.com/Finspire13/RL-Surgical-Gesture-Segmentation) |'
- en: '| Cutting agent for video object segmentation [[124](#bib.bib124)] | 2018 |
    DQN | 8 actions: 4 translation actions (Up, Down, Left, Right), 4 scaling action
    (Horizontal shrink, Vertical shrink, Horizontal zoom, Vertical zoom) and 1 terminal
    action (Stop) | States: input frame, action history and segmentation mask. Reward:
    change in IOU. cutting-policy network for box-context pair and cutting-execution
    network for mask generation | DenseNet [[166](#bib.bib166)] | Higher mean region
    similarity, counter accuracy and temporal stability [[293](#bib.bib293)] as compared
    to methods like MSK [[292](#bib.bib292)], ARP [[173](#bib.bib173)], CTN [[165](#bib.bib165)],
    VPN [[164](#bib.bib164)], etc. | DAVIS dataset [[293](#bib.bib293)] and the YouTube
    Objects dataset [[162](#bib.bib162)], [[300](#bib.bib300)] |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 视频对象分割的切割策略 [[124](#bib.bib124)] | 2018 | DQN | 8 种动作：4 种平移动作（上、下、左、右）、4
    种缩放动作（水平方向收缩、垂直方向收缩、水平方向放大、垂直方向放大）和 1 种终端动作（停止） | 状态：输入帧、动作历史和分割掩码。奖励：IOU 的变化。用于框上下文对的切割策略网络和用于掩码生成的切割执行网络
    | DenseNet [[166](#bib.bib166)] | 相较于 MSK [[292](#bib.bib292)]、ARP [[173](#bib.bib173)]、CTN
    [[165](#bib.bib165)]、VPN [[164](#bib.bib164)] 等方法，具有更高的均值区域相似性、计数准确性和时间稳定性 [[293](#bib.bib293)]
    | DAVIS 数据集 [[293](#bib.bib293)] 和 YouTube Objects 数据集 [[162](#bib.bib162)]、[[300](#bib.bib300)]
    |'
- en: '| Unsupervised video object segmentation (MOREL) [[111](#bib.bib111)] | 2018
    | Actor-critic (a2c) [[262](#bib.bib262)] | Not specified | States: consecutive
    frames. Two step process with optical flow using Spatial Transformer Networks
    [[159](#bib.bib159)] and reconstruction loss using structural dissimilarity [[388](#bib.bib388)].
    | Multi-layer CNN | Higher total episodic reward as compared to methods that used
    actor-critic without MOREL | 59 Atari games. [Available Code](https://github.com/vik-goel/MOREL)
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 无监督视频对象分割 (MOREL) [[111](#bib.bib111)] | 2018 | Actor-critic (a2c) [[262](#bib.bib262)]
    | 未指定 | 状态：连续帧。通过空间变换网络 [[159](#bib.bib159)] 进行光流的两步处理和使用结构差异 [[388](#bib.bib388)]
    的重建损失。 | 多层 CNN | 相较于未使用 MOREL 的 Actor-critic 方法，具有更高的总情节奖励 | 59 款 Atari 游戏。 [可用代码](https://github.com/vik-goel/MOREL)
    |'
- en: '| Face video segmentation [[387](#bib.bib387)] | 2020 | Not specified | 2 actions:
    categorising a frame as a key or a non-key | States: deviation information which
    described the difference between current non-key and last key decision, and expert
    information which encapsulated the key decision history. Reward: improvement in
    mean IOU/accuracy score between segmentation of key and non-key frames | Multi-layer
    CNN | Higher mean IOU then other methods like DVSNet [[410](#bib.bib410)], DFF
    [[431](#bib.bib431)]. | 300VW dataset [[336](#bib.bib336)] and Cityscape dataset
    [[61](#bib.bib61)] |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 面部视频分割 [[387](#bib.bib387)] | 2020 | 未指定 | 2 种动作：将帧分类为关键帧或非关键帧 | 状态：描述当前非关键帧与上一个关键帧决策之间差异的偏差信息，以及封装关键决策历史的专家信息。奖励：改善关键帧和非关键帧分割之间的平均
    IOU/准确性分数 | 多层 CNN | 比如 DVSNet [[410](#bib.bib410)]、DFF [[431](#bib.bib431)] 等方法，具有更高的平均
    IOU。 | 300VW 数据集 [[336](#bib.bib336)] 和 Cityscape 数据集 [[61](#bib.bib61)] |'
- en: '| Multi-agent Video Object Segmentation [[373](#bib.bib373)] | 2020 | DQN |
    Actions of 2 types: movement actions (up, down, left and right) and set action
    (action to place location prior at a random location on the patch) | States: input
    frame, optical flow [[156](#bib.bib156)] from previous frame and action history.
    Reward: clicks generated by gamification. Down-sampling and up-sampling similar
    to U-Net [[313](#bib.bib313)] | DenseNet [[147](#bib.bib147)] | Higher mean region
    similarity and contour accuracy [[293](#bib.bib293)] as compared to semi-supervised
    methods such as SeamSeg [[14](#bib.bib14)], BSVS [[248](#bib.bib248)], VSOF [[363](#bib.bib363)],
    OSVOS [[41](#bib.bib41)] and weakly-supervised methods such as GVOS [[346](#bib.bib346)],
    Spftn [[419](#bib.bib419)] | DAVIS-17 dataset [[293](#bib.bib293)] |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 多智能体视频目标分割 [[373](#bib.bib373)] | 2020 | DQN | 2 种类型的动作：移动动作（上、下、左、右）和设置动作（将位置放置在补丁上的随机位置）
    | 状态：输入帧、来自前一帧的光流 [[156](#bib.bib156)] 和动作历史。奖励：由游戏化产生的点击。与 U-Net [[313](#bib.bib313)]
    类似的下采样和上采样 | DenseNet [[147](#bib.bib147)] | 与半监督方法如 SeamSeg [[14](#bib.bib14)]、BSVS
    [[248](#bib.bib248)]、VSOF [[363](#bib.bib363)]、OSVOS [[41](#bib.bib41)] 和弱监督方法如
    GVOS [[346](#bib.bib346)]、Spftn [[419](#bib.bib419)] 相比，具有更高的平均区域相似度和轮廓精度 [[293](#bib.bib293)]
    | DAVIS-17 数据集 [[293](#bib.bib293)] |'
- en: '| Skeleton-based Action Recognition [[357](#bib.bib357)] | 2018 | DQN | 3 actions:
    shifting to left, staying the same and shifting to right | States: Global video
    information and selected frames. Reward: change in categorical probability. 2
    step network (FDNet) to filter frames and GCNN for action labels | Multi-layer
    CNN | Higher cross subject and cross view metrics for NTU+RGBD dataset [[333](#bib.bib333)],
    and higher accuracy for SYSU-3D [[145](#bib.bib145)] and UT-Kinect Dataset [[399](#bib.bib399)]
    when compared with other methods like Dynamic Skeletons [[145](#bib.bib145)],
    HBRNN-L [[81](#bib.bib81)], Part-aware LSTM [[333](#bib.bib333)], LieNet-3Blocks
    [[151](#bib.bib151)], Two-Stream CNN [[211](#bib.bib211)], etc. | NTU+RGBD [[333](#bib.bib333)],
    SYSU-3D [[145](#bib.bib145)] and UT-Kinect Dataset [[399](#bib.bib399)] |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 基于骨架的动作识别 [[357](#bib.bib357)] | 2018 | DQN | 3 种动作：向左移动、保持不变和向右移动 | 状态：全局视频信息和选择的帧。奖励：类别概率的变化。2
    步网络 (FDNet) 过滤帧，GCNN 用于动作标签 | 多层 CNN | 对 NTU+RGBD 数据集 [[333](#bib.bib333)] 的跨主体和跨视角度量更高，对
    SYSU-3D [[145](#bib.bib145)] 和 UT-Kinect 数据集 [[399](#bib.bib399)] 的准确率更高，与其他方法如动态骨架
    [[145](#bib.bib145)]、HBRNN-L [[81](#bib.bib81)]、Part-aware LSTM [[333](#bib.bib333)]、LieNet-3Blocks
    [[151](#bib.bib151)]、Two-Stream CNN [[211](#bib.bib211)] 等相比。 | NTU+RGBD [[333](#bib.bib333)]、SYSU-3D
    [[145](#bib.bib145)] 和 UT-Kinect 数据集 [[399](#bib.bib399)] |'
- en: '| Video summarisation [[429](#bib.bib429)] | 2018 | DQN | 2 actions: selecting
    and rejecting the frame | tates: bidirectional LSTM [[150](#bib.bib150)] produced
    states by input frame features. Reward: Diversity-Representativeness Reward Function.
    | GoogLeNet [[355](#bib.bib355)] | Higher F-score [[421](#bib.bib421)] as compared
    to methods like Uniform sampling, K-medoids, Dictionary selection [[88](#bib.bib88)],
    Video-MMR [[218](#bib.bib218)], Vsumm [[69](#bib.bib69)], etc. | TVSum [[344](#bib.bib344)]
    and SumMe [[122](#bib.bib122)]. [Available Code](https://github.com/KaiyangZhou/pytorch-vsumm-reinforce)
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 视频总结 [[429](#bib.bib429)] | 2018 | DQN | 2 种动作：选择和拒绝帧 | 状态：由输入帧特征生成的双向 LSTM
    [[150](#bib.bib150)] 状态。奖励：多样性-代表性奖励函数。 | GoogLeNet [[355](#bib.bib355)] | 与均匀采样、K-medoids、字典选择
    [[88](#bib.bib88)]、视频-MMR [[218](#bib.bib218)]、Vsumm [[69](#bib.bib69)] 等方法相比，F-score
    [[421](#bib.bib421)] 较高。 | TVSum [[344](#bib.bib344)] 和 SumMe [[122](#bib.bib122)]。[可用代码](https://github.com/KaiyangZhou/pytorch-vsumm-reinforce)
    |'
- en: '| Video summarization [[430](#bib.bib430)] | 2018 | Duel DQN Double DQN | 2
    actions: selecting and rejecting the frame | States: sequence of frames Reward:
    Diversity-Representativeness Reward Function 2 stage implementation: classification
    and summarisation network using bidirectional GRU network and LSTM [[150](#bib.bib150)]
    | GoogLeNet [[355](#bib.bib355)] | Higher F-score [[421](#bib.bib421)] as compared
    to methods like Dictionary selection [[88](#bib.bib88)], GAN [[245](#bib.bib245)],
    DR-DSN [[429](#bib.bib429)], Backprop-Grad [[287](#bib.bib287)], etc in most cases.
    | TVSum [[344](#bib.bib344)] and CoSum [[57](#bib.bib57)] datasets. [Available
    Code](https://github.com/KaiyangZhou) |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 视频摘要 [[430](#bib.bib430)] | 2018 | Duel DQN Double DQN | 2 种动作：选择和拒绝帧 | 状态：帧序列
    奖励：多样性-代表性奖励函数 2 阶段实现：使用双向 GRU 网络和 LSTM [[150](#bib.bib150)] 的分类和总结网络 | GoogLeNet
    [[355](#bib.bib355)] | 与字典选择 [[88](#bib.bib88)]、GAN [[245](#bib.bib245)]、DR-DSN
    [[429](#bib.bib429)]、Backprop-Grad [[287](#bib.bib287)] 等方法相比，F-score [[421](#bib.bib421)]
    较高。 | TVSum [[344](#bib.bib344)] 和 CoSum [[57](#bib.bib57)] 数据集。[可用代码](https://github.com/KaiyangZhou)
    |'
- en: '| Video summarization in Ultrasound [[233](#bib.bib233)] | 2020 | Not specified
    | 2 actions: selecting and rejecting the frame | States: frame latent scores Reward:
    $R_{det}$, $R_{rep}$ and $R_{div}$ bidirectional LSTM [[150](#bib.bib150)] and
    Kernel temporal segmentation [[298](#bib.bib298)] | Not specified | Higher F1-scores
    in supervised and unsupervised fashion as compared to methods like FCSN [[312](#bib.bib312)]
    and DR-DSN [[429](#bib.bib429)]. | Fetal Ultrasound [[179](#bib.bib179)] |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 超声视频摘要 [[233](#bib.bib233)] | 2020 | 未指定 | 2种动作：选择和拒绝帧 | 状态：帧潜在分数 奖励：$R_{det}$、$R_{rep}$
    和 $R_{div}$ 双向LSTM[[150](#bib.bib150)] 和内核时间分割[[298](#bib.bib298)] | 未指定 | 与FCSN[[312](#bib.bib312)]和DR-DSN[[429](#bib.bib429)]等方法相比，监督和非监督方式下的F1得分更高。
    | 胎儿超声[[179](#bib.bib179)] |'
- en: Video object segmentation using human-provided location priors have been capable
    of producing promising results. An RL method for this task was proposed by [[373](#bib.bib373)],
    in which the authors proposed MASK-RL, a multiagent RL framework for object segmentation
    in videos. They proposed a weakly supervised method where the location priors
    were provided by the user in form of clicks using gamification (Web game to collect
    location priors by different users) to support the segmentation and used a Gaussian
    filter to emphasize the areas. The segmentation network is fed a 12 channel input
    tensor that contained a sequence of video frames and their corresponding location
    priors (3 $\times$ 3 color channels + three gray-scale images). The authors used
    a fully convoluted DenseNet [[147](#bib.bib147)] with down-sampling and up-sampling
    similar to U-Net [[313](#bib.bib313)] and an LSTM [[139](#bib.bib139)] for the
    segmentation network. For the RL method, the actor takes a series of steps over
    a frame divided into a grid of equal size patches and makes the decision whether
    there is an object in the patch or not. In their MDP formulation the states consisted
    of the input frame, optical flow (computed by [[156](#bib.bib156)]) from the previous
    frame, patch from the previous iteration, and the episode location history, the
    actions consisted of movement actions (up, down, left and right) and set action
    (action to place location prior at a random location on the patch), and two types
    of rewards one for set actions and one for movement actions were used. The reward
    was calculated using the clicks generated by the game player.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人类提供的位置先验进行的视频对象分割已能够产生令人满意的结果。一种针对该任务的强化学习方法由[[373](#bib.bib373)]提出，其中作者提出了MASK-RL，一个用于视频中对象分割的多智能体强化学习框架。他们提出了一种弱监督方法，其中位置先验通过用户点击的形式提供，并通过游戏化（Web游戏收集不同用户的位置先验）来支持分割，并使用高斯滤波器来强调这些区域。分割网络接收一个12通道的输入张量，该张量包含一系列视频帧及其对应的位置先验（3
    $\times$ 3 色彩通道 + 三张灰度图像）。作者使用了一种完全卷积的DenseNet[[147](#bib.bib147)]，其下采样和上采样方式类似于U-Net[[313](#bib.bib313)]，以及用于分割网络的LSTM[[139](#bib.bib139)]。对于强化学习方法，演员在一个被划分为大小相等的补丁的帧上采取一系列步骤，并决定补丁中是否存在对象。在他们的MDP模型中，状态包括输入帧、来自前一帧的光流（由[[156](#bib.bib156)]计算）、来自前一迭代的补丁和情节位置历史，动作包括移动动作（上、下、左和右）和设置动作（在补丁上的随机位置放置位置先验的动作），并使用了两种奖励，一种用于设置动作，另一种用于移动动作。奖励是使用游戏玩家生成的点击来计算的。
- en: 'Action recognition is an important task in the computer vision field which
    focuses on categorizing the action that is being performed in the video frame.
    To address the problem a deep progressive RL (DPRL) method for action recognition
    in skeleton-based videos was proposed by [[357](#bib.bib357)]. The authors proposed
    a method that distills the most informative frames and discards ambiguous frames
    by considering the quality of the frame and the relationship of the frame with
    the complete video along with a graph-based structure to map the human body in
    form of joints and vertices. DPRL was utilized to filter out informative frames
    in a video and graph-based CNNs were used to learn the spatial dependency between
    the joints. The approach consisted of two sub-networks, a frame distillation network
    (FDNet) to filter a fixed number of frames from input sequence using DPRL and
    GCNN to recognize the action labels using output in form of a graphical structure
    by the FDNet. The authors modeled the problem as an MDP where the state consisted
    of the concatenation of two tensors $F$ and $M$, where $F$ consisted of global
    information about the video and $M$ consisted of the frames that were filtered,
    The actions which correspond to the output of FDNet were divided into three types:
    shifting to left, staying the same and shifting to the right, and the reward function
    corresponded to the change in probability of categorizing the video equal to the
    ground truth clipped it between [-1 and 1] and is provided by GCNN to FDNet.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 动作识别是计算机视觉领域中的一项重要任务，专注于对视频帧中正在执行的动作进行分类。为了解决这一问题，[[357](#bib.bib357)] 提出了一个用于骨架视频的深度渐进强化学习（DPRL）方法。作者提出了一种方法，通过考虑帧的质量以及帧与完整视频的关系，并结合图形结构将人体映射为关节和顶点，提取最有信息量的帧并丢弃模糊帧。DPRL
    被用于过滤视频中的信息帧，而基于图形的卷积神经网络（CNNs）则用于学习关节之间的空间依赖关系。该方法包括两个子网络：一个帧提取网络（FDNet）用于使用
    DPRL 从输入序列中筛选出固定数量的帧，另一个是 GCNN，用于通过 FDNet 生成的图形结构输出识别动作标签。作者将问题建模为一个马尔可夫决策过程（MDP），其中状态由两个张量
    $F$ 和 $M$ 的拼接组成，其中 $F$ 包含视频的全局信息，$M$ 包含过滤后的帧。对应于 FDNet 输出的动作被分为三种类型：向左移动、保持不变和向右移动，奖励函数与将视频分类为与真实标签相等的概率变化相关，并在[-1和1]之间裁剪，由
    GCNN 提供给 FDNet。
- en: Video summarization is a useful yet difficult task in the computer vision field
    that involves predicting the object or the task that is being performed in a video.
    A DRL method for unsupervised video summarisation was proposed by [[429](#bib.bib429)],
    in which the authors proposed a Diversity-Representativeness reward system and
    a deep summarisation network (DSN) which was capable of predicting a probability
    for each video frame that specified the likeliness of selecting the frame and
    then take actions to form video summaries. They used an encode-decoder framework
    for the DSN where GoogLeNet [[355](#bib.bib355)] pre-trained on ImageNet [[320](#bib.bib320)]
    [[72](#bib.bib72)] was used as an encoder and a bidirectional RNNs (BiRNNs) topped
    with a fully connected (FC) layer was used as a decoder. The authors modeled the
    problem as an MDP where the action corresponded to the task of selecting or rejecting
    a frame. They proposed a novel Diversity-Representativeness Reward Function in
    their implementation, where diversity reward corresponded to the degree of dissimilarity
    among the selected frames in feature space, and representativeness reward measured
    how well the generated summary can represent the original video. For the RNN unit
    they used an LSTM [[139](#bib.bib139)] to capture long-term video dependencies
    and used REINFORCE algorithm for training the policy function.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 视频摘要是计算机视觉领域中一项有用但困难的任务，涉及预测视频中正在执行的对象或任务。[[429](#bib.bib429)] 提出了一个无监督视频摘要的深度强化学习（DRL）方法，作者提出了一种多样性-代表性奖励系统和一个深度摘要网络（DSN），该网络能够为每个视频帧预测一个概率，以指定选择该帧的可能性，并采取行动形成视频摘要。他们为
    DSN 使用了一个编码-解码框架，其中预训练于 ImageNet [[320](#bib.bib320)] [[72](#bib.bib72)] 的 GoogLeNet
    [[355](#bib.bib355)] 被用作编码器，双向 RNN（BiRNNs）加上全连接层（FC）被用作解码器。作者将问题建模为一个 MDP，其中动作对应于选择或拒绝一个帧的任务。他们在实现中提出了一种新颖的多样性-代表性奖励函数，其中多样性奖励对应于选择的帧在特征空间中的不相似程度，而代表性奖励则衡量生成的摘要对原始视频的代表性。对于
    RNN 单元，他们使用了 LSTM [[139](#bib.bib139)] 以捕捉长期视频依赖关系，并使用 REINFORCE 算法训练策略函数。
- en: '![Refer to caption](img/dd162a8f5e9bde527aab23443aacd365.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd162a8f5e9bde527aab23443aacd365.png)'
- en: 'Figure 16: DRL implementation for video summarization. For state a sequence
    of consecutive frames are used and the DRL agent decided whether to include the
    frame in the summary set that is used to predict video summary.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：视频摘要的DRL实现。对于状态，使用一系列连续的帧，DRL代理决定是否将帧包含在用于预测视频摘要的摘要集中。
- en: 'An improvement to [[429](#bib.bib429)] was proposed by [[430](#bib.bib430)],
    where the summarisation network was implemented using Deep Q-learning (DQSN),
    and a trained classification network was used to provide a reward for training
    the DQSN. The approach included using (Bi-GRU) bidirectional recurrent networks
    with a gated recurrent unit (GRU) [[50](#bib.bib50)] for both classification and
    summarisation network. The authors first trained the classification network using
    a supervised classification loss and then used the classification network with
    fixed weights for the classification of summaries generated by the summarisation
    network. The summarisation network included an MDP-based framework in which states
    consisted of a sequence of video frames and actions reflected the task of either
    keeping the frame or discarding it. They used a structure similar to Duel-DQN
    where value function and advantage function are trained together. In their implementation,
    the authors considered 3 different rewards: Global Recognisability reward using
    the classification network with +1 as reward and -5 as punishment, Local Relative
    Importance Reward for rewarding the action of accepting or rejecting a frame by
    summarisation network, and an Unsupervised Reward that is computed globally using
    the unsupervised diversity-representativeness (DR) reward proposed in [[429](#bib.bib429)].
    The authors trained both the networks using the features generated by GoogLeNet
    [[355](#bib.bib355)] pre-trained on ImageNet [[72](#bib.bib72)].'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[[429](#bib.bib429)]的改进由[[430](#bib.bib430)]提出，其中摘要网络使用了深度Q学习（DQSN）进行实现，并且使用了一个训练好的分类网络来为训练DQSN提供奖励。该方法包括使用（Bi-GRU）双向递归网络与门控递归单元（GRU）[[50](#bib.bib50)]来处理分类和摘要网络。作者首先使用有监督的分类损失训练分类网络，然后使用固定权重的分类网络对摘要网络生成的摘要进行分类。摘要网络包括一个基于MDP的框架，其中状态由一系列视频帧组成，而动作反映了要么保留帧，要么丢弃帧。他们使用了一种类似于Duel-DQN的结构，其中价值函数和优势函数一起训练。在他们的实现中，作者考虑了3种不同的奖励：使用分类网络的全局可识别性奖励，+1作为奖励，-5作为惩罚；摘要网络对接受或拒绝帧的动作进行奖励的局部相对重要性奖励；以及一个无监督奖励，该奖励使用[[429](#bib.bib429)]中提出的无监督多样性-代表性（DR）奖励进行全局计算。作者使用在ImageNet
    [[72](#bib.bib72)]上预训练的GoogLeNet [[355](#bib.bib355)]生成的特征训练了这两个网络。
- en: A method for video summarization in Ultrasound using DRL was proposed by [[233](#bib.bib233)],
    in which the authors proposed a deep summarisation network in an encoder-decoder
    fashion and used a bidirectional LSTM (Bi-LSTM) [[150](#bib.bib150)] for sequential
    modeling. In their implementation, the encoder-decoder convolution network extracted
    features from video frames and fed them into the Bi-LSTM. The RL network accepted
    states in form of latent scores from Bi-LSTM and produced actions, where the actions
    consist of the task of including or discarding the video frame inside the summary
    set that is used to produce video summaries. The authors used three different
    rewards $R_{det}$, $R_{rep}$ and $R_{div}$ where $R_{det}$ evaluated the likelihood
    of a frame being a standard diagnostic plane, $R_{rep}$ defined the representativeness
    reward and $R_{div}$ was the diversity reward that evaluated the quality of the
    selected summary. They used Kernel temporal segmentation (KTS) [[298](#bib.bib298)]
    for video summary generalization.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[[233](#bib.bib233)]提出了一种使用DRL的视频摘要方法，其中作者提出了一种以编码器-解码器方式构建的深度摘要网络，并使用双向LSTM（Bi-LSTM）[[150](#bib.bib150)]进行序列建模。在他们的实现中，编码器-解码器卷积网络从视频帧中提取特征，并将其输入到Bi-LSTM中。RL网络接受来自Bi-LSTM的潜在分数形式的状态，并产生动作，其中动作包括将视频帧包含或丢弃在用于生成视频摘要的摘要集中。作者使用了三种不同的奖励：$R_{det}$，$R_{rep}$和$R_{div}$，其中$R_{det}$评估帧作为标准诊断平面的可能性，$R_{rep}$定义了代表性奖励，$R_{div}$是评估所选摘要质量的多样性奖励。他们使用了Kernel时间分割（KTS）[[298](#bib.bib298)]来进行视频摘要泛化。'
- en: 'Various works associated with video analysis have been summarised and compared
    in Table LABEL:tab:vid and a basic implementation of video summarization using
    DRL has been shown in Fig. [16](#S10.F16 "Figure 16 ‣ 10 DRL in Video Analysis
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey"), where
    the states consist of a sequence of video frames. The DRL agent performs actions
    to include or discard a frame from the summary set that is later used by the summarization
    network to predict video summary. Each research paper propose their own reward
    function for this application, for example [[429](#bib.bib429)] and [[430](#bib.bib430)]
    used diversity representativeness reward function and [[233](#bib.bib233)] used
    a combination of various reward functions.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '与视频分析相关的各种工作已经在表格 LABEL:tab:vid 中总结和比较，使用深度强化学习（DRL）进行视频摘要的基本实现已在图 [16](#S10.F16
    "Figure 16 ‣ 10 DRL in Video Analysis ‣ Deep Reinforcement Learning in Computer
    Vision: A Comprehensive Survey") 中展示，其中状态包括一系列视频帧。DRL 代理执行动作以包括或丢弃摘要集合中的帧，之后由摘要网络用来预测视频摘要。每篇研究论文为此应用提出了自己的奖励函数，例如
    [[429](#bib.bib429)] 和 [[430](#bib.bib430)] 使用了多样性代表性奖励函数，[[233](#bib.bib233)]
    使用了各种奖励函数的组合。'
- en: 11 Others Applications
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 个其他应用
- en: 'Object manipulation refers to the task of handling and manipulating an object
    using a robot. A method for deformable object manipulation using RL was proposed
    by [[250](#bib.bib250)], where the authors used a modified version of Deep Deterministic
    Policy Gradients (DDPG) [[224](#bib.bib224)]. They used the simulator Pybullet
    [[63](#bib.bib63)] for the environment where the observation consisted of a $84\times
    84\times 3$ image, the state consists of joint angles and gripper positions and
    action of four dimensions: first three for velocity and lasts for gripper velocity
    was used. The authors used sparse reward for the task that returns the reward
    at the completion of the task. They used the algorithm to perform tasks such as
    folding and hanging cloth and got a success rate of up to 90%.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 物体操控指的是使用机器人处理和操控物体的任务。[[250](#bib.bib250)] 提出了使用强化学习（RL）进行可变形物体操控的方法，其中作者使用了修改版的深度确定性策略梯度（DDPG）[[224](#bib.bib224)]。他们使用了模拟器
    Pybullet [[63](#bib.bib63)] 作为环境，其中观察值是 $84\times 84\times 3$ 图像，状态包括关节角度和夹持器位置，四维动作：前三维为速度，最后一维为夹持器速度。作者使用了稀疏奖励来完成任务，任务完成时返回奖励。他们使用该算法执行折叠和悬挂布料等任务，并取得了高达
    90% 的成功率。
- en: Visual perception-based control refers to the task of controlling robotic systems
    using a visual input. A virtual to real method for control using semantic segmentation
    was proposed by [[142](#bib.bib142)], in which the authors combined various modules
    such as, Perception module, control policy module, and a visual guidance module
    to perform the task. For the perception module, the authors directly used models
    such as DeepLab [[46](#bib.bib46)] and ICNet [[424](#bib.bib424)], pre-trained
    on ADE20K [[428](#bib.bib428)] and Cityscape [[61](#bib.bib61)], and used the
    output of these model as the state for the control policy module. They implemented
    the control policy module using the actor-critic [[262](#bib.bib262)] framework,
    where the action consisted of forward, turn right, and turn left. In their implementation,
    a reward of 0.001 is given at each time step. They used the Unity3D engine for
    the environment and got higher success and lower collision rate than other implementations
    such as ResNet-A3C and Depth-A3C.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视觉感知的控制指的是使用视觉输入来控制机器人系统的任务。[[142](#bib.bib142)] 提出了一个使用语义分割进行控制的虚拟到现实方法，作者结合了多个模块，例如感知模块、控制策略模块和视觉指导模块来执行任务。在感知模块中，作者直接使用了如
    DeepLab [[46](#bib.bib46)] 和 ICNet [[424](#bib.bib424)] 这样的模型，这些模型在 ADE20K [[428](#bib.bib428)]
    和 Cityscape [[61](#bib.bib61)] 上进行了预训练，并将这些模型的输出用作控制策略模块的状态。他们使用了 actor-critic
    [[262](#bib.bib262)] 框架来实现控制策略模块，其中动作包括前进、右转和左转。在他们的实现中，每个时间步给予 0.001 的奖励。他们使用
    Unity3D 引擎作为环境，并取得了比 ResNet-A3C 和 Depth-A3C 等其他实现更高的成功率和更低的碰撞率。
- en: Automatic tracing of structures such as axons and blood vessels is an important
    yet challenging task in the field of biomedical imaging. A DRL method for sub-pixel
    neural tracking was proposed by [[65](#bib.bib65)], where the authors used 2D
    grey-scale images as the environment. They considered a full resolution 11px $\times$
    11px window and a 21px $\times$ 21px window down-scaled to 11px $\times$ 11px
    as state and the actions were responsible for moving the position of agent in
    2D space using continuous control for sub-pixel tracking because axons can be
    smaller then a pixel. The authors used a reward that was calculated using the
    average integral of intensity between the agent’s current and next location, and
    the agent was penalized if it does not move or changes directions more than once.
    They used an Actor-critic [[262](#bib.bib262)] framework to estimate value and
    policy functions.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 自动追踪轴突和血管等结构在生物医学成像领域是一个重要但具有挑战性的任务。[[65](#bib.bib65)] 提出了用于亚像素神经跟踪的 DRL 方法，其中作者使用了
    2D 灰度图像作为环境。他们考虑了一个全分辨率的 11px $\times$ 11px 窗口和一个缩小到 11px $\times$ 11px 的 21px
    $\times$ 21px 窗口作为状态，动作负责通过连续控制在 2D 空间中移动代理的位置，因为轴突可能比像素还要小。作者使用了一个奖励，该奖励通过计算代理当前位置与下一个位置之间的强度平均积分来获得，如果代理不移动或改变方向超过一次，则会受到惩罚。他们使用了
    Actor-critic [[262](#bib.bib262)] 框架来估计价值和策略函数。
- en: An RL method for automatic diagnosis of acute appendicitis in abdominal CT images
    was proposed by [[8](#bib.bib8)], in which the authors used RL to find the location
    of the appendix and then used a CNN classifier to find the likelihood of Acute
    Appendicitis, finally they defined a region of low-entropy (RLE) using the spatial
    representation of output scores to obtain optimal diagnosis scores. The authors
    considered the problem of appendix localization as an MDP, where the state consisted
    of a $50\times 50\times 50$ volume around the predicted appendix location, 6 actions
    (2 per axis) were used and the reward consisted of the change in distance between
    the predicted appendix location and actual appendix location across an action.
    They utilized an Actor-critic [[262](#bib.bib262)] framework to estimate policy
    and value functions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8](#bib.bib8)] 提出了一种用于腹部 CT 图像中急性阑尾炎自动诊断的 RL 方法，其中作者使用 RL 来寻找阑尾的位置，然后使用 CNN
    分类器来确定急性阑尾炎的可能性，最后他们使用输出分数的空间表示定义了低熵区域 (RLE) 来获得最佳诊断分数。作者将阑尾定位问题视为 MDP，其中状态由预测阑尾位置周围的
    $50\times 50\times 50$ 体积组成，使用了 6 个动作（每个轴 2 个），奖励包括预测阑尾位置和实际阑尾位置之间的距离变化。他们利用 Actor-critic
    [[262](#bib.bib262)] 框架来估计策略和值函数。'
- en: 'Table 9: Comparing various other methods besides landmark detection, object
    detection, object tracking, image registration, image segmentation, video analysis,
    that is associated with DRL'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：比较除地标检测、物体检测、物体跟踪、图像配准、图像分割、视频分析之外的各种方法，这些方法与深度强化学习（DRL）相关
- en: '| Approaches | Year | Training Technique | Actions | Remarks | Backbone | Performance
    | Datasets Source code |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 训练技术 | 动作 | 备注 | 骨干网络 | 性能 | 数据集 源代码 |'
- en: '| Object manipulation [[250](#bib.bib250)] | 2018 | Rainbow DDPG | 4 actions:
    3 for velocity 1 for gripper velocity | State: joint angle and gripper position.
    Reward: at the end of task. | Multi layer CNN | Success rate up to 90% | Pybullet
    [[63](#bib.bib63)]. [Code](https://github.com/JanMatas/Rainbow_ddpg) |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 物体操作 [[250](#bib.bib250)] | 2018 | Rainbow DDPG | 4 个动作：3 个用于速度，1 个用于抓取器速度
    | 状态：关节角度和抓取器位置。奖励：任务结束时。 | 多层 CNN | 成功率高达 90% | Pybullet [[63](#bib.bib63)]。
    [代码](https://github.com/JanMatas/Rainbow_ddpg) |'
- en: '| Visual based control [[142](#bib.bib142)] | 2018 | Actor-critic (a3c) [[262](#bib.bib262)]
    | 3 actions: forward, turn right and turn left | State: output by backbones. Reward:
    0.001 at each time-step. | DeepLab [[46](#bib.bib46)] and ICNet [[424](#bib.bib424)]
    | Higher success and lower collision rate then ResNet-A3C and Depth-A3C | Unity3D
    engine |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 基于视觉的控制 [[142](#bib.bib142)] | 2018 | Actor-critic (a3c) [[262](#bib.bib262)]
    | 3 个动作：前进、向右转和向左转 | 状态：由骨干网络输出。奖励：每个时间步为0.001。 | DeepLab [[46](#bib.bib46)] 和
    ICNet [[424](#bib.bib424)] | 成功率更高，碰撞率低于 ResNet-A3C 和 Depth-A3C | Unity3D 引擎 |'
- en: '| Automatic tracing [[65](#bib.bib65)] | 2019 | Actor-critic [[262](#bib.bib262)]
    | 4 actions | State: 11px $\times$ 11px window. Reward: average integral of intensity
    between the agent’s current and next location. | Multi layer CNN | Comparable
    convergence $\%$ and average error as compared to other methods like Vaa3D software
    [[291](#bib.bib291)] and APP2 neuron tracer [[403](#bib.bib403)] | Synthetic and
    microscopy dataset [[24](#bib.bib24)] |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 自动追踪 [[65](#bib.bib65)] | 2019 | 演员-评论员 [[262](#bib.bib262)] | 4 个动作 | 状态：11px
    $\times$ 11px 窗口。奖励：代理当前和下一个位置之间强度的平均积分。 | 多层CNN | 与其他方法如Vaa3D软件 [[291](#bib.bib291)]
    和APP2神经元追踪器 [[403](#bib.bib403)] 相比，收敛率$\%$ 和平均误差相当 | 合成数据集和显微镜数据集 [[24](#bib.bib24)]
    |'
- en: '| Automatic diagnosis (RLE) [[8](#bib.bib8)] | 2019 | Actor-critic [[262](#bib.bib262)]
    | 6 actions: 2 per axis | State: $50\times 50\times 50$ volume. Reward: change
    in distance error. | Fully connected CNN | Higher sensitivity and specificity
    as compared to only CNN classifier and CNN classifier with RL without RLE. | Abdominal
    CT Scans |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 自动诊断 (RLE) [[8](#bib.bib8)] | 2019 | 演员-评论员 [[262](#bib.bib262)] | 6 个动作：每轴
    2 个 | 状态：$50\times 50\times 50$ 体积。奖励：距离误差的变化。 | 全连接CNN | 与仅CNN分类器和不使用RLE的带RL
    CNN分类器相比，灵敏度和特异性更高。 | 腹部CT扫描 |'
- en: '| Learning to paint [[149](#bib.bib149)] | 2019 | Actor-critic with DDPG |
    Actions control the stoke parameter: location, shape, color and transparency |
    State: Reference image, Drawing canvas and time step. Reward: change in discriminator
    score (calculated by WGAN-GP [[117](#bib.bib117)] across an action. GANs [[113](#bib.bib113)]
    to improve image quality | ResNet18 [[133](#bib.bib133)] | Able to replicate the
    original images to a large extent, and better resemblance to the original image
    as compared to SPIRAL [[98](#bib.bib98)] with same number of brush strokes. |
    MNIST [[202](#bib.bib202)], SVHN [[276](#bib.bib276)], CelebA [[235](#bib.bib235)]
    and ImageNet [[320](#bib.bib320)]. [Code](https://github.com/hzwer/ICCV2019-LearningToPaint)
    |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 学习绘画 [[149](#bib.bib149)] | 2019 | 带有DDPG的演员-评论员 | 动作控制笔触参数：位置、形状、颜色和透明度
    | 状态：参考图像、绘图画布和时间步长。奖励：鉴别器分数的变化（通过WGAN-GP [[117](#bib.bib117)] 在动作中计算）。GANs [[113](#bib.bib113)]
    用于提高图像质量 | ResNet18 [[133](#bib.bib133)] | 能在很大程度上复制原始图像，并且与SPIRAL [[98](#bib.bib98)]
    在相同数量的笔触下更好地与原始图像相似。 | MNIST [[202](#bib.bib202)]、SVHN [[276](#bib.bib276)]、CelebA
    [[235](#bib.bib235)] 和 ImageNet [[320](#bib.bib320)]。 [代码](https://github.com/hzwer/ICCV2019-LearningToPaint)
    |'
- en: '| Guiding medical robots [[129](#bib.bib129)] | 2020 | Double-DQN, Duel-DQN
    | 5 actions: up, down, left, right and stop | State: probe position. Reward: Move
    closer: 0.05, Move away: -0.1, Correct stop: 1.0, Incorrect stop: -0.25. | ResNet18
    [[133](#bib.bib133)] | Higher % of policy correctness and reachability as compared
    to CNN Classifier, where MS-DQN showed the best results | Ultrasound Images [Dataset](https://github.com/hhase/sacrumdata-set).
    [Code](https://github.com/hhase/spinal-navigation-rl) |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 指导医疗机器人 [[129](#bib.bib129)] | 2020 | Double-DQN，Duel-DQN | 5 个动作：上、下、左、右和停止
    | 状态：探针位置。奖励：靠近：0.05，远离：-0.1，正确停止：1.0，错误停止：-0.25。 | ResNet18 [[133](#bib.bib133)]
    | 与CNN分类器相比，政策正确性和可达性更高，其中MS-DQN表现最佳 | 超声图像 [数据集](https://github.com/hhase/sacrumdata-set)。
    [代码](https://github.com/hhase/spinal-navigation-rl) |'
- en: '| Crowd counting [[230](#bib.bib230)] | 2020 | DQN | 9 actions: -10, -5, -2,
    -1, +1, +2, +5, +10 and end | State: weight vector $W_{t}$ and image feature vector
    $FV_{I}$. Reward: Intermediate reward and ending reward | VGG16 [[340](#bib.bib340)]
    | Lower/comparable mean squared error (MSE) and mean absolute error (MAE) as compared
    to other methods like DRSAN [[232](#bib.bib232)], PGCNet [[412](#bib.bib412)],
    MBTTBF [[341](#bib.bib341)], S-DCNet [[405](#bib.bib405)], CAN [[234](#bib.bib234)],
    etc. | The ShanghaiTech (SHT) Dataset [[423](#bib.bib423)], The UCFCC50 Dataset
    [[154](#bib.bib154)] and The UCF-QNRF Dataset [[155](#bib.bib155)]. [Code](https://github.com/poppinace/libranet)
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 人群计数 [[230](#bib.bib230)] | 2020 | DQN | 9 个动作：-10、-5、-2、-1、+1、+2、+5、+10
    和结束 | 状态：权重向量 $W_{t}$ 和图像特征向量 $FV_{I}$。奖励：中间奖励和结束奖励 | VGG16 [[340](#bib.bib340)]
    | 与其他方法如DRSAN [[232](#bib.bib232)]、PGCNet [[412](#bib.bib412)]、MBTTBF [[341](#bib.bib341)]、S-DCNet
    [[405](#bib.bib405)]、CAN [[234](#bib.bib234)] 等相比，均方误差 (MSE) 和平均绝对误差 (MAE) 更低/相当
    | 上海科技大学 (SHT) 数据集 [[423](#bib.bib423)]、UCFCC50 数据集 [[154](#bib.bib154)] 和 UCF-QNRF
    数据集 [[155](#bib.bib155)]。 [代码](https://github.com/poppinace/libranet) |'
- en: '| Automated Exposure bracketing [[389](#bib.bib389)] | 2020 | Not Specified
    | selecting optimal bracketing from candidates | State: quality of generated HDR
    image. Reward: improvement in peak signal to noise ratio | AlexNet [[188](#bib.bib188)]
    | Higher peak signal to noise ratio as compared to other methods like Barakat
    [[22](#bib.bib22)], Pourreza-Shahri [[299](#bib.bib299)], Beek [[369](#bib.bib369)],
    etc. | Proposed benchmark dataset. [Code/data](https://github.com/wzhouxiff/EBSNetMEFNet)
    |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 自动曝光包围 [[389](#bib.bib389)] | 2020 | 未指定 | 从候选中选择最佳包围 | 状态：生成的 HDR 图像的质量。奖励：峰值信噪比的改善
    | AlexNet [[188](#bib.bib188)] | 与 Barakat [[22](#bib.bib22)]、Pourreza-Shahri
    [[299](#bib.bib299)]、Beek [[369](#bib.bib369)] 等方法相比，具有更高的峰值信噪比 | 提出的基准数据集。 [代码/数据](https://github.com/wzhouxiff/EBSNetMEFNet)
    |'
- en: '| Urban Autonomous driving [[361](#bib.bib361)] | 2020 | Rainbow-IQN | 36 or
    108 actions: ($9\times 4$) or ($27\times 4$), 9/27 steering and 4 throttle | State:
    environment variables like traffic light, pedestrians, position with respect to
    center lane. Reward: generated by CARLA waypoint API | Resnet18 [[133](#bib.bib133)]
    | Won the 2019 camera only CARLA challenge [[314](#bib.bib314)]. | CARLA urban
    driving simulator [[314](#bib.bib314)] [Code](https://github.com/valeoai/learningbycheating)
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 城市自动驾驶 [[361](#bib.bib361)] | 2020 | Rainbow-IQN | 36 或 108 个动作：($9\times
    4$) 或 ($27\times 4$)，9/27 个转向和 4 个油门 | 状态：环境变量，如交通灯、行人、与中心车道的相对位置。奖励：由 CARLA 路径点
    API 生成 | Resnet18 [[133](#bib.bib133)] | 赢得了 2019 年仅限相机的 CARLA 挑战 [[314](#bib.bib314)]。
    | CARLA 城市驾驶模拟器 [[314](#bib.bib314)] [代码](https://github.com/valeoai/learningbycheating)
    |'
- en: '| Mitigating bias in Facial Recognition [[382](#bib.bib382)] | 2020 | DQN |
    3 actions:(Margin adjustment) staying the same, shifting to a larger value and
    shifting to a smaller value | State: the race group, current adaptive margin and
    bias between the race group and Caucasians. Reward: change in the sum of inter-class
    and intra-class bias | Multi-layer CNN | Proposed algorithm had higher verification
    accuracy as compared to other methods such as CosFace [[379](#bib.bib379)] and
    ArcFace [[73](#bib.bib73)]. | RFW [[383](#bib.bib383)] and proposed novel datasets:
    BUPT-Globalface and BUPT-Balancedface [Data](http://www.whdeng.cn/RFW/index.html)
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 缓解面部识别中的偏见 [[382](#bib.bib382)] | 2020 | DQN | 3 个动作：（边距调整）保持不变、调整到更大值和调整到更小值
    | 状态：种族组、当前自适应边距和种族组与高加索人之间的偏见。奖励：类间和类内偏见的总和变化 | 多层 CNN | 与 CosFace [[379](#bib.bib379)]
    和 ArcFace [[73](#bib.bib73)] 等方法相比，提出的算法具有更高的验证准确率。 | RFW [[383](#bib.bib383)]
    和提出的新数据集：BUPT-Globalface 和 BUPT-Balancedface [数据](http://www.whdeng.cn/RFW/index.html)
    |'
- en: '| Attention mechanism to improve CNN performance [[212](#bib.bib212)] | 2020
    | DQN [[264](#bib.bib264)] | Actions are weights for every location or channel
    in the feature map. | State: Feature map at each intermediate layer of model.
    Reward: predicted by a LSTM model. | ResNet-101 [[133](#bib.bib133)] | Improves
    the performances of [[144](#bib.bib144)], [[205](#bib.bib205)] and [[396](#bib.bib396)],
    which attend on feature channel, spatial-channel and style, respectively | ImageNet
    [[72](#bib.bib72)] |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 用于提升 CNN 性能的注意力机制 [[212](#bib.bib212)] | 2020 | DQN [[264](#bib.bib264)]
    | 动作是特征图中每个位置或通道的权重。 | 状态：模型每个中间层的特征图。奖励：由 LSTM 模型预测。 | ResNet-101 [[133](#bib.bib133)]
    | 提高了 [[144](#bib.bib144)]、[[205](#bib.bib205)] 和 [[396](#bib.bib396)] 的性能，这些方法分别关注特征通道、空间通道和风格
    | ImageNet [[72](#bib.bib72)] |'
- en: '![Refer to caption](img/70ccfd4e0c164cd86be06ea3df464348.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/70ccfd4e0c164cd86be06ea3df464348.png)'
- en: 'Figure 17: A general DRL implementation for agent movement with visual inputs.
    The state is provided by the environment based on which the agent performs movement
    actions to get a new state and a reward from the environment.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：一种用于代理运动的通用 DRL 实现，使用视觉输入。状态由环境提供，代理基于此执行运动动作，从环境中获得新的状态和奖励。
- en: 'Painting using an algorithm is a fantastic yet challenging task in the computer
    vision field. An automated painting method was proposed by [[149](#bib.bib149)],
    where the authors introduced a model-based DRL technique for this task. The specified
    work involved using a neural renderer in DRL, where the agent was responsible
    for making a decision about the position and color of each stroke, and making
    long-term decisions to organize those strokes into a visual masterpiece. In this
    work, GANs [[113](#bib.bib113)] were employed to improve image quality at pixel-level
    and DDPG [[224](#bib.bib224)] was utilized for determining the policy. The authors
    formulated the problem as an MDP, where the state consisted of three parts: the
    target image $I$, the canvas on which actions (paint strokes) are performed $C_{t}$,
    and the time step. The actions corresponding to a set of parameters that controlled
    the position, shape, color, and transparency of strokes, and for reward the WGAN
    with gradient penalty (WGAN-GP) [[117](#bib.bib117)] was used to calculate the
    discriminator score between the target image $I$ and the canvas $C_{t}$, and the
    change in discriminator score across an action (time-step) was used as the reward.
    The agent that predicted the stroke parameters was trained in actor-critic [[262](#bib.bib262)]
    fashion with backbone similar to Resnet18 [[133](#bib.bib133)], and the stroke
    parameters by the actor were used by the neural renderer network to predict paint
    strokes. The network structure of the neural renderer and discriminator consisted
    of multiple convolutions and fully connected blocks.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 使用算法进行绘画是计算机视觉领域一个极具挑战性的任务。[[149](#bib.bib149)]提出了一种自动化绘画方法，作者在此任务中引入了一种基于模型的DRL技术。指定的工作涉及在DRL中使用神经渲染器，代理负责决定每一笔的的位置和颜色，并做出长期决策以将这些笔触组织成一幅视觉杰作。在这项工作中，使用了生成对抗网络（GANs）
    [[113](#bib.bib113)] 来提高图像质量至像素级，并利用DDPG [[224](#bib.bib224)] 确定策略。作者将问题表述为一个MDP，其中状态包括三部分：目标图像
    $I$，执行动作（绘画笔触）的画布 $C_{t}$，以及时间步。对应一组参数的动作控制笔触的位置、形状、颜色和透明度，奖励则由WGAN带梯度惩罚（WGAN-GP）[[117](#bib.bib117)]
    计算，判别器分数在目标图像 $I$ 和画布 $C_{t}$ 之间的变化被用作奖励。预测笔触参数的代理以类似于Resnet18 [[133](#bib.bib133)]
    的骨干网在演员-评论家 [[262](#bib.bib262)] 模式下进行训练，演员生成的笔触参数由神经渲染器网络用于预测绘画笔触。神经渲染器和判别器的网络结构包括多个卷积层和全连接块。
- en: 'A method for guiding medical robots using Ultrasound images with the help of
    DRL was proposed by [[129](#bib.bib129)]. The authors treated the problem as an
    MDP where the agent takes the Ultrasound images as input and estimates the state
    hence the problem became Partially observable MDP (POMDP). They used Double-DQN
    and Duel-DQN for estimating Q-Values and ResNet18 [[133](#bib.bib133)] backbone
    for extracting feature to be used by the algorithm along with Prioritized Replay
    Memory. In their implementation the action space consisted of 8 actions (up, down,
    left, right, and stop), probe position as compared to the sacrum was used as the
    state and the reward was calculated by considering the agent position as compared
    to the target (Move closer: 0.05, Move away: -0.1, Correct stop: 1.0, Incorrect
    stop: -0.25). In their implementation, the authors proposed various architectures
    such as V-DQN, M-DQN, and MS-DQN for the task and performed experimentation on
    Ultrasound images.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 使用超声图像辅以深度强化学习（DRL）来引导医疗机器人这一方法由[[129](#bib.bib129)]提出。作者将问题视为一个马尔可夫决策过程（MDP），其中代理以超声图像作为输入并估计状态，因此问题变为部分可观测马尔可夫决策过程（POMDP）。他们使用了Double-DQN和Duel-DQN来估计Q值，并使用了ResNet18
    [[133](#bib.bib133)]作为特征提取的骨干网，同时配备了优先级回放记忆。在他们的实现中，动作空间包括8个动作（上、下、左、右和停止），探头位置相对于骶骨被用作状态，奖励通过考虑代理位置与目标的相对位置来计算（靠近：0.05，远离：-0.1，正确停止：1.0，错误停止：-0.25）。在他们的实现中，作者提出了V-DQN、M-DQN和MS-DQN等各种架构，并在超声图像上进行了实验。
- en: 'Crowd counting is considered a tricky task in computer vision and is even trickier
    for humans. A DRL method for crowd counting was proposed by [[230](#bib.bib230)],
    where the authors used sequential decision making to approach the task through
    RL. In the specified work, the authors proposed a DQN agent (LibraNet) based on
    the motivation of a weighing scale. In their implementation crowd counting was
    modeled using a weighing scale where the agent was responsible for adding weights
    on one side of the scale sequentially to balance the crowded image on the other
    side. The problem of adding weights on one side of the pan for balancing was formulated
    as an MDP, where state consisted weight vector $W_{t}$ and image feature vector
    $FV_{I}$, and the actions space was defined similar to scale weighing and money
    system [[372](#bib.bib372)] containing values $(-10,-5,-2,-1,+1,+2,+5,+10,end)$.
    For reinforcing the agent two different rewards: ending reward and intermediate
    reward were utilized, where ending reward (following [[43](#bib.bib43)]) was calculated
    by comparing the absolute value error between the ground-truth count and the accumulated
    value with the error tolerance, and three counting specific rewards: force ending
    reward, guiding reward and squeezing reward were calculated for the intermediate
    rewards.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 人群计数在计算机视觉中被认为是一个棘手的任务，对人类来说更是如此。[[230](#bib.bib230)] 提出了一个用于人群计数的深度强化学习（DRL）方法，其中作者通过强化学习（RL）使用序列决策方法来解决这一任务。在指定的工作中，作者提出了一种基于称重秤动机的
    DQN 智能体（LibraNet）。在他们的实现中，人群计数被建模为一个称重秤，其中智能体负责在秤的一侧依次添加重量，以平衡另一侧的拥挤图像。添加重量以平衡秤的一侧的问题被表述为一个马尔可夫决策过程（MDP），其中状态由重量向量
    $W_{t}$ 和图像特征向量 $FV_{I}$ 组成，动作空间的定义类似于称重秤和货币系统 [[372](#bib.bib372)]，包含值 $(-10,-5,-2,-1,+1,+2,+5,+10,end)$。为了增强智能体的表现，利用了两种不同的奖励：终结奖励和中间奖励，其中终结奖励（参见
    [[43](#bib.bib43)]）通过比较真实计数和累积值之间的绝对误差与误差容忍度来计算，同时，为了计算中间奖励，还计算了三个特定的计数奖励：强制终结奖励、指导奖励和挤压奖励。
- en: Exposure bracketing is a method used in digital photography, where one scene
    is captured using multiple exposures for getting a high dynamic range (HDR) image.
    An RL method for automated bracketing selection was proposed by [[389](#bib.bib389)].
    For flexible automated bracketing selection, an exposure bracketing selection
    network (EBSNet) was proposed for selecting optimal exposure bracketing and a
    multi-exposure fusion network (MEFNet) for generating an HDR image from selected
    exposure bracketing which consisted of 3 images. Since there is no ground truth
    for the exposure bracketing selection procedure, an RL scheme was utilized to
    train the agent (EBSNet). The authors also introduced a novel dataset consisting
    of a single auto-exposure image that was used as input to the EBSNet, 10 images
    with varying exposures from which EBSNet generated probability distribution for
    120 possible candidate exposure bracketing ($C^{3}_{10}$) and a reference HDR
    image. The reward for EBSNet was defined as the difference between peak signal-to-noise
    ratio between generated and reference HDR for the current and previous iteration,
    and the MEFNet was trained by minimizing the Charbonnier loss [[23](#bib.bib23)].
    For performing the action of bracketing selection ESBNet consisted of a semantic
    branch using AlexNet [[188](#bib.bib188)] for feature extraction, an illumination
    branch to understand the global and local illuminations by calculating a histogram
    of input and feeding it to CNN layers, and a policy module to generate a probability
    distribution for the candidate exposure bracketing from semantic and illumination
    branches. The neural network for MEFNet was derived from HDRNet [[103](#bib.bib103)].
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 曝光包围是一种数字摄影方法，其中通过多次曝光捕
- en: Autonomous driving in an urban environment is a challenging task, because of
    a large number of environmental variables and constraints. A DRL approach to this
    problem was proposed by [[361](#bib.bib361)]. In their implementation, the authors
    proposed an end-to-end model-free RL method, where they introduced a novel technique
    called Implicit Affordances. For the environment, the CARLA Simulator [[80](#bib.bib80)]
    was utilized, which provided the observations and the training reward was obtained
    by using the CARLA waypoint API. In the novel implicit affordances technique the
    training was broken into two phases, The first phase included using a Resnet18
    [[133](#bib.bib133)] encoder to predict the state of various environment variables
    such as traffic light, pedestrians, position with respect to the center lane,
    etc., and the output features were used as a state for the RL agent, For which
    a modified version of Rainbow-IQN Ape-X [[136](#bib.bib136)] was used. CARLA simulator
    accepts actions in form of continuous steering and throttle values, so to make
    it work with Rainbow-IQN which supports discrete actions, the authors sampled
    steering values into 9 or 27 discrete values and throttle into 4 discrete values
    (including braking), making a total of 36($9\times 4$) or 108($27\times 4$) actions.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在城市环境中进行自动驾驶是一项具有挑战性的任务，因为有大量的环境变量和约束条件。[[361](#bib.bib361)] 提出了一个解决该问题的DRL（深度强化学习）方法。在他们的实现中，作者提出了一种端到端的无模型RL方法，其中引入了一种称为隐式可用性的创新技术。环境使用了CARLA模拟器[[80](#bib.bib80)]，该模拟器提供了观测信息，训练奖励通过使用CARLA
    waypoint API获得。在创新的隐式可用性技术中，训练被分为两个阶段。第一阶段包括使用Resnet18[[133](#bib.bib133)]编码器预测各种环境变量的状态，如交通信号灯、行人、相对于中心车道的位置等，输出特征用作RL代理的状态，为此使用了Rainbow-IQN
    Ape-X的修改版[[136](#bib.bib136)]。CARLA模拟器接受连续的转向和油门值，因此为了使其与支持离散动作的Rainbow-IQN兼容，作者将转向值采样为9或27个离散值，将油门值采样为4个离散值（包括制动），总共形成了36（$9\times
    4$）或108（$27\times 4$）个动作。
- en: 'Racial discrimination has been one of the hottest topics of the 21st century.
    To mitigate racial discrimination in facial recognition, [[382](#bib.bib382)]
    proposed a facial recognition method using skewness-aware RL. According to the
    authors, the reason for racial bias in facial recognition algorithms can be either
    due to the data or due to the algorithm, so the authors provided two ethnicity-aware
    datasets, BUPT-Globalface and BUPT-Balancedface along with an RL based race balanced
    network (RL-RBN). In their implementation, the authors formulated an MDP for adaptive
    margin policy learning where the state consisted of three parts: the race group
    (0: Indian, 1: Asian, 2: African), current adaptive margin, and bias or the skewness
    between the race group and Caucasians. A DQN was used as a policy network that
    performed three actions (staying the same, shifting to a larger value, and shifting
    to a smaller value) to change the adaptive margin, and accepted reward in form
    of change in the sum of inter-class and intra-class bias.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '种族歧视一直是21世纪最热门的话题之一。为了减少面部识别中的种族歧视，[[382](#bib.bib382)] 提出了一种使用偏斜感知强化学习（RL）的面部识别方法。根据作者的说法，面部识别算法中的种族偏见可能由于数据或算法本身造成，因此作者提供了两个种族感知的数据集，BUPT-Globalface
    和 BUPT-Balancedface，以及一个基于RL的种族平衡网络（RL-RBN）。在他们的实现中，作者制定了一个MDP（马尔可夫决策过程）用于自适应边际策略学习，其中状态包括三个部分：种族组（0:
    印度人，1: 亚洲人，2: 非洲人）、当前自适应边际，以及种族组与高加索人之间的偏见或偏斜。使用了DQN作为策略网络，它执行三种动作（保持不变、调整为更大值和调整为更小值）以改变自适应边际，并接受以类间和类内偏见变化形式的奖励。'
- en: Attention mechanisms are currently gaining popularity because of their powerful
    ability in eliminating uninformative parts of the input to leverage the other
    parts having a more useful information. Recently, attention mechanism has been
    integrated into typical CNN models at every individual layer to strengthen the
    intermediate outputs of each layer, in turn improving the final predictions for
    recognition in images. This model is usually trained with a weakly supervised
    method, however, this optimization method may lead to sub-optimal weights in the
    attention module. Hence, [[212](#bib.bib212)] proposed to train attention module
    by deep Q-learning with an LSTM model is trained to predict the reward, the whole
    process is called Deep REinforced Attention Learning (DREAL).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制目前越来越受到关注，因为它能够强大地消除输入中不重要的部分，以利用其他更有用的信息部分。最近，注意力机制已经被集成到每个典型 CNN 模型的每一层中，以加强每层的中间输出，从而提高最终的图像识别预测。该模型通常采用弱监督方法进行训练，但这种优化方法可能导致注意力模块的子优化权重。因此，[[212](#bib.bib212)]
    提出了通过深度 Q 学习训练注意力模块，并利用 LSTM 模型预测奖励，整个过程称为深度强化注意力学习（DREAL）。
- en: 'Various works specified here have been summarised and compared in Table LABEL:tab:oth
    and general implementation of a DRL method to control an agents movement in an
    environment has been shown in fig [17](#S11.F17 "Figure 17 ‣ 11 Others Applications
    ‣ Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey") where
    state consists of an image frame provided by the environment, the DRL agent predicts
    actions to move the agent in the environment providing next state and the reward
    is provided by the environment, for example, [[142](#bib.bib142)].'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 此处指定的各种工作已在表 LABEL:tab:oth 中总结和比较，DRL 方法在环境中控制智能体运动的一般实现已显示在图 [17](#S11.F17
    "图 17 ‣ 11 其他应用 ‣ 深度强化学习在计算机视觉中的应用：综合调查") 中，其中状态由环境提供的图像帧组成，DRL 智能体预测动作以在环境中移动智能体，提供下一个状态，奖励由环境提供，例如
    [[142](#bib.bib142)]。
- en: 12 Future Perspectives
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12 未来展望
- en: 12.1 Challenge Discussion
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 挑战讨论
- en: DRL is a powerful framework, which has been successfully applied to various
    computer vision applications including landmark detection, object detection, object
    tracking, image registration, image segmentation, video analysis, and other computer
    vision applications. DRL has also demonstrated to be an effective alternative
    for solving difficult optimization problems, including tuning parameters, selecting
    augmentation strategies, and neural architecture search (NAS). However, most approaches,
    that we have reviewed, assume a stationary environment, from which observations
    are made. Take landmark detection as an instance, the environment takes into account
    the image itself, and each state is defined as an image patch consisting of the
    landmark location. In such a case, the environment is known while the RL/DRL framework
    naturally accommodates a dynamic environment, that is the environment itself evolves
    with the state and action. Realizing the full potential of DRL for computer vision
    requires solving several challenges. In this section, we would like to discuss
    the challenges of DRL in computer vision for real-world systems.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 是一个强大的框架，已成功应用于各种计算机视觉应用，包括地标检测、物体检测、物体跟踪、图像配准、图像分割、视频分析和其他计算机视觉应用。DRL 也被证明是解决困难优化问题的有效替代方案，包括调整参数、选择数据增强策略和神经网络结构搜索（NAS）。然而，我们审查的大多数方法假设一个静态环境，从中获取观察。例如，地标检测考虑到图像本身，每个状态被定义为包含地标位置的图像块。在这种情况下，环境是已知的，而
    RL/DRL 框架自然适应动态环境，即环境本身随着状态和动作的变化而演变。实现 DRL 在计算机视觉中的全部潜力需要解决几个挑战。在本节中，我们将讨论 DRL
    在实际系统中的计算机视觉挑战。
- en: •
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reward function: In most real-world applications, it is hard to define a specified
    reward function because it requires the knowledge from different domains that
    may not always be available. Thus, the intermediate rewards at each time step
    are not always easily computed. Furthermore, a reward function with too long delay
    will make training difficult. In contrast, assigning a reward for each action
    requires careful and manual human design.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励函数：在大多数实际应用中，很难定义一个指定的奖励函数，因为这需要来自不同领域的知识，而这些知识可能并不总是可用的。因此，每个时间步的中间奖励并不总是容易计算。此外，奖励函数的延迟过长会使训练变得困难。相反，为每个动作分配奖励需要仔细和手动的人为设计。
- en: •
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Continuous state and action space: Training an RL system on a continuous state
    and action space is challenging because most RL algorithms, i.e. Q learning, can
    only deal with discrete states and discrete action space. To address this limitation,
    most existing works discretize the continuous state and action space.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连续状态和动作空间：在连续状态和动作空间上训练RL系统具有挑战性，因为大多数RL算法，如Q学习，只能处理离散状态和离散动作空间。为了解决这一限制，大多数现有工作将连续状态和动作空间离散化。
- en: •
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'High-dimensional state and action space: Training Q-function on a high-dimensional
    action space is challenging. For this reason, existing works use low-dimensional
    parameterization, whose dimensions are typically less than 10 with an exception
    [[184](#bib.bib184)] that uses 15-D and 25-D to model 2D and 3D registration,
    respectively.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高维状态和动作空间：在高维动作空间上训练Q函数具有挑战性。因此，现有的工作使用低维参数化，其维度通常少于10，但有一个例外[[184](#bib.bib184)]使用15-D和25-D来分别建模2D和3D配准。
- en: •
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Environment is complicated: Almost all real-world systems, where we would want
    to deploy DRL/RL, are partially observable and non-stationary. Currently, the
    approaches we have reviewed assume a stationary environment, from which observations
    are made. However, the DRL/RL framework naturally accommodates dynamic environment,
    that is the environment itself evolves with the state and action. Furthermore,
    those systems are often stochastic and noisy (action delay, sensor and action
    noise) as compared to most simulated environments.'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 环境复杂：几乎所有我们希望部署DRL/RL的实际系统都是部分可观测和非平稳的。目前，我们回顾的方法假设了一个平稳的环境，从中获取观察数据。然而，DRL/RL框架自然适应动态环境，即环境本身随着状态和动作而演变。此外，这些系统通常比大多数模拟环境更具随机性和噪声（动作延迟、传感器和动作噪声）。
- en: •
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training data requirement: RL/DRL requires a large amount of training data
    or expert demonstrations. Large-scale datasets with annotations are expensive
    and hard to come by.'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练数据需求：RL/DRL需要大量的训练数据或专家演示。带有注释的大规模数据集既昂贵又难以获得。
- en: More details of challenges that embody difficulties to deploy RL/DRL in the
    real world are discussed in [[82](#bib.bib82)]. In this work, they designed a
    set of experiments and analyzed their effects on common RL agents. Open-sourcing
    an environmental suite, [realworldrl-suite](https://github.com/google-research/realworldrl_suite)
    [[83](#bib.bib83)] is provided in this work as well.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[82](#bib.bib82)]中讨论了实现RL/DRL在实际应用中的困难挑战的更多细节。在这项工作中，他们设计了一组实验并分析了其对常见RL代理的影响。此工作还提供了一个开源环境套件，[realworldrl-suite](https://github.com/google-research/realworldrl_suite)
    [[83](#bib.bib83)]。
- en: 12.2 DRL Recent Advances
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 DRL 最近的进展
- en: Some advanced DRL approaches such as Inverse DRL, Multi-agent DRL, Meta DRL,
    and imitation learning are worth the attention and may promote new insights for
    many machine learning and computer vision tasks.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 一些高级DRL方法，如逆向DRL、多智能体DRL、元DRL和模仿学习，值得关注，并可能为许多机器学习和计算机视觉任务提供新的见解。
- en: •
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inverse DRL: DRL has been successfully applied into domains where the reward
    function is clearly defined. However, this is limited in real-world applications
    because it requires knowledge from different domains that may not always be available.
    Inverse DRL is one of the special cases of imitation learning. An example is autonomous
    driving, the reward function should be based on all factors such as driver’s behavior,
    gas consumption, time, speed, safety, driving quality, etc. In real-world scenario,
    it is exhausting and hard to control all these factors. Different from DRL, inverse
    DRL [[278](#bib.bib278)], [[4](#bib.bib4)], [[413](#bib.bib413)], [[86](#bib.bib86)]
    a specific form of imitation learning [[286](#bib.bib286)], infers the reward
    function of an agent, given its policy or observed behavior, thereby avoiding
    a manual specification of its reward function. In the same problem of autonomous
    driving, inverse RL first uses a dataset collected from the human-generated driving
    and then approximates the reward function. Inverse RL has been successfully applied
    to many domains [[4](#bib.bib4)]. Recently, to analyze complex human movement
    and control high-dimensional robot systems, [[215](#bib.bib215)] proposed an online
    inverse RL algorithm. [[2](#bib.bib2)] combined both RL and Inverse RL to address
    planning problems in autonomous driving.'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逆向 DRL：DRL 已成功应用于奖励函数明确的领域。然而，这在实际应用中受到限制，因为这需要来自不同领域的知识，这些知识可能并不总是可用。逆向 DRL
    是模仿学习的一个特殊案例。一个例子是自动驾驶，奖励函数应该基于所有因素，如驾驶员行为、油耗、时间、速度、安全性、驾驶质量等。在实际场景中，控制所有这些因素是耗时且困难的。与
    DRL 不同，逆向 DRL[[278](#bib.bib278)], [[4](#bib.bib4)], [[413](#bib.bib413)], [[86](#bib.bib86)]
    是模仿学习[[286](#bib.bib286)]的一种特定形式，它通过给定智能体的策略或观察到的行为来推断奖励函数，从而避免了对奖励函数的手动指定。在自动驾驶的同一问题中，逆向
    RL 首先使用从人类生成的驾驶数据集中收集的数据，然后近似奖励函数。逆向 RL 已成功应用于许多领域[[4](#bib.bib4)]。最近，为了分析复杂的人体运动和控制高维机器人系统，[[215](#bib.bib215)]
    提出了一个在线逆向 RL 算法。[[2](#bib.bib2)] 将 RL 和逆向 RL 结合起来，以解决自动驾驶中的规划问题。
- en: •
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-Agent DRL: Most of the successful DRL applications such as game[[38](#bib.bib38)],
    [[376](#bib.bib376)], robotics[[181](#bib.bib181)], and autonomous driving [[335](#bib.bib335)],
    stock trading [[206](#bib.bib206)], social science [[207](#bib.bib207)], etc.,
    involve multiple players that requires a model with multi-agent. Take autonomous
    driving as an instance, multi-agent DRL addresses the sequential decision-making
    problem which involves many autonomous agents, each of which aims to optimize
    its own utility return by interacting with the environment and other agents [[40](#bib.bib40)].
    Learning in a multi-agent scenario is more difficult than a single-agent scenario
    because non-stationarity [[135](#bib.bib135)], multi-dimensionality [[40](#bib.bib40)],
    credit assignment [[5](#bib.bib5)], etc., depend on the multi-agent DRL approach
    of either fully cooperative or fully competitive. The agents can either collaborate
    to optimize a long-term utility or compete so that the utility is summed to zero.
    Recent work on Multi-Agent RL pays attention to learning new criteria or new setup
    [[348](#bib.bib348)].'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多智能体 DRL：大多数成功的 DRL 应用如游戏[[38](#bib.bib38)], [[376](#bib.bib376)], 机器人[[181](#bib.bib181)],
    和自动驾驶[[335](#bib.bib335)], 股票交易[[206](#bib.bib206)], 社会科学[[207](#bib.bib207)]等，涉及多个玩家，这需要一个多智能体模型。以自动驾驶为例，多智能体
    DRL 解决了一个序列决策问题，该问题涉及许多自主智能体，每个智能体都旨在通过与环境和其他智能体互动来优化自身的效用回报[[40](#bib.bib40)]。在多智能体场景中学习比在单智能体场景中更困难，因为非平稳性[[135](#bib.bib135)],
    多维性[[40](#bib.bib40)], 归因问题[[5](#bib.bib5)]等，取决于多智能体 DRL 方法是完全合作还是完全竞争。智能体可以合作以优化长期效用，或竞争以使效用总和为零。最近的多智能体
    RL 研究关注于学习新标准或新设置[[348](#bib.bib348)]。
- en: •
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Meta DRL: As aforementioned, DRL algorithms consume large amounts of experience
    in order to learn an individual task and are unable to generalize the learned
    policy to newer problems. To alleviate the data challenge, Meta-RL algorithms
    [[330](#bib.bib330)], [[380](#bib.bib380)] are studied to enable agents to learn
    new skills from small amounts of experience. Recently, there is a research interest
    in meta RL [[271](#bib.bib271)], [[119](#bib.bib119)], [[322](#bib.bib322)], [[303](#bib.bib303)],
    [[229](#bib.bib229)], each using a different approach. For benchmarking and evaluation
    of meta RL algorithms, [[415](#bib.bib415)] presented Meta-world, which is an
    open-source simulator consisting of 50 distinct robotic manipulation tasks.'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 元DRL：如前所述，DRL算法消耗大量经验来学习个别任务，并且无法将学习到的策略推广到新的问题。为了解决数据挑战，Meta-RL算法[[330](#bib.bib330)]、[[380](#bib.bib380)]的研究使得智能体能够从少量经验中学习新技能。最近，meta
    RL [[271](#bib.bib271)]、[[119](#bib.bib119)]、[[322](#bib.bib322)]、[[303](#bib.bib303)]、[[229](#bib.bib229)]的研究兴趣逐渐增加，每种方法使用不同的方法。为了对meta
    RL算法进行基准测试和评估，[[415](#bib.bib415)]提出了Meta-world，这是一种包含50个不同机器人操作任务的开源模拟器。
- en: •
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Imitation Learning: Imitation learning is close to learning from demonstrations
    which aims at training a policy to mimic an expert’s behavior given the samples
    collected from that expert. Imitation learning is also considered as an alternative
    to RL/DRL to solve sequential decision-making problems. Besides inverse DRL, an
    imitation learning approach as aforementioned, behavior cloning is another imitation
    learning approach to train policy under supervised learning manner. Bradly et
    al. [[347](#bib.bib347)] presented a method for unsupervised third-person imitation
    learning to observe how other humans perform and infer the task. Building on top
    of Deep Deterministic Policy Gradients and Hindsight Experience Replay, Nair et
    al. [[272](#bib.bib272)] proposed behavior cloning Loss to increase imitating
    the demonstrations. Besides Q-learning, Generative Adversarial Imitation Learning
    [[364](#bib.bib364)] proposes P-GAIL that integrates imitation learning into the
    policy gradient framework. P-GAIL considers both smoothness and causal entropy
    in policy update by utilizing Deep P-Network [[365](#bib.bib365)].'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模仿学习：模仿学习接近于从示范中学习，旨在训练一个策略来模仿专家的行为，前提是从专家那里收集样本。模仿学习也被视为解决序列决策问题的RL/DRL替代方案。除了逆向DRL，前述的模仿学习方法，行为克隆是另一种在监督学习方式下训练策略的模仿学习方法。Bradly等人[[347](#bib.bib347)]提出了一种无监督的第三方模仿学习方法，以观察其他人如何执行任务并推断任务内容。在Deep
    Deterministic Policy Gradients和Hindsight Experience Replay的基础上，Nair等人[[272](#bib.bib272)]提出了行为克隆损失，以增加对示范的模仿。除了Q-learning，生成对抗模仿学习[[364](#bib.bib364)]提出了P-GAIL，将模仿学习整合到策略梯度框架中。P-GAIL通过利用Deep
    P-Network [[365](#bib.bib365)]，在策略更新中考虑了平滑性和因果熵。
- en: Conclusion
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Deep Reinforcement Learning (DRL) is nowadays the most popular technique for
    an artificial agent to learn closely optimal strategies by experiences. This paper
    aims to provide a state-of-the-art comprehensive survey of DRL applications to
    a variety of decision-making problems in the area of computer vision. In this
    work, we firstly provided a structured summarization of the theoretical foundations
    in Deep Learning (DL) including AutoEncoder (AE), Multi-Layer Perceptron (MLP),
    Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN). We then
    continued to introduce key techniques in RL research including model-based methods
    (value functions, transaction models, policy search, return functions) and model-free
    methods (value-based, policy-based, and actor-critic). Main techniques in DRL
    were thirdly presented under two categories of model-based and model-free approaches.
    We fourthly surveyed the broad-ranging applications of DRL methods in solving
    problems affecting areas of computer vision, from landmark detection, object detection,
    object tracking, image registration, image segmentation, video analysis, and many
    other applications in the computer vision area. We finally discussed several challenges
    ahead of us in order to realize the full potential of DRL for computer vision.
    Some latest advanced DRL techniques were included in the last discussion.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）如今是人工智能体通过经验学习接近最优策略的最流行技术。本文旨在提供一个前沿的深度强化学习在计算机视觉领域各种决策问题中的应用的综合调查。在这项工作中，我们首先对深度学习（DL）的理论基础进行了结构化总结，包括自动编码器（AE）、多层感知机（MLP）、卷积神经网络（CNN）和递归神经网络（RNN）。然后，我们继续介绍了强化学习研究中的关键技术，包括基于模型的方法（价值函数、转换模型、策略搜索、回报函数）和无模型的方法（基于价值的、基于策略的、演员-评论员）。接下来，深度强化学习的主要技术在基于模型和无模型的方法两个类别下进行了介绍。我们进一步调查了深度强化学习方法在解决计算机视觉领域各种问题中的广泛应用，包括地标检测、目标检测、目标跟踪、图像配准、图像分割、视频分析以及计算机视觉领域的其他许多应用。最后，我们讨论了实现深度强化学习在计算机视觉领域的全部潜力所面临的若干挑战。最后的讨论中包括了一些最新的先进深度强化学习技术。
- en: References
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Model-based contextual policy search for data-efficient generalization
    of robot skills. Artificial Intelligence, 247:415 – 439, 2017.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 基于模型的上下文策略搜索用于机器人技能的数据高效泛化。《人工智能》，247:415 – 439，2017年。'
- en: '[2] Advanced planning for autonomous vehicles using reinforcement learning
    and deep inverse reinforcement learning. Robotics and Autonomous Systems, 114:1
    – 18, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 使用强化学习和深度逆向强化学习进行自主车辆的高级规划。《机器人与自主系统》，114:1 – 18，2019年。'
- en: '[3] Pieter Abbeel, Adam Coates, and Andrew Y. Ng. Autonomous helicopter aerobatics
    through apprenticeship learning. The International Journal of Robotics Research,
    29(13):1608–1639, 2010.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Pieter Abbeel、Adam Coates 和 Andrew Y. Ng. 通过学徒学习实现自主直升机特技飞行。《国际机器人研究杂志》，29(13):1608–1639，2010年。'
- en: '[4] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement
    learning. In Proceedings of the Twenty-First International Conference on Machine
    Learning, pages 1–8\. Association for Computing Machinery, 2004.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Pieter Abbeel 和 Andrew Y. Ng. 通过逆向强化学习进行学徒学习。载于《第二十一届国际机器学习会议论文集》，第1–8页。计算机协会，2004年。'
- en: '[5] Adrian K. Agogino and Kagan Tumer. Unifying temporal and structural credit
    assignment problems. In Proceedings of the Third International Joint Conference
    on Autonomous Agents and Multiagent Systems - Volume 2, page 980–987\. IEEE Computer
    Society, 2004.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Adrian K. Agogino 和 Kagan Tumer. 统一的时间和结构信用分配问题。载于《第三届国际自主代理和多代理系统联合会议论文集
    - 第二卷》，第980–987页。IEEE计算机学会，2004年。'
- en: '[6] Narges Ahmidi, Lingling Tao, Shahin Sefati, Yixin Gao, Colin Lea, Benjamin Bejar
    Haro, Luca Zappella, Sanjeev Khudanpur, René Vidal, and Gregory D Hager. A dataset
    and benchmarks for segmentation and recognition of gestures in robotic surgery.
    IEEE Transactions on Biomedical Engineering, 64(9):2025–2041, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Narges Ahmidi、Lingling Tao、Shahin Sefati、Yixin Gao、Colin Lea、Benjamin Bejar
    Haro、Luca Zappella、Sanjeev Khudanpur、René Vidal 和 Gregory D Hager. 用于机器人手术中手势分割和识别的数据集和基准。IEEE
    生物医学工程学报，64(9):2025–2041，2017年。'
- en: '[7] Walid Abdullah Al and Il Dong Yun. Partial policy-based reinforcement learning
    for anatomical landmark localization in 3d medical images. IEEE transactions on
    medical imaging, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Walid Abdullah Al 和 Il Dong Yun. 基于部分策略的强化学习在三维医学图像中的解剖标志物定位。IEEE 医学成像学报，2019年。'
- en: '[8] Walid Abdullah Al, Il Dong Yun, and Kyong Joon Lee. Reinforcement learning-based
    automatic diagnosis of acute appendicitis in abdominal ct. arXiv preprint arXiv:1909.00617,
    2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Walid Abdullah Al、Il Dong Yun 和 Kyong Joon Lee。基于强化学习的腹部CT急性阑尾炎自动诊断。arXiv
    预印本 arXiv:1909.00617，2019年。'
- en: '[9] Stephan Alaniz. Deep reinforcement learning with model learning and monte
    carlo tree search in minecraft. In Conference on Reinforcement Learning and Decision
    Making, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Stephan Alaniz。在 Minecraft 中使用模型学习和蒙特卡洛树搜索的深度强化学习。发表于强化学习与决策会议，2018年。'
- en: '[10] Amir Alansary, Ozan Oktay, Yuanwei Li, Loic Le Folgoc, Benjamin Hou, Ghislain
    Vaillant, Konstantinos Kamnitsas, Athanasios Vlontzos, Ben Glocker, Bernhard Kainz,
    et al. Evaluating reinforcement learning agents for anatomical landmark detection.
    Medical image analysis, 53:156–164, 2019.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Amir Alansary、Ozan Oktay、Yuanwei Li、Loic Le Folgoc、Benjamin Hou、Ghislain
    Vaillant、Konstantinos Kamnitsas、Athanasios Vlontzos、Ben Glocker、Bernhard Kainz
    等。评估用于解剖标志检测的强化学习代理。医学图像分析，53:156–164，2019年。'
- en: '[11] Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection
    using reconstruction probability. Special Lecture on IE, 2(1):1–18, 2015.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jinwon An 和 Sungzoon Cho。基于变分自编码器的异常检测，使用重建概率。IE 特别讲座，2(1):1–18，2015年。'
- en: '[12] O. Andersson, F. Heintz, and P. Doherty. Model-based reinforcement learning
    in continuous environments using real-time constrained optimization. In AAAI,
    2015.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] O. Andersson、F. Heintz 和 P. Doherty。在连续环境中使用实时约束优化的基于模型的强化学习。发表于 AAAI，2015年。'
- en: '[13] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony
    Bharath. A brief survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866,
    2017.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Kai Arulkumaran、Marc Peter Deisenroth、Miles Brundage 和 Anil Anthony Bharath。深度强化学习的简要综述。arXiv
    预印本 arXiv:1708.05866，2017年。'
- en: '[14] S Avinash Ramakanth and R Venkatesh Babu. Seamseg: Video object segmentation
    using patch seams. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 376–383, 2014.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] S Avinash Ramakanth 和 R Venkatesh Babu。Seamseg：使用补丁接缝的视频对象分割。发表于 IEEE
    计算机视觉与模式识别会议，页码 376–383，2014年。'
- en: '[15] Morgane Ayle, Jimmy Tekli, Julia El-Zini, Boulos El-Asmar, and Mariette
    Awad. Bar-a reinforcement learning agent for bounding-box automated refinement.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Morgane Ayle、Jimmy Tekli、Julia El-Zini、Boulos El-Asmar 和 Mariette Awad。Bar-a
    一种用于边界框自动优化的强化学习代理。'
- en: '[16] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan
    Kautz. GA3C: gpu-based A3C for deep reinforcement learning. CoRR, abs/1611.06256,
    2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Mohammad Babaeizadeh、Iuri Frosio、Stephen Tyree、Jason Clemons 和 Jan Kautz。GA3C：基于
    GPU 的 A3C 深度强化学习。CoRR，abs/1611.06256，2016年。'
- en: '[17] Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. Visual tracking with
    online multiple instance learning. In 2009 IEEE conference on computer vision
    and pattern recognition, pages 983–990\. IEEE, 2009.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Boris Babenko、Ming-Hsuan Yang 和 Serge Belongie。基于在线多实例学习的视觉跟踪。发表于 2009
    年 IEEE 计算机视觉与模式识别会议，页码 983–990。IEEE，2009年。'
- en: '[18] Seung-Hwan Bae and Kuk-Jin Yoon. Robust online multi-object tracking based
    on tracklet confidence and online discriminative appearance learning. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 1218–1225,
    2014.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Seung-Hwan Bae 和 Kuk-Jin Yoon。基于跟踪片段置信度和在线区分外观学习的鲁棒在线多目标跟踪。发表于 IEEE 计算机视觉与模式识别会议，页码
    1218–1225，2014年。'
- en: '[19] Seung-Hwan Bae and Kuk-Jin Yoon. Confidence-based data association and
    discriminative deep appearance learning for robust online multi-object tracking.
    IEEE transactions on pattern analysis and machine intelligence, 40(3):595–610,
    2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Seung-Hwan Bae 和 Kuk-Jin Yoon。基于置信度的数据关联和用于鲁棒在线多目标跟踪的区分深度外观学习。IEEE 模式分析与机器智能汇刊，40(3):595–610，2017年。'
- en: '[20] J. Bagnell. Learning decision: Robustness, uncertainty, and approximation.
    04 2012.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. Bagnell。学习决策：鲁棒性、不确定性和近似。2012年4月。'
- en: '[21] J. A. Bagnell and J. G. Schneider. Autonomous helicopter control using
    reinforcement learning policy search methods. In Proceedings 2001 ICRA. IEEE International
    Conference on Robotics and Automation (Cat. No.01CH37164), volume 2, pages 1615–1620,
    2001.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. A. Bagnell 和 J. G. Schneider。使用强化学习策略搜索方法的自主直升机控制。发表于 2001 年 ICRA。IEEE
    国际机器人与自动化会议（Cat. No.01CH37164），第 2 卷，页码 1615–1620，2001年。'
- en: '[22] Neil Barakat, A Nicholas Hone, and Thomas E Darcie. Minimal-bracketing
    sets for high-dynamic-range image capture. IEEE Transactions on Image Processing,
    17(10):1864–1875, 2008.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Neil Barakat、A Nicholas Hone 和 Thomas E Darcie。用于高动态范围图像捕获的最小括号集合。IEEE
    图像处理汇刊，17(10):1864–1875，2008年。'
- en: '[23] Jonathan T Barron. A general and adaptive robust loss function. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4331–4339,
    2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jonathan T Barron. 一种通用和自适应的鲁棒损失函数. 见《IEEE计算机视觉与模式识别会议论文集》，第4331–4339页,
    2019.'
- en: '[24] Cher Bass, Pyry Helkkula, Vincenzo De Paola, Claudia Clopath, and Anil Anthony
    Bharath. Detection of axonal synapses in 3d two-photon images. PloS one, 12(9):e0183309,
    2017.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Cher Bass、Pyry Helkkula、Vincenzo De Paola、Claudia Clopath 和 Anil Anthony
    Bharath. 在 3d 双光子图像中检测轴突突触. 《PloS one》，12(9):e0183309, 2017.'
- en: '[25] Miriam Bellver, Xavier Giró-i Nieto, Ferran Marqués, and Jordi Torres.
    Hierarchical object detection with deep reinforcement learning. arXiv preprint
    arXiv:1611.03718, 2016.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Miriam Bellver、Xavier Giró-i Nieto、Ferran Marqués 和 Jordi Torres. 基于深度强化学习的层次化物体检测.
    arXiv 预印本 arXiv:1611.03718, 2016.'
- en: '[26] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term
    dependencies with gradient descent is difficult. IEEE Trans. Neural Networks,
    5(2):157–166, 1994.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Yoshua Bengio、Patrice Simard 和 Paolo Frasconi. 使用梯度下降学习长期依赖关系是困难的. 《IEEE神经网络汇刊》，5(2):157–166,
    1994.'
- en: '[27] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad — a comprehensive
    real-world dataset for unsupervised anomaly detection. In 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 9584–9592, 2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] P. Bergmann、M. Fauser、D. Sattlegger 和 C. Steger. MVTEC AD — 用于无监督异常检测的综合真实世界数据集.
    见《2019年IEEE/CVF计算机视觉与模式识别会议（CVPR）》，第9584–9592页, 2019.'
- en: '[28] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking
    performance: the clear mot metrics. EURASIP Journal on Image and Video Processing,
    2008:1–10, 2008.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Keni Bernardin 和 Rainer Stiefelhagen. 评估多目标跟踪性能：CLEAR MOT 指标. 《EURASIP
    图像与视频处理期刊》，2008:1–10, 2008.'
- en: '[29] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and
    Philip HS Torr. Fully-convolutional siamese networks for object tracking. In European
    conference on computer vision, pages 850–865. Springer, 2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Luca Bertinetto、Jack Valmadre、Joao F Henriques、Andrea Vedaldi 和 Philip
    HS Torr. 用于物体跟踪的全卷积 Siamese 网络. 见《欧洲计算机视觉会议》，第850–865页. Springer, 2016.'
- en: '[30] Shalabh Bhatnagar. An actor–critic algorithm with function approximation
    for discounted cost constrained markov decision processes. Systems & Control Letters,
    59(12):760–766, 2010.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Shalabh Bhatnagar. 一种用于折扣成本约束马尔可夫决策过程的演员–评论员算法与函数近似. 《系统与控制快报》，59(12):760–766,
    2010.'
- en: '[31] Shalabh Bhatnagar, Richard S. Sutton, Mohammad Ghavamzadeh, and Mark Lee.
    Natural actorâ-critic algorithms. Automatica, 45(11):2471 – 2482, 2009.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Shalabh Bhatnagar、Richard S. Sutton、Mohammad Ghavamzadeh 和 Mark Lee. 自然演员–评论员算法.
    《自动化》，45(11):2471 – 2482, 2009.'
- en: '[32] Michael J Black and Yaser Yacoob. Tracking and recognizing rigid and non-rigid
    facial motions using local parametric models of image motion. In Proceedings of
    IEEE international conference on computer vision, pages 374–381\. IEEE, 1995.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Michael J Black 和 Yaser Yacoob. 使用局部参数模型跟踪和识别刚性和非刚性面部运动. 见《IEEE国际计算机视觉会议论文集》，第374–381页\.
    IEEE, 1995.'
- en: '[33] N Bloch, A Madabhushi, H Huisman, J Freymann, J Kirby, M Grauer, A Enquobahrie,
    C Jaffe, L Clarke, and K Farahani. Nci-isbi 2013 challenge: automated segmentation
    of prostate structures. The Cancer Imaging Archive, 370, 2015.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] N Bloch、A Madabhushi、H Huisman、J Freymann、J Kirby、M Grauer、A Enquobahrie、C
    Jaffe、L Clarke 和 K Farahani. Nci-isbi 2013 挑战：前列腺结构的自动分割. 《癌症影像档案》，370, 2015.'
- en: '[34] J. Boedecker, J. T. Springenberg, J. Wülfing, and M. Riedmiller. Approximate
    real-time optimal control based on sparse gaussian process models. In 2014 IEEE
    Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),
    pages 1–8, 2014.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Boedecker、J. T. Springenberg、J. Wülfing 和 M. Riedmiller. 基于稀疏高斯过程模型的近似实时最优控制.
    见《2014年IEEE自适应动态规划与强化学习研讨会（ADPRL）》，第1–8页, 2014.'
- en: '[35] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d region proposal
    network for object detection. In Proceedings of the IEEE International Conference
    on Computer Vision, Seoul, South Korea, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Garrick Brazil 和 Xiaoming Liu. M3d-rpn: 单目 3d 区域提议网络用于物体检测. 见《IEEE国际计算机视觉会议论文集》，首尔，韩国，2019年.'
- en: '[36] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative interaction
    training for segmentation editing networks. In International Workshop on Machine
    Learning in Medical Imaging, pages 363–370\. Springer, 2018.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Gustav Bredell、Christine Tanner 和 Ender Konukoglu. 用于分割编辑网络的迭代交互训练. 见《国际医学影像机器学习研讨会》,
    第363–370页\. Springer, 2018.'
- en: '[37] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Leo Breiman. Bagging 预测器. 《机器学习》，24(2):123–140, 1996.'
- en: '[38] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science,
    365(6456):885–890, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Noam Brown 和 Tuomas Sandholm. 超人级人工智能用于多人扑克游戏。科学, 365(6456):885–890, 2019。'
- en: '[39] Antoine Buetti-Dinh, Vanni Galli, SÃ¶ren Bellenberg, Olga Ilie, Malte
    Herold, Stephan Christel, Mariia Boretska, Igor V. Pivkin, Paul Wilmes, Wolfgang
    Sand, Mario Vera, and Mark Dopson. Deep neural networks outperform human expert’s
    capacity in characterizing bioleaching bacterial biofilm composition. Biotechnology
    Reports, 22:e00321, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Antoine Buetti-Dinh, Vanni Galli, Sören Bellenberg, Olga Ilie, Malte Herold,
    Stephan Christel, Mariia Boretska, Igor V. Pivkin, Paul Wilmes, Wolfgang Sand,
    Mario Vera, 和 Mark Dopson. 深度神经网络在表征生物浸出细菌生物膜成分方面超越了人类专家的能力。生物技术报告, 22:e00321,
    2019。'
- en: '[40] L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of
    multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics,
    Part C (Applications and Reviews), 38(2):156–172, 2008.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] L. Busoniu, R. Babuska, 和 B. De Schutter. 多智能体强化学习的全面调查。IEEE系统、人和控制论汇刊C部分（应用与评论）,
    38(2):156–172, 2008。'
- en: '[41] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixé,
    Daniel Cremers, and Luc Van Gool. One-shot video object segmentation. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 221–230,
    2017.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixé,
    Daniel Cremers, 和 Luc Van Gool. 一次性视频物体分割。发表于IEEE计算机视觉与模式识别会议论文集, 页码 221–230,
    2017。'
- en: '[42] Yunliang Cai, Said Osman, Manas Sharma, Mark Landis, and Shuo Li. Multi-modality
    vertebra recognition in arbitrary views using 3d deformable hierarchical model.
    IEEE transactions on medical imaging, 34(8):1676–1693, 2015.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Yunliang Cai, Said Osman, Manas Sharma, Mark Landis, 和 Shuo Li. 使用3D变形层次模型在任意视角下的多模态椎骨识别。IEEE医学成像汇刊,
    34(8):1676–1693, 2015。'
- en: '[43] Juan C Caicedo and Svetlana Lazebnik. Active object localization with
    deep reinforcement learning. In Proceedings of the IEEE international conference
    on computer vision, pages 2488–2496, 2015.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Juan C Caicedo 和 Svetlana Lazebnik. 基于深度强化学习的主动物体定位。发表于IEEE国际计算机视觉会议论文集,
    页码 2488–2496, 2015。'
- en: '[44] D. Carrera, F. Manganini, G. Boracchi, and E. Lanzarone. Defect detection
    in sem images of nanofibrous materials. IEEE Transactions on Industrial Informatics,
    13(2):551–561, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] D. Carrera, F. Manganini, G. Boracchi, 和 E. Lanzarone. 纳米纤维材料的半成像缺陷检测。IEEE工业信息学汇刊,
    13(2):551–561, 2017。'
- en: '[45] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and Huchuan Lu. Real-time’actor-critic’tracking.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 318–334,
    2018.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, 和 Huchuan Lu. 实时“演员-评论员”跟踪。发表于欧洲计算机视觉会议（ECCV）论文集,
    页码 318–334, 2018。'
- en: '[46] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
    Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets,
    atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis
    and machine intelligence, 40(4):834–848, 2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, 和
    Alan L Yuille. Deeplab: 基于深度卷积网络、空洞卷积和全连接条件随机场的语义图像分割。IEEE模式分析与机器智能汇刊, 40(4):834–848,
    2017。'
- en: '[47] Yushi Chen, Xing Zhao, and Xiuping Jia. Spectral–spatial classification
    of hyperspectral data based on deep belief network. IEEE Journal of Selected Topics
    in Applied Earth Observations and Remote Sensing, 8(6):2381–2392, 2015.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Yushi Chen, Xing Zhao, 和 Xiuping Jia. 基于深度置信网络的高光谱数据谱-空间分类。IEEE地球观测与遥感精选专题汇刊,
    8(6):2381–2392, 2015。'
- en: '[48] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-Hsuan Yang. Segflow:
    Joint learning for video object segmentation and optical flow. In Proceedings
    of the IEEE international conference on computer vision, pages 686–695, 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, 和 Ming-Hsuan Yang. Segflow:
    视频物体分割与光流的联合学习。发表于IEEE国际计算机视觉会议论文集, 页码 686–695, 2017。'
- en: '[49] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min
    Hu. Global contrast based salient region detection. IEEE transactions on pattern
    analysis and machine intelligence, 37(3):569–582, 2014.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, 和 Shi-Min
    Hu. 基于全局对比度的显著区域检测。IEEE模式分析与机器智能汇刊, 37(3):569–582, 2014。'
- en: '[50] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using rnn encoder-decoder for statistical machine translation. arXiv preprint
    arXiv:1406.1078, 2014.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, 和 Yoshua Bengio. 使用RNN编码器-解码器学习短语表示用于统计机器翻译。arXiv预印本
    arXiv:1406.1078, 2014。'
- en: '[51] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares,
    Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
    for statistical machine translation. CoRR, abs/1406.1078, 2014.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares,
    Holger Schwenk, 和 Yoshua Bengio。利用RNN编码器-解码器学习短语表示，用于统计机器翻译。CoRR, abs/1406.1078,
    2014。'
- en: '[52] Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris,
    and Jin Young Choi. Attentional correlation filter network for adaptive visual
    tracking. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pages 4807–4816, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris,
    和 Jin Young Choi。用于自适应视觉跟踪的注意力相关滤波器网络。在IEEE计算机视觉与模式识别会议论文集，第4807–4816页，2017。'
- en: '[53] Wongun Choi. Near-online multi-target tracking with aggregated local flow
    descriptor. In Proceedings of the IEEE international conference on computer vision,
    pages 3029–3037, 2015.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Wongun Choi。使用聚合的局部流描述符进行近在线多目标跟踪。在IEEE国际计算机视觉会议论文集，第3029–3037页，2015。'
- en: '[54] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, and Yoshua
    Bengio. Attention-based models for speech recognition. CoRR, abs/1506.07503, 2015.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, 和 Yoshua
    Bengio。基于注意力的语音识别模型。CoRR, abs/1506.07503, 2015。'
- en: '[55] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai
    Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal
    attention mechanism. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 4836–4845, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, 和 Nenghai
    Yu。使用基于cnn的单目标跟踪器和时空注意力机制的在线多目标跟踪。IEEE计算机视觉国际会议论文集，第4836–4845页，2017。'
- en: '[56] Wen-Hsuan Chu and Kris M. Kitani. Neural batch sampling with reinforcement
    learning for semi-supervised anomaly detection. In European Conference on Computer
    Vision, pages 751–766, 2020.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Wen-Hsuan Chu 和 Kris M. Kitani。使用强化学习的神经批量采样用于半监督异常检测。在欧洲计算机视觉会议论文集，第751–766页，2020。'
- en: '[57] Wen-Sheng Chu, Yale Song, and Alejandro Jaimes. Video co-summarization:
    Video summarization by visual co-occurrence. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 3584–3592, 2015.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Wen-Sheng Chu, Yale Song, 和 Alejandro Jaimes。视频共同总结：通过视觉共现进行视频总结。在IEEE计算机视觉与模式识别会议论文集，第3584–3592页，2015。'
- en: '[58] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim
    Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy
    optimization. CoRR, abs/1809.05214, 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim
    Asfour, 和 Pieter Abbeel。通过元策略优化的基于模型的强化学习。CoRR, abs/1809.05214, 2018。'
- en: '[59] Adam Coates, Pieter Abbeel, and Andrew Y. Ng. Apprenticeship learning
    for helicopter control. Commun. ACM, 52(7):97–105, July 2009.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Adam Coates, Pieter Abbeel, 和 Andrew Y. Ng。用于直升机控制的学徒学习。Commun. ACM, 52(7):97–105,
    2009年7月。'
- en: '[60] Dorin Comaniciu, Visvanathan Ramesh, and Peter Meer. Real-time tracking
    of non-rigid objects using mean shift. In Proceedings IEEE Conference on Computer
    Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662), volume 2, pages
    142–149\. IEEE, 2000.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Dorin Comaniciu, Visvanathan Ramesh, 和 Peter Meer。利用均值漂移实时跟踪非刚性物体。在IEEE计算机视觉与模式识别会议论文集。CVPR
    2000（Cat. No. PR00662），第2卷，第142–149页。IEEE, 2000。'
- en: '[61] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
    Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset
    for semantic urban scene understanding. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 3213–3223, 2016.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
    Rodrigo Benenson, Uwe Franke, Stefan Roth, 和 Bernt Schiele。城市景观数据集用于语义城市场景理解。在IEEE计算机视觉与模式识别会议论文集，第3213–3223页，2016。'
- en: '[62] Rémi Coulom. Efficient selectivity and backup operators in monte-carlo
    tree search. In Proceedings of the 5th International Conference on Computers and
    Games, page 72–83, 2006.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Rémi Coulom。蒙特卡罗树搜索中的高效选择性和备份操作符。在第5届国际计算机与游戏会议论文集，第72–83页，2006。'
- en: '[63] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation
    for games, robotics and machine learning. 2016.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Erwin Coumans 和 Yunfei Bai。Pybullet，一个用于游戏、机器人和机器学习的物理仿真Python模块。2016。'
- en: '[64] Antonio Criminisi, Jamie Shotton, Duncan Robertson, and Ender Konukoglu.
    Regression forests for efficient anatomy detection and localization in ct studies.
    In International MICCAI Workshop on Medical Computer Vision, pages 106–117\. Springer,
    2010.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Antonio Criminisi, Jamie Shotton, Duncan Robertson 和 Ender Konukoglu.
    用于CT研究的高效解剖检测与定位的回归森林。见于国际MICCAI医学计算机视觉研讨会，页码106–117。Springer，2010年。'
- en: '[65] Tianhong Dai, Magda Dubois, Kai Arulkumaran, Jonathan Campbell, Cher Bass,
    Benjamin Billot, Fatmatulzehra Uslu, Vincenzo De Paola, Claudia Clopath, and Anil Anthony
    Bharath. Deep reinforcement learning for subpixel neural tracking. In International
    Conference on Medical Imaging with Deep Learning, pages 130–150, 2019.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Tianhong Dai, Magda Dubois, Kai Arulkumaran, Jonathan Campbell, Cher Bass,
    Benjamin Billot, Fatmatulzehra Uslu, Vincenzo De Paola, Claudia Clopath 和 Anil
    Anthony Bharath. 用于亚像素神经跟踪的深度强化学习。见于国际医学影像深度学习会议，页码130–150，2019年。'
- en: '[66] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg.
    Eco: Efficient convolution operators for tracking. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 6638–6646, 2017.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan 和 Michael Felsberg.
    ECO：用于跟踪的高效卷积操作符。见于IEEE计算机视觉与模式识别会议论文集，页码6638–6646，2017年。'
- en: '[67] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and Michael Felsberg.
    Learning spatially regularized correlation filters for visual tracking. In Proceedings
    of the IEEE international conference on computer vision, pages 4310–4318, 2015.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan 和 Michael Felsberg.
    学习空间正则化相关滤波器用于视觉跟踪。见于IEEE国际计算机视觉会议论文集，页码4310–4318，2015年。'
- en: '[68] Kristopher De Asis, J Fernando Hernandez-Garcia, G Zacharias Holland,
    and Richard S Sutton. Multi-step reinforcement learning: A unifying algorithm.
    In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Kristopher De Asis, J Fernando Hernandez-Garcia, G Zacharias Holland 和
    Richard S Sutton. 多步强化学习：统一算法。见于第三十二届AAAI人工智能会议，2018年。'
- en: '[69] Sandra Eliza Fontes De Avila, Ana Paula Brandão Lopes, Antonio da Luz Jr,
    and Arnaldo de Albuquerque Araújo. Vsumm: A mechanism designed to produce static
    video summaries and a novel evaluation method. Pattern Recognition Letters, 32(1):56–68,
    2011.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Sandra Eliza Fontes De Avila, Ana Paula Brandão Lopes, Antonio da Luz
    Jr 和 Arnaldo de Albuquerque Araújo. VSUMM：一种用于生成静态视频摘要的机制及一种新型评估方法。模式识别快报，32(1)：56–68，2011年。'
- en: '[70] Antonio de Marvao, Timothy JW Dawes, Wenzhe Shi, Christopher Minas, Niall G
    Keenan, Tamara Diamond, Giuliana Durighel, Giovanni Montana, Daniel Rueckert,
    Stuart A Cook, et al. Population-based studies of myocardial hypertrophy: high
    resolution cardiovascular magnetic resonance atlases improve statistical power.
    Journal of cardiovascular magnetic resonance, 16(1):16, 2014.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Antonio de Marvao, Timothy JW Dawes, Wenzhe Shi, Christopher Minas, Niall
    G Keenan, Tamara Diamond, Giuliana Durighel, Giovanni Montana, Daniel Rueckert,
    Stuart A Cook 等。心肌肥厚的群体研究：高分辨率心血管磁共振图谱提高了统计能力。心血管磁共振杂志，16(1)：16，2014年。'
- en: '[71] M. P. Deisenroth, P. Englert, J. Peters, and D. Fox. Multi-task policy
    search for robotics. In 2014 IEEE International Conference on Robotics and Automation
    (ICRA), pages 3876–3881, 2014.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. P. Deisenroth, P. Englert, J. Peters 和 D. Fox. 机器人多任务策略搜索。见于2014 IEEE国际机器人与自动化会议（ICRA），页码3876–3881，2014年。'
- en: '[72] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
    Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on
    computer vision and pattern recognition, pages 248–255\. Ieee, 2009.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li 和 Li Fei-Fei. ImageNet：大规模分层图像数据库。见于2009
    IEEE计算机视觉与模式识别会议，页码248–255。IEEE，2009年。'
- en: '[73] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface:
    Additive angular margin loss for deep face recognition. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 4690–4699, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Jiankang Deng, Jia Guo, Niannan Xue 和 Stefanos Zafeiriou. ArcFace：用于深度人脸识别的加性角度边际损失。见于IEEE计算机视觉与模式识别会议论文集，页码4690–4699，2019年。'
- en: '[74] Joachim Denzler and Dietrich WR Paulus. Active motion detection and object
    tracking. In Proceedings of 1st International Conference on Image Processing,
    volume 3, pages 635–639\. IEEE, 1994.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Joachim Denzler 和 Dietrich WR Paulus. 主动运动检测和物体跟踪。见于第一届国际图像处理会议论文集，第3卷，页码635–639。IEEE，1994年。'
- en: '[75] B. Depraetere, M. Liu, G. Pinte, I. Grondman, and R. BabuÅ¡ka. Comparison
    of model-free and model-based methods for time optimal hit control of a badminton
    robot. Mechatronics, 24(8):1021 – 1030, 2014.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] B. Depraetere, M. Liu, G. Pinte, I. Grondman 和 R. Babuška. 无模型与有模型方法在羽毛球机器人时间最优击球控制中的比较。机电一体化，24(8)：1021–1030，2014年。'
- en: '[76] Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S Swaroop Vedula,
    Gyusung I Lee, Mija R Lee, and Gregory D Hager. Recognizing surgical activities
    with recurrent neural networks. In International conference on medical image computing
    and computer-assisted intervention, pages 551–558\. Springer, 2016.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S Swaroop Vedula,
    Gyusung I Lee, Mija R Lee 和 Gregory D Hager. 使用递归神经网络识别手术活动。发表于医学图像计算与计算机辅助干预国际会议，页码
    551–558。Springer，2016 年。'
- en: '[77] Piotr Dollár, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian
    detection: A benchmark. In 2009 IEEE Conference on Computer Vision and Pattern
    Recognition, pages 304–311\. IEEE, 2009.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Piotr Dollár, Christian Wojek, Bernt Schiele 和 Pietro Perona. 行人检测：一个基准。发表于
    2009 IEEE 计算机视觉与模式识别会议，页码 304–311。IEEE，2009 年。'
- en: '[78] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach,
    Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional
    networks for visual recognition and description. CoRR, abs/1411.4389, 2014.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach,
    Subhashini Venugopalan, Kate Saenko 和 Trevor Darrell. 用于视觉识别和描述的长期递归卷积网络。CoRR，abs/1411.4389，2014
    年。'
- en: '[79] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
    Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
    Learning optical flow with convolutional networks. In Proceedings of the IEEE
    international conference on computer vision, pages 2758–2766, 2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
    Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers 和 Thomas Brox. Flownet：使用卷积网络学习光流。发表于
    IEEE 国际计算机视觉会议论文集，页码 2758–2766，2015 年。'
- en: '[80] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
    Koltun. Carla: An open urban driving simulator. arXiv preprint arXiv:1711.03938,
    2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez 和 Vladlen
    Koltun. Carla：一个开放的城市驾驶模拟器。arXiv 预印本 arXiv:1711.03938，2017 年。'
- en: '[81] Yong Du, Wei Wang, and Liang Wang. Hierarchical recurrent neural network
    for skeleton based action recognition. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 1110–1118, 2015.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Yong Du, Wei Wang 和 Liang Wang. 基于骨架的动作识别的层次递归神经网络。发表于 IEEE 计算机视觉与模式识别会议论文集，页码
    1110–1118，2015 年。'
- en: '[82] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin
    Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges
    of real-world reinforcement learning, 2020.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin
    Paduraru, Sven Gowal 和 Todd Hester. 对现实世界强化学习挑战的实证调查，2020 年。'
- en: '[83] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin
    Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges
    of real-world reinforcement learning. 2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin
    Paduraru, Sven Gowal 和 Todd Hester. 对现实世界强化学习挑战的实证调查。2020 年。'
- en: '[84] Matteo Dunnhofer, Niki Martinel, Gian Luca Foresti, and Christian Micheloni.
    Visual tracking by means of deep reinforcement learning and an expert demonstrator.
    In Proceedings of the IEEE International Conference on Computer Vision Workshops,
    pages 0–0, 2019.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Matteo Dunnhofer, Niki Martinel, Gian Luca Foresti 和 Christian Micheloni.
    利用深度强化学习和专家演示者进行视觉跟踪。发表于 IEEE 国际计算机视觉研讨会论文集，页码 0–0，2019 年。'
- en: '[85] Chi Nhan Duong, Kha Gia Quach, Ibsa Jalata, Ngan Le, and Khoa Luu. Mobiface:
    A lightweight deep learning face recognition on mobile devices. In 2019 IEEE 10th
    International Conference on Biometrics Theory, Applications and Systems (BTAS),
    pages 1–6\. IEEE, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Chi Nhan Duong, Kha Gia Quach, Ibsa Jalata, Ngan Le 和 Khoa Luu. Mobiface：一种轻量级的移动设备深度学习面部识别。发表于
    2019 IEEE 第十届生物特征理论、应用和系统国际会议（BTAS），页码 1–6。IEEE，2019 年。'
- en: '[86] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Le, Marios Savvides,
    and Tien D. Bui. Learning from longitudinal face demonstration–where tractable
    deep modeling meets inverse reinforcement learning. 127(6–7), 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Le, Marios Savvides
    和 Tien D. Bui. 从纵向面部演示中学习——可处理的深度建模与逆强化学习相遇的地方。127(6–7)，2019 年。'
- en: '[87] A. El-Fakdi and M. Carreras. Policy gradient based reinforcement learning
    for real autonomous underwater cable tracking. In 2008 IEEE/RSJ International
    Conference on Intelligent Robots and Systems, pages 3635–3640, 2008.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] A. El-Fakdi 和 M. Carreras. 基于策略梯度的强化学习用于真实的自主水下电缆跟踪。发表于 2008 IEEE/RSJ
    国际智能机器人与系统会议，页码 3635–3640，2008 年。'
- en: '[88] Ehsan Elhamifar, Guillermo Sapiro, and Rene Vidal. See all by looking
    at a few: Sparse modeling for finding representative objects. In 2012 IEEE conference
    on computer vision and pattern recognition, pages 1600–1607\. IEEE, 2012.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Ehsan Elhamifar, Guillermo Sapiro 和 Rene Vidal。通过查看少量信息看清所有：用于寻找代表性对象的稀疏建模。在
    2012 年 IEEE 计算机视觉与模式识别大会上，第 1600–1607 页。IEEE，2012 年。'
- en: '[89] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov.
    Scalable object detection using deep neural networks. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 2147–2154, 2014.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Dumitru Erhan, Christian Szegedy, Alexander Toshev 和 Dragomir Anguelov。使用深度神经网络进行可扩展对象检测。在
    IEEE 计算机视觉与模式识别大会论文集中，第 2147–2154 页，2014 年。'
- en: '[90] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and
    Andrew Zisserman. The pascal visual object classes challenge 2007 (voc2007) results.
    2007.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn 和 Andrew
    Zisserman。Pascal 视觉对象类别挑战 2007 (voc2007) 结果。2007 年。'
- en: '[91] Mark Everingham and John Winn. The pascal visual object classes challenge
    2012 (voc2012) development kit. Pattern Analysis, Statistical Modelling and Computational
    Learning, Tech. Rep, 8, 2011.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Mark Everingham 和 John Winn。Pascal 视觉对象类别挑战 2012 (voc2012) 开发工具包。模式分析、统计建模与计算学习，技术报告，第
    8 期，2011 年。'
- en: '[92] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai,
    Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale
    single object tracking. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 5374–5383, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai,
    Yong Xu, Chunyuan Liao 和 Haibin Ling。Lasot：高质量的大规模单目标跟踪基准。在 IEEE 计算机视觉与模式识别大会论文集中，第
    5374–5383 页，2019 年。'
- en: '[93] Heng Fan and Haibin Ling. Parallel tracking and verifying: A framework
    for real-time and high accuracy visual tracking. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 5486–5494, 2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Heng Fan 和 Haibin Ling。并行跟踪与验证：一种用于实时和高精度视觉跟踪的框架。在 IEEE 国际计算机视觉大会论文集中，第
    5486–5494 页，2017 年。'
- en: '[94] Jialue Fan, Wei Xu, Ying Wu, and Yihong Gong. Human tracking using convolutional
    neural networks. IEEE Transactions on Neural Networks, 21(10):1610–1623, 2010.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Jialue Fan, Wei Xu, Ying Wu 和 Yihong Gong。使用卷积神经网络进行人体跟踪。IEEE 神经网络交易，21(10)：1610–1623，2010
    年。'
- en: '[95] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and
    Pieter Abbeel. Deep spatial autoencoders for visuomotor learning. In Danica Kragic,
    Antonio Bicchi, and Alessandro De Luca, editors, 2016 IEEE International Conference
    on Robotics and Automation, ICRA 2016, Stockholm, Sweden, May 16-21, 2016, pages
    512–519.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine 和 Pieter
    Abbeel。用于视觉运动学习的深度空间自编码器。在 Danica Kragic, Antonio Bicchi 和 Alessandro De Luca
    编辑的 2016 年 IEEE 国际机器人与自动化会议，ICRA 2016，瑞典斯德哥尔摩，2016 年 5 月 16-21 日，第 512–519 页。'
- en: '[96] J Michael Fitzpatrick and Jay B West. The distribution of target registration
    error in rigid-body point-based registration. IEEE transactions on medical imaging,
    20(9):917–927, 2001.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J Michael Fitzpatrick 和 Jay B West。刚体点对点配准中的目标注册误差分布。IEEE 医学影像交易，20(9)：917–927，2001
    年。'
- en: '[97] Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare,
    and Joelle Pineau. An introduction to deep reinforcement learning. arXiv preprint
    arXiv:1811.12560, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare
    和 Joelle Pineau。深度强化学习简介。arXiv 预印本 arXiv:1811.12560，2018 年。'
- en: '[98] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Eslami, and Oriol
    Vinyals. Synthesizing programs for images using reinforced adversarial learning.
    arXiv preprint arXiv:1804.01118, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Eslami 和 Oriol Vinyals。使用强化对抗学习合成图像程序。arXiv
    预印本 arXiv:1804.01118，2018 年。'
- en: '[99] Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and Larry S Davis. Dynamic
    zoom-in network for fast object detection in large images. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 6926–6935, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu 和 Larry S Davis。用于大图像快速对象检测的动态缩放网络。在
    IEEE 计算机视觉与模式识别大会论文集中，第 6926–6935 页，2018 年。'
- en: '[100] Yixin Gao, S Swaroop Vedula, Carol E Reiley, Narges Ahmidi, Balakrishnan
    Varadarajan, Henry C Lin, Lingling Tao, Luca Zappella, Benjamın Béjar, David D
    Yuh, et al. Jhu-isi gesture and skill assessment working set (jigsaws): A surgical
    activity dataset for human motion modeling. In Miccai workshop: M2cai, volume 3,
    page 3, 2014.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Yixin Gao, S Swaroop Vedula, Carol E Reiley, Narges Ahmidi, Balakrishnan
    Varadarajan, Henry C Lin, Lingling Tao, Luca Zappella, Benjamín Béjar, David D
    Yuh 等人。Jhu-isi 手势与技能评估工作集 (jigsaws)：用于人体运动建模的外科活动数据集。在 Miccai workshop: M2cai，第
    3 卷，第 3 页，2014 年。'
- en: '[101] Romane Gauriau, Rémi Cuingnet, David Lesage, and Isabelle Bloch. Multi-organ
    localization combining global-to-local regression and confidence maps. In International
    Conference on Medical Image Computing and Computer-Assisted Intervention, pages
    337–344\. Springer, 2014.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Romane Gauriau, Rémi Cuingnet, David Lesage 和 Isabelle Bloch. 结合全局到局部回归和置信图的多器官定位。发表于《医学图像计算与计算机辅助手术国际会议论文集》，第337–344页。Springer，2014年。'
- en: '[102] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving?
    the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and
    Pattern Recognition, pages 3354–3361, 2012.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. Geiger, P. Lenz 和 R. Urtasun. 我们准备好进行自动驾驶了吗？KITTI视觉基准套件。发表于2012年IEEE计算机视觉与模式识别会议论文集，第3354–3361页，2012年。'
- en: '[103] Michaël Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W Hasinoff, and
    Frédo Durand. Deep bilateral learning for real-time image enhancement. ACM Transactions
    on Graphics (TOG), 36(4):1–12, 2017.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Michaël Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W Hasinoff 和 Frédo
    Durand. 实时图像增强的深度双边学习。ACM图形学汇刊（TOG），36(4)：1–12，2017年。'
- en: '[104] Florin C Ghesu, Edward Krubasik, Bogdan Georgescu, Vivek Singh, Yefeng
    Zheng, Joachim Hornegger, and Dorin Comaniciu. Marginal space deep learning: efficient
    architecture for volumetric image parsing. IEEE transactions on medical imaging,
    35(5):1217–1228, 2016.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Florin C Ghesu, Edward Krubasik, Bogdan Georgescu, Vivek Singh, Yefeng
    Zheng, Joachim Hornegger 和 Dorin Comaniciu. 边际空间深度学习：高效的体积图像解析架构。IEEE医学影像学汇刊，35(5)：1217–1228，2016年。'
- en: '[105] Florin-Cristian Ghesu, Bogdan Georgescu, Yefeng Zheng, Sasa Grbic, Andreas
    Maier, Joachim Hornegger, and Dorin Comaniciu. Multi-scale deep reinforcement
    learning for real-time 3d-landmark detection in ct scans. IEEE transactions on
    pattern analysis and machine intelligence, 41(1):176–189, 2017.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Florin-Cristian Ghesu, Bogdan Georgescu, Yefeng Zheng, Sasa Grbic, Andreas
    Maier, Joachim Hornegger 和 Dorin Comaniciu. 实时3D标志检测的多尺度深度强化学习。IEEE模式分析与机器智能汇刊，41(1)：176–189，2017年。'
- en: '[106] M Giles. Mit technology review. Google researchers have reportedly achieved”
    quantum supremacy” URL: https:/www.technologyreview. com/f, 614416, 2017.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] M Giles. 《麻省理工科技评论》。谷歌研究人员已 reportedly 实现了“量子霸权” URL: https:/www.technologyreview.
    com/f, 614416, 2017年。'
- en: '[107] Justin Girard and M Reza Emami. Concurrent markov decision processes
    for robot team learning. Engineering applications of artificial intelligence,
    39:223–234, 2015.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Justin Girard 和 M Reza Emami. 机器人团队学习的并发马尔可夫决策过程。人工智能工程应用，39：223–234，2015年。'
- en: '[108] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference
    on computer vision, pages 1440–1448, 2015.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Ross Girshick. 快速 R-CNN。发表于《IEEE国际计算机视觉大会论文集》，第1440–1448页，2015年。'
- en: '[109] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich
    feature hierarchies for accurate object detection and semantic segmentation. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 580–587, 2014.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Ross Girshick, Jeff Donahue, Trevor Darrell 和 Jitendra Malik. 准确的物体检测和语义分割的丰富特征层次结构。发表于《IEEE计算机视觉与模式识别会议论文集》，第580–587页，2014年。'
- en: '[110] Georgia Gkioxari, Ross Girshick, and Jitendra Malik. Contextual action
    recognition with r* cnn. In Proceedings of the IEEE international conference on
    computer vision, pages 1080–1088, 2015.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Georgia Gkioxari, Ross Girshick 和 Jitendra Malik. 使用 R* CNN 的上下文动作识别。发表于《IEEE国际计算机视觉会议论文集》，第1080–1088页，2015年。'
- en: '[111] Vikash Goel, Jameson Weng, and Pascal Poupart. Unsupervised video object
    segmentation for deep reinforcement learning. In Advances in Neural Information
    Processing Systems, pages 5683–5694, 2018.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Vikash Goel, Jameson Weng 和 Pascal Poupart. 无监督视频物体分割用于深度强化学习。发表于《神经信息处理系统进展》，第5683–5694页，2018年。'
- en: '[112] Abel Gonzalez-Garcia, Alexander Vezhnevets, and Vittorio Ferrari. An
    active search strategy for efficient object class detection. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 3022–3031,
    2015.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Abel Gonzalez-Garcia, Alexander Vezhnevets 和 Vittorio Ferrari. 一种高效物体类别检测的主动搜索策略。发表于《IEEE计算机视觉与模式识别会议论文集》，第3022–3031页，2015年。'
- en: '[113] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Advances in neural information processing systems, pages 2672–2680, 2014.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville 和 Yoshua Bengio. 生成对抗网络。发表于《神经信息处理系统进展》，第2672–2680页，2014年。'
- en: '[114] Leo Grady. Random walks for image segmentation. IEEE transactions on
    pattern analysis and machine intelligence, 28(11):1768–1783, 2006.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Leo Grady. 图像分割的随机游走。IEEE模式分析与机器智能汇刊，28(11)：1768–1783，2006年。'
- en: '[115] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition
    with deep recurrent neural networks. CoRR, abs/1303.5778, 2013.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Alex Graves, Abdel-rahman Mohamed, 和 Geoffrey E. Hinton. 使用深度递归神经网络的语音识别.
    CoRR, abs/1303.5778, 2013.'
- en: '[116] Albert Gubern-Mérida, Robert Martí, Jaime Melendez, Jakob L Hauth, Ritse M
    Mann, Nico Karssemeijer, and Bram Platel. Automated localization of breast cancer
    in dce-mri. Medical image analysis, 20(1):265–274, 2015.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Albert Gubern-Mérida, Robert Martí, Jaime Melendez, Jakob L Hauth, Ritse
    M Mann, Nico Karssemeijer, 和 Bram Platel. DCE-MRI 中乳腺癌的自动定位. 医学图像分析, 20(1):265–274,
    2015.'
- en: '[117] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
    Aaron C Courville. Improved training of wasserstein gans. In Advances in neural
    information processing systems, pages 5767–5777, 2017.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, 和 Aaron
    C Courville. 改进的 Wasserstein GAN 训练. 在神经信息处理系统进展, 页码 5767–5777, 2017.'
- en: '[118] Minghao Guo, Jiwen Lu, and Jie Zhou. Dual-agent deep reinforcement learning
    for deformable face tracking. In Proceedings of the European Conference on Computer
    Vision (ECCV), pages 768–783, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Minghao Guo, Jiwen Lu, 和 Jie Zhou. 双代理深度强化学习用于可变形面部跟踪. 在欧洲计算机视觉会议 (ECCV)
    论文集, 页码 768–783, 2018.'
- en: '[119] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey
    Levine. Meta-reinforcement learning of structured exploration strategies. In Advances
    in Neural Information Processing Systems, pages 5302–5311, 2018.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, 和 Sergey
    Levine. 结构化探索策略的元强化学习. 在神经信息处理系统进展, 页码 5302–5311, 2018.'
- en: '[120] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization
    and recognition of indoor scenes from rgb-d images. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 564–571, 2013.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Saurabh Gupta, Pablo Arbelaez, 和 Jitendra Malik. 从 RGB-D 图像中感知组织与室内场景识别.
    在 IEEE 计算机视觉与模式识别会议论文集, 页码 564–571, 2013.'
- en: '[121] Saurabh Gupta, Ross Girshick, Pablo Arbeláez, and Jitendra Malik. Learning
    rich features from rgb-d images for object detection and segmentation. In European
    conference on computer vision, pages 345–360. Springer, 2014.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Saurabh Gupta, Ross Girshick, Pablo Arbeláez, 和 Jitendra Malik. 从 RGB-D
    图像中学习丰富特征以进行物体检测与分割. 在欧洲计算机视觉大会, 页码 345–360. Springer, 2014.'
- en: '[122] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool.
    Creating summaries from user videos. In European conference on computer vision,
    pages 505–520. Springer, 2014.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, 和 Luc Van Gool.
    从用户视频中创建摘要. 在欧洲计算机视觉大会, 页码 505–520. Springer, 2014.'
- en: '[123] Seyed Hamid Rezatofighi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony
    Dick, and Ian Reid. Joint probabilistic data association revisited. In Proceedings
    of the IEEE international conference on computer vision, pages 3047–3055, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Seyed Hamid Rezatofighi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony
    Dick, 和 Ian Reid. 重新审视联合概率数据关联. 在 IEEE 国际计算机视觉会议论文集, 页码 3047–3055, 2015.'
- en: '[124] Junwei Han, Le Yang, Dingwen Zhang, Xiaojun Chang, and Xiaodan Liang.
    Reinforcement cutting-agent learning for video object segmentation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9080–9089,
    2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Junwei Han, Le Yang, Dingwen Zhang, Xiaojun Chang, 和 Xiaodan Liang. 用于视频对象分割的强化学习切割代理.
    在 IEEE 计算机视觉与模式识别会议论文集, 页码 9080–9089, 2018.'
- en: '[125] Robert M Haralick and Linda G Shapiro. Image segmentation techniques.
    Computer vision, graphics, and image processing, 29(1):100–132, 1985.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Robert M Haralick 和 Linda G Shapiro. 图像分割技术. 计算机视觉、图形学与图像处理, 29(1):100–132,
    1985.'
- en: '[126] Sam Hare, Stuart Golodetz, Amir Saffari, Vibhav Vineet, Ming-Ming Cheng,
    Stephen L Hicks, and Philip HS Torr. Struck: Structured output tracking with kernels.
    IEEE transactions on pattern analysis and machine intelligence, 38(10):2096–2109,
    2015.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Sam Hare, Stuart Golodetz, Amir Saffari, Vibhav Vineet, Ming-Ming Cheng,
    Stephen L Hicks, 和 Philip HS Torr. STRUCK: 使用核的结构化输出跟踪. IEEE 模式分析与机器智能学报, 38(10):2096–2109,
    2015.'
- en: '[127] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Jitendra Malik.
    Simultaneous detection and segmentation. In European Conference on Computer Vision,
    pages 297–312. Springer, 2014.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, 和 Jitendra Malik. 同时检测与分割.
    在欧洲计算机视觉大会, 页码 297–312. Springer, 2014.'
- en: '[128] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Jitendra Malik.
    Hypercolumns for object segmentation and fine-grained localization. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 447–456,
    2015.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, 和 Jitendra Malik. 用于物体分割和细粒度定位的
    Hypercolumns. 在 IEEE 计算机视觉与模式识别会议论文集, 页码 447–456, 2015.'
- en: '[129] Hannes Hase, Mohammad Farid Azampour, Maria Tirindelli, Magdalini Paschali,
    Walter Simson, Emad Fatemizadeh, and Nassir Navab. Ultrasound-guided robotic navigation
    with deep reinforcement learning. arXiv preprint arXiv:2003.13321, 2020.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Hannes Hase, Mohammad Farid Azampour, Maria Tirindelli, Magdalini Paschali,
    Walter Simson, Emad Fatemizadeh 和 Nassir Navab。利用深度强化学习进行超声引导的机器人导航。arXiv 预印本
    arXiv:2003.13321, 2020。'
- en: '[130] Hado V Hasselt. Double q-learning. In Advances in neural information
    processing systems, pages 2613–2621, 2010.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Hado V Hasselt。双重 Q 学习。发表于神经信息处理系统进展, 页码 2613–2621, 2010。'
- en: '[131] Matthew J. Hausknecht and Peter Stone. Deep recurrent q-learning for
    partially observable mdps. CoRR, abs/1507.06527, 2015.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Matthew J. Hausknecht 和 Peter Stone。深度递归 Q 学习用于部分可观测的 MDPS。CoRR, abs/1507.06527,
    2015。'
- en: '[132] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn.
    In Proceedings of the IEEE international conference on computer vision, pages
    2961–2969, 2017.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Kaiming He, Georgia Gkioxari, Piotr Dollár 和 Ross Girshick。Mask R-CNN。发表于
    IEEE 国际计算机视觉会议论文集, 页码 2961–2969, 2017。'
- en: '[133] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual
    learning for image recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 770–778, 2016.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun。图像识别的深度残差学习。发表于 IEEE
    计算机视觉与模式识别会议论文集, 页码 770–778, 2016。'
- en: '[134] João F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed
    tracking with kernelized correlation filters. IEEE transactions on pattern analysis
    and machine intelligence, 37(3):583–596, 2014.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] João F Henriques, Rui Caseiro, Pedro Martins 和 Jorge Batista。使用核相关滤波器的高速跟踪。IEEE
    模式分析与机器智能学报, 37(3):583–596, 2014。'
- en: '[135] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz
    de Cote. A survey of learning in multiagent environments: Dealing with non-stationarity.
    CoRR, abs/1707.09183, 2017.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag 和 Enrique Munoz de
    Cote。关于多智能体环境中的学习调查：处理非平稳性。CoRR, abs/1707.09183, 2017。'
- en: '[136] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski,
    Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow:
    Combining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298,
    2017.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski,
    Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar 和 David Silver。Rainbow：结合深度强化学习的改进。arXiv
    预印本 arXiv:1710.02298, 2017。'
- en: '[137] Todd Hester, Michael Quinlan, and Peter Stone. A real-time model-based
    reinforcement learning architecture for robot control. CoRR, abs/1105.1749, 2011.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Todd Hester, Michael Quinlan 和 Peter Stone。一种实时基于模型的强化学习架构用于机器人控制。CoRR,
    abs/1105.1749, 2011。'
- en: '[138] Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks
    principle: Reading children’s books with explicit memory representations. CoRR,
    abs/1511.02301, 2015.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Felix Hill, Antoine Bordes, Sumit Chopra 和 Jason Weston。Goldilocks 原则：通过显式记忆表示阅读儿童书籍。CoRR,
    abs/1511.02301, 2015。'
- en: '[139] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Sepp Hochreiter 和 Jürgen Schmidhuber。长短期记忆。神经计算, 9(8):1735–1780, 1997。'
- en: '[140] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recovering surface layout
    from an image. International Journal of Computer Vision, 75(1):151–172, 2007.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Derek Hoiem, Alexei A Efros 和 Martial Hebert。从图像中恢复表面布局。国际计算机视觉杂志, 75(1):151–172,
    2007。'
- en: '[141] James B. Holliday and Ngan T.H. Le. Follow then forage exploration: Improving
    asynchronous advantage actor critic. International Conference on Soft Computing,
    Artificial Intelligence and Applications (SAI 2020), pages 107–118, 2020.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] James B. Holliday 和 Ngan T.H. Le。跟随再觅探索：改进异步优势演员-评论员。国际软计算、人工智能与应用会议（SAI
    2020）, 页码 107–118, 2020。'
- en: '[142] Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang
    Chang, Hsuan-Kung Yang, Brian Hsi-Lin Ho, Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching
    Hsiao, et al. Virtual-to-real: Learning to control in visual semantic segmentation.
    arXiv preprint arXiv:1802.00285, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang
    Chang, Hsuan-Kung Yang, Brian Hsi-Lin Ho, Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching
    Hsiao 等。虚拟到真实：在视觉语义分割中学习控制。arXiv 预印本 arXiv:1802.00285, 2018。'
- en: '[143] Ju Hong Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang, and Kuk-Jin Yoon. Online
    multi-object tracking via structural constraint event aggregation. In Proceedings
    of the IEEE Conference on computer vision and pattern recognition, pages 1392–1400,
    2016.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Ju Hong Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang 和 Kuk-Jin Yoon。通过结构约束事件聚合进行在线多目标跟踪。发表于
    IEEE 计算机视觉与模式识别会议论文集, 页码 1392–1400, 2016。'
- en: '[144] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In 2018
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7132–7141,
    2018.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] 胡杰、申龙和孙刚。挤压与激励网络。见于 2018 IEEE/CVF 计算机视觉与模式识别会议，页码 7132–7141, 2018。'
- en: '[145] Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. Jointly
    learning heterogeneous features for rgb-d activity recognition. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 5344–5352,
    2015.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] 胡建芳、郑伟石、赖建煌和张建国。联合学习异构特征进行 RGB-D 活动识别。见于 IEEE 计算机视觉与模式识别会议论文集，页码 5344–5352,
    2015。'
- en: '[146] Weiming Hu, Xi Li, Wenhan Luo, Xiaoqin Zhang, Stephen Maybank, and Zhongfei
    Zhang. Single and multiple object tracking using log-euclidean riemannian subspace
    and block-division appearance model. IEEE transactions on pattern analysis and
    machine intelligence, 34(12):2420–2440, 2012.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] 胡维明、李熙、罗文汉、张晓琴、斯蒂芬·梅银行和张中飞。使用对数欧几里得黎曼子空间和块划分外观模型进行单对象和多对象跟踪。IEEE 模式分析与机器智能学报，34(12):2420–2440,
    2012。'
- en: '[147] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
    Densely connected convolutional networks. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 4700–4708, 2017.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] 高黄、刘庄、劳伦斯·范·德尔·马滕和基利安·Q·温伯格。密集连接卷积网络。见于 IEEE 计算机视觉与模式识别会议论文集，页码 4700–4708,
    2017。'
- en: '[148] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity
    benchmark for generic object tracking in the wild. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 2019.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] 梁华、辛赵和黄凯琪。Got-10k：用于野外通用目标跟踪的大型高多样性基准。IEEE 模式分析与机器智能学报，2019。'
- en: '[149] Zhewei Huang, Wen Heng, and Shuchang Zhou. Learning to paint with model-based
    deep reinforcement learning. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 8709–8718, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] 黄哲伟、温恒和邹舒畅。基于模型的深度强化学习绘画。见于 IEEE 国际计算机视觉会议论文集，页码 8709–8718, 2019。'
- en: '[150] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for
    sequence tagging. arXiv preprint arXiv:1508.01991, 2015.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] 黄志恒、徐伟和余凯。用于序列标注的双向 LSTM-CRF 模型。arXiv 预印本 arXiv:1508.01991, 2015。'
- en: '[151] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning
    on lie groups for skeleton-based action recognition. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 6099–6108, 2017.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] 黄志武、万成德、托马斯·普罗布斯特和卢克·范·古尔。基于李群的深度学习用于骨架动作识别。见于 IEEE 计算机视觉与模式识别会议论文集，页码
    6099–6108, 2017。'
- en: '[152] Gabriel Efrain Humpire-Mamani, Arnaud Arindra Adiyoso Setio, Bram van
    Ginneken, and Colin Jacobs. Efficient organ localization using multi-label convolutional
    neural networks in thorax-abdomen ct scans. Physics in Medicine & Biology, 63(8):085003,
    2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] 加布里埃尔·埃弗兰·洪皮雷-马马尼、阿尔诺·阿林德拉·阿迪约索·塞蒂奥、布拉姆·范·吉尼肯和科林·雅各布斯。利用多标签卷积神经网络在胸腹部
    CT 扫描中高效定位器官。医学与生物学物理学，63(8):085003, 2018。'
- en: '[153] Luis Ibanez, Will Schroeder, Lydia Ng, and Josh Cates. The itk software
    guide: updated for itk version 2.4, 2005.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] 路易斯·伊班内斯、威尔·施罗德、莉迪亚·吴和乔什·凯茨。ITK 软件指南：更新至 ITK 版本 2.4, 2005。'
- en: '[154] Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. Multi-source
    multi-scale counting in extremely dense crowd images. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 2547–2554, 2013.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] 哈伦·伊德里斯、伊姆兰·萨利米、科迪·塞伯特和穆巴拉克·沙赫。极其密集人群图像中的多源多尺度计数。见于 IEEE 计算机视觉与模式识别会议论文集，页码
    2547–2554, 2013。'
- en: '[155] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed,
    Nasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map estimation
    and localization in dense crowds. In Proceedings of the European Conference on
    Computer Vision (ECCV), pages 532–546, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] 哈伦·伊德里斯、穆罕默德·塔亚布、基尚·阿特雷、董张、索玛雅·阿尔-马阿德德、纳西尔·拉杰普特和穆巴拉克·沙赫。用于密集人群计数、密度图估计和定位的组合损失。见于欧洲计算机视觉会议
    (ECCV) 论文集，页码 532–546, 2018。'
- en: '[156] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy,
    and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2462–2470, 2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] 埃迪·伊尔格、尼古拉斯·迈耶、汤莫伊·赛基亚、玛格丽特·凯普尔、阿列克谢·多索维茨基和托马斯·布罗克。Flownet 2.0：深度网络的光流估计演变。见于
    IEEE 计算机视觉与模式识别会议论文集，页码 2462–2470, 2017。'
- en: '[157] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167,
    2015.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Sergey Ioffe 和 Christian Szegedy。批量归一化：通过减少内部协变量偏移来加速深度网络训练。arXiv预印本arXiv:1502.03167，2015年。'
- en: '[158] Clifford R Jack Jr, Matt A Bernstein, Nick C Fox, Paul Thompson, Gene
    Alexander, Danielle Harvey, Bret Borowski, Paula J Britson, Jennifer L. Whitwell,
    Chadwick Ward, et al. The alzheimer’s disease neuroimaging initiative (adni):
    Mri methods. Journal of Magnetic Resonance Imaging: An Official Journal of the
    International Society for Magnetic Resonance in Medicine, 27(4):685–691, 2008.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Clifford R Jack Jr、Matt A Bernstein、Nick C Fox、Paul Thompson、Gene Alexander、Danielle
    Harvey、Bret Borowski、Paula J Britson、Jennifer L. Whitwell、Chadwick Ward 等。阿尔茨海默病神经影像学计划（ADNI）：MRI方法。《磁共振成像杂志：国际磁共振学会官方期刊》，27(4)：685–691，2008年。'
- en: '[159] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer
    networks. In Advances in neural information processing systems, pages 2017–2025,
    2015.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Max Jaderberg、Karen Simonyan、Andrew Zisserman 等。空间变换网络。发表于神经信息处理系统进展，第2017–2025页，2015年。'
- en: '[160] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Deep features for
    text spotting. In European conference on computer vision, pages 512–528. Springer,
    2014.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Max Jaderberg、Andrea Vedaldi 和 Andrew Zisserman。用于文本检测的深度特征。发表于欧洲计算机视觉会议，第512–528页。Springer，2014年。'
- en: '[161] Arjit Jain, Alexander Powers, and Hans J Johnson. Robust automatic multiple
    landmark detection. In 2020 IEEE 17th International Symposium on Biomedical Imaging
    (ISBI), pages 1178–1182\. IEEE, 2020.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Arjit Jain、Alexander Powers 和 Hans J Johnson。鲁棒的自动多标志检测。发表于2020 IEEE第17届国际生物医学成像研讨会（ISBI），第1178–1182页。IEEE，2020年。'
- en: '[162] Suyog Dutt Jain and Kristen Grauman. Supervoxel-consistent foreground
    propagation in video. In European conference on computer vision, pages 656–671.
    Springer, 2014.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Suyog Dutt Jain 和 Kristen Grauman。视频中的超像素一致前景传播。发表于欧洲计算机视觉会议，第656–671页。Springer，2014年。'
- en: '[163] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusionseg: Learning to
    combine motion and appearance for fully automatic segmentation of generic objects
    in videos. In 2017 IEEE conference on computer vision and pattern recognition
    (CVPR), pages 2117–2126\. IEEE, 2017.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Suyog Dutt Jain、Bo Xiong 和 Kristen Grauman。Fusionseg：学习结合运动和外观以实现视频中通用对象的完全自动分割。发表于2017
    IEEE计算机视觉与模式识别会议（CVPR），第2117–2126页。IEEE，2017年。'
- en: '[164] Varun Jampani, Raghudeep Gadde, and Peter V Gehler. Video propagation
    networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 451–461, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Varun Jampani、Raghudeep Gadde 和 Peter V Gehler。视频传播网络。发表于IEEE计算机视觉与模式识别会议论文集，第451–461页，2017年。'
- en: '[165] Won-Dong Jang and Chang-Su Kim. Online video object segmentation via
    convolutional trident network. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 5849–5858, 2017.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Won-Dong Jang 和 Chang-Su Kim。通过卷积三叉网进行在线视频对象分割。发表于IEEE计算机视觉与模式识别会议论文集，第5849–5858页，2017年。'
- en: '[166] Simon Jégou, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua
    Bengio. The one hundred layers tiramisu: Fully convolutional densenets for semantic
    segmentation. In Proceedings of the IEEE conference on computer vision and pattern
    recognition workshops, pages 11–19, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Simon Jégou、Michal Drozdzal、David Vazquez、Adriana Romero 和 Yoshua Bengio。百层提拉米苏：用于语义分割的全卷积DenseNets。发表于IEEE计算机视觉与模式识别会议研讨会论文集，第11–19页，2017年。'
- en: '[167] Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang. Model-based reinforcement
    learning with value-targeted regression. In Proceedings of the 2nd Conference
    on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning
    Research, pages 666–686, The Cloud, 10–11 Jun 2020.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Zeyu Jia、Lin Yang、Csaba Szepesvari 和 Mengdi Wang。基于模型的强化学习与价值目标回归。发表于第二届动态与控制学习会议论文集，第120卷，机器学习研究论文集，第666–686页，Cloud，2020年6月10–11日。'
- en: '[168] Ming-xin Jiang, Chao Deng, Zhi-geng Pan, Lan-fang Wang, and Xing Sun.
    Multiobject tracking in videos based on lstm and deep reinforcement learning.
    Complexity, 2018, 2018.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Ming-xin Jiang、Chao Deng、Zhi-geng Pan、Lan-fang Wang 和 Xing Sun。基于LSTM和深度强化学习的视频多目标跟踪。复杂性，2018年，2018年。'
- en: '[169] Mingxin Jiang, Tao Hai, Zhigeng Pan, Haiyan Wang, Yinjie Jia, and Chao
    Deng. Multi-agent deep reinforcement learning for multi-object tracker. IEEE Access,
    7:32400–32407, 2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Mingxin Jiang、Tao Hai、Zhigeng Pan、Haiyan Wang、Yinjie Jia 和 Chao Deng。用于多目标跟踪的多智能体深度强化学习。IEEE
    Access，第7卷：32400–32407，2019年。'
- en: '[170] Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Lu, and Shuicheng
    Yan. Tree-structured reinforcement learning for sequential object localization.
    In Advances in Neural Information Processing Systems, pages 127–135, 2016.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Lu 和 Shuicheng
    Yan. 用于顺序物体定位的树状结构强化学习。载于《神经信息处理系统进展》，页码 127–135，2016年。'
- en: '[171] Oscar Jimenez-del Toro, Henning Müller, Markus Krenn, Katharina Gruenberg,
    Abdel Aziz Taha, Marianne Winterstein, Ivan Eggel, Antonio Foncubierta-Rodríguez,
    Orcun Goksel, András Jakab, et al. Cloud-based evaluation of anatomical structure
    segmentation and landmark detection algorithms: Visceral anatomy benchmarks. IEEE
    transactions on medical imaging, 35(11):2459–2475, 2016.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Oscar Jimenez-del Toro, Henning Müller, Markus Krenn, Katharina Gruenberg,
    Abdel Aziz Taha, Marianne Winterstein, Ivan Eggel, Antonio Foncubierta-Rodríguez,
    Orcun Goksel, András Jakab 等。基于云的解剖结构分割和标志物检测算法评估：内脏解剖基准。IEEE 医学影像交易，35(11):2459–2475，2016年。'
- en: '[172] V Craig Jordan. Long-term adjuvant tamoxifen therapy for breast cancer.
    Breast cancer research and treatment, 15(3):125–136, 1990.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] V Craig Jordan. 乳腺癌的长期辅助他莫昔芬治疗。乳腺癌研究与治疗，15(3):125–136，1990年。'
- en: '[173] Yeong Jun Koh and Chang-Su Kim. Primary object segmentation in videos
    based on region augmentation and reduction. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 3442–3450, 2017.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Yeong Jun Koh 和 Chang-Su Kim. 基于区域增强和缩减的视频中主要物体分割。载于 IEEE 计算机视觉与模式识别会议论文集，页码
    3442–3450，2017年。'
- en: '[174] Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas. Tracking-learning-detection.
    IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422,
    2011.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Zdenek Kalal, Krystian Mikolajczyk 和 Jiri Matas. 跟踪-学习-检测。IEEE 模式分析与机器智能交易，34(7):1409–1422，2011年。'
- en: '[175] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models.
    Association for Computational Linguistics, October 2013.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Nal Kalchbrenner 和 Phil Blunsom. 循环连续翻译模型。计算语言学协会，2013年10月。'
- en: '[176] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech
    Jaśkowski. Vizdoom: A doom-based ai research platform for visual reinforcement
    learning. In 2016 IEEE Conference on Computational Intelligence and Games (CIG),
    pages 1–8\. IEEE, 2016.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek 和 Wojciech
    Jaśkowski. Vizdoom：一个基于 Doom 的视觉强化学习 AI 研究平台。载于 2016 IEEE 计算智能与游戏会议 (CIG)，页码 1–8，IEEE，2016年。'
- en: '[177] Du Yong Kim and Moongu Jeon. Data fusion of radar and image measurements
    for multi-object tracking via kalman filtering. Information Sciences, 278:641–652,
    2014.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Du Yong Kim 和 Moongu Jeon. 基于卡尔曼滤波的雷达和图像测量数据融合用于多物体跟踪。信息科学，278:641–652，2014年。'
- en: '[178] Kye Kyung Kim, Soo Hyun Cho, Hae Jin Kim, and Jae Yeon Lee. Detecting
    and tracking moving object using an active camera. In The 7th International Conference
    on Advanced Communication Technology, 2005, ICACT 2005., volume 2, pages 817–820\.
    IEEE, 2005.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Kye Kyung Kim, Soo Hyun Cho, Hae Jin Kim 和 Jae Yeon Lee. 使用主动摄像头检测和跟踪移动物体。载于第七届国际先进通信技术大会，2005年，ICACT
    2005，第 2 卷，页码 817–820，IEEE，2005年。'
- en: '[179] Donna Kirwan. Nhs fetal anomaly screening programme. National Standards
    and Guidance for England, 18(0), 2010.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Donna Kirwan. Nhs 胎儿异常筛查计划。英格兰国家标准和指南，18(0)，2010年。'
- en: '[180] Stefan Klein, Marius Staring, Keelin Murphy, Max A Viergever, and Josien PW
    Pluim. Elastix: a toolbox for intensity-based medical image registration. IEEE
    transactions on medical imaging, 29(1):196–205, 2009.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Stefan Klein, Marius Staring, Keelin Murphy, Max A Viergever 和 Josien
    PW Pluim. Elastix：一个基于强度的医学图像配准工具箱。IEEE 医学影像交易，29(1):196–205，2009年。'
- en: '[181] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning
    in robotics: A survey. The International Journal of Robotics Research, 32(11):1238–1274,
    2013.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Jens Kober, J. Andrew Bagnell 和 Jan Peters. 机器人中的强化学习：综述。《国际机器人研究期刊》，32(11):1238–1274，2013年。'
- en: '[182] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances
    in neural information processing systems, pages 1008–1014, 2000.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Vijay R Konda 和 John N Tsitsiklis. Actor-critic 算法。载于《神经信息处理系统进展》，页码
    1008–1014，2000年。'
- en: '[183] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected
    crfs with gaussian edge potentials. In Advances in neural information processing
    systems, pages 109–117, 2011.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Philipp Krähenbühl 和 Vladlen Koltun. 高斯边缘潜力下全连接 CRF 的高效推断。载于《神经信息处理系统进展》，页码
    109–117，2011年。'
- en: '[184] Julian Krebs, Tommaso Mansi, Hervé Delingette, Li Zhang, Florin C Ghesu,
    Shun Miao, Andreas K Maier, Nicholas Ayache, Rui Liao, and Ali Kamen. Robust non-rigid
    registration through agent-based action learning. In International Conference
    on Medical Image Computing and Computer-Assisted Intervention, pages 344–352\.
    Springer, 2017.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Julian Krebs, Tommaso Mansi, Hervé Delingette, Li Zhang, Florin C Ghesu,
    Shun Miao, Andreas K Maier, Nicholas Ayache, Rui Liao 和 Ali Kamen。通过基于代理的动作学习实现鲁棒的非刚性配准。发表于国际医学图像计算与计算机辅助干预会议，页码344–352。Springer，2017年。'
- en: '[185] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder,
    Luka Cehovin Zajc, Tomas Vojir, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey,
    et al. The sixth visual object tracking vot2018 challenge results. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 0–0, 2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder,
    Luka Cehovin Zajc, Tomas Vojir, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey
    等。第六届视觉目标跟踪VOT2018挑战赛结果。发表于欧洲计算机视觉会议（ECCV）论文集，页码0–0，2018年。'
- en: '[186] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin,
    Gustavo Fernandez, Tomas Vojir, Gustav Hager, Georg Nebehay, and Roman Pflugfelder.
    The visual object tracking vot2015 challenge results. In Proceedings of the IEEE
    international conference on computer vision workshops, pages 1–23, 2015.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin,
    Gustavo Fernandez, Tomas Vojir, Gustav Hager, Georg Nebehay 和 Roman Pflugfelder。视觉目标跟踪VOT2015挑战赛结果。发表于IEEE国际计算机视觉会议论文集，页码1–23，2015年。'
- en: '[187] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems, pages 1097–1105, 2012.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E Hinton。使用深度卷积神经网络进行Imagenet分类。发表于神经信息处理系统进展，页码1097–1105，2012年。'
- en: '[188] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. Communications of the ACM, 60(6):84–90,
    2017.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E Hinton。使用深度卷积神经网络进行Imagenet分类。ACM通讯，60(6):84–90，2017年。'
- en: '[189] A. Kupcsik, M. Deisenroth, Jan Peters, and G. Neumann. Data-efficient
    generalization of robot skills with contextual policy search. In AAAI, 2013.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] A. Kupcsik, M. Deisenroth, Jan Peters 和 G. Neumann。利用上下文策略搜索实现机器人技能的数据高效泛化。发表于AAAI，2013年。'
- en: '[190] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel.
    Model-ensemble trust-region policy optimization. 02 2018.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar 和 Pieter Abbeel。模型集成信任区域策略优化。2018年2月。'
- en: '[191] Ngan Le, Trung Le, Kashu Yamazaki, Toan Duc Bui, Khoa Luu, and Marios
    Savides. Offset curves loss for imbalanced problem in medical segmentation. arXiv
    preprint arXiv:2012.02463, 2020.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Ngan Le, Trung Le, Kashu Yamazaki, Toan Duc Bui, Khoa Luu 和 Marios Savides。医学分割中用于不平衡问题的偏移曲线损失。arXiv预印本
    arXiv:2012.02463，2020年。'
- en: '[192] Ngan Le, Kashu Yamazaki, Dat Truong, Kha Gia Quach, and Marios Savvides.
    A multi-task contextual atrous residual network for brain tumor detection & segmentation.
    arXiv preprint arXiv:2012.02073, 2020.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Ngan Le, Kashu Yamazaki, Dat Truong, Kha Gia Quach 和 Marios Savvides。用于脑肿瘤检测与分割的多任务上下文扩张残差网络。arXiv预印本
    arXiv:2012.02073，2020年。'
- en: '[193] T Hoang Ngan Le, Chi Nhan Duong, Ligong Han, Khoa Luu, Kha Gia Quach,
    and Marios Savvides. Deep contextual recurrent residual networks for scene labeling.
    Pattern Recognition, 80:32–41, 2018.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] T Hoang Ngan Le, Chi Nhan Duong, Ligong Han, Khoa Luu, Kha Gia Quach
    和 Marios Savvides。用于场景标注的深度上下文递归残差网络。模式识别，80:32–41，2018年。'
- en: '[194] T Hoang Ngan Le, Kha Gia Quach, Khoa Luu, Chi Nhan Duong, and Marios
    Savvides. Reformulating level sets as deep recurrent neural network approach to
    semantic segmentation. IEEE Transactions on Image Processing, 27(5):2393–2407,
    2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] T Hoang Ngan Le, Kha Gia Quach, Khoa Luu, Chi Nhan Duong 和 Marios Savvides。将水平集重新表述为深度递归神经网络方法以进行语义分割。IEEE图像处理汇刊，27(5):2393–2407，2018年。'
- en: '[195] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D
    Hager. Temporal convolutional networks for action segmentation and detection.
    In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 156–165, 2017.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter 和 Gregory D Hager。用于动作分割和检测的时间卷积网络。发表于IEEE计算机视觉与模式识别会议论文集，页码156–165，2017年。'
- en: '[196] Colin Lea, Austin Reiter, René Vidal, and Gregory D Hager. Segmental
    spatiotemporal cnns for fine-grained action segmentation. In European Conference
    on Computer Vision, pages 36–52. Springer, 2016.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Colin Lea, Austin Reiter, René Vidal 和 Gregory D Hager。用于细粒度动作分割的分段时空卷积神经网络。发表于欧洲计算机视觉会议，页码36–52。Springer，2016年。'
- en: '[197] Colin Lea, René Vidal, and Gregory D Hager. Learning convolutional action
    primitives for fine-grained action recognition. In 2016 IEEE international conference
    on robotics and automation (ICRA), pages 1642–1649\. IEEE, 2016.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Colin Lea、René Vidal 和 Gregory D Hager。学习卷积动作原语以进行细粒度动作识别。在2016年IEEE国际机器人与自动化会议（ICRA）上，页码1642–1649。IEEE，2016年。'
- en: '[198] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional
    networks: A unified approach to action segmentation. In European Conference on
    Computer Vision, pages 47–54. Springer, 2016.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Colin Lea、Rene Vidal、Austin Reiter 和 Gregory D Hager。时间卷积网络：一种统一的动作分割方法。在欧洲计算机视觉会议上，页码47–54。Springer，2016年。'
- en: '[199] Laura Leal-Taixé, Cristian Canton-Ferrer, and Konrad Schindler. Learning
    by tracking: Siamese cnn for robust target association. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 33–40,
    2016.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Laura Leal-Taixé、Cristian Canton-Ferrer 和 Konrad Schindler。通过跟踪学习：用于稳健目标关联的孪生CNN。在IEEE计算机视觉与模式识别会议论文集的研讨会上，页码33–40，2016年。'
- en: '[200] Laura Leal-Taixé, Michele Fenzi, Alina Kuznetsova, Bodo Rosenhahn, and
    Silvio Savarese. Learning an image-based motion context for multiple people tracking.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 3542–3549, 2014.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Laura Leal-Taixé、Michele Fenzi、Alina Kuznetsova、Bodo Rosenhahn 和 Silvio
    Savarese。学习基于图像的运动上下文以进行多人物跟踪。在IEEE计算机视觉与模式识别会议的论文集中，页码3542–3549，2014年。'
- en: '[201] Laura Leal-Taixé, Anton Milan, Ian Reid, Stefan Roth, and Konrad Schindler.
    Motchallenge 2015: Towards a benchmark for multi-target tracking. arXiv preprint
    arXiv:1504.01942, 2015.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Laura Leal-Taixé、Anton Milan、Ian Reid、Stefan Roth 和 Konrad Schindler。Motchallenge
    2015：迈向多目标跟踪的基准。arXiv预印本 arXiv:1504.01942，2015年。'
- en: '[202] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
    com/exdb/mnist/, 1998.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Yann LeCun。手写数字的MNIST数据库。http://yann.lecun.com/exdb/mnist/，1998年。'
- en: '[203] Yann LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efficient
    backprop. In Neural networks: Tricks of the trade, pages 9–50\. Springer, 1998.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Yann LeCun、Léon Bottou、Genevieve B Orr 和 Klaus-Robert Müller。高效反向传播。在《神经网络：技巧与窍门》中，页码9–50。Springer，1998年。'
- en: '[204] Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework
    for back-propagation. In Proceedings of the 1988 connectionist models summer school,
    pages 21–28\. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Yann LeCun、D Touresky、G Hinton 和 T Sejnowski。反向传播的理论框架。在1988年连接主义模型暑期学校论文集中，页码21–28。CMU，匹兹堡，Pa：Morgan
    Kaufmann，1988年。'
- en: '[205] Hyunjae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm: A style-based recalibration
    module for convolutional neural networks. pages 1854–1862, 10 2019.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Hyunjae Lee、Hyo-Eun Kim 和 Hyeonseob Nam。SRM：一种基于风格的卷积神经网络重校准模块。页码1854–1862，2019年10月。'
- en: '[206] Jae Won Lee, Jonghun Park, Jangmin O, Jongwoo Lee, and Euyseok Hong.
    A multiagent approach to q-learning for daily stock trading. Trans. Sys. Man Cyber.
    Part A, 37(6):864–877, November 2007.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Jae Won Lee、Jonghun Park、Jangmin O、Jongwoo Lee 和 Euyseok Hong。日常股票交易的多智能体Q学习方法。Trans.
    Sys. Man Cyber. A部分，37(6)：864–877，2007年11月。'
- en: '[207] Joel Z. Leibo, Vinícius Flores Zambaldi, Marc Lanctot, Janusz Marecki,
    and Thore Graepel. Multi-agent reinforcement learning in sequential social dilemmas.
    CoRR, abs/1702.03037, 2017.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Joel Z. Leibo、Vinícius Flores Zambaldi、Marc Lanctot、Janusz Marecki 和
    Thore Graepel。序列社会困境中的多智能体强化学习。CoRR，abs/1702.03037，2017年。'
- en: '[208] Sergey Levine and Vladlen Koltun. Learning complex neural network policies
    with trajectory optimization. In Proceedings of the 31st International Conference
    on Machine Learning, pages 829–837, 2014.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Sergey Levine 和 Vladlen Koltun。通过轨迹优化学习复杂的神经网络策略。在第31届国际机器学习会议论文集中，页码829–837，2014年。'
- en: '[209] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance
    visual tracking with siamese region proposal network. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 8971–8980, 2018.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Bo Li、Junjie Yan、Wei Wu、Zheng Zhu 和 Xiaolin Hu。高性能视觉跟踪与孪生区域提议网络。在IEEE计算机视觉与模式识别会议论文集，页码8971–8980，2018年。'
- en: '[210] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiaogang Wang. GS3D:
    an efficient 3d object detection framework for autonomous driving. In IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
    16-20, 2019, pages 1019–1028. Computer Vision Foundation / IEEE, 2019.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Buyu Li、Wanli Ouyang、Lu Sheng、Xingyu Zeng 和 Xiaogang Wang。GS3D：用于自动驾驶的高效3D物体检测框架。在IEEE计算机视觉与模式识别会议（CVPR
    2019），洛杉矶，加州，美国，2019年6月16-20日，页码1019–1028。计算机视觉基金会/IEEE，2019年。'
- en: '[211] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu. Co-occurrence feature
    learning from skeleton data for action recognition and detection with hierarchical
    aggregation. arXiv preprint arXiv:1804.06055, 2018.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Chao Li、Qiaoyong Zhong、Di Xie 和 Shiliang Pu。基于骨架数据的共现特征学习，用于动作识别与检测的层次聚合。arXiv预印本
    arXiv:1804.06055，2018年。'
- en: '[212] Duo Li and Qifeng Chen. Deep reinforced attention learning for quality-aware
    visual recognition. In European Conference on Computer Vision, pages 493–509,
    2020.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Duo Li 和 Qifeng Chen。用于质量感知视觉识别的深度强化注意力学习。在欧洲计算机视觉会议论文集中，第493–509页，2020年。'
- en: '[213] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features.
    arXiv preprint arXiv:1503.08663, 2015.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Guanbin Li 和 Yizhou Yu。基于多尺度深度特征的视觉显著性。arXiv预印本 arXiv:1503.08663，2015年。'
- en: '[214] Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder
    for paragraphs and documents. CoRR, abs/1506.01057, 2015.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Jiwei Li、Minh-Thang Luong 和 Dan Jurafsky。用于段落和文档的层次神经自编码器。CoRR，abs/1506.01057，2015年。'
- en: '[215] K. Li, M. Rath, and J. W. Burdick. Inverse reinforcement learning via
    function approximation for clinical motion analysis. In 2018 IEEE International
    Conference on Robotics and Automation (ICRA), pages 610–617, 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] K. Li、M. Rath 和 J. W. Burdick。通过函数近似进行逆强化学习用于临床运动分析。在2018年IEEE国际机器人与自动化会议（ICRA）论文集中，第610–617页，2018年。'
- en: '[216] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit
    approach to personalized news article recommendation. In Proceedings of the 19th
    international conference on World wide web, pages 661–670, 2010.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Lihong Li、Wei Chu、John Langford 和 Robert E Schapire。基于上下文的个性化新闻推荐方法。在第19届国际万维网大会论文集中，第661–670页，2010年。'
- en: '[217] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep visual tracking:
    Review and experimental comparison. Pattern Recognition, 76:323–338, 2018.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Peixia Li、Dong Wang、Lijun Wang 和 Huchuan Lu。深度视觉跟踪：综述与实验比较。模式识别，76：323–338，2018年。'
- en: '[218] Yingbo Li and Bernard Merialdo. Multi-video summarization based on video-mmr.
    In 11th International Workshop on Image Analysis for Multimedia Interactive Services
    WIAMIS 10, pages 1–4\. IEEE, 2010.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Yingbo Li 和 Bernard Merialdo。基于视频-MMR的多视频摘要。在第11届多媒体互动服务图像分析国际研讨会WIAMIS
    10中，第1–4页。IEEE，2010年。'
- en: '[219] Yuanwei Li, Amir Alansary, Juan J Cerrolaza, Bishesh Khanal, Matthew
    Sinclair, Jacqueline Matthew, Chandni Gupta, Caroline Knight, Bernhard Kainz,
    and Daniel Rueckert. Fast multiple landmark localisation using a patch-based iterative
    network. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 563–571\. Springer, 2018.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Yuanwei Li、Amir Alansary、Juan J Cerrolaza、Bishesh Khanal、Matthew Sinclair、Jacqueline
    Matthew、Chandni Gupta、Caroline Knight、Bernhard Kainz 和 Daniel Rueckert。使用基于补丁的迭代网络的快速多标记点定位。在医学图像计算与计算机辅助干预国际会议论文集中，第563–571页。Springer，2018年。'
- en: '[220] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding color information
    for visual tracking: Algorithms and benchmark. IEEE Transactions on Image Processing,
    24(12):5630–5644, 2015.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Pengpeng Liang、Erik Blasch 和 Haibin Ling。用于视觉跟踪的颜色信息编码：算法与基准。IEEE图像处理汇刊，24(12)：5630–5644，2015年。'
- en: '[221] Rui Liao, Shun Miao, Pierre de Tournemire, Sasa Grbic, Ali Kamen, Tommaso
    Mansi, and Dorin Comaniciu. An artificial agent for robust image registration.
    In Thirty-First AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Rui Liao、Shun Miao、Pierre de Tournemire、Sasa Grbic、Ali Kamen、Tommaso
    Mansi 和 Dorin Comaniciu。用于稳健图像配准的人工智能代理。在第三十一届AAAI人工智能会议，2017年。'
- en: '[222] Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang,
    Yanfeng Wang, and Ya Zhang. Iteratively-refined interactive 3d medical image segmentation
    with multi-agent reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 9394–9402, 2020.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Xuan Liao、Wenhao Li、Qisen Xu、Xiangfeng Wang、Bo Jin、Xiaoyun Zhang、Yanfeng
    Wang 和 Ya Zhang。基于多智能体强化学习的迭代优化交互式3D医学图像分割。在IEEE/CVF计算机视觉与模式识别会议论文集中，第9394–9402页，2020年。'
- en: '[223] Timothy P. Lillicrap, Jonathan J. Hunt, Alexand er Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv e-prints, page arXiv:1509.02971, September
    2015.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Timothy P. Lillicrap、Jonathan J. Hunt、Alexander Pritzel、Nicolas Heess、Tom
    Erez、Yuval Tassa、David Silver 和 Daan Wierstra。基于深度强化学习的连续控制。arXiv电子版，第 arXiv:1509.02971
    页，2015年9月。'
- en: '[224] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] 提摩太·P·利利克拉普，乔纳森·J·亨特，亚历山大·普里策尔，尼古拉斯·赫斯，汤姆·埃雷兹，尤瓦尔·塔萨，大卫·银，和丹·维尔斯特拉。使用深度强化学习进行连续控制。arXiv预印本arXiv:1509.02971，2015。'
- en: '[225] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
    Focal loss for dense object detection. In Proceedings of the IEEE international
    conference on computer vision, pages 2980–2988, 2017.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] 林宗义，普莉雅·戈亚尔，罗斯·吉什克，凯明·何，和皮奥特·多拉尔。用于密集物体检测的焦点损失。见于IEEE国际计算机视觉会议论文集，第2980–2988页，2017。'
- en: '[226] Tony Lindeberg. Scale-space theory in computer vision, volume 256. Springer
    Science & Business Media, 2013.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] 托尼·林德伯格。计算机视觉中的尺度空间理论，第256卷。施普林格科学与商业媒体，2013。'
- en: '[227] Geert Litjens, Robert Toth, Wendy van de Ven, Caroline Hoeks, Sjoerd
    Kerkstra, Bram van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang
    Zhang, et al. Evaluation of prostate segmentation algorithms for mri: the promise12
    challenge. Medical image analysis, 18(2):359–373, 2014.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] 吉尔特·利特金斯，罗伯特·托斯，温迪·范·德·文，卡罗琳·霍克斯，肖尔德·克克斯特拉，布拉姆·范·吉内肯，格雷厄姆·文森特，格温埃尔·吉亚尔德，尼尔·比贝克，金当张等。前列腺分割算法在MRI中的评估：PROMISE12挑战。医学影像分析，第18卷第2期：359–373，2014。'
- en: '[228] Daochang Liu and Tingting Jiang. Deep reinforcement learning for surgical
    gesture segmentation and classification. In International conference on medical
    image computing and computer-assisted intervention, pages 247–255\. Springer,
    2018.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] 道昌·刘和婷婷·姜。用于手术手势分割和分类的深度强化学习。见于国际医学影像计算与计算机辅助干预会议论文集，第247–255页。施普林格，2018。'
- en: '[229] Hao Liu, Richard Socher, and Caiming Xiong. Taming maml: Efficient unbiased
    meta-reinforcement learning. In International Conference on Machine Learning,
    pages 4061–4071, 2019.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] 郝刘，理查德·索彻，和蔡铭·熊。驯服MAML：高效无偏的元强化学习。见于国际机器学习会议论文集，第4061–4071页，2019。'
- en: '[230] Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua
    Shen. Weighing counts: Sequential crowd counting by reinforcement learning. 2020.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] 梁刘，郝陆，洪伟邹，海鹏熊，志国曹，和春华申。称重计数：通过强化学习进行的序列人群计数。2020。'
- en: '[231] Lijie Liu, Chufan Wu, Jiwen Lu, Lingxi Xie, Jie Zhou, and Qi Tian. Reinforced
    axial refinement network for monocular 3d object detection. In European Conference
    on Computer Vision ECCV, pages 540–556, 2020.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] 李杰·刘，楚凡·吴，纪文·卢，凌熙·谢，杰·周，和齐天。用于单目3D物体检测的强化轴向细化网络。见于欧洲计算机视觉会议ECCV论文集，第540–556页，2020。'
- en: '[232] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and Liang Lin. Crowd
    counting using deep recurrent spatial-aware network. arXiv preprint arXiv:1807.00601,
    2018.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] 凌博·刘，洪军·王，关斌·李，万里·欧阳，和梁林。使用深度递归空间感知网络进行人群计数。arXiv预印本arXiv:1807.00601，2018。'
- en: '[233] Tianrui Liu, Qingjie Meng, Athanasios Vlontzos, Jeremy Tan, Daniel Rueckert,
    and Bernhard Kainz. Ultrasound video summarization using deep reinforcement learning.
    arXiv preprint arXiv:2005.09531, 2020.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] 天睿·刘，庆杰·孟，阿萨纳修斯·弗伦佐斯，杰里米·谭，丹尼尔·鲁克特，和伯恩哈德·凯因茨。使用深度强化学习进行超声视频摘要。arXiv预印本arXiv:2005.09531，2020。'
- en: '[234] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5099–5108, 2019.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] 魏哲·刘，马修·萨尔茨曼，和帕斯卡尔·富阿。上下文感知的人群计数。见于IEEE计算机视觉与模式识别会议论文集，第5099–5108页，2019。'
- en: '[235] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face
    attributes in the wild. In Proceedings of the IEEE international conference on
    computer vision, pages 3730–3738, 2015.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] 自伟·刘，平·罗，肖刚·王，和肖欧·唐。在自然环境下进行深度学习面部属性分析。见于IEEE国际计算机视觉会议论文集，第3730–3738页，2015。'
- en: '[236] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] 乔纳森·隆，埃文·谢尔哈默，和特雷弗·达雷尔。用于语义分割的全卷积网络。见于IEEE计算机视觉与模式识别会议论文集，第3431–3440页，2015。'
- en: '[237] Marco Lorenzi, Nicholas Ayache, Giovanni B Frisoni, Xavier Pennec, Alzheimer’s
    Disease Neuroimaging Initiative (ADNI, et al. Lcc-demons: a robust and accurate
    symmetric diffeomorphic registration algorithm. NeuroImage, 81:470–483, 2013.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] 马尔科·洛伦齐，尼古拉斯·阿亚切，乔瓦尼·B·弗里索尼，泽维尔·佩内克，阿尔茨海默病神经影像倡议（ADNI）等。Lcc-demons：一种稳健而准确的对称差分配准算法。神经影像，第81卷：470–483，2013。'
- en: '[238] Tayebeh Lotfi, Lisa Tang, Shawn Andrews, and Ghassan Hamarneh. Improving
    probabilistic image registration via reinforcement learning and uncertainty evaluation.
    In International Workshop on Machine Learning in Medical Imaging, pages 187–194\.
    Springer, 2013.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Tayebeh Lotfi、Lisa Tang、Shawn Andrews 和 Ghassan Hamarneh. 通过强化学习和不确定性评估改进概率图像配准。在《医学影像中的机器学习国际研讨会》上，页面
    187–194\. Springer，2013年。'
- en: '[239] David G Lowe. Distinctive image features from scale-invariant keypoints.
    International journal of computer vision, 60(2):91–110, 2004.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] David G Lowe. 从尺度不变关键点提取独特的图像特征。《国际计算机视觉杂志》，60(2)：91–110，2004年。'
- en: '[240] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, and Yizhou
    Wang. End-to-end active object tracking via reinforcement learning. arXiv preprint
    arXiv:1705.10561, 2017.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Wenhan Luo、Peng Sun、Fangwei Zhong、Wei Liu、Tong Zhang 和 Yizhou Wang. 通过强化学习进行端到端的主动对象跟踪。arXiv
    预印本 arXiv:1705.10561，2017年。'
- en: '[241] Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech
    Zaremba. Addressing the rare word problem in neural machine translation. CoRR,
    abs/1410.8206, 2014.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Thang Luong、Ilya Sutskever、Quoc V. Le、Oriol Vinyals 和 Wojciech Zaremba.
    解决神经机器翻译中的稀有词问题。CoRR，abs/1410.8206，2014年。'
- en: '[242] Khoa Luu, Chenchen Zhu, Chandrasekhar Bhagavatula, T Hoang Ngan Le, and
    Marios Savvides. A deep learning approach to joint face detection and segmentation.
    In Advances in Face Detection and Facial Image Analysis, pages 1–12\. Springer,
    2016.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] Khoa Luu、Chenchen Zhu、Chandrasekhar Bhagavatula、T Hoang Ngan Le 和 Marios
    Savvides. 一种深度学习方法用于联合面部检测和分割。在《面部检测与面部图像分析进展》一书中，页面 1–12\. Springer，2016年。'
- en: '[243] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang. Hierarchical
    convolutional features for visual tracking. In Proceedings of the IEEE international
    conference on computer vision, pages 3074–3082, 2015.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Chao Ma、Jia-Bin Huang、Xiaokang Yang 和 Ming-Hsuan Yang. 用于视觉跟踪的层次卷积特征。在《IEEE国际计算机视觉会议论文集》上，页面
    3074–3082，2015年。'
- en: '[244] Kai Ma, Jiangping Wang, Vivek Singh, Birgi Tamersoy, Yao-Jen Chang, Andreas
    Wimmer, and Terrence Chen. Multimodal image registration with deep context reinforcement
    learning. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 240–248\. Springer, 2017.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Kai Ma、Jiangping Wang、Vivek Singh、Birgi Tamersoy、Yao-Jen Chang、Andreas
    Wimmer 和 Terrence Chen. 使用深度上下文强化学习进行多模态图像配准。在《医学图像计算与计算机辅助干预国际会议》上，页面 240–248\.
    Springer，2017年。'
- en: '[245] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video
    summarization with adversarial lstm networks. In Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition, pages 202–211, 2017.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] Behrooz Mahasseni、Michael Lam 和 Sinisa Todorovic. 使用对抗性 LSTM 网络进行无监督视频摘要。在《IEEE计算机视觉与模式识别会议论文集》上，页面
    202–211，2017年。'
- en: '[246] Gabriel Maicas, Gustavo Carneiro, Andrew P Bradley, Jacinto C Nascimento,
    and Ian Reid. Deep reinforcement learning for active breast lesion detection from
    dce-mri. In International conference on medical image computing and computer-assisted
    intervention, pages 665–673\. Springer, 2017.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] Gabriel Maicas、Gustavo Carneiro、Andrew P Bradley、Jacinto C Nascimento
    和 Ian Reid. 用于主动乳腺病变检测的深度强化学习。 在《医学图像计算与计算机辅助干预国际会议》上，页面 665–673\. Springer，2017年。'
- en: '[247] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille. Deep captioning
    with multimodal recurrent neural networks (m-rnn). CoRR, abs/1412.6632, 2014.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] Junhua Mao、Wei Xu、Yi Yang、Jiang Wang 和 Alan L. Yuille. 使用多模态递归神经网络 (m-rnn)
    进行深度图像描述。CoRR，abs/1412.6632，2014年。'
- en: '[248] Nicolas Märki, Federico Perazzi, Oliver Wang, and Alexander Sorkine-Hornung.
    Bilateral space video segmentation. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 743–751, 2016.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] Nicolas Märki、Federico Perazzi、Oliver Wang 和 Alexander Sorkine-Hornung.
    双边空间视频分割。在《IEEE计算机视觉与模式识别会议论文集》上，页面 743–751，2016年。'
- en: '[249] T. Martinez-Marin and T. Duckett. Fast reinforcement learning for vision-guided
    mobile robots. In Proceedings of the 2005 IEEE International Conference on Robotics
    and Automation, pages 4170–4175, 2005.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] T. Martinez-Marin 和 T. Duckett. 用于视觉引导移动机器人的快速强化学习。在 2005 年 IEEE 国际机器人与自动化会议论文集上，页面
    4170–4175，2005年。'
- en: '[250] Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforcement
    learning for deformable object manipulation. arXiv preprint arXiv:1806.07851,
    2018.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] Jan Matas、Stephen James 和 Andrew J Davison. 用于可变形物体操作的模拟到现实强化学习。arXiv
    预印本 arXiv:1806.07851，2018年。'
- en: '[251] Stefan Mathe, Aleksis Pirinen, and Cristian Sminchisescu. Reinforcement
    learning for visual object detection. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 2894–2902, 2016.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] Stefan Mathe, Aleksis Pirinen, 和 Cristian Sminchisescu。用于视觉物体检测的强化学习。在
    IEEE 计算机视觉与模式识别会议论文集中，页面 2894–2902，2016年。'
- en: '[252] George K Matsopoulos, Nicolaos A Mouravliansky, Konstantinos K Delibasis,
    and Konstantina S Nikita. Automatic retinal image registration scheme using global
    optimization techniques. IEEE Transactions on Information Technology in Biomedicine,
    3(1):47–60, 1999.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] George K Matsopoulos, Nicolaos A Mouravliansky, Konstantinos K Delibasis,
    和 Konstantina S Nikita。使用全局优化技术的自动视网膜图像配准方案。《IEEE 生物医学信息技术汇刊》，3(1)：47–60，1999年。'
- en: '[253] Darryl McClymont, Andrew Mehnert, Adnan Trakic, Dominic Kennedy, and
    Stuart Crozier. Fully automatic lesion segmentation in breast mri using mean-shift
    and graph-cuts on a region adjacency graph. Journal of Magnetic Resonance Imaging,
    39(4):795–804, 2014.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] Darryl McClymont, Andrew Mehnert, Adnan Trakic, Dominic Kennedy, 和 Stuart
    Crozier。使用均值漂移和图割在区域邻接图上进行乳腺 MRI 的完全自动化病变分割。《磁共振成像杂志》，39(4)：795–804，2014年。'
- en: '[254] Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer,
    Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom,
    Roland Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats).
    IEEE transactions on medical imaging, 34(10):1993–2024, 2014.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer,
    Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom,
    Roland Wiest 等人。多模态脑肿瘤图像分割基准（BRATS）。《IEEE 医学成像汇刊》，34(10)：1993–2024，2014年。'
- en: '[255] Shun Miao, Rui Liao, Marcus Pfister, Li Zhang, and Vincent Ordy. System
    and method for 3-d/3-d registration between non-contrast-enhanced cbct and contrast-enhanced
    ct for abdominal aortic aneurysm stenting. In International Conference on Medical
    Image Computing and Computer-Assisted Intervention, pages 380–387\. Springer,
    2013.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] Shun Miao, Rui Liao, Marcus Pfister, Li Zhang, 和 Vincent Ordy。用于非对比增强
    CBCT 和对比增强 CT 之间 3D/3D 配准的系统和方法，用于腹主动脉瘤支架置入。在国际医学图像计算与计算机辅助干预会议上，页面 380–387。Springer，2013年。'
- en: '[256] Shun Miao, Z Jane Wang, and Rui Liao. A cnn regression approach for real-time
    2d/3d registration. IEEE transactions on medical imaging, 35(5):1352–1363, 2016.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] Shun Miao, Z Jane Wang, 和 Rui Liao。一种用于实时 2D/3D 配准的 CNN 回归方法。《IEEE 医学成像汇刊》，35(5)：1352–1363，2016年。'
- en: '[257] Tomas Mikolov, Stefan Kombrink, Lukás Burget, Jan Cernocký, and Sanjeev
    Khudanpur. Extensions of recurrent neural network language model. In ICASSP, pages
    5528–5531, 2011.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Tomas Mikolov, Stefan Kombrink, Lukás Burget, Jan Cernocký, 和 Sanjeev
    Khudanpur。递归神经网络语言模型的扩展。在 ICASSP 会议上，页面 5528–5531，2011年。'
- en: '[258] Anton Milan, Laura Leal-Taixé, Ian Reid, Stefan Roth, and Konrad Schindler.
    Mot16: A benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831,
    2016.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] Anton Milan, Laura Leal-Taixé, Ian Reid, Stefan Roth, 和 Konrad Schindler。Mot16：多目标跟踪基准。arXiv
    预印本 arXiv:1603.00831，2016年。'
- en: '[259] Anton Milan, Laura Leal-Taixé, Konrad Schindler, and Ian Reid. Joint
    tracking and segmentation of multiple targets. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 5397–5406, 2015.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] Anton Milan, Laura Leal-Taixé, Konrad Schindler, 和 Ian Reid。多个目标的联合跟踪与分割。在
    IEEE 计算机视觉与模式识别会议论文集中，页面 5397–5406，2015年。'
- en: '[260] Anton Milan, S Hamid Rezatofighi, Anthony Dick, Ian Reid, and Konrad
    Schindler. Online multi-target tracking using recurrent neural networks. In Thirty-First
    AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] Anton Milan, S Hamid Rezatofighi, Anthony Dick, Ian Reid, 和 Konrad Schindler。使用递归神经网络的在线多目标跟踪。在第31届
    AAAI 人工智能大会上，2017年。'
- en: '[261] Shervin Minaee, AmirAli Abdolrashidi, Hang Su, Mohammed Bennamoun, and
    David Zhang. Biometric recognition using deep learning: A survey. CoRR, abs/1912.00271,
    2019.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] Shervin Minaee, AmirAli Abdolrashidi, Hang Su, Mohammed Bennamoun, 和
    David Zhang。基于深度学习的生物识别识别：综述。CoRR，abs/1912.00271，2019年。'
- en: '[262] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937, 2016.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, 和 Koray Kavukcuoglu。深度强化学习的异步方法。在国际机器学习会议上，页面
    1928–1937，2016年。'
- en: '[263] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In Proceedings of The 33rd International Conference
    on Machine Learning, pages 1928–1937, 20–22 Jun 2016.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, 和 Koray Kavukcuoglu. 用于深度强化学习的异步方法. 见于第33届国际机器学习大会论文集，页码1928–1937，2016年6月20–22日。'
- en: '[264] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel
    Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
    Georg Ostrovski, et al. Human-level control through deep reinforcement learning.
    Nature, 518(7540):529–533, 2015.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel
    Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
    Georg Ostrovski 等. 通过深度强化学习实现人类水平的控制. 《自然》，518(7540):529–533，2015年。'
- en: '[265] I. Mordatch, N. Mishra, C. Eppner, and P. Abbeel. Combining model-based
    policy search with online model learning for control of physical humanoids. In
    2016 IEEE International Conference on Robotics and Automation (ICRA), pages 242–248,
    2016.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] I. Mordatch, N. Mishra, C. Eppner, 和 P. Abbeel. 将基于模型的策略搜索与在线模型学习结合用于物理类人机器人控制.
    见于2016 IEEE国际机器人与自动化会议（ICRA），页码242–248，2016年。'
- en: '[266] J. Morimoto, G. Zeglin, and C. G. Atkeson. Minimax differential dynamic
    programming: application to a biped walking robot. In Proceedings 2003 IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),
    volume 2, pages 1927–1932, 2003.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] J. Morimoto, G. Zeglin, 和 C. G. Atkeson. 极小极大微分动态规划：应用于双足行走机器人. 见于2003
    IEEE/RSJ国际智能机器人与系统会议（IROS 2003）论文集（Cat. No.03CH37453），第2卷，页码1927–1932，2003年。'
- en: '[267] Jun Morimoto and Christopher G. Atkeson. Nonparametric representation
    of an approximated poincaré map for learning biped locomotion. In Autonomous Robots,
    page 131–144, 2009.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] Jun Morimoto 和 Christopher G. Atkeson. 用于学习双足行走的近似庞加莱映射的非参数表示. 见于《自主机器人》，页码131–144，2009年。'
- en: '[268] A. Mousavian, D. Anguelov, J. Flynn, and J. Košecká. 3d bounding box
    estimation using deep learning and geometry. In 2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 5632–5640, 2017.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] A. Mousavian, D. Anguelov, J. Flynn, 和 J. Košecká. 使用深度学习和几何进行3D边界框估计.
    见于2017 IEEE计算机视觉与模式识别会议（CVPR），页码5632–5640，2017年。'
- en: '[269] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator
    for uav tracking. In European conference on computer vision, pages 445–461. Springer,
    2016.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] Matthias Mueller, Neil Smith, 和 Bernard Ghanem. 无人机跟踪的基准和模拟器. 见于欧洲计算机视觉会议，页码445–461.
    Springer, 2016年。'
- en: '[270] Don Murray and Anup Basu. Motion tracking with an active camera. IEEE
    transactions on pattern analysis and machine intelligence, 16(5):449–459, 1994.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] Don Murray 和 Anup Basu. 使用主动相机进行运动跟踪. IEEE模式分析与机器智能交易，16(5):449–459，1994年。'
- en: '[271] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter
    Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world
    environments through meta-reinforcement learning. arXiv preprint arXiv:1803.11347,
    2018.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter
    Abbeel, Sergey Levine, 和 Chelsea Finn. 通过元强化学习在动态真实环境中学习适应. arXiv预印本 arXiv:1803.11347，2018年。'
- en: '[272] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming
    exploration in reinforcement learning with demonstrations. In 2018 IEEE International
    Conference on Robotics and Automation (ICRA), pages 6292–6299, 2018.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, 和 P. Abbeel. 通过演示克服强化学习中的探索问题.
    见于2018 IEEE国际机器人与自动化会议（ICRA），页码6292–6299，2018年。'
- en: '[273] Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural
    networks for visual tracking. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 4293–4302, 2016.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] Hyeonseob Nam 和 Bohyung Han. 学习多领域卷积神经网络用于视觉跟踪. 见于IEEE计算机视觉与模式识别会议论文集，页码4293–4302，2016年。'
- en: '[274] Ali Bou Nassif, Ismail Shahin, Imtinan Attili, Mohammad Azzeh, and Khaled
    Shaalan. Speech recognition using deep neural networks: A systematic review. IEEE
    Access, 7:19143–19165, 2019.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] Ali Bou Nassif, Ismail Shahin, Imtinan Attili, Mohammad Azzeh, 和 Khaled
    Shaalan. 使用深度神经网络进行语音识别：系统评价. IEEE Access, 7:19143–19165, 2019年。'
- en: '[275] Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, Jan C Peeken,
    Stephanie E Combs, and Bjoern H Menze. Deep reinforcement learning for organ localization
    in ct. arXiv preprint arXiv:2005.04974, 2020.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, Jan C Peeken,
    Stephanie E Combs, 和 Bjoern H Menze. 用于CT中器官定位的深度强化学习. arXiv预印本 arXiv:2005.04974，2020年。'
- en: '[276] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and
    Andrew Y Ng. Reading digits in natural images with unsupervised feature learning.
    2011.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu 和 Andrew
    Y Ng。使用无监督特征学习阅读自然图像中的数字。2011年。'
- en: '[277] Dominik Neumann, Saša Grbić, Matthias John, Nassir Navab, Joachim Hornegger,
    and Razvan Ionasec. Probabilistic sparse matching for robust 3d/3d fusion in minimally
    invasive surgery. IEEE transactions on medical imaging, 34(1):49–60, 2014.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] Dominik Neumann, Saša Grbić, Matthias John, Nassir Navab, Joachim Hornegger
    和 Razvan Ionasec。用于稳健3D/3D融合的概率稀疏匹配，在最小侵入性手术中。IEEE医学成像交易，34(1):49–60，2014年。'
- en: '[278] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement
    learning. In Proceedings of the Seventeenth International Conference on Machine
    Learning, ICML ’00, page 663–670, San Francisco, CA, USA, 2000. Morgan Kaufmann
    Publishers Inc.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] Andrew Y. Ng 和 Stuart J. Russell。逆向强化学习的算法。发表于第十七届国际机器学习大会，ICML ’00，第663–670页，美国加利福尼亚州旧金山，2000年。摩根考夫曼出版社。'
- en: '[279] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement
    learning. In Icml, volume 1, page 2, 2000.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] Andrew Y Ng, Stuart J Russell 等。逆向强化学习的算法。发表于Icml，第1卷，第2页，2000年。'
- en: '[280] Trung Thanh Nguyen, Zhuoru Li, Tomi Silander, and Tze-Yun Leong. Online
    feature selection for model-based reinforcement learning. In Proceedings of the
    30th International Conference on International Conference on Machine Learning
    - Volume 28, page I–498–I–506, 2013.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] Trung Thanh Nguyen, Zhuoru Li, Tomi Silander 和 Tze-Yun Leong. 基于模型的强化学习的在线特征选择。发表于第30届国际机器学习大会论文集
    - 卷28，第I–498–I–506页，2013年。'
- en: '[281] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, Ngan Le, and Marios Savvides.
    Temporal non-volume preserving approach to facial age-progression and age-invariant
    face recognition. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 3735–3743, 2017.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, Ngan Le 和 Marios Savvides。面部年龄进展和年龄不变面部识别的时间非体积保持方法。发表于IEEE国际计算机视觉会议，页码3735–3743，2017年。'
- en: '[282] C. Niedzwiedz, I. Elhanany, Zhenzhen Liu, and S. Livingston. A consolidated
    actor-critic model with function approximation for high-dimensional pomdps. In
    AAAI 2008Workshop for Advancement in POMDP Solvers, 2008.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] C. Niedzwiedz, I. Elhanany, Zhenzhen Liu 和 S. Livingston。用于高维POMDPs的综合演员-评论家模型与函数逼近。发表于AAAI
    2008POMDP求解器进展研讨会，2008年。'
- en: '[283] Yishuang Ning, Sheng He, Zhiyong Wu, Chunxiao Xing, and Liang-Jie Zhang.
    A review of deep learning based speech synthesis. Applied Sciences, 9(19), 2019.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] Yishuang Ning, Sheng He, Zhiyong Wu, Chunxiao Xing 和 Liang-Jie Zhang。基于深度学习的语音合成综述。应用科学，9(19)，2019年。'
- en: '[284] Kenji Okuma, Ali Taleghani, Nando De Freitas, James J Little, and David G
    Lowe. A boosted particle filter: Multitarget detection and tracking. In European
    conference on computer vision, pages 28–39. Springer, 2004.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] Kenji Okuma, Ali Taleghani, Nando De Freitas, James J Little 和 David
    G Lowe。增强型粒子滤波器：多目标检测与跟踪。发表于欧洲计算机视觉会议，页码28–39。施普林格，2004年。'
- en: '[285] José Ignacio Orlando, Huazhu Fu, João Barbosa Breda, Karel van Keer,
    Deepti R Bathula, Andrés Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim,
    JoonHo Lee, et al. Refuge challenge: A unified framework for evaluating automated
    methods for glaucoma assessment from fundus photographs. Medical image analysis,
    59:101570, 2020.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] José Ignacio Orlando, Huazhu Fu, João Barbosa Breda, Karel van Keer,
    Deepti R Bathula, Andrés Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim,
    JoonHo Lee 等人。难治性青光眼挑战：评估自动化青光眼评估方法的统一框架。医学图像分析，59:101570，2020年。'
- en: '[286] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters.
    2018.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel 和 J. Peters。2018年。'
- en: '[287] Rameswar Panda, Abir Das, Ziyan Wu, Jan Ernst, and Amit K Roy-Chowdhury.
    Weakly supervised summarization of web videos. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 3657–3666, 2017.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] Rameswar Panda, Abir Das, Ziyan Wu, Jan Ernst 和 Amit K Roy-Chowdhury。弱监督的网络视频摘要。发表于IEEE国际计算机视觉会议，页码3657–3666，2017年。'
- en: '[288] Anestis Papazoglou and Vittorio Ferrari. Fast object segmentation in
    unconstrained video. In Proceedings of the IEEE international conference on computer
    vision, pages 1777–1784, 2013.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] Anestis Papazoglou 和 Vittorio Ferrari。无约束视频中的快速物体分割。发表于IEEE国际计算机视觉会议，页码1777–1784，2013年。'
- en: '[289] I. C. Paschalidis, K. Li, and R. Moazzez Estanjini. An actor-critic method
    using least squares temporal difference learning. In Proceedings of the 48h IEEE
    Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control
    Conference, pages 2564–2569, 2009.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] I. C. Paschalidis、K. Li 和 R. Moazzez Estanjini。使用最小二乘时间差学习的演员-评论家方法。在第
    48 届 IEEE 决策与控制会议（CDC）与 2009 年第 28 届中国控制会议联合举行的会议论文集上，第 2564–2569 页，2009 年。'
- en: '[290] Massimiliano Patacchiola and Angelo Cangelosi. Head pose estimation in
    the wild using convolutional neural networks and adaptive gradient methods. Pattern
    Recognition, 71:132–143, 2017.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] Massimiliano Patacchiola 和 Angelo Cangelosi。利用卷积神经网络和自适应梯度方法在自然环境中进行头部姿态估计。模式识别，71:132–143，2017
    年。'
- en: '[291] Hanchuan Peng, Zongcai Ruan, Fuhui Long, Julie H Simpson, and Eugene W
    Myers. V3d enables real-time 3d visualization and quantitative analysis of large-scale
    biological image data sets. Nature biotechnology, 28(4):348–353, 2010.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] Hanchuan Peng、Zongcai Ruan、Fuhui Long、Julie H Simpson 和 Eugene W Myers。V3d
    实现大规模生物图像数据集的实时 3d 可视化和定量分析。自然生物技术，28(4):348–353，2010 年。'
- en: '[292] Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and
    Alexander Sorkine-Hornung. Learning video object segmentation from static images.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2663–2672, 2017.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] Federico Perazzi、Anna Khoreva、Rodrigo Benenson、Bernt Schiele 和 Alexander
    Sorkine-Hornung。从静态图像中学习视频目标分割。在 IEEE 计算机视觉与模式识别会议论文集上，第 2663–2672 页，2017 年。'
- en: '[293] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus
    Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology
    for video object segmentation. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 724–732, 2016.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] Federico Perazzi、Jordi Pont-Tuset、Brian McWilliams、Luc Van Gool、Markus
    Gross 和 Alexander Sorkine-Hornung。视频目标分割的基准数据集和评估方法。在 IEEE 计算机视觉与模式识别会议论文集上，第
    724–732 页，2016 年。'
- en: '[294] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills
    with policy gradients. Neural Networks, 21(4):682 – 697, 2008.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] Jan Peters 和 Stefan Schaal。使用策略梯度的运动技能强化学习。神经网络，21(4):682–697，2008 年。'
- en: '[295] Aleksis Pirinen and Cristian Sminchisescu. Deep reinforcement learning
    of region proposal networks for object detection. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 6945–6954, 2018.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] Aleksis Pirinen 和 Cristian Sminchisescu。用于目标检测的区域提议网络的深度强化学习。在 IEEE 计算机视觉与模式识别会议论文集上，第
    6945–6954 页，2018 年。'
- en: '[296] Hamed Pirsiavash, Deva Ramanan, and Charless C Fowlkes. Globally-optimal
    greedy algorithms for tracking a variable number of objects. In CVPR 2011, pages
    1201–1208\. IEEE, 2011.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] Hamed Pirsiavash、Deva Ramanan 和 Charless C Fowlkes。用于跟踪可变数量对象的全局最优贪婪算法。在
    CVPR 2011 上，第 1201–1208 页。IEEE，2011 年。'
- en: '[297] Aske Plaat, Walter Kosters, and Mike Preuss. Deep model-based reinforcement
    learning for high-dimensional problems, a survey, 2020.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] Aske Plaat、Walter Kosters 和 Mike Preuss。用于高维问题的深度模型基础强化学习，综述，2020 年。'
- en: '[298] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid.
    Category-specific video summarization. In European conference on computer vision,
    pages 540–555. Springer, 2014.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] Danila Potapov、Matthijs Douze、Zaid Harchaoui 和 Cordelia Schmid。类别特定的视频总结。在欧洲计算机视觉会议上，第
    540–555 页。Springer，2014 年。'
- en: '[299] Reza Pourreza-Shahri and Nasser Kehtarnavaz. Exposure bracketing via
    automatic exposure selection. In 2015 IEEE International Conference on Image Processing
    (ICIP), pages 320–323\. IEEE, 2015.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] Reza Pourreza-Shahri 和 Nasser Kehtarnavaz。通过自动曝光选择进行曝光包围。在 2015 IEEE
    国际图像处理会议（ICIP）上，第 320–323 页。IEEE，2015 年。'
- en: '[300] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid,
    and Vittorio Ferrari. Learning object class detectors from weakly annotated video.
    In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3282–3289\.
    IEEE, 2012.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] Alessandro Prest、Christian Leistner、Javier Civera、Cordelia Schmid 和 Vittorio
    Ferrari。从弱标注视频中学习对象类别检测器。在 2012 IEEE 计算机视觉与模式识别会议上，第 3282–3289 页。IEEE，2012 年。'
- en: '[301] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao, Qingming Huang, Jongwoo
    Lim, and Ming-Hsuan Yang. Hedged deep tracking. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 4303–4311, 2016.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] Yuankai Qi、Shengping Zhang、Lei Qin、Hongxun Yao、Qingming Huang、Jongwoo
    Lim 和 Ming-Hsuan Yang。风险对冲深度跟踪。在 IEEE 计算机视觉与模式识别会议论文集上，第 4303–4311 页，2016 年。'
- en: '[302] Zengyi Qin, Jinglu Wang, and Yan Lu. Monogrnet: A geometric reasoning
    network for monocular 3d object localization. Proceedings of the AAAI Conference
    on Artificial Intelligence, 33(01):8851–8858, Jul. 2019.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] Zengyi Qin, Jinglu Wang, 和 Yan Lu. Monogrnet: 一种用于单目3D物体定位的几何推理网络。发表于AAAI人工智能会议论文集，33(01):8851–8858，2019年7月。'
- en: '[303] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.
    Efficient off-policy meta-reinforcement learning via probabilistic context variables.
    In International conference on machine learning, pages 5331–5340, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, 和 Deirdre Quillen.
    通过概率上下文变量进行高效的离策略元强化学习。发表于国际机器学习会议，页码5331–5340，2019年。'
- en: '[304] Vidhiwar Singh Rathour, Kashu Yamakazi, and T Le. Roughness index and
    roughness distance for benchmarking medical segmentation. arXiv preprint arXiv:2103.12350,
    2021.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] Vidhiwar Singh Rathour, Kashu Yamakazi, 和 T Le. 粗糙度指数和粗糙度距离用于医学分割基准测试。arXiv预印本
    arXiv:2103.12350，2021年。'
- en: '[305] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only
    look once: Unified, real-time object detection. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 779–788, 2016.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] Joseph Redmon, Santosh Divvala, Ross Girshick, 和 Ali Farhadi. 你只看一次:
    统一的实时目标检测。发表于IEEE计算机视觉与模式识别会议，页码779–788，2016年。'
- en: '[306] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] Joseph Redmon 和 Ali Farhadi. Yolov3: 增量改进。arXiv预印本 arXiv:1804.02767，2018年。'
- en: '[307] Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian, and Jie Zhou. Collaborative
    deep reinforcement learning for multi-object tracking. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 586–602, 2018.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian, 和 Jie Zhou. 协作深度强化学习用于多目标跟踪。发表于《欧洲计算机视觉会议论文集》(ECCV)，页码586–602，2018年。'
- en: '[308] Liangliang Ren, Xin Yuan, Jiwen Lu, Ming Yang, and Jie Zhou. Deep reinforcement
    learning with iterative shift for visual tracking. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 684–700, 2018.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] Liangliang Ren, Xin Yuan, Jiwen Lu, Ming Yang, 和 Jie Zhou. 具有迭代偏移的深度强化学习用于视觉跟踪。发表于《欧洲计算机视觉会议论文集》(ECCV)，页码684–700，2018年。'
- en: '[309] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. In Advances
    in neural information processing systems, pages 91–99, 2015.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun. Faster R-CNN: 通过区域提议网络实现实时目标检测。发表于《神经信息处理系统进展》，页码91–99，2015年。'
- en: '[310] Md Reza, Jana Kosecka, et al. Reinforcement learning for semantic segmentation
    in indoor scenes. arXiv preprint arXiv:1606.01178, 2016.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] Md Reza, Jana Kosecka, 等。用于室内场景语义分割的强化学习。arXiv预印本 arXiv:1606.01178，2016年。'
- en: '[311] Alexander Richard and Juergen Gall. Temporal action detection using a
    statistical language model. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 3131–3140, 2016.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] Alexander Richard 和 Juergen Gall. 使用统计语言模型进行时间动作检测。发表于IEEE计算机视觉与模式识别会议，页码3131–3140，2016年。'
- en: '[312] Mrigank Rochan, Linwei Ye, and Yang Wang. Video summarization using fully
    convolutional sequence networks. In Proceedings of the European Conference on
    Computer Vision (ECCV), pages 347–363, 2018.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] Mrigank Rochan, Linwei Ye, 和 Yang Wang. 使用全卷积序列网络的视频总结。发表于《欧洲计算机视觉会议论文集》(ECCV)，页码347–363，2018年。'
- en: '[313] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional
    networks for biomedical image segmentation. In International Conference on Medical
    image computing and computer-assisted intervention, pages 234–241\. Springer,
    2015.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] Olaf Ronneberger, Philipp Fischer, 和 Thomas Brox. U-net: 用于生物医学图像分割的卷积网络。发表于国际医学图像计算与计算机辅助干预会议，页码234–241。Springer，2015年。'
- en: '[314] German Ros, Vladfen Koltun, Felipe Codevilla, and Antonio Lopez. The
    carla autonomous driving challenge, 2019.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] German Ros, Vladfen Koltun, Felipe Codevilla, 和 Antonio Lopez. Carla自主驾驶挑战赛，2019年。'
- en: '[315] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. ” grabcut” interactive
    foreground extraction using iterated graph cuts. ACM transactions on graphics
    (TOG), 23(3):309–314, 2004.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] Carsten Rother, Vladimir Kolmogorov, 和 Andrew Blake. “grabcut” 交互式前景提取使用迭代图割。ACM图形学交易
    (TOG)，23(3):309–314，2004年。'
- en: '[316] David Rotman. Mit technology review. Retrieved from Meet the Man with
    a Cheap and Easy Plan to Stop Global Warming: http://www. technologyreview. com/featuredstor
    y/511016/a-cheap-and-easy-plan-to-stop-globalwarming, 2013.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] David Rotman. MIT技术评论。摘录自《遇见拥有廉价且简单计划以阻止全球变暖的人》: [http://www.technologyreview.com/featuredstory/511016/a-cheap-and-easy-plan-to-stop-globalwarming](http://www.technologyreview.com/featuredstory/511016/a-cheap-and-easy-plan-to-stop-globalwarming)，2013年。'
- en: '[317] J-M Rouet, J-J Jacq, and Christian Roux. Genetic algorithms for a robust
    3-d mr-ct registration. IEEE transactions on information technology in biomedicine,
    4(2):126–136, 2000.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] J-M Rouet、J-J Jacq 和 Christian Roux。用于稳健3D MR-CT配准的遗传算法。《IEEE生物医学信息技术汇刊》，4(2):126–136，2000年。'
- en: '[318] David E Rumelhart. The architecture of mind: A connectionist approach.
    Mind readings, pages 207–238, 1998.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] David E Rumelhart。心智的架构：一种连接主义方法。《心智阅读》，页面207–238，1998年。'
- en: '[319] T. P. Runarsson and S. M. Lucas. Imitating play from game trajectories:
    Temporal difference learning versus preference learning. In 2012 IEEE Conference
    on Computational Intelligence and Games (CIG), pages 79–82, 2012.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] T. P. Runarsson 和 S. M. Lucas。从游戏轨迹中模仿游戏：时间差分学习与偏好学习。在2012 IEEE计算智能与游戏会议（CIG），页面79–82，2012年。'
- en: '[320] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
    Imagenet large scale visual recognition challenge. International Journal of Computer
    Vision, 115(3):211–252, 2015.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] Olga Russakovsky、Jia Deng、Hao Su、Jonathan Krause、Sanjeev Satheesh、Sean
    Ma、Zhiheng Huang、Andrej Karpathy、Aditya Khosla、Michael Bernstein 等。Imagenet 大规模视觉识别挑战。《计算机视觉国际期刊》，115(3):211–252，2015年。'
- en: '[321] Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Tracking the untrackable:
    Learning to track multiple cues with long-term dependencies. In Proceedings of
    the IEEE International Conference on Computer Vision, pages 300–311, 2017.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] Amir Sadeghian、Alexandre Alahi 和 Silvio Savarese。追踪不可追踪的目标：学习追踪多个线索与长期依赖。在IEEE计算机视觉国际会议论文集，页面300–311，2017年。'
- en: '[322] Steindór Sæmundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta reinforcement
    learning with latent variable gaussian processes. arXiv preprint arXiv:1803.07551,
    2018.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] Steindór Sæmundsson、Katja Hofmann 和 Marc Peter Deisenroth。具有潜在变量高斯过程的元强化学习。arXiv预印本
    arXiv:1803.07551，2018年。'
- en: '[323] Farhang Sahba. Deep reinforcement learning for object segmentation in
    video sequences. In 2016 International Conference on Computational Science and
    Computational Intelligence (CSCI), pages 857–860\. IEEE, 2016.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] Farhang Sahba。用于视频序列中物体分割的深度强化学习。在2016年国际计算科学与计算智能会议（CSCI），页面857–860。IEEE，2016年。'
- en: '[324] Farhang Sahba, Hamid R Tizhoosh, and Magdy MA Salama. A reinforcement
    learning framework for medical image segmentation. In The 2006 IEEE International
    Joint Conference on Neural Network Proceedings, pages 511–517\. IEEE, 2006.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] Farhang Sahba、Hamid R Tizhoosh 和 Magdy MA Salama。用于医学图像分割的强化学习框架。在2006年IEEE国际联合神经网络会议论文集，页面511–517。IEEE，2006年。'
- en: '[325] Farhang Sahba, Hamid R Tizhoosh, and Magdy MMA Salama. Application of
    opposition-based reinforcement learning in image segmentation. In 2007 IEEE Symposium
    on Computational Intelligence in Image and Signal Processing, pages 246–251\.
    IEEE, 2007.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] Farhang Sahba、Hamid R Tizhoosh 和 Magdy MMA Salama。基于对抗的强化学习在图像分割中的应用。在2007年IEEE计算智能在图像与信号处理中的应用研讨会，页面246–251。IEEE，2007年。'
- en: '[326] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
    Moritz. Trust region policy optimization. In International conference on machine
    learning, pages 1889–1897, 2015.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] John Schulman、Sergey Levine、Pieter Abbeel、Michael Jordan 和 Philipp Moritz。信任区域策略优化。在国际机器学习会议，页面1889–1897，2015年。'
- en: '[327] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and
    Pieter Abbeel. Trust Region Policy Optimization. arXiv e-prints, February 2015.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] John Schulman、Sergey Levine、Philipp Moritz、Michael I. Jordan 和 Pieter
    Abbeel。信任区域策略优化。arXiv电子预印本，2015年2月。'
- en: '[328] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal Policy Optimization Algorithms. arXiv e-prints, July 2017.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford 和 Oleg Klimov。近端策略优化算法。arXiv电子预印本，2017年7月。'
- en: '[329] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford 和 Oleg Klimov。近端策略优化算法。arXiv预印本
    arXiv:1707.06347，2017年。'
- en: '[330] Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning.
    Neural Networks, 16(1):5–9, 2003.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] Nicolas Schweighofer 和 Kenji Doya。强化学习中的元学习。《神经网络》，16(1):5–9，2003年。'
- en: '[331] Shahin Sefati, Noah J Cowan, and René Vidal. Learning shared, discriminative
    dictionaries for surgical gesture segmentation and classification. In MICCAI Workshop:
    M2CAI, volume 4, 2015.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] Shahin Sefati、Noah J Cowan 和 René Vidal。学习共享的、区分性字典用于外科手势分割与分类。在MICCAI工作坊：M2CAI，第4卷，2015年。'
- en: '[332] Mohammad Javad Shafiee, Brendan Chywl, Francis Li, and Alexander Wong.
    Fast yolo: A fast you only look once system for real-time embedded object detection
    in video. arXiv preprint arXiv:1709.05943, 2017.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] Mohammad Javad Shafiee, Brendan Chywl, Francis Li, 和 Alexander Wong.
    快速YOLO：用于实时嵌入式视频目标检测的快速“你只看一次”系统。arXiv预印本 arXiv:1709.05943，2017年。'
- en: '[333] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: A
    large scale dataset for 3d human activity analysis. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 1010–1019, 2016.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, 和 Gang Wang. NTU RGB+D：用于3D人类活动分析的大规模数据集。发表于IEEE计算机视觉与模式识别会议论文集，第1010–1019页，2016年。'
- en: '[334] M. R. Shaker, Shigang Yue, and T. Duckett. Vision-based reinforcement
    learning using approximate policy iteration. In 2009 International Conference
    on Advanced Robotics, pages 1–6, 2009.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] M. R. Shaker, Shigang Yue, 和 T. Duckett. 基于视觉的强化学习使用近似策略迭代。发表于2009年国际先进机器人会议论文集，第1–6页，2009年。'
- en: '[335] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent,
    reinforcement learning for autonomous driving. CoRR, abs/1610.03295, 2016.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] Shai Shalev-Shwartz, Shaked Shammah, 和 Amnon Shashua. 用于自主驾驶的安全多智能体强化学习。CoRR，abs/1610.03295，2016年。'
- en: '[336] Jie Shen, Stefanos Zafeiriou, Grigoris G Chrysos, Jean Kossaifi, Georgios
    Tzimiropoulos, and Maja Pantic. The first facial landmark tracking in-the-wild
    challenge: Benchmark and results. In Proceedings of the IEEE international conference
    on computer vision workshops, pages 50–58, 2015.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] Jie Shen, Stefanos Zafeiriou, Grigoris G Chrysos, Jean Kossaifi, Georgios
    Tzimiropoulos, 和 Maja Pantic. 第一个野外面部标志点跟踪挑战：基准和结果。发表于IEEE国际计算机视觉研讨会论文集，第50–58页，2015年。'
- en: '[337] Y. Shi, L. Cui, Z. Qi, F. Meng, and Z. Chen. Automatic road crack detection
    using random structured forests. IEEE Transactions on Intelligent Transportation
    Systems, 17(12):3434–3445, 2016.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] Y. Shi, L. Cui, Z. Qi, F. Meng, 和 Z. Chen. 使用随机结构森林的自动道路裂缝检测。IEEE智能交通系统学报，17(12)：3434–3445，2016年。'
- en: '[338] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor
    segmentation and support inference from rgbd images. In European conference on
    computer vision, pages 746–760. Springer, 2012.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, 和 Rob Fergus. 从RGBD图像中进行室内分割和支持推断。发表于欧洲计算机视觉会议论文集，第746–760页。Springer，2012年。'
- en: '[339] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
    and Martin Riedmiller. Deterministic policy gradient algorithms. 2014.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[339] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
    和 Martin Riedmiller. 确定性策略梯度算法。2014年。'
- en: '[340] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks
    for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[340] Karen Simonyan 和 Andrew Zisserman. 大规模图像识别的深度卷积网络。arXiv预印本 arXiv:1409.1556，2014年。'
- en: '[341] Vishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom
    feature fusion for crowd counting. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1002–1012, 2019.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[341] Vishwanath A Sindagi 和 Vishal M Patel. 多层次的自下而上和自上而下特征融合用于人群计数。发表于IEEE国际计算机视觉会议论文集，第1002–1012页，2019年。'
- en: '[342] Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan,
    and Balázs Gulyás. 3d deep learning on medical images: A review, 2020.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[342] Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan,
    和 Balázs Gulyás. 医学图像上的3D深度学习：综述，2020年。'
- en: '[343] Gwangmo Song, Heesoo Myeong, and Kyoung Mu Lee. Seednet: Automatic seed
    generation with deep reinforcement learning for robust interactive segmentation.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 1760–1768, 2018.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[343] Gwangmo Song, Heesoo Myeong, 和 Kyoung Mu Lee. SeedNet：用于鲁棒交互分割的深度强化学习自动种子生成。发表于IEEE计算机视觉与模式识别会议论文集，第1760–1768页，2018年。'
- en: '[344] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum:
    Summarizing web videos using titles. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 5179–5187, 2015.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[344] Yale Song, Jordi Vallmitjana, Amanda Stent, 和 Alejandro Jaimes. TVSum：使用标题总结网页视频。发表于IEEE计算机视觉与模式识别会议论文集，第5179–5187页，2015年。'
- en: '[345] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson WH Lau, and Ming-Hsuan
    Yang. Crest: Convolutional residual learning for visual tracking. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 2555–2564, 2017.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[345] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson WH Lau, 和 Ming-Hsuan
    Yang. Crest: 卷积残差学习用于视觉跟踪。发表于IEEE国际计算机视觉会议论文集，第2555–2564页，2017年。'
- en: '[346] Concetto Spampinato, Simone Palazzo, and Daniela Giordano. Gamifying
    video object segmentation. IEEE transactions on pattern analysis and machine intelligence,
    39(10):1942–1958, 2016.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[346] Concetto Spampinato, Simone Palazzo, 和Daniela Giordano. 视频对象分割的游戏化。 IEEE模式分析与机器智能期刊，39(10):1942–1958,
    2016。'
- en: '[347] Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation
    learning. CoRR, abs/1703.01703, 2017.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[347] Bradly C. Stadie, Pieter Abbeel, 和Ilya Sutskever. 第三人称模仿学习。 CoRR，abs/1703.01703,
    2017。'
- en: '[348] Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary
    mean-field games. page 251–259\. International Foundation for Autonomous Agents
    and Multiagent Systems, 2019.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[348] Jayakumar Subramanian 和Aditya Mahajan. 静止均场博弈中的强化学习。 页码251–259。国际独立智能体和多智能体系统基金会,
    2019。'
- en: '[349] Shanhui Sun, Jing Hu, Mingqing Yao, Jinrong Hu, Xiaodong Yang, Qi Song,
    and Xi Wu. Robust multimodal image registration using deep recurrent reinforcement
    learning. In Asian Conference on Computer Vision, pages 511–526. Springer, 2018.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[349] Shanhui Sun, Jing Hu, Mingqing Yao, Jinrong Hu, Xiaodong Yang, Qi Song,
    和Xi Wu. 利用深度递归强化学习进行多模态图像配准。 在亚洲计算机视觉会议上，页码511–526。 Springer, 2018。'
- en: '[350] Kalaivani Sundararajan and Damon L. Woodard. Deep learning for biometrics:
    A survey. 51(3), 2018.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[350] Kalaivani Sundararajan和Damon L. Woodard. 深度学习在生物特征识别中的应用：一项调查。51(3),
    2018.'
- en: '[351] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[351] Richard S Sutton 和Andrew G Barto. 强化学习导论。 麻省理工学院出版社, 2018。'
- en: '[352] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
    Policy gradient methods for reinforcement learning with function approximation.
    In Proceedings of the 12th International Conference on Neural Information Processing
    Systems, NIPS’99, page 1057–1063, 1999.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[352] Richard S. Sutton, David McAllester, Satinder Singh, 和Yishay Mansour.
    用于带有函数逼近的强化学习的策略梯度方法。 在第12届国际神经信息处理系统会议论文集NIPS’99中，页码1057–1063, 1999。'
- en: '[353] Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay
    Mansour. Policy gradient methods for reinforcement learning with function approximation.
    In Advances in Neural Information Processing Systems 12, pages 1057–1063\. 2000.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[353] Richard S Sutton, David A. McAllester, Satinder P. Singh, 和Yishay Mansour.
    用于带有函数逼近的强化学习的策略梯度方法。 在第12届神经信息处理系统国际会议论文集中，页码1057–1063, 2000。'
- en: '[354] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In Thirty-first AAAI conference on artificial intelligence, 2017.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[354] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, 和Alexander A Alemi.
    Inception-v4，inception-resnet及残差连接对学习的影响。 在第31届AAAI人工智能大会上，2017。'
- en: '[355] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1–9, 2015.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[355] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, 和Andrew Rabinovich. 深入卷积。
    在IEEE计算机视觉和模式识别会议论文集中，页码1–9, 2015。'
- en: '[356] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks
    for object detection. In Advances in neural information processing systems, pages
    2553–2561, 2013.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[356] Christian Szegedy, Alexander Toshev,和Dumitru Erhan. 目标检测的深度神经网络。 在神经信息处理系统的进展中，页码2553–2561,
    2013。'
- en: '[357] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep progressive
    reinforcement learning for skeleton-based action recognition. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pages 5323–5332,
    2018.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[357] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li,和Jie Zhou. 基于骨架的行为识别的深度逐步强化学习。
    在IEEE计算机视觉和模式识别会议论文集中，页码5323–5332, 2018。'
- en: '[358] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders. Siamese instance
    search for tracking. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1420–1429, 2016.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[358] Ran Tao, Efstratios Gavves, 和Arnold WM Smeulders. 用于跟踪的共生实例搜索。 在IEEE计算机视觉和模式识别会议论文集中，页码1420–1429,
    2016。'
- en: '[359] Philippe Thévenaz and Michael Unser. Optimization of mutual information
    for multiresolution image registration. IEEE transactions on image processing,
    9(12):2083–2099, 2000.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[359] Philippe Thévenaz 和Michael Unser. 多分辨率图像配准的互信息优化。 IEEE图像处理期刊，9(12):2083–2099,
    2000。'
- en: '[360] Zhiqiang Tian, Xiangyu Si, Yaoyue Zheng, Zhang Chen, and Xiaojian Li.
    Multi-step medical image segmentation based on reinforcement learning. JOURNAL
    OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING, 2020.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[360] Zhiqiang Tian、Xiangyu Si、Yaoyue Zheng、Zhang Chen 和 Xiaojian Li。基于强化学习的多步骤医学图像分割。《环境智能与人性化计算杂志》，2020年。'
- en: '[361] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free
    reinforcement learning for urban driving using implicit affordances. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7153–7162,
    2020.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[361] Marin Toromanoff、Emilie Wirbel 和 Fabien Moutarde。用于城市驾驶的端到端无模型强化学习，利用隐式赋能。发表于《IEEE/CVF计算机视觉与模式识别会议》，第7153–7162页，2020年。'
- en: '[362] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation
    via deep neural networks. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1653–1660, 2014.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[362] Alexander Toshev 和 Christian Szegedy。Deeppose：通过深度神经网络进行人体姿态估计。发表于《IEEE计算机视觉与模式识别会议》，第1653–1660页，2014年。'
- en: '[363] Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J Black. Video segmentation
    via object flow. In Proceedings of the IEEE conference on computer vision and
    pattern recognition, pages 3899–3908, 2016.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[363] Yi-Hsuan Tsai、Ming-Hsuan Yang 和 Michael J Black。通过对象流进行视频分割。发表于《IEEE计算机视觉与模式识别会议》，第3899–3908页，2016年。'
- en: '[364] Y. Tsurumine, Y. Cui, K. Yamazaki, and T. Matsubara. Generative adversarial
    imitation learning with deep p-network for robotic cloth manipulation. In 2019
    IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids), pages 274–280,
    2019.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[364] Y. Tsurumine、Y. Cui、K. Yamazaki 和 T. Matsubara。用于机器人布料操控的生成对抗模仿学习，配有深度
    P 网络。发表于2019 IEEE-RAS 第19届国际人形机器人会议（Humanoids），第274–280页，2019年。'
- en: '[365] Yoshihisa Tsurumine, Yunduan Cui, Eiji Uchibe, and Takamitsu Matsubara.
    Deep reinforcement learning with smooth policy update: Application to robotic
    cloth manipulation. Robotics and Autonomous Systems, 112:72 – 83, 2019.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[365] Yoshihisa Tsurumine、Yunduan Cui、Eiji Uchibe 和 Takamitsu Matsubara。具有平滑策略更新的深度强化学习：应用于机器人布料操控。《机器人与自主系统》，112:72
    – 83，2019年。'
- en: '[366] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM
    Smeulders. Selective search for object recognition. International journal of computer
    vision, 104(2):154–171, 2013.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[366] Jasper RR Uijlings、Koen EA Van De Sande、Theo Gevers 和 Arnold WM Smeulders。用于目标识别的选择性搜索。《国际计算机视觉杂志》，104(2):154–171，2013年。'
- en: '[367] Burak Uzkent, Christopher Yeh, and Stefano Ermon. Efficient object detection
    in large images using deep reinforcement learning. In The IEEE Winter Conference
    on Applications of Computer Vision, pages 1824–1833, 2020.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[367] Burak Uzkent、Christopher Yeh 和 Stefano Ermon。利用深度强化学习在大图像中进行高效目标检测。发表于《IEEE计算机视觉应用冬季会议》，第1824–1833页，2020年。'
- en: '[368] Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, and Philip HS
    Torr. End-to-end representation learning for correlation filter based tracking.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 2805–2813, 2017.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[368] Jack Valmadre、Luca Bertinetto、Joao Henriques、Andrea Vedaldi 和 Philip
    HS Torr。基于相关滤波器的端到端表示学习跟踪。发表于《IEEE计算机视觉与模式识别会议》，第2805–2813页，2017年。'
- en: '[369] Peter van Beek. Improved image selection for stack-based hdr imaging.
    arXiv preprint arXiv:1806.07420, 2018.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[369] Peter van Beek。改进的图像选择用于基于堆栈的 HDR 成像。arXiv 预印本 arXiv:1806.07420，2018年。'
- en: '[370] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning
    with Double Q-learning. arXiv e-prints, page arXiv:1509.06461, September 2015.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[370] Hado van Hasselt、Arthur Guez 和 David Silver。双重 Q 学习的深度强化学习。arXiv 电子印刷版，第
    arXiv:1509.06461 页，2015年9月。'
- en: '[371] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning
    with double q-learning. In Thirtieth AAAI conference on artificial intelligence,
    2016.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[371] Hado Van Hasselt、Arthur Guez 和 David Silver。通过双重 Q 学习的深度强化学习。发表于第三十届
    AAAI 人工智能会议，2016年。'
- en: '[372] Leo Van Hove. Optimal denominations for coins and bank notes: in defense
    of the principle of least effort. Journal of Money, Credit and Banking, pages
    1015–1021, 2001.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[372] Leo Van Hove。硬币和纸币的最佳面额：支持最小努力原则。 《货币、信贷与银行杂志》，第1015–1021页，2001年。'
- en: '[373] Giuseppe Vecchio, Simone Palazzo, Daniela Giordano, Francesco Rundo,
    and Concetto Spampinato. Mask-rl: Multiagent video object segmentation framework
    through reinforcement learning. IEEE Transactions on Neural Networks and Learning
    Systems, 2020.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[373] Giuseppe Vecchio、Simone Palazzo、Daniela Giordano、Francesco Rundo 和 Concetto
    Spampinato。Mask-rl：通过强化学习的多代理视频对象分割框架。《IEEE神经网络与学习系统汇刊》，2020年。'
- en: '[374] Kashu Yamakazi Akihiro Sugimoto Viet-Khoa Vo-Ho, Ngan T.H. Le and Triet
    Tran. Agent-environment network for temporal action proposal generation. In International
    Conference on Acoustics, Speech and Signal Processing. 2021.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[374] Kashu Yamakazi Akihiro Sugimoto Viet-Khoa Vo-Ho, Ngan T.H. Le 和 Triet
    Tran。用于时序动作提议生成的代理-环境网络。在国际声学、语音和信号处理会议上。2021年。'
- en: '[375] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar,
    and Katerina Fragkiadaki. Sfm-net: Learning of structure and motion from video.
    arXiv preprint arXiv:1704.07804, 2017.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[375] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar
    和 Katerina Fragkiadaki。Sfm-net：从视频中学习结构和运动。arXiv预印本 arXiv:1704.07804，2017年。'
- en: '[376] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max
    Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard
    Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk
    Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets,
    James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang,
    Tobias Pfaff, Toby Pohlen, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver
    Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis,
    and David Silver. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II.
    [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/),
    2019.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[376] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max
    Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard
    Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk
    Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets,
    James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang,
    Tobias Pfaff, Toby Pohlen, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver
    Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis
    和 David Silver。AlphaStar：掌握实时策略游戏《星际争霸II》。 [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)，2019年。'
- en: '[377] Athanasios Vlontzos, Amir Alansary, Konstantinos Kamnitsas, Daniel Rueckert,
    and Bernhard Kainz. Multiple landmark detection using multi-agent reinforcement
    learning. In International Conference on Medical Image Computing and Computer-Assisted
    Intervention, pages 262–270\. Springer, 2019.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[377] Athanasios Vlontzos, Amir Alansary, Konstantinos Kamnitsas, Daniel Rueckert
    和 Bernhard Kainz。使用多智能体强化学习进行多重标志物检测。在国际医学图像计算与计算机辅助手术会议上，页码262–270。Springer，2019年。'
- en: '[378] Guotai Wang, Maria A Zuluaga, Wenqi Li, Rosalind Pratt, Premal A Patel,
    Michael Aertsen, Tom Doel, Anna L David, Jan Deprest, Sébastien Ourselin, et al.
    Deepigeos: a deep interactive geodesic framework for medical image segmentation.
    IEEE transactions on pattern analysis and machine intelligence, 41(7):1559–1572,
    2018.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[378] Guotai Wang, Maria A Zuluaga, Wenqi Li, Rosalind Pratt, Premal A Patel,
    Michael Aertsen, Tom Doel, Anna L David, Jan Deprest, Sébastien Ourselin 等。Deepigeos：用于医学图像分割的深度交互地质框架。IEEE模式分析与机器智能交易，41(7):1559–1572，2018年。'
- en: '[379] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou,
    Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5265–5274, 2018.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[379] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou,
    Zhifeng Li 和 Wei Liu。Cosface：用于深度人脸识别的大间隔余弦损失。在IEEE计算机视觉与模式识别会议的论文集中，页码5265–5274，2018年。'
- en: '[380] Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z.
    Leibo, Rémi Munos, Charles Blundell, Dharshan Kumaran, and Matthew Botvinick.
    Learning to reinforcement learn. CoRR, abs/1611.05763, 2016.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[380] Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z.
    Leibo, Rémi Munos, Charles Blundell, Dharshan Kumaran 和 Matthew Botvinick。学习强化学习。CoRR，abs/1611.05763，2016年。'
- en: '[381] Lijun Wang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Deep networks
    for saliency detection via local estimation and global search. In Computer Vision
    and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3183–3192\. IEEE,
    2015.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[381] Lijun Wang, Huchuan Lu, Xiang Ruan 和 Ming-Hsuan Yang。通过局部估计和全局搜索进行显著性检测的深度网络。在计算机视觉与模式识别（CVPR）会议上，页码3183–3192。IEEE，2015年。'
- en: '[382] Mei Wang and Weihong Deng. Mitigating bias in face recognition using
    skewness-aware reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 9322–9331, 2020.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[382] Mei Wang 和 Weihong Deng。使用偏斜感知强化学习减轻面部识别中的偏差。在IEEE/CVF计算机视觉与模式识别会议的论文集中，页码9322–9331，2020年。'
- en: '[383] Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial
    faces in the wild: Reducing racial bias by information maximization adaptation
    network. In Proceedings of the IEEE International Conference on Computer Vision,
    pages 692–702, 2019.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[383] 王梅、邓伟宏、胡佳妮、陶迅强和黄耀海。野外的种族面孔：通过信息最大化适应网络减少种族偏见。发表于《IEEE国际计算机视觉会议论文集》，第692–702页，2019年。'
- en: '[384] Naiyan Wang and Dit-Yan Yeung. Learning a deep compact image representation
    for visual tracking. In Advances in neural information processing systems, pages
    809–817, 2013.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[384] 王乃彦和叶迪燕。学习用于视觉跟踪的深度紧凑图像表示。发表于《神经信息处理系统进展》，第809–817页，2013年。'
- en: '[385] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric
    Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking
    model-based reinforcement learning. CoRR, abs/1907.02057, 2019.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[385] 汤婷婷、包旭铨、伊格纳西·克拉维拉、杰里克·黄、温叶明、埃里克·朗格洛伊斯、张顺施、张国栋、皮特·阿贝尔和吉米·巴。基于模型的强化学习基准测试。CoRR,
    abs/1907.02057，2019年。'
- en: '[386] Yan Wang, Lei Zhang, Lituan Wang, and Zizhou Wang. Multitask learning
    for object localization with deep reinforcement learning. IEEE Transactions on
    Cognitive and Developmental Systems, 11(4):573–580, 2018.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[386] 王艳、张磊、王立川和王子洲。用于目标定位的多任务学习与深度强化学习。IEEE认知与发展系统学报，11(4):573–580，2018年。'
- en: '[387] Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, and Maja
    Pantic. Dynamic face video segmentation via reinforcement learning. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6959–6969,
    2020.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[387] 王玉江、董明智、沈杰、吴杨、程世阳和玛雅·潘提克。通过强化学习进行动态面部视频分割。发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，第6959–6969页，2020年。'
- en: '[388] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image
    quality assessment: from error visibility to structural similarity. IEEE transactions
    on image processing, 13(4):600–612, 2004.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[388] 周望、艾伦·C·博维克、哈米德·R·谢赫和埃罗·P·西蒙切利。图像质量评估：从误差可视性到结构相似性。IEEE图像处理学报，13(4):600–612，2004年。'
- en: '[389] Zhouxia Wang, Jiawei Zhang, Mude Lin, Jiong Wang, Ping Luo, and Jimmy
    Ren. Learning a reinforced agent for flexible exposure bracketing selection. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1820–1828, 2020.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[389] 周霞、张家伟、林木德、王炯、罗平和吉米·任。学习一种强化代理用于灵活曝光包围选择。发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，第1820–1828页，2020年。'
- en: '[390] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot,
    and Nando De Freitas. Dueling network architectures for deep reinforcement learning.
    arXiv preprint arXiv:1511.06581, 2015.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[390] 王子瑜、汤姆·肖尔、马泰奥·赫塞尔、哈多·范·哈塞尔、马克·兰克托和南多·德·弗雷塔斯。用于深度强化学习的对抗网络架构。arXiv预印本
    arXiv:1511.06581，2015年。'
- en: '[391] Wayne A Wickelgren. The long and the short of memory. Psychological Bulletin,
    80(6):425, 1973.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[391] 韦恩·A·威克尔格伦。记忆的长短。心理学公报，80(6):425，1973年。'
- en: '[392] Ronald J Williams. Simple statistical gradient-following algorithms for
    connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[392] 罗纳德·J·威廉姆斯。用于连接主义强化学习的简单统计梯度跟随算法。机器学习，8(3-4):229–256，1992年。'
- en: '[393] Aaron Wilson, Alan Fern, and Prasad Tadepalli. Using trajectory data
    to improve bayesian optimization for reinforcement learning. Journal of Machine
    Learning Research, 15(8):253–282, 2014.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[393] 亚伦·威尔逊、艾伦·费恩和普拉萨德·塔德帕利。使用轨迹数据改进贝叶斯优化的强化学习。机器学习研究杂志，15(8):253–282，2014年。'
- en: '[394] C. Wirth and J. Fürnkranz. On learning from game annotations. IEEE Transactions
    on Computational Intelligence and AI in Games, 7(3):304–316, 2015.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[394] C·维尔特和J·弗尔克兰茨。关于从游戏注释中学习。IEEE计算智能与游戏中的人工智能学报，7(3):304–316，2015年。'
- en: '[395] Paul Wohlhart and Vincent Lepetit. Learning descriptors for object recognition
    and 3d pose estimation. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 3109–3118, 2015.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[395] 保罗·沃尔哈特和文森特·勒佩提。用于对象识别和3D姿态估计的描述符学习。发表于《IEEE计算机视觉与模式识别会议论文集》，第3109–3118页，2015年。'
- en: '[396] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional
    block attention module. In European Conference on Computer Vision, pages 3–19,
    2018.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[396] 吴相贤、朴钟灿、李俊英和权仁素。CBAM：卷积块注意力模块。发表于《欧洲计算机视觉会议》，第3–19页，2018年。'
- en: '[397] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2411–2418, 2013.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[397] 吴毅、林宗宇和杨名轩。在线目标跟踪：一个基准。发表于《IEEE计算机视觉与模式识别会议论文集》，第2411–2418页，2013年。'
- en: '[398] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 37(9):1834–1848, 2015.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[398] 吴毅，林钟宇，和杨名轩。物体跟踪基准。IEEE《模式分析与机器智能学报》，37(9):1834–1848，2015年。'
- en: '[399] Lu Xia, Chia-Chih Chen, and Jake K Aggarwal. View invariant human action
    recognition using histograms of 3d joints. In 2012 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition Workshops, pages 20–27\. IEEE, 2012.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[399] 陆霞，陈家志，和杰克·K·阿格瓦尔。使用3D关节直方图进行视角不变的人体动作识别。载于2012 IEEE计算机学会计算机视觉与模式识别研讨会论文集，第20–27页。IEEE，2012年。'
- en: '[400] Sitao Xiang and Hao Li. On the effects of batch and weight normalization
    in generative adversarial networks. arXiv preprint arXiv:1704.03971, 2017.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[400] 向思涛和李浩。批量和权重归一化在生成对抗网络中的效果。arXiv预印本arXiv:1704.03971，2017年。'
- en: '[401] Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track: Online
    multi-object tracking by decision making. In Proceedings of the IEEE international
    conference on computer vision, pages 4705–4713, 2015.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[401] 向宇，亚历山大·阿拉希，和西尔维奥·萨瓦雷斯。学习跟踪：通过决策进行在线多物体跟踪。载于《IEEE国际计算机视觉会议论文集》，第4705–4713页，2015年。'
- en: '[402] Fanyi Xiao and Yong Jae Lee. Track and segment: An iterative unsupervised
    approach for video object proposals. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 933–942, 2016.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[402] 翻译小和李永杰。跟踪与分割：一种迭代无监督的视频物体提议方法。载于《IEEE计算机视觉与模式识别会议论文集》，第933–942页，2016年。'
- en: '[403] Hang Xiao and Hanchuan Peng. App2: automatic tracing of 3d neuron morphology
    based on hierarchical pruning of a gray-weighted image distance-tree. Bioinformatics,
    29(11):1448–1454, 2013.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[403] 萧航和彭汉川。App2：基于灰度加权图像距离树的层次修剪的3D神经元形态自动追踪。生物信息学，29(11):1448–1454，2013年。'
- en: '[404] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training
    with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 10687–10698, 2020.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[404] 谢奇哲，刘敏祥，爱德华·霍维，和阮国伟。自我训练与噪声学生改进ImageNet分类。载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第10687–10698页，2020年。'
- en: '[405] Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, and Chunhua
    Shen. From open set to closed set: Counting objects by spatial divide-and-conquer.
    In Proceedings of the IEEE International Conference on Computer Vision, pages
    8362–8371, 2019.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[405] 熊海鹏，卢浩，刘承鑫，刘亮，曹志国，和沈春华。从开放集到闭合集：通过空间分治计数物体。载于《IEEE国际计算机视觉会议论文集》，第8362–8371页，2019年。'
- en: '[406] Hailiang Xu and Feng Su. Robust seed localization and growing with deep
    convolutional features for scene text detection. In Proceedings of the 5th ACM
    on International Conference on Multimedia Retrieval, pages 387–394\. ACM, 2015.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[406] 许海亮和苏峰。基于深度卷积特征的鲁棒种子定位与生长，用于场景文本检测。载于《第5届ACM国际多媒体检索会议论文集》，第387–394页。ACM，2015年。'
- en: '[407] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang. Deep
    interactive object selection. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 373–381, 2016.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[407] 徐宁，布赖恩·普赖斯，斯科特·科恩，杨吉美，和托马斯·S·黄。深度交互式物体选择。载于《IEEE计算机视觉与模式识别会议论文集》，第373–381页，2016年。'
- en: '[408] Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, and Josef Kittler. Learning
    adaptive discriminative correlation filters via temporal consistency preserving
    spatial feature selection for robust visual object tracking. IEEE Transactions
    on Image Processing, 28(11):5596–5609, 2019.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[408] 许天阳，冯振华，吴晓军，和约瑟夫·基特勒。通过保持时间一致性的空间特征选择学习自适应判别性相关滤波器，以实现鲁棒的视觉物体跟踪。IEEE《图像处理学报》，28(11):5596–5609，2019年。'
- en: '[409] Xuanang Xu, Fugen Zhou, Bo Liu, Dongshan Fu, and Xiangzhi Bai. Efficient
    multiple organ localization in ct image using 3d region proposal network. IEEE
    transactions on medical imaging, 38(8):1885–1898, 2019.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[409] 徐玄昂，周富根，刘博，傅东山，和白向智。使用3D区域提议网络在CT图像中高效定位多个器官。IEEE《医学成像学报》，38(8):1885–1898，2019年。'
- en: '[410] Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, and Chun-Yi Lee. Dynamic video
    segmentation network. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 6556–6565, 2018.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[410] 徐宇轩，傅子睿，杨宣珑，和李春艺。动态视频分割网络。载于《IEEE计算机视觉与模式识别会议论文集》，第6556–6565页，2018年。'
- en: '[411] Kashu Yamazaki, Vidhiwar Singh Rathour, and T Le. Invertible residual
    network with regularization for effective medical image segmentation. arXiv preprint
    arXiv:2103.09042, 2021.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[411] Kashu Yamazaki, Vidhiwar Singh Rathour, 和 T Le. 具有正则化的可逆残差网络用于有效的医学图像分割。arXiv
    预印本 arXiv:2103.09042, 2021。'
- en: '[412] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei
    Wen, and Errui Ding. Perspective-guided convolution networks for crowd counting.
    In Proceedings of the IEEE International Conference on Computer Vision, pages
    952–961, 2019.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[412] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei
    Wen, 和 Errui Ding. 面向视角的卷积网络用于人群计数。发表于 IEEE 国际计算机视觉大会论文集，第 952–961 页，2019。'
- en: '[413] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory
    Zelinsky, Dimitris Samaras, and Minh Hoai. Predicting goal-directed human attention
    using inverse reinforcement learning. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), June 2020.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[413] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory
    Zelinsky, Dimitris Samaras, 和 Minh Hoai. 使用逆强化学习预测目标导向的人类注意力。发表于 IEEE/CVF 计算机视觉与模式识别会议（CVPR），2020年6月。'
- en: '[414] Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare:
    a survey. arXiv preprint arXiv:1908.08796, 2019.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[414] Chao Yu, Jiming Liu, 和 Shamim Nemati. 医疗保健中的强化学习：综述。arXiv 预印本 arXiv:1908.08796,
    2019。'
- en: '[415] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman,
    Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task
    and meta reinforcement learning. In Conference on Robot Learning, pages 1094–1100,
    2020.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[415] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman,
    Chelsea Finn, 和 Sergey Levine. Meta-world：用于多任务和元强化学习的基准和评估。发表于机器人学习会议论文集，第 1094–1100
    页，2020。'
- en: '[416] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, and Jin Young Choi.
    Action-decision networks for visual tracking with deep reinforcement learning.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2711–2720, 2017.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[416] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, 和 Jin Young Choi.
    用深度强化学习进行视觉跟踪的动作决策网络。发表于 IEEE 计算机视觉与模式识别会议论文集，第 2711–2720 页，2017。'
- en: '[417] Daochen Zha, Kwei-Herng Lai, Kaixiong Zhou, and Xia Hu. Experience replay
    optimization. arXiv preprint arXiv:1906.08387, 2019.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[417] Daochen Zha, Kwei-Herng Lai, Kaixiong Zhou, 和 Xia Hu. 经验回放优化。arXiv 预印本
    arXiv:1906.08387, 2019。'
- en: '[418] Da Zhang, Hamid Maei, Xin Wang, and Yuan-Fang Wang. Deep reinforcement
    learning for visual object tracking in videos. arXiv preprint arXiv:1701.08936,
    2017.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[418] Da Zhang, Hamid Maei, Xin Wang, 和 Yuan-Fang Wang. 深度强化学习用于视频中的视觉目标跟踪。arXiv
    预印本 arXiv:1701.08936, 2017。'
- en: '[419] Dingwen Zhang, Le Yang, Deyu Meng, Dong Xu, and Junwei Han. Spftn: A
    self-paced fine-tuning network for segmenting objects in weakly labelled videos.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4429–4437, 2017.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[419] Dingwen Zhang, Le Yang, Deyu Meng, Dong Xu, 和 Junwei Han. Spftn：一种自适应微调网络，用于分割弱标记视频中的对象。发表于
    IEEE 计算机视觉与模式识别会议论文集，第 4429–4437 页，2017。'
- en: '[420] Jing Zhang, Wanqing Li, Philip O Ogunbona, Pichao Wang, and Chang Tang.
    Rgb-d-based action recognition datasets: A survey. Pattern Recognition, 60:86–105,
    2016.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[420] Jing Zhang, Wanqing Li, Philip O Ogunbona, Pichao Wang, 和 Chang Tang.
    基于 RGB-D 的动作识别数据集：综述。模式识别，60:86–105, 2016。'
- en: '[421] Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman. Video summarization
    with long short-term memory. In European conference on computer vision, pages
    766–782. Springer, 2016.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[421] Ke Zhang, Wei-Lun Chao, Fei Sha, 和 Kristen Grauman. 基于长短期记忆的视频摘要。发表于欧洲计算机视觉会议论文集，第
    766–782 页。Springer，2016。'
- en: '[422] Pengyu Zhang, Dong Wang, and Huchuan Lu. Multi-modal visual tracking:
    Review and experimental comparison, 2020.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[422] Pengyu Zhang, Dong Wang, 和 Huchuan Lu. 多模态视觉跟踪：综述与实验比较，2020。'
- en: '[423] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image
    crowd counting via multi-column convolutional neural network. In Proceedings of
    the IEEE conference on computer vision and pattern recognition, pages 589–597,
    2016.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[423] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, 和 Yi Ma. 通过多列卷积神经网络进行单图像人群计数。发表于
    IEEE 计算机视觉与模式识别会议论文集，第 589–597 页，2016。'
- en: '[424] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya
    Jia. Icnet for real-time semantic segmentation on high-resolution images. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 405–420, 2018.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[424] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, 和 Jiaya Jia.
    用于高分辨率图像的实时语义分割的 Icnet。发表于欧洲计算机视觉会议论文集（ECCV），第 405–420 页，2018。'
- en: '[425] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya
    Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2881–2890, 2017.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[425] 赵恒双、石剑平、齐晓娟、王小刚和贾佳雅。《金字塔场景解析网络》。在 IEEE 计算机视觉与模式识别会议论文集，页码 2881–2890，2017
    年。'
- en: '[426] Zhong-Qiu Zhao, Shou-Tao Xu, Dian Liu, Wei-Dong Tian, and Zhi-Da Jiang.
    A review of image set classification. Neurocomputing, 335:251–260, 2019.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[426] 赵中秋、徐守涛、刘典、田伟东和蒋志达。《图像集分类综述》。神经计算，335:251–260，2019 年。'
- en: '[427] Yefeng Zheng, David Liu, Bogdan Georgescu, Hien Nguyen, and Dorin Comaniciu.
    3d deep learning for efficient and robust landmark detection in volumetric data.
    In International Conference on Medical Image Computing and Computer-Assisted Intervention,
    pages 565–572\. Springer, 2015.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[427] 郑业峰、刘大伟、乔治斯库、阮贤和科马尼丘。《用于体积数据中高效且鲁棒的地标检测的 3D 深度学习》。在国际医学图像计算与计算机辅助干预会议论文集，页码
    565–572。斯普林格，2015 年。'
- en: '[428] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and
    Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 633–641, 2017.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[428] 周博磊、赵航、普伊格、桑贾·菲德勒、阿德拉·巴里乌索和安东尼奥·托拉尔巴。《通过 ade20k 数据集进行场景解析》。在 IEEE 计算机视觉与模式识别会议论文集，页码
    633–641，2017 年。'
- en: '[429] Kaiyang Zhou, Yu Qiao, and Tao Xiang. Deep reinforcement learning for
    unsupervised video summarization with diversity-representativeness reward. In
    Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[429] 周开扬、乔宇和向涛。《具有多样性-代表性奖励的无监督视频总结的深度强化学习》。在第三十二届 AAAI 人工智能会议，2018 年。'
- en: '[430] Kaiyang Zhou, Tao Xiang, and Andrea Cavallaro. Video summarisation by
    classification with deep reinforcement learning. arXiv preprint arXiv:1807.03089,
    2018.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[430] 周开扬、向涛和安德烈亚·卡瓦拉罗。《基于深度强化学习的 视频总结分类》。arXiv 预印本 arXiv:1807.03089，2018 年。'
- en: '[431] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature
    flow for video recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2349–2358, 2017.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[431] 朱熙洲、熊钰文、戴季峰、袁露和魏一辰。《用于视频识别的深度特征流》。在 IEEE 计算机视觉与模式识别会议论文集，页码 2349–2358，2017
    年。'
- en: '[432] Xiahai Zhuang and Juan Shen. Multi-scale patch and multi-modality atlases
    for whole heart segmentation of mri. Medical image analysis, 31:77–87, 2016.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[432] 夏海庄和卷申。《用于 MRI 整体心脏分割的多尺度补丁和多模态图谱》。医学图像分析，31:77–87，2016 年。'
- en: '[433] Will Y Zou, Xiaoyu Wang, Miao Sun, and Yuanqing Lin. Generic object detection
    with dense neural patterns and regionlets. arXiv preprint arXiv:1404.4316, 2014.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[433] 威尔·Y·邹、王晓宇、孙苗和林元庆。《通过密集神经模式和区域块进行通用物体检测》。arXiv 预印本 arXiv:1404.4316，2014
    年。'
