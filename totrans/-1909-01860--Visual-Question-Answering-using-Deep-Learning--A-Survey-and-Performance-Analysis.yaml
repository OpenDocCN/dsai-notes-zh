- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:05:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:05:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1909.01860] Visual Question Answering using Deep Learning: A Survey and Performance
    Analysis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1909.01860] 基于深度学习的视觉问答：调查与性能分析'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1909.01860](https://ar5iv.labs.arxiv.org/html/1909.01860)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1909.01860](https://ar5iv.labs.arxiv.org/html/1909.01860)
- en: '¹¹institutetext: Computer Vision Group,'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构文本：计算机视觉组，
- en: Indian Institute of Information Technology, Sri City, Chittoor, Andhra Pradesh,
    India.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 印度安得拉邦斯里市印度信息技术学院。
- en: '¹¹email: {srivastava.y15, murali.v15, srdubey, snehasis.mukherjee}@iiits.in'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹电子邮件：{srivastava.y15, murali.v15, srdubey, snehasis.mukherjee}@iiits.in
- en: 'Visual Question Answering using Deep Learning: A Survey and Performance Analysis'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的视觉问答：调查与性能分析
- en: Yash Srivastava    Vaishnav Murali    Shiv Ram Dubey    Snehasis Mukherjee
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Yash Srivastava    Vaishnav Murali    Shiv Ram Dubey    Snehasis Mukherjee
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The Visual Question Answering (VQA) task combines challenges for processing
    data with both Visual and Linguistic processing, to answer basic ‘common sense’
    questions about given images. Given an image and a question in natural language,
    the VQA system tries to find the correct answer to it using visual elements of
    the image and inference gathered from textual questions. In this survey, we cover
    and discuss the recent datasets released in the VQA domain dealing with various
    types of question-formats and robustness of the machine-learning models. Next,
    we discuss about new deep learning models that have shown promising results over
    the VQA datasets. At the end, we present and discuss some of the results computed
    by us over the vanilla VQA model, Stacked Attention Network and the VQA Challenge
    2017 winner model. We also provide the detailed analysis along with the challenges
    and future research directions.¹¹1This paper is accepted in Fifth IAPR International
    Conference on Computer Vision and Image Processing (CVIP), 2020.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答（VQA）任务结合了处理视觉和语言数据的挑战，以回答有关给定图像的基本“常识”问题。给定一幅图像和一个自然语言的问题，VQA系统尝试利用图像的视觉元素和从文本问题中获得的推理来找到正确答案。在这项调查中，我们覆盖并讨论了VQA领域中发布的最新数据集，这些数据集处理各种类型的问题格式和机器学习模型的鲁棒性。接下来，我们讨论了在VQA数据集上显示出有前景的新深度学习模型。最后，我们展示并讨论了一些我们在原始VQA模型、堆叠注意网络和VQA挑战赛2017获胜模型上计算的结果。我们还提供了详细的分析以及挑战和未来的研究方向。¹¹1本文已被第五届IAPR国际计算机视觉与图像处理会议（CVIP）2020接收。
- en: 'Keywords:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Visual Question Answering Artificial Intelligence Human Computer Interaction
    Deep Learning CNN LSTM.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答 人工智能 人机交互 深度学习 CNN LSTM。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Visual Question Answering (VQA) refers to a challenging task which lies at
    the intersection of image understanding and language processing. The VQA task
    has witnessed a significant progress the recent years by the machine intelligence
    community. The aim of VQA is to develop a system to answer specific questions
    about an input image. The answer could be in any of the following forms: a word,
    a phrase, binary answer, multiple choice answer, or a fill in the blank answer.
    Agarwal et al. [[2](#bib.bib2)] presented a novel way of combining computer vision
    and natural language processing concepts of to achieve Visual Grounded Dialogue,
    a system mimicking the human understanding of the environment with the use of
    visual observation and language understanding.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答（VQA）指的是一个挑战性的任务，位于图像理解与语言处理的交集。近年来，机器智能社区在VQA任务上取得了显著进展。VQA的目标是开发一个系统，以回答关于输入图像的特定问题。答案可以是以下任何一种形式：一个单词、一个短语、二元答案、多项选择答案或填空答案。Agarwal
    等人[[2](#bib.bib2)] 提出了将计算机视觉和自然语言处理概念结合起来的全新方式，以实现视觉基础对话系统，这一系统通过视觉观察和语言理解模拟人类对环境的理解。
- en: '![Refer to caption](img/9fb8b11d86bcfa84e610c4c7f81d0920.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9fb8b11d86bcfa84e610c4c7f81d0920.png)'
- en: 'Figure 1: Major Breakthrough Timeline in Visual Question Answering.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：视觉问答的主要突破时间线。
- en: The advancements in the field of deep learning have certainly helped to develop
    systems for the task of Image Question Answering. Krizhevsky et al [[14](#bib.bib14)]
    proposed the AlexNet model, which created a revolution in the computer vision
    domain. The paper introduced the concept of Convolution Neural Networks (CNN)
    to the mainstream computer vision application. Later many authors have worked
    on CNN, which has resulted in robust, deep learning models like VGGNet [[28](#bib.bib28)],
    Inception [[29](#bib.bib29)], ResNet [[6](#bib.bib6)], and etc. Similarly, the
    recent advancements in natural language processing area based on deep learning
    have improved the text understanding performance as well. The first major algorithm
    in the context of text processing is considered to be the Recurrent Neural Networks
    (RNN) [[21](#bib.bib21)] which introduced the concept of prior context for time
    series based data. This architecture helped the growth of machine text understanding
    which gave new boundaries to machine translation, text classification and contextual
    understanding. Another major breakthrough in the domain was the introduction of
    Long-Short Term Memory (LSTM) architecture [[7](#bib.bib7)] which improvised over
    the RNN by introducing a context cell which stores the prior relevant information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域的进展无疑促进了图像问答任务系统的发展。Krizhevsky 等人 [[14](#bib.bib14)] 提出了 AlexNet 模型，彻底改变了计算机视觉领域。论文将卷积神经网络（CNN）的概念引入主流计算机视觉应用。随后，许多作者对
    CNN 进行了研究，导致了如 VGGNet [[28](#bib.bib28)]、Inception [[29](#bib.bib29)]、ResNet [[6](#bib.bib6)]
    等强大的深度学习模型的出现。同样，基于深度学习的自然语言处理领域的最新进展也提升了文本理解性能。在文本处理的背景下，第一个重要的算法被认为是递归神经网络（RNN）
    [[21](#bib.bib21)]，它引入了时间序列数据的前置上下文概念。这一架构促进了机器文本理解的发展，为机器翻译、文本分类和上下文理解开辟了新的边界。另一个重大突破是引入了长短期记忆（LSTM）架构
    [[7](#bib.bib7)]，它通过引入一个上下文单元来存储先前相关信息，从而改进了 RNN。
- en: 'The vanilla VQA model [[2](#bib.bib2)] used a combination of VGGNet [[28](#bib.bib28)]
    and LSTM [[7](#bib.bib7)]. This model has been revised over the years, employing
    newer architectures and mathematical formulations as seen in Fig. [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Visual Question Answering using Deep Learning: A Survey and
    Performance Analysis"). Along with this, many authors have worked on producing
    datasets for eliminating bias, strengthening the performance of the model by robust
    question-answer pairs which try to cover the various types of questions, testing
    the visual and language understanding of the system. Among the recent developments
    in the topic of VQA, Li et al. have used the context-aware knowledge aggregation
    to improve the VQA performance [[15](#bib.bib15)]. Yu et al. have perfomed the
    cross-modal knowledge reasoning in the network for obtaining a knowledge-driven
    VQA [[35](#bib.bib35)]. Chen et al. have improved the robustness of VQA approach
    by synthesizing the Counterfactual samples for training [[3](#bib.bib3)]. Li et
    al. have employed the attention based mechanism through transfer learning alongwith
    a cross-modal gating approach to improve the VQA performance [[16](#bib.bib16)].
    Huang et al. [[8](#bib.bib8)] have utilized the graph based convolutional network
    to increase the encoding relational informatoin for VQA. The VQA has been also
    observed in other domains, such as VQA for remote sensing data [[19](#bib.bib19)]
    and medical VQA [[37](#bib.bib37)].'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '原始的 VQA 模型 [[2](#bib.bib2)] 使用了 VGGNet [[28](#bib.bib28)] 和 LSTM [[7](#bib.bib7)]
    的组合。这个模型经过多年的修订，采用了更新的架构和数学公式，如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Visual
    Question Answering using Deep Learning: A Survey and Performance Analysis") 所示。与此同时，许多作者致力于创建数据集以消除偏差，通过强健的问答对来提升模型性能，涵盖各种问题类型，测试系统的视觉和语言理解能力。在
    VQA 领域的最新进展中，Li 等人使用了上下文感知知识聚合来提高 VQA 性能 [[15](#bib.bib15)]。Yu 等人则在网络中执行了跨模态知识推理，以获得知识驱动的
    VQA [[35](#bib.bib35)]。Chen 等人通过合成反事实样本进行训练，提升了 VQA 方法的鲁棒性 [[3](#bib.bib3)]。Li
    等人通过迁移学习与跨模态门控方法相结合，采用了基于注意力的机制来改善 VQA 性能 [[16](#bib.bib16)]。Huang 等人 [[8](#bib.bib8)]
    利用基于图的卷积网络来增加 VQA 的编码关系信息。VQA 也在其他领域有所应用，如遥感数据的 VQA [[19](#bib.bib19)] 和医学 VQA
    [[37](#bib.bib37)]。'
- en: 'In this survey, first we cover major datasets published for validating the
    Visual Question Answering task, such as VQA dataset [[2](#bib.bib2)], DAQUAR [[20](#bib.bib20)],
    Visual7W [[39](#bib.bib39)] and most recent datasets up to 2019 include Tally-QA
    [[1](#bib.bib1)] and KVQA [[26](#bib.bib26)]. Next, we discuss the state-of-the-art
    architectures designed for the task of Visual Question Answering such as Vanilla
    VQA [[2](#bib.bib2)], Stacked Attention Networks [[33](#bib.bib33)] and Pythia
    v1.0 [[10](#bib.bib10)]. Next we present some of our computed results over the
    three architectures: vanilla VQA model [[2](#bib.bib2)], Stacked Attention Network
    (SAN) [[33](#bib.bib33)] and Teney et al. model [[31](#bib.bib31)]. Finally, we
    discuss the observations and future directions.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们首先介绍了用于验证视觉问答任务的主要数据集，如VQA数据集[[2](#bib.bib2)]、DAQUAR [[20](#bib.bib20)]、Visual7W
    [[39](#bib.bib39)]，以及2019年之前的最新数据集，包括Tally-QA [[1](#bib.bib1)]和KVQA [[26](#bib.bib26)]。接下来，我们讨论了为视觉问答任务设计的最先进架构，如Vanilla
    VQA [[2](#bib.bib2)]、Stacked Attention Networks [[33](#bib.bib33)]和Pythia v1.0
    [[10](#bib.bib10)]。然后我们展示了我们对三种架构的计算结果：vanilla VQA模型[[2](#bib.bib2)]、Stacked Attention
    Network (SAN) [[33](#bib.bib33)]和Teney等人模型[[31](#bib.bib31)]。最后，我们讨论了观察结果和未来方向。
- en: 'Table 1: Overview of VQA datasets described in this paper.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：本文描述的VQA数据集概览。
- en: '| Dataset | # Images | # Questions | Question Type(s) | Venue | Model(s) |
    Accuracy |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 图片数量 | 问题数量 | 问题类型 | 会议 | 模型 | 准确率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| DAQUAR [[20](#bib.bib20)] | 1449 | 12468 | Object Identitfication | NIPS
    2014 | AutoSeg [[5](#bib.bib5)] | 13.75% |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| DAQUAR [[20](#bib.bib20)] | 1449 | 12468 | 对象识别 | NIPS 2014 | AutoSeg [[5](#bib.bib5)]
    | 13.75% |'
- en: '| VQA [[2](#bib.bib2)] | 204721 | 614163 | Combining vision, language and common-sense
    | ICCV 2015 | CNN + LSTM | 54.06% |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| VQA [[2](#bib.bib2)] | 204721 | 614163 | 结合视觉、语言和常识 | ICCV 2015 | CNN + LSTM
    | 54.06% |'
- en: '| Visual Madlibs [[36](#bib.bib36)] | 10738 | 360001 | Fill in the blanks |
    ICCV 2015 | nCCA (bbox) | 47.9% |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Visual Madlibs [[36](#bib.bib36)] | 10738 | 360001 | 填空题 | ICCV 2015 | nCCA
    (bbox) | 47.9% |'
- en: '| Visual7W [[39](#bib.bib39)] | 47300 | 2201154 | 7Ws, locating objects | CVPR
    2016 | LSTM + Attention | 55.6% |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Visual7W [[39](#bib.bib39)] | 47300 | 2201154 | 7Ws，定位对象 | CVPR 2016 | LSTM
    + Attention | 55.6% |'
- en: '| CLEVR [[12](#bib.bib12)] | 100000 | 853554 | Synthetic question generation
    using relations | CVPR 2017 | CNN + LSTM + Spatial Relationship | 93% |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| CLEVR [[12](#bib.bib12)] | 100000 | 853554 | 基于关系的合成问题生成 | CVPR 2017 | CNN
    + LSTM + Spatial Relationship | 93% |'
- en: '| Tally-QA [[1](#bib.bib1)] | 165000 | 306907 | Counting objects on varying
    complexities | AAAI 2019 | RCN Network | 71.8% |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Tally-QA [[1](#bib.bib1)] | 165000 | 306907 | 计数不同复杂度的对象 | AAAI 2019 | RCN
    Network | 71.8% |'
- en: '| KVQA [[26](#bib.bib26)] | 24602 | 183007 | Questions based on Knowledge Graphs
    | AAAI 2019 | MemNet | 59.2% |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| KVQA [[26](#bib.bib26)] | 24602 | 183007 | 基于知识图谱的问题 | AAAI 2019 | MemNet
    | 59.2% |'
- en: 2 Datasets
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据集
- en: 'The major VQA datasets are summarized in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Visual Question Answering using Deep Learning: A Survey and Performance Analysis").
    We present the datasets below.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '主要的VQA数据集总结见表[1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Visual Question Answering
    using Deep Learning: A Survey and Performance Analysis")。我们在下文中展示这些数据集。'
- en: 'DAQUAR: DAQUAR stands for Dataset for Question Answering on Real World Images,
    released by Malinowski et al. [[20](#bib.bib20)]. It was the first dataset released
    for the IQA task. The images are taken from NYU-Depth V2 dataset [[27](#bib.bib27)].
    The dataset is small with a total of 1449 images. The question bank includes 12468
    question-answer pairs with 2483 unique questions. The questions have been generated
    by human annotations and confined within 9 question templates using annotations
    of the NYU-Depth dataset.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DAQUAR：DAQUAR代表“用于真实世界图像问答的数据集”，由Malinowski等人发布[[20](#bib.bib20)]。这是第一个发布的IQA任务数据集。图像来自NYU-Depth
    V2数据集[[27](#bib.bib27)]。数据集规模较小，总共有1449张图像。问题库包括12468个问答对，其中2483个独特问题。问题是通过人工注释生成的，并限定在9个问题模板中，使用NYU-Depth数据集的注释。
- en: 'VQA Dataset: The Visual Question Answering (VQA) dataset [[2](#bib.bib2)] is
    one of the largest datasets collected from the MS-COCO [[18](#bib.bib18)] dataset.
    The VQA dataset contains at least 3 questions per image with 10 answers per question.
    The dataset contains 614,163 questions in the form of open-ended and multiple
    choice. In multiple choice questions, the answers can be classified as: 1) Correct
    Answer, 2) Plausible Answer, 3) Popular Answers and 4) Random Answers. Recently,
    VQA V2 dataset [[2](#bib.bib2)] is released with additional confusing images.
    The VQA sample images and questions are shown in Fig. [2](#S2.F2 "Figure 2 ‣ 2
    Datasets ‣ Visual Question Answering using Deep Learning: A Survey and Performance
    Analysis").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'VQA 数据集: Visual Question Answering (VQA) 数据集 [[2](#bib.bib2)] 是从 MS-COCO [[18](#bib.bib18)]
    数据集中收集的最大的数据集之一。VQA 数据集每张图像至少包含 3 个问题，每个问题有 10 个答案。数据集包含 614,163 个开放式和选择题形式的问题。在选择题中，答案可以被分类为：1)
    正确答案，2) 合理答案，3) 流行答案和 4) 随机答案。最近，VQA V2 数据集 [[2](#bib.bib2)] 发布了附加的混淆图像。VQA 样本图像和问题如图
    [2](#S2.F2 "Figure 2 ‣ 2 Datasets ‣ Visual Question Answering using Deep Learning:
    A Survey and Performance Analysis") 所示。'
- en: 'Visual Madlibs: The Visual Madlibs dataset [[36](#bib.bib36)] presents a different
    form of template for the Image Question Answering task. One of the forms is the
    fill in the blanks type, where the system needs to supplement the words to complete
    the sentence and it mostly targets people, objects, appearances, activities and
    interactions. The Visual Madlibs samples are shown in Fig. [3](#S2.F3 "Figure
    3 ‣ 2 Datasets ‣ Visual Question Answering using Deep Learning: A Survey and Performance
    Analysis").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'Visual Madlibs: Visual Madlibs 数据集 [[36](#bib.bib36)] 提供了一种不同的图像问答任务模板。其中一种形式是填空题，系统需要补充单词以完成句子，主要针对人、物体、外观、活动和互动。Visual
    Madlibs 样本如图 [3](#S2.F3 "Figure 3 ‣ 2 Datasets ‣ Visual Question Answering using
    Deep Learning: A Survey and Performance Analysis") 所示。'
- en: '![Refer to caption](img/55349f3c2927aa040f5a4b323102e3aa.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/55349f3c2927aa040f5a4b323102e3aa.png)'
- en: 'Figure 2: Samples from VQA dataset [[2](#bib.bib2)].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 2: VQA 数据集样本 [[2](#bib.bib2)]。'
- en: '![Refer to caption](img/e83eef3028bf566d39f7804c9da40d13.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e83eef3028bf566d39f7804c9da40d13.png)'
- en: 'Figure 3: Samples from Madlibs dataset [[36](#bib.bib36)].'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 3: Madlibs 数据集样本 [[36](#bib.bib36)]。'
- en: 'Visual7W: The Visual7W dataset [[39](#bib.bib39)] is also based on the MS-COCO
    dataset. It contains 47,300 COCO images with 327,939 question-answer pairs. The
    dataset also consists of 1,311,756 multiple choice questions and answers with
    561,459 groundings. The dataset mainly deals with seven forms of questions (from
    where it derives its name): What, Where, When, Who, Why, How, and Which. It is
    majorly formed by two types of questions. The ‘telling’ questions are the ones
    which are text-based, giving a sort of description. The ‘pointing’ questions are
    the ones that begin with ‘Which,’ and have to be correctly identified by the bounding
    boxes among the group of plausible answers.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'Visual7W: Visual7W 数据集 [[39](#bib.bib39)] 同样基于 MS-COCO 数据集。它包含 47,300 张 COCO
    图像，配有 327,939 对问题和答案。数据集还包括 1,311,756 道选择题及其答案，并有 561,459 个定位标注。数据集主要处理七种形式的问题（这也是其名称的来源）：What、Where、When、Who、Why、How
    和 Which。它主要由两种类型的问题组成。‘Telling’ 问题是基于文本的，提供某种描述。‘Pointing’ 问题以‘Which’开头，需要通过在一组合理的答案中识别边界框来正确回答。'
- en: '![Refer to caption](img/6690f8a85241cc9c26d2b9535a72077a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6690f8a85241cc9c26d2b9535a72077a.png)'
- en: 'Figure 4: Samples from Tally-QA dataset [[1](#bib.bib1)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 4: Tally-QA 数据集样本 [[1](#bib.bib1)]。'
- en: '![Refer to caption](img/3f50599ec8c1c116ea9c347efc7f7304.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3f50599ec8c1c116ea9c347efc7f7304.png)'
- en: 'Figure 5: Samples from KVQA dataset [[26](#bib.bib26)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 5: KVQA 数据集样本 [[26](#bib.bib26)]。'
- en: 'CLEVR: CLEVR [[12](#bib.bib12)] is a synthetic dataset to test the visual understanding
    of the VQA systems. The dataset is generated using three objects in each image,
    namely cylinder, sphere and cube. These objects are in two different sizes, two
    different materials and placed in eight different colors. The questions are also
    synthetically generated based on the objects placed in the image. The dataset
    also accompanies the ground-truth bounding boxes for each object in the image.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'CLEVR: CLEVR [[12](#bib.bib12)] 是一个合成数据集，用于测试 VQA 系统的视觉理解。数据集生成的每张图像中包含三种物体，分别是圆柱体、球体和立方体。这些物体有两种不同的大小、两种不同的材料和八种不同的颜色。问题也是根据图像中的物体合成生成的。数据集还包含图像中每个物体的真实边界框。'
- en: 'Tally-QA: Very recently, in 2019, the Tally-QA [[1](#bib.bib1)] dataset is
    proposed which is the largest dataset of object counting in the open-ended task.
    The dataset includes both simple and complex question types which can be seen
    in Fig. [2](#S2 "2 Datasets ‣ Visual Question Answering using Deep Learning: A
    Survey and Performance Analysis"). The dataset is quite large in numbers as well
    as it is 2.5 times the VQA dataset. The dataset contains 287,907 questions, 165,000
    images and 19,000 complex questions. The Tally-QA samples are shown in Fig. [4](#S2.F4
    "Figure 4 ‣ 2 Datasets ‣ Visual Question Answering using Deep Learning: A Survey
    and Performance Analysis").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'KVQA: The recent interest in common-sense questions has led to the development
    of Knowledge based VQA dataset [[26](#bib.bib26)]. The dataset contains questions
    targeting various categories of nouns and also require world knowledge to arrive
    at a solution. Questions in this dataset require multi-entity, multi-relation,
    and multi- hop reasoning over large Knowledge Graphs (KG) to arrive at an answer.
    The dataset contains 24,000 images with 183,100 question-answer pairs employing
    around 18K proper nouns. The KVQA samples are shown in Fig. [5](#S2.F5 "Figure
    5 ‣ 2 Datasets ‣ Visual Question Answering using Deep Learning: A Survey and Performance
    Analysis").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Overview of Models described in this paper. The Pythia v0.1 is the
    best performing model over VQA dataset.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Dataset(s) | Method | Accuracy | Venue |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| Vanilla VQA [[2](#bib.bib2)] | VQA [[2](#bib.bib2)] | CNN + LSTM | 54.06
    (VQA) | ICCV 2015 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| Stacked Attention Networks [[33](#bib.bib33)] | VQA [[2](#bib.bib2)], DAQAUR
    [[20](#bib.bib20)], COCO-QA [[24](#bib.bib24)] | Multiple Attention Layers | 58.9
    (VQA), 46.2 (DAQAUR), 61.6 (COCO-QA) | CVPR 2016 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| Teney et al. [[31](#bib.bib31)] | VQA [[2](#bib.bib2)] | Faster-RCNN + Glove
    Vectors | 63.15 (VQA-v2) | CVPR 2018 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| Neural-Symbolic VQA [[34](#bib.bib34)] | CLEVR [[12](#bib.bib12)] | Symbolic
    Structure as Prior Knowledge | 99.8 (CLEVR) | NIPS 2018 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| FVTA [[17](#bib.bib17)] | MemexQA [[9](#bib.bib9)], MovieQA [[30](#bib.bib30)]
    | Attention over Sequential Data | 66.9 (MemexQA), 37.3 (MovieQA) | CVPR 2018
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Pythia v1.0 [[11](#bib.bib11)] | VQA [[2](#bib.bib2)] | Teney et al. [[31](#bib.bib31)]
    + Deep Layers | 72.27 (VQA-v2) | VQA Challenge 2018 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| Differential Networks [[32](#bib.bib32)] | VQA [[2](#bib.bib2)], TDIUC [[13](#bib.bib13)],
    COCO-QA [[24](#bib.bib24)] | Faster-RCNN, Differential Modules, GRU | 68.59 (VQA-v2),
    86.73 (TDIUC), 69.36 (COCO-QA) | AAAI 2019 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| GNN [[38](#bib.bib38)] | VisDial and VisDial-Q | Graph neural network | Recall:
    48.95 (VisDial), 27.15 (VisDial-Q) | CVPR 2019 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: 3 Deep Learning Based VQA Methods
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The emergence of deep-learning architectures have led to the development of
    the VQA systems. We discuss the state-of-the-art methods with an overview in Table
    [2](#S2.T2 "Table 2 ‣ 2 Datasets ‣ Visual Question Answering using Deep Learning:
    A Survey and Performance Analysis").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习架构的出现促使了 VQA 系统的发展。我们在表 [2](#S2.T2 "Table 2 ‣ 2 Datasets ‣ Visual Question
    Answering using Deep Learning: A Survey and Performance Analysis") 中讨论了最先进的方法的概述。'
- en: 'Vanilla VQA [[2](#bib.bib2)]: Considered as a benchmark for deep learning methods,
    the vanilla VQA model uses CNN for feature extraction and LSTM or Recurrent networks
    for language processing. These features are combined using element-wise operations
    to a common feature, which is used to classify to one of the answers as shown
    in Fig. [6](#S3.F6 "Figure 6 ‣ 3 Deep Learning Based VQA Methods ‣ Visual Question
    Answering using Deep Learning: A Survey and Performance Analysis").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vanilla VQA [[2](#bib.bib2)]：被认为是深度学习方法的基准，vanilla VQA 模型使用 CNN 进行特征提取，并使用
    LSTM 或递归网络进行语言处理。这些特征通过逐元素操作结合成一个公共特征，最终用于分类到其中一个答案，如图 [6](#S3.F6 "Figure 6 ‣
    3 Deep Learning Based VQA Methods ‣ Visual Question Answering using Deep Learning:
    A Survey and Performance Analysis") 所示。'
- en: 'Stacked Attention Networks [[33](#bib.bib33)]: This model introduced the attention
    using the softmax output of the intermediate question feature. The attention between
    the features are stacked which helps the model to focus on the important portion
    of the image.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠注意力网络 [[33](#bib.bib33)]：该模型引入了使用中间问题特征的 softmax 输出的注意力。特征之间的注意力被堆叠起来，这有助于模型关注图像的重要部分。
- en: 'Teney et al. Model [[31](#bib.bib31)]: Teney et al. introduced the use of object
    detection on VQA models and won the VQA Challenge 2017\. The model helps in narrowing
    down the features and apply better attention to images. The model employs the
    use of R-CNN architecture and showed significant performance in accuracy over
    other architectures. This model is depicted in Fig. [7](#S3.F7 "Figure 7 ‣ 3 Deep
    Learning Based VQA Methods ‣ Visual Question Answering using Deep Learning: A
    Survey and Performance Analysis").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'Teney 等人的模型 [[31](#bib.bib31)]：Teney 等人引入了在 VQA 模型中使用物体检测的方法，并赢得了 2017 年 VQA
    挑战赛。该模型有助于缩小特征范围，并对图像施加更好的注意力。该模型采用了 R-CNN 架构，并在准确性上表现出了比其他架构更显著的性能。该模型如图 [7](#S3.F7
    "Figure 7 ‣ 3 Deep Learning Based VQA Methods ‣ Visual Question Answering using
    Deep Learning: A Survey and Performance Analysis") 所示。'
- en: 'Neural-Symbolic VQA [[34](#bib.bib34)]: Specifically made for CLEVR dataset,
    this model leverages the question formation and image generation strategy of CLEVR.
    The images are converted to structured features and the question features are
    converted to their original root question strategy. This feature is used to filter
    out the required answer.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 神经符号 VQA [[34](#bib.bib34)]：专门为 CLEVR 数据集制作的，该模型利用了 CLEVR 的问题生成和图像生成策略。图像被转换为结构化特征，而问题特征被转换为其原始的根问题策略。这个特征用于筛选出所需的答案。
- en: 'Focal Visual Text Attention (FVTA) [[17](#bib.bib17)]: This model combines
    the sequence of image features generated by the network, text features of the
    image (or probable answers) and the question. It applies the attention based on
    the both text components, and finally classifies the features to answer the question.
    This model is better suited for the VQA in videos which has more use cases than
    images. This model is shown in Fig. [8](#S3.F8 "Figure 8 ‣ 3 Deep Learning Based
    VQA Methods ‣ Visual Question Answering using Deep Learning: A Survey and Performance
    Analysis").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '焦点视觉文本注意力 (FVTA) [[17](#bib.bib17)]：该模型结合了网络生成的图像特征序列、图像的文本特征（或可能的答案）以及问题。它基于这两个文本组件应用注意力，最终将特征分类以回答问题。该模型更适合用于视频中的
    VQA，因为视频比图像有更多的应用场景。该模型如图 [8](#S3.F8 "Figure 8 ‣ 3 Deep Learning Based VQA Methods
    ‣ Visual Question Answering using Deep Learning: A Survey and Performance Analysis")
    所示。'
- en: '![Refer to caption](img/424a7752c941a51dfdc3da972fa99839.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/424a7752c941a51dfdc3da972fa99839.png)'
- en: 'Figure 6: Vanilla VQA Network Model [[2](#bib.bib2)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Vanilla VQA 网络模型 [[2](#bib.bib2)]。
- en: '![Refer to caption](img/98bbacf99b55dc9822f051de83486d64.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/98bbacf99b55dc9822f051de83486d64.png)'
- en: 'Figure 7: Teney et al. VQA Model [[31](#bib.bib31)]'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：Teney 等人的 VQA 模型 [[31](#bib.bib31)]
- en: '![Refer to caption](img/d32785bc9d21b4aaa6bcac77dccf1a67.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d32785bc9d21b4aaa6bcac77dccf1a67.png)'
- en: 'Figure 8: Focal Visual Text Attention Model [[17](#bib.bib17)]'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：焦点视觉文本注意力模型 [[17](#bib.bib17)]
- en: '![Refer to caption](img/76adab22b21538bada6ceac6a98fb9fd.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/76adab22b21538bada6ceac6a98fb9fd.png)'
- en: 'Figure 9: Differential Networks Model [[32](#bib.bib32)].'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：差分网络模型 [[32](#bib.bib32)]。
- en: 'Pythia v1.0 [[11](#bib.bib11)]: Pythia v1.0 is the award winning architecture
    for VQA Challenge 2018²²2[https://github.com/facebookresearch/pythia](https://github.com/facebookresearch/pythia).
    The architecture is similar to Teney et al. [[31](#bib.bib31)] with reduced computations
    with element-wise multiplication, use of GloVe vectors [[23](#bib.bib23)], and
    ensemble of 30 models.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pythia v1.0 [[11](#bib.bib11)]: Pythia v1.0 是2018年VQA挑战赛的获奖架构²²2[https://github.com/facebookresearch/pythia](https://github.com/facebookresearch/pythia)。该架构类似于Teney等人提出的
    [[31](#bib.bib31)]，通过逐元素乘法减少计算量，使用GloVe向量 [[23](#bib.bib23)]，并集成了30个模型。'
- en: 'Differential Networks [[32](#bib.bib32)]: This model uses the differences between
    forward propagation steps to reduce the noise and to learn the interdependency
    between features. Image features are extracted using Faster-RCNN [[25](#bib.bib25)].
    The differential modules [[22](#bib.bib22)] are used to refine the features in
    both text and images. GRU [[4](#bib.bib4)] is used for question feature extraction.
    Finally, it is combined with an attention module to classify the answers. The
    Differential Networks architecture is illustrated in Fig. [9](#S3.F9 "Figure 9
    ‣ 3 Deep Learning Based VQA Methods ‣ Visual Question Answering using Deep Learning:
    A Survey and Performance Analysis").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '差分网络 [[32](#bib.bib32)]: 该模型使用前向传播步骤之间的差异来减少噪声，并学习特征之间的相互依赖性。图像特征通过Faster-RCNN
    [[25](#bib.bib25)] 提取。差分模块 [[22](#bib.bib22)] 用于精细化文本和图像中的特征。GRU [[4](#bib.bib4)]
    用于提取问题特征。最终，它与注意力模块结合以分类答案。差分网络架构如图 [9](#S3.F9 "Figure 9 ‣ 3 Deep Learning Based
    VQA Methods ‣ Visual Question Answering using Deep Learning: A Survey and Performance
    Analysis") 所示。'
- en: '![Refer to caption](img/e7df2f3759b634a45a1f92c7ee335692.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e7df2f3759b634a45a1f92c7ee335692.png)'
- en: 'Figure 10: Differentiable Graph Neural Network [[38](#bib.bib38)].'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '图10: 可微分图神经网络 [[38](#bib.bib38)]。'
- en: 'Differentiable Graph Neural Network (GNN) [[38](#bib.bib38)]: Recently, Zheng
    at al. have discussed about a new way to model visual dialogs as structural graph
    and Markov Random Field. They have considered the dialog entities as the observed
    nodes with answer as a node with missing value. This model is illustrated in Fig.
    [10](#S3.F10 "Figure 10 ‣ 3 Deep Learning Based VQA Methods ‣ Visual Question
    Answering using Deep Learning: A Survey and Performance Analysis").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '可微分图神经网络 (GNN) [[38](#bib.bib38)]: 最近，Zheng等人讨论了一种将视觉对话建模为结构图和马尔可夫随机场的新方法。他们将对话实体视为观测节点，将答案视为缺失值的节点。该模型如图
    [10](#S3.F10 "Figure 10 ‣ 3 Deep Learning Based VQA Methods ‣ Visual Question
    Answering using Deep Learning: A Survey and Performance Analysis") 所示。'
- en: 4 Experimental Results and Analysis
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验结果与分析
- en: 'The reported results for different methods over different datasets are summarized
    in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Visual Question Answering using
    Deep Learning: A Survey and Performance Analysis") and Table [2](#S2.T2 "Table
    2 ‣ 2 Datasets ‣ Visual Question Answering using Deep Learning: A Survey and Performance
    Analysis"). It can be observed that VQA dataset is very commonly used by different
    methods to test the performance. Other datasets like Visual7W, Tally-QA and KVQA
    are also very challenging and recent datasets. It can be also seen that the Pythia
    v1.0 is one of the recent methods performing very well over VQA dataset. The Differential
    Network is the very recent method proposed for VQA task and shows very promising
    performance over different datasets.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '不同方法在不同数据集上的报告结果汇总在表 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Visual Question
    Answering using Deep Learning: A Survey and Performance Analysis") 和表 [2](#S2.T2
    "Table 2 ‣ 2 Datasets ‣ Visual Question Answering using Deep Learning: A Survey
    and Performance Analysis") 中。可以观察到，VQA数据集被不同方法广泛使用来测试性能。其他数据集如Visual7W、Tally-QA和KVQA也非常具有挑战性且较为新颖。同时可以看到，Pythia
    v1.0 是在VQA数据集上表现非常出色的近期方法之一。差分网络是最近提出的一种VQA任务方法，在不同数据集上的表现也非常有前景。'
- en: As part of this survey, we also implemented different methods over different
    datasets and performed the experiments. We considered the following three models
    for our experiments, 1) the baseline Vanilla VQA model [[2](#bib.bib2)] which
    uses the VGG16 CNN architecture [[28](#bib.bib28)] and LSTMs [[7](#bib.bib7)],
    2) the Stacked Attention Networks [[33](#bib.bib33)] architecture, and 3) the
    2017 VQA challenge winner Teney et al. model [[31](#bib.bib31)]. We considered
    the widely adapted datasets such as standard VQA dataset [[2](#bib.bib2)] and
    Visual7W dataset [[39](#bib.bib39)] for the experiments. We used the Adam Optimizer
    for all models with Cross-Entropy loss function. Each model is trained for 100
    epochs for each dataset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这项调查的一部分，我们还在不同的数据集上实现了不同的方法并进行了实验。我们考虑了以下三种模型进行实验：1）基础的 Vanilla VQA 模型 [[2](#bib.bib2)]，它使用
    VGG16 CNN 架构 [[28](#bib.bib28)] 和 LSTM [[7](#bib.bib7)]；2）堆叠注意力网络 [[33](#bib.bib33)]
    架构；3）2017 年 VQA 挑战赛获胜者 Teney 等模型 [[31](#bib.bib31)]。我们选择了广泛采用的数据集，如标准 VQA 数据集
    [[2](#bib.bib2)] 和 Visual7W 数据集 [[39](#bib.bib39)] 进行实验。我们对所有模型使用了 Adam 优化器和交叉熵损失函数。每个模型在每个数据集上训练了
    100 个周期。
- en: 'Table 3: The accuracies obtained using Vanilla VQA [[2](#bib.bib2)], Stacked
    Attention Networks [[33](#bib.bib33)] and Teney et al. [[31](#bib.bib31)] models
    when trained on VQA [[2](#bib.bib2)] and Visual7W [[39](#bib.bib39)] datasets.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用 Vanilla VQA [[2](#bib.bib2)]、堆叠注意力网络 [[33](#bib.bib33)] 和 Teney 等 [[31](#bib.bib31)]
    模型在 VQA [[2](#bib.bib2)] 和 Visual7W [[39](#bib.bib39)] 数据集上训练得到的准确率。
- en: '| Model Name | Accuracy |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 准确率 |'
- en: '| VQA Dataset | Visual7W Dataset |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| VQA 数据集 | Visual7W 数据集 |'
- en: '| CNN + LSTM | 58.11 | 56.93 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| CNN + LSTM | 58.11 | 56.93 |'
- en: '| Stacked Attention Networks | 60.49 | 61.67 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 堆叠注意力网络 | 60.49 | 61.67 |'
- en: '| Teney et al. | 67.23 | 65.82 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Teney 等 | 67.23 | 65.82 |'
- en: 'The experimental results are presented in Table [3](#S4.T3 "Table 3 ‣ 4 Experimental
    Results and Analysis ‣ Visual Question Answering using Deep Learning: A Survey
    and Performance Analysis") in terms of the accuracy for three models over two
    datasets. In the experiments, we found that the Teney et al. [[31](#bib.bib31)]
    is the best performing model on both VQA and Visual7W Dataset. The accuracies
    obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W
    datasets for the open-ended question-answering task, respectively. The above results
    re-affirmed that the Teney et al. model is the best performing model till 2018
    which has been pushed by Pythia v1.0 [[10](#bib.bib10)], recently, where they
    have utilized the same model with more layers to boost the performance. The accuracy
    for VQA is quite low due to the nature of this problem. VQA is one of the hard
    problems of computer vision, where the network has to understand the semantics
    of images, questions and relation in feature space.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果以表 [3](#S4.T3 "Table 3 ‣ 4 Experimental Results and Analysis ‣ Visual Question
    Answering using Deep Learning: A Survey and Performance Analysis") 的形式呈现，展示了三种模型在两个数据集上的准确率。在实验中，我们发现
    Teney 等 [[31](#bib.bib31)] 是 VQA 和 Visual7W 数据集上表现最好的模型。Teney 等模型在 VQA 和 Visual7W
    数据集上的开放式问答任务的准确率分别为 67.23% 和 65.82%。上述结果再次确认了 Teney 等模型是 2018 年之前表现最好的模型，而最近由
    Pythia v1.0 [[10](#bib.bib10)] 推出了该模型的改进版，其中增加了更多层以提升性能。由于问题的性质，VQA 的准确率相对较低。VQA
    是计算机视觉中的一个难题，网络必须理解图像、问题及特征空间中的关系的语义。'
- en: 5 Conclusion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: The Visual Question Answering has recently witnessed a great interest and development
    by the group of researchers and scientists from all around the world. The recent
    trends are observed in the area of developing more and more real life looking
    datasets by incorporating the real world type questions and answers. The recent
    trends are also seen in the area of development of sophisticated deep learning
    models by better utilizing the visual cues as well as textual cues by different
    means. The performance of the best model is still lagging and around 60-70% only.
    Thus, it is still an open problem to develop better deep learning models as well
    as more challenging datasets for VQA. Different strategies like object level details,
    segmentation masks, deeper models, sentiment of the question, etc. can be considered
    to develop the next generation VQA models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答（Visual Question Answering）最近引起了全球研究人员和科学家的极大兴趣和发展。最近的趋势表现在开发越来越多逼真的数据集，结合真实世界的问题和答案。在深度学习模型的发展领域中，也观察到趋势，利用视觉线索和文本线索进行更好的处理。最佳模型的性能仍然滞后，大约仅为60-70%。因此，开发更好的深度学习模型以及更具挑战性的数据集仍然是一个开放的问题。可以考虑不同的策略，如对象级细节、分割掩码、更深的模型、问题的情感等，以开发下一代VQA模型。
- en: References
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Acharya, M., Kafle, K., Kanan, C.: Tallyqa: Answering complex counting
    questions. arXiv preprint arXiv:1810.12440 (2018)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Acharya, M., Kafle, K., Kanan, C.: Tallyqa: 回答复杂计数问题。arXiv 预印本 arXiv:1810.12440
    (2018)'
- en: '[2] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick,
    C., Parikh, D.: Vqa: Visual question answering. In: IEEE ICCV, pp. 2425–2433 (2015)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick,
    C., Parikh, D.: VQA: 视觉问答。发表于IEEE ICCV，页码 2425–2433 (2015)'
- en: '[3] Chen, L., Yan, X., Xiao, J., Zhang, H., Pu, S., Zhuang, Y.: Counterfactual
    samples synthesizing for robust visual question answering. In: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10,800–10,809
    (2020)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Chen, L., Yan, X., Xiao, J., Zhang, H., Pu, S., Zhuang, Y.: 针对鲁棒视觉问答的反事实样本合成。发表于IEEE/CVF计算机视觉与模式识别会议，页码
    10,800–10,809 (2020)'
- en: '[4] Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated
    recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555
    (2014)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: 门控递归神经网络在序列建模中的实证评估。arXiv
    预印本 arXiv:1412.3555 (2014)'
- en: '[5] Gupta, S., Arbelaez, P., Malik, J.: Perceptual organization and recognition
    of indoor scenes from rgb-d images. In: IEEE CVPR, pp. 564–571 (2013)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Gupta, S., Arbelaez, P., Malik, J.: 从RGB-D图像中感知组织和识别室内场景。发表于IEEE CVPR，页码
    564–571 (2013)'
- en: '[6] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
    In: IEEE CVPR, pp. 770–778 (2016)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] He, K., Zhang, X., Ren, S., Sun, J.: 图像识别的深度残差学习。发表于IEEE CVPR，页码 770–778
    (2016)'
- en: '[7] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation
    9(8), 1735–1780 (1997)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Hochreiter, S., Schmidhuber, J.: 长短期记忆。神经计算 9(8), 1735–1780 (1997)'
- en: '[8] Huang, Q., Wei, J., Cai, Y., Zheng, C., Chen, J., Leung, H.f., Li, Q.:
    Aligned dual channel graph convolutional network for visual question answering.
    In: Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pp. 7166–7176 (2020)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Huang, Q., Wei, J., Cai, Y., Zheng, C., Chen, J., Leung, H.f., Li, Q.:
    对齐的双通道图卷积网络用于视觉问答。发表于第58届计算语言学协会年会，页码 7166–7176 (2020)'
- en: '[9] Jiang, L., Liang, J., Cao, L., Kalantidis, Y., Farfade, S., Hauptmann,
    A.: Memexqa: Visual memex question answering. arXiv preprint arXiv:1708.01336
    (2017)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jiang, L., Liang, J., Cao, L., Kalantidis, Y., Farfade, S., Hauptmann,
    A.: Memexqa: 视觉记忆问答。arXiv 预印本 arXiv:1708.01336 (2017)'
- en: '[10] Jiang, Y., Natarajan, V., Chen, X., Rohrbach, M., Batra, D., Parikh, D.:
    Pythia v0\. 1: the winning entry to the vqa challenge 2018. arXiv preprint arXiv:1807.09956
    (2018)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Jiang, Y., Natarajan, V., Chen, X., Rohrbach, M., Batra, D., Parikh, D.:
    Pythia v0\. 1: VQA挑战赛2018的获胜作品。arXiv 预印本 arXiv:1807.09956 (2018)'
- en: '[11] Jiang, Y., Natarajan, V., Chen, X., Rohrbach, M., Batra, D., Parikh, D.:
    Pythia v0\. 1: the winning entry to the vqa challenge 2018. arXiv preprint arXiv:1807.09956
    (2018)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jiang, Y., Natarajan, V., Chen, X., Rohrbach, M., Batra, D., Parikh, D.:
    Pythia v0\. 1: VQA挑战赛2018的获胜作品。arXiv 预印本 arXiv:1807.09956 (2018)'
- en: '[12] Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick,
    C., Girshick, R.: Clevr: A diagnostic dataset for compositional language and elementary
    visual reasoning. In: IEEE CVPR, pp. 2901–2910 (2017)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence
    Zitnick, C., Girshick, R.: CLEVR: 组合语言和基础视觉推理的诊断数据集。发表于IEEE CVPR，页码 2901–2910
    (2017)'
- en: '[13] Kafle, K., Kanan, C.: An analysis of visual question answering algorithms.
    In: ICCV (2017)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with
    deep convolutional neural networks. In: NIPS, pp. 1097–1105 (2012)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Li, G., Wang, X., Zhu, W.: Boosting visual question answering with context-aware
    knowledge aggregation. In: Proceedings of the 28th ACM International Conference
    on Multimedia, pp. 1227–1235 (2020)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Li, W., Sun, J., Liu, G., Zhao, L., Fang, X.: Visual question answering
    with attention transfer and a cross-modal gating mechanism. Pattern Recognition
    Letters 133, 334–340 (2020)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Liang, J., Jiang, L., Cao, L., Li, L.J., Hauptmann, A.G.: Focal visual-text
    attention for visual question answering. In: IEEE CVPR, pp. 6135–6143 (2018)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
    Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV,
    pp. 740–755 (2014)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Lobry, S., Marcos, D., Murray, J., Tuia, D.: Rsvqa: Visual question answering
    for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing (2020)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Malinowski, M., Fritz, M.: A multi-world approach to question answering
    about real-world scenes based on uncertain input. In: NIPS, pp. 1682–1690 (2014)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Medsker, L.R., Jain, L.: Recurrent neural networks. Design and Applications
    5 (2001)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Patro, B., Namboodiri, V.P.: Differential attention for visual question
    answering. In: IEEE CVPR, pp. 7680–7688 (2018)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Pennington, J., Socher, R., Manning, C.: Glove: Global vectors for word
    representation. In: EMNLP, pp. 1532–1543 (2014)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Ren, M., Kiros, R., Zemel, R.: Exploring models and data for image question
    answering. In: Advances in neural information processing systems, pp. 2953–2961
    (2015)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time
    object detection with region proposal networks. In: NIPS, pp. 91–99 (2015)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Shah, S., Mishra, A., Yadati, N., Talukdar, P.P.: Kvqa: Knowledge-aware
    visual question answering. In: AAAI (2019)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and
    support inference from rgbd images. In: ECCV, pp. 746–760 (2012)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
    image recognition. arXiv preprint arXiv:1409.1556 (2014)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking
    the inception architecture for computer vision. In: IEEE CVPR, pp. 2818–2826 (2016)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler,
    S.: Movieqa: Understanding stories in movies through question-answering. In: IEEE
    CVPR, pp. 4631–4640 (2016)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Teney, D., Anderson, P., He, X., van den Hengel, A.: Tips and tricks for
    visual question answering: Learnings from the 2017 challenge. In: IEEE CVPR, pp.
    4223–4232 (2018)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Wu, C., Liu, J., Wang, X., Li, R.: Differential networks for visual question
    answering. AAAI 2019 (2019)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Yang, Z., He, X., Gao, J., Deng, L., Smola, A.: Stacked attention networks
    for image question answering. In: IEEE CVPR, pp. 21–29 (2016)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Yang, Z., He, X., Gao, J., Deng, L., Smola, A.: 图像问答的堆叠注意力网络。发表于：IEEE
    CVPR，第21–29页（2016）'
- en: '[34] Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., Tenenbaum, J.: Neural-symbolic
    vqa: Disentangling reasoning from vision and language understanding. In: NIPS,
    pp. 1031–1042 (2018)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., Tenenbaum, J.: 神经符号视觉问答：将推理与视觉和语言理解解开。发表于：NIPS，第1031–1042页（2018）'
- en: '[35] Yu, J., Zhu, Z., Wang, Y., Zhang, W., Hu, Y., Tan, J.: Cross-modal knowledge
    reasoning for knowledge-based visual question answering. Pattern Recognition 108,
    107,563 (2020)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Yu, J., Zhu, Z., Wang, Y., Zhang, W., Hu, Y., Tan, J.: 基于知识的视觉问答的跨模态知识推理。模式识别108，第107,563页（2020）'
- en: '[36] Yu, L., Park, E., Berg, A.C., Berg, T.L.: Visual madlibs: Fill in the
    blank description generation and question answering. In: IEEE ICCV, pp. 2461–2469
    (2015)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Yu, L., Park, E., Berg, A.C., Berg, T.L.: 视觉填空：填补空白的描述生成与问答。发表于：IEEE ICCV，第2461–2469页（2015）'
- en: '[37] Zhan, L.M., Liu, B., Fan, L., Chen, J., Wu, X.M.: Medical visual question
    answering via conditional reasoning. In: Proceedings of the 28th ACM International
    Conference on Multimedia, pp. 2345–2354 (2020)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Zhan, L.M., Liu, B., Fan, L., Chen, J., Wu, X.M.: 通过条件推理进行医学视觉问答。发表于：第28届ACM国际多媒体会议论文集，第2345–2354页（2020）'
- en: '[38] Zheng, Z., Wang, W., Qi, S., Zhu, S.C.: Reasoning visual dialogs with
    structural and partial observations. In: Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 6669–6678 (2019)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Zheng, Z., Wang, W., Qi, S., Zhu, S.C.: 通过结构化和部分观察推理视觉对话。发表于：IEEE计算机视觉与模式识别会议论文集，第6669–6678页（2019）'
- en: '[39] Zhu, Y., Groth, O., Bernstein, M., Fei-Fei, L.: Visual7w: Grounded question
    answering in images. In: IEEE CVPR, pp. 4995–5004 (2016)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Zhu, Y., Groth, O., Bernstein, M., Fei-Fei, L.: Visual7w：图像中的有根据的问答。发表于：IEEE
    CVPR，第4995–5004页（2016）'
