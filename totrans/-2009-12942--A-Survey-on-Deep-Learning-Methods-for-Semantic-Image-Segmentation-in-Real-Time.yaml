- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:59:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2009.12942] A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2009.12942] 实时语义图像分割中的深度学习方法综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.12942](https://ar5iv.labs.arxiv.org/html/2009.12942)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2009.12942](https://ar5iv.labs.arxiv.org/html/2009.12942)
- en: A Survey on Deep Learning Methods for Semantic Image Segmentation in Real-Time
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时语义图像分割中的深度学习方法综述
- en: Georgios Takos
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Georgios Takos
- en: Mountain View, CA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 山景城，加州
- en: georgios.takos@gmail.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: georgios.takos@gmail.com
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Semantic image segmentation is one of fastest growing areas in computer vision
    with a variety of applications. In many areas, such as robotics and autonomous
    vehicles, semantic image segmentation is crucial, since it provides the necessary
    context for actions to be taken based on a scene understanding at the pixel level.
    Moreover, the success of medical diagnosis and treatment relies on the extremely
    accurate understanding of the data under consideration and semantic image segmentation
    is one of the important tools in many cases. Recent developments in deep learning
    have provided a host of tools to tackle this problem efficiently and with increased
    accuracy. This work provides a comprehensive analysis of state-of-the-art deep
    learning architectures in image segmentation and, more importantly, an extensive
    list of techniques to achieve fast inference and computational efficiency. The
    origins of these techniques as well as their strengths and trade-offs are discussed
    with an in-depth analysis of their impact in the area. The best-performing architectures
    are summarized with a list of methods used to achieve these state-of-the-art results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 语义图像分割是计算机视觉中增长最快的领域之一，具有广泛的应用。在许多领域，例如机器人技术和自动驾驶车辆，语义图像分割至关重要，因为它提供了基于像素级场景理解采取行动所需的背景。此外，医疗诊断和治疗的成功依赖于对所考虑数据的极其准确的理解，而语义图像分割在许多情况下是重要的工具。深度学习的最新进展提供了一系列工具，以高效且更准确地解决这一问题。这项工作提供了对图像分割中最新深度学习架构的全面分析，更重要的是，提供了一份详细的技术列表，以实现快速推断和计算效率。这些技术的起源以及它们的优缺点进行了深入分析，并讨论了它们在该领域的影响。最佳表现架构的总结及实现这些最先进结果的方法列表。
- en: '*K*eywords Semantic Image Segmentation  $\cdot$ Real-Time Segmentation  $\cdot$
    Deep Learning  $\cdot$ Convolutional Networks'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 语义图像分割  $\cdot$ 实时分割  $\cdot$ 深度学习  $\cdot$ 卷积网络'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Semantic segmentation is one of the fastest growing areas in Computer Vision
    and Machine Learning. The availability of cameras and over devices have dramatically
    increased the interest in better understanding the context of the scene they are
    capturing and image segmentation is one of the most important components of this
    process. When an image is analyzed the following levels of understanding are sought:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割是计算机视觉和机器学习中增长最快的领域之一。相机和设备的普及大大提高了人们对更好地理解捕捉场景背景的兴趣，而图像分割是这一过程中的重要组成部分。当分析图像时，寻求以下几个层次的理解：
- en: '1.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Classification, i.e., label the most prominent object of an image [[1](#bib.bib1)].
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类，即标记图像中最显著的物体 [[1](#bib.bib1)]。
- en: '2.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Classification with localization, i.e., extend the previous solution with a
    bounding box of the object in question.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有定位的分类，即在前述解决方案的基础上扩展，增加目标物体的边界框。
- en: '3.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Object detection, where multiple objects of different types are classified and
    localized [[2](#bib.bib2)].
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物体检测，即对多种类型的物体进行分类和定位 [[2](#bib.bib2)]。
- en: '4.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Semantic segmentation, where every pixel in the image is classified and localized.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语义分割，即对图像中的每个像素进行分类和定位。
- en: '5.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Instance segmentation, an extension to semantic segmentation where different
    objects of the same type are treated as distinct objects.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实例分割，是语义分割的扩展，其中将相同类型的不同物体视为独立的物体。
- en: '6.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Panoptic segmentation, which combines semantic and instance segmentation such
    that all pixels are assigned a class label and all object instances are uniquely
    segmented.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全景分割，将语义分割和实例分割相结合，使所有像素都被分配一个类别标签，并且所有物体实例都被唯一地分割。
- en: The focus of this work is semantic image segmentation, where a pixel-level classification
    is targeted, and where image pixels which belong to the same object class are
    clustered together. An example of this pixel-level classification can be seen
    in Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning
    Methods for Semantic Image Segmentation in Real-Time") and Figure [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time") (see [[3](#bib.bib3)]). The original image on
    the left (Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ A Survey on
    Deep Learning Methods for Semantic Image Segmentation in Real-Time")) can be compared
    to the semantic segmentation target is the one on the right (Figure [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time")), where all objects of interest have been classified.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的重点是语义图像分割，目标是像素级分类，将属于同一对象类别的图像像素聚集在一起。图像[1(a)](#S1.F1.sf1 "在图1 ‣ 1 引言 ‣
    语义图像分割的深度学习方法综述")和图像[1(b)](#S1.F1.sf2 "在图1 ‣ 1 引言 ‣ 语义图像分割的深度学习方法综述")中可以看到这种像素级分类的一个示例（见[[3](#bib.bib3)]）。左侧的原始图像（图像[1(a)](#S1.F1.sf1
    "在图1 ‣ 1 引言 ‣ 语义图像分割的深度学习方法综述")）可以与右侧的语义分割目标（图像[1(b)](#S1.F1.sf2 "在图1 ‣ 1 引言 ‣
    语义图像分割的深度学习方法综述")）进行比较，其中所有感兴趣的对象已被分类。
- en: '![Refer to caption](img/9a42ad72ae890248e586aae365ea9190.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9a42ad72ae890248e586aae365ea9190.png)'
- en: (a) Original image
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始图像
- en: '![Refer to caption](img/5044b910cd87513857c901eac9539e81.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5044b910cd87513857c901eac9539e81.png)'
- en: (b) Semantic segmentation ground truth
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 语义分割的实际结果
- en: 'Figure 1: PASCAL VOC trainining images'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：PASCAL VOC 训练图像
- en: 'Semantic segmentation plays an imprortant role in diverse applications such
    as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割在各种应用中扮演着重要角色，例如：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Medical image diagnosis [[4](#bib.bib4)], [[5](#bib.bib5)].
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医学图像诊断 [[4](#bib.bib4)], [[5](#bib.bib5)]。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Autonomous driving [[6](#bib.bib6)], [[7](#bib.bib7)].
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动驾驶 [[6](#bib.bib6)], [[7](#bib.bib7)]。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Satellite image processing [[8](#bib.bib8)].
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卫星图像处理 [[8](#bib.bib8)]。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Environmental analysis [[9](#bib.bib9)].
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 环境分析 [[9](#bib.bib9)]。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Agricultural development [[10](#bib.bib10)].
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 农业发展 [[10](#bib.bib10)]。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Image search engines [[11](#bib.bib11)].
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像搜索引擎 [[11](#bib.bib11)]。
- en: In this paper we provide a comprehensive summary of the most recent developments
    in the area of semantic segmentation with a focus on real-time systems. Efficient
    techniques for semantic segmentation, where memory requirements and inference
    time are the main consideration, have not been sufficiently summarized before
    to the best of the author’s knowledge and have been instrumental for the increasing
    popularity of semantic segmentation across different fields.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了有关语义分割领域最新发展的全面总结，重点关注实时系统。根据作者的知识，关于语义分割的高效技术，特别是在内存需求和推理时间方面的考虑，之前并未得到充分总结，并且这些技术对于语义分割在各个领域的日益普及起到了重要作用。
- en: 'This paper is organized as follows: in Section 2 the evolution of traditional
    image segmentation methods is summarized, followed by, in Section 3, a comprehensive
    summary of deep learning approaches. Section 4 summarizes the most seminal works
    on real-time systems, while analyzing the most effective techniques in terms of
    computational cost and memory load. The following Section enumerates the different
    datasets that have been used to benchmark different architectures, followed by
    a Section on the metrics used in the evaluation. The paper concludes with a summary
    of the performance of different real-time architectures.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的组织结构如下：第2节总结了传统图像分割方法的发展，然后在第3节中，对深度学习方法进行了全面总结。第4节总结了实时系统中最具开创性的工作，同时分析了在计算成本和内存负载方面最有效的技术。接下来的节列举了用于基准测试不同架构的不同数据集，接着是关于评估中使用的指标的部分。文章最后总结了不同实时架构的性能。
- en: 2 History of Semantic Segmentation
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 语义分割的历史
- en: One of the earlier approaches to semantic segmentation is thresholding [[12](#bib.bib12)],
    [[13](#bib.bib13)]. It attempts to divide the image to two regions, the target
    and the background. It works quite well in gray-level images that can be classified
    in a straightforward manner by using a single threshold. This technique has evolved
    by taking both local and global threshold values to better capture the image features.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的语义分割方法之一是阈值分割[[12](#bib.bib12)], [[13](#bib.bib13)]。它试图将图像分成两个区域：目标区域和背景区域。在可以通过单一阈值直接分类的灰度图像中，这种方法效果相当好。这项技术已经通过采用局部和全局阈值的方式演变，以更好地捕捉图像特征。
- en: A second technique involves clustering of pixels or regions with similar characteristics,
    where the image is split into $K$ groups or clusters. All pixels are the assigned
    a cluster based on a similarity metric that can involve the pixel features (e.g.
    color, gradient) as well as the relative distance [[14](#bib.bib14)]. Several
    popular segmentation techniques have been successfully applied, such as K-means
    [[15](#bib.bib15)], GMMs [[16](#bib.bib16)], mean-shift [[17](#bib.bib17)], and
    fuzzy k-means [[18](#bib.bib18)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种技术涉及对具有相似特征的像素或区域进行聚类，其中图像被分成$K$个组或簇。所有像素根据相似性度量被分配到一个簇中，这些度量可以涉及像素特征（如颜色、梯度）以及相对距离[[14](#bib.bib14)]。许多流行的分割技术已成功应用，如K均值[[15](#bib.bib15)],
    GMMs[[16](#bib.bib16)], 均值漂移[[17](#bib.bib17)]和模糊K均值[[18](#bib.bib18)]。
- en: Edge detection methods [[19](#bib.bib19)], have used the fact the edges frequently
    represent boundaries that can help in segmenting the image. Different edge types
    have been used (e.g. step edges, ramp edges, line edges and roof edges). Most
    popular line edge detection methods include Roberts edge detection [[20](#bib.bib20)],
    Sobel edge detection [[21](#bib.bib21)], and Prewitt edge detection[[22](#bib.bib22)],
    which utilize different two-dimensional masks that when convolved with the image
    will highlight the edges.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘检测方法[[19](#bib.bib19)]利用了边缘通常表示可以帮助分割图像的边界这一事实。使用了不同类型的边缘（例如，阶跃边缘、坡度边缘、线边缘和屋顶边缘）。最流行的线边缘检测方法包括Roberts边缘检测[[20](#bib.bib20)]，Sobel边缘检测[[21](#bib.bib21)]和Prewitt边缘检测[[22](#bib.bib22)]，这些方法利用不同的二维掩模，与图像卷积时可以突出边缘。
- en: A fourth approach looks at images like graphs, where each pixel is a vertex
    connected with all other pixels, with the weight of each edge measuring the similarity
    between the pixels. Similarity measures can use features such as distance, intensity,
    color, and texture to calculate the edge weights. Image segmentation is then treated
    like a graph partitioning problem, where graph segments are partitioned based
    on the similarity of the groups [[23](#bib.bib23)] – [[25](#bib.bib25)]. An affinity
    matrix is computed and the solution to the graph cut problem is given by the generalized
    eigenvalue of the matrix.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第四种方法将图像视为图形，其中每个像素都是一个顶点，与所有其他像素连接，每条边的权重测量像素之间的相似性。相似性度量可以使用距离、强度、颜色和纹理等特征来计算边权重。图像分割被视为图分割问题，图的分段基于组的相似性[[23](#bib.bib23)]
    – [[25](#bib.bib25)]。计算亲和矩阵，图割问题的解由矩阵的广义特征值给出。
- en: Conditional Random Fields (CRF), a probabilistic framework that can be used
    to label and segment data, have been used extensively in image segmentation. In
    this framework, every pixel (that can belong to any of the target class) is assigned
    a unary cost, i.e., the price to assign a pixel to a class. In addition, a pair-wise
    cost is added that can model interactions between pixels. For example, zero cost
    can be assigned when two neighboring pixels belong to the same class, but non-zero
    cost when the pixels belong to different classes. The unary costs capture the
    cost for disregarding a class annotation, while the latter penalize non-smooth
    regions. The goal of the CRF is to find a configuration where the overall cost
    is minimized. An excellent explanation of CRFs is in [[26](#bib.bib26)], whereas
    applications in semantic segmentation can be found in [[27](#bib.bib27)], [[28](#bib.bib28)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 条件随机场（CRF），一种可以用于标记和分割数据的概率框架，已广泛应用于图像分割。在这个框架中，每个像素（可能属于任何目标类别）被分配一个单一代价，即将像素分配给某个类别的代价。此外，还增加了一个对偶代价，可以建模像素之间的交互。例如，当两个相邻像素属于同一类别时，可以分配零代价，而当像素属于不同类别时则分配非零代价。单一代价捕捉了忽略类别注释的代价，而后者则惩罚不光滑的区域。CRF的目标是找到一个总体代价最小化的配置。对CRF的优秀解释见[[26](#bib.bib26)]，而在语义分割中的应用可以参考[[27](#bib.bib27)],
    [[28](#bib.bib28)]。
- en: 3 Deep Learning Approaches to Semantic Image Segmentation
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习方法在语义图像分割中的应用
- en: 3.1 Fully Convolutional Networks
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 全卷积网络
- en: 'Convolution networks were initially used for classification tasks (AlexNet
    [[1](#bib.bib1)], VGG [[29](#bib.bib29)], GoogLeNet [[30](#bib.bib30)]). These
    networks first processed the input image with several convolutional layers with
    increasing number of filters and decreasing resolution, with the last convolutional
    layer vectorized. The vectorized features were followed by fully connected layers
    that learn the probability distribution of the classes with a softmax output layer.
    In FCN [[31](#bib.bib31)], the fully connected layers that result in loss of the
    spatial information were removed from popular architectures ([[1](#bib.bib1)],
    [[29](#bib.bib29)], and [[30](#bib.bib30)]) and replaced with a layer that allow
    the classification of the image on a per-pixel basis (see Figure [2](#S3.F2 "Figure
    2 ‣ 3.1 Fully Convolutional Networks ‣ 3 Deep Learning Approaches to Semantic
    Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time")). The replacement of fully-connected layers with convolutional
    ones had two distinct advantages: (a) it allowed the same network architecture
    to be applied to an image of any resolution and, (b) convolutional layers have
    fewer parameters which allowed for faster training and inference. This novel approach
    resulted in state-of-the-art results in several image segmentation milestones
    and is assumed one of the most influential in the area.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络最初用于分类任务（AlexNet [[1](#bib.bib1)]，VGG [[29](#bib.bib29)]，GoogLeNet [[30](#bib.bib30)]）。这些网络首先通过几个卷积层处理输入图像，滤波器数量逐渐增加，分辨率逐渐降低，最后一个卷积层被向量化。向量化特征后接完全连接层，这些层通过
    softmax 输出层学习类别的概率分布。在 FCN [[31](#bib.bib31)] 中，完全连接层导致空间信息的丧失，从流行架构（[[1](#bib.bib1)]，[[29](#bib.bib29)]，[[30](#bib.bib30)]）中移除，并用一个允许按像素分类图像的层替代（见图
    [2](#S3.F2 "Figure 2 ‣ 3.1 Fully Convolutional Networks ‣ 3 Deep Learning Approaches
    to Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time")）。用卷积层替代完全连接层有两个明显的优点：（a）允许将相同的网络架构应用于任何分辨率的图像，（b）卷积层具有更少的参数，允许更快的训练和推理。这种新颖的方法在几个图像分割里程碑中取得了最先进的结果，被认为是该领域最有影响力的方法之一。
- en: '![Refer to caption](img/afed3d383e12203aa6336e864613fd20.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/afed3d383e12203aa6336e864613fd20.png)'
- en: 'Figure 2: Fully Convolutional Network Architecture (from [[31](#bib.bib31)]).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：全卷积网络架构（来自 [[31](#bib.bib31)]）。
- en: 3.2 Encoder-Decoder Architecture
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 编码器-解码器架构
- en: In DeconvNet [[32](#bib.bib32)], the authors noted that the approach in [[31](#bib.bib31)]
    was leading to loss of information due to the the absence of real deconvolution
    and the small size of the feature map. They then proposed the architecture in
    Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Encoder-Decoder Architecture ‣ 3 Deep Learning
    Approaches to Semantic Image Segmentation ‣ A Survey on Deep Learning Methods
    for Semantic Image Segmentation in Real-Time"), where a multi-layer deconvolution
    network is learned. The trained network is applied to individual object proposals
    using fully-connected CRF to obtain instance-wise segmentations, which are combined
    for the final semantic segmentation. The encoder architecture is based on [[29](#bib.bib29)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DeconvNet [[32](#bib.bib32)] 中，作者指出[[31](#bib.bib31)]的方法由于缺乏实际的反卷积和特征图的尺寸较小，导致信息的丢失。然后他们提出了图
    [3](#S3.F3 "Figure 3 ‣ 3.2 Encoder-Decoder Architecture ‣ 3 Deep Learning Approaches
    to Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time") 中的架构，其中学习了一个多层反卷积网络。训练后的网络应用于单个对象提议，使用完全连接的
    CRF 来获得实例级分割，最终合并得到最终的语义分割。编码器架构基于 [[29](#bib.bib29)]。
- en: '![Refer to caption](img/8d9839213ba24eb93396ab483f19017d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d9839213ba24eb93396ab483f19017d.png)'
- en: 'Figure 3: DeconvNet Architecture (from [[32](#bib.bib32)]).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：DeconvNet 架构（来自 [[32](#bib.bib32)]）。
- en: In parallel to [[32](#bib.bib32)], decoder/encoder architectures were also used
    for medical applications [[33](#bib.bib33)]. The authors proposed an architecture
    that works well when little training data is available (30 images), which, with
    appropriate data augmentation can lead to state-of-the-art performance. In Figure
    [4](#S3.F4 "Figure 4 ‣ 3.2 Encoder-Decoder Architecture ‣ 3 Deep Learning Approaches
    to Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time") the decoder part on the left (contracting path
    according to the authors) downsamples the image while increasing the number of
    features. On the upsampling path, the opposite procedure is followed (i.e., increasing
    the image resolution while decreasing the number of features), while concatenating
    the corresponding encoder layer. They also proposed a weighted loss around different
    regions in order to achieve more accurate class separation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[32](#bib.bib32)]类似，解码器/编码器架构也被用于医疗应用[[33](#bib.bib33)]。作者提出了一种在训练数据稀少（30张图像）的情况下表现良好的架构，通过适当的数据增强可以实现最先进的性能。在图[4](#S3.F4
    "图4 ‣ 3.2 编码器-解码器架构 ‣ 3 深度学习方法用于语义图像分割 ‣ 语义图像分割实时深度学习方法综述")中，左侧的解码器部分（根据作者的说法是收缩路径）在增加特征数量的同时下采样图像。在上采样路径中，采取相反的过程（即增加图像分辨率同时减少特征数量），并连接相应的编码器层。他们还提出了在不同区域应用加权损失，以实现更准确的类别分离。
- en: '![Refer to caption](img/0b5ee35acd246b036b5e2ab210e7768b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0b5ee35acd246b036b5e2ab210e7768b.png)'
- en: 'Figure 4: UNet Architecture (from [[33](#bib.bib33)]).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：UNet架构（来源于[[33](#bib.bib33)]）。
- en: 'A similar architecture to [[33](#bib.bib33)] was proposed in SegNet [[34](#bib.bib34)],
    where the authors used VGG [[29](#bib.bib29)] as the backbone encoder, removed
    the fully connected layers, and added a symmetric decoder structure. The main
    difference is that every decoder layer uses the max pooling indices from the corresponding
    encoder layer, as opposed to concatenating it. Reusing max-pooling indices in
    the decoding process has several practical advantages: (i) it improves boundary
    delineation , (ii) it reduces the number of parameters enabling end-to-end training,
    and (iii) this form of upsampling can be incorporated into any encoder-decoder
    architecture. Although it was originally published in 2015, it originally received
    little traction until 2017, and has since become one of the most referenced works
    in semantic segmentation.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[33](#bib.bib33)]类似的架构在SegNet [[34](#bib.bib34)]中被提出，其中作者使用VGG [[29](#bib.bib29)]作为主干编码器，去除了全连接层，并添加了对称解码器结构。主要区别在于，每个解码器层使用来自相应编码器层的最大池化索引，而不是将其连接起来。在解码过程中重用最大池化索引具有几个实际优点：(i)
    提高边界划分，(ii) 减少参数数量，实现端到端训练，以及 (iii) 这种上采样方式可以被纳入任何编码器-解码器架构。虽然最初在2015年发表，但直到2017年才开始受到关注，并且现在已成为语义分割领域最受引用的工作之一。
- en: 3.3 Conditional Random Fields with Neural Networks
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 条件随机场与神经网络
- en: Conditional Random Fields (CRFs), were one of the most popular methods in semantic
    segmentation before the arrival of deep learning. CRFs, however, due to their
    slow training and inference speeds, as well as the difficulty to learn their internal
    parameters, lost part of their appeal. On the other hand, CNNs by design are not
    expected to perform well in boundary regions, where two or more classes intersect,
    or can lose high-level information through the multiple processing stages.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 条件随机场（CRFs）曾是深度学习出现前语义分割领域最受欢迎的方法之一。然而，由于其训练和推理速度慢，以及内部参数难以学习，CRFs的吸引力有所下降。另一方面，CNNs本身并不擅长处理边界区域，即两个或更多类别交汇的地方，或可能在多个处理阶段丢失高级信息。
- en: 'The authors of [[35](#bib.bib35)] pooled the two approaches by combining the
    responses at the final neural network layer with a fully connected Conditional
    Random Field. This way, the model’s ability to capture fine details is enhanced
    by incorporating the local interactions between neighboring pixels and edges.
    This work evolved into DeepLab [[36](#bib.bib36)], where several improvements
    were added (e.g., atrous spatial pyramid pooling), and several variants were proposed.
    The basic idea can be best explained by Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Conditional
    Random Fields with Neural Networks ‣ 3 Deep Learning Approaches to Semantic Image
    Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time"): a fully convolutional network is used to get a coarse score map
    for the different classes. The image is then upsampled to it full resolution and
    the CRF is then deployed to better capture the object boundaries. DeepLab achieved
    state-of-the-art performance in multiple segmentation datasets with an inference
    time of 125ms or 8 frames per second (FPS).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[[35](#bib.bib35)]的作者将两种方法结合起来，通过在最终神经网络层与完全连接的条件随机场之间组合响应。这样，通过结合邻近像素和边缘之间的局部交互，模型的细节捕捉能力得到了增强。这项工作演变成了DeepLab
    [[36](#bib.bib36)]，其中增加了若干改进（例如，空洞空间金字塔池化），并提出了几个变体。基本思路可以通过图 [5](#S3.F5 "Figure
    5 ‣ 3.3 Conditional Random Fields with Neural Networks ‣ 3 Deep Learning Approaches
    to Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time") 最好地解释：使用全卷积网络获取不同类别的粗略得分图。然后将图像上采样到其完整分辨率，并部署CRF以更好地捕捉对象边界。DeepLab在多个分割数据集上达到了最先进的性能，推断时间为125毫秒或每秒8帧（FPS）。'
- en: '![Refer to caption](img/9628c1db4c9845ac02244c8a62ef1322.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9628c1db4c9845ac02244c8a62ef1322.png)'
- en: 'Figure 5: DeepLab Architecture (from [[36](#bib.bib36)]).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：DeepLab架构（来自[[36](#bib.bib36)]）。
- en: In the previous work, CRFs are not trained jointly with the fully convolutional
    network. This can lead to suboptimal end-to-end performance. In [[37](#bib.bib37)],
    the authors proposed to formulate CRF as an RNN to obtain a deep network that
    has desirable properties of both CNNs and CRFs. The two networks are then fully
    integrated and trained jointly to achieve top results on the PASCAL VOC 2012 segmentation
    benchmark [[3](#bib.bib3)].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的工作中，CRF没有与全卷积网络联合训练。这可能导致亚优的端到端性能。在[[37](#bib.bib37)]中，作者提出将CRF形式化为RNN，以获得具有CNN和CRF两者理想特性的深度网络。这两个网络随后被完全整合并联合训练，以在PASCAL
    VOC 2012分割基准[[3](#bib.bib3)]上取得最佳结果。
- en: 3.4 Feature Fusion
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 特征融合
- en: Semantic segmentation involves the task of classifying an image on a pixel level.
    A lot of the techniques in the area have focused on getting the details of the
    image right, whereas the context at different stages gets lost. The authors in
    [[38](#bib.bib38)] suggest enhancing the performance of fully convolutional networks
    by adding global context to help clarify local confusions. In particular, they
    propose using the average feature for each layer to augment the features at each
    location, and thus use the combined feature map to perform segmentation. The effect
    of global context can be significant; a lot of the misclassified pixels using
    traditional fully convolutional networks can be recovered when the global context
    clarifies local confusion and, as a result, a smoother segmentation output is
    produced.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割涉及到在像素级别对图像进行分类的任务。该领域的许多技术都集中在正确获取图像细节上，而不同阶段的上下文信息则可能丢失。[[38](#bib.bib38)]的作者建议通过添加全局上下文来提升全卷积网络的性能，以帮助澄清局部混淆。特别地，他们提议使用每一层的平均特征来增强每个位置的特征，从而使用组合特征图进行分割。全局上下文的效果可能是显著的；使用传统全卷积网络时，许多被错误分类的像素在全局上下文澄清局部混淆后可以被恢复，因此，产生更平滑的分割结果。
- en: The authors in [[39](#bib.bib39)] proposed the Enhanced Semantic Segmentation
    Network (ESSN), that upsamples and concatenates the residual feature maps from
    each convolutional layer in order to maintain features from all stages of the
    network (as seen in Figure [6](#S3.F6 "Figure 6 ‣ 3.4 Feature Fusion ‣ 3 Deep
    Learning Approaches to Semantic Image Segmentation ‣ A Survey on Deep Learning
    Methods for Semantic Image Segmentation in Real-Time")). In [[40](#bib.bib40)],
    there is a downsampling stage that extracts feature information, followed by an
    upsampling part to recover the spatial resolution. The features of the corresponding
    pooling and unpooling layers are upsampled and concatenated, before the final
    prediction stage that produces the segmentation output. This fusion at multiple
    levels of the features maps was evaluated on three major semantic segmentation
    datasets and achieved promising results.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[39](#bib.bib39)]中，作者提出了增强型语义分割网络（ESSN），该网络上采样并连接来自每个卷积层的残差特征图，以保持网络所有阶段的特征（如图[6](#S3.F6
    "Figure 6 ‣ 3.4 Feature Fusion ‣ 3 Deep Learning Approaches to Semantic Image
    Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time")所示）。在[[40](#bib.bib40)]中，有一个下采样阶段提取特征信息，接着是一个上采样部分恢复空间分辨率。对应的池化和反池化层的特征被上采样并连接，然后是生成分割输出的最终预测阶段。这种多级特征图融合在三个主要的语义分割数据集上进行了评估，并取得了令人鼓舞的结果。
- en: '![Refer to caption](img/cf7c96669dced73fa12643f4b19ff82a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf7c96669dced73fa12643f4b19ff82a.png)'
- en: 'Figure 6: Enhanced Semantic Segmentation Network Architecture (from [[39](#bib.bib39)]).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：增强型语义分割网络架构（来源于[[39](#bib.bib39)]）。
- en: 3.5 Generative Adversarial Networks
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 生成对抗网络
- en: Generative Adversarial Networks (GANs) were originally introduced in [[41](#bib.bib41)]
    as a generative model for unsupervised learning, where the model learns to generate
    new data with the same statistics as the training set. Its first demonstration
    was on images, where the artificially generated images looked very similar to
    those of the training set. Since then, GANs have had quite an impact in diverse
    areas such as astronomical images [[42](#bib.bib42)], 3D object reconstruction
    [[43](#bib.bib43)], and image super-resolution [[44](#bib.bib44)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）最初在[[41](#bib.bib41)]中作为一种无监督学习的生成模型被提出，该模型学习生成与训练集具有相同统计特征的新数据。它的第一次展示是在图像上，其中人工生成的图像与训练集中的图像非常相似。从那时起，GANs
    在天文图像[[42](#bib.bib42)]、3D物体重建[[43](#bib.bib43)]和图像超分辨率[[44](#bib.bib44)]等不同领域产生了相当大的影响。
- en: The idea to apply GANs in semantic segmentation was first introduced in [[45](#bib.bib45)],
    where the authors used two different networks. First, a segmentation network that
    took the image as an input and generated per-pixel predictions much like the traditional
    CNN approaches described earlier in this work, and, second, an adversarial network
    that discriminates segmentation maps coming either from the ground truth or from
    the segmentation network. The adversarial network takes as input the image, the
    segmentation ground truth, and the segmentation network output and outputs a class
    label (1 for ground truth and 0 for synthetic). An adversarial term is added to
    the cross-entropy loss function. The adversarial term encourages the segmentation
    model to produce label maps that cannot be distinguished from ground-truth ones
    and lead to improved labeling accuracy in the Stanford Background and PASCAL VOC
    2012 datasets.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将GANs应用于语义分割的想法最早在[[45](#bib.bib45)]中提出，作者使用了两种不同的网络。首先，是一个分割网络，该网络将图像作为输入，并生成每个像素的预测，类似于本文之前描述的传统CNN方法；其次，是一个对抗网络，该网络区分来自真实标签还是分割网络的分割图。对抗网络将图像、分割真实标签和分割网络输出作为输入，并输出一个类别标签（1表示真实标签，0表示合成标签）。对抗项被添加到交叉熵损失函数中。对抗项鼓励分割模型生成无法与真实标签区分的标签图，从而提高了斯坦福背景和PASCAL
    VOC 2012数据集上的标注准确性。
- en: In [[46](#bib.bib46)], the authors proposed a semi-supervised framework – based
    on Generative Adversarial Networks (GANs) – which consists of a generator network
    to provide extra training examples to a multi-class classifier, acting as discriminator
    in the GAN framework, that assigns every sample a label from the K possible classes
    or marks it as a fake sample (extra class) as seen in Figure [7](#S3.F7 "Figure
    7 ‣ 3.5 Generative Adversarial Networks ‣ 3 Deep Learning Approaches to Semantic
    Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time"). The underlying idea is that adding large fake visual data forces
    real samples to be close in the feature space, which, in turn, improves multiclass
    pixel classification.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[46](#bib.bib46)] 中，作者提出了一个基于生成对抗网络（GANs）的半监督框架，该框架由一个生成器网络组成，用于为多类分类器提供额外的训练样本，分类器作为
    GAN 框架中的判别器，给每个样本分配一个 K 个可能类别中的标签，或者标记为假样本（额外类别），如图 [7](#S3.F7 "图 7 ‣ 3.5 生成对抗网络
    ‣ 3 深度学习方法用于语义图像分割 ‣ 实时语义图像分割的深度学习方法综述") 所示。基本思想是，添加大量的虚假视觉数据迫使真实样本在特征空间中接近，从而提高多类像素分类的效果。
- en: '![Refer to caption](img/b71de33ad07a4975017e150bb9c38c03.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b71de33ad07a4975017e150bb9c38c03.png)'
- en: 'Figure 7: Semi-Supervised Convolutional GAN Architecture (from [[46](#bib.bib46)]).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：半监督卷积 GAN 架构（来自 [[46](#bib.bib46)]）。
- en: The authors of [[47](#bib.bib47)], applied semantic segmentation with GANs in
    medical images. In a similar fashion to [[45](#bib.bib45)], the adversarial network
    takes as inputs the original image, the segmentation network output, and the ground
    truth, and optimize a multi-scale loss function that uses the mean absolute error
    distance in a min-max fashion. The segmentation network consists of four layers
    of convolutional stages as in [[33](#bib.bib33)], tailored to work with the limited
    training data sets, and the network significantly outperforms [[33](#bib.bib33)].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[[47](#bib.bib47)] 的作者在医学图像中应用了基于 GAN 的语义分割。类似于 [[45](#bib.bib45)]，对抗网络以原始图像、分割网络输出和真实值作为输入，优化一个多尺度损失函数，该函数以最小-最大方式使用平均绝对误差距离。分割网络由四层卷积阶段组成，如
    [[33](#bib.bib33)] 所示，特别针对有限的训练数据集进行调整，网络在性能上显著优于 [[33](#bib.bib33)]。'
- en: 3.6 Recurrent Neural Nets (RNNs)
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 循环神经网络 (RNNs)
- en: RNNs [[48](#bib.bib48)] have been widly used for sequential tasks. In [[49](#bib.bib49)]
    the authors proposed ReSeg, which was based on the recently introduced ReNet model
    for image classification [[50](#bib.bib50)]. The latter was tailored to semantic
    segmentation tasks by transforming each ReNet layer. In particular, each ReNet
    layer is composed of four RNN (GRUs, see [[48](#bib.bib48)]) that sweep the image
    horizontally and vertically in both directions, encoding patches or activations,
    and providing relevant global information. Moreover, ReNet layers are stacked
    on top of pre-trained convolutional layers, benefiting from generic local features.
    Upsampling layers follow ReNet layers to recover the original image resolution
    in the final predictions. The network architecture can be better understood by
    looking at Figure [8](#S3.F8 "Figure 8 ‣ 3.6 Recurrent Neural Nets (RNNs) ‣ 3
    Deep Learning Approaches to Semantic Image Segmentation ‣ A Survey on Deep Learning
    Methods for Semantic Image Segmentation in Real-Time"). The first 2 RNNs (blue
    and green) are applied on small patches of the image, their feature maps are concatenated
    and fed as input to the next two RNNs (red and yellow) which emit the output of
    the first ReNet layer. Two similar ReNet layers are stacked, followed by an upsampling
    layer and a softmax nonlinearity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs [[48](#bib.bib48)] 已被广泛用于序列任务。在 [[49](#bib.bib49)] 中，作者提出了 ReSeg，该方法基于最近引入的
    ReNet 图像分类模型 [[50](#bib.bib50)]。后者通过转变每个 ReNet 层以适应语义分割任务。在具体实现上，每个 ReNet 层由四个
    RNN（GRUs，见 [[48](#bib.bib48)]）组成，这些 RNN 在图像的水平和垂直方向上进行扫描，编码图像块或激活，并提供相关的全局信息。此外，ReNet
    层叠加在预训练的卷积层之上，利用通用的局部特征。上采样层紧跟在 ReNet 层之后，以恢复最终预测的原始图像分辨率。通过查看图 [8](#S3.F8 "图
    8 ‣ 3.6 循环神经网络 (RNNs) ‣ 3 深度学习方法用于语义图像分割 ‣ 实时语义图像分割的深度学习方法综述") 可以更好地理解网络架构。前两个
    RNN（蓝色和绿色）应用于图像的小块，其特征图被连接并作为输入提供给接下来的两个 RNN（红色和黄色），这些 RNN 产生第一个 ReNet 层的输出。两个类似的
    ReNet 层被堆叠，然后是一个上采样层和一个 softmax 非线性激活。
- en: '![Refer to caption](img/105d6bc461c747427e35d027d3445ed9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/105d6bc461c747427e35d027d3445ed9.png)'
- en: 'Figure 8: ReSeg Network Architecture (from [[49](#bib.bib49)]).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：ReSeg 网络架构（来自 [[49](#bib.bib49)]）。
- en: Another interesting application to image segmentation was in [[51](#bib.bib51)].
    There the authors looked at the problem of video segmentation, where consecutive
    video frames are segmented. One approach would be to independently segment each
    frame, but this looks like an inefficient approach due to the highly correlated
    nature of video frames. The authors suggested to incorporate the temporal information
    by adding an LSTM [[48](#bib.bib48)], a type of RNN that can efficiently handle
    long time dependencies, at different stages in the network and they reported significant
    performance improvement over their CNN counterparts.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的图像分割应用出现在 [[51](#bib.bib51)] 中。在那里，作者研究了视频分割的问题，即对连续的视频帧进行分割。一个方法是独立分割每一帧，但由于视频帧高度相关，这种方法显得不够高效。作者建议通过在网络的不同阶段加入
    LSTM [[48](#bib.bib48)]（一种可以有效处理长期依赖关系的 RNN），以融合时间信息，并报告了相对于其 CNN 对应模型显著的性能提升。
- en: 3.7 Panoptic Segmentation
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 全景分割
- en: Panoptic segmentation [[59](#bib.bib59)], the task that tries to combine semantic
    and instance segmentation such that all pixels are assigned a class label and
    all object instances are uniquely segmented, has shown very promising results
    [[60](#bib.bib60)], [[61](#bib.bib61)]. The task to provide a coherent scene segmentation
    incorporating both semantic and instance segmentation seems to be leading to state-of-art
    results in semantic segmentation over several benchmark data sets as shall be
    seen later in this report.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 全景分割 [[59](#bib.bib59)]，旨在结合语义分割和实例分割，使所有像素都被分配一个类别标签，并且所有对象实例都被唯一分割，已显示出非常有前景的结果
    [[60](#bib.bib60)], [[61](#bib.bib61)]。提供一个既包括语义分割又包括实例分割的连贯场景分割任务似乎在多个基准数据集上取得了最先进的结果，正如本报告后续部分将展示的那样。
- en: 3.8 Attention-based Models
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 基于注意力的模型
- en: Attention in deep learning was first introduced in the filed of machine translation
    [[52](#bib.bib52)]. The attention mechanism captured long-range dependencies in
    an effective manner by allowing the model to automatically search for parts of
    the source sentence that are relevant to predicting a target word.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在深度学习中首次引入是用于机器翻译 [[52](#bib.bib52)]。注意力机制通过允许模型自动搜索与预测目标词相关的源句子部分，以有效捕捉长距离依赖关系。
- en: One interesting way that attention was introduced in semantic segmentation was
    to incorporate multi-scale features in fully convolutional networks. Instead of
    the traditional method of feeding multiple resized images to a shared deep network,
    the authors of [[53](#bib.bib53)] proposed an attention mechanism that learns
    to softly weight the multi-scale features at each pixel location. The convolutional
    neural network is jointly trained with the attention model as seen in Figure [9](#S3.F9
    "Figure 9 ‣ 3.8 Attention-based Models ‣ 3 Deep Learning Approaches to Semantic
    Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time"). As a result, the model learns to scale different size images in
    an appropriate fashion so that more accurate segmentation is achieved.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的注意力引入方式是在语义分割中结合多尺度特征于全卷积网络中。与传统的将多个尺寸调整后的图像输入共享深度网络的方法不同，[[53](#bib.bib53)]
    的作者提出了一种注意力机制，该机制学习在每个像素位置上软性加权多尺度特征。如图 [9](#S3.F9 "Figure 9 ‣ 3.8 Attention-based
    Models ‣ 3 Deep Learning Approaches to Semantic Image Segmentation ‣ A Survey
    on Deep Learning Methods for Semantic Image Segmentation in Real-Time") 所示，卷积神经网络与注意力模型共同训练。因此，模型学会了以适当的方式缩放不同尺寸的图像，从而实现更准确的分割。
- en: '![Refer to caption](img/3546f6794ad84780a285ef934df1faa0.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3546f6794ad84780a285ef934df1faa0.png)'
- en: 'Figure 9: Scale-aware Semantic Image Segmentation Architecture (from [[53](#bib.bib53)]).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：尺度感知语义图像分割架构（来自 [[53](#bib.bib53)]）。
- en: In a similar fashion, [[54](#bib.bib54)] tried to address the spatial resolution
    loss of fully convolutional networks by introducing the feature pyramid attention
    module. The latter combines the context features from different scales in order
    to improve classification performance of smaller objects. Attention-aided semantic
    segmentation networks have been widely used in a variety of applications [[55](#bib.bib55)]–[[58](#bib.bib58)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，[[54](#bib.bib54)] 通过引入特征金字塔注意力模块来解决全卷积网络的空间分辨率丧失问题。后者结合了来自不同尺度的上下文特征，以提高对小物体的分类性能。基于注意力的语义分割网络已广泛应用于各种场景
    [[55](#bib.bib55)]–[[58](#bib.bib58)]。
- en: 4 Real-Time Deep Learning Architectures for Semantic Image Segmentation
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实时深度学习架构用于语义图像分割
- en: Deep learning based semantic segmentation accuracy has improved significantly
    from the early approaches. For example, [[31](#bib.bib31)] achieved 65% mean intersection
    over union (mIoU) in the Cityscapes data set [[62](#bib.bib62)] and 67% mIoU in
    the PASCAL VOC 2012 data set [[63](#bib.bib63)]. More recent architectures have
    outperformed these initial results quite significantly. The authors of HRNet [[64](#bib.bib64)],
    have build a hierarchical scheme to use images of different scales with an appropriate
    attention mechanism, like we saw at the end of the previous section. This approach
    achieves >85% mIoU in the Cityscapes data set. On the other hand, the authors
    in [[65](#bib.bib65)], have used a combination of data augmentation and self-training
    [[66](#bib.bib66)] – that uses noisy labels generated from a model trained on
    a much smaller labeled data set – to get >90% mIoU in the PASCAL VOC 2012 data
    set.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的语义分割准确性从早期方法中显著提高。例如，[[31](#bib.bib31)] 在 Cityscapes 数据集 [[62](#bib.bib62)]
    中达到了 65% 的平均交集比（mIoU），在 PASCAL VOC 2012 数据集 [[63](#bib.bib63)] 中达到了 67% 的 mIoU。更近期的架构在这些初步结果上取得了显著超越。HRNet
    [[64](#bib.bib64)] 的作者构建了一个分层方案，使用不同尺度的图像以及适当的注意机制，就像我们在上一节末尾看到的那样。该方法在 Cityscapes
    数据集中的 mIoU 超过了 85%。另一方面，[[65](#bib.bib65)] 中的作者结合了数据增强和自我训练 [[66](#bib.bib66)]
    —— 使用从在更小标记数据集上训练的模型生成的噪声标签 —— 在 PASCAL VOC 2012 数据集中获得了超过 90% 的 mIoU。
- en: Computation efficiency, however, is also of paramount importance in several
    areas like self-driving cars and segmentation on mobile devices, where inference
    requirements are quite limiting. Computational/memory cost and inference time
    have to be taken into account when designing a real-time system. In this section,
    we will go over an exhaustive list of the techniques to build such a system and
    explain how these improvements were implemented in the literature.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在一些领域，如自动驾驶汽车和移动设备上的分割，计算效率也是至关重要的，因为推理需求非常有限。在设计实时系统时，必须考虑计算/内存成本和推理时间。在本节中，我们将详细介绍构建此类系统的技术清单，并解释这些改进在文献中是如何实现的。
- en: 4.1 Fast Fourier Transform (FFT)
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 快速傅里叶变换（FFT）
- en: The well-known convolution theorem [[67](#bib.bib67)], states that under suitable
    conditions the Fourier transform of a convolution of two signals is the pointwise
    product of their Fourier transforms. The authors in [[68](#bib.bib68)] exploited
    that fact to improve the training and inference time of convolutional networks.
    A convolution of an image of size $n\times n$ with a kernel of size $k\times k$
    will take $\mathcal{O}(n^{2}*k^{2})$ operations using the direct convolution,
    but the complexity can be reduced to $\mathcal{O}(n^{2}\log n)$ by using the FFT-based
    method. Some additional memory is required to store the feature maps in the Fourier
    domain, which insignificant compared to the overall memory requirements of a deep
    neural network.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 著名的卷积定理 [[67](#bib.bib67)] 说明在适当条件下，两个信号卷积的傅里叶变换是它们傅里叶变换的逐点乘积。[[68](#bib.bib68)]
    中的作者利用这一事实来改进卷积网络的训练和推理时间。对大小为 $n\times n$ 的图像与大小为 $k\times k$ 的内核进行卷积，将使用直接卷积花费
    $\mathcal{O}(n^{2}*k^{2})$ 操作，但使用基于 FFT 的方法可以将复杂度降低到 $\mathcal{O}(n^{2}\log n)$。需要额外的内存来存储傅里叶域中的特征图，与深度神经网络的总体内存需求相比，这些额外内存是微不足道的。
- en: In [[69](#bib.bib69)], the training and inference algorithms were developed
    based on FFT and achieved reduced asymptotic complexity of both computation and
    storage. The authors claim a 1000x reduction in the required number of ASIC cores,
    as well as a 10x faster inference with a small reduction in accuracy.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[69](#bib.bib69)]中，基于 FFT 开发了训练和推理算法，实现了计算和存储的渐近复杂度减少。作者声称所需的 ASIC 核心数量减少了
    1000 倍，以及推理速度提高了 10 倍，准确度略有降低。
- en: 4.2 Pruning
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 修剪
- en: 'Storage and memory requirements of a neural network can also be reduced by
    pruning the redundant weights. In [[70](#bib.bib70)], suggested a three-step approach:
    first train the network to learn which connections are important, then prune the
    unimportant connections, and, finally, retrain the network to fine tune the weights
    of the remaining connections. The number of connections was, therefore, reduced
    by 9x to 13x with little performance degradation.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修剪冗余权重，神经网络的存储和内存需求也可以减少。在[[70](#bib.bib70)]中，提出了一种三步法：首先训练网络以了解哪些连接是重要的，然后修剪不重要的连接，最后重新训练网络以微调剩余连接的权重。因此，连接的数量减少了9倍到13倍，性能损失很小。
- en: 'The work in [[71](#bib.bib71)], focuses on channel pruning for semantic segmentation
    networks. They reduce the number of operations by 50% while only losing 1% in
    mIoU using the following stategy: pruning convolutional filters based on both
    classification and segmentation tasks. This is particularly useful in cases where
    the network backbone was transferred from an architecture originally built for
    classification tasks as we have seen earlier in this report. Scaling factors for
    each convolutional filter are computed based on both tasks and the pruned network
    is used for inference.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[[71](#bib.bib71)]中的工作集中于语义分割网络的通道剪枝。他们通过以下策略将操作数量减少了50%，而在mIoU上仅损失1%：基于分类和分割任务剪枝卷积滤波器。这在网络骨干从最初为分类任务构建的架构转移过来时特别有用，如我们在本报告中早先所见。每个卷积滤波器的缩放因子是基于两个任务计算的，并且剪枝后的网络用于推理。'
- en: Network pruning is a very active area to improve performance in convolutional
    neural nets and semantic segmentation, see [[72](#bib.bib72)] and [[73](#bib.bib73)],
    where channel pruning methods can lead to significant compression and speed-up
    on various architectures that would work on multiple tasks (classification, detection,
    and segmentation), by either reducing the number of channels on a layer by layer
    fashion solving a LASSO regression optimization problem, or by pruning the backbone
    network before transferring it to the segmentation network.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 网络剪枝是提高卷积神经网络和语义分割性能的一个非常活跃的领域，参见[[72](#bib.bib72)]和[[73](#bib.bib73)]，其中通道剪枝方法可以显著压缩和加速各种架构，这些架构可以用于多个任务（分类、检测和分割），通过减少每一层的通道数来解决LASSO回归优化问题，或在将骨干网络转移到分割网络之前对其进行剪枝。
- en: 4.3 Quantization
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 量化
- en: Another way to make the network more efficient is by reducing the number of
    bits required to represent each weight. Typically 32 bits are reserved for the
    representation of weights. 32-bit operations are slow and have large memory requirements.
    In [[74](#bib.bib74)] the authors suggest, among others, to reduce the weight
    representation to 5 bits, while limiting the number of effective weights by having
    multiple connections share the same weight, and then fine-tune those shared weights,
    thus reducing storage requirements.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种提高网络效率的方法是减少表示每个权重所需的比特数。通常，权重的表示保留32位。32位操作较慢且需要较大的内存。在[[74](#bib.bib74)]中，作者建议将权重表示减少到5位，同时通过让多个连接共享相同的权重来限制有效权重的数量，然后对这些共享权重进行微调，从而减少存储需求。
- en: In Bi-Real Net [[75](#bib.bib75)], the authors investigate the enhancement of
    1-bit convolutional neural networks, where both the weights and the activations
    are binary. The performance of these 1-bit CNNs is improved by taking the real-valued
    output of the batch-normalization layer before the binary activation and connecting
    it to the real-valued activation of the next block. Thus the representational
    capability of the proposed model is much higher than that of the original 1-bit
    CNNs, with only a negligible computational cost.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在Bi-Real Net [[75](#bib.bib75)]中，作者研究了1位卷积神经网络的增强，其中权重和激活都是二进制的。通过在二进制激活之前获取批量归一化层的实值输出，并将其连接到下一块的实值激活，从而改进这些1位CNN的性能。因此，提出的模型的表示能力远高于原始1位CNN，且计算成本仅为微不足道。
- en: 4.4 Depthwise Separable Convolutions
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 深度可分卷积
- en: The two previous methods aim at reducing the network size, by either pruning
    unnecessary components or compressing the weight information. Sifre in his Ph.D.
    thesis [[76](#bib.bib76)] introduced a novel method to make 2-dimensional convolutions
    a lot more computationally efficient called depthwise separable convolution. This
    idea was picked up by Xception [[77](#bib.bib77)] and MobileNets [[78](#bib.bib78)]
    that used slightly modified versions of the original idea to greatly improve the
    efficiency of their relative architectures. In a regular convolutional layer,
    the computation complexity depends on (a) the input/output feature map of size
    $D\times D$ (square feature map is assumed for simplicity), (b) the number of
    inputs channels $M$, (c) the number of output channels $N$, and (d) the spatial
    dimension of the kernel $K$. The overall computation requires $D^{2}\times K^{2}\times
    M\times N$ multiplications.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种方法旨在通过修剪不必要的组件或压缩权重信息来减少网络规模。Sifre 在他的博士论文[[76](#bib.bib76)]中介绍了一种使二维卷积更具计算效率的新方法，称为深度可分离卷积。这个想法被
    Xception [[77](#bib.bib77)] 和 MobileNets [[78](#bib.bib78)] 采纳，他们使用稍微修改过的原始想法大大提高了其相对架构的效率。在常规卷积层中，计算复杂度取决于
    (a) 输入/输出特征图的大小 $D\times D$（为简化假设为方形特征图），(b) 输入通道数量 $M$，(c) 输出通道数量 $N$，以及 (d)
    核的空间维度 $K$。总体计算需要 $D^{2}\times K^{2}\times M\times N$ 次乘法。
- en: In depthwise separable convolutions, the convolution with the filter of size
    $K\times K\times M\times N$ is broken into two parts. First, a depthwise convolution
    of a single filter per channel, i.e., of size $K\times K$ for all $M$ input channels,
    and (b) a pointwise convolution that uses $1\times 1$ convolutional filters to
    generate the appropriate output channel dimension. The first operation requires
    $D^{2}\times K^{2}\times M$, while the second $D^{2}\times M\times N$. The computational
    improvement is of the order $\max{\big{(}\mathcal{O}(N),\mathcal{O}(D^{2})\big{)}}$,
    which can be quite significant especially when the fiter size or depth increases.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度可分离卷积中，大小为 $K\times K\times M\times N$ 的卷积被分解为两部分。首先，进行每个通道一个滤波器的深度卷积，即对所有
    $M$ 个输入通道的大小为 $K\times K$ 的卷积；其次，进行点卷积，使用 $1\times 1$ 卷积滤波器来生成适当的输出通道维度。第一次操作需要
    $D^{2}\times K^{2}\times M$，而第二次需要 $D^{2}\times M\times N$。计算改进的顺序为 $\max{\big{(}\mathcal{O}(N),\mathcal{O}(D^{2})\big{)}}$，尤其是在滤波器大小或深度增加时，这种改进可能非常显著。
- en: 4.5 Dilated Convolutions
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 膨胀卷积
- en: In their seminal work [[79](#bib.bib79)], the authors introduced dilated convolution
    to expand the effective receptive field of kernels by inserting zeros between
    each pixel in the convolutional kernel. As seen on the left side of Figure [10](#S4.F10
    "Figure 10 ‣ 4.5 Dilated Convolutions ‣ 4 Real-Time Deep Learning Architectures
    for Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time"), a $3\times 3$ kernel will cover nine pixels.
    However, if a dilation rate of $2$ is introduced, then the eight outer pixels
    will expand to cover twenty-five pixels, by skipping a pixel (see middle of Figure
    [10](#S4.F10 "Figure 10 ‣ 4.5 Dilated Convolutions ‣ 4 Real-Time Deep Learning
    Architectures for Semantic Image Segmentation ‣ A Survey on Deep Learning Methods
    for Semantic Image Segmentation in Real-Time")). If the dilation rate further
    doubles, the coverage will then be of eighty-one pixels as seen on the right of
    Figure [10](#S4.F10 "Figure 10 ‣ 4.5 Dilated Convolutions ‣ 4 Real-Time Deep Learning
    Architectures for Semantic Image Segmentation ‣ A Survey on Deep Learning Methods
    for Semantic Image Segmentation in Real-Time"). In summary, a kernel of size $K\times
    K$ with a dilation rate of $N$ will cover $(N-1)*K\times(N-1)*K$ pixels for an
    expansion of $(N-1)\times(N-1)$. In semantic segmentation tasks, where context
    is critical for the network accuracy, dilated convolutions can expand the receptive
    field exponentially without increasing the computation cost. By stacking multiple
    convolutional layers with different dilation rates, [[79](#bib.bib79)] managed
    to capture image context of increasing receptive fields and was able to significantly
    improve segmentation performance of previous state-of-the-art works.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的开创性工作[[79](#bib.bib79)]中，作者引入了扩张卷积，通过在卷积核的每个像素之间插入零来扩展卷积核的有效感受野。如图[10](#S4.F10
    "图 10 ‣ 4.5 扩张卷积 ‣ 4 实时深度学习架构用于语义图像分割 ‣ 语义图像分割的深度学习方法综述")左侧所示，一个 $3\times 3$ 的卷积核会覆盖九个像素。然而，如果引入扩张率为
    $2$，则八个外部像素会扩展到覆盖二十五个像素，通过跳过一个像素（见图[10](#S4.F10 "图 10 ‣ 4.5 扩张卷积 ‣ 4 实时深度学习架构用于语义图像分割
    ‣ 语义图像分割的深度学习方法综述")中间）。如果扩张率进一步加倍，则覆盖范围将达到八十一像素，如图[10](#S4.F10 "图 10 ‣ 4.5 扩张卷积
    ‣ 4 实时深度学习架构用于语义图像分割 ‣ 语义图像分割的深度学习方法综述")右侧所示。总之，扩张率为 $N$ 的 $K\times K$ 大小的卷积核将覆盖
    $(N-1)*K\times(N-1)*K$ 像素，扩展为 $(N-1)\times(N-1)$。在语义分割任务中，背景信息对网络准确性至关重要，扩张卷积可以在不增加计算成本的情况下指数级扩展感受野。通过堆叠具有不同扩张率的多个卷积层，[[79](#bib.bib79)]成功捕捉了具有增大感受野的图像背景，并显著提高了以前最先进工作的分割性能。
- en: '![Refer to caption](img/2f28541c4a9f7d9ec1386c2dcefae47f.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2f28541c4a9f7d9ec1386c2dcefae47f.png)'
- en: 'Figure 10: Schematic of $3\times 3$ dilated convolutional kernel. Left: dilation
    rate = 1, Center: dilation rate = 2, Right: dilation rate = 4.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: $3\times 3$ 扩张卷积核的示意图。左：扩张率 = 1，中：扩张率 = 2，右：扩张率 = 4。'
- en: In [[80](#bib.bib80)] a new convolutional module was introduced, the efficient
    spatial pyramid (ESP). ESPNet combines dilated convolution with the depthwise
    separable convolutions of the previous subsection. In other words, the authors
    formed a factorized set of convolutions that decompose a standard convolution
    into a point-wise convolution and a spatial pyramid of dilated convolutions. ESPNet
    had among the smallest number of parameters of similar works while maintaining
    the largest effective receptive field. This work is especially interesting since
    it introduced several new system-level metrics that help to analyze the performance
    of CNNs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[80](#bib.bib80)]中，引入了一种新的卷积模块——高效空间金字塔（ESP）。ESPNet将扩张卷积与上一小节中的深度可分卷积结合起来。换句话说，作者形成了一组分解标准卷积的卷积操作，将其分解为点卷积和扩张卷积的空间金字塔。ESPNet在类似工作中拥有最少的参数数量，同时保持了最大的有效感受野。这项工作尤其有趣，因为它引入了几个新的系统级指标，有助于分析CNN的性能。
- en: 4.6 Width and Resolution Multipliers
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 宽度和分辨率倍增器
- en: 'In [[78](#bib.bib78)], the authors explored several ways to further reduce
    the network complexity. They introduced two hyperpameters: (a) the width multiplier
    that would produce thinner models and (b) the resolution multiplier that would
    reduce input resolution. In the former case, the authors scaled down the computational
    requirements of every layer in a uniform fashion by scaling the number of input
    and out channels by a factor $\alpha$. From the analysis of the depthwise separable
    convolutions, it can be seen that the original computational complexity of $D^{2}\times
    K^{2}\times M+D^{2}\times M\times N$ becomes $D^{2}\times K^{2}\times M\times\alpha+D^{2}\times
    M\times N\times\alpha^{2}$, for an overall reduction of somewhere between $\alpha$
    and $\alpha^{2}$. The basic idea is to find an appropriate scaling factor to define
    a new smaller model with a reasonable accuracy, latency, and size trade off.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[78](#bib.bib78)]中，作者探索了几种进一步减少网络复杂性的方式。他们引入了两个超参数：（a）宽度乘数，用于生成更薄的模型；（b）分辨率乘数，用于减少输入分辨率。在前一种情况下，作者通过将输入和输出通道的数量按因子$\alpha$进行缩放，均匀地缩小了每一层的计算需求。从深度可分离卷积的分析中可以看出，原始的计算复杂度$D^{2}\times
    K^{2}\times M+D^{2}\times M\times N$变为$D^{2}\times K^{2}\times M\times\alpha+D^{2}\times
    M\times N\times\alpha^{2}$，总的缩减范围在$\alpha$和$\alpha^{2}$之间。基本思想是找到一个适当的缩放因子，以定义一个新的更小模型，并在准确性、延迟和大小之间取得合理的平衡。
- en: On the other hand, the resolution multiplier $\rho$ can scale the input image
    dimensions by a factor of $\rho^{2}$ leading to an overall computational cost
    of $D^{2}\times K^{2}\times M\times\rho^{2}+D^{2}\times M\times N\times\rho^{2}$.
    Again this reduction process should be optimized with the accuracy, latency, and
    size in mind. The two methods can be combined for further improvements.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，分辨率乘数$\rho$可以将输入图像的尺寸按$\rho^{2}$的因子进行缩放，从而导致总体计算成本为$D^{2}\times K^{2}\times
    M\times\rho^{2}+D^{2}\times M\times N\times\rho^{2}$。这个缩减过程仍然需要在准确性、延迟和大小之间进行优化。两种方法可以结合使用以获得进一步的改进。
- en: 4.7 Early Downsampling
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 早期下采样
- en: A similar idea was presented in [[81](#bib.bib81)], where a number of design
    choices were discussed based on the authors experimental results and intuitions.
    In particular, very large input frames are very expensive computationally and
    it is a good idea to downsample these frames in the early stages of the network,
    while keeping the number of features relatively low. This downsampling does not
    have a severe impact in the overall performance because the visual information
    is typically highly redundant and can be compressed into a more efficient representation.
    Furthermore, it is noted that the first few layers do not really contribute to
    the classification task but rather provide useful representations for the subsequent
    layers. On the other hand, filters operating on downsampled images have a larger
    receptive field and can provide more context to the segmentation task.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的观点在[[81](#bib.bib81)]中提出，其中讨论了基于作者的实验结果和直觉的一些设计选择。特别地，非常大的输入帧在计算上非常昂贵，因此在网络的早期阶段对这些帧进行下采样是个好主意，同时保持特征数量相对较低。这种下采样对整体性能影响不大，因为视觉信息通常具有高度冗余，可以压缩成更高效的表示。此外，值得注意的是，前几层实际上并不对分类任务做出贡献，而是为后续层提供有用的表示。另一方面，操作在下采样图像上的滤波器具有更大的感受野，并能为分割任务提供更多上下文信息。
- en: Since the downsampling can lead to loss of spatial information like exact edge
    shape, ENet follows the paradigm set in SegNet [[34](#bib.bib34)], where the indices
    of elements chosen in max pooling layers are stored and used to produce sparse
    upsampled maps in the decoder, therefore, partially recovering spatial information,
    with small memory requirements.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于下采样可能导致空间信息丢失，如精确的边缘形状，ENet遵循SegNet[[34](#bib.bib34)]中设定的范式，其中在最大池化层中选择的元素的索引被存储并用于在解码器中生成稀疏的上采样图，因此部分恢复空间信息，同时占用较小的内存。
- en: 4.8 Smaller Decoder Size
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8 较小的解码器尺寸
- en: 'Another design choice discussed in [[81](#bib.bib81)] was the fact that, in
    the typical encoder/decoder structure of a semantic segmentation network, the
    two subcomponents do not have to be symmetric. Encoders need to be deep in order
    to capture the features in a similar fashion to their classification counterparts.
    Decoders, however, have one main task: to upsample the compressed feature space
    in order to provide pixel-level classification. The latter can be achieved with
    a much less deep architecure, providing significant computational savings.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[81](#bib.bib81)]中讨论的另一个设计选择是，在典型的编码器/解码器结构的语义分割网络中，两个子组件不必对称。编码器需要足够深以便捕捉特征，类似于分类网络中的编码器。解码器则有一个主要任务：对压缩的特征空间进行上采样，以提供像素级分类。后者可以通过一个更浅的架构实现，从而显著节省计算资源。
- en: 4.9 Efficient Grid Size Reduction
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9 高效的网格尺寸减少
- en: The authors in [[82](#bib.bib82)] noticed that because the pooling operation
    can lead to a representational bottleneck, it is typically compensated by increasing
    the number of channels used before the pooling operation. Unfortunately, this
    means that the doubling of the filters is effectively dominating the computational
    cost. Reversing the order of the convolution/pooling operations would definitely
    improve the computation speed, but would not help with the representational bottleneck.
    What the authors suggest is to perform pooling operation in parallel with a convolution
    of stride 2, and concatenate the resulting filter banks. This technique allowed
    the authors of [[81](#bib.bib81)] to speed up inference time of the initial block
    by a factor of 10.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[[82](#bib.bib82)]的作者注意到，由于池化操作可能导致表示瓶颈，通常会通过在池化操作之前增加使用的通道数来弥补这一点。不幸的是，这意味着滤波器的倍增实际上主导了计算成本。反转卷积/池化操作的顺序肯定会提高计算速度，但不会解决表示瓶颈。作者建议将池化操作与步幅为2的卷积并行进行，并将结果滤波器组进行连接。这一技术使得[[81](#bib.bib81)]的作者将初始块的推理时间提高了10倍。'
- en: 4.10 Drop Bias Terms
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.10 去除偏置项
- en: Bias terms do not have significant impact on the overall performance of a semantic
    segmentation network and are typically dropped.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置项对语义分割网络的整体性能没有显著影响，因此通常会被省略。
- en: 4.11 Stack Multiple Layers with Small Kernels
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.11 使用小卷积核堆叠多个层
- en: 'Total computational cost increases with the square of the kernel size. In [[29](#bib.bib29)],
    it was argued that having multiple convolutional layers with small kernel size
    is superior to having a single layer with a larger kernel for two reasons: (a)
    by stacking three $3\times 3$ convolutional layers correspond to the same effective
    receptive field of a $7\times 7$ layer while reducing the number of parameters
    to almost half and, (b) by incorporating three non-linear rectification layers
    instead of a single one, the decision function is made more discriminative.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 总体计算成本随着卷积核大小的平方增加。在[[29](#bib.bib29)]中，认为使用多个小卷积核的卷积层优于使用一个大卷积核的单层，原因有两个：（a）通过堆叠三个$3\times
    3$卷积层，可以实现与一个$7\times 7$层相同的有效感受野，同时将参数数量减少近一半；（b）通过引入三个非线性激活层而不是一个，决策函数变得更具判别性。
- en: 4.12 Channel Shuffle Operation
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.12 通道洗牌操作
- en: Grouped convolutions were first introduced in [[1](#bib.bib1)] to distribute
    the model over multiple GPUs. It uses multiple convolutions in parallel in order
    to derive multiple channel outputs per layer. [[83](#bib.bib83)] showed that the
    use of grouped convolutions can improve accuracy in classification tasks. However,
    this architecture become less efficient when applied to much smaller networks,
    where the performance bottleneck is the large number of dense $1\times 1$ convolutions.
    The authors in [[84](#bib.bib84)] propose a novel channel shuffle operation to
    overcome this difficulty. In particular, on the left side of Figure [11](#S4.F11
    "Figure 11 ‣ 4.12 Channel Shuffle Operation ‣ 4 Real-Time Deep Learning Architectures
    for Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time") a typical group convolution can be seen with
    two stacked layers of convolutions and an equal number of groups. If a group convolution
    is allowed to obtain data from different groups then the input and output channels
    are fully related (see middle portion of Figure [11](#S4.F11 "Figure 11 ‣ 4.12
    Channel Shuffle Operation ‣ 4 Real-Time Deep Learning Architectures for Semantic
    Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time")). However, the operation above can be efficiently implemented by
    following the process at the right of Figure [11](#S4.F11 "Figure 11 ‣ 4.12 Channel
    Shuffle Operation ‣ 4 Real-Time Deep Learning Architectures for Semantic Image
    Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time"). By adding a channel shuffle operation the output channel dimension
    is reshaped, transposed and the flattened before being fed to the subsequent layer.
    Channel shuffle reduces the number of operations by a factor of $g$, which is
    the number of groups.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 分组卷积首次在[[1](#bib.bib1)]中被引入，用于将模型分布到多个GPU上。它在并行中使用多个卷积，以便每层生成多个通道输出。[[83](#bib.bib83)]显示，使用分组卷积可以提高分类任务的准确性。然而，当应用于较小的网络时，这种架构变得效率较低，性能瓶颈在于大量的密集$1\times
    1$卷积。[[84](#bib.bib84)]的作者提出了一种新颖的通道洗牌操作来克服这一困难。特别是，在图[11](#S4.F11 "Figure 11
    ‣ 4.12 Channel Shuffle Operation ‣ 4 Real-Time Deep Learning Architectures for
    Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image
    Segmentation in Real-Time")的左侧可以看到一个典型的分组卷积，它有两层叠加的卷积和相等数量的组。如果允许分组卷积从不同的组中获取数据，那么输入和输出通道是完全相关的（见图[11](#S4.F11
    "Figure 11 ‣ 4.12 Channel Shuffle Operation ‣ 4 Real-Time Deep Learning Architectures
    for Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time")的中间部分）。然而，上述操作可以通过遵循图[11](#S4.F11 "Figure 11
    ‣ 4.12 Channel Shuffle Operation ‣ 4 Real-Time Deep Learning Architectures for
    Semantic Image Segmentation ‣ A Survey on Deep Learning Methods for Semantic Image
    Segmentation in Real-Time")右侧的过程高效地实现。通过添加通道洗牌操作，输出通道维度被重塑、转置并展平，然后再输入到后续层。通道洗牌通过一个因子$g$（即组的数量）减少操作次数。
- en: '![Refer to caption](img/3a8f7185dacd908b113034ffeecded5a.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3a8f7185dacd908b113034ffeecded5a.png)'
- en: 'Figure 11: Channel Shuffle Architecture (from [[84](#bib.bib84)]).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：通道洗牌架构（来自[[84](#bib.bib84)]）。
- en: 4.13 Two Branch Networks
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.13 两分支网络
- en: 'Downsampling the original image can lead to significant improvements in the
    inference time of a semantic segmentation architecture, but it can lead to significant
    loss of spatial detail. Two branch networks try to reconcile this by introducing
    two separate branches: (a) a relatively shallow branch that uses the full resolution
    image that captures the spatial details, and (b) a deeper branch with a downsampled
    image that will be able to learn efficiently the features for an effective classification
    outcome. The two branches can share layers as to further improve computational
    complexity (see [[85](#bib.bib85)]) or can have different backbone architectures
    before being aggregated [[86](#bib.bib86)]. The latter work is composed of a detail
    branch that uses wide channel and shallow layers to capture the low-level details,
    a semantic branch with narrow channel s and deep layers to maintain the high-level
    context, and an aggregation layer to fuse the two kinds of features. As a result,
    BiSeNet-V2 achieves, arguably, the highest inference speed in semantic segmentation
    tasks (156 frames per second) while maintaining one of the best mIoU performance.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对原始图像进行下采样可以显著提高语义分割架构的推理速度，但也可能导致空间细节的显著丧失。两个分支网络试图通过引入两个独立的分支来调和这一点：（a）一个相对浅的分支，使用捕获空间细节的全分辨率图像，以及（b）一个具有下采样图像的更深分支，能够有效地学习特征以实现有效的分类结果。这两个分支可以共享层以进一步改善计算复杂性（见[[85](#bib.bib85)]），也可以在汇总之前拥有不同的主干架构[[86](#bib.bib86)]。后者的工作由一个使用宽通道和浅层的细节分支组成，用于捕获低级细节，一个具有窄通道和深层的语义分支，用于保持高级上下文，以及一个聚合层，用于融合这两种特征。因此，BiSeNet-V2可以说在语义分割任务中实现了最高的推理速度（每秒156帧），同时保持了最佳的mIoU性能之一。
- en: 4.14 Other Design Choices
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.14 其他设计选择
- en: Apart from the computationally efficient methods presented in this section so
    far, there are a handful of other good design choices that would help maintain
    good performance despite using a lightweight architecture. For example, a common
    theme in many of the papers is batch normalization [[87](#bib.bib87)] that allows
    for faster and more accurate training process. Also the choice of the activation
    function can be significant. ReLU is the nonlinearity that most works use in the
    area, but several researchers have reported improved results with parametric ReLU
    (PReLU). Finally, regularization [[88](#bib.bib88)] can help avoid overfitting
    since in many applications the input image dimension is small compared to the
    number of parameters in a segmentation deep neural network.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本节迄今介绍的计算高效方法外，还有一些其他良好的设计选择可以帮助在使用轻量级架构的情况下保持良好的性能。例如，许多论文中的一个共同主题是批量归一化[[87](#bib.bib87)]，这可以加快和提高训练过程的准确性。此外，激活函数的选择也可能很重要。ReLU是大多数工作中使用的非线性函数，但一些研究人员报告称，参数化ReLU（PReLU）能获得更好的结果。最后，正则化[[88](#bib.bib88)]可以帮助避免过拟合，因为在许多应用中，输入图像的维度相对于分割深度神经网络中的参数数量较小。
- en: 5 Semantic Segmentation Data Sets
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 语义分割数据集
- en: Several data sets have been generated in order to facilitate faster growth in
    key areas of semantic segmentation as well as establish performance benchmarks.
    Table [1](#S5.T1 "Table 1 ‣ 5 Semantic Segmentation Data Sets ‣ A Survey on Deep
    Learning Methods for Semantic Image Segmentation in Real-Time") summarizes several
    of the image sets that have been annotated on a pixel level. It contains diverse
    datasets that were originally developed for classification tasks, as well as more
    specialized image sets that are appropriate for specific applications (e.g., self-driving
    and motion-based segmentation) covering a wide range of scenes and object categories
    with pixel-wise annotations. Additional information for each of these datasets
    will be provided in the remainder of this section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进语义分割关键领域的快速发展并建立性能基准，已生成了多个数据集。表[1](#S5.T1 "Table 1 ‣ 5 Semantic Segmentation
    Data Sets ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time")总结了几个在像素级别上注释的图像集。它包含了最初为分类任务开发的多样化数据集，以及适用于特定应用（例如自动驾驶和基于运动的分割）的更专业的图像集，涵盖了各种场景和对象类别，并具有像素级的注释。本节的其余部分将提供每个数据集的附加信息。
- en: '| Data Set | Images | Classes | Year |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 图像 | 类别 | 年份 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| COCO | 164K | 172 | 2017 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| COCO | 164K | 172 | 2017 |'
- en: '| ADE20K | 25.2K | 2693 | 2017 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ADE20K | 25.2K | 2693 | 2017 |'
- en: '| Cityscapes | 25K | 30 | 2016 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Cityscapes | 25K | 30 | 2016 |'
- en: '| SYNTHIA | 13K | 13 | 2016 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| SYNTHIA | 13K | 13 | 2016 |'
- en: '| PASCAL Context | 10.1K | 540 | 2014 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL Context | 10.1K | 540 | 2014 |'
- en: '| SIFT Flow | 2.7K | 33 | 2009 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| SIFT Flow | 2.7K | 33 | 2009 |'
- en: '| CamVid | 701 | 32 | 2008 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CamVid | 701 | 32 | 2008 |'
- en: '| KITTI | 203 | 13 | 2012 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| KITTI | 203 | 13 | 2012 |'
- en: 'Table 1: Semantic Segmentation Data Set Summary'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 语义分割数据集总结'
- en: 5.1 Common Objects in Context (COCO)
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 Common Objects in Context (COCO)
- en: Common Objects in Context (COCO) [[89](#bib.bib89)] is a large-scale object
    detection, segmentation, and captioning dataset. It is one of the most extensive
    datasets available with 330K images of which half are labeled. Semantic classes
    can be either things (objects with a well-defined shape, e.g. car, person) or
    stuff (amorphous background regions, e.g. grass, sky).There are 80 object categories,
    91 stuff classes, 1.5 million object instances, and due to the size of the data
    set it is considered one of the most challenging ones for image segmentation tasks.
    As a result, the COCO leader board [[90](#bib.bib90)] for semantic segmentation
    consists of only five entries with some of the most seminal works in the area
    occupying the top spots.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Common Objects in Context (COCO) [[89](#bib.bib89)] 是一个大规模的物体检测、分割和描述数据集。它是可用的最广泛的数据集之一，共有330K张图像，其中一半已标注。语义类别可以是事物（形状明确的物体，例如汽车、人物）或背景物（无定形的背景区域，例如草地、天空）。共有80个物体类别、91个背景类别、150万个物体实例，因其数据集的规模，它被认为是图像分割任务中最具挑战性的之一。因此，COCO
    的语义分割排行榜 [[90](#bib.bib90)] 仅包含五个条目，其中一些最具开创性的工作占据了前几名。
- en: COCO-Stuff [[91](#bib.bib91)] augments all images of the COCO 2017 data set
    with pixel-wise annotations for the 91 stuff classes. The original COCO data set
    already provided outline-level annotation for the 80 thing classes, but COCO-stuff
    completed the annotation for more complex tasks such as semantic segmentation
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: COCO-Stuff [[91](#bib.bib91)] 对 COCO 2017 数据集中的所有图像进行了像素级标注，涵盖了91个物品类别。原始的 COCO
    数据集已经提供了80个事物类别的轮廓级标注，但 COCO-stuff 完成了更复杂任务如语义分割的标注。
- en: 5.2 PASCAL Visual Object Classes (VOC)
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 PASCAL Visual Object Classes (VOC)
- en: One of the most popular image sets is the PASCAL Visual Object Classes (VOC)
    [[3](#bib.bib3)] that can be used for classification, detection, segmentation,
    action classification, and person layout. The data are available in [[63](#bib.bib63)],
    have been annotated, and are periodically updated. For the image segmentation
    challenge the data include $20$ classes categorized in every-day objects (airplane,
    bicycle, bird, boat, etc.). The training set consists of 1464 images and the validation
    set consists of 1449 images. The test set is reserved for evaluation in the PASCAL
    VOC Challenge, a competition that started in 2005 and it had its most recent data
    set in 2012\. It is a generic data set that includes a variety of scenes/objects
    and, as a result, it is regularly used to evaluate novel image segmentation approaches.
    An example of an image and its semantic segmentation can be seen in Figure [1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time") and Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣
    1 Introduction ‣ A Survey on Deep Learning Methods for Semantic Image Segmentation
    in Real-Time") respectively.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的图像数据集之一是 PASCAL Visual Object Classes (VOC) [[3](#bib.bib3)]，它可以用于分类、检测、分割、动作分类和人员布局。这些数据可以在
    [[63](#bib.bib63)] 中找到，已经被标注，并且定期更新。对于图像分割挑战，这些数据包括 $20$ 个类别，涵盖日常物品（飞机、自行车、鸟、船等）。训练集包含1464张图像，验证集包含1449张图像。测试集保留用于
    PASCAL VOC Challenge 的评估，这是一项始于2005年的竞赛，其最新的数据集在2012年发布。这是一个通用数据集，涵盖了各种场景/对象，因此，常用于评估新颖的图像分割方法。图像及其语义分割的示例可以在图
    [1(a)](#S1.F1.sf1 "在图1 ‣ 1 引言 ‣ 关于实时语义图像分割的深度学习方法调研") 和图 [1(b)](#S1.F1.sf2 "在图1
    ‣ 1 引言 ‣ 关于实时语义图像分割的深度学习方法调研") 中看到。
- en: 'Several extensions have been made to the former image set, most notably PASCAL
    Context [[92](#bib.bib92)] and PASCAL Part [[93](#bib.bib93)]. The former annotates
    the same image with over 500 classes, while the latter breaks down the original
    objects into several parts and annotates them. Two other PASCAL extensions are:
    (a) the Semantic Boundaries Data set (SBD) [[94](#bib.bib94)], and (b) the PASCAL
    Semantic Parts (PASParts) [[95](#bib.bib95)].'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对前述图像数据集做出了几项扩展，最著名的有 PASCAL Context [[92](#bib.bib92)] 和 PASCAL Part [[93](#bib.bib93)]。前者在相同的图像上标注了超过500个类别，而后者将原始对象分解为多个部分并对其进行标注。另有两个
    PASCAL 扩展： (a) 语义边界数据集 (SBD) [[94](#bib.bib94)]，和 (b) PASCAL 语义部件 (PASParts) [[95](#bib.bib95)]。
- en: 5.3 ADE20K
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 ADE20K
- en: ADE20K [[96](#bib.bib96)] was developed by the MIT computer vision lab. The
    authors saw that the datasets available at the time were quite restrictive in
    the number and type of objects as well as the kinds of scenes. As a result, they
    collected a data set of 25K images that has densely annotated images (every pixel
    has a semantic label) with a large and an unrestricted open vocabulary of almost
    2700 classes. The images in this data set were manually segmented in great detail,
    covering a diverse set of scenes, object and object part categories. A single
    expert annotator, providing extremely detailed and exhaustive image annotations
    without suffering from annotation inconsistencies common when multiple annotators
    are used. The detail in the annotation can be seen in Figures [12(a)](#S5.F12.sf1
    "In Figure 12 ‣ 5.3 ADE20K ‣ 5 Semantic Segmentation Data Sets ‣ A Survey on Deep
    Learning Methods for Semantic Image Segmentation in Real-Time") and [12(b)](#S5.F12.sf2
    "In Figure 12 ‣ 5.3 ADE20K ‣ 5 Semantic Segmentation Data Sets ‣ A Survey on Deep
    Learning Methods for Semantic Image Segmentation in Real-Time"). On average there
    are 19.5 instances and 10.5 object classes per image.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ADE20K [[96](#bib.bib96)] 是由麻省理工学院计算机视觉实验室开发的。作者发现当时可用的数据集在对象数量和类型以及场景种类上都非常有限。因此，他们收集了一个包含25K张图像的数据集，这些图像具有密集注释（每个像素都有语义标签），并且拥有一个几乎2700类的大型开放词汇。该数据集中的图像经过人工详细分割，覆盖了各种场景、对象及对象部分类别。由单个专家标注员提供了极其详细和全面的图像注释，避免了使用多个标注员时常见的注释不一致问题。注释的详细程度可见于图[12(a)](#S5.F12.sf1
    "在图12 ‣ 5.3 ADE20K ‣ 5 语义分割数据集 ‣ 实时语义图像分割的深度学习方法调查")和[12(b)](#S5.F12.sf2 "在图12
    ‣ 5.3 ADE20K ‣ 5 语义分割数据集 ‣ 实时语义图像分割的深度学习方法调查")。平均每张图像包含19.5个实例和10.5个对象类别。
- en: '![Refer to caption](img/839ced1a26202b2aa06334d5573bf0c0.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/839ced1a26202b2aa06334d5573bf0c0.png)'
- en: (a) Original image from ADE20K [[97](#bib.bib97)]
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ADE20K的原始图像[[97](#bib.bib97)]
- en: '![Refer to caption](img/d524cb826291163106223abef1c994be.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/d524cb826291163106223abef1c994be.png)'
- en: (b) Annotated image from ADE20K [[97](#bib.bib97)]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ADE20K的注释图像[[97](#bib.bib97)]
- en: 'Figure 12: ADE20K trainining images'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：ADE20K训练图像
- en: 'For their scene parsing benchmark [[97](#bib.bib97)], they selected the top
    150 categories ranked by their total pixel ratios and they use the following metrics:
    (a) pixel accuracy, (b) mean accuracy, (c) mean IoU, and (d) weighted IoU. A little
    over 20K images were used for the training set, 2K images for the validation,
    and the remainder was reserved for testing. Stereo video sequences recorded in
    streets from 50 different cities and annotations involved 30 diverse classes.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的场景解析基准测试[[97](#bib.bib97)]中，他们选择了按总像素比排名前150的类别，并使用了以下指标：（a）像素准确率，（b）均值准确率，（c）均值IoU，以及（d）加权IoU。用于训练集的图像略多于20K张，2K张用于验证，其余用于测试。记录了50个不同城市的街道立体视频序列，注释涉及30个不同类别。
- en: 5.4 Cityscapes
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 Cityscapes
- en: The Cityscapes data set [[98](#bib.bib98)] focuses on visual understanding of
    complex urban street scenes. It has 25K images, 5K of which have high quality
    pixel-level annotations, while 20K additional images have coarse annotations (i.e.,
    weakly-labeled data) as be seen in Figures [13(a)](#S5.F13.sf1 "In Figure 13 ‣
    5.4 Cityscapes ‣ 5 Semantic Segmentation Data Sets ‣ A Survey on Deep Learning
    Methods for Semantic Image Segmentation in Real-Time") and [13(b)](#S5.F13.sf2
    "In Figure 13 ‣ 5.4 Cityscapes ‣ 5 Semantic Segmentation Data Sets ‣ A Survey
    on Deep Learning Methods for Semantic Image Segmentation in Real-Time"), respectively.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Cityscapes数据集[[98](#bib.bib98)] 专注于复杂城市街景的视觉理解。该数据集包含25K张图像，其中5K张具有高质量的像素级注释，而另外20K张图像具有粗略注释（即弱标记数据），如图[13(a)](#S5.F13.sf1
    "在图13 ‣ 5.4 Cityscapes ‣ 5 语义分割数据集 ‣ 实时语义图像分割的深度学习方法调查")和[13(b)](#S5.F13.sf2 "在图13
    ‣ 5.4 Cityscapes ‣ 5 语义分割数据集 ‣ 实时语义图像分割的深度学习方法调查")所示。
- en: '![Refer to caption](img/08419f4e3e0d461a139f6f5ebaa06165.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/08419f4e3e0d461a139f6f5ebaa06165.png)'
- en: (a) Fine annotation example from Cityscapes [[99](#bib.bib99)]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Cityscapes的精细注释示例[[99](#bib.bib99)]
- en: '![Refer to caption](img/36e3048f699e3cf5daf6121033d3fb93.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/36e3048f699e3cf5daf6121033d3fb93.png)'
- en: (b) Coarse annotation example from Cityscapes [[99](#bib.bib99)]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Cityscapes的粗略注释示例[[99](#bib.bib99)]
- en: 'Figure 13: Cityscapes trainining images'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：Cityscapes训练图像
- en: Their benchmark suite (found in [[99](#bib.bib99)]) involves, among others,
    a pixel-level semantic labeling task with over 200 entries. It is considered the
    most diverse and challenging urban scene data sets and, as a result, it is very
    popular performance evaluation tool.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的基准套件（见 [[99](#bib.bib99)]）涉及，包括其他任务，在内的像素级语义标注任务有超过 200 项。它被认为是最具多样性和挑战性的城市场景数据集，因此，它是非常流行的性能评估工具。
- en: 5.5 SYNTHIA
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 SYNTHIA
- en: The SYNTHIA data set [[100](#bib.bib100)] is another collection of urban scene
    images that focuses on self-driving applications. The authors generated realistic
    synthetic images with pixel-level annotations and tried to address the question
    of how useful such data can be for semantic segmentation. 13K urban images were
    created with automatically generated pixel level annotations from 13 categories
    (e.g. sky, building, road). It was concluded that, when SYNTHIA is used in the
    training stage together with publicly available real-world urban images, the semantic
    segmentation task performance significantly improves. An example of a synthetic
    image from SYNTHIA can be seen in Figure [14](#S5.F14 "Figure 14 ‣ 5.5 SYNTHIA
    ‣ 5 Semantic Segmentation Data Sets ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time") as well as the general view of the city used
    for the image generation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: SYNTHIA 数据集 [[100](#bib.bib100)] 是另一个专注于自动驾驶应用的城市场景图像集合。作者生成了带有像素级注释的逼真合成图像，并尝试解决这样的数据对于语义分割任务的有效性。创建了
    13K 张城市图像，包含来自 13 个类别（如天空、建筑、道路）的自动生成的像素级注释。研究得出结论，当 SYNTHIA 与公开可用的真实城市图像一起用于训练阶段时，语义分割任务的性能显著提升。合成图像的示例可以在图
    [14](#S5.F14 "Figure 14 ‣ 5.5 SYNTHIA ‣ 5 Semantic Segmentation Data Sets ‣ A
    Survey on Deep Learning Methods for Semantic Image Segmentation in Real-Time")
    中看到，以及用于图像生成的城市总体视图。
- en: '![Refer to caption](img/e44ee6faa8d944c121d2e0121a76c7df.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e44ee6faa8d944c121d2e0121a76c7df.png)'
- en: 'Figure 14: Sample synthetic image from SYNTHIA with its semantic label and
    a general view of the city (from [[100](#bib.bib100)]).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：来自 SYNTHIA 的合成图像样本及其语义标签以及城市的总体视图（来自 [[100](#bib.bib100)]）。
- en: 5.6 SIFT Flow
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 SIFT Flow
- en: SIFT Flow [[102](#bib.bib102)] is data set that processed a subset of LabelMe
    images [[103](#bib.bib103)] in ordered to provide accurate pixel-level annotation
    of 2688 frames. The top 33 object categories (with the most labeled pixels) were
    selected mostly from outdoor scenes. The images were relatively small in size
    ($256\times 256$ pixels), and they were generated to evaluate the scene parsing
    algorithm of the authors.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT Flow [[102](#bib.bib102)] 是一个处理了 LabelMe 图像 [[103](#bib.bib103)] 子集的数据集，提供了
    2688 帧的准确像素级注释。选择了前 33 个对象类别（标注像素最多的），主要来自户外场景。图像的大小相对较小（$256\times 256$ 像素），生成这些图像的目的是评估作者的场景解析算法。
- en: 5.7 CamVid
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 CamVid
- en: CamVid [[104](#bib.bib104)] is another urban scene dataset that includes four
    high-definition video sequences captured at $960\times 720$ pixels at 30 frames
    per second. The total duration of the videos was a little over 22 minutes or around
    40K frames. Of the latter, 701 were manually labeled with 32 object classes. Interestingly,
    the annotation effort took approximately 230 man-hours for an average annotation
    time of just under 20 minutes. Every annotated image was inspected and confirmed
    by a second person for accuracy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: CamVid [[104](#bib.bib104)] 是另一个城市场景数据集，包括四个高定义视频序列，每秒 30 帧，分辨率为 $960\times
    720$ 像素。视频的总时长略超过 22 分钟，大约 40K 帧。其中，701 帧被手动标注，分为 32 个对象类别。有趣的是，注释工作花费了大约 230
    人小时，平均注释时间刚不到 20 分钟。每张注释图像都由第二人检查以确认准确性。
- en: 5.8 KITTI
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.8 KITTI
- en: The last dataset we will be looking at is KITTI [[105](#bib.bib105)] that is
    quite popular in the self-driving research, since it contains not only camera
    images, but also laser scans, high-precision GPS measurements and IMU accelerations
    from a combined GPS/IMU system, sensor data that most autonomous vehicle efforts
    typically collect. The data were collected while driving in and around Karlsruhe,
    Germany and it contains over 200 fully annotated images from 13 different classes
    [[106](#bib.bib106)]. Their semantic segmentation benchmark contains 14 entries,
    where performance metrics include runtime and environment information to the time-sensitive
    target applications.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的最后一个数据集是 KITTI [[105](#bib.bib105)]，它在自动驾驶研究中相当受欢迎，因为它不仅包含相机图像，还有激光扫描、高精度
    GPS 测量和来自 GPS/IMU 系统的 IMU 加速度，这些传感器数据是大多数自动驾驶车辆研究通常收集的。数据是在德国卡尔斯鲁厄及其周边地区驾驶时收集的，包含了来自
    13 个不同类别的 200 多张完全标注的图像 [[106](#bib.bib106)]。它们的语义分割基准包含 14 个条目，其中性能指标包括运行时间和环境信息，适用于时间敏感的目标应用。
- en: 6 Metrics
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 指标
- en: In this section we will be summarizing the basic metrics used to evaluate different
    semantic segmentation approaches. They either look at the accuracy of the segmentation
    output (i.e., how close it is to the ground truth) or the efficiency of the approach
    (i.e., inference time and memory usage).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将总结用于评估不同语义分割方法的基本指标。它们要么关注分割输出的准确性（即，它与真实值的接近程度），要么关注方法的效率（即，推断时间和内存使用）。
- en: 6.1 Confusion matrix
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 混淆矩阵
- en: In a segmentation task where there is a total of $C$ classes, the confusion
    matrix is a $C\times C$ table, where the element in position $(i,j)$ represents
    the count of pixels that should belong to class $i$ but where classified to belong
    to class $j$. A good model would result in a confusion matrix that has high counts
    in its diagonal elements (i.e., correctly classified pixels).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个分割任务中，如果总共有 $C$ 个类别，那么混淆矩阵是一个 $C\times C$ 的表格，其中位置 $(i,j)$ 处的元素表示应属于类别 $i$
    的像素数，但被分类为类别 $j$。一个好的模型会产生一个对角线元素（即正确分类的像素）的计数较高的混淆矩阵。
- en: 6.2 Normalized confusion matrix
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 归一化混淆矩阵
- en: It is derived from the confusion matrix, but every entry is normalized by dividing
    it to the total number of the predicted class $j$. This way all entries are in
    the range $[0,1]$.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 它源自混淆矩阵，但每个条目都通过除以预测类别 $j$ 的总数来进行归一化。这样所有条目都在 $[0,1]$ 范围内。
- en: 6.3 Accuracy
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 准确率
- en: Accuracy, or global accuracy, is the ratio of the correctly classified pixels
    over the total pixels. It can be derived from the confusion matrix by dividing
    the sum of the diagonal elements to the total pixels in the image. Accuracy can
    be misleading especiallly when the classes under consideration are not balanced.
    For example, if $95\%$ of the pixels are of one class (typically background),
    a trivial model always predicting this class will lead to $95\%$ accuracy, which
    does definitely not capture the dependencies of the segmentation task.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率，或称为全局准确率，是正确分类的像素与总像素的比率。可以通过将对角线元素的总和除以图像中的总像素数来从混淆矩阵中得出。准确率可能会误导，特别是当考虑的类别不平衡时。例如，如果
    $95\%$ 的像素属于一个类别（通常是背景），一个总是预测该类别的简单模型将导致 $95\%$ 的准确率，但这并不能准确捕捉分割任务的依赖关系。
- en: 6.4 Mean accuracy
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 平均准确率
- en: It is defined as the ratio of correctly classified pixels in each class to total
    pixels averaged over all classes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 它定义为每个类别中正确分类的像素与所有类别中的总像素的比率。
- en: 6.5 Mean intersection over union
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 平均交并比
- en: Mean intersection over union (mIoU) is a metric that addresses the class imbalance
    weakness of the accuracy metric. In particular, it compares the pixel-wise classification
    output of a model with the ground truth and finds their intersection and union
    (i.e., how many pixels were correctly classified as class $i$ for all classes
    $i$, as well as how many pixels where either classified or were annotated as class
    $i$ for all classes $i$). The ratio of the intersection over the union (summed
    over all classes) is the mIoU or Jaccard index. It is robust to class imbalances
    and is, arguably, the most popular metric when evaluating semantic segmentation
    tasks.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 平均交并比（mIoU）是一种解决准确率指标类别不平衡弱点的指标。特别是，它比较模型的逐像素分类输出与真实值，并找到它们的交集和并集（即，所有类别 $i$
    的像素中有多少像素被正确分类为类别 $i$，以及有多少像素被分类或标注为类别 $i$）。交集与并集的比率（对所有类别求和）即为 mIoU 或 Jaccard
    指数。它对类别不平衡具有鲁棒性，且在评估语义分割任务时可谓是最受欢迎的指标。
- en: 6.6 Weighted intersection over union
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 加权交并比
- en: This is a small variation of the previous metric to account for the number of
    pixels per class. It calculates the weighted average of the IoU for each class,
    weighted by the number of pixels in the class.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对先前指标的小幅调整，用于考虑每个类别的像素数量。它计算了每个类别的IoU的加权平均值，加权由类别中的像素数量决定。
- en: 6.7 Precision
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 精确度
- en: Precision for class $i$ is defined as the proportion of pixels classified as
    $i$ that were correctly classified. An average precision metric can be defined
    accordingly for multiple classes.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 $i$ 的精确度定义为被分类为 $i$ 的像素中正确分类的比例。可以相应地为多个类别定义一个平均精确度指标。
- en: 6.8 Recall
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8 召回率
- en: Recall for class $i$ is defined as the proportion of the actual pixels of class
    $i$ that were correctly classified. Similarly, an average recall metric can be
    defined accordingly for multiple classes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 $i$ 的召回率定义为被正确分类的类别 $i$ 的实际像素的比例。类似地，可以为多个类别定义一个平均召回率指标。
- en: 6.9 F1-score
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9 F1-score
- en: F1-score is aggregating the precision/recall metrics by calculating their harmonic
    mean. It combines features of both and provides information for both types of
    errors.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: F1-score是通过计算精确度/召回率指标的调和均值来汇总这两者的特征。它结合了这两者的特征，并提供了关于两种错误的信息。
- en: 6.10 Frames per second
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.10 每秒帧数
- en: All previous metrics measure at the accuracy of the model output, but do not
    capture the efficiency of the method. One important metric to capture is the inference
    speed of a network, i.e., the execution time measured in frames per second (fps).
    It is the inverse of the time to run inference of a new image on a fully trained
    network. In most real time applications, an fps of 30 or more is required, usually
    to outperform a typical video frame rate.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 所有先前的指标都衡量模型输出的准确性，但未能捕捉方法的效率。一个重要的指标是网络的推理速度，即以每秒帧数（fps）为单位的执行时间。它是对在完全训练的网络上推理新图像所需时间的倒数。在大多数实时应用中，需要达到30帧或更多的fps，通常以超越典型的视频帧率。
- en: 6.11 Memory usage
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.11 内存使用量
- en: Memory usage is a measure of the network size. It can either be measured in
    number of parameters (for a deep neural network approach), or the memory size
    to represent the network, or the number of floating point operations (FLOPs) required
    to run the model.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 内存使用量是衡量网络规模的指标。它可以通过参数数量（对于深度神经网络方法）、表示网络的内存大小，或运行模型所需的浮点运算次数（FLOPs）来衡量。
- en: 7 Performance Summary
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 性能总结
- en: In this section we will provide summary tables of the best performing models
    in semantic segmentation. Most papers get evaluated on a subset of the data sets
    provided earlier in this report and, for most works, computational efficiency
    is not an critical aspect of the design. As a result, it was decided to summarize
    the best performing models on the Cityscapes data set [[98](#bib.bib98)], which
    has been popular with most real-time architectures as an evaluation benchmark.
    Table [2](#S7.T2 "Table 2 ‣ 7 Performance Summary ‣ A Survey on Deep Learning
    Methods for Semantic Image Segmentation in Real-Time") summarizes the top ten
    performing models with respect to the mIoU with a short summary of the methods
    used to achieve these results. Anonymous submissions were not included in this
    section despite occupying some of the top performing spots in the benchmark evaluation.
    As can be seen in Table [2](#S7.T2 "Table 2 ‣ 7 Performance Summary ‣ A Survey
    on Deep Learning Methods for Semantic Image Segmentation in Real-Time"), most
    entries were published over the past few months, suggesting a very competitive
    landscape with remarkably fast progress.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提供语义分割中表现最佳模型的总结表。大多数论文都在本报告早期提供的数据集的子集上进行评估，对于大多数工作，计算效率并不是设计的关键方面。因此，决定总结在Cityscapes数据集[[98](#bib.bib98)]上的表现最佳模型，该数据集在大多数实时架构中作为评估基准。表[2](#S7.T2
    "Table 2 ‣ 7 Performance Summary ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time")总结了关于mIoU的前十名模型及其实现这些结果的方法的简要总结。匿名提交虽然占据了基准评估中的一些前列位置，但未包括在本节中。如表[2](#S7.T2
    "Table 2 ‣ 7 Performance Summary ‣ A Survey on Deep Learning Methods for Semantic
    Image Segmentation in Real-Time")所示，大多数条目是在过去几个月内发布的，这表明竞争激烈且进展显著迅速。
- en: '| Model | mIoU | Methods | Year |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | mIoU | 方法 | 年份 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Hierarchical Multi-Scale Attention for Semantic Segmentation [[64](#bib.bib64)]
    | 85.4 | Hierarchical Attention | 2020 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 层次多尺度注意力用于语义分割 [[64](#bib.bib64)] | 85.4 | 层次注意力 | 2020 |'
- en: '| Naive-Student (iterative semi-supervised learning with Panoptic-DeepLab)
    [[107](#bib.bib107)] | 85.2 | Pseudo-Label Prediction, Data Augmentation | 2020
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Naive-Student（使用 Panoptic-DeepLab 的迭代半监督学习） [[107](#bib.bib107)] | 85.2 |
    伪标签预测，数据增强 | 2020 |'
- en: '| Object-Contextual Representations for Semantic Segmentation [[108](#bib.bib108)]
    | 84.5 | Coarse Soft Segmentation, Weighted Aggregation | 2020 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 语义分割的对象上下文表示 [[108](#bib.bib108)] | 84.5 | 粗略软分割，加权聚合 | 2020 |'
- en: '| Panoptic-DeepLab [[60](#bib.bib60)] | 84.5 | Panoptic Segmentation | 2020
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Panoptic-DeepLab [[60](#bib.bib60)] | 84.5 | 全景分割 | 2020 |'
- en: '| EfficientPS: Efficient Panoptic Segmentation [[61](#bib.bib61)] | 84.2 |
    Panoptic Segmentation | 2020 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| EfficientPS: 高效全景分割 [[61](#bib.bib61)] | 84.2 | 全景分割 | 2020 |'
- en: '| Axial-DeepLab [[109](#bib.bib109)] | 84.1 | Panoptic Segmentation, Self Attention
    | 2020 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Axial-DeepLab [[109](#bib.bib109)] | 84.1 | 全景分割，自注意力 | 2020 |'
- en: '| Improving Semantic Segmentation via Decoupled Body and Edge Supervision [[110](#bib.bib110)]
    | 83.7 | Decoupled Multi-scale Feature Training | 2020 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 通过解耦体部和边缘监督改进语义分割 [[110](#bib.bib110)] | 83.7 | 解耦多尺度特征训练 | 2020 |'
- en: '| Improving Semantic Segmentation via Video Propagation and Label Relaxation
    [[111](#bib.bib111)] | 83.5 | Joint Future Frame/Label Propagation, Data Augmentation
    | 2019 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 通过视频传播和标签松弛改进语义分割 [[111](#bib.bib111)] | 83.5 | 联合未来帧/标签传播，数据增强 | 2019 |'
- en: '| Hard Pixel Mining for Depth Privileged Semantic Segmentation [[112](#bib.bib112)]
    | 83.4 | Depth Information, Depth-Aware Loss | 2019 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 深度特权语义分割的硬像素挖掘 [[112](#bib.bib112)] | 83.4 | 深度信息，深度感知损失 | 2019 |'
- en: '| Global Aggregation then Local Distribution in Fully Convolutional Networks
    [[113](#bib.bib113)] | 83.3 | Global Aggregation, Local Distribution | 2019 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 在全卷积网络中先全局聚合再局部分布 [[113](#bib.bib113)] | 83.3 | 全局聚合，局部分布 | 2019 |'
- en: 'Table 2: Cityscapes Pixel-Level Semantic Labeling Task Top Performing Models'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 2: Cityscapes 像素级语义标注任务的最佳表现模型'
- en: Table [3](#S7.T3 "Table 3 ‣ 7 Performance Summary ‣ A Survey on Deep Learning
    Methods for Semantic Image Segmentation in Real-Time") ranks real-time semantic
    segmentation works where the performance metric is inference speed (i.e., frames
    per second (FPS)). Three of the top ten positions are occupied by a single paper
    [[85](#bib.bib85)], which clearly demonstrates the performance/efficiency trade-offs.
    However, as this table shows, real-time semantic segmentation is a reality and
    several architectures achieve accuracy close to state-of-the-art semantic segmentation
    models.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [3](#S7.T3 "表格 3 ‣ 7 性能总结 ‣ 实时语义图像分割深度学习方法综述") 排列了实时语义分割工作，其中性能指标是推理速度（即每秒帧数（FPS））。前十名中的三个位次由单篇论文
    [[85](#bib.bib85)] 占据，这清楚地展示了性能/效率的权衡。然而，正如这张表所示，实时语义分割已经成为现实，多个架构达到了接近最先进的语义分割模型的准确性。
- en: '| Model | FPS | mIoU | Methods | Year |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | FPS | mIoU | 方法 | 年份 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| FastSCNN (quarter-resolution) [[85](#bib.bib85)] | 485 | 51.9 | Two-branch
    Networks, Depthwise Separable Convolutions | 2019 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| FastSCNN（四分之一分辨率） [[85](#bib.bib85)] | 485 | 51.9 | 双分支网络，深度可分离卷积 | 2019
    |'
- en: '| FastSCNN (half-resolution) [[85](#bib.bib85)] | 286 | 63.8 | Two-branch Networks,
    Depthwise Separable Convolutions | 2019 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| FastSCNN（半分辨率） [[85](#bib.bib85)] | 286 | 63.8 | 双分支网络，深度可分离卷积 | 2019 |'
- en: '| FasterSeg [[114](#bib.bib114)] | 163 | 71.5 | Neural Architecture Search
    | 2020 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| FasterSeg [[114](#bib.bib114)] | 163 | 71.5 | 神经架构搜索 | 2020 |'
- en: '| LiteSeg [[115](#bib.bib115)] | 161 | 67.8 | Depthwise Separable Convolutions,
    Dilated Convolutions | 2019 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| LiteSeg [[115](#bib.bib115)] | 161 | 67.8 | 深度可分离卷积，膨胀卷积 | 2019 |'
- en: '| Partial Order Pruning [[116](#bib.bib116)] | 143 | 71.4 | Neural Architecture
    Search, Pruning | 2019 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 部分顺序剪枝 [[116](#bib.bib116)] | 143 | 71.4 | 神经架构搜索，剪枝 | 2019 |'
- en: '| RPNet [[117](#bib.bib117)] | 125 | 68.3 | Early Downsampling, Residual Blocks
    | 2019 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| RPNet [[117](#bib.bib117)] | 125 | 68.3 | 早期下采样，残差块 | 2019 |'
- en: '| FastSCNN [[85](#bib.bib85)] | 123 | 68 | Two-branch Networks, Depthwise Separable
    Convolutions | 2019 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| FastSCNN [[85](#bib.bib85)] | 123 | 68 | 双分支网络，深度可分离卷积 | 2019 |'
- en: '| Spatial Sampling Network for Fast Scene Understanding [[118](#bib.bib118)]
    | 113 | 68.9 | Smaller Decoder Size | 2019 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 快速场景理解的空间采样网络 [[118](#bib.bib118)] | 113 | 68.9 | 较小的解码器尺寸 | 2019 |'
- en: '| ESPNet [[80](#bib.bib80)] | 112 | 60.3 | Dilated Convolutions, Depthwise
    Separable Convolutions | 2018 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ESPNet [[80](#bib.bib80)] | 112 | 60.3 | 膨胀卷积，深度可分离卷积 | 2018 |'
- en: '| Efficient Dense Modules of Asymmetric Convolution [[119](#bib.bib119)] |
    108 | 67.3 | Dilated Convolutions, Asymmetric Convolutions | 2018 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 高效的非对称卷积密集模块 [[119](#bib.bib119)] | 108 | 67.3 | 膨胀卷积、非对称卷积 | 2018 |'
- en: 'Table 3: Cityscapes Pixel-Level Semantic Labeling Task Top Performing Real-Time
    Models'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '表3: Cityscapes像素级语义标注任务顶级实时模型'
- en: 8 Summary
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 摘要
- en: This work provides an extensive summary of the most recent advances in semantic
    image segmentation with deep learning methods focusing on real-time applications.
    It starts with an explanation of the segmentation task and how it differs from
    similar tasks, continues with a history of early segmentation methods, and provides
    a detailed description of the different deep learning approaches of the last decade.
    An extensive list of techniques to improve the efficiency of deep learning networks,
    by optimizing different aspects of the network, is then provided and the trade-offs
    in these design choices are explained. The most widely used benchmark data sets
    are subsequently described, followed by a list of metrics used to evaluate the
    accuracy and efficiency of the proposed models. Finally, performance tables are
    provided to summarize the state-of-the-art approaches in the area of semantic
    segmentation, both from an accuracy perspective, as well as an efficiency one.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了关于语义图像分割的最新进展的广泛总结，重点关注实时应用。首先解释了分割任务及其与类似任务的区别，接着回顾了早期分割方法的历史，并详细描述了过去十年中不同的深度学习方法。随后提供了通过优化网络不同方面来提高深度学习网络效率的技术列表，并解释了这些设计选择中的权衡。接下来描述了最常用的基准数据集，随后列出了用于评估提出模型准确性和效率的指标。最后，提供了性能表格，以总结在语义分割领域的最先进方法，从准确性和效率两个方面进行总结。
- en: Recent advances in deep learning methods, contemporaneously with a rapid increase
    in image capturing capabilities, have made image segmentation a crucial tool in
    a plethora of applications, from medical imaging to time-critical applications
    such as autonomous driving. This survey summarizes recent breakthroughs that transformed
    the field of image segmentation and provides a comprehensive insight on the design
    choices that led to this transformation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法的最新进展，加上图像捕捉能力的快速提高，使得图像分割成为从医学成像到诸如自动驾驶等时间敏感应用中至关重要的工具。此调查总结了变革图像分割领域的最新突破，并提供了对导致这些变革的设计选择的全面见解。
- en: References
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Krizhevsky, Alex, Sutskever, Ilya, Hinton, Geoffrey. (2012). "ImageNet
    Classification with Deep Convolutional Neural Networks", Neural Information Processing
    Systems.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Krizhevsky, Alex, Sutskever, Ilya, Hinton, Geoffrey. (2012). "使用深度卷积神经网络进行ImageNet分类",
    神经信息处理系统。'
- en: '[2] Z. Zhao, P. Zheng, S. Xu, X. Wu, "Object Detection with Deep Learning:
    A Review," arXiv:1807.05511v2.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Z. Zhao, P. Zheng, S. Xu, X. Wu, "深度学习的目标检测：综述," arXiv:1807.05511v2。'
- en: '[3] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The PASCAL Visual Object Classes (VOC) Challenge,” *International Journal of
    Computer Vision*, vol. 88, pp. 303–338, 2010.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, 和 A. Zisserman, “PASCAL视觉目标类别（VOC）挑战赛,”
    *国际计算机视觉杂志*, 卷 88, 页 303–338, 2010。'
- en: '[4] A. Novikov, D. Lenis, D. Major, J. Hladůvka, M. Wimmer, K. Bühler "Fully
    Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs,"
    *arXiv:1701.08816*.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Novikov, D. Lenis, D. Major, J. Hladůvka, M. Wimmer, K. Bühler "用于胸部X光图像的多类分割的全卷积架构,"
    *arXiv:1701.08816*。'
- en: '[5] Wang G., Li W., Ourselin S., Vercauteren T. (2019) Automatic Brain Tumor
    Segmentation Using Convolutional Neural Networks with Test-Time Augmentation.
    In: Crimi A., Bakas S., Kuijf H., Keyvan F., Reyes M., van Walsum T. (eds) Brainlesion:
    Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. BrainLes 2018\.
    Lecture Notes in Computer Science, vol 11384\. Springer, Cham.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Wang G., Li W., Ourselin S., Vercauteren T. (2019) 使用卷积神经网络和测试时增强的自动脑肿瘤分割。见：Crimi
    A., Bakas S., Kuijf H., Keyvan F., Reyes M., van Walsum T. (eds) Brainles: 胶质瘤、多发性硬化症、中风和创伤性脑损伤。BrainLes
    2018\. 计算机科学讲义，卷 11384\. Springer, Cham。'
- en: '[6] V. Mukha, I. Sharony, "Disparity Image Segmentation For ADAS," arXiv:1806.10350.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] V. Mukha, I. Sharony, "用于ADAS的视差图像分割," arXiv:1806.10350。'
- en: '[7] A. Sagar, R. Soundrapandiyan, "Semantic Segmentation With Multi Scale Spatial
    Attention For Self Driving Cars," arXiv:2007.12685.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Sagar, R. Soundrapandiyan, "用于自驾车的多尺度空间注意力语义分割," arXiv:2007.12685。'
- en: '[8] Yoshihara, A., Hascoet, T., Takiguchi, T., Ariki, Y., "Satellite Image
    Semantic Segmentation Using Fully Convolutional Network" (2018).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Yoshihara, A., Hascoet, T., Takiguchi, T., Ariki, Y.，“使用全卷积网络的卫星图像语义分割”（2018）。'
- en: '[9] A. King, S. M. Bhandarkar and B. M. Hopkinson, "A Comparison of Deep Learning
    Methods for Semantic Segmentation of Coral Reef Survey Images," 2018 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Salt
    Lake City, UT, 2018, pp. 1475-14758.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. King, S. M. Bhandarkar 和 B. M. Hopkinson，“深度学习方法在珊瑚礁调查图像语义分割中的比较”，2018
    IEEE/CVF计算机视觉与模式识别研讨会（CVPRW），盐湖城，UT，2018年，第1475-14758页。'
- en: '[10] Andres Milioto, Philipp Lottes, Cyrill Stachniss, "Real-time Semantic
    Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background
    Knowledge in CNNs," arXiv:1709.06764.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Andres Milioto, Philipp Lottes, Cyrill Stachniss，“利用CNN中的背景知识进行精准农业机器人作物和杂草的实时语义分割”，arXiv:1709.06764。'
- en: '[11] J. Martinsson and O. Mogren, "Semantic Segmentation of Fashion Images
    Using Feature Pyramid Networks," 2019 IEEE/CVF International Conference on Computer
    Vision Workshop (ICCVW), Seoul, Korea (South), 2019, pp. 3133-3136.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Martinsson 和 O. Mogren，“使用特征金字塔网络的时尚图像语义分割”，2019 IEEE/CVF国际计算机视觉会议研讨会（ICCVW），首尔，韩国，2019年，第3133-3136页。'
- en: '[12] L. S. Davis, A. Rosenfeld and J. S. Weszka, "Region Extraction by Averaging
    and Thresholding," in IEEE Transactions on Systems, Man, and Cybernetics, vol.
    SMC-5, no. 3, pp. 383-388, May 1975, doi: 10.1109/TSMC.1975.5408419.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L. S. Davis, A. Rosenfeld 和 J. S. Weszka，“通过平均和阈值提取区域”，发表于《IEEE系统、人工智能与控制论学报》，第SMC-5卷，第3期，第383-388页，1975年5月，doi:
    10.1109/TSMC.1975.5408419。'
- en: '[13] Salem Saleh Al-amri, N.V. Kalyankar and Khamitkar S.D., "Image Segmentation
    by Using Threshold Techniques," arXiv:1005.4020.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Salem Saleh Al-amri, N.V. Kalyankar 和 Khamitkar S.D.，“使用阈值技术的图像分割”，arXiv:1005.4020。'
- en: '[14] M. Özden and E. Polat, "Image segmentation using color and texture features,"
    2005 13th European Signal Processing Conference, Antalya, 2005, pp. 1-4.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Özden 和 E. Polat，“使用颜色和纹理特征的图像分割”，2005年第13届欧洲信号处理会议，安塔利亚，2005年，第1-4页。'
- en: '[15] H. P. Ng, S. H. Ong, K. W. C. Foong, P. S. Goh and W. L. Nowinski, "Medical
    Image Segmentation Using K-Means Clustering and Improved Watershed Algorithm,"
    2006 IEEE Southwest Symposium on Image Analysis and Interpretation, Denver, CO,
    2006, pp. 61-65.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] H. P. Ng, S. H. Ong, K. W. C. Foong, P. S. Goh 和 W. L. Nowinski，“使用K均值聚类和改进的分水岭算法的医学图像分割”，2006年IEEE西南图像分析与解释研讨会，丹佛，CO，2006年，第61-65页。'
- en: '[16] Z. Huang and D. Liu, "Segmentation of Color Image Using EM algorithm in
    HSV Color Space," 2007 International Conference on Information Acquisition, Seogwipo-si,
    2007, pp. 316-319.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Z. Huang 和 D. Liu，“使用HSV颜色空间中的EM算法的彩色图像分割”，2007年国际信息获取会议，西归浦市，2007年，第316-319页。'
- en: '[17] W. Tao, H. Jin and Y. Zhang, "Color Image Segmentation Based on Mean Shift
    and Normalized Cuts," in IEEE Transactions on Systems, Man, and Cybernetics, Part
    B (Cybernetics), vol. 37, no. 5, pp. 1382-1389, Oct. 2007.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] W. Tao, H. Jin 和 Y. Zhang，“基于均值漂移和归一化割的彩色图像分割”，发表于《IEEE系统、人工智能与控制论学报》，第37卷，第5期，第1382-1389页，2007年10月。'
- en: '[18] S. N. Sulaiman and N. A. Mat Isa, "Adaptive fuzzy-K-means clustering algorithm
    for image segmentation," in IEEE Transactions on Consumer Electronics, vol. 56,
    no. 4, pp. 2661-2668, November 2010.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. N. Sulaiman 和 N. A. Mat Isa，“用于图像分割的自适应模糊-K均值聚类算法”，发表于《IEEE消费电子学报》，第56卷，第4期，第2661-2668页，2010年11月。'
- en: '[19] N. Senthilkumaran and R. Rajesh, "Edge Detection Techniques for Image
    Segmentation – A Survey of Soft Computing Approaches," International Journal of
    Recent Trends in Engineering, Vol. 1, No. 2, May 2009.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] N. Senthilkumaran 和 R. Rajesh，“图像分割的边缘检测技术 – 软计算方法的调查”，《国际工程近期趋势杂志》，第1卷，第2期，2009年5月。'
- en: '[20] Roberts, Lawrence. (1963). Machine Perception of Three-Dimensional Solids.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Roberts, Lawrence. (1963). 三维固体的机器感知。'
- en: '[21] Sobel, Irwin. (2014). An Isotropic 3x3 Image Gradient Operator. Presentation
    at Stanford A.I. Project 1968.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Sobel, Irwin. (2014). 一种各向同性的3x3图像梯度算子。斯坦福人工智能项目1968年的演讲。'
- en: '[22] Prewitt, J.M.S. (1970). "Object Enhancement and Extraction". Picture processing
    and Psychopictorics. Academic Press.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Prewitt, J.M.S. (1970). “物体增强与提取”。《图像处理与心理图像学》。学术出版社。'
- en: '[23] Jianbo Shi and J. Malik, "Normalized cuts and image segmentation," in
    IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8,
    pp. 888-905, Aug. 2000.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jianbo Shi 和 J. Malik，“归一化割和图像分割”，发表于《IEEE模式分析与机器智能学报》，第22卷，第8期，第888-905页，2000年8月。'
- en: '[24] Peng B., Zhang L., Yang J. (2010) "Iterated Graph Cuts for Image Segmentation".
    In: Zha H., Taniguchi R., Maybank S. (eds) Computer Vision – ACCV 2009\. ACCV
    2009\. Lecture Notes in Computer Science, vol 5995\. Springer, Berlin, Heidelberg.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Peng B., Zhang L., Yang J. (2010) "图像分割的迭代图切割"。在：Zha H., Taniguchi R.,
    Maybank S. (编辑) 《计算机视觉 – ACCV 2009》。ACCV 2009。计算机科学讲义系列，第 5995 卷。施普林格，柏林，海德堡。'
- en: '[25] C. L. Zhao, "Image segmentation based on fast normalized cut." Open Cybernetics
    and Systemics Journal, 2015, 9(1):28-31.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. L. Zhao，"基于快速归一化切割的图像分割"，《开放网络系统与控制学报》，2015 年，第 9 卷，第 1 期，第 28-31 页。'
- en: '[26] Nowozin, Sebastian, and Christoph H. Lampert. "Structured learning and
    prediction in computer vision." Foundations and Trends in Computer Graphics and
    Vision.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Nowozin, Sebastian 和 Christoph H. Lampert，"计算机视觉中的结构化学习与预测"，《计算机图形学与视觉基础与趋势》。'
- en: '[27] John Lafferty, Andrew McCallum, and Fernando C.N. Pereira, "Conditional
    Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,"
    June 2001.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] John Lafferty, Andrew McCallum 和 Fernando C.N. Pereira，"条件随机场：用于分割和标记序列数据的概率模型"，2001
    年 6 月。'
- en: '[28] Philipp Krähenbühl, Vladlen Koltun, "Efficient Inference in Fully-Connected
    CRFs with Gaussian Edge Potentials," arXiv:1210.5644.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Philipp Krähenbühl, Vladlen Koltun，"具有高斯边缘势的全连接 CRFs 的高效推理"，arXiv:1210.5644。'
- en: '[29] K Simonyan, A. Zisserman, "Very Deep Convolutional Networks for Large-Scale
    Image Recognition," arXiv:1409.1556v6.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K Simonyan, A. Zisserman，"用于大规模图像识别的非常深的卷积网络"，arXiv:1409.1556v6。'
- en: '[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, A. Rabinovich, "Going Deeper with Convolutions," arXiv:1409.4842v1.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, A. Rabinovich，"更深的卷积网络"，arXiv:1409.4842v1。'
- en: '[31] J. Long, E. Shelhamer and T. Darrell, "Fully convolutional networks for
    semantic segmentation," 2015 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), Boston, MA, 2015, pp. 3431-3440, doi: 10.1109/CVPR.2015.7298965.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Long, E. Shelhamer 和 T. Darrell，"用于语义分割的全卷积网络"，2015 年 IEEE 计算机视觉与模式识别会议（CVPR），波士顿，MA，2015
    年，第 3431-3440 页，doi: 10.1109/CVPR.2015.7298965。'
- en: '[32] H. Noh, S. Hong and B. Han, "Learning Deconvolution Network for Semantic
    Segmentation," 2015 IEEE International Conference on Computer Vision (ICCV), Santiago,
    2015, pp. 1520-1528, doi: 10.1109/ICCV.2015.178.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] H. Noh, S. Hong 和 B. Han，"学习去卷积网络用于语义分割"，2015 年 IEEE 国际计算机视觉会议（ICCV），圣地亚哥，2015
    年，第 1520-1528 页，doi: 10.1109/ICCV.2015.178。'
- en: '[33] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks
    for biomedical image segmentation," in International Conference on Medical image
    computing and computer-assisted intervention. Springer, 2015, pp. 234–241.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] O. Ronneberger, P. Fischer 和 T. Brox，"U-net：用于生物医学图像分割的卷积网络"，在医学图像计算与计算机辅助干预国际会议上。施普林格，2015
    年，第 234–241 页。'
- en: '[34] V. Badrinarayanan, A. Kendall, and R. Cipolla, "Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation," IEEE transactions on pattern
    analysis and machine intelligence, vol. 39, no. 12, pp. 2481–2495, 2017.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] V. Badrinarayanan, A. Kendall 和 R. Cipolla，"Segnet：用于图像分割的深度卷积编码器-解码器架构"，IEEE
    模式分析与机器智能学报，第 39 卷，第 12 期，第 2481–2495 页，2017 年。'
- en: '[35] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan
    L. Yuille, "Semantic Image Segmentation with Deep Convolutional Nets and Fully
    Connected CRFs," arXiv:1412.7062v4.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan
    L. Yuille，"使用深度卷积网络和全连接 CRFs 的语义图像分割"，arXiv:1412.7062v4。'
- en: '[36] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy and A. L. Yuille, "DeepLab:
    Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution,
    and Fully Connected CRFs," in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 40, no. 4, pp. 834-848, 1 April 2018, doi: 10.1109/TPAMI.2017.2699184.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy 和 A. L. Yuille，"DeepLab：使用深度卷积网络、空洞卷积和全连接
    CRFs 的语义图像分割"，发表于 IEEE 模式分析与机器智能杂志，第 40 卷，第 4 期，第 834-848 页，2018 年 4 月 1 日，doi:
    10.1109/TPAMI.2017.2699184。'
- en: '[37] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet,
    Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, "Conditional Random Fields
    as Recurrent Neural Networks," arXiv:1502.03240v3.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet,
    Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr，"条件随机场作为递归神经网络"，arXiv:1502.03240v3。'
- en: '[38] Wei Liu, Andrew Rabinovich, Alexander C. Berg, "ParseNet: Looking Wider
    to See Better," arXiv:1506.04579v2.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Wei Liu, Andrew Rabinovich, Alexander C. Berg，"ParseNet：更广泛的视角以获得更好的效果"，arXiv:1506.04579v2。'
- en: '[39] Kim, Dong, Arsalan, Muhammad, Owais, Muhammad, Park, Kang. (2020). "ESSN:
    Enhanced Semantic Segmentation Network by Residual Concatenation of Feature Maps."
    IEEE Access. PP. 10.1109/ACCESS.2020.2969442.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Kim, Dong, Arsalan, Muhammad, Owais, Muhammad, Park, Kang. (2020). “ESSN:
    通过特征图残差级联增强的语义分割网络。” IEEE Access. 页码：10.1109/ACCESS.2020.2969442。'
- en: '[40] Tao Yang, Yan Wu, Junqiao Zhao, Linting Guan, "Semantic Segmentation via
    Highly Fused Convolutional Network with Multiple Soft Cost Functions," arXiv:1801.01317v1.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Tao Yang, Yan Wu, Junqiao Zhao, Linting Guan, “通过具有多个软代价函数的高度融合卷积网络进行语义分割，”
    arXiv:1801.01317v1。'
- en: '[41] Goodfellow, Ian; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley,
    David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). "Generative Adversarial
    Networks." Proceedings of the International Conference on Neural Information Processing
    Systems (NIPS 2014). pp. 2672–2680.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Goodfellow, Ian; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley,
    David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). “生成对抗网络。” 国际神经信息处理系统会议（NIPS
    2014）论文集。页码：2672–2680。'
- en: '[42] Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler, Gokula Krishnan
    Santhanam, "Generative Adversarial Networks recover features in astrophysical
    images of galaxies beyond the deconvolution limit," arXiv:1702.00403v1.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler, Gokula Krishnan
    Santhanam, “生成对抗网络在超出反卷积极限的星系天体物理图像中恢复特征，” arXiv:1702.00403v1。'
- en: '[43] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B.
    Tenenbaum, "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial
    Modeling," arXiv:1610.07584v2.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B.
    Tenenbaum, “通过 3D 生成对抗建模学习物体形状的概率潜在空间，” arXiv:1610.07584v2。'
- en: '[44] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham,
    Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe
    Shi, "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial
    Network," arXiv:1609.04802v5.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham,
    Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe
    Shi, “使用生成对抗网络进行照片级真实单图像超分辨率，” arXiv:1609.04802v5。'
- en: '[45] Pauline Luc, Camille Couprie, Soumith Chintala, Jakob Verbeek, "Semantic
    Segmentation using Adversarial Networks," NIPS Workshop on Adversarial Training,
    Dec 2016, Barcelona, Spain.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Pauline Luc, Camille Couprie, Soumith Chintala, Jakob Verbeek, “使用对抗网络进行语义分割，”
    NIPS 对抗训练研讨会，2016年12月，西班牙巴塞罗那。'
- en: '[46] N. Souly, C. Spampinato and M. Shah, "Semi Supervised Semantic Segmentation
    Using Generative Adversarial Network," 2017 IEEE International Conference on Computer
    Vision (ICCV), Venice, 2017, pp. 5689-5697, doi: 10.1109/ICCV.2017.606.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] N. Souly, C. Spampinato 和 M. Shah，“使用生成对抗网络的半监督语义分割，” 2017 年 IEEE 计算机视觉国际会议（ICCV），威尼斯，2017
    年，页码：5689-5697，doi: 10.1109/ICCV.2017.606。'
- en: '[47] X. Zhang, X. Zhu, 3\. X. Zhang, N. Zhang, P. Li and L. Wang, "SegGAN:
    Semantic Segmentation with Generative Adversarial Network," 2018 IEEE Fourth International
    Conference on Multimedia Big Data (BigMM), Xi’an, 2018, pp. 1-5, doi: 10.1109/BigMM.2018.8499105.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] X. Zhang, X. Zhu, X. Zhang, N. Zhang, P. Li 和 L. Wang，“SegGAN：使用生成对抗网络的语义分割，”
    2018 年 IEEE 第四届国际多媒体大数据会议（BigMM），西安，2018 年，页码：1-5，doi: 10.1109/BigMM.2018.8499105。'
- en: '[48] Deep Learning (Ian J. Goodfellow, Yoshua Bengio and Aaron Courville),
    MIT Press, 2016.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 深度学习（Ian J. Goodfellow, Yoshua Bengio 和 Aaron Courville），麻省理工学院出版社，2016年。'
- en: '[49] Francesco Visin, Marco Ciccone, Adriana Romero, Kyle Kastner, Kyunghyun
    Cho, Yoshua Bengio, Matteo Matteucci, Aaron Courville, "ReSeg: A Recurrent Neural
    Network-Based Model for Semantic Segmentation," 2016 IEEE Conference on Computer
    Vision and Pattern Recognition Workshops (CVPRW), Las Vegas, NV, 2016, pp. 426-433,
    doi: 10.1109/CVPRW.2016.60.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Francesco Visin, Marco Ciccone, Adriana Romero, Kyle Kastner, Kyunghyun
    Cho, Yoshua Bengio, Matteo Matteucci, Aaron Courville，“ReSeg：基于递归神经网络的语义分割模型，”
    2016 年 IEEE 计算机视觉与模式识别会议研讨会（CVPRW），拉斯维加斯，NV，2016 年，页码：426-433，doi: 10.1109/CVPRW.2016.60。'
- en: '[50] Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron
    Courville, Yoshua Bengio, "ReNet: A Recurrent Neural Network Based Alternative
    to Convolutional Networks," arXiv:1505.00393v3.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron
    Courville, Yoshua Bengio, “ReNet：基于递归神经网络的卷积网络替代方案，” arXiv:1505.00393v3。'
- en: '[51] Andreas Pfeuffer, Karina Schulz, Klaus Dietmayer, "Semantic Segmentation
    of Video Sequences with Convolutional LSTMs," arXiv:1905.01058v1.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Andreas Pfeuffer, Karina Schulz, Klaus Dietmayer, “使用卷积 LSTM 对视频序列进行语义分割，”
    arXiv:1905.01058v1。'
- en: '[52] Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, "Neural Machine Translation
    by Jointly Learning to Align and Translate," arXiv:1409.0473v7.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio，"通过共同学习对齐和翻译进行神经机器翻译"，arXiv:1409.0473v7。'
- en: '[53] L. Chen, Y. Yang, J. Wang, W. Xu and A. L. Yuille, "Attention to Scale:
    Scale-Aware Semantic Image Segmentation," 2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 3640-3649, doi: 10.1109/CVPR.2016.396.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] L. Chen, Y. Yang, J. Wang, W. Xu 和 A. L. Yuille，"关注尺度：尺度感知的语义图像分割"，2016
    IEEE 计算机视觉与模式识别会议（CVPR），美国内华达州拉斯维加斯，2016年，第3640-3649页，doi: 10.1109/CVPR.2016.396。'
- en: '[54] Hanchao Li, Pengfei Xiong, Jie An, Lingxue Wang, "Pyramid Attention Network
    for Semantic Segmentation," arXiv:1805.10180v3.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Hanchao Li, Pengfei Xiong, Jie An, Lingxue Wang，"用于语义分割的金字塔注意力网络"，arXiv:1805.10180v3。'
- en: '[55] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, "Dual Attention
    Network for Scene Segmentation," 2019 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR), Long Beach, CA, USA, 2019, pp. 3141-3149, doi: 10.1109/CVPR.2019.00326.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang 和 H. Lu，"用于场景分割的双重注意力网络"，2019
    IEEE/CVF 计算机视觉与模式识别会议（CVPR），美国加利福尼亚州长滩，2019年，第3141-3149页，doi: 10.1109/CVPR.2019.00326。'
- en: '[56] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei and W. Liu, "CCNet: Criss-Cross
    Attention for Semantic Segmentation," 2019 IEEE/CVF International Conference on
    Computer Vision (ICCV), Seoul, Korea (South), 2019, pp. 603-612, doi: 10.1109/ICCV.2019.00069.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei 和 W. Liu，"CCNet: 跨越交叉注意力用于语义分割"，2019
    IEEE/CVF 计算机视觉国际会议（ICCV），韩国首尔，2019年，第603-612页，doi: 10.1109/ICCV.2019.00069。'
- en: '[57] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia, "PSANet:
    Point-wise Spatial Attention Network for Scene Parsing". In: Ferrari V., Hebert
    M., Sminchisescu C., Weiss Y. (eds) Computer Vision – ECCV 2018\. ECCV 2018\.
    Lecture Notes in Computer Science, vol 11213.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin 和 J. Jia，"PSANet:
    点状空间注意力网络用于场景解析"。见：Ferrari V., Hebert M., Sminchisescu C., Weiss Y.（编）计算机视觉 –
    ECCV 2018，ECCV 2018，计算机科学讲义，第11213卷。'
- en: '[58] C. Kaul, S. Manandhar and N. Pears, "Focusnet: An Attention-Based Fully
    Convolutional Network for Medical Image Segmentation," 2019 IEEE 16th International
    Symposium on Biomedical Imaging (ISBI 2019), Venice, Italy, 2019, pp. 455-458,
    doi: 10.1109/ISBI.2019.8759477.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] C. Kaul, S. Manandhar 和 N. Pears，"Focusnet: 一种基于注意力的全卷积网络用于医学图像分割"，2019
    IEEE 第16届生物医学成像国际研讨会（ISBI 2019），意大利威尼斯，2019年，第455-458页，doi: 10.1109/ISBI.2019.8759477。'
- en: '[59] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár,
    "Panoptic Segmentation," arXiv:1801.00868v3.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár，"全景分割"，arXiv:1801.00868v3。'
- en: '[60] Bowen Cheng, Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang,
    Hartwig Adam, Liang-Chieh Chen, "Panoptic-DeepLab: A Simple, Strong, and Fast
    Baseline for Bottom-Up Panoptic Segmentation," arXiv:1911.10194v3.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Bowen Cheng, Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang,
    Hartwig Adam, Liang-Chieh Chen，"Panoptic-DeepLab: 一种简单、强大且快速的自下而上的全景分割基线"，arXiv:1911.10194v3。'
- en: '[61] Rohit Mohan, Abhinav Valada, "EfficientPS: Efficient Panoptic Segmentation,"
    arXiv:2004.02307v2.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Rohit Mohan, Abhinav Valada，"EfficientPS: 高效的全景分割"，arXiv:2004.02307v2。'
- en: '[62] *https://www.Cityscapes-dataset.com/benchmarks/*'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] *https://www.Cityscapes-dataset.com/benchmarks/*'
- en: '[63] *http://host.robots.ox.ac.uk/pascal/VOC/*'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] *http://host.robots.ox.ac.uk/pascal/VOC/*'
- en: '[64] Andrew Tao, Karan Sapra, Bryan Catanzaro, "Hierarchical Multi-Scale Attention
    for Semantic Segmentation," arXiv:2005.10821v1.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Andrew Tao, Karan Sapra, Bryan Catanzaro，"用于语义分割的层次多尺度注意力"，arXiv:2005.10821v1。'
- en: '[65] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D.
    Cubuk, Quoc V. Le, "Rethinking Pre-training and Self-training," arXiv:2006.06882v1.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D.
    Cubuk, Quoc V. Le，"重新思考预训练和自我训练"，arXiv:2006.06882v1。'
- en: '[66] H. Scudder, "Probability of error of some adaptive pattern-recognition
    machines," in IEEE Transactions on Information Theory, vol. 11, no. 3, pp. 363-371,
    July 1965.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] H. Scudder，"一些自适应模式识别机器的错误概率"，见 IEEE 信息理论汇刊，第11卷，第3期，第363-371页，1965年7月。'
- en: '[67] Oppenheim, Alan V.; Schafer, Ronald W.; Buck, John R. (1999). Discrete-time
    signal processing (2nd ed.). Upper Saddle River, N.J.: Prentice Hall. ISBN 0-13-754920-2.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Oppenheim, Alan V.; Schafer, Ronald W.; Buck, John R.（1999）。离散时间信号处理（第2版）。新泽西州上萨德尔河：普伦蒂斯霍尔。ISBN
    0-13-754920-2。'
- en: '[68] Michael Mathieu, Mikael Henaff, Yann LeCun, "Fast Training of Convolutional
    Networks through FFTs," arXiv:1312.5851v5.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Michael Mathieu, Mikael Henaff, Yann LeCun，"通过 FFTs 进行卷积网络的快速训练"，arXiv:1312.5851v5。'
- en: '[69] Sheng Lin, Ning Liu, Mahdi Nazemi, Hongjia Li, Caiwen Ding, Yanzhi Wang,
    Massoud Pedram, "FFT-based deep learning deployment in embedded systems," 2018
    Design, Automation and Test in Europe Conference and Exhibition (DATE), Dresden,
    2018, pp. 1045-1050.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Sheng Lin, Ning Liu, Mahdi Nazemi, Hongjia Li, Caiwen Ding, Yanzhi Wang,
    Massoud Pedram, "基于FFT的深度学习在嵌入式系统中的部署," 2018年设计、自动化与测试欧洲会议及展览 (DATE), 德累斯顿, 2018,
    页码 1045-1050。'
- en: '[70] Song Han, Jeff Pool, John Tran, William J. Dally, "Learning both Weights
    and Connections for Efficient Neural Networks," arXiv:1506.02626v3.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Song Han, Jeff Pool, John Tran, William J. Dally, "学习权重和连接以实现高效神经网络,"
    arXiv:1506.02626v3。'
- en: '[71] Xinghao Chen, Yunhe Wang, Yiman Zhang, Peng Du, Chunjing Xu, Chang Xu,
    "Multi-Task Pruning for Semantic Segmentation Networks," arXiv:2007.08386v1.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Xinghao Chen, Yunhe Wang, Yiman Zhang, Peng Du, Chunjing Xu, Chang Xu,
    "用于语义分割网络的多任务剪枝," arXiv:2007.08386v1。'
- en: '[72] Yihui He, Xiangyu Zhang, Jian Sun, "Channel Pruning for Accelerating Very
    Deep Neural Networks," arXiv:1707.06168v2.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Yihui He, Xiangyu Zhang, Jian Sun, "通道剪枝以加速非常深的神经网络," arXiv:1707.06168v2。'
- en: '[73] J. Luo, H. Zhang, H. Zhou, C. Xie, J. Wu and W. Lin, "ThiNet: Pruning
    CNN Filters for a Thinner Net," in IEEE Transactions on Pattern Analysis and Machine
    Intelligence, vol. 41, no. 10, pp. 2525-2538, 1 Oct. 2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. Luo, H. Zhang, H. Zhou, C. Xie, J. Wu 和 W. Lin, "ThiNet: 剪枝CNN滤波器以获得更细的网络,"
    IEEE模式分析与机器智能汇刊, 卷 41, 号 10, 页码 2525-2538, 2019年10月1日。'
- en: '[74] Song Han, Huizi Mao, William J. Dally "Deep Compression: Compressing Deep
    Neural Networks with Pruning, Trained Quantization and Huffman Coding," arXiv:1510.00149v5.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Song Han, Huizi Mao, William J. Dally "深度压缩: 通过剪枝、训练量化和霍夫曼编码压缩深度神经网络,"
    arXiv:1510.00149v5。'
- en: '[75] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, Kwang-Ting Cheng,
    "Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational
    Capability and Advanced Training Algorithm," arXiv:1808.00278v5.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, Kwang-Ting Cheng,
    "Bi-Real Net: 通过改进的表示能力和先进的训练算法增强1-bit CNN的性能," arXiv:1808.00278v5。'
- en: '[76] L. Sifre. Rigid-motion scattering for image classification. PhD thesis,
    Ph. D. thesis, 2014.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] L. Sifre. 图像分类的刚性运动散射. 博士学位论文, 2014。'
- en: '[77] F. Chollet, "Xception: Deep Learning with Depthwise Separable Convolutions,"
    2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu,
    HI, 2017.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] F. Chollet, "Xception: 深度学习中的深度可分离卷积," 2017 IEEE计算机视觉与模式识别会议 (CVPR), 夏威夷檀香山,
    2017。'
- en: '[78] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, Hartwig Adam, "MobileNets: Efficient Convolutional
    Neural Networks for Mobile Vision Applications," arXiv:1704.04861v1.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, Hartwig Adam, "MobileNets: 移动视觉应用中的高效卷积神经网络,"
    arXiv:1704.04861v1。'
- en: '[79] Fisher Yu, Vladlen Koltun, "Multi-Scale Context Aggregation by Dilated
    Convolutions," arXiv:1511.07122v3.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Fisher Yu, Vladlen Koltun, "通过膨胀卷积进行多尺度上下文聚合," arXiv:1511.07122v3。'
- en: '[80] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, Hannaneh
    Hajishirzi, "ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic
    Segmentation," arXiv:1803.06815v3.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, Hannaneh
    Hajishirzi, "ESPNet: 用于语义分割的高效空间金字塔膨胀卷积," arXiv:1803.06815v3。'
- en: '[81] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello, "ENet:
    A Deep Neural Network Architecture for Real-Time Semantic Segmentation," arXiv:1606.02147v1.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello, "ENet:
    用于实时语义分割的深度神经网络架构," arXiv:1606.02147v1。'
- en: '[82] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens and Z. Wojna, "Rethinking
    the Inception Architecture for Computer Vision," 2016 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 2818-2826.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens 和 Z. Wojna, "重新思考计算机视觉的Inception架构,"
    2016 IEEE计算机视觉与模式识别会议 (CVPR), 拉斯维加斯, NV, 2016, 页码 2818-2826。'
- en: '[83] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He, "Aggregated
    Residual Transformations for Deep Neural Networks," arXiv:1611.05431v2.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He, "用于深度神经网络的聚合残差变换,"
    arXiv:1611.05431v2。'
- en: '[84] X. Zhang, X. Zhou, M. Lin and J. Sun, "ShuffleNet: An Extremely Efficient
    Convolutional Neural Network for Mobile Devices," 2018 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 6848-6856.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Zhang, X. Zhou, M. Lin 和 J. Sun, "ShuffleNet: 一种极其高效的移动设备卷积神经网络," 2018
    IEEE/CVF计算机视觉与模式识别会议, 盐湖城, UT, 2018, 页码 6848-6856。'
- en: '[85] Rudra P K Poudel, Stephan Liwicki, Roberto Cipolla, "Fast-SCNN: Fast Semantic
    Segmentation Network," arXiv:1902.04502v1.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Rudra P K Poudel, Stephan Liwicki, Roberto Cipolla，“Fast-SCNN：快速语义分割网络”，arXiv:1902.04502v1。'
- en: '[86] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, Nong Sang,
    "BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic
    Segmentation," arXiv:2004.02147v1.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, Nong Sang，“BiSeNet
    V2：具有引导聚合的双边网络，用于实时语义分割”，arXiv:2004.02147v1。'
- en: '[87] Sergey Ioffe, Christian Szegedy, "Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift," arXiv:1502.03167v3.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Sergey Ioffe, Christian Szegedy，“批量归一化：通过减少内部协变量偏移来加速深度网络训练”，arXiv:1502.03167v3。'
- en: '[88] J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and C. Bregler, “Efficient
    object localization using convolutional networks,” in Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition,2015, pp. 648–656.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] J. Tompson, R. Goroshin, A. Jain, Y. LeCun, 和 C. Bregler，“使用卷积网络进行高效的物体定位”，在IEEE计算机视觉与模式识别会议论文集，2015年，页码648–656。'
- en: '[89] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona,
    Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects
    in context. In European conference on computer vision, pages 740–755\. Springer,
    2014'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona,
    Deva Ramanan, Piotr Dollár, 和 C Lawrence Zitnick。“Microsoft coco：上下文中的常见物体”。在欧洲计算机视觉大会，页码740–755。Springer，2014年。'
- en: '[90] *https://cocodataset.org/#home*'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] *https://cocodataset.org/#home*'
- en: '[91] H. Caesar, J. Uijlings and V. Ferrari, "COCO-Stuff: Thing and Stuff Classes
    in Context," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    Salt Lake City, UT, 2018, pp. 1209-1218.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] H. Caesar, J. Uijlings 和 V. Ferrari，“COCO-Stuff：上下文中的事物和物品类别”，2018 IEEE/CVF计算机视觉与模式识别会议，盐湖城，UT，2018年，页码1209-1218。'
- en: '[92] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun,
    and A. Yuille, “The role of context for object detection and semantic segmentation
    in the wild,” *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*,
    2014.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun,
    和 A. Yuille，“上下文在野外物体检测和语义分割中的作用”，*IEEE计算机视觉与模式识别会议（CVPR）*，2014年。'
- en: '[93] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and A. Yuille, "Detect
    what you can: Detecting and representing objects using holistic models and body
    parts," *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2014.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, 和 A. Yuille，“检测你能检测的：使用整体模型和身体部位检测和表示物体”，*IEEE计算机视觉与模式识别会议（CVPR）*，2014年。'
- en: '[94] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. Yuille. Joint object
    and part segmentation using deep learned potentials. In 2015 IEEE International
    Conference on Computer Vision (ICCV), pages 1573–1581, 2015.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, 和 A. Yuille。“使用深度学习潜力进行联合物体和部分分割”。在2015
    IEEE国际计算机视觉会议（ICCV），页码1573–1581，2015年。'
- en: '[95] B. Hariharan, P. Arbeláez, L. Bourdev, S. Maji, and J. Malik. Semantic
    contours from inverse detectors. In 2011 International Conference on Computer
    Vision, pages 991–998, 2011.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] B. Hariharan, P. Arbeláez, L. Bourdev, S. Maji, 和 J. Malik。“从逆检测器中提取语义轮廓”。在2011年国际计算机视觉大会，页码991–998，2011年。'
- en: '[96] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
    Antonio Torralba, "Semantic Understanding of Scenes through the ADE20K Dataset,"
    arXiv:1608.05442v2.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
    Antonio Torralba，“通过ADE20K数据集对场景的语义理解”，arXiv:1608.05442v2。'
- en: '[97] *https://groups.csail.mit.edu/vision/datasets/ADE20K/*'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] *https://groups.csail.mit.edu/vision/datasets/ADE20K/*'
- en: '[98] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
    Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, "The Cityscapes Dataset
    for Semantic Urban Scene Understanding," arXiv:1604.01685v2.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
    Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele，“用于语义城市场景理解的Cityscapes数据集”，arXiv:1604.01685v2。'
- en: '[99] *https://www.cityscapes-dataset.com/*'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] *https://www.cityscapes-dataset.com/*'
- en: '[100] G. Ros, L. Sellart, J. Materzynska, D. Vazquez and A. M. Lopez, "The
    SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation
    of Urban Scenes," 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), Las Vegas, NV, 2016, pp. 3234-3243'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] G. Ros, L. Sellart, J. Materzynska, D. Vazquez 和 A. M. Lopez，“SYNTHIA数据集：用于城市场景语义分割的大量合成图像”，2016
    IEEE计算机视觉与模式识别会议（CVPR），拉斯维加斯，NV，2016年，页码3234-3243。'
- en: '[101] *https://synthia-dataset.net/*'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] *https://synthia-dataset.net/*'
- en: '[102] C. Liu, J. Yuen and A. Torralba, "Nonparametric scene parsing: Label
    transfer via dense scene alignment," 2009 IEEE Conference on Computer Vision and
    Pattern Recognition, Miami, FL, 2009, pp. 1972-1979.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] C. Liu, J. Yuen 和 A. Torralba，"非参数场景解析：通过密集场景对齐进行标签转移"，2009 IEEE 计算机视觉与模式识别会议，迈阿密，FL，2009，第1972-1979页。'
- en: '[103] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. "LabelMe:
    a database and web-based tool for image annotation." IJCV, 77(1-3):157–173, 2008.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] B. C. Russell, A. Torralba, K. P. Murphy 和 W. T. Freeman。"LabelMe：一个用于图像注释的数据库和基于网络的工具。"
    IJCV, 77(1-3):157–173, 2008年。'
- en: '[104] Gabriel J. Brostow, Julien Fauqueur, and Roberto Cipolla. "Semantic object
    classes in video: A high-definition ground truth database." Pattern Recognition
    Letters, 30:88–97, 2009.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Gabriel J. Brostow, Julien Fauqueur 和 Roberto Cipolla。"视频中的语义对象类别：一个高清晰度真实数据集。"
    《模式识别快报》，30:88–97, 2009年。'
- en: '[105] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, "Vision meets Robotics:
    The KITTI Dataset," The International Journal of Robotics Research, vol. 32, no.
    11, pp. 1231–1237, 2013.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] A. Geiger, P. Lenz, C. Stiller 和 R. Urtasun，"视觉遇见机器人：KITTI 数据集"，《国际机器人研究期刊》，第32卷，第11期，第1231-1237页，2013年。'
- en: '[106] *http://www.cvlibs.net/datasets/kitti/*'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*http://www.cvlibs.net/datasets/kitti/*'
- en: '[107] Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng, Maxwell D. Collins,
    Ekin D. Cubuk, Barret Zoph, Hartwig Adam, Jonathon Shlens, "Naive-Student: Leveraging
    Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation," arXiv:2005.10266v4.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng, Maxwell D. Collins,
    Ekin D. Cubuk, Barret Zoph, Hartwig Adam, Jonathon Shlens，"Naive-Student：利用半监督学习进行城市场景分割"，arXiv:2005.10266v4。'
- en: '[108] Yuhui Yuan, Xilin Chen, Jingdong Wang, "Object-Contextual Representations
    for Semantic Segmentation", arXiv:1909.11065v5.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Yuhui Yuan, Xilin Chen, Jingdong Wang，"用于语义分割的对象上下文表示"，arXiv:1909.11065v5。'
- en: '[109] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh
    Chen, "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation,"
    arXiv:2003.07853v2.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh
    Chen，"Axial-DeepLab：用于全景分割的独立轴注意力"，arXiv:2003.07853v2。'
- en: '[110] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen
    Lin, Shaohua Tan, Yunhai Tong, "Improving Semantic Segmentation via Decoupled
    Body and Edge Supervision," arXiv:2007.10035v2.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen
    Lin, Shaohua Tan, Yunhai Tong，"通过解耦体部和边缘监督改进语义分割"，arXiv:2007.10035v2。'
- en: '[111] Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam, Andrew
    Tao, Bryan Catanzaro, "Improving Semantic Segmentation via Video Propagation and
    Label Relaxation," arXiv:1812.01593v3.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam, Andrew
    Tao, Bryan Catanzaro，"通过视频传播和标签放松改进语义分割"，arXiv:1812.01593v3。'
- en: '[112] Zhangxuan Gu, Li Niu, Haohua Zhao, Liqing Zhang, "Hard Pixel Mining for
    Depth Privileged Semantic Segmentation," arXiv:1906.11437v5.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Zhangxuan Gu, Li Niu, Haohua Zhao, Liqing Zhang，"深度特权语义分割的困难像素挖掘"，arXiv:1906.11437v5。'
- en: '[113] Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai
    Tong, "Global Aggregation then Local Distribution in Fully Convolutional Networks,"
    arXiv:1909.07229v1.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai
    Tong，"全卷积网络中的全局聚合与局部分布"，arXiv:1909.07229v1。'
- en: '[114] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, Zhangyang
    Wang, "FasterSeg: Searching for Faster Real-time Semantic Segmentation," arXiv:1912.10917v2.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, Zhangyang
    Wang，"FasterSeg：寻找更快的实时语义分割"，arXiv:1912.10917v2。'
- en: '[115] Taha Emara, Hossam E. Abd El Munim, Hazem M. Abbas, "LiteSeg: A Novel
    Lightweight ConvNet for Semantic Segmentation," arXiv:1912.06683v1.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Taha Emara, Hossam E. Abd El Munim, Hazem M. Abbas，"LiteSeg：一种新型轻量级卷积网络用于语义分割"，arXiv:1912.06683v1。'
- en: '[116] Xin Li, Yiming Zhou, Zheng Pan, Jiashi Feng, "Partial Order Pruning:
    for Best Speed/Accuracy Trade-off in Neural Architecture Search," arXiv:1903.03777v2.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Xin Li, Yiming Zhou, Zheng Pan, Jiashi Feng，"部分有序修剪：神经网络架构搜索中的最佳速度/准确性权衡"，arXiv:1903.03777v2。'
- en: '[117] Xiaoyu Chen, Xiaotian Lou, Lianfa Bai, Jing Han, "Residual Pyramid Learning
    for Single-Shot Semantic Segmentation," arXiv:1903.09746v1.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Xiaoyu Chen, Xiaotian Lou, Lianfa Bai, Jing Han，"残差金字塔学习用于单次语义分割"，arXiv:1903.09746v1。'
- en: '[118] Davide Mazzini, Raimondo Schettini, "Spatial Sampling Network for Fast
    Scene Understanding," Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR) Workshops, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Davide Mazzini, Raimondo Schettini，"快速场景理解的空间采样网络"，IEEE/CVF 计算机视觉与模式识别会议（CVPR）研讨会论文集，2019年。'
- en: '[119] Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin, "Efficient
    Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation,"
    arXiv:1809.06323v3.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin, “高效的非对称卷积密集模块用于实时语义分割，”
    arXiv:1809.06323v3。'
