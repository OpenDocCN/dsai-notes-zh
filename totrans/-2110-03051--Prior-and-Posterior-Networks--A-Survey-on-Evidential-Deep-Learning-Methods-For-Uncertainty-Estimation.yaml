- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:51:02'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2110.03051] Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.03051](https://ar5iv.labs.arxiv.org/html/2110.03051)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods
    For Uncertainty Estimation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dennis Ulmer^(\faCompass) dennis.ulmer@mailbox.org Christian Hardmeier^(\faCompass)
    chrha@itu.dk Jes Frellsen^(\faRobot,\faCompressArrows*) jefr@dtu.dk
  prefs: []
  type: TYPE_NORMAL
- en: ^(\faCompass)IT University of Copenhagen, ^(\faRobot)Technical University of
    Denmark, ^(\faCompressArrows*)Pioneer Centre for Artificial Intelligence
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Popular approaches for quantifying predictive uncertainty in deep neural networks
    often involve distributions over weights or multiple models, for instance via
    Markov Chain sampling, ensembling, or Monte Carlo dropout. These techniques usually
    incur overhead by having to train multiple model instances or do not produce very
    diverse predictions. This comprehensive and extensive survey aims to familiarize
    the reader with an alternative class of models based on the concept of *Evidential
    Deep Learning*: For unfamiliar data, they aim to admit “what they don’t know”,
    and fall back onto a prior belief. Furthermore, they allow uncertainty estimation
    in a single model and forward pass by parameterizing *distributions over distributions*.
    This survey recapitulates existing works, focusing on the implementation in a
    classification setting, before surveying the application of the same paradigm
    to regression. We also reflect on the strengths and weaknesses compared to other
    existing methods and provide the most fundamental derivations using a unified
    notation to aid future research.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b257b25423bf826d0d0c7fb7d841b431.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Taxonomy of surveyed approaches, divided into tractable parameterizations
    of the prior or posterior on one axis (see [Tables 1](#S3.T1 "In 3.4.1 Prior Networks
    ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") and [2](#S3.T2 "Table 2 ‣ 3.4.2
    Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") for an overview) and into approaches
    for classification and regression on the other. Regression methods are outlined
    in [Table 3](#S4.T3 "In 4 Evidential Deep Learning for Regression ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many existing methods for uncertainty estimation leverage the concept of Bayesian
    model averaging: These include ensembling (Lakshminarayanan et al., [2017](#bib.bib99);
    Wilson & Izmailov, [2020](#bib.bib178)), Markov chain Monte Carlo sampling (de Freitas,
    [2003](#bib.bib28); Andrieu et al., [2000](#bib.bib2)) as well as variational
    inference approaches (Mackay, [1992](#bib.bib114); MacKay, [1995](#bib.bib112);
    Hinton & Van Camp, [1993](#bib.bib67); Neal, [2012](#bib.bib134)), including approaches
    such as Monte Carlo (MC) dropout (Gal & Ghahramani, [2016](#bib.bib45)) and Bayes-by-backprop
    (Blundell et al., [2015](#bib.bib13)). Bayesian model averaging for neural networks
    usually involves the approximation of an otherwise infeasible integral using MC
    samples. This causes the following problems: Firstly, the quality of the MC approximation
    depends on the veracity and diversity of samples from the weight posterior. Secondly,
    the approach often involves increasing the number of parameters in a model or
    training more model instances altogether. Recently, a new class of models has
    been proposed to side-step this conundrum by using a different factorization of
    the posterior predictive distribution. This allows computing uncertainty in a
    single forward pass and with a single set of weights. These models are grounded
    in a concept coined *Evidential Deep Learning*: For out-of-distribution (OOD)
    inputs, they are encouraged to fall back onto a prior. This is often described
    as *knowing what they don’t know*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we summarize the existing literature and provide an overview
    of Evidential Deep Learning approaches. We give an overview over all discussed
    work in [Figure 1](#S1.F1 "In 1 Introduction ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), where
    we distinguish surveyed works for classification between models parameterizing
    a Dirichlet prior ([Section 3.4.1](#S3.SS4.SSS1 "3.4.1 Prior Networks ‣ 3.4 Existing
    Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")) or posterior ([Section 3.4.2](#S3.SS4.SSS2 "3.4.2 Posterior
    Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep
    Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")). We further discuss similar
    methods for regression problems ([Section 4](#S4 "4 Evidential Deep Learning for
    Regression ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation")). As we will see, obtaining well-behaving
    uncertainty estimates can be challenging in the Evidential Deep Learning framework;
    proposed solutions that are also reflected in [Figure 1](#S1.F1 "In 1 Introduction
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") are the usage of OOD examples during training (Malinin
    & Gales, [2018](#bib.bib115); [2019](#bib.bib116); Nandy et al., [2020](#bib.bib133);
    Shen et al., [2020](#bib.bib151); Chen et al., [2018](#bib.bib18); Zhao et al.,
    [2019](#bib.bib188); Hu et al., [2021](#bib.bib71); Sensoy et al., [2020](#bib.bib147)),
    knowledge distillation (Malinin et al., [2020b](#bib.bib118); [a](#bib.bib117))
    or the incorporation of density estimation (Charpentier et al., [2020](#bib.bib16);
    [2022](#bib.bib17); Stadler et al., [2021](#bib.bib155)), which we discuss in
    more detail in [Section 6](#S6 "6 Discussion ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). This
    survey aims to both serve as an accessible introduction to this model family to
    the unfamiliar reader as well as an informative overview, in order to promote
    a wider application outside the uncertainty quantification literature. We also
    provide a collection of the most important derivations for the Dirichlet distribution
    for Machine Learning, which plays a central role in many of the discussed approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first introduce the central concepts for this survey, including Bayesian
    inference in [Section 2.1](#S2.SS1 "2.1 Bayesian Inference ‣ 2 Background ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), Bayesian model averaging in [Section 2.2](#S2.SS2 "2.2 Predictive
    Uncertainty in Neural Networks ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation") and
    Evidential Deep Learning in [Section 2.3](#S2.SS3 "2.3 Evidential Deep Learning
    ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation").¹¹1Note that in the following we will use
    the suggested notation of the TMLR journal, e.g. by using $P$ for probability
    mass and $p$ for probability density functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Bayesian Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The foundation of the following sections is Bayesian inference: Given some
    prior belief $p(\bm{\theta})$ about parameters of interest $\bm{\theta}$, we use
    available observations $\mathbb{D}=\{(x_{i},y_{i})\}_{i=1}^{N}$ and their likelihood
    $p(\mathbb{D}|\bm{\theta})$ to obtain an updated belief in form of the posterior
    $p(\bm{\theta}|\mathbb{D})\propto p(\mathbb{D}|\bm{\theta})p(\bm{\theta})$. This
    update rule is derived from Bayes’ rule, namely'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(\bm{\theta}&#124;\mathbb{D})=\frac{p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})}{p(\mathbb{D})}=\frac{p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})}{\int
    p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})d\bm{\theta}},$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where we often try to avoid computing the term in the denominator since marginalization
    over a large (continuous) parameter space of $\bm{\theta}$ is usually intractable.
    In order to perform a prediction $y$ for a new data point $\mathbf{x}$, we can
    now utilize the *posterior predictive distribution* defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(y&#124;\mathbf{x},\mathbb{D})=\int P(y&#124;\mathbf{x},\bm{\theta})p(\bm{\theta}&#124;\mathbb{D})d\bm{\theta}.$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Since we integrate over the entire parameter space of $\bm{\theta}$, weighting
    each prediction by the posterior probability of its parameters to obtain the final
    result, this process is referred to as *Bayesian model averaging* (BMA). Here,
    predictions $P(y|\mathbf{x},\bm{\theta})$ stemming from parameters that are plausible
    given the observed data will receive a higher weight $p(\bm{\theta}|\mathbb{D})$
    in the final prediction $P(y|\mathbf{x},\mathbb{D})$. As we will see in the following
    section, this factorization of the predictive predictive distribution also has
    beneficial properties for analyzing the uncertainty of a model.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Predictive Uncertainty in Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In probabilistic modelling, uncertainty is commonly divided into aleatoric
    and epistemic uncertainty (Der Kiureghian & Ditlevsen, [2009](#bib.bib31); Kendall
    & Gal, [2017](#bib.bib85); Hüllermeier & Waegeman, [2021](#bib.bib75)). Aleatoric
    uncertainty refers to the uncertainty that is induced by the data-generating process,
    for instance noise or inherent overlap between observed instances of classes.
    Epistemic uncertainty is the type of uncertainty about the optimal model parameters
    (or even hypothesis class). It is reducible with an increasing amount of data,
    as fewer and fewer possible models become a plausible fit. These two notions resurface
    when formulating the posterior predictive distribution for a new data point $\mathbf{x}$:²²2Note
    that the predictive distribution in [Equation 2](#S2.E2 "In 2.1 Bayesian Inference
    ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation") generalizes the common case for a single
    network prediction where $P(y|\mathbf{x},\mathbb{D})\approx P(y|\mathbf{x},\hat{\bm{\theta}})$.
    Mathematically, this is expressed by replacing the posterior $p(\bm{\theta}|\mathbb{D})$
    by a Dirac delta distribution as in [Equation 5](#S2.E5 "In 2.3 Evidential Deep
    Learning ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), where all probability density
    rests on a single parameter configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(y&#124;\mathbf{x},\mathbb{D})=\int\underbrace{P(y&#124;\mathbf{x},\bm{\theta})}_{\text{Aleatoric}}\underbrace{p(\bm{\theta}&#124;\mathbb{D})}_{\text{Epistemic}}d\bm{\theta}.$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Here, the first factor captures the aleatoric uncertainty about the correct
    prediction, while the second one expresses uncertainty about the correct model
    parameters—the more data we observe, the more density of $p(\bm{\theta}|\mathbb{D})$
    should lie on reasonable parameter values for $\bm{\theta}$. For high-dimensional
    real-valued parameters $\bm{\theta}$ like in neural networks, this integral becomes
    intractable, and is usually approximated using Monte Carlo samples:³³3For easier
    distributions, the integral can often be evaluated analytically exploiting conjugacy.
    Another approach for more complex distributions can be the method of moments (see
    e.g. Duan, [2021](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(y&#124;\mathbf{x},\mathbb{D})\approx\frac{1}{K}\sum_{k=1}^{K}P(y&#124;\mathbf{x},\bm{\theta}^{(k)});\quad\bm{\theta}^{(k)}\sim
    p(\bm{\theta}&#124;\mathbb{D})$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: based on $K$ different sets of parameters $\bm{\theta}^{(k)}$. Since this requires
    obtaining multiple versions of model parameters through some additional procedure,
    this however comes with the aforementioned problems of computational overhead
    and approximation errors, motivating the approaches discussed in this survey.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Evidential Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the traditional approach to predictive uncertainty estimation requires
    multiple parameter sets and can only approximate the predictive posterior, we
    can factorize [Equation 2](#S2.E2 "In 2.1 Bayesian Inference ‣ 2 Background ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") further to obtain a tractable form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(y&#124;\mathbf{x},\mathbb{D})$ | $\displaystyle=\iint\underbrace{P(y&#124;\bm{\pi})}_{\vphantom{\big{[}}\text{Aleatoric}}\underbrace{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}_{\vphantom{\big{[}}\text{\
    Distributional\ }}\underbrace{p(\bm{\theta}&#124;\mathbb{D})}_{\vphantom{\big{[}}\text{Epistemic}}d\bm{\pi}d\bm{\theta}\approx\int
    P(y&#124;\bm{\pi})\underbrace{\vphantom{\big{[}}p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})}_{p(\bm{\theta}&#124;\mathbb{D})\approx\delta(\bm{\theta}-\hat{\bm{\theta}})}d\bm{\pi}.$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'This factorization contains another type of uncertainty, which Malinin & Gales
    ([2018](#bib.bib115)) call the *distributional* uncertainty, uncertainty caused
    by the mismatch of training and test data distributions. In the last step, Malinin
    & Gales ([2018](#bib.bib115)) replace $p(\bm{\theta}|\mathbb{D})$ by a point estimate
    $\hat{\bm{\theta}}$ using the Dirac delta function, i.e. a single trained neural
    network, to get rid of the intractable integral. Although another integral remains,
    retrieving the uncertainty from this predictive distribution actually has a closed-form
    analytical solution for the Dirichlet (see [Section 3.3](#S3.SS3 "3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")). The advantage of this approach is further that it allows
    us to distinguish uncertainty about a data point because it is ambiguous from
    points coming from an entirely different data distribution. As an example, consider
    a binary classification problem, in which the data manifold consists of two overlapping
    clusters. As we are classifying a new data point, we obtain a distribution $P(y|\mathbf{x},\bm{\theta})$
    which is uniform over the two classes. What does this mean? The model might either
    be confident that the point lies in the region of overlap and is inherently ambiguous,
    or that the model is uncertain about the correct class. Without further context,
    we cannot distinguish between these two cases (Bengs et al., [2022](#bib.bib8);
    Hüllermeier, [2022](#bib.bib74)). Compare that to instead predicting $p(\bm{\pi}|\mathbf{x},\bm{\theta})$:
    If the data point is ambiguous, the resulting distribution will be centered on
    $0.5$, if the model is generally uncertain, the distribution will be uniform,
    allowing this distinction. We will illustrate this principle further in the upcoming
    [Sections 2.4](#S2.SS4 "2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and [3.3](#S3.SS3 "3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the neural network context in [Equation 5](#S2.E5 "In 2.3 Evidential Deep
    Learning ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), it should be noted that restricting
    oneself to a point estimate of the parameters prevent the estimation of epistemic
    uncertainty like in earlier works through the weight posterior $p(\bm{\theta}|\mathbb{D})$,
    as discussed in the next section. However, there are works like Haussmann et al.
    ([2019](#bib.bib58)); Zhao et al. ([2020](#bib.bib189)) that combine both approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The term *Evidential Deep Learning* (EDL) originates from the work of Sensoy
    et al. ([2018](#bib.bib146)) and is based on the *Theory of Evidence* (Dempster,
    [1968](#bib.bib29); Audun, [2018](#bib.bib5)): Within the theory, belief mass
    is assigned to set of possible states, e.g. class labels, and can also express
    a lack of evidence, i.e. an “I don’t know”. We can for instance generalize the
    predicted output of a neural classifier using the Dirichlet distribution, allowing
    us to express a lack of evidence through a uniform Dirichlet. This is different
    from a uniform Categorical distribution, which does not distinguish an equal probability
    for all classes from the lack of evidence. For the purpose of this survey, we
    define Evidential Deep Learning as a family of approaches in which a neural network
    can fall back onto a uniform prior for unknown inputs. While neural networks usually
    parameterize likelihood functions, approaches in this survey parameterize prior
    or posterior distributions instead. The advantages of this methodology are now
    demonstrated using the example in the following section.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.4 An Illustrating Example: The Iris Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8634e0ef5fefc1e0e3894ddde313e44.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) *Iris setosa*
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37b1a1b0ee80c1db32c014efec6fec47.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) *Iris versicolor*
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/85d09e49724a78779aaed213a2584675.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) *Iris virginica*
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c2c10915edf4368ab9f834c4d1ae644.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of different approaches to uncertainty quantifying on
    the Iris dataset, with examples for the classes given on the left ([Figures 2(a)](#S2.F2.sf1
    "In Figure 2 ‣ 2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"), [2(b)](#S2.F2.sf2 "Figure 2(b) ‣ Figure 2 ‣ 2.4 An Illustrating
    Example: The Iris Dataset ‣ 2 Background ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation") and [2(c)](#S2.F2.sf3
    "Figure 2(c) ‣ Figure 2 ‣ 2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")). On the right, the data is plotted alongside some predictions
    of a prior network (lighter colors indicate higher density) and an ensemble and
    MC Dropout model on the probability simplex, with $50$ predictions each. Iris
    images were taken from Wikimedia Commons, [2022a](#bib.bib175); [b](#bib.bib176);
    [c](#bib.bib177).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the advantages of EDL, we choose a classification problem based
    on the Iris dataset (Fisher, [1936](#bib.bib40)). It contains measurements of
    three different species of iris flowers (shown in [Figures 2(a)](#S2.F2.sf1 "In
    Figure 2 ‣ 2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), [2(b)](#S2.F2.sf2 "Figure 2(b) ‣ Figure 2 ‣ 2.4 An Illustrating
    Example: The Iris Dataset ‣ 2 Background ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation") and [2(c)](#S2.F2.sf3
    "Figure 2(c) ‣ Figure 2 ‣ 2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")). We use the dataset as made available through scikit-learn
    (Pedregosa et al., [2011](#bib.bib142)) and plot the relationship between the
    width and lengths measurements of the flowers’ petals in [Figure 2](#S2.F2 "In
    2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We train an deep neural network ensemble (Lakshminarayanan et al., [2017](#bib.bib99))
    with $50$ model instances, a model with MC Dropout (Gal & Ghahramani, [2016](#bib.bib45))
    with $50$ predictions and a prior network (Sensoy et al., [2018](#bib.bib146)),
    an example of EDL, on all available data points, and plot their predictions on
    three test points on the 3-probability simplex in [Figure 2](#S2.F2 "In 2.4 An
    Illustrating Example: The Iris Dataset ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").⁴⁴4For
    information about training and model details, see [Section A.1](#A1.SS1 "A.1 Iris
    Example Training Details ‣ Appendix A Code Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). On
    these simplices, each point signifies a Categorical distribution, with the proximity
    to one of the corners indicating a higher probability for the corresponding class.
    EDL methods for classification do not predict a single output distribution, but
    an entire *density over output distributions*.'
  prefs: []
  type: TYPE_NORMAL
- en: Test point <svg   height="14.16" overflow="visible"
    version="1.1" width="14.16"><g transform="translate(0,14.16) matrix(1 0 0 -1 0
    0) translate(7.08,0) translate(0,7.08)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    lies in a region of overlap between instances of *Iris versicolor* and *Iris virginica*,
    thus inducing high aleatoric uncertainty. In this case, we can see that the prior
    network places all of its density on between these two classes, similar to most
    of the predictions of the ensemble and MC Dropout (bottom right). However, some
    of the latter predictions still land in the center of the simplex. The point <svg
      height="14.16" overflow="visible" version="1.1"
    width="14.16"><g transform="translate(0,14.16) matrix(1 0 0 -1 0 0) translate(7.08,0)
    translate(0,7.08)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    is located in an area without training examples between instances of *Iris versicolor*
    and *setosa*, as well as close to a single *virginica* outlier. As shown in the
    top left, ensemble and MC Dropout predictions agree that the point belongs to
    either the *setosa* or *versicolor* class, with a slight preference for the former.
    The prior network concentrates its prediction on *versicolor*, but admits some
    uncertainty towards the two other choices. The last test point <svg 
     height="14.16" overflow="visible" version="1.1" width="14.16"><g
    transform="translate(0,14.16) matrix(1 0 0 -1 0 0) translate(7.08,0) translate(0,7.08)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    is placed in an area of the feature space devoid of any data, roughly equidistant
    from the three clusters of flowers. Similar to the previous example, the ensemble
    and MC dropout predictions on the top right show a preference for *Iris setosa*
    and *versicolor*, albeit with higher uncertainy. The prior network however shows
    an almost uniform density, admitting distributional uncertainty about this particular
    input.
  prefs: []
  type: TYPE_NORMAL
- en: 'This simple example provides some insights into the potential advantages of
    EDL: First of all, the prior network was able to provide reasonable uncertainty
    estimates in comparison with BMA methods. Secondly, the prior network is able
    to admit its lack of knowledge for the OOD data point by predicting an almost
    uniform prior, something that the other models are not able to. As laid out in
    [Section 3.3](#S3.SS3 "3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3
    Evidential Deep Learning for Classification ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), EDL actually
    allows the user to disentangle model uncertainty due to a simple lack of data
    and due to the input being out-of-distribution. Lastly, training the prior network
    only required a single model, which is a noticeable speed-up compared to MC Dropout
    and especially the training of ensembles.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Evidential Deep Learning for Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to introduce EDL methods for classification, we first give a brief
    introduction to the Dirichlet distribution and its role as a conjugate prior in
    Bayesian inference in [Section 3.1](#S3.SS1 "3.1 The Dirichlet distribution ‣
    3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). We
    then show in [Section 3.2](#S3.SS2 "3.2 Parameterization ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") how neural networks can parameterize
    Dirichlet distributions, while [Section 3.3](#S3.SS3 "3.3 Uncertainty Estimation
    with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation") reveals how such a parameterization can be exploited for efficient
    uncertainty estimation. The remaining sections enumerate different examples from
    the literature parameterizing either a prior ([Section 3.4.1](#S3.SS4.SSS1 "3.4.1
    Prior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")) or posterior Dirichlet distribution
    ([Section 3.4.2](#S3.SS4.SSS2 "3.4.2 Posterior Networks ‣ 3.4 Existing Approaches
    for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The Dirichlet distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e3a701ff2a27eda0f281f65a2b314678.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A prior Dirichlet distribution is updated with a vector of class
    observations. The posterior Dirichlet then shifts density towards the classes
    $k$ with more observed instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Modelling for instance a binary classification problem is commonly done using
    the Bernoulli likelihood. The Bernoulli likelihood has a single parameter $\pi$,
    indicating the probability of success (or of the positive class), and is given
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Bernoulli}(y&#124;\pi)=\pi^{y}(1-\pi)^{(1-y)}.$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'Within Bayesian inference as introduced in [Section 2](#S2 "2 Background ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"), the Beta distribution is a commonly used prior for a
    Bernoulli likelihood. It defines a probability distribution over the parameter
    $\pi$, itself possessing two shape parameters $\alpha_{1}$ and $\alpha_{2}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Beta}(\pi;\alpha_{1},\alpha_{2})=\frac{1}{B(\alpha_{1},\alpha_{2})}\pi^{\alpha_{1}-1}(1-\pi)^{\alpha_{2}-1};\quad
    B(\alpha_{1},\alpha_{2})=\frac{\Gamma(\alpha_{1})\Gamma(\alpha_{2})}{\Gamma(\alpha_{1}+\alpha_{2})};$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\Gamma(\cdot)$ stands for the gamma function, a generalization of the
    factorial to the real numbers, and $B(\cdot)$ is called the Beta function (not
    to be confused with the distribution). When extending the classification problem
    from two to an arbitrary number of classes, we use a Categorical likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Categorical}(y&#124;\bm{\pi})=\prod_{k=1}^{K}\pi_{k}^{\mathbf{1}_{y=k}},$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'in which $K$ denotes the number of categories or classes, and the class probabilities
    are expressed using a vector $\bm{\pi}\in[0,1]^{K}$with $\sum_{k}\pi_{k}=1$, and
    $\mathbf{1}_{(\cdot)}$ is the indicator function. This distribution appears for
    instance in classification problems when using neural networks, since most neural
    networks for classification use a softmax function after their last layer to produce
    a Categorical distribution of classes s.t. $\pi_{k}\equiv P(y=k|x)$. In this setting,
    the Dirichlet distribution arises as a suitable prior and multivariate generalization
    of the Beta distribution (and is thus also called the *multivariate Beta distribution*):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Dir}(\bm{\pi};\bm{\alpha})=\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1};\quad
    B(\bm{\alpha})=\frac{\prod_{k=1}^{K}\Gamma(\alpha_{k})}{\Gamma(\alpha_{0})};\quad\alpha_{0}=\sum_{k=1}^{K}\alpha_{k};\quad\alpha_{k}\in\mathbb{R}^{+};$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'where the Beta function $B(\cdot)$ is now defined for $K$ shape parameters
    compared to [Equation 7](#S3.E7 "In 3.1 The Dirichlet distribution ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"). For notational convenience,
    we also define $\mathbb{K}=\{1,\ldots,K\}$ as the set of all classes. The distribution
    is characterized by its *concentration parameters* $\bm{\alpha}$, the sum of which,
    often denoted as $\alpha_{0}$, is called the *precision*.⁵⁵5The precision is analogous
    to the precision of a Gaussian, where a larger $\alpha_{0}$ signifies a sharper
    distribution. The Dirichlet is a *conjugate prior* for such a Categorical likelihood,
    meaning that according to Bayes’ rule in [Equation 1](#S2.E1 "In 2.1 Bayesian
    Inference ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), they produce a Dirichlet posterior
    with parameters $\bm{\beta}$, given a data set $\mathbb{D}=\{(x_{i},y_{i})\}_{i=1}^{N}$
    of $N$ observations with corresponding labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(\bm{\pi}&#124;\mathbb{D},\bm{\alpha})$ | $\displaystyle\propto
    p\big{(}\{y_{i}\}_{i=1}^{N}&#124;\bm{\pi},\{x_{i}\}_{i=1}^{N}\big{)}p(\bm{\pi}&#124;\bm{\alpha})=\prod_{i=1}^{N}\prod_{k=1}^{K}\pi_{k}^{\mathbf{1}_{y_{i}=k}}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\prod_{k=1}^{K}\pi_{k}^{\big{(}\sum_{i=1}^{N}\mathbf{1}_{y_{i}=k}\big{)}}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}=\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{N_{k}+\alpha_{k}-1}\propto\text{Dir}(\bm{\pi};\bm{\beta}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\bm{\beta}$ is a vector with $\beta_{k}=\alpha_{k}+N_{k}$, with $N_{k}$
    denoting the number of observations for class $k$. Intuitively, this implies that
    the prior belief encoded by the initial Dirichlet is updated using the actual
    data, sharpening the distribution for classes for which many instances have been
    observed. Similar to the Beta distribution in [Equation 7](#S3.E7 "In 3.1 The
    Dirichlet distribution ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), the Dirichlet is a *distribution over Categorical distributions*
    on the $K-1$ probability simplex; we show an example with its concentration parameters
    and the Bayesian update in [Figure 3](#S3.F3 "In 3.1 The Dirichlet distribution
    ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Parameterization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a classification problem with $K$ classes, a neural classifier is usually
    realized as a function $f_{\bm{\theta}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{K}$,
    mapping an input $\mathbf{x}\in\mathbb{R}^{D}$ to *logits* for each class. Followed
    by a softmax function, this then defines a Categorical distribution over classes
    with a vector $\bm{\pi}$ with $\pi_{k}\equiv p(y=k|\mathbf{x},\bm{\theta})$. The
    same underlying architecture can be used without any major modification to instead
    parameterize a *Dirichlet* distribution, predicting a distribution *over Categorical
    distributions* $p(\bm{\pi}|\mathbf{x},\hat{\bm{\theta}})$ as in [Equation 9](#S3.E9
    "In 3.1 The Dirichlet distribution ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation").⁶⁶6The only thing to note here is that the every $\alpha_{k}$
    has to be strictly positive, which can for instance be enforced by using an additional
    softplus, exponential or ReLU function (Sensoy et al., [2018](#bib.bib146); Malinin
    & Gales, [2018](#bib.bib115); Sensoy et al., [2020](#bib.bib147)). In order to
    classify a data point $\mathbf{x}$, a Categorical distribution is created from
    the predicted concentration parameters of the Dirichlet as follows (this corresponds
    to the mean of the Dirichlet, see [Section C.1](#A3.SS1 "C.1 Expectation of a
    Dirichlet ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\alpha}=\exp\big{(}f_{\bm{\theta}}(\mathbf{x})\big{)};\quad\pi_{k}=\frac{\alpha_{k}}{\alpha_{0}};\quad\hat{y}=\operatorname*{arg\,max}_{k\in\mathbb{K}}\
    \pi_{1},\ldots,\pi_{K}.$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'Parameterizing a Dirichlet posterior distribution follows a similar logic,
    as we will discuss in [Section 3.4.2](#S3.SS4.SSS2 "3.4.2 Posterior Networks ‣
    3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for
    Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Uncertainty Estimation with Dirichlet Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6cba51fe62adf4c3b92c6822e45e2a47.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Categorical distributions predicted by a neural ensemble on the probability
    simplex.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c6249a879b04f062c463de8c991d9215.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Probability simplex for a confident prediction, for with the density concentrated
    in a single corner.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18146da21dde022402712deca8bbfc7f.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Dirichlet distribution for a case of data uncertainty, with the density
    concentrated in the center.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/167bb1ad810dc8f055bc906dcf672fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Dirichlet distribution for a case of model uncertainty, with the density
    spread out more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e84ff79feac78ba0ed80ca9874740f8.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Dirichlet for a case of distributional uncertainty, with the density spread
    across the whole simplex.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f6eeb1b7a56bd28a07a4622dd6eeff81.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Alternative approach to distributional uncertainty called representation
    gap, with density concentrated along the edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Examples of the probability simplex for a $K=3$ classification problem,
    where every corner corresponds to a class and every point to a Categorical distribution.
    Brighter colors correspond to higher density. (a) Predicted Categorical distributions
    by an ensemble of discriminators. (b) – (e) (Desired) Behavior of Dirichlet in
    different scenarios by Malinin & Gales ([2018](#bib.bib115)): (b) For a confident
    prediction, the density is concentrated in the corner of the simplex corresponding
    to the assumed class. (c) In the case of aleatoric uncertainty, the density is
    concentrated in the center, and thus uniform Categorical distributions are most
    likely. (d) In the case of model uncertainty, the density may still be concentrated
    in a corner, but more spread out, expressing the uncertainty about the right prediction.
    (e) In the case of an OOD input, a uniform Dirichlet expresses that any Categorical
    distribution is equally likely, since there is no evidence for any known class.
    (f) Representation gap by Nandy et al. ([2020](#bib.bib133)), proposed as an alternative
    behavior for OOD data. Here, the density is instead concentrated solely on the
    edges of the simplex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now turn our attention to how to estimate the aleatoric, epistemic and
    distributional uncertainty as laid out in [Section 2.2](#S2.SS2 "2.2 Predictive
    Uncertainty in Neural Networks ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation") within
    the Dirichlet framework. In [Figure 4](#S3.F4 "In 3.3 Uncertainty Estimation with
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), we show different shapes of a Dirichlet distribution parameterized
    by a neural network, corresponding to different cases of uncertainty, where each
    point on the simplex represents a Categorical distribution, with proximity to
    a corner indicating a high probability for the corresponding class. [Figure 4(a)](#S3.F4.sf1
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") displays the predictions of
    an ensemble of classifiers as a point cloud on the simplex. Using a Dirichlet,
    this finite set of distributions can be extended to a continuous density over
    the whole simplex. As we will see in the following sections, parameterizing a
    Dirichlet distribution with a neural network enables us to distinguish different
    scenarios using the shape of its density, as shown in [Figures 4(b)](#S3.F4.sf2
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), [4(c)](#S3.F4.sf3 "Figure
    4(c) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), [4(d)](#S3.F4.sf4 "Figure
    4(d) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), [4(e)](#S3.F4.sf5 "Figure
    4(e) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") and [4(f)](#S3.F4.sf6 "Figure
    4(f) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), which we will discuss in more
    detail along the way.'
  prefs: []
  type: TYPE_NORMAL
- en: However, since we do not want to inspect Dirichlets visually, we instead use
    closed form expression to quantify uncertainty, which we will discuss now. Although
    stated for the prior parameters $\bm{\alpha}$, the following methods can also
    be applied to the posterior parameters $\bm{\beta}$ without loss of generality.
  prefs: []
  type: TYPE_NORMAL
- en: Data (aleatoric) uncertainty
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To obtain a measure of data uncertainty, we can evaluate the expected entropy
    of the data distribution $p(y|\bm{\pi})$ (similar to previous works like e.g.
    [Gal & Ghahramani](#bib.bib45), [2016](#bib.bib45)). As the entropy captures the
    “peakiness” of the output distribution, a lower entropy indicates that the model
    is concentrating most probability mass on a single class, while high entropy characterizes
    a more uniform distribution—the model is undecided about the right prediction.
    For Dirichlet networks, this quantity has a closed-form solution (for the full
    derivation, refer to [Section D.1](#A4.SS1 "D.1 Derivation of Expected Entropy
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $\psi$ denotes the digamma function, defined as $\psi(x)=\frac{d}{dx}\log\Gamma(x)$,
    and $H$ the Shannon entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Model (epistemic) uncertainty
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As we saw in [Section 2.2](#S2.SS2 "2.2 Predictive Uncertainty in Neural Networks
    ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"), most approaches in the Dirichlet framework
    avoid the intractable integral over network parameters $\bm{\theta}$ by using
    a point estimate $\hat{\bm{\theta}}$.⁷⁷7With exceptions such as Haussmann et al.
    ([2019](#bib.bib58)); Zhao et al. ([2020](#bib.bib189)). When the distribution
    over parameters in [Equation 5](#S2.E5 "In 2.3 Evidential Deep Learning ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") is retained, alternate expressions of the aleatoric and
    epistemic uncertainty are derived by Woo ([2022](#bib.bib180)). This means that
    computing the model uncertainty via the weight posterior $p(\bm{\theta}|\mathbb{D})$
    like in Blundell et al. ([2015](#bib.bib13)); Gal & Ghahramani ([2016](#bib.bib45));
    Smith & Gal ([2018](#bib.bib153)) is not possible. Nevertheless, a key property
    of Dirichlet networks is that epistemic uncertainty is expressed in the spread
    of the Dirichlet distribution (for instance in [Figure 4](#S3.F4 "In 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") (d) and (e)). Therefore, the epistemic uncertainty can
    be quantified considering the concentration parameters $\bm{\alpha}$ that shape
    this distribution: Charpentier et al. ([2020](#bib.bib16)) simply consider the
    maximum $\alpha_{k}$ as a score akin to the maximum probability score by Hendrycks
    & Gimpel ([2017](#bib.bib64)), while Sensoy et al. ([2018](#bib.bib146)) compute
    it by $K/\sum_{k=1}^{K}(\alpha_{k}+1)$ or simply $\alpha_{0}$ (Charpentier et al.,
    [2020](#bib.bib16)). In both cases, the underlying intuition is that larger $\alpha_{k}$
    produce a sharper density, and thus indicate increased confidence in a prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Distributional uncertainty
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Another appealing property of this model family is being able to distinguish
    uncertainty due to model underspecification ([Figure 4(d)](#S3.F4.sf4 "In Figure
    4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")) from uncertainty due to unknown
    inputs ([Figure 4(e)](#S3.F4.sf5 "In Figure 4 ‣ 3.3 Uncertainty Estimation with
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")). In the Dirichlet framework, the distributional uncertainty can
    be quantified by computing the difference between the total amount of uncertainty
    and the data uncertainty, which can be expressed through the mutual information
    between the label $y$ and its Categorical distribution $\bm{\pi}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}=\underbrace{H\bigg{[}\mathbb{E}_{#1}\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}}_{\text{Total
    Uncertainty}}-\underbrace{\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}}_{\text{Data
    Uncertainty}}$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'This quantity expresses how much information we would receive about $\bm{\pi}$
    if we were given the label $y$, conditioned on the new input $\mathbf{x}$ and
    the training data $\mathbb{D}$. In regions in which the model is well-defined,
    receiving $y$ should not provide much new information about $\bm{\pi}$—and thus
    the mutual information would be low. Yet, such knowledge should be very informative
    in regions in which few data have been observed, and there this mutual information
    would indicate higher distributional uncertainty. Given that $\mathbb{E}_{#1}[\pi_{k}]=\frac{\alpha_{k}}{\alpha_{0}}$
    ([Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")) and assuming the point estimate
    $p(\bm{\pi}|\mathbf{x},\mathbb{D})\approx p(\bm{\pi}|\mathbf{x},\hat{\bm{\theta}})$
    to be sufficient (Malinin & Gales, [2018](#bib.bib115)), we obtain an expression
    very similar to [Equation 12](#S3.E12 "In Data (aleatoric) uncertainty ‣ 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\log\frac{\alpha_{k}}{\alpha_{0}}-\psi(\alpha_{k}+1)+\psi(\alpha_{0}+1)\bigg{)}$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: Note on epistemic uncertainty estimation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The introduction of distributional uncertainty, a notion that is non-existent
    in the Bayesian Model Averaging framework, warrants a note on the estimation of
    epistemic uncertainty in general. Firstly, since we often use the point estimate
    $p(\bm{\theta}|\mathbb{D})\approx\delta(\bm{\theta}-\hat{\bm{\theta}})$ from [Equation 5](#S2.E5
    "In 2.3 Evidential Deep Learning ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation") in Evidential
    Deep Learning, model uncertainty usually is no longer estimated via the uncertainty
    in the weight posterior, but instead through the parameters of the prior or posterior
    distribution. Furthermore, even though they appear similar, distributional uncertainty
    is different from epistemic uncertainty, since it is the uncertainty in the distribution
    $p(\bm{\pi}|\mathbf{x},\bm{\theta})$. Distinguishing epistemic from distributional
    uncertainty also allows us to differentiate uncertainty due to underspecification
    from uncertainty due to a lack of evidence. In BMA, these notions are indistinguishable:
    In theory, model uncertainty on OOD data should be high since the model is underspecified
    on them, however theoretical and empirical work has shown this is not always the
    case (Ulmer et al., [2020](#bib.bib162); Ulmer & Cinà, [2021](#bib.bib161); Van Landeghem
    et al., [2022](#bib.bib168)). Even then, the additive decomposition of the mutual
    information has been critized since the model will also have a great deal of *uncertainty
    about its aleatoric uncertainty* in the beginning of the training process (Hüllermeier,
    [2022](#bib.bib74)), and thus this decomposition might not be accurate. Furthermore,
    even when we obtain the best possible model within its hypothesis class, using
    the discussed methods it is impossible to estimate uncertainty induced by a misspecified
    hypothesis class. This can motivate approaches in which a second, auxiliary model
    directly predicts model uncertainty of a target model (Lahlou et al., [2022](#bib.bib97);
    Zerva et al., [2022](#bib.bib187)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Existing Approaches for Dirichlet Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Being able to quantify aleatoric, epistemic and distributional uncertainty
    in a single forward pass and in closed form are desirable traits, as they simplify
    the process of obtaining different uncertainty scores. However, it is important
    to note that the behavior of the Dirichlet distributions in [Figure 4](#S3.F4
    "In 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") is idealized. In the usual way of
    training neural networks through empirical risk minimization, Dirichlet networks
    are not incentivized to behave in the depicted way. Thus, when comparing existing
    approaches for parameterizing Dirichlet priors in [Section 3.4.1](#S3.SS4.SSS1
    "3.4.1 Prior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") and posteriors in [Section 3.4.2](#S3.SS4.SSS2
    "3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3
    Evidential Deep Learning for Classification ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"),⁸⁸8Even
    though the term *prior* and *posterior network* were coined by Malinin & Gales
    ([2018](#bib.bib115)) and Charpentier et al. ([2020](#bib.bib16)) for their respective
    approaches, we use them in the following as an umbrella term for all methods targeting
    a prior or posterior distribution. we mainly focus on the different ways in which
    authors try to tackle this problem by means of loss functions and training procedures.
    We give an overview over the discussed works in [Tables 1](#S3.T1 "In 3.4.1 Prior
    Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep
    Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") and [2](#S3.T2 "Table 2 ‣ 3.4.2
    Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") in these respective sections.
    For additional details, we refer the reader to [Appendix C](#A3 "Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") for general derivations concerning
    the Dirichlet distribution. We dedicate [Appendix D](#A4 "Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") to derivations of the different
    loss functions and regularizers and give a detailed overview over their mathematical
    forms in [Appendix E](#A5 "Appendix E Overview over Loss Functions Appendix ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"). Available code repositories for all works surveyed are
    listed in [Section A.2](#A1.SS2 "A.2 Code Availability ‣ Appendix A Code Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Prior Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 1: Overview over prior networks for classification. $(*)$ OOD samples
    were created inspired by the approach of Liang et al. ([2018](#bib.bib105)). ID:
    Using in-distribution data samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Loss function | Architecture | OOD-free training? |'
  prefs: []
  type: TYPE_TB
- en: '| Prior network (Malinin & Gales, [2018](#bib.bib115)) | ID KL w.r.t smoothed
    label & OOD KL w.r.t. uniform prior | MLP / CNN | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks (Malinin & Gales, [2019](#bib.bib116)) | Reverse KL of Malinin
    & Gales ([2018](#bib.bib115)) | CNN | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Information Robust Dirichlet Networks (Tsiligkaridis, [2019](#bib.bib159))
    | $l_{p}$ norm w.r.t one-hot label & Approx. Rényi divergence w.r.t. uniform prior
    | CNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Dirichlet via Function Decomposition (Biloš et al., [2019](#bib.bib11)) |
    Uncertainty Cross-entropy & mean & variance regularizer | RNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Prior network with PAC Regularization (Haussmann et al., [2019](#bib.bib58))
    | Negative log-likelihood loss + PAC regularizer | BNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Ensemble Distribution Distillation (Malinin et al., [2020b](#bib.bib118))
    | Knowledge distillation objective | MLP / CNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Distribution Distillation (Fathullah & Gales, [2022](#bib.bib39)) |
    Knowledge distillation objective | CNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks with representation gap (Nandy et al., [2020](#bib.bib133))
    | ID & OOD Cross-entropy + precision regularizer | MLP / CNN | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Prior RNN (Shen et al., [2020](#bib.bib151)) | Cross-entropy + entropy regularizer
    | RNN | (✗)^∗ |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-based Kernel Dirichlet distribution estimation (GKDE) (Zhao et al.,
    [2020](#bib.bib189)) | $l_{2}$ norm w.r.t. one-hot label & KL reg. with node-level
    distance prior & Knowledge distillation objective | GNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'The key challenge in training Dirichlet networks is to ensure both high classification
    performance and the intended behavior under OOD inputs. For this reason, most
    discussed works follow a loss function design using two parts: One optimizing
    for task accuracy to achieve the former goal, the other optimizing for a flat
    Dirichlet distribution, as flatness suggests a lack of evidence. To enforce flatness,
    the predicted Dirichlet is compared to a uniform distribution using some probabilistic
    divergence measure. We divide prior networks into two groups: Approaches using
    additional OOD data for this purpose (*OOD-dependent approaches*), and those which
    do not required OOD data (*OOD-free approaches*), as listed in [Table 1](#S3.T1
    "In 3.4.1 Prior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3
    Evidential Deep Learning for Classification ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: OOD-free approaches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Apart from a standard negative log-likelihood loss (NLL) as used by Haussmann
    et al. ([2019](#bib.bib58)), one simple approach to optimizing the model is to
    impose a $l_{p}$-loss between the one-hot encoding $\mathbf{y}$ of the original
    label $y$ and the Categorical distribution $\bm{\pi}$. Tsiligkaridis ([2019](#bib.bib159))
    show that since the values of $\bm{\pi}$ depend directly on the predicted concentration
    parameters $\bm{\alpha}$, a generalized loss can be derived to be upper-bounded
    by the following expression (see the full derivation given in [Section D.3](#A4.SS3
    "D.3 𝑙_∞ Norm Derivation ‣ Appendix D Additional Derivations Appendix ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}\big{]}\leq\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\bBigg@{4}(\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})^{\frac{1}{p}}$ |  |
    (15) |'
  prefs: []
  type: TYPE_TB
- en: 'Since the sum over concentration parameters excludes the one corresponding
    to the true label, this loss can be seen as reducing the density on the areas
    of the probability simplex that do not correspond to the target class. Sensoy
    et al. ([2018](#bib.bib146)) specifically utilize the $l_{2}$ loss, which has
    the following form (see [Section D.4](#A4.SS4 "D.4 𝑙₂ Norm Loss Derivation ‣ Appendix
    D Additional Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{2}^{2}\Big{]}=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{1}_{(\cdot)}$ denotes the indicator function. Since $\alpha_{k}/\alpha_{0}\leq
    1$, we can see that the term with the indicator functions penalizes the network
    when the concentration parameter $\alpha_{k}$ corresponding to the correct label
    does not exceed the others. The remaining aspect lies in the regularization: To
    achieve reliable predictive uncertainty, the density associated with incorrect
    classes should be reduced. One such option is to decrease the Kullback-Leibler
    divergence from a uniform Dirichlet (see [Section C.3](#A3.SS3 "C.3 Kullback-Leibler
    Divergence between two Dirichlets ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{1})\Big{]}=\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'In the case of Zhao et al. ([2020](#bib.bib189)), who apply their model to
    graph structures, they do not decrease the divergence from a uniform Dirichlet,
    but incorporate information about the local graph neighborhood into the reference
    distribution by considering the distance from and label of close nodes.⁹⁹9They
    also add another knowledge distillation term (Hinton et al., [2015](#bib.bib66))
    to their loss, for which the model tries to imitate the predictions of a vanilla
    Graph Neural Network that functions as the teacher network. Nevertheless, the
    KL-divergence w.r.t. a uniform Dirichlet is used by many of the following works.
    Other divergence measures are also possible: Tsiligkaridis ([2019](#bib.bib159))
    instead use a local approximation of the Rényi divergence.^(10)^(10)10The Kullback-Leibler
    divergence can be seen as a special case of the Rényi divergence (van Erven &
    Harremoës, [2014](#bib.bib167)), where the latter has a stronger information-theoretic
    underpinning. First, the concentration parameter for the correct class $\alpha_{y}$
    is removed from the Dirichlet by creating $\tilde{\bm{\alpha}}=(1-\mathbf{y})\cdot\bm{\alpha}+\mathbf{y}$.
    Then, the remaining concentration parameters are pushed towards uniformity by
    the divergence measure, which can be derived to be'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{R\''{e}nyi}\Big{[}p(\bm{\pi}&#124;\tilde{\bm{\alpha}})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{1})\Big{]}\approx\frac{1}{2}\Big{[}\sum_{k\neq
    y}\big{(}\alpha_{k}-1\big{)}^{2}\big{(}\psi^{(1)}(\alpha_{j})-\psi^{(1)}(\tilde{\alpha}_{0})\big{)}-\psi^{(1)}(\tilde{\alpha}_{0})\sum_{\begin{subarray}{c}k\neq
    k^{\prime}\\ k\neq y,\ k^{\prime}\neq y\end{subarray}}\big{(}\alpha_{k}-1\big{)}\big{(}\alpha_{k^{\prime}}-1\big{)}\Big{]}$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\psi^{(1)}$ denotes the first-order polygamma function, defined as $\psi^{(1)}(x)=\frac{d}{dx}\psi(x)$.
    Since the sums ignore the concentration parameter of the correct class, only the
    ones of the incorrect classes are penalized. Haussmann et al. ([2019](#bib.bib58))
    derive an entirely different regularizer using Probably Approximately Correct
    (PAC) bounds from learning theory, that together with the negative log-likelihood
    gives a proven bound to the expected true risk of the classifier. Setting a scalar
    $\delta$ allows one to set the desired risk, i.e. the model’s expected risk is
    guaranteed to be the same or less than the derived PAC bound with a probability
    of $1-\delta$. For a problem with $N$ available training data points, the following
    *upper bound* is presented:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sqrt{\frac{\text{KL}\big{[}p(\bm{\pi}&#124;\bm{\alpha})\big{&#124;}\big{&#124;}p(\bm{\pi}&#124;\mathbf{1})\big{]}-\log\delta}{N}-1}.$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: This upper bound is then used as the actual regularizer term in practice. We
    see that even from the learning-theoretic perspective, this method follows the
    intuition of the original KL regularizer in a shifted and scaled form. Haussmann
    et al. ([2019](#bib.bib58)) also admit that in this form, the regularizer does
    not allow for a direct PAC interpretation anymore, since its approximates only
    admits a loose bound on the risk. Yet, they demonstrate its usefulness in their
    experiments. Summarizing all of the presented approaches thus far, we can see
    that they try to force the model to concentrate the Dirichlet’s density solely
    on the parameter corresponding to the right label—expecting a more flat density
    for difficult or unknown inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A way to avoid the use of OOD examples while still using external information
    for regularization is to use *knowledge distillation* (Hinton et al., [2015](#bib.bib66)).
    Here, the core idea lies in a student model learning to imitate the predictions
    of a more complex teacher model. Malinin et al. ([2020b](#bib.bib118)) exploit
    this idea and show that prior networks can also be distilled using an ensemble
    of classifiers and their predicted Categorical distributions (akin to learning
    [Figure 4(e)](#S3.F4.sf5 "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    from [Figure 4(a)](#S3.F4.sf1 "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")),
    which does not require regularization at all, but comes at the cost of having
    to train an entire ensemble a priori. Trying to solve this shortcoming, Fathullah
    & Gales ([2022](#bib.bib39)) propose to use a shared feature extractor between
    the student and the teacher network. Instead of training an ensemble, diverse
    predictions are obtained from the teacher network through the use of Gaussian
    dropout, which are distilled into a Dirichlet distribution as in Malinin et al.
    ([2020b](#bib.bib118)).'
  prefs: []
  type: TYPE_NORMAL
- en: OOD-dependent approaches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A uniform Dirichlet in the face of unknown inputs can also be achieved explicitly
    by training with OOD inputs and learning to be uncertain on them. We discuss a
    series of works utilizing this direction next. Malinin & Gales ([2018](#bib.bib115))
    simply minimize the KL divergence to a uniform Dirichlet on OOD data points. This
    way, the model is encouraged to be agnostic about its prediction in the face of
    unknown inputs. Further, instead of an $l_{p}$ norm, they utilize another KL term
    to train the model on predicting the correct label, minimizing the distance between
    the predicted concentration parameters and the true label. However, since only
    a gold *label* and not a gold *distribution* is available, they create one by
    re-distributing some of the density from the correct class onto the rest of the
    simplex (see [Appendix E](#A5 "Appendix E Overview over Loss Functions Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") for full form). In their follow-up work, Malinin & Gales
    ([2019](#bib.bib116)) argue that the asymmetry of the KL divergence as the main
    objective creates undesirable properties in producing the correct behavior of
    the predicted Dirichlet, since it creates a multi- instead of unimodal target
    distribution. They therefore propose to use the reverse KL instead (see [Section D.5](#A4.SS5
    "D.5 Derivation of Reverse KL loss ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") for the derivation), which enforces the desired unimodal
    target. Nandy et al. ([2020](#bib.bib133)) refine this idea further, stating that
    even with reverse KL training high epistemic and high distributional uncertainty
    ([Figures 4(d)](#S3.F4.sf4 "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    and [4(e)](#S3.F4.sf5 "Figure 4(e) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")) might be confused, and instead propose novel loss functions producing
    a *representation gap* ([Figure 4(f)](#S3.F4.sf6 "In Figure 4 ‣ 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")), which aims to be more easily distinguishable. In this
    case, spread out densities signify epistemic uncertainty, whereas densities concentrated
    entirely on the edges of the simplex indicate distributional uncertainty. The
    way they achieve this goal is two-fold: In addition to minimizing the negative
    log-likelihood loss on in-domain and maximizing the entropy on OOD examples, they
    also penalize the precision of the Dirichlet (see [Appendix E](#A5 "Appendix E
    Overview over Loss Functions Appendix ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation") for full form).
    Maximizing the entropy on OOD examples hereby serves the same function as minimizing
    the KL w.r.t. to a uniform distribution, and can be implemented using the closed-form
    solution in [Section C.2](#A3.SS2 "C.2 Entropy of Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $H\big{[}p(\bm{\pi}&#124;\bm{\alpha})\big{]}=\log B(\bm{\alpha})+(\alpha_{0}-K)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: Sequential models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We also have identified two sequential applications of prior networks in the
    literature: For Natural Language Processing, Shen et al. ([2020](#bib.bib151))
    train a recurrent neural network for spoken language understanding using a simple
    cross-entropy loss. Instead of using OOD examples for training, they maximize
    the entropy of the model on data inputs given a learned, noisy version of the
    predicted concentration parameters. In comparison, Biloš et al. ([2019](#bib.bib11))
    apply their model to asynchronous event classification and note that the standard
    cross-entropy loss only involves a point estimate of a Categorical distribution,
    discarding all the information contained in the predicted Dirichlet. For this
    reason, they propose an *uncertainty-aware* cross-entropy (UCE) loss instead,
    which has a closed-form solution in the Dirichlet case (see [Section D.6](#A4.SS6
    "D.6 Uncertainty-aware Cross-Entropy Loss ‣ Appendix D Additional Derivations
    Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"))'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{UCE}}=\psi(\alpha_{y})-\psi(\alpha_{0}),$ |  | (21)
    |'
  prefs: []
  type: TYPE_TB
- en: with $\psi$ referring to the digamma function. By mimizing the difference between
    the digamma values of $\alpha_{y}$ and $\alpha_{0}$, the model learns to concentrate
    density on the correct class. Since their final concentration parameters are created
    using additional information from a class-specific Gaussian process, they further
    regularize the mean and variance for OOD data points using an extra loss term,
    incentivizing a loss mean and a variance corresponding to a pre-defined hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Posterior Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table 2: Overview over posterior networks for classification. OOD data is created
    using $(\dagger)$ the fast-sign gradient method (Kurakin et al., [2017](#bib.bib96)),
    a $(\ddagger)$ Variational Auto-Encoder (VAE; [Kingma & Welling](#bib.bib88),
    [2014](#bib.bib88)) or $(\mathsection)$ a Wasserstein GAN (WGAN; [Arjovsky et al.](#bib.bib4),
    [2017](#bib.bib4)). NLL: Negative log-likelihood. CE: Cross-entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Loss function | Architecture | OOD-free training? |'
  prefs: []
  type: TYPE_TB
- en: '| Evidential Deep Learning (Sensoy et al., [2018](#bib.bib146)) | $l_{2}$ norm
    w.r.t. one-hot label + KL w.r.t. uniform prior | CNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Regularized ENN (Zhao et al., [2019](#bib.bib188)) | $l_{2}$ norm w.r.t.
    one-hot label + Uncertainty regularizer on OOD/ difficult samples | MLP / CNN
    | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | $l_{2}$ norm w.r.t. one-hot label
    + Uncertainty regularizer on synth. OOD | MLP / CNN + WGAN | (✗)^§ |'
  prefs: []
  type: TYPE_TB
- en: '| Variational Dirichlet (Chen et al., [2018](#bib.bib18)) | ELBO + Contrastive
    Adversarial Loss | CNN | (✗)^† |'
  prefs: []
  type: TYPE_TB
- en: '| Dirichlet Meta-Model (Shen et al., [2022](#bib.bib150)) | ELBO + KL w.r.t.
    uniform prior | CNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Belief Matching (Joo et al., [2020](#bib.bib82)) | ELBO | CNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Posterior Networks (Charpentier et al., [2020](#bib.bib16)) | Uncertainty
    CE (Biloš et al., [2019](#bib.bib11)) + Entropy regularizer | MLP / CNN + Norm.
    Flow | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Graph Posterior Networks (Stadler et al., [2021](#bib.bib155)) | Same as
    Charpentier et al. ([2020](#bib.bib16)) | GNN | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Evidential Neural Networks (Sensoy et al., [2020](#bib.bib147))
    | Contrastive NLL + KL between uniform & Dirichlet of wrong classes | CNN | (✗)^‡
    |'
  prefs: []
  type: TYPE_TB
- en: 'As elaborated on in [Section 3.1](#S3.SS1 "3.1 The Dirichlet distribution ‣
    3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), choosing
    a Dirichlet prior, due to its conjugacy to the Categorical distribution, induces
    a Dirichlet posterior distribution. Like the prior before, surveyed works listed
    in [Table 2](#S3.T2 "In 3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation") parameterize the posterior with a neural network. The challenges
    hereby are two-fold: Accounting for the number of class observations $N_{k}$ that
    make up part of the posterior density parameters $\bm{\beta}$ ([Equation 10](#S3.E10
    "In 3.1 The Dirichlet distribution ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")), and, similarly to prior networks, ensuring the wanted
    behavior on the probability simplex for in- and out-of-distribution inputs. Sensoy
    et al. ([2018](#bib.bib146)) base their approach on the Dempster-Shafer theory
    of evidence (Yager & Liu, [2008](#bib.bib183); lending its name to the term “Evidential
    Deep Learning”) and its formalization via subjective logic (Audun, [2018](#bib.bib5)),
    where subjective beliefs about probabilities are expressed through Dirichlet distributions.
    In doing so, an agnostic belief in form of a uniform Dirichlet prior $\forall
    k:\alpha_{k}=1$ is updated using pseudo-counts $N_{k}$, which are predicted by
    a neural network. This is different from prior networks, where the prior concentration
    parameters $\bm{\alpha}$ are predicted instead. In both cases, this does not require
    any modification to a model’s architecture except for replacing the softmax output
    function by a ReLU (or similar). Sensoy et al. ([2018](#bib.bib146)) for instance
    train their model using the same techniques presented in the previous section:
    The main objective is the $l_{2}$ loss, penalizing the difference between the
    predicted Dirichlet and the one-hot encoded class label ([Section D.4](#A4.SS4
    "D.4 𝑙₂ Norm Loss Derivation ‣ Appendix D Additional Derivations Appendix ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")), and the KL divergence from a uniform Dirichlet is used for regularization.'
  prefs: []
  type: TYPE_NORMAL
- en: Generating OOD samples using generative models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since OOD examples are not always readily available, several works try to create
    artificial samples using deep generative models. Hu et al. ([2021](#bib.bib71))
    train a Wasserstein GAN (Arjovsky et al., [2017](#bib.bib4)) to generate OOD samples,
    on which the network’s uncertainty is maximized. The uncertainty is given through
    *vacuity*, defined as $K/\sum_{k}\beta_{k}$. The vacuity compares a uniform prior
    belief against the amassed evidence $\sum_{k}\beta_{k}$, and thus is $1$ when
    there is no additonal evidence available. In a follow-up work, Sensoy et al. ([2020](#bib.bib147))
    similarly train a model using a contrastive loss with artificial OOD samples from
    a Variational Autoencoder (Kingma & Welling, [2014](#bib.bib88)), and a KL-based
    regularizer similar to that of Tsiligkaridis ([2019](#bib.bib159)), where the
    density for posterior concentration parameters $\beta_{k}$ that do not correspond
    to the true label are pushed to the uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aa47c0fbc42381a0abf6ccf494a3c00c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Posterior Network (Charpentier et al., [2020](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d6a86af5774681a556a67f9520ad1417.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Natural Posterior Network (Charpentier et al., [2022](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Schematic of the Posterior Network and Natural Posterior Network,
    taken from Charpentier et al. ([2020](#bib.bib16); [2022](#bib.bib17)), respectively.
    In both cases, an encoder $f_{\bm{\theta}}$ maps inputs to a latent representation
    $\mathbf{z}$. NFs then model the latent densities, which are used together with
    the prior concentration to produce the posterior parameters. In (a), the latent
    representation of $\mathbf{x}^{(1)}$ lies right in the modelled density of the
    first class, and thus receives a confident prediction. The latent $\mathbf{z}^{(2)}$
    lies between densities, creating aleatoric uncertainty. $\mathbf{x}^{(3)}$ is
    an OOD input, is mapped to a low-density area of the latent space and thus produces
    an uncertain prediction. The differences in the two approaches is that the Posterior
    Network in (a) uses one NF per class, while only one NF is used in (b). Furthermore,
    (b) constitutes a generalization to different exponential family distributions,
    and is not restricted to classification problems (see main text for more detail).'
  prefs: []
  type: TYPE_NORMAL
- en: Posterior networks via Normalizing Flows
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Charpentier et al. ([2020](#bib.bib16)) also set $\bm{\alpha}$ to a uniform
    prior, but obtain the pseudo-observations $N_{k}$ in a different way: Instead
    of a model predicting them directly, $N_{k}$ is determined by the number of examples
    of a certain class in the training set. This quantity is further modified in the
    following way: An encoder model $f_{\bm{\theta}}$ produces a latent representation
    $\mathbf{z}$ of some input. A (class-specific) normalizing flow^(11)^(11)11A NF
    is a generative model, estimating a density in the feature space by mapping it
    to a Gaussian in a latent space by a series of invertible, bijective transformations.
    The probability of an input can then be estimated by calculating the probability
    of its latent encoding under that Gaussian and applying the change-of-variable
    formula, traversing the flow in reverse. Instead of mapping from the feature space
    into latent space, the flows in Charpentier et al. ([2020](#bib.bib16)) map from
    the encoder latent space into a separate, second latent space. (NF; [Rezende &
    Mohamed](#bib.bib145), [2015](#bib.bib145)) with parameters $\bm{\phi}$ then assigns
    a probability to this latent representation, which is used to weight $N_{k}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\beta_{k}=\alpha_{k}+N_{k}\cdot p(\mathbf{z}&#124;y=k,\bm{\phi});\quad\mathbf{z}=f_{\bm{\theta}}(\mathbf{x}).$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'This has the advantage of producing low probabilities for strange inputs like
    the noise as depicted in [Figure 5(a)](#S3.F5.sf1 "In Figure 5 ‣ Generating OOD
    samples using generative models ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing Approaches
    for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), which in turn translate to low concentration parameters of the posterior
    Dirichlet, as it falls back onto the uniform prior. The model is optimized using
    the same uncertainty-aware cross-entropy loss as in Biloš et al. ([2019](#bib.bib11))
    with an additional entropy regularizer, encouraging density only around the correct
    class. This scheme is also applied to Graph Neural Networks by Stadler et al.
    ([2021](#bib.bib155)): In order to take the neighborhood structure of the graph
    into account, the authors also use a Personalized Page Rank scheme to diffuse
    node-specific posterior parameters $\bm{\beta}$ between neighboring nodes. The
    Page Rank scores, reflecting the importance of a neighboring node to the current
    node, can be approximated using power iteration (Klicpera et al., [2019](#bib.bib89))
    and used to aggregate the originally predicted concentration parameters $\bm{\beta}$
    on a per-node basis.'
  prefs: []
  type: TYPE_NORMAL
- en: A generalization of the posterior network method to exponential family distributions
    is given by Charpentier et al. ([2022](#bib.bib17)). Akin to the update for the
    posterior Dirichlet parameters, the authors formulate a general Bayesian update
    rule as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\chi}_{i}^{\text{post}}=\frac{n^{\text{prior}}\bm{\chi}^{\text{prior}}+n_{i}\bm{\chi}_{i}}{n^{\text{prior}}+n_{i}};\quad\mathbf{z}_{i}=f_{\bm{\theta}}(\mathbf{x}_{i});\quad
    n_{i}=N\cdot p(\mathbf{z}&#124;\bm{\phi});\quad\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{x}_{i}).$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: '$\bm{\chi}$ here denotes the parameters of the exponential family distribution
    and $n$ the evidence. Thus, posterior parameters for a sample $\mathbf{x}_{i}$
    are obtained by updating the prior parameter and some prior evidence by some input-dependent
    pseudo-evidence $n_{i}$ and parameters $\bm{\chi}_{i}$: Again, given a latent
    representation by an encoder $\mathbf{z}$, a (this time single) normalizing flow
    predicts $n_{i}=N_{H}\cdot p(\mathbf{z}|\bm{\phi})$ based on some pre-defined
    certainty budget $N_{H}$.^(12)^(12)12The certainty budget can simply be set to
    the number of available datapoints, however Charpentier et al. ([2022](#bib.bib17))
    suggest to set it to $\log N_{H}=\frac{1}{2}\big{(}H\log(2\pi)+\log(H+1)\big{)}$
    to better scale with the dimensionality $H$ of the latent space. The update parameters
    $\bm{\chi}_{i}$ are predicted by an additional network $\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{z})$,
    see [Figure 5(b)](#S3.F5.sf2 "In Figure 5 ‣ Generating OOD samples using generative
    models ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks
    ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). For
    classification, $n^{\text{prior}}=1$ and $\bm{\chi}^{\text{prior}}$ corresponds
    to the uniform Dirichlet, while $\bm{\chi}_{i}$ are concentration parameters predicted
    by an output layer based on the latent encoding. For unfamiliar inputs, this method
    will again result in a small pseudo-evidence term $n_{i}$, reflecting high model
    uncertainty. Since the generalization to the exponential family implies the application
    of this scheme to normal distributions, we will discuss the same method applied
    to regression in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Posterior networks via variational inference
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Another route lies in directly parameterizing the posterior parameters $\bm{\beta}$.
    Given a target distribution defined by a uniform Dirichlet prior plus the number
    of times an input is associated with a specific label, Chen et al. ([2018](#bib.bib18))
    optimize a distribution matching objective, i.e. the KL-divergence between the
    posterior parameters predicted by a neural network and the target distribution.
    Since this objective is intractable to optimize directly, this leaves us to instead
    model an *approximate posterior* using variational inference methods. As the KL
    divergence between the true and approximate posterior is infeasible to estimate,
    variational methods usually optimize the *evidence lower bound* (ELBO):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{ELBO}}=\underbrace{\psi(\beta_{y})-\psi(\beta_{0})}_{\text{UCE
    loss}}-\underbrace{\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}}_{\text{KL-divergence}}$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: 'in which we can identify to consist of the uncertainty-aware cross-entropy
    (UCE) loss used by Biloš et al. ([2019](#bib.bib11)); Charpentier et al. ([2020](#bib.bib16);
    [2022](#bib.bib17)) and the KL-divergence between two Dirichlets ([Section C.3](#A3.SS3
    "C.3 Kullback-Leibler Divergence between two Dirichlets ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")). This approach is also employed
    by Joo et al. ([2020](#bib.bib82)), Chen et al. ([2018](#bib.bib18)) and Shen
    et al. ([2022](#bib.bib150)), while the latter predict posterior parameters based
    on the activations of different layers of a pre-trained feature extractor.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evidential Deep Learning for Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 3: Overview over Evidential Deep Learning methods for regression.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Parameterized distribution | Loss function | Model |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Evidential Regression (Amini et al., [2020](#bib.bib1)) | Normal-Inverse
    Gamma Prior | Negative log-likelihood loss + KL w.r.t. uniform prior | MLP / CNN
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Evidential Regression with Multi-task Learning (Oh & Shin, [2022](#bib.bib136))
    | Normal-Inverse Gamma Prior | Like Amini et al. ([2020](#bib.bib1)), with additional
    Lipschitz-modified MSE loss | MLP / CNN |'
  prefs: []
  type: TYPE_TB
- en: '| Multivariate Deep Evidential Regression (Meinert & Lavin, [2021](#bib.bib123))
    | Normal-Inverse Wishart Prior | Like Amini et al. ([2020](#bib.bib1)), but tying
    two predicted params. instead of using a regularizer | MLP |'
  prefs: []
  type: TYPE_TB
- en: '| Regression Prior Network (Malinin et al., [2020a](#bib.bib117)) | Normal-Wishart
    Prior | Reverse KL (Malinin & Gales, [2019](#bib.bib116)) | MLP / CNN |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Posterior Network (Charpentier et al., [2022](#bib.bib17)) | Inverse-$\chi^{2}$
    Posterior | Uncertainty Cross-entropy (Biloš et al., [2019](#bib.bib11)) + Entropy
    regularizer | MLP / CNN + Norm. Flow |'
  prefs: []
  type: TYPE_TB
- en: 'Because the EDL framework provides convenient uncertainty estimation, the question
    naturally arises of whether it can be extended to regression problems as well.
    The answer is affirmative, although the Dirichlet distribution is not an appropriate
    choice in this case. It is very common to model a regression problem using a normal
    likelihood ([Bishop](#bib.bib12), [2006](#bib.bib12); Chapter 3.3). As such, there
    are multiple potential choices for a prior distribution. The methods listed in
    [Table 3](#S4.T3 "In 4 Evidential Deep Learning for Regression ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    either choose the Normal-Inverse Gamma distribution (Amini et al., [2020](#bib.bib1);
    Charpentier et al., [2022](#bib.bib17)), inducing a scaled inverse-$\chi^{2}$
    posterior (Gelman et al., [1995](#bib.bib49)),^(13)^(13)13The form of the Normal-Inverse
    Gamma posterior and the Normal Inverse-$\chi^{2}$ posterior are interchangable
    using some parameter substitutions (Murphy, [2007](#bib.bib129)). or a Normal-Wishart
    prior (Malinin et al., [2020a](#bib.bib117)). We will discuss these approaches
    in turn.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f3c218cd63a240aa5448cf25e820961.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Example of an application of Evidential Deep Learning for regression,
    taken from Amini et al. ([2020](#bib.bib1)). The neural network predicts an Normal
    Inverse-Gamma prior, whose corresponding normal likelihoods display decreasing
    variance (and thus uncertainty) in the face of stronger evidence.'
  prefs: []
  type: TYPE_NORMAL
- en: Univariate regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Amini et al. ([2020](#bib.bib1)) model the regression problem as a normal distribution
    with unknown mean and variance $\mathcal{N}(y;\pi,\sigma^{2})$, and use a normal
    prior for the mean with $\pi\sim\mathcal{N}(\gamma,\sigma^{2}\nu^{-1})$ and an
    inverse Gamma prior for the variance with $\sigma^{2}\sim\Gamma^{-1}(\alpha,\beta)$,
    resulting in a combined Inverse-Gamma prior with parameters $\gamma,\nu,\alpha,\beta$,
    shown in [Figure 6](#S4.F6 "In 4 Evidential Deep Learning for Regression ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"). These are predicted by different “heads” of a neural network. For
    predictions, the expectation of the mean corresponds to $\mathbb{E}_{#1}[\pi]=\gamma$,
    and aleatoric and epistemic uncertainty can then be estimated using the expected
    value of the variance as well as the variance of the mean, respectively, which
    have closed form solutions under this parameterization:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{#1}[\sigma^{2}]=\frac{\beta}{\alpha-1};\quad\text{Var}[\pi]=\frac{\beta}{\nu(\alpha-1)}$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'By choosing to optimize using a negative log-likelihood objective, we can actually
    evaluate the loss function analytically, since the likelihood function corresponds
    to a Student’s t-distribution with $\gamma$ degrees of freedom, mean $\beta(1+\nu)/(\nu\alpha)$
    and $2\alpha$ variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{NLL}}=\frac{1}{2}\log\Big{(}\frac{\pi}{\nu}\Big{)}-\alpha\log\Omega+\Big{(}\alpha+\frac{1}{2}\Big{)}\log\Big{(}(y_{i}-\gamma)^{2}\nu+\Omega\Big{)}+\log\bigg{(}\frac{\Gamma(\alpha)}{\Gamma(\alpha+\frac{1}{2})}\bigg{)}$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'using $\Omega=2\beta(1+\nu)$. Akin to the entropy regularizer for Dirichlet
    networks, Amini et al. ([2020](#bib.bib1)) propose a regularization term that
    only allows for concentrating density on the correct prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{reg}}=&#124;y_{i}-\gamma&#124;\cdot(2\nu+\alpha)$
    |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: 'Since $\mathbb{E}_{#1}[\pi]=\gamma$ is the prediction of the network, the second
    term in the product will be scaled by the degree to which the current prediction
    deviates from the target value. Since $\nu$ and $\alpha$ control the variance
    of the mean and the variance of the normal likelihood, this term encourages the
    network to decrease the evidence for mispredicted data samples. As Amini et al.
    ([2020](#bib.bib1)) point out, large amounts of evidence are not punished in cases
    where the prediction is close to the target. However, Oh & Shin ([2022](#bib.bib136))
    argue that this combination of objectives might create adverse incentives for
    the model during training: Since the difference between the prediction and target
    in [Equation 26](#S4.E26 "In Univariate regression ‣ 4 Evidential Deep Learning
    for Regression ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation") is scaled by $\nu$, the model could learn
    to increase the predictive uncertainty by decreasing $\nu$ instead of improving
    its prediction. They propose to ameliorate this issue by using a third loss term
    of the form'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{MSE}}=\begin{cases}(y_{i}-\gamma)^{2}&amp;\quad\text{if
    }(y_{i}-\gamma)^{2}<U_{\nu,\alpha}\\ 2\sqrt{U_{\nu,\alpha}}&#124;y_{i}-\gamma&#124;-U_{\nu,\alpha}&amp;\quad\text{if
    }(y_{i}-\gamma)^{2}\geq U_{\nu,\alpha}\end{cases}$ |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where $U_{\nu,\alpha}=\min(U_{\nu},U_{\alpha})$ denotes the minimum value for
    the uncertainty thresholds for $\nu,\alpha$ given over a mini-batch, which are
    themselves defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $U_{\nu}=\frac{\beta(\nu+1)}{\alpha\nu};\quad U_{\alpha}=\frac{2\beta(\nu+1)}{\nu}\Big{[}\exp\Big{(}\psi\Big{(}\alpha+\frac{1}{2}\Big{)}-\psi(\alpha))-1\Big{)}\Big{]}.\\
    $ |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: 'These expression are obtained by taking the derivatives $\partial\mathcal{L}_{\text{NLL}}/\partial\nu$,
    $\partial\mathcal{L}_{\text{NLL}}/\partial\alpha$ and solving for the parameters,
    thus giving us the values for $\nu$ and $\alpha$ for which the loss gradients
    are maximal. In combination with [Equation 28](#S4.E28 "In Univariate regression
    ‣ 4 Evidential Deep Learning for Regression ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), [Equation 29](#S4.E29
    "In Univariate regression ‣ 4 Evidential Deep Learning for Regression ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation") ensures that, should the model error exceed $U_{\nu,\alpha}$, the
    error is rescaled. Thus, this rescaling bounds the Lipschitz constant of the loss
    function, motivating the model to ensure the correctness of its prediction, since
    its ability to increase uncertainty to decrease its loss is now limited.'
  prefs: []
  type: TYPE_NORMAL
- en: Posterior networks for regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Another approach for regression is the Natural Posterior Network by Charpentier
    et al. ([2022](#bib.bib17)), which was already discussed for classification in
    [Section 3.4.2](#S3.SS4.SSS2 "3.4.2 Posterior Networks ‣ 3.4 Existing Approaches
    for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"). But since the proposed approach is a generalization for exponential
    family distributions, it can be applied to regression as well, using a Normal
    likelihood and Normal Inverse-Gamma prior. The Bayesian update rule in [Equation 23](#S3.E23
    "In Posterior networks via Normalizing Flows ‣ 3.4.2 Posterior Networks ‣ 3.4
    Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") is adapted as follows: $n$ is set to $n=\lambda=2\alpha$,
    and $\bm{\chi}=\big{[}\pi_{0}\ |\ \pi_{0}^{2}+2\beta/n\big{]}^{T}$. Feeding an
    input into the natural posterior network again first produces a latent encoding
    $\mathbf{z}$, from which a NF predicts $n_{i}=N_{H}\cdot p(\mathbf{z}|\bm{\phi})$,
    and an additional network produces $\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{z})$,
    which are used in [Equation 23](#S3.E23 "In Posterior networks via Normalizing
    Flows ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks
    ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation") to produce
    $\bm{\chi}^{\text{post}}$ and $n^{\text{post}}$, from which the parameters of
    the posterior Normal Inverse-Gamma can be derived. The authors also produce a
    general exponential family form of the UCE loss by Biloš et al. ([2019](#bib.bib11)),
    consisting of expected log-likelihood and an entropy regularizer, which they derive
    for the regression parameterization. Again, this approach relies on the density
    estimation capabilities of the NF to produce an agnostic belief about the right
    prediction for OOD examples (see [Figure 5(b)](#S3.F5.sf2 "In Figure 5 ‣ Generating
    OOD samples using generative models ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing
    Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")).'
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate evidential regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are also some works offering solutions for multivariate regression problems:
    Malinin et al. ([2020a](#bib.bib117)) can be seen as a multivariate generalization
    of the work of Amini et al. ([2020](#bib.bib1)), where a combined Normal-Wishart
    prior is formed to fit the now Multivariate Normal likelihood. Again, the prior
    parameters are the output of a neural network, and uncertainty can be quantified
    in a similar way. For training purposes, they apply two different training objectives
    using the equivalent of the reverse KL objective of Malinin & Gales ([2019](#bib.bib116))
    as well as of the knowledge distillation objective of Malinin et al. ([2020b](#bib.bib118)),
    which does not require OOD data for regularization purposes. Meinert & Lavin ([2021](#bib.bib123))
    also provide a solution using a Normal Inverse-Wishart prior. In a similar vein
    to Oh & Shin ([2022](#bib.bib136)), they argue that the original objective proposed
    by Amini et al. ([2020](#bib.bib1)) can be minimized by increasing the network’s
    uncertainty instead of decreasing the mismatch of its prediction. As a solution,
    they simply propose to tie $\beta$ and $\nu$ via a hyperparameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Other Approaches to Uncertainty Quantification
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The need for the quantification of uncertainty in order to earn the trust of
    end-users and stakeholders has been a key driver for research (Bhatt et al., [2021](#bib.bib10);
    Jacovi et al., [2021](#bib.bib79); Liao & Sundar, [2022](#bib.bib106)). Existing
    methods can broadly be divided into frequentist and Bayesian methods, where the
    former judge the confidence of a model based on its predicted probabilities. Unfortunately,
    standard neural discriminator architectures have been proven to possess unwanted
    theoretical properties w.r.t. OOD inputs (Hein et al., [2019](#bib.bib61); Ulmer
    & Cinà, [2021](#bib.bib161)) and might therefore be unable to detect potentially
    risky inputs.^(14)^(14)14Pearce et al. ([2021](#bib.bib141)) argue that some insights
    might partially be misled by low-dimensional intuitions, and that empirically
    OOD data in higher dimensions tend to be mapped into regions of higher uncertainty.
    Further, a large line of research works has questioned the calibration of models
    (Guo et al., [2017](#bib.bib56); Nixon et al., [2019](#bib.bib135); Desai & Durrett,
    [2020](#bib.bib32); Minderer et al., [2021](#bib.bib126); Wang et al., [2021b](#bib.bib171)),
    i.e. to what extend the probability score of a class—also referred to as its confidence—corresponds
    to the chance of a correct prediction. Instead of relying on the confidence score
    alone, another way lies in constructing prediction sets consisting of the classes
    accumulating a certain share of the total predictive mass (Kompa et al., [2021](#bib.bib90);
    Ulmer et al., [2022](#bib.bib163)). By scoring a held-out population of data points
    to calibrate these prediction sets, we can also obtain frequentist guarantees
    in a procedure referred to a *conformal prediction* (Papadopoulos et al., [2002](#bib.bib138);
    Vovk et al., [2005](#bib.bib169); Lei & Wasserman, [2014](#bib.bib103); Angelopoulos
    & Bates, [2021](#bib.bib3)). This however still does not let us distinguish different
    notions of uncertainty. A popular *Bayesian* way to overcome this blemish by aggregating
    multiple predictions by networks in the Bayesian model averaging framework (Mackay,
    [1992](#bib.bib114); MacKay, [1995](#bib.bib112); Hinton & Van Camp, [1993](#bib.bib67);
    Neal, [2012](#bib.bib134); Jeffreys, [1998](#bib.bib80); Wilson & Izmailov, [2020](#bib.bib178);
    Kristiadi et al., [2020](#bib.bib92); Daxberger et al., [2021](#bib.bib27); Gal
    & Ghahramani, [2016](#bib.bib45); Blundell et al., [2015](#bib.bib13); Lakshminarayanan
    et al., [2017](#bib.bib99)). Nevertheless, many of these methods have been shown
    not to produce diverse predictions (Wilson & Izmailov, [2020](#bib.bib178); Fort
    et al., [2019](#bib.bib41)) and to deliver subpar performance and potentially
    misleading uncertainty estimates under distributional shift (Ovadia et al., [2019](#bib.bib137);
    Masegosa, [2020](#bib.bib120); Wenzel et al., [2020](#bib.bib174); Izmailov et al.,
    [2021a](#bib.bib77); [b](#bib.bib78)), raising doubts about their efficacy. The
    most robust method in this context is often given by an ensemble of neural predictors
    (Lakshminarayanan et al., [2017](#bib.bib99)), with multiple works exploring ways
    to make their training more efficient (Huang et al., [2017](#bib.bib72); Wilson
    & Izmailov, [2020](#bib.bib178); Wen et al., [2020](#bib.bib173); Turkoglu et al.,
    [2022](#bib.bib160)) or to provide theoretical guarantees (Pearce et al., [2020](#bib.bib140);
    Ciosek et al., [2020](#bib.bib19); He et al., [2020](#bib.bib60); D’Angelo & Fortuin,
    [2021](#bib.bib25)).
  prefs: []
  type: TYPE_NORMAL
- en: Related Approaches to EDL
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Kull et al. ([2019](#bib.bib94)) found an appealing use of the Dirichlet distribution
    as a post-training calibration map. Hobbhahn et al. ([2022](#bib.bib68)) use the
    Laplace bridge, a modified inverse based on an idea by MacKay ([1998](#bib.bib113)),
    to map from the model’s logit space to a Dirichlet distribution. The proposed
    Posterior Network (Charpentier et al., [2020](#bib.bib16); [2022](#bib.bib17))
    can furthermore be seen as related to another, competing approach, namely the
    combination of neural discriminators with density estimation methods, for instance
    in the form of energy-based models ([Grathwohl et al.,](#bib.bib54) ; Elflein
    et al., [2021](#bib.bib37)) or other hybrid architectures (Lee et al., [2018](#bib.bib102);
    Mukhoti et al., [2021](#bib.bib128)). Furthermore, there is a line of other single-pass
    uncertainty quantification approaches which do not originate from the evidential
    framework, for instance by taking inspiration from RBF networks (van Amersfoort
    et al., [2020b](#bib.bib165)) or via Gaussian Process output layers (Liu et al.,
    [2020](#bib.bib108); Fortuin et al., [2021](#bib.bib43); van Amersfoort et al.,
    [2021](#bib.bib166)).
  prefs: []
  type: TYPE_NORMAL
- en: Applications of EDL
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some of the discussed models have already found a variety of applications, such
    as in autonomous driving (Capellier et al., [2019](#bib.bib15); Liu et al., [2021](#bib.bib110);
    Petek et al., [2022](#bib.bib143); Wang et al., [2021a](#bib.bib170)), remote
    sensing (Gawlikowski et al., [2022](#bib.bib48)), medical screening (Ghesu et al.,
    [2019](#bib.bib51); Gu et al., [2021](#bib.bib55); Li et al., [2022](#bib.bib104)),
    molecular analysis (Soleimany et al., [2021](#bib.bib154)), open set recognition
    (Bao et al., [2021](#bib.bib6)), active learning (Hemmer et al., [2022](#bib.bib62))
    and model selection (Radev et al., [2021](#bib.bib144)).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is state-of-the-art?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As apparent from [Table 5](#A2.T5 "In Appendix B Datasets & Evaluation Techniques
    Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"), evaluation methods and datasets can vary
    tremendously between different research works (for an overview, refer to [Appendix B](#A2
    "Appendix B Datasets & Evaluation Techniques Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"),). This
    can make it hard to accurately compare different approaches in a fair manner.
    Nevertheless, we try to draw some conclusion about the state-of-art in this research
    direction to the best extent possible: For image classification, the posterior
    (Charpentier et al., [2020](#bib.bib16)) and natural posterior network (Charpentier
    et al., [2022](#bib.bib17)) provide the best results on the tested benchmarks,
    both in terms of task performance and uncertainty quality. When the training an
    extra normalizing flow creates too much computational overhead, prior networks
    (Malinin & Gales, [2018](#bib.bib115)) with the PAC-based regularizer ([Haussmann
    et al.](#bib.bib58), [2019](#bib.bib58); see [Table 6](#A5.T6 "In Appendix E Overview
    over Loss Functions Appendix ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") for final form) or a simple
    entropy regularizer ([Section C.2](#A3.SS2 "C.2 Entropy of Dirichlet ‣ Appendix
    C Fundamental Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation")) can be used. In
    the case of regression problems, the natural posterior network (Stadler et al.,
    [2021](#bib.bib155)) performs better or on par with the evidential regression
    by Amini et al. ([2020](#bib.bib1)) or an ensemble Lakshminarayanan et al. ([2017](#bib.bib99))
    or MC Dropout (Gal & Ghahramani, [2016](#bib.bib45)). For graph neural networks,
    the graph posterior network (Stadler et al., [2021](#bib.bib155)) and a ensemble
    provide similar performance, but with the former displaying better uncertainty
    results. Again, this model requires training a NF, so a simpler fallback is provided
    by evidential regression (Amini et al., [2020](#bib.bib1)) with the improvement
    by Oh & Shin ([2022](#bib.bib136)). For NLP and count prediction, the works of
    Shen et al. ([2020](#bib.bib151)) and Charpentier et al. ([2022](#bib.bib17))
    are the only available instances from this model family, respectively. In the
    latter case, ensembles and the evidential regression framework (Amini et al.,
    [2020](#bib.bib1)) produce a lower root mean-squared error, but worse uncertainty
    estimates on OOD.'
  prefs: []
  type: TYPE_NORMAL
- en: Computational Cost
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When it comes to the computational requirements, most of the proposed methods
    in this survey incur the same cost as a single deterministic network using a softmax
    output, since most of the architecture remains unchanged. Additional cost is mostly
    only produced when using knowledge distillation (Malinin et al., [2020b](#bib.bib118);
    Fathullah & Gales, [2022](#bib.bib39)), adding normalizing flow components like
    for posterior networks (Charpentier et al., [2020](#bib.bib16); [2022](#bib.bib17);
    Stadler et al., [2021](#bib.bib155)) or using generative models to produce synthetic
    OOD data (Chen et al., [2018](#bib.bib18); Sensoy et al., [2020](#bib.bib147);
    Hu et al., [2021](#bib.bib71)).
  prefs: []
  type: TYPE_NORMAL
- en: Comparison to Other Approaches to Uncertainty Quantification
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As discussed in [Section 5](#S5 "5 Related Work ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), several
    existing approaches to uncertainty quantification equally suffer from shortcomings
    with respect to their reliability. One possible explanation for this behavior
    might lie in the insight that neural networks trained in the empirical risk minimization
    framework tend to learn spurious but highly predictive features (Ilyas et al.,
    [2019](#bib.bib76); Nagarajan et al., [2021](#bib.bib130)). This way, inputs stemming
    from the training distribution can be mapped to similar parts of the latent space
    as data points outside the distribution even though they display (from a human
    perspective) blatant semantic differences, simply because these semantic features
    were not useful to optimize for the training objective. This can result in ID
    and OOD points having assigned similar feature representations by a network, a
    phenomenon has been coined “feature collapse” (Nalisnick et al., [2019](#bib.bib131);
    van Amersfoort et al., [2021](#bib.bib166); Havtorn et al., [2021](#bib.bib59)).
    One strategy to mitigate (but not solve) this issue has been to enforce a constraint
    on the smoothness of the neural network function (Wei et al., [2018](#bib.bib172);
    van Amersfoort et al., [2020a](#bib.bib164); [2021](#bib.bib166); Liu et al.,
    [2020](#bib.bib108)), thereby maintaining both a sensitivity to semantic changes
    in the input and robustness against adversarial inputs (Yu et al., [2019](#bib.bib186)).
    Another approach lies in the usage of OOD data as well, sometimes dubbed “outlier
    exposure” (Fort et al., [2021](#bib.bib42)), but displaying the same shortcomings
    as in the EDL case. A generally promising strategy seems to seek functional diversity
    through ensembling: Juneja et al. ([2022](#bib.bib84)) show how model instances
    ending up in different low-loss modes correspond to distinct generalization strategies,
    indicating that combining diverse strategies may lead to better generalization
    and thus potentially more reliable uncertainty. Attaining different solutions
    still creates computational overhead, despite new methods to reduce it (Garipov
    et al., [2018](#bib.bib47); Dusenberry et al., [2020](#bib.bib36); Benton et al.,
    [2021](#bib.bib9)).'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian model averaging
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'One of the most fundamental differences between EDL and existing approaches
    is the sacrifice of Bayesian model averaging ([Equations 2](#S2.E2 "In 2.1 Bayesian
    Inference ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") and [5](#S2.E5 "Equation 5
    ‣ 2.3 Evidential Deep Learning ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")): In
    principle, combining multiple parameter estimates is supposed to result in a lower
    predictive risk (Fragoso et al., [2018](#bib.bib44)). The Machine Learning community
    has ascribed further desiderata to this approach, such as better generalization
    and robustness to distributional shifts. Recent studies with exact Bayesian Neural
    Networks however have cast doubts on these assumptions (Izmailov et al., [2021a](#bib.bib77);
    [b](#bib.bib78)). Nevertheless, ensembles, that approximate [Equation 2](#S2.E2
    "In 2.1 Bayesian Inference ‣ 2 Background ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation") via Monte Carlo
    estimates, remain state-of-the-art on many uncertainty benchmarks. EDL abandons
    modelling epistemic uncertainty through the learnable parameters, and instead
    expresses it through the uncertainty in prior / posterior parameters. This loses
    functional diversity which could aid generalization, while sidestepping computational
    costs. Future research could therefore explore the combination of both paradigms,
    as proposed by Haussmann et al. ([2019](#bib.bib58)); Zhao et al. ([2020](#bib.bib189));
    Charpentier et al. ([2022](#bib.bib17)).'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Despite their advantages, the last chapters have pointed out key weaknesses
    of Dirichlet networks as well: In order to achieve the right behavior of the distribution
    and thus guarantee sensible uncertainty estimates (since ground truth estimates
    are not available), the surveyed literature proposes a variety of loss functions.
    Bengs et al. ([2022](#bib.bib8)) show formally that many of the loss functions
    used so far are *not* appropriate and violate basic asymptotic assumptions about
    epistemic uncertainty: With increasing amount of data, epistemic uncertainty should
    vanish, but this is not guaranteed using the commonly used loss functions. Furthermore,
    some approaches (Malinin & Gales, [2018](#bib.bib115); [2019](#bib.bib116); Nandy
    et al., [2020](#bib.bib133); Malinin et al., [2020a](#bib.bib117)) require out-of-distribution
    data points during training. This comes with two problems: Such data is often
    not available or in the first place, or cannot guarantee robustness against *other*
    kinds of unseen OOD data, of which infinite types exist in a real-valued feature
    space.^(15)^(15)15The same applies to the artificial OOD data in Chen et al. ([2018](#bib.bib18));
    Shen et al. ([2020](#bib.bib151)); Sensoy et al. ([2020](#bib.bib147)). Indeed,
    Kopetzki et al. ([2021](#bib.bib91)) found OOD detection to deteriorate across
    a family of EDL models under adversarial perturbation and OOD data. Stadler et al.
    ([2021](#bib.bib155)) point out that much of the ability of posterior networks
    stems from the addition of a NF, which have been shown to also sometimes behave
    unreliably on OOD data (Nalisnick et al., [2019](#bib.bib131)). Although the NFs
    in posterior networks operate on the latent and not the feature space, they are
    also restricted to operate on features that the underlying network has learned
    to recognize. Recent work by Dietterich & Guyer ([2022](#bib.bib33)) has hinted
    at the fact that networks might identify OOD by the absence of known features,
    and not by the presence of new ones, providing a case in which posterior networks
    are likely to fail. Such evidence on OOD data and adversarial examples has indeed
    been identified by a study by Kopetzki et al. ([2021](#bib.bib91)).'
  prefs: []
  type: TYPE_NORMAL
- en: Future Research Directions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Overall, the following directions for future research on EDL crystallize from
    our previous reflections: *(1) Explicit epistemic uncertainty estimation:* Since
    we often employ the point estimate in [Equation 5](#S2.E5 "In 2.3 Evidential Deep
    Learning ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") to avoid the posterior $p(\bm{\theta}|\mathbb{D})$,
    explicit estimation of the epistemic uncertainty is not possible, and some summary
    statistic of the concentration parameters is used for classification problems
    instead ([Section 3.3](#S3.SS3 "3.3 Uncertainty Estimation with Dirichlet Networks
    ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")). Estimating
    model uncertainty through modelling the (approximate) posterior $p(\bm{\theta}|\mathbb{D})$
    in Bayesian model averaging is a popular technique (Houlsby et al., [2011](#bib.bib69);
    Gal et al., [2016](#bib.bib46); Smith & Gal, [2018](#bib.bib153); Ulmer et al.,
    [2020](#bib.bib162)), but comes with the disadvantage of additional computational
    overhead. However, Sharma et al. ([2022](#bib.bib148)) recently showed that a
    Bayesian treatment of all model parameters may not be necessary, potentially allowing
    for a compromise. *(2) Robustness to diverse OOD data:* The emprical evidence
    compiled by Kopetzki et al. ([2021](#bib.bib91)) indicates that EDL classification
    models are not completely able to robustly classify and detect OOD and adversarial
    inputs. These findings hold both for prior networks trained with OOD data, or
    for posterior networks using density estimators. We speculate that through the
    information bottleneck principle (Tishby & Zaslavsky, [2015](#bib.bib157)), EDL
    models might not learn input features that are useful to indicate uncertainty
    in their prediction, or at best identify the absence of known features, but not
    the presence of new ones (Dietterich & Guyer, [2022](#bib.bib33)). Finding a way
    to have models identify unusual features could this help to mitigate this problem.
    *(3) Theoretical guarantees:* Even though some guarantees have been derived for
    EDL classifiers w.r.t. OOD data points (Charpentier et al., [2020](#bib.bib16);
    Stadler et al., [2021](#bib.bib155)), Bengs et al. ([2022](#bib.bib8)) point out
    the flaws of current training regimes for epistemic uncertainty in the limit of
    infinite limit. Furthermore, Hüllermeier & Waegeman ([2021](#bib.bib75)) argue
    that even uncertainty estimates are affected by uncertainty themselves, impacting
    their usefulness.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This survey has given an overview over contemporary approaches for uncertainty
    estimation using neural networks to parameterize conjugate priors or the corresponding
    posteriors instead of likelihoods, called Evidential Deep Learning. We highlighted
    their appealing theoretical properties allowing for uncertainty estimation with
    minimal computational overhead, rendering them as a viable alternative to existing
    strategies. We also emphasized practical problems: In order to nudge models towards
    the desired behavior in the face of unseen or out-of-distribution samples, the
    design of the model architecture and loss function have to be carefully considered.
    Based on a summary and discussion of experimental findings in [Section 6](#S6
    "6 Discussion ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"), the entropy regularizer seems to be a sensible
    choice in prior networks when OOD data is not available. Combining discriminators
    with generative models like normalizing flows as in Charpentier et al. ([2020](#bib.bib16);
    [2022](#bib.bib17)), embedded in a sturdy Bayesian framework, also appears as
    an exciting direction for practical applications. In summary, we believe that
    recent advances show promising results for Evidential Deep Learning, making it
    a viable option in uncertainty estimation to improve safety and trustworthiness
    in Machine Learning systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like to thank Giovanni Cinà, Max Müller-Eberstein, Daniel Varab and
    Mike Zhang for reading early versions of this draft and providing tremendously
    useful feedback. Further, we would like to explicitly thank Mike Zhang for helping
    to improve [Figure 1](#S1.F1 "In 1 Introduction ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). We
    also would like to thank Alexander Amini for providing a long list of references
    that helped to further improve the coverage of this work and the anonymous reviewers
    for their suggestions. Lastly, we owe our gratitude to the anonymous reviewers
    that helped us such much to improve the different versions of this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amini et al. (2020) Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela
    Rus. Deep Evidential Regression. In *Advances in Neural Information Processing
    Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
    2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrieu et al. (2000) Christophe Andrieu, Nando de Freitas, and Arnaud Doucet.
    Reversible Jump MCMC Simulated Annealing for Neural Networks. In Craig Boutilier
    and Moisés Goldszmidt (eds.), *UAI ’00: Proceedings of the 16th Conference in
    Uncertainty in Artificial Intelligence, Stanford University, Stanford, California,
    USA, June 30 - July 3, 2000*, pp.  11–18\. Morgan Kaufmann, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angelopoulos & Bates (2021) Anastasios N Angelopoulos and Stephen Bates. A Gentle
    Introduction to Conformal Prediction and Distribution-free Uncertainty Quantification.
    *arXiv preprint arXiv:2107.07511*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein
    Generative Adversarial Networks. In *International conference on machine learning*,
    pp. 214–223\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Audun (2018) Jsang Audun. *Subjective Logic: A Formalism for Reasoning under
    Uncertainty*. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bao et al. (2021) Wentao Bao, Qi Yu, and Yu Kong. Evidential Deep Learning for
    Open Set Action Recognition. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pp.  13349–13358, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bastidas (2017) Alexei Bastidas. Tiny Imagenet Image Classification, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengs et al. (2022) Viktor Bengs, Eyke Hüllermeier, and Willem Waegeman. On
    the Difficulty of Epistemic Uncertainty Quantification in Machine Learning: The
    Case of Direct Uncertainty Estimation through Loss Minimisation. *arXiv preprint
    arXiv:2203.06102*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benton et al. (2021) Gregory W. Benton, Wesley J. Maddox, Sanae Lotfi, and Andrew Gordon
    Wilson. Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling.
    In *Proceedings of the 38th International Conference on Machine Learning, ICML
    2021, 18-24 July 2021, Virtual Event*, volume 139 of *Proceedings of Machine Learning
    Research*, pp.  769–779\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhatt et al. (2021) Umang Bhatt, Javier Antorán, Yunfeng Zhang, Q. Vera Liao,
    Prasanna Sattigeri, Riccardo Fogliato, Gabrielle Gauthier Melançon, Ranganath
    Krishnan, Jason Stanley, Omesh Tickoo, Lama Nachman, Rumi Chunara, Madhulika Srikumar,
    Adrian Weller, and Alice Xiang. Uncertainty as a Form of Transparency: Measuring,
    Communicating, and Using Uncertainty. In *AIES ’21: AAAI/ACM Conference on AI,
    Ethics, and Society, Virtual Event, USA, May 19-21, 2021*, pp.  401–413\. ACM,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biloš et al. (2019) Marin Biloš, Bertrand Charpentier, and Stephan Günnemann.
    Uncertainty on Asynchronous Time Event Prediction. In *Advances in Neural Information
    Processing Systems*, pp. 12851–12860, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bishop (2006) Christopher M Bishop. Pattern Recognition. *Machine learning*,
    128(9), 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blundell et al. (2015) Charles Blundell, Julien Cornebise, Koray Kavukcuoglu,
    and Daan Wierstra. Weight Uncertainty in Neural Networks. *arXiv preprint arXiv:1505.05424*,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bulatov (2011) Yaroslav Bulatov. NotMNIST Dataset. *Google (Books/OCR), Tech.
    Rep.[Online]. Available: http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset.
    html*, 2, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capellier et al. (2019) Edouard Capellier, Franck Davoine, Véronique Cherfaoui,
    and You Li. Evidential Deep Learning for Arbitrary LIDAR Object Classification
    in the Context of Autonomous Driving. In *2019 IEEE Intelligent Vehicles Symposium,
    IV 2019, Paris, France, June 9-12, 2019*, pp.  1304–1311\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Charpentier et al. (2020) Bertrand Charpentier, Daniel Zügner, and Stephan
    Günnemann. Posterior network: Uncertainty estimation without ood samples via density-based
    pseudo-counts. *Advances in Neural Information Processing Systems*, 33:1356–1367,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Charpentier et al. (2022) Bertrand Charpentier, Oliver Borchert, Daniel Zügner,
    Simon Geisler, and Stephan Günnemann. Natural Posterior Network: Deep Bayesian
    Predictive Uncertainty for Exponential Family Distributions. In *The Tenth International
    Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,
    2022*. OpenReview.net, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Wenhu Chen, Yilin Shen, Hongxia Jin, and William Wang. A
    Variational Dirichlet Framework for Out-Of-Distribution Detection. *arXiv preprint
    arXiv:1811.07308*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ciosek et al. (2020) Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann,
    and Richard Turner. Conservative Uncertainty Estimation by Fitting Prior Networks.
    In *International Conference on Learning Representations*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clanuwat et al. (2018) Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto,
    Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep Learning for Classical Japanese
    Literature. *arXiv preprint arXiv:1812.01718*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coraddu et al. (2016) Andrea Coraddu, Luca Oneto, Aessandro Ghio, Stefano Savio,
    Davide Anguita, and Massimo Figari. Machine Learning Approaches for Improving
    Condition-Based Maintenance of Naval Propulsion Plants. *Proceedings of the Institution
    of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment*,
    230(1):136–153, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corke (1996) Peter I Corke. A Robotics Toolbox for MATLAB. *IEEE Robotics &
    Automation Magazine*, 3(1):24–32, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cortez et al. (2009) Paulo Cortez, António Cerdeira, Fernando Almeida, Telmo
    Matos, and José Reis. Modeling Wine Preferences by Data Mining from Physicochemical
    Properties. *Decision support systems*, 47(4):547–553, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coucke et al. (2018) Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche,
    Alexandre Caulier, David Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco
    Caltagirone, Thibaut Lavril, et al. Snips Voice Platform: An Embedded Spoken Language
    Understanding System for Private-by-Design Voice Interfaces. *arXiv preprint arXiv:1805.10190*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D’Angelo & Fortuin (2021) Francesco D’Angelo and Vincent Fortuin. Repulsive
    deep ensembles are bayesian. *Advances in Neural Information Processing Systems*,
    34:3451–3465, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davis et al. (2011) Mindy I Davis, Jeremy P Hunt, Sanna Herrgard, Pietro Ciceri,
    Lisa M Wodicka, Gabriel Pallares, Michael Hocker, Daniel K Treiber, and Patrick P
    Zarrinkar. Comprehensive Analysis of Kinase Inhibitor Selectivity. *Nature biotechnology*,
    29(11):1046–1051, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daxberger et al. (2021) Erik Daxberger, Agustinus Kristiadi, Alexander Immer,
    Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace Redux - Effortless
    Bayesian Deep Learning. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,
    Percy Liang, and Jennifer Wortman Vaughan (eds.), *Advances in Neural Information
    Processing Systems 34: Annual Conference on Neural Information Processing Systems
    2021, NeurIPS 2021, December 6-14, 2021, virtual*, pp. 20089–20103, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Freitas (2003) João Ferdinando Gomes de Freitas. *Bayesian Methods for Neural
    Networks*. PhD thesis, University of Cambridge, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dempster (1968) Arthur P Dempster. A Generalization of Bayesian Inference.
    *Journal of the Royal Statistical Society: Series B (Methodological)*, 30(2):205–232,
    1968.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. Imagenet: A Large-Scale Hierarchical Image Database. In *2009 IEEE
    conference on computer vision and pattern recognition*, pp.  248–255\. Ieee, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Der Kiureghian & Ditlevsen (2009) Armen Der Kiureghian and Ove Ditlevsen. Aleatory
    or Epistemic? Does it matter? *Structural safety*, 31(2):105–112, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Desai & Durrett (2020) Shrey Desai and Greg Durrett. Calibration of Pre-trained
    Transformers. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2020, Online, November 16-20, 2020*, pp. 295–302\. Association for Computational
    Linguistics, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dietterich & Guyer (2022) Thomas G. Dietterich and Alexander Guyer. The Familiarity
    Hypothesis: Explaining the Behavior of Deep Open Set Methods. *Pattern Recognit.*,
    132:108931, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dua et al. (2017) Dheeru Dua, Casey Graff, et al. UCI Machine Learning Repository.
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan (2021) Haonan Duan. Method of Moments in Approximate Bayesian Inference:
    From Theory to Practice. Master’s thesis, University of Waterloo, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dusenberry et al. (2020) Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-An
    Ma, Jasper Snoek, Katherine A. Heller, Balaji Lakshminarayanan, and Dustin Tran.
    Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors. In *Proceedings
    of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
    2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning Research*,
    pp.  2782–2792\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elflein et al. (2021) Sven Elflein, Bertrand Charpentier, Daniel Zügner, and
    Stephan Günnemann. On Out-of-distribution Detection with Energy-based Models.
    *arXiv preprint arXiv:2107.08785*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fanaee-T & Gama (2014) Hadi Fanaee-T and Joao Gama. Event Labeling Combining
    Ensemble Detectors and Background Knowledge. *Progress in Artificial Intelligence*,
    2(2):113–127, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fathullah & Gales (2022) Yassir Fathullah and Mark J. F. Gales. Self-distribution
    distillation: efficient uncertainty estimation. In James Cussens and Kun Zhang
    (eds.), *Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth
    Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022,
    Eindhoven, The Netherlands*, volume 180 of *Proceedings of Machine Learning Research*,
    pp.  663–673\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fisher (1936) Ronald A Fisher. The Use of Multiple Measurements in Taxonomic
    Problems. *Annals of eugenics*, 7(2):179–188, 1936.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fort et al. (2019) Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep
    Ensembles: A Loss Landscape Perspective. *arXiv preprint arXiv:1912.02757*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fort et al. (2021) Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring
    the Limits of Out-of-Distribution Detection. In *Advances in Neural Information
    Processing Systems 34: Annual Conference on Neural Information Processing Systems
    2021, NeurIPS 2021, December 6-14, 2021, virtual*, pp.  7068–7081, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortuin et al. (2021) Vincent Fortuin, Mark Collier, Florian Wenzel, James Allingham,
    Jeremiah Liu, Dustin Tran, Balaji Lakshminarayanan, Jesse Berent, Rodolphe Jenatton,
    and Effrosyni Kokiopoulou. Deep Classifiers with Label Noise Modeling and Distance
    Awareness. *arXiv preprint arXiv:2110.02609*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fragoso et al. (2018) Tiago M Fragoso, Wesley Bertoli, and Francisco Louzada.
    Bayesian Model Averaging: A Systematic Review and Conceptual Classification. *International
    Statistical Review*, 86(1):1–28, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal & Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian
    Approximation: Representing Model Uncertainty in Deep Learning. In *International
    conference on Machine Learning*, pp. 1050–1059, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gal et al. (2016) Yarin Gal et al. Uncertainty in Deep Learning. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garipov et al. (2018) Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P.
    Vetrov, and Andrew Gordon Wilson. Loss Surfaces, Mode Connectivity, and Fast Ensembling
    of DNNs. In *Advances in Neural Information Processing Systems 31: Annual Conference
    on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, pp.  8803–8812, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gawlikowski et al. (2022) Jakob Gawlikowski, Sudipan Saha, Anna M. Kruspe, and
    Xiao Xiang Zhu. An Advanced Dirichlet Prior Network for Out-of-Distribution Detection
    in Remote Sensing. *IEEE Trans. Geosci. Remote. Sens.*, 60:1–19, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gelman et al. (1995) Andrew Gelman, John B Carlin, Hal S Stern, and Donald B
    Rubin. *Bayesian Data Analysis*. Chapman and Hall/CRC, 1995.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gerritsma et al. (1981) J Gerritsma, R Onnink, and A Versluis. Geometry, Resistance
    and Stability of the Delft Systematic Yacht Hull Series. *International shipbuilding
    progress*, 28(328):276–297, 1981.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghesu et al. (2019) Florin C. Ghesu, Bogdan Georgescu, Eli Gibson, Sebastian
    Gündel, Mannudeep K. Kalra, Ramandeep Singh, Subba R. Digumarthy, Sasa Grbic,
    and Dorin Comaniciu. Quantifying and Leveraging Classification Uncertainty for
    Chest Radiograph Assessment. In *Medical Image Computing and Computer Assisted
    Intervention - MICCAI 2019 - 22nd International Conference, Shenzhen, China, October
    13-17, 2019, Proceedings, Part VI*, volume 11769 of *Lecture Notes in Computer
    Science*, pp.  676–684\. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giles et al. (1998) C Lee Giles, Kurt D Bollacker, and Steve Lawrence. CiteSeer:
    An Automatic Citation Indexing System. In *Proceedings of the third ACM conference
    on Digital libraries*, pp.  89–98, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz,
    Sacha Arnoud, and Vinay D. Shet. Multi-Digit Number Recognition from Street View
    Imagery using Deep Convolutional Neural Networks. In *2nd International Conference
    on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
    Conference Track Proceedings*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (54) Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud,
    Mohammad Norouzi, and Kevin Swersky. Your Classifier is Secretly an Energy-Based
    Model and You Should Treat It Like One. In *8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2021) Ang Nan Gu, Christina Luong, Mohammad H. Jafari, Nathan Van
    Woudenberg, Hany Girgis, Purang Abolmaesumi, and Teresa Tsang. Efficient Echocardiogram
    View Classification with Sampling-Free Uncertainty Estimation. In *Simplifying
    Medical Ultrasound - Second International Workshop, ASMUS 2021, Held in Conjunction
    with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings*, volume
    12967 of *Lecture Notes in Computer Science*, pp.  139–148\. Springer, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
    On Calibration of Modern Neural Networks. In *Proceedings of the 34th International
    Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
    2017*, volume 70 of *Proceedings of Machine Learning Research*, pp.  1321–1330\.
    PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harrison Jr & Rubinfeld (1978) David Harrison Jr and Daniel L Rubinfeld. Hedonic
    Housing Prices and the Demand for Clean Air. *Journal of environmental economics
    and management*, 5(1):81–102, 1978.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haussmann et al. (2019) Manuel Haussmann, Sebastian Gerwinn, and Melih Kandemir.
    Bayesian Evidential Deep Learning with PAC Regularization. *arXiv preprint arXiv:1906.00816*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Havtorn et al. (2021) Jakob Drachmann Havtorn, Jes Frellsen, Søren Hauberg,
    and Lars Maaløe. Hierarchical VAEs Know What They Don’t Know. In *Proceedings
    of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
    2021, Virtual Event*, volume 139 of *Proceedings of Machine Learning Research*,
    pp.  4117–4128\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian
    Deep Ensembles via the Neural Tangent Kernel. *Advances in neural information
    processing systems*, 33:1010–1022, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hein et al. (2019) Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf.
    Why ReLU Networks Yield High-Confidence Predictions Far Away From the Training
    Data and How to Mitigate the Problem. In *IEEE Conference on Computer Vision and
    Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*, pp. 41–50\.
    Computer Vision Foundation / IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hemmer et al. (2022) Patrick Hemmer, Niklas Kühl, and Jakob Schöffer. Deal:
    Deep Evidential Active Learning for Image Classification. In *Deep Learning Applications,
    Volume 3*, pp.  171–192. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hemphill et al. (1990) Charles T Hemphill, John J Godfrey, and George R Doddington.
    The ATIS Spoken Language Systems Pilot Corpus. In *Speech and Natural Language:
    Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990*,
    1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks & Gimpel (2017) Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting
    Misclassified and Out-of-Distribution Examples in Neural Networks. In *5th International
    Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
    2017, Conference Track Proceedings*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hernández-Lobato & Adams (2015) José Miguel Hernández-Lobato and Ryan Adams.
    Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks.
    In *International conference on machine learning*, pp. 1861–1869\. PMLR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling
    the Knowledge in a Neural Network. *arXiv preprint arXiv:1503.02531*, 2(7), 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton & Van Camp (1993) Geoffrey E Hinton and Drew Van Camp. Keeping the Neural
    Networks Simple by Minimizing the Description Length of the Weights. In *Proceedings
    of the sixth annual conference on Computational learning theory*, pp.  5–13, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hobbhahn et al. (2022) Marius Hobbhahn, Agustinus Kristiadi, and Philipp Hennig.
    Fast predictive uncertainty for classification with bayesian deep networks. In
    *Uncertainty in Artificial Intelligence*, pp.  822–832. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2011) Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté
    Lengyel. Bayesian Active Learning for Classification and Preference Learning.
    *arXiv preprint arXiv:1112.5745*, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets
    for Machine Learning on Graphs. *Advances in neural information processing systems*,
    33:22118–22133, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2021) Yibo Hu, Yuzhe Ou, Xujiang Zhao, Jin-Hee Cho, and Feng Chen.
    Multidimensional Uncertainty-Aware Evidential Neural Networks. In *Thirty-Fifth
    AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference
    on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual
    Event, February 2-9, 2021*, pp.  7815–7822\. AAAI Press, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2017) Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E.
    Hopcroft, and Kilian Q. Weinberger. Snapshot Ensembles: Train 1, Get M for Free.
    In *5th International Conference on Learning Representations, ICLR 2017, Toulon,
    France, April 24-26, 2017, Conference Track Proceedings*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao, Dingfu
    Zhou, Peng Wang, Yuanqing Lin, and Ruigang Yang. The Apolloscape Dataset for Autonomous
    Driving. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition workshops*, pp.  954–960, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hüllermeier (2022) Eyke Hüllermeier. Quantifying Aleatoric and Epistemic Uncertainty
    in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate
    Measures? *arXiv preprint arXiv:2209.03302*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hüllermeier & Waegeman (2021) Eyke Hüllermeier and Willem Waegeman. Aleatoric
    and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and
    Methods. *Mach. Learn.*, 110(3):457–506, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ilyas et al. (2019) Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan
    Engstrom, Brandon Tran, and Aleksander Madry. Adversarial Examples Are Not Bugs,
    They Are Features. In *Advances in Neural Information Processing Systems 32: Annual
    Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
    8-14, 2019, Vancouver, BC, Canada*, pp.  125–136, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izmailov et al. (2021a) Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, and
    Andrew G Wilson. Dangers of Bayesian Model Averaging under Covariate Shift. *Advances
    in Neural Information Processing Systems*, 34:3309–3322, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Izmailov et al. (2021b) Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, and
    Andrew Gordon Wilson. What Are Bayesian Neural Network Posteriors Really Like?
    In *Proceedings of the 38th International Conference on Machine Learning, ICML
    2021, 18-24 July 2021, Virtual Event*, volume 139 of *Proceedings of Machine Learning
    Research*, pp.  4629–4640\. PMLR, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jacovi et al. (2021) Alon Jacovi, Ana Marasović, Tim Miller, and Yoav Goldberg.
    Formalizing trust in artificial intelligence: Prerequisites, Causes and Goals
    of Human Trust in AI. In *Proceedings of the 2021 ACM conference on fairness,
    accountability, and transparency*, pp.  624–635, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeffreys (1998) Harold Jeffreys. *The Theory of Probability*. OUP Oxford, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2017) Robin Jia, Larry Heck, Dilek Hakkani-Tür, and Georgi Nikolov.
    Learning Concepts through Conversations in Spoken Dialogue Systems. In *2017 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    pp.  5725–5729\. IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joo et al. (2020) Taejong Joo, Uijung Chung, and Min-Gwan Seo. Being bayesian
    about categorical probability. In *International conference on machine learning*,
    pp. 4950–4961\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jordan et al. (1999) Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola,
    and Lawrence K Saul. An Introduction to Variational Methods for Graphical Models.
    *Machine learning*, 37(2):183–233, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Juneja et al. (2022) Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc,
    and Naomi Saphra. Linear Connectivity Reveals Generalization Strategies. *arXiv
    preprint arXiv:2205.12411*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall & Gal (2017) Alex Kendall and Yarin Gal. What Uncertainties do We Need
    in Bayesian Deep Learning for Computer Vision? *Advances in neural information
    processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2019) Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia
    He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al.
    PubChem 2019 Update: Improved Access to Chemical Data. *Nucleic acids research*,
    47(D1):D1102–D1109, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma & Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic
    Optimization. In Yoshua Bengio and Yann LeCun (eds.), *3rd International Conference
    on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
    Track Proceedings*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Welling (2014) Diederik P. Kingma and Max Welling. Auto-Encoding Variational
    Bayes. In *2nd International Conference on Learning Representations, ICLR 2014,
    Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klicpera et al. (2019) Johannes Klicpera, Aleksandar Bojchevski, and Stephan
    Günnemann. Predict then Propagate: Graph Neural Networks meet Personalized PageRank.
    In *7th International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kompa et al. (2021) Benjamin Kompa, Jasper Snoek, and Andrew L. Beam. Empirical
    Frequentist Coverage of Deep Learning Uncertainty Quantification Procedures. *Entropy*,
    23(12):1608, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kopetzki et al. (2021) Anna-Kathrin Kopetzki, Bertrand Charpentier, Daniel
    Zügner, Sandhya Giri, and Stephan Günnemann. Evaluating Robustness of Predictive
    Uncertainty Estimation: Are Dirichlet-based Models Reliable? In *Proceedings of
    the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
    Virtual Event*, volume 139 of *Proceedings of Machine Learning Research*, pp. 
    5707–5718\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kristiadi et al. (2020) Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
    Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks. In *Proceedings
    of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
    2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning Research*,
    pp.  5436–5446\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning Multiple
    Layers of Features from Tiny Images. 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kull et al. (2019) Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo
    Silva Filho, Hao Song, and Peter Flach. Beyond Temperature Scaling: Obtaining
    Well-Calibrated Multi-Class Probabilities with Dirichlet Calibration. *Advances
    in neural information processing systems*, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kupperman (1964) Morton Kupperman. Probabilities of Hypotheses and Information-Statistics
    in Sampling from Exponential-Class Populations. *Selected Mathematical Papers*,
    29(2):57, 1964.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurakin et al. (2017) Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial
    Examples in the Physical World. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lahlou et al. (2022) Salem Lahlou, Moksh Jain, Hadi Nekoei, Victor I Butoi,
    Paul Bertin, Jarrid Rector-Brooks, Maksym Korablyov, and Yoshua Bengio. DEUP:
    Direct Epistemic Uncertainty Prediction. *Transactions on Machine Learning Research*,
    2022. ISSN 2835-8856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lake et al. (2015) Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum.
    Human-Level Concept Learning Through Probabilistic Program Induction. *Science*,
    350(6266):1332–1338, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and
    Charles Blundell. Simple and Scalable Predictive Uncertainty Estimation using
    Deep Ensembles. In *Advances in neural information processing systems*, pp. 6402–6413,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun (1998) Yann LeCun. The MNIST Database of Handwritten Digits, 1998. URL
    [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    Gradient-Based Learning Applied to Document Recognition. *Proceedings of the IEEE*,
    86(11):2278–2324, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2018) Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A Simple
    Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, pp.  7167–7177, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei & Wasserman (2014) Jing Lei and Larry Wasserman. Distribution-free Prediction
    Bands for Non-parametric Regression. *Journal of the Royal Statistical Society:
    Series B (Statistical Methodology)*, 76(1):71–96, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Hao Li, Yang Nan, Javier Del Ser, and Guang Yang. Region-Based
    Evidential Deep Learning to Quantify Uncertainty and Improve Robustness of Brain
    Tumor Segmentation. *arXiv preprint arXiv:2208.06038*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2018) Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing The Reliability
    of Out-of-distribution Image Detection in Neural Networks. In *6th International
    Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
    30 - May 3, 2018, Conference Track Proceedings*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao & Sundar (2022) Q. Vera Liao and S. Shyam Sundar. Designing for Responsible
    Trust in AI Systems: A Communication Perspective. In *FAccT ’22: 2022 ACM Conference
    on Fairness, Accountability, and Transparency, Seoul, Republic of Korea, June
    21 - 24, 2022*, pp.  1257–1268\. ACM, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin (2016) Jiayu Lin. On the Dirichlet Distribution. *Mater’s Report*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) Jeremiah Z. Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania
    Bedrax-Weiss, and Balaji Lakshminarayanan. Simple and Principled Uncertainty Estimation
    with Deterministic Deep Learning via Distance Awareness. In *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2007) Tiqing Liu, Yuhmei Lin, Xin Wen, Robert N Jorissen, and Michael K
    Gilson. BindingDB: A Web-Accessible Database of Experimentally Determined Protein–Ligand
    Binding Affinities. *Nucleic acids research*, 35(suppl_1):D198–D201, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) Zhijian Liu, Alexander Amini, Sibo Zhu, Sertac Karaman, Song
    Han, and Daniela L. Rus. Efficient and Robust LiDAR-Based End-to-End Navigation.
    In *IEEE International Conference on Robotics and Automation, ICRA 2021, Xi’an,
    China, May 30 - June 5, 2021*, pp.  13247–13254. IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2015) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep
    Learning Face Attributes in the Wild. In *Proceedings of the IEEE international
    conference on computer vision*, pp.  3730–3738, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MacKay (1995) David JC MacKay. Developments in Probabilistic Modelling with
    Neural Networks—Ensemble Learning. In *Neural Networks: Artificial Intelligence
    and Industrial Applications*, pp.  191–198\. Springer, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MacKay (1998) David JC MacKay. Choice of basis for Laplace approximation. *Machine
    learning*, 33:77–86, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mackay (1992) David John Cameron Mackay. *Bayesian Methods for Adaptive Models*.
    California Institute of Technology, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malinin & Gales (2018) Andrey Malinin and Mark J. F. Gales. Predictive Uncertainty
    Estimation via Prior Networks. In *Advances in Neural Information Processing Systems
    31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
    3-8 December 2018, Montréal, Canada*, pp.  7047–7058, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malinin & Gales (2019) Andrey Malinin and Mark J. F. Gales. Reverse KL-Divergence
    Training of Prior Networks: Improved Uncertainty and Adversarial Robustness. In
    *Advances in Neural Information Processing Systems 32: Annual Conference on Neural
    Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
    BC, Canada*, pp.  14520–14531, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malinin et al. (2020a) Andrey Malinin, Sergey Chervontsev, Ivan Provilkov, and
    Mark Gales. Regression Prior Networks. *arXiv preprint arXiv:2006.11590*, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malinin et al. (2020b) Andrey Malinin, Bruno Mlodozeniec, and Mark J. F. Gales.
    Ensemble Distribution Distillation. In *8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao (2019) Lei Mao. Introduction to Exponential Family, 2019. URL [https://zhiyzuo.github.io/Exponential-Family-Distributions/](https://zhiyzuo.github.io/Exponential-Family-Distributions/).
    Accessed April 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Masegosa (2020) Andrés R. Masegosa. Learning under Model Misspecification:
    Applications to Variational and Ensemble methods. In *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McAuley et al. (2015) Julian McAuley, Christopher Targett, Qinfeng Shi, and
    Anton Van Den Hengel. Image-Based Recommendations on Styles and Substitutes. In
    *Proceedings of the 38th international ACM SIGIR conference on research and development
    in information retrieval*, pp.  43–52, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCallum et al. (2000) Andrew Kachites McCallum, Kamal Nigam, Jason Rennie,
    and Kristie Seymore. Automating the Construction of Internet Portals with Machine
    Learning. *Information Retrieval*, 3(2):127–163, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meinert & Lavin (2021) Nis Meinert and Alexander Lavin. Multivariate Deep Evidential
    Regression. *arXiv preprint arXiv:2104.06135*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menze & Geiger (2015) Moritz Menze and Andreas Geiger. Object Scene Flow for
    Autonomous Vehicles. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, pp.  3061–3070, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miller (2011) Jeffrey W. Miller. (ML 7.7.A2) Expectation of a Dirichlet Random
    Variable, 2011. URL [https://www.youtube.com/watch?v=emnfq4txDuI](https://www.youtube.com/watch?v=emnfq4txDuI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minderer et al. (2021) Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances
    Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the
    Calibration of Modern Neural Networks. *Advances in Neural Information Processing
    Systems*, 34:15682–15694, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreno-Torres et al. (2012) Jose G Moreno-Torres, Troy Raeder, RocíO Alaiz-RodríGuez,
    Nitesh V Chawla, and Francisco Herrera. A Unifying View on Dataset Shift in Classification.
    *Pattern recognition*, 45(1):521–530, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mukhoti et al. (2021) Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort,
    Philip HS Torr, and Yarin Gal. Deterministic Neural Networks With Appropriate
    Inductive Biases Capture Epistemic and Aleatoric Uncertainty. *arXiv preprint
    arXiv:2102.11582*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Murphy (2007) Kevin P Murphy. Conjugate Bayesian Analysis of the Gaussian Distribution.
    2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagarajan et al. (2021) Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur.
    Understanding the Failure Modes of Out-Of-Distribution Generalization. In *9th
    International Conference on Learning Representations, ICLR 2021, Virtual Event,
    Austria, May 3-7, 2021*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nalisnick et al. (2019) Eric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh,
    Dilan Görür, and Balaji Lakshminarayanan. Do Deep Generative Models Know What
    They Don’t Know? In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namata et al. (2012) Galileo Namata, Ben London, Lise Getoor, Bert Huang, and
    UMD EDU. Query-Driven Active Surveying for Collective Classification. In *10th
    International Workshop on Mining and Learning with Graphs*, volume 8, pp.  1,
    2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nandy et al. (2020) Jay Nandy, Wynne Hsu, and Mong Li Lee. Towards Maximizing
    the Representation Gap between In-Domain & Out-of-Distribution Examples. *Advances
    in Neural Information Processing Systems*, 33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neal (2012) Radford M Neal. *Bayesian Learning for Neural Networks*, volume
    118. Springer Science & Business Media, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nixon et al. (2019) Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen
    Jerfel, and Dustin Tran. Measuring Calibration in Deep Learning. In *CVPR Workshops*,
    volume 2, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oh & Shin (2022) Dongpin Oh and Bonggun Shin. Improving Evidential Deep Learning
    via Multi-Task Learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 36, pp.  7895–7903, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ovadia et al. (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David
    Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper
    Snoek. Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty
    under Dataset Shift. In *Advances in Neural Information Processing Systems*, pp. 13991–14002,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papadopoulos et al. (2002) Harris Papadopoulos, Kostas Proedrou, Volodya Vovk,
    and Alex Gammerman. Inductive Confidence Machines for Regression. In *European
    Conference on Machine Learning*, pp.  345–356. Springer, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paschke et al. (2013) Fabian Paschke, Christian Bayer, Martyna Bator, Uwe Mönks,
    Alexander Dicks, Olaf Enge-Rosenblatt, and Volker Lohweg. Sensorlose Zustandsüberwachung
    an Synchronmotoren. In *Proc*, pp.  211, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pearce et al. (2020) Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty
    in Neural Networks: Approximately Bayesian Ensembling. In *International conference
    on artificial intelligence and statistics*, pp.  234–244\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearce et al. (2021) Tim Pearce, Alexandra Brintrup, and Jun Zhu. Understanding
    Softmax Confidence and Uncertainty. *arXiv preprint arXiv:2106.04972*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pedregosa et al. (2011) F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
    B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas,
    A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
    Machine Learning in Python. *Journal of Machine Learning Research*, 12:2825–2830,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petek et al. (2022) Kürsat Petek, Kshitij Sirohi, Daniel Büscher, and Wolfram
    Burgard. Robust Monocular Localization in Sparse HD Maps Leveraging Multi-Task
    Uncertainty Estimation. In *2022 International Conference on Robotics and Automation,
    ICRA 2022, Philadelphia, PA, USA, May 23-27, 2022*, pp.  4163–4169. IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radev et al. (2021) Stefan T Radev, Marco D’Alessandro, Ulf K Mertens, Andreas
    Voss, Ullrich Köthe, and Paul-Christian Bürkner. Amortized Bayesian Model Comparison
    with Evidential Deep Learning. *IEEE Transactions on Neural Networks and Learning
    Systems*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende & Mohamed (2015) Danilo Jimenez Rezende and Shakir Mohamed. Variational
    Inference with Normalizing Flows. In *Proceedings of the 32nd International Conference
    on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015*, volume 37 of *JMLR
    Workshop and Conference Proceedings*, pp.  1530–1538. JMLR.org, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensoy et al. (2018) Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential
    Deep Learning to Quantify Classification Uncertainty. In *Advances in Neural Information
    Processing Systems*, pp. 3179–3189, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensoy et al. (2020) Murat Sensoy, Lance M. Kaplan, Federico Cerutti, and Maryam
    Saleki. Uncertainty-Aware Deep Classifiers Using Generative Models. In *The Thirty-Fourth
    AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
    Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
    NY, USA, February 7-12, 2020*, pp.  5620–5627\. AAAI Press, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2022) Mrinank Sharma, Sebastian Farquhar, Eric Nalisnick, and
    Tom Rainforth. Do Bayesian Neural Networks Need To Be Fully Stochastic? *arXiv
    preprint arXiv:2211.06291*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shchur et al. (2018) Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski,
    and Stephan Günnemann. Pitfalls of Graph Neural Network Evaluation. *arXiv preprint
    arXiv:1811.05868*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2022) Maohao Shen, Yuheng Bu, Prasanna Sattigeri, Soumya Ghosh,
    Subhro Das, and Gregory Wornell. Post-hoc Uncertainty Learning using a Dirichlet
    Meta-Model. *arXiv preprint arXiv:2212.07359*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2020) Yilin Shen, Wenhu Chen, and Hongxia Jin. Modeling Token-level
    Uncertainty to Learn Unknown Concepts in SLU via Calibrated Dirichlet Prior RNN.
    *CoRR*, abs/2010.08101, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silberman et al. (2012) Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
    Fergus. Indoor Segmentation and Support Inference from RGBD Images. In *European
    conference on computer vision*, pp.  746–760. Springer, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smith & Gal (2018) Lewis Smith and Yarin Gal. Understanding Measures of Uncertainty
    for Adversarial Example Detection. In *Proceedings of the Thirty-Fourth Conference
    on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA,
    August 6-10, 2018*, pp.  560–569, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soleimany et al. (2021) Ava P Soleimany, Alexander Amini, Samuel Goldman, Daniela
    Rus, Sangeeta N Bhatia, and Connor W Coley. Evidential Deep Learning for Guided
    Molecular Property Prediction and Discovery. *ACS central science*, 7(8):1356–1367,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stadler et al. (2021) Maximilian Stadler, Bertrand Charpentier, Simon Geisler,
    Daniel Zügner, and Stephan Günnemann. Graph Posterior Network: Bayesian Predictive
    Uncertainty for Node Classification. *Advances in Neural Information Processing
    Systems*, 34, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2014) Jing Tang, Agnieszka Szwajda, Sushil Shakyawar, Tao Xu,
    Petteri Hintsanen, Krister Wennerberg, and Tero Aittokallio. Making Sense of Large-Scale
    Kinase Inhibitor Bioactivity Data Sets: A Comparative and Integrative Analysis.
    *Journal of Chemical Information and Modeling*, 54(3):735–743, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tishby & Zaslavsky (2015) Naftali Tishby and Noga Zaslavsky. Deep Learning and
    the Information Bottleneck Principle. In *2015 ieee information theory workshop
    (itw)*, pp.  1–5. IEEE, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsanas & Xifara (2012) Athanasios Tsanas and Angeliki Xifara. Accurate Quantitative
    Estimation of Energy Performance of Residential Buildings using Statistical Machine
    Learning Tools. *Energy and buildings*, 49:560–567, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsiligkaridis (2019) Theodoros Tsiligkaridis. Information Robust Dirichlet Networks
    for Predictive Uncertainty Estimation. *arXiv preprint arXiv:1910.04819*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turkoglu et al. (2022) Mehmet Ozgur Turkoglu, Alexander Becker, Hüseyin Anil
    Gündüz, Mina Rezaei, Bernd Bischl, Rodrigo Caye Daudt, Stefano D’Aronco, Jan Dirk
    Wegner, and Konrad Schindler. FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise
    Linear Modulation. *arXiv preprint arXiv:2206.00050*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ulmer & Cinà (2021) Dennis Ulmer and Giovanni Cinà. Know Your Limits: Uncertainty
    Estimation with ReLU Classifiers Fails at Reliable OOD Detection. In *Uncertainty
    in Artificial Intelligence*, pp.  1766–1776. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ulmer et al. (2020) Dennis Ulmer, Lotta Meijerink, and Giovanni Cinà. Trust
    Issues: Uncertainty Estimation Does not Enable Reliable OOD Detection on Medical
    Tabular Data. In *Machine Learning for Health*, pp.  341–354\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ulmer et al. (2022) Dennis Ulmer, Jes Frellsen, and Christian Hardmeier. "Exploring
    Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method
    & Data Scarcity". In *Findings of the Association for Computational Linguistics:
    EMNLP 2022*, pp.  2707–2735, Abu Dhabi, United Arab Emirates, December 2022\.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Amersfoort et al. (2020a) Joost van Amersfoort, Lewis Smith, Yee Whye Teh,
    and Yarin Gal. Uncertainty Estimation Using a Single Deep Deterministic Neural
    Network. In *Proceedings of the 37th International Conference on Machine Learning,
    ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine
    Learning Research*, pp.  9690–9700\. PMLR, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Amersfoort et al. (2020b) Joost van Amersfoort, Lewis Smith, Yee Whye Teh,
    and Yarin Gal. Uncertainty Estimation Using a Single Deep Deterministic Neural
    Network. In *Proceedings of the 37th International Conference on Machine Learning,
    ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine
    Learning Research*, pp.  9690–9700\. PMLR, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Amersfoort et al. (2021) Joost van Amersfoort, Lewis Smith, Andrew Jesson,
    Oscar Key, and Yarin Gal. On Feature Collapse and Deep Kernel Learning for Single
    Forward Pass Uncertainty. *arXiv preprint arXiv:2102.11409*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Erven & Harremoës (2014) Tim van Erven and Peter Harremoës. Rényi Divergence
    and Kullback-Leibler Divergence. *IEEE Trans. Inf. Theory*, 60(7):3797–3820, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Landeghem et al. (2022) Jordy Van Landeghem, Matthew Blaschko, Bertrand
    Anckaert, and Marie-Francine Moens. Benchmarking Scalable Predictive Uncertainty
    in Text Classification. *Ieee Access*, 10:43703–43737, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vovk et al. (2005) Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. *Algorithmic
    lLarning in a Random World*. Springer Science & Business Media, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Chen Wang, Xiang Wang, Jiawei Zhang, Liang Zhang, Xiao Bai,
    Xin Ning, Jun Zhou, and Edwin Hancock. Uncertainty Estimation for Stereo Matching
    Based on Evidential Deep Learning. *Pattern Recognition*, pp.  108498, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Deng-Bao Wang, Lei Feng, and Min-Ling Zhang. Rethinking
    Calibration of Deep Neural Networks: Do not be Afraid of Overconfidence. *Advances
    in Neural Information Processing Systems*, 34:11809–11820, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2018) Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang.
    Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its
    Dual Effect. In *6th International Conference on Learning Representations, ICLR
    2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. (2020) Yeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: an
    Alternative Approach to Efficient Ensemble and Lifelong Learning. In *8th International
    Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
    26-30, 2020*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wenzel et al. (2020) Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Światkowski,
    Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian
    Nowozin. How Good is the Bayes Posterior in Deep Neural Networks Really? *arXiv
    preprint arXiv:2002.02405*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wikimedia Commons (2022a) Wikimedia Commons. Iris setosa, 2022a. URL [{https://en.wikipedia.org/wiki/Iris_setosa}](%7Bhttps://en.wikipedia.org/wiki/Iris_setosa%7D).
    File:Irissetosa1.jpg.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wikimedia Commons (2022b) Wikimedia Commons. Iris versicolor, 2022b. URL [{https://en.wikipedia.org/wiki/Iris_versicolor}](%7Bhttps://en.wikipedia.org/wiki/Iris_versicolor%7D).
    File:Blue_Flag,_Ottawa.jpg.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wikimedia Commons (2022c) Wikimedia Commons. Iris virginica, 2022c. URL [{https://en.wikipedia.org/wiki/Iris_virginica#/media/File:Iris_virginica_2.jpg}](%7Bhttps://en.wikipedia.org/wiki/Iris_virginica#/media/File:Iris_virginica_2.jpg%7D).
    File:Iris_virginica_2.jpg.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wilson & Izmailov (2020) Andrew Gordon Wilson and Pavel Izmailov. Bayesian
    Deep Learning and a Probabilistic Perspective of Generalization. In *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Winn (2004) John Michael Winn. Variational Message Passing and Its Applications.
    2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Woo (2022) Jae Oh Woo. Analytic Mutual Information in Bayesian Neural Networks.
    In *IEEE International Symposium on Information Theory, ISIT 2022, Espoo, Finland,
    June 26 - July 1, 2022*, pp.  300–305\. IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2017) Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST:
    A Novel Image Dataset for Benchmarking Machine Learning Algorithms. *arXiv preprint
    arXiv:1708.07747*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2010) Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
    and Antonio Torralba. Sun Database: Large-scale Scene Recognition from Abbey to
    Zoo. In *2010 IEEE computer society conference on computer vision and pattern
    recognition*, pp.  3485–3492\. IEEE, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yager & Liu (2008) Ronald R Yager and Liping Liu. *Classic Works of the Dempster-Shafer
    Theory of Belief Functions*, volume 219. Springer, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yeh (1998) I-C Yeh. Modeling of Strength of High-Performance Concrete using
    Artificial Neural Networks. *Cement and Concrete research*, 28(12):1797–1808,
    1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2015) Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser,
    and Jianxiong Xiao. LSUN: Construction of a Large-Scale Image Dataset using Deep
    Learning with Humans in the Loop. *arXiv preprint arXiv:1506.03365*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019) Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang,
    and Xiang Chen. Interpreting and Evaluating Neural Network Robustness. In *Proceedings
    of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    IJCAI 2019, Macao, China, August 10-16, 2019*, pp.  4199–4205\. ijcai.org, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zerva et al. (2022) Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei, and André
    F. T. Martins. "Disentangling Uncertainty in Machine Translation Evaluation".
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pp.  8622–8641, Abu Dhabi, United Arab Emirates, December 2022\.
    Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Xujiang Zhao, Yuzhe Ou, Lance Kaplan, Feng Chen, and Jin-Hee
    Cho. Quantifying Classification Uncertainty Using Regularized Evidential Neural
    Networks. *arXiv preprint arXiv:1910.06864*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020) Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty
    Aware Semi-Supervised Learning on Graph Data. In *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Code Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Iris Example Training Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code used to produce [Figure 2](#S2.F2 "In 2.4 An Illustrating Example:
    The Iris Dataset ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") is available online.^(16)^(16)16Code
    will be made available upon acceptance. All models use three layers with $100$
    hidden units and ReLU activations each. We furthermore optimized all of the models
    with a learning rate of $0.001$ using the Adam optimizer (Kingma & Ba, [2015](#bib.bib87))
    with its default parameter settings. We also regularize the ensemble and MC Dropout
    model with a dropout probability of $0.1$ each.'
  prefs: []
  type: TYPE_NORMAL
- en: Prior Network specifics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We choose the expected $l_{2}$ loss by Sensoy et al. ([2018](#bib.bib146)) and
    regularize the network using the KL divergence w.r.t. to a uniform Dirichlet as
    in Sensoy et al. ([2018](#bib.bib146)). In the regularization term, we do not
    use the original concentration parameters $\bm{\alpha}$, but a version in which
    the concentration of the parameter $\alpha_{k}$ corresponding to the correct class
    is removed using a one-hot label encoding $\mathbf{y}$ by $\tilde{\bm{\alpha}}=(1-\bm{\alpha})\odot\bm{\alpha}+\mathbf{y}\odot\bm{\alpha}$,
    where $\odot$ denotes point-wise multiplication. The regularization term is added
    to the loss using a weighting factor of $0.05$.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Code Availability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: Overview over code repositories of surveyed works.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Code Repository |'
  prefs: []
  type: TYPE_TB
- en: '| Prior network (Malinin & Gales, [2018](#bib.bib115)) | [https://github.com/KaosEngineer/PriorNetworks-OLD](https://github.com/KaosEngineer/PriorNetworks-OLD)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks (Malinin & Gales, [2019](#bib.bib116)) | [https://github.com/KaosEngineer/PriorNetworks](https://github.com/KaosEngineer/PriorNetworks)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dirichlet via Function Decomposition (Biloš et al., [2019](#bib.bib11)) |
    [https://github.com/sharpenb/Uncertainty-Event-Prediction](https://github.com/sharpenb/Uncertainty-Event-Prediction)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prior network with PAC Regularization (Haussmann et al., [2019](#bib.bib58))
    | [https://github.com/manuelhaussmann/bedl](https://github.com/manuelhaussmann/bedl)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks with representation gap (Nandy et al., [2020](#bib.bib133))
    | [https://github.com/jayjaynandy/maximize-representation-gap](https://github.com/jayjaynandy/maximize-representation-gap)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-based Kernel Dirichlet distribution estimation (GKDE) (Zhao et al.,
    [2020](#bib.bib189)) | [https://github.com/zxj32/uncertainty-GNN](https://github.com/zxj32/uncertainty-GNN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Evidential Deep Learning (Sensoy et al., [2018](#bib.bib146)) | [https://muratsensoy.github.io/uncertainty.html](https://muratsensoy.github.io/uncertainty.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | [https://github.com/snowood1/wenn](https://github.com/snowood1/wenn)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Belief Matching (Joo et al., [2020](#bib.bib82)) | [https://github.com/tjoo512/belief-matching-framework](https://github.com/tjoo512/belief-matching-framework)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Posterior Networks (Charpentier et al., [2020](#bib.bib16)) | [https://github.com/sharpenb/Posterior-Network](https://github.com/sharpenb/Posterior-Network)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Graph Posterior Networks (Stadler et al., [2021](#bib.bib155)) | [https://github.com/stadlmax/Graph-Posterior-Network](https://github.com/stadlmax/Graph-Posterior-Network)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Evidential Neural Networks (Sensoy et al., [2020](#bib.bib147))
    | [https://muratsensoy.github.io/gen.html](https://muratsensoy.github.io/gen.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Evidential Regression with Multi-task Learning (Oh & Shin, [2022](#bib.bib136))
    | [https://github.com/deargen/MT-ENet](https://github.com/deargen/MT-ENet) |'
  prefs: []
  type: TYPE_TB
- en: '| Multivariate Deep Evidential Regression (Meinert & Lavin, [2021](#bib.bib123))
    | [https://github.com/avitase/mder/](https://github.com/avitase/mder/) |'
  prefs: []
  type: TYPE_TB
- en: '| Regression Prior Network (Malinin et al., [2020a](#bib.bib117)) | [https://github.com/JanRocketMan/regression-prior-networks](https://github.com/JanRocketMan/regression-prior-networks)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Posterior Network (Charpentier et al., [2022](#bib.bib17)) | [https://github.com/borchero/natural-posterior-network](https://github.com/borchero/natural-posterior-network)
    |'
  prefs: []
  type: TYPE_TB
- en: 'We list the available code repositories for surveyed works in [Table 4](#A1.T4
    "In A.2 Code Availability ‣ Appendix A Code Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). Works
    for which no official implementation could be found are not listed.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Datasets & Evaluation Techniques Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 5: Overview over uncertainty evaluation techniques and datasets. ^((∗))
    indicates that a dataset was used as an OOD dataset for evaluation purposes, while
    ^((⋄)) signifies that it was used as an in-distribution or out-of-distribution
    dataset. ^((†)) means that a dataset was modified to create ID and OOD splits
    (for instance by removing some classes for evaluation or corrupting samples with
    noise).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Data Modality |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Uncertainty Evaluation | Images | Tabular | Other |'
  prefs: []
  type: TYPE_TB
- en: '| Prior network (Malinin & Gales, [2018](#bib.bib115)) | OOD Detection, Misclassification
    Detection | MNIST, CIFAR-10, Omniglot^((∗)), SVHN^((∗)), LSUN^((∗)), TIM^((∗))
    | ✗ | Clusters (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks (Malinin & Gales, [2019](#bib.bib116)) | OOD Detection, Adversarial
    Attack Detection | MNIST, CIFAR-10/100, SVHN^((∗)), LSUN^((∗)), TIM^((∗)) | ✗
    | Clusters (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| Information Robust Dirichlet Networks (Tsiligkaridis, [2019](#bib.bib159))
    | OOD Detection, Adversarial Attack Detection | MNIST, FashionMNIST^((∗)) notMNIST^((∗)),
    Omniglot^((∗)) CIFAR-10, TIM^((∗)), SVHN^((∗)) | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Dirichlet via Function Decomposition (Biloš et al., [2019](#bib.bib11)) |
    OOD Detection | ✗ | Erdős-Rényi Graph (Synthetic), Stack Exchange, Smart Home,
    Car Indicators | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Prior network with PAC Regularization (Haussmann et al., [2019](#bib.bib58))
    | OOD Detection | MNIST, FashionMNIST^((∗)) CIFAR-10^((†)) | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Ensemble Distribution Distillation (Malinin et al., [2020b](#bib.bib118))
    | OOD Detection, Misclassification Detection, Calibration | CIFAR-10, CIFAR-100^((⋄))
    TIM^((⋄)), LSUN^((∗)) | ✗ | Spirals (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Distribution Distillation (Fathullah & Gales, [2022](#bib.bib39)) |
    OOD Detection, Calibration | CIFAR-100 SVHN^((∗)), LSUN^((∗)) | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks with representation gap (Nandy et al., [2020](#bib.bib133))
    | OOD Detection | CIFAR-10^((⋄)), CIFAR-100^((⋄)) TIM, ImageNet^((∗)) | ✗ | Clusters
    (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| Prior RNN (Shen et al., [2020](#bib.bib151)) | New Concept Extraction | ✗
    | ✗ | Concept Learning^((⋄)), Snips^((⋄)), ATIS^((⋄)) (Language) |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-based Kernel Dirichlet distribution estimation (GKDE) (Zhao et al.,
    [2020](#bib.bib189)) | OOD Detection Misclassification Detection | ✗ | ✗ | Coauthors
    Physics^((⋄)), Amazon Computer^((⋄)) Amazon Photo^((⋄)) (Graph) |'
  prefs: []
  type: TYPE_TB
- en: '| Evidential Deep Learning (Sensoy et al., [2018](#bib.bib146)) | OOD Detection,
    Adversarial Attack Detection | MNIST, notMNIST^((∗)), CIFAR-10^((†)) | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Regularized ENN Zhao et al. ([2019](#bib.bib188)) | OOD Detection | CIFAR-10^((†))
    | ✗ | Clusters (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | OOD Detection, Adversarial Attack
    Detection | MNIST, notMNIST^((∗)), CIFAR-10^((†)) | ✗ | Clusters (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| Variational Dirichlet (Chen et al., [2018](#bib.bib18)) | OOD Detection,
    Adversarial Attack Detection | MNIST, CIFAR-10/100, iSUN^((∗)), LSUN^((∗)), SVHN^((∗)),
    TIM^((∗)) | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Dirichlet Meta-Model (Shen et al., [2022](#bib.bib150)) | OOD Detection Misclassification
    Detection | MNIST^((⋄,†)), CIFAR-10^((⋄,†)), CIFAR-100^((⋄)), Omniglot^((∗)),
    FashionMNIST^((∗)), K-MNIST^((∗)), SVHN^((∗)), LSUN^((∗)), TIM^((∗)) | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Belief Matching (Joo et al., [2020](#bib.bib82)) | OOD Detection, Calibration
    | CIFAR-10/100, SVHN^((∗)) | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Posterior Networks (Charpentier et al., [2020](#bib.bib16)) | OOD Detection,
    Misclassification Detection, Calibration | MNIST, FashionMNIST^((∗)), K-MNIST^((∗)),
    CIFAR-10, SVHN^((∗)) | Segment^((†)), Sensorless Drive^((†)) | Clusters (Synthetic)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Graph Posterior Networks (Stadler et al., [2021](#bib.bib155)) | OOD Detection,
    Misclassification Detection, Calibration | ✗ | ✗ | Amazon Computer^((⋄)), Amazon
    Photo^((⋄)) CoraML^((⋄)), CiteSeerCoraML^((⋄)), PubMed^((⋄)), Coauthors Physics^((⋄)),
    CoauthorsCS^((⋄)), OBGN Arxiv^((⋄)) (Graph) |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Evidential Regression (Amini et al., [2020](#bib.bib1)) | OOD Detection,
    Misclassification Detection, Adversarial Attack Detection Calibration | NYU Depth
    v2 ApolloScape^∗ (Depth Estimation) | UCI Regression Benchmark | Univariate Regression
    (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Evidential Regression with Multi-task Learning (Oh & Shin, [2022](#bib.bib136))
    | OOD Detection, Calibration | ✗ | Davis, Kiba^((†)), BindingDB, PubChem^((∗))
    (Drug discovery), UCI Regression Benchmark | Univariate Regression (Synthetic)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multivariate Deep Evidential Regression Meinert & Lavin ([2021](#bib.bib123))
    | Qualitative Evaluation | ✗ | ✗ | Multivariate Regression (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| Regression Prior Network (Malinin et al., [2020a](#bib.bib117)) | OOD Detection
    | NYU Depth v2^⋄, KITTI^⋄ (Depth Estimation) | UCI Regression Benchmark | Univariate
    Regression (Synthetic) |'
  prefs: []
  type: TYPE_TB
- en: '| Natural Posterior Network (Charpentier et al., [2022](#bib.bib17)) | OOD
    Detection, Calibration | NYU Depth v2, KITTI^∗, LSUN^((∗)) (Depth Estimation),
    MNIST, FashionMNIST^((∗)), K-MNIST^((∗)), CIFAR-10^((†)), SVHN^((∗)), CelebA^((∗))
    | UCI Regression Benchmark^((†)), Sensorless Drive^((†)), Bike Sharing^((†)) |
    Clusters (Synthetic), Univariate Regression (Synthetic)) |'
  prefs: []
  type: TYPE_TB
- en: 'This section contains a discussion of the used datasets, methods to evaluate
    the quality of uncertainty evaluation, as well as a direct of available models
    based on the reported results to determine the most useful choices for practitioners.
    An overview over the differences between the surveyed works is given in [Table 5](#A2.T5
    "In Appendix B Datasets & Evaluation Techniques Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Most models are applied to image classification problems, where popular choices
    involve the MNIST dataset (LeCun, [1998](#bib.bib100)), using as OOD datasets
    Fashion-MNIST (Xiao et al., [2017](#bib.bib181)), notMNIST (Bulatov, [2011](#bib.bib14))
    containing English letters, K-MNIST (Clanuwat et al., [2018](#bib.bib20)) with
    ancient Japanese Kuzushiji characters, and the Omniglot dataset (Lake et al.,
    [2015](#bib.bib98)), featuring handwritten characters from more than 50 alphabets.
    Other choices involve different versions of the CIFAR-10 object recognition dataset
    (LeCun et al., [1998](#bib.bib101); Krizhevsky et al., [2009](#bib.bib93)) for
    training purposes and SVHN (Goodfellow et al., [2014](#bib.bib53)), iSUN (Xiao
    et al., [2010](#bib.bib182)), LSUN (Yu et al., [2015](#bib.bib185)), CelebA (Liu
    et al., [2015](#bib.bib111)), ImageNet (Deng et al., [2009](#bib.bib30)) and TinyImagenet
    (Bastidas, [2017](#bib.bib7)) for OOD samples. Regression image datasets include
    for instance the NYU Depth Estimation v2 dataset (Silberman et al., [2012](#bib.bib152)),
    using ApolloScape (Huang et al., [2018](#bib.bib73)) or KITTI (Menze & Geiger,
    [2015](#bib.bib124)) as an OOD dataset. Many authors also illustrate model uncertainty
    on synthetic data, for instance by simulating clusters of data points using Gaussians
    (Malinin & Gales, [2018](#bib.bib115); [2019](#bib.bib116); Nandy et al., [2020](#bib.bib133);
    Zhao et al., [2019](#bib.bib188); Hu et al., [2020](#bib.bib70); Charpentier et al.,
    [2020](#bib.bib16); [2022](#bib.bib17)), spiral data (Malinin et al., [2020b](#bib.bib118))
    or polynomials for regression (Amini et al., [2020](#bib.bib1); Oh & Shin, [2022](#bib.bib136);
    Meinert & Lavin, [2021](#bib.bib123); Malinin et al., [2020a](#bib.bib117); Charpentier
    et al., [2022](#bib.bib17)). Tabular datasets include the Segment dataset, predicting
    image segments based on pixel features (Dua et al., [2017](#bib.bib34)), and the
    sensorless drive dataset (Dua et al., [2017](#bib.bib34); Paschke et al., [2013](#bib.bib139)),
    describing the maintenance state of electric current drives as well as popular
    regression datasets included in the UCI regression benchmark used by Hernández-Lobato
    & Adams ([2015](#bib.bib65)); Gal & Ghahramani ([2016](#bib.bib45)): Boston house
    prices (Harrison Jr & Rubinfeld, [1978](#bib.bib57)), concrete compression strength
    (Yeh, [1998](#bib.bib184)), energy efficiency of buildings (Tsanas & Xifara, [2012](#bib.bib158)),
    forward kinematics of an eight link robot arm (Corke, [1996](#bib.bib22)), maintenance
    of naval propulsion systems (Coraddu et al., [2016](#bib.bib21)), properties of
    protein tertiary stuctures, wine quality (Cortez et al., [2009](#bib.bib23)),
    and yacht hydrodynamics (Gerritsma et al., [1981](#bib.bib50)). Furthermore, Oh
    & Shin ([2022](#bib.bib136)) use a number of drug discovery datasets, such as
    Davis (Davis et al., [2011](#bib.bib26)), Kiba (Tang et al., [2014](#bib.bib156)),
    BindingDB (Liu et al., [2007](#bib.bib109)) and PubChem (Kim et al., [2019](#bib.bib86)).
    Biloš et al. ([2019](#bib.bib11)) are the only authors working on asynchronous
    time even prediction, and supply their own data in the form of processed stack
    exchange postings, smart home data, and car indicators. Shen et al. ([2020](#bib.bib151))
    provide the sole method on language data, and use three different concept learning
    datasets, i.e. Concept Learning (Jia et al., [2017](#bib.bib81)), Snips (Coucke
    et al., [2018](#bib.bib24)) and ATIS (Hemphill et al., [1990](#bib.bib63)), which
    contains new OOD concepts to be learned by design. For graph neural networks,
    Zhao et al. ([2020](#bib.bib189)) and Stadler et al. ([2021](#bib.bib155)) select
    data from the co-purchase datasets Amazon Computer, Amazon Photos (McAuley et al.,
    [2015](#bib.bib121)) and the CoraML (McCallum et al., [2000](#bib.bib122)), CiteSeer
    (Giles et al., [1998](#bib.bib52)) and PubMed (Namata et al., [2012](#bib.bib132)),
    Coauthors Physics (Shchur et al., [2018](#bib.bib149)), CoauthorCS (Namata et al.,
    [2012](#bib.bib132)) and OGBN Arxiv (Hu et al., [2020](#bib.bib70)) citation datasets.
    Lastly, Charpentier et al. ([2022](#bib.bib17)) use a single count prediction
    dataset concerned with predicting the number of bike rentals (Fanaee-T & Gama,
    [2014](#bib.bib38)).'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty Evaluation Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There usually are no gold labels for uncertainty estimates, which is why the
    efficacy of proposed solutions has to be evaluated in a different way. One such
    way used by almost all the surveyed works is using uncertainty estimates in a
    proxy OOD detection task: Since the model is underspecified on unseen samples
    from another distribution, it should be more uncertain. By labelling OOD samples
    as the positive and ID inputs as the negative class, we can measure the performance
    of uncertainty estimates using the area under the receiver-operator characteristic
    (AUROC) or the area under the precision-recall curve (AUPR). We can thereby characterize
    the usage of data from another dataset as a form of covariate shift, while using
    left-out classes for testing can be seen as a kind of concept shift (Moreno-Torres
    et al., [2012](#bib.bib127)). Instead of using OOD data, another approach is to
    use adversarial examples (Malinin & Gales, [2019](#bib.bib116); Tsiligkaridis,
    [2019](#bib.bib159); Sensoy et al., [2018](#bib.bib146); Hu et al., [2021](#bib.bib71);
    Chen et al., [2018](#bib.bib18); Amini et al., [2020](#bib.bib1)), checking if
    they can be identified through uncertainty. In the case of Shen et al. ([2020](#bib.bib151)),
    OOD detection or new concept extraction is the actual and not a proxy task, and
    thus can be evaluated using classical metrics such as the $F_{1}$ score. Another
    way is misclassification detection: In general, we would desire the model to be
    more uncertain about inputs it incurs a higher loss on, i.e., what it is more
    wrong about. For this purpose, some works (Malinin & Gales, [2018](#bib.bib115);
    Zhao et al., [2020](#bib.bib189); Charpentier et al., [2020](#bib.bib16)) measure
    whether let missclassified inputs be the positive class in another binary proxy
    classification test, and again measure AUROC and AUPR. Alternatively, Malinin
    et al. ([2020b](#bib.bib118)); Stadler et al. ([2021](#bib.bib155)); Amini et al.
    ([2020](#bib.bib1)) show or measure the area under the prediction / rejection
    curve, graphing how task performance varies as predictions on increasingly uncertain
    inputs is suspended. Lastly, some authors look at a model’s calibration (Guo et al.,
    [2017](#bib.bib56)): While this does not allow to judge the quality of uncertainty
    estimates themselves, quantities like the expected calibration error quantify
    to what extent the output distribution of a classifier corresponds to the true
    label distribution, and thus whether aleatoric uncertainty is accurately reflected.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Fundamental Derivations Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This appendix section walks the reader through generalized versions of recurring
    theoretical results using Dirichlet distributions in a Machine Learning context,
    such as their expectation in [Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet
    ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), their
    entropy in [Section C.2](#A3.SS2 "C.2 Entropy of Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") and the Kullback-Leibler divergence
    between two Dirichlets in [Section D.3](#A4.SS3 "D.3 𝑙_∞ Norm Derivation ‣ Appendix
    D Additional Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: C.1 Expectation of a Dirichlet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we show results for the quantities $\mathbb{E}_{#1}[\pi_{k}]$ and $\mathbb{E}_{#1}[\log\pi_{k}]$.
    For the first, we follow the derivation by Miller ([2011](#bib.bib125)). Another
    proof is given by Lin ([2016](#bib.bib107)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{#1}[\pi_{k}]$ | $\displaystyle=\int\cdots\int\pi_{k}\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k}^{\prime})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\pi_{1}\ldots
    d\pi_{K}$ |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: '| Moving $\pi_{k}^{\alpha_{k}-1}$ out of the product: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\int\cdots\int\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1+1}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\pi_{1}\ldots d\pi_{K}$ |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: '| For the next step, we define a new set of Dirichlet parameters with $\beta_{k}=\alpha_{k}+1$
    and $\forall k^{\prime}\neq k:\beta_{k^{\prime}}=\alpha_{k^{\prime}}$. For those
    new parameters, $\beta_{0}=\sum_{k}\beta_{k}=1+\alpha_{0}$. So by virtue of the
    Gamma function’s property that $\Gamma(\beta_{0})=\Gamma(\alpha_{0}+1)=\alpha_{0}\Gamma(\alpha_{0})$,
    replacing all terms in the normalization factor yields |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\int\cdots\int\frac{\alpha_{k}}{\alpha_{0}}\frac{\Gamma(\beta_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\beta_{k^{\prime}})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\beta_{k^{\prime}}-1}d\pi_{1}\ldots
    d\pi_{K}=\frac{\alpha_{k}}{\alpha_{0}}$ |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: where in the last step we obtain the final result, since the Dirichlet with
    new parameters $\beta_{k}$ must nevertheless integrate to $1$, and the integrals
    do not regard $\alpha_{k}$ or $\alpha_{0}$. For the expectation $\mathbb{E}_{#1}[\log\pi_{k}]$,
    we first rephrase the Dirichlet distribution in terms of the exponential family
    (Kupperman, [1964](#bib.bib95)). The exponential family encompasses many commonly-used
    distributions, such as the normal, exponential, Beta or Poisson, which all follow
    the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(\mathbf{x};\bm{\eta})=h(\mathbf{x})\exp\big{(}\bm{\eta}^{T}u(\mathbf{x})-A(\bm{\eta})\big{)}$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: with *natural parameters* $\bm{\eta}$, *sufficient statistic* $u(\mathbf{x})$,
    and *log-partition function* $A(\bm{\eta})$. For the Dirichlet distribution, Winn
    ([2004](#bib.bib179)) provides the sufficient statistic as $u(\bm{\pi})=[\log\bm{\pi}_{1},\ldots,\bm{\pi}_{K}]^{T}$
    and the log-partition function
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A(\bm{\alpha})=\sum_{k=1}^{K}\log\Gamma(\alpha_{k})-\log\Gamma(\alpha_{o})$
    |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: By Mao ([2019](#bib.bib119)), we also find that by the moment-generating function
    that for the sufficient statistic, its expectation can be derived by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{#1}[u(\mathbf{x})_{k}]=\frac{\partial A(\bm{\eta})}{\partial\eta_{k}}$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore we can evaluate the expected value of $\log\pi_{k}$ (i.e. the sufficient
    statistic) by inserting the definition of the log-partition function in [Equation 34](#A3.E34
    "In C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") into [Equation 35](#A3.E35 "In C.1 Expectation of a Dirichlet
    ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{#1}[\log\pi_{k}]=\frac{\partial}{\partial\alpha_{k}}\sum_{k=1}^{K}\log\Gamma(\alpha_{k})-\log\Gamma(\alpha_{0})=\psi(\alpha_{k})-\psi(\alpha_{0})$
    |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: which corresponds precisely to the definition of the digamma function as $\psi(x)=\frac{d}{dx}\log\Gamma(x)$.
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Entropy of Dirichlet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following derivation is adapted from Lin ([2016](#bib.bib107)), with the
    result stated in Charpentier et al. ([2020](#bib.bib16)) as well.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle H[\bm{\pi}]$ | $\displaystyle=-\mathbb{E}_{#1}[\log p(\bm{\pi}&#124;\bm{\alpha})]$
    |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\mathbb{E}_{#1}\bigg{[}\log\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}\Big{)}\bigg{]}$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\log B(\bm{\alpha})-\sum_{k=1}^{K}(\alpha_{k}-1)\mathbb{E}_{#1}[\log\pi_{k}]$
    |  | (40) |'
  prefs: []
  type: TYPE_TB
- en: '| Using [Equation 36](#A3.E36 "In C.1 Expectation of a Dirichlet ‣ Appendix
    C Fundamental Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation"): |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\log B(\bm{\alpha})-\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\log B(\bm{\alpha})+(\alpha_{0}-K)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: C.3 Kullback-Leibler Divergence between two Dirichlets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following result is presented using an adapted derivation by Lin ([2016](#bib.bib107))
    and appears in Chen et al. ([2018](#bib.bib18)) and Joo et al. ([2020](#bib.bib82))
    as a starting point for their variational objective (see [Section D.7](#A4.SS7
    "D.7 Evidence-Lower Bound For Dirichlet Posterior Estimation ‣ Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")). In the following we use $\text{Dir}(\bm{\pi};\bm{\alpha})$
    to denote the optimized distribution, and $\text{Dir}(\bm{\pi};\bm{\gamma})$ the
    reference or target distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{\gamma})\Big{]}=$
    | $\displaystyle\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\bm{\alpha})}{p(\bm{\pi}&#124;\bm{\gamma})}\bigg{]}=\mathbb{E}_{#1}\bigg{[}\log
    p(\bm{\pi}&#124;\bm{\alpha})\bigg{]}-\mathbb{E}_{#1}\bigg{[}\log p(\bm{\pi}&#124;\bm{\gamma})\bigg{]}$
    |  | (44) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}\bigg{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle-$ | $\displaystyle\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\gamma})+\sum_{k=1}^{K}(\gamma_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: '| Distributing and pulling out $B(\bm{\alpha})$ and $B(\bm{\gamma})$ out of
    the expectation (they don’t depend on $\bm{\pi}$): |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}-(\gamma_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\alpha_{k}-\gamma_{k})\log\pi_{k}\bigg{]}$
    |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: '| Moving the expectation inward and using the identity $\mathbb{E}_{#1}[\pi_{k}]=\psi(\alpha_{k})-\psi(\alpha_{0})$
    from [Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"): |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\gamma_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: The KL divergence is also used by some works as regularizer by penalizing the
    distance to a uniform Dirichlet with $\bm{\gamma}=\mathbf{1}$ (Sensoy et al.,
    [2018](#bib.bib146)). In this case, the result above can be derived to be
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{1})\Big{]}=\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (49) |'
  prefs: []
  type: TYPE_TB
- en: where the $\log\Gamma(K)$ term can also be omitted for optimization purposes,
    since it does not depend on $\bm{\alpha}$.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Additional Derivations Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this appendix we present relevant results in a Machine Learning context,
    including from some of the surveyed works, featuring as unified notation and annotated
    derivation steps. These include derivations of expected entropy ([Section D.1](#A4.SS1
    "D.1 Derivation of Expected Entropy ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")) and mutual information ([Section D.2](#A4.SS2 "D.2 Derivation
    of Mutual Information ‣ Appendix D Additional Derivations Appendix ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")) as uncertainty metrics for Dirichlet networks. Also, we derive a
    multitude of loss functions, including the $l_{\infty}$ norm loss of a Dirichlet
    w.r.t. a one-hot encoded class label in [Section D.3](#A4.SS3 "D.3 𝑙_∞ Norm Derivation
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), the $l_{2}$
    norm loss in [Section D.4](#A4.SS4 "D.4 𝑙₂ Norm Loss Derivation ‣ Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"), as well as the reverse KL loss
    by Malinin & Gales ([2019](#bib.bib116)), the UCE objective Biloš et al. ([2019](#bib.bib11));
    Charpentier et al. ([2020](#bib.bib16)) and ELBO Shen et al. ([2020](#bib.bib151));
    Chen et al. ([2018](#bib.bib18)) as training objectives ([Sections D.5](#A4.SS5
    "D.5 Derivation of Reverse KL loss ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"), [D.6](#A4.SS6 "D.6 Uncertainty-aware Cross-Entropy Loss
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation") and [D.7](#A4.SS7
    "D.7 Evidence-Lower Bound For Dirichlet Posterior Estimation ‣ Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")).'
  prefs: []
  type: TYPE_NORMAL
- en: D.1 Derivation of Expected Entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following derivation is adapted from Malinin & Gales ([2018](#bib.bib115))
    appendix section C.4\. In the following, we assume that $\forall k\in\mathbb{K}:\pi_{k}>0$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}=\int
    p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})\bigg{(}-\sum_{k=1}^{K}\pi_{k}\log\pi_{k}\bigg{)}d\bm{\pi}$
    |  | (50) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\int p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})\Big{(}\pi_{k}\log\pi_{k}\Big{)}d\bm{\pi}$
    |  | (51) |'
  prefs: []
  type: TYPE_TB
- en: '| Inserting the definition of $p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})\approx
    p(\bm{\pi}&#124;\mathbf{x},\mathbb{D})$: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k^{\prime}})}\int\pi_{k}\log\pi_{k}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$
    |  | (52) |'
  prefs: []
  type: TYPE_TB
- en: '| Singling out the factor $\pi_{k}$: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{k})\prod_{k^{\prime}\neq
    k}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1}\int\pi_{k}\log\pi_{k}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$ |  | (53) |'
  prefs: []
  type: TYPE_TB
- en: '| Adjusting the normalizing constant (this is the same trick used in [Section C.1](#A3.SS1
    "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")): |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\alpha_{k}}{\alpha_{0}}\int\frac{\Gamma(\alpha_{0}+1)}{\Gamma(\alpha_{k}+1)\prod_{k^{\prime}\neq
    k}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1}\log\pi_{k}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$ |  | (54) |'
  prefs: []
  type: TYPE_TB
- en: '| Using the identity $\mathbb{E}_{#1}[\log\pi_{k}]=\psi(\alpha_{k})-\psi(\alpha_{0})$
    ([Equation 36](#A3.E36 "In C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")). Since the expectation here is
    w.r.t to a Dirichlet with concentration parameters $\alpha_{k}+1$, we obtain |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (55) |'
  prefs: []
  type: TYPE_TB
- en: D.2 Derivation of Mutual Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start from the expression in [Equation 13](#S3.E13 "In Distributional uncertainty
    ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}$
    | $\displaystyle=H\bigg{[}\mathbb{E}_{#1}\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}-\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}$
    |  | (56) |'
  prefs: []
  type: TYPE_TB
- en: '| Given that $\mathbb{E}_{#1}[\pi_{k}]=\frac{\alpha_{k}}{\alpha_{0}}$ ([Section C.1](#A3.SS1
    "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")) and assuming that point estimate $p(\bm{\pi}&#124;\mathbf{x},\mathbb{D})\approx
    p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})$ is sufficient (Malinin & Gales,
    [2018](#bib.bib115)), we can identify the first term as the Shannon entropy $-\sum_{k=1}^{K}\pi_{k}\log\pi_{k}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\log\frac{\alpha_{k}}{\alpha_{0}}$.
    Furthermore, the second part we already derived in [Section D.1](#A4.SS1 "D.1
    Derivation of Expected Entropy ‣ Appendix D Additional Derivations Appendix ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and thus we obtain: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\log\frac{\alpha_{k}}{\alpha_{0}}+\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (57) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\log\frac{\alpha_{k}}{\alpha_{0}}-\psi(\alpha_{k}+1)+\psi(\alpha_{0}+1)\bigg{)}$
    |  | (58) |'
  prefs: []
  type: TYPE_TB
- en: D.3 $l_{\infty}$ Norm Derivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we elaborate on the derivation of Tsiligkaridis ([2019](#bib.bib159))
    deriving a generalized $l_{p}$ loss, upper-bounding the $l_{\infty}$ loss. This
    in turn allows us to easily derive the $l_{2}$ loss used by Sensoy et al. ([2018](#bib.bib146));
    Zhao et al. ([2020](#bib.bib189)). Here we assume the classification target $y$
    is provided in the form of a one-hot encoded label $\mathbf{y}=[\mathbf{1}_{y=1},\ldots,\mathbf{1}_{y=K}]^{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{\infty}\big{]}$
    | $\displaystyle\leq\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}\big{]}$
    |  | (59) |'
  prefs: []
  type: TYPE_TB
- en: '| Using Jensen’s inequality |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\Big{(}\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}^{p}\big{]}\Big{)}^{1/p}$
    |  | (60) |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluating the expression with $\forall k\neq y:\mathbf{y}_{k}=0$: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\Big{(}\mathbb{E}_{#1}[(1-\pi_{y})^{p}]+\sum_{k\neq
    y}\mathbb{E}_{#1}[\pi_{k}^{p}]\Big{)}^{1/p}$ |  | (61) |'
  prefs: []
  type: TYPE_TB
- en: 'In order to compute the expression above, we first realize that all components
    of $\pi$ are distributed according to a Beta distribution $\text{Beta}(\alpha,\beta)$
    (since the Dirichlet is a multivariate generalization of the beta distribution)
    for which the moment-generating function is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{#1}[\pi^{p}]=\frac{\Gamma(\alpha+p)\Gamma(\beta)\Gamma(\alpha+\beta)}{\Gamma(\alpha+p+\beta)\Gamma(\alpha)\Gamma(\beta)}=\frac{\Gamma(\alpha+p)\Gamma(\alpha+\beta)}{\Gamma(\alpha+p+\beta)\Gamma(\alpha)}$
    |  | (62) |'
  prefs: []
  type: TYPE_TB
- en: 'Given that the first term in [Equation 59](#A4.E59 "In D.3 𝑙_∞ Norm Derivation
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation") is characterized
    by $\text{Beta}(\alpha_{0}-\alpha_{y},\alpha_{y})$ and the second one by $\text{Beta}(\alpha_{k},\alpha_{0}-\alpha_{k})$,
    we can evaluate the result in [Equation 59](#A4.E59 "In D.3 𝑙_∞ Norm Derivation
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation") using
    the moment generating function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{\infty}\Big{]}$
    | $\displaystyle\leq\Bigg{(}\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)\Gamma(\alpha_{0}-\cancel{\alpha_{y}}+\cancel{\alpha_{y}})}{\Gamma(\alpha_{0}-\cancel{\alpha_{y}}+p+\cancel{\alpha_{y})}\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)\Gamma(\cancel{\alpha_{k}}+\alpha_{0}-\cancel{\alpha_{k}})}{\Gamma(\cancel{\alpha_{k}}+p+\alpha_{0}-\cancel{\alpha_{k}})\Gamma(\alpha_{k})}\Bigg{)}^{\frac{1}{p}}$
    |  | (63) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\Bigg{(}\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)\Gamma(\alpha_{0})}{\Gamma(p+\alpha_{0})\Gamma(\alpha_{k})}\Bigg{)}^{\frac{1}{p}}$
    |  | (64) |'
  prefs: []
  type: TYPE_TB
- en: '| Factoring out common terms: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\bBigg@{4}(\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bBigg@{4}(\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)}{\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})\bBigg@{4})^{\frac{1}{p}}$
    |  | (65) |'
  prefs: []
  type: TYPE_TB
- en: '| Expressing $\alpha_{0}-\alpha_{k}=\sum_{k\neq y}\alpha_{k}$: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\bBigg@{4}(\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})^{\frac{1}{p}}$ |  |
    (66) |'
  prefs: []
  type: TYPE_TB
- en: D.4 $l_{2}$ Norm Loss Derivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we present an adapted derivation by Sensoy et al. ([2018](#bib.bib146))
    for the $l_{2}$-norm loss to train Dirichlet networks. Here we again use a one-hot
    vector for a label with $\mathbf{y}=[\mathbf{1}_{y=1},\ldots,\mathbf{1}_{y=K}]^{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{2}^{2}\Big{]}$
    | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\mathbf{1}_{y=k}-\pi_{k})^{2}\bigg{]}$
    |  | (67) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\pi_{k}\mathbf{1}_{y=k}+\pi_{k}^{2}\bigg{]}$
    |  | (68) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\mathbb{E}_{#1}[\pi_{k}]\mathbf{1}_{y=k}+\mathbb{E}_{#1}[\pi_{k}^{2}]$
    |  | (69) |'
  prefs: []
  type: TYPE_TB
- en: '| Using the identity that $\mathbb{E}_{#1}[\pi_{k}^{2}]=\mathbb{E}_{#1}[\pi_{k}]^{2}+\text{Var}(\pi_{k})$:
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\mathbb{E}_{#1}[\pi_{k}]\mathbf{1}_{y=k}+\mathbb{E}_{#1}[\pi_{k}]^{2}+\text{Var}(\pi_{k})$
    |  | (70) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\mathbb{E}_{#1}[\pi_{k}]\Big{)}^{2}+\text{Var}(\pi_{k})$
    |  | (71) |'
  prefs: []
  type: TYPE_TB
- en: '| Finally, we use the result from [Section C.1](#A3.SS1 "C.1 Expectation of
    a Dirichlet ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    and the result that $\displaystyle\text{Var}(\pi_{k})=\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    (see [Lin](#bib.bib107), [2016](#bib.bib107)): |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    |  | (72) |'
  prefs: []
  type: TYPE_TB
- en: D.5 Derivation of Reverse KL loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we re-state and annotate the derivation of reverse KL loss by Malinin &
    Gales ([2019](#bib.bib116)) in more detail, starting from the forward KL loss
    by Malinin & Gales ([2018](#bib.bib115)). Note that here, $\hat{\bm{\alpha}}$
    contains a dependence on $k$, since Malinin & Gales ([2018](#bib.bib115)) let
    $\hat{\alpha}_{k}=\hat{\pi_{k}}\hat{\alpha}_{0}$ with $\hat{\alpha}_{0}$ being
    a hyperparameter and $\hat{\pi}_{k}=\mathbf{1}_{k=y}+(-\mathbf{1}_{k=y}K+1)\varepsilon$
    and $\varepsilon$ being a small number.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\quad\ \mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}\text{KL}\Big{[}p(\bm{\pi}&#124;\hat{\bm{\alpha}})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\Big{]}\bigg{]}$
    |  | (73) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}d\bm{\pi}\bigg{]}$
    |  | (74) |'
  prefs: []
  type: TYPE_TB
- en: '| Writing the expectation explicitly: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\int\sum_{k=1}^{K}p(y=k,\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}d\bm{\pi}d\mathbf{x}$
    |  | (75) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\int\sum_{k=1}^{K}p(\mathbf{x})P(y=k&#124;\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}d\bm{\pi}d\mathbf{x}$
    |  | (76) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}d\bm{\pi}\Bigg{]}$
    |  | (77) |'
  prefs: []
  type: TYPE_TB
- en: '| Adding factor in log, collapsing double sum: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\Bigg{(}\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}\Bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (78) |'
  prefs: []
  type: TYPE_TB
- en: '| Reordering, separating constant factor from log: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{p(\mathbf{x})}\Bigg{[}\int\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\bm{\alpha}})\Bigg{(}\log\bigg{(}\frac{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}\bigg{)}$
    |  | (79) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\underbrace{\log\Big{(}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\Big{)}}_{=0}\Bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (80) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\text{KL}\bigg{[}\underbrace{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\alpha})}_{\text{Mixture
    of }K\text{ Dirichlets}}\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{]}\Bigg{]}$
    |  | (81) |'
  prefs: []
  type: TYPE_TB
- en: 'where we can see that this objective actually tries to minimizes the divergence
    towards a mixture of $K$ Dirichlet distributions. In the case of high data uncertainty,
    this is claimed incentivize the model to distribute mass around each of the corners
    of the simplex, instead of the desired behavior shown in [Figure 4(c)](#S3.F4.sf3
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"). Therefore, Malinin & Gales
    ([2019](#bib.bib116)) propose to swap the order of arguments in the KL-divergence,
    resulting in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\quad\ \mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\cdot\text{KL}\Big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\hat{\bm{\alpha}})\Big{]}\bigg{]}$
    |  | (82) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\cdot\int
    p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}d\bm{\pi}\bigg{]}$
    |  | (83) |'
  prefs: []
  type: TYPE_TB
- en: '| Reordering: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}d\bm{\pi}\bigg{]}$
    |  | (84) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log
    p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})-\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\bigg{]}\bigg{]}$ |  | (85) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\bigg{(}\prod_{k=1}^{K}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})^{P(y=k&#124;\mathbf{x})}\bigg{)}-\log\bigg{(}\prod_{k=1}^{K}p(\bm{\pi}&#124;\hat{\bm{\alpha}})^{P(y=k&#124;\mathbf{x})}\bigg{)}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (86) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{p(\mathbf{x})}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\bigg{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})^{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}\bigg{)}-\log\bigg{(}\prod_{k=1}^{K}\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}\Big{)}^{p(y=k&#124;\mathbf{x})}\bigg{)}\bigg{)}d\bm{\pi}\bigg{]}$
    |  | (87) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\big{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{)}-\log\bigg{(}\prod_{k=1}^{K}\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}\Big{)}^{P(y=k&#124;\mathbf{x})}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (88) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\big{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{)}-\log\bigg{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\alpha_{k^{\prime}}-1}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (89) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\text{KL}\Big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})&#124;&#124;p(\bm{\pi}&#124;\bar{\bm{\alpha}})\Big{]}\Bigg{]}\quad\text{where}\quad\bar{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\alpha_{k^{\prime}}$
    |  | (90) |'
  prefs: []
  type: TYPE_TB
- en: Therefore, instead of a mixture of Dirichlet distribution, we obtain a single
    distribution whose *parameters are a mixture* of the concentrations of each class.
  prefs: []
  type: TYPE_NORMAL
- en: D.6 Uncertainty-aware Cross-Entropy Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The uncertainty-aware cross-entropy loss in Biloš et al. ([2019](#bib.bib11));
    Charpentier et al. ([2020](#bib.bib16)) has the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{UCE}}=\mathbb{E}_{#1}[\log p(y&#124;\bm{\pi})]=\mathbb{E}_{#1}[\log\pi_{y}]=\psi(\alpha_{y})-\psi(\alpha_{0})$
    |  | (91) |'
  prefs: []
  type: TYPE_TB
- en: 'as $p(y|\bm{\pi})$ is given by the true label in form of a delta distribution,
    we can apply the result from [Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet
    ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  prefs: []
  type: TYPE_NORMAL
- en: D.7 Evidence-Lower Bound For Dirichlet Posterior Estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evidence lower bound is a well-known objective to optimize the KL-divergence
    between an approximate proposal and target distribution (Jordan et al., [1999](#bib.bib83);
    Kingma & Welling, [2014](#bib.bib88)). We derive it based on Chen et al. ([2018](#bib.bib18))
    in the following for the Dirichlet case with a proposal distribution $p(\bm{\pi}|\mathbf{x},\bm{\theta})$
    to the target distribution $p(\bm{\pi}|y)$. For the first part of the derivation,
    we omit the dependence on $\bm{\beta}$ for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{KL}\big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{&#124;}\big{&#124;}p(\bm{\pi}&#124;y)\big{]}$
    | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;y)}\bigg{]}=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})p(y)}{p(\bm{\pi},y)}\bigg{]}$
    |  | (92) |'
  prefs: []
  type: TYPE_TB
- en: '| Factorizing $p(\bm{\pi},y)=P(y&#124;\bm{\pi})p(\bm{\pi})$, pulling out $p(y)$
    as it doesn’t depend on $\pi$: |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{P(y&#124;\bm{\pi})p(\bm{\pi})}\bigg{]}+p(y)$
    |  | (93) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi})}\bigg{]}-\mathbb{E}_{#1}\big{[}\log
    P(y&#124;\bm{\pi})\big{]}+p(y)$ |  | (94) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\leq\text{KL}\big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{&#124;}\big{&#124;}p(\bm{\pi})\big{]}-\mathbb{E}_{#1}\big{[}\log
    P(y&#124;\bm{\pi})\big{]}$ |  | (95) |'
  prefs: []
  type: TYPE_TB
- en: 'Now note that the second part of the result is the uncertainty-aware cross-entropy
    loss from [Section D.6](#A4.SS6 "D.6 Uncertainty-aware Cross-Entropy Loss ‣ Appendix
    D Additional Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation") and re-adding the
    dependence of $p(\pi)$ on $\bm{\gamma}$, we can re-use our result regarding the
    KL-divergence between two Dirichlets in [Section C.3](#A3.SS3 "C.3 Kullback-Leibler
    Divergence between two Dirichlets ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and thus obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{ELBO}}$ | $\displaystyle=\psi(\beta_{y})-\psi(\beta_{0})-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  | (96) |'
  prefs: []
  type: TYPE_TB
- en: which is exactly the solution obtained by both Chen et al. ([2018](#bib.bib18))
    and Joo et al. ([2020](#bib.bib82)).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Overview over Loss Functions Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Tables 6](#A5.T6 "In Appendix E Overview over Loss Functions Appendix ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and [7](#A5.T7 "Table 7 ‣ Appendix E Overview over Loss
    Functions Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"), we compare the forms of the loss
    function used by Evidential Deep Learning methods for classification, using the
    consistent notation from the paper. Most of the presented results can be found
    in the previous [Appendix C](#A3 "Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and [Appendix D](#A4 "Appendix D Additional Derivations
    Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"). We refer to the original work for details
    about the objective of Nandy et al. ([2020](#bib.bib133)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Overview over objectives used by prior networks for classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Loss function | Regularizer | Comment |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks (Malinin & Gales, [2018](#bib.bib115)) | $\log\frac{B(\hat{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)(\psi(\alpha_{k})-\psi(\alpha_{0}))$
    | Target concentration parameters $\hat{\bm{\alpha}}$ are created using a label
    smoothing approach, i.e. $\hat{\pi}_{k}=\begin{cases}1-(K-1)\varepsilon&amp;\quad\text{if
    }y=k\\ \varepsilon&amp;\quad\text{if }y\neq k\end{cases}$. Together with setting
    $\hat{\alpha}_{0}$ as a hyperparameter, $\hat{\alpha_{k}}=\hat{\pi_{k}}\hat{\alpha_{0}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks (Malinin & Gales, [2019](#bib.bib116)) | $\log\frac{B(\hat{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $\log\frac{B(\bar{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\bar{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | Similar to above, $\hat{\alpha}_{c}^{(k)}=\mathbf{1}_{c=k}\alpha_{\text{in}}+1$
    for in-distribution and $\bar{\alpha}_{c}^{(k)}=\mathbf{1}_{c=k}\alpha_{\text{out}}+1$
    where we have hyperparameters set to $\alpha_{\text{in}}=0.01$ and $\alpha_{\text{out}}=0$.
    Then finally, $\hat{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\hat{\alpha}_{k}$
    and $\bar{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\bar{\alpha}_{k}$.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Information Robust Dirichlet Networks (Tsiligkaridis, [2019](#bib.bib159))
    | $\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\bBigg@{4}(\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})^{\frac{1}{p}}$ |
    $\frac{1}{2}\sum_{k\neq y}(\alpha_{k}-1)^{2}(\psi^{(1)}(\alpha_{k})-\psi^{(1)})(\alpha_{0}))$
    | $\psi^{(1)}$ is the polygamma function defined as $\psi^{(1)}(x)=\frac{d}{dx}\psi(x)$.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dirichlet via Function Decomposition (Biloš et al., [2019](#bib.bib11)) |
    $\psi(\alpha_{y})-\psi(\alpha_{0})$ | $\lambda_{1}\int_{0}^{T}\pi_{k}(\tau)^{2}d\tau+\lambda_{2}\int_{0}^{T}(\nu-\sigma^{2}(\tau))^{2}d\tau$
    | Factors $\lambda_{1}$ and $\lambda_{2}$ that are treated as hyperparameters
    that weigh first term pushing the for logit $k$ to zero, while pushing the variance
    in the first term to $\nu$. |'
  prefs: []
  type: TYPE_TB
- en: '| Prior network with PAC Reg. (Haussmann et al., [2019](#bib.bib58)) | $-\log\mathbb{E}_{#1}\bigg{[}\prod_{k=1}^{K}\bigg{(}\frac{\alpha_{k}}{\alpha_{0}}\bigg{)}^{\mathbf{1}_{k=y}}\bigg{]}$
    | $\sqrt{\frac{\text{KL}\big{[}p(\bm{\pi}&#124;\bm{\alpha})\big{&#124;}\big{&#124;}p(\bm{\pi}&#124;\mathbf{1})\big{]}-\log\delta}{N}-1}$
    | The expectation in the loss function is evaluated using parameter samples from
    a weight distribution. $\delta\in[0,1]$. |'
  prefs: []
  type: TYPE_TB
- en: '| Ensemble Distribution Distillation (Malinin et al., [2020b](#bib.bib118))
    | $\psi(\alpha_{0})-\sum_{k=1}^{K}\psi(\alpha_{k})+\frac{1}{M}\sum_{m=1}^{M}\sum_{k=1}^{K}(\alpha_{k}-1)$
    $\log p(y=k&#124;\mathbf{x},\bm{\theta}^{(m)})$ | - | The objective uses predictions
    from a trained ensemble with parameters $\bm{\theta}_{1},\ldots,\bm{\theta}_{M}$.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prior networks with representation gap (Nandy et al., [2020](#bib.bib133))
    | $-\log\pi_{y}-\frac{\lambda_{\text{in}}}{K}\sum_{k=1}^{K}\sigma(\alpha_{k})$
    | $-\sum_{k=1}\frac{1}{K}\log\pi_{k}-\frac{\lambda_{\text{out}}}{K}\sum_{k=1}^{K}\sigma(\alpha_{k})$
    | The main objective is being optimized on in-distribution, the regularizer on
    out-of-distribution data. $\lambda_{\text{in}}$ and $\lambda_{\text{out}}$ weighing
    terms and $\sigma$ denotes the sigmoid function. |'
  prefs: []
  type: TYPE_TB
- en: '| Prior RNN (Shen et al., [2020](#bib.bib151)) | $\sum_{k=1}\mathbf{1}_{k=y}\log\pi_{k}$
    | $-\log B(\tilde{\bm{\alpha})}+(\hat{\alpha}_{0}-K)\psi(\hat{\alpha}_{0})-\sum_{k=1}^{K}(\hat{\alpha}_{k}-1)\psi(\hat{\alpha}_{k})$
    | Here, the entropy regularizer operates on a scaled version of the concentration
    parameters $\tilde{\bm{\alpha}}=(\mathbf{I}_{K}-\mathbf{W})\bm{\alpha}$, where
    $\mathbf{W}$ is learned. |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-based Kernel Dirichlet dist. est. (GKDE) (Zhao et al., [2020](#bib.bib189))
    | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    | $-\log\frac{B(\bm{\alpha})}{B(\hat{\bm{\alpha}})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $\hat{\bm{\alpha}}$ here corresponds to a uniform prior including some information
    about the local graph structure. The authors also use an additional knowledge
    distillation objective, which was omitted here since it doesn’t related to the
    Dirichlet. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Overview over objectives used by posterior networks for classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Loss function | Regularizer | Comment |'
  prefs: []
  type: TYPE_TB
- en: '| Evidential Deep Learning (Sensoy et al., [2018](#bib.bib146)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\beta})}+\sum_{k=1}^{K}(\beta_{k}-1)(\psi(\beta_{k})-\psi(\beta_{0}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Variational Dirichlet (Chen et al., [2018](#bib.bib18)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Regularized ENN Zhao et al. ([2019](#bib.bib188)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\lambda_{1}\mathbb{E}_{#1}\Big{[}\frac{\alpha_{y}}{\alpha_{0}}\Big{]}-\lambda_{2}\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}\bigg{(}\frac{\beta_{k}\sum_{k^{\prime}\neq
    k}\beta_{k^{\prime}}\big{(}1-\frac{&#124;\beta_{k^{\prime}}-\beta_{k}&#124;}{\beta_{k^{\prime}}+\beta_{k}}\big{)}}{\sum_{k^{\prime}\neq
    k}\beta_{k^{\prime}}}\bigg{)}\Bigg{]}$ | The first term represents *vacuity*,
    i.e. the lack of evidence and is optimized using OOD examples. The second term
    stands for *dissonance*, and is computed using points with neighborhoods with
    different classes from their own. $\lambda_{1},\lambda_{2}$ are hyperparameters.
    |'
  prefs: []
  type: TYPE_TB
- en: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\lambda\mathbb{E}_{#1}\Big{[}\frac{\alpha_{y}}{\alpha_{0}}\Big{]}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Belief Matching (Joo et al., [2020](#bib.bib82)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Posterior networks (Charpentier et al., [2020](#bib.bib16)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log B(\bm{\beta})+(\beta_{0}-K)\psi(\beta_{0})-\sum_{k=1}^{K}(\beta_{k}-1)\psi(\beta_{k})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Graph Posterior Networks (Stadler et al., [2021](#bib.bib155)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log B(\bm{\beta})+(\beta_{0}-K)\psi(\beta_{0})-\sum_{k=1}^{K}(\beta_{k}-1)\psi(\beta_{k})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Evidential Neural Network (Sensoy et al., [2020](#bib.bib147))
    | $-\sum_{k=1}^{K}\bigg{(}\mathbb{E}_{#1}\big{[}\log(\sigma(f_{\bm{\theta}}(\mathbf{x})))\big{]}+\mathbb{E}_{#1}\big{[}\log(1-\sigma(f_{\bm{\theta}}(\mathbf{x})))\big{]}\bigg{)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\beta}_{-y})}+\sum_{k\neq y}(\beta_{k}-1)(\psi(\beta_{k})-\psi(\beta_{0}))$
    | The main loss is a discriminative loss using ID and OOD samples, generated by
    a VAE. The regularizer is taken over all classes *excluding* the true class $y$
    (also indicated by $\bm{\beta}_{-y}$). |'
  prefs: []
  type: TYPE_TB
