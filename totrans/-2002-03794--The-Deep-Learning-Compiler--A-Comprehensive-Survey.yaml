- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:02:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2002.03794] The Deep Learning Compiler: A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.03794](https://ar5iv.labs.arxiv.org/html/2002.03794)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The Deep Learning Compiler: A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mingzhen Li^∗ ,  Yi Liu^∗ ,  Xiaoyan Liu^∗ ,  Qingxiao Sun^∗ ,  Xin You^∗ , 
    Hailong Yang^∗^† ,  Zhongzhi Luan^∗ ,  Lin Gan^§ ,  Guangwen Yang^§  and  Depei
    Qian^∗ Beihang University^∗ Tsinghua University^§ [lmzhhh, yi.liu, liuxiaoyan,
    sunqingxiao, youxin2015, hailong.yang, zhongzhi.luan, depeiq@buaa.edu.cn](mailto:lmzhhh,%20yi.liu,%20liuxiaoyan,%20sunqingxiao,%20youxin2015,%20hailong.yang,%20zhongzhi.luan,%20depeiq@buaa.edu.cn)
    [lingan, ygw@tsinghua.edu.cn](mailto:lingan,%20ygw@tsinghua.edu.cn)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The difficulty of deploying various deep learning (DL) models on diverse DL
    hardware has boosted the research and development of DL compilers in the community.
    Several DL compilers have been proposed from both industry and academia such as
    Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described
    in different DL frameworks as input, and then generate optimized codes for diverse
    DL hardware as output. However, none of the existing survey has analyzed the unique
    design architecture of the DL compilers comprehensively. In this paper, we perform
    a comprehensive survey of existing DL compilers by dissecting the commonly adopted
    design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend
    optimizations. We present detailed analysis on the design of multi-level IRs and
    illustrate the commonly adopted optimization techniques. Finally, several insights
    are highlighted as the potential research directions of DL compiler. This is the
    first survey paper focusing on the design architecture of DL compilers, which
    we hope can pave the road for future research towards DL compiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Networks, Deep Learning, Compiler, Intermediate Representation, Optimization^†Corresponding
    author.^†^†copyright: none'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of deep learning (DL) has generated profound impact on various
    scientific fields. It has not only demonstrated remarkable value in artificial
    intelligence such as natural language processing (NLP) (Manning et al., [1999](#bib.bib65))
    and computer vision (CV) (Forsyth and Ponce, [2002](#bib.bib27)), but also proved
    great success in broader applications such as e-commerce (Ha et al., [2016](#bib.bib37)),
    smart city (Mohammadi et al., [2017](#bib.bib69)) and drug discovery (Chen et al.,
    [2018a](#bib.bib16)). With the emergence of versatile deep learning models such
    as convolutional neural network (CNN) (LeCun et al., [1998](#bib.bib55)), recurrent
    neural network (RNN) (Rumelhart et al., [1986](#bib.bib81)), long short-term memory
    (LSTM) (Hochreiter and Schmidhuber, [1997](#bib.bib39)) and generative adversarial
    network (GAN) (Goodfellow et al., [2014](#bib.bib30)), it is critical to ease
    the programming of diverse DL models in order to realize their widely adoption.
  prefs: []
  type: TYPE_NORMAL
- en: With the continuous efforts from both industry and academia, several popular
    DL frameworks have been proposed such as TensorFlow (Abadi et al., [2016](#bib.bib2)),
    PyTorch (Paszke et al., [2019](#bib.bib76)), MXNet (Chen et al., [2015](#bib.bib17))
    and CNTK (Seide and Agarwal, [2016](#bib.bib82)), in order to simplify the implementation
    of various DL models. Although there are strengths and weaknesses among the above
    DL frameworks depending on the tradeoffs in their designs, the interoperability
    becomes important to reduce the redundant engineering efforts when supporting
    emerging DL models across the existing DL models. To provide interoperability,
    ONNX (Microsoft, [2017](#bib.bib67)) has been proposed, that defines a unified
    format for representing DL models to facilitate model conversion between different
    DL frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the meanwhile, the unique computing characteristics such as matrix multiplication
    have spurred the passion of chip architects to design customized DL accelerators
    for higher efficiency. Internet giants (e.g., Google TPU (Jouppi et al., [2017](#bib.bib45)),
    Hisilicon NPU (Liao et al., [2019](#bib.bib57)), Apple Bonic (Kingsley-Hughes,
    [2017](#bib.bib50))), processor vendors (e.g., NVIDIA Turing (NVIDIA, [2019b](#bib.bib73)),
    Intel NNP (Intel, [2019](#bib.bib42))), service providers (e.g., Amazon Inferentia (Amazon,
    [2018](#bib.bib9)), Alibaba Hanguang (Alibaba, [2019](#bib.bib8))), and even startups
    (e.g., Cambricon (Liu et al., [2016](#bib.bib58)), Graphcore (Jia et al., [2019](#bib.bib44)))
    are investing tremendous workforce and capital in developing DL chips in order
    to boost the performance for DL models. Generally, the DL hardware can be divided
    into the following categories: 1) general-purpose hardware with software-hardware
    co-design, 2) dedicated hardware fully customized for DL models, and 3) neuromorphic
    hardware inspired by biological brain science. For example, the general-purpose
    hardware (e.g., CPU, GPU) has added special hardware components such as AVX512
    vector units and tensor core to accelerate DL models. Whereas for dedicated hardware
    such as Google TPU, application-specific integrated circuits (e.g., matrix multiplication
    engine and high-bandwidth memory) have been designed to elevate the performance
    and energy efficiency to extreme. To the foreseeable future, the design of DL
    hardware would become even more diverse.'
  prefs: []
  type: TYPE_NORMAL
- en: To embrace the hardware diversity, it is important to map the computation to
    DL hardware efficiently. On general-purpose hardware, the highly optimized linear
    algebra libraries such as Basic Linear Algebra Subprograms (BLAS) libraries (e.g.,
    MKL and cuBLAS) serve as the basics for efficient computation of DL models. Take
    the convolution operation for example, the DL frameworks convert the convolution
    to matrix multiplication and then invoke the GEMM function in the BLAS libraries.
    In addition, the hardware vendors have released specially optimized libraries
    tailored for DL computations (e.g., MKL-DNN and cuDNN), including forward and
    backward convolution, pooling, normalization, and activation. More advanced tools
    have also been developed to further speedup the DL operations. For example, TensorRT (NVIDIA,
    [2019c](#bib.bib74)) supports graph optimization (e.g., layer fusion) and low-bit
    quantization with large collection of highly optimized GPU kernels. On dedicated
    DL hardware, similar libraries are also provided (Liu et al., [2016](#bib.bib58);
    Jia et al., [2019](#bib.bib44)). However, the drawback of relying on the libraries
    is that they usually fall behind the rapid development of DL models, and thus
    fail to utilize the DL chips efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: To address the drawback of DL libraries and tools, as well as alleviate the
    burden of optimizing the DL models on each DL hardware manually, the DL community
    has resorted to the domain specific compilers for rescue. Rapidly, several popular
    DL compilers have been proposed such as TVM (Chen et al., [2018b](#bib.bib18)),
    Tensor Comprehension (Vasilache et al., [2018](#bib.bib92)), Glow (Rotem et al.,
    [2018](#bib.bib80)), nGraph (Cyphers et al., [2018](#bib.bib22)) and XLA (Leary
    and Wang, [2017](#bib.bib54)), from both industry and academia. The DL compilers
    take the model definitions described in the DL frameworks as inputs, and generate
    efficient code implementations on various DL hardware as outputs. The transformation
    between model definition and specific code implementation are highly optimized
    targeting the model specification and hardware architecture. Specifically, they
    incorporate DL oriented optimizations such as layer and operator fusion, which
    enables highly efficient code generation. Moreover, existing DL compilers also
    leverage mature tool-chains from general-purpose compilers (e.g., LLVM (Lattner
    and Adve, [2004](#bib.bib52))), which provides better portability across diverse
    hardware architectures. Similar to traditional compiler, DL compilers also adopt
    the layered design including frontend, intermediate representation (IR) and backend.
    However, the uniqueness of DL compiler lies in the design of multi-level IRs and
    DL specific optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we provide a comprehensive survey of existing DL compilers by
    dissecting the compiler design into frontend, multi-level IRs and backend, with
    special emphasis on the IR design and optimization methods. To the best of our
    knowledge, this is the first paper that provides a comprehensive survey on the
    design of DL compiler. Specifically, this paper makes the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We dissect the commonly adopted design architecture of existing DL compilers,
    and provide detailed analysis of the key design components such as multi-level
    IRs, frontend optimizations (including node-level, block-level and dataflow-level
    optimizations) and backend optimizations (including hardware-specific optimization,
    auto-tuning and optimized kernel libraries).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a comprehensive taxonomy of existing DL compilers from various aspects,
    which corresponds to the key components described in this survey. The target of
    this taxonomy is to provide guidelines about the selection of DL compilers for
    the practitioners considering their requirements, as well as to give a thorough
    summary of the DL compilers for researchers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have provided the quantitative performance comparison among DL compilers
    on CNN models, including full-fledged models and lightweight models. We have compared
    both end-to-end and per-layer (convolution layers since they dominate the inference
    time) performance to show the effectiveness of optimizations. The evaluation scripts
    and results are open sourced¹¹1[https://github.com/buaa-hipo/dlcompiler-comparison](https://github.com/buaa-hipo/dlcompiler-comparison)
    for reference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We highlight several insights for the future development of DL compilers, including
    dynamic shape and pre-/post-processing, advanced auto-tuning, polyhedral model,
    subgraph partitioning, quantization, unified optimizations, differentiable programming
    and privacy protection, which we hope to boost the research in the DL compiler
    community.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of this paper is organized as follows. Section [2](#S2 "2\. Background
    ‣ The Deep Learning Compiler: A Comprehensive Survey") presents the background
    of DL compilers, including the DL frameworks, DL hardware, as well as hardware
    (FPGA) specific DL code generators. Section [3](#S3 "3\. Common Design Architecture
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey") describes
    the common design architecture of DL compilers. Section [4](#S4 "4\. Key Components
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey") discusses
    the key components of DL compilers, including multi-level IRs, frontend optimizations
    and backend optimizations. Section [5](#S5 "5\. Taxonomy of DL Compilers ‣ The
    Deep Learning Compiler: A Comprehensive Survey") presents a comprehensive taxonomy.
    Section [6](#S6 "6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive
    Survey") provides the quantitative performance comparison. Section [7](#S7 "7\.
    Conclusion and Future Directions ‣ The Deep Learning Compiler: A Comprehensive
    Survey") highlights the future directions for DL compiler research.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Deep Learning Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we provide an overview of popular DL frameworks. The discussion
    might not be exhaustive but is meant to provide a guideline fo DL practitioners.
    Figure [1](#S2.F1 "Figure 1 ‣ 2.1\. Deep Learning Frameworks ‣ 2\. Background
    ‣ The Deep Learning Compiler: A Comprehensive Survey") presents the landscape
    of DL frameworks including currently popular frameworks, historical frameworks
    and ONNX supported frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow - Among all the DL frameworks, TensorFlow has the most comprehensive
    support for language interfaces, including C ++, Python, Java, Go, R, and Haskell.
    TensorFlow employs a dataflow graph of primitive operators extended with restricted
    control edges to represent differentiable programs (Roesch et al., [2019](#bib.bib79)).
    TensorFlow Lite is designed for mobile and embedded deep learning and provides
    an Android neural network API. To reduce the complexity of using TensorFlow, Google
    adopts Keras as a frontend to the TensorFlow core. Furthermore, The eager-mode
    in TensorFlow applies an approach similar to PyTorch to support dynamic computation
    graphs better.
  prefs: []
  type: TYPE_NORMAL
- en: Keras - Keras (Chollet et al., [2015](#bib.bib20)) is a high-level neural network
    library for quickly building DL models, written in pure Python. Though not a DL
    framework on its own, Keras provides a high-level API that integrates with TensorFlow,
    MXNet, Theano, and CNTK. With Keras, DL developers can build a neural network
    with just a few lines of code. Besides, Keras can integrate with other common
    DL packages, such as scikit-learn. However, Keras is not flexible enough due to
    over-encapsulation, which makes it too difficult to add operators or obtain low-level
    data information.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch - Facebook has rewritten the Lua-based DL framework Torch in Python
    and refactored all modules on Tensor level, which leads to the release of PyTorch.
    As the most popular dynamic framework, PyTorch embeds primitives for constructing
    dynamic dataflow graphs in Python, where the control flow is executed in the Python
    interpreter. PyTorch 1.0 integrated the codebases of PyTorch 0.4 and Caffe2 to
    create a unified framework. This allows PyTorch to absorb the benefits of Caffe2
    to support efficient graph execution and mobile deployment. FastAI (Howard et al.,
    [2018](#bib.bib40)) is an advanced API layer based on PyTorch’s upper-layer encapsulation.
    It fully borrows Keras to ease the use of PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Caffe/Caffe2 - Caffe (Jia et al., [2014](#bib.bib43)) was designed for deep
    learning and image classification by UC Berkeley. Caffe has the command line,
    Python, and MATLAB APIs. Caffe’s simplicity makes the source codes easy to extend,
    which is suitable for developers to analyze in-depth. Therefore, Caffe is mainly
    positioned in research, which has made it popular from the beginning to the present.
    Caffe2 is built upon the original Caffe project. Caffe2 is similar to TensorFlow
    in code structure, albeit with a lighter API and easier access to the intermediate
    results in the computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'MXNet - MXNet supports multiple language APIs including Python, C++, R, Scala,
    Julia, Matlab, and JavaScript. It was intended to be scalable and was designed
    from the perspective to reduce data loading and I/O complexity (Chen et al., [2015](#bib.bib17)).
    MXNet offers different paradigms: declarative programming like Caffe and Tensorflow
    as well as imperative like PyTorch. In December 2017, Amazon and Microsoft jointly
    released Gluon (MXNet, [2017](#bib.bib70)) based on MXNet, which is an advanced
    interface similar to Keras and FastAI. Gluon supports both flexible, dynamic graphs
    and efficient, static graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: CNTK - CNTK can be used through Python, C++ and C# APIs, or its own scripting
    language (i.e., BrainScript). CNTK is designed to be easy-to-use and production-ready
    for large-scale data in production (Hatcher and Yu, [2018](#bib.bib38)). However,
    CNTK does not yet support the ARM architecture, which limits its usage on mobile
    devices. It uses the static computation graph similar to TensorFlow and Caffe,
    in which a DL model is treated as a series of computational steps through a directed
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: PaddlePaddle - The original design of PaddlePaddle (Baidu, [2016](#bib.bib12))
    is similar to Caffe, where each model can be represented as a set of layers. However,
    PaddlePaddle v2 has adopted the concept of operators with reference to TensorFlow,
    which breaks layers into finer-grained operators, thereby supporting more complex
    DL models. And PaddlePaddle Fluid is similar to PyTorch because it provides own
    interpreter so as to avoid the limited performance of Python interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'ONNX - The Open Neural Network Exchange (ONNX) (Microsoft, [2017](#bib.bib67))
    defines a scalable computation graph model, and thus computation graphs built
    by different DL frameworks can be easily transformed into ONNX. With ONNX, it
    becomes easier to convert models between DL frameworks. For example, it allows
    developers to build an MXNet model and then run the model using PyTorch for inference.
    As shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1\. Deep Learning Frameworks ‣ 2\.
    Background ‣ The Deep Learning Compiler: A Comprehensive Survey"), ONNX has been
    integrated into PyTorch, MXNet, PaddlePaddle, and so on. For several DL frameworks
    (e.g., TensorFlow and Keras) that are not directly supported yet, and ONNX adds
    converters to them.'
  prefs: []
  type: TYPE_NORMAL
- en: Historical Frameworks - Due to the rapid evolvement in DL community, many historical
    DL frameworks are no longer active. For example, PyTorch has replaced Torch (Collobert
    et al., [2011](#bib.bib21)). As one of the oldest DL frameworks, Theano (Team
    et al., [2016a](#bib.bib87)) is no longer under maintenance. Deeplearning4J (Team
    et al., [2016b](#bib.bib86)) a distributed DL framework based on Java and Scala,
    however becomes inactive due to the lack of large developer community. Chainer (Tokui
    et al., [2019](#bib.bib88)) was once the preferred framework for dynamic computation
    graphs, however replaced by MXNet, PyTorch and TensorFlow with similar features.
  prefs: []
  type: TYPE_NORMAL
- en: Previous works (Bahrampour et al., [2015](#bib.bib11); Fonnegra et al., [2017](#bib.bib26);
    Shams et al., [2017](#bib.bib83); Guo et al., [2018](#bib.bib36); Nara et al.,
    [2019](#bib.bib71); Wei et al., [2019](#bib.bib101)) have compared the performance
    of DL frameworks on different applications (e.g., computer vision and image classification)
    and different hardware (e.g., CPU, GPU, and TPU). For detailed information about
    each DL framework, the readers can refer to (Hatcher and Yu, [2018](#bib.bib38)).
    Different from them, this survey focuses on the research efforts on DL compilers
    which provide more general approach to execute various DL models on diverse hardware
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/879823e1104f967e4d01c4a8cd2eb9fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1\. DL framework landscape: 1) Currently popular DL frameworks; 2) Historical
    DL frameworks; 3) ONNX supported frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Deep Learning Hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The DL hardware can be divided into three categories based on the generality:
    1) general-purpose hardware that can support DL workloads through hardware and
    software optimization; 2) dedicated hardware that focus on accelerating DL workloads
    with fully customized circuit design; 3) neuromorphic hardware that function by
    mimicking the human brain.'
  prefs: []
  type: TYPE_NORMAL
- en: General-purpose Hardware - The most representative general-purpose hardware
    for DL models is Graphic Processing Unit (GPU), which achieves high parallelism
    with many-core architecture. For example, Nvidia GPUs have introduced tensor cores
    since the Volta architecture. Tensor cores can accelerate mixed-precision matrix
    multiply-and-accumulate calculations in parallel, which are widely used in DL
    models during both training and inference. Co-optimized with the hardware, NVIDIA
    also launches highly optimized DL libraries and tools such as cuDNN (Chetlur et al.,
    [2014](#bib.bib19)) and TensorRT (NVIDIA, [2019c](#bib.bib74)) to further accelerate
    the computation of DL models.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated Hardware - Dedicated hardware is fully customized for DL computation
    to improve performance and energy efficiency to extreme. The rapid expansion of
    DL applications and algorithms has spurred many startups developing dedicated
    DL hardware (e.g., Graphcore GC2, Cambricon MLU270). Besides, traditional hardware
    companies (e.g., Intel NNP, Qualcomm Cloud AI 100) and cloud service providers
    (e.g., Google TPU, Amazon Inferentia, and Alibaba Hanguang) have also invested
    in this field. The most well known dedicated DL hardware is Google’s TPU series.
    A TPU includes Matrix Multiplier Unit (MXU), Unified Buffer (UB), and Activation
    Unit (AU), which is driven with CISC instructions by the host processor. The MXU
    is mainly composed of a systolic array, which is optimized for power and area
    efficiency in performing matrix multiplications. Compared to CPU and GPU, TPU
    is still programmable but uses a matrix as a primitive instead of a vector or
    scalar. The Amazon Inferentia has also attracts the attention recently. This chip
    has four NeuroCores that are designed for tensor-level operations, and it has
    large on-chip cache to avoid the frequent main memory access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neuromorphic Hardware - Neuromorphic chips use electronic technology to simulate
    the biological brain. Representative products of the this kind are IBM’s TrueNorth
    and Intel’s Loihi. Neuromorphic chips (e.g., TrueNorth) have very high connectivity
    between their artificial neurons. Neuromorphic chips also replicate a structure
    similar to the brain tissue: neurons can simultaneously store and process the
    data. Traditional chips distribute processors and memory in different locations,
    but neuromorphic chips usually have many microprocessors, each of which has a
    small amount of local memory. Compared to TrueNorth, Loihi has a learning ability
    more similar to the brain. Loihi introduces the pulse-time-dependent synaptic
    plasticity model (STDP), a mechanism that regulates synaptic strength by the relative
    time of pre-synaptic and post-synaptic pulses. However, neuromorphic chips are
    far away from Large-scale commercial production. Despite that, in computer science
    domain, neuromorphic chips can help to capture the process of rapid, life-long
    learning which is ignored by regular DL models, and in neurology domain, they
    are helpful to figure out how the various parts of the brain work together to
    create thoughts, feelings, and even consciousness.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Hardware-specific DL Code Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Field Programmable Gate Arrays (FPGAs) are reprogrammable integrated circuits
    that contain an array of programmable logic blocks. Programmers can configure
    them after manufacturing. Besides the reprogrammable nature, the low-power and
    high-performance nature of the FPGA make it widely used in so many domains, such
    as communication, medical, image processing, and ASIC prototyping. As for the
    domain of deep learning, the high-performance CPUs and GPUs are highly-reprogrammable
    but power-hungry, while the power-efficient ASICs are specialized for fixed applications.
    However, the FPGA can bridge the gap between CPUs/GPUs and ASICs, which causes
    the FPGA to be an attractive platform for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The High-Level Synthesis (HLS) programming model enables the FPGA programmers
    to generate effective hardware designs conveniently using high-level languages
    such as C and C++. It avoids writing lots of Verilog or VHDL descriptions, which
    lowers the programming threshold and reduces the long design circle. Xilinx Vivado
    HLS and Intel FPGA SDK for OpenCL are two of the popular HLS tools targeting their
    own FPGAs. However, mapping DL models to FPGAs remains a complicated work even
    with HLS, because that 1) DL models are usually described by the languages of
    DL frameworks rather than bare mental C/C++ code, and 2) DL-specific information
    and optimizations are hard to be leveraged.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hardware-specific code generator targeting FPGA take the DL models or their
    domain-specific languages (DSLs) as the input, conduct the domain-specific (about
    FPGA and DL) optimizations and mappings, then generate the HLS or Verilog/VHDL
    and finally generate the bitstream. They can be classified into two categories
    according to the generated architectures of FPGA-based accelerators: the processor
    architecture and the streaming architecture (Venieris et al., [2018](#bib.bib94)).'
  prefs: []
  type: TYPE_NORMAL
- en: The processor architecture has similarities with general-purpose processors.
    An FPGA accelerator of this architecture usually comprises several Processing
    Units (PUs), which are comprised of on-chip buffers and multiple smaller Processing
    Engines (PEs). It usually has a virtual instruction set (ISA), and the control
    of hardware and the scheduling of the execution should be determined by software.
    What’s more, the static scheduling method avoids the overheads of von Neumann
    execution (including instruction fetching and decoding). A hardware template is
    a generic and fundamental implementation with configurable parameters. The DL
    code generator targeting this architecture adopt the hardware templates to generate
    the accelerator designs automatically. With the configurable parameters of templates,
    the code generator achieve the scalability and flexibility (Zhao et al., [2018](#bib.bib105)).
    The scalability means that the code generator can generate designs for FPGAs ranging
    from high-performance to power-efficient, and the flexibility means that the code
    generator can generate designs for various DL models with different layer types
    and parameters. The number of PUs and the number of PEs per PU are template parameters
    of importance. Besides, the tilling size and batch size are also essential scheduling
    parameters about mapping the DL models to PUs and PEs. All these parameters are
    usually determined by the design space exploration using various strategies, such
    as combining the performance model and auto-tuning. DNN Weaver (Sharma et al.,
    [2016](#bib.bib84)), Angel-Eye (Guo et al., [2017a](#bib.bib34)), ALAMO (Ma et al.,
    [2018](#bib.bib64)), FP-DNN (Guan et al., [2017](#bib.bib33)), SysArrayAccel (Wei
    et al., [2017](#bib.bib102)) are typical FPGA DL code generator targeting the
    processor architecture. What’s more, the PUs and PEs are usually responsible for
    coarse-grained basic operations such as matrix-vector multiplication, matrix-matrix
    multiplication, pooling, and some element-wise operations. The optimizations of
    these basic operations are mainly guided by the tradeoff between the parallelism
    and data reuse, which is similar to general optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: The streaming architecture has similarities with pipelines. An FPGA accelerator
    of this architecture consists of multiple different hardware blocks, and it nearly
    has one hardware block for each layer of an input DL model. With the input data
    of a DL model, this kind of accelerators process the data through the different
    hardware blocks in the same sequence with layers. Additionally, with the streaming
    input data, all hardware blocks can be fully utilized in a pipeline manner. However,
    the streaming architecture usually follows an initial assumption that the on-chip
    memory the computation resources on target FPGA are sufficient to accommodate
    the DL models, which bring barriers to deploy deep models with complicated layers.
    The DL code generator targeting this architecture can solve this problem by leveraging
    the reconfigurability of FPGA or adopting dynamic control flow. And the further
    optimization of a single block resembles that of basic operations of the processor
    architecture. fpgaConvNet (Venieris and Bouganis, [2016](#bib.bib93)), DeepBurning (Wang
    et al., [2016](#bib.bib99)), Haddoc2 (Abdelouahab et al., [2017](#bib.bib3)),
    and AutoCodeGen (Liu et al., [2016](#bib.bib60)) are typical corresponding DL
    code generator.
  prefs: []
  type: TYPE_NORMAL
- en: For the detailed survey of specific compilation techniques that map DL models
    to FPGAs, the readers can refer to (Venieris et al., [2018](#bib.bib94); Zhao
    et al., [2018](#bib.bib105); Guo et al., [2017b](#bib.bib35)). Different from (Venieris
    et al., [2018](#bib.bib94); Zhao et al., [2018](#bib.bib105); Guo et al., [2017b](#bib.bib35)),
    this survey focuses on general DL compilation techniques that can be applied to
    broader DL hardware other than bounding to FPGA.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Common Design Architecture of DL Compilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d6bfa752ac25b3f4b1fd32c1b4a0333.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. The overview of commonly adopted design architecture of DL compilers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The common design architecture of a DL compiler primarily contains two parts:
    the compiler frontend and the compiler backend, as shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3\. Common Design Architecture of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey"). The intermediate representation (IR) is spread
    across both the frontend and the backend. Generally, IR is an abstraction of the
    program and is used for program optimizations. Specifically, the DL models are
    translated into multi-level IRs in DL compilers, where the high-level IR resides
    in the frontend, and the low-level IR resides in the backend. Based on the high-level
    IR, the compiler frontend is responsible for hardware-independent transformations
    and optimizations. Based on the low-level IR, the compiler backend is responsible
    for hardware-specific optimizations, code generation, and compilation. Note that
    this survey focuses on the design principles of DL compilers. For functional and
    experimental comparisons of DL compilers, the readers can refer to (Xing et al.,
    [2019](#bib.bib103); Li et al., [2020](#bib.bib56)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-level IR, also known as graph IR, represents the computation and the
    control flow and is hardware-independent. The design challenge of high-level IR
    is the ability of abstraction of the computation and the control flow, which can
    capture and express diverse DL models. The goal of the high-level IR is to establish
    the control flow and the dependency between the operators and the data, as well
    as provide an interface for graph-level optimizations. It also contains rich semantic
    information for compilation as well as offers extensibility for customized operators.
    The detailed discussion of high-level IR is presented in Section [4.1](#S4.SS1
    "4.1\. High-level IR ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The low-level IR is designed for hardware-specific optimization and code generation
    on diverse hardware targets. Thus, the low-level IR should be fine-grained enough
    to reflect the hardware characteristics and represent the hardware-specific optimizations.
    It should also allow the use of mature third-party tool-chains in compiler backends
    such as Halide (Ragan-Kelley et al., [2013](#bib.bib78)), polyhedral model (Grosser,
    [2000](#bib.bib32)), and LLVM (Lattner and Adve, [2004](#bib.bib52)). The detailed
    discussion of low-level IR is presented in Section [4.2](#S4.SS2 "4.2\. Low-level
    IR ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The frontend takes a DL model from existing DL frameworks as input, and then
    transforms the model into the computation graph representation (e.g., graph IR).
    The frontend needs to implement various format transformations To support the
    diverse formats in different frameworks. The computation graph optimizations incorporate
    the optimization techniques from both general-purpose compilers and the DL specific
    optimizations, which reduce the redundancy and improve the efficiency upon the
    graph IR. Such optimizations can be classified into node-level (e.g., nop elimination
    and zero-dim-tensor elimination), block-level (e.g., algebraic simplification,
    operator fusion, and operator sinking) and dataflow-level (e.g., CSE, DCE, static
    memory planning, and layout transformation). After the frontend, the optimized
    computation graph is generated and passed to the backend. The detailed discussion
    of the frontend is presented in Section [4.3](#S4.SS3 "4.3\. Frontend Optimizations
    ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The backend transforms the high-level IR into low-level IR and performs hardware-specific
    optimizations. On the one hand, it can directly transform the high-level IR to
    third-party tool-chains such as LLVM IR to utilize the existing infrastructures
    for general-purpose optimizations and code generation. On the other hand, it can
    take advantage of the prior knowledge of both DL models and hardware characteristics
    for more efficient code generation, with customized compilation passes. The commonly
    applied hardware-specific optimizations include hardware intrinsic mapping, memory
    allocation and fetching, memory latency hiding, parallelization as well as loop
    oriented optimizations. To determine the optimal parameter setting in the large
    optimization space, two approaches are widely adopted in existing DL compilers
    such as auto-scheduling (e.g., polyhedral model) and auto-tuning (e.g., AutoTVM).
    The optimized low-level IR is compiled using JIT or AOT to generate codes for
    different hardware targets. The detailed discussion of the backend is presented
    in Section [4.4](#S4.SS4 "4.4\. Backend Optimizations ‣ 4\. Key Components of
    DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Key Components of DL Compilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1\. High-level IR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To overcome the limitation of IR adopted in traditional compilers that constrains
    the expression of complex computations used in DL models, existing DL compilers
    leverage high-level IR (as known as graph IR) with special designs for efficient
    code optimizations. To better understand the graph IR used in the DL compilers,
    we describe the representation and implementation of graph IR as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1\. Representation of Graph IR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The representation of graph IR influences the expressiveness of graph IR and
    also decides the way the DL compilers analyze the graph IR.
  prefs: []
  type: TYPE_NORMAL
- en: 'DAG-based IR - DAG-based IR is one of the most traditional ways for the compilers
    to build a computation graph, with nodes and edges organized as a directed acyclic
    graph (DAG). In DL compilers (Chen et al., [2018b](#bib.bib18); Cyphers et al.,
    [2018](#bib.bib22); Rotem et al., [2018](#bib.bib80); Vasilache et al., [2018](#bib.bib92);
    Leary and Wang, [2017](#bib.bib54)), the nodes of a DAG represent the atomic DL
    operators (convolution, pooling, etc.), and the edges represent the tensors. And
    the graph is acyclic without loops, which differs from the data dependence graphs (Kuck
    et al., [1981](#bib.bib51)) (DDG) of generic compilers (Lattner and Adve, [2004](#bib.bib52);
    Lattner et al., [2020](#bib.bib53)). And with the help of the DAG computation
    graph, DL compilers can analyze the relationship and dependencies between various
    operators and use them to guide the optimizations. There are already plenty of
    optimizations on DDG, such as common sub-expression elimination (CSE) and dead
    code elimination (DCE). By combining the domain knowledge of DL with these algorithms,
    further optimizations can be applied to the DAG computation graph, which will
    be elaborated in Section [4.3](#S4.SS3 "4.3\. Frontend Optimizations ‣ 4\. Key
    Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey").
    DAG-based IR is convenient for programming and compiling due to its simplicity,
    but it has deficiencies such as semantic ambiguity caused by the missing definition
    of computation scope.'
  prefs: []
  type: TYPE_NORMAL
- en: Let-binding-based IR - Let-binding is one method to solve the semantic ambiguity
    by offering let expression to certain functions with restricted scope used by
    many high-level programming languages such as Javascript (Goodman, [2007](#bib.bib31)),
    F# (Petricek and Syme, [2012](#bib.bib77)), and Scheme (Abelson et al., [1998](#bib.bib4)).
    When using the let keyword to define an expression, a let node is generated, and
    then it points to the operator and variable in the expression instead of just
    building computational relation between variables as a DAG. In DAG-based compiler,
    when a process needs to get the return value of one expression, it first accesses
    the corresponding node and searches related nodes, also known as recursive descent
    technique. In contrast, the let-binding based compiler figures out all results
    of the variables in let expression and builds a variable map. When a particular
    result is needed, the compiler looks up this map to decide the result of the expression.
    Among the DL compilers, the Relay IR (Roesch et al., [2019](#bib.bib79)) of TVM
    adopts both DAG-based IR and let-binding-based IR to obtain the benefits of both.
  prefs: []
  type: TYPE_NORMAL
- en: Representing Tensor Computation - Different graph IRs have different ways to
    represent the computation on tensors. The operators of diverse DL frameworks are
    translated to graph IRs according to such specific representations. And the customized
    operators also need to be programmed in such representation. The representation
    of tensor computation can be divided into the following three categories.
  prefs: []
  type: TYPE_NORMAL
- en: '1) Function-based: The function-based representation just provides encapsulated
    operators, which is adopted by Glow, nGraph and XLA. Take High Level Optimizer
    (HLO, the IR of XLA) for example, it consists of a set of functions in symbolic
    programming, and most of them have no side-effect. The instructions are organized
    into three levels, including HloModule (the whole program), HloComputaion (a function),
    and HloInstruction (the operation). XLA uses HLO IR to represent both graph IR
    and operation IR so that the operation of HLO ranges from the dataflow level to
    the operator level.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) Lambda expression: The lambda expression, an index formula expression, describes
    calculation by variable binding and substitution. Using lambda expression, programmers
    can define a computation quickly without implementing a new function. TVM represents
    the tensor computation using the tensor expression, which is based on the lambda
    expression. In TVM, computational operators in tensor expression are defined by
    the shape of output tensor and the lambda expression of computing rules.'
  prefs: []
  type: TYPE_NORMAL
- en: '3) Einstein notation: The Einstein notation, also known as the summation convention,
    is a notation to express summation. Its programming simplicity is superior to
    lambda expression. Taking TC for example, the indexes for temporary variables
    do not need to be defined. The IR can figure out the actual expression by the
    occurrence of undefined variables based on Einstein notation. In Einstein notation,
    the operators need to be associative and commutative. This restriction guarantees
    the reduction operator can be executed by any order, making it possible for further
    parallelization.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. Implementation of Graph IR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The implementation of graph IR in DL compilers fulfills the management of data
    and operation.
  prefs: []
  type: TYPE_NORMAL
- en: Data representation - The data in DL compilers (e.g., inputs, weights, and intermediate
    data) are usually organized in the form of tensors, which are also known as multi-dimensional
    arrays. The DL compilers can represent tensor data directly by memory pointers,
    or in a more flexible way by placeholders. A placeholder contains the size for
    each dimension of a tensor. Alternatively, the dimension sizes of the tensor can
    be marked as unknown. For optimizations, the DL compilers require the data layout
    information. In addition, the bound of iterators should be inferred according
    to the placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: '1) Placeholder: Placeholder is widely used in symbolic programming (e.g., Lisp (McCarthy
    and Levin, [1965](#bib.bib66)), Tensorflow (Abadi et al., [2016](#bib.bib2))).
    A placeholder is simply a variable with explicit shape information (e.g., size
    in each dimension), and it will be populated with values at the later stage of
    the computation. It allows the programmers to describe the operations and build
    the computation graph without concerning the exact data elements, which helps
    separate the computation definition from the exact execution in DL compilers.
    Besides, it is convenient for the programmers to change the shape of input/output
    and other corresponding intermediate data by using placeholders without changing
    the computation definition.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) Unknown (Dynamic) shape representation: The unknown dimension size is usually
    supported when declaring the placeholders. For instance, TVM uses Any to represent
    an unknown dimension (e.g., $Tensor\langle(Any,3),fp32\rangle$); XLA uses None
    to achieve the same purpose (e.g., $tf.placeholder$ $(``float&quot;,[None,3])$);
    nGraph uses its PartialShape class. The unknown shape representation is necessary
    to support the dynamic model. However, to fully support dynamic model, the bound
    inference and dimension checking should be relaxed. In addition, extra mechanism
    should be implemented to guarantee memory validity.'
  prefs: []
  type: TYPE_NORMAL
- en: '3) Data layout: The data layout describes how a tensor is organized in memory,
    and it is usually a mapping from logical indices to memory indices. The data layout
    usually includes the sequence of dimensions (e.g., NCHW and NHWC), tiling, padding,
    striding, etc. TVM and Glow represent data layout as operator parameters and require
    such information for computation and optimization. However, combining data layout
    information with operators rather than tensors enables intuitive implementation
    for certain operators and reduces the compilation overhead. XLA represents data
    layout as constraints related to its backend hardware. Relay and MLIR are going
    to add data layout information into their type systems for tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: '4) Bound inference: The bound inference is applied to determine the bound of
    iterators when compiling DL models in DL compilers. Although the tensor representation
    in DL compilers is convenient to describe the inputs and outputs, it exposes special
    challenges for inferring the iterator bound. The bound inference is usually performed
    recursively or iteratively, according to the computation graph and the known placeholders.
    For example, in TVM the iterators form a directed acyclic hyper-graph, where each
    node of the graph represents an iterator and each hyper-edge represents the relation
    (e.g., split, fuse or rebase) among two or more iterators. Once the bound of the
    root iterator is determined based on the shapes of placeholders, other iterators
    can be inferred according to the relations recursively.'
  prefs: []
  type: TYPE_NORMAL
- en: Operators supported - The operators supported by DL compilers are responsible
    for representing the DL workloads, and they are nodes of the computation graph.
    The operators usually include algebraic operators (e.g., $+$, $\times$, $\exp$
    and topK), neural network operators (e.g., convolution and pooling), tensor operators
    (e.g., reshape, resize and copy), broadcast and reduction operators (e.g., min
    and argmin), as well as control flow operators (e.g., conditional and loop). Here,
    we choose three representative operators that are frequently used across different
    DL compilers for illustration. In addition, we discuss the case for customized
    operators.
  prefs: []
  type: TYPE_NORMAL
- en: '1) Broadcast: The broadcast operators can replicate the data and generate new
    data with compatible shape. Without broadcast operators, the input tensor shapes
    are more constrained. For example, for an add operator, the input tensors are
    expected to be of the same shape. Some compilers such as XLA and Relay relax such
    restriction by offering the broadcasting operator. For example, XLA allows the
    element-wise addition on a matrix and a vector by replicating it until its shape
    matches the matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) Control flow: Control flow is needed when representing complex and flexible
    models. Models such as RNN and Reinforcement learning (RL) depend on recurrent
    relations and data-dependent conditional execution (Yu et al., [2018](#bib.bib104)),
    which requires control flow. Without supporting control flow in graph IR of DL
    compilers, these models must rely on the control flow support of the host languages
    (e.g., if and while in Python) or static unrolling, which deteriorates the computation
    efficiency. Relay notices that arbitrary control flow can be implemented by recursion
    and pattern, which has been demonstrated by functional programming (Roesch et al.,
    [2019](#bib.bib79)). Therefore, it provides if operator and recursive function
    for implementing control flow. On the contrary, XLA represents control flow by
    special HLO operators such as while and conditional.'
  prefs: []
  type: TYPE_NORMAL
- en: '3) Derivative: The derivative operator of an operator $Op$ takes the output
    gradients and the input data of $Op$ as its inputs, and then calculates the gradient
    of $Op$. Although some DL compilers (e.g., TVM and TC) support automatic differentiation (Van Merriënboer
    et al., [2018](#bib.bib89)), they require the derivatives of all operators in
    high-level IR when the chain rule is applied. TVM is working towards providing
    the derivative operators of both algebraic operators and neural network operators.
    The programmers can use these derivative operators for building the derivatives
    of customized operators. On the contrary, PlaidML can generate derivative operators
    automatically, even for customized operators. Notably, DL compilers unable to
    support derivative operators fail to provide the capability of model training.'
  prefs: []
  type: TYPE_NORMAL
- en: '4) Customized operators: It allows programmers to define their operators for
    a particular purpose. Providing support for customized operators improves the
    extensibility of DL compilers. For example, when defining new operators in Glow,
    the programmers need to realize the logic and node encapsulation. In addition,
    extra efforts are needed, such as the lowering step, operation IR generation,
    and instruction generation, if necessary. Whereas, TVM and TC require less programming
    efforts except describing the computation implementation. Specifically, the users
    of TVM only need to describe the computation and the schedule and declare the
    shape of input/output tensors. Moreover, the customized operators integrate Python
    functions through hooks, which further reduces the programmers’ burden.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Nearly all DL compilers have their unique high-level IRs. However, they share
    similar design philosophies, such as using DAG and let-binding to build the computation
    graph. In addition, they usually provide convenient ways for programmers to represent
    tensor computation. The data and operators designed in high-level IRs are flexible
    and extensible enough to support diverse DL models. More importantly, the high-level
    IRs are hardware-independent and thus can be applied with different hardware backend.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Low-level IR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1\. Implementation of Low-Level IR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Low-level IR describes the computation of a DL model in a more fine-grained
    representation than that in high-level IR, which enables the target-dependent
    optimizations by providing interfaces to tune the computation and memory access.
    In this section, we classify the common implementations of low-level IRs into
    three categories: Halide-based IR, polyhedral-based IR, and other unique IR.'
  prefs: []
  type: TYPE_NORMAL
- en: Halide-based IR - Halide is firstly proposed to parallelize image processing,
    and it is proven to be extensible and efficient in DL compilers (e.g., TVM). The
    fundamental philosophy of Halide is the separation of computation and schedule.
    Rather than giving a specific scheme directly, the compilers adopting Halide try
    various possible schedule and choose the best one. The boundaries of memory reference
    and loop nests in Halide are restricted to bounded boxes aligned to the axes.
    Thus, Halide cannot express the computation with complicated patterns (e.g., non-rectangular).
    Fortunately, the computations in DL are quite regular to be expressed perfectly
    by Halide. Besides, Halide can easily parameterize these boundaries and expose
    them to the tuning mechanism. The original IR of the Halide needs to be modified
    when applied to backend of DL compilers. For example, the input shape of Halide
    is infinite, whereas the DL compilers need to know the exact shape of data in
    order to map the operator to hardware instructions. Some compilers, such as TC,
    require the fixed size of data, to ensure better temporal locality for tensor
    data.
  prefs: []
  type: TYPE_NORMAL
- en: TVM has improved Halide IR into an independent symbolic IR by following efforts.
    It removes the dependency on LLVM and refactors the structure of both the project
    module and the IR design of Halide, pursuing better organization as well as accessibility
    for graph IR and frontend language such as Python. The re-usability is also improved,
    with a runtime dispatching mechanism implemented to add customized operators conveniently.
    TVM simplifies the variable definition from string matching to pointer matching,
    guaranteeing that each variable has a single define location (static single-assignment,
    SSA) (Cytron et al., [1991](#bib.bib23))).
  prefs: []
  type: TYPE_NORMAL
- en: Polyhedral-based IR - The polyhedral model is an important technique adopted
    in DL compilers. It uses linear programming, affine transformations, and other
    mathematical methods to optimize loop-based codes with static control flow of
    bounds and branches. In contrast to Halide, the boundaries of memory reference
    and loop nests can be polyhedrons with any shapes in the polyhedral model. Such
    flexibility makes polyhedral models widely used in generic compilers. However,
    such flexibility also prevents the integration with the tuning mechanisms. Nevertheless,
    due to the ability to deal with deeply nested loops, many DL compilers, such as
    TC and PlaidML (as the backend of nGraph), have adopted the polyhedral model as
    their low-level IR. The polyhedral-based IR makes it easy to apply various polyhedral
    transformations (e.g., fusion, tiling, sinking, and mapping), including both device-dependent
    and device-independent optimizations. There are many toolchains that are borrowed
    by polyhedral-based compilers, such as isl (Verdoolaege, [2010](#bib.bib97)),
    Omega (Kelly et al., [1996](#bib.bib49)), PIP (Feautrier, [1988](#bib.bib24)),
    Polylib (Loechner, [1999](#bib.bib61)), and PPL (Bagnara et al., [2006](#bib.bib10)).
  prefs: []
  type: TYPE_NORMAL
- en: TC has its unique design in low-level IR, which combines the Halide and polyhedral
    model. It uses Halide-based IR to represent the computation and adopts the polyhedral-based
    IR to represent the loop structures. TC presents detailed expressions through
    abstract instances and introduces specific node types. In brief, TC uses the domain
    node to specify the ranges of index variables and uses the context node to describe
    new iterative variables that are related to hardware. And it uses the band node
    to determine the order of iterations. A filter node represents an iterator combined
    with a statement instance. Set and sequence are keywords to specify the execution
    types (parallel and serial execution) for filters. Besides, TC uses extension
    nodes to describe other necessary instructions for code generation, such as the
    memory movement.
  prefs: []
  type: TYPE_NORMAL
- en: PlaidML uses polyhedral-based IR (called Stripe) to represent tensor operations.
    It creates a hierarchy of parallelizable code by extending the nesting of parallel
    polyhedral blocks to multiple levels. Besides, it allows nested polyhedrons to
    be allocated to nested memory units, providing a way to match the computation
    with the memory hierarchy. In Stripe, the hardware configuration is independent
    of the kernel code. The tags in Stripe (known as passes in other compilers) do
    not change the kernel structure, but provide additional information about the
    hardware target for the optimization passes. Stripe splits the DL operators into
    tiles that fit into local hardware resources.
  prefs: []
  type: TYPE_NORMAL
- en: Other unique IR - There are DL compilers implementing customized low-level IRs
    without using Halide and polyhedral model. Upon the customized low-level IRs,
    they apply hardware-specific optimizations and lowers to LLVM IR.
  prefs: []
  type: TYPE_NORMAL
- en: 'The low-level IR in Glow is an instruction-based expression that operates on
    tensors referenced by addresses (Rotem et al., [2018](#bib.bib80)). There are
    two kinds of instruction-based functions in Glow low-level IR: declare and program.
    The first one declares the number of constant memory regions that live throughout
    the lifetime of the program (e.g., input, weight, bias). The second one is a list
    of locally allocated regions, including functions (e.g., conv and pool) and temporary
    variables. Instructions can run on the global memory regions or locally allocated
    regions. Besides, each operand is annotated with one of the qualifiers: @in indicates
    the operand reads from the buffer; @out indicates that the operand writes to the
    buffer; @inout indicates that the operand reads and writes to the buffer. These
    instructions and operand qualifiers help Glow determine when certain memory optimizations
    can be performed.'
  prefs: []
  type: TYPE_NORMAL
- en: MLIR is highly influenced by LLVM, and it is a purer compiler infrastructure
    than LLVM. MLIR reuses many ideas and interfaces in LLVM, and sits between the
    model representation and code generation. MLIR has a flexible type system and
    allows multiple abstraction levels, and it introduces dialects to represent these
    multiple levels of abstraction. Each dialect consists of a set of defined immutable
    operations. The current dialects of MLIR include TensorFlow IR, XLA HLO IR, experimental
    polyhedral IR, LLVM IR, and TensorFlow Lite. The flexible transformations between
    dialects are also supported. Furthermore, MLIR can create new dialects to connect
    to a new low-level compiler, which paves the way for hardware developers and compiler
    researchers.
  prefs: []
  type: TYPE_NORMAL
- en: The HLO IR of XLA can be considered as both high-level IR and low-level IR because
    HLO is fine-grained enough to represent the hardware-specific information. Besides,
    HLO supports hardware-specific optimizations and can be used to emit LLVM IR.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Code Generation based on Low-Level IR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The low-level IR adopted by most DL compilers can be eventually lowered to
    LLVM IR, and benefits from LLVM’s mature optimizer and code generator. Furthermore,
    LLVM can explicitly design custom instruction sets for specialized accelerators
    from scratch. However, traditional compilers may generate poor code when passed
    directly to LLVM IR. In order to avoid this situation, two approaches are applied
    by DL compilers to achieve hardware-dependent optimization: 1) perform target-specific
    loop transformation in the upper IR of LLVM (e.g., Halide-based IR and polyhedral-based
    IR), and 2) provide additional information about the hardware target for the optimization
    passes. Most DL compilers apply both approaches, but the emphasis is different.
    In general, the DL compilers that prefer frontend users (e.g., TC, TVM, XLA, and
    nGraph) might focus on 1), whereas the DL compilers that are more inclined to
    backend developers (e.g., Glow, PlaidML, and MLIR) might focus on 2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The compilation scheme in DL compilers can be mainly classified into two categories:
    just-in-time (JIT) and ahead-of-time (AOT). For JIT compilers, it can generate
    executable codes on the fly, and they can optimize codes with better runtime knowledge.
    AOT compilers generate all executable binaries first and then execute them. Thus
    they have a larger scope in static analysis than JIT compilation. In addition,
    AOT approaches can be applied with cross-compilers of embedded platforms (e.g.,
    C-GOOD (Kang et al., [2018](#bib.bib47))) as well as enable execution on remote
    machines (TVM RPC) and customized accelerators.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In DL compilers, the low-level IR is a fine-grained representation of DL models,
    and it reflects detailed implantation of DL models on diverse hardware. The low-level
    IRs include Halide-based IRs, polyhedral-based IRs, and other unique IRs. Although
    they differ in designs, they leverage the mature compiler tool-chains and infrastructure,
    to provide tailored interfaces of hardware-specific optimizations and code generation.
    The design of low-level IRs can also impact the design of new DL accelerators
    (e.g., TVM HalideIR and Inferentia, as well as XLA HLO and TPU).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Frontend Optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After constructing the computation graph, the frontend applies graph-level optimizations.
    Many optimizations are easier to be identified and performed at graph level because
    the graph provides a global view of the computation. These optimizations are only
    applied to the computation graph, rather than the implementations on backends.
    Thus they are hardware-independent and can be applied to various backend targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The frontend optimizations are usually defined by passes, and can be applied
    by traversing the nodes of the computation graph and performing the graph transformations.
    The frontend provides methods to 1) capture the specific features from the computation
    graph and 2) rewrite the graph for optimization. Besides the pre-defined passes,
    the developers can also define customized passes in the frontend. Most DL compilers
    can determine the shape of both input tensors and output tensors of every operation
    once a DL model is imported and transformed as a computation graph. This feature
    allows DL compilers to perform optimizations according to the shape information.
    Figure [3](#S4.F3 "Figure 3 ‣ 4.3\. Frontend Optimizations ‣ 4\. Key Components
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey") shows an
    example of computation graph optimizations with Tensorflow XLA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we classify the frontend optimizations into three categories:
    1) node-level optimizations, 2) block-level (peephole, local) optimizations, and
    3) dataflow-level (global) optimizations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f77908cf1eea0409c8df057ecb7795db.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Example of computation graph optimizations, taken from the HLO graph
    of Alexnet on Volta GPU using Tensorflow XLA.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1\. Node-level optimizations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The nodes of the computation graph are coarse enough to enable optimizations
    inside a single node. And the node-level optimizations include node elimination
    that eliminates unnecessary nodes and node replacement that replaces nodes with
    other lower-cost nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In general-purpose compilers, Nop Elimination removes the no-op instructions
    which occupy a small amount of space but specify no operation. In DL compilers,
    Nop Elimination is responsible for eliminating the operations lacking adequate
    inputs. For example, the sum node with only one input tensor can be eliminated,
    the padding node with zero padding width can be eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-dim-tensor elimination is responsible for removing the unnecessary operations
    whose inputs are zero-dimension tensors. Assume that $A$ is a zero-dimension tensor,
    and $B$ is a constant tensor, then the sum operation node of $A$ and $B$ can be
    replaced with the already existing constant node $B$ without affecting the correctness.
    Assume that $C$ is a 3-dimension tensor, but the shape of one dimension is zero,
    such as {0,2,3}, therefore, $C$ has no element, and the argmin/argmax operation
    node can be eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2\. Block-level optimizations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Algebraic simplification - The algebraic simplification optimizations consist
    of 1) algebraic identification, 2) strength reduction, with which we can replace
    more expensive operators by cheaper ones; 3) constant folding, with which we can
    replace the constant expressions by their values. Such optimizations consider
    a sequence of nodes, then take advantage of commutativity, associativity, and
    distributivity of different kinds of nodes to simplify the computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the typical operators ($+$, $\times$, etc.), the algebraic simplification
    can also be applied to DL specific operators (e.g., reshape, transpose, and pooling).
    The operators can be reordered and sometimes eliminated, which reduces redundancy
    and improves the efficiency. Here we illustrate the common cases where algebraic
    simplification can be applied: 1) optimization of computation order, in such case,
    the optimization finds and removes reshape/transpose operations according to specific
    characteristics. Taking the matrix multiplication (GEMM) for example, there are
    two matrices (e.g., $A$ and $B$), both matrices are transposed (to produce $A^{T}$
    and $B^{T}$, respectively), then $A^{T}$ and $B^{T}$ are multiplied together.
    However, a more efficient way to implement GEMM is to switch the order of the
    arguments $A$ and $B$, multiply them together, and then transpose the output of
    the GEMM, which reduces two transpose to just one; 2) optimization of node combination,
    in such case, the optimization combines multiple consecutive transpose nodes into
    a single node, eliminates identity transpose nodes, and optimizes transpose nodes
    into reshape nodes when they actually move no data; 3) optimization of ReduceMean
    nodes, in such case, the optimization performs substitutions of ReduceMean with
    AvgPool node (e.g., in Glow), if the input of the reduce operator is 4D with the
    last two dimensions to be reduced.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operator fusion - Operator fusion is indispensable optimization of DL compilers.
    It enables better sharing of computation, eliminates intermediate allocations,
    facilitates further optimization by combining loop nests (Roesch et al., [2019](#bib.bib79)),
    as well as reduces launch and synchronization overhead (Vasilache et al., [2018](#bib.bib92)).
    In TVM, the operators are classified into four categories: injective, reduction,
    complex-out-fusible, and opaque. When the operators are defined, their corresponding
    categories are determined. Targeting the above categories, TVM designs the fusion
    rules across operators. In TC, fusion is performed differently based on the automatic
    polyhedron transformations. However, how to identify and fuse more complicated
    graph patterns, such as blocks with multiple broadcast and reduce nodes, remains
    to be a problem. Recent works (Long et al., [2018](#bib.bib63), [2019](#bib.bib62))
    try to tackle this problem and propose a framework to explore and optimize aggressive
    fusion plans. It supports not only element-wise and reduction nodes, but also
    other computation/memory intensive nodes with complex dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: Operator sinking - This optimization sinks the operations such as transposes
    below operations such as batch normalization, ReLU, sigmoid, and channel shuffle.
    By this optimization, many similar operations are moved closer to each other,
    creating more opportunities for algebraic simplification.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. Dataflow-level optimizations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Common sub-expression elimination (CSE) - An expression $E$ is a common sub-expression
    if the value of $E$ is previously computed, and the value of $E$ has not to be
    changed since previous computation (Aho et al., [1986](#bib.bib7)). In this case,
    the value of $E$ is computed once, and the already computed value of $E$ can be
    used to avoid recomputing in other places. The DL compilers search for common
    sub-expressions through the whole computation graph and replace the following
    common sub-expressions with the previously computed results.
  prefs: []
  type: TYPE_NORMAL
- en: Dead code elimination (DCE) - A set of code is dead if its computed results
    or side-effects are not used. And the DCE optimization removes the dead code.
    The dead code is usually not caused by programmers but is caused by other graph
    optimizations. Thus, the DCE, as well as CSE, are applied after other graph optimizations.
    Other optimizations, such as dead store elimination (DSE), which removes stores
    into tensors that are never going to be used, also belong to DCE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Static memory planning - Static memory planning optimizations are performed
    to reuse the memory buffers as much as possible. Usually, there are two approaches:
    in-place memory sharing and standard memory sharing. The in-place memory sharing
    uses the same memory for input and output for an operation, and just allocates
    one copy of memory before computing. Standard memory sharing reuses the memory
    of previous operations without overlapping. The static memory planning is done
    offline, which allows more complicated planning algorithms to be applied. A recent
    work (Ahn et al., [2020a](#bib.bib5)) firstly designs and performs memory-aware
    scheduling to minimize the peak activation memory footprint on edge devices, which
    presents new research directions of memory planning on memory-constrained devices.'
  prefs: []
  type: TYPE_NORMAL
- en: Layout transformation - Layout transformation tries to find the best data layouts
    to store tensors in the computation graph and then inserts the layout transformation
    nodes to the graph. Note that the actual transformation is not performed here,
    instead, it will be performed when evaluating the computation graph by the compiler
    backend.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the performance of the same operation in different data layouts is
    different, and the best layouts are also different on different hardware. For
    example, operations in the NCHW format on GPU usually run faster, so it is efficient
    to transform to NCHW format on GPU (e.g., TensorFlow). Some DL compilers rely
    on hardware-specific libraries to achieve higher performance, and the libraries
    may require certain layouts. Besides, some DL accelerators prefer more complicated
    layouts (e.g., tile). In addition, edge devices usually equip heterogenous computing
    units, and different units may require different data layouts for better utilization,
    thus layout transformation needs careful considerations. Therefore, the compilers
    need to provide a way to perform layout transformations across various hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Not only the data layouts of tensors have a nontrivial influence on the final
    performance, but also the transformation operations have a significant overhead.
    Because they also consume the memory and computation resource.
  prefs: []
  type: TYPE_NORMAL
- en: A recent work (Liu et al., [2019](#bib.bib59)) based on TVM targeting on CPUs
    alters the layout of all convolution operations to NCHW[$x$]c first in the computation
    graph, in which c means the split sub-dimension of channel C and $x$ indicates
    the split size of the sub-dimension. Then all $x$ parameters are globally explored
    by auto-tuning when providing hardware details, such as cache line size, vectorization
    unit size, and memory access pattern, during hardware-specific optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The frontend is one of the most important components in DL compilers, which
    is responsible for transformation from DL models to high-level IR (e.g., computation
    graph) and hardware-independent optimizations based on high-level IR. Although
    the implementation of frontend may differ in the data representation and operator
    definition of high-level IR across DL compilers, the hardware-independent optimizations
    converge at three levels: node-level, block-level, and dataflow-level. The optimization
    methods at each level leverage the DL specific as well as general compilation
    optimization techniques, which reduce the computation redundancy as well as improve
    the performance of DL models at the computation graph level.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Backend Optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The backends of DL compilers have commonly included various hardware-specific
    optimizations, auto-tuning techniques, and optimized kernel libraries. Hardware-specific
    optimizations enable efficient code generation for different hardware targets.
    Whereas, auto-tuning has been essential in the compiler backend to alleviate the
    manual efforts to derive the optimal parameter configurations. Besides, highly-optimized
    kernel libraries are also widely used on general-purpose processors and other
    customized DL accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1\. Hardware-specific Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Hardware-specific optimizations, also known as target-dependent optimizations,
    are applied to obtain high-performance codes targeting specific hardware. One
    way to apply the backend optimizations is to transform the low-level IR into LLVM
    IR, to utilize the LLVM infrastructure to generate optimized CPU/GPU codes. The
    other way is to design customized optimizations with DL domain knowledge, leveraging
    the target hardware more efficiently. Since hardware-specific optimizations are
    tailored for particular hardware and cannot be included exhaustively in this paper,
    we present five widely adopted approaches in existing DL compilers. The overview
    of these hardware-specific optimizations is shown in Figure [4](#S4.F4 "Figure
    4 ‣ 4.4.1\. Hardware-specific Optimization ‣ 4.4\. Backend Optimizations ‣ 4\.
    Key Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey"),
    and the detailed descriptions are provided as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3460aa904a8e34fd5b6ea484a6a93545.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Overview of hardware-specific optimizations applied in DL compilers.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware intrinsic mapping - Hardware intrinsic mapping can transform a certain
    set of low-level IR instructions to kernels that have already been highly optimized
    on the hardware. In TVM, the hardware intrinsic mapping is realized in the method
    of extensible tensorization, which can declare the behavior of hardware intrinsic
    and the lowering rule for intrinsic mapping. This method enables the compiler
    backend to apply hardware implementations as well as highly optimized handcraft
    micro-kernels to a specific pattern of operations, which results in a significant
    performance gain. Whereas, Glow supports hardware intrinsic mapping such as quantization.
    It can estimate the possible numeric range for each stage of the neural network
    and support profile-guided optimization to perform quantization automatically.
    Besides, Halide/TVM maps specific IR patterns to SIMD opcodes on each architecture
    to avoid the inefficiency of LLVM IR mapping when encountering vector patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocation and fetching - Memory allocation is another challenge in code
    generation, especially for GPUs and customized accelerators. For example, GPU
    contains primarily shared memory space (lower access latency with limited memory
    size) and local memory space (higher access latency with large capacity). Such
    memory hierarchy requires efficient memory allocation and fetching techniques
    for improving data locality. To realize this optimization, TVM introduces the
    scheduling concept of memory scope. Memory scope schedule primitives can tag a
    compute stage as shared or thread-local. For compute stages tagged as shared,
    TVM generates code with shared memory allocation as well as cooperative data fetching,
    which inserts memory barrier at the proper code position to guarantee correctness.
    Besides, TC also provides similar features (known as memory promotion) by extending
    PPCG (Verdoolaege et al., [2013](#bib.bib98)) compiler. However, TC only supports
    limited pre-defined rules. Particularly, TVM enables special buffering in accelerators
    through memory scope schedule primitives.
  prefs: []
  type: TYPE_NORMAL
- en: Memory latency hiding - Memory latency hiding is also an important technique
    used in the backend by reordering the execution pipeline. As most DL compilers
    support parallelization on CPU and GPU, memory latency hiding can be naturally
    achieved by hardware (e.g., warp context switching on GPU). But for TPU-like accelerators
    with decoupled access-execute (DAE) architecture, the backend needs to perform
    scheduling and fine-grained synchronization to obtain correct and efficient codes.
    To achieve better performance as well as reduce programming burden, TVM introduces
    virtual threading schedule primitive, which enables users to specify the data
    parallelism on virtualized multi-thread architecture. Then TVM lowers these virtually
    parallelized threads by inserting necessary memory barriers and interleaves the
    operations from these threads into a single instruction stream, which forms a
    better execution pipeline of each thread to hide the memory access latency.
  prefs: []
  type: TYPE_NORMAL
- en: Loop oriented optimizations - Loop oriented optimizations are also applied in
    the backend to generate efficient codes for target hardware. Since Halide and
    LLVM (Lattner and Adve, [2004](#bib.bib52)) (integrated with the polyhedral method)
    have already incorporated such optimization techniques, some DL compilers leverage
    Halide and LLVM in their backends. The key techniques applied in loop oriented
    optimizations include loop fusion, sliding windows, tiling, loop reordering, and
    loop unrolling.
  prefs: []
  type: TYPE_NORMAL
- en: '1) Loop fusion: Loop fusion is a loop optimization technique that can fuse
    loops with the same boundaries for better data reuse. For compilers such as PlaidML,
    TVM, TC, and XLA, such optimization is performed by the Halide schedule or polyhedral
    approach, while Glow applies loop fusion by its operator stacking.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) Sliding windows: Sliding windows is a loop optimization technique adopted
    by Halide. Its central concept is to compute values when needed and store them
    on the fly for data reuse until they are no longer required. As sliding windows
    interleaves the computation of two loops and make them serial, it is a tradeoff
    between parallelism and data reuse.'
  prefs: []
  type: TYPE_NORMAL
- en: '3) Tiling: Tiling splits loops into several tiles, and thus loops are divided
    into outer loops iterating through tiles and inner loops iterating inside a tile.
    This transformation enables better data locality inside a tile by fitting a tile
    into hardware caches. As the size of a tile is hardware-specific, many DL compilers
    determine the tiling pattern and size by auto-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '4) Loop reordering: Loop reordering (also known as loop permutation) changes
    the order of iterations in a nested loop, which can optimize the memory access
    and thus increase the spatial locality. It is specific to data layout and hardware
    features. However, it is not safe to perform loop reordering when there are dependencies
    along the iteration order.'
  prefs: []
  type: TYPE_NORMAL
- en: '5) Loop unrolling: Loop unrolling can unroll a specific loop to a fixed number
    of copies of loop bodies, which allows the compilers to apply aggressive instruction-level
    parallelism. Usually, loop unrolling is applied in combination with loop split,
    which first splits the loop into two nested loops and then unrolls the inner loop
    completely.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallelization - As modern processors generally support multi-threading and
    SIMD parallelism, the compiler backend needs to exploit parallelism to maximize
    hardware utilization for high performance. Halide uses a schedule primitive called
    parallel to specify the parallelized dimension of the loop for thread-level parallelization
    and supports GPU parallelization by mapping loop dimensions tagged as parallel
    with annotation of block and thread. And it replaces a loop of size $n$ with a
    n-wide vector statement, which can be mapped to hardware-specific SIMD opcodes
    through hardware intrinsic mapping. Stripe develops a variant of the polyhedral
    model called nested polyhedral model, which introduces parallel polyhedral block
    as its basic execution element of iteration. After this extension, a nested polyhedral
    model can detect hierarchy parallelization among levels of tiling and striding.
    In addition, some DL compilers rely on handcraft libraries such as Glow or optimized
    math libraries provided by hardware vendors (discussed in Section [4.4.3](#S4.SS4.SSS3
    "4.4.3\. Optimized Kernel Libraries ‣ 4.4\. Backend Optimizations ‣ 4\. Key Components
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey")). In the
    meanwhile, Glow offloads the vectorization to LLVM because the LLVM auto-vectorizer
    works well when the information of tensor dimension and loop trip count is provided.
    However, exploiting the parallelism entirely by compiler backend allows to apply
    more domain-specific knowledge of DL models, and thus leads to higher performance
    at the expense of more engineering efforts.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2\. Auto-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the enormous search space for parameter tuning in hardware-specific optimizations,
    it is necessary to leverage auto-tuning to determine the optimal parameter configurations.
    Among the studied DL compilers in this survey, TVM, TC, and XLA support the auto-tuning.
    Generally, the auto-tuning implementation includes four key components, such as
    parameterization, cost model, searching technique, and acceleration. .
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameterization - 1) Data and target: The data parameter describes the specification
    of the data, such as input shapes. The target parameter describes hardware-specific
    characteristics and constraints to be considered during optimization scheduling
    and code generation. For example, for the GPU target, the hardware parameters
    such as shared memory and register size need to be specified. 2) Optimization
    options: The optimization options include the optimization scheduling and corresponding
    parameters, such as loop oriented optimizations and tile size. In TVM, both pre-defined
    and user-defined scheduling, as well as parameters, are taken into consideration.
    Whereas, TC and XLA prefer to parameterize the optimizations, which have a strong
    correlation with performance and can be changed later at a low cost. For example,
    the minibatch dimension is one of the parameters that is usually mapped to grid
    dimensions in CUDA and can be optimized during auto-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cost model - The comparison of different cost models applied in auto-tuning
    are as follows. 1) Black-box model: This model only considers the final execution
    time rather than the characteristics of the compilation task. It is easy to build
    a black-box model, but easily ends up with higher overhead and less optimal solution
    without the guidance of task characteristics. TC adopts this model. 2) ML-based
    cost model: ML-based cost model is a statistical approach to predict performance
    using a machine learning method. It enables the model to update as the new configuration
    is explored, which helps achieve higher prediction accuracy. TVM and XLA adopt
    this kind of model, for example, gradient tree boosting model (GBDT) and feedforward
    neural network (Kaufman et al., [2019](#bib.bib48)) (FNN) respectively. 3) Pre-defined
    cost model: An approach based on a pre-defined cost model expects a perfect model
    built on the characteristics of the compilation task and able to evaluate the
    overall performance of the task. Compared to the ML-based model, the pre-defined
    model generates less computation overhead when applied, but requires large engineering
    efforts for re-building the model on each new DL model and hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Searching technique - 1) Initialization and searching space determination:
    The initial option can either be set randomly or based on the known configurations,
    such as configurations given by users or historical optimal configurations. In
    terms of searching space, it should be specified before auto-tuning. TVM allows
    developers to specify the searching space with their domain-specific knowledge
    and provides automatic search space extraction for each hardware target based
    on the computational description. In contrast, TC relies on the compilation cache
    and the pre-defined rules. 2) Genetic algorithm (GA) (Goldberg, [1989](#bib.bib29)):
    GA considers each tuning parameter as genes and each configuration as a candidate.
    The new candidate is iteratively generated by crossover, mutation, and selection
    according to the fitness value, which is a metaheuristic inspired by the process
    of natural selection. And finally, the optimal candidate is derived. The rate
    of crossover, mutation, and selection is used for controlling the tradeoff between
    exploration and exploitation. TC adopts GA in its auto-tuning technique. 3) Simulated
    annealing algorithm (SA) (Bertsimas et al., [1993](#bib.bib13)): SA is also a
    metaheuristic inspired by annealing. It allows us to accept worse solutions in
    a decreasing probability, which can find the approximate global optimum and avoid
    the precise local optimum in a fixed amount of iterations. TVM adopts SA in its
    auto-tuning technique. 4) Reinforcement learning (RL): RL performs with learning
    to maximize reward given an environment by the tradeoff between exploration and
    exploitation. Chameleon (Ahn et al., [2020b](#bib.bib6)) (built upon TVM) adopts
    RLRL in its auto-tuning technique.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Acceleration - 1) Parallelization: One direction for accelerating auto-tuning
    is parallelization. TC proposes a multi-thread, multi-GPU strategy considering
    that the genetic algorithm needs to evaluate all candidates in each generation.
    First, it enqueues candidate configurations and compiles them on multiple CPU
    threads. The generated code is evaluated on GPUs in parallel, and each candidate
    owns its fitness used by the parent choosing step. After finishing the whole evaluation,
    the new candidate is generated, and the new compilation job is enqueued, waiting
    for compiling on CPU. Similarly, TVM supports cross-compilation and RPC, allowing
    users to compile on the local machine and run the programs with different auto-tuning
    configurations on multiple targets. 2) Configuration reuse: Another direction
    for accelerating auto-tuning is to reuse the previous auto-tuning configurations.
    TC stores the fastest known generated code version corresponding to the given
    configuration by compilation cache. The cache is queried before each kernel optimization
    during the compilation, and the auto-tuning is triggered if cache miss. Similarly,
    TVM produces a log file that stores the optimal configurations for all scheduling
    operators and queries the log file for best configurations during compilation.
    It is worth mentioning that TVM performs auto-tuning for each operator in Halide
    IR (e.g., conv2d), and thus the optimal configurations are determined for each
    operator separately.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3\. Optimized Kernel Libraries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are several highly-optimized kernel libraries widely used to accelerate
    DL training and inference on various hardware. DNNL (previously MKL-DNN) from
    Intel, cuDNN from NVIDIA, and MIOpen from AMD are widely used libraries. Both
    computation-intensive primitives (e.g., convolution, GEMM, and RNN) and memory
    bandwidth limited primitives (e.g., batch normalization, pooling, and shuffle)
    are highly optimized according to the hardware features (e.g., AVX-512 ISA, tensor
    cores). And customizable data layouts are supported to make it easy to integrate
    into DL applications and avoid frequent data layout transformations. Besides,
    low-precision training and inference, including FP32, FP16, INT8, and non-IEEE
    floating-point format bfloat16 (Kalamkar et al., [2019](#bib.bib46)) are also
    supported. Other customized DL accelerators also maintain their specific kernel
    libraries (Liu et al., [2016](#bib.bib58); Jia et al., [2019](#bib.bib44)).
  prefs: []
  type: TYPE_NORMAL
- en: Existing DL compilers, such as TVM, nGraph, and TC, can generate the function
    calls to these libraries during code generation. However, if DL compilers need
    to leverage the existing optimized kernel libraries, they should first transform
    the data layouts and fusion styles into the types that are pre-defined in kernel
    libraries. Such transformation may break the optimal control flow. Moreover, the
    DL compilers treat the kernel libraries as a black box. Therefore they are unable
    to apply optimizations across operators (e.g., operator fusion) when invoking
    kernel libraries. In sum, using optimized kernel libraries achieves significant
    performance improvement when the computation can be satisfied by specific highly-optimized
    primitives, otherwise it may be constrained from further optimization and suffer
    from less optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4\. Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The backend is responsible for bare-metal optimizations and code generation
    based on low-level IR. Although the design of backends may differ due to various
    low-level IRs, their optimizations can be classified into hardware-specific optimizations:
    auto-tuning techniques, and optimized kernel libraries. These optimizations can
    be performed separately or combined, to achieve better data locality and parallelization
    by exploiting the hardware/software characteristics. Eventually, the high-level
    IR of DL models is transformed into efficient code implementation on different
    hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Taxonomy of DL Compilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DL compilers studied in this survey include TVM, nGraph, Tensor Comprehension
    (TC), Glow, and XLA. We select these compilers since they are well-known, well
    maintained, and most importantly, widely used. Thus, we can find enough papers,
    documents, and discussions from both industry and academia in order to study their
    designs and implementations in-depth. Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey") illustrates
    the taxonomy of the selected DL compilers from four perspectives, including frontend,
    backend, IR, and optimizations, which corresponds with the key components described
    in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we provide more information about the compilers to the best of
    our knowledge. We not only provide whether a compiler supports a specific feature,
    but also describe how to use this feature through its programming interface. In
    addition, we also describe the developing status of specific features and the
    reasons why specific features are not supported in particular compilers. The target
    of this taxonomy is to provide guidelines about the selection of DL compilers
    for the practitioners considering their requirements, as well as to give a thorough
    summary of the DL compilers for researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey"), we present the features of each DL compiler,
    including developer, programming language, ONNX/framework support, training support,
    and quantization support in the frontend category, and we present the compilation
    methods and supported devices in the backend category. These features are summarized
    because they strongly affect the usage of DL compilers in particular scenarios.
    Based on these features, practitioners or researchers can easily decide which
    DL compiler they would like to work upon.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey"), together with Figure [2](#S3.F2 "Figure 2
    ‣ 3\. Common Design Architecture of DL Compilers ‣ The Deep Learning Compiler:
    A Comprehensive Survey") can serve as a systematic summary of this survey. Through
    them, readers can identify the features each compiler supports as well as the
    key components of each compiler. More detailed information is presented in the
    following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. The comparison of DL compilers, including TVM, nGraph, TC, Glow, and
    XLA.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | TVM | nGraph | TC | Glow | XLA |'
  prefs: []
  type: TYPE_TB
- en: '|  | Developer | Apache | Intel | Facebook | Facebook | Google |'
  prefs: []
  type: TYPE_TB
- en: '| Frontend | Programm- ing | Python/C++ Lambda expression | Python/C++ Tensor
    expression | Python/C++ Einstein notation | Python/C++ Layer programming | Python/C++
    Tensorflow interface |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX support | ✓ tvm.relay.frontend .from_onnx (built-in) | ✓ Use ngraph-onnx
    (Python package) | $\times$ | ✓ ONNXModelLoader (built-in) | ✓ Use tensorflow-onnx
    (Python package) |'
  prefs: []
  type: TYPE_TB
- en: '| Framework support | tvm.relay.frontend .from_* (built-in) tensorflow/tflite/keras
    pytorch/caffe2 mxnet/coreml/darknet | tensorflow paddlepaddle (Use *-bridge, act
    as the backend) | (Define and optimize a TC kernel, which is finally called by
    other frameworks.) pytorch/other DLPack supported frameworks | pytorch/caffe2
    tensorflowlite (Use built-in ONNXIFI interface) | Use tensorflow interface |'
  prefs: []
  type: TYPE_TB
- en: '| Training support | $\times$ Under developing (Support derivative operators
    now) | ✓ Only on NNP-T processor | ✓ (Support auto differentiation) | ✓ (Limited
    support) | ✓ Use tensorflow interface |'
  prefs: []
  type: TYPE_TB
- en: '| Quantization support | ✓ int8/fp16 | ✓ int8 (include training) | $\times$
    | ✓ int8 | ✓ int8/int16 (Use tensorflow interface) |'
  prefs: []
  type: TYPE_TB
- en: '| IR | High-/low- level IR | Relay/Halide | nGraph IR/None | TC IR/Polyhedral
    | Its own high-/low- level IR | HLO (Both high- and low- level) |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamic shape | ✓ (Any) | ✓ (PartialShape) | $\times$ | $\times$ | ✓ (None)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Optimization | Frontend opt | Hardware independent optimizations (refer to
    Section [4.3](#S4.SS3 "4.3\. Frontend Optimizations ‣ 4\. Key Components of DL
    Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey")) Hardware specific
    optimizations (refer to Section [4.4](#S4.SS4 "4.4\. Backend Optimizations ‣ 4\.
    Key Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey"))
    And hybrid optimizations |'
  prefs: []
  type: TYPE_TB
- en: '| Backend opt |'
  prefs: []
  type: TYPE_TB
- en: '| Autotuning | ✓ (To select the best schedule parameters) | $\times$ (Call
    optimized kernel libraries, no need) | ✓ (To reduce JIT overhead) | $\times$ (Additional
    info is already provided in IR) | ✓ (On default convolution and gemm ) |'
  prefs: []
  type: TYPE_TB
- en: '| Kernel libraries | ✓ mkl/cudnn/cublas | ✓ eigen/mkldnn/cudnn/ Others | $\times$
    | $\times$ | ✓ eigen/mkl/ cudnn/tensorrt |'
  prefs: []
  type: TYPE_TB
- en: '| Backend | Compilation methods | JIT AOT (experimental) | JIT | JIT | JIT
    AOT (Use built-in executable bundles) | JIT AOT (Generate executable libraries)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Supported devices | CPU/GPU/ARM FPGA/Customized ( Use VTA) | CPU/Intel GPU/NNP
    GPU/Customized ( Use OpenCL support in PlaidML) | Nvidia GPU | CPU/GPU Customized
    ( Official docs) | CPU/GPU/TPU Customized ( Official docs) |'
  prefs: []
  type: TYPE_TB
- en: 6\. Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1\. Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our experiments are conducted on two GPU-equipped machines, and the hardware
    configuration is shown in Table [2](#S6.T2 "Table 2 ‣ 6.1\. Experimental Setup
    ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"). We evaluate
    the performance of TVM (v0.6.0), nGraph (0.29.0-rc.0), TC (commit fd01443), Glow
    (commit 7e68188) and XLA (TensorFlow 2.2.0) on CPU and GPU. We select 19 neural
    network models in ONNX format as our datasets, which are converted from the Torchvison²²2[https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)
    model zoo and the GluonCV³³3[https://gluon-cv.mxnet.io/model_zoo/index.html](https://gluon-cv.mxnet.io/model_zoo/index.html)
    model zoo. These models include full-fledged models: ResNet, DenseNet and VGG
    series, and lightweight models: MobileNet and MNASNet series. To import the ONNX
    models, as shown in Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of DL Compilers ‣
    The Deep Learning Compiler: A Comprehensive Survey"), we use the built-in tvm.relay.frontend.from_onnx
    interface of TVM, the ngraph-onnx Python package of nGraph, the built-in ONNXModelLoader
    of Glow, and the tensorflow-onnx Python package of XLA. Notably, TC lacks the
    support of ONNX, so we only evaluate it in the following per-layer performance
    comparison. Each model is executed for 15 times, and we report the average execution
    time of the last 10 executions for each compiler, because we regard the first
    5 executions as the warm-up to eliminate the overhead of JIT compilation.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. The hardware configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | CPU | GPU |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Platform a |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Broadwell E5-2680v4 *2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (28 physical cores, 2.4GHz) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Tesla V100 32GB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (15.7TFlops, FP32) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Platform b |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Skylake Silver 4110 *2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (16 physical cores, 2.1GHz) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Turing RTX2080Ti 11GB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (13.4TFlops, FP32) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. End-to-end Performance Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a13ec83135e07c6fc575af1f9c49b860.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. The performance comparison of end-to-end inference across TVM, nGraph,
    Glow and XLA on CPU and GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [5](#S6.F5 "Figure 5 ‣ 6.2\. End-to-end Performance Comparison
    ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"), we compare
    the performance of end-to-end inference across TVM, nGraph, Glow, and XLA. We
    evaluate these compilers on both CPUs (Broadwell and Skylake) and GPUs (V100 and
    2080Ti). Note that, we omit the comparison of TC here. Because TC is more similar
    to a kernel library other than fully functional DL compiler, and it requires the
    users to implement all layers of a model with its Einstein notion manually, which
    leads to heavy engineering efforts for a fair comparison. Another reason is that
    TC only supports running on GPU, thus we cannot obtain its performance results
    on CPU. However, for detailed comparisons (Figure [6](#S6.F6 "Figure 6 ‣ 6.3\.
    Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler:
    A Comprehensive Survey") and [8](#S6.F8 "Figure 8 ‣ 6.3\. Per-layer Performance
    Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey")),
    we still implement several ResNet and MobileNetV2 models in TC. In sum, we compare
    and analyze the performance results from the following perspectives.'
  prefs: []
  type: TYPE_NORMAL
- en: Compatibility - Although nGraph and XLA claims to support ONNX , there are still
    compatibility problems. 1) nGraph fails to run the DenseNet121, VGG16/19 and MNASNet0_5/1_0
    models due to tensors with dynamic shapes. Alternatively, we replace the DenseNet121,
    VGG16/19 models with the corresponding models from the ONNX model zoo⁴⁴4[https://github.com/onnx/models](https://github.com/onnx/models),
    while MNASNet0_5/1_0 models are not available. Besides, when we set PlaidML as
    the backend of nGraph on GPU, we fail to run all MobileNet models. Because PlaidML
    cannot handle the inconsistent definition of operators across different DL frameworks.
    2) XLA can run all selected models, however, the performance is quite low. Thus,
    we replace the selected ONNX models with the savedmodels from the Tensorflow Hub⁵⁵5[https://tfhub.dev/](https://tfhub.dev/),
    while the MNASNet0_5/1_0 models are not available. With models from Tensorflow
    Hub, XLA becomes two orders of magnitude faster, and the performance of XLA becomes
    competitive with other compilers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance - From Figure [5](#S6.F5 "Figure 5 ‣ 6.2\. End-to-end Performance
    Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"),
    we have several observations about the performance illustrated as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 1) On CPU, the performance of Glow is worse than other compilers. This is because
    Glow does not support thread parallelism. Thus it cannot fully utilize the multi-core
    CPU. Whereas TVM, nGraph, and XLA can leverage all CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: 2) XLA has the similar end-to-end inference performance for both full-fledged
    models (ResNet, DenseNet and VGG series) and lightweight models (MobileNet and
    MNASNet series). Besides, its inference performance on CPU and GPU is almost the
    same. It is known that XLA is embedded in the Tensorflow framework. Tensorflow
    contains a complicated runtime compared to TVM, nGraph, and Glow, which introduces
    non-trivial overhead to XLA. In addition, if we increase the batch size (set to
    one by default in our evaluation) and focus on the throughput of DL compilers,
    then the overhead of XLA can be ignored with higher throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 3) In general, on CPU, TVM and nGraph achieve better performance across all
    models than other DL compilers, due to the limitations of Glow and XLA described
    above. TVM has comparable performance with nGraph on full-fledged models, while
    it is better than nGraph on lightweight models. nGraph relies on the DNNL (previously
    MKL-DNN) library for acceleration. Thus, nGraph can offload the optimized subgraphs
    to DNNL and benefit from DNNL’s fine-grained instruction-level JIT optimizations
    tailored for Intel CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 4) The tuned TVM (tuned with 200 trials) almost achieves the best performance
    on both CPU and GPU across all models, especially on lightweight models (MobileNet,
    MNASNet series). Based on our investigation, this is because the schedules of
    classic operators inside these models have already been well designed by TVM developers,
    with the default parameters provided in TVM tophub. The default schedules and
    parameters can help TVM to achieve similar performance compared to other DL compilers.
    In addition, the performance difference between the tuned TVM and untuned TVM
    is negligible on CPU but quite significant on GPU (41.26$\times$ speedup on average).
    This is because the GPU has more complicated thread and memory hierarchy than
    CPU, thus to exploit the computation power, GPU requires more fine-grained scheduling
    (e.g., tile, split, and reorder in TVM). Therefore, it is crucial to determine
    the optimal scheduling parameters on GPU, where the autotuning exhibits its effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Per-layer Performance Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To further compare the capability of backend optimizations of DL compilers,
    we evaluate the per-layer (convolution layers since they dominate the inference
    time) performance of the ResNet50 and MobileNetV2$\_$1.0 on V100 GPU and Broadwell
    CPU (single-threaded since Glow lacks multi-threading support).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b79c412dd420b4baa0c6baeef9408c52.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. The performance comparison of convolution layers in MobileNetV2$\_$1.0
    across TVM, TC, Glow and XLA on V100 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6205b3a72615e3eafcc799e8fac7de42.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. The performance comparison of convolution layers in MobileNetV2$\_$1.0
    across TVM, nGraph and Glow on Broadwell CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21fcf2de0bcd9e4217039daefc9d6b68.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8\. The performance comparison of convolution layers in ResNet50 across
    TVM, TC and Glow on V100 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c39ac7ebb44cd0048568b4261234d4af.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. The performance comparison of convolution layers in ResNet50 across
    TVM, nGraph and Glow on Broadwell CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Methodology - To measure the execution time of individual layers, we adopt different
    methods considering the DL compilers, the hardware (CPU/GPU), and the CNN models.
    Specifically, 1) On TVM, we re-use the logs of autotuning to extract the kernel
    shapes and the optimal schedule. Then we rebuild the individual convolution layers
    and use the time_evaluator for evaluation. 2) We extract the execution time through
    the tracing files of Glow. 3) And we measure the execution time of hand-written
    kernels on TC. 4) As for nGraph, we make use of the timeline to measure the execution
    time on CPU. However, the timeline is not supported by its PlaidML backend (which
    provides GPU support through OpenCL). Besides, there are no available methods
    to profile the command queues within OpenCL. Therefore, we leave the profiling
    of the per-layer performance of nGraph on GPU for future work. 4) As for XLA,
    we leverage the built-in tf.profiler.experimental method for CPU performance and
    the DLProf (NVIDIA, [2019a](#bib.bib72)) toolkit from Nvidia for GPU performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance - From Figure [6](#S6.F6 "Figure 6 ‣ 6.3\. Per-layer Performance
    Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"), [7](#S6.F7
    "Figure 7 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep
    Learning Compiler: A Comprehensive Survey") [8](#S6.F8 "Figure 8 ‣ 6.3\. Per-layer
    Performance Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive
    Survey"), [9](#S6.F9 "Figure 9 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\.
    Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"), we have several
    observations about the performance illustrated as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 1) nGraph achieves a better performance of the convolution layers on CPU, which
    benefits from the co-design of hardware (Intel CPU) and software (compiler, library,
    and runtime). Whereas, TVM performs better on GPU across these compilers. On MobileNetV2$\_$1.0,
    the performance of TVM is not stable, especially on conv1 layer. This is because
    the autotuning process is affected by other processes on the same machine, and
    thus it tends to derive the imprecise, even negative scheduling parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 2) TC allows users to define a tensor computation kernel (e.g., convolution)
    by the Einstein notion without specifying the shape of input/output tensors (e.g.,
    kernel size). Then the kernel is autotuned and stored in its compilation cache
    to accelerate further autotuning and compilation. However, in our evaluation,
    we find the performance of TC heavily relies on the initially compiled kernels.
    Take MobileNetV2$\_$1.0 for example, if we initialize the autotuning with layer
    c1, then c1 can perform well. But the following c$*\_$b$*\_*$ layers become much
    slower as the layers go deeper (far away from c1 layer). To derive a consistent
    performance, we need to tune each kernel separately.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Glow falls behind other compilers to optimize the $1\times 1$ convolutions
    (e.g., the b$*\_$linear layers) of MobileNetV2$\_$1.0 as well as the depth-wise
    separable convolutions (e.g., c$*\_$b$*\_$2 layers) of ResNet50. It takes a longer
    time to compute these convolutions both on GPU and CPU. We notice the convolutions
    are usually fused with other layers (e.g., ReLU, BatchNorm) on Glow, which could
    be why the lower performance compared to other compilers. Moreover, on CPU, the
    convolutions at the end of MobileNetV2$\_$1.0 take a quite shorter time than convolutions
    at the beginning. According to the tracing log, we notice these convolutions are
    accelerated by the CPUConvDKKC8 optimization (Rotem et al., [2018](#bib.bib80)),
    which applies tiling, layout transformation, and vectorization to convolutions
    with specific patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '4) As for XLA, it can automatically compile (_XlaCompile) the eligible subgraphs
    from Tensorflow and replace the subgraphs with the resultant binaries (_XlaRun).
    In addition, the convolution layers may be clustered with other kernels, and thus
    their performance is not easy to measure individually. Therefore, we have counted
    the clustered and the non-clustered convolutions, and the data is shown in Table [3](#S6.T3
    "Table 3 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep
    Learning Compiler: A Comprehensive Survey"). Note that the MobileNetV2$\_$1.0
    model in Tensorflow is a little bit different from the ONNX model for the beginning
    and ending layers, however, the linearbottleneck layers are the same. Moreover,
    if a convolution is to be clustered, it could be measured at most twice till the
    finishing of _XlaCompile. Therefore, there are five extreme value in Figure [6](#S6.F6
    "Figure 6 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep
    Learning Compiler: A Comprehensive Survey") (corresponding with 5 clustered convolutions
    in MobileNetV2$\_$1.0). Actually, only the clustered kernels are optimized by
    XLA, while the non-clustered ones are optimized by Tensorflow. Therefore, it is
    impossible to measure the execution time of a standalone convolution layer optimized
    by XLA. Consequently, we decide not to include the performance of XLA in Figure [7](#S6.F7
    "Figure 7 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep
    Learning Compiler: A Comprehensive Survey") - [9](#S6.F9 "Figure 9 ‣ 6.3\. Per-layer
    Performance Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. The number of the clustered and non-clustered convolutions of XLA
    on V100 GPU and Broadwell CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MobileNetV2_1.0 | ResNet50 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Clustered | Non-clu- | Clustered | Non-clu- |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V100 | 5 | 47 | 0 | 53 |'
  prefs: []
  type: TYPE_TB
- en: '| Broadwell | 17 | 35 | 53 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 6.4\. Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Through the above quantitative performance comparison across DL compilers, we
    can in-depth analyze the coarse-grained end-to-end performance with both frontend
    (graph-level) and backend (operator-level) optimizations, as well as the fine-grained
    per-layer performance about the convolutions with backend optimizations. However,
    there are still open challenges to accurately measure the effectiveness of the
    optimizations adopted by different DL compilers. One particular difficulty during
    our evaluation is that the frontend and backend optimizations are usually tightly
    coupled in existing DL compilers, because 1) the frontend optimizations usually
    affect a series of operators. Thus the optimized operators as the inputs to the
    backend optimizations differ across different compilers; 2) these optimizations
    tend to be co-designed for further exploit the performance opportunities (e.g.,
    clustering in XLA and more advanced optimizations (Long et al., [2019](#bib.bib62);
    Liu et al., [2019](#bib.bib59))). Therefore, it is difficult if not impossible
    to evaluate and compare specific optimizations across DL compilers individually.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this problem, we have been working on building a universal benchmarking
    framework for existing DL compilers to measure the per-layer performance. The
    fundamental idea is to extract the necessary structures and parameters of the
    target layers (we name them as model fragments), and rebuild the layers as acceptable
    inputs to a particular DL compiler, which allows the compiler to apply corresponding
    frontend and backend optimizations faithfully. We can then measure the performance
    of these optimized model fragments to understand the effectiveness of DL compilers
    at layers of interests. The benchmarking framework using model fragments is scalable
    to customized layers (e.g., fused layers) of interest. With such benchmarking
    framework available, we can derive both coarse-grained (e.g., end-to-end) and
    fine-grained (e.g., per-layer) performance metrics for each DL compiler, and thus
    compare the effectiveness of optimizations across different DL compilers at the
    level of interest. Currently, we have successfully experimented by extracting
    the target layers from the state-of-the-art CNN models, such as the bottleneck
    of ResNet50 and the linearbottleneck of MobileNetV2_1.0. Our benchmarking framework
    is still under rapid development, and we hope to make it available to the community
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we present a thorough analysis of the existing DL compilers
    targeting the design principles. First, we take a deep dive into the common architecture
    adopted in the existing DL compilers including the multi-level IR, the frontend
    and the backend. We present the design philosophies and reference implementations
    of each component in detail, with the emphasis on the unique IRs and optimizations
    specific to DL compilers. We summarize the findings in this survey and highlight
    the future directions in DL compiler as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic shape and pre/post processing - Dynamic model becomes more and more
    popular in the field of DL, whose input shape or even model itself may change
    during execution. Particularly, in the area of NLP, models may accept inputs of
    various shapes, which is challenging for DL compilers since the shape of data
    is unknown until runtime. Existing DL compilers require more research efforts
    to support dynamic shape efficiently for emerging dynamic models.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, as future DL models become more complex, their entire control flow
    may inevitably include complicated pre/post-processing procedures. Currently,
    most DL compilers use Python as their programming language, the pre/post-processing
    could become a performance bottleneck when it is executed by the Python interpreter.
    Such potential performance bottleneck has not yet been considered by existing
    DL compilers. Supporting the entire control flow in DL compiler enables express
    and optimize the pre/post-processing along with DL models, which opens up new
    opportunities for performance acceleration in model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced auto-tuning - Existing auto-tuning techniques focus on the optimization
    of individual operators. However, the combination of the local optimal does not
    lead to global optimal. For example, two adjacent operators that apply on different
    data layouts can be tuned together without introducing extra memory transformations
    in between. Besides, with the rise of edge computing, execution time is not only
    the optimization objective for DL compilers. New optimization targets should also
    be considered in the auto-tuning such as memory footprint and energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Particularly, for the ML-based auto-tuning techniques, there are several directions
    worth further exploring. First, the ML techniques can be applied in other stages
    of auto-tuning, other than the cost model. For example, in the stage of selecting
    compiler options and optimization schedules, ML techniques can be used to predict
    the possibility directly and develop algorithms to determine the final configurations.
    Second, the ML-based auto-tuning techniques can be improved based on the domain
    knowledge. For example, incorporating the feature engineering (selecting features
    to represent program) (Wang and O’Boyle, [2018](#bib.bib100)) in auto-tuning techniques
    could be a potential direction for achieving better tuning results.
  prefs: []
  type: TYPE_NORMAL
- en: Polyhedral model - It is a promising research direction to combine polyhedral
    model and auto-tuning techniques in the design of DL compilers for efficiency.
    On one hand, the auto-tuning can be applied to minimize the overhead of polyhedral
    JIT compilation by reusing the previous configurations. On the other hand, the
    polyhedral model can be used to perform auto-scheduling, which can reduce the
    search space of auto-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge of applying polyhedral model in DL compilers is to support
    the sparse tensor. In general, the format of a sparse tensor such as CSF (Smith
    and Karypis, [2015](#bib.bib85)) expresses the loop indices with index arrays
    (e.g., $a[b[i]]$) that is no longer linear. Such indirect index addressing leads
    to non-affine subscript expressions and loop bounds, which prohibits the loop
    optimization of the polyhedral model (Vasilache et al., [2006](#bib.bib91); Chen,
    [2012](#bib.bib15)). Fortunately, the polyhedral community has made progress in
    supporting sparse tensor (Venkat et al., [2014](#bib.bib96), [2015](#bib.bib95)),
    and integrating the latest advancement of the polyhedral model can increase the
    performance opportunities for DL compilers.
  prefs: []
  type: TYPE_NORMAL
- en: Subgraph partitioning - DL compilers supporting subgraph partitioning can divide
    the computation graph into several subgraphs, and the subgraphs can be processed
    in different manners. The subgraph partitioning presents more research opportunities
    for DL compilers. First, it opens up the possibility to integrate graph libraries
    for optimization. Take nGraph and DNNL for example, DNNL is a DL library with
    graph optimizations leveraging vast collection of highly optimized kernels. The
    integration of DNNL with nGraph enables DNNL to speedup the execution of the subgraphs
    generated by nGraph. Secondly, it opens up the possibility of heterogeneous and
    parallel execution. Once the computation graph is partitioned into subgraphs,
    the execution of different subgraphs can be assigned to heterogeneous hardware
    targets at the same time. Take the edge device for example, its computation units
    may consist of ARM CPU, Mail GPU, DSP, and probably NPU. Generating subgraphs
    from the DL compilers that utilizes all computation units efficiently can deliver
    significant speedup of the DL tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization - Traditional quantization strategies applied in DL frameworks
    are based on a set of fixed schemes and datatypes with little customization for
    codes running on different hardware. Whereas, supporting quantization in DL compilers
    can leverage optimization opportunities during compilation to derive more efficient
    quantization strategies. For example, Relay (Roesch et al., [2019](#bib.bib79))
    provides a quantization rewriting flow that can automatically generate quantized
    code for various schemes.
  prefs: []
  type: TYPE_NORMAL
- en: To support quantization, there are several challenges to be solved in DL compilers.
    The first challenge is how to implement new quantized operators without heavy
    engineering efforts. The attempt from AWS points out a possible direction that
    uses the concept of dialect to implement new operators upon basic operators, so
    that the optimizations at graph level and operator level can be reused. The second
    challenge is the interaction between quantization and other optimizations during
    compilation. For example, determining the appropriate stage for quantization and
    collaborating with optimizations such as operator fusion require future research
    investigations.
  prefs: []
  type: TYPE_NORMAL
- en: Unified optimizations - Although existing DL compilers adopt similar designs
    in both computation graph optimizations and hardware-specific optimizations, each
    compiler has its own advantages in certain aspects. There is a missing way to
    share the state-of-the-art optimizations, as well as support of emerging hardware
    targets across existing compilers. We advocate unifying the optimizations from
    existing DL compilers so that the best practices adopted in each DL compiler can
    be reused. In addition, unifying the optimizations across DL compilers can accumulate
    a strong force to impact the design of general-purpose and dedicated DL accelerators,
    and provide an environment for efficient co-design of DL compiler and hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, Google MLIR is a promising initiative towards such direction. It
    provides the infrastructure of multi-level IRs, and contains IR specification
    and toolkit to perform transformations across IRs at each level. It also provides
    flexible dialects, so that each DL compiler can construct its customized dialects
    for both high-level and low-level IRs. Through transformation across dialects,
    optimizations of one DL compiler can be reused by another compiler. However, the
    transformation of dialects requires further research efforts to reduce the dependency
    on delicate design.
  prefs: []
  type: TYPE_NORMAL
- en: Differentiable programming - Differentiable programming is a programming paradigm,
    where the programs are differentiable thoroughly. Algorithms written in differentiable
    programming paradigm can be automatically differentiated, which is attractive
    for DL community. Many compiler projects have adopted differentiable programming,
    such as Myia (van Merriënboer et al., [2018](#bib.bib90)), Flux (Innes et al.,
    [2018](#bib.bib41)) and Julia (Bezanson et al., [2017](#bib.bib14)). Unfortunately,
    there is little support for differential programming in existing DL compilers.
  prefs: []
  type: TYPE_NORMAL
- en: To support differential programming is quite challenging for existing DL compilers.
    The difficulties come from not only data structure, but also language semantic.
    For example, to realize the transformation from Julia to XLA HLO IR, one of the
    challenges (Fischer and Saba, [2018](#bib.bib25)) is that the control flow is
    different between the imperative language used by Julia and the symbolic language
    used by XLA. In order to use HLO IR efficiently, the compiler also needs to provide
    operation abstraction for Julia in order to support the particular semantic of
    XLA, such as MapReduce and broadcast. Moreover, the semantic difference of differentiation
    between Julia and XLA, also requires significant changes of compiler designs.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy protection - In edge-cloud system, the DL models are usually split into
    two halves with each partial model running on the edge device and cloud service
    respectively, which can provide better response latency and consume less communication
    bandwidth. However, one of the drawbacks with the edge-cloud system is that the
    user privacy becomes vulnerable. The reason is that the attackers can intercept
    the intermediate results sent from the edge devices to cloud, and then use the
    intermediate results to train another model that can reveal the privacy information
    deviated from the original user task.
  prefs: []
  type: TYPE_NORMAL
- en: To protect privacy in edge-cloud system, existing approaches (Mireshghallah
    et al., [2020](#bib.bib68); Osia et al., [2018](#bib.bib75); Gao et al., [2019](#bib.bib28))
    propose to add noise with special statistic properties to the intermediate results
    that can reduce the accuracy of the attacker task without severely deteriorating
    the accuracy of the user task. However, the difficulty is to determine the layer
    where the noise should be inserted, which is quite labor intensive to identify
    the optimal layer. The above difficulty presents a great opportunity for DL compilers
    to support privacy protection, because the compilers maintain rich information
    of the DL model, which can guide the noise insertion across layers automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training support - In general, the model training is far less supported in
    current DL compilers. As shown in Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of
    DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey"), nGraph only
    supports training on the Intel NNP-T accelerator, TC only supports the auto differentiation
    of a single kernel, Glow has experimental training support for limited models,
    the training support of TVM is under development, while XLA relies on the training
    support of TensorFlow. In sum, current DL compilers mainly focus on bridging the
    gap of deploying DL models onto diverse hardware efficiently, and thus they choose
    inference as their primary optimization targets. However, expanding the capability
    of DL compilers to support model training would open up a large body of research
    opportunities such as optimization of gradient operators and high-order auto differentiation.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank Jun Yang from Alibaba, Yu Xing from Xilinx,
    and Byung Hoon Ahn from UCSD for their valuable comments and suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In
    *12th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation
    ($\{$OSDI$\}$ 16)*. USENIX Association, Savannah, GA, USA, 265–283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abdelouahab et al. (2017) Kamel Abdelouahab, Maxime Pelcat, Jocelyn Serot, Cedric
    Bourrasset, and François Berry. 2017. Tactics to directly map CNN graphs on embedded
    FPGAs. *IEEE Embedded Systems Letters* 9, 4 (2017), 113–116.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abelson et al. (1998) Harold Abelson, R. Kent Dybvig, Christopher T. Haynes,
    Guillermo Juan Rozas, NI Adams, Daniel P. Friedman, E Kohlbecker, GL Steele, David H
    Bartley, Robert Halstead, et al. 1998. Revised 5 report on the algorithmic language
    Scheme. *Higher-order and symbolic computation* 11, 1 (1998), 7–105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahn et al. (2020a) Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin, Hsin-Pai Cheng,
    Jilei Hou, and Hadi Esmaeilzadeh. 2020a. Ordering Chaos: Memory-Aware Scheduling
    of Irregularly Wired Neural Networks for Edge Devices. arXiv:cs.DC/2003.02369'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahn et al. (2020b) Byung Hoon Ahn, Prannoy Pilligundla, Amir Yazdanbakhsh,
    and Hadi Esmaeilzadeh. 2020b. Chameleon: Adaptive Code Optimization for Expedited
    Deep Neural Network Compilation. arXiv:cs.LG/2001.08743'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aho et al. (1986) Alfred V Aho, Ravi Sethi, and Jeffrey D Ullman. 1986. Compilers,
    principles, techniques. *Addison wesley* 7, 8 (1986), 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alibaba (2019) Alibaba. 2019. Announcing Hanguang 800: Alibaba’s First AI-Inference
    Chip. [https://www.alibabacloud.com/blog/announcing-hanguang-800-alibabas-first-ai-inference-chip_595482](https://www.alibabacloud.com/blog/announcing-hanguang-800-alibabas-first-ai-inference-chip_595482).
    Accessed February 4, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon (2018) Amazon. 2018. AWS Inferentia. [https://aws.amazon.com/machine-learning/inferentia](https://aws.amazon.com/machine-learning/inferentia).
    Accessed February 4, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bagnara et al. (2006) Roberto Bagnara, Patricia M Hill, and Enea Zaffanella.
    2006. The Parma Polyhedra Library: Toward a complete set of numerical abstractions
    for the analysis and verification of hardware and software systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahrampour et al. (2015) Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott,
    and Mohak Shah. 2015. Comparative Study of Deep Learning Software Frameworks.
    arXiv:cs.LG/1511.06435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baidu (2016) Baidu. 2016. PaddlePaddle Github repository. [https://github.com/PaddlePaddle/Paddle](https://github.com/PaddlePaddle/Paddle).
    Accessed February 4, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertsimas et al. (1993) Dimitris Bertsimas, John Tsitsiklis, et al. 1993. Simulated
    annealing. *Statistical science* 8, 1 (1993), 10–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bezanson et al. (2017) Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B
    Shah. 2017. Julia: A fresh approach to numerical computing. *SIAM review* 59,
    1 (2017), 65–98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen (2012) Chun Chen. 2012. Polyhedra scanning revisited. In *Proceedings of
    the 33rd ACM SIGPLAN conference on Programming Language Design and Implementation*.
    ACM, Beijing, China, 499–508.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018a) Hongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona,
    and Thomas Blaschke. 2018a. The rise of deep learning in drug discovery. *Drug
    discovery today* 23, 6 (2018), 1241–1250.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2015) Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie
    Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexible
    and efficient machine learning library for heterogeneous distributed systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018b) Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng,
    Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al.
    2018b. $\{$TVM$\}$: An automated end-to-end optimizing compiler for deep learning.
    In *13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation
    ($\{$OSDI$\}$ 18)*. USENIX Association, Carlsbad, CA, USA, 578–594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chetlur et al. (2014) Sharan Chetlur, Cliff Woolley, Philippe Vandermersch,
    Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: Efficient
    Primitives for Deep Learning. arXiv:cs.NE/1410.0759'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet et al. (2015) François Chollet et al. 2015. Keras. [https://keras.io](https://keras.io).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collobert et al. (2011) R. Collobert, K. Kavukcuoglu, and C. Farabet. 2011.
    Torch7: A Matlab-like Environment for Machine Learning. In *BigLearn, NIPS Workshop*.
    Curran Associates, Granada, Spain, 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cyphers et al. (2018) Scott Cyphers, Arjun K Bansal, Anahita Bhiwandiwalla,
    Jayaram Bobba, Matthew Brookhart, Avijit Chakraborty, Will Constable, Christian
    Convey, Leona Cook, Omar Kanawi, et al. 2018. Intel ngraph: An intermediate representation,
    compiler, and executor for deep learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cytron et al. (1991) Ron Cytron, Jeanne Ferrante, Barry K Rosen, Mark N Wegman,
    and F Kenneth Zadeck. 1991. Efficiently computing static single assignment form
    and the control dependence graph. *ACM Transactions on Programming Languages and
    Systems (TOPLAS)* 13, 4 (1991), 451–490.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feautrier (1988) P. Feautrier. 1988. Parametric integer programming. *RAIRO
    Recherche Opérationnelle* 22, 3 (1988), 243–268.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fischer and Saba (2018) Keno Fischer and Elliot Saba. 2018. Automatic full compilation
    of julia programs and ML models to cloud TPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fonnegra et al. (2017) Rubén D Fonnegra, Bryan Blair, and Gloria M Díaz. 2017.
    Performance comparison of deep learning frameworks in image classification problems
    using convolutional and recurrent networks. In *2017 IEEE Colombian Conference
    on Communications and Computing (COLCOM)*. IEEE, Cartagena, Colombia, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Forsyth and Ponce (2002) David A Forsyth and Jean Ponce. 2002. *Computer vision:
    a modern approach*. Prentice Hall Professional Technical Reference, Upper Saddle
    River, NJ, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2019) Ruiyuan Gao, Ming Dun, Hailong Yang, Zhongzhi Luan, and Depei
    Qian. 2019. Privacy for Rescue: A New Testimony Why Privacy is Vulnerable In Deep
    Models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goldberg (1989) David E. Goldberg. 1989. *Genetic Algorithms in Search, Optimization
    and Machine Learning* (1st ed.). Addison-Wesley Longman Publishing Co., Inc.,
    USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
    2014. Generative Adversarial Networks. arXiv:stat.ML/1406.2661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodman (2007) Danny Goodman. 2007. *JavaScript bible*. John Wiley & Sons, Hoboken,
    NJ, USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grosser (2000) Tobias Grosser. 2000. Polyhedral Compilation. [https://polyhedral.info](https://polyhedral.info).
    Accessed February 4, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan et al. (2017) Yijin Guan, Hao Liang, Ningyi Xu, Wenqiang Wang, Shaoshuai
    Shi, Xi Chen, Guangyu Sun, Wei Zhang, and Jason Cong. 2017. FP-DNN: An automated
    framework for mapping deep neural networks onto FPGAs with RTL-HLS hybrid templates.
    In *2017 IEEE 25th Annual International Symposium on Field-Programmable Custom
    Computing Machines (FCCM)*. IEEE, IEEE Computer Society, Napa, CA, USA, 152–159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2017a) Kaiyuan Guo, Lingzhi Sui, Jiantao Qiu, Jincheng Yu, Junbin
    Wang, Song Yao, Song Han, Yu Wang, and Huazhong Yang. 2017a. Angel-Eye: A complete
    design flow for mapping CNN onto embedded FPGA. *IEEE Transactions on Computer-Aided
    Design of Integrated Circuits and Systems* 37, 1 (2017), 35–47.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2017b) Kaiyuan Guo, Shulin Zeng, Jincheng Yu, Yu Wang, and Huazhong
    Yang. 2017b. A Survey of FPGA-Based Neural Network Accelerator. arXiv:cs.AR/1712.08934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2018) Qianyu Guo, Xiaofei Xie, Lei Ma, Qiang Hu, Ruitao Feng, Li
    Li, Yang Liu, Jianjun Zhao, and Xiaohong Li. 2018. An Orchestrated Empirical Study
    on Deep Learning Frameworks and Platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ha et al. (2016) Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. 2016. Large-scale
    item categorization in e-commerce using multiple recurrent neural networks. In
    *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*. ACM, San Francisco, CA, USA, 107–115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hatcher and Yu (2018) William Grant Hatcher and Wei Yu. 2018. A survey of deep
    learning: platforms, applications and emerging research trends. *IEEE Access*
    6 (2018), 24411–24432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard et al. (2018) Jeremy Howard et al. 2018. fastai. [https://github.com/fastai/fastai](https://github.com/fastai/fastai).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Innes et al. (2018) Michael Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi,
    Marco Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal Singh, and
    Viral Shah. 2018. Fashionable Modelling with Flux.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intel (2019) Intel. 2019. Nervana Neural Network Processor. [https://www.intel.ai/nervana-nnp/](https://www.intel.ai/nervana-nnp/).
    Accessed February 4, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. (2014) Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev,
    Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe:
    Convolutional architecture for fast feature embedding. In *Proceedings of the
    22nd ACM international conference on Multimedia*. ACM, ACM, Orlando, FL, USA,
    675–678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2019) Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo
    Scarpazza. 2019. Dissecting the Graphcore IPU Architecture via Microbenchmarking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jouppi et al. (2017) Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson,
    Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers,
    et al. 2017. In-datacenter performance analysis of a tensor processing unit. In
    *Proceedings of the 44th Annual International Symposium on Computer Architecture*.
    ACM, Toronto, ON, Canada, 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalamkar et al. (2019) Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi,
    Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj
    Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander
    Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
    Bharat Kaul, and Pradeep Dubey. 2019. A Study of BFLOAT16 for Deep Learning Training.
    arXiv:cs.LG/1905.12322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2018) Duseok Kang, Euiseok Kim, Inpyo Bae, Bernhard Egger, and
    Soonhoi Ha. 2018. C-GOOD: C-code generation framework for optimized on-device
    deep learning. In *Proceedings of the International Conference on Computer-Aided
    Design*. ACM, ACM, San Diego, CA, USA, 105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaufman et al. (2019) Samuel Kaufman, Phitchaya Mangpo Phothilimthana, and Mike
    Burrows. 2019. Learned TPU Cost Model for XLA Tensor Programs. In *Proceedings
    of the Workshop on ML for Systems at NeurIPS 2019*. Curran Associates, Vancouver,
    Canada, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kelly et al. (1996) Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana
    Shpeisman, and Dave Wonnacott. 1996. The Omega calculator and library, version
    1.1\. 0. *College Park, MD* 20742 (1996), 18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingsley-Hughes (2017) Adrian Kingsley-Hughes. 2017. Inside Apple’s new A11
    Bionic processor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuck et al. (1981) D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe.
    1981. Dependence Graphs and Compiler Optimizations. In *Proceedings of the 8th
    ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages* *(POPL ’81)*.
    Association for Computing Machinery, New York, NY, USA, 207–218. [https://doi.org/10.1145/567532.567555](https://doi.org/10.1145/567532.567555)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lattner and Adve (2004) Chris Lattner and Vikram Adve. 2004. LLVM: A compilation
    framework for lifelong program analysis & transformation. In *Proceedings of the
    international symposium on Code generation and optimization: feedback-directed
    and runtime optimization*. IEEE Computer Society, IEEE Computer Society, San Jose,
    CA, USA, 75.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lattner et al. (2020) Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen,
    Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache,
    and Oleksandr Zinenko. 2020. MLIR: A Compiler Infrastructure for the End of Moore’s
    Law. arXiv:cs.PL/2002.11054'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leary and Wang (2017) Chris Leary and Todd Wang. 2017. XLA: TensorFlow, compiled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (1998), 2278–2324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong
    Yang, Zhongzhi Luan, and Depei Qian. 2020. The Deep Learning Compiler: A Comprehensive
    Survey.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2019) Heng Liao, Jiajin Tu, Jing Xia, and Xiping Zhou. 2019. DaVinci:
    A Scalable Architecture for Neural Network Computing. In *2019 IEEE Hot Chips
    31 Symposium (HCS)*. IEEE, IEEE, Cupertino, CA, USA, 1–44.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2016) S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y. Xie, Y. Chen, and
    T. Chen. 2016. Cambricon: An Instruction Set Architecture for Neural Networks.
    In *2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture
    (ISCA)*. IEEE Computer Society, Seoul, South Korea, 393–405. [https://doi.org/10.1109/ISCA.2016.42](https://doi.org/10.1109/ISCA.2016.42)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Yizhi Liu, Yao Wang, Ruofei Yu, Mu Li, Vin Sharma, and Yida
    Wang. 2019. Optimizing $\{$CNN$\}$ Model Inference on CPUs. In *2019 $\{$USENIX$\}$
    Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 19)*. USENIX Association,
    Renton, WA, USA, 1025–1040.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2016) Zhiqiang Liu, Yong Dou, Jingfei Jiang, and Jinwei Xu. 2016.
    Automatic code generation of convolutional neural networks in FPGA implementation.
    In *2016 International Conference on Field-Programmable Technology (FPT)*. IEEE,
    IEEE, Xi’an, China, 61–68.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loechner (1999) Vincent Loechner. 1999. PolyLib: A library for manipulating
    parameterized polyhedra. [https://repo.or.cz/polylib.git/blob_plain/HEAD:/doc/parampoly-doc.ps.gz](https://repo.or.cz/polylib.git/blob_plain/HEAD:/doc/parampoly-doc.ps.gz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Long et al. (2019) Guoping Long, Jun Yang, and Wei Lin. 2019. FusionStitching:
    Boosting Execution Efficiency of Memory Intensive Computations for DL Workloads.
    arXiv:cs.DC/1911.11576'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Long et al. (2018) Guoping Long, Jun Yang, Kai Zhu, and Wei Lin. 2018. FusionStitching:
    Deep Fusion and Code Generation for Tensorflow Computations on GPUs. arXiv:cs.DC/1811.05213'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2018) Yufei Ma, Naveen Suda, Yu Cao, Sarma Vrudhula, and Jae-sun
    Seo. 2018. ALAMO: FPGA acceleration of deep learning algorithms with a modularized
    RTL compiler. *Integration* 62 (2018), 14–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manning et al. (1999) Christopher D Manning, Christopher D Manning, and Hinrich
    Schütze. 1999. *Foundations of statistical natural language processing*. MIT press,
    Cambridge, MA, USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCarthy and Levin (1965) John McCarthy and Michael I Levin. 1965. LISP 1.5
    programmer’s manual.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft (2017) Microsoft. 2017. ONNX Github repository. [https://github.com/onnx/onnx](https://github.com/onnx/onnx).
    Accessed February 4, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mireshghallah et al. (2020) Fatemehsadat Mireshghallah, Mohammadkazem Taram,
    Prakash Ramrakhyani, Ali Jalali, Dean Tullsen, and Hadi Esmaeilzadeh. 2020. Shredder:
    Learning noise distributions to protect inference privacy. In *Proceedings of
    the Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems*. ACM, Lausanne, Switzerland, 3–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohammadi et al. (2017) Mehdi Mohammadi, Ala Al-Fuqaha, Mohsen Guizani, and
    Jun-Seok Oh. 2017. Semisupervised deep reinforcement learning in support of IoT
    and smart city services. *IEEE Internet of Things Journal* 5, 2 (2017), 624–635.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MXNet (2017) MXNet. 2017. Gluon. [https://gluon.mxnet.io](https://gluon.mxnet.io).
    Accessed February 4, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nara et al. (2019) Madhumitha Nara, BR Mukesh, Preethi Padala, and Bharath Kinnal.
    2019. Performance Evaluation of Deep Learning frameworks on Computer Vision problems.
    In *2019 3rd International Conference on Trends in Electronics and Informatics
    (ICOEI)*. IEEE, Tirunelveli, India, India, 670–674.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2019a) NVIDIA. 2019a. DLProf User-guide. [https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/](https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/).
    Accessed August 26, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2019b) NVIDIA. 2019b. Nvidia Turing Architecture. [https://www.nvidia.com/en-us/design-visualization/technologies/turing-architecture/](https://www.nvidia.com/en-us/design-visualization/technologies/turing-architecture/).
    Accessed February 4, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2019c) NVIDIA. 2019c. TensorRT Github repository. [https://github.com/NVIDIA/TensorRT](https://github.com/NVIDIA/TensorRT).
    Accessed February 4, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Osia et al. (2018) Seyed Ali Osia, Ali Taheri, Ali Shahin Shamsabadi, Kleomenis
    Katevas, Hamed Haddadi, and Hamid R Rabiee. 2018. Deep private-feature extraction.
    *IEEE Transactions on Knowledge and Data Engineering* 32, 1 (2018), 54–66.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. PyTorch: An imperative style, high-performance deep learning
    library. In *Advances in Neural Information Processing Systems*. Curran Associates,
    Vancouver, BC, Canada, 8024–8035.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Petricek and Syme (2012) Tomas Petricek and Don Syme. 2012. Syntax Matters:
    Writing abstract computations in F#.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ragan-Kelley et al. (2013) Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams,
    Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: A Language and
    Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing
    Pipelines. In *Proceedings of the 34th ACM SIGPLAN Conference on Programming Language
    Design and Implementation* *(PLDI ’13)*. Association for Computing Machinery,
    New York, NY, USA, 519–530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roesch et al. (2019) Jared Roesch, Steven Lyubomirsky, Marisa Kirisame, Logan
    Weber, Josh Pollock, Luis Vega, Ziheng Jiang, Tianqi Chen, Thierry Moreau, and
    Zachary Tatlock. 2019. Relay: A High-Level Compiler for Deep Learning. arXiv:cs.LG/1904.08368'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rotem et al. (2018) Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron,
    Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein,
    Jack Montgomery, Bert Maher, Satish Nadathur, Jakob Olesen, Jongsoo Park, Artem
    Rakhov, Misha Smelyanskiy, and Man Wang. 2018. Glow: Graph Lowering Compiler Techniques
    for Neural Networks. arXiv:cs.PL/1805.00907'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533–536.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Seide and Agarwal (2016) Frank Seide and Amit Agarwal. 2016. CNTK: Microsoft’s
    open-source deep-learning toolkit. In *Proceedings of the 22nd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining*. ACM, ACM, San Francisco, CA,
    USA, 2135–2135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shams et al. (2017) Shayan Shams, Richard Platania, Kisung Lee, and Seung-Jong
    Park. 2017. Evaluation of deep learning frameworks over different HPC architectures.
    In *2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)*.
    IEEE, IEEE Computer Society, Atlanta, GA, USA, 1389–1396.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2016) Hardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro,
    Joon Kyung Kim, Chenkai Shao, Asit Mishra, and Hadi Esmaeilzadeh. 2016. From high-level
    deep neural models to FPGAs. In *The 49th Annual IEEE/ACM International Symposium
    on Microarchitecture*. IEEE Press, IEEE Computer Society, Taipei, Taiwan, China,
    17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith and Karypis (2015) Shaden Smith and George Karypis. 2015. Tensor-matrix
    products with a compressed sparse tensor. In *Proceedings of the 5th Workshop
    on Irregular Applications: Architectures and Algorithms*. ACM, Austin, Texas,
    USA, 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. (2016b) D Team et al. 2016b. Deeplearning4j: Open-source distributed
    deep learning for the jvm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. (2016a) The Theano Development Team, Rami Al-Rfou, Guillaume Alain,
    Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Frédéric
    Bastien, Justin Bayer, Anatoly Belikov, et al. 2016a. Theano: A Python framework
    for fast computation of mathematical expressions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokui et al. (2019) Seiya Tokui, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani,
    Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki
    Yamazaki Vincent. 2019. Chainer: A deep learning framework for accelerating the
    research cycle. In *Proceedings of the 25th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. ACM, Anchorage, AK, USA, 2002–2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van Merriënboer et al. (2018) Bart Van Merriënboer, Olivier Breuleux, Arnaud
    Bergeron, and Pascal Lamblin. 2018. Automatic differentiation in ML: Where we
    are and where we should be going. In *Advances in neural information processing
    systems*. Curran Associates, Montréal, Canada, 8757–8767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'van Merriënboer et al. (2018) Bart van Merriënboer, Olivier Breuleux, Arnaud
    Bergeron, and Pascal Lamblin. 2018. Automatic differentiation in ML: Where we
    are and where we should be going. arXiv:cs.LG/1810.11530'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vasilache et al. (2006) Nicolas Vasilache, Cédric Bastoul, and Albert Cohen.
    2006. Polyhedral code generation in the real world. In *International Conference
    on Compiler Construction*. Springer, Springer, Vienna, Austria, 185–201.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vasilache et al. (2018) Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis,
    Priya Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege, Andrew Adams,
    and Albert Cohen. 2018. Tensor comprehensions: Framework-agnostic high-performance
    machine learning abstractions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Venieris and Bouganis (2016) Stylianos I Venieris and Christos-Savvas Bouganis.
    2016. fpgaConvNet: A framework for mapping convolutional neural networks on FPGAs.
    In *2016 IEEE 24th Annual International Symposium on Field-Programmable Custom
    Computing Machines (FCCM)*. IEEE, IEEE Computer Society, Washington, DC, USA,
    40–47.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Venieris et al. (2018) Stylianos I. Venieris, Alexandros Kouris, and Christos-Savvas
    Bouganis. 2018. Toolflows for Mapping Convolutional Neural Networks on FPGAs.
    *Comput. Surveys* 51, 3 (Jun 2018), 1–39. [https://doi.org/10.1145/3186332](https://doi.org/10.1145/3186332)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Venkat et al. (2015) Anand Venkat, Mary Hall, and Michelle Strout. 2015. Loop
    and Data Transformations for Sparse Matrix Code. In *Proceedings of the 36th ACM
    SIGPLAN Conference on Programming Language Design and Implementation* *(PLDI ’15)*.
    ACM, Portland, OR, USA, 521–532.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Venkat et al. (2014) Anand Venkat, Manu Shantharam, Mary Hall, and Michelle Mills
    Strout. 2014. Non-affine extensions to polyhedral code generation. In *Proceedings
    of Annual IEEE/ACM International Symposium on Code Generation and Optimization*.
    ACM, Orlando, FL, USA, 185–194.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verdoolaege (2010) Sven Verdoolaege. 2010. isl: An integer set library for
    the polyhedral model. In *International Congress on Mathematical Software*. Springer,
    Springer, Kobe, Japan, 299–302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verdoolaege et al. (2013) Sven Verdoolaege, Juan Carlos Juega, Albert Cohen,
    José Ignacio Gómez, Christian Tenllado, and Francky Catthoor. 2013. Polyhedral
    parallel code generation for CUDA. *ACM Trans. Archit. Code Optim.* 9, 4 (Jan.
    2013), 54:1–54:23. [https://doi.org/10.1145/2400682.2400713](https://doi.org/10.1145/2400682.2400713)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Ying Wang, Jie Xu, Yinhe Han, Huawei Li, and Xiaowei Li.
    2016. DeepBurning: automatic generation of FPGA-based learning accelerators for
    the neural network family. In *Proceedings of the 53rd Annual Design Automation
    Conference*. ACM, ACM, Austin, TX, USA, 110.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and O’Boyle (2018) Zheng Wang and Michael O’Boyle. 2018. Machine learning
    in compiler optimization. *Proc. IEEE* 106, 11 (2018), 1879–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2019) Gu-Yeon Wei, David Brooks, et al. 2019. Benchmarking tpu,
    gpu, and cpu platforms for deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2017) Xuechao Wei, Cody Hao Yu, Peng Zhang, Youxiang Chen, Yuxin
    Wang, Han Hu, Yun Liang, and Jason Cong. 2017. Automated systolic array architecture
    synthesis for high throughput CNN inference on FPGAs. In *Proceedings of the 54th
    Annual Design Automation Conference 2017*. ACM, ACM, Austin, TX, USA, 29.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xing et al. (2019) Yu Xing, Jian Weng, Yushun Wang, Lingzhi Sui, Yi Shan, and
    Yu Wang. 2019. An In-depth Comparison of Compilers for Deep Neural Networks on
    Hardware. In *2019 IEEE International Conference on Embedded Software and Systems
    (ICESS)*. IEEE, IEEE, Las Vegas, NV, USA, 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2018) Yuan Yu, Martín Abadi, Paul Barham, Eugene Brevdo, Mike Burrows,
    Andy Davis, Jeff Dean, Sanjay Ghemawat, Tim Harley, Peter Hawkins, Michael Isard,
    Manjunath Kudlur, Rajat Monga, Derek Murray, and Xiaoqiang Zheng. 2018. Dynamic
    Control Flow in Large-Scale Machine Learning. In *Proceedings of the Thirteenth
    EuroSys Conference* *(EuroSys ’18)*. Association for Computing Machinery, New
    York, NY, USA, Article Article 18, 15 pages. [https://doi.org/10.1145/3190508.3190551](https://doi.org/10.1145/3190508.3190551)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2018) R. Zhao, S. Liu, H. Ng, E. Wang, J. J. Davis, X. Niu, X.
    Wang, H. Shi, G. A. Constantinides, P. Y. K. Cheung, and W. Luk. 2018. Hardware
    Compilation of Deep Neural Networks: An Overview. In *2018 IEEE 29th International
    Conference on Application-specific Systems, Architectures and Processors (ASAP)*.
    IEEE Computer Society, Milano, Italy, 1–8. [https://doi.org/10.1109/ASAP.2018.8445088](https://doi.org/10.1109/ASAP.2018.8445088)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
