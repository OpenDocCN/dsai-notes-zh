- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:02:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:02:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2002.03794] The Deep Learning Compiler: A Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2002.03794] 深度学习编译器：全面调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.03794](https://ar5iv.labs.arxiv.org/html/2002.03794)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2002.03794](https://ar5iv.labs.arxiv.org/html/2002.03794)
- en: 'The Deep Learning Compiler: A Comprehensive Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习编译器：全面调查
- en: Mingzhen Li^∗ ,  Yi Liu^∗ ,  Xiaoyan Liu^∗ ,  Qingxiao Sun^∗ ,  Xin You^∗ , 
    Hailong Yang^∗^† ,  Zhongzhi Luan^∗ ,  Lin Gan^§ ,  Guangwen Yang^§  and  Depei
    Qian^∗ Beihang University^∗ Tsinghua University^§ [lmzhhh, yi.liu, liuxiaoyan,
    sunqingxiao, youxin2015, hailong.yang, zhongzhi.luan, depeiq@buaa.edu.cn](mailto:lmzhhh,%20yi.liu,%20liuxiaoyan,%20sunqingxiao,%20youxin2015,%20hailong.yang,%20zhongzhi.luan,%20depeiq@buaa.edu.cn)
    [lingan, ygw@tsinghua.edu.cn](mailto:lingan,%20ygw@tsinghua.edu.cn)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 李明震^∗ ，刘怡^∗ ，刘晓燕^∗ ，孙青晓^∗ ，游鑫^∗ ，杨海龙^∗^† ，段中智^∗ ，甘林^§ ，杨光文^§  和 钱德培^∗ 北京航空航天大学^∗
    清华大学^§ [lmzhhh, yi.liu, liuxiaoyan, sunqingxiao, youxin2015, hailong.yang, zhongzhi.luan,
    depeiq@buaa.edu.cn](mailto:lmzhhh,%20yi.liu,%20liuxiaoyan,%20sunqingxiao,%20youxin2015,%20hailong.yang,%20zhongzhi.luan,%20depeiq@buaa.edu.cn)
    [lingan, ygw@tsinghua.edu.cn](mailto:lingan,%20ygw@tsinghua.edu.cn)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: The difficulty of deploying various deep learning (DL) models on diverse DL
    hardware has boosted the research and development of DL compilers in the community.
    Several DL compilers have been proposed from both industry and academia such as
    Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described
    in different DL frameworks as input, and then generate optimized codes for diverse
    DL hardware as output. However, none of the existing survey has analyzed the unique
    design architecture of the DL compilers comprehensively. In this paper, we perform
    a comprehensive survey of existing DL compilers by dissecting the commonly adopted
    design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend
    optimizations. We present detailed analysis on the design of multi-level IRs and
    illustrate the commonly adopted optimization techniques. Finally, several insights
    are highlighted as the potential research directions of DL compiler. This is the
    first survey paper focusing on the design architecture of DL compilers, which
    we hope can pave the road for future research towards DL compiler.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 部署各种深度学习（DL）模型在不同的DL硬件上面临的难度，推动了社区对DL编译器的研究与开发。来自行业和学术界的多个DL编译器被提出，如Tensorflow
    XLA和TVM。同样，DL编译器将不同DL框架中描述的DL模型作为输入，然后生成针对各种DL硬件的优化代码作为输出。然而，现有的调查中没有全面分析DL编译器的独特设计架构。本文通过详细剖析常用的设计，包括DL导向的多级中间表示（IR）和前端/后端优化，对现有的DL编译器进行了全面调查。我们对多级IR的设计进行了详细分析，并说明了常用的优化技术。最后，突出了几个作为DL编译器潜在研究方向的见解。这是第一篇聚焦于DL编译器设计架构的调查论文，我们希望这能为未来的DL编译器研究铺平道路。
- en: 'Neural Networks, Deep Learning, Compiler, Intermediate Representation, Optimization^†Corresponding
    author.^†^†copyright: none'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络、深度学习、编译器、中间表示、优化^†通讯作者。^†^†版权：无
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: The development of deep learning (DL) has generated profound impact on various
    scientific fields. It has not only demonstrated remarkable value in artificial
    intelligence such as natural language processing (NLP) (Manning et al., [1999](#bib.bib65))
    and computer vision (CV) (Forsyth and Ponce, [2002](#bib.bib27)), but also proved
    great success in broader applications such as e-commerce (Ha et al., [2016](#bib.bib37)),
    smart city (Mohammadi et al., [2017](#bib.bib69)) and drug discovery (Chen et al.,
    [2018a](#bib.bib16)). With the emergence of versatile deep learning models such
    as convolutional neural network (CNN) (LeCun et al., [1998](#bib.bib55)), recurrent
    neural network (RNN) (Rumelhart et al., [1986](#bib.bib81)), long short-term memory
    (LSTM) (Hochreiter and Schmidhuber, [1997](#bib.bib39)) and generative adversarial
    network (GAN) (Goodfellow et al., [2014](#bib.bib30)), it is critical to ease
    the programming of diverse DL models in order to realize their widely adoption.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的发展对各个科学领域产生了深远的影响。它不仅在人工智能领域，如自然语言处理（NLP）（Manning et al., [1999](#bib.bib65)）和计算机视觉（CV）（Forsyth
    and Ponce, [2002](#bib.bib27)），展示了显著的价值，而且在更广泛的应用中，如电子商务（Ha et al., [2016](#bib.bib37)）、智慧城市（Mohammadi
    et al., [2017](#bib.bib69)）和药物发现（Chen et al., [2018a](#bib.bib16)），也取得了巨大成功。随着多功能深度学习模型的出现，如卷积神经网络（CNN）（LeCun
    et al., [1998](#bib.bib55)）、递归神经网络（RNN）（Rumelhart et al., [1986](#bib.bib81)）、长短期记忆（LSTM）（Hochreiter
    and Schmidhuber, [1997](#bib.bib39)）和生成对抗网络（GAN）（Goodfellow et al., [2014](#bib.bib30)），简化各种DL模型的编程变得至关重要，以实现其广泛应用。
- en: With the continuous efforts from both industry and academia, several popular
    DL frameworks have been proposed such as TensorFlow (Abadi et al., [2016](#bib.bib2)),
    PyTorch (Paszke et al., [2019](#bib.bib76)), MXNet (Chen et al., [2015](#bib.bib17))
    and CNTK (Seide and Agarwal, [2016](#bib.bib82)), in order to simplify the implementation
    of various DL models. Although there are strengths and weaknesses among the above
    DL frameworks depending on the tradeoffs in their designs, the interoperability
    becomes important to reduce the redundant engineering efforts when supporting
    emerging DL models across the existing DL models. To provide interoperability,
    ONNX (Microsoft, [2017](#bib.bib67)) has been proposed, that defines a unified
    format for representing DL models to facilitate model conversion between different
    DL frameworks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着工业界和学术界的持续努力，已经提出了几个流行的DL框架，如TensorFlow（Abadi et al., [2016](#bib.bib2)）、PyTorch（Paszke
    et al., [2019](#bib.bib76)）、MXNet（Chen et al., [2015](#bib.bib17)）和CNTK（Seide
    and Agarwal, [2016](#bib.bib82)），以简化各种DL模型的实现。尽管这些DL框架在设计中的权衡使得它们各有优缺点，但在支持现有DL模型中的新兴DL模型时，互操作性变得重要，以减少冗余的工程工作。为提供互操作性，ONNX（Microsoft,
    [2017](#bib.bib67)）被提出，它定义了一个统一的格式来表示DL模型，以促进不同DL框架之间的模型转换。
- en: 'In the meanwhile, the unique computing characteristics such as matrix multiplication
    have spurred the passion of chip architects to design customized DL accelerators
    for higher efficiency. Internet giants (e.g., Google TPU (Jouppi et al., [2017](#bib.bib45)),
    Hisilicon NPU (Liao et al., [2019](#bib.bib57)), Apple Bonic (Kingsley-Hughes,
    [2017](#bib.bib50))), processor vendors (e.g., NVIDIA Turing (NVIDIA, [2019b](#bib.bib73)),
    Intel NNP (Intel, [2019](#bib.bib42))), service providers (e.g., Amazon Inferentia (Amazon,
    [2018](#bib.bib9)), Alibaba Hanguang (Alibaba, [2019](#bib.bib8))), and even startups
    (e.g., Cambricon (Liu et al., [2016](#bib.bib58)), Graphcore (Jia et al., [2019](#bib.bib44)))
    are investing tremendous workforce and capital in developing DL chips in order
    to boost the performance for DL models. Generally, the DL hardware can be divided
    into the following categories: 1) general-purpose hardware with software-hardware
    co-design, 2) dedicated hardware fully customized for DL models, and 3) neuromorphic
    hardware inspired by biological brain science. For example, the general-purpose
    hardware (e.g., CPU, GPU) has added special hardware components such as AVX512
    vector units and tensor core to accelerate DL models. Whereas for dedicated hardware
    such as Google TPU, application-specific integrated circuits (e.g., matrix multiplication
    engine and high-bandwidth memory) have been designed to elevate the performance
    and energy efficiency to extreme. To the foreseeable future, the design of DL
    hardware would become even more diverse.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，矩阵乘法等独特的计算特性激发了芯片架构师设计定制化 DL 加速器以提高效率的热情。互联网巨头（例如，Google TPU（Jouppi et
    al., [2017](#bib.bib45)）、Hisilicon NPU（Liao et al., [2019](#bib.bib57)）、Apple
    Bonic（Kingsley-Hughes, [2017](#bib.bib50)））、处理器供应商（例如，NVIDIA Turing（NVIDIA, [2019b](#bib.bib73)）、Intel
    NNP（Intel, [2019](#bib.bib42)））、服务提供商（例如，Amazon Inferentia（Amazon, [2018](#bib.bib9)）、Alibaba
    Hanguang（Alibaba, [2019](#bib.bib8)））甚至初创公司（例如，Cambricon（Liu et al., [2016](#bib.bib58)）、Graphcore（Jia
    et al., [2019](#bib.bib44)））正在投入大量的人力和资本开发 DL 芯片，以提升 DL 模型的性能。一般来说，DL 硬件可以分为以下几类：1）具有软硬件协同设计的通用硬件，2）完全为
    DL 模型定制的专用硬件，以及 3）受生物脑科学启发的神经形态硬件。例如，通用硬件（例如，CPU、GPU）增加了诸如 AVX512 向量单元和张量核心等特殊硬件组件，以加速
    DL 模型。而对于专用硬件，如 Google TPU，则设计了应用特定集成电路（例如，矩阵乘法引擎和高带宽内存）以极大提高性能和能效。在可预见的未来，DL
    硬件的设计将变得更加多样化。
- en: To embrace the hardware diversity, it is important to map the computation to
    DL hardware efficiently. On general-purpose hardware, the highly optimized linear
    algebra libraries such as Basic Linear Algebra Subprograms (BLAS) libraries (e.g.,
    MKL and cuBLAS) serve as the basics for efficient computation of DL models. Take
    the convolution operation for example, the DL frameworks convert the convolution
    to matrix multiplication and then invoke the GEMM function in the BLAS libraries.
    In addition, the hardware vendors have released specially optimized libraries
    tailored for DL computations (e.g., MKL-DNN and cuDNN), including forward and
    backward convolution, pooling, normalization, and activation. More advanced tools
    have also been developed to further speedup the DL operations. For example, TensorRT (NVIDIA,
    [2019c](#bib.bib74)) supports graph optimization (e.g., layer fusion) and low-bit
    quantization with large collection of highly optimized GPU kernels. On dedicated
    DL hardware, similar libraries are also provided (Liu et al., [2016](#bib.bib58);
    Jia et al., [2019](#bib.bib44)). However, the drawback of relying on the libraries
    is that they usually fall behind the rapid development of DL models, and thus
    fail to utilize the DL chips efficiently.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应硬件多样性，重要的是将计算映射到深度学习（DL）硬件上以提高效率。在通用硬件上，高度优化的线性代数库，如基本线性代数子程序（BLAS）库（例如，MKL
    和 cuBLAS）作为高效计算 DL 模型的基础。例如，以卷积操作为例，DL 框架将卷积转换为矩阵乘法，然后调用 BLAS 库中的 GEMM 函数。此外，硬件供应商还发布了专门优化的库，针对
    DL 计算（例如，MKL-DNN 和 cuDNN），包括前向和后向卷积、池化、归一化和激活。更高级的工具也已开发，以进一步加速 DL 操作。例如，TensorRT（NVIDIA，[2019c](#bib.bib74)）支持图优化（例如，层融合）和低位量化，并配有大量高度优化的
    GPU 内核。在专用 DL 硬件上，也提供了类似的库（Liu et al., [2016](#bib.bib58); Jia et al., [2019](#bib.bib44)）。然而，依赖这些库的缺点是，它们通常跟不上
    DL 模型的快速发展，从而未能高效利用 DL 芯片。
- en: To address the drawback of DL libraries and tools, as well as alleviate the
    burden of optimizing the DL models on each DL hardware manually, the DL community
    has resorted to the domain specific compilers for rescue. Rapidly, several popular
    DL compilers have been proposed such as TVM (Chen et al., [2018b](#bib.bib18)),
    Tensor Comprehension (Vasilache et al., [2018](#bib.bib92)), Glow (Rotem et al.,
    [2018](#bib.bib80)), nGraph (Cyphers et al., [2018](#bib.bib22)) and XLA (Leary
    and Wang, [2017](#bib.bib54)), from both industry and academia. The DL compilers
    take the model definitions described in the DL frameworks as inputs, and generate
    efficient code implementations on various DL hardware as outputs. The transformation
    between model definition and specific code implementation are highly optimized
    targeting the model specification and hardware architecture. Specifically, they
    incorporate DL oriented optimizations such as layer and operator fusion, which
    enables highly efficient code generation. Moreover, existing DL compilers also
    leverage mature tool-chains from general-purpose compilers (e.g., LLVM (Lattner
    and Adve, [2004](#bib.bib52))), which provides better portability across diverse
    hardware architectures. Similar to traditional compiler, DL compilers also adopt
    the layered design including frontend, intermediate representation (IR) and backend.
    However, the uniqueness of DL compiler lies in the design of multi-level IRs and
    DL specific optimizations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决深度学习（DL）库和工具的缺陷，以及减轻在每个深度学习硬件上手动优化深度学习模型的负担，深度学习社区求助于领域特定的编译器。迅速地，出现了几种流行的深度学习编译器，例如
    TVM (Chen et al., [2018b](#bib.bib18))、Tensor Comprehension (Vasilache et al.,
    [2018](#bib.bib92))、Glow (Rotem et al., [2018](#bib.bib80))、nGraph (Cyphers et
    al., [2018](#bib.bib22)) 和 XLA (Leary and Wang, [2017](#bib.bib54))，这些编译器来自工业界和学术界。深度学习编译器将深度学习框架中描述的模型定义作为输入，并在各种深度学习硬件上生成高效的代码实现。模型定义和特定代码实现之间的转换经过高度优化，以针对模型规格和硬件架构。具体来说，它们融合了深度学习特定的优化，例如层和操作符融合，这使得代码生成高度高效。此外，现有的深度学习编译器还利用了通用编译器（例如
    LLVM (Lattner and Adve, [2004](#bib.bib52))）的成熟工具链，这提供了更好的硬件架构跨平台移植性。类似于传统编译器，深度学习编译器也采用了分层设计，包括前端、中间表示（IR）和后端。然而，深度学习编译器的独特之处在于多层级
    IR 的设计和深度学习特定的优化。
- en: 'In this paper, we provide a comprehensive survey of existing DL compilers by
    dissecting the compiler design into frontend, multi-level IRs and backend, with
    special emphasis on the IR design and optimization methods. To the best of our
    knowledge, this is the first paper that provides a comprehensive survey on the
    design of DL compiler. Specifically, this paper makes the following contributions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们通过将编译器设计拆分为前端、多层级 IR 和后端，提供了对现有深度学习编译器的全面调查，特别强调了 IR 设计和优化方法。据我们所知，这是第一篇提供深度学习编译器设计全面调查的论文。具体来说，本文做出了以下贡献：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We dissect the commonly adopted design architecture of existing DL compilers,
    and provide detailed analysis of the key design components such as multi-level
    IRs, frontend optimizations (including node-level, block-level and dataflow-level
    optimizations) and backend optimizations (including hardware-specific optimization,
    auto-tuning and optimized kernel libraries).
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们剖析了现有深度学习编译器常用的设计架构，并对关键设计组件进行了详细分析，例如多层级 IR、前端优化（包括节点级、块级和数据流级优化）以及后端优化（包括硬件特定优化、自动调优和优化内核库）。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive taxonomy of existing DL compilers from various aspects,
    which corresponds to the key components described in this survey. The target of
    this taxonomy is to provide guidelines about the selection of DL compilers for
    the practitioners considering their requirements, as well as to give a thorough
    summary of the DL compilers for researchers.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从各个方面提供了现有深度学习编译器的全面分类，这些方面对应于本调查中描述的关键组件。这个分类的目标是为实践者提供关于选择深度学习编译器的指导，以满足他们的需求，并为研究人员提供深度学习编译器的详细总结。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have provided the quantitative performance comparison among DL compilers
    on CNN models, including full-fledged models and lightweight models. We have compared
    both end-to-end and per-layer (convolution layers since they dominate the inference
    time) performance to show the effectiveness of optimizations. The evaluation scripts
    and results are open sourced¹¹1[https://github.com/buaa-hipo/dlcompiler-comparison](https://github.com/buaa-hipo/dlcompiler-comparison)
    for reference.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了关于深度学习编译器在卷积神经网络（CNN）模型上的定量性能比较，包括完整模型和轻量级模型。我们比较了端到端和每层（卷积层，因为它们主导了推理时间）性能，以展示优化的有效性。评估脚本和结果已经开源¹¹1[https://github.com/buaa-hipo/dlcompiler-comparison](https://github.com/buaa-hipo/dlcompiler-comparison)
    供参考。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We highlight several insights for the future development of DL compilers, including
    dynamic shape and pre-/post-processing, advanced auto-tuning, polyhedral model,
    subgraph partitioning, quantization, unified optimizations, differentiable programming
    and privacy protection, which we hope to boost the research in the DL compiler
    community.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们突出了一些对深度学习编译器未来发展的见解，包括动态形状和预处理/后处理、先进的自动调优、**多面体模型**、子图分割、量化、统一优化、可微编程和隐私保护，我们希望能够推动深度学习编译器社区的研究。
- en: 'The rest of this paper is organized as follows. Section [2](#S2 "2\. Background
    ‣ The Deep Learning Compiler: A Comprehensive Survey") presents the background
    of DL compilers, including the DL frameworks, DL hardware, as well as hardware
    (FPGA) specific DL code generators. Section [3](#S3 "3\. Common Design Architecture
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey") describes
    the common design architecture of DL compilers. Section [4](#S4 "4\. Key Components
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey") discusses
    the key components of DL compilers, including multi-level IRs, frontend optimizations
    and backend optimizations. Section [5](#S5 "5\. Taxonomy of DL Compilers ‣ The
    Deep Learning Compiler: A Comprehensive Survey") presents a comprehensive taxonomy.
    Section [6](#S6 "6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive
    Survey") provides the quantitative performance comparison. Section [7](#S7 "7\.
    Conclusion and Future Directions ‣ The Deep Learning Compiler: A Comprehensive
    Survey") highlights the future directions for DL compiler research.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文其余部分组织如下。第[2](#S2 "2. 背景 ‣ 深度学习编译器：综合调查")节介绍了深度学习编译器的背景，包括深度学习框架、深度学习硬件以及特定于硬件（FPGA）的深度学习代码生成器。第[3](#S3
    "3. 深度学习编译器的常见设计架构 ‣ 深度学习编译器：综合调查")节描述了深度学习编译器的常见设计架构。第[4](#S4 "4. 深度学习编译器的关键组件
    ‣ 深度学习编译器：综合调查")节讨论了深度学习编译器的关键组件，包括多层次中间表示（IR）、前端优化和后端优化。第[5](#S5 "5. 深度学习编译器的分类
    ‣ 深度学习编译器：综合调查")节介绍了全面的分类。第[6](#S6 "6. 评估 ‣ 深度学习编译器：综合调查")节提供了定量性能比较。第[7](#S7
    "7. 结论与未来方向 ‣ 深度学习编译器：综合调查")节突出了深度学习编译器研究的未来方向。
- en: 2\. Background
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 背景
- en: 2.1\. Deep Learning Frameworks
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. 深度学习框架
- en: 'In this section, we provide an overview of popular DL frameworks. The discussion
    might not be exhaustive but is meant to provide a guideline fo DL practitioners.
    Figure [1](#S2.F1 "Figure 1 ‣ 2.1\. Deep Learning Frameworks ‣ 2\. Background
    ‣ The Deep Learning Compiler: A Comprehensive Survey") presents the landscape
    of DL frameworks including currently popular frameworks, historical frameworks
    and ONNX supported frameworks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了对流行深度学习框架的概述。讨论可能不全面，但旨在为深度学习从业者提供指南。图[1](#S2.F1 "图 1 ‣ 2.1. 深度学习框架
    ‣ 2. 背景 ‣ 深度学习编译器：综合调查")展示了深度学习框架的全景，包括当前流行的框架、历史框架和ONNX支持的框架。
- en: TensorFlow - Among all the DL frameworks, TensorFlow has the most comprehensive
    support for language interfaces, including C ++, Python, Java, Go, R, and Haskell.
    TensorFlow employs a dataflow graph of primitive operators extended with restricted
    control edges to represent differentiable programs (Roesch et al., [2019](#bib.bib79)).
    TensorFlow Lite is designed for mobile and embedded deep learning and provides
    an Android neural network API. To reduce the complexity of using TensorFlow, Google
    adopts Keras as a frontend to the TensorFlow core. Furthermore, The eager-mode
    in TensorFlow applies an approach similar to PyTorch to support dynamic computation
    graphs better.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow - 在所有深度学习框架中，TensorFlow 对语言接口的支持最为全面，包括 C ++、Python、Java、Go、R 和 Haskell。TensorFlow
    使用一个扩展了受限控制边的原始操作符的数据流图来表示可微分程序（Roesch 等，[2019](#bib.bib79)）。TensorFlow Lite 旨在用于移动和嵌入式深度学习，并提供了
    Android 神经网络 API。为了减少使用 TensorFlow 的复杂性，Google 采用 Keras 作为 TensorFlow 核心的前端。此外，TensorFlow
    中的 eager-mode 应用了一种类似于 PyTorch 的方法，以更好地支持动态计算图。
- en: Keras - Keras (Chollet et al., [2015](#bib.bib20)) is a high-level neural network
    library for quickly building DL models, written in pure Python. Though not a DL
    framework on its own, Keras provides a high-level API that integrates with TensorFlow,
    MXNet, Theano, and CNTK. With Keras, DL developers can build a neural network
    with just a few lines of code. Besides, Keras can integrate with other common
    DL packages, such as scikit-learn. However, Keras is not flexible enough due to
    over-encapsulation, which makes it too difficult to add operators or obtain low-level
    data information.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Keras - Keras（Chollet 等，[2015](#bib.bib20)）是一个高层次的神经网络库，用于快速构建深度学习模型，使用纯 Python
    编写。尽管 Keras 本身不是一个深度学习框架，但它提供了一个高级 API，与 TensorFlow、MXNet、Theano 和 CNTK 集成。使用
    Keras，深度学习开发者可以用几行代码构建神经网络。此外，Keras 可以与其他常见的深度学习包（如 scikit-learn）集成。然而，由于过度封装，Keras
    的灵活性不足，导致添加操作符或获取低级数据信息变得过于困难。
- en: PyTorch - Facebook has rewritten the Lua-based DL framework Torch in Python
    and refactored all modules on Tensor level, which leads to the release of PyTorch.
    As the most popular dynamic framework, PyTorch embeds primitives for constructing
    dynamic dataflow graphs in Python, where the control flow is executed in the Python
    interpreter. PyTorch 1.0 integrated the codebases of PyTorch 0.4 and Caffe2 to
    create a unified framework. This allows PyTorch to absorb the benefits of Caffe2
    to support efficient graph execution and mobile deployment. FastAI (Howard et al.,
    [2018](#bib.bib40)) is an advanced API layer based on PyTorch’s upper-layer encapsulation.
    It fully borrows Keras to ease the use of PyTorch.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch - Facebook 将基于 Lua 的深度学习框架 Torch 用 Python 重写，并在 Tensor 级别上重构了所有模块，从而发布了
    PyTorch。作为最受欢迎的动态框架，PyTorch 在 Python 中嵌入了构建动态数据流图的原语，其中控制流在 Python 解释器中执行。PyTorch
    1.0 将 PyTorch 0.4 和 Caffe2 的代码库整合，创建了一个统一的框架。这使得 PyTorch 吸收了 Caffe2 的优点，以支持高效的图执行和移动部署。FastAI（Howard
    等，[2018](#bib.bib40)）是一个基于 PyTorch 上层封装的高级 API 层。它完全借鉴了 Keras 的设计，简化了 PyTorch
    的使用。
- en: Caffe/Caffe2 - Caffe (Jia et al., [2014](#bib.bib43)) was designed for deep
    learning and image classification by UC Berkeley. Caffe has the command line,
    Python, and MATLAB APIs. Caffe’s simplicity makes the source codes easy to extend,
    which is suitable for developers to analyze in-depth. Therefore, Caffe is mainly
    positioned in research, which has made it popular from the beginning to the present.
    Caffe2 is built upon the original Caffe project. Caffe2 is similar to TensorFlow
    in code structure, albeit with a lighter API and easier access to the intermediate
    results in the computation graph.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Caffe/Caffe2 - Caffe（Jia 等，[2014](#bib.bib43)）是由 UC Berkeley 设计的深度学习和图像分类框架。Caffe
    提供了命令行、Python 和 MATLAB API。Caffe 的简单性使得源代码易于扩展，适合开发者深入分析。因此，Caffe 主要定位于研究，从最初到现在一直很受欢迎。Caffe2
    建立在原始 Caffe 项目之上。Caffe2 在代码结构上类似于 TensorFlow，但 API 更轻量，且更容易访问计算图中的中间结果。
- en: 'MXNet - MXNet supports multiple language APIs including Python, C++, R, Scala,
    Julia, Matlab, and JavaScript. It was intended to be scalable and was designed
    from the perspective to reduce data loading and I/O complexity (Chen et al., [2015](#bib.bib17)).
    MXNet offers different paradigms: declarative programming like Caffe and Tensorflow
    as well as imperative like PyTorch. In December 2017, Amazon and Microsoft jointly
    released Gluon (MXNet, [2017](#bib.bib70)) based on MXNet, which is an advanced
    interface similar to Keras and FastAI. Gluon supports both flexible, dynamic graphs
    and efficient, static graphs.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet - MXNet支持多种语言API，包括Python、C++、R、Scala、Julia、Matlab和JavaScript。它旨在实现可扩展性，并从减少数据加载和I/O复杂性的角度进行设计（Chen等，[2015](#bib.bib17)）。MXNet提供不同的范式：类似Caffe和Tensorflow的声明式编程以及类似PyTorch的命令式编程。2017年12月，亚马逊和微软联合发布了基于MXNet的Gluon（MXNet，[2017](#bib.bib70)），这是一种类似于Keras和FastAI的高级接口。Gluon同时支持灵活的动态图和高效的静态图。
- en: CNTK - CNTK can be used through Python, C++ and C# APIs, or its own scripting
    language (i.e., BrainScript). CNTK is designed to be easy-to-use and production-ready
    for large-scale data in production (Hatcher and Yu, [2018](#bib.bib38)). However,
    CNTK does not yet support the ARM architecture, which limits its usage on mobile
    devices. It uses the static computation graph similar to TensorFlow and Caffe,
    in which a DL model is treated as a series of computational steps through a directed
    graph.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK - CNTK可以通过Python、C++和C# API或其自己的脚本语言（即BrainScript）使用。CNTK旨在易于使用，并且适用于生产环境中的大规模数据（Hatcher和Yu，[2018](#bib.bib38)）。然而，CNTK尚不支持ARM架构，这限制了它在移动设备上的使用。它使用静态计算图，类似于TensorFlow和Caffe，其中DL模型被视为通过有向图进行一系列计算步骤。
- en: PaddlePaddle - The original design of PaddlePaddle (Baidu, [2016](#bib.bib12))
    is similar to Caffe, where each model can be represented as a set of layers. However,
    PaddlePaddle v2 has adopted the concept of operators with reference to TensorFlow,
    which breaks layers into finer-grained operators, thereby supporting more complex
    DL models. And PaddlePaddle Fluid is similar to PyTorch because it provides own
    interpreter so as to avoid the limited performance of Python interpreter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PaddlePaddle - PaddlePaddle的原始设计（百度，[2016](#bib.bib12)）类似于Caffe，其中每个模型可以表示为一组层。然而，PaddlePaddle
    v2已经采用了参考TensorFlow的运算符的概念，将层分解为更细粒度的算子，从而支持更复杂的DL模型。PaddlePaddle Fluid类似于PyTorch，因为它提供了自己的解释器，以避免Python解释器的性能限制。
- en: 'ONNX - The Open Neural Network Exchange (ONNX) (Microsoft, [2017](#bib.bib67))
    defines a scalable computation graph model, and thus computation graphs built
    by different DL frameworks can be easily transformed into ONNX. With ONNX, it
    becomes easier to convert models between DL frameworks. For example, it allows
    developers to build an MXNet model and then run the model using PyTorch for inference.
    As shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1\. Deep Learning Frameworks ‣ 2\.
    Background ‣ The Deep Learning Compiler: A Comprehensive Survey"), ONNX has been
    integrated into PyTorch, MXNet, PaddlePaddle, and so on. For several DL frameworks
    (e.g., TensorFlow and Keras) that are not directly supported yet, and ONNX adds
    converters to them.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX - 开放神经网络交换（ONNX）（微软，[2017](#bib.bib67)）定义了可扩展的计算图模型，因此不同DL框架构建的计算图可以轻松转换为ONNX。有了ONNX，可以更容易地在DL框架之间转换模型。例如，它允许开发人员构建一个MXNet模型，然后使用PyTorch来运行模型进行推理。正如图[1](#S2.F1
    "图1 ‣ 2.1\. 深度学习框架 ‣ 2\. 背景 ‣ 深度学习编译器：全面调查")所示，ONNX已经集成到PyTorch、MXNet、PaddlePaddle等框架中。对于一些尚未直接支持的DL框架（例如TensorFlow和Keras），ONNX为它们添加了转换器。
- en: Historical Frameworks - Due to the rapid evolvement in DL community, many historical
    DL frameworks are no longer active. For example, PyTorch has replaced Torch (Collobert
    et al., [2011](#bib.bib21)). As one of the oldest DL frameworks, Theano (Team
    et al., [2016a](#bib.bib87)) is no longer under maintenance. Deeplearning4J (Team
    et al., [2016b](#bib.bib86)) a distributed DL framework based on Java and Scala,
    however becomes inactive due to the lack of large developer community. Chainer (Tokui
    et al., [2019](#bib.bib88)) was once the preferred framework for dynamic computation
    graphs, however replaced by MXNet, PyTorch and TensorFlow with similar features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 历史框架 - 由于 DL 社区的快速发展，许多历史上的 DL 框架不再活跃。例如，PyTorch 已经取代了 Torch (Collobert et al.,
    [2011](#bib.bib21))。作为最早的 DL 框架之一，Theano (Team et al., [2016a](#bib.bib87)) 已不再维护。Deeplearning4J (Team
    et al., [2016b](#bib.bib86)) 是一个基于 Java 和 Scala 的分布式 DL 框架，然而由于缺乏大型开发者社区而变得不活跃。Chainer (Tokui
    et al., [2019](#bib.bib88)) 曾是动态计算图的首选框架，但被具有类似功能的 MXNet、PyTorch 和 TensorFlow
    取代。
- en: Previous works (Bahrampour et al., [2015](#bib.bib11); Fonnegra et al., [2017](#bib.bib26);
    Shams et al., [2017](#bib.bib83); Guo et al., [2018](#bib.bib36); Nara et al.,
    [2019](#bib.bib71); Wei et al., [2019](#bib.bib101)) have compared the performance
    of DL frameworks on different applications (e.g., computer vision and image classification)
    and different hardware (e.g., CPU, GPU, and TPU). For detailed information about
    each DL framework, the readers can refer to (Hatcher and Yu, [2018](#bib.bib38)).
    Different from them, this survey focuses on the research efforts on DL compilers
    which provide more general approach to execute various DL models on diverse hardware
    efficiently.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的工作 (Bahrampour et al., [2015](#bib.bib11); Fonnegra et al., [2017](#bib.bib26);
    Shams et al., [2017](#bib.bib83); Guo et al., [2018](#bib.bib36); Nara et al.,
    [2019](#bib.bib71); Wei et al., [2019](#bib.bib101)) 比较了不同应用（如计算机视觉和图像分类）和不同硬件（如
    CPU、GPU 和 TPU）上 DL 框架的性能。有关每个 DL 框架的详细信息，读者可以参考 (Hatcher and Yu, [2018](#bib.bib38))。与这些工作不同，本调查专注于
    DL 编译器的研究工作，这些编译器提供了更通用的方法，以高效地在各种硬件上执行不同的 DL 模型。
- en: '![Refer to caption](img/879823e1104f967e4d01c4a8cd2eb9fa.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/879823e1104f967e4d01c4a8cd2eb9fa.png)'
- en: 'Figure 1\. DL framework landscape: 1) Currently popular DL frameworks; 2) Historical
    DL frameworks; 3) ONNX supported frameworks.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. DL 框架概览：1) 当前流行的 DL 框架；2) 历史 DL 框架；3) ONNX 支持的框架。
- en: 2.2\. Deep Learning Hardware
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 深度学习硬件
- en: 'The DL hardware can be divided into three categories based on the generality:
    1) general-purpose hardware that can support DL workloads through hardware and
    software optimization; 2) dedicated hardware that focus on accelerating DL workloads
    with fully customized circuit design; 3) neuromorphic hardware that function by
    mimicking the human brain.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习硬件可以根据通用性分为三类：1) 通用硬件，通过硬件和软件优化支持 DL 工作负载；2) 专用硬件，专注于通过完全定制的电路设计加速 DL 工作负载；3)
    神经形态硬件，通过模仿人脑的方式工作。
- en: General-purpose Hardware - The most representative general-purpose hardware
    for DL models is Graphic Processing Unit (GPU), which achieves high parallelism
    with many-core architecture. For example, Nvidia GPUs have introduced tensor cores
    since the Volta architecture. Tensor cores can accelerate mixed-precision matrix
    multiply-and-accumulate calculations in parallel, which are widely used in DL
    models during both training and inference. Co-optimized with the hardware, NVIDIA
    also launches highly optimized DL libraries and tools such as cuDNN (Chetlur et al.,
    [2014](#bib.bib19)) and TensorRT (NVIDIA, [2019c](#bib.bib74)) to further accelerate
    the computation of DL models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通用硬件 - 代表性的深度学习（DL）模型通用硬件是图形处理单元（GPU），它通过多核心架构实现了高并行性。例如，Nvidia GPU 自 Volta
    架构起引入了张量核心。张量核心可以并行加速混合精度矩阵乘法和累加计算，这些计算在 DL 模型的训练和推理过程中广泛使用。与硬件共同优化，NVIDIA 还推出了高度优化的
    DL 库和工具，如 cuDNN (Chetlur et al., [2014](#bib.bib19)) 和 TensorRT (NVIDIA, [2019c](#bib.bib74))，进一步加速
    DL 模型的计算。
- en: Dedicated Hardware - Dedicated hardware is fully customized for DL computation
    to improve performance and energy efficiency to extreme. The rapid expansion of
    DL applications and algorithms has spurred many startups developing dedicated
    DL hardware (e.g., Graphcore GC2, Cambricon MLU270). Besides, traditional hardware
    companies (e.g., Intel NNP, Qualcomm Cloud AI 100) and cloud service providers
    (e.g., Google TPU, Amazon Inferentia, and Alibaba Hanguang) have also invested
    in this field. The most well known dedicated DL hardware is Google’s TPU series.
    A TPU includes Matrix Multiplier Unit (MXU), Unified Buffer (UB), and Activation
    Unit (AU), which is driven with CISC instructions by the host processor. The MXU
    is mainly composed of a systolic array, which is optimized for power and area
    efficiency in performing matrix multiplications. Compared to CPU and GPU, TPU
    is still programmable but uses a matrix as a primitive instead of a vector or
    scalar. The Amazon Inferentia has also attracts the attention recently. This chip
    has four NeuroCores that are designed for tensor-level operations, and it has
    large on-chip cache to avoid the frequent main memory access.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 专用硬件 - 专用硬件是为深度学习计算量身定制的，以极大地提升性能和能源效率。深度学习应用和算法的快速扩展促使许多初创公司开发专用深度学习硬件（例如，Graphcore
    GC2、Cambricon MLU270）。此外，传统硬件公司（如，Intel NNP、Qualcomm Cloud AI 100）和云服务提供商（例如，Google
    TPU、Amazon Inferentia 和 Alibaba Hanguang）也在这一领域进行了投资。最著名的专用深度学习硬件是Google的TPU系列。TPU包括矩阵乘法单元（MXU）、统一缓冲区（UB）和激活单元（AU），由主处理器通过CISC指令驱动。MXU主要由一个流线型阵列组成，优化了矩阵乘法的功耗和面积效率。与CPU和GPU相比，TPU仍然是可编程的，但它使用矩阵作为基本单元，而不是向量或标量。Amazon
    Inferentia 最近也引起了关注。该芯片具有四个神经核心，专为张量级操作设计，并具有大型片上缓存，以避免频繁的主内存访问。
- en: 'Neuromorphic Hardware - Neuromorphic chips use electronic technology to simulate
    the biological brain. Representative products of the this kind are IBM’s TrueNorth
    and Intel’s Loihi. Neuromorphic chips (e.g., TrueNorth) have very high connectivity
    between their artificial neurons. Neuromorphic chips also replicate a structure
    similar to the brain tissue: neurons can simultaneously store and process the
    data. Traditional chips distribute processors and memory in different locations,
    but neuromorphic chips usually have many microprocessors, each of which has a
    small amount of local memory. Compared to TrueNorth, Loihi has a learning ability
    more similar to the brain. Loihi introduces the pulse-time-dependent synaptic
    plasticity model (STDP), a mechanism that regulates synaptic strength by the relative
    time of pre-synaptic and post-synaptic pulses. However, neuromorphic chips are
    far away from Large-scale commercial production. Despite that, in computer science
    domain, neuromorphic chips can help to capture the process of rapid, life-long
    learning which is ignored by regular DL models, and in neurology domain, they
    are helpful to figure out how the various parts of the brain work together to
    create thoughts, feelings, and even consciousness.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态硬件 - 神经形态芯片使用电子技术模拟生物大脑。这类芯片的代表产品有IBM的TrueNorth和Intel的Loihi。神经形态芯片（例如，TrueNorth）具有非常高的人工神经元间连接性。神经形态芯片还复制了类似于脑组织的结构：神经元可以同时存储和处理数据。传统芯片将处理器和内存分布在不同的位置，而神经形态芯片通常具有许多微处理器，每个处理器都有少量的本地内存。与TrueNorth相比，Loihi具有更接近大脑的学习能力。Loihi引入了脉冲时间依赖突触可塑性模型（STDP），这是一个通过前突触和后突触脉冲的相对时间来调节突触强度的机制。然而，神经形态芯片距离大规模商业生产仍然有很长的路要走。尽管如此，在计算机科学领域，神经形态芯片可以帮助捕捉快速、终身学习的过程，而这些过程通常被常规深度学习模型忽略；在神经科学领域，它们有助于揭示大脑的不同部分如何协同工作以产生思想、情感甚至意识。
- en: 2.3\. Hardware-specific DL Code Generator
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 硬件特定深度学习代码生成器
- en: Field Programmable Gate Arrays (FPGAs) are reprogrammable integrated circuits
    that contain an array of programmable logic blocks. Programmers can configure
    them after manufacturing. Besides the reprogrammable nature, the low-power and
    high-performance nature of the FPGA make it widely used in so many domains, such
    as communication, medical, image processing, and ASIC prototyping. As for the
    domain of deep learning, the high-performance CPUs and GPUs are highly-reprogrammable
    but power-hungry, while the power-efficient ASICs are specialized for fixed applications.
    However, the FPGA can bridge the gap between CPUs/GPUs and ASICs, which causes
    the FPGA to be an attractive platform for deep learning.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 可编程逻辑门阵列（FPGAs）是可重新编程的集成电路，包含一组可编程逻辑块。程序员可以在制造后进行配置。除了可重新编程的特性，FPGA的低功耗和高性能使其在许多领域得到广泛应用，如通信、医疗、图像处理和ASIC原型设计。在深度学习领域，高性能的CPU和GPU具有很强的重新编程能力但耗电量大，而功耗高效的ASIC则专门用于固定应用。然而，FPGA可以弥合CPU/GPU和ASIC之间的差距，使FPGA成为深度学习的一个有吸引力的平台。
- en: The High-Level Synthesis (HLS) programming model enables the FPGA programmers
    to generate effective hardware designs conveniently using high-level languages
    such as C and C++. It avoids writing lots of Verilog or VHDL descriptions, which
    lowers the programming threshold and reduces the long design circle. Xilinx Vivado
    HLS and Intel FPGA SDK for OpenCL are two of the popular HLS tools targeting their
    own FPGAs. However, mapping DL models to FPGAs remains a complicated work even
    with HLS, because that 1) DL models are usually described by the languages of
    DL frameworks rather than bare mental C/C++ code, and 2) DL-specific information
    and optimizations are hard to be leveraged.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 高级合成（HLS）编程模型使FPGA程序员可以方便地使用C和C++等高级语言生成有效的硬件设计。这避免了编写大量的Verilog或VHDL描述，从而降低了编程门槛，缩短了设计周期。Xilinx
    Vivado HLS和Intel FPGA SDK for OpenCL是两个流行的HLS工具，针对各自的FPGA。然而，即使有HLS，将深度学习模型映射到FPGA仍然是复杂的工作，因为1）深度学习模型通常由深度学习框架的语言描述，而不是纯粹的C/C++代码，2）深度学习特定的信息和优化难以利用。
- en: 'The hardware-specific code generator targeting FPGA take the DL models or their
    domain-specific languages (DSLs) as the input, conduct the domain-specific (about
    FPGA and DL) optimizations and mappings, then generate the HLS or Verilog/VHDL
    and finally generate the bitstream. They can be classified into two categories
    according to the generated architectures of FPGA-based accelerators: the processor
    architecture and the streaming architecture (Venieris et al., [2018](#bib.bib94)).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 针对FPGA的硬件特定代码生成器将深度学习模型或其领域特定语言（DSLs）作为输入，进行领域特定（关于FPGA和深度学习）的优化和映射，然后生成HLS或Verilog/VHDL，最终生成比特流。根据生成的FPGA加速器架构，它们可以分为两类：处理器架构和流处理架构（Venieris
    et al., [2018](#bib.bib94)）。
- en: The processor architecture has similarities with general-purpose processors.
    An FPGA accelerator of this architecture usually comprises several Processing
    Units (PUs), which are comprised of on-chip buffers and multiple smaller Processing
    Engines (PEs). It usually has a virtual instruction set (ISA), and the control
    of hardware and the scheduling of the execution should be determined by software.
    What’s more, the static scheduling method avoids the overheads of von Neumann
    execution (including instruction fetching and decoding). A hardware template is
    a generic and fundamental implementation with configurable parameters. The DL
    code generator targeting this architecture adopt the hardware templates to generate
    the accelerator designs automatically. With the configurable parameters of templates,
    the code generator achieve the scalability and flexibility (Zhao et al., [2018](#bib.bib105)).
    The scalability means that the code generator can generate designs for FPGAs ranging
    from high-performance to power-efficient, and the flexibility means that the code
    generator can generate designs for various DL models with different layer types
    and parameters. The number of PUs and the number of PEs per PU are template parameters
    of importance. Besides, the tilling size and batch size are also essential scheduling
    parameters about mapping the DL models to PUs and PEs. All these parameters are
    usually determined by the design space exploration using various strategies, such
    as combining the performance model and auto-tuning. DNN Weaver (Sharma et al.,
    [2016](#bib.bib84)), Angel-Eye (Guo et al., [2017a](#bib.bib34)), ALAMO (Ma et al.,
    [2018](#bib.bib64)), FP-DNN (Guan et al., [2017](#bib.bib33)), SysArrayAccel (Wei
    et al., [2017](#bib.bib102)) are typical FPGA DL code generator targeting the
    processor architecture. What’s more, the PUs and PEs are usually responsible for
    coarse-grained basic operations such as matrix-vector multiplication, matrix-matrix
    multiplication, pooling, and some element-wise operations. The optimizations of
    these basic operations are mainly guided by the tradeoff between the parallelism
    and data reuse, which is similar to general optimizations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器架构与通用处理器有相似之处。这种架构的FPGA加速器通常由几个处理单元（PUs）组成，这些处理单元包括片上缓冲区和多个较小的处理引擎（PEs）。它通常具有虚拟指令集（ISA），并且硬件控制和执行调度应由软件确定。此外，静态调度方法避免了冯·诺依曼执行的开销（包括指令获取和解码）。硬件模板是一种通用的基本实现，具有可配置的参数。针对这种架构的DL代码生成器采用硬件模板来自动生成加速器设计。通过模板的可配置参数，代码生成器实现了可扩展性和灵活性（Zhao
    et al., [2018](#bib.bib105)）。可扩展性意味着代码生成器可以生成适用于从高性能到节能的FPGAs的设计，而灵活性意味着代码生成器可以为不同层类型和参数的各种DL模型生成设计。PUs的数量和每个PU的PE数量是重要的模板参数。此外，分块大小和批量大小也是将DL模型映射到PUs和PEs时的重要调度参数。这些参数通常通过使用各种策略（例如结合性能模型和自动调优）来确定设计空间探索。DNN
    Weaver（Sharma et al., [2016](#bib.bib84)），Angel-Eye（Guo et al., [2017a](#bib.bib34)），ALAMO（Ma
    et al., [2018](#bib.bib64)），FP-DNN（Guan et al., [2017](#bib.bib33)），SysArrayAccel（Wei
    et al., [2017](#bib.bib102)）是针对处理器架构的典型FPGA DL代码生成器。此外，PUs和PEs通常负责粗粒度的基本操作，如矩阵-向量乘法、矩阵-矩阵乘法、池化和一些逐元素操作。这些基本操作的优化主要受并行性和数据重用之间权衡的指导，这与通用优化类似。
- en: The streaming architecture has similarities with pipelines. An FPGA accelerator
    of this architecture consists of multiple different hardware blocks, and it nearly
    has one hardware block for each layer of an input DL model. With the input data
    of a DL model, this kind of accelerators process the data through the different
    hardware blocks in the same sequence with layers. Additionally, with the streaming
    input data, all hardware blocks can be fully utilized in a pipeline manner. However,
    the streaming architecture usually follows an initial assumption that the on-chip
    memory the computation resources on target FPGA are sufficient to accommodate
    the DL models, which bring barriers to deploy deep models with complicated layers.
    The DL code generator targeting this architecture can solve this problem by leveraging
    the reconfigurability of FPGA or adopting dynamic control flow. And the further
    optimization of a single block resembles that of basic operations of the processor
    architecture. fpgaConvNet (Venieris and Bouganis, [2016](#bib.bib93)), DeepBurning (Wang
    et al., [2016](#bib.bib99)), Haddoc2 (Abdelouahab et al., [2017](#bib.bib3)),
    and AutoCodeGen (Liu et al., [2016](#bib.bib60)) are typical corresponding DL
    code generator.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理架构与管道有相似之处。此架构的FPGA加速器由多个不同的硬件模块组成，并且几乎为输入DL模型的每一层配备一个硬件模块。通过DL模型的输入数据，这种加速器按层的顺序通过不同的硬件模块处理数据。此外，利用流输入数据，所有硬件模块可以以管道方式充分利用。然而，流处理架构通常基于一个初步假设，即目标FPGA上的片上内存计算资源足以容纳DL模型，这对部署具有复杂层的深度模型带来了障碍。针对这一架构的DL代码生成器可以通过利用FPGA的可重构性或采用动态控制流来解决这个问题。而单个模块的进一步优化类似于处理器架构的基本操作。fpgaConvNet (Venieris
    and Bouganis, [2016](#bib.bib93)), DeepBurning (Wang et al., [2016](#bib.bib99)),
    Haddoc2 (Abdelouahab et al., [2017](#bib.bib3)), 和 AutoCodeGen (Liu et al., [2016](#bib.bib60))是典型的DL代码生成器。
- en: For the detailed survey of specific compilation techniques that map DL models
    to FPGAs, the readers can refer to (Venieris et al., [2018](#bib.bib94); Zhao
    et al., [2018](#bib.bib105); Guo et al., [2017b](#bib.bib35)). Different from (Venieris
    et al., [2018](#bib.bib94); Zhao et al., [2018](#bib.bib105); Guo et al., [2017b](#bib.bib35)),
    this survey focuses on general DL compilation techniques that can be applied to
    broader DL hardware other than bounding to FPGA.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 关于将深度学习（DL）模型映射到FPGA的具体编译技术的详细调查，读者可以参考 (Venieris et al., [2018](#bib.bib94);
    Zhao et al., [2018](#bib.bib105); Guo et al., [2017b](#bib.bib35))。与 (Venieris
    et al., [2018](#bib.bib94); Zhao et al., [2018](#bib.bib105); Guo et al., [2017b](#bib.bib35))不同，本调查专注于可以应用于更广泛深度学习硬件的通用DL编译技术，而不仅仅限于FPGA。
- en: 3\. Common Design Architecture of DL Compilers
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. DL编译器的常见设计架构
- en: '![Refer to caption](img/5d6bfa752ac25b3f4b1fd32c1b4a0333.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5d6bfa752ac25b3f4b1fd32c1b4a0333.png)'
- en: Figure 2\. The overview of commonly adopted design architecture of DL compilers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. DL编译器常用设计架构概述。
- en: 'The common design architecture of a DL compiler primarily contains two parts:
    the compiler frontend and the compiler backend, as shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3\. Common Design Architecture of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey"). The intermediate representation (IR) is spread
    across both the frontend and the backend. Generally, IR is an abstraction of the
    program and is used for program optimizations. Specifically, the DL models are
    translated into multi-level IRs in DL compilers, where the high-level IR resides
    in the frontend, and the low-level IR resides in the backend. Based on the high-level
    IR, the compiler frontend is responsible for hardware-independent transformations
    and optimizations. Based on the low-level IR, the compiler backend is responsible
    for hardware-specific optimizations, code generation, and compilation. Note that
    this survey focuses on the design principles of DL compilers. For functional and
    experimental comparisons of DL compilers, the readers can refer to (Xing et al.,
    [2019](#bib.bib103); Li et al., [2020](#bib.bib56)).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习编译器的常见设计架构主要包括两个部分：编译器前端和编译器后端，如图[2](#S3.F2 "Figure 2 ‣ 3\. Common Design
    Architecture of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey")所示。中间表示（IR）分布在前端和后端。一般而言，IR是程序的抽象形式，用于程序优化。具体而言，深度学习模型在深度学习编译器中被转换为多层IR，其中高级IR位于前端，低级IR位于后端。基于高级IR，编译器前端负责硬件无关的转换和优化。基于低级IR，编译器后端负责硬件特定的优化、代码生成和编译。需要注意的是，本综述关注于深度学习编译器的设计原理。有关深度学习编译器的功能和实验比较，读者可以参考（Xing等，[2019](#bib.bib103)；Li等，[2020](#bib.bib56)）。'
- en: 'The high-level IR, also known as graph IR, represents the computation and the
    control flow and is hardware-independent. The design challenge of high-level IR
    is the ability of abstraction of the computation and the control flow, which can
    capture and express diverse DL models. The goal of the high-level IR is to establish
    the control flow and the dependency between the operators and the data, as well
    as provide an interface for graph-level optimizations. It also contains rich semantic
    information for compilation as well as offers extensibility for customized operators.
    The detailed discussion of high-level IR is presented in Section [4.1](#S4.SS1
    "4.1\. High-level IR ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '高级IR，也称为图IR，表示计算和控制流，并且与硬件无关。高级IR的设计挑战在于计算和控制流的抽象能力，这种能力能够捕捉和表达各种深度学习模型。高级IR的目标是建立操作符与数据之间的控制流和依赖关系，并提供图级优化的接口。它还包含丰富的语义信息用于编译，并提供对自定义操作符的扩展性。高级IR的详细讨论见第[4.1](#S4.SS1
    "4.1\. High-level IR ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey")节。'
- en: 'The low-level IR is designed for hardware-specific optimization and code generation
    on diverse hardware targets. Thus, the low-level IR should be fine-grained enough
    to reflect the hardware characteristics and represent the hardware-specific optimizations.
    It should also allow the use of mature third-party tool-chains in compiler backends
    such as Halide (Ragan-Kelley et al., [2013](#bib.bib78)), polyhedral model (Grosser,
    [2000](#bib.bib32)), and LLVM (Lattner and Adve, [2004](#bib.bib52)). The detailed
    discussion of low-level IR is presented in Section [4.2](#S4.SS2 "4.2\. Low-level
    IR ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive
    Survey").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '低级IR旨在实现针对特定硬件的优化和代码生成，以适应各种硬件目标。因此，低级IR应足够精细，以反映硬件特性并表示硬件特定的优化。它还应支持在编译器后端使用成熟的第三方工具链，如Halide（Ragan-Kelley等，[2013](#bib.bib78)）、多面体模型（Grosser，[2000](#bib.bib32)）和LLVM（Lattner和Adve，[2004](#bib.bib52)）。低级IR的详细讨论见第[4.2](#S4.SS2
    "4.2\. Low-level IR ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning Compiler:
    A Comprehensive Survey")节。'
- en: 'The frontend takes a DL model from existing DL frameworks as input, and then
    transforms the model into the computation graph representation (e.g., graph IR).
    The frontend needs to implement various format transformations To support the
    diverse formats in different frameworks. The computation graph optimizations incorporate
    the optimization techniques from both general-purpose compilers and the DL specific
    optimizations, which reduce the redundancy and improve the efficiency upon the
    graph IR. Such optimizations can be classified into node-level (e.g., nop elimination
    and zero-dim-tensor elimination), block-level (e.g., algebraic simplification,
    operator fusion, and operator sinking) and dataflow-level (e.g., CSE, DCE, static
    memory planning, and layout transformation). After the frontend, the optimized
    computation graph is generated and passed to the backend. The detailed discussion
    of the frontend is presented in Section [4.3](#S4.SS3 "4.3\. Frontend Optimizations
    ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive
    Survey").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '前端从现有DL框架中获取DL模型作为输入，然后将模型转换为计算图表示（例如，图IR）。前端需要实现各种格式转换，以支持不同框架中的多样格式。计算图优化结合了通用编译器的优化技术和DL特定的优化技术，这些优化技术减少了冗余，提高了图IR的效率。这些优化可以分为节点级（例如，nop消除和零维张量消除）、块级（例如，代数简化、操作符融合和操作符下沉）和数据流级（例如，CSE、DCE、静态内存规划和布局转换）。前端处理后，优化的计算图被生成并传递到后端。关于前端的详细讨论见第[4.3](#S4.SS3
    "4.3\. Frontend Optimizations ‣ 4\. Key Components of DL Compilers ‣ The Deep
    Learning Compiler: A Comprehensive Survey")节。'
- en: 'The backend transforms the high-level IR into low-level IR and performs hardware-specific
    optimizations. On the one hand, it can directly transform the high-level IR to
    third-party tool-chains such as LLVM IR to utilize the existing infrastructures
    for general-purpose optimizations and code generation. On the other hand, it can
    take advantage of the prior knowledge of both DL models and hardware characteristics
    for more efficient code generation, with customized compilation passes. The commonly
    applied hardware-specific optimizations include hardware intrinsic mapping, memory
    allocation and fetching, memory latency hiding, parallelization as well as loop
    oriented optimizations. To determine the optimal parameter setting in the large
    optimization space, two approaches are widely adopted in existing DL compilers
    such as auto-scheduling (e.g., polyhedral model) and auto-tuning (e.g., AutoTVM).
    The optimized low-level IR is compiled using JIT or AOT to generate codes for
    different hardware targets. The detailed discussion of the backend is presented
    in Section [4.4](#S4.SS4 "4.4\. Backend Optimizations ‣ 4\. Key Components of
    DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '后端将高级中间表示（IR）转换为低级IR，并执行特定于硬件的优化。一方面，它可以将高级IR直接转换为诸如LLVM IR的第三方工具链，以利用现有的基础设施进行通用优化和代码生成。另一方面，它可以利用对DL模型和硬件特性的先验知识，通过自定义编译过程进行更高效的代码生成。常用的硬件特定优化包括硬件内在映射、内存分配与获取、内存延迟隐藏、并行化以及面向循环的优化。为了确定大优化空间中的最佳参数设置，现有DL编译器广泛采用两种方法：自动调度（例如，多面体模型）和自动调优（例如，AutoTVM）。优化后的低级IR通过JIT或AOT编译，生成针对不同硬件目标的代码。关于后端的详细讨论见第[4.4](#S4.SS4
    "4.4\. Backend Optimizations ‣ 4\. Key Components of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey")节。'
- en: 4\. Key Components of DL Compilers
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. DL编译器的关键组件
- en: 4.1\. High-level IR
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 高级IR
- en: To overcome the limitation of IR adopted in traditional compilers that constrains
    the expression of complex computations used in DL models, existing DL compilers
    leverage high-level IR (as known as graph IR) with special designs for efficient
    code optimizations. To better understand the graph IR used in the DL compilers,
    we describe the representation and implementation of graph IR as follows.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服传统编译器中IR的局限性，这些局限性限制了DL模型中复杂计算的表达，现有DL编译器利用具有特殊设计的高级IR（也称为图IR）来实现高效的代码优化。为了更好地理解DL编译器中使用的图IR，我们如下描述了图IR的表示和实现。
- en: 4.1.1\. Representation of Graph IR
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 图IR的表示
- en: The representation of graph IR influences the expressiveness of graph IR and
    also decides the way the DL compilers analyze the graph IR.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图IR的表示影响图IR的表达能力，并决定DL编译器分析图IR的方式。
- en: 'DAG-based IR - DAG-based IR is one of the most traditional ways for the compilers
    to build a computation graph, with nodes and edges organized as a directed acyclic
    graph (DAG). In DL compilers (Chen et al., [2018b](#bib.bib18); Cyphers et al.,
    [2018](#bib.bib22); Rotem et al., [2018](#bib.bib80); Vasilache et al., [2018](#bib.bib92);
    Leary and Wang, [2017](#bib.bib54)), the nodes of a DAG represent the atomic DL
    operators (convolution, pooling, etc.), and the edges represent the tensors. And
    the graph is acyclic without loops, which differs from the data dependence graphs (Kuck
    et al., [1981](#bib.bib51)) (DDG) of generic compilers (Lattner and Adve, [2004](#bib.bib52);
    Lattner et al., [2020](#bib.bib53)). And with the help of the DAG computation
    graph, DL compilers can analyze the relationship and dependencies between various
    operators and use them to guide the optimizations. There are already plenty of
    optimizations on DDG, such as common sub-expression elimination (CSE) and dead
    code elimination (DCE). By combining the domain knowledge of DL with these algorithms,
    further optimizations can be applied to the DAG computation graph, which will
    be elaborated in Section [4.3](#S4.SS3 "4.3\. Frontend Optimizations ‣ 4\. Key
    Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey").
    DAG-based IR is convenient for programming and compiling due to its simplicity,
    but it has deficiencies such as semantic ambiguity caused by the missing definition
    of computation scope.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '基于 DAG 的 IR - 基于 DAG 的 IR 是编译器构建计算图的最传统方式之一，节点和边组织为有向无环图（DAG）。在 DL 编译器 (Chen
    等, [2018b](#bib.bib18); Cyphers 等, [2018](#bib.bib22); Rotem 等, [2018](#bib.bib80);
    Vasilache 等, [2018](#bib.bib92); Leary 和 Wang, [2017](#bib.bib54)) 中，DAG 的节点表示原子
    DL 操作符（卷积、池化等），边表示张量。图是无环的，没有循环，这不同于通用编译器的依赖关系图 (Kuck 等, [1981](#bib.bib51))（DDG）(Lattner
    和 Adve, [2004](#bib.bib52); Lattner 等, [2020](#bib.bib53))。借助 DAG 计算图，DL 编译器可以分析各种操作符之间的关系和依赖性，并利用这些信息指导优化。DDG
    上已经有很多优化，如公共子表达式消除（CSE）和死代码消除（DCE）。通过将 DL 的领域知识与这些算法结合，可以对 DAG 计算图进行进一步的优化，这将在第
    [4.3](#S4.SS3 "4.3\. Frontend Optimizations ‣ 4\. Key Components of DL Compilers
    ‣ The Deep Learning Compiler: A Comprehensive Survey") 节中详细阐述。由于其简洁性，基于 DAG 的
    IR 方便编程和编译，但也存在一些不足之处，例如由于计算范围缺失定义而导致的语义模糊性。'
- en: Let-binding-based IR - Let-binding is one method to solve the semantic ambiguity
    by offering let expression to certain functions with restricted scope used by
    many high-level programming languages such as Javascript (Goodman, [2007](#bib.bib31)),
    F# (Petricek and Syme, [2012](#bib.bib77)), and Scheme (Abelson et al., [1998](#bib.bib4)).
    When using the let keyword to define an expression, a let node is generated, and
    then it points to the operator and variable in the expression instead of just
    building computational relation between variables as a DAG. In DAG-based compiler,
    when a process needs to get the return value of one expression, it first accesses
    the corresponding node and searches related nodes, also known as recursive descent
    technique. In contrast, the let-binding based compiler figures out all results
    of the variables in let expression and builds a variable map. When a particular
    result is needed, the compiler looks up this map to decide the result of the expression.
    Among the DL compilers, the Relay IR (Roesch et al., [2019](#bib.bib79)) of TVM
    adopts both DAG-based IR and let-binding-based IR to obtain the benefits of both.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 let 绑定的 IR - Let 绑定是一种通过为某些具有限制范围的函数提供 let 表达式来解决语义模糊性的方法，这种方法被许多高级编程语言（如
    Javascript (Goodman, [2007](#bib.bib31))、F# (Petricek 和 Syme, [2012](#bib.bib77))、Scheme (Abelson
    等, [1998](#bib.bib4))）所使用。当使用 let 关键字定义一个表达式时，会生成一个 let 节点，然后它指向表达式中的运算符和变量，而不是仅仅建立变量之间的计算关系作为
    DAG。在基于 DAG 的编译器中，当一个过程需要获取一个表达式的返回值时，它首先访问相应的节点并搜索相关节点，这也被称为递归下降技术。相比之下，基于 let
    绑定的编译器会计算 let 表达式中所有变量的结果，并建立一个变量映射。当需要特定结果时，编译器会查阅这个映射以决定表达式的结果。在 DL 编译器中，TVM
    的 Relay IR (Roesch 等, [2019](#bib.bib79)) 采用了 DAG 基于 IR 和 let 绑定基于 IR 以获得两者的优点。
- en: Representing Tensor Computation - Different graph IRs have different ways to
    represent the computation on tensors. The operators of diverse DL frameworks are
    translated to graph IRs according to such specific representations. And the customized
    operators also need to be programmed in such representation. The representation
    of tensor computation can be divided into the following three categories.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 张量计算表示 - 不同的图形 IR 有不同的方式来表示张量计算。各种 DL 框架的操作符根据这些特定的表示方式被转换为图形 IR。同时，定制的操作符也需要以这种表示方式进行编程。张量计算的表示可以分为以下三类。
- en: '1) Function-based: The function-based representation just provides encapsulated
    operators, which is adopted by Glow, nGraph and XLA. Take High Level Optimizer
    (HLO, the IR of XLA) for example, it consists of a set of functions in symbolic
    programming, and most of them have no side-effect. The instructions are organized
    into three levels, including HloModule (the whole program), HloComputaion (a function),
    and HloInstruction (the operation). XLA uses HLO IR to represent both graph IR
    and operation IR so that the operation of HLO ranges from the dataflow level to
    the operator level.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基于函数的：基于函数的表示方式仅提供封装的操作符，这种方式被 Glow、nGraph 和 XLA 采用。以高层优化器（HLO，XLA 的 IR）为例，它由一组符号编程中的函数组成，其中大多数没有副作用。指令被组织成三个层次，包括
    HloModule（整个程序）、HloComputaion（一个函数）和 HloInstruction（操作）。XLA 使用 HLO IR 来表示图形 IR
    和操作 IR，使得 HLO 的操作范围从数据流层次到操作符层次。
- en: '2) Lambda expression: The lambda expression, an index formula expression, describes
    calculation by variable binding and substitution. Using lambda expression, programmers
    can define a computation quickly without implementing a new function. TVM represents
    the tensor computation using the tensor expression, which is based on the lambda
    expression. In TVM, computational operators in tensor expression are defined by
    the shape of output tensor and the lambda expression of computing rules.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2) Lambda 表达式：Lambda 表达式，即索引公式表达式，通过变量绑定和替换来描述计算。使用 Lambda 表达式，程序员可以快速定义计算而无需实现新函数。TVM
    使用基于 Lambda 表达式的张量表达式来表示张量计算。在 TVM 中，张量表达式中的计算操作符由输出张量的形状和计算规则的 Lambda 表达式定义。
- en: '3) Einstein notation: The Einstein notation, also known as the summation convention,
    is a notation to express summation. Its programming simplicity is superior to
    lambda expression. Taking TC for example, the indexes for temporary variables
    do not need to be defined. The IR can figure out the actual expression by the
    occurrence of undefined variables based on Einstein notation. In Einstein notation,
    the operators need to be associative and commutative. This restriction guarantees
    the reduction operator can be executed by any order, making it possible for further
    parallelization.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 爱因斯坦记号：爱因斯坦记号，也称为求和约定，是一种表达求和的记号。它在编程上的简便性优于 Lambda 表达式。以 TC 为例，临时变量的索引不需要定义。IR
    可以根据爱因斯坦记号通过未定义变量的出现来推断实际表达式。在爱因斯坦记号中，操作符需要是结合性和交换性的。这一限制保证了减少操作符可以按任意顺序执行，从而使得进一步的并行化成为可能。
- en: 4.1.2\. Implementation of Graph IR
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 图形 IR 的实现
- en: The implementation of graph IR in DL compilers fulfills the management of data
    and operation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: DL 编译器中图形 IR 的实现完成了数据和操作的管理。
- en: Data representation - The data in DL compilers (e.g., inputs, weights, and intermediate
    data) are usually organized in the form of tensors, which are also known as multi-dimensional
    arrays. The DL compilers can represent tensor data directly by memory pointers,
    or in a more flexible way by placeholders. A placeholder contains the size for
    each dimension of a tensor. Alternatively, the dimension sizes of the tensor can
    be marked as unknown. For optimizations, the DL compilers require the data layout
    information. In addition, the bound of iterators should be inferred according
    to the placeholders.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据表示 - DL 编译器中的数据（例如，输入、权重和中间数据）通常以张量的形式组织，张量也称为多维数组。DL 编译器可以通过内存指针直接表示张量数据，或者通过占位符以更灵活的方式表示。占位符包含张量每个维度的大小。或者，张量的维度大小也可以标记为未知。为了优化，DL
    编译器需要数据布局信息。此外，迭代器的边界应根据占位符进行推断。
- en: '1) Placeholder: Placeholder is widely used in symbolic programming (e.g., Lisp (McCarthy
    and Levin, [1965](#bib.bib66)), Tensorflow (Abadi et al., [2016](#bib.bib2))).
    A placeholder is simply a variable with explicit shape information (e.g., size
    in each dimension), and it will be populated with values at the later stage of
    the computation. It allows the programmers to describe the operations and build
    the computation graph without concerning the exact data elements, which helps
    separate the computation definition from the exact execution in DL compilers.
    Besides, it is convenient for the programmers to change the shape of input/output
    and other corresponding intermediate data by using placeholders without changing
    the computation definition.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 占位符：占位符广泛用于符号编程（例如，Lisp （McCarthy 和 Levin，[1965](#bib.bib66)），Tensorflow （Abadi
    等，[2016](#bib.bib2)））。占位符只是一个具有明确形状信息（例如，每个维度的大小）的变量，并且它将在计算的后期阶段填充值。它允许程序员描述操作并构建计算图，而不必担心确切的数据元素，这有助于将计算定义与
    DL 编译器中的确切执行分开。此外，使用占位符可以方便地改变输入/输出及其他相应中间数据的形状，而无需更改计算定义。
- en: '2) Unknown (Dynamic) shape representation: The unknown dimension size is usually
    supported when declaring the placeholders. For instance, TVM uses Any to represent
    an unknown dimension (e.g., $Tensor\langle(Any,3),fp32\rangle$); XLA uses None
    to achieve the same purpose (e.g., $tf.placeholder$ $(``float&quot;,[None,3])$);
    nGraph uses its PartialShape class. The unknown shape representation is necessary
    to support the dynamic model. However, to fully support dynamic model, the bound
    inference and dimension checking should be relaxed. In addition, extra mechanism
    should be implemented to guarantee memory validity.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 未知（动态）形状表示：在声明占位符时通常支持未知维度大小。例如，TVM 使用 Any 表示未知维度（例如，$Tensor\langle(Any,3),fp32\rangle$）；XLA
    使用 None 达到相同的目的（例如，$tf.placeholder$ $(``float&quot;,[None,3])$）；nGraph 使用其 PartialShape
    类。未知形状表示对于支持动态模型是必要的。然而，为了完全支持动态模型，边界推断和维度检查应当放宽。此外，应实现额外的机制以保证内存的有效性。
- en: '3) Data layout: The data layout describes how a tensor is organized in memory,
    and it is usually a mapping from logical indices to memory indices. The data layout
    usually includes the sequence of dimensions (e.g., NCHW and NHWC), tiling, padding,
    striding, etc. TVM and Glow represent data layout as operator parameters and require
    such information for computation and optimization. However, combining data layout
    information with operators rather than tensors enables intuitive implementation
    for certain operators and reduces the compilation overhead. XLA represents data
    layout as constraints related to its backend hardware. Relay and MLIR are going
    to add data layout information into their type systems for tensors.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 数据布局：数据布局描述了张量在内存中的组织方式，通常是逻辑索引到内存索引的映射。数据布局通常包括维度的顺序（例如，NCHW 和 NHWC）、切分、填充、步幅等。TVM
    和 Glow 将数据布局表示为操作符参数，并需要这些信息进行计算和优化。然而，将数据布局信息与操作符而不是张量结合，可以使某些操作符的实现更加直观，并减少编译开销。XLA
    将数据布局表示为与其后端硬件相关的约束。Relay 和 MLIR 将把数据布局信息添加到它们的张量类型系统中。
- en: '4) Bound inference: The bound inference is applied to determine the bound of
    iterators when compiling DL models in DL compilers. Although the tensor representation
    in DL compilers is convenient to describe the inputs and outputs, it exposes special
    challenges for inferring the iterator bound. The bound inference is usually performed
    recursively or iteratively, according to the computation graph and the known placeholders.
    For example, in TVM the iterators form a directed acyclic hyper-graph, where each
    node of the graph represents an iterator and each hyper-edge represents the relation
    (e.g., split, fuse or rebase) among two or more iterators. Once the bound of the
    root iterator is determined based on the shapes of placeholders, other iterators
    can be inferred according to the relations recursively.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 边界推断：边界推断用于确定在 DL 编译器中编译 DL 模型时迭代器的边界。尽管在 DL 编译器中张量表示方便用于描述输入和输出，但它对推断迭代器边界提出了特殊挑战。边界推断通常根据计算图和已知的占位符递归或迭代地进行。例如，在
    TVM 中，迭代器形成一个有向无环超图，其中图的每个节点代表一个迭代器，每个超边代表两个或更多迭代器之间的关系（例如，拆分、融合或重基）。一旦根据占位符的形状确定了根迭代器的边界，就可以根据关系递归地推断其他迭代器。
- en: Operators supported - The operators supported by DL compilers are responsible
    for representing the DL workloads, and they are nodes of the computation graph.
    The operators usually include algebraic operators (e.g., $+$, $\times$, $\exp$
    and topK), neural network operators (e.g., convolution and pooling), tensor operators
    (e.g., reshape, resize and copy), broadcast and reduction operators (e.g., min
    and argmin), as well as control flow operators (e.g., conditional and loop). Here,
    we choose three representative operators that are frequently used across different
    DL compilers for illustration. In addition, we discuss the case for customized
    operators.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的操作符 - DL编译器支持的操作符负责表示DL工作负载，它们是计算图的节点。这些操作符通常包括代数操作符（例如$+$、$\times$、$\exp$和topK）、神经网络操作符（例如卷积和池化）、张量操作符（例如reshape、resize和copy）、广播和归约操作符（例如min和argmin），以及控制流操作符（例如conditional和loop）。在这里，我们选择了三个在不同DL编译器中经常使用的代表性操作符进行说明。此外，我们还讨论了自定义操作符的情况。
- en: '1) Broadcast: The broadcast operators can replicate the data and generate new
    data with compatible shape. Without broadcast operators, the input tensor shapes
    are more constrained. For example, for an add operator, the input tensors are
    expected to be of the same shape. Some compilers such as XLA and Relay relax such
    restriction by offering the broadcasting operator. For example, XLA allows the
    element-wise addition on a matrix and a vector by replicating it until its shape
    matches the matrix.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 广播：广播操作符可以复制数据并生成具有兼容形状的新数据。如果没有广播操作符，输入张量的形状会受到更多限制。例如，对于一个加法操作符，输入张量期望具有相同的形状。一些编译器如XLA和Relay通过提供广播操作符来放宽这种限制。例如，XLA允许在矩阵和向量之间进行逐元素加法，通过复制向量直到其形状与矩阵匹配。
- en: '2) Control flow: Control flow is needed when representing complex and flexible
    models. Models such as RNN and Reinforcement learning (RL) depend on recurrent
    relations and data-dependent conditional execution (Yu et al., [2018](#bib.bib104)),
    which requires control flow. Without supporting control flow in graph IR of DL
    compilers, these models must rely on the control flow support of the host languages
    (e.g., if and while in Python) or static unrolling, which deteriorates the computation
    efficiency. Relay notices that arbitrary control flow can be implemented by recursion
    and pattern, which has been demonstrated by functional programming (Roesch et al.,
    [2019](#bib.bib79)). Therefore, it provides if operator and recursive function
    for implementing control flow. On the contrary, XLA represents control flow by
    special HLO operators such as while and conditional.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 控制流：在表示复杂和灵活的模型时，需要控制流。模型如RNN和强化学习（RL）依赖于递归关系和数据依赖的条件执行（Yu et al., [2018](#bib.bib104)），这需要控制流。如果图形IR中不支持控制流，这些模型必须依赖宿主语言的控制流支持（例如Python中的if和while）或静态展开，这会降低计算效率。Relay注意到任意控制流可以通过递归和模式来实现，这一点已通过函数式编程得到验证（Roesch
    et al., [2019](#bib.bib79)）。因此，它提供了if操作符和递归函数来实现控制流。相反，XLA通过特殊的HLO操作符（如while和conditional）来表示控制流。
- en: '3) Derivative: The derivative operator of an operator $Op$ takes the output
    gradients and the input data of $Op$ as its inputs, and then calculates the gradient
    of $Op$. Although some DL compilers (e.g., TVM and TC) support automatic differentiation (Van Merriënboer
    et al., [2018](#bib.bib89)), they require the derivatives of all operators in
    high-level IR when the chain rule is applied. TVM is working towards providing
    the derivative operators of both algebraic operators and neural network operators.
    The programmers can use these derivative operators for building the derivatives
    of customized operators. On the contrary, PlaidML can generate derivative operators
    automatically, even for customized operators. Notably, DL compilers unable to
    support derivative operators fail to provide the capability of model training.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 导数：操作符$Op$的导数操作符以$Op$的输出梯度和输入数据作为输入，然后计算$Op$的梯度。尽管一些DL编译器（例如TVM和TC）支持自动微分（Van
    Merriënboer et al., [2018](#bib.bib89)），但在应用链式法则时，它们要求高层IR中所有操作符的导数。TVM正在致力于提供代数操作符和神经网络操作符的导数操作符。程序员可以使用这些导数操作符来构建自定义操作符的导数。相反，PlaidML可以自动生成导数操作符，即使是自定义操作符。值得注意的是，无法支持导数操作符的DL编译器无法提供模型训练的能力。
- en: '4) Customized operators: It allows programmers to define their operators for
    a particular purpose. Providing support for customized operators improves the
    extensibility of DL compilers. For example, when defining new operators in Glow,
    the programmers need to realize the logic and node encapsulation. In addition,
    extra efforts are needed, such as the lowering step, operation IR generation,
    and instruction generation, if necessary. Whereas, TVM and TC require less programming
    efforts except describing the computation implementation. Specifically, the users
    of TVM only need to describe the computation and the schedule and declare the
    shape of input/output tensors. Moreover, the customized operators integrate Python
    functions through hooks, which further reduces the programmers’ burden.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 定制操作符：这允许程序员为特定目的定义自己的操作符。支持定制操作符提高了深度学习编译器的可扩展性。例如，在Glow中定义新操作符时，程序员需要实现逻辑和节点封装。此外，还需要额外的工作，如降级步骤、操作IR生成和指令生成（如有必要）。而TVM和TC除了描述计算实现外，编程工作量较少。具体来说，TVM的用户只需描述计算和调度，并声明输入/输出张量的形状。此外，定制操作符通过钩子集成Python函数，这进一步减轻了程序员的负担。
- en: 4.1.3\. Discussion
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 讨论
- en: Nearly all DL compilers have their unique high-level IRs. However, they share
    similar design philosophies, such as using DAG and let-binding to build the computation
    graph. In addition, they usually provide convenient ways for programmers to represent
    tensor computation. The data and operators designed in high-level IRs are flexible
    and extensible enough to support diverse DL models. More importantly, the high-level
    IRs are hardware-independent and thus can be applied with different hardware backend.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的深度学习编译器都有其独特的高层IR。然而，它们共享类似的设计理念，例如使用DAG和let-binding来构建计算图。此外，它们通常为程序员提供了表示张量计算的便利方法。高层IR中设计的数据和操作符足够灵活和可扩展，以支持各种深度学习模型。更重要的是，高层IR是硬件独立的，因此可以与不同的硬件后端一起使用。
- en: 4.2\. Low-level IR
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 低层IR
- en: 4.2.1\. Implementation of Low-Level IR
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 低层IR的实现
- en: 'Low-level IR describes the computation of a DL model in a more fine-grained
    representation than that in high-level IR, which enables the target-dependent
    optimizations by providing interfaces to tune the computation and memory access.
    In this section, we classify the common implementations of low-level IRs into
    three categories: Halide-based IR, polyhedral-based IR, and other unique IR.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 低层IR以比高层IR更细粒度的表示描述深度学习模型的计算，这通过提供调优计算和内存访问的接口来实现目标相关的优化。在本节中，我们将低层IR的常见实现分类为三类：基于Halide的IR、基于多面体的IR以及其他独特的IR。
- en: Halide-based IR - Halide is firstly proposed to parallelize image processing,
    and it is proven to be extensible and efficient in DL compilers (e.g., TVM). The
    fundamental philosophy of Halide is the separation of computation and schedule.
    Rather than giving a specific scheme directly, the compilers adopting Halide try
    various possible schedule and choose the best one. The boundaries of memory reference
    and loop nests in Halide are restricted to bounded boxes aligned to the axes.
    Thus, Halide cannot express the computation with complicated patterns (e.g., non-rectangular).
    Fortunately, the computations in DL are quite regular to be expressed perfectly
    by Halide. Besides, Halide can easily parameterize these boundaries and expose
    them to the tuning mechanism. The original IR of the Halide needs to be modified
    when applied to backend of DL compilers. For example, the input shape of Halide
    is infinite, whereas the DL compilers need to know the exact shape of data in
    order to map the operator to hardware instructions. Some compilers, such as TC,
    require the fixed size of data, to ensure better temporal locality for tensor
    data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Halide的IR - Halide最初是为了并行处理图像而提出的，并且已证明在深度学习编译器（如TVM）中具有扩展性和高效性。Halide的基本理念是计算和调度的分离。编译器在采用Halide时不会直接给出特定的方案，而是尝试各种可能的调度方案并选择最佳方案。Halide中的内存引用和循环嵌套的边界被限制在对齐到轴的有界盒子中。因此，Halide不能表达具有复杂模式的计算（例如，非矩形的）。幸运的是，深度学习中的计算相当规则，可以被Halide完美表达。此外，Halide可以轻松参数化这些边界并将其暴露给调优机制。在应用于深度学习编译器的后端时，Halide的原始IR需要进行修改。例如，Halide的输入形状是无限的，而深度学习编译器需要知道数据的确切形状，以便将操作符映射到硬件指令上。一些编译器，如TC，需要固定的数据大小，以确保张量数据的更好时间局部性。
- en: TVM has improved Halide IR into an independent symbolic IR by following efforts.
    It removes the dependency on LLVM and refactors the structure of both the project
    module and the IR design of Halide, pursuing better organization as well as accessibility
    for graph IR and frontend language such as Python. The re-usability is also improved,
    with a runtime dispatching mechanism implemented to add customized operators conveniently.
    TVM simplifies the variable definition from string matching to pointer matching,
    guaranteeing that each variable has a single define location (static single-assignment,
    SSA) (Cytron et al., [1991](#bib.bib23))).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TVM 通过以下努力将 Halide IR 改进为独立的符号 IR。它去除了对 LLVM 的依赖，并重构了项目模块的结构以及 Halide 的 IR 设计，追求更好的组织结构以及对图形
    IR 和前端语言（如 Python）的可访问性。重用性也得到了提升，运行时调度机制的实现方便地添加了自定义操作符。TVM 将变量定义从字符串匹配简化为指针匹配，保证每个变量只有一个定义位置（静态单分配，SSA）（Cytron
    等，[1991](#bib.bib23)）。
- en: Polyhedral-based IR - The polyhedral model is an important technique adopted
    in DL compilers. It uses linear programming, affine transformations, and other
    mathematical methods to optimize loop-based codes with static control flow of
    bounds and branches. In contrast to Halide, the boundaries of memory reference
    and loop nests can be polyhedrons with any shapes in the polyhedral model. Such
    flexibility makes polyhedral models widely used in generic compilers. However,
    such flexibility also prevents the integration with the tuning mechanisms. Nevertheless,
    due to the ability to deal with deeply nested loops, many DL compilers, such as
    TC and PlaidML (as the backend of nGraph), have adopted the polyhedral model as
    their low-level IR. The polyhedral-based IR makes it easy to apply various polyhedral
    transformations (e.g., fusion, tiling, sinking, and mapping), including both device-dependent
    and device-independent optimizations. There are many toolchains that are borrowed
    by polyhedral-based compilers, such as isl (Verdoolaege, [2010](#bib.bib97)),
    Omega (Kelly et al., [1996](#bib.bib49)), PIP (Feautrier, [1988](#bib.bib24)),
    Polylib (Loechner, [1999](#bib.bib61)), and PPL (Bagnara et al., [2006](#bib.bib10)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于多面体的 IR - 多面体模型是 DL 编译器采用的一项重要技术。它利用线性规划、仿射变换和其他数学方法来优化具有静态控制流的基于循环的代码。与 Halide
    相比，多面体模型中的内存引用和循环嵌套的边界可以是任意形状的多面体。这种灵活性使多面体模型在通用编译器中得到广泛应用。然而，这种灵活性也阻碍了与调优机制的集成。尽管如此，由于能够处理深度嵌套的循环，许多
    DL 编译器（如 TC 和 PlaidML（作为 nGraph 的后端））已将多面体模型作为其低级 IR。基于多面体的 IR 使得应用各种多面体变换（例如融合、平铺、沉降和映射）变得容易，包括设备相关和设备无关的优化。有许多工具链被多面体编译器借用，例如
    isl（Verdoolaege，[2010](#bib.bib97)），Omega（Kelly 等，[1996](#bib.bib49)），PIP（Feautrier，[1988](#bib.bib24)），Polylib（Loechner，[1999](#bib.bib61)）和
    PPL（Bagnara 等，[2006](#bib.bib10)）。
- en: TC has its unique design in low-level IR, which combines the Halide and polyhedral
    model. It uses Halide-based IR to represent the computation and adopts the polyhedral-based
    IR to represent the loop structures. TC presents detailed expressions through
    abstract instances and introduces specific node types. In brief, TC uses the domain
    node to specify the ranges of index variables and uses the context node to describe
    new iterative variables that are related to hardware. And it uses the band node
    to determine the order of iterations. A filter node represents an iterator combined
    with a statement instance. Set and sequence are keywords to specify the execution
    types (parallel and serial execution) for filters. Besides, TC uses extension
    nodes to describe other necessary instructions for code generation, such as the
    memory movement.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: TC 在低级 IR 方面有其独特的设计，它结合了 Halide 和多面体模型。它使用基于 Halide 的 IR 来表示计算，并采用基于多面体的 IR
    来表示循环结构。TC 通过抽象实例呈现详细的表达，并引入了特定的节点类型。简言之，TC 使用领域节点来指定索引变量的范围，使用上下文节点来描述与硬件相关的新迭代变量，并使用带状节点来确定迭代顺序。过滤器节点表示一个结合了语句实例的迭代器。集合和序列是用于指定过滤器执行类型（并行和串行执行）的关键字。此外，TC
    使用扩展节点来描述代码生成所需的其他指令，如内存移动。
- en: PlaidML uses polyhedral-based IR (called Stripe) to represent tensor operations.
    It creates a hierarchy of parallelizable code by extending the nesting of parallel
    polyhedral blocks to multiple levels. Besides, it allows nested polyhedrons to
    be allocated to nested memory units, providing a way to match the computation
    with the memory hierarchy. In Stripe, the hardware configuration is independent
    of the kernel code. The tags in Stripe (known as passes in other compilers) do
    not change the kernel structure, but provide additional information about the
    hardware target for the optimization passes. Stripe splits the DL operators into
    tiles that fit into local hardware resources.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PlaidML 使用基于多面体的 IR（称为 Stripe）来表示张量操作。它通过将并行多面体块的嵌套扩展到多个层次，创建了一个可并行化代码的层次结构。此外，它允许将嵌套的多面体分配到嵌套的内存单元，提供了一种将计算与内存层次匹配的方法。在
    Stripe 中，硬件配置与内核代码无关。Stripe 中的标签（在其他编译器中称为 passes）不会改变内核结构，而是提供有关硬件目标的附加信息以进行优化
    passes。Stripe 将 DL 运算符拆分为适合本地硬件资源的瓦片。
- en: Other unique IR - There are DL compilers implementing customized low-level IRs
    without using Halide and polyhedral model. Upon the customized low-level IRs,
    they apply hardware-specific optimizations and lowers to LLVM IR.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其他独特的 IR —— 有些 DL 编译器实现了自定义的低级 IR，而不使用 Halide 和多面体模型。在这些自定义低级 IR 上，它们应用了特定于硬件的优化，并将其转换为
    LLVM IR。
- en: 'The low-level IR in Glow is an instruction-based expression that operates on
    tensors referenced by addresses (Rotem et al., [2018](#bib.bib80)). There are
    two kinds of instruction-based functions in Glow low-level IR: declare and program.
    The first one declares the number of constant memory regions that live throughout
    the lifetime of the program (e.g., input, weight, bias). The second one is a list
    of locally allocated regions, including functions (e.g., conv and pool) and temporary
    variables. Instructions can run on the global memory regions or locally allocated
    regions. Besides, each operand is annotated with one of the qualifiers: @in indicates
    the operand reads from the buffer; @out indicates that the operand writes to the
    buffer; @inout indicates that the operand reads and writes to the buffer. These
    instructions and operand qualifiers help Glow determine when certain memory optimizations
    can be performed.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Glow 中的低级 IR 是基于指令的表达式，它在通过地址引用的张量上操作（Rotem 等， [2018](#bib.bib80)）。Glow 低级 IR
    中有两种基于指令的函数：声明和程序。第一个函数声明了程序生命周期内存在的常量内存区域的数量（例如，输入、权重、偏置）。第二个函数是一个局部分配区域的列表，包括函数（例如，conv
    和 pool）和临时变量。指令可以在全局内存区域或局部分配区域上运行。此外，每个操作数都用以下限定符之一进行注释：@in 表示操作数从缓冲区读取；@out
    表示操作数写入缓冲区；@inout 表示操作数从缓冲区读取和写入。这些指令和操作数限定符帮助 Glow 确定何时可以执行某些内存优化。
- en: MLIR is highly influenced by LLVM, and it is a purer compiler infrastructure
    than LLVM. MLIR reuses many ideas and interfaces in LLVM, and sits between the
    model representation and code generation. MLIR has a flexible type system and
    allows multiple abstraction levels, and it introduces dialects to represent these
    multiple levels of abstraction. Each dialect consists of a set of defined immutable
    operations. The current dialects of MLIR include TensorFlow IR, XLA HLO IR, experimental
    polyhedral IR, LLVM IR, and TensorFlow Lite. The flexible transformations between
    dialects are also supported. Furthermore, MLIR can create new dialects to connect
    to a new low-level compiler, which paves the way for hardware developers and compiler
    researchers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: MLIR 受到 LLVM 的高度影响，它比 LLVM 更纯粹的编译器基础设施。MLIR 重用 LLVM 中的许多思想和接口，并介于模型表示和代码生成之间。MLIR
    具有灵活的类型系统并支持多层次的抽象，它引入了方言来表示这些多个抽象层次。每个方言由一组定义的不可变操作组成。当前的 MLIR 方言包括 TensorFlow
    IR、XLA HLO IR、实验性的多面体 IR、LLVM IR 和 TensorFlow Lite。方言之间的灵活转换也得到了支持。此外，MLIR 可以创建新的方言以连接到新的低级编译器，为硬件开发人员和编译器研究人员铺平了道路。
- en: The HLO IR of XLA can be considered as both high-level IR and low-level IR because
    HLO is fine-grained enough to represent the hardware-specific information. Besides,
    HLO supports hardware-specific optimizations and can be used to emit LLVM IR.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: XLA 的 HLO IR 可以被视为高层 IR 和低层 IR，因为 HLO 足够细粒度地表示硬件特定的信息。此外，HLO 支持特定于硬件的优化，并且可以用于发出
    LLVM IR。
- en: 4.2.2\. Code Generation based on Low-Level IR
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 基于低级 IR 的代码生成
- en: 'The low-level IR adopted by most DL compilers can be eventually lowered to
    LLVM IR, and benefits from LLVM’s mature optimizer and code generator. Furthermore,
    LLVM can explicitly design custom instruction sets for specialized accelerators
    from scratch. However, traditional compilers may generate poor code when passed
    directly to LLVM IR. In order to avoid this situation, two approaches are applied
    by DL compilers to achieve hardware-dependent optimization: 1) perform target-specific
    loop transformation in the upper IR of LLVM (e.g., Halide-based IR and polyhedral-based
    IR), and 2) provide additional information about the hardware target for the optimization
    passes. Most DL compilers apply both approaches, but the emphasis is different.
    In general, the DL compilers that prefer frontend users (e.g., TC, TVM, XLA, and
    nGraph) might focus on 1), whereas the DL compilers that are more inclined to
    backend developers (e.g., Glow, PlaidML, and MLIR) might focus on 2).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习（DL）编译器采用的低级中间表示（IR）最终可以转换为LLVM IR，并受益于LLVM成熟的优化器和代码生成器。此外，LLVM可以从头开始为专用加速器设计自定义指令集。然而，传统编译器在直接传递给LLVM
    IR时可能生成较差的代码。为避免这种情况，DL编译器采用了两种方法来实现硬件相关优化：1）在LLVM的上层IR（例如，基于Halide的IR和基于多面体的IR）中执行针对特定目标的循环变换，以及2）为优化过程提供关于硬件目标的额外信息。大多数DL编译器同时应用这两种方法，但重点不同。一般来说，倾向于前端用户的DL编译器（例如，TC、TVM、XLA和nGraph）可能会侧重于1），而更倾向于后端开发者的DL编译器（例如，Glow、PlaidML和MLIR）可能会侧重于2）。
- en: 'The compilation scheme in DL compilers can be mainly classified into two categories:
    just-in-time (JIT) and ahead-of-time (AOT). For JIT compilers, it can generate
    executable codes on the fly, and they can optimize codes with better runtime knowledge.
    AOT compilers generate all executable binaries first and then execute them. Thus
    they have a larger scope in static analysis than JIT compilation. In addition,
    AOT approaches can be applied with cross-compilers of embedded platforms (e.g.,
    C-GOOD (Kang et al., [2018](#bib.bib47))) as well as enable execution on remote
    machines (TVM RPC) and customized accelerators.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: DL编译器中的编译方案主要可以分为两类：即时编译（JIT）和预编译（AOT）。对于JIT编译器，它可以动态生成可执行代码，并能够以更好的运行时知识优化代码。AOT编译器首先生成所有可执行二进制文件，然后执行它们。因此，它们在静态分析方面的范围比JIT编译更大。此外，AOT方法可以与嵌入式平台的交叉编译器（例如，C-GOOD (Kang等，[2018](#bib.bib47))）一起使用，并能够在远程机器（TVM
    RPC）和定制加速器上执行。
- en: 4.2.3\. Discussion
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 讨论
- en: In DL compilers, the low-level IR is a fine-grained representation of DL models,
    and it reflects detailed implantation of DL models on diverse hardware. The low-level
    IRs include Halide-based IRs, polyhedral-based IRs, and other unique IRs. Although
    they differ in designs, they leverage the mature compiler tool-chains and infrastructure,
    to provide tailored interfaces of hardware-specific optimizations and code generation.
    The design of low-level IRs can also impact the design of new DL accelerators
    (e.g., TVM HalideIR and Inferentia, as well as XLA HLO and TPU).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在DL编译器中，低级IR是DL模型的细粒度表示，它反映了DL模型在各种硬件上的详细实现。低级IR包括基于Halide的IR、基于多面体的IR以及其他独特的IR。虽然它们在设计上有所不同，但它们利用成熟的编译工具链和基础设施，提供针对硬件特定优化和代码生成的定制接口。低级IR的设计也可能影响新DL加速器的设计（例如，TVM
    HalideIR和Inferentia，以及XLA HLO和TPU）。
- en: 4.3\. Frontend Optimizations
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 前端优化
- en: After constructing the computation graph, the frontend applies graph-level optimizations.
    Many optimizations are easier to be identified and performed at graph level because
    the graph provides a global view of the computation. These optimizations are only
    applied to the computation graph, rather than the implementations on backends.
    Thus they are hardware-independent and can be applied to various backend targets.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建计算图后，前端会应用图级优化。许多优化在图级上更容易识别和执行，因为图提供了计算的全局视图。这些优化仅应用于计算图，而不是后端实现。因此，它们是硬件无关的，可以应用于各种后端目标。
- en: 'The frontend optimizations are usually defined by passes, and can be applied
    by traversing the nodes of the computation graph and performing the graph transformations.
    The frontend provides methods to 1) capture the specific features from the computation
    graph and 2) rewrite the graph for optimization. Besides the pre-defined passes,
    the developers can also define customized passes in the frontend. Most DL compilers
    can determine the shape of both input tensors and output tensors of every operation
    once a DL model is imported and transformed as a computation graph. This feature
    allows DL compilers to perform optimizations according to the shape information.
    Figure [3](#S4.F3 "Figure 3 ‣ 4.3\. Frontend Optimizations ‣ 4\. Key Components
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey") shows an
    example of computation graph optimizations with Tensorflow XLA.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '前端优化通常通过传递定义，并可以通过遍历计算图的节点并执行图变换来应用。前端提供了方法来 1) 从计算图中捕获特定特征，2) 重写图以进行优化。除了预定义的传递，开发者还可以在前端定义自定义传递。大多数深度学习编译器可以在导入并转换为计算图后确定每个操作的输入张量和输出张量的形状。这一特性使得深度学习编译器能够根据形状信息执行优化。图 [3](#S4.F3
    "Figure 3 ‣ 4.3\. Frontend Optimizations ‣ 4\. Key Components of DL Compilers
    ‣ The Deep Learning Compiler: A Comprehensive Survey") 显示了使用 Tensorflow XLA 的计算图优化示例。'
- en: 'In this section, we classify the frontend optimizations into three categories:
    1) node-level optimizations, 2) block-level (peephole, local) optimizations, and
    3) dataflow-level (global) optimizations.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将前端优化分为三类：1) 节点级优化，2) 块级（窥视孔、本地）优化，和 3) 数据流级（全局）优化。
- en: '![Refer to caption](img/f77908cf1eea0409c8df057ecb7795db.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f77908cf1eea0409c8df057ecb7795db.png)'
- en: Figure 3\. Example of computation graph optimizations, taken from the HLO graph
    of Alexnet on Volta GPU using Tensorflow XLA.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 计算图优化的示例，取自使用 Tensorflow XLA 在 Volta GPU 上的 Alexnet 的 HLO 图。
- en: 4.3.1\. Node-level optimizations
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 节点级优化
- en: The nodes of the computation graph are coarse enough to enable optimizations
    inside a single node. And the node-level optimizations include node elimination
    that eliminates unnecessary nodes and node replacement that replaces nodes with
    other lower-cost nodes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图的节点足够粗糙，以便在单个节点内部进行优化。节点级优化包括节点消除，去除不必要的节点，以及节点替换，用其他低成本节点替换节点。
- en: In general-purpose compilers, Nop Elimination removes the no-op instructions
    which occupy a small amount of space but specify no operation. In DL compilers,
    Nop Elimination is responsible for eliminating the operations lacking adequate
    inputs. For example, the sum node with only one input tensor can be eliminated,
    the padding node with zero padding width can be eliminated.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在通用编译器中，Nop 消除去除占用少量空间但不执行任何操作的无操作指令。在深度学习编译器中，Nop 消除负责消除缺少足够输入的操作。例如，仅有一个输入张量的求和节点可以被消除，填充宽度为零的填充节点可以被消除。
- en: Zero-dim-tensor elimination is responsible for removing the unnecessary operations
    whose inputs are zero-dimension tensors. Assume that $A$ is a zero-dimension tensor,
    and $B$ is a constant tensor, then the sum operation node of $A$ and $B$ can be
    replaced with the already existing constant node $B$ without affecting the correctness.
    Assume that $C$ is a 3-dimension tensor, but the shape of one dimension is zero,
    such as {0,2,3}, therefore, $C$ has no element, and the argmin/argmax operation
    node can be eliminated.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 零维张量消除负责去除输入为零维张量的不必要操作。假设 $A$ 是一个零维张量，$B$ 是一个常量张量，那么 $A$ 和 $B$ 的求和操作节点可以被已经存在的常量节点
    $B$ 替代，而不会影响正确性。假设 $C$ 是一个三维张量，但其中一个维度的形状为零，例如 {0,2,3}，因此 $C$ 没有元素，argmin/argmax
    操作节点可以被消除。
- en: 4.3.2\. Block-level optimizations
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 块级优化
- en: Algebraic simplification - The algebraic simplification optimizations consist
    of 1) algebraic identification, 2) strength reduction, with which we can replace
    more expensive operators by cheaper ones; 3) constant folding, with which we can
    replace the constant expressions by their values. Such optimizations consider
    a sequence of nodes, then take advantage of commutativity, associativity, and
    distributivity of different kinds of nodes to simplify the computation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 代数简化 - 代数简化优化包括 1) 代数识别，2) 强度减小，通过这种方式我们可以用更便宜的操作符替换更昂贵的操作符；3) 常量折叠，通过这种方式我们可以用常量表达式的值替换常量表达式。这些优化考虑节点序列，然后利用不同类型节点的交换性、结合性和分配性来简化计算。
- en: 'In addition to the typical operators ($+$, $\times$, etc.), the algebraic simplification
    can also be applied to DL specific operators (e.g., reshape, transpose, and pooling).
    The operators can be reordered and sometimes eliminated, which reduces redundancy
    and improves the efficiency. Here we illustrate the common cases where algebraic
    simplification can be applied: 1) optimization of computation order, in such case,
    the optimization finds and removes reshape/transpose operations according to specific
    characteristics. Taking the matrix multiplication (GEMM) for example, there are
    two matrices (e.g., $A$ and $B$), both matrices are transposed (to produce $A^{T}$
    and $B^{T}$, respectively), then $A^{T}$ and $B^{T}$ are multiplied together.
    However, a more efficient way to implement GEMM is to switch the order of the
    arguments $A$ and $B$, multiply them together, and then transpose the output of
    the GEMM, which reduces two transpose to just one; 2) optimization of node combination,
    in such case, the optimization combines multiple consecutive transpose nodes into
    a single node, eliminates identity transpose nodes, and optimizes transpose nodes
    into reshape nodes when they actually move no data; 3) optimization of ReduceMean
    nodes, in such case, the optimization performs substitutions of ReduceMean with
    AvgPool node (e.g., in Glow), if the input of the reduce operator is 4D with the
    last two dimensions to be reduced.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了典型的运算符（$+$、$\times$ 等），代数简化也可以应用于深度学习特定的运算符（例如，reshape、transpose 和 pooling）。这些运算符可以重新排序，有时可以被消除，这样可以减少冗余并提高效率。以下是代数简化可以应用的常见情况：1）计算顺序优化，在这种情况下，优化会根据特定特征找到并移除
    reshape/transpose 操作。以矩阵乘法（GEMM）为例，假设有两个矩阵（例如，$A$ 和 $B$），这两个矩阵都被转置（产生 $A^{T}$
    和 $B^{T}$），然后 $A^{T}$ 和 $B^{T}$ 被相乘。然而，实现 GEMM 的一种更高效的方法是交换参数 $A$ 和 $B$ 的顺序，将它们相乘，然后转置
    GEMM 的输出，这样可以将两个转置减少为一个；2）节点组合优化，在这种情况下，优化将多个连续的转置节点合并为一个节点，消除恒等转置节点，并在转置节点实际上没有移动数据时将其优化为
    reshape 节点；3）ReduceMean 节点优化，在这种情况下，如果 reduce 运算符的输入是 4D，且最后两个维度被减少，则优化会用 AvgPool
    节点（例如，在 Glow 中）替代 ReduceMean 节点。
- en: 'Operator fusion - Operator fusion is indispensable optimization of DL compilers.
    It enables better sharing of computation, eliminates intermediate allocations,
    facilitates further optimization by combining loop nests (Roesch et al., [2019](#bib.bib79)),
    as well as reduces launch and synchronization overhead (Vasilache et al., [2018](#bib.bib92)).
    In TVM, the operators are classified into four categories: injective, reduction,
    complex-out-fusible, and opaque. When the operators are defined, their corresponding
    categories are determined. Targeting the above categories, TVM designs the fusion
    rules across operators. In TC, fusion is performed differently based on the automatic
    polyhedron transformations. However, how to identify and fuse more complicated
    graph patterns, such as blocks with multiple broadcast and reduce nodes, remains
    to be a problem. Recent works (Long et al., [2018](#bib.bib63), [2019](#bib.bib62))
    try to tackle this problem and propose a framework to explore and optimize aggressive
    fusion plans. It supports not only element-wise and reduction nodes, but also
    other computation/memory intensive nodes with complex dependencies.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符融合 - 运算符融合是深度学习编译器中不可或缺的优化。它能够更好地共享计算，消除中间分配，通过结合循环嵌套（Roesch 等，[2019](#bib.bib79)）促进进一步优化，并减少启动和同步开销（Vasilache
    等，[2018](#bib.bib92)）。在 TVM 中，运算符被分类为四类：单射、归约、复杂不可融合和不透明。当运算符被定义时，其对应的类别也被确定。针对上述类别，TVM
    设计了跨运算符的融合规则。在 TC 中，融合是基于自动多面体变换进行的。然而，如何识别和融合更复杂的图模式，例如包含多个广播和归约节点的块，仍然是一个问题。最近的研究（Long
    等，[2018](#bib.bib63)，[2019](#bib.bib62)）尝试解决这个问题，并提出了一个框架来探索和优化激进的融合计划。该框架不仅支持逐元素和归约节点，还支持具有复杂依赖关系的其他计算/内存密集型节点。
- en: Operator sinking - This optimization sinks the operations such as transposes
    below operations such as batch normalization, ReLU, sigmoid, and channel shuffle.
    By this optimization, many similar operations are moved closer to each other,
    creating more opportunities for algebraic simplification.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符下沉 - 这种优化将诸如转置等操作下沉到批量归一化、ReLU、sigmoid 和通道混洗等操作下方。通过这种优化，许多相似的操作被移动得更靠近，从而创造了更多的代数简化机会。
- en: 4.3.3\. Dataflow-level optimizations
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 数据流级别优化
- en: Common sub-expression elimination (CSE) - An expression $E$ is a common sub-expression
    if the value of $E$ is previously computed, and the value of $E$ has not to be
    changed since previous computation (Aho et al., [1986](#bib.bib7)). In this case,
    the value of $E$ is computed once, and the already computed value of $E$ can be
    used to avoid recomputing in other places. The DL compilers search for common
    sub-expressions through the whole computation graph and replace the following
    common sub-expressions with the previously computed results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 公共子表达式消除（CSE） - 如果表达式 $E$ 的值之前已计算，并且 $E$ 的值自之前计算以来未改变，则 $E$ 是公共子表达式（Aho et al.,
    [1986](#bib.bib7)）。在这种情况下，$E$ 的值只计算一次，已计算的 $E$ 的值可以用于避免在其他地方重新计算。深度学习编译器通过整个计算图搜索公共子表达式，并用之前计算的结果替换以下公共子表达式。
- en: Dead code elimination (DCE) - A set of code is dead if its computed results
    or side-effects are not used. And the DCE optimization removes the dead code.
    The dead code is usually not caused by programmers but is caused by other graph
    optimizations. Thus, the DCE, as well as CSE, are applied after other graph optimizations.
    Other optimizations, such as dead store elimination (DSE), which removes stores
    into tensors that are never going to be used, also belong to DCE.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 死代码消除（DCE） - 如果一组代码的计算结果或副作用未被使用，则该代码是死代码。DCE优化会删除这些死代码。死代码通常不是程序员造成的，而是由其他图优化导致的。因此，DCE以及CSE会在其他图优化之后应用。其他优化，如死存储消除（DSE），也属于DCE，它会删除那些永远不会被使用的张量存储。
- en: 'Static memory planning - Static memory planning optimizations are performed
    to reuse the memory buffers as much as possible. Usually, there are two approaches:
    in-place memory sharing and standard memory sharing. The in-place memory sharing
    uses the same memory for input and output for an operation, and just allocates
    one copy of memory before computing. Standard memory sharing reuses the memory
    of previous operations without overlapping. The static memory planning is done
    offline, which allows more complicated planning algorithms to be applied. A recent
    work (Ahn et al., [2020a](#bib.bib5)) firstly designs and performs memory-aware
    scheduling to minimize the peak activation memory footprint on edge devices, which
    presents new research directions of memory planning on memory-constrained devices.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 静态内存规划 - 静态内存规划优化旨在尽可能重用内存缓冲区。通常有两种方法：就地内存共享和标准内存共享。就地内存共享为操作的输入和输出使用相同的内存，并在计算前只分配一份内存。标准内存共享在没有重叠的情况下重用之前操作的内存。静态内存规划是在离线完成的，这允许应用更复杂的规划算法。最近的一项工作（Ahn
    et al., [2020a](#bib.bib5)）首次设计并执行了内存感知调度，以最小化边缘设备上的峰值激活内存占用，提出了内存受限设备上内存规划的新研究方向。
- en: Layout transformation - Layout transformation tries to find the best data layouts
    to store tensors in the computation graph and then inserts the layout transformation
    nodes to the graph. Note that the actual transformation is not performed here,
    instead, it will be performed when evaluating the computation graph by the compiler
    backend.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 布局变换 - 布局变换尝试找到最佳的数据布局来存储计算图中的张量，然后将布局变换节点插入到图中。注意，这里并不会实际进行变换，而是在编译器后端评估计算图时进行。
- en: In fact, the performance of the same operation in different data layouts is
    different, and the best layouts are also different on different hardware. For
    example, operations in the NCHW format on GPU usually run faster, so it is efficient
    to transform to NCHW format on GPU (e.g., TensorFlow). Some DL compilers rely
    on hardware-specific libraries to achieve higher performance, and the libraries
    may require certain layouts. Besides, some DL accelerators prefer more complicated
    layouts (e.g., tile). In addition, edge devices usually equip heterogenous computing
    units, and different units may require different data layouts for better utilization,
    thus layout transformation needs careful considerations. Therefore, the compilers
    need to provide a way to perform layout transformations across various hardware.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，相同操作在不同数据布局中的性能是不同的，而最佳布局在不同硬件上也有所不同。例如，GPU上的NCHW格式操作通常运行得更快，因此在GPU上转换为NCHW格式（如TensorFlow）是有效的。一些深度学习编译器依赖于特定硬件的库以实现更高性能，这些库可能需要特定的布局。此外，一些深度学习加速器更喜欢更复杂的布局（如tile）。此外，边缘设备通常配备异构计算单元，不同单元可能需要不同的数据布局以便更好地利用，因此布局变换需要仔细考虑。因此，编译器需要提供一种在各种硬件上进行布局变换的方法。
- en: Not only the data layouts of tensors have a nontrivial influence on the final
    performance, but also the transformation operations have a significant overhead.
    Because they also consume the memory and computation resource.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的数据布局不仅对最终性能有非凡影响，转换操作也有显著的开销，因为它们还会消耗内存和计算资源。
- en: A recent work (Liu et al., [2019](#bib.bib59)) based on TVM targeting on CPUs
    alters the layout of all convolution operations to NCHW[$x$]c first in the computation
    graph, in which c means the split sub-dimension of channel C and $x$ indicates
    the split size of the sub-dimension. Then all $x$ parameters are globally explored
    by auto-tuning when providing hardware details, such as cache line size, vectorization
    unit size, and memory access pattern, during hardware-specific optimizations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一项工作（Liu et al., [2019](#bib.bib59)）基于针对 CPU 的 TVM，将计算图中所有卷积操作的布局更改为 NCHW[$x$]c，其中
    c 表示通道 C 的分裂子维度，而 $x$ 表示子维度的分裂大小。然后，在提供硬件细节（如缓存行大小、向量化单元大小和内存访问模式）时，通过自动调优全局探索所有
    $x$ 参数，以进行硬件特定优化。
- en: 4.3.4\. Discussion
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4\. 讨论
- en: 'The frontend is one of the most important components in DL compilers, which
    is responsible for transformation from DL models to high-level IR (e.g., computation
    graph) and hardware-independent optimizations based on high-level IR. Although
    the implementation of frontend may differ in the data representation and operator
    definition of high-level IR across DL compilers, the hardware-independent optimizations
    converge at three levels: node-level, block-level, and dataflow-level. The optimization
    methods at each level leverage the DL specific as well as general compilation
    optimization techniques, which reduce the computation redundancy as well as improve
    the performance of DL models at the computation graph level.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 前端是 DL 编译器中最重要的组件之一，负责将 DL 模型转换为高级 IR（例如计算图）以及基于高级 IR 的硬件独立优化。尽管前端的实现可能在数据表示和高级
    IR 的操作符定义上有所不同，但硬件独立优化在三个层次上趋于一致：节点级、块级和数据流级。每个层次的优化方法利用了 DL 特定的以及通用的编译优化技术，从而减少计算冗余，并提高
    DL 模型在计算图级别的性能。
- en: 4.4\. Backend Optimizations
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 后端优化
- en: The backends of DL compilers have commonly included various hardware-specific
    optimizations, auto-tuning techniques, and optimized kernel libraries. Hardware-specific
    optimizations enable efficient code generation for different hardware targets.
    Whereas, auto-tuning has been essential in the compiler backend to alleviate the
    manual efforts to derive the optimal parameter configurations. Besides, highly-optimized
    kernel libraries are also widely used on general-purpose processors and other
    customized DL accelerators.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: DL 编译器的后端通常包括各种硬件特定优化、自动调优技术和优化的内核库。硬件特定优化可以实现针对不同硬件目标的高效代码生成。而自动调优在编译器后端中至关重要，能够减轻手动确定最佳参数配置的工作。此外，高度优化的内核库也广泛应用于通用处理器和其他定制的
    DL 加速器。
- en: 4.4.1\. Hardware-specific Optimization
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 硬件特定优化
- en: 'Hardware-specific optimizations, also known as target-dependent optimizations,
    are applied to obtain high-performance codes targeting specific hardware. One
    way to apply the backend optimizations is to transform the low-level IR into LLVM
    IR, to utilize the LLVM infrastructure to generate optimized CPU/GPU codes. The
    other way is to design customized optimizations with DL domain knowledge, leveraging
    the target hardware more efficiently. Since hardware-specific optimizations are
    tailored for particular hardware and cannot be included exhaustively in this paper,
    we present five widely adopted approaches in existing DL compilers. The overview
    of these hardware-specific optimizations is shown in Figure [4](#S4.F4 "Figure
    4 ‣ 4.4.1\. Hardware-specific Optimization ‣ 4.4\. Backend Optimizations ‣ 4\.
    Key Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey"),
    and the detailed descriptions are provided as follows.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '硬件特定优化，也称为目标依赖优化，旨在获得针对特定硬件的高性能代码。应用后端优化的一种方法是将低级 IR 转换为 LLVM IR，利用 LLVM 基础设施生成优化的
    CPU/GPU 代码。另一种方法是设计基于 DL 领域知识的定制优化，更有效地利用目标硬件。由于硬件特定优化是针对特定硬件量身定制的，无法在本文中详尽列出，我们介绍了现有
    DL 编译器中五种广泛采用的方法。这些硬件特定优化的概述如图 [4](#S4.F4 "Figure 4 ‣ 4.4.1\. Hardware-specific
    Optimization ‣ 4.4\. Backend Optimizations ‣ 4\. Key Components of DL Compilers
    ‣ The Deep Learning Compiler: A Comprehensive Survey") 所示，详细描述如下。'
- en: '![Refer to caption](img/3460aa904a8e34fd5b6ea484a6a93545.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/3460aa904a8e34fd5b6ea484a6a93545.png)'
- en: Figure 4\. Overview of hardware-specific optimizations applied in DL compilers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 硬件特定优化在DL编译器中的概述。
- en: Hardware intrinsic mapping - Hardware intrinsic mapping can transform a certain
    set of low-level IR instructions to kernels that have already been highly optimized
    on the hardware. In TVM, the hardware intrinsic mapping is realized in the method
    of extensible tensorization, which can declare the behavior of hardware intrinsic
    and the lowering rule for intrinsic mapping. This method enables the compiler
    backend to apply hardware implementations as well as highly optimized handcraft
    micro-kernels to a specific pattern of operations, which results in a significant
    performance gain. Whereas, Glow supports hardware intrinsic mapping such as quantization.
    It can estimate the possible numeric range for each stage of the neural network
    and support profile-guided optimization to perform quantization automatically.
    Besides, Halide/TVM maps specific IR patterns to SIMD opcodes on each architecture
    to avoid the inefficiency of LLVM IR mapping when encountering vector patterns.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件内建映射 - 硬件内建映射可以将一组低级IR指令转换为在硬件上已高度优化的内核。在TVM中，硬件内建映射通过可扩展张量化的方法实现，该方法可以声明硬件内建的行为和内建映射的降级规则。这种方法使编译器后端能够将硬件实现以及高度优化的手工微内核应用于特定的操作模式，从而显著提升性能。而Glow则支持诸如量化的硬件内建映射。它可以估算神经网络每个阶段的可能数值范围，并支持基于配置文件的优化，自动执行量化。此外，Halide/TVM将特定IR模式映射到每个架构上的SIMD操作码，以避免在遇到向量模式时LLVM
    IR映射的低效。
- en: Memory allocation and fetching - Memory allocation is another challenge in code
    generation, especially for GPUs and customized accelerators. For example, GPU
    contains primarily shared memory space (lower access latency with limited memory
    size) and local memory space (higher access latency with large capacity). Such
    memory hierarchy requires efficient memory allocation and fetching techniques
    for improving data locality. To realize this optimization, TVM introduces the
    scheduling concept of memory scope. Memory scope schedule primitives can tag a
    compute stage as shared or thread-local. For compute stages tagged as shared,
    TVM generates code with shared memory allocation as well as cooperative data fetching,
    which inserts memory barrier at the proper code position to guarantee correctness.
    Besides, TC also provides similar features (known as memory promotion) by extending
    PPCG (Verdoolaege et al., [2013](#bib.bib98)) compiler. However, TC only supports
    limited pre-defined rules. Particularly, TVM enables special buffering in accelerators
    through memory scope schedule primitives.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 内存分配和提取 - 内存分配是代码生成中的另一个挑战，尤其是对于GPU和定制加速器。例如，GPU主要包含共享内存空间（访问延迟较低但内存大小有限）和本地内存空间（访问延迟较高但容量较大）。这种内存层次结构要求高效的内存分配和提取技术以提高数据局部性。为了实现这种优化，TVM引入了内存范围的调度概念。内存范围调度原语可以将计算阶段标记为共享或线程本地。对于标记为共享的计算阶段，TVM生成带有共享内存分配以及协作数据提取的代码，这在适当的代码位置插入内存屏障以保证正确性。此外，TC通过扩展PPCG（Verdoolaege等，[2013](#bib.bib98)）编译器也提供类似的功能（称为内存提升）。然而，TC仅支持有限的预定义规则。特别是，TVM通过内存范围调度原语在加速器中实现了特殊的缓冲。
- en: Memory latency hiding - Memory latency hiding is also an important technique
    used in the backend by reordering the execution pipeline. As most DL compilers
    support parallelization on CPU and GPU, memory latency hiding can be naturally
    achieved by hardware (e.g., warp context switching on GPU). But for TPU-like accelerators
    with decoupled access-execute (DAE) architecture, the backend needs to perform
    scheduling and fine-grained synchronization to obtain correct and efficient codes.
    To achieve better performance as well as reduce programming burden, TVM introduces
    virtual threading schedule primitive, which enables users to specify the data
    parallelism on virtualized multi-thread architecture. Then TVM lowers these virtually
    parallelized threads by inserting necessary memory barriers and interleaves the
    operations from these threads into a single instruction stream, which forms a
    better execution pipeline of each thread to hide the memory access latency.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 内存延迟隐藏 - 内存延迟隐藏也是后端使用的一种重要技术，通过重新排序执行流水线来实现。由于大多数深度学习编译器支持CPU和GPU上的并行化，内存延迟隐藏可以通过硬件自然实现（例如，GPU上的warp上下文切换）。但对于具有解耦访问-执行（DAE）架构的TPU类加速器，后端需要执行调度和细粒度同步以获得正确且高效的代码。为了实现更好的性能并减少编程负担，TVM引入了虚拟线程调度原语，使用户能够在虚拟化的多线程架构上指定数据并行性。然后，TVM通过插入必要的内存屏障来降低这些虚拟并行线程，并将这些线程的操作交错到单一指令流中，从而形成每个线程的更好执行流水线，以隐藏内存访问延迟。
- en: Loop oriented optimizations - Loop oriented optimizations are also applied in
    the backend to generate efficient codes for target hardware. Since Halide and
    LLVM (Lattner and Adve, [2004](#bib.bib52)) (integrated with the polyhedral method)
    have already incorporated such optimization techniques, some DL compilers leverage
    Halide and LLVM in their backends. The key techniques applied in loop oriented
    optimizations include loop fusion, sliding windows, tiling, loop reordering, and
    loop unrolling.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 循环导向优化 - 循环导向优化也应用于后端以生成针对目标硬件的高效代码。由于Halide和LLVM（Lattner和Adve，[2004](#bib.bib52)）（与多面体方法集成）已经包含了这些优化技术，一些深度学习编译器在其后端利用了Halide和LLVM。循环导向优化中应用的关键技术包括循环融合、滑动窗口、分块、循环重排和循环展开。
- en: '1) Loop fusion: Loop fusion is a loop optimization technique that can fuse
    loops with the same boundaries for better data reuse. For compilers such as PlaidML,
    TVM, TC, and XLA, such optimization is performed by the Halide schedule or polyhedral
    approach, while Glow applies loop fusion by its operator stacking.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 循环融合：循环融合是一种循环优化技术，可以将具有相同边界的循环融合在一起以实现更好的数据重用。对于像PlaidML、TVM、TC和XLA这样的编译器，这种优化是通过Halide调度或多面体方法来执行的，而Glow则通过其操作符堆叠应用循环融合。
- en: '2) Sliding windows: Sliding windows is a loop optimization technique adopted
    by Halide. Its central concept is to compute values when needed and store them
    on the fly for data reuse until they are no longer required. As sliding windows
    interleaves the computation of two loops and make them serial, it is a tradeoff
    between parallelism and data reuse.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 滑动窗口：滑动窗口是一种由Halide采用的循环优化技术。其核心概念是在需要时计算值并即时存储以实现数据重用，直到这些值不再需要。由于滑动窗口将两个循环的计算交错在一起并使它们串行化，它在并行性和数据重用之间做了权衡。
- en: '3) Tiling: Tiling splits loops into several tiles, and thus loops are divided
    into outer loops iterating through tiles and inner loops iterating inside a tile.
    This transformation enables better data locality inside a tile by fitting a tile
    into hardware caches. As the size of a tile is hardware-specific, many DL compilers
    determine the tiling pattern and size by auto-tuning.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 分块：分块将循环拆分成多个块，因此循环被划分为迭代通过块的外循环和在块内部迭代的内循环。这种变换通过将一个块适配到硬件缓存中，提升了块内部的数据局部性。由于块的大小是特定于硬件的，许多深度学习编译器通过自动调优来确定分块模式和大小。
- en: '4) Loop reordering: Loop reordering (also known as loop permutation) changes
    the order of iterations in a nested loop, which can optimize the memory access
    and thus increase the spatial locality. It is specific to data layout and hardware
    features. However, it is not safe to perform loop reordering when there are dependencies
    along the iteration order.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 循环重排：循环重排（也称为循环置换）改变了嵌套循环中迭代的顺序，这可以优化内存访问，从而提高空间局部性。它特定于数据布局和硬件特性。然而，当迭代顺序中存在依赖关系时，执行循环重排是不安全的。
- en: '5) Loop unrolling: Loop unrolling can unroll a specific loop to a fixed number
    of copies of loop bodies, which allows the compilers to apply aggressive instruction-level
    parallelism. Usually, loop unrolling is applied in combination with loop split,
    which first splits the loop into two nested loops and then unrolls the inner loop
    completely.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 循环展开：循环展开可以将特定的循环展开为固定数量的循环体副本，这允许编译器应用激进的指令级并行性。通常，循环展开与循环拆分结合使用，循环拆分首先将循环拆分为两个嵌套循环，然后完全展开内循环。
- en: 'Parallelization - As modern processors generally support multi-threading and
    SIMD parallelism, the compiler backend needs to exploit parallelism to maximize
    hardware utilization for high performance. Halide uses a schedule primitive called
    parallel to specify the parallelized dimension of the loop for thread-level parallelization
    and supports GPU parallelization by mapping loop dimensions tagged as parallel
    with annotation of block and thread. And it replaces a loop of size $n$ with a
    n-wide vector statement, which can be mapped to hardware-specific SIMD opcodes
    through hardware intrinsic mapping. Stripe develops a variant of the polyhedral
    model called nested polyhedral model, which introduces parallel polyhedral block
    as its basic execution element of iteration. After this extension, a nested polyhedral
    model can detect hierarchy parallelization among levels of tiling and striding.
    In addition, some DL compilers rely on handcraft libraries such as Glow or optimized
    math libraries provided by hardware vendors (discussed in Section [4.4.3](#S4.SS4.SSS3
    "4.4.3\. Optimized Kernel Libraries ‣ 4.4\. Backend Optimizations ‣ 4\. Key Components
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey")). In the
    meanwhile, Glow offloads the vectorization to LLVM because the LLVM auto-vectorizer
    works well when the information of tensor dimension and loop trip count is provided.
    However, exploiting the parallelism entirely by compiler backend allows to apply
    more domain-specific knowledge of DL models, and thus leads to higher performance
    at the expense of more engineering efforts.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化 - 由于现代处理器通常支持多线程和SIMD并行性，编译器后端需要利用并行性以最大化硬件利用率，从而提高性能。Halide 使用一个称为 parallel
    的调度原语来指定循环的并行化维度，以实现线程级并行，并通过将标记为 parallel 的循环维度映射为块和线程的注释来支持 GPU 并行。它还将大小为 $n$
    的循环替换为一个 n 宽的向量语句，该语句可以通过硬件固有映射映射到特定硬件的 SIMD 操作码。Stripe 开发了一种称为嵌套多面体模型的多面体模型变体，该模型引入了并行多面体块作为其基本的执行元素。在此扩展之后，嵌套多面体模型可以检测到切分和步进层级之间的层次并行性。此外，一些
    DL 编译器依赖于手工制作的库，例如 Glow 或硬件供应商提供的优化数学库（详见第 [4.4.3](#S4.SS4.SSS3 "4.4.3\. 优化的内核库
    ‣ 4.4\. 后端优化 ‣ 4\. DL 编译器的关键组件 ‣ 深度学习编译器：全面调查") 节）。与此同时，Glow 将向量化任务转交给 LLVM，因为当提供张量维度和循环迭代次数的信息时，LLVM
    的自动向量化器表现良好。然而，完全通过编译器后端利用并行性允许应用更多的 DL 模型特定知识，从而在付出更多工程努力的代价下实现更高的性能。
- en: 4.4.2\. Auto-tuning
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 自动调整
- en: Due to the enormous search space for parameter tuning in hardware-specific optimizations,
    it is necessary to leverage auto-tuning to determine the optimal parameter configurations.
    Among the studied DL compilers in this survey, TVM, TC, and XLA support the auto-tuning.
    Generally, the auto-tuning implementation includes four key components, such as
    parameterization, cost model, searching technique, and acceleration. .
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在硬件特定优化中的参数调整具有巨大的搜索空间，因此需要利用自动调整来确定最佳参数配置。在本调查中研究的 DL 编译器中，TVM、TC 和 XLA 支持自动调整。通常，自动调整的实现包括四个关键组件，如参数化、成本模型、搜索技术和加速。
- en: 'Parameterization - 1) Data and target: The data parameter describes the specification
    of the data, such as input shapes. The target parameter describes hardware-specific
    characteristics and constraints to be considered during optimization scheduling
    and code generation. For example, for the GPU target, the hardware parameters
    such as shared memory and register size need to be specified. 2) Optimization
    options: The optimization options include the optimization scheduling and corresponding
    parameters, such as loop oriented optimizations and tile size. In TVM, both pre-defined
    and user-defined scheduling, as well as parameters, are taken into consideration.
    Whereas, TC and XLA prefer to parameterize the optimizations, which have a strong
    correlation with performance and can be changed later at a low cost. For example,
    the minibatch dimension is one of the parameters that is usually mapped to grid
    dimensions in CUDA and can be optimized during auto-tuning.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化 - 1) 数据和目标：数据参数描述了数据的规范，例如输入形状。目标参数描述了在优化调度和代码生成过程中需要考虑的硬件特性和约束。例如，对于GPU目标，需要指定如共享内存和寄存器大小等硬件参数。2)
    优化选项：优化选项包括优化调度和相应的参数，例如以循环为导向的优化和块大小。在TVM中，会考虑预定义和用户定义的调度以及参数。而TC和XLA则倾向于参数化优化，这与性能有强相关性，并且可以在后期以低成本进行更改。例如，minibatch维度是一个通常映射到CUDA网格维度的参数，并且可以在自动调优期间优化。
- en: 'Cost model - The comparison of different cost models applied in auto-tuning
    are as follows. 1) Black-box model: This model only considers the final execution
    time rather than the characteristics of the compilation task. It is easy to build
    a black-box model, but easily ends up with higher overhead and less optimal solution
    without the guidance of task characteristics. TC adopts this model. 2) ML-based
    cost model: ML-based cost model is a statistical approach to predict performance
    using a machine learning method. It enables the model to update as the new configuration
    is explored, which helps achieve higher prediction accuracy. TVM and XLA adopt
    this kind of model, for example, gradient tree boosting model (GBDT) and feedforward
    neural network (Kaufman et al., [2019](#bib.bib48)) (FNN) respectively. 3) Pre-defined
    cost model: An approach based on a pre-defined cost model expects a perfect model
    built on the characteristics of the compilation task and able to evaluate the
    overall performance of the task. Compared to the ML-based model, the pre-defined
    model generates less computation overhead when applied, but requires large engineering
    efforts for re-building the model on each new DL model and hardware.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 成本模型 - 不同成本模型在自动调优中的比较如下。1) 黑箱模型：该模型只考虑最终的执行时间，而不是编译任务的特征。黑箱模型容易构建，但没有任务特征的指导，可能会导致较高的开销和较差的优化解决方案。TC采用了这种模型。2)
    基于机器学习的成本模型：基于机器学习的成本模型是一种统计方法，用于使用机器学习方法预测性能。它使得模型能够在探索新配置时进行更新，从而帮助实现更高的预测准确性。TVM和XLA分别采用了这种模型，例如，梯度树提升模型（GBDT）和前馈神经网络（Kaufman等，[2019](#bib.bib48)）(FNN)。3)
    预定义成本模型：基于预定义成本模型的方法期望构建一个完美的模型，基于编译任务的特征，并能够评估任务的整体性能。与基于机器学习的模型相比，预定义模型在应用时产生的计算开销较少，但需要为每个新的深度学习模型和硬件重新构建模型，涉及较大的工程努力。
- en: 'Searching technique - 1) Initialization and searching space determination:
    The initial option can either be set randomly or based on the known configurations,
    such as configurations given by users or historical optimal configurations. In
    terms of searching space, it should be specified before auto-tuning. TVM allows
    developers to specify the searching space with their domain-specific knowledge
    and provides automatic search space extraction for each hardware target based
    on the computational description. In contrast, TC relies on the compilation cache
    and the pre-defined rules. 2) Genetic algorithm (GA) (Goldberg, [1989](#bib.bib29)):
    GA considers each tuning parameter as genes and each configuration as a candidate.
    The new candidate is iteratively generated by crossover, mutation, and selection
    according to the fitness value, which is a metaheuristic inspired by the process
    of natural selection. And finally, the optimal candidate is derived. The rate
    of crossover, mutation, and selection is used for controlling the tradeoff between
    exploration and exploitation. TC adopts GA in its auto-tuning technique. 3) Simulated
    annealing algorithm (SA) (Bertsimas et al., [1993](#bib.bib13)): SA is also a
    metaheuristic inspired by annealing. It allows us to accept worse solutions in
    a decreasing probability, which can find the approximate global optimum and avoid
    the precise local optimum in a fixed amount of iterations. TVM adopts SA in its
    auto-tuning technique. 4) Reinforcement learning (RL): RL performs with learning
    to maximize reward given an environment by the tradeoff between exploration and
    exploitation. Chameleon (Ahn et al., [2020b](#bib.bib6)) (built upon TVM) adopts
    RLRL in its auto-tuning technique.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索技术 - 1) 初始化和搜索空间确定：初始选项可以随机设置或基于已知配置，例如用户提供的配置或历史最佳配置。在搜索空间方面，必须在自动调优之前指定。TVM允许开发者使用他们的领域知识指定搜索空间，并根据计算描述为每个硬件目标提供自动搜索空间提取。相比之下，TC依赖于编译缓存和预定义规则。2)
    遗传算法（GA）（Goldberg，[1989](#bib.bib29)）：GA将每个调优参数视为基因，每个配置视为候选者。新的候选者通过交叉、突变和选择根据适应度值进行迭代生成，这是一种受到自然选择过程启发的元启发式方法。最终，得出最佳候选者。交叉、突变和选择的比率用于控制探索与利用之间的权衡。TC在其自动调优技术中采用了GA。3)
    模拟退火算法（SA）（Bertsimas 等，[1993](#bib.bib13)）：SA也是一种受到退火启发的元启发式方法。它允许我们以递减的概率接受较差的解决方案，从而在固定的迭代次数内找到近似的全局最优解，并避免精确的局部最优解。TVM在其自动调优技术中采用了SA。4)
    强化学习（RL）：RL通过在探索和利用之间的权衡来学习最大化在特定环境下的奖励。Chameleon（Ahn 等，[2020b](#bib.bib6)）（建立在TVM基础上）在其自动调优技术中采用了RL。
- en: 'Acceleration - 1) Parallelization: One direction for accelerating auto-tuning
    is parallelization. TC proposes a multi-thread, multi-GPU strategy considering
    that the genetic algorithm needs to evaluate all candidates in each generation.
    First, it enqueues candidate configurations and compiles them on multiple CPU
    threads. The generated code is evaluated on GPUs in parallel, and each candidate
    owns its fitness used by the parent choosing step. After finishing the whole evaluation,
    the new candidate is generated, and the new compilation job is enqueued, waiting
    for compiling on CPU. Similarly, TVM supports cross-compilation and RPC, allowing
    users to compile on the local machine and run the programs with different auto-tuning
    configurations on multiple targets. 2) Configuration reuse: Another direction
    for accelerating auto-tuning is to reuse the previous auto-tuning configurations.
    TC stores the fastest known generated code version corresponding to the given
    configuration by compilation cache. The cache is queried before each kernel optimization
    during the compilation, and the auto-tuning is triggered if cache miss. Similarly,
    TVM produces a log file that stores the optimal configurations for all scheduling
    operators and queries the log file for best configurations during compilation.
    It is worth mentioning that TVM performs auto-tuning for each operator in Halide
    IR (e.g., conv2d), and thus the optimal configurations are determined for each
    operator separately.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 加速 - 1) 并行化：加速自动调优的一个方向是并行化。TC 提出了一个多线程、多 GPU 策略，因为遗传算法需要评估每一代中的所有候选者。首先，它将候选配置排入队列，并在多个
    CPU 线程上进行编译。生成的代码在 GPU 上并行评估，每个候选者都有其在父选择步骤中使用的适应度。在完成整个评估后，会生成新的候选者，并将新的编译任务排入队列，等待在
    CPU 上编译。同样，TVM 支持交叉编译和 RPC，允许用户在本地机器上编译，并在多个目标上运行不同的自动调优配置的程序。 2) 配置重用：加速自动调优的另一个方向是重用先前的自动调优配置。TC
    通过编译缓存存储与给定配置对应的已知最快生成代码版本。在每次内核优化之前都会查询缓存，如果缓存未命中，则触发自动调优。同样，TVM 生成一个日志文件，存储所有调度操作符的最佳配置，并在编译期间查询日志文件以获取最佳配置。值得一提的是，TVM
    对 Halide IR 中的每个操作符（例如，conv2d）执行自动调优，因此为每个操作符单独确定最佳配置。
- en: 4.4.3\. Optimized Kernel Libraries
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 优化内核库
- en: There are several highly-optimized kernel libraries widely used to accelerate
    DL training and inference on various hardware. DNNL (previously MKL-DNN) from
    Intel, cuDNN from NVIDIA, and MIOpen from AMD are widely used libraries. Both
    computation-intensive primitives (e.g., convolution, GEMM, and RNN) and memory
    bandwidth limited primitives (e.g., batch normalization, pooling, and shuffle)
    are highly optimized according to the hardware features (e.g., AVX-512 ISA, tensor
    cores). And customizable data layouts are supported to make it easy to integrate
    into DL applications and avoid frequent data layout transformations. Besides,
    low-precision training and inference, including FP32, FP16, INT8, and non-IEEE
    floating-point format bfloat16 (Kalamkar et al., [2019](#bib.bib46)) are also
    supported. Other customized DL accelerators also maintain their specific kernel
    libraries (Liu et al., [2016](#bib.bib58); Jia et al., [2019](#bib.bib44)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个高度优化的内核库被广泛用于加速各种硬件上的深度学习训练和推断。来自英特尔的 DNNL（前身为 MKL-DNN）、来自 NVIDIA 的 cuDNN
    和来自 AMD 的 MIOpen 是广泛使用的库。计算密集型原语（例如，卷积、GEMM 和 RNN）以及受内存带宽限制的原语（例如，批量归一化、池化和洗牌）都根据硬件特性（例如，AVX-512
    ISA、张量核心）进行了高度优化。并且支持可定制的数据布局，以便于集成到深度学习应用中并避免频繁的数据布局转换。此外，还支持低精度训练和推断，包括 FP32、FP16、INT8
    和非 IEEE 浮点格式 bfloat16（Kalamkar 等， [2019](#bib.bib46)）。其他定制的深度学习加速器也维护其特定的内核库（Liu
    等， [2016](#bib.bib58)；Jia 等， [2019](#bib.bib44)）。
- en: Existing DL compilers, such as TVM, nGraph, and TC, can generate the function
    calls to these libraries during code generation. However, if DL compilers need
    to leverage the existing optimized kernel libraries, they should first transform
    the data layouts and fusion styles into the types that are pre-defined in kernel
    libraries. Such transformation may break the optimal control flow. Moreover, the
    DL compilers treat the kernel libraries as a black box. Therefore they are unable
    to apply optimizations across operators (e.g., operator fusion) when invoking
    kernel libraries. In sum, using optimized kernel libraries achieves significant
    performance improvement when the computation can be satisfied by specific highly-optimized
    primitives, otherwise it may be constrained from further optimization and suffer
    from less optimal performance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的深度学习编译器，如 TVM、nGraph 和 TC，可以在代码生成期间生成对这些库的函数调用。然而，如果深度学习编译器需要利用现有的优化内核库，它们应该首先将数据布局和融合样式转换为内核库中预定义的类型。这种转换可能会破坏最佳控制流。此外，深度学习编译器将内核库视为黑箱。因此，它们在调用内核库时无法跨操作符应用优化（例如，操作符融合）。总之，当计算可以由特定的高度优化原语满足时，使用优化的内核库可以显著提高性能，否则可能会受到进一步优化的限制，性能也会有所降低。
- en: 4.4.4\. Discussion
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4\. 讨论
- en: 'The backend is responsible for bare-metal optimizations and code generation
    based on low-level IR. Although the design of backends may differ due to various
    low-level IRs, their optimizations can be classified into hardware-specific optimizations:
    auto-tuning techniques, and optimized kernel libraries. These optimizations can
    be performed separately or combined, to achieve better data locality and parallelization
    by exploiting the hardware/software characteristics. Eventually, the high-level
    IR of DL models is transformed into efficient code implementation on different
    hardware.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 后端负责基于低级 IR 的裸机优化和代码生成。尽管后端的设计可能因各种低级 IR 的不同而有所差异，但其优化可以归类为硬件特定的优化：自动调优技术和优化内核库。这些优化可以单独执行或结合进行，以通过利用硬件/软件特性来实现更好的数据局部性和并行化。最终，深度学习模型的高级
    IR 被转化为在不同硬件上高效的代码实现。
- en: 5\. Taxonomy of DL Compilers
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 深度学习编译器的分类
- en: 'The DL compilers studied in this survey include TVM, nGraph, Tensor Comprehension
    (TC), Glow, and XLA. We select these compilers since they are well-known, well
    maintained, and most importantly, widely used. Thus, we can find enough papers,
    documents, and discussions from both industry and academia in order to study their
    designs and implementations in-depth. Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy
    of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey") illustrates
    the taxonomy of the selected DL compilers from four perspectives, including frontend,
    backend, IR, and optimizations, which corresponds with the key components described
    in this survey.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '本次调查研究的深度学习编译器包括 TVM、nGraph、Tensor Comprehension (TC)、Glow 和 XLA。我们选择这些编译器是因为它们知名、维护良好，最重要的是广泛使用。因此，我们可以从工业界和学术界找到足够的论文、文档和讨论，以便深入研究它们的设计和实现。表 [1](#S5.T1
    "Table 1 ‣ 5\. Taxonomy of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive
    Survey") 从前端、后端、中间表示（IR）和优化四个方面展示了所选深度学习编译器的分类，这些方面与本调查中描述的关键组件相对应。'
- en: Specifically, we provide more information about the compilers to the best of
    our knowledge. We not only provide whether a compiler supports a specific feature,
    but also describe how to use this feature through its programming interface. In
    addition, we also describe the developing status of specific features and the
    reasons why specific features are not supported in particular compilers. The target
    of this taxonomy is to provide guidelines about the selection of DL compilers
    for the practitioners considering their requirements, as well as to give a thorough
    summary of the DL compilers for researchers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们提供了关于编译器的更多信息，尽可能详尽。我们不仅提供编译器是否支持某个特性，还描述了如何通过其编程接口使用该特性。此外，我们还描述了特定特性的开发状态以及为何某些编译器不支持特定特性的原因。这一分类的目标是为从业人员在选择深度学习编译器时提供指导，并为研究人员提供深度学习编译器的全面总结。
- en: 'In Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey"), we present the features of each DL compiler,
    including developer, programming language, ONNX/framework support, training support,
    and quantization support in the frontend category, and we present the compilation
    methods and supported devices in the backend category. These features are summarized
    because they strongly affect the usage of DL compilers in particular scenarios.
    Based on these features, practitioners or researchers can easily decide which
    DL compiler they would like to work upon.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [1](#S5.T1 "表 1 ‣ 5\. 深度学习编译器分类 ‣ 深度学习编译器：全面调查")中，我们展示了每个深度学习编译器的特性，包括开发者、编程语言、ONNX/框架支持、前端类别的训练支持和量化支持，以及后端类别的编译方法和支持的设备。这些特性被总结出来，因为它们会强烈影响深度学习编译器在特定场景中的使用。基于这些特性，实践者或研究人员可以轻松决定他们希望使用哪个深度学习编译器。
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of DL Compilers ‣ The Deep Learning
    Compiler: A Comprehensive Survey"), together with Figure [2](#S3.F2 "Figure 2
    ‣ 3\. Common Design Architecture of DL Compilers ‣ The Deep Learning Compiler:
    A Comprehensive Survey") can serve as a systematic summary of this survey. Through
    them, readers can identify the features each compiler supports as well as the
    key components of each compiler. More detailed information is presented in the
    following sections.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S5.T1 "表 1 ‣ 5\. 深度学习编译器分类 ‣ 深度学习编译器：全面调查")，连同图 [2](#S3.F2 "图 2 ‣ 3\.
    深度学习编译器的常见设计架构 ‣ 深度学习编译器：全面调查")，可以作为本调查的系统总结。通过这些表格，读者可以识别每个编译器支持的特性以及每个编译器的关键组件。更多详细信息将在以下章节中呈现。
- en: Table 1\. The comparison of DL compilers, including TVM, nGraph, TC, Glow, and
    XLA.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 深度学习编译器的比较，包括 TVM、nGraph、TC、Glow 和 XLA。
- en: '|  |  | TVM | nGraph | TC | Glow | XLA |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  |  | TVM | nGraph | TC | Glow | XLA |'
- en: '|  | Developer | Apache | Intel | Facebook | Facebook | Google |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 开发者 | Apache | Intel | Facebook | Facebook | Google |'
- en: '| Frontend | Programm- ing | Python/C++ Lambda expression | Python/C++ Tensor
    expression | Python/C++ Einstein notation | Python/C++ Layer programming | Python/C++
    Tensorflow interface |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 前端 | 编程 | Python/C++ Lambda 表达式 | Python/C++ 张量表达式 | Python/C++ 爱因斯坦记号 |
    Python/C++ 层编程 | Python/C++ Tensorflow 接口 |'
- en: '| ONNX support | ✓ tvm.relay.frontend .from_onnx (built-in) | ✓ Use ngraph-onnx
    (Python package) | $\times$ | ✓ ONNXModelLoader (built-in) | ✓ Use tensorflow-onnx
    (Python package) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| ONNX 支持 | ✓ tvm.relay.frontend.from_onnx（内置） | ✓ 使用 ngraph-onnx（Python 包）
    | $\times$ | ✓ ONNXModelLoader（内置） | ✓ 使用 tensorflow-onnx（Python 包） |'
- en: '| Framework support | tvm.relay.frontend .from_* (built-in) tensorflow/tflite/keras
    pytorch/caffe2 mxnet/coreml/darknet | tensorflow paddlepaddle (Use *-bridge, act
    as the backend) | (Define and optimize a TC kernel, which is finally called by
    other frameworks.) pytorch/other DLPack supported frameworks | pytorch/caffe2
    tensorflowlite (Use built-in ONNXIFI interface) | Use tensorflow interface |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 框架支持 | tvm.relay.frontend.from_*（内置）tensorflow/tflite/keras pytorch/caffe2
    mxnet/coreml/darknet | tensorflow paddlepaddle（使用 *-bridge，作为后端） |（定义和优化一个 TC
    内核，最终由其他框架调用。）pytorch/其他 DLPack 支持的框架 | pytorch/caffe2 tensorflowlite（使用内置 ONNXIFI
    接口） | 使用 tensorflow 接口 |'
- en: '| Training support | $\times$ Under developing (Support derivative operators
    now) | ✓ Only on NNP-T processor | ✓ (Support auto differentiation) | ✓ (Limited
    support) | ✓ Use tensorflow interface |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 训练支持 | $\times$ 正在开发中（现在支持导数运算符） | ✓ 仅在 NNP-T 处理器上 | ✓（支持自动微分） | ✓（有限支持）
    | ✓ 使用 tensorflow 接口 |'
- en: '| Quantization support | ✓ int8/fp16 | ✓ int8 (include training) | $\times$
    | ✓ int8 | ✓ int8/int16 (Use tensorflow interface) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 量化支持 | ✓ int8/fp16 | ✓ int8（包括训练） | $\times$ | ✓ int8 | ✓ int8/int16（使用 tensorflow
    接口） |'
- en: '| IR | High-/low- level IR | Relay/Halide | nGraph IR/None | TC IR/Polyhedral
    | Its own high-/low- level IR | HLO (Both high- and low- level) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| IR | 高/低级 IR | Relay/Halide | nGraph IR/None | TC IR/Polyhedral | 自有的高/低级
    IR | HLO（高级和低级） |'
- en: '| Dynamic shape | ✓ (Any) | ✓ (PartialShape) | $\times$ | $\times$ | ✓ (None)
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 动态形状 | ✓（任意） | ✓（PartialShape） | $\times$ | $\times$ | ✓（无） |'
- en: '| Optimization | Frontend opt | Hardware independent optimizations (refer to
    Section [4.3](#S4.SS3 "4.3\. Frontend Optimizations ‣ 4\. Key Components of DL
    Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey")) Hardware specific
    optimizations (refer to Section [4.4](#S4.SS4 "4.4\. Backend Optimizations ‣ 4\.
    Key Components of DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey"))
    And hybrid optimizations |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 优化 | 前端优化 | 硬件无关的优化（参见第 [4.3](#S4.SS3 "4.3\. 前端优化 ‣ 4\. 深度学习编译器的关键组件 ‣ 深度学习编译器：全面调查")
    节） 硬件特定的优化（参见第 [4.4](#S4.SS4 "4.4\. 后端优化 ‣ 4\. 深度学习编译器的关键组件 ‣ 深度学习编译器：全面调查") 节）
    及混合优化 |'
- en: '| Backend opt |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 后端优化 |'
- en: '| Autotuning | ✓ (To select the best schedule parameters) | $\times$ (Call
    optimized kernel libraries, no need) | ✓ (To reduce JIT overhead) | $\times$ (Additional
    info is already provided in IR) | ✓ (On default convolution and gemm ) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 自动调优 | ✓（选择最佳调度参数） | $\times$（调用优化的内核库，无需） | ✓（减少 JIT 开销） | $\times$（IR 中已提供额外信息）
    | ✓（默认卷积和 gemm） |'
- en: '| Kernel libraries | ✓ mkl/cudnn/cublas | ✓ eigen/mkldnn/cudnn/ Others | $\times$
    | $\times$ | ✓ eigen/mkl/ cudnn/tensorrt |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 内核库 | ✓ mkl/cudnn/cublas | ✓ eigen/mkldnn/cudnn/ 其他 | $\times$ | $\times$
    | ✓ eigen/mkl/ cudnn/tensorrt |'
- en: '| Backend | Compilation methods | JIT AOT (experimental) | JIT | JIT | JIT
    AOT (Use built-in executable bundles) | JIT AOT (Generate executable libraries)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 后端 | 编译方法 | JIT AOT（实验性） | JIT | JIT | JIT AOT（使用内置的可执行包） | JIT AOT（生成可执行库）
    |'
- en: '| Supported devices | CPU/GPU/ARM FPGA/Customized ( Use VTA) | CPU/Intel GPU/NNP
    GPU/Customized ( Use OpenCL support in PlaidML) | Nvidia GPU | CPU/GPU Customized
    ( Official docs) | CPU/GPU/TPU Customized ( Official docs) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 支持的设备 | CPU/GPU/ARM FPGA/定制（使用 VTA） | CPU/Intel GPU/NNP GPU/定制（使用 PlaidML
    中的 OpenCL 支持） | Nvidia GPU | CPU/GPU 定制（官方文档） | CPU/GPU/TPU 定制（官方文档） |'
- en: 6\. Evaluation
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 评估
- en: 6.1\. Experimental Setup
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 实验设置
- en: 'Our experiments are conducted on two GPU-equipped machines, and the hardware
    configuration is shown in Table [2](#S6.T2 "Table 2 ‣ 6.1\. Experimental Setup
    ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"). We evaluate
    the performance of TVM (v0.6.0), nGraph (0.29.0-rc.0), TC (commit fd01443), Glow
    (commit 7e68188) and XLA (TensorFlow 2.2.0) on CPU and GPU. We select 19 neural
    network models in ONNX format as our datasets, which are converted from the Torchvison²²2[https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)
    model zoo and the GluonCV³³3[https://gluon-cv.mxnet.io/model_zoo/index.html](https://gluon-cv.mxnet.io/model_zoo/index.html)
    model zoo. These models include full-fledged models: ResNet, DenseNet and VGG
    series, and lightweight models: MobileNet and MNASNet series. To import the ONNX
    models, as shown in Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of DL Compilers ‣
    The Deep Learning Compiler: A Comprehensive Survey"), we use the built-in tvm.relay.frontend.from_onnx
    interface of TVM, the ngraph-onnx Python package of nGraph, the built-in ONNXModelLoader
    of Glow, and the tensorflow-onnx Python package of XLA. Notably, TC lacks the
    support of ONNX, so we only evaluate it in the following per-layer performance
    comparison. Each model is executed for 15 times, and we report the average execution
    time of the last 10 executions for each compiler, because we regard the first
    5 executions as the warm-up to eliminate the overhead of JIT compilation.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验在两台配备 GPU 的机器上进行，硬件配置如表 [2](#S6.T2 "表 2 ‣ 6.1\. 实验设置 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查")
    所示。我们评估了 TVM（v0.6.0）、nGraph（0.29.0-rc.0）、TC（提交 fd01443）、Glow（提交 7e68188）和 XLA（TensorFlow
    2.2.0）在 CPU 和 GPU 上的性能。我们选择了 19 个 ONNX 格式的神经网络模型作为数据集，这些模型是从 Torchvision²²2[https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)
    模型库和 GluonCV³³3[https://gluon-cv.mxnet.io/model_zoo/index.html](https://gluon-cv.mxnet.io/model_zoo/index.html)
    模型库中转换而来的。这些模型包括全面的模型：ResNet、DenseNet 和 VGG 系列，以及轻量级模型：MobileNet 和 MNASNet 系列。为了导入
    ONNX 模型，如表 [1](#S5.T1 "表 1 ‣ 5\. 深度学习编译器的分类 ‣ 深度学习编译器：全面调查") 所示，我们使用了 TVM 的内置
    tvm.relay.frontend.from_onnx 接口、nGraph 的 ngraph-onnx Python 包、Glow 的内置 ONNXModelLoader
    和 XLA 的 tensorflow-onnx Python 包。值得注意的是，TC 不支持 ONNX，因此我们仅在以下逐层性能比较中评估它。每个模型执行
    15 次，我们报告每个编译器最后 10 次执行的平均执行时间，因为我们将前 5 次执行视为热身，以消除 JIT 编译的开销。
- en: Table 2\. The hardware configuration.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 硬件配置。
- en: '|  | CPU | GPU |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | CPU | GPU |'
- en: '| --- | --- | --- |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Platform a |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 平台 a |'
- en: '&#124; Broadwell E5-2680v4 *2 &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Broadwell E5-2680v4 *2 &#124;'
- en: '&#124; (28 physical cores, 2.4GHz) &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （28 个物理核心，2.4GHz） &#124;'
- en: '|'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Tesla V100 32GB &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Tesla V100 32GB &#124;'
- en: '&#124; (15.7TFlops, FP32) &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (15.7TFlops, FP32) &#124;'
- en: '|'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Platform b |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 平台 b |'
- en: '&#124; Skylake Silver 4110 *2 &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Skylake Silver 4110 *2 &#124;'
- en: '&#124; (16 physical cores, 2.1GHz) &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (16 个物理核心，2.1GHz) &#124;'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Turing RTX2080Ti 11GB &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Turing RTX2080Ti 11GB &#124;'
- en: '&#124; (13.4TFlops, FP32) &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (13.4TFlops, FP32) &#124;'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 6.2\. End-to-end Performance Comparison
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 端到端性能比较
- en: '![Refer to caption](img/a13ec83135e07c6fc575af1f9c49b860.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a13ec83135e07c6fc575af1f9c49b860.png)'
- en: Figure 5\. The performance comparison of end-to-end inference across TVM, nGraph,
    Glow and XLA on CPU and GPU.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. TVM、nGraph、Glow 和 XLA 在 CPU 和 GPU 上的端到端推断性能比较。
- en: 'As shown in Figure [5](#S6.F5 "Figure 5 ‣ 6.2\. End-to-end Performance Comparison
    ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"), we compare
    the performance of end-to-end inference across TVM, nGraph, Glow, and XLA. We
    evaluate these compilers on both CPUs (Broadwell and Skylake) and GPUs (V100 and
    2080Ti). Note that, we omit the comparison of TC here. Because TC is more similar
    to a kernel library other than fully functional DL compiler, and it requires the
    users to implement all layers of a model with its Einstein notion manually, which
    leads to heavy engineering efforts for a fair comparison. Another reason is that
    TC only supports running on GPU, thus we cannot obtain its performance results
    on CPU. However, for detailed comparisons (Figure [6](#S6.F6 "Figure 6 ‣ 6.3\.
    Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler:
    A Comprehensive Survey") and [8](#S6.F8 "Figure 8 ‣ 6.3\. Per-layer Performance
    Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey")),
    we still implement several ResNet and MobileNetV2 models in TC. In sum, we compare
    and analyze the performance results from the following perspectives.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [5](#S6.F5 "图 5 ‣ 6.2\. 端到端性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查")所示，我们比较了 TVM、nGraph、Glow
    和 XLA 在端到端推断中的性能。我们在 CPU（Broadwell 和 Skylake）和 GPU（V100 和 2080Ti）上评估了这些编译器。请注意，我们在此省略了
    TC 的比较，因为 TC 更类似于一个内核库，而不是一个完全功能的深度学习编译器，并且需要用户手动用其 Einstein 概念实现模型的所有层，这导致了公平比较时需要大量的工程工作。另一个原因是
    TC 仅支持在 GPU 上运行，因此我们无法获得其在 CPU 上的性能结果。然而，对于详细的比较（图 [6](#S6.F6 "图 6 ‣ 6.3\. 每层性能比较
    ‣ 6\. 评估 ‣ 深度学习编译器：全面调查") 和 [8](#S6.F8 "图 8 ‣ 6.3\. 每层性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查")），我们仍在
    TC 中实现了几个 ResNet 和 MobileNetV2 模型。总之，我们从以下几个方面比较和分析性能结果。
- en: Compatibility - Although nGraph and XLA claims to support ONNX , there are still
    compatibility problems. 1) nGraph fails to run the DenseNet121, VGG16/19 and MNASNet0_5/1_0
    models due to tensors with dynamic shapes. Alternatively, we replace the DenseNet121,
    VGG16/19 models with the corresponding models from the ONNX model zoo⁴⁴4[https://github.com/onnx/models](https://github.com/onnx/models),
    while MNASNet0_5/1_0 models are not available. Besides, when we set PlaidML as
    the backend of nGraph on GPU, we fail to run all MobileNet models. Because PlaidML
    cannot handle the inconsistent definition of operators across different DL frameworks.
    2) XLA can run all selected models, however, the performance is quite low. Thus,
    we replace the selected ONNX models with the savedmodels from the Tensorflow Hub⁵⁵5[https://tfhub.dev/](https://tfhub.dev/),
    while the MNASNet0_5/1_0 models are not available. With models from Tensorflow
    Hub, XLA becomes two orders of magnitude faster, and the performance of XLA becomes
    competitive with other compilers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 兼容性 - 尽管 nGraph 和 XLA 声称支持 ONNX，但仍存在兼容性问题。1) 由于张量具有动态形状，nGraph 无法运行 DenseNet121、VGG16/19
    和 MNASNet0_5/1_0 模型。作为替代方案，我们用来自 ONNX 模型库的相应模型⁴⁴4[https://github.com/onnx/models](https://github.com/onnx/models)
    替换 DenseNet121、VGG16/19 模型，而 MNASNet0_5/1_0 模型则不可用。此外，当我们将 PlaidML 设置为 nGraph
    在 GPU 上的后端时，无法运行所有 MobileNet 模型，因为 PlaidML 无法处理不同深度学习框架之间操作符定义的不一致。2) XLA 可以运行所有选定模型，但性能较低。因此，我们用来自
    Tensorflow Hub 的 savedmodels⁵⁵5[https://tfhub.dev/](https://tfhub.dev/) 替换选定的
    ONNX 模型，而 MNASNet0_5/1_0 模型不可用。使用来自 Tensorflow Hub 的模型时，XLA 变得快了两个数量级，XLA 的性能与其他编译器竞争。
- en: 'Performance - From Figure [5](#S6.F5 "Figure 5 ‣ 6.2\. End-to-end Performance
    Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"),
    we have several observations about the performance illustrated as follows.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 性能 - 从图 [5](#S6.F5 "图 5 ‣ 6.2\. 端到端性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查")中，我们对性能有如下几方面的观察。
- en: 1) On CPU, the performance of Glow is worse than other compilers. This is because
    Glow does not support thread parallelism. Thus it cannot fully utilize the multi-core
    CPU. Whereas TVM, nGraph, and XLA can leverage all CPU cores.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上，Glow 的性能比其他编译器差。这是因为 Glow 不支持线程并行。因此，它无法充分利用多核 CPU。而 TVM、nGraph 和 XLA
    可以利用所有 CPU 核心。
- en: 2) XLA has the similar end-to-end inference performance for both full-fledged
    models (ResNet, DenseNet and VGG series) and lightweight models (MobileNet and
    MNASNet series). Besides, its inference performance on CPU and GPU is almost the
    same. It is known that XLA is embedded in the Tensorflow framework. Tensorflow
    contains a complicated runtime compared to TVM, nGraph, and Glow, which introduces
    non-trivial overhead to XLA. In addition, if we increase the batch size (set to
    one by default in our evaluation) and focus on the throughput of DL compilers,
    then the overhead of XLA can be ignored with higher throughput.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: XLA 对于完整模型（ResNet、DenseNet 和 VGG 系列）和轻量级模型（MobileNet 和 MNASNet 系列）具有类似的端到端推理性能。此外，它在
    CPU 和 GPU 上的推理性能几乎相同。已知 XLA 嵌入在 Tensorflow 框架中。与 TVM、nGraph 和 Glow 相比，Tensorflow
    包含一个复杂的运行时，这给 XLA 带来了不容忽视的开销。此外，如果我们增加批量大小（在我们的评估中默认设置为一），并关注深度学习编译器的吞吐量，则 XLA
    的开销在更高吞吐量下可以忽略。
- en: 3) In general, on CPU, TVM and nGraph achieve better performance across all
    models than other DL compilers, due to the limitations of Glow and XLA described
    above. TVM has comparable performance with nGraph on full-fledged models, while
    it is better than nGraph on lightweight models. nGraph relies on the DNNL (previously
    MKL-DNN) library for acceleration. Thus, nGraph can offload the optimized subgraphs
    to DNNL and benefit from DNNL’s fine-grained instruction-level JIT optimizations
    tailored for Intel CPU.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，在 CPU 上，由于上述 Glow 和 XLA 的限制，TVM 和 nGraph 在所有模型上都比其他深度学习编译器表现更好。TVM 在完整模型上的性能与
    nGraph 相当，而在轻量级模型上则优于 nGraph。nGraph 依赖于 DNNL（之前是 MKL-DNN）库进行加速。因此，nGraph 可以将优化的子图卸载到
    DNNL，并受益于 DNNL 针对 Intel CPU 的精细化指令级 JIT 优化。
- en: 4) The tuned TVM (tuned with 200 trials) almost achieves the best performance
    on both CPU and GPU across all models, especially on lightweight models (MobileNet,
    MNASNet series). Based on our investigation, this is because the schedules of
    classic operators inside these models have already been well designed by TVM developers,
    with the default parameters provided in TVM tophub. The default schedules and
    parameters can help TVM to achieve similar performance compared to other DL compilers.
    In addition, the performance difference between the tuned TVM and untuned TVM
    is negligible on CPU but quite significant on GPU (41.26$\times$ speedup on average).
    This is because the GPU has more complicated thread and memory hierarchy than
    CPU, thus to exploit the computation power, GPU requires more fine-grained scheduling
    (e.g., tile, split, and reorder in TVM). Therefore, it is crucial to determine
    the optimal scheduling parameters on GPU, where the autotuning exhibits its effectiveness.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 调优后的 TVM（经过 200 次试验调优）在所有模型中几乎在 CPU 和 GPU 上都实现了最佳性能，特别是在轻量级模型（MobileNet、MNASNet
    系列）上。根据我们的调查，这是因为这些模型中的经典操作的调度已经由 TVM 开发者进行了良好的设计，默认参数也由 TVM tophub 提供。默认的调度和参数可以帮助
    TVM 实现与其他深度学习编译器类似的性能。此外，调优后的 TVM 与未调优的 TVM 在 CPU 上的性能差异微乎其微，但在 GPU 上差异相当显著（平均加速比为
    41.26$\times$）。这是因为 GPU 的线程和内存层次结构比 CPU 更复杂，因此要发挥计算能力，GPU 需要更精细的调度（例如，在 TVM 中的
    tile、split 和 reorder）。因此，确定 GPU 上的最佳调度参数至关重要，这也是自动调优展示其有效性的地方。
- en: 6.3\. Per-layer Performance Comparison
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 每层性能比较
- en: To further compare the capability of backend optimizations of DL compilers,
    we evaluate the per-layer (convolution layers since they dominate the inference
    time) performance of the ResNet50 and MobileNetV2$\_$1.0 on V100 GPU and Broadwell
    CPU (single-threaded since Glow lacks multi-threading support).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步比较深度学习编译器的后端优化能力，我们评估了在 V100 GPU 和 Broadwell CPU（单线程，因为 Glow 缺乏多线程支持）上的
    ResNet50 和 MobileNetV2$\_$1.0 的每层（卷积层，因为它们主导了推理时间）性能。
- en: '![Refer to caption](img/b79c412dd420b4baa0c6baeef9408c52.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b79c412dd420b4baa0c6baeef9408c52.png)'
- en: Figure 6\. The performance comparison of convolution layers in MobileNetV2$\_$1.0
    across TVM, TC, Glow and XLA on V100 GPU.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 在 V100 GPU 上，TVM、TC、Glow 和 XLA 在 MobileNetV2$\_$1.0 中的卷积层性能比较
- en: '![Refer to caption](img/6205b3a72615e3eafcc799e8fac7de42.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6205b3a72615e3eafcc799e8fac7de42.png)'
- en: Figure 7\. The performance comparison of convolution layers in MobileNetV2$\_$1.0
    across TVM, nGraph and Glow on Broadwell CPU.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. MobileNetV2$\_$1.0 中卷积层在 TVM、nGraph 和 Glow 上的性能比较，测试平台为 Broadwell CPU。
- en: '![Refer to caption](img/21fcf2de0bcd9e4217039daefc9d6b68.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/21fcf2de0bcd9e4217039daefc9d6b68.png)'
- en: Figure 8\. The performance comparison of convolution layers in ResNet50 across
    TVM, TC and Glow on V100 GPU.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. ResNet50 中卷积层在 TVM、TC 和 Glow 上的性能比较，测试平台为 V100 GPU。
- en: '![Refer to caption](img/c39ac7ebb44cd0048568b4261234d4af.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c39ac7ebb44cd0048568b4261234d4af.png)'
- en: Figure 9\. The performance comparison of convolution layers in ResNet50 across
    TVM, nGraph and Glow on Broadwell CPU.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. ResNet50 中卷积层在 TVM、nGraph 和 Glow 上的性能比较，测试平台为 Broadwell CPU。
- en: Methodology - To measure the execution time of individual layers, we adopt different
    methods considering the DL compilers, the hardware (CPU/GPU), and the CNN models.
    Specifically, 1) On TVM, we re-use the logs of autotuning to extract the kernel
    shapes and the optimal schedule. Then we rebuild the individual convolution layers
    and use the time_evaluator for evaluation. 2) We extract the execution time through
    the tracing files of Glow. 3) And we measure the execution time of hand-written
    kernels on TC. 4) As for nGraph, we make use of the timeline to measure the execution
    time on CPU. However, the timeline is not supported by its PlaidML backend (which
    provides GPU support through OpenCL). Besides, there are no available methods
    to profile the command queues within OpenCL. Therefore, we leave the profiling
    of the per-layer performance of nGraph on GPU for future work. 4) As for XLA,
    we leverage the built-in tf.profiler.experimental method for CPU performance and
    the DLProf (NVIDIA, [2019a](#bib.bib72)) toolkit from Nvidia for GPU performance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 方法论 - 为了测量各个层的执行时间，我们采用了不同的方法，考虑了 DL 编译器、硬件（CPU/GPU）和 CNN 模型。具体而言，1) 在 TVM 上，我们重新利用自动调优的日志来提取内核形状和最佳调度。然后，我们重建了各个卷积层，并使用
    `time_evaluator` 进行评估。2) 我们通过 Glow 的跟踪文件提取执行时间。3) 我们测量了在 TC 上手写内核的执行时间。4) 对于 nGraph，我们使用时间线来测量
    CPU 上的执行时间。然而，时间线不被其 PlaidML 后端（通过 OpenCL 提供 GPU 支持）所支持。此外，没有可用的方法来分析 OpenCL 中的命令队列。因此，我们将
    nGraph 在 GPU 上每层性能的分析留待未来工作。4) 对于 XLA，我们利用内置的 `tf.profiler.experimental` 方法来评估
    CPU 性能，并使用 NVIDIA 提供的 DLProf（NVIDIA，[2019a](#bib.bib72)）工具包来评估 GPU 性能。
- en: 'Performance - From Figure [6](#S6.F6 "Figure 6 ‣ 6.3\. Per-layer Performance
    Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"), [7](#S6.F7
    "Figure 7 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep
    Learning Compiler: A Comprehensive Survey") [8](#S6.F8 "Figure 8 ‣ 6.3\. Per-layer
    Performance Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive
    Survey"), [9](#S6.F9 "Figure 9 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\.
    Evaluation ‣ The Deep Learning Compiler: A Comprehensive Survey"), we have several
    observations about the performance illustrated as follows.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 性能 - 从图 [6](#S6.F6 "图 6 ‣ 6.3\. 每层性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查")、[7](#S6.F7
    "图 7 ‣ 6.3\. 每层性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查")、[8](#S6.F8 "图 8 ‣ 6.3\. 每层性能比较 ‣
    6\. 评估 ‣ 深度学习编译器：全面调查")、[9](#S6.F9 "图 9 ‣ 6.3\. 每层性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查")
    中，我们对性能有以下几方面的观察。
- en: 1) nGraph achieves a better performance of the convolution layers on CPU, which
    benefits from the co-design of hardware (Intel CPU) and software (compiler, library,
    and runtime). Whereas, TVM performs better on GPU across these compilers. On MobileNetV2$\_$1.0,
    the performance of TVM is not stable, especially on conv1 layer. This is because
    the autotuning process is affected by other processes on the same machine, and
    thus it tends to derive the imprecise, even negative scheduling parameters.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 1) nGraph 在 CPU 上的卷积层性能更佳，这得益于硬件（Intel CPU）和软件（编译器、库和运行时）的协同设计。而 TVM 在这些编译器中在
    GPU 上表现更好。在 MobileNetV2$\_$1.0 上，TVM 的性能不稳定，特别是在 conv1 层。这是因为自动调优过程受到同一机器上其他进程的影响，因此倾向于得出不准确甚至负面的调度参数。
- en: 2) TC allows users to define a tensor computation kernel (e.g., convolution)
    by the Einstein notion without specifying the shape of input/output tensors (e.g.,
    kernel size). Then the kernel is autotuned and stored in its compilation cache
    to accelerate further autotuning and compilation. However, in our evaluation,
    we find the performance of TC heavily relies on the initially compiled kernels.
    Take MobileNetV2$\_$1.0 for example, if we initialize the autotuning with layer
    c1, then c1 can perform well. But the following c$*\_$b$*\_*$ layers become much
    slower as the layers go deeper (far away from c1 layer). To derive a consistent
    performance, we need to tune each kernel separately.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 2) TC允许用户通过爱因斯坦表示法定义张量计算内核（例如，卷积），而无需指定输入/输出张量的形状（例如，内核大小）。然后，内核会自动调优并存储在其编译缓存中，以加速进一步的自动调优和编译。然而，在我们的评估中，我们发现TC的性能严重依赖于最初编译的内核。以MobileNetV2$\_$1.0为例，如果我们从层c1开始初始化自动调优，那么c1的表现良好。但是，随着层数的增加（远离c1层），接下来的c$*\_$b$*\_*$层变得更慢。为了获得一致的性能，我们需要分别调整每个内核。
- en: 3) Glow falls behind other compilers to optimize the $1\times 1$ convolutions
    (e.g., the b$*\_$linear layers) of MobileNetV2$\_$1.0 as well as the depth-wise
    separable convolutions (e.g., c$*\_$b$*\_$2 layers) of ResNet50. It takes a longer
    time to compute these convolutions both on GPU and CPU. We notice the convolutions
    are usually fused with other layers (e.g., ReLU, BatchNorm) on Glow, which could
    be why the lower performance compared to other compilers. Moreover, on CPU, the
    convolutions at the end of MobileNetV2$\_$1.0 take a quite shorter time than convolutions
    at the beginning. According to the tracing log, we notice these convolutions are
    accelerated by the CPUConvDKKC8 optimization (Rotem et al., [2018](#bib.bib80)),
    which applies tiling, layout transformation, and vectorization to convolutions
    with specific patterns.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 3) Glow在优化$1\times 1$卷积（例如，b$*\_$linear层）和深度可分离卷积（例如，c$*\_$b$*\_$2层）方面落后于其他编译器。计算这些卷积的时间在GPU和CPU上都较长。我们注意到，Glow通常将卷积与其他层（例如，ReLU、BatchNorm）融合，这可能是其性能低于其他编译器的原因。此外，在CPU上，MobileNetV2$\_$1.0末尾的卷积所需时间明显短于开始处的卷积。根据跟踪日志，我们注意到这些卷积被CPUConvDKKC8优化（Rotem等，[2018](#bib.bib80)）加速，该优化应用了切分、布局转换和特定模式的向量化。
- en: '4) As for XLA, it can automatically compile (_XlaCompile) the eligible subgraphs
    from Tensorflow and replace the subgraphs with the resultant binaries (_XlaRun).
    In addition, the convolution layers may be clustered with other kernels, and thus
    their performance is not easy to measure individually. Therefore, we have counted
    the clustered and the non-clustered convolutions, and the data is shown in Table [3](#S6.T3
    "Table 3 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep
    Learning Compiler: A Comprehensive Survey"). Note that the MobileNetV2$\_$1.0
    model in Tensorflow is a little bit different from the ONNX model for the beginning
    and ending layers, however, the linearbottleneck layers are the same. Moreover,
    if a convolution is to be clustered, it could be measured at most twice till the
    finishing of _XlaCompile. Therefore, there are five extreme value in Figure [6](#S6.F6
    "Figure 6 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep
    Learning Compiler: A Comprehensive Survey") (corresponding with 5 clustered convolutions
    in MobileNetV2$\_$1.0). Actually, only the clustered kernels are optimized by
    XLA, while the non-clustered ones are optimized by Tensorflow. Therefore, it is
    impossible to measure the execution time of a standalone convolution layer optimized
    by XLA. Consequently, we decide not to include the performance of XLA in Figure [7](#S6.F7
    "Figure 7 ‣ 6.3\. Per-layer Performance Comparison ‣ 6\. Evaluation ‣ The Deep
    Learning Compiler: A Comprehensive Survey") - [9](#S6.F9 "Figure 9 ‣ 6.3\. Per-layer
    Performance Comparison ‣ 6\. Evaluation ‣ The Deep Learning Compiler: A Comprehensive
    Survey").'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 对于 XLA，它可以自动编译（`_XlaCompile`）Tensorflow 中的符合条件的子图，并用结果二进制文件（`_XlaRun`）替换这些子图。此外，卷积层可能会与其他内核进行聚类，因此它们的性能难以单独测量。因此，我们统计了聚类和非聚类的卷积，数据如表
    [3](#S6.T3 "表 3 ‣ 6.3\. 每层性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查") 所示。请注意，Tensorflow 中的
    MobileNetV2$\_$1.0 模型在开始和结束层与 ONNX 模型略有不同，但线性瓶颈层是相同的。此外，如果卷积要被聚类，它最多可以在完成 `_XlaCompile`
    之前测量两次。因此，图 [6](#S6.F6 "图 6 ‣ 6.3\. 每层性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查") 中有五个极值（对应
    MobileNetV2$\_$1.0 中的 5 个聚类卷积）。实际上，只有聚类内核由 XLA 优化，而非聚类的则由 Tensorflow 优化。因此，无法测量
    XLA 优化的单独卷积层的执行时间。因此，我们决定不在图 [7](#S6.F7 "图 7 ‣ 6.3\. 每层性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查")
    - [9](#S6.F9 "图 9 ‣ 6.3\. 每层性能比较 ‣ 6\. 评估 ‣ 深度学习编译器：全面调查") 中包括 XLA 的性能。
- en: Table 3\. The number of the clustered and non-clustered convolutions of XLA
    on V100 GPU and Broadwell CPU.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. XLA 在 V100 GPU 和 Broadwell CPU 上的聚类和非聚类卷积数量。
- en: '|  | MobileNetV2_1.0 | ResNet50 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | MobileNetV2_1.0 | ResNet50 |'
- en: '| --- | --- | --- |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Clustered | Non-clu- | Clustered | Non-clu- |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | 非聚类 | 聚类 | 非聚类 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| V100 | 5 | 47 | 0 | 53 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| V100 | 5 | 47 | 0 | 53 |'
- en: '| Broadwell | 17 | 35 | 53 | 0 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Broadwell | 17 | 35 | 53 | 0 |'
- en: 6.4\. Discussion
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 讨论
- en: Through the above quantitative performance comparison across DL compilers, we
    can in-depth analyze the coarse-grained end-to-end performance with both frontend
    (graph-level) and backend (operator-level) optimizations, as well as the fine-grained
    per-layer performance about the convolutions with backend optimizations. However,
    there are still open challenges to accurately measure the effectiveness of the
    optimizations adopted by different DL compilers. One particular difficulty during
    our evaluation is that the frontend and backend optimizations are usually tightly
    coupled in existing DL compilers, because 1) the frontend optimizations usually
    affect a series of operators. Thus the optimized operators as the inputs to the
    backend optimizations differ across different compilers; 2) these optimizations
    tend to be co-designed for further exploit the performance opportunities (e.g.,
    clustering in XLA and more advanced optimizations (Long et al., [2019](#bib.bib62);
    Liu et al., [2019](#bib.bib59))). Therefore, it is difficult if not impossible
    to evaluate and compare specific optimizations across DL compilers individually.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述对深度学习编译器的定量性能比较，我们可以深入分析粗粒度的端到端性能，包括前端（图级）和后端（操作符级）优化，以及关于卷积的细粒度每层性能与后端优化。然而，准确衡量不同深度学习编译器所采用优化的有效性仍然面临开放的挑战。我们评估中的一个特别困难是前端和后端优化在现有深度学习编译器中通常是紧密耦合的，因为
    1) 前端优化通常会影响一系列操作符。因此，优化后的操作符作为后端优化的输入在不同编译器中有所不同；2) 这些优化往往是共同设计的，以进一步挖掘性能机会（例如，XLA
    中的聚类以及更先进的优化 (Long et al., [2019](#bib.bib62); Liu et al., [2019](#bib.bib59))）。因此，单独评估和比较深度学习编译器中的具体优化是困难的，甚至是不可能的。
- en: To tackle this problem, we have been working on building a universal benchmarking
    framework for existing DL compilers to measure the per-layer performance. The
    fundamental idea is to extract the necessary structures and parameters of the
    target layers (we name them as model fragments), and rebuild the layers as acceptable
    inputs to a particular DL compiler, which allows the compiler to apply corresponding
    frontend and backend optimizations faithfully. We can then measure the performance
    of these optimized model fragments to understand the effectiveness of DL compilers
    at layers of interests. The benchmarking framework using model fragments is scalable
    to customized layers (e.g., fused layers) of interest. With such benchmarking
    framework available, we can derive both coarse-grained (e.g., end-to-end) and
    fine-grained (e.g., per-layer) performance metrics for each DL compiler, and thus
    compare the effectiveness of optimizations across different DL compilers at the
    level of interest. Currently, we have successfully experimented by extracting
    the target layers from the state-of-the-art CNN models, such as the bottleneck
    of ResNet50 and the linearbottleneck of MobileNetV2_1.0. Our benchmarking framework
    is still under rapid development, and we hope to make it available to the community
    soon.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这个问题，我们一直致力于构建一个通用的基准测试框架，用于现有的深度学习编译器，以衡量每一层的性能。其基本思想是提取目标层的必要结构和参数（我们称之为模型片段），并将这些层重建为特定深度学习编译器可以接受的输入，这样编译器可以忠实地应用相应的前端和后端优化。我们可以测量这些优化过的模型片段的性能，以了解深度学习编译器在感兴趣层次上的效果。使用模型片段的基准测试框架可扩展到定制化层（例如，融合层）。有了这样的基准测试框架，我们可以为每个深度学习编译器推导出粗粒度（例如，端到端）和细粒度（例如，每层）性能指标，从而在感兴趣的层次上比较不同深度学习编译器优化的有效性。目前，我们已经成功地从最先进的
    CNN 模型中提取了目标层，例如 ResNet50 的瓶颈层和 MobileNetV2_1.0 的线性瓶颈。我们的基准测试框架仍在快速开发中，我们希望尽快将其提供给社区。
- en: 7\. Conclusion and Future Directions
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 结论与未来方向
- en: 'In this survey, we present a thorough analysis of the existing DL compilers
    targeting the design principles. First, we take a deep dive into the common architecture
    adopted in the existing DL compilers including the multi-level IR, the frontend
    and the backend. We present the design philosophies and reference implementations
    of each component in detail, with the emphasis on the unique IRs and optimizations
    specific to DL compilers. We summarize the findings in this survey and highlight
    the future directions in DL compiler as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们对现有的DL编译器的设计原则进行了深入分析。首先，我们深入研究了现有DL编译器中采用的常见架构，包括多级IR、前端和后端。我们详细介绍了每个组成部分的设计理念和参考实现，重点介绍了特定于DL编译器的独特IR和优化。我们总结了本调查的研究结果，并在DL编译器的未来方向上做了重点指出：
- en: Dynamic shape and pre/post processing - Dynamic model becomes more and more
    popular in the field of DL, whose input shape or even model itself may change
    during execution. Particularly, in the area of NLP, models may accept inputs of
    various shapes, which is challenging for DL compilers since the shape of data
    is unknown until runtime. Existing DL compilers require more research efforts
    to support dynamic shape efficiently for emerging dynamic models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 动态形状和前/后处理 - 动态模型在DL领域越来越受欢迎，其输入形状甚至模型本身在执行过程中可能发生变化。特别是在NLP领域，模型可能接受各种形状的输入，这对DL编译器来说是一个挑战，因为数据的形状直到运行时才知道。现有的DL编译器需要更多的研究工作来高效支持新兴动态模型的动态形状。
- en: In addition, as future DL models become more complex, their entire control flow
    may inevitably include complicated pre/post-processing procedures. Currently,
    most DL compilers use Python as their programming language, the pre/post-processing
    could become a performance bottleneck when it is executed by the Python interpreter.
    Such potential performance bottleneck has not yet been considered by existing
    DL compilers. Supporting the entire control flow in DL compiler enables express
    and optimize the pre/post-processing along with DL models, which opens up new
    opportunities for performance acceleration in model deployment.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着未来DL模型变得更加复杂，它们的整个控制流程可能不可避免地包括复杂的前/后处理过程。目前，大多数DL编译器使用Python作为编程语言，当前/后处理在Python解释器中执行时可能成为性能瓶颈。现有的DL编译器尚未考虑到这一潜在的性能瓶颈。在DL编译器中支持整个控制流程可以在DL模型与前/后处理一起表达和优化，为模型部署中的性能加速开辟了新的机会。
- en: Advanced auto-tuning - Existing auto-tuning techniques focus on the optimization
    of individual operators. However, the combination of the local optimal does not
    lead to global optimal. For example, two adjacent operators that apply on different
    data layouts can be tuned together without introducing extra memory transformations
    in between. Besides, with the rise of edge computing, execution time is not only
    the optimization objective for DL compilers. New optimization targets should also
    be considered in the auto-tuning such as memory footprint and energy consumption.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 高级自动调优 - 现有的自动调优技术集中在优化单个操作符上。然而，局部最优的组合并不会导致全局最优。例如，对于应用在不同数据布局上的两个相邻操作符，可以在它们之间进行调优而不引入额外的内存转换。此外，随着边缘计算的兴起，执行时间不再是DL编译器的唯一优化目标。自动调优还应考虑新的优化目标，如内存占用和能量消耗。
- en: Particularly, for the ML-based auto-tuning techniques, there are several directions
    worth further exploring. First, the ML techniques can be applied in other stages
    of auto-tuning, other than the cost model. For example, in the stage of selecting
    compiler options and optimization schedules, ML techniques can be used to predict
    the possibility directly and develop algorithms to determine the final configurations.
    Second, the ML-based auto-tuning techniques can be improved based on the domain
    knowledge. For example, incorporating the feature engineering (selecting features
    to represent program) (Wang and O’Boyle, [2018](#bib.bib100)) in auto-tuning techniques
    could be a potential direction for achieving better tuning results.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于基于ML的自动调优技术，值得进一步探索以下几个方向。首先，ML技术可以应用于自动调优的其他阶段，而不仅仅是成本模型。例如，在选择编译器选项和优化调度的阶段，ML技术可以用于直接预测可能性，并开发算法来确定最终配置。其次，可以基于领域知识改进基于ML的自动调优技术。例如，将特征工程（选择用于表示程序的特征）（Wang
    and O'Boyle，[2018](#bib.bib100)）纳入自动调优技术中，可能是实现更好调优结果的潜在方向。
- en: Polyhedral model - It is a promising research direction to combine polyhedral
    model and auto-tuning techniques in the design of DL compilers for efficiency.
    On one hand, the auto-tuning can be applied to minimize the overhead of polyhedral
    JIT compilation by reusing the previous configurations. On the other hand, the
    polyhedral model can be used to perform auto-scheduling, which can reduce the
    search space of auto-tuning.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 多面体模型 - 在设计深度学习编译器时，将多面体模型与自动调优技术相结合是一个有前景的研究方向。一方面，自动调优可以通过重用先前的配置来最小化多面体 JIT
    编译的开销。另一方面，多面体模型可以用于执行自动调度，从而减少自动调优的搜索空间。
- en: Another challenge of applying polyhedral model in DL compilers is to support
    the sparse tensor. In general, the format of a sparse tensor such as CSF (Smith
    and Karypis, [2015](#bib.bib85)) expresses the loop indices with index arrays
    (e.g., $a[b[i]]$) that is no longer linear. Such indirect index addressing leads
    to non-affine subscript expressions and loop bounds, which prohibits the loop
    optimization of the polyhedral model (Vasilache et al., [2006](#bib.bib91); Chen,
    [2012](#bib.bib15)). Fortunately, the polyhedral community has made progress in
    supporting sparse tensor (Venkat et al., [2014](#bib.bib96), [2015](#bib.bib95)),
    and integrating the latest advancement of the polyhedral model can increase the
    performance opportunities for DL compilers.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 应用多面体模型于深度学习编译器的另一个挑战是支持稀疏张量。一般来说，稀疏张量的格式，如 CSF（Smith 和 Karypis, [2015](#bib.bib85)），用索引数组（例如，$a[b[i]]$）表示循环索引，这不再是线性的。这种间接的索引寻址导致非仿射下标表达式和循环边界，这妨碍了多面体模型的循环优化（Vasilache
    et al., [2006](#bib.bib91); Chen, [2012](#bib.bib15)）。幸运的是，多面体社区在支持稀疏张量方面取得了进展（Venkat
    et al., [2014](#bib.bib96), [2015](#bib.bib95)），将最新的多面体模型进展整合进来可以增加深度学习编译器的性能机会。
- en: Subgraph partitioning - DL compilers supporting subgraph partitioning can divide
    the computation graph into several subgraphs, and the subgraphs can be processed
    in different manners. The subgraph partitioning presents more research opportunities
    for DL compilers. First, it opens up the possibility to integrate graph libraries
    for optimization. Take nGraph and DNNL for example, DNNL is a DL library with
    graph optimizations leveraging vast collection of highly optimized kernels. The
    integration of DNNL with nGraph enables DNNL to speedup the execution of the subgraphs
    generated by nGraph. Secondly, it opens up the possibility of heterogeneous and
    parallel execution. Once the computation graph is partitioned into subgraphs,
    the execution of different subgraphs can be assigned to heterogeneous hardware
    targets at the same time. Take the edge device for example, its computation units
    may consist of ARM CPU, Mail GPU, DSP, and probably NPU. Generating subgraphs
    from the DL compilers that utilizes all computation units efficiently can deliver
    significant speedup of the DL tasks.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 子图划分 - 支持子图划分的深度学习编译器可以将计算图划分为几个子图，并且这些子图可以以不同的方式进行处理。子图划分为深度学习编译器提供了更多的研究机会。首先，它开启了集成图形库以进行优化的可能性。例如，nGraph
    和 DNNL，DNNL 是一个深度学习库，具有利用大量高度优化内核的图形优化功能。DNNL 与 nGraph 的集成使得 DNNL 能够加速由 nGraph
    生成的子图的执行。其次，它开启了异构和并行执行的可能性。一旦计算图被划分为子图，多个子图的执行可以同时分配到不同的硬件目标。例如，在边缘设备上，它的计算单元可能包括
    ARM CPU、Mail GPU、DSP 和可能的 NPU。从深度学习编译器生成的子图有效利用所有计算单元，可以显著加速深度学习任务。
- en: Quantization - Traditional quantization strategies applied in DL frameworks
    are based on a set of fixed schemes and datatypes with little customization for
    codes running on different hardware. Whereas, supporting quantization in DL compilers
    can leverage optimization opportunities during compilation to derive more efficient
    quantization strategies. For example, Relay (Roesch et al., [2019](#bib.bib79))
    provides a quantization rewriting flow that can automatically generate quantized
    code for various schemes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 量化 - 传统的深度学习框架中应用的量化策略基于一套固定的方案和数据类型，对在不同硬件上运行的代码几乎没有自定义。而支持深度学习编译器中的量化可以利用编译期间的优化机会来推导出更高效的量化策略。例如，Relay（Roesch
    et al., [2019](#bib.bib79)）提供了一种量化重写流程，可以自动生成各种方案的量化代码。
- en: To support quantization, there are several challenges to be solved in DL compilers.
    The first challenge is how to implement new quantized operators without heavy
    engineering efforts. The attempt from AWS points out a possible direction that
    uses the concept of dialect to implement new operators upon basic operators, so
    that the optimizations at graph level and operator level can be reused. The second
    challenge is the interaction between quantization and other optimizations during
    compilation. For example, determining the appropriate stage for quantization and
    collaborating with optimizations such as operator fusion require future research
    investigations.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持量化，DL 编译器面临几个挑战。第一个挑战是如何在没有大量工程工作的情况下实现新的量化运算符。AWS 的尝试指出了一种可能的方向，即利用方言的概念在基本运算符之上实现新运算符，以便在图级和运算符级的优化可以被重用。第二个挑战是量化与其他编译优化之间的互动。例如，确定量化的适当阶段并与运算符融合等优化协作，需要未来的研究探索。
- en: Unified optimizations - Although existing DL compilers adopt similar designs
    in both computation graph optimizations and hardware-specific optimizations, each
    compiler has its own advantages in certain aspects. There is a missing way to
    share the state-of-the-art optimizations, as well as support of emerging hardware
    targets across existing compilers. We advocate unifying the optimizations from
    existing DL compilers so that the best practices adopted in each DL compiler can
    be reused. In addition, unifying the optimizations across DL compilers can accumulate
    a strong force to impact the design of general-purpose and dedicated DL accelerators,
    and provide an environment for efficient co-design of DL compiler and hardware.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 统一优化 - 尽管现有 DL 编译器在计算图优化和硬件特定优化中采用了类似的设计，但每个编译器在某些方面都有其独特的优势。目前缺乏共享最先进优化的方法，以及对新兴硬件目标的支持。我们倡导统一现有
    DL 编译器的优化，以便每个 DL 编译器采用的最佳实践可以被重用。此外，跨 DL 编译器的优化统一可以积累强大的力量，影响通用和专用 DL 加速器的设计，并提供一个高效的
    DL 编译器和硬件协同设计的环境。
- en: Currently, Google MLIR is a promising initiative towards such direction. It
    provides the infrastructure of multi-level IRs, and contains IR specification
    and toolkit to perform transformations across IRs at each level. It also provides
    flexible dialects, so that each DL compiler can construct its customized dialects
    for both high-level and low-level IRs. Through transformation across dialects,
    optimizations of one DL compiler can be reused by another compiler. However, the
    transformation of dialects requires further research efforts to reduce the dependency
    on delicate design.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Google MLIR 是朝着这一方向迈出的有前景的举措。它提供了多级 IR 的基础设施，并包含 IR 规范和工具包，以在每个级别的 IR 之间执行转换。它还提供灵活的方言，使每个
    DL 编译器可以为高层和低层 IR 构建自定义方言。通过方言之间的转换，一个 DL 编译器的优化可以被另一个编译器重用。然而，方言的转换需要进一步的研究工作，以减少对精细设计的依赖。
- en: Differentiable programming - Differentiable programming is a programming paradigm,
    where the programs are differentiable thoroughly. Algorithms written in differentiable
    programming paradigm can be automatically differentiated, which is attractive
    for DL community. Many compiler projects have adopted differentiable programming,
    such as Myia (van Merriënboer et al., [2018](#bib.bib90)), Flux (Innes et al.,
    [2018](#bib.bib41)) and Julia (Bezanson et al., [2017](#bib.bib14)). Unfortunately,
    there is little support for differential programming in existing DL compilers.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 可微编程 - 可微编程是一种编程范式，其中程序可以完全微分。在可微编程范式中编写的算法可以被自动微分，这对 DL 社区具有吸引力。许多编译器项目已经采用了可微编程，例如
    Myia（van Merriënboer 等，[2018](#bib.bib90)），Flux（Innes 等，[2018](#bib.bib41)）和 Julia（Bezanson
    等，[2017](#bib.bib14)）。不幸的是，现有 DL 编译器对可微编程的支持很少。
- en: To support differential programming is quite challenging for existing DL compilers.
    The difficulties come from not only data structure, but also language semantic.
    For example, to realize the transformation from Julia to XLA HLO IR, one of the
    challenges (Fischer and Saba, [2018](#bib.bib25)) is that the control flow is
    different between the imperative language used by Julia and the symbolic language
    used by XLA. In order to use HLO IR efficiently, the compiler also needs to provide
    operation abstraction for Julia in order to support the particular semantic of
    XLA, such as MapReduce and broadcast. Moreover, the semantic difference of differentiation
    between Julia and XLA, also requires significant changes of compiler designs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 支持微分编程对现有深度学习编译器来说是相当具有挑战性的。困难不仅来自数据结构，还来自语言语义。例如，实现从 Julia 到 XLA HLO IR 的转换，其中一个挑战（Fischer
    和 Saba，[2018](#bib.bib25)）是 Julia 使用的命令式语言与 XLA 使用的符号语言之间的控制流不同。为了高效使用 HLO IR，编译器还需要为
    Julia 提供操作抽象，以支持 XLA 特定的语义，如 MapReduce 和广播。此外，Julia 和 XLA 之间的微分语义差异，也需要对编译器设计进行重大修改。
- en: Privacy protection - In edge-cloud system, the DL models are usually split into
    two halves with each partial model running on the edge device and cloud service
    respectively, which can provide better response latency and consume less communication
    bandwidth. However, one of the drawbacks with the edge-cloud system is that the
    user privacy becomes vulnerable. The reason is that the attackers can intercept
    the intermediate results sent from the edge devices to cloud, and then use the
    intermediate results to train another model that can reveal the privacy information
    deviated from the original user task.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护 - 在边缘-云系统中，深度学习模型通常被分成两半，每个部分模型分别在边缘设备和云服务上运行，这可以提供更好的响应延迟并减少通信带宽的消耗。然而，边缘-云系统的一个缺点是用户隐私变得脆弱。原因在于攻击者可以拦截从边缘设备发送到云的中间结果，然后使用这些中间结果训练另一个模型，该模型可以揭示与原始用户任务不同的隐私信息。
- en: To protect privacy in edge-cloud system, existing approaches (Mireshghallah
    et al., [2020](#bib.bib68); Osia et al., [2018](#bib.bib75); Gao et al., [2019](#bib.bib28))
    propose to add noise with special statistic properties to the intermediate results
    that can reduce the accuracy of the attacker task without severely deteriorating
    the accuracy of the user task. However, the difficulty is to determine the layer
    where the noise should be inserted, which is quite labor intensive to identify
    the optimal layer. The above difficulty presents a great opportunity for DL compilers
    to support privacy protection, because the compilers maintain rich information
    of the DL model, which can guide the noise insertion across layers automatically.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘-云系统中，为了保护隐私，现有方法（Mireshghallah 等人，[2020](#bib.bib68)；Osia 等人，[2018](#bib.bib75)；Gao
    等人，[2019](#bib.bib28)）建议在中间结果中添加具有特殊统计特性的噪声，这可以降低攻击者任务的准确性，而不会严重影响用户任务的准确性。然而，确定噪声应插入的层级是困难的，识别最佳层级非常费力。上述困难为深度学习编译器提供了支持隐私保护的重大机会，因为编译器保持了深度学习模型的丰富信息，可以自动指导跨层噪声的插入。
- en: 'Training support - In general, the model training is far less supported in
    current DL compilers. As shown in Table [1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of
    DL Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey"), nGraph only
    supports training on the Intel NNP-T accelerator, TC only supports the auto differentiation
    of a single kernel, Glow has experimental training support for limited models,
    the training support of TVM is under development, while XLA relies on the training
    support of TensorFlow. In sum, current DL compilers mainly focus on bridging the
    gap of deploying DL models onto diverse hardware efficiently, and thus they choose
    inference as their primary optimization targets. However, expanding the capability
    of DL compilers to support model training would open up a large body of research
    opportunities such as optimization of gradient operators and high-order auto differentiation.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '训练支持 - 通常，当前的深度学习编译器对模型训练的支持远远不足。如表[1](#S5.T1 "Table 1 ‣ 5\. Taxonomy of DL
    Compilers ‣ The Deep Learning Compiler: A Comprehensive Survey")所示，nGraph 仅支持在
    Intel NNP-T 加速器上进行训练，TC 仅支持单个内核的自动微分，Glow 对有限模型有实验性的训练支持，TVM 的训练支持正在开发中，而 XLA
    依赖于 TensorFlow 的训练支持。总之，当前的深度学习编译器主要集中于弥合将深度学习模型高效部署到多种硬件上的差距，因此它们选择将推理作为主要优化目标。然而，扩展深度学习编译器以支持模型训练将开辟大量研究机会，如梯度操作的优化和高阶自动微分。'
- en: Acknowledgements
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank Jun Yang from Alibaba, Yu Xing from Xilinx,
    and Byung Hoon Ahn from UCSD for their valuable comments and suggestions.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢来自阿里巴巴的 Jun Yang、来自赛灵思的 Yu Xing 以及来自 UCSD 的 Byung Hoon Ahn 对他们宝贵的意见和建议。
- en: References
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In
    *12th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation
    ($\{$OSDI$\}$ 16)*. USENIX Association, Savannah, GA, USA, 265–283.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi 等人（2016）Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,
    Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard
    等人。2016。Tensorflow：一个大规模机器学习系统。发表于 *第12届 $\{$USENIX$\}$ 操作系统设计与实现 Symposium ($\{$OSDI$\}$
    16)*。USENIX 协会，乔治亚州萨凡纳市，美国，265–283。
- en: Abdelouahab et al. (2017) Kamel Abdelouahab, Maxime Pelcat, Jocelyn Serot, Cedric
    Bourrasset, and François Berry. 2017. Tactics to directly map CNN graphs on embedded
    FPGAs. *IEEE Embedded Systems Letters* 9, 4 (2017), 113–116.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelouahab 等人（2017）Kamel Abdelouahab, Maxime Pelcat, Jocelyn Serot, Cedric
    Bourrasset 和 François Berry。2017。将 CNN 图直接映射到嵌入式 FPGA 的策略。*IEEE 嵌入式系统通讯* 9, 4
    (2017), 113–116。
- en: Abelson et al. (1998) Harold Abelson, R. Kent Dybvig, Christopher T. Haynes,
    Guillermo Juan Rozas, NI Adams, Daniel P. Friedman, E Kohlbecker, GL Steele, David H
    Bartley, Robert Halstead, et al. 1998. Revised 5 report on the algorithmic language
    Scheme. *Higher-order and symbolic computation* 11, 1 (1998), 7–105.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abelson 等人（1998）Harold Abelson, R. Kent Dybvig, Christopher T. Haynes, Guillermo
    Juan Rozas, NI Adams, Daniel P. Friedman, E Kohlbecker, GL Steele, David H Bartley,
    Robert Halstead 等人。1998。修订版 5 关于算法语言 Scheme 的报告。*高级和符号计算* 11, 1 (1998), 7–105。
- en: 'Ahn et al. (2020a) Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin, Hsin-Pai Cheng,
    Jilei Hou, and Hadi Esmaeilzadeh. 2020a. Ordering Chaos: Memory-Aware Scheduling
    of Irregularly Wired Neural Networks for Edge Devices. arXiv:cs.DC/2003.02369'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等人（2020a）Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin, Hsin-Pai Cheng, Jilei
    Hou 和 Hadi Esmaeilzadeh。2020a。混乱排序：面向边缘设备的非规则连线神经网络的内存感知调度。arXiv:cs.DC/2003.02369
- en: 'Ahn et al. (2020b) Byung Hoon Ahn, Prannoy Pilligundla, Amir Yazdanbakhsh,
    and Hadi Esmaeilzadeh. 2020b. Chameleon: Adaptive Code Optimization for Expedited
    Deep Neural Network Compilation. arXiv:cs.LG/2001.08743'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等人（2020b）Byung Hoon Ahn, Prannoy Pilligundla, Amir Yazdanbakhsh 和 Hadi Esmaeilzadeh。2020b。变色龙：加速深度神经网络编译的自适应代码优化。arXiv:cs.LG/2001.08743
- en: Aho et al. (1986) Alfred V Aho, Ravi Sethi, and Jeffrey D Ullman. 1986. Compilers,
    principles, techniques. *Addison wesley* 7, 8 (1986), 9.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aho 等人（1986）Alfred V Aho, Ravi Sethi 和 Jeffrey D Ullman。1986。编译原理与技术。*Addison
    wesley* 7, 8 (1986), 9。
- en: 'Alibaba (2019) Alibaba. 2019. Announcing Hanguang 800: Alibaba’s First AI-Inference
    Chip. [https://www.alibabacloud.com/blog/announcing-hanguang-800-alibabas-first-ai-inference-chip_595482](https://www.alibabacloud.com/blog/announcing-hanguang-800-alibabas-first-ai-inference-chip_595482).
    Accessed February 4, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿里巴巴（2019）阿里巴巴。2019。宣布 Hanguang 800：阿里巴巴首款 AI 推理芯片。 [https://www.alibabacloud.com/blog/announcing-hanguang-800-alibabas-first-ai-inference-chip_595482](https://www.alibabacloud.com/blog/announcing-hanguang-800-alibabas-first-ai-inference-chip_595482)。访问日期：2020年2月4日。
- en: Amazon (2018) Amazon. 2018. AWS Inferentia. [https://aws.amazon.com/machine-learning/inferentia](https://aws.amazon.com/machine-learning/inferentia).
    Accessed February 4, 2020.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊（2018）亚马逊。2018。AWS Inferentia。 [https://aws.amazon.com/machine-learning/inferentia](https://aws.amazon.com/machine-learning/inferentia)。访问日期：2020年2月4日。
- en: 'Bagnara et al. (2006) Roberto Bagnara, Patricia M Hill, and Enea Zaffanella.
    2006. The Parma Polyhedra Library: Toward a complete set of numerical abstractions
    for the analysis and verification of hardware and software systems.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagnara 等人（2006）Roberto Bagnara, Patricia M Hill 和 Enea Zaffanella。2006。帕尔马多面体库：向硬件和软件系统分析与验证的完整数值抽象集迈进。
- en: Bahrampour et al. (2015) Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott,
    and Mohak Shah. 2015. Comparative Study of Deep Learning Software Frameworks.
    arXiv:cs.LG/1511.06435
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahrampour 等人（2015）Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott 和 Mohak
    Shah。2015。深度学习软件框架的比较研究。arXiv:cs.LG/1511.06435
- en: Baidu (2016) Baidu. 2016. PaddlePaddle Github repository. [https://github.com/PaddlePaddle/Paddle](https://github.com/PaddlePaddle/Paddle).
    Accessed February 4, 2020.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 百度（2016）百度。2016。PaddlePaddle Github 仓库。 [https://github.com/PaddlePaddle/Paddle](https://github.com/PaddlePaddle/Paddle)。访问日期：2020年2月4日。
- en: Bertsimas et al. (1993) Dimitris Bertsimas, John Tsitsiklis, et al. 1993. Simulated
    annealing. *Statistical science* 8, 1 (1993), 10–15.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertsimas 等人（1993）Dimitris Bertsimas, John Tsitsiklis 等人。1993。模拟退火。*统计科学* 8,
    1 (1993), 10–15。
- en: 'Bezanson et al. (2017) Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B
    Shah. 2017. Julia: A fresh approach to numerical computing. *SIAM review* 59,
    1 (2017), 65–98.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bezanson 等 (2017) Jeff Bezanson, Alan Edelman, Stefan Karpinski 和 Viral B Shah.
    2017. Julia：一种新的数值计算方法。*SIAM review* 59, 1 (2017), 65–98。
- en: Chen (2012) Chun Chen. 2012. Polyhedra scanning revisited. In *Proceedings of
    the 33rd ACM SIGPLAN conference on Programming Language Design and Implementation*.
    ACM, Beijing, China, 499–508.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen (2012) Chun Chen. 2012. 多面体扫描的重新审视。在 *第33届 ACM SIGPLAN 编程语言设计与实现会议论文集*。ACM，北京，中国，499–508。
- en: Chen et al. (2018a) Hongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona,
    and Thomas Blaschke. 2018a. The rise of deep learning in drug discovery. *Drug
    discovery today* 23, 6 (2018), 1241–1250.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2018a) Hongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona 和
    Thomas Blaschke. 2018a. 深度学习在药物发现中的崛起。*Drug discovery today* 23, 6 (2018), 1241–1250。
- en: 'Chen et al. (2015) Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie
    Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexible
    and efficient machine learning library for heterogeneous distributed systems.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2015) Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang,
    Tianjun Xiao, Bing Xu, Chiyuan Zhang 和 Zheng Zhang. 2015. Mxnet：一个灵活高效的异构分布式系统的机器学习库。
- en: 'Chen et al. (2018b) Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng,
    Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al.
    2018b. $\{$TVM$\}$: An automated end-to-end optimizing compiler for deep learning.
    In *13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation
    ($\{$OSDI$\}$ 18)*. USENIX Association, Carlsbad, CA, USA, 578–594.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2018b) Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie
    Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze 等。2018b. $\{$TVM$\}$：一个自动化的端到端优化编译器，用于深度学习。在
    *第13届 $\{$USENIX$\}$ 操作系统设计与实现研讨会 ($\{$OSDI$\}$ 18)*。USENIX 协会，加州卡尔斯巴德，美国，578–594。
- en: 'Chetlur et al. (2014) Sharan Chetlur, Cliff Woolley, Philippe Vandermersch,
    Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: Efficient
    Primitives for Deep Learning. arXiv:cs.NE/1410.0759'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chetlur 等 (2014) Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan
    Cohen, John Tran, Bryan Catanzaro 和 Evan Shelhamer. 2014. cuDNN：深度学习的高效原语。arXiv:cs.NE/1410.0759
- en: Chollet et al. (2015) François Chollet et al. 2015. Keras. [https://keras.io](https://keras.io).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet 等 (2015) François Chollet 等。2015. Keras. [https://keras.io](https://keras.io)。
- en: 'Collobert et al. (2011) R. Collobert, K. Kavukcuoglu, and C. Farabet. 2011.
    Torch7: A Matlab-like Environment for Machine Learning. In *BigLearn, NIPS Workshop*.
    Curran Associates, Granada, Spain, 6.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collobert 等 (2011) R. Collobert, K. Kavukcuoglu 和 C. Farabet. 2011. Torch7：一个类似
    Matlab 的机器学习环境。在 *BigLearn, NIPS Workshop*。Curran Associates，西班牙格拉纳达，6。
- en: 'Cyphers et al. (2018) Scott Cyphers, Arjun K Bansal, Anahita Bhiwandiwalla,
    Jayaram Bobba, Matthew Brookhart, Avijit Chakraborty, Will Constable, Christian
    Convey, Leona Cook, Omar Kanawi, et al. 2018. Intel ngraph: An intermediate representation,
    compiler, and executor for deep learning.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cyphers 等 (2018) Scott Cyphers, Arjun K Bansal, Anahita Bhiwandiwalla, Jayaram
    Bobba, Matthew Brookhart, Avijit Chakraborty, Will Constable, Christian Convey,
    Leona Cook, Omar Kanawi 等。2018. Intel ngraph：一个用于深度学习的中间表示、编译器和执行器。
- en: Cytron et al. (1991) Ron Cytron, Jeanne Ferrante, Barry K Rosen, Mark N Wegman,
    and F Kenneth Zadeck. 1991. Efficiently computing static single assignment form
    and the control dependence graph. *ACM Transactions on Programming Languages and
    Systems (TOPLAS)* 13, 4 (1991), 451–490.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cytron 等 (1991) Ron Cytron, Jeanne Ferrante, Barry K Rosen, Mark N Wegman 和
    F Kenneth Zadeck. 1991. 高效计算静态单赋值形式和控制依赖图。*ACM Transactions on Programming Languages
    and Systems (TOPLAS)* 13, 4 (1991), 451–490。
- en: Feautrier (1988) P. Feautrier. 1988. Parametric integer programming. *RAIRO
    Recherche Opérationnelle* 22, 3 (1988), 243–268.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feautrier (1988) P. Feautrier. 1988. 参数化整数编程。*RAIRO Recherche Opérationnelle*
    22, 3 (1988), 243–268。
- en: Fischer and Saba (2018) Keno Fischer and Elliot Saba. 2018. Automatic full compilation
    of julia programs and ML models to cloud TPUs.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischer 和 Saba (2018) Keno Fischer 和 Elliot Saba. 2018. 自动化将 Julia 程序和 ML 模型完全编译到云
    TPU。
- en: Fonnegra et al. (2017) Rubén D Fonnegra, Bryan Blair, and Gloria M Díaz. 2017.
    Performance comparison of deep learning frameworks in image classification problems
    using convolutional and recurrent networks. In *2017 IEEE Colombian Conference
    on Communications and Computing (COLCOM)*. IEEE, Cartagena, Colombia, 1–6.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fonnegra 等 (2017) Rubén D Fonnegra, Bryan Blair 和 Gloria M Díaz. 2017. 深度学习框架在图像分类问题中的性能比较，使用卷积和递归网络。在
    *2017 IEEE Colombian Conference on Communications and Computing (COLCOM)*。IEEE，哥伦比亚
    Cartagena，1–6。
- en: 'Forsyth and Ponce (2002) David A Forsyth and Jean Ponce. 2002. *Computer vision:
    a modern approach*. Prentice Hall Professional Technical Reference, Upper Saddle
    River, NJ, USA.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Forsyth and Ponce (2002) David A Forsyth 和 Jean Ponce. 2002. *计算机视觉：现代方法*。Prentice
    Hall Professional Technical Reference, Upper Saddle River, NJ, USA.
- en: 'Gao et al. (2019) Ruiyuan Gao, Ming Dun, Hailong Yang, Zhongzhi Luan, and Depei
    Qian. 2019. Privacy for Rescue: A New Testimony Why Privacy is Vulnerable In Deep
    Models.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2019) Ruiyuan Gao, Ming Dun, Hailong Yang, Zhongzhi Luan, 和 Depei
    Qian. 2019. 救援隐私：深度模型中隐私为何脆弱的新证据。
- en: Goldberg (1989) David E. Goldberg. 1989. *Genetic Algorithms in Search, Optimization
    and Machine Learning* (1st ed.). Addison-Wesley Longman Publishing Co., Inc.,
    USA.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldberg (1989) David E. Goldberg. 1989. *遗传算法在搜索、优化与机器学习中的应用*（第1版）。Addison-Wesley
    Longman Publishing Co., Inc., USA.
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
    2014. Generative Adversarial Networks. arXiv:stat.ML/1406.2661
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio.
    2014. 生成对抗网络。arXiv:stat.ML/1406.2661
- en: Goodman (2007) Danny Goodman. 2007. *JavaScript bible*. John Wiley & Sons, Hoboken,
    NJ, USA.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodman (2007) Danny Goodman. 2007. *JavaScript圣经*。John Wiley & Sons, Hoboken,
    NJ, USA.
- en: Grosser (2000) Tobias Grosser. 2000. Polyhedral Compilation. [https://polyhedral.info](https://polyhedral.info).
    Accessed February 4, 2020.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grosser (2000) Tobias Grosser. 2000. 多面体编译。 [https://polyhedral.info](https://polyhedral.info)。访问日期：2020年2月4日。
- en: 'Guan et al. (2017) Yijin Guan, Hao Liang, Ningyi Xu, Wenqiang Wang, Shaoshuai
    Shi, Xi Chen, Guangyu Sun, Wei Zhang, and Jason Cong. 2017. FP-DNN: An automated
    framework for mapping deep neural networks onto FPGAs with RTL-HLS hybrid templates.
    In *2017 IEEE 25th Annual International Symposium on Field-Programmable Custom
    Computing Machines (FCCM)*. IEEE, IEEE Computer Society, Napa, CA, USA, 152–159.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan et al. (2017) Yijin Guan, Hao Liang, Ningyi Xu, Wenqiang Wang, Shaoshuai
    Shi, Xi Chen, Guangyu Sun, Wei Zhang, 和 Jason Cong. 2017. FP-DNN：用于将深度神经网络映射到FPGA的自动化框架，采用RTL-HLS混合模板。发表于
    *2017 IEEE第25届国际现场可编程定制计算机会议（FCCM）*。IEEE, IEEE计算机学会, Napa, CA, USA, 152–159。
- en: 'Guo et al. (2017a) Kaiyuan Guo, Lingzhi Sui, Jiantao Qiu, Jincheng Yu, Junbin
    Wang, Song Yao, Song Han, Yu Wang, and Huazhong Yang. 2017a. Angel-Eye: A complete
    design flow for mapping CNN onto embedded FPGA. *IEEE Transactions on Computer-Aided
    Design of Integrated Circuits and Systems* 37, 1 (2017), 35–47.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2017a) Kaiyuan Guo, Lingzhi Sui, Jiantao Qiu, Jincheng Yu, Junbin
    Wang, Song Yao, Song Han, Yu Wang, 和 Huazhong Yang. 2017a. Angel-Eye：将CNN映射到嵌入式FPGA的完整设计流程。
    *IEEE集成电路与系统计算机辅助设计汇刊* 37, 1 (2017), 35–47。
- en: Guo et al. (2017b) Kaiyuan Guo, Shulin Zeng, Jincheng Yu, Yu Wang, and Huazhong
    Yang. 2017b. A Survey of FPGA-Based Neural Network Accelerator. arXiv:cs.AR/1712.08934
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2017b) Kaiyuan Guo, Shulin Zeng, Jincheng Yu, Yu Wang, 和 Huazhong
    Yang. 2017b. 基于FPGA的神经网络加速器调查。arXiv:cs.AR/1712.08934
- en: Guo et al. (2018) Qianyu Guo, Xiaofei Xie, Lei Ma, Qiang Hu, Ruitao Feng, Li
    Li, Yang Liu, Jianjun Zhao, and Xiaohong Li. 2018. An Orchestrated Empirical Study
    on Deep Learning Frameworks and Platforms.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2018) Qianyu Guo, Xiaofei Xie, Lei Ma, Qiang Hu, Ruitao Feng, Li
    Li, Yang Liu, Jianjun Zhao, 和 Xiaohong Li. 2018. 深度学习框架和平台的协同实证研究。
- en: Ha et al. (2016) Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. 2016. Large-scale
    item categorization in e-commerce using multiple recurrent neural networks. In
    *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*. ACM, San Francisco, CA, USA, 107–115.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ha et al. (2016) Jung-Woo Ha, Hyuna Pyo, 和 Jeonghee Kim. 2016. 使用多重递归神经网络的大规模电商商品分类。发表于
    *第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。ACM, San Francisco, CA, USA, 107–115。
- en: 'Hatcher and Yu (2018) William Grant Hatcher and Wei Yu. 2018. A survey of deep
    learning: platforms, applications and emerging research trends. *IEEE Access*
    6 (2018), 24411–24432.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hatcher and Yu (2018) William Grant Hatcher 和 Wei Yu. 2018. 深度学习调查：平台、应用和新兴研究趋势。
    *IEEE Access* 6 (2018), 24411–24432。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 1997.
    长短期记忆。 *神经计算* 9, 8 (1997), 1735–1780。
- en: Howard et al. (2018) Jeremy Howard et al. 2018. fastai. [https://github.com/fastai/fastai](https://github.com/fastai/fastai).
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard et al. (2018) Jeremy Howard 等。2018. fastai。 [https://github.com/fastai/fastai](https://github.com/fastai/fastai)。
- en: Innes et al. (2018) Michael Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi,
    Marco Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal Singh, and
    Viral Shah. 2018. Fashionable Modelling with Flux.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Innes 等 (2018) 迈克尔·英尼斯、艾略特·萨巴、基诺·费舍尔、达希利亚·甘地、马尔科·孔切托·鲁迪洛索、尼图·玛利亚·乔伊、泰詹·卡尔马利、阿维克·帕尔·辛格和维拉尔·沙阿。2018年。时尚建模与
    Flux。
- en: Intel (2019) Intel. 2019. Nervana Neural Network Processor. [https://www.intel.ai/nervana-nnp/](https://www.intel.ai/nervana-nnp/).
    Accessed February 4, 2020.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英特尔 (2019) 英特尔。2019年。Nervana 神经网络处理器。 [https://www.intel.ai/nervana-nnp/](https://www.intel.ai/nervana-nnp/)。访问日期：2020年2月4日。
- en: 'Jia et al. (2014) Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev,
    Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe:
    Convolutional architecture for fast feature embedding. In *Proceedings of the
    22nd ACM international conference on Multimedia*. ACM, ACM, Orlando, FL, USA,
    675–678.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等 (2014) 杨青·贾、埃文·谢尔哈默、杰夫·多纳休、谢尔盖·卡拉耶夫、乔纳森·朗、罗斯·吉尔希克、塞尔吉奥·瓜达拉马和特雷弗·达雷尔。2014年。Caffe：用于快速特征嵌入的卷积架构。在
    *第22届 ACM 国际多媒体会议* 上。ACM，美国佛罗里达州奥兰多，675–678。
- en: Jia et al. (2019) Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo
    Scarpazza. 2019. Dissecting the Graphcore IPU Architecture via Microbenchmarking.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等 (2019) Zhe Jia、布莱克·蒂尔曼、马尔科·马基奥尼 和 达尼埃莱·保罗·斯卡帕扎。2019年。通过微基准测试解剖 Graphcore
    IPU 架构。
- en: Jouppi et al. (2017) Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson,
    Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers,
    et al. 2017. In-datacenter performance analysis of a tensor processing unit. In
    *Proceedings of the 44th Annual International Symposium on Computer Architecture*.
    ACM, Toronto, ON, Canada, 1–12.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jouppi 等 (2017) 诺曼·P·乔普皮、克里夫·杨、尼尚特·帕蒂尔、大卫·帕特森、加乌拉夫·阿格拉瓦尔、拉敏德·巴贾瓦、萨拉·贝茨、苏雷什·巴蒂亚、南·博登、阿尔·博尔彻斯等。2017年。数据中心内张量处理单元的性能分析。在
    *第44届国际计算机体系结构年会* 上。ACM，加拿大多伦多，1–12。
- en: Kalamkar et al. (2019) Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi,
    Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj
    Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander
    Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
    Bharat Kaul, and Pradeep Dubey. 2019. A Study of BFLOAT16 for Deep Learning Training.
    arXiv:cs.LG/1905.12322
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalamkar 等 (2019) 德拉吉·卡拉姆卡尔、德赫瓦察·穆迪杰雷、纳文·梅伦普迪、迪潘卡尔·达斯、库纳尔·班纳吉、萨西坎特·阿瓦查、达尔玛·提贾·武图里、纳塔拉吉·贾马拉马达卡、简宇·黄、赫克托·袁、纪延·杨、郑秀·朴、亚历山大·海内克、埃万吉洛斯·乔尔加纳斯、苏达尔尚·斯里尼瓦桑、阿比谢克·昆杜、米莎·斯梅良斯基、巴拉特·考尔和普拉迪普·杜比。2019年。BFLOAT16
    在深度学习训练中的研究。arXiv:cs.LG/1905.12322
- en: 'Kang et al. (2018) Duseok Kang, Euiseok Kim, Inpyo Bae, Bernhard Egger, and
    Soonhoi Ha. 2018. C-GOOD: C-code generation framework for optimized on-device
    deep learning. In *Proceedings of the International Conference on Computer-Aided
    Design*. ACM, ACM, San Diego, CA, USA, 105.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等 (2018) 都锡·姜、义锡·金、仁孝·裴、伯恩哈德·埃格和顺辉·哈。2018年。C-GOOD：优化的设备端深度学习 C 代码生成框架。在
    *国际计算机辅助设计会议* 上。ACM，美国加利福尼亚州圣地亚哥，105。
- en: Kaufman et al. (2019) Samuel Kaufman, Phitchaya Mangpo Phothilimthana, and Mike
    Burrows. 2019. Learned TPU Cost Model for XLA Tensor Programs. In *Proceedings
    of the Workshop on ML for Systems at NeurIPS 2019*. Curran Associates, Vancouver,
    Canada, 1–6.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaufman 等 (2019) 塞缪尔·考夫曼、皮查雅·芒坡·波提利姆塔纳和迈克·巴罗斯。2019年。XLA 张量程序的学习 TPU 成本模型。在 *NeurIPS
    2019 机器学习系统研讨会* 上。Curran Associates，加拿大温哥华，1–6。
- en: Kelly et al. (1996) Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana
    Shpeisman, and Dave Wonnacott. 1996. The Omega calculator and library, version
    1.1\. 0. *College Park, MD* 20742 (1996), 18.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kelly 等 (1996) 韦恩·凯利、瓦迪姆·马斯洛夫、威廉·普、埃文·罗斯、塔蒂亚娜·什佩伊斯曼和戴夫·沃纳科特。1996年。Omega 计算器及库，版本
    1.1。0。 *College Park, MD* 20742 (1996)，18。
- en: Kingsley-Hughes (2017) Adrian Kingsley-Hughes. 2017. Inside Apple’s new A11
    Bionic processor.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingsley-Hughes (2017) 亚德里安·金斯利-休斯。2017年。揭秘苹果新款 A11 Bionic 处理器。
- en: Kuck et al. (1981) D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe.
    1981. Dependence Graphs and Compiler Optimizations. In *Proceedings of the 8th
    ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages* *(POPL ’81)*.
    Association for Computing Machinery, New York, NY, USA, 207–218. [https://doi.org/10.1145/567532.567555](https://doi.org/10.1145/567532.567555)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuck 等 (1981) D. J. 库克、R. H. 库恩、D. A. 帕杜亚、B. 李瑟和 M. 沃尔夫。1981年。依赖图与编译器优化。在 *第8届
    ACM SIGPLAN-SIGACT 编程语言原理研讨会* *(POPL ’81)* 上。计算机协会，美国纽约，207–218。 [https://doi.org/10.1145/567532.567555](https://doi.org/10.1145/567532.567555)
- en: 'Lattner and Adve (2004) Chris Lattner and Vikram Adve. 2004. LLVM: A compilation
    framework for lifelong program analysis & transformation. In *Proceedings of the
    international symposium on Code generation and optimization: feedback-directed
    and runtime optimization*. IEEE Computer Society, IEEE Computer Society, San Jose,
    CA, USA, 75.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lattner 和 Adve (2004) Chris Lattner 和 Vikram Adve. 2004. LLVM: 一种终身程序分析与转换的编译框架。发表于
    *国际代码生成与优化研讨会: 反馈导向与运行时优化的论文集*。IEEE 计算机学会, IEEE 计算机学会, 圣荷西, CA, 美国, 75。'
- en: 'Lattner et al. (2020) Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen,
    Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache,
    and Oleksandr Zinenko. 2020. MLIR: A Compiler Infrastructure for the End of Moore’s
    Law. arXiv:cs.PL/2002.11054'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lattner 等 (2020) Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen,
    Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache,
    和 Oleksandr Zinenko. 2020. MLIR: 摩尔定律终结的编译器基础设施。arXiv:cs.PL/2002.11054'
- en: 'Leary and Wang (2017) Chris Leary and Todd Wang. 2017. XLA: TensorFlow, compiled.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Leary 和 Wang (2017) Chris Leary 和 Todd Wang. 2017. XLA: TensorFlow, compiled.'
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (1998), 2278–2324.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, 和 Patrick Haffner. 1998.
    应用于文档识别的基于梯度的学习。*Proc. IEEE* 86, 11 (1998), 2278–2324。
- en: 'Li et al. (2020) Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong
    Yang, Zhongzhi Luan, and Depei Qian. 2020. The Deep Learning Compiler: A Comprehensive
    Survey.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2020) Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong
    Yang, Zhongzhi Luan, 和 Depei Qian. 2020. 深度学习编译器: 一项全面的调查。'
- en: 'Liao et al. (2019) Heng Liao, Jiajin Tu, Jing Xia, and Xiping Zhou. 2019. DaVinci:
    A Scalable Architecture for Neural Network Computing. In *2019 IEEE Hot Chips
    31 Symposium (HCS)*. IEEE, IEEE, Cupertino, CA, USA, 1–44.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liao 等 (2019) Heng Liao, Jiajin Tu, Jing Xia, 和 Xiping Zhou. 2019. DaVinci:
    一种可扩展的神经网络计算架构。发表于 *2019 IEEE Hot Chips 31 研讨会 (HCS)*。IEEE, IEEE, 库比蒂诺, CA, 美国,
    1–44。'
- en: 'Liu et al. (2016) S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y. Xie, Y. Chen, and
    T. Chen. 2016. Cambricon: An Instruction Set Architecture for Neural Networks.
    In *2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture
    (ISCA)*. IEEE Computer Society, Seoul, South Korea, 393–405. [https://doi.org/10.1109/ISCA.2016.42](https://doi.org/10.1109/ISCA.2016.42)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2016) S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y. Xie, Y. Chen, 和 T. Chen.
    2016. Cambricon: 神经网络的指令集架构。发表于 *2016 年 ACM/IEEE 第 43 届年度国际计算机架构研讨会 (ISCA)*。IEEE
    计算机学会, 首尔, 韩国, 393–405。 [https://doi.org/10.1109/ISCA.2016.42](https://doi.org/10.1109/ISCA.2016.42)'
- en: Liu et al. (2019) Yizhi Liu, Yao Wang, Ruofei Yu, Mu Li, Vin Sharma, and Yida
    Wang. 2019. Optimizing $\{$CNN$\}$ Model Inference on CPUs. In *2019 $\{$USENIX$\}$
    Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 19)*. USENIX Association,
    Renton, WA, USA, 1025–1040.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019) Yizhi Liu, Yao Wang, Ruofei Yu, Mu Li, Vin Sharma, 和 Yida Wang.
    2019. 在 CPU 上优化 $\{$CNN$\}$ 模型推理。发表于 *2019 年 $\{$USENIX$\}$ 年度技术会议 ($\{$USENIX$\}$$\{$ATC$\}$
    19)*。USENIX 协会, Renton, WA, 美国, 1025–1040。
- en: Liu et al. (2016) Zhiqiang Liu, Yong Dou, Jingfei Jiang, and Jinwei Xu. 2016.
    Automatic code generation of convolutional neural networks in FPGA implementation.
    In *2016 International Conference on Field-Programmable Technology (FPT)*. IEEE,
    IEEE, Xi’an, China, 61–68.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2016) Zhiqiang Liu, Yong Dou, Jingfei Jiang, 和 Jinwei Xu. 2016. FPGA
    实现中的卷积神经网络自动代码生成。发表于 *2016 年国际现场可编程技术会议 (FPT)*。IEEE, IEEE, 西安, 中国, 61–68。
- en: 'Loechner (1999) Vincent Loechner. 1999. PolyLib: A library for manipulating
    parameterized polyhedra. [https://repo.or.cz/polylib.git/blob_plain/HEAD:/doc/parampoly-doc.ps.gz](https://repo.or.cz/polylib.git/blob_plain/HEAD:/doc/parampoly-doc.ps.gz)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Loechner (1999) Vincent Loechner. 1999. PolyLib: 一种用于操作参数化多面体的库。 [https://repo.or.cz/polylib.git/blob_plain/HEAD:/doc/parampoly-doc.ps.gz](https://repo.or.cz/polylib.git/blob_plain/HEAD:/doc/parampoly-doc.ps.gz)'
- en: 'Long et al. (2019) Guoping Long, Jun Yang, and Wei Lin. 2019. FusionStitching:
    Boosting Execution Efficiency of Memory Intensive Computations for DL Workloads.
    arXiv:cs.DC/1911.11576'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Long 等 (2019) Guoping Long, Jun Yang, 和 Wei Lin. 2019. FusionStitching: 提升深度学习工作负载的内存密集计算执行效率。arXiv:cs.DC/1911.11576'
- en: 'Long et al. (2018) Guoping Long, Jun Yang, Kai Zhu, and Wei Lin. 2018. FusionStitching:
    Deep Fusion and Code Generation for Tensorflow Computations on GPUs. arXiv:cs.DC/1811.05213'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Long 等 (2018) Guoping Long, Jun Yang, Kai Zhu, 和 Wei Lin. 2018. FusionStitching:
    针对 GPU 上 Tensorflow 计算的深度融合与代码生成。arXiv:cs.DC/1811.05213'
- en: 'Ma et al. (2018) Yufei Ma, Naveen Suda, Yu Cao, Sarma Vrudhula, and Jae-sun
    Seo. 2018. ALAMO: FPGA acceleration of deep learning algorithms with a modularized
    RTL compiler. *Integration* 62 (2018), 14–23.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2018）**Yufei Ma**, **Naveen Suda**, **Yu Cao**, **Sarma Vrudhula** 和 **Jae-sun
    Seo**。2018年。《ALAMO：具有模块化 RTL 编译器的 FPGA 加速深度学习算法》。*Integration* 62卷（2018），14–23。
- en: Manning et al. (1999) Christopher D Manning, Christopher D Manning, and Hinrich
    Schütze. 1999. *Foundations of statistical natural language processing*. MIT press,
    Cambridge, MA, USA.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manning 等（1999）**Christopher D Manning**, **Christopher D Manning** 和 **Hinrich
    Schütze**。1999年。*统计自然语言处理基础*。MIT出版社，美国马萨诸塞州剑桥市。
- en: McCarthy and Levin (1965) John McCarthy and Michael I Levin. 1965. LISP 1.5
    programmer’s manual.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCarthy 和 Levin（1965）**John McCarthy** 和 **Michael I Levin**。1965年。《LISP 1.5
    程序员手册》。
- en: Microsoft (2017) Microsoft. 2017. ONNX Github repository. [https://github.com/onnx/onnx](https://github.com/onnx/onnx).
    Accessed February 4, 2020.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft（2017）**Microsoft**。2017年。《ONNX Github 仓库》。[https://github.com/onnx/onnx](https://github.com/onnx/onnx)。访问时间：2020年2月4日。
- en: 'Mireshghallah et al. (2020) Fatemehsadat Mireshghallah, Mohammadkazem Taram,
    Prakash Ramrakhyani, Ali Jalali, Dean Tullsen, and Hadi Esmaeilzadeh. 2020. Shredder:
    Learning noise distributions to protect inference privacy. In *Proceedings of
    the Twenty-Fifth International Conference on Architectural Support for Programming
    Languages and Operating Systems*. ACM, Lausanne, Switzerland, 3–18.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mireshghallah 等（2020）**Fatemehsadat Mireshghallah**, **Mohammadkazem Taram**,
    **Prakash Ramrakhyani**, **Ali Jalali**, **Dean Tullsen** 和 **Hadi Esmaeilzadeh**。2020年。《Shredder：学习噪声分布以保护推理隐私》。见于
    *第二十五届国际编程语言与操作系统支持会议论文集*。ACM，瑞士洛桑，3–18。
- en: Mohammadi et al. (2017) Mehdi Mohammadi, Ala Al-Fuqaha, Mohsen Guizani, and
    Jun-Seok Oh. 2017. Semisupervised deep reinforcement learning in support of IoT
    and smart city services. *IEEE Internet of Things Journal* 5, 2 (2017), 624–635.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohammadi 等（2017）**Mehdi Mohammadi**, **Ala Al-Fuqaha**, **Mohsen Guizani**
    和 **Jun-Seok Oh**。2017年。《支持物联网和智能城市服务的半监督深度强化学习》。*IEEE 物联网期刊* 5卷，2期（2017），624–635。
- en: MXNet (2017) MXNet. 2017. Gluon. [https://gluon.mxnet.io](https://gluon.mxnet.io).
    Accessed February 4, 2020.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXNet（2017）**MXNet**。2017年。《Gluon》。[https://gluon.mxnet.io](https://gluon.mxnet.io)。访问时间：2020年2月4日。
- en: Nara et al. (2019) Madhumitha Nara, BR Mukesh, Preethi Padala, and Bharath Kinnal.
    2019. Performance Evaluation of Deep Learning frameworks on Computer Vision problems.
    In *2019 3rd International Conference on Trends in Electronics and Informatics
    (ICOEI)*. IEEE, Tirunelveli, India, India, 670–674.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nara 等（2019）**Madhumitha Nara**, **BR Mukesh**, **Preethi Padala** 和 **Bharath
    Kinnal**。2019年。《计算机视觉问题上的深度学习框架性能评估》。见于 *2019年第三届电子与信息学趋势国际会议（ICOEI）*。IEEE，印度提鲁内来利，670–674。
- en: NVIDIA (2019a) NVIDIA. 2019a. DLProf User-guide. [https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/](https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/).
    Accessed August 26, 2020.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA（2019a）**NVIDIA**。2019a。《DLProf 用户指南》。[https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/](https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/)。访问时间：2020年8月26日。
- en: NVIDIA (2019b) NVIDIA. 2019b. Nvidia Turing Architecture. [https://www.nvidia.com/en-us/design-visualization/technologies/turing-architecture/](https://www.nvidia.com/en-us/design-visualization/technologies/turing-architecture/).
    Accessed February 4, 2020.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA（2019b）**NVIDIA**。2019b。《Nvidia Turing 架构》。[https://www.nvidia.com/en-us/design-visualization/technologies/turing-architecture/](https://www.nvidia.com/en-us/design-visualization/technologies/turing-architecture/)。访问时间：2020年2月4日。
- en: NVIDIA (2019c) NVIDIA. 2019c. TensorRT Github repository. [https://github.com/NVIDIA/TensorRT](https://github.com/NVIDIA/TensorRT).
    Accessed February 4, 2020.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA（2019c）**NVIDIA**。2019c。《TensorRT Github 仓库》。[https://github.com/NVIDIA/TensorRT](https://github.com/NVIDIA/TensorRT)。访问时间：2020年2月4日。
- en: Osia et al. (2018) Seyed Ali Osia, Ali Taheri, Ali Shahin Shamsabadi, Kleomenis
    Katevas, Hamed Haddadi, and Hamid R Rabiee. 2018. Deep private-feature extraction.
    *IEEE Transactions on Knowledge and Data Engineering* 32, 1 (2018), 54–66.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osia 等（2018）**Seyed Ali Osia**, **Ali Taheri**, **Ali Shahin Shamsabadi**, **Kleomenis
    Katevas**, **Hamed Haddadi** 和 **Hamid R Rabiee**。2018年。《深度私密特征提取》。*IEEE 知识与数据工程学报*
    32卷，1期（2018），54–66。
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. PyTorch: An imperative style, high-performance deep learning
    library. In *Advances in Neural Information Processing Systems*. Curran Associates,
    Vancouver, BC, Canada, 8024–8035.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等（2019）**Adam Paszke**, **Sam Gross**, **Francisco Massa**, **Adam Lerer**,
    **James Bradbury**, **Gregory Chanan**, **Trevor Killeen**, **Zeming Lin**, **Natalia
    Gimelshein**, **Luca Antiga** 等。2019年。《PyTorch：一种命令式风格、高性能深度学习库》。见于 *神经信息处理系统进展*。Curran
    Associates，加拿大温哥华，8024–8035。
- en: 'Petricek and Syme (2012) Tomas Petricek and Don Syme. 2012. Syntax Matters:
    Writing abstract computations in F#.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petricek and Syme (2012) Tomas Petricek 和 Don Syme. 2012. 语法重要性：用F#编写抽象计算。
- en: 'Ragan-Kelley et al. (2013) Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams,
    Sylvain Paris, Frédo Durand, and Saman Amarasinghe. 2013. Halide: A Language and
    Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing
    Pipelines. In *Proceedings of the 34th ACM SIGPLAN Conference on Programming Language
    Design and Implementation* *(PLDI ’13)*. Association for Computing Machinery,
    New York, NY, USA, 519–530.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ragan-Kelley et al. (2013) Jonathan Ragan-Kelley，Connelly Barnes，Andrew Adams，Sylvain
    Paris，Frédo Durand 和 Saman Amarasinghe. 2013. Halide: 用于优化图像处理管道中的并行性、局部性和重计算的语言和编译器。发表于
    *第34届ACM SIGPLAN编程语言设计与实现会议论文集* *(PLDI ’13)*。计算机协会，纽约，NY，美国，519–530。'
- en: 'Roesch et al. (2019) Jared Roesch, Steven Lyubomirsky, Marisa Kirisame, Logan
    Weber, Josh Pollock, Luis Vega, Ziheng Jiang, Tianqi Chen, Thierry Moreau, and
    Zachary Tatlock. 2019. Relay: A High-Level Compiler for Deep Learning. arXiv:cs.LG/1904.08368'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roesch et al. (2019) Jared Roesch, Steven Lyubomirsky, Marisa Kirisame, Logan
    Weber, Josh Pollock, Luis Vega, Ziheng Jiang, Tianqi Chen, Thierry Moreau, 和 Zachary
    Tatlock. 2019. Relay: 一种用于深度学习的高级编译器。arXiv:cs.LG/1904.08368'
- en: 'Rotem et al. (2018) Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron,
    Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein,
    Jack Montgomery, Bert Maher, Satish Nadathur, Jakob Olesen, Jongsoo Park, Artem
    Rakhov, Misha Smelyanskiy, and Man Wang. 2018. Glow: Graph Lowering Compiler Techniques
    for Neural Networks. arXiv:cs.PL/1805.00907'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rotem et al. (2018) Nadav Rotem，Jordan Fix，Saleem Abdulrasool，Garret Catron，Summer
    Deng，Roman Dzhabarov，Nick Gibson，James Hegeman，Meghan Lele，Roman Levenstein，Jack
    Montgomery，Bert Maher，Satish Nadathur，Jakob Olesen，Jongsoo Park，Artem Rakhov，Misha
    Smelyanskiy 和 Man Wang. 2018. Glow: 用于神经网络的图降级编译技术。arXiv:cs.PL/1805.00907'
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533–536.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart et al. (1986) David E Rumelhart，Geoffrey E Hinton 和 Ronald J Williams.
    1986. 通过反向传播误差来学习表示。*自然* 323，6088 (1986)，533–536。
- en: 'Seide and Agarwal (2016) Frank Seide and Amit Agarwal. 2016. CNTK: Microsoft’s
    open-source deep-learning toolkit. In *Proceedings of the 22nd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining*. ACM, ACM, San Francisco, CA,
    USA, 2135–2135.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Seide and Agarwal (2016) Frank Seide 和 Amit Agarwal. 2016. CNTK: 微软的开源深度学习工具包。发表于
    *第22届ACM SIGKDD国际知识发现与数据挖掘大会论文集*。ACM，ACM，旧金山，加利福尼亚州，美国，2135–2135。'
- en: Shams et al. (2017) Shayan Shams, Richard Platania, Kisung Lee, and Seung-Jong
    Park. 2017. Evaluation of deep learning frameworks over different HPC architectures.
    In *2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)*.
    IEEE, IEEE Computer Society, Atlanta, GA, USA, 1389–1396.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shams et al. (2017) Shayan Shams，Richard Platania，Kisung Lee 和 Seung-Jong Park.
    2017. 在不同HPC架构下评估深度学习框架。发表于 *2017年IEEE第37届国际分布式计算系统会议（ICDCS）*。IEEE，IEEE计算机学会，亚特兰大，乔治亚州，美国，1389–1396。
- en: Sharma et al. (2016) Hardik Sharma, Jongse Park, Divya Mahajan, Emmanuel Amaro,
    Joon Kyung Kim, Chenkai Shao, Asit Mishra, and Hadi Esmaeilzadeh. 2016. From high-level
    deep neural models to FPGAs. In *The 49th Annual IEEE/ACM International Symposium
    on Microarchitecture*. IEEE Press, IEEE Computer Society, Taipei, Taiwan, China,
    17.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma et al. (2016) Hardik Sharma，Jongse Park，Divya Mahajan，Emmanuel Amaro，Joon
    Kyung Kim，Chenkai Shao，Asit Mishra 和 Hadi Esmaeilzadeh. 2016. 从高级深度神经模型到FPGA。发表于
    *第49届IEEE/ACM国际微架构年会*。IEEE出版社，IEEE计算机学会，台北，台湾，中国，17。
- en: 'Smith and Karypis (2015) Shaden Smith and George Karypis. 2015. Tensor-matrix
    products with a compressed sparse tensor. In *Proceedings of the 5th Workshop
    on Irregular Applications: Architectures and Algorithms*. ACM, Austin, Texas,
    USA, 1–7.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith and Karypis (2015) Shaden Smith 和 George Karypis. 2015. 使用压缩稀疏张量的张量-矩阵乘积。发表于
    *第5届不规则应用：架构与算法研讨会论文集*。ACM，奥斯汀，德克萨斯州，美国，1–7。
- en: 'Team et al. (2016b) D Team et al. 2016b. Deeplearning4j: Open-source distributed
    deep learning for the jvm.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. (2016b) D Team 等。2016b. Deeplearning4j: 用于JVM的开源分布式深度学习。'
- en: 'Team et al. (2016a) The Theano Development Team, Rami Al-Rfou, Guillaume Alain,
    Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Frédéric
    Bastien, Justin Bayer, Anatoly Belikov, et al. 2016a. Theano: A Python framework
    for fast computation of mathematical expressions.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. (2016a) The Theano 开发团队，Rami Al-Rfou，Guillaume Alain，Amjad Almahairi，Christof
    Angermueller，Dzmitry Bahdanau，Nicolas Ballas，Frédéric Bastien，Justin Bayer，Anatoly
    Belikov，等。2016a. Theano: 用于快速计算数学表达式的Python框架。'
- en: 'Tokui et al. (2019) Seiya Tokui, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani,
    Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki
    Yamazaki Vincent. 2019. Chainer: A deep learning framework for accelerating the
    research cycle. In *Proceedings of the 25th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. ACM, Anchorage, AK, USA, 2002–2011.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tokui 等 (2019) Seiya Tokui、Ryosuke Okuta、Takuya Akiba、Yusuke Niitani、Toru Ogawa、Shunta
    Saito、Shuji Suzuki、Kota Uenishi、Brian Vogel 和 Hiroyuki Yamazaki Vincent。2019。Chainer：加速研究周期的深度学习框架。发表于
    *第25届 ACM SIGKDD 国际知识发现与数据挖掘会议*。ACM，安克雷奇，阿拉斯加州，美国，2002–2011。
- en: 'Van Merriënboer et al. (2018) Bart Van Merriënboer, Olivier Breuleux, Arnaud
    Bergeron, and Pascal Lamblin. 2018. Automatic differentiation in ML: Where we
    are and where we should be going. In *Advances in neural information processing
    systems*. Curran Associates, Montréal, Canada, 8757–8767.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Merriënboer 等 (2018) Bart Van Merriënboer、Olivier Breuleux、Arnaud Bergeron
    和 Pascal Lamblin。2018。机器学习中的自动微分：我们目前的位置和未来的方向。发表于 *神经信息处理系统进展*。Curran Associates，蒙特利尔，加拿大，8757–8767。
- en: 'van Merriënboer et al. (2018) Bart van Merriënboer, Olivier Breuleux, Arnaud
    Bergeron, and Pascal Lamblin. 2018. Automatic differentiation in ML: Where we
    are and where we should be going. arXiv:cs.LG/1810.11530'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Merriënboer 等 (2018) Bart van Merriënboer、Olivier Breuleux、Arnaud Bergeron
    和 Pascal Lamblin。2018。机器学习中的自动微分：我们目前的位置和未来的方向。arXiv:cs.LG/1810.11530
- en: Vasilache et al. (2006) Nicolas Vasilache, Cédric Bastoul, and Albert Cohen.
    2006. Polyhedral code generation in the real world. In *International Conference
    on Compiler Construction*. Springer, Springer, Vienna, Austria, 185–201.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vasilache 等 (2006) Nicolas Vasilache、Cédric Bastoul 和 Albert Cohen。2006。现实世界中的多面体代码生成。发表于
    *国际编译器构造会议*。Springer，Springer，维也纳，奥地利，185–201。
- en: 'Vasilache et al. (2018) Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis,
    Priya Goyal, Zachary DeVito, William S Moses, Sven Verdoolaege, Andrew Adams,
    and Albert Cohen. 2018. Tensor comprehensions: Framework-agnostic high-performance
    machine learning abstractions.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vasilache 等 (2018) Nicolas Vasilache、Oleksandr Zinenko、Theodoros Theodoridis、Priya
    Goyal、Zachary DeVito、William S Moses、Sven Verdoolaege、Andrew Adams 和 Albert Cohen。2018。张量理解：框架无关的高性能机器学习抽象。
- en: 'Venieris and Bouganis (2016) Stylianos I Venieris and Christos-Savvas Bouganis.
    2016. fpgaConvNet: A framework for mapping convolutional neural networks on FPGAs.
    In *2016 IEEE 24th Annual International Symposium on Field-Programmable Custom
    Computing Machines (FCCM)*. IEEE, IEEE Computer Society, Washington, DC, USA,
    40–47.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venieris 和 Bouganis (2016) Stylianos I Venieris 和 Christos-Savvas Bouganis。2016。fpgaConvNet：一个将卷积神经网络映射到
    FPGA 的框架。发表于 *2016 IEEE 第24届年度国际现场可编程定制计算机会议 (FCCM)*。IEEE，IEEE 计算机学会，华盛顿特区，美国，40–47。
- en: Venieris et al. (2018) Stylianos I. Venieris, Alexandros Kouris, and Christos-Savvas
    Bouganis. 2018. Toolflows for Mapping Convolutional Neural Networks on FPGAs.
    *Comput. Surveys* 51, 3 (Jun 2018), 1–39. [https://doi.org/10.1145/3186332](https://doi.org/10.1145/3186332)
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venieris 等 (2018) Stylianos I. Venieris、Alexandros Kouris 和 Christos-Savvas
    Bouganis。2018。将卷积神经网络映射到 FPGA 的工具流。*计算机调查* 51, 3 (2018年6月)，1–39。 [https://doi.org/10.1145/3186332](https://doi.org/10.1145/3186332)
- en: Venkat et al. (2015) Anand Venkat, Mary Hall, and Michelle Strout. 2015. Loop
    and Data Transformations for Sparse Matrix Code. In *Proceedings of the 36th ACM
    SIGPLAN Conference on Programming Language Design and Implementation* *(PLDI ’15)*.
    ACM, Portland, OR, USA, 521–532.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkat 等 (2015) Anand Venkat、Mary Hall 和 Michelle Strout。2015。稀疏矩阵代码的循环和数据转换。发表于
    *第36届 ACM SIGPLAN 编程语言设计与实现会议* *(PLDI ’15)*。ACM，波特兰，俄勒冈州，美国，521–532。
- en: Venkat et al. (2014) Anand Venkat, Manu Shantharam, Mary Hall, and Michelle Mills
    Strout. 2014. Non-affine extensions to polyhedral code generation. In *Proceedings
    of Annual IEEE/ACM International Symposium on Code Generation and Optimization*.
    ACM, Orlando, FL, USA, 185–194.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkat 等 (2014) Anand Venkat、Manu Shantharam、Mary Hall 和 Michelle Mills Strout。2014。对多面体代码生成的非仿射扩展。发表于
    *年度 IEEE/ACM 国际代码生成与优化研讨会*。ACM，奥兰多，佛罗里达州，美国，185–194。
- en: 'Verdoolaege (2010) Sven Verdoolaege. 2010. isl: An integer set library for
    the polyhedral model. In *International Congress on Mathematical Software*. Springer,
    Springer, Kobe, Japan, 299–302.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verdoolaege (2010) Sven Verdoolaege。2010。isl：用于多面体模型的整数集合库。发表于 *国际数学软件大会*。Springer，Springer，神户，日本，299–302。
- en: Verdoolaege et al. (2013) Sven Verdoolaege, Juan Carlos Juega, Albert Cohen,
    José Ignacio Gómez, Christian Tenllado, and Francky Catthoor. 2013. Polyhedral
    parallel code generation for CUDA. *ACM Trans. Archit. Code Optim.* 9, 4 (Jan.
    2013), 54:1–54:23. [https://doi.org/10.1145/2400682.2400713](https://doi.org/10.1145/2400682.2400713)
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verdoolaege 等（2013）Sven Verdoolaege, Juan Carlos Juega, Albert Cohen, José Ignacio
    Gómez, Christian Tenllado, 和 Francky Catthoor。2013。用于 CUDA 的多面体并行代码生成。 *ACM Trans.
    Archit. Code Optim.* 9, 4 (2013年1月), 54:1–54:23。 [https://doi.org/10.1145/2400682.2400713](https://doi.org/10.1145/2400682.2400713)
- en: 'Wang et al. (2016) Ying Wang, Jie Xu, Yinhe Han, Huawei Li, and Xiaowei Li.
    2016. DeepBurning: automatic generation of FPGA-based learning accelerators for
    the neural network family. In *Proceedings of the 53rd Annual Design Automation
    Conference*. ACM, ACM, Austin, TX, USA, 110.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2016）Ying Wang, Jie Xu, Yinhe Han, Huawei Li, 和 Xiaowei Li。2016。DeepBurning：为神经网络系列自动生成基于
    FPGA 的学习加速器。载于 *第53届年度设计自动化会议论文集*。ACM，ACM，奥斯汀，TX，美国，第110页。
- en: Wang and O’Boyle (2018) Zheng Wang and Michael O’Boyle. 2018. Machine learning
    in compiler optimization. *Proc. IEEE* 106, 11 (2018), 1879–1901.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 O’Boyle（2018）Zheng Wang 和 Michael O’Boyle。2018。编译器优化中的机器学习。 *Proc. IEEE*
    106, 11 (2018), 1879–1901。
- en: Wei et al. (2019) Gu-Yeon Wei, David Brooks, et al. 2019. Benchmarking tpu,
    gpu, and cpu platforms for deep learning.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2019）Gu-Yeon Wei, David Brooks 等。2019。TPU、GPU 和 CPU 平台在深度学习中的基准测试。
- en: Wei et al. (2017) Xuechao Wei, Cody Hao Yu, Peng Zhang, Youxiang Chen, Yuxin
    Wang, Han Hu, Yun Liang, and Jason Cong. 2017. Automated systolic array architecture
    synthesis for high throughput CNN inference on FPGAs. In *Proceedings of the 54th
    Annual Design Automation Conference 2017*. ACM, ACM, Austin, TX, USA, 29.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2017）Xuechao Wei, Cody Hao Yu, Peng Zhang, Youxiang Chen, Yuxin Wang,
    Han Hu, Yun Liang, 和 Jason Cong。2017。针对 FPGA 上高吞吐量 CNN 推理的自动化脉动阵列架构合成。载于 *第54届年度设计自动化会议论文集
    2017*。ACM，ACM，奥斯汀，TX，美国，第29页。
- en: Xing et al. (2019) Yu Xing, Jian Weng, Yushun Wang, Lingzhi Sui, Yi Shan, and
    Yu Wang. 2019. An In-depth Comparison of Compilers for Deep Neural Networks on
    Hardware. In *2019 IEEE International Conference on Embedded Software and Systems
    (ICESS)*. IEEE, IEEE, Las Vegas, NV, USA, 1–8.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing 等（2019）Yu Xing, Jian Weng, Yushun Wang, Lingzhi Sui, Yi Shan, 和 Yu Wang。2019。硬件上深度神经网络编译器的深入比较。载于
    *2019 IEEE 国际嵌入式软件与系统会议（ICESS）*。IEEE，IEEE，拉斯维加斯，NV，美国，第1–8页。
- en: Yu et al. (2018) Yuan Yu, Martín Abadi, Paul Barham, Eugene Brevdo, Mike Burrows,
    Andy Davis, Jeff Dean, Sanjay Ghemawat, Tim Harley, Peter Hawkins, Michael Isard,
    Manjunath Kudlur, Rajat Monga, Derek Murray, and Xiaoqiang Zheng. 2018. Dynamic
    Control Flow in Large-Scale Machine Learning. In *Proceedings of the Thirteenth
    EuroSys Conference* *(EuroSys ’18)*. Association for Computing Machinery, New
    York, NY, USA, Article Article 18, 15 pages. [https://doi.org/10.1145/3190508.3190551](https://doi.org/10.1145/3190508.3190551)
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2018）Yuan Yu, Martín Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy
    Davis, Jeff Dean, Sanjay Ghemawat, Tim Harley, Peter Hawkins, Michael Isard, Manjunath
    Kudlur, Rajat Monga, Derek Murray, 和 Xiaoqiang Zheng。2018。大规模机器学习中的动态控制流。载于 *第十三届
    EuroSys 会议论文集* *(EuroSys ’18)*。计算机协会，纽约，NY，美国，第18篇，15页。 [https://doi.org/10.1145/3190508.3190551](https://doi.org/10.1145/3190508.3190551)
- en: 'Zhao et al. (2018) R. Zhao, S. Liu, H. Ng, E. Wang, J. J. Davis, X. Niu, X.
    Wang, H. Shi, G. A. Constantinides, P. Y. K. Cheung, and W. Luk. 2018. Hardware
    Compilation of Deep Neural Networks: An Overview. In *2018 IEEE 29th International
    Conference on Application-specific Systems, Architectures and Processors (ASAP)*.
    IEEE Computer Society, Milano, Italy, 1–8. [https://doi.org/10.1109/ASAP.2018.8445088](https://doi.org/10.1109/ASAP.2018.8445088)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2018）R. Zhao, S. Liu, H. Ng, E. Wang, J. J. Davis, X. Niu, X. Wang, H.
    Shi, G. A. Constantinides, P. Y. K. Cheung, 和 W. Luk。2018。深度神经网络的硬件编译：概述。载于 *2018
    IEEE 第29届应用特定系统、架构和处理器国际会议（ASAP）*。IEEE计算机协会，米兰，意大利，第1–8页。 [https://doi.org/10.1109/ASAP.2018.8445088](https://doi.org/10.1109/ASAP.2018.8445088)
