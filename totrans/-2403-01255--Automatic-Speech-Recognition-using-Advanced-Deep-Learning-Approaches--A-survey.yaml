- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:34:06'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2403.01255] Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01255](https://ar5iv.labs.arxiv.org/html/2403.01255)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[orcid=0000-0002-9532-2453] \cormark[1] \creditConceptualization; Methodology;
    Data Curation; Resources; Investigation; Visualization; Writing original draft;
    Writing, review, and editing'
  prefs: []
  type: TYPE_NORMAL
- en: '[orcid=0000-0002-6353-0215] \creditConceptualization; Methodology; Resources;
    Investigation; Writing original draft; Writing, review, and editing'
  prefs: []
  type: TYPE_NORMAL
- en: '[orcid=0000-0001-8904-5587] \creditConceptualization; Methodology; Resources;
    Investigation; Writing original draft; Writing, review, and editing'
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hamza Kheddar kheddar.hamza@univ-medea.dz    Mustapha Hemis hemismustapha@yahoo.fr
       Yassine Himeur yhimeur@ud.ac.ae LSEA Laboratory, Department of Electrical Engineering,
    University of Medea, 26000, Algeria LCPTS Laboratory, University of Sciences and
    Technology Houari Boumediene (USTHB), P.O. Box 32, El-Alia, Bab-Ezzouar, Algiers
    16111, Algeria. College of Engineering and Information Technology, University
    of Dubai, Dubai, UAE
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent advancements in deep learning (DL) have posed a significant challenge
    for automatic speech recognition (ASR). ASR relies on extensive training datasets,
    including confidential ones, and demands substantial computational and storage
    resources. Enabling adaptive systems improves ASR performance in dynamic environments.
    DL techniques assume training and testing data originate from the same domain,
    which is not always true. Advanced DL techniques like deep transfer learning (DTL),
    federated learning (FL), and reinforcement learning (RL) address these issues.
    DTL allows high-performance models using small yet related datasets, FL enables
    training on confidential data without dataset possession, and RL optimizes decision-making
    in dynamic environments, reducing computation costs.
  prefs: []
  type: TYPE_NORMAL
- en: This survey offers a comprehensive review of DTL, FL, and RL-based ASR frameworks,
    aiming to provide insights into the latest developments and aid researchers and
    professionals in understanding the current challenges. Additionally, transformers,
    which are advanced DL techniques heavily used in proposed ASR frameworks, are
    considered in this survey for their ability to capture extensive dependencies
    in the input ASR sequence. The paper starts by presenting the background of DTL,
    FL, RL, and Transformers and then adopts a well-designed taxonomy to outline the
    [state-of-the-art](#Sx1.44.44.44) ([SOTA](#Sx1.44.44.44)) approaches. Subsequently,
    a critical analysis is conducted to identify the strengths and weaknesses of each
    framework. Additionally, a comparative study is presented to highlight the existing
    challenges, paving the way for future research opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Automatic speech recognition \sepDeep transfer learning \sepTransformers \sepFederated
    learning \sepReinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1.1 Preliminary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Advancements in [artificial intelligence](#Sx1.1.1.1) ([AI](#Sx1.1.1.1)) have
    significantly improved [human-machine interaction](#Sx1.22.22.22) ([HMI](#Sx1.22.22.22)),
    especially with technologies that convert speech into executable actions. [automatic
    speech recognition](#Sx1.4.4.4) ([ASR](#Sx1.4.4.4)) emerges as a leading communication
    technology in [HMI](#Sx1.22.22.22), extensively utilized by corporations and service
    providers for facilitating interactions through AI platforms like chatbots and
    digital assistants. Spoken language forms the core of these interactions, emphasizing
    the necessity for sophisticated speech processing in [AI](#Sx1.1.1.1) systems
    tailored for [ASR](#Sx1.4.4.4). [ASR](#Sx1.4.4.4) technology encompasses the analysis
    of (i) acoustic, lexical, and syntactic aspects; and (ii) semantic understanding.
    The [acoustic model](#Sx1.2.2.2) ([AM](#Sx1.2.2.2)) processing includes speech
    coding [[1](#bib.bib1)], enhancement [[2](#bib.bib2)], and source separation [[3](#bib.bib3)],
    alongside securing speech via steganography [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]
    and watermarking [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]. These components
    are integral to audio analysis. On the other hand, the [semantic model](#Sx1.42.42.42)
    ([SM](#Sx1.42.42.42)), often identified as [language model](#Sx1.24.24.24) ([LM](#Sx1.24.24.24))
    processing in literature, involves all [natural language processing](#Sx1.31.31.31)
    ([NLP](#Sx1.31.31.31)) techniques. This AI branch aims at teaching computers to
    understand and interpret human language, serving as the basis for applications
    like music information retrieval [[10](#bib.bib10)], sound file organization [[11](#bib.bib11)],
    audio tagging, and [event detection](#Sx1.17.17.17) ([ED](#Sx1.17.17.17)) [[12](#bib.bib12)],
    as well as converting speech to text and vice versa [[13](#bib.bib13)], detecting
    hate speech [[14](#bib.bib14)], and cyberbullying [[15](#bib.bib15)]. Employing
    [NLP](#Sx1.31.31.31) across various domains enables AI models to effectively comprehend
    and respond to human inputs, unveiling extensive research prospects in diverse
    sectors.
  prefs: []
  type: TYPE_NORMAL
- en: Abbreviations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI
  prefs: []
  type: TYPE_NORMAL
- en: artificial intelligence
  prefs: []
  type: TYPE_NORMAL
- en: AM
  prefs: []
  type: TYPE_NORMAL
- en: acoustic model
  prefs: []
  type: TYPE_NORMAL
- en: APT
  prefs: []
  type: TYPE_NORMAL
- en: audio pyramid transformer
  prefs: []
  type: TYPE_NORMAL
- en: ASR
  prefs: []
  type: TYPE_NORMAL
- en: automatic speech recognition
  prefs: []
  type: TYPE_NORMAL
- en: AST
  prefs: []
  type: TYPE_NORMAL
- en: audio spectrogram transformer
  prefs: []
  type: TYPE_NORMAL
- en: AT
  prefs: []
  type: TYPE_NORMAL
- en: audio tagging
  prefs: []
  type: TYPE_NORMAL
- en: CAFT
  prefs: []
  type: TYPE_NORMAL
- en: client adaptive federated training
  prefs: []
  type: TYPE_NORMAL
- en: CER
  prefs: []
  type: TYPE_NORMAL
- en: character error rate
  prefs: []
  type: TYPE_NORMAL
- en: CNN
  prefs: []
  type: TYPE_NORMAL
- en: convolutional neural network
  prefs: []
  type: TYPE_NORMAL
- en: CS
  prefs: []
  type: TYPE_NORMAL
- en: code-switching
  prefs: []
  type: TYPE_NORMAL
- en: CTC
  prefs: []
  type: TYPE_NORMAL
- en: connectionist temporal classification
  prefs: []
  type: TYPE_NORMAL
- en: CV
  prefs: []
  type: TYPE_NORMAL
- en: computer vision
  prefs: []
  type: TYPE_NORMAL
- en: DA
  prefs: []
  type: TYPE_NORMAL
- en: domain adaptation
  prefs: []
  type: TYPE_NORMAL
- en: DL
  prefs: []
  type: TYPE_NORMAL
- en: deep learning
  prefs: []
  type: TYPE_NORMAL
- en: DRL
  prefs: []
  type: TYPE_NORMAL
- en: deep reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: DTL
  prefs: []
  type: TYPE_NORMAL
- en: deep transfer learning
  prefs: []
  type: TYPE_NORMAL
- en: ED
  prefs: []
  type: TYPE_NORMAL
- en: event detection
  prefs: []
  type: TYPE_NORMAL
- en: FCF
  prefs: []
  type: TYPE_NORMAL
- en: feature correlation-based fusion
  prefs: []
  type: TYPE_NORMAL
- en: FedNST
  prefs: []
  type: TYPE_NORMAL
- en: federated noisy student training
  prefs: []
  type: TYPE_NORMAL
- en: FL
  prefs: []
  type: TYPE_NORMAL
- en: federated learning
  prefs: []
  type: TYPE_NORMAL
- en: FR
  prefs: []
  type: TYPE_NORMAL
- en: form recognition
  prefs: []
  type: TYPE_NORMAL
- en: HMI
  prefs: []
  type: TYPE_NORMAL
- en: human-machine interaction
  prefs: []
  type: TYPE_NORMAL
- en: HMM
  prefs: []
  type: TYPE_NORMAL
- en: hidden Markov models
  prefs: []
  type: TYPE_NORMAL
- en: LM
  prefs: []
  type: TYPE_NORMAL
- en: language model
  prefs: []
  type: TYPE_NORMAL
- en: LPC
  prefs: []
  type: TYPE_NORMAL
- en: linear predictive coding
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs: []
  type: TYPE_NORMAL
- en: long short term memory
  prefs: []
  type: TYPE_NORMAL
- en: mAP
  prefs: []
  type: TYPE_NORMAL
- en: mean average precision
  prefs: []
  type: TYPE_NORMAL
- en: MFCC
  prefs: []
  type: TYPE_NORMAL
- en: Mel-frequency cepstral coefficient
  prefs: []
  type: TYPE_NORMAL
- en: MHSA
  prefs: []
  type: TYPE_NORMAL
- en: multi-head self-attention
  prefs: []
  type: TYPE_NORMAL
- en: ML
  prefs: []
  type: TYPE_NORMAL
- en: machine learning
  prefs: []
  type: TYPE_NORMAL
- en: NLP
  prefs: []
  type: TYPE_NORMAL
- en: natural language processing
  prefs: []
  type: TYPE_NORMAL
- en: NT
  prefs: []
  type: TYPE_NORMAL
- en: negative transfer
  prefs: []
  type: TYPE_NORMAL
- en: PESQ
  prefs: []
  type: TYPE_NORMAL
- en: perceptual evaluation of speech quality
  prefs: []
  type: TYPE_NORMAL
- en: RER
  prefs: []
  type: TYPE_NORMAL
- en: relative error rate
  prefs: []
  type: TYPE_NORMAL
- en: RL
  prefs: []
  type: TYPE_NORMAL
- en: reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: RNN
  prefs: []
  type: TYPE_NORMAL
- en: recurrent neural network
  prefs: []
  type: TYPE_NORMAL
- en: RTF
  prefs: []
  type: TYPE_NORMAL
- en: real-time factor
  prefs: []
  type: TYPE_NORMAL
- en: S2S
  prefs: []
  type: TYPE_NORMAL
- en: sequence-to-sequence
  prefs: []
  type: TYPE_NORMAL
- en: S2S
  prefs: []
  type: TYPE_NORMAL
- en: sequence-to-sequence
  prefs: []
  type: TYPE_NORMAL
- en: SD
  prefs: []
  type: TYPE_NORMAL
- en: source domain
  prefs: []
  type: TYPE_NORMAL
- en: SER
  prefs: []
  type: TYPE_NORMAL
- en: speech emotion recognition
  prefs: []
  type: TYPE_NORMAL
- en: SM
  prefs: []
  type: TYPE_NORMAL
- en: semantic model
  prefs: []
  type: TYPE_NORMAL
- en: SNR
  prefs: []
  type: TYPE_NORMAL
- en: signal-to-noise ratio
  prefs: []
  type: TYPE_NORMAL
- en: SOTA
  prefs: []
  type: TYPE_NORMAL
- en: state-of-the-art
  prefs: []
  type: TYPE_NORMAL
- en: SS
  prefs: []
  type: TYPE_NORMAL
- en: speech security
  prefs: []
  type: TYPE_NORMAL
- en: SSAST
  prefs: []
  type: TYPE_NORMAL
- en: self-supervised audio spectrogram transformer
  prefs: []
  type: TYPE_NORMAL
- en: SWBD
  prefs: []
  type: TYPE_NORMAL
- en: switchboard
  prefs: []
  type: TYPE_NORMAL
- en: TD
  prefs: []
  type: TYPE_NORMAL
- en: target domain
  prefs: []
  type: TYPE_NORMAL
- en: TNR
  prefs: []
  type: TYPE_NORMAL
- en: true negative rate
  prefs: []
  type: TYPE_NORMAL
- en: TPR
  prefs: []
  type: TYPE_NORMAL
- en: true positive rate
  prefs: []
  type: TYPE_NORMAL
- en: TRUNet
  prefs: []
  type: TYPE_NORMAL
- en: transformer-recurrent-U network
  prefs: []
  type: TYPE_NORMAL
- en: WER
  prefs: []
  type: TYPE_NORMAL
- en: word error rate
  prefs: []
  type: TYPE_NORMAL
- en: WSJ
  prefs: []
  type: TYPE_NORMAL
- en: wall street journal
  prefs: []
  type: TYPE_NORMAL
- en: GAN
  prefs: []
  type: TYPE_NORMAL
- en: generative adversarial network
  prefs: []
  type: TYPE_NORMAL
- en: MTL
  prefs: []
  type: TYPE_NORMAL
- en: multitask learning
  prefs: []
  type: TYPE_NORMAL
- en: FMTL
  prefs: []
  type: TYPE_NORMAL
- en: federated multi-task learning
  prefs: []
  type: TYPE_NORMAL
- en: DSLM
  prefs: []
  type: TYPE_NORMAL
- en: domain-specific language modeling
  prefs: []
  type: TYPE_NORMAL
- en: LLM
  prefs: []
  type: TYPE_NORMAL
- en: large language model
  prefs: []
  type: TYPE_NORMAL
- en: DDQN
  prefs: []
  type: TYPE_NORMAL
- en: double deep Q-network
  prefs: []
  type: TYPE_NORMAL
- en: AC
  prefs: []
  type: TYPE_NORMAL
- en: actor-critic
  prefs: []
  type: TYPE_NORMAL
- en: SARSA
  prefs: []
  type: TYPE_NORMAL
- en: State–action–reward–state–action
  prefs: []
  type: TYPE_NORMAL
- en: DDPG
  prefs: []
  type: TYPE_NORMAL
- en: deep deterministic policy gradien
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements in [ASR](#Sx1.4.4.4) have been significantly propelled by
    the evolution of [deep learning](#Sx1.14.14.14) ([DL](#Sx1.14.14.14)) methodologies.
    An extensive range of [DL](#Sx1.14.14.14) models has been developed, demonstrating
    remarkable improvements and surpassing former [SOTA](#Sx1.44.44.44) achievements
    [[16](#bib.bib16), [17](#bib.bib17)]. Transformers, a notable innovation within
    these [DL](#Sx1.14.14.14) approaches, have become a cornerstone in advancing various
    [NLP](#Sx1.31.31.31) tasks, including [ASR](#Sx1.4.4.4). Initially conceptualized
    for [sequence-to-sequence](#Sx1.39.39.39) ([S2S](#Sx1.39.39.39)) applications
    in [NLP](#Sx1.31.31.31), their success is largely attributed to their adeptness
    at discerning long-range dependencies and complex patterns within sequential data.
    A hallmark of transformer models is their utilization of an attention mechanism,
    which precisely focuses on specific portions of the input sequence during prediction
    tasks. This mechanism is particularly effective in [ASR](#Sx1.4.4.4), facilitating
    the detailed modeling of contextual nuances and the interconnections among acoustic
    signals, essential for accurate transcription. Models such as the Transformer
    Transducer, Conformer, and ESPnet, leveraging self-attention and parallel processing,
    have achieved leading performance in [ASR](#Sx1.4.4.4) tasks. Their robustness
    across diverse languages further underscores their capability to adapt to a wide
    range of linguistic features and acoustic variations, making transformers an exceptionally
    promising option for enhancing [ASR](#Sx1.4.4.4) systems, surpassing the constraints
    of conventional models.
  prefs: []
  type: TYPE_NORMAL
- en: The integration of [DL](#Sx1.14.14.14) with its variants in [ASR](#Sx1.4.4.4)
    introduces substantial challenges, especially concerning its application in natural
    [HMI](#Sx1.22.22.22). Despite [DL](#Sx1.14.14.14)’s numerous advantages, it encounters
    various obstacles. The inherent complexity of [DL](#Sx1.14.14.14) models, which
    stems from their need for extensive training data to attain high performance,
    demands significant computational and storage resources [[18](#bib.bib18)]. Moreover,
    the issue of data scarcity in [ASR](#Sx1.4.4.4) reflects the inadequate quantities
    of training data available for exploiting complex [DL](#Sx1.14.14.14) algorithms
    effectively [[19](#bib.bib19)]. The paucity of annotated data further complicates
    the development of supervised [DL](#Sx1.14.14.14)-based [ASR](#Sx1.4.4.4) models.
    Additionally, the presumption that training and testing datasets originate from
    the same domain, possessing identical feature spaces and distribution characteristics,
    is often misguided. This mismatch challenges the practical deployment of [DL](#Sx1.14.14.14)
    models in real-world settings [[20](#bib.bib20)]. Thus, the performance of [DL](#Sx1.14.14.14)
    models may be compromised when faced with limited training datasets or discrepancies
    in data distribution between training and testing environments [[21](#bib.bib21)].
    These challenges highlight the critical need for adaptive methodologies and improved
    data management approaches to fully harness the capabilities of [DL](#Sx1.14.14.14)
    in [ASR](#Sx1.4.4.4) systems.
  prefs: []
  type: TYPE_NORMAL
- en: In an effort to address existing challenges and increase the robustness and
    flexibility of [ASR](#Sx1.4.4.4) systems, novel [DL](#Sx1.14.14.14) methodologies
    have been introduced. These include [deep transfer learning](#Sx1.16.16.16) ([DTL](#Sx1.16.16.16)),
    [deep reinforcement learning](#Sx1.15.15.15) ([DRL](#Sx1.15.15.15)), and [federated
    learning](#Sx1.20.20.20) ([FL](#Sx1.20.20.20)), which collectively aim at overcoming
    difficulties related to the transfer of knowledge, enhancing the generalization
    capabilities of models, and optimizing training processes. These innovative approaches
    significantly broaden the operational scope of conventional [DL](#Sx1.14.14.14)
    frameworks within the [ASR](#Sx1.4.4.4) field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#Sx1.F1 "Figure 1 ‣ 1.1 Preliminary ‣ 1 Introduction ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") highlights
    critical areas in speech processing where [DTL](#Sx1.16.16.16), [DRL](#Sx1.15.15.15),
    and [FL](#Sx1.20.20.20) can be applied. Consequently, domains such as [ASR](#Sx1.4.4.4),
    speech enhancement (SE), hate speech detection (HSD), and [speech security](#Sx1.45.45.45)
    ([SS](#Sx1.45.45.45)) are closely interconnected. [ASR](#Sx1.4.4.4) provides acoustic
    parameters to [NLP](#Sx1.31.31.31) for HSD task, which in turn provides semantic
    details to [ASR](#Sx1.4.4.4). Additionally, [ASR](#Sx1.4.4.4) can be employed
    in the SS domain as a steganalytic process to verify the integrity of speech [[4](#bib.bib4),
    [22](#bib.bib22)]. Furthermore, [ASR](#Sx1.4.4.4) and SE can mutually offer performance
    feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/831c165f8b6ed8f5a79c8c80547a59aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Summary of critical areas in speech processing where [DTL](#Sx1.16.16.16),
    DRL, FL, and transformers can be applied.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Contribution of the paper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This article offers an extensive examination of contemporary frameworks within
    advanced deep learning approaches, spanning the period from 2016 to 2023\. These
    approaches include [DTL](#Sx1.16.16.16), [DRL](#Sx1.15.15.15), [FL](#Sx1.20.20.20),
    and Transformers, all within the context of [ASR](#Sx1.4.4.4). To the best of
    the authors’ knowledge, there has been no prior research paper that has intricately
    explored and critically evaluated contributions in the aforementioned advanced
    DL-based [ASR](#Sx1.4.4.4) until now.
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, numerous survey papers have been published to assess various
    aspects of [ASR](#Sx1.4.4.4) models. Some of these surveys concentrate on specific
    languages, such as Portuguese [[23](#bib.bib23)], Indian [[24](#bib.bib24)], Turkish
    [[25](#bib.bib25)], Arabic [[26](#bib.bib26)] and tonal languages (including Asian,
    Indo-European and African) [[27](#bib.bib27)]. Additionally, Abushariah et al.’s
    review emphasizes bilingual ASR [[28](#bib.bib28)]. On the non-specific language
    review front, specific areas within ASR have been targeted, including ASR using
    limited vocabulary [[29](#bib.bib29)], ASR for children [[30](#bib.bib30)], error
    detection and correction [[31](#bib.bib31)], and unsupervised ASR [[32](#bib.bib32)].
    Systematic reviews with a focus on neural networks [[33](#bib.bib33)] and deep
    neural networks [[34](#bib.bib34)] have also been proposed. In another comprehensive
    review, Malik et al. [[35](#bib.bib35)] discussed diverse feature extraction methods,
    [SOTA](#Sx1.44.44.44) classification models, and some deep learning approaches.
    Recently, the authors presented an ASR review focused on DTL for ASR [[36](#bib.bib36)].
    Table [1](#Sx1.T1 "Table 1 ‣ 1.2 Contribution of the paper ‣ 1 Introduction ‣
    Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    presents a summary of the main contributions of the proposed ASR review compared
    to other existing ASR reviews/surveys.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey article offers several significant enhancements and additions compared
    to previous ASR surveys. Firstly, it consolidates works that utilize both ASR
    and advanced DL approaches, providing a comprehensive overview of their intersection.
    Secondly, it provides performance evaluation results of all considered approaches.
    Thirdly, it includes metrics and dataset reviews used in ASR models. Furthermore,
    it tackles ongoing challenges and consequently proposes future directions. The
    main contributions of this article can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presenting the background of advanced DL techniques including DTL, DRL, FL and
    transformers. Describing the evaluation metrics and datasets employed for validating
    [ASR](#Sx1.4.4.4) approaches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing a well-defined taxonomy categorizing [ASR](#Sx1.4.4.4) methodologies
    based on the domains of [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying challenges and gaps in advanced DL-based [ASR](#Sx1.4.4.4).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proposing future directions to enhance the performance of advanced DL-based
    [ASR](#Sx1.4.4.4) solutions and predicting the potential advancements in the field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 1: Contribution comparison of the proposed contribution against other
    hand ASR review. The tick mark (✓) indicates that the specific field has been
    addressed, whereas the cross mark (✗) means addressing the specific fields has
    been missed.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Refs | Year | Description of the survey/review | Advanced DL methods | Performances
    | Metrics | Dataset | Current | Future |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | DRL | FL | TL | Transf. | evaluation |  | review | challe/Gaps |
    directions |'
  prefs: []
  type: TYPE_TB
- en: '| [[31](#bib.bib31)] | 2018 | ASR review for error errors detection and correction
    | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| [[34](#bib.bib34)] | 2019 | Systematic review on DL-based speech recognition
    | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| [[24](#bib.bib24)] | 2020 | ASR survey for Indian languages | ✗ | ✗ | ✗ |
    ✗ | ✗ | ✗ | ✓ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| [[23](#bib.bib23)] | 2020 | ASR survey for Portuguese language | ✗ | ✗ |
    ✗ | ✗ | ✗ | ✗ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| [[25](#bib.bib25)] | 2020 | ASR survey for Turkich language | ✗ | ✗ | ✗ |
    ✗ | ✓ | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| [[35](#bib.bib35)] | 2021 | ASR survey | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| [[27](#bib.bib27)] | 2021 | ASR survey for tonal languages | ✗ | ✗ | ✗ |
    ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| [[32](#bib.bib32)] | 2022 | Unsupervised ASR review | ✗ | ✗ | ✗ | ✗ | ✓ |
    ✗ | ✗ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| [[30](#bib.bib30)] | 2022 | ASR Systematic review for children | ✗ | ✗ |
    ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] | 2022 | ASR survey for limited vocabulary | ✓ | ✗ | ✗
    | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| [[26](#bib.bib26)] | 2022 | ASR Systematic review for Arabic language | ✗
    | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| [[28](#bib.bib28)] | 2022 | Bilingual ASR review | ✗ | ✗ | ✗ | ✗ | ✓ | ✗
    | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| [[33](#bib.bib33)] | 2023 | ASR survey on neural network techniques | ✗ |
    ✗ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| [[36](#bib.bib36)] | 2023 | ASR based on [DTL](#Sx1.16.16.16) review | ✗
    | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Our | 2024 | ASR review on advanced DL techniques | ✓ | ✓ | ✓ | ✓ | ✓ | ✓
    | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 1.3 Review methodology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The methodology for the review is delineated in this segment, encompassing
    the search strategy and study selection. Inclusion criteria, comprising keyword
    alignment, creativity and impact, and uniqueness, are explicated, collectively
    influencing the formulation of the paper’s quality assessment protocol. To locate
    and compile extant advanced DL-based [ASR](#Sx1.4.4.4) studies, a thorough search
    was executed on renowned publication databases recognized for hosting top-tier
    scientific research articles. The exploration encompassed Scopus and Web of Science.
    Keywords were extracted and organized from the initial set of references through
    manual analysis. Employing "theme clustering," these publications were sorted
    based on keywords found in the "Abstract," "Title," and "Authors keywords." The
    outcome of this process yielded the formulation of the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: References=FROM "Abstract" || "Title"|| "Authors keywords" SELECT( Papers WHERE
    keywords= ([ASR](#Sx1.4.4.4) || [NLP](#Sx1.31.31.31)) & ( [DTL](#Sx1.16.16.16)
    || [DRL](#Sx1.15.15.15) || [FL](#Sx1.20.20.20) || Transformers)).
  prefs: []
  type: TYPE_NORMAL
- en: The symbols || and & denote OR and AND logical operations, respectively. The
    evaluation of publications considered the innovation level in [ASR](#Sx1.4.4.4),
    the study’s quality, and the contributions and findings presented. This review
    exclusively encompassed research contributions that were published within the
    timeframe of 2016 to 2023.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Structure of the paper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This paper is structured into six sections. The current section provides an
    introduction to the paper. Section [1.3](#Sx1.SS3 "1.3 Review methodology ‣ 1
    Introduction ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey") providing background on [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24), and
    reviewing evaluation metrics and datasets utilized in [ASR](#Sx1.4.4.4). Moving
    forward, Section [4](#S4 "4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") delves
    into a comprehensive review of recent advancements in [ASR](#Sx1.4.4.4) utilizing
    advanced [DL](#Sx1.14.14.14) approaches, including Transformers, [DTL](#Sx1.16.16.16),
    [FL](#Sx1.20.20.20) and [DRL](#Sx1.15.15.15). Sections [5](#S5 "5 Open Issues
    and of Key challenges ‣ Automatic Speech Recognition using Advanced Deep Learning
    Approaches: A survey") and [6](#S6 "6 Future directions ‣ Automatic Speech Recognition
    using Advanced Deep Learning Approaches: A survey") respectively address the existing
    challenges and future directions concerning advanced DL-based [ASR](#Sx1.4.4.4).
    Finally, Section [7](#S7 "7 Conclusion ‣ Automatic Speech Recognition using Advanced
    Deep Learning Approaches: A survey") presents concluding remarks summarizing the
    key findings of the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58ddfc5ff4acf16ee636d0f80a2461db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Survey roadmap: A guide for navigating paper sections and subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: table[t!] Literature acquisition databases. to be modified Database Research
    Articles Conference Papers Book Chapter Total ACM 5 1 – 6 Elsevier 26 – – 26 Springer
    23 – 10 33 IEEE 39 28 – 67 Others 44 29 – 70
  prefs: []
  type: TYPE_NORMAL
- en: 2 Overview of DTL techniques for speech recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Taxonomy of existing DTL techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To date, there is no standardized and comprehensive technique for classifying
    DTL into categories. However, DTL algorithms could be classified into several
    types depending on what, when, and how knowledge is transferred:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) What knowledge is transferred? Enquires about which characteristics of knowledge
    are transferable across domains or tasks. Some information is particular to certain
    domains or tasks, while other knowledge is shared across domains and can aid increase
    performance in the target task or domain. Based on this definition, DTL could
    be feature-based, instance-based, relation-based, or model-based.
  prefs: []
  type: TYPE_NORMAL
- en: (b) How is knowledge transferred? Enquires about which learning algorithms must
    be implemented to transfer the knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: (c) When is knowledge transferred? Inquires as to when and under what circumstances
    knowledge should or should not be transferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, researchers have proposed taxonomies to categorize DTL-based ASR
    techniques. For instance, Niu et al. [[21](#bib.bib21)] present a taxonomy with
    two levels. The first level consists of four sub-groups based on the availability
    of labeled data and the data modality in the source and target domains. These
    sub-groups include inductive DTL, transductive DTL, cross-modality DTL, and unsupervised
    DTL [[80](#bib.bib80)]. Table [2](#S2.T2 "Table 2 ‣ 2.1 Taxonomy of existing DTL
    techniques ‣ 2 Overview of DTL techniques for speech recognition ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey") provides a summary
    of these possibilities. Moreover, each sub-group at the first level can be further
    divided into four distinct learning types: learning on instances, learning on
    features, learning on parameters, and learning on relations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: DTL possibilities. whereas the mark ($\varsubsetneq$) indicates that
    the domains/tasks are different but related, ($\exists!$) indicates that there
    exists one and only one domain/task, and ($\cong$) indicates that domains, tasks,
    or spaces are not always equals.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Domains | Tasks | Math. propriety | Sub-categories / Usage |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional ML/DL | $\mathbb{D}_{S}=\mathbb{D}_{T}$ | $\mathbb{T}_{S}=\mathbb{T}_{T}$
    | $X_{S}\neq X_{T}$, $Y_{S}=Y_{T}$ | ASR model trained with $X_{S}$ database and
    used to recognise $X_{T}$ database. |'
  prefs: []
  type: TYPE_TB
- en: '| Inductive DTL | $\mathbb{D}_{S}\cong\mathbb{D}_{T}$ | $\mathbb{T}_{S}\neq\mathbb{T}_{T}$
    | $X_{S}\neq X_{T}$, $Y_{S}\exists,Y_{T}\exists$ | If $Y_{S}\exists$, DTL is multitask
    learning. If $Y_{S}\nexists$, DTL is self-taught learning, thus $\chi_{S}\cong\chi_{T}$.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Transductive DTL | $\mathbb{D}_{S}\neq\mathbb{D}_{T}$ | $\mathbb{T}_{S}=\mathbb{T}_{T}$
    | $P(X_{S})\neq P(X_{T})$, $Y_{S}\exists,Y_{T}\nexists$ ,'
  prefs: []
  type: TYPE_NORMAL
- en: $\chi_{S}=\chi_{T}$ | When $\chi_{S}=\chi_{T}$, DTL is is related to DA. If
    $\mathbb{D}_{T}\exists!$ and $\mathbb{T}_{T}\exists!$, DTL is used for sample
    selection bias or covariate shift. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Cross-modality DTL | $\mathbb{D}_{S}\neq\mathbb{D}_{T}$ | $\mathbb{T}_{S}\neq\mathbb{T}_{T}$
    | $P(Y_{S}/X_{S})\neq P(Y_{T}/X_{T})$, $Y_{S}\neq Y_{T}$, $\chi_{S}\neq\chi_{T}$
    | i,e. the dataset $X_{S}$ of $\mathbb{D}_{S}$ is speech data, and the dataset
    $X_{T}$ of $\mathbb{D}_{T}$ is text data. |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised DTL | $\mathbb{D}_{S}\varsubsetneq\mathbb{D}_{T}$ | $\mathbb{T}_{S}\varsubsetneq\mathbb{T}_{T}$
    | $Y_{S}\nexists,Y_{T}\nexists$ | DTL used for clustering, dimensionality reduction,
    and density estimation, etc. |'
  prefs: []
  type: TYPE_TB
- en: 'Inductive DTL In comparison to classical ML, which may be used as a reference
    for DTL comparison, and given that the target tasks $\mathbb{T}_{T}$ are distinct
    from the source tasks $\mathbb{T}_{S}$, the goal of inductive DTL is to enhance
    the target prediction function $\mathbb{F}_{T}$ in the TD, mentioned above in
    subsection LABEL:sub12. However, the SD $\mathbb{D}_{S}$ and TD $\mathbb{D}_{T}$
    may not always be the same (Table [2](#S2.T2 "Table 2 ‣ 2.1 Taxonomy of existing
    DTL techniques ‣ 2 Overview of DTL techniques for speech recognition ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey")). The inductive
    DTL can be stated similarly to the following two cases, depending on whether labeled
    or unlabeled data is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '(a) Multi-task DTL: The SD has a huge labeled database ($X_{S}$ labeled with
    $Y_{S}$), which is a distinctive form of multi-task learning. However, with multi-task
    approaches, many tasks $(T_{1},T_{2},\dots,T_{n})$ are learned at the same time
    (in parallel), including both source and target activities (tasks).'
  prefs: []
  type: TYPE_NORMAL
- en: '(b) Sequential DTL: ( Commonly known as self-taught learning) Dataset is not
    labeled in the SD ($X_{S}$ is not labeled with $Y_{S}$) but the labels are available
    in the destination domain ($X_{T}$ is labeled with $Y_{T}$). Sequential learning
    is a DL system that can be realized in two steps for classification purposes.
    The first step is the feature representation transfer, which is learned from a
    large collection of the unlabeled datasets, and the second stage is when this
    learned representation is applied to labeled data to accomplish classification
    tasks. Hence, sequential DTL is a method of sequentially learning a number of
    activities (Tasks). The spaces between the source and destination domains may
    differ. For example, let suppose we have a pre-trained model $M$ and consider
    applying DTL to a number of tasks $(T_{1},T_{2},\dots,T_{n})$. We learn a specific
    task $\mathbb{T}_{T}$ at each time step $t$, which is slower than multi-task learning.
    However, when not all the tasks are present during training time, it might be
    beneficial. Sequential DTL is additionally classified into several types [alyafeai2020survey]:'
  prefs: []
  type: TYPE_NORMAL
- en: 1-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-tuning: The principle is to learn a new function $\mathbb{F}_{T}$ that
    translates the parameters $\mathbb{F}_{T}(W_{S})=W_{T}$ by using $M$, given a
    pre-trained model $M_{S}$ having $W_{S}$ as weights and target task $\mathbb{T}_{T}$
    having $W_{T}$ as weights. The settings can be adjusted across all layers or just
    some of (Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Taxonomy of existing DTL techniques ‣
    2 Overview of DTL techniques for speech recognition ‣ Automatic Speech Recognition
    using Advanced Deep Learning Approaches: A survey") (a)). The learning rate for
    each layer could be distinct (discriminative fine tuning). A new set of parameters
    $K$ could be added to most of the tasks so that $\mathbb{F}_{T}(W_{T},K)=W_{S}\circ
    K$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adapter modules: Given an $M_{S}$ model that has been pre-trained and output
    $W_{S}$, for a target task $\mathbb{T}_{T}$. The adapter module aims to lunch
    a different set of parameters $K$ that is too much less than $W_{S}$, i.e, $K\ll
    W_{S}$. $K$ and $W_{S}$ must have the ability to be decomposed into more compact
    modules such that, $W_{S}=\{w\}_{n}$ and $K=\{k\}_{n}$. The adapter module permit
    learning the following new function $\mathbb{F}_{T}$:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbb{F}_{T}(K,W_{S})=k_{1}^{\prime}\circ w_{1}\circ\dots k_{n}^{\prime}\circ
    w_{n}.$ |  | (1) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'According to equation [1](#S2.E1 "In item 2- ‣ 2.1 Taxonomy of existing DTL
    techniques ‣ 2 Overview of DTL techniques for speech recognition ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey"), during the adaptation
    procedure, the set of original weights $W_{S}=\{w\}_{n}$ is left unaltered, but
    the set of weights K is changed to $K^{\prime}=\{k^{\prime}\}_{n}$. The principle
    of the adaptation domain is illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Taxonomy
    of existing DTL techniques ‣ 2 Overview of DTL techniques for speech recognition
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    (b).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature based: Interested only in learning concepts and representations, at
    various levels, such as, word, character, phrase, or paragraph embedding $E$.
    The collection of $E$ based on a model $M$ remains unaltered, i.e., $\mathbb{F}_{T}(W_{S},E)=E\circ
    W^{\prime}$, in the way that $W^{\prime}$ is fine-tuned. For example, researchers
    have applied the generative adversarial network (GAN) principle to DTL where,
    the generators send features from the SD and the TD to a discriminator, which
    determines the source of the features and feeds the result back to the generators
    until they can no longer be distinguished. In this procedure, GAN obtains the
    common properties of two domains, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1
    Taxonomy of existing DTL techniques ‣ 2 Overview of DTL techniques for speech
    recognition ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey") (c).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zero-shot: Is the easiest method among all of the others. Making the assumption
    that the parameters $W_{S}$ can’t be modified or add $K$ as a new parameters to
    a pre-trained model $M_{S}$ using $W_{S}$. To put this into context, in zero-shot
    there is no training technique to optimize or learn new parameters.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b511658b064674ac061c891b2172afe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Structures of: (a) Fine-tuning, (b) DA, and (c) DTL-based GAN.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Transductive DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Compared to the traditional ML, which can be considered as a reference for
    DTL comparison, and given that the TDs $\mathbb{D}_{T}$ are distinct from the
    SDs $\mathbb{D}_{S}$. The SD has a labeled dataset ($X_{S}$ labeled with $Y_{S}$),
    whereas the TD has no labeled dataset, the source and target tasks are equal (
    Table [2](#S2.T2 "Table 2 ‣ 2.1 Taxonomy of existing DTL techniques ‣ 2 Overview
    of DTL techniques for speech recognition ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey")). The goal of transductive DTL is
    to build the target prediction function $\mathbb{F}_{T}$ in the $\mathbb{D}_{T}$
    by knowledge of the $\mathbb{D}_{S}$ and $\mathbb{T}_{T}$. Furthermore, the transductive
    DTL environment may be further classified into two categories depending on different
    conditions between the source and destination domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '(a) Domain adaptation (DA): The feature spaces across domains, $\chi_{S}$ and
    $\chi_{T}$, are the identical, but the marginal probability distributions of the
    input dataset are not, $P(Y_{S}/X_{S})\neq P(Y_{T}/X_{T})$. For example, an assessment
    may be done on the topic of the resort in the $\mathbb{D}_{S}$ and it will be
    used to train a model for restaurants in the $\mathbb{D}_{T}$. DA is mostly effective
    when the $\mathbb{T}_{T}$ has a distinct distribution or there is a scarcity of
    labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: '(b) Cross-modality DTL: Also known as cross-lingual DTL in the spoken language
    field, most DTL methods, more or less, a connection in feature spaces or label
    spaces is required between $\mathbb{D}_{S}$ and $\mathbb{D}_{T}$. In other words,
    DTL can only occur when the source and destination data are both in the same modality,
    like video, speech, or text. cross-lingual DTL, in contrast to all other DTL approaches,
    is one of the most complicated issues in DTL. It is assumed that the feature spaces
    of the source and destination domains are completely distinct ($\chi_{S}\neq\chi_{T}$),
    as in speech-to-image, image-to-text, and text-to-speech. Furthermore, the label
    spaces of source $Y_{S}$ and destination $Y_{S}$ domains might differ ($Y_{S}\neq
    Y_{T}$).'
  prefs: []
  type: TYPE_NORMAL
- en: '(c) Unsupervised DTL: Intends to enhance the learning of the target predictive
    function $\mathbb{F}_{T}$ in $\mathbb{D}_{T}$ using the knowledge in $\mathbb{D}_{S}$
    and $\mathbb{T}_{S}$, where $\mathbb{T}_{S}$ different from $\mathbb{T}_{T}$ but
    related, and $Y_{S}$ and $Y_{T}$ are not visible, given a SD $\mathbb{D}_{S}$
    with a learning task $\mathbb{T}_{S}$, a TD $\mathbb{D}_{T}$ and a matching learning
    task $\mathbb{T}_{T}$ ($\mathbb{D}_{S}$ different from $\mathbb{D}_{T}$, but related).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Adversarial DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to the methods described above for DTL, adversarial learning [wang2020transfer]
    aids in the learning of more transferable and discriminative representations.
    The work in [ganin2016domain], was the first that introduced the domain-adversarial
    neural network (DANN). Instead of using a predefined distance function like maximum
    mean discrepancy (MMD), the core idea is to use a domain-adversarial loss in the
    network. This has greatly aided the network’s ability to learn more discriminative
    data. Many studies have used domain-adversarial training as a result of DANN’s
    idea [bousmalis2016domain, chen2019joint, long2017deep, zhang2018collaborative].
    All of the previous work ignores the different effects of marginal and conditional
    distributions in adversarial TL, whereas in [wang2020transfer], the proposed scheme,
    named dynamic distribution alignment (DDA), can dynamically evaluate the importance
    of each distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Acoustic and language models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The [AM](#Sx1.2.2.2) is in charge of capturing the sound characteristics of
    different phonetic units. This involves generating statistical measures for characteristic
    vector sequences from the audio waveform. Various techniques, such as [linear
    predictive coding](#Sx1.25.25.25) ([LPC](#Sx1.25.25.25)), Cepstral analysis, filter-bank
    analysis, [Mel-frequency cepstral coefficients](#Sx1.28.28.28), wavelet analysis,
    and others, can be used to extract these features [[37](#bib.bib37)]. In the processing
    stage, a decoder (search algorithm) uses the acoustic lexicon and [LM](#Sx1.24.24.24)
    to create the hypothesized word or phoneme. You can see the overall process illustrated
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.1 Acoustic and language models ‣ 3 Background
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: \Acp
  prefs: []
  type: TYPE_NORMAL
- en: 'LM provide probabilities of sequences of words, crucial for [ASR](#Sx1.4.4.4)
    systems to predict the likelihood of subsequent words in a sentence [[38](#bib.bib38)].
    A domain-specific LM is trained on text data from the target domain to capture
    its unique vocabulary and grammatical structures [[39](#bib.bib39)]. For n-gram
    models, this involves calculating the conditional probability of a word given
    the previous $n-1$ words [[40](#bib.bib40)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(w_{n}&#124;w_{n-1},w_{n-2},\ldots,w_{n-(n-1)})=\frac{C(w_{n-(n-1)},\ldots,w_{n})}{C(w_{n-(n-1)},\ldots,w_{n-1})}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: In the context of [ASR](#Sx1.4.4.4), the [LM](#Sx1.24.24.24) complements the
    [AM](#Sx1.2.2.2) by providing linguistic context. The combined probability from
    the [AM](#Sx1.2.2.2) and the [LM](#Sx1.24.24.24) helps in determining the most
    likely transcription for a given audio input during the decoding process. The
    frequently utilized [LM](#Sx1.24.24.24) in [ASR](#Sx1.4.4.4) systems is the backoff
    n-gram model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c7b78cedf3451c18a4ac47c0bc1b5439.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Diagram illustrating the end-to-end framework for [ASR](#Sx1.4.4.4).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Evaluation criteria in ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To assess the effectiveness and suitability of [ASR](#Sx1.4.4.4) techniques,
    researchers have employed diverse methods. Some of these encompass well-established
    [DL](#Sx1.14.14.14) metrics, including accuracy, F1-score, recall (sensitivity
    or [true positive rate](#Sx1.50.50.50) ([TPR](#Sx1.50.50.50))), precision (also
    known as positive predictive value), and specificity (commonly referred to as
    [true negative rate](#Sx1.49.49.49) ([TNR](#Sx1.49.49.49))) [[41](#bib.bib41)].
    These metrics serve as crucial evaluation criteria for experimental outcomes,
    as evidenced in studies such as [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)].
    Additionally, there are [ASR](#Sx1.4.4.4)-specific metrics, which are detailed
    in Table [3](#S3.T3 "Table 3 ‣ 3.2 Evaluation criteria in ASR ‣ 3 Background ‣
    Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: An overview of the metrics employed for evaluating [ASR](#Sx1.4.4.4)
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Metric | Formula | Description |'
  prefs: []
  type: TYPE_TB
- en: '| [WER](#Sx1.52.52.52) | $\displaystyle\mathrm{\frac{S+D+I}{N}=\frac{S+D+I}{H+S+D}}.$
    | The [word error rate](#Sx1.52.52.52) ([WER](#Sx1.52.52.52)) serves as a frequently
    utilized metric to assess the performance of Automatic Speech Recognition ([ASR](#Sx1.4.4.4)).
    It is computed by determining the ratio of incorrectly recognized words to the
    overall number of processed words [[45](#bib.bib45), [17](#bib.bib17), [46](#bib.bib46)].
    In the given context, $\mathrm{I,D,S,H}$, and $\mathrm{N}$ denote the quantities
    of insertions, deletions, substitutions, hits, and input words, respectively.
    Instead of [WER](#Sx1.52.52.52), the [character error rate](#Sx1.8.8.8) ([CER](#Sx1.8.8.8))
    has been employed, while adhering to the same evaluation principle. |'
  prefs: []
  type: TYPE_TB
- en: '| PESQ and MOS-LQO | $\displaystyle\mathrm{MOS-LQO}=0.999+\frac{4.999-0.999}{1+e^{-1.4945.PESQ+4.6607}}$
    | [PESQ](#Sx1.33.33.33) serves as an objective technique for evaluating the perceived
    quality of speech [[47](#bib.bib47)]. The assessment involves assigning numerical
    scores within the range of -0.5 to 4.5\. Additionally, a correlation can be established
    between MOS and PESQ scores, giving rise to a novel evaluation metric termed the
    mean opinion score-listening quality objective (MOS-LQO), also identified as PESQ
    Rec.862.1\. [[22](#bib.bib22)] |'
  prefs: []
  type: TYPE_TB
- en: '| [RTF](#Sx1.37.37.37) | $\displaystyle\mathrm{RTF=\frac{\text{Total Processing
    Time}}{\text{Total Duration}}}$ | \Ac RTF serves as a standard metric to assess
    the processing time cost of an [ASR](#Sx1.4.4.4) system. It represents the average
    processing time required for one second of speech |'
  prefs: []
  type: TYPE_TB
- en: '| [RER](#Sx1.34.34.34) | $\displaystyle\frac{(E_{\text{baseline}}-E_{\text{proposed}})}{E_{\text{baseline}}}\times
    100\%$ | The [relative error rate](#Sx1.34.34.34) ([RER](#Sx1.34.34.34)) expresses
    the percentage error rate achieved by the proposed DL model compared to the baseline.
    $E_{\text{baseline}}$ is the error rate of the baseline model. $E_{\text{proposed}}$
    is the error rate of the proposed model or method. |'
  prefs: []
  type: TYPE_TB
- en: '| D | $\displaystyle\frac{\sum_{i=1}^{n}\Big{(}1-\frac{\sum_{j=1}^{n}a_{ij}\cdot&#124;i-j&#124;}{\max(&#124;i-1&#124;,&#124;i-2&#124;,\ldots,&#124;i-n&#124;)}\Big{)}}{n}\hskip
    28.45274pt$ | Diagonal centrality of an attention matrix (D) is defined as the
    mean value across the centrality of all its rows. where $j$ represents the index
    of each column, $n$ signifies the length of the input sequence, $a_{ij}$ denotes
    the attention weight between the $i$-th and $j$-th elements of the input sequence,
    and $&#124;i-j&#124;$ signifies the distance between the $i$-th and $j$-th elements
    of the input sequence [[48](#bib.bib48)]. |'
  prefs: []
  type: TYPE_TB
- en: 3.3 ASR datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Various datasets have been employed in the literature for diverse [ASR](#Sx1.4.4.4)
    tasks. Table [4](#S3.T4 "Table 4 ‣ 3.3 ASR datasets ‣ 3 Background ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") presents
    a selection of datasets utilized for DTL-based [ASR](#Sx1.4.4.4) applications,
    along with their respective characteristics. It is important to note that the
    table primarily includes publicly accessible repositories. Furthermore, it is
    worth mentioning that certain datasets have undergone multiple updates and improvements
    over time, leading to their enhanced development.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: List of publicly available datasets used for advanced DL-based [ASR](#Sx1.4.4.4)
    applications'
  prefs: []
  type: TYPE_NORMAL
- en: . Dataset Used by Default ASR task Characteristics LibriSpeech [[49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51)] Train and assess systems for recognizing speech.
    The collection consists of 1000 hours of speech recorded at a 16 kHz sampling
    rate, sourced from audiobooks included in the LibriVox project. DCASE [[52](#bib.bib52),
    [53](#bib.bib53)] Identifying acoustic environments and detecting sound occurrences.
    Comprise 8 coarse-level and 23 fine-level urban sound categories, collected in
    New York City in 2020 using 50 acoustic sensors. WSJ [[48](#bib.bib48)] Acoustic
    scene and sound event corpus Comprises an extensive 81 hours of meticulously curated
    read speech training data. SWBD [[48](#bib.bib48)] Conversational telephone speech
    corpus Is a comprehensive collection, boasting a substantial 260 hours of conversational
    telephone speech training data. AISHELL [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]
    Chinese Mandarin speech corpus 400 participants from diverse Chinese accent regions
    recorded in a quiet indoor space using high-fidelity microphones, later downsampled
    to 16kHz. CHIME3 [[57](#bib.bib57)] SR for distant microphone in real-world settings.
    Includes around 342 hours of English speech with noisy transcripts and 50 hours
    of noisy environment recordings. Google-SC [[57](#bib.bib57)] Speech commands
    with a restricted range of words. The dataset contains 105,829 one-second utterances
    of 35 words categorized by frequency. Each utterance is stored as a one-second
    WAVE format file with 16-bit single-channel at 16KHz rate. It involves 2,618 speakers
    AURORA4 [[57](#bib.bib57)] Compare front-ends for large vocabulary recognition
    performance. Aurora-4 is a speech recognition dataset derived from the WSJ corpus,
    offering four conditions (Clean, channel, noisy, channel+noisy) with two microphone
    types and six noise types, totaling 4,620 utterances per set. Car-env [[57](#bib.bib57)]
    Vehicle environment sound Is a dataset from Korea that spans 100 hours of recordings
    in a vehicle. It comprises brief commands, with an average of 1.6 words per command.
    HKUST [[50](#bib.bib50), [56](#bib.bib56)] Classify Mandarin speech into standard
    and accented types. Comprises roughly 149 hours of telephone conversations in
    Mandarin. AudioSet [[51](#bib.bib51)] Audio event recognition Includes 1,789,621
    segments of 10 seconds each (equivalent to 4,971 hours). It consists of at least
    100 instances clustered into 632 audio classes, with only 485 audio event categories
    clearly identified. AWIC-19 [[58](#bib.bib58)] Arabic words recognition It comprises
    770 recordings featuring isolated Arabic words. TED2 [[59](#bib.bib59)] English
    corpus for ASR The dataset was first made available in May 2012, for training,
    it comprising 118 hours, 4 minutes, and 48 seconds of training data from 666 speakers,
    containing approximately 1.7 million words.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Advanced ASR methods and applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional statistical [LMs](#Sx1.24.24.24), such as backoff n-gram [LMs](#Sx1.24.24.24),
    have been widely used due to their simplicity and reliability. However, bidirectional
    encoder representations from transformers (BERT), which utilize attention models,
    have shown better contextual understanding compared to single-direction LMs, as
    demonstrated in the work of Devlin et al. [[60](#bib.bib60)].
  prefs: []
  type: TYPE_NORMAL
- en: In terms of [AM](#Sx1.2.2.2), deep learning-based models like the deep neural
    network-hidden Markov model (DNN-HMM) and the [connectionist temporal classification](#Sx1.11.11.11)
    ([CTC](#Sx1.11.11.11)) have made significant advancements. DNN-HMM models have
    been extensively studied in [ASR](#Sx1.4.4.4) research, while [CTC](#Sx1.11.11.11)
    is an end-to-end training method that does not require pre-alignment and only
    needs input and output sequences. The [S2S](#Sx1.39.39.39) model has also been
    successful in solving [ASR](#Sx1.4.4.4) tasks without using an [LM](#Sx1.24.24.24)
    or pronunciation dictionary, as described in Chiu et al. [[61](#bib.bib61)].
  prefs: []
  type: TYPE_NORMAL
- en: '[ASR](#Sx1.4.4.4) systems often face performance degradation in certain situations
    due to the "one-model-fits-all" approach. Additionally, the lack of diverse and
    sufficient training data affects [AM](#Sx1.2.2.2) performance. To overcome these
    constraints and improve the resilience and flexibility of [ASR](#Sx1.4.4.4) systems,
    advanced DL methodologies such as [DTL](#Sx1.16.16.16) and it sub-field [domain
    adaptation](#Sx1.13.13.13) ([DA](#Sx1.13.13.13)), [DRL](#Sx1.15.15.15), and [FL](#Sx1.20.20.20)
    have surfaced. These innovative methodologies collectively address issues concerning
    knowledge transfer, model generalization, and training effectiveness, offering
    remedies that expand upon the capabilities of traditional DL models within the
    [ASR](#Sx1.4.4.4) sphere. Thus, many research studies have focused on enhancing
    existing [ASR](#Sx1.4.4.4) systems by applying the aforementioned algorithms.
    Figure [5](#S4.F5 "Figure 5 ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") provides
    an overview of the current [SOTA](#Sx1.44.44.44) advanced DL-based [ASR](#Sx1.4.4.4)
    and its most useful related schemes in both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/715294b829e105e4ec94b5782bd80b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Overview of advanced DL-driven [ASR](#Sx1.4.4.4) algorithms and their
    commonly utilized models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Transformer-based ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Transformer stands as a prominent deep learning model extensively employed
    across diverse domains, including [NLP](#Sx1.31.31.31), [computer vision](#Sx1.12.12.12)
    ([CV](#Sx1.12.12.12)), and speech processing. Originally conceived for machine
    translation as a [S2S](#Sx1.39.39.39) model, it has evolved to find applications
    in various fields. The Transformer heavily relies on the self-attention mechanism,
    enabling it to capture extensive dependencies in input sequences. The standard
    Transformer model incorporates the query–key–value (QKV) attention mechanism.
    In this setup, given matrix representations of queries $\mathbf{Q}\in\mathbb{R}^{N\times
    D_{k}}$, keys $\mathbf{K}\in\mathbb{R}^{M\times D_{k}}$, and values $\mathbf{V}\in\mathbb{R}^{M\times
    D_{v}}$, the scaled dot-product attention is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{D_{k}}}\right)\mathbf{V}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $N$ and $M$ represent the lengths of queries and keys (or values), and
    $D_{k}$ and $D_{v}$ denote the dimensions of keys (or queries) and values. The
    softmax operation is applied row-wise to the matrix $\mathbf{A}$. Within the Transformer
    architecture, three attention mechanisms exist based on the source of queries
    and key–value pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Self-attention: In the Transformer encoder, queries $\mathbf{Q}$, keys $\mathbf{K}$,
    and values $\mathbf{V}$ all equal the outputs of the previous layer, denoted as
    $\mathbf{X}$ in Eq. (2).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Masked Self-attention: In the Transformer decoder, self-attention is constrained,
    allowing queries at each position to attend only to key–value pairs up to and
    including that position. This is accomplished by implementing a mask function,
    before normalization, on the attention matrix $\hat{\mathbf{A}}=\exp\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{D_{k}}}\right)$,
    where illegal positions are masked out by setting $\hat{A}_{ij}=-\infty$ if $i<j$.
    This type of self-attention is often referred to as autoregressive or causal attention.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross-attention: In cross-attention, queries originate from the results of
    the preceding (decoder) layer, while keys and values stem from the outputs of
    the encoder.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Numerous studies in the [ASR](#Sx1.4.4.4) field have introduced transformer-based
    approaches, encompassing both the acoustic and language domains. In the subsequent
    subsections, we delve into a comprehensive review and detailed analysis of several
    cutting-edge techniques within each of these categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of some proposed work in transformer-based ASR. The symbol
    ($\uparrow$) denotes result increase, whereas ($\downarrow$) signifies result
    decrease. In cases where multiple scenarios are examined, only the top-performing
    outcome is mentioned.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | Based on | Speech recognition task | Transformer | AM/LM | Result
    with metric |'
  prefs: []
  type: TYPE_TB
- en: '| [[55](#bib.bib55)] | CNN | Solve the problem of code-switching | Multi-head
    attention | LM | RER= 10.2% |'
  prefs: []
  type: TYPE_TB
- en: '| [[56](#bib.bib56)] | VGGnet | Compress ASR parameters and speeds up the inference
    time | Low-rank multi-head attention | AM | CER= 13.09% |'
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] | DNN-HMM | Improve ASR | Attention | AM | RER= 4.7%$\downarrow$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[62](#bib.bib62)] | Emformer | Large scale [ASR](#Sx1.4.4.4) | Attention
    | AM | RERR= 26% |'
  prefs: []
  type: TYPE_TB
- en: '| [[63](#bib.bib63)] | TRUNet | Sound source separation | TNet | AM | PESQ=
    0.22$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64)] | [MHSA](#Sx1.29.29.29) | Improve speech/ASR | D²Net |
    AM | PESQ= 0.96$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[58](#bib.bib58)] | HMM | Improve ASR | Acoustic Encoder | AM | Acc= 96%
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[65](#bib.bib65)] | RNN-T | Acoustic re-scoring scenario | Transformer-
    Transducer | AM | Acc= 8%$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[66](#bib.bib66)] | CTC | ASR, ST, Acoustic [ED](#Sx1.17.17.17) | All-in-one
    | AM | WER=0.3%$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[67](#bib.bib67)] | CNN | Speech recognition with low latency, reduced frame
    rate, and streamability. | Transformer-Transducer | AM | WER= 3.6% |'
  prefs: []
  type: TYPE_TB
- en: '| [[68](#bib.bib68)] | CTC alignment | Retrieve the acoustic embedding at the
    token level for better ASR | Attention | AM | 51.2x RTF$\uparrow$ WER= 2.3% |'
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | RNN-LSTM | Improve the efficiency of end-to-end ASR
    | Attention | LM | CER=1.98% |'
  prefs: []
  type: TYPE_TB
- en: '| [[59](#bib.bib59)] | CTC Alignment | Enhance the performance of end-to-end
    [ASR](#Sx1.4.4.4) | Autoregressive Transformer | AM | RTF= 0.0134 WER= 2.7% |'
  prefs: []
  type: TYPE_TB
- en: 4.1.1 Acoustic domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The study [[57](#bib.bib57)] reveals the Transformer model’s increased susceptibility
    to input sparsity compared to the [convolutional neural network](#Sx1.9.9.9) ([CNN](#Sx1.9.9.9)).
    The authors analyze the performance decline, attributing it to the Transformer’s
    structural characteristics. Additionally, they introduce a novel regularization
    method to enhance the Transformer’s resilience to input sparsity. This method
    directly regulates attention weights through silence label information in forced-alignment,
    offering the advantage of not requiring extra module training and excessive computation.
  prefs: []
  type: TYPE_NORMAL
- en: The paper [[50](#bib.bib50)] addresses a limitation in Transformer-based end-to-end
    modeling for [ASR](#Sx1.4.4.4) tasks, where intermediate features from multiple
    input streams may lack diversity. The proposed solution introduces a multi-level
    acoustic feature extraction framework, incorporating shallow and deep streams
    to capture both traditional features for classification and speaker-invariant
    deep features for diversity. A [feature correlation-based fusion](#Sx1.18.18.18)
    ([FCF](#Sx1.18.18.18)) strategy, employed to combine intermediate features across
    both the frequency and time domains, correlates and combines these features before
    feeding them into the Transformer encoder-decoder module.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed masked autoencoding audio spectrogram Transformer (MAE-AST) operates
    solely on unmasked tokens [[51](#bib.bib51)], utilizing a large encoder. It concatenates
    mask tokens with encoder output embeddings, feeding them into a shallow decoder.
    Fine-tuning for downstream tasks involves using only the encoder, eliminating
    the decoder’s reconstruction layers. MAE-AST represents a significant improvement
    over the [self-supervised audio spectrogram transformer](#Sx1.46.46.46) ([SSAST](#Sx1.46.46.46))
    model for speech and audio classification. Addressing the high masking ratio issue,
    the method achieves a 3$\times$ speedup and 2$\times$ memory usage reduction.
    During downstream tasks, the approach consistently outperforms [SSAST](#Sx1.46.46.46).
    To identify varities of sounds types, Bai et al. [[52](#bib.bib52)] introduce
    SE-Trans, a cross-task model for environmental sound recognition, encompassing
    acoustic scene classification, urban sound tagging, and anomalous sound detection.
    Utilizing attention mechanisms and Transformer encoder modules, SE-Trans learns
    channel-wise relationships and temporal dependencies in acoustic features. The
    model incorporates FMix data augmentation, involving the creation of a binary
    mask from a randomly sampled complex matrix with a low-pass filter. SE-Trans achieves
    outstanding performance in ESR tasks, proven through evaluations on DCASE challenge
    databases, underscoring its robustness and versatility in environmental sound
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automated audio captioning (AAC) involves generating textual descriptions for
    audio recordings, covering sound events, acoustic scenes, and event relationships.
    Current AAC systems typically employ an encoder-decoder architecture, with the
    decoder crafting captions based on extracted audio features. Chen et al. in their
    paper [[53](#bib.bib53)] introduces a novel approach that enhances caption generation
    by leveraging multi-level information extracted from the audio clip. The proposed
    method consists of a CNN encoder with multi-level feature extraction (channel
    attention, spatial attention), A module specialized in predicting keywords to
    generate guidance information at the word level and Transformer decoder. Figure
    [6](#S4.F6 "Figure 6 ‣ 4.1.1 Acoustic domain ‣ 4.1 Transformer-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") depicts the overall architecture incorporating
    the three mentioned modules. Results demonstrate significant improvements in various
    metrics, achieving [SOTA](#Sx1.44.44.44) performance during the cross-entropy
    training stage.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0fa6f421045cf2a9b6f459effbd6fa0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An example of CNN-based transformer for automated audio captioning
    [[53](#bib.bib53)].'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial audio involves manipulating sound to deceive or compromise [machine
    learning](#Sx1.30.30.30) ([ML](#Sx1.30.30.30)) systems, exploiting vulnerabilities
    in audio recognition models. Both [[70](#bib.bib70), [71](#bib.bib71)] work are
    built to combat adversarial noise using transformers. The authors in [[70](#bib.bib70)]
    employed a vision transformer customized for audio signals to identify speech
    regions amidst challenging acoustic conditions. To enhance adaptability, they
    incorporated an augmentation module as an additional head in the transformer,
    integrating low-pass and band-pass filters. Experimental results reveal that the
    augmented vision architecture achieves an F1-score of up to 85.2% when using a
    low-pass filter, surpassing the baseline vision transformer, which attains an
    F1-score of up to 81.2%, in speech detection. However, in [[71](#bib.bib71)] the
    authors present an adversarial detection framework using an attention-based transformer
    mechanism to identify adversarial audio. Spectrogram features are segmented and
    integrated with positional information before input into the transformer encoder,
    achieving 96.5% accuracy under diverse conditions such as noisy environments,
    black-box attacks, and white-box attacks.
  prefs: []
  type: TYPE_NORMAL
- en: The paper [[72](#bib.bib72)] introduces a parallel-path transformer model to
    address computation cost challenges for speech separation tasks. Using improved
    feed-forward networks and transformer modules, it employs a parallel processing
    strategy with intra-chunk and inter-chunk transformers. This enables parallel
    local and global modeling of speech signals, enhancing overall system performance
    by capturing short and long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: A Hybrid [ASR](#Sx1.4.4.4) approach outlines the conceptualization and execution
    of a technique that integrates neural network methodologies into advanced continuous
    speech recognition systems. This integration is built upon [hidden Markov modelss](#Sx1.23.23.23)
    with the aim of enhancing their overall performance. Wang et al. [[73](#bib.bib73)]
    introduce and assess transformer-based [AMs](#Sx1.2.2.2) for hybrid speech recognition.
    The approach incorporates various positional embedding methods and an iterated
    loss for training deep transformers. Demonstrating superior performance on the
    Librispeech benchmark, the suggested transformer-based [AM](#Sx1.2.2.2) outperforms
    the best hybrid result by 19% to 26% relative with a standard n-gram [LM](#Sx1.24.24.24).
  prefs: []
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: 'TRUNet proposed in [[63](#bib.bib63)] represents an innovative approach to
    end-to-end multi-channel reverberant sound source separation. The model incorporates
    a recurrent-U network that directly estimates multi-channel filters from input
    spectra, enabling the exploitation of spatial and spectro-temporal diversity in
    sound source separation. In Figure [7](#S4.F7 "Figure 7 ‣ 4.1.1 Acoustic domain
    ‣ 4.1 Transformer-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey"), the block
    diagram illustrates the architecture of TRUNet’s transformer. The transformer
    network (TNet) encompasses three variations: (i) TNet–Cat, which concatenates
    multi-channel spectra, treating them as a single input. This approach allows for
    the direct utilization of spatial information between channels. (ii) TNet–RealImag,
    utilizing two separate transformer stacks for real and imaginary parts, respectively.
    Queries and keys are computed from the multi-channel spectra. Despite this, the
    method may not fully exploit spatial information, such as phase differences, directly.
    (iii) TNet–MagPhase, analogous to TNet–RealImag, but employing spectral magnitude
    and spectral phase instead of real and imaginary parts. This variation proves
    superior in extracting spatial information from complex-valued spectra, resulting
    in maximum enhanced performance in sound source separation when employing TRUNet-MagPhase
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c9d1a546ad646a23b07c65baa27d073e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example of source separation scheme based on Transformer [[63](#bib.bib63)].'
  prefs: []
  type: TYPE_NORMAL
- en: In recent times, dual-path networks have demonstrated effective results in many
    speech processing tasks such as speech separation and speech enhancement. In light
    of this, Wang Ke and colleagues incorporated the transformer into the structure
    of dual-path networks, presenting a time-domain speech enhancement model called
    the Two-stage Transformer-based Neural Network (TSTNN). This model significantly
    enhances the performance of speech enhancement [[74](#bib.bib74)]. Some research
    findings suggest that the dot-product self-attention may not be essential for
    transformer models. Similarly, The paper [[64](#bib.bib64)] introduces D²Net,
    a denoising and dereverberation network for challenging single-channel mixture
    speech in complex acoustic environments. D²Net incorporates a two-branch encoder
    (TBE) for feature extraction and fusion, along with a global-local dual-path transformer
    (GLDPT) featuring local dense synthesizer attention (LDSA) to enhance local information
    perception.
  prefs: []
  type: TYPE_NORMAL
- en: The study proposed by gong et al. [[75](#bib.bib75)] explores self-attention-based
    neural networks like the [audio spectrogram transformer](#Sx1.5.5.5) ([AST](#Sx1.5.5.5))
    for audio tasks. It introduces a self-supervised framework, improving [AST](#Sx1.5.5.5)
    performance by 60.9% on various speech classification tasks such as [ASR](#Sx1.4.4.4),
    speaker recognition, and more, reducing reliance on labeled data. The approach
    marks a pioneering effort in audio self-supervised learning. Likewise, a novel
    augmented memory self-attention addresses limitations of transformer-based acoustic
    modeling in streaming applications has been proposed in [[76](#bib.bib76)], outperforming
    existing streamable methods by over 15% in relative error reduction on benchmark
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Shareef et al. in [[58](#bib.bib58)] propose a collaborative training method
    for acoustic encoders in Arabic ASR systems for speech-impaired children, achieving
    a 10% relative accuracy improvement on phoneme alignment in the output sequence.
    Pioneering in recognizing impaired children’s Arabic speech. Similarly in [[77](#bib.bib77)],
    collaboratively training acoustic encoders of various sizes for on-device [ASR](#Sx1.4.4.4)
    improves efficiency and reduces redundancy. Using co-distillation with an auxiliary
    task, collaborative training achieves up to 11% relative [WER](#Sx1.52.52.52)
    improvement on LibriSpeech corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Transducer models, in the context of [ASR](#Sx1.4.4.4), map input sequences
    (acoustic features) to output sequences (transcriptions). Unlike traditional [S2S](#Sx1.39.39.39)
    models, transducers can handle variable-length input and output sequences more
    efficiently. The study [[65](#bib.bib65)] explores attention masking in transformer-transducer-based
    [ASR](#Sx1.4.4.4), comparing fixed masking with chunked masking in terms of accuracy
    and latency. The authors claim that variable masking is the viable choice in acoustic
    rescoring scenarios. Similarly, to adapt the Transformer for streaming ASR, the
    authors in [[67](#bib.bib67)] employ the Transducer framework for streamable alignments.
    Using a unidirectional Transformer with interleaved convolution layers for audio
    encoding, they model future context and gradually downsample input to reduce computation
    cost, while limiting history context length.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, The work [[66](#bib.bib66)] introduces an all-in-one [AM](#Sx1.2.2.2)
    based on the Transformer architecture, combined with the [CTC](#Sx1.11.11.11)
    to ensure a sequential arrangement and utilize timing details. It addresses [ASR](#Sx1.4.4.4),
    [audio tagging](#Sx1.6.6.6) ([AT](#Sx1.6.6.6)), and acoustic [ED](#Sx1.17.17.17)
    simultaneously. The model demonstrates superior performance, showcasing its suitability
    for comprehensive acoustic scene transcription.
  prefs: []
  type: TYPE_NORMAL
- en: Winata et al. [[56](#bib.bib56)] propose a memory-efficient Transformer architecture
    for end-to-end speech recognition. It significantly reduces parameters, boosting
    training speed by over 50% and inference time by 1.35$\times$ compared to baseline.
    Experiments show better generalization, lower error rates, and outperformance
    existing schemes without external language or acoustic models. Growing demand
    for on-device [ASR](#Sx1.4.4.4) systems prompts interest in model compression.compression.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Language domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Self-attention models, such as Transformers, excel in speech recognition and
    reveal an important pattern. As upper self-attention layers are replaced with
    feed-forward layers, resembling CLDNN architecture in [[48](#bib.bib48)], experiments
    on [wall street journal](#Sx1.53.53.53) ([WSJ](#Sx1.53.53.53)) and [switchboard](#Sx1.47.47.47)
    ([SWBD](#Sx1.47.47.47)) datasets show no performance drop and minor gains. The
    novel proposed metric of attention matrix diagonality indicates increased diagonality
    in lower to upper encoder self-attention layers. The authors conclude that a global
    view appears unnecessary for training upper encoder layers in speech recognition
    Transformers when lower layers capture sufficient contextual information. The
    study conducted by Hrinchuk et al. [[49](#bib.bib49)] presents a proficient postprocessing
    model for [ASR](#Sx1.4.4.4) with a Transformer-based encoder-decoder architecture,
    initialized with the weights of pre-trained BERT model [[36](#bib.bib36)]. The
    model effectively refines [ASR](#Sx1.4.4.4) output, demonstrating substantial
    performance gains through strategies like extensive data augmentation and pretrained
    weight initialization. On the LibriSpeech benchmark dataset, significant reductions
    in [WERs](#Sx1.52.52.52) are observed, particularly on noisier evaluation dataset
    portions, outperforming baseline models and approaching the performance of Transformer-XL
    neural language model re-scoring with 6-gram.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ea04dd52913074b01f1d7e0b8d76a12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Some end-to-end models: Basic CTC , RNN-Transducer, and attention
    architectures [[78](#bib.bib78)].'
  prefs: []
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: 'CTC is an architecture and principle commonly used in [S2S](#Sx1.39.39.39)
    tasks (Figure [8](#S4.F8 "Figure 8 ‣ 4.1.2 Language domain ‣ 4.1 Transformer-based
    ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey")), such as [ASR](#Sx1.4.4.4). It
    enables alignment-free training by introducing a blank symbol and allowing variable-length
    alignments between input and output sequences. During training, the model learns
    to align the input sequence with the target sequence, and the blank symbol accounts
    for multiple possible alignments. [CTC](#Sx1.11.11.11) is particularly effective
    in tasks with variable-length outputs, making it well-suited for applications
    like speech recognition where the duration of spoken words may vary. Figure Figure
    [8](#S4.F8 "Figure 8 ‣ 4.1.2 Language domain ‣ 4.1 Transformer-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") (a) illustrates the operational principles and
    components of [CTC](#Sx1.11.11.11). This latter has been used in many [ASR](#Sx1.4.4.4)
    schemes, for example, Deng et al. [[54](#bib.bib54)] presents the innovative pretrained
    Transformer [S2S](#Sx1.39.39.39) [ASR](#Sx1.4.4.4) architecture, which integrates
    self-supervised pretraining techniques for comprehensive end-to-end [ASR](#Sx1.4.4.4).
    Employing a hybrid [CTC](#Sx1.11.11.11)/attention model, it maximizes the potential
    of pretrained [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24). The inclusion of a [CTC](#Sx1.11.11.11)
    branch aids in the encoder’s convergence during training and considers all potential
    time boundaries in beam searching. The encoder is initiated with wav2vec2.0, and
    the introduction of a one-cross decoder (OCD) mitigates reliance on acoustic representations,
    enabling initialization with pretrained DistilGPT2 and overcoming the constraint
    of conditioning on acoustic features.'
  prefs: []
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: CS takes place when a speaker switches between words of two or more languages
    within a single sentence or across sentences. Zhou et al. [[55](#bib.bib55)] introduces
    a multi-encoder-decoder Transformer, for [code-switching](#Sx1.10.10.10) ([CS](#Sx1.10.10.10))
    problem. It employs language-specific encoders and attention mechanisms to enhance
    acoustic representations, pre-trained on monolingual data to address limited [CS](#Sx1.10.10.10)
    training data. Hadwan et al. research [[69](#bib.bib69)] employ an attention-based
    encoder-decoder transformer, to enhance end-to-end [ASR](#Sx1.4.4.4) for the Arabic
    language, focusing on Qur’an recitation. The proposed model incorporates a multi-head
    attention mechanism and Mel filter bank for feature extraction. For constructing
    a [LM](#Sx1.24.24.24), [recurrent neural network](#Sx1.36.36.36) ([RNN](#Sx1.36.36.36))
    and [long short term memory](#Sx1.26.26.26) ([LSTM](#Sx1.26.26.26)) techniques
    were employed to train an n-gram word-based [LM](#Sx1.24.24.24). The study introduces
    a new dataset, yielding [SOTA](#Sx1.44.44.44) results with a low character error
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 TL-based ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overall, DTL consists of training a DL model on a specific domain (or task)
    and then transferring the acquired knowledge to a new, similar domain (or task).
    In what follows, we present some of the definitions that are essential to understand
    the principle of DTL for [ASR](#Sx1.4.4.4) applications.
  prefs: []
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: 'DTL refers to a [DL](#Sx1.14.14.14) paradigm where knowledge gained from pre-training
    a model (Source model) on one domain or task is leveraged to enhance performance
    of target model, on a different but related domain or task. In this context, a
    "domain" refers to a specific data distribution, while a "task" represents a learning
    objective. \AcDA in [DTL](#Sx1.16.16.16) involves adapting a model trained on
    a $D_{S}$ to perform well on a target domain. This is crucial when there are differences
    in data distributions between the two domains. Fine-tuning is a technique where
    a pre-trained model is further trained on task-specific data to improve its performance
    on a related task. Cross-domain learning extends transfer learning to scenarios
    where the source and target domains are distinct. Zero-shot learning involves
    training a model to recognize classes not present in the training data. Transductive
    [DTL](#Sx1.16.16.16) focuses on adapting a model based on a specific set of target
    instances. Inductive [DTL](#Sx1.16.16.16) aims to generalize knowledge across
    domains by training a model to handle diverse tasks and domains simultaneously
    [[36](#bib.bib36), [79](#bib.bib79), [80](#bib.bib80)]. These techniques contribute
    to the versatility and adaptability of [DL](#Sx1.14.14.14) models in various applications.
    Fig. [9](#S4.F9 "Figure 9 ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    depicted the principle of [DTL](#Sx1.16.16.16) techniques. Table [6](#S4.T6 "Table
    6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    summarises the most recent DTL-based [ASR](#Sx1.4.4.4) techniques used in [AM](#Sx1.2.2.2)
    and [LM](#Sx1.24.24.24) domains.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/62e0398eeaf05a91872fe5b5b52f9571.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Deep transfer learning principle.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Acoustic domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Schneider et al. [[81](#bib.bib81)] explored unsupervised pre-training for speech
    recognition using wav2vec model on large unlabeled audio data. The learned representations
    enhanced [AM](#Sx1.2.2.2) training with a simple [CNN](#Sx1.9.9.9) optimized through
    noise contrastive binary classification. In [[82](#bib.bib82)], a source filter
    warping data augmentation strategy is proposed to enhance the robustness of children’s
    speech [ASR](#Sx1.4.4.4). The authors constructed an end-to-end acoustic model
    using the XLS-R wav2vec 2.0 model, pre-trained in a self-supervised manner on
    extensive cross-lingual corpora of adult speech. The work proposed in [[83](#bib.bib83)]
    introduces a multi-dialect acoustic model employing soft-parameter-sharing multi-task
    learning, a transductive DTL subcategory, within the Transformer architecture.
    Auxiliary cross-attentions aid dialect ID recognition, providing dialect information.
    Adaptive cross-entropy loss automatically balances multi-task learning. Experimental
    results demonstrate a significant reduction in error rates compared to various
    single- and multi-task models on multi-dialect speech recognition and dialect
    ID recognition tasks. Similarly, in the realm of computer vision, [CNNs](#Sx1.9.9.9)
    models like ConvNeXt have outperformed cutting-edge transformers, partly due to
    the integration of depthwise separable convolutions (DSC). DSC, which approximates
    regular convolutions, enhances the efficiency of [CNNs](#Sx1.9.9.9) in terms of
    time and memory usage without compromising accuracy—in some cases, even enhancing
    it. The study [[84](#bib.bib84)] introduces DSC into the pre-trained audio model
    family for audio classification on AudioSet (target task), demonstrating its advantages
    in balancing accuracy and model size. Xin et al. [[85](#bib.bib85)] introduce
    an audio pyramid Transformer with an attention tree structure, with four branches,
    to reduce computational complexity in fine-grained audio spectrogram processing.
    It proposes a [DA](#Sx1.13.13.13) transfer learning approach for weakly supervised
    sound event detection, a sub-field of [ASR](#Sx1.4.4.4), enhancing localization
    performance by aligning feature distributions between frame and clip domains with
    a [DA](#Sx1.13.13.13) detection loss.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Language domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The methodology is founded on the utilization of the BERT model [[86](#bib.bib86)],
    which involves pretraining language models and demonstrates improved performance
    across various downstream tasks. DTL approaches for language models, specifically
    employed in the domain of voice recognition, are referred to as [LM](#Sx1.24.24.24)
    adaptation. These approaches aim to bridge the gap between the source distribution
    $\mathbb{D}_{S}$ and the target distribution $\mathbb{D}_{T}$. Song et al. [[87](#bib.bib87)]
    present a novel learning-to-rescore (L2RS) approach, which relies on two main
    components: (i) utilizing diverse textual data from [SOTA](#Sx1.44.44.44) NLP
    models, such as BERT, and (ii) automatically determining their weights to rescore
    the N-best lists for [ASR](#Sx1.4.4.4) systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent advancements in [S2S](#Sx1.39.39.39) models have shown promising results
    for training monolingual [ASR](#Sx1.4.4.4) systems. The [CTC](#Sx1.11.11.11) and
    encoder-decoder models are two popular architectures for end-to-end [ASR](#Sx1.4.4.4).
    Additionally, joint training of these architectures in a multi-task hybrid approach
    has been explored, demonstrating improved overall performance. For instance, the
    architecture illustrated in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.2 Language domain
    ‣ 4.1 Transformer-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") (a) comprises
    [S2S](#Sx1.39.39.39) layers. The encoder network of [S2S](#Sx1.39.39.39) consists
    of a series of [RNNs](#Sx1.36.36.36) that generate embedding vectors, while the
    [RNN](#Sx1.36.36.36) decoder utilizes these vectors to produce final results.
    The [RNN](#Sx1.36.36.36) also benefits from prior predictions ($P_{i},i=0,\dots,n$),
    enhancing the accuracy of subsequent predictions. Moving on, a novel TL-based
    approach that enhances end-to-end speech recognition has been proposed in [[88](#bib.bib88)].
    The novelty lies in applying [DTL](#Sx1.16.16.16) through multilingual training
    and multi-task learning at two levels. The initial stage utilizes nonnegative
    matrix factorization (NMF), instead of a bottleneck layer, and multilingual training
    for high-level feature extraction. The subsequent stage employs joint CTC-attention
    models on these features, where the CTC was transferred to the target attention-based
    model. The scheme demonstrated superior performance on TIMIT but requires testing
    on high-resource data. Further optimization is needed for standard end-to-end
    training. In addition, Integrating both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24)
    methodologies has the potential to enhance or construct an effective DTL-based
    ASR model, as demonstrated in [[46](#bib.bib46), [89](#bib.bib89), [90](#bib.bib90),
    [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: In contemporary cutting-edge frameworks, diverse pre-trained models
    are utilized for distinct tasks within the field. These frameworks employ different
    DTL approaches and assess their efficacy using specific metrics. The symbol ($\uparrow$)
    result increase, whereas ($\downarrow$) signifies result decrease. In cases where
    multiple scenarios are examined, only the top-performing outcome is mentioned.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheme | Based on | Speech recognition task ($\mathbb{T}_{T}$) | AM/LM |
    Adaptation | Result with metric |'
  prefs: []
  type: TYPE_TB
- en: '| [[84](#bib.bib84)] | ConvNeXt-Tiny | Audio classification | AM | DA | [mAP](#Sx1.27.27.27)=
    0.471 |'
  prefs: []
  type: TYPE_TB
- en: '| [[85](#bib.bib85)] | [APT](#Sx1.3.3.3) | Sound event detection | AM | DA
    | F1= 79.6% |'
  prefs: []
  type: TYPE_TB
- en: '| [[49](#bib.bib49)] | BERT (Jasper) | Speech-to-text | LM | TL | WER= 14%
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[54](#bib.bib54)] | DistilGPT2 | Improve ASR | Both | Fine-tuning | CER=
    4.6% |'
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | XLRS Wave2vec | Improve ASR in low resource language
    | Both | Fine-tuning | 5.6% WER $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[82](#bib.bib82)] | XLRS Wave2vec | Improve ASR for children’s speech |
    AM | TL | WER = 4.86% |'
  prefs: []
  type: TYPE_TB
- en: '| [[90](#bib.bib90)] | [S2S](#Sx1.39.39.39) | Speaker adaptation | Both | Features
    norm. | 25.0% WER$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[83](#bib.bib83)] | Transformer | Multi-dialect model aids recognizing diverse
    speech dialects effectively | AM | Multi-task learning | Acc= 100% |'
  prefs: []
  type: TYPE_TB
- en: '| [[94](#bib.bib94)] | [S2S](#Sx1.39.39.39) | Enhancing the existing multilingual
    [S2S](#Sx1.39.39.39) model. | LM | DTL | 4%CER $\downarrow$ 6% WER |'
  prefs: []
  type: TYPE_TB
- en: '| [[95](#bib.bib95)] | PaSST | Audio tagging and event detection | AM | Fine-tuning
    | F1= 64.85% |'
  prefs: []
  type: TYPE_TB
- en: '| [[81](#bib.bib81)] | Wav2vec | WSJ data speech | AM | affine transform. |
    36% WER$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[96](#bib.bib96)] | ARoBERT | Spoken language understanding | LM | Fine-tuning
    | F1-score=92.56% |'
  prefs: []
  type: TYPE_TB
- en: 'Abbreviations: Transformer (T)'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 FL-based ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: 'FL revolutionizes AI model training by enabling collaboration without the need
    to share sensitive training data. Traditional centralized approaches are evolving
    towards decentralized models, where [ML](#Sx1.30.30.30) algorithms are trained
    collaboratively on edge devices like mobile phones, laptops, or private servers
    [[97](#bib.bib97)]. The mathematical formulation of FL focuses on training a single
    global model across multiple devices or nodes (clients) while keeping the data
    localized. The objective is to minimize a global loss function that is typically
    the weighted sum of the local loss functions on all clients. The standard [FL](#Sx1.20.20.20)
    problem can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}F(\theta)=\min_{\theta}\sum_{k=1}^{K}\frac{n_{k}}{N}F_{k}(\theta)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: In this context, $\theta$ denotes the parameters of the global model to be learned,
    $K$ represents the total number of clients, $n_{k}$ signifies the number of data
    samples at client $k$, $N=\sum_{k=1}^{K}n_{k}$ stands for the total number of
    data samples across all clients, and $F_{k}(\theta)$ indicates the local loss
    function computed on the data of client $k$. In [FL](#Sx1.20.20.20), the goal
    is to find the global model parameters $\theta$ that minimize the global loss
    function $F(\theta)$, which is an aggregate of the local loss functions from all
    participating clients. This process typically involves iterative updates to the
    model parameters using algorithms like federated averaging (FedAvg), where clients
    compute gradients or updates based on their local data and send these updates
    to a central server. The server then aggregates these updates to improve the global
    model.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horizontal federated fearning (HFL): In HFL, clients train a shared global
    model using their respective datasets, characterized by the same feature space
    but different sample spaces. Each client utilizes a local AI model, and their
    updates are aggregated by a central server without exposing raw data. The HFL
    training process involves: (1) initialization, (2) local training, (3) encryption
    of gradients, (4) secure aggregation, and (5) global model parameter updates.
    The objective function minimizes a global loss across all parties’ datasets [[97](#bib.bib97)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vertical federated learning (VFL): VFL trains models on datasets sharing the
    same sample space but having different feature spaces. Through entity data alignment
    (EDA) and encrypted model trained (EMT), VFL allows clients to cooperatively train
    models without sharing raw data. The training process involves the same steps
    as HFL [[97](#bib.bib97)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d782211be5f99014cc3b67a0f5c17de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Federated learning principle: (a) Horizontal FL, (b) Vertical FL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'FL presents a paradigm shift in AI training, promoting collaboration while
    respecting data privacy. The working principle of HFL, and VFL is depicted on
    Figure [10](#S4.F10 "Figure 10 ‣ 4.3 FL-based ASR ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey"), they cater to various data distribution scenarios, offering flexible
    solutions for decentralized and secure [ML](#Sx1.30.30.30). The application of
    these FL frameworks extends across diverse domains, promising improved model accuracy
    and privacy preservation. The first work introducing [FL](#Sx1.20.20.20) in [ASR](#Sx1.4.4.4)
    is presented in [[98](#bib.bib98)]. The authors introduced a FL platform that
    is easily generalizable, incorporating hierarchical optimization and a gradient
    selection algorithm to enhance training time and SR performance. Guliani et al.
    [[99](#bib.bib99)] proposed a strategy to compensate non-independent and identically
    distributed (non-IID) data in federated training of [ASR](#Sx1.4.4.4) systems.
    The proposed strategy involved random client data sampling, which resulted in
    a cost-quality trade-off. Zhu et al. [[100](#bib.bib100)] addressed also FL-based
    [ASR](#Sx1.4.4.4) in non-IID scenarios with personalized [FL](#Sx1.20.20.20).
    They introduced two approaches: adapting personalization layer-based FL for [ASR](#Sx1.4.4.4),
    involving local layers for personalized models, and proposing decoupled federated
    learning (DecoupleFL). DecoupleFL reduces computation on clients by shifting the
    computation burden to the server. Additionally, it communicates secure high-level
    features instead of model parameters, reducing communication costs, particularly
    for large models. In [[101](#bib.bib101)], the authors proposed a client-adaptive
    federated training scheme to mitigate data heterogeneity when training [ASR](#Sx1.4.4.4)
    models. Nguyen et al. [[102](#bib.bib102)] used FL to train an [ASR](#Sx1.4.4.4)
    model based on a wav2vec 2.0 model pre-trained by self supervision. Yang et al.
    [[103](#bib.bib103)] proposed a decentralized feature extraction approach in federated
    learning. This approach is built upon a quantum CNN (QCNN) composed of a quantum
    circuit encoder for feature extraction, and an RNN based end-to-end [AM](#Sx1.2.2.2).
    This framework takes advantage of the quantum learning progress to secure models
    and to avoid privacy leakage attacks. Gao et al. [[104](#bib.bib104)] tackled
    a challenging and realistic [ASR](#Sx1.4.4.4) federated experimental setup with
    clients having heterogeneous data distributions, featuring thousands of different
    speakers, acoustic environments, and noises. Their empirical study focused on
    attention-based [S2S](#Sx1.39.39.39) End-to-End (E2E) [ASR](#Sx1.4.4.4) models,
    evaluating three aggregation weighting strategies: standard FedAvg, loss-based
    aggregation, and a novel WER-based aggregation. Table [7](#S4.T7 "Table 7 ‣ 4.3
    FL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition
    using Advanced Deep Learning Approaches: A survey") summarizes the most recent
    FL-based [ASR](#Sx1.4.4.4) techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Summary of recent proposed work in FL-based [ASR](#Sx1.4.4.4). All
    the schemes are suggested for [AM](#Sx1.2.2.2). The symbol ($\uparrow$) result
    increase, whereas ($\downarrow$) signifies result decrease. In cases where multiple
    scenarios are examined, only the top-performing outcome is mentioned.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheme | Based on | Speech recognition task | FL technique | Metric and result
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[98](#bib.bib98)] | [S2S](#Sx1.39.39.39) | Improve ASR | FedAvg | WER =
    6% |'
  prefs: []
  type: TYPE_TB
- en: '| [[99](#bib.bib99)] | End-to-end RNN-T | ASR on non-IDD data | FedAvg | WER=
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[100](#bib.bib100)] | CNN+Transformer extractor | ASR on non-IDD data |
    DecoupleFL | 2.3- 3.4% WER $\downarrow$ compared with FedAvg |'
  prefs: []
  type: TYPE_TB
- en: '| [[101](#bib.bib101)] | LSTM | ASR on non-IDD data | [CAFT](#Sx1.7.7.7) |
    WER = 15.13% |'
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bib102)] | wav2vec 2.0 | Improve ASR | FedAvg | WER= 10.92% EER=
    5-20% |'
  prefs: []
  type: TYPE_TB
- en: '| [[103](#bib.bib103)] | QCNN and RNN | Improve privacy-preservation in ASR
    | VFL | Accuracy = 95.12% |'
  prefs: []
  type: TYPE_TB
- en: '| [[104](#bib.bib104)] | [S2S](#Sx1.39.39.39) | ASR on heterogeneous data distributions
    | FedAvg | WER= 19.98-23.86% |'
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | [S2S](#Sx1.39.39.39) | ASR on private and unlabelled
    user data. | [FedNST](#Sx1.19.19.19) | WER= 22.5% |'
  prefs: []
  type: TYPE_TB
- en: '| [[106](#bib.bib106)] | Wav2vec 2.0 and Whisper | ASR & KWS for child exploitation
    settings | FedAvg | WER = 11-25% |'
  prefs: []
  type: TYPE_TB
- en: '| [[107](#bib.bib107)] | Kaldi and backoff n-gram | Improve privacy-preservation
    in ASR | Merging models | WER= 17.7% |'
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bib108)] | TDNN | Improve privacy-preservation in ASR | Aggregation
    | EER = 1-2%. |'
  prefs: []
  type: TYPE_TB
- en: '| [[109](#bib.bib109)] | Non-Streaming & Streaming Conformer | Reduce client
    ASR model size | Federated Dropout | 6-22% $\downarrow$ Client size model; 34-3%
    WER $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: 4.4 RL-based ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: 'RL is a [ML](#Sx1.30.30.30) paradigm where an agent learns optimal decision-making
    by interacting with an environment. The agent receives feedback in the form of
    rewards or penalties, adapting its behavior to maximize cumulative reward over
    time through a trial-and-error process. [reinforcement learning](#Sx1.35.35.35)
    ([RL](#Sx1.35.35.35)) involves several key concepts, as defined in the following
    terms: Environment model, serving as a representation of contextual dynamics;
    State (s), denoting the current situation perceived by the agent; Observation
    (o), a subset of the state directly perceived by the agent; Action (a), the decision
    made by the agent in response to the environment; Policy ($\pi$), describing how
    the agent converts environmental conditions into actions; Agent, the entity making
    decisions based on current states and past experiences; Reward, numerical values
    assigned by the environment to the agent based on state-action interactions. Figure
    [11](#S4.F11 "Figure 11 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    illustrate the principle of [RL](#Sx1.35.35.35).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d950f54e56c9c73a22f77d9dbedbac92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: RL principle.'
  prefs: []
  type: TYPE_NORMAL
- en: Markov Decision Process (MDP) is a fundamental framework for dynamic and stochastic
    decision-making, characterized by state space $S$, action space $\mathbb{A}$,
    transition probabilities $\mathbb{P}$, and a reward function $R$. The primary
    objective in an MDP is to identify an optimal policy $\pi^{*}$ maximizing the
    expected discounted total reward over time, expressed as $\pi^{*}=\max_{\pi}\mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}r_{t}(s_{t},\pi(s_{t}))]$,
    where $\gamma$ is the discount factor. MDPs find extensive applications in addressing
    uncertainties in intelligent systems within dynamic wireless environments, including
    spectrum management, cognitive radios, and wireless security.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the field of [ASR](#Sx1.4.4.4), [RL](#Sx1.35.35.35) has primarily been proposed
    to tackle discrepancies between training and testing phases. Two main discrepancies
    leading to potential performance deterioration have been identified: 1) The conventional
    use of the cross-entropy criterion maximizes log-likelihood during training, while
    performance is assessed by [WER](#Sx1.52.52.52), not log-likelihood; 2) The teacher-forcing
    method, which relies on ground truth during training, implies that the model has
    never encountered its own predictions before testing. [RL](#Sx1.35.35.35) addresses
    these discrepancies by bridging the gap between the training and testing phases.
    Several [RL](#Sx1.35.35.35)-based approaches for [ASR](#Sx1.4.4.4) have been proposed
    [[110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113),
    [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119)]. For example, in [[110](#bib.bib110)],
    the authors introduced a [RL](#Sx1.35.35.35)-based optimization method for the
    [S2S](#Sx1.39.39.39)  [ASR](#Sx1.4.4.4) task called Self-critical sequence training
    (SCST). This method can be conceptualized as a sequential decision model, depicted
    in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods
    and applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey"). The entire encoder-decoder neural network is treated as an agent.
    At each time step $t$, the current state $s_{t}$ is formed by concatenating the
    acoustic feature $x_{t}$ and the previous prediction $Y_{t-1}$. The output token
    serves as the action, updating the generated hypotheses sequence. SCST associates
    training loss and [WER](#Sx1.52.52.52) using a WER-related reward function, calculating
    the reward $r_{t}$ at each token generation step by comparing it with the ground
    truth sequence $Y^{*}$. SCST uses the test-time beam search algorithm to sample
    hypotheses for reward normalization, assigning positive weights to high-reward
    hypotheses that outperform the current test-time system and negative weights to
    low-reward hypotheses. The framework is illustrated in Figure [12](#S4.F12 "Figure
    12 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [ASR](#Sx1.4.4.4), RL has been primarily proposed to address mismatches
    between training and testing phases. Two main mismatches leading to potential
    performance degradation are identified: 1) The commonly used cross-entropy criterion
    maximizes log-likelihood during training, while performance is evaluated by WER,
    not log-likelihood; 2) The teacher-forcing method, relying on ground truth during
    training, implies that the model has never encountered its own predictions before
    testing. RL bridges the gap between the training and testing phases, addressing
    these mismatches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Several RL-based [ASR](#Sx1.4.4.4) approaches have proposed [[110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118),
    [119](#bib.bib119)]. For instance, In [[110](#bib.bib110)], the authors presented
    a RL-based optimization method for [S2S](#Sx1.39.39.39)  [ASR](#Sx1.4.4.4) task
    called self-critical sequence training (SCST). This can be viewed as a sequential
    decision model as shown in Fig. [12](#S4.F12 "Figure 12 ‣ 4.4 RL-based ASR ‣ 4
    Advanced ASR methods and applications ‣ Automatic Speech Recognition using Advanced
    Deep Learning Approaches: A survey"). The whole encoder-decoder neural network
    can be viewed as an agent. In each time step $t$, acoustic feature $x_{t}$ and
    previous $Y_{t-1}$ prediction are concatenated as current state $s_{t}$. The output
    token is the action at that will update the generated hypotheses sequence. After
    comparing it with ground truth sequence $Y^{*}$, a reward $r_{t}$ of this time
    step is calculated. SCST associates the training loss and WER using WER-related
    reward function, which considers the intermediate reward at each token generation
    step. Furthermore, SCST utilizes the test-time beam search algorithm to sample
    a set of hypotheses for reward normalization. As a result, the high-reward hypotheses
    that outperform the current test-time system are given positive weights, while
    the low-reward hypotheses are given negative weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire encoder-decoder neural network is treated as an agent. At each time
    step $t$, the current state $s_{t}$ is formed by concatenating the acoustic feature
    $x_{t}$ and the previous prediction $Y_{t-1}$. The output token serves as the
    action, updating the generated hypotheses sequence. SCST associates training loss
    and WER using a WER-related reward function, calculating the reward $r_{t}$ at
    each token generation step by comparing it with the ground truth sequence $Y^{*}$.
    SCST uses the test-time beam search algorithm to sample hypotheses for reward
    normalization, assigning positive weights to high-reward hypotheses that outperform
    the current test-time system and negative weights to low-reward hypotheses. The
    framework is illustrated in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 RL-based ASR
    ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4af5b569eede20f04fbced21f432978.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: SCST sequential decision model of [ASR](#Sx1.4.4.4) [[110](#bib.bib110)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[111](#bib.bib111)], the authors developed a [RL](#Sx1.35.35.35) framework
    for speech recognition systems using the policy gradient method. They introduced
    a [RL](#Sx1.35.35.35) method within this framework, incorporating user feedback
    through hypothesis selection. Tjandra et al. [[112](#bib.bib112), [113](#bib.bib113)]
    also employed policy gradient [RL](#Sx1.35.35.35) to train a [S2S](#Sx1.39.39.39)  [ASR](#Sx1.4.4.4)
    model. In [[114](#bib.bib114)], the authors constructed a generic [RL](#Sx1.35.35.35)-based
    AutoML system. This system automatically optimizes per-layer compression ratios
    for a [SOTA](#Sx1.44.44.44) attention-based end-to-end [ASR](#Sx1.4.4.4) model,
    which consists of multiple [LSTM](#Sx1.26.26.26) layers. The compression method
    employed in this work is singular value decomposition (SVD) low-rank matrix factorization.
    The authors improved this approach by combining iterative compression with AutoML-based
    rank searching, achieving over 5 x [ASR](#Sx1.4.4.4) compression without degrading
    the WER [[115](#bib.bib115)]. Shen et al. [[116](#bib.bib116)] suggested employing
    [RL](#Sx1.35.35.35) to optimize a speech enhancement model based on recognition
    results, aiming to directly enhance [ASR](#Sx1.4.4.4) performance. AutoML-based
    low-rank factorization (LRF) achieves up to 3.7× speedup. In the shade of this,
    Mehrotra et al. in their work [[115](#bib.bib115)] propose an iterative AutoML-based
    LRF that employs [RL](#Sx1.35.35.35) for the iterative search, surpassing 5× compression
    without degrading [WERs](#Sx1.52.52.52), advancing [ASR](#Sx1.4.4.4). Table [8](#S4.T8
    "Table 8 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") summarizes
    the recent RL-based [ASR](#Sx1.4.4.4) techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Summary of recent proposed works in RL-based ASR. The symbol ($\uparrow$)
    result increase, whereas ($\downarrow$) signifies result decrease. In cases where
    multiple scenarios are examined, only the top-performing outcome is mentioned.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheme | Model-based | ASR Tasks | RL technique | Metric and result |'
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | [S2S](#Sx1.39.39.39) Conformer | Improve ASR | Policy
    gradient | 8.7%-7.8% WER $\downarrow$ over Baseline model |'
  prefs: []
  type: TYPE_TB
- en: '| [[111](#bib.bib111)] | DNN-HMM | Improve ASR for AM | Policy gradient | WER=23.82-25.43%
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[112](#bib.bib112)] | [S2S](#Sx1.39.39.39) | Improve ASR FOR am | Policy
    gradient | CER=6.10% |'
  prefs: []
  type: TYPE_TB
- en: '| [[113](#bib.bib113)] | [S2S](#Sx1.39.39.39) | Improve ASR for AM | Policy
    gradient | CER=6.10% |'
  prefs: []
  type: TYPE_TB
- en: '| [[114](#bib.bib114)] | End-to-end encoder-attention- decoder | Improve ASR
    Model compression for AM | Policy gradient | Up to $\sim$3x compression; WER=8.06%
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[115](#bib.bib115)] | End-to-end encoder-attention- decoder | Improve ASR
    Model compression for AM | Policy gradient | Up to $\sim$5x compression; WER=8.19%
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[116](#bib.bib116)] | CD-DNN-HMM AM & SRI LM | Speech enhancement for ASR
    for AM and LM | Q-learning | 12.40% and 19.23% CER $\downarrow$ at 5 and 0 dB
    SNR conditions. |'
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bib117)] | LSTM | Improve ASR for dialogue state tracking | DQN
    | Acc= 3.1%$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[118](#bib.bib118)] | [S2S](#Sx1.39.39.39) | Improve ASR for AM | Policy
    gradient | CER=8.7% |'
  prefs: []
  type: TYPE_TB
- en: '| [[119](#bib.bib119)] | Wav2vec 2.0 | Improve ASR for AM | Policy gradient
    | 4% WER $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: DTL for acoustic models (AMs) Two types of deep learning-based AMs commonly
    used in ASR are the end-to-end model and the layered deep neural network-hidden
    Markov model (DNN-HMM) model. The DNN component is responsible for extracting
    high-level features, such as MFCCs and HMM lexical sequences, from acoustic signals.
    These features are then decoded into transcripts. The DNN takes acoustic characteristics
    as input and produces context-dependent lexical units (tri-phonemes) that are
    used as input for the downstream HMM component.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the end-to-end model is a DNN-based technique that directly takes
    acoustic characteristics (features) as input and outputs the recognition rate
    without the need for separate components like the DNN and HMM. A typical end-to-end
    framework for voice recognition is illustrated in Figure [13](#S4.F13 "Figure
    13 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey"). The neural network
    in this framework generates embeddings from the input features, which are then
    passed through a stack of recurrent layers. These recurrent layers analyze patterns
    based on prior and current input information to produce the final output. The
    network is trained using back-propagation with the CTC loss [mridha2021study].
    Regarding domain transfer learning (DTL), three main strategies have been commonly
    employed with AMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b3f26de3a4f0aeb8fb3b23f3112a2872.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: An example of end-to-end source model for DTL-based ASR [mridha2021study].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Feature normalisation based-DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The concept behind linear transformation is to normalize speech characteristics
    through a linear mapping process. This involves adding a transformation network
    or layer to an existing network. The transformation network is an effective method
    for adapting neural networks. Typically, the last hidden layer is designed to
    act as a bottleneck, reducing the number of parameters to be adjusted by using
    fewer neurons. Figure [14](#S4.F14 "Figure 14 ‣ 4.4.1 Feature normalisation based-DTL
    ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey") illustrates this
    architecture. The transformation network can be either a linear input network
    or a linear output network.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e80ab26db2766ad3b7256f598ae2db8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The basics of transformation in DTL involve estimating the weights
    connected to the links within the dashed rectangles, while leaving the remaining
    weights unchanged. In the case of mono-task DTL, feature normalization is applied
    as shown in Figure (a). For multi-task DTL, feature normalization is also performed,
    as illustrated in Figure (b).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that the last hidden network (LHN) functions as a feature extractor
    and the output layer serves as a discriminative source model ($M_{S}$), the weights
    of the linear transformation matrix $W_{L}$ in the output layer correspond to
    the parameters of the target model, $M_{T}$. The representation of $M_{T}$ can
    be expressed as follows [huang2015maximum]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{M_{T}=softmax(W_{L}M_{S})}.$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'The activation of the last hidden layer in $M_{S}$ can serve as the extracted
    feature representation for the hidden layers in $M_{T}$. Transforming the model
    parameters using a transformation matrix $W_{LHN}$ to generate an adapted set
    of model parameters is equivalent to incorporating an extended last hidden layer
    after the existing last hidden layer [huang2015maximum]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{M_{T}=softmax(W_{LHN}W_{L}M_{S})}.$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'Several ASR research studies have utilized linear transformation strategies
    to improve performance. In one example [elaraby2016deep], the authors aimed to
    enhance a computer-aided language learner (CAPL) system for teaching Arabic pronunciation
    for Quran recitation regulations. They implemented various improvements, including
    speaker adaptive training (SAT), integrating a hybrid DNN-HMM model, combining
    the hybrid DNN with minimum phone error (MPE), and using a grammar-based decoding
    graph during testing. Another study [mimura2016joint] employed a multi-target
    learning approach to optimize the output of a denoising auto-encoder (DAE) and
    the input of a DNN. The DAE was trained in the first stage to reduce input-related
    errors propagated to the DNN. Then, a unified network of DAE and DNN was fine-tuned
    for phone state ASR, with an additional target of input voice augmentation applied
    to the DAE. Additionally, in [ma2017approaches], an adaptation layer was introduced
    for fine-tuning, and non-linearity was incorporated to learn a more complex function
    than a linear transformation in the softmax layer. During fine-tuning, adjustments
    were made to the cluster softmax and NNadapt layers while keeping the other layers
    unchanged. Table [6](#S4.T6 "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR
    ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey") provides a summary of additional
    schemes that employ linear transformation techniques, along with their performance
    details.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Conservative training for DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The conservative training approach is commonly used for accent adaptation in
    ASR. It is an efficient method that requires only a limited amount of spoken data
    to achieve satisfactory results. However, it can lead to an excessive number of
    parameters, which may disrupt the model’s structure. To address this, KL-Divergence
    (KLD) is widely employed as a DNN-based DTL algorithm for ASR. KLD regularization
    provides a mathematical framework for DL training, aiming to minimize the loss
    and make the output distributions of the source model $M_{S}$ and target model
    $M_{T}$ more similar. The KL-divergence prevents overfitting and helps to keep
    the adapted $M_{T}$ close to the domain of $M_{S}$  [[89](#bib.bib89), [90](#bib.bib90)].
    In terms of model-based DTL with KLD-regularization, assuming the loss functions
    for training DNNs in the source and target domains are $\mathbb{D}_{S}$ and $\mathbb{D}_{T}$,
    respectively, the conservative approach can be summarized mathematically as follows
    [[89](#bib.bib89), [90](#bib.bib90)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{D}_{T}^{KLD}=(1-\rho)\mathbb{D}_{S}+\frac{\rho}{N}\sum P_{S}(x_{T}/x_{T})logP_{T}(y_{T}/x_{T}),$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'In the context of DTL-based ASR, the conservative training approach with KLD-regularization
    is utilized to build a target AM. This approach involves training DNNs on speech
    samples collected from the target domain $\mathbb{D}_{T}$, denoted as ($x_{T},y_{T}$),
    where $N$ represents the number of speech samples in $\mathbb{D}_{T}$. The hyper-parameter
    $\rho$ controls the transfer ratio from the source domain $\mathbb{D}_{S}$. An
    example of applying this DTL-based ASR approach with conservative training and
    KLD-regularization can be found in [[89](#bib.bib89)] for building a target AM.
    Additionally, in [[90](#bib.bib90)], both KLD and LHN techniques were employed
    for speaker adaptation using a pre-trained seq2seq ASR model. Table [6](#S4.T6
    "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey") summarizes other techniques that employ conservative training along
    with their corresponding performance details.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Subspace-based DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Subspace-based DTL approaches in ASR utilize unsupervised techniques such as
    PCA, SVD, and NMF for data dimensionality reduction. These techniques aim to identify
    a subspace of model parameters or transformations that capture essential information.
    PCA maps high-dimensional data to lower-dimensional subspaces while preserving
    correlation and maximizing variance. SVD, similar to PCA, condenses networks by
    selecting a specific number of singular values. However, SVD introduces nonlinearity
    in the weight matrix, which can lead to information loss when constructing a linear
    projection layer. NMF algorithms require at least one nonnegative matrix and express
    the target matrix as a weighted sum of base matrix columns, making it more interpretable
    than SVD and PCA. In the study [[88](#bib.bib88)], Convex Nonnegative Matrix Factorization
    (CNMF), a variant of NMF, is employed to extract high-level features. Subsequently,
    DTL is applied to both high and low-level features through multilingual training
    and multi-task learning. These techniques yield significant performance improvements
    compared to state-of-the-art ASR studies. Other studies utilizing subspace techniques
    and their respective performance details can be found in Table [6](#S4.T6 "Table
    6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: LDA-based DTL In the case of discrete data collection, generative probabilistic
    models, such as the latent Dirichlet allocation (LDA), are employed. Typically,
    LDA is a hierarchical Bayesian model with three levels, where each item in a collection
    is represented as a finite mixture over a set of underlying topics. Each topic,
    in turn, is modeled as an infinite mixture of topic probabilities. To capture
    the relationship between words and create language models for specific documents,
    topic model-based techniques, such as LDA, have been utilized as described in
    [song2019topic]. In the work presented in [hentschel2018feature], LDA features
    are transformed using a linear layer consisting of a weight matrix and a bias
    vector. These transformed features are then utilized as inputs in the LHN (Local
    Hidden Node) during the training and evaluation of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 NNLM-based DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Neural network language models (NN-LMs) generally outperform count-based LM
    models in automatic speech recognition (ASR) across various tasks. Specifically,
    when applied to N-best rescoring, NN-LMs achieve WER as mentioned in [hentschel2018feature].
    Adapting NN-LMs to new domains poses a research challenge, and current approaches
    can be categorized as either model-based or feature-based adaptations. In feature-based
    adaptation, auxiliary features are incorporated into the input of an NN-LM, while
    model-based adaptation involves fine-tuning and adapting network layers. The authors
    of [[91](#bib.bib91)] propose a recurrent neural network-based LM (RNN-LM) approach
    where both types of adaptation are explored. As an illustrative example, Figures
    [15](#S4.F15 "Figure 15 ‣ 4.4.4 NNLM-based DTL ‣ 4.4 RL-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") (a) and (b) provide detailed explanations of the
    adopted RNN-based DTL in the study. Furthermore, the authors in [[92](#bib.bib92)]
    suggest a DNN-based model for LM modification in ASR, employing a factorized time-delay
    neural network (TDNN-F). Specifically, the TDNN-F model is trained using a combination
    of cross-entropy and lattice-free maximum mutual information objective functions
    (LF-MMI). The effectiveness of TDNN-F is demonstrated in the recognition of English
    child speech [[92](#bib.bib92)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2c2171b391d75043b013d096ab808962.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: DTL-based RNNLM with different adaptation techniques, where the
    out-of-vocabulary (OOV) node represents an input word that does not belong to
    the specified vocabulary but can be included in the input. Similarly, out-of-shortlist
    (OOS) nodes can also be included in the output: (a) RNNLM with LHN adaptation
    layer. (b) RNNLM with feature-based adaptation layer.'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM-based DTL Generally, the NNLM models used in ASR are still trained on a
    sentence-level corpus, despite the attempts to train them at the document level.
    This is due to various factors; for example, a more extended context may not be
    relevant for enhancing next-word prediction in conventional ASR systems. It is
    also challenging to gather training data representing extended session contexts
    in many conversational circumstances. Long-span models are becoming more common
    in scenarios where they are beneficial. Long-span models will likely help scenarios,
    such as transcriptions of conversations and meetings, human-to-human communication,
    and document production by voice [parthasarathy2019long]. LSTM models are widely
    employed, and their architectures are well-suited to variable-length sequences.
    Therefore, they can exploit extreme long-range dependencies without using n-gram
    approximation. For instance, by employing equal context, the authors in [tuske2018investigation]
    demonstrate that the deep 4-gram LSTM outperforms big interpolated count models
    by performing considerably better backing off and smoothing. In another example,
    the central part of a shared encoder is constructed using BLSTM [[94](#bib.bib94)].
  prefs: []
  type: TYPE_NORMAL
- en: Cross-domain ASR
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.5 Cross-language DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cross-language DTL is a particular application of cross-modality DTL. It serves
    as a commonly employed method for constructing ASR models for low resource languages
    by leveraging models trained on other languages. This approach is based on the
    assumption that phoneme features can be shared across languages. Additionally,
    a generic ASR model can be adapted to a specific narrow domain using DTL techniques.
    To address the issue of data sparsity, various knowledge transfer methods are
    explored in [liu2019investigation] with the assistance of high-resource languages.
    These methods include DTL and fine-tuning, where a well-trained neural network
    initializes the parameters of the LHN. Progressive neural networks (Prognets)
    are also examined, as they are resistant to the forgetting effect and excel at
    knowledge transfer due to the presence of lateral connections in the network architecture.
    Furthermore, the utilization of cross-lingual DNNs is explored, where bottleneck
    features are extracted to enhance the effectiveness of the ASR system. Recent
    approaches related to ASR using cross-language DTL and their respective performances
    are summarized in Tables [6](#S4.T6 "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based
    ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey") and [9](#S4.T9 "Table 9 ‣ 4.4.5
    Cross-language DTL ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: A summary of the recent ASR-based cross-language DTL technique. Whereas
    the marks ($\uparrow$) and ($\downarrow$) indicate improvement and reduction,
    respectively. If many scenarios has been conducted in one metric, only the best
    result is mentioned.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheme | Model-based | ASR Tasks ($\mathbb{T}_{T}$) | Characteristic | Performance
    |'
  prefs: []
  type: TYPE_TB
- en: '| [yusuf2019low] | EDML | Framework of performing the DTL that reduces the
    impact of the prevalence of out-of vocabulary terms. | query-by-example task |
    74% TWV$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [liu2019investigation] | Prognets | Improving ASR scheme quality by overcoming
    the data sparsity problems by means of high-resource languages. | Fine-tuning
    LHN adapt. | 38.6% WER$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [feng2019low] | FNN and CNN architectures | Indo-European speech samples
    used to improve the identification of African languages. | Using PLP coeff. Fine-tuning
    | 2.1% EER$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [sahraeian2018cross] | DNN | Weighted averaging schemes are used to combine
    the ensemble’s constituents, with the combination weights being trained to minimize
    the cross-entropy objective function. | Weights interpolation | 7.7% WER$\downarrow$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [wilkinson2020semi] | CNN-GMM-HMM | Fully-automatic segmentation, semi-supervised
    training of ASR systems for five-lingual code-switched speech | Semi-supervised
    | 1.1 % WER$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '4.4.6 DTL-based ASR for emotion recognition:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Combining language information with acoustic data has been shown to enhance
    the accuracy of speech emotion recognition (SER). Therefore, integrating both
    systems can be advantageous to enhance the capability of ASR systems to handle
    emotional speech and provide linguistic input to SER systems. Figure [16](#S4.F16
    "Figure 16 ‣ 4.4.6 DTL-based ASR for emotion recognition: ‣ 4.4 RL-based ASR ‣
    4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using Advanced
    Deep Learning Approaches: A survey") illustrates a hybrid ASR-SER system [fayek2016deep],
    where a spectrogram is inputted into shared convolutional layers, followed by
    specialized layers that have shared levels to facilitate interaction between the
    two systems. This integration allows for improved performance in handling emotional
    speech and leveraging linguistic features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/be0a4f56dcdad101d578426c2822fbf3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: A linguistic-paralinguistic hybrid system [fayek2016deep]. (a) ASR-SER
    Hybrid system. (b) An example of DTL-based ASR-SER hybrid system.'
  prefs: []
  type: TYPE_NORMAL
- en: The approach presented in [tits2018asr] utilizes the internal representation
    of a speech-to-text system to explore the connection between valence/arousal and
    different modalities through the use of DTL. A speech-to-text or ASR system learns
    to map audio speech signals to their corresponding transcriptions. By employing
    DTL, the proposed method can estimate valence and arousal using features learned
    from an ASR task. This approach offers the advantage of combining large datasets
    of speech with transcriptions with smaller datasets annotated with emotional dimensions.
    In a similar vein, the work described in [ananthram2020multi] fine-tunes a TDNN-based
    speaker recognition model for the task of emotion detection using the crema-D
    multi-modal emotion dataset and canonical label clustering. By adapting the model
    using fine-tuning, the authors aim to improve its performance on emotion detection.
    The study presented in [boateng2020speech] focuses on extracting features from
    audio segments with extreme positive and negative ratings, as well as the ending
    of the audio. They employ the peak-end rule and a DTL approach to extract acoustic
    features. The authors utilize a pre-trained CNN speech model called YAMNet and
    a linear SVM for binary classification of partner valence.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.7 Cross-corpus SER (CC-SER)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the context of Speech Emotion Recognition (SER), it is typically assumed
    that speech utterances in the training and testing domains are recorded under
    the same conditions. However, in real-world scenarios, speech data is often collected
    from diverse environments or devices, resulting in a domain discrepancy that adversely
    affects recognition performance. As a solution, researchers have recently investigated
    the problem of cross-corpus SER (CC-SER) and explored various DTL models.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [song2019transfer] proposes a transfer linear subspace learning
    (TLSL) scheme to develop a CC-SER framework. This approach facilitates the learning
    of shared feature space for both the source and target domains. The similarity
    between different corpora is estimated using a nearest-neighbor graph algorithm.
    Additionally, a feature grouping method is devised to partition emotional features
    into highly transferable parts (HTP) and low transferable parts (LTP).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of unsupervised CC-SER examined in [liu2018unsupervised], only the
    training data is annotated. The authors introduce a domain-adaptive subspace learning
    (DoSL) technique to learn a projection matrix that transforms the source and target
    speech data from the initial domain to the labeled domain. This allows the classifier
    trained on the labeled source domain data to effectively predict the emotional
    states of the unlabeled target domain data. Furthermore, [liu2021transfer] presents
    an improvement to the DoSL-based CC-SER method by introducing transfer subspace
    learning (TRaSL). In another study by [luo2019cross], a semi-supervised CC-SER
    approach is proposed using non-negative matrix factorization (NMF). This approach
    incorporates training corpus labels into NMF and seeks a latent low-rank feature
    space where the differences in conditional and marginal distributions between
    the two corpora can be simultaneously minimized.
  prefs: []
  type: TYPE_NORMAL
- en: Advancing further, [zhang2019transfer] proposes a transfer sparse discriminant
    subspace learning (TSDSL) method to discover a shared feature subspace among multiple
    corpora. This is achieved by incorporating the $\ell_{2,1}$-norm penalty and discriminative
    learning, facilitating the identification of the most discriminative characteristics
    across the corpora. Similarly, [luo2020nonnegative] introduces a non-negative
    matrix factorization-based transfer subspace learning (NMFTSL) scheme. The goal
    is to minimize the distances between the marginal and conditional distributions
    in the common subspace. The distances are estimated using the maximum mean discrepancy
    (MMD) criterion. In [zhang2021cross], a joint transfer subspace learning and regression
    (JTSLR) technique is employed. It learns a latent subspace using discriminative
    MMD as the discrepancy metric, followed by modeling the relationships between
    features and annotations using a regression function in the latent subspace. A
    label graph is utilized to enhance knowledge transfer from the source domain (SD)
    data to the target domain (TD) data. Similarly, [chen2019target] presents a target-adapted
    subspace learning (TaSL) approach for CC-SER. It aims to find a projection subspace
    that enables more accurate regression of labels from the features. This effectively
    bridges the gap in feature distributions between the TD and SD. The projection
    matrix is optimized by combining $\ell_{1}$-norm and $\ell_{2,1}$-norm penalty
    terms with other regularization terms. Furthermore, [zhao2021cross] employs sparse
    subspace transfer learning (SSTL) to develop a CC-SER technique. It learns a robust
    common subspace projection using discriminative subspace learning and transfers
    knowledge from the source corpus to the target corpus through sparse reconstruction
    based on $\ell_{2,1}$-norm. The target samples are suitably represented as linear
    combinations of the SD data. On a different note, [braunschweiler2021study] investigates
    the impact of cross-corpus data complementation and data augmentation on the performance
    of SER models. The study focuses on six emotional speech corpora, considering
    factors such as single and multiple speakers, as well as variations in emotion
    style (natural, elicited, and acted).
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Adversarial TL-based ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In many cases, the source model is trained using multilingual training, where
    abundant speech data is available in multiple languages [[88](#bib.bib88), [94](#bib.bib94)].
    Multilingual training involves shared hidden layers (SHL) and language-specific
    layers or classifier layers for different languages. The SHL of the source model
    acts as a feature converter, transforming language-specific features into a common
    feature space [liu2019investigation]. However, there might be language-dependent
    features present in the common feature space, which hinders effective cross-lingual
    knowledge transfer. To address this issue, language-adversarial training is employed.
    Adversarial training helps in creating a language-invariant feature space. Once
    the source model is prepared, the first $n$ SHL can be transferred to the target
    model of an unknown language. In [yi2018language], the authors propose language-adversarial
    transfer learning as a solution to mitigate the performance degradation of the
    target model caused by shared features that may contain unnecessary language-dependent
    information. Fig. [17](#S4.F17 "Figure 17 ‣ 4.5 Adversarial TL-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") illustrates the architecture of the suggested
    language-adversarial transfer learning method [yi2018language]. The source model,
    also known as the adversarial SHL model, is shown on the left, while the target
    model is depicted on the right. The presence of an additional language discriminator
    in the SHL model is denoted. The fully connected layer is represented as FC. The
    gradient reversal layer (GRL) ensures that the feature distributions across all
    languages are made as similar as possible for the language discriminator. The
    output labels of the language discriminator indicate the languages.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f77e3e1bf8deb2575dfea1a07de292ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: An example of proposed model architecture language-adversarial TL
    for limited ASR resource [yi2018language]. Senones refers to feature cluster’s
    name, representing similar acoustic states/events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enhance the effectiveness of ASR in low-resource scenarios, a combination
    of semi-supervised training and language adversarial TL is explored in [kumar2021exploration].
    The research presented in [yi2020adversarial] suggests utilizing adversarial transfer
    learning to improve punctuation prediction performance. Specifically, a pre-trained
    BERT model is employed to transfer bidirectional representations to punctuation
    prediction models. The proposed approach is applied to the ASR task as the target
    task. Table [10](#S4.T10 "Table 10 ‣ 4.5 Adversarial TL-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") provides a summary of the performance achieved
    in recent studies involving adversarial TL for ASR.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: A summary of the recent ASR-based adversarial-language TL technique.
    Whereas the marks ($\uparrow$) and ($\downarrow$) indicate improvement and reduction,
    respectively. If many scenarios has been conducted in one metric, only the best
    result is mentioned.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheme | Model-based | ASR Tasks ($\mathbb{T}_{T}$) | Characteristic | Performance
    |'
  prefs: []
  type: TYPE_TB
- en: '| [kumar2021exploration] | Adversarial SHL-Mode ( SincNet-CNN-LiGRU ) | Use
    three Indian languages ( Hindi, Marathi, and Bengali ) cross-lingual to improve
    Hindi ASR | Semi-supervised | WER=5.5% (25.65% WER $\downarrow$) |'
  prefs: []
  type: TYPE_TB
- en: '| [yi2018language] | SHL Model | Improve the performance of low-resource ASR
    | Cross-lingual | 10.1 % WER $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [yi2020adversarial] | BERT | Improve the performance of punctuation predicting
    | Multi-task | 9.4% F1-score $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: 4.6 DTL-based ASR for medical diagnosis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DTL-based ASR has made significant advancements in the field of medicine, particularly
    in the early detection of diseases. These advancements have been observed in various
    medical domains, as outlined in Table [11](#S4.T11 "Table 11 ‣ 4.6.3 Other medical
    diagnosis ‣ 4.6 DTL-based ASR for medical diagnosis ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey"), which includes:'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1 Heart sound classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The components of the heart sound encompass various elements. The first (S1)
    and second (S2) heart sounds are considered normal, whereas the third (S3) and
    fourth (S4) heart sounds are often associated with murmurs, and ejection clicks
    typically indicate certain illnesses or abnormalities. Koike et al. [koike2020audio]
    introduced a novel DTL approach based on Probabilistic Audio Neural Networks (PANNs),
    which involves pre-training a model on a large-scale audio dataset for the purpose
    of classifying heart sounds. Another approach for heart sound classification was
    proposed by Boulares et al. [boulares2020transfer]. Their method utilizes DTL
    on the Pascal public dataset, without any denoising or cleaning procedures, to
    establish an experimental benchmark. The primary objective is to provide a foundation
    of experimental results that can serve as a starting point for future research
    on cardiovascular disease (CVD) recognition using phonocardiogram (PCG)-based
    cardiac cycle vibration sounds. This proposed scheme addresses the absence of
    a CVD recognition benchmark and the lack of objective comparability among classification
    results, which tend to vary significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2 Parkinson disease detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Parkinson’s disease (PD) is a progressive neurodegenerative condition with a
    global impact. Accurate diagnosis of PD is crucial for improving daily activities
    and extending patients’ lives. However, predicting symptom changes and their impact
    on patients’ lives is challenging due to the variability in symptoms and disease
    progression. Traditional PD detection methods are often manual and require specialized
    expertise. Language impairment and atrophy are common early symptoms in over 90%
    of PD patients, characterized by reduced voice volume, monotonous and rapid speech,
    and eventual loss of audibility. Karaman et al. [[43](#bib.bib43)] proposed a
    robust automated PD detection approach using DTL-based ASR, where pre-trained
    models like SqueezeNet1_1, ResNet101, and DenseNet161 were fine-tuned and retrained.
    This scheme showed promising results in PD detection. To address the scarcity
    of speech data for PD and the distribution inconsistency among subjects, Li et
    al. [[42](#bib.bib42)] introduced a two-step unsupervised DTL algorithm called
    two-step sparse transfer learning (TSTL). This algorithm helps extract useful
    information from large amounts of unlabeled speech data, align the distribution
    of training and test sets, and preserve the original sample structure simultaneously.
    Another strategy proposed by Qing et al. [[45](#bib.bib45)] involves enhancing
    ASR for Parkinson’s patients using a pre-trained long short-term memory (LSTM)
    neural network model. To mitigate overfitting and reduce WER, the scheme employs
    frequency spectrogram masking data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.3 Other medical diagnosis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the field of speech-based depression prediction, Harati et al. [harati2021speech]
    proposed a DTL method that utilizes a lightweight encoder and transfers only the
    encoder weights. This approach simplifies the runtime model for depression prediction.
    For individuals with dysarthria, Xiong et al. [xiong2020source] developed an improved
    DTL framework for robust personalized speech recognition models. They adapted
    the CNN-TDNN-F ASR AM onto the target dysarthric speakers using neural network
    weight adaptation. In the context of dysarthria speaking identification, Takashima
    et al. [takashima2019knowledge] proposed a method that transfers two types of
    knowledge from different datasets: the language-dependent characteristics of unimpaired
    speech and the language-independent characteristics of dysarthric speech. They
    focused on Japanese people with articulation disorders. Additionally, Sertolli
    et al. [sertolli2021representation] presented a novel feature representation for
    health states identification using an end-to-end DTL-based ASR framework. They
    utilized ASR DNNs as feature extractors, combined multiple feature representations
    using compact bilinear pooling (CBP), and employed an optimized RNN classifier
    for inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: A summary of a DTL-based ASR technique in medical diagnosis, whereas
    the marks ($\uparrow$) and ($\downarrow$) indicate the improvement and reduction,
    respectively. If many scenarios have been conducted in one metric, only the best
    result is mentioned.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheme | Model-based | ASR Tasks ($\mathbb{T}_{T}$) $\uparrow$ | DTL Type
    | Performance |'
  prefs: []
  type: TYPE_TB
- en: '| [[43](#bib.bib43)] | DenseNet-161 | PD detection | Fine-tuning | Accuracy=
    91.17% |'
  prefs: []
  type: TYPE_TB
- en: '| [[42](#bib.bib42)] | TSTL-based CSC&SF | PD speech diagnosis | Unsupervised
    | Accuracy= 97.50% |'
  prefs: []
  type: TYPE_TB
- en: '| [[45](#bib.bib45)] | Proposed four layers | PD speech | Fine-tuning | 13.5%
    WER$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [koike2020audio] | PANN CNN14 | Heart sound classification | Fine-tuning
    | UAR= 89.7% |'
  prefs: []
  type: TYPE_TB
- en: '| [boulares2020transfer] | InceptionResNet-v2 | PCG-based CVD classification
    | Fine-tuning | Accuracy= 0.89% |'
  prefs: []
  type: TYPE_TB
- en: '| [harati2021speech] | EH-AC | Depression prediction | LHN (encoder weights)
    | 27% AUC $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [xiong2020source] | CNN-TDNN-F | Dysarthric speech | Neural weight adapter
    | 11.6% WER$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [takashima2019knowledge] | LAS | Dysarthric Speech | Multilingual | 45.9%
    PER$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [sertolli2021representation] | Wav2Letter and DeepSpeech | Health states
    classification | Transductive | UAR= 73.0% (8.6% $\uparrow$) |'
  prefs: []
  type: TYPE_TB
- en: 4.7 DTL-based ASR attacks and security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Adversarial examples are created by introducing small perturbations or noise
    to valid audio files or speech characteristics in order to manipulate or deceive
    ASR systems. These perturbations, although often imperceptible to the human auditory
    system and may be perceived as background noise by the ASR model, can lead to
    correct or incorrect classification of the inputs. For instance, by introducing
    a slight disturbance to the speech "At the still point, there the dance is," an
    ASR system may produce the incorrect transcription "At the tail point, there the
    tense is" [hu2019adversarial]. The concept behind attacking ASR systems lies in
    the vulnerability of these models to adversarial examples, which has prompted
    speech researchers to explore the creation of such examples. By generating adversarial
    examples for different representations of speech in the time or frequency domain,
    which capture various speech features and can be used as inputs to neural networks,
    researchers can manipulate ASR performance, either improving it or decreasing
    it. DTL plays a key role in achieving transferability, whereby adversarial examples
    crafted to attack a source model can also be effective in attacking target models
    that classify the same type of data. In this context, adversarial attacks on DTL-based
    ASR models can be categorized into two groups, as illustrated in Fig. [18](#S4.F18
    "Figure 18 ‣ 4.7 DTL-based ASR attacks and security ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b2528a932381c3f998b4b99c917d263.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Possible adversarial attacks in DTL-based ASR schemes.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.1 Positive adversarial attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Encompassing various techniques that aim to enhance or ensure the efficacy of
    current ASR systems, these methods strive to improve the robustness and performance
    of acoustic models. In [sun2018training], the authors introduced a methodology
    that combines natural data with adversarial data to train a resilient acoustic
    model. Specifically, they focused on utilizing MFCC features and employed a gradient-based
    approach to generate adversarial MFCC features, taking into account the network
    model and input parameters for each mini-batch. By applying the teacher/student
    training concept, the neural network was trained using a combination of natural
    data and the generated adversarial data. The effectiveness of the proposed approach
    was validated through experiments conducted on the CHiME-4 and Aurora-4 tasks,
    utilizing a customized convolutional neural network (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.2 Negative adversarial attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Encompassing various techniques that aim to diminish or undermine the effectiveness
    of existing ASR systems, these methods pose potential threats to the performance
    and robustness of acoustic models. In [abdullah2021hear], the authors propose
    a framework that utilizes the Google (Phone) model as the source model and investigates
    the impact of adversarial attacks on the target model (DeepSpeech 1). The adversarial
    attacks are applied to the audio waveform, specifically after the signal decomposition
    and thresholding processes, and the resulting manipulated input is fed into the
    ASR source model. According to [hu2019adversarial], the adversarial attack models
    can be categorized into two types based on the adversary’s objectives, knowledge,
    and background, as illustrated in Figure [18](#S4.F18 "Figure 18 ‣ 4.7 DTL-based
    ASR attacks and security ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey"):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adversary knowledge: It is divided into two types: white-box attack, which
    assumes the adversary has complete knowledge of $M_{T}$ including its architecture,
    training weights, and parameters; and black-box attack, which assumes the adversary
    has no access to $M_{T}$ and only knows its output like a regular user.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adversarial specificity: It is divided into two types: non-targeted attack,
    which aims to make the adversarial example’s $M_{T}$ predict any incorrect class
    with the sole goal of compromising the ASR algorithm, and targeted attack, which
    aims to deceive $M_{T}$ into assigning the adversarial example to a specific class
    selected by the attacker, imposing specific instructions on the ASR scheme.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Moving on, Table [12](#S4.T12 "Table 12 ‣ 4.7.2 Negative adversarial attacks
    ‣ 4.7 DTL-based ASR attacks and security ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    shows a summary of DTL-based adversarial models for existing works.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: A summary of the recent DTL-based ASR for adversarial attacks. Whereas
    the marks ($\uparrow$)and ($\downarrow$), indicate improvement and reduction respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scheme | Model-based | ASR Tasks ($\mathbb{T}_{T}$) | Target object | Adversary
    knowledge | Adversarial specificity | Performance |'
  prefs: []
  type: TYPE_TB
- en: '| [sun2018training] | Aurora-4 | Boost | MFCC | White-box | Targeted | 23%
    WER $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [schonherr2018adversarial] | DNN-HMM (Kaldi) | Boost | Waveform | White-box
    | Targeted | Accuracy= 98 % |'
  prefs: []
  type: TYPE_TB
- en: '| [zelasko2021adversarial] | DeepSpeech | Fool | Waveform | White-box | Targeted
    | 4-5% WER$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| [subramanian2020study] | VGG13 | Fool (Dense_mel) | Mel-spectrogram | White-box
    | Non-targeted | SNR=29.06 dB |'
  prefs: []
  type: TYPE_TB
- en: '| [carlini2018audio] | DeepSpeech | Fool (Speech-to-Text) | Waveform | White-box
    | Targeted | Attack success rate= 100% |'
  prefs: []
  type: TYPE_TB
- en: '| [abdullah2021hear] | Google (Phone) | Fool (Deep-Speech 1) | Waveform | Black-box
    | Non-targeted | Attack success rate=87% |'
  prefs: []
  type: TYPE_TB
- en: '| [kwon2019selective] | DeepSpeech | Fool (victime DeepSpeech) | Mel-frequency
    cepstrum | White-box | Targeted | Attack success rate=91.67%) |'
  prefs: []
  type: TYPE_TB
- en: 4.8 DTL-based ASR for other applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DTL-based ASR has been applied in various fields. In the work by Boes et al.
    [[12](#bib.bib12)], DTL is explored for audio tagging and sound event detection
    tasks, where pre-trained auditory and visual features are incorporated into a
    baseline system using feature fusion. Arora et al. [arora2017study] use DTL to
    address the lack of annotated databases for audio event detection. Wang et al.
    [wang2020cross] propose an environment adaptation technique using DTL for deep
    speech enhancement models. Chen et al. [chen2018transfer] employ DTL for wearable
    devices to evaluate social speech in natural daily situations. Wu et al. [wu2020self]
    investigate self-supervised pre-trained speech for speech translation, while Zhu
    et al. [zhu2021conwst] propose a self-supervised bidirectional distillation system
    for low-resource speech translation. DTL is used in the speaker verification field
    by Hong et al. [hong2017transfer] for discriminative learning, and in the detection
    of marine mammal sounds by Lu et al. [lu2021detection] to classify different species
    using DTL with the AlexNet pre-trained model. Lastly, Azizah et al. [azizah2020hierarchical]
    employ hierarchical DTL for multilingual text-to-speech synthesis using DNNs for
    low-resource languages.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Open Issues and of Key challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integrating advanced techniques like [DTL](#Sx1.16.16.16), [FL](#Sx1.20.20.20),
    and [RL](#Sx1.35.35.35) into [ASR](#Sx1.4.4.4) systems presents exciting opportunities
    but comes with its set of challenges. This section delves into the distinct challenges
    associated with each approach, emphasizing the critical areas that demand attention
    and innovation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 DTL and domain adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section discusses challenges and concepts related to [DTL](#Sx1.16.16.16)
    and [DA](#Sx1.13.13.13) in speech recognition, including distribution shift, feature
    space adaptation, label distribution shift, catastrophic forgetting, domain-invariant
    feature learning, sample selection bias, and hyperparameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: When applying a model trained on one domain (source) to another (target), a
    distribution shift often occurs, referred to as domain shift. Formally, if $P_{S}(X,Y)$
    and $P_{T}(X,Y)$ represent the joint distributions of features $X$ and labels
    $Y$ in the source and target domains, respectively, the challenge arises when
    $P_{S}(X,Y)\neq P_{T}(X,Y)$. To address this, techniques focus on learning a transformation
    of the feature space to minimize the difference between the source and target
    distributions. This involves finding a mapping function $f:X\rightarrow Z$, where
    $Z$ is a latent space in which the distributions of transformed features $f(X_{S})$
    and $f(X_{T})$ are more similar, quantified using measures such as the maximum
    mean discrepancy (MMD). Label distribution shift occurs when the distributions
    of labels ($P_{S}(Y)$ vs. $P_{T}(Y)$) differ, even if the feature distributions
    align. This poses challenges, especially with underrepresented classes in the
    target domain. Addressing this mathematically involves adjusting the model or
    learning process, possibly by re-weighting the loss function based on class distribution
    estimates. Catastrophic Forgetting is a risk during fine-tuning on a new domain,
    where the model may lose its performance on the original task. Balancing loss
    functions ($L_{S}$ for the source and $L_{T}$ for the target) is crucial, often
    weighted by a hyperparameter $\lambda$ to control their importance. Domain-invariant
    feature learning aims to learn features invariant across domains while remaining
    predictive. It involves optimizing a feature extractor $f$ and predictor $g$ to
    minimize the $D_{S}$ loss $L_{S}$ and the domain discrepancy (e.g., MMD). The
    problem of sample selection bias occurs in selecting samples for [DA](#Sx1.13.13.13),
    affecting the effectiveness of adaptation strategies. Mathematically, addressing
    this bias involves weighting or selecting samples to minimize it, often using
    importance sampling or re-weighting techniques. Hyperparameter optimization is
    critical in [DA](#Sx1.13.13.13), where the choice of hyperparameters (e.g., $\lambda$)
    significantly impacts performance. Finding the optimal hyperparameters typically
    involves solving complex optimization problems using techniques like grid search,
    random search, or Bayesian optimization on a validation set.
  prefs: []
  type: TYPE_NORMAL
- en: MMD^2[f(X_S), f(X_T)] = ∥1nS∑_i=1^n_Sϕ(x_S_i) - 1nT∑_j=1^n_Tϕ(x_T_j)∥^2
  prefs: []
  type: TYPE_NORMAL
- en: where $\phi(\cdot)$ represents the mapping to a reproducing kernel Hilbert space
    (RKHS), and $n_{S}$, $n_{T}$ are the numbers of samples in the source and target
    domains, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the unification of [DTL](#Sx1.16.16.16) in [ASR](#Sx1.4.4.4) poses
    a challenge due to the varied mathematical formulations used in different studies.
    While efforts have been made to unify definitions and formulations, further work
    is needed for a consistent understanding of [DTL](#Sx1.16.16.16). Speech-based
    [DTL](#Sx1.16.16.16) processing faces challenges compared to image-based processing
    due to potential mismatches between source and target databases arising from factors
    like language, speakers, age groups, ethnicity, and acoustic environments. The
    [CTC](#Sx1.11.11.11) approach, while promising, is limited by the assumption of
    frame independence. Cross-lingual [DTL](#Sx1.16.16.16) challenges include incorporating
    linguistic characteristics from multiple sources and integrating knowledge at
    different hierarchical levels, considering linguistic differences. Finally, computational
    burden remains a significant challenge in [DTL](#Sx1.16.16.16) and [DA](#Sx1.13.13.13)
    processes. Knowledge transfer between domains can incur additional computational
    costs, especially considering the extensive computational resources required for
    deep architectures inherent in [DTL](#Sx1.16.16.16) techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 FL-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FL has significant potential for ASR systems, particularly in enhancing privacy
    and personalization. However, deploying this technology in ASR also comes with
    a set of challenges. Typically, in FL, data is inherently decentralized and can
    vary significantly across devices. This heterogeneity in speech data—due to differences
    in accents, dialects, languages, and background noise—can make it challenging
    to train a model that performs well across all nodes. Ensuring robustness and
    generalization of the ASR model under these conditions is a complex task. Moving
    on, FL requires periodic communication between the central server and the devices
    to update the model. For ASR systems, where models can be quite large, this can
    result in substantial communication overhead. Optimizing the efficiency of these
    updates, in terms of both bandwidth usage and energy consumption, especially on
    mobile devices, is a significant challenge. Besides, although FL is designed to
    enhance privacy by not sharing raw data, there are still privacy challenges. For
    instance, it’s possible to infer sensitive information from model updates. Ensuring
    that these updates do not leak private information about the users’ speech data
    is a critical concern that requires sophisticated privacy-preserving techniques
    like differential privacy or secure multi-party computation.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, one of the advantages of FL is the ability to personalize models
    based on local data. However, balancing personalization with the need for a generally
    effective model—especially in a diverse ecosystem with varying speech patterns—is
    challenging. Achieving this balance without compromising the model’s overall performance
    or the personalization benefits is a key challenge. Moving forward, FL systems
    need to manage potentially thousands or millions of devices participating in the
    training process. Scalability issues, including managing updates from such a large
    and potentially unreliable network of devices, ensuring consistent model improvements,
    and handling devices joining or leaving the network, are significant technical
    hurdles. Lastly, in FL, the distribution of data across devices is often non-identically
    distributed (non-IID). This means that the speech data on one device might be
    very different from that on another, leading to challenges in training a model
    that generalizes well across all devices. Overcoming the bias introduced by non-IID
    data is a major challenge in [FL](#Sx1.20.20.20) for [ASR](#Sx1.4.4.4).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 RL-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using RL in [ASR](#Sx1.4.4.4) systems offers promising avenues for improvement
    but also presents several challenges. Specifically, one of the primary challenges
    in applying [RL](#Sx1.35.35.35) to [ASR](#Sx1.4.4.4) is the issue of sparse and
    delayed rewards. In many [ASR](#Sx1.4.4.4) tasks, the system only receives feedback
    (rewards or penalties) after processing lengthy sequences of speech, making it
    difficult to attribute the reward to specific actions or decisions. This delay
    complicates the learning process, as the model struggles to identify which actions
    led to successful outcomes. Moreover, balancing exploration, trying new actions
    to discover their effects, with exploitation, using known actions that yield the
    best results, is a critical challenge in RL. In the context of [ASR](#Sx1.4.4.4),
    this means the system must balance between adhering to known speech patterns and
    exploring new patterns or interpretations. Overemphasis on exploration can lead
    to inaccurate transcriptions, while excessive exploitation may prevent the model
    from adapting to new speakers or accents. Additionally, RL models typically require
    a significant amount of interaction data to learn effectively. In [ASR](#Sx1.4.4.4),
    obtaining large volumes of labeled speech data, especially with user feedback,
    can be challenging and expensive. Additionally, RL algorithms can be sample-inefficient,
    meaning they need a lot of data before they start performing well, which can be
    a bottleneck in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, most [ASR](#Sx1.4.4.4) systems are built using supervised learning
    techniques that rely on vast amounts of annotated data. Integrating RL into these
    systems poses technical challenges, as it requires a different training paradigm
    that focuses on learning from user interactions and feedback rather than static
    datasets. Besides, using RL in [ASR](#Sx1.4.4.4) often involves collecting and
    analyzing user feedback and interactions to improve the model. This raises concerns
    about user privacy and data security, as sensitive information might be inadvertently
    captured and used for training. Ensuring that data is handled securely and in
    compliance with privacy regulations is a significant challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Designing an appropriate reward function that accurately reflects the desired
    outcomes in [ASR](#Sx1.4.4.4) is challenging. The reward function must capture
    the nuances of speech recognition, such as accuracy, naturalness, and user satisfaction,
    which can be difficult to quantify. Poorly designed reward functions can lead
    to suboptimal learning outcomes or unintended behaviors. Lestly, [ASR](#Sx1.4.4.4)
    systems are used in a wide range of environments, from quiet offices to noisy
    streets. RL models need to adapt to these varying conditions, but training them
    to handle such diversity can be complex. The environment’s variability requires
    models that can generalize well across different acoustic conditions, which remains
    a challenge for RL-based [ASR](#Sx1.4.4.4) systems.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Personalized data augmentation for dysarthric and older people
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While ASR technologies have advanced significantly, especially in recognizing
    typical speech patterns, they still struggle to accurately identify speech from
    individuals with dysarthria or older adults [[120](#bib.bib120)]. Gathering extensive
    datasets from these groups is challenging due to mobility limitations often associated
    with these populations. In this context, personalized data augmentation plays
    a crucial role [[121](#bib.bib121), [122](#bib.bib122)]. Personalized data augmentation
    tailores the training process to accommodate the unique speech patterns and challenges
    associated with these groups. Dysarthria, a motor speech disorder, and the natural
    aging process can lead to speech that deviates from the normative models typically
    used to train ASR systems, making accurate recognition difficult [[123](#bib.bib123)].
    Personalized data augmentation introduces a wider range of speech variations into
    the training dataset, including those specific to dysarthric speakers or older
    adults. This can include variations in speech rate, pitch, articulation, and clarity.
    By training on this augmented dataset, the ASR system learns to recognize and
    accurately transcribe speech that exhibits these characteristics [[124](#bib.bib124)].
    Moreover, this helps the ASR models generalize better to unseen examples of speech
    from dysarthric speakers or older adults. This enhanced generalization is crucial
    for real-world applications where the system encounters a wide range of speech
    variations. Moving forward, personalized data augmentation can employ specific
    techniques tailored to the needs of dysarthric speakers or older adults, such
    as simulating the slurring of words, varying speech tempo, or introducing background
    noise [[125](#bib.bib125)], commonly challenging for these groups [[126](#bib.bib126)].
    Techniques like pitch perturbation, temporal stretching, and adding noise can
    simulate real-world conditions more accurately for these users. A typical example
    is presented in [[127](#bib.bib127)], where a unique approach utilizes speaker-dependent
    [generative adversarial networks](#Sx1.54.54.54) has been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Multitask learning for ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: MTL enhances the performance of ASR systems by leveraging the inherent relatedness
    of multiple learning tasks to improve the generalization of the primary [ASR](#Sx1.4.4.4)
    task. This approach allows the [ASR](#Sx1.4.4.4) model to learn shared representations
    that capture underlying patterns across different but related tasks, leading to
    several key benefits [[128](#bib.bib128)]. Typically, MTL encourages the [ASR](#Sx1.4.4.4)
    system to learn representations that are beneficial across multiple tasks. This
    can lead to more robust feature extraction, as the model is not optimized solely
    for transcribing speech but also for other related tasks, such as speaker identification
    or emotion recognition. This shared learning process helps in capturing a broader
    range of speech characteristics, which can improve the [ASR](#Sx1.4.4.4) system’s
    ability to handle varied speech inputs. Moving one, By simultaneously learning
    related tasks, MTL acts as a form of regularization, reducing the risk of overfitting
    on the primary [ASR](#Sx1.4.4.4) task. This is because the model must find a solution
    that performs well across all tasks, preventing it from relying too heavily on
    noise or idiosyncrasies specific to the training data of the main task. Besides,
    learning auxiliary tasks alongside the main [ASR](#Sx1.4.4.4) task can improve
    the model’s generalization capabilities. For example, learning to identify the
    speaker or the language can provide additional contextual clues that help the
    [ASR](#Sx1.4.4.4) system better understand and transcribe ambiguous audio signals.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, MTL can make more efficient use of available data by leveraging
    auxiliary tasks for which more data might be available. In scenarios where annotated
    data for [ASR](#Sx1.4.4.4) is limited, incorporating additional tasks with more
    abundant data can help improve the learning process and performance of the [ASR](#Sx1.4.4.4)
    system. Moreover, MTL allows [ASR](#Sx1.4.4.4) systems to better handle acoustic
    variability in speech, such as accents, dialects, or noisy environments, by incorporating
    tasks that directly or indirectly encourage the model to learn features that are
    invariant to these variations. Last but not least, modern [ASR](#Sx1.4.4.4) systems
    often employ [DL](#Sx1.14.14.14) architectures that can benefit from end-to-end
    learning strategies. MTL fits naturally into this paradigm, allowing for the joint
    optimization of multiple objectives within a single model architecture. This can
    simplify the training process and reduce the need for separately trained models
    or handcrafted features.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Federated multi-task learning and distillation for ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: 'FMTL extends the concept of FL by allowing each client to learn a personalized
    model that addresses its specific task, while still benefiting from collaboration
    with other clients.This approach recognizes the heterogeneity in clients’ data
    distributions and tasks. Mathematically and compared with [FL](#Sx1.20.20.20)
    (Equation [3](#S4.E3 "In 4.3 FL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")),
    [federated multi-task learning](#Sx1.56.56.56) ([FMTL](#Sx1.56.56.56)) can be
    formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta_{1},\theta_{2},...,\theta_{K}}\sum_{k=1}^{K}F_{k}(\theta_{k})+\lambda
    R(\theta_{1},\theta_{2},...,\theta_{K})$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Different from [FL](#Sx1.20.20.20), $R(\theta_{1},\theta_{2},...,\theta_{K})$
    is a regularization term that encourages some form of similarity or sharing among
    the model parameters of different tasks, promoting collaboration among clients.
    $\lambda$ is a regularization coefficient that balances the trade-off between
    fitting the local data well and collaborating with other clients. FMTL has task-specific
    model parameters $\theta_{k}$ for each client, where only a single global model
    parameter $\theta$ in FL.
  prefs: []
  type: TYPE_NORMAL
- en: In this regard, FMTL offers a promising approach to improving [ASR](#Sx1.4.4.4)
    systems while also enhancing privacy and security measures. This learning paradigm
    extends the traditional [FL](#Sx1.20.20.20) model by enabling the simultaneous
    training of multiple tasks across distributed devices or nodes, without the need
    to share raw data [[129](#bib.bib129)]. [FMTL](#Sx1.56.56.56) leverages data from
    a wide range of devices and users, each potentially offering unique speech data,
    accents, dialects, and noise conditions. This diversity helps in training more
    robust [ASR](#Sx1.4.4.4) models that can perform well across various speech patterns
    and environments [[130](#bib.bib130)]. By learning from many tasks simultaneously,
    FMTL can personalize [ASR](#Sx1.4.4.4) models to individual users or specific
    groups without compromising the model’s general performance [[131](#bib.bib131)].
    This is particularly beneficial for users with unique speech patterns, such as
    those with accents or speech impairments. Moreover, FMTL encourages the development
    of compact models that can handle multiple tasks efficiently. For [ASR](#Sx1.4.4.4)
    systems, this means that a single model can potentially perform speech recognition,
    speaker identification, and even emotion detection, reducing the computational
    overhead on client devices [[132](#bib.bib132)].
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in FMTL, raw data remains on the user’s device and does not
    need to be shared or transferred to a central server. This inherently reduces
    the risk of data breaches and unauthorized access, as sensitive speech data is
    not centralized [[133](#bib.bib133)]. Additionally, FMTL can be combined with
    differential privacy techniques to further anonymize the model updates sent from
    devices to the central server. This ensures that the shared information does not
    reveal sensitive details about the data or the user, enhancing privacy protection
    [[134](#bib.bib134)]. Moving on, the aggregation process in FMTL can be secured
    using cryptographic techniques, ensuring that the aggregated model updates cannot
    be traced back to individual users. This secure aggregation process protects user
    privacy while allowing the benefits of collective learning [[135](#bib.bib135)].
    Lastly, by aggregating model updates from a wide range of tasks and users, FMTL
    can improve the system’s robustness to malicious attempts at data poisoning. The
    diversity of inputs helps in diluting the impact of any adversarial data introduced
    to compromise the model.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End ASR models Continuing to develop and refine end-to-end deep learning
    models that directly map speech inputs to text outputs without the need for intermediate
    representations (like phonetic transcriptions) [mamyrbayev2023hybrid]. Advances
    in models such as Transformer and Conformer architectures, which can capture long-range
    dependencies in speech, are promising for improving [ASR](#Sx1.4.4.4) accuracy
    and efficiency [liu2023sfa]. Specifically, end-to-end models in [ASR](#Sx1.4.4.4)
    aim to directly convert speech input into text output using a single neural network
    architecture. This simplifies the [ASR](#Sx1.4.4.4) pipeline by bypassing traditional
    stages such as acoustic, pronunciation, and language modeling [yang2023attention].
  prefs: []
  type: TYPE_NORMAL
- en: 'An end-to-end [ASR](#Sx1.4.4.4) model maps an input sequence of speech features
    $X=(x_{1},x_{2},\ldots,x_{T})$ to an output sequence of tokens $Y=(y_{1},y_{2},\ldots,y_{N})$.
    The model function $f$ with parameters $\theta$ aims to minimize the difference
    between the predicted sequence and the ground truth. Two common loss functions
    are used [almadhor2023e2e]:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CTC Loss: Connectionist Temporal Classification (CTC) introduces a ’blank’
    token for aligning sequences. The CTC loss is given by:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{CTC}}(\theta)=-\sum_{(X,Y)\in\mathcal{D}}\log P(Y&#124;X;\theta)$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $\mathcal{D}$ is the dataset, and $P(Y|X;\theta)$ is the sequence probability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attention-based Seq2Seq Loss: This uses an encoder-decoder architecture with
    attention to predict each token. The loss is the negative log-likelihood:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{Seq2Seq}}(\theta)=-\sum_{(X,Y)\in\mathcal{D}}\log
    P(Y&#124;X;\theta)$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Besides, model architectures can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs) including Long Short-Term Memory (LSTM) networks
    for temporal dependencies [vander2023using].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs) for capturing local patterns in speech
    features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer Models utilize self-attention mechanisms for parallel processing
    and superior performance [wang2022optimizing].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Gradient-based methods like stochastic gradient descent (SGD) or Adam are used
    to minimize the loss function, optimizing the model parameters $\theta$ to improve
    the speech-to-text mapping. However, end-to-end models face challenges such as
    data scarcity for low-resource languages, computational resource demands, and
    integrating external language models for linguistic improvements [qu2023emphasizing].
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Domain-specific language modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: 'DSLM represents a significant advancement in the field of [ASR](#Sx1.4.4.4),
    especially as systems increasingly cater to specialized fields like healthcare,
    legal, or customer service [jia2023deep, li2023prompting]. \AcpLM provide probabilities
    of sequences of words, crucial for [ASR](#Sx1.4.4.4) systems to predict the likelihood
    of subsequent words in a sentence [[38](#bib.bib38)]. A domain-specific LM is
    trained on text data from the target domain to capture its unique vocabulary and
    grammatical structures [[39](#bib.bib39)]. For n-gram models, this involves calculating
    the conditional probability of a word given the previous $n-1$ words [[40](#bib.bib40)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(w_{n}&#124;w_{n-1},w_{n-2},\ldots,w_{n-(n-1)})=\frac{C(w_{n-(n-1)},\ldots,w_{n})}{C(w_{n-(n-1)},\ldots,w_{n-1})}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'Additionally, data augmentation plays a crucial role in ASR-based [DA](#Sx1.13.13.13).
    Typically, data augmentation in domain-specific ASR involves generating synthetic
    training examples by altering existing recordings or using text-to-speech (TTS)
    systems to create new audio samples from domain-specific texts. Mathematically,
    augmentation techniques can include time stretching, pitch shifting, adding noise,
    or simulating room acoustics, each represented as transformations $T$ applied
    to the original audio signal $x(t)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x^{\prime}(t)=T(x(t);\phi)$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\phi$ represents the parameters of the transformation. Moving on, regularization
    techniques are applied to prevent overfitting to the domain-specific data, ensuring
    the model maintains generalization capabilities. Techniques such as dropout, L1/L2
    regularization, or elastic net involve adding terms to the loss function or modifying
    the optimization process to penalize large weights or complex models. For instance,
    L2 regularization adds a penalty equal to the square of the magnitude of coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L^{\prime}(D;\theta)=L(D;\theta)+\lambda\&#124;\theta\&#124;^{2}$ |  |
    (10) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda$ is a regularization parameter.
  prefs: []
  type: TYPE_NORMAL
- en: While DSLM offers substantial benefits, challenges remain, such as data scarcity
    in niche domains, the need for continual model updates to keep pace with evolving
    language use, and ensuring privacy and security in sensitive domains like healthcare
    and finance. Addressing these challenges through innovative approaches in model
    training, data augmentation, and privacy-preserving technologies will be crucial
    for the advancement of domain-specific ASR systems.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy preservation With the advancements in [ML](#Sx1.30.30.30), DL, and [DTL](#Sx1.16.16.16),
    [ASR](#Sx1.4.4.4) systems have become more practical and scalable. However, these
    systems also introduce serious privacy concerns due to the abundance of sensitive
    acoustic and textual information in speech data.
  prefs: []
  type: TYPE_NORMAL
- en: While open-source and offline [ASR](#Sx1.4.4.4) systems can mitigate privacy
    risks, online [DTL](#Sx1.16.16.16)-based systems can amplify these threats. Additionally,
    the transcription performance of offline and open-source [ASR](#Sx1.4.4.4) systems
    is typically inferior to cloud-based [ASR](#Sx1.4.4.4) systems, especially in
    real-world scenarios [ahmed2020preech]. In this context, the $D_{S}$ data may
    contain sensitive information that needs to be protected. Thus, preserving users’
    privacy during the knowledge transfer from the [source domain](#Sx1.40.40.40)
    ([SD](#Sx1.40.40.40)) to the [target domain](#Sx1.48.48.48) ([TD](#Sx1.48.48.48))
    becomes a critical issue.
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, future research efforts should focus on integrating
    effective security and privacy protection strategies. Examples include decentralized
    [DTL](#Sx1.16.16.16) approaches utilizing blockchain technology [ul2020decentralized,
    wang2021enabling] and federated [DTL](#Sx1.16.16.16) methods [zhang2021federated,
    maurya2021federated]. These approaches aim to ensure privacy-preserving knowledge
    transfer in [DTL](#Sx1.16.16.16)-based [ASR](#Sx1.4.4.4) systems.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Interpretation of [DTL](#Sx1.16.16.16) models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[DTL](#Sx1.16.16.16)-based [ASR](#Sx1.4.4.4) models, despite their success,
    are often considered as "black box" systems lacking interpretability. This lack
    of interpretability raises doubts about the credibility and repeatability of their
    decisions, making it crucial to explain the reasoning behind their predictions.
    The field of explainable and interpretable [ML](#Sx1.30.30.30)/DL has gained increasing
    interest in various applications, including speech processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Several studies have explored the interpretability of [DTL](#Sx1.16.16.16) models.
    In one study [ramakrishnan2016towards], an agent was designed to explain how it
    learns a new task using prior common knowledge, aiming to enhance users’ trust
    and acceptance of the system results. Another study [kim2019the] defined interpretable
    features in a [DTL](#Sx1.16.16.16) algorithm and examined the relationship between
    the [SD](#Sx1.40.40.40) and TD in the task, focusing on the interpretability of
    the pretrained [DTL](#Sx1.16.16.16) model.
  prefs: []
  type: TYPE_NORMAL
- en: The work by Lee et al. [lee2021interpretable] introduced a knowledge distillation
    approach that generated interpretable embedding procedure (IEP) knowledge based
    on PCA and transferred it to the student network using a message passing neural
    network. This approach enhanced interpretability while maintaining accuracy through
    multi-task learning. Additionally, Carr et al. proposed an interpretable staged
    TL (iSTL) scheme for accurate and explainable classification of optical coherence
    tomography (OCT) scans with a small sample size [carr2021interpretable]. iSTL
    outperformed [DTL](#Sx1.16.16.16) techniques on unseen data, utilizing clinical
    features for predictions with interpretable attention maps.
  prefs: []
  type: TYPE_NORMAL
- en: Delving deeper into techniques for FL distillation (optimizing model size) within
    [FL](#Sx1.20.20.20) frameworks is essential. This exploration involves researching
    methods to compress neural [ASR](#Sx1.4.4.4) models effectively while ensuring
    their performance remains intact, especially tailored for edge devices with storage
    and computational constraints. It is imperative to investigate the trade-offs
    associated with reducing model size while maintaining performance metrics like
    [WER](#Sx1.52.52.52). Developing strategies to strike a balance between downsizing
    models and preserving satisfactory performance levels within federated learning
    environments is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Recent RL techniques for [ASR](#Sx1.4.4.4)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exploring incremental [DRL](#Sx1.15.15.15) approaches [[136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138)] in DRL-based [ASR](#Sx1.4.4.4) systems, could be very interesting.
    This approach involves the model continuously learning from newly acquired data
    and dynamically adjusting its ASR functionalities over time. By incrementally
    updating its knowledge base, the model can enhance its performance without necessitating
    full retraining, thus enabling continual enhancement of [ASR](#Sx1.4.4.4) systems.
    This capability not only fosters greater resilience and adaptability in speech
    recognition capabilities but also offers potential applications in scenarios where
    real-time adaptation to changing conditions is crucial, such as in noisy environments
    or with varying speaker accents. Moreover, incremental RL can potentially lead
    to more efficient use of computational resources, as the model only needs to focus
    on learning from new data, rather than reprocessing the entire dataset. Further
    research in this area could unlock new possibilities for [ASR](#Sx1.4.4.4) systems
    to evolve and improve over time, ultimately enhancing their usability and effectiveness
    in diverse real-world settings.
  prefs: []
  type: TYPE_NORMAL
- en: Although some ASR schemes based on RL have been proposed, there remains a notable
    scarcity in the application of RL techniques to enhance ASR methods. While policy
    gradient and Q-learning are commonly employed, the realm of RL encompasses various
    subcategories such as [double deep Q-network](#Sx1.59.59.59) ([DDQN](#Sx1.59.59.59)),
    [actor-critic](#Sx1.60.60.60) ([AC](#Sx1.60.60.60)), [deep deterministic policy
    gradien](#Sx1.62.62.62) ([DDPG](#Sx1.62.62.62)), and more [[139](#bib.bib139)],
    which hold promise for advancing ASR with innovative approaches. Researchers are
    encouraged to delve into these diverse DRL-based methods to further enrich the
    field of ASR for both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24) fields.
  prefs: []
  type: TYPE_NORMAL
- en: 6.7 Online [DTL](#Sx1.16.16.16)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Online DTL combines the principles of TL and online learning with deep neural
    networks, enabling models to adapt in real-time to new tasks or data distributions.
    This approach is beneficial in dynamic environments where data arrives sequentially.
    [DL](#Sx1.14.14.14) models, specifically Deep Neural Networks (DNNs), learn through
    optimizing the weights $\theta$ to minimize a loss function $L$, which measures
    the discrepancy between predicted outputs $\hat{y}$ and true outputs $y$: $\theta^{*}=\arg\min_{\theta}L(D;\theta)$.
    On the other hand, TL improves learning in a new target task through the transfer
    of knowledge from a related source task, adapting a pre-trained model $\theta_{S}$
    on $D_{S}$ to a $D_{T}$: $\theta_{T}^{*}=\arg\min_{\theta}L(D_{T};\theta_{T})$.
    In this regard, online learning updates the model incrementally as new data $(x_{t},y_{t})$
    arrives:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{t+1}=\theta_{t}-\alpha_{t}\nabla_{\theta}L(y_{t},f(x_{t};\theta_{t}))$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: Where $\alpha_{t}$ is the learning rate, and $\nabla_{\theta}L$ is the gradient
    of the loss with respect to $\theta$. Moving on, online TL integrates these concepts
    to continuously adapt a deep learning model to new tasks or data streams, often
    involving techniques such as feature extraction, fine-tuning, model adaptation,
    and continual learning. The adaptation process at each time step $t$ can be viewed
    as $\theta_{t+1}^{*}=\arg\min_{\theta}L(D_{T_{t}};\theta_{T_{t}})$. Where $D_{T_{t}}$
    represents the data available at time $t$, including new target domain data.
  prefs: []
  type: TYPE_NORMAL
- en: The approach of online DTL offers a forward-looking solution to this issue.
    Particularly, discrepancies in class distributions and the representation of features
    between the $D_{S}$ and $D_{T}$ amplify the complexity of online DTL [[140](#bib.bib140)].
    To navigate the complexities mentioned, online DTL has been dissected into two
    primary methodologies. The first, known as homogeneous online DTL, operates on
    the premise of a unified feature space across both domains. Conversely, heterogeneous
    online DTL acknowledges the distinct feature spaces intrinsic to each domain [[141](#bib.bib141)].
    An exemplary solution to the challenges of heterogeneous online DTL includes leveraging
    unlabeled instances of co-occurrence to forge a connective bridge between the
    $D_{S}$ and $D_{T}$, facilitating the precursor to knowledge transfer [[142](#bib.bib142)].
    Furthering the discourse, online DTL augmented with extreme learning machines
    introduces a novel framework [[143](#bib.bib143)]. Addressing the challenge of
    limited data in the $D_{T}$, the technique of transfer learning with lag (TLL),
    rooted in shallow neural network embeddings, has been applied. This method ensures
    the continuity of knowledge transfer, notwithstanding fluctuations in the feature
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 6.8 Transformers and LLMs-based ASR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \Acp
  prefs: []
  type: TYPE_NORMAL
- en: LLM and transformers represent the forefront of [AI](#Sx1.1.1.1), trained on
    vast datasets spanning various domains, including text, speech, images, and multi-modal
    inputs. Despite extensive research on [ASR](#Sx1.4.4.4), existing [SOTA](#Sx1.44.44.44)
    approaches often lack integration of advanced [AI](#Sx1.1.1.1) techniques like
    [DRL](#Sx1.15.15.15) and [FL](#Sx1.20.20.20) into both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24)
    domains. For example, \AcLLM based on [DTL](#Sx1.16.16.16) has demonstrated significant
    potential for [ASR](#Sx1.4.4.4) tasks, particularly for both [LM](#Sx1.24.24.24)
    and [AM](#Sx1.2.2.2) components. The incorporation of TL techniques into [large
    language model](#Sx1.58.58.58) ([LLM](#Sx1.58.58.58)) facilitates the transfer
    of knowledge from extensive pre-training tasks to enhance [ASR](#Sx1.4.4.4) effectiveness.
    In terms of [AM](#Sx1.2.2.2), fine-tuning [LLM](#Sx1.58.58.58) can leverage insights
    gained from pre-trained models exposed to sample acoustic data. This enables the
    [AM](#Sx1.2.2.2) component to grasp acoustic features like spectrograms or Mel-frequency
    cepstral coefficients (MFCCs) and utilize pre-trained knowledge to improve speech
    recognition accuracy. Through fine-tuning, the model can adjust and specialize
    in specific datasets or acoustic domains, leading to enhanced [ASR](#Sx1.4.4.4)
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the [LM](#Sx1.24.24.24) aspect of [LLM](#Sx1.58.58.58) based on [DTL](#Sx1.16.16.16)
    can enhance [ASR](#Sx1.4.4.4) by leveraging TL. Pre-training [LLM](#Sx1.58.58.58)
    on vast text corpora equips it with extensive language representations, aiding
    in addressing diverse language challenges. Fine-tuning enables adaptation to specific
    language characteristics, improving transcription accuracy and contextual appropriateness.
    Both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24) fine-tuning can benefit from [DA](#Sx1.13.13.13),
    incorporating target domain data to tailor models, reducing domain mismatch, and
    enhancing effectiveness and generalization. Utilizing [LLM](#Sx1.58.58.58) for
    objective [ASR](#Sx1.4.4.4) testing and MOS evaluation involves compiling diverse
    datasets, fine-tuning [LLM](#Sx1.58.58.58), and integrating it into the [ASR](#Sx1.4.4.4)
    system. Evaluation metrics like [CER](#Sx1.8.8.8) and Pearson’s correlation gauge
    system performance, guiding further fine-tuning iterations for improved results.
    This iterative process ensures the [ASR](#Sx1.4.4.4) system’s continual enhancement
    and accurate MOS scale generation. Researchers are invited to explore these gaps
    and advance the integration of more advanced [AI](#Sx1.1.1.1) techniques such
    as [DRL](#Sx1.15.15.15) and [FL](#Sx1.20.20.20) into both [AM](#Sx1.2.2.2) and
    [LM](#Sx1.24.24.24) domains within transformer-based models. Additionally, there
    is a need for further investigation into Transformers and DTL-based ASR schemes
    specifically tailored for the [LM](#Sx1.24.24.24) domain. Closing these gaps will
    contribute to the development of more robust and effective language models across
    various applications.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, recent advancements in deep learning have presented both challenges
    and opportunities for[ASR](#Sx1.4.4.4). Traditional [ASR](#Sx1.4.4.4) systems
    require extensive training datasets, often including confidential information,
    and consume significant computational resources. However, the demand for adaptive
    systems capable of performing well in dynamic environments has spurred the development
    of advanced deep learning techniques such as [DTL](#Sx1.16.16.16), [FL](#Sx1.20.20.20),
    and [DRL](#Sx1.15.15.15), with all their variant techniques. These advanced techniques
    address issues related to [DA](#Sx1.13.13.13), privacy preservation, and dynamic
    decision-making, thereby enhancing [ASR](#Sx1.4.4.4) performance and reducing
    computational costs.
  prefs: []
  type: TYPE_NORMAL
- en: This survey has provided a comprehensive review of [DTL](#Sx1.16.16.16), [FL](#Sx1.20.20.20),
    and [RL](#Sx1.35.35.35)-based ASR frameworks, offering insights into the latest
    developments and helping researchers and professionals understand current challenges.
    Additionally, the integration of transformers, powerful [DL](#Sx1.14.14.14) models,
    has been explored for their ability to capture complex dependencies in ASR sequences.
    By presenting a structured taxonomy and conducting critical analyses, this paper
    has shed light on the strengths and weaknesses of existing frameworks, as well
    as highlighted ongoing challenges. Moving forward, further research is needed
    to overcome these challenges and unlock the full potential of advanced DL techniques
    in ASR. Future work should focus on refining existing approaches, addressing privacy
    concerns in FL, improving RL algorithms for ASR optimization, and exploring innovative
    ways to leverage transformers for more efficient and accurate speech recognition.
    By continuing to innovate and collaborate across disciplines, we can push the
    boundaries of [ASR](#Sx1.4.4.4) technology and realize its transformative impact
    on various fields, including healthcare, communication, and accessibility.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] H. Haneche, A. Ouahabi, B. Boudraa, Compressed sensing-speech coding scheme
    for mobile communications, Circuits, Systems, and Signal Processing (2021) 1–21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] D. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, J. Jensen,
    An overview of deep-learning-based audio-visual speech enhancement and separation,
    IEEE/ACM Transactions on Audio, Speech, and Language Processing (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Luo, C. Han, N. Mesgarani, Group communication with context codec for
    lightweight source separation, IEEE/ACM Transactions on Audio, Speech, and Language
    Processing 29 (2021) 1752–1761.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] H. Kheddar, M. Bouzid, D. Megías, Pitch and fourier magnitude based steganography
    for hiding 2.4 kbps melp bitstream, IET Signal Processing 13 (3) (2019) 396–407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] H. Kheddar, A. C. Mazari, G. H. Ilk, Speech steganography based on double
    approximation of lsfs parameters in amr coding, in: 2022 7th International Conference
    on Image and Signal Processing and their Applications (ISPA), IEEE, 2022, pp.
    1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] H. Kheddar, D. Megias, M. Bouzid, Fourier magnitude-based steganography
    for hiding 2.4 kbpsmelp secret speech, in: 2018 International Conference on Applied
    Smart Systems (ICASS), IEEE, 2018, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] H. Yassine, B. Bachir, K. Aziz, A secure and high robust audio watermarking
    system for copyright protection, International Journal of Computer Applications
    53 (17) (2012) 33–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] M. Yamni, H. Karmouni, M. Sayyouri, H. Qjidaa, Efficient watermarking algorithm
    for digital audio/speech signal, Digital Signal Processing 120 (2022) 103251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] H. Chen, B. D. Rouhani, F. Koushanfar, Specmark: A spectral watermarking
    framework for ip protection of speech recognition systems., in: INTERSPEECH, 2020,
    pp. 2312–2316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Olivieri, R. Malvermi, M. Pezzoli, M. Zanoni, S. Gonzalez, F. Antonacci,
    A. Sarti, Audio information retrieval and musical acoustics, IEEE Instrumentation
    & Measurement Magazine 24 (7) (2021) 10–20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] E. Wold, T. Blum, D. Keislar, J. Wheaten, Content-based classification,
    search, and retrieval of audio, IEEE multimedia 3 (3) (1996) 27–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] W. Boes, et al., Audiovisual transfer learning for audio tagging and sound
    event detection, Proceedings Interspeech 2021 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Tang, J. Pino, C. Wang, X. Ma, D. Genzel, A general multi-task learning
    framework to leverage text data for speech to text tasks, in: ICASSP 2021-2021
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, 2021, pp. 6209–6213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] F. M. Plaza-del Arco, M. D. Molina-González, L. A. Ureña-López, M. T.
    Martín-Valdivia, Comparing pre-trained language models for spanish hate speech
    detection, Expert Systems with Applications 166 (2021) 114120.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] A. C. Mazari, H. Kheddar, Deep learning-based analysis of algerian dialect
    dataset targeted hate speech, offensive language and cyberbullying, International
    Journal of Computing and Digital Systems (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. Meghraoui, B. Boudraa, T. Merazi, P. G. Vilda, A novel pre-processing
    technique in pathologic voice detection: Application to parkinson’s disease phonation,
    Biomedical Signal Processing and Control 68 (2021) 102604.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y.-Y. Lin, W.-Z. Zheng, W. C. Chu, J.-Y. Han, Y.-H. Hung, G.-M. Ho, C.-Y.
    Chang, Y.-H. Lai, A speech command control-based recognition system for dysarthric
    patients based on deep learning technology, Applied Sciences 11 (6) (2021) 2477.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y. Kumar, S. Gupta, W. Singh, A novel deep transfer learning models for
    recognition of birds sounds in different environment, Soft Computing 26 (3) (2022)
    1003–1023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. Padi, S. O. Sadjadi, R. D. Sriram, D. Manocha, Improved speech emotion
    recognition using transfer learning and spectrogram augmentation, in: Proceedings
    of the 2021 International Conference on Multimodal Interaction, 2021, pp. 645–652.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Y. Himeur, M. Elnour, F. Fadli, N. Meskin, I. Petri, Y. Rezgui, F. Bensaali,
    A. Amra, Next-generation energy systems for sustainable smart cities: Roles of
    transfer learning, Sustainable Cities and Society (2022) 1–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. Niu, Y. Liu, J. Wang, H. Song, A decade survey of transfer learning
    (2010–2020), IEEE Transactions on Artificial Intelligence 1 (2) (2020) 151–166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Kheddar, D. Megías, High capacity speech steganography for the g723\.
    1 coder based on quantised line spectral pairs interpolation and cnn auto-encoding,
    Applied Intelligence (2022) 1–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] T. A. de Lima, M. Da Costa-Abreu, A survey on automatic speech recognition
    systems for portuguese language and its variations, Computer Speech & Language
    62 (2020) 101055.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Singh, V. Kadyan, M. Kumar, N. Bassan, Asroil: a comprehensive survey
    for automatic speech recognition of indian languages, Artificial Intelligence
    Review 53 (2020) 3673–3704.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] R. S. Arslan, N. BARIŞÇI, A detailed survey of turkish automatic speech
    recognition, Turkish journal of electrical engineering and computer sciences 28 (6)
    (2020) 3253–3269.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Dhouib, A. Othman, O. El Ghoul, M. K. Khribi, A. Al Sinani, Arabic
    automatic speech recognition: a systematic literature review, Applied Sciences
    12 (17) (2022) 8898.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] J. Kaur, A. Singh, V. Kadyan, Automatic speech recognition system for
    tonal languages: State-of-the-art survey, Archives of Computational Methods in
    Engineering 28 (2021) 1039–1068.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] A. A. Abushariah, H.-N. Ting, M. B. P. Mustafa, A. S. M. Khairuddin, M. A.
    Abushariah, T.-P. Tan, Bilingual automatic speech recognition: A review, taxonomy
    and open challenges, IEEE Access (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. L. K. E. Fendji, D. C. Tala, B. O. Yenke, M. Atemkeng, Automatic speech
    recognition using limited vocabulary: A survey, Applied Artificial Intelligence
    36 (1) (2022) 2095039.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] V. Bhardwaj, M. T. Ben Othman, V. Kukreja, Y. Belkhier, M. Bajaj, B. S.
    Goud, A. U. Rehman, M. Shafiq, H. Hamam, Automatic speech recognition (asr) systems
    for children: A systematic literature review, Applied Sciences 12 (9) (2022) 4419.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] R. Errattahi, A. El Hannani, H. Ouahmane, Automatic speech recognition
    errors detection and correction: A review, Procedia Computer Science 128 (2018)
    32–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] H. Aldarmaki, A. Ullah, S. Ram, N. Zaki, Unsupervised automatic speech
    recognition: A review, Speech Communication 139 (2022) 76–91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. S. Dhanjal, W. Singh, A comprehensive survey on automatic speech recognition
    using neural networks, Multimedia Tools and Applications (2023) 1–46.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. B. Nassif, I. Shahin, I. Attili, M. Azzeh, K. Shaalan, Speech recognition
    using deep neural networks: A systematic review, IEEE access 7 (2019) 19143–19165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. Malik, M. K. Malik, K. Mehmood, I. Makhdoom, Automatic speech recognition:
    a survey, Multimedia Tools and Applications 80 (6) (2021) 9411–9457.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, Deep transfer
    learning for automatic speech recognition: Towards better generalization, Knowledge-Based
    Systems 277 (2023) 110851.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] F. Filippidou, L. Moussiades, A benchmarking of ibm, google and wit automatic
    speech recognition systems, in: IFIP International Conference on Artificial Intelligence
    Applications and Innovations, Springer, 2020, pp. 73–82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Suzuki, H. Sakaji, M. Hirano, K. Izumi, Constructing and analyzing
    domain-specific language model for financial text mining, Information Processing
    & Management 60 (2) (2023) 103194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] C.-H. H. Yang, Y. Gu, Y.-C. Liu, S. Ghosh, I. Bulyko, A. Stolcke, Generative
    speech recognition error correction with large language models and task-activating
    prompting, in: 2023 IEEE Automatic Speech Recognition and Understanding Workshop
    (ASRU), IEEE, 2023, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Z. Dong, Q. Ding, W. Zhai, M. Zhou, A speech recognition method based
    on domain-specific datasets and confidence decision networks, Sensors 23 (13)
    (2023) 6036.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] H. Kheddar, Y. Himeur, A. I. Awad, Deep transfer learning for intrusion
    detection in industrial control networks: A comprehensive review, Journal of Network
    and Computer Applications 220 (2023) 103760.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Li, X. Zhang, P. Wang, X. Zhang, Y. Liu, Insight into an unsupervised
    two-step sparse transfer learning algorithm for speech diagnosis of parkinson’s
    disease, Neural Computing and Applications (2021) 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] O. Karaman, H. Çakın, A. Alhudhaif, K. Polat, Robust automated parkinson
    disease detection based on voice signals with transfer learning, Expert Systems
    with Applications 178 (2021) 115013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] R. A. Ramadan, Detecting adversarial attacks on audio-visual speech recognition
    using deep learning method, International Journal of Speech Technology (2021)
    1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Q. Yu, Y. Ma, Y. Li, Enhancing speech recognition for parkinson’s disease
    patient using transfer learning technique, Journal of Shanghai Jiaotong University
    (Science) (2021) 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Bai, J. Yi, J. Tao, Z. Tian, Z. Wen, S. Zhang, Fast end-to-end speech
    recognition via non-autoregressive models and cross-modal knowledge transferring
    from bert, IEEE/ACM Transactions on Audio, Speech, and Language Processing 29
    (2021) 1897–1911.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] I.-T. Recommendation, Perceptual evaluation of speech quality (pesq):
    An objective method for end-to-end speech quality assessment of narrow-band telephone
    networks and speech codecs, Rec. ITU-T P. 862 (2001).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] S. Zhang, E. Loweimi, P. Bell, S. Renals, On the usefulness of self-attention
    for automatic speech recognition with transformers, in: 2021 IEEE Spoken Language
    Technology Workshop (SLT), IEEE, 2021, pp. 89–96.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] O. Hrinchuk, M. Popova, B. Ginsburg, Correction of automatic speech recognition
    with transformer sequence-to-sequence model, in: Icassp 2020-2020 ieee international
    conference on acoustics, speech and signal processing (icassp), IEEE, 2020, pp.
    7074–7078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Li, R. Su, X. Xie, N. Yan, L. Wang, A multi-level acoustic feature
    extraction framework for transformer based end-to-end speech recognition, arXiv
    preprint arXiv:2108.07980 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Baade, P. Peng, D. Harwath, Mae-ast: Masked autoencoding audio spectrogram
    transformer, arXiv preprint arXiv:2203.16691 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Bai, J. Chen, M. Wang, M. S. Ayub, Q. Yan, A squeeze-and-excitation
    and transformer based cross-task model for environmental sound recognition, IEEE
    Transactions on Cognitive and Developmental Systems (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] K. Chen, J. Wang, F. Deng, X. Wang, icnn-transformer: An improved cnn-transformer
    with channel-spatial attention and keyword prediction for automated audio captioning,
    in: INTERSPEECH, 2022, pp. 4167–4171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] K. Deng, S. Cao, Y. Zhang, L. Ma, Improving hybrid ctc/attention end-to-end
    speech recognition with pretrained acoustic and language models, in: 2021 IEEE
    Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2021, pp.
    76–82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] X. Zhou, E. Yılmaz, Y. Long, Y. Li, H. Li, Multi-encoder-decoder transformer
    for code-switching speech recognition, arXiv preprint arXiv:2006.10414 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] G. I. Winata, S. Cahyawijaya, Z. Lin, Z. Liu, P. Fung, Lightweight and
    efficient end-to-end speech recognition using low-rank transformer, in: ICASSP
    2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), IEEE, 2020, pp. 6144–6148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M.-H. Lee, S.-E. Lee, J.-S. Seong, J.-H. Chang, H. Kwon, C. Park, Regularizing
    transformer-based acoustic models by penalizing attention weights for robust speech
    recognition, in: Proceedings of the Annual Conference of the International Speech
    Communication Association, INTERSPEECH, Vol. 2022, International Speech Communication
    Association, 2022, pp. 56–60.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S. R. Shareef, Y. F. Mohammed, Collaborative training of acoustic encoder
    for recognizing the impaired children speech, in: 2022 Fifth College of Science
    International Conference of Recent Trends in Information Technology (CSCTIT),
    IEEE, 2022, pp. 79–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] R. Fan, W. Chu, P. Chang, A. Alwan, A ctc alignment-based non-autoregressive
    transformer for end-to-end automatic speech recognition, IEEE/ACM Transactions
    on Audio, Speech, and Language Processing 31 (2023) 1436–1448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
    bidirectional transformers for language understanding (2019). [arXiv:1810.04805](http://arxiv.org/abs/1810.04805).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen,
    A. Kannan, R. J. Weiss, K. Rao, E. Gonina, et al., State-of-the-art speech recognition
    with sequence-to-sequence models, in: 2018 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), IEEE, 2018, pp. 4774–4778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Y. Wang, Y. Shi, F. Zhang, C. Wu, J. Chan, C.-F. Yeh, A. Xiao, Transformer
    in action: a comparative study of transformer-based acoustic models for large
    scale speech recognition applications, in: ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
    6778–6782.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Aroudi, S. Uhlich, M. F. Font, Trunet: Transformer-recurrent-u network
    for multi-channel reverberant sound source separation, arXiv preprint arXiv:2110.04047
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] L. Wang, W. Wei, Y. Chen, Y. Hu, D 2 net: A denoising and dereverberation
    network based on two-branch encoder and dual-path transformer, in: 2022 Asia-Pacific
    Signal and Information Processing Association Annual Summit and Conference (APSIPA
    ASC), IEEE, 2022, pp. 1649–1654.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] P. Swietojanski, S. Braun, D. Can, T. F. Da Silva, A. Ghoshal, T. Hori,
    R. Hsiao, H. Mason, E. McDermott, H. Silovsky, et al., Variable attention masking
    for configurable transformer transducer speech recognition, in: ICASSP 2023-2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] N. Moritz, G. Wichern, T. Hori, J. Le Roux, All-in-one transformer: Unifying
    speech recognition, audio tagging, and event detection., in: INTERSPEECH, 2020,
    pp. 3112–3116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] W. Huang, W. Hu, Y. T. Yeung, X. Chen, Conv-transformer transducer: Low
    latency, low frame rate, streamable end-to-end speech recognition, arXiv preprint
    arXiv:2008.05750 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R. Fan, W. Chu, P. Chang, J. Xiao, Cass-nat: Ctc alignment-based single
    step non-autoregressive transformer for speech recognition, in: ICASSP 2021-2021
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, 2021, pp. 5889–5893.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. Hadwan, H. A. Alsayadi, S. AL-Hagree, An end-to-end transformer-based
    automatic speech recognition for qur’an reciters., Computers, Materials & Continua
    74 (2) (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] L. Smietanka, T. Maka, Augmented transformer for speech detection in adverse
    acoustical conditions, in: 2023 Signal Processing: Algorithms, Architectures,
    Arrangements, and Applications (SPA), IEEE, 2023, pp. 14–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Y. Li, D. Luo, Adversarial audio detection method based on transformer,
    in: 2022 International Conference on Machine Learning and Intelligent Systems
    Engineering (MLISE), IEEE, 2022, pp. 77–82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] C. Wang, M. Jia, Y. Zhang, L. Li, Parallel-path transformer network for
    time-domain monaural speech separation, in: 2023 International Conference on Cyber-Physical
    Social Intelligence (ICCSI), IEEE, 2023, pp. 509–514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang,
    A. Tjandra, X. Zhang, F. Zhang, et al., Transformer-based acoustic modeling for
    hybrid speech recognition, in: ICASSP 2020-2020 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp. 6874–6878.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] K. Wang, B. He, W.-P. Zhu, Tstnn: Two-stage transformer based neural network
    for speech enhancement in the time domain, in: ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
    7098–7102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Y. Gong, C.-I. Lai, Y.-A. Chung, J. Glass, Ssast: Self-supervised audio
    spectrogram transformer, in: Proceedings of the AAAI Conference on Artificial
    Intelligence, Vol. 36, 2022, pp. 10699–10709.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. Wu, Y. Wang, Y. Shi, C.-F. Yeh, F. Zhang, Streaming transformer-based
    acoustic models using self-attention with augmented memory, arXiv preprint arXiv:2005.08042
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] V. Nagaraja, Y. Shi, G. Venkatesh, O. Kalinli, M. L. Seltzer, V. Chandra,
    Collaborative training of acoustic encoders for speech recognition, arXiv preprint
    arXiv:2106.08960 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] G. Ahmed, A. A. Lawaye, T. A. Mir, P. Rana, Toward developing attention-based
    end-to-end automatic speech recognition, in: International Conference On Innovative
    Computing And Communication, Springer, 2023, pp. 147–161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y. Himeur, S. Al-Maadeed, H. Kheddar, N. Al-Maadeed, K. Abualsaud, A. Mohamed,
    T. Khattab, Video surveillance using deep transfer learning and deep domain adaptation:
    Towards better generalization, Engineering Applications of Artificial Intelligence
    119 (2023) 105698.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] H. Kheddar, M. Hemis, Y. Himeur, D. Megías, A. Amira, Deep learning for
    diverse data types steganalysis: A review, arXiv preprint arXiv:2308.04522 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] S. Schneider, A. Baevski, R. Collobert, M. Auli, wav2vec: Unsupervised
    pre-training for speech recognition, arXiv preprint arXiv:1904.05862 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J. Thienpondt, K. Demuynck, Transfer learning for robust low-resource
    children’s speech asr with transformers and source-filter warping, arXiv preprint
    arXiv:2206.09396 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Z. Dan, Y. Zhao, X. Bi, L. Wu, Q. Ji, Multi-task transformer with adaptive
    cross-entropy loss for multi-dialect speech recognition, Entropy 24 (10) (2022)
    1429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] T. Pellegrini, I. Khalfaoui-Hassani, E. Labbé, T. Masquelier, Adapting
    a convnext model to audio classification on audioset, arXiv preprint arXiv:2306.00830
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. Xin, D. Yang, Y. Zou, Audio pyramid transformer with domain adaption
    for weakly supervised sound event detection and audio classification, in: Proc.
    Interspeech 2022, 2022, pp. 1546–1550.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
    bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Song, D. Jiang, X. Zhao, Q. Xu, R. C.-W. Wong, L. Fan, Q. Yang, L2rs:
    a learning-to-rescore mechanism for automatic speech recognition, arXiv preprint
    arXiv:1910.11496 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C.-X. Qin, D. Qu, L.-H. Zhang, Towards end-to-end speech recognition with
    transfer learning, EURASIP Journal on Audio, Speech, and Music Processing 2018 (1)
    (2018) 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] D. Jiang, C. Tan, J. Peng, C. Chen, X. Wu, W. Zhao, Y. Song, Y. Tong,
    C. Liu, Q. Xu, et al., A gdpr-compliant ecosystem for speech recognition with
    transfer, federated, and evolutionary learning, ACM Transactions on Intelligent
    Systems and Technology (TIST) 12 (3) (2021) 1–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] F. Weninger, J. Andrés-Ferrer, X. Li, P. Zhan, Listen, attend, spell and
    adapt: Speaker adapted sequence-to-sequence asr, arXiv preprint arXiv:1907.04916
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] S. Deena, M. Hasan, M. Doulaty, O. Saz, T. Hain, Recurrent neural network
    language model adaptation for multi-genre broadcast speech recognition and alignment,
    IEEE/ACM Transactions on Audio, Speech, and Language Processing 27 (3) (2018)
    572–582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S.-I. Ng, W. Liu, Z. Peng, S. Feng, H.-P. Huang, O. Scharenborg, T. Lee,
    The cuhk-tudelft system for the slt 2021 children speech recognition challenge,
    arXiv preprint arXiv:2011.06239 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] K. Manohar, G. G. Menon, A. Abraham, R. Rajan, A. Jayan, Automatic recognition
    of continuous malayalam speech using pretrained multilingual transformers, in:
    2023 International Conference on Intelligent Systems for Communication, IoT and
    Security (ICISCoIS), IEEE, 2023, pp. 671–675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] J. Cho, M. K. Baskar, R. Li, M. Wiesner, S. H. Mallidi, N. Yalta, M. Karafiat,
    S. Watanabe, T. Hori, Multilingual sequence-to-sequence speech recognition: architecture,
    transfer learning, and language modeling, in: 2018 IEEE Spoken Language Technology
    Workshop (SLT), IEEE, 2018, pp. 521–527.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] K. Li, Y. Song, I. McLoughlin, L. Liu, J. Li, L.-R. Dai, Fine-tuning audio
    spectrogram transformer with task-aware adapters for sound event detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] C. Wang, S. Dai, Y. Wang, F. Yang, M. Qiu, K. Chen, W. Zhou, J. Huang,
    Arobert: An asr robust pre-trained language model for spoken language understanding,
    IEEE/ACM Transactions on Audio, Speech, and Language Processing (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Y. Himeur, I. Varlamis, H. Kheddar, A. Amira, S. Atalla, Y. Singh, F. Bensaali,
    W. Mansoor, Federated learning for computer vision, arXiv preprint arXiv:2308.13558
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] D. Dimitriadis, R. G. Ken’ichi Kumatani, R. Gmyr, Y. Gaur, S. E. Eskimez,
    A federated approach in training acoustic models., in: Interspeech, 2020, pp.
    981–985.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] D. Guliani, F. Beaufays, G. Motta, Training speech recognition models
    with federated learning: A quality/cost framework, in: ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
    3080–3084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Zhu, J. Wang, G. Cheng, P. Zhang, Y. Yan, Decoupled federated learning
    for asr with non-iid data, arXiv preprint arXiv:2206.09102 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Cui, S. Lu, B. Kingsbury, Federated acoustic modeling for automatic
    speech recognition, in: ICASSP 2021-2021 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), IEEE, 2021, pp. 6748–6752.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. Nguyen, S. Mdhaffar, N. Tomashenko, J.-F. Bonastre, Y. Estève, Federated
    learning for asr based on wav2vec 2.0, in: ICASSP 2023-2023 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2023, pp.
    1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, P.-Y. Chen, S. M. Siniscalchi, X. Ma,
    C.-H. Lee, Decentralizing feature extraction with quantum convolutional neural
    network for automatic speech recognition, in: ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
    6523–6527.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Gao, T. Parcollet, S. Zaiem, J. Fernandez-Marques, P. P. de Gusmao,
    D. J. Beutel, N. D. Lane, End-to-end speech recognition from federated acoustic
    models, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), IEEE, 2022, pp. 7227–7231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. Mehmood, A. Dobrowolska, K. Saravanan, M. Ozay, Fednst: Federated
    noisy student training for automatic speech recognition, arXiv preprint arXiv:2206.02797
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. C. Vásquez-Correa, A. Álvarez Muniain, Novel speech recognition systems
    applied to forensics within child exploitation: Wav2vec2\. 0 vs. whisper, Sensors
    23 (4) (2023) 1843.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] C. Tan, D. Jiang, H. Mo, J. Peng, Y. Tong, W. Zhao, C. Chen, R. Lian,
    Y. Song, Q. Xu, Federated acoustic model optimization for automatic speech recognition,
    in: Database Systems for Advanced Applications: 25th International Conference,
    DASFAA 2020, Jeju, South Korea, September 24–27, 2020, Proceedings, Part III 25,
    Springer, 2020, pp. 771–774.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] N. Tomashenko, S. Mdhaffar, M. Tommasi, Y. Estève, J.-F. Bonastre, Privacy
    attacks for automatic speech recognition acoustic models in a federated learning
    framework, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), IEEE, 2022, pp. 6972–6976.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] D. Guliani, L. Zhou, C. Ryu, T.-J. Yang, H. Zhang, Y. Xiao, F. Beaufays,
    G. Motta, Enabling on-device training of speech recognition models with federated
    dropout, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), IEEE, 2022, pp. 8757–8761.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] C. Chen, Y. Hu, N. Hou, X. Qi, H. Zou, E. S. Chng, Self-critical sequence
    training for automatic speech recognition, in: ICASSP 2022-2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp.
    3688–3692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] T. Kala, T. Shinozaki, Reinforcement learning of speech recognition system
    based on policy gradient and hypothesis selection, in: 2018 IEEE international
    conference on acoustics, speech and signal processing (ICASSP), IEEE, 2018, pp.
    5759–5763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A. Tjandra, S. Sakti, S. Nakamura, Sequence-to-sequence asr optimization
    via reinforcement learning, in: 2018 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), IEEE, 2018, pp. 5829–5833.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Tjandra, S. Sakti, S. Nakamura, End-to-end speech recognition sequence
    training with reinforcement learning, IEEE Access 7 (2019) 79758–79769.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Ł. Dudziak, M. S. Abdelfattah, R. Vipperla, S. Laskaridis, N. D. Lane,
    Shrinkml: End-to-end asr model compression using reinforcement learning, arXiv
    preprint arXiv:1907.03540 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] A. Mehrotra, Ł. Dudziak, J. Yeo, Y.-y. Lee, R. Vipperla, M. S. Abdelfattah,
    S. Bhattacharya, S. Ishtiaq, A. G. C. Ramos, S. Lee, et al., Iterative compression
    of end-to-end asr model using automl, arXiv preprint arXiv:2008.02897 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Y.-L. Shen, C.-Y. Huang, S.-S. Wang, Y. Tsao, H.-M. Wang, T.-S. Chi,
    Reinforcement learning based speech enhancement for robust speech recognition,
    in: ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), IEEE, 2019, pp. 6750–6754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] R. T.-H. Tsai, C.-H. Chen, C.-K. Wu, Y.-C. Hsiao, H.-y. Lee, Using deep-q
    network to select candidates from n-best speech recognition hypotheses for enhancing
    dialogue state tracking, in: ICASSP 2019-2019 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2019, pp. 7375–7379.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] H. Chung, H.-B. Jeon, J. G. Park, Semi-supervised training for sequence-to-sequence
    speech recognition using reinforcement learning, in: 2020 international joint
    conference on neural networks (IJCNN), IEEE, 2020, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Z. Chen, W. Zhang, End-to-end speech recognition with reinforcement learning,
    in: Eighth International Conference on Electronic Technology and Information Science
    (ICETIS 2023), Vol. 12715, SPIE, 2023, pp. 392–398.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. Hamza, D. Addou, H. Kheddar, Machine learning approaches for automated
    detection and classification of dysarthria severity, in: 2023 2nd International
    Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023,
    pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] S. Feng, B. M. Halpern, O. Kudina, O. Scharenborg, Towards inclusive
    automatic speech recognition, Computer Speech & Language 84 (2024) 101567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Z. Zhou, J. Chen, N. Wang, L. Li, D. Wang, Adversarial data augmentation
    for robust speaker verification, arXiv preprint arXiv:2402.02699 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Adastreamlite: Environment-adaptive streaming speech recognition on mobile
    devices, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
    Technologies 7 (4) (2024) 1–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. H. Yeo, M. Kim, J. Choi, D. H. Kim, Y. M. Ro, Akvsr: Audio knowledge
    empowered visual speech recognition by compressing audio knowledge of a pretrained
    model, IEEE Transactions on Multimedia (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] N. Djeffal, D. Addou, H. Kheddar, S. A. Selouani, Noise-robust speech
    recognition: A comparative analysis of lstm and cnn approaches, in: 2023 2nd International
    Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023,
    pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Q. Zhao, L. Yang, N. Lyu, A driver stress detection model via data augmentation
    based on deep convolutional recurrent neural network, Expert Systems with Applications
    238 (2024) 122056.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Z. Jin, M. Geng, J. Deng, T. Wang, S. Hu, G. Li, X. Liu, Personalized
    adversarial data augmentation for dysarthric and elderly speech recognition, IEEE/ACM
    Transactions on Audio, Speech, and Language Processing (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Brack, E. Entrup, M. Stamatakis, P. Buschermöhle, A. Hoppe, R. Ewerth,
    Sequential sentence classification in research papers using cross-domain multi-task
    learning, International Journal on Digital Libraries (2024) 1–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] H. Zhang, M. Tao, Y. Shi, X. Bi, K. B. Letaief, Federated multi-task
    learning with non-stationary and heterogeneous data in wireless networks, IEEE
    Transactions on Wireless Communications (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] A. Singh, S. Chandrasekar, T. Sen, S. Saha, Federated multi-task learning
    for complaint identification using graph attention network, IEEE Transactions
    on Artificial Intelligence (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] X. Jiang, J. Zhang, L. Zhang, Fedradar: Federated multi-task transfer
    learning for radar-based internet of medical things, IEEE Transactions on Network
    and Service Management (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] B. Azadi, M. Haslgrübler, B. Anzengruber-Tanase, G. Sopidis, A. Ferscha,
    Robust feature representation using multi-task learning for human activity recognition,
    Sensors 24 (2) (2024) 681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Ji, Z. Shu, H. Li, K. X. Lai, M. Lu, G. Jiang, W. Wang, Y. Zheng,
    X. Jiang, Edge-computing based knowledge distillation and multi-task learning
    for partial discharge recognition, IEEE Transactions on Instrumentation and Measurement
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] R. Šajina, N. Tanković, I. Ipšić, Multi-task peer-to-peer learning using
    an encoder-only transformer model, Future Generation Computer Systems 152 (2024)
    170–178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] C. Ye, H. Zheng, Z. Hu, M. Zheng, Pfedsa: Personalized federated multi-task
    learning via similarity awareness, in: 2023 IEEE International Parallel and Distributed
    Processing Symposium (IPDPS), IEEE, 2023, pp. 480–488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Z. Wang, C. Chen, D. Dong, Lifelong incremental reinforcement learning
    with online bayesian inference, IEEE Transactions on Neural Networks and Learning
    Systems 33 (8) (2021) 4003–4016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] J. Wang, J. Cao, S. Wang, Z. Yao, W. Li, Irda: Incremental reinforcement
    learning for dynamic resource allocation, IEEE Transactions on Big Data 8 (3)
    (2020) 770–783.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Z. Wang, C. Chen, H.-X. Li, D. Dong, T.-J. Tarn, Incremental reinforcement
    learning with prioritized sweeping for dynamic environments, IEEE/ASME Transactions
    on Mechatronics 24 (2) (2019) 621–632.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. Gueriani, H. Kheddar, A. C. Mazari, Deep reinforcement learning for
    intrusion detection in iot: A survey, in: 2023 2nd International Conference on
    Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] P. Zhao, S. C. Hoi, J. Wang, B. Li, Online transfer learning, Artificial
    intelligence 216 (2014) 76–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Q. Wu, H. Wu, X. Zhou, M. Tan, Y. Xu, Y. Yan, T. Hao, Online transfer
    learning with multiple homogeneous or heterogeneous sources, IEEE Transactions
    on Knowledge and Data Engineering 29 (7) (2017) 1494–1507.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] H. Wu, Y. Yan, Y. Ye, H. Min, M. K. Ng, Q. Wu, Online heterogeneous transfer
    learning by knowledge transition, ACM Transactions on Intelligent Systems and
    Technology (TIST) 10 (3) (2019) 1–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] R. Alasbahi, X. Zheng, An online transfer learning framework with extreme
    learning machine for automated credit scoring, IEEE Access 10 (2022) 46697–46716.
    [doi:10.1109/ACCESS.2022.3171569](https://doi.org/10.1109/ACCESS.2022.3171569).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
