- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:03:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:03:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1912.12033] Deep Learning for 3D Point Clouds: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1912.12033] 深度学习在3D点云中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.12033](https://ar5iv.labs.arxiv.org/html/1912.12033)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1912.12033](https://ar5iv.labs.arxiv.org/html/1912.12033)
- en: 'Deep Learning for 3D Point Clouds: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在3D点云中的应用：综述
- en: 'Yulan Guo^($\ast$), Hanyun Wang^($\ast$), Qingyong Hu^($\ast$), Hao Liu^($\ast$),
    Li Liu, and Mohammed Bennamoun Y Guo and H. Liu are with the School of Electronics
    and Communication Engineering, Sun Yat-sen University, China. H. Wang is with
    the School of Surveying and Mapping, Information Engineering University, China.
    Q. Hu is with Department of Computer Science, University of Oxford, UK. L. Liu
    is with the College of System Engineering, National University of Defense Technology,
    China, and also with the Center for Machine Vision and Signal Analysis, University
    of Oulu, Finland. M. Bennamoun is with the Department of Computer Science and
    Software Engineering, the University of Western Australia, Australia. *Y. Guo,
    H. Wang, Q. Hu and H. Liu have equal contribution to this work and are co-first
    authors. Corresponding author: Yulan Guo (yulan.guo@nudt.edu.cn).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yulan Guo^($\ast$)、Hanyun Wang^($\ast$)、Qingyong Hu^($\ast$)、Hao Liu^($\ast$)、Li
    Liu 和 Mohammed Bennamoun Y Guo 和 H. Liu 在中国中山大学电子与通信工程学院工作。H. Wang 在中国信息工程大学测绘与地图学院工作。Q.
    Hu 在英国牛津大学计算机科学系工作。L. Liu 在中国国防科技大学系统工程学院工作，同时也在芬兰奥卢大学机器视觉与信号分析中心工作。M. Bennamoun
    在澳大利亚西澳大学计算机科学与软件工程系工作。*Y. Guo, H. Wang, Q. Hu 和 H. Liu 对这项工作做出了同等贡献，并且是共同第一作者。通讯作者：Yulan
    Guo（yulan.guo@nudt.edu.cn）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Point cloud learning has lately attracted increasing attention due to its wide
    applications in many areas, such as computer vision, autonomous driving, and robotics.
    As a dominating technique in AI, deep learning has been successfully used to solve
    various 2D vision problems. However, deep learning on point clouds is still in
    its infancy due to the unique challenges faced by the processing of point clouds
    with deep neural networks. Recently, deep learning on point clouds has become
    even thriving, with numerous methods being proposed to address different problems
    in this area. To stimulate future research, this paper presents a comprehensive
    review of recent progress in deep learning methods for point clouds. It covers
    three major tasks, including 3D shape classification, 3D object detection and
    tracking, and 3D point cloud segmentation. It also presents comparative results
    on several publicly available datasets, together with insightful observations
    and inspiring future research directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其在计算机视觉、自动驾驶和机器人等众多领域的广泛应用，点云学习最近引起了越来越多的关注。作为人工智能的主导技术，深度学习已成功应用于解决各种2D视觉问题。然而，由于深度神经网络处理点云所面临的独特挑战，点云上的深度学习仍处于起步阶段。最近，点云上的深度学习已变得非常活跃，提出了许多方法以解决该领域的不同问题。为了激发未来的研究，本文对点云深度学习方法的最新进展进行了全面回顾。它涵盖了三项主要任务，包括3D形状分类、3D对象检测和跟踪，以及3D点云分割。同时，提供了在多个公开数据集上的比较结果，以及有洞察力的观察和启发性的未来研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: deep learning, point clouds, 3D data, shape classification, shape retrieval,
    object detection, object tracking, scene flow, instance segmentation, semantic
    segmentation, part segmentation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，点云，3D数据，形状分类，形状检索，对象检测，对象跟踪，场景流，实例分割，语义分割，部件分割。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the rapid development of 3D acquisition technologies, 3D sensors are becoming
    increasingly available and affordable, including various types of 3D scanners,
    LiDARs, and RGB-D cameras (such as Kinect, RealSense and Apple depth cameras)
    [[1](#bib.bib1)]. 3D data acquired by these sensors can provide rich geometric,
    shape and scale information [[2](#bib.bib2), [3](#bib.bib3)]. Complemented with
    2D images, 3D data provides an opportunity for a better understanding of the surrounding
    environment for machines. 3D data has numerous applications in different areas,
    including autonomous driving, robotics, remote sensing, and medical treatment
    [[4](#bib.bib4)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着3D采集技术的迅速发展，3D传感器变得越来越普及和经济实惠，包括各种类型的3D扫描仪、激光雷达（LiDAR）和RGB-D摄像头（如Kinect、RealSense和Apple深度摄像头）[[1](#bib.bib1)]。这些传感器获取的3D数据可以提供丰富的几何、形状和尺度信息[[2](#bib.bib2),
    [3](#bib.bib3)]。结合2D图像，3D数据为机器提供了更好理解周围环境的机会。3D数据在不同领域有着广泛的应用，包括自动驾驶、机器人技术、遥感和医疗治疗[[4](#bib.bib4)]。
- en: 3D data can usually be represented with different formats, including depth images,
    point clouds, meshes, and volumetric grids. As a commonly used format, point cloud
    representation preserves the original geometric information in 3D space without
    any discretization. Therefore, it is the preferred representation for many scene
    understanding related applications such as autonomous driving and robotics. Recently,
    deep learning techniques have dominated many research areas, such as computer
    vision, speech recognition, and natural language processing. However, deep learning
    on 3D point clouds still face several significant challenges [[5](#bib.bib5)],
    such as the small scale of datasets, the high dimensionality and the unstructured
    nature of 3D point clouds. On this basis, this paper focuses on the analysis of
    deep learning methods which have been used to process 3D point clouds.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 3D数据通常可以用不同的格式表示，包括深度图像、点云、网格和体积网格。作为一种常用格式，点云表示保留了3D空间中的原始几何信息，而没有任何离散化。因此，它是许多场景理解相关应用（如自动驾驶和机器人技术）的首选表示方式。近年来，深度学习技术主导了许多研究领域，如计算机视觉、语音识别和自然语言处理。然而，3D点云的深度学习仍面临若干重大挑战[[5](#bib.bib5)]，如数据集规模小、高维性和3D点云的无结构性。在此基础上，本文集中分析了用于处理3D点云的深度学习方法。
- en: 'Deep learning on point clouds has been attracting more and more attention,
    especially in the last five years. Several publicly available datasets are also
    released, such as ModelNet [[6](#bib.bib6)], ScanObjectNN [[7](#bib.bib7)], ShapeNet
    [[8](#bib.bib8)], PartNet [[9](#bib.bib9)], S3DIS [[10](#bib.bib10)], ScanNet
    [[11](#bib.bib11)], Semantic3D [[12](#bib.bib12)], ApolloCar3D [[13](#bib.bib13)],
    and the KITTI Vision Benchmark Suite [[14](#bib.bib14), [15](#bib.bib15)]. These
    datasets have further boosted the research of deep learning on 3D point clouds,
    with an increasingly number of methods being proposed to address various problems
    related to point cloud processing, including 3D shape classification, 3D object
    detection and tracking, 3D point cloud segmentation, 3D point cloud registration,
    6-DOF pose estimation, and 3D reconstruction [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18)]. Few surveys of deep learning on 3D data are also available,
    such as [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)].
    However, our paper is the first to specifically focus on deep learning methods
    for point cloud understanding. A taxonomy of existing deep learning methods for
    3D point clouds is shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep
    Learning for 3D Point Clouds: A Survey").'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '对点云的深度学习在过去五年里越来越受到关注。也发布了若干公开可用的数据集，如ModelNet [[6](#bib.bib6)]、ScanObjectNN
    [[7](#bib.bib7)]、ShapeNet [[8](#bib.bib8)]、PartNet [[9](#bib.bib9)]、S3DIS [[10](#bib.bib10)]、ScanNet
    [[11](#bib.bib11)]、Semantic3D [[12](#bib.bib12)]、ApolloCar3D [[13](#bib.bib13)]，以及KITTI
    Vision Benchmark Suite [[14](#bib.bib14), [15](#bib.bib15)]。这些数据集进一步推动了对3D点云深度学习的研究，提出了越来越多的方法来解决与点云处理相关的各种问题，包括3D形状分类、3D物体检测与跟踪、3D点云分割、3D点云配准、6自由度姿态估计和3D重建[[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18)]。也有一些关于3D数据深度学习的综述，如[[19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]。然而，本文是首个专门关注点云理解的深度学习方法的研究。现有的3D点云深度学习方法的分类见图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep Learning for 3D Point Clouds: A Survey")。'
- en: 'Compared with the existing literatures, the major contributions of this work
    can be summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有文献相比，本研究的主要贡献可以总结如下：
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To the best of our knowledge, this is the first survey paper to comprehensively
    cover deep learning methods for several important point cloud understanding tasks,
    including 3D shape classification, 3D object detection and tracking, and 3D point
    cloud segmentation.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一篇全面涵盖多个重要点云理解任务的深度学习方法的综述论文，包括3D形状分类、3D目标检测与跟踪以及3D点云分割。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: As opposed to existing reviews [[20](#bib.bib20), [19](#bib.bib19)], we specifically
    focus on deep learning methods for 3D point clouds rather than all types of 3D
    data.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与现有的综述 [[20](#bib.bib20), [19](#bib.bib19)] 相比，我们特别关注3D点云的深度学习方法，而非所有类型的3D数据。
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: This paper covers the most recent and advanced progresses of deep learning on
    point clouds. Therefore, it provides the readers with the state-of-the-art methods.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文涵盖了深度学习在点云上的最新和最先进进展。因此，为读者提供了最前沿的方法。
- en: '4.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Comprehensive comparisons of existing methods on several publicly available
    datasets are provided (e.g., in Tables [II](#S3.T2 "TABLE II ‣ 3.3.4 Hierarchical
    Data Structure-based Methods ‣ 3.3 Point-based Methods ‣ 3 3D Shape Classification
    ‣ Deep Learning for 3D Point Clouds: A Survey"), [III](#S4.T3 "TABLE III ‣ 4.1.1
    Region Proposal-based Methods ‣ 4.1 3D Object Detection ‣ 4 3D Object Detection
    and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey"), [IV](#S4.T4 "TABLE
    IV ‣ 4.1.2 Single Shot Methods ‣ 4.1 3D Object Detection ‣ 4 3D Object Detection
    and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey"), [V](#S5.T5 "TABLE
    V ‣ 5.1.4 Point-based Methods ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point Cloud
    Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")), with brief summaries
    and insightful discussions being presented.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对几个公开数据集上现有方法的全面比较已提供（例如，见表格 [II](#S3.T2 "TABLE II ‣ 3.3.4 基于层次数据结构的方法 ‣ 3.3
    基于点的方法 ‣ 3 3D 形状分类 ‣ 3D 点云的深度学习：综述")、[III](#S4.T3 "TABLE III ‣ 4.1.1 区域提议方法 ‣
    4.1 3D 目标检测 ‣ 4 3D 目标检测与跟踪 ‣ 3D 点云的深度学习：综述")、[IV](#S4.T4 "TABLE IV ‣ 4.1.2 单次检测方法
    ‣ 4.1 3D 目标检测 ‣ 4 3D 目标检测与跟踪 ‣ 3D 点云的深度学习：综述")、[V](#S5.T5 "TABLE V ‣ 5.1.4 基于点的方法
    ‣ 5.1 3D 语义分割 ‣ 5 3D 点云分割 ‣ 3D 点云的深度学习：综述")），提供了简要总结和有见地的讨论。
- en: '![Refer to caption](img/3a4c77489e36e767792a1b55671037c4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3a4c77489e36e767792a1b55671037c4.png)'
- en: 'Figure 1: A taxonomy of deep learning methods for 3D point clouds.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：3D 点云深度学习方法的分类。
- en: 'TABLE I: A summary of existing datasets for 3D shape classification, 3D object
    detection and tracking, and 3D point cloud segmentation. ¹ The number of classes
    used for evaluation and the number of annotated classes (shown in brackets).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：现有数据集的总结，包括3D形状分类、3D目标检测与跟踪以及3D点云分割。¹ 用于评估的类别数和注释类别数（括号中显示）。
- en: '| Datasets for 3D Shape Classification |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 3D形状分类的数据集 |'
- en: '| --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Name and Reference | Year | #Samples | #Classes | #Training | #Test | Type
    | Representation |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 名称与参考文献 | 年份 | 样本数 | 类别数 | 训练集数量 | 测试集数量 | 类型 | 表示方式 |'
- en: '| McGill Benchmark[[23](#bib.bib23)] | 2008 | 456 | 19 | 304 | 152 | Synthetic
    | Mesh |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| McGill Benchmark[[23](#bib.bib23)] | 2008 | 456 | 19 | 304 | 152 | 合成 | 网格
    |'
- en: '| Sydney Urban Objects[[24](#bib.bib24)] | 2013 | 588 | 14 | - | - | Real-World
    | Point Clouds |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 悉尼城市物体[[24](#bib.bib24)] | 2013 | 588 | 14 | - | - | 真实世界 | 点云 |'
- en: '| ModelNet10[[6](#bib.bib6)] | 2015 | 4899 | 10 | 3991 | 605 | Synthetic |
    Mesh |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet10[[6](#bib.bib6)] | 2015 | 4899 | 10 | 3991 | 605 | 合成 | 网格 |'
- en: '| ModelNet40[[6](#bib.bib6)] | 2015 | 12311 | 40 | 9843 | 2468 | Synthetic
    | Mesh |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet40[[6](#bib.bib6)] | 2015 | 12311 | 40 | 9843 | 2468 | 合成 | 网格 |'
- en: '| ShapeNet[[8](#bib.bib8)] | 2015 | 51190 | 55 | - | - | Synthetic | Mesh |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet[[8](#bib.bib8)] | 2015 | 51190 | 55 | - | - | 合成 | 网格 |'
- en: '| ScanNet[[11](#bib.bib11)] | 2017 | 12283 | 17 | 9677 | 2606 | Real-World
    | RGB-D |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet[[11](#bib.bib11)] | 2017 | 12283 | 17 | 9677 | 2606 | 真实世界 | RGB-D
    |'
- en: '| ScanObjectNN[[7](#bib.bib7)] | 2019 | 2902 | 15 | 2321 | 581 | Real-World
    | Point Clouds |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ScanObjectNN[[7](#bib.bib7)] | 2019 | 2902 | 15 | 2321 | 581 | 真实世界 | 点云
    |'
- en: '| Datasets for 3D Object Detection and Tracking |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 3D目标检测与跟踪的数据集'
- en: '| Name and Reference | Year | #Scenes | #Classes | #Annotated Frames | #3D
    Boxes | Secne Type | Sensors |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 名称与参考文献 | 年份 | 场景数 | 类别数 | 注释帧数量 | 3D框数量 | 场景类型 | 传感器 |'
- en: '| KITTI [[14](#bib.bib14)] | 2012 | 22 | 8 | 15K | 200K | Urban (Driving) |
    RGB & LiDAR |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| KITTI [[14](#bib.bib14)] | 2012 | 22 | 8 | 15K | 200K | 城市（驾驶） | RGB & LiDAR
    |'
- en: '| SUN RGB-D [[25](#bib.bib25)] | 2015 | 47 | 37 | 5K | 65K | Indoor | RGB-D
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGB-D [[25](#bib.bib25)] | 2015 | 47 | 37 | 5K | 65K | 室内 | RGB-D |'
- en: '| ScanNetV2 [[11](#bib.bib11)] | 2018 | 1.5K | 18 | - | - | Indoor | RGB-D
    & Mesh |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ScanNetV2 [[11](#bib.bib11)] | 2018 | 1.5K | 18 | - | - | 室内 | RGB-D & 网格
    |'
- en: '| H3D [[26](#bib.bib26)] | 2019 | 160 | 8 | 27K | 1.1M | Urban (Driving) |
    RGB & LiDAR |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| H3D [[26](#bib.bib26)] | 2019 | 160 | 8 | 27K | 1.1M | 城市（驾驶） | RGB & LiDAR
    |'
- en: '| Argoverse [[27](#bib.bib27)] | 2019 | 113 | 15 | 44K | 993K | Urban (Driving)
    | RGB & LiDAR |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Argoverse [[27](#bib.bib27)] | 2019 | 113 | 15 | 44K | 993K | 城市（驾驶） | RGB
    & LiDAR |'
- en: '| Lyft L5 [[28](#bib.bib28)] | 2019 | 366 | 9 | 46K | 1.3M | Urban (Driving)
    | RGB & LiDAR |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Lyft L5 [[28](#bib.bib28)] | 2019 | 366 | 9 | 46K | 1.3M | 城市（驾驶） | RGB &
    LiDAR |'
- en: '| A*3D [[29](#bib.bib29)] | 2019 | - | 7 | 39K | 230K | Urban (Driving) | RGB
    & LiDAR |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| A*3D [[29](#bib.bib29)] | 2019 | - | 7 | 39K | 230K | 城市（驾驶） | RGB & LiDAR
    |'
- en: '| Waymo Open [[30](#bib.bib30)] | 2020 | 1K | 4 | 200K | 12M | Urban (Driving)
    | RGB & LiDAR |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Waymo Open [[30](#bib.bib30)] | 2020 | 1K | 4 | 200K | 12M | 城市（驾驶） | RGB
    & LiDAR |'
- en: '| nuScenes [[31](#bib.bib31)] | 2020 | 1K | 23 | 40K | 1.4M | Urban (Driving)
    | RGB & LiDAR |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| nuScenes [[31](#bib.bib31)] | 2020 | 1K | 23 | 40K | 1.4M | 城市（驾驶） | RGB
    & LiDAR |'
- en: '| Datasets for 3D Point Cloud Segmentation |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 3D点云分割数据集 |'
- en: '| Name and Reference | Year | #Points | #Classes¹ | #Scans | Spatial Size |
    RGB | Sensors |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 名称和参考 | 年份 | 点数 | 类别¹ | 扫描数 | 空间大小 | RGB | 传感器 |'
- en: '| Oakland[[32](#bib.bib32)] | 2009 | 1.6M | 5(44) | 17 | - | N/A | MLS |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Oakland[[32](#bib.bib32)] | 2009 | 1.6M | 5(44) | 17 | - | 不适用 | MLS |'
- en: '| ISPRS[[33](#bib.bib33)] | 2012 | 1.2M | 9 | - | - | N/A | ALS |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| ISPRS[[33](#bib.bib33)] | 2012 | 1.2M | 9 | - | - | 不适用 | ALS |'
- en: '| Paris-rue-Madame[[34](#bib.bib34)] | 2014 | 20M | 17 | 2 | - | N/A | MLS
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Paris-rue-Madame[[34](#bib.bib34)] | 2014 | 20M | 17 | 2 | - | 不适用 | MLS
    |'
- en: '| IQmulus[[35](#bib.bib35)] | 2015 | 300M | 8(22) | 10 | - | N/A | MLS |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| IQmulus[[35](#bib.bib35)] | 2015 | 300M | 8(22) | 10 | - | 不适用 | MLS |'
- en: '| ScanNet[[11](#bib.bib11)] | 2017 | - | 20(20) | 1513 | 8$\times$4$\times$4
    | Yes | RGB-D |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet[[11](#bib.bib11)] | 2017 | - | 20(20) | 1513 | 8$\times$4$\times$4
    | 是 | RGB-D |'
- en: '| S3DIS[[10](#bib.bib10)] | 2017 | 273M | 13(13) | 272 | 10$\times$5$\times$5
    | Yes | Matterport |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| S3DIS[[10](#bib.bib10)] | 2017 | 273M | 13(13) | 272 | 10$\times$5$\times$5
    | 是 | Matterport |'
- en: '| Semantic3D[[12](#bib.bib12)] | 2017 | 4000M | 8(9) | 15/15 | 250$\times$260$\times$80
    | Yes | TLS |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Semantic3D[[12](#bib.bib12)] | 2017 | 4000M | 8(9) | 15/15 | 250$\times$260$\times$80
    | 是 | TLS |'
- en: '| Paris-Lille-3D[[36](#bib.bib36)] | 2018 | 143M | 9(50) | 3 | 200$\times$280$\times$
    30 | N/A | MLS |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Paris-Lille-3D[[36](#bib.bib36)] | 2018 | 143M | 9(50) | 3 | 200$\times$280$\times$
    30 | 不适用 | MLS |'
- en: '| SemanticKITTI[[15](#bib.bib15)] | 2019 | 4549M | 25(28) | 23201/20351 | 150$\times$100$\times$10
    | N/A | MLS |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SemanticKITTI[[15](#bib.bib15)] | 2019 | 4549M | 25(28) | 23201/20351 | 150$\times$100$\times$10
    | 不适用 | MLS |'
- en: '| Toronto-3D[[37](#bib.bib37)] | 2020 | 78.3M | 8(9) | 4 | 260$\times$350$\times$
    40 | Yes | MLS |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Toronto-3D[[37](#bib.bib37)] | 2020 | 78.3M | 8(9) | 4 | 260$\times$350$\times$
    40 | 是 | MLS |'
- en: '| DALES[[38](#bib.bib38)] | 2020 | 505M | 8(9) | 40 | 500$\times$500$\times$65
    | N/A | ALS |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| DALES[[38](#bib.bib38)] | 2020 | 505M | 8(9) | 40 | 500$\times$500$\times$65
    | 不适用 | ALS |'
- en: 'The structure of this paper is as follows. Section [2](#S2 "2 Background ‣
    Deep Learning for 3D Point Clouds: A Survey") introduces the datasets and evaluation
    metrics for the respective tasks. Section [3](#S3 "3 3D Shape Classification ‣
    Deep Learning for 3D Point Clouds: A Survey") reviews the methods for 3D shape
    classification. Section [4](#S4 "4 3D Object Detection and Tracking ‣ Deep Learning
    for 3D Point Clouds: A Survey") provides a survey of existing methods for 3D object
    detection and tracking. Section [5](#S5 "5 3D Point Cloud Segmentation ‣ Deep
    Learning for 3D Point Clouds: A Survey") presents a review of methods for point
    cloud segmentation, including semantic segmentation, instance segmentation, and
    part segmentation. Finally, Section [6](#S6 "6 Conclusion ‣ Deep Learning for
    3D Point Clouds: A Survey") concludes the paper. We also provide a regularly updated
    project page on: [https://github.com/QingyongHu/SoTA-Point-Cloud](https://github.com/QingyongHu/SoTA-Point-Cloud).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '本论文的结构如下。第[2](#S2 "2 Background ‣ Deep Learning for 3D Point Clouds: A Survey")节介绍了各自任务的数据集和评估指标。第[3](#S3
    "3 3D Shape Classification ‣ Deep Learning for 3D Point Clouds: A Survey")节回顾了3D形状分类的方法。第[4](#S4
    "4 3D Object Detection and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey")节提供了现有3D对象检测和跟踪方法的综述。第[5](#S5
    "5 3D Point Cloud Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")节介绍了点云分割方法的回顾，包括语义分割、实例分割和部件分割。最后，第[6](#S6
    "6 Conclusion ‣ Deep Learning for 3D Point Clouds: A Survey")节总结了本文。我们还提供了一个定期更新的项目页面：[https://github.com/QingyongHu/SoTA-Point-Cloud](https://github.com/QingyongHu/SoTA-Point-Cloud)。'
- en: 2 Background
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Datasets
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据集
- en: 'A large number of datasets have been collected to evaluate the performance
    of deep learning algorithms for different 3D point clouds applications. Table
    [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning for 3D Point Clouds: A Survey")
    lists some typical datasets used for 3D shape classification, 3D object detection
    and tracking, and 3D point cloud segmentation. In particular, the attributes of
    these datasets are also summarized.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '收集了大量数据集以评估深度学习算法在不同 3D 点云应用中的性能。表格 [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣
    Deep Learning for 3D Point Clouds: A Survey") 列出了一些典型的数据集，这些数据集用于 3D 形状分类、3D 物体检测与跟踪以及
    3D 点云分割。特别是，这些数据集的属性也进行了总结。'
- en: 'For 3D shape classification, there are two types of datasets: synthetic datasets
    [[6](#bib.bib6), [8](#bib.bib8)] and real-world datasets [[11](#bib.bib11), [7](#bib.bib7)].
    Objects in the synthetic datasets are complete, without any occlusion and background.
    In contrast, objects in the real-world datasets are occluded at different levels
    and some objects are contaminated with background noise.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 3D 形状分类，有两种类型的数据集：合成数据集 [[6](#bib.bib6), [8](#bib.bib8)] 和真实世界数据集 [[11](#bib.bib11),
    [7](#bib.bib7)]。合成数据集中的物体是完整的，没有任何遮挡和背景。相比之下，真实世界数据集中的物体在不同程度上被遮挡，并且一些物体受到背景噪声的污染。
- en: 'For 3D object detection and tracking, there are two types of datasets: indoor
    scenes [[25](#bib.bib25), [11](#bib.bib11)] and outdoor urban scenes [[14](#bib.bib14),
    [28](#bib.bib28), [31](#bib.bib31), [30](#bib.bib30)]. The point clouds in the
    indoor datasets are either converted from dense depth maps or sampled from 3D
    meshes. The outdoor urban datasets are designed for autonomous driving, where
    objects are spatially well separated and these point clouds are sparse.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 3D 物体检测和跟踪，有两种类型的数据集：室内场景 [[25](#bib.bib25), [11](#bib.bib11)] 和户外城市场景 [[14](#bib.bib14),
    [28](#bib.bib28), [31](#bib.bib31), [30](#bib.bib30)]。室内数据集中的点云要么是从密集的深度图转换而来，要么是从
    3D 网格中采样的。户外城市数据集则设计用于自动驾驶，其中物体在空间上分隔良好，这些点云是稀疏的。
- en: For 3D point cloud segmentation, these datasets are acquired by different types
    of sensors, including Mobile Laser Scanners (MLS) [[15](#bib.bib15), [34](#bib.bib34),
    [36](#bib.bib36)], Aerial Laser Scanners (ALS) [[33](#bib.bib33), [38](#bib.bib38)],
    static Terrestrial Laser Scanners (TLS) [[12](#bib.bib12)], RGB-D cameras [[11](#bib.bib11)]
    and other 3D scanners [[10](#bib.bib10)]. These datasets can be used to develop
    algorithms for various challenges including similar distractors, shape incompleteness,
    and class imbalance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 3D 点云分割，这些数据集是通过不同类型的传感器获取的，包括移动激光扫描仪（MLS） [[15](#bib.bib15), [34](#bib.bib34),
    [36](#bib.bib36)]，空中激光扫描仪（ALS） [[33](#bib.bib33), [38](#bib.bib38)]，静态地面激光扫描仪（TLS）
    [[12](#bib.bib12)]，RGB-D 摄像头 [[11](#bib.bib11)] 和其他 3D 扫描仪 [[10](#bib.bib10)]。这些数据集可用于开发各种挑战的算法，包括类似的干扰物、形状不完整性和类别不平衡。
- en: 2.2 Evaluation Metrics
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 评估指标
- en: Different evaluation metrics have been proposed to test these methods for various
    point cloud understanding tasks.  For 3D shape classification, Overall Accuracy
    (OA) and mean class accuracy (mAcc) are the most frequently used performance criteria.
    ‘OA’ represents the mean accuracy for all test instances and ‘mAcc’ represents
    the mean accuracy for all shape classes. For 3D object detection, Average Precision
    (AP) is the most frequently used criterion. It is calculated as the area under
    the precision-recall curve. Precision and Success are commonly used to evaluate
    the overall performance of a 3D single object tracker. Average Multi-Object Tracking
    Accuracy (AMOTA) and Average Multi-Object Tracking Precision (AMOTP) are the most
    frequently used criteria for the evaluation of 3D multi-object tracking.  For
    3D point cloud segmentation, OA, mean Intersection over Union (mIoU) and mean
    class Accuracy (mAcc) [[10](#bib.bib10), [12](#bib.bib12), [36](#bib.bib36), [15](#bib.bib15),
    [37](#bib.bib37)] are the most frequently used criteria for performance evaluation.
    In particular, mean Average Precision (mAP) [[39](#bib.bib39)] is also used in
    instance segmentation of 3D point clouds.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了不同的评估指标来测试这些方法在各种点云理解任务中的表现。对于3D形状分类，整体准确率（OA）和平均类准确率（mAcc）是最常用的性能标准。‘OA’代表所有测试实例的平均准确率，而‘mAcc’代表所有形状类别的平均准确率。对于3D目标检测，平均精度（AP）是最常用的标准。它通过精度-召回曲线下的面积来计算。精度和成功率常用于评估3D单目标跟踪的整体性能。平均多目标跟踪准确率（AMOTA）和平均多目标跟踪精度（AMOTP）是评估3D多目标跟踪最常用的标准。对于3D点云分割，OA、平均交并比（mIoU）和平均类准确率（mAcc）[[10](#bib.bib10),
    [12](#bib.bib12), [36](#bib.bib36), [15](#bib.bib15), [37](#bib.bib37)]是性能评估最常用的标准。特别地，平均平均精度（mAP）[[39](#bib.bib39)]也用于3D点云的实例分割。
- en: 3 3D Shape Classification
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 3D形状分类
- en: 'Methods for this task usually learn the embedding of each point first and then
    extract a global shape embedding from the whole point cloud using an aggregation
    method. Classification is finally achieved by feeding the global embedding into
    several fully connected layers. According to the data type of input for neural
    networks, existing 3D shape classification methods can be divided into multi-view
    based, volumetric-based and point-based methods. Several milestone methods are
    illustrated in Fig. [2](#S3.F2 "Figure 2 ‣ 3 3D Shape Classification ‣ Deep Learning
    for 3D Point Clouds: A Survey").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '这种任务的方法通常先学习每个点的嵌入，然后使用聚合方法从整个点云中提取全局形状嵌入。最终，通过将全局嵌入输入到几个全连接层中实现分类。根据神经网络输入的数据类型，现有的3D形状分类方法可以分为基于多视角、基于体积和基于点的方法。图[2](#S3.F2
    "Figure 2 ‣ 3 3D Shape Classification ‣ Deep Learning for 3D Point Clouds: A Survey")展示了几个里程碑式的方法。'
- en: Multi-view based methods project an unstructured point cloud into 2D images,
    while volumetric-based methods convert a point cloud into a 3D volumetric representation.
    Then, well-established 2D or 3D convolutional networks are leveraged to achieve
    shape classification. In contrast, point-based methods directly work on raw point
    clouds without any voxelization or projection. Point-based methods do not introduce
    explicit information loss and become increasingly popular. Note that, this paper
    mainly focuses on point-based methods, but also includes few multi-view based
    and volumetric-based methods for completeness.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 基于多视角的方法将非结构化点云投影到2D图像中，而基于体积的方法则将点云转换为3D体积表示。然后，利用成熟的2D或3D卷积网络实现形状分类。相比之下，基于点的方法直接在原始点云上进行操作，不进行任何体素化或投影。基于点的方法不会引入明显的信息损失，且变得越来越受欢迎。请注意，本文主要关注基于点的方法，但也包含一些基于多视角和基于体积的方法以保证完整性。
- en: '![Refer to caption](img/a6a4e979a16e0b93da0f9d1e760588fd.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6a4e979a16e0b93da0f9d1e760588fd.png)'
- en: 'Figure 2: Chronological overview of the most relevant deep learning-based 3D
    shape classification methods.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的3D形状分类方法的时间概述。
- en: 3.1 Multi-view based Methods
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于多视角的方法
- en: These methods first project a 3D shape into multiple views and extract view-wise
    features, and then fuse these features for accurate shape classification. How
    to aggregate multiple view-wise features into a discriminative global representation
    is a key challenge for these methods.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法首先将3D形状投影到多个视角中并提取视角特征，然后融合这些特征以实现准确的形状分类。如何将多个视角特征聚合成具有区分性的全局表示是这些方法面临的关键挑战。
- en: MVCNN [[40](#bib.bib40)] is a pioneering work, which simply max-pools multi-view
    features into a global descriptor. However, max-pooling only retains the maximum
    elements from a specific view, resulting in information loss. MHBN [[41](#bib.bib41)]
    integrates local convolutional features by harmonized bilinear pooling to produce
    a compact global descriptor. Yang et al. [[42](#bib.bib42)] first leveraged a
    relation network to exploit the inter-relationships (e.g., region-region relationship
    and view-view relationship) over a group of views, and then aggregated these views
    to obtain a discriminative 3D object representation. In addition, several other
    methods [[43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)]
    have also been proposed to improve the recognition accuracy. Unlike previous methods,
    Wei et al. [[47](#bib.bib47)] used a directed graph in View-GCN by considering
    multiple views as grpah nodes. The core layer composing of local graph convolution,
    non-local message passing and selective view-sampling is then applied to the constructed
    graph. The concatenation of max-pooled node features at all levels is finally
    used to form the global shape descriptor.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: MVCNN [[40](#bib.bib40)] 是一项开创性的工作，它将多视角特征通过最大池化简化为全局描述符。然而，最大池化只保留来自特定视角的最大元素，导致信息丢失。MHBN
    [[41](#bib.bib41)] 通过协调双线性池化整合局部卷积特征，以生成紧凑的全局描述符。杨等人 [[42](#bib.bib42)] 首次利用关系网络来挖掘一组视角之间的相互关系（例如，区域-区域关系和视角-视角关系），然后将这些视角聚合以获得具有判别性的3D对象表示。此外，还提出了若干其他方法
    [[43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)] 来提高识别精度。与之前的方法不同，魏等人
    [[47](#bib.bib47)] 在 View-GCN 中使用了有向图，将多个视角视为图节点。核心层由局部图卷积、非局部消息传递和选择性视角采样组成，随后应用于构建的图。最后，将所有层级的最大池化节点特征连接起来，形成全局形状描述符。
- en: 3.2 Volumetric-based Methods
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于体积的方法
- en: These methods usually voxelize a point cloud into 3D grids, and then apply a
    3D Convolution Neural Network (CNN) on the volumetric representation for shape
    classification.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法通常将点云体素化为3D网格，然后在体积表示上应用3D卷积神经网络（CNN）进行形状分类。
- en: Maturana et al. [[48](#bib.bib48)] introduced a volumetric occupancy network
    called VoxNet to achieve robust 3D object recognition. Wu et al. [[6](#bib.bib6)]
    proposed a convolutional deep belief-based 3D ShapeNets to learn the distribution
    of points from various 3D shapes (which are represented by a probability distribution
    of binary variables on voxel grids). Although encouraging performance has been
    achieved, these methods are unable to scale well to dense 3D data since the computation
    and memory footprint grow cubically with the resolution.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 马图拉纳等人 [[48](#bib.bib48)] 引入了一种称为 VoxNet 的体积占用网络，以实现稳健的3D对象识别。吴等人 [[6](#bib.bib6)]
    提出了基于卷积深度置信的3D ShapeNets，学习来自各种3D形状的点的分布（这些形状由体素网格上的二元变量概率分布表示）。尽管取得了令人鼓舞的性能，但这些方法在处理密集3D数据时无法很好地扩展，因为计算和内存占用随着分辨率的立方增长。
- en: To this end, a hierarchical and compact structure (such as octree) is introduced
    to reduce the computational and memory costs of these methods. OctNet [[49](#bib.bib49)]
    first hierarchically partitions a point cloud using a hybrid grid-octree structure,
    which represents the scene with several shallow octrees along a regular grid.
    The structure of octree is encoded efficiently using a bit string representation,
    and the feature vector of each voxel is indexed by simple arithmetic. Wang et
    al. [[50](#bib.bib50)] proposed an Octree-based CNN for 3D shape classification.
    The average normal vectors of a 3D model sampled in the finest leaf octants are
    fed into the network, and 3D-CNN is applied on the octants occupied by the 3D
    shape surface. Compared to a baseline network based on dense input grids, OctNet
    requires much less memory and runtime for high-resolution point clouds. Le et
    al. [[51](#bib.bib51)] proposed a hybrid network called PointGrid, which integrates
    the point and grid representation for efficient point cloud processing. A constant
    number of points is sampled within each embedding volumetric grid cell, which
    allows the network to extract geometric details by using 3D convolutions. Ben-Shabat
    et al. [[52](#bib.bib52)] transformed the input point cloud into 3D grids which
    are further represented by 3D modified Fisher Vector (3DmFV) method, and then
    learned the global representation through a conventional CNN architecture.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，引入了一种分层和紧凑的结构（如八叉树）以减少这些方法的计算和内存开销。OctNet [[49](#bib.bib49)] 首先使用混合网格-八叉树结构分层划分点云，这种结构通过沿规则网格的几个浅层八叉树来表示场景。八叉树的结构使用位字符串表示法有效地编码，每个体素的特征向量通过简单的算术索引。Wang
    等人 [[50](#bib.bib50)] 提出了一个基于八叉树的 CNN 用于 3D 形状分类。将最精细叶八叉体中采样的 3D 模型的平均法向量输入网络，并在
    3D 形状表面占据的八叉体上应用 3D-CNN。与基于稠密输入网格的基线网络相比，OctNet 对于高分辨率点云的内存和运行时间要求要少得多。Le 等人 [[51](#bib.bib51)]
    提出了一个名为 PointGrid 的混合网络，该网络整合了点和网格表示以进行高效的点云处理。每个嵌入体积网格单元内采样固定数量的点，这使得网络能够通过使用
    3D 卷积提取几何细节。Ben-Shabat 等人 [[52](#bib.bib52)] 将输入点云转换为 3D 网格，进一步由 3D 修改 Fisher
    向量（3DmFV）方法表示，然后通过传统的 CNN 架构学习全局表示。
- en: 3.3 Point-based Methods
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于点的方法
- en: According to the network architecture used for the feature learning of each
    point, methods in this category can be divided into pointwise MLP, convolution-based,
    graph-based, hierarchical data structure-based methods and other typical methods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用于每个点特征学习的网络架构，这一类方法可以分为点状 MLP、基于卷积、基于图、基于分层数据结构的方法以及其他典型方法。
- en: 3.3.1 Pointwise MLP Methods
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 点状 MLP 方法
- en: 'These methods model each point independently with several shared Multi-Layer
    Perceptrons (MLPs) and then aggregate a global feature using a symmetric aggregation
    function, as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.3.1 Pointwise MLP Methods
    ‣ 3.3 Point-based Methods ‣ 3 3D Shape Classification ‣ Deep Learning for 3D Point
    Clouds: A Survey").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法使用几个共享的多层感知器（MLPs）独立建模每个点，然后通过对称聚合函数汇总全局特征，如图 [3](#S3.F3 "图 3 ‣ 3.3.1 点状
    MLP 方法 ‣ 3.3 基于点的方法 ‣ 3 3D 形状分类 ‣ 深度学习用于 3D 点云：综述") 所示。
- en: '![Refer to caption](img/936f3c4814f0ca2b6d4d7a051521f9c7.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/936f3c4814f0ca2b6d4d7a051521f9c7.png)'
- en: 'Figure 3: A lightweight architecture of PointNet. $n$ denotes the number of
    input points, $M$ denotes the dimension of the learned features for each point.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：PointNet 的轻量级架构。$n$ 表示输入点的数量，$M$ 表示每个点学习到的特征的维度。
- en: 'Typical deep learning methods for 2D images cannot be directly applied to 3D
    point clouds due to their inherent data irregularities. As a pioneering work,
    PointNet [[5](#bib.bib5)] directly takes point clouds as its input and achieves
    permutation invariance with a symmetric function. Specifically, PointNet learns
    pointwise features independently with several MLP layers and extracts global features
    with a max-pooling layer. Deep sets [[53](#bib.bib53)] achieves permutation invariance
    by summing up all representations and applying nonlinear transformations. Since
    features are learned independently for each point in PointNet [[5](#bib.bib5)],
    the local structural information between points cannot be captured. Therefore,
    Qi et al. [[54](#bib.bib54)] proposed a hierarchical network PointNet++ to capture
    fine geometric structures from the neighborhood of each point. As the core of
    PointNet++ hierarchy, its set abstraction level is composed of three layers: the
    sampling layer, the grouping layer and the PointNet based learning layer. By stacking
    several set abstraction levels, PointNet++ learns features from a local geometric
    structure and abstracts the local features layer by layer.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的2D图像深度学习方法无法直接应用于3D点云，因为它们固有的数据不规则性。作为开创性工作，PointNet [[5](#bib.bib5)] 直接将点云作为输入，并通过对称函数实现排列不变性。具体而言，PointNet通过几个MLP层独立学习逐点特征，并通过最大池化层提取全局特征。Deep
    sets [[53](#bib.bib53)] 通过对所有表示进行求和并应用非线性变换来实现排列不变性。由于在PointNet [[5](#bib.bib5)]
    中，特征是针对每个点独立学习的，因此无法捕捉点之间的局部结构信息。因此，Qi等人 [[54](#bib.bib54)] 提出了一个分层网络PointNet++，以从每个点的邻域捕捉精细的几何结构。作为PointNet++层次结构的核心，其集合抽象层由三层组成：采样层、分组层和基于PointNet的学习层。通过堆叠多个集合抽象层，PointNet++
    从局部几何结构中学习特征，并逐层抽象局部特征。
- en: Because of its simplicity and strong representation ability, many networks have
    been developed based on PointNet [[5](#bib.bib5)]. The architecture of Mo-Net
    [[55](#bib.bib55)] is similar to PointNet [[5](#bib.bib5)] but it takes a finite
    set of moments as its input. Point Attention Transformers (PATs) [[56](#bib.bib56)]
    represents each point by its own absolute position and relative positions with
    respect to its neighbors and learns high dimensional features through MLPs. Then,
    Group Shuffle Attention (GSA) is used to capture relations between points, and
    a permutation invariant, differentiable and trainable end-to-end Gumbel Subset
    Sampling (GSS) layer is developed to learn hierarchical features. Based on PointNet++
    [[54](#bib.bib54)], PointWeb [[57](#bib.bib57)] utilizes the context of the local
    neighborhood to improve point features using Adaptive Feature Adjustment (AFA).
    Duan et al. [[58](#bib.bib58)] proposed a Structural Relational Network (SRN)
    to learn structural relational features between different local structures using
    MLP. Lin et al. [[59](#bib.bib59)] accelerated the inference process by constructing
    a lookup table for both input and function spaces learned by PointNet. The inference
    time on the ModelNet and ShapeNet datasets is sped up by 1.5 ms and 32 times over
    PointNet on a moderate machine. SRINet [[60](#bib.bib60)] first projects a point
    cloud to obtain rotation invariant representations, and then utilizes PointNet-based
    backbone to extract a global feature and graph-based aggregation to extract local
    features. In PointASNL, Yan et al. [[61](#bib.bib61)] utilized an Adaptive Sampling
    (AS) module to adaptively adjust the coordinates and features of points sampled
    by the Furthest Point Sampling (FPS) algorithm, and proposed a local-non-local
    (L-NL) module to capture the local and long range dependencies of these sampled
    points.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简单性和强大的表示能力，许多网络都是基于 PointNet [[5](#bib.bib5)] 开发的。Mo-Net [[55](#bib.bib55)]
    的架构类似于 PointNet [[5](#bib.bib5)]，但其输入为有限集合的矩。Point Attention Transformers (PATs)
    [[56](#bib.bib56)] 通过自身的绝对位置和相对于邻居的相对位置来表示每个点，并通过 MLPs 学习高维特征。然后，使用 Group Shuffle
    Attention (GSA) 捕捉点之间的关系，并开发了一个排列不变、可微分且可训练的端到端 Gumbel 子集采样 (GSS) 层来学习层次特征。基于
    PointNet++ [[54](#bib.bib54)]，PointWeb [[57](#bib.bib57)] 利用局部邻域的上下文通过自适应特征调整
    (AFA) 改进点特征。Duan 等人 [[58](#bib.bib58)] 提出了一个结构关系网络 (SRN) 以通过 MLP 学习不同局部结构之间的结构关系特征。Lin
    等人 [[59](#bib.bib59)] 通过为 PointNet 学习的输入和函数空间构建查找表来加速推理过程。在中等配置的机器上，ModelNet 和
    ShapeNet 数据集上的推理时间分别比 PointNet 提快了 1.5 ms 和 32 倍。SRINet [[60](#bib.bib60)] 首先将点云投影以获得旋转不变的表示，然后利用基于
    PointNet 的主干提取全局特征，并通过图基聚合提取局部特征。在 PointASNL 中，Yan 等人 [[61](#bib.bib61)] 利用自适应采样
    (AS) 模块自适应地调整通过最远点采样 (FPS) 算法采样的点的坐标和特征，并提出了一个局部-非局部 (L-NL) 模块来捕捉这些采样点的局部和长程依赖关系。
- en: 3.3.2 Convolution-based Methods
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 基于卷积的方法
- en: 'Compared with kernels defined on 2D grid structures (e.g., images), convolutional
    kernels for 3D point clouds are hard to design due to the irregularity of point
    clouds. According to the type of convolutional kernels, current 3D convolution
    methods can be divided into continuous and discrete convolution methods, as shown
    in Fig. [4](#S3.F4 "Figure 4 ‣ 3.3.2 Convolution-based Methods ‣ 3.3 Point-based
    Methods ‣ 3 3D Shape Classification ‣ Deep Learning for 3D Point Clouds: A Survey").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与定义在 2D 网格结构（例如图像）上的卷积核相比，由于点云的不规则性，3D 点云的卷积核设计较为困难。根据卷积核的类型，目前的 3D 卷积方法可以分为连续卷积方法和离散卷积方法，如图
    [4](#S3.F4 "图 4 ‣ 3.3.2 基于卷积的方法 ‣ 3.3 基于点的方法 ‣ 3 3D 形状分类 ‣ 3D 点云的深度学习：综述") 所示。
- en: 3D Continuous Convolution Methods. These methods define convolutional kernels
    on a continuous space, where the weights for neighboring points are related to
    the spatial distribution with respect to the center point.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 连续卷积方法。这些方法在连续空间上定义卷积核，其中相邻点的权重与中心点的空间分布有关。
- en: 3D convolution can be interpreted as a weighted sum over a given subset. As
    the core layer of RS-CNN [[62](#bib.bib62)], RS-Conv takes a local subset of points
    around a certain point as its input, and the convolution is implemented using
    an MLP by learning the mapping from low-level relations (such as Euclidean distance
    and relative position) to high-level relations between points in the local subset.
    In [[63](#bib.bib63)], kernel elements are selected randomly in a unit sphere.
    An MLP-based continuous function is then used to establish relation between the
    locations of the kernel elements and the point cloud. In DensePoint [[64](#bib.bib64)],
    convolution is defined as a Single-Layer Perceptron (SLP) with a nonlinear activator.
    Features are learned by concatenating features from all previous layers to sufficiently
    exploit the contextual information. Thomas et al. [[65](#bib.bib65)] proposed
    both rigid and deformable Kernel Point Convolution (KPConv) operators for 3D point
    clouds using a set of learnable kernel points. ConvPoint [[66](#bib.bib66)] separates
    the convolution kernel into spatial and feature parts. The locations of the spatial
    part are randomly selected from a unit sphere and the weighting function is learned
    through a simple MLP.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 卷积可以解释为在给定子集上的加权和。作为 RS-CNN [[62](#bib.bib62)] 的核心层，RS-Conv 以某一点周围的局部子集作为输入，卷积通过
    MLP 实现，通过学习从低级关系（如欧几里得距离和相对位置）到局部子集中点之间高级关系的映射来完成。在 [[63](#bib.bib63)] 中，内核元素在单位球体中随机选择。然后使用基于
    MLP 的连续函数来建立内核元素位置与点云之间的关系。在 DensePoint [[64](#bib.bib64)] 中，卷积定义为带有非线性激活函数的单层感知机（SLP）。通过连接所有前一层的特征来学习特征，以充分利用上下文信息。Thomas
    等人 [[65](#bib.bib65)] 提出了用于 3D 点云的刚性和可变形内核点卷积（KPConv）算子，使用一组可学习的内核点。ConvPoint
    [[66](#bib.bib66)] 将卷积核分为空间部分和特征部分。空间部分的位置从单位球体中随机选择，权重函数通过简单的 MLP 进行学习。
- en: 'Some methods also use existing algorithms to perform convolution. In PointConv
    [[67](#bib.bib67)], convolution is defined as a Monte Carlo estimation of the
    continuous 3D convolution with respect to an importance sampling. The convolutional
    kernels consist of a weighting function (which is learned with MLP layers) and
    a density function (which is learned by a kernelized density estimation and an
    MLP layer). To improve memory and computational efficiency, the 3D convolution
    is further reduced into two operations: matrix multiplication and 2D convolution.
    With the same parameter setting, its memory consumption can be reduced by about
    64 times. In MCCNN [[68](#bib.bib68)], convolution is considered as a Monte Carlo
    estimation process relying on a sample’s density function (which is implemented
    with MLP). Poisson disk sampling is then used to construct a point cloud hierarchy.
    This convolution operator can be used to perform convolution between two or multiple
    sampling methods and can handle varying sampling densities. In SpiderCNN [[69](#bib.bib69)],
    SpiderConv is proposed to define convolution as the product of a step function
    and a Taylor expansion defined on the $k$ nearest neighbors. The step function
    captures the coarse geometry by encoding the local geodesic distance, and the
    Taylor expansion captures the intrinsic local geometric variations by interpolating
    arbitrary values at the vertices of a cube. Besides, a convolution network PCNN
    [[70](#bib.bib70)] is also proposed for 3D point clouds based on the radial basis
    function.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法还使用现有算法进行卷积。在 PointConv [[67](#bib.bib67)] 中，卷积被定义为相对于重要性采样的连续 3D 卷积的蒙特卡洛估计。卷积核由一个权重函数（通过
    MLP 层学习）和一个密度函数（通过核化密度估计和 MLP 层学习）组成。为了提高内存和计算效率，3D 卷积进一步简化为两个操作：矩阵乘法和 2D 卷积。在相同的参数设置下，其内存消耗可减少约
    64 倍。在 MCCNN [[68](#bib.bib68)] 中，卷积被认为是依赖于样本密度函数（通过 MLP 实现）的蒙特卡洛估计过程。然后使用泊松圆盘采样构建点云层次结构。该卷积算子可用于执行两个或多个采样方法之间的卷积，并能处理不同的采样密度。在
    SpiderCNN [[69](#bib.bib69)] 中，提出了 SpiderConv，将卷积定义为 $k$ 个最近邻的阶跃函数和泰勒展开的乘积。阶跃函数通过编码局部测地距离来捕捉粗略的几何形状，而泰勒展开通过在立方体的顶点插值任意值来捕捉固有的局部几何变化。此外，还提出了一种基于径向基函数的卷积网络
    PCNN [[70](#bib.bib70)] 用于 3D 点云。
- en: '![Refer to caption](img/4c97ba3af080226baab1ad2cfc76e6e4.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4c97ba3af080226baab1ad2cfc76e6e4.png)'
- en: 'Figure 4: An illustration of a continuous and discrete convolution for local
    neighbors of a point. (a) represents a local neighborhood ${q_{i}}$ centered at
    point $p$; (b) and (c) represent 3D continuous and discrete convolution, respectively.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：点局部邻域的连续卷积和离散卷积示意图。（a）表示以点 $p$ 为中心的局部邻域 ${q_{i}}$；（b）和（c）分别表示 3D 连续卷积和离散卷积。
- en: Several methods have been proposed to address the rotation equivariant problem
    faced by 3D convolution networks. Esteves et al. [[71](#bib.bib71)] proposed 3D
    Spherical CNN to learn rotation equivariant representation for 3D shapes, which
    takes multi-valued spherical functions as its input. Localized convolutional filters
    are obtained by parameterizing spectrum with anchor points in the spherical harmonic
    domain. Tensor field networks [[72](#bib.bib72)] are proposed to define the point
    convolution operation as the product of a learnable radial function and spherical
    harmonics, which are locally equivariant to 3D rotations, translations, and permutations.
    The convolution in [[73](#bib.bib73)] is defined based on the spherical cross-correlation
    and implemented using a generalized Fast Fourier Transformation (FFT) algorithm.
    Based on PCNN, SPHNet [[74](#bib.bib74)] achieves rotation invariance by incorporating
    spherical harmonic kernels during convolution on volumetric functions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出几种方法来解决 3D 卷积网络面临的旋转等变问题。Esteves 等人 [[71](#bib.bib71)] 提出了 3D 球面 CNN，用于学习
    3D 形状的旋转等变表示，输入为多值球面函数。通过在球面调和域中用锚点参数化谱来获得局部卷积滤波器。提出的张量场网络 [[72](#bib.bib72)]
    将点卷积操作定义为可学习的径向函数和球面调和函数的乘积，这些函数对 3D 旋转、平移和排列局部等变。在 [[73](#bib.bib73)] 中，卷积基于球面互相关定义，并使用广义快速傅里叶变换
    (FFT) 算法实现。基于 PCNN，SPHNet [[74](#bib.bib74)] 通过在体积函数卷积过程中引入球面调和核来实现旋转不变性。
- en: To accelerate computing speed, Flex-Convolution [[75](#bib.bib75)] defines weights
    of convolution kernel as standard scalar product over $k$ nearest neighbors, which
    can be accelerated using CUDA. Experimental results have demonstrated its competitive
    performance on a small dataset with fewer parameters and lower memory consumption.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速计算速度，Flex-Convolution [[75](#bib.bib75)] 将卷积核的权重定义为 $k$ 个最近邻的标准标量积，可以使用
    CUDA 加速。实验结果表明，在具有较少参数和较低内存消耗的小数据集上，其性能具有竞争力。
- en: 3D Discrete Convolution Methods. These methods define convolutional kernels
    on regular grids, where the weights for neighboring points are related to the
    offsets with respect to the center point.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 离散卷积方法。这些方法在规则网格上定义卷积核，其中邻近点的权重与中心点的偏移量相关。
- en: Hua et al. [[76](#bib.bib76)] transformed non-uniform 3D point clouds into uniform
    grids and defined convolutional kernels on each grid. The proposed 3D kernel assigns
    the same weights to all points falling into the same grid. For a given point,
    the mean features of all the neighboring points that are located on the same grid
    are computed from the previous layer. Then, mean features of all grids are weighted
    and summed to produce the output of the current layer. Lei et al. [[77](#bib.bib77)]
    defined a spherical convolutional kernel by partitioning a 3D spherical neighboring
    region into multiple volumetric bins and associating each bin with a learnable
    weighting matrix. The output of the spherical convolutional kernel for a point
    is determined by the non-linear activation of the mean of weighted activation
    values of its neighboring points. In GeoConv [[78](#bib.bib78)], the geometric
    relationship between a point and its neighboring points is explicitly modeled
    based on six bases. Edge features along each direction of the basis are weighted
    independently by a direction-associated learnable matrix. These direction-associated
    features are then aggregated according to the angles formed by the given point
    and its neighboring points. For a given point, its feature at the current layer
    is defined as the sum of features of the given point and its neighboring edge
    features at the previous layer.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Hua 等人 [[76](#bib.bib76)] 将非均匀的 3D 点云转换为均匀网格，并在每个网格上定义卷积核。所提出的 3D 核将相同的权重分配给落入相同网格的所有点。对于给定点，从前一层计算所有位于相同网格上的邻近点的均值特征。然后，对所有网格的均值特征进行加权和求和，以产生当前层的输出。Lei
    等人 [[77](#bib.bib77)] 通过将 3D 球形邻域区域划分为多个体积箱，并将每个箱与可学习的加权矩阵关联，定义了一个球形卷积核。球形卷积核对某一点的输出由其邻近点的加权激活值均值的非线性激活决定。在
    GeoConv [[78](#bib.bib78)] 中，基于六个基底明确建模了一个点与其邻近点之间的几何关系。沿每个基底方向的边缘特征由与方向相关的可学习矩阵独立加权。这些与方向相关的特征随后根据给定点与其邻近点之间形成的角度进行聚合。对于给定点，其在当前层的特征定义为给定点及其邻近边缘特征在前一层的特征之和。
- en: PointCNN [[79](#bib.bib79)] transforms the input points into a latent and potentially
    canonical order through a $\chi$-conv transformation (which is implemented through
    MLP) and then applies typical convolutional operator on the transformed features.
    By interpolating point features to neighboring discrete convolutional kernel-weight
    coordinates, Mao et al. [[80](#bib.bib80)] proposed an interpolated convolution
    operator InterpConv to measure the geometric relations between input point clouds
    and kernel-weight coordinates. Zhang et al. [[81](#bib.bib81)] proposed a RIConv
    operator to achieve rotation invariance, which takes low-level rotation invariant
    geometric features as input and then turns the convolution into 1D by a simple
    binning approach. A-CNN [[82](#bib.bib82)] defines an annular convolution by looping
    the array of neighbors with respect to the size of kernel on each ring of the
    query point and learns the relationship between neighboring points in a local
    subset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: PointCNN [[79](#bib.bib79)] 通过 $\chi$-conv 变换（通过 MLP 实现）将输入点转换为潜在且可能的标准顺序，然后在变换后的特征上应用典型的卷积操作。通过将点特征插值到邻近的离散卷积核权重坐标，Mao
    等人 [[80](#bib.bib80)] 提出了插值卷积操作符 InterpConv，用于测量输入点云与卷积核权重坐标之间的几何关系。Zhang 等人 [[81](#bib.bib81)]
    提出了 RIConv 操作符以实现旋转不变性，该操作符以低级旋转不变几何特征作为输入，然后通过简单的分箱方法将卷积转化为 1D。A-CNN [[82](#bib.bib82)]
    定义了一种环形卷积，通过在查询点的每个环上的邻居数组与卷积核的大小进行循环，学习局部子集中的邻近点之间的关系。
- en: To reduce the computational and memory cost of 3D CNNs, Kumawat et al. [[83](#bib.bib83)]
    proposed a Rectified Local Phase Volume (ReLPV) block to extract phase in a 3D
    local neighborhood based on 3D Short Term Fourier Transform (STFT), which significantly
    reduces the number of parameters. In SFCNN [[84](#bib.bib84)], a point cloud is
    projected onto regular icosahedral lattices with aligned spherical coordinates.
    Convolutions are then conducted upon the features concatenated from vertices of
    spherical lattices and their neighbors through convolution-maxpooling-convolution
    structures. SFCNN is resistant to rotations and perturbations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低3D CNN的计算和内存成本，Kumawat等人[[83](#bib.bib83)]提出了一个基于3D短时傅里叶变换（STFT）的矩形局部相位体积（ReLPV）块，以提取3D局部邻域中的相位，这显著减少了参数数量。在SFCNN[[84](#bib.bib84)]中，点云被投影到具有对齐球面坐标的规则二十面体晶格上。然后在通过卷积-池化-卷积结构从球面晶格顶点及其邻居拼接的特征上进行卷积。SFCNN对旋转和扰动具有抵抗力。
- en: 3.3.3 Graph-based Methods
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 基于图的方法
- en: 'Graph-based networks consider each point in a point cloud as a vertex of a
    graph, and generate directed edges for the graph based on the neighbors of each
    point. Feature learning is then performed in spatial or spectral domains [[85](#bib.bib85)].
    A typical graph-based network is shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.3.3 Graph-based
    Methods ‣ 3.3 Point-based Methods ‣ 3 3D Shape Classification ‣ Deep Learning
    for 3D Point Clouds: A Survey").'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '基于图的网络将点云中的每个点视为图的一个顶点，并基于每个点的邻居生成有向边。然后在空间或频谱域[[85](#bib.bib85)]中进行特征学习。一个典型的基于图的网络如图[5](#S3.F5
    "Figure 5 ‣ 3.3.3 Graph-based Methods ‣ 3.3 Point-based Methods ‣ 3 3D Shape Classification
    ‣ Deep Learning for 3D Point Clouds: A Survey")所示。'
- en: '![Refer to caption](img/959b4b50a9fa2e521501189b86a0f56c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/959b4b50a9fa2e521501189b86a0f56c.png)'
- en: 'Figure 5: An illustration of a graph-based network.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于图的网络的示意图。
- en: Graph-based Methods in Spatial Domain. These methods define operations (e.g.,
    convolution and pooling) in spatial domain. Specifically, convolution is usually
    implemented through MLP over spatial neighbors, and pooling is adopted to produce
    a new coarsened graph by aggregating information from each point’s neighbors.
    Features at each vertex are usually assigned with coordinates, laser intensities
    or colors, while features at each edge are usually assigned with geometric attributes
    between two connected points.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 空间域中的图形方法。这些方法在空间域中定义操作（例如，卷积和池化）。具体来说，卷积通常通过对空间邻居应用多层感知器（MLP）来实现，池化则用于通过汇聚每个点邻居的信息来生成一个新的粗化图。每个顶点的特征通常分配有坐标、激光强度或颜色，而每条边的特征则通常分配有连接两个点之间的几何属性。
- en: As a pioneering work, Simonovsky et al. [[85](#bib.bib85)] considered each point
    as a vertex of the graph, and connected each vertex to all its neighbors by a
    directed edge. Then, Edge-Conditioned Convolution (ECC) is proposed using a filter-generating
    network (e.g., MLP). Max pooling is adopted to aggregate neighborhood information
    and graph coarsening is implemented based on VoxelGrid [[86](#bib.bib86)]. In
    DGCNN [[87](#bib.bib87)], a graph is constructed in the feature space and dynamically
    updated after each layer of the network. As the core layer of EdgeConv, an MLP
    is used as the feature learning function for each edge, and channel-wise symmetric
    aggregation is applied onto the edge features associated with the neighbors of
    each point. Further, LDGCNN [[88](#bib.bib88)] removes the transformation network
    and links the hierarchical features from different layers in DGCNN [[87](#bib.bib87)]
    to improve its performance and reduce the model size. An end-to-end unsupervised
    deep AutoEncoder network (namely, FoldingNet [[89](#bib.bib89)]) is also proposed
    to use the concatenation of a vectorized local covariance matrix and point coordinates
    as its input. Inspired by Inception [[90](#bib.bib90)] and DGCNN [[87](#bib.bib87)],
    Hassani and Haley [[91](#bib.bib91)] proposed an unsupervised multi-task autoencoder
    to learn point and shape features. The encoder is constructed based on mutli-scale
    graphs. The decoder is constructed using three unsupervised tasks including clustering,
    self-supervised classification and reconstruction, which are trained jointly with
    a mutli-task loss. Liu et al. [[92](#bib.bib92)] proposed a Dynamic Points Agglomeration
    Module (DPAM) based on graph convolution to simplify the process of points agglomeration
    (sampling, grouping and pooling) into a simple step, which is implemented through
    multiplication of the agglomeration matrix and points feature matrix. Based on
    the PointNet architecture, a hierarchical learning architecture is constructed
    by stacking multiple DPAMs. Compared with the hierarchy strategy of PointNet++
    [[54](#bib.bib54)], DPAM dynamically exploits the relation of points and agglomerates
    points in a semantic space.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开创性工作，Simonovsky等人[[85](#bib.bib85)]将每个点视为图的一个顶点，并通过有向边将每个顶点连接到所有邻居。随后，提出了使用滤波器生成网络（如MLP）的边条件卷积（ECC）。采用最大池化来聚合邻域信息，并基于VoxelGrid[[86](#bib.bib86)]实现图的粗化。在DGCNN[[87](#bib.bib87)]中，在特征空间中构建图，并在每一层网络之后动态更新。作为EdgeConv的核心层，MLP被用作每条边的特征学习函数，并对与每个点的邻居相关的边特征应用通道级对称聚合。此外，LDGCNN[[88](#bib.bib88)]移除了转换网络，并将DGCNN[[87](#bib.bib87)]中不同层的层次特征连接起来，以提高性能并减少模型大小。还提出了一种端到端的无监督深度自编码网络（即FoldingNet[[89](#bib.bib89)]），使用向量化局部协方差矩阵和点坐标的拼接作为输入。受到Inception[[90](#bib.bib90)]和DGCNN[[87](#bib.bib87)]的启发，Hassani和Haley[[91](#bib.bib91)]提出了一种无监督多任务自编码器，以学习点和形状特征。编码器基于多尺度图构建，解码器使用包括聚类、自监督分类和重建在内的三个无监督任务，这些任务通过多任务损失共同训练。Liu等人[[92](#bib.bib92)]提出了一种基于图卷积的动态点聚合模块（DPAM），将点聚合的过程（采样、分组和池化）简化为一个简单步骤，该步骤通过聚合矩阵和点特征矩阵的乘法实现。基于PointNet架构，通过堆叠多个DPAM构建了层次学习架构。与PointNet++[[54](#bib.bib54)]的层次策略相比，DPAM动态地利用点的关系，并在语义空间中聚合点。
- en: To exploit the local geometric structures, KCNet [[93](#bib.bib93)] learns features
    based on kernel correlation. Specifically, a set of learnable points characterizing
    geometric types of local structures are defined as kernels. Then, affinity between
    the kernel and the neighborhood of a given point is calculated. In G3D [[94](#bib.bib94)],
    convolution is defined as a variant of polynomial of adjacency matrix, and pooling
    is defined as multiplying the Laplacian matrix and the vertex matrix by a coarsening
    matrix. ClusterNet [[95](#bib.bib95)] utilizes a rigorously rotation-invariant
    module to extract rotation-invariant features from $k$ nearest neighbors for each
    point, and constructs hierarchical structures of a point cloud based on the unsupervised
    agglomerative hierarchical clustering method with ward-linkage criteria [[96](#bib.bib96)].
    The features in each sub-cluster are first learned through an EdgeConv block and
    then aggregated through max pooling.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用局部几何结构，KCNet[[93](#bib.bib93)]基于核相关性学习特征。具体而言，一组可学习的点被定义为核，这些点表征局部结构的几何类型。然后，计算核与给定点邻域之间的亲和性。在G3D[[94](#bib.bib94)]中，卷积被定义为邻接矩阵的多项式变体，池化则被定义为通过粗化矩阵对拉普拉斯矩阵和顶点矩阵进行乘法操作。ClusterNet[[95](#bib.bib95)]利用一个严格的旋转不变模块，从每个点的$k$个最近邻中提取旋转不变特征，并基于具有Ward-linkage标准的无监督凝聚层次聚类方法构建点云的层次结构[[96](#bib.bib96)]。每个子簇中的特征首先通过EdgeConv块学习，然后通过最大池化进行聚合。
- en: To address the time-consuming problem of current data structuring methods (such
    as FPS and neighbor points querying), Xu et al. [[97](#bib.bib97)] proposed to
    blend the advantages of volumetric based and point based methods to improve the
    computational efficiency. Experiments on the ModelNet classification task demonstrate
    that the computational efficiency of the proposed Grid-GCN network is 5$\times$
    faster than other models in average.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决当前数据结构化方法（如FPS和邻点查询）的时间消耗问题，Xu等人[[97](#bib.bib97)]提出将体积基方法和点基方法的优点结合起来，以提高计算效率。在ModelNet分类任务上的实验表明，所提出的Grid-GCN网络的计算效率平均比其他模型快5$\times$。
- en: Graph-based Methods in Spectral Domain. These methods define convolutions as
    spectral filtering, which is implemented as the multiplication of signals on graph
    with eigenvectors of the graph Laplacian matrix [[98](#bib.bib98), [99](#bib.bib99)].
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**谱域中的基于图的方法**。这些方法将卷积定义为谱滤波，具体实现为图信号与图拉普拉斯矩阵特征向量的乘法[[98](#bib.bib98), [99](#bib.bib99)]。'
- en: RGCNN [[100](#bib.bib100)] constructs a graph by connecting each point with
    all other points in the point cloud and updates the graph Laplacian matrix in
    each layer. To make features of adjacent vertices more similar, a graph-signal
    smoothness prior is added into the loss function. To address the challenges caused
    by diverse graph topology of data, the SGC-LL layer in AGCN [[101](#bib.bib101)]
    utilizes a learnable distance metric to parameterize the similarity between two
    vertices on the graph. The adjacency matrix obtained from graph is normalized
    using Gaussian kernels and learned distances. HGNN [[102](#bib.bib102)] builds
    a hyperedge convolutional layer by applying spectral convolution on a hypergraph.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: RGCNN[[100](#bib.bib100)]通过将每个点与点云中的所有其他点连接来构建图，并在每一层更新图拉普拉斯矩阵。为了使邻近顶点的特征更相似，在损失函数中加入了图信号平滑先验。为应对数据中多样化图拓扑带来的挑战，AGCN[[101](#bib.bib101)]中的SGC-LL层利用可学习的距离度量来参数化图上两个顶点之间的相似性。通过高斯核和学习到的距离对图获得的邻接矩阵进行归一化。HGNN[[102](#bib.bib102)]通过在超图上应用谱卷积来构建超边卷积层。
- en: Aforementioned methods operate on full graphs. To exploit local structural information,
    Wang et al. [[103](#bib.bib103)] proposed an end-to-end spectral convolution network
    LocalSpecGCN to work on a local graph (which is constructed from the $k$ nearest
    neighbors). This method does not require any offline computation of the graph
    Laplacian matrix and graph coarsening hierarchy. In PointGCN [[104](#bib.bib104)],
    a graph is constructed based on $k$ nearest neighbors from a point cloud and each
    edge is weighted using a Gaussian kernel. Convolutional filters are defined as
    Chebyshev polynomials in graph spectral domain. Global pooling and multi-resolution
    pooling are used to capture global and local features of the point cloud. Pan
    et al. [[105](#bib.bib105)] proposed 3DTI-Net by applying convolution on the $k$
    nearest neighboring graphs in spectral domain. The invariance to geometry transformation
    is achieved by learning from relative Euclidean and direction distances.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法在完整图上操作。为了利用局部结构信息，Wang 等人 [[103](#bib.bib103)] 提出了一个端到端的光谱卷积网络 LocalSpecGCN，以在局部图上工作（该图由
    $k$ 个最近邻构造）。该方法不需要任何离线计算图拉普拉斯矩阵和图粗化层级。在 PointGCN [[104](#bib.bib104)] 中，基于点云中的
    $k$ 个最近邻构建图，每条边使用高斯核加权。卷积滤波器在图光谱域中定义为切比雪夫多项式。使用全局池化和多分辨率池化来捕捉点云的全局和局部特征。Pan 等人
    [[105](#bib.bib105)] 提出了 3DTI-Net，通过在光谱域对 $k$ 个最近邻图应用卷积来实现。通过从相对欧几里得距离和方向距离中学习，达到对几何变换的平移不变性。
- en: 3.3.4 Hierarchical Data Structure-based Methods
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 基于分层数据结构的方法
- en: These networks are constructed based on different hierarchical data structures
    (e.g., octree and kd-tree). In these methods, point features are learned hierarchically
    from leaf nodes to the root node along a tree.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络基于不同的分层数据结构（例如，八叉树和 kd 树）构建。在这些方法中，点特征从叶节点到根节点沿树结构逐层学习。
- en: 'Lei et al. [[77](#bib.bib77)] proposed an octree guided CNN using spherical
    convolutional kernels (as described in Section [4](#S3.F4 "Figure 4 ‣ 3.3.2 Convolution-based
    Methods ‣ 3.3 Point-based Methods ‣ 3 3D Shape Classification ‣ Deep Learning
    for 3D Point Clouds: A Survey")). Each layer of the network corresponds to one
    layer of the octree and a spherical convolutional kernel is applied at each layer.
    The values of neurons in the current layer are determined as the mean values of
    all relevant children nodes in the previous layer. Unlike OctNet [[49](#bib.bib49)]
    which is based on octree, Kd-Net [[106](#bib.bib106)] is built using multiple
    K-d trees with different splitting directions at each iteration. Following a bottom-up
    approach, the representation of a non-leaf node is computed from representations
    of its children using MLP. The feature of the root node (which describes the whole
    point cloud) is finally fed to fully connected layers to predict classification
    scores. Note that, Kd-Net shares parameters at each level according to the splitting
    type of nodes. 3DContextNet [[107](#bib.bib107)] uses a standard balanced K-d
    tree to achieve feature learning and aggregation. At each level, point features
    are first learned through MLP based on local cues (which models inter-dependencies
    between points in a local region) and global contextual cues (which models the
    relationship for one position with respect to all other positions). Then, the
    feature of a non-leaf node is computed from its child nodes using MLP and aggregated
    by max pooling. For classification, the above process is repeated until the root
    node is attained.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lei 等人 [[77](#bib.bib77)] 提出了一个使用球形卷积核的八叉树引导 CNN（详见第 [4](#S3.F4 "Figure 4 ‣
    3.3.2 Convolution-based Methods ‣ 3.3 Point-based Methods ‣ 3 3D Shape Classification
    ‣ Deep Learning for 3D Point Clouds: A Survey) 节）。网络的每一层对应于八叉树的一个层级，每层应用一个球形卷积核。当前层中神经元的值是由前一层中所有相关子节点的均值决定的。与基于八叉树的
    OctNet [[49](#bib.bib49)] 不同，Kd-Net [[106](#bib.bib106)] 使用多个 K-d 树，并在每次迭代中采用不同的分裂方向。采用自下而上的方法，通过
    MLP 从子节点的表示中计算非叶节点的表示。根节点的特征（描述整个点云）最终被输入到全连接层以预测分类分数。请注意，Kd-Net 根据节点的分裂类型在每一层共享参数。3DContextNet
    [[107](#bib.bib107)] 使用标准的平衡 K-d 树实现特征学习和聚合。在每一层中，首先通过 MLP 学习点特征，基于局部线索（建模局部区域内点之间的相互依赖性）和全局上下文线索（建模一个位置相对于所有其他位置的关系）。然后，非叶节点的特征通过
    MLP 从其子节点计算并通过最大池化进行聚合。对于分类，该过程会重复，直到获得根节点。'
- en: The hierarchy of SO-Net network is constructed by performing point-to-node $k$
    nearest neighbor search [[108](#bib.bib108)]. Specifically, a modified permutation
    invariant Self-Organizing Map (SOM) is used to model the spatial distribution
    of a point cloud. Individual point features are learned from normalized point-to-node
    coordinates through a series of fully connected layers. The feature of each node
    in SOM is extracted from point features associated with this node using channel-wise
    max pooling. The final feature is then learned from node features using an approach
    similar to PointNet [[5](#bib.bib5)]. Compared to PointNet++ [[54](#bib.bib54)],
    the hierarchy of SOM is more efficient and the spatial distribution of the point
    cloud is fully explored.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: SO-Net 网络的层次结构是通过执行点到节点的 $k$ 最近邻搜索 [[108](#bib.bib108)] 构建的。具体来说，使用了一种修改后的不变排列自组织映射（SOM）来建模点云的空间分布。通过一系列全连接层，从归一化的点到节点坐标中学习单个点特征。SOM
    中每个节点的特征是通过通道级最大池化从与该节点关联的点特征中提取的。最终特征是通过类似于 PointNet [[5](#bib.bib5)] 的方法从节点特征中学习的。与
    PointNet++ [[54](#bib.bib54)] 相比，SOM 的层次结构更高效，并且点云的空间分布被充分探索。
- en: 'TABLE II: Comparative 3D shape classification results on the ModelNet10/40
    benchmarks. Here, we only focus on point-based networks. ‘#params’ represents
    the number of parameters of a model, ‘OA’ represents the mean accuracy for all
    test instances and ‘mAcc’ represents the mean accuracy for all shape classes in
    the table. The symbol ‘-’ means the results are unavailable.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: ModelNet10/40 基准上的 3D 形状分类结果比较。这里我们仅关注基于点的方法。‘#params’ 代表模型的参数数量，‘OA’
    代表所有测试实例的平均准确率，‘mAcc’ 代表表中所有形状类别的平均准确率。符号 ‘-’ 表示结果不可用。'
- en: '| Methods | Input | #params (M) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 输入 | #params (M) |'
- en: '&#124; ModelNet40 &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ModelNet40 &#124;'
- en: '&#124; (OA) &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (OA) &#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ModelNet40 &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ModelNet40 &#124;'
- en: '&#124; (mAcc) &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (mAcc) &#124;'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ModelNet10 &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ModelNet10 &#124;'
- en: '&#124; (OA) &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (OA) &#124;'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ModelNet10 &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ModelNet10 &#124;'
- en: '&#124; (mAcc) &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (mAcc) &#124;'
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Pointwise MLP Methods | PointNet [[5](#bib.bib5)] | Coordinates | 3.48 |
    89.2% | 86.2% | - | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Pointwise MLP 方法 | PointNet [[5](#bib.bib5)] | 坐标 | 3.48 | 89.2% | 86.2%
    | - | - |'
- en: '| PointNet++ [[54](#bib.bib54)] | Coordinates | 1.48 | 90.7% | - | - | - |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| PointNet++ [[54](#bib.bib54)] | 坐标 | 1.48 | 90.7% | - | - | - |'
- en: '| MO-Net [[55](#bib.bib55)] | Coordinates | 3.1 | 89.3% | 86.1% | - | - |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| MO-Net [[55](#bib.bib55)] | 坐标 | 3.1 | 89.3% | 86.1% | - | - |'
- en: '| Deep Sets [[53](#bib.bib53)] | Coordinates | - | 87.1% | - | - | - |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Deep Sets [[53](#bib.bib53)] | 坐标 | - | 87.1% | - | - | - |'
- en: '| PAT [[56](#bib.bib56)] | Coordinates | - | 91.7% | - | - | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| PAT [[56](#bib.bib56)] | 坐标 | - | 91.7% | - | - | - |'
- en: '| PointWeb [[57](#bib.bib57)] | Coordinates | - | 92.3% | 89.4% | - | - |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| PointWeb [[57](#bib.bib57)] | 坐标 | - | 92.3% | 89.4% | - | - |'
- en: '| SRN-PointNet++ [[58](#bib.bib58)] | Coordinates | - | 91.5% | - | - | - |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| SRN-PointNet++ [[58](#bib.bib58)] | 坐标 | - | 91.5% | - | - | - |'
- en: '| JUSTLOOKUP [[59](#bib.bib59)] | Coordinates | - | 89.5% | 86.4% | 92.9% |
    92.1% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| JUSTLOOKUP [[59](#bib.bib59)] | 坐标 | - | 89.5% | 86.4% | 92.9% | 92.1% |'
- en: '|  | PointASNL [[61](#bib.bib61)] | Coordinates | - | 92.9% | - | 95.7% | -
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | PointASNL [[61](#bib.bib61)] | 坐标 | - | 92.9% | - | 95.7% | - |'
- en: '|  | PointASNL [[61](#bib.bib61)] | Coordinates+Normals | - | 93.2% | - | 95.9%
    | - |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | PointASNL [[61](#bib.bib61)] | 坐标+法线 | - | 93.2% | - | 95.9% | - |'
- en: '| Convolution-based Methods | Pointwise-CNN [[76](#bib.bib76)] | Coordinates
    | - | 86.1% | 81.4% | - | - |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 基于卷积的方法 | Pointwise-CNN [[76](#bib.bib76)] | 坐标 | - | 86.1% | 81.4% | - |
    - |'
- en: '| PointConv [[67](#bib.bib67)] | Coordinates+Normals | - | 92.5% | - | - |
    - |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| PointConv [[67](#bib.bib67)] | 坐标+法线 | - | 92.5% | - | - | - |'
- en: '| MC Convolution [[68](#bib.bib68)] | Coordinates | - | 90.9% | - | - | - |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| MC 卷积 [[68](#bib.bib68)] | 坐标 | - | 90.9% | - | - | - |'
- en: '| SpiderCNN [[69](#bib.bib69)] | Coordinates+Normals | - | 92.4% | - | - |
    - |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| SpiderCNN [[69](#bib.bib69)] | 坐标+法线 | - | 92.4% | - | - | - |'
- en: '| PointCNN [[79](#bib.bib79)] | Coordinates | 0.45 | 92.2% | 88.1% | - | -
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| PointCNN [[79](#bib.bib79)] | 坐标 | 0.45 | 92.2% | 88.1% | - | - |'
- en: '| Flex-Convolution [[75](#bib.bib75)] | Coordinates | - | 90.2% | - | - | -
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Flex-Convolution [[75](#bib.bib75)] | 坐标 | - | 90.2% | - | - | - |'
- en: '| PCNN [[70](#bib.bib70)] | Coordinates | 1.4 | 92.3% | - | 94.9% | - |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| PCNN [[70](#bib.bib70)] | 坐标 | 1.4 | 92.3% | - | 94.9% | - |'
- en: '| Boulch [[63](#bib.bib63)] | Coordinates | - | 91.6% | 88.1% | - | - |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Boulch [[63](#bib.bib63)] | 坐标 | - | 91.6% | 88.1% | - | - |'
- en: '| RS-CNN [[62](#bib.bib62)] | Coordinates | - | 93.6% | - | - | - |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| RS-CNN [[62](#bib.bib62)] | 坐标 | - | 93.6% | - | - | - |'
- en: '| Spherical CNNs [[71](#bib.bib71)] | Coordinates | 0.5 | 88.9% | - | - | -
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 球面 CNNs [[71](#bib.bib71)] | 坐标 | 0.5 | 88.9% | - | - | - |'
- en: '| GeoCNN [[78](#bib.bib78)] | Coordinates | - | 93.4% | 91.1% | - | - |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| GeoCNN [[78](#bib.bib78)] | 坐标 | - | 93.4% | 91.1% | - | - |'
- en: '| $\Psi$-CNN [[77](#bib.bib77)] | Coordinates | - | 92.0% | 88.7% | 94.6% |
    94.4% |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| $\Psi$-CNN [[77](#bib.bib77)] | 坐标 | - | 92.0% | 88.7% | 94.6% | 94.4% |'
- en: '| A-CNN [[82](#bib.bib82)] | Coordinates | - | 92.6% | 90.3% | 95.5% | 95.3%
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| A-CNN [[82](#bib.bib82)] | 坐标 | - | 92.6% | 90.3% | 95.5% | 95.3% |'
- en: '| SFCNN [[84](#bib.bib84)] | Coordinates | - | 91.4% | - | - | - |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| SFCNN [[84](#bib.bib84)] | 坐标 | - | 91.4% | - | - | - |'
- en: '| SFCNN [[84](#bib.bib84)] | Coordinates+Normals | - | 92.3% | - | - | - |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| SFCNN [[84](#bib.bib84)] | 坐标+法线 | - | 92.3% | - | - | - |'
- en: '| DensePoint [[64](#bib.bib64)] | Coordinates | 0.53 | 93.2% | - | 96.6% |
    - |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| DensePoint [[64](#bib.bib64)] | 坐标 | 0.53 | 93.2% | - | 96.6% | - |'
- en: '| KPConv rigid [[65](#bib.bib65)] | Coordinates | - | 92.9% | - | - | - |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| KPConv rigid [[65](#bib.bib65)] | 坐标 | - | 92.9% | - | - | - |'
- en: '| KPConv deform [[65](#bib.bib65)] | Coordinates | - | 92.7% | - | - | - |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| KPConv deform [[65](#bib.bib65)] | 坐标 | - | 92.7% | - | - | - |'
- en: '| InterpCNN [[80](#bib.bib80)] | Coordinates | 12.8 | 93.0% | - | - | - |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| InterpCNN [[80](#bib.bib80)] | 坐标 | 12.8 | 93.0% | - | - | - |'
- en: '| ConvPoint [[66](#bib.bib66)] | Coordinates | - | 91.8% | 88.5% | - | - |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ConvPoint [[66](#bib.bib66)] | 坐标 | - | 91.8% | 88.5% | - | - |'
- en: '| Graph-based Methods | ECC [[85](#bib.bib85)] | Coordinates | - | 87.4% |
    83.2% | 90.8% | 90.0% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的方法 | ECC [[85](#bib.bib85)] | 坐标 | - | 87.4% | 83.2% | 90.8% | 90.0%
    |'
- en: '| KCNet [[93](#bib.bib93)] | Coordinates | 0.9 | 91.0% | - | 94.4% | - |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| KCNet [[93](#bib.bib93)] | 坐标 | 0.9 | 91.0% | - | 94.4% | - |'
- en: '| DGCNN [[87](#bib.bib87)] | Coordinates | 1.84 | 92.2% | 90.2% | - | - |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN [[87](#bib.bib87)] | 坐标 | 1.84 | 92.2% | 90.2% | - | - |'
- en: '| LocalSpecGCN [[103](#bib.bib103)] | Coordinates+Normals | - | 92.1% | - |
    - | - |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| LocalSpecGCN [[103](#bib.bib103)] | 坐标+法线 | - | 92.1% | - | - | - |'
- en: '| RGCNN [[100](#bib.bib100)] | Coordinates+Normals | 2.24 | 90.5% | 87.3% |
    - | - |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| RGCNN [[100](#bib.bib100)] | 坐标+法线 | 2.24 | 90.5% | 87.3% | - | - |'
- en: '| LDGCNN [[88](#bib.bib88)] | Coordinates | - | 92.9% | 90.3% | - | - |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| LDGCNN [[88](#bib.bib88)] | 坐标 | - | 92.9% | 90.3% | - | - |'
- en: '| 3DTI-Net [[105](#bib.bib105)] | Coordinates | 2.6 | 91.7% | - | - | - |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 3DTI-Net [[105](#bib.bib105)] | 坐标 | 2.6 | 91.7% | - | - | - |'
- en: '| PointGCN [[104](#bib.bib104)] | Coordinates | - | 89.5% | 86.1% | 91.9% |
    91.6% |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| PointGCN [[104](#bib.bib104)] | 坐标 | - | 89.5% | 86.1% | 91.9% | 91.6% |'
- en: '| ClusterNet [[95](#bib.bib95)] | Coordinates | - | 87.1% | - | - | - |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| ClusterNet [[95](#bib.bib95)] | 坐标 | - | 87.1% | - | - | - |'
- en: '| Hassani et al. [[91](#bib.bib91)] | Coordinates | - | 89.1% | - | - | - |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Hassani et al. [[91](#bib.bib91)] | 坐标 | - | 89.1% | - | - | - |'
- en: '| DPAM [[92](#bib.bib92)] | Coordinates | - | 91.9% | 89.9% | 94.6% | 94.3%
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| DPAM [[92](#bib.bib92)] | 坐标 | - | 91.9% | 89.9% | 94.6% | 94.3% |'
- en: '|  | Grid-GCN [[97](#bib.bib97)] | Coordinates | - | 93.1% | 91.3% | 97.5%
    | 97.4% |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | Grid-GCN [[97](#bib.bib97)] | 坐标 | - | 93.1% | 91.3% | 97.5% | 97.4% |'
- en: '| Hierarchical Data Structure -based Methods | KD-Net [[106](#bib.bib106)]
    | Coordinates | 2.0 | 91.8% | 88.5% | 94.0% | 93.5% |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 基于层次数据结构的方法 | KD-Net [[106](#bib.bib106)] | 坐标 | 2.0 | 91.8% | 88.5% | 94.0%
    | 93.5% |'
- en: '| SO-Net [[108](#bib.bib108)] | Coordinates | - | 90.9% | 87.3% | 94.1% | 93.9%
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| SO-Net [[108](#bib.bib108)] | 坐标 | - | 90.9% | 87.3% | 94.1% | 93.9% |'
- en: '| SCN [[109](#bib.bib109)] | Coordinates | - | 90.0% | 87.6% | - | - |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| SCN [[109](#bib.bib109)] | 坐标 | - | 90.0% | 87.6% | - | - |'
- en: '| A-SCN [[109](#bib.bib109)] | Coordinates | - | 89.8% | 87.4% | - | - |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| A-SCN [[109](#bib.bib109)] | 坐标 | - | 89.8% | 87.4% | - | - |'
- en: '| 3DContextNet [[107](#bib.bib107)] | Coordinates | - | 90.2% | - | - | - |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 3DContextNet [[107](#bib.bib107)] | 坐标 | - | 90.2% | - | - | - |'
- en: '| 3DContextNet [[107](#bib.bib107)] | Coordinates+Normals | - | 91.1% | - |
    - | - |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 3DContextNet [[107](#bib.bib107)] | 坐标+法线 | - | 91.1% | - | - | - |'
- en: '| Other Methods | 3DmFV-Net [[52](#bib.bib52)] | Coordinates | 4.6 | 91.6%
    | - | 95.2% | - |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 其他方法 | 3DmFV-Net [[52](#bib.bib52)] | 坐标 | 4.6 | 91.6% | - | 95.2% | - |'
- en: '| PVNet [[110](#bib.bib110)] | Coordinates+Views | - | 93.2% | - | - | - |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| PVNet [[110](#bib.bib110)] | 坐标+视图 | - | 93.2% | - | - | - |'
- en: '| PVRNet [[111](#bib.bib111)] | Coordinates+Views | - | 93.6% | - | - | - |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| PVRNet [[111](#bib.bib111)] | 坐标+视图 | - | 93.6% | - | - | - |'
- en: '| 3DPointCapsNet [[112](#bib.bib112)] | Coordinates | - | 89.3% | - | - | -
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 3DPointCapsNet [[112](#bib.bib112)] | 坐标 | - | 89.3% | - | - | - |'
- en: '| DeepRBFNet [[113](#bib.bib113)] | Coordinates | 3.2 | 90.2% | 87.8% | - |
    - |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| DeepRBFNet [[113](#bib.bib113)] | 坐标 | 3.2 | 90.2% | 87.8% | - | - |'
- en: '| DeepRBFNet [[113](#bib.bib113)] | Coordinates+Normals | 3.2 | 92.1% | 88.8%
    | - | - |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| DeepRBFNet [[113](#bib.bib113)] | 坐标+法线 | 3.2 | 92.1% | 88.8% | - | - |'
- en: '| Point2Sequences [[114](#bib.bib114)] | Coordinates | - | 92.6% | 90.4% |
    95.3% | 95.1% |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Point2Sequences [[114](#bib.bib114)] | 坐标 | - | 92.6% | 90.4% | 95.3% | 95.1%
    |'
- en: '| RCNet [[115](#bib.bib115)] | Coordinates | - | 91.6% | - | 94.7% | - |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| RCNet [[115](#bib.bib115)] | 坐标 | - | 91.6% | - | 94.7% | - |'
- en: '| RCNet-E [[115](#bib.bib115)] | Coordinates | - | 92.3% | - | 95.6% | - |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| RCNet-E [[115](#bib.bib115)] | 坐标 | - | 92.3% | - | 95.6% | - |'
- en: 3.3.5 Other Methods
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5 其他方法
- en: In addition, many other schemes have also been proposed. RBFNet [[113](#bib.bib113)]
    explicitly models the spatial distribution of points by aggregating features from
    sparsely distributed Radial Basis Function (RBF) kernels with learnable kernel
    positions and sizes. 3DPointCapsNet [[112](#bib.bib112)] learns point independent
    features with pointwise MLP and convolutional layers, and extracts global latent
    representation with multiple max-pooling layers. Based on unsupervised dynamic
    routing, powerful representative latent capsules are then learned. Qin et al.
    [[116](#bib.bib116)] proposed an end-to-end unsupervised domain adaptation network
    PointDAN for 3D point cloud representation. To capture semantic properties of
    a point cloud, a self-supervised method is proposed to reconstruct the point cloud,
    whose parts have been randomly rearranged [[117](#bib.bib117)]. Li et al. [[118](#bib.bib118)]
    proposed an auto-augmentation framework, PointAugment, to automatically optimize
    and augment point cloud samples for network training. Specifically, shape-wise
    transformation and point-wise displacement for each input sample are automatically
    learned, and the network is trained by alternatively optimizing and updating the
    learnable parameters of its augmentor and classifier. Inspired by shape context
    [[119](#bib.bib119)], Xie et al. [[109](#bib.bib109)] proposed a ShapeContextNet
    architecture by combining affinity point selection and compact feature aggregation
    into a soft alignment operation using dot-product self-attention [[120](#bib.bib120)].
    To handle noise and occlusion in 3D point clouds, Bobkov et al. [[121](#bib.bib121)]
    fed handcrafted point pair function based 4D rotation invariant descriptors into
    a 4D convolutional neural network. Prokudin et al. [[122](#bib.bib122)] first
    randomly sampled a basis point set with a uniform distribution from a unit ball,
    and then encoded a point cloud as minimal distances to the basis point set. Consequently,
    the point cloud is converted to a vector with a relatively small fixed length.
    The encoded representation can then be processed with existing machine learning
    methods.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还提出了许多其他方案。RBFNet [[113](#bib.bib113)] 通过聚合来自稀疏分布的径向基函数（RBF）核的特征，明确地建模点的空间分布，这些RBF核具有可学习的核位置和大小。3DPointCapsNet
    [[112](#bib.bib112)] 通过点独立特征的点对点多层感知器（MLP）和卷积层进行学习，并通过多个最大池化层提取全局潜在表示。基于无监督动态路由，学习到强大的代表性潜在胶囊。Qin
    等 [[116](#bib.bib116)] 提出了一个端到端的无监督领域适配网络 PointDAN，用于3D点云表示。为了捕捉点云的语义特性，提出了一种自监督方法来重建点云，其部分已经被随机重新排列
    [[117](#bib.bib117)]。Li 等 [[118](#bib.bib118)] 提出了一个自动增强框架 PointAugment，用于自动优化和增强点云样本以进行网络训练。具体来说，每个输入样本的形状变换和点位移被自动学习，网络通过交替优化和更新其增强器和分类器的可学习参数进行训练。受形状上下文
    [[119](#bib.bib119)] 启发，Xie 等 [[109](#bib.bib109)] 提出了一个 ShapeContextNet 架构，通过结合亲和点选择和紧凑特征聚合，使用点积自注意力
    [[120](#bib.bib120)] 进行软对齐操作。为了处理3D点云中的噪声和遮挡，Bobkov 等 [[121](#bib.bib121)] 将基于手工设计的点对点函数的4D旋转不变描述符输入到4D卷积神经网络中。Prokudin
    等 [[122](#bib.bib122)] 首先从单位球体中随机采样一个均匀分布的基点集，然后将点云编码为到基点集的最小距离。因此，点云被转换为一个相对较小的固定长度向量。编码表示随后可以使用现有的机器学习方法进行处理。
- en: RCNet [[115](#bib.bib115)] utilizes standard RNN and 2D CNN to construct a permutation-invariant
    network for 3D point cloud processing. The point cloud is first partitioned into
    parallel beams and sorted along a specific dimension, and each beam is then fed
    into a shared RNN. The learned features are further fed into an efficient 2D CNN
    for hierarchical feature aggregation. To enhance its description ability, RCNet-E
    is proposed to ensemble multiple RCNets along different partition and sorting
    directions. Point2Sequences [[114](#bib.bib114)] is another RNN-based model that
    captures correlations between different areas in local regions of point clouds.
    It considers features learned from a local region at multiple scales as sequences
    and feeds these sequences from all local regions into an RNN-based encoder-decoder
    structure to aggregate local region features.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: RCNet [[115](#bib.bib115)] 利用标准 RNN 和 2D CNN 构建了一个对置换不变的 3D 点云处理网络。点云首先被划分为平行光束，并沿特定维度排序，然后每个光束被送入共享的
    RNN。学习到的特征进一步输入高效的 2D CNN 以进行层次特征聚合。为了增强其描述能力，提出了 RCNet-E，通过不同的分区和排序方向来集成多个 RCNet。Point2Sequences
    [[114](#bib.bib114)] 是另一个基于 RNN 的模型，捕捉点云局部区域内不同区域之间的相关性。它将从多个尺度的局部区域学习到的特征视为序列，并将这些序列从所有局部区域输入
    RNN 基的编码解码结构中以聚合局部区域特征。
- en: Several methods also learn from both 3D point clouds and 2D images. In PVNet
    [[110](#bib.bib110)], high-level global features extracted from multi-view images
    are projected into the subspace of point clouds through an embedding network,
    and fused with point cloud features through a soft attention mask. Finally, a
    residual connection is employed for fused features and multi-view features to
    perform shape recognition. Later, PVRNet [[111](#bib.bib111)] is further proposed
    to exploit the relation between a 3D point cloud and its multiple views by a relation
    score module. Based on the relation scores, the original 2D global view features
    are enhanced for point-single-view fusion and point-multi-view fusion.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法还从 3D 点云和 2D 图像中学习。在 PVNet [[110](#bib.bib110)] 中，从多视角图像中提取的高级全局特征通过嵌入网络投影到点云的子空间中，并通过软注意力掩码与点云特征融合。最后，使用残差连接来融合特征和多视图特征以进行形状识别。随后，提出了
    PVRNet [[111](#bib.bib111)]，通过关系评分模块来利用 3D 点云及其多个视图之间的关系。基于关系评分，原始的 2D 全局视图特征得到增强，以实现点单视图融合和点多视图融合。
- en: 3.4 Summary
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 总结
- en: 'The ModelNet10/40 [[6](#bib.bib6)] datasets are the most frequently used datasets
    for 3D shape classification. Table [II](#S3.T2 "TABLE II ‣ 3.3.4 Hierarchical
    Data Structure-based Methods ‣ 3.3 Point-based Methods ‣ 3 3D Shape Classification
    ‣ Deep Learning for 3D Point Clouds: A Survey") shows the results achieved by
    different point-based networks. Several observations can be drawn:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 'ModelNet10/40 [[6](#bib.bib6)] 数据集是用于 3D 形状分类的最常用数据集。表 [II](#S3.T2 "TABLE II
    ‣ 3.3.4 Hierarchical Data Structure-based Methods ‣ 3.3 Point-based Methods ‣
    3 3D Shape Classification ‣ Deep Learning for 3D Point Clouds: A Survey") 显示了不同点基网络所取得的结果。可以得出以下几点观察：'
- en: $\bullet$
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Pointwise MLP networks are usually served as the basic building block for other
    types of networks to learn pointwise features.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点wise MLP 网络通常作为其他类型网络学习点wise 特征的基本构建块。
- en: $\bullet$
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: As a standard deep learning architecture, convolution-based networks can achieve
    superior performance on irregular 3D point clouds. More attention should be paid
    to both discrete and continuous convolution networks for irregular data.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为一种标准深度学习架构，基于卷积的网络可以在不规则的 3D 点云上实现优越的性能。应更加关注针对不规则数据的离散卷积网络和连续卷积网络。
- en: $\bullet$
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Due to its inherent strong capability to handle irregular data, graph-based
    networks have attracted increasingly more attention in recent years. However,
    it is still challenging to extend graph-based networks in the spectral domain
    to various graph structures.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于其固有的强大处理不规则数据的能力，基于图的网络近年来受到了越来越多的关注。然而，将基于图的网络扩展到各种图结构的频谱域仍然具有挑战性。
- en: 4 3D Object Detection and Tracking
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 3D 对象检测与跟踪
- en: In this section, we will review existing methods for 3D object detection, 3D
    object tracking and 3D scene flow estimation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾现有的 3D 对象检测、3D 对象跟踪和 3D 场景流估计方法。
- en: 4.1 3D Object Detection
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 3D 对象检测
- en: 'A typical 3D object detector takes the point cloud of a scene as its input
    and produces an oriented 3D bounding box around each detected object, as shown
    in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 3D Object Detection ‣ 4 3D Object Detection
    and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey"). Similar to object
    detection in images [[123](#bib.bib123)], 3D object detection methods can be divided
    into two categories: region proposal-based and single shot methods. Several milestone
    methods are presented in Fig. [7](#S4.F7 "Figure 7 ‣ 4.1 3D Object Detection ‣
    4 3D Object Detection and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '一个典型的 3D 物体检测器将场景的点云作为输入，并产生一个围绕每个检测到的物体的定向 3D 边界框，如图 [6](#S4.F6 "Figure 6
    ‣ 4.1 3D Object Detection ‣ 4 3D Object Detection and Tracking ‣ Deep Learning
    for 3D Point Clouds: A Survey") 所示。与图像中的物体检测 [[123](#bib.bib123)] 相似，3D 物体检测方法可以分为两类：区域提议基础的方法和单次检测方法。图
    [7](#S4.F7 "Figure 7 ‣ 4.1 3D Object Detection ‣ 4 3D Object Detection and Tracking
    ‣ Deep Learning for 3D Point Clouds: A Survey") 中展示了几个具有里程碑意义的方法。'
- en: '![Refer to caption](img/3394aed96c9ad1de69aa54987673fb06.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/3394aed96c9ad1de69aa54987673fb06.png)'
- en: (a) ScanNetV2 [[11](#bib.bib11)] dataset
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ScanNetV2 [[11](#bib.bib11)] 数据集
- en: '![Refer to caption](img/aa9c2c44c58651a4e4931f660882569f.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/aa9c2c44c58651a4e4931f660882569f.png)'
- en: (b) KITTI [[14](#bib.bib14)] dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: (b) KITTI [[14](#bib.bib14)] 数据集
- en: 'Figure 6: An illustration of 3D object detection. (a) and (b) are originally
    shown in [[124](#bib.bib124)] and [[125](#bib.bib125)], respectively.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：3D 物体检测的示意图。(a) 和 (b) 最初展示在 [[124](#bib.bib124)] 和 [[125](#bib.bib125)]
    中。
- en: '![Refer to caption](img/e4c1386d72354e7de00fd5a5ea01d1d9.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/e4c1386d72354e7de00fd5a5ea01d1d9.png)'
- en: 'Figure 7: Chronological overview of the most relevant deep learning-based 3D
    object detection methods.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：最相关的基于深度学习的 3D 物体检测方法的时间概览。
- en: 4.1.1 Region Proposal-based Methods
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 区域提议基础的方法
- en: 'These methods first propose several possible regions (also called proposals)
    containing objects, and then extract region-wise features to determine the category
    label of each proposal. According to their object proposal generation approach,
    these methods can further be divided into three categories: multi-view based,
    segmentation-based and frustum-based methods.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法首先提出几个可能包含物体的区域（也称为提议），然后提取区域特征以确定每个提议的类别标签。根据其物体提议生成方法，这些方法可以进一步分为三类：基于多视角、基于分割和基于截锥体的方法。
- en: 'Multi-view based Methods. These methods fuse proposal-wise features from different
    view maps (e.g., LiDAR front view, Bird’s Eye View (BEV), and image) to obtain
    3D rotated boxes, as shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.1 Region Proposal-based
    Methods ‣ 4.1 3D Object Detection ‣ 4 3D Object Detection and Tracking ‣ Deep
    Learning for 3D Point Clouds: A Survey")(a). The computational cost of these methods
    is usually high.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '基于多视角的方法。这些方法融合来自不同视角地图（例如，LiDAR 前视图、鸟瞰图 (BEV) 和图像）的提议特征，以获得 3D 旋转框，如图 [8](#S4.F8
    "Figure 8 ‣ 4.1.1 Region Proposal-based Methods ‣ 4.1 3D Object Detection ‣ 4
    3D Object Detection and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey")(a)
    所示。这些方法的计算成本通常很高。'
- en: '![Refer to caption](img/7e70c55963e62a47d5e426bf62061ff8.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/7e70c55963e62a47d5e426bf62061ff8.png)'
- en: 'Figure 8: Typical networks for three categories of region proposal-based 3D
    object detection methods. From top to bottom: (a) multi-view based, (b) segmentation-based
    and (c) frustum-based methods.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：三类区域提议基础的 3D 物体检测方法的典型网络。从上到下：(a) 基于多视角的方法，(b) 基于分割的方法，(c) 基于截锥体的方法。
- en: 'Chen et al. [[4](#bib.bib4)] generated a group of highly accurate 3D candidate
    boxes from the BEV map and projected them to the feature maps of multiple views
    (e.g., LiDAR front view image, RGB image). They then combined these region-wise
    features from different views to predict oriented 3D bounding boxes, as shown
    in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.1 Region Proposal-based Methods ‣ 4.1 3D Object
    Detection ‣ 4 3D Object Detection and Tracking ‣ Deep Learning for 3D Point Clouds:
    A Survey")(a). Although this method achieves a recall of 99.1% at an Intersection
    over Union (IoU) of 0.25 with only 300 proposals, its speed is too slow for practical
    applications. Subsequently, several approaches have been developed to improve
    multi-view 3D object detection methods from two aspects.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '陈等人 [[4](#bib.bib4)] 从BEV地图生成了一组高度准确的3D候选框，并将其投影到多个视角的特征图上（例如，LiDAR前视图像，RGB图像）。然后，他们结合这些来自不同视角的区域特征，以预测定向的3D边界框，如图[8](#S4.F8
    "Figure 8 ‣ 4.1.1 Region Proposal-based Methods ‣ 4.1 3D Object Detection ‣ 4
    3D Object Detection and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey")(a)所示。尽管该方法在仅使用300个提议的情况下，在0.25的交并比（IoU）下实现了99.1%的召回率，但其速度对于实际应用来说仍然过慢。因此，随后开发了几种方法，从两个方面改进多视角3D目标检测方法。'
- en: First, several methods have been proposed to efficiently fuse the information
    of different modalities. To generate 3D proposals with a high recall for small
    objects, Ku et al. [[126](#bib.bib126)] proposed a multi-modal fusion-based region
    proposal network. They first extracted equal-sized features from both BEV and
    image views using cropping and resizing operations, and then fused these features
    using element-wise mean pooling. Liang et al. [[127](#bib.bib127)] exploited continuous
    convolutions to enable effective fusion of image and 3D LiDAR feature maps at
    different resolutions. Specifically, they extracted nearest corresponding image
    features for each point in the BEV space and then used bilinear interpolation
    to obtain a dense BEV feature map by projecting image features into the BEV plane.
    Experimental results show that dense BEV feature maps are more suitable for 3D
    object detection than discrete image feature maps and sparse LiDAR feature maps.
    Liang et al. [[128](#bib.bib128)] presented a multi-task multi-sensor 3D object
    detection network for end-to-end training. Specifically, multiple tasks (e.g.,
    2D object detection, ground estimation and depth completion) are exploited to
    help the network learn better feature representations. The learned cross-modality
    representation is further exploited to produce highly accurate object detection
    results. Experimental results show that this method achieves a significant improvement
    on 2D, 3D and BEV detection tasks, and outperforms previous state-of-the-art methods
    on the TOR4D benchmark [[129](#bib.bib129), [130](#bib.bib130)].
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，提出了几种方法来高效融合不同模态的信息。为了生成具有高召回率的小物体3D提议，Ku等人 [[126](#bib.bib126)] 提出了基于多模态融合的区域提议网络。他们首先通过裁剪和调整大小操作从BEV和图像视图中提取等大小的特征，然后使用逐元素均值池化融合这些特征。Liang等人
    [[127](#bib.bib127)] 利用连续卷积实现了不同分辨率下图像和3D LiDAR特征图的有效融合。具体而言，他们为BEV空间中的每个点提取了最接近的图像特征，然后使用双线性插值通过将图像特征投影到BEV平面来获得密集的BEV特征图。实验结果表明，密集的BEV特征图比离散的图像特征图和稀疏的LiDAR特征图更适合3D目标检测。Liang等人
    [[128](#bib.bib128)] 提出了一个用于端到端训练的多任务多传感器3D目标检测网络。具体而言，利用多个任务（例如，2D目标检测、地面估计和深度补全）帮助网络学习更好的特征表示。学到的跨模态表示进一步被用于生成高度准确的目标检测结果。实验结果表明，该方法在2D、3D和BEV检测任务上实现了显著的改进，并在TOR4D基准测试
    [[129](#bib.bib129), [130](#bib.bib130)] 中超越了之前的最先进方法。
- en: Second, different methods have been investigated to extract robust representations
    of the input data. Lu et al. [[39](#bib.bib39)] explored multi-scale contextual
    information by introducing a Spatial Channel Attention (SCA) module, which captures
    the global and multi-scale context of a scene and highlights useful features.
    They also proposed an Extension Spatial Unsample (ESU) module to obtain high-level
    features with rich spatial information by combining multi-scale low-level features,
    thus generating reliable 3D object proposals. Although better detection performance
    can be achieved, the aforementioned multi-view methods take a long runtime since
    they perform feature pooling for each proposal. Subsequently, Zeng et al. [[131](#bib.bib131)]
    used a pre-RoI pooling convolution to improve the efficiency of [[4](#bib.bib4)].
    Specifically, they moved the majority of convolution operations to be ahead of
    the RoI pooling module. Therefore, RoI convolutions are performed once for all
    object proposals. Experimental results show that this method can run at a speed
    of 11.1 fps, which is 5 times faster than MV3D [[4](#bib.bib4)].
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，研究了不同的方法以提取输入数据的稳健表示。Lu等人[[39](#bib.bib39)]通过引入空间通道注意力（SCA）模块，探索了多尺度上下文信息，该模块捕捉场景的全局和多尺度上下文，并突出有用的特征。他们还提出了一种扩展空间无采样（ESU）模块，通过结合多尺度低级特征获得丰富空间信息的高级特征，从而生成可靠的3D物体建议。尽管可以实现更好的检测性能，但上述的多视角方法由于对每个建议进行特征池化，导致运行时间较长。随后，Zeng等人[[131](#bib.bib131)]使用预先RoI池化卷积来提高[[4](#bib.bib4)]的效率。具体而言，他们将大多数卷积操作移动到RoI池化模块之前进行。因此，对所有物体建议只执行一次RoI卷积。实验结果表明，该方法可以以11.1
    fps的速度运行，比MV3D[[4](#bib.bib4)]快5倍。
- en: 'Segmentation-based Methods. These methods first leverage existing semantic
    segmentation techniques to remove most background points, and then generate a
    large amount of high-quality proposals on foreground points to save computation,
    as shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.1 Region Proposal-based Methods ‣
    4.1 3D Object Detection ‣ 4 3D Object Detection and Tracking ‣ Deep Learning for
    3D Point Clouds: A Survey")(b). Compared to multi-view methods [[4](#bib.bib4),
    [126](#bib.bib126), [131](#bib.bib131)], these methods achieve higher object recall
    rates and are more suitable for complicated scenes with highly occluded and crowded
    objects.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '基于分割的方法。这些方法首先利用现有的语义分割技术去除大部分背景点，然后在前景点上生成大量高质量的建议以节省计算，如图[8](#S4.F8 "Figure
    8 ‣ 4.1.1 Region Proposal-based Methods ‣ 4.1 3D Object Detection ‣ 4 3D Object
    Detection and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey")(b)所示。与多视角方法[[4](#bib.bib4),
    [126](#bib.bib126), [131](#bib.bib131)]相比，这些方法实现了更高的物体召回率，并且更适用于复杂的场景，其中物体高度遮挡和拥挤。'
- en: Yang et al. [[132](#bib.bib132)] used a 2D segmentation network to predict foreground
    pixels and projected them into point clouds to remove most background points.
    They then generated proposals on the predicted foreground points and designed
    a new criterion named PointsIoU to reduce the redundancy and ambiguity of proposals.
    Following [[132](#bib.bib132)], Shi et al. [[133](#bib.bib133)] proposed a PointRCNN
    framework. Specifically, they directly segmented 3D point clouds to obtain foreground
    points and then fused semantic features and local spatial features to produce
    high-quality 3D boxes. Following the Region Proposal Network (RPN) stage of [[133](#bib.bib133)],
    Jesus et al. [[134](#bib.bib134)] proposed a pioneering work to leverage Graph
    Convolution Network (GCN) for 3D object detection. Specifically, two modules are
    introduced to refine object proposals using graph convolution. The first module
    R-GCN utilizes all points contained in a proposal to achieve per-proposal feature
    aggregation. The second module C-GCN fuses per-frame information from all proposals
    to regress accurate object boxes by exploiting contexts. Sourabh et al. [[135](#bib.bib135)]
    projected a point cloud into the output of the image-based segmentation network
    and appended the semantic prediction scores to the points. The painted points
    are fed into existing detectors [[133](#bib.bib133), [136](#bib.bib136), [137](#bib.bib137)]
    to achieve significant performance improvement. Yang et al. [[138](#bib.bib138)]
    associated each point with a spherical anchor. The semantic score of each point
    is then used to remove redundant anchors. Consequently, this method achieves a
    higher recall with lower computational cost as compared to previous methods [[132](#bib.bib132),
    [133](#bib.bib133)]. In addition, a PointsPool layer is proposed to learn compact
    features for interior points in proposals and a parallel IoU branch is introduced
    to improve localization accuracy and detection performance.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Yang等人[[132](#bib.bib132)] 使用2D分割网络来预测前景像素，并将其投影到点云中以去除大部分背景点。然后，他们在预测的前景点上生成提案，并设计了一种新的标准，称为PointsIoU，以减少提案的冗余和模糊性。随后，Shi等人[[133](#bib.bib133)]
    提出了一个PointRCNN框架。具体而言，他们直接对3D点云进行分割以获得前景点，然后融合语义特征和局部空间特征来生成高质量的3D框。继[[133](#bib.bib133)]的区域提案网络（RPN）阶段之后，Jesus等人[[134](#bib.bib134)]
    提出了利用图卷积网络（GCN）进行3D目标检测的开创性工作。具体而言，引入了两个模块，通过图卷积来细化目标提案。第一个模块R-GCN利用提案中包含的所有点来实现每个提案的特征聚合。第二个模块C-GCN融合所有提案的每帧信息，通过利用上下文来回归准确的目标框。Sourabh等人[[135](#bib.bib135)]
    将点云投影到基于图像的分割网络的输出中，并将语义预测分数附加到点上。着色后的点被输入到现有的检测器[[133](#bib.bib133), [136](#bib.bib136),
    [137](#bib.bib137)]中，从而显著提高了性能。Yang等人[[138](#bib.bib138)] 将每个点与一个球形锚点相关联。然后，使用每个点的语义分数来去除冗余锚点。因此，与之前的方法[[132](#bib.bib132),
    [133](#bib.bib133)]相比，这种方法在降低计算成本的同时实现了更高的召回率。此外，提出了一个PointsPool层，用于学习提案中内部点的紧凑特征，并引入了一个并行的IoU分支，以提高定位精度和检测性能。
- en: 'Frustum-based Methods. These methods first leverage existing 2D object detectors
    to generate 2D candidate regions of objects and then extract a 3D frustum proposal
    for each 2D candidate region, as shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.1 Region
    Proposal-based Methods ‣ 4.1 3D Object Detection ‣ 4 3D Object Detection and Tracking
    ‣ Deep Learning for 3D Point Clouds: A Survey")(c). Although these methods can
    efficiently propose possible locations of 3D objects, the step-by-step pipeline
    makes their performance limited by 2D image detectors.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 基于截锥体的方法。这些方法首先利用现有的2D目标检测器生成2D候选区域，然后为每个2D候选区域提取一个3D截锥体提案，如图[8](#S4.F8 "图 8
    ‣ 4.1.1 区域提案方法 ‣ 4.1 3D目标检测 ‣ 4 3D目标检测与跟踪 ‣ 深度学习在3D点云中的应用调查")所示(c)。虽然这些方法能够有效地提出3D目标的可能位置，但逐步的流程使得它们的性能受到2D图像检测器的限制。
- en: F-PointNets [[139](#bib.bib139)] is a pioneering work in this direction. It
    generates a frustum proposal for each 2D region and applies PointNet [[5](#bib.bib5)]
    (or PointNet++ [[54](#bib.bib54)]) to learn point cloud features of each 3D frustum
    for amodal 3D box estimation. In a follow-up work, Zhao et al. [[140](#bib.bib140)]
    proposed a Point-SENet module to predict a set of scaling factors, which were
    further used to adaptively highlight useful features and suppress informative-less
    features. They also integrated the PointSIFT [[141](#bib.bib141)] module into
    the network to capture orientation information of point clouds, which achieved
    strong robustness to shape scaling. This method achieves significant improvement
    on both indoor and outdoor datasets [[14](#bib.bib14), [25](#bib.bib25)] as compared
    to F-PointNets [[139](#bib.bib139)].
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: F-PointNets [[139](#bib.bib139)] 是在这个方向上的开创性工作。它为每个 2D 区域生成一个截锥体提案，并应用 PointNet
    [[5](#bib.bib5)]（或 PointNet++ [[54](#bib.bib54)]）来学习每个 3D 截锥体的点云特征，以进行 amodal
    3D 框估计。在后续工作中，Zhao 等人 [[140](#bib.bib140)] 提出了一个 Point-SENet 模块来预测一组缩放因子，这些因子进一步用于自适应突出有用特征并抑制信息量少的特征。他们还将
    PointSIFT [[141](#bib.bib141)] 模块集成到网络中，以捕捉点云的方向信息，从而实现了对形状缩放的强鲁棒性。与 F-PointNets
    [[139](#bib.bib139)] 相比，这种方法在室内和室外数据集 [[14](#bib.bib14)，[25](#bib.bib25)] 上取得了显著改进。
- en: Xu et al. [[142](#bib.bib142)] leveraged both 2D image region and its corresponding
    frustum points to accurately regress 3D boxes. To fuse image features and global
    features of point clouds, they presented a global fusion network for direct regression
    of box corner locations. They also proposed a dense fusion network for the prediction
    of point-wise offsets to each corner. Shin et al. [[143](#bib.bib143)] first estimated
    2D bounding boxes and 3D poses of objects from a 2D image, and then extracted
    multiple geometrically feasible object candidates. These 3D candidates are fed
    into a box regression network to predict accurate 3D object boxes. Wang et al.
    [[144](#bib.bib144)] generated a sequence of frustums along the frustum axis for
    each 2D region and applied PointNet [[5](#bib.bib5)] to extract features for each
    frustum. The frustum-level features are reformed to generate a 2D feature map,
    which is then fed into a fully convolutional network for 3D box estimation. This
    method achieves the state-of-the-art performance among 2D image-based methods
    and was ranked in the top position of the official KITTI leaderboard. Johannes
    et al. [[145](#bib.bib145)] first obtained a preliminary detection results on
    the BEV map, and then extracted small point subsets (also called patches) based
    on the BEV predictions. A local refinement network is applied to learn the local
    features of patches to predict highly accurate 3D bounding boxes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人 [[142](#bib.bib142)] 利用 2D 图像区域及其对应的截锥体点来准确回归 3D 框。为了融合图像特征和点云的全局特征，他们提出了一个全局融合网络，以直接回归框角位置。他们还提出了一个密集融合网络，用于预测每个角的点位偏移。Shin
    等人 [[143](#bib.bib143)] 首先从 2D 图像中估计 2D 边界框和 3D 对象姿态，然后提取多个几何上可行的对象候选。这些 3D 候选被送入一个框回归网络，以预测准确的
    3D 对象框。Wang 等人 [[144](#bib.bib144)] 为每个 2D 区域沿截锥体轴生成一系列截锥体，并应用 PointNet [[5](#bib.bib5)]
    提取每个截锥体的特征。截锥体级特征被重新构建以生成 2D 特征图，然后输入到一个全卷积网络中进行 3D 框估计。这种方法在基于 2D 图像的方法中实现了最先进的性能，并在官方
    KITTI 排行榜中排名第一。Johannes 等人 [[145](#bib.bib145)] 首先在 BEV 图上获得初步检测结果，然后基于 BEV 预测提取小点子集（也称为补丁）。应用局部优化网络来学习补丁的局部特征，以预测高度准确的
    3D 边界框。
- en: 'TABLE III: Comparative 3D object detection results on the KITTI test 3D detection
    benchmark. 3D bounding box IoU threshold is 0.7 for cars and 0.5 for pedestrians
    and cyclists. The modalities are LiDAR (L) and image (I). ‘E’, ‘M’ and ‘H’ represent
    easy, moderate and hard classes of objects, respectively. For simplicity, we omit
    the ‘%’ after the value. The symbol ‘-’ means the results are unavailable.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：KITTI 测试 3D 检测基准上的比较 3D 对象检测结果。3D 边界框 IoU 阈值对汽车为 0.7，对行人和骑自行车者为 0.5。模态包括
    LiDAR (L) 和图像 (I)。“E”，“M”和“H”分别表示简单、中等和困难的对象类别。为简便起见，我们省略了值后的“%”符号。符号“-”表示结果不可用。
- en: '| Method | Modality | Speed (fps) | Cars | Pedestrians | Cyclists |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 模态 | 速度 (fps) | 汽车 | 行人 | 骑自行车者 |'
- en: '| E | M | H | E | M | H | E | M | H |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| E | M | H | E | M | H | E | M | H |'
- en: '| Region Proposal -based Methods | Multi-view Methods | MV3D [[4](#bib.bib4)]
    | L & I | 2.8 | 74.97 | 63.63 | 54.00 | - | - | - | - | - | - |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 基于区域提案的方法 | 多视角方法 | MV3D [[4](#bib.bib4)] | L & I | 2.8 | 74.97 | 63.63 |
    54.00 | - | - | - | - | - | - |'
- en: '| AVOD [[126](#bib.bib126)] | L & I | 12.5 | 76.39 | 66.47 | 60.23 | 36.10
    | 27.86 | 25.76 | 57.19 | 42.08 | 38.29 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| AVOD [[126](#bib.bib126)] | L & I | 12.5 | 76.39 | 66.47 | 60.23 | 36.10
    | 27.86 | 25.76 | 57.19 | 42.08 | 38.29 |'
- en: '| ContFuse [[127](#bib.bib127)] | L & I | 16.7 | 83.68 | 68.78 | 61.67 | -
    | - | - | - | - | - |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| ContFuse [[127](#bib.bib127)] | L & I | 16.7 | 83.68 | 68.78 | 61.67 | -
    | - | - | - | - | - |'
- en: '| MMF [[128](#bib.bib128)] | L & I | 12.5 | 88.40 | 77.43 | 70.22 | - | - |
    - | - | - | - |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| MMF [[128](#bib.bib128)] | L & I | 12.5 | 88.40 | 77.43 | 70.22 | - | - |
    - | - | - | - |'
- en: '| SCANet [[39](#bib.bib39)] | L & I | 11.1 | 79.22 | 67.13 | 60.65 | - | -
    | - | - | - | - |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| SCANet [[39](#bib.bib39)] | L & I | 11.1 | 79.22 | 67.13 | 60.65 | - | -
    | - | - | - | - |'
- en: '| RT3D [[131](#bib.bib131)] | L & I | 11.1 | 23.74 | 19.14 | 18.86 | - | -
    | - | - | - | - |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| RT3D [[131](#bib.bib131)] | L & I | 11.1 | 23.74 | 19.14 | 18.86 | - | -
    | - | - | - | - |'
- en: '| Segmentation -based Methods | IPOD [[132](#bib.bib132)] | L & I | 5.0 | 80.30
    | 73.04 | 68.73 | 55.07 | 44.37 | 40.05 | 71.99 | 52.23 | 46.50 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 基于分割的方法 | IPOD [[132](#bib.bib132)] | L & I | 5.0 | 80.30 | 73.04 | 68.73
    | 55.07 | 44.37 | 40.05 | 71.99 | 52.23 | 46.50 |'
- en: '| PointRCNN [[133](#bib.bib133)] | L | 10.0 | 86.96 | 75.64 | 70.70 | 47.98
    | 39.37 | 36.01 | 74.96 | 58.82 | 52.53 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| PointRCNN [[133](#bib.bib133)] | L | 10.0 | 86.96 | 75.64 | 70.70 | 47.98
    | 39.37 | 36.01 | 74.96 | 58.82 | 52.53 |'
- en: '| PointRGCN [[134](#bib.bib134)] | L | 3.8 | 85.97 | 75.73 | 70.60 | - | -
    | - | - | - | - |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| PointRGCN [[134](#bib.bib134)] | L | 3.8 | 85.97 | 75.73 | 70.60 | - | -
    | - | - | - | - |'
- en: '|  | PointPainting [[135](#bib.bib135)] | L & I | 2.5 | 82.11 | 71.70 | 67.08
    | 50.32 | 40.97 | 37.87 | 77.63 | 63.78 | 55.89 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | PointPainting [[135](#bib.bib135)] | L & I | 2.5 | 82.11 | 71.70 | 67.08
    | 50.32 | 40.97 | 37.87 | 77.63 | 63.78 | 55.89 |'
- en: '|  | STD [[138](#bib.bib138)] | L | 12.5 | 87.95 | 79.71 | 75.09 | 53.29 |
    42.47 | 38.35 | 78.69 | 61.59 | 55.30 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | STD [[138](#bib.bib138)] | L | 12.5 | 87.95 | 79.71 | 75.09 | 53.29 |
    42.47 | 38.35 | 78.69 | 61.59 | 55.30 |'
- en: '| Frustum -based Methods | F-PointNets [[139](#bib.bib139)] | L & I | 5.9 |
    82.19 | 69.79 | 60.59 | 50.53 | 42.15 | 38.08 | 72.27 | 56.12 | 49.01 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 基于视锥的方法 | F-PointNets [[139](#bib.bib139)] | L & I | 5.9 | 82.19 | 69.79
    | 60.59 | 50.53 | 42.15 | 38.08 | 72.27 | 56.12 | 49.01 |'
- en: '| SIFRNet [[140](#bib.bib140)] | L & I | - | - | - | - | - | - | - | - | -
    | - |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| SIFRNet [[140](#bib.bib140)] | L & I | - | - | - | - | - | - | - | - | -
    | - |'
- en: '| PointFusion [[142](#bib.bib142)] | L & I | - | 77.92 | 63.00 | 53.27 | 33.36
    | 28.04 | 23.38 | 49.34 | 29.42 | 26.98 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| PointFusion [[142](#bib.bib142)] | L & I | - | 77.92 | 63.00 | 53.27 | 33.36
    | 28.04 | 23.38 | 49.34 | 29.42 | 26.98 |'
- en: '| RoarNet [[143](#bib.bib143)] | L & I | 10.0 | 83.71 | 73.04 | 59.16 | - |
    - | - | - | - | - |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| RoarNet [[143](#bib.bib143)] | L & I | 10.0 | 83.71 | 73.04 | 59.16 | - |
    - | - | - | - | - |'
- en: '| F-ConvNet [[144](#bib.bib144)] | L & I | 2.1 | 87.36 | 76.39 | 66.69 | 52.16
    | 43.38 | 38.80 | 81.98 | 65.07 | 56.54 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| F-ConvNet [[144](#bib.bib144)] | L & I | 2.1 | 87.36 | 76.39 | 66.69 | 52.16
    | 43.38 | 38.80 | 81.98 | 65.07 | 56.54 |'
- en: '|'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Patch Refinement [[145](#bib.bib145)] &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Patch Refinement [[145](#bib.bib145)] &#124;'
- en: '| L | 6.7 | 88.67 | 77.20 | 71.82 | - | - | - | - | - | - |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| L | 6.7 | 88.67 | 77.20 | 71.82 | - | - | - | - | - | - |'
- en: '| Other Methods | 3D IoU loss [[146](#bib.bib146)] | L | 12.5 | 86.16 | 76.50
    | 71.39 | - | - | - | - | - | - |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 其他方法 | 3D IoU loss [[146](#bib.bib146)] | L | 12.5 | 86.16 | 76.50 | 71.39
    | - | - | - | - | - | - |'
- en: '|'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fast Point R-CNN [[147](#bib.bib147)] &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fast Point R-CNN [[147](#bib.bib147)] &#124;'
- en: '| L | 16.7 | 84.80 | 74.59 | 67.27 | - | - | - | - | - | - |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| L | 16.7 | 84.80 | 74.59 | 67.27 | - | - | - | - | - | - |'
- en: '|  | PV-RCNN [[148](#bib.bib148)] | L | 12.5 | 90.25 | 81.43 | 76.82 | - |
    - | - | - | - | - |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | PV-RCNN [[148](#bib.bib148)] | L | 12.5 | 90.25 | 81.43 | 76.82 | - |
    - | - | - | - | - |'
- en: '|  | VoteNet [[124](#bib.bib124)] | L | - | - | - | - | - | - | - | - | - |
    - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | VoteNet [[124](#bib.bib124)] | L | - | - | - | - | - | - | - | - | - |
    - |'
- en: '|  |  | Feng et al. [[149](#bib.bib149)] | L | - | - | - | - | - | - | - |
    - | - | - |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Feng et al. [[149](#bib.bib149)] | L | - | - | - | - | - | - | - |
    - | - | - |'
- en: '|  |  | ImVoteNet [[150](#bib.bib150)] | L & I | - | - | - | - | - | - | -
    | - | - | - |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ImVoteNet [[150](#bib.bib150)] | L & I | - | - | - | - | - | - | -
    | - | - | - |'
- en: '|  |  | Part-A^2 [[151](#bib.bib151)] | L | 12.5 | 87.81 | 78.49 | 73.51 |
    - | - | - | - | - | - |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Part-A^2 [[151](#bib.bib151)] | L | 12.5 | 87.81 | 78.49 | 73.51 |
    - | - | - | - | - | - |'
- en: '| Single Shot Methods | BEV-based Methods | PIXOR [[129](#bib.bib129)] | L
    | 28.6 | - | - | - | - | - | - | - | - | - |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 单次方法 | BEV-based 方法 | PIXOR [[129](#bib.bib129)] | L | 28.6 | - | - | - |
    - | - | - | - | - | - |'
- en: '| HDNET [[152](#bib.bib152)] | L | 20.0 | - | - | - | - | - | - | - | - | -
    |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| HDNET [[152](#bib.bib152)] | L | 20.0 | - | - | - | - | - | - | - | - | -
    |'
- en: '| BirdNet [[153](#bib.bib153)] | L | 9.1 | 13.53 | 9.47 | 8.49 | 12.25 | 8.99
    | 8.06 | 16.63 | 10.46 | 9.53 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| BirdNet [[153](#bib.bib153)] | L | 9.1 | 13.53 | 9.47 | 8.49 | 12.25 | 8.99
    | 8.06 | 16.63 | 10.46 | 9.53 |'
- en: '| Discretization -based Methods | VeloFCN [[154](#bib.bib154)] | L | 1.0 |
    - | - | - | - | - | - | - | - | - |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 基于离散化的方法 | VeloFCN [[154](#bib.bib154)] | L | 1.0 | - | - | - | - | - | -
    | - | - | - |'
- en: '| 3D FCN [[155](#bib.bib155)] | L | <0.2 | - | - | - | - | - | - | - | - |
    - |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 3D FCN [[155](#bib.bib155)] | L | <0.2 | - | - | - | - | - | - | - | - |
    - |'
- en: '| Vote3Deep [[156](#bib.bib156)] | L | - | - | - | - | - | - | - | - | - |
    - |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Vote3Deep [[156](#bib.bib156)] | L | - | - | - | - | - | - | - | - | - |
    - |'
- en: '| 3DBN [[157](#bib.bib157)] | L | 7.7 | 83.77 | 73.53 | 66.23 | - | - | - |
    - | - | - |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 3DBN [[157](#bib.bib157)] | L | 7.7 | 83.77 | 73.53 | 66.23 | - | - | - |
    - | - | - |'
- en: '| VoxelNet [[136](#bib.bib136)] | L | 2.0 | 77.47 | 65.11 | 57.73 | 39.48 |
    33.69 | 31.51 | 61.22 | 48.36 | 44.37 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| VoxelNet [[136](#bib.bib136)] | L | 2.0 | 77.47 | 65.11 | 57.73 | 39.48 |
    33.69 | 31.51 | 61.22 | 48.36 | 44.37 |'
- en: '| SECOND [[158](#bib.bib158)] | L | 26.3 | 83.34 | 72.55 | 65.82 | 48.96 |
    38.78 | 34.91 | 71.33 | 52.08 | 45.83 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| SECOND [[158](#bib.bib158)] | L | 26.3 | 83.34 | 72.55 | 65.82 | 48.96 |
    38.78 | 34.91 | 71.33 | 52.08 | 45.83 |'
- en: '| MVX-Net [[159](#bib.bib159)] | L & I | 16.7 | 84.99 | 71.95 | 64.88 | - |
    - | - | - | - | - |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| MVX-Net [[159](#bib.bib159)] | L & I | 16.7 | 84.99 | 71.95 | 64.88 | - |
    - | - | - | - | - |'
- en: '| PointPillars [[137](#bib.bib137)] | L | 62.0 | 82.58 | 74.31 | 68.99 | 51.45
    | 41.92 | 38.89 | 77.10 | 58.65 | 51.92 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| PointPillars [[137](#bib.bib137)] | L | 62.0 | 82.58 | 74.31 | 68.99 | 51.45
    | 41.92 | 38.89 | 77.10 | 58.65 | 51.92 |'
- en: '|  | SA-SSD [[160](#bib.bib160)] | L | 25.0 | 88.75 | 79.79 | 74.16 | - | -
    | - | - | - | - |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | SA-SSD [[160](#bib.bib160)] | L | 25.0 | 88.75 | 79.79 | 74.16 | - | -
    | - | - | - | - |'
- en: '|'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Point-based &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于点的方法 &#124;'
- en: '&#124; Methods &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '| 3DSSD [[161](#bib.bib161)] | L | 25.0 | 88.36 | 79.57 | 74.55 | 54.64 | 44.27
    | 40.23 | 82.48 | 64.10 | 56.90 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 3DSSD [[161](#bib.bib161)] | L | 25.0 | 88.36 | 79.57 | 74.55 | 54.64 | 44.27
    | 40.23 | 82.48 | 64.10 | 56.90 |'
- en: '|  | Other Methods | LaserNet [[162](#bib.bib162)] | L | 83.3 | - | - | - |
    - | - | - | - | - | - |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | 其他方法 | LaserNet [[162](#bib.bib162)] | L | 83.3 | - | - | - | - | - |
    - | - | - | - |'
- en: '|  | LaserNet++ [[163](#bib.bib163)] | L & I | 26.3 | - | - | - | - | - | -
    | - | - | - |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | LaserNet++ [[163](#bib.bib163)] | L & I | 26.3 | - | - | - | - | - | -
    | - | - | - |'
- en: '|  |  | OHS-Dense [[164](#bib.bib164)] | L | 33.3 | 88.12 | 78.34 | 73.49 |
    47.14 | 39.72 | 37.25 | 79.09 | 62.72 | 56.76 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  |  | OHS-Dense [[164](#bib.bib164)] | L | 33.3 | 88.12 | 78.34 | 73.49 |
    47.14 | 39.72 | 37.25 | 79.09 | 62.72 | 56.76 |'
- en: '|  |  | OHS-Direct [[164](#bib.bib164)] | L | 33.3 | 86.40 | 77.74 | 72.97
    | 51.29 | 44.81 | 41.13 | 77.70 | 63.16 | 57.16 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  |  | OHS-Direct [[164](#bib.bib164)] | L | 33.3 | 86.40 | 77.74 | 72.97
    | 51.29 | 44.81 | 41.13 | 77.70 | 63.16 | 57.16 |'
- en: '|  |  | Point-GNN [[125](#bib.bib125)] | L | 1.7 | 88.33 | 79.47 | 72.29 |
    51.92 | 43.77 | 40.14 | 78.60 | 63.48 | 57.08 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Point-GNN [[125](#bib.bib125)] | L | 1.7 | 88.33 | 79.47 | 72.29 |
    51.92 | 43.77 | 40.14 | 78.60 | 63.48 | 57.08 |'
- en: 'Other Methods. Motivated by the success of axis-aligned IoU in object detection
    in images, Zhou et al. [[146](#bib.bib146)] integrated the IoU of two 3D rotated
    bounding boxes into several state-of-the-art detectors [[158](#bib.bib158), [137](#bib.bib137),
    [133](#bib.bib133)] to achieve consistent performance improvement. Chen et al.
    [[147](#bib.bib147)] proposed a two-stage network architecture to use both point
    cloud and voxel representations. First, point clouds are voxelized and fed to
    a 3D backbone network to produce initial detection results. Second, the interior
    point features of initial predictions are further exploited for box refinements.
    Although this design is conceptually simple, it achieves comparable performance
    to [[133](#bib.bib133)] while maintaining a speed of 16.7 fps. Shi et al. [[148](#bib.bib148)]
    proposed PointVoxel-RCNN (PV-RCNN) to leverage both 3D convolutional network and
    PointNet-based set abstraction for the learning of point cloud features. Specifically,
    the input point clouds are first voxelized and then fed into a 3D sparse convolutional
    network to generate high-quality proposals. The learned voxel-wise features are
    then encoded into a small set of key points via a voxel set abstraction module.
    In addition, they also proposed a keypoint-to-grid ROI abstraction module to capture
    rich context information for box refinement. Experimental results show that this
    method outperforms previous methods by a remarkable margin and is ranked first¹¹1The
    ranking refers to the time of the submission: 12th June, 2020 on the $Car$ class
    of the KITTI 3D detection benchmark.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法。受轴对齐IoU在图像目标检测中的成功启发，Zhou 等人 [[146](#bib.bib146)] 将两个3D旋转边界框的IoU集成到几个最先进的检测器中
    [[158](#bib.bib158), [137](#bib.bib137), [133](#bib.bib133)]，实现了性能的一致性提升。Chen
    等人 [[147](#bib.bib147)] 提出了一个两阶段的网络架构，使用点云和体素表示。首先，点云被体素化并输入到3D骨干网络中以产生初始检测结果。其次，进一步利用初始预测的内部点特征进行框体精修。虽然这个设计在概念上简单，但它在保持16.7
    fps的速度下，与 [[133](#bib.bib133)] 的性能相当。Shi 等人 [[148](#bib.bib148)] 提出了 PointVoxel-RCNN
    (PV-RCNN)，利用3D卷积网络和基于PointNet的集合抽象来学习点云特征。具体来说，输入的点云首先被体素化，然后输入到3D稀疏卷积网络中以生成高质量的提议。学习到的体素特征随后通过体素集合抽象模块编码成一小组关键点。此外，他们还提出了一个关键点到网格的ROI抽象模块，以捕捉丰富的上下文信息用于框体精修。实验结果表明，该方法显著超越了之前的方法，并在KITTI
    3D检测基准的$Car$类别中排名第一¹¹1排名指的是提交时间：2020年6月12日。
- en: Inspired by Hough voting-based 2D object detectors, Qi et al. [[124](#bib.bib124)]
    proposed VoteNet to directly vote for virtual center points of objects from point
    clouds and to generate a group of high-quality 3D object proposals by aggregating
    vote features. VoteNet significantly outperforms previous approaches using only
    geometric information, and achieves the state-of-the-art performance on two large
    indoor benchmarks (i.e., ScanNet [[11](#bib.bib11)] and SUN RGB-D [[25](#bib.bib25)]).
    However, the prediction of virtual center point is unstable for a partially occluded
    object. Further, Feng et al. [[149](#bib.bib149)] added an auxiliary branch of
    direction vectors to improve the prediction accuracy of virtual center points
    and 3D candidate boxes. In addition, a 3D object-object relationship graph between
    proposals is built to emphasize useful features for accurate object detection.
    Qi et al. [[150](#bib.bib150)] proposed an ImVoteNet detector by fusing 2D object
    detection cues (e.g., geometric and semantic/texture cues) into a 3D voting pipeline.
    Inspired by the observation that the ground truth boxes of 3D objects provide
    accurate locations of intra-object parts, Shi et al. [[151](#bib.bib151)] proposed
    the Part-$A^{2}$ Net, which is composed of a part-aware stage and a part-aggregation
    stage. The part-aware stage applies a UNet-like [[165](#bib.bib165)] network with
    sparse convolution and sparse deconvolution to learn point-wise features for the
    prediction and coarse generation of intra-object part locations. The part-aggregation
    stage adopts RoI-aware pooling to aggregate predicted part locations for box refinement.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 受到基于霍夫投票的2D物体检测器的启发，Qi 等人 [[124](#bib.bib124)] 提出了 VoteNet，该方法通过点云直接投票以获得物体的虚拟中心点，并通过聚合投票特征生成一组高质量的3D物体提议。VoteNet显著优于仅使用几何信息的前述方法，并在两个大型室内基准（即
    ScanNet [[11](#bib.bib11)] 和 SUN RGB-D [[25](#bib.bib25)])上实现了最先进的性能。然而，对于部分遮挡的物体，虚拟中心点的预测不稳定。进一步地，Feng
    等人 [[149](#bib.bib149)] 增加了一个方向向量的辅助分支，以提高虚拟中心点和3D候选框的预测准确性。此外，构建了一个提议之间的3D物体-物体关系图，以强调对准确物体检测有用的特征。Qi
    等人 [[150](#bib.bib150)] 提出了一个 ImVoteNet 检测器，将2D物体检测线索（例如，几何和语义/纹理线索）融合到3D投票管道中。受到3D物体的真实边界框提供准确的物体内部部件位置的观察启发，Shi
    等人 [[151](#bib.bib151)] 提出了 Part-$A^{2}$ Net，该网络由一个部件感知阶段和一个部件聚合阶段组成。部件感知阶段应用了类似
    UNet 的 [[165](#bib.bib165)] 网络，利用稀疏卷积和稀疏反卷积来学习逐点特征，用于预测和粗略生成物体内部部件的位置。部件聚合阶段采用
    RoI 感知池化来聚合预测的部件位置，以进行框的精细化。
- en: 4.1.2 Single Shot Methods
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 单次检测方法
- en: 'These methods directly predict class probabilities and regress 3D bounding
    boxes of objects using a single-stage network. They do not need region proposal
    generation and post-processing. As a result, they can run at a high speed. According
    to the type of input data, single shot methods can be divided into three categories:
    BEV-based, discretization-based and point-based methods.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法直接预测类别概率，并使用单阶段网络回归物体的3D边界框。它们不需要区域提议生成和后处理，因此可以高速运行。根据输入数据的类型，单次检测方法可以分为三类：基于BEV的、基于离散化的和基于点的。
- en: BEV-based Methods. These methods mainly take BEV representation as their input.
    Yang et al. [[129](#bib.bib129)] discretized the point cloud of a scene with equally
    spaced cells and encoded the reflectance in a similar way, resulting in a regular
    representation. A Fully Convolution Network (FCN) network is then applied to estimate
    the locations and heading angles of objects. This method outperforms most single
    shot methods (including VeloFCN [[154](#bib.bib154)], 3D-FCN [[155](#bib.bib155)]
    and Vote3Deep [[156](#bib.bib156)]) while running at 28.6 fps. Later, Yang et
    al. [[152](#bib.bib152)] exploited the geometric and semantic prior information
    provided by High-Definition (HD) maps to improve the robustness and detection
    performance of [[129](#bib.bib129)]. Specifically, they obtained the coordinates
    of ground points from the HD map and then used the distance relative to the ground
    for BEV representation to remedy the translation variance caused by the slope
    of the road. In addition, they concatenated a binary road mask with the BEV representation
    along the channel dimension to focus on moving objects. Since HD maps are not
    available everywhere, they also proposed an online map prediction module to estimate
    the map priors from single LiDAR point cloud. This map-aware method significantly
    outperforms its baseline on the TOR4D [[129](#bib.bib129), [130](#bib.bib130)]
    and KITTI [[14](#bib.bib14)] datasets. However, its generalization performance
    to point clouds with different densities is poor. To solve this problem, Beltrán
    et al. [[153](#bib.bib153)] proposed a normalization map to consider the differences
    among different LiDAR sensors. The normalization map is a 2D grid with the same
    resolution as the BEV map, and it encodes the maximum number of points contained
    in each cell. It is shown that this normalization map significantly improves the
    generalization ability of BEV-based detectors.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 BEV 的方法。这些方法主要以 BEV 表示作为输入。杨等人[[129](#bib.bib129)] 使用等间距的单元格离散化场景中的点云，并以类似的方式编码反射率，得到了一种规则的表示。然后应用全卷积网络（FCN）来估计物体的位置和朝向角度。该方法在
    28.6 fps 的速度下超越了大多数单次检测方法（包括 VeloFCN [[154](#bib.bib154)]、3D-FCN [[155](#bib.bib155)]
    和 Vote3Deep [[156](#bib.bib156)]）。随后，杨等人[[152](#bib.bib152)] 利用高清地图（HD）提供的几何和语义先验信息来提高[[129](#bib.bib129)]
    的鲁棒性和检测性能。具体来说，他们从高清地图中获得了地面点的坐标，然后使用相对于地面的距离进行 BEV 表示，以修正由于道路坡度引起的平移方差。此外，他们将一个二值道路掩码与
    BEV 表示在通道维度上连接，以聚焦于移动物体。由于高清地图并非随处可用，他们还提出了一个在线地图预测模块，用于从单个 LiDAR 点云估计地图先验。该地图感知方法在
    TOR4D [[129](#bib.bib129)、[130](#bib.bib130)] 和 KITTI [[14](#bib.bib14)] 数据集上的表现显著优于基线。然而，它对不同密度点云的泛化性能较差。为解决此问题，Beltrán
    等人[[153](#bib.bib153)] 提出了一个归一化地图，以考虑不同 LiDAR 传感器之间的差异。归一化地图是一个与 BEV 地图分辨率相同的
    2D 网格，编码了每个单元格中包含的最大点数。结果表明，这种归一化地图显著提高了基于 BEV 的检测器的泛化能力。
- en: Discretization-based Methods. These methods convert a point cloud into a regular
    discrete representation, and then apply CNN to predict both categories and 3D
    boxes of objects.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 离散化基础的方法。这些方法将点云转换为规则的离散表示，然后应用 CNN 预测物体的类别和 3D 盒子。
- en: Li et al. [[154](#bib.bib154)] proposed the first method to use a FCN for 3D
    object detection. They converted a point cloud into a 2D point map and used a
    2D FCN to predict the bounding boxes and confidences of objects. Later, they [[155](#bib.bib155)]
    discretized the point cloud into a 4D tensor with dimensions of length, width,
    height and channels, and extended the 2D FCN-based detection technologies to 3D
    domain for 3D object detection. Compared to [[154](#bib.bib154)], 3D FCN-based
    method [[155](#bib.bib155)] obtains a gain of over 20% in accuracy, but inevitably
    costs more computing resources due to 3D convolutions and the sparsity of the
    data. To address the sparsity problem of voxels, Engelcke et al. [[156](#bib.bib156)]
    leveraged a feature-centric voting scheme to generate a set of votes for each
    non-empty voxel and to obtain the convolutional results by accumulating the votes.
    Its computational complexity is proportional to the number of occupied voxels.
    Li et al. [[157](#bib.bib157)] constructed a 3D backbone network by stacking multiple
    sparse 3D CNNs. This method is designed to save memory and accelerate computation
    by fully using the sparsity of voxels. This 3D backbone network extracts rich
    3D features for object detection without introducing heavy computational burden.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 [[154](#bib.bib154)] 提出了第一种使用 FCN 进行 3D 对象检测的方法。他们将点云转换为 2D 点图，并使用 2D
    FCN 来预测物体的边界框和置信度。后来，他们 [[155](#bib.bib155)] 将点云离散化为一个具有长度、宽度、高度和通道维度的 4D 张量，并将基于
    2D FCN 的检测技术扩展到 3D 领域进行 3D 对象检测。与 [[154](#bib.bib154)] 相比，基于 3D FCN 的方法 [[155](#bib.bib155)]
    在准确性上提高了超过 20%，但由于 3D 卷积和数据的稀疏性，必然会消耗更多的计算资源。为了解决体素稀疏性问题，Engelcke 等人 [[156](#bib.bib156)]
    利用了一种以特征为中心的投票方案，为每个非空体素生成一组投票，并通过累积投票来获得卷积结果。其计算复杂度与占用体素的数量成正比。Li 等人 [[157](#bib.bib157)]
    通过堆叠多个稀疏 3D CNN 构建了一个 3D 主干网络。这种方法旨在通过充分利用体素的稀疏性来节省内存和加速计算。该 3D 主干网络提取丰富的 3D 特征用于对象检测，而不会引入过重的计算负担。
- en: Zhou et al. [[136](#bib.bib136)] presented a voxel-based end-to-end trainable
    framework VoxelNet. They partitioned a point cloud into equally spaced voxels
    and encoded the features within each voxel into a 4D tensor. A region proposal
    network is then connected to produce detection results. Although its performance
    is strong, this method is very slow due to the sparsity of voxels and 3D convolutions.
    Later, Yan et al. [[158](#bib.bib158)] used the sparse convolutional network [[166](#bib.bib166)]
    to improve the inference efficiency of [[136](#bib.bib136)]. They also proposed
    a sine-error angle loss to solve the ambiguity between orientations of 0 and $\pi$.
    Sindagi et al. [[159](#bib.bib159)] extended VoxelNet by fusing image and point
    cloud features at early stages. Specifically, they projected non-empty voxels
    generated by [[136](#bib.bib136)] into the image and used a pre-trained network
    to extract image features for each projected voxel. These image features are then
    concatenated with voxel features to produce accurate 3D boxes. Compared to [[136](#bib.bib136),
    [158](#bib.bib158)], this method can effectively exploit multi-modal information
    to reduce false positives and negatives. Lang et al. [[137](#bib.bib137)] proposed
    a 3D object detector named PointPillars. This method leverages PointNet [[5](#bib.bib5)]
    to learn the feature of point clouds organized in vertical columns (Pillars) and
    encodes the learned features as a pesudo image. A 2D object detection pipeline
    is then applied to predict 3D bounding boxes. PointPillars outperforms most fusion
    approaches (including MV3D [[4](#bib.bib4)], RoarNet [[143](#bib.bib143)] and
    AVOD [[126](#bib.bib126)]) in terms of Average Precision (AP). Moreover, PointPillars
    can run at a speed of 62 fps on both the 3D and BEV KITTI [[14](#bib.bib14)] benchmarks,
    making it highly suitable for practical applications.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou 等人 [[136](#bib.bib136)] 提出了一个基于体素的端到端可训练框架 VoxelNet。他们将点云划分为等间距的体素，并将每个体素中的特征编码为
    4D 张量。然后连接一个区域提议网络以生成检测结果。尽管性能强大，但由于体素的稀疏性和 3D 卷积，该方法非常缓慢。随后，Yan 等人 [[158](#bib.bib158)]
    使用稀疏卷积网络 [[166](#bib.bib166)] 提高了 [[136](#bib.bib136)] 的推理效率。他们还提出了一种正弦误差角度损失来解决
    0 和 $\pi$ 之间的方向模糊问题。Sindagi 等人 [[159](#bib.bib159)] 通过在早期阶段融合图像和点云特征扩展了 VoxelNet。具体来说，他们将
    [[136](#bib.bib136)] 生成的非空体素投影到图像中，并使用预训练网络为每个投影体素提取图像特征。这些图像特征随后与体素特征连接，以生成准确的
    3D 边界框。与 [[136](#bib.bib136)] 和 [[158](#bib.bib158)] 相比，该方法能够有效利用多模态信息来减少假阳性和假阴性。Lang
    等人 [[137](#bib.bib137)] 提出了一个名为 PointPillars 的 3D 目标检测器。该方法利用 PointNet [[5](#bib.bib5)]
    学习以垂直列（Pillars）组织的点云特征，并将学习到的特征编码为伪图像。然后应用 2D 目标检测管道来预测 3D 边界框。PointPillars 在平均精度
    (AP) 方面优于大多数融合方法（包括 MV3D [[4](#bib.bib4)]、RoarNet [[143](#bib.bib143)] 和 AVOD
    [[126](#bib.bib126)]）。此外，PointPillars 在 3D 和 BEV KITTI [[14](#bib.bib14)] 基准测试中均能以
    62 fps 的速度运行，使其非常适合实际应用。
- en: 'Inspired by the observation that partial spatial information of a point cloud
    is inevitably lost in progressively downscaled feature maps of existing single
    shot detectors, He et al. [[160](#bib.bib160)] proposed a SA-SSD detector to leverage
    the fine-grained structure information to improve localization accuracy. Specifically,
    they first converted a point cloud to a tensor and fed it into a backbone network
    to extract multi-stage features. In addition, an auxiliary network with point-level
    supervision is employed to guide the features to learn the structure of point
    clouds. Experimental results show that SA-SSD ranks the first²²2The ranking refers
    to the time of the submission: 12th June, 2020 on the $Car$ class of the KITTI
    BEV detection benchmark.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 受限于现有单次检测器的逐步下采样特征图中不可避免地丢失了点云的部分空间信息的观察，He 等人 [[160](#bib.bib160)] 提出了一个 SA-SSD
    检测器，利用细粒度的结构信息来提高定位精度。具体来说，他们首先将点云转换为张量，并将其输入主干网络以提取多阶段特征。此外，还采用了一个带有点级监督的辅助网络来指导特征学习点云的结构。实验结果表明，SA-SSD
    在 KITTI BEV 检测基准的 $Car$ 类别中排名第一²²2排名指的是提交时间：2020年6月12日。
- en: Point-based Methods. These methods directly take raw point clouds as their inputs.
    3DSSD [[161](#bib.bib161)] is a pioneering work in this direction. It introduces
    a fusion sampling strategy for Distance-FPS (D-FPS) and Feature-FPS (F-FPS) to
    remove time-consuming Feature Propagation (FP) layers and the refinement module
    in [[133](#bib.bib133)]. Then, a Candidate Generation (CG) layer is used to fully
    exploit representative points, which are further fed into an anchor-free regression
    head with a 3D centerness label to predict 3D object boxes. Experimental results
    show that 3DSSD outperforms the two-stage point-based method PointRCNN [[133](#bib.bib133)]
    while maintaining a speed of 25 fps.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 基于点的方法。这些方法直接以原始点云作为输入。3DSSD [[161](#bib.bib161)] 是在这一方向上的开创性工作。它引入了一种融合采样策略，用于距离-FPS
    (D-FPS) 和特征-FPS (F-FPS)，以去除耗时的特征传播 (FP) 层和 [[133](#bib.bib133)] 中的细化模块。然后，使用候选生成
    (CG) 层充分利用代表性点，这些点进一步输入到无锚回归头，并带有3D中心度标签，以预测3D物体框。实验结果表明，3DSSD 在保持25 fps的速度下，优于两阶段的基于点的方法
    PointRCNN [[133](#bib.bib133)]。
- en: Other Methods. Meyer et al. [[162](#bib.bib162)] proposed an efficient 3D object
    detector called LaserNet. This method predicts a probability distribution over
    bounding boxes for each point and then combines these per-point distributions
    to generate final 3D object boxes. Further, the dense Range View (RV) representation
    of point cloud is used as input and a fast mean-shift algorithm is proposed to
    reduce the noise produced by per-point prediction. LaserNet achieves the state-of-the-art
    performance at the range of 0 to 50 meters, and its runtime is significantly lower
    than existing methods. Meyer et al. [[163](#bib.bib163)] then extended LaserNet
    [[162](#bib.bib162)] to exploit the dense texture provided by RGB images (e.g.,
    50 to 70 meters). Specifically, they associated LiDAR points with image pixels
    by projecting 3D point clouds onto 2D images and exploited this association to
    fuse RGB information into 3D points. They also considered 3D semantic segmentation
    as an auxiliary task to learn better representations. This method achieves a significant
    improvement in both long-range (e.g., 50 to 70 meters) object detection and semantic
    segmentation while maintaining high efficiency of LaserNet. Inspired by the observation
    that points on an isolated object part can provide abundant information about
    position and orientation of the object, Chen et al. [[164](#bib.bib164)] proposed
    a novel $Hotspot$ representation and the first hotspot-based anchor-free detector.
    Specifically, raw point clouds are first voxelized and then fed into a backbone
    network to produce 3D feature maps. These feature maps are used to classify hotspots
    and predict 3D bounding boxes simultaneously. Note that, hotspots are assigned
    at the last convolutional layer of the backbone network. Experimental results
    show that this method achieves comparable performance and is robust to sparse
    point clouds. Shi et el. [[125](#bib.bib125)] proposed a graph neural network
    Point-GNN to detect 3D objects from lidar point clouds. They first encoded an
    input point cloud as a graph of near neighbors with a fixed radius and then fed
    the graph into Point-GNN to predict both the categories and boxes of objects.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法。Meyer 等人 [[162](#bib.bib162)] 提出了一个高效的3D物体检测器，称为 LaserNet。该方法预测每个点的边界框概率分布，然后将这些每点分布组合以生成最终的3D物体框。此外，点云的密集范围视图
    (RV) 表示被用作输入，并提出了一种快速均值漂移算法以减少每点预测产生的噪声。LaserNet 在0到50米范围内实现了最先进的性能，其运行时间显著低于现有方法。Meyer
    等人 [[163](#bib.bib163)] 随后扩展了 LaserNet [[162](#bib.bib162)]，利用了 RGB 图像提供的密集纹理（例如，50到70米）。具体而言，他们通过将3D点云投影到2D图像上，将
    LiDAR 点与图像像素关联，并利用这种关联将 RGB 信息融合到3D点中。他们还考虑了3D语义分割作为辅助任务，以学习更好的表示。该方法在长距离（例如，50到70米）的物体检测和语义分割上取得了显著的改进，同时保持了
    LaserNet 的高效性。受到孤立物体部分的点可以提供有关物体位置和方向丰富信息的观察启发，Chen 等人 [[164](#bib.bib164)] 提出了一个新颖的
    $Hotspot$ 表示法和第一个基于热点的无锚检测器。具体而言，原始点云首先被体素化，然后输入到主干网络中生成3D特征图。这些特征图被用来同时分类热点和预测3D边界框。注意，热点是在主干网络的最后一层卷积层分配的。实验结果表明，该方法在处理稀疏点云时表现出色，并且性能可与其他方法相媲美。Shi
    等人 [[125](#bib.bib125)] 提出了一个图神经网络 Point-GNN，以从 LiDAR 点云中检测3D物体。他们首先将输入点云编码为固定半径的邻近图，然后将图输入到
    Point-GNN 中，以预测物体的类别和框。
- en: 'TABLE IV: Comparative 3D object detection results on the KITTI test BEV detection
    benchmark. 3D bounding box IoU threshold is 0.7 for cars and 0.5 for pedestrians
    and cyclists. The modalities are LiDAR (L) and image (I). ‘E’, ‘M’ and ‘H’ represent
    easy, moderate and hard classes of objects, respectively. For simplicity, we omit
    the ‘%’ after the value. The symbol ‘-’ means the results are unavailable.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Modality | Speed (fps) | Cars | Pedestrians | Cyclists |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| E | M | H | E | M | H | E | M | H |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| Region Proposal -based Methods | Multi-view Methods | MV3D [[4](#bib.bib4)]
    | L & I | 2.8 | 86.62 | 78.93 | 69.80 | - | - | - | - | - | - |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| AVOD [[126](#bib.bib126)] | L & I | 12.5 | 89.75 | 84.95 | 78.32 | 42.58
    | 33.57 | 30.14 | 64.11 | 48.15 | 42.37 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| ContFuse [[127](#bib.bib127)] | L & I | 16.7 | 94.07 | 85.35 | 75.88 | -
    | - | - | - | - | - |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| MMF [[128](#bib.bib128)] | L & I | 12.5 | 93.67 | 88.21 | 81.99 | - | - |
    - | - | - | - |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| SCANet [[39](#bib.bib39)] | L & I | 11.1 | 90.33 | 82.85 | 76.06 | - | -
    | - | - | - | - |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| RT3D [[131](#bib.bib131)] | L & I | 11.1 | 56.44 | 44.00 | 42.34 | - | -
    | - | - | - | - |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| Segmentation -based Methods | IPOD [[132](#bib.bib132)] | L & I | 5.0 | 89.64
    | 84.62 | 79.96 | 60.88 | 49.79 | 45.43 | 78.19 | 59.40 | 51.38 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| PointRCNN [[133](#bib.bib133)] | L | 10.0 | 92.13 | 87.39 | 82.72 | 54.77
    | 46.13 | 42.84 | 82.56 | 67.24 | 60.28 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| PointRGCN [[134](#bib.bib134)] | L | 3.8 | 91.63 | 87.49 | 80.73 | - | -
    | - | - | - | - |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '|  | PointPainting [[135](#bib.bib135)] | L & I | 2.5 | 92.45 | 88.11 | 83.36
    | 58.70 | 49.93 | 46.29 | 83.91 | 71.54 | 62.97 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '|  | STD [[138](#bib.bib138)] | L | 12.5 | 94.74 | 89.19 | 86.42 | 60.02 |
    48.72 44.55 | 81.36 | 67.23 | 59.35 |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| Frustum -based Methods | F-PointNets [[139](#bib.bib139)] | L & I | 5.9 |
    91.17 | 84.67 | 74.77 | 57.13 | 49.57 | 45.48 | 77.26 | 61.37 | 53.78 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| SIFRNet [[140](#bib.bib140)] | L & I | - | - | - | - | - | - | - | - | -
    | - |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| PointFusion [[142](#bib.bib142)] | L & I | - | - | - | - | - | - | - | -
    | - | - |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| RoarNet [[143](#bib.bib143)] | L & I | 10.0 | 88.20 | 79.41 | 70.02 | - |
    - | - | - | - | - |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| F-ConvNet [[144](#bib.bib144)] | L & I | 2.1 | 91.51 | 85.84 | 76.11 | 57.04
    | 48.96 | 44.33 | 84.16 | 68.88 | 60.05 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Patch Refinement [[145](#bib.bib145)] &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '| L | 6.7 | 92.72 | 88.39 | 83.19 | - | - | - | - | - | - |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| Other Methods | 3D IoU loss [[146](#bib.bib146)] | L | 12.5 | 91.36 | 86.22
    | 81.20 | - | - | - | - | - | - |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fast Point R-CNN [[147](#bib.bib147)] &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '| L | 16.7 | 90.76 | 85.61 | 79.99 | - | - | - | - | - | - |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '|  | PV-RCNN [[148](#bib.bib148)] | L | 12.5 | 94.98 | 90.65 | 86.14 | - |
    - | - | 82.49 | 68.89 | 62.41 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '|  | VoteNet [[124](#bib.bib124)] | L | - | - | - | - | - | - | - | - | - |
    - |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '|  |  | Feng et al. [[149](#bib.bib149)] | L | - | - | - | - | - | - | - |
    - | - | - |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '|  |  | ImVoteNet [[150](#bib.bib150)] | L & I | - | - | - | - | - | - | -
    | - | - | - |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '|  |  | Part-A^2 [[151](#bib.bib151)] | L | 12.5 | 91.70 | 87.79 | 84.61 |
    - | - | - | 81.91 | 68.12 | 61.92 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Part-A^2 [[151](#bib.bib151)] | L | 12.5 | 91.70 | 87.79 | 84.61 |
    - | - | - | 81.91 | 68.12 | 61.92 |'
- en: '| Single Shot Methods | BEV-based Methods | PIXOR [[129](#bib.bib129)] | L
    | 28.6 | 83.97 | 80.01 | 74.31 | - | - | - | - | - | - |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 单次方法 | BEV-based 方法 | PIXOR [[129](#bib.bib129)] | L | 28.6 | 83.97 | 80.01
    | 74.31 | - | - | - | - | - | - |'
- en: '| HDNET [[152](#bib.bib152)] | L | 20.0 | 89.14 | 86.57 | 78.32 | - | - | -
    | - | - | - |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| HDNET [[152](#bib.bib152)] | L | 20.0 | 89.14 | 86.57 | 78.32 | - | - | -
    | - | - | - |'
- en: '| BirdNet [[153](#bib.bib153)] | L | 9.1 | 76.88 | 51.51 | 50.27 | 20.73 |
    15.80 | 14.59 | 36.01 | 23.78 | 21.09 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| BirdNet [[153](#bib.bib153)] | L | 9.1 | 76.88 | 51.51 | 50.27 | 20.73 |
    15.80 | 14.59 | 36.01 | 23.78 | 21.09 |'
- en: '| Discretization -based Methods | VeloFCN [[154](#bib.bib154)] | L | 1.0 |
    0.02 | 0.14 | 0.21 | - | - | - | - | - | - |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 离散化方法 | VeloFCN [[154](#bib.bib154)] | L | 1.0 | 0.02 | 0.14 | 0.21 | - |
    - | - | - | - | - |'
- en: '| 3D FCN [[155](#bib.bib155)] | L | <0.2 | 70.62 | 61.67 | 55.61 | - | - |
    - | - | - | - |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 3D FCN [[155](#bib.bib155)] | L | <0.2 | 70.62 | 61.67 | 55.61 | - | - |
    - | - | - | - |'
- en: '| Vote3Deep [[156](#bib.bib156)] | L | - | - | - | - | - | - | - | - | - |
    - |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| Vote3Deep [[156](#bib.bib156)] | L | - | - | - | - | - | - | - | - | - |
    - |'
- en: '| 3DBN [[157](#bib.bib157)] | L | 7.7 | 89.66 | 83.94 | 76.50 | - | - | - |
    - | - | - |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 3DBN [[157](#bib.bib157)] | L | 7.7 | 89.66 | 83.94 | 76.50 | - | - | - |
    - | - | - |'
- en: '| VoxelNet [[136](#bib.bib136)] | L | 2.0 | 89.35 | 79.26 | 77.39 | 46.13 |
    40.74 | 38.11 | 66.70 | 54.76 | 50.55 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| VoxelNet [[136](#bib.bib136)] | L | 2.0 | 89.35 | 79.26 | 77.39 | 46.13 |
    40.74 | 38.11 | 66.70 | 54.76 | 50.55 |'
- en: '| SECOND [[158](#bib.bib158)] | L | 26.3 | 89.39 | 83.77 | 78.59 | 55.99 |
    45.02 | 40.93 | 76.50 | 56.05 | 49.45 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| SECOND [[158](#bib.bib158)] | L | 26.3 | 89.39 | 83.77 | 78.59 | 55.99 |
    45.02 | 40.93 | 76.50 | 56.05 | 49.45 |'
- en: '| MVX-Net [[159](#bib.bib159)] | L & I | 16.7 | 92.13 | 86.05 | 78.68 | - |
    - | - | - | - | - |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| MVX-Net [[159](#bib.bib159)] | L & I | 16.7 | 92.13 | 86.05 | 78.68 | - |
    - | - | - | - | - |'
- en: '| PointPillars [[137](#bib.bib137)] | L | 62.0 | 90.07 | 86.56 | 82.81 | 57.60
    | 48.64 | 45.78 | 79.90 | 62.73 | 55.58 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| PointPillars [[137](#bib.bib137)] | L | 62.0 | 90.07 | 86.56 | 82.81 | 57.60
    | 48.64 | 45.78 | 79.90 | 62.73 | 55.58 |'
- en: '|  | SA-SSD [[160](#bib.bib160)] | L | 25.0 | 95.03 | 91.03 | 85.96 | - | -
    | - | - | - | - |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | SA-SSD [[160](#bib.bib160)] | L | 25.0 | 95.03 | 91.03 | 85.96 | - | -
    | - | - | - | - |'
- en: '|'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Point-based &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于点的方法 &#124;'
- en: '&#124; Methods &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '| 3DSSD [[161](#bib.bib161)] | L | 25.0 | 92.66 | 89.02 | 85.86 | 60.54 | 49.94
    | 45.73 | 85.04 | 67.62 | 61.14 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 3DSSD [[161](#bib.bib161)] | L | 25.0 | 92.66 | 89.02 | 85.86 | 60.54 | 49.94
    | 45.73 | 85.04 | 67.62 | 61.14 |'
- en: '|  | Other Methods | LaserNet [[162](#bib.bib162)] | L | 83.3 | 79.19 | 74.52
    | 68.45 | - | - | - | - | - | - |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | 其他方法 | LaserNet [[162](#bib.bib162)] | L | 83.3 | 79.19 | 74.52 | 68.45
    | - | - | - | - | - | - |'
- en: '|  | LaserNet++ [[163](#bib.bib163)] | L & I | 26.3 | - | - | - | - | - | -
    | - | - | - |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | LaserNet++ [[163](#bib.bib163)] | L & I | 26.3 | - | - | - | - | - | -
    | - | - | - |'
- en: '|  |  | OHS-Dense [[164](#bib.bib164)] | L | 33.3 | 93.73 | 88.11 | 84.98 |
    50.87 | 44.59 | 42.14 | 82.13 | 66.86 | 60.86 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  |  | OHS-Dense [[164](#bib.bib164)] | L | 33.3 | 93.73 | 88.11 | 84.98 |
    50.87 | 44.59 | 42.14 | 82.13 | 66.86 | 60.86 |'
- en: '|  |  | OHS-Direct [[164](#bib.bib164)] | L | 33.3 | 93.59 | 87.95 | 83.21
    | 55.90 | 49.48 | 45.79 | 79.66 | 67.20 | 61.04 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  |  | OHS-Direct [[164](#bib.bib164)] | L | 33.3 | 93.59 | 87.95 | 83.21
    | 55.90 | 49.48 | 45.79 | 79.66 | 67.20 | 61.04 |'
- en: '|  |  | Point-GNN [[125](#bib.bib125)] | L | 1.7 | 93.11 | 89.17 | 83.90 |
    55.36 | 47.07 | 44.61 | 81.17 | 67.28 | 59.67 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Point-GNN [[125](#bib.bib125)] | L | 1.7 | 93.11 | 89.17 | 83.90 |
    55.36 | 47.07 | 44.61 | 81.17 | 67.28 | 59.67 |'
- en: 4.2 3D Object Tracking
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 3D目标跟踪
- en: Given the locations of an object in the first frame, the task of object tracking
    is to estimate its state in subsequent frames [[167](#bib.bib167), [168](#bib.bib168)].
    Since 3D object tracking can use the rich geometric information in point clouds,
    it is expected to overcome several drawbacks faced by image-based tracking, including
    occlusion, illumination and scale variation.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 给定对象在第一帧的位置，目标跟踪的任务是估计其在后续帧中的状态 [[167](#bib.bib167), [168](#bib.bib168)]。由于3D目标跟踪可以利用点云中的丰富几何信息，预计可以克服图像跟踪所面临的一些缺点，包括遮挡、光照和尺度变化。
- en: Inspired by the success of Siamese network [[169](#bib.bib169)] for imaged-based
    object tracking, Giancola et al. [[170](#bib.bib170)] proposed a 3D Siamese network
    with shape completion regularization. Specifically, they first generated candidates
    using a Kalman filter, and encoded model and candidates into a compact representation
    using shape regularization. The cosine similarity is then used to search the location
    of the tracked object in the next frame. This method can be used as an alternative
    for object tracking, and significantly outperforms most 2D object tracking methods,
    including $\mathrm{STAPLE_{CA}}$ [[171](#bib.bib171)] and SiamFC [[169](#bib.bib169)].
    To efficiently search the target object, Zarzar et al. [[172](#bib.bib172)] leveraged
    a 2D Siamese network to generate a large number of coarse object candidates on
    BEV representation. They then refined the candidates by exploiting the cosine
    similarity in 3D Siamese network. This method significantly outperforms [[170](#bib.bib170)]
    in terms of both precision (i.e., by 18%) and success rate (i.e., by 12%). Simon
    et al. [[173](#bib.bib173)] proposed a 3D object detection and tracking architecture
    for semantic point clouds. They first generated voxelized semantic point clouds
    by fusing 2D visual semantic information, and then utilized the temporal information
    to improve accuracy and robustness of multi-target tracking. In addition, they
    introduced a powerful and simplified evaluation metric (i.e., Scale-Rotation-Translation
    score (SRFs)) to speed up training and inference. Complexer-YOLO achieves promising
    tracking performance and can still run in real-time. Further, Qi et al. [[174](#bib.bib174)]
    proposed a Point-to-Box (P2B) network. They fed template and search areas into
    the backbone to obtain their seeds. The search area seeds are augmented with target-specific
    features and then the potential target centers are regressed by Hough voting.
    Experimental results show that P2B outperforms [[170](#bib.bib170)] by over 10%
    while running at 40 fps.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 Siamese 网络 [[169](#bib.bib169)] 在基于图像的目标跟踪成功的启发，Giancola 等人 [[170](#bib.bib170)]
    提出了一个带有形状完成正则化的 3D Siamese 网络。具体而言，他们首先使用卡尔曼滤波器生成候选框，然后利用形状正则化将模型和候选框编码为紧凑表示。接着，使用余弦相似度在下一帧中搜索跟踪目标的位置。这种方法可以作为目标跟踪的替代方案，显著优于大多数
    2D 目标跟踪方法，包括 $\mathrm{STAPLE_{CA}}$ [[171](#bib.bib171)] 和 SiamFC [[169](#bib.bib169)]。为了有效搜索目标对象，Zarzar
    等人 [[172](#bib.bib172)] 利用 2D Siamese 网络在 BEV 表示上生成大量粗略的目标候选框。然后，他们通过利用 3D Siamese
    网络中的余弦相似度对候选框进行细化。这种方法在精度（即提高 18%）和成功率（即提高 12%）方面显著优于 [[170](#bib.bib170)]。Simon
    等人 [[173](#bib.bib173)] 提出了一个用于语义点云的 3D 目标检测和跟踪架构。他们首先通过融合 2D 视觉语义信息生成体素化的语义点云，然后利用时间信息来提高多目标跟踪的准确性和鲁棒性。此外，他们引入了一种强大且简化的评估指标（即缩放-旋转-平移评分（SRFs）），以加快训练和推理速度。Complexer-YOLO
    达到了令人满意的跟踪性能，并且仍能实时运行。此外，Qi 等人 [[174](#bib.bib174)] 提出了一个 Point-to-Box (P2B) 网络。他们将模板和搜索区域输入主干网络以获得其种子。搜索区域的种子通过目标特定特征进行增强，然后通过霍夫投票回归潜在目标中心。实验结果表明，P2B
    在运行速度为 40 fps 的情况下，相比于 [[170](#bib.bib170)] 提高了超过 10%。
- en: '![Refer to caption](img/ac2e3bde7d7347b9f397e7e3fae32a42.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ac2e3bde7d7347b9f397e7e3fae32a42.png)'
- en: 'Figure 9: A 3D scene flow between two KITTI point clouds, originally shown
    in [[175](#bib.bib175)]. Point clouds $\mathcal{X}$, $\mathcal{Y}$ and the translated
    point cloud of $\mathcal{X}$ are highlighted in red, green, and blue, respectively.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：两个 KITTI 点云之间的 3D 场景流，最初显示在 [[175](#bib.bib175)] 中。点云 $\mathcal{X}$、$\mathcal{Y}$
    及 $\mathcal{X}$ 的平移点云分别用红色、绿色和蓝色突出显示。
- en: 4.3 3D Scene Flow Estimation
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 3D 场景流估计
- en: 'Given two point clouds $\mathcal{X}$ and $\mathcal{Y}$, 3D scene flow $D=\{d_{i}\}^{N}$
    describes the movement of each point $x_{i}$ in $\mathcal{X}$ to its corresponding
    position $x_{i}^{\prime}$ in $\mathcal{Y}$, such that $x_{i}^{\prime}=x_{i}+d_{i}$.
    Figure [9](#S4.F9 "Figure 9 ‣ 4.2 3D Object Tracking ‣ 4 3D Object Detection and
    Tracking ‣ Deep Learning for 3D Point Clouds: A Survey") shows a 3D scene flow
    between two KITTI point clouds. Analogous to optical flow estimation in 2D vision,
    several methods have started to learn useful information (e.g. 3D scene flow,
    spatial-temporary information) from a sequence of point clouds.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个点云$\mathcal{X}$和$\mathcal{Y}$，3D场景流$D=\{d_{i}\}^{N}$描述了每个点$x_{i}$在$\mathcal{X}$中的移动到$\mathcal{Y}$中的对应位置$x_{i}^{\prime}$，其中$x_{i}^{\prime}=x_{i}+d_{i}$。图
    [9](#S4.F9 "图 9 ‣ 4.2 3D物体跟踪 ‣ 4 3D物体检测与跟踪 ‣ 深度学习用于3D点云：综述") 显示了两个KITTI点云之间的3D场景流。类似于2D视觉中的光流估计，一些方法已经开始从点云序列中学习有用的信息（例如3D场景流、时空信息）。
- en: Liu et al. [[175](#bib.bib175)] proposed FlowNet3D to directly learn scene flows
    from a pair of consecutive point clouds. FlowNet3D learns both point-level features
    and motion features through a flow embedding layer. However, there are two problems
    with FlowNet3D. First, some predicted motion vectors differ significantly from
    the ground truth in their directions. Second, it is difficult to apply FlowNet
    to non-static scenes, especially for the scenes which are dominated by deformable
    objects. To solve this problem, Wang et al. [[176](#bib.bib176)] introduced a
    cosine distance loss to minimize the angle between the predictions and the ground
    truth. In addition, they also proposed a point-to-plane distance loss to improve
    the accuracy for both rigid and dynamic scenes. Experimental results show that
    these two loss terms improve the accuracy of FlowNet3D from 57.85% to 63.43%,
    and speed up and stabilize the training process. Gu et al. [[177](#bib.bib177)]
    proposed a Hierarchical Permutohedral Lattice FlowNet (HPLFlowNet) to directly
    estimate scene flow from large-scale point clouds. Several bilateral convolution
    layers are proposed to restore structural information from raw point clouds, while
    reducing the computational cost.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人[[175](#bib.bib175)]提出了FlowNet3D来直接从一对连续的点云中学习场景流。FlowNet3D通过流嵌入层学习点级特征和运动特征。然而，FlowNet3D存在两个问题。首先，一些预测的运动向量在方向上与真实值有显著差异。其次，将FlowNet应用于非静态场景（尤其是由可变形物体主导的场景）是困难的。为了解决这个问题，王等人[[176](#bib.bib176)]引入了余弦距离损失来最小化预测和真实值之间的角度。此外，他们还提出了一种点到平面的距离损失，以提高刚性和动态场景的准确性。实验结果表明，这两个损失项将FlowNet3D的准确性从57.85%提高到63.43%，并加速和稳定了训练过程。顾等人[[177](#bib.bib177)]提出了一种层次化的Permutohedral
    Lattice FlowNet（HPLFlowNet）来直接从大规模点云中估计场景流。提出了几种双边卷积层，以从原始点云中恢复结构信息，同时降低计算成本。
- en: To effectively process sequential point clouds, Fan and Yang [[178](#bib.bib178)]
    proposed PointRNN, PointGRU and PointLSTM networks and a sequence-to-sequence
    model to track moving points. PointRNN, PointGRU, and PointLSTM are able to capture
    the spatial-temporary information and model dynamic point clouds. Similarly, Liu
    et al. [[179](#bib.bib179)] proposed MeteorNet to directly learn a representation
    from dynamic point clouds. This method learns to aggregate information from spatiotemporal
    neighboring points. Direct grouping and chained-flow grouping are further introduced
    to determine the temporal neighbors. However, the performance of the aforementioned
    methods is limited by the scale of datasets. Mittal et al. [[180](#bib.bib180)]
    proposed two self-supervised losses to train their network on large unlabeled
    datasets. Their main idea is that a robust scene flow estimation method should
    be effective in both forward and backward predictions. Due to the unavailability
    of scene flow annotation, the nearest neighbor of the predicted transformed point
    is considered as pesudo ground truth. However, the true ground truth may not be
    the same as the nearest point. To avoid this problem, they computed the scene
    flow in the reverse direction and proposed a cycle consistency loss to translate
    the point to the original position. Experimental results show that this self-supervised
    method exceeds the state-of-the-art performance of supervised learning-based methods.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效处理序列点云，Fan 和 Yang [[178](#bib.bib178)] 提出了 PointRNN、PointGRU 和 PointLSTM
    网络及一个序列到序列模型来跟踪移动点。PointRNN、PointGRU 和 PointLSTM 能够捕捉空间-时间信息并建模动态点云。类似地，Liu 等人
    [[179](#bib.bib179)] 提出了 MeteorNet 以直接从动态点云中学习表示。这种方法学习从时空邻域点中聚合信息。进一步引入了直接分组和链式流分组来确定时间邻域。然而，前述方法的性能受限于数据集的规模。Mittal
    等人 [[180](#bib.bib180)] 提出了两种自监督损失来在大型未标记数据集上训练他们的网络。他们的主要思想是，一个鲁棒的场景流估计方法在前向和后向预测中都应有效。由于场景流注释不可用，预测变换点的最近邻被视为伪地面真实值。然而，真实的地面真实值可能与最近的点不同。为避免此问题，他们计算了反向的场景流并提出了一个循环一致性损失，将点转换到原始位置。实验结果表明，这种自监督方法超越了基于监督学习方法的最新性能。
- en: 4.4 Summary
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 摘要
- en: 'The KITTI [[14](#bib.bib14)] benchmark is one of the most influential datasets
    in autonomous driving and has been commonly used in both academia and industry.
    Tables [III](#S4.T3 "TABLE III ‣ 4.1.1 Region Proposal-based Methods ‣ 4.1 3D
    Object Detection ‣ 4 3D Object Detection and Tracking ‣ Deep Learning for 3D Point
    Clouds: A Survey") and [IV](#S4.T4 "TABLE IV ‣ 4.1.2 Single Shot Methods ‣ 4.1
    3D Object Detection ‣ 4 3D Object Detection and Tracking ‣ Deep Learning for 3D
    Point Clouds: A Survey") present the results achieved by different detectors on
    the KITTI test 3D benchmarks. The following observations can be made:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 'KITTI [[14](#bib.bib14)] 基准是自动驾驶领域最具影响力的数据集之一，并在学术界和工业界广泛使用。表 [III](#S4.T3
    "TABLE III ‣ 4.1.1 Region Proposal-based Methods ‣ 4.1 3D Object Detection ‣ 4
    3D Object Detection and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey")
    和 [IV](#S4.T4 "TABLE IV ‣ 4.1.2 Single Shot Methods ‣ 4.1 3D Object Detection
    ‣ 4 3D Object Detection and Tracking ‣ Deep Learning for 3D Point Clouds: A Survey")
    展示了不同检测器在 KITTI 测试 3D 基准上的结果。可以做出以下观察：'
- en: $\bullet$
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Region proposal-based methods are the most frequently investigated methods among
    these two categories, and outperform single shot methods by a large margin on
    both KITTI test 3D and BEV benchmarks.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 区域提议方法是这两类方法中研究最频繁的，并且在 KITTI 测试 3D 和 BEV 基准上大幅超越单次检测方法。
- en: $\bullet$
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: There are two limitations for existing 3D object detectors. First, the long-range
    detection capability of existing methods is relatively poor. Second, how to fully
    exploit the texture information in images is still an open problem.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有的 3D 物体检测器存在两个限制。首先，现有方法的远程检测能力相对较差。其次，如何充分利用图像中的纹理信息仍然是一个未解决的问题。
- en: $\bullet$
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Multi-task learning is a future direction in 3D object detection. E.g., MMF
    [[128](#bib.bib128)] learns a cross-modality representation to achieve state-of-the-art
    detection performance by incorporating multiple tasks.
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多任务学习是 3D 物体检测的未来方向。例如，MMF [[128](#bib.bib128)] 通过结合多个任务来学习跨模态表示，从而实现了最先进的检测性能。
- en: $\bullet$
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 3D object tracking and scene flow estimation are emerging research topics, and
    have gradually attracted increasing attention since 2019.
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3D 物体跟踪和场景流估计是新兴的研究主题，自 2019 年以来逐渐受到越来越多的关注。
- en: 5 3D Point Cloud Segmentation
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 3D 点云分割
- en: '3D point cloud segmentation requires the understanding of both the global geometric
    structure and the fine-grained details of each point. According to the segmentation
    granularity, 3D point cloud segmentation methods can be classified into three
    categories: semantic segmentation (scene level), instance segmentation (object
    level) and part segmentation (part level).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 点云分割需要理解每个点的全球几何结构和细粒度细节。根据分割的粒度，3D 点云分割方法可以分为三类：语义分割（场景级别）、实例分割（对象级别）和部件分割（部件级别）。
- en: '![Refer to caption](img/9cf6e46e67173fc352cef3647da95224.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9cf6e46e67173fc352cef3647da95224.png)'
- en: 'Figure 10: Chronological overview of the most relevant deep learning-based
    3D semantic segmentation methods.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：基于深度学习的最相关 3D 语义分割方法的时间概述。
- en: 5.1 3D Semantic Segmentation
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 3D 语义分割
- en: 'Given a point cloud, the goal of semantic segmentation is to separate it into
    several subsets according to the semantic meanings of points. Similar to the taxonomy
    for 3D shape classification (Section [3](#S3 "3 3D Shape Classification ‣ Deep
    Learning for 3D Point Clouds: A Survey")), there are four paradigms for semantic
    segmentation: projection-based, discretization-based, point-based, and hybrid
    methods.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '给定一个点云，语义分割的目标是根据点的语义意义将其分为若干子集。类似于 3D 形状分类的分类法（第 [3](#S3 "3 3D Shape Classification
    ‣ Deep Learning for 3D Point Clouds: A Survey") 节），语义分割有四种范式：投影基、离散化基、点基和混合方法。'
- en: 'The first step of both the projection and discretization-based methods is to
    transform a point cloud to an intermediate regular representation, such as multi-view
    [[181](#bib.bib181), [182](#bib.bib182)], spherical [[183](#bib.bib183), [184](#bib.bib184),
    [185](#bib.bib185)], volumetric [[186](#bib.bib186), [187](#bib.bib187), [166](#bib.bib166)],
    permutohedral lattice [[188](#bib.bib188), [189](#bib.bib189)], and hybrid representations
    [[190](#bib.bib190), [191](#bib.bib191)], as shown in Fig. [11](#S5.F11 "Figure
    11 ‣ 5.1.1 Projection-based Methods ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point
    Cloud Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey"). The intermediate
    segmentation results are then projected back to the raw point cloud. In contrast,
    point-based methods directly work on irregular point clouds. Several representative
    methods are shown in Fig. [10](#S5.F10 "Figure 10 ‣ 5 3D Point Cloud Segmentation
    ‣ Deep Learning for 3D Point Clouds: A Survey").'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '投影和离散化方法的第一步是将点云转换为中间的规则表示形式，如多视图 [[181](#bib.bib181), [182](#bib.bib182)]、球面
    [[183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185)]、体积 [[186](#bib.bib186),
    [187](#bib.bib187), [166](#bib.bib166)]、置换对称格 [[188](#bib.bib188), [189](#bib.bib189)]
    和混合表示 [[190](#bib.bib190), [191](#bib.bib191)]，如图 [11](#S5.F11 "Figure 11 ‣ 5.1.1
    Projection-based Methods ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point Cloud Segmentation
    ‣ Deep Learning for 3D Point Clouds: A Survey") 所示。中间分割结果然后被投影回原始点云。相比之下，点基方法直接作用于不规则点云。几种具有代表性的方法如图
    [10](#S5.F10 "Figure 10 ‣ 5 3D Point Cloud Segmentation ‣ Deep Learning for 3D
    Point Clouds: A Survey") 所示。'
- en: 5.1.1 Projection-based Methods
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 投影基方法
- en: These methods usually project a 3D point cloud into 2D images, including multi-view
    and spherical images.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法通常将 3D 点云投影到 2D 图像中，包括多视图和球面图像。
- en: Multi-view Representation. Lawin et al. [[181](#bib.bib181)] first projected
    a 3D point cloud onto 2D planes from multiple virtual camera views. Then, a multi-stream
    FCN is used to predict pixel-wise scores on synthetic images. The final semantic
    label of each point is obtained by fusing the re-projected scores over different
    views. Similarly, Boulch et al. [[182](#bib.bib182)] first generated several RGB
    and depth snapshots of a point cloud using multiple camera positions. They then
    performed pixel-wise labeling on these snapshots using 2D segmentation networks.
    The scores predicted from RGB and depth images are further fused using residual
    correction [[192](#bib.bib192)]. Based on the assumption that point clouds are
    sampled from locally Euclidean surfaces, Tatarchenko et al. [[193](#bib.bib193)]
    introduced tangent convolutions for dense point cloud segmentation. This method
    first projects the local surface geometry around each point to a virtual tangent
    plane. Tangent convolutions are then directly operated on the surface geometry.
    This method shows great scalability and is able to process large-scale point clouds
    with millions of points. Overall, the performance of multi-view segmentation methods
    is sensitive to viewpoint selection and occlusions. Besides, these methods have
    not fully exploited the underlying geometric and structural information, as the
    projection step inevitably introduces information loss.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角表示。Lawin 等人 [[181](#bib.bib181)] 首先将 3D 点云投影到多个虚拟摄像机视角下的 2D 平面上。然后，使用多流 FCN
    预测合成图像上的像素级评分。每个点的最终语义标签通过融合不同视角的重投影评分获得。类似地，Boulch 等人 [[182](#bib.bib182)] 首先使用多个摄像机位置生成点云的多个
    RGB 和深度快照。随后，他们在这些快照上使用 2D 分割网络进行像素级标注。来自 RGB 和深度图像的预测评分通过残差校正进一步融合 [[192](#bib.bib192)]。基于点云从局部欧几里得表面采样的假设，Tatarchenko
    等人 [[193](#bib.bib193)] 引入了切线卷积用于密集点云分割。该方法首先将每个点周围的局部表面几何投影到虚拟切线平面上。然后，直接在表面几何上进行切线卷积操作。该方法显示出极大的可扩展性，能够处理具有数百万点的大规模点云。总体而言，多视角分割方法对视点选择和遮挡非常敏感。此外，由于投影步骤不可避免地引入信息丢失，这些方法尚未充分利用潜在的几何和结构信息。
- en: '![Refer to caption](img/e91c45c9e983d9ffa4a7a98bf9e0fa95.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e91c45c9e983d9ffa4a7a98bf9e0fa95.png)'
- en: 'Figure 11: An illustration of the intermediate representation. (a) and (b)
    are originally shown in [[182](#bib.bib182)] and [[183](#bib.bib183)], respectively.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：中间表示的示意图。(a) 和 (b) 分别最初展示在 [[182](#bib.bib182)] 和 [[183](#bib.bib183)]
    中。
- en: Spherical Representation. To achieve fast and accurate segmentation of 3D point
    clouds, Wu et al. [[183](#bib.bib183)] proposed an end-to-end network based on
    SqueezeNet [[194](#bib.bib194)] and Conditional Random Field (CRF). To further
    improve segmentation accuracy, SqueezeSegV2 [[184](#bib.bib184)] is introduced
    to address domain shift by utilizing an unsupervised domain adaptation pipeline.
    Milioto et al. [[185](#bib.bib185)] proposed RangeNet++ for real-time semantic
    segmentation of LiDAR point clouds. The semantic labels of 2D range images are
    first transferred to 3D point clouds, an efficient GPU-enabled KNN-based post-processing
    step is further used to alleviate the problem of discretization errors and blurry
    inference outputs. Compared to single view projection, spherical projection retains
    more information and is suitable for the labeling of LiDAR point clouds. However,
    this intermediate representation inevitably brings several problems such as discretization
    errors and occlusions.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 球面表示。为了实现 3D 点云的快速和准确分割，Wu 等人 [[183](#bib.bib183)] 提出了基于 SqueezeNet [[194](#bib.bib194)]
    和条件随机场（CRF）的端到端网络。为了进一步提高分割准确性，引入了 SqueezeSegV2 [[184](#bib.bib184)] 通过利用无监督领域适应管道来解决领域迁移问题。Milioto
    等人 [[185](#bib.bib185)] 提出了 RangeNet++ 用于 LiDAR 点云的实时语义分割。首先将 2D 范围图像的语义标签转移到
    3D 点云中，然后使用高效的 GPU 加速 KNN 基后处理步骤来缓解离散化误差和模糊推断输出的问题。与单视角投影相比，球面投影保留了更多的信息，并适用于
    LiDAR 点云的标注。然而，这种中间表示不可避免地带来了离散化误差和遮挡等几个问题。
- en: 5.1.2 Discretization-based Methods
  id: totrans-373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 基于离散化的方法
- en: These methods usually convert a point cloud into a dense/sparse discrete representation,
    such as volumetric and sparse permutohedral lattices.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法通常将点云转换为密集/稀疏离散表示，例如体积表示和稀疏置换晶格。
- en: Dense Discretization Representation. Early methods usually voxelized the point
    clouds as dense grids and then leverage the standard 3D convolutions. Huang et
    al. [[195](#bib.bib195)] first divided a point cloud into a set of occupancy voxels,
    then fed these intermediate data to a fully-3D CNN for voxel-wise segmentation.
    Finally, all points within a voxel are assigned the same semantic label as the
    voxel. The performance of this method is severely limited by the granularity of
    the voxels and the boundary artifacts caused by the point cloud partition. Further,
    Tchapmi et al. [[196](#bib.bib196)] proposed SEGCloud to achieve fine-grained
    and global consistent semantic segmentation. This method introduces a deterministic
    trilinear interpolation to map the coarse voxel predictions generated by 3D-FCNN
    [[197](#bib.bib197)] back to the point cloud, and then uses Fully Connected CRF
    (FC-CRF) to enforce spatial consistency of these inferred per-point labels. Meng
    et al. [[186](#bib.bib186)] introduced a kernel-based interpolated variational
    autoencoder architecture to encode the local geometrical structures within each
    voxel. Instead of a binary occupancy representation, RBFs are employed for each
    voxel to obtain a continuous representation and capture the distribution of points
    in each voxel. VAE is further used to map the point distribution within each voxel
    to a compact latent space. Then, both symmetry groups and an equivalence CNN are
    used to achieve robust feature learning.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 密集离散化表示。早期方法通常将点云体素化为密集网格，然后利用标准3D卷积。Huang等人[[195](#bib.bib195)]首先将点云分为一组占用体素，然后将这些中间数据输入到全3D
    CNN进行体素级分割。最后，将体素内的所有点分配相同的语义标签。该方法的性能受到体素粒度和点云分割导致的边界伪影的严重限制。此外，Tchapmi等人[[196](#bib.bib196)]提出了SEGCloud，以实现细粒度和全局一致的语义分割。该方法引入了一种确定性三线性插值，将3D-FCNN
    [[197](#bib.bib197)]生成的粗体素预测映射回点云，然后使用全连接CRF（FC-CRF）来强制执行这些推断的每点标签的空间一致性。Meng等人[[186](#bib.bib186)]引入了一种基于核的插值变分自编码器架构，以编码每个体素内的局部几何结构。RBF被用于每个体素，以获得连续表示并捕捉每个体素中的点分布。进一步使用VAE将每个体素内的点分布映射到紧凑的潜在空间中。然后，利用对称群和等效CNN来实现鲁棒的特征学习。
- en: Thanks to the good scalability of 3D CNN, volumetric-based networks are free
    to be trained and tested on point clouds with different spatial sizes. In Fully-Convolutional
    Point Network (FCPN) [[187](#bib.bib187)], different levels of geometric relations
    are first hierarchically abstracted from point clouds, 3D convolutions and weighted
    average pooling are then used to extract features and incorporate long-range dependencies.
    This method can process large-scale point clouds and has good scalability during
    inference. Dai et al. [[198](#bib.bib198)] proposed ScanComplete to achieve 3D
    scan completion and per-voxel semantic labeling. This method leverages the scalability
    of fully-convolutional neural networks and can adapt to different input data sizes
    during training and test. A coarse-to-fine strategy is used to hierarchically
    improve the resolution of the predicted results.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 由于3D CNN的良好可扩展性，基于体积的网络可以在具有不同空间大小的点云上进行训练和测试。在全卷积点网络（FCPN）[[187](#bib.bib187)]中，不同层次的几何关系首先从点云中分层抽象出来，然后使用3D卷积和加权平均池化来提取特征并结合远程依赖。这种方法可以处理大规模点云，并且在推理过程中具有良好的可扩展性。Dai等人[[198](#bib.bib198)]提出了ScanComplete，以实现3D扫描补全和每体素语义标注。该方法利用了全卷积神经网络的可扩展性，并能在训练和测试过程中适应不同的输入数据大小。使用了粗到细的策略来分层提高预测结果的分辨率。
- en: Overall, the volumetric representation naturally preserves the neighborhood
    structure of 3D point clouds. Its regular data format also allows direct application
    of standard 3D convolutions. These factors lead to a steady performance improvement
    in this area. However, the voxelization step inherently introduces discretization
    artifacts and information loss. Usually, a high resolution leads to high memory
    and computational costs, while a low resolution introduces loss of details. It
    is non-trivial to select an appropriate grid resolution in practice.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，体积表示自然地保留了3D点云的邻域结构。其规则的数据格式也允许直接应用标准3D卷积。这些因素导致了该领域性能的稳定提升。然而，体素化步骤固有地引入了离散化伪影和信息损失。通常，高分辨率会导致高内存和计算成本，而低分辨率则会导致细节丢失。在实践中选择合适的网格分辨率并非易事。
- en: Sparse Discretization Representation. Volumetric representation is naturally
    sparse, as the number of non-zero values only accounts for a small percentage.
    Therefore, it is inefficient to apply dense convolution neural networks on the
    spatially-sparse data. To this end, Graham et al. [[166](#bib.bib166)] proposed
    submanifold sparse convolutional networks based on the indexing structure. This
    method significantly reduces memory and computational costs by restricting the
    output of convolution to be only related to occupied voxels. Meanwhile, its sparse
    convolution can also control the sparsity of the extracted features. This submanifold
    sparse convolution is suitable for efficient processing of high-dimensional and
    spatially-sparse data. Further, Choy et al. [[199](#bib.bib199)] proposed a 4D
    spatio-temporal convolutional neural network called MinkowskiNet for 3D video
    perception. A generalized sparse convolution is proposed to effectively process
    high-dimensional data. A trilateral-stationary conditional random field is further
    applied to enforce consistency.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏离散化表示。体积表示本质上是稀疏的，因为非零值的数量只占很小的百分比。因此，在空间稀疏数据上应用密集卷积神经网络效率低下。为此，Graham等人[[166](#bib.bib166)]
    提出了基于索引结构的子流形稀疏卷积网络。该方法通过限制卷积输出仅与占用体素相关，显著减少了内存和计算成本。同时，其稀疏卷积还可以控制提取特征的稀疏性。这种子流形稀疏卷积适用于高维和空间稀疏数据的高效处理。此外，Choy等人[[199](#bib.bib199)]
    提出了一个名为MinkowskiNet的4D时空卷积神经网络，用于3D视频感知。提出了一种广义稀疏卷积，以有效处理高维数据。进一步地，应用了三边形-平稳条件随机场以强制一致性。
- en: On the other hand, Su et al. [[188](#bib.bib188)] proposed the Sparse Lattice
    Networks (SPLATNet) based on Bilateral Convolution Layers (BCLs). This method
    first interpolates a raw point cloud to a permutohedral sparse lattice, BCL is
    then applied to convolve on occupied parts of the sparsely populated lattice.
    The filtered output is then interpolated back to the raw point cloud. In addition,
    this method allows flexible joint processing of multi-view images and point clouds.
    Further, Rosu et al. [[189](#bib.bib189)] proposed LatticeNet to achieve efficient
    processing of large point clouds. A data-dependent interpolation module called
    DeformsSlice is also introduced to back project the lattice feature to point clouds.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，苏等人[[188](#bib.bib188)] 提出了基于双边卷积层（BCLs）的稀疏晶格网络（SPLATNet）。该方法首先将原始点云插值到一个置换稀疏晶格，然后在稀疏晶格的占用部分应用BCL进行卷积。滤波后的输出再插值回原始点云。此外，该方法还允许灵活地联合处理多视角图像和点云。进一步地，Rosu等人[[189](#bib.bib189)]
    提出了LatticeNet，以实现对大规模点云的高效处理。还引入了一个数据依赖的插值模块，称为DeformsSlice，用于将晶格特征回投影到点云中。
- en: 5.1.3 Hybrid Methods
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 混合方法
- en: To further leverage all available information, several methods have been proposed
    to learn multi-modal features from 3D scans. Dai and Nießner [[190](#bib.bib190)]
    presented a joint 3D-multi-view network to combine RGB features and geometric
    features. A 3D CNN stream and several 2D streams are used to extract features,
    and a differentiable back-projection layer is proposed to jointly fuse the learned
    2D embeddings and 3D geometric features. Further, Chiang et al. [[200](#bib.bib200)]
    proposed a unified point-based framework to learn 2D textural appearance, 3D structures
    and global context features from point clouds. This method directly applies point-based
    networks to extracts local geometric features and global context from sparsely
    sampled point sets without any voxelization. Jaritz et al. [[191](#bib.bib191)]
    proposed Multi-view PointNet (MVPNet) to aggregate appearance features from 2D
    multi-view images and spatial geometric features in the canonical point cloud
    space.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步利用所有可用信息，已经提出了几种方法来从3D扫描中学习多模态特征。Dai和Nießner[[190](#bib.bib190)] 提出了一个联合3D-多视角网络，以结合RGB特征和几何特征。使用3D
    CNN流和几个2D流来提取特征，并提出了一种可微分的回投影层，用于共同融合学习到的2D嵌入和3D几何特征。此外，Chiang等人[[200](#bib.bib200)]
    提出了一个统一的基于点的框架，从点云中学习2D纹理外观、3D结构和全局上下文特征。该方法直接应用基于点的网络，从稀疏采样点集中提取局部几何特征和全局上下文，而无需任何体素化。Jaritz等人[[191](#bib.bib191)]
    提出了Multi-view PointNet (MVPNet)，用于聚合来自2D多视角图像的外观特征和规范点云空间中的空间几何特征。
- en: 5.1.4 Point-based Methods
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 基于点的方法
- en: Point-based networks directly work on irregular point clouds. However, point
    clouds are orderless and unstructured, making it infeasible to directly apply
    standard CNNs. To this end, the pioneering work PointNet [[5](#bib.bib5)] is proposed
    to learn per-point features using shared MLPs and global features using symmetrical
    pooling functions. Based on PointNet, a series of point-based networks have been
    proposed recently. Overall, these methods can be roughly divided into pointwise
    MLP methods, point convolution methods, RNN-based methods, and graph-based methods.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 基于点的网络直接处理不规则的点云。然而，点云是无序且无结构的，使得直接应用标准CNN变得不可行。为此，开创性工作PointNet [[5](#bib.bib5)]
    被提出，使用共享的MLP学习每个点的特征，并使用对称池化函数学习全局特征。基于PointNet，最近提出了一系列基于点的网络。总体而言，这些方法大致可以分为点wise
    MLP方法、点卷积方法、基于RNN的方法和基于图的方法。
- en: Pointwise MLP Methods. These methods usually use shared MLP as the basic unit
    in their network for its high efficiency. However, point-wise features extracted
    by shared MLP cannot capture the local geometry in point clouds and the mutual
    interactions between points [[5](#bib.bib5)]. To capture wider context for each
    point and learn richer local structures, several dedicated networks have been
    introduced, including methods based on neighboring feature pooling, attention-based
    aggregation, and local-global feature concatenation.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 点wise MLP 方法。这些方法通常使用共享MLP作为网络中的基本单元，因为它具有高效率。然而，共享MLP提取的点wise特征无法捕捉点云中的局部几何和点之间的相互作用
    [[5](#bib.bib5)]。为了捕捉每个点的更广泛上下文并学习更丰富的局部结构，引入了几种专门的网络，包括基于邻域特征池化、基于注意力的聚合和局部-全局特征连接的方法。
- en: '![Refer to caption](img/3c6226c387926728246ebc67871001b2.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3c6226c387926728246ebc67871001b2.png)'
- en: 'Figure 12: An illustration of point-based methods. (a)-(d) are originally shown
    in [[54](#bib.bib54), [201](#bib.bib201), [202](#bib.bib202), [203](#bib.bib203)],
    respectively.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 基于点的方法的示意图。 (a)-(d) 原始展示于 [[54](#bib.bib54), [201](#bib.bib201), [202](#bib.bib202),
    [203](#bib.bib203)]。'
- en: 'Neighboring feature pooling: To capture local geometric patterns, these methods
    learn a feature for each point by aggregating the information from local neighboring
    points. In particular, PointNet++ [[54](#bib.bib54)] groups points hierarchically
    and progressively learns from larger local regions, as illustrated in Fig. [12](#S5.F12
    "Figure 12 ‣ 5.1.4 Point-based Methods ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point
    Cloud Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")(a). Multi-scale
    grouping and multi-resolution grouping are also proposed to overcome the problems
    caused by non-uniformity and varying density of point clouds. Later, Jiang et
    al. [[141](#bib.bib141)] proposed a PointSIFT module to achieve orientation encoding
    and scale awareness. This module stacks and encodes the information from eight
    spatial orientations through a three-stage ordered convolution. Multi-scale features
    are concatenated to achieve adaptivity to different scales. Different from the
    grouping techniques used in PointNet++ (i.e., ball query), Engelmann et al. [[204](#bib.bib204)]
    utilized $K$-means clustering and KNN to separately define two neighborhoods in
    the world space and feature space. Based on the assumption that points from the
    same class are expected to be closer in feature space, a pairwise distance loss
    and a centroid loss are introduced to further regularize feature learning. To
    model the mutual interactions between different points, Zhao et al. [[57](#bib.bib57)]
    proposed PointWeb to explore the relations between all pairs of points in a local
    region by densely constructing a locally fully-linked web. An Adaptive Feature
    Adjustment (AFA) module is proposed to achieve information interchange and feature
    refinement. This aggregation operation helps the network to learn a discriminative
    feature representation. Zhang et al. [[205](#bib.bib205)] proposed a permutation
    invariant convolution called Shellconv based on the statistics from concentric
    spherical shells. This method first queries a set of multi-scale concentric spheres,
    the max-pooling operation is then used within different shells to summarize the
    statistics, MLPs and 1D convolution are used to obtain the final convolution output.
    Hu et al. [[206](#bib.bib206)] proposed an efficient and lightweight network called
    RandLA-Net for large-scale point cloud segmentation. This network utilizes random
    point sampling to achieve remarkably high efficiency in terms of memory and computation.
    A local feature aggregation module is further proposed to capture and preserve
    geometric features.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '邻域特征池化：为了捕捉局部几何模式，这些方法通过汇总来自局部邻域点的信息来为每个点学习特征。特别地，PointNet++ [[54](#bib.bib54)]
    以分层方式对点进行分组，并逐步从更大的局部区域学习，如图 [12](#S5.F12 "Figure 12 ‣ 5.1.4 Point-based Methods
    ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point Cloud Segmentation ‣ Deep Learning
    for 3D Point Clouds: A Survey")(a) 所示。为了克服点云非均匀性和密度变化造成的问题，还提出了多尺度分组和多分辨率分组。后来，姜等人
    [[141](#bib.bib141)] 提出了一个 PointSIFT 模块来实现方向编码和尺度感知。该模块通过三阶段有序卷积堆叠和编码来自八个空间方向的信息。多尺度特征被串联以适应不同的尺度。与
    PointNet++ 中使用的分组技术（即球体查询）不同，Engelmann 等人 [[204](#bib.bib204)] 利用 $K$-均值聚类和 KNN
    在世界空间和特征空间中分别定义了两个邻域。基于同一类别的点在特征空间中应更靠近的假设，引入了成对距离损失和中心点损失来进一步规范特征学习。为了建模不同点之间的相互作用，赵等人
    [[57](#bib.bib57)] 提出了 PointWeb，通过密集构建局部完全连接的网络来探索局部区域内所有点对之间的关系。提出了一种自适应特征调整（AFA）模块来实现信息交换和特征细化。这种聚合操作帮助网络学习具有辨别力的特征表示。张等人
    [[205](#bib.bib205)] 提出了一个基于同心球壳统计的置换不变卷积，称为 Shellconv。该方法首先查询一组多尺度同心球，然后在不同的球壳内使用最大池化操作来总结统计信息，MLP
    和 1D 卷积用于获得最终的卷积输出。胡等人 [[206](#bib.bib206)] 提出了一个高效且轻量级的网络，称为 RandLA-Net，用于大规模点云分割。该网络利用随机点采样在内存和计算方面实现了显著的高效性。进一步提出了一个局部特征聚合模块来捕捉和保留几何特征。'
- en: 'Attention-based aggregation: To further improve segmentation accuracy, an attention
    mechanism [[120](#bib.bib120)] is introduced to point cloud segmentation. Yang
    et al. [[56](#bib.bib56)] proposed a group shuffle attention to model the relations
    between points, and presented a permutation-invariant, task-agnostic and differentiable
    Gumbel Subset Sampling (GSS) to replace the widely used FPS approach. This module
    is less sensitive to outliers and can select a representative subset of points.
    To better capture the spatial distribution of a point cloud, Chen et al. [[207](#bib.bib207)]
    proposed a Local Spatial Aware (LSA) layer to learn spatial awareness weights
    based on the spatial layouts and the local structures of point clouds. Similar
    to CRF, Zhao et al. [[208](#bib.bib208)] proposed an Attention-based Score Refinement
    (ASR) module to post-process the segmentation results produced by the network.
    The initial segmentation result is refined by pooling the scores of neighboring
    points with learned attention weights. This module can be easily integrated into
    existing deep networks to improve segmentation performance.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'Local-global concatenation: Zhao et al. [[112](#bib.bib112)] proposed a permutation-invariant
    PS²-Net to incorporate local structures and global context from point clouds.
    Edgeconv [[87](#bib.bib87)] and NetVLAD [[209](#bib.bib209)] are repeatedly stacked
    to capture the local information and scene-level global features.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Comparative semantic segmentation results on the S3DIS (including
    both Area5 and 6-fold cross validation) [[10](#bib.bib10)], Semantic3D (including
    both semantic-8 and reduced-8 subsets) [[12](#bib.bib12)] , ScanNet [[11](#bib.bib11)],
    and SemanticKITTI [[15](#bib.bib15)] datasets. Overall Accuracy (OA), Mean Intersection
    over Union (mIoU) are the main evaluation metric. For simplicity, we omit the
    ‘%’ after the value. The symbol ‘-’ means the results are unavailable.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | S3DIS | Semantic3D | ScanNet(v2) | Sem. KITTI (mIoU) |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Area5 &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (OA) &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Area5 &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (mIoU) &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 6-fold &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (mIoU) &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 6-fold &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (mIoU) &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; sem. &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (OA) &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; sem. &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (mIoU) &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; red. &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (OA) &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; red. &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (mIoU) &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '| OA | mIoU |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| Projection -based Methods | Multi-view | DeePr3SS [[181](#bib.bib181)] |
    - | - | - | - | - | - | 88.9 | 58.5 | - | - | - |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| SnapNet [[182](#bib.bib182)] | - | - | - | - | 91.0 | 67.4 | 88.6 | 59.1
    | - | - | - |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| TangentConv [[193](#bib.bib193)] | 82.5 | 52.8 | - | - | - | - | - | - |
    80.1 | 40.9 | 40.9 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| Spherical | SqueezeSeg [[183](#bib.bib183)] | - | - | - | - | - | - | - |
    - | - | - | 29.5 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '| SqueezeSegV2 [[184](#bib.bib184)] | - | - | - | - | - | - | - | - | - | -
    | 39.7 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '| RangeNet++ [[185](#bib.bib185)] | - | - | - | - | - | - | - | - | - | - |
    52.2 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| Discretization -based Methods | Volumetric | SEGCloud [[196](#bib.bib196)]
    | - | 48.9 | - | - | - | - | 88.1 | 61.3 | - | - | - |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| Discretization -based Methods | Volumetric | SEGCloud [[196](#bib.bib196)]
    | - | 48.9 | - | - | - | - | 88.1 | 61.3 | - | - | - |'
- en: '| SparseConvNet [[166](#bib.bib166)] | - | - | - | - | - | - | - | - | - |
    72.5 | - |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| SparseConvNet [[166](#bib.bib166)] | - | - | - | - | - | - | - | - | - |
    72.5 | - |'
- en: '| MinkowskiNet [[199](#bib.bib199)] | - | - | - | - | - | - | - | - | - | 73.6
    | - |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| MinkowskiNet [[199](#bib.bib199)] | - | - | - | - | - | - | - | - | - | 73.6
    | - |'
- en: '| VV-Net [[186](#bib.bib186)] | - | - | 87.8 | 78.2 | - | - | - | - | - | -
    | - |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| VV-Net [[186](#bib.bib186)] | - | - | 87.8 | 78.2 | - | - | - | - | - | -
    | - |'
- en: '| Permutohedral lattice | SPLATNet [[188](#bib.bib188)] | - | - | - | - | -
    | - | - | - | - | 39.3 | 18.4 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| Permutohedral lattice | SPLATNet [[188](#bib.bib188)] | - | - | - | - | -
    | - | - | - | - | 39.3 | 18.4 |'
- en: '| LatticeNet [[189](#bib.bib189)] | - | - | - | - | - | - | - | - | - | 64.0
    | 52.2 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| LatticeNet [[189](#bib.bib189)] | - | - | - | - | - | - | - | - | - | 64.0
    | 52.2 |'
- en: '| Hybrid Methods | Hybrid | 3DMV [[190](#bib.bib190)] | - | - | - | - | - |
    - | - | - | - | 48.4 | - |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| Hybrid Methods | Hybrid | 3DMV [[190](#bib.bib190)] | - | - | - | - | - |
    - | - | - | - | 48.4 | - |'
- en: '| UPB [[200](#bib.bib200)] | - | - | - | - | - | - | - | - | - | 63.4 | - |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| UPB [[200](#bib.bib200)] | - | - | - | - | - | - | - | - | - | 63.4 | - |'
- en: '| MVPNet [[191](#bib.bib191)] | - | - | - | - | - |  | - | - | - | 64.1 | -
    |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| MVPNet [[191](#bib.bib191)] | - | - | - | - | - |  | - | - | - | 64.1 | -
    |'
- en: '| Point -based Methods | Point-wise MLP | PointNet [[5](#bib.bib5)] | - | 41.1
    | 78.6 | 47.6 | - | - | - | - | - | - | 14.6 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| Point -based Methods | Point-wise MLP | PointNet [[5](#bib.bib5)] | - | 41.1
    | 78.6 | 47.6 | - | - | - | - | - | - | 14.6 |'
- en: '| PointNet++ [[54](#bib.bib54)] | - | - | 81.0 | 54.5 | 85.7 | 63.1 | - | -
    | 84.5 | 33.9 | 20.1 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| PointNet++ [[54](#bib.bib54)] | - | - | 81.0 | 54.5 | 85.7 | 63.1 | - | -
    | 84.5 | 33.9 | 20.1 |'
- en: '| PointSIFT [[141](#bib.bib141)] | - | - | 88.7 | 70.2 | - | - | - | - | 86.2
    | 41.5 | - |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| PointSIFT [[141](#bib.bib141)] | - | - | 88.7 | 70.2 | - | - | - | - | 86.2
    | 41.5 | - |'
- en: '| Engelmann [[210](#bib.bib210)] | 84.2 | 52.2 | 84.0 | 58.3 | - | - | - |
    - | - | - | - |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| Engelmann [[210](#bib.bib210)] | 84.2 | 52.2 | 84.0 | 58.3 | - | - | - |
    - | - | - | - |'
- en: '| 3DContextNet [[107](#bib.bib107)] | - | - | 84.9 | 55.6 | - | - | - | - |
    - | - | - |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 3DContextNet [[107](#bib.bib107)] | - | - | 84.9 | 55.6 | - | - | - | - |
    - | - | - |'
- en: '| A-SCN [[109](#bib.bib109)] | - | - | 81.6 | 52.7 | - | - | - | - | - | -
    | - |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| A-SCN [[109](#bib.bib109)] | - | - | 81.6 | 52.7 | - | - | - | - | - | -
    | - |'
- en: '| PointWeb [[57](#bib.bib57)] | 87.0 | 60.3 | 87.3 | 66.7 | - | - | - | - |
    85.9 | - | - |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| PointWeb [[57](#bib.bib57)] | 87.0 | 60.3 | 87.3 | 66.7 | - | - | - | - |
    85.9 | - | - |'
- en: '| PAT [[56](#bib.bib56)] |  | 60.1 |  | 64.3 | - | - | - | - | - | - | - |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| PAT [[56](#bib.bib56)] |  | 60.1 |  | 64.3 | - | - | - | - | - | - | - |'
- en: '| LSANet [[207](#bib.bib207)] | - | - | 86.8 | 62.2 | - | - | - | - | 85.1
    | - | - |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| LSANet [[207](#bib.bib207)] | - | - | 86.8 | 62.2 | - | - | - | - | 85.1
    | - | - |'
- en: '| ShellNet [[205](#bib.bib205)] | - | - | 87.1 | 66.8 | - | - | 93.2 | 69.3
    | 85.2 | - | - |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| ShellNet [[205](#bib.bib205)] | - | - | 87.1 | 66.8 | - | - | 93.2 | 69.3
    | 85.2 | - | - |'
- en: '| RandLA-Net [[206](#bib.bib206)] | - | - | 88.0 | 70.0 | 94.6 | 74.8 | 94.8
    | 77.4 | - | - | 55.9 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| RandLA-Net [[206](#bib.bib206)] | - | - | 88.0 | 70.0 | 94.6 | 74.8 | 94.8
    | 77.4 | - | - | 55.9 |'
- en: '| Point convolution | PointCNN [[79](#bib.bib79)] | 85.9 | 57.3 | 88.1 | 65.4
    | - | - | - | - | 85.1 | 45.8 | - |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| Point convolution | PointCNN [[79](#bib.bib79)] | 85.9 | 57.3 | 88.1 | 65.4
    | - | - | - | - | 85.1 | 45.8 | - |'
- en: '| PCCN [[201](#bib.bib201)] | - | 58.3 | - | - | - | - | - | - | - | - | -
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| PCCN [[201](#bib.bib201)] | - | 58.3 | - | - | - | - | - | - | - | - | -
    |'
- en: '| A-CNN [[82](#bib.bib82)] | - | - | 87.3 | - | - | - | - | - | 85.4 | - |
    - |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| A-CNN [[82](#bib.bib82)] | - | - | 87.3 | - | - | - | - | - | 85.4 | - |
    - |'
- en: '| ConvPoint [[66](#bib.bib66)] | - | - | 88.8 | 68.2 | 93.4 | 76.5 | - | -
    | - | - | - |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| ConvPoint [[66](#bib.bib66)] | - | - | 88.8 | 68.2 | 93.4 | 76.5 | - | -
    | - | - | - |'
- en: '| KPConv [[65](#bib.bib65)] | - | 67.1 | - | 70.6 | - | - | 92.9 | 74.6 | -
    | 68.4 | - |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| KPConv [[65](#bib.bib65)] | - | 67.1 | - | 70.6 | - | - | 92.9 | 74.6 | -
    | 68.4 | - |'
- en: '| DPC [[211](#bib.bib211)] | 86.8 | 61.3 | - | - | - | - | - | - | - | 59.2
    | - |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| DPC [[211](#bib.bib211)] | 86.8 | 61.3 | - | - | - | - | - | - | - | 59.2
    | - |'
- en: '| InterpCNN [[80](#bib.bib80)] | - | - | 88.7 | 66.7 | - | - | - | - | - |
    - | - |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| InterpCNN [[80](#bib.bib80)] | - | - | 88.7 | 66.7 | - | - | - | - | - |
    - | - |'
- en: '| RNN -based | RSNet [[212](#bib.bib212)] | - | 51.9 | - | 56.5 | - | - | -
    | - | 84.9 | 39.4 | - |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| RNN -based | RSNet [[212](#bib.bib212)] | - | 51.9 | - | 56.5 | - | - | -
    | - | 84.9 | 39.4 | - |'
- en: '| G+RCU [[213](#bib.bib213)] | - | 45.1 | 81.1 | 49.7 | - | - | - | - | - |
    - | - |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| G+RCU [[213](#bib.bib213)] | - | 45.1 | 81.1 | 49.7 | - | - | - | - | - |
    - | - |'
- en: '| 3P-RNN [[202](#bib.bib202)] | 85.7 | 53.4 | 86.9 | 56.3 | - | - | - | - |
    - | - | - |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 3P-RNN [[202](#bib.bib202)] | 85.7 | 53.4 | 86.9 | 56.3 | - | - | - | - |
    - | - | - |'
- en: '| Graph -based | DGCNN [[87](#bib.bib87)] | - | - | 84.1 | 56.1 | - | - | -
    | - | - | - | - |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| Graph -based | DGCNN [[87](#bib.bib87)] | - | - | 84.1 | 56.1 | - | - | -
    | - | - | - |'
- en: '| SPG [[203](#bib.bib203)] | 86.4 | 58.0 | 85.5 | 62.1 | 92.9 | 76.2 | 94.0
    | 73.2 | - | - | 17.4 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| SPG [[203](#bib.bib203)] | 86.4 | 58.0 | 85.5 | 62.1 | 92.9 | 76.2 | 94.0
    | 73.2 | - | - | 17.4 |'
- en: '| SSP+SPG [[214](#bib.bib214)] | 87.9 | 61.7 | 87.9 | 68.4 | - | - | - | -
    | - | - | - |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| SSP+SPG [[214](#bib.bib214)] | 87.9 | 61.7 | 87.9 | 68.4 | - | - | - | -
    | - | - | - |'
- en: '| GACNet [[215](#bib.bib215)] | 87.8 | 62.9 | - | - | - | - | 91.9 | 70.8 |
    - | - | - |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| GACNet [[215](#bib.bib215)] | 87.8 | 62.9 | - | - | - | - | 91.9 | 70.8 |
    - | - | - |'
- en: '| PAG [[216](#bib.bib216)] | 86.8 | 59.3 | 88.1 | 65.9 | - | - | - | - | -
    | - |  |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| PAG [[216](#bib.bib216)] | 86.8 | 59.3 | 88.1 | 65.9 | - | - | - | - | -
    | - |  |'
- en: '| HDGCN [[217](#bib.bib217)] | - | 59.3 | - | 66.9 | - | - | - | - | - | -
    | - |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| HDGCN [[217](#bib.bib217)] | - | 59.3 | - | 66.9 | - | - | - | - | - | -
    | - |'
- en: '| HPEIN [[218](#bib.bib218)] | 87.2 | 61.9 | 88.2 | 67.8 | - | - | - | - |
    - | 61.8 | - |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| HPEIN [[218](#bib.bib218)] | 87.2 | 61.9 | 88.2 | 67.8 | - | - | - | - |
    - | 61.8 | - |'
- en: '| SPH3D-GCN [[219](#bib.bib219)] | 87.7 | 59.5 | 88.6 | 68.9 | - | - | - |
    - | - | 61.0 | - |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| SPH3D-GCN [[219](#bib.bib219)] | 87.7 | 59.5 | 88.6 | 68.9 | - | - | - |
    - | - | 61.0 | - |'
- en: '| DPAM [[92](#bib.bib92)] | 86.1 | 60.0 | 87.6 | 64.5 | - | - | - | - | - |
    - | - |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| DPAM [[92](#bib.bib92)] | 86.1 | 60.0 | 87.6 | 64.5 | - | - | - | - | - |
    - | - |'
- en: 'Point Convolution Methods. These methods tend to propose effective convolution
    operators for point clouds. Hua et al. [[76](#bib.bib76)] proposed a point-wise
    convolution operator, where the neighboring points are binned into kernel cells
    and then convolved with kernel weights. As shown in Fig. [12](#S5.F12 "Figure
    12 ‣ 5.1.4 Point-based Methods ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point Cloud
    Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")(b), Wang et al. [[201](#bib.bib201)]
    proposed a network called PCCN based on parametric continuous convolution layers.
    The kernel function of this layer is parameterized by MLPs and spans the continuous
    vector space. Thomas et al. [[65](#bib.bib65)] proposed a Kernel Point Fully Convolutional
    Network (KP-FCNN) based on Kernel Point Convolution (KPConv). Specifically, the
    convolution weights of KPConv are determined by the Euclidean distances to kernel
    points, and the number of kernel points is not fixed. The positions of the kernel
    points are formulated as an optimization problem of best coverage in a sphere
    space. Note that, the radius neighbourhood is used to keep a consistent receptive
    field, while grid subsampling is used in each layer to achieve high robustness
    under varying densities of point clouds. In [[211](#bib.bib211)], Engelmann et
    al. provided rich ablation experiments and visualization results to show the impact
    of receptive field on the performance of aggregation-based methods. They also
    proposed a Dilated Point Convolution (DPC) operation to aggregate dilated neighboring
    features, instead of the K nearest neighbours. This operation is demonstrated
    to be very effective in increasing the receptive field and can be easily integrated
    into existing aggregation-based networks.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '点卷积方法。这些方法倾向于为点云提出有效的卷积算子。Hua 等人 [[76](#bib.bib76)] 提出了一个点对点卷积算子，其中邻近的点被分配到核单元中，然后与核权重进行卷积。如图
    [12](#S5.F12 "图 12 ‣ 5.1.4 基于点的方法 ‣ 5.1 3D 语义分割 ‣ 5 3D 点云分割 ‣ 深度学习用于 3D 点云: 一项综述")(b)
    所示，Wang 等人 [[201](#bib.bib201)] 提出了一个名为 PCCN 的网络，该网络基于参数连续卷积层。该层的核函数由 MLP 参数化，覆盖连续向量空间。Thomas
    等人 [[65](#bib.bib65)] 提出了基于核点卷积 (KPConv) 的核点全卷积网络 (KP-FCNN)。具体而言，KPConv 的卷积权重由与核点的欧几里得距离确定，且核点的数量不是固定的。核点的位置被公式化为在球面空间中的最佳覆盖优化问题。注意，半径邻域用于保持一致的感受野，而网格下采样则用于每一层，以在点云的不同密度下实现高鲁棒性。在
    [[211](#bib.bib211)] 中，Engelmann 等人提供了丰富的消融实验和可视化结果，以显示感受野对基于聚合的方法性能的影响。他们还提出了一种扩张点卷积
    (DPC) 操作来聚合扩张的邻近特征，而不是 K 最近邻。这种操作在增加感受野方面被证明非常有效，并且可以轻松集成到现有的基于聚合的网络中。'
- en: 'RNN-based Methods. To capture inherent context features from point clouds,
    Recurrent Neural Networks (RNN) have also been used for semantic segmentation
    of point clouds. Based on PointNet [[5](#bib.bib5)], Engelmann et al. [[213](#bib.bib213)]
    first transformed a block of points into multi-scale blocks and grid blocks to
    obtain input-level context. Then, the block-wise features extracted by PointNet
    are sequentially fed into Consolidation Units (CU) or Recurrent Consolidation
    Units (RCU) to obtain output-level context. Experimental results show that incorporating
    spatial context is important for the improvement of the segmentation performance.
    Huang et al. [[212](#bib.bib212)] proposed a lightweight local dependency modeling
    module, and utilized a slice pooling layer to convert unordered point feature
    sets into an ordered sequence of feature vectors. As shown in Fig. [12](#S5.F12
    "Figure 12 ‣ 5.1.4 Point-based Methods ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point
    Cloud Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")(c), Ye et al.
    [[202](#bib.bib202)] first proposed a Pointwise Pyramid Pooling (3P) module to
    capture the coarse-to-fine local structure, and then utilized two-direction hierarchical
    RNNs to further obtain long-range spatial dependencies. RNN is then applied to
    achieve an end-to-end learning. However, these methods lose rich geometric features
    and density distribution from point clouds when aggregating the local neighbourhood
    features with global structure features [[220](#bib.bib220)]. To alleviate the
    problems caused by the rigid and static pooling operations, Zhao et al. [[220](#bib.bib220)]
    proposed a Dynamic Aggregation Network (DAR-Net) to consider both global scene
    complexity and local geometric features. The inter-medium features are dynamically
    aggregated using a self-adapted receptive field and node weights. Liu et al. [[221](#bib.bib221)]
    proposed 3DCNN-DQN-RNN for efficient semantic parsing of large-scale point clouds.
    This network first learns the spatial distribution and color features using a
    3D CNN network, DQN is further used to localize objects belonging to a specific
    class. The final concatenated feature vector is fed into a residual RNN to obtain
    the final segmentation results.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '基于RNN的方法。为了从点云中捕捉固有的上下文特征，递归神经网络（RNN）也被用于点云的语义分割。基于PointNet [[5](#bib.bib5)]，Engelmann等人[[213](#bib.bib213)]首先将一块点云转化为多尺度块和网格块，以获得输入级别的上下文。然后，将PointNet提取的块级特征依次输入到整合单元（CU）或递归整合单元（RCU）中，以获得输出级别的上下文。实验结果表明，结合空间上下文对于提高分割性能至关重要。Huang等人[[212](#bib.bib212)]提出了一个轻量级的局部依赖建模模块，并利用切片池化层将无序的点特征集转化为有序的特征向量序列。如图[12](#S5.F12
    "Figure 12 ‣ 5.1.4 Point-based Methods ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point
    Cloud Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")(c)所示，Ye等人[[202](#bib.bib202)]首次提出了一个点位金字塔池化（3P）模块，以捕捉从粗到细的局部结构，然后利用双向层次RNN进一步获得长程空间依赖。随后应用RNN实现端到端学习。然而，这些方法在将局部邻域特征与全局结构特征聚合时会丢失点云中的丰富几何特征和密度分布[[220](#bib.bib220)]。为了解决由刚性和静态池化操作造成的问题，Zhao等人[[220](#bib.bib220)]提出了一个动态聚合网络（DAR-Net），以同时考虑全局场景复杂性和局部几何特征。中间特征通过自适应感受野和节点权重动态聚合。Liu等人[[221](#bib.bib221)]提出了3DCNN-DQN-RNN，用于大规模点云的高效语义解析。该网络首先使用3D
    CNN网络学习空间分布和颜色特征，然后使用DQN进一步定位属于特定类别的对象。最终的拼接特征向量被输入到残差RNN中，以获得最终的分割结果。'
- en: 'Graph-based Methods. To capture the underlying shapes and geometric structures
    of 3D point clouds, several methods resort to graph networks. As shown in Fig.
    [12](#S5.F12 "Figure 12 ‣ 5.1.4 Point-based Methods ‣ 5.1 3D Semantic Segmentation
    ‣ 5 3D Point Cloud Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")(d),
    Landrieu et al. [[203](#bib.bib203)] represented a point cloud as a set of interconnected
    simple shapes and superpoints, and used an attributed directed graph (i.e., superpoint
    graph) to capture the structure and context information. Then, the large-scale
    point cloud segmentation problem is spilt into three sub-problems, i.e., geometrically
    homogeneous partition, superpoint embedding, and contextual segmentation. To further
    improve the partition step, Landrieu and Boussaha [[214](#bib.bib214)] proposed
    a supervised framework to oversegment a point cloud into pure superpoints. This
    problem is formulated as a deep metric learning problem structured by an adjacency
    graph. In addition, a graph-structured contrastive loss is also proposed to help
    the recognition of borders between objects.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '基于图的方法。为了捕捉 3D 点云的潜在形状和几何结构，一些方法借助图网络。如图 [12](#S5.F12 "Figure 12 ‣ 5.1.4 Point-based
    Methods ‣ 5.1 3D Semantic Segmentation ‣ 5 3D Point Cloud Segmentation ‣ Deep
    Learning for 3D Point Clouds: A Survey")(d) 所示，Landrieu 等人 [[203](#bib.bib203)]
    将点云表示为一组互连的简单形状和超点，并使用带有属性的有向图（即超点图）来捕捉结构和上下文信息。然后，将大规模点云分割问题分解为几何上同质的分区、超点嵌入和上下文分割三个子问题。为了进一步改善分区步骤，Landrieu
    和 Boussaha [[214](#bib.bib214)] 提出了一个监督框架，将点云过度分割为纯超点。这个问题被构造成一个由邻接图结构化的深度度量学习问题。此外，还提出了一种图结构对比损失，帮助识别物体之间的边界。'
- en: To better capture the local geometric relationships in high-dimensional space,
    Kang et al. [[222](#bib.bib222)] proposed a PyramNet based on Graph Embedding
    Module (GEM) and Pyramid Attention Network (PAN). The GEM module formulates a
    point cloud as a directed acyclic graph and utilzes a covariance matrix to replace
    the Euclidean distance for the construction of adjacent similarity matrix. Convolution
    kernels with four different sizes are used in the PAN module to extract features
    with different semantic intensities. In [[215](#bib.bib215)], Graph Attention
    Convolution (GAC) is proposed to selectively learn relevant features from a local
    neighboring set. This operation is achieved by dynamically assigning attention
    weights to different neighboring points and feature channels based on their spatial
    positions and feature differences. GAC can learn to capture discriminative features
    for segmentation, and has similar characteristics to the commonly used CRF model.
    Ma et al. [[223](#bib.bib223)] proposed a Point Global Context Reasoning (PointGCR)
    module to capture global contextual information along the channel dimension using
    an undirected graph representation. PointGCR is a plug-and-play and end-to-end
    trainable module. It can easily be integrated into an existing segmentation network
    to achieve performance improvement.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地捕捉高维空间中的局部几何关系，Kang 等人 [[222](#bib.bib222)] 提出了基于图嵌入模块（GEM）和金字塔注意力网络（PAN）的
    PyramNet。GEM 模块将点云形式化为有向无环图，并利用协方差矩阵替代欧氏距离来构建邻接相似度矩阵。在 PAN 模块中使用四种不同尺寸的卷积核来提取具有不同语义强度的特征。在
    [[215](#bib.bib215)] 中，提出了图注意力卷积（GAC），以选择性地从局部邻域集合中学习相关特征。该操作通过根据空间位置和特征差异动态分配注意力权重到不同的邻域点和特征通道来实现。GAC
    可以学习捕捉用于分割的判别特征，并具有类似于常用 CRF 模型的特性。Ma 等人 [[223](#bib.bib223)] 提出了一个点全局上下文推理（PointGCR）模块，通过无向图表示来捕捉通道维度上的全局上下文信息。PointGCR
    是一个即插即用且端到端可训练的模块，可以轻松集成到现有的分割网络中以实现性能提升。
- en: In addition, several very recent work tries to achieve semantic segmentation
    of point clouds under weak supervision. Wei et al. [[224](#bib.bib224)] proposed
    a two-stage approach to train a segmentation network with subcloud level labels.
    Xu et al. [[225](#bib.bib225)] investigated several inexact supervision schemes
    for semantic segmentation of point clouds. They also proposed a network that is
    able to be trained with only partially labeled points (e.g. 10%).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些非常近期的工作尝试在弱监督下实现点云的语义分割。Wei 等人 [[224](#bib.bib224)] 提出了一个两阶段的方法来训练具有子云级标签的分割网络。Xu
    等人 [[225](#bib.bib225)] 探索了几种不准确的监督方案用于点云的语义分割。他们还提出了一个能够仅用部分标记点（例如 10%）进行训练的网络。
- en: 5.2 Instance Segmentation
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 实例分割
- en: 'Compared to semantic segmentation, instance segmentation is more challenging
    as it requires more accurate and fine-grained reasoning of points. In particular,
    it not only needs to distinguish the points with different semantic meanings,
    but also separate instances with the same semantic meaning. Overall, existing
    methods can be divided into two groups: proposal-based methods and proposal-free
    methods. Several milestone methods are illustrated in Fig [13](#S5.F13 "Figure
    13 ‣ 5.2 Instance Segmentation ‣ 5 3D Point Cloud Segmentation ‣ Deep Learning
    for 3D Point Clouds: A Survey").'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 与语义分割相比，实例分割更具挑战性，因为它需要对点进行更准确、更细致的推理。特别是，它不仅需要区分具有不同语义含义的点，还要分开具有相同语义含义的实例。总体而言，现有方法可以分为两类：基于提议的方法和无提议的方法。几个具有里程碑意义的方法在图
    [13](#S5.F13 "图 13 ‣ 5.2 实例分割 ‣ 5 3D 点云分割 ‣ 深度学习在3D点云中的应用：综述") 中进行了说明。
- en: '![Refer to caption](img/a9bd484412de52b8a31d4881bd1a4359.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a9bd484412de52b8a31d4881bd1a4359.png)'
- en: 'Figure 13: Chronological overview of the most relevant deep learning-based
    3D instance segmentation methods.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：基于深度学习的3D实例分割方法的时间概览。
- en: 5.2.1 Proposal-based Methods
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 基于提议的方法
- en: 'These methods convert the instance segmentation problem into two sub-tasks:
    3D object detection and instance mask prediction.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法将实例分割问题转换为两个子任务：3D目标检测和实例掩码预测。
- en: Hou et al. [[226](#bib.bib226)] proposed a 3D fully-convolutional Semantic Instance
    Segmentation (3D-SIS) network to achieve semantic instance segmentation on RGB-D
    scans. This network learns from both color and geometry features. Similar to 3D
    object detection, a 3D Region Proposal Network (3D-RPN) and a 3D Region of Interesting
    (3D-RoI) layer are used to predict bounding box locations, object class labels
    and instance masks. Following the analysis-by-synthesis strategy, Yi et al. [[227](#bib.bib227)]
    proposed a Generative Shape Proposal Network (GSPN) to generate high-objectness
    3D proposals. These proposals are further refined by a Region-based PointNet (R-PointNet).
    The final label is obtained by predicting a per-point binary mask for each class
    label. Different from direct regression of 3D bounding boxes from point clouds,
    this method removes a large amount of meaningless proposals by enforcing geometric
    understanding.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: Hou 等人 [[226](#bib.bib226)] 提出了一个3D完全卷积语义实例分割（3D-SIS）网络，以在RGB-D扫描上实现语义实例分割。该网络同时学习颜色和几何特征。类似于3D目标检测，使用3D区域提议网络（3D-RPN）和3D感兴趣区域（3D-RoI）层来预测边界框位置、目标类别标签和实例掩码。按照分析-合成策略，Yi
    等人 [[227](#bib.bib227)] 提出了生成形状提议网络（GSPN）来生成高物体性3D提议。这些提议通过基于区域的PointNet（R-PointNet）进一步精炼。最终的标签是通过为每个类别标签预测一个每点的二进制掩码获得的。与直接回归点云中的3D边界框不同，该方法通过强制几何理解来去除大量无意义的提议。
- en: By extending 2D panoptic segmentation to 3D mapping, Narita et al. [[228](#bib.bib228)]
    proposed an online volumetric 3D mapping system to jointly achieve large-scale
    3D reconstruction, semantic labeling, and instance segmentation. They first utilized
    2D semantic and instance segmentation networks to obtain pixel-wise panoptic labels
    and then integrated these labels to the volumtric map. A fully-connected CRF is
    further used to achieve accurate segmentation. This semantic mapping system can
    achieve high-quality semantic mapping and discriminative object recognition. Yang
    et al. [[229](#bib.bib229)] proposed a single-stage, anchor-free and end-to-end
    trainable network called 3D-BoNet to achieve instance segmentation on point clouds.
    This method directly regresses rough 3D bounding boxes for all potential instances,
    and then utilizes a point-level binary classifier to obtain instance labels. Particularly,
    the bounding box generation task is formulated as an optimal assignment problem.
    In addition, a multi-criteria loss function is also proposed to regularize the
    generated bounding boxes. This method does not need any post-processing and is
    computationally efficient. Zhang et al. [[230](#bib.bib230)] proposed a network
    for instance segmentation of large-scale outdoor LiDAR point clouds. This method
    learns a feature representation on the bird’s-eye view of point clouds using self-attention
    blocks. The final instance labels are obtained based on the predicted horizontal
    center and the height limits. Shi et al. [[231](#bib.bib231)] proposed a hierarchy-aware
    Variational Denoising Recursive AutoEncoder (VDRAE) to predict the layout of indoor
    3D space. The object proposals are iteratively generated and refined by recursive
    context aggregation and propagation.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将2D全景分割扩展到3D映射，Narita等人[[228](#bib.bib228)] 提出了一个在线体积3D映射系统，以共同实现大规模3D重建、语义标注和实例分割。他们首先利用2D语义和实例分割网络获得像素级全景标签，然后将这些标签集成到体积地图中。进一步使用全连接CRF以实现准确的分割。该语义映射系统可以实现高质量的语义映射和辨别性目标识别。Yang等人[[229](#bib.bib229)]
    提出了一个单阶段、无锚点和端到端可训练的网络，称为3D-BoNet，以在点云上实现实例分割。这种方法直接回归所有潜在实例的粗略3D边界框，然后利用点级二分类器获取实例标签。特别是，边界框生成任务被形式化为最优分配问题。此外，还提出了一种多标准损失函数来规范生成的边界框。这种方法不需要任何后处理，并且计算效率高。Zhang等人[[230](#bib.bib230)]
    提出了一个用于大规模户外LiDAR点云实例分割的网络。这种方法通过自注意力块在点云的鸟瞰图上学习特征表示。最终的实例标签基于预测的水平中心和高度限制获得。Shi等人[[231](#bib.bib231)]
    提出了一个层次感知的变分去噪递归自编码器（VDRAE），用于预测室内3D空间的布局。对象提案通过递归上下文聚合和传播进行迭代生成和优化。
- en: Overall, proposal-based methods [[227](#bib.bib227), [226](#bib.bib226), [229](#bib.bib229),
    [232](#bib.bib232)] are intuitive and straightforward, and the instance segmentation
    results usually have good objectness. However, these methods require multi-stage
    training and pruning of redundant proposals. Therefore, they are usually time-consuming
    and computationally expensive.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，基于提案的方法[[227](#bib.bib227), [226](#bib.bib226), [229](#bib.bib229), [232](#bib.bib232)]
    直观且简单，实例分割结果通常具有较好的目标性。然而，这些方法需要多阶段训练和剪枝冗余提案，因此通常耗时且计算成本高。
- en: 5.2.2 Proposal-free Methods
  id: totrans-476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 无提案方法
- en: Proposal-free methods [[233](#bib.bib233), [234](#bib.bib234), [235](#bib.bib235),
    [236](#bib.bib236), [237](#bib.bib237), [238](#bib.bib238), [239](#bib.bib239),
    [240](#bib.bib240)] do not have an object detection module. Instead, they usually
    consider instance segmentation as a subsequent clustering step after semantic
    segmentation. In particular, most existing methods are based on the assumption
    that points belonging to the same instance should have very similar features.
    Therefore, these methods mainly focus on discriminative feature learning and point
    grouping.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 无提案方法[[233](#bib.bib233), [234](#bib.bib234), [235](#bib.bib235), [236](#bib.bib236),
    [237](#bib.bib237), [238](#bib.bib238), [239](#bib.bib239), [240](#bib.bib240)]
    没有目标检测模块。相反，它们通常将实例分割视为语义分割后的后续聚类步骤。特别是，大多数现有方法基于这样的假设：属于同一实例的点应该具有非常相似的特征。因此，这些方法主要关注于区分特征学习和点分组。
- en: In a pioneering work, Wang et al. [[233](#bib.bib233)] first introduced a Similarity
    Group Proposal Network (SGPN). This method first learns a feature and semantic
    map for each point, and then introduces a similarity matrix to represent the similarity
    between each paired features. To learn more discriminative features, they use
    a double-hinge loss to mutually adjust the similarity matrix and semantic segmentation
    results. Finally, a heuristic and non-maximal suppression method is adopted to
    merge similar points into instances. Since the construction of a similarity matrix
    requires large memory consumption, the scalability of this method is limited.
    Similarly, Liu et al. [[237](#bib.bib237)] first leveraged submanifold sparse
    convolution [[166](#bib.bib166)] to predict semantic scores of each voxel and
    affinity between neighboring voxels. They then introduced a clustering algorithm
    to group points into instances based on the predicted affinity and the mesh topology.
    Mo et al. [[241](#bib.bib241)] introduced a detection-by-segmentation network
    in PartNet to achieve instance segmentation. PointNet++ is used as the backbone
    to predict semantic labels of each point and disjoint instance masks. Further,
    Liang et al. [[238](#bib.bib238)] proposed a structure-aware loss for the learning
    of discriminative embeddings. This loss considers both the similarity of features
    and the geometric relations among points. An attention-based graph CNN is further
    used to adaptively refine the learned features by aggregating different information
    from neighbors.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项开创性的工作中，王等人 [[233](#bib.bib233)] 首次提出了相似性群体提议网络（SGPN）。该方法首先为每个点学习一个特征和语义图，然后引入一个相似性矩阵来表示每对特征之间的相似性。为了学习更具区分性的特征，他们使用双重铰链损失来相互调整相似性矩阵和语义分割结果。最后，采用启发式和非极大抑制方法将相似点合并成实例。由于构建相似性矩阵需要大量内存，这种方法的可扩展性有限。类似地，刘等人
    [[237](#bib.bib237)] 首次利用子流形稀疏卷积 [[166](#bib.bib166)] 来预测每个体素的语义分数和相邻体素之间的亲和力。他们随后引入了聚类算法，根据预测的亲和力和网格拓扑将点分组为实例。莫等人
    [[241](#bib.bib241)] 在 PartNet 中引入了基于分割的检测网络以实现实例分割。PointNet++ 被用作主干网络，以预测每个点的语义标签和不重叠的实例掩膜。此外，梁等人
    [[238](#bib.bib238)] 提出了一个结构感知损失用于学习区分性嵌入。该损失考虑了特征的相似性和点之间的几何关系。进一步地，基于注意力的图 CNN
    被用来通过聚合邻居的不同信息来自适应地优化学习到的特征。
- en: Since the semantic category and instance label of a point are usually dependent
    on each other, several methods have been proposed to couple these two tasks into
    a single task. Wang et al. [[234](#bib.bib234)] integrated these two tasks by
    introducing an end-to-end and learnable Associatively Segmenting Instances and
    Semantics (ASIS) module. Experiments show that semantic features and instance
    features can mutually support each other to achieve an improved performance through
    this ASIS module. Similarly, Zhao et al. [[242](#bib.bib242)] proposed JSNet to
    achieve both semantic and instance segmentation. Further, Pham et al. [[235](#bib.bib235)]
    first introduced a Multi-Task Point-wise Network (MT-PNet) to assign a label to
    each point and regularized the embeddings in the feature space by introducing
    a discriminative loss [[243](#bib.bib243)]. They then fused the predicted semantic
    labels and embeddings to a Multi-Value Conditional Random Field (MV-CRF) model
    for joint optimization. Finally, mean-field variational inference is used to produce
    semantic labels and instance labels. Hu et al. [[244](#bib.bib244)] first proposed
    a Dynamic Region Growing (DRG) method to dynamically separate a point cloud into
    a set of disjoint patches, and then used an unsupervised K-means++ algorithm to
    group all these patches. Multi-scale patch segmentation is then performed with
    the guidance of contextual information between patches. Finally, these labeled
    patches are merged into object level to obtain final semantic and instance labels.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点的语义类别和实例标签通常是相互依赖的，已经提出了几种将这两项任务结合为一个单一任务的方法。Wang 等人 [[234](#bib.bib234)]
    通过引入一个端到端且可学习的关联实例和语义（ASIS）模块来整合这两项任务。实验表明，语义特征和实例特征可以通过这个 ASIS 模块相互支持，从而实现性能的提升。同样，Zhao
    等人 [[242](#bib.bib242)] 提出了 JSNet 来同时实现语义和实例分割。此外，Pham 等人 [[235](#bib.bib235)]
    首次引入了一个多任务点-wise 网络（MT-PNet），为每个点分配标签，并通过引入判别损失 [[243](#bib.bib243)] 来规范化特征空间中的嵌入。他们随后将预测的语义标签和嵌入融合到一个多值条件随机场（MV-CRF）模型中进行联合优化。最后，使用均值场变分推断来生成语义标签和实例标签。Hu
    等人 [[244](#bib.bib244)] 首次提出了一种动态区域生长（DRG）方法，以动态地将点云分离为一组不相交的补丁，然后使用无监督的 K-means++
    算法对这些补丁进行分组。接着在补丁之间的上下文信息指导下进行多尺度补丁分割。最后，这些标记的补丁被合并为对象级别，以获得最终的语义和实例标签。
- en: To achieve instance segmentation on full 3D scenes, Elich et al. [[236](#bib.bib236)]
    presented a hybrid 2D-3D network to jointly learn global consistent instance features
    from a BEV representation and local geometric features of point clouds. The learned
    features are then combined to achieve semantic and instance segmentation. Note
    that, rather than heuristic GroupMerging algorithms [[233](#bib.bib233)], a more
    flexible Meanshift [[245](#bib.bib245)] algorithm is used to group these points
    into instances. Alternatively, multi-task learning is also introduced for instance
    segmentation. Lahoud et al. [[246](#bib.bib246)] learned both the unique feature
    embedding of each instance and the directional information to estimate the object’s
    center. Feature embedding loss and directional loss are proposed to adjust the
    learned feature embeddings in latent feature space. Mean-shift clustering and
    non-maximum suppression are adopted to group voxels into instances. This method
    achieves the state-of-the-art performance on the ScanNet [[11](#bib.bib11)] benchmark.
    Besides, the predicted directional information is particularly useful to determine
    the boundary of instances. Zhang et al. [[247](#bib.bib247)] introduced probabilistic
    embeddings to instance segmentation of point clouds. This method also incorporates
    uncertainty estimation and proposes a new loss function for the clustering step.
    Jiang et al. [[240](#bib.bib240)] proposed a PointGroup network, which is composed
    of a semantic segmentation branch and an offset prediction branch. A dual-set
    clustering algorithm and the ScoreNet is further utilized to achieve better grouping
    results.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现对完整3D场景的实例分割，Elich 等人[[236](#bib.bib236)] 提出了一个混合的2D-3D网络，用于从BEV表示中共同学习全局一致的实例特征和点云的局部几何特征。然后将学习到的特征结合起来实现语义和实例分割。注意，与启发式的GroupMerging算法[[233](#bib.bib233)]不同，这里使用了更灵活的Meanshift[[245](#bib.bib245)]算法来将这些点分组为实例。或者，多任务学习也被引入到实例分割中。Lahoud
    等人[[246](#bib.bib246)] 学习了每个实例的独特特征嵌入和方向信息来估计物体的中心。提出了特征嵌入损失和方向损失来调整潜在特征空间中的学习特征嵌入。采用均值漂移聚类和非极大值抑制来将体素分组为实例。该方法在ScanNet
    [[11](#bib.bib11)]基准上实现了最先进的性能。此外，预测的方向信息对于确定实例的边界特别有用。张 等人[[247](#bib.bib247)]
    将概率嵌入引入了点云的实例分割。这种方法还结合了不确定性估计，并为聚类步骤提出了一种新的损失函数。姜 等人[[240](#bib.bib240)] 提出了一个PointGroup网络，该网络由语义分割分支和偏移预测分支组成。进一步利用了双集合聚类算法和ScoreNet来实现更好的分组结果。
- en: In summary, proposal-free methods do not require computationally expensive region-proposal
    components. However, the objectness of instance segments grouped by these methods
    is usually low since these methods do not explicitly detect object boundaries.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，无提议方法不需要计算成本高昂的区域提议组件。然而，由这些方法分组的实例段的对象性通常较低，因为这些方法没有明确检测物体边界。
- en: 5.3 Part Segmentation
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 部分分割
- en: The difficulties for part segmentation of 3D shapes are twofold. First, shape
    parts with the same semantic label have a large geometric variation and ambiguity.
    Second, the number of parts in objects with the same semantic meanings may be
    different.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 3D形状的部分分割面临双重困难。首先，具有相同语义标签的形状部件在几何上有很大的变化和模糊性。其次，具有相同语义意义的物体的部件数量可能不同。
- en: VoxSegNet [[248](#bib.bib248)] is proposed to achieve fine-grained part segmentation
    on 3D voxelized data under a limited solution. A Spatial Dense Extraction (SDE)
    module (which consists of stacked atrous residual blocks) is proposed to extract
    multi-scale discriminative features from sparse volumetric data. The learned features
    are further re-weighted and fused by progressively applying an Attention Feature
    Aggregation (AFA) module. Kalogerakis et al. [[249](#bib.bib249)] combined FCNs
    and surface-based CRFs to achieve end-to-end 3D part segmentation. They first
    generated images from multiple views to achieve optimal surface coverage and fed
    these images into a 2D network to produce confidence maps. Then, these confidence
    maps are aggregated by a surface-based CRF, which is responsible for a consistent
    labeling of the entire scene. Yi et al. [[250](#bib.bib250)] introduced a Synchronized
    Spectral CNN (SyncSpecCNN) to perform convolution on irregular and non-isomorphic
    shape graphs. A spectral parameterization of dilated convolutional kernels and
    a spectral transformer network is introduced to solve the problem of multi-scale
    analysis in parts and information sharing across shapes.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: VoxSegNet [[248](#bib.bib248)] 被提出用于在有限的解决方案下实现 3D 体素数据的精细分割。提出了一种空间密集提取（SDE）模块（由堆叠的膨胀残差块组成），用于从稀疏的体积数据中提取多尺度的判别特征。学习到的特征通过逐步应用注意力特征聚合（AFA）模块进一步加权和融合。Kalogerakis
    等人 [[249](#bib.bib249)] 将 FCNs 和基于表面的 CRFs 结合起来，实现了端到端的 3D 部件分割。他们首先从多个视角生成图像，以实现最佳的表面覆盖，然后将这些图像输入到
    2D 网络中以生成置信度图。接着，这些置信度图通过基于表面的 CRF 进行聚合，该 CRF 负责对整个场景进行一致的标注。Yi 等人 [[250](#bib.bib250)]
    引入了一种同步谱卷积神经网络（SyncSpecCNN），用于在不规则和非同构的形状图上进行卷积。引入了膨胀卷积核的谱参数化和谱变换网络，以解决部件中的多尺度分析和形状间信息共享的问题。
- en: Wang et al. [[251](#bib.bib251)] first performed shape segmentation on 3D meshes
    by introducing Shape Fully Convolutional Networks (SFCN) and taking three low-level
    geometric features as its input. They then utilized voting-based multi-label graph
    cuts to further refine the segmentation results. Zhu et al. [[252](#bib.bib252)]
    proposed a weakly-supervised CoSegNet for 3D shape co-segmentation. This network
    takes a collection of unsegmented 3D point cloud shapes as input, and produces
    shape part labels by iteratively minimizing a group consistency loss. Similar
    to CRF, a pre-trained part-refinement network is proposed to further refine and
    denoise part proposals. Chen et al. [[253](#bib.bib253)] proposed a Branched AutoEncoder
    network (BAE-NET) for unsupervised, one-shot and weakly supervised 3D shape co-segmentation.
    This method formulates the shape co-segmentation task as a representation learning
    problem and aims at finding the simplest part representations by minimizing the
    shape reconstruction loss. Based on the encoder-decoder architecture, each branch
    of this network can learn a compact representation for a specific part shape.
    The features learned from each branch and the point coordinate are then fed to
    the decoder to produce a binary value (which indicates whether the point belongs
    to this part). This method has good generalization ability and can process large
    3D shape collections (up to 5000+ shapes). However, it is sensitive to initial
    parameters and does not incorporate shape semantics into the network, which hinders
    this method to obtain a robust and stable estimation in each iteration. Yu et
    al. [[254](#bib.bib254)] proposed a top-down recursive part decomposition network
    (PartNet) for hierarchical shape segmentation. Different from existing methods
    that segment a shape to a fixed label set, this network formulates part segmentation
    as a problem of cascade binary labeling, and decompose the input point cloud to
    an arbitrary number of parts based on the geometric structure. Luo et al. [[255](#bib.bib255)]
    introduced a learning-based grouping framework for the task of zero-shot 3D part
    segmentation. To improve the cross-category generalization ability, this method
    tends to learn a grouping policy that restricts the network to learn part-level
    features within the part local context.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[251](#bib.bib251)] 首次通过引入形状全卷积网络 (SFCN) 并以三个低级几何特征作为输入来对 3D 网格进行形状分割。然后，他们利用基于投票的多标签图切割方法进一步细化分割结果。Zhu
    等人 [[252](#bib.bib252)] 提出了一个弱监督的 CoSegNet 用于 3D 形状的共同分割。该网络以一组未分割的 3D 点云形状作为输入，通过迭代最小化组一致性损失来生成形状部件标签。类似于
    CRF，提出了一个预训练的部件细化网络来进一步细化和去噪部件提议。Chen 等人 [[253](#bib.bib253)] 提出了一个分支自编码器网络 (BAE-NET)
    用于无监督、单次和弱监督的 3D 形状共同分割。该方法将形状共同分割任务公式化为表示学习问题，并通过最小化形状重建损失来寻找最简单的部件表示。基于编码器-解码器架构，该网络的每个分支可以为特定部件形状学习一个紧凑的表示。然后，将从每个分支学到的特征和点坐标输入解码器，以生成一个二进制值（指示点是否属于该部件）。该方法具有良好的泛化能力，可以处理大型
    3D 形状集合（多达 5000+ 个形状）。然而，它对初始参数敏感，并且未将形状语义纳入网络，这阻碍了该方法在每次迭代中获得稳健和稳定的估计。Yu 等人 [[254](#bib.bib254)]
    提出了一个自上而下的递归部件分解网络 (PartNet) 用于分层形状分割。与现有方法将形状分割到固定标签集的不同，该网络将部件分割公式化为级联二进制标注的问题，并根据几何结构将输入点云分解为任意数量的部件。Luo
    等人 [[255](#bib.bib255)] 引入了一个基于学习的分组框架，用于零样本 3D 部件分割任务。为了提高跨类别泛化能力，该方法倾向于学习一种分组策略，以限制网络在部件局部上下文内学习部件级特征。
- en: 5.4 Summary
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 总结
- en: 'Table [V](#S5.T5 "TABLE V ‣ 5.1.4 Point-based Methods ‣ 5.1 3D Semantic Segmentation
    ‣ 5 3D Point Cloud Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")
    shows the results achieved by existing methods on public benchmark, including
    S3DIS [[10](#bib.bib10)], Semantic3D [[12](#bib.bib12)], ScanNet [[39](#bib.bib39)],
    and SemanticKITTI [[15](#bib.bib15)]. The following issues need to be further
    investigated:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [V](#S5.T5 "TABLE V ‣ 5.1.4 Point-based Methods ‣ 5.1 3D Semantic Segmentation
    ‣ 5 3D Point Cloud Segmentation ‣ Deep Learning for 3D Point Clouds: A Survey")
    显示了现有方法在公共基准测试上的成果，包括 S3DIS [[10](#bib.bib10)]、Semantic3D [[12](#bib.bib12)]、ScanNet
    [[39](#bib.bib39)] 和 SemanticKITTI [[15](#bib.bib15)]。以下问题需要进一步探讨：'
- en: $\bullet$
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Thanks to the regular data representation, both projection-based methods and
    discretization-based methods can leverage the mature network architecture from
    their 2D image counterparts. However, the main limitation of projection-based
    methods lies in the information loss caused by 3D-2D projection, while the main
    bottleneck for discretization-based methods is the cubically increased computational
    and memory costs caused by the increase of the resolution. To this end, sparse
    convolution building upon indexing structures would be a feasible solution and
    worth further exploration.
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于常规数据表示，投影基础方法和离散化基础方法都可以利用其2D图像对应物的成熟网络架构。然而，投影基础方法的主要限制在于3D-2D投影引起的信息丢失，而离散化基础方法的主要瓶颈在于分辨率增加导致的计算和内存成本的立方级增长。为此，基于索引结构的稀疏卷积将是一个可行的解决方案，值得进一步探讨。
- en: $\bullet$
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Point-based networks are the most frequently investigated methods. However,
    point representation naturally does not have explicit neighboring information,
    most existing point-based methods resort to expensive neighbor searching mechanisms
    (e.g., KNN [[79](#bib.bib79)] or ball query [[54](#bib.bib54)]). This inherently
    limits the efficiency of these methods, the recently proposed point-voxel joint
    representation [[256](#bib.bib256)] would be an interesting direction for further
    investigation.
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于点的方法是研究最频繁的技术。然而，点表示自然没有明确的邻域信息，大多数现有的基于点的方法依赖于昂贵的邻域搜索机制（例如，KNN [[79](#bib.bib79)]
    或球查询 [[54](#bib.bib54)]）。这在本质上限制了这些方法的效率，最近提出的点-体素联合表示 [[256](#bib.bib256)] 将是一个有趣的进一步研究方向。
- en: $\bullet$
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Learning from imbalanced data is still a challenging problem in point cloud
    segmentation. Although several approaches [[205](#bib.bib205), [65](#bib.bib65),
    [203](#bib.bib203)] have achieved a remarkable overall performance, their performance
    on minority classes is still limited. For example, RandLA-Net [[206](#bib.bib206)]
    achieves an overall IoU of 76.0% on the reduced-8 subset of Semantic3D, but a
    very low IOU of 41.1% on the class of hardscape.
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从不平衡数据中学习仍然是点云分割中的一个挑战性问题。尽管一些方法[[205](#bib.bib205), [65](#bib.bib65), [203](#bib.bib203)]取得了显著的整体性能，但在少数类上的表现仍然有限。例如，RandLA-Net
    [[206](#bib.bib206)] 在Semantic3D的减少-8子集上实现了76.0%的整体IoU，但在硬景观类上的IoU仅为41.1%。
- en: $\bullet$
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: The majority of existing approaches [[5](#bib.bib5), [54](#bib.bib54), [79](#bib.bib79),
    [207](#bib.bib207), [205](#bib.bib205)] work on small point clouds (e.g., 1m$\times$1m
    with 4096 points). In practice, the point clouds acquired by depth sensors are
    usually immense and large-scale. Therefore, it is desirable to further investigate
    the problem of efficient segmentation of large-scale point clouds.
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前大多数方法[[5](#bib.bib5), [54](#bib.bib54), [79](#bib.bib79), [207](#bib.bib207),
    [205](#bib.bib205)]处理的是小型点云（例如，1m$\times$1m，4096点）。实际上，由深度传感器获取的点云通常是庞大且大规模的。因此，进一步研究大规模点云的高效分割问题是必要的。
- en: $\bullet$
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: A handful of works [[178](#bib.bib178), [179](#bib.bib179), [199](#bib.bib199)]
    have started to learn spatio-temporal information from dynamic point clouds. It
    is expected that the spatio-temporal information can help to improve the performance
    of subsequent tasks such as 3D object recognition, segmentation, and completion.
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些研究[[178](#bib.bib178), [179](#bib.bib179), [199](#bib.bib199)]已经开始从动态点云中学习时空信息。预计这些时空信息可以帮助提高后续任务的性能，如3D对象识别、分割和补全。
- en: 6 Conclusion
  id: totrans-498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper has presented a contemporary survey of the state-of-the-art methods
    for 3D understanding, including 3D shape classification, 3D object detection and
    tracking, and 3D scene and object segmentation. A comprehensive taxonomy and performance
    comparison of these methods have been presented. Merits and demerits of various
    methods are also covered, with potential research directions being listed.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 本文呈现了3D理解领域的前沿方法综述，包括3D形状分类、3D对象检测与跟踪以及3D场景与对象分割。提供了这些方法的全面分类和性能比较，同时也涵盖了各种方法的优缺点，并列出了潜在的研究方向。
- en: Acknowledgments
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was partially supported by the National Natural Science Foundation
    of China (No. 61972435, 61602499, 61872379), the Natural Science Foundation of
    Guangdong Province (2019A1515011271), the Science and Technology Innovation Committee
    of Shenzhen Municipality (JCYJ20190807152209394), the Australian Research Council
    (Grants DP150100294 and DP150104251), the China Scholarship Council (CSC) and
    the Academy of Finland.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Z. Liang, Y. Guo, Y. Feng, W. Chen, L. Qiao, L. Zhou, J. Zhang, and H. Liu,
    “Stereo matching using multi-level cost volume and multi-scale feature constancy,”
    *IEEE TPAMI*, 2019.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y. Guo, F. Sohel, M. Bennamoun, M. Lu, and J. Wan, “Rotational projection
    statistics for 3D local surface description and object recognition,” *IJCV*, 2013.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Guo, M. Bennamoun, F. Sohel, M. Lu, and J. Wan, “3D object recognition
    in cluttered scenes with local surface features: a survey,” *IEEE TPAMI*, 2014.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3D object detection
    network for autonomous driving,” in *CVPR*, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning on point
    sets for 3D classification and segmentation,” in *CVPR*, 2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3D shapeNets:
    A deep representation for volumetric shapes,” in *CVPR*, 2015.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *ICCV*, 2019.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, and H. Su, “ShapeNet: An information-rich 3D model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su,
    “PartNet: A large-scale benchmark for fine-grained and hierarchical part-level
    3D object understanding,” in *CVPR*, 2019.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and
    S. Savarese, “3D semantic parsing of large-scale indoor spaces,” in *CVPR*, 2016.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “ScanNet: Richly-annotated 3D reconstructions of indoor scenes,” in *CVPR*, 2017.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] T. Hackel, N. Savinov, L. Ladicky, J. Wegner, K. Schindler, and M. Pollefeys,
    “Semantic3D.net: A new large-scale point cloud classification benchmark,” *ISPRS*,
    2017.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su, H. Li, and
    R. Yang, “Apollocar3D: A large 3D car instance understanding benchmark for autonomous
    driving,” in *CVPR*, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving,”
    in *CVPR*, 2012.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    and J. Gall, “SemanticKITTI: A dataset for semantic scene understanding of lidar
    sequences,” in *ICCV*, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] G. Elbaz, T. Avraham, and A. Fischer, “3D point cloud registration for
    localization using a deep neural network auto-encoder,” in *CVPR*, 2017, pp. 4631–4640.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, and J. Xiao,
    “Multi-view self-supervised deep learning for 6D pose estimation in the amazon
    picking challenge,” in *ICRA*, 2017, pp. 1386–1383.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Han, H. Laga, and M. Bennamoun, “Image-based 3D object reconstruction:
    State-of-the-art and trends in the deep learning era,” *IEEE TPAMI*, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. Ioannidou, E. Chatzilari, S. Nikolopoulos, and I. Kompatsiaris, “Deep
    learning advances in computer vision with 3D data: A survey,” *ACM Computing Surveys*,
    2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] E. Ahmed, A. Saint, A. E. R. Shabayek, K. Cherenkova, R. Das, G. Gusev,
    D. Aouada, and B. Ottersten, “Deep learning advances on different 3D data representations:
    A survey,” *arXiv preprint arXiv:1808.01462*, 2018.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Xie, J. Tian, and X. Zhu, “A review of point cloud semantic segmentation,”
    *IEEE GRSM*, 2020.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. M. Rahman, Y. Tan, J. Xue, and K. Lu, “Recent advances in 3D object
    detection in the era of deep neural networks: A survey,” *IEEE TIP*, 2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] K. Siddiqi, J. Zhang, D. Macrini, A. Shokoufandeh, S. Bouix, and S. Dickinson,
    “Retrieving articulated 3-D models using medial surfaces,” *Machine Vision and
    Applications*, vol. 19, no. 4, pp. 261–275, 2008.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] M. De Deuge, B. Douillard, C. Hung, and A. Quadros, “Unsupervised feature
    learning for classification of outdoor 3D scans,” in *ACRA*, 2013.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun RGB-D: A RGB-D scene understanding
    benchmark suite,” in *CVPR*, 2015.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Patil, S. Malla, H. Gang, and Y.-T. Chen, “The H3D dataset for full-surround
    3D multi-object detection and tracking in crowded urban scenes,” in *ICRA*, 2019.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett, D. Wang,
    P. Carr, S. Lucey, D. Ramanan *et al.*, “Argoverse: 3D tracking and forecasting
    with rich maps,” in *CVPR*, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira,
    M. Yuan, B. Low, A. Jain, P. Ondruska *et al.*, “Lyft level 5 av dataset 2019,”
    2019.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Q.-H. Pham, P. Sevestre, R. S. Pahwa, H. Zhan, C. H. Pang, Y. Chen, A. Mustafa,
    V. Chandrasekhar, and J. Lin, “A*3D dataset: Towards autonomous driving in challenging
    environments,” *ICRA*, 2020.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev,
    S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and
    D. Anguelov, “Scalability in perception for autonomous driving: Waymo open dataset,”
    in *CVPR*, 2020.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” in *CVPR*, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] D. Munoz, J. A. Bagnell, N. Vandapel, and M. Hebert, “Contextual classification
    with functional max-margin markov networks,” in *CVPR*, 2009, pp. 975–982.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] F. Rottensteiner, G. Sohn, J. Jung, M. Gerke, C. Baillard, S. Benitez,
    and U. Breitkopf, “The isprs benchmark on urban object classification and 3D building
    reconstruction,” *ISPRS*, 2012.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Serna, B. Marcotegui, F. Goulette, and J.-E. Deschaud, “Paris-rue-madame
    database: a 3D mobile laser scanner dataset for benchmarking urban detection,
    segmentation and classification methods,” in *ICRA*, 2014.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] B. Vallet, M. Brédif, A. Serna, B. Marcotegui, and N. Paparoditis, “Terramobilita/iqmulus
    urban point cloud analysis benchmark,” *Computers & Graphics*, vol. 49, pp. 126–133,
    2015.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] X. Roynard, J.-E. Deschaud, and F. Goulette, “Paris-lille-3d: A large
    and high-quality ground-truth urban point cloud dataset for automatic segmentation
    and classification,” *IJRR*, 2018.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] W. Tan, N. Qin, L. Ma, Y. Li, J. Du, G. Cai, K. Yang, and J. Li, “Toronto-3D:
    A large-scale mobile lidar dataset for semantic segmentation of urban roadways,”
    *arXiv preprint arXiv:2003.08284*, 2020.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] N. Varney, V. K. Asari, and Q. Graehling, “Dales: A large-scale aerial
    lidar data set for semantic segmentation,” *arXiv preprint arXiv:2004.11985*,
    2020.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] H. Lu, X. Chen, G. Zhang, Q. Zhou, Y. Ma, and Y. Zhao, “SCANet: Spatial-channel
    attention network for 3D object detection,” in *ICASSP*, 2019.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view convolutional
    neural networks for 3D shape recognition,” in *ICCV*, 2015.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] T. Yu, J. Meng, and J. Yuan, “Multi-view harmonized bilinear network for
    3D object recognition,” in *CVPR*, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Yang and L. Wang, “Learning relationships for multi-view 3D object
    recognition,” in *ICCV*, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J. Guibas, “Volumetric
    and multi-view CNNs for object classification on 3D data,” in *CVPR*, 2016.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Y. Feng, Z. Zhang, X. Zhao, R. Ji, and Y. Gao, “GVCNN: Group-view convolutional
    neural networks for 3D shape recognition,” in *CVPR*, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C. Wang, M. Pelillo, and K. Siddiqi, “Dominant set clustering and pooling
    for multi-view 3D object recognition,” *BMVC*, 2017.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] C. Ma, Y. Guo, J. Yang, and W. An, “Learning multi-view representation
    with LSTM for 3D shape recognition and retrieval,” *IEEE TMM*, 2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] X. Wei, R. Yu, and J. Sun, “View-gcn: View-based graph convolutional network
    for 3D shape analysis,” in *CVPR*, 2020.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] D. Maturana and S. Scherer, “VoxNet: A 3D convolutional neural network
    for real-time object recognition,” in *IROS*, 2015.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] G. Riegler, A. Osman Ulusoy, and A. Geiger, “OctNet: Learning deep 3D
    representations at high resolutions,” in *CVPR*, 2017.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, “O-CNN: Octree-based
    convolutional neural networks for 3D shape analysis,” *ACM TOG*, 2017.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] T. Le and Y. Duan, “PointGrid: A deep network for 3D shape understanding,”
    in *CVPR*, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Y. Ben-Shabat, M. Lindenbaum, and A. Fischer, “3D point cloud classification
    and segmentation using 3D modified fisher vector representation for convolutional
    neural networks,” *arXiv preprint arXiv:1711.08241*, 2017.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov,
    and A. J. Smola, “Deep sets,” in *NeurIPS*, 2017.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deep hierarchical
    feature learning on point sets in a metric space,” in *NeurIPS*, 2017.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Joseph-Rivlin, A. Zvirin, and R. Kimmel, “Mo-Net: Flavor the moments
    in learning to classify shapes,” in *ICCVW*, 2018.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian, “Modeling
    point clouds with self-attention and gumbel subset sampling,” in *CVPR*, 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Zhao, L. Jiang, C.-W. Fu, and J. Jia, “PointWeb: Enhancing local neighborhood
    features for point cloud processing,” in *CVPR*, 2019.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Duan, Y. Zheng, J. Lu, J. Zhou, and Q. Tian, “Structural relational
    reasoning of point clouds,” in *CVPR*, 2019.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] H. Lin, Z. Xiao, Y. Tan, H. Chao, and S. Ding, “Justlookup: One millisecond
    deep feature extraction for point clouds by lookup tables,” in *ICME*, 2019.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] X. Sun, Z. Lian, and J. Xiao, “SRINet: Learning strictly rotation-invariant
    representations for point cloud classification and segmentation,” in *ACM MM*,
    2019.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui, “Pointasnl: Robust point
    clouds processing using nonlocal neural networks with adaptive sampling,” in *CVPR*,
    2020.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Y. Liu, B. Fan, S. Xiang, and C. Pan, “Relation-shape convolutional neural
    network for point cloud analysis,” in *CVPR*, 2019.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Boulch, “Generalizing discrete convolutions for unstructured point
    clouds,” *arXiv preprint arXiv:1904.02375*, 2019.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Liu, B. Fan, G. Meng, J. Lu, S. Xiang, and C. Pan, “DensePoint: Learning
    densely contextual representation for efficient point cloud processing,” in *ICCV*,
    2019.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J.
    Guibas, “KPConv: Flexible and deformable convolution for point clouds,” in *ICCV*,
    2019.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. Boulch, “ConvPoint: continuous convolutions for point cloud processing,”
    *Computers & Graphics*, 2020.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] W. Wu, Z. Qi, and L. Fuxin, “PointConv: Deep convolutional networks on
    3D point clouds,” in *CVPR*, 2019.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] P. Hermosilla, T. Ritschel, P.-P. Vázquez, À. Vinacua, and T. Ropinski,
    “Monte carlo convolution for learning on non-uniformly sampled point clouds,”
    *ACM TOG*, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Xu, T. Fan, M. Xu, L. Zeng, and Y. Qiao, “SpiderCNN: Deep learning
    on point sets with parameterized convolutional filters,” in *ECCV*, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. Matan, M. Haggai, and L. Yaron, “Point convolutional neural networks
    by extension operators,” *ACM TOG*, 2018.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C. Esteves, C. Allen-Blanchette, A. Makadia, and K. Daniilidis, “Learning
    so(3) equivariant representations with spherical CNNs,” in *ECCV*, 2017.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley,
    “Tensor field networks: Rotation-and translation-equivariant neural networks for
    3D point clouds,” *arXiv preprint arXiv:1802.08219*, 2018.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] T. S. Cohen, M. Geiger, J. Koehler, and M. Welling, “Spherical CNNs,”
    *ICLR*, 2018.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] A. Poulenard, M.-J. Rakotosaona, Y. Ponty, and M. Ovsjanikov, “Effective
    rotation-invariant point CNN with spherical harmonics kernels,” in *3DV*, 2019.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] F. Groh, P. Wieschollek, and H. P. Lensch, “Flex-Convolution,” in *ACCV*,
    2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] B.-S. Hua, M.-K. Tran, and S.-K. Yeung, “Pointwise convolutional neural
    networks,” in *CVPR*, 2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Lei, N. Akhtar, and A. Mian, “Octree guided cnn with spherical kernels
    for 3D point clouds,” in *CVPR*, 2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] S. Lan, R. Yu, G. Yu, and L. S. Davis, “Modeling local geometric structure
    of 3D point clouds using geo-cnn,” in *CVPR*, 2019.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “PointCNN: Convolution
    on x-transformed points,” in *NeurIPS*, 2018.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] J. Mao, X. Wang, and H. Li, “Interpolated convolutional networks for 3D
    point cloud understanding,” in *ICCV*, 2019.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Z. Zhang, B.-S. Hua, D. W. Rosen, and S.-K. Yeung, “Rotation invariant
    convolutions for 3D point clouds deep learning,” in *3DV*, 2019.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] A. Komarichev, Z. Zhong, and J. Hua, “A-CNN: Annularly convolutional neural
    networks on point clouds,” in *CVPR*, 2019.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Kumawat and S. Raman, “LP-3DCNN: Unveiling local phase in 3D convolutional
    neural networks,” in *CVPR*, 2019.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Rao, J. Lu, and J. Zhou, “Spherical fractal convolutional neural networks
    for point cloud recognition,” in *CVPR*, 2019.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] M. Simonovsky and N. Komodakis, “Dynamic edge-conditioned filters in convolutional
    neural networks on graphs,” in *CVPR*, 2017.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] R. B. Rusu and S. Cousins, “3D is here: Point cloud library (PCL),” in
    *ICRA*, 2011.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
    “Dynamic graph CNN for learning on point clouds,” *ACM TOG*, 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] K. Zhang, M. Hao, J. Wang, C. W. de Silva, and C. Fu, “Linked dynamic
    graph CNN: Learning on point cloud via linking hierarchical features,” *arXiv
    preprint arXiv:1904.10014*, 2019.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Yang, C. Feng, Y. Shen, and D. Tian, “FoldingNet: Point cloud auto-encoder
    via deep grid deformation,” in *CVPR*, 2018.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] K. Hassani and M. Haley, “Unsupervised multi-task feature learning on
    point clouds,” in *ICCV*, 2019.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Liu, B. Ni, C. Li, J. Yang, and Q. Tian, “Dynamic points agglomeration
    for hierarchical point sets learning,” in *ICCV*, 2019.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. Shen, C. Feng, Y. Yang, and D. Tian, “Mining point cloud local structures
    by kernel correlation and graph pooling,” in *CVPR*, 2018.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] M. Dominguez, R. Dhamdhere, A. Petkar, S. Jain, S. Sah, and R. Ptucha,
    “General-purpose deep point cloud feature extractor,” in *WACV*, 2018.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] C. Chen, G. Li, R. Xu, T. Chen, M. Wang, and L. Lin, “ClusterNet: Deep
    hierarchical cluster network with rigorously rotation-invariant representation
    for point cloud analysis,” in *CVPR*, 2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] D. Müllner, “Modern hierarchical, agglomerative clustering algorithms,”
    *arXiv preprint arXiv:1109.2378*, 2011.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Q. Xu, X. Sun, C.-Y. Wu, P. Wang, and U. Neumann, “Grid-gcn for fast and
    scalable point cloud learning,” in *CVPR*, 2020.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. Bruna, W. Zaremba, A. Szlam, and Y. Lecun, “Spectral networks and locally
    connected networks on graphs,” *ICLR*, 2014.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural
    networks on graphs with fast localized spectral filtering,” in *NeurIPS*, 2016.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] G. Te, W. Hu, A. Zheng, and Z. Guo, “RGCNN: Regularized graph CNN for
    point cloud segmentation,” in *ACM MM*, 2018.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] R. Li, S. Wang, F. Zhu, and J. Huang, “Adaptive graph convolutional neural
    networks,” in *AAAI*, 2018.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao, “Hypergraph neural networks,”
    in *AAAI*, 2019.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] C. Wang, B. Samari, and K. Siddiqi, “Local spectral graph convolution
    for point set feature learning,” in *ECCV*, 2018.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Zhang and M. Rabbat, “A Graph-CNN for 3D point cloud classification,”
    in *ICASSP*, 2018.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] G. Pan, J. Wang, R. Ying, and P. Liu, “3DTI-Net: Learn inner transform
    invariant 3D geometry features using dynamic GCN,” *arXiv preprint arXiv:1812.06254*,
    2018.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] R. Klokov and V. Lempitsky, “Escape from cells: Deep kd-networks for
    the recognition of 3D point cloud models,” in *ICCV*, 2017.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] W. Zeng and T. Gevers, “3DContextNet: K-d tree guided hierarchical learning
    of point clouds using local and global contextual cues,” in *ECCV*, 2018.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Li, B. M. Chen, and G. Hee Lee, “SO-Net: Self-organizing network for
    point cloud analysis,” in *CVPR*, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] S. Xie, S. Liu, Z. Chen, and Z. Tu, “Attentional ShapeContextNet for
    point cloud recognition,” in *CVPR*, 2018.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] H. You, Y. Feng, R. Ji, and Y. Gao, “PVNet: A joint convolutional network
    of point cloud and multi-view for 3D shape recognition,” in *ACM MM*, 2018.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] H. You, Y. Feng, X. Zhao, C. Zou, R. Ji, and Y. Gao, “PVRNet: Point-view
    relation neural network for 3D shape recognition,” in *AAAI*, 2019.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Zhao, T. Birdal, H. Deng, and F. Tombari, “3D point capsule networks,”
    in *CVPR*, 2019.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] W. Chen, X. Han, G. Li, C. Chen, J. Xing, Y. Zhao, and H. Li, “Deep RBFNet:
    Point cloud feature learning using radial basis functions,” *arXiv preprint arXiv:1812.04302*,
    2018.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker, “Point2Sequence: Learning
    the shape representation of 3D point clouds with an attention-based sequence to
    sequence network,” in *AAAI*, 2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] P. Wu, C. Chen, J. Yi, and D. Metaxas, “Point cloud processing via recurrent
    set encoding,” in *AAAI*, 2019.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] C. Qin, H. You, L. Wang, C.-C. J. Kuo, and Y. Fu, “PointDAN: A multi-scale
    3D domain adaption network for point cloud representation,” in *NIPS*, 2019.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] B. Sievers and J. Sauder, “Self-supervised deep learning on point clouds
    by reconstructing space,” in *NIPS*, 2019.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] R. Li, X. Li, P.-A. Heng, and C.-W. Fu, “PointAugment: An auto-augmentation
    framework for point cloud classification,” in *CVPR*, 2020.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and object recognition
    using shape contexts,” *IEEE TPAMI*, 2002.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, 2017.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] D. Bobkov, S. Chen, R. Jian, Z. Iqbal, and E. Steinbach, “Noise-resistant
    deep learning for object classification in 3D point clouds using a point pair
    descriptor,” *IEEE RAL*, 2018.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] S. Prokudin, C. Lassner, and J. Romero, “Efficient learning on point
    clouds with basis point sets,” in *ICCV*, 2019.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” *IJCV*, 2020.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for
    3D object detection in point clouds,” *ICCV*, 2019.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] W. Shi and R. Rajkumar, “Point-GNN: Graph neural network for 3D object
    detection in a point cloud,” in *CVPR*, 2020.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, “Joint 3D
    proposal generation and object detection from view aggregation,” in *IROS*, 2018.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion for
    multi-sensor 3D object detection,” in *ECCV*, 2018.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-task multi-sensor
    fusion for 3D object detection,” in *CVPR*, 2019.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] B. Yang, W. Luo, and R. Urtasun, “PIXOR: Real-time 3D object detection
    from point clouds,” in *CVPR*, 2018.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time end-to-end
    3D detection, tracking and motion forecasting with a single convolutional net,”
    in *CVPR*, 2018.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Y. Zeng, Y. Hu, S. Liu, J. Ye, Y. Han, X. Li, and N. Sun, “RT3D: Real-time
    3D vehicle detection in lidar point cloud for autonomous driving,” *IEEE RAL*,
    2018.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “IPOD: Intensive point-based
    object detector for point cloud,” *arXiv preprint arXiv:1812.05276*, 2018.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] S. Shi, X. Wang, and H. Li, “PointRCNN: 3D object proposal generation
    and detection from point cloud,” in *CVPR*, 2019.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Z. Jesus, G. Silvio, and G. Bernard, “PointRGCN: Graph convolution networks
    for 3D vehicles detection refinement,” *arXiv preprint arXiv:1911.12236*, 2019.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] V. Sourabh, L. Alex H., H. Bassam, and B. Oscar, “PointPainting: Sequential
    fusion for 3D object detection,” in *CVPR*, 2020.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Y. Zhou and O. Tuzel, “VoxelNet: End-to-end learning for point cloud
    based 3D object detection,” in *CVPR*, 2018.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, “PointPillars:
    Fast encoders for object detection from point clouds,” in *CVPR*, 2019.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “STD: Sparse-to-dense 3D
    object detector for point cloud,” in *ICCV*, 2019.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum PointNets
    for 3D object detection from RGB-D data,” in *CVPR*, 2018.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] X. Zhao, Z. Liu, R. Hu, and K. Huang, “3D object detection using scale
    invariant and feature reweighting networks,” in *AAAI*, 2019.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M. Jiang, Y. Wu, and C. Lu, “PointSIFT: A sift-like network module for
    3D point cloud semantic segmentation,” *arXiv preprint arXiv:1807.00652*, 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] D. Xu, D. Anguelov, and A. Jain, “PointFusion: Deep sensor fusion for
    3D bounding box estimation,” in *CVPR*, 2018.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] K. Shin, Y. P. Kwon, and M. Tomizuka, “RoarNet: A robust 3D object detection
    based on region approximation refinement,” in *IEEE IV*, 2019.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Z. Wang and K. Jia, “Frustum convNet: Sliding frustums to aggregate local
    point-wise features for amodal 3D object detection,” in *IROS*, 2019.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] L. Johannes, M. Andreas, A. Thomas, H. Markus, N. Bernhard, and H. Sepp,
    “Patch refinement - localized 3D object detection,” *arXiv preprint arXiv:1910.04093*,
    2019.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] D. Zhou, J. Fang, X. Song, C. Guan, J. Yin, Y. Dai, and R. Yang, “Iou
    loss for 2D/3D object detection,” in *3DV*, 2019.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Y. Chen, S. Liu, X. Shen, and J. Jia, “Fast point r-cnn,” in *ICCV*,
    2019.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “PV-RCNN:
    Point-voxel feature set abstraction for 3D object detection,” in *CVPR*, 2020.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M. Feng, S. Z. Gilani, Y. Wang, L. Zhang, and A. Mian, “Relation graph
    network for 3D object detection in point clouds,” *arXiv preprint arXiv:1912.00202*,
    2019.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] C. R. Qi, X. Chen, O. Litany, and L. J. Guibas, “ImVoteNet: Boosting
    3D object detection in point clouds with image votes,” in *CVPR*, 2020.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] S. Shi, Z. Wang, X. Wang, and H. Li, “From points to parts: 3D object
    detection from point cloud with part-aware and part-aggregation network,” *TPAMI*,
    2020.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] B. Yang, M. Liang, and R. Urtasun, “HDNET: Exploiting hd maps for 3D
    object detection,” in *CoRL*, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] J. Beltrán, C. Guindel, F. M. Moreno, D. Cruzado, F. García, and A. De La Escalera,
    “BirdNet: a 3D object detection framework from lidar information,” in *ITSC*,
    2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3D lidar using fully
    convolutional network,” *arXiv preprint arXiv:1608.07916*, 2016.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] B. Li, “3D fully convolutional network for vehicle detection in point
    cloud,” in *IROS*, 2017.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner, “Vote3Deep:
    Fast object detection in 3D point clouds using efficient convolutional neural
    networks,” in *ICRA*, 2017.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] X. Li, J. E. Guivant, N. Kwok, and Y. Xu, “3D backbone network for 3D
    object detection,” in *CoRR*, 2019.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Y. Yan, Y. Mao, and B. Li, “SECOND: Sparsely embedded convolutional detection,”
    *Sensors*, 2018.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] V. A. Sindagi, Y. Zhou, and O. Tuzel, “MVX-Net: Multimodal voxelnet for
    3D object detection,” in *ICRA*, 2019.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] C. He, H. Zeng, J. Huang, X.-S. Hua, and L. Zhang, “Structure aware single-stage
    3D object detection from point cloud,” in *CVPR*, 2020.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Z. Yang, Y. Sun, S. Liu, and J. Jia, “3DSSD: Point-based 3D single stage
    object detector,” in *CVPR*, 2020.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] G. P. Meyer, A. Laddha, E. Kee, C. Vallespi-Gonzalez, and C. K. Wellington,
    “LaserNet: An efficient probabilistic 3D object detector for autonomous driving,”
    *CVPR*, 2019.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] G. P. Meyer, J. Charland, D. Hegde, A. Laddha, and C. Vallespi-Gonzalez,
    “Sensor fusion for joint 3D object detection and semantic segmentation,” *CVPRW*,
    2019.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Q. Chen, L. Sun, Z. Wang, K. Jia, and A. Yuille, “Object as hotspots:
    An anchor-free 3D object detection approach via firing of hotspots,” *arXiv preprint
    arXiv:1912.12791*, 2019.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks
    for biomedical image segmentation,” in *MICCAI*, 2015, pp. 234–241.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] B. Graham, M. Engelcke, and L. van der Maaten, “3D semantic segmentation
    with submanifold sparse convolutional networks,” in *CVPR*, 2018.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Q. Hu, Y. Guo, Y. Chen, J. Xiao, and W. An, “Correlation filter tracking:
    Beyond an open-loop system,” in *BMVC*, 2017.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] H. Liu, Q. Hu, B. Li, and Y. Guo, “Robust long-term tracking via instance
    specific proposals,” *IEEE TIM*, 2019.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,
    “Fully-convolutional siamese networks for object tracking,” in *ECCV*, 2016.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] S. Giancola, J. Zarzar, and B. Ghanem, “Leveraging shape completion for
    3D siamese tracking,” *CVPR*, 2019.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] M. Mueller, N. Smith, and B. Ghanem, “Context-aware correlation filter
    tracking,” in *CVPR*, 2017.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] J. Zarzar, S. Giancola, and B. Ghanem, “Efficient tracking proposals
    using 2D-3D siamese networks on lidar,” *arXiv preprint arXiv:1903.10168*, 2019.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] M. Simon, K. Amende, A. Kraus, J. Honer, T. Sämann, H. Kaulbersch, S. Milz,
    and H. M. Gross, “Complexer-YOLO: Real-time 3D object detection and tracking on
    semantic point clouds,” *CVPRW*, 2019.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, “P2B: Point-to-box network
    for 3D object tracking in point clouds,” in *CVPR*, 2020.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] X. Liu, C. R. Qi, and L. J. Guibas, “FlowNet3D: Learning scene flow in
    3D point clouds,” in *CVPR*, 2019.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Z. Wang, S. Li, H. Howard-Jenkins, V. Prisacariu, and M. Chen, “FlowNet3D++:
    Geometric losses for deep scene flow estimation,” in *WACV*, 2020.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] X. Gu, Y. Wang, C. Wu, Y. J. Lee, and P. Wang, “HPLFlowNet: Hierarchical
    permutohedral lattice flowNet for scene flow estimation on large-scale point clouds,”
    in *CVPR*, 2019.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] H. Fan and Y. Yang, “PointRNN: Point recurrent neural network for moving
    point cloud processing,” *arXiv preprint arXiv:1910.08287*, 2019.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] X. Liu, M. Yan, and J. Bohg, “MeteorNet: Deep learning on dynamic 3D
    point cloud sequences,” in *ICCV*, 2019.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] H. Mittal, B. Okorn, and D. Held, “Just go with the flow: Self-supervised
    scene flow estimation,” in *CVPR*, 2020.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] F. J. Lawin, M. Danelljan, P. Tosteberg, G. Bhat, F. S. Khan, and M. Felsberg,
    “Deep projective 3D semantic segmentation,” in *CAIP*, 2017.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] A. Boulch, B. Le Saux, and N. Audebert, “Unstructured point cloud semantic
    labeling using deep segmentation networks.” in *3DOR*, 2017.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] B. Wu, A. Wan, X. Yue, and K. Keutzer, “SqueezeSeg: Convolutional neural
    nets with recurrent crf for real-time road-object segmentation from 3D lidar point
    cloud,” in *ICRA*, 2018.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer, “SqueezeSegV2: Improved
    model structure and unsupervised domain adaptation for road-object segmentation
    from a lidar point cloud,” in *ICRA*, 2019.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss, “RangeNet++: Fast
    and accurate lidar semantic segmentation,” in *IROS*, 2019.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] H.-Y. Meng, L. Gao, Y.-K. Lai, and D. Manocha, “VV-Net: Voxel vae net
    with group convolutions for point cloud segmentation,” in *ICCV*, 2019.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] D. Rethage, J. Wald, J. Sturm, N. Navab, and F. Tombari, “Fully-convolutional
    point networks for large-scale point clouds,” in *ECCV*, 2018.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H. Yang, and J. Kautz,
    “SplatNet: Sparse lattice networks for point cloud processing,” in *CVPR*, 2018.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] R. A. Rosu, P. Schütt, J. Quenzel, and S. Behnke, “LatticeNet: Fast point
    cloud segmentation using permutohedral lattices,” *arXiv preprint arXiv:1912.05905*,
    2019.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] A. Dai and M. Nießner, “3DMV: Joint 3D-multi-view prediction for 3D semantic
    scene segmentation,” in *ECCV*, 2018.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] M. Jaritz, J. Gu, and H. Su, “Multi-view pointNet for 3D scene understanding,”
    in *ICCVW*, 2019.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] N. Audebert, B. Le Saux, and S. Lefèvre, “Semantic segmentation of earth
    observation data using multimodal and multi-scale deep networks,” in *ACCV*, 2016.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] M. Tatarchenko, J. Park, V. Koltun, and Q.-Y. Zhou, “Tangent convolutions
    for dense prediction in 3D,” in *CVPR*, 2018.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer,
    “SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and $<$ 0.5 MB model
    size,” in *ICLR*, 2016.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Huang and S. You, “Point cloud labeling using 3D convolutional neural
    network,” in *ICPR*, 2016.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] L. Tchapmi, C. Choy, I. Armeni, J. Gwak, and S. Savarese, “SEGCloud:
    Semantic segmentation of 3D point clouds,” in *3DV*, 2017.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” in *CVPR*, 2015.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nießner, “ScanComplete:
    Large-scale scene completion and semantic segmentation for 3D scans,” in *CVPR*,
    2018.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] C. Choy, J. Gwak, and S. Savarese, “4D spatio-temporal convnets: Minkowski
    convolutional neural networks,” in *CVPR*, 2019.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] H.-Y. Chiang, Y.-L. Lin, Y.-C. Liu, and W. H. Hsu, “A unified point-based
    framework for 3D segmentation,” in *3DV*, 2019.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] S. Wang, S. Suo, W.-C. Ma, A. Pokrovsky, and R. Urtasun, “Deep parametric
    continuous convolutional neural networks,” in *CVPR*, 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] X. Ye, J. Li, H. Huang, L. Du, and X. Zhang, “3D recurrent neural networks
    with context fusion for point cloud semantic segmentation,” in *ECCV*, 2018.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] L. Landrieu and M. Simonovsky, “Large-scale point cloud semantic segmentation
    with superpoint graphs,” in *CVPR*, 2018.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] F. Engelmann, T. Kontogianni, J. Schult, and B. Leibe, “Know what your
    neighbors do: 3D semantic segmentation of point clouds,” in *ECCVW*, 2018.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Z. Zhang, B.-S. Hua, and S.-K. Yeung, “ShellNet: Efficient point cloud
    convolutional neural networks using concentric shells statistics,” in *ICCV*,
    2019.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and A. Markham,
    “RandLA-Net: Efficient semantic segmentation of large-scale point clouds,” *CVPR*,
    2020.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] L.-Z. Chen, X.-Y. Li, D.-P. Fan, M.-M. Cheng, K. Wang, and S.-P. Lu,
    “LSANet: Feature learning on point sets by local spatial attention,” *arXiv preprint
    arXiv:1905.05442*, 2019.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] C. Zhao, W. Zhou, L. Lu, and Q. Zhao, “Pooling scores of neighboring
    points for improved 3D point cloud segmentation,” in *ICIP*, 2019.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “NetVLAD:
    CNN architecture for weakly supervised place recognition,” in *CVPR*, 2016.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] F. Engelmann, T. Kontogianni, J. Schult, and B. Leibe, “Know what your
    neighbors do: 3D semantic segmentation of point clouds,” in *ECCV*, 2018.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] F. Engelmann, T. Kontogianni, and B. Leibe, “Dilated point convolutions:
    On the receptive field of point convolutions,” in *ICRA*, 2020.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Q. Huang, W. Wang, and U. Neumann, “Recurrent slice networks for 3D segmentation
    of point clouds,” in *CVPR*, 2018.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] F. Engelmann, T. Kontogianni, A. Hermans, and B. Leibe, “Exploring spatial
    context for 3D semantic segmentation of point clouds,” in *ICCV*, 2017.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] L. Landrieu and M. Boussaha, “Point cloud oversegmentation with graph-structured
    deep metric learning,” in *CVPR*, 2019.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] L. Wang, Y. Huang, Y. Hou, S. Zhang, and J. Shan, “Graph attention convolution
    for point cloud semantic segmentation,” in *CVPR*, 2019.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] L. Pan, C.-M. Chew, and G. H. Lee, “Pointatrousgraph: Deep hierarchical
    encoder-decoder with atrous convolution for point clouds,” *arXiv preprint arXiv:1907.09798*,
    2019.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Z. Liang, M. Yang, L. Deng, C. Wang, and B. Wang, “Hierarchical depthwise
    graph convolutional neural network for 3D semantic segmentation of point clouds,”
    in *ICRA*, 2019.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] L. Jiang, H. Zhao, S. Liu, X. Shen, C.-W. Fu, and J. Jia, “Hierarchical
    point-edge interaction network for point cloud semantic segmentation,” in *ICCV*,
    2019.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] H. Lei, N. Akhtar, and A. Mian, “Spherical convolutional neural network
    for 3D point clouds,” *arXiv preprint arXiv:1805.07872*, 2018.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Z. Zhao, M. Liu, and K. Ramani, “DAR-Net: Dynamic aggregation network
    for semantic scene segmentation,” *arXiv preprint arXiv:1907.12022*, 2019.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] F. Liu, S. Li, L. Zhang, C. Zhou, R. Ye, Y. Wang, and J. Lu, “3DCNN-DQN-RNN:
    A deep reinforcement learning framework for semantic parsing of large-scale 3D
    point clouds,” in *ICCV*, 2017.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Z. Kang and N. Li, “PyramNet: Point cloud pyramid attention network and
    graph embedding module for classification and segmentation,” in *ICONIP*, 2019.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Y. Ma, Y. Guo, H. Liu, Y. Lei, and G. Wen, “Global context reasoning
    for semantic segmentation of 3D point clouds,” in *WACV*, 2020.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] J. Wei, G. Lin, K.-H. Yap, T.-Y. Hung, and L. Xie, “Multi-path region
    mining for weakly supervised 3D semantic segmentation on point clouds,” in *CVPR*,
    2020.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] X. Xu and G. H. Lee, “Weakly supervised semantic point cloud segmentation:
    Towards 10x fewer labels,” in *CVPR*, 2020, pp. 13 706–13 715.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] J. Hou, A. Dai, and M. Nießner, “3D-SIS: 3D semantic instance segmentation
    of RGB-D scans,” in *CVPR*, 2019.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] L. Yi, W. Zhao, H. Wang, M. Sung, and L. J. Guibas, “GSPN: Generative
    shape proposal network for 3D instance segmentation in point cloud,” in *CVPR*,
    2019.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] G. Narita, T. Seno, T. Ishikawa, and Y. Kaji, “PanopticFusion: Online
    volumetric semantic mapping at the level of stuff and things,” in *IROS*, 2019.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] B. Yang, J. Wang, R. Clark, Q. Hu, S. Wang, A. Markham, and N. Trigoni,
    “Learning object bounding boxes for 3D instance segmentation on point clouds,”
    in *NeurIPS*, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] F. Zhang, C. Guan, J. Fang, S. Bai, R. Yang, P. Torr, and V. Prisacariu,
    “Instance segmentation of lidar point clouds,” in *ICRA*, 2020.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Y. Shi, A. X. Chang, Z. Wu, M. Savva, and K. Xu, “Hierarchy denoising
    recursive autoencoders for 3D scene layout prediction,” in *CVPR*, 2019.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] F. Engelmann, M. Bokeloh, A. Fathi, B. Leibe, and M. Nießner, “3d-mpa:
    Multi-proposal aggregation for 3d semantic instance segmentation,” in *CVPR*,
    2020.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] W. Wang, R. Yu, Q. Huang, and U. Neumann, “SGPN: Similarity group proposal
    network for 3D point cloud instance segmentation,” in *CVPR*, 2018.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] X. Wang, S. Liu, X. Shen, C. Shen, and J. Jia, “Associatively segmenting
    instances and semantics in point clouds,” in *CVPR*, 2019.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Q.-H. Pham, T. Nguyen, B.-S. Hua, G. Roig, and S.-K. Yeung, “JSIS3D:
    Joint semantic-instance segmentation of 3D point clouds with multi-task pointwise
    networks and multi-value conditional random fields,” in *CVPR*, 2019.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] C. Elich, F. Engelmann, J. Schult, T. Kontogianni, and B. Leibe, “3D-BEVIS:
    Birds-eye-view instance segmentation,” in *GCPR*, 2019.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] C. Liu and Y. Furukawa, “MASC: Multi-scale affinity with sparse convolution
    for 3D instance segmentation,” *arXiv preprint arXiv:1902.04478*, 2019.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Z. Liang, M. Yang, and C. Wang, “3D graph embedding learning with a structure-aware
    loss function for point cloud semantic instance segmentation,” *arXiv preprint
    arXiv:1902.05247*, 2019.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] L. Han, T. Zheng, L. Xu, and L. Fang, “Occuseg: Occupancy-aware 3d instance
    segmentation,” in *CVPR*, 2020.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] L. Jiang, H. Zhao, S. Shi, S. Liu, C.-W. Fu, and J. Jia, “PointGroup:
    Dual-set point grouping for 3D instance segmentation,” in *CVPR*, 2020.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su,
    “PartNet: A large-scale benchmark for fine-grained and hierarchical part-level
    3D object understanding,” in *CVPR*, 2019.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] L. Zhao and W. Tao, “JSNet: Joint instance and semantic segmentation
    of 3D point clouds,” in *AAAI*, 2020.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] B. De Brabandere, D. Neven, and L. Van Gool, “Semantic instance segmentation
    with a discriminative loss function,” in *CVPRW*, 2017.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] S.-M. Hu, J.-X. Cai, and Y.-K. Lai, “Semantic labeling and instance segmentation
    of 3D point clouds using patch context analysis and multiscale processing,” *IEEE
    TVCG*, 2018.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] D. Comaniciu and P. Meer, “Mean shift: A robust approach toward feature
    space analysis,” *IEEE TPAMI*, 2002.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] J. Lahoud, B. Ghanem, M. Pollefeys, and M. R. Oswald, “3D instance segmentation
    via multi-task metric learning,” in *ICCV*, 2019.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] B. Zhang and P. Wonka, “Point cloud instance segmentation using probabilistic
    embeddings,” *arXiv preprint arXiv:1912.00145*, 2019.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Z. Wang and F. Lu, “VoxSegNet: Volumetric CNNs for semantic part segmentation
    of 3D shapes,” *IEEE TVCG*, 2019.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri, “3D shape segmentation
    with projective convolutional networks,” in *CVPR*, 2017.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] L. Yi, H. Su, X. Guo, and L. J. Guibas, “SyncSpecCNN: Synchronized spectral
    CNN for 3D shape segmentation,” in *CVPR*, 2017.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] P. Wang, Y. Gan, P. Shui, F. Yu, Y. Zhang, S. Chen, and Z. Sun, “3D shape
    segmentation via shape fully convolutional networks,” *Computers & Graphics*,
    2018.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] C. Zhu, K. Xu, S. Chaudhuri, L. Yi, L. Guibas, and H. Zhang, “CoSegNet:
    Deep co-segmentation of 3D shapes with group consistency loss,” *arXiv preprint
    arXiv:1903.10297*, 2019.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Z. Chen, K. Yin, M. Fisher, S. Chaudhuri, and H. Zhang, “BAE-NET: Branched
    autoencoder for shape co-segmentation,” in *ICCV*, 2019.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] F. Yu, K. Liu, Y. Zhang, C. Zhu, and K. Xu, “PartNet: A recursive part
    decomposition network for fine-grained and hierarchical shape segmentation,” in
    *CVPR*, 2019.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] T. Luo, K. Mo, Z. Huang, J. Xu, S. Hu, L. Wang, and H. Su, “Learning
    to group: A bottom-up framework for 3D part discovery in unseen categories,” in
    *ICLR*, 2020.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Z. Liu, H. Tang, Y. Lin, and S. Han, “Point-Voxel CNN for efficient 3D
    deep learning,” in *NeurIPS*, 2019.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/56fa4c3416a3cde230779a890f402d92.png) | Yulan
    Guo is currently as associate professor. He received the B.Eng. and Ph.D. degrees
    from National University of Defense Technology (NUDT) in 2008 and 2015, respectively.
    He was a visiting Ph.D. student with the University of Western Australia from
    2011 to 2014\. He worked as a postdoctorial research fellow with the Institute
    of Computing Technology, Chinese Academy of Sciences from 2016 to 2018\. He has
    authored over 90 articles in journals and conferences, such as the IEEE TPAMI
    and IJCV. His current research interests focus on 3D vision, particularly on 3D
    feature learning, 3D modeling, 3D object recognition, and scene understanding.
    Dr. Guo received the ACM China SIGAI Rising Star Award in 2019, Wu-Wenjun Outstanding
    AI Youth Award in 2019, and the CAAI Outstanding Doctoral Dissertation Award in
    2016\. He served as an associate editor for IET Computer Vision and IET Image
    Processing, a guest editor for IEEE TPAMI, and an area chair for CVPR 2021 and
    ICPR 2020. |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/6add7983035710ba793325aa4593ea15.png) | Hanyun
    Wang received his Ph.D. degree from National University of Defense Technology
    in 2015\. He was a visiting Ph.D. student with Xiamen University from 2011 to
    2014\. He has authored over 20 articles in journals and conferences, such as IEEE
    TGRS and IEEE TITS. His research interests include mobile laser scanning data
    analysis and 3D computer vision, especially on 3D object detection and 3D scene
    understanding. He also served as reviewers for many journals, such as IEEE TGRS,
    IEEE GRSL and IET Image Processing. |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/b5cd35805a43cf326820b1cb0e96f828.png) | Qingyong
    Hu received his M.Eng. degree in information and communication engineering from
    the National University of Defense Technology (NUDT) in 2018\. He is currently
    a DPhil candidate in the Department of Computer Science at the University of Oxford.
    His research interests lie in 3D computer vision, large-scale point cloud processing,
    and visual tracking. |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/45d4e206c65033c9ffc068986339cce1.png) | Hao Liu
    received the B.Eng. degree from University of Electronic Science and Technology
    of China (UESTC) in 2016, and M.S. degree from National University of Defense
    Technology (NUDT) in 2018\. He is currently pursuing the Ph.D. degree with the
    School of Electronics and Communication Engineering, Sun Yat-sen University. His
    research interests lie in 3D computer vision and point cloud processing. |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/e4d7306aef9fb655337470ddf5c3cc0c.png) | Li Liu
    received the BSc degree in communication engineering, the MSc degree in photogrammetry
    and remote sensing and the Ph.D. degree in information and communication engineering
    from the National University of Defense Technology (NUDT), China, in 2003, 2005
    and 2012, respectively. She joined the faculty at NUDT in 2012, where she is currently
    an Associate Professor with the College of System Engineering. During her PhD
    study, she spent more than two years as a Visiting Student at the University of
    Waterloo, Canada, from 2008 to 2010\. From 2015 to 2016, she spent ten months
    visiting the Multimedia Laboratory at the Chinese University of Hong Kong. From
    2016.12 to 2018.11, she worked as a senior researcher at the Machine Vision Group
    at the University of Oulu, Finland. She was a cochair of nine International Workshops
    at CVPR, ICCV, and ECCV. She was a guest editor of special issues for IEEE TPAMI
    and IJCV. Her current research interests include computer vision, pattern recognition
    and machine learning. Her papers have currently over 2300+ citations in Google
    Scholar. She currently serves as Associate Editor of the Visual Computer Journal
    and Pattern Recognition Letter. She serves as Area Chair of ICME 2020. |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/9ddd9c5f0c7a71088792a6bbf330be8a.png) | Mohammed
    Bennamoun is Winthrop Professor in the Department of Computer Science and Software
    Engineering at UWA and is a researcher in computer vision, machine/deep learning,
    robotics, and signal/speech processing. He has published 4 books (available on
    Amazon), 1 edited book, 1 Encyclopedia article, 14 book chapters, 120+ journal
    papers, 250+ conference publications, 16 invited & keynote publications. His h-index
    is 50 and his number of citations is 11,000+ (Google Scholar). He was awarded
    65+ competitive research grants, from the Australian Research Council, and numerous
    other Government, UWA and industry Research Grants. He successfully supervised
    26+ PhD students to completion. He won the Best Supervisor of the Year Award at
    QUT (1998), and received award for research supervision at UWA (2008 & 2016) and
    Vice-Chancellor Award for mentorship (2016). He delivered conference tutorials
    at major conferences, including: IEEE Computer Vision and Pattern Recognition
    (CVPR 2016), Interspeech 2014, IEEE International Conference on Acoustics Speech
    and Signal Processing (ICASSP) and European Conference on Computer Vision (ECCV).
    He was also invited to give a Tutorial at an International Summer School on Deep
    Learning (DeepLearn 2017). |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
