- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:39:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:39:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.17522] Deep Learning based Fingerprint Presentation Attack Detection:
    A Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.17522] 基于深度学习的指纹呈现攻击检测：全面综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.17522](https://ar5iv.labs.arxiv.org/html/2305.17522)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.17522](https://ar5iv.labs.arxiv.org/html/2305.17522)
- en: 'Deep Learning based Fingerprint Presentation Attack Detection: A Comprehensive
    Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的指纹呈现攻击检测：全面综述
- en: Hailin Li [hailin.li@ntnu.no](mailto:hailin.li@ntnu.no) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID identifier")  and  Raghavendra Ramachandra [0000-0003-0484-3956](https://orcid.org/0000-0003-0484-3956
    "ORCID identifier") [raghavendra.ramachandra@ntnu.no](mailto:raghavendra.ramachandra@ntnu.no)
    Institute of Information Security and Communication Technology (IIK), Norwegian
    University of Science and Technology (NTNU)GjøvikNorway2816(2018; 20 February
    2007; 12 March 2009; 5 June 2009)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hailin Li [hailin.li@ntnu.no](mailto:hailin.li@ntnu.no) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID identifier") 和 Raghavendra Ramachandra [0000-0003-0484-3956](https://orcid.org/0000-0003-0484-3956
    "ORCID identifier") [raghavendra.ramachandra@ntnu.no](mailto:raghavendra.ramachandra@ntnu.no)
    信息安全与通信技术研究所（IIK），挪威科技大学（NTNU）Gjøvik 挪威 2816（2018; 2007年2月20日; 2009年3月12日; 2009年6月5日）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: The vulnerabilities of fingerprint authentication systems have raised security
    concerns when adapting them to highly secure access-control applications. Therefore,
    Fingerprint Presentation Attack Detection (FPAD) methods are essential for ensuring
    reliable fingerprint authentication. Owing to the lack of generation capacity
    of traditional handcrafted based approaches, deep learning-based FPAD has become
    mainstream and has achieved remarkable performance in the past decade. Existing
    reviews have focused more on hand-cratfed rather than deep learning-based methods,
    which are outdated. To stimulate future research, we will concentrate only on
    recent deep-learning-based FPAD methods. In this paper, we first briefly introduce
    the most common Presentation Attack Instruments (PAIs) and publicly available
    fingerprint Presentation Attack (PA) datasets. We then describe the existing deep-learning
    FPAD by categorizing them into contact, contactless, and smartphone-based approaches.
    Finally, we conclude the paper by discussing the open challenges at the current
    stage and emphasizing the potential future perspective.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹认证系统的脆弱性在将其应用于高度安全的访问控制时引发了安全担忧。因此，指纹呈现攻击检测（FPAD）方法对于确保指纹认证的可靠性至关重要。由于传统手工方法的生成能力不足，基于深度学习的FPAD已成为主流，并在过去十年取得了显著成绩。现有的综述更侧重于手工方法而非基于深度学习的方法，这些综述已经过时。为了刺激未来的研究，我们将仅关注近期的基于深度学习的FPAD方法。在本文中，我们首先简要介绍最常见的呈现攻击工具（PAIs）和公开可用的指纹呈现攻击（PA）数据集。接着，我们通过将现有的深度学习FPAD方法分类为接触式、非接触式和基于智能手机的方法来描述它们。最后，我们通过讨论当前阶段的开放挑战并强调潜在的未来视角来总结本文。
- en: 'Deep learning, fingerprint presentation attack detection^†^†copyright: acmcopyright^†^†journalyear:
    2018^†^†doi: XXXXXXX.XXXXXXX^†^†conference: Make sure to enter the correct conference
    title from your rights confirmation emai; June 03–05, 2018; Woodstock, NY^†^†price:
    15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs: Computing methodologies Biometrics'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，指纹呈现攻击检测^†^†版权：acm版权^†^†期刊年：2018^†^†doi：XXXXXXX.XXXXXXX^†^†会议：请确保从您的版权确认邮件中输入正确的会议标题；2018年6月3日至5日；纽约伍德斯托克^†^†价格：15.00^†^†isbn：978-1-4503-XXXX-X/18/06^†^†ccs：计算方法  生物特征识别
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: Biometric verification is widely deployed in numerous access control applications,
    including border control, forensic science, smartphone access, attendance systems,
    etc. Biometric systems can be designed using either physiological (e.g. fingerprint,
    face, iris), behavioral (e.g. gait, keystroke, voice) or a combination of both.
    Among various physiological traits, the face, iris, and fingerprint have dominated
    the majority of applications because of their reliability and accuracy in performance,
    which can be attributed to the uniqueness of these biometric characteristics.
    However, the fingerprint biometrics is one of the traditional biometric characteristics
    in various application considering the reliability of finger patterns over long
    period of time and the representation and matching of the features that can achieve
    billion fingerprint comparison in a single second with high accuracy (AFI, [2003](#bib.bib2)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生物识别验证在许多访问控制应用中得到广泛部署，包括边境控制、法医科学、智能手机访问、考勤系统等。生物识别系统可以使用生理特征（例如指纹、面部、虹膜）、行为特征（例如步态、打字、声音）或两者的组合来设计。在各种生理特征中，面部、虹膜和指纹因其可靠性和准确性而占据了大多数应用，这可以归因于这些生物识别特征的唯一性。然而，指纹生物识别是各种应用中传统的生物特征之一，考虑到指纹模式在长时间内的可靠性以及特征的表示和匹配，可以在一秒钟内以高准确度实现十亿次指纹比对（AFI，[2003](#bib.bib2)）。
- en: '![Refer to caption](img/093fe68cdb31b3817d8538e3e46f5d59.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/093fe68cdb31b3817d8538e3e46f5d59.png)'
- en: 'Figure 1\. Publications of FPAD in recent years obtained through Google scholar
    search with keywords: “fingerprint spoof detection”, “fingerprint presentation
    attack detection”, and “fingerprint liveness detection”.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 最近几年通过Google Scholar搜索获得的FPAD出版物，关键词包括：“指纹欺骗检测”、“指纹展示攻击检测”和“指纹活体检测”。
- en: 'Fingerprint images are captured using contact and contactless sensing among
    which the contact-based sensing is used predominantly. Contact-based fingerprint
    sensing includes optical, capacitive, and ultrasonic sensors that are available
    as standalone devices and/or integrated devices in smartphones. Optical fingerprint
    scanners are the oldest type of capturing fingerprint patterns. These sensors
    typically have a very high number of diodes per inch to capture the fine details
    of the finger and the optical camera comes with a finite resolution. Capacitive
    sensors use arrays of small capacitor circuits to capture the fingerprint data.
    Since capacitors store electrical charges, connecting them to conductive plates
    on the scanner’s surface allows them to track the details of a fingerprint. Recently,
    ultrasonic sensors have been introduced to scan fingerprints, particularly in
    smartphones. These sensors employ an ultrasonic transmitter that can send the
    pulse against the finger and record the echoes used to construct the fingerprint.
    Further, Scanning for extended periods allows for additional depth data to be
    captured, resulting in a 3D fingerprint. There are two types of contactless sensing:
    (a) use of custom industrial cameras (b) off-the-shelf smartphone cameras used
    to capture finger photos. The use of custom cameras for contactless capture will
    allow designers to include multi-spectral cameras and other sophisticated cameras
    to ensure reliable verification with resilience to presentation attacks. However,
    fingerprint capture using contactless sensing introduces additional challenges,
    such as uncontrolled poses, low-quality fingerprints, environmental noises, and
    degraded performance.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹图像的捕获可以通过接触式和非接触式传感器来完成，其中接触式传感器使用最为广泛。接触式指纹传感包括光学、容性和超声波传感器，这些传感器可以作为独立设备和/或集成在智能手机中使用。光学指纹扫描仪是捕获指纹模式的最古老类型。这些传感器通常每英寸有非常高的二极管数量，以捕捉指纹的细节，并且光学相机具有有限的分辨率。容性传感器使用小型电容电路阵列来捕获指纹数据。由于电容器存储电荷，将它们连接到扫描仪表面的导电板上可以追踪指纹的细节。最近，超声波传感器被引入用于扫描指纹，特别是在智能手机中。这些传感器采用超声波发射器，能够向手指发送脉冲并记录回声以构建指纹。此外，长时间扫描可以捕获额外的深度数据，从而生成3D指纹。非接触式传感有两种类型：（a）使用定制工业相机（b）使用现成的智能手机相机来捕捉手指照片。使用定制相机进行非接触式捕获可以让设计师包括多光谱相机和其他复杂的相机，以确保可靠的验证并对展示攻击具有一定的抗干扰能力。然而，使用非接触式传感进行指纹捕获也带来了额外的挑战，如姿势不受控制、低质量的指纹、环境噪音和性能下降。
- en: The widespread deployment of the Fingerprint Recognition System (FRS) has raised
    concerns about the system being attacked, and the attacker may maliciously gain
    access to the fingerprint system. FRS can be attacked in two ways (a) direct attack
    and (b) indirect attacks. Direct attacks typically target the sensors of the FRS
    such that a Presentation Attack Instrument (PAI) is presented to the sensor to
    gain access. An indirect attack aims to attack the biometric subsystem components
    to modify the functionality. Compared to direct attacks, indirect attacks require
    special skills and knowledge of the biometric system to successfully gain access
    to the FRS. Therefore, direct attacks on FRS have been used extensively in real-life
    scenarios. One real-life example of hacking the FRS using direct attacks, mainly
    on the national population registry, was reported in (Hac, [2022](#bib.bib5)).
    The Aadhaar-enabled Payment System (AePS) is spoofed to withdraw money from various
    victims. Attackers achieved this by collecting the fingerprints of the victims
    from registry papers that have ink-prints of fingerprints. The attackers then
    created a polymer fingerprint attack instrument that was used to withdraw money
    through the AePS. Therefore, the detection of presentation attacks is of paramount
    importance in ensuring the security of the FRS system to facilitate reliable verification.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹识别系统（FRS）的广泛部署引发了对系统被攻击的担忧，攻击者可能恶意获取指纹系统的访问权限。FRS 可以通过两种方式受到攻击：（a）直接攻击和（b）间接攻击。直接攻击通常针对
    FRS 的传感器，利用演示攻击工具（PAI）向传感器展示以获取访问权限。间接攻击旨在攻击生物识别子系统组件以修改其功能。与直接攻击相比，间接攻击需要对生物识别系统有特别的技能和知识才能成功访问
    FRS。因此，直接攻击在现实生活中得到了广泛应用。一个现实生活中的例子是（Hac，[2022](#bib.bib5)）中报告的针对国家人口登记册的 FRS
    直接攻击案例。攻击者通过从登记文件中收集受害者的指纹（这些文件上有指纹的墨迹），然后制作了一个聚合物指纹攻击工具，利用 AePS 从多个受害者那里提取资金。因此，检测演示攻击对于确保
    FRS 系统的安全、实现可靠的验证至关重要。
- en: '| Paper Title / Reference | Year | Deep Learning included | Modality & Hardware
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 论文标题 / 参考文献 | 年份 | 包含深度学习 | 模态与硬件 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Survey on fingerprint liveness detection (Al-Ajlan, [2013](#bib.bib6)) |
    2013 | No | Contact based |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 指纹活体检测调查（Al-Ajlan，[2013](#bib.bib6)） | 2013 | 否 | 接触式 |'
- en: '| Presentation attack detection methods for fingerprint recognition systems:
    a survey (Sousedik and Busch, [2014](#bib.bib111)) | 2014 | No | Contact based
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 指纹识别系统的演示攻击检测方法：一项调查（Sousedik 和 Busch，[2014](#bib.bib111)） | 2014 | 否 | 接触式
    |'
- en: '| A Survey on Antispoofing Schemes for Fingerprint Recognition Systems (Marasco
    and Ross, [2014](#bib.bib75)) | 2014 | No | Contact based |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 关于指纹识别系统的反欺诈方案调查（Marasco 和 Ross，[2014](#bib.bib75)） | 2014 | 否 | 接触式 |'
- en: '| Survey on Fingerprint Spoofing, Detection Techniques and Databases(Kulkarni
    and Patil, [2015](#bib.bib63)) | 2015 | No | Contact based |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 关于指纹欺诈、检测技术和数据库的调查（Kulkarni 和 Patil，[2015](#bib.bib63)） | 2015 | 否 | 接触式
    |'
- en: '| Security and Accuracy of Fingerprint-Based Biometrics: A Review (Yang et al.,
    [2019](#bib.bib122)) | 2019 | Few | Contact based |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 指纹生物识别的安全性和准确性：一项综述（Yang 等，[2019](#bib.bib122)） | 2019 | 少量 | 接触式 |'
- en: '| A Survey on Unknown Presentation Attack Detection for Fingerprint (Singh
    et al., [2021](#bib.bib110)) | 2021 | Few | Contact based, SWIR, LSCI |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 关于指纹未知演示攻击检测的调查（Singh 等，[2021](#bib.bib110)） | 2021 | 少量 | 接触式，短波红外，光谱成像
    |'
- en: '| Robust anti-spoofing techniques for fingerprint liveness detection: A Survey
    (Habib and Selwal, [2021](#bib.bib40)) | 2021 | Few | Contact based |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 针对指纹活体检测的鲁棒反欺诈技术：一项调查（Habib 和 Selwal，[2021](#bib.bib40)） | 2021 | 少量 | 接触式
    |'
- en: '| FinPAD: State-of-the-art of fingerprint presentation attack detection mechanisms,
    taxonomy and future perspectives (Sharma and Selwal, [2021](#bib.bib108)) | 2021
    | Yes,¡30 | Contact based, SWIR |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| FinPAD：指纹演示攻击检测机制的最新进展、分类及未来展望（Sharma 和 Selwal，[2021](#bib.bib108)） | 2021
    | 是，¡30 | 接触式，短波红外 |'
- en: '| Fingerprint Liveness Detection Schemes: A Review on Presentation Attack (Ametefe
    et al., [2022](#bib.bib7)) | 2022 | Yes, ¡30 | Contact based, SWIR, LSCI, smartphone
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 指纹活体检测方案：关于演示攻击的综述（Ametefe 等，[2022](#bib.bib7)） | 2022 | 是，¡30 | 接触式，短波红外，光谱成像，智能手机
    |'
- en: '| Deep Learning for Fingerprint Presentation Attack Detection: A Survey (Ours)
    | 2023 | Comprehensive (¿50) | Contact based, SWIR, LSCI, FTIR, OCT, smartphone
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 指纹演示攻击检测的深度学习：一项调查（我们） | 2023 | 全面（¿50） | 接触式，短波红外，光谱成像，傅里叶变换红外，光学相干层析，智能手机
    |'
- en: Table 1\. A summary of existing surveys in FPAD
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 现有FPAD综述的总结
- en: 'Presentation Attack Detection (PAD), a.k.a anti-spoofing method, has been widely
    investigated for fingerprint biometrics, resulting in several PAD techniques.
    The progress of Fingerprint PAD (FPAD) techniques is shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Deep Learning based Fingerprint Presentation Attack
    Detection: A Comprehensive Survey") which illustrates the increased interest of
    researchers in developing FPAD techniques. Early works on developing FPAD techniques
    were based on hand-crafted features, in which texture-based features (Local Binary
    Patterns (LBP), Binarized Statistical Image Features (BSIF), etc.) were widely
    employed in designing FPAD. However, owing to the limitations of the texture features
    to generalize across different types of PAIs, researchers have started to investigate
    FPAD using deep learning. Further, the importance of FPAD also resulted in the
    development of the competition platform LivDet (Liv, [2022](#bib.bib4)) which
    allows participants to submit FPAD algorithms for independent evaluation.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '呈现攻击检测（PAD），也称为防伪方法，在指纹生物识别中得到了广泛研究，产生了几种PAD技术。指纹PAD（FPAD）技术的进展如图[1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Deep Learning based Fingerprint Presentation Attack
    Detection: A Comprehensive Survey")所示，这显示了研究人员在开发FPAD技术方面的兴趣增加。早期的FPAD技术开发工作基于手工设计的特征，其中基于纹理的特征（局部二值模式（LBP）、二值统计图像特征（BSIF）等）被广泛用于FPAD的设计。然而，由于纹理特征在不同类型的PAI中泛化的局限性，研究人员开始探索使用深度学习进行FPAD。此外，FPAD的重要性还促使了竞赛平台LivDet（Liv,
    [2022](#bib.bib4)）的发展，该平台允许参与者提交FPAD算法进行独立评估。'
- en: 'The exponential growth of the FPAD algorithms over the years has resulted in
    several survey papers, as listed in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ Deep Learning based Fingerprint Presentation Attack Detection: A Comprehensive
    Survey"). However, existing surveys are limited to non-deep learning approaches
    and thus do not consider the recent progress in which deep learning approaches
    are prominent. Hence, in this paper, we are motivated to present a comprehensive
    review of deep learning-based fingerprint presentation attack detection, in which
    we discuss recent progress, competition, performance evaluation metrics, and future
    work. The main contributions of this study are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '随着**FPAD**算法多年来的指数级增长，出现了几篇综述论文，如表[1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣
    Deep Learning based Fingerprint Presentation Attack Detection: A Comprehensive
    Survey")所示。然而，现有的综述论文限于非深度学习的方法，因此未考虑到深度学习方法的最新进展。因此，本文旨在全面回顾基于深度学习的指纹呈现攻击检测，其中讨论了最近的进展、竞争、性能评估指标和未来的工作。本文的主要贡献如下：'
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present the comprehensive survey on deep-learning based FPAD techniques for
    both contact and contact less fingerprint. Comprehensive literature survey together
    with the taxonomy and compare these approaches on the different attributes of
    design.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了基于深度学习的FPAD技术的全面综述，涵盖了接触式和非接触式指纹。综述包括文献综述、分类法，并对不同设计属性的这些方法进行了比较。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present a comprehensive survey on the PAI that are widely employed in both
    contact and contactless fingerprint biometrics.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对广泛应用于接触式和非接触式指纹生物识别的**PAI**进行了全面调查。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We outline the main challenges and the potential future work for reliable fingerprint
    detection.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们概述了可靠指纹检测的主要挑战和潜在未来工作。
- en: 'The rest of the paper is organized as follows. Section [2](#S2 "2\. Fingerprint
    Recognition Systems (FRS) ‣ Deep Learning based Fingerprint Presentation Attack
    Detection: A Comprehensive Survey") presents the pipeline of a fingerprint recognition
    system and indicates how the FPAD system can be adjusted in the overall system.
    Section [3](#S3 "3\. Fingerprint Presentation Attack Instrument (PAI) ‣ Deep Learning
    based Fingerprint Presentation Attack Detection: A Comprehensive Survey") provides
    a comprehensive report on the different types of PAIs for both contact and contactless
    fingerprints. Section [4](#S4 "4\. Existing Datasets for Fingerprint PAD ‣ Deep
    Learning based Fingerprint Presentation Attack Detection: A Comprehensive Survey")
    introduces the most common publicly available FPAD dataset. Section [5](#S5 "5\.
    Deep learning based Fingerprint presentation attack detection ‣ Deep Learning
    based Fingerprint Presentation Attack Detection: A Comprehensive Survey") presents
    a comprehensive survey of fingerprint presentation attack detection based on deep
    learning. Then, Section [6](#S6 "6\. Performance Evaluation Metrics ‣ Deep Learning
    based Fingerprint Presentation Attack Detection: A Comprehensive Survey") includes
    the most common PAD performance evaluation metrics from the ISO standard and the
    LivDet competition. Section [7](#S7 "7\. Future Work ‣ Deep Learning based Fingerprint
    Presentation Attack Detection: A Comprehensive Survey") discusses the open challenges
    and the potential future research directions. Finally, we conclude this review
    in section [8](#S8 "8\. Conclusion ‣ Deep Learning based Fingerprint Presentation
    Attack Detection: A Comprehensive Survey").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织如下。第[2](#S2 "2\. Fingerprint Recognition Systems (FRS) ‣ Deep Learning
    based Fingerprint Presentation Attack Detection: A Comprehensive Survey")节展示了指纹识别系统的流程，并指出了FPAD系统如何在整体系统中进行调整。第[3](#S3
    "3\. Fingerprint Presentation Attack Instrument (PAI) ‣ Deep Learning based Fingerprint
    Presentation Attack Detection: A Comprehensive Survey")节提供了有关接触式和非接触式指纹的各种PAI的综合报告。第[4](#S4
    "4\. Existing Datasets for Fingerprint PAD ‣ Deep Learning based Fingerprint Presentation
    Attack Detection: A Comprehensive Survey")节介绍了最常见的公开FPAD数据集。第[5](#S5 "5\. Deep
    learning based Fingerprint presentation attack detection ‣ Deep Learning based
    Fingerprint Presentation Attack Detection: A Comprehensive Survey")节展示了基于深度学习的指纹展示攻击检测的全面调查。然后，第[6](#S6
    "6\. Performance Evaluation Metrics ‣ Deep Learning based Fingerprint Presentation
    Attack Detection: A Comprehensive Survey")节包括ISO标准和LivDet竞赛中最常见的PAD性能评估指标。第[7](#S7
    "7\. Future Work ‣ Deep Learning based Fingerprint Presentation Attack Detection:
    A Comprehensive Survey")节讨论了开放挑战和潜在的未来研究方向。最后，我们在第[8](#S8 "8\. Conclusion ‣ Deep
    Learning based Fingerprint Presentation Attack Detection: A Comprehensive Survey")节总结了这篇综述。'
- en: 2\. Fingerprint Recognition Systems (FRS)
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 指纹识别系统 (FRS)
- en: '![Refer to caption](img/150fdca0bcd88ded4db1011e9b301f48.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/150fdca0bcd88ded4db1011e9b301f48.png)'
- en: Figure 2\. Block diagram of fingerprint verification system with PAD
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 带有PAD的指纹验证系统框图
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2\. Fingerprint Recognition Systems (FRS) ‣ Deep
    Learning based Fingerprint Presentation Attack Detection: A Comprehensive Survey")
    shows the block diagram of the fingerprint verification system. Given a fingerprint
    image, the signal processing unit performs various operations, including data
    preprocessing, quality checking, feature extraction, and template creation. Fingerprint
    feature extraction techniques can be level 1 (pattern), level 2 (minutiae points),
    and level 3 (pores and ridge shape). The common and successful fingerprint features
    that are widely deployed in commercial applications are based on Level 2 features.
    Final decisions are made using a comparator that can compare the enrolled fingerprint
    with the probe fingerprint image to output the comparison score. The comparison
    score was then compared with the preset threshold to make the final decision.
    The PAD system can be integrated into a fingerprint recognition system, either
    parallel or serial to the fingerprint comparator. In a parallel system, both the
    PAD and comparator perform processing independently to obtain the decisions that
    are combined to make the final decision. However, in a serial system, the PAD
    and fingerprint comparator work in a sequential manner in which the fingerprint
    is first processed through the PAD unit. If the output of the PAD unit indicates
    a bona fide, then the fingerprint template is passed through the comparator to
    make the final decision. For more detailed information on the fingerprint systems,
    readers can refer to (Maltoni et al., [2009](#bib.bib74)) (Maltoni et al., [2022](#bib.bib73)).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2](#S2.F2 "Figure 2 ‣ 2\. Fingerprint Recognition Systems (FRS) ‣ Deep Learning
    based Fingerprint Presentation Attack Detection: A Comprehensive Survey") 显示了指纹验证系统的框图。给定一张指纹图像，信号处理单元执行各种操作，包括数据预处理、质量检查、特征提取和模板创建。指纹特征提取技术可以分为第
    1 级（模式）、第 2 级（细节点）和第 3 级（毛孔和脊形状）。在商业应用中广泛部署的成功指纹特征通常基于第 2 级特征。最终决策使用比较器，该比较器可以将登记的指纹与探测指纹图像进行比较，以输出比较分数。然后将比较分数与预设阈值进行比较，以做出最终决策。PAD
    系统可以集成到指纹识别系统中，与指纹比较器并行或串行。在并行系统中，PAD 和比较器独立执行处理，以获得结合后的最终决策。然而，在串行系统中，PAD 和指纹比较器以顺序方式工作，指纹首先经过
    PAD 单元处理。如果 PAD 单元的输出表明是真实的，则指纹模板将通过比较器做出最终决策。有关指纹系统的更多详细信息，请读者参考 (Maltoni et
    al., [2009](#bib.bib74)) (Maltoni et al., [2022](#bib.bib73))。'
- en: 3\. Fingerprint Presentation Attack Instrument (PAI)
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 指纹呈现攻击工具（PAI）
- en: 'The success of a presentation attack on fingerprint systems depends on the
    high-quality generation of PAI. Figure [3](#S3.F3 "Figure 3 ‣ 3\. Fingerprint
    Presentation Attack Instrument (PAI) ‣ Deep Learning based Fingerprint Presentation
    Attack Detection: A Comprehensive Survey") shows the taxonomy of the existing
    PAI types commonly studied in the fingerprint literature (Maltoni et al., [2009](#bib.bib74)).
    Available PAI can be broadly categorized into two main types: (a) digital generation
    and (b) artificial fabrication. Digital attack generation is performed using a
    computer program in which the attacks are synthetically generated using deep learning
    methods (Karras et al., [2019](#bib.bib55)) or the custom algorithms (Bontrager
    et al., [2018](#bib.bib11)) (Gajawada et al., [2019](#bib.bib27)) (Engelsma et al.,
    [2022](#bib.bib22)) (Kim et al., [2019](#bib.bib56)). Digital attacks can be used
    as injection attacks and/or to generate physical artifacts that can be used as
    presentation attacks. Examples of digital attacks include synthetic fingerprint
    attacks (Grosz and Jain, [2022](#bib.bib38)), master print (Roy et al., [2018](#bib.bib101))
    and morphing attacks (Makrushin et al., [2021](#bib.bib72)) (Ferrara et al., [2016](#bib.bib25)).
    The artificial fabrication method uses the target fingerprint impression top to
    generate a physical artifact that can be used as a presentation attack. Artificial
    fabrication methods can be broadly categorized into 2D/3D printing and gummy fingers.
    The 2D/3D print, in which the physical artifacts are printed fingerprints, can
    be used as presentation attacks (Espinoza et al., [2011](#bib.bib24)) (Kanich
    et al., [2018](#bib.bib53)) (Prabakaran and Pillay, [2020](#bib.bib95)). The gummy
    fingerprints are made of various materials (Gelatin, Play-doh, Silicone, etc.)
    that may contain a specific fingerprint template and have shown potential risks
    to the FRS (Chugh and Jain, [2019](#bib.bib18)). Table [2](#S3.T2 "Table 2 ‣ 3\.
    Fingerprint Presentation Attack Instrument (PAI) ‣ Deep Learning based Fingerprint
    Presentation Attack Detection: A Comprehensive Survey") lists the different features
    of the Digital and Artificial fabrication types of PAI generation.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 展示攻击成功的关键在于高质量的PAI生成。图[3](#S3.F3 "图3 ‣ 3\. 指纹展示攻击工具（PAI） ‣ 基于深度学习的指纹展示攻击检测：综合调查")展示了在指纹文献中常见的现有PAI类型的分类（Maltoni等，[2009](#bib.bib74)）。现有的PAI可以大致分为两种主要类型：（a）数字生成和（b）人工制作。数字攻击生成是使用计算机程序执行的，其中攻击是通过深度学习方法（Karras等，[2019](#bib.bib55)）或自定义算法（Bontrager等，[2018](#bib.bib11)）合成生成的（Gajawada等，[2019](#bib.bib27)）（Engelsma等，[2022](#bib.bib22)）（Kim等，[2019](#bib.bib56)）。数字攻击可以作为注入攻击使用，和/或生成可以作为展示攻击使用的物理伪造品。数字攻击的例子包括合成指纹攻击（Grosz和Jain，[2022](#bib.bib38)），主打印（Roy等，[2018](#bib.bib101)）和变形攻击（Makrushin等，[2021](#bib.bib72)）（Ferrara等，[2016](#bib.bib25)）。人工制作方法使用目标指纹印模生成可以作为展示攻击使用的物理伪造品。人工制作方法可以大致分为2D/3D打印和胶指。2D/3D打印中，物理伪造品是打印出的指纹，可以作为展示攻击使用（Espinoza等，[2011](#bib.bib24)）（Kanich等，[2018](#bib.bib53)）（Prabakaran和Pillay，[2020](#bib.bib95)）。胶指由各种材料（明胶、游戏泥、硅胶等）制成，这些材料可能包含特定的指纹模板，并且对FRS（Chugh和Jain，[2019](#bib.bib18)）显示出潜在风险。表[2](#S3.T2
    "表2 ‣ 3\. 指纹展示攻击工具（PAI） ‣ 基于深度学习的指纹展示攻击检测：综合调查")列出了数字和人工制作类型PAI生成的不同特征。
- en: '![Refer to caption](img/29f2d213d1957b92b3307da369c23b22.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/29f2d213d1957b92b3307da369c23b22.png)'
- en: Figure 3\. Taxonomy of fingerprint Presentation Attack Instrument (PAI)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 指纹展示攻击工具（PAI）的分类
- en: '| Digital PAI | Artificial Fabrication PAI |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 数字生成的PAI | 人工制作的PAI |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Generate high-quality attack instrument | Generate near high-quality attack
    instrument |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 生成高质量攻击工具 | 生成接近高质量的攻击工具 |'
- en: '| High attack potential | Moderate attack potential |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 高攻击潜力 | 中等攻击潜力 |'
- en: '| Able to attack multiple identities in single | Mostly designed to attack
    single identity |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 能够在单一攻击中针对多个身份 | 大多设计用于攻击单一身份 |'
- en: '| Requires more technical knowledge | No need of more technical knowledge |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 需要更多的技术知识 | 不需要更多的技术知识 |'
- en: '| High-computation cost | Low computation cost |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 高计算成本 | 低计算成本 |'
- en: '| Low-cost generation | High-cost generation |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 低成本生成 | 高成本生成 |'
- en: '| Very challenging to detect | Easy to detect particularly with the multi-spectral
    sensors |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 检测非常具有挑战性 | 特别是使用多光谱传感器容易检测 |'
- en: Table 2\. Comparison of different PAI generation techniques
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 不同PAI生成技术的比较
- en: '![Refer to caption](img/127e278fe32708ab0b23fb732eace8d4.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/127e278fe32708ab0b23fb732eace8d4.png)'
- en: Figure 4\. Examples of PAIs (taken from (Chugh and Jain, [2019](#bib.bib18)))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. PAIs 示例（摘自 (Chugh 和 Jain, [2019](#bib.bib18)））
- en: 'The common PAI examples are shown in figure [4](#S3.F4 "Figure 4 ‣ 3\. Fingerprint
    Presentation Attack Instrument (PAI) ‣ Deep Learning based Fingerprint Presentation
    Attack Detection: A Comprehensive Survey"). In accordance with the experiments
    under the fingerprint verification application, the digital synthetic fingerprint
    shows the high attack potential against the FRS.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的PAI示例如图[4](#S3.F4 "图 4 ‣ 3\. 指纹展示攻击工具 (PAI) ‣ 基于深度学习的指纹展示攻击检测：综合调查")所示。根据指纹验证应用下的实验，数字合成指纹显示出对FRS的高攻击潜力。
- en: 4\. Existing Datasets for Fingerprint PAD
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 指纹PAD的现有数据集
- en: 'A large-scale dataset is required to achieve better results when using deep-learning-based
    methods during both the training and testing phases. In this section, we summarize
    the publicly available FPAD datasets containing the data amount, subject numbers,
    bona fide/attack amount, and used PAI species. The most common datasets were from
    the Fingerprint Liveness Detection Competition (LivDet) series from 2009 to 2021\.
    Readers can refer to a review of the LivDet series (Micheletto et al., [2022](#bib.bib81))
    for detailed information on the LivDet challenge. In the first edition of the
    LivDet 2009 dataset, three optical scanners, Crossmatch, Identix, and Biometrika,
    were included with gelatin, silicone, and play-doh as spoof materials. In LivDet
    2011, four sub-datasets were based on four different optical scanners: Biometrika,
    Digital Persona, ItalData, and Sagem. In addition, more PAIs such as Silgum, Ecoflex,
    Wood Glue, and Latex are included than LivDet 2009\. The first non-optical scanner
    was introduced in the competition, in which the spoof materials were replenished
    with body doubling and modasil. In LivDet 2015, with the raised concern of FPAD
    methods against unseen attacks, the competition included some unknown materials
    for evaluation in the test dataset. Additionally, liquid Ecoflex and a two-component
    silicone rubber(RTV) are included in the Green Bit, Biometrika, and Digital Persona
    datasets, whereas the silicone rubber OOMOO is included in the Crossmatch dataset.
    In LivDet 2017, the competition focused on the impact of the FPAD based on user-specific
    effects and operator skills in fabricating replicas. In the training set, the
    spoofing is made of wood glue, Ecoflex, and body double, whereas gelatin, latex,
    and liquid Ecoflex compose the test set in order to stimulate the completely unseen
    scenario. Furthermore, two sets of people with different manufacturing spoofing
    abilities were involved in spoofing. In contrast to the previous editions, some
    subjects were included in the training and testing datasets to explore the impact
    of user-specific effects. LivDet 2019 utilized the same scanners as in the previous
    edition, but presented multi-material combinations, both of different consistency
    and of different nature for the first time. LivDet 2021 only consists of two scanners,
    GreenBit and Dermalog, in which the consensual approach and the new pseudo-consensual
    method, ScreenSpoof, are included. Novel materials RProFast, Elmers glue, gls20,
    and RPro30\. were chosen for the fakes. In the most recent LivDet 2023, four datasets
    correspond to two known sensors, Green Bit and Dermalog, and two unknown capture
    devices.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用基于深度学习的方法进行训练和测试阶段时，需要一个大规模的数据集以获得更好的结果。在这一部分中，我们总结了公开可用的 FPAD 数据集，包括数据量、样本数量、真实/攻击样本量以及使用的PAI物种。最常见的数据集来自2009年至2021年的指纹活体检测竞赛（LivDet）系列。读者可以参考
    LivDet 系列的综述（Micheletto et al., [2022](#bib.bib81)）获取有关 LivDet 挑战的详细信息。在 LivDet
    2009 数据集的第一版中，包括了三个光学扫描仪：Crossmatch、Identix 和 Biometrika，以及明胶、硅胶和粘土作为欺骗材料。在 LivDet
    2011 中，四个子数据集基于四种不同的光学扫描仪：Biometrika、Digital Persona、ItalData 和 Sagem。此外，还包括了比
    LivDet 2009 更多的 PAIs，如 Silgum、Ecoflex、木胶和乳胶。竞赛中引入了首个非光学扫描仪，欺骗材料被补充了身体替代和 Modasil。在
    LivDet 2015 中，由于对 FPAD 方法对未知攻击的担忧增加，竞赛在测试数据集中包含了一些未知材料以供评估。此外，液体 Ecoflex 和两种成分的硅胶橡胶（RTV）被包括在
    Green Bit、Biometrika 和 Digital Persona 数据集中，而硅胶橡胶 OOMOO 则包含在 Crossmatch 数据集中。在
    LivDet 2017 中，竞赛专注于 FPAD 基于用户特定效应和操作员技能对复制品的影响。在训练集中，欺骗材料由木胶、Ecoflex 和身体替代品组成，而明胶、乳胶和液体
    Ecoflex 组成了测试集，以刺激完全未见的场景。此外，涉及了两组具有不同制造欺骗能力的人。与之前的版本相比，一些受试者被纳入训练和测试数据集中，以探讨用户特定效应的影响。LivDet
    2019 使用了与前一版相同的扫描仪，但首次呈现了多种材料组合，既有不同的一致性，也有不同的性质。LivDet 2021 仅包含两个扫描仪，GreenBit
    和 Dermalog，其中包括了共识方法和新的伪共识方法 ScreenSpoof。新颖的欺骗材料包括 RProFast、Elmers 胶水、gls20 和
    RPro30。最近的 LivDet 2023 包含四个数据集，对应于两个已知的传感器，Green Bit 和 Dermalog，以及两个未知的捕捉设备。
- en: 'Besides LivDet datasets, there are also many proposed methods that include
    custom datasets which is publicly available. In Tsinghua dataset (Jia et al.,
    [2007](#bib.bib49)), the Capacitive device Veridicom is utilized as the capture
    device and the attack samples are created using Silicone. The samples are acquired
    from 15 volunteers from Tsinghua University. Two fingers are captured for each
    participant and ten image sequences are recorded for each real finger. 47 fake
    finger are manufactured with ten image sequences for each finger. BSL dataset
    (Antonelli et al., [2006](#bib.bib9)) are collected at the Biometric System Laboratory
    of the University of Bologna which contains more subjects and samples using four
    different PAIs. For each of 45 volunteers, the thumb and forefinger of the right
    hand with 10 image sequences are recorded. Instead of making whole 3D fingers,
    the obtained manufactures are focused on the fingertip area. ATVS dataset utilize
    slilicone and play-Doh as materials and involved 17 subjects. Two scanners are
    used in Precise Biometrics dataset comprised of 100 subjects and 500 attack samples
    produced using five different materials. In Precise Biometrics Spoof-Kit(PBSKD)
    (Chugh et al., [2018](#bib.bib17)), 900 spoof fingerprint images are fabricated
    using 10 different types of spoof materials. Another dataset MSU-FPAD is also
    proposed in (Chugh et al., [2018](#bib.bib17)), there are up to 9000 live samples
    and 10500 spoof samples included using these two readers and 4 different spoof
    fabrication materials. Recently, Kolberg et.al (Kolberg et al., [2023](#bib.bib60))
    publish a new dataset COLFISPOOF based on contactless fingerphoto which contains
    72 different PAI species. All the fingerprints for the PAIs are generated using
    fingerprint synthetic algorithms. The details of each dataset are demonstrated
    in Table [3](#S4.T3 "Table 3 ‣ 4\. Existing Datasets for Fingerprint PAD ‣ Deep
    Learning based Fingerprint Presentation Attack Detection: A Comprehensive Survey").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '除了LivDet数据集，还有许多提出的方法包含公开的自定义数据集。在Tsinghua数据集（Jia et al., [2007](#bib.bib49)）中，使用了电容设备Veridicom作为采集设备，攻击样本使用硅胶制成。样本来自15名清华大学的志愿者。每位参与者采集了两个手指，并记录了每个真实手指的十个图像序列。制造了47个假手指，每个手指有十个图像序列。BSL数据集（Antonelli
    et al., [2006](#bib.bib9)）在博洛尼亚大学生物特征系统实验室收集，包含更多的受试者和样本，使用了四种不同的PAI。对于45名志愿者中的每一位，记录了右手的大拇指和食指的10个图像序列。制作过程中没有制作整个3D手指，而是专注于指尖区域。ATVS数据集使用了硅胶和Play-Doh作为材料，涉及17名受试者。Precise
    Biometrics数据集中使用了两台扫描仪，包括100名受试者和500个攻击样本，使用了五种不同的材料。在Precise Biometrics Spoof-Kit(PBSKD)（Chugh
    et al., [2018](#bib.bib17)）中，使用10种不同类型的伪造材料制造了900个伪造指纹图像。另一个数据集MSU-FPAD也在（Chugh
    et al., [2018](#bib.bib17)）中提出，包含多达9000个真实样本和10500个伪造样本，使用了这两台读取器和4种不同的伪造材料。最近，Kolberg
    et.al（Kolberg et al., [2023](#bib.bib60)）发布了一个新的数据集COLFISPOOF，基于非接触式指纹照片，包含72种不同的PAI种类。所有PAI的指纹都是使用指纹合成算法生成的。每个数据集的详细信息在表[3](#S4.T3
    "Table 3 ‣ 4\. Existing Datasets for Fingerprint PAD ‣ Deep Learning based Fingerprint
    Presentation Attack Detection: A Comprehensive Survey")中展示。'
- en: '| Dataset | No. of Subjects | Bona fide samples | Attack samples | PAI type
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 受试者数量 | 真正样本 | 攻击样本 | PAI类型 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Tsinghua (Jia et al., [2007](#bib.bib49)) | 15 | 300 | 470 | Silicone |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Tsinghua (Jia et al., [2007](#bib.bib49)) | 15 | 300 | 470 | 硅胶 |'
- en: '| BSL (Antonelli et al., [2006](#bib.bib9)) | 45 | 900 | 400 | Silicone, gelatin,
    latex, wood glue |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| BSL (Antonelli et al., [2006](#bib.bib9)) | 45 | 900 | 400 | 硅胶、明胶、乳胶、木胶
    |'
- en: '| LivDet 2009 (Marcialis et al., [2009](#bib.bib78)) | 254 | 5500 | 5500 |
    Gelatine, Silicone and Play-Doh |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| LivDet 2009 (Marcialis et al., [2009](#bib.bib78)) | 254 | 5500 | 5500 |
    明胶、硅胶和Play-Doh |'
- en: '| LivDet 2011 (Yambay et al., [2012](#bib.bib121)) | 200 | 3000 | 3000 | Gelatine,
    Silgum, Ecoflex, Wood Glue, Play-Doh, Silicone and Latex |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| LivDet 2011 (Yambay et al., [2012](#bib.bib121)) | 200 | 3000 | 3000 | 明胶、Silgum、Ecoflex、木胶、Play-Doh、硅胶和乳胶
    |'
- en: '| LivDet 2013 (Ghiani et al., [2013b](#bib.bib33)) | 225 | 8000 | 8000 | Gelatine,
    Wood Glue, Latex, Ecoflex and Modasil |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| LivDet 2013 (Ghiani et al., [2013b](#bib.bib33)) | 225 | 8000 | 8000 | 明胶、木胶、乳胶、Ecoflex和Modasil
    |'
- en: '| LivDet 2015 (Mura et al., [2015](#bib.bib83)) | 100 | 4500 | 5948 | Body
    Double, EcoFlex, Play-Doh, Gelatine, Latex, Wood Glue and Liquid Ecoflex |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| LivDet 2015 (Mura et al., [2015](#bib.bib83)) | 100 | 4500 | 5948 | Body
    Double、EcoFlex、Play-Doh、明胶、乳胶、木胶和液体Ecoflex |'
- en: '| LivDet 2017 (Mura et al., [2018](#bib.bib84)) | 150 | 8099 | 9685 | Gelatine,
    Wood Glue, Latex, Ecoflex, Body Double and Liquid Ecoflex |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LivDet 2017 (Mura et al., [2018](#bib.bib84)) | 150 | 8099 | 9685 | 明胶、木胶、乳胶、Ecoflex、Body
    Double和液体Ecoflex |'
- en: '| LivDet 2019 (Orrù et al., [2019](#bib.bib89)) | NA | 6029 | 6936 | Gelatine,
    Wood Glue, Latex, Ecoflex, Body Double and Liquid Ecoflex |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| LivDet 2019 (Orrù et al., [2019](#bib.bib89)) | NA | 6029 | 6936 | 明胶、木胶、乳胶、Ecoflex、Body
    Double 和 Liquid Ecoflex |'
- en: '| LivDet 2021 (Casula et al., [2021a](#bib.bib13)) | 66 | 10700 | 11740 | GLS20,
    Body Double, Mix 1, ElmersGlue, and RFast30 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| LivDet 2021 (Casula et al., [2021a](#bib.bib13)) | 66 | 10700 | 11740 | GLS20、Body
    Double、Mix 1、ElmersGlue 和 RFast30 |'
- en: '| LivDet 2023 | 25 | 5000 | 3000 | NA |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| LivDet 2023 | 25 | 5000 | 3000 | NA |'
- en: '| ATVS-FFp (Galbally et al., [2011](#bib.bib28)) | 17 | 816 | 816 | Silicone,
    Play-Doh |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| ATVS-FFp (Galbally et al., [2011](#bib.bib28)) | 17 | 816 | 816 | 硅胶、Play-Doh
    |'
- en: '| Precise Biometrics Spoof-Kit (Chugh et al., [2018](#bib.bib17)) | NA | 1000
    | 900 | Ecoflex, Gelatine, Latex, Crayola, Wood glue, 2D print |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Precise Biometrics Spoof-Kit (Chugh et al., [2018](#bib.bib17)) | NA | 1000
    | 900 | Ecoflex、明胶、乳胶、Crayola、木胶、2D打印 |'
- en: '| MSU-FPAD (Chugh et al., [2018](#bib.bib17)) | NA | 9000 | 10500 | Ecoflex,
    2D Print-Matte Paper, Play-Doh and 2D Print (Transparency) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| MSU-FPAD (Chugh et al., [2018](#bib.bib17)) | NA | 9000 | 10500 | Ecoflex，2D
    Print-Matte Paper，Play-Doh 和 2D Print (Transparency) |'
- en: '| COLFISPOOF (Kolberg et al., [2023](#bib.bib60)) | NA | NA | 7200 | Print
    out and Replay |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| COLFISPOOF (Kolberg et al., [2023](#bib.bib60)) | NA | NA | 7200 | 打印和重播
    |'
- en: Table 3\. Most utilized and publicly available datasets
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 使用最广泛的公共数据集
- en: 5\. Deep learning based Fingerprint presentation attack detection
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 基于深度学习的指纹呈现攻击检测
- en: In this section, we discuss the Fingerprint PAD (FPAD) algorithms that are presented
    for contact, contactless, and smartphone-based fingerprint sensing. The FPAD aims
    to detect whether the given fingerprint image is bona fide or a presentation attack.
    FPAD techniques can be broadly classified into two main categories (a) Hardware-based
    approaches and (b) Software-based approaches. The hardware-based approaches are
    designed to extract the liveness cues that require explicit (or dedicated) sensors
    to be integrated into the conventional contact fingerprint biometric system. Some
    of the widely used liveness measures include the capture of blood flow (Drahansky
    et al., [2006](#bib.bib21)), Electro-tactile (Yau et al., [2008](#bib.bib123)),
    and pulse Oximetry (Reddy et al., [2008](#bib.bib99)) to detect whether the fingerprint
    is live or not. Over the past few years, new expensive sensors like optical coherence
    tomography (OCT) have evolved. OCT is an imaging technique that allows some of
    the subsurface characteristics of the skin to be imaged and extracts relevant
    features of multi-layered tissues up to a maximum depth of 3 mm (Cheng and Larin,
    [2007](#bib.bib15)) (Bossen et al., [2010](#bib.bib12)) (Liu and Buma, [2010](#bib.bib69)).
    Further, several contactless fingerprint capture devices like multi-spectral and
    3D capture devices can also inherently capture the signature of liveness.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了用于接触、非接触和基于智能手机的指纹识别的指纹PAD（FPAD）算法。FPAD旨在检测给定的指纹图像是否真实或是呈现攻击。FPAD技术大致可分为两大类：（a）基于硬件的方法和（b）基于软件的方法。基于硬件的方法旨在提取活体线索，这些线索需要将显式（或专用）传感器集成到传统的接触指纹生物识别系统中。一些广泛使用的活体检测方法包括血流捕捉（Drahansky
    et al., [2006](#bib.bib21)）、电触觉（Yau et al., [2008](#bib.bib123)）和脉搏血氧饱和度（Reddy
    et al., [2008](#bib.bib99)），用于检测指纹是否真实。近年来，像光学相干断层扫描（OCT）这样的新型昂贵传感器也得到了发展。OCT是一种成像技术，可以成像皮肤的一些表面下特征，并提取多层组织的相关特征，最大深度为3毫米（Cheng
    and Larin, [2007](#bib.bib15)）（Bossen et al., [2010](#bib.bib12)）（Liu and Buma,
    [2010](#bib.bib69)）。此外，像多光谱和3D捕捉设备这样的非接触式指纹捕捉设备也可以固有地捕捉活体的特征。
- en: 'The Software-based approach refers to algorithms to detect whether the presented
    fingerprint is bona fide or a presentation attack, irrespective of the capture
    device. Software-based FPAD can be further divided broadly into two types handcrafted-based
    and deep learning. Handcrafted-based method refers to the conventional feature
    representation that includes techniques used to extract features like gradients,
    texture, micro-textures, etc. The handcrafted-based approach is applied to contact-based
    fingerprint images mostly. From the fingerprint images, micro-textural features
    that can be computed using Scale-invariant feature transform (SIFT) (Lowe, [1999](#bib.bib70)),
    Binarized Statistical Image Features (BSIF) (Kannala and Rahtu, [2012](#bib.bib54)),
    Local Binary Pattern (LBP) (Guo et al., [2010](#bib.bib39)), Local Phase Quantization
    (LPQ) (Ojansivu and Heikkilä, [2008](#bib.bib88)). Many hand-crafted based approaches
    like (Ghiani et al., [2012](#bib.bib32)) (Ghiani et al., [2013a](#bib.bib31))
    (Gragnaniello et al., [2013](#bib.bib36)) (Zhang et al., [2014](#bib.bib127))
    (Gragnaniello et al., [2015](#bib.bib37)) achieved promised performance in recent
    years. However, handcrafted-based approaches may have a limitation in that the
    extraction process becomes difficult due to variations in the acquired fingerprint
    image quality. These challenges are tackled using Deep Neural Network (DNN) terms
    such as deep learning, which hierarchically learns deep-level features from the
    images. With the rapid development of graphical processing units training a large-scale
    model has become achievable. In 2012, Krizhevsky et al. (Krizhevsky et al., [2012](#bib.bib62))
    trained a network to classify the 1.2 million high-resolution images in the ImageNet
    LSVRC-2010 contest into 1000 different classes which achieved a huge success,
    which started a revolution in the computer vision field. Afterward, training a
    deep convolutional neural network (CNN) has dominated the image classification
    tasks in various applications. Figure [5](#S5.F5 "Figure 5 ‣ 5\. Deep learning
    based Fingerprint presentation attack detection ‣ Deep Learning based Fingerprint
    Presentation Attack Detection: A Comprehensive Survey") shows the taxonomy of
    the deep learning based algorithms that are developed for fingerprint PAD on three
    different types of sensing that are discussed below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基于软件的方法指的是检测所呈现的指纹是否是真实的或是伪造攻击的算法，与捕捉设备无关。基于软件的指纹攻击检测（FPAD）可以大致分为两种类型：手工特征方法和深度学习。手工特征方法指的是包括用于提取特征的技术，如梯度、纹理、微纹理等的传统特征表示。手工特征方法主要应用于接触式指纹图像。从指纹图像中，可以使用尺度不变特征变换（SIFT）（Lowe，[1999](#bib.bib70)），二值化统计图像特征（BSIF）（Kannala
    和 Rahtu，[2012](#bib.bib54)），局部二值模式（LBP）（Guo 等，[2010](#bib.bib39)），局部相位量化（LPQ）（Ojansivu
    和 Heikkilä，[2008](#bib.bib88)）计算微纹理特征。许多手工特征方法如（Ghiani 等，[2012](#bib.bib32)）（Ghiani
    等，[2013a](#bib.bib31)）（Gragnaniello 等，[2013](#bib.bib36)）（Zhang 等，[2014](#bib.bib127)）（Gragnaniello
    等，[2015](#bib.bib37)）近年来取得了令人满意的性能。然而，手工特征方法可能存在一个限制，即由于获取的指纹图像质量的变化，提取过程变得困难。这些挑战通过使用深度神经网络（DNN）等术语进行处理，如深度学习，它从图像中分层学习深层特征。随着图形处理单元的快速发展，训练大规模模型已经变得可行。在2012年，Krizhevsky
    等（Krizhevsky 等，[2012](#bib.bib62)）训练了一个网络，将120万张高分辨率图像在ImageNet LSVRC-2010竞赛中分类为1000个不同的类别，取得了巨大成功，这引发了计算机视觉领域的革命。随后，训练深度卷积神经网络（CNN）在各种应用中主导了图像分类任务。图
    [5](#S5.F5 "图 5 ‣ 5\. 基于深度学习的指纹伪造攻击检测 ‣ 基于深度学习的指纹伪造攻击检测：综合调查") 展示了用于指纹PAD的深度学习算法的分类，这些算法针对以下三种不同的传感类型进行了开发：
- en: '![Refer to caption](img/781e6b5b9ecabd4774281a3eb866fd29.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/781e6b5b9ecabd4774281a3eb866fd29.png)'
- en: Figure 5\. Taxonomy of deep learning based fingerprint presentation attack detection
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 基于深度学习的指纹伪造攻击检测的分类
- en: 5.1\. Contact based FPAD
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 基于接触的FPAD
- en: 'Benefited from the rapid development of robust CNN architectures (Simonyan
    and Zisserman, [2014](#bib.bib109)) (He et al., [2016](#bib.bib41)) (Huang et al.,
    [2017](#bib.bib43)) as well as the advanced regularization techniques (Srivastava
    et al., [2014](#bib.bib113)) (Ioffe and Szegedy, [2015](#bib.bib46)), researchers
    have paid more attention to employing deep neural networks to detect fingerprint
    attacks reliably. Instead of extracting texture features through handcrafted-based
    descriptors, deep-learning-based approaches can learn deep features that directly
    map fingerprint inputs to spoof detection. The traditional CNN architecture comprises
    convolutional layers and a pooling layer that convolves several filters to map
    the input images to deep-learnable features. The extracted features can be further
    fed into a fully connected layer for classification tasks. As shown in Figure
    [5](#S5.F5 "Figure 5 ‣ 5\. Deep learning based Fingerprint presentation attack
    detection ‣ Deep Learning based Fingerprint Presentation Attack Detection: A Comprehensive
    Survey"), deep-learning-based methods can be generally divided into two categories.
    Supervised learning is a straightforward way to determine bona fide and PA as
    a binary classification task. However, these approaches may not be generalizable
    to unseen domain attacks (i.e., unknown presentation attacks). Many researchers
    have considered generalized deep-learning models that can achieve domain generalization
    to enhance the generalization capacity against unseen PA types.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '受益于强健的CNN架构（Simonyan和Zisserman，[2014](#bib.bib109)）（He等，[2016](#bib.bib41)）（Huang等，[2017](#bib.bib43)）以及先进的正则化技术（Srivastava等，[2014](#bib.bib113)）（Ioffe和Szegedy，[2015](#bib.bib46)），研究人员越来越关注利用深度神经网络可靠地检测指纹攻击。与通过手工设计的描述符提取纹理特征不同，基于深度学习的方法可以学习深层特征，直接将指纹输入映射到伪造检测。传统的CNN架构包括卷积层和池化层，这些层通过多个滤波器卷积将输入图像映射到深度可学习的特征。提取的特征可以进一步输入到全连接层进行分类任务。如图[5](#S5.F5
    "Figure 5 ‣ 5\. Deep learning based Fingerprint presentation attack detection
    ‣ Deep Learning based Fingerprint Presentation Attack Detection: A Comprehensive
    Survey")所示，基于深度学习的方法通常可以分为两类。监督学习是一种直接的方式来确定真实和PA作为二分类任务。然而，这些方法可能无法推广到未见过的领域攻击（即未知的呈现攻击）。许多研究人员考虑了广义的深度学习模型，这些模型可以实现领域泛化，以增强对未见过的PA类型的泛化能力。'
- en: Initially, Nogueira et al. (Nogueira et al., [2014](#bib.bib86)) first introduced
    a conventional network for fingerprint feature extraction. They trained a Support
    Vector Machine (SVM) classifier to detect presentation attacks based on CNN deep
    features and LBP features. To improve the classifier’s performance, several data
    augmentation techniques such as frequency filtering, contrast equalization, and
    Region Of Interest (ROI) filtering have been applied. Through comparison, the
    experiments indicate high classification accuracy of CNN model which leads to
    a new direction that inspires more researchers to concentrate on using Deep learning
    based approaches on FPAD tasks. However, the feature extraction and classification
    tasks are designed into two different parts so that the model is not optimized
    simultaneously.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Nogueira等（Nogueira等，[2014](#bib.bib86)）首次引入了一种用于指纹特征提取的传统网络。他们训练了一个支持向量机（SVM）分类器，以检测基于CNN深度特征和LBP特征的呈现攻击。为了提高分类器的性能，应用了多种数据增强技术，如频率过滤、对比度均衡和感兴趣区域（ROI）过滤。通过比较，实验表明CNN模型具有高分类精度，这为更多研究人员提供了新的方向，激励他们专注于在FPAD任务上使用深度学习方法。然而，特征提取和分类任务被设计成两个不同的部分，以致于模型未能同时优化。
- en: '![Refer to caption](img/047c8c0d5b703467e052a62233ba9b98.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/047c8c0d5b703467e052a62233ba9b98.png)'
- en: Figure 6\. Deep learning frameworks for Contact based FPAD. (a) End to end deep
    learning model using cross-entropy loss. (b) Transfer learning/fine tuning based
    FPAD approach. (c) FPAD using generalized deep learning model
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 基于接触的FPAD的深度学习框架。(a) 使用交叉熵损失的端到端深度学习模型。(b) 基于迁移学习/微调的FPAD方法。(c) 使用广义深度学习模型的FPAD
- en: 5.1.1\. End-to-end deep learning
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 端到端深度学习
- en: 'As shown in figure [6](#S5.F6 "Figure 6 ‣ 5.1\. Contact based FPAD ‣ 5\. Deep
    learning based Fingerprint presentation attack detection ‣ Deep Learning based
    Fingerprint Presentation Attack Detection: A Comprehensive Survey"), end-to-end
    deep learning models were trained to automatically outperform classification tasks.
    The feature representation of the fingerprint image was extracted from the convolutional
    layer and passed into the fully connected layer to calculate the liveness probability
    using the softmax function. Table LABEL:tab:end_to_end_DL presents a short description
    of existing end-to-end deep learning techniques.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[6](#S5.F6 "Figure 6 ‣ 5.1\. Contact based FPAD ‣ 5\. Deep learning based
    Fingerprint presentation attack detection ‣ Deep Learning based Fingerprint Presentation
    Attack Detection: A Comprehensive Survey")所示，端到端的深度学习模型经过训练后能自动超越分类任务。指纹图像的特征表示从卷积层提取，并传递到全连接层，以使用softmax函数计算活性概率。表LABEL:tab:end_to_end_DL展示了现有端到端深度学习技术的简要描述。'
- en: In 2015, Wang et al. (Wang et al., [2015](#bib.bib119)) divide the input labeled
    fingerprint images into $32\times 32$ pixel nonoverlapped patches and pass them
    into the CNN model for training. Then, the authors adopt a voting strategy to
    integrate the labels of all the patches to finally determine the result. Similarly,
    Park et al. (Park et al., [2016](#bib.bib92)) propose extending Wang’s work to
    reduce the processing time. Instead of extracting patches from the entire image,
    the authors indicate that it is more efficient to extract patches from the effective
    fingerprint area. Thus, after extracting patches with normal probability positions
    of segmented fingerprint regions, the authors train those patches with CNN and
    make the decision using a voting strategy to make the classification. Menotti
    et al. (Menotti et al., [2015](#bib.bib80))evaluate the effectiveness of the proposed
    newly Derived CNN architecture for spoofing detection terms using SpoofNet. Kim
    et.al (Kim et al., [2016](#bib.bib57)) present an FPAD method based on a Deep
    Belief Network (DBN) with a series of constrained Boltzmann machines connected,
    which learns features from training samples and determines the liveness of fingerprints.
    The DBN is trained in two steps. Firstly, it is trained on a set of examples without
    supervision from the first layer to the penultimate layer to learn the reconstruction
    probabilistically of its inputs. Then the model will be further trained using
    labeled data to perform the classification.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年，王等人（Wang et al., [2015](#bib.bib119)）将输入的标记指纹图像划分为$32\times 32$像素的不重叠块，并将它们传递到CNN模型进行训练。随后，作者采用投票策略将所有块的标签整合，最终确定结果。类似地，朴等人（Park
    et al., [2016](#bib.bib92)）提出扩展王的工作以减少处理时间。作者指出，从有效的指纹区域提取块比从整个图像中提取更为高效。因此，在提取了分割指纹区域的正常概率位置的块之后，作者使用CNN对这些块进行训练，并利用投票策略进行分类。门诺提等人（Menotti
    et al., [2015](#bib.bib80)）使用SpoofNet评估了提出的新的CNN架构在欺骗检测中的有效性。金等人（Kim et al., [2016](#bib.bib57)）提出了一种基于深度置信网络（DBN）的FPAD方法，该方法通过一系列受限玻尔兹曼机连接，学习训练样本中的特征并确定指纹的活性。DBN的训练分为两个步骤。首先，它在没有监督的情况下，从第一层到倒数第二层对一组样本进行训练，以概率地学习输入的重建。然后，模型将使用标记数据进行进一步训练以执行分类。
- en: With an increasing number of publicly datasets set released, Chugh et.al (Chugh
    et al., [2017](#bib.bib16)) utilize Inception-v3 CNN (Szegedy et al., [2016](#bib.bib114))
    model implemented using TF-Slim library with a replacement from multiple-class
    softmax layer to two-unit layer for the two-class problem. Then, the model is
    trained with local patches extracted around the fingerprint minutiae since local
    patches around these minutiae are able to provide significant cues to distinguish
    a spoof fingerprint from live fingerprints. Along with score-level average fusion,
    this method evaluates several experiments such as intra-sensors and the same material,
    intra-sensor and cross-material, cross-sensor, and cross-dataset scenarios to
    consider both known and unknown attacks, which demonstrated a good average classification
    error (ACE). In the next work, Chugh et al. (Chugh et al., [2018](#bib.bib17))
    further presented a Fingerprint Spoof Buster based on the MobileNet-v1 (Howard
    et al., [2017](#bib.bib42)) model trained using local patches that are centered
    and aligned around minutia points, and defined a global Spoofness Score to integrate
    the local Spoofness Score to determine the PA.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的公共数据集发布，Chugh等人（Chugh et al., [2017](#bib.bib16)）利用了基于TF-Slim库实现的Inception-v3
    CNN模型（Szegedy et al., [2016](#bib.bib114)），将多类softmax层替换为两类问题的二单位层。然后，该模型使用提取自指纹特征点周围的局部补丁进行训练，因为这些局部补丁能够提供重要线索，用于区分伪造指纹和真实指纹。结合得分级别的平均融合，这种方法评估了多个实验，如同传感器和相同材料、同传感器和跨材料、跨传感器以及跨数据集场景，以考虑已知和未知攻击，展示了良好的平均分类错误（ACE）。在后续工作中，Chugh等人（Chugh
    et al., [2018](#bib.bib17)）进一步提出了一种基于MobileNet-v1（Howard et al., [2017](#bib.bib42)）模型的指纹伪造检测器，该模型使用围绕特征点中心对齐的局部补丁进行训练，并定义了全球伪造度评分以整合局部伪造度评分来确定PA。
- en: However, FPAD methods based on CNN suffer from generalization and high computational
    costs. The selection of PA materials used in training (known PAs) directly affects
    the performance against unknown PAs. Some materials (e.g. EcoFlex) has been reported
    to be easier to detect than others (e.g. Silgum) (Chugh et al., [2018](#bib.bib17)).
    Hence, to further investigate the better representation of different PA materials,
    a new dataset namely MSU-FPAD v2.0 which combines the MSU-FPAD v1.0 and Precise
    Biometrics Spoof kit (Chugh et al., [2018](#bib.bib17)) was presented in (Chugh
    and Jain, [2019](#bib.bib18)). Specifically, the database is constructed using
    12 different PAIs. Then, the leave-one-out protocol is adopted; one PAI is excluded
    from the training set at every iteration and utilizes the 3D t-SNE technique to
    visualize the characteristics. Through experiments, Silicone, 2D Paper, Play Doh,
    Gelatin, Latex Body Paint, and Monster Liquid Latex are observed to cover the
    entire feature space around the Bona fide. Furthermore, it is considered a challenge
    to integrate existing deep learning-based algorithms with millions of parameters
    into an embedded or mobile device. Nguyen et.al (Nguyen et al., [2018](#bib.bib85))
    present the FPAD technique following the architecture of the Fire module of SqueezeNet
    (Iandola et al., [2016](#bib.bib45)) and introduce the Gram Matrix (Gatys et al.,
    [2015](#bib.bib29)) to form the structure of the basis of the proposed fPADnet.
    This model only contains 0.3 million parameters, which is 2.4 times smaller than
    that of the original SqueezeNet. Similarly, Park et al. (Park et al., [2019](#bib.bib91))
    introduced a fully convolutional neural network that uses the fire module of SqueezeNet
    as the foundation. The model can interfere with images of any size since it has
    no fully connected layer. The model takes the patch extracted from the input image
    as input and outputs three values that show the probabilities of the classes (live,
    false, and background) to which the patch belongs. Additional experiments with
    different input patch sizes at $32\times 32$, $48\times 48$, where $32\times 32$
    and $64\times 64$ are evaluated to identify the optimum patch size to achieve
    the highest detection accuracy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于CNN的FPAD方法存在泛化问题和高计算成本。用于训练的PA材料（已知的PAs）的选择直接影响对未知PAs的性能。一些材料（如EcoFlex）比其他材料（如Silgum）更容易被检测到（Chugh
    et al., [2018](#bib.bib17)）。因此，为了进一步研究不同PA材料的更好表征，提出了一个新的数据集MSU-FPAD v2.0，它结合了MSU-FPAD
    v1.0和Precise Biometrics Spoof kit（Chugh et al., [2018](#bib.bib17)），在（Chugh and
    Jain, [2019](#bib.bib18)）中介绍。具体而言，该数据库使用12种不同的PAI构建。然后，采用留一法协议；在每次迭代中排除一个PAI，并利用3D
    t-SNE技术可视化特征。通过实验观察到，硅胶、2D纸张、Play Doh、明胶、乳胶身体涂料和Monster液态乳胶覆盖了真伪样本周围的整个特征空间。此外，将现有的基于深度学习的算法与数百万个参数集成到嵌入式或移动设备中被认为是一项挑战。Nguyen
    et.al (Nguyen et al., [2018](#bib.bib85)) 提出了遵循SqueezeNet的Fire模块架构的FPAD技术，并引入Gram
    Matrix (Gatys et al., [2015](#bib.bib29)) 形成所提出的fPADnet的基础结构。该模型仅包含30万个参数，比原始SqueezeNet小2.4倍。同样，Park
    et al. (Park et al., [2019](#bib.bib91)) 介绍了一个完全卷积神经网络，以SqueezeNet的fire模块为基础。由于没有全连接层，该模型可以处理任何大小的图像。模型将从输入图像中提取的补丁作为输入，并输出三个值，表示补丁所属的类别（真实、伪造和背景）的概率。在不同输入补丁尺寸为$32\times
    32$、$48\times 48$、$32\times 32$和$64\times 64$的额外实验中，评估了最佳补丁尺寸以实现最高检测准确率。
- en: Table 4\. Existing contact based FPAD using end to end deep learning
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 基于端到端深度学习的现有接触式FPAD
- en: '| Author | Year | Backbone | Loss function | Description |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | 年份 | 主干网络 | 损失函数 | 描述 |'
- en: '| Nogueira et.al (Nogueira et al., [2014](#bib.bib86)) | 2014 | CNN | SVM |
    Extract the deep features using CNN and trained the SVM to classify the liveness
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Nogueira et.al (Nogueira et al., [2014](#bib.bib86)) | 2014 | CNN | 支持向量机
    | 使用CNN提取深度特征，并训练SVM进行活体分类 |'
- en: '| Wang et.al (Wang et al., [2015](#bib.bib119)) | 2015 | CNN | Binary CE loss
    | Patch based method with voting strategy |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Wang et.al (Wang et al., [2015](#bib.bib119)) | 2015 | CNN | 二元交叉熵损失 | 基于补丁的投票策略方法
    |'
- en: '| Menotti et.al (Menotti et al., [2015](#bib.bib80)) | 2015 | CNN | Binary
    CE loss | Propose a Derived CNN terms as SpoofNet |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Menotti et.al (Menotti et al., [2015](#bib.bib80)) | 2015 | CNN | 二元交叉熵损失
    | 提出了一个派生CNN术语作为SpoofNet |'
- en: '| Kim et.al (Kim et al., [2016](#bib.bib57)) | 2016 | Deep Belief Network (DBN)
    | Mean squared error loss | Propose a DBN based method for liveness detection
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Kim et.al (Kim et al., [2016](#bib.bib57)) | 2016 | 深度信念网络 (DBN) | 均方误差损失
    | 提出了基于DBN的方法用于活体检测 |'
- en: '| Park et.al (Park et al., [2016](#bib.bib92)) | 2016 | CNN | Binary CE loss
    | Extract the patches from the effective fingerprint area to train the CNN model
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Park 等（Park 等，[2016](#bib.bib92)） | 2016 | CNN | 二元交叉熵损失 | 从有效指纹区域提取补丁以训练CNN模型
    |'
- en: '| Lazimul and Binoy (Lazimul and Binoy, [2017](#bib.bib64)) | 2017 | CNN |
    Binary CE loss | Enhance the fingerprint image through several steps and train
    with CNN model |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Lazimul 和 Binoy（Lazimul 和 Binoy，[2017](#bib.bib64)） | 2017 | CNN | 二元交叉熵损失
    | 通过多个步骤增强指纹图像并使用CNN模型进行训练 |'
- en: '| Jang et.al (Jang et al., [2017](#bib.bib48)) | 2017 | CNN | Binary CE loss
    | Utilize the histogram equalization for contrast enhancement and train the CNN
    model with divided blocks |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 张等（张等，[2017](#bib.bib48)） | 2017 | CNN | 二元交叉熵损失 | 利用直方图均衡化进行对比度增强，并使用分块训练CNN模型
    |'
- en: '| Chugh et.al (Chugh et al., [2017](#bib.bib16)) | 2017 | Inception-v3 CNN
    | Binary CE loss | Use local patches extracted around fingerprint minutiae to
    train the Inception-v3 model with a slightly modification of last layer |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Chugh 等（Chugh 等，[2017](#bib.bib16)） | 2017 | Inception-v3 CNN | 二元交叉熵损失 |
    使用提取自指纹细节点周围的局部补丁来训练经过略微修改的Inception-v3模型 |'
- en: '| Chugh et.al (Chugh et al., [2018](#bib.bib17)) | 2018 | MobileNet-v1 | Binary
    CE loss | Define a global Spoofness Score to integrate the local patch based on
    minutia points to determine the liveness |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Chugh 等（Chugh 等，[2018](#bib.bib17)） | 2018 | MobileNet-v1 | 二元交叉熵损失 | 定义全局欺骗度评分，通过基于细节点的局部补丁来确定活体性
    |'
- en: '| Pala (Pala and Bhanu, [2017](#bib.bib90)) | 2017 | CNN | Triples loss | A
    triplet convolutional network is proposed |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Pala（Pala 和 Bhanu，[2017](#bib.bib90)） | 2017 | CNN | 三重损失 | 提出了一个三重卷积网络 |'
- en: '| Jung and Heo (Jung and Heo, [2018](#bib.bib51)) | 2018 | CNN | Squared Regression
    Error(SRE) | Employ SRE layer instead of CE loss layer as loss function |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Jung 和 Heo（Jung 和 Heo，[2018](#bib.bib51)） | 2018 | CNN | 平方回归误差（SRE） | 使用SRE层代替交叉熵损失层作为损失函数
    |'
- en: '| Nguyen et.al (Nguyen et al., [2018](#bib.bib85)) | 2018 | SqueezeNet | Binary
    CE loss | A lightweight model which optimize the SqueezeNet using designed Gram-Matrix
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 阮等（阮等，[2018](#bib.bib85)） | 2018 | SqueezeNet | 二元交叉熵损失 | 通过设计的Gram矩阵优化SqueezeNet的轻量级模型
    |'
- en: '| Chugh and Jain (Chugh and Jain, [2019](#bib.bib18)) | 2019 | CNN | Binary
    CE loss | Adopte leave-one-out protocol to exclude one PAI out of 12 from the
    training set every iteration to visualize the characteristics and find out which
    PAI achieves similar feature space as bona fide |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Chugh 和 Jain（Chugh 和 Jain，[2019](#bib.bib18)） | 2019 | CNN | 二元交叉熵损失 | 采用留一法协议，每次迭代从训练集中排除一个PAI，以可视化特征并找出哪个PAI具有类似于真实指纹的特征空间
    |'
- en: '| Park et.al (Park et al., [2019](#bib.bib91)) | 2019 | SqueezeNet | Three
    class CE loss | A Tiny Fully Convolutional Network without fully connected layer
    which can fit any size of input |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Park 等（Park 等，[2019](#bib.bib91)） | 2019 | SqueezeNet | 三类交叉熵损失 | 一个没有全连接层的小型全卷积网络，能够适应任何大小的输入
    |'
- en: '| Yuan et.al (Yuan et al., [2019a](#bib.bib124)) | 2019 | CNN | Binary CE loss
    | Between the last convolutional layer and the fully connected layer, an extra
    Image Scale Equalization(ISE) layer is added to preserve the texture information
    and maintain image resolution |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 袁等（袁等，[2019a](#bib.bib124)） | 2019 | CNN | 二元交叉熵损失 | 在最后的卷积层和全连接层之间增加一个图像尺度均衡（ISE）层，以保持纹理信息和图像分辨率
    |'
- en: '| Jung et.al (Jung et al., [2019](#bib.bib52)) | 2019 | CNN | Binary CE loss
    | Design a Livness Map CNN (LM-CNN) to map the probe and template images to a
    stacked feature vector and use a Template-Probe CNN to decide the spoofness |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Jung 等（Jung 等，[2019](#bib.bib52)） | 2019 | CNN | 二元交叉熵损失 | 设计一个活体图（LM-CNN），将探测图像和模板图像映射到堆叠的特征向量，并使用模板-探测CNN来判断欺骗性
    |'
- en: '| Yuan et.al (Yuan et al., [2019b](#bib.bib125)) | 2019 | ResNet | Binary CE
    loss | Introduce the ResNet with adaptive learning to tackle the gradient disappearance
    issue |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 袁等（袁等，[2019b](#bib.bib125)） | 2019 | ResNet | 二元交叉熵损失 | 引入具有自适应学习的ResNet，以解决梯度消失问题
    |'
- en: '| Zhang et.al (Zhang et al., [2019](#bib.bib129)) | 2019 | ResNet | Binary
    CE loss | Propose a lightweight framework that makes use of the specifically designed
    robust Residual block against fingerprint spoofing |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 张等（张等，[2019](#bib.bib129)） | 2019 | ResNet | 二元交叉熵损失 | 提出了一个轻量级框架，利用专门设计的抗指纹欺骗的强健残差块
    |'
- en: '| Zhang et.al (Zhang et al., [2020](#bib.bib128)) | 2020 | DenseNet | Binary
    CE loss | Adopt the attention mechanism instead of Global Average Pooling (GAP)
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 张等（张等，[2020](#bib.bib128)） | 2020 | DenseNet | 二元交叉熵损失 | 采用注意力机制代替全局平均池化（GAP）
    |'
- en: '| Jian et.al (Jian et al., [2020](#bib.bib50)) | 2020 | DenseNet | Binary CE
    loss | Introduce the genetic algorithm to find the optimal DenseNet structure
    for fingerprint liveness detection |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Jian 等（Jian et al., [2020](#bib.bib50)） | 2020 | DenseNet | Binary CE 损失
    | 引入遗传算法寻找用于指纹活体检测的最优 DenseNet 结构 |'
- en: '| Liu et.al (Liu et al., [2021b](#bib.bib68)) | 2021 | Mobile Net V3 | Binary
    CE loss | Global–local model-based with rethinking strategy |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等（Liu et al., [2021b](#bib.bib68)） | 2021 | Mobile Net V3 | Binary CE
    损失 | 基于全球–局部模型的重新思考策略 |'
- en: '| Rai et.al (Rai et al., [2023](#bib.bib97)) | 2023 | MoblieNet V1 | Support
    Vector Classifier (SVC) | Use MoblieNet V1 network for feature extraction and
    trained with loss from SVC |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Rai 等（Rai et al., [2023](#bib.bib97)） | 2023 | MoblieNet V1 | 支持向量分类器（SVC）
    | 使用 MoblieNet V1 网络进行特征提取，并通过 SVC 的损失进行训练 |'
- en: 'Image enhancement before detection has been well studied in deep-learning-based
    FPAD. Jang et al. (Jang et al., [2017](#bib.bib48)) utilized histogram equalization
    for contrast enhancement to improve the recognition rate of fingerprint images.
    The fingerprint image is divided into several non-overlapped blocks and trained
    with a CNN model for classification. The Majority Voting System (MVS) is applied
    to totalize the votes of all sub-blocks and make the final decision. Similarly,
    Lazimul and Binoy (Lazimul and Binoy, [2017](#bib.bib64)) propose to enhance a
    fingerprint image through six steps: Image Segmentation, Image Local Normalization,
    Orientation Estimation, Ridge frequency Estimation, Gabor Filtering, Image Binarization/thinning.
    Subsequently, a CNN model is used to train the data. Typically, cross-entropy
    is the most common loss function used to measure the difference between two probability
    distributions and is widely applied in classification tasks. Pala (Pala and Bhanu,
    [2017](#bib.bib90)) introduces a triplet loss (Schroff et al., [2015](#bib.bib106))
    that encourages dissimilar pairs to be distant from any similar pair by at least
    a certain margin value. A triplet network takes two patches of one class and one
    patch of the other to measure the inter- and intra-class distances supervised
    by triplet loss. Furthermore, Jung and Heo (Jung and Heo, [2018](#bib.bib51))
    introduce a new CNN architecture that employs a Squared Regression Error(SRE)
    layer instead of a cross-entropy loss layer. This method allows setting a threshold
    as a liveness probability to adjust the model, which provides an accuracy trade-off
    option to fit different application scenarios. Furthermore, Jung et al. (Jung
    et al., [2019](#bib.bib52)) extend their previous work by introducing two CNNs
    terms as Livness Map CNN (LM-CNN) and Template-Probe CNN (TP-CNN). The LM-CNN
    performs pre-computation during fingerprint registration to map the fingerprint
    image to a $32\times 32$ feature map. Then, the output map from the probe fingerprint
    and template fingerprint will be stacked as a $2\times 32\times 32$ liveness map,
    which will be fed into the TP-CNN for the final decision. Most CNN models require
    a fixed length of input images because of the restriction of the fully connected
    layer. Thus, the fingerprint dataset requires additional preprocessing such as
    cropping or scaling, which leads to information loss. To address this problem,
    Yuan et al. (Yuan et al., [2019a](#bib.bib124)) propose an improved DCNN model
    with Image Scale Equalization (ISE) to preserve texture information and maintain
    image resolution. Between the last convolutional layer and the fully connected
    layer, an extra ISE layer is added to obtain the feature map from the convolutional
    layer and convert the image of any scale into a fixed-length vector to fix the
    fully connected layer.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图像增强在基于深度学习的FPAD（指纹图像自动检测）中得到了充分研究。Jang等（Jang et al., [2017](#bib.bib48)）利用直方图均衡化进行对比度增强，以提高指纹图像的识别率。指纹图像被分成多个非重叠的块，并使用CNN模型进行分类训练。采用多数投票系统（MVS）对所有子块的投票进行汇总并做出最终决定。类似地，Lazimul和Binoy（Lazimul
    and Binoy, [2017](#bib.bib64)）提出通过六个步骤来增强指纹图像：图像分割、图像局部归一化、方向估计、脊频率估计、Gabor滤波、图像二值化/细化。随后，使用CNN模型进行数据训练。通常，交叉熵是最常用的损失函数，用于测量两个概率分布之间的差异，并广泛应用于分类任务。Pala（Pala
    and Bhanu, [2017](#bib.bib90)）引入了三元组损失（Schroff et al., [2015](#bib.bib106)），该损失函数鼓励不相似的样本与任何相似样本之间的距离至少大于某个边际值。三元组网络接受来自一个类别的两个补丁和另一个类别的一个补丁，以三元组损失监督下测量类内和类间的距离。此外，Jung和Heo（Jung
    and Heo, [2018](#bib.bib51)）引入了一种新的CNN架构，该架构使用平方回归误差（SRE）层代替交叉熵损失层。这种方法允许设置一个阈值作为活性概率来调整模型，提供了适应不同应用场景的准确性权衡选项。此外，Jung等（Jung
    et al., [2019](#bib.bib52)）通过引入两个CNN术语，即活性图CNN（LM-CNN）和模板-探测CNN（TP-CNN），扩展了他们之前的工作。LM-CNN在指纹注册期间进行预计算，将指纹图像映射到$32\times
    32$特征图。然后，来自探测指纹和模板指纹的输出图将堆叠为$2\times 32\times 32$的活性图，这将被输入到TP-CNN中进行最终决策。大多数CNN模型由于全连接层的限制，需要固定长度的输入图像。因此，指纹数据集需要额外的预处理，如裁剪或缩放，这会导致信息丢失。为了解决这个问题，Yuan等（Yuan
    et al., [2019a](#bib.bib124)）提出了一种改进的DCNN模型，采用图像尺度均衡（ISE）以保留纹理信息并保持图像分辨率。在最后的卷积层和全连接层之间，添加了额外的ISE层，以从卷积层获得特征图，并将任何尺度的图像转换为固定长度的向量，以适应全连接层。
- en: Furthermore, Yuan et.al (Yuan et al., [2019b](#bib.bib125)) first introduced
    the Deep Residual Network (He et al., [2016](#bib.bib41)) for FPAD. The authors
    designed a novel ROI extraction technique to remove the noise caused by background
    noise. Then, the gradient disappearance in the DCNN and learning parameters falling
    into local optimal value issues are tackled by applying a Deep Residual Network
    with adaptive learning. Owing to the concern of potential low generalization capability
    against unknown attack detection, a texture enhancement based on a Local Gradient
    Pattern (LGP) is introduced to highlight the difference between a bona fide sample
    and an attack sample to achieve a better generalization. Zhang et.al (Zhang et al.,
    [2019](#bib.bib129)) proposed a lightweight framework that makes use of the specifically
    designed robust Residual block (He et al., [2016](#bib.bib41)) against fingerprint
    spoofing. Silm-ResCNN consists of nine modified residual blocks. The authors make
    some changes by inserting a dropout layer into each pair of convolutional layers
    and removing the activation function (ReLU) of the second convolutional kernel
    to make the model more generalized. In another specific type, the $1\times 1$
    convolutional layer is replaced with max pooling, along with a padding zero channel.
    Therefore, the overall structure of The Slim-ResCNN consists of Conv1, Conv2,
    Conv3 (Conv3$\_$1, Conv3$\_$2), and Conv4 (Conv4$\_$1, Conv4$\_$2), followed by
    global average pooling (Avg$\_$Pool) and a final classification layer. The model
    will take the extracted local patches as input and the cross-entropy is used as
    the loss function. It should be noted that this method achieves the top performance
    in the Fingerprint Liveness Detection Competition 2017 (Mura et al., [2018](#bib.bib84)),
    with an overall accuracy of 95.25%. Zhang et.al (Zhang et al., [2020](#bib.bib128))
    discussed the limitation of Global Average Pooling against fingerprint spoofing
    and overcome it by adopting the attention mechanism. A lightweight model with
    only 0.48 million parameters was designed. Its blocks are designed where the residual
    path and densely connected path are incorporated, which benefits from DenseNet
    (Huang et al., [2017](#bib.bib43)) and ResNet (He et al., [2016](#bib.bib41)).
    Jian et al. (Jian et al., [2020](#bib.bib50)) point out the limitations of DenseNet
    based architecture (Zhang et al., [2020](#bib.bib128)) and optimize the model
    by adopting the genetic algorithm (Xie and Yuille, [2017](#bib.bib120)). Liu et.al
    (Liu et al., [2021b](#bib.bib68)) proposed a framework based on the rethinking
    strategy. The model consists of three modules, a global PAD module, a rethinking
    module, and a local PAD module. Firstly, the global PAD module receives the entire
    image as input and then predicts the global spoofness score. The rethinking module
    then takes the activation map to highlight the important regions for PAD through
    class activation mapping(CAM). Finally, these regions will be cropped and passed
    into the local PAD module to refine the prediction of the global PAD module. Recently,
    Rai et.al (Rai et al., [2023](#bib.bib97)) adopt MoblieNet V1 as a feature extractor
    due to the capacity of utilizing depth-wise separable convolution operation instead
    of traditional convolution operation, then the network is trained by the loss
    obtained from SVC. Furthermore, a comprehensive comparison among many existing
    approaches indicates that the proposed method, namely MoSFPAD, achieved state-of-the-art
    results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Yuan 等（Yuan et al., [2019b](#bib.bib125)）首次引入了用于 FPAD 的深度残差网络（He et al.,
    [2016](#bib.bib41)）。作者设计了一种新颖的 ROI 提取技术，以去除背景噪声引起的噪声。接着，通过应用自适应学习的深度残差网络来解决 DCNN
    中的梯度消失和学习参数陷入局部最优值的问题。由于对未知攻击检测的潜在低泛化能力的担忧，引入了一种基于局部梯度模式（LGP）的纹理增强方法，以突出真实样本和攻击样本之间的差异，从而实现更好的泛化。Zhang
    等（Zhang et al., [2019](#bib.bib129)）提出了一个轻量级框架，该框架利用了针对指纹欺诈特别设计的强健残差块（He et al.,
    [2016](#bib.bib41)）。Silm-ResCNN 包含九个修改过的残差块。作者通过在每对卷积层中插入一个 dropout 层，并移除第二个卷积核的激活函数（ReLU），使模型更具泛化性。在另一种特定类型中，$1\times
    1$ 卷积层被替换为最大池化，并增加了零填充通道。因此，Slim-ResCNN 的整体结构包括 Conv1、Conv2、Conv3（Conv3$_1$、Conv3$_2$）和
    Conv4（Conv4$_1$、Conv4$_2$），接着是全局平均池化（Avg$_{Pool}$）和最终分类层。该模型将提取的局部补丁作为输入，交叉熵作为损失函数。需要注意的是，这种方法在
    2017 年的指纹活体检测比赛（Mura et al., [2018](#bib.bib84)）中实现了 95.25% 的整体准确率。Zhang 等（Zhang
    et al., [2020](#bib.bib128)）讨论了全局平均池化在指纹欺诈中的局限性，并通过采用注意力机制克服了这一问题。设计了一个仅有 48 万参数的轻量级模型。其块设计中包含了残差路径和密集连接路径，这得益于
    DenseNet（Huang et al., [2017](#bib.bib43)）和 ResNet（He et al., [2016](#bib.bib41)）。Jian
    等（Jian et al., [2020](#bib.bib50)）指出了基于 DenseNet 的架构（Zhang et al., [2020](#bib.bib128)）的局限性，并通过采用遗传算法（Xie
    和 Yuille, [2017](#bib.bib120)）来优化模型。Liu 等（Liu et al., [2021b](#bib.bib68)）提出了一个基于重新思考策略的框架。该模型包括三个模块，一个全局
    PAD 模块、一个重新思考模块和一个局部 PAD 模块。首先，全局 PAD 模块接收整个图像作为输入，然后预测全局伪造分数。接着，重新思考模块利用激活图通过类激活映射（CAM）突出
    PAD 的重要区域。最后，这些区域将被裁剪并传入局部 PAD 模块，以细化全局 PAD 模块的预测。最近，Rai 等（Rai et al., [2023](#bib.bib97)）采用
    MoblieNet V1 作为特征提取器，因为它可以利用深度可分离卷积操作代替传统卷积操作，然后通过 SVC 获得的损失来训练网络。此外，对许多现有方法的综合比较表明，所提出的方法，即
    MoSFPAD，达到了最先进的结果。
- en: 5.1.2\. FPAD using transfer learning/fine tuning
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 使用迁移学习/微调的FPAD
- en: 'However, end-to-end deep learning-based FPAD achieved a notable improvement
    in classification accuracy. The size of the public fingerprint training set is
    insufficient to optimize a CNN model, which typically requires a large number
    of samples for training. On one hand, many researchers include data augmentation
    to apply small variations to the original data to extend the dataset. On the other
    hand, transfer learning and fine-tuning are normal ways to tackle the issue of
    small datasets. As shown in figure [6](#S5.F6 "Figure 6 ‣ 5.1\. Contact based
    FPAD ‣ 5\. Deep learning based Fingerprint presentation attack detection ‣ Deep
    Learning based Fingerprint Presentation Attack Detection: A Comprehensive Survey"),
    transfer learning/fine-tuning is a technique that does not train a deep learning
    model from scratch. Instead, transfer learning uses the representations learned
    by a pre-trained model to extract meaningful features and outperform classification
    with a new classifier. Differently, fine tuning technique is to unfreeze the weights
    corresponding to the top few layers of a pre-trained model based on general sets
    of images and ”fine-tune” the higher-order feature to make them more relevant
    for the specific task. Table [8](#S5.T8 "Table 8 ‣ 5.2.1\. Anomaly detection ‣
    5.2\. Contactless based FPAD ‣ 5\. Deep learning based Fingerprint presentation
    attack detection ‣ Deep Learning based Fingerprint Presentation Attack Detection:
    A Comprehensive Survey") presents a quick overview of the existing transfer-learning-based
    FPAD.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，基于端到端深度学习的FPAD在分类准确性上取得了显著的提升。公共指纹训练集的规模不足以优化CNN模型，而CNN模型通常需要大量样本进行训练。一方面，许多研究人员通过数据增强在原始数据上应用小的变化来扩展数据集。另一方面，迁移学习和微调是解决小数据集问题的常见方法。如图
    [6](#S5.F6 "Figure 6 ‣ 5.1\. Contact based FPAD ‣ 5\. Deep learning based Fingerprint
    presentation attack detection ‣ Deep Learning based Fingerprint Presentation Attack
    Detection: A Comprehensive Survey") 所示，迁移学习/微调是一种不从头开始训练深度学习模型的技术。相反，迁移学习利用预训练模型学到的表示来提取有意义的特征，并使用新的分类器在分类上表现更佳。不同的是，微调技术是解冻预训练模型中与顶部几层对应的权重，并对更高阶特征进行“微调”，使其更适合特定任务。表
    [8](#S5.T8 "Table 8 ‣ 5.2.1\. Anomaly detection ‣ 5.2\. Contactless based FPAD
    ‣ 5\. Deep learning based Fingerprint presentation attack detection ‣ Deep Learning
    based Fingerprint Presentation Attack Detection: A Comprehensive Survey") 展示了基于迁移学习的FPAD的快速概览。'
- en: Nogueira et al. (Nogueira et al., [2016](#bib.bib87)) extend their work (Nogueira
    et al., [2014](#bib.bib86)) by utilizing transfer learning on a pre-trained VGG
    (Simonyan and Zisserman, [2014](#bib.bib109)) and AlexNet (Krizhevsky et al.,
    [2012](#bib.bib62)) model and fine-tuned it using a fingerprint liveness detection
    dataset. By comparing four different models (two CNN models pre-trained with natural
    images and fine-tuned with fingerprint images, one CNN-Random model that uses
    only random filter weights drawn from a Gaussian distribution, and a traditional
    LBP-SVM model), the authors elaborate on the superiority of pre-trained CNNs on
    FPAD fields. Furthermore, Toosi et al. (Toosi et al., [2017b](#bib.bib118)) extract
    a set of small-sized patches that contain foreground pixels only, and pass those
    patches into a pre-trained AlexNet (Krizhevsky et al., [2012](#bib.bib62)) with
    a further training step that exploits features from fingerprint datasets. Similarly,
    Toosi et al. (Toosi et al., [2017a](#bib.bib117)) extract small-sized foreground
    patches of raw images and fine-tuned pre-trained AlexNet (Krizhevsky et al., [2012](#bib.bib62))
    and VGG19 (Simonyan and Zisserman, [2014](#bib.bib109)) models. Similarly, Ametefe
    et al. (S Ametefe et al., [2021](#bib.bib102)) utilize transfer learning using
    DenseNet(DenseNet201) (Huang et al., [2017](#bib.bib43)) which also achieves promising
    results compared with VGG and AlexNet features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Nogueira 等人（Nogueira et al., [2016](#bib.bib87)）通过利用迁移学习在预训练的 VGG（Simonyan 和
    Zisserman, [2014](#bib.bib109)）和 AlexNet（Krizhevsky et al., [2012](#bib.bib62)）模型上，扩展了他们的工作（Nogueira
    et al., [2014](#bib.bib86)），并使用指纹活跃性检测数据集进行了微调。通过比较四种不同的模型（两种用自然图像预训练并用指纹图像微调的
    CNN 模型，一种仅使用从高斯分布中抽取的随机滤波器权重的 CNN-Random 模型，以及一种传统的 LBP-SVM 模型），作者详细阐述了预训练 CNN
    在 FPAD 领域的优越性。此外，Toosi 等人（Toosi et al., [2017b](#bib.bib118)）提取了一组仅包含前景像素的小尺寸补丁，并将这些补丁传递给预训练的
    AlexNet（Krizhevsky et al., [2012](#bib.bib62)）进行进一步的训练，以利用指纹数据集中的特征。类似地，Toosi
    等人（Toosi et al., [2017a](#bib.bib117)）提取原始图像的小尺寸前景补丁，并微调预训练的 AlexNet（Krizhevsky
    et al., [2012](#bib.bib62)）和 VGG19（Simonyan 和 Zisserman, [2014](#bib.bib109)）模型。同样，Ametefe
    等人（S Ametefe et al., [2021](#bib.bib102)）利用 DenseNet(DenseNet201)（Huang et al.,
    [2017](#bib.bib43)）进行迁移学习，与 VGG 和 AlexNet 特征相比也取得了令人满意的结果。
- en: '| Author | Year | Backbone | Loss function | Description |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | 年份 | 主干网络 | 损失函数 | 描述 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Nogueira et.al (Nogueira et al., [2016](#bib.bib87)) | 2016 | AlexNet, VGG
    | Binary CE loss | Utilize pre-trained model and fine-tuned using fingerprint
    liveness detection dataset |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Nogueira 等人（Nogueira et al., [2016](#bib.bib87)） | 2016 | AlexNet, VGG |
    Binary CE 损失 | 利用预训练模型并使用指纹活跃性检测数据集进行微调 |'
- en: '| Toosi et.al (Toosi et al., [2017b](#bib.bib118)) | 2017 | AlexNet | Binary
    CE loss | Extract a set of small-sized patches containing only foreground pixels
    to fine-tune a pre-trained model |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Toosi 等人（Toosi et al., [2017b](#bib.bib118)) | 2017 | AlexNet | Binary CE
    损失 | 提取一组仅包含前景像素的小尺寸补丁，以微调预训练模型 |'
- en: '| Toosi et.al (Toosi et al., [2017a](#bib.bib117)) | 2017 | AlexNet, VGG19
    | Binary CE loss | Utilize transfer learning on two pre-trained CNN models |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Toosi 等人（Toosi et al., [2017a](#bib.bib117)） | 2017 | AlexNet, VGG19 | Binary
    CE 损失 | 在两个预训练 CNN 模型上利用迁移学习 |'
- en: '| Ametefe et.al (S Ametefe et al., [2021](#bib.bib102)) | 2021 | DenseNet |
    Binary CE loss | Utilize deep transfer learning on densenet201 network |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Ametefe 等人（S Ametefe et al., [2021](#bib.bib102)) | 2021 | DenseNet | Binary
    CE 损失 | 在 densenet201 网络上利用深度迁移学习 |'
- en: Table 5\. Existing contact based FPAD methods using transfer learning/fine-tuning
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5. 使用迁移学习/微调的现有接触基 FPAD 方法
- en: 5.1.3\. Generalized deep learning
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3. **广义深度学习**
- en: 'To improve the generalization capacity of the model, many researchers have
    considered applying a generalized model to transfer one domain to another using
    an adversarial learning-based model. The goal here is to address the generalizability
    of the FPAD techniques by performing domain transformation between the PAI and
    bona fide samples. Table [6](#S5.T6 "Table 6 ‣ 5.1.3\. Generalized deep learning
    ‣ 5.1\. Contact based FPAD ‣ 5\. Deep learning based Fingerprint presentation
    attack detection ‣ Deep Learning based Fingerprint Presentation Attack Detection:
    A Comprehensive Survey") presents a quick overview of existing generalizable FPAD
    techniques.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高模型的泛化能力，许多研究者考虑应用广义模型将一个领域转移到另一个领域，使用基于对抗学习的模型。这里的目标是通过在 PAI 和真实样本之间进行领域转换来解决
    FPAD 技术的泛化问题。表 [6](#S5.T6 "Table 6 ‣ 5.1.3. **广义深度学习** ‣ 5.1. 联系基 FPAD ‣ 5. 深度学习基指纹展示攻击检测
    ‣ 深度学习基指纹展示攻击检测：全面调查") 提供了现有可泛化 FPAD 技术的快速概述。
- en: 'Pereira et.al (Pereira et al., [2020](#bib.bib93)) proposed a novel model based
    on adversarial training, which consists of three subnetworks: (i) an encoder network
    that maps the input image into a latent space, (ii) a task-classifier network
    that maps the latent representation to the corresponding attack and bona fide
    probabilities, (iii) a species-classifier network that aims to predict the PAI
    species according to the attack latent representation. The species classifier
    is trained to minimize classification loss among the PAI species, whereas the
    task classifier and encoder are trained to minimize the classification loss between
    attacks and bona fide samples while trying to keep the PAI species classification
    close to random guessing. To further improve the generalization performance of
    the detector against spoofs made from materials that were not seen during training.
    A style-transfer-based wrapper, namely, a Universal Material Generator (UMG) is
    proposed to reliably detect the FPAD (Chugh and Jain, [2020](#bib.bib19)). The
    UMG is able to generate synthetic spoof images corresponding to unknown spoof
    materials by transferring the style (texture) characteristics between fingerprint
    images of known spoofing materials. Then, the synthesized images provide the model
    with more generalization capability to detect spoofs made of unknown materials.
    Sandouka et al. (Sandouka et al., [2021a](#bib.bib105)) propose a Unified Generative
    Adversarial Networks(UGAN) that can translate a single generator learning mapping
    across multiple domains. Subsequently, a share-weighted fusion layer acts as a
    classifier that fuses the output from all translated domains to determine the
    detection result. Similarly, Sandouka et al. (Sandouka et al., [2021b](#bib.bib104))
    further utilize a CycleGAN (Zhu et al., [2017](#bib.bib131)) network for domain
    adoption, which transforms the source domain to the target domain. In contrast
    to their previous work, a transformer model is employed to take a sequence of
    patches of the feature map as the input. The outputs are then concatenated and
    projected linearly to obtain a final output that is further fed into a fully connected
    layer for the classification task. This work further improved the performance
    rather than (Sandouka et al., [2021a](#bib.bib105)). Furthermore, Lee et al. (Lee
    et al., [2022](#bib.bib65)) proposed a generalization improvement method that
    utilizes style transfer to transfer the material styles between fingerprints.
    Liu et al. (Liu et al., [2022](#bib.bib66)) recently proposed a channel-wise feature
    denoising model. They extract the ’noise’ channels from the feature map by evaluating
    each channel of the image. Then, the interference from those channels is suppressed,
    and a PA-adaptation loss is designed to align the feature domain of the fingerprint.
    This method achieves promising results on the LivDet 2017 (Mura et al., [2018](#bib.bib84))
    dataset.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Pereira 等人（Pereira et al., [2020](#bib.bib93)）提出了一种基于对抗训练的创新模型，该模型由三个子网络组成：(i)
    一个编码器网络，将输入图像映射到潜在空间，(ii) 一个任务分类器网络，将潜在表示映射到相应的攻击和真实概率，(iii) 一个物种分类器网络，旨在根据攻击潜在表示预测
    PAI 物种。物种分类器的训练目标是最小化 PAI 物种之间的分类损失，而任务分类器和编码器则训练以最小化攻击和真实样本之间的分类损失，同时尽量使 PAI
    物种分类接近随机猜测。为了进一步提高检测器对训练期间未见过的材料伪造的泛化性能，提出了一种基于风格迁移的包装器，即通用材料生成器（UMG），以可靠地检测 FPAD（Chugh
    和 Jain, [2020](#bib.bib19)）。UMG 能够通过在已知伪造材料的指纹图像之间转移风格（纹理）特征来生成对应于未知伪造材料的合成伪造图像。然后，这些合成图像为模型提供了更好的泛化能力，以检测由未知材料制成的伪造。Sandouka
    等人（Sandouka et al., [2021a](#bib.bib105)）提出了一种统一生成对抗网络（UGAN），可以在多个领域之间进行单一生成器学习映射。随后，一个共享加权融合层作为分类器融合所有翻译领域的输出，以确定检测结果。类似地，Sandouka
    等人（Sandouka et al., [2021b](#bib.bib104)）进一步利用 CycleGAN（Zhu et al., [2017](#bib.bib131)）网络进行领域适应，将源领域转换为目标领域。与他们之前的工作相比，采用了
    transformer 模型，将特征图的多个补丁序列作为输入。然后，这些输出被串联并线性投影以获得最终输出，再进一步输入到全连接层进行分类任务。此项工作进一步提高了性能，而不是（Sandouka
    et al., [2021a](#bib.bib105)）。此外，Lee 等人（Lee et al., [2022](#bib.bib65)）提出了一种利用风格迁移在指纹之间转移材料风格的泛化改进方法。Liu
    等人（Liu et al., [2022](#bib.bib66)）最近提出了一种通道级特征去噪模型。他们通过评估图像的每个通道来提取“噪声”通道。然后，抑制这些通道的干扰，并设计了一种
    PA 适应损失来对齐指纹的特征域。这种方法在 LivDet 2017（Mura et al., [2018](#bib.bib84)）数据集上取得了令人满意的结果。
- en: '| Author | Year | Backbone | Loss function | Description |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | 年份 | 主干 | 损失函数 | 描述 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Pereira et.al (Pereira et al., [2020](#bib.bib93)) | 2020 | Species-invariant
    neural network | Adversarial loss, species-transfer loss | An adversarial learning
    approach to improve the capacity to detect the unseen attack |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Pereira 等（Pereira et al., [2020](#bib.bib93)） | 2020 | 物种不变神经网络 | 对抗损失，物种转移损失
    | 一种对抗学习方法，旨在提高检测未知攻击的能力 |'
- en: '| Chugh and Jain (Chugh and Jain, [2020](#bib.bib19)) | 2020 | Universal Material
    Generator (UMG) | Adversarial loss, style loss, content loss | A style transfer
    based approach that transfer texture characteristics between fingerprint images
    of known materials to unknown materials |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Chugh 和 Jain（Chugh and Jain, [2020](#bib.bib19)） | 2020 | 通用材料生成器（UMG） |
    对抗损失，风格损失，内容损失 | 一种基于风格迁移的方法，将已知材料的指纹图像的纹理特征迁移到未知材料上 |'
- en: '| Sandouka et.al (Sandouka et al., [2021a](#bib.bib105)) | 2021 | Generative
    Adversarial Networks, EfficientNet V2 (Tan and Le, [2021](#bib.bib115)) | Adversarial
    loss, reconstruction loss, domain classification loss | A Unified Generative Adversarial
    Networks(UGAN) that can translate a single generator learning the mapping across
    multiple domains |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Sandouka 等（Sandouka et al., [2021a](#bib.bib105)） | 2021 | 生成对抗网络，EfficientNet
    V2（Tan 和 Le, [2021](#bib.bib115)） | 对抗损失，重建损失，领域分类损失 | 一个统一的生成对抗网络（UGAN），可以通过单一生成器学习在多个领域之间的映射
    |'
- en: '| Sandouka et.al (Sandouka et al., [2021b](#bib.bib104)) | 2021 | Transformers
    and CycleGAN | Adversarial loss, cycle consistency loss | Utilize a CycleGAN to
    transform from the source domain to the target domain |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Sandouka 等（Sandouka et al., [2021b](#bib.bib104)） | 2021 | Transformers 和
    CycleGAN | 对抗损失，循环一致性损失 | 利用 CycleGAN 从源领域转换到目标领域 |'
- en: '| Lee et.al (Lee et al., [2022](#bib.bib65)) | 2022 | CycleGAN and CNN | Adversarial
    loss, binary CE loss | Transform fingerprint image from an untrained material
    style to a trained material style using CycleGAN |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Lee 等（Lee et al., [2022](#bib.bib65)） | 2022 | CycleGAN 和 CNN | 对抗损失，二进制交叉熵损失
    | 使用 CycleGAN 将指纹图像从未经训练的材料风格转换为已训练的材料风格 |'
- en: '| Liu et.al (Liu et al., [2022](#bib.bib66)) | 2022 | MoblieNet V2 | PA-Adaptation
    loss, binary CE loss | Propose a channel-wise feature denoising model |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等（Liu et al., [2022](#bib.bib66)） | 2022 | MoblieNet V2 | PA-适应损失，二进制交叉熵损失
    | 提出了一个按通道特征去噪的模型 |'
- en: Table 6\. Existing Generalized deep learning FPAD methods
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 现有的通用深度学习 FPAD 方法
- en: 5.2\. Contactless based FPAD
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 基于无接触的 FPAD
- en: '![Refer to caption](img/2a5e1c14e99e3904acdac6f369e93b4a.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2a5e1c14e99e3904acdac6f369e93b4a.png)'
- en: Figure 7\. Examples of contactless based fingerprint system using sensor-specific
    approaches
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 基于无接触的指纹系统示例，使用传感器特定的方法
- en: 'Traditional fingerprint images suffer from presentation attacks since the texture
    features of spoofing are typically inconspicuous. Additionally, the development
    of new synthetic materials brings more challenges to the generalization ability
    of the current models. However, the texture of the spoofing fingerprint surface
    can be visible through a multi-spectrum capture device or even a smartphone camera.
    A demonstration of multi-spectrum capture device is shown in Figure [7](#S5.F7
    "Figure 7 ‣ 5.2\. Contactless based FPAD ‣ 5\. Deep learning based Fingerprint
    presentation attack detection ‣ Deep Learning based Fingerprint Presentation Attack
    Detection: A Comprehensive Survey"). Hussein et.al (Hussein et al., [2018](#bib.bib44))
    propose a patch-based CNN model that takes multi-spectral short-wave infrared
    (SWIR) imaging and laser speckle contrast imaging (LSCI) images as input to classify
    the skin or no skin. Furthermore, Mirzaalian et al. (Mirzaalian et al., [2019](#bib.bib82))
    utilize LSCI images and evaluate four different models: the model proposed by
    (Hussein et al., [2018](#bib.bib44)) terms as baseN, adding residual connections
    between every two 2D convolution layers of the BaseN terms as ResN, introducing
    inception module terms as IncpN, and a double-layer long short-term memory (LSTM)-based
    network. The obtained result shows that LSTM based approach achieves the best
    performance. Kolberg et al. (Kolberg et al., [2020](#bib.bib61)) analyze the long
    short-term memory (LSTM) network in comparison with different CNN models on a
    fingerprint image captured by a 1310 nm laser device. The obtained experimental
    results indicate the advancement of the LSTM model compared with the CNN models.
    Furthermore, Spinoulas et al. (Spinoulas et al., [2021](#bib.bib112)) evaluate
    FPAD performance under different sensing modalities using a fully convolutional
    neural network (FCN). The evaluation experiments are carried under fingerprint
    images captured from Visible (VIS), near-infrared (NIR), SWIR, LSCI and Near-infrared
    back-illumination domains.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 传统指纹图像容易受到伪造攻击，因为伪造的纹理特征通常不明显。此外，新型合成材料的出现对当前模型的泛化能力提出了更多挑战。然而，伪造指纹表面的纹理可以通过多光谱捕获设备甚至智能手机摄像头变得可见。多光谱捕获设备的演示如图[7](#S5.F7
    "图 7 ‣ 5.2\. 无接触式 FPAD ‣ 5\. 基于深度学习的指纹伪造攻击检测 ‣ 基于深度学习的指纹伪造攻击检测：全面调查")所示。Hussein
    et.al (Hussein et al., [2018](#bib.bib44)) 提出了一个基于补丁的CNN模型，该模型将多光谱短波红外（SWIR）成像和激光散斑对比成像（LSCI）图像作为输入，用于分类皮肤与非皮肤。此外，Mirzaalian
    et al. (Mirzaalian et al., [2019](#bib.bib82)) 利用LSCI图像评估了四种不同的模型：Hussein et al.
    (Hussein et al., [2018](#bib.bib44))提出的模型称为baseN，在BaseN的每两个2D卷积层之间添加残差连接称为ResN，引入了
    inception 模块的模型称为IncpN，以及一个双层长短期记忆（LSTM）网络。结果表明，基于LSTM的方法表现最佳。Kolberg et al. (Kolberg
    et al., [2020](#bib.bib61)) 比较了LSTM网络与不同CNN模型在1310 nm激光设备捕获的指纹图像上的表现。实验结果表明，与CNN模型相比，LSTM模型具有更好的性能。此外，Spinoulas
    et al. (Spinoulas et al., [2021](#bib.bib112)) 使用完全卷积神经网络（FCN）评估了不同感测模式下的FPAD性能。评估实验是在从可见光（VIS）、近红外（NIR）、SWIR、LSCI和近红外背照射领域捕获的指纹图像上进行的。
- en: '| Author | Year | Backbone | Loss function | Description |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | 年份 | 主干网络 | 损失函数 | 描述 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Hussein et.al (Hussein et al., [2018](#bib.bib44)) | 2016 | CNN | Binary
    CE loss | A patch-based CNN based on SWIR and LSCI images |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Hussein et.al (Hussein et al., [2018](#bib.bib44)) | 2016 | CNN | 二元交叉熵损失
    | 基于SWIR和LSCI图像的基于补丁的CNN |'
- en: '| Mirzaalian et.al (Mirzaalian et al., [2019](#bib.bib82)) | 2019 | CNN | Binary
    CE loss | Compare four different models |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Mirzaalian et.al (Mirzaalian et al., [2019](#bib.bib82)) | 2019 | CNN | 二元交叉熵损失
    | 比较四种不同的模型 |'
- en: '| Kolberg et.al (Kolberg et al., [2020](#bib.bib61)) | 2020 | Long short-term
    memory (LSTM) network and CNN | Binary CE loss | Compare the performance among
    LSTM, LRCN (long-term recurrent convolutional network) and CNN models |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Kolberg et.al (Kolberg et al., [2020](#bib.bib61)) | 2020 | 长短期记忆（LSTM）网络和CNN
    | 二元交叉熵损失 | 比较LSTM、LRCN（长短期递归卷积网络）和CNN模型的性能 |'
- en: '| Spinoulas et.al (Spinoulas et al., [2021](#bib.bib112)) | 2021 | fully-convolutional
    neural network (FCN) | Binary CE loss | Evaluate the performance with images captured
    from Visible (VIS), near-infrared (NIR), SWIR, LSCI and Near-infrared back-illumination
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Spinoulas et.al (Spinoulas et al., [2021](#bib.bib112)) | 2021 | 完全卷积神经网络（FCN）
    | 二元交叉熵损失 | 评估从可见光（VIS）、近红外（NIR）、SWIR、LSCI和近红外背照射捕获的图像中的性能 |'
- en: Table 7\. Existing State-Of-The-Art contactless based FPAD methods
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 现有的最先进的无接触FPAD方法
- en: 5.2.1\. Anomaly detection
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 异常检测
- en: 'Most previous deep learning models formulate FPAD as a close-set problem to
    detect various predefined PAs, which require large-scale training data to cover
    as many attacks as possible. In addition, the training data must be labeled prior
    to training. However, this leads to an overfitting issue in that the model is
    highly sensitive to the PAs already included in the training dataset but lacks
    generalization capability against unseen attacks. An increasing number of novel
    materials have been developed to produce gummy fingers to deceive FRS easily (Saguy
    et al., [2022](#bib.bib103)). The unknown FPAD method has become an open issue
    and has attracted increasing attention in recent years. Compared with the most
    common binary classifier, the one-class classifier only learns the representation
    of a live fingerprint and does not use spoof impressions of any specific material.
    Then, the unseen attack is detected as an anomaly, which is performed as an outlier
    compared with the bona fide samples. Figure [8](#S5.F8 "Figure 8 ‣ 5.2.1\. Anomaly
    detection ‣ 5.2\. Contactless based FPAD ‣ 5\. Deep learning based Fingerprint
    presentation attack detection ‣ Deep Learning based Fingerprint Presentation Attack
    Detection: A Comprehensive Survey") shows the difference between the binary classifier
    and anomaly detection-based approaches.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数之前的深度学习模型将FPAD视为一个封闭集问题，以检测各种预定义的PA，这需要大规模的训练数据来涵盖尽可能多的攻击。此外，训练数据必须在训练前进行标注。然而，这导致了过拟合问题，即模型对训练数据集中已包含的PA非常敏感，但对未见过的攻击缺乏泛化能力。越来越多的新材料被开发出来，容易伪造FRS（Saguy等人，[2022](#bib.bib103)）。未知FPAD方法已成为一个开放问题，并在近年来受到越来越多的关注。与最常见的二分类器相比，一类分类器只学习活体指纹的表示，而不使用任何特定材料的伪造印象。然后，未见过的攻击被检测为异常，与真实样本相比表现为离群值。图[8](#S5.F8
    "Figure 8 ‣ 5.2.1\. Anomaly detection ‣ 5.2\. Contactless based FPAD ‣ 5\. Deep
    learning based Fingerprint presentation attack detection ‣ Deep Learning based
    Fingerprint Presentation Attack Detection: A Comprehensive Survey")展示了二分类器和基于异常检测的方法之间的区别。'
- en: '![Refer to caption](img/3cb9d692ff4e7536c955eed36ee9fab1.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3cb9d692ff4e7536c955eed36ee9fab1.png)'
- en: Figure 8\. An example of binary classifier and anomaly detection
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 二分类器和异常检测的示例
- en: '| Author | Year | Backbone | Loss function | Description |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | 年份 | 主干 | 损失函数 | 描述 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Engelsma and Jain (Engelsma and Jain, [2019](#bib.bib23)) | 2019 | Generative
    Adversarial Networks (GANS) | Adversarial loss | Train three GANS respectively
    on raw FTIR images, processed FTIR images, and direct-view images |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Engelsma 和 Jain（Engelsma 和 Jain，[2019](#bib.bib23)） | 2019 | 生成对抗网络（GANS）
    | 对抗损失 | 分别在原始FTIR图像、处理后的FTIR图像和直接查看的图像上训练三个GANS |'
- en: '| Rohrer and Kolberg (Rohrer and Kolberg, [2021](#bib.bib100)) | 2021 | Wasserstein
    GAN and AutoEncoder | Reconstruction loss | Utilize pre-trained WGAN weight on
    AutoEncoder network |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Rohrer 和 Kolberg（Rohrer 和 Kolberg，[2021](#bib.bib100)） | 2021 | Wasserstein
    GAN和自编码器 | 重建损失 | 在自编码器网络上利用预训练的WGAN权重 |'
- en: '| kolberg et.al (Kolberg et al., [2021b](#bib.bib59)) | 2021 | Three AutoEncoders
    | Reconstruction loss | Three AutoEncoder are proposed |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| kolberg 等人（Kolberg 等人，[2021b](#bib.bib59)） | 2021 | 三个自编码器 | 重建损失 | 提出了三个自编码器
    |'
- en: '| Liu et.al (Liu et al., [2021a](#bib.bib67)) | 2021 | AutoEncoder | Reconstruction
    loss | AutoEncoder based on TOptical Coherence Technology (OCT) images |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 刘等人（刘等人，[2021a](#bib.bib67)） | 2021 | 自编码器 | 重建损失 | 基于T光学相干技术（OCT）图像的自编码器
    |'
- en: Table 8\. Existing State-Of-The-Art anomaly detection based FPAD methods
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表8\. 基于FPAD的现有最先进的异常检测方法
- en: Engelsma and Jain (Engelsma and Jain, [2019](#bib.bib23)) propose a fingerprint
    spoof detector on only the live fingerprints through training multiple generative
    adversarial network (GANS) (Goodfellow et al., [2014](#bib.bib35)) for live fingerprint
    images. Three GAN models were trained on raw FTIR images from RaspiReader, processed
    FTIR images, and direct-view images. For each GAN, the generator attempts to synthesize
    live fingerprint images, where the discriminator uses the generated samples as
    well as the true samples from the dataset to distinguish them. Thus, through long
    iteration training, the generator is trained to generate high-quality images where
    the discriminator is able to separate the liveness sample from the generated sample.
    After the training phase, the generator is discarded, and the discriminator can
    be used as an FPAD module to detect attacks. Finally, fusion of the scores output
    by all three discriminators constitutes the final spoofness score of an input
    fingerprint sample. Rohrer and Kolberg (Rohrer and Kolberg, [2021](#bib.bib100))
    first utilize the Wasserstein GAN (Arjovsky et al., [2017](#bib.bib10)) as a pretrained
    model trained with LivDet2021 (Casula et al., [2021a](#bib.bib13)) Dermalog Sensor
    dataset from scratch. The GANs discriminator weights are transferred to the AutoEncoder’s(AE)
    encoder, whereas the generator weights are transferred to the decoder. A convolutional
    layer is added between the encoder and decoder to connect them. The AE learns
    to minimize reconstruction loss (Géron, [2022](#bib.bib30)) so that the model
    can reconstruct the input image with minimal reconstruction error. The PA will
    be detected with a large reconstruction error.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Engelsma 和 Jain（Engelsma and Jain, [2019](#bib.bib23)）提出了一种仅通过训练多个生成对抗网络（GANs）（Goodfellow
    et al., [2014](#bib.bib35)）来检测活体指纹的伪造检测器。三个 GAN 模型分别在来自 RaspiReader 的原始 FTIR 图像、处理后的
    FTIR 图像和直接视图图像上进行训练。对于每个 GAN，生成器尝试合成活体指纹图像，而鉴别器则使用生成的样本和数据集中真实的样本来区分它们。因此，通过长时间的迭代训练，生成器被训练生成高质量图像，使鉴别器能够将活体样本与生成样本分开。在训练阶段结束后，生成器会被丢弃，而鉴别器可以作为
    FPAD 模块用于检测攻击。最后，所有三个鉴别器输出的分数融合构成输入指纹样本的最终伪造分数。Rohrer 和 Kolberg（Rohrer and Kolberg,
    [2021](#bib.bib100)）首次使用 Wasserstein GAN（Arjovsky et al., [2017](#bib.bib10)）作为预训练模型，并从头开始使用
    LivDet2021（Casula et al., [2021a](#bib.bib13)）Dermalog 传感器数据集进行训练。GAN 的鉴别器权重被转移到自编码器（AE）的编码器中，而生成器的权重则转移到解码器中。在编码器和解码器之间添加了一个卷积层以连接它们。AE
    学习最小化重建损失（Géron, [2022](#bib.bib30)），以便模型可以以最小的重建误差重建输入图像。PA 将通过大的重建误差来检测。
- en: Kolberg et al. (Kolberg et al., [2021b](#bib.bib59)) propose a new PAD technique
    based on autoencoders (AE) trained only on bona fide samples captured in the short-wave
    infrared domain, which converts the two-class classification problem into a one-class
    domain. The authors introduced three AE architectures for Conv-AE, Pooling-AE,
    and Dense-AE, and compared the results with other state-of-the-art one-class PAD.
    In addition, Liu et al. (Liu et al., [2021a](#bib.bib67)) proposes a novel One-Class
    PAD (OCPAD) method for Optical Coherence Technology (OCT) images that provides
    an internal representation of the fingertip skin rather than a simple feature.
    The proposed PAD framework consists of auto-encoder network-based reference bona
    fide modeling and spoofness score-based PA detection.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Kolberg 等（Kolberg et al., [2021b](#bib.bib59)）提出了一种基于自编码器（AE）的新型 PAD 技术，该技术仅在短波红外域中对真实样本进行训练，将二分类问题转换为一类域。作者介绍了三种
    AE 架构：Conv-AE、Pooling-AE 和 Dense-AE，并将其结果与其他最先进的一类 PAD 进行了比较。此外，Liu 等（Liu et al.,
    [2021a](#bib.bib67)）提出了一种新型的光学相干技术（OCT）图像的一类 PAD（OCPAD）方法，该方法提供了指尖皮肤的内部表示，而不是简单的特征。所提出的
    PAD 框架包括基于自编码器网络的参考真实建模和基于伪造分数的 PA 检测。
- en: 5.3\. Smartphone based FPAD
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 基于智能手机的 FPAD
- en: The rapid development of smartphone-based authentication applications has achieved
    high verification accuracy (Ramachandra and Li, [2023](#bib.bib98)), which raised
    concerns about the smartphone-based system being easily spoofed. Zhang et.al (Zhang
    et al., [2016](#bib.bib130)) proposes a 2D smartphone based approach that combines
    the features of two local descriptors (LBP and LPQ) with deep features extracted
    from a CNN model. The CNN model is optimized by integrating global average pooling
    and batch normalization. Due to the lack of publicly available datasets, a self
    obtained bona fide samples and 2D attack samples made of wood glue, electrosol
    from PCB, or printed by special conductive ink are built. By fusing the results
    of the two descriptors and the CNN, the final decision can be output. Fujio et.al.
    (Fujio et al., [2018](#bib.bib26)) compare the performance of using hand-crafted
    based method(LBP, LPQ) and deep learning based method. The obtained results indicate
    that the DCNN (AlexNet) can achieve a stable accuracy when the intensity of the
    blurring noise increases. Marasco and Vurity (Marasco and Vurity, [2021](#bib.bib76))
    explored detection performance by training the IIITD database using various CNN
    architectures (AlexNet (Krizhevsky et al., [2012](#bib.bib62)) and ResNet18 (He
    et al., [2016](#bib.bib41))). The comparison results demonstrate that AlexNet
    achieved a robust performance against unseen attacks. The authors further propose
    a novel method (Marasco et al., [2022](#bib.bib77)) that explores the detection
    effectiveness of different CNN models on different color spaces. The raw image
    is converted into RGB, YCBCr, HSV, LAB, and XYZ color spaces, then the five images
    are then further fed into five pre-trained CNN models(AlexNet (Krizhevsky et al.,
    [2012](#bib.bib62)), DenseNet201 (Huang et al., [2017](#bib.bib43)), DenseNet121,
    ResNet18 (He et al., [2016](#bib.bib41)), ResNet34, and MobileNet-V2(Howard et al.,
    [2017](#bib.bib42))). The Best network is selected, and the score of the five
    color spaces is fused to obtain the final decision. Recently, to address the lack
    of publicly available fingerphoto presentation attack detection datasets, Purnapatra
    et al.. al (Purnapatra et al., [2023](#bib.bib96)) propose a new dataset comprised
    of six different PAIs. The FPAD methods use the state-of-the-art CNN models DenseNet
    121 and NASNet (Zoph et al., [2018](#bib.bib132)) which achieve promising PAD
    accuracy on the proposed dataset.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 智能手机认证应用的快速发展已经实现了高验证准确率（Ramachandra and Li, [2023](#bib.bib98)），这引发了对智能手机系统容易被伪造的担忧。张等人（Zhang
    et al., [2016](#bib.bib130)）提出了一种基于2D智能手机的方法，该方法结合了两种局部描述符（LBP和LPQ）的特征与从CNN模型中提取的深度特征。CNN模型通过集成全局平均池化和批归一化进行了优化。由于缺乏公开的可用数据集，构建了自获取的真实样本和由木胶、PCB电解液或特殊导电墨水打印的2D攻击样本。通过融合两种描述符和CNN的结果，可以输出最终决策。藤井等人（Fujio
    et al., [2018](#bib.bib26)）比较了基于手工特征的方法（LBP, LPQ）和基于深度学习的方法的性能。获得的结果表明，当模糊噪声强度增加时，DCNN（AlexNet）能够实现稳定的准确率。Marasco和Vurity（Marasco
    and Vurity, [2021](#bib.bib76)）通过使用各种CNN架构（AlexNet (Krizhevsky et al., [2012](#bib.bib62))
    和 ResNet18 (He et al., [2016](#bib.bib41))) 训练IIITD数据库，探索了检测性能。比较结果表明，AlexNet在未见过的攻击面前表现出强大的性能。作者进一步提出了一种新方法（Marasco
    et al., [2022](#bib.bib77)），探索了不同CNN模型在不同颜色空间上的检测效果。原始图像被转换为RGB、YCBCr、HSV、LAB和XYZ颜色空间，然后将这五张图像进一步输入到五个预训练的CNN模型（AlexNet
    (Krizhevsky et al., [2012](#bib.bib62))、DenseNet201 (Huang et al., [2017](#bib.bib43))、DenseNet121、ResNet18
    (He et al., [2016](#bib.bib41))、ResNet34和MobileNet-V2(Howard et al., [2017](#bib.bib42))）中。选择最佳网络，并融合五个颜色空间的得分以获得最终决策。最近，为了应对缺乏公开的指纹照片展示攻击检测数据集的问题，Purnapatra等人（Purnapatra
    et al., [2023](#bib.bib96)）提出了一个由六种不同PAI组成的新数据集。FPAD方法使用了最先进的CNN模型DenseNet 121和NASNet（Zoph
    et al., [2018](#bib.bib132)），在所提出的数据集上实现了有希望的PAD准确率。
- en: '| Author | Year | Backbone | Loss function | Description |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | 年份 | 主干网络 | 损失函数 | 描述 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Zhang et.al (Zhang et al., [2016](#bib.bib130)) | 2016 | CNN | Binary CE
    loss | Two handcrafted-features trained by SVM integrate with CNN. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 张等人（Zhang et al., [2016](#bib.bib130)） | 2016 | CNN | 二元交叉熵损失 | 两种手工特征通过SVM训练，与CNN集成。'
- en: '| Fujio et.al. (Fujio et al., [2018](#bib.bib26)) | 2018 | AlexNet | Binary
    CE loss | Compared with handcrafted based method |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Fujio et.al. (Fujio et al., [2018](#bib.bib26)) | 2018 | AlexNet | 二元交叉熵损失
    | 与手工特征方法的比较 |'
- en: '| Marasco and Vurity (Marasco and Vurity, [2021](#bib.bib76)) | 2021 | AlexNet,
    ResNet18 | Binary CE loss | Evaluate the different CNNs on the IIITD database
    with spoof data including printout and various display attacks. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Marasco 和 Vurity (Marasco and Vurity, [2021](#bib.bib76)) | 2021 | AlexNet,
    ResNet18 | 二元交叉熵损失 | 评估不同的 CNN 在 IIITD 数据库上使用伪造数据，包括打印输出和各种显示攻击。 |'
- en: '| Marasco et.al (Marasco et al., [2022](#bib.bib77)) | 2022 | AlexNet, DenseNet201,
    DenseNet121, ResNet18, ResNet34, MobileNet-V2 | Binary CE loss | Explore the effectiveness
    of deep representations derived from various color spaces based on the best model
    of six CNN models. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Marasco 等 (Marasco et al., [2022](#bib.bib77)) | 2022 | AlexNet, DenseNet201,
    DenseNet121, ResNet18, ResNet34, MobileNet-V2 | 二元交叉熵损失 | 探索基于六种 CNN 模型中最佳模型的各种颜色空间派生的深度表示的有效性。
    |'
- en: '| Purnapatra et. al (Purnapatra et al., [2023](#bib.bib96)) | 2023 | DenseNet
    121 and NASNet (Zoph et al., [2018](#bib.bib132)) | Binary CE loss | Develop a
    new fingerphoto PAD dataset and utilize DenseNet and NASNet for testing detection
    accuracy |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Purnapatra 等 (Purnapatra et al., [2023](#bib.bib96)) | 2023 | DenseNet 121
    和 NASNet (Zoph et al., [2018](#bib.bib132)) | 二元交叉熵损失 | 开发一个新的指纹照片 PAD 数据集，并利用
    DenseNet 和 NASNet 测试检测准确性。 |'
- en: Table 9\. Existing smartphone based FPAD methods
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9\. 现有的基于智能手机的 FPAD 方法
- en: 5.4\. FPAD using hybrid feature extraction methods
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 使用混合特征提取方法的 FPAD
- en: 'The hybrid method refers to combining more than one type of feature (handcrafted
    features, deep features, and multi-spectrum features). etc.) to detect the PAIs.
    The hybrid features can be used with all types of fingerprint capture devices
    and have been demonstrated to achieve higher detection accuracy at the cost of
    computation. Table [10](#S5.T10 "Table 10 ‣ 5.4\. FPAD using hybrid feature extraction
    methods ‣ 5\. Deep learning based Fingerprint presentation attack detection ‣
    Deep Learning based Fingerprint Presentation Attack Detection: A Comprehensive
    Survey") briefly discusses the state-of-the-art hybrid FPAD methods.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '混合方法指的是结合多种特征类型（手工特征、深度特征和多光谱特征等）来检测 PAIs。这些混合特征可以与所有类型的指纹捕捉设备一起使用，并已被证明在计算成本的代价下实现了更高的检测准确性。表
    [10](#S5.T10 "Table 10 ‣ 5.4\. FPAD using hybrid feature extraction methods ‣
    5\. Deep learning based Fingerprint presentation attack detection ‣ Deep Learning
    based Fingerprint Presentation Attack Detection: A Comprehensive Survey") 简要讨论了最先进的混合
    FPAD 方法。'
- en: Jomaa et.al (M. Jomaa et al., [2020](#bib.bib71)) utilize electrocardiogram
    (ECG) signals as well as the deep features to jointly make the decision. Furthermore,
    Tolosana et al. (Tolosana et al., [2018](#bib.bib116)) propose a multi-model-based
    approach that utilizes four deep features respectively extracted from 5-layer
    residual network, pre-trained Moblie-Net-based network, pre-trained VGG 19 based
    network and CNN model, and combined the deep features with a spectral signal from
    a short-wave infrared (SWIR) spectrum capture device to jointly determine the
    liveness. Gomez et.al (Gomez-Barrero et al., [2019](#bib.bib34)) analysis the
    surface of a finger within the SWIR spectrum and inside the finger using laser
    speckle contrast imaging (LSCI) technology. The SWIR feature is extracted by the
    ResNet and VGG19 network combined with the handcrafted feature extracted from
    the LSCI using BSIF, HOG, and LBP descriptors. Then, the fusion of the results
    obtained from the above techniques determines the final liveness score. Plesh
    et al. (Plesh et al., [2019](#bib.bib94)) proposes a novel approach that combines
    dynamic time-series features with static features extracted by the Inception-V3
    (Szegedy et al., [2016](#bib.bib114)) CNN model. Then the final result will be
    obtained by the fusion of two feature sets. The experiments demonstrated that
    the fusion of two feature sets achieves a better performance than the individual
    features. Kolberg et al. (Kolberg et al., [2021a](#bib.bib58)) also include SWIR
    and laser techniques to analyze spoofing. In contrast to (Gomez-Barrero et al.,
    [2019](#bib.bib34)), they select a long-term recurrent convolutional network (LRCN)
    (Donahue et al., [2015](#bib.bib20)), a pre‐trained CNN model, and an autoencoder
    network to independently obtain a liveness score from the laser image. Meanwhile,
    the CNN and AutoEncoder models process the image from the SWIR. Finally, subsequent
    score-fusion was applied to obtain the final score for classification.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Jomaa 等人 (M. Jomaa et al., [2020](#bib.bib71)) 利用心电图（ECG）信号以及深度特征共同做出决策。此外，Tolosana
    等人 (Tolosana et al., [2018](#bib.bib116)) 提出了一种基于多模型的方法，利用从 5 层残差网络、预训练的 Moblie-Net
    网络、预训练的 VGG 19 网络和 CNN 模型中分别提取的四种深度特征，并将深度特征与短波红外（SWIR）光谱捕获设备的光谱信号结合以共同确定活体性。Gomez
    等人 (Gomez-Barrero et al., [2019](#bib.bib34)) 分析了手指表面及内部的 SWIR 光谱，并使用激光散斑对比成像（LSCI）技术。SWIR
    特征由 ResNet 和 VGG19 网络提取，并结合从 LSCI 中提取的使用 BSIF、HOG 和 LBP 描述符的手工特征。然后，通过融合上述技术获得的结果来确定最终的活体分数。Plesh
    等人 (Plesh et al., [2019](#bib.bib94)) 提出了一种新方法，将动态时间序列特征与由 Inception-V3（Szegedy
    et al., [2016](#bib.bib114)) CNN 模型提取的静态特征结合。最终结果通过两个特征集的融合获得。实验表明，两个特征集的融合比单独的特征具有更好的性能。Kolberg
    等人 (Kolberg et al., [2021a](#bib.bib58)) 还包括 SWIR 和激光技术来分析欺骗行为。与 (Gomez-Barrero
    et al., [2019](#bib.bib34)) 相比，他们选择了长短期记忆卷积网络（LRCN）（Donahue et al., [2015](#bib.bib20)）、预训练的
    CNN 模型和自编码网络来独立从激光图像中获得活体分数。同时，CNN 和自编码模型处理来自 SWIR 的图像。最后，应用后续分数融合以获得分类的最终分数。
- en: '| Author | Year | Backbone | Loss function | Result |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 作者 | 年份 | 主干网络 | 损失函数 | 结果 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Tolosana et.al (Tolosana et al., [2018](#bib.bib116)) | 2018 | RenNet, MobileNet,
    VGG19, CNN | Binary CE loss | A multi-model approach that utilizes four deep features
    and SWIR image features |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Tolosana 等人 (Tolosana et al., [2018](#bib.bib116)) | 2018 | RenNet, MobileNet,
    VGG19, CNN | Binary CE 损失 | 利用四种深度特征和 SWIR 图像特征的多模型方法 |'
- en: '| Gomez et.al (Gomez-Barrero et al., [2019](#bib.bib34)) | 2019 | ResNet and
    VGG | Binary CE loss | SWIR features extracted by ResNet and VGG model and LSCI
    features extracted by handcrafted-based methods |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Gomez 等人 (Gomez-Barrero et al., [2019](#bib.bib34)) | 2019 | ResNet 和 VGG
    | Binary CE 损失 | 由 ResNet 和 VGG 模型提取的 SWIR 特征以及由手工方法提取的 LSCI 特征 |'
- en: '| Plesh et.al (Plesh et al., [2019](#bib.bib94)) | 2019 | Inception-V3 | Binary
    CE loss | Dynamic time-series feature with static feature extracted by Inception-V3
    model |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Plesh 等人 (Plesh et al., [2019](#bib.bib94)) | 2019 | Inception-V3 | Binary
    CE 损失 | 由 Inception-V3 模型提取的动态时间序列特征与静态特征 |'
- en: '| Jomaa et.al (M. Jomaa et al., [2020](#bib.bib71)) | 2020 | Mobilenet-v2 |
    Binary CE loss | Electrocardiogram(ECG) features and deep features |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Jomaa 等人 (M. Jomaa et al., [2020](#bib.bib71)) | 2020 | Mobilenet-v2 | Binary
    CE 损失 | 心电图（ECG）特征和深度特征 |'
- en: '| Kolberg et.al (Kolberg et al., [2021a](#bib.bib58)) | 2021 | Long‐term recurrent
    convolutional network(LRCN), CNN and AutoEncoder | Binary CE loss, reconstruction
    loss | Combine the liveness score obtained from laser image and SWIR images |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Kolberg et.al (Kolberg et al., [2021a](#bib.bib58)) | 2021 | 长期递归卷积网络（LRCN）、CNN
    和自编码器 | 二元 CE 损失，重构损失 | 结合从激光图像和 SWIR 图像中获得的活跃度评分 |'
- en: Table 10\. State-of-the-art hybrid FPAD methods
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10\. 最先进的混合 FPAD 方法
- en: 6\. Performance Evaluation Metrics
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 性能评估指标
- en: In this section, we discuss different evaluation metrics that are widely used
    in the fingerprint PAD literature. First, we present the metrics used in LivDet
    competitions (Marcialis et al., [2009](#bib.bib78)) followed by ISO/IEC using
    ISO/IEC 30107- 3 (ISO/IEC JTC1 SC37 Biometrics, [2017](#bib.bib47)) metrics.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了指纹 PAD 文献中广泛使用的不同评估指标。首先，我们介绍了 LivDet 比赛中使用的指标（Marcialis et al., [2009](#bib.bib78)），接着是使用
    ISO/IEC 30107-3（ISO/IEC JTC1 SC37 生物识别，[2017](#bib.bib47)）的 ISO/IEC 指标。
- en: 6.1\. Evaluation metrics from LivDet competitions
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. LivDet 比赛的评估指标
- en: 'since the first edition of Fingerprint Liveness Detection competition (LivDet)
    in 2009 (Marcialis et al., [2009](#bib.bib78)), the following performance evaluation
    metrics are used to benchmark the performance of the FPAD algorithms:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2009 年第一次举办指纹活跃度检测竞赛（LivDet）（Marcialis et al., [2009](#bib.bib78)）以来，以下性能评估指标用于基准测试
    FPAD 算法的性能：
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Frej: Rate of failure to enroll.'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Frej: 注册失败率。'
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fcorrlive: Rate of the live fingerprint to be classified correctly.'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Fcorrlive: 真实指纹被正确分类的率。'
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fcorrfake: Rate of the fake fingerprint to be classified correctly.'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Fcorrfake: 假指纹被正确分类的率。'
- en: •
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ferrlive: Rate of the live fingerprint to be misclassified.'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Ferrlive: 真实指纹被误分类的率。'
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ferrfake: Rate of the fake fingerprint to be misclassified.'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Ferrfake: 假指纹被误分类的率。'
- en: 'Additional evaluation metrics such as average classification error (ACE) are
    defined in (Chugh et al., [2017](#bib.bib16)):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的评估指标如平均分类错误（ACE）在（Chugh et al., [2017](#bib.bib16)）中定义：
- en: '| (1) |  | $ACE=\frac{Fcorrlive+Fcorrfake}{2}$ |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $ACE=\frac{Fcorrlive+Fcorrfake}{2}$ |  |'
- en: Starting from LivDet 2021 competition, evaluation metrics are defined according
    to the ISO/IEC 30107–1 standard presented below.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从 LivDet 2021 比赛开始，评估指标根据 ISO/IEC 30107–1 标准定义如下。
- en: 6.2\. ISO/IEC Metrics for PAD
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. ISO/IEC PAD 指标
- en: 'International Standard Organization (ISO/IEC 30107–1:2016) (ISO, [2016](#bib.bib3))
    has described the general framework to present the attack detection performance
    results. The ISO/IEC 30107–1 framework defined following metrics:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 国际标准化组织（ISO/IEC 30107–1:2016）（ISO，[2016](#bib.bib3)）描述了展示攻击检测性能结果的总体框架。ISO/IEC
    30107–1 框架定义了以下指标：
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Liveness Accuracy: Rate of samples correctly classified by the PAD system.'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 活跃度准确率：由 PAD 系统正确分类的样本率。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'APCER (Attack Presentation Classification Error Rate): Rate of fake fingerprints
    to be misclassified. The APCER for a given presentation attack instrument species(PAIS)
    is defined as:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: APCER（攻击展示分类错误率）：假指纹被误分类的率。给定展示攻击工具物种（PAIS）的 APCER 定义为：
- en: '| (2) |  | $APCER_{PAIS}=1-(\frac{1}{N_{PAIS}})\sum_{i=1}^{N_{PAIS}}RES_{i}$
    |  |'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (2) |  | $APCER_{PAIS}=1-(\frac{1}{N_{PAIS}})\sum_{i=1}^{N_{PAIS}}RES_{i}$
    |  |'
- en: $N_{PAIS}$ indicates the number of attack presentations for the given PAI species,
    $RES_{i}$ takes the value 1 if the $i^{th}$ presentation is classified as an attack
    presentation, and value 0 if classified as a bona fide presentation.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $N_{PAIS}$ 表示给定 PAI 物种的攻击展示次数，$RES_{i}$ 的值为 1 如果第 $i^{th}$ 次展示被分类为攻击展示，值为 0
    如果分类为真实展示。
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BPCER (Bona fide Presentation Classification Error Rate): Rate of the live
    fingerprint to be classified correctly. BPCER shall be calculated as:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BPCER（真实展示分类错误率）：真实指纹被正确分类的率。BPCER 计算方法为：
- en: '| (3) |  | $BPCER=\frac{\sum_{i=1}^{N_{BF}}RES_{i}}{N_{BF}}$ |  |'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (3) |  | $BPCER=\frac{\sum_{i=1}^{N_{BF}}RES_{i}}{N_{BF}}$ |  |'
- en: where $N_{BF}$ indicates the number of bona fide presentations. $RES_{i}$ takes
    the value 1 if the $i^{th}$ presentation is classified as an attack presentation
    and value 0 if classified as a bona fide presentation.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $N_{BF}$ 表示真实展示的数量。$RES_{i}$ 的值为 1 如果第 $i^{th}$ 次展示被分类为攻击展示，值为 0 如果分类为真实展示。
- en: 'D-EER (Detection Equal Error Rate) indicates that the APCER is equal to the
    BPCER. Normally, D-EER is valuable which stands for the performance of a biometric
    system. To further evaluate the performance of the integrated system, additional
    metrics are also defined:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: D-EER（检测等错误率）表示APCER等于BPCER。通常，D-EER是一个有价值的指标，代表生物识别系统的性能。为了进一步评估集成系统的性能，还定义了其他指标：
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FNMR (False Non-Match Rate): Rate of genuine fingerprints to be classified
    as an impostor.'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FNMR（虚假非匹配率）：真实指纹被分类为冒充者的比例。
- en: •
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FMR (False Match Rate): Rate of zero-effort impostors classified as genuine.'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FMR（虚假匹配率）：零努力冒充者被分类为真实的比例。
- en: •
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'IAPMR (Impostor Attack Presentation Match Rate): Rate of impostor attack presentations
    classified as genuine.'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IAPMR（冒充者攻击展示匹配率）：冒充者攻击展示被分类为真实的比例。
- en: •
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Integrated Matching Accuracy: Rate of samples correctly classified by the integrated
    system.'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成匹配准确率：集成系统正确分类的样本比例。
- en: 7\. Future Work
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 未来工作
- en: With the revolution of deep learning, training deep neural networks (DNN) has
    dominated the field of image classification and object recognition. This technique
    was further extended to FPAD methods, and achieved notable improvements in the
    detection of fabricated fingerprint replicas. However, there are still some limitations
    that can be considered and discussed. In this section, we introduce the major
    challenges of current research and future perspectives.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的革命，训练深度神经网络（DNN）已经主导了图像分类和物体识别领域。这项技术进一步扩展到FPAD方法，并在检测伪造指纹副本方面取得了显著的改进。然而，仍然存在一些可以考虑和讨论的局限性。在这一部分，我们介绍当前研究的主要挑战和未来展望。
- en: 7.1\. Generalization to unknown attack detection
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 对未知攻击检测的泛化
- en: Normally, a deep learning-based FPAD model takes both bona fide and attack samples
    for training so that the classifier can distinguish liveness based on the probability
    score calculated corresponding to the labels. However, these methods suffer from
    low generalization ability against PAIs not included in the training set. Another
    type of method is anomaly detection, which trains a one-class classifier based
    on only the bona fide samples to represent the real fingerprint images better
    to detect anomalies that achieve acceptable results against unknown attacks to
    some extent. In (Sandouka et al., [2021a](#bib.bib105)), the authors proposed
    a domain-adaptation approach that can generate mappings across sensors to reduce
    the distribution shift between different fingerprint representations. Based on
    this approach as a starting point, it is worth exploring how to make the model
    learn the mapping from a source domain to an unseen domain to achieve a general
    representation of the fake fingerprint.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基于深度学习的FPAD模型会同时使用真实样本和攻击样本进行训练，以便分类器可以根据计算出的概率分数区分活体。然而，这些方法在处理未包含在训练集中的PAI时往往泛化能力较低。另一种方法是异常检测，它基于真实样本训练一个单类分类器，以更好地表示真实指纹图像，从而检测异常，对某些未知攻击取得了可接受的结果。在（Sandouka等，[2021a](#bib.bib105)）中，作者提出了一种领域适应方法，该方法可以生成传感器之间的映射，以减少不同指纹表示之间的分布偏移。以这种方法为起点，值得探索如何使模型学习从源领域到未见领域的映射，以实现伪造指纹的通用表示。
- en: 7.2\. Interpretability to fingerprint presentation attack detection
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 对指纹展示攻击检测的可解释性
- en: The widely deployed AI application raised the interpretability issue of how
    to make the deep learning model explainable. Therefore, it is important to build
    ’Transparent’ models that have the ability to explain why they predict what they
    predict. In order to provide an analysis of the interpretation of FPAD methods
    using visualization techniques. Techniques such as (Zeiler and Fergus, [2014](#bib.bib126))
    (Selvaraju et al., [2017](#bib.bib107)) (Ancona et al., [2017](#bib.bib8)) can
    be used to highlight the region of an image that affects the final decision. Lie
    et.al (Liu et al., [2022](#bib.bib66)) include Grad-CAM to visualize the important
    regions related to the given label. It will be interesting to consider applying
    visualization, especially to images captured by multi-spectrum devices.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛部署的人工智能应用提出了如何使深度学习模型具有可解释性的问题。因此，建立能够解释为什么做出某种预测的“透明”模型非常重要。为了提供对 FPAD 方法的可解释性分析，可以使用可视化技术。例如，技术如
    (Zeiler 和 Fergus, [2014](#bib.bib126)) (Selvaraju 等, [2017](#bib.bib107)) (Ancona
    等, [2017](#bib.bib8)) 可以用来突出影响最终决策的图像区域。Lie 等 (Liu 等, [2022](#bib.bib66)) 包括 Grad-CAM
    来可视化与给定标签相关的重要区域。考虑将可视化技术应用于多谱段设备捕获的图像将会很有趣。
- en: 7.3\. Light-weight models for finger photo presentation attack detection
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 用于手指照片呈现攻击检测的轻量级模型
- en: With the rapid development of smartphone cameras, high-resolution fingerphotos
    can be captured efficiently and directly from a mobile device to aid reliable
    biometric authentication. Because mobile devices mostly do not have a high-computation
    environment, a lightweight deep learning model with fewer parameters and focus
    on only a small region of the fingerprint images would be an optimal solution.
    Hence, another perspective could be the presentation attack detection of smartphone-based
    approaches.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 随着智能手机摄像头的快速发展，高分辨率的手指照片可以有效地直接从移动设备上捕获，以辅助可靠的生物识别认证。由于移动设备大多没有高计算环境，一个具有较少参数且仅关注手指图像小区域的轻量级深度学习模型将是一个最佳解决方案。因此，另一个方向可以是针对智能手机的呈现攻击检测。
- en: 7.4\. Lack of large-scale publicly available dataset
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 缺乏大规模公开数据集
- en: As deep learning has shown a significant impact on the image classification
    domain, research on FPAD methods has concentrated on training a large-scale neural
    network to detect spoofing. Because of the requirement of the large scale of both
    bonafide and attack sample data using a deep learning model, there have been plenty
    of publicly available datasets published using different capture devices and spoofing
    materials. However, datasets comprising a large number of samples are still lacking.
    Especially in contactless finger photos, there is currently a lack of datasets
    containing bonafide and spoofing samples. A further perspective could be to produce
    large-scale finger photo presentation attack datasets comprised of various presentation
    attack instruments materials.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习对图像分类领域产生了显著影响，FPAD 方法的研究已经集中在训练大规模神经网络以检测欺骗行为。由于深度学习模型对真实样本和攻击样本数据的大规模要求，目前已经发布了大量使用不同捕获设备和欺骗材料的公开数据集。然而，包含大量样本的数据集仍然不足。特别是在非接触式手指照片中，目前缺乏包含真实和欺骗样本的数据集。一个进一步的方向可能是制作大规模的手指照片呈现攻击数据集，包括各种呈现攻击工具材料。
- en: 7.5\. Potential adversarial presentation attack
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5\. 潜在的对抗性呈现攻击
- en: An adversarial attack generates adversarial samples by purpose in order to mislead
    the image classification result of a machine learning model. One simple way to
    generate adversarial examples is to add a perturbation of some pixels so that
    the output image looks no different from the input image, but the classification
    result will be changed. Another notable observation was proposed by Casula et
    al. (Casula et al., [2021b](#bib.bib14)), who produce high-quality spoofs through
    the snapshot picture of a smartphone to obtain the fingerprint latent. Through
    digital processing, the spoof was fabricated using a transparent sheet. The experiment
    indicated that this ScreenSpoof presents a threat at the same level as a normal
    presentation attack. Hence, Marrone et al. (Marrone et al., [2021](#bib.bib79))
    investigate the feasibility of adopting an adversarial attack on a physical domain
    by materially realizing a fake image based on an adversarial fingerprint example.
    The evaluation of the attack indicates that printed adversarial images exhibit
    a high attack rate with multiple attacks and fairly good results with a one-shot
    attack. According to this, it should be considered that with adversarial examples
    targeting the FPAD module, it is possible to combine other digital attacks(Masterprint,
    morphing. etc.) as well as adversarial perturbation to fool the FPAD system to
    perform more dangerous attacks on the FRS system. Hence, it is interesting to
    exploring countermeasures against emerging adversarial attacks.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击通过故意生成对抗样本以误导机器学习模型的图像分类结果。生成对抗示例的一种简单方法是添加一些像素的扰动，使输出图像看起来与输入图像没有区别，但分类结果会发生变化。另一个值得注意的观察是由
    Casula 等人提出的 (Casula et al., [2021b](#bib.bib14))，他们通过智能手机的快照照片生成高质量的欺骗图像以获取指纹隐含信息。通过数字处理，这些欺骗图像使用透明薄膜伪造。实验表明，这种
    ScreenSpoof 的威胁等级与正常的呈现攻击相同。因此，Marrone 等人 (Marrone et al., [2021](#bib.bib79))
    研究了在物理领域采用对抗性攻击的可行性，通过实际实现基于对抗性指纹示例的假图像。攻击评估表明，打印的对抗性图像显示出高攻击率和多个攻击，同时单次攻击的结果也相当不错。根据这些结果，考虑到针对
    FPAD 模块的对抗性示例，可能会结合其他数字攻击（如 Masterprint、变形等）以及对抗性扰动，以欺骗 FPAD 系统，从而对 FRS 系统进行更危险的攻击。因此，探索对抗性攻击的对策是非常有趣的。
- en: 8\. Conclusion
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: This article provides a comprehensive overview on the common fingerprint presentation
    attack instruments and presentation attack detection techniques that are widely
    employed in both contact and contactless fingerprint biometrics. A brief introduction
    to publicly available databases and relevant standards regarding FPAD evaluation
    metrics are presented. Then a comprehensive literature review of the existing
    state-of-the-art deep-learning-based FPAD methods is presented, which covers contact,
    contactless, and smartphone-based approaches. Finally, potential future perspectives
    are discussed to motivate future research. Overall, this study provides an intuitive
    guide to researchers interested in this area.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了关于常见指纹呈现攻击工具和广泛应用于接触式和非接触式指纹生物识别的呈现攻击检测技术的全面概述。介绍了公开可用的数据库和关于 FPAD 评估指标的相关标准。接着，全面回顾了现有的最先进的基于深度学习的
    FPAD 方法，涵盖了接触式、非接触式以及基于智能手机的方法。最后，讨论了潜在的未来发展方向，以激励未来的研究。总体而言，本研究为对该领域感兴趣的研究人员提供了直观的指南。
- en: Acknowledgements.
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: This work is supported by OFFPAD project funded by the Research Council of Norway.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到挪威研究委员会资助的OFFPAD项目支持。
- en: References
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: AFI (2003) 2003. Automated Fingerprint Identification System. [https://www.innovatrics.com/glossary/afis-automated-fingerprint-identification-system/](https://www.innovatrics.com/glossary/afis-automated-fingerprint-identification-system/).
    Innovatrics.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AFI (2003) 2003. 自动指纹识别系统。 [https://www.innovatrics.com/glossary/afis-automated-fingerprint-identification-system/](https://www.innovatrics.com/glossary/afis-automated-fingerprint-identification-system/)。Innovatrics。
- en: 'ISO (2016) 2016. *Information technology — Biometric presentation attack detection
    — Part 1: Framework*. Standard ISO/IEC 30107-1:2016. International Organization
    for Standardization, Geneva, CH. [https://www.iso.org/standard/53227.html](https://www.iso.org/standard/53227.html)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ISO (2016) 2016. *信息技术 — 生物识别呈现攻击检测 — 第 1 部分：框架*。标准 ISO/IEC 30107-1:2016。国际标准化组织，日内瓦，瑞士。
    [https://www.iso.org/standard/53227.html](https://www.iso.org/standard/53227.html)
- en: Liv (2022) 2022. LIVDET. [https://livdet.diee.unica.it](https://livdet.diee.unica.it).
    Spoofing Adhaar.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liv (2022) 2022. LIVDET. [https://livdet.diee.unica.it](https://livdet.diee.unica.it).
    伪造 Adhaar。
- en: Hac (2022) 2022. Spoofing Adhaar. [http://timesofindia.indiatimes.com/articleshow/83324403.cms?utm_source=contentofinterest&utm_medium=text&utm_campaign=cppst/](http://timesofindia.indiatimes.com/articleshow/83324403.cms?utm_source=contentofinterest&utm_medium=text&utm_campaign=cppst/).
    Spoofing Adhaar.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hac (2022) 2022. 伪造 Adhaar。 [http://timesofindia.indiatimes.com/articleshow/83324403.cms?utm_source=contentofinterest&utm_medium=text&utm_campaign=cppst/](http://timesofindia.indiatimes.com/articleshow/83324403.cms?utm_source=contentofinterest&utm_medium=text&utm_campaign=cppst/).
    伪造 Adhaar。
- en: Al-Ajlan (2013) Amani Al-Ajlan. 2013. Survey on fingerprint liveness detection.
    In *2013 International Workshop on Biometrics and Forensics (IWBF)*. IEEE, 1–5.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Ajlan (2013) Amani Al-Ajlan. 2013. 指纹活体检测综述。载于 *2013 国际生物识别与取证研讨会 (IWBF)*。IEEE，第
    1–5 页。
- en: 'Ametefe et al. (2022) DS Ametefe, SS Sarnin, DM Ali, and MZ Zaheer. 2022. Fingerprint
    Liveness Detection Schemes: A Review on Presentation Attack. *Computer Methods
    in Biomechanics and Biomedical Engineering: Imaging & Visualization* 10, 2 (2022),
    217–240.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ametefe et al. (2022) DS Ametefe, SS Sarnin, DM Ali, 和 MZ Zaheer. 2022. 指纹活体检测方案：对展示攻击的综述。*计算方法在生物力学和生物医学工程中的应用：成像与可视化*
    10, 2 (2022), 第 217–240 页。
- en: Ancona et al. (2017) Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus
    Gross. 2017. Towards better understanding of gradient-based attribution methods
    for deep neural networks. *arXiv preprint arXiv:1711.06104* (2017).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ancona et al. (2017) Marco Ancona, Enea Ceolini, Cengiz Öztireli, 和 Markus Gross.
    2017. 朝着更好理解基于梯度的深度神经网络归因方法。*arXiv 预印本 arXiv:1711.06104* (2017)。
- en: Antonelli et al. (2006) Athos Antonelli, Raffaele Cappelli, Dario Maio, and
    Davide Maltoni. 2006. Fake finger detection by skin distortion analysis. *IEEE
    Transactions on Information Forensics and Security* 1, 3 (2006), 360–373.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antonelli et al. (2006) Athos Antonelli, Raffaele Cappelli, Dario Maio, 和 Davide
    Maltoni. 2006. 通过皮肤变形分析检测假指纹。*IEEE 信息取证与安全学报* 1, 3 (2006), 第 360–373 页。
- en: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017.
    Wasserstein generative adversarial networks. In *International conference on machine
    learning*. PMLR, 214–223.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, 和 Léon Bottou. 2017.
    Wasserstein 生成对抗网络。载于 *国际机器学习大会*。PMLR，第 214–223 页。
- en: 'Bontrager et al. (2018) Philip Bontrager, Aditi Roy, Julian Togelius, Nasir
    Memon, and Arun Ross. 2018. Deepmasterprints: Generating masterprints for dictionary
    attacks via latent variable evolution. In *2018 IEEE 9th International Conference
    on Biometrics Theory, Applications and Systems (BTAS)*. IEEE, 1–9.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bontrager et al. (2018) Philip Bontrager, Aditi Roy, Julian Togelius, Nasir
    Memon, 和 Arun Ross. 2018. Deepmasterprints：通过潜变量演化生成字典攻击的主打印。载于 *2018 IEEE 第 9
    届国际生物识别理论、应用与系统大会 (BTAS)*。IEEE，第 1–9 页。
- en: Bossen et al. (2010) Anke Bossen, Roland Lehmann, and Christoph Meier. 2010.
    Internal fingerprint identification with optical coherence tomography. *IEEE photonics
    technology letters* 22, 7 (2010), 507–509.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bossen et al. (2010) Anke Bossen, Roland Lehmann, 和 Christoph Meier. 2010. 采用光学相干断层扫描进行内部指纹识别。*IEEE
    光子技术快报* 22, 7 (2010), 第 507–509 页。
- en: Casula et al. (2021a) Roberto Casula, Marco Micheletto, Giulia Orrù, Rita Delussu,
    Sara Concas, Andrea Panzino, and Gian Luca Marcialis. 2021a. LivDet 2021 fingerprint
    liveness detection competition-into the unknown. In *2021 IEEE International Joint
    Conference on Biometrics (IJCB)*. IEEE, 1–6.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casula et al. (2021a) Roberto Casula, Marco Micheletto, Giulia Orrù, Rita Delussu,
    Sara Concas, Andrea Panzino, 和 Gian Luca Marcialis. 2021a. LivDet 2021 指纹活体检测比赛——未知领域。载于
    *2021 IEEE 国际联合生物识别会议 (IJCB)*。IEEE，第 1–6 页。
- en: Casula et al. (2021b) Roberto Casula, Giulia Orrù, Daniele Angioni, Xiaoyi Feng,
    Gian Luca Marcialis, and Fabio Roli. 2021b. Are spoofs from latent fingerprints
    a real threat for the best state-of-art liveness detectors?. In *2020 25th International
    Conference on Pattern Recognition (ICPR)*. IEEE, 3412–3418.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casula et al. (2021b) Roberto Casula, Giulia Orrù, Daniele Angioni, Xiaoyi Feng,
    Gian Luca Marcialis, 和 Fabio Roli. 2021b. 潜在指纹的伪造是否对最先进的活体检测器构成真实威胁？载于 *2020 第
    25 届国际模式识别大会 (ICPR)*。IEEE，第 3412–3418 页。
- en: Cheng and Larin (2007) Yezeng Cheng and Kirill V Larin. 2007. In vivo two-and
    three-dimensional imaging of artificial and real fingerprints with optical coherence
    tomography. *IEEE Photonics Technology Letters* 19, 20 (2007), 1634–1636.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng and Larin (2007) Yezeng Cheng 和 Kirill V Larin. 2007. 使用光学相干断层扫描对人工和真实指纹进行体内二维和三维成像。*IEEE
    光子技术快报* 19, 20 (2007), 第 1634–1636 页。
- en: Chugh et al. (2017) Tarang Chugh, Kai Cao, and Anil K Jain. 2017. Fingerprint
    spoof detection using minutiae-based local patches. In *2017 IEEE International
    Joint Conference on Biometrics (IJCB)*. IEEE, 581–589.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chugh et al. (2017) Tarang Chugh, Kai Cao, 和 Anil K Jain. 2017. 使用基于细节的局部补丁进行指纹伪造检测。在
    *2017 IEEE 国际联合生物识别会议 (IJCB)* 中。IEEE, 581–589.
- en: 'Chugh et al. (2018) Tarang Chugh, Kai Cao, and Anil K Jain. 2018. Fingerprint
    spoof buster: Use of minutiae-centered patches. *IEEE Transactions on Information
    Forensics and Security* 13, 9 (2018), 2190–2202.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chugh et al. (2018) Tarang Chugh, Kai Cao, 和 Anil K Jain. 2018. 指纹伪造破解器：使用中心化细节补丁。*IEEE
    信息取证与安全期刊* 13, 9 (2018), 2190–2202.
- en: Chugh and Jain (2019) Tarang Chugh and Anil K Jain. 2019. Fingerprint spoof
    generalization. *arXiv preprint arXiv:1912.02710* (2019).
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chugh and Jain (2019) Tarang Chugh 和 Anil K Jain. 2019. 指纹伪造泛化。*arXiv 预印本 arXiv:1912.02710*
    (2019).
- en: Chugh and Jain (2020) Tarang Chugh and Anil K Jain. 2020. Fingerprint spoof
    detector generalization. *IEEE Transactions on Information Forensics and Security*
    16 (2020), 42–55.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chugh and Jain (2020) Tarang Chugh 和 Anil K Jain. 2020. 指纹伪造检测器的泛化。*IEEE 信息取证与安全期刊*
    16 (2020), 42–55.
- en: Donahue et al. (2015) Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
    Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2015.
    Long-term recurrent convolutional networks for visual recognition and description.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    2625–2634.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donahue et al. (2015) Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,
    Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, 和 Trevor Darrell. 2015.
    用于视觉识别和描述的长期递归卷积网络。在 *IEEE 计算机视觉与模式识别会议论文集* 中，2625–2634.
- en: Drahansky et al. (2006) Martin Drahansky, Ralf Notzel, and Wolfgang Funk. 2006.
    Liveness detection based on fine movements of the fingertip surface. In *2006
    IEEE Information Assurance Workshop*. IEEE, 42–47.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drahansky et al. (2006) Martin Drahansky, Ralf Notzel, 和 Wolfgang Funk. 2006.
    基于指尖表面微小运动的活体检测。在 *2006 IEEE 信息保障研讨会* 中。IEEE, 42–47.
- en: 'Engelsma et al. (2022) Joshua J Engelsma, Steven A Grosz, and Anil K Jain.
    2022. PrintsGAN: synthetic fingerprint generator. *arXiv preprint arXiv:2201.03674*
    (2022).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Engelsma et al. (2022) Joshua J Engelsma, Steven A Grosz, 和 Anil K Jain. 2022.
    PrintsGAN: 合成指纹生成器。*arXiv 预印本 arXiv:2201.03674* (2022).'
- en: 'Engelsma and Jain (2019) Joshua J Engelsma and Anil K Jain. 2019. Generalizing
    fingerprint spoof detector: Learning a one-class classifier. In *2019 International
    Conference on Biometrics (ICB)*. IEEE, 1–8.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engelsma and Jain (2019) Joshua J Engelsma 和 Anil K Jain. 2019. 泛化指纹伪造检测器：学习单类分类器。在
    *2019 国际生物识别会议 (ICB)* 中。IEEE, 1–8.
- en: Espinoza et al. (2011) Marcela Espinoza, Christophe Champod, and Pierre Margot.
    2011. Vulnerabilities of fingerprint reader to fake fingerprints attacks. *Forensic
    science international* 204, 1-3 (2011), 41–49.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Espinoza et al. (2011) Marcela Espinoza, Christophe Champod, 和 Pierre Margot.
    2011. 指纹读取器对伪造指纹攻击的脆弱性。*法医学国际* 204, 1-3 (2011), 41–49.
- en: Ferrara et al. (2016) Matteo Ferrara, Raffaele Cappelli, and Davide Maltoni.
    2016. On the feasibility of creating double-identity fingerprints. *IEEE Transactions
    on Information Forensics and Security* 12, 4 (2016), 892–900.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrara et al. (2016) Matteo Ferrara, Raffaele Cappelli, 和 Davide Maltoni. 2016.
    创建双重身份指纹的可行性研究。*IEEE 信息取证与安全期刊* 12, 4 (2016), 892–900.
- en: Fujio et al. (2018) Masakazu Fujio, Yosuke Kaga, Takao Murakami, Tetsushi Ohki,
    and Kenta Takahashi. 2018. Face/Fingerphoto Spoof Detection under Noisy Conditions
    by using Deep Convolutional Neural Network.. In *BIOSIGNALS*. 54–62.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fujio et al. (2018) Masakazu Fujio, Yosuke Kaga, Takao Murakami, Tetsushi Ohki,
    和 Kenta Takahashi. 2018. 使用深度卷积神经网络在噪声条件下进行面部/指纹照片伪造检测。在 *BIOSIGNALS* 中，54–62.
- en: 'Gajawada et al. (2019) Rohit Gajawada, Additya Popli, Tarang Chugh, Anoop Namboodiri,
    and Anil K Jain. 2019. Universal material translator: Towards spoof fingerprint
    generalization. In *2019 International Conference on Biometrics (ICB)*. IEEE,
    1–8.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gajawada et al. (2019) Rohit Gajawada, Additya Popli, Tarang Chugh, Anoop Namboodiri,
    和 Anil K Jain. 2019. 通用材料翻译器：迈向指纹伪造泛化。在 *2019 国际生物识别会议 (ICB)* 中。IEEE, 1–8.
- en: Galbally et al. (2011) Javier Galbally, Julian Fierrez, Fernando Alonso-Fernandez,
    and Marcos Martinez-Diaz. 2011. Evaluation of direct attacks to fingerprint verification
    systems. *Telecommunication Systems* 47 (2011), 243–254.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galbally et al. (2011) Javier Galbally, Julian Fierrez, Fernando Alonso-Fernandez,
    和 Marcos Martinez-Diaz. 2011. 对指纹验证系统直接攻击的评估。*电信系统* 47 (2011), 243–254.
- en: Gatys et al. (2015) Leon A Gatys, Alexander S Ecker, and Matthias Bethge. 2015.
    A neural algorithm of artistic style. *arXiv preprint arXiv:1508.06576* (2015).
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gatys et al. (2015) Leon A Gatys, Alexander S Ecker, 和 Matthias Bethge. 2015.
    艺术风格的神经算法。*arXiv 预印本 arXiv:1508.06576* (2015).
- en: Géron (2022) Aurélien Géron. 2022. *Hands-on machine learning with Scikit-Learn,
    Keras, and TensorFlow*. ” O’Reilly Media, Inc.”.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Géron（2022）Aurélien Géron。2022年。*Scikit-Learn、Keras 和 TensorFlow 实践机器学习*。O’Reilly
    Media, Inc.。
- en: 'Ghiani et al. (2013a) Luca Ghiani, Abdenour Hadid, Gian Luca Marcialis, and
    Fabio Roli. 2013a. Fingerprint liveness detection using binarized statistical
    image features. In *2013 IEEE sixth international conference on biometrics: theory,
    applications and systems (BTAS)*. IEEE, 1–6.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghiani 等（2013a）Luca Ghiani、Abdenour Hadid、Gian Luca Marcialis 和 Fabio Roli。2013a。基于二值化统计图像特征的指纹活体检测。在
    *2013 IEEE 第六届国际生物识别会议：理论、应用与系统（BTAS）*。IEEE，1–6。
- en: Ghiani et al. (2012) Luca Ghiani, Gian Luca Marcialis, and Fabio Roli. 2012.
    Fingerprint liveness detection by local phase quantization. In *Proceedings of
    the 21st international conference on pattern recognition (ICPR2012)*. IEEE, 537–540.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghiani 等（2012）Luca Ghiani、Gian Luca Marcialis 和 Fabio Roli。2012年。通过局部相位量化进行指纹活体检测。在
    *第21届国际模式识别大会（ICPR2012）* 论文集。IEEE，537–540。
- en: Ghiani et al. (2013b) Luca Ghiani, David Yambay, Valerio Mura, Simona Tocco,
    Gian Luca Marcialis, Fabio Roli, and Stephanie Schuckcrs. 2013b. Livdet 2013 fingerprint
    liveness detection competition 2013\. In *2013 International Conference on Biometrics
    (ICB)*. IEEE, 1–6.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghiani 等（2013b）Luca Ghiani、David Yambay、Valerio Mura、Simona Tocco、Gian Luca
    Marcialis、Fabio Roli 和 Stephanie Schuckcrs。2013b。Livdet 2013 指纹活体检测竞赛 2013。在 *2013年国际生物识别会议（ICB）*。IEEE，1–6。
- en: 'Gomez-Barrero et al. (2019) Marta Gomez-Barrero, Jascha Kolberg, and Christoph
    Busch. 2019. Multi-modal fingerprint presentation attack detection: Analysing
    the surface and the inside. In *2019 International Conference on Biometrics (ICB)*.
    IEEE, 1–8.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gomez-Barrero 等（2019）Marta Gomez-Barrero、Jascha Kolberg 和 Christoph Busch。2019年。多模态指纹伪造攻击检测：分析表面与内部。在
    *2019年国际生物识别会议（ICB）*。IEEE，1–8。
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. *Advances in neural information processing systems*
    27 (2014).
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2014）Ian Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David
    Warde-Farley、Sherjil Ozair、Aaron Courville 和 Yoshua Bengio。2014年。生成对抗网络。*神经信息处理系统进展*
    27（2014）。
- en: Gragnaniello et al. (2013) Diego Gragnaniello, Giovanni Poggi, Carlo Sansone,
    and Luisa Verdoliva. 2013. Fingerprint liveness detection based on weber local
    image descriptor. In *2013 IEEE workshop on biometric measurements and systems
    for security and medical applications*. IEEE, 46–50.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gragnaniello 等（2013）Diego Gragnaniello、Giovanni Poggi、Carlo Sansone 和 Luisa
    Verdoliva。2013年。基于 Weber 局部图像描述符的指纹活体检测。在 *2013 IEEE 生物测量与安全及医疗应用研讨会*。IEEE，46–50。
- en: Gragnaniello et al. (2015) Diego Gragnaniello, Giovanni Poggi, Carlo Sansone,
    and Luisa Verdoliva. 2015. Local contrast phase descriptor for fingerprint liveness
    detection. *Pattern Recognition* 48, 4 (2015), 1050–1058.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gragnaniello 等（2015）Diego Gragnaniello、Giovanni Poggi、Carlo Sansone 和 Luisa
    Verdoliva。2015年。用于指纹活体检测的局部对比相位描述符。*模式识别* 48，4（2015），1050–1058。
- en: 'Grosz and Jain (2022) Steven A Grosz and Anil K Jain. 2022. Spoofgan: Synthetic
    fingerprint spoof images. *IEEE Transactions on Information Forensics and Security*
    18 (2022), 730–743.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grosz 和 Jain（2022）Steven A Grosz 和 Anil K Jain。2022年。Spoofgan：合成指纹伪造图像。*IEEE
    信息取证与安全学报* 18（2022），730–743。
- en: Guo et al. (2010) Zhenhua Guo, Lei Zhang, and David Zhang. 2010. A completed
    modeling of local binary pattern operator for texture classification. *IEEE transactions
    on image processing* 19, 6 (2010), 1657–1663.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2010）Zhenhua Guo、Lei Zhang 和 David Zhang。2010年。局部二值模式操作符在纹理分类中的完整建模。*IEEE
    图像处理学报* 19，6（2010），1657–1663。
- en: 'Habib and Selwal (2021) A Habib and A Selwal. 2021. Robust anti-spoofing techniques
    for fingerprint liveness detection: A Survey. In *IOP Conference Series: Materials
    Science and Engineering*, Vol. 1033\. IOP Publishing, 012026.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Habib 和 Selwal（2021）A Habib 和 A Selwal。2021年。用于指纹活体检测的鲁棒抗伪造技术：综述。在 *IOP 会议系列：材料科学与工程*，第1033卷。IOP
    出版，012026。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2016）Kaiming He、Xiangyu Zhang、Shaoqing Ren 和 Jian Sun。2016年。用于图像识别的深度残差学习。在
    *IEEE 计算机视觉与模式识别会议论文集*。770–778。
- en: 'Howard et al. (2017) Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
    Efficient convolutional neural networks for mobile vision applications. *arXiv
    preprint arXiv:1704.04861* (2017).'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 等（2017）Andrew G Howard、Menglong Zhu、Bo Chen、Dmitry Kalenichenko、Weijun
    Wang、Tobias Weyand、Marco Andreetto 和 Hartwig Adam。2017年。Mobilenets：用于移动视觉应用的高效卷积神经网络。*arXiv
    预印本 arXiv:1704.04861*（2017年）。
- en: Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. 2017. Densely connected convolutional networks. In *Proceedings of
    the IEEE conference on computer vision and pattern recognition*. 4700–4708.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2017）Gao Huang、Zhuang Liu、Laurens Van Der Maaten 和 Kilian Q Weinberger。2017年。密集连接的卷积网络。发表于
    *IEEE 计算机视觉与模式识别会议论文集*。4700–4708。
- en: Hussein et al. (2018) Mohamed E Hussein, Leonidas Spinoulas, Fei Xiong, and
    Wael Abd-Almageed. 2018. Fingerprint presentation attack detection using a novel
    multi-spectral capture device and patch-based convolutional neural networks. In
    *2018 IEEE international workshop on information forensics and security (WIFS)*.
    IEEE, 1–8.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hussein 等（2018）Mohamed E Hussein、Leonidas Spinoulas、Fei Xiong 和 Wael Abd-Almageed。2018年。使用新型多光谱捕捉设备和基于补丁的卷积神经网络进行指纹展示攻击检测。发表于
    *2018 IEEE 国际信息取证与安全研讨会（WIFS）*。IEEE，1–8。
- en: 'Iandola et al. (2016) Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid
    Ashraf, William J Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level accuracy
    with 50x fewer parameters and¡ 0.5 MB model size. *arXiv preprint arXiv:1602.07360*
    (2016).'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iandola 等（2016）Forrest N Iandola、Song Han、Matthew W Moskewicz、Khalid Ashraf、William
    J Dally 和 Kurt Keutzer。2016年。SqueezeNet：AlexNet 级别的准确性，参数减少 50 倍，模型大小仅为 0.5 MB。*arXiv
    预印本 arXiv:1602.07360*（2016年）。
- en: 'Ioffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. 2015. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In *International
    conference on machine learning*. PMLR, 448–456.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe 和 Szegedy（2015）Sergey Ioffe 和 Christian Szegedy。2015年。批量归一化：通过减少内部协变量偏移来加速深度网络训练。发表于
    *国际机器学习会议*。PMLR，448–456。
- en: 'ISO/IEC JTC1 SC37 Biometrics (2017) ISO/IEC JTC1 SC37 Biometrics. 2017. *ISO/IEC
    30107-3\. Information Technology - Biometric presentation attack detection - Part
    3: Testing and Reporting*. International Organization for Standardization.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ISO/IEC JTC1 SC37 生物识别（2017）ISO/IEC JTC1 SC37 生物识别。2017年。*ISO/IEC 30107-3\.
    信息技术 - 生物识别展示攻击检测 - 第3部分：测试与报告*。国际标准化组织。
- en: Jang et al. (2017) Han-Ul Jang, Hak-Yeol Choi, Dongkyu Kim, Jeongho Son, and
    Heung-Kyu Lee. 2017. Fingerprint spoof detection using contrast enhancement and
    convolutional neural networks. In *International Conference on Information Science
    and Applications*. Springer, 331–338.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等（2017）Han-Ul Jang、Hak-Yeol Choi、Dongkyu Kim、Jeongho Son 和 Heung-Kyu Lee。2017年。使用对比度增强和卷积神经网络进行指纹欺骗检测。发表于
    *信息科学与应用国际会议*。Springer，331–338。
- en: 'Jia et al. (2007) Jia Jia, Lianhong Cai, Kaifu Zhang, and Dawei Chen. 2007.
    A new approach to fake finger detection based on skin elasticity analysis. In
    *Advances in Biometrics: International Conference, ICB 2007, Seoul, Korea, August
    27-29, 2007\. Proceedings*. Springer, 309–318.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等（2007）Jia Jia、Lianhong Cai、Kaifu Zhang 和 Dawei Chen。2007年。一种基于皮肤弹性分析的新型假指检测方法。发表于
    *生物识别技术进展：国际会议，ICB 2007，韩国首尔，2007年8月27-29日\. 会议论文集*。Springer，309–318。
- en: Jian et al. (2020) WEN Jian, Yujie Zhou, and Hongming Liu. 2020. Densely connected
    convolutional network optimized by genetic algorithm for fingerprint liveness
    detection. *IEEE Access* 9 (2020), 2229–2243.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jian 等（2020）WEN Jian、Yujie Zhou 和 Hongming Liu。2020年。通过遗传算法优化的密集连接卷积网络用于指纹活体检测。*IEEE
    Access* 9（2020年），2229–2243。
- en: Jung and Heo (2018) HY Jung and YS Heo. 2018. Fingerprint liveness map construction
    using convolutional neural network. *Electronics Letters* 54, 9 (2018), 564–566.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung 和 Heo（2018）HY Jung 和 YS Heo。2018年。使用卷积神经网络构建指纹活体图。*电子快报* 54, 9（2018年），564–566。
- en: Jung et al. (2019) Ho Yub Jung, Yong Seok Heo, and Soochahn Lee. 2019. Fingerprint
    liveness detection by a template-probe convolutional neural network. *IEEE Access*
    7 (2019), 118986–118993.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung 等（2019）Ho Yub Jung、Yong Seok Heo 和 Soochahn Lee。2019年。通过模板-探测卷积神经网络进行指纹活体检测。*IEEE
    Access* 7（2019年），118986–118993。
- en: Kanich et al. (2018) Ondřej Kanich, Martin Drahanskỳ, and Martin Mézl. 2018.
    Use of creative materials for fingerprint spoofs. In *2018 International Workshop
    on Biometrics and Forensics (IWBF)*. IEEE, 1–8.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanich 等（2018）Ondřej Kanich、Martin Drahanskỳ 和 Martin Mézl。2018年。使用创意材料进行指纹欺骗。发表于
    *2018 国际生物识别与取证研讨会（IWBF）*。IEEE，1–8。
- en: 'Kannala and Rahtu (2012) Juho Kannala and Esa Rahtu. 2012. Bsif: Binarized
    statistical image features. In *Proceedings of the 21st international conference
    on pattern recognition (ICPR2012)*. IEEE, 1363–1366.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kannala 和 Rahtu（2012）Juho Kannala 和 Esa Rahtu。2012。Bsif：二值化统计图像特征。见于 *第21届国际模式识别大会（ICPR2012）论文集*。IEEE，1363–1366。
- en: Karras et al. (2019) Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based
    generator architecture for generative adversarial networks. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*. 4401–4410.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等人（2019）Tero Karras、Samuli Laine 和 Timo Aila。2019。基于风格的生成对抗网络生成器架构。见于
    *IEEE/CVF计算机视觉与模式识别会议论文集*。4401–4410。
- en: Kim et al. (2019) Hakil Kim, Xuenan Cui, Man-Gyu Kim, and Thi Hai Binh Nguyen.
    2019. Fingerprint generation and presentation attack detection using deep neural
    networks. In *2019 IEEE Conference on Multimedia Information Processing and Retrieval
    (MIPR)*. IEEE, 375–378.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2019）Hakil Kim、Xuenan Cui、Man-Gyu Kim 和 Thi Hai Binh Nguyen。2019。使用深度神经网络进行指纹生成和呈现攻击检测。见于
    *2019 IEEE 多媒体信息处理与检索会议（MIPR）*。IEEE，375–378。
- en: Kim et al. (2016) Soowoong Kim, Bogun Park, Bong Seop Song, and Seungjoon Yang.
    2016. Deep belief network based statistical feature learning for fingerprint liveness
    detection. *Pattern Recognition Letters* 77 (2016), 58–65.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2016）Soowoong Kim、Bogun Park、Bong Seop Song 和 Seungjoon Yang。2016。基于深度信念网络的指纹活跃度检测统计特征学习。*模式识别快报*
    77（2016），58–65。
- en: Kolberg et al. (2021a) Jascha Kolberg, Marta Gomez-Barrero, and Christoph Busch.
    2021a. On the generalisation capabilities of fingerprint presentation attack detection
    methods in the short wave infrared domain. *IET Biometrics* 10, 4 (2021), 359–373.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolberg 等人（2021a）Jascha Kolberg、Marta Gomez-Barrero 和 Christoph Busch。2021a。短波红外域下指纹呈现攻击检测方法的泛化能力。*IET
    生物识别* 10，4（2021），359–373。
- en: Kolberg et al. (2021b) Jascha Kolberg, Marcel Grimmer, Marta Gomez-Barrero,
    and Christoph Busch. 2021b. Anomaly detection with convolutional autoencoders
    for fingerprint presentation attack detection. *IEEE Transactions on Biometrics,
    Behavior, and Identity Science* 3, 2 (2021), 190–202.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolberg 等人（2021b）Jascha Kolberg、Marcel Grimmer、Marta Gomez-Barrero 和 Christoph
    Busch。2021b。利用卷积自编码器进行指纹呈现攻击检测的异常检测。*IEEE 生物识别、行为和身份科学汇刊* 3，2（2021），190–202。
- en: 'Kolberg et al. (2023) Jascha Kolberg, Jannis Priesnitz, Christian Rathgeb,
    and Christoph Busch. 2023. COLFISPOOF: A New Database for Contactless Fingerprint
    Presentation Attack Detection Research. In *Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision*. 653–661.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolberg 等人（2023）Jascha Kolberg、Jannis Priesnitz、Christian Rathgeb 和 Christoph
    Busch。2023。COLFISPOOF：一个用于无接触指纹呈现攻击检测研究的新数据库。见于 *IEEE/CVF 冬季计算机视觉应用会议论文集*。653–661。
- en: Kolberg et al. (2020) Jascha Kolberg, Alexandru-Cosmin Vasile, Marta Gomez-Barrero,
    and Christoph Busch. 2020. Analysing the performance of LSTMs and CNNs on 1310
    nm laser data for fingerprint presentation attack detection. In *2020 IEEE International
    Joint Conference on Biometrics (IJCB)*. IEEE, 1–7.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolberg 等人（2020）Jascha Kolberg、Alexandru-Cosmin Vasile、Marta Gomez-Barrero 和
    Christoph Busch。2020。分析 LSTM 和 CNN 在 1310 nm 激光数据上的指纹呈现攻击检测性能。见于 *2020 IEEE 国际生物识别联合会议（IJCB）*。IEEE，1–7。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. In *Advances
    in Neural Information Processing Systems 25*, F. Pereira, C. J. C. Burges, L. Bottou,
    and K. Q. Weinberger (Eds.). Curran Associates, Inc., 1097–1105.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人（2012）Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E Hinton。2012。使用深度卷积神经网络进行
    ImageNet 分类。见于 *神经信息处理系统进展 25*，F. Pereira、C. J. C. Burges、L. Bottou 和 K. Q. Weinberger（编）。Curran
    Associates, Inc.，1097–1105。
- en: Kulkarni and Patil (2015) Samruddhi S Kulkarni and Hemprasad Y Patil. 2015.
    Survey on fingerprint spoofing detection techniques and databases. *International
    Journal of Computer Applications* 975 (2015), 8887.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 和 Patil（2015）Samruddhi S Kulkarni 和 Hemprasad Y Patil。2015。关于指纹欺骗检测技术和数据库的综述。*国际计算机应用期刊*
    975（2015），8887。
- en: Lazimul and Binoy (2017) Limnd TP Lazimul and DL Binoy. 2017. Fingerprint liveness
    detection using convolutional neural network and fingerprint image enhancement.
    In *2017 International Conference on Energy, Communication, Data Analytics and
    Soft Computing (ICECDS)*. IEEE, 731–735.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lazimul 和 Binoy（2017）Limnd TP Lazimul 和 DL Binoy。2017。使用卷积神经网络和指纹图像增强进行指纹活跃度检测。见于
    *2017 国际能源、通信、数据分析与软计算会议（ICECDS）*。IEEE，731–735。
- en: 'Lee et al. (2022) Soo-Hyun Lee, Min Young Lim, Seong Hee Park, Hwa Jung Yoo,
    and Youn Kyu Lee. 2022. Towards Cross-materials: Fingerprint Liveness Detection
    based on Style Transfer. In *2022 13th International Conference on Information
    and Communication Technology Convergence (ICTC)*. IEEE, 1332–1334.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2022）Soo-Hyun Lee, Min Young Lim, Seong Hee Park, Hwa Jung Yoo 和 Youn
    Kyu Lee. 2022. 跨材料的前进：基于风格迁移的指纹活跃性检测. 在 *2022 第13届国际信息与通信技术融合会议 (ICTC)*. IEEE,
    1332–1334.
- en: Liu et al. (2022) Feng Liu, Zhe Kong, Haozhe Liu, Wentian Zhang, and Linlin
    Shen. 2022. Fingerprint Presentation Attack Detection by Channel-Wise Feature
    Denoising. *IEEE Transactions on Information Forensics and Security* 17 (2022),
    2963–2976.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2022）Feng Liu, Zhe Kong, Haozhe Liu, Wentian Zhang 和 Linlin Shen. 2022.
    通过通道级特征去噪进行指纹呈现攻击检测. *IEEE 信息取证与安全汇刊* 17 (2022), 2963–2976.
- en: Liu et al. (2021a) Feng Liu, Haozhe Liu, Wentian Zhang, Guojie Liu, and Linlin
    Shen. 2021a. One-class fingerprint presentation attack detection using auto-encoder
    network. *IEEE Transactions on Image Processing* 30 (2021), 2394–2407.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021a）Feng Liu, Haozhe Liu, Wentian Zhang, Guojie Liu 和 Linlin Shen.
    2021a. 使用自编码器网络的单类指纹呈现攻击检测. *IEEE 图像处理汇刊* 30 (2021), 2394–2407.
- en: Liu et al. (2021b) Haozhe Liu, Wentian Zhang, Feng Liu, Haoqian Wu, and Linlin
    Shen. 2021b. Fingerprint presentation attack detector using global-local model.
    *IEEE Transactions on Cybernetics* (2021).
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021b）Haozhe Liu, Wentian Zhang, Feng Liu, Haoqian Wu 和 Linlin Shen.
    2021b. 使用全局-局部模型的指纹呈现攻击检测器. *IEEE 控制论汇刊* (2021).
- en: Liu and Buma (2010) Mengyang Liu and Takashi Buma. 2010. Biometric mapping of
    fingertip eccrine glands with optical coherence tomography. *IEEE Photonics Technology
    Letters* 22, 22 (2010), 1677–1679.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Buma (2010) Mengyang Liu 和 Takashi Buma. 2010. 使用光学相干层析成像进行指尖汗腺的生物识别映射.
    *IEEE 光子技术快报* 22, 22 (2010), 1677–1679.
- en: Lowe (1999) D.G. Lowe. 1999. Object recognition from local scale-invariant features.
    In *Proceedings of the Seventh IEEE International Conference on Computer Vision*,
    Vol. 2. 1150–1157 vol.2. [https://doi.org/10.1109/ICCV.1999.790410](https://doi.org/10.1109/ICCV.1999.790410)
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe (1999) D.G. Lowe. 1999. 从局部尺度不变特征中进行对象识别。在 *第七届 IEEE 国际计算机视觉会议论文集*, 第2卷.
    1150–1157，第2卷. [https://doi.org/10.1109/ICCV.1999.790410](https://doi.org/10.1109/ICCV.1999.790410)
- en: M. Jomaa et al. (2020) Rami M. Jomaa, Hassan Mathkour, Yakoub Bazi, and Md Saiful
    Islam. 2020. End-to-end deep learning fusion of fingerprint and electrocardiogram
    signals for presentation attack detection. *Sensors* 20, 7 (2020), 2085.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M. Jomaa 等人（2020）Rami M. Jomaa, Hassan Mathkour, Yakoub Bazi 和 Md Saiful Islam.
    2020. 用于呈现攻击检测的指纹和心电图信号的端到端深度学习融合. *传感器* 20, 7 (2020), 2085.
- en: Makrushin et al. (2021) Andrey Makrushin, Mark Trebeljahr, Stefan Seidlitz,
    and Jana Dittmann. 2021. On feasibility of GAN-based fingerprint morphing. In
    *2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP)*.
    IEEE, 1–6.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makrushin 等人（2021）Andrey Makrushin, Mark Trebeljahr, Stefan Seidlitz 和 Jana
    Dittmann. 2021. 基于 GAN 的指纹伪造的可行性研究。在 *2021 IEEE 第23届国际多媒体信号处理研讨会 (MMSP)*. IEEE,
    1–6.
- en: Maltoni et al. (2022) Davide Maltoni, Dario Maio, Anil K Jain, and Jianjiang
    Feng. 2022. *Handbook of Fingerprint Recognition*. Springer Nature.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maltoni 等人（2022）Davide Maltoni, Dario Maio, Anil K Jain 和 Jianjiang Feng. 2022.
    *指纹识别手册*. Springer Nature.
- en: Maltoni et al. (2009) Davide Maltoni, Dario Maio, Anil K Jain, Salil Prabhakar,
    et al. 2009. *Handbook of fingerprint recognition*. Vol. 2. Springer.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maltoni 等人（2009）Davide Maltoni, Dario Maio, Anil K Jain, Salil Prabhakar 等人.
    2009. *指纹识别手册*. 第2卷. Springer.
- en: Marasco and Ross (2014) Emanuela Marasco and Arun Ross. 2014. A survey on antispoofing
    schemes for fingerprint recognition systems. *ACM Computing Surveys (CSUR)* 47,
    2 (2014), 1–36.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marasco 和 Ross (2014) Emanuela Marasco 和 Arun Ross. 2014. 关于指纹识别系统抗欺骗方案的调查.
    *ACM 计算机调查汇刊 (CSUR)* 47, 2 (2014), 1–36.
- en: 'Marasco and Vurity (2021) Emanuela Marasco and Anudeep Vurity. 2021. Fingerphoto
    Presentation Attack Detection: Generalization in Smartphones. In *2021 IEEE International
    Conference on Big Data (Big Data)*. IEEE, 4518–4523.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marasco 和 Vurity (2021) Emanuela Marasco 和 Anudeep Vurity. 2021. 指纹照片呈现攻击检测：在智能手机上的泛化.
    在 *2021 IEEE 大数据国际会议 (Big Data)*. IEEE, 4518–4523.
- en: Marasco et al. (2022) Emanuela Marasco, Anudeep Vurity, and Asem Otham. 2022.
    Deep Color Spaces for Fingerphoto Presentation Attack Detection in Mobile Devices.
    In *International Conference on Computer Vision and Image Processing*. Springer,
    351–362.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marasco 等人（2022）Emanuela Marasco, Anudeep Vurity 和 Asem Otham. 2022. 用于移动设备的深度颜色空间指纹照片呈现攻击检测.
    在 *国际计算机视觉与图像处理会议*. Springer, 351–362.
- en: 'Marcialis et al. (2009) Gian Luca Marcialis, Aaron Lewicke, Bozhao Tan, Pietro
    Coli, Dominic Grimberg, Alberto Congiu, Alessandra Tidu, Fabio Roli, and Stephanie
    Schuckers. 2009. First international fingerprint liveness detection competition—LivDet
    2009\. In *Image Analysis and Processing–ICIAP 2009: 15th International Conference
    Vietri sul Mare, Italy, September 8-11, 2009 Proceedings 15*. Springer, 12–23.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marcialis 等人 (2009) Gian Luca Marcialis, Aaron Lewicke, Bozhao Tan, Pietro
    Coli, Dominic Grimberg, Alberto Congiu, Alessandra Tidu, Fabio Roli, 和 Stephanie
    Schuckers. 2009. 首届国际指纹活体检测竞赛—LivDet 2009\. 载于 *图像分析与处理–ICIAP 2009: 第十五届国际会议,
    意大利维耶特里, 2009年9月8-11日论文集 15*. Springer, 12–23.'
- en: 'Marrone et al. (2021) Stefano Marrone, Roberto Casula, Giulia Orrù, Gian Luca
    Marcialis, and Carlo Sansone. 2021. Fingerprint adversarial presentation attack
    in the physical domain. In *Pattern Recognition. ICPR International Workshops
    and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part VI*. Springer,
    530–543.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marrone 等人 (2021) Stefano Marrone, Roberto Casula, Giulia Orrù, Gian Luca Marcialis,
    和 Carlo Sansone. 2021. 物理领域中的指纹对抗性展示攻击. 载于 *模式识别. ICPR 国际研讨会与挑战：虚拟活动, 2021年1月10–15日,
    论文集, 第 VI 部分*. Springer, 530–543.
- en: Menotti et al. (2015) David Menotti, Giovani Chiachia, Allan Pinto, William Robson
    Schwartz, Helio Pedrini, Alexandre Xavier Falcao, and Anderson Rocha. 2015. Deep
    representations for iris, face, and fingerprint spoofing detection. *IEEE Transactions
    on Information Forensics and Security* 10, 4 (2015), 864–879.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menotti 等人 (2015) David Menotti, Giovani Chiachia, Allan Pinto, William Robson
    Schwartz, Helio Pedrini, Alexandre Xavier Falcao, 和 Anderson Rocha. 2015. 深度表示用于虹膜、面部和指纹伪造检测.
    *IEEE 信息取证与安全学报* 10, 4 (2015), 864–879.
- en: 'Micheletto et al. (2022) Marco Micheletto, Giulia Orrù, Roberto Casula, David
    Yambay, Gian Luca Marcialis, and Stephanie C Schuckers. 2022. Review of the Fingerprint
    Liveness Detection (LivDet) competition series: from 2009 to 2021. *arXiv preprint
    arXiv:2202.07259* (2022).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micheletto 等人 (2022) Marco Micheletto, Giulia Orrù, Roberto Casula, David Yambay,
    Gian Luca Marcialis, 和 Stephanie C Schuckers. 2022. 指纹活体检测 (LivDet) 竞赛系列回顾：从 2009
    到 2021. *arXiv 预印本 arXiv:2202.07259* (2022).
- en: Mirzaalian et al. (2019) Hengameh Mirzaalian, Mohamed Hussein, and Wael Abd-Almageed.
    2019. On the effectiveness of laser speckle contrast imaging and deep neural networks
    for detecting known and unknown fingerprint presentation attacks. In *2019 International
    Conference on Biometrics (ICB)*. IEEE, 1–8.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirzaalian 等人 (2019) Hengameh Mirzaalian, Mohamed Hussein, 和 Wael Abd-Almageed.
    2019. 激光散斑对比成像和深度神经网络在检测已知和未知指纹展示攻击中的有效性. 载于 *2019 国际生物识别大会 (ICB)*. IEEE, 1–8.
- en: Mura et al. (2015) Valerio Mura, Luca Ghiani, Gian Luca Marcialis, Fabio Roli,
    David A. Yambay, and Stephanie A. Schuckers. 2015. LivDet 2015 fingerprint liveness
    detection competition 2015\. In *2015 IEEE 7th International Conference on Biometrics
    Theory, Applications and Systems (BTAS)*. 1–6. [https://doi.org/10.1109/BTAS.2015.7358776](https://doi.org/10.1109/BTAS.2015.7358776)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mura 等人 (2015) Valerio Mura, Luca Ghiani, Gian Luca Marcialis, Fabio Roli, David
    A. Yambay, 和 Stephanie A. Schuckers. 2015. LivDet 2015 指纹活体检测竞赛 2015\. 载于 *2015
    IEEE 第七届生物识别理论、应用与系统国际会议 (BTAS)*. 1–6. [https://doi.org/10.1109/BTAS.2015.7358776](https://doi.org/10.1109/BTAS.2015.7358776)
- en: Mura et al. (2018) Valerio Mura, Giulia Orrù, Roberto Casula, Alessandra Sibiriu,
    Giulia Loi, Pierluigi Tuveri, Luca Ghiani, and Gian Luca Marcialis. 2018. LivDet
    2017 fingerprint liveness detection competition 2017\. In *2018 International
    Conference on Biometrics (ICB)*. IEEE, 297–302.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mura 等人 (2018) Valerio Mura, Giulia Orrù, Roberto Casula, Alessandra Sibiriu,
    Giulia Loi, Pierluigi Tuveri, Luca Ghiani, 和 Gian Luca Marcialis. 2018. LivDet
    2017 指纹活体检测竞赛 2017\. 载于 *2018 国际生物识别大会 (ICB)*. IEEE, 297–302.
- en: 'Nguyen et al. (2018) Thi Hai Binh Nguyen, Eunsoo Park, Xuenan Cui, Van Huan
    Nguyen, and Hakil Kim. 2018. fPADnet: Small and efficient convolutional neural
    network for presentation attack detection. *Sensors* 18, 8 (2018), 2532.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen 等人 (2018) Thi Hai Binh Nguyen, Eunsoo Park, Xuenan Cui, Van Huan Nguyen,
    和 Hakil Kim. 2018. fPADnet: 小型高效卷积神经网络用于展示攻击检测. *传感器* 18, 8 (2018), 2532.'
- en: Nogueira et al. (2014) Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo,
    and Rubens Campos Machado. 2014. Evaluating software-based fingerprint liveness
    detection using convolutional networks and local binary patterns. In *2014 IEEE
    workshop on biometric measurements and systems for security and medical applications
    (BIOMS) Proceedings*. IEEE, 22–29.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira 等人 (2014) Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo, 和
    Rubens Campos Machado. 2014. 使用卷积网络和局部二值模式评估基于软件的指纹活体检测. 载于 *2014 IEEE 生物测量与安全医疗应用系统研讨会
    (BIOMS) 论文集*. IEEE, 22–29.
- en: Nogueira et al. (2016) Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo,
    and Rubens Campos Machado. 2016. Fingerprint liveness detection using convolutional
    neural networks. *IEEE transactions on information forensics and security* 11,
    6 (2016), 1206–1213.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira等人（2016年）Rodrigo Frassetto Nogueira，Roberto de Alencar Lotufo和Rubens
    Campos Machado。2016年。使用卷积神经网络进行指纹活体检测。*IEEE信息取证与安全交易*11, 6（2016年），1206–1213。
- en: Ojansivu and Heikkilä (2008) Ville Ojansivu and Janne Heikkilä. 2008. Blur insensitive
    texture classification using local phase quantization. In *International conference
    on image and signal processing*. Springer, 236–243.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ojansivu和Heikkilä（2008年）Ville Ojansivu和Janne Heikkilä。2008年。使用局部相位量化的模糊不敏感纹理分类。在*图像和信号处理国际会议*上。斯普林格，236–243。
- en: Orrù et al. (2019) Giulia Orrù, Roberto Casula, Pierluigi Tuveri, Carlotta Bazzoni,
    Giovanna Dessalvi, Marco Micheletto, Luca Ghiani, and Gian Luca Marcialis. 2019.
    Livdet in action-fingerprint liveness detection competition 2019\. In *2019 International
    Conference on Biometrics (ICB)*. IEEE, 1–6.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orrù等人（2019年）Giulia Orrù，Roberto Casula，Pierluigi Tuveri，Carlotta Bazzoni，Giovanna
    Dessalvi，Marco Micheletto，Luca Ghiani和Gian Luca Marcialis。2019年。活体检测行动-指纹活体检测竞赛2019\.
    在*2019年国际生物识别会议（ICB）*上。IEEE，1–6。
- en: Pala and Bhanu (2017) Federico Pala and Bir Bhanu. 2017. Deep triplet embedding
    representations for liveness detection. In *Deep Learning for Biometrics*. Springer,
    287–307.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pala和Bhanu（2017年）Federico Pala和Bir Bhanu。2017年。用于活体检测的深三元嵌入表示。在*生物识别的深度学习*中。斯普林格，287–307。
- en: Park et al. (2019) Eunsoo Park, Xuenan Cui, Thi Hai Binh Nguyen, and Hakil Kim.
    2019. Presentation attack detection using a tiny fully convolutional network.
    *IEEE Transactions on Information Forensics and Security* 14, 11 (2019), 3016–3025.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人（2019年）Eunsoo Park，Xuenan Cui，Thi Hai Binh Nguyen和Hakil Kim。2019年。使用微型全卷积网络的呈现攻击检测。*IEEE信息取证与安全交易*14,
    11（2019年），3016–3025。
- en: Park et al. (2016) Eunsoo Park, Weonjin Kim, Qiongxiu Li, Jungmin Kim, and Hakil
    Kim. 2016. Fingerprint liveness detection using CNN features of random sample
    patches. In *2016 International Conference of the Biometrics Special Interest
    Group (BIOSIG)*. IEEE, 1–4.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人（2016年）Eunsoo Park，Weonjin Kim，Qiongxiu Li，Jungmin Kim和Hakil Kim。2016年。使用随机样本补丁的CNN特征进行指纹活体检测。在*2016年国际生物识别特兴趣小组（BIOSIG）会议*上。IEEE，1–4。
- en: Pereira et al. (2020) Joao Afonso Pereira, Ana F Sequeira, Diogo Pernes, and
    Jaime S Cardoso. 2020. A robust fingerprint presentation attack detection method
    against unseen attacks through adversarial learning. In *2020 International Conference
    of the Biometrics Special Interest Group (BIOSIG)*. IEEE, 1–5.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pereira等人（2020年）Joao Afonso Pereira，Ana F Sequeira，Diogo Pernes和Jaime S Cardoso。2020年。通过对抗性学习对未知攻击的鲁棒指纹呈现攻击检测方法。在*2020年国际生物识别特兴趣小组（BIOSIG）会议*上。IEEE，1–5。
- en: Plesh et al. (2019) Richard Plesh, Keivan Bahmani, Ganghee Jang, David Yambay,
    Ken Brownlee, Timothy Swyka, Peter Johnson, Arun Ross, and Stephanie Schuckers.
    2019. Fingerprint presentation attack detection utilizing time-series, color fingerprint
    captures. In *2019 International Conference on Biometrics (ICB)*. IEEE, 1–8.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plesh等人（2019年）Richard Plesh，Keivan Bahmani，Ganghee Jang，David Yambay，Ken Brownlee，Timothy
    Swyka，Peter Johnson，Arun Ross和Stephanie Schuckers。2019年。利用时间序列，彩色指纹捕获的指纹呈现攻击检测。在*2019年国际生物识别会议（ICB）*上。IEEE，1–8。
- en: Prabakaran and Pillay (2020) Eswaran Prabakaran and Kriveshini Pillay. 2020.
    Synthesis and characterization of fluorescent N-CDs/ZnONPs nanocomposite for latent
    fingerprint detection by using powder brushing method. *Arabian Journal of Chemistry*
    13, 2 (2020), 3817–3835.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prabakaran和Pillay（2020年）Eswaran Prabakaran和Kriveshini Pillay。2020年。用于潜在指纹检测的荧光N-CDs/ZnONPs纳米复合材料的合成和表征。*阿拉伯化学杂志*13,
    2（2020年），3817–3835。
- en: Purnapatra et al. (2023) Sandip Purnapatra, Conor Miller-Lynch, Stephen Miner,
    Yu Liu, Keivan Bahmani, Soumyabrata Dey, and Stephanie Schuckers. 2023. Presentation
    Attack Detection with Advanced CNN Models for Noncontact-based Fingerprint Systems.
    *arXiv preprint arXiv:2303.05459* (2023).
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Purnapatra等人（2023年）Sandip Purnapatra，Conor Miller-Lynch，Stephen Miner，Yu Liu，Keivan
    Bahmani，Soumyabrata Dey和Stephanie Schuckers。2023年。用于非接触式指纹系统的先进CNN模型的呈现攻击检测。*arXiv预印本arXiv：2303.05459*（2023年）。
- en: 'Rai et al. (2023) Anuj Rai, Somnath Dey, Pradeep Patidar, and Prakhar Rai.
    2023. MoSFPAD: An end-to-end Ensemble of MobileNet and Support Vector Classifier
    for Fingerprint Presentation Attack Detection. *arXiv preprint arXiv:2303.01465*
    (2023).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rai等人（2023年）Anuj Rai，Somnath Dey，Pradeep Patidar和Prakhar Rai。2023年。MoSFPAD：MobileNet和支持向量分类器的端到端集成用于指纹呈现攻击检测。*arXiv预印本arXiv：2303.01465*（2023年）。
- en: 'Ramachandra and Li (2023) Raghavendra Ramachandra and Hailin Li. 2023. Finger-NestNet:
    Interpretable Fingerphoto Verification on Smartphone using Deep Nested Residual
    Network. In *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*. 693–700.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandra 和 Li（2023）Raghavendra Ramachandra 和 Hailin Li. 2023. Finger-NestNet：使用深度嵌套残差网络的智能手机指纹照片验证。在
    *IEEE/CVF 冬季计算机视觉应用会议论文集*。693–700.
- en: Reddy et al. (2008) P Venkata Reddy, Ajay Kumar, SMK Rahman, and Tanvir Singh
    Mundra. 2008. A new antispoofing approach for biometric devices. *IEEE transactions
    on biomedical circuits and systems* 2, 4 (2008), 328–337.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddy 等（2008）P Venkata Reddy、Ajay Kumar、SMK Rahman 和 Tanvir Singh Mundra. 2008.
    一种新的生物识别设备防欺骗方法。*IEEE 生物医学电路与系统汇刊* 2, 4 (2008), 328–337.
- en: Rohrer and Kolberg (2021) Tobias Rohrer and Jascha Kolberg. 2021. GAN pretraining
    for deep convolutional autoencoders applied to Software-based Fingerprint Presentation
    Attack Detection. *arXiv preprint arXiv:2105.10213* (2021).
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rohrer 和 Kolberg（2021）Tobias Rohrer 和 Jascha Kolberg. 2021. 用于深度卷积自编码器的 GAN
    预训练，应用于基于软件的指纹展示攻击检测。*arXiv 预印本 arXiv:2105.10213* (2021).
- en: 'Roy et al. (2018) Aditi Roy, Nasir Memon, Julian Togelius, and Arun Ross. 2018.
    Evolutionary methods for generating synthetic masterprint templates: Dictionary
    attack in fingerprint recognition. In *2018 International Conference on Biometrics
    (ICB)*. IEEE, 39–46.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 等（2018）Aditi Roy、Nasir Memon、Julian Togelius 和 Arun Ross. 2018. 生成合成主纹模板的进化方法：指纹识别中的字典攻击。在
    *2018 国际生物识别会议（ICB）*。IEEE, 39–46.
- en: S Ametefe et al. (2021) Divine S Ametefe, Suzi S Seroja, and Darmawaty M Ali.
    2021. Fingerprint presentation attack detection using deep transfer learning and
    densenet201 network/Divine S. Ametefe, Suzi S. Seroja, and Darmawaty M. Ali. *Journal
    of Electrical and Electronic Systems Research (JEESR)* 19 (2021), 95–105.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S Ametefe 等（2021）Divine S Ametefe、Suzi S Seroja 和 Darmawaty M Ali. 2021. 使用深度迁移学习和
    densenet201 网络的指纹展示攻击检测。*电气与电子系统研究期刊（JEESR）* 19 (2021), 95–105.
- en: 'Saguy et al. (2022) Michel Saguy, Joseph Almog, Daniel Cohn, and Christophe
    Champod. 2022. Proactive forensic science in biometrics: Novel materials for fingerprint
    spoofing. *Journal of Forensic Sciences* 67, 2 (2022), 534–542.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saguy 等（2022）Michel Saguy、Joseph Almog、Daniel Cohn 和 Christophe Champod. 2022.
    生物特征识别中的主动法医学：指纹欺骗的新材料。*法医学期刊* 67, 2 (2022), 534–542.
- en: Sandouka et al. (2021b) Soha B Sandouka, Yakoub Bazi, and Naif Alajlan. 2021b.
    Transformers and generative adversarial networks for liveness detection in multitarget
    fingerprint sensors. *Sensors* 21, 3 (2021), 699.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sandouka 等（2021b）Soha B Sandouka、Yakoub Bazi 和 Naif Alajlan. 2021b. 用于多目标指纹传感器的活体检测的变换器和生成对抗网络。*传感器*
    21, 3 (2021), 699.
- en: Sandouka et al. (2021a) Soha B Sandouka, Yakoub Bazi, Haikel Alhichri, and Naif
    Alajlan. 2021a. Unified Generative Adversarial Networks for Multidomain Fingerprint
    Presentation Attack Detection. *Entropy* 23, 8 (2021), 1089.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sandouka 等（2021a）Soha B Sandouka、Yakoub Bazi、Haikel Alhichri 和 Naif Alajlan.
    2021a. 统一生成对抗网络用于多领域指纹展示攻击检测。*熵* 23, 8 (2021), 1089.
- en: 'Schroff et al. (2015) Florian Schroff, Dmitry Kalenichenko, and James Philbin.
    2015. Facenet: A unified embedding for face recognition and clustering. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 815–823.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schroff 等（2015）Florian Schroff、Dmitry Kalenichenko 和 James Philbin. 2015. Facenet：用于面部识别和聚类的统一嵌入。在
    *IEEE 计算机视觉与模式识别会议论文集*。815–823.
- en: 'Selvaraju et al. (2017) Ramprasaath R Selvaraju, Michael Cogswell, Abhishek
    Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual
    explanations from deep networks via gradient-based localization. In *Proceedings
    of the IEEE international conference on computer vision*. 618–626.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selvaraju 等（2017）Ramprasaath R Selvaraju、Michael Cogswell、Abhishek Das、Ramakrishna
    Vedantam、Devi Parikh 和 Dhruv Batra. 2017. Grad-cam：通过基于梯度的定位从深度网络中获得的视觉解释。在 *IEEE
    国际计算机视觉会议论文集*。618–626.
- en: 'Sharma and Selwal (2021) Deepika Sharma and Arvind Selwal. 2021. FinPAD: State-of-the-art
    of fingerprint presentation attack detection mechanisms, taxonomy and future perspectives.
    *Pattern Recognition Letters* 152 (2021), 225–252.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 和 Selwal（2021）Deepika Sharma 和 Arvind Selwal. 2021. FinPAD：指纹展示攻击检测机制的最新进展、分类和未来展望。*模式识别快报*
    152 (2021), 225–252.
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Very
    deep convolutional networks for large-scale image recognition. *arXiv preprint
    arXiv:1409.1556* (2014).
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2014）Karen Simonyan 和 Andrew Zisserman. 2014. 用于大规模图像识别的非常深的卷积网络。*arXiv
    预印本 arXiv:1409.1556* (2014).
- en: Singh et al. (2021) Jag Mohan Singh, Ahmed Madhun, Guoqiang Li, and Raghavendra
    Ramachandra. 2021. A survey on unknown presentation attack detection for fingerprint.
    In *International Conference on Intelligent Technologies and Applications*. Springer,
    189–202.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh等人（2021）Jag Mohan Singh、Ahmed Madhun、Guoqiang Li 和 Raghavendra Ramachandra。2021。指纹未知呈现攻击检测的综述。在*国际智能技术与应用会议*。Springer，第189–202页。
- en: 'Sousedik and Busch (2014) Ctirad Sousedik and Christoph Busch. 2014. Presentation
    attack detection methods for fingerprint recognition systems: a survey. *Iet Biometrics*
    3, 4 (2014), 219–233.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sousedik 和 Busch（2014）Ctirad Sousedik 和 Christoph Busch。2014。指纹识别系统的呈现攻击检测方法：一项综述。*Iet
    Biometrics* 3，4（2014），第219–233页。
- en: 'Spinoulas et al. (2021) Leonidas Spinoulas, Hengameh Mirzaalian, Mohamed E
    Hussein, and Wael AbdAlmageed. 2021. Multi-modal fingerprint presentation attack
    detection: Evaluation on a new dataset. *IEEE Transactions on Biometrics, Behavior,
    and Identity Science* 3, 3 (2021), 347–364.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spinoulas等人（2021）Leonidas Spinoulas、Hengameh Mirzaalian、Mohamed E Hussein 和
    Wael AbdAlmageed。2021。多模态指纹呈现攻击检测：在新数据集上的评估。*IEEE生物特征识别、行为和身份科学期刊* 3，3（2021），第347–364页。
- en: 'Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent
    neural networks from overfitting. *The journal of machine learning research* 15,
    1 (2014), 1929–1958.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava等人（2014）Nitish Srivastava、Geoffrey Hinton、Alex Krizhevsky、Ilya Sutskever
    和 Ruslan Salakhutdinov。2014。Dropout：一种防止神经网络过拟合的简单方法。*机器学习研究杂志* 15，1（2014），第1929–1958页。
- en: Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer
    vision. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. 2818–2826.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy等人（2016）Christian Szegedy、Vincent Vanhoucke、Sergey Ioffe、Jon Shlens 和
    Zbigniew Wojna。2016。重新思考计算机视觉的Inception架构。在*IEEE计算机视觉与模式识别会议论文集*。第2818–2826页。
- en: 'Tan and Le (2021) Mingxing Tan and Quoc Le. 2021. Efficientnetv2: Smaller models
    and faster training. In *International conference on machine learning*. PMLR,
    10096–10106.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 和 Le（2021）Mingxing Tan 和 Quoc Le。2021。Efficientnetv2：更小的模型和更快的训练。在*国际机器学习会议*。PMLR，第10096–10106页。
- en: Tolosana et al. (2018) Ruben Tolosana, Marta Gomez-Barrero, Jascha Kolberg,
    Aythami Morales, Christoph Busch, and Javier Ortega-Garcia. 2018. Towards fingerprint
    presentation attack detection based on convolutional neural networks and short
    wave infrared imaging. In *2018 International Conference of the Biometrics Special
    Interest Group (BIOSIG)*. IEEE, 1–5.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tolosana等人（2018）Ruben Tolosana、Marta Gomez-Barrero、Jascha Kolberg、Aythami Morales、Christoph
    Busch 和 Javier Ortega-Garcia。2018。基于卷积神经网络和短波红外成像的指纹呈现攻击检测。 在*2018年生物特征识别特别兴趣小组国际会议（BIOSIG）*。IEEE，第1–5页。
- en: Toosi et al. (2017a) Amirhosein Toosi, Sandro Cumani, and Andrea Bottino. 2017a.
    Assessing transfer learning on convolutional neural networks for patch-based fingerprint
    liveness detection. In *International Joint Conference on Computational Intelligence*.
    Springer, 263–279.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toosi等人（2017a）Amirhosein Toosi、Sandro Cumani 和 Andrea Bottino。2017a。评估卷积神经网络在基于补丁的指纹活体检测中的迁移学习。在*国际计算智能联合会议*。Springer，第263–279页。
- en: Toosi et al. (2017b) Amirhosein Toosi, Sandro Cumani, and Andrea Bottino. 2017b.
    CNN Patch-Based Voting for Fingerprint Liveness Detection.. In *IJCCI*. 158–165.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toosi等人（2017b）Amirhosein Toosi、Sandro Cumani 和 Andrea Bottino。2017b。基于CNN的补丁投票用于指纹活体检测。在*IJCCI*。第158–165页。
- en: Wang et al. (2015) Chenggang Wang, Ke Li, Zhihong Wu, and Qijun Zhao. 2015.
    A DCNN based fingerprint liveness detection algorithm with voting strategy. In
    *Chinese Conference on Biometric Recognition*. Springer, 241–249.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2015）Chenggang Wang、Ke Li、Zhihong Wu 和 Qijun Zhao。2015。一种基于DCNN的指纹活体检测算法与投票策略。在*中国生物特征识别会议*。Springer，第241–249页。
- en: Xie and Yuille (2017) Lingxi Xie and Alan Yuille. 2017. Genetic cnn. In *Proceedings
    of the IEEE international conference on computer vision*. 1379–1388.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 和 Yuille（2017）Lingxi Xie 和 Alan Yuille。2017。遗传CNN。在*IEEE国际计算机视觉会议论文集*。第1379–1388页。
- en: Yambay et al. (2012) David Yambay, Luca Ghiani, Paolo Denti, Gian Luca Marcialis,
    Fabio Roli, and S Schuckers. 2012. LivDet 2011—Fingerprint liveness detection
    competition 2011\. In *2012 5th IAPR international conference on biometrics (ICB)*.
    IEEE, 208–215.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yambay等人（2012）David Yambay、Luca Ghiani、Paolo Denti、Gian Luca Marcialis、Fabio
    Roli 和 S Schuckers。2012。LivDet 2011—指纹活体检测竞赛2011。在*2012年第五届IAPR国际生物特征识别会议（ICB）*。IEEE，第208–215页。
- en: 'Yang et al. (2019) Wencheng Yang, Song Wang, Jiankun Hu, Guanglou Zheng, and
    Craig Valli. 2019. Security and accuracy of fingerprint-based biometrics: A review.
    *Symmetry* 11, 2 (2019), 141.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2019）Wencheng Yang, Song Wang, Jiankun Hu, Guanglou Zheng, 和 Craig Valli。2019。基于指纹的生物识别技术的安全性与准确性：综述。*对称性*
    11, 2 (2019), 141。
- en: Yau et al. (2008) Wei-Yun Yau, Hai-Linh Tran, and Eam-Khwang Teoh. 2008. Fake
    finger detection using an electrotactile display system. In *2008 10th International
    Conference on Control, Automation, Robotics and Vision*. IEEE, 962–966.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yau 等（2008）Wei-Yun Yau, Hai-Linh Tran, 和 Eam-Khwang Teoh。2008。使用电触觉显示系统进行假指纹检测。在*2008年第10届国际控制、自动化、机器人与视觉会议*。IEEE,
    962–966。
- en: Yuan et al. (2019a) Chengsheng Yuan, Zhihua Xia, Leqi Jiang, Yi Cao, QM Jonathan
    Wu, and Xingming Sun. 2019a. Fingerprint liveness detection using an improved
    CNN with image scale equalization. *IEEE Access* 7 (2019), 26953–26966.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等（2019a）Chengsheng Yuan, Zhihua Xia, Leqi Jiang, Yi Cao, QM Jonathan Wu,
    和 Xingming Sun。2019a。使用改进CNN和图像尺度均衡的指纹活体检测。*IEEE Access* 7 (2019), 26953–26966。
- en: Yuan et al. (2019b) Chengsheng Yuan, Zhihua Xia, Xingming Sun, and QM Jonathan
    Wu. 2019b. Deep residual network with adaptive learning framework for fingerprint
    liveness detection. *IEEE Transactions on Cognitive and Developmental Systems*
    12, 3 (2019), 461–473.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等（2019b）Chengsheng Yuan, Zhihua Xia, Xingming Sun, 和 QM Jonathan Wu。2019b。具有自适应学习框架的深度残差网络用于指纹活体检测。*IEEE认知与发展系统汇刊*
    12, 3 (2019), 461–473。
- en: Zeiler and Fergus (2014) Matthew D Zeiler and Rob Fergus. 2014. Visualizing
    and understanding convolutional networks. In *European conference on computer
    vision*. Springer, 818–833.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler 和 Fergus（2014）Matthew D Zeiler 和 Rob Fergus。2014。可视化和理解卷积网络。在*欧洲计算机视觉会议*。Springer,
    818–833。
- en: Zhang et al. (2014) Yongliang Zhang, Shanshan Fang, Yu Xie, and Tingting Xu.
    2014. Fake fingerprint detection based on wavelet analysis and local binary pattern.
    In *Chinese Conference on Biometric Recognition*. Springer, 191–198.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2014）Yongliang Zhang, Shanshan Fang, Yu Xie, 和 Tingting Xu。2014。基于小波分析和局部二值模式的假指纹检测。在*中国生物识别技术大会*。Springer,
    191–198。
- en: 'Zhang et al. (2020) Yongliang Zhang, Shengyi Pan, Xiaosi Zhan, Zhiwei Li, Minghua
    Gao, and Chenhao Gao. 2020. FLDNet: light dense CNN for fingerprint liveness detection.
    *IEEE Access* 8 (2020), 84141–84152.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020）Yongliang Zhang, Shengyi Pan, Xiaosi Zhan, Zhiwei Li, Minghua Gao,
    和 Chenhao Gao。2020。FLDNet：一种用于指纹活体检测的轻量级密集CNN。*IEEE Access* 8 (2020), 84141–84152。
- en: 'Zhang et al. (2019) Yongliang Zhang, Daqiong Shi, Xiaosi Zhan, Di Cao, Keyi
    Zhu, and Zhiwei Li. 2019. Slim-ResCNN: A deep residual convolutional neural network
    for fingerprint liveness detection. *IEEE Access* 7 (2019), 91476–91487.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2019）Yongliang Zhang, Daqiong Shi, Xiaosi Zhan, Di Cao, Keyi Zhu, 和
    Zhiwei Li。2019。Slim-ResCNN：一种用于指纹活体检测的深度残差卷积神经网络。*IEEE Access* 7 (2019), 91476–91487。
- en: Zhang et al. (2016) Yongliang Zhang, Bing Zhou, Hongtao Wu, and Conglin Wen.
    2016. 2D fake fingerprint detection based on improved CNN and local descriptors
    for smart phone. In *Chinese Conference on Biometric Recognition*. Springer, 655–662.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2016）Yongliang Zhang, Bing Zhou, Hongtao Wu, 和 Conglin Wen。2016。基于改进CNN和局部描述符的2D假指纹检测。
    在*中国生物识别技术大会*。Springer, 655–662。
- en: Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *Proceedings of the IEEE international conference on computer vision*. 2223–2232.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2017）Jun-Yan Zhu, Taesung Park, Phillip Isola, 和 Alexei A Efros。2017。使用循环一致对抗网络进行无配对图像到图像的翻译。在*IEEE国际计算机视觉会议论文集*。2223–2232。
- en: Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V
    Le. 2018. Learning transferable architectures for scalable image recognition.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    8697–8710.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph 等（2018）Barret Zoph, Vijay Vasudevan, Jonathon Shlens, 和 Quoc V Le。2018。可扩展图像识别的可迁移架构学习。在*IEEE计算机视觉与模式识别会议论文集*。8697–8710。
