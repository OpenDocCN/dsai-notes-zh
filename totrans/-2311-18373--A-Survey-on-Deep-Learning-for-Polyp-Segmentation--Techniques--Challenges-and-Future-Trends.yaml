- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:35:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:35:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2311.18373] A Survey on Deep Learning for Polyp Segmentation: Techniques,
    Challenges and Future Trends'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2311.18373] 深度学习在息肉分割中的调查：技术、挑战和未来趋势'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.18373](https://ar5iv.labs.arxiv.org/html/2311.18373)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.18373](https://ar5iv.labs.arxiv.org/html/2311.18373)
- en: \WarningFilter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \WarningFilter
- en: latexfontFont shape ‘ \WarningFilterlatexfontSize substitution
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: latexfont字体形状‘ \WarningFilterlatexfontSize 替换
- en: 'A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and
    Future Trends'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在息肉分割中的调查：技术、挑战和未来趋势
- en: 'Jiaxin Mei, Tao Zhou, Kaiwen Huang, Yizhe Zhang, Yi Zhou, Ye Wu, Huazhu Fu
    J. Mei, T. Zhou, K. Huang, Y. Zhang and Y. Wu are with the PCA Lab, and the School
    of Computer Science and Engineering, Nanjing University of Science and Technology,
    Nanjing 210094, China. Y. Zhou is with the School of Computer Science and Engineering,
    Southeast University, Nanjing, China. H. Fu is with the Institute of High Performance
    Computing, A*STAR, Singapore. Corresponding author: Tao Zhou (e-mail: taozhou.ai@gmail.com).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Jiaxin Mei, Tao Zhou, Kaiwen Huang, Yizhe Zhang, Yi Zhou, Ye Wu, Huazhu Fu J. Mei,
    T. Zhou, K. Huang, Y. Zhang 和 Y. Wu 现为南京理工大学计算机科学与工程学院的PCA实验室成员。Y. Zhou 现为东南大学计算机科学与工程学院的成员。H.
    Fu 现为新加坡A*STAR高性能计算研究所的成员。通讯作者：Tao Zhou（电子邮件：taozhou.ai@gmail.com）。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Early detection and assessment of polyps play a crucial role in the prevention
    and treatment of colorectal cancer (CRC). Polyp segmentation provides an effective
    solution to assist clinicians in accurately locating and segmenting polyp regions.
    In the past, people often relied on manually extracted lower-level features such
    as color, texture, and shape, which often had issues capturing global context
    and lacked robustness to complex scenarios. With the advent of deep learning,
    more and more outstanding medical image segmentation algorithms based on deep
    learning networks have emerged, making significant progress in this field. This
    paper provides a comprehensive review of polyp segmentation algorithms. We first
    review some traditional algorithms based on manually extracted features and deep
    segmentation algorithms, then detail benchmark datasets related to the topic.
    Specifically, we carry out a comprehensive evaluation of recent deep learning
    models and results based on polyp sizes, considering the pain points of research
    topics and differences in network structures. Finally, we discuss the challenges
    of polyp segmentation and future trends in this field. The models, benchmark datasets,
    and source code links we collected are all published at [https://github.com/taozh2017/Awesome-Polyp-Segmentation](https://github.com/taozh2017/Awesome-Polyp-Segmentation).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 早期发现和评估息肉在结直肠癌（CRC）的预防和治疗中发挥了关键作用。息肉分割提供了一种有效的解决方案，以帮助临床医生准确定位和分割息肉区域。过去，人们常常依赖于手动提取的低级特征，如颜色、纹理和形状，这些特征往往无法捕捉全局背景且对复杂场景缺乏鲁棒性。随着深度学习的兴起，越来越多基于深度学习网络的卓越医学图像分割算法应运而生，在这一领域取得了显著进展。本文提供了对息肉分割算法的综合评述。我们首先回顾一些基于手动提取特征和深度分割算法的传统算法，然后详细介绍相关主题的基准数据集。具体而言，我们对近期深度学习模型和基于息肉大小的结果进行全面评估，考虑到研究主题的痛点和网络结构的差异。最后，我们讨论了息肉分割的挑战及该领域的未来趋势。我们收集的模型、基准数据集和源代码链接均发布在
    [https://github.com/taozh2017/Awesome-Polyp-Segmentation](https://github.com/taozh2017/Awesome-Polyp-Segmentation)。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Polyp Segmentation, Deep Learning, Comprehensive Evaluation, Medical Imaging.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 息肉分割、深度学习、综合评估、医学影像。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 介绍
- en: Polyp segmentation is a crucial task in medical image analysis that aims to
    automatically identify and segment polyp regions within the colon. Its primary
    objective is to assist clinical doctors in efficiently and accurately locating
    and delineating these regions, providing vital support for early diagnosis and
    treatment of colorectal cancer (CRC) [[1](#bib.bib1)]. Polyps exhibit varying
    sizes and shapes at different stages of development [[2](#bib.bib2)], and their
    precise segmentation poses challenges due to their strong adherence to adjacent
    organs or mucosa [[3](#bib.bib3)]. Despite significant progress made in the domain
    of polyp segmentation, it still faces several challenges, such as limited annotated
    data, unclear boundaries, complex foregrounds, and real-time demand [[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 息肉分割是医学图像分析中的一项关键任务，旨在自动识别和分割结肠内的息肉区域。其主要目标是帮助临床医生高效且准确地定位和描绘这些区域，为结直肠癌（CRC）的早期诊断和治疗提供重要支持[[1](#bib.bib1)]。息肉在不同发展阶段展现出不同的大小和形状[[2](#bib.bib2)]，由于其与邻近器官或粘膜的强附着，其精确分割面临挑战[[3](#bib.bib3)]。尽管在息肉分割领域取得了显著进展，但仍面临多个挑战，如标注数据有限、边界不清晰、前景复杂以及实时需求[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]。
- en: '![Refer to caption](img/7b88d19f224bfea38d2f1eefc90031fb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7b88d19f224bfea38d2f1eefc90031fb.png)'
- en: 'Figure 1: Comparison of polyp segmentation results on some sample images using
    six CNN-based models (*i.e.*, SFA [[8](#bib.bib8)], PraNet [[9](#bib.bib9)], SANet [[10](#bib.bib10)],
    MSNet [[11](#bib.bib11)], ACSNet [[12](#bib.bib12)], and CFA-Net [[7](#bib.bib7)])
    and three transformer-based models (*i.e.*, Polyp-PVT [[13](#bib.bib13)], HSNet [[14](#bib.bib14)],
    and DuAT [[15](#bib.bib15)]).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用六种基于CNN的模型（*即*，SFA [[8](#bib.bib8)]，PraNet [[9](#bib.bib9)]，SANet [[10](#bib.bib10)]，MSNet [[11](#bib.bib11)]，ACSNet [[12](#bib.bib12)]，和CFA-Net [[7](#bib.bib7)]）以及三种基于Transformer的模型（*即*，Polyp-PVT [[13](#bib.bib13)]，HSNet [[14](#bib.bib14)]，和DuAT [[15](#bib.bib15)]）在一些样本图像上的息肉分割结果比较。
- en: In the early stage, polyp segmentation primarily relied on manually extracted
    features [[16](#bib.bib16), [17](#bib.bib17)]. For example, Tajbakhsh et al. [[16](#bib.bib16)]
    proposed a method that utilized shape features and surrounding environmental information
    to automatically detect polyps in colonoscopy videos. Iwahori et al. [[17](#bib.bib17)]
    utilized edge and color information to generate a likelihood map, extracted the
    Directional Gradient Histogram (DGH) features, and applied a random forest classifier
    to classify the detected regions as polyp areas or not. However, relying on manually
    extracted low-level features for segmentation tasks makes it difficult to handle
    complex scenarios and does not effectively utilize global contextual information [[18](#bib.bib18)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期阶段，息肉分割主要依赖于手动提取的特征[[16](#bib.bib16), [17](#bib.bib17)]。例如，Tajbakhsh 等[[16](#bib.bib16)]
    提出了利用形状特征和周围环境信息自动检测结肠镜视频中的息肉的方法。Iwahori 等[[17](#bib.bib17)] 利用边缘和颜色信息生成可能性图，提取了方向梯度直方图（DGH）特征，并应用随机森林分类器将检测区域分类为息肉区域或非息肉区域。然而，依赖手动提取的低级特征进行分割任务使得处理复杂场景变得困难，并且没有有效利用全局上下文信息[[18](#bib.bib18)]。
- en: \begin{overpic}[width=433.62pt]{figures/timeline.pdf} \par\put(8.3,17.3){\footnotesize~{}\cite[cite]{[\@@bibref{}{yao2004colonic}{}{}]}}
    \put(14.3,21.1){\footnotesize~{}\cite[cite]{[\@@bibref{}{lu2008accurate}{}{}]}}
    \put(20.3,24.9){\footnotesize~{}\cite[cite]{[\@@bibref{}{gross2009polyp}{}{}]}}
    \put(24.9,5.3){\footnotesize~{}\cite[cite]{[\@@bibref{}{ganz2012automatic}{}{}]}}
    \put(31.0,18.4){\footnotesize~{}\cite[cite]{[\@@bibref{}{long2015fully}{}{}]}}
    \put(34.7,3.9){\footnotesize~{}\cite[cite]{[\@@bibref{}{ronneberger2015u}{}{}]}}
    \put(44.0,23.9){\footnotesize~{}\cite[cite]{[\@@bibref{}{vazquez2017benchmark}{}{}]}}
    \put(48.6,6.4){\footnotesize~{}\cite[cite]{[\@@bibref{}{akbari2018polyp}{}{}]}}
    \put(51.6,20.3){\footnotesize~{}\cite[cite]{[\@@bibref{}{zhou2018unet++}{}{}]}}
    \put(57.6,25.6){\footnotesize~{}\cite[cite]{[\@@bibref{}{fang2019selective}{}{}]}}
    \put(61.8,3.0){\footnotesize~{}\cite[cite]{[\@@bibref{}{fan2020pranet}{}{}]}}
    \put(67.8,24.1){\footnotesize~{}\cite[cite]{[\@@bibref{}{wei2021shallow}{}{}]}}
    \put(71.6,6.2){\footnotesize~{}\cite[cite]{[\@@bibref{}{dong2021polyp}{}{}]}}
    \put(75.3,20.0){\footnotesize~{}\cite[cite]{[\@@bibref{}{srivastava2022msrf}{}{}]}}
    \put(78.8,2.4){\footnotesize~{}\cite[cite]{[\@@bibref{}{tomar2022tganet}{}{}]}}
    \put(80.1,25.1){\footnotesize~{}\cite[cite]{[\@@bibref{}{wang2022stepwise}{}{}]}}
    \put(86.0,6.6){\footnotesize~{}\cite[cite]{[\@@bibref{}{rahman2023medical}{}{}]}}
    \put(89.2,25.4){\footnotesize~{}\cite[cite]{[\@@bibref{}{chang2023esfpnet}{}{}]}}
    \put(90.6,2.6){\footnotesize~{}\cite[cite]{[\@@bibref{}{kirillov2023segment}{}{}]}}
    \put(94.5,18.6){\footnotesize~{}\cite[cite]{[\@@bibref{}{li2023polyp}{}{}]}} \par\end{overpic}
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{overpic}[width=433.62pt]{figures/timeline.pdf} \par\put(8.3,17.3){\footnotesize~{}\cite[cite]{[\@@bibref{}{yao2004colonic}{}{}]}}
    \put(14.3,21.1){\footnotesize~{}\cite[cite]{[\@@bibref{}{lu2008accurate}{}{}]}}
    \put(20.3,24.9){\footnotesize~{}\cite[cite]{[\@@bibref{}{gross2009polyp}{}{}]}}
    \put(24.9,5.3){\footnotesize~{}\cite[cite]{[\@@bibref{}{ganz2012automatic}{}{}]}}
    \put(31.0,18.4){\footnotesize~{}\cite[cite]{[\@@bibref{}{long2015fully}{}{}]}}
    \put(34.7,3.9){\footnotesize~{}\cite[cite]{[\@@bibref{}{ronneberger2015u}{}{}]}}
    \put(44.0,23.9){\footnotesize~{}\cite[cite]{[\@@bibref{}{vazquez2017benchmark}{}{}]}}
    \put(48.6,6.4){\footnotesize~{}\cite[cite]{[\@@bibref{}{akbari2018polyp}{}{}]}}
    \put(51.6,20.3){\footnotesize~{}\cite[cite]{[\@@bibref{}{zhou2018unet++}{}{}]}}
    \put(57.6,25.6){\footnotesize~{}\cite[cite]{[\@@bibref{}{fang2019selective}{}{}]}}
    \put(61.8,3.0){\footnotesize~{}\cite[cite]{[\@@bibref{}{fan2020pranet}{}{}]}}
    \put(67.8,24.1){\footnotesize~{}\cite[cite]{[\@@bibref{}{wei2021shallow}{}{}]}}
    \put(71.6,6.2){\footnotesize~{}\cite[cite]{[\@@bibref{}{dong2021polyp}{}{}]}}
    \put(75.3,20.0){\footnotesize~{}\cite[cite]{[\@@bibref{}{srivastava2022msrf}{}{}]}}
    \put(78.8,2.4){\footnotesize~{}\cite[cite]{[\@@bibref{}{tomar2022tganet}{}{}]}}
    \put(80.1,25.1){\footnotesize~{}\cite[cite]{[\@@bibref{}{wang2022stepwise}{}{}]}}
    \put(86.0,6.6){\footnotesize~{}\cite[cite]{[\@@bibref{}{rahman2023medical}{}{}]}}
    \put(89.2,25.4){\footnotesize~{}\cite[cite]{[\@@bibref{}{chang2023esfpnet}{}{}]}}
    \put(90.6,2.6){\footnotesize~{}\cite[cite]{[\@@bibref{}{kirillov2023segment}{}{}]}}
    \put(94.5,18.6){\footnotesize~{}\cite[cite]{[\@@bibref{}{li2023polyp}{}{}]}} \par\end{overpic}
- en: 'Figure 2: A brief chronology of polyp segmentation. Methods before 2015 were
    based on handcrafted features combined with machine learning algorithms. The development
    of U-Net [[24](#bib.bib24)] and FCN [[23](#bib.bib23)] since 2015 has greatly
    propelled the advancement of deep learning techniques in polyp segmentation.More
    details can be found in Sec. [II](#S2 "II Polyp Segmentation Models ‣ A Survey
    on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends").'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：息肉分割的简要时间线。2015年之前的方法基于手工特征并结合了机器学习算法。自2015年起，U-Net [[24](#bib.bib24)] 和
    FCN [[23](#bib.bib23)] 的发展极大地推动了深度学习技术在息肉分割中的进步。更多详细信息请参见第[II](#S2 "II Polyp Segmentation
    Models ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges
    and Future Trends)节。'
- en: 'In recent years, deep learning-based polyp segmentation models have made significant
    advancements, showcasing impressive capabilities in locating and segmenting polyp
    regions. For instance, Wei et al. [[10](#bib.bib10)] proposed a Shallow Attention
    Network, which leverages low-level features to mitigate degradation caused by
    multiple downsampling. They also proposed an innovative color-swapping method
    to reduce color dependency by exchanging color statistical data. To address the
    issue of blurry segmentation edges, Zhao et al. [[11](#bib.bib11)] proposed a
    Multi-Scale Subtraction Network, incorporating a subtraction unit to extract differential
    features between adjacent levels in the encoder. This network assigns different
    receptive fields to various levels of these units in a pyramid-like fashion, enabling
    the extraction of rich and diverse multi-scale differential information. To handle
    variations in sizes, Zhou et al. [[7](#bib.bib7)] presented a Cross-Level Feature
    Aggregation Network, employing a dual-stream structure-based segmentation network
    and a layer-by-layer fusion strategy for effective handling of scale variations
    and integration of high-level semantic information with low-level features. Rahman
    et al. [[31](#bib.bib31)] proposed a Cascade Attention Decoder, which effectively
    addresses the issue of inconsistent feature sizes by a hierarchical structure
    of transformers and attention-based convolutional modules to aggregate multi-level
    features and capture global and local contexts. Considering variations across
    datasets collected from different devices, Yang et al. [[34](#bib.bib34)] presented
    a mutual-prototype adaptation model to reduce domain shifts in multi-centers and
    multi-devices colonoscopy datasets. Furthermore, Jha et al. [[4](#bib.bib4)] proposed
    a Transformer-based Residual Network, which demonstrates strong generalization
    ability in multi-center external testing. In addition to innovative network architectures,
    refined models based on general segmentation networks have demonstrated promising
    results in polyp segmentation. For example, Li et al. [[33](#bib.bib33)] fine-tuned
    the Polyp-SAM model based on the Segment Anything Model (SAM) [[32](#bib.bib32)],
    which performs well in polyp segmentation tasks. To provide a clearer depiction
    of the progress in polyp segmentation tasks, we show a concise timeline in Fig. [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '最近几年，基于深度学习的息肉分割模型取得了显著的进展，展示了在定位和分割息肉区域方面的令人印象深刻的能力。例如，Wei等人[[10](#bib.bib10)]
    提出了一个浅层注意力网络，该网络利用低级特征来缓解由于多次下采样造成的降级。他们还提出了一种创新的颜色交换方法，通过交换颜色统计数据来减少颜色依赖。为了解决模糊分割边缘的问题，Zhao等人[[11](#bib.bib11)]
    提出了一个多尺度减法网络，该网络结合了一个减法单元，以提取编码器中相邻层之间的差异特征。该网络以类似金字塔的方式将不同的感受野分配给这些单元的不同层，能够提取丰富多样的多尺度差异信息。为了处理大小变化，Zhou等人[[7](#bib.bib7)]
    提出了一个跨层特征聚合网络，该网络采用基于双流结构的分割网络和逐层融合策略，有效处理尺度变化，并将高级语义信息与低级特征进行集成。Rahman等人[[31](#bib.bib31)]
    提出了一个级联注意力解码器，该解码器通过变压器和基于注意力的卷积模块的分层结构有效解决了特征尺寸不一致的问题，以聚合多级特征并捕获全局和局部上下文。考虑到从不同设备收集的数据集中的变化，Yang等人[[34](#bib.bib34)]
    提出了一个互原型适应模型，以减少多中心和多设备结肠镜数据集中的领域转移。此外，Jha等人[[4](#bib.bib4)] 提出了一个基于变压器的残差网络，该网络在多中心外部测试中展示了强大的泛化能力。除了创新的网络架构外，基于通用分割网络的精细化模型在息肉分割中也展示了良好的结果。例如，Li等人[[33](#bib.bib33)]
    基于分割任何模型（SAM）[[32](#bib.bib32)] 对Polyp-SAM模型进行了微调，该模型在息肉分割任务中表现良好。为了更清晰地展示息肉分割任务的进展，我们在图[2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends")中展示了一个简明的时间线。'
- en: In this paper, we present a comprehensive study and survey of polyp segmentation
    methods. This survey first reviews existing polyp segmentation methods from different
    perspectives. Additionally, we conduct a thorough evaluation of various representative
    polyp segmentation models and analyze their respective advantages. Furthermore,
    we discuss future challenges and potential research directions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对息肉分割方法进行了全面的研究和调查。该调查首先从不同角度回顾了现有的息肉分割方法。此外，我们对各种具有代表性的息肉分割模型进行了深入评估，并分析了它们各自的优势。此外，我们还讨论了未来的挑战和潜在的研究方向。
- en: I-A Related Reviews and Surveys
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 相关评论与调查
- en: There have been several recent investigations and reviews closely related to
    the field of polyp segmentation. For instance, Gupta et al. [[35](#bib.bib35)]
    reviewed deep-learning-based methods for efficient colorectal cancer screening,
    specifically focusing on polyp segmentation. They highlighted the importance of
    addressing challenges related to data scarcity, transfer learning, and interpretability
    in future research. Sanchez et al. [[36](#bib.bib36)] performed a systematic review
    of 35 studies since 2015 that utilized deep learning techniques for polyp detection,
    localization, and segmentation. Furthermore, Xiao et al. [[37](#bib.bib37)] classified
    and reviewed the segmentation methods involving UNet-based Transformers and other
    model-based Transformers in medical images. Their work summarized Transformer-based
    segmentation models applied to various anatomical regions such as abdominal organs,
    heart, brain, and lungs, based on relevant studies in the past two years.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近有几项与息肉分割领域密切相关的调查和评审。例如，Gupta 等人 [[35](#bib.bib35)] 回顾了基于深度学习的方法以实现高效的结直肠癌筛查，特别关注息肉分割。他们强调了未来研究中解决数据稀缺、迁移学习和可解释性等挑战的重要性。Sanchez
    等人 [[36](#bib.bib36)] 对自 2015 年以来利用深度学习技术进行息肉检测、定位和分割的 35 项研究进行了系统评审。此外，Xiao 等人 [[37](#bib.bib37)]
    对涉及基于 UNet 的 Transformers 和其他模型基础 Transformers 在医学图像中的分割方法进行了分类和评审。他们的工作总结了基于
    Transformer 的分割模型在腹部器官、心脏、大脑和肺部等各种解剖区域的应用，基于过去两年的相关研究。
- en: There have been some studies that reviewed the achievements in medical image
    processing. Qureshi et al. [[38](#bib.bib38)] conducted a survey on the latest
    developments in medical image segmentation techniques, focusing on computational
    image processing and machine learning methods. They examined the contributions
    of different architectures to medical image segmentation, discussed the advantages,
    identified open challenges, and highlighted potential future directions. Chowdhary
    et al. [[39](#bib.bib39)] investigated various segmentation and feature extraction
    methods used for preprocessing in medical images. Liu et al. [[40](#bib.bib40)]
    provided a comprehensive review of U-Net architectures in medical image segmentation
    tasks. They focused on the architecture, expansion mechanism, and application
    fields of U-Net architectures. Thisanke et al. [[41](#bib.bib41)] discussed different
    Vision Transformer (ViT) architectures applicable to semantic segmentation tasks
    and analyzed how their evolution has contributed to dense prediction tasks. Their
    investigation aimed to review and compare the performance of ViT architectures
    designed for semantic segmentation using benchmark datasets. Bennai et al. [[42](#bib.bib42)]
    conducted a comparative study of recently published multi-agent methods dedicated
    to medical image segmentation. Their work aimed to provide insights into the performance
    and effectiveness of these methods in the context of medical image analysis. Overall,
    these studies contribute to the understanding and advancement of medical image
    processing and segmentation techniques, highlighting the progress made, existing
    challenges, and future research directions in this field.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些研究回顾了医学图像处理的成就。Qureshi 等人 [[38](#bib.bib38)] 对医学图像分割技术的最新进展进行了调查，重点关注计算图像处理和机器学习方法。他们考察了不同架构对医学图像分割的贡献，讨论了优点，识别了开放挑战，并强调了潜在的未来方向。Chowdhary
    等人 [[39](#bib.bib39)] 研究了用于医学图像预处理的各种分割和特征提取方法。Liu 等人 [[40](#bib.bib40)] 对医学图像分割任务中的
    U-Net 架构进行了全面回顾。他们重点关注了 U-Net 架构的结构、扩展机制和应用领域。Thisanke 等人 [[41](#bib.bib41)] 讨论了适用于语义分割任务的不同
    Vision Transformer (ViT) 架构，并分析了其演变如何有助于密集预测任务。他们的调查旨在回顾和比较针对语义分割设计的 ViT 架构在基准数据集上的表现。Bennai
    等人 [[42](#bib.bib42)] 对近期发布的多智能体方法进行了比较研究，专注于医学图像分割。他们的工作旨在提供对这些方法在医学图像分析中的表现和有效性的见解。总体而言，这些研究有助于理解和推进医学图像处理和分割技术，突显了取得的进展、存在的挑战以及该领域的未来研究方向。
- en: In contrast to previous reviews on polyp segmentation or medical image segmentation,
    this paper aims to provide a comprehensive review of polyp segmentation methods.
    It covers both traditional algorithms and deep learning model-based approaches
    for polyp segmentation. The paper systematically and comprehensively analyzes
    the strengths, limitations, and outcomes of these methods.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前关于息肉分割或医学图像分割的综述相比，本文旨在提供对息肉分割方法的全面回顾。它涵盖了传统算法和基于深度学习模型的息肉分割方法。本文系统地、全面地分析了这些方法的优点、局限性和结果。
- en: I-B Contributions
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 贡献
- en: 'Our main contributions can be summarized as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献可以总结如下：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a systematic review of polyp segmentation models from different perspectives.
    Our study encompasses a thorough examination of both deep learning-based and traditional
    methods, boundary-aware models, attention-aware models, and feature fusion models.
    We discuss the strengths associated with each methodological approach, offering
    valuable insights into their respective applicability and performance characteristics.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从不同的角度系统地回顾了息肉分割模型。我们的研究涵盖了对基于深度学习和传统方法、边界感知模型、注意力感知模型和特征融合模型的全面检查。我们讨论了每种方法的优点，提供了对其适用性和性能特征的宝贵见解。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have conducted an analysis of five publicly available colonoscopy image datasets
    specifically curated for polyp segmentation. Additionally, we present a comprehensive
    overview of commonly used evaluation metrics employed in polyp segmentation tasks.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对五个公开可用的结肠镜图像数据集进行了分析，这些数据集专门用于息肉分割。此外，我们提供了息肉分割任务中常用评估指标的全面概述。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Moreover, we provide a comprehensive as well as a scale-based evaluation of
    several representative polyp segmentation models.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们提供了对几种代表性的息肉分割模型的全面以及基于规模的评估。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We delve deeply into the numerous challenges encountered in polyp segmentation
    and offer a comprehensive analysis of each one. Furthermore, we propose potential
    directions for future research that can help tackle these challenges and advance
    the field.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们深入探讨了息肉分割中遇到的众多挑战，并提供了每个挑战的全面分析。此外，我们提出了未来研究的潜在方向，这些方向可以帮助应对这些挑战并推动该领域的发展。
- en: I-C Organization
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-C 组织
- en: 'The structure of this review is organized as follows. In Sec. [II](#S2 "II
    Polyp Segmentation Models ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends"), we review the existing polyp segmentation
    models from different aspects. In Sec. [III](#S3 "III Polyp Segmentation Datasets
    ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and
    Future Trends"), we summarize and provide detailed information and usage of the
    current publicly available datasets used for polyp segmentation. Then, we conduct
    a comprehensive assessment of the segmentation performance for polyp sizes and
    an analysis of the advantages and disadvantages of several representative polyp
    segmentation models n Sec. [IV](#S4 "IV Model Evaluation and Analysis ‣ A Survey
    on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends").
    After that, in Sec. [V](#S5 "V Challenges and Future Trends ‣ A Survey on Deep
    Learning for Polyp Segmentation: Techniques, Challenges and Future Trends"), we
    discuss the challenges and future trends for development in this field. Finally,
    we conclude the paper in Sec. [VI](#S6 "VI Conclusion ‣ A Survey on Deep Learning
    for Polyp Segmentation: Techniques, Challenges and Future Trends").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '本综述的结构安排如下。在第[II](#S2 "II Polyp Segmentation Models ‣ A Survey on Deep Learning
    for Polyp Segmentation: Techniques, Challenges and Future Trends")节中，我们从不同方面回顾现有的息肉分割模型。在第[III](#S3
    "III Polyp Segmentation Datasets ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends")节中，我们总结并提供当前公开可用的数据集的详细信息和使用方法，这些数据集用于息肉分割。然后，我们在第[IV](#S4
    "IV Model Evaluation and Analysis ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends")节中对息肉尺寸的分割性能进行全面评估，并分析几种代表性的息肉分割模型的优缺点。在第[V](#S5
    "V Challenges and Future Trends ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends")节中，我们讨论了该领域的发展挑战和未来趋势。最后，在第[VI](#S6
    "VI Conclusion ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques,
    Challenges and Future Trends")节中，我们总结了本文的内容。'
- en: 'TABLE I: Summary of Polyp Segmentation Methods (published from 2019 to 2021).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：息肉分割方法的总结（2019年至2021年发布）。
- en: '| # | Year | Method | Pub. | Backbone | Description | Code |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| # | 年份 | 方法 | 发表 | 主干网络 | 描述 | 代码 |'
- en: '| 1 | 2019 | SFA  [[8](#bib.bib8)] | MICCAI | light UNet | Boundary-sensitive
    loss; Selective feature aggregation | [link](https://github.com/Yuqi-cuhk/Polyp-Seg)
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2019 | SFA  [[8](#bib.bib8)] | MICCAI | 轻量级UNet | 边界敏感损失；选择性特征聚合 | [链接](https://github.com/Yuqi-cuhk/Polyp-Seg)
    |'
- en: '| 2 | 2019 | ResUNet++ [[43](#bib.bib43)] | ISM | ResUNet | Squeeze and excitation
    blocks; ASPP; Attention blocks | [link](https://github.com/DebeshJha/ResUNetPlusPlus)
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2019 | ResUNet++ [[43](#bib.bib43)] | ISM | ResUNet | 挤压和激励块；ASPP；注意力块
    | [链接](https://github.com/DebeshJha/ResUNetPlusPlus) |'
- en: '| 3 | 2020 | PolypSeg [[44](#bib.bib44)] | MICCAI | UNet | Improved attention
    mechanism; Separable convolution | N/A |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2020 | PolypSeg [[44](#bib.bib44)] | MICCAI | UNet | 改进的注意力机制；可分离卷积 |
    无 |'
- en: '| 4 | 2020 | ThresholdNet [[1](#bib.bib1)] | TMI | DeepLabv3+ | Confidence-guided
    manifold mixup; Threshold loss | [link](https://github.com/Guo-Xiaoqing/ThresholdNet)
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2020 | ThresholdNet [[1](#bib.bib1)] | TMI | DeepLabv3+ | 置信度引导的流形混合；阈值损失
    | [链接](https://github.com/Guo-Xiaoqing/ThresholdNet) |'
- en: '| 5 | 2020 | ACSNet [[12](#bib.bib12)] | MICCAI | ResNet34 | Adaptively select;
    Aggregate context features through channel attention | [link](https://github.com/ReaFly/ACSNet)
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 2020 | ACSNet [[12](#bib.bib12)] | MICCAI | ResNet34 | 自适应选择；通过通道注意力聚合上下文特征
    | [链接](https://github.com/ReaFly/ACSNet) |'
- en: '| 6 | 2020 | PraNet [[9](#bib.bib9)] | MICCAI | Res2Net | Parallel partial
    decoders; Reverse attention | [link](https://github.com/DengPingFan/PraNet) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 2020 | PraNet [[9](#bib.bib9)] | MICCAI | Res2Net | 并行部分解码器；反向注意力 | [链接](https://github.com/DengPingFan/PraNet)
    |'
- en: '| 7 | 2021 | DDANet [[45](#bib.bib45)] | PR | ResUNet | Dual-decoder attention;
    Out-of-training-set testing. | [link](https://github.com/nikhilroxtomar/DDANet)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 2021 | DDANet [[45](#bib.bib45)] | PR | ResUNet | 双解码器注意力；训练集外测试 | [链接](https://github.com/nikhilroxtomar/DDANet)
    |'
- en: '| 8 | 2021 | GMSRF-Net [[46](#bib.bib46)] | ICPR | ResNet50 | Cross-multi-scale
    attention; Multi-scale feature selection | [link](https://github.com/NoviceMAn-prog/GMSRFNet)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2021 | GMSRF-Net [[46](#bib.bib46)] | ICPR | ResNet50 | 跨尺度注意力；多尺度特征选择
    | [链接](https://github.com/NoviceMAn-prog/GMSRFNet) |'
- en: '| 9 | 2021 | HarDNet-MSEG [[47](#bib.bib47)] | Arxiv | HarDNet68 | Cascaded
    partial decoder; Dense aggregation | [link](https://github.com/james128333/HarDNet-MSEG)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 2021 | HarDNet-MSEG [[47](#bib.bib47)] | Arxiv | HarDNet68 | 级联部分解码器；密集聚合
    | [链接](https://github.com/james128333/HarDNet-MSEG) |'
- en: '| 10 | 2021 | EU-Net [[48](#bib.bib48)] | CRV | ResNet34 | Semantic feature;
    Adaptive global context module | [link](https://github.com/rucv/Enhanced-U-Net)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 2021 | EU-Net [[48](#bib.bib48)] | CRV | ResNet34 | 语义特征；自适应全局上下文模块
    | [链接](https://github.com/rucv/Enhanced-U-Net) |'
- en: '| 11 | 2021 | FANet [[49](#bib.bib49)] | TNNLS | N/A | Feedback attention learning;
    Iterative refining; Embedded run-length encoding strategy | [link](https://github.com/nikhilroxtomar/fanet)
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 2021 | FANet [[49](#bib.bib49)] | TNNLS | 无 | 反馈注意力学习；迭代精炼；嵌入式运行长度编码策略
    | [链接](https://github.com/nikhilroxtomar/fanet) |'
- en: '| 12 | 2021 | Polyp-PVT [[13](#bib.bib13)] | CAAI AIR | PVT | Cascaded fusion;
    Camouflage identification; Similarity aggregation | [link](https://github.com/DengPingFan/Polyp-PVT)
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 2021 | Polyp-PVT [[13](#bib.bib13)] | CAAI AIR | PVT | 级联融合；伪装识别；相似性聚合
    | [链接](https://github.com/DengPingFan/Polyp-PVT) |'
- en: '| 13 | 2021 | UACANet  [[50](#bib.bib50)] | ACM MM | Res2Net | Parallel axial
    attention; Uncertainty augmented context attention | [link](https://github.com/plemeri/UACANet)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 2021 | UACANet  [[50](#bib.bib50)] | ACM MM | Res2Net | 并行轴向注意力；不确定性增强上下文注意力
    | [链接](https://github.com/plemeri/UACANet) |'
- en: '| 14 | 2021 | C2FNet [[51](#bib.bib51)] | IJCAI | Res2Net-50 | Context-aware
    cross-level fusion; Dual-branch global context | [link](https://github.com/thograce/C2FNet)
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 2021 | C2FNet [[51](#bib.bib51)] | IJCAI | Res2Net-50 | 上下文感知的跨层融合；双分支全局上下文
    | [链接](https://github.com/thograce/C2FNet) |'
- en: '| 15 | 2021 | ResUNet++ + TTA + CRF [[52](#bib.bib52)] | JBHI | ResUNet | Conditional
    random fields; Test-time augmentation | [link](https://github.com/DebeshJha/ResUNet-with-CRF-and-TTA)
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 2021 | ResUNet++ + TTA + CRF [[52](#bib.bib52)] | JBHI | ResUNet | 条件随机场；测试时增强
    | [链接](https://github.com/DebeshJha/ResUNet-with-CRF-and-TTA) |'
- en: '| 16 | 2021 | MPA-DA [[34](#bib.bib34)] | JBHI | ResNet-101 | Inter-prototype
    adaptation network; Progressive self-training; Disentangled reconstruction | [link](https://github.com/CityU-AIM-Group/MPA-DA)
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 2021 | MPA-DA [[34](#bib.bib34)] | JBHI | ResNet-101 | 原型间适应网络；渐进式自我训练；解缠重建
    | [链接](https://github.com/CityU-AIM-Group/MPA-DA) |'
- en: '| 17 | 2021 | TransFuse [[53](#bib.bib53)] | MICCAI | ResNet-34 + DeiT-S |
    Self-attention; Bilinear Hadamard product; Gated skip-connection | [link](https://github.com/Rayicer/TransFuse)
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 17 | 2021 | TransFuse [[53](#bib.bib53)] | MICCAI | ResNet-34 + DeiT-S |
    自注意力；双线性Hadamard积；门控跳跃连接 | [链接](https://github.com/Rayicer/TransFuse) |'
- en: '| 18 | 2021 | SANet [[10](#bib.bib10)] | MICCAI | Res2Net | Color exchange;
    Shallow attention | [link](https://github.com/weijun88/sanet) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 18 | 2021 | SANet [[10](#bib.bib10)] | MICCAI | Res2Net | 颜色交换；浅层注意力 | [link](https://github.com/weijun88/sanet)
    |'
- en: '| 19 | 2021 | STFT [[54](#bib.bib54)] | MICCAI | ResNet-50 | Spatial-temporal
    feature transformation; Deformable convolutions and channel-aware attention |
    [link](https://github.com/lingyunwu14/STFT) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 19 | 2021 | STFT [[54](#bib.bib54)] | MICCAI | ResNet-50 | 空间-时间特征变换；可变形卷积和通道感知注意力
    | [link](https://github.com/lingyunwu14/STFT) |'
- en: '| 20 | 2021 | LOD-Net [[55](#bib.bib55)] | MICCAI | ResNet + FPN | Model the
    probability of each pixel locating in border region; Adaptive thresholding policy
    | [link](https://github.com/midsdsy/LOD-Net) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 2021 | LOD-Net [[55](#bib.bib55)] | MICCAI | ResNet + FPN | 模型每个像素位于边界区域的概率；自适应阈值策略
    | [link](https://github.com/midsdsy/LOD-Net)'
- en: '| 21 | 2021 | MSNet [[11](#bib.bib11)] | MICCAI | Res2Net-50 | Multi-scale
    subtraction; Training-free network | [link](https://github.com/Xiaoqi-Zhao-DLUT/MSNet)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 2021 | MSNet [[11](#bib.bib11)] | MICCAI | Res2Net-50 | 多尺度减法；免训练网络
    | [link](https://github.com/Xiaoqi-Zhao-DLUT/MSNet) |'
- en: '| 22 | 2021 | CCBANet [[56](#bib.bib56)] | MICCAI | ResNet34 | Cascading context;
    Balancing attention | [link](https://github.com/ntcongvn/CCBANet) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 2021 | CCBANet [[56](#bib.bib56)] | MICCAI | ResNet34 | 级联上下文；平衡注意力
    | [link](https://github.com/ntcongvn/CCBANet) |'
- en: '| 23 | 2021 | HRENet [[57](#bib.bib57)] | MICCAI | ResNet34 | Hard region enhancement;
    Adaptive feature aggregation; Edge and structure consistency aware loss | [link](https://github.com/CathySH/HRENet)
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 23 | 2021 | HRENet [[57](#bib.bib57)] | MICCAI | ResNet34 | 硬区域增强；自适应特征聚合；边缘和结构一致性感知损失
    | [link](https://github.com/CathySH/HRENet) |'
- en: 'TABLE II: Summary of Polyp Segmentation Methods (published from 2022 to 2023).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 多态分割方法总结（发表时间：2022年至2023年）。'
- en: '| # | Year | Method | Pub. | Backbone | Description | Code |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| # | 年份 | 方法 | 发表 | 主干网络 | 描述 | 代码 |'
- en: '| 24 | 2022 | MSRF-Net [[28](#bib.bib28)] | JBHI | N/A | Multi-Scale residual
    fusion; Dual-scale dense fusion | [link](https://github.com/NoviceMAn-prog/MSRF-Net)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 2022 | MSRF-Net [[28](#bib.bib28)] | JBHI | N/A | 多尺度残差融合；双尺度密集融合 |
    [link](https://github.com/NoviceMAn-prog/MSRF-Net) |'
- en: '| 25 | 2022 | TGANet [[29](#bib.bib29)] | MICCAI | ResNet50 | Text-guided attention;
    Weights the text-based embeddings | [link](https://github.com/nikhilroxtomar/tganet)
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 2022 | TGANet [[29](#bib.bib29)] | MICCAI | ResNet50 | 文本引导注意力；加权文本嵌入
    | [link](https://github.com/nikhilroxtomar/tganet) |'
- en: '| 26 | 2022 | PolypSeg+ [[58](#bib.bib58)] | TCYB | ResNet50 | Adaptive scale
    context module; Lightweight attention mechanism | [link](https://github.com/szuzzb/polypsegplus)
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 26 | 2022 | PolypSeg+ [[58](#bib.bib58)] | TCYB | ResNet50 | 自适应尺度上下文模块；轻量级注意力机制
    | [link](https://github.com/szuzzb/polypsegplus) |'
- en: '| 27 | 2022 | MSRAformer [[59](#bib.bib59)] | CBM | Swin Transformer | Multiscale
    spatial reverse attention | [link](https://github.com/ChengLong1222/MSRAformer-main)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 27 | 2022 | MSRAformer [[59](#bib.bib59)] | CBM | Swin Transformer | 多尺度空间反向注意力
    | [link](https://github.com/ChengLong1222/MSRAformer-main) |'
- en: '| 28 | 2022 | HSNet [[14](#bib.bib14)] | CBM | PVTv2 | Cross-semantic attention;
    Hybrid semantic complementary; Multi-scale prediction | [link](https://github.com/baiboat/HSNet)
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 28 | 2022 | HSNet [[14](#bib.bib14)] | CBM | PVTv2 | 跨语义注意力；混合语义互补；多尺度预测
    | [link](https://github.com/baiboat/HSNet) |'
- en: '| 29 | 2022 | FuzzyNet [[60](#bib.bib60)] | NeurIPS | Res2Net/ ConvNext/ PVT
    | Fuzzy attention; Focus on the blurry pixels | [link](https://github.com/krushi1992/FuzzyNet)
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 29 | 2022 | FuzzyNet [[60](#bib.bib60)] | NeurIPS | Res2Net/ ConvNext/ PVT
    | 模糊注意力；关注模糊像素 | [link](https://github.com/krushi1992/FuzzyNet) |'
- en: '| 30 | 2022 | LDNet [[61](#bib.bib61)] | MICCAI | Res2Net | Lesion-aware dynamic
    kernel; Self-attention | [link](https://github.com/ReaFly/LDNet) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 2022 | LDNet [[61](#bib.bib61)] | MICCAI | Res2Net | 病灶感知动态卷积核；自注意力
    | [link](https://github.com/ReaFly/LDNet) |'
- en: '| 31 | 2022 | HarDNet-DFUS [[62](#bib.bib62)] | Arxiv | HarDNetV2 | Real-time
    model; Enhanced backbone | [link](https://github.com/YuWenLo/HarDNet-DFUS) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 31 | 2022 | HarDNet-DFUS [[62](#bib.bib62)] | Arxiv | HarDNetV2 | 实时模型；增强主干网络
    | [link](https://github.com/YuWenLo/HarDNet-DFUS) |'
- en: '| 32 | 2022 | BDG-Net [[63](#bib.bib63)] | SPIE MI | EfficientNet-B5 | Boundary
    distribution guided; Boundary distribution generate | [link](https://github.com/zihuanqiu/BDG-Net)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 2022 | BDG-Net [[63](#bib.bib63)] | SPIE MI | EfficientNet-B5 | 边界分布引导；边界分布生成
    | [link](https://github.com/zihuanqiu/BDG-Net) |'
- en: '| 33 | 2022 | ColonFormer [[64](#bib.bib64)] | Access | MiT | Integrate a hierarchical
    Transformer and a hierarchical pyramid CNN; Residual axial attention | [link](https://github.com/ducnt9907/ColonFormer)
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 33 | 2022 | ColonFormer [[64](#bib.bib64)] | Access | MiT | 结合层次化Transformer和层次化金字塔CNN；残差轴向注意力
    | [link](https://github.com/ducnt9907/ColonFormer) |'
- en: '| 34 | 2022 | FCBFormer [[65](#bib.bib65)] | MIUA | PVTv2 | Improved progressive
    locality decoder; Fully convolutional branch + Transformer bramch | [link](https://github.com/ESandML/FCBFormer)
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 34 | 2022 | FCBFormer [[65](#bib.bib65)] | MIUA | PVTv2 | 改进的渐进局部解码器；全卷积分支
    + Transformer 分支 | [link](https://github.com/ESandML/FCBFormer) |'
- en: '| 35 | 2022 | DCRNet [[66](#bib.bib66)] | ISBI | ResNet-34 | Duplex contextual
    relation network; Cross-image contextual relations | [link](https://github.com/PRIS-CV/DCRNet)
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 35 | 2022 | DCRNet [[66](#bib.bib66)] | ISBI | ResNet-34 | 双重上下文关系网络；跨图像上下文关系
    | [link](https://github.com/PRIS-CV/DCRNet) |'
- en: '| 36 | 2022 | SSFormer [[30](#bib.bib30)] | MICCAI | PVTv2 | Aggregate local
    and global features stepwise | [link](https://github.com/Qiming-Huang/ssformer)
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 36 | 2022 | SSFormer [[30](#bib.bib30)] | MICCAI | PVTv2 | 层次聚合局部和全局特征 |
    [link](https://github.com/Qiming-Huang/ssformer)'
- en: '| 37 | 2022 | DuAT [[15](#bib.bib15)] | PRCV | PVT | Dual-aggregation transformer;
    Global-to-local spatial aggregation; Selective boundary aggregation | [link](https://github.com/Barrett-python/DuAT)
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 37 | 2022 | DuAT [[15](#bib.bib15)] | PRCV | PVT | 双重聚合 Transformer；全局到局部空间聚合；选择性边界聚合
    | [link](https://github.com/Barrett-python/DuAT) |'
- en: '| 38 | 2022 | LAPFormer [[67](#bib.bib67)] | Arxiv | MiT-B1 | Hierarchical
    transformer encoder and CNN decoder; Progressive feature fusion | N/A |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 38 | 2022 | LAPFormer [[67](#bib.bib67)] | Arxiv | MiT-B1 | 分层 Transformer
    编码器和 CNN 解码器；渐进特征融合 | N/A |'
- en: '| 39 | 2022 | PPFormer [[68](#bib.bib68)] | MICCAI | CvT | Shallow CNN encoder;
    Deep Transformer-based encoder | N/A |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 39 | 2022 | PPFormer [[68](#bib.bib68)] | MICCAI | CvT | 浅层 CNN 编码器；深层 Transformer
    编码器 | N/A |'
- en: '| 40 | 2022 | BSCA-Net [[69](#bib.bib69)] | PR | Res2Net | Bit-plane slicing
    information; Segmentation squeeze bottleneck union module; Multi-Path Connection
    Attention | N/A |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 40 | 2022 | BSCA-Net [[69](#bib.bib69)] | PR | Res2Net | 位平面切片信息；分割挤压瓶颈联合模块；多路径连接注意力
    | N/A |'
- en: '| 41 | 2022 | BoxPolyp [[70](#bib.bib70)] | MICCAI | Res2Net/ PVT | Box annotations;
    Fusion filter sampling module | N/A |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 41 | 2022 | BoxPolyp [[70](#bib.bib70)] | MICCAI | Res2Net/ PVT | 框注释；融合滤波采样模块
    | N/A |'
- en: '| 42 | 2022 | ICBNet [[71](#bib.bib71)] | BIBM | PVT | Iterative feedback learning
    strategy; Context and boundary-aware information | N/A |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 42 | 2022 | ICBNet [[71](#bib.bib71)] | BIBM | PVT | 迭代反馈学习策略；上下文和边界感知信息
    | N/A |'
- en: '| 43 | 2022 | CLD-Net [[72](#bib.bib72)] | BIBM | MiT | Small polyp segmentation;
    Local edge feature extraction | N/A |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 43 | 2022 | CLD-Net [[72](#bib.bib72)] | BIBM | MiT | 小息肉分割；局部边缘特征提取 | N/A
    |'
- en: '| 44 | 2022 | BANet [[73](#bib.bib73)] | PRCV | Res2Net-50 | Attention-aware
    localization; Residual pyramid convolution | N/A |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 44 | 2022 | BANet [[73](#bib.bib73)] | PRCV | Res2Net-50 | 注意力感知定位；残差金字塔卷积
    | N/A |'
- en: '| 45 | 2022 | CaraNet  [[74](#bib.bib74)] | JMI | Res2Net | Context axial reverse
    attention | [link](https://github.com/AngeLouCN/CaraNet) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 45 | 2022 | CaraNet  [[74](#bib.bib74)] | JMI | Res2Net | 上下文轴向反向注意力 | [link](https://github.com/AngeLouCN/CaraNet)
    |'
- en: '| 46 | 2023 | APCNet [[75](#bib.bib75)] | TIM | ResNet50 | Attention-guided
    multi-level aggregation strategy; Complementary information from different layers
    | N/A |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 46 | 2023 | APCNet [[75](#bib.bib75)] | TIM | ResNet50 | 注意力引导的多层级聚合策略；来自不同层的互补信息
    | N/A |'
- en: '| 47 | 2023 | RA-DENet [[76](#bib.bib76)] | CBM | Res2Net | Improved reverse
    attention; Distraction elimination | N/A |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 47 | 2023 | RA-DENet [[76](#bib.bib76)] | CBM | Res2Net | 改进的反向注意力；干扰消除 |
    N/A |'
- en: '| 48 | 2023 | EFB-Seg [[77](#bib.bib77)] | Neurocom-puting | ConvNet | Boundary
    Embedding; Semantic offset field learned | N/A |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 48 | 2023 | EFB-Seg [[77](#bib.bib77)] | 神经计算 | ConvNet | 边界嵌入；学习的语义偏移场 |
    N/A |'
- en: '| 49 | 2023 | PPNet [[78](#bib.bib78)] | CBM | P2T | Channel attention; Pyramid
    feature fusion | N/A |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 49 | 2023 | PPNet [[78](#bib.bib78)] | CBM | P2T | 通道注意力；金字塔特征融合 | N/A |'
- en: '| 50 | 2023 | Fu-TransHNet [[6](#bib.bib6)] | Arxiv | HardNet68 | CNN and Transformer;
    Multi-view learning | N/A |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 2023 | Fu-TransHNet [[6](#bib.bib6)] | Arxiv | HardNet68 | CNN 和 Transformer；多视角学习
    | N/A |'
- en: '| 51 | 2023 | DilatedSegNet  [[79](#bib.bib79)] | MMM | ResNet50 | Dilated
    convolution pooling block; Convolutional attention | [link](https://github.com/suyanzhou626/FeDNet-BSPC)
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 51 | 2023 | DilatedSegNet [[79](#bib.bib79)] | MMM | ResNet50 | 膨胀卷积池化块；卷积注意力
    | [link](https://github.com/suyanzhou626/FeDNet-BSPC) |'
- en: '| 52 | 2023 | FeDNet [[80](#bib.bib80)] | BSPC | PVT | Decouple edge features
    and main body features | [link](https://github.com/suyanzhou626/FeDNet-BSPC) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 52 | 2023 | FeDNet [[80](#bib.bib80)] | BSPC | PVT | 解耦边缘特征和主体特征 | [link](https://github.com/suyanzhou626/FeDNet-BSPC)
    |'
- en: '| 53 | 2023 | PEFNet [[81](#bib.bib81)] | MMM | EfficientNet V2-L | Positional
    encoding and information fusion | [link](https://github.com/huyquoctrinh/PEFNet)
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 53 | 2023 | PEFNet [[81](#bib.bib81)] | MMM | EfficientNet V2-L | 位置信息编码和信息融合
    | [link](https://github.com/huyquoctrinh/PEFNet) |'
- en: '| 54 | 2023 | Polyp-SAM [[33](#bib.bib33)] | Arxiv | ViT | Finetuned SAM model
    for polyp segmentation [[32](#bib.bib32)]. | [link](https://github.com/ricklisz/Polyp-SAM)
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 54 | 2023 | Polyp-SAM [[33](#bib.bib33)] | Arxiv | ViT | 细化的SAM模型用于多发性息肉分割 [[32](#bib.bib32)]。
    | [link](https://github.com/ricklisz/Polyp-SAM) |'
- en: '| 55 | 2023 | ESFPNet [[5](#bib.bib5)] | SPIE MI | MiT | Efficient stage-wise
    feature pyramid decoder | [link](https://github.com/dumyCq/ESFPNet) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 55 | 2023 | ESFPNet [[5](#bib.bib5)] | SPIE MI | MiT | 高效的阶段性特征金字塔解码器 | [link](https://github.com/dumyCq/ESFPNet)
    |'
- en: '| 56 | 2023 | TransNetR [[4](#bib.bib4)] | MIDL | ResNet50 | Transformer-based
    residual network; Multi-center out-of-distribution testing | [link](https://github.com/DebeshJha/TransNetR)
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 56 | 2023 | TransNetR [[4](#bib.bib4)] | MIDL | ResNet50 | 基于Transformer的残差网络；多中心分布外测试
    | [link](https://github.com/DebeshJha/TransNetR) |'
- en: '| 57 | 2023 | CASCADE [[31](#bib.bib31)] | WACV | PVTv2/ TransUNet | Cascaded
    attention-based decoder; Multi-stage loss optimization; Feature aggregation |
    [link](https://github.com/SLDGroup/CASCADE) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 57 | 2023 | CASCADE [[31](#bib.bib31)] | WACV | PVTv2/ TransUNet | 基于级联的注意力解码器；多阶段损失优化；特征聚合
    | [link](https://github.com/SLDGroup/CASCADE) |'
- en: '| 58 | 2023 | CFA-Net [[7](#bib.bib7)] | PR | Res2Net-50 | Cross-level feature
    aggregated; Boundary aggregated | [link](https://github.com/taozh2017/CFANet)
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 58 | 2023 | CFA-Net [[7](#bib.bib7)] | PR | Res2Net-50 | 跨层特征聚合；边界聚合 | [link](https://github.com/taozh2017/CFANet)
    |'
- en: II Polyp Segmentation Models
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 多发性息肉分割模型
- en: 'Over the past decade, significant efforts have been made to enhance the performance
    of automatic polyp segmentation models, improving the detection capability of
    colonoscopy, and reducing disease risks. Recent research on polyp segmentation
    has leveraged popular deep-learning methods to achieve noteworthy outcomes. In
    contrast, earlier approaches relied primarily on manually engineered features
    for polyp segmentation. A summary of these models can be found in Table [I](#S1.T1
    "TABLE I ‣ I-C Organization ‣ I Introduction ‣ A Survey on Deep Learning for Polyp
    Segmentation: Techniques, Challenges and Future Trends"),[II](#S1.T2 "TABLE II
    ‣ I-C Organization ‣ I Introduction ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends"). To provide a comprehensive review
    of these polyp segmentation algorithms, we will introduce them from the following
    aspects.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，已付出了重大努力来提高自动多发性息肉分割模型的性能，改善结肠镜检查的检测能力，并降低疾病风险。近期关于多发性息肉分割的研究利用了流行的深度学习方法，取得了显著的成果。相比之下，早期方法主要依赖手工设计的特征进行多发性息肉分割。这些模型的总结可以在表 [I](#S1.T1
    "TABLE I ‣ I-C 组织 ‣ I 介绍 ‣ 关于多发性息肉分割的深度学习调查：技术、挑战和未来趋势")，[II](#S1.T2 "TABLE II
    ‣ I-C 组织 ‣ I 介绍 ‣ 关于多发性息肉分割的深度学习调查：技术、挑战和未来趋势")中找到。为了对这些多发性息肉分割算法进行全面的综述，我们将从以下方面进行介绍。
- en: '(1) Traditional Models: They primarily rely on manually designed features,
    such as color, texture, and shape information, to formulate for algorithm design.
    (2) Deep models: Deep learning models automatically learn deep features, capable
    of handling more complex structures, and offering stronger expression. (3) Boundary-aware
    models: Edge information is crucial in providing boundary cues to boost the segmentation
    performance, therefore we will discuss the application of edge information in
    some existing models. (4) Attention-aware models: attention mechanisms have widely
    been applied in various visual tasks. We conduct a comprehensive review of related
    works on polyp segmentation to analyze different attention strategies. This analysis
    provides insights into the potential design of attention modules in future works.
    (5) Feature fusion models: The integration and utilization of multi-level features
    often contribute significantly to enhancing model performance. Therefore, we investigate
    the effectiveness of feature fusion strategies in polyp segmentation models.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 传统模型：它们主要依赖手工设计的特征，如颜色、纹理和形状信息，来制定算法设计。 (2) 深度模型：深度学习模型自动学习深层特征，能够处理更复杂的结构，并提供更强的表达能力。
    (3) 边界感知模型：边缘信息在提供边界线索以提升分割性能方面至关重要，因此我们将讨论一些现有模型中边缘信息的应用。 (4) 注意力感知模型：注意力机制已经广泛应用于各种视觉任务中。我们对多发性息肉分割相关工作的全面综述，以分析不同的注意力策略。此分析提供了对未来工作的注意力模块设计的潜在见解。
    (5) 特征融合模型：多层次特征的集成和利用往往对提升模型性能有显著贡献。因此，我们调查了特征融合策略在多发性息肉分割模型中的有效性。
- en: II-A Traditional Models
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 传统模型
- en: Early works primarily relied on manually designed features, such as color, texture,
    and shape, and then employed traditional machine learning techniques for heuristic
    modeling. For instance, Yao et al. [[19](#bib.bib19)] proposed an automatic polyp
    segmentation method that combines knowledge-guided intensity adjustment, fuzzy
    c-means clustering, and deformable models. Lu et al. [[20](#bib.bib20)] proposed
    a three-stage probabilistic binary classification method that integrates low-level
    and mid-level information to segment polyps in 3D CT colonography. Gross et al. [[21](#bib.bib21)]
    studied a segmentation algorithm that enhances primary edges through multiscale
    filtering. Ganz et al. [[22](#bib.bib22)] utilized prior knowledge of polyp shapes
    using NBI Narrow Band Imaging (NBI) to optimize the inherent scale selection problem
    of gPb-OWT-UCM. This optimization aims to achieve better segmentation results
    by incorporating the shape information of polyps.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 早期工作主要依赖于手动设计的特征，如颜色、纹理和形状，然后采用传统机器学习技术进行启发式建模。例如，Yao 等人 [[19](#bib.bib19)]
    提出了结合知识引导的强度调整、模糊 c-means 聚类和可变形模型的自动息肉分割方法。Lu 等人 [[20](#bib.bib20)] 提出了一个三阶段的概率二分类方法，将低级和中级信息结合起来以分割
    3D CT 结肠造影中的息肉。Gross 等人 [[21](#bib.bib21)] 研究了一种通过多尺度过滤增强主要边缘的分割算法。Ganz 等人 [[22](#bib.bib22)]
    利用息肉形状的先验知识，通过 NBI Narrow Band Imaging (NBI) 优化了 gPb-OWT-UCM 的固有尺度选择问题。该优化旨在通过结合息肉的形状信息来实现更好的分割结果。
- en: II-B Deep Models
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 深度模型
- en: However, the aforementioned methods are constrained by the limited information
    representation capabilities of manual features. They lack generalization capabilities
    and are not suitable for large-scale deployment. As a result, there is a growing
    reliance on deep features to handle these limitations. We will review some representative
    deep learning-based methods in the field of polyp segmentation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述方法受到手动特征信息表示能力有限的限制。它们缺乏泛化能力，不适合大规模部署。因此，越来越多地依赖深度特征来处理这些限制。我们将回顾一些在息肉分割领域的具有代表性的深度学习方法。
- en: 1) CNN-based Methods. Thanks to the development of Convolutional Neural Networks
    (CNN), especially with the introduction of U-Net [[24](#bib.bib24)], many models
    inspired by this architecture have demonstrated promising results.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基于 CNN 的方法。得益于卷积神经网络（CNN）的发展，特别是 U-Net [[24](#bib.bib24)] 的引入，许多受此架构启发的模型展示了令人满意的结果。
- en: $\bullet$ ACSNet [[12](#bib.bib12)] modifies the skip connections in U-Net into
    local context extraction modules and adds a global information extraction module.
    The features are integrated and then adaptively selected based on a channel attention
    strategy.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ACSNet [[12](#bib.bib12)] 将 U-Net 中的跳跃连接修改为局部上下文提取模块，并增加了全局信息提取模块。特征被整合并根据通道注意力策略自适应选择。
- en: $\bullet$ EU-Net [[48](#bib.bib48)] is an enhanced U-Net framework that enhances
    semantic information and introduces an adaptive global context module to extract
    key features. It improves the quality of features at each layer, thereby enhancing
    the final segmentation performance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ EU-Net [[48](#bib.bib48)] 是一个增强的 U-Net 框架，增强了语义信息，并引入了自适应全局上下文模块来提取关键特征。它提高了每一层特征的质量，从而增强了最终的分割性能。
- en: $\bullet$ MSNet [[11](#bib.bib11)] designs a subtraction unit to generate difference
    features between adjacent layers and pyramidically equips it with different receptive
    fields to capture multi-scale information. In addition, it introduces LossNet
    to supervise the perceptual features at each layer.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MSNet [[11](#bib.bib11)] 设计了一个减法单元以生成相邻层之间的差异特征，并以金字塔形式配备不同的感受野以捕捉多尺度信息。此外，它引入了
    LossNet 来监督每层的感知特征。
- en: $\bullet$ PEFNet [[81](#bib.bib81)] utilizes an improved U-Net in the merging
    phase and embeds new location feature information. Thanks to the rich knowledge
    of positional information and concatenated features, this model achieves higher
    accuracy and universality in polyp segmentation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PEFNet [[81](#bib.bib81)] 在合并阶段利用改进的 U-Net 并嵌入新的位置特征信息。凭借丰富的位置知识和连接特征，该模型在息肉分割中实现了更高的准确性和通用性。
- en: 2) Transformer-based Methods. Although CNNs have been successful in various
    computer vision tasks, they have limitations in capturing long-range dependencies.
    However, the introduction of Transformer models, originally popular in natural
    language processing, has revolutionized the field of computer vision. Specifically,
    Vision Transformers (ViTs) [[82](#bib.bib82)] have emerged as a powerful approach
    for image understanding and have contributed to the development of numerous algorithms
    that leverage their strengths.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 基于Transformer的方法。尽管CNN在各种计算机视觉任务中取得了成功，但它们在捕捉长程依赖关系方面存在局限。然而，Transformer模型的引入，最初在自然语言处理领域流行，彻底革新了计算机视觉领域。特别是视觉Transformer（ViTs）[[82](#bib.bib82)]已经成为图像理解的强大方法，并推动了利用其优势的众多算法的发展。
- en: $\bullet$ MSRAformer [[59](#bib.bib59)]. It adopts a Swin Transformer as the
    encoder with a pyramid structure to extract features at different stages and utilizes
    a multi-scale channel attention module to extract multi-scale feature information.
    It also adds a spatial reverse attention module to supplement edge structure and
    detail information, thereby exhibiting strong generalization capability and performance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MSRAformer [[59](#bib.bib59)]。它采用Swin Transformer作为编码器，配备金字塔结构以在不同阶段提取特征，并利用多尺度通道注意力模块提取多尺度特征信息。它还增加了一个空间反向注意力模块，以补充边缘结构和细节信息，从而展示出强大的泛化能力和性能。
- en: $\bullet$ DuAT [[15](#bib.bib15)] is a dual-aggregation transformer network
    for polyp segmentation. It includes a global-to-local spatial aggregation module
    for aggregating global and local spatial features and locating multi-scale objects.
    This method also employs a selective boundary aggregation module to integrate
    low-level edge features and high-level semantic features.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ DuAT [[15](#bib.bib15)] 是一个用于息肉分割的双重聚合Transformer网络。它包括一个全球到局部的空间聚合模块，用于聚合全球和局部空间特征以及定位多尺度对象。该方法还采用了选择性边界聚合模块，以集成低级边缘特征和高级语义特征。
- en: $\bullet$ SSFormer [[30](#bib.bib30)] incorporates PVTv2 and Segformer as the
    encoder while introducing a novel progressive local decoder. This decoder is specifically
    designed to complement the pyramid Transformer backbone by emphasizing local features
    and mitigating attention dispersion.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ SSFormer [[30](#bib.bib30)] 结合了PVTv2和Segformer作为编码器，同时引入了一种新颖的渐进本地解码器。该解码器专门设计用来补充金字塔Transformer骨干，通过强调局部特征和减轻注意力分散来发挥作用。
- en: $\bullet$ ColonFormer [[64](#bib.bib64)] adopts a lightweight architecture based
    on the transformer as the encoder and uses a hierarchical network structure for
    learning multi-level features in the decoder. Additionally, the model incorporates
    a novel skip connection technique that refines polyp boundary information, resulting
    in precise segmentation outcomes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ColonFormer [[64](#bib.bib64)] 采用基于Transformer的轻量级架构作为编码器，并在解码器中使用层次化网络结构以学习多级特征。此外，该模型还引入了一种新颖的跳跃连接技术，用于细化息肉边界信息，从而实现精确的分割结果。
- en: $\bullet$ TransNetR [[4](#bib.bib4)] is a transformer-based residual network,
    comprising a pre-trained encoder, three decoder blocks, and an upsampling layer,
    demonstrating excellent real-time processing speed and multi-center generalization
    capability.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ TransNetR [[4](#bib.bib4)] 是一个基于Transformer的残差网络，包括一个预训练编码器、三个解码块和一个上采样层，展示了卓越的实时处理速度和多中心泛化能力。
- en: $\bullet$ Polyp-PVT [[13](#bib.bib13)] comprises a cascaded fusion module that
    combines high-level semantic and position information, a camouflage recognition
    module that captures low-level features, and a similarity aggregation module that
    extends high-level features throughout the entire region. This integrated approach
    effectively mitigates noise in the features and yields substantial improvements
    in polyp segmentation performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Polyp-PVT [[13](#bib.bib13)] 包括一个级联融合模块，结合了高级语义和位置数据，一个捕捉低级特征的伪装识别模块，以及一个将高级特征扩展到整个区域的相似性聚合模块。这种集成方法有效减轻了特征中的噪声，并在息肉分割性能上取得了显著改善。
- en: 3) Hybrid Methods. Furthermore, numerous models have combined the strengths
    of both CNN and Transformer, capturing both local context information and long-range
    dependencies to significantly enhance segmentation performance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 混合方法。此外，许多模型结合了CNN和Transformer的优点，捕捉局部上下文信息和长程依赖关系，从而显著提升了分割性能。
- en: $\bullet$ TransFuse [[53](#bib.bib53)] combines Transformer and CNN in a parallel
    manner to capture global dependencies and low-level spatial details. When fusing
    the multi-level features from the two branches, it incorporates a self-attention
    mechanism and a multi-modal fusion mechanism. Moreover, spatial attention is used
    to enhance local details and suppress irrelevant regions while modeling fine-grained
    interactions between the two branches to suppress noise in low-level features.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ TransFuse [[53](#bib.bib53)] 将 Transformer 和 CNN 以并行方式结合，以捕捉全局依赖关系和低级空间细节。在融合两个分支的多层特征时，它结合了自注意力机制和多模态融合机制。此外，空间注意力用于增强局部细节并抑制无关区域，同时建模两个分支之间的细粒度交互，以抑制低级特征中的噪声。
- en: $\bullet$ LAPFormer [[67](#bib.bib67)] employs a hierarchical transformer encoder
    to extract global features and combines it with a CNN decoder to capture the local
    appearance of polyps. Additionally, it introduces a progressive feature fusion
    module to integrate multi-scale features and incorporates a feature refinement
    module and a feature selection module for feature handling.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ LAPFormer [[67](#bib.bib67)] 使用层次化的 Transformer 编码器来提取全局特征，并与 CNN
    解码器结合以捕捉息肉的局部外观。此外，它引入了一个渐进特征融合模块来整合多尺度特征，并结合了特征细化模块和特征选择模块以进行特征处理。
- en: $\bullet$ PPFormer [[68](#bib.bib68)] adopts a shallow CNN encoder and deep
    Transformer-based encoder to extract features. It then uses prediction maps to
    guide self-attention for enhanced boundary perception.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PPFormer [[68](#bib.bib68)] 采用浅层 CNN 编码器和深层 Transformer 编码器来提取特征。然后使用预测图来指导自注意力，以增强边界感知。
- en: $\bullet$ HSNet [[14](#bib.bib14)] utilizes a dual-branch structure composed
    of Transformer and CNN networks to capture both long-range dependencies and local
    appearance details. Additionally, an interaction mechanism is incorporated to
    facilitate the exchange of semantic information among different network layers,
    bridging the gap between low-level and high-level features. This design enhances
    the model’s capability to incorporate comprehensive information and improve overall
    performance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ HSNet [[14](#bib.bib14)] 利用由 Transformer 和 CNN 网络组成的双分支结构，以捕捉长范围依赖关系和局部外观细节。此外，结合了交互机制，以促进不同网络层之间语义信息的交换，弥合低级特征和高级特征之间的差距。这一设计增强了模型整合全面信息的能力，提升了整体性能。
- en: $\bullet$ Fu-TransHNet [[6](#bib.bib6)] designs a novel feature fusion module
    to fully make use of the local and global features obtained from CNN and Transformer
    networks. The fusion module enables dense fusion of features at the same scale
    and multiple scales. Furthermore, weights for the CNN and Transformer branches
    and the fusion module are obtained through multi-view learning, resulting in flexibility
    to achieve optimal performance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Fu-TransHNet [[6](#bib.bib6)] 设计了一种新颖的特征融合模块，以充分利用从 CNN 和 Transformer
    网络获得的局部和全局特征。融合模块实现了相同尺度和多尺度特征的密集融合。此外，通过多视角学习获得 CNN 和 Transformer 分支以及融合模块的权重，从而在实现**最佳性能**方面具有灵活性。
- en: II-C Boundary-aware Models
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 边界感知模型
- en: The utilization of edge information as a guiding strategy was initially prevalent
    in object detection and has been increasingly applied to polyp segmentation tasks.
    Given the specific nature of medical image segmentation, precise edge-aware representation
    provides particular significance. In the following sections, we will review several
    models that exhibit exceptional capability in perceiving and incorporating edge
    information.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘信息作为指导策略最初在目标检测中很流行，近年来也越来越多地应用于息肉分割任务。鉴于医学图像分割的特殊性，精确的边缘感知表示具有特别的重要性。在接下来的章节中，我们将回顾几种在感知和整合边缘信息方面表现出色的模型。
- en: $\bullet$ FeDNet [[80](#bib.bib80)] simultaneously optimizes the main body and
    edges to improve polyp segmentation performance. It explicitly decouples the input
    features into body features and edge features and then conducts targeted optimization
    by introducing a feature decoupling module.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ FeDNet [[80](#bib.bib80)] 同时优化主体和边缘，以提高息肉分割性能。它明确地将输入特征解耦为主体特征和边缘特征，然后通过引入特征解耦模块进行针对性优化。
- en: $\bullet$ BSCA-Net [[69](#bib.bib69)] utilizes bit-plane slicing information
    to effectively extract boundary information. Additionally, a Segmentation Squeeze
    Bottleneck Union module is designed to utilize geometric information from different
    perspectives, and Multi-Path Connection Attention Decoders and Multi-Path Attention
    Connection Encoders are used to further enhance the network performance for polyp
    segmentation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ BSCA-Net [[69](#bib.bib69)] 利用位平面切片信息有效提取边界信息。此外，设计了一个分割挤压瓶颈联合模块来利用来自不同视角的几何信息，并使用多路径连接注意力解码器和多路径注意力连接编码器进一步增强网络性能以进行息肉分割。
- en: $\bullet$ BoxPolyp [[70](#bib.bib70)] mitigates overfitting issues by using
    box annotations, iteratively enhancing the segmentation model to generate fine-grained
    polyp regions. A Fusion Filter Sampling module is designed to generate pixel-level
    pseudo labels from less noisy box annotations, significantly improving performance.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ BoxPolyp [[70](#bib.bib70)] 通过使用盒注释来缓解过拟合问题，迭代地增强分割模型以生成细粒度的息肉区域。设计了一个融合滤波采样模块，从较少噪声的盒注释中生成像素级伪标签，从而显著提高性能。
- en: $\bullet$ BDG-Net [[63](#bib.bib63)] is a Boundary Distribution Guided Network.
    It utilizes a boundary distribution generation module to aggregate high-level
    features, which are taken as supplementary spatial information and fed to the
    Boundary Distribution Guided Decoder (BDGD) for guiding polyp segmentation. Additionally,
    the BDGD adopts a multi-scale feature interaction strategy to cope with size variations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ BDG-Net [[63](#bib.bib63)] 是一种边界分布引导网络。它利用边界分布生成模块来聚合高级特征，这些特征被作为补充空间信息输入到边界分布引导解码器（BDGD）中，以指导息肉分割。此外，BDGD采用多尺度特征交互策略来应对大小变化。
- en: $\bullet$ ICBNet [[71](#bib.bib71)] innovatively adopts an iterative feedback
    learning strategy, supplementing and perfecting encoder features from preliminary
    segmentation and boundary predictions using context and boundary-aware information.
    This strategy is iteratively employed to achieve progressive optimization improvements.
    Furthermore, a dual-branch iterative feedback unit is developed to enhance features
    under the guidance of segmentation and boundary prediction.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ICBNet [[71](#bib.bib71)] 创新性地采用了迭代反馈学习策略，通过使用上下文和边界感知信息补充和完善初步分割和边界预测的编码器特征。这一策略被迭代地应用，以实现逐步优化改进。此外，还开发了一个双分支迭代反馈单元，以在分割和边界预测的指导下增强特征。
- en: $\bullet$ CLD-Net [[72](#bib.bib72)] focuses on addressing the issue of feature
    loss during downsampling and effectively tackles the challenge of small polyp
    segmentation. It achieves this by incorporating a local edge feature extraction
    module and a local-global feature fusion module. The model initially extracts
    a series of edge features using a progressive strategy, subsequently handles noise,
    and finally integrates the edge features into the global features through an upsampling
    fusion strategy.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CLD-Net [[72](#bib.bib72)] 重点解决了下采样过程中特征丢失的问题，并有效应对了小息肉分割的挑战。它通过结合局部边缘特征提取模块和局部-全局特征融合模块来实现这一目标。该模型最初使用渐进策略提取一系列边缘特征，随后处理噪声，最后通过上采样融合策略将边缘特征整合到全局特征中。
- en: $\bullet$ BANet [[73](#bib.bib73)] accurately identifies the main location of
    polyps through an attention-aware localization module. In addition, it mines polyp
    boundary information through a residual pyramid convolution module and leverages
    boundary information for constrained polyp region prediction via a boundary-guided
    refinement module, thereby achieving more accurate segmentation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ BANet [[73](#bib.bib73)] 通过一个关注注意力的定位模块准确识别息肉的主要位置。此外，它通过残差金字塔卷积模块挖掘息肉边界信息，并利用边界信息通过边界引导的细化模块进行受限的息肉区域预测，从而实现更精确的分割。
- en: $\bullet$ SFA [[8](#bib.bib8)] improves segmentation performance by constructing
    a Selective Feature Aggregation Network with region and boundary constraints.
    It primarily consists of a shared encoder that predicts the polyp region and two
    mutually constrained decoders that extract edge information. By introducing three
    upwardly cascaded components between the encoder and decoder and embedding selective
    kernel modules into the convolutional layers, it achieves selective feature aggregation.
    Furthermore, a novel boundary-sensitive loss function is innovatively proposed
    to measure the dependency between regions and boundaries.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ SFA [[8](#bib.bib8)] 通过构建一个具有区域和边界约束的选择性特征聚合网络来提高分割性能。它主要由一个共享编码器预测息肉区域和两个互相约束的解码器组成，这些解码器提取边缘信息。通过在编码器和解码器之间引入三个向上级联的组件，并将选择性卷积模块嵌入卷积层中，实现选择性特征聚合。此外，还创新性地提出了一种新颖的边界敏感损失函数，用于测量区域和边界之间的依赖关系。
- en: $\bullet$ FCBFormer [[65](#bib.bib65)] fully leverages the strengths of fully
    convolutional networks (FCNs) and transformers for polyp segmentation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ FCBFormer [[65](#bib.bib65)] 充分利用了全卷积网络（FCNs）和变换器在息肉分割中的优势。
- en: II-D Attention-aware Models
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 注意力感知模型
- en: In the field of polyp segmentation, achieving superior performance requires
    the ability to prioritize relevant information rather than treating all information
    equally. By incorporating an attention mechanism, this issue can be effectively
    addressed, allowing the model to focus on the most important features and ultimately
    improving segmentation performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在息肉分割领域，要获得卓越的性能需要优先处理相关信息，而不是将所有信息一视同仁。通过引入注意力机制，可以有效地解决这个问题，使模型能够集中关注最重要的特征，从而提高分割性能。
- en: $\bullet$ CASCADE [[31](#bib.bib31)] takes advantage of the multi-scale features
    using a hierarchical visual transformer. It includes an attention gate that fuses
    skip-connection features and a convolutional attention module that suppresses
    background information to enhance distant and local contexts.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CASCADE [[31](#bib.bib31)] 利用分层视觉变换器优势多尺度特征。它包括一个注意力门，融合跳跃连接特征，以及一个卷积注意力模块，抑制背景信息以增强远程和局部上下文。
- en: $\bullet$ APCNet [[75](#bib.bib75)] extracts multi-level features from a pyramid
    structure, then presents an attention-guided multi-level aggregation strategy,
    enhancing each layer’s context features by leveraging complementary information
    from different layers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ APCNet [[75](#bib.bib75)] 从金字塔结构中提取多层次特征，然后呈现一种注意力引导的多层次聚合策略，通过利用不同层的互补信息来增强每一层的上下文特征。
- en: $\bullet$ RA-DENet [[76](#bib.bib76)] enhances the representation of different
    regions through inverse attention, then eliminates noise through distractor removal.
    It extracts low-level polyp features to obtain edge features, and by connecting
    these edge features with refined polyp features, resulting in promising polyp
    segmentation performance.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ RA-DENet [[76](#bib.bib76)] 通过逆注意力增强不同区域的表示，然后通过干扰物去除消除噪声。它提取低层次的息肉特征以获得边缘特征，并通过将这些边缘特征与精细化的息肉特征连接，从而取得了有前景的息肉分割性能。
- en: $\bullet$ TGANet [[29](#bib.bib29)] incorporates a text attention mechanism,
    which utilizes features related to the size and number of polyps to adapt to varying
    polyp sizes and effectively handle scenarios with multiple polyps. By assigning
    weights to the text-based embeddings through an auxiliary classification task,
    the network can learn additional feature representations and enhance its overall
    performance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ TGANet [[29](#bib.bib29)] 引入了文本注意力机制，利用与息肉大小和数量相关的特征来适应不同的息肉尺寸，并有效处理多息肉场景。通过通过辅助分类任务给文本嵌入分配权重，网络可以学习额外的特征表示，从而增强整体性能。
- en: $\bullet$ LDNet [[61](#bib.bib61)] extracts global context features from the
    input image and then updates them iteratively based on the lesion features predicted
    by the segmentation. Then, a self-attention module is presented to capture remote
    context relationships and improve segmentation performance. This model exhibits
    strong segmentation performance and generalization ability.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ LDNet [[61](#bib.bib61)] 从输入图像中提取全局上下文特征，然后根据分割预测的病变特征进行迭代更新。接着，提出了一个自注意力模块，以捕捉远程上下文关系并提高分割性能。该模型展现出强大的分割性能和泛化能力。
- en: $\bullet$ SANet [[10](#bib.bib10)] eliminates the impact of colors through a
    color swap operation, then filters out background noise from shallow features
    based on a shallow attention module. Additionally, it addresses the pixel imbalance
    issue in small polyps using a probability correction strategy. Thanks to these
    measures, it performs well on small polyp tasks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ SANet [[10](#bib.bib10)] 通过颜色交换操作消除颜色的影响，然后基于浅层注意模块从浅层特征中过滤出背景噪声。此外，它通过概率修正策略解决了小息肉中的像素不平衡问题。得益于这些措施，它在小息肉任务中表现良好。
- en: II-E Feature Fusion Models
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 特征融合模型
- en: In the domain of semantic segmentation, incorporating multi-scale features is
    critical for effectively dealing with variations in object sizes. Furthermore,
    integrating multi-level features through feature fusion and leveraging both high-level
    and low-level features can greatly enhance segmentation performance. In the context
    of polyp segmentation tasks, several feature fusion strategies have been employed
    to achieve this objective.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义分割领域，整合多尺度特征对于有效处理对象尺寸变化至关重要。此外，通过特征融合整合多级特征，并利用高级特征和低级特征可以极大地提升分割性能。在息肉分割任务的背景下，已经采用了多种特征融合策略来实现这一目标。
- en: $\bullet$ CFA-Net [[7](#bib.bib7)] is a novel cross-level feature aggregation
    network that adopts a hierarchical strategy to incorporate edge features into
    the dual-stream segmentation network. Additionally, the model proposes a cross-layer
    feature fusion module to integrate adjacent features from different levels.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CFA-Net [[7](#bib.bib7)] 是一个新颖的跨层级特征聚合网络，采用分层策略将边缘特征纳入双流分割网络。此外，该模型提出了一个跨层特征融合模块，以整合不同层级的相邻特征。
- en: $\bullet$ EFB-Seg [[77](#bib.bib77)] enhances multi-level feature fusion by
    introducing a feature fusion module that utilizes the learned semantic offset
    field to align multi-level feature maps, thereby addressing the issue of feature
    misalignment.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ EFB-Seg [[77](#bib.bib77)] 通过引入一个特征融合模块来增强多级特征融合，该模块利用学习到的语义偏移场对齐多级特征图，从而解决特征错位问题。
- en: $\bullet$ MSRF-Net [[28](#bib.bib28)]. This model innovatively uses a dual-scale
    dense fusion block to exchange multi-scale features with different receptive fields.
    It can preserve resolution, and propagate high-level and low-level features to
    achieve more accurate segmentation results.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MSRF-Net [[28](#bib.bib28)]。该模型创新性地使用了一个双尺度密集融合块来交换具有不同感受野的多尺度特征。它可以保持分辨率，并传播高级和低级特征，以实现更准确的分割结果。
- en: $\bullet$ DCRNet [[66](#bib.bib66)] captures both intra-image and inter-image
    context relationships. Within the image, a positional attention module is presented
    to capture pixel-level context information. Between images, feature enhancement
    is achieved by embedding context relation matrices, then relationship fusion is
    implemented through region cross-batch memory.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ DCRNet [[66](#bib.bib66)] 捕获了图像内和图像间的上下文关系。在图像内，提供了一个位置注意模块来捕获像素级的上下文信息。在图像间，通过嵌入上下文关系矩阵实现特征增强，然后通过区域跨批次记忆实现关系融合。
- en: $\bullet$ PPNet [[78](#bib.bib78)] uses a channel attention scheme in the pyramid
    feature fusion module to learn global context features, thereby guiding the information
    transformation of the decoder branches. Furthermore, it introduces a memory-retained
    pyramid pooling module in each side branch of the encoder to enhance feature extraction
    effectiveness.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PPNet [[78](#bib.bib78)] 在金字塔特征融合模块中使用通道注意机制来学习全局上下文特征，从而指导解码器分支的信息转换。此外，它在编码器的每个侧分支中引入了一个记忆保留金字塔池化模块，以增强特征提取的有效性。
- en: $\bullet$ PraNet [[9](#bib.bib9)] aggregates high-level features using parallel
    partial decoders, uses a reverse attention module to mine boundary clues, and
    establishes the relationship between region and boundary cues.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PraNet [[9](#bib.bib9)] 使用并行部分解码器聚合高级特征，采用反向注意模块挖掘边界线索，并建立区域与边界线索之间的关系。
- en: $\bullet$ PolypSeg [[44](#bib.bib44)] aggregates multi-scale context information
    and focuses on the target area using an improved attention mechanism. It then
    eliminates background noise from low-level features, enhancing the feature fusion
    between high-level and low-level features. Furthermore, the model reduces computational
    cost using depthwise separable convolution.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PolypSeg [[44](#bib.bib44)] 聚合了多尺度上下文信息，并通过改进的注意力机制专注于目标区域。然后，它从低级特征中消除背景噪声，增强了高级和低级特征之间的特征融合。此外，该模型通过使用深度可分离卷积来降低计算成本。
- en: II-F Video Polyp Segmentation
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F 视频息肉分割
- en: Accurate and real-time polyp image segmentation presents significant challenges
    due to the dependence on annotation quality and the complexity of deep learning
    models. To facilitate the deployment of automated segmentation methods in clinical
    settings, a shift in focus has been observed in some studies toward video-based
    polyp segmentation methods. By considering temporal information, these approaches
    aim to overcome the limitations of single-image segmentation and enable more precise
    and efficient segmentation in real-time scenarios within clinical requirements.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 准确和实时的息肉图像分割面临重大挑战，因为它依赖于标注质量和深度学习模型的复杂性。为了促进自动分割方法在临床环境中的部署，一些研究已经将重点转向基于视频的息肉分割方法。通过考虑时间信息，这些方法旨在克服单图像分割的局限性，并在临床要求的实时场景中实现更精确和高效的分割。
- en: $\bullet$ ESFPNet [[5](#bib.bib5)] constructs a pre-trained Mixed Transformer
    (MiT) encoder and an efficient stage-wise feature pyramid decoder, in which the
    MiT uses overlapping path merging modules and self-attention prediction, ultimately
    showing efficient performance and potential applicability in related fields.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ESFPNet [[5](#bib.bib5)] 构建了一个预训练的混合变换器（MiT）编码器和一个高效的分阶段特征金字塔解码器，其中
    MiT 使用重叠路径合并模块和自注意力预测，*最终展示了高效的性能和在相关领域的潜在应用*。
- en: $\bullet$ PNS+ [[83](#bib.bib83)] extracts long-term spatiotemporal representations
    using global encoders and local encoders and refines them gradually with normalized
    self-attention blocks. They introduce a frame-by-frame annotated video polyp segmentation
    dataset called SUN-SEG, which consists of 158,690 colonoscopy video frames. Extensive
    experiments have shown that PNS+ exhibits the best performance and real-time inference
    speed.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PNS+ [[83](#bib.bib83)] 使用全局编码器和局部编码器提取长期时空表示，并通过标准化自注意力块逐步精炼这些表示。他们引入了一个逐帧标注的视频息肉分割数据集
    SUN-SEG，其中包含 158,690 个结肠镜视频帧。大量实验表明，PNS+ 展现了最佳的性能和实时推理速度。
- en: $\bullet$ SSTAN [[84](#bib.bib84)] presents a semi-supervised video polyp segmentation
    task that only requires sparsely annotated frames for training. It introduces
    a novel spatio-temporal attention structure, consisting of temporal local context
    attention modules that refine the current prediction using predicted results from
    nearby frames, and spatial-temporal attention modules that capture long-range
    dependencies in both time and space using hybrid transformers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ SSTAN [[84](#bib.bib84)] 提出了一个半监督的视频息肉分割任务，仅需稀疏标注的帧进行训练。它引入了一种新颖的时空注意力结构，包括时间局部上下文注意力模块，这些模块利用来自相邻帧的预测结果来优化当前预测，以及时空注意力模块，这些模块使用混合变换器捕获时间和空间上的长距离依赖。
- en: $\bullet$ PNS-Net [[85](#bib.bib85)] leverages standard self-attention modules
    and CNN to efficiently learn representations from polyp videos in a real-time
    manner without the need for post-processing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PNS-Net [[85](#bib.bib85)] 利用标准自注意力模块和 CNN 高效地从息肉视频中学习表示，*无需后处理*。
- en: $\bullet$ NanoNet [[86](#bib.bib86)] has fewer parameters and can be integrated
    with mobile and embedded devices. It utilizes a pre-trained MobileNetV2 as the
    encoder. In the architecture between the encoder and decoder, a modified residual
    block is incorporated to enhance the generalization capability of the decoder.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ NanoNet [[86](#bib.bib86)] 参数更少，可与移动和嵌入式设备集成。它利用预训练的 MobileNetV2 作为编码器。在编码器和解码器之间的架构中，加入了改进的残差块，以增强解码器的泛化能力。
- en: '![Refer to caption](img/8e3bd5e9f1a23649164a6e0df46846e6.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8e3bd5e9f1a23649164a6e0df46846e6.png)'
- en: 'Figure 3: Examples of images, ground truth maps, and edges in five polyp segmentation
    datasets, including (a) ETIS-LaribPolypDB [[87](#bib.bib87)], (b) CVC-ColonDB [[16](#bib.bib16)],
    (c) CVC-ClinicDB [[18](#bib.bib18)], (d) CVC-300 [[25](#bib.bib25)], and (e) Kvasir-SEG [[88](#bib.bib88)].
    In each dataset, the image, ground truth maps, and edges are shown from top to
    bottom.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：五个息肉分割数据集的图像示例、地面实况图和边缘，包括（a）ETIS-LaribPolypDB [[87](#bib.bib87)]，（b）CVC-ColonDB [[16](#bib.bib16)]，（c）CVC-ClinicDB [[18](#bib.bib18)]，（d）CVC-300 [[25](#bib.bib25)]，以及（e）Kvasir-SEG [[88](#bib.bib88)]。在每个数据集中，图像、地面实况图和边缘从上到下依次展示。
- en: III Polyp Segmentation Datasets
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 息肉分割数据集
- en: 'The rapid progress in the field of medical image segmentation has led to the
    construction of various public benchmark datasets specifically designed for polyp
    segmentation tasks. These datasets have emerged in recent years and serve as standardized
    evaluation platforms for assessing the performance of different segmentation models.
    By utilizing these benchmark datasets, researchers can compare their methods against
    established baselines, facilitate reproducible research, and foster further advancements
    in the field of polyp segmentation. Table [III](#S3.T3 "TABLE III ‣ III Polyp
    Segmentation Datasets ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques,
    Challenges and Future Trends") summarizes eight popular image-level polyp segmentation
    datasets, and Fig. [3](#S2.F3 "Figure 3 ‣ II-F Video Polyp Segmentation ‣ II Polyp
    Segmentation Models ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques,
    Challenges and Future Trends") shows examples of images (including edge maps,
    and annotations) from these datasets. Moreover, we provide some details about
    each dataset below. Please note that some video-level polyp segmentation datasets
    are summarized in Table [III](#S3.T3 "TABLE III ‣ III Polyp Segmentation Datasets
    ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and
    Future Trends"), and some works [[16](#bib.bib16), [18](#bib.bib18), [89](#bib.bib89),
    [90](#bib.bib90), [83](#bib.bib83)] are based on them.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '医学图像分割领域的快速进展促使了各种公开基准数据集的建设，这些数据集专门为息肉分割任务设计。这些数据集近年来出现，作为评估不同分割模型性能的标准化平台。通过利用这些基准数据集，研究人员可以将他们的方法与既定基准进行比较，促进可重复的研究，并推动息肉分割领域的进一步发展。表[III](#S3.T3
    "TABLE III ‣ III Polyp Segmentation Datasets ‣ A Survey on Deep Learning for Polyp
    Segmentation: Techniques, Challenges and Future Trends")总结了八个流行的图像级息肉分割数据集，图[3](#S2.F3
    "Figure 3 ‣ II-F Video Polyp Segmentation ‣ II Polyp Segmentation Models ‣ A Survey
    on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends")展示了这些数据集中的图像示例（包括边缘图和标注）。此外，下面提供了每个数据集的一些详细信息。请注意，一些视频级息肉分割数据集在表[III](#S3.T3
    "TABLE III ‣ III Polyp Segmentation Datasets ‣ A Survey on Deep Learning for Polyp
    Segmentation: Techniques, Challenges and Future Trends")中进行了总结，一些工作[[16](#bib.bib16),
    [18](#bib.bib18), [89](#bib.bib89), [90](#bib.bib90), [83](#bib.bib83)]也基于这些数据集。'
- en: $\bullet$ ETIS-LaribPolypDB [[87](#bib.bib87)] collects early colorectal polyp
    images, consisting of $196$ polyp instances of size $966\times{1225}$.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ETIS-LaribPolypDB [[87](#bib.bib87)] 收集了早期结直肠息肉图像，包括$196$个大小为$966\times{1225}$的息肉实例。
- en: $\bullet$ CVC-ClinicDB [[18](#bib.bib18)] comes from clinical cases at a hospital
    in Barcelona, Spain, and is generated from 23 different standard white light colonoscopy
    intervention videos. This dataset contains 612 high-resolution color images of
    size $576\times{768}$, all derived from clinical colonoscopy examinations. Each
    image comes with a corresponding manual annotation file that clearly delineates
    the location of the polyp.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CVC-ClinicDB [[18](#bib.bib18)] 来源于西班牙巴塞罗那的一家医院的临床病例，并由23个不同的标准白光结肠镜干预视频生成。该数据集包含$612$张高分辨率的彩色图像，大小为$576\times{768}$，均源于临床结肠镜检查。每张图像附带一个手动标注文件，清晰标明息肉的位置。
- en: $\bullet$ CVC-ColonDB [[16](#bib.bib16)] is maintained by the Computer Vision
    Center (CVC) in Barcelona. It includes $380$ colonoscopy images of size $500\times{574}$
    and segmentation masks manually annotated to precisely mark the location of polyps.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CVC-ColonDB [[16](#bib.bib16)] 由位于巴塞罗那的计算机视觉中心（CVC）维护。该数据集包括$380$张大小为$500\times{574}$的结肠镜图像以及手动标注的分割掩码，用于精确标记息肉的位置。
- en: $\bullet$ CVC-300 [[25](#bib.bib25)] includes $60$ colonoscopy images with a
    resolution of $500\times{574}$.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CVC-300 [[25](#bib.bib25)] 包括$60$张分辨率为$500\times{574}$的结肠镜图像。
- en: $\bullet$ CVC-EndoSceneStill [[25](#bib.bib25)] includes CVC-ClinicDB and CVC-300,
    thereby containing 912 colonoscopy examination images along with corresponding
    annotations.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CVC-EndoSceneStill [[25](#bib.bib25)] 包含CVC-ClinicDB和CVC-300，因此包含912张结肠镜检查图像及其对应的标注。
- en: $\bullet$ Kvasir-SEG [[88](#bib.bib88)] contains $1,000$ images of gastrointestinal
    polyps along with corresponding segmentation masks and bounding boxes. These were
    manually annotated by a doctor and validated by a gastroenterology expert.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Kvasir-SEG [[88](#bib.bib88)] 包含$1,000$张胃肠息肉图像及其相应的分割掩码和边界框。这些图像由医生手动标注，并由胃肠科专家验证。
- en: $\bullet$ PICCOLO [[91](#bib.bib91)] consists of $3,433$ clinical colonoscopy
    images from 48 patients, including white light and narrow-band imaging images.
    It also provides annotations including the number and size of polyps detected
    during colonoscopy. The data is divided into a training set ($2,203$), a validation
    set ($897$), and a test set ($333$).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PICCOLO [[91](#bib.bib91)] 由48名患者的$3,433$张临床结肠镜图像组成，包括白光和窄带成像图像。它还提供了包括在结肠镜检查中检测到的息肉数量和大小的注释。数据被划分为训练集（$2,203$）、验证集（$897$）和测试集（$333$）。
- en: $\bullet$ PolypGen [[92](#bib.bib92)] originates from colonoscopy detection
    images of over $300$ patients from six different centers. It includes both single-frame
    data and sequence data, containing $3,762$ annotated polyp labels. The delineation
    of polyp boundaries has been validated by six senior gastroenterology experts.
    Specifically, the dataset also contains $4,275$ negative samples.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PolypGen [[92](#bib.bib92)] 来源于六个不同中心超过$300$名患者的结肠镜检测图像。它包含单帧数据和序列数据，含有$3,762$个标注的息肉标签。息肉边界的划分已由六位资深胃肠科专家验证。具体而言，数据集还包含$4,275$个负样本。
- en: 'TABLE III: According to the year (Year), publication journal (Pub.), dataset
    size (Size), number of objects in the images (Obj.), and resolution (Resolution),
    statistics on the datasets are conducted. More detailed information about each
    dataset can be found in Sec. [III](#S3 "III Polyp Segmentation Datasets ‣ A Survey
    on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends").
    These datasets can be downloaded from our website: [https://github.com/taozh2017/Awesome-Polyp-Segmentation](https://github.com/taozh2017/Awesome-Polyp-Segmentation).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 根据年份（Year）、出版期刊（Pub.）、数据集大小（Size）、图像中的物体数量（Obj.）和分辨率（Resolution），对数据集进行了统计。关于每个数据集的更详细信息，请参见第[III](#S3
    "III Polyp Segmentation Datasets ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends")节。这些数据集可以从我们的网站下载：[https://github.com/taozh2017/Awesome-Polyp-Segmentation](https://github.com/taozh2017/Awesome-Polyp-Segmentation)。'
- en: '|  | # | Dataset | Year | Pub. | Size | Obj. | Resolution |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | # | 数据集 | 年份 | 出版 | 大小 | 物体 | 分辨率 |'
- en: '| Image-level | 1 | ETIS-LaribPolypDB  [[87](#bib.bib87)] | 2014 | IJCARS |
    196 | Multiple | $1225\times{966}$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 图像级 | 1 | ETIS-LaribPolypDB  [[87](#bib.bib87)] | 2014 | IJCARS | 196 | 多重
    | $1225\times{966}$ |'
- en: '| 2 | CVC-ColonDB  [[16](#bib.bib16)] | 2015 | TMI | 380 | One | $574\times{500}$
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2 | CVC-ColonDB  [[16](#bib.bib16)] | 2015 | TMI | 380 | 单个 | $574\times{500}$
    |'
- en: '| 3 | CVC-ClinicDB  [[18](#bib.bib18)] | 2015 | CMIG | 612 | Multiple | $768\times{576}$
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 3 | CVC-ClinicDB  [[18](#bib.bib18)] | 2015 | CMIG | 612 | 多重 | $768\times{576}$
    |'
- en: '| 4 | CVC-300  [[25](#bib.bib25)] | 2017 | JHE | 60 | One | $574\times{500}$
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 4 | CVC-300  [[25](#bib.bib25)] | 2017 | JHE | 60 | 单个 | $574\times{500}$
    |'
- en: '| 5 | CVC-EndoSceneStill  [[25](#bib.bib25)] | 2017 | JHE | 912 | Multiple
    | $[574\sim 768]\times$ [500$\sim$ 576] |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 5 | CVC-EndoSceneStill  [[25](#bib.bib25)] | 2017 | JHE | 912 | 多重 | $[574\sim
    768]\times$ [500$\sim$ 576] |'
- en: '| 6 | Kvasir-SEG  [[88](#bib.bib88)] | 2020 | MMM | 1,000 | Multiple | $[487\sim
    1072]\times[332\sim 1920]$ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Kvasir-SEG  [[88](#bib.bib88)] | 2020 | MMM | 1,000 | 多重 | $[487\sim
    1072]\times[332\sim 1920]$ |'
- en: '| 7 | PICCOLO  [[91](#bib.bib91)] | 2020 | AS | 3,433 | Multiple | $[854\sim
    1920]\times[480\sim 1080]$ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 7 | PICCOLO  [[91](#bib.bib91)] | 2020 | AS | 3,433 | 多重 | $[854\sim 1920]\times[480\sim
    1080]$ |'
- en: '| 8 | PolypGen  [[92](#bib.bib92)] | 2021 | SD | 8,037 | Multiple | $[384\sim
    1920]\times[288\sim 1080]$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 8 | PolypGen  [[92](#bib.bib92)] | 2021 | SD | 8,037 | 多重 | $[384\sim 1920]\times[288\sim
    1080]$ |'
- en: '| Video-level | 9 | ASU-Mayo Clinic [[16](#bib.bib16)] | 2016 | TMI | 36,458
    | One | $688\times 550$ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 视频级 | 9 | ASU-Mayo Clinic [[16](#bib.bib16)] | 2016 | TMI | 36,458 | 单个 |
    $688\times 550$ |'
- en: '| 10 | CVC-ClinicVideoDB  [[93](#bib.bib93)] | 2017 | GIANA | 11,954 | Multiple
    | $N/A$ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 10 | CVC-ClinicVideoDB  [[93](#bib.bib93)] | 2017 | GIANA | 11,954 | 多重 |
    $N/A$ |'
- en: '| 11 | LDPolypVideo  [[89](#bib.bib89)] | 2021 | MICCAI | 40,266 | Multiple
    | $560\times 480$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 11 | LDPolypVideo  [[89](#bib.bib89)] | 2021 | MICCAI | 40,266 | 多重 | $560\times
    480$ |'
- en: '| 12 | SUN-SEG  [[83](#bib.bib83)] | 2022 | MIR | 158,690 | One | $[1158\sim
    1240]\times[1008\sim 1080]$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 12 | SUN-SEG  [[83](#bib.bib83)] | 2022 | MIR | 158,690 | 单个 | $[1158\sim
    1240]\times[1008\sim 1080]$ |'
- en: IV Model Evaluation and Analysis
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 模型评估与分析
- en: IV-A Evaluation Metrics
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 评估指标
- en: We briefly review several popular metrics for polyp segmentation task, *i.e.*,
    Intersection over Union (IoU), precision-recall ($PR$), specificity, Dice coefficient
    (Dice), F-measure ($F_{\beta}$) [[94](#bib.bib94)], mean absolute error ($MAE$) [[95](#bib.bib95)],
    structural measure ($S_{\alpha}$) [[96](#bib.bib96)], and enhanced-alignment measure
    ($E_{\phi}$) [[97](#bib.bib97)].
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要回顾了几种用于息肉分割任务的流行指标，*即*，交并比（IoU），精确度-召回率（PR），特异性，Dice系数（Dice），F-measure ($F_{\beta}$)
    [[94](#bib.bib94)]，平均绝对误差（MAE）[[95](#bib.bib95)]，结构测量（$S_{\alpha}$）[[96](#bib.bib96)]，以及增强对齐测量（$E_{\phi}$）[[97](#bib.bib97)]。
- en: First, let’s introduce some parameters. *TP* represents True Positives, which
    indicates the number of positive samples that the model correctly predicts as
    positive. *FP* represents False Positives, which represents the number of negative
    samples that the model incorrectly predicts as positive. *FN* represents False
    Negatives, which represents the number of positive samples that the model incorrectly
    predicts as negative.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们介绍一些参数。*TP* 代表真正例，表示模型正确预测为正的正样本数量。*FP* 代表假正例，表示模型错误地预测为正的负样本数量。*FN* 代表假负例，表示模型错误地预测为负的正样本数量。
- en: $\bullet$ PR. Precision represents the proportion of correctly predicted positive
    samples by the model. Recall (also known as Sensitivity) represents the proportion
    of true positive samples successfully detected by the model out of all actual
    positive samples. PR (Precision-Recall) curve can be plotted by taking recall
    as the x-axis and precision as the y-axis.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ PR。精确度表示模型正确预测为正样本的比例。召回率（也称为灵敏度）表示模型从所有实际正样本中成功检测到的正样本比例。PR（精确度-召回率）曲线可以通过将召回率作为x轴，精确度作为y轴来绘制。
- en: '|  | $\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}},~{}\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}.$
    |  | (1) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{精确度}=\frac{\text{TP}}{\text{TP}+\text{FP}},~{}\text{召回率}=\frac{\text{TP}}{\text{TP}+\text{FN}}.$
    |  | (1) |'
- en: $\bullet$ IoU. Intersection over Union, also known as the Jaccard coefficient,
    is used to measure the overlap between the predicted result and the ground truth
    target. It is commonly used in handling imbalanced datasets.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ IoU。交并比，也称为雅卡尔系数，用于测量预测结果与真实目标之间的重叠程度。它通常用于处理不平衡的数据集。
- en: '|  | $\text{IoU}=\frac{\text{TP}}{\text{TP}+\text{FP}+\text{FN}}.$ |  | (2)
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{IoU}=\frac{\text{TP}}{\text{TP}+\text{FP}+\text{FN}}.$ |  | (2)
    |'
- en: $\bullet$ F-measure($F_{\beta}$). The F-measure, also known as the F1-score,
    combines precision and recall by considering their harmonic mean. It allows adjusting
    the weights of precision and recall to address specific problems and is effective
    in handling imbalanced datasets.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ F-measure($F_{\beta}$)。F-measure，也称为F1-score，通过考虑精确度和召回率的调和均值来结合精确度和召回率。它允许调整精确度和召回率的权重，以解决特定问题，并在处理不平衡的数据集时效果显著。
- en: '|  | $F_{\beta}=(1+\beta^{2})\frac{(\text{Precision}\cdot\text{Recall})}{(\beta^{2}\cdot\text{Precision})+\text{Recall}}.$
    |  | (3) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{\beta}=(1+\beta^{2})\frac{(\text{精确度}\cdot\text{召回率})}{(\beta^{2}\cdot\text{精确度})+\text{召回率}}.$
    |  | (3) |'
- en: $\bullet$ Dice. Dice coefficient is a commonly used evaluation metric for measuring
    the similarity between predicted segmentation results and the ground truth segmentation
    targets. It provides a score ranging from 0 to 1, where 1 indicates a perfect
    match and 0 indicates no match at all.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Dice。Dice系数是用于衡量预测分割结果与真实分割目标之间相似性的常用评估指标。它提供了一个从0到1的评分，其中1表示完全匹配，0表示完全不匹配。
- en: '|  | $\text{Dice}=\frac{2\times\text{TP}}{2\times\text{TP}+\text{FP}+\text{FN}}.$
    |  | (4) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Dice}=\frac{2\times\text{TP}}{2\times\text{TP}+\text{FP}+\text{FN}}.$
    |  | (4) |'
- en: $\bullet$ S-measure ($S_{\alpha}$). Structure-measure [[96](#bib.bib96)] evaluates
    the structural similarity of segmentation results.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ S-measure ($S_{\alpha}$)。结构测量[[96](#bib.bib96)] 评估分割结果的结构相似性。
- en: '|  | $S_{\alpha}=\alpha\cdot S_{O}+(1-\alpha)\cdot S_{R}.$ |  | (5) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{\alpha}=\alpha\cdot S_{O}+(1-\alpha)\cdot S_{R}.$ |  | (5) |'
- en: In this context, $S_{O}$ represents the Object Similarity, which measures the
    overlap between the segmentation result and the ground truth target. $S_{R}$ represents
    the Region Similarity, which assesses the structural similarity between the segmentation
    result and the ground truth target. The parameter $\alpha$ is used to balance
    the weights between these two measures.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，$S_{O}$ 代表对象相似度，衡量分割结果与真实目标之间的重叠情况。$S_{R}$ 代表区域相似度，评估分割结果与真实目标之间的结构相似性。参数
    $\alpha$ 用于平衡这两种度量之间的权重。
- en: $\bullet$ E-measure ($E_{\phi}$). Enhanced-measure [[97](#bib.bib97)] is used
    to evaluate region coverage.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ E-measure ($E_{\phi}$)。增强度量[[97](#bib.bib97)] 用于评估区域覆盖。
- en: '|  | $E_{\phi}=\frac{(1+\phi^{2})\cdot S_{O}\cdot S_{R}}{\phi^{2}\cdot S_{O}+S_{R}},$
    |  | (6) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{\phi}=\frac{(1+\phi^{2})\cdot S_{O}\cdot S_{R}}{\phi^{2}\cdot S_{O}+S_{R}},$
    |  | (6) |'
- en: where, the parameter $\phi$ is a balancing parameter that adjusts the weight
    ratio between accuracy and region coverage.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，参数$\phi$是一个平衡参数，用于调整准确性和区域覆盖之间的权重比例。
- en: $\bullet$ MAE. Mean Absolute Error (MAE) is commonly used for evaluating regression
    tasks.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MAE。均方绝对误差（MAE）常用于评估回归任务。
- en: '|  | $\text{MAE}=\frac{1}{W\times H}\sum_{i=1}^{W}\sum_{j=1}^{H}\lvert S_{i,j}-G_{i,j}\rvert,$
    |  | (7) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MAE}=\frac{1}{W\times H}\sum_{i=1}^{W}\sum_{j=1}^{H}\lvert S_{i,j}-G_{i,j}\rvert,$
    |  | (7) |'
- en: where $W$ and $H$ denote the width and height of the map, $S$ represents the
    predicted segmentation map, and $G$ represents the ground truth segmentation map.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$W$和$H$分别表示地图的宽度和高度，$S$表示预测的分割图，$G$表示真实分割图。
- en: '$\bullet$ Specificity. Specificity measures the model’s ability to recognize
    negative samples. Specifically, Specificity can be calculated using the following
    formula:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Specificity。Specificity衡量模型识别负样本的能力。具体地，Specificity可以使用以下公式计算：
- en: '|  | $\text{Specificity}=\frac{\text{TN}}{\text{TN}+\text{FP}}.$ |  | (8) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Specificity}=\frac{\text{TN}}{\text{TN}+\text{FP}}.$ |  | (8) |'
- en: The value of Specificity ranges between 0 and 1, with a higher value indicating
    a stronger ability of the model to recognize negative samples.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Specificity的值介于0和1之间，值越高表示模型识别负样本的能力越强。
- en: IV-B Performance Comparison and Analysis
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 性能比较与分析
- en: 'TABLE IV: Benchmark results of 24 representative polyp segmentation models
    (18 CNN-based and 6 transformer-based models) on five commonly used datasets in
    terms of Dice, IoU, and $S_{\alpha}$. The three best results are shown in red,
    blue, and green fonts.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：24个代表性息肉分割模型（18个基于CNN的模型和6个基于Transformer的模型）在五个常用数据集上的基准结果，按Dice、IoU和$S_{\alpha}$进行评估。前三名结果以红色、蓝色和绿色字体显示。
- en: '| Method | Pub. | ETIS-Larib | CVC-ColonDB | CVC-ClinicDB | CVC-300 | Kvasir
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表 | ETIS-Larib | CVC-ColonDB | CVC-ClinicDB | CVC-300 | Kvasir |'
- en: '| Dice | IoU | $S_{\alpha}$ | Dice | IoU | $S_{\alpha}$ | Dice | IoU | $S_{\alpha}$
    | Dice | IoU | $S_{\alpha}$ | Dice | IoU | $S_{\alpha}$ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Dice | IoU | $S_{\alpha}$ | Dice | IoU | $S_{\alpha}$ | Dice | IoU | $S_{\alpha}$
    | Dice | IoU | $S_{\alpha}$ | Dice | IoU | $S_{\alpha}$ |'
- en: '| UNet [[24](#bib.bib24)] | MICCAI 2015 | .398 | .335 | .684 | .504 | .436
    | .710 | .823 | .755 | .889 | .710 | .627 | .843 | .818 | .746 | .858 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| UNet [[24](#bib.bib24)] | MICCAI 2015 | .398 | .335 | .684 | .504 | .436
    | .710 | .823 | .755 | .889 | .710 | .627 | .843 | .818 | .746 | .858 |'
- en: '| UNet++ [[27](#bib.bib27)] | MICCAI 2018 | .401 | .344 | .683 | .482 | .408
    | .692 | .794 | .729 | .873 | .707 | .624 | .839 | .821 | .743 | .862 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| UNet++ [[27](#bib.bib27)] | MICCAI 2018 | .401 | .344 | .683 | .482 | .408
    | .692 | .794 | .729 | .873 | .707 | .624 | .839 | .821 | .743 | .862 |'
- en: '| SFA [[8](#bib.bib8)] | MICCAI 2018 | .297 | .217 | .557 | .456 | .337 | .628
    | .700 | .607 | .793 | .467 | .329 | .640 | .723 | .611 | .782 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| SFA [[8](#bib.bib8)] | MICCAI 2018 | .297 | .217 | .557 | .456 | .337 | .628
    | .700 | .607 | .793 | .467 | .329 | .640 | .723 | .611 | .782 |'
- en: '| PraNet [[9](#bib.bib9)] | MICCAI 2020 | .628 | .567 | .794 | .712 | .640
    | .820 | .899 | .849 | .936 | .871 | .797 | .925 | .898 | .840 | .915 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| PraNet [[9](#bib.bib9)] | MICCAI 2020 | .628 | .567 | .794 | .712 | .640
    | .820 | .899 | .849 | .936 | .871 | .797 | .925 | .898 | .840 | .915 |'
- en: '| ACSNet [[12](#bib.bib12)] | MICCAI 2020 | .578 | .509 | .754 | .716 | .649
    | .829 | .882 | .826 | .927 | .863 | .787 | .923 | .898 | .838 | .920 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| ACSNet [[12](#bib.bib12)] | MICCAI 2020 | .578 | .509 | .754 | .716 | .649
    | .829 | .882 | .826 | .927 | .863 | .787 | .923 | .898 | .838 | .920 |'
- en: '| MSEG [[47](#bib.bib47)] | ArXiv 2021 | .700 | .630 | .828 | .735 | .666 |
    .834 | .909 | .864 | .938 | .874 | .804 | .924 | .897 | .839 | .912 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| MSEG [[47](#bib.bib47)] | ArXiv 2021 | .700 | .630 | .828 | .735 | .666 |
    .834 | .909 | .864 | .938 | .874 | .804 | .924 | .897 | .839 | .912 |'
- en: '| EU-Net [[48](#bib.bib48)] | CRV 2021 | .687 | .609 | .793 | .756 | .681 |
    .831 | .902 | .846 | .936 | .837 | .765 | .904 | .908 | .854 | .917 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| EU-Net [[48](#bib.bib48)] | CRV 2021 | .687 | .609 | .793 | .756 | .681 |
    .831 | .902 | .846 | .936 | .837 | .765 | .904 | .908 | .854 | .917 |'
- en: '| SANet [[10](#bib.bib10)] | MICCAI 2021 | .750 | .654 | .849 | .753 | .670
    | .837 | .916 | .859 | .939 | .888 | .815 | .928 | .904 | .847 | .915 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[10](#bib.bib10)] | MICCAI 2021 | .750 | .654 | .849 | .753 | .670
    | .837 | .916 | .859 | .939 | .888 | .815 | .928 | .904 | .847 | .915 |'
- en: '| MSNet [[11](#bib.bib11)] | MICCAI 2021 | .723 | .652 | .845 | .751 | .671
    | .838 | .918 | .869 | .946 | .865 | .799 | .926 | .905 | .849 | .923 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| MSNet [[11](#bib.bib11)] | MICCAI 2021 | .723 | .652 | .845 | .751 | .671
    | .838 | .918 | .869 | .946 | .865 | .799 | .926 | .905 | .849 | .923 |'
- en: '| UACANet-S [[50](#bib.bib50)] | ACM MM 2021 | .694 | .615 | .815 | .783 |
    .704 | .847 | .916 | .870 | .939 | .902 | .837 | .934 | .905 | .852 | .914 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| UACANet-S [[50](#bib.bib50)] | ACM MM 2021 | .694 | .615 | .815 | .783 |
    .704 | .847 | .916 | .870 | .939 | .902 | .837 | .934 | .905 | .852 | .914 |'
- en: '| UACANet-L [[50](#bib.bib50)] | ACM MM 2021 | .766 | .689 | .859 | .751 |
    .678 | .835 | .926 | .880 | .942 | .910 | .849 | .938 | .912 | .859 | .917 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| UACANet-L [[50](#bib.bib50)] | ACM MM 2021 | .766 | .689 | .859 | .751 |
    .678 | .835 | .926 | .880 | .942 | .910 | .849 | .938 | .912 | .859 | .917 |'
- en: '| C2FNet [[51](#bib.bib51)] | IJCAI 2021 | .699 | .624 | .827 | .724 | .650
    | .826 | .919 | .872 | .941 | .874 | .801 | .927 | .886 | .831 | .905 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| C2FNet [[51](#bib.bib51)] | IJCAI 2021 | .699 | .624 | .827 | .724 | .650
    | .826 | .919 | .872 | .941 | .874 | .801 | .927 | .886 | .831 | .905 |'
- en: '| DCRNet [[66](#bib.bib66)] | ISBI 2022 | .556 | .496 | .736 | .704 | .631
    | .821 | .896 | .844 | .933 | .856 | .788 | .921 | .886 | .825 | .911 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| DCRNet [[66](#bib.bib66)] | ISBI 2022 | .556 | .496 | .736 | .704 | .631
    | .821 | .896 | .844 | .933 | .856 | .788 | .921 | .886 | .825 | .911 |'
- en: '| BDG-Net [[63](#bib.bib63)] | SPIE MI 2022 | .752 | .681 | .860 | .797 | .723
    | .870 | .905 | .857 | .936 | .902 | .837 | .940 | .915 | .863 | .920 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| BDG-Net [[63](#bib.bib63)] | SPIE MI 2022 | .752 | .681 | .860 | .797 | .723
    | .870 | .905 | .857 | .936 | .902 | .837 | .940 | .915 | .863 | .920 |'
- en: '| CaraNet [[74](#bib.bib74)] | SPIE MI 2022 | .747 | .672 | .868 | .773 | .689
    | .853 | .936 | .887 | .954 | .900 | .838 | .940 | .916 | .865 | .929 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| CaraNet [[74](#bib.bib74)] | SPIE MI 2022 | .747 | .672 | .868 | .773 | .689
    | .853 | .936 | .887 | .954 | .900 | .838 | .940 | .916 | .865 | .929 |'
- en: '| EFA-Net [[98](#bib.bib98)] | Arxiv 2023 | .749 | .670 | .858 | .774 | .696
    | .855 | .919 | .871 | .943 | .894 | .830 | .941 | .914 | .861 | .929 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| EFA-Net [[98](#bib.bib98)] | Arxiv 2023 | .749 | .670 | .858 | .774 | .696
    | .855 | .919 | .871 | .943 | .894 | .830 | .941 | .914 | .861 | .929 |'
- en: '| CFANet [[7](#bib.bib7)] | PR 2023 | .732 | .655 | .845 | .743 | .665 | .835
    | .932 | .883 | .950 | .893 | .827 | .938 | .915 | .861 | .924 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| CFANet [[7](#bib.bib7)] | PR 2023 | .732 | .655 | .845 | .743 | .665 | .835
    | .932 | .883 | .950 | .893 | .827 | .938 | .915 | .861 | .924 |'
- en: '| M2SNet [[99](#bib.bib99)] | Arxiv 2023 | .723 | .652 | .845 | .751 | .671
    | .838 | .918 | .869 | .946 | .865 | .799 | .926 | .905 | .849 | .923 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| M2SNet [[99](#bib.bib99)] | Arxiv 2023 | .723 | .652 | .845 | .751 | .671
    | .838 | .918 | .869 | .946 | .865 | .799 | .926 | .905 | .849 | .923 |'
- en: '| HSNet [[14](#bib.bib14)] | CBM 2022 | .808 | .734 | .882 | .810 | .735 |
    .868 | .948 | .905 | .953 | .903 | .839 | .937 | .926 | .877 | .927 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| HSNet [[14](#bib.bib14)] | CBM 2022 | .808 | .734 | .882 | .810 | .735 |
    .868 | .948 | .905 | .953 | .903 | .839 | .937 | .926 | .877 | .927 |'
- en: '| DuAT [[15](#bib.bib15)] | Arxiv 2022 | .822 | .746 | .889 | .819 | .737 |
    .873 | .948 | .906 | .956 | .901 | .840 | .940 | .924 | .876 | .929 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| DuAT [[15](#bib.bib15)] | Arxiv 2022 | .822 | .746 | .889 | .819 | .737 |
    .873 | .948 | .906 | .956 | .901 | .840 | .940 | .924 | .876 | .929 |'
- en: '| Polyp-PVT [[13](#bib.bib13)] | AIR 2023 | .787 | .706 | .871 | .808 | .727
    | .865 | .937 | .889 | .949 | .900 | .833 | .935 | .917 | .864 | .925 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Polyp-PVT [[13](#bib.bib13)] | AIR 2023 | .787 | .706 | .871 | .808 | .727
    | .865 | .937 | .889 | .949 | .900 | .833 | .935 | .917 | .864 | .925 |'
- en: '| ESFPNet [[5](#bib.bib5)] | MI 2023 | .823 | .748 | .891 | .811 | .730 | .864
    | .928 | .883 | .943 | .902 | .836 | .934 | .917 | .866 | .923 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ESFPNet [[5](#bib.bib5)] | MI 2023 | .823 | .748 | .891 | .811 | .730 | .864
    | .928 | .883 | .943 | .902 | .836 | .934 | .917 | .866 | .923 |'
- en: '| FeDNet [[80](#bib.bib80)] | BSPC 2023 | .810 | .733 | .892 | .823 | .744
    | .878 | .930 | .885 | .949 | .911 | .848 | .946 | .924 | .876 | .933 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| FeDNet [[80](#bib.bib80)] | BSPC 2023 | .810 | .733 | .892 | .823 | .744
    | .878 | .930 | .885 | .949 | .911 | .848 | .946 | .924 | .876 | .933 |'
- en: '| SAM-B [[100](#bib.bib100)] | Arxiv 2023 | .406 | .370 | .672 | .215 | .188
    | .553 | .268 | .231 | .572 | .371 | .339 | .650 | .515 | .459 | .682 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| SAM-B [[100](#bib.bib100)] | Arxiv 2023 | .406 | .370 | .672 | .215 | .188
    | .553 | .268 | .231 | .572 | .371 | .339 | .650 | .515 | .459 | .682 |'
- en: '| SAM-H [[100](#bib.bib100)] | Arxiv 2023 | .517 | .477 | .730 | .441 | .396
    | .676 | .547 | .500 | .738 | .651 | .606 | .812 | .778 | .707 | .829 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| SAM-H [[100](#bib.bib100)] | Arxiv 2023 | .517 | .477 | .730 | .441 | .396
    | .676 | .547 | .500 | .738 | .651 | .606 | .812 | .778 | .707 | .829 |'
- en: '| SAM-L [[100](#bib.bib100)] | Arxiv 2023 | .551 | .507 | .751 | .468 | .422
    | .690 | .578 | .526 | .744 | .726 | .676 | .849 | .782 | .710 | .832 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| SAM-L [[100](#bib.bib100)] | Arxiv 2023 | .551 | .507 | .751 | .468 | .422
    | .690 | .578 | .526 | .744 | .726 | .676 | .849 | .782 | .710 | .832 |'
- en: 'TABLE V: Benchmark results of 24 representative polyp segmentation models (18
    CNN-based and 6 transformer-based models) on five commonly used datasets in terms
    of $F_{\beta}$, $E_{\phi}$, and MAE. The three best results are shown in red,
    blue, and green fonts.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 24 个代表性息肉分割模型（18 个基于 CNN 的模型和 6 个基于变换器的模型）在五个常用数据集上的基准结果，依据 $F_{\beta}$、$E_{\phi}$
    和 MAE。前三个最佳结果以红色、蓝色和绿色字体显示。'
- en: '| Method | Pub. | ETIS-Larib | CVC-ColonDB | CVC-ClinicDB | CVC-300 | Kvasir
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Pub. | ETIS-Larib | CVC-ColonDB | CVC-ClinicDB | CVC-300 | Kvasir |'
- en: '| $F_{\beta}$ | $E_{\phi}$ | MAE | $F_{\beta}$ | $E_{\phi}$ | MAE | $F_{\beta}$
    | $E_{\phi}$ | MAE | $F_{\beta}$ | $E_{\phi}$ | MAE | $F_{\beta}$ | $E_{\phi}$
    | MAE |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| $F_{\beta}$ | $E_{\phi}$ | MAE | $F_{\beta}$ | $E_{\phi}$ | MAE | $F_{\beta}$
    | $E_{\phi}$ | MAE | $F_{\beta}$ | $E_{\phi}$ | MAE | $F_{\beta}$ | $E_{\phi}$
    | MAE |'
- en: '| UNet [[24](#bib.bib24)] | MICCAI 2015 | .366 | .643 | .036 | .491 | .692
    | .059 | .811 | .913 | .019 | .684 | .848 | .022 | .794 | .881 | .055 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| UNet [[24](#bib.bib24)] | MICCAI 2015 | .366 | .643 | .036 | .491 | .692
    | .059 | .811 | .913 | .019 | .684 | .848 | .022 | .794 | .881 | .055 |'
- en: '| UNet++ [[27](#bib.bib27)] | MICCAI 2018 | .390 | .629 | .035 | .467 | .680
    | .061 | .785 | .891 | .022 | .687 | .834 | .018 | .808 | .886 | .048 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| UNet++ [[27](#bib.bib27)] | MICCAI 2018 | .390 | .629 | .035 | .467 | .680
    | .061 | .785 | .891 | .022 | .687 | .834 | .018 | .808 | .886 | .048 |'
- en: '| SFA [[8](#bib.bib8)] | MICCAI 2018 | .231 | .531 | .109 | .366 | .661 | .094
    | .647 | .840 | .042 | .341 | .644 | .065 | .670 | .834 | .075 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| SFA [[8](#bib.bib8)] | MICCAI 2018 | .231 | .531 | .109 | .366 | .661 | .094
    | .647 | .840 | .042 | .341 | .644 | .065 | .670 | .834 | .075 |'
- en: '| PraNet [[9](#bib.bib9)] | MICCAI 2020 | .600 | .808 | .031 | .699 | .847
    | .043 | .896 | .963 | .009 | .843 | .950 | .010 | .885 | .944 | .030 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| PraNet [[9](#bib.bib9)] | MICCAI 2020 | .600 | .808 | .031 | .699 | .847
    | .043 | .896 | .963 | .009 | .843 | .950 | .010 | .885 | .944 | .030 |'
- en: '| ACSNet [[12](#bib.bib12)] | MICCAI 2020 | .530 | .737 | .059 | .697 | .839
    | .039 | .873 | .947 | .011 | .825 | .939 | .013 | .882 | .941 | .032 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| ACSNet [[12](#bib.bib12)] | MICCAI 2020 | .530 | .737 | .059 | .697 | .839
    | .039 | .873 | .947 | .011 | .825 | .939 | .013 | .882 | .941 | .032 |'
- en: '| MSEG [[47](#bib.bib47)] | ArXiv 2021 | .671 | .855 | .015 | .724 | .859 |
    .038 | .907 | .961 | .007 | .852 | .948 | .009 | .885 | .942 | .028 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| MSEG [[47](#bib.bib47)] | ArXiv 2021 | .671 | .855 | .015 | .724 | .859 |
    .038 | .907 | .961 | .007 | .852 | .948 | .009 | .885 | .942 | .028 |'
- en: '| EU-Net [[48](#bib.bib48)] | CRV 2021 | .636 | .807 | .067 | .730 | .863 |
    .045 | .891 | .959 | .011 | .805 | .918 | .015 | .893 | .951 | .028 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| EU-Net [[48](#bib.bib48)] | CRV 2021 | .636 | .807 | .067 | .730 | .863 |
    .045 | .891 | .959 | .011 | .805 | .918 | .015 | .893 | .951 | .028 |'
- en: '| SANet [[10](#bib.bib10)] | MICCAI 2021 | .685 | .881 | .015 | .726 | .869
    | .043 | .909 | .971 | .012 | .859 | .962 | .008 | .892 | .949 | .028 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[10](#bib.bib10)] | MICCAI 2021 | .685 | .881 | .015 | .726 | .869
    | .043 | .909 | .971 | .012 | .859 | .962 | .008 | .892 | .949 | .028 |'
- en: '| MSNet [[11](#bib.bib11)] | MICCAI 2021 | .677 | .875 | .020 | .736 | .872
    | .041 | .913 | .973 | .008 | .848 | .945 | .010 | .892 | .947 | .028 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| MSNet [[11](#bib.bib11)] | MICCAI 2021 | .677 | .875 | .020 | .736 | .872
    | .041 | .913 | .973 | .008 | .848 | .945 | .010 | .892 | .947 | .028 |'
- en: '| UACANet-S [[50](#bib.bib50)] | ACM MM 2021 | .650 | .848 | .023 | .772 |
    .894 | .034 | .917 | .965 | .008 | .886 | .974 | .006 | .897 | .948 | .026 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| UACANet-S [[50](#bib.bib50)] | ACM MM 2021 | .650 | .848 | .023 | .772 |
    .894 | .034 | .917 | .965 | .008 | .886 | .974 | .006 | .897 | .948 | .026 |'
- en: '| UACANet-L [[50](#bib.bib50)] | ACM MM 2021 | .740 | .903 | .012 | .746 |
    .875 | .039 | .928 | .974 | .006 | .901 | .977 | .005 | .902 | .955 | .025 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| UACANet-L [[50](#bib.bib50)] | ACM MM 2021 | .740 | .903 | .012 | .746 |
    .875 | .039 | .928 | .974 | .006 | .901 | .977 | .005 | .902 | .955 | .025 |'
- en: '| C2FNet [[51](#bib.bib51)] | IJCAI 2021 | .668 | .860 | .022 | .705 | .854
    | .044 | .906 | .969 | .009 | .844 | .949 | .009 | .869 | .929 | .036 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| C2FNet [[51](#bib.bib51)] | IJCAI 2021 | .668 | .860 | .022 | .705 | .854
    | .044 | .906 | .969 | .009 | .844 | .949 | .009 | .869 | .929 | .036 |'
- en: '| DCRNet [[66](#bib.bib66)] | ISBI 2022 | .506 | .742 | .096 | .684 | .840
    | .052 | .890 | .964 | .010 | .830 | .943 | .010 | .868 | .933 | .035 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| DCRNet [[66](#bib.bib66)] | ISBI 2022 | .506 | .742 | .096 | .684 | .840
    | .052 | .890 | .964 | .010 | .830 | .943 | .010 | .868 | .933 | .035 |'
- en: '| BDG-Net [[63](#bib.bib63)] | SPIE MI 2022 | .719 | .901 | .014 | .781 | .901
    | .028 | .898 | .959 | .008 | .883 | .969 | .005 | .906 | .959 | .025 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| BDG-Net [[63](#bib.bib63)] | SPIE MI 2022 | .719 | .901 | .014 | .781 | .901
    | .028 | .898 | .959 | .008 | .883 | .969 | .005 | .906 | .959 | .025 |'
- en: '| CaraNet [[74](#bib.bib74)] | SPIE MI 2022 | .709 | .875 | .017 | .729 | .880
    | .042 | .931 | .985 | .007 | .887 | .977 | .007 | .909 | .962 | .023 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| CaraNet [[74](#bib.bib74)] | SPIE MI 2022 | .709 | .875 | .017 | .729 | .880
    | .042 | .931 | .985 | .007 | .887 | .977 | .007 | .909 | .962 | .023 |'
- en: '| EFA-Net [[98](#bib.bib98)] | Arxiv 2023 | .698 | .872 | .018 | .753 | .884
    | .036 | .916 | .972 | .009 | .878 | .961 | .009 | .906 | .955 | .024 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| EFA-Net [[98](#bib.bib98)] | Arxiv 2023 | .698 | .872 | .018 | .753 | .884
    | .036 | .916 | .972 | .009 | .878 | .961 | .009 | .906 | .955 | .024 |'
- en: '| CFANet [[7](#bib.bib7)] | PR 2023 | .693 | .881 | .014 | .728 | .869 | .039
    | .924 | .981 | .007 | .875 | .962 | .008 | .903 | .956 | .023 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| CFANet [[7](#bib.bib7)] | PR 2023 | .693 | .881 | .014 | .728 | .869 | .039
    | .924 | .981 | .007 | .875 | .962 | .008 | .903 | .956 | .023 |'
- en: '| M2SNet [[99](#bib.bib99)] | Arxiv 2023 | .677 | .875 | .020 | .736 | .872
    | .041 | .913 | .973 | .008 | .848 | .945 | .010 | .892 | .947 | .028 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| M2SNet [[99](#bib.bib99)] | Arxiv 2023 | .677 | .875 | .020 | .736 | .872
    | .041 | .913 | .973 | .008 | .848 | .945 | .010 | .892 | .947 | .028 |'
- en: '| HSNet [[14](#bib.bib14)] | CBM 2022 | .777 | .904 | .021 | .796 | .912 |
    .032 | .951 | .990 | .006 | .887 | .970 | .007 | .918 | .961 | .023 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| HSNet [[14](#bib.bib14)] | CBM 2022 | .777 | .904 | .021 | .796 | .912 |
    .032 | .951 | .990 | .006 | .887 | .970 | .007 | .918 | .961 | .023 |'
- en: '| DuAT [[15](#bib.bib15)] | Arxiv 2022 | .789 | .917 | .013 | .805 | .922 |
    .026 | .950 | .990 | .006 | .890 | .965 | .005 | .916 | .960 | .023 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| DuAT [[15](#bib.bib15)] | Arxiv 2022 | .789 | .917 | .013 | .805 | .922 |
    .026 | .950 | .990 | .006 | .890 | .965 | .005 | .916 | .960 | .023 |'
- en: '| Polyp-PVT [[13](#bib.bib13)] | AIR 2023 | .750 | .906 | .013 | .795 | .913
    | .031 | .936 | .985 | .006 | .884 | .973 | .007 | .911 | .956 | .023 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Polyp-PVT [[13](#bib.bib13)] | AIR 2023 | .750 | .906 | .013 | .795 | .913
    | .031 | .936 | .985 | .006 | .884 | .973 | .007 | .911 | .956 | .023 |'
- en: '| ESFPNet [[5](#bib.bib5)] | MI 2023 | .786 | .930 | .012 | .798 | .908 | .030
    | .930 | .976 | .007 | .882 | .970 | .006 | .913 | .957 | .024 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| ESFPNet [[5](#bib.bib5)] | MI 2023 | .786 | .930 | .012 | .798 | .908 | .030
    | .930 | .976 | .007 | .882 | .970 | .006 | .913 | .957 | .024 |'
- en: '| FeDNet [[80](#bib.bib80)] | BSPC 2023 | .773 | .931 | .016 | .809 | .918
    | .029 | .928 | .978 | .007 | .897 | .976 | .006 | .918 | .963 | .021 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| FeDNet [[80](#bib.bib80)] | BSPC 2023 | .773 | .931 | .016 | .809 | .918
    | .029 | .928 | .978 | .007 | .897 | .976 | .006 | .918 | .963 | .021 |'
- en: '| SAM-B [[100](#bib.bib100)] | Arxiv 2023 | .404 | .574 | .035 | .210 | .412
    | .077 | .259 | .431 | .092 | .374 | .563 | .058 | .509 | .624 | .104 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| SAM-B [[100](#bib.bib100)] | Arxiv 2023 | .404 | .574 | .035 | .210 | .412
    | .077 | .259 | .431 | .092 | .374 | .563 | .058 | .509 | .624 | .104 |'
- en: '| SAM-H [[100](#bib.bib100)] | Arxiv 2023 | .513 | .658 | .029 | .434 | .585
    | .056 | .546 | .676 | .040 | .653 | .765 | .020 | .769 | .828 | .062 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| SAM-H [[100](#bib.bib100)] | Arxiv 2023 | .513 | .658 | .029 | .434 | .585
    | .056 | .546 | .676 | .040 | .653 | .765 | .020 | .769 | .828 | .062 |'
- en: '| SAM-L [[100](#bib.bib100)] | Arxiv 2023 | .544 | .686 | .030 | .463 | .607
    | .054 | .563 | .683 | .057 | .729 | .824 | .020 | .773 | .834 | .061 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| SAM-L [[100](#bib.bib100)] | Arxiv 2023 | .544 | .686 | .030 | .463 | .607
    | .054 | .563 | .683 | .057 | .729 | .824 | .020 | .773 | .834 | .061 |'
- en: IV-B1 Overall Evaluation
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 总体评估
- en: '![Refer to caption](img/7a44468a8ff36b6cb29cefb4519810e8.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7a44468a8ff36b6cb29cefb4519810e8.png)'
- en: 'Figure 4: Some images of polyps in large, medium, and small sizes, along with
    the segmentation maps of six typical models, including three CNN-based models:
    PraNet [[9](#bib.bib9)], SANet [[10](#bib.bib10)], and CFA-Net [[7](#bib.bib7)],
    and three transformer-based models: Polyp-PVT [[13](#bib.bib13)], HSNet [[14](#bib.bib14)],
    and DuAT [[15](#bib.bib15)]. The numbers on the GT map represent the proportion
    of polyp pixels to the total number of pixels in the image. The images are sourced
    from the Kvasir-SEG [[88](#bib.bib88)] dataset.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：展示了大、中、小尺寸的息肉图像，以及六种典型模型的分割图，包括三种基于CNN的模型：PraNet [[9](#bib.bib9)]、SANet [[10](#bib.bib10)]
    和 CFA-Net [[7](#bib.bib7)]，以及三种基于Transformer的模型：Polyp-PVT [[13](#bib.bib13)]、HSNet [[14](#bib.bib14)]
    和 DuAT [[15](#bib.bib15)]。GT图上的数字表示息肉像素占图像总像素的比例。图像来源于Kvasir-SEG [[88](#bib.bib88)]数据集。
- en: 'To quantify the performance of different models, we conducted a comprehensive
    evaluation of 24 representative polyp segmentation models, including 1) eighteen
    CNN-based models: UNet  [[24](#bib.bib24)], UNet++  [[27](#bib.bib27)], SFA [[8](#bib.bib8)],
    PraNet [[9](#bib.bib9)], ACSNet [[12](#bib.bib12)], MSEG [[47](#bib.bib47)], EU-Net [[48](#bib.bib48)],
    SANet [[10](#bib.bib10)], MSNet [[11](#bib.bib11)], UACANet-S [[50](#bib.bib50)],
    UACANet-L [[50](#bib.bib50)], C2FNet [[51](#bib.bib51)], DCRNet [[66](#bib.bib66)],
    BDG-Net [[63](#bib.bib63)], CaraNet [[74](#bib.bib74)], EFA-Net [[98](#bib.bib98)],
    CFA-Net [[7](#bib.bib7)], and M2SNet [[99](#bib.bib99)], and 2) six Transformer-based
    methods: Polyp-PVT [[13](#bib.bib13)], HSNet [[14](#bib.bib14)], DuAT [[15](#bib.bib15)],
    ESFPNet [[5](#bib.bib5)], FeDNet [[80](#bib.bib80)], and SAM-based poly segmentation
    model (with three different backbones, *i.e.*, SAM-B, SAM-H, and SAM-L) [[100](#bib.bib100)].
    Firstly, we individually evaluated the performance of the aforementioned 24 models
    on Dice, IoU, $S_{\alpha}$, $F_{\beta}$, $E_{\phi}$, and MAE metrics across five
    public datasets (ETIS-LaribPolypDB [[87](#bib.bib87)], CVC-ColonDB [[16](#bib.bib16)],
    CVC-ClinicDB [[18](#bib.bib18)], CVC-300 [[25](#bib.bib25)], and Kvasir-SEG [[88](#bib.bib88)]).
    The evaluation results are presented in Tab. [IV](#S4.T4 "TABLE IV ‣ IV-B Performance
    Comparison and Analysis ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep
    Learning for Polyp Segmentation: Techniques, Challenges and Future Trends") and
    Tab. [V](#S4.T5 "TABLE V ‣ IV-B Performance Comparison and Analysis ‣ IV Model
    Evaluation and Analysis ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques,
    Challenges and Future Trends"). Secondly, we also report the mean values of Dice
    and MAE across the five datasets for each model in Fig. [5](#S4.F5 "Figure 5 ‣
    IV-B1 Overall Evaluation ‣ IV-B Performance Comparison and Analysis ‣ IV Model
    Evaluation and Analysis ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques,
    Challenges and Future Trends"). It is worth noting that better models are shown
    in the upper left corner (*i.e.*, with a larger mDice and smaller MAE). From the
    results shown in Fig. [5](#S4.F5 "Figure 5 ‣ IV-B1 Overall Evaluation ‣ IV-B Performance
    Comparison and Analysis ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep
    Learning for Polyp Segmentation: Techniques, Challenges and Future Trends"), we
    have the following observations:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '为了量化不同模型的性能，我们对24种具有代表性的息肉分割模型进行了全面评估，包括1) 十八种基于CNN的模型：UNet [[24](#bib.bib24)]、UNet++
    [[27](#bib.bib27)]、SFA [[8](#bib.bib8)]、PraNet [[9](#bib.bib9)]、ACSNet [[12](#bib.bib12)]、MSEG
    [[47](#bib.bib47)]、EU-Net [[48](#bib.bib48)]、SANet [[10](#bib.bib10)]、MSNet [[11](#bib.bib11)]、UACANet-S
    [[50](#bib.bib50)]、UACANet-L [[50](#bib.bib50)]、C2FNet [[51](#bib.bib51)]、DCRNet
    [[66](#bib.bib66)]、BDG-Net [[63](#bib.bib63)]、CaraNet [[74](#bib.bib74)]、EFA-Net
    [[98](#bib.bib98)]、CFA-Net [[7](#bib.bib7)] 和 M2SNet [[99](#bib.bib99)]，以及2) 六种基于Transformer的方法：Polyp-PVT
    [[13](#bib.bib13)]、HSNet [[14](#bib.bib14)]、DuAT [[15](#bib.bib15)]、ESFPNet [[5](#bib.bib5)]、FeDNet
    [[80](#bib.bib80)] 和 基于SAM的息肉分割模型（具有三种不同骨干网络，*即*，SAM-B、SAM-H 和 SAM-L）[[100](#bib.bib100)]。首先，我们分别评估了上述24种模型在五个公共数据集（ETIS-LaribPolypDB
    [[87](#bib.bib87)]、CVC-ColonDB [[16](#bib.bib16)]、CVC-ClinicDB [[18](#bib.bib18)]、CVC-300
    [[25](#bib.bib25)] 和 Kvasir-SEG [[88](#bib.bib88)]）上的Dice、IoU、$S_{\alpha}$、$F_{\beta}$、$E_{\phi}$
    和 MAE指标的表现。评估结果见表格[IV](#S4.T4 "TABLE IV ‣ IV-B Performance Comparison and Analysis
    ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends") 和表格[V](#S4.T5 "TABLE V ‣ IV-B Performance
    Comparison and Analysis ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep
    Learning for Polyp Segmentation: Techniques, Challenges and Future Trends")。其次，我们还报告了每个模型在五个数据集上的Dice和MAE的均值，见图[5](#S4.F5
    "Figure 5 ‣ IV-B1 Overall Evaluation ‣ IV-B Performance Comparison and Analysis
    ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends")。值得注意的是，表现较好的模型显示在左上角（*即*，具有较大的mDice和较小的MAE）。从图[5](#S4.F5
    "Figure 5 ‣ IV-B1 Overall Evaluation ‣ IV-B Performance Comparison and Analysis
    ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends")中显示的结果，我们有如下观察：'
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CNN vs. Transformer. Compared with CNN-based models, Transformer-based methods
    obtain significantly better performance. Due to its feature extraction network
    structure based on the self-attention mechanism, the Transformer can effectively
    capture global context information.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CNN与Transformer。与基于CNN的模型相比，基于Transformer的方法获得了显著更好的性能。由于其基于自注意力机制的特征提取网络结构，Transformer能够有效捕捉全局上下文信息。
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Comparison of Deep Models. Among the deep learning-based models, DuAT[[15](#bib.bib15)],
    FeDNet[[80](#bib.bib80)], ESFPNet[[5](#bib.bib5)], Polyp-PVT[[13](#bib.bib13)],
    HSNet[[14](#bib.bib14)], and BDG-Net[[63](#bib.bib63)] obtain much better performance.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度模型比较。在基于深度学习的模型中，DuAT [[15](#bib.bib15)]、FeDNet [[80](#bib.bib80)]、ESFPNet
    [[5](#bib.bib5)]、Polyp-PVT [[13](#bib.bib13)]、HSNet [[14](#bib.bib14)] 和 BDG-Net
    [[63](#bib.bib63)] 表现远远优于其他模型。
- en: 'Moreover, Fig. [6](#S4.F6 "Figure 6 ‣ IV-B1 Overall Evaluation ‣ IV-B Performance
    Comparison and Analysis ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep
    Learning for Polyp Segmentation: Techniques, Challenges and Future Trends") shows
    the PR and F-measure curves for the 23 representative polyp segmentation models
    on five datasets (ETIS-LaribPolypDB [[87](#bib.bib87)], CVC-ColonDB [[16](#bib.bib16)],
    CVC-ClinicDB [[18](#bib.bib18)], CVC-300 [[25](#bib.bib25)], and Kvasir-SEG [[88](#bib.bib88)]).'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图 [6](#S4.F6 "图 6 ‣ IV-B1 总体评估 ‣ IV-B 性能比较与分析 ‣ IV 模型评估与分析 ‣ 关于息肉分割的深度学习调查：技术、挑战与未来趋势")
    显示了 23 种代表性息肉分割模型在五个数据集（ETIS-LaribPolypDB [[87](#bib.bib87)]、CVC-ColonDB [[16](#bib.bib16)]、CVC-ClinicDB
    [[18](#bib.bib18)]、CVC-300 [[25](#bib.bib25)] 和 Kvasir-SEG [[88](#bib.bib88)]）上的
    PR 和 F 测量曲线。
- en: To provide a deeper understanding of the better-performing models, we will discuss
    the main characteristics of the following six models in the sections below.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地了解表现更好的模型，我们将在以下各节中讨论以下六种模型的主要特征。
- en: '![Refer to caption](img/e97240082752d984805a6130eb17b92c.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e97240082752d984805a6130eb17b92c.png)'
- en: 'Figure 5: A comprehensive evaluation is conducted on 23 representative deep-learning
    models, including UNet  [[24](#bib.bib24)], UNet++  [[27](#bib.bib27)], SFA [[8](#bib.bib8)],
    PraNet [[9](#bib.bib9)], ACSNet [[12](#bib.bib12)], MSEG [[47](#bib.bib47)], EU-Net [[48](#bib.bib48)],
    SANet [[10](#bib.bib10)], MSNet [[11](#bib.bib11)], UACANet-S [[50](#bib.bib50)],
    UACANet-L [[50](#bib.bib50)], C2FNet [[51](#bib.bib51)], DCRNet [[66](#bib.bib66)],
    BDG-Net [[63](#bib.bib63)], CaraNet [[74](#bib.bib74)], EFA-Net [[98](#bib.bib98)],
    CFA-Net [[7](#bib.bib7)], M2SNet [[99](#bib.bib99)], Polyp-PVT [[13](#bib.bib13)],
    HSNet [[14](#bib.bib14)], DuAT [[15](#bib.bib15)], ESFPNet [[5](#bib.bib5)], and
    FeDNet [[80](#bib.bib80)]. We report the average Dice and MAE values for each
    model across five datasets (*i.e.*., ETIS-LaribPolypDB [[87](#bib.bib87)], CVC-ColonDB [[16](#bib.bib16)],
    CVC-ClinicDB [[18](#bib.bib18)], CVC-300 [[25](#bib.bib25)], and Kvasir-SEG [[88](#bib.bib88)]).
    Please note that the models represented in the top left corner are better (*i.e.*.,
    they have larger Dice scores and smaller MAE values). In this context, the green
    triangles represent Transformer-based models, while the red diamonds signify CNN-based
    models.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：对 23 种代表性深度学习模型进行综合评估，包括 UNet [[24](#bib.bib24)]、UNet++ [[27](#bib.bib27)]、SFA
    [[8](#bib.bib8)]、PraNet [[9](#bib.bib9)]、ACSNet [[12](#bib.bib12)]、MSEG [[47](#bib.bib47)]、EU-Net
    [[48](#bib.bib48)]、SANet [[10](#bib.bib10)]、MSNet [[11](#bib.bib11)]、UACANet-S
    [[50](#bib.bib50)]、UACANet-L [[50](#bib.bib50)]、C2FNet [[51](#bib.bib51)]、DCRNet
    [[66](#bib.bib66)]、BDG-Net [[63](#bib.bib63)]、CaraNet [[74](#bib.bib74)]、EFA-Net
    [[98](#bib.bib98)]、CFA-Net [[7](#bib.bib7)]、M2SNet [[99](#bib.bib99)]、Polyp-PVT
    [[13](#bib.bib13)]、HSNet [[14](#bib.bib14)]、DuAT [[15](#bib.bib15)]、ESFPNet [[5](#bib.bib5)]
    和 FeDNet [[80](#bib.bib80)]。我们报告了每个模型在五个数据集（*即*，ETIS-LaribPolypDB [[87](#bib.bib87)]、CVC-ColonDB
    [[16](#bib.bib16)]、CVC-ClinicDB [[18](#bib.bib18)]、CVC-300 [[25](#bib.bib25)]
    和 Kvasir-SEG [[88](#bib.bib88)])上的平均 Dice 和 MAE 值。请注意，位于左上角的模型表现更好（*即*，它们具有更大的
    Dice 分数和更小的 MAE 值）。在此上下文中，绿色三角形表示基于 Transformer 的模型，而红色菱形表示基于 CNN 的模型。
- en: '![Refer to caption](img/d2e42f720a34a558f31d9050e63edcdb.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d2e42f720a34a558f31d9050e63edcdb.png)'
- en: 'Figure 6: The PR curves and F-measures under different thresholds for 23 deep
    polyp segmentation models on five datasets in ETIS-LaribPolypDB [[87](#bib.bib87)],
    CVC-ColonDB [[16](#bib.bib16)], CVC-ClinicDB [[18](#bib.bib18)], CVC-300 [[25](#bib.bib25)],
    and Kvasir-SEG [[88](#bib.bib88)].'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：23 种深度息肉分割模型在 ETIS-LaribPolypDB [[87](#bib.bib87)]、CVC-ColonDB [[16](#bib.bib16)]、CVC-ClinicDB
    [[18](#bib.bib18)]、CVC-300 [[25](#bib.bib25)] 和 Kvasir-SEG [[88](#bib.bib88)]
    五个数据集上的 PR 曲线和 F 测量值在不同阈值下的表现。
- en: 'TABLE VI: Performance studies based on polyp size. Comparison results for 24
    representative polyp segmentation models (18 CNN-based models and 6 transformer-based
    models, in this context, SAM [[100](#bib.bib100)] refers to SAM-L) are provided
    in terms of MAE, mDice, and $S_{\alpha}$. The three best results are shown in
    red, blue, and green fonts.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：基于息肉大小的性能研究。提供了 24 种代表性息肉分割模型（18 种基于 CNN 的模型和 6 种基于 Transformer 的模型，此上下文中的
    SAM [[100](#bib.bib100)] 指的是 SAM-L）在 MAE、mDice 和 $S_{\alpha}$ 方面的比较结果。三种最佳结果以红色、蓝色和绿色字体显示。
- en: '|  |  | CNN-based models | Transformer-based models |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 基于 CNN 的模型 | 基于 Transformer 的模型 |'
- en: '|  Scale  |  UNet [[24](#bib.bib24)]  |  UNet++  [[27](#bib.bib27)]  |  SFA [[8](#bib.bib8)]  |  PraNet [[9](#bib.bib9)]  |  ACSNet [[12](#bib.bib12)]  |  MSEG [[47](#bib.bib47)]  |  EU-Net [[48](#bib.bib48)]  |  SANet [[10](#bib.bib10)]  |  MSNet [[11](#bib.bib11)]  |  UACANet-S [[50](#bib.bib50)]  |  UACANet-L [[50](#bib.bib50)]  |  C2FNet [[51](#bib.bib51)]  |  DCRNet [[66](#bib.bib66)]  |  BDG-Net [[63](#bib.bib63)]  |  CaraNet [[74](#bib.bib74)]  |  EFA-Net [[98](#bib.bib98)]  |  CFA-Net [[7](#bib.bib7)]  |  M2SNet [[99](#bib.bib99)]  |  Polyp-PVT [[13](#bib.bib13)]  |  HSNet [[14](#bib.bib14)]  |  DuAT [[15](#bib.bib15)]  |  ESFPNet [[5](#bib.bib5)]  |  FeDNet [[80](#bib.bib80)]  |  SAM [[100](#bib.bib100)]  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 规模 | UNet [[24](#bib.bib24)] | UNet++  [[27](#bib.bib27)] | SFA [[8](#bib.bib8)]
    | PraNet [[9](#bib.bib9)] | ACSNet [[12](#bib.bib12)] | MSEG [[47](#bib.bib47)]
    | EU-Net [[48](#bib.bib48)] | SANet [[10](#bib.bib10)] | MSNet [[11](#bib.bib11)]
    | UACANet-S [[50](#bib.bib50)] | UACANet-L [[50](#bib.bib50)] | C2FNet [[51](#bib.bib51)]
    | DCRNet [[66](#bib.bib66)] | BDG-Net [[63](#bib.bib63)] | CaraNet [[74](#bib.bib74)]
    | EFA-Net [[98](#bib.bib98)] | CFA-Net [[7](#bib.bib7)] | M2SNet [[99](#bib.bib99)]
    | Polyp-PVT [[13](#bib.bib13)] | HSNet [[14](#bib.bib14)] | DuAT [[15](#bib.bib15)]
    | ESFPNet [[5](#bib.bib5)] | FeDNet [[80](#bib.bib80)] | SAM [[100](#bib.bib100)]
    |'
- en: '| MAE | Small | .026 | .015 | .109 | .037 | .034 | .014 | .041 | .014 | .019
    | .020 | .012 | .026 | .068 | .011 | .022 | .017 | .011 | .021 | .017 | .023 |
    .012 | .014 | .012 | .020 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 小 | .026 | .015 | .109 | .037 | .034 | .014 | .041 | .014 | .019 |
    .020 | .012 | .026 | .068 | .011 | .022 | .017 | .011 | .021 | .017 | .023 | .012
    | .014 | .012 | .020 |'
- en: '| Medium | .040 | .038 | .062 | .021 | .030 | .016 | .035 | .018 | .019 | .016
    | .017 | .019 | .035 | .013 | .021 | .019 | .017 | .016 | .014 | .013 | .013 |
    .012 | .016 | .038 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 中等 | .040 | .038 | .062 | .021 | .030 | .016 | .035 | .018 | .019 | .016
    | .017 | .019 | .035 | .013 | .021 | .019 | .017 | .016 | .014 | .013 | .013 |
    .012 | .016 | .038 |'
- en: '| Large | .173 | .215 | .201 | .120 | .135 | .138 | .140 | .164 | .139 | .120
    | .142 | .150 | .152 | .114 | .106 | .112 | .138 | .122 | .101 | .112 | .094 |
    .107 | .099 | .199 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 大 | .173 | .215 | .201 | .120 | .135 | .138 | .140 | .164 | .139 | .120 |
    .142 | .150 | .152 | .114 | .106 | .112 | .138 | .122 | .101 | .112 | .094 | .107
    | .099 | .199 |'
- en: '| Overall | .051 | .052 | .095 | .039 | .044 | .031 | .050 | .035 | .034 |
    .030 | .031 | .038 | .061 | .025 | .032 | .030 | .030 | .031 | .026 | .029 | .023
    | .025 | .025 | .051 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | .051 | .052 | .095 | .039 | .044 | .031 | .050 | .035 | .034 | .030
    | .031 | .038 | .061 | .025 | .032 | .030 | .030 | .031 | .026 | .029 | .023 |
    .025 | .025 | .051 |'
- en: '| mDice | Small | .306 | .373 | .158 | .499 | .550 | .603 | .620 | .680 | .642
    | .624 | .687 | .569 | .541 | .674 | .667 | .669 | .650 | .634 | .705 | .734 |
    .743 | .727 | .766 | .501 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| mDice | 小 | .306 | .373 | .158 | .499 | .550 | .603 | .620 | .680 | .642
    | .624 | .687 | .569 | .541 | .674 | .667 | .669 | .650 | .634 | .705 | .734 |
    .743 | .727 | .766 | .501 |'
- en: '| Medium | .666 | .637 | .632 | .837 | .804 | .856 | .852 | .866 | .860 | .875
    | .857 | .859 | .806 | .887 | .870 | .860 | .855 | .872 | .890 | .898 | .896 |
    .900 | .884 | .563 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 中等 | .666 | .637 | .632 | .837 | .804 | .856 | .852 | .866 | .860 | .875
    | .857 | .859 | .806 | .887 | .870 | .860 | .855 | .872 | .890 | .898 | .896 |
    .900 | .884 | .563 |'
- en: '| Large | .590 | .462 | .589 | .763 | .717 | .677 | .711 | .617 | .692 | .734
    | .672 | .682 | .659 | .753 | .817 | .775 | .683 | .736 | .803 | .763 | .823 |
    .794 | .804 | .721 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 大 | .590 | .462 | .589 | .763 | .717 | .677 | .711 | .617 | .692 | .734 |
    .672 | .682 | .659 | .753 | .817 | .775 | .683 | .736 | .803 | .763 | .823 | .794
    | .804 | .721 |'
- en: '| Overall | .534 | .525 | .465 | .713 | .707 | .748 | .756 | .772 | .765 |
    .772 | .776 | .738 | .698 | .798 | .794 | .784 | .764 | .774 | .816 | .826 | .835
    | .828 | .834 | .561 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | .534 | .525 | .465 | .713 | .707 | .748 | .756 | .772 | .765 | .772
    | .776 | .738 | .698 | .798 | .794 | .784 | .764 | .774 | .816 | .826 | .835 |
    .828 | .834 | .561 |'
- en: '| $S_{\alpha}$ | Small | .641 | .679 | .493 | .732 | .756 | .786 | .778 | .826
    | .804 | .780 | .824 | .769 | .747 | .824 | .827 | .825 | .809 | .807 | .833 |
    .849 | .854 | .842 | .871 | .737 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| $S_{\alpha}$ | 小 | .641 | .679 | .493 | .732 | .756 | .786 | .778 | .826
    | .804 | .780 | .824 | .769 | .747 | .824 | .827 | .825 | .809 | .807 | .833 |
    .849 | .854 | .842 | .871 | .737 |'
- en: '| Medium | .800 | .784 | .736 | .893 | .876 | .904 | .885 | .902 | .907 | .907
    | .897 | .903 | .878 | .922 | .912 | .905 | .902 | .909 | .916 | .922 | .921 |
    .922 | .918 | .747 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 中等 | .800 | .784 | .736 | .893 | .876 | .904 | .885 | .902 | .907 | .907
    | .897 | .903 | .878 | .922 | .912 | .905 | .902 | .909 | .916 | .922 | .921 |
    .922 | .918 | .747 |'
- en: '| Large | .684 | .590 | .639 | .791 | .772 | .743 | .745 | .698 | .751 | .768
    | .735 | .741 | .731 | .790 | .824 | .801 | .745 | .768 | .806 | .782 | .823 |
    .796 | .813 | .696 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 大 | .684 | .590 | .639 | .791 | .772 | .743 | .745 | .698 | .751 | .768 |
    .735 | .741 | .731 | .790 | .824 | .801 | .745 | .768 | .806 | .782 | .823 | .796
    | .813 | .696 |'
- en: '| Overall | .731 | .724 | .642 | .826 | .822 | .844 | .831 | .851 | .853 |
    .846 | .852 | .837 | .815 | .873 | .872 | .865 | .851 | .857 | .874 | .880 | .886
    | .879 | .889 | .737 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | .731 | .724 | .642 | .826 | .822 | .844 | .831 | .851 | .853 | .846
    | .852 | .837 | .815 | .873 | .872 | .865 | .851 | .857 | .874 | .880 | .886 |
    .879 | .889 | .737 |'
- en: $\bullet$ DuAT [[15](#bib.bib15)]. Dual-Aggregation Transformer Network uses
    a transformer based on a pyramid structure as an encoder, and the decoder adopts
    a dual-stream design, building Local Spatial Attention modules and Global Spatial
    Attention modules to enhance the segmentation performance of polyps of different
    sizes.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ DuAT [[15](#bib.bib15)] 双聚合 Transformer 网络使用基于金字塔结构的 Transformer 作为编码器，解码器采用双流设计，构建局部空间注意力模块和全局空间注意力模块，以增强对不同大小息肉的分割性能。
- en: $\bullet$ FeDNet [[80](#bib.bib80)] decouples the input features into high-frequency
    edge features and low-frequency main body features through a feature decoupling
    operator, and then predicts by fusing the optimized features through a feature
    fusion operator.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ FeDNet [[80](#bib.bib80)] 通过特征解耦操作将输入特征解耦为高频边缘特征和低频主体特征，然后通过特征融合操作融合优化后的特征进行预测。
- en: $\bullet$ ESFPNet [[5](#bib.bib5)] utilizes a pre-trained Mixed Transformer
    encoder and an efficient Stage-wise Feature Pyramid decoder structure, which fuses
    features from deep to shallow layers and also performs linear fusion of features
    from global to local, and connects them with intermediate aggregated features
    to obtain the final segmentation result.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ESFPNet [[5](#bib.bib5)] 使用了一个预训练的混合 Transformer 编码器和一个高效的阶段特征金字塔解码器结构，该结构融合了从深层到浅层的特征，并且对从全局到局部的特征进行线性融合，将它们与中间聚合特征连接，以获得最终的分割结果。
- en: $\bullet$ Polyp-PVT [[13](#bib.bib13)] introduces a Pyramid Vision Transformer
    encoder to extract multi-scale features with long-range dependencies, utilizes
    high-level features for side output supervision, incorporates attention mechanisms
    to enhance low-level features and eliminate noise, and finally fuses multi-level
    features.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Polyp-PVT [[13](#bib.bib13)] 引入了一个金字塔视觉 Transformer 编码器来提取具有长距离依赖的多尺度特征，利用高层特征进行侧输出监督，结合注意力机制以增强低层特征并消除噪声，最后融合多层次特征。
- en: $\bullet$ HSNet [[14](#bib.bib14)] is also based on a PVT encoder and suppresses
    noise information in low-level features by modeling the semantic spatial relationships
    and channel dependencies of lower-layer features. It bridges feature disparities
    through a semantic interaction mechanism and captures long-range dependencies
    and local appearance details via a dual-branch structure.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ HSNet [[14](#bib.bib14)] 也是基于 PVT 编码器，通过建模低层特征的语义空间关系和通道依赖来抑制噪声信息。它通过语义交互机制弥合特征差异，并通过双分支结构捕捉长距离依赖和局部外观细节。
- en: $\bullet$ BDG-Net [[63](#bib.bib63)] aggregates high-level features to generate
    a boundary distribution map, which is fed into a boundary distribution-guided
    decoder and employs a multi-scale feature interaction strategy to enhance segmentation
    precision.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ BDG-Net [[63](#bib.bib63)] 聚合高层特征生成边界分布图，将其输入到一个边界分布引导解码器中，并采用多尺度特征交互策略以增强分割精度。
- en: IV-B2 Scale-based Evaluation
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 基于尺度的评估
- en: 'To investigate the influence of scale variations, we carry out evaluations
    on several representative polyp segmentation models. To achieve this evaluation,
    we compute the ratio ($r$) of the size of the polyp body area in a given ground
    truth image, which is used to characterize the size of the polyp. For this purpose,
    three types of polyp scales are defined: 1) when $r$ is less than $0.025$, the
    polyp is considered “small”; 2) when $r$ is more than $0.2$, it is considered
    “large”; 3) when the ratio is within the range $[0.025,0.2]$, we call it “medium”.
    In addition, we mix and classify the five colonoscopy datasets, deriving a newly
    constructed dataset containing $212$, $334$, and $77$ images of “small”, “medium”,
    and “large” types, respectively. Fig. [7](#S4.F7 "Figure 7 ‣ IV-B2 Scale-based
    Evaluation ‣ IV-B Performance Comparison and Analysis ‣ IV Model Evaluation and
    Analysis ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges
    and Future Trends") presents the comparison results associated with scale variations.
    Segmentation performance is represented by three metrics (*i.e.*, IoU, $F_{\beta}$,
    and $E_{\phi}$).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究尺度变化的影响，我们对几个代表性息肉分割模型进行评估。为实现该评估，我们计算给定真实图像中息肉主体区域的比例（$r$），该比例用于表征息肉的大小。为此，定义了三种息肉尺度：1）当
    $r$ 小于 $0.025$ 时，息肉被认为是“小型”；2）当 $r$ 大于 $0.2$ 时，息肉被认为是“大型”；3）当比例在 $[0.025,0.2]$
    范围内时，称之为“中型”。此外，我们混合并分类了五个结肠镜检查数据集，得到了一个新构建的数据集，其中包含 $212$、$334$ 和 $77$ 张分别属于“小型”、“中型”和“大型”的图像。图
    [7](#S4.F7 "Figure 7 ‣ IV-B2 Scale-based Evaluation ‣ IV-B Performance Comparison
    and Analysis ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep Learning for
    Polyp Segmentation: Techniques, Challenges and Future Trends") 展示了与尺度变化相关的比较结果。分割性能由三个指标表示（*即*，IoU、$F_{\beta}$
    和 $E_{\phi}$）。'
- en: '![Refer to caption](img/d1f518e59d7e97d45e71c235cec70acb.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1f518e59d7e97d45e71c235cec70acb.png)'
- en: 'Figure 7: Performance study based on the size of polyps (*i.e.*, small vs.
    medium vs. large). The comparison results on 23 representative polyp segmentation
    models (*i.e.*, ) are given in terms of mIoU (top), $F_{\beta}$ (medium), and
    $E_{\phi}$ (bottom).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于息肉大小的性能研究（*即*，小型 vs. 中型 vs. 大型）。23 个代表性息肉分割模型的比较结果（*即*）按 mIoU（顶部）、$F_{\beta}$（中部）和
    $E_{\phi}$（底部）给出。
- en: 'Some sample images with different scales of polyps are shown in Fig. [4](#S4.F4
    "Figure 4 ‣ IV-B1 Overall Evaluation ‣ IV-B Performance Comparison and Analysis
    ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep Learning for Polyp Segmentation:
    Techniques, Challenges and Future Trends"). Visual comparison results of the scale
    variation evaluation are shown in Tab. [VI](#S4.T6 "TABLE VI ‣ IV-B1 Overall Evaluation
    ‣ IV-B Performance Comparison and Analysis ‣ IV Model Evaluation and Analysis
    ‣ A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and
    Future Trends"). According to the results, we can draw the following conclusions:
    (1) In terms of overall performance, DuAT [[15](#bib.bib15)] and FeDNet [[80](#bib.bib80)]
    perform better across all types, and (2) Vertical analysis reveals that most models
    achieve better performance in segmenting “medium” polyps while they exhibit relatively
    lower performance in segmenting other types.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#S4.F4 "Figure 4 ‣ IV-B1 Overall Evaluation ‣ IV-B Performance Comparison
    and Analysis ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep Learning for
    Polyp Segmentation: Techniques, Challenges and Future Trends") 显示了不同规模息肉的样本图像。尺度变化评估的视觉比较结果见表
    [VI](#S4.T6 "TABLE VI ‣ IV-B1 Overall Evaluation ‣ IV-B Performance Comparison
    and Analysis ‣ IV Model Evaluation and Analysis ‣ A Survey on Deep Learning for
    Polyp Segmentation: Techniques, Challenges and Future Trends")。根据结果，我们可以得出以下结论：（1）在整体性能方面，DuAT
    [[15](#bib.bib15)] 和 FeDNet [[80](#bib.bib80)] 在所有类型中表现更好，（2）垂直分析揭示大多数模型在分割“中等”大小的息肉时表现更佳，而在分割其他类型的息肉时表现相对较差。'
- en: V Challenges and Future Trends
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 挑战与未来趋势
- en: V-A Effective Network Structure
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 有效的网络结构
- en: Edge Feature Extraction. The diverse shapes of polyps and the complex background
    of endoscopic images, combined with blurred edges and adherences of polyps often
    pose challenges for accurate segmentation. To further improve accuracy and robustness,
    and to leverage richer and more complex image features, some models introduce
    edge feature extraction modules ([[65](#bib.bib65), [8](#bib.bib8), [73](#bib.bib73),
    [71](#bib.bib71)]) to utilize low-level features to assist in polyp segmentation.
    Most polyp segmentation models capture the edge information by utilizing low-level
    features extracted by backbone networks, which are then aggregated with high-level
    features to accomplish segmentation. However, after operations like convolution,
    the extracted image features, including edge and body features, become intertwined,
    making decoupling extremely challenging. Additionally, as the tasks of feature
    extraction and segmentation are not orthogonal, incorrect edge estimation can
    lead to error propagation. Therefore, an optional choice is to decouple the original
    images using traditional image processing methods. There has already been some
    work in the fields of object detection and other semantic segmentation domains.
    Shan et al. [[101](#bib.bib101)] utilized Fourier Transform to derive high and
    low-frequency components from images. These components are then input into two
    parallel branches to obtain edge and body features, which are subsequently merged
    to perform semantic segmentation. Cong et al. [[102](#bib.bib102)] designed a
    full-frequency perception module based on Octave Convolution. This module can
    automatically learn low and high-frequency features for coarse positioning, providing
    auxiliary information for segmentation. FeDNet  [[80](#bib.bib80)] employs Laplacian
    pyramid decomposition to decouple the input features into high-frequency edge
    features and low-frequency body features. Following this, these two types of features
    undergo deep supervised optimization.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘特征提取。息肉的多样形状和内镜图像的复杂背景，加上息肉的模糊边缘和附着，常常对准确分割构成挑战。为了进一步提高准确性和鲁棒性，并利用更丰富和复杂的图像特征，一些模型引入了边缘特征提取模块（[[65](#bib.bib65),
    [8](#bib.bib8), [73](#bib.bib73), [71](#bib.bib71)]），以利用低级特征来辅助息肉分割。大多数息肉分割模型通过利用骨干网络提取的低级特征来捕捉边缘信息，然后将其与高级特征聚合以完成分割。然而，经过卷积等操作后，提取的图像特征，包括边缘和主体特征，变得交织在一起，使得解耦变得极为困难。此外，由于特征提取和分割的任务并不正交，错误的边缘估计可能导致错误传播。因此，一个可选的选择是使用传统图像处理方法解耦原始图像。在物体检测和其他语义分割领域已有一些相关工作。Shan
    等人[[101](#bib.bib101)]利用傅里叶变换从图像中提取高频和低频分量。这些分量随后输入到两个并行分支中以获取边缘和主体特征，随后将其合并进行语义分割。Cong
    等人[[102](#bib.bib102)]设计了一个基于Octave卷积的全频感知模块。该模块可以自动学习低频和高频特征以进行粗略定位，为分割提供辅助信息。FeDNet
    [[80](#bib.bib80)]采用拉普拉斯金字塔分解将输入特征解耦为高频边缘特征和低频主体特征。随后，这两种特征进行深度监督优化。
- en: Dual-stream Structure. CNNs exhibit a powerful capability to extract local features,
    such as texture and shape. However, CNNs are relatively weaker in capturing long-distance
    global dependencies. In this context, the Transformer can serve as a supplement.
    Due to its self-attention mechanism, it possesses an excellent ability to capture
    global information. Therefore, introducing a dual-stream structure in existing
    polyp segmentation models can integrate local information captured by CNNs and
    global information captured by the Transformer, enhancing the performance of polyp
    segmentation. In addition, this structure may improve the model’s capabilities
    to handle complex, uneven backgrounds and noise interference, thereby increasing
    the robustness of the model [[14](#bib.bib14), [53](#bib.bib53), [68](#bib.bib68),
    [67](#bib.bib67), [6](#bib.bib6)]. It is important to note that how to effectively
    merge the outputs from the two networks and balance their weights remains a problem.
    However, the tremendous potential and possibilities undoubtedly justify further
    exploration and research.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 双流结构。CNN 展现了强大的局部特征提取能力，如纹理和形状。然而，CNN 在捕捉远距离全局依赖性方面相对较弱。在这种情况下，Transformer 可以作为补充。由于其自注意力机制，Transformer
    具有出色的全局信息捕捉能力。因此，在现有的息肉分割模型中引入双流结构可以将 CNN 捕捉到的局部信息和 Transformer 捕捉到的全局信息整合起来，从而提高息肉分割的性能。此外，这种结构还可能改善模型处理复杂、不均匀背景和噪声干扰的能力，从而提高模型的鲁棒性[[14](#bib.bib14),
    [53](#bib.bib53), [68](#bib.bib68), [67](#bib.bib67), [6](#bib.bib6)]。值得注意的是，如何有效地合并来自两个网络的输出并平衡它们的权重仍然是一个问题。然而，这些巨大的潜力和可能性无疑值得进一步探索和研究。
- en: V-B Different Supervision Strategies
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 不同的监督策略
- en: Existing polyp segmentation models typically use fully-supervised strategies
    to learn features and produce segmentation results. However, annotating colonoscopy
    data is time-consuming and labor-intensive, especially for video data. To alleviate
    this issue, attention has been shifted towards weakly-supervised and semi-supervised
    learning, applying them to polyp segmentation tasks [[15](#bib.bib15), [103](#bib.bib103)].
    Actually, semi-supervised or weakly supervised methods have been widely applied
    in the field of medical image segmentation. For example, [[104](#bib.bib104)]
    proposed a novel data augmentation approach for medical image segmentation that
    does not lose essential semantic information of the key objects. [[105](#bib.bib105)]
    redefined the traditional per-pixel segmentation task as a contour regression
    problem and modeled the position uncertainty. [[106](#bib.bib106)] introduced
    a semi-supervised medical image segmentation technique that first trains the segmentation
    model on a small number of unlabeled images, generates initial labels for them,
    and also introduces a consistency-based pseudo-label enhancement scheme to improve
    the quality of the model’s predictions. Therefore, in the future, semi/weakly
    supervised methods can be used for image-level labeling and pseudo-annotation
    to improve the accuracy of polyp segmentation.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的息肉分割模型通常使用完全监督的方法来学习特征并生成分割结果。然而，标注结肠镜数据既耗时又费力，尤其是视频数据。为了解决这个问题，研究重点已经转向弱监督和半监督学习，并将其应用于息肉分割任务[[15](#bib.bib15),
    [103](#bib.bib103)]。实际上，半监督或弱监督方法已经在医学图像分割领域得到了广泛应用。例如，[[104](#bib.bib104)] 提出了一个新颖的数据增强方法，用于医学图像分割，该方法不会丢失关键对象的基本语义信息。[[105](#bib.bib105)]
    将传统的逐像素分割任务重新定义为轮廓回归问题，并建模了位置不确定性。[[106](#bib.bib106)] 引入了一种半监督医学图像分割技术，该技术首先在少量未标记图像上训练分割模型，生成初始标签，并引入了一种基于一致性的伪标签增强方案，以提高模型预测的质量。因此，未来可以使用半监督/弱监督方法进行图像级标注和伪标注，以提高息肉分割的准确性。
- en: V-C Clinical Requirements
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 临床需求
- en: Dataset Collection. The shape, texture, and color of polyps can vary depending
    on the time and stage of the disease. Colonoscopy data from multiple centers also
    often exhibit different morphologies. Existing polyp segmentation datasets mostly
    consist of images containing a single polyp, and there are not many specialized
    datasets with a large number of images. Only $23$ out of the 623 images in the
    aforementioned five datasets (ETIS-LaribPolypDB [[87](#bib.bib87)], CVC-ColonDB [[16](#bib.bib16)],
    CVC-ClinicDB [[18](#bib.bib18)], CVC-300 [[25](#bib.bib25)], and Kvasir-SEG [[88](#bib.bib88)])
    contain multiple polyps, as such, models trained on these datasets often perform
    poorly on tasks involving multiple polyps. Although there are multiple public
    datasets available for polyp segmentation, their scale is quite limited. For instance,
    the largest dataset we present is PolypGen [[92](#bib.bib92)], which contains
    $3,762$ images. Most of the data in these datasets comes from colonoscopy images
    and video slices. Therefore, it is necessary to develop a new large-scale polyp
    segmentation dataset to serve as a baseline for future research. In addition to
    this, collecting datasets for complex specialized scenarios is also a potential
    direction. For example, constructing multi-center datasets, multi-target datasets,
    and specialized datasets for small or large polyps could enhance the model’s performance
    in different scenarios.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集收集。息肉的形状、纹理和颜色可能会根据疾病的时间和阶段有所不同。来自多个中心的结肠镜数据也常常显示出不同的形态。现有的息肉分割数据集大多包含单个息肉的图像，专门的大量图像数据集则不多。在前述五个数据集（ETIS-LaribPolypDB [[87](#bib.bib87)]，CVC-ColonDB [[16](#bib.bib16)]，CVC-ClinicDB [[18](#bib.bib18)]，CVC-300 [[25](#bib.bib25)]，和Kvasir-SEG [[88](#bib.bib88)]）中的623张图像中，只有$23$张包含多个息肉，因此，基于这些数据集训练的模型在处理多个息肉的任务时往往表现不佳。虽然有多个公共数据集用于息肉分割，但其规模相当有限。例如，我们展示的最大数据集是PolypGen [[92](#bib.bib92)]，它包含$3,762$张图像。这些数据集中的大多数数据来源于结肠镜图像和视频切片。因此，有必要开发一个新的大规模息肉分割数据集，作为未来研究的基准。此外，收集复杂专用场景的数据集也是一个潜在的方向。例如，构建多中心数据集、多目标数据集以及针对小型或大型息肉的专用数据集可能会提升模型在不同场景下的表现。
- en: Cross-domain Segmentation. Deep learning-based polyp segmentation methods have
    achieved promising performance, but they often suffer from performance degradation
    when applied to unseen target domain datasets collected from different imaging
    devices. Thus, it is still challenging to apply existing polyp segmentation methods
    to unseen datasets. More importantly, manual annotation for new target datasets
    is tedious and labor-intensive, leveraging knowledge learned from the labeled
    source domains to boost the performance in the unlabeled target domain is highly
    demanded in clinical practice. To achieve this, Yang et al. [[34](#bib.bib34)]
    proposed a mutual-prototype adaptation network for cross-domain polyp segmentation,
    which significantly reduces the gap between the two domains to improve the segmentation
    performance on the target domain datasets. Thus, this direction deserves further
    exploration to develop more cross-domain segmentation models in this task.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 跨域分割。基于深度学习的息肉分割方法已取得了良好的表现，但当应用于来自不同成像设备的未见目标域数据集时，往往会出现性能下降。因此，将现有的息肉分割方法应用于未见数据集仍然具有挑战性。更重要的是，手动注释新目标数据集既繁琐又劳动强度大，利用从标注源域中学到的知识来提升在未标注目标域中的表现在临床实践中需求很大。为此，Yang等人 [[34](#bib.bib34)]
    提出了一个用于跨域息肉分割的互原型适应网络，这显著缩小了两个领域之间的差距，从而提高了目标域数据集上的分割性能。因此，这一方向值得进一步探索，以开发更多跨域分割模型。
- en: Real-Time Polyp Segmentation. It is worth noting that real-time segmentation
    is important for this task, as it is anticipated that the segmented results can
    be immediately presented to the doctor during the colonoscopy procedures for further
    decisions and treatments. However, the current deep learning-based models often
    require huge computation complexity, making them challenging to apply for real-time
    segmentation. Several real-time polyp segmentation models have been developed [[107](#bib.bib107),
    [108](#bib.bib108), [44](#bib.bib44), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [86](#bib.bib86)]. Indeed, the development
    of efficient lightweight networks for polyp segmentation without sacrificing performance
    is of utmost importance. However, it poses a significant challenge due to the
    inherent trade-off between model complexity and efficiency. Efficient lightweight
    networks can enable real-time segmentation, reduce computational costs, and facilitate
    deployment in resource-constrained clinical settings. Thus, overcoming this challenge
    involves finding innovative solutions that strike a balance between model complexity
    and performance, ultimately enhancing the practical applicability of polyp segmentation
    algorithms in clinical practice.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 实时息肉分割。值得注意的是，对于这个任务而言，实时分割至关重要，因为预计分割结果可以在结肠镜检查过程中立即呈现给医生，以便做出进一步的决策和治疗。然而，目前基于深度学习的模型通常需要巨大的计算复杂性，使得实时分割应用面临挑战。已经开发了若干种实时息肉分割模型[[107](#bib.bib107),
    [108](#bib.bib108), [44](#bib.bib44), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [86](#bib.bib86)]。实际上，开发高效轻量级网络进行息肉分割而不牺牲性能是**极为重要**的。然而，由于模型复杂性与效率之间的固有权衡，这一任务面临重大挑战。高效轻量级网络可以实现实时分割，减少计算成本，并促进在资源受限的临床环境中的应用。因此，克服这一挑战涉及到寻找创新的解决方案，**平衡模型复杂性和性能**，*最终提升息肉分割算法在临床实践中的实际适用性*。
- en: V-D Ethical Issues
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 伦理问题
- en: The specificity of medical issues often raises privacy concerns when using real
    patient examination data from hospitals. Moreover, there are inherent differences
    between data obtained from different centers. Models trained on data from a single
    center tend to perform worse when applied to unseen data acquired from different
    scanners or other centers. Thus, it becomes crucial to leverage the knowledge
    gained from labeled source domains to enhance the performance in unlabeled target
    domains. The goal is to mitigate the domain shift observed in colonoscopy images
    sourced from multiple centers and devices. Federated learning emerges as a promising
    approach in this context, enabling multiple centers to collaboratively learn a
    shared prediction model while ensuring privacy protection. For instance, Liu et
    al. [[113](#bib.bib113)] presented a novel method of incidental learning in continuous
    frequency space, enabling diverse endpoints to utilize multi-source data distributions
    while addressing challenging constraints associated with data dispersion.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗问题的特异性通常在使用医院的真实患者检查数据时引发隐私问题。此外，不同中心获得的数据之间存在固有差异。基于单一中心数据训练的模型在应用于从不同扫描仪或其他中心获得的未见数据时表现较差。因此，利用从标记源领域获得的知识来提升在未标记目标领域的性能变得至关重要。目标是缓解从多个中心和设备获取的结肠镜图像中观察到的领域转移。在这种背景下，联邦学习成为一种**有前景**的方法，它允许多个中心协同学习一个共享的预测模型，同时确保隐私保护。例如，刘等人[[113](#bib.bib113)]提出了一种在连续频域中进行意外学习的新方法，使得多样化的终端能够利用多源数据分布，同时解决与数据分散相关的挑战性约束。
- en: VI Conclusion
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: 'In this paper, to the best of our knowledge, we provide the first comprehensive
    review of the development and evaluation in the field of polyp segmentation. We
    categorize models into traditional and deep ones initially, then focus on reviewing
    existing deep models from various perspectives, followed by a summary of popular
    polyp segmentation datasets and providing detailed information for each dataset.
    Following that, we conduct a comprehensive assessment as well as an evaluation
    based on polyp sizes for 24 representative deep learning-based polyp segmentation
    models. Specifically, we perform a size-based performance analysis by constructing
    a new dataset for 24 representative polyp segmentation models. Furthermore, we
    discuss some challenges and highlight open directions for future research. Although
    significant progress has been made in the field of polyp segmentation over the
    past few decades, there is still ample room for improvement. We hope this survey
    will spark more interest and understanding in this field. To promote future research
    for polyp segmentation, we will continue to collect newly released polyp segmentation
    models at: [https://github.com/taozh2017/Awesome-Polyp-Segmentation](https://github.com/taozh2017/Awesome-Polyp-Segmentation).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，尽我们所知，我们提供了息肉分割领域发展的首次全面回顾。我们最初将模型分类为传统模型和深度模型，然后从各个角度重点回顾现有深度模型，接着总结了流行的息肉分割数据集并提供了每个数据集的详细信息。随后，我们对
    24 个具有代表性的基于深度学习的息肉分割模型进行了全面评估，并根据息肉大小进行了评估。具体而言，我们通过构建一个新的数据集，对 24 个具有代表性的息肉分割模型进行基于大小的性能分析。此外，我们讨论了一些挑战，并强调了未来研究的开放方向。尽管在过去几十年中，息肉分割领域取得了显著进展，但仍有很大的改进空间。我们希望这项调查能够激发对这一领域的更多兴趣和理解。为了促进息肉分割的未来研究，我们将继续收集新发布的息肉分割模型，网址为：[https://github.com/taozh2017/Awesome-Polyp-Segmentation](https://github.com/taozh2017/Awesome-Polyp-Segmentation)。
- en: References
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] X. Guo, C. Yang, Y. Liu, and Y. Yuan, “Learn to threshold: Thresholdnet
    with confidence-guided manifold mixup for polyp segmentation,” *IEEE TMI*, vol. 40,
    no. 4, pp. 1134–1146, 2020.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] X. Guo, C. Yang, Y. Liu, 和 Y. Yuan, “学会阈值：具有置信度引导流形混合的 Thresholdnet 用于息肉分割，”
    *IEEE TMI*，卷 40，第 4 期，第 1134–1146 页，2020年。'
- en: '[2] X. Yang, Q. Wei, C. Zhang, K. Zhou, L. Kong, and W. Jiang, “Colon polyp
    detection and segmentation based on improved mrcnn,” *IEEE TIM*, vol. 70, pp.
    1–10, 2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] X. Yang, Q. Wei, C. Zhang, K. Zhou, L. Kong, 和 W. Jiang, “基于改进 MRCNN 的结肠息肉检测与分割，”
    *IEEE TIM*，卷 70，第 1–10 页，2020年。'
- en: '[3] D.-P. Fan, G.-P. Ji, M.-M. Cheng, and L. Shao, “Concealed object detection,”
    *IEEE TPAMI*, vol. 44, no. 10, pp. 6024–6042, 2021.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D.-P. Fan, G.-P. Ji, M.-M. Cheng, 和 L. Shao, “隐蔽物体检测，” *IEEE TPAMI*，卷 44，第
    10 期，第 6024–6042 页，2021年。'
- en: '[4] D. Jha, N. K. Tomar, V. Sharma, and U. Bagci, “Transnetr: Transformer-based
    residual network for polyp segmentation with multi-center out-of-distribution
    testing,” *arXiv preprint arXiv:2303.07428*, 2023.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. Jha, N. K. Tomar, V. Sharma, 和 U. Bagci, “Transnetr: 基于 transformer
    的残差网络，用于具有多中心的分布外测试的息肉分割，” *arXiv 预印本 arXiv:2303.07428*，2023年。'
- en: '[5] Q. Chang, D. Ahmad, J. Toth, R. Bascom, and W. E. Higgins, “Esfpnet: efficient
    deep learning architecture for real-time lesion segmentation in autofluorescence
    bronchoscopic video,” in *Medical Imaging: Medical Imaging: Biomedical Applications
    in Molecular, Structural, and Functional Imaging*, vol. 12468.   SPIE, 2023, p.
    1246803.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Q. Chang, D. Ahmad, J. Toth, R. Bascom, 和 W. E. Higgins, “Esfpnet: 高效深度学习架构用于自动荧光支气管镜视频中的实时病变分割，”
    见于 *Medical Imaging: Medical Imaging: Biomedical Applications in Molecular, Structural,
    and Functional Imaging*，卷 12468。 SPIE，2023年，第 1246803 页。'
- en: '[6] Y. Wang, Z. Deng, Q. Lou, S. Hu, K.-s. Choi, and S. Wang, “Cooperation
    learning enhanced colonic polyp segmentation based on transformer-cnn fusion,”
    *arXiv preprint arXiv:2301.06892*, 2023.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Y. Wang, Z. Deng, Q. Lou, S. Hu, K.-s. Choi, 和 S. Wang, “基于 transformer-cnn
    融合的合作学习增强结肠息肉分割，” *arXiv 预印本 arXiv:2301.06892*，2023年。'
- en: '[7] T. Zhou, Y. Zhou, K. He, C. Gong, J. Yang, H. Fu, and D. Shen, “Cross-level
    feature aggregation network for polyp segmentation,” *PR*, vol. 140, p. 109555,
    2023.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. Zhou, Y. Zhou, K. He, C. Gong, J. Yang, H. Fu, 和 D. Shen, “用于息肉分割的跨层特征聚合网络，”
    *PR*，卷 140，第 109555 页，2023年。'
- en: '[8] Y. Fang, C. Chen, Y. Yuan, and K.-y. Tong, “Selective feature aggregation
    network with area-boundary constraints for polyp segmentation,” in *MICCAI*.   Springer,
    2019, pp. 302–310.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Fang, C. Chen, Y. Yuan, 和 K.-y. Tong, “具有区域-边界约束的选择性特征聚合网络用于息肉分割，” 见于
    *MICCAI*。 Springer，2019年，第 302–310 页。'
- en: '[9] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, “Pranet:
    Parallel reverse attention network for polyp segmentation,” in *MICCAI*.   Springer,
    2020, pp. 263–273.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, 和 L. Shao，“Pranet：用于息肉分割的并行反向注意网络”，见于*MICCAI*。Springer,
    2020, pp. 263–273。'
- en: '[10] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, “Shallow attention
    network for polyp segmentation,” in *MICCAI*.   Springer, 2021, pp. 699–708.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, 和 S. Cui，“用于息肉分割的浅层注意网络”，见于*MICCAI*。Springer,
    2021, pp. 699–708。'
- en: '[11] X. Zhao, L. Zhang, and H. Lu, “Automatic polyp segmentation via multi-scale
    subtraction network,” in *MICCAI*.   Springer, 2021, pp. 120–130.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] X. Zhao, L. Zhang, 和 H. Lu，“通过多尺度减法网络进行自动息肉分割”，见于*MICCAI*。Springer, 2021,
    pp. 120–130。'
- en: '[12] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, and Y. Yu, “Adaptive context
    selection for polyp segmentation,” in *MICCAI*.   Springer, 2020, pp. 253–262.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, 和 Y. Yu，“用于息肉分割的自适应上下文选择”，见于*MICCAI*。Springer,
    2020, pp. 253–262。'
- en: '[13] B. Dong, W. Wang, D.-P. Fan, J. Li, H. Fu, and L. Shao, “Polyp-pvt: Polyp
    segmentation with pyramid vision transformers,” *CAAI AIR*, 2023.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] B. Dong, W. Wang, D.-P. Fan, J. Li, H. Fu, 和 L. Shao，“Polyp-PVT：使用金字塔视觉变换器进行息肉分割”，*CAAI
    AIR*，2023。'
- en: '[14] W. Zhang, C. Fu, Y. Zheng, F. Zhang, Y. Zhao, and C.-W. Sham, “Hsnet:
    A hybrid semantic network for polyp segmentation,” *Computers in biology and medicine*,
    vol. 150, p. 106173, 2022.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] W. Zhang, C. Fu, Y. Zheng, F. Zhang, Y. Zhao, 和 C.-W. Sham，“Hsnet：用于息肉分割的混合语义网络”，*生物医学计算机与医学*，第
    150 卷，p. 106173，2022。'
- en: '[15] F. Tang, Z. Xu, Q. Huang, J. Wang, X. Hou, J. Su, and J. Liu, “Duat: Dual-aggregation
    transformer network for medical image segmentation,” in *Chinese Conference on
    Pattern Recognition and Computer Vision (PRCV)*, 2023, pp. 343–356.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] F. Tang, Z. Xu, Q. Huang, J. Wang, X. Hou, J. Su, 和 J. Liu，“DUAT：用于医学图像分割的双聚合变换网络”，见于*中国模式识别与计算机视觉会议
    (PRCV)*，2023，pp. 343–356。'
- en: '[16] N. Tajbakhsh, S. R. Gurudu, and J. Liang, “Automated polyp detection in
    colonoscopy videos using shape and context information,” *IEEE TMI*, vol. 35,
    no. 2, pp. 630–644, 2015.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] N. Tajbakhsh, S. R. Gurudu, 和 J. Liang，“基于形状和上下文信息的结肠镜视频自动息肉检测”，*IEEE
    TMI*，第 35 卷，第 2 期，pp. 630–644，2015。'
- en: '[17] Y. Iwahori, H. Hagi, H. Usami, R. J. Woodham, A. Wang, M. K. Bhuyan, and
    K. Kasugai, “Automatic polyp detection from endoscope image using likelihood map
    based on edge information,” in *ICPRAM*, vol. 2.   SciTePress, 2017, pp. 402–409.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Iwahori, H. Hagi, H. Usami, R. J. Woodham, A. Wang, M. K. Bhuyan, 和
    K. Kasugai，“基于边缘信息的可能性图的内窥镜图像自动息肉检测”，见于*ICPRAM*，第 2 卷。SciTePress, 2017, pp. 402–409。'
- en: '[18] J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, D. Gil, C. Rodríguez,
    and F. Vilariño, “Wm-dova maps for accurate polyp highlighting in colonoscopy:
    Validation vs. saliency maps from physicians,” *CMIG*, vol. 43, pp. 99–111, 2015.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, D. Gil, C. Rodríguez,
    和 F. Vilariño，“用于准确突出结肠镜息肉的 WM-DOVA 图：与医生的显著性图的验证”，*CMIG*，第 43 卷，pp. 99–111，2015。'
- en: '[19] J. Yao, M. Miller, M. Franaszek, and R. M. Summers, “Colonic polyp segmentation
    in ct colonography-based on fuzzy clustering and deformable models,” *IEEE TMI*,
    vol. 23, no. 11, pp. 1344–1352, 2004.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Yao, M. Miller, M. Franaszek, 和 R. M. Summers，“基于模糊聚类和变形模型的 CT 结肠造影中的结肠息肉分割”，*IEEE
    TMI*，第 23 卷，第 11 期，pp. 1344–1352，2004。'
- en: '[20] L. Lu, A. Barbu, M. Wolf, J. Liang, M. Salganicoff, and D. Comaniciu,
    “Accurate polyp segmentation for 3d ct colongraphy using multi-staged probabilistic
    binary learning and compositional model,” in *IEEE CVPR*, 2008, pp. 1–8.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] L. Lu, A. Barbu, M. Wolf, J. Liang, M. Salganicoff, 和 D. Comaniciu，“使用多阶段概率二值学习和组合模型进行
    3D CT 结肠造影的准确息肉分割”，见于*IEEE CVPR*，2008, pp. 1–8。'
- en: '[21] S. Gross, M. Kennel, T. Stehle, J. Wulff, J. Tischendorf, C. Trautwein,
    and T. Aach, “Polyp segmentation in nbi colonoscopy,” in *Bildverarbeitung für
    die Medizin*.   Springer, 2009, pp. 252–256.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Gross, M. Kennel, T. Stehle, J. Wulff, J. Tischendorf, C. Trautwein,
    和 T. Aach，“NBI 胶囊镜中的息肉分割”，见于*医学图像处理*。Springer, 2009, pp. 252–256。'
- en: '[22] M. Ganz, X. Yang, and G. Slabaugh, “Automatic segmentation of polyps in
    colonoscopic narrow-band imaging data,” *IEEE TBE*, vol. 59, no. 8, pp. 2144–2151,
    2012.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Ganz, X. Yang, 和 G. Slabaugh，“结肠镜窄带成像数据中息肉的自动分割”，*IEEE TBE*，第 59 卷，第
    8 期，pp. 2144–2151，2012。'
- en: '[23] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *IEEE CVPR*, 2015, pp. 3431–3440.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Long, E. Shelhamer, 和 T. Darrell，“用于语义分割的全卷积网络”，见于*IEEE CVPR*，2015,
    pp. 3431–3440。'
- en: '[24] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *MICCAI*.   Springer, 2015, pp. 234–241.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] O. Ronneberger, P. Fischer, 和 T. Brox，“U-Net：用于生物医学图像分割的卷积网络”，见于*MICCAI*。Springer,
    2015, pp. 234–241。'
- en: '[25] D. Vázquez, J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, A. M. López,
    A. Romero, M. Drozdzal, A. Courville *et al.*, “A benchmark for endoluminal scene
    segmentation of colonoscopy images,” *JHE*, vol. 2017, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] D. Vázquez, J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, A. M. López,
    A. Romero, M. Drozdzal, A. Courville *等*，“结肠镜图像的内腔场景分割基准，” *JHE*，第 2017 卷，2017
    年。'
- en: '[26] M. Akbari, M. Mohrekesh, E. Nasr-Esfahani, S. R. Soroushmehr, N. Karimi,
    S. Samavi, and K. Najarian, “Polyp segmentation in colonoscopy images using fully
    convolutional network,” in *Proceedings of the IEEE International Conference on
    Medicine and Biology Society*, 2018, pp. 69–72.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Akbari, M. Mohrekesh, E. Nasr-Esfahani, S. R. Soroushmehr, N. Karimi,
    S. Samavi, 和 K. Najarian，“使用全卷积网络在结肠镜图像中进行息肉分割，” 见 *IEEE 国际医学与生物学学会会议论文集*，2018
    年，第 69–72 页。'
- en: '[27] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:
    A nested u-net architecture for medical image segmentation,” in *Deep Learning
    in Medical Image Analysis and Multimodal Learning for Clinical Decision Support*.   Springer,
    2018, pp. 3–11.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, 和 J. Liang，“Unet++：用于医学图像分割的嵌套
    U-Net 架构，” 见 *医学图像分析中的深度学习和临床决策支持的多模态学习*。  Springer，2018 年，第 3–11 页。'
- en: '[28] A. Srivastava, D. Jha, S. Chanda, U. Pal, H. D. Johansen, D. Johansen,
    M. A. Riegler, S. Ali, and P. Halvorsen, “Msrf-net: A multi-scale residual fusion
    network for biomedical image segmentation,” *IEEE JBHI*, vol. 26, no. 5, pp. 2252–2263,
    2022.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Srivastava, D. Jha, S. Chanda, U. Pal, H. D. Johansen, D. Johansen,
    M. A. Riegler, S. Ali, 和 P. Halvorsen，“Msrf-net：一种用于生物医学图像分割的多尺度残差融合网络，” *IEEE
    JBHI*，第 26 卷，第 5 期，第 2252–2263 页，2022 年。'
- en: '[29] N. K. Tomar, D. Jha, U. Bagci, and S. Ali, “Tganet: Text-guided attention
    for improved polyp segmentation,” in *MICCAI*.   Springer, 2022, pp. 151–160.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] N. K. Tomar, D. Jha, U. Bagci, 和 S. Ali，“Tganet：用于改进息肉分割的文本引导注意力，” 见 *MICCAI*。  Springer，2022
    年，第 151–160 页。'
- en: '[30] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, and S. Song, “Stepwise feature
    fusion: Local guides global,” in *MICCAI*.   Springer, 2022, pp. 110–120.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, 和 S. Song，“逐步特征融合：局部引导全局，”
    见 *MICCAI*。  Springer，2022 年，第 110–120 页。'
- en: '[31] M. M. Rahman and R. Marculescu, “Medical image segmentation via cascaded
    attention decoding,” in *IEEE/CVF WACVW*, 2023, pp. 6222–6231.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. M. Rahman 和 R. Marculescu，“通过级联注意力解码进行医学图像分割，” 见 *IEEE/CVF WACVW*，2023
    年，第 6222–6231 页。'
- en: '[32] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo *et al.*, “Segment anything,” *arXiv preprint
    arXiv:2304.02643*, 2023.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T.
    Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo *等*，“Segment anything，” *arXiv 预印本 arXiv:2304.02643*，2023
    年。'
- en: '[33] Y. Li, M. Hu, and X. Yang, “Polyp-sam: Transfer sam for polyp segmentation,”
    *arXiv preprint arXiv:2305.00293*, 2023.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Li, M. Hu, 和 X. Yang，“Polyp-sam：用于息肉分割的转移 sam，” *arXiv 预印本 arXiv:2305.00293*，2023
    年。'
- en: '[34] C. Yang, X. Guo, M. Zhu, B. Ibragimov, and Y. Yuan, “Mutual-prototype
    adaptation for cross-domain polyp segmentation,” *IEEE JBHI*, vol. 25, no. 10,
    pp. 3886–3897, 2021.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C. Yang, X. Guo, M. Zhu, B. Ibragimov, 和 Y. Yuan，“跨领域息肉分割的互原型适配，” *IEEE
    JBHI*，第 25 卷，第 10 期，第 3886–3897 页，2021 年。'
- en: '[35] S. Gupta, G. Sikka, and A. Malik, “A review on deep learning-based polyp
    segmentation for efficient colorectal cancer screening,” in *IEEE ICSCCC*, 2023,
    pp. 501–506.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] S. Gupta, G. Sikka, 和 A. Malik，“基于深度学习的息肉分割综述：高效结直肠癌筛查，” 见 *IEEE ICSCCC*，2023
    年，第 501–506 页。'
- en: '[36] L. F. Sanchez-Peralta, L. Bote-Curiel, A. Picon, F. M. Sanchez-Margallo,
    and J. B. Pagador, “Deep learning to find colorectal polyps in colonoscopy: A
    systematic literature review,” *Artificial Intelligence in Medicine*, vol. 108,
    p. 101923, 2020.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] L. F. Sanchez-Peralta, L. Bote-Curiel, A. Picon, F. M. Sanchez-Margallo,
    和 J. B. Pagador，“使用深度学习发现结直肠息肉：系统文献综述，” *Artificial Intelligence in Medicine*，第
    108 卷，页码 101923，2020 年。'
- en: '[37] H. Xiao, L. Li, Q. Liu, X. Zhu, and Q. Zhang, “Transformers in medical
    image segmentation: A review,” *BSPC*, vol. 84, p. 104791, 2023.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] H. Xiao, L. Li, Q. Liu, X. Zhu, 和 Q. Zhang，“医学图像分割中的变换器：综述，” *BSPC*，第
    84 卷，页码 104791，2023 年。'
- en: '[38] I. Qureshi, J. Yan, Q. Abbas, K. Shaheed, A. B. Riaz, A. Wahid, M. W. J.
    Khan, and P. Szczuko, “Medical image segmentation using deep semantic-based methods:
    A review of techniques, applications and emerging trends,” *Information Fusion*,
    2022.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] I. Qureshi, J. Yan, Q. Abbas, K. Shaheed, A. B. Riaz, A. Wahid, M. W.
    J. Khan, 和 P. Szczuko，“使用深度语义方法的医学图像分割：技术、应用和新兴趋势的综述，” *Information Fusion*，2022
    年。'
- en: '[39] C. L. Chowdhary and D. P. Acharjya, “Segmentation and feature extraction
    in medical imaging: a systematic review,” *Procedia Computer Science*, vol. 167,
    pp. 26–36, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. L. Chowdhary 和 D. P. Acharjya，“医学影像中的分割与特征提取：系统综述，” *Procedia Computer
    Science*，第 167 卷，第 26–36 页，2020 年。'
- en: '[40] L. Liu, J. Cheng, Q. Quan, F.-X. Wu, Y.-P. Wang, and J. Wang, “A survey
    on u-shaped networks in medical image segmentations,” *Neurocomputing*, vol. 409,
    pp. 244–258, 2020.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] L. Liu, J. Cheng, Q. Quan, F.-X. Wu, Y.-P. Wang, 和 J. Wang, “医学图像分割中的U型网络调查，”
    *Neurocomputing*, vol. 409, pp. 244–258, 2020。'
- en: '[41] H. Thisanke, C. Deshan, K. Chamith, S. Seneviratne, R. Vidanaarachchi,
    and D. Herath, “Semantic segmentation using vision transformers: A survey,” *EAAI*,
    vol. 126, p. 106669, 2023.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Thisanke, C. Deshan, K. Chamith, S. Seneviratne, R. Vidanaarachchi,
    和 D. Herath, “使用视觉变压器的语义分割: 一项调查，” *EAAI*, vol. 126, p. 106669, 2023。'
- en: '[42] M. T. Bennai, Z. Guessoum, S. Mazouzi, S. Cormier, and M. Mezghiche, “Multi-agent
    medical image segmentation: A survey,” *CMPB*, p. 107444, 2023.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. T. Bennai, Z. Guessoum, S. Mazouzi, S. Cormier, 和 M. Mezghiche, “多智能体医学图像分割:
    一项调查，” *CMPB*, p. 107444, 2023。'
- en: '[43] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange, P. Halvorsen,
    and H. D. Johansen, “Resunet++: An advanced architecture for medical image segmentation,”
    in *IEEE ISM*.   IEEE, 2019, pp. 225–2255.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange, P. Halvorsen,
    和 H. D. Johansen, “Resunet++: 一种先进的医学图像分割架构，” 在 *IEEE ISM*。 IEEE, 2019, pp. 225–2255。'
- en: '[44] J. Zhong, W. Wang, H. Wu, Z. Wen, and J. Qin, “Polypseg: An efficient
    context-aware network for polyp segmentation from colonoscopy videos,” in *MICCAI*.   Springer,
    2020, pp. 285–294.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] J. Zhong, W. Wang, H. Wu, Z. Wen, 和 J. Qin, “Polypseg: 一种高效的上下文感知网络，用于从结肠镜视频中分割息肉，”
    在 *MICCAI*。 Springer, 2020, pp. 285–294。'
- en: '[45] N. K. Tomar, D. Jha, S. Ali, H. D. Johansen, D. Johansen, M. A. Riegler,
    and P. Halvorsen, “Ddanet: Dual decoder attention network for automatic polyp
    segmentation,” in *Pattern Recognition. ICPR International Workshops and Challenges*.   Springer,
    2021, pp. 307–314.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] N. K. Tomar, D. Jha, S. Ali, H. D. Johansen, D. Johansen, M. A. Riegler,
    和 P. Halvorsen, “Ddanet: 双解码器注意力网络用于自动息肉分割，” 在 *Pattern Recognition. ICPR International
    Workshops and Challenges*。 Springer, 2021, pp. 307–314。'
- en: '[46] A. Srivastava, S. Chanda, D. Jha, U. Pal, and S. Ali, “Gmsrf-net: An improved
    generalizability with global multi-scale residual fusion network for polyp segmentation,”
    in *IEEE ICPR*, 2022, pp. 4321–4327.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Srivastava, S. Chanda, D. Jha, U. Pal, 和 S. Ali, “Gmsrf-net: 一种改进的全局多尺度残差融合网络，用于息肉分割，”
    在 *IEEE ICPR*, 2022, pp. 4321–4327。'
- en: '[47] C.-H. Huang, H.-Y. Wu, and Y.-L. Lin, “Hardnet-mseg: A simple encoder-decoder
    polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps,”
    *arXiv preprint arXiv:2101.07172*, 2021.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] C.-H. Huang, H.-Y. Wu, 和 Y.-L. Lin, “Hardnet-mseg: 一种简单的编码器-解码器息肉分割神经网络，达到超过0.9的平均Dice系数和86
    fps，” *arXiv preprint arXiv:2101.07172*, 2021。'
- en: '[48] K. Patel, A. M. Bur, and G. Wang, “Enhanced u-net: A feature enhancement
    network for polyp segmentation,” in *IEEE CRV*, 2021, pp. 181–188.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] K. Patel, A. M. Bur, 和 G. Wang, “增强的U-net: 一种用于息肉分割的特征增强网络，” 在 *IEEE CRV*,
    2021, pp. 181–188。'
- en: '[49] N. K. Tomar, D. Jha, M. A. Riegler, H. D. Johansen, D. Johansen, J. Rittscher,
    P. Halvorsen, and S. Ali, “Fanet: A feedback attention network for improved biomedical
    image segmentation,” *IEEE TNNLS*, 2022.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] N. K. Tomar, D. Jha, M. A. Riegler, H. D. Johansen, D. Johansen, J. Rittscher,
    P. Halvorsen, 和 S. Ali, “Fanet: 一种反馈注意力网络用于改进生物医学图像分割，” *IEEE TNNLS*, 2022。'
- en: '[50] T. Kim, H. Lee, and D. Kim, “Uacanet: Uncertainty augmented context attention
    for polyp segmentation,” in *ACM MM*, 2021, pp. 2167–2175.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] T. Kim, H. Lee, 和 D. Kim, “Uacanet: 不确定性增强的上下文注意力用于息肉分割，” 在 *ACM MM*,
    2021, pp. 2167–2175。'
- en: '[51] Y. Sun, G. Chen, T. Zhou, Y. Zhang, and N. Liu, “Context-aware cross-level
    fusion network for camouflaged object detection,” *arXiv preprint arXiv:2105.12555*,
    2021.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Sun, G. Chen, T. Zhou, Y. Zhang, 和 N. Liu, “用于伪装物体检测的上下文感知跨层融合网络，”
    *arXiv preprint arXiv:2105.12555*, 2021。'
- en: '[52] D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen, P. Halvorsen,
    and M. A. Riegler, “A comprehensive study on colorectal polyp segmentation with
    resunet++, conditional random field and test-time augmentation,” *IEEE JBHI*,
    vol. 25, no. 6, pp. 2029–2040, 2021.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen, P. Halvorsen,
    和 M. A. Riegler, “关于结直肠息肉分割的综合研究，结合了resunet++、条件随机场和测试时增强，” *IEEE JBHI*, vol.
    25, no. 6, pp. 2029–2040, 2021。'
- en: '[53] Y. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and cnns
    for medical image segmentation,” in *MICCAI*.   Springer, 2021, pp. 14–24.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Zhang, H. Liu, 和 Q. Hu, “Transfuse: 将变压器和CNN融合用于医学图像分割，” 在 *MICCAI*。
    Springer, 2021, pp. 14–24。'
- en: '[54] L. Wu, Z. Hu, Y. Ji, P. Luo, and S. Zhang, “Multi-frame collaboration
    for effective endoscopic video polyp detection via spatial-temporal feature transformation,”
    in *MICCAI*.   Springer, 2021, pp. 302–312.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] L. Wu, Z. Hu, Y. Ji, P. Luo, 和 S. Zhang, “通过时空特征变换实现有效的内窥镜视频息肉检测的多帧协作，”
    在 *MICCAI*。 Springer, 2021, pp. 302–312。'
- en: '[55] M. Cheng, Z. Kong, G. Song, Y. Tian, Y. Liang, and J. Chen, “Learnable
    oriented-derivative network for polyp segmentation,” in *MICCAI*.   Springer,
    2021, pp. 720–730.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M. Cheng, Z. Kong, G. Song, Y. Tian, Y. Liang, 和 J. Chen，“可学习的定向导数网络用于息肉分割，”
    见 *MICCAI*。 Springer，2021年，页码 720–730。'
- en: '[56] T.-C. Nguyen, T.-P. Nguyen, G.-H. Diep, A.-H. Tran-Dinh, T. V. Nguyen,
    and M.-T. Tran, “Ccbanet: cascading context and balancing attention for polyp
    segmentation,” in *MICCAI*.   Springer, 2021, pp. 633–643.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] T.-C. Nguyen, T.-P. Nguyen, G.-H. Diep, A.-H. Tran-Dinh, T. V. Nguyen,
    和 M.-T. Tran，“Ccbanet: 用于息肉分割的级联上下文和平衡注意力，” 见 *MICCAI*。 Springer，2021年，页码 633–643。'
- en: '[57] Y. Shen, X. Jia, and M. Q.-H. Meng, “Hrenet: A hard region enhancement
    network for polyp segmentation,” in *MICCAI*.   Springer, 2021, pp. 559–568.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Shen, X. Jia, 和 M. Q.-H. Meng，“Hrenet: 一种硬区域增强网络用于息肉分割，” 见 *MICCAI*。
    Springer，2021年，页码 559–568。'
- en: '[58] H. Wu, Z. Zhao, J. Zhong, W. Wang, Z. Wen, and J. Qin, “Polypseg+: A lightweight
    context-aware network for real-time polyp segmentation,” *IEEE TCYB*, vol. 53,
    no. 4, pp. 2610–2621, 2022.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Wu, Z. Zhao, J. Zhong, W. Wang, Z. Wen, 和 J. Qin，“Polypseg+: 一种轻量级上下文感知网络用于实时息肉分割，”
    *IEEE TCYB*，第 53 卷，第 4 期，页码 2610–2621，2022年。'
- en: '[59] C. Wu, C. Long, S. Li, J. Yang, F. Jiang, and R. Zhou, “Msraformer: Multiscale
    spatial reverse attention network for polyp segmentation,” *CBM*, vol. 151, p.
    106274, 2022.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] C. Wu, C. Long, S. Li, J. Yang, F. Jiang, 和 R. Zhou，“Msraformer: 多尺度空间反向注意力网络用于息肉分割，”
    *CBM*，第 151 卷，页码 106274，2022年。'
- en: '[60] K. B. Patel, F. Li, and G. Wang, “Fuzzynet: A fuzzy attention module for
    polyp segmentation,” in *NeurIPS’22 Workshop on All Things Attention: Bridging
    Different Perspectives on Attention*, 2022.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] K. B. Patel, F. Li, 和 G. Wang，“Fuzzynet: 一种模糊注意力模块用于息肉分割，” 见 *NeurIPS’22
    Workshop on All Things Attention: Bridging Different Perspectives on Attention*，2022年。'
- en: '[61] R. Zhang, P. Lai, X. Wan, D.-J. Fan, F. Gao, X.-J. Wu, and G. Li, “Lesion-aware
    dynamic kernel for polyp segmentation,” in *MICCAI*.   Springer, 2022, pp. 99–109.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] R. Zhang, P. Lai, X. Wan, D.-J. Fan, F. Gao, X.-J. Wu, 和 G. Li，“病灶感知动态内核用于息肉分割，”
    见 *MICCAI*。 Springer，2022年，页码 99–109。'
- en: '[62] T.-Y. Liao, C.-H. Yang, Y.-W. Lo, K.-Y. Lai, P.-H. Shen, and Y.-L. Lin,
    “Hardnet-dfus: An enhanced harmonically-connected network for diabetic foot ulcer
    image segmentation and colonoscopy polyp segmentation,” *arXiv preprint arXiv:2209.07313*,
    2022.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] T.-Y. Liao, C.-H. Yang, Y.-W. Lo, K.-Y. Lai, P.-H. Shen, 和 Y.-L. Lin，“Hardnet-dfus:
    一种增强的和谐连接网络用于糖尿病足溃疡图像分割和结肠镜息肉分割，” *arXiv 预印本 arXiv:2209.07313*，2022年。'
- en: '[63] Z. Qiu, Z. Wang, M. Zhang, Z. Xu, J. Fan, and L. Xu, “Bdg-net: boundary
    distribution guided network for accurate polyp segmentation,” in *Medical Imaging:
    Image Processing*, vol. 12032.   SPIE, 2022, pp. 792–799.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Z. Qiu, Z. Wang, M. Zhang, Z. Xu, J. Fan, 和 L. Xu，“Bdg-net: 边界分布引导网络用于准确的息肉分割，”
    见 *Medical Imaging: Image Processing*，第 12032 卷。 SPIE，2022年，页码 792–799。'
- en: '[64] N. T. Duc, N. T. Oanh, N. T. Thuy, T. M. Triet, and V. S. Dinh, “Colonformer:
    An efficient transformer based method for colon polyp segmentation,” *IEEE Access*,
    vol. 10, pp. 80 575–80 586, 2022.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] N. T. Duc, N. T. Oanh, N. T. Thuy, T. M. Triet, 和 V. S. Dinh，“Colonformer:
    一种基于变换器的高效结肠息肉分割方法，” *IEEE Access*，第 10 卷，页码 80 575–80 586，2022年。'
- en: '[65] E. Sanderson and B. J. Matuszewski, “Fcn-transformer feature fusion for
    polyp segmentation,” in *MIUA*.   Springer, 2022, pp. 892–907.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] E. Sanderson 和 B. J. Matuszewski，“Fcn-transformer 特征融合用于息肉分割，” 见 *MIUA*。
    Springer，2022年，页码 892–907。'
- en: '[66] Z. Yin, K. Liang, Z. Ma, and J. Guo, “Duplex contextual relation network
    for polyp segmentation,” in *IEEE ISBI*.   IEEE, 2022, pp. 1–5.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Z. Yin, K. Liang, Z. Ma, 和 J. Guo，“用于息肉分割的双重上下文关系网络，” 见 *IEEE ISBI*。 IEEE，2022年，页码
    1–5。'
- en: '[67] M. Nguyen, T. T. Bui, Q. Van Nguyen, T. T. Nguyen, and T. Van Pham, “Lapformer:
    A light and accurate polyp segmentation transformer,” *arXiv preprint arXiv:2210.04393*,
    2022.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] M. Nguyen, T. T. Bui, Q. Van Nguyen, T. T. Nguyen, 和 T. Van Pham，“Lapformer:
    一种轻量且准确的息肉分割变换器，” *arXiv 预印本 arXiv:2210.04393*，2022年。'
- en: '[68] L. Cai, M. Wu, L. Chen, W. Bai, M. Yang, S. Lyu, and Q. Zhao, “Using guided
    self-attention with local information for polyp segmentation,” in *MICCAI*.   Springer,
    2022, pp. 629–638.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] L. Cai, M. Wu, L. Chen, W. Bai, M. Yang, S. Lyu, 和 Q. Zhao，“利用带局部信息的引导自注意力进行息肉分割，”
    见 *MICCAI*。 Springer，2022年，页码 629–638。'
- en: '[69] Y. Lin, J. Wu, G. Xiao, J. Guo, G. Chen, and J. Ma, “Bsca-net: Bit slicing
    context attention network for polyp segmentation,” *PR*, vol. 132, p. 108917,
    2022.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Lin, J. Wu, G. Xiao, J. Guo, G. Chen, 和 J. Ma，“Bsca-net: 位切片上下文注意力网络用于息肉分割，”
    *PR*，第 132 卷，页码 108917，2022年。'
- en: '[70] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, and Z. Li, “Boxpolyp: Boost
    generalized polyp segmentation using extra coarse bounding box annotations,” in
    *MICCAI*.   Springer, 2022, pp. 67–77.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, 和 Z. Li，“Boxpolyp: 通过额外粗略边界框注释提升通用息肉分割，”
    见 *MICCAI*。 Springer，2022年，页码 67–77。'
- en: '[71] Y. Xiao, Z. Chen, L. Wan, L. Yu, and L. Zhu, “Icbnet: Iterative context-boundary
    feedback network for polyp segmentation,” in *IEEE BIBM*, 2022, pp. 1297–1304.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Y. Xiao, Z. Chen, L. Wan, L. Yu, 和 L. Zhu, “Icbnet: 用于息肉分割的迭代上下文-边界反馈网络，”
    在 *IEEE BIBM*, 2022, pp. 1297–1304。'
- en: '[72] R. Chen, X. Wang, B. Jin, J. Tu, F. Zhu, and Y. Li, “Cld-net: Complement
    local detail for medical small-object segmentation,” in *IEEE BIBM*, 2022, pp.
    942–947.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] R. Chen, X. Wang, B. Jin, J. Tu, F. Zhu, 和 Y. Li, “Cld-net: 适用于医学小物体分割的补充局部细节，”
    在 *IEEE BIBM*, 2022, pp. 942–947。'
- en: '[73] L. Lu, X. Zhou, S. Chen, Z. Chen, J. Yu, H. Tang, and X. Hu, “Boundary-aware
    polyp segmentation network,” in *PRCV*.   Springer, 2022, pp. 66–77.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] L. Lu, X. Zhou, S. Chen, Z. Chen, J. Yu, H. Tang, 和 X. Hu, “边界感知息肉分割网络，”
    在 *PRCV*。   Springer, 2022, pp. 66–77。'
- en: '[74] A. Lou, S. Guan, H. Ko, and M. H. Loew, “Caranet: Context axial reverse
    attention network for segmentation of small medical objects,” in *Medical Imaging
    2022: Image Processing*, vol. 12032.   SPIE, 2022, pp. 81–92.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] A. Lou, S. Guan, H. Ko, 和 M. H. Loew, “Caranet: 上下文轴向逆向注意力网络用于小型医学物体的分割，”
    在 *Medical Imaging 2022: 图像处理*, vol. 12032。   SPIE, 2022, pp. 81–92。'
- en: '[75] G. Yue, S. Li, R. Cong, T. Zhou, B. Lei, and T. Wang, “Attention-guided
    pyramid context network for polyp segmentation in colonoscopy images,” *IEEE TIM*,
    vol. 72, pp. 1–13, 2023.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] G. Yue, S. Li, R. Cong, T. Zhou, B. Lei, 和 T. Wang, “注意力引导金字塔上下文网络用于结肠镜图像中的息肉分割，”
    *IEEE TIM*, vol. 72, pp. 1–13, 2023。'
- en: '[76] K. Wang, L. Liu, X. Fu, L. Liu, and W. Peng, “Ra-denet: Reverse attention
    and distractions elimination network for polyp segmentation,” *Computers in Biology
    and Medicine*, vol. 155, p. 106704, 2023.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] K. Wang, L. Liu, X. Fu, L. Liu, 和 W. Peng, “Ra-denet: 逆向注意力与干扰消除网络用于息肉分割，”
    *Computers in Biology and Medicine*, vol. 155, p. 106704, 2023。'
- en: '[77] Y. Su, J. Cheng, C. Zhong, C. Jiang, J. Ye, and J. He, “Accurate polyp
    segmentation through enhancing feature fusion and boosting boundary performance,”
    *Neurocomputing*, vol. 545, p. 126233, 2023.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. Su, J. Cheng, C. Zhong, C. Jiang, J. Ye, 和 J. He, “通过增强特征融合和提升边界性能实现准确的息肉分割，”
    *Neurocomputing*, vol. 545, p. 126233, 2023。'
- en: '[78] K. Hu, W. Chen, Y. Sun, X. Hu, Q. Zhou, and Z. Zheng, “Ppnet: Pyramid
    pooling based network for polyp segmentation,” *Computers in Biology and Medicine*,
    vol. 160, p. 107028, 2023.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] K. Hu, W. Chen, Y. Sun, X. Hu, Q. Zhou, 和 Z. Zheng, “Ppnet: 基于金字塔池化的息肉分割网络，”
    *Computers in Biology and Medicine*, vol. 160, p. 107028, 2023。'
- en: '[79] N. K. Tomar, D. Jha, and U. Bagci, “Dilatedsegnet: A deep dilated segmentation
    network for polyp segmentation,” in *MMM*.   Springer, 2023, pp. 334–344.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] N. K. Tomar, D. Jha, 和 U. Bagci, “Dilatedsegnet: 用于息肉分割的深度扩张分割网络，” 在 *MMM*。   Springer,
    2023, pp. 334–344。'
- en: '[80] Y. Su, J. Cheng, C. Zhong, Y. Zhang, J. Ye, J. He, and J. Liu, “Fednet:
    Feature decoupled network for polyp segmentation from endoscopy images,” *BSPC*,
    vol. 83, p. 104699, 2023.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Su, J. Cheng, C. Zhong, Y. Zhang, J. Ye, J. He, 和 J. Liu, “Fednet:
    从内镜图像中分离特征的息肉分割网络，” *BSPC*, vol. 83, p. 104699, 2023。'
- en: '[81] T.-H. Nguyen-Mau, Q.-H. Trinh, N.-T. Bui, P.-T. V. Thi, M.-V. Nguyen,
    X.-N. Cao, M.-T. Tran, and H.-D. Nguyen, “Pefnet: Positional embedding feature
    for polyp segmentation,” in *MMM*.   Springer, 2023, pp. 240–251.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] T.-H. Nguyen-Mau, Q.-H. Trinh, N.-T. Bui, P.-T. V. Thi, M.-V. Nguyen,
    X.-N. Cao, M.-T. Tran, 和 H.-D. Nguyen, “Pefnet: 用于息肉分割的位置信息嵌入特征，” 在 *MMM*。   Springer,
    2023, pp. 240–251。'
- en: '[82] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” *arXiv preprint arXiv:2010.11929*,
    2020.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*, “一张图像值16x16个单词: 用于大规模图像识别的变换器，”
    *arXiv 预印本 arXiv:2010.11929*, 2020。'
- en: '[83] G.-P. Ji, G. Xiao, Y.-C. Chou, D.-P. Fan, K. Zhao, G. Chen, and L. Van Gool,
    “Video polyp segmentation: A deep learning perspective,” *MIR*, vol. 19, no. 6,
    pp. 531–549, 2022.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] G.-P. Ji, G. Xiao, Y.-C. Chou, D.-P. Fan, K. Zhao, G. Chen, 和 L. Van Gool,
    “视频息肉分割: 深度学习视角，” *MIR*, vol. 19, no. 6, pp. 531–549, 2022。'
- en: '[84] X. Zhao, Z. Wu, S. Tan, D.-J. Fan, Z. Li, X. Wan, and G. Li, “Semi-supervised
    spatial temporal attention network for video polyp segmentation,” in *MICCAI*.   Springer,
    2022, pp. 456–466.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Zhao, Z. Wu, S. Tan, D.-J. Fan, Z. Li, X. Wan, 和 G. Li, “半监督空间时间注意力网络用于视频息肉分割，”
    在 *MICCAI*。   Springer, 2022, pp. 456–466。'
- en: '[85] G.-P. Ji, Y.-C. Chou, D.-P. Fan, G. Chen, H. Fu, D. Jha, and L. Shao,
    “Progressively normalized self-attention network for video polyp segmentation,”
    in *MICCAI*.   Springer, 2021, pp. 142–152.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] G.-P. Ji, Y.-C. Chou, D.-P. Fan, G. Chen, H. Fu, D. Jha, 和 L. Shao, “逐步归一化自注意力网络用于视频息肉分割，”
    在 *MICCAI*。   Springer, 2021, pp. 142–152。'
- en: '[86] D. Jha, N. K. Tomar, S. Ali, M. A. Riegler, H. D. Johansen, D. Johansen,
    T. de Lange, and P. Halvorsen, “Nanonet: Real-time polyp segmentation in video
    capsule endoscopy and colonoscopy,” in *IEEE CBMS*.   IEEE, 2021, pp. 37–43.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] D. Jha, N. K. Tomar, S. Ali, M. A. Riegler, H. D. Johansen, D. Johansen,
    T. de Lange, 和 P. Halvorsen，“Nanonet: 视频胶囊内镜和结肠镜中的实时息肉分割，” 见于 *IEEE CBMS*。IEEE,
    2021, 页 37–43。'
- en: '[87] J. Silva, A. Histace, O. Romain, X. Dray, and B. Granado, “Toward embedded
    detection of polyps in wce images for early diagnosis of colorectal cancer,” *IJCARS*,
    vol. 9, pp. 283–293, 2014.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] J. Silva, A. Histace, O. Romain, X. Dray, 和 B. Granado，“针对内窥镜图像中息肉的嵌入式检测以便早期诊断结直肠癌，”
    *IJCARS*, 卷 9, 页 283–293, 2014。'
- en: '[88] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen,
    and H. D. Johansen, “Kvasir-seg: A segmented polyp dataset,” in *MMM*.   Springer,
    2020, pp. 451–462.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen,
    和 H. D. Johansen，“Kvasir-seg: 一个分割的息肉数据集，” 见于 *MMM*。Springer, 2020, 页 451–462。'
- en: '[89] Y. Ma, X. Chen, K. Cheng, Y. Li, and B. Sun, “Ldpolypvideo benchmark:
    a large-scale colonoscopy video dataset of diverse polyps,” in *MICCAI 2021*.   Springer,
    pp. 387–396.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. Ma, X. Chen, K. Cheng, Y. Li, 和 B. Sun，“Ldpolypvideo 基准：大规模结肠镜视频数据集中的多样息肉，”
    见于 *MICCAI 2021*。Springer, 页 387–396。'
- en: '[90] A. Wang, M. Xu, Y. Zhang, M. Islam, and H. Ren, “S2me: Spatial-spectral
    mutual teaching and ensemble learning for scribble-supervised polyp segmentation,”
    *arXiv preprint arXiv:2306.00451*, 2023.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. Wang, M. Xu, Y. Zhang, M. Islam, 和 H. Ren，“S2me: 空间-光谱互教与集成学习用于涂鸦监督的息肉分割，”
    *arXiv preprint arXiv:2306.00451*, 2023。'
- en: '[91] L. F. Sánchez-Peralta, J. B. Pagador, A. Picón, Á. J. Calderón, F. Polo,
    N. Andraka, R. Bilbao, B. Glover, C. L. Saratxaga, and F. M. Sánchez-Margallo,
    “Piccolo white-light and narrow-band imaging colonoscopic dataset: A performance
    comparative of models and datasets,” *Applied Sciences*, vol. 10, no. 23, p. 8501,
    2020.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] L. F. Sánchez-Peralta, J. B. Pagador, A. Picón, Á. J. Calderón, F. Polo,
    N. Andraka, R. Bilbao, B. Glover, C. L. Saratxaga, 和 F. M. Sánchez-Margallo，“Piccolo
    白光和窄带成像结肠镜数据集：模型和数据集的性能比较，” *Applied Sciences*, 卷 10, 期 23, 页 8501, 2020。'
- en: '[92] S. Ali, D. Jha, N. Ghatwary, S. Realdon, R. Cannizzaro, O. E. Salem, D. Lamarque,
    C. Daul, M. A. Riegler, K. V. Anonsen *et al.*, “A multi-centre polyp detection
    and segmentation dataset for generalisability assessment,” *Scientific Data*,
    vol. 10, no. 1, p. 75, 2023.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Ali, D. Jha, N. Ghatwary, S. Realdon, R. Cannizzaro, O. E. Salem, D.
    Lamarque, C. Daul, M. A. Riegler, K. V. Anonsen *等*, “一个多中心息肉检测和分割数据集用于泛化评估，”
    *Scientific Data*, 卷 10, 期 1, 页 75, 2023。'
- en: '[93] “Gastrointestinal image analysis (giana) challenge,” Online, available:
    [https://endovissub2017-giana.grand-challenge.org/](https://endovissub2017-giana.grand-challenge.org/).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] “胃肠图像分析 (giana) 挑战，” 在线, 可用: [https://endovissub2017-giana.grand-challenge.org/](https://endovissub2017-giana.grand-challenge.org/)。'
- en: '[94] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient object detection:
    A benchmark,” *IEEE TIP*, vol. 24, no. 12, pp. 5706–5722, 2015.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] A. Borji, M.-M. Cheng, H. Jiang, 和 J. Li，“显著目标检测：基准测试，” *IEEE TIP*, 卷
    24, 期 12, 页 5706–5722, 2015。'
- en: '[95] F. Perazzi, P. Krähenbühl, Y. Pritch, and A. Hornung, “Saliency filters:
    Contrast based filtering for salient region detection,” in *IEEE CVPR*, 2012,
    pp. 733–740.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] F. Perazzi, P. Krähenbühl, Y. Pritch, 和 A. Hornung，“显著性过滤器：用于显著区域检测的对比度基过滤，”
    见于 *IEEE CVPR*, 2012, 页 733–740。'
- en: '[96] D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, and A. Borji, “Structure-measure:
    A new way to evaluate foreground maps,” in *IEEE ICCV*, 2017, pp. 4548–4557.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, 和 A. Borji，“结构测量：评估前景图的新方法，” 见于
    *IEEE ICCV*, 2017, 页 4548–4557。'
- en: '[97] D.-P. Fan, C. Gong, Y. Cao, B. Ren, M.-M. Cheng, and A. Borji, “Enhanced-alignment
    measure for binary foreground map evaluation,” in *IJCAI*, 2018, pp. 698–704.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] D.-P. Fan, C. Gong, Y. Cao, B. Ren, M.-M. Cheng, 和 A. Borji，“用于二进制前景图评估的增强对齐测量，”
    见于 *IJCAI*, 2018, 页 698–704。'
- en: '[98] T. Zhou, Y. Zhang, G. Chen, Y. Zhou, Y. Wu, and D.-P. Fan, “Edge-aware
    feature aggregation network for polyp segmentation,” *arXiv preprint arXiv:2309.10523*,
    2023.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] T. Zhou, Y. Zhang, G. Chen, Y. Zhou, Y. Wu, 和 D.-P. Fan，“边缘感知特征聚合网络用于息肉分割，”
    *arXiv preprint arXiv:2309.10523*, 2023。'
- en: '[99] X. Zhao, H. Jia, Y. Pang, L. Lv, F. Tian, L. Zhang, W. Sun, and H. Lu,
    “M2snet: Multi-scale in multi-scale subtraction network for medical image segmentation,”
    *arXiv preprint arXiv:2303.10894*, 2023.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] X. Zhao, H. Jia, Y. Pang, L. Lv, F. Tian, L. Zhang, W. Sun, 和 H. Lu，“M2snet:
    多尺度中的多尺度减法网络用于医学图像分割，” *arXiv preprint arXiv:2303.10894*, 2023。'
- en: '[100] T. Zhou, Y. Zhang, Y. Zhou, Y. Wu, and C. Gong, “Can sam segment polyps?”
    *arXiv preprint arXiv:2304.07583*, 2023.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] T. Zhou, Y. Zhang, Y. Zhou, Y. Wu, 和 C. Gong，“SAM 能分割息肉吗？” *arXiv preprint
    arXiv:2304.07583*, 2023。'
- en: '[101] L. Shan, X. Li, and W. Wang, “Decouple the high-frequency and low-frequency
    information of images for semantic segmentation,” in *IEEE ICASSP*, 2021, pp.
    1805–1809.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] L. Shan, X. Li, 和 W. Wang，“解耦图像的高频和低频信息以进行语义分割，”发表于 *IEEE ICASSP*，2021年，第1805–1809页。'
- en: '[102] R. Cong, M. Sun, S. Zhang, X. Zhou, W. Zhang, and Y. Zhao, “Frequency
    perception network for camouflaged object detection,” in *ACM MM*, 2023, pp. 1179–1189.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] R. Cong, M. Sun, S. Zhang, X. Zhou, W. Zhang, 和 Y. Zhao，“用于伪装物体检测的频率感知网络，”发表于
    *ACM MM*，2023年，第1179–1189页。'
- en: '[103] H. Wu, G. Chen, Z. Wen, and J. Qin, “Collaborative and adversarial learning
    of focused and dispersive representations for semi-supervised polyp segmentation,”
    in *IEEE ICCV*, October 2021, pp. 3489–3498.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] H. Wu, G. Chen, Z. Wen, 和 J. Qin，“专注和分散表示的协作与对抗学习用于半监督息肉分割，”发表于 *IEEE
    ICCV*，2021年10月，第3489–3498页。'
- en: '[104] H. Cho, Y. Han, and W. H. Kim, “Anti-adversarial consistency regularization
    for data augmentation: Applications to robust medical image segmentation,” in
    *MICCAI*.   Springer, 2023, pp. 555–566.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] H. Cho, Y. Han, 和 W. H. Kim，“用于数据增强的抗对抗一致性正则化：应用于鲁棒的医学图像分割，”发表于 *MICCAI*。
     Springer，2023年，第555–566页。'
- en: '[105] T. Judge, O. Bernard, W.-J. Cho Kim, A. Gomez, A. Chartsias, and P.-M.
    Jodoin, “Asymmetric contour uncertainty estimation for medical image segmentation,”
    in *MICCAI*.   Springer, 2023, pp. 210–220.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] T. Judge, O. Bernard, W.-J. Cho Kim, A. Gomez, A. Chartsias, 和 P.-M.
    Jodoin，“用于医学图像分割的非对称轮廓不确定性估计，”发表于 *MICCAI*。  Springer，2023年，第210–220页。'
- en: '[106] Q. Wei, L. Yu, X. Li, W. Shao, C. Xie, L. Xing, and Y. Zhou, “Consistency-guided
    meta-learning for bootstrapping semi-supervised medical image segmentation,” in
    *MICCAI*.   Springer, 2023, pp. 183–193.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Q. Wei, L. Yu, X. Li, W. Shao, C. Xie, L. Xing, 和 Y. Zhou，“一致性引导的元学习用于引导半监督医学图像分割，”发表于
    *MICCAI*。  Springer，2023年，第183–193页。'
- en: '[107] H. Wu, Z. Zhao, J. Zhong, W. Wang, Z. Wen, and J. Qin, “Polypseg+: A
    lightweight context-aware network for real-time polyp segmentation,” *IEEE TCYB*,
    vol. 53, no. 4, pp. 2610–2621, 2022.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Wu, Z. Zhao, J. Zhong, W. Wang, Z. Wen, 和 J. Qin，“Polypseg+：一种轻量级上下文感知网络用于实时息肉分割，”
    *IEEE TCYB*，第53卷，第4期，第2610–2621页，2022年。'
- en: '[108] D. Jha, S. Ali, N. K. Tomar, H. D. Johansen, D. Johansen, J. Rittscher,
    M. A. Riegler, and P. Halvorsen, “Real-time polyp detection, localization and
    segmentation in colonoscopy using deep learning,” *IEEE Access*, vol. 9, pp. 40 496–40 510,
    2021.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] D. Jha, S. Ali, N. K. Tomar, H. D. Johansen, D. Johansen, J. Rittscher,
    M. A. Riegler, 和 P. Halvorsen，“使用深度学习进行结肠镜实时息肉检测、定位和分割，” *IEEE Access*，第9卷，第40,496–40,510页，2021年。'
- en: '[109] I. Wichakam, T. Panboonyuen, C. Udomcharoenchaikit, and P. Vateekul,
    “Real-time polyps segmentation for colonoscopy video frames using compressed fully
    convolutional network,” in *MMM*.   Springer, 2018, pp. 393–404.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] I. Wichakam, T. Panboonyuen, C. Udomcharoenchaikit, 和 P. Vateekul，“使用压缩全卷积网络对结肠镜视频帧进行实时息肉分割，”发表于
    *MMM*。  Springer，2018年，第393–404页。'
- en: '[110] N. K. Tomar, A. Shergill, B. Rieders, U. Bagci, and D. Jha, “Transresu-net:
    Transformer based ResU-Net for real-time colonoscopy polyp segmentation,” *arXiv
    preprint arXiv:2206.08985*, 2022.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] N. K. Tomar, A. Shergill, B. Rieders, U. Bagci, 和 D. Jha，“Transresu-net：基于Transformer的ResU-Net用于实时结肠镜息肉分割，”
    *arXiv preprint arXiv:2206.08985*，2022年。'
- en: '[111] H. Wu, J. Zhong, W. Wang, Z. Wen, and J. Qin, “Precise yet efficient
    semantic calibration and refinement in convnets for real-time polyp segmentation
    from colonoscopy videos,” in *AAAI*, vol. 35, no. 4, 2021, pp. 2916–2924.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] H. Wu, J. Zhong, W. Wang, Z. Wen, 和 J. Qin，“在卷积网络中进行实时息肉分割的精确而高效的语义校准与优化，”发表于
    *AAAI*，第35卷，第4期，2021年，第2916–2924页。'
- en: '[112] R. Feng, B. Lei, W. Wang, T. Chen, J. Chen, D. Z. Chen, and J. Wu, “Ssn:
    A stair-shape network for real-time polyp segmentation in colonoscopy images,”
    in *IEEE ISBI*, 2020, pp. 225–229.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] R. Feng, B. Lei, W. Wang, T. Chen, J. Chen, D. Z. Chen, 和 J. Wu，“Ssn：一种楼梯形网络用于实时结肠镜图像中的息肉分割，”发表于
    *IEEE ISBI*，2020年，第225–229页。'
- en: '[113] Q. Liu, C. Chen, J. Qin, Q. Dou, and P.-A. Heng, “Feddg: Federated domain
    generalization on medical image segmentation via episodic learning in continuous
    frequency space,” in *IEEE CVPR*, 2021, pp. 1013–1023.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Q. Liu, C. Chen, J. Qin, Q. Dou, 和 P.-A. Heng，“Feddg：通过在连续频率空间中的情节学习进行医学图像分割的联邦领域泛化，”发表于
    *IEEE CVPR*，2021年，第1013–1023页。'
