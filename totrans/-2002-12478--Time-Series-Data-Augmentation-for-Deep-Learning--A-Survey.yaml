- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:02:14'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2002.12478] Time Series Data Augmentation for Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.12478](https://ar5iv.labs.arxiv.org/html/2002.12478)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Time Series Data Augmentation for Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Qingsong Wen¹, Liang Sun¹, Fan Yang², Xiaomin Song¹, Jingkun Gao³¹¹1The work
    was done when Jingkun Gao was at Alibaba Group., Xue Wang¹, Huan Xu² ¹DAMO Academy,
    Alibaba Group, Bellevue, WA, USA
  prefs: []
  type: TYPE_NORMAL
- en: ²Alibaba Group, Hangzhou, China
  prefs: []
  type: TYPE_NORMAL
- en: ³Twitter, Seattle, WA, USA {qingsong.wen, liang.sun, fanyang.yf, xiaomin.song,
    xue.w, huan.xu}@alibaba-inc.com, jingkung@twitter.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning performs remarkably well on many time series analysis tasks recently.
    The superior performance of deep neural networks relies heavily on a large number
    of training data to avoid overfitting. However, the labeled data of many real-world
    time series applications may be limited such as classification in medical time
    series and anomaly detection in AIOps. As an effective way to enhance the size
    and quality of the training data, data augmentation is crucial to the successful
    application of deep learning models on time series data. In this paper, we systematically
    review different data augmentation methods for time series. We propose a taxonomy
    for the reviewed methods, and then provide a structured review for these methods
    by highlighting their strengths and limitations. We also empirically compare different
    data augmentation methods for different tasks including time series classification,
    anomaly detection, and forecasting. Finally, we discuss and highlight five future
    directions to provide useful research guidance.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has achieved remarkable success in many fields, including computer
    vision (CV), natural language processing (NLP), and speech processing, etc. Recently,
    it is increasingly embraced for solving time series related tasks, including time
    series classification Fawaz et al. ([2019](#bib.bib16)), time series forecasting Han
    et al. ([2019](#bib.bib23)), and time series anomaly detection Gamboa ([2017](#bib.bib20)).
    The success of deep learning relies heavily on a large number of training data
    to avoid overfitting. Unfortunately, many time series tasks do not have enough
    labeled data. As an effective tool to enhance the size and quality of the training
    data, data augmentation is crucial to the successful application of deep learning
    models. The basic idea of data augmentation is to generate synthetic dataset covering
    unexplored input space while maintaining correct labels. Data augmentation has
    shown its effectiveness in many applications, such as AlexNet Krizhevsky et al.
    ([2012](#bib.bib32)) for ImageNet classification.
  prefs: []
  type: TYPE_NORMAL
- en: However, less attention has been paid to find better data augmentation methods
    specifically for time series data. Here we highlight some challenges arising from
    data augmentation methods for time series data. Firstly, the intrinsic properties
    of time series data are not fully utilized in current data augmentation methods.
    One unique property of time series data is the so-called temporal dependency.
    Unlike image data, the time series data can be transformed into the frequency
    and time-frequency domains and effective data augmentation methods can be designed
    and implemented in the transformed domain. This becomes more complicated when
    we model multivariate time series where we need to consider the potentially complex
    dynamics of these variables across time. Thus, simply applying those data augmentation
    methods from image and speech processing may not result in valid synthetic data.
    Secondly, the data augmentation methods are also task dependent. For example,
    the data augmentation methods applicable for time series classification may not
    be valid for time series anomaly detection. In addition, data augmentation becomes
    more crucial in many time series classification problems where class imbalance
    is often observed. In this case, how to effective generate a large number of synthetic
    data with labels with less samples remains a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike data augmentation for CV Shorten and Khoshgoftaar ([2019](#bib.bib48))
    or speech Cui et al. ([2015](#bib.bib9)), data augmentation for time series has
    not yet been comprehensively and systematically reviewed to the best of our knowledge.
    One work closely related to ours is Iwana and Uchida ([2020](#bib.bib27)) which
    presents a survey of existing data augmentation methods for time series classification.
    However, it does not review the data augmentation methods for other common tasks
    like time series forecasting Bandara et al. ([2020](#bib.bib2)); Hu et al. ([2020](#bib.bib26));
    Lee and Kim ([2020](#bib.bib35)) and anomaly detection Lim et al. ([2018](#bib.bib37));
    Zhou et al. ([2019](#bib.bib61)); Gao et al. ([2020](#bib.bib21)). Furthermore,
    the potential avenues for future research opportunities of time series data augmentations
    are also missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we aim to fill the aforementioned gaps by summarizing existing
    time series data augmentation methods in common tasks, including time series forecasting,
    anomaly detection, classification, as well as providing insightful future directions.
    To this end, we propose a taxonomy of data augmentation methods for time series,
    as illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Time Series Data
    Augmentation for Deep Learning: A Survey"). Based on the taxonomy, we review these
    data augmentation methods systematically. We start the discussion from the simple
    transformations in time domain first. And then we discuss more transformations
    on time series in the transformed frequency and time-frequency domains. Besides
    the transformations in different domains for time series, we also summarize more
    advanced methods, including decomposition-based methods, model-based methods,
    and learning-based methods. For learning-based methods, we further divide them
    into embedding space, deep generative models (DGMs), and automated data augmentation
    methods. To demonstrate effectiveness of data augmentation, we conduct preliminary
    evaluation of augmentation methods in three typical time series tasks, including
    time series classification, anomaly detection, and forecasting. Finally, we discuss
    and highlight five future directions: augmentation in time-frequency domain, augmentation
    for imbalanced class, augmentation selection and combination, augmentation with
    Gaussian processes, and augmentation with deep generative models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4042f0e5fd2fda5151e9266e0142e481.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A taxonomy of time series data augmentation techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Basic Data Augmentation Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Time Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The transforms in the time domain are the most straightforward data augmentation
    methods for time series data. Most of them manipulate the original input time
    series directly, like injecting Gaussian noise or more complicated noise patterns
    such as spike, step-like trend, and slope-like trend. Besides this straightforward
    methods, we will also discuss a particular data augmentation method for time series
    anomaly detection, i.e., label expansion in the time domain.
  prefs: []
  type: TYPE_NORMAL
- en: Window cropping or slicing has been mentioned in  Le Guennec et al. ([2016](#bib.bib34)).
    Introduced in  Cui et al. ([2016](#bib.bib10)), window cropping is similar to
    cropping in CV area. It is a sub-sample method to randomly extract continuous
    slices from the original time series. The length of the slice is a tunable parameter.
    For classification problem, the labels of sliced samples are the same as the original
    time series. During test time, each slice from a test time series is classified
    using majority voting. For anomaly detection problem, the anomaly label will be
    sliced along with value series.
  prefs: []
  type: TYPE_NORMAL
- en: Window warping is a unique augmentation method for time series. Similar to dynamic
    time warping (DTW), this method selects a random time range, then compresses (down
    sample) or extends (up sample) it, while keeps other time range unchanged. Window
    warping would change the total length of the original time series, so it should
    be conducted along with window cropping for deep learning models. This method
    contains the normal down sampling which takes down sample through the whole length
    of the original time series.
  prefs: []
  type: TYPE_NORMAL
- en: Flipping is another method that generates the new sequence $x^{{}^{\prime}}_{1},\cdots,x^{{}^{\prime}}_{N}$
    by flipping the sign of original time series $x_{1},\cdots,x_{N}$, where $x^{{}^{\prime}}_{t}=-x_{t}$.
    The labels are still the same, for both anomaly detection and classification,
    assuming that we have symmetry between up and down directions.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting perturbation and also ensemble based method is introduced
    in  Fawaz et al. ([2018](#bib.bib15)). This method generates new time series with
    DTW and then ensembles them by a weighted version of the Barycentric Averaging
    (DBA) algorithm. It shows improvement of classification in some of the UCR datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Noise injection is a method by injecting small amount of noise/outlier into
    time series without changing the corresponding labels. This includes injecting
    Gaussian noise, spike, step-like trend, and slope-like trend, etc. For spike,
    we can randomly pick index and direction, randomly assign magnitude but bounded
    by multiples of standard deviation of the original time series. For step-like
    trend, it is the cumulative summation of the spikes from left index to right index.
    The slope-like trend is adding a linear trend into the original time series. These
    schemes are mostly mentioned in Wen and Keyes ([2019](#bib.bib54))
  prefs: []
  type: TYPE_NORMAL
- en: In time series anomaly detection, the anomalies generally last long enough during
    a continuous span so that the start and end points are sometimes “blurry”. As
    a result, a data point close to a labeled anomaly in terms of both time distance
    and value distance is very likely to be an anomaly. In this case, the label expansion
    method is proposed to change those data points and their labels as anomalies (by
    assign it an anomaly score or switch its label), which brings performance improvement
    for time series anomaly detection as shown in Gao et al. ([2020](#bib.bib21)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Frequency Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While most of the existing data augmentation methods focus on time domain, only
    a few studies investigate data augmentation from frequency domain perspective
    for time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'A recent work in Gao et al. ([2020](#bib.bib21)) proposes to utilize perturbations
    in both amplitude spectrum and phase spectrum in frequency domain for data augmentation
    in time series anomaly detection by convolutional neural network. Specifically,
    for the input time series $x_{1},\cdots,x_{N}$, its frequency spectrum $F(\omega_{k})$
    through Fourier transform is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle F(\omega_{k})\!=\!\frac{1}{{N}}\!\!\sum_{t=0}^{N-1}\!x_{t}e^{-j\omega_{k}t}\
    =A(\omega_{k})\exp[j\theta(\omega_{k})]$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\omega_{k}=\frac{2\pi k}{N}$ is the angular frequency, $A(\omega_{k})$
    is the amplitude spectrum, and $\theta(\omega_{k})$ is the phase spectrum. For
    perturbations in amplitude spectrum $A(\omega_{k})$, the amplitude values of randomly
    selected segments are replaced with Gaussian noise by considering the original
    mean and variance in the amplitude spectrum. While for perturbations in phase
    spectrum $\theta(\omega_{k})$, the phase values of randomly selected segments
    are added by an extra zero-mean Gaussian noise in the phase spectrum. The amplitude
    and phase perturbations (APP) based data augmentation combined with aforementioned
    time-domain augmentation methods bring significant time series anomaly detection
    improvements as shown in the experiments of Gao et al. ([2020](#bib.bib21)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another recent work in Lee et al. ([2019](#bib.bib36)) proposes to utilize
    the surrogate data to improve the classification performance of rehabilitative
    time series in deep neural network. Two conventional types of surrogate time series
    are adopted in the work: the amplitude adjusted Fourier transform (AAFT) and the
    iterated AAFT (IAAFT) Schreiber and Schmitz ([2000](#bib.bib47)). The main idea
    is to perform random phase shuffle in phase spectrum after Fourier transform and
    then perform rank-ordering of time series after inverse Fourier transform. The
    generated time series from AAFT and IAAFT can approximately preserve the temporal
    correlation, power spectra, and the amplitude distribution of the original time
    series. In the experiments of Lee et al. ([2019](#bib.bib36)), the authors conducted
    two types of data augmentation by extending the data by 10 then 100 times through
    AAFT and IAAFT methods, and demonstrated promising classification accuracy improvements
    compared to the original time series without data augmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Time-Frequency Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time-frequency analysis is a widely applied technique for time series analysis,
    which can be utilized as an appropriate input features in deep neural networks.
    However, similar to data augmentation in frequency domain, only a few studies
    considered data augmentation from time-frequency domain for time series.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in Steven Eyobu and Han ([2018](#bib.bib50)) adopt short Fourier
    transform (STFT) to generate time-frequency features for sensor time series, and
    conduct data augmentation on the time-frequency features for human activity classification
    by a deep LSTM neural network. Specifically, two augmentation techniques are proposed.
    One is the local averaging based on a defined criteria with the generated features
    appended at the tail end of the feature set. Another is the shuffling of feature
    vectors to create variation in the data. Similarly, in speech time series, recently
    SpecAugment Park et al. ([2019](#bib.bib39)) is proposed to make data augmentation
    in Mel-Frequency (a time-frequency representation based on STFT for speech time
    series), where the augmentation scheme consists of warping the features, masking
    blocks of frequency channels, and masking blocks of time steps. They demonstrate
    that SpecAugment can greatly improve the performance of speech recognition neural
    networks and obtain state-of-the-art results.
  prefs: []
  type: TYPE_NORMAL
- en: 'For illustration, we summarize several typical time series data augmentation
    methods in time, frequency, and time-frequency domains in Fig. [2](#S2.F2 "Figure
    2 ‣ 2.3 Time-Frequency Domain ‣ 2 Basic Data Augmentation Methods ‣ Time Series
    Data Augmentation for Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b5109805a7194b1647331e6d4d73de64.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) time domain
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a303c0c8038e87a71c71f8f5c7a4b6d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) (time-)frequency domain
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Illustration of several typical time series data augmentations in
    time, frequency, and time-frequency domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Advanced Data Augmentation Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Decomposition-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decomposition-based time series augmentation has also been adopted and shown
    success in many time series related tasks, such as forecasting and anomaly detection.
    Common decomposition method like STL Cleveland et al. ([1990](#bib.bib6)) or RobustSTL Wen
    et al. ([2019b](#bib.bib56)) decomposes time series $x_{t}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{t}=\tau_{t}+s_{t}+r_{t},\quad t=1,2,...N$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\tau_{t}$ is the trend signal, $s_{t}$ is the seasonal/periodic signal,
    and the $r_{t}$ denotes the remainder signal.
  prefs: []
  type: TYPE_NORMAL
- en: In Kegel et al. ([2018](#bib.bib29)), authors discussed the decomposition method
    to generate new time series. After STL, it recombines new time series with a deterministic
    component and a stochastic component. The deterministic part is reconstructed
    by adjusting weights for base, trend, and seasonality. The stochastic part is
    generated by building a composite statistical model based on residual, such as
    an auto-regressive model. The summed generated time series is validated by examining
    whether a feature-based distance to its original signal is within certain range.
    Meanwhile, authors in Bergmeir et al. ([2016](#bib.bib3)) proposed to apply bootstrapping
    on the STL decomposed residuals to generate augmented signals, which are then
    added back with trend and seasonality to assemble a new time series. An ensemble
    of the forecasting models on the augmented time series has outperformed the original
    forecasting model consistently, demonstrating the effectiveness of decomposition-based
    time series augmentation approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, in Gao et al. ([2020](#bib.bib21)), authors showed that applying time-domain
    and frequency-domain augmentation on the decomposed residual that is generated
    using robust decomposition Wen et al. ([2020](#bib.bib57), [2019a](#bib.bib55))
    can help increase the performance of anomaly detection significantly, compared
    with the same method without augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Statistical Generative Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time series augmentation approaches based on statistical generative models typically
    involve modelling the dynamics of the time series with statistical models. In Cao
    et al. ([2014](#bib.bib4)), authors proposed a parsimonious statistical model,
    known as mixture of Gaussian trees, for modeling multi-modal minority class time
    series data to solve the problem of imbalanced classification, which shows advantages
    compared with existing oversampling approaches that do not exploit time series
    correlations between neighboring points. Authors in Smyl and Kuber ([2016](#bib.bib49))
    use samples of parameters and forecast paths calculated by a statistical algorithm
    called LGT (Local and Global Trend). More recently, in Kang et al. ([2020](#bib.bib28))
    researchers use mixture autoregressive (MAR) models to simulate sets of time series
    and investigate the diversity and coverage of the generated time series in a time
    series feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, these models describe the conditional distribution of time series
    by assuming the value at time $t$ depends on previous points. Once the initial
    value is perturbed, a new time series sequence could be generated following the
    conditional distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Learning-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time series data augmentation methods should be capable of not only generating
    diverse samples, but also mimicking the characteristics of real data. In this
    section, we summarize some recent learning based schemes that have such potentials.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Embedding Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In DeVries and Taylor ([2017](#bib.bib13)), the data augmentation is proposed
    to perform in the learned embedding space (aka., latent space). It assumes that
    simple transforms applied to encoded inputs rather than the raw inputs would produce
    more plausible synthetic data due to the manifold unfolding in feature space.
    Note that the selection of the representation model in this framework is open
    and depends on the specific task and data type. When the time series data is addressed,
    a sequence autoencoder is selected in DeVries and Taylor ([2017](#bib.bib13)).
    Specifically, the interpolation and extrapolation are applied to generate new
    samples. The first $k$ nearest labels in the transformed space with the same label
    are identified. Then for each pair of neighboring samples, a new sample is generated
    which is the linear combination of them. The difference of interpolation and extrapolation
    lies in the weight selection in sample generation. This technique is particular
    useful for time series classification as demonstrated in DeVries and Taylor ([2017](#bib.bib13)).
    Recently, another data augmentation method in the embedding space named MODALS
    (Modality-agnostic Automated Data Augmentation in the Latent Space) is proposed
    in  Cheung and Yeung ([2021](#bib.bib5)). Instead of training an autoencoder to
    learn the latent space and generate additional synthetic data for training, the
    MODALS method train a classification model jointly with different compositions
    of latent space augmentations, which demonstrates superior performance for time
    series classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Deep Generative Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep generative models (DGMs) have recently been shown to be able to generate
    near-realistic high-dimensional data objects such as images and sequences. DGMs
    developed for sequential data, such as audio and text, often can be extended to
    model time series data. Among DGMs, generative adversarial networks (GANs) are
    popular methods to generate synthetic samples and increase the training set effectively.
    Although the GAN frameworks have received significant attention in many fields,
    how to generate effective time series data still remains a challenging problem.
    In this subsection, we briefly review several recent works on GANs for time series
    data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: In Esteban et al. ([2017](#bib.bib14)), a Recurrent GAN (RGAN) and Recurrent
    Conditional GAN (RCGAN) are proposed to produce realistic real-valued multi-dimensional
    time series data. The RGAN adopts RNN in the generator and discriminator, while
    the RCGAN adopts both RNNs conditioned on auxiliary information. Besides desirable
    performance of RGAN and RCGAN for time series data augmentation, differential
    privacy can be used in training the RCGAN for stricter privacy guarantees like
    medicine or other sensitive domains. Recently, Yoon et al. ([2019](#bib.bib59))
    proposed TimeGAN, a natural framework for generating realistic time series data
    in various domains. TimeGAN is a generative time series model, trained adversarially
    and jointly via a learned embedding space with both supervised and unsupervised
    losses. Specifically, a stepwise supervised loss is introduced to learn the stepwise
    conditional distributions in data. It also introduces an embedding network to
    provide a reversible mapping between features and latent representations to reduce
    the high-dimensionality of the adversarial learning space. Note that the supervised
    loss is minimized by jointly training both the embedding and generator networks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3 Automated Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The idea of automated data augmentation is to automatically search for optimal
    data augmentation policies through reinforcement learning, meta learning, or evolutionary
    search  Ratner et al. ([2017](#bib.bib43)); Cubuk et al. ([2019](#bib.bib7));
    Zhang et al. ([2020](#bib.bib60)); Cheung and Yeung ([2021](#bib.bib5)). The TANDA
    (Transformation Adversarial Networks for Data Augmentations) scheme in Ratner
    et al. ([2017](#bib.bib43)) is designed to train a generative sequence model over
    specified transformation functions using reinforcement learning in a GAN-like
    framework to generate realistic transformed data points, which yields strong gains
    over common heuristic data augmentation methods for a range of applications including
    image recognition and natural language understanding tasks.  Cubuk et al. ([2019](#bib.bib7))
    proposes a procedure called AutoAugment to automatically search for improved data
    augmentation policies in a reinforcement learning framework. It adopts a controller
    RNN network to predicts an augmentation policy from the search space and another
    network is trained to achieve convergence accuracy. Then, the accuracy is used
    as reward to update the RNN controller for better policies in the next iteration.
    The experimental results show that AutoAugment improves the accuracy of modern
    image classifiers significantly in a wide range of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For time series data augmentation, the MODALS Cheung and Yeung ([2021](#bib.bib5))
    is designed to find the optimal composition of latent space transformations for
    data augmentation using evolution search strategy based on population based augmentation
    (PBA) Ho et al. ([2019](#bib.bib24)), which demonstrates superior performance
    on classification problems in continuous and discrete time series data. Another
    recent work on automated data augmentation is proposed in  Fons et al. ([2021](#bib.bib18)),
    where two sample-adaptive automatic weighting schemes are designed specifically
    for time series data: one learns to weight the contribution of the augmented samples
    to the loss, and the other selects a subset of transformations based on the ranking
    of the predicted training loss. Both adaptive policies demonstrate improvement
    on classification problems in multiple time series datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Preliminary Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we demonstrate preliminary evaluations in three common time
    series tasks to show the effectiveness of data augmentation for performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Time Series Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this experiment, we compare the classification performance with and without
    data augmentation. Specifically, we collect $5000$ time series of one-week long
    and 5-min interval samples with binary class labels (seasonal or non-seasonal)
    from Alibaba Cloud monitoring system. The data is randomly splitted into training
    and test sets where training contains $80\%$ of total samples. We train a fully
    convolutional network Wang et al. ([2017](#bib.bib53)) to classify each time series
    in the training set. In our experiment, we inject different types of outliers,
    including spike, step, and slope, into the test set to evaluate the robustness
    of the trained classifier. The data augmentations methods applied include cropping,
    warping, and flipping. Table [1](#S4.T1 "Table 1 ‣ 4.1 Time Series Classification
    ‣ 4 Preliminary Evaluation ‣ Time Series Data Augmentation for Deep Learning:
    A Survey") summarizes the accuracies with and without data augmentation when different
    types of outliers are injected into the test set. It can be observed that data
    augmentation leads to $0.1\%\sim 1.9\%$ accuracy improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Outlier injection | w/o aug | w/ aug | Improvement |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| spike | 96.26% | 96.37% | 0.11% |'
  prefs: []
  type: TYPE_TB
- en: '| step | 93.70% | 95.62% | 1.92% |'
  prefs: []
  type: TYPE_TB
- en: '| slope | 95.84% | 96.16% | 0.32% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Accuracy improvement from data augmentation under outlier injection
    in time series classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Time Series Anomaly Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the challenges of both *data scarcity* and *data imbalance* in time series
    anomaly detection, it is beneficial by adopting data augmentation to generate
    more labeled data. We briefly summarize the results in Gao et al. ([2020](#bib.bib21)),
    where a U-Net based network is designed and evaluated on public Yahoo! dataset Laptev
    et al. ([2015](#bib.bib33)) for time series anomaly detection. The performance
    comparison under different settings are summarized in Table [2](#S4.T2 "Table
    2 ‣ 4.2 Time Series Anomaly Detection ‣ 4 Preliminary Evaluation ‣ Time Series
    Data Augmentation for Deep Learning: A Survey"), including applying the model
    on the raw data (U-Net-Raw), on the decomposed residuals (U-Net-DeW), and on the
    residuals with data augmentation (U-Net-DeWA). The applied data augmentation methods
    include flipping, cropping, label expansion, and APP based augmentation in frequency
    domain. It can be observed that the decomposition helps the increase of the F1
    score and the data augmentation further boosts the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Precision | Recall | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| U-Net-Raw | 0.473 | 0.351 | 0.403 |'
  prefs: []
  type: TYPE_TB
- en: '| U-Net-DeW | 0.793 | 0.569 | 0.662 |'
  prefs: []
  type: TYPE_TB
- en: '| U-Net-DeWA (w/ aug) | 0.859 | 0.581 | 0.693 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Time series anomaly detection improvement from data augmentation based
    on precision, recall, and F1 score.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Time Series Forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this subsection we demonstrate the practical effectiveness of data augmentation
    in two popular deep models DeepAR Salinas et al. ([2019](#bib.bib46)) and Transformer
    Vaswani et al. ([2017](#bib.bib52)). In Table [3](#S4.T3 "Table 3 ‣ 4.3 Time Series
    Forecasting ‣ 4 Preliminary Evaluation ‣ Time Series Data Augmentation for Deep
    Learning: A Survey"), we report the performance improvement on mean absolute scaled
    error (MASE) on several public datasets: electricity and traffic from UCI Learning
    Repository²²2[http://archive.ics.uci.edu/ml/datasets.php](http://archive.ics.uci.edu/ml/datasets.php)
    and 3 datasets from the M4 competition³³3[https://github.com/Mcompetitions/M4-methods/tree/master/Dataset](https://github.com/Mcompetitions/M4-methods/tree/master/Dataset).
    We consider the basic augmentation methods including cropping, warping, flipping,
    and APP based augmentation in frequency domain. In Table [3](#S4.T3 "Table 3 ‣
    4.3 Time Series Forecasting ‣ 4 Preliminary Evaluation ‣ Time Series Data Augmentation
    for Deep Learning: A Survey"), we summarize average MASE without augmentation,
    with augmentation and average relative improvement (ARI) which is computed as
    the mean of $(\textrm{MASE}_{\textrm{w/o aug}}-\textrm{MASE}_{\textrm{w aug}})/\textrm{MASE}_{\textrm{w
    aug}}$. We observe that the data augmentation methods bring promising results
    for all models in average sense. However, the negative results can still be observed
    for specific data/model pairs. As a future work, it motivates us to search for
    advanced automated data augmentation policies that stabilize the influence of
    data augmentation specifically for time series forecasting.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | DeepAR | Transformer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| w/o aug | w/ aug | ARI | w/o aug | w/ aug | ARI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| electricity | $0.87$ | $0.97$ | $1.92\%$ | $1.04$ | $1.11$ | $-2\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| traffic | $0.66$ | $0.80$ | $-12\%$ | $0.70$ | $0.91$ | $-16\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| m4-hourly | $6.33$ | $5.35$ | $56\%$ | $7.77$ | $7.87$ | $38\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| m4-daily | $4.88$ | $4.48$ | $10\%$ | $7.85$ | $7.38$ | $37\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| m4-weekly | $12.00$ | $9.34$ | $76\%$ | $6.62$ | $7.09$ | $23\%$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Time seires forecasting improvement from data augmentation based on
    MASE.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion for Future Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Augmentation in Time-Frequency Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in Section [2.3](#S2.SS3 "2.3 Time-Frequency Domain ‣ 2 Basic
    Data Augmentation Methods ‣ Time Series Data Augmentation for Deep Learning: A
    Survey"), so far there are only limited studies of time series data augmentation
    methods based on STFT in the time-frequency domain. Besides STFT, wavelet transform
    and its variants including continuous wavelet transform (CWT) and discrete wavelet
    transform (DWT), are another family of adaptive time–frequency domain analysis
    methods to characterize time-varying properties of time series. Compared to STFT,
    they can handle non-stationary time series and non-Gaussian noises more effectively
    and robustly. Among many wavelet transform variants, maximum overlap discrete
    wavelet transform (MODWT) is especially attractive for time series analysis Percival
    and Walden ([2000](#bib.bib40)); Wen et al. ([2021](#bib.bib58)) due to the following
    advantages: 1) more computationally efficiency compared to CWT; 2) ability to
    handle any time series length; 3) increased resolution at coarser scales compared
    with DWT. MODWT based surrogate time series have been proposed in Keylock ([2006](#bib.bib30)),
    where wavelet iterative amplitude adjusted Fourier transform (WIAAFT) is designed
    by combining the iterative amplitude adjusted Fourier transform (IAAFT) scheme
    to each level of MODWT coefficients. In contrast to IAAFT, WIAAFT does not assume
    sationarity and can roughly maintain the shape of the original data in terms of
    the temporal evolution. Besides WIAAFT, we can also consider the perturbation
    of both amplitude spectrum and phase spectrum as Gao et al. ([2020](#bib.bib21))
    at each level of MODWT coefficients as a data augmentation scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: It would be an interesting future direction to investigate how to exploit different
    wavelet transforms (CWT, DWT, MODWT, etc.) for an effective time-frequency domain
    based time series data augmentation in deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Augmentation for Imbalanced Class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In time series classification, class imbalance occurs very frequently. One classical
    approach addressing imbalanced classification problem is to oversample the minority
    class as the synthetic minority oversampling technique (SMOTE) Fernández et al.
    ([2018](#bib.bib17)) to artificially mitigate the imbalance. However, this oversampling
    strategy may change the distribution of raw data and cause overfitting. Another
    approach is to design cost-sensitive model by using adjust loss function Geng
    and Luo ([2018](#bib.bib22)). Furthermore, Gao et al. ([2020](#bib.bib21)) designed
    label-based weight and value-based weight in the loss function in convolution
    neural networks, which considers weight adjustment for class labels and the neighborhood
    of each sample. Thus, both class imbalance and temporal dependency are explicitly
    considered.
  prefs: []
  type: TYPE_NORMAL
- en: Performing data augmentation and weighting for imbalanced class together would
    be an interesting and effective direction. A recent study investigates this topic
    in the area of CV and NLP Hu et al. ([2019](#bib.bib25)), which significantly
    improves text and image classification in low data regime and imbalanced class
    problems. In future, it is interesting to design deep network by jointly considering
    data augmentation and weighting for imbalanced class in time series data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Augmentation Selection and Combination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given different data augmentation methods summarized in Fig. [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Time Series Data Augmentation for Deep Learning: A Survey"),
    one key strategy is how to select and combine various augmentation methods together.
    The experiments in Um et al. ([2017](#bib.bib51)) show that the combination of
    three basic time-domain methods (permutation, rotation, and time warping) is better
    than that of a single method and achieves the best performance in time series
    classification. Also, the results in Rashid and Louis ([2019](#bib.bib41)) demonstrate
    substantial performance improvement for a time series classification task when
    using a deep neural network by combining four data augmentation methods (i.e,
    jittering, scaling, rotation and time-warping). However, considering various data
    augmentation methods, directly combining different augmentations may result in
    a huge amount of data, and may not be efficient and effective for performance
    improvement. Recently, RandAugment Cubuk et al. ([2020](#bib.bib8)) is proposed
    as a practical way for augmentation combination in image classification and object
    detection. For each random generated dataset, RandAugment is based on only two
    interpretable hyperparameters $N$ (number of augmentation methods to combine)
    and $M$ (magnitude for all augmentation methods), where each augmentation is randomly
    selected from $K$=14 available augmentation methods. Furthermore, this randomly
    combined augmentation with simple grid search can be used in the reinforcement
    learning based data augmentation as  Cubuk et al. ([2019](#bib.bib7)) for efficient
    space searching.'
  prefs: []
  type: TYPE_NORMAL
- en: An interesting future direction is how to design effective augmentation selection
    and/or combination strategies suitable for time series data in deep learning.
    Customized reinforcement learning and meta learning optimized for time series
    could be potential approaches. Furthermore, algorithm efficiency is another important
    consideration in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Augmentation with Gaussian Processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gaussian Processes (GPs) Rasmussen and Williams ([2005](#bib.bib42)) are well-known
    Bayesian non-parametric models suitable for time series analysis Roberts et al.
    ([2013](#bib.bib44)). From the function-space view, GPs induce a distribution
    over functions, i.e., a stochastic process. Time series can be viewed as functions
    with time as input and observation as output, and thus can be modeled with GPs.
    A GP $f(t)\sim\mathcal{GP}(m(t),k(t,t^{\prime}))$ is characterized by a mean function
    $m(t)$ and a covariance kernel function $k(t,t^{\prime})$. The choice of the kernel
    allows to place assumptions on some general properties of the modeled functions,
    such as smoothness, scale, periodicity and noise level. Kernels can be composed
    through addition and multiplication, resulting in compositional function properties,
    such as pseudo-periodicity, additive decomposability, and change point. GPs are
    often applied to interpolation and extrapolation tasks, which correspond to imputation
    and forecasting in time series analysis. Furthermore, deep Gaussian processes(DGPs) Damianou
    and Lawrence ([2013](#bib.bib11)); Salimbeni and Deisenroth ([2017](#bib.bib45)),
    which are richer models with hierarchical composition of GPs and often exceed
    standard (single-layer) GPs significantly in many cases, have not been well studied
    for time series. We believe GPs and DGPs are future directions as they allow to
    sample time series with those properties mentioned above through the design of
    kernels, and to generate new data instances from existing ones by exploiting their
    interpolation/extrapolation abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Augmentation with Deep Generative Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current DGMs adopted for time series data augmentation are mainly GANs. However,
    other DGMs also have great potentials for time series modeling. For example, deep
    autoregressive networks (DARNs) exhibit a natural fit for time series because
    they generate data in a sequential manner, obeying the causal direction of physical
    time series data generating process. DARNs like Wavenet Oord et al. ([2016](#bib.bib38))
    and Transformer Vaswani et al. ([2017](#bib.bib52)) have demonstrated promising
    performance in time series forecasting tasks Alexandrov et al. ([2020](#bib.bib1)).
    Another example is normalizing flows (NFs) Kobyzev et al. ([2020](#bib.bib31)),
    which recently have shown success in modeling time series stochastic processes
    with excellent inter-/extrapolation performance given observed data Deng et al.
    ([2020](#bib.bib12)). Most recently, variational autoencoders (VAEs) based data
    augmentation Fu et al. ([2020](#bib.bib19)) are investigated for human activity
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, besides the common GAN architectures, how to leverage other deep
    generative models like DARNs, NFs, and VAEs, which are less investigated for time
    series data augmentation, remain exciting future opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As deep learning models are becoming more popular on time series data, the limited
    labeled data calls for effective data augmentation methods. In this paper, we
    give a comprehensive survey on time series data augmentation methods in various
    tasks. We organize the reviewed methods in a taxonomy consisting of basic and
    advanced approaches, summarize representative methods in each category, compare
    them empirically in typical tasks, and highlight future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alexandrov et al. [2020] Alexander Alexandrov, Konstantinos Benidis, Michael
    Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C
    Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, et al. Gluonts: Probabilistic
    and neural time series modeling in python. Journal of Machine Learning Research,
    21(116):1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bandara et al. [2020] Kasun Bandara, Hansika Hewamalage, Yuan-Hao Liu, Yanfei
    Kang, and Christoph Bergmeir. Improving the accuracy of global forecasting models
    using time series data augmentation. arXiv preprint arXiv:2008.02663, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergmeir et al. [2016] Christoph Bergmeir, Rob J. Hyndman, and José M. Benítez.
    Bagging exponential smoothing methods using STL decomposition and Box–Cox transformation.
    International Journal of Forecasting, 32(2):303–312, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. [2014] Hong Cao, Vincent YF Tan, and John ZF Pang. A parsimonious
    mixture of gaussian trees model for oversampling in imbalanced and multimodal
    time-series classification. IEEE TNNLS, 25(12):2226–2239, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheung and Yeung [2021] Tsz-Him Cheung and Dit-Yan Yeung. MODALS: Modality-agnostic
    automated data augmentation in the latent space. In International Conference on
    Learning Representations (ICLR), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cleveland et al. [1990] Robert B Cleveland, William S Cleveland, Jean E McRae,
    and Irma Terpenning. STL: A seasonal-trend decomposition procedure based on loess.
    Journal of Official Statistics, 6(1):3–73, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cubuk et al. [2019] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,
    and Quoc V. Le. AutoAugment: Learning augmentation strategies from data. In IEEE
    CVPR 2019, pages 113–123, June 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cubuk et al. [2020] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
    Le. RandAugment: Practical automated data augmentation with a reduced search space.
    In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
    pages 3008–3017, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui et al. [2015] X Cui, V Goel, and B Kingsbury. Data augmentation for deep
    neural network acoustic modeling. IEEE/ACM TASLP, 23(9):1469–1477, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui et al. [2016] Zhicheng Cui, Wenlin Chen, et al. Multi-scale convolutional
    neural networks for time series classification. arXiv preprint arXiv:1603.06995,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Damianou and Lawrence [2013] Andreas Damianou and Neil D Lawrence. Deep gaussian
    processes. In Artificial intelligence and statistics, pages 207–215. PMLR, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. [2020] Ruizhi Deng, Bo Chang, Marcus A Brubaker, Greg Mori, and
    Andreas Lehrmann. Modeling continuous stochastic processes with dynamic normalizing
    flows. In NeurIPS 2020, Dec 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeVries and Taylor [2017] Terrance DeVries and Graham W. Taylor. Dataset augmentation
    in feature space. In ICLR 2017, pages 1–12, Toulon, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esteban et al. [2017] Cristóbal Esteban, Stephanie L Hyland, and Gunnar Rätsch.
    Real-valued (medical) time series generation with recurrent conditional gans.
    arXiv preprint arXiv:1706.02633, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fawaz et al. [2018] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber,
    Lhassane Idoumghar, and Pierre-Alain Muller. Data augmentation using synthetic
    data for time series classification with deep residual networks. In ECML/PKDD
    Workshop on AALTD, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fawaz et al. [2019] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber,
    et al. Deep learning for time series classification: a review. Data Mining and
    Knowledge Discovery, 33(4):917–963, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fernández et al. [2018] Alberto Fernández, Salvador Garcia, Francisco Herrera,
    and Nitesh V Chawla. SMOTE for learning from imbalanced data: progress and challenges,
    marking the 15-year anniversary. Journal of Artificial Intelligence Research,
    61:863–905, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fons et al. [2021] Elizabeth Fons, Paula Dawson, Xiao-jun Zeng, John Keane,
    and Alexandros Iosifidis. Adaptive weighting scheme for automatic time-series
    data augmentation. arXiv preprint arXiv:2102.08310, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2020] Biying Fu, Florian Kirchbuchner, and Arjan Kuijper. Data augmentation
    for time series: traditional vs generative models on capacitive proximity time
    series. In ACM PETRA, pages 1–10, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamboa [2017] John Cristian Borges Gamboa. Deep learning for time-series analysis.
    arXiv preprint arXiv:1701.01887, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2020] Jingkun Gao, Xiaomin Song, Qingsong Wen, Pichao Wang, Liang
    Sun, and Huan Xu. Robusttad: Robust time series anomaly detection via decomposition
    and convolutional neural networks. MileTS’20: 6th KDD Workshop on Mining and Learning
    from Time Series, pages 1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geng and Luo [2018] Yue Geng and Xinyu Luo. Cost-sensitive convolution based
    neural networks for imbalanced time-series classification. arXiv preprint arXiv:1801.04396,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. [2019] Z Han, J Zhao, H Leung, K F Ma, and W Wang. A review of deep
    learning models for time series prediction. IEEE Sensors Journal, page 1, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ho et al. [2019] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel.
    Population based augmentation: Efficient learning of augmentation policy schedules.
    In International Conference on Machine Learning (ICML), pages 2731–2741, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2019] Zhiting Hu, Bowen Tan, Russ R Salakhutdinov, Tom M Mitchell,
    and Eric P Xing. Learning data manipulation for augmentation and weighting. In
    NeurIPS 2019, pages 15764–15775, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2020] Hailin Hu, MingJian Tang, and Chengcheng Bai. Datsing: Data
    augmented time series forecasting with adversarial domain adaptation. In Proceedings
    of the 29th ACM International Conference on Information & Knowledge Management,
    pages 2061–2064, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iwana and Uchida [2020] Brian Kenji Iwana and Seiichi Uchida. An empirical survey
    of data augmentation for time series classification with neural networks. arXiv
    preprint arXiv:2007.15951, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. [2020] Yanfei Kang, Rob J Hyndman, and Feng Li. GRATIS: Generating
    time series with diverse and controllable characteristics. Statistical Analysis
    and Data Mining: The ASA Data Science Journal, 13(4):354–376, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kegel et al. [2018] Lars Kegel, Martin Hahmann, and Wolfgang Lehner. Feature-based
    comparison and generation of time series. In SSDBM 2018, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keylock [2006] C J Keylock. Constrained surrogate time series with preservation
    of the mean and variance structure. Phys. Rev. E, 73(3):36707, Mar 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kobyzev et al. [2020] Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing
    flows: An introduction and review of current methods. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, pages 1–17, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. [2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    Imagenet classification with deep convolutional neural networks. In NeurIPS 2012,
    pages 1097–1105, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laptev et al. [2015] Nikolay Laptev, Saeed Amizadeh, et al. Generic and scalable
    framework for automated time-series anomaly detection. KDD, pages 1939–1947, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le Guennec et al. [2016] Arthur Le Guennec, Simon Malinowski, and Romain Tavenard.
    Data augmentation for time series classification using convolutional neural networks.
    In ECML/PKDD Workshop on AALTD, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee and Kim [2020] Si Woon Lee and Ha Young Kim. Stock market forecasting with
    super-high dimensional time-series data using convlstm, trend sampling, and specialized
    data augmentation. Expert Systems with Applications, 161:113704, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2019] Tracey Kah-Mein Lee, YL Kuah, Kee-Hao Leo, Saeid Sanei, Effie
    Chew, and Ling Zhao. Surrogate rehabilitative time series data for image-based
    deep learning. In EUSIPCO 2019, pages 1–5, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lim et al. [2018] Swee Kiat Lim, Yi Loo, Ngoc-Trung Tran, Ngai-Man Cheung,
    Gemma Roig, and Yuval Elovici. DOPING: Generative data augmentation for unsupervised
    anomaly detection with gan. In 2018 IEEE International Conference on Data Mining
    (ICDM), pages 1122–1127\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oord et al. [2016] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan,
    Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
    Wavenet: A generative model for raw audio. In International Conference on Learning
    Representations, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2019] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
    Barret Zoph, Ekin D Cubuk, et al. SpecAugment: A simple data augmentation method
    for automatic speech recognition. In INTERSPEECH 2019, pages 2613–2617, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percival and Walden [2000] Donald B Percival and Andrew T Walden. Wavelet methods
    for time series analysis, volume 4. Cambridge university press, New York, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rashid and Louis [2019] Khandakar M Rashid and Joseph Louis. Times-series data
    augmentation and deep learning for construction equipment activity recognition.
    Advanced Engineering Informatics, 42:100944, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rasmussen and Williams [2005] Carl Edward Rasmussen and Christopher KI Williams.
    Gaussian Processes for Machine Learning. The MIT Press, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ratner et al. [2017] Alexander J Ratner, Henry R Ehrenberg, Zeshan Hussain,
    Jared Dunnmon, and Christopher Ré. Learning to compose domain-specific transformations
    for data augmentation. NeurIPS 2017, 30:3239, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roberts et al. [2013] Stephen Roberts, Michael Osborne, Mark Ebden, Steven
    Reece, Neale Gibson, and Suzanne Aigrain. Gaussian processes for time-series modelling.
    Philosophical Transactions of the Royal Society A: Mathematical, Physical and
    Engineering Sciences, 371(1984):20110550, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salimbeni and Deisenroth [2017] Hugh Salimbeni and Marc Peter Deisenroth. Doubly
    stochastic variational inference for deep gaussian processes. In NeurIPS 2017,
    pages 4591–4602, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salinas et al. [2019] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim
    Januschowski. DeepAR: Probabilistic forecasting with autoregressive recurrent
    networks. International Journal of Forecasting, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schreiber and Schmitz [2000] Thomas Schreiber and Andreas Schmitz. Surrogate
    time series. Physica D: Nonlinear Phenomena, 142(3):346–382, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shorten and Khoshgoftaar [2019] Connor Shorten and Taghi M Khoshgoftaar. A survey
    on image data augmentation for deep learning. Journal of Big Data, 6(1):60, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smyl and Kuber [2016] Slawek Smyl and Karthik Kuber. Data preprocessing and
    augmentation for multiple short time series forecasting with recurrent neural
    networks. In 36th International Symposium on Forecasting, June 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steven Eyobu and Han [2018] Odongo Steven Eyobu and Dong Seog Han. Feature representation
    and data augmentation for human activity classification based on wearable IMU
    sensor data using a deep LSTM neural network. Sensors, 18(9):2892, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Um et al. [2017] Terry T Um, Franz M J Pfister, Daniel Pichler, Satoshi Endo,
    Muriel Lang, Sandra Hirche, Urban Fietzek, and Dana Kulić. Data augmentation of
    wearable sensor data for Parkinson’s disease monitoring using convolutional neural
    networks. In ACM ICMI 2017, pages 216–220, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, et al. Attention is all you need. In NeurIPS 2017, pages 5998–6008,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2017] Zhiguang Wang, Weizhong Yan, and Tim Oates. Time series
    classification from scratch with deep neural networks: A strong baseline. In IJCNN,
    pages 1578–1585, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen and Keyes [2019] Tailai Wen and Roy Keyes. Time series anomaly detection
    using convolutional neural networks and transfer learning. In IJCAI Workshop on
    AI4IoT, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. [2019a] Qingsong Wen, Jingkun Gao, Xiaomin Song, Liang Sun, and
    Jian Tan. RobustTrend: A Huber loss with a combined first and second order difference
    regularization for time series trend filtering. In IJCAI, pages 3856–3862, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. [2019b] Qingsong Wen, Jingkun Gao, Xiaomin Song, Liang Sun, Huan
    Xu, and Shenghuo Zhu. RobustSTL: A robust seasonal-trend decomposition algorithm
    for long time series. In AAAI, volume 33, pages 5409–5416, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. [2020] Qingsong Wen, Zhe Zhang, Yan Li, and Liang Sun. Fast RobustSTL:
    Efficient and robust seasonal-trend decomposition for time series with complex
    patterns. In KDD, pages 2203–2213, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. [2021] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke,
    and Huan Xu. RobustPeriod: Time-frequency mining for robust multiple periodicities
    detection. In International Conference on Management of Data (SIGMOD), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoon et al. [2019] Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar.
    Time-series generative adversarial networks. In NeurIPS 2019, pages 5508–5518,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial
    autoaugment. In International Conference on Learning Representations (ICLR), 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2019] Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, and Jing
    Ye. Beatgan: Anomalous rhythm detection using adversarially generated time series.
    In IJCAI, pages 4433–4439, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
