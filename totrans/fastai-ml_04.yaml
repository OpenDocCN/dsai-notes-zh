- en: 'Machine Learning 1: Lesson 4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第4课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d)'
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自* [*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*
    的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢* [*Jeremy*](https://twitter.com/jeremyphoward)
    *和* [*Rachel*](https://twitter.com/math_rachel) *给了我这个学习的机会。*'
- en: '**A question before getting started:** Could we summarize the relationship
    between the hyper parameters of the random forest and its effect on overfitting,
    dealing with collinearity, etc?[[1:51](https://youtu.be/0v93qHDqq_g?t=1m51s)]
    Absolutely. Going back to the [lesson 1 notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson1-rf.ipynb).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前有一个问题：我们能否总结随机森林的超参数与过拟合、处理共线性等之间的关系？绝对可以。回到[第1课笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson1-rf.ipynb)。
- en: 'Hyper parameters of interest:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的超参数：
- en: '**1.** `set_rf_samples`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 设置_rf_samples
- en: Determines how many rows are in each tree. So before we start a new tree, we
    either bootstrap a sample (i.e. sampling with replacement from the whole thing)
    or we pull out a subsample of a smaller number of rows and then we build a tree
    from there.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定每棵树中有多少行。因此，在我们开始新树之前，我们要么对整个数据进行自助抽样（即有放回地抽样），要么从中抽取较少行数的子样本，然后从中构建一棵树。
- en: Step 1 is we have our whole big dataset, we grab a few rows at random from it,
    and we turn them into a smaller dataset. From that, we build a tree.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步是我们有一个完整的大数据集，我们随机抽取几行数据，并将它们转换成一个较小的数据集。然后，我们构建一棵树。
- en: '![](../Images/fc76565056f12bb5bb46993090bf0ba6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法描述图片中的内容。如果您可以提供文本描述，我将很乐意帮助翻译。
- en: Assuming that the tree remains balanced as we grow it, how many layers deep
    will this tree be (assuming we are growing it until every leaf is of size one)?
    log2(20000). The depth of the tree doesn’t actually vary that much depending on
    the number of samples because it is related to the log of the size.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设树在我们生长过程中保持平衡，这棵树将有多少层深（假设我们生长到每个叶子的大小为一）？log2(20000)。树的深度实际上并不会因为样本数量的不同而变化太大，因为它与大小的对数相关。
- en: Once we go all the way down to the bottom, how many leaf nodes would there be?
    20K. We have a linear relationship between the number of leaf nodes and the size
    of the sample. So when you decrease the sample size, there are less final decisions
    that can be made. Therefore, the tree is going to be less rich in terms of what
    it can predict because it is making less different individual decisions and it
    also is making less binary choices to get to those decisions.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们一直走到底部时，会有多少叶节点？20K。叶节点的数量与样本大小之间存在线性关系。因此，当你减少样本大小时，可以做出的最终决策就会减少。因此，树在预测方面会变得不那么丰富，因为它做出的个别决策更少，也做出更少的二元选择来达到这些决策。
- en: Setting RF samples lower is going to mean that you overfit less, but it also
    means that you are going to have a less accurate individual tree model. The way
    Breiman, the inventor of random forest, described this is that you are trying
    to do two things when you build a model with bagging. One is that each individual
    tree/estimator is as accurate as possible (so each model is a strong predictive
    model). But then the across the estimators, correlation between them is as low
    as possible sot hat when you average them out together, you end up with something
    that generalizes. By decreasing the `set_rf_samples` number, we are actually decreasing
    the power of the estimator and increasing the correlation — so is that going to
    result in a better or worse validation set result for you? It depends. This is
    the kind of compromise which you have to figure out when you do machine learning
    models.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RF样本设置较低意味着过拟合的可能性较小，但也意味着每个单独的树模型的准确性会降低。随机森林的发明者Breiman描述了这一点，即在使用装袋法构建模型时，你要做两件事情。一是确保每个单独的树/估计器尽可能准确（因此每个模型都是一个强预测模型）。但是在估计器之间，相关性要尽可能低，这样当将它们平均在一起时，你会得到一个泛化的模型。通过降低`set_rf_samples`的数量，实际上是降低了估计器的能力并增加了相关性，那么这会对你的验证集结果产生更好还是更差的影响呢？这取决于情况。这就是在进行机器学习模型时必须要考虑的妥协。
- en: A question about `oob=True` [[6:46](https://youtu.be/0v93qHDqq_g?t=6m46s)].
    All `oob=True` does is it says whatever your subsample is (it might be a bootstrap
    sample or a subsample), take all of the other rows (for each tree), put them into
    a different data set, and calculate the error on those. So it doesn’t actually
    impact training at all. It just gives you an additional metric which is the OOB
    error. So if you don’t have a validation set, then this allows you to get kind
    of a quasi validation set for free.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`oob=True`的问题[[6:46](https://youtu.be/0v93qHDqq_g?t=6m46s)]。`oob=True`的作用就是说，无论你的子样本是什么（可能是一个自助采样或一个子样本），将所有其他行（对于每棵树）放入一个不同的数据集中，并计算这些行的错误。因此，它实际上并不影响训练。它只是给你一个额外的度量，即OOB错误。因此，如果你没有验证集，那么这允许你免费获得一种准验证集。
- en: 'Question: If I don’t do `set_rf_samples`, what would it be called? [[7:55](https://youtu.be/0v93qHDqq_g?t=7m55s)]
    The default is, if you say `reset_rf_samples`, that causes it to bootstrap, so
    it will sample a new dataset as big as the original one but with replacement.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：如果我不执行`set_rf_samples`，那会被称为什么？默认情况是，如果你说`reset_rf_samples`，那会导致引导，因此它将对原始数据集进行重新采样，但是会有替换。
- en: The second benefit of `set_rf_samples` is that you can run more quickly [[8:28](https://youtu.be/0v93qHDqq_g?t=8m28s)].
    Particularly if you are running on a really large dataset like a hundred million
    rows, it will not be possible to run it on the full dataset. So you would either
    have to pick a subsample yourself before you start or you `set_rf_samples`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`set_rf_samples`的第二个好处是你可以更快地运行。特别是当你在一个非常庞大的数据集上运行，比如一亿行，就不可能在完整的数据集上运行。所以你要么在开始之前自己选择一个子样本，要么使用`set_rf_samples`。'
- en: '**2.** `min_samples_leaf` [[8:48](https://youtu.be/0v93qHDqq_g?t=8m48s)]'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2. `min_samples_leaf` [[8:48](https://youtu.be/0v93qHDqq_g?t=8m48s)]
- en: Before, we assumed that `min_samples_leaf=1`, if it is set to 2, the new depth
    of the tree is `log2(20000)-1`. Each time we double the `min_samples_leaf` , we
    are removing one layer from the tree, and halving the number of leaf nodes (i.e.
    10k). The result of increasing `min_samples_leaf` is that now each of our leaf
    nodes has more than one thing in, so we are going to get a more stable average
    that we are calculating in each tree. We have a little less depth (i.e. we have
    less decisions to make) and we have a smaller number of leaf nodes. So again,
    we would expect the result of that node would be that each estimator would be
    less predictive, but the estimators would be also less correlated. So this might
    help us avoid overfitting.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们假设`min_samples_leaf=1`，如果设置为2，树的新深度为`log2(20000)-1`。每次将`min_samples_leaf`加倍，我们都会从树中移除一层，并将叶节点数量减半（即10k）。增加`min_samples_leaf`的结果是现在每个叶节点中都有多于一个元素，因此我们在每棵树中计算的平均值会更加稳定。我们的深度稍微减少（即我们需要做出的决策更少），叶节点数量也减少。因此，我们预期每个估算器的结果会更少预测性，但估算器之间的相关性也会减少。这可能有助于我们避免过拟合。
- en: '**Question**: I am not sure if every leaf node will have exactly two nodes
    [[10:33](https://youtu.be/0v93qHDqq_g?t=10m33s)]. No, it won’t necessarily have
    exactly two. The example of uneven split such as a leaf node containing 100 items
    is when they are all the same in terms of the dependent variable (suppose either,
    but much more likely would be the dependent). So if you get to a leaf node where
    every single one of them has the same auction price, or in classification every
    single one of them is a dog, then there is no split that you can do that’s going
    to improve your information. Remember, **information** is the term we use in a
    general sense in random forest to describe the amount of difference about the
    additional information we create from a split is how much we are improving the
    model. So you will often see this word information gain which means how much better
    the model got by adding an additional split point, and it could be based on RMSE
    or cross-entropy or how different to the standard deviations, etc.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我不确定每个叶节点是否一定会有两个节点。不，不一定会有两个。不均匀分裂的例子，比如一个叶节点包含100个项目，当它们在因变量方面都相同时（假设是这样，但更有可能是因变量）。所以，如果你到达一个叶节点，每一个都有相同的拍卖价格，或者在分类中每一个都是一只狗，那么你无法进行任何可以改善你的信息的分裂。记住，“信息”是我们在随机森林中使用的一个术语，用来描述我们从分裂中创造的额外信息的差异量，我们通过分裂改善模型的程度。所以你经常会看到这个词“信息增益”，意思是通过添加额外的分裂点，模型变得更好了多少，这可能基于RMSE或交叉熵或与标准差的差异等。
- en: So that is the second thing that we can do. It’s going to speed up our training
    because it has one less set of decisions to make. Even though there is one less
    set of decisions, those decisions have as much data as the previous set. So each
    layer of the tree can take twice as long as the previous layer. So it could definitely
    speed up training and generalize better.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们可以做的第二件事情。这将加快我们的训练速度，因为它少了一组决策要做。尽管少了一组决策，但这些决策的数据量与之前的一样多。因此，树的每一层可能比前一层花费的时间多一倍。因此，它肯定可以加快训练速度并且泛化得更好。
- en: '**3.** `max_features` [[12:22](https://youtu.be/0v93qHDqq_g?t=12m22s)]'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** `max_features` [[12:22](https://youtu.be/0v93qHDqq_g?t=12m22s)]'
- en: At each split, it will randomly sample columns (as opposed to `set_rf_samples`
    pick a subset of rows for each tree). It sounds like a small difference but it’s
    actually quite a different way of thinking about it. We do `set_rf_samples` so
    we pull out our sub sample or a bootstrap sample and that’s kept for the whole
    tree and we have all of the columns in there. With `max_features=0.5`, at each
    split, we’d pick a different half of the features. The reason we do that is because
    we want the trees to be as rich as possible. Particularly, if you were only doing
    a small number of trees (e.g. 10 trees) and you picked the same column set all
    the way through the tree, you are not really getting much variety in what kind
    of things it can find. So this way, at least in theory, seems to be something
    which is going to give us a better set of trees by picking a different random
    subset of features at every decision point.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次分裂时，它会随机抽样列（与`set_rf_samples`选择每棵树的行子集相对）。听起来是一个小的区别，但实际上这是一种完全不同的思考方式。我们使用`set_rf_samples`来提取我们的子样本或自举样本，并将其保留整个树中，其中包含所有列。使用`max_features=0.5`，在每次分裂时，我们会选择不同的一半特征。我们这样做的原因是因为我们希望树尽可能丰富。特别是，如果您只做了少量的树（例如10棵树），并且在整个树中选择了相同的列集，那么您实际上并没有获得太多不同种类的发现。因此，这种方式，至少在理论上，似乎会通过在每个决策点处选择不同的随机特征子集来给我们提供更好的树集。
- en: The overall effect of the max_features is the same — it’s going to mean that
    each individual tree is probably going to be less accurate but the trees are going
    to be more varied. In particular, here this can be critical because imagine that
    you got one feature that is just super predictive. It’s so predictive that every
    random subsample you look at always starts out by splitting on that same feature
    then the trees are going to be very similar in the sense they all have the same
    initial split. But there may be some other interesting initial splits because
    they create different interactions of variables. So by half the time that feature
    won’t even be available at the top of the tree, at least half the tree are going
    to have a different initial split. It definitely can give us more variation and
    therefore it can help us to create more generalized trees that have less correlation
    with each other even though the individual trees probably won’t be as predictive.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: max_features的整体效果是相同的 - 这意味着每棵单独的树可能会更不准确，但树的变化会更多。特别是在这里，这可能是至关重要的，因为想象一下，你有一个特征是非常具有预测性的。它是如此具有预测性，以至于你查看的每个随机子样本总是从相同的特征开始分裂，那么这些树在某种意义上将非常相似，因为它们都具有相同的初始分裂。但可能会有一些其他有趣的初始分裂，因为它们会创建不同的变量交互。因此，有一半的时间该特征甚至不会出现在树的顶部，至少有一半的树会有不同的初始分裂。这绝对可以给我们更多的变化，因此可以帮助我们创建更具一般性的树，这些树之间的相关性更小，即使单独的树可能不会那么具有预测性。
- en: '![](../Images/adfc1111b4f4884bc85204c6e99302e6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adfc1111b4f4884bc85204c6e99302e6.png)'
- en: In practice, as you add more trees, if you have `max_features=None`, that is
    going to use all the features every time. Then with very few trees, that can still
    give you a pretty good error. But as you create more trees, it’s not going to
    help as much because they are all pretty similar as they are all trying every
    single variable. Where else, if you say `max_features=sqrt` or `log2` , then as
    we add more estimators, we see improvements so there is an interesting interaction
    between those two. The chart above is from scikit-learn docs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，当你添加更多的树时，如果你设置`max_features=None`，那么每次都会使用所有的特征。然后在很少的树的情况下，这仍然可以给你一个相当不错的误差。但是随着你创建更多的树，它不会帮助太多，因为它们都很相似，它们都在尝试每一个变量。另外，如果你设置`max_features=sqrt`或`log2`，那么随着我们添加更多的估计器，我们会看到改进，所以这两者之间存在有趣的互动。上面的图表来自scikit-learn文档。
- en: '**4\.** Things which do not impact our training at all [[16:32](https://youtu.be/0v93qHDqq_g?t=16m32s)]'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\.** 完全不影响我们训练的事情'
- en: '`n_jobs`: simply specifies how many CPU or cores we run on, so it’ll make it
    faster up to a point. Generally speaking, making this more than 8 or so, they
    may have diminishing returns. -1 says use all of your cores. It seems weird that
    the default is to use one core. You will definitely get more performance by using
    more cores because all of you have computers with more than one core nowadays.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_jobs`：简单地指定我们运行在多少个 CPU 或核心上，因此在某种程度上会使其更快。一般来说，将其设置为超过 8 个左右，可能会有递减的回报。-1
    表示使用所有核心。默认使用一个核心似乎有点奇怪。通过使用更多核心，您肯定会获得更好的性能，因为现在大多数计算机都有多个核心。'
- en: '`oob_score=True`: This simply allows us to see OOB score. If you had set_rf_samples
    pretty small compared to a big dataset, OOB is going to take forever to calculate.
    Hopefully at some point, we will be able to fix the library so that doesn’t happen.
    There is no reason that need to be that way, but right now, that’s how the library
    works.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`oob_score=True`: 这只是让我们看到OOB得分。如果你将set_rf_samples设置得相对较小，而数据集很大，OOB将需要很长时间来计算。希望在某个时候，我们能够修复库，使其不再发生这种情况。没有理由需要那样，但目前，库就是这样工作的。'
- en: 'So they are our key basic parameters we can change [[17:38](https://youtu.be/0v93qHDqq_g?t=17m38s)].
    There are more that you can see in the docs or `shift+tab` to have a look at them,
    but the ones you’ve seen are the ones that I’ve found useful to play with so feel
    free to play with others as well. Generally speaking, these values work well:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们是我们可以更改的关键基本参数。您可以在文档中查看更多内容，或者按`shift+tab`查看它们，但您已经看到的是我发现有用的，可以随意尝试其他参数。一般来说，这些值效果很好。
- en: '`max_features`: None, 0.5, sqrt, log2'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features`: None, 0.5, sqrt, log2'
- en: '`min_samples_leaf` : 1, 3, 5, 10, 25, 100… As you increase, if you notice by
    the time you get to 10, it’s already getting worse then there is no point going
    further. If you get to 100 and it’s still going better, then you can keep trying.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_leaf` : 1, 3, 5, 10, 25, 100… 随着增加，如果你注意到当你达到10时，情况已经变得更糟，那么继续下去就没有意义了。如果你达到100时情况仍在好转，那么你可以继续尝试。'
- en: Random Forest Interpretation [[18:50](https://youtu.be/0v93qHDqq_g?t=18m50s)]
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林解释 [[18:50](https://youtu.be/0v93qHDqq_g?t=18m50s)]
- en: Random forest interpretation is something which you could use to create some
    really cool Kaggle kernels. Confidence based on tree variance is something which
    doesn’t exist anywhere else. Feature importance definitely does and that’s already
    in quite a lot of Kaggle kernels. If you are looking at a competition or a dataset
    where nobody’s done feature importance, being the first person to do that is always
    going to win lots of votes because the most important thing is which features
    are important.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林解释是你可以用来创建一些非常酷的Kaggle内核的东西。基于树方差的置信度是其他地方不存在的。特征重要性肯定存在，并且已经在许多Kaggle内核中。如果你正在看一个竞赛或一个数据集，没有人做过特征重要性，成为第一个这样做的人总是会赢得很多票，因为最重要的是哪些特征是重要的。
- en: Confidence based on tree variance [[20:43](https://youtu.be/0v93qHDqq_g?t=20m43s)]
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于树方差的置信度
- en: As I mentioned, when we do model interpretation, I tend to `set_rf_samples`
    to some subset — something small enough that I can run a model in under 10 seconds
    because there is no point running a super accurate model. Fifty thousand is more
    than enough to see each time you run an interpretation, you’ll get the same results
    back and so as long as that’s true, then you are already using enough data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所提到的，当我们进行模型解释时，我倾向于将`set_rf_samples`设置为某个子集——足够小，可以在不到10秒内运行一个模型，因为运行一个超级准确的模型没有意义。五万个样本已经足够了，每次运行解释时，你会得到相同的结果，只要这是真的，那么你已经在使用足够的数据了。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Feature Importance [[21:14](https://youtu.be/0v93qHDqq_g?t=21m14s)]
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: We learnt it works by randomly shuffling a column, each column one at a time,
    then seeing how accurate the pre-trained model is when you pass that in all the
    data as before but with one column shuffled.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到它是通过随机洗牌一列，每次一列，然后看看在将所有数据传递给预训练模型时，当其中一列被洗牌时，模型的准确性如何。
- en: Some of the questions I got after class reminded me that it is very easy to
    under appreciate how powerful and magic this approach is. To explain, I’ll mention
    a couple of the questions I heard.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 课后我收到的一些问题让我想起，很容易低估这种方法有多么强大和神奇。为了解释，我会提到我听到的一些问题。
- en: One question was “what if we just took one column at a time, and created a tree
    on just that column”. Then we will see which column’s tree is the most predictive.
    Why may that give misleading results about feature importance? We are going to
    lose the interactions between the features. If we just shuffle them, it will add
    randomness and we are able to both capture the interactions and the importance
    of the feature. This issue of interaction is not a minor detail. It is massively
    important. Think about this bulldozers dataset where, for example, there is one
    field called “year made” and another field called “sale date.” If we think about
    it, it’s pretty obvious that what matters is the combination of these two. In
    other words, the difference between the two is how old the piece of the equipment
    was when it got sold. So if we only included one of these, we are going to massively
    underestimate how important that feature is. Now, here is a really important point
    though. It’s pretty much always possible to create a simple logistic regression
    which is as good as pretty much any random forest if you know ahead of time exactly
    what variables you need, exactly how they interact, exactly how they need to be
    transformed. In this case, for example, we could have created a new field which
    was equal to sale year minus year made and we could have fed that to a model and
    got that interaction for us. But the point is, we never know that. You might have
    a guess of it — I think some of these things are interacting in this way, and
    I think this thing we need to take the log, and so forth. But the truth is that
    the way the world works, the causal structures, they have many many things interacting
    in many many subtle ways. That’s why using trees, whether it be gradient boosting
    machines or random forests, work so well.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是“如果我们一次只取一列，然后在那一列上创建一棵树会怎样”。然后我们会看到哪一列的树是最具预测性的。为什么这可能会导致关于特征重要性的误导性结果？我们将失去特征之间的相互作用。如果我们只是随机打乱它们，那么会增加随机性，我们就能捕捉到特征之间的相互作用和重要性。这种相互作用的问题并不是一个细枝末节。它非常重要。想想这个推土机数据集，例如，有一个字段叫做“制造年份”，另一个字段叫做“销售日期”。如果我们想一想，很明显重要的是这两者的组合。换句话说，两者之间的区别是设备在售出时的年龄。因此，如果我们只包含其中一个，我们将严重低估该特征的重要性。现在，这里有一个非常重要的观点。如果你事先知道你需要哪些变量，它们如何相互作用，以及它们需要如何转换，那么几乎总是可以创建一个简单的逻辑回归，它和几乎任何随机森林一样好。在这种情况下，例如，我们可以创建一个新字段，它等于销售年份减去制造年份，然后将其输入模型并为我们获取该相互作用。但关键是，我们永远不知道这一点。你可能会猜测
    — 我认为其中一些事物是以这种方式相互作用的，我认为这个东西我们需要取对数，等等。但事实是，世界运作的方式，因果结构，有许多许多事物以许多微妙的方式相互作用。这就是为什么使用树，无论是梯度提升机还是随机森林，都能够如此出色地工作。
- en: '**Terrance’s comment:** One thing that bit me years ago was also I tried doing
    one variable at a time thinking “oh well, I’ll figure out which one’s most correlated
    with the dependent variable” [[24:45](https://youtu.be/0v93qHDqq_g?t=24m45s)].
    But what it doesn’t pull apart is that what if all variables are basically copied
    the same variable then they are all going to seem equally important but in fact
    it’s really just one factor.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Terrance的评论：** 多年前咬我一口的一件事也是我尝试一次只处理一个变量，认为“哦，好吧，我会弄清楚哪个与因变量最相关”[[24:45](https://youtu.be/0v93qHDqq_g?t=24m45s)]。但它没有分开的是，如果所有变量基本上都是复制的同一个变量，那么它们看起来都同样重要，但实际上只是一个因素。'
- en: That is also true here. If we had a column appear twice, then shuffling that
    column isn’t going to make the model much worse. If you think about how it’s built,
    particularly if we had `max_features=0.5`, some of the times, we are going to
    get version A of the column, some of the times, we are going to get version B
    of the column. So half the time, shuffling version A of the column is going to
    make a tree a bit worse, half the time it’s going to make column B it’ll make
    it a bit worse, and so it’ll show that both of those features are somewhat important.
    And it will share the importance between the two features. So this is why “collinearity”
    (I write collinearity but it means that they are linearly related, so this isn’t
    quite right) — but this is why having two variables that are closely related to
    each other or more variables that are closely related to each other means that
    you will often underestimate their importance using this random forest technique.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这在这里也是正确的。如果我们有一列出现两次，那么对该列进行洗牌不会使模型变得更糟。如果你考虑它是如何构建的，特别是如果我们设置了`max_features=0.5`，有时我们会得到列的版本A，有时我们会得到列的版本B。因此，一半的时间，对列的版本A进行洗牌会使树变得稍微糟糕，一半的时间对列的版本B进行洗牌会使其稍微糟糕，因此它将显示这两个特征都有一定重要性。它将在这两个特征之间共享重要性。这就是为什么“共线性”（我写的是共线性，但它意味着它们是线性相关的，所以这不太对）——但这就是为什么拥有两个彼此密切相关的变量或更多彼此密切相关的变量意味着您经常会低估它们在使用这种随机森林技术时的重要性。
- en: 'Question: Once we’ve shuffled and we get a new model, what exactly are the
    units of these importance? Is this a change in the R² [[26:26](https://youtu.be/0v93qHDqq_g?t=26m26s)]?
    It depends on the library we are using. So the units are kind of like… I never
    think about them. I just know that in this particular library, 0.005 is often
    a cutoff I would tend to use. But all I actually care about is this picture (the
    feature importance ordered for each variable):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：一旦我们洗牌并获得一个新模型，这些重要性的单位究竟是什么？这是否是R²的变化？这取决于我们使用的库。所以这些单位有点像……我从来没有考虑过它们。我只知道在这个特定的库中，0.005经常是我倾向于使用的一个截止值。但我真正关心的是这张图片（每个变量的特征重要性排序）：
- en: '![](../Images/019901f14f9972d223ff1e491bb1e234.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/019901f14f9972d223ff1e491bb1e234.png)'
- en: Then zooming in, turning it into a bar plot and then find where it becomes flat
    (~0.005).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后放大，将其转换为条形图，然后找到其变平的地方（约0.005）。
- en: '![](../Images/b9fb41f6c136deafed557437e2802117.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法查看图片。如果您能提供图片中的文本，我将很乐意帮助您翻译。
- en: So I removed them at that point and check the validation score didn’t get worse.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我在那时将它们移除，并检查验证分数没有变差。
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If it did get worse, I will just decrease the cutoff a little bit until it doesn’t
    get worse. So the units of measure of this don’t matter too much. We will learn
    later about a second way of doing variable importance, by the way.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果情况变得更糟，我只需稍微降低截止值，直到情况不再恶化。因此，这个度量单位并不太重要。顺便说一下，我们以后会学习另一种计算变量重要性的方法。
- en: What is the purpose of removing them [[27:42](https://youtu.be/0v93qHDqq_g?t=27m42s)]?
    Having looked at our feature importance plot, we see the ones less than 0.005
    is this long tail of boringness. So I said let’s just try grabbing the columns
    where it is greater than 0.005, create a new data frame called `df_keep` which
    is `df_train` with just those kept columns, create a new training and validation
    sets with just those columns, create a new random forest, and look to see how
    the validation set score. And the validation set RMSE changed and they got a bit
    better. So if they are about the same or a tiny bit better then my thinking is
    well this is just as good a model, but it’s now simpler.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 移除它们的目的是什么？在查看我们的特征重要性图后，我们发现小于0.005的那些是无聊的长尾。所以我说让我们尝试只选择大于0.005的列，创建一个名为`df_keep`的新数据框，其中只包含那些保留的列，创建一个只包含这些列的新训练和验证集，创建一个新的随机森林，并查看验证集得分。验证集的RMSE发生了变化，变得更好了一点。所以如果它们大致相同或稍微好一点，那么我的想法是这是一个同样好的模型，但现在更简单。
- en: So when I redo the feature importance, there is less collinearity. In this case,
    I saw that year made went from being a bit better than the next best thing (coupler
    system), but now it’s way better. So it did seem to definitely change these feature
    importances and hopefully give me some more insight there.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我重新进行特征重要性分析时，相关性较小。在这种情况下，我发现制造年份从略优于下一个最好的特征（连接器系统）变得更好了，但现在它更好了。因此，它似乎确实改变了这些特征的重要性，并希望能给我一些更多的见解。
- en: '![](../Images/9c09d7629b7b08324236e8364f3ebc52.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法识别图片中的文本。如果您能提供文本内容，我将很乐意帮助您翻译。
- en: '**Question**: So how did that help our model [[29:30](https://youtu.be/0v93qHDqq_g?t=29m30s)]?
    We are going to dig into that now. Basically it tells us that, for example, if
    we are looking for how we are dealing with missing value, is there noise in the
    data, if it is a high cardinality categorical variable — they are all different
    steps we would take. So for example, if it was a high cardinality categorical
    variable that was originally a string, maybe fiProductClassDesc in above case,
    I remember one of the ones we looked at the other day had first of all was the
    type of vehicle and then a hyphen, and then the size of the vehicle. We might
    look at that and say “okay, that was an important column. Let’s try splitting
    it into two on hyphen and then take that bit which is a size of it and parse it
    and convert it into an integer.” We can try and do some feature engineering. Basically
    until you know which ones are important, you don’t know where to focus that feature
    engineering time. You can talk to your client or folks that are responsible for
    creating this data. If you were actually working at a bulldozer auction company,
    you might now go to the actual auctioneers and say “I am really surprised that
    coupler system seems to be driving people’s pricing decisions so much. Why do
    you think that might be?” and they can say to you “oh, it’s actually because only
    these classes of vehicles have coupler systems or only this manufacturer has coupler
    systems. So frankly this is actually not telling you about coupler systems but
    about something else. Oh hey, that reminds me, that’s something else we actually
    have measured that. It is in this different CSV file. I’ll go get it for you.”
    So it helps you focus your attention.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：那么这如何帮助我们的模型呢？我们现在要深入研究这个问题。基本上，它告诉我们，例如，如果我们正在寻找如何处理缺失值，数据中是否有噪音，如果是高基数分类变量——这些都是我们会采取的不同步骤。例如，如果原来是一个字符串的高基数分类变量，也许在上面的情况下是fiProductClassDesc，我记得我们前几天看的一个，首先是车辆类型，然后是一个连字符，然后是车辆的大小。我们可能会看到这个并说“好的，这是一个重要的列。让我们尝试在连字符上分割它成两部分，然后取那部分，即它的大小，并解析它并转换为整数。”我们可以尝试进行一些特征工程。基本上，直到你知道哪些是重要的，你就不知道在哪里集中特征工程的时间。你可以与负责创建这些数据的客户或相关人员交谈。如果你实际上在一个推土机拍卖公司工作，你现在可能会去找实际的拍卖人，说“我真的很惊讶，连接器系统似乎对人们的定价决策产生了如此大的影响。你认为这可能是为什么？”他们可能会告诉你“哦，实际上是因为只有这些类别的车辆有连接器系统，或者只有这个制造商有连接器系统。所以实际上这并不是告诉你关于连接器系统的，而是关于其他事情。哦，嘿，这让我想起来，我们实际上还测量了其他东西。它在另一个不同的CSV文件中。我去拿给你。”所以它帮助你集中注意力。'
- en: '**Question**: So I had a fun little problem this weekend as you know. I introduced
    a couple of crazy computation into my random forest and all of a sudden they’re
    like oh my god these are the most important variables ever squashing all of the
    others. But then I got a terrible score and then is that because now that I think
    I have my scores computed correctly, what I noticed is that the importance went
    through the roof but the validation set was still bad or got worse. Is that because
    somehow that computation allow the training to almost like an identifier map exactly
    what the answer was going to be for training but of course that doesn’t generalize
    to the validation set. Is that what I observed [[31:33](https://youtu.be/0v93qHDqq_g?t=31m33s)]?
    There are two reasons why your validation score might not be very good.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：所以你知道，这个周末我遇到了一个有趣的小问题。我在我的随机森林中引入了一些疯狂的计算，突然间它们就像是哦，这些是最重要的变量，压制了其他所有变量。但是我得到了一个糟糕的分数，那是因为我现在认为我的分数计算正确了吗，我注意到重要性飙升了，但验证集仍然很糟糕，甚至更糟。这是因为某种计算方式让训练几乎像一个标识符映射到了训练答案，但当然这并不能推广到验证集。这就是我观察到的吗？你的验证分数可能不太好的两个原因。'
- en: '![](../Images/8bb6d59a9098cedcd4df454a5e115bdd.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: 把这个文件夹拖到另一个文件夹中。
- en: 'So we got these five numbers: RMSE of training, validation, R² of the training,
    validation, and R² of OOB. There’re two reasons and really in the end what we
    care about for this Kaggle competition is the RMSE of the validation set assuming
    we’ve created a good validation set. So Terrance’s case, he is saying that RMSE
    of the validation got worse when I did some feature engineering. Why is that?
    There are two possible reasons.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们得到了这五个数字：训练的RMSE，验证的RMSE，训练的R²，验证的R²和OOB的R²。最终我们关心的是这个Kaggle竞赛的验证集的RMSE，假设我们已经创建了一个好的验证集。Terrance的情况，他说当我进行一些特征工程时，验证的RMSE变糟了。为什么呢？有两个可能的原因。
- en: Reason one is that you are overfitting. If you are overfitting, then your OOB
    will also get worse. If you are doing a huge dataset with a small `set_rf_samples`
    so you can’t use an OOB, then instead create a second validation set which is
    a random sample and do that. So in other words, if your OOB or your random sample
    validation set has gotten much worse then you must be overfitting. I think in
    your case, Terrance, it’s unlikely that’s the problem because random forests don’t
    overfit that badly. It’s very hard to get them to overfit that badly unless you
    use some really weird parameters like only one estimator, for example. Once we’ve
    got ten trees in there, there should be enough variation that you can definitely
    overfit but not so much that you’re going to destroy your validation score by
    adding a variable. So I’d think you’ll find that’s probably not the case, but
    it’s easy to check. And if it’s not the case, then you’ll see that your OOB score
    or your random sample validation score hasn’t gotten much worse.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原因一是你过拟合了。如果你过拟合了，那么你的OOB也会变得更糟。如果你在一个大数据集上使用了一个小的`set_rf_samples`，以至于无法使用OOB，那么可以创建一个第二个验证集，这个验证集是一个随机样本。换句话说，如果你的OOB或者随机样本验证集变得更糟，那么你一定是过拟合了。我认为在你的情况下，Terrance，这不太可能是问题，因为随机森林不会过拟合得那么严重。除非你使用一些非常奇怪的参数，比如只有一个估计器，否则很难让它们过拟合得那么严重。一旦我们有了十棵树，应该有足够的变化，你肯定可以过拟合，但不会过度到添加一个变量就破坏你的验证分数。所以我认为你会发现这可能不是问题，但很容易检查。如果不是这种情况，那么你会发现你的OOB分数或者随机样本验证分数并没有变得更糟。
- en: The second reason your validation score can get worse, if your OOB score hasn’t
    got worse, you’re not overfitting but your validation score has gotten worse that
    means you’re doing something that is true in the training set but not true in
    the validation set. So this can only happen when your validation set is not a
    random sample. For example, in this bulldozers competition or in the grocery shopping
    competition, we’ve intentionally made a validation set that is for a different
    date range — it’s for the most recent two weeks. So if something different happened
    in the last two weeks to the previous weeks, then you could totally break your
    validation set. For example, if there was some kind of unique identifier which
    is different in the two date periods, then you could learn to identify things
    using that identifier in the training set. But then the last two weeks may have
    a totally different set of IDs or the different set of behavior, it could get
    a lot worse. What you are describing is not common though. So I’m a bit skeptical
    — it might be a bug but hopefully there’s enough things you can now use to figure
    out if it is a bug. We will be interested to hear what you learned.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的验证分数变差的第二个原因是，如果您的OOB分数没有变差，那么您并没有过拟合，但是您的验证分数变差了，这意味着您在训练集中做了一些在验证集中不成立的事情。因此，这种情况只会发生在您的验证集不是随机抽样的情况下。例如，在这个推土机比赛或者杂货购物比赛中，我们故意制作了一个验证集，该验证集涵盖了不同的日期范围——最近的两周。因此，如果在最近两周发生了与之前几周不同的事情，那么您可能会完全破坏您的验证集。例如，如果有一种在两个日期段中不同的唯一标识符，那么您可能会学会在训练集中使用该标识符来识别事物。但是最近的两周可能有完全不同的ID集或不同的行为集，这可能会变得更糟。尽管您所描述的情况并不常见。所以我有点怀疑——这可能是一个错误，但希望您现在有足够的方法来确定是否是一个错误。我们将很乐意听到您学到了什么。
- en: Linear regression, logistic regression [[36:01](https://youtu.be/0v93qHDqq_g?t=36m1s)]
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归，逻辑回归
- en: That’s feature importance. I’d like to compare that to how feature importance
    is normally done in industry and in academic communities outside of machine learning,
    like in psychology, economics, and so forth. Generally speaking, people in those
    environments tend to use some kind of linear regression, logistic regression,
    general linear models. They start with their dataset and they say I am going to
    assume that I know the kind of parametric relationship between my independent
    variables and my dependent variable. So I’m going to assume that it’s a linear
    relationship or a linear relationship with a link function like a sigmoid to create
    logistic regression. So assuming I already know that, I can now write this as
    an equation. So if you have x1, x2, so forth.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是特征重要性。我想将其与在机器学习之外的行业和学术界（如心理学、经济学等）通常进行的特征重要性比较一下。一般来说，在这些环境中，人们倾向于使用某种线性回归、逻辑回归、一般线性模型等方法。他们从数据集开始，然后说我要假设我知道自己的自变量和因变量之间的参数关系。所以我要假设这是一个线性关系或者一个带有链接函数（如sigmoid）的线性关系，从而创建逻辑回归。所以假设我已经知道了这一点，我现在可以将其写成一个方程。所以如果你有x1、x2等等。
- en: '![](../Images/a827f2384b3daa554e46c4bd3b25fc8f.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法识别图片中的文本。如果您能提供文本内容，我将很乐意帮助您翻译。
- en: I can say my y values are equal to *ax1 + bx2 = y*, therefore I can find out
    the feature importance easily enough by just looking at these coefficients and
    see which one is the highest, particularly if you have normalized the data first.
    There is this trop out there that is very common is that this is somehow more
    accurate, more pure, in some way better way of doing feature importance but that
    couldn’t be farther from the truth. If you think about it, if you were missing
    an interaction, if you were missing a transformation you needed, or if you have
    any way being anything less than a 100% perfect in all of your pre-processing
    so that your model is the absolute correct truth of the situation — unless you’ve
    got all of that correct, then your coefficients are wrong. Your coefficients are
    telling you “in your totally wrong model, this is how important those things are”
    which is basically meaningless. Where else, the random forest feature importance
    is telling you in this extremely high parameter, highly flexible functional form,
    with few if any statistical assumptions, this is your feature importance. So I
    would be very cautious.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以说我的y值等于*ax1 + bx2 = y*，因此我可以通过查看这些系数并看到哪个最高来很容易地找出特征的重要性，特别是如果您首先对数据进行了归一化。有一个常见的误解是，这种方法在某种程度上更准确，更纯粹，更好，但事实并非如此。如果您考虑一下，如果您缺少一个交互作用，如果您缺少所需的转换，或者如果您在任何预处理方面不完美，以至于您的模型是情况的绝对正确真相
    - 除非您全部正确，否则您的系数是错误的。您的系数告诉您“在您完全错误的模型中，这些事物有多重要”，这基本上是毫无意义的。而另一方面，随机森林的特征重要性告诉您，在这种极高参数、高度灵活的函数形式中，几乎没有任何统计假设，这是您的特征重要性。所以我会非常谨慎。
- en: Again, I can’t stress this enough when you leave this program, you are much
    more often going to see people talk about logistic regression coefficients than
    you are going to see them talk about random forest variable importance. And every
    time you see that happen, you should be very very skeptical of what you are seeing.
    Anytime you read a paper in economics or in psychology, or the marketing department
    tells you that this regression or whatever, every single those coefficients are
    going to be massively biased by any issues in the model. Furthermore, if they’ve
    done so much pre-processing that actually the model is pretty accurate then now
    you are looking at coefficients that are going to be like a coefficient of some
    principal component from a PCA or a coefficient of some distance from some cluster
    or something. At which point, they are very very hard to interpret anyway. They
    are not actual variables. So they are kind of the two options I’ve seen when people
    try to use classic statistical techniques to do a variable importance equivalent.
    I think things are starting to change slowly. There are some fields that are starting
    to realize that this is totally the wrong way to do things. But it’s been nearly
    20 years since random forests appeared so it takes a long time. People say that
    the only way that knowledge really advances is when the previous generation dies,
    and that’s kind of true. Particularly academics, they make a career of being good
    at a particular sub thing and often it’s not until the next generation comes along
    that people notice that’s actually no longer a good way to do things. And I think
    that’s what happened here.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，当您离开这个程序时，您更多地会看到人们谈论逻辑回归系数，而不是随机森林变量重要性。每当您看到这种情况发生时，您应该非常怀疑您所看到的内容。每当您阅读经济学或心理学的论文，或者市场营销部门告诉您这种回归或其他内容时，这些系数都会受到模型中任何问题的严重偏见。此外，如果他们进行了大量的预处理，实际上模型相当准确，那么现在您看到的系数将会像来自PCA的某个主成分的系数或某个集群的某个距离的系数。在这种情况下，它们非常难以解释。它们不是实际的变量。所以这是我看到的人们尝试使用经典统计技术来进行等效变量重要性时的两种选择。我认为事情开始慢慢改变。有一些领域开始意识到这完全是错误的做法。但自从随机森林出现以来已经将近20年了，所以需要很长时间。人们说，只有当上一代人死去时，知识才会真正进步，这在某种程度上是真的。特别是学者，他们以擅长某个特定子领域而成为职业，通常直到下一代人出现时，人们才会注意到实际上这不再是一个好的做事方式。我认为这就是这里发生的事情。
- en: 'We’ve got now a model which isn’t really any better predictive accuracy wise,
    but we are getting a good sense that there seems to be four main important things
    [[40:38](https://youtu.be/0v93qHDqq_g?t=40m38s)]: YearMade, Coupler_System, ProductSize,
    fiProductClassDesc.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个模型，从预测准确性的角度来看并没有更好，但我们有一种很好的感觉，似乎有四个主要重要的因素：YearMade，Coupler_System，ProductSize，fiProductClassDesc。
- en: One hot encoding [[41:00](https://youtu.be/0v93qHDqq_g?t=41m)]
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一热编码
- en: There is something else that we can do, however, which is we can do something
    called one hot encoding. So this is going to where we were talking about categorical
    variable. Remember, a categorical variable, let’s say we had a string high, low,
    medium (the order we got was kind of weird — in alphabetical order by default).
    So we mapped it to 0, 1, 2\. By the time it gets into our data frame, it’s now
    a number so the random forest doesn’t know that it was originally a category —
    it’s just a number. So when the random forest is built, it basically says oh is
    it greater than 1 or not. Or is it greater than naught or not. They are basically
    the two possible decisions it could have made. For something with 5 or 6 bands,
    it could be that just one of the levels of category is actually interesting. Maybe
    the only thing that mattered was whether it was unknown. Maybe not knowing its
    size somehow impacts the price. So if we wanted to be able to recognize that and
    particularly if it just so happened that the way that the numbers were coded was
    it unknown ended up in the middle, then it going to take two splits to get to
    the point where we can see that it’s actually unknown that matters. So this is
    a little inefficient and we are wasting tree computation. Wasting tree computation
    matters because every time we do a split, we are halving the amount of data at
    least that we have to do more analysis. So it’s going to make our tree less rich
    and less effective if we are not giving the data in a way that is convenient for
    it to do the work it needs to do.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还可以做另一件事，那就是我们可以做一种称为独热编码的东西。这就是我们在谈论分类变量时所说的。记住，分类变量，假设我们有一个字符串高、低、中（我们得到的顺序有点奇怪——默认按字母顺序排列）。所以我们将其映射为0、1、2。当它进入我们的数据框时，现在它是一个数字，因此随机森林不知道它最初是一个类别——它只是一个数字。因此，当构建随机森林时，它基本上会说它是否大于1或不大于1。或者它是否大于0或不大于0。这基本上是它可以做出的两个可能决定。对于有5或6个级别的东西，可能只有一个类别级别是有趣的。也许唯一重要的是它是否未知。也许不知道它的大小会以某种方式影响价格。因此，如果我们想要能够识别这一点，特别是如果恰好数字编码的方式是未知的最终出现在中间，那么它将需要两次分割才能看到实际上重要的是未知的事情。因此，这有点低效，我们正在浪费树的计算。浪费树的计算很重要，因为每次我们进行分割时，我们至少要减少一半的数据量来进行更多的分析。因此，如果我们没有以方便它进行所需工作的方式提供数据，那么我们的树将变得不那么丰富和有效。
- en: What we could do instead is create 6 columns for each category and each column
    would contain 1’s and 0’s. Having added 6 additional columns to our dataset, the
    random forest now has the ability to pick one of these and say oh, let’s have
    a look at is_unknown. There is one possible fit I can do which is 1 vs. 0\. Let’s
    see that’s any good. So it now has the ability in a single step to pull out a
    single category level and this kind of coding is called one-hot encoding. For
    many types of machine learning model, something like this is necessary. If you
    are doing logistic regression, you can’t possibly put in a categorical variable
    that goes naught through five because there is obviously no written linear relationship
    between that and anything. So one hot encoding, a lot of people incorrectly assume
    that all machine learning requires one hot encoding. But in this case, I’m going
    to show you how we could use it optionally and see whether it might improve things
    sometimes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的是为每个类别创建6列，每列包含1和0。在我们的数据集中添加了6列后，随机森林现在可以选择其中一列并说“哦，让我们看看is_unknown”。我可以做一个可能的拟合，即1对0。让我们看看这是否有效。因此，它现在可以在一个步骤中提取一个类别级别，并且这种编码称为独热编码。对于许多类型的机器学习模型，这样的东西是必要的。如果你正在进行逻辑回归，你不可能放入一个分类变量，它经过0到5，因为显然它与任何东西之间没有线性关系。因此，许多人错误地认为所有机器学习都需要独热编码。但在这种情况下，我将向您展示如何可以选择使用它，并查看它是否有时可能会改善事情。
- en: '**Question**: If we have six categories like in this case, would there be any
    problems with adding a column for each of the categories? In linear regression,
    if there are six categories, we should only do it for five of them [[45:17](https://youtu.be/0v93qHDqq_g?t=45m17s)].
    You certainly can say let’s not worry about adding `is_medium` because we can
    infer it from the other five. I would say include it anyway because otherwise,
    the random forest has to make five decisions to get to that point. The reason
    you need to not include one in linear models is because linear models hate collinearity
    but we don’t care about that here.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如果我们有六个类别，就像在这种情况下一样，为每个类别添加一列会有什么问题吗？在线性回归中，如果有六个类别，我们应该只对其中五个进行操作。你当然可以说，让我们不要担心添加`is_medium`，因为我们可以从其他五个中推断出来。我会建议无论如何都要包括它，因为否则，随机森林就必须做出五个决定才能到达那一点。你不包括一个在线性模型中的原因是因为线性模型讨厌共线性，但在这里我们不在乎这个。'
- en: So we can do one hot encoding easily enough and the way we do it is we pass
    one extra parameter to `proc_df` which is what is the max number of categories
    (`max_n_cat`). So if we say it’s seven, then anything with less than seven levels
    is going to be turned into a one-hot encoded bunch of columns.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以很容易地进行独热编码，我们的做法是向`proc_df`传递一个额外的参数，即最大类别数（`max_n_cat`）。因此，如果我们说是七，那么任何级别少于七的东西都将被转换为一组独热编码的列。
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Like zip code has more than six levels so that would be left as a number. Generally
    speaking, you obviously probably wouldn’t want to one hot encode zip code because
    that’s just going to create masses of data, memory problems, computation problems,
    and so forth. So this is another parameter you can play around with.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如邮政编码有超过六个级别，因此将保留为数字。一般来说，您显然不希望对邮政编码进行独热编码，因为这只会创建大量数据、内存问题、计算问题等。因此，这是您可以尝试的另一个参数。
- en: So if I try it out, run the random forest as per usual, you can see what happens
    to the R² of the validation set and to the RMSE of the validation set. In this
    case, I found it got a little bit worse. This isn’t always the case and it’s going
    to depend on your dataset. It depends on if you have a dataset where single categories
    tend to be quite important or not. In this particular case, it did not make it
    more predictive. However, what it did do is that we now have different features.
    proc_df puts the name of the variable, an underscore, and the level name. So interestingly,
    it turns out that before, it said that enclosure was somewhat important. When
    we do it as one hot encoded, it actually says `Enclosure_EROPS w AC` is the most
    important thing. So for at least the purpose of interpreting your model, you should
    always try one hot encoding quite a few of your variables. I often find somewhere
    around 6 or 7 pretty good. You can try making that number as high as you can so
    that it doesn’t take forever to compute and the feature importance doesn’t include
    really tiny levels that aren’t interesting. That is up to you to play around with,
    but in this case, I found this very interesting. It clearly tells me I need to
    find out what `Enclosure_EROPS w AC` is and why it is important because it means
    nothing to me right now but it is the most important thing. So I should go figure
    that out.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我尝试一下，像往常一样运行随机森林，你可以看到验证集的R²和验证集的RMSE会发生什么变化。在这种情况下，我发现它变得稍微糟糕了。这并不总是这样，这将取决于你的数据集。这取决于你的数据集是否有单个类别往往相当重要。在这种特殊情况下，它并没有使预测更准确。然而，它所做的是我们现在有了不同的特征。proc_df将变量的名称、下划线和级别名称放在一起。有趣的是，结果表明以前说围栏是有些重要的。当我们将其进行独热编码时，它实际上说`Enclosure_EROPS
    w AC`是最重要的事情。所以至少在解释模型的目的上，你应该尝试对你的变量进行独热编码。我经常发现大约6或7个变量相当不错。你可以尝试将这个数字尽可能地提高，这样计算不会花费太长时间，而且特征重要性不会包括那些不感兴趣的非常小的级别。这取决于你自己去尝试，但在这种情况下，我发现这非常有趣。它清楚地告诉我我需要找出`Enclosure_EROPS
    w AC`是什么，为什么它很重要，因为现在对我来说毫无意义，但它是最重要的事情。所以我应该去弄清楚。
- en: '![](../Images/8cc615706bdfac9c7fba609ef5963b14.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cc615706bdfac9c7fba609ef5963b14.png)'
- en: '**Question**: Can you explain how changing the max number of category works?
    Because for me, it just seems like there are five categories or six categories
    [[49:15](https://youtu.be/0v93qHDqq_g?t=49m15s)]. All it’s doing is is here is
    a column called zip code, usage band, and sex, for example. Say, zip code has
    5,000 levels. The number of levels in a category, we call its “cardinality”. So
    it has a cardinality of 5,000\. Usage band may have a cardinality of six. Sex
    has a cardinality of two. So when proc_df goes through and says okay, this is
    a categorical variable, should I one-hot encode it? It checks the cardinality
    against `max_n_cat` and says 5,000 is bigger than seven so I don’t one hot encode
    it. Then it goes to usage band — 6 is less than 7, so I do one hot encode it.
    It goes to sex, and 2 is less than 7, so one hot encode that too. So it just says
    for each variable, how I decide whether ton one hot encode it or not. Once we
    decide to one hot encode it, it does not keep the original variable.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你能解释一下如何改变类别的最大数量吗？因为对我来说，似乎只有五个类别或六个类别[[49:15](https://youtu.be/0v93qHDqq_g?t=49m15s)]。它所做的就是这里有一个叫做邮政编码、使用频段和性别的列，例如。比如说，邮政编码有5,000个级别。类别中的级别数量，我们称之为“基数”。所以它的基数是5,000。使用频段可能有六个基数。性别有两个基数。所以当proc_df遍历并说好的时候，这是一个分类变量，我应该进行独热编码吗？它会检查基数与`max_n_cat`进行比较，说5,000大于七，所以我不进行独热编码。然后它转到使用频段——6小于7，所以我进行独热编码。它转到性别，2小于7，所以也进行独热编码。所以它只是为每个变量决定是否进行独热编码。一旦我们决定进行独热编码，它就不会保留原始变量。'
- en: 'If you have actually made an effort to turn your ordinal variables into proper
    ordinals, using proc_df can destroy that. The simple way to avoid that is if we
    know that we always want to use the codes for usage band, you could just go ahead
    and replace it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实努力将你的有序变量转换为适当的有序变量，使用proc_df可能会破坏这一点。避免这种情况的简单方法是，如果我们知道我们总是想要使用使用频段的代码，你可以直接替换它：
- en: '![](../Images/157f32c922842b4c045192aba902ce72.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/157f32c922842b4c045192aba902ce72.png)'
- en: Now it’s an integer. So it will never get changed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它是一个整数。所以它永远不会改变。
- en: Removing redundant features [[54:57](https://youtu.be/0v93qHDqq_g?t=54m57s)]
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除冗余特征[[54:57](https://youtu.be/0v93qHDqq_g?t=54m57s)]
- en: We’ve already seen how variables which are basically measuring the same thing
    can confuse our variable importance. They can also make our random forest slightly
    less good because it requires more computation to do the same thing and there’re
    more columns to check. So we are going to do some more work to try and remove
    redundant features. The way I do that is to do something called “**dendrogram”**.
    And it is kind of hierarchical clustering.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，基本上测量相同事物的变量会混淆我们的变量重要性。它也会使我们的随机森林稍微不那么好，因为需要更多的计算来做同样的事情，还有更多的列要检查。所以我们要做一些额外的工作来尝试去除冗余特征。我做的方法是做一些叫做“**树状图**”的东西。它有点像分层聚类。
- en: '**Cluster analysis** is something where you are trying to look at objects,
    they can be either rows in the dataset or columns and find which ones are similar
    to each other. Often you will see people particularly talking about cluster analysis,
    they normally refer to rows of data and they will say “let’s plot it” and find
    clusters. A common type of cluster analysis, time permitting, we may get around
    to talking about this in some detail, is called k-means. It is basically where
    you assume that you don’t have any labels at all and you take a couple of data
    points at random and you gradually find the ones that are near to it and move
    them closer and closer to centroids, and you repeat it again and again. It is
    an iterative approach that you tell it how many clusters you want and it will
    tell you where it thinks that classes are.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类分析**是一种尝试查看对象的方法，它们可以是数据集中的行或列，并找出彼此相似的对象。通常你会看到人们特别谈论聚类分析，他们通常指的是数据的行，并会说“让我们绘制它”并找出聚类。一种常见的聚类分析类型，如果时间允许，我们可能会详细讨论一下，被称为k均值。基本上，你假设你根本没有任何标签，然后随机选择几个数据点，逐渐找到靠近它的数据点，并将它们移动到离质心更近的位置，然后再次重复这个过程。这是一种迭代的方法，你告诉它你想要多少个聚类，它会告诉你它认为哪些类别在哪里。'
- en: A really under used technique (20 or 30 years ago it was much more popular than
    it is today) is a hierarchical clustering also known as agglomerated clustering.
    In hierarchical or agglomerated clustering, we look at every pair of objects and
    say which two objects are the closest. We then take the closest pair, delete them,
    and replace them with the midpoint of the two. Then repeat that again and again.
    Since we are removing points and replacing them with their averages, you are gradually
    reducing a number of points by pairwise combining. The cool thing is, you can
    plot that.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个真正被低估的技术（20或30年前比今天更受欢迎）是层次聚类，也称为凝聚聚类。在层次或凝聚聚类中，我们查看每对对象，并说哪两个对象最接近。然后我们取最接近的一对，删除它们，并用两者的中点替换它们。然后再重复这个过程。由于我们正在删除点并用它们的平均值替换它们，您逐渐通过成对组合减少了点的数量。很酷的是，您可以绘制出来。
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/585a23d42f454690fe4a5175d6637dd4.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法识别图片中的文本。如果您能提供文本内容，我将很乐意帮助您翻译。
- en: Like so. Rather than looking at points, you look at variables and we can see
    which two variables are the most similar. `saleYear` and `saleElapsed` are very
    similar. So the horizontal axis here is how similar are the two points that are
    being compared. If they are closer to the right, that means that they are very
    similar. So saleYear and saleElapsed have been combined and they were very similar.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这样。不是看点，而是看变量，我们可以看到哪两个变量最相似。`saleYear`和`saleElapsed`非常相似。因此，这里的横轴是正在比较的两个点有多相似。如果它们更靠近右侧，那意味着它们非常相似。因此，`saleYear`和`saleElapsed`已经被合并，并且它们非常相似。
- en: In this case, I actually used Spearman’s R. You guys familiar with correlation
    coefficients already? So correlation is almost exactly the same as the R², but
    it’s between two variables rather than a variable and its prediction. The problem
    with a normal correlation is that if you have data that looks like this then you
    can do a correlation and you’ll get a good result.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我实际上使用了斯皮尔曼相关系数R。你们已经熟悉相关系数了吗？所以相关性几乎与R²完全相同，但它是在两个变量之间而不是一个变量和它的预测之间。普通相关性的问题在于，如果你有这样的数据，那么你可以进行相关性分析，你会得到一个好的结果。
- en: '![](../Images/6d054bfdfc7b524a9f1d1cb08655895f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d054bfdfc7b524a9f1d1cb08655895f.png)'
- en: But if you’ve got data which looks like this and you try and do a correlation
    (assuming linearity), that’s not very good.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你有这样的数据，并尝试进行相关性分析（假设是线性的），那就不太好了。
- en: '![](../Images/542a32bfb51894e5c8a241e85d996490.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/542a32bfb51894e5c8a241e85d996490.png)'
- en: So there is a thing called a rank correlation which is a really simple idea.
    Replace every point by its rank.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '因此有一种称为秩相关的东西，这是一个非常简单的想法。用每个点的秩替换它。 '
- en: '![](../Images/c32d3e2abcb4bc0d3cbc58562eaa72e0.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c32d3e2abcb4bc0d3cbc58562eaa72e0.png)'
- en: From left to right, we rank from 1, 2, …6\. Then you do the same for the y-axis.
    Then you create a new plot where you don’t plot the data but you plot the rank
    of the data. If you think about it, the rank of this dataset is going to look
    like an exact line because every time something was greater on the x-axis, it
    was also greater on the y-axis. So if we do a correlation on the rank, that’s
    called a rank correlation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右，我们按照1、2、…6的顺序排名。然后你也要对y轴做同样的操作。然后你创建一个新的图，不是绘制数据，而是绘制数据的排名。如果你仔细想一想，这个数据集的排名看起来会像一条直线，因为每当x轴上的某个值更大时，y轴上的值也更大。因此，如果我们对排名进行相关性分析，那就是称为排名相关性。
- en: Because we want to find the columns that are similar in a way that the random
    forest would find them similar (random forests do not care about linearity, they
    just care about ordering), so a rank correlation is the right way to think about
    that [[1:00:05](https://youtu.be/0v93qHDqq_g?t=1h5s)]. So Spearman’s R is the
    name of the most common rank correlation. But you can literally replace the data
    with its rank and chuck it at the regular correlation and you will get basically
    the same answer. The only difference is in how ties are handled which is a pretty
    minor issue.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们想要找到那些在某种方式上与随机森林发现它们相似的列（随机森林不关心线性，它们只关心排序），所以秩相关是正确的思考方式。所以斯皮尔曼相关系数是最常见的秩相关的名称。但你可以用数据的秩替换数据，然后将其传递给常规相关性，你将得到基本相同的答案。唯一的区别在于如何处理并列的数据，这是一个相当次要的问题。
- en: 'Once we have a correlation matrix, there is basically a couple of standard
    steps you do to turn that into a dendrogram which I have to look up on stackoverflow
    each time I do it. You basically turn it into a distance matrix and then you create
    something that tells you which things are connected to each other things hierarchically.
    So these are three standard steps you always have to do to create a dendrogram:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了一个相关矩阵，基本上有几个标准步骤可以将其转换为树状图，每次我都必须在stackoverflow上查找。你基本上将其转换为一个距离矩阵，然后创建一个告诉你哪些东西在层次上连接到彼此的东西的东西。所以这是你总是必须做的三个标准步骤来创建一个树状图：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then you can plot it [[1:01:30](https://youtu.be/0v93qHDqq_g?t=1h1m30s)]. `saleYear`
    and `saleElapsed` are measuring basically the same thing (at least in terms of
    rank) which is not surprising because `saleElapsed` is the number of days since
    the first day in my dataset so obviously these two are nearly entirely correlated.
    `Grouser_Tracks`, `Hidraulics_Flow`, and `Coupler_System` all seem to be measuring
    the same thing. This is interesting because remember, `Coupler_System` it said
    was super important. So this rather supports our hypothesis there is nothing to
    do with whether it’s a coupler system but whether it is whatever kind of vehicle
    it is has these kind of features. `ProductGroup` and `ProductGroupDesc` seem to
    be measuring the same thing, and so are `fiBaseModel` and `fiModelDesc`. Once
    we get past that, suddenly things are further away, so I’m probably going to not
    worry about those. So we are going to look into those four groups that are very
    similar.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以绘制它[[1:01:30](https://youtu.be/0v93qHDqq_g?t=1h1m30s)]。`saleYear`和`saleElapsed`基本上在衡量相同的东西（至少在排名上），这并不奇怪，因为`saleElapsed`是自我的数据集中的第一天以来的天数，所以显然这两者几乎完全相关。`Grouser_Tracks`、`Hidraulics_Flow`和`Coupler_System`似乎在衡量相同的东西。这很有趣，因为记住，`Coupler_System`被认为非常重要。所以这更支持了我们的假设，这与是否是一个连接器系统无关，而是与它是什么类型的车辆具有这种特征。`ProductGroup`和`ProductGroupDesc`似乎在衡量相同的东西，`fiBaseModel`和`fiModelDesc`也是如此。一旦我们超过这一点，突然之间的距离更远，所以我可能不会担心那些。所以我们将研究那些非常相似的四组。
- en: '![](../Images/585a23d42f454690fe4a5175d6637dd4.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/585a23d42f454690fe4a5175d6637dd4.png)'
- en: If you just want to know how similar is this thing to this thing, the best way
    is to look at the Spearman’s R correlation matrix [[1:03:43](https://youtu.be/0v93qHDqq_g?t=1h3m43s)].
    There is no random forest being used here. The distance measure is being done
    entirely on rank correlation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想知道这个东西与那个东西有多相似，最好的方法是查看Spearman's R相关矩阵[[1:03:43](https://youtu.be/0v93qHDqq_g?t=1h3m43s)]。这里没有使用随机森林。距离度量完全是基于秩相关性进行的。
- en: What I then do is I take these groups and I create a little function `get_oob`
    (get Out Of Band score) [[1:04:29](https://youtu.be/0v93qHDqq_g?t=1h4m29s)]. It
    does a random forest for some data frame. I make sure that I have taken that data
    frame and split it into a training and validation set, and then I call `fit` and
    return the OOB score.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我做的是，我取这些组并创建一个小函数`get_oob`（获取Out Of Band分数）[[1:04:29](https://youtu.be/0v93qHDqq_g?t=1h4m29s)]。它为某个数据框执行一个随机森林。我确保已经将该数据框拆分为训练集和验证集，然后调用`fit`并返回OOB分数。
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Basically what I’m going to do is try removing each one of these 9 or so variables
    one at a time and see which ones I can remove and it doesn’t make the OOB score
    get worse.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我要做的是尝试逐个去掉这9个左右的变量中的每一个，看看哪些我可以去掉而不会使OOB分数变得更糟。
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And each time I run this, I get slightly different results so actually it looks
    like the last time I had 6 things and not 9 things. So you can see, I just do
    a loop through each of the things that I am thinking maybe I can get rid of this
    because it’s redundant and I print out the column name and the OOB score of a
    model that is trained after dropping that one column.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我运行这个，我得到稍微不同的结果，所以实际上看起来上一次我有6个而不是9个。所以你可以看到，我只是循环遍历我认为可能可以去掉的每一个东西，因为它是多余的，然后打印出模型的列名和在去掉那个列之后训练的模型的OOB分数。
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The OOB score on my whole data frame is .89 and then after dropping each one
    of these things, basically none of them got much worse. `saleElapsed` is getting
    quite a bit worse than `saleYear`. Burt it looks like pretty much everything else,
    I can drop with only like a third decimal place problem. So obviously though,
    you’ve got to remember the dendrogram. Let’s take fiModelDesc and fiBaseModel,
    they are very similar to each other. So what this says isn’t that I can get rid
    of both of them, I can get rid of one of them because they are basically measuring
    the same thing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据框的OOB分数为0.89，然后在去掉每一个这些东西之后，基本上没有一个变得更糟。`saleElapsed`比`saleYear`要糟糕得多。但看起来其他几乎所有的东西，我只能去掉一个小数点问题。所以显然，你必须记住树状图。让我们看看fiModelDesc和fiBaseModel，它们非常相似。所以这意味着的不是我可以去掉它们中的两个，而是我可以去掉其中一个，因为它们基本上在衡量同一件事情。
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So then I try it. Let’s try getting rid of one from each group:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我尝试了。让我们尝试每组中去掉一个：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We’ve gone from .890 to .888, again, it’s so close as to be meaningless. So
    that sounds good. Simpler is better. So I’m now going to drop these columns from
    my data frame, and then I can try running the full model again.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从0.890到0.888，再次，它们之间的差距太小以至于无关紧要。听起来不错。简单就是好。所以我现在要从我的数据框中删除这些列，然后我可以尝试再次运行完整的模型。
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`reset_rf_samples` means I’m using my whole bootstrapped sample. With 40 estimators,
    we got 0.907.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset_rf_samples`意味着我使用了整个自助采样。有40个估计器，我们得到了0.907。'
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So I’ve now got a model which is smaller and simpler, and I’m getting a good
    score for. So at this point, I’ve now got rid of as many columns as I feel I comfortably
    can (ones that either didn’t have a good feature importance or were highly related
    to other variables, and the model didn’t get worse significantly when I removed
    them).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我有了一个更小更简单的模型，并且得分很好。所以在这一点上，我已经尽可能地去掉了许多列（那些要么没有很好的特征重要性，要么与其他变量高度相关，当我去掉它们时，模型没有显著变差）。
- en: Partial dependence [[1:07:34](https://youtu.be/0v93qHDqq_g?t=1h7m34s)]
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部分依赖[[1:07:34](https://youtu.be/0v93qHDqq_g?t=1h7m34s)]
- en: So now I’m at the point where I want to try and really understand my data better
    by taking advantage of the model. And we are going to use something called partial
    dependence. Again, this is something that you could use in the Kaggle kernel and
    lots of people are going to appreciate this because almost nobody knows about
    partial dependence and it’s a very very powerful technique. What we are going
    to do is we are going to find out, for the features that are important, how do
    they relate to the dependent variable. Let’s have a look.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我到了想要通过利用模型更好地了解我的数据的阶段。我们将使用一种称为偏依赖的技术。再次强调，这是你可以在Kaggle内核中使用的东西，很多人会欣赏这一点，因为几乎没有人知道偏依赖，它是一种非常强大的技术。我们要做的是找出对于重要的特征，它们如何与因变量相关。让我们来看看。
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, since we are doing interpretation, we will set `set_rf_samples` to 50,000
    to run things quickly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，由于我们正在进行解释，我们将设置`set_rf_samples`为50,000，以便快速运行事务。
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We’ll take our data frame, we will get our feature importance and notice that
    we are using `max_n_cat` because I am actually pretty interested in seeing the
    individual levels for interpretation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获取我们的特征重要性，并注意我们正在使用`max_n_cat`，因为我实际上对看到解释的各个级别很感兴趣。
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the top 10:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前10个：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/7b77f6f118b8a26274ddbb81c9df6471.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b77f6f118b8a26274ddbb81c9df6471.png)'
- en: Let’s try to learn more about those top 10\. `YearMade` is the second most important.
    So one obvious thing we could do would be to plot `YearMade` against `saleElapsed`
    because as we’ve talked about already, it seems to make sense that they are both
    important but it seems very likely that they are combined together to find how
    old was the product when it was sold. So we could try plotting `YearMade` against
    `saleElapsed` to see how they relate to each other.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试更多地了解那些前10个。`YearMade`是第二重要的。所以一个明显的事情是我们可以做的是绘制`YearMade`与`saleElapsed`的关系，因为正如我们已经讨论过的，它们似乎是重要的，但很可能它们是结合在一起找出产品在销售时的年龄。所以我们可以尝试绘制`YearMade`与`saleElapsed`，看看它们之间的关系。
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/ad9a77c110facd9f242fcb4fd790fd37.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad9a77c110facd9f242fcb4fd790fd37.png)'
- en: And when we do, we get this very ugly graph [[1:09:08](https://youtu.be/0v93qHDqq_g?t=1h9m8s)].
    It shows us that `YearMade` actually has a whole bunch that are a thousand. Clearly,
    this is where I would tend to go back to the client and say okay, I’m guessing
    that these bulldozers weren’t actually made in the year 1000 and they would presumably
    say to me “oh yes, they are ones where we don’t know where it was made”. Maybe
    “before 1986, we didn’t track that” or maybe “the things that are sold in Illinois,
    we don’t have that data provided”, etc — they will tell us some reason. So in
    order to understand this plot better, I’m just going to remove them from this
    interpretation section of the analysis. We will just grab things where `YearMade`
    is greater than 1930.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们这样做时，我们得到了这个非常丑陋的图表。它告诉我们`YearMade`实际上有很多是一千。显然，这是我会倾向于回到客户那里并说好的，我猜这些推土机实际上不是在公元1000年制造的，他们可能会对我说“是的，这些是我们不知道制造地点的产品”。也许“1986年之前，我们没有追踪”或者“在伊利诺伊州销售的产品，我们没有提供这些数据”等等——他们会告诉我们一些原因。为了更好地理解这个图，我只是要从分析的解释部分中将它们移除。我们只会获取`YearMade`大于1930的数据。
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s now look at the relationship between `YearMade` and `SalePrice`. There
    is a really great package called `ggplot`. `ggplot` originally was an R package
    (GG stands for the Grammar of Graphics). The grammar of graphics is this very
    powerful way of thinking about how to produce charts in a very flexible way. I’m
    not going to be talking about it much in this class. There is lots of information
    available online. But I definitely recommend it as a great package to use. `ggplot`
    which you can `pip` install, it’s part of the fast.ai environment already. `ggplot`
    in Python has basically the same parameters and API as the R version. The R version
    is much better documented so you should read its documentation to learn how to
    use it. But basically you say “okay, I want to create a plot for this data frame
    (`x_all`). When you create plots, most of the datasets you are using are going
    to be too big to plot. For example, if you do a scatter plot, it will create so
    many dots that it’s just a big mess and it will take forever. Remember, when you
    are plotting things, you are looking at it, so there is no point plotting something
    with a hundred million samples when if you only used a hundred thousand, it’s
    going to be pixel identical. That’s why I call `get_sample` first. `get_sample`
    just grabs a random sample.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下`YearMade`和`SalePrice`之间的关系。有一个非常棒的包叫做`ggplot`。`ggplot`最初是一个R包（GG代表图形语法）。图形语法是一种非常强大的思考方式，可以以非常灵活的方式生成图表。我在这门课上不会谈论它太多。网上有很多信息可供参考。但我绝对推荐它作为一个很棒的包来使用。`ggplot`可以通过`pip`安装，它已经是fast.ai环境的一部分。Python中的`ggplot`基本上具有与R版本相同的参数和API。R版本有更好的文档，所以你应该阅读它的文档以了解如何使用它。但基本上你会说“好的，我想为这个数据框（`x_all`）创建一个图。当你创建图时，你使用的大多数数据集都太大而无法绘制。例如，如果你做一个散点图，它会创建很多点，导致一团糟，而且会花费很长时间。记住，当你绘制东西时，你是在看它，所以没有必要绘制一个有一亿个样本的东西，当你只使用十万个时，它们会完全相同。这就是为什么我首先调用`get_sample`。`get_sample`只是抓取一个随机样本。
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So I’m just going to grab 500 points from my data frame and plot `YearMade`
    against `SalePrice`. `aes` stands for “aesthetic” — this is the basic way that
    you set up your columns in `ggplot`. Then there is this weird thing in `ggplot`
    where “+” means add chart elements. So I’m going to add a smoother. Often you
    will find that a scatter plot is very hard to see what is going on because there’s
    too much randomness. Or else, a smoother basically creates a little linear regression
    for every little subset of the graph. So it joins it up and allows you to see
    a nice smooth curve. This is the main way that I tend to look at univariate relationships.
    By adding standard error equals true (`se=True`), it also shows me the confidence
    interval of this smoother. `loess` stands for locally weighted regression which
    is this idea of doing lots of little mini regressions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我只是从我的数据框中抓取500个点，然后绘制`YearMade`和`SalePrice`。`aes`代表“美学” - 这是你在`ggplot`中设置列的基本方式。然后在`ggplot`中有一个奇怪的东西，“+”表示添加图表元素。所以我要添加一个平滑线。通常你会发现散点图很难看清楚发生了什么，因为有太多的随机性。或者，平滑线基本上为图的每个小子集创建一个小线性回归。这样可以连接起来，让你看到一个漂亮的平滑曲线。这是我倾向于查看单变量关系的主要方式。通过添加`se=True`，它还会显示这个平滑线的置信区间。`loess`代表局部加权回归，这是做许多小型回归的想法。
- en: '![](../Images/9fef5ff6825c0519befb686704772c41.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fef5ff6825c0519befb686704772c41.png)'
- en: So we can see here [[1:12:48](https://youtu.be/0v93qHDqq_g?t=1h12m48s)], the
    relationship between `YearMade` and `SalePrice` is all over the place which is
    not really what we would expect. I would have expected that stuff that’s sold
    more recently would probably be more expensive because of inflation and they are
    more current models. The problem is that when you look at a univariate relationship
    like this, there is a whole lot of collinearity going on — a whole lot of interactions
    that are being lost. For example, why did the price drop? Is it actually because
    things made between 1991 and 1997 are less valuable? Or is it actually because
    most of them were also sold during that time and there was maybe a recession then?
    Or maybe it was because products sold during that time, a lot more people were
    buying types of vehicles that were less expensive? There’s all kind of reasons
    for that. So again, as data scientists, one of the things you are going to keep
    seeing is that at the companies that you join, people will come to you with these
    kind of univariate charts where they’ll say “oh my gosh, our sales in Chicago
    have disappeared. They got really baed.” or “people aren’t clicking on this add
    anymore” and they will show you a chart that looks like this and ask what happened.
    Most of the time, you’ll find the answer to the question “what happened” is that
    there is something else going on. So for instance, “actually in Chicago last week,
    actually we were doing a new promotion and that’s why our revenue went down —
    it’s not because people are not buying stuff in Chicago anymore; the prices were
    lower”.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以在这里看到，`YearMade`和`SalePrice`之间的关系非常混乱，这并不是我们所期望的。我本来以为最近卖出的东西可能会更贵，因为通货膨胀和更现代的型号。问题在于，当你看一个像这样的单变量关系时，会有很多共线性发生
    - 很多互动被忽略了。例如，价格为什么会下降？是因为1991年至1997年之间制造的东西价值更低吗？还是因为大部分产品在那个时期也被卖出，那时可能有经济衰退？或者是因为在那个时期卖出的产品，更多人购买了价格更低的车辆类型？有各种各样的原因。所以再次，作为数据科学家，你将会看到的一件事是，在你加入的公司里，人们会拿着这种单变量图来找你，他们会说“哦天啊，我们在芝加哥的销售量消失了。变得很糟糕。”或者“人们不再点击这个广告了”，然后他们会给你看一个看起来像这样的图表，问发生了什么。大多数情况下，你会发现答案是“发生了什么”的问题是有其他原因的。比如，“实际上上周在芝加哥，我们在做一个新的促销活动，这就是为什么我们的收入下降了
    - 不是因为人们不再在芝加哥购买东西了；价格更低了”。
- en: So what we really want to be able to do is say “well, what’s the relationship
    between `SalePrice` and `YearMade` all other things being equal. “All other things
    being equal” basically means if we sold something in 1990 vs. 1980 and it was
    exactly the same thing to exactly the same person in exactly the same auction
    so on and so forth, what would have been the difference in price? To do that,
    we do something called a **partial dependence plot** [[1:15:02](https://youtu.be/0v93qHDqq_g?t=1h15m2s)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们真正想要做的是说“嗯，`SalePrice`和`YearMade`之间的关系是什么，其他所有事情都相等。” “其他所有事情都相等”基本上意味着如果我们在1990年和1980年卖了同样的东西给同样的人在同样的拍卖会上等等，价格会有什么不同？为了做到这一点，我们做了一个叫做**部分依赖图**的东西。
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There is a really nice library which nobody’s heard of called `pdp` which does
    these partial dependence plots, and what happens is this. We’ve got our sample
    of 500 data points and we are going to do something really interesting. We are
    going to take each one of those five hundred randomly chosen auctions and we are
    going to make a little dataset out of it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个非常好的库，没有人听说过，叫做`pdp`，它可以做这些部分依赖图，发生的情况是这样的。我们有500个数据点的样本，我们要做一些非常有趣的事情。我们将对这500个随机选择的拍卖会进行处理，然后我们将从中制作一个小数据集。
- en: Here is our dataset of 500 auctions and here is our columns, one of which is
    the thing that we are interested in which is `YearMade`. We are now going to try
    and create a chart where we say all other things being equal in 1960, how much
    did things cost in auctions? The way we are going to do that is we are going to
    replace the `YearMade` column with 1960\. We are going to copy in the value 1960
    again and again all the way down. Now every row, the year made is 1960 and all
    of the other data is going to be exactly the same. We are going to take our random
    forest, we are going to pass all this through our random forest to predict the
    sale price. That will tell us for everything that was auctioned, how much do we
    think it would have been sold for if that thing was made in 1960\. And that’s
    what we are going to plot on the right.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的数据集，有500个拍卖品，这是我们的列，其中一个是我们感兴趣的事物`YearMade`。我们现在要尝试创建一个图表，在这个图表中我们说在1960年，其他所有事物都相等的情况下，拍卖品的成本是多少？我们将用1960年替换`YearMade`列。我们将一直复制值1960，直到最后。现在每一行，制造年份都是1960，所有其他数据都将完全相同。我们将使用我们的随机森林，将所有这些数据传递给我们的随机森林来预测销售价格。这将告诉我们，对于所有被拍卖的物品，如果那个物品是在1960年制造的，我们认为它将被卖出多少钱。这就是我们将在右侧绘制的内容。
- en: '![](../Images/73801b62b05a310161e3a70f12f68e0d.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73801b62b05a310161e3a70f12f68e0d.png)'
- en: And we are going to do the same thing for 1961.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为1961年做同样的事情。
- en: 'Question: To be clear, we’ve already fit the random forest and then we are
    just passing a new year and seeing what it determines the price should be [[1:17:10](https://youtu.be/0v93qHDqq_g?t=1h17m10s)]?
    Yes, so this is a lot like the way we did feature importance. But rather than
    randomly shuffling the column, we are going to replace the column with a constant
    value. Randomly shuffling the column tells us how accurate it is when you don’t
    use that column anymore. Replacing the whole column with a constant estimates
    for us how much we would have sold that product for in that auction on that day
    in that place if that product had been made in 1961\. We then take the average
    of all of the sale prices that we calculate from that random forest. We do it
    in 1961 and we get this value:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：明确一点，我们已经拟合了随机森林，然后我们只是传递一个新的年份，看看它确定的价格应该是多少？是的，这很像我们做特征重要性的方式。但是，我们不是随机洗牌列，而是用一个常数值替换列。随机洗牌列告诉我们当您不再使用该列时它有多准确。用一个常数值替换整个列为我们估计了如果那个产品是在1961年制造的，我们将在那天在那个地方的那个拍卖会上卖出那个产品多少钱。然后我们取所有从那个随机森林计算出的销售价格的平均值。我们在1961年这样做，得到这个值：
- en: '![](../Images/32c52873fe53ee9e45fc039141dc1eb2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32c52873fe53ee9e45fc039141dc1eb2.png)'
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/f160732f7a5a7de5e50ef30f746d7acc.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f160732f7a5a7de5e50ef30f746d7acc.png)'
- en: So what the partial dependence plot (PDP)here shows us is each of these light
    blue lines actually is showing us all 500 lines [[1:18:01](https://youtu.be/0v93qHDqq_g?t=1h18m1s)].
    So for row number 1 in our dataset, if we sold it in 1960, we are going to index
    that to zero so call that zero. If we sold it in 1970 that particular auction,
    it would have been here, etc. We actually plot all 500 predictions of how much
    every one of those 500 auctions would have gone for if we replaced its `YearMade`
    with each of these different values. Then this dark line is the average. So this
    tells us how much would we have sold on average all of those auctions for if all
    of those products were actually made in 1985, 1990, 1993, etc. So you can see,
    what’s happened here is at least in the period where we have a reasonable amount
    of data which is since 1990, this is basically a totally straight line — which
    is what you would except. Because if it was sold on the same date, and it was
    the same kind of tractor, sold to the same person in the same auction house, then
    you would expect more recent vehicles to be more expensive because of inflation
    and they are newer. You would expect that relationship to be roughly linear and
    that is exactly what we are finding. By removing all these externalities, it often
    allows us to see the truth much more clearly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的部分依赖图（PDP）向我们展示的是每一条浅蓝色线实际上都显示了所有500条线。因此，对于我们数据集中的第1行，如果我们在1960年卖出它，我们将将其索引为零，称之为零。如果在1970年卖出那个特定的拍卖品，它将在这里，等等。我们实际上绘制了所有500个预测，即如果我们用不同的值替换其`YearMade`，那么这500个拍卖品中的每一个将会卖出多少钱。然后这条深色线是平均值。因此，这告诉我们，如果所有这些产品实际上是在1985年、1990年、1993年等制造的，我们将平均卖出这些拍卖品多少钱。因此，您可以看到，这里发生的情况是，至少在我们有相当多数据的时期，即自1990年以来，这基本上是一条完全直线，这是您所期望的。因为如果在同一日期卖出，而且是同一种拖拉机，卖给同一个人在同一个拍卖行，那么您会期望更近期的车辆更昂贵，因为通货膨胀和它们是更新的。您会期望这种关系大致是线性的，这正是我们发现的。通过消除所有这些外部因素，通常能够更清楚地看到真相。
- en: 'This partial dependence plot is something which is using a random forest to
    get us a more clear interpretation of what’s going on in our data [[1:20:02](https://youtu.be/0v93qHDqq_g?t=1h20m2s)].
    The steps were:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部分依赖图是使用随机森林来更清晰地解释我们数据中发生的情况。步骤是：
- en: First of all look at the future importance to tell us which things do we think
    we care about.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先看一下未来的重要性，告诉我们我们认为我们关心哪些事情。
- en: Then to use the partial dependence plot to tell us what’s going on on average.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用部分依赖图告诉我们平均情况下发生了什么。
- en: 'There is another cool thing we can do with PDP which is we can use clusters.
    What clusters does is it uses cluster analysis to look at each one of the 500
    rows and say do some those 500 rows move in the same way. We could kind of see
    it seems like there’s a whole a lot of rows that go down and then up, and there
    seems to be a bunch of rows that go up and then go flat. It does seem like there’s
    some kind of different types of behaviors being hidden and so here is the result
    of doing that cluster analysis:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用PDP做另一件很酷的事情，那就是我们可以使用聚类。聚类的作用是利用聚类分析来查看这500行中的每一行，并判断这500行中是否有一些行以相同的方式移动。我们可以看到似乎有很多行是先下降然后上升，还有一些行是先上升然后趋于平缓。看起来似乎有一些不同类型的行为被隐藏了，所以这里是进行聚类分析的结果：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/94c8d8df61a3a304f4f04c4a170185f7.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: We still get the same average but it says here are five most common shapes that
    we see. And this is where you could then go in and say all right, it looks like
    some kinds of vehicle, after 1990, their prices are pretty flat. Before that,
    they were pretty linear. Some other kinds of vehicle were exactly the opposite,
    so different kinds of vehicle have these different shapes. So, this is something
    you could dig into.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然得到相同的平均值，但这里列出了我们看到的五种最常见的形状。这就是你可以进去并说好吧，看起来有些车辆在1990年后，它们的价格相当稳定。在那之前，它们是相当线性的。其他一些车辆则恰恰相反，所以不同种类的车辆有不同的形状。因此，这是你可以深入研究的内容。
- en: '**Question**: So what are we going to do with this information [[1:21:40](https://youtu.be/0v93qHDqq_g?t=1h21m40s)]?
    The purpose of interpretation is to learn about a dataset and so why do you want
    to learn about a dataset? It’s because you want to do something with it. So in
    this case, it’s not so much something if you are trying to win a Kaggle competition
    — it can be a little bit like some of these insights might make you realize I
    could transform this variable or create this interaction, etc. Obviously feature
    importance is super important for Kaggle competitions. But this one is much more
    for real life. So this is when you are talking to somebody and you say to them
    “okay, those plots you’ve been showing me which actually say that there was this
    kind of dip in prices based on things made between 1990 and 1997\. There wasn’t
    really. Actually they were increasing, and there was something else going on at
    that time.” It’s basically the thing that allows you to say for whatever this
    outcome I’m trying to drive in my business is, this is how something is driving
    it. So if it’s like I’m looking at advertising technology, what’s driving clicks
    that I I’m actually digging in to say okay, this is actually how clicks are being
    driven. This is actually the variable that’s driving it. This is how it’s related.
    So therefore, we should change our behavior in this way. That’s really the goal
    of any model. I guess there are two possible goals: one goal of a model is just
    to get the predictions, like if you are doing hedge fund trading, you probably
    want to know what the price of that equity is going to be. If you are doing insurance,
    you probably just want to know how much claims that guy is going to have. But
    probably most of the time, you are actually trying to change something about how
    you do business — how you do marketing, how you do logistics, so the thing you
    actually care about is how the things are related to each other.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：那么我们要如何处理这些信息呢？解释的目的是为了了解数据集，那么你为什么想要了解一个数据集呢？因为你想要对其进行某些操作。所以在这种情况下，如果你试图赢得Kaggle竞赛，这并不是什么大不了的事情——一些洞察可能让你意识到我可以转换这个变量或创建这种互动等等。显然，特征重要性对于Kaggle竞赛非常重要。但这更多地是为了现实生活。所以当你与某人交谈时，你对他们说“好的，你一直向我展示的那些图表实际上表明在1990年至1997年之间基于某些因素价格出现了下降。实际上并没有。实际上它们是在增长，那时发生了其他事情。”这基本上是让你说出，无论我试图在我的业务中推动的结果是什么，这就是某种驱动力。所以如果我在看广告技术，是什么在推动点击，我实际上正在深入研究，看看点击是如何被推动的。这实际上是在推动它的变量。这是它们之间的关系。因此，我们应该以这种方式改变我们的行为。这实际上是任何模型的目标。我想有两个可能的目标：一个模型的目标只是为了获得预测，比如如果你在进行对冲基金交易，你可能想知道那只股票的价格会是多少。如果你在做保险，你可能只想知道那个人会有多少索赔。但大多数情况下，你实际上是在尝试改变你的业务方式——你如何做市场营销，如何做物流，所以你真正关心的是这些事物之间的关系。'
- en: '![](../Images/f22e74acac8333f45ef97c4b654dac45.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法识别图片中的文本。如果您能提供文本内容，我将很乐意帮助您翻译。
- en: '**Question**: Could you explain again why the dip did not signify what we thought
    it did [[1:23:36](https://youtu.be/0v93qHDqq_g?t=1h23m36s)]? Yes. So this is a
    classic boring univariate plot. So this is just taking all of the dots, all of
    the options, plotting YearMade against SalePrice and we are just fitting a rough
    average through them. It’s true that the products made between 1992 and 1997 on
    average in our dataset are being sold for less. Very often in business, you’ll
    hear somebody look at something like this and say “ we should stop auctioning
    equipment that is made in those years because we are getting less money for”,
    for example. But if the truth actually is that during those years, it’s just that
    people were making more small industrial equipment where you would expect it to
    be sold for less and actually our profit on it is just as high, for instance.
    Or it’s not that things made during those years now would now be cheaper, it’s
    that when we were selling things in those years, they were cheaper because there
    was a recession going on. If you are trying to actually take some action based
    on this, you probably don’t just care about the fact that things made in those
    years are cheaper on average, but how does that impact today. So PDP approach
    where we actually say let’s try and remove all of these externalities. So if something
    is sold on the same day to the same person of the same kind of vehicle, then actually
    how does year made impact the price. This basically says, for example, if I am
    deciding what to buy at an auction, then this is saying to me that getting a more
    recent vehicle on average really does give you more money which is not what the
    naive univariate plot said.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：你能再解释一下为什么这个下降并不意味着我们所认为的吗？是的。这是一个经典的无聊的单变量图。这只是将所有的点，所有的选项，将制造年份与销售价格进行绘图，并且我们只是通过它们拟合一个粗略的平均值。在我们的数据集中，1992年至1997年制造的产品平均销售价格较低。在商业中，你经常会听到有人看到这样的情况并说“我们应该停止拍卖那些在这些年份制造的设备，因为我们得到的钱更少”，例如。但事实上，可能是在那些年份，人们制造了更多的小型工业设备，你会期望它们的售价更低，而实际上我们的利润也同样高。或者并不是那些年份制造的东西现在会更便宜，而是在那些年份卖东西时，它们更便宜，因为当时正值经济衰退。如果你真的想根据这个采取一些行动，你可能并不只关心那些年份制造的东西平均更便宜，而是这对今天有什么影响。因此，我们采用PDP方法，实际上是说让我们尝试消除所有这些外部因素。因此，如果同一天向同一人出售同一种类型的车辆，那么实际上制造年份如何影响价格。这基本上是说，例如，如果我在拍卖会上决定买什么，那么这对我来说意味着平均而言，购买一辆更近期的车辆确实会给你更多的钱，这并不是单变量图所说的。
- en: '**Comment**: Bulldozers made in 2010 probably are not close to the type of
    bulldozers that were made in 1960\. If you are taking something that would be
    so very different, like a 2010 bulldozer, and then trying to just drop it to say
    “oh if it was made in 1960” that may cause poor prediction at a point because
    it’s so far outside of the training set [[1:26:12](https://youtu.be/0v93qHDqq_g?t=1h26m12s)].
    Absolutely. That’s a good point. It is a limitation, however, if you’ve got a
    datapoint that’s in a part of the space that it has not seen before, like maybe
    people didn’t put air conditioning in bulldozers in 1960 and you are saying how
    much would this bulldozer with air conditioning would have gone for in 1960, you
    don’t really have any information to know that. This is still the best technique
    I know of but it’s not perfect. And you kind of hope that the trees are still
    going to find some useful truth even though it hasn’t seen that combination of
    features before. But yeah, it’s something to be aware of.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**评论**：2010年生产的推土机可能与1960年生产的推土机类型不太接近。如果你拿一个非常不同的东西，比如2010年的推土机，然后试图说“哦，如果它是1960年生产的”，这可能会导致预测不准确，因为它远远超出了训练集的范围。绝对。这是一个很好的观点。然而，这是一个限制，如果你有一个数据点在它以前没有见过的空间中，比如也许1960年的推土机没有安装空调，你在说这台带空调的推土机在1960年会卖多少钱，你实际上没有任何信息来知道这一点。这仍然是我知道的最好的技术，但并不完美。你希望树仍然能找到一些有用的真相，即使它以前没有见过这些特征的组合。但是，是的，这是需要注意的事情。'
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/95f7f616311655fbea375623a66c6d69.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法识别图片中的文本。如果您能提供文本内容，我将很乐意帮助您翻译。
- en: You can also do the same thing in a PDP interaction plot [[1:27:36](https://youtu.be/0v93qHDqq_g?t=1h27m36s)].
    And PDP interaction plot which is really what I’m trying to get to here is how
    does saleElapsed and YearMade together impact the price. If I do a PDP interaction
    plot, it shows me saleElapsed vs. price, YearMade vs. price, and the combination
    vs. price. Remember, this is always log of price. That’s why these prices look
    weird. You can see that the combination of saleElapsed and YearMade is as you
    would expect —the highest prices are those where there’s the least elapsed and
    the most recent year made. The upper right is the univariate relationship between
    saleElapsed and price, the lower left is the univariate relationship between YearMade
    and price, and the lower right is the combination of the two. It’s enough to see
    clearly that these two things are driving price together. You can also see these
    are not simple diagonal lines so there is some interesting interaction going on.
    Based on looking at these plots, it’s enough to make me think, oh, we should maybe
    put in some kind of interaction term and see what happens. So let’s come back
    to that in a moment, but let’s just look at a couple more.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在PDP交互图中执行相同的操作[[1:27:36](https://youtu.be/0v93qHDqq_g?t=1h27m36s)]。而我真正想要的PDP交互图是，saleElapsed和YearMade如何共同影响价格。如果我做一个PDP交互图，它会显示给我saleElapsed
    vs. price，YearMade vs. price，以及两者的组合 vs. price。请记住，这里始终是价格的对数。这就是为什么这些价格看起来很奇怪。您可以看到saleElapsed和YearMade的组合正如您所期望的那样——价格最高的是那些经过的时间最短和最近制造的年份。右上角是saleElapsed和价格之间的单变量关系，左下角是YearMade和价格之间的单变量关系，右下角是两者的组合。足以清楚地看到这两个因素共同推动价格。您还可以看到这些不是简单的对角线，因此存在一些有趣的交互作用。根据观察这些图，我认为，也许我们应该加入某种交互项并看看会发生什么。所以让我们稍后再回到这个问题，但让我们先看几个例子。
- en: Remember, in this case, we did one-hot-encoding — way back at the top, we said
    `max_n_cat=7` [[1:29:18](https://youtu.be/0v93qHDqq_g?t=1h29m18s)]. So we have
    things like `Enclosure_EROPS w AC`. So if you have one-hot-encoded variables,
    you can pass an array of them to `plot_pdp` and it will treat them as a category.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这种情况下，我们进行了独热编码——在最开始时，我们说`max_n_cat=7`[[1:29:18](https://youtu.be/0v93qHDqq_g?t=1h29m18s)]。因此，我们有像`Enclosure_EROPS
    w AC`这样的变量。因此，如果您有独热编码的变量，您可以将它们的数组传递给`plot_pdp`，它将把它们视为一个类别。
- en: '![](../Images/107209cf69c2ed2bdbd5a5e08737fb09.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/107209cf69c2ed2bdbd5a5e08737fb09.png)'
- en: So in this case, I’m going to create a PDP plot of these three categories, and
    I’m going to call it “Enclosure”.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，我将创建这三个类别的PDP图，并将其命名为“Enclosure”。
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/fed9b9a0979008b80056330d178201c1.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fed9b9a0979008b80056330d178201c1.png)'
- en: I can see here that `Enclosure_EROPS w AC` on average are more expensive than
    `Enclosure_EROPS` or `Enclosure_OROPS`. It actually looks like the latter two
    are pretty similar or else `Enclosure_EROPS w AC` is higher. So at this point,
    I’m probably being inclined to hop on to Google and type “erops orops” and find
    out what these things are and here we go.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以看到`Enclosure_EROPS w AC`的平均价格要高于`Enclosure_EROPS`或`Enclosure_OROPS`。实际上，后两者看起来相似，或者`Enclosure_EROPS
    w AC`更高。因此，此时我可能倾向于跳转到Google并搜索“erops orops”以了解这些内容，然后我们继续。
- en: '![](../Images/6c13347106d0d318066becc5d024c38f.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c13347106d0d318066becc5d024c38f.png)'
- en: So it turns out that EROPS is enclosed rollover protective structure and so
    it turns out that if your bulldozer is fully enclosed then optionally you can
    also get air conditioning. So actually this thing is telling us whether it has
    air conditioning. If it’s an open structure, then obviously you don’t have air
    conditioning at all. So that’s what these three levels are. So we’ve now learnt
    all other things being equal, the same bulldozer, sold at the same time, built
    at the same time, sold to the same person is going to be quite a bit more expensive
    if it has air conditioning than if it doesn’t. So again, we are getting this nice
    interpretation ability. Now that I spent some time with this dataset, I’d certainly
    noticed that knowing this is the most important thing, you do notice that there
    is a lot more air conditioned bulldozers nowadays than they used to be and so
    there is definitely an interaction between date and that.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，EROPS是封闭式翻转保护结构，因此如果您的推土机完全封闭，则可以选择安装空调。因此，这实际上告诉我们它是否有空调。如果是开放式结构，那么显然根本没有空调。这就是这三个级别的含义。因此，我们现在知道，其他条件相同的情况下，同一时间销售，同一时间制造，销售给同一人的推土机，如果有空调，价格会比没有空调的要高得多。因此，我们再次获得了这种很好的解释能力。现在我花了一些时间处理这个数据集，我肯定注意到了知道这一点是最重要的事情，您会注意到现在有更多的带空调的推土机，比过去有更多，因此日期和这之间肯定存在交互作用。
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/fa60744a6a711852413ca5c1fe5da5ad.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa60744a6a711852413ca5c1fe5da5ad.png)'
- en: Based on the earlier interaction analysis, I’ve tried, first of all, setting
    everything before 1950 to 1950 because it seems to be some kind of missing value
    [[1:31:25](https://youtu.be/0v93qHDqq_g?t=1h31m25s)]. I’ve set `age` to be equal
    to `saleYear - YearMade`. Then I tried running a random forest on that. Indeed,
    `age` is now the single biggest thing, saleElapsed is way back down here, YearMade
    is back down here. So we’ve used this to find an interaction. But remember, of
    course a random forest can create an interaction through having multiple split
    points, so we shouldn’t assume that this is actually going to be a better result.
    And in practice, I actually found when I looked at my score and my RMSE, adding
    age was actually a little worse. We will see about that later probably in the
    next lesson.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的交互分析，首先我尝试将1950年之前的所有内容设置为1950年，因为这似乎是某种缺失值[[1:31:25](https://youtu.be/0v93qHDqq_g?t=1h31m25s)]。我将`age`设置为`saleYear
    - YearMade`。然后我尝试在此基础上运行随机森林。确实，`age`现在是最重要的因素，saleElapsed远远落后，YearMade也落后。因此，我们使用这个找到了一个交互作用。但请记住，随机森林可以通过具有多个分割点来创建交互作用，因此我们不应该假设这实际上会带来更好的结果。实际上，当我查看我的得分和RMSE时，我发现添加age实际上效果稍差。也许在下一节课中我们会看到更多相关内容。
- en: Tree interpreter [[1:32:34](https://youtu.be/0v93qHDqq_g?t=1h32m34s)]
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树解释器[[1:32:34](https://youtu.be/0v93qHDqq_g?t=1h32m34s)]
- en: One last thing is tree interpreter. This is also in the category of things that
    most people don’t know exists, but it’s super important. Almost pointless for
    Kaggle competitions but super important for real life. Here is the idea. Let’s
    say you are an insurance company and somebody rings up and you give them a quote
    and they say “oh, that’s $500 more than last year. Why?” So in general, you’ve
    made a prediction from some model and somebody asks why. This is where we use
    this method called tree interpreter. What tree interpreter does is it allows us
    to take a particular row.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一件事是树解释器。这也属于大多数人不知道存在的事物类别，但它非常重要。对于Kaggle竞赛几乎毫无意义，但对于现实生活非常重要。这是个想法。假设你是一家保险公司，有人打电话给你，你给他们报价，他们说“哦，比去年贵了500美元。为什么？”总的来说，你从某个模型中做出了预测，有人问为什么。这就是我们使用的这种叫做树解释器的方法。树解释器的作用是允许我们取出特定的一行。
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So in this case, we are going to pick row number zero.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这种情况下，我们将选择零行。
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here are all the columns in row zero.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是零行中的所有列。
- en: '![](../Images/1733bb765d330099d0a7d63c7540e124.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1733bb765d330099d0a7d63c7540e124.png)'
- en: 'What I can do with a tree interpreter is I can go `ti.predict`, pass in my
    random forest and my row (so this would be like this particular customer’s insurance
    information, or in this case this particular auction). And it will give me back
    three things:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以用树解释器做的是，我可以调用`ti.predict`，传入我的随机森林和我的行（这将是这个特定客户的保险信息，或者在这种情况下是这个特定拍卖）。它会给我三件事：
- en: '`prediction`: The prediction from the random forest'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`预测`: 随机森林的预测'
- en: '`bias`: The average sale price across the whole original dataset'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`偏差`: 整个原始数据集的平均销售价格'
- en: '`contributions`: A column and the value to split by (i.e. the predictor), and
    how much it changed the predicted value.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`贡献度`: 一列和要拆分的值（即预测器），以及它对预测值的影响有多大。'
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So you can think of it this way [[1:34:51](https://youtu.be/0v93qHDqq_g?t=1h34m51s)].
    The whole dataset had an average log sale price of 102\. The dataset for those
    with `Coupler_system ≤ 0.5` had an average of 10.3\. The dataset for `Coupler_system
    ≤ 0.5` and `Enclosure ≤ 2.0` was 9.9, and then eventually we get all the way up
    here and also with `ModelID ≤ 4573.0`, it’s 10.2\. So you could ask, okay, why
    did we predict 10.2 for this particular row?
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以这样想[[1:34:51](https://youtu.be/0v93qHDqq_g?t=1h34m51s)]。整个数据集的平均对数销售价格为102。那些`联接器系统≤0.5`的数据集平均为10.3。`联接器系统≤0.5`和`围栏≤2.0`的数据集为9.9，然后最终我们一直到这里，还有`ModelID≤4573.0`，为10.2。所以你可以问，为什么我们为这个特定行预测了10.2？
- en: '![](../Images/ea69e6c9aeb64e4556bd5a52318b64f9.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea69e6c9aeb64e4556bd5a52318b64f9.png)'
- en: 'That is because we started with 10.19:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们从10.19开始：
- en: Because the coupler system was less than .3, we added about .2 to that (so we
    went from 10.19 to 10.34).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为联接器系统小于0.3，我们增加了大约0.2（所以我们从10.19增加到10.34）。
- en: Because enclosure was less than 2, we subtracted about .4.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为围栏小于2，我们减去了大约0.4。
- en: Then because model ID was less than 4573, we added about .7
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后因为模型ID小于4573，我们增加了大约0.7
- en: So you can see with a single tree, you could break down why is it that we predicted
    10.2\. At each one of these decision points, we are adding or subtracting a little
    bit from the value. What we could then do is we could do that for all the trees
    and then we could take the average. So every time we see enclosure did we increase
    or decrease the value and by how much? Every time we see model ID, did we increase
    or decrease the value and by how much? We could take the average of all of those
    and that’s what ends up in this thing called `contributions`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，通过一棵树，你可以分解为什么我们预测了10.2。在每一个决策点，我们都会对值进行一点点的加减。然后我们可以对所有树都这样做，然后我们可以取平均值。每次我们看到围栏，我们增加还是减少了值，以及多少？每次我们看到模型ID，我们增加还是减少了值，以及多少？我们可以取所有这些的平均值，这就是所谓的`贡献度`。
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So here is all of our predictors and the value of each [[1:37:54](https://youtu.be/0v93qHDqq_g?t=1h37m54s)].
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们所有的预测因子和每个值[[1:37:54](https://youtu.be/0v93qHDqq_g?t=1h37m54s)]。
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*There was an issue with sorting in the video as it was not using the index
    sort, but above example is the corrected version.*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*视频中存在排序问题，因为没有使用索引排序，但上面的示例是已更正的版本。*'
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So then there is this thing called bias and the bias is just the average before
    we start doing any splits [[1:39:03](https://youtu.be/0v93qHDqq_g?t=1h39m3s)].
    If you start with the average log of value and then we went down each tree and
    each time we saw YearMade, we had some impact, coupler system some impact, product
    size some impact, and so forth.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一个叫做偏差的东西，偏差只是我们在开始进行任何拆分之前的平均值[[1:39:03](https://youtu.be/0v93qHDqq_g?t=1h39m3s)]。如果你从平均对数值开始，然后我们沿着每棵树走，每次看到YearMade时，我们有一些影响，联接器系统有一些影响，产品尺寸有一些影响，等等。
- en: We might come back to tree interpreter next time, but the basic idea (this is
    the last of our key interpretation points) is that we want some ability to not
    only tell us about the model as a whole and how it works on average, but to look
    at how the model makes prediction for an individual row. And that’s what we are
    doing here.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会在下次回到树解释器，但基本思想（这是我们关键解释点的最后一个）是，我们希望不仅能告诉我们关于整个模型以及平均工作原理的信息，还要查看模型如何为单个行进行预测。这就是我们在这里所做的。
