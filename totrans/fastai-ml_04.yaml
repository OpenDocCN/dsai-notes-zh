- en: 'Machine Learning 1: Lesson 4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第4课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d)'
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自* [*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*
    的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢* [*Jeremy*](https://twitter.com/jeremyphoward)
    *和* [*Rachel*](https://twitter.com/math_rachel) *给了我这个学习的机会。*'
- en: '**A question before getting started:** Could we summarize the relationship
    between the hyper parameters of the random forest and its effect on overfitting,
    dealing with collinearity, etc?[[1:51](https://youtu.be/0v93qHDqq_g?t=1m51s)]
    Absolutely. Going back to the [lesson 1 notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson1-rf.ipynb).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前有一个问题：我们能否总结随机森林的超参数与过拟合、处理共线性等之间的关系？绝对可以。回到[第1课笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson1-rf.ipynb)。
- en: 'Hyper parameters of interest:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的超参数：
- en: '**1.** `set_rf_samples`'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 设置_rf_samples
- en: Determines how many rows are in each tree. So before we start a new tree, we
    either bootstrap a sample (i.e. sampling with replacement from the whole thing)
    or we pull out a subsample of a smaller number of rows and then we build a tree
    from there.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定每棵树中有多少行。因此，在我们开始新树之前，我们要么对整个数据进行自助抽样（即有放回地抽样），要么从中抽取较少行数的子样本，然后从中构建一棵树。
- en: Step 1 is we have our whole big dataset, we grab a few rows at random from it,
    and we turn them into a smaller dataset. From that, we build a tree.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步是我们有一个完整的大数据集，我们随机抽取几行数据，并将它们转换成一个较小的数据集。然后，我们构建一棵树。
- en: '![](../Images/fc76565056f12bb5bb46993090bf0ba6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法描述图片中的内容。如果您可以提供文本描述，我将很乐意帮助翻译。
- en: Assuming that the tree remains balanced as we grow it, how many layers deep
    will this tree be (assuming we are growing it until every leaf is of size one)?
    log2(20000). The depth of the tree doesn’t actually vary that much depending on
    the number of samples because it is related to the log of the size.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设树在我们生长过程中保持平衡，这棵树将有多少层深（假设我们生长到每个叶子的大小为一）？log2(20000)。树的深度实际上并不会因为样本数量的不同而变化太大，因为它与大小的对数相关。
- en: Once we go all the way down to the bottom, how many leaf nodes would there be?
    20K. We have a linear relationship between the number of leaf nodes and the size
    of the sample. So when you decrease the sample size, there are less final decisions
    that can be made. Therefore, the tree is going to be less rich in terms of what
    it can predict because it is making less different individual decisions and it
    also is making less binary choices to get to those decisions.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们一直走到底部时，会有多少叶节点？20K。叶节点的数量与样本大小之间存在线性关系。因此，当你减少样本大小时，可以做出的最终决策就会减少。因此，树在预测方面会变得不那么丰富，因为它做出的个别决策更少，也做出更少的二元选择来达到这些决策。
- en: Setting RF samples lower is going to mean that you overfit less, but it also
    means that you are going to have a less accurate individual tree model. The way
    Breiman, the inventor of random forest, described this is that you are trying
    to do two things when you build a model with bagging. One is that each individual
    tree/estimator is as accurate as possible (so each model is a strong predictive
    model). But then the across the estimators, correlation between them is as low
    as possible sot hat when you average them out together, you end up with something
    that generalizes. By decreasing the `set_rf_samples` number, we are actually decreasing
    the power of the estimator and increasing the correlation — so is that going to
    result in a better or worse validation set result for you? It depends. This is
    the kind of compromise which you have to figure out when you do machine learning
    models.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RF样本设置较低意味着过拟合的可能性较小，但也意味着每个单独的树模型的准确性会降低。随机森林的发明者Breiman描述了这一点，即在使用装袋法构建模型时，你要做两件事情。一是确保每个单独的树/估计器尽可能准确（因此每个模型都是一个强预测模型）。但是在估计器之间，相关性要尽可能低，这样当将它们平均在一起时，你会得到一个泛化的模型。通过降低`set_rf_samples`的数量，实际上是降低了估计器的能力并增加了相关性，那么这会对你的验证集结果产生更好还是更差的影响呢？这取决于情况。这就是在进行机器学习模型时必须要考虑的妥协。
- en: A question about `oob=True` [[6:46](https://youtu.be/0v93qHDqq_g?t=6m46s)].
    All `oob=True` does is it says whatever your subsample is (it might be a bootstrap
    sample or a subsample), take all of the other rows (for each tree), put them into
    a different data set, and calculate the error on those. So it doesn’t actually
    impact training at all. It just gives you an additional metric which is the OOB
    error. So if you don’t have a validation set, then this allows you to get kind
    of a quasi validation set for free.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`oob=True`的问题[[6:46](https://youtu.be/0v93qHDqq_g?t=6m46s)]。`oob=True`的作用就是说，无论你的子样本是什么（可能是一个自助采样或一个子样本），将所有其他行（对于每棵树）放入一个不同的数据集中，并计算这些行的错误。因此，它实际上并不影响训练。它只是给你一个额外的度量，即OOB错误。因此，如果你没有验证集，那么这允许你免费获得一种准验证集。
- en: 'Question: If I don’t do `set_rf_samples`, what would it be called? [[7:55](https://youtu.be/0v93qHDqq_g?t=7m55s)]
    The default is, if you say `reset_rf_samples`, that causes it to bootstrap, so
    it will sample a new dataset as big as the original one but with replacement.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：如果我不执行`set_rf_samples`，那会被称为什么？默认情况是，如果你说`reset_rf_samples`，那会导致引导，因此它将对原始数据集进行重新采样，但是会有替换。
- en: The second benefit of `set_rf_samples` is that you can run more quickly [[8:28](https://youtu.be/0v93qHDqq_g?t=8m28s)].
    Particularly if you are running on a really large dataset like a hundred million
    rows, it will not be possible to run it on the full dataset. So you would either
    have to pick a subsample yourself before you start or you `set_rf_samples`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`set_rf_samples`的第二个好处是你可以更快地运行。特别是当你在一个非常庞大的数据集上运行，比如一亿行，就不可能在完整的数据集上运行。所以你要么在开始之前自己选择一个子样本，要么使用`set_rf_samples`。'
- en: '**2.** `min_samples_leaf` [[8:48](https://youtu.be/0v93qHDqq_g?t=8m48s)]'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2. `min_samples_leaf` [[8:48](https://youtu.be/0v93qHDqq_g?t=8m48s)]
- en: Before, we assumed that `min_samples_leaf=1`, if it is set to 2, the new depth
    of the tree is `log2(20000)-1`. Each time we double the `min_samples_leaf` , we
    are removing one layer from the tree, and halving the number of leaf nodes (i.e.
    10k). The result of increasing `min_samples_leaf` is that now each of our leaf
    nodes has more than one thing in, so we are going to get a more stable average
    that we are calculating in each tree. We have a little less depth (i.e. we have
    less decisions to make) and we have a smaller number of leaf nodes. So again,
    we would expect the result of that node would be that each estimator would be
    less predictive, but the estimators would be also less correlated. So this might
    help us avoid overfitting.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们假设`min_samples_leaf=1`，如果设置为2，树的新深度为`log2(20000)-1`。每次将`min_samples_leaf`加倍，我们都会从树中移除一层，并将叶节点数量减半（即10k）。增加`min_samples_leaf`的结果是现在每个叶节点中都有多于一个元素，因此我们在每棵树中计算的平均值会更加稳定。我们的深度稍微减少（即我们需要做出的决策更少），叶节点数量也减少。因此，我们预期每个估算器的结果会更少预测性，但估算器之间的相关性也会减少。这可能有助于我们避免过拟合。
- en: '**Question**: I am not sure if every leaf node will have exactly two nodes
    [[10:33](https://youtu.be/0v93qHDqq_g?t=10m33s)]. No, it won’t necessarily have
    exactly two. The example of uneven split such as a leaf node containing 100 items
    is when they are all the same in terms of the dependent variable (suppose either,
    but much more likely would be the dependent). So if you get to a leaf node where
    every single one of them has the same auction price, or in classification every
    single one of them is a dog, then there is no split that you can do that’s going
    to improve your information. Remember, **information** is the term we use in a
    general sense in random forest to describe the amount of difference about the
    additional information we create from a split is how much we are improving the
    model. So you will often see this word information gain which means how much better
    the model got by adding an additional split point, and it could be based on RMSE
    or cross-entropy or how different to the standard deviations, etc.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我不确定每个叶节点是否一定会有两个节点。不，不一定会有两个。不均匀分裂的例子，比如一个叶节点包含100个项目，当它们在因变量方面都相同时（假设是这样，但更有可能是因变量）。所以，如果你到达一个叶节点，每一个都有相同的拍卖价格，或者在分类中每一个都是一只狗，那么你无法进行任何可以改善你的信息的分裂。记住，“信息”是我们在随机森林中使用的一个术语，用来描述我们从分裂中创造的额外信息的差异量，我们通过分裂改善模型的程度。所以你经常会看到这个词“信息增益”，意思是通过添加额外的分裂点，模型变得更好了多少，这可能基于RMSE或交叉熵或与标准差的差异等。
- en: So that is the second thing that we can do. It’s going to speed up our training
    because it has one less set of decisions to make. Even though there is one less
    set of decisions, those decisions have as much data as the previous set. So each
    layer of the tree can take twice as long as the previous layer. So it could definitely
    speed up training and generalize better.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们可以做的第二件事情。这将加快我们的训练速度，因为它少了一组决策要做。尽管少了一组决策，但这些决策的数据量与之前的一样多。因此，树的每一层可能比前一层花费的时间多一倍。因此，它肯定可以加快训练速度并且泛化得更好。
- en: '**3.** `max_features` [[12:22](https://youtu.be/0v93qHDqq_g?t=12m22s)]'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** `max_features` [[12:22](https://youtu.be/0v93qHDqq_g?t=12m22s)]'
- en: At each split, it will randomly sample columns (as opposed to `set_rf_samples`
    pick a subset of rows for each tree). It sounds like a small difference but it’s
    actually quite a different way of thinking about it. We do `set_rf_samples` so
    we pull out our sub sample or a bootstrap sample and that’s kept for the whole
    tree and we have all of the columns in there. With `max_features=0.5`, at each
    split, we’d pick a different half of the features. The reason we do that is because
    we want the trees to be as rich as possible. Particularly, if you were only doing
    a small number of trees (e.g. 10 trees) and you picked the same column set all
    the way through the tree, you are not really getting much variety in what kind
    of things it can find. So this way, at least in theory, seems to be something
    which is going to give us a better set of trees by picking a different random
    subset of features at every decision point.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次分裂时，它会随机抽样列（与`set_rf_samples`选择每棵树的行子集相对）。听起来是一个小的区别，但实际上这是一种完全不同的思考方式。我们使用`set_rf_samples`来提取我们的子样本或自举样本，并将其保留整个树中，其中包含所有列。使用`max_features=0.5`，在每次分裂时，我们会选择不同的一半特征。我们这样做的原因是因为我们希望树尽可能丰富。特别是，如果您只做了少量的树（例如10棵树），并且在整个树中选择了相同的列集，那么您实际上并没有获得太多不同种类的发现。因此，这种方式，至少在理论上，似乎会通过在每个决策点处选择不同的随机特征子集来给我们提供更好的树集。
- en: The overall effect of the max_features is the same — it’s going to mean that
    each individual tree is probably going to be less accurate but the trees are going
    to be more varied. In particular, here this can be critical because imagine that
    you got one feature that is just super predictive. It’s so predictive that every
    random subsample you look at always starts out by splitting on that same feature
    then the trees are going to be very similar in the sense they all have the same
    initial split. But there may be some other interesting initial splits because
    they create different interactions of variables. So by half the time that feature
    won’t even be available at the top of the tree, at least half the tree are going
    to have a different initial split. It definitely can give us more variation and
    therefore it can help us to create more generalized trees that have less correlation
    with each other even though the individual trees probably won’t be as predictive.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: max_features的整体效果是相同的 - 这意味着每棵单独的树可能会更不准确，但树的变化会更多。特别是在这里，这可能是至关重要的，因为想象一下，你有一个特征是非常具有预测性的。它是如此具有预测性，以至于你查看的每个随机子样本总是从相同的特征开始分裂，那么这些树在某种意义上将非常相似，因为它们都具有相同的初始分裂。但可能会有一些其他有趣的初始分裂，因为它们会创建不同的变量交互。因此，有一半的时间该特征甚至不会出现在树的顶部，至少有一半的树会有不同的初始分裂。这绝对可以给我们更多的变化，因此可以帮助我们创建更具一般性的树，这些树之间的相关性更小，即使单独的树可能不会那么具有预测性。
- en: '![](../Images/adfc1111b4f4884bc85204c6e99302e6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adfc1111b4f4884bc85204c6e99302e6.png)'
- en: In practice, as you add more trees, if you have `max_features=None`, that is
    going to use all the features every time. Then with very few trees, that can still
    give you a pretty good error. But as you create more trees, it’s not going to
    help as much because they are all pretty similar as they are all trying every
    single variable. Where else, if you say `max_features=sqrt` or `log2` , then as
    we add more estimators, we see improvements so there is an interesting interaction
    between those two. The chart above is from scikit-learn docs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，当你添加更多的树时，如果你设置`max_features=None`，那么每次都会使用所有的特征。然后在很少的树的情况下，这仍然可以给你一个相当不错的误差。但是随着你创建更多的树，它不会帮助太多，因为它们都很相似，它们都在尝试每一个变量。另外，如果你设置`max_features=sqrt`或`log2`，那么随着我们添加更多的估计器，我们会看到改进，所以这两者之间存在有趣的互动。上面的图表来自scikit-learn文档。
- en: '**4\.** Things which do not impact our training at all [[16:32](https://youtu.be/0v93qHDqq_g?t=16m32s)]'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\.** 完全不影响我们训练的事情'
- en: '`n_jobs`: simply specifies how many CPU or cores we run on, so it’ll make it
    faster up to a point. Generally speaking, making this more than 8 or so, they
    may have diminishing returns. -1 says use all of your cores. It seems weird that
    the default is to use one core. You will definitely get more performance by using
    more cores because all of you have computers with more than one core nowadays.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_jobs`：简单地指定我们运行在多少个 CPU 或核心上，因此在某种程度上会使其更快。一般来说，将其设置为超过 8 个左右，可能会有递减的回报。-1
    表示使用所有核心。默认使用一个核心似乎有点奇怪。通过使用更多核心，您肯定会获得更好的性能，因为现在大多数计算机都有多个核心。'
- en: '`oob_score=True`: This simply allows us to see OOB score. If you had set_rf_samples
    pretty small compared to a big dataset, OOB is going to take forever to calculate.
    Hopefully at some point, we will be able to fix the library so that doesn’t happen.
    There is no reason that need to be that way, but right now, that’s how the library
    works.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`oob_score=True`: 这只是让我们看到OOB得分。如果你将set_rf_samples设置得相对较小，而数据集很大，OOB将需要很长时间来计算。希望在某个时候，我们能够修复库，使其不再发生这种情况。没有理由需要那样，但目前，库就是这样工作的。'
- en: 'So they are our key basic parameters we can change [[17:38](https://youtu.be/0v93qHDqq_g?t=17m38s)].
    There are more that you can see in the docs or `shift+tab` to have a look at them,
    but the ones you’ve seen are the ones that I’ve found useful to play with so feel
    free to play with others as well. Generally speaking, these values work well:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们是我们可以更改的关键基本参数。您可以在文档中查看更多内容，或者按`shift+tab`查看它们，但您已经看到的是我发现有用的，可以随意尝试其他参数。一般来说，这些值效果很好。
- en: '`max_features`: None, 0.5, sqrt, log2'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf` : 1, 3, 5, 10, 25, 100… As you increase, if you notice by
    the time you get to 10, it’s already getting worse then there is no point going
    further. If you get to 100 and it’s still going better, then you can keep trying.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_leaf` : 1, 3, 5, 10, 25, 100… 随着增加，如果你注意到当你达到10时，情况已经变得更糟，那么继续下去就没有意义了。如果你达到100时情况仍在好转，那么你可以继续尝试。'
- en: Random Forest Interpretation [[18:50](https://youtu.be/0v93qHDqq_g?t=18m50s)]
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林解释 [[18:50](https://youtu.be/0v93qHDqq_g?t=18m50s)]
- en: Random forest interpretation is something which you could use to create some
    really cool Kaggle kernels. Confidence based on tree variance is something which
    doesn’t exist anywhere else. Feature importance definitely does and that’s already
    in quite a lot of Kaggle kernels. If you are looking at a competition or a dataset
    where nobody’s done feature importance, being the first person to do that is always
    going to win lots of votes because the most important thing is which features
    are important.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林解释是你可以用来创建一些非常酷的Kaggle内核的东西。基于树方差的置信度是其他地方不存在的。特征重要性肯定存在，并且已经在许多Kaggle内核中。如果你正在看一个竞赛或一个数据集，没有人做过特征重要性，成为第一个这样做的人总是会赢得很多票，因为最重要的是哪些特征是重要的。
- en: Confidence based on tree variance [[20:43](https://youtu.be/0v93qHDqq_g?t=20m43s)]
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于树方差的置信度
- en: As I mentioned, when we do model interpretation, I tend to `set_rf_samples`
    to some subset — something small enough that I can run a model in under 10 seconds
    because there is no point running a super accurate model. Fifty thousand is more
    than enough to see each time you run an interpretation, you’ll get the same results
    back and so as long as that’s true, then you are already using enough data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所提到的，当我们进行模型解释时，我倾向于将`set_rf_samples`设置为某个子集——足够小，可以在不到10秒内运行一个模型，因为运行一个超级准确的模型没有意义。五万个样本已经足够了，每次运行解释时，你会得到相同的结果，只要这是真的，那么你已经在使用足够的数据了。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Feature Importance [[21:14](https://youtu.be/0v93qHDqq_g?t=21m14s)]
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: We learnt it works by randomly shuffling a column, each column one at a time,
    then seeing how accurate the pre-trained model is when you pass that in all the
    data as before but with one column shuffled.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到它是通过随机洗牌一列，每次一列，然后看看在将所有数据传递给预训练模型时，当其中一列被洗牌时，模型的准确性如何。
- en: Some of the questions I got after class reminded me that it is very easy to
    under appreciate how powerful and magic this approach is. To explain, I’ll mention
    a couple of the questions I heard.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 课后我收到的一些问题让我想起，很容易低估这种方法有多么强大和神奇。为了解释，我会提到我听到的一些问题。
- en: One question was “what if we just took one column at a time, and created a tree
    on just that column”. Then we will see which column’s tree is the most predictive.
    Why may that give misleading results about feature importance? We are going to
    lose the interactions between the features. If we just shuffle them, it will add
    randomness and we are able to both capture the interactions and the importance
    of the feature. This issue of interaction is not a minor detail. It is massively
    important. Think about this bulldozers dataset where, for example, there is one
    field called “year made” and another field called “sale date.” If we think about
    it, it’s pretty obvious that what matters is the combination of these two. In
    other words, the difference between the two is how old the piece of the equipment
    was when it got sold. So if we only included one of these, we are going to massively
    underestimate how important that feature is. Now, here is a really important point
    though. It’s pretty much always possible to create a simple logistic regression
    which is as good as pretty much any random forest if you know ahead of time exactly
    what variables you need, exactly how they interact, exactly how they need to be
    transformed. In this case, for example, we could have created a new field which
    was equal to sale year minus year made and we could have fed that to a model and
    got that interaction for us. But the point is, we never know that. You might have
    a guess of it — I think some of these things are interacting in this way, and
    I think this thing we need to take the log, and so forth. But the truth is that
    the way the world works, the causal structures, they have many many things interacting
    in many many subtle ways. That’s why using trees, whether it be gradient boosting
    machines or random forests, work so well.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是“如果我们一次只取一列，然后在那一列上创建一棵树会怎样”。然后我们会看到哪一列的树是最具预测性的。为什么这可能会导致关于特征重要性的误导性结果？我们将失去特征之间的相互作用。如果我们只是随机打乱它们，那么会增加随机性，我们就能捕捉到特征之间的相互作用和重要性。这种相互作用的问题并不是一个细枝末节。它非常重要。想想这个推土机数据集，例如，有一个字段叫做“制造年份”，另一个字段叫做“销售日期”。如果我们想一想，很明显重要的是这两者的组合。换句话说，两者之间的区别是设备在售出时的年龄。因此，如果我们只包含其中一个，我们将严重低估该特征的重要性。现在，这里有一个非常重要的观点。如果你事先知道你需要哪些变量，它们如何相互作用，以及它们需要如何转换，那么几乎总是可以创建一个简单的逻辑回归，它和几乎任何随机森林一样好。在这种情况下，例如，我们可以创建一个新字段，它等于销售年份减去制造年份，然后将其输入模型并为我们获取该相互作用。但关键是，我们永远不知道这一点。你可能会猜测
    — 我认为其中一些事物是以这种方式相互作用的，我认为这个东西我们需要取对数，等等。但事实是，世界运作的方式，因果结构，有许多许多事物以许多微妙的方式相互作用。这就是为什么使用树，无论是梯度提升机还是随机森林，都能够如此出色地工作。
- en: '**Terrance’s comment:** One thing that bit me years ago was also I tried doing
    one variable at a time thinking “oh well, I’ll figure out which one’s most correlated
    with the dependent variable” [[24:45](https://youtu.be/0v93qHDqq_g?t=24m45s)].
    But what it doesn’t pull apart is that what if all variables are basically copied
    the same variable then they are all going to seem equally important but in fact
    it’s really just one factor.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Terrance的评论：** 多年前咬我一口的一件事也是我尝试一次只处理一个变量，认为“哦，好吧，我会弄清楚哪个与因变量最相关”[[24:45](https://youtu.be/0v93qHDqq_g?t=24m45s)]。但它没有分开的是，如果所有变量基本上都是复制的同一个变量，那么它们看起来都同样重要，但实际上只是一个因素。'
- en: That is also true here. If we had a column appear twice, then shuffling that
    column isn’t going to make the model much worse. If you think about how it’s built,
    particularly if we had `max_features=0.5`, some of the times, we are going to
    get version A of the column, some of the times, we are going to get version B
    of the column. So half the time, shuffling version A of the column is going to
    make a tree a bit worse, half the time it’s going to make column B it’ll make
    it a bit worse, and so it’ll show that both of those features are somewhat important.
    And it will share the importance between the two features. So this is why “collinearity”
    (I write collinearity but it means that they are linearly related, so this isn’t
    quite right) — but this is why having two variables that are closely related to
    each other or more variables that are closely related to each other means that
    you will often underestimate their importance using this random forest technique.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这在这里也是正确的。如果我们有一列出现两次，那么对该列进行洗牌不会使模型变得更糟。如果你考虑它是如何构建的，特别是如果我们设置了`max_features=0.5`，有时我们会得到列的版本A，有时我们会得到列的版本B。因此，一半的时间，对列的版本A进行洗牌会使树变得稍微糟糕，一半的时间对列的版本B进行洗牌会使其稍微糟糕，因此它将显示这两个特征都有一定重要性。它将在这两个特征之间共享重要性。这就是为什么“共线性”（我写的是共线性，但它意味着它们是线性相关的，所以这不太对）——但这就是为什么拥有两个彼此密切相关的变量或更多彼此密切相关的变量意味着您经常会低估它们在使用这种随机森林技术时的重要性。
- en: 'Question: Once we’ve shuffled and we get a new model, what exactly are the
    units of these importance? Is this a change in the R² [[26:26](https://youtu.be/0v93qHDqq_g?t=26m26s)]?
    It depends on the library we are using. So the units are kind of like… I never
    think about them. I just know that in this particular library, 0.005 is often
    a cutoff I would tend to use. But all I actually care about is this picture (the
    feature importance ordered for each variable):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：一旦我们洗牌并获得一个新模型，这些重要性的单位究竟是什么？这是否是R²的变化？这取决于我们使用的库。所以这些单位有点像……我从来没有考虑过它们。我只知道在这个特定的库中，0.005经常是我倾向于使用的一个截止值。但我真正关心的是这张图片（每个变量的特征重要性排序）：
- en: '![](../Images/019901f14f9972d223ff1e491bb1e234.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Then zooming in, turning it into a bar plot and then find where it becomes flat
    (~0.005).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后放大，将其转换为条形图，然后找到其变平的地方（约0.005）。
- en: '![](../Images/b9fb41f6c136deafed557437e2802117.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法查看图片。如果您能提供图片中的文本，我将很乐意帮助您翻译。
- en: So I removed them at that point and check the validation score didn’t get worse.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我在那时将它们移除，并检查验证分数没有变差。
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If it did get worse, I will just decrease the cutoff a little bit until it doesn’t
    get worse. So the units of measure of this don’t matter too much. We will learn
    later about a second way of doing variable importance, by the way.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果情况变得更糟，我只需稍微降低截止值，直到情况不再恶化。因此，这个度量单位并不太重要。顺便说一下，我们以后会学习另一种计算变量重要性的方法。
- en: What is the purpose of removing them [[27:42](https://youtu.be/0v93qHDqq_g?t=27m42s)]?
    Having looked at our feature importance plot, we see the ones less than 0.005
    is this long tail of boringness. So I said let’s just try grabbing the columns
    where it is greater than 0.005, create a new data frame called `df_keep` which
    is `df_train` with just those kept columns, create a new training and validation
    sets with just those columns, create a new random forest, and look to see how
    the validation set score. And the validation set RMSE changed and they got a bit
    better. So if they are about the same or a tiny bit better then my thinking is
    well this is just as good a model, but it’s now simpler.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 移除它们的目的是什么？在查看我们的特征重要性图后，我们发现小于0.005的那些是无聊的长尾。所以我说让我们尝试只选择大于0.005的列，创建一个名为`df_keep`的新数据框，其中只包含那些保留的列，创建一个只包含这些列的新训练和验证集，创建一个新的随机森林，并查看验证集得分。验证集的RMSE发生了变化，变得更好了一点。所以如果它们大致相同或稍微好一点，那么我的想法是这是一个同样好的模型，但现在更简单。
- en: So when I redo the feature importance, there is less collinearity. In this case,
    I saw that year made went from being a bit better than the next best thing (coupler
    system), but now it’s way better. So it did seem to definitely change these feature
    importances and hopefully give me some more insight there.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我重新进行特征重要性分析时，相关性较小。在这种情况下，我发现制造年份从略优于下一个最好的特征（连接器系统）变得更好了，但现在它更好了。因此，它似乎确实改变了这些特征的重要性，并希望能给我一些更多的见解。
- en: '![](../Images/9c09d7629b7b08324236e8364f3ebc52.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: 抱歉，我无法识别图片中的文本。如果您能提供文本内容，我将很乐意帮助您翻译。
- en: '**Question**: So how did that help our model [[29:30](https://youtu.be/0v93qHDqq_g?t=29m30s)]?
    We are going to dig into that now. Basically it tells us that, for example, if
    we are looking for how we are dealing with missing value, is there noise in the
    data, if it is a high cardinality categorical variable — they are all different
    steps we would take. So for example, if it was a high cardinality categorical
    variable that was originally a string, maybe fiProductClassDesc in above case,
    I remember one of the ones we looked at the other day had first of all was the
    type of vehicle and then a hyphen, and then the size of the vehicle. We might
    look at that and say “okay, that was an important column. Let’s try splitting
    it into two on hyphen and then take that bit which is a size of it and parse it
    and convert it into an integer.” We can try and do some feature engineering. Basically
    until you know which ones are important, you don’t know where to focus that feature
    engineering time. You can talk to your client or folks that are responsible for
    creating this data. If you were actually working at a bulldozer auction company,
    you might now go to the actual auctioneers and say “I am really surprised that
    coupler system seems to be driving people’s pricing decisions so much. Why do
    you think that might be?” and they can say to you “oh, it’s actually because only
    these classes of vehicles have coupler systems or only this manufacturer has coupler
    systems. So frankly this is actually not telling you about coupler systems but
    about something else. Oh hey, that reminds me, that’s something else we actually
    have measured that. It is in this different CSV file. I’ll go get it for you.”
    So it helps you focus your attention.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: So I had a fun little problem this weekend as you know. I introduced
    a couple of crazy computation into my random forest and all of a sudden they’re
    like oh my god these are the most important variables ever squashing all of the
    others. But then I got a terrible score and then is that because now that I think
    I have my scores computed correctly, what I noticed is that the importance went
    through the roof but the validation set was still bad or got worse. Is that because
    somehow that computation allow the training to almost like an identifier map exactly
    what the answer was going to be for training but of course that doesn’t generalize
    to the validation set. Is that what I observed [[31:33](https://youtu.be/0v93qHDqq_g?t=31m33s)]?
    There are two reasons why your validation score might not be very good.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bb6d59a9098cedcd4df454a5e115bdd.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: 把这个文件夹拖到另一个文件夹中。
- en: 'So we got these five numbers: RMSE of training, validation, R² of the training,
    validation, and R² of OOB. There’re two reasons and really in the end what we
    care about for this Kaggle competition is the RMSE of the validation set assuming
    we’ve created a good validation set. So Terrance’s case, he is saying that RMSE
    of the validation got worse when I did some feature engineering. Why is that?
    There are two possible reasons.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Reason one is that you are overfitting. If you are overfitting, then your OOB
    will also get worse. If you are doing a huge dataset with a small `set_rf_samples`
    so you can’t use an OOB, then instead create a second validation set which is
    a random sample and do that. So in other words, if your OOB or your random sample
    validation set has gotten much worse then you must be overfitting. I think in
    your case, Terrance, it’s unlikely that’s the problem because random forests don’t
    overfit that badly. It’s very hard to get them to overfit that badly unless you
    use some really weird parameters like only one estimator, for example. Once we’ve
    got ten trees in there, there should be enough variation that you can definitely
    overfit but not so much that you’re going to destroy your validation score by
    adding a variable. So I’d think you’ll find that’s probably not the case, but
    it’s easy to check. And if it’s not the case, then you’ll see that your OOB score
    or your random sample validation score hasn’t gotten much worse.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second reason your validation score can get worse, if your OOB score hasn’t
    got worse, you’re not overfitting but your validation score has gotten worse that
    means you’re doing something that is true in the training set but not true in
    the validation set. So this can only happen when your validation set is not a
    random sample. For example, in this bulldozers competition or in the grocery shopping
    competition, we’ve intentionally made a validation set that is for a different
    date range — it’s for the most recent two weeks. So if something different happened
    in the last two weeks to the previous weeks, then you could totally break your
    validation set. For example, if there was some kind of unique identifier which
    is different in the two date periods, then you could learn to identify things
    using that identifier in the training set. But then the last two weeks may have
    a totally different set of IDs or the different set of behavior, it could get
    a lot worse. What you are describing is not common though. So I’m a bit skeptical
    — it might be a bug but hopefully there’s enough things you can now use to figure
    out if it is a bug. We will be interested to hear what you learned.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression, logistic regression [[36:01](https://youtu.be/0v93qHDqq_g?t=36m1s)]
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That’s feature importance. I’d like to compare that to how feature importance
    is normally done in industry and in academic communities outside of machine learning,
    like in psychology, economics, and so forth. Generally speaking, people in those
    environments tend to use some kind of linear regression, logistic regression,
    general linear models. They start with their dataset and they say I am going to
    assume that I know the kind of parametric relationship between my independent
    variables and my dependent variable. So I’m going to assume that it’s a linear
    relationship or a linear relationship with a link function like a sigmoid to create
    logistic regression. So assuming I already know that, I can now write this as
    an equation. So if you have x1, x2, so forth.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a827f2384b3daa554e46c4bd3b25fc8f.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: I can say my y values are equal to *ax1 + bx2 = y*, therefore I can find out
    the feature importance easily enough by just looking at these coefficients and
    see which one is the highest, particularly if you have normalized the data first.
    There is this trop out there that is very common is that this is somehow more
    accurate, more pure, in some way better way of doing feature importance but that
    couldn’t be farther from the truth. If you think about it, if you were missing
    an interaction, if you were missing a transformation you needed, or if you have
    any way being anything less than a 100% perfect in all of your pre-processing
    so that your model is the absolute correct truth of the situation — unless you’ve
    got all of that correct, then your coefficients are wrong. Your coefficients are
    telling you “in your totally wrong model, this is how important those things are”
    which is basically meaningless. Where else, the random forest feature importance
    is telling you in this extremely high parameter, highly flexible functional form,
    with few if any statistical assumptions, this is your feature importance. So I
    would be very cautious.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Again, I can’t stress this enough when you leave this program, you are much
    more often going to see people talk about logistic regression coefficients than
    you are going to see them talk about random forest variable importance. And every
    time you see that happen, you should be very very skeptical of what you are seeing.
    Anytime you read a paper in economics or in psychology, or the marketing department
    tells you that this regression or whatever, every single those coefficients are
    going to be massively biased by any issues in the model. Furthermore, if they’ve
    done so much pre-processing that actually the model is pretty accurate then now
    you are looking at coefficients that are going to be like a coefficient of some
    principal component from a PCA or a coefficient of some distance from some cluster
    or something. At which point, they are very very hard to interpret anyway. They
    are not actual variables. So they are kind of the two options I’ve seen when people
    try to use classic statistical techniques to do a variable importance equivalent.
    I think things are starting to change slowly. There are some fields that are starting
    to realize that this is totally the wrong way to do things. But it’s been nearly
    20 years since random forests appeared so it takes a long time. People say that
    the only way that knowledge really advances is when the previous generation dies,
    and that’s kind of true. Particularly academics, they make a career of being good
    at a particular sub thing and often it’s not until the next generation comes along
    that people notice that’s actually no longer a good way to do things. And I think
    that’s what happened here.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve got now a model which isn’t really any better predictive accuracy wise,
    but we are getting a good sense that there seems to be four main important things
    [[40:38](https://youtu.be/0v93qHDqq_g?t=40m38s)]: YearMade, Coupler_System, ProductSize,
    fiProductClassDesc.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: One hot encoding [[41:00](https://youtu.be/0v93qHDqq_g?t=41m)]
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is something else that we can do, however, which is we can do something
    called one hot encoding. So this is going to where we were talking about categorical
    variable. Remember, a categorical variable, let’s say we had a string high, low,
    medium (the order we got was kind of weird — in alphabetical order by default).
    So we mapped it to 0, 1, 2\. By the time it gets into our data frame, it’s now
    a number so the random forest doesn’t know that it was originally a category —
    it’s just a number. So when the random forest is built, it basically says oh is
    it greater than 1 or not. Or is it greater than naught or not. They are basically
    the two possible decisions it could have made. For something with 5 or 6 bands,
    it could be that just one of the levels of category is actually interesting. Maybe
    the only thing that mattered was whether it was unknown. Maybe not knowing its
    size somehow impacts the price. So if we wanted to be able to recognize that and
    particularly if it just so happened that the way that the numbers were coded was
    it unknown ended up in the middle, then it going to take two splits to get to
    the point where we can see that it’s actually unknown that matters. So this is
    a little inefficient and we are wasting tree computation. Wasting tree computation
    matters because every time we do a split, we are halving the amount of data at
    least that we have to do more analysis. So it’s going to make our tree less rich
    and less effective if we are not giving the data in a way that is convenient for
    it to do the work it needs to do.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: What we could do instead is create 6 columns for each category and each column
    would contain 1’s and 0’s. Having added 6 additional columns to our dataset, the
    random forest now has the ability to pick one of these and say oh, let’s have
    a look at is_unknown. There is one possible fit I can do which is 1 vs. 0\. Let’s
    see that’s any good. So it now has the ability in a single step to pull out a
    single category level and this kind of coding is called one-hot encoding. For
    many types of machine learning model, something like this is necessary. If you
    are doing logistic regression, you can’t possibly put in a categorical variable
    that goes naught through five because there is obviously no written linear relationship
    between that and anything. So one hot encoding, a lot of people incorrectly assume
    that all machine learning requires one hot encoding. But in this case, I’m going
    to show you how we could use it optionally and see whether it might improve things
    sometimes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: If we have six categories like in this case, would there be any
    problems with adding a column for each of the categories? In linear regression,
    if there are six categories, we should only do it for five of them [[45:17](https://youtu.be/0v93qHDqq_g?t=45m17s)].
    You certainly can say let’s not worry about adding `is_medium` because we can
    infer it from the other five. I would say include it anyway because otherwise,
    the random forest has to make five decisions to get to that point. The reason
    you need to not include one in linear models is because linear models hate collinearity
    but we don’t care about that here.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: So we can do one hot encoding easily enough and the way we do it is we pass
    one extra parameter to `proc_df` which is what is the max number of categories
    (`max_n_cat`). So if we say it’s seven, then anything with less than seven levels
    is going to be turned into a one-hot encoded bunch of columns.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Like zip code has more than six levels so that would be left as a number. Generally
    speaking, you obviously probably wouldn’t want to one hot encode zip code because
    that’s just going to create masses of data, memory problems, computation problems,
    and so forth. So this is another parameter you can play around with.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: So if I try it out, run the random forest as per usual, you can see what happens
    to the R² of the validation set and to the RMSE of the validation set. In this
    case, I found it got a little bit worse. This isn’t always the case and it’s going
    to depend on your dataset. It depends on if you have a dataset where single categories
    tend to be quite important or not. In this particular case, it did not make it
    more predictive. However, what it did do is that we now have different features.
    proc_df puts the name of the variable, an underscore, and the level name. So interestingly,
    it turns out that before, it said that enclosure was somewhat important. When
    we do it as one hot encoded, it actually says `Enclosure_EROPS w AC` is the most
    important thing. So for at least the purpose of interpreting your model, you should
    always try one hot encoding quite a few of your variables. I often find somewhere
    around 6 or 7 pretty good. You can try making that number as high as you can so
    that it doesn’t take forever to compute and the feature importance doesn’t include
    really tiny levels that aren’t interesting. That is up to you to play around with,
    but in this case, I found this very interesting. It clearly tells me I need to
    find out what `Enclosure_EROPS w AC` is and why it is important because it means
    nothing to me right now but it is the most important thing. So I should go figure
    that out.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cc615706bdfac9c7fba609ef5963b14.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: '**Question**: Can you explain how changing the max number of category works?
    Because for me, it just seems like there are five categories or six categories
    [[49:15](https://youtu.be/0v93qHDqq_g?t=49m15s)]. All it’s doing is is here is
    a column called zip code, usage band, and sex, for example. Say, zip code has
    5,000 levels. The number of levels in a category, we call its “cardinality”. So
    it has a cardinality of 5,000\. Usage band may have a cardinality of six. Sex
    has a cardinality of two. So when proc_df goes through and says okay, this is
    a categorical variable, should I one-hot encode it? It checks the cardinality
    against `max_n_cat` and says 5,000 is bigger than seven so I don’t one hot encode
    it. Then it goes to usage band — 6 is less than 7, so I do one hot encode it.
    It goes to sex, and 2 is less than 7, so one hot encode that too. So it just says
    for each variable, how I decide whether ton one hot encode it or not. Once we
    decide to one hot encode it, it does not keep the original variable.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have actually made an effort to turn your ordinal variables into proper
    ordinals, using proc_df can destroy that. The simple way to avoid that is if we
    know that we always want to use the codes for usage band, you could just go ahead
    and replace it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/157f32c922842b4c045192aba902ce72.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Now it’s an integer. So it will never get changed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Removing redundant features [[54:57](https://youtu.be/0v93qHDqq_g?t=54m57s)]
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve already seen how variables which are basically measuring the same thing
    can confuse our variable importance. They can also make our random forest slightly
    less good because it requires more computation to do the same thing and there’re
    more columns to check. So we are going to do some more work to try and remove
    redundant features. The way I do that is to do something called “**dendrogram”**.
    And it is kind of hierarchical clustering.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster analysis** is something where you are trying to look at objects,
    they can be either rows in the dataset or columns and find which ones are similar
    to each other. Often you will see people particularly talking about cluster analysis,
    they normally refer to rows of data and they will say “let’s plot it” and find
    clusters. A common type of cluster analysis, time permitting, we may get around
    to talking about this in some detail, is called k-means. It is basically where
    you assume that you don’t have any labels at all and you take a couple of data
    points at random and you gradually find the ones that are near to it and move
    them closer and closer to centroids, and you repeat it again and again. It is
    an iterative approach that you tell it how many clusters you want and it will
    tell you where it thinks that classes are.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: A really under used technique (20 or 30 years ago it was much more popular than
    it is today) is a hierarchical clustering also known as agglomerated clustering.
    In hierarchical or agglomerated clustering, we look at every pair of objects and
    say which two objects are the closest. We then take the closest pair, delete them,
    and replace them with the midpoint of the two. Then repeat that again and again.
    Since we are removing points and replacing them with their averages, you are gradually
    reducing a number of points by pairwise combining. The cool thing is, you can
    plot that.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/585a23d42f454690fe4a5175d6637dd4.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: Like so. Rather than looking at points, you look at variables and we can see
    which two variables are the most similar. `saleYear` and `saleElapsed` are very
    similar. So the horizontal axis here is how similar are the two points that are
    being compared. If they are closer to the right, that means that they are very
    similar. So saleYear and saleElapsed have been combined and they were very similar.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: In this case, I actually used Spearman’s R. You guys familiar with correlation
    coefficients already? So correlation is almost exactly the same as the R², but
    it’s between two variables rather than a variable and its prediction. The problem
    with a normal correlation is that if you have data that looks like this then you
    can do a correlation and you’ll get a good result.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d054bfdfc7b524a9f1d1cb08655895f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: But if you’ve got data which looks like this and you try and do a correlation
    (assuming linearity), that’s not very good.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/542a32bfb51894e5c8a241e85d996490.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: So there is a thing called a rank correlation which is a really simple idea.
    Replace every point by its rank.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c32d3e2abcb4bc0d3cbc58562eaa72e0.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: From left to right, we rank from 1, 2, …6\. Then you do the same for the y-axis.
    Then you create a new plot where you don’t plot the data but you plot the rank
    of the data. If you think about it, the rank of this dataset is going to look
    like an exact line because every time something was greater on the x-axis, it
    was also greater on the y-axis. So if we do a correlation on the rank, that’s
    called a rank correlation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Because we want to find the columns that are similar in a way that the random
    forest would find them similar (random forests do not care about linearity, they
    just care about ordering), so a rank correlation is the right way to think about
    that [[1:00:05](https://youtu.be/0v93qHDqq_g?t=1h5s)]. So Spearman’s R is the
    name of the most common rank correlation. But you can literally replace the data
    with its rank and chuck it at the regular correlation and you will get basically
    the same answer. The only difference is in how ties are handled which is a pretty
    minor issue.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a correlation matrix, there is basically a couple of standard
    steps you do to turn that into a dendrogram which I have to look up on stackoverflow
    each time I do it. You basically turn it into a distance matrix and then you create
    something that tells you which things are connected to each other things hierarchically.
    So these are three standard steps you always have to do to create a dendrogram:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then you can plot it [[1:01:30](https://youtu.be/0v93qHDqq_g?t=1h1m30s)]. `saleYear`
    and `saleElapsed` are measuring basically the same thing (at least in terms of
    rank) which is not surprising because `saleElapsed` is the number of days since
    the first day in my dataset so obviously these two are nearly entirely correlated.
    `Grouser_Tracks`, `Hidraulics_Flow`, and `Coupler_System` all seem to be measuring
    the same thing. This is interesting because remember, `Coupler_System` it said
    was super important. So this rather supports our hypothesis there is nothing to
    do with whether it’s a coupler system but whether it is whatever kind of vehicle
    it is has these kind of features. `ProductGroup` and `ProductGroupDesc` seem to
    be measuring the same thing, and so are `fiBaseModel` and `fiModelDesc`. Once
    we get past that, suddenly things are further away, so I’m probably going to not
    worry about those. So we are going to look into those four groups that are very
    similar.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/585a23d42f454690fe4a5175d6637dd4.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: If you just want to know how similar is this thing to this thing, the best way
    is to look at the Spearman’s R correlation matrix [[1:03:43](https://youtu.be/0v93qHDqq_g?t=1h3m43s)].
    There is no random forest being used here. The distance measure is being done
    entirely on rank correlation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: What I then do is I take these groups and I create a little function `get_oob`
    (get Out Of Band score) [[1:04:29](https://youtu.be/0v93qHDqq_g?t=1h4m29s)]. It
    does a random forest for some data frame. I make sure that I have taken that data
    frame and split it into a training and validation set, and then I call `fit` and
    return the OOB score.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Basically what I’m going to do is try removing each one of these 9 or so variables
    one at a time and see which ones I can remove and it doesn’t make the OOB score
    get worse.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And each time I run this, I get slightly different results so actually it looks
    like the last time I had 6 things and not 9 things. So you can see, I just do
    a loop through each of the things that I am thinking maybe I can get rid of this
    because it’s redundant and I print out the column name and the OOB score of a
    model that is trained after dropping that one column.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The OOB score on my whole data frame is .89 and then after dropping each one
    of these things, basically none of them got much worse. `saleElapsed` is getting
    quite a bit worse than `saleYear`. Burt it looks like pretty much everything else,
    I can drop with only like a third decimal place problem. So obviously though,
    you’ve got to remember the dendrogram. Let’s take fiModelDesc and fiBaseModel,
    they are very similar to each other. So what this says isn’t that I can get rid
    of both of them, I can get rid of one of them because they are basically measuring
    the same thing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So then I try it. Let’s try getting rid of one from each group:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We’ve gone from .890 to .888, again, it’s so close as to be meaningless. So
    that sounds good. Simpler is better. So I’m now going to drop these columns from
    my data frame, and then I can try running the full model again.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`reset_rf_samples` means I’m using my whole bootstrapped sample. With 40 estimators,
    we got 0.907.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So I’ve now got a model which is smaller and simpler, and I’m getting a good
    score for. So at this point, I’ve now got rid of as many columns as I feel I comfortably
    can (ones that either didn’t have a good feature importance or were highly related
    to other variables, and the model didn’t get worse significantly when I removed
    them).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Partial dependence [[1:07:34](https://youtu.be/0v93qHDqq_g?t=1h7m34s)]
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So now I’m at the point where I want to try and really understand my data better
    by taking advantage of the model. And we are going to use something called partial
    dependence. Again, this is something that you could use in the Kaggle kernel and
    lots of people are going to appreciate this because almost nobody knows about
    partial dependence and it’s a very very powerful technique. What we are going
    to do is we are going to find out, for the features that are important, how do
    they relate to the dependent variable. Let’s have a look.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, since we are doing interpretation, we will set `set_rf_samples` to 50,000
    to run things quickly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We’ll take our data frame, we will get our feature importance and notice that
    we are using `max_n_cat` because I am actually pretty interested in seeing the
    individual levels for interpretation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the top 10:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/7b77f6f118b8a26274ddbb81c9df6471.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Let’s try to learn more about those top 10\. `YearMade` is the second most important.
    So one obvious thing we could do would be to plot `YearMade` against `saleElapsed`
    because as we’ve talked about already, it seems to make sense that they are both
    important but it seems very likely that they are combined together to find how
    old was the product when it was sold. So we could try plotting `YearMade` against
    `saleElapsed` to see how they relate to each other.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/ad9a77c110facd9f242fcb4fd790fd37.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: And when we do, we get this very ugly graph [[1:09:08](https://youtu.be/0v93qHDqq_g?t=1h9m8s)].
    It shows us that `YearMade` actually has a whole bunch that are a thousand. Clearly,
    this is where I would tend to go back to the client and say okay, I’m guessing
    that these bulldozers weren’t actually made in the year 1000 and they would presumably
    say to me “oh yes, they are ones where we don’t know where it was made”. Maybe
    “before 1986, we didn’t track that” or maybe “the things that are sold in Illinois,
    we don’t have that data provided”, etc — they will tell us some reason. So in
    order to understand this plot better, I’m just going to remove them from this
    interpretation section of the analysis. We will just grab things where `YearMade`
    is greater than 1930.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s now look at the relationship between `YearMade` and `SalePrice`. There
    is a really great package called `ggplot`. `ggplot` originally was an R package
    (GG stands for the Grammar of Graphics). The grammar of graphics is this very
    powerful way of thinking about how to produce charts in a very flexible way. I’m
    not going to be talking about it much in this class. There is lots of information
    available online. But I definitely recommend it as a great package to use. `ggplot`
    which you can `pip` install, it’s part of the fast.ai environment already. `ggplot`
    in Python has basically the same parameters and API as the R version. The R version
    is much better documented so you should read its documentation to learn how to
    use it. But basically you say “okay, I want to create a plot for this data frame
    (`x_all`). When you create plots, most of the datasets you are using are going
    to be too big to plot. For example, if you do a scatter plot, it will create so
    many dots that it’s just a big mess and it will take forever. Remember, when you
    are plotting things, you are looking at it, so there is no point plotting something
    with a hundred million samples when if you only used a hundred thousand, it’s
    going to be pixel identical. That’s why I call `get_sample` first. `get_sample`
    just grabs a random sample.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So I’m just going to grab 500 points from my data frame and plot `YearMade`
    against `SalePrice`. `aes` stands for “aesthetic” — this is the basic way that
    you set up your columns in `ggplot`. Then there is this weird thing in `ggplot`
    where “+” means add chart elements. So I’m going to add a smoother. Often you
    will find that a scatter plot is very hard to see what is going on because there’s
    too much randomness. Or else, a smoother basically creates a little linear regression
    for every little subset of the graph. So it joins it up and allows you to see
    a nice smooth curve. This is the main way that I tend to look at univariate relationships.
    By adding standard error equals true (`se=True`), it also shows me the confidence
    interval of this smoother. `loess` stands for locally weighted regression which
    is this idea of doing lots of little mini regressions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fef5ff6825c0519befb686704772c41.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: So we can see here [[1:12:48](https://youtu.be/0v93qHDqq_g?t=1h12m48s)], the
    relationship between `YearMade` and `SalePrice` is all over the place which is
    not really what we would expect. I would have expected that stuff that’s sold
    more recently would probably be more expensive because of inflation and they are
    more current models. The problem is that when you look at a univariate relationship
    like this, there is a whole lot of collinearity going on — a whole lot of interactions
    that are being lost. For example, why did the price drop? Is it actually because
    things made between 1991 and 1997 are less valuable? Or is it actually because
    most of them were also sold during that time and there was maybe a recession then?
    Or maybe it was because products sold during that time, a lot more people were
    buying types of vehicles that were less expensive? There’s all kind of reasons
    for that. So again, as data scientists, one of the things you are going to keep
    seeing is that at the companies that you join, people will come to you with these
    kind of univariate charts where they’ll say “oh my gosh, our sales in Chicago
    have disappeared. They got really baed.” or “people aren’t clicking on this add
    anymore” and they will show you a chart that looks like this and ask what happened.
    Most of the time, you’ll find the answer to the question “what happened” is that
    there is something else going on. So for instance, “actually in Chicago last week,
    actually we were doing a new promotion and that’s why our revenue went down —
    it’s not because people are not buying stuff in Chicago anymore; the prices were
    lower”.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: So what we really want to be able to do is say “well, what’s the relationship
    between `SalePrice` and `YearMade` all other things being equal. “All other things
    being equal” basically means if we sold something in 1990 vs. 1980 and it was
    exactly the same thing to exactly the same person in exactly the same auction
    so on and so forth, what would have been the difference in price? To do that,
    we do something called a **partial dependence plot** [[1:15:02](https://youtu.be/0v93qHDqq_g?t=1h15m2s)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: There is a really nice library which nobody’s heard of called `pdp` which does
    these partial dependence plots, and what happens is this. We’ve got our sample
    of 500 data points and we are going to do something really interesting. We are
    going to take each one of those five hundred randomly chosen auctions and we are
    going to make a little dataset out of it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Here is our dataset of 500 auctions and here is our columns, one of which is
    the thing that we are interested in which is `YearMade`. We are now going to try
    and create a chart where we say all other things being equal in 1960, how much
    did things cost in auctions? The way we are going to do that is we are going to
    replace the `YearMade` column with 1960\. We are going to copy in the value 1960
    again and again all the way down. Now every row, the year made is 1960 and all
    of the other data is going to be exactly the same. We are going to take our random
    forest, we are going to pass all this through our random forest to predict the
    sale price. That will tell us for everything that was auctioned, how much do we
    think it would have been sold for if that thing was made in 1960\. And that’s
    what we are going to plot on the right.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73801b62b05a310161e3a70f12f68e0d.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: And we are going to do the same thing for 1961.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: To be clear, we’ve already fit the random forest and then we are
    just passing a new year and seeing what it determines the price should be [[1:17:10](https://youtu.be/0v93qHDqq_g?t=1h17m10s)]?
    Yes, so this is a lot like the way we did feature importance. But rather than
    randomly shuffling the column, we are going to replace the column with a constant
    value. Randomly shuffling the column tells us how accurate it is when you don’t
    use that column anymore. Replacing the whole column with a constant estimates
    for us how much we would have sold that product for in that auction on that day
    in that place if that product had been made in 1961\. We then take the average
    of all of the sale prices that we calculate from that random forest. We do it
    in 1961 and we get this value:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32c52873fe53ee9e45fc039141dc1eb2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/f160732f7a5a7de5e50ef30f746d7acc.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: So what the partial dependence plot (PDP)here shows us is each of these light
    blue lines actually is showing us all 500 lines [[1:18:01](https://youtu.be/0v93qHDqq_g?t=1h18m1s)].
    So for row number 1 in our dataset, if we sold it in 1960, we are going to index
    that to zero so call that zero. If we sold it in 1970 that particular auction,
    it would have been here, etc. We actually plot all 500 predictions of how much
    every one of those 500 auctions would have gone for if we replaced its `YearMade`
    with each of these different values. Then this dark line is the average. So this
    tells us how much would we have sold on average all of those auctions for if all
    of those products were actually made in 1985, 1990, 1993, etc. So you can see,
    what’s happened here is at least in the period where we have a reasonable amount
    of data which is since 1990, this is basically a totally straight line — which
    is what you would except. Because if it was sold on the same date, and it was
    the same kind of tractor, sold to the same person in the same auction house, then
    you would expect more recent vehicles to be more expensive because of inflation
    and they are newer. You would expect that relationship to be roughly linear and
    that is exactly what we are finding. By removing all these externalities, it often
    allows us to see the truth much more clearly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'This partial dependence plot is something which is using a random forest to
    get us a more clear interpretation of what’s going on in our data [[1:20:02](https://youtu.be/0v93qHDqq_g?t=1h20m2s)].
    The steps were:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: First of all look at the future importance to tell us which things do we think
    we care about.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then to use the partial dependence plot to tell us what’s going on on average.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There is another cool thing we can do with PDP which is we can use clusters.
    What clusters does is it uses cluster analysis to look at each one of the 500
    rows and say do some those 500 rows move in the same way. We could kind of see
    it seems like there’s a whole a lot of rows that go down and then up, and there
    seems to be a bunch of rows that go up and then go flat. It does seem like there’s
    some kind of different types of behaviors being hidden and so here is the result
    of doing that cluster analysis:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/94c8d8df61a3a304f4f04c4a170185f7.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: We still get the same average but it says here are five most common shapes that
    we see. And this is where you could then go in and say all right, it looks like
    some kinds of vehicle, after 1990, their prices are pretty flat. Before that,
    they were pretty linear. Some other kinds of vehicle were exactly the opposite,
    so different kinds of vehicle have these different shapes. So, this is something
    you could dig into.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: So what are we going to do with this information [[1:21:40](https://youtu.be/0v93qHDqq_g?t=1h21m40s)]?
    The purpose of interpretation is to learn about a dataset and so why do you want
    to learn about a dataset? It’s because you want to do something with it. So in
    this case, it’s not so much something if you are trying to win a Kaggle competition
    — it can be a little bit like some of these insights might make you realize I
    could transform this variable or create this interaction, etc. Obviously feature
    importance is super important for Kaggle competitions. But this one is much more
    for real life. So this is when you are talking to somebody and you say to them
    “okay, those plots you’ve been showing me which actually say that there was this
    kind of dip in prices based on things made between 1990 and 1997\. There wasn’t
    really. Actually they were increasing, and there was something else going on at
    that time.” It’s basically the thing that allows you to say for whatever this
    outcome I’m trying to drive in my business is, this is how something is driving
    it. So if it’s like I’m looking at advertising technology, what’s driving clicks
    that I I’m actually digging in to say okay, this is actually how clicks are being
    driven. This is actually the variable that’s driving it. This is how it’s related.
    So therefore, we should change our behavior in this way. That’s really the goal
    of any model. I guess there are two possible goals: one goal of a model is just
    to get the predictions, like if you are doing hedge fund trading, you probably
    want to know what the price of that equity is going to be. If you are doing insurance,
    you probably just want to know how much claims that guy is going to have. But
    probably most of the time, you are actually trying to change something about how
    you do business — how you do marketing, how you do logistics, so the thing you
    actually care about is how the things are related to each other.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f22e74acac8333f45ef97c4b654dac45.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: '**Question**: Could you explain again why the dip did not signify what we thought
    it did [[1:23:36](https://youtu.be/0v93qHDqq_g?t=1h23m36s)]? Yes. So this is a
    classic boring univariate plot. So this is just taking all of the dots, all of
    the options, plotting YearMade against SalePrice and we are just fitting a rough
    average through them. It’s true that the products made between 1992 and 1997 on
    average in our dataset are being sold for less. Very often in business, you’ll
    hear somebody look at something like this and say “ we should stop auctioning
    equipment that is made in those years because we are getting less money for”,
    for example. But if the truth actually is that during those years, it’s just that
    people were making more small industrial equipment where you would expect it to
    be sold for less and actually our profit on it is just as high, for instance.
    Or it’s not that things made during those years now would now be cheaper, it’s
    that when we were selling things in those years, they were cheaper because there
    was a recession going on. If you are trying to actually take some action based
    on this, you probably don’t just care about the fact that things made in those
    years are cheaper on average, but how does that impact today. So PDP approach
    where we actually say let’s try and remove all of these externalities. So if something
    is sold on the same day to the same person of the same kind of vehicle, then actually
    how does year made impact the price. This basically says, for example, if I am
    deciding what to buy at an auction, then this is saying to me that getting a more
    recent vehicle on average really does give you more money which is not what the
    naive univariate plot said.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '**Comment**: Bulldozers made in 2010 probably are not close to the type of
    bulldozers that were made in 1960\. If you are taking something that would be
    so very different, like a 2010 bulldozer, and then trying to just drop it to say
    “oh if it was made in 1960” that may cause poor prediction at a point because
    it’s so far outside of the training set [[1:26:12](https://youtu.be/0v93qHDqq_g?t=1h26m12s)].
    Absolutely. That’s a good point. It is a limitation, however, if you’ve got a
    datapoint that’s in a part of the space that it has not seen before, like maybe
    people didn’t put air conditioning in bulldozers in 1960 and you are saying how
    much would this bulldozer with air conditioning would have gone for in 1960, you
    don’t really have any information to know that. This is still the best technique
    I know of but it’s not perfect. And you kind of hope that the trees are still
    going to find some useful truth even though it hasn’t seen that combination of
    features before. But yeah, it’s something to be aware of.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/95f7f616311655fbea375623a66c6d69.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: You can also do the same thing in a PDP interaction plot [[1:27:36](https://youtu.be/0v93qHDqq_g?t=1h27m36s)].
    And PDP interaction plot which is really what I’m trying to get to here is how
    does saleElapsed and YearMade together impact the price. If I do a PDP interaction
    plot, it shows me saleElapsed vs. price, YearMade vs. price, and the combination
    vs. price. Remember, this is always log of price. That’s why these prices look
    weird. You can see that the combination of saleElapsed and YearMade is as you
    would expect —the highest prices are those where there’s the least elapsed and
    the most recent year made. The upper right is the univariate relationship between
    saleElapsed and price, the lower left is the univariate relationship between YearMade
    and price, and the lower right is the combination of the two. It’s enough to see
    clearly that these two things are driving price together. You can also see these
    are not simple diagonal lines so there is some interesting interaction going on.
    Based on looking at these plots, it’s enough to make me think, oh, we should maybe
    put in some kind of interaction term and see what happens. So let’s come back
    to that in a moment, but let’s just look at a couple more.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Remember, in this case, we did one-hot-encoding — way back at the top, we said
    `max_n_cat=7` [[1:29:18](https://youtu.be/0v93qHDqq_g?t=1h29m18s)]. So we have
    things like `Enclosure_EROPS w AC`. So if you have one-hot-encoded variables,
    you can pass an array of them to `plot_pdp` and it will treat them as a category.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/107209cf69c2ed2bdbd5a5e08737fb09.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: So in this case, I’m going to create a PDP plot of these three categories, and
    I’m going to call it “Enclosure”.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/fed9b9a0979008b80056330d178201c1.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: I can see here that `Enclosure_EROPS w AC` on average are more expensive than
    `Enclosure_EROPS` or `Enclosure_OROPS`. It actually looks like the latter two
    are pretty similar or else `Enclosure_EROPS w AC` is higher. So at this point,
    I’m probably being inclined to hop on to Google and type “erops orops” and find
    out what these things are and here we go.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c13347106d0d318066becc5d024c38f.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: So it turns out that EROPS is enclosed rollover protective structure and so
    it turns out that if your bulldozer is fully enclosed then optionally you can
    also get air conditioning. So actually this thing is telling us whether it has
    air conditioning. If it’s an open structure, then obviously you don’t have air
    conditioning at all. So that’s what these three levels are. So we’ve now learnt
    all other things being equal, the same bulldozer, sold at the same time, built
    at the same time, sold to the same person is going to be quite a bit more expensive
    if it has air conditioning than if it doesn’t. So again, we are getting this nice
    interpretation ability. Now that I spent some time with this dataset, I’d certainly
    noticed that knowing this is the most important thing, you do notice that there
    is a lot more air conditioned bulldozers nowadays than they used to be and so
    there is definitely an interaction between date and that.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/fa60744a6a711852413ca5c1fe5da5ad.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Based on the earlier interaction analysis, I’ve tried, first of all, setting
    everything before 1950 to 1950 because it seems to be some kind of missing value
    [[1:31:25](https://youtu.be/0v93qHDqq_g?t=1h31m25s)]. I’ve set `age` to be equal
    to `saleYear - YearMade`. Then I tried running a random forest on that. Indeed,
    `age` is now the single biggest thing, saleElapsed is way back down here, YearMade
    is back down here. So we’ve used this to find an interaction. But remember, of
    course a random forest can create an interaction through having multiple split
    points, so we shouldn’t assume that this is actually going to be a better result.
    And in practice, I actually found when I looked at my score and my RMSE, adding
    age was actually a little worse. We will see about that later probably in the
    next lesson.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Tree interpreter [[1:32:34](https://youtu.be/0v93qHDqq_g?t=1h32m34s)]
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One last thing is tree interpreter. This is also in the category of things that
    most people don’t know exists, but it’s super important. Almost pointless for
    Kaggle competitions but super important for real life. Here is the idea. Let’s
    say you are an insurance company and somebody rings up and you give them a quote
    and they say “oh, that’s $500 more than last year. Why?” So in general, you’ve
    made a prediction from some model and somebody asks why. This is where we use
    this method called tree interpreter. What tree interpreter does is it allows us
    to take a particular row.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So in this case, we are going to pick row number zero.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here are all the columns in row zero.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1733bb765d330099d0a7d63c7540e124.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'What I can do with a tree interpreter is I can go `ti.predict`, pass in my
    random forest and my row (so this would be like this particular customer’s insurance
    information, or in this case this particular auction). And it will give me back
    three things:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '`prediction`: The prediction from the random forest'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias`: The average sale price across the whole original dataset'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`contributions`: A column and the value to split by (i.e. the predictor), and
    how much it changed the predicted value.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So you can think of it this way [[1:34:51](https://youtu.be/0v93qHDqq_g?t=1h34m51s)].
    The whole dataset had an average log sale price of 102\. The dataset for those
    with `Coupler_system ≤ 0.5` had an average of 10.3\. The dataset for `Coupler_system
    ≤ 0.5` and `Enclosure ≤ 2.0` was 9.9, and then eventually we get all the way up
    here and also with `ModelID ≤ 4573.0`, it’s 10.2\. So you could ask, okay, why
    did we predict 10.2 for this particular row?
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea69e6c9aeb64e4556bd5a52318b64f9.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'That is because we started with 10.19:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Because the coupler system was less than .3, we added about .2 to that (so we
    went from 10.19 to 10.34).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because enclosure was less than 2, we subtracted about .4.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then because model ID was less than 4573, we added about .7
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So you can see with a single tree, you could break down why is it that we predicted
    10.2\. At each one of these decision points, we are adding or subtracting a little
    bit from the value. What we could then do is we could do that for all the trees
    and then we could take the average. So every time we see enclosure did we increase
    or decrease the value and by how much? Every time we see model ID, did we increase
    or decrease the value and by how much? We could take the average of all of those
    and that’s what ends up in this thing called `contributions`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So here is all of our predictors and the value of each [[1:37:54](https://youtu.be/0v93qHDqq_g?t=1h37m54s)].
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*There was an issue with sorting in the video as it was not using the index
    sort, but above example is the corrected version.*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So then there is this thing called bias and the bias is just the average before
    we start doing any splits [[1:39:03](https://youtu.be/0v93qHDqq_g?t=1h39m3s)].
    If you start with the average log of value and then we went down each tree and
    each time we saw YearMade, we had some impact, coupler system some impact, product
    size some impact, and so forth.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: We might come back to tree interpreter next time, but the basic idea (this is
    the last of our key interpretation points) is that we want some ability to not
    only tell us about the model as a whole and how it works on average, but to look
    at how the model makes prediction for an individual row. And that’s what we are
    doing here.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
