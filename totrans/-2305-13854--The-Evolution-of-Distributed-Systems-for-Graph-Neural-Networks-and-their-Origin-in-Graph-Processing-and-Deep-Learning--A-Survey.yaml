- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:39:28'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2305.13854] The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.13854](https://ar5iv.labs.arxiv.org/html/2305.13854)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jana Vatter [jana.vatter@tum.de](mailto:jana.vatter@tum.de) [0000-0002-5900-5709](https://orcid.org/0000-0002-5900-5709
    "ORCID identifier") Technical University of MunichDepartment of Computer ScienceMunichGermany
    ,  Ruben Mayer [ruben.mayer@uni-bayreuth.de](mailto:ruben.mayer@uni-bayreuth.de)
    [0000-0001-9870-7466](https://orcid.org/0000-0001-9870-7466 "ORCID identifier")
    University of BayreuthFaculty of Mathematics, Physics & Computer ScienceBayreuthGermany
     and  Hans-Arno Jacobsen [jacobsen@eecg.toronto.edu](mailto:jacobsen@eecg.toronto.edu)
    [0000-0003-0813-0101](https://orcid.org/0000-0003-0813-0101 "ORCID identifier")
    University of TorontoDepartment of Electrical & Computer EngineeringTorontoCanada
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Graph Neural Networks (GNNs) are an emerging research field. This specialized
    Deep Neural Network (DNN) architecture is capable of processing graph structured
    data and bridges the gap between graph processing and Deep Learning (DL). As graphs
    are everywhere, GNNs can be applied to various domains including recommendation
    systems, computer vision, natural language processing, biology and chemistry.
    With the rapid growing size of real world graphs, the need for efficient and scalable
    GNN training solutions has come. Consequently, many works proposing GNN systems
    have emerged throughout the past few years. However, there is an acute lack of
    overview, categorization and comparison of such systems. We aim to fill this gap
    by summarizing and categorizing important methods and techniques for large-scale
    GNN solutions. In addition, we establish connections between GNN systems, graph
    processing systems and DL systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph Neural Networks, Deep Learning Systems, Graph Processing Systems^†^†ccs:
    General and reference Surveys and overviews^†^†ccs: Computing methodologies Machine
    learning^†^†ccs: Mathematics of computing Graph algorithms^†^†ccs: Computing methodologies Distributed
    computing methodologies^†^†ccs: Computer systems organization Neural networks{NoHyper}^†^†© Owner
    2023\. This is the author’s version of the work. It is posted here for your personal
    use. Not for redistribution. The definite version was published in ACM Computing
    Surveys, https://doi.org/10.1145/3597428'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep Learning (DL) on graph structured data is a promising and rising field.
    As graphs are all around us (Miller, [1995](#bib.bib133); Gupta et al., [2013](#bib.bib65);
    Balaban, [1985](#bib.bib7)), they can be used in numerous DL applications to model
    and analyze complex problems. Due to the differing properties of the input data,
    common DL architectures such as Convolutional Neural Networks (CNNs) (LeCun et al.,
    [1995](#bib.bib105)) or Recurrent Neural Networks (RNNs) (Rumelhart et al., [1986](#bib.bib151);
    Hochreiter and Schmidhuber, [1997](#bib.bib76)) can not easily be used for DL
    on graphs. Therefore, a new type of Deep Neural Network (DNN) architecture has
    been developed in the late 2000s, the so-called Graph Neural Network (GNN) (Gori
    et al., [2005](#bib.bib61); Scarselli et al., [2008](#bib.bib156)). This architecture
    bridges the gap between graph processing and DL by combining message passing with
    neural network (NN) operations. The field of applications of GNNs ranges from
    recommendation systems (Fan et al., [2019](#bib.bib38); Ying et al., [2018](#bib.bib201)),
    computer vision (Gao et al., [2020a](#bib.bib49); Sarlin et al., [2020](#bib.bib155))
    and natural language processing (Yao et al., [2019](#bib.bib200); LeClair et al.,
    [2020](#bib.bib104)) to biology and medicine (Gaudelet et al., [2021](#bib.bib53);
    Gao et al., [2020b](#bib.bib50)).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the ever-growing size of real world graphs (Hu et al., [2021](#bib.bib77))
    and deeper GNN models (Liu et al., [2020](#bib.bib113); Li et al., [2020](#bib.bib107),
    [2021b](#bib.bib106)), the need for efficient GNN training solutions has emerged
    that aim to process large-scale graphs in a fast and efficient manner. With large
    data sets containing millions of nodes and billions of edges (Hu et al., [2021](#bib.bib77)),
    a high level of parallelization is demanded along with the opportunity to run
    the computations on distributed architectures. As a result, current research increasingly
    deals with developing large-scale GNN systems (Zheng et al., [2020](#bib.bib214);
    Wang et al., [2021a](#bib.bib184); Jia et al., [2020a](#bib.bib88)). Multiple
    issues arise when designing such a system: First, systems originally designed
    for DL or for graph processing cannot be directly used for GNN training as the
    former do not support graph processing operations and the latter do not support
    DL operations. A vertex in a graph usually only is connected to few other vertices
    leading to many zero values in the graph adjacency matrix. In contrast, DL operations
    incorporate high-dimensional features leading to dense matrices. Consequently,
    both sparse and dense matrix operations need to be supported. Thus, specialized
    frameworks are being developed incorporating and optimizing both types of tasks.
    Second, redundant computations and repeated data access might occur during a training
    iteration. For instance, if nodes share the same neighbor, the neighbors’ activation
    will be computed multiple times in most cases since the computation of the nodes
    is regarded independently of one another (Chen et al., [2022](#bib.bib19); Jia
    et al., [2020b](#bib.bib89)). Thus, the same activation is calculated redundantly.
    Therefore, the need for appropriate memory management and inter-process communication
    is enforced. Third, the interdependence of training samples is challenging and
    leads to increased communication between the machines. One has to decide how to
    partition and distribute the graph among the machines and which strategy to choose
    for synchronizing intermediate results.'
  prefs: []
  type: TYPE_NORMAL
- en: While there are many works proposing GNN systems that aim to solve the above
    issues and propose optimization methods, there is an acute lack of overview, categorization,
    and comparison. There are numerous surveys that classify either scalable graph
    processing or DL systems, but rarely any articles coping with systems for GNNs.
    Hence, we aim to fill this gap by giving an overview and categorization of large-scale
    GNN training techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Related Surveys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the one hand, there are surveys on graph processing systems. For instance,
    Batafari et al. (Batarfi et al., [2015](#bib.bib8)) provide an overview of large-scale
    graph processing systems and present five popular platforms in detail. In addition,
    they investigate the performance using selected algorithms and graph datasets.
    Heidari et al. (Heidari et al., [2018](#bib.bib71)) and Coimbra et al. (Coimbra
    et al., [2021](#bib.bib28)) discuss and categorize graph processing systems in
    regard to concepts such as partitioning, communication and dynamism. While the
    above papers give a general overview of systems irrespective of the hardware set
    up, Shi et al. (Shi et al., [2018](#bib.bib164)) focus on graph processing on
    GPUs. They distinguish between graph processing systems on a single GPU and others
    with a multi GPU setting. Another specialized work is done by Kalavri et al. (Kalavri
    et al., [2017](#bib.bib91)) where programming abstractions for large-scale graph
    processing systems are investigated and evaluated. Gui et al. (Gui et al., [2019](#bib.bib63))
    concentrate on preprocessing, parallel graph computing and runtime scheduling
    from the hardware side. Xu et al. (Xu et al., [2014](#bib.bib195)) further explore
    hardware acceleration for graph processing and mainly investigate GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, an overview of systems for DNN training is provided in numerous
    works. Chahal et al. (Chahal et al., [2020](#bib.bib17)) give an overview of distributed
    training methods and algorithms. The authors also dive into frameworks for distributed
    DNN training. In their survey, Ben-Nun et al. (Ben-Nun and Hoefler, [2019](#bib.bib9))
    give insights into parallelization strategies for DL. They approach the problem
    from a theoretical angle, model different types of concurrency and explore distributed
    system architectures. The survey by Zhang et al. (Zhang et al., [2018](#bib.bib212))
    introduces distributed acceleration techniques from the algorithm, distributed
    system and applications side. In addition, the price and cost of acceleration
    as well as challenges and limitations are presented. Mayer and Jacobsen (Mayer
    and Jacobsen, [2020](#bib.bib125)) investigate challenges, techniques and tools
    for distributed DL. In their paper, a selection of 11 open-source DL frameworks
    are analyzed and compared. The key aspects of Ouyang et al. (Ouyang et al., [2021](#bib.bib135))
    and Tang et al. (Tang et al., [2020](#bib.bib171)) are communication optimization
    strategies when performing distributed DL. The former highlight algorithm optimizations
    as well as network-level optimizations for large-scale DNN training, the latter
    present communication synchronization methods, compression techniques, systems
    architectures, and different types of parallelization. Other research focusing
    on communication optimizations for large-scale distributed DL is done by Shi et
    al. (Shi et al., [2020](#bib.bib163)). Instead of examining the scaling problem
    from a qualitative perspective, the authors pursue a quantitative approach. Therefore,
    they conduct experiments with selected communication optimization techniques on
    a GPU cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To our knowledge, there are many surveys on general aspects of GNNs, like methods,
    architectures and applications (Zhou et al., [2020](#bib.bib216); Zhang et al.,
    [2020b](#bib.bib210); Hamilton et al., [2017b](#bib.bib68); Chami et al., [2020](#bib.bib18);
    Wu et al., [2020](#bib.bib190)), but only few study optimizations for GNN training
    (Abadal et al., [2021](#bib.bib2); Besta and Hoefler, [2022](#bib.bib10); Serafini
    and Guan, [2021](#bib.bib160)). Abadal et al. (Abadal et al., [2021](#bib.bib2))
    provide an overview of algorithms and accelerators for GNN training. They focus
    on methods from the software side as well as hardware specific optimizations,
    but do not draw parallels to methods introduced by graph processing systems, which
    is one of our core contributions. Serafini et al. (Serafini and Guan, [2021](#bib.bib160))
    also explore scalable GNN training. However, they specialize on the comparison
    of whole-graph and sample-based training and do not investigate other techniques
    for large-scale GNN training. Besta and Hoefler (Besta and Hoefler, [2022](#bib.bib10))
    provide an in-depth analysis of accelerator techniques for GNN training. Their
    main contribution lies in the field of parallelism techniques whereas we give
    insight into the overall GNN training pipeline, corresponding optimizations and,
    most importantly, their origin. In contrast to all of the above, our focus lies
    on large-scale systems for GNNs and the corresponding optimization techniques
    within each step of the training process. Additionally, we provide an overview
    of the two background technologies, systems for graph processing and systems for
    DNN training, and draw the connection to GNN systems.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Our Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This survey examines distributed systems for scalable GNN training. We differentiate
    ourselves from other surveys by not only presenting methods for GNN systems, but
    also by giving insights into the two crucial background topics, systems for graph
    processing and systems for DNN training. It is important to know about the basic
    ideas behind those topics to get a better understanding of how and why these techniques
    are used in large-scale GNN training. We establish connections between the fields
    and show that most techniques used for large-scale GNN training are inspired by
    methods either from graph processing or DNN training. This way, we bring together
    the graph processing, DL and systems community that are all contributing to distributed
    GNN training. In addition, we categorize and compare the methods regarding questions
    of partitioning, sampling, inter-process communication, level of parallelism and
    scheduling. This should make it easier for researchers and practitioners to asses
    the usability of the presented systems and methods to their own application scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3\. Structure of the Survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We structure this survey as follows: In Section [2](#S2 "2\. Foundations ‣
    The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey"), we provide fundamentals of
    distributed systems for graph processing and for DL. We discuss the main aspects
    for achieving parallelism, scalability and efficiency. In Section [3](#S3 "3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey"),
    we relate the insights from graph processing systems and DL systems to facilitate
    the comprehension of methods used by systems for GNN training. Finally, open challenges,
    limitations and research trends are presented in Section [4](#S4 "4\. Discussion
    and Outlook ‣ The Evolution of Distributed Systems for Graph Neural Networks and
    their Origin in Graph Processing and Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Foundations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes relevant topics building the basis of distributed GNN
    training. We start by giving a short introduction to general graph processing
    followed by presenting selected graph processing systems. Furthermore, we explore
    the basic steps of neural network training, and how the training process can be
    realized in a distributed fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Graph Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A graph $G$ is a non-linear data structure, meaning the data elements are not
    sequentially or linearly ordered. It can be formally described as $G=(V,E)$ where
    $V$ denotes the set of vertices and $E$ the set of edges. A vertex $v$, also known
    as node, represents an object and the edge $e$ describes a relation between two
    vertices. For example, a vertex can represent a person and an edge models the
    relationship between two persons. There are many different types of graphs, e.g.,
    the graph can contain cycles (cyclic graph) or no cycles (acyclic graph), the
    edges can be directed or undirected and a weight can be assigned to each edge
    signifying its importance (weighted graph). Depending on the density of edges
    within the graph, it can be sparse, dense or fully connected.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs occur in numerous domains including social networks (Tang and Liu, [2010](#bib.bib170);
    Fan, [2012](#bib.bib36); Gupta et al., [2013](#bib.bib65)), route optimization
    and transportation (Sobota et al., [2008](#bib.bib166); Czerwionka et al., [2011](#bib.bib29)),
    natural language processing (Miller, [1995](#bib.bib133); Manning et al., [2014](#bib.bib121)),
    recommendation systems (Huang et al., [2002](#bib.bib83); Silva et al., [2010](#bib.bib165);
    Guo et al., [2020](#bib.bib64)) and the natural sciences (Balaban, [1985](#bib.bib7);
    Ralaivola et al., [2005](#bib.bib143); Mason and Verwoerd, [2007](#bib.bib123)).
    For instance, a social network graph models the relation between users, graphs
    in natural language processing represent the relations between words or sentences,
    and in the natural sciences, graphs help to model the constitution of molecules.
  prefs: []
  type: TYPE_NORMAL
- en: Graph processing algorithms explore properties of the vertices, relations between
    them or characteristics of a subgraph or the whole graph. Depending on the use
    case, different types of graph problems need to be solved by those such as traversals,
    component identification, community detection, centrality calculation, pattern
    matching and graph anonymization (Dominguez-Sal et al., [2010](#bib.bib34); Coimbra
    et al., [2021](#bib.bib28)). Algorithms like depth-first search (Tarjan, [1972](#bib.bib172)),
    breadth-first search (Bundy and Wallen, [1984](#bib.bib15)), Dijkstra (Dijkstra
    et al., [1959](#bib.bib32)), Girvan-Newman algorithm (Girvan and Newman, [2002](#bib.bib58)),
    and PageRank (Page et al., [1999](#bib.bib138)) are among the most famous to tackle
    the presented problems.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Distributed Graph Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To process large graphs and to accelerate computation, graph processing is parallelized
    and distributed across machines. One of the most important methods to process
    data in parallel is the MapReduce computing model (Dean and Ghemawat, [2008](#bib.bib31)).
    MapReduce incorporates two user-defined primitive functions map and reduce. The
    map function takes a key-value pair as input and processes it in regard to the
    specified functionality. All intermediate results are grouped and passed to the
    reduce function which merges them to obtain a smaller set of values. A master
    node is responsible for assigning the map and reduce functions to the worker nodes.
    Each map worker takes a shard of the input pairs, applies the given function and
    stores the intermediate results locally. The reduce workers load the intermediate
    files, apply the reduce function and write the results to output files. With the
    help of the MapReduce computing model, large amounts of data can be handled; however,
    it reaches its limits when employing it for graph processing. A reason for MapReduce
    being unsuitable for graph processing is that the vertices within a graph are
    not independent. When performing a computing step of a graph algorithm, knowledge
    about multiple vertices is needed, leading to time- and resource-consuming data
    accesses. While graph processing algorithms may perform multiple iterations, the
    MapReduce model is optimized for sequential algorithms (McCune et al., [2015](#bib.bib130)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the Pregel system (Malewicz et al., [2010](#bib.bib120)) was developed
    which specializes on parallel graph processing. It supports iterative computations
    more efficiently than MapReduce and keeps the data set in memory to facilitate
    repeated accesses. Further, a vertex-centric programming model is provided allowing
    the user to express graph algorithms more easily. Thus, the user thinks of how
    the algorithm is modeled from the perspective of a vertex rather than dataflows
    and transformation operators. During the process, messages are passed between
    the vertices in a Bulk Synchronous Parallel (BSP) (Valiant, [1990](#bib.bib176))
    manner. Hence, the messages are transmitted synchronously after all machines have
    finished their computation step. The iterative model works as follows: in each
    iteration, called superstep $S$, a user-defined function is executed in parallel
    for each vertex $v$. Aside from computing the vertex value, the function can pass
    messages to other vertices along outgoing edges during an iteration. In superstep
    $S$, it can send a message which will be received in superstep $S+1$ while messages
    sent to $v$ in superstep $S-1$ can be read. Similar to MapReduce, Pregel uses
    a reduction mechanism called ”aggregator” to combine resulting values and make
    them available in the next superstep $S+1$. The introduced synchronous superstep
    model can be used for various graph algorithms and is the basis for subsequent
    systems (Salihoglu and Widom, [2013](#bib.bib153); Khayyat et al., [2013](#bib.bib95);
    Bu et al., [2014](#bib.bib14)).'
  prefs: []
  type: TYPE_NORMAL
- en: The GAS (gather-apply-scatter) model used in PowerGraph (Gonzalez et al., [2012](#bib.bib59))
    follows the four steps gather, sum, apply and scatter. The gather and sum operations
    resemble the map and reduce scheme aiming at assembling information about the
    vertices’ neighborhood. This phase is invoked on the edges adjacent to the vertex
    and executed locally on each machine. The results are sent to the master which
    runs the apply function with the aggregated information. It sends the resulting
    updates to all machines which execute the scatter-phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the two programming models, a general distributed iterative vertex-centric
    graph processing scheme can be described: Before starting the iterative computation,
    the graph is split into partitions which are distributed across the machines.
    After that, the iterative process starts. Each vertex aggregates messages and
    applies a given function to compute a new state. Subsequently, the vertex state
    is updated. Then, the vertex passes messages to adjacent vertices containing the
    updated information and the aggregation phase starts over.'
  prefs: []
  type: TYPE_NORMAL
- en: The vertex-centric paradigm lets the user intuitively define computations on
    graphs, but it reaches its limitations when processing real-world graphs with
    skewed degree distributions, large diameters and high densities (Yan et al., [2014](#bib.bib197)).
    Therefore, block-centric models have been developed (Yan et al., [2014](#bib.bib197);
    Fan et al., [2017](#bib.bib39); Tian et al., [2013](#bib.bib174)). Instead of
    sending messages from a vertex to its neighbors, blocks comprising of multiple
    vertices send messages to other blocks. Inside the blocks, information moves freely
    (Tian et al., [2013](#bib.bib174)) resulting in reduced communication cost and
    improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this general iterative model, we present and categorize selected distributed
    graph processing systems. We start by discussing partitioning strategies and how
    to store the resulting subgraphs. We distinguish between the synchronization method
    used in the message propagation phase and show how the messages can be transmitted.
    Table [1](#S2.T1 "Table 1 ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey") summarizes the most important
    properties of the selected systems and categories.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1. Categorization of different distributed graph processing systems
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Partitioning | Execution Mode | Message Propagation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Year | System |  | edge-cut | vertex-cut | synchronous | asynchronous | push
    | pull |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2010 | Pregel | (Malewicz et al., [2010](#bib.bib120)) | ✓ |  | ✓ |  | ✓
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | Apache Giraph | (FOUNDATION, [2012](#bib.bib45)) | ✓ |  | ✓ |  | ✓
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | GraphLab | (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117))
    | ✓ |  |  | ✓ |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | Distributed GraphLab | (Low et al., [2012](#bib.bib116)) | ✓ |  |  |
    ✓ |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | PowerGraph | (Gonzalez et al., [2012](#bib.bib59)) |  | ✓ | ✓ | ✓
    |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | GPS | (Salihoglu and Widom, [2013](#bib.bib153)) | ✓ |  | ✓ |  | ✓
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | GRACE | (Wang et al., [2013](#bib.bib178)) | ✓ |  |  | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | PowerLyra | (Chen et al., [2015b](#bib.bib22)) | ✓ | ✓ | ✓ | ✓ |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: 2.2.1\. Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Graph sampling is a preprocessing step aiming to make the graph sparser by removing
    vertices or edges to reduce processing time and memory consumption. A key challenge
    here is to ensure that important graph structures are preserved. Iyer et al. (Iyer
    et al., [2018b](#bib.bib85)) calculate a probability signifying whether an edge
    between vertex $a$ and vertex $b$ should be kept in the graph or not. The probability
    value depends on the average degree of the graph, the out-degree of vertex $a$,
    the in-degree of vertex $b$ and the level of sparsification $s$ which can be chosen
    by the user. In this manner, less memory is needed while preserving the most important
    structures within the graph. The fast, approximate ASAP engine (Iyer et al., [2018a](#bib.bib84))
    for graph pattern mining consists of two phases, namely the sampling and closing
    phase. In the sampling phase, the graph is treated as a stream of edges. The edges
    are either randomly selected or depending on the previously streamed ones. Then,
    the closing phase awaits certain edges to complete patterns. This technique helps
    to preserve certain structures within the graph while excluding certain edges.
    An application-aware approach that drops certain messages is proposed by Schramm
    et al. (Schramm et al., [2022](#bib.bib158)). In every superstep, a given percentage
    of messages is identified as least important and dropped on-the-fly. The calculation
    of the importance value is dependent on the desired application and the deployed
    graph algorithm. Therefore, this method can be used in a plethora of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Partitioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There exist numerous approaches on graph partitioning algorithms that the systems
    adopt and refine. Those algorithms either follow the edge-cut (Karypis and Kumar,
    [1998a](#bib.bib93); Stanton and Kliot, [2012](#bib.bib167); Tsourakakis et al.,
    [2014](#bib.bib175)), vertex-cut (Schlag et al., [2019](#bib.bib157); Hanai et al.,
    [2019](#bib.bib69); Petroni et al., [2015](#bib.bib141); Mayer et al., [2018](#bib.bib124);
    Mayer and Jacobsen, [2021](#bib.bib126); Mayer et al., [2022](#bib.bib128)) or
    hybrid-cut model (Fan et al., [2022](#bib.bib40); Chen et al., [2015b](#bib.bib22)).
    Further, we distinguish between offline and online partitioners. While offline
    partitioners (Karypis, [1997](#bib.bib92); Hendrickson and Leland, [1993](#bib.bib72))
    determine the partitions according to the whole graph, online partitioners (Tsourakakis
    et al., [2014](#bib.bib175); Petroni et al., [2015](#bib.bib141); Mayer et al.,
    [2018](#bib.bib124)) stream vertices or edges and assign the vertex or edge to
    a partition on-the-fly. An interesting approach is HEP (Mayer and Jacobsen, [2021](#bib.bib126))
    where offline and online components are combined into a hybrid scheme.
  prefs: []
  type: TYPE_NORMAL
- en: To asses the quality of the partitions, metrics like replication factor, communication
    cost and workload balancing are used. The replication factor indicates the ratio
    between the number of replicas and the total number of vertices. Based on that,
    the communication cost can be determined. Whenever an edge is cut, communication
    between the partitions is needed. When using an edge-cut method for example, the
    communication increases proportional to the number of edges that are cut. The
    so-called workload balancing aims to partition the graph in a way such that during
    the computation phase of the graph processing algorithm the load is balanced among
    the workers.
  prefs: []
  type: TYPE_NORMAL
- en: There exist various cost functions that help to form partitions. Depending on
    the goal and the application, the cost function needs to be defined differently.
    In the Linear Deterministic Greedy (LDG) (Stanton and Kliot, [2012](#bib.bib167))
    method, a vertex is allocated to the partition where it has most edges. This is
    combined with a penalty function indicating the capacity of a partition. Consequently,
    communication costs are minimized and balanced workloads across partitions is
    ensured. FENNEL (Tsourakakis et al., [2014](#bib.bib175)) unifies two heuristics,
    a vertex is either assigned to the cluster that shares the largest number of neighbors
    or the one with the smallest number of non-neighbors. This results in a minimal
    number of edges that are cut and consequently minimal communication costs. The
    High Degree (are) Replicated First (HDRF) (Petroni et al., [2015](#bib.bib141))
    method handles graphs with skewed distribution where the degree of the nodes highly
    varies. The goal is to balance the load evenly by cutting and replicating high-degree
    vertices. 2PS (Mayer et al., [2020](#bib.bib127)) gathers clustering information
    in a preprocessing phase which is then used in the scoring mechanism. Instead
    of defining one particular cost function, Fan et al. (Fan et al., [2022](#bib.bib40))
    use an application-driven approach. A cost model based on a given application
    algorithm is minimized in order to partition the graph. This results in an adaptive
    partitiong strategy suitable for numerous use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b59722995ade3a63e6c1bf3d402cf951.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) edge-cut
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9171e27aa2ed74e51312b00ef33cf115.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) vertex-cut
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1. Edge-cut and vertex-cut partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pregel (Malewicz et al., [2010](#bib.bib120)) uses one of the simplest methods
    to initially partition the graph. Here, the partitioning is based on the vertex
    ID using a hash function $hash(ID)\,mod\,N$ where $N$ is the number of partitions.
    The partitions are then evenly distributed across all workers. The underlying
    partitioning principle is called edge-cut as the edges are cut and replicated
    while the vertices are assigned to the different machines (Fig. [1(a)](#S2.F1.sf1
    "In Figure 1 ‣ 2.2.2\. Partitioning ‣ 2.2\. Distributed Graph Processing ‣ 2\.
    Foundations ‣ The Evolution of Distributed Systems for Graph Neural Networks and
    their Origin in Graph Processing and Deep Learning: A Survey")). Other systems
    relying on edge-cut partitioning are Apache Giraph (FOUNDATION, [2012](#bib.bib45)),
    GraphLab (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117)), Distributed
    GraphLab (Low et al., [2012](#bib.bib116)), Graph Processing System (GPS) (Salihoglu
    and Widom, [2013](#bib.bib153)) and GRACE (Wang et al., [2013](#bib.bib178)).
    Compared to other partitioning strategies, the presented edge-cut methods induce
    less computation cost and less overheads. Conversely, the following methods aim
    to reduce the cut size at the cost of higher runtimes and memory overheads. However,
    the obtained partitions are of improved quality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With edge-cut partitioning, the number of vertices is balanced across partitions,
    however, the number of edges per partition might highly vary. To obtain more equal
    partitions and to improve distributed processing of natural graphs, Gonzalez et
    al. propose balanced vertex-cut partitioning (Fig. [1(b)](#S2.F1.sf2 "In Figure
    1 ‣ 2.2.2\. Partitioning ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")) in their PowerGraph (Gonzalez
    et al., [2012](#bib.bib59)) model. Natural graphs typically are imbalanced and
    thus, difficult to partition. Some vertices have a high degree of outgoing edges
    while other vertices are of low degree. Hence, the computation costs per partition
    vary a lot. For that reason, the system employs balanced vertex-cut partitioning.
    The novel partitioning strategy improves the processing of natural graphs by equally
    distributing the edges between the machines whereas adjacent vertices are mirrored
    on the machines. One vertex among all replicas is randomly assigned as the master
    to compute and update the vertex state. All mirrors keep local copies of the vertex
    state.'
  prefs: []
  type: TYPE_NORMAL
- en: A dynamic repartitioning strategy to reduce communication is introduced in GPS
    (Salihoglu and Widom, [2013](#bib.bib153)). First, the system partitions the input
    graph with a standard partitioning technique. During the execution of the graph
    processing algorithm, communication is observed to determine which vertices to
    reassign to another worker and when to do so. Messages that are passed over the
    network are significantly reduced because repeated vertex accesses are performed
    by workers that already loaded the vertex in a previous iteration. Therefore,
    processing time is sped up, the workload is balanced and scaling to larger graphs
    is improved.
  prefs: []
  type: TYPE_NORMAL
- en: PowerLyra (Chen et al., [2015b](#bib.bib22)) further optimizes the processing
    of natural graphs by introducing a hybrid-cut partitioning algorithm which combines
    edge- and vertex-cut. Furthermore, it handles high-degree vertices and low-degree
    vertices separately to minimize the replication of edges and vertices. The proposed
    balanced p-way hybrid-cut algorithm exploits a form of edge-cut if a vertex has
    a low number of outgoing edges and vertex-cut if a vertex is of high degree.
  prefs: []
  type: TYPE_NORMAL
- en: GridGraph (Zhu et al., [2015](#bib.bib217)) introduces a grid representation
    for graphs to speed up partitioning. First, the vertices are partitioned into
    $P$ chunks where each chunk contains connected vertices. The edges are sorted
    into the resulting $P\times P$ grid according to their source and destination
    vertices. In this method, the list of edges does not need to be ordered leading
    to small preprocessing times. Further, the grid representation can facilitate
    the execution of the following graph processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: As there are numerous partitioning strategies with different characteristics
    and objectives, it is challenging to choose the optimal one for a given application.
    Therefore, some experimental studies investigate the performance and resource
    usage of different strategies (Gill et al., [2018](#bib.bib56); Pacaci and Özsu,
    [2019](#bib.bib137); Abbas et al., [2018](#bib.bib4)) while the EASE system (Merkel
    et al., [pear](#bib.bib132)) provides a quantitative prediction and automatic
    selection.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. Inter-Process Communication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once the graph has been partitioned, it has to be decided how the worker processes
    synchronize their data, e.g., by sending messages or by accessing shared memory.
    In the former case, the systems store the subgraphs locally on the assigned machines
    and exchange synchronization messages. GraphLab (Low, [2013](#bib.bib115); Low
    et al., [2014](#bib.bib117)) supports the latter case and uses a shared-memory
    abstraction. A data graph accessible for all workers stores the program state
    as well as the corresponding data structures. Oriented on GraphLab, PowerGraph
    (Gonzalez et al., [2012](#bib.bib59)) also performs computation following a shared-memory
    view.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4\. State Synchronization and Scheduling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When iteratively executing graph processing algorithms, the computation steps
    can be performed following a synchronous, asynchronous or hybrid scheme. The underlying
    basic principle Pregel follows is the BSP model. Every node goes through an iteration
    by aggregating and combining the desired features and subsequently updating the
    state. When the node has finished the computation step, it waits for all other
    nodes to finish before continuing with the next iteration. This assures that every
    node shares the same parameters.
  prefs: []
  type: TYPE_NORMAL
- en: While the synchronous mode works well in a lot of cases, it can be inefficient
    in other cases. An example is the label propagation algorithm for community detection
    (Raghavan et al., [2007](#bib.bib142)). Here, each vertex is assigned an initial
    label. In each iteration, the vertex adopts the label the maximum of neighbors
    has. After some propagation rounds through the network, dense communities consent
    to the same label. If the synchronous mode is chosen and the graph is bipartite,
    meaning each vertex of one subgraph connects to each vertex in another subgraph,
    the labels might oscillate and change after each iteration. This makes it impossible
    for the algorithm to terminate, as the labels therefore need to be stable. To
    solve this issue, asynchronous execution is used. Asynchronous processing means
    that vertices can already read state updates of the current iteration in addition
    to the updates of the previous iterations. In GNN training, the asynchronous mode
    has the ability to prioritize specific state updates which results in faster convergence
    of the overall computation. Another important benefit is the avoidance of long
    idle times through stragglers.
  prefs: []
  type: TYPE_NORMAL
- en: 'GraphLab (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117)) uses an
    asynchronous execution method. Three steps are performed independently by each
    worker: fold, merge and apply. In the fold step, the data across all vertices
    is gathered. If provided, a merge operation is performed. Otherwise, the apply
    function directly completes the computation and stores the data in shared memory.
    This process is done without regarding the phase of computation the other workers
    are currently passing. GRACE (Wang et al., [2013](#bib.bib178)) allows for high
    performance execution by adapting the BSP model to permit asynchronous processing.
    Hence, idle times are minimized, but it is possible that the workers perform their
    iteration with stale data. As both synchronization methods have their benefits
    and drawbacks, PowerGraph (Gonzalez et al., [2012](#bib.bib59)) allows the user
    to either choose a synchronous or asynchronous mode. The synchronous mode is performed
    analogously to the synchronous execution in Pregel, the asynchronous mode resembles
    the GraphLab computation model.'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, choosing a mode manually requires the user to deeply understand the
    graph processing system and often does not lead to the optimal performance. Hysync
    (Xie et al., [2015](#bib.bib191)) removes this issue by automatically switching
    between the synchronous and asynchronous mode according to a set of heuristics.
    The heuristics aim to predict the performance of the current mode and determine
    the step at which a switch to the other mode can be beneficial. In the Adaptive
    Asynchronous Parallel (AAP) model (Fan et al., [2020](#bib.bib37)), each worker
    decides on its own when to start the next computation step depending on two parameters.
    The first parameter is the relative progress of a worker compared to the other
    ones. The second parameter indicates data staleness. Consequently, stragglers
    are avoided while also reducing stale computations.
  prefs: []
  type: TYPE_NORMAL
- en: PowerLyra (Chen et al., [2015b](#bib.bib22)) goes a step further and not only
    provides both synchronization modes, but also distinguishes between high- and
    low-degree vertices to determine how they are processed. The former are processed
    based on the GAS model (i.e. gather, apply, scatter) (Gonzalez et al., [2012](#bib.bib59)).
    The master vertex activates the mirrors to execute the gather function and the
    results are sent back to the master. After having received all messages, the master
    runs the apply function. A combined message with the updated data and the activation
    for the scatter function is sent from master to mirror. In contrast to the original
    GAS model, PowerLyra combines the apply and scatter messages from master to mirror
    vertices to minimize communication. The system handles the low-degree vertices
    similar to GraphLab. The master vertex performs the gather and apply phase locally.
    Hereafter, activation and update messages are combined and sent to the mirrored
    vertices. Each mirror then performs the scatter phase. Because of the adapted
    scheme, replication of edges is eliminated and in each iteration, only one message
    needs to be sent by a mirror.
  prefs: []
  type: TYPE_NORMAL
- en: Opposed to the synchronous mode, using an asynchronous execution implies the
    need of a scheduling scheme which can influence the convergence of the overall
    computation. Besides, a high level of concurrent execution can be achieved as
    scheduling helps to determine the order of the tasks and assigns the tasks to
    the machines. GraphLab (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117))
    provides the so-called set scheduler. Based on the dependencies between the tasks,
    an execution plan is established and an overall speed-up of the computations can
    be observed. In their GRACE (Wang et al., [2013](#bib.bib178)) system, Wang et
    al. incorporate a customizable scheduling policy. The system loosens the restrictions
    of the BSP model and lets the user prioritize specific vertices for faster convergence.
    One can choose an individual set of vertices and also the desired processing order
    of those. Then, the system calculates a scheduling priority for each vertex to
    determine the overall execution order.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5\. Message Propagation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Regardless of the execution mode and scheduling technique, messages need to
    be transmitted between the vertices containing the updated states. There are two
    main methods to transfer messages: push and pull. While Pregel-based systems (Malewicz
    et al., [2010](#bib.bib120); FOUNDATION, [2012](#bib.bib45); Salihoglu and Widom,
    [2013](#bib.bib153); Wang et al., [2013](#bib.bib178)) use the push-operation,
    systems supporting the GAS model (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117),
    [2012](#bib.bib116); Gonzalez et al., [2012](#bib.bib59); Chen et al., [2015b](#bib.bib22))
    rely on pulling the messages. After each iteration, Pregel-based systems synchronously
    propagate the update messages. All vertices simultaneously push their messages,
    meaning each vertex directly sends a message to all its adjacent vertices containing
    the updated state. GraphLab (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117)),
    on the other hand, stores the data graph with associated features in shared memory.
    Like this, all workers are able to access the data any time. At the beginning
    of an iteration, the worker pulls the current data graph via the gather operation
    to perform calculations on the most recent features. At the end of an iteration,
    the worker updates the data graph with the newly computed state.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Distributed Neural Network Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A neural network consists of a number of connected nodes, called neurons, organized
    in one or multiple layers. Each neuron takes an input and processes it in regard
    to given weights and an update function. When having computed the new value, an
    activation function (Sharma et al., [2017](#bib.bib161); Ramachandran et al.,
    [2017](#bib.bib144)) decides how important the output value is. The values are
    passed through the network until the last set of neurons is reached, called forward
    pass. After that, a loss function is computed in regard to the calculated and
    expected output. In order to minimize the loss, the weights need to be adapted.
    The backpropagation (Rumelhart et al., [1995](#bib.bib150)) algorithm is used
    to adjust weights in a backward pass through the network. A widely used technique
    to do so is stochastic gradient descent (SGD) (Robbins and Monro, [1951](#bib.bib146);
    Ruder, [2016](#bib.bib149)). It computes the partial gradients by considering
    the calculated loss. With the help of those gradients, the weights are adjusted.
    This is called backward pass. The whole process, forward and backward pass, is
    iteratively repeated until convergence or a maximum number of iterations is reached.
    The final neural network weights can be used to make predictions on unseen data
    (Widrow and Lehr, [1990](#bib.bib187); Lawrence, [1993](#bib.bib103); Goodfellow
    et al., [2016](#bib.bib60)).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the growing amount of training data and the increase of model size, distributed
    neural network training has become necessary. In the following, we discuss different
    techniques to scale neural network training with regard to parallelism, execution
    mode and coordination. Table [2](#S2.T2 "Table 2 ‣ 2.3\. Distributed Neural Network
    Training ‣ 2\. Foundations ‣ The Evolution of Distributed Systems for Graph Neural
    Networks and their Origin in Graph Processing and Deep Learning: A Survey") gives
    an overview of the described systems and techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2. Categorization of systems for distributed neural network training
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Parallelism | Synchronization Mode | Coordination |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Year | System |  | data | model | synchronous | asynchronous | centralized
    | decentralized |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | DistBelief | (Dean et al., [2012](#bib.bib30)) | ✓ |  |  | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2014 | Project Adam | (Chilimbi et al., [2014](#bib.bib26)) |  | ✓ |  | ✓
    | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | MALT | (Li et al., [2015a](#bib.bib108)) | ✓ |  |  | ✓ |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | Tensorflow | (Abadi et al., [2016](#bib.bib3)) | ✓ | ✓ | ✓ | ✓ | ✓
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | Ako | (Watcharapichat et al., [2016](#bib.bib185)) | ✓ |  |  | ✓ |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CROSSBOW | (Koliousis et al., [2019](#bib.bib100)) | ✓ |  | ✓ |  |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | PyTorch | (Paszke et al., [2019](#bib.bib139)) | ✓ | ✓ | ✓ | ✓ | ✓
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 2.3.1\. Parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data parallelism allows for parallel training on multiple processors. Therefore,
    the data is divided into a fixed number of subsets. Each worker is assigned a
    subset which it processes on a local replica of the model. After that, the resulting
    model parameters are exchanged with the other workers. The next iteration is performed
    with the updated parameter configuration. This process is repeated until convergence.
    Systems using data parallelism are for instance MALT (Li et al., [2015a](#bib.bib108))
    and Ako (Watcharapichat et al., [2016](#bib.bib185)). If the model itself is too
    big to fit on a single machine (Krizhevsky et al., [2012](#bib.bib101); Brown
    et al., [2020](#bib.bib13)), it is split up. In model parallelism, the computation
    nodes process the whole data set on their partition of the model. After having
    finished the computation, the intermediate output of the forward pass is passed
    to the machines responsible for computing the subsequent layer. Here, scheduling
    is important to efficiently coordinate the training process. The most intuitive
    way to partition the model is layer-wise, meaning each layer is assigned to one
    node. However, this sometimes does not benefit parallelism as the worker controlling
    the current layer needs to wait for the worker handling the prior layer to finish
    before being able to run their computation. A system exploiting model parallelism
    is Project Adam (Chilimbi et al., [2014](#bib.bib26)) where each machine is assigned
    a certain part of the model. Another possibility to distribute the model more
    intelligently is according to its specific architecture. Suitable could be architectures
    like the two-fold Siamese network (He et al., [2018](#bib.bib70)) where some of
    the components can be easily run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The two techniques are combined, e.g., by PipeDream (Narayanan et al., [2019](#bib.bib134))
    and GPipe (Huang et al., [2019](#bib.bib82)), into the so-called hybrid or pipeline
    parallelism. Here, the data as well as the model are shared among the workers.
    In contrast to PipeDream, the GPipe algorithm further splits the input mini-batches
    into chunks to maximize the number of concurrent calculations within an iteration.
    In general, the use of hybrid parallelism can significantly improve training speed
    in comparison to data and model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2\. Synchronization Mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When training neural networks in a distributed fashion, it is important that
    the machines regularly exchange parameter updates or intermediate results to ensure
    convergence. This can either follow a synchronous, asynchronous or hybrid mode
    (Dean et al., [2012](#bib.bib30); Chen et al., [2016](#bib.bib21); Jin et al.,
    [2016](#bib.bib90)). When using a synchronous mode, the updates are sent simultaneously
    to the other nodes as soon as all machines have finished their computation. Then,
    the nodes continue with the computation of the next iteration. In this manner,
    every machine is always aware of the current parameters. A downside of this approach
    is that a single straggler decreases the speed of the whole training process (Cipar
    et al., [2013](#bib.bib27)). A system following the synchronous execution mode
    is CROSSBOW (Koliousis et al., [2019](#bib.bib100)).
  prefs: []
  type: TYPE_NORMAL
- en: Models using asynchronous execution (Dean et al., [2012](#bib.bib30); Chilimbi
    et al., [2014](#bib.bib26); Li et al., [2015a](#bib.bib108); Watcharapichat et al.,
    [2016](#bib.bib185)) eliminate the problem of stragglers by not waiting for all
    workers to finish. Instead, the updates are sent as soon as they are available.
    Thus, the training speed is increased and the resources are efficiently used.
    A drawback of this method is that the worker nodes might not always be up to date,
    therefore computing their updates on stale parameters. The computation of gradients
    on outdated parameters can lead to a slower or no convergence at all which is
    called stale gradients problem (Dutta et al., [2018](#bib.bib35)). An attempt
    to merge the synchronous and asynchronous execution scheme was made by Ho et al.
    (Ho et al., [2013](#bib.bib73)). Their model is based on synchronous execution,
    but incorporates a staleness threshold determining how many time steps two workers
    may be apart until the faster worker needs to wait for the slower worker to finish
    its current computation. In contrast to synchronous execution, this decreases
    the impact of stragglers while also limiting the staleness of the parameters to
    ensure up-to-date computation. Popular frameworks like TensorFlow (Abadi et al.,
    [2016](#bib.bib3)) and PyTorch (Paszke et al., [2019](#bib.bib139)) leave the
    choice of synchronization level to the user. Here, both methods are supported.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3\. Coordination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides determining the synchronization mode, it needs to be decided how the
    parameters are stored and the updates are coordinated. Common ways to do so are
    in a centralized or decentralized manner. The centralized method operates with
    a global parameter server which stores, aggregates and distributes the relevant
    updates. Consequently, all machines share the same set of parameters. However,
    the use of a parameter server introduces a single-point of failure as it is part
    of all update requests. DistBelief (Dean et al., [2012](#bib.bib30)) and Project
    Adam (Chilimbi et al., [2014](#bib.bib26)) are systems operating in a centralized
    fashion. Decentralized systems (Li et al., [2015a](#bib.bib108); Watcharapichat
    et al., [2016](#bib.bib185); Koliousis et al., [2019](#bib.bib100)) eliminate
    the need for a parameter server by passing the update messages directly from machine
    to machine by using a collective communication primitive such as an all-reduce
    operation (Sanders et al., [2019](#bib.bib154)). Here, each machine exchanges
    update information with its peers and combines the received parameters with its
    own. As a result, each machine holds the latest set of parameters. An advantage
    of this approach is that the computation of updates is balanced among all machines
    (Koliousis et al., [2019](#bib.bib100)). As both strategies have their benefits
    and drawbacks, PyTorch (Paszke et al., [2019](#bib.bib139)) leaves the choice
    of coordination to the user.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, there are several ways to perform distributed DNN training. Depending
    on the architecture and the data, one might choose between data, model or hybrid
    parallelism, synchronous or asynchronous updates and a centralized or decentralized
    system. In all methods, parameters need to be exchanged between the machines.
    Therefore, communication is a bottleneck that needs to be addressed when training
    neural networks in a multi-machine setting. In addition, it is important that
    the resources are fully utilized without long idle times (Zhang et al., [2020a](#bib.bib209)).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Systems for Graph Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. Graph Neural Network Basics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term graph neural network (GNN) initially emerged in a work by Gori et al.
    (Gori et al., [2005](#bib.bib61)) and was further investigated by Scarselli et
    al. (Scarselli et al., [2008](#bib.bib156)). It refers to neural network architectures
    that do not take multiple independent or sequenced data samples as input like
    CNNs (LeCun et al., [1995](#bib.bib105)) or RNNs (Sherstinsky, [2020](#bib.bib162)),
    but rather graphs. In contrast to images or text, graphs do not follow a specific
    structure and are not sequentially ordered. A graph $G$ can be formally denoted
    as $G=(V,E)$. It consists of a set of vertices $V$ and a set of edges $E$. A vertex
    $v$ represents an object and is also known as node. An edge $e$ describes the
    relation between two vertices. As graphs are unstructured, it is necessary to
    employ a new type of neural network, the GNN. They combine DNN operations like
    matrix multiplication and convolution with methods known from graph processing
    like iterative message propagation (Jia et al., [2020a](#bib.bib88); Wang et al.,
    [2021c](#bib.bib179)). Due to uniting DNN operations and message passing, GNNs
    are sometimes also denoted as Message Passing Neural Networks (Gilmer et al.,
    [2017](#bib.bib57); Riba et al., [2018](#bib.bib145); Zhang et al., [2020d](#bib.bib206);
    Hamilton, [2020](#bib.bib66)).
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, each vertex of an input graph is initially represented by a feature
    vector, called activation. This initial activation only incorporates information
    about the vertex itself but not about the context within the graph. At each layer,
    a set of DNN operations and message passing steps are performed vertex-wise to
    update the activation values. In the first step of an iteration, each vertex aggregates
    the activations of its adjacent vertices by exchanging messages according to an
    aggregation function
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $a_{v}^{(k)}=AGGREGATE^{(k)}(\{h_{u}^{(k-1)}&#124;u\in N(v)\})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $a_{v}^{(k)}$ denotes the aggregated values of vertex $v$ at the $k$-th
    layer. The term $h_{u}^{(k-1)}|u\in N(v)$ describes the activations of the neighboring
    vertices at the previous layer with $N(v)$ denoting the neighbors of $v$ in the
    given graph. Next, the gathered information is combined and the current value
    of the vertex is updated with an update function. The update function $h_{v}{(k)}$
    can include standard DNN operations like matrix multiplication and is defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $h_{v}^{(k)}=UPDATE^{(k)}(a_{v}^{(k)},h_{v}^{(k-1)})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $h_{v}^{(k)}$ is the new activation of vertex $v$ at the $k$-th layer.
    To obtain the updated activation, the aggregated activations $a_{v}^{(k)}$ are
    combined with the vertices’ activation of the previous layer $h_{v}^{(k-1)}$.
    A special case occurs if $k=1$, then the initial activations $h_{v}{(0)}$ or $h_{u}{(0)}|u\in
    N(V)$ are needed, respectively (Jia et al., [2020a](#bib.bib88); Hamilton, [2020](#bib.bib66)).
    The new values now serve as starting point for the next layer where the activations
    are aggregated and combined again. This process is repeated iteratively. Consequently,
    more and more vertices are explored. After $k$ layers, the $k$-hop neighborhood
    of a vertex is captured. When the final layer has been passed, the final representation
    of a vertex includes information about the vertex itself as well as about the
    vertices within the graph context.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1eaf5e3f926665c3221cd2f0b13c390d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2. Schematic of the GNN training process
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [2](#S3.F2 "Figure 2 ‣ 3.1\. Graph Neural Network Basics ‣ 3\. Systems
    for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural
    Networks and their Origin in Graph Processing and Deep Learning: A Survey"), an
    overview of a complete forward pass is given. It consists of the above described
    steps: (1) fetch the initial weights, (2) pass and aggregate messages from neighboring
    nodes, (3) perform DNN operations and (4) update the weights according to a given
    function. The steps (2) to (4) are repeatedly executed. After $n$ iterations,
    a final model configuration (5) is obtained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogous to the general neural network training described in Section [2.3](#S2.SS3
    "2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution of
    Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey"), a loss function is computed relative to the output
    of the forward pass. Backpropagation is applied during the backward pass in order
    to adapt the weights of the network (Xu et al., [2018a](#bib.bib193)). After multiple
    forward and backward passes, the model can make vertex- and edge-level predictions.
    In order to make assertions about the whole graph, a pooling layer needs to be
    added which aggregates and combines all states and labels contained in the output
    graph based on a specified pooling operation (Zhou et al., [2020](#bib.bib216)).'
  prefs: []
  type: TYPE_NORMAL
- en: There exist several types of GNNs (Wu et al., [2020](#bib.bib190)), among the
    most famous ones are Gated Graph Neural Networks (GG-NN) (Li et al., [2015b](#bib.bib111)),
    Graph Convolution Networks (GCN) (Kipf and Welling, [2016a](#bib.bib98)), GraphSAGE
    (Hamilton et al., [2017b](#bib.bib68)), Graph Attention Networks (GAT) (Veličković
    et al., [2017](#bib.bib177)) and Graph Auto-Encoders (GAE) (Kipf and Welling,
    [2016b](#bib.bib99)). Architectural differences involve the message propagation
    process, sampling method, pooling operation as well as the composition of the
    layers. For instance, the GraphSAGE model uses a max-pooling strategy while GCNs
    use mean-pooling instead. GATs include masked self-attention in the pooling process
    and GG-NNs capture spatial and temporal changes throughout time by using gated
    recurrent units as update module. In general, if the relations between objects
    are essential to make predictions on the data, GNNs can be used and are often
    preferable over common DNN architectures (LeCun et al., [1995](#bib.bib105); Hochreiter
    and Schmidhuber, [1997](#bib.bib76)). In contrast to DNNs which read in the data
    object by object or in an ordered sequence, GNNs naturally capture the relations
    within a graph and are also able to predict the relations between data points.
    This cannot be easily done with other DNN models. Thus, GNNs benefit the processing
    of and prediction on graph data. Sometimes it can be advantageous to combine GNNs
    with other DNN models, for instance, when handling temporal graphs where nodes
    and edges are updated sequentially (Kumar et al., [2019](#bib.bib102); Rossi et al.,
    [2020](#bib.bib147); Zhang et al., [2021b](#bib.bib208)). Here, a GNN is combined
    with a Recurrent Neural Network. However, if there are no significant connections
    between the individual data points, there usually is no need to choose a GNN over
    another DNN model to perform DL.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training GNNs on large graphs is a challenging task which requires high communication,
    large memory capacity and high bandwidth (Md et al., [2021](#bib.bib131)). In
    contrast to general distributed DNN training, all data points within a graph are
    connected and not independent of each other. Due to this dependency, one can not
    simply split the data to process the batches in parallel. Furthermore, memory-intensive
    edge-centric operations in addition to arithmetic-intensive vertex-centric operations
    need to be regarded when optimizing GNNs (Kiningham et al., [2020b](#bib.bib97)).
    Thus, large-scale graph processing methods or efficient DNN training operations
    can not directly be used for GNN training. More specialized techniques adjusted
    to GNN characteristics are needed to overcome the described challenges and to
    speed up training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d7a58ba629aca972d356afbe7dac7f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3. General set up of a GNN system
  prefs: []
  type: TYPE_NORMAL
- en: 'A general set up of GNN systems is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey").
    Similar to the first step in distributed graph processing, the graph can initially
    be partitioned and distributed across the workers (①: Section [3.2.1](#S3.SS2.SSS1
    "3.2.1\. Partitioning ‣ 3.2\. Categorization of Methods for Distributed Graph
    Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")). Other types of parallelism (Section [3.2.5](#S3.SS2.SSS5
    "3.2.5\. Parallelism ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey")) are also possible, however data parallelism is the most
    common choice. After that, questions of how to store the subgraphs and whether
    to cache any data need to be addressed (②: Section [3.2.4](#S3.SS2.SSS4 "3.2.4\.
    Inter-Process Communication ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")). This can either be done locally on the machines
    or globally on a dedicated graph store. Depending on the training mode, a sampling
    step is performed (③: Section [3.2.2](#S3.SS2.SSS2 "3.2.2\. Sampling ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey")). Here, only
    a selection of vertices instead of the whole partition is used to train the model.
    The main training begins with a random initial representation of the vertices.
    Then, messages are pulled to or pushed from adjacent vertices (④: Section [3.2.6](#S3.SS2.SSS6
    "3.2.6\. Message Propagation ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")), DNN operations are applied and the updated vertex
    states are shared with the other vertices before the next iteration starts. The
    whole process can be executed in a synchronous or asynchronous fashion (⑤: Section
    [3.2.7](#S3.SS2.SSS7 "3.2.7\. Synchronization Mode ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")) and various scheduling techniques
    may be applied (Section [3.2.8](#S3.SS2.SSS8 "3.2.8\. Scheduling ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey")). Instead of
    a decentral all-reduce operation to share the parameters, a centralized parameter
    server might be used (⑥: Section [3.2.9](#S3.SS2.SSS9 "3.2.9\. Coordination ‣
    3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey")). After having finished all iterations, predictions can be made based
    on the final set of parameters. Most systems additionally provide a programming
    abstraction adapted to the individual optimizations (Section [3.2.3](#S3.SS2.SSS3
    "3.2.3\. Programming Abstractions ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")). In the following, we will present and categorize
    systems for GNNs based on the described steps for setting up such a system.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Partitioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Graph processing systems rely on partitioning the input graph and distributing
    it across machines before starting the main computation. GNN systems adopt the
    idea of partitioning because the input graphs are unlikely to fit in a single
    machine’s memory. Some systems use traditional edge-cut or vertex-cut methods
    (Zheng et al., [2020](#bib.bib214); Md et al., [2021](#bib.bib131)) whereas others
    combine those with features like a cost model (Jia et al., [2020a](#bib.bib88)),
    feasibility score (Lin et al., [2020](#bib.bib112)) or dataflow partitioning (Kiningham
    et al., [2020a](#bib.bib96)). Table [3](#S3.T3 "Table 3 ‣ 3.2.1\. Partitioning
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") summarizes the different partitioning methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3. Categorization of partitioning strategies
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Cut type | Static vs. Dynamic | Offline vs. Online |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Year | System |  | Edge | Vertex | Hybrid | Static | Dynamic | Offline |
    Online | Balancing Objective |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  |  | ✓ | ✓ |  | ✓ |  |
    2D partitioning with equally-sized disjoint vertex chunks |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | GReTA | (Kiningham et al., [2020a](#bib.bib96)) |  |  | ✓ | ✓ |  |
    ✓ |  | 2D dataflow partitioning |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) | ✓ |  |  |  | ✓ | ✓ |  |
    runtime of a partition |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) | ✓ |  |  | ✓ |  | ✓ |  |
    neighborhood size |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  |  |  | ✓ |  |
    ✓ | feasibility score, node degree, computation expense |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) | ✓ |  |  | ✓ |  | ✓
    |  | minimum edge-cut |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  |  | ✓ |  | ✓ |  |
    random hash |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) | ✓ |  |  | ✓ |  |
    ✓ |  | 2D workload partitioning |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DistGNN | (Md et al., [2021](#bib.bib131)) |  | ✓ |  | ✓ |  |  | ✓
    | edges per partition |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DeepGalois | (Hoang et al., [2021](#bib.bib74)) | ✓ |  |  |  |  |  |
    ✓ | user-defined policy |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) |  | ✓ |  | ✓ |  | ✓
    |  | 2D partitioning with equally-sized disjoint vertex chunks |'
  prefs: []
  type: TYPE_TB
- en: In DistDGL (Zheng et al., [2020](#bib.bib214)), the input graph is partitioned
    using the METIS (Karypis, [1997](#bib.bib92); Karypis and Kumar, [1998a](#bib.bib93))
    edge-cut algorithm. In addition, optimizations like multi-constraint partitioning
    (Karypis and Kumar, [1998b](#bib.bib94)) and a refinement phase are used to improve
    load balancing. DistGNN (Md et al., [2021](#bib.bib131)) generates partitions
    with a minimum vertex-cut algorithm and the tool Libra (Xie et al., [2014](#bib.bib192)).
    Here, edges belong to one specific partition while vertices can correspond to
    multiple partitions, hence they need to be replicated. The so-called replication
    factor measures the number of replicas. A lower replication factor induces less
    communication across partitions. Moreover, Libra generates balanced partitions
    by equally distributing edges among subgraphs.
  prefs: []
  type: TYPE_NORMAL
- en: NeuGraph (Ma et al., [2019](#bib.bib118)) first preprocesses the input graph
    with the METIS edge-cut algorithm and then applies a grid-based partitioning scheme
    which combines edge- and vertex-cut. It is similar to the method used in GridGraph
    (Zhu et al., [2015](#bib.bib217)) and assigns each vertex and its corresponding
    features to one of $P$ vertex chunks. Then, the adjacency matrix is tiled into
    $P\times P$ chunks containing the corresponding edges. This partitioning strategy
    benefits the edge-wise processing, because only the source and destination vertex
    data need to be loaded. Unlike NeuGraph, GReTA (Kiningham et al., [2020a](#bib.bib96))
    does not partition the graph itself, but the dataflow into blocks. The dataflow,
    also called nodeflow, is a graph structure representing the propagation of feature
    vectors throughout the forward pass of the GNN model. The vertices each represent
    a mathematical unit of computation while the edges represent the inputs and outputs
    of the units. At first, GReTA partitions the vertices of the dataflow graph into
    $n$- and $m$-sized chunks. Then, blocks of size $n\times m$ are formed out of
    the adjacency matrix containing the relevant edges. Hence, only a part of the
    grid representation instead of the whole graph needs to be loaded for performing
    a computation step. Additionally, an entire column can be processed in the aggregation
    phase. Inspired by NeuGraph and GReTA, ZIPPER (Zhang et al., [2021a](#bib.bib211))
    also uses a grid-based partitioning technique where the graph adjacency matrix
    is tiled into multiple rectangular blocks. It is distinguished between source
    and destination vertices throughout the partitioning process to ensure that each
    block is only associated with one source and one destination partition. Thus,
    each edge is uniquely identified. As described by the former systems, this partitioning
    strategy is applied to reduce the memory footprint and communication.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the graph is partitioned at the beginning of the computation and these
    partitions are used until the end of the overall process. ROC (Jia et al., [2020a](#bib.bib88)),
    however, repartitions the graph before each iteration using an online regression
    model. A cost model predicts the execution time of various operations on a given
    graph partition based on parameters like the number of vertices and edges and
    the number of memory accesses. The cost model is updated and minimized at the
    end of each iteration with the actual runtimes needed for the subgraph. Then,
    the graph is repartitioned based on the updated costs.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. provide AGL (Ant Graph ML system) (Zhang et al., [2020c](#bib.bib205))
    to employ large GNNs for industrial use cases. The GraphFlat module is responsible
    for dividing the input graph into $k$-hop neighborhoods. It uses a pipeline inspired
    by message passing to produce the desired neighborhoods. In a MapReduce-style,
    self-information about a vertex is generated, propagated along outgoing edges
    and aggregated. This is done until $k$ iterations are reached. Now, the nodes
    contain information determining the partitions regarding their $k$-hop neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'PaGraph (Lin et al., [2020](#bib.bib112)) designs a GNN-aware partitioning
    algorithm which distributes the vertices among partitions depending on a score.
    In each iteration of the algorithm, a vertex is scanned and a vector is calculated
    where the $i$-th position determines the feasibility for assigning the vertex
    to partition $i$. The score incorporates features of the vertices so far assigned
    to partition $i$, the in-neighbor set of the vertex and the expected number of
    vertices in the partition. The current vertex is allocated to the partition with
    the highest feasibility score and it is proceeded with the subsequent vertex.
    Balanced partitions and an evenly distributed computing effort are ensured. GNNAdvisor
    (Wang et al., [2021a](#bib.bib184)) relies on neighbor partitioning where only
    the adjacent vertices of a target vertex belong to the given partition. A reason
    to choose neighbor partitioning over edge- or vertex-cut partitioning is the mitigation
    of highly varying partition sizes. Further, the probability for tiny partitions
    is lower which reduces the managing costs. In contrast to the above presented
    systems, DeepGalois (Hoang et al., [2021](#bib.bib74)) allows for customized partitioning
    by incorporating the Customizable Streaming Partitioner (CuSP) (Hoang et al.,
    [2019](#bib.bib75)) framework. A simple API lets the user determine the specific
    partitioning policy, supported are edge-cut, vertex-cut and hybrid-cut. Hence,
    the user can tailor the partitioning strategy to the specific GNN architecture.
    Unlike systems exploiting a compute-intensive customized partitioning strategy
    (Kiningham et al., [2020a](#bib.bib96); Jia et al., [2020a](#bib.bib88); Lin et al.,
    [2020](#bib.bib112)), P³ (Gandhi and Iyer, [2021](#bib.bib48)) relies on a simple
    random hash partitioner. This ensures simple, fast and efficient partitioning
    with only minimal overhead. Here, the initial graph partitioning helps to balance
    the workload, but the main optimizations to scale to large graphs are done in
    the consecutive steps of the system, namely Sampling (Section [3.2.2](#S3.SS2.SSS2
    "3.2.2\. Sampling ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey")), Inter-Process Communication (Section [3.2.4](#S3.SS2.SSS4
    "3.2.4\. Inter-Process Communication ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")), choice of Parallelism (Section [3.2.5](#S3.SS2.SSS5
    "3.2.5\. Parallelism ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey")), Synchronization Mode (Section [3.2.7](#S3.SS2.SSS7 "3.2.7\.
    Synchronization Mode ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey")), Scheduling (Section [3.2.8](#S3.SS2.SSS8 "3.2.8\. Scheduling
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey")) and Coordination (Section [3.2.9](#S3.SS2.SSS9 "3.2.9\. Coordination
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The underlying idea of sampling originates in graph processing. However, the
    idea of sampling slightly differs in connection with GNN training. Different to
    DNN training, the samples within a graph are not independent. When performing
    mini-batch training, one can not randomly pick out a number of vertices without
    regarding the relation to other ones. Thus, training samples of a mini-batch need
    to include the k-hop neighborhood of a vertex. However, without sampling, these
    neighborhoods are likely to ”explode” as the neighborhood size quickly grows with
    each hop. For that reason, numerous strategies such as vertex-wise (Hamilton et al.,
    [2017a](#bib.bib67)), layer-wise (Zou et al., [2019](#bib.bib219)) or subgraph-based
    (Chiang et al., [2019](#bib.bib25)) sampling are introduced. The underlying idea
    of all these methods is to restrict the number of k-hop neighbors to be explored
    to prevent the described neighborhood explosion issue (Zheng et al., [2020](#bib.bib214)).
    While sampling is an established technique working well for many tasks, the choice
    of the specific strategy depends on the desired downstream ML task, the graph
    structure and the objective of the sampling method (Rozemberczki et al., [2020](#bib.bib148)).
  prefs: []
  type: TYPE_NORMAL
- en: An early algorithm for efficient sampling in GNN training is GraphSAGE (SAmple
    and aggreGatE) (Hamilton et al., [2017a](#bib.bib67)). The model trains mini-batches
    and restricts the neighborhood size per layer. The number of vertices to be sampled
    is fixed and the vertices are randomly chosen. GraphSAGE is able to work on larger
    graphs compared to the general GCN architecture. However, by picking the vertices
    at random, neighborhood information might be lost leading to a decrease in accuracy.
    Therefore, PinSage (Ying et al., [2018](#bib.bib201)) adopts importance-based
    neighborhood sampling. The new technique incorporates random walks to compute
    a score for each vertex and select a fixed number of neighbors accordingly. Hence,
    memory usage can be controlled and adjusted if needed, while yielding higher accuracy
    than random sampling.
  prefs: []
  type: TYPE_NORMAL
- en: FastGCN (Chen et al., [2018](#bib.bib20)) further explores the idea of sampling
    based on a calculated score. The authors introduce an importance based layer-wise
    sampling mechanism where an importance score and a fixed neighborhood size determine
    which vertices to select. The score mainly depends on the degree of each vertex
    and is calculated for each layer to restrict the corresponding number of vertices.
    Thus, the neighborhood explosion problem can be avoided and large graphs can be
    handled. However, as the vertex-wise importance is calculated independently per
    layer, the selected neighborhoods for two subsequent layers may differ which might
    lead to slow convergence. This issue is faced by LADIES (Zou et al., [2019](#bib.bib219))
    which exploits importance sampling in a layer-dependent way. Depending on the
    sampled vertices in the previous layer, the neighboring vertices in the current
    layer are selected and a bipartite graph between the two layers is constructed.
    After that, the sampling probability is calculated and a fixed number of vertices
    is sampled. This procedure is repeated for each layer to sample the needed vertices.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterGCN (Chiang et al., [2019](#bib.bib25)) allows for subgraph-based sampling.
    Contrary to general mini-batch GCN training where the vertices are randomly chosen,
    a graph clustering algorithm is used to form the mini-batches. The clustering
    strategy aims at minimizing the number of links between vertices in the batch
    or between multiple batches. As a consequence, ClusterGCN is faster and uses less
    memory compared to previous methods. Zeng et al. propose GraphSAINT (Zeng et al.,
    [2019](#bib.bib203)) which also supports subgraph-based sampling. In contrast
    to former sampling-based systems first building a GCN and then sampling the input
    graph, GraphSAINT starts with sampling subgraphs and builds a GCN for each subgraph
    after that. By building a complete GCN for each sample, extensions like skip connections
    as proposed by JK-Net (Xu et al., [2018b](#bib.bib194)) are applicable without
    needing to adapt the sampling process. JK-Net requires the samples of the current
    layer to be a subset of the previous layers’ samples which is naturally met by
    using GraphSAINT. Further, GraphSAINT ensures a minimized neighborhood size while
    maintaining a high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'AliGraph (Yang, [2019](#bib.bib198)) incorporates sampling by providing three
    steps: traverse, neighborhood and negative. Traverse draws a set of vertices and
    edges from the subgraphs and neighborhood is responsible for building a vertex
    context which may be one- or multi-hop neighbor vertices. The last step, negative,
    speeds up convergence of the training by setting up negative samples. Here, negative
    sampling refers to including exemplary vertices in the training process that are
    not part of the given sample. For instance, given a Graph $G$ where vertex $A$
    is connected to vertex $B$, but there is no edge between vertex $A$ and vertex
    $C$. Negative sampling would mean to not only incorporate edge $(A,B)$ in the
    training process as positive example, but also edge $(A,C)$ with an explicit negative
    tag indicating there is no connection between those vertices.'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by ClusterGCN (Chiang et al., [2019](#bib.bib25)), GraphTheta (Li et al.,
    [2021a](#bib.bib109)) uses a clustering algorithm to form the graph samples. Within
    one subgraph, the algorithm detects and builds communities maximizing intra-community
    and minimizing inter-community edges. Due to forming communities before the main
    sampling step, the sampled vertices tend to overlap not as much as with (random)
    neighbor sampling leading to fewer repeated vertex accesses. Despite the advantage
    of less redundant calculations, graphs with weak community structures are not
    supported and the batch size can be imbalanced due to varying community sizes.
  prefs: []
  type: TYPE_NORMAL
- en: The AGL system (Zhang et al., [2020c](#bib.bib205)) provides a variety of sampling
    methods that can be chosen from, for instance, uniform sampling and weighted sampling.
    This ensures that the user can select the best strategy for each application.
    Wang et al. also pursue the idea of providing several sampling techniques in DGL
    (Wang et al., [2019](#bib.bib180)). A set of methods is provided, like the well-known
    neighbor sampling (Hamilton et al., [2017a](#bib.bib67)) and cluster sampling
    (Chiang et al., [2019](#bib.bib25)). The DistDGL system (Zheng et al., [2020](#bib.bib214))
    is based on DGL and supports several sampling techniques, but implements the sampling
    step in a distributed way. The sampling request originates from the trainer process
    and is sent to the machine responsible for the target set of vertices. After having
    received the request, the sampling worker calls the sampling operators of DGL
    and performs sampling on the local partition. It sends the results back to the
    trainer process which generates a mini-batch by assembling all acquired results.
    Like AGL, DGL and DistDGL, P³ (Gandhi and Iyer, [2021](#bib.bib48)) does not provide
    a particular sampling strategy. However, it adopts the sampling method given by
    the GNN architecture. If no specific method is included, P³ proceeds without a
    sampling phase. This ensures that a variety of GNN architectures are supported
    by the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although sampling-based methods may decrease training time of GNNs, there remain
    issues like lack of consistency (Hu et al., [2020a](#bib.bib78)) and limited applicability
    to GNN architectures with many-hop or global context layers. For that reason,
    NeuGraph (Ma et al., [2019](#bib.bib118)) and ROC (Jia et al., [2020a](#bib.bib88))
    omit the sampling phase and rely on full-batch training. A short overview of the
    different sampling strategies can be found in Table [4](#S3.T4 "Table 4 ‣ 3.2.2\.
    Sampling ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network
    Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4. Categorization of sampling strategies
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Method | Coordination |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Year | System |  | Community | User Defined | Full Batch | Centralized |
    Distributed | Main Sampling Concepts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) |  | ✓ |  | ✓ |  | choose
    method based on application |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  |  | ✓ | ✓ |  | no
    sampling |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Aligraph | (Yang, [2019](#bib.bib198)) |  | ✓ |  | ✓ |  | three steps:
    traverse, neighborhood, negative |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) |  |  | ✓ | ✓ |  | no sampling
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) |  | ✓ |  | ✓ |  | choose
    method based on application |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) |  | ✓ |  |  | ✓ | sampling
    worker responsible for local partition |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) | ✓ |  |  | ✓ |  |
    sample from clusters, minimize overlapping vertices |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) |  | ✓ |  | ✓ |  | adapt
    to given GNN architecture and application |'
  prefs: []
  type: TYPE_TB
- en: 3.2.3\. Programming Abstractions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To facilitate the implementation of the desired GNN architecture and to support
    custom optimizations, the proposed systems come with programming abstractions
    including user-defined functions. There are programming models based on message
    passing (Wang et al., [2019](#bib.bib180); Fey and Lenssen, [2019](#bib.bib43);
    Hu et al., [2020b](#bib.bib79)) while other abstractions are using a dataflow
    paradigm (Ma et al., [2019](#bib.bib118); Kiningham et al., [2020a](#bib.bib96);
    Li et al., [2021a](#bib.bib109)).
  prefs: []
  type: TYPE_NORMAL
- en: A PyTorch (Paszke et al., [2019](#bib.bib139)) extension tailored to GNN training
    is PyTorch Geometric (PyG) (Fey and Lenssen, [2019](#bib.bib43)). The library
    comes with a message passing base class where the user only needs to define the
    construction of messages, the update function as well as the aggregation scheme.
    Message propagation is handled automatically. Numerous GNN architectures can be
    implemented, for instance, GCN (Kipf and Welling, [2016a](#bib.bib98)), SGC (Wu
    et al., [2019](#bib.bib188)), GraphSAGE (Hamilton et al., [2017a](#bib.bib67)),
    GAT (Veličković et al., [2017](#bib.bib177)), and GIN (Xu et al., [2018a](#bib.bib193)).
    The Deep Graph Library (DGL) (Wang et al., [2019](#bib.bib180)) also lets the
    user define the desired GNN model as a set of message passing primitives covering
    forward and backward pass. The central abstraction is the graph data structure
    DGLGraph. (Pre-)defined functions such as neighbor sampling directly operate on
    a DGLGraph and return a subgraph object. Therefore, manually slicing tensors and
    manipulating graph data is made obsolete in contrast to frameworks such as PyG.
    Another message passing-based programming interface is introduced by FeatGraph
    (Hu et al., [2020b](#bib.bib79)). In addition to customizing the GNN model, the
    user is able to determine the parallelization strategy for the vertex- and edge-wise
    feature dimension computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For applying the optimization techniques proposed by P³ (Gandhi and Iyer, [2021](#bib.bib48)),
    the system provides a message passing API that developers can use to implement
    GNN models that automatically include the optimizations. The P-TAGS API consists
    of six functions: partition, scatter, gather, transform, sync and apply. All functions
    are user-defined and target a different step in the GNN training process. An independent
    partition function is provided where the developer can implement an individual
    partitioning algorithm to reduce communication. While scatter is a message passing
    function defined on each edge, gather assembles the messages vertex-wise with
    a commutative and associative function. The transform function applies the given
    NN operations on each vertex to compute the partial activations. Then, a neighborhood
    representation is computed with NN operations such as convolution. Those representations
    are collected over the network with sync and a composite apply function is used
    to update the vertices’ states.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Oriented on the GAS model (Gonzalez et al., [2012](#bib.bib59)), NeuGraph (Ma
    et al., [2019](#bib.bib118)) introduces the SAGA-NN (scatter-applyedge-gather-applyvertex
    with Neural Networks) programming scheme. The vertex-centric model expresses one
    GNN forward pass as four steps: scatter, applyedge, gather, and applyvertex. scatter
    and gather are predefined and responsible for data propagation while the other
    two, applyedge and applyvertex, are defined by the user and are expressed as dataflow
    with tensor-based operations. In the first phase, namely scatter, information
    about the vertices is passed onto adjacent edges where the values are aggregated
    and subsequently combined to form a single edge value in applyedge. The gather
    step propagates the updated values to the vertices where they are assembled. The
    vertex state is updated in the last step, applyvertex. Within each step, the computation
    is parallelized. The abstraction combines graph processing and NN training by
    uniting the dataflow model with a vertex-centric view. In general, SAGA-NN follows
    the common iterative GNN computation model which makes it applicable to various
    architectures (Kipf and Welling, [2016a](#bib.bib98); Sukhbaatar et al., [2016](#bib.bib168);
    Li et al., [2015b](#bib.bib111)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow for efficient execution of GNN training on an accelerator, GReTA (Kiningham
    et al., [2020a](#bib.bib96)) introduces four stateless user-defined functions:
    gather, reduce, transform and activate. gather loads and aggregates edge and vertex
    data. The reduce operation merges the data into one single value. The current
    and previous reduced vertex state are combined in transform. Finally, activate
    updates the vertices with new values. A GNN layer is expressed as one or multiple
    GReTA programs making it expressive enough for a diverse set of GNN models (Kipf
    and Welling, [2016a](#bib.bib98); Bresson and Laurent, [2017](#bib.bib12); Hamilton
    et al., [2017a](#bib.bib67); Xu et al., [2018a](#bib.bib193)). NN-TGAR proposed
    by GraphTheta (Li et al., [2021a](#bib.bib109)) provides user-friendly programming
    and enables training on a cluster of machines. Moreover, it unites graph processing
    and graph learning frameworks. In contrast to the GAS model and GReTA, GraphTheta
    lets the user implement the GNN model in a vertex- and edge-centric view. While
    some functions are applied to each vertex, other functions are applied to each
    edge. The first step of the abstraction, the NN-T operation, transforms values
    vertex-wise and generates corresponding messages. After that, NN-G (gather) is
    applied to each edge to update the edge values and transmit the message to the
    destination vertex. The sum operation in turn operates on each vertex and combines
    the received messages with methods like averaging or concatenation (NN-A). Then,
    the result is applied to the vertices and the parameters are updated with reduce.
    NN-T, NN-G and NN-A are implemented as neural networks, making it easy to perform
    the forward pass and subsequent gradient computation. In addition, an encoding
    layer is decomposed into subsequent independent stages allowing for general applicability.'
  prefs: []
  type: TYPE_NORMAL
- en: FlexGraph (Wang et al., [2021c](#bib.bib179)) introduces the programming model
    NAU (neighborselection, aggregation and update). In contrast to models based on
    the GAS abstraction (Gonzalez et al., [2012](#bib.bib59); Ma et al., [2019](#bib.bib118)),
    NAU comprises neighborselection which builds Hierarchical Dependency Graphs (HDGs)
    including chosen neighbors to capture the dependencies among vertices. After that,
    the neighborhood features are aggregated and a neighborhood representation is
    computed in the aggregation step. In the update phase, a new representation is
    calculated consisting of old features and the new neighborhood representation.
    Moreover, one single message comprises of multiple features and messages are assembled
    to reduce traffic. As opposed to programming models like SAGA-NN (Ma et al., [2019](#bib.bib118))
    and its variants, NAU is not limited to 1-hop neighbors during computation. Additional
    to flat aggregation operations, hierarchical aggregation can be used to support
    various GNN architectures. Therefore, NAU also supports GNN models with indirect
    neighbors and hierarchical aggregations, for instance, PinSage (Ying et al., [2018](#bib.bib201)),
    MAGNN (Fu et al., [2020](#bib.bib47)), P-GNN (You et al., [2019](#bib.bib202)),
    JK-Net (Xu et al., [2018b](#bib.bib194)), while SAGA-NN merely supports architectures
    where direct neighbors and flat aggregations are regarded (Kipf and Welling, [2016a](#bib.bib98);
    Xu et al., [2018a](#bib.bib193); Marcheggiani and Titov, [2017](#bib.bib122)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5. Categorization of programming abstractions
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | System |  | Expressiveness | Optimizations | Algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | message passing abstraction
    | operates on DGLGraph | GCN, SGC, GraphSAGE, GAT, GIN |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | PyG | (Fey and Lenssen, [2019](#bib.bib43)) | message passing abstraction
    | optimized sparse softmax kernels | GCN, SGC, GraphSAGE, GAT, GIN, ARMA, APPNP
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | GReTA | (Kiningham et al., [2020a](#bib.bib96)) | dataflow abstraction
    | one or multiple GReTA programs per GNN layer | GCN, G-GCN, GraphSAGE, GIN |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) | dataflow model with
    vertex-centric view, direct neighbors and flat aggregations | tensor-based operations
    | CommNet, GCN, GG-NN |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | FeatGraph | (Hu et al., [2020b](#bib.bib79)) | message passing abstraction
    | custom parallelization strategy | GCN, GraphSAGE, GAT |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) | vertex- and edge-centric
    abstraction | independent steps are implemented as neural networks | GCN, FastGCN,
    VR-GCN |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | FlexGraph | (Wang et al., [2021c](#bib.bib179)) | indirect neighbors
    and hierarchical aggregations | hierarchical dependency graphs | PinSage, MAGNN,
    P-GNN, JK-Net |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | vertex- and edge-centric
    abstraction | user-defined partition function | S-GCN, GCN, GraphSAGE, GAT |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Seastar | (Wu et al., [2021](#bib.bib189)) | vertex-centric abstraction
    | improved usability | GCN, GAT, APPNP, R-GCN |'
  prefs: []
  type: TYPE_TB
- en: 'Inspired by Pregel (Malewicz et al., [2010](#bib.bib120)), the underlying programming
    model of Seastar (Wu et al., [2021](#bib.bib189)) is realized in a vertex-centric
    fashion. From the viewpoint of a vertex, the user defines functions to implement
    the GNN architecture. Seastar then executes the given operations on each vertex.
    This improves usability compared to message passing systems (Fey and Lenssen,
    [2019](#bib.bib43); Wang et al., [2019](#bib.bib180); Ma et al., [2019](#bib.bib118))
    and dataflow programming systems (Ma et al., [2019](#bib.bib118); Yang, [2019](#bib.bib198);
    Alibaba, [2020](#bib.bib5)). With the proposed abstraction, it is possible to
    implement GNN models more easily and the implementation can be adjusted faster.
    More details on the various programming abstractions are shown in Table [5](#S3.T5
    "Table 5 ‣ 3.2.3\. Programming Abstractions ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4\. Inter-Process Communication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before starting the GNN training process, how to store the partitions, samples
    and corresponding features and whether to cache any vertex data needs to be determined.
    DGL (Wang et al., [2019](#bib.bib180)) is built on top of DNN frameworks like
    TensorFlow (Abadi et al., [2016](#bib.bib3)) or PyTorch (Paszke et al., [2019](#bib.bib139))
    and leaves the memory management those frameworks instead of developing its own
    storing and caching strategy. Even though those frameworks are able to store datasets
    used for DNN training efficiently, more specialized techniques are beneficial
    for GNN training. Here, samples might contain overlapping neighborhoods, some
    vertices are repeatedly accessed, and it is crucial to preserve the connections
    within the graph. Therefore, the following systems employ more sophisticated methods
    which are summarized in Table [6](#S3.T6 "Table 6 ‣ 3.2.4\. Inter-Process Communication
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6. Categorization of inter-process communication methods
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Storage | Caching |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Year | System | Centralized | Distributed | Data | Objective | Optimizations
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ | ✓ | - | - | leaves
    memory management and caching to base framework |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Aligraph | (Yang, [2019](#bib.bib198)) |  | ✓ | neighbors of selected
    vertices | based on importance value | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) |  | ✓ | intermediate tensors
    on GPU | minimize cost model based on the graph, GNN model and GPU device | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  | frequently accessed
    feature vectors | minimize computation and communication | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) |  | ✓ | - | - | KVStore,
    co-location of data and computation |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  | ✓ | - | - | task-oriented
    layout |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  | graph and/or features
    | user-defined | store partial activations |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) |  | ✓ | - | - |
    vertex reordering |'
  prefs: []
  type: TYPE_TB
- en: AliGraph (Yang, [2019](#bib.bib198)) proposes to cache neighbors of important
    vertices. An importance value for each vertex with respect to the number of k-hop
    in- and out-neighbors is calculated. The out-neighbors of vertices with an importance
    value exceeding a user-defined threshold are locally cached. In this way, frequently
    needed data becomes easily accessible and communication cost is reduced. ROC (Jia
    et al., [2020a](#bib.bib88)) optimizes runtime performance by caching intermediate
    tensors on GPUs while maintaining the remaining data in the host memory. By caching
    these tensors, the data transfers between CPU and GPU are decreased. A cost model
    is built with respect to a given input graph, a GNN model and a GPU device. This
    cost model is minimized with a dynamic programming algorithm to find the globally
    optimal caching strategy. As the data copy operation from CPU to GPU is a major
    bottleneck in distributed GNN training, PaGraph (Lin et al., [2020](#bib.bib112))
    uses a computation-aware caching mechanism to minimize data copy. Vertex features
    as well as structural information about the graph are stored in a Graph Store
    Server on the CPU. This shared memory is globally accessible. Additionally, a
    cache on each GPU keeps frequently accessed feature vectors. Before deciding which
    vertices to cache, Lin et al. analyze the characteristics of the training process.
    GNNs including a sampling technique randomly shuffle the samples in each epoch
    making it impossible to predict at runtime which vertices belong to which mini-batch.
    As a consequence, it is not possible to forecast which vertices are accessed at
    the next training iteration. However, the out-degree of a vertex indicates how
    likely it is to be sampled in the whole epoch. With a higher out-degree, it is
    an in-vertex for a higher number of neighbors. Thus, those vertices are chosen
    more often in other samples, yield higher computation costs and should be easily
    accessible. The caching policy of PaGraph is oriented on those findings. It pre-sorts
    the vertices by out-degree and fills up the cache following that order. This leads
    to a high cache hit ratio and reduced data transfer.
  prefs: []
  type: TYPE_NORMAL
- en: A distributed file system is used by AGL (Zhang et al., [2020c](#bib.bib205))
    to store the neighborhoods. During computation, one or a batch of them is loaded
    instead of the whole graph. This highly decreases communication between graph
    stores and workers. AGL can be run on a single machine or a CPU cluster. DistDGL
    (Zheng et al., [2020](#bib.bib214)) also works on multiple CPUs. Thus, the graph
    structure, corresponding features and embeddings are stored on multiple machines.
    The so-called distributed key-value store (KVStore) is globally accessible for
    all trainer processes. DistDGL co-locates data and computation, meaning the distribution
    of vertices and edges among the KVStore servers on the machines resemble the obtained
    graph partitions in the partitioning step. Consequently, trainer processes can
    directly access the data and communication is reduced. Euler (Alibaba, [2020](#bib.bib5))
    also exploits a distributed storage architecture. The graph engine layer is responsible
    for loading and dividing the graph into subgraphs and distributing them among
    the machines.
  prefs: []
  type: TYPE_NORMAL
- en: A more specialized memory optimization is introduced by GNNAdvisor (Wang et al.,
    [2021a](#bib.bib184)). The underlying idea is to couple vertices and the computing
    units where they are processed more tightly. Therefore, graph reordering is performed
    as by RabbitOrder (Arai et al., [2016](#bib.bib6)) and the GO-PQ algorithm (Wei
    et al., [2016](#bib.bib186)). Neighbor groups being close to each other are assigned
    consecutive vertex IDs increasing the possibility for them to be scheduled closely
    on the same machine. As two adjacent neighborhoods often share common neighbors,
    the L1 cache is more efficiently used and data locality is exploited. ZIPPER (Zhang
    et al., [2021a](#bib.bib211)) also includes a vertex reordering technique. Here,
    a heuristic degree sorting strategy is used to group the out-edges of the source
    vertices. As a consequence, vertex data is more efficiently reused and redundancies
    are minimized.
  prefs: []
  type: TYPE_NORMAL
- en: GraphTheta (Li et al., [2021a](#bib.bib109)) also stores subgraphs in a distributed
    fashion. To achieve low-latency access and reduced memory overhead, the proposed
    parallel tensor storage utilizes a task-oriented layout. The memory needed for
    each task, e.g., forward pass, backward pass or aggregation phase, is grouped
    process-wise. The task-specific memory includes raw data and tensors which are
    further sliced into frames for more efficient access. For each frame, memory is
    allocated and deallocated immediately after usage throughout the whole computation
    process to reduce the memory utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Gandhi et al. (Gandhi and Iyer, [2021](#bib.bib48)) extend the KVStore introduced
    in DistDGL (Zheng et al., [2020](#bib.bib214)). In addition to vertex and edge
    data, P³ also stores partial activations in the KVStore. The extended KVStore
    coordinates data movement across machines. As soon as the machines are synchronized,
    the accumulated activation is moved to the device memory and shared with the trainer
    process. Additionally, P³ allows the user to define a caching strategy. A simple
    method tested by the authors is to store the input on a minimum number of machines
    and replicate partitions on so far unused machines.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5\. Parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In distributed DNN training, the appropriate type of parallelism can scale computations
    to large sets of data. The proposed methods, namely data parallelism, model parallelism
    and hybrid parallelism, have proven to work well. For that reason, researchers
    apply methods taken from DNN training to GNN training.
  prefs: []
  type: TYPE_NORMAL
- en: The most common method is data parallelism. Analog to parallelism in DNN training,
    the graph, is split into subgraphs. The model is replicated on the machines while
    each machine handles its own subgraph. Systems like NeuGraph (Ma et al., [2019](#bib.bib118)),
    PaGraph (Lin et al., [2020](#bib.bib112)) and DistGNN (Md et al., [2021](#bib.bib131))
    rely on data parallelism. Two different implementations of data parallelism, namely
    edge and vertex parallelism, are introduced by DGL (Wang et al., [2019](#bib.bib180)).
    Two types of matrix multiplications are distinguished to determine whether to
    process the data edge- or vertex-parallel. Vertex-parallel computation is used
    for generalized sparse-dense matrix multiplication. In this case, the entire adjacency
    list of a vertex is managed by one thread. The edge-parallel strategy is used
    for generalized sampled dense-dense matrix multiplication where one edge is managed
    by one thread. Here, the workload is balanced implicitly while the workload extent
    of vertex-parallel processing depends on the vertex degree.
  prefs: []
  type: TYPE_NORMAL
- en: A hybrid between data and model parallelism is employed in P³ by Gandhi et al.
    (Gandhi and Iyer, [2021](#bib.bib48)) to tackle issues like ineffectiveness of
    partitioning and GPU underutilization. First, the model is partitioned and distributed
    among the machines. After having computed the partial activations for Layer 1,
    the machines apply a reduce function to aggregate those activations. Then, P³
    switches to data parallelism to finish the forward pass. The backward pass is
    very similar, until Layer 1, data parallelism is exploited and the error gradient
    is exchanged among the machines. Then, P³ switches back to model-parallel execution
    to perform the remaining steps of the backward pass locally.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7. Categorization of types of parallelism
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Type of Parallelism |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Year | System | Data | Model | Hybrid | Main Concepts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ |  |  | Edge and vertex
    parallelism |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) | ✓ |  |  | Mini-batch
    training |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  |  | Mini-batch
    training |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  |  | ✓ | Data and
    operations are split |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) |  |  | ✓ | Push-pull parallelism
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DistGNN | (Md et al., [2021](#bib.bib131)) | ✓ |  |  | Mini-batch
    training |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) |  |  | ✓ | Tile- and
    operator-level parallelism |'
  prefs: []
  type: TYPE_TB
- en: GraphTheta (Li et al., [2021a](#bib.bib109)) also deploys a form of hybrid parallelism
    to overcome the scalability issue when handling graphs with highly skewed vertex
    degree distribution. When processing a full iteration with a high-degree vertex,
    a worker could run out of memory. For that reason, GraphTheta not only splits
    up and distributes the input graph among workers, but also the operations forming
    forward and backward pass. This ensures an efficient training phase with natural
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data and model parallelism are exploited by ZIPPER (Zhang et al., [2021a](#bib.bib211)).
    Subgraphs are formed by applying grid-based partitioning on the adjacency matrix.
    The partitions are processed in parallel resulting in data parallelism. Further,
    model parallelism is achieved by separating and overlapping the operations forming
    forward and backward pass. Now, the different operations can be executed concurrently
    on selected partitions to speed up computation and use the memory more efficiently.
    Table [7](#S3.T7 "Table 7 ‣ 3.2.5\. Parallelism ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey") summarizes the types of parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6\. Message Propagation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the exchange of data in graph processing system, machines in GNN
    systems may synchronize by pushing or pulling the relevant values. NeuGraph (Ma
    et al., [2019](#bib.bib118)) and GReTA (Kiningham et al., [2020a](#bib.bib96))
    propagate messages by pushing them directly to adjacent vertices. Messages containing
    graph features and other required information are sent across the network. The
    pull-based approach is utilized by systems like DGL (Wang et al., [2019](#bib.bib180)),
    Dorylus (Thorpe et al., [2021](#bib.bib173)) and GNNAutoScale (Fey et al., [2021](#bib.bib44)).
    The associated features as well as the k-hop neighborhood are pulled from memory
    to construct the computation graph and perform a training step. However, the moving
    of features across the network may lead to high communication. For that reason,
    no features are transferred across the network by the P³ system (Gandhi and Iyer,
    [2021](#bib.bib48)), except for partial activations and error gradients. Moreover,
    the system proposes a push-pull parallelism which switches between pushing and
    pulling during the training phase. First, P³ pulls the desired neighborhood of
    a vertex to build a computation graph which is pushed to all the machines to start
    the training phase. After having computed the partial activations at Layer 1,
    the machines pull them from all other machines. Then, the computation of the forward
    pass is performed until the last layer and the backward pass starts. At Layer
    1, the error gradients are pushed back to all machines and the backward pass ends.
    The authors chose to switch between the push and pull method to decrease the messages
    transferred over the network.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.7\. Synchronization Mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When performing tasks in parallel, it is important to determine the execution
    mode. It needs to be decided whether to follow a synchronous, asynchronous or
    hybrid scheme. As seen in distributed graph processing and distributed NN training,
    it depends on the specific architecture which mode is best. Therefore, AliGraph
    (Yang, [2019](#bib.bib198)) does not enforce a particular synchronization method,
    but chooses the updating mode based on the provided training algorithm. If the
    implemented algorithm uses synchronous updates, the system will adopt the synchronous
    scheme, whereas the asynchronous mode will be chosen if the given training algorithm
    is based on asynchronous execution. Besides AliGraph, GraphTheta (Li et al., [2021a](#bib.bib109))
    is also not fixed to a specific mode.
  prefs: []
  type: TYPE_NORMAL
- en: In DistGNN (Md et al., [2021](#bib.bib131)), three update algorithms with varying
    communication intensity during the aggregation phase are implemented and compared.
    The update algorithms regard target vertices and their replicas that emerged during
    vertex-cut partitioning. The first algorithm does not allow any communication
    between split-vertices in local partitions and their cloned vertices. Hence, there
    is no need for synchronization. The second algorithm supports communication of
    local partitions with their replicas, the vertices send partial aggregates to
    their replicas. Only if all vertices have finished communication, the vertices
    move to the next step of the training phase. A delayed update mechanism is exploited
    in the third algorithm. It is an asynchronous execution mode where the vertices
    send partial aggregates in the current epoch and receive them in a consecutive
    one. In this way, remote communication and local computation are overlapped. Communication
    is further avoided by only regarding selected split-vertices during each epoch.
    The overall results show that the zero-communication strategy is the fastest while
    maintaining only slight fluctuations in accuracy, followed by the asynchronous
    delayed update algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Dorylus (Thorpe et al., [2021](#bib.bib173)) compares three execution mode variants,
    a synchronous version and two asynchronous ones which differ in the choice of
    the staleness threshold. Synchronization is performed at the gather operation,
    meaning if the neighbors of a vertex have not finished to scatter their updated
    values, the vertex cannot start computing the next layer. For the asynchronous
    versions, the staleness threshold $s$ determines which stale values of neighbors
    are allowed to be used by a vertex. For one experiment, the authors chose a value
    of $s=0$. In this case, stale values of neighbors might be used if the neighbor
    is in the same epoch. A staleness value of $s=1$ was chosen for another experiment
    allowing for two successive epochs. By employing asynchronous updates with $s=0$,
    the per-epoch time could be sped up by around $1.234\times$. A staleness value
    that is too high induces slow convergence. Even though the time needed for one
    epoch decreases with $s=0$ compared to synchronous execution, the number of epochs
    to obtain the same accuracy rises.
  prefs: []
  type: TYPE_NORMAL
- en: The execution with P³ (Gandhi and Iyer, [2021](#bib.bib48)) needs to be highly
    coordinated as all machines switch concurrently from data to model parallelism.
    Additionally, during the data-parallel phase, global gradient synchronization
    is performed. Therefore, P³ follows a synchronous execution mode.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.8\. Scheduling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To distribute the workload evenly across all workers, tasks and data need to
    be assigned in an intelligent way. Therefore, scheduling methods are applied which
    help to increase workload balance and minimize idle times. In the following, we
    describe important scheduling techniques. An overview and categorization can be
    found in Table [8](#S3.T8 "Table 8 ‣ 3.2.8\. Scheduling ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8. Categorization of scheduling strategies
  prefs: []
  type: TYPE_NORMAL
- en: '|  | System | Static vs. Dynamic |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Year | System | Static | Dynamic | Main Concepts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  | ✓ | Selective scheduling
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) | ✓ |  | Parallel preprocessing
    and model computation stage |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  | ✓ | Work stealing
    technique |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | FlexGraph | (Wang et al., [2021c](#bib.bib179)) | ✓ |  | Computation
    cost based |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  | Inspired by PipeDream
    (Narayanan et al., [2019](#bib.bib134)), based on dependencies within computation
    graph |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Dorylus | (Thorpe et al., [2021](#bib.bib173)) |  | ✓ | Divide tasks
    based on data and computation type |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) | ✓ |  | Co-locate operations
    of different subgraphs |'
  prefs: []
  type: TYPE_TB
- en: 'The AGL (Zhang et al., [2020c](#bib.bib205)) pipeline divides the training
    procedure into two main stages: a preprocessing stage where the data is loaded
    and a model computation stage. Instead of performing the two stages sequentially,
    AGL schedules the stages in parallel. The time needed for the preprocessing stage
    is smaller compared to the model computation stage. Thus, the training time almost
    equals the time needed for model computation after some iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: Selective scheduling in NeuGraph (Ma et al., [2019](#bib.bib118)) chooses the
    most important vertices for computing the edge values based on costs for data
    copy and transfer. Thus, only a subset of vertex data is transmitted to the GPU
    and unnecessary vertices are not regarded. Further, the system makes use of pipeline
    scheduling to find the best execution configuration. To hide the transfer latency,
    transfer of data chunks between host and device memory and computation is overlapped.
    An initial scheduling plan is iteratively refined during the training process.
    The initial random order is gradually adjusted by swapping pairs of chunks while
    monitoring computation and transfer time to ensure an optimal final schedule.
    GraphTheta (Li et al., [2021a](#bib.bib109)) adopts a work-stealing scheduling
    technique (Blumofe and Leiserson, [1999](#bib.bib11)). Tasks are assigned to all
    machines which then start computing. As soon as a machine has finished its tasks,
    it ”steals” tasks that are queued for other machines and processes them. Benefits
    of this method are improved load balance and efficiency due to reduced idle times.
  prefs: []
  type: TYPE_NORMAL
- en: FlexGraph (Wang et al., [2021c](#bib.bib179)) deploys workload balancing using
    a cost function to reduce communication. In place of metrics like vertex weight
    or edge weight, the proposed cost function is based upon the GNN training cost
    per partition. To predict the computation cost of a vertex, features like the
    number of neighbors as well as the size of each neighborhood are taken into account.
    The predicted costs of all vertices are summed to estimate the final computation
    cost of the partition. An online workload balancing strategy uses the estimations
    to construct a fixed number of balancing plans where certain vertices should be
    moved from overloaded to other partitions. Finally, the system chooses the plan
    cutting the least number of edges. For an even more efficient computational process,
    FlexGraph uses a pipeline processing strategy overlapping computation and communication.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by PipeDream (Narayanan et al., [2019](#bib.bib134)), P³ exploits a
    simple pipelining mechanism. As soon as a computation phase of a mini-batch is
    dependent on another phase, communication starts. This communication is overlapped
    with the computation of other mini-batches to avoid stalls. Due to the pipeline
    delay, weight staleness occurs. Consequently, a weight update function regarding
    weights from the previous forward and backward pass is applied. Dorylus (Thorpe
    et al., [2021](#bib.bib173)) decomposes forward and backward pass into fine-grained
    tasks. The tasks are categorized based on data type and computation type. Depending
    on the type, the tasks are processed differently and can be performed concurrently.
    Hereby, communication latency is avoided. Furthermore, the tasks are pooled and
    whenever a worker is ready, it takes the one that is scheduled next and executes
    it. ZIPPER (Zhang et al., [2021a](#bib.bib211)) co-locates operations of different
    subgraphs with a pipelining strategy. As different operations target different
    resources, the overall performance increases due to more efficiently utilized
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.9\. Coordination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Section [2.3.3](#S2.SS3.SSS3 "2.3.3\. Coordination ‣ 2.3\. Distributed Neural
    Network Training ‣ 2\. Foundations ‣ The Evolution of Distributed Systems for
    Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") showed that it is possible to perform the training phase in a centralized,
    decentralized or hybrid manner. DistDGL (Zheng et al., [2020](#bib.bib214)) leaves
    the choice whether to operate central or decentral to the underlying framework.
    For example, if DistDGL is built on top of PyTorch (Paszke et al., [2019](#bib.bib139)),
    an all-reduce primitive is executed to collect and distribute information. However,
    if the backend framework is TensorFlow (Abadi et al., [2016](#bib.bib3)), DistDGL
    supports a parameter server implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AGL (Zhang et al., [2020c](#bib.bib205)) operates central and makes use of
    a parameter server as introduced in Section [2.3.3](#S2.SS3.SSS3 "2.3.3\. Coordination
    ‣ 2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey"). The parameter server stores all required data and
    features. Each machine accesses it to fetch the assigned graph partition and exchanges
    updates without extra communication from machine to machine. GraphTheta (Li et al.,
    [2021a](#bib.bib109)) also supports computation in a central fashion. Whereas
    systems like AGL store current model parameters, the parameter server in GraphTheta
    keeps multiple version of parameters. In this manner, machines can fetch the required
    parameter version at any time helping to concurrently execute tasks with the appropriate
    parameters. DistGNN (Md et al., [2021](#bib.bib131)) shares updates in a decentralized
    way with an all-reduce operation and direct communication from machine to machine.
    Consequently, the need for a parameter server is eliminated. Another decentral
    system is PaGraph (Lin et al., [2020](#bib.bib112)). Here, trainer processes directly
    interact to exchange locally computed gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.10\. Datasets and Benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table 9. Overview of graph datasets
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  | Task Type |  |'
  prefs: []
  type: TYPE_TB
- en: '| Name |  | #Vertices | #Edges | Vertex | Edge | Graph | Systems |'
  prefs: []
  type: TYPE_TB
- en: '| CiteSeer | (Giles et al., [1998](#bib.bib55)) | 3,327 | 4,732 | ✓ | ✓ |  |
    PyG, GraphTheta, GNNAdvisor, GNNAutoScale |'
  prefs: []
  type: TYPE_TB
- en: '| CORA | (McCallum et al., [2000](#bib.bib129)) | 2,708 | 5,429 | ✓ | ✓ |  |
    PyG, AGL, GraphTheta, GNNAdvisor, GNNAutoScale |'
  prefs: []
  type: TYPE_TB
- en: '| PubMed | (Sen et al., [2008](#bib.bib159)) | 19,717 | 44,338 | ✓ | ✓ |  |
    PyG, NeuGraph, ROC, GraphTheta, GNNAdvisor, GNNAutoScale |'
  prefs: []
  type: TYPE_TB
- en: '| PPI | (Zitnik and Leskovec, [2017](#bib.bib218); Hamilton et al., [2017a](#bib.bib67))
    | 2,373 | 61,318 | ✓ |  |  | PyG, ROC, AGL, GNNAdvisor, GNNAutoScale |'
  prefs: []
  type: TYPE_TB
- en: '| Reddit | (Hamilton et al., [2017a](#bib.bib67)) | 232,965 | 114,848,857 |
    ✓ |  | ✓ | DGL, GReTA, NeuGraph, ROC, PaGraph, GraphTheta, FlexGraph, Dorylus,
    DistGNN, DeepGalois |'
  prefs: []
  type: TYPE_TB
- en: '| LiveJournal | (Yang and Leskovec, [2015](#bib.bib199)) | 4,847,571 | 68,993,773
    |  | ✓ |  | GReTA, PaGraph, ZIPPER |'
  prefs: []
  type: TYPE_TB
- en: '| OGBL-ppa | (Hu et al., [2020a](#bib.bib78)) | 576,289 | 30,326,273 |  | ✓
    |  | DGL |'
  prefs: []
  type: TYPE_TB
- en: '| OGBL-citation2 | (Hu et al., [2020a](#bib.bib78)) | 2,927,963 | 30,561,187
    |  | ✓ |  | DGL |'
  prefs: []
  type: TYPE_TB
- en: '| OGBN-arxiv | (Hu et al., [2020a](#bib.bib78)) | 169,343 | 1,166,243 | ✓ |  |  |
    DGL, GNNAutoScale |'
  prefs: []
  type: TYPE_TB
- en: '| OGBN-proteins | (Hu et al., [2020a](#bib.bib78)) | 132,534 | 39,561,252 |
    ✓ |  |  | DGL, GNNAdvisor |'
  prefs: []
  type: TYPE_TB
- en: '| OGBN-products | (Hu et al., [2020a](#bib.bib78)) | 2,449,029 | 61,859,140
    | ✓ |  |  | DGL, DistDGL, P3, GNNAutoScale, DistGNN, DeepGalois |'
  prefs: []
  type: TYPE_TB
- en: '| OGBN-papers100M | (Hu et al., [2020a](#bib.bib78)) | 111,059,956 | 1,615,685,872
    | ✓ |  |  | DistDGL, P3, DistGNN |'
  prefs: []
  type: TYPE_TB
- en: '| MAG240M | (Hu et al., [2021](#bib.bib77)) | 244,160,499 | 1,728,364,232 |
    ✓ |  |  | - |'
  prefs: []
  type: TYPE_TB
- en: '| WikiKG90Mv2 | (Hu et al., [2021](#bib.bib77)) | 91,230,610 | 601,062,811
    |  | ✓ |  | - |'
  prefs: []
  type: TYPE_TB
- en: '| PCQM4Mv2 | (Hu et al., [2021](#bib.bib77)) | 52,970,652 | 54,546,813 |  |  |
    ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: 'This section gives an overview of commonly used datasets in the literature
    to provide a summary of applications and use cases of GNN systems. We highlight
    selected publicly available graph datasets and show their characteristics (see
    Table [9](#S3.T9 "Table 9 ‣ 3.2.10\. Datasets and Benchmarks ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey")). There are
    some early citation graph datasets also used to evaluate graph processing systems
    called CiteSeer (Giles et al., [1998](#bib.bib55)), CORA (McCallum et al., [2000](#bib.bib129))
    and PubMed (Sen et al., [2008](#bib.bib159)). Here, the vertices represent documents
    and the edges represent the citations between them. The size is rather small with
    approximately 3,000 vertices and 5,000 edges in CiteSeer and CORA and around 19,700
    vertices and 44,300 edges in PubMed. Predictions about the vertices and edges
    can be made, however, no graph-level tasks are currently included. Those sets
    are included in systems like PyG (Fey and Lenssen, [2019](#bib.bib43)), GraphTheta
    (Li et al., [2021a](#bib.bib109)), GNNAdvisor (Wang et al., [2021a](#bib.bib184))
    and GNNAutoScale (Fey et al., [2021](#bib.bib44)). The Protein-Protein-Interaction
    (PPI) dataset (Zitnik and Leskovec, [2017](#bib.bib218); Hamilton et al., [2017a](#bib.bib67))
    models the role of proteins in different types of human tissue. It contains 20
    graphs, each with an average number of 2,373 vertices and is applicable to vertex-level
    tasks and is included in systems like PyG (Fey and Lenssen, [2019](#bib.bib43)),
    ROC (Jia et al., [2020a](#bib.bib88)) and AGL (Zhang et al., [2020c](#bib.bib205)).'
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, a common strategy to acquire graph structured data is to crawl social
    networks and use community information as basis for the resulting graph. About
    233,000 Reddit posts from different communities are included in the Reddit (Hamilton
    et al., [2017a](#bib.bib67)) dataset and the LiveJournal (Yang and Leskovec, [2015](#bib.bib199))
    set represents around 4.8 million users and their connections. Especially the
    Reddit dataset is used by numerous systems (Wang et al., [2019](#bib.bib180);
    Kiningham et al., [2020a](#bib.bib96); Ma et al., [2019](#bib.bib118); Jia et al.,
    [2020a](#bib.bib88); Lin et al., [2020](#bib.bib112); Li et al., [2021a](#bib.bib109);
    Md et al., [2021](#bib.bib131)) to measure performance. In contrast to the PPI
    set, vertex- and graph-level tasks may be carried out on the Reddit set.
  prefs: []
  type: TYPE_NORMAL
- en: The Open Graph Benchmark (OGB) (Hu et al., [2020a](#bib.bib78)) comprises a
    collection of datasets of varying size, origin and task types. It is differentiated
    between small, medium and large sets. The small ones consist of up to 170,000
    vertices (OGBN-arxiv), the medium of up to 2.9 million vertices (OGBL-citation2)
    and the large ones of up to 111 million vertices (OGBN-papers100M). Recently,
    even larger sets have been added in the course of a large-scale challenge (Hu
    et al., [2021](#bib.bib77)) to represent real world data. The largest set, namely
    MAG240M, includes an academic graph representing papers, paper subjects, authors
    and institutions. To store the approximately 244 million vertices and 1 billion
    edges, more than 200 GB are needed. The WikiKG90Mv2 knowledge graph consists of
    91 million vertices and around 601 million edges resulting in a file size of up
    to 160 GB. The third set, PCQM4Mv2, is around 8 GB large and contains over 3,700
    graphs with a total vertex number of around 53 million and 54 million edges. OGB
    also provides a unified evaluation and benchmarking suite. In this manner, researchers
    are able to run, test and compare the performance of their model to the existing
    state-of-the-art.
  prefs: []
  type: TYPE_NORMAL
- en: To date, there is no established standard dataset used by all systems for evaluation.
    Thus, comparison of their performance is difficult. One could categorize the datasets
    depending on the sizes and task types as done for the OGB datasets to compare
    the systems. However, the characteristics of the graphs might differ. For example,
    some graphs with a similar number of vertices may have varying numbers of edges
    resulting in more sparse or more dense graphs. A special case are fully connected
    graphs, where each vertex is connected to each other vertex. Another issue is
    the continuously growing size of real world graphs. If a graph might be suitable
    for representing real world data right now, it could not be suitable anymore in
    a couple of years and a new or updated dataset is needed, making comparison of
    system performance extremely difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10. Overview of availability and compatibility of publicly available systems
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Programming language | Hardware |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Year | System | Python | C/C++ | CPU | GPU | Compatibility |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ | ✓ | ✓ | ✓ | PyTorch,
    Tensorflow, MXNet |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | PyG | (Fey and Lenssen, [2019](#bib.bib43)) | ✓ |  | ✓ | ✓ | PyTorch
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | AliGraph | (Yang, [2019](#bib.bib198)) | ✓ | ✓ | ✓ | ✓ | PyTorch,
    Tensorflow |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) | ✓ |  | ✓ |  | DGL
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Dorylus | (Thorpe et al., [2021](#bib.bib173)) | ✓ | ✓ | serverless
    | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) | ✓ | ✓ | ✓ | ✓ |
    PyG, DGL, Gunrock |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GNNAutoScale | (Fey et al., [2021](#bib.bib44)) | ✓ | ✓ | ✓ | ✓ |
    PyG |  |'
  prefs: []
  type: TYPE_TB
- en: 3.2.11\. Availability and Compatibility
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To better asses which system to choose, it is important to know which ones
    are freely accessible and which frameworks are compatible (see Table [10](#S3.T10
    "Table 10 ‣ 3.2.10\. Datasets and Benchmarks ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")). We found seven systems to
    be publicly available, namely PyG (Fey and Lenssen, [2019](#bib.bib43)), DGL (Wang
    et al., [2019](#bib.bib180)), AliGraph (Yang, [2019](#bib.bib198)), DistDGL (Zheng
    et al., [2020](#bib.bib214)), GNNAdvisor (Wang et al., [2021a](#bib.bib184)),
    GNNAutoScale (Fey et al., [2021](#bib.bib44)) and Dorylus (Thorpe et al., [2021](#bib.bib173)).
    PyG is a library built upon PyTorch (Paszke et al., [2019](#bib.bib139)) and is
    fully implemented in Python. It comes with multi-GPU support to achieve scalability.
    Also, there is a GitHub community¹¹1[https://github.com/pyg-team/pytorch_geometric](https://github.com/pyg-team/pytorch_geometric)
    with over 240 contributors. In contrast to PyG, DGL is framework agnostic and
    GNN models can be built with PyTorch, Tensorflow (Abadi et al., [2016](#bib.bib3))
    or Apache MXNet (Chen et al., [2015a](#bib.bib23)). DGL includes CPU and GPU support
    and is implemented in Python and C++. The GitHub community²²2[https://github.com/dmlc/dgl](https://github.com/dmlc/dgl)
    includes almost 200 contributors. DistDGL is integrated in DGL as a module³³3[https://docs.dgl.ai/en/0.6.x/api/python/dgl.distributed.html](https://docs.dgl.ai/en/0.6.x/api/python/dgl.distributed.html).
    The used programming language is Python and it runs on a cluster of CPUs. AliGraph⁴⁴4[https://github.com/alibaba/graph-learn](https://github.com/alibaba/graph-learn)
    is compatible with PyTorch and Tensorflow and uses Python and C++. GNNAdvisor⁵⁵5[https://github.com/YukeWang96/OSDI21_AE](https://github.com/YukeWang96/OSDI21_AE)
    is also implemented with Python and C++. It can either be built upon DGL, PyG
    or Gunrock (Wang et al., [2016](#bib.bib182)) as underlying framework and includes
    CPU and GPU support. GNNAutoScale, also referred to as PyGAS, is implemented in
    PyTorch and uses PyG. The code can be downloaded on GitHub⁶⁶6[https://github.com/rusty1s/pyg_autoscale](https://github.com/rusty1s/pyg_autoscale).
    Dorylus combines data servers with serverless computing. The main logic is written
    in Python and C++, the code is available on GitHub⁷⁷7[https://github.com/uclasystem/dorylus](https://github.com/uclasystem/dorylus).
    Besides the target use case, it is important to be aware of the framework support
    as well as utilized programming languages when deciding on a system. Some systems
    support various underlying frameworks, for instance DGL, AliGraph or GNNAdvisor.
    Other systems are bound to a specific set up, for example GNNAutoScale is built
    upon PyG, and PyG is implemented on top of PyTorch. Ultimately, the user needs
    to decide which one suits the application and the personal preferences the best.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.12\. Performance assessment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After reviewing the optimizations, availabilities and compatibilities of a plethora
    of systems, it would be interesting to understand the performance behavior of
    each approach. As a full fledged benchmarking effort is beyond the scope of this
    survey, we now present a selection of performance results available from the literature.
    DistDGL (Zheng et al., [2020](#bib.bib214)) gains an overall speedup of $2.2\times$
    over Euler (Alibaba, [2020](#bib.bib5)) and is more than $5\times$ faster in the
    data copy phase. GNNAdvisor (Wang et al., [2021a](#bib.bib184)) outperforms DGL
    (Wang et al., [2019](#bib.bib180)) by a factor of $3$ and NeuGraph (Ma et al.,
    [2019](#bib.bib118)) by a factor of up to $4$ when measuring training time. When
    evaluating ROC (Jia et al., [2020a](#bib.bib88)), the system achieves to perform
    up to $4\times$ higher as measured in number of epochs per second in the given
    experiments than NeuGraph. ROC, in turn, is outperformed by P³ (Gandhi and Iyer,
    [2021](#bib.bib48)) which completes epochs up to $2\times$ faster. Although some
    general points can be made above about how well the systems perform, the used
    datasets and hardware setups differ across the reported systems. This makes it
    difficult to draw explicit conclusions from the reported quantifications across
    different papers. Consequently, the need for a comprehensive performance comparison
    of GNN systems arises, a worthy endeavor for future work.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11. Connections across systems for graph processing, DL and GNNs
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Graph Processing Systems | DL Systems | GNN Systems |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Partitioning | Section [2.2.2](#S2.SS2.SSS2 "2.2.2\. Partitioning ‣ 2.2\.
    Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") | - | Section [3.2.1](#S3.SS2.SSS1 "3.2.1\. Partitioning
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") |'
  prefs: []
  type: TYPE_TB
- en: '| Sampling | Section [2.2.1](#S2.SS2.SSS1 "2.2.1\. Sampling ‣ 2.2\. Distributed
    Graph Processing ‣ 2\. Foundations ‣ The Evolution of Distributed Systems for
    Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") | - | Section [3.2.2](#S3.SS2.SSS2 "3.2.2\. Sampling ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey") |'
  prefs: []
  type: TYPE_TB
- en: '| Programming Abstraction | Section [2.2](#S2.SS2 "2.2\. Distributed Graph
    Processing ‣ 2\. Foundations ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey")
    | - | Section [3.2.3](#S3.SS2.SSS3 "3.2.3\. Programming Abstractions ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey") |'
  prefs: []
  type: TYPE_TB
- en: '| Inter-Process Communication | Section [2.2.3](#S2.SS2.SSS3 "2.2.3\. Inter-Process
    Communication ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") | - | Section [3.2.4](#S3.SS2.SSS4 "3.2.4\. Inter-Process
    Communication ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network
    Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") |'
  prefs: []
  type: TYPE_TB
- en: '| Parallelism | - | Section [2.3.1](#S2.SS3.SSS1 "2.3.1\. Parallelism ‣ 2.3\.
    Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") | Section [3.2.5](#S3.SS2.SSS5 "3.2.5\. Parallelism ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey")
    |'
  prefs: []
  type: TYPE_TB
- en: '| Message Propagation | Section [2.2.5](#S2.SS2.SSS5 "2.2.5\. Message Propagation
    ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") | - | Section [3.2.6](#S3.SS2.SSS6 "3.2.6\. Message Propagation
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") |'
  prefs: []
  type: TYPE_TB
- en: '| Synchronization Mode | Section [2.2.4](#S2.SS2.SSS4 "2.2.4\. State Synchronization
    and Scheduling ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") | Section [2.3.2](#S2.SS3.SSS2 "2.3.2\. Synchronization
    Mode ‣ 2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") | Section [3.2.7](#S3.SS2.SSS7 "3.2.7\. Synchronization
    Mode ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling | Section [2.2.4](#S2.SS2.SSS4 "2.2.4\. State Synchronization
    and Scheduling ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") | - | Section [3.2.8](#S3.SS2.SSS8 "3.2.8\. Scheduling
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") |'
  prefs: []
  type: TYPE_TB
- en: '| Coordination | - | Section [2.3.3](#S2.SS3.SSS3 "2.3.3\. Coordination ‣ 2.3\.
    Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") | Section [3.2.9](#S3.SS2.SSS9 "3.2.9\. Coordination ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey")
    |'
  prefs: []
  type: TYPE_TB
- en: 3.3\. Connections across graph processing, deep learning and GNN systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have introduced methods used to distribute and optimize graph processing
    (Section [2.2](#S2.SS2 "2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣
    The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")) and DL (Section [2.3](#S2.SS3
    "2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution of
    Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")) and techniques commonly used by systems for GNNs
    in Section [3.2](#S3.SS2 "3.2\. Categorization of Methods for Distributed Graph
    Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey"). Table [11](#S3.T11 "Table 11 ‣ 3.2.12\. Performance
    assessment ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network
    Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") shows how the introduced methods and the systems are connected.
    Systems for GNNs and systems for graph processing share a wide range of principles,
    such as partitioning, message propagation and scheduling. DL systems share ideas
    of parallelism, synchronization and coordination with GNN systems. Sampling occurs
    in GNN training and in graph processing. However, sampling in GNN training is
    not exactly the same as sampling in graph processing. In GNN training, sampling
    is used to exclude certain vertices during a training epoch and to create mini-batches,
    while sampling in graph processing means to sparsify the whole input graph before
    starting the computation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Discussion and Outlook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen that designing systems for GNN training is a challenging task.
    The recently proposed systems face issues like workload imbalance, redundant vertex
    accesses, communication overhead or changing the parallelism. In the following,
    we discuss current topics that are increasingly investigated as well as open challenges
    that still arise when developing GNN systems.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Current Research Trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An increasing amount of research dealing with the co-design of software and
    hardware to accelerate GNN training (Zhang et al., [2020e](#bib.bib204); Chen
    et al., [2021](#bib.bib24); Kiningham et al., [2020b](#bib.bib97), [a](#bib.bib96)).
    Here, not only software and algorithms are optimized, but also hardware modules
    are developed to better address the characteristics of GNNs. Another interesting
    approach examines the acceleration of quantized GNN architectures (Wang et al.,
    [2022](#bib.bib183)). Quantized GNNs (Tailor et al., [2020](#bib.bib169); Feng
    et al., [2020](#bib.bib41); Ding et al., [2021](#bib.bib33); Saad and Beferull-Lozano,
    [2021](#bib.bib152)) have increasingly emerged in the past years and incorporate
    compressed weights and node embeddings to reduce the memory footprint and computation.
    In addition, they are robust and the loss of accuracy is marginal. Systems adapted
    to quantized GNNs can further accelerate training and inference times. Beyond
    quantized GNNs, acceleration techniques for dynamic graphs have been proposed
    (Guan et al., [2022](#bib.bib62); Wang et al., [2021b](#bib.bib181)). Dynamic
    graphs change over time, edges might be added or deleted and vertex features evolve.
    Some architectures exist that are able to process dynamic graphs, for example,
    SDG (Fu and He, [2021](#bib.bib46)), Dynamic-GRCNN (Peng et al., [2020](#bib.bib140)),
    and DyGNN (Ma et al., [2020](#bib.bib119)). Dynamic graphs and architectures for
    processing them pose new research challenges that are currently being solved.
    Ongoing research also aims at optimizing inference for GNNs (Zhang et al., [2022](#bib.bib207);
    Gao et al., [2022](#bib.bib51)), as real-time inference might be crucial for applications
    such as self-driving cars (Zhou et al., [2021](#bib.bib215)). Methods range from
    combining multi-layer perceptrons (Jain et al., [1996](#bib.bib86)) with GNNs
    (Zhang et al., [2022](#bib.bib207)), introducing a novel propagation scheme which
    is adapted per node (Gao et al., [2022](#bib.bib51)) and pruning channels of the
    feature matrices which leads to pruned weight matrices (Zhou et al., [2021](#bib.bib215)).
    Moreover, work focusing on improving the data loading and preprocessing step is
    proposed (Liu et al., [2021](#bib.bib114); Jangda et al., [2021](#bib.bib87)).
    When measuring the time of the different training steps, it can be noted that
    a prominent proportion of the overall training time is consumed by reading in
    and preprocessing the graphs. By investigating and improving data I/O and preprocessing,
    the training process can be made more efficient in future work. Lately, neural
    architecture search (NAS) and automated methods for GNNs has caught the attention
    of many experts (Gao et al., [2020c](#bib.bib52); Cai et al., [2021](#bib.bib16);
    Li and King, [2020](#bib.bib110); Huan et al., [2021](#bib.bib80); Zhao et al.,
    [2020](#bib.bib213)). Going a step further, mechanisms like NAS can inspire researchers
    to also include automated methods assessing which optimization technique fits
    best for the given architecture, data and system. For instance, it could be automatically
    made a recommendation which partitioning strategy is most suitable depending on
    the requirements of the user. But not only single analyses could be made, also
    an overall estimation about the combination and composition of optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Open Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As GNNs and systems for GNNs are young research fields, there still are open
    challenges to be solved. Most systems are evaluated on static graphs and standard
    GNN models like GCN, GAT and GraphSAGE, but there is no information on how the
    systems would perform on more specialized models and types of data. There are,
    for instance, GNNs for hypergraphs (Feng et al., [2019](#bib.bib42); Yadati et al.,
    [2019](#bib.bib196)) and multigraphs (Ouyang et al., [2019](#bib.bib136); Geng
    et al., [2019](#bib.bib54)). As they come with different characteristics than
    general GNNs, it is likely that optimization techniques need to be adapted to
    those types of GNNs. Beyond that, deeper GNN architectures with numerous layers
    have recently been proposed (Li et al., [2020](#bib.bib107), [2021b](#bib.bib106);
    Liu et al., [2020](#bib.bib113)) which could pose new challenges for developing
    GNN systems. Another topic is the size of the feature vectors. When evaluating
    the systems, fixed feature vector lengths are used, but there is no consistent
    size within the presented systems. Now, the question is whether the systems perform
    equally well with variable feature vector sizes as the size of the feature vector
    might have a great effect on storage, data loading and computation time (Huang
    et al., [2021](#bib.bib81)). Moreover, the presented systems are all either based
    upon an existing graph processing system or a DL framework. Then, the missing
    operations needed for GNN training are added and optimized. Although this is an
    effective approach to design a GNN system, the characteristics of GNNs are not
    fully exploited. A GNN training iteration alternately incorporates dense computation
    of NN operations with regular memory access and a sparse aggregation phase with
    irregular memory accesses (Wang et al., [2021a](#bib.bib184)). Furthermore, the
    first layers of a GNN generally are the most compute intensive ones. Hence, it
    could be beneficial to adapt the system to handle the computations of the first
    layers differently compared to the subsequent ones. P³ is the first system going
    this direction, but it is difficult to find the optimal spot when to change from
    model to data parallelism and vice versa. In addition, finding a good way to partition
    and distribute the model among the machines is challenging. In general, such a
    method is not favorable if the underlying assumption that the activations are
    significantly smaller than the features is not met.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GNNs are increasingly used in various fields of application. With the steadily
    growing size of real-world graphs, the need for large-scale GNN systems arises.
    To better comprehend the main concepts of GNN systems, we first provided an overview
    of two fundamental topics, systems for graph processing and systems for DNN training.
    We established connections between the used methods and showed that many ideas
    of GNN systems are inspired by the two related fields. In the main part of this
    survey, we discussed, categorized and compared concepts for distributed GNN training.
    This included partitioning strategies, sampling methods, different types of parallelism
    as well as efficient scheduling and caching. We further investigated datasets
    and benchmarks for evaluation as well as availability and compatibility of the
    systems. Although the current systems are able to scale to large graphs, there
    still are unresolved issues. For instance, the support for specialized GNN architectures
    like dynamic GNNs or those using hierarchical aggregation are not fully explored.
    Further, it should be investigated how to handle variable feature vector sizes
    as the systems only provide insights into a fixed length. When looking into the
    future, we see a trend in investigating system support for quantized GNNs as they
    are robust and only have a small memory footprint. Another interesting type of
    GNN architecture that could be increasingly supported are dynamic GNNs. The software-hardware
    co-design of systems is receiving growing attention and systems even more adapted
    to the characteristics of GNNs can further improve the performance. Lastly, a
    closer look at the data loading and preprocessing phase can help to minimize the
    overall training time.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadal et al. (2021) Sergi Abadal, Akshay Jain, Robert Guirado, Jorge López-Alonso,
    and Eduard Alarcón. 2021. Computing graph neural networks: A survey from algorithms
    to accelerators. *ACM Comput. Surveys (CSUR)* 54, 9 (2021), 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, et al. 2016. $\{$TensorFlow$\}$: A System for $\{$Large-Scale$\}$ Machine
    Learning. In *12th USENIX symposium on operating systems design and implementation
    (OSDI 16)*. 265–283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abbas et al. (2018) Zainab Abbas, Vasiliki Kalavri, Paris Carbone, and Vladimir
    Vlassov. 2018. Streaming graph partitioning: an experimental study. *Proceedings
    of the VLDB Endowment* 11, 11 (2018), 1590–1603.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alibaba (2020) Alibaba. 2020. Euler. [https://github.com/alibaba/euler](https://github.com/alibaba/euler).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arai et al. (2016) Junya Arai, Hiroaki Shiokawa, Takeshi Yamamuro, Makoto Onizuka,
    and Sotetsu Iwamura. 2016. Rabbit order: Just-in-time parallel reordering for
    fast graph analysis. In *2016 IEEE IPDPS*. IEEE, 22–31.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balaban (1985) Alexandru T Balaban. 1985. Applications of graph theory in chemistry.
    *J.of chemical information and computer sciences* 25, 3 (1985), 334–343.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batarfi et al. (2015) Omar Batarfi, Radwa El Shawi, Ayman G Fayoumi, Reza Nouri,
    Ahmed Barnawi, Sherif Sakr, et al. 2015. Large scale graph processing systems:
    survey and an experimental evaluation. *Cluster Computing* 18, 3 (2015), 1189–1213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ben-Nun and Hoefler (2019) Tal Ben-Nun and Torsten Hoefler. 2019. Demystifying
    parallel and distributed deep learning: An in-depth concurrency analysis. *ACM
    Comput. Surveys (CSUR)* 52, 4 (2019), 1–43.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besta and Hoefler (2022) Maciej Besta and Torsten Hoefler. 2022. Parallel and
    Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. *arXiv preprint
    arXiv:2205.09702* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blumofe and Leiserson (1999) Robert D Blumofe and Charles E Leiserson. 1999.
    Scheduling multithreaded computations by work stealing. *J. of the ACM (JACM)*
    46, 5 (1999), 720–748.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bresson and Laurent (2017) Xavier Bresson and Thomas Laurent. 2017. Residual
    gated graph convnets. *arXiv preprint arXiv:1711.07553* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language models are few-shot learners. *arXiv preprint
    arXiv:2005.14165* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bu et al. (2014) Yingyi Bu, Vinayak Borkar, Jianfeng Jia, Michael J Carey,
    and Tyson Condie. 2014. Pregelix: Big (ger) graph analytics on a dataflow engine.
    *arXiv preprint arXiv:1407.0455* (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bundy and Wallen (1984) Alan Bundy and Lincoln Wallen. 1984. Breadth-first search.
    In *Catalogue of AI tools*. Springer, 13–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2021) Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun
    Zha, Li Su, and Qingming Huang. 2021. Rethinking graph neural network search from
    message-passing. *arXiv e-prints* (2021), arXiv–2103.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chahal et al. (2020) Karanbir Singh Chahal, Manraj Singh Grover, Kuntal Dey,
    and Rajiv Ratn Shah. 2020. A hitchhiker’s guide on distributed training of deep
    neural networks. *J. Parallel and Distrib. Comput.* 137 (2020), 65–76.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chami et al. (2020) Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher
    Ré, and Kevin Murphy. 2020. Machine learning on graphs: A model and comprehensive
    taxonomy. *arXiv preprint arXiv:2005.03675* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Cen Chen, Kenli Li, Yangfan Li, and Xiaofeng Zou. 2022.
    ReGNN: A Redundancy-Eliminated Graph Neural Networks Accelerator. In *2022 IEEE
    Int’l Symposium on High-Performance Computer Architecture (HPCA)*. IEEE, 429–443.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast
    learning with graph convolutional networks via importance sampling. *arXiv preprint
    arXiv:1801.10247* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016) Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and
    Rafal Jozefowicz. 2016. Revisiting distributed synchronous SGD. *arXiv preprint
    arXiv:1604.00981* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2015b) Rong Chen, Jiaxin Shi, Yanzhe Chen, and Haibo Chen. 2015b.
    PowerLyra: Differentiated Graph Computation and Partitioning on Skewed Graphs.
    In *Proc. of the Tenth European Conf. on Computer Systems* *(EuroSys ’15)*. Association
    for Comput. Machinery, Article 1, 15 pages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2015a) Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie
    Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015a. Mxnet: A flexible
    and efficient machine learning library for heterogeneous distributed systems.
    *arXiv preprint arXiv:1512.01274* (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Xiaobing Chen, Yuke Wang, Xinfeng Xie, Xing Hu, Abanti Basak,
    Ling Liang, Mingyu Yan, Lei Deng, Yufei Ding, Zidong Du, et al. 2021. Rubik: A
    hierarchical architecture for efficient graph neural network training. *IEEE Trans.
    on Computer-Aided Design of Integrated Circuits and Systems* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2019) Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio,
    and Cho-Jui Hsieh. 2019. Cluster-gcn: An efficient algorithm for training deep
    and large graph convolutional networks. In *Proc. of the 25th ACM SIGKDD Int’l
    Conf. on Knowledge Discovery & Data Mining*. 257–266.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chilimbi et al. (2014) Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and
    Karthik Kalyanaraman. 2014. Project adam: Building an efficient and scalable deep
    learning training system. In *11th $\{$USENIX$\}$ Symposium on Operating Systems
    Design and Implementation ($\{$OSDI$\}$ 14)*. 571–582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cipar et al. (2013) James Cipar, Qirong Ho, Jin Kyu Kim, Seunghak Lee, Gregory R
    Ganger, Garth Gibson, Kimberly Keeton, and Eric Xing. 2013. Solving the straggler
    problem with bounded staleness. In *14th Workshop on Hot Topics in Operating Systems
    (HotOS $\{$XIV$\}$)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coimbra et al. (2021) Miguel E Coimbra, Alexandre P Francisco, and Luís Veiga.
    2021. An analysis of the graph processing landscape. *J. of big Data* 8, 1 (2021),
    1–41.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Czerwionka et al. (2011) Paul Czerwionka, Miao Wang, and Fabian Wiesel. 2011.
    Optimized route network graph as map reference for autonomous cars operating on
    german autobahn. In *The 5th Int’l Conf. on Autom., Robotics and Appl.* IEEE,
    78–83.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dean et al. (2012) Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu
    Devin, Mark Mao, Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al.
    2012. Large scale distributed deep networks. *Advances in neural information processing
    systems* 25 (2012), 1223–1231.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dean and Ghemawat (2008) Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
    simplified data processing on large clusters. *Commun. ACM* 51, 1 (2008), 107–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dijkstra et al. (1959) Edsger W Dijkstra et al. 1959. A note on two problems
    in connexion with graphs. *Numer. math.* 1, 1 (1959), 269–271.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2021) Mucong Ding, Kezhi Kong, Jingling Li, Chen Zhu, John Dickerson,
    Furong Huang, and Tom Goldstein. 2021. VQ-GNN: A Universal Framework to Scale
    up Graph Neural Networks using Vector Quantization. *Advances in Neural Information
    Processing Systems* 34 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dominguez-Sal et al. (2010) David Dominguez-Sal, Norbert Martinez-Bazan, Victor
    Muntes-Mulero, Pere Baleta, and Josep Lluis Larriba-Pey. 2010. A discussion on
    the design of graph database benchmarks. In *Technology Conf. on Performance Evaluation
    and Benchmarking*. Springer, 25–40.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dutta et al. (2018) Sanghamitra Dutta, Gauri Joshi, Soumyadip Ghosh, Parijat
    Dube, and Priya Nagpurkar. 2018. Slow and stale gradients can win the race: Error-runtime
    trade-offs in distributed SGD. In *Int’l Conf. on AI and Statistics*. PMLR, 803–812.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan (2012) Wenfei Fan. 2012. Graph pattern matching revised for social network
    analysis. In *Proc. of the 15th Int’l Conf. on Database Theory*. 8–21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Wenfei Fan, Ping Lu, Wenyuan Yu, Jingbo Xu, Qiang Yin, Xiaojian
    Luo, Jingren Zhou, and Ruochun Jin. 2020. Adaptive asynchronous parallelization
    of graph algorithms. *ACM Trans. on Database Systems (TODS)* 45, 2 (2020), 1–45.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2019) Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang,
    and Dawei Yin. 2019. Graph neural networks for social recommendation. In *The
    world wide web conf.* 417–426.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2017) Wenfei Fan, Jingbo Xu, Yinghui Wu, Wenyuan Yu, and Jiaxin
    Jiang. 2017. GRAPE: Parallelizing sequential graph computations. *Proc. of the
    VLDB Endowment* 10, 12 (2017), 1889–1892.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2022) Wenfei Fan, Ruiqi Xu, Qiang Yin, Wenyuan Yu, and Jingren Zhou.
    2022. Application-driven graph partitioning. *The VLDB Journal* (2022), 1–24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and
    Yufei Ding. 2020. Sgquant: Squeezing the last bit on graph neural networks with
    specialized quantization. In *2020 IEEE 32nd Int’l Conf. on Tools with AI (ICTAI)*.
    IEEE, 1044–1052.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2019) Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue
    Gao. 2019. Hypergraph neural networks. In *Proc. of the AAAI Conf. on AI*, Vol. 33\.
    3558–3565.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fey and Lenssen (2019) Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation
    learning with PyTorch Geometric. *arXiv preprint arXiv:1903.02428* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fey et al. (2021) Matthias Fey, Jan E Lenssen, Frank Weichert, and Jure Leskovec.
    2021. Gnnautoscale: Scalable and expressive graph neural networks via historical
    embeddings. In *Int’l Conf. on ML*. PMLR, 3294–3304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FOUNDATION (2012) APACHE SOFTWARE FOUNDATION. 2012. Giraph. [https://giraph.apache.org/](https://giraph.apache.org/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu and He (2021) Dongqi Fu and Jingrui He. 2021. SDG: A Simplified and Dynamic
    Graph Neural Network. In *Proc. of the 44th Int’l ACM SIGIR Conf. on Research
    and Development in Information Retrieval*. 2273–2277.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2020) Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020.
    Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding.
    In *Proc. of The Web Conf. 2020*. 2331–2341.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gandhi and Iyer (2021) Swapnil Gandhi and Anand Padmanabha Iyer. 2021. P3:
    Distributed deep graph learning at scale. In *15th $\{$USENIX$\}$ Symposium on
    Operating Systems Design and Implementation ($\{$OSDI$\}$ 21)*. 551–568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2020a) Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin
    Chen. 2020a. Multi-modal graph neural network for joint reasoning on vision and
    scene text. In *Proc. of the IEEE/CVF Conf. on CV and Pattern Recognition*. 12746–12756.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2020b) Jianliang Gao, Tengfei Lyu, Fan Xiong, Jianxin Wang, Weimao
    Ke, and Zhao Li. 2020b. Mgnn: A multimodal graph neural network for predicting
    the survival of cancer patients. In *Proc. of the 43rd Int’l ACM SIGIR Conf. on
    Research and Development in Information Retrieval*. 1697–1700.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2022) Xinyi Gao, Wentao Zhang, Yingxia Shao, Quoc Viet Hung Nguyen,
    Bin Cui, and Hongzhi Yin. 2022. Efficient Graph Neural Network Inference at Large
    Scale. *arXiv preprint arXiv:2211.00495* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2020c) Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu.
    2020c. Graph Neural Architecture Search.. In *IJCAI*, Vol. 20. 1403–1409.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaudelet et al. (2021) Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman,
    Cristian Regep, Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts,
    Jian Tang, et al. 2021. Utilizing graph machine learning within drug discovery
    and development. *Briefings in bioinformatics* 22, 6 (2021), bbab159.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geng et al. (2019) Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang,
    Jieping Ye, and Yan Liu. 2019. Spatiotemporal multi-graph convolution network
    for ride-hailing demand forecasting. In *Proc. of the AAAI conf. on AI*, Vol. 33\.
    3656–3663.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giles et al. (1998) C Lee Giles, Kurt D Bollacker, and Steve Lawrence. 1998.
    CiteSeer: An automatic citation indexing system. In *Proc. of the third ACM conf.
    on Digital libraries*. 89–98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gill et al. (2018) Gurbinder Gill, Roshan Dathathri, Loc Hoang, and Keshav Pingali.
    2018. A study of partitioning policies for graph analytics on large-scale distributed
    platforms. *Proceedings of the VLDB Endowment* 12, 4 (2018), 321–334.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol
    Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry.
    In *Int’l conf. on ML*. PMLR, 1263–1272.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Girvan and Newman (2002) Michelle Girvan and Mark EJ Newman. 2002. Community
    structure in social and biological networks. *Proc. of the national academy of
    sciences* 99, 12 (2002), 7821–7826.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gonzalez et al. (2012) Joseph E Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson,
    and Carlos Guestrin. 2012. Powergraph: Distributed graph-parallel computation
    on natural graphs. In *10th $\{$USENIX$\}$ Symposium on Operating Systems Design
    and Implementation ($\{$OSDI$\}$ 12)*. 17–30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2016) Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
    2016. Deep feedforward networks. *Deep learning* (2016), 164–223.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gori et al. (2005) Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005.
    A new model for learning in graph domains. In *Proceedings. 2005 IEEE Int’l Joint
    Conf. on Neural Networks, 2005.*, Vol. 2\. IEEE, 729–734.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan et al. (2022) Mingyu Guan, Anand Padmanabha Iyer, and Taesoo Kim. 2022.
    DynaGraph: dynamic graph neural networks at scale. In *Proc. of the 5th ACM SIGMOD
    Joint Int’l Workshop on Graph Data Management Experiences & Systems (GRADES) and
    Network Data Analytics (NDA)*. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gui et al. (2019) Chuang-Yi Gui, Long Zheng, Bingsheng He, Cheng Liu, Xin-Yu
    Chen, Xiao-Fei Liao, and Hai Jin. 2019. A survey on graph processing accelerators:
    Challenges and opportunities. *J. of Comp. Science and Tech.* 34, 2 (2019), 339–371.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie,
    Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems.
    *IEEE Trans. on Knowledge and Data Engineering* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2013) Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong
    Wang, and Reza Zadeh. 2013. Wtf: The who to follow service at twitter. In *Proc.
    of the 22nd int’l conf. on World Wide Web*. 505–514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamilton (2020) William L Hamilton. 2020. Graph representation learning. *Synthesis
    Lectures on AI and ML* 14, 3 (2020), 1–159.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamilton et al. (2017a) William L Hamilton, Rex Ying, and Jure Leskovec. 2017a.
    Inductive representation learning on large graphs. In *Proc. of the 31st Int’l
    Conf. on Neural Information Processing Systems*. 1025–1035.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hamilton et al. (2017b) William L Hamilton, Rex Ying, and Jure Leskovec. 2017b.
    Representation learning on graphs: Methods and applications. *arXiv preprint arXiv:1709.05584*
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanai et al. (2019) Masatoshi Hanai, Toyotaro Suzumura, Wen Jun Tan, Elvis Liu,
    Georgios Theodoropoulos, and Wentong Cai. 2019. Distributed edge partitioning
    for trillion-edge graphs. *arXiv preprint arXiv:1908.05855* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2018) Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. 2018. A
    twofold siamese network for real-time object tracking. In *Proc. of the IEEE Conf.
    on Computer Vision and Pattern Recognition*. 4834–4843.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heidari et al. (2018) Safiollah Heidari, Yogesh Simmhan, Rodrigo N Calheiros,
    and Rajkumar Buyya. 2018. Scalable graph processing frameworks: A taxonomy and
    open challenges. *ACM Computing Surveys (CSUR)* 51, 3 (2018), 1–53.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrickson and Leland (1993) Bruce Hendrickson and Robert Leland. 1993. *The
    chaco users guide. version 1.0*. Technical Report. Sandia National Labs., Albuquerque,
    NM (United States).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2013) Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu
    Kim, Phillip B Gibbons, Garth A Gibson, Greg Ganger, and Eric P Xing. 2013. More
    effective distributed ml via a stale synchronous parallel parameter server. In
    *Advances in neural information processing systems*. 1223–1231.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoang et al. (2021) Loc Hoang, Xuhao Chen, Hochan Lee, Roshan Dathathri, Gurbinder
    Gill, and Keshav Pingali. 2021. Efficient distribution for deep learning on large
    graphs. *update* 1050 (2021), 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoang et al. (2019) Loc Hoang, Roshan Dathathri, Gurbinder Gill, and Keshav
    Pingali. 2019. Cusp: A customizable streaming edge partitioner for distributed
    graph analytics. In *2019 IEEE Int’l Parallel and Distrib. Proc. Symp. (IPDPS)*.
    IEEE, 439–450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong,
    and Jure Leskovec. 2021. Ogb-lsc: A large-scale challenge for machine learning
    on graphs. *arXiv preprint arXiv:2103.09430* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020a) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020a. Open graph benchmark:
    Datasets for machine learning on graphs. *Advances in neural information processing
    systems* 33 (2020), 22118–22133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020b) Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li,
    Zheng Zhang, Zhiru Zhang, and Yida Wang. 2020b. Featgraph: A flexible and efficient
    backend for graph neural network systems. In *SC20: Int’l Conf. for High Performance
    Comput., Networking, Storage and Analysis*. IEEE, 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huan et al. (2021) ZHAO Huan, YAO Quanming, and TU Weiwei. 2021. Search to aggregate
    neighborhood for graph neural network. In *2021 IEEE 37th Int’l Conf. on Data
    Engineering (ICDE)*. IEEE, 552–563.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2021) Kezhao Huang, Jidong Zhai, Zhen Zheng, Youngmin Yi, and
    Xipeng Shen. 2021. Understanding and bridging the gaps in current GNN performance
    optimizations. In *Proc. of the 26th ACM SIGPLAN Symposium on PPoPP*. 119–132.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al.
    2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism.
    *Advances in neural information processing systems* 32 (2019), 103–112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2002) Zan Huang, Wingyan Chung, Thian-Huat Ong, and Hsinchun Chen.
    2002. A graph-based recommender system for digital library. In *Proc. of the 2nd
    ACM/IEEE-CS joint conf. on Digital libraries*. 65–73.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iyer et al. (2018a) Anand Padmanabha Iyer, Zaoxing Liu, Xin Jin, Shivaram Venkataraman,
    Vladimir Braverman, and Ion Stoica. 2018a. $\{$ASAP$\}$: Fast, approximate graph
    pattern mining at scale. In *13th USENIX Symposium on Operating Systems Design
    and Implementation (OSDI 18)*. 745–761.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iyer et al. (2018b) Anand Padmanabha Iyer, Aurojit Panda, Shivaram Venkataraman,
    Mosharaf Chowdhury, Aditya Akella, Scott Shenker, and Ion Stoica. 2018b. Bridging
    the GAP: towards approximate graph analytics. In *Proc. of the 1st ACM SIGMOD
    Joint Int’l Workshop on Graph Data Management Experiences & Systems (GRADES) and
    Network Data Analytics (NDA)*. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain et al. (1996) Anil K Jain, Jianchang Mao, and K Moidin Mohiuddin. 1996.
    Artificial neural networks: A tutorial. *Computer* 29, 3 (1996), 31–44.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jangda et al. (2021) Abhinav Jangda, Sandeep Polisetty, Arjun Guha, and Marco
    Serafini. 2021. Accelerating graph sampling for graph machine learning using GPUs.
    In *Proc. of the Sixteenth European Conf. on Computer Systems*. 311–326.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2020a) Zhihao Jia, Sina Lin, Mingyu Gao, Matei Zaharia, and Alex
    Aiken. 2020a. Improving the accuracy, scalability, and performance of graph neural
    networks with roc. *Proc. of ML and Systems* 2 (2020), 187–198.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2020b) Zhihao Jia, Sina Lin, Rex Ying, Jiaxuan You, Jure Leskovec,
    and Alex Aiken. 2020b. Redundancy-free computation for graph neural networks.
    In *Proc. of the 26th ACM SIGKDD Int’l Conf. on Knowledge Discovery & Data Mining*.
    997–1005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2016) Peter H Jin, Qiaochu Yuan, Forrest Iandola, and Kurt Keutzer.
    2016. How to scale distributed deep learning? *arXiv preprint arXiv:1611.04581*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalavri et al. (2017) Vasiliki Kalavri, Vladimir Vlassov, and Seif Haridi. 2017.
    High-level programming abstractions for distributed graph processing. *IEEE Trans.
    on Knowledge and Data Engineering* 30, 2 (2017), 305–324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karypis (1997) George Karypis. 1997. METIS: Unstructured graph partitioning
    and sparse matrix ordering system. *Tech. report* (1997).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karypis and Kumar (1998a) George Karypis and Vipin Kumar. 1998a. A fast and
    high quality multilevel scheme for partitioning irregular graphs. *SIAM J. on
    scientific Comput.* 20, 1 (1998), 359–392.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karypis and Kumar (1998b) George Karypis and Vipin Kumar. 1998b. Multilevel
    algorithms for multi-constraint graph partitioning. In *SC’98: Proc. of the 1998
    ACM/IEEE Conf. on Supercomputing*. IEEE, 28–28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khayyat et al. (2013) Zuhair Khayyat, Karim Awara, Amani Alonazi, Hani Jamjoom,
    Dan Williams, and Panos Kalnis. 2013. Mizan: a system for dynamic load balancing
    in large-scale graph processing. In *Proc. of the 8th ACM European conf. on computer
    systems*. 169–182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kiningham et al. (2020a) Kevin Kiningham, Philip Levis, and Christopher Ré.
    2020a. GReTA: Hardware Optimized Graph Processing for GNNs. In *Proc. of the Workshop
    on Resource-Constrained ML (ReCoML 2020)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kiningham et al. (2020b) Kevin Kiningham, Christopher Re, and Philip Levis.
    2020b. GRIP: a graph neural network accelerator architecture. *arXiv preprint
    arXiv:2007.13828* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2016a) Thomas N Kipf and Max Welling. 2016a. Semi-supervised
    classification with graph convolutional networks. *arXiv preprint arXiv:1609.02907*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2016b) Thomas N Kipf and Max Welling. 2016b. Variational graph
    auto-encoders. *arXiv preprint arXiv:1611.07308* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koliousis et al. (2019) Alexandros Koliousis, Pijika Watcharapichat, Matthias
    Weidlich, Luo Mai, Paolo Costa, and Peter Pietzuch. 2019. CROSSBOW: scaling deep
    learning with small batch sizes on multi-gpu servers. *arXiv preprint arXiv:1901.02244*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. *Advances
    in neural information processing systems* 25 (2012), 1097–1105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2019) Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting
    dynamic embedding trajectory in temporal interaction networks. In *Proc. of the
    25th ACM SIGKDD int’l conf. on knowledge discovery & data mining*. 1269–1278.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lawrence (1993) Jeannette Lawrence. 1993. *Introduction to neural networks*.
    California Scientific Software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeClair et al. (2020) Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin
    McMillan. 2020. Improved code summarization via a graph neural network. In *Proc.
    of the 28th int’l conf. on program comprehension*. 184–195.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1995) Yann LeCun, Yoshua Bengio, et al. 1995. Convolutional networks
    for images, speech, and time series. *The handbook of brain theory and neural
    networks* 3361, 10 (1995), 1995.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021b) Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun.
    2021b. Training graph neural networks with 1000 layers. In *Int’l conf. on ML*.
    PMLR, 6437–6449.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem.
    2020. Deepergcn: All you need to train deeper gcns. *arXiv preprint arXiv:2006.07739*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2015a) Hao Li, Asim Kadav, Erik Kruus, and Cristian Ungureanu. 2015a.
    Malt: distributed data-parallelism for existing ml applications. In *Proc. of
    the Tenth European Conf. on Computer Systems*. 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021a) Houyi Li, Yongchao Liu, Yongyong Li, Bin Huang, Peng Zhang,
    Guowei Zhang, Xintan Zeng, Kefeng Deng, Wenguang Chen, and Changhua He. 2021a.
    GraphTheta: A Distributed Graph Neural Network Learning System With Flexible Training
    Strategy. *arXiv preprint arXiv:2104.10569* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and King (2020) Yaoman Li and Irwin King. 2020. Autograph: Automated graph
    neural network. In *Int’l Conf. on Neural Information Processing*. Springer, 189–201.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2015b) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015b. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020) Zhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, and Yinlong
    Xu. 2020. Pagraph: Scaling gnn training on large graphs via computation-aware
    caching. In *Proc. of the 11th ACM Symposium on Cloud Comput.* 401–415.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Meng Liu, Hongyang Gao, and Shuiwang Ji. 2020. Towards deeper
    graph neural networks. In *Proc. of the 26th ACM SIGKDD int’l conf. on knowledge
    discovery & data mining*. 338–348.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Tianfeng Liu, Yangrui Chen, Dan Li, Chuan Wu, Yibo Zhu, Jun
    He, Yanghua Peng, Hongzheng Chen, Hongzhi Chen, and Chuanxiong Guo. 2021. BGL:
    GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing. *arXiv
    preprint arXiv:2112.08541* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low (2013) Yucheng Low. 2013. Graphlab: A distributed abstraction for large
    scale machine learning. *Univ. of California* (2013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low et al. (2012) Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson,
    Carlos Guestrin, and Joseph M Hellerstein. 2012. Distributed graphlab: A framework
    for machine learning in the cloud. *arXiv preprint arXiv:1204.6078* (2012).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low et al. (2014) Yucheng Low, Joseph E Gonzalez, Aapo Kyrola, Danny Bickson,
    Carlos E Guestrin, and Joseph Hellerstein. 2014. Graphlab: A new framework for
    parallel machine learning. *arXiv preprint arXiv:1408.2041* (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2019) Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu,
    Lidong Zhou, and Yafei Dai. 2019. Neugraph: parallel deep neural network computation
    on large graphs. In *2019 $\{$USENIX$\}$ Ann. Tech. Conf. ($\{$USENIX$\}$$\{$ATC$\}$
    19)*. 443–458.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020) Yao Ma, Ziyi Guo, Zhaocun Ren, Jiliang Tang, and Dawei Yin.
    2020. Streaming graph neural networks. In *Proc. of the 43rd Int’l ACM SIGIR Conf.
    on Research and Development in Information Retrieval*. 719–728.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malewicz et al. (2010) Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C
    Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. 2010. Pregel: a system
    for large-scale graph processing. In *Proc. of the 2010 ACM SIGMOD Int’l Conf.
    on Management of data*. 135–146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manning et al. (2014) Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose
    Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural
    language processing toolkit. In *Proc. of 52nd ann. meeting of the association
    for computational linguistics: system demonstrations*. 55–60.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marcheggiani and Titov (2017) Diego Marcheggiani and Ivan Titov. 2017. Encoding
    sentences with graph convolutional networks for semantic role labeling. *arXiv
    preprint arXiv:1703.04826* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mason and Verwoerd (2007) Oliver Mason and Mark Verwoerd. 2007. Graph theory
    and networks in biology. *IET systems biology* 1, 2 (2007), 89–119.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mayer et al. (2018) Christian Mayer, Ruben Mayer, Muhammad Adnan Tariq, Heiko
    Geppert, Larissa Laich, Lukas Rieger, and Kurt Rothermel. 2018. Adwise: Adaptive
    window-based streaming edge partitioning for high-speed graph processing. In *2018
    IEEE 38th Int’l Conf. on Distrib. Comput. Systems (ICDCS)*. IEEE, 685–695.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mayer and Jacobsen (2020) Ruben Mayer and Hans-Arno Jacobsen. 2020. Scalable
    deep learning on distributed infrastructures: Challenges, techniques, and tools.
    *ACM Comput. Surveys (CSUR)* 53, 1 (2020), 1–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mayer and Jacobsen (2021) Ruben Mayer and Hans-Arno Jacobsen. 2021. Hybrid
    Edge Partitioner: Partitioning Large Power-Law Graphs under Memory Constraints.
    In *Proc. of the 2021 Int’l Conf. on Management of Data*. 1289–1302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mayer et al. (2020) Ruben Mayer, Kamil Orujzade, and Hans-Arno Jacobsen. 2020.
    2ps: High-quality edge partitioning with two-phase streaming. *arXiv preprint
    arXiv:2001.07086* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mayer et al. (2022) Ruben Mayer, Kamil Orujzade, and Hans-Arno Jacobsen. 2022.
    Out-of-core edge partitioning at linear run-time. In *2022 IEEE 38th Int’l Conf.
    on Data Engineering (ICDE)*. IEEE, 2629–2642.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCallum et al. (2000) Andrew Kachites McCallum, Kamal Nigam, Jason Rennie,
    and Kristie Seymore. 2000. Automating the construction of internet portals with
    machine learning. *Information Retrieval* 3, 2 (2000), 127–163.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCune et al. (2015) Robert Ryan McCune, Tim Weninger, and Greg Madey. 2015.
    Thinking like a vertex: a survey of vertex-centric frameworks for large-scale
    distributed graph processing. *ACM Computing Surveys (CSUR)* 48, 2 (2015), 1–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Md et al. (2021) Vasimuddin Md, Sanchit Misra, Guixiang Ma, Ramanarayan Mohanty,
    Evangelos Georganas, Alexander Heinecke, Dhiraj Kalamkar, Nesreen K Ahmed, and
    Sasikanth Avancha. 2021. DistGNN: Scalable Distributed Training for Large-Scale
    Graph Neural Networks. *arXiv preprint arXiv:2104.06700* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merkel et al. (pear) Nikolai Merkel, Ruben Mayer, Tawkir Ahmed Fakir, and Hans-Arno
    Jacobsen. To Appear. Partitioner Selection with EASE to Optimize Distributed Graph
    Processing. *Proceedings of the 2023 IEEE 39th Int’l Conf. on Data Engineering
    (ICDE’23)* (To Appear), 15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miller (1995) George A Miller. 1995. WordNet: a lexical database for English.
    *Commun. ACM* 38, 11 (1995), 39–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.
    2019. PipeDream: generalized pipeline parallelism for DNN training. In *Proc.
    of the 27th ACM Symposium on Operating Systems Principles*. 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2021) Shuo Ouyang, Dezun Dong, Yemao Xu, and Liquan Xiao. 2021.
    Communication optimization strategies for distributed deep neural network training:
    A survey. *J. Parallel and Distrib. Comput.* 149 (2021), 52–65.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2019) Yi Ouyang, Bin Guo, Xing Tang, Xiuqiang He, Jian Xiong,
    and Zhiwen Yu. 2019. Learning cross-domain representation with multi-graph neural
    network. *arXiv preprint arXiv:1905.10095* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pacaci and Özsu (2019) Anil Pacaci and M Tamer Özsu. 2019. Experimental analysis
    of streaming algorithms for graph partitioning. In *Proceedings of the 2019 International
    Conference on Management of Data*. 1375–1392.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Page et al. (1999) Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.
    1999. *The PageRank citation ranking: Bringing order to the web.* Technical Report.
    Stanford InfoLab.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *Advances in neural information processing systems* 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2020) Hao Peng, Hongfei Wang, Bowen Du, Md Zakirul Alam Bhuiyan,
    Hongyuan Ma, Jianwei Liu, Lihong Wang, Zeyu Yang, Linfeng Du, Senzhang Wang, et al.
    2020. Spatial temporal incidence dynamic graph neural networks for traffic flow
    forecasting. *Information Sciences* 521 (2020), 277–290.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Petroni et al. (2015) Fabio Petroni, Leonardo Querzoni, Khuzaima Daudjee, Shahin
    Kamali, and Giorgio Iacoboni. 2015. Hdrf: Stream-based partitioning for power-law
    graphs. In *Proc. of the 24th ACM int’l on conf. on information and knowledge
    management*. 243–252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raghavan et al. (2007) Usha Nandini Raghavan, Réka Albert, and Soundar Kumara.
    2007. Near linear time algorithm to detect community structures in large-scale
    networks. *Physical review E* 76, 3 (2007), 036106.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ralaivola et al. (2005) Liva Ralaivola, Sanjay J Swamidass, Hiroto Saigo, and
    Pierre Baldi. 2005. Graph kernels for chemical informatics. *Neural networks*
    18, 8 (2005), 1093–1110.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le.
    2017. Searching for activation functions. *arXiv preprint arXiv:1710.05941* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riba et al. (2018) Pau Riba, Andreas Fischer, Josep Lladós, and Alicia Fornés.
    2018. Learning graph distances with message passing neural networks. In *2018
    24th Int’l Conf. on Pattern Recognition (ICPR)*. IEEE, 2239–2244.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robbins and Monro (1951) Herbert Robbins and Sutton Monro. 1951. A stochastic
    approximation method. *The annals of math. statistics* (1951), 400–407.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rossi et al. (2020) Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide
    Eynard, Federico Monti, and Michael Bronstein. 2020. Temporal graph networks for
    deep learning on dynamic graphs. *arXiv preprint arXiv:2006.10637* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozemberczki et al. (2020) Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar.
    2020. Little ball of fur: a python library for graph sampling. In *Proc. of the
    29th ACM Int’l Conf. on Information & Knowledge Management*. 3133–3140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder (2016) Sebastian Ruder. 2016. An overview of gradient descent optimization
    algorithms. *arXiv preprint arXiv:1609.04747* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rumelhart et al. (1995) David E Rumelhart, Richard Durbin, Richard Golden,
    and Yves Chauvin. 1995. Backpropagation: The basic theory. *Backpropagation: Theory,
    architectures and applications* (1995), 1–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533–536.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saad and Beferull-Lozano (2021) Leila Ben Saad and Baltasar Beferull-Lozano.
    2021. Quantization in Graph Convolutional Neural Networks. In *2021 29th European
    Signal Processing Conf. (EUSIPCO)*. IEEE, 1855–1859.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salihoglu and Widom (2013) Semih Salihoglu and Jennifer Widom. 2013. Gps: A
    graph processing system. In *Proc. of the 25th int’l conf. on scientific and statistical
    database management*. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanders et al. (2019) Peter Sanders, Kurt Mehlhorn, Martin Dietzfelbinger, and
    Roman Dementiev. 2019. *Sequential and Parallel Algorithms and Data Structures*.
    Springer, 403–404.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarlin et al. (2020) Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
    and Andrew Rabinovich. 2020. Superglue: Learning feature matching with graph neural
    networks. In *Proc. of the IEEE/CVF conf. on CV and pattern recognition*. 4938–4947.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scarselli et al. (2008) Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
    Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural network model. *IEEE
    trans. on neural networks* 20, 1 (2008), 61–80.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schlag et al. (2019) Sebastian Schlag, Christian Schulz, Daniel Seemaier, and
    Darren Strash. 2019. Scalable edge partitioning. In *2019 Proc. of the Twenty-First
    Workshop on Algorithm Engineering and Experiments (ALENEX)*. SIAM, 211–225.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schramm et al. (2022) Michael Schramm, Sukanya Bhowmik, and Kurt Rothermel.
    2022. Flexible application-aware approximation for modern distributed graph processing
    frameworks. In *Proc. of the 5th ACM SIGMOD Joint Int’l Workshop on Graph Data
    Management Experiences & Systems (GRADES) and Network Data Analytics (NDA)*. 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sen et al. (2008) Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor,
    Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network
    data. *AI magazine* 29, 3 (2008), 93–93.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Serafini and Guan (2021) Marco Serafini and Hui Guan. 2021. Scalable Graph
    Neural Network Training: The Case for Sampling. *ACM SIGOPS Operating Systems
    Review* 55, 1 (2021), 68–76.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2017) Sagar Sharma, Simone Sharma, and Anidhya Athaiya. 2017.
    Activation functions in neural networks. *towards data science* 6, 12 (2017),
    310–316.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sherstinsky (2020) Alex Sherstinsky. 2020. Fundamentals of recurrent neural
    network (RNN) and long short-term memory (LSTM) network. *Physica D: Nonlinear
    Phenomena* 404 (2020), 132306.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2020) Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Chengjian Liu, Wei
    Wang, and Bo Li. 2020. A quantitative survey of communication optimizations in
    distributed deep learning. *IEEE Network* 35, 3 (2020), 230–237.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2018) Xuanhua Shi, Zhigao Zheng, Yongluan Zhou, Hai Jin, Ligang
    He, Bo Liu, and Qiang-Sheng Hua. 2018. Graph processing on GPUs: A survey. *ACM
    Computing Surveys (CSUR)* 50, 6 (2018), 1–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silva et al. (2010) Nitai B Silva, Ren Tsang, George DC Cavalcanti, and Jyh
    Tsang. 2010. A graph-based friend recommendation system using genetic algorithm.
    In *IEEE congress on evolutionary computation*. IEEE, 1–7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sobota et al. (2008) B Sobota, Cs Szabó, and J Perhac. 2008. Using path-finding
    algorithms of graph theory for route-searching in geographical information systems.
    In *2008 6th Int’l Symposium on Intelligent Systems and Informatics*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stanton and Kliot (2012) Isabelle Stanton and Gabriel Kliot. 2012. Streaming
    graph partitioning for large distributed graphs. In *Proc. of the 18th ACM SIGKDD
    int’l conf. on Knowledge discovery and data mining*. 1222–1230.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sukhbaatar et al. (2016) Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learning
    multiagent communication with backpropagation. *Advances in neural information
    processing systems* 29 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tailor et al. (2020) Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D
    Lane. 2020. Degree-quant: Quantization-aware training for graph neural networks.
    *arXiv preprint arXiv:2008.05000* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang and Liu (2010) Lei Tang and Huan Liu. 2010. Graph mining applications to
    social network analysis. In *Managing and Mining Graph Data*. Springer, 487–513.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2020) Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, and
    Bo Li. 2020. Communication-efficient distributed deep learning: A comprehensive
    survey. *arXiv preprint arXiv:2003.06307* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarjan (1972) Robert Tarjan. 1972. Depth-first search and linear graph algorithms.
    *SIAM J. on Computing* 1, 2 (1972), 146–160.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thorpe et al. (2021) John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng,
    Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim,
    et al. 2021. Dorylus: Affordable, Scalable, and Accurate $\{$GNN$\}$ Training
    with Distributed $\{$CPU$\}$ Servers and Serverless Threads. In *15th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 21)*. 495–514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2013) Yuanyuan Tian, Andrey Balmin, Severin Andreas Corsten, Shirish
    Tatikonda, and John McPherson. 2013. From” think like a vertex” to” think like
    a graph”. *Proc. of the VLDB Endowment* 7, 3 (2013), 193–204.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsourakakis et al. (2014) Charalampos Tsourakakis, Christos Gkantsidis, Bozidar
    Radunovic, and Milan Vojnovic. 2014. Fennel: Streaming graph partitioning for
    massive scale graphs. In *Proc. of the 7th ACM int’l conf. on Web search and data
    mining*. 333–342.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valiant (1990) Leslie G Valiant. 1990. A bridging model for parallel computation.
    *Commun. ACM* 33, 8 (1990), 103–111.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks.
    *arXiv preprint arXiv:1710.10903* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2013) Guozhang Wang, Wenlei Xie, Alan J Demers, and Johannes Gehrke.
    2013. Asynchronous Large-Scale Graph Processing Made Easy.. In *CIDR*, Vol. 13.
    3–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021c) Lei Wang, Qiang Yin, Chao Tian, Jianbang Yang, Rong Chen,
    Wenyuan Yu, Zihang Yao, and Jingren Zhou. 2021c. FlexGraph: a flexible and efficient
    distributed framework for GNN training. In *Proc. of the Sixteenth EuroSys*. 67–82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao
    Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, et al. 2019. Deep Graph Library:
    Towards Efficient and Scalable Deep Learning on Graphs. (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Xuhong Wang, Ding Lyu, Mengjian Li, Yang Xia, Qi Yang,
    Xinwen Wang, Xinguang Wang, Ping Cui, Yupu Yang, Bowen Sun, et al. 2021b. APAN:
    Asynchronous propagation attention network for real-time temporal graph embedding.
    In *Proc. of the 2021 Int’l Conf. on Management of Data*. 2628–2638.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Yangzihao Wang, Andrew Davidson, Yuechao Pan, Yuduo Wu,
    Andy Riffel, and John D Owens. 2016. Gunrock: A high-performance graph processing
    library on the GPU. In *Proc. of the 21st ACM SIGPLAN symposium on principles
    and practice of parallel programming*. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Yuke Wang, Boyuan Feng, and Yufei Ding. 2022. QGTC: accelerating
    quantized graph neural networks via GPU tensor core. In *Proc. of the 27th ACM
    SIGPLAN Symposium on Principles and Practice of Parallel Programming*. 107–119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Yuke Wang, Boyuan Feng, Gushu Li, Shuangchen Li, Lei Deng,
    Yuan Xie, and Yufei Ding. 2021a. $\{$GNNAdvisor$\}$: An Adaptive and Efficient
    Runtime System for $\{$GNN$\}$ Acceleration on $\{$GPUs$\}$. In *15th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 21)*. 515–531.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Watcharapichat et al. (2016) Pijika Watcharapichat, Victoria Lopez Morales,
    Raul Castro Fernandez, and Peter Pietzuch. 2016. Ako: Decentralised deep learning
    with partial gradient exchange. In *Proc. of the Seventh ACM Symposium on Cloud
    Comput.* 84–97.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2016) Hao Wei, Jeffrey Xu Yu, Can Lu, and Xuemin Lin. 2016. Speedup
    graph processing by graph ordering. In *Proc. of the 2016 Int’l Conf. on Management
    of Data*. 1813–1828.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Widrow and Lehr (1990) Bernard Widrow and Michael A Lehr. 1990. 30 years of
    adaptive neural networks: perceptron, madaline, and backpropagation. *Proc. of
    the IEEE* 78, 9 (1990), 1415–1442.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019) Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao
    Yu, and Kilian Weinberger. 2019. Simplifying graph convolutional networks. In
    *Int’l conf. on ML*. PMLR, 6861–6871.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021) Yidi Wu, Kaihao Ma, Zhenkun Cai, Tatiana Jin, Boyang Li, Chenguang
    Zheng, James Cheng, and Fan Yu. 2021. Seastar: vertex-centric programming for
    graph neural networks. In *Proc. of the 16th Europ. Conf. on Comput. Systems*.
    359–375.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
    Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks.
    *IEEE trans. on neural networks and learning systems* 32, 1 (2020), 4–24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2015) Chenning Xie, Rong Chen, Haibing Guan, Binyu Zang, and Haibo
    Chen. 2015. Sync or async: Time to fuse for distributed graph-parallel computation.
    *ACM SIGPLAN Notices* 50, 8 (2015), 194–204.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2014) Cong Xie, Ling Yan, Wu-Jun Li, and Zhihua Zhang. 2014. Distributed
    Power-law Graph Computing: Theoretical and Empirical Analysis.. In *Nips*, Vol. 27\.
    1673–1681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018a) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
    2018a. How powerful are graph neural networks? *arXiv preprint arXiv:1810.00826*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018b) Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
    Kawarabayashi, and Stefanie Jegelka. 2018b. Representation learning on graphs
    with jumping knowledge networks. In *Int’l Conf. on ML*. PMLR, 5453–5462.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2014) Qiumin Xu, Hyeran Jeon, and Murali Annavaram. 2014. Graph
    processing on GPUs: Where are the bottlenecks?. In *2014 IEEE Int’l Symposium
    on Workload Characterization (IISWC)*. IEEE, 140–149.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yadati et al. (2019) Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram
    Nitin, Anand Louis, and Partha Talukdar. 2019. Hypergcn: A new method for training
    graph convolutional networks on hypergraphs. *Advances in neural information processing
    systems* 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2014) Da Yan, James Cheng, Yi Lu, and Wilfred Ng. 2014. Blogel:
    A block-centric framework for distributed computation on real-world graphs. *Proc.
    of the VLDB Endowment* 7, 14 (2014), 1981–1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang (2019) Hongxia Yang. 2019. Aligraph: A comprehensive graph neural network
    platform. In *Proc. of the 25th ACM SIGKDD int’l conf. on knowledge discovery
    & data mining*. 3165–3166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Leskovec (2015) Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating
    network communities based on ground-truth. *Knowledge and Information Systems*
    42, 1 (2015), 181–213.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2019) Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph convolutional
    networks for text classification. In *Proc. of the AAAI conf. on AI*, Vol. 33\.
    7370–7377.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ying et al. (2018) Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L
    Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
    recommender systems. In *Proc. of the 24th ACM SIGKDD Int’l Conf. on Knowledge
    Discovery & Data Mining*. 974–983.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You et al. (2019) Jiaxuan You, Rex Ying, and Jure Leskovec. 2019. Position-aware
    graph neural networks. In *Int’l Conf. on ML*. PMLR, 7134–7143.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2019) Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal
    Kannan, and Viktor Prasanna. 2019. Graphsaint: Graph sampling based inductive
    learning method. *arXiv preprint arXiv:1907.04931* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020e) Bingyi Zhang, Hanqing Zeng, and Viktor Prasanna. 2020e.
    Hardware acceleration of large scale gcn inference. In *2020 IEEE 31st Int’l Conf.
    on Application-specific Systems, Architectures and Processors (ASAP)*. IEEE, 61–68.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020c) Dalong Zhang, Xin Huang, Ziqi Liu, Zhiyang Hu, Xianzheng
    Song, Zhibang Ge, Zhiqiang Zhang, Lin Wang, Jun Zhou, Yang Shuang, et al. 2020c.
    Agl: a scalable system for industrial-purpose graph machine learning. *arXiv preprint
    arXiv:2003.02454* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020d) Fuyang Zhang, Nelson Nauata, and Yasutaka Furukawa. 2020d.
    Conv-mpn: Convolutional message passing neural network for structured outdoor
    architecture reconstruction. In *Proc. of the IEEE/CVF Conf. on Computer Vision
    and Pattern Recognition*. 2798–2807.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. 2022.
    Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation. In
    *Int’l Conf. on Learning Representations*. [https://openreview.net/forum?id=4p6_5HBWPCw](https://openreview.net/forum?id=4p6_5HBWPCw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021b) Yao Zhang, Yun Xiong, Dongsheng Li, Caihua Shan, Kan Ren,
    and Yangyong Zhu. 2021b. CoPE: Modeling Continuous Propagation and Evolution on
    Interaction Graph. In *Proc. of the 30th ACM Int’l Conf. on Information & Knowledge
    Management*. 2627–2636.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Zhen Zhang, Chaokun Chang, Haibin Lin, Yida Wang, Raman
    Arora, and Xin Jin. 2020a. Is network the bottleneck of distributed training?.
    In *Proc. of the Workshop on Network Meets AI & ML*. 8–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020b. Deep learning
    on graphs: A survey. *IEEE Trans. on Knowledge and Data Engineering* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021a) Zhihui Zhang, Jingwen Leng, Shuwen Lu, Youshan Miao, Yijia
    Diao, Minyi Guo, Chao Li, and Yuhao Zhu. 2021a. ZIPPER: Exploiting Tile-and Operator-level
    Parallelism for General and Scalable Graph Neural Network Acceleration. *arXiv
    preprint arXiv:2107.08709* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Zhaoning Zhang, Lujia Yin, Yuxing Peng, and Dongsheng Li.
    2018. A quick survey on large scale distributed deep learning systems. In *2018
    IEEE 24th Int’l Conf. on Parallel and Distrib. Systems (ICPADS)*. IEEE, 1052–1056.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020) Yiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja
    Jamnik, and Pietro Lio. 2020. Learned low precision graph neural networks. *arXiv
    preprint arXiv:2009.09232* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2020) Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su,
    Xiang Song, Quan Gan, Zheng Zhang, and George Karypis. 2020. Distdgl: distributed
    graph neural network training for billion-scale graphs. In *2020 IEEE/ACM 10th
    Workshop on Irregular Applications: Architectures and Algorithms (IA3)*. IEEE,
    36–44.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021) Hongkuan Zhou, Ajitesh Srivastava, Hanqing Zeng, Rajgopal
    Kannan, and Viktor Prasanna. 2021. Accelerating Large Scale Real-Time GNN Inference
    Using Channel Pruning. *Proc. VLDB Endow.* 14, 9 (oct 2021), 1597–1605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020) Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng
    Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural
    networks: A review of methods and applications. *AI Open* 1 (2020), 57–81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2015) Xiaowei Zhu, Wentao Han, and Wenguang Chen. 2015. $\{$GridGraph$\}$:$\{$Large-Scale$\}$
    Graph Processing on a Single Machine Using 2-Level Hierarchical Partitioning.
    In *2015 USENIX Ann. Tech. Conf. (USENIX ATC 15)*. 375–386.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zitnik and Leskovec (2017) Marinka Zitnik and Jure Leskovec. 2017. Predicting
    multicellular function through multi-layer tissue networks. *Bioinformatics* 33,
    14 (2017), i190–i198.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2019) Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and
    Quanquan Gu. 2019. Layer-dependent importance sampling for training deep and large
    graph convolutional networks. *arXiv preprint arXiv:1911.07323* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
