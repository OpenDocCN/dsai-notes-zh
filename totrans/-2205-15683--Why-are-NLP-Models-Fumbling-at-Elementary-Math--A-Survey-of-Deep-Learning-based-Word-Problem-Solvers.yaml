- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:46:14'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2205.15683] Why are NLP Models Fumbling at Elementary Math? A Survey of Deep
    Learning based Word Problem Solvers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.15683](https://ar5iv.labs.arxiv.org/html/2205.15683)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why are NLP Models Fumbling at Elementary Math?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Survey of Deep Learning based Word Problem Solvers
  prefs: []
  type: TYPE_NORMAL
- en: Sowmya S Sundaram L3S Research Center, Hannover, Germany Sairam Gurajada IBM
    Research, Almaden, USA¹¹1Work done while author was here Marco Fisichella L3S
    Research Center, Hannover, Germany Deepak P Queen’s University, Belfast, UK Savitha
    Sam Abraham University of Örebro, Sweden
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the latter half of the last decade, there has been a growing interest in
    developing algorithms for automatically solving mathematical word problems (MWP).
    It is a challenging and unique task that demands blending surface level text pattern
    recognition with mathematical reasoning. In spite of extensive research, we are
    still miles away from building robust representations of elementary math word
    problems and effective solutions for the general task. In this paper, we critically
    examine the various models that have been developed for solving word problems,
    their pros and cons and the challenges ahead. In the last two years, a lot of
    deep learning models have recorded competing results on benchmark datasets, making
    a critical and conceptual analysis of literature highly useful at this juncture.
    We take a step back and analyse why, in spite of this abundance in scholarly interest,
    the predominantly used experiment and dataset designs continue to be a stumbling
    block. From the vantage point of having analyzed the literature closely, we also
    endeavour to provide a road-map for future math word problem research.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural language processing has been one of the most popular and intriguing
    AI-complete sub-fields of artificial intelligence. One of the earliest systems
    arguably was the PhD Thesis on automatically solving arithmetic word problems
    Bobrow ([1964](#bib.bib2)). The challenge lay on two fronts (a) analysing unconstrained
    natural language, and (b) mapping intricate text patterns onto a small mathematical
    vocabulary, for usage within its reasoning framework.
  prefs: []
  type: TYPE_NORMAL
- en: Right up until 2010, there has been prolific exploration of MWP solvers, for
    various domains (such as algebra, percentages, ratio etc). These solvers relied
    heavily on hand-crafted rules for bridging the gap between language and the corresponding
    mathematical notation. As can be surmised, these approaches, while being effective
    within their niches, did not generalise well to address the broader problem of
    solving MWPs. Moreover, due to the lack of well accepted datasets, it is hard
    to measure the relative performance across proposed systems Mukherjee and Garain
    ([2008](#bib.bib45)).
  prefs: []
  type: TYPE_NORMAL
- en: '| Input | Kevin has 3 books. Kylie has 7 books. How many books do they have
    together? |'
  prefs: []
  type: TYPE_TB
- en: '| Answer | 10 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Typical Example'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pioneering work by Kushman et al. ([2014](#bib.bib27)) employed statistical
    methods to solve word problems, which set the stage for the development of automatic
    MWP solvers using traditional machine learning methods. The work also introduced
    the first dataset, popularly referred to as Alg514, that had multiple linear equations
    associated with a problem. The machine learning task was to map the coefficients
    in the equation to the numbers in the problem. The dataset comprises data units
    with a triplet structure: natural language question, equation set, and the final
    answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Mirroring recent trends in NLP, there has been an explosion of deep learning
    models for MWP. Some of the early ones Wang et al. ([2017](#bib.bib68)); Ling
    et al. ([2017](#bib.bib38)) modeled the task of converting the text to equation
    as a sequence-to-sequence (seq2seq, for short) problem. In this context, increasingly
    complex models have been proposed to capture semantics beyond the surface text.
    Some have captured structural information (pertaining to input text, domain knowledge,
    output equation structure) in the form of graphs and used advances in graph neural
    networks (Li et al. ([2020](#bib.bib33)), Zhang et al. ([2020c](#bib.bib80)),
    etc.). Others have utilised the benefits of transformers in their modelling (Liang
    et al. ([2021](#bib.bib35)), Piękos et al. ([2021](#bib.bib49)), etc.). We will
    explore these models in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a problem that has consistently attracted steady (arguably, slow
    and steady) attention, ostensibly right from the birth of the field of NLP, a
    survey of the problem solving techniques offers a good horizon for researchers.
    The authors collected 30+ papers on deep learning for word problem solving, published
    over the last three years across premier NLP avenues. Each paper has its own unique
    intuitive basis, but most achieve comparable empirical performance. The profusion
    of methods has made it hard to crisply point out the state-of-the-art, even for
    fairly general word problem solving settings. Hence, a broad overview of the techniques
    employed gives a good grounding for further research. Similarly, understanding
    the source, settings and relevance of datasets is often important. For example,
    there are many datasets that are often referred to by multiple names at different
    points in time. Also, the finer aspects of problem scenario varies across systems
    (whether multiple equations can be solved, whether it is restricted to algebra
    or more domains etc.). In this survey, we systematically analyse the models, list
    the benchmark datasets and examine word problem solving literature using a critical
    analysis perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Related Surveys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two seminal surveys that cover word problem solving research. One, Mukherjee
    and Garain ([2008](#bib.bib45)), has a detailed overview of the symbolic solvers
    for this problem. The second, more recent one Zhang et al. ([2020a](#bib.bib78)),
    covers models proposed up until 2020\. In the last two years, there has been a
    sharp spike in algorithms developed, that focus on various aspects of deep learning,
    to model this problem. Our survey is predominantly based on these deep learning
    models. The differentiating aspects of our survey from another related one, Faldu
    et al. ([2021](#bib.bib11)) are: the usage of a critical perspective to analyze
    deep learning models, which enables us to identify robustness deficiencies in
    the methods analytically, and also to trace them back to model design and dataset
    choice issues. We will also include empirical performance values of various methods
    on popular datasets, and deliberate on future directions.'
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic Solvers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin our discussion with traditional solvers that employ a rule-based method
    to convert text input to a set of symbols. Early solvers within this family such
    as STUDENT Bobrow ([1964](#bib.bib2)) and other subsequent ones (Fletcher ([1985](#bib.bib13)),
    Dellarosa ([1986](#bib.bib8))), the dominant methodology was to map natural language
    input to an underlying pre-defined schema. This calls for a mechanism to distil
    common expectations of language, word problems and the corresponding mathematical
    notation, to form bespoke rulesets that will power the conversion. This may be
    seen as setting up a slot-filling mechanism that map the main entities of the
    word problem to a slots within a set of equation templates.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a schema for algebraic MWP is shown in Table [2](#Sx2.T2 "Table
    2 ‣ Symbolic Solvers ‣ Why are NLP Models Fumbling at Elementary Math? A Survey
    of Deep Learning based Word Problem Solvers").
  prefs: []
  type: TYPE_NORMAL
- en: '| Problem | John has 5 apples. He gave 2 to Mary. How many does he have now?
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Template | [Owner[1]] has [X] [obj]. |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Owner[1]] [transfer] [Y] [obj] to [Owner[2]]. |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Owner[1]] has [Z] [obj]. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Z = X - Y |'
  prefs: []
  type: TYPE_TB
- en: '| Slot-Filling | [John] has [5] [apple]. |'
  prefs: []
  type: TYPE_TB
- en: '|  | [John] [give] [2] [apple] to [Mary]. |'
  prefs: []
  type: TYPE_TB
- en: '|  | [Mary] has [Z] [apple]. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Z = 5 - 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Answer | Z = 3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Workflow of Symbolic Solvers'
  prefs: []
  type: TYPE_NORMAL
- en: The advantage is that these systems are robust in handling irrelevant information,
    with expert-authored rulesets enabling focus towards pertinent parts of the problem.
    To further enhance the practical effectiveness within applications focusing niche
    domains, research focused on tailoring these symbolic systems for target domains Mukherjee
    and Garain ([2008](#bib.bib45)). As one can observe, the rules would need to be
    exhaustive to capture the myriad nuances of language. Thus, they did not generalise
    well across varying language styles. Since each system was designed for a particular
    domain, comparative performance evaluation was hindered by the unavailability
    of cross-domain datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Solvers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with many tasks in natural language processing, statistical machine learning
    techniques to solve word problems started dominating the field from 2014\. The
    central theme of these techniques has been to score a number of potential solutions
    (may be equations or expression trees as we will see shortly) within an optimization
    based scoring framework, and subsequently arrive at the correct mathematical model
    for the given text. This may be thought of as viewing the task as a structure
    prediction challenge Zhang et al. ([2020a](#bib.bib78)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(y&#124;x;\theta)=\frac{e^{\theta.\phi(x,y)}}{\sum_{y^{\prime}\in Y}e^{\theta.\phi(x,y^{\prime})}}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: As with optimization problems, Equation 1 refers to the problem of learning
    parameters $\theta$, which relate to the feature function $\phi$. Consider labeled
    dataset $D$ consisting of $n$ pairs $(x,y,a)$ where $x$ is the natural language
    question, $y$ is the mathematical expression and $a$ is the numerical answer.
    The task is to score all possible expressions $Y$, and maximise the choice of
    the labelled $y$ through an optimisation setting. This is done by modifying the
    parameters $\theta$ of the feature function $\phi(x,y)$. Different models propose
    different formulations of $\phi$. In practise, beam search is used as a control
    mechanism. We grouped the prolific algorithms that were developed, based on the
    type of mathematical structure $y$ - either as equation templates or expression
    trees. Equation templates were mined from training data, much like the slot filling
    idea of symbolic systems. However, they became a bottleneck to generalizability,
    if the word problem at inference time, was from an unseen equation template. To
    address this issue, expression trees, with unambiguous post-fix traversals, were
    used to model equations. Though they restricted the complexity of the systems
    to single equation models, they offered wider scope for generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: Equation Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Equation templates extract out the numeric coefficients and maintain the variable
    and operator structure. This was used as a popular representation of mathematical
    modelling. To begin with, Kushman et al. ([2014](#bib.bib27)), used structure
    prediction to score both equation templates and alignment of the numerals in the
    input text to coefficients in the template. Using a state based representation,
    Hosseini et al. ([2014](#bib.bib18)) modelled simple elementary level word problems
    with emphasis on verb categorisation. Zhou et al. ([2015](#bib.bib81)) enhanced
    the work done by Kushman et al. ([2014](#bib.bib27)) by using quadratic programming
    to increase efficiency. Upadhyay and Chang ([2017](#bib.bib65)) introduced a sophisticated
    method of representing derivations in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Expression Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Expression trees are applicable only to single equation systems. The single
    equation is represented as a tree, with leaves of the tree being numbers and the
    internal nodes being operators as illustrated in Koncel-Kedziorski et al. ([2015](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: Expression tree based methods converge faster, understandably due to the diminished
    complexity of the model. Some solvers (such as Roy and Roth ([2015](#bib.bib52)))
    had a joint optimisation objective to identify relevant numbers and populating
    the expression tree. On the other hand, Koncel-Kedziorski et al. ([2015](#bib.bib24));
    Mitra and Baral ([2016](#bib.bib44)) used domain knowledge to constrain the search
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Solvers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the major challenges for the solvers we have seen so far was that of converting
    the input text into a meaningful feature space to enable downstream solving; the
    main divergences across papers seen across the previous sections has been based
    on the technological flavour and methodology employed for such text-to-representation
    conversion.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: for tree= draw, text width=1.4cm, align=center , forked edges, [Automatic
  prefs: []
  type: TYPE_NORMAL
- en: Word
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs: []
  type: TYPE_NORMAL
- en: Solvers [Symbolic
  prefs: []
  type: TYPE_NORMAL
- en: Solvers] [Statistical
  prefs: []
  type: TYPE_NORMAL
- en: Solvers [Expression
  prefs: []
  type: TYPE_NORMAL
- en: Trees] [Equation
  prefs: []
  type: TYPE_NORMAL
- en: Templates] ] [Neural
  prefs: []
  type: TYPE_NORMAL
- en: Solvers [Seq2Seq] [Graph-Based] [Transformers] [Contrastive] [Knowledge
  prefs: []
  type: TYPE_NORMAL
- en: Distillation] ] ] \node[draw, fit=(current bounding box.south east) (current
    bounding box.north west)] ;
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Types of Word Problem Solvers'
  prefs: []
  type: TYPE_NORMAL
- en: The advent of distributed representations for text Le and Mikolov ([2014](#bib.bib29));
    Peters et al. ([2018](#bib.bib48)); Pennington et al. ([2014](#bib.bib47)); Devlin
    et al. ([2018](#bib.bib9)), marked a sharp departure in the line of inquiry towards
    solving math word problems, focusing on the details of the learning architecture
    rather than feature-space modelling. There have even been domain specific distributed
    representation learners for word problems Sundaram et al. ([2020](#bib.bib61)).
    As an example of solvers, Ling et al. ([2017](#bib.bib38)) designed a seq2seq
    model that incorporated learning a program as an intermediate step. This and other
    early works made it fashionable to treat the word problem solving task as a language
    translation task, i.e., translating from the input natural language text to a
    sequence of characters representing either the equation or a sequence of predicates.
    This design choice, however, has its limitations, which are sometimes severe in
    terms of the restrictions they place on math problems that can be admitted within
    such architectures Patel et al. ([2021](#bib.bib46)). A few of these linguistic
    vs. math structure understanding challenges, especially for neural solvers, are
    illustrated in Table  [3](#Sx4.T3 "Table 3 ‣ Neural Solvers ‣ Why are NLP Models
    Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers").
    As an important example, equation systems that involve solving multiple equations
    are not straightforward to address within such a framework. A notable exception
    to this is the popular baseline MathDQN Wang et al. ([2018](#bib.bib67)), which
    employs deep reinforcement learning. We consider different families of deep learning
    solvers within separate sub-sections herein.
  prefs: []
  type: TYPE_NORMAL
- en: '| Input | Kevin has 3 books. Kylie has 7 books and 3 pencils. How many books
    do they have together? |'
  prefs: []
  type: TYPE_TB
- en: '| Mathematical Structure | 3 + 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Linguistic Structure | (Person1) has (X) (object1). (Person2) has (Y) (object1)
    and (Z) (object2). |'
  prefs: []
  type: TYPE_TB
- en: '| Challenges | (1) Order of X and Y does not matter in addition (2) multiple
    equations do not make a sequence (3) Similar objects need to be grouped together
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Typical Challenges'
  prefs: []
  type: TYPE_NORMAL
- en: Seq2Seq Solvers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ubiquitous Seq2Seq Sutskever et al. ([2014](#bib.bib63)) architecture is
    widely popular for automatic word problem solving. From early direct use of LSTMs
    Hochreiter and Schmidhuber ([1997](#bib.bib16)) / GRUs Cho et al. ([2014](#bib.bib6))
    in Seq2Seq models (Huang et al. ([2017](#bib.bib19)), Wang et al. ([2017](#bib.bib68)))
    to complex models that include domain knowledge Ling et al. ([2017](#bib.bib38));
    Qin et al. ([2020](#bib.bib51)); Chiang and Chen ([2019](#bib.bib5)); Qin et al.
    ([2021](#bib.bib50))), diverse formulations of this basic architecture have been
    employed.
  prefs: []
  type: TYPE_NORMAL
- en: <svg   height="198.51"
    overflow="visible" version="1.1" width="353.91"><g transform="translate(0,198.51)
    matrix(1 0 0 -1 0 0) translate(61.76,0) translate(0,178.55)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -45.39 -3.38)" fill="#000000"
    stroke="#000000"><foreignobject width="90.79" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Input sequence</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 97.84 -3.38)" fill="#000000" stroke="#000000"><foreignobject width="101.93"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Output sequence</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -44.12 -84.1)" fill="#000000" stroke="#000000"><foreignobject
    width="88.25" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Math
    Problem</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 121.23 -82.68)"
    fill="#000000" stroke="#000000"><foreignobject width="55.16" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Equation</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -55.89 -162.05)" fill="#000000" stroke="#000000"><foreignobject width="112.16"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word Embeddings</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 107.27 -162.05)" fill="#000000" stroke="#000000"><foreignobject
    width="180.27" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Character/Word
    Embeddings</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: General Seq2Seq Formulations'
  prefs: []
  type: TYPE_NORMAL
- en: The initial set of models used Seq2Seq as is, with small variations in the usage
    of LSTM or GRUs or with simple heuristics (for example, Huang et al. ([2016](#bib.bib20))
    used retrieval to enhance the results). Significant improvements were made by
    including some mathematical aspects. This, once again, demonstrates that the task
    is not merely that of language translation. Ling et al. ([2017](#bib.bib38)) converted
    the word problem to a text containing the explanation or rationale. This was done
    through an intermediate step of generating a step-by-step program on a large dataset.
    Though the accuracy values reported were low, the domains spanned anywhere between
    probability to relative velocity, and the unified framework demonstrated performing
    meaningful analysis through qualitative illustrations. This was improved upon
    by Amini et al. ([2019](#bib.bib1)), which enhanced the dataset and added domain
    information through a label on the category. The SAU-Solver Qin et al. ([2020](#bib.bib51))
    introduced a tree like representation with semantic elements that align to the
    word problem. As seen in Table  [6](#Sx8.T6 "Table 6 ‣ Performance of Deep Models
    ‣ Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based
    Word Problem Solvers"), this is a formidable contender. In Chiang and Chen ([2019](#bib.bib5)),
    a novel way of decomposing the equation construction into a set of stack operations
    - such that more nuanced mapping between language and operators can be learned
    - was designed. There is a burgeoning section of the literature that is invested
    in using neuro-symbolic reasoning to bridge this gap between perception level
    tasks (language understanding) and cognitive level tasks (mathematical reasoning).
    An example of this is Qin et al. ([2021](#bib.bib50)). With this discussion, it
    is clear that adding some form of domain knowledge benefits an automatic solver.
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based Solvers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the advent of graph modeling Xia et al. ([2019](#bib.bib74)) and enhanced
    interest in multi-modal processing, the graph data structure became a vehicle
    for adding knowledge to solvers. One way of enabling this has been to simply model
    the input problem as a graph Feng et al. ([2021](#bib.bib12)); Li et al. ([2020](#bib.bib33));
    Yu et al. ([2021](#bib.bib76)); Hong et al. ([2021](#bib.bib17)). This incorporates
    domain knowledge of (a) language interactions pertinent to mathematical reasoning,
    or (b) quantity graphs stating how various numerals in the text are connected.
    Another way is to model the decoder side to accept graphical input of equations
    Xie and Sun ([2019](#bib.bib75)); Lin et al. ([2021](#bib.bib37)); Zaporojets
    et al. ([2021](#bib.bib77)); Cao et al. ([2021](#bib.bib3)); Liu et al. ([2019](#bib.bib40));
    Wu et al. ([2021b](#bib.bib72)). Another natural pathway that has been employed
    towards leveraging graphs is to use graph neural networks for both encoder and
    decoder Zhang et al. ([2020c](#bib.bib80)); Wu et al. ([2020](#bib.bib70), [2021a](#bib.bib71));
    Shen and Jin ([2020](#bib.bib58)).
  prefs: []
  type: TYPE_NORMAL
- en: <svg   height="198.51"
    overflow="visible" version="1.1" width="258.25"><g transform="translate(0,198.51)
    matrix(1 0 0 -1 0 0) translate(54.72,0) translate(0,178.55)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -38.31 -3.46)" fill="#000000"
    stroke="#000000"><foreignobject width="76.62" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Graph Input</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 96.73 -3.38)" fill="#000000" stroke="#000000"><foreignobject width="104.16"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Output Sequence</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -46.51 -82.68)" fill="#000000" stroke="#000000"><foreignobject
    width="93.02" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Input
    Sequence</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 104.93 -82.75)"
    fill="#000000" stroke="#000000"><foreignobject width="87.77" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Graph Output</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -38.31 -162.05)" fill="#000000" stroke="#000000"><foreignobject width="76.62"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Graph Input</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 104.93 -162.05)" fill="#000000" stroke="#000000"><foreignobject
    width="87.77" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Graph
    Output</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: General Graph based Formulations'
  prefs: []
  type: TYPE_NORMAL
- en: Graphs are capable of representing complex relationships. With the time-tested
    success of graph neural networks (GNNs) Wu et al. ([2021c](#bib.bib73)), they
    fit easily into the encoder-decoder architecture. Intuitively, when graphs are
    used on the input side, we can model complex semantic relationships in the linguistic
    side of the task. When graphs are used on the decoder side, relationships between
    the numerical entities or an intermediate representation of the problem can be
    captured. Analogously, graph-to-graph modelling enables matching the semantics
    of both language and math. This does not necessarily imply graph-to-graph outperforms
    all the other formulations. There are unique pros and cons of each of the graph-based
    papers, as both language and mathematical models are hard to (a) model separately
    and (b) model the interactions. The interesting observation as seen in Table  [6](#Sx8.T6
    "Table 6 ‣ Performance of Deep Models ‣ Why are NLP Models Fumbling at Elementary
    Math? A Survey of Deep Learning based Word Problem Solvers"), graph based models
    are both popular and powerful. Unlike sequences, when the input text is represented
    as a graph, the focus is more on relevant entities rather than a stream of text.
    Similarly quantity graphs or semantics informed graphs, eliminate ordering ambiguities
    in equations. This formulation, however, still does not address the multiple equation
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers Vaswani et al. ([2017](#bib.bib66)) have lately revolutionised
    the field of NLP. Word problem solving has been no exception. Through the use
    of BERT Devlin et al. ([2018](#bib.bib9)) embeddings or through transformer based
    encoder-decoder models, some recent research has leveraged concepts from transformer
    models Liu et al. ([2019](#bib.bib40)); Kim et al. ([2020](#bib.bib22)). The translation
    has been modeled variously, such as from text to explanation Piękos et al. ([2021](#bib.bib49));
    Griffith and Kalita ([2020](#bib.bib15)), or from text to equation Shen et al.
    ([2021](#bib.bib57)); Liang et al. ([2021](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: When moving from Word2Vec Mikolov et al. ([2013](#bib.bib43)) vectors to BERT
    embeddings Devlin et al. ([2018](#bib.bib9)), massive gains were expected due
    to (a) greater incorporation of context level information and (b) automatic capturing
    of relevant information as BERT is essentially a Masked Language Model. Interestingly,
    the gains do not have as large a margin as seen in other language tasks such as
    question answering or machine translation Devlin et al. ([2018](#bib.bib9)). BERT
    is a large model that needs to be fine tuned with domain specific information.
    The small gains point towards low quality of word problem datasets, which is in
    line with the fact that the datasets are either quite small by deep learning standards
    or that they have high lexical overlap, effectively suggesting that the set of
    characteristic word problems are small.
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive Solvers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the widespread usage of Siamese networks Koch et al. ([2015](#bib.bib23)),
    the idea of building representations that contrast between vectorial representations
    across classes in data has seen some interest. In the context of word problem
    solving, a few bespoke transformer based encoder-decoder models Li et al. ([2021b](#bib.bib34));
    Hong et al. ([2021](#bib.bib17)) have been proposed; these seek to effectively
    leverage contrastive learning Le-Khac et al. ([2020](#bib.bib30)).
  prefs: []
  type: TYPE_NORMAL
- en: This is a relatively new paradigm and more research needs to emerge to ascertain
    definite trends. One of the main stumbling blocks of word problem solving is that
    two highly linguistically similar looking word problems may have entirely different
    mathematical structure. Since contrastive learning is built on the principle that
    similar input examples lead to closer representations, it allows one to use the
    notion of similarity and dissimilarity to overcome this bottleneck and consciously
    design semantically informed intermediate representations, such that the similarity
    is built not only from the language vocabulary, but also from the mathematical
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Teacher-Student Solvers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The paradigm of knowledge distillation, in the wake of large, generic end-to-end
    models, has become popular in NLP Li et al. ([2021a](#bib.bib32)). The underlying
    idea behind this is to distill smaller task-specific models from a generic large
    pre-trained or generic model. Since word problem datasets are of comparatively
    smaller size, it is but logical that large generic networks can be fine-tuned
    for downstream processing of word problem solving, as favourably demonstrated
    by Zhang et al. ([2020b](#bib.bib79)) and Hong et al. ([2021](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: Once again, this is an emerging paradigm. Similar to the discussion we presented
    with transformer based models, the fact that the presence of pre-trained language
    models alone is not sufficient for this task has bolstered initial efforts in
    this direction. Knowledge distillation enables a model to focus the learnings
    of one generic model on to a smaller, more focussed one, especially with less
    datapoints. Hence, the method of adding semantic information through the usage
    of knowledge distillation algorithms is promising and one to look out for.
  prefs: []
  type: TYPE_NORMAL
- en: Domain-Niche Solvers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some research, encompassing families of statistical solvers and deep models,
    focus on the pertinent characteristics of a particular domain in mathematics,
    such as probability word problems Dries et al. ([2017](#bib.bib10)); Suster et al.
    ([2021](#bib.bib62)); Tsai et al. ([2021](#bib.bib64)), number theory word problems
    Shi et al. ([2015](#bib.bib59)), geometry word problems Seo et al. ([2015](#bib.bib56));
    Chen et al. ([2021](#bib.bib4)) and age word problems Sundaram and Abraham ([2019](#bib.bib60)).
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Datasets used for math word problem solving are listed in Table [4](#Sx6.T4
    "Table 4 ‣ Datasets ‣ Why are NLP Models Fumbling at Elementary Math? A Survey
    of Deep Learning based Word Problem Solvers") with their characteristics. The
    top section of the table describes datasets with relatively fewer data objects
    ($\leq 1k$, to be specific). The bottom half consists of more recent datasets
    that are larger and more popularly used within deep learning methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Type | Domain | Size | Source |'
  prefs: []
  type: TYPE_TB
- en: '| Alg514 | Multi-equation | (+,-,*,/) | 514 | Kushman et al. ([2014](#bib.bib27))
    |'
  prefs: []
  type: TYPE_TB
- en: '| (SimulEq-S) |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| AddSub | Single-equation | (+,-) | 340 | Hosseini et al. ([2014](#bib.bib18))
    |'
  prefs: []
  type: TYPE_TB
- en: '| (AI2) |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SingleOp | Single-equation | (+,-,*,/) | 562 | Roy et al. ([2015](#bib.bib55))
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Illinois, IL) |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SingleEq | Single-equation | (+,-,*,/) | 508 | Koncel-Kedziorski et al. ([2015](#bib.bib24))
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAWPS | Multi-equation | (+,-,*,/) | 3320 | Koncel-Kedziorski et al. ([2016](#bib.bib25))
    |'
  prefs: []
  type: TYPE_TB
- en: '| MultiArith | Single-equation | (+,-,*,/) | 600 | Roy and Roth ([2015](#bib.bib52))
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Common Core, CC) |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| AllArith | Single-equation | (+,-,*,/) | 831 | Roy and Roth ([2017](#bib.bib53))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Perturb | Single-equation | (+,-,*,/) | 661 | Roy and Roth ([2017](#bib.bib53))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Aggregate | Single-equation | (+,-,*,/) | 1492 | Roy and Roth ([2017](#bib.bib53))
    |'
  prefs: []
  type: TYPE_TB
- en: '| DRAW-1k | Multi-equation | (+,-,*,/) | 1k | Upadhyay and Chang ([2017](#bib.bib65))
    |'
  prefs: []
  type: TYPE_TB
- en: '| AsDIV-A | Single-equation | (+,-,*,/) | 2373 | Miao et al. ([2020](#bib.bib42))
    |'
  prefs: []
  type: TYPE_TB
- en: '| SVAMP | Single-equation | (+,-,*,/) | 1000 | Patel et al. ([2021](#bib.bib46))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dolphin18k | Multi-equation | (+,-,*,/) | 18k | Huang et al. ([2016](#bib.bib20))
    |'
  prefs: []
  type: TYPE_TB
- en: '| AQuA-RAT | Multiple-choice | - | 100k | Ling et al. ([2017](#bib.bib38))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Math23k* | Single-equation | (+,-,*,/) | 23k | Huang et al. ([2017](#bib.bib19))
    |'
  prefs: []
  type: TYPE_TB
- en: '| MathQA | Single-equation | (+,-,*,/) | 35k | Amini et al. ([2019](#bib.bib1))
    |'
  prefs: []
  type: TYPE_TB
- en: '| HMWP* | Multi-equation | (+,-,*,/) | 5k | Qin et al. ([2020](#bib.bib51))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ape210k* | Single-equation | (+,-,*,/) | 210k | Liang et al. ([2021](#bib.bib35))
    |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8k | Single-equation | (+,-,*,/) | 8.5k | Cobbe et al. ([2021](#bib.bib7))
    |'
  prefs: []
  type: TYPE_TB
- en: '| CM17k* | Multi-equation | (+,-,*,/) | 17k | Qin et al. ([2021](#bib.bib50))
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Datasets'
  prefs: []
  type: TYPE_NORMAL
- en: (*Chinese Datasets)
  prefs: []
  type: TYPE_NORMAL
- en: Small Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pioneering work in solving word problems Kushman et al. ([2014](#bib.bib27)),
    introduced a classical dataset (Alg514) of 514 word problems, across various domains
    in algebra (such as percentages, mixtures, speeds etc). This dataset was annotated
    with multiple equations per problem. AddSub was introduced in Hosseini et al.
    ([2014](#bib.bib18)), with simple addition/subtraction problems, exhibiting limited
    language complexity. SingleOp Roy et al. ([2015](#bib.bib55)) and MultiArith Roy
    and Roth ([2015](#bib.bib52)) were proposed such that there is a control over
    the operators (single operator in the former and two operators in the latter).
    SingleEq Koncel-Kedziorski et al. ([2015](#bib.bib24)) is unique in incorporating
    long sentence structures for elementary level school problems. AllArith Roy and
    Roth ([2017](#bib.bib53)) is a subset of the union of AddSub, SingleEq and SingleOp.
    "Perturb" is a set of slightly perturbed word problems of AllArith, whereas Aggregate
    is the union of AllArith and Perturb. MAWPS (A Math Word Problem Solving Repository)
    Koncel-Kedziorski et al. ([2016](#bib.bib25)) is a curated dataset (with deliberate
    template overlap control) that comprises all proposed datasets till that date.
    A single equation subset of MAWPS (AsDIV-A) Miao et al. ([2020](#bib.bib42)) has
    been studied , for diagnostic analysis of solvers. Similarly, the critique offered
    by Patel et al. ([2021](#bib.bib46)) was demonstrated using their newly proposed
    dataset SVAMP. In SVAMP, minutely perturbed word problems from the popular dataset
    AsDIV-A. This particular subset is used to demonstrate that, while high values
    of accuracy can be obtained on AsDIV-A easily, SVAMP poses a formidable challenge
    to most solvers, as it captures nuances in the relationship between similar language
    formation and dissimilar equations. All aforementioned datasets incorporate an
    annotation of both the equation and the answer. Given the subset-superset relationships
    between some of these datasets, empirical usage of these datasets would need to
    ensure careful sampling to creating subsets for training, testing and cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Large Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dolphin18k Huang et al. ([2016](#bib.bib20)) is an early proprietary dataset
    that was evaluated primarily with the statistical solvers. AQuA-RAT Ling et al.
    ([2017](#bib.bib38)) introduced the first large crowd-sourced dataset for word
    problems with rationales or explanations. This makes the setting quite different
    from the aforementioned datasets, not only with respect to size, but also in the
    wide variety of domain areas (spanning physics, algebra, geometry, probability
    etc). Another point of difference is that the annotation involves the entire textual
    explanation, rather than just the equations. MathQA Amini et al. ([2019](#bib.bib1))
    critically analysed AQuA-RAT and selected the core subset and annotated it with
    a predicate list, to widen the remit of its usage. Once again, researchers must
    be mindful of the fact that MathQA is a subset of AQuA-RAT. GSM8k Cobbe et al.
    ([2021](#bib.bib7)) is a recent single-equation dataset, that is the large scale
    version of AsDIV-A Miao et al. ([2020](#bib.bib42)). Math23K is a popular Chinese
    dataset for single equation math word problem solving. A recent successor is Ape210k
    Liang et al. ([2021](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most popular metric is answer accuracy, which evaluates the predicted equation
    and checks whether it is the same as the labelled one. The other metric is equation
    accuracy, which predominantly does string matching and assesses the match between
    the produced equation and the equation from the annotation label.
  prefs: []
  type: TYPE_NORMAL
- en: Performance of Deep Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the performance of neural solvers towards providing
    the reader with a high-level view of the comparative performance across the several
    proposed models.
  prefs: []
  type: TYPE_NORMAL
- en: We have listed the performance of the deep models in Table  [6](#Sx8.T6 "Table
    6 ‣ Performance of Deep Models ‣ Why are NLP Models Fumbling at Elementary Math?
    A Survey of Deep Learning based Word Problem Solvers"), on two major datasets
    - Math23K and MAWPS.
  prefs: []
  type: TYPE_NORMAL
- en: Some of these deep models report scores on other datasets as well. For conciseness,
    we have chosen the most popular datasets for deep models. We see that, in general,
    the models achieve around 70-80 percentage points on answer accuracy. Shen et al.
    ([2021](#bib.bib57)) outperforms all other models on Math23k whereas RPKHS Yu
    et al. ([2021](#bib.bib76)) is the best model for MAWPS till date. As mentioned
    before, graph based models are both popular and effective. A note of caution is
    that, as inferred from the discussion on datasets, (a) both Math23k and MAWPS
    are single equation datasets and (b) though some lexical overlap has been performed
    in the design of these two datasets, the semantic quality of these datasets are
    quite similar. This aspect has also been experimented and explored in Patel et al.
    ([2021](#bib.bib46)). Hence, though we present the best performing algorithms
    in this table, more research is required to design a suitable metric or a suitable
    dataset, such that one can conclusively compare these various algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Type | AQuA-RAT | MathQA | Source |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AQuA | Seq2Seq | 36.4 | - | Ling et al. ([2017](#bib.bib38)) |'
  prefs: []
  type: TYPE_TB
- en: '| Seq2Prog | Seq2Seq | 37.9 | 57.2 | Amini et al. ([2019](#bib.bib1)) |'
  prefs: []
  type: TYPE_TB
- en: '| BERT-NPROP | Transformer | 37.0 | - | Piękos et al. ([2021](#bib.bib49))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-To-Tree | Graph-based | - | 69.65 | Li et al. ([2020](#bib.bib33))
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Performance on Large Multi-Domain Datasets'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these algebraic datasets, multi-domain datasets MathQA and AquA are
    also of special interest. This is described in Table  [5](#Sx8.T5 "Table 5 ‣ Performance
    of Deep Models ‣ Why are NLP Models Fumbling at Elementary Math? A Survey of Deep
    Learning based Word Problem Solvers"). The interesting takeaway is that, the addition
    of BERT modelling to AQuA Piękos et al. ([2021](#bib.bib49)), still performed
    slightly worse than the Seq2Prog Amini et al. ([2019](#bib.bib1)) model, which
    is a derivative of the Seq2Seq paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Name | Type | Math23k | MAWPS | Source |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GTS | Graph-based | 74.3 | - | Xie and Sun ([2019](#bib.bib75)) |'
  prefs: []
  type: TYPE_TB
- en: '| SAU-SOLVER | Graph-based | 74.8 | - | Chiang and Chen ([2019](#bib.bib5))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Group-att | Transformer | 69.5 | 76.1 | Li et al. ([2019](#bib.bib31)) |'
  prefs: []
  type: TYPE_TB
- en: '| Graph2Tree | Graph-based | 77.4 | - | Li et al. ([2020](#bib.bib33)) |'
  prefs: []
  type: TYPE_TB
- en: '| KA-S2T | Graph-based | 76.3 | - | Wu et al. ([2020](#bib.bib70)) |'
  prefs: []
  type: TYPE_TB
- en: '| NS-Solver | Seq2Seq | 75.67 | - | Qin et al. ([2020](#bib.bib51)) |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-To-Tree | Graph-based | 78.8 | - | Li et al. ([2020](#bib.bib33)) |'
  prefs: []
  type: TYPE_TB
- en: '| TSN-MD | Teacher Student | 77.4 | 84.4 | Zhang et al. ([2020b](#bib.bib79))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-Teacher | Graph & Teacher | 79.1 | 84.2 | Liang and Zhang ([2021](#bib.bib36))
    |'
  prefs: []
  type: TYPE_TB
- en: '| NumS2T | Graph-based | 78.1 | - | Wu et al. ([2020](#bib.bib70)) |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-E/D | Graph-based | 78.4 | - | Shen and Jin ([2020](#bib.bib58)) |'
  prefs: []
  type: TYPE_TB
- en: '| EPT | Transformer | - | 84.5 | Kim et al. ([2020](#bib.bib22)) |'
  prefs: []
  type: TYPE_TB
- en: '| Seq2DAG | Graph-based | 77.1 | - | Cao et al. ([2021](#bib.bib3)) |'
  prefs: []
  type: TYPE_TB
- en: '| EEH-D2T | Graph-based | 78.5 | 84.8 | Wu et al. ([2021a](#bib.bib71)) |'
  prefs: []
  type: TYPE_TB
- en: '| Generate and Rank | Graph-based | 85.4 | 84.0 | Shen et al. ([2021](#bib.bib57))
    |'
  prefs: []
  type: TYPE_TB
- en: '| HMS | Graph-based | 76.1 | 80.3 | Lin et al. ([2021](#bib.bib37)) |'
  prefs: []
  type: TYPE_TB
- en: '| RPKHS | Graph-based | 83.9 | 89.8 | Yu et al. ([2021](#bib.bib76)) |'
  prefs: []
  type: TYPE_TB
- en: '| CL | Contrastive Learning | 83.2 | - | Li et al. ([2021b](#bib.bib34)) |'
  prefs: []
  type: TYPE_TB
- en: '| GTS+RODA | Graph-based | 77.9 | - | Liu et al. ([2022](#bib.bib39)) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Answer Accuracy of Deep Models'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of Deep Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section of the paper, we analyze the pros and cons of applying deep
    learning techniques to solve word problems automatically. At the outset, two layers
    of understanding are imperative: (i) linguistic structures that describe a situation
    or a sequence of events and (ii) mathematical structures that govern these language
    descriptions. Though deep learning models have rapidly scaled and demonstrated
    commendable results for capturing these two characteristics, a closer look reveals
    much potential for further exploration. The predominant modus-operandus is to
    create a deep model that converts the input natural language to the underlying
    equation. In some cases, the input is converted into a set of predicates Amini
    et al. ([2019](#bib.bib1)) or explanations Ling et al. ([2017](#bib.bib38)).'
  prefs: []
  type: TYPE_NORMAL
- en: What Shortcuts are being Learned?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shortcut Learning Geirhos et al. ([2020](#bib.bib14)) is a recently well-studied
    phenomenon of deep neural networks. It describes how deep learning models learn
    patterns in a shallow way and fall prey to questionable generalizations across
    datasets (an example is an image being classified as sheep if there was grass
    alone; due to peculiarities in the dataset).This is a function of the low-level
    input we provide to such models (pixels, word embeddings etc.). In the context
    of word problems, Patel et al. ([2021](#bib.bib46)) exposed how removing the question
    and simply passing the situational context, leads to the correct equation being
    predicted. This suggests two things, issues with model design as well as issues
    with dataset design. The datasets have high equation template overlap, as well
    as text overlap. Word problem solving is a hard because two otherwise identical
    word problems, with a small word change (say changing the word give to take),
    would completely change the equation. Hence high lexical similarity does not translate
    to corresponding similarity in the mathematical realm Patel et al. ([2021](#bib.bib46));
    Sundaram et al. ([2020](#bib.bib61)), and attention to key aspects within the
    text is critical.
  prefs: []
  type: TYPE_NORMAL
- en: Is Language or Math being Learned?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Problem | Solved? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| John has 5 apples. Mary has 2 apples more than John. How many apples does
    Mary have? | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| John has 5 apples. Mary has 2 apples more than John. Who has less apples?
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| What should be added to two to make it five? | No |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Behaviour of Baseline BERT Model'
  prefs: []
  type: TYPE_NORMAL
- en: The question that looms large is whether adequate mapping of language to math
    has been modelled, whether linguistic modelling has been unfavourably highlighted
    or that the mathematical aspects have been captured succinctly. We observe that
    there are opportunities to refine the modelling of both language and math aspects
    of word problems. Apart from the perturbations experiment done by SVAMP Patel
    et al. ([2021](#bib.bib46)), which exposes that the mapping between linguistic
    and mathematical structures is not captured, we suggest two more experimental
    analysis frameworks that illustrate deficiencies in linguistic and mathematical
    modelling. The first one involves imposing a question answering task on top of
    the word problem as a probing test. For example, a baseline BERT model that converts
    from input language to equation (Table  [7](#Sx9.T7 "Table 7 ‣ Is Language or
    Math being Learned? ‣ Analysis of Deep Models ‣ Why are NLP Models Fumbling at
    Elementary Math? A Survey of Deep Learning based Word Problem Solvers")), trained
    on MAWPS, can solve a simple word problem such as "John has 5 apples. Mary has
    2 apples more than John. How many apples does Mary have?", but cannot answer the
    following allied question "John has 5 apples. Mary has 2 apples more than John.
    Who has less apples?". One reason is of course, dataset design. The governing
    equation for this problem is "X = 5-2". However, the text version of this, "What
    should be added to two to make it five?", cannot be solved by the baseline model.
    Similarly, many solvers wrongly output equations such as "X = 2 - 5" Patel et al.
    ([2021](#bib.bib46)), which suggests mathematical modelling of subtraction of
    whole numbers could potentially be improved by simply embedding more basic mathematical
    aspects. Hence, we observe, that deep translation models neither model language,
    nor the math sufficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Is Accuracy Enough?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As suggested by the discussion above, a natural line of investigation is to
    examine the evaluation measures, and perhaps the error measures for the deep models,
    in order to bring about a closer coupling between syntax and semantics. High accuracy
    of the models to predicting the answer or the equation suggests a shallow mapping
    between the text and the mathematical symbols. This is analogous to the famously
    observed McNamara fallacy²²2[https://en.wikipedia.org/wiki/McNamara_fallacy](https://en.wikipedia.org/wiki/McNamara_fallacy),
    which cautions against the overuse of a single metric to evaluate a complex problem.
    One direction of exploration is data augmentation with a single word problem annotated
    with multiple equivalent equations. Metrics that measure the soundness of the
    equations generated, the robustness of the model to simple perturbations (perhaps
    achieved using a denoising autoencoder) and the ability of the model to discern
    important entities in a word problem (perhaps using an attention analysis based
    metric), are the need of the future. An endeavour has been done by Kumar et al.
    ([2021](#bib.bib26)), where adversarial examples have been generated and utilised
    to evaluate SOTA models.
  prefs: []
  type: TYPE_NORMAL
- en: Are the Trained Models Accessible?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the SOTA systems come with their own, well-documented repositories.
    Though an aggregated toolkit Lan et al. ([2021](#bib.bib28)) (open-source MIT
    License) is available, running saved models in inference mode, to probe the quality
    of the datasets, proved to be a hard task, with varying missing hyper-parameters
    or missing saved models. This, however, interestingly suggests that API’s that
    can take a single word problem as input and computes the output, would be highly
    useful for application designers. This has been done in the earlier systems such
    as Roy and Roth ([2018](#bib.bib54)) and Wolfram ([2015](#bib.bib69)).
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of Benchmark Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section of the paper, we explore the various dimensions of the popular
    datasets (Table  [4](#Sx6.T4 "Table 4 ‣ Datasets ‣ Why are NLP Models Fumbling
    at Elementary Math? A Survey of Deep Learning based Word Problem Solvers")) with
    a critical and constructive perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Low Resource Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to usual text related tasks, the available datasets are quite small
    in size. They also suffer from a large lexical overlap Amini et al. ([2019](#bib.bib1)).
    This taxes algorithms, that now have to generalise from an effectively small dataset.
    The fact that the field of word problem solving is niche, where we cannot simply
    lift text from generic sources like Wikipedia, is one of the primary reasons why
    these datasets are small. Language precision is required, while maintaining mathematical
    sense. Hence, language generation is also a hard task.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation Cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The datasets currently have little to no annotation costs involved as they are
    usually scraped from homework websites. There are some exceptions that involve
    crowd-sourcing Ling et al. ([2017](#bib.bib38)) or intermediate representations
    apart from equations Amini et al. ([2019](#bib.bib1)).
  prefs: []
  type: TYPE_NORMAL
- en: Template Overlap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many studies Zhang et al. ([2020a](#bib.bib78)) have demonstrated that there
    is a high lexical and mathematical overlap between the word problems in popular
    datasets. While lexical overlap is desirable in a principled fashion, as demonstrated
    by Patel et al. ([2021](#bib.bib46)), it often limits the diversity and thus utility
    of the datasets. Consequently, many strategies have been adopted to mitigate such
    issues. Early attempts include controlling linguistic and equation template overlap
    (Koncel-Kedziorski et al. ([2016](#bib.bib25)), Miao et al. ([2020](#bib.bib42))).
    Later ideas revolve around controlled design and quality control of crowd-sourcing
    Amini et al. ([2019](#bib.bib1)).
  prefs: []
  type: TYPE_NORMAL
- en: Road Ahead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe exciting frontiers of research for word problem
    solving algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Parsing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As rightly suggested by Zhang et al. ([2020a](#bib.bib78)), the closest natural
    language task for word problem solving is that of semantic parsing, and not translation
    as most of the deep learning models have modelled. The mapping between extremely
    long chunks of text to short equation sentences has the advantage of generalising
    on the decoder side, but equally has the danger of overloading many involved semantics
    into a simplistic equation model. To illustrate, an equation may be derived after
    applying a sequence of steps that is lost in a simple translation process. A lot
    of efforts have already been employed in adding such nuances in the modelling.
    One way is to model the input intelligently (for e.g., Liang et al. ([2021](#bib.bib35)))
    Here, sophisticated embeddings are learned from BERT based models, using the word
    problem text as a training bed. The intermediate representations include simple
    predicates Roy and Roth ([2018](#bib.bib54)), while others involve a programmatic
    description (Ling et al. ([2017](#bib.bib38)), Amini et al. ([2019](#bib.bib1))).
    Yet another way is to include semantic information in the form of graphs as shown
    in (Huang et al. ([2018](#bib.bib21)), Chiang and Chen ([2019](#bib.bib5)), Qin
    et al. ([2020](#bib.bib51)), Li et al. ([2020](#bib.bib33)), etc.)).
  prefs: []
  type: TYPE_NORMAL
- en: Informed Dataset Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As most datasets are sourced from websites, there is bound to be repetition.
    Efforts invested in modelling things such as the following could help aiding word
    problem research: (a) different versions of the same problem, (b) different equivalent
    equation types, (c) semantics of the language and the math. A step in this direction
    has been explored by Patel et al. ([2021](#bib.bib46)), which provides a challenge
    dataset for evaluating word problems, and Kumar et al. ([2021](#bib.bib26)) where
    adversarial examples are automatically generated.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A natural extension of dataset design, is dataset augmentation. Augmentation
    is a natural choice when we have datasets that are small and focused on a single
    domain. Then, linguistic and mathematical augmentation can be automated by domain
    experts. While template overlap is a concern in dataset design, it can be leveraged
    in contrastive designs as in Sundaram et al. ([2020](#bib.bib61)); Li et al. ([2021b](#bib.bib34)).
    A principled approach of reversing operators and building equivalent expression
    trees for augmentation has been explored here Liu et al. ([2022](#bib.bib39)).
  prefs: []
  type: TYPE_NORMAL
- en: Few Shot Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is useful if we have a large number of non-annotated word problems or if
    we can come up with complex annotations (that capture semantics) for a small set
    of word problems. In this way few shot learning can generalise from few annotated
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Aware Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We propose that word problem solving is more involved than even semantic parsing.
    From an intuitive space, we learn language from examples and interactions but
    we need to be explicitly trained in math to solve word problems Marshall ([1996](#bib.bib41)).
    This suggests we need to include mathematical models into our deep learning models
    to build generalisability and robustness. As mentioned before, a common approach
    is to include domain knowledge as a graph Chiang and Chen ([2019](#bib.bib5));
    Wu et al. ([2020](#bib.bib70)); Qin et al. ([2020](#bib.bib51), [2021](#bib.bib50)).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we surveyed the existing math word problem solvers, with a focus
    on deep learning models. Deep models are predominantly modeled as encoder-decoder
    models, with input as text and decoder output as equations. We listed several
    interesting formulations of this paradigm - namely Seq2Seq models, graph-based
    models, transformer-based models, contrastive models and teacher-student models.
    In general, graph based models tend to capture complex structural elements that
    can benefit both linguistic and mathematical aspects. We then explored in detail
    the various datasets in use. Subsequently, we analysed the various approaches
    of modelling word problem solving, followed by the characteristics of the popular
    datasets. We saw an overwhelming trend that paying heed to the mathematical modelling
    and tying to the linguistic aspects reaped rich dividends. We concluded that the
    brittleness of the SOTA models was due to: (a) modelling decisions, and (b) dataset
    design. This is intended as a comprehensive survey, but the authors acknowledge
    that there may be methods that have escaped their attention. We also caution that
    the analysis provided could be subjective and opinionated, and there could be
    legitimate disagreements with the perspectives put forward. Finally, we mentioned
    few avenues of further exploration such as the use of semantically rich models,
    informed dataset design and incorporation of domain knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amini et al. (2019) Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R.,
    Choi, Y., and Hajishirzi, H. (2019). MathQA: Towards interpretable math word problem
    solving with operation-based formalisms. In Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357–2367,
    Minneapolis, Minnesota. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bobrow (1964) Bobrow, D. G. (1964). A question-answering system for high school
    algebra word problems. In Proceedings of the October 27-29, 1964, fall joint computer
    conference, part I, pages 591–614\. ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2021) Cao, Y., Hong, F., Li, H., and Luo, P. (2021). A bottom-up
    dag structure extraction model for math word problems. Proceedings of the AAAI
    Conference on Artificial Intelligence, 35(1):39–46.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Chen, J., Tang, J., Qin, J., Liang, X., Liu, L., Xing, E.,
    and Lin, L. (2021). GeoQA: A geometric question answering benchmark towards multimodal
    numerical reasoning. In Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021, pages 513–523, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang and Chen (2019) Chiang, T.-R. and Chen, Y.-N. (2019). Semantically-aligned
    equation generation for solving and reasoning math word problems. In Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages
    2656–2668, Minneapolis, Minnesota. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2014) Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D.,
    Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning phrase representations
    using RNN encoder–decoder for statistical machine translation. In Proceedings
    of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pages 1724–1734, Doha, Qatar. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano,
    R., Hesse, C., and Schulman, J. (2021). Training verifiers to solve math word
    problems. CoRR, abs/2110.14168.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dellarosa (1986) Dellarosa, D. (1986). A computer simulation of children’s arithmetic
    word-problem solving. Behavior Research Methods, Instruments, & Computers, 18(2):147–154.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018).
    BERT: pre-training of deep bidirectional transformers for language understanding.
    CoRR, abs/1810.04805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dries et al. (2017) Dries, A., Kimmig, A., Davis, J., Belle, V., and De Raedt,
    L. (2017). Solving probability problems in natural language. Proceedings Twenty-Sixth
    International Joint Conference on Artificial Intelligence, pages 3981–3987.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Faldu et al. (2021) Faldu, K., Sheth, A. P., Kikani, P., Gaur, M., and Avasthi,
    A. (2021). Towards tractable mathematical reasoning: Challenges, strategies, and
    opportunities for solving math word problems. CoRR, abs/2111.05364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2021) Feng, W., Liu, B., Xu, D., Zheng, Q., and Xu, Y. (2021).
    GraphMR: Graph neural network for mathematical reasoning. In Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, pages 3395–3404,
    Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fletcher (1985) Fletcher, C. R. (1985). Understanding and solving arithmetic
    word problems: A computer simulation. Behavior Research Methods, Instruments,
    & Computers, 17(5):565–571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geirhos et al. (2020) Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,
    Brendel, W., Bethge, M., and Wichmann, F. A. (2020). Shortcut learning in deep
    neural networks. Nature Machine Intelligence, 2(11):665–673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Griffith and Kalita (2020) Griffith, K. and Kalita, J. (2020). Solving arithmetic
    word problems using transformer and pre-processing of problem texts. In Proceedings
    of the 17th International Conference on Natural Language Processing (ICON), pages
    76–84, Indian Institute of Technology Patna, Patna, India. NLP Association of
    India (NLPAI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. (1997).
    Long short-term memory. Neural Comput., 9(8):1735–1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. (2021) Hong, Y., Li, Q., Ciao, D., Huang, S., and Zhu, S.-C. (2021).
    Learning by fixing: Solving math word problems with weak supervision. Proceedings
    of the AAAI Conference on Artificial Intelligence, 35(6):4959–4967.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosseini et al. (2014) Hosseini, M. J., Hajishirzi, H., Etzioni, O., and Kushman,
    N. (2014). Learning to solve arithmetic word problems with verb categorization.
    In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pages 523–533.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2017) Huang, D., Shi, S., Lin, C.-Y., and Yin, J. (2017). Learning
    fine-grained expressions to solve math word problems. Proceedings of the 2017
    Conference on Empirical Methods in Natural Language Processing, pages 805–814.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2016) Huang, D., Shi, S., Lin, C.-Y., Yin, J., and Ma, W.-Y.
    (2016). How well do computers solve math word problems? large-scale dataset construction
    and evaluation. In Proceedings of the 54th Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers), pages 887–896, Berlin, Germany.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2018) Huang, D., Yao, J.-G., Lin, C.-Y., Zhou, Q., and Yin, J.
    (2018). Using intermediate representations to solve math word problems. In Proceedings
    of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 419–428, Melbourne, Australia. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2020) Kim, B., Ki, K. S., Lee, D., and Gweon, G. (2020). Point
    to the Expression: Solving Algebraic Word Problems using the Expression-Pointer
    Transformer Model. In Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pages 3768–3779, Online. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koch et al. (2015) Koch, G., Zemel, R., and Salakhutdinov, R. (2015). Siamese
    neural networks for one-shot image recognition. In ICML Deep Learning Workshop,
    volume 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koncel-Kedziorski et al. (2015) Koncel-Kedziorski, R., Hajishirzi, H., Sabharwal,
    A., Etzioni, O., and Ang, S. D. (2015). Parsing algebraic word problems into equations.
    Transactions of the Association for Computational Linguistics, 3:585–597.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koncel-Kedziorski et al. (2016) Koncel-Kedziorski, R., Roy, S., Amini, A.,
    Kushman, N., and Hajishirzi, H. (2016). Mawps: A math word problem repository.
    In Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, pages 1152–1157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar et al. (2021) Kumar, V., Maheshwary, R., and Pudi, V. (2021). Adversarial
    examples for evaluating math word problem solvers. In Findings of the Association
    for Computational Linguistics: EMNLP 2021, pages 2705–2712, Punta Cana, Dominican
    Republic. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kushman et al. (2014) Kushman, N., Artzi, Y., Zettlemoyer, L., and Barzilay,
    R. (2014). Learning to automatically solve algebra word problems. ACL (1), pages
    271–281.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2021) Lan, Y., Wang, L., Zhang, Q., Lan, Y., Dai, B. T., Wang,
    Y., Zhang, D., and Lim, E.-P. (2021). Mwptoolkit: An open-source framework for
    deep learning-based math word problem solvers. arXiv preprint arXiv:2109.00799.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le and Mikolov (2014) Le, Q. and Mikolov, T. (2014). Distributed representations
    of sentences and documents. In International conference on machine learning, pages
    1188–1196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le-Khac et al. (2020) Le-Khac, P. H., Healy, G., and Smeaton, A. F. (2020).
    Contrastive representation learning: A framework and review. IEEE Access, 8:193907–193934.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Li, J., Wang, L., Zhang, J., Wang, Y., Dai, B. T., and Zhang,
    D. (2019). Modeling intra-relation in math word problems with different functional
    multi-head attentions. In Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics, pages 6162–6167, Florence, Italy. Association for
    Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021a) Li, L., Lin, Y., Ren, S., Li, P., Zhou, J., and Sun, X. (2021a).
    Dynamic knowledge distillation for pre-trained language models. In Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    379–389, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Li, S., Wu, L., Feng, S., Xu, F., Xu, F., and Zhong, S. (2020).
    Graph-to-tree neural networks for learning structured input-output translation
    with applications to semantic parsing and math word problem. In Findings of the
    Association for Computational Linguistics: EMNLP 2020, pages 2841–2852, Online.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021b) Li, Z., Zhang, W., Yan, C., Zhou, Q., Li, C., Liu, H., and
    Cao, Y. (2021b). Seeking patterns, not just memorizing procedures: Contrastive
    learning for solving math word problems. CoRR, abs/2110.08464.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021) Liang, Z., Zhang, J., Shao, J., and Zhang, X. (2021). MWP-BERT:
    A strong baseline for math word problems. CoRR, abs/2107.13435.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang and Zhang (2021) Liang, Z. and Zhang, X. (2021). Solving math word problems
    with teacher supervision. In Zhou, Z.-H., editor, Proceedings of the Thirtieth
    International Joint Conference on Artificial Intelligence, IJCAI-21, pages 3522–3528\.
    International Joint Conferences on Artificial Intelligence Organization. Main
    Track.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021) Lin, X., Huang, Z., Zhao, H., Chen, E., Liu, Q., Wang, H.,
    and Wang, S. (2021). Hms: A hierarchical solver with dependency-enhanced understanding
    for math word problem. Proceedings of the AAAI Conference on Artificial Intelligence,
    35(5):4232–4240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. (2017) Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. (2017).
    Program induction by rationale generation: Learning to solve and explain algebraic
    word problems. arXiv preprint arXiv:1705.04146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Liu, Q., Guan, W., Li, S., Cheng, F., Kawahara, D., and Kurohashi,
    S. (2022). Roda: Reverse operation based data augmentation for solving math word
    problems. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Liu, Q., Guan, W., Li, S., and Kawahara, D. (2019). Tree-structured
    decoding for solving math word problems. In Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2370–2379,
    Hong Kong, China. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marshall (1996) Marshall, S. P. (1996). Schemas in Problem Solving. Cambridge
    University Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miao et al. (2020) Miao, S.-y., Liang, C.-C., and Su, K.-Y. (2020). A diverse
    corpus for evaluating and developing English math word problem solvers. In Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pages
    975–984, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S.,
    and Dean, J. (2013). Distributed representations of words and phrases and their
    compositionality. In Advances in neural information processing systems, pages
    3111–3119.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mitra and Baral (2016) Mitra, A. and Baral, C. (2016). Learning to use formulas
    to solve simple arithmetic problems. In Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), pages
    2144–2153, Berlin, Germany. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mukherjee and Garain (2008) Mukherjee, A. and Garain, U. (2008). A review of
    methods for automatic understanding of natural language mathematical problems.
    Artificial Intelligence Review, 29(2):93–122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patel et al. (2021) Patel, A., Bhattamishra, S., and Goyal, N. (2021). Are
    NLP models really able to solve simple math word problems? In Proceedings of the
    2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, pages 2080–2094, Online. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Pennington, J., Socher, R., and Manning, C. (2014).
    Glove: Global vectors for word representation. In Proceedings of the 2014 conference
    on empirical methods in natural language processing (EMNLP), pages 1532–1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
    C., Lee, K., and Zettlemoyer, L. (2018). Deep contextualized word representations.
    In Proc. of NAACL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Piękos et al. (2021) Piękos, P., Malinowski, M., and Michalewski, H. (2021).
    Measuring and improving BERT’s mathematical abilities by predicting the order
    of reasoning. In Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 2: Short Papers), pages 383–394, Online. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2021) Qin, J., Liang, X., Hong, Y., Tang, J., and Lin, L. (2021).
    Neural-symbolic solver for math word problems with auxiliary tasks. In Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers), pages 5870–5881, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2020) Qin, J., Lin, L., Liang, X., Zhang, R., and Lin, L. (2020).
    Semantically-aligned universal tree-structured solver for math word problems.
    In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pages 3780–3789, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy and Roth (2015) Roy, S. and Roth, D. (2015). Solving general arithmetic
    word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural
    Language Processing, pages 1743–1752, Lisbon, Portugal. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy and Roth (2017) Roy, S. and Roth, D. (2017). Unit dependency graph and its
    application to arithmetic word problem solving. In Proceedings of the Thirty-First
    AAAI Conference on Artificial Intelligence, AAAI’17, page 3082–3088\. AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy and Roth (2018) Roy, S. and Roth, D. (2018). Mapping to declarative knowledge
    for word problem solving. Transactions of the Association of Computational Linguistics,
    6:159–172.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy et al. (2015) Roy, S., Vieira, T., and Roth, D. (2015). Reasoning about
    quantities in natural language. Transactions of the Association for Computational
    Linguistics, 3:1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Seo et al. (2015) Seo, M., Hajishirzi, H., Farhadi, A., Etzioni, O., and Malcolm,
    C. (2015). Solving geometry problems: Combining text and diagram interpretation.
    Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
    pages 1466–1476.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2021) Shen, J., Yin, Y., Li, L., Shang, L., Jiang, X., Zhang,
    M., and Liu, Q. (2021). Generate & rank: A multi-task framework for math word
    problems. In Findings of the Association for Computational Linguistics: EMNLP
    2021, pages 2269–2279, Punta Cana, Dominican Republic. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen and Jin (2020) Shen, Y. and Jin, C. (2020). Solving math word problems
    with multi-encoders and multi-decoders. In Proceedings of the 28th International
    Conference on Computational Linguistics, pages 2924–2934, Barcelona, Spain (Online).
    International Committee on Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2015) Shi, S., Wang, Y., Lin, C., Liu, X., and Rui, Y. (2015). Automatically
    solving number word problems by semantic parsing and reasoning. Proceedings of
    the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2015, Lisbon, Portugal, September 17-21, 2015, pages 1132–1142.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sundaram and Abraham (2019) Sundaram, S. S. and Abraham, S. S. (2019). Semantic
    representation for age word problems with schemas. New Generation Computing, 37(4):429–452.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sundaram et al. (2020) Sundaram, S. S., P, D., and Abraham, S. S. (2020). Distributed
    representations for arithmetic word problems. Thirty-Fourth AAAI Conference on
    Artificial Intelligence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suster et al. (2021) Suster, S., Fivez, P., Totis, P., Kimmig, A., Davis, J.,
    de Raedt, L., and Daelemans, W. (2021). Mapping probability word problems to executable
    representations. In Proceedings of the 2021 Conference on Empirical Methods in
    Natural Language Processing, pages 3627–3640, Online and Punta Cana, Dominican
    Republic. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence
    to sequence learning with neural networks. In Proceedings of the 27th International
    Conference on Neural Information Processing Systems - Volume 2, NIPS’14, page
    3104–3112, Cambridge, MA, USA. MIT Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsai et al. (2021) Tsai, S.-h., Liang, C.-C., Wang, H.-M., and Su, K.-Y. (2021).
    Sequence to general tree: Knowledge-guided geometry word problem solving. In Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    2: Short Papers), pages 964–972, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upadhyay and Chang (2017) Upadhyay, S. and Chang, M.-W. (2017). Annotating
    derivations: A new evaluation strategy and dataset for algebra word problems.
    In Proceedings of the 15th Conference of the European Chapter of the Association
    for Computational Linguistics: Volume 1, Long Papers, pages 494–504, Valencia,
    Spain. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you
    need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,
    S., and Garnett, R., editors, Advances in Neural Information Processing Systems,
    volume 30\. Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Wang, L., Zhang, D., Gao, L., Song, J., Guo, L., and Shen,
    H. T. (2018). Mathdqn: Solving arithmetic word problems via deep reinforcement
    learning. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Wang, Y., Liu, X., and Shi, S. (2017). Deep neural solver
    for math word problems. In Proceedings of the 2017 Conference on Empirical Methods
    in Natural Language Processing, pages 845–854.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wolfram (2015) Wolfram, S. (2015). Wolfram|alpha. On the WWW. URL http://www.
    wolframalpha. com.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Wu, Q., Zhang, Q., Fu, J., and Huang, X. (2020). A knowledge-aware
    sequence-to-tree network for math word problem solving. In Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
    7137–7146, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021a) Wu, Q., Zhang, Q., and Wei, Z. (2021a). An edge-enhanced
    hierarchical graph-to-tree network for math word problem solving. In Findings
    of the Association for Computational Linguistics: EMNLP 2021, pages 1473–1482,
    Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021b) Wu, Q., Zhang, Q., Wei, Z., and Huang, X. (2021b). Math word
    problem solving with explicit numerical values. In Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages
    5859–5869, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021c) Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S.
    (2021c). A comprehensive survey on graph neural networks. IEEE Transactions on
    Neural Networks and Learning Systems, 32(1):4–24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia et al. (2019) Xia, M., Huang, G., Liu, L., and Shi, S. (2019). Graph based
    translation memory for neural machine translation. In Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 33, pages 7297–7304.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie and Sun (2019) Xie, Z. and Sun, S. (2019). A goal-driven tree-structured
    neural model for math word problems. In Proceedings of the Twenty-Eighth International
    Joint Conference on Artificial Intelligence, IJCAI-19, pages 5299–5305. International
    Joint Conferences on Artificial Intelligence Organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021) Yu, W., Wen, Y., Zheng, F., and Xiao, N. (2021). Improving
    math word problems with pre-trained knowledge and hierarchical reasoning. In Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    3384–3394, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zaporojets et al. (2021) Zaporojets, K., Bekoulis, G., Deleu, J., Demeester,
    T., and Develder, C. (2021). Solving arithmetic word problems by scoring equations
    with recursive neural networks. Expert Systems with Applications, 174:114704.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Zhang, D., Wang, L., Zhang, L., Dai, B. T., and Shen,
    H. T. (2020a). The gap of semantic parsing: A survey on automatic math word problem
    solvers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(9):2287–2305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Zhang, J., Lee, R. K.-W., Lim, E.-P., Qin, W., Wang, L.,
    Shao, J., and Sun, Q. (2020b). Teacher-student networks with multiple decoders
    for solving math word problem. In Bessiere, C., editor, Proceedings of the Twenty-Ninth
    International Joint Conference on Artificial Intelligence, IJCAI-20, pages 4011–4017\.
    International Joint Conferences on Artificial Intelligence Organization. Main
    track.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020c) Zhang, J., Wang, L., Lee, R. K.-W., Bin, Y., Wang, Y.,
    Shao, J., and Lim, E.-P. (2020c). Graph-to-tree learning for solving math word
    problems. In Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pages 3928–3937, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2015) Zhou, L., Dai, S., and Chen, L. (2015). Learn to solve algebra
    word problems using quadratic programming. In Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing, pages 817–822, Lisbon, Portugal.
    Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
