- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:40:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:40:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2304.08193] Applications of Deep Learning for Top-View Omnidirectional Imaging:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2304.08193] 深度学习在顶视全景成像中的应用：一项调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.08193](https://ar5iv.labs.arxiv.org/html/2304.08193)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2304.08193](https://ar5iv.labs.arxiv.org/html/2304.08193)
- en: 'Applications of Deep Learning for Top-View Omnidirectional Imaging: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在顶视全景成像中的应用：一项调查
- en: Jingrui Yu   Ana Cecilia Perez Grassi   Gangolf Hirtz
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jingrui Yu   Ana Cecilia Perez Grassi   Gangolf Hirtz
- en: Chemnitz University of Technology, Germany
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 德国开姆尼茨工业大学
- en: '{jingrui.yu, ana-cecilia.perez-grassi, g.hirtz}@etit.tu-chemnitz.de'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{jingrui.yu, ana-cecilia.perez-grassi, g.hirtz}@etit.tu-chemnitz.de'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: A large field-of-view fisheye camera allows for capturing a large area with
    minimal numbers of cameras when they are mounted on a high position facing downwards.
    This top-view omnidirectional setup greatly reduces the work and cost for deployment
    compared to traditional solutions with multiple perspective cameras. In recent
    years, deep learning has been widely employed for vision related tasks, including
    for such omnidirectional settings. In this survey, we look at the application
    of deep learning in combination with omnidirectional top-view cameras, including
    the available datasets, human and object detection, human pose estimation, activity
    recognition and other miscellaneous applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大视野鱼眼相机在高位向下安装时能够捕捉广阔的区域，并且所需相机数量较少。相比传统的多视角相机解决方案，这种顶视全景设置大大减少了部署工作和成本。近年来，深度学习已广泛应用于视觉相关任务，包括这种全景设置。在本次调查中，我们探讨了深度学习与全景顶视相机的结合应用，包括可用的数据集、人类和物体检测、人类姿态估计、活动识别以及其他各种应用。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Omnidirectional cameras have the advantage of being able to capture a wide field
    of view (FOV). However, their projection models introduce a large distortion into
    their images. For this reason, computer vision methods developed for perspective
    images are not suitable for omnidirectional ones. In the last decade, computer
    vision has experienced a great advance thanks to the development of deep neural
    networks and the availability of large databases. However, this advance has focused
    almost exclusively on perspective images, both in the development of architectures
    and in the collection and annotation of data. It has not been until recent years
    that deep learning has begun to reach omnidirectional image processing, by collecting
    datasets and adapting existing architectures or developing new ones for this type
    of image.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 全景相机具有捕捉宽广视野（FOV）的优势。然而，它们的投影模型会对图像引入较大的失真。因此，针对透视图像开发的计算机视觉方法不适用于全景图像。在过去的十年里，由于深度神经网络的发展和大型数据库的可用性，计算机视觉取得了巨大的进步。然而，这些进展几乎完全集中在透视图像上，无论是架构的开发还是数据的收集和注释。直到最近几年，深度学习才开始进入全景图像处理领域，通过收集数据集、调整现有架构或为这种类型的图像开发新架构。
- en: Omnidirectionality can be achieved by using catadioptric, dioptric or polydioptric
    cameras. Catadioptric cameras combine a normal camera with a shaped mirror [[66](#bib.bib66),
    [35](#bib.bib35), [101](#bib.bib101)]. This mirror provides omnidirectionality
    as a surround-view, but the camera itself occludes the central part of the image.
    This problem is solved by dioptric cameras, which use a fisheye lens instead of
    a mirror. Finally, polydioptric cameras capture a spherical field of view by combining
    multiple cameras in a setup [[8](#bib.bib8), [54](#bib.bib54)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用反射透镜、透镜或多透镜相机来实现全景视角。反射透镜相机结合了普通相机和一个成形的镜子[[66](#bib.bib66), [35](#bib.bib35),
    [101](#bib.bib101)]。这个镜子提供了全景视角，但相机本身遮挡了图像的中央部分。这个问题由透镜相机解决，透镜相机使用鱼眼镜头而不是镜子。最后，多透镜相机通过结合多个相机在一个设置中捕捉球形视角[[8](#bib.bib8),
    [54](#bib.bib54)]。
- en: Especially dioptric cameras are gaining attention in many applications because
    they are simple and inexpensive. Depending on the task, these cameras can be mounted
    with a frontal view, as for example in driving applications [[72](#bib.bib72),
    [104](#bib.bib104), [106](#bib.bib106), [51](#bib.bib51)], with a vertical view
    as in teleconference applications [[70](#bib.bib70)] or with a top view as in
    surveillance applications [[56](#bib.bib56), [67](#bib.bib67), [64](#bib.bib64),
    [52](#bib.bib52)]. Also, their use for 3D-reconstruction, using one or more cameras,
    increases in the recent years [[82](#bib.bib82), [55](#bib.bib55), [102](#bib.bib102)].
    In this survey we focus on deep learning algorithms developed for fisheye images
    captured from a top view. This kind of images are essential in surveillance and
    Ambient Assisted Living (AAL) applications [[84](#bib.bib84)], where the main
    research areas include person and object detection and human pose estimation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是折射相机在许多应用中引起了关注，因为它们简单且价格便宜。根据任务的不同，这些相机可以以正面视图安装，例如在驾驶应用中[[72](#bib.bib72),
    [104](#bib.bib104), [106](#bib.bib106), [51](#bib.bib51)]，以垂直视图安装，如在远程会议应用中[[70](#bib.bib70)]，或以顶视图安装，如在监控应用中[[56](#bib.bib56),
    [67](#bib.bib67), [64](#bib.bib64), [52](#bib.bib52)]。此外，它们在3D重建中的使用（使用一个或多个相机）近年来有所增加[[82](#bib.bib82),
    [55](#bib.bib55), [102](#bib.bib102)]。在本次调查中，我们专注于为从顶视图捕获的鱼眼图像开发的深度学习算法。这类图像在监控和环境辅助生活（AAL）应用中至关重要[[84](#bib.bib84)]，主要研究领域包括人物和物体检测以及人类姿态估计。
- en: Although there exist other surveys that focus on omnidirectional fisheye images,
    such as [[50](#bib.bib50), [19](#bib.bib19), [2](#bib.bib2)], they mostly discuss
    the frontal view. Other surveys of top-view imaging [[58](#bib.bib58), [1](#bib.bib1)]
    concentrate only on one application of the top-view perspective and do not specify
    the usage of omnidirectional cameras. The methods surveyed are mostly confined
    to classical computer vision algorithms. Therefore, we consider our survey essential
    for grasping the trend in applications of the combination of top-view fisheye
    imaging and deep learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在其他聚焦于全向鱼眼图像的调查，例如[[50](#bib.bib50), [19](#bib.bib19), [2](#bib.bib2)]，但它们大多讨论的是正面视图。其他关于顶视图成像的调查[[58](#bib.bib58),
    [1](#bib.bib1)]只关注顶视图视角的一个应用，并未具体说明全向相机的使用。调查中的方法大多局限于经典的计算机视觉算法。因此，我们认为我们的调查对于掌握顶视图鱼眼成像与深度学习结合应用的趋势至关重要。
- en: 'This paper is organized as follows: in [Sec. 2](#S2 "2 The omnidirectional
    top-view setup ‣ Applications of Deep Learning for Top-View Omnidirectional Imaging:
    A Survey") we describe the camera geometry and the top view setup in detail. The
    available omnidirectional datasets are presented in [Sec. 3](#S3 "3 Datasets ‣
    Applications of Deep Learning for Top-View Omnidirectional Imaging: A Survey").
    Sections [4](#S4 "4 Person and object detection ‣ Applications of Deep Learning
    for Top-View Omnidirectional Imaging: A Survey"), [5](#S5 "5 Human pose estimation
    and activity recognition ‣ Applications of Deep Learning for Top-View Omnidirectional
    Imaging: A Survey") and [6](#S6 "6 Other applications ‣ Applications of Deep Learning
    for Top-View Omnidirectional Imaging: A Survey") cover object detection, pose
    estimation, human activity recognition and other miscellaneous applications. We
    conclude the survey in [Sec. 7](#S7 "7 Conclusion ‣ Applications of Deep Learning
    for Top-View Omnidirectional Imaging: A Survey").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '本文组织如下：在[Sec. 2](#S2 "2 The omnidirectional top-view setup ‣ Applications of
    Deep Learning for Top-View Omnidirectional Imaging: A Survey")中，我们详细描述了相机几何和顶视图设置。在[Sec. 3](#S3
    "3 Datasets ‣ Applications of Deep Learning for Top-View Omnidirectional Imaging:
    A Survey")中介绍了现有的全向数据集。第[4](#S4 "4 Person and object detection ‣ Applications
    of Deep Learning for Top-View Omnidirectional Imaging: A Survey")、[5](#S5 "5 Human
    pose estimation and activity recognition ‣ Applications of Deep Learning for Top-View
    Omnidirectional Imaging: A Survey")和[6](#S6 "6 Other applications ‣ Applications
    of Deep Learning for Top-View Omnidirectional Imaging: A Survey")节涵盖了物体检测、姿态估计、人类活动识别以及其他杂项应用。我们在[Sec. 7](#S7
    "7 Conclusion ‣ Applications of Deep Learning for Top-View Omnidirectional Imaging:
    A Survey")中总结了调查结果。'
- en: 2 The omnidirectional top-view setup
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 顶视全向设置
- en: 'As introduced in [Sec. 1](#S1 "1 Introduction ‣ Applications of Deep Learning
    for Top-View Omnidirectional Imaging: A Survey"), this survey focuses of the top-view
    omnidirectional vision utilizing one or multiple dioptric cameras. The camera
    or the camera rig consisting of multiple cameras is usually hung on the ceiling
    near the center of the room. Figure [1](#S2.F1 "Figure 1 ‣ 2 The omnidirectional
    top-view setup ‣ Applications of Deep Learning for Top-View Omnidirectional Imaging:
    A Survey") shows an example of such setting. Do not confuse this with the top
    view or bird’s-eye-view in autonomous driving applications [[85](#bib.bib85)],
    which is synthesized from the surround view images.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '如[第1节](#S1 "1 Introduction ‣ Applications of Deep Learning for Top-View Omnidirectional
    Imaging: A Survey")所述，本调查专注于利用一个或多个光学相机的顶视全景视觉。相机或由多个相机组成的相机装置通常悬挂在房间中心附近的天花板上。图[1](#S2.F1
    "Figure 1 ‣ 2 The omnidirectional top-view setup ‣ Applications of Deep Learning
    for Top-View Omnidirectional Imaging: A Survey")显示了这样的设置示例。请不要与自主驾驶应用中的顶视图或鸟瞰图混淆[[85](#bib.bib85)]，后者是从环视图图像合成的。'
- en: '![Refer to caption](img/e9bbd200beb79ca4a32c60471171fee7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e9bbd200beb79ca4a32c60471171fee7.png)'
- en: (a) The top-view omnidirectional set-up in a one-room apartment. The red dot
    in the center illustrates the position of the camera.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 一室公寓中的顶视全景设置。中心的红点表示相机的位置。
- en: '![Refer to caption](img/c1d95d13a9c5cbfd9b55f78f5c2383d5.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c1d95d13a9c5cbfd9b55f78f5c2383d5.png)'
- en: (b) Example output of the set-up in a synthetic environment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 在合成环境中的设置示例输出。
- en: 'Figure 1: The top-view omnidirectional set-up and its output.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：顶视全景设置及其输出。
- en: 'To utilize the advantages and tackle the shortcomings of this set-up, we need
    to understand the model of fisheye camera. An ideal fisheye lens can be described
    with the equidistant projection in [Eq. 1](#S2.E1 "In 2 The omnidirectional top-view
    setup ‣ Applications of Deep Learning for Top-View Omnidirectional Imaging: A
    Survey"). $\theta$ is the angle between the principal axis and the incoming ray,
    $r$ is the distance between the image point and the principal point, and $f$ is
    the focal length (see [Fig. 2](#S2.F2 "In 2 The omnidirectional top-view setup
    ‣ Applications of Deep Learning for Top-View Omnidirectional Imaging: A Survey")).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '为了利用这种设置的优点并解决其不足，我们需要了解鱼眼相机的模型。理想的鱼眼镜头可以用[公式1](#S2.E1 "In 2 The omnidirectional
    top-view setup ‣ Applications of Deep Learning for Top-View Omnidirectional Imaging:
    A Survey")中的等距投影来描述。$\theta$是主轴与入射光线之间的角度，$r$是图像点与主点之间的距离，$f$是焦距（参见[图2](#S2.F2
    "In 2 The omnidirectional top-view setup ‣ Applications of Deep Learning for Top-View
    Omnidirectional Imaging: A Survey")）。'
- en: '|  | $r=f\theta$ |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $r=f\theta$ |  | (1) |'
- en: 'There are other projection models that are less frequently used, see [Tab. 1](#S2.T1
    "In 2 The omnidirectional top-view setup ‣ Applications of Deep Learning for Top-View
    Omnidirectional Imaging: A Survey").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '还有其他较少使用的投影模型，请参见[表1](#S2.T1 "In 2 The omnidirectional top-view setup ‣ Applications
    of Deep Learning for Top-View Omnidirectional Imaging: A Survey")。'
- en: '![Refer to caption](img/1e142a8d1ab89e1e3cc5501e5bf7c243.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1e142a8d1ab89e1e3cc5501e5bf7c243.png)'
- en: 'Figure 2: The equidistant projection. The red ray is the principle axis. The
    black ray is from the maximum visible angle and $r_{img}$ is the radius of the
    image area.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：等距投影。红色光线是主轴。黑色光线来自最大可视角度，$r_{img}$是图像区域的半径。
- en: 'Table 1: Other projection models used for fisheye cameras besides equidistant
    projection.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：除了等距投影外，其他用于鱼眼相机的投影模型。
- en: '| Projection type | Math. expression |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 投影类型 | 数学表达式 |'
- en: '| --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Equisolid | $r=2f\sin(\theta/2)$ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 等均分 | $r=2f\sin(\theta/2)$ |'
- en: '| Stereographic | $r=2f\tan(\theta/2)$ |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 立体投影 | $r=2f\tan(\theta/2)$ |'
- en: '| Orthographic | $r=f\sin\theta$ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 正射投影 | $r=f\sin\theta$ |'
- en: A realistic fisheye camera is not perfect and require a more complex model for
    calibration and precise image unwrapping and odometry. The publication [[47](#bib.bib47)]
    describes a generic model that enables the calibration of a fisheye camera with
    a single planar calibration pattern. DeepCalib presents the possibility of using
    DL for acquiring calibration parameters of fisheye cameras [[7](#bib.bib7)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现实中的鱼眼相机并不完美，需要更复杂的模型进行校准、精确的图像展开和测程。文献[[47](#bib.bib47)]描述了一个通用模型，该模型使得用单一平面校准图案对鱼眼相机进行校准成为可能。DeepCalib展示了利用DL获取鱼眼相机校准参数的可能性[[7](#bib.bib7)]。
- en: An omnidirectional camera can be built with a normal CCD or CMOS camera and
    a fisheye lens [[76](#bib.bib76)]. Besides, there are commercial products from
    various companies, such as the Quasar™ Hemispheric Mini-Dome by TELEDYNE FLIR,
    the HemiStereo™ DK1 and NX by 3DVisionlabs, the panoramic series by HIKVISION,
    the IP Fisheye series by ABUS, the C71 and Q71 by MOBOTIX, the FE series by VIVOTEK,
    etc. They can be most easily found under the term “hemispherical camera” with
    a search engine.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一台全景相机可以通过普通的 CCD 或 CMOS 相机和鱼眼镜头 [[76](#bib.bib76)] 来构建。此外，还有来自各种公司的商业产品，如 TELEDYNE
    FLIR 的 Quasar™ Hemispheric Mini-Dome、3DVisionlabs 的 HemiStereo™ DK1 和 NX、HIKVISION
    的全景系列、ABUS 的 IP Fisheye 系列、MOBOTIX 的 C71 和 Q71、VIVOTEK 的 FE 系列等。通过搜索引擎，您可以最容易地找到这些产品，搜索词为“半球相机”。
- en: 3 Datasets
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个数据集
- en: 'In deep learning approaches, the availability of quality data and ground truth
    annotations is essential for the training process. Today, models of almost every
    architecture can be obtained with weights trained on popular large-scale image
    datasets such as MS COCO [[60](#bib.bib60)], ImageNet [[79](#bib.bib79), [105](#bib.bib105)],
    etc. This enables users to avoid long and costly trainings from scratch and to
    facilitate the transfer learning ability of neural networks to adapt these existing
    models to new domains and new tasks. However, the images in these datasets are
    mostly collected with a perspective camera from a frontal view. Therefore, the
    pictured objects present a different appearance from those in top-view omnidirectional
    images, especially when they are close to the camera. This prevents not only the
    direct use of these models in such images, but also makes transfer learning extremely
    difficult, if a large amount of omnidirectional data for fine-tuning is not available.
    In the early 2010s, omnidirectional image datasets were scarce and insufficient
    for training or fine-tuning complex architectures. However, with the increasing
    interest in using omnidirectional cameras, the first real-world and synthetic
    datasets of top view fisheye images were created. Unlike general-purpose image
    datasets for classification, object recognition or segmentation, which are collected
    on the internet and have great intra-dataset variability, omnidirectional datasets
    are mostly recorded in a specific setting for a specific task. Therefore, they
    are usually continuous sequences extracted from videos, and the variability between
    images is lower. In the following subsections, we present and describe these datasets.
    [Tab. 2](#S3.T2 "In 3 Datasets ‣ Applications of Deep Learning for Top-View Omnidirectional
    Imaging: A Survey") summarizes their technical characteristics. Links to the datasets
    in this chapter are accessed on March 17th, 2023.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习方法中，质量数据和真实标注的可用性对于训练过程至关重要。如今，几乎所有架构的模型都可以通过在流行的大规模图像数据集如 MS COCO [[60](#bib.bib60)]、ImageNet [[79](#bib.bib79),
    [105](#bib.bib105)] 上训练得到的权重获得。这使得用户能够避免从零开始的漫长且昂贵的训练，并促进神经网络的迁移学习能力，使这些现有模型能够适应新的领域和任务。然而，这些数据集中的图像大多是通过前视透视相机拍摄的。因此，拍摄的物体与从顶视全景图像中看到的物体外观不同，尤其是当它们靠近相机时。这不仅阻碍了这些模型在此类图像中的直接使用，也使得迁移学习变得极其困难，如果没有大量的全景数据用于微调。在2010年代初，全景图像数据集稀缺且不足以用于训练或微调复杂架构。然而，随着对使用全景相机的兴趣增加，首批顶视鱼眼图像的真实世界和合成数据集被创建。与用于分类、目标识别或分割的一般图像数据集不同，这些数据集是在特定设置下为特定任务记录的，因此它们通常是从视频中提取的连续序列，图像之间的变化较小。在以下小节中，我们将介绍和描述这些数据集。[Tab. 2](#S3.T2
    "在3个数据集 ‣ 深度学习在顶视全景成像中的应用：综述")总结了它们的技术特征。本章中数据集的链接于2023年3月17日访问。
- en: 'Table 2: Technical attributes of omnidirectional image and video datasets.
    Types include real-world (R), synthetic (S) or hybrid (R+S). Res. stands for resolution
    and the unit is megapixels (MP). Year indicates the year of publication, not the
    year of appearance.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：全景图像和视频数据集的技术属性。类型包括真实世界（R）、合成（S）或混合（R+S）。分辨率（Res.）的单位是百万像素（MP）。年份（Year）指的是出版年份，而非出现年份。
- en: '| Dataset | Type | # of frames | Res. (MP) | Annotations | Classes | Year |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 帧数 | 分辨率（MP） | 注释 | 类别 | 年份 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Bomni | R | 10,340 | 0.3 | bbox, tracking ID, actions | 9 actions | 2012
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Bomni | R | 10,340 | 0.3 | bbox, tracking ID, actions | 9 actions | 2012
    |'
- en: '| HDA (Cam 02) | R | 1,388 | 0.3 | bbox | person | 2013 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| HDA (Cam 02) | R | 1,388 | 0.3 | bbox | 人物 | 2013 |'
- en: '| LMS | R+S | 515 | 1.2 | – | – | 2016 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| LMS | R+S | 515 | 1.2 | – | – | 2016 |'
- en: '| PIROPO | R | 111,283 | 0.48 | head point, bbox (3rd party) | person | 2021
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| PIROPO | R | 111,283 | 0.48 | 头部点，边界框（第三方） | 人物 | 2021 |'
- en: '| MW-18Mar | R+S | 14,040 | 1.1 to 2.2 | bbox, rotated bbox (3rd party) | person
    | 2018 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| MW-18Mar | R+S | 14,040 | 1.1 到 2.2 | 边界框，旋转边界框（第三方） | 人物 | 2018 |'
- en: '| HABBOF | R | 5,837 | 4.2 | rotated bbox | person | 2019 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| HABBOF | R | 5,837 | 4.2 | 旋转边界框 | 人物 | 2019 |'
- en: '| CEPDOF | R | 25,504 | 1.1 to 4.2 | rotated bbox | person | 2020 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| CEPDOF | R | 25,504 | 1.1 到 4.2 | 旋转边界框 | 人物 | 2020 |'
- en: '| WEBDTOF | R | 10,544 | 0.6 to 5 | rotated bbox | person | 2022 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| WEBDTOF | R | 10,544 | 0.6 到 5 | 旋转边界框 | 人物 | 2022 |'
- en: '| FRIDA | R | 18,318 | 4.2 | rotated bbox, person ID | person | 2022 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| FRIDA | R | 18,318 | 4.2 | 旋转边界框，人物 ID | 人物 | 2022 |'
- en: '| DEPOF | R | 3,594 | 4.2 | bbox, point location | person | 2023 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| DEPOF | R | 3,594 | 4.2 | 边界框，点位 | 人物 | 2023 |'
- en: '| FES | R | 301 | 2.8 | bbox, instance mask | 6 classes | 2020 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| FES | R | 301 | 2.8 | 边界框，实例掩码 | 6 类别 | 2020 |'
- en: '| 360Action | R | 784 clips | 8.3 | actions per video clip | 19 actions | 2020
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 360Action | R | 784 个剪辑 | 8.3 | 每个视频剪辑的动作 | 19 种动作 | 2020 |'
- en: '| FRailTRI20_DOD | R | 44,099 | 0.93 | temporal anomalies | 7 anomalies | 2020
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| FRailTRI20_DOD | R | 44,099 | 0.93 | 时间异常 | 7 种异常 | 2020 |'
- en: '| OSD | R+S | 39,200 | 1.0 | bbox | person | 2021 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| OSD | R+S | 39,200 | 1.0 | 边界框 | 人物 | 2021 |'
- en: '| THEODORE | S | 100,000 | 1.0 | bbox, segmentation & instance mask | 14 classes
    | 2020 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| THEODORE | S | 100,000 | 1.0 | 边界框，分割 & 实例掩码 | 14 类别 | 2020 |'
- en: '| THEOStereo | S | 31,250 pairs | 1.0 | depth map | – | 2021 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| THEOStereo | S | 31,250 对 | 1.0 | 深度图 | – | 2021 |'
- en: 3.1 Real-world datasets
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 现实世界数据集
- en: The Bomni Database¹¹1[https://www.cmpe.boun.edu.tr/pilab/pilabfiles/databases/bomni/](https://www.cmpe.boun.edu.tr/pilab/pilabfiles/databases/bomni/)
    (Boğaziçi University Multi-Omnidirectional Video Tracking Database) [[24](#bib.bib24)]
    is one of the earliest datasets of omnidirectional fisheye camera images. Although
    the authors list a few other datasets, they are not available anymore at the time
    of this review. Bomni DB is recorded for the purpose of human tracking in indoor
    scenes. Two fisheye cameras, one mounted on the ceiling and the other on a side
    wall, are used to simultaneously record two scenarios with a resolution of $640\times
    480$ pixels and a frame rate of $8\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$. Scenario
    1 shows a single subject entering a room and performing six different actions
    before leaving. For this scenario a total of 10 videos with 5 different subjects
    are recorded. Scenario 2 presents 36 videos of multiple persons interacting in
    the same room. For this scenario a total of five actions are defined. The dataset
    provides tracking IDs, bounding boxes for moving subjects and action labels as
    annotations, which are given in vatic [[94](#bib.bib94)] format. It is to be noted
    that a portion of the annotations are generated from automatic tracking and interpolation.
    This often results in slight misalignment between the subject and its bounding
    box. Additionally, Bomni DB lacks labels for quasi-static persons in the scene.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Bomni 数据库¹¹1[https://www.cmpe.boun.edu.tr/pilab/pilabfiles/databases/bomni/](https://www.cmpe.boun.edu.tr/pilab/pilabfiles/databases/bomni/)（博阿兹奇大学多全向视频跟踪数据库）[[24](#bib.bib24)]
    是最早的全向鱼眼相机图像数据集之一。尽管作者列出了其他几个数据集，但在本次评审时这些数据集已经无法获取。Bomni 数据库的录制目的是为了在室内场景中进行人体跟踪。使用了两台鱼眼相机，一台安装在天花板上，另一台安装在侧墙上，分别同时记录两个场景，分辨率为
    $640\times 480$ 像素，帧率为 $8\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$。场景 1 展示了一个单一的主体进入房间并在离开前执行六种不同动作的过程。在这个场景中，总共录制了
    10 个视频，涉及 5 个不同的主体。场景 2 展示了 36 个视频，记录了多个人在同一个房间中的互动。在这个场景中，总共定义了五种动作。数据集提供了跟踪
    ID、移动主体的边界框和动作标签作为注释，注释格式为 vatic [[94](#bib.bib94)]。需要注意的是，部分注释是通过自动跟踪和插值生成的，这通常会导致主体和其边界框之间的轻微错位。此外，Bomni
    数据库缺乏场景中准静态人物的标签。
- en: HDA Person Dataset²²2[https://vislab.isr.tecnico.ulisboa.pt/hda-dataset/](https://vislab.isr.tecnico.ulisboa.pt/hda-dataset/) [[30](#bib.bib30)]
    is a dataset for surveillance. Most of the image data are recorded by classic
    surveillance cameras, but the sequence *Cam 02* is recorded by a fisheye camera
    mounted on the ceiling of an elevator waiting area. The sequence has a resolution
    of $640\times 480$ and a frame rate of $5\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$.
    In total 9819 frames are recorded. Bounding boxes of persons are provided in this
    dataset. Heavy motion blur is present throughout this recording.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: HDA Person 数据集²²2[https://vislab.isr.tecnico.ulisboa.pt/hda-dataset/](https://vislab.isr.tecnico.ulisboa.pt/hda-dataset/)
    [[30](#bib.bib30)] 是一个用于监控的 数据集。大多数图像数据由经典监控摄像机录制，但序列 *Cam 02* 是由安装在电梯候车区天花板上的鱼眼摄像机录制的。该序列的分辨率为
    $640\times 480$，帧率为 $5\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$。总共记录了 9819 帧。该数据集中提供了人员的边界框。整个录制过程中存在严重的运动模糊。
- en: PIROPO database³³3[https://sites.google.com/site/piropodatabase/](https://sites.google.com/site/piropodatabase/)
    (People in Indoor ROoms with Perspective and Omnidirectional cameras) [[21](#bib.bib21)]
    is recorded simultaneously using a ceiling-mounted fisheye camera and a normal
    perspective camera. The scenes consist of a single person or multiple people walking,
    standing or sitting in a room. There are no interactions between the persons.
    It is a large scale dataset with over 100,000 annotated frames and a number of
    unannotated frames. The annotation is provided in the form of points, which mark
    the head positions of the persons in the image. In the work [[107](#bib.bib107)],
    the authors mention they down-sampled the original dataset with annotations and
    manually annotated the resulting dataset with bounding boxes for the persons.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PIROPO 数据库³³3[https://sites.google.com/site/piropodatabase/](https://sites.google.com/site/piropodatabase/)（使用透视和全景摄像机的室内人物）[[21](#bib.bib21)]
    同时使用安装在天花板上的鱼眼摄像机和普通透视摄像机录制。场景包含单个或多个在房间内行走、站立或坐着的人。人物之间没有互动。这是一个大规模数据集，包含超过 100,000
    帧标注帧和若干未标注帧。标注以点的形式提供，标记了图像中人物的头部位置。在[[107](#bib.bib107)]的工作中，作者提到他们对原始标注数据集进行了下采样，并用边界框手动标注了结果数据集中的人物。
- en: 'MW-18Mar Dataset⁴⁴4[https://www2.icat.vt.edu/mirrorworlds/challenge/index.html](https://www2.icat.vt.edu/mirrorworlds/challenge/index.html)
    from Mirror Worlds Challenge is an indoor top-view fisheye video dataset which
    consists of 30 videos an 13k frames. The original dataset are annotated with axis-aligned
    bounding boxes. For tracking purpose there are also annotated track trajectories.
    There are 3 main scenarios in this dataset: an observation room, a hallway and
    a synthetic scene of an observation room. The train set of this dataset is later
    annotated with rotated bounding boxes by the authors of [[27](#bib.bib27)] and
    is named MW-R⁵⁵5[https://vip.bu.edu/projects/vsns/cossy/datasets/mw-r/](https://vip.bu.edu/projects/vsns/cossy/datasets/mw-r/).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MW-18Mar 数据集⁴⁴4[https://www2.icat.vt.edu/mirrorworlds/challenge/index.html](https://www2.icat.vt.edu/mirrorworlds/challenge/index.html)
    来源于 Mirror Worlds Challenge，是一个室内顶视鱼眼视频数据集，包括 30 个视频和 13k 帧。原始数据集使用了轴对齐的边界框进行标注。为了跟踪目的，还提供了标注的轨迹。在此数据集中有
    3 个主要场景：一个观察室、一个走廊和一个合成的观察室场景。该数据集的训练集后来由[[27](#bib.bib27)]的作者用旋转边界框进行了标注，并命名为
    MW-R⁵⁵5[https://vip.bu.edu/projects/vsns/cossy/datasets/mw-r/](https://vip.bu.edu/projects/vsns/cossy/datasets/mw-r/)。
- en: Tamura *et al*. annotated Bomni, PIROPO and MW-18Mar datasets with rotated bounding
    boxes for their work [[90](#bib.bib90)]. The annotation files are in Pascal VOC
    format [[29](#bib.bib29)] and available online⁶⁶6[https://github.com/hitachi-rd-cv/omnidet-rotinv](https://github.com/hitachi-rd-cv/omnidet-rotinv).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*Tamura* 等人对 Bomni、PIROPO 和 MW-18Mar 数据集进行了旋转边界框的标注，用于他们的工作[[90](#bib.bib90)]。标注文件采用
    Pascal VOC 格式[[29](#bib.bib29)]，并在线提供⁶⁶6[https://github.com/hitachi-rd-cv/omnidet-rotinv](https://github.com/hitachi-rd-cv/omnidet-rotinv)。'
- en: 'HABBOF (Human-Aligned Bounding Boxes from Overhead Fisheye Cameras) [[56](#bib.bib56)],
    CEPDOF (Challenging Events for Person Detection from Overhead Fisheye Images) [[27](#bib.bib27)]
    and WEBDTOF (In-the-Wild Events for People Detection and Tracking from Overhead
    Fisheye Cameras) [[91](#bib.bib91)] are datasets collected by the Visual Information
    Processing Laboratory of Boston University⁷⁷7[https://vip.bu.edu/projects/vsns/cossy/datasets](https://vip.bu.edu/projects/vsns/cossy/datasets).
    HABBOF provides two indoor scenes of 5837 frames. The annotations are given as
    bounding boxes aligned to the human body, which appear mostly in line with the
    radial axis of the omnidirectional image. CEPDOF is an extension of HABBOF. It
    provides 8 video sequences of different levels of crowdedness under different
    lighting conditions. Unlike any earlier datasets, which are recorded in controlled
    settings, WEBDTOF is recorded in real-life situations. 14 scenes are recorded
    with different cameras and lens to form 16 videos. Thus, it covers common difficulties
    presented in real life: occlusions, camouflage, cropping, tiny people, non-circular
    FOV and children. The same research group also presents two datasets for other
    applications: DEPOF (Distance Estimation between People from Overhead Fisheye
    cameras) [[62](#bib.bib62)] and FRIDA (Fisheye Re-Identification Dataset with
    Annotations) [[17](#bib.bib17)]. DEPOF provides 3,526 frames for calibration purpose
    and 68 frames for training and testing of a person distance measurement method.
    FRIDA is destined for person re-identification, but rotated bounding boxes are
    also available.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: HABBOF (Human-Aligned Bounding Boxes from Overhead Fisheye Cameras) [[56](#bib.bib56)]、CEPDOF
    (Challenging Events for Person Detection from Overhead Fisheye Images) [[27](#bib.bib27)]
    和 WEBDTOF (In-the-Wild Events for People Detection and Tracking from Overhead
    Fisheye Cameras) [[91](#bib.bib91)] 是由波士顿大学视觉信息处理实验室收集的数据集⁷⁷7[https://vip.bu.edu/projects/vsns/cossy/datasets](https://vip.bu.edu/projects/vsns/cossy/datasets)。HABBOF
    提供了两个室内场景，共5837帧。注释以对齐于人体的边界框形式给出，这些边界框大多与全景图像的径向轴线一致。CEPDOF 是 HABBOF 的扩展，提供了在不同光照条件下、不同拥挤程度的8个视频序列。与以往在受控环境下录制的数据集不同，WEBDTOF
    是在现实生活中录制的。记录了14个场景，使用不同的相机和镜头，形成16个视频。因此，它涵盖了现实生活中常见的困难：遮挡、伪装、裁剪、小个子人物、非圆形视野和儿童。相同的研究小组还提供了其他应用的数据集：DEPOF
    (Distance Estimation between People from Overhead Fisheye cameras) [[62](#bib.bib62)]
    和 FRIDA (Fisheye Re-Identification Dataset with Annotations) [[17](#bib.bib17)]。DEPOF
    提供了3526帧用于校准，并提供了68帧用于训练和测试人员距离测量方法。FRIDA 旨在用于人员再识别，但也提供了旋转边界框。
- en: 'FES⁸⁸8[https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/fes/](https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/fes/)
    (Fisheye Evaluation Suite) [[81](#bib.bib81)] is an indoor dataset. It differs
    greatly from the afore mentioned datasets by providing bounding boxes and instance
    segmentation masks for 6 classes: person, TV, table, armchair, chair and wheeled
    walker. The disadvantage of this dataset is its relative small size at only 301
    frames.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: FES⁸⁸8[https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/fes/](https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/fes/)
    (Fisheye Evaluation Suite) [[81](#bib.bib81)] 是一个室内数据集。它与前述数据集有很大不同，提供了6个类别的边界框和实例分割掩码：人、电视、桌子、扶手椅、椅子和带轮步行器。该数据集的缺点是相对较小，仅有301帧。
- en: 360action⁹⁹9[https://github.com/ryukenzen/360action](https://github.com/ryukenzen/360action) [[53](#bib.bib53)]
    is a dataset for action recognition in the form of video clips. In each clip,
    which has the length of 6 to 10 seconds at $30\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$,
    a number of subjects perform daily actions by themselves or with interactions.
    Action labels are given for each video, without specifying the subject.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 360action⁹⁹9[https://github.com/ryukenzen/360action](https://github.com/ryukenzen/360action) [[53](#bib.bib53)]
    是一个用于动作识别的视频片段数据集。在每个片段中，长度为6到10秒，帧率为 $30\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$，一些被试者进行日常动作，单独完成或进行互动。每个视频都有动作标签，但未指定具体的被试者。
- en: FRailTRI20_DOD (French Rail Technological Research Institute Door Obstacle Detection
    2020) [[52](#bib.bib52)] was created specifically for the surveillance of door
    areas of trains. The images were captured at $20\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$.
    Seven anomalies regarding train doors and passengers were performed by five actors.
    The authors defined a set of annotations regarding instance positions and displacement,
    door state, hazardous events, pedestrian actions and combine them into a temporal
    segmentation of an event. The dataset as well as a meticulous annotation guide
    can be acquired by contacting the first author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: FRailTRI20_DOD（法国铁路技术研究所门障碍检测 2020）[[52](#bib.bib52)] 专门用于监控火车门区域。图像以 $20\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$
    速率捕获。通过五名演员执行了关于火车门和乘客的七种异常行为。作者定义了一组注释，涉及实例位置和位移、门状态、危险事件、行人动作，并将其合并为事件的时间分割。可以通过联系第一作者获取数据集及详细的注释指南。
- en: 3.2 Synthetic datasets
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 合成数据集
- en: Apart from the subset in MW-18Mar dataset, multiple synthetic datasets of omnidirectional
    images have been created for a variety of purposes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 MW-18Mar 数据集中的子集外，还创建了多个全景图像的合成数据集，用于各种目的。
- en: LMS Fisheye Dataset^(10)^(10)10[https://www.lms.tf.fau.eu/research/downloads/fisheye-data-set/](https://www.lms.tf.fau.eu/research/downloads/fisheye-data-set/) [[28](#bib.bib28)]
    provides a variety of synthetic and real-world fisheye image video sequences,
    among which HallwayC, LivingroomB, Room, HallwayB, LivingroomA and LivingroomC
    are top view. These sequences do not have corresponding annotations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LMS Fisheye 数据集^(10)^(10)10[https://www.lms.tf.fau.eu/research/downloads/fisheye-data-set/](https://www.lms.tf.fau.eu/research/downloads/fisheye-data-set/)
    [[28](#bib.bib28)] 提供了多种合成和真实世界的鱼眼图像视频序列，其中 HallwayC、LivingroomB、Room、HallwayB、LivingroomA
    和 LivingroomC 是顶视图。这些序列没有相应的注释。
- en: OSD^(11)^(11)11[https://datasets.vicomtech.org/v4-osd/OSD_download.zip](https://datasets.vicomtech.org/v4-osd/OSD_download.zip)
    (Omnidirectional Synthetic Datasets) [[5](#bib.bib5)] is a dataset for person
    recognition and surveillance. Therefore, it only provides annotations for persons
    in the form of segmentation masks and bounding boxes. The persons are small in
    size to simulate actual surveillance situations where a large area is monitored
    by one omnidirectional camera. Besides Omnidirectional images it also provides
    rectified images.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: OSD^(11)^(11)11[https://datasets.vicomtech.org/v4-osd/OSD_download.zip](https://datasets.vicomtech.org/v4-osd/OSD_download.zip)
    (全景合成数据集) [[5](#bib.bib5)] 是一个用于人脸识别和监控的数据集。因此，它仅提供以分割掩膜和边界框形式的人员注释。人员体积较小，以模拟实际监控场景中一个全景摄像机监控大面积的情况。除了全景图像外，它还提供了校正图像。
- en: THEODORE^(12)^(12)12[https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/theodore/](https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/theodore/)
    (synTHEtic tOp-view inDoOR scEnes dataset) [[81](#bib.bib81)] provides a large-scale
    diverse dataset with annotations for semantic segmentation, instance segmentation
    and bounding boxes for object detection. THEOStereo^(13)^(13)13[https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/theostereo/](https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/theostereo/) [[86](#bib.bib86)]
    is derived from THEODORE and aims to aid depth estimation using top-view fisheye
    cameras. It provides image pairs from two virtual cameras and the corresponding
    depth maps as ground truth annotations. The baseline of the stereo cameras is
    $0.3\text{\,}\mathrm{m}$.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: THEODORE^(12)^(12)12[https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/theodore/](https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/theodore/)
    (合成顶视室内场景数据集) [[81](#bib.bib81)] 提供了一个大规模多样的数据集，包含用于语义分割、实例分割和物体检测的边界框注释。THEOStereo^(13)^(13)13[https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/theostereo/](https://www.tu-chemnitz.de/etit/dst/forschung/comp_vision/datasets/theostereo/)
    [[86](#bib.bib86)] 来源于 THEODORE，旨在使用顶视鱼眼摄像头辅助深度估计。它提供了来自两个虚拟摄像头的图像对以及作为真实注释的相应深度图。立体摄像头的基线为
    $0.3\text{\,}\mathrm{m}$。
- en: 3.3 Other datasets
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 其他数据集
- en: Other datasets such as DPI-T^(14)^(14)14[https://github.com/zhengkang86/ram_person_id](https://github.com/zhengkang86/ram_person_id)
    (Depth-Based Person Identification from Top View) [[38](#bib.bib38)], TVPR^(15)^(15)15[https://vrai.dii.univpm.it/re-id-dataset](https://vrai.dii.univpm.it/re-id-dataset)
    (Top View Person Re-Identification) dataset [[57](#bib.bib57)] does not use fisheye
    cameras, but the overhead viewpoint is nonetheless useful for training networks
    applied for omnidirectional images. These two datasets contain a depth channel
    in addition to RGB channels. PanopTOP31K^(16)^(16)16[https://github.com/mmlab-cv/PanopTOP](https://github.com/mmlab-cv/PanopTOP) [[34](#bib.bib34)]
    is a semisynthetic RGB dataset from the top-view for view-invariant 3D human pose
    estimation. This is the first large-scale dataset that features top-view human
    keypoints. However, its low resolution at $256\times 256$ pixels and heavy artifacts
    affect its usability negatively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据集，如DPI-T^(14)^(14)14[https://github.com/zhengkang86/ram_person_id](https://github.com/zhengkang86/ram_person_id)（基于深度的顶视人物识别）[[38](#bib.bib38)]、TVPR^(15)^(15)15[https://vrai.dii.univpm.it/re-id-dataset](https://vrai.dii.univpm.it/re-id-dataset)（顶视人物再识别）数据集[[57](#bib.bib57)]不使用鱼眼相机，但其俯视视角仍然对训练应用于全景图像的网络有用。这两个数据集除了RGB通道外，还包含深度通道。PanopTOP31K^(16)^(16)16[https://github.com/mmlab-cv/PanopTOP](https://github.com/mmlab-cv/PanopTOP)
    [[34](#bib.bib34)]是一个用于视图不变3D人体姿态估计的顶视半合成RGB数据集。这是第一个包含顶视人体关键点的大规模数据集。然而，其$256\times
    256$像素的低分辨率和明显的伪影对其可用性产生了负面影响。
- en: 4 Person and object detection
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 人物与物体检测
- en: 4.1 Person detection
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 人物检测
- en: 'The datasets in [Sec. 3](#S3 "3 Datasets ‣ Applications of Deep Learning for
    Top-View Omnidirectional Imaging: A Survey") clearly show that the main application
    of omnidirectional cameras is person detection and tracking. Using deep learning
    for person detection in omnidirectional images, typically a CNN-based object detector,
    faces a few obstacles. Firstly, standing people appear in line with the radial
    axis of the image rather than upwards in images from side-mounted cameras. Secondly,
    the equidistant projection of fisheye cameras results in considerable deformation
    of objects. These two problems restricts the utilization of pre-trained models
    and reduces the effectiveness of transfer learning. Additionally, when the person
    stands directly under the camera, it has a unique appearance that is unseen in
    normal perspective images. We review the researches to see how these problems
    are progressively solved. An overview is provided in [Tab. 3](#S4.T3 "In 4.1 Person
    detection ‣ 4 Person and object detection ‣ Applications of Deep Learning for
    Top-View Omnidirectional Imaging: A Survey"). Note that due to the lack of a common
    large scale dataset, researchers have used different datasets and metrics for
    the evaluation, therefore it is not possible to compare the performances to each
    other directly. Thus, the performances are not listed in the table.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Sec. 3](#S3 "3 数据集 ‣ 深度学习在顶视全景成像中的应用：综述")中的数据集清楚地显示，全景相机的主要应用是人物检测和跟踪。使用深度学习进行全景图像中的人物检测，通常一个基于CNN的物体检测器，会面临一些障碍。首先，站立的人物在图像中的径向轴线对齐，而不是在侧装相机的图像中向上。其次，鱼眼相机的等距投影导致物体的显著变形。这两个问题限制了预训练模型的利用，降低了迁移学习的有效性。此外，当人物直接站在相机下方时，其外观独特，与普通透视图像中的外观不同。我们回顾了研究，看看这些问题是如何逐步解决的。概述见[Tab.
    3](#S4.T3 "4.1 人物检测 ‣ 4 人物与物体检测 ‣ 深度学习在顶视全景成像中的应用：综述")。请注意，由于缺乏统一的大规模数据集，研究人员使用了不同的数据集和指标进行评估，因此无法直接比较性能。因此，表中未列出性能数据。
- en: 'Table 3: Person detection in Omnidirectional images'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：全景图像中的人物检测
- en: '| Architecture | Main algorithm |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 主要算法 |'
- en: '| --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nguyen *et al*. [[67](#bib.bib67)] | AGMM-background subtraction + tiny YOLO
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen *et al*. [[67](#bib.bib67)] | AGMM-背景减除 + tiny YOLO |'
- en: '| OmniDetector [[83](#bib.bib83)] | Unwapping + YOLOv2 + NMS |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| OmniDetector [[83](#bib.bib83)] | 解卷积 + YOLOv2 + NMS |'
- en: '| Li *et al*. [[56](#bib.bib56)] | Rotating window + background subtraction
    + YOLOv3 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Li *et al*. [[56](#bib.bib56)] | 旋转窗口 + 背景减除 + YOLOv3 |'
- en: '| Tamura *et al*. [[90](#bib.bib90)] | Rotation-invariant training + YOLOv2
    + BBR |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Tamura *et al*. [[90](#bib.bib90)] | 旋转不变训练 + YOLOv2 + BBR |'
- en: '| OmniPD [[107](#bib.bib107)] | Hybrid training + rotation augmentation |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| OmniPD [[107](#bib.bib107)] | 混合训练 + 旋转增强 |'
- en: '| RAPiD [[27](#bib.bib27)] | YOLOv3-based network + orientation prediction
    head + angle aware loss |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| RAPiD [[27](#bib.bib27)] | 基于YOLOv3的网络 + 定向预测头 + 角度感知损失 |'
- en: '| ARPD [[64](#bib.bib64)] | CenterNet + orientation prediction head + rotation
    aware loss function |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ARPD [[64](#bib.bib64)] | CenterNet + 方向预测头 + 旋转感知损失函数 |'
- en: '| Haggui *et al*. [[37](#bib.bib37)] | RAPiD + color histograms for tracking
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Haggui *等* [[37](#bib.bib37)] | RAPiD + 颜色直方图跟踪 |'
- en: '| Wang *et al*. [[98](#bib.bib98)] | Dual Mask R-CNN + image region separation
    + scene specific training |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Wang *等* [[98](#bib.bib98)] | 双重Mask R-CNN + 图像区域分离 + 场景特定训练 |'
- en: '| GSAC-DNN [[32](#bib.bib32)] | 2D grid of simple CNN-classifiers |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| GSAC-DNN [[32](#bib.bib32)] | 简单CNN分类器的2D网格 |'
- en: '| Callemein *et al*. [[9](#bib.bib9)] | Low resolution image + temporal interlacing
    kernel in YOLOv2 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Callemein *等* [[9](#bib.bib9)] | 低分辨率图像 + YOLOv2中的时间交错核 |'
- en: '| OmniDRL [[71](#bib.bib71)] | Deep Q-Network + camera calibration |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| OmniDRL [[71](#bib.bib71)] | 深度Q网络 + 相机标定 |'
- en: '| Wiedemer *et al*. [[100](#bib.bib100)] | Faster-RCNN + few-shot training
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Wiedemer *等* [[100](#bib.bib100)] | Faster-RCNN + 少量样本训练 |'
- en: Nguyen *et al*. [[67](#bib.bib67)] combined Adaptive Gaussian Mixture Model
    (AGMM)-based background subtraction and a simple CNN inspired by Tiny Yolo [[73](#bib.bib73)]
    to perform pedestrian detection. The network takes the foreground mask and single-channel
    grayscale images as input. Their evaluation shows an AP of 0.86 at their house
    dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Nguyen *等* [[67](#bib.bib67)] 结合了基于自适应高斯混合模型（AGMM）的背景减除和受Tiny Yolo [[73](#bib.bib73)]
    启发的简单CNN来进行行人检测。该网络以前景掩膜和单通道灰度图像作为输入。他们的评估显示，在他们的本地数据集上AP为0.86。
- en: Seidel *et al*. proposed OmniDetector [[83](#bib.bib83)], which uses the camera
    calibration parameters to unwrap one omnidirectional image into 94 highly overlapping
    perspective images and then apply the pre-trained YOLOv2 [[74](#bib.bib74)] to
    detect persons. The bounding boxes are projected back into the omnidirectional
    image using a look-up table (LUT). Non-maximum suppression (NMS) is applied to
    the detections to eliminate overlapping bounding boxes and generate the final
    detection. They achieved an AP@0.5IoU of 0.646 on PIROPO when using soft-NMS with
    Gaussian smoothing. This method enables the use of CNN-based detectors without
    the need for collecting new data and training. However, its shortcomings are obvious.
    It has a large overhead, partly because of the transformations and partly because
    of the large amount of inferences for one image. It cannot detect persons directly
    under the camera, since the network has not seen such examples. Furthermore, it
    requires that the camera calibration parameters are known, which isn’t feasible
    at all times. Curiously, Chiang *et al*. [[15](#bib.bib15)] used the same approach
    in 2021, with the only improvement of reducing the number of ROIs to eight.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Seidel *等* 提出了OmniDetector [[83](#bib.bib83)]，该方法利用相机标定参数将一个全景图像展开成94个高度重叠的透视图像，然后应用预训练的YOLOv2 [[74](#bib.bib74)]进行行人检测。边界框使用查找表（LUT）重新投影回全景图像。通过应用非极大值抑制（NMS）来消除重叠的边界框，并生成最终检测结果。他们在PIROPO上使用软NMS和高斯平滑达到了0.646的AP@0.5IoU。此方法使得可以使用基于CNN的检测器，而无需收集新数据和训练。然而，它的缺点也很明显。它有较大的开销，部分原因是变换，部分原因是对单张图像的大量推断。它无法直接检测摄像头下的行人，因为网络未见过此类示例。此外，它要求相机标定参数是已知的，这在所有情况下都不可行。奇怪的是，Chiang
    *等* [[15](#bib.bib15)] 在2021年使用了相同的方法，仅改进了将ROIs的数量减少到八个。
- en: Li *et al*. proposed in [[56](#bib.bib56)] to use a rotating rectangular focus
    windows to extract a part of the image, which will be rotated to maintain the
    upright direction of the person. The maximum number of focus windows is 24. Then
    the detection is performed with YOLOv3 [[75](#bib.bib75)] and consecutive NMS.
    The authors used background subtraction to identify regions of interest (ROI)
    where people are present and discard the focus windows without human activity,
    thus reducing the computational cost. This method is tested on HABBOF with the
    F-score of 0.88. The similarities with OmniDetector are the usage of multiple
    overlapping windows and that the detector does not need to be re-trained or fine-tuned.
    But this method reduces the computational cost and does not require the camera
    parameters to be known. However, it still does not address the person-under-the-camera
    problem, and it is by design not able to detect stationary persons.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Li *等* 在 [[56](#bib.bib56)] 中提出使用旋转矩形焦点窗口提取图像的一部分，该部分将被旋转以保持人员的直立方向。最大焦点窗口数量为
    24。然后使用 YOLOv3 [[75](#bib.bib75)] 和连续 NMS 进行检测。作者使用背景减除法识别有人员出现的兴趣区域（ROI），并丢弃没有人类活动的焦点窗口，从而降低计算成本。这种方法在
    HABBOF 上测试，F 分数为 0.88。与 OmniDetector 相似的是，使用了多个重叠窗口，并且检测器无需重新训练或微调。但该方法降低了计算成本，不需要知道相机参数。然而，它仍未解决摄像机下方人员的问题，并且设计上无法检测静止人员。
- en: Yu *et al*. took another approach and tried to achieve person detection directly
    in omnidirectional images with OmniPD [[107](#bib.bib107)]. They presented a training
    paradigm, by which omnidirectional images are combined with a dataset of normal
    perspective, in this case the PASCAL VOC dataset [[29](#bib.bib29)], to finetune
    a CNN-based object detector. Random horizontal and vertical flipping and random
    90-degree rotation was used as data augmentation to compensate for rotation variance
    in omnidirectional images. Their best result was achieved with SSD [[61](#bib.bib61)]
    at AP@0.5IoU at 0.863, albeit on their own dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Yu *等* 采用了另一种方法，尝试直接在全景图像中实现人员检测，使用了 OmniPD [[107](#bib.bib107)]。他们提出了一种训练范式，将全景图像与正常视角的数据集（在此情况下为
    PASCAL VOC 数据集 [[29](#bib.bib29)]) 结合起来，以微调基于 CNN 的物体检测器。使用了随机水平和垂直翻转以及随机 90 度旋转作为数据增强，以补偿全景图像中的旋转变异。他们在自己的数据集上使用
    SSD [[61](#bib.bib61)] 达到了 AP@0.5IoU 为 0.863 的最佳结果。
- en: 'Tamura *et al*. tried to achieve pedestrian detection in omnidirectional images
    by training YOLOv2 with randomly rotated perspective images from COCO [[90](#bib.bib90)].
    DPI-T dataset was also used in the training. To overcome YOLOv2’s problem of generating
    overlapping bounding boxes, they proposed bounding box regression (BBR) based
    on mean shift clustering of the center points of bounding boxes. A simple position-based
    bounding box angle determination was added to the refinement process. The authors
    manually annotated MW-18Mar, PIROPO, Bomni and CVRG for evaluation, as mentioned
    in [Sec. 3.1](#S3.SS1 "3.1 Real-world datasets ‣ 3 Datasets ‣ Applications of
    Deep Learning for Top-View Omnidirectional Imaging: A Survey").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tamura *等* 尝试通过训练 YOLOv2 来实现全景图像中的行人检测，使用了来自 COCO [[90](#bib.bib90)] 的随机旋转视角图像。训练中还使用了
    DPI-T 数据集。为了解决 YOLOv2 生成重叠边界框的问题，他们提出了基于边界框中心点均值漂移聚类的边界框回归（BBR）。在细化过程中添加了基于位置的边界框角度确定。作者手动注释了
    MW-18Mar、PIROPO、Bomni 和 CVRG 以供评估，如 [第 3.1 节](#S3.SS1 "3.1 Real-world datasets
    ‣ 3 Datasets ‣ Applications of Deep Learning for Top-View Omnidirectional Imaging:
    A Survey") 中提到。'
- en: Duan *et al*. proposed RAPiD [[27](#bib.bib27)], a new YOLO-inspired network
    architechture, which predicts the rotation angle besides the usual position and
    size of the bounding boxes. To train this network they added rotation-angle loss
    to the loss function of YOLOv3. There network is first pre-trained on COCO, then
    finetuned on two of the three datasets they annotated (MW-R, HABBOF, CEPDOF) and
    tested on the remaining one. They reached AP@0.50IoU of 0.967, 0.981 and 0.858
    for MW-R, HABBOF and CEPDOF, respectively. Their team further improved the performance
    by extending RAPiD with temporal information [[91](#bib.bib91)]. Minh *et al*.
    used the same strategy to extend CenterNet to predict human aligned bounding boxes,
    and named their architecture ARPD [[64](#bib.bib64)]. Their experiments show that
    their method reaches similar AP as RAPiD while doubling the inference speed. Haggui
    *et al*. used RAPiD for initial detection and added tracking by using the color
    histograms [[37](#bib.bib37)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 段*等*提出了RAPiD [[27](#bib.bib27)]，这是一种新的YOLO灵感网络架构，它除了预测边界框的常规位置和大小外，还预测旋转角度。为了训练这个网络，他们在YOLOv3的损失函数中添加了旋转角度损失。该网络首先在COCO数据集上进行预训练，然后在他们标注的三个数据集中的两个（MW-R、HABBOF、CEPDOF）上进行微调，并在剩下的一个数据集上进行测试。他们在MW-R、HABBOF和CEPDOF数据集上分别达到了AP@0.50IoU值0.967、0.981和0.858。他们的团队通过扩展RAPiD以引入时间信息进一步提高了性能 [[91](#bib.bib91)]。敏*等*使用了相同的策略将CenterNet扩展为预测对齐的人体边界框，并将其架构命名为ARPD [[64](#bib.bib64)]。他们的实验表明，他们的方法在达到类似的AP的同时，推理速度提高了两倍。哈吉*等*使用RAPiD进行初步检测，并通过使用颜色直方图来添加跟踪功能 [[37](#bib.bib37)]。
- en: The most recent development is [[100](#bib.bib100)]. Wiedemar *et al*. proposed
    a few-shot adversarial training scheme for Faster-RCNN [[77](#bib.bib77)] so that
    a pre-trained detection model can be adapted for person detection in top-view
    omnidirectional images with less than 100 annotated training samples. The techniques
    they used include loss coupling, global and instance level feature alignment.
    Their method can achieve higher accuracy when the number of annotated samples
    are smaller than 100. A key difference to previously mentioned methods is that
    this method is aimed at adapting an existing model with minimum amount of effort
    to a certain use case, instead of trying to create a model with maximum generalization
    power. Therefore, cross-dataset evaluation by the author shows that the model
    loses generalization power when the number of training examples exceed 50.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的发展是[[100](#bib.bib100)]。维德马*等*提出了一种针对Faster-RCNN的少样本对抗训练方案 [[77](#bib.bib77)]，使得一个预训练的检测模型可以在少于100个标注训练样本的情况下，适应于顶视全向图像中的人检测。他们使用的技术包括损失耦合、全局和实例级别特征对齐。该方法在标注样本数量少于100时可以达到更高的准确率。与之前提到的方法的一个关键区别在于，这种方法旨在用最小的努力将现有模型适应于特定的使用场景，而不是试图创建一个具有最大泛化能力的模型。因此，作者的跨数据集评估显示，当训练样本数量超过50时，该模型的泛化能力会下降。
- en: Besides the common methods for detection, researches have experimented with
    other ways of person detection with special features. Wang *et al*. [[98](#bib.bib98)]
    proposed to use Mask-RCNN [[41](#bib.bib41)]. The advantage is that segmentation
    mask does not have the problem of not aligning with the orientation of the person.
    They divided the images into a central region and a peripheral ring. The peripheral
    ring is further divided into three sectors, which are then warped into rectangles
    and stacked together to form one square image. The detection is performed using
    two separate detectors, one for the central region and the other for the outer
    region. Fuertes *et al*. proposed a Grid of Spatial-Aware Classifiers [[21](#bib.bib21)]
    based on Deep Neural Networks (GSAC-DNN) [[32](#bib.bib32)]. A feature map is
    generated by a ResNet-32 [[42](#bib.bib42)] backbone. It is fed to a 2D grid of
    simple classifiers consisting of a convolution layer and a linear layer. The location
    of the person is calculated based on the confidence scores of the classifiers.
    GSAC-DNN is end-to-end trainable, however it can only detect the general position
    of the person but not a bounding box. The work of Callemein *et al*. [[9](#bib.bib9)]
    is intended for occupancy detection in meeting rooms or for flex-desking, yet
    the detection results are still presented as bounding boxes. They use extremely
    low resolution images of $96\times 96$ pixels to preserve privacy. To compensate
    for the information loss caused by the low resolution, they implemented a temporal
    interlacing kernel, which combines multiple consecutive frames into one high resolution
    feature map. Their network is able to run on embedded systems such as the Raspberry
    Pi 3B [[31](#bib.bib31)] at $0.77\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$. Pais
    *et al*. used a deep Q-Net (DQN)-based [[65](#bib.bib65)] network and the camera
    calibration parameters to perform person detection and predict the 3D position
    of the person in the world coordinate [[71](#bib.bib71)]. The employment of reinforced
    learning is quite unusual. Their implementation is named OmniDRL.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了常见的检测方法外，研究人员还尝试了具有特殊功能的其他人员检测方式。Wang *等* 提出了使用 Mask-RCNN [[41](#bib.bib41)]。其优势在于分割掩码不具有与人员方向对不上的问题。他们将图像分为中心区域和外围环带。外围环带进一步分为三个扇区，然后将这些扇区扭曲成矩形并叠加在一起形成一个正方形图像。检测通过两个独立的检测器进行，一个用于中心区域，另一个用于外部区域。Fuertes
    *等* 提出了基于深度神经网络（GSAC-DNN）的空间感知分类器网格 [[32](#bib.bib32)]。该方法由一个 ResNet-32 [[42](#bib.bib42)]
    主干生成特征图。该特征图输入到由卷积层和线性层组成的二维简单分类器网格中。基于分类器的置信度评分计算人员的位置。GSAC-DNN 是端到端可训练的，但它只能检测人员的一般位置，而不能生成边界框。Callemein
    *等* 的工作 [[9](#bib.bib9)] 旨在用于会议室的占用检测或灵活办公桌，但检测结果仍以边界框呈现。他们使用分辨率极低的 $96\times
    96$ 像素图像以保护隐私。为了补偿低分辨率带来的信息损失，他们实现了一个时间交错内核，将多个连续帧合并为一个高分辨率特征图。他们的网络能够在如 Raspberry
    Pi 3B [[31](#bib.bib31)] 这样的嵌入式系统上以 $0.77\text{\,}\mathrm{f}\mathrm{p}\mathrm{s}$
    运行。Pais *等* 使用基于深度 Q-Net (DQN) [[65](#bib.bib65)] 的网络和相机标定参数来进行人员检测，并预测人员在世界坐标系中的
    3D 位置 [[71](#bib.bib71)]。使用强化学习的做法相当不寻常。他们的实现称为 OmniDRL。
- en: 4.2 Object detection
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 对象检测
- en: 'Object detection in omnidirectional images has not been widely researched.
    One reason is that it is not as useful as person detection. Another reason is
    the lack of data for training. Scheck *et al*. created the THEODORE dataset [[81](#bib.bib81)]
    to solve this issue. THEODORE contains five classes besides person: armchair,
    chair, table, TV and wheeled walker. They trained SSD [[61](#bib.bib61)], R-FCN [[20](#bib.bib20)]
    and Faster R-CNN [[77](#bib.bib77)] using this dataset and tested the trained
    networks on FES dataset. The mAP for all six classes reached 0.613 with Faster
    R-CNN. They used THEODORE to further train the anchorless CenterNet [[26](#bib.bib26)]
    for object detection [[80](#bib.bib80)]. In this work, they introduced unsupervised
    domain adaptation (UDA) to bridge the gap between synthetic image domain and real-world
    image domain, which is typically used in semantic segmentation. CenterNet was
    extended with two methods of UDA: entropy minimization (EM) [[95](#bib.bib95)]
    and maximum squares loss (MSL) [[14](#bib.bib14)]. The unlabeled target dataset
    used in [[80](#bib.bib80)] is CEPDOF. With the UDA-extended CenterNet they raised
    the mAP on the same FES dataset to 0.690 and doubled the inference speed at the
    same time. Another reason that object detection in omnidirectional view is underexplored
    is perhaps the very limited use cases. However, this could still be useful for
    accomplishing complex tasks with omnidirectional images, such as action recognition
    for smart monitoring systems, for example, in [[84](#bib.bib84)], a system is
    built for monitoring elderlies with dementia.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在全景图像中的物体检测尚未被广泛研究。一个原因是它不像人检测那样有用。另一个原因是缺乏用于训练的数据。Scheck *等人*创建了THEODORE数据集[[81](#bib.bib81)]来解决这个问题。THEODORE除了包含人类之外，还包含五个类别：扶手椅、椅子、桌子、电视和轮椅。他们使用该数据集训练了SSD[[61](#bib.bib61)]、R-FCN[[20](#bib.bib20)]和Faster
    R-CNN[[77](#bib.bib77)]，并在FES数据集上测试了训练后的网络。使用Faster R-CNN，所有六个类别的mAP达到了0.613。他们还利用THEODORE进一步训练了无锚点的CenterNet[[26](#bib.bib26)]用于物体检测[[80](#bib.bib80)]。在这项工作中，他们引入了无监督领域适应（UDA）来弥合合成图像领域和真实图像领域之间的差距，这通常用于语义分割。CenterNet通过两种UDA方法扩展：熵最小化（EM）[[95](#bib.bib95)]和最大平方损失（MSL）[[14](#bib.bib14)]。在[[80](#bib.bib80)]中使用的无标签目标数据集是CEPDOF。利用扩展了UDA的CenterNet，他们将相同FES数据集上的mAP提升至0.690，并同时将推断速度提高了一倍。另一个物体检测在全景视图中被研究不足的原因可能是应用场景非常有限。然而，这仍然可能对完成复杂任务有用，如使用全景图像进行动作识别，例如，在[[84](#bib.bib84)]中，建立了一个监控老年痴呆症患者的系统。
- en: 5 Human pose estimation and activity recognition
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 人体姿态估计与活动识别
- en: Human pose estimation (HPE) refers to the process of finding the joints of a
    person and connecting them into a skeleton. Pose estimation is the second most
    researched application of omnidirectional images. In this section we take a look
    at the approaches for 2D and 3D pose estimation. We also check out the researches
    for Human Activity Recognition (HAR), which often follows pose estimation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计（HPE）是指找到一个人的关节并将其连接成一个骨架的过程。姿态估计是全景图像应用中研究第二多的领域。在这一部分，我们将探讨2D和3D姿态估计的方法。我们还将查看人体活动识别（HAR）的研究，这通常跟随姿态估计。
- en: 5.1 Pose estimation with overhead fisheye camera
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 使用顶视鱼眼相机进行姿态估计
- en: Georgakopoulos *et al*. [[22](#bib.bib22), [36](#bib.bib36)] employ a 3D human
    model to create a dataset of binary silhouettes, which are rendered through the
    calibration of a fisheye camera. The CNN is trained to differentiate between the
    pre-set postures, rather than estimate the joint positions. Denecke and Jauch [[25](#bib.bib25)]
    use the 3D point cloud calculated by the smart sensor and prior knowledge of the
    human body to estimate the joint positions. The results of this method are restricted
    by factors such as the mounting position of the camera and differences between
    each individual body. The inference speed is limited by the speed of the smart
    sensor. Heindl *et al*. [[43](#bib.bib43)] generated rectilinear views of the
    area that contains human in an omnidirectional image, thus the person appears
    upright in the virtual view. OpenPose [[10](#bib.bib10)] is then applied to this
    virtual image to perform pose estimation. They used a pair of calibrated fisheye
    cameras to get two skeletons, which then are combined into a 3D skeleton by using
    Direct Linear Transform (DLT) [[40](#bib.bib40)] in the rectilinear views.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Georgakopoulos *et al*. [[22](#bib.bib22), [36](#bib.bib36)] 使用3D人体模型创建二值轮廓数据集，这些数据集通过鱼眼相机的校准渲染。CNN被训练以区分预设姿势，而不是估计关节位置。Denecke和Jauch
    [[25](#bib.bib25)] 利用智能传感器计算的3D点云和人体的先验知识来估计关节位置。这种方法的结果受限于相机的安装位置以及个体身体之间的差异。推理速度受限于智能传感器的速度。Heindl
    *et al*. [[43](#bib.bib43)] 生成包含人类的全向图像区域的直线视图，从而使人类在虚拟视图中看起来是直立的。然后，将OpenPose
    [[10](#bib.bib10)] 应用于这个虚拟图像以进行姿态估计。他们使用了一对校准的鱼眼相机来获取两个骨架，然后通过使用直线变换（DLT） [[40](#bib.bib40)]
    将这些骨架组合成3D骨架。
- en: Though not using omnidirectional images, the following two works perform HPE
    for the top-view. Haque *et al*. train CNN and LSTM [[44](#bib.bib44)] to achieve
    view-point invariant 3d pose estimation on a singular *depth image* [[38](#bib.bib38),
    [39](#bib.bib39)]. Garau *et al*. achieve viewpoint-invariant 3D HPE with a capsule
    auto-encoder named DECA [[33](#bib.bib33)] on depth and RGB images, namely ITOP [[39](#bib.bib39)]
    and PanopTOP31K datasets.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有使用全向图像，以下两个工作在顶视图中进行HPE。Haque *et al*. 训练CNN和LSTM [[44](#bib.bib44)] 以实现单一*深度图像*
    [[38](#bib.bib38), [39](#bib.bib39)]上的视角不变3D姿态估计。Garau *et al*. 使用名为DECA的胶囊自编码器
    [[33](#bib.bib33)] 在深度图像和RGB图像上实现视角不变的3D HPE，即ITOP [[39](#bib.bib39)] 和PanopTOP31K数据集。
- en: 5.2 Egocentric 3D pose estimation
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 自我中心的3D姿态估计
- en: Egocentric pose estimation is a special case of using fisheye cameras in the
    top-view. The camera is not mounted over the person, instead, it is mounted with
    an apparatus on the head of the person with a small horizontal distance. Visible
    to the camera is the front or the frontal side of the body, as well as the peripheral
    of the person. EgoCap is a dual fisheye camera set-up mounted on a bike helmet
    with either a T-shaped or a Y-shaped wooden frame [[78](#bib.bib78)]. The cameras
    extrude about $25\text{\,}\mathrm{cm}$ to the front of the carrier. The authors
    created a dataset using a motion capture system to create the ground truth and
    projected the joint locations into the images from their set-up. With this dataset
    they finetuned ResNet101, which is pre-trained on MPII [[4](#bib.bib4)] and Leed
    Sports Extended Dataset [[46](#bib.bib46)], to generate heatmaps for 18 joints.
    The 3D skeleton is constructed in real-time from the 2D skeleton and a 3D body
    model, which must be adjusted for each user. Mo²Cap² [[103](#bib.bib103)] and
    xR-EgoPose [[93](#bib.bib93)] / SelfPose [[92](#bib.bib92)] are similar implementations
    with a single fisheye camera. Both works developed their own synthetic datasets
    for training. Mo²Cap² used one branch of CNN to generate heatmaps of joints for
    the whole body and another branch for the zoomed-in lower body. With the help
    of a CNN that estimates the distance between the joints and the camera, the joints
    are finally projected into the 3D coordinates. The calibration information of
    the cameras are essential for accurate 3D pose estimation. xR-Egopose used ResNet101
    for joint heatmap generation. A lifting module takes the heatmaps as input and
    regresses the 3D pose from them as well as outputs the 2D heatmaps in high resolution.
    Wang *et al*. proposed in [[97](#bib.bib97)] a method for estimating not only
    the local pose, but also the global pose, which means the 3D joint positions in
    the world coordinate system are estimated. Their pipeline makes use of image sequences
    instead of inferencing on a single frame. At the same time, they utilized motion
    prior, which is learned from AMASS dataset [[63](#bib.bib63)], to reduce temporal
    jitter and unrealistic motions from the estimated poses. Their set-up is similar
    to Mo²Cap² and xR-EgoPose by mounting a single fisheye camera onto a helmet. In
    [[96](#bib.bib96)] they further proposed to additionally use an external camera
    for weakly supervised training. Their dataset for this task is named EgoPW. EgoGlass [[108](#bib.bib108)]
    is more extreme in terms of minimizing the apparatus size. They mounted two cameras
    to a normal eyeglass, each of which records a side of the body. The pose estimation
    is solved in the stitched-together image. We also notice Cha *et al*. [[11](#bib.bib11),
    [12](#bib.bib12)] used similar set-ups for their implementation, but not fisheye
    cameras.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 自我中心姿态估计是使用鱼眼相机在顶视图中的一种特殊情况。相机并未安装在人的正上方，而是通过一个装置安装在人头部的小水平距离上。相机可以看到的是身体的前面或前侧，以及人的周边。EgoCap
    是一种双鱼眼相机系统，安装在自行车头盔上，配有 T 形或 Y 形木框架[[78](#bib.bib78)]。相机从支架的前方伸出约 $25\text{\,}\mathrm{cm}$。作者使用运动捕捉系统创建了一个数据集，以生成地面真实数据，并将关节位置投影到他们设置的图像中。利用这个数据集，他们对预训练于
    MPII [[4](#bib.bib4)] 和 Leed Sports Extended Dataset [[46](#bib.bib46)] 的 ResNet101
    进行了微调，以生成 18 个关节的热图。通过 2D 骨架和 3D 身体模型实时构建 3D 骨架，每个用户必须调整 3D 模型。Mo²Cap² [[103](#bib.bib103)]
    和 xR-EgoPose [[93](#bib.bib93)] / SelfPose [[92](#bib.bib92)] 是类似的实现，使用了单个鱼眼相机。两个工作都为训练开发了自己的合成数据集。Mo²Cap²
    使用了一个 CNN 分支来生成全身关节的热图，另一个分支生成放大下半身的热图。通过估计关节与相机之间距离的 CNN，最终将关节投影到 3D 坐标中。相机的校准信息对于准确的
    3D 姿态估计至关重要。xR-Egopose 使用了 ResNet101 生成关节热图。一个提升模块以热图作为输入，从中回归 3D 姿态，并输出高分辨率的
    2D 热图。Wang *et al* 在 [[97](#bib.bib97)] 中提出了一种方法，不仅估计局部姿态，还估计全局姿态，即估计世界坐标系统中的
    3D 关节位置。他们的流程利用了图像序列，而不是仅对单帧进行推断。同时，他们利用了从 AMASS 数据集 [[63](#bib.bib63)] 学习到的运动先验，以减少估计姿态中的时间抖动和不现实的动作。他们的设置类似于
    Mo²Cap² 和 xR-EgoPose，通过将单个鱼眼相机安装到头盔上来实现。在 [[96](#bib.bib96)] 中，他们进一步提出了额外使用外部相机进行弱监督训练。他们的任务数据集名为
    EgoPW。EgoGlass [[108](#bib.bib108)] 在最小化设备尺寸方面更为极端。他们将两台相机安装到普通眼镜上，每台相机记录身体的一侧。姿态估计在拼接图像中解决。我们还注意到
    Cha *et al* [[11](#bib.bib11), [12](#bib.bib12)] 使用了类似的设置进行他们的实现，但不是鱼眼相机。
- en: 5.3 Action recognition
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 动作识别
- en: Li *et al*. [[53](#bib.bib53)] proposed to perform action recognition in top-view
    fisheye camera images. They first use Mask-RCNN to find the spine lines of standing
    persons in the image. The cross point of the spine lines are deemed the optical
    center of the fisheye camera and the spherical image is dewarped into a panoramic
    image around it. Camera calibration information is not necessary in this process
    and the panoramic image is set to a pre-defined size. The authors use Mask-RCNN
    to perform person detection in the panoramic image and max pool the bounding boxes
    across 16 frames in each clip to form the ROIs. A 3D ResNet is used for action
    recognition through the 16 frames. A binary mask, which is generated from the
    ROIs, is multiplied with the feature maps from the 3D ResNet to reduce calculation
    cost. They used Multi-instance Multi-label Learning (MIML) to train a network
    for estimating scores for a series of actions. This work is further developed
    by Stephen *et al*. [[88](#bib.bib88)] by adding a second parallel pipeline for
    persons in the central area. Instead of using the panoramic view, this pipeline
    directly generates stacked feature maps for each person in the omnidirectional
    image, in which person detection is performed by RAPiD [[27](#bib.bib27)].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Li *等* [[53](#bib.bib53)] 提出了在顶视鱼眼摄像机图像中进行动作识别的方法。他们首先使用Mask-RCNN找出图像中站立人的脊柱线。脊柱线的交点被视为鱼眼摄像机的光学中心，球面图像在其周围被去畸变成全景图像。在此过程中无需相机标定信息，全景图像被设置为预定义大小。作者使用Mask-RCNN在全景图像中进行人物检测，并在每个剪辑的16帧中最大池化边界框以形成ROI。通过这16帧使用3D
    ResNet进行动作识别。由ROI生成的二进制掩码与3D ResNet的特征图相乘以降低计算成本。他们使用多实例多标签学习（MIML）训练网络，以估计一系列动作的分数。Stephen
    *等* [[88](#bib.bib88)] 进一步发展了这项工作，通过在中央区域添加第二个并行管道来处理人员。这个管道直接为广角图像中的每个人生成堆叠的特征图，其中人物检测由RAPiD
    [[27](#bib.bib27)] 执行。
- en: 6 Other applications
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 其他应用
- en: Except for the intensively researched topics, multiple applications of omnidirectional
    images exist, mainly due to its wider FOV. Researchers have applied deep learning
    to these applications, however, DL does not stand in the focus in these applications.
    Much effort is given to solve the unique challenge of the equidistant projection
    of fisheye cameras, as well as other domain-specific problems.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 除了被广泛研究的主题，广角图像还有多种应用，主要是由于其更广的视野。研究人员已将深度学习应用于这些应用中，但深度学习并不是这些应用的重点。大量的工作致力于解决鱼眼摄像机的等距投影以及其他特定领域的问题。
- en: 'Laurendin *et al*. proposed to use a top-view camera for anomaly detection
    in train door area for autonomous trains [[52](#bib.bib52)]. They created a dataset,
    in which the door area of a train is simulated, and systematically annotated it,
    see [Sec. 3.1](#S3.SS1 "3.1 Real-world datasets ‣ 3 Datasets ‣ Applications of
    Deep Learning for Top-View Omnidirectional Imaging: A Survey"). They adapted the
    network in [[68](#bib.bib68)]. The results are inconclusive. Kim *et al*. proposed
    to use multiple top-view fisheye cameras for parking lot surveillance to determine
    vehicle positions [[48](#bib.bib48)]. The cameras were first calibrated using
    RANSAC to obtain their intrinsic and extrinsic parameters. SegNet[[6](#bib.bib6)]
    was used to generate segmentation masks for vehicle detections. They developed
    a method to estimate the actual size of the vehicle based on the calibration parameters
    of the camera and the generated segmentation mask. To get precise groundtruth
    data, the authors built a $1/18$-scale test bench using model cars and wooden
    frames. Their method was tested with an average distance error of $0.24\text{\,}\mathrm{m}$
    (scaled to real-life) for vehicle position estimation and an average direction
    error of $4.8\text{\,}\mathrm{\SIUnitSymbolDegree}$ for vehicle moving direction.
    Akai *et al*. used a fisheye camera for grape bunch counting [[3](#bib.bib3)].
    In contrary to other examples reviewed in this paper, this work uses the bottom-up
    view instead of the top-down view. But this is the same approach in essence. The
    difference of viewpoint is just because the ROI is over the head instead of on
    the ground. Another important application is indoor livestock monitoring, such
    as body segmentation, identification, behavior recognition. Using top-view omnidirectional
    cameras in such farming areas, which are usually large and densely packed with
    animals, provides an unoccluded view with minimum number of cameras. Chen *et
    al*. [[13](#bib.bib13)] provide a comprehensive review of this application area,
    including the use of top-view omnidirectional cameras and deep learning. Li *et
    al*. performed 3D room reconstruction using a single top-view fisheye camera [[55](#bib.bib55)].
    They used RefineNet [[59](#bib.bib59)] for semantic segmentation of the room to
    aid structural line selection. The final result of their method is a cuboid representation
    of the room.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Laurendin *等人* 提出了使用顶视摄像头在自动列车的车门区域进行异常检测 [[52](#bib.bib52)]。他们创建了一个数据集，其中模拟了列车的车门区域，并进行了系统的标注，详见
    [第 3.1 节](#S3.SS1 "3.1 现实世界数据集 ‣ 3 数据集 ‣ 顶视全向成像中的深度学习应用综述")。他们在 [[68](#bib.bib68)]
    中调整了网络。结果尚不明确。Kim *等人* 提出了使用多个顶视鱼眼摄像头进行停车场监控，以确定车辆位置 [[48](#bib.bib48)]。这些摄像头首先使用
    RANSAC 进行标定，以获取其内参和外参。SegNet [[6](#bib.bib6)] 被用来生成车辆检测的分割掩码。他们开发了一种基于摄像头的标定参数和生成的分割掩码来估计车辆实际大小的方法。为了获得准确的真实数据，作者使用模型车和木框建造了一个
    $1/18$ 比例的测试台。他们的方法在车辆位置估计方面的平均距离误差为 $0.24\text{\,}\mathrm{m}$（按实际情况缩放），在车辆移动方向上的平均方向误差为
    $4.8\text{\,}\mathrm{\SIUnitSymbolDegree}$。Akai *等人* 使用鱼眼摄像头进行葡萄串计数 [[3](#bib.bib3)]。与本文中其他审查的示例不同，这项工作使用的是自下而上的视角，而不是自上而下的视角。但从本质上讲，这是一种相同的方法。视角的不同只是因为
    ROI 位于头顶而不是地面上。另一个重要的应用是室内牲畜监控，如身体分割、识别、行为识别。在这种通常规模较大且动物密集的农业区域中使用顶视全向摄像头，可以提供无遮挡的视角，同时使用的摄像头数量最少。Chen
    *等人* [[13](#bib.bib13)] 提供了这一应用领域的全面综述，包括顶视全向摄像头和深度学习的使用。Li *等人* 使用单个顶视鱼眼摄像头进行了
    3D 房间重建 [[55](#bib.bib55)]。他们使用 RefineNet [[59](#bib.bib59)] 对房间进行语义分割，以帮助选择结构线。他们方法的最终结果是房间的立方体表示。
- en: 7 Conclusion
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We can conclude from our survey that the main application areas of top-view
    omnidirectional imaging are surveillance and AAL. Researchers have created a considerable
    amount of data to facilitate the development of deep learning algorithms. With
    this, the implementations of deep learning algorithms show very promising results.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从我们的调查中得出结论，顶视全向成像的主要应用领域是监控和辅助生活。研究人员已经创建了大量的数据来促进深度学习算法的发展。通过这些，深度学习算法的实施显示出了非常有前途的结果。
- en: An obviously underexplored research area is human pose estimation. Though it
    has been greatly advanced for perspective images, the transfer to omnidirectional
    images is slow due to the high expense related to collecting human keypoints data
    with reliable ground truth [[45](#bib.bib45)]. The recent development in novel-view
    synthesis such as A-Nerf [[89](#bib.bib89)] and HumanNerf [[99](#bib.bib99)] could
    be the solution to this problem. More researches in this area will benefit further
    applications, such as fall detection, where only rule-based methods have been
    explored [[23](#bib.bib23), [49](#bib.bib49), [87](#bib.bib87), [69](#bib.bib69)].
    Another possible research direction could be the usage of network architectures
    that are specifically developed for the geometry of fisheye images and related
    projections, such as spherical CNNs [[16](#bib.bib16)] and SphereNet [[18](#bib.bib18)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显然被忽视的研究领域是人体姿态估计。虽然在透视图像上已有很大进展，但由于收集可靠地面真实数据的人体关键点的高成本，将其转移到全景图像上进展缓慢[[45](#bib.bib45)]。最近在新视角合成方面的发展，如A-Nerf
    [[89](#bib.bib89)] 和 HumanNerf [[99](#bib.bib99)]，可能是解决这个问题的方案。这个领域的更多研究将有利于进一步的应用，比如跌倒检测，目前只探索了基于规则的方法[[23](#bib.bib23),
    [49](#bib.bib49), [87](#bib.bib87), [69](#bib.bib69)]。另一个可能的研究方向是使用专门为鱼眼图像几何及相关投影开发的网络架构，如球面卷积神经网络
    [[16](#bib.bib16)] 和 SphereNet [[18](#bib.bib18)]。
- en: References
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Misbah Ahmad, Imran Ahmed, Kaleem Ullah, Iqbal khan, Ayesha Khattak, and
    Awais Adnan. Person detection from overhead view: A survey. International Journal
    of Advanced Computer Science and Applications, 10(4), 2019. Copyright - © 2019\.
    This work is licensed under https://creativecommons.org/licenses/by/4.0/ (the
    “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this
    content in accordance with the terms of the License; Last updated - 2022-11-29.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Misbah Ahmad, Imran Ahmed, Kaleem Ullah, Iqbal khan, Ayesha Khattak, 和
    Awais Adnan. 从俯视图进行人员检测：综述。《高级计算机科学与应用国际期刊》，10(4)，2019年。版权 - © 2019。本作品依据 https://creativecommons.org/licenses/by/4.0/（“许可证”）授权使用。尽管有ProQuest的条款和条件，你可以按照许可证的条款使用该内容；最后更新
    - 2022-11-29。'
- en: '[2] Hao Ai, Zidong Cao, Jinjing Zhu, Haotian Bai, Yucheng Chen, and Lin Wang.
    Deep learning for omnidirectional vision: A survey and new perspectives, 2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Hao Ai, Zidong Cao, Jinjing Zhu, Haotian Bai, Yucheng Chen, 和 Lin Wang.
    针对全景视觉的深度学习：综述与新视角，2022年。'
- en: '[3] Ryota Akai, Yuzuko Utsumi, Yuka Miwa, Masakazu Iwamura, and Koichi Kise.
    Distortion-adaptive grape bunch counting for omnidirectional images. In 2020 25th
    International Conference on Pattern Recognition (ICPR), pages 599–606\. IEEE,
    2021.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Ryota Akai, Yuzuko Utsumi, Yuka Miwa, Masakazu Iwamura, 和 Koichi Kise.
    针对全景图像的畸变自适应葡萄串计数。在2020年第25届国际模式识别大会（ICPR）上，页码599–606。IEEE，2021年。'
- en: '[4] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.
    2d human pose estimation: New benchmark and state of the art analysis. In Proceedings
    of the IEEE Conference on computer Vision and Pattern Recognition, pages 3686–3693,
    2014.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, 和 Bernt Schiele. 2D人体姿态估计：新的基准和最新技术分析。在IEEE计算机视觉与模式识别会议论文集中，页码3686–3693，2014年。'
- en: '[5] Nerea Aranjuelo, Sara García, Estíbaliz Loyo, Luis Unzueta, and Oihana
    Otaegui. Key strategies for synthetic data generation for training intelligent
    systems based on people detection from omnidirectional cameras. Computers & Electrical
    Engineering, 92:107105, 2021.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Nerea Aranjuelo, Sara García, Estíbaliz Loyo, Luis Unzueta, 和 Oihana Otaegui.
    基于全景相机的人体检测的智能系统训练合成数据生成的关键策略。《计算机与电气工程》，92:107105，2021年。'
- en: '[6] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep
    convolutional encoder-decoder architecture for image segmentation. IEEE transactions
    on pattern analysis and machine intelligence, 39(12):2481–2495, 2017.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Vijay Badrinarayanan, Alex Kendall, 和 Roberto Cipolla. Segnet：一种用于图像分割的深度卷积编码解码器架构。《IEEE模式分析与机器智能交易》，39(12):2481–2495，2017年。'
- en: '[7] Oleksandr Bogdan, Viktor Eckstein, Francois Rameau, and Jean-Charles Bazin.
    Deepcalib: a deep learning approach for automatic intrinsic calibration of wide
    field-of-view cameras. In Proceedings of the 15th ACM SIGGRAPH European Conference
    on Visual Media Production, pages 1–10, 2018.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Oleksandr Bogdan, Viktor Eckstein, Francois Rameau, 和 Jean-Charles Bazin.
    Deepcalib：一种用于广角相机自动内参标定的深度学习方法。在第15届ACM SIGGRAPH欧洲视觉媒体制作会议论文集中，页码1–10，2018年。'
- en: '[8] Josep Bosch, Klemen Istenič, Nuno Gracias, Rafael Garcia, and Pere Ridao.
    Omnidirectional multicamera video stitching using depth maps. IEEE Journal of
    Oceanic Engineering, 45(4):1337–1352, 2020.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Josep Bosch, Klemen Istenič, Nuno Gracias, Rafael Garcia, 和 Pere Ridao。使用深度图的全向多摄像头视频拼接。IEEE海洋工程杂志，45(4):1337–1352，2020年。'
- en: '[9] Timothy Callemein, Kristof Van Beeck, and Toon Goedemé. Anyone here? smart
    embedded low-resolution omnidirectional video sensor to measure room occupancy.
    In 2019 18th IEEE International Conference On Machine Learning And Applications
    (ICMLA), pages 1993–2000\. IEEE, 2019.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Timothy Callemein, Kristof Van Beeck, 和 Toon Goedemé。这里有人吗？智能嵌入式低分辨率全向视频传感器用于测量房间占用情况。发表于2019年第18届IEEE国际机器学习与应用会议（ICMLA），第1993–2000页。IEEE，2019年。'
- en: '[10] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person
    2d pose estimation using part affinity fields. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 7291–7299, 2017.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zhe Cao, Tomas Simon, Shih-En Wei, 和 Yaser Sheikh。实时多人的2D姿态估计使用部件亲和场。发表于IEEE计算机视觉与模式识别会议论文集，第7291–7299页，2017年。'
- en: '[11] Young-Woon Cha, True Price, Zhen Wei, Xinran Lu, Nicholas Rewkowski, Rohan
    Chabra, Zihe Qin, Hyounghun Kim, Zhaoqi Su, Yebin Liu, et al. Towards fully mobile
    3d face, body, and environment capture using only head-worn cameras. IEEE transactions
    on visualization and computer graphics, 24(11):2993–3004, 2018.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Young-Woon Cha, True Price, Zhen Wei, Xinran Lu, Nicholas Rewkowski, Rohan
    Chabra, Zihe Qin, Hyounghun Kim, Zhaoqi Su, Yebin Liu，等。实现完全移动的3D面部、身体和环境捕捉，仅使用头戴式相机。IEEE视觉化与计算机图形学事务，24(11):2993–3004，2018年。'
- en: '[12] Young-Woon Cha, Husam Shaik, Qian Zhang, Fan Feng, Andrei State, Adrian
    Ilie, and Henry Fuchs. Mobile. egocentric human body motion reconstruction using
    only eyeglasses-mounted cameras and a few body-worn inertial sensors. In 2021
    IEEE Virtual Reality and 3D User Interfaces (VR), pages 616–625\. IEEE, 2021.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Young-Woon Cha, Husam Shaik, Qian Zhang, Fan Feng, Andrei State, Adrian
    Ilie, 和 Henry Fuchs。移动式以眼镜为基础的人体运动重建，仅使用眼镜-mounted 相机和少量佩戴的惯性传感器。发表于2021 IEEE虚拟现实与3D用户界面（VR），第616–625页。IEEE，2021年。'
- en: '[13] Chen Chen, Weixing Zhu, and Tomas Norton. Behaviour recognition of pigs
    and cattle: Journey from computer vision to deep learning. Computers and Electronics
    in Agriculture, 187:106255, 2021.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Chen Chen, Weixing Zhu, 和 Tomas Norton。猪和牛的行为识别：从计算机视觉到深度学习的旅程。《计算机与电子农业》，187:106255，2021年。'
- en: '[14] Minghao Chen, Hongyang Xue, and Deng Cai. Domain adaptation for semantic
    segmentation with maximum squares loss. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 2090–2099, 2019.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Minghao Chen, Hongyang Xue, 和 Deng Cai。最大平方损失的语义分割领域适应。发表于IEEE/CVF国际计算机视觉会议论文集，第2090–2099页，2019年。'
- en: '[15] Sheng-Ho Chiang, Tsaipei Wang, and Yi-Fu Chen. Efficient pedestrian detection
    in top-view fisheye images using compositions of perspective view patches. Image
    and Vision Computing, 105:104069, 2021.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Sheng-Ho Chiang, Tsaipei Wang, 和 Yi-Fu Chen。在顶部视角鱼眼图像中使用透视视图补丁的组合进行高效行人检测。《图像与视觉计算》，105:104069，2021年。'
- en: '[16] Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical
    CNNs. In International Conference on Learning Representations, 2018.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Taco S. Cohen, Mario Geiger, Jonas Köhler, 和 Max Welling。球面CNN。发表于2018年国际学习表征会议。'
- en: '[17] Mertcan Cokbas, John Bolognino, Janusz Konrad, and Prakash Ishwar. Frida:
    Fisheye re-identification dataset with annotations. In 2022 18th IEEE International
    Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1–8,
    2022.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Mertcan Cokbas, John Bolognino, Janusz Konrad, 和 Prakash Ishwar。Frida：带注释的鱼眼重识别数据集。发表于2022年第18届IEEE国际先进视频与信号监测会议（AVSS），第1–8页，2022年。'
- en: '[18] Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. Spherenet:
    Learning spherical representations for detection and classification in omnidirectional
    images. In Proceedings of the European conference on computer vision (ECCV), pages
    518–533, 2018.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Benjamin Coors, Alexandru Paul Condurache, 和 Andreas Geiger。Spherenet：学习球面表征以进行全向图像的检测和分类。发表于欧洲计算机视觉会议（ECCV）论文集，第518–533页，2018年。'
- en: '[19] Thiago L. T. da Silveira, Paulo G. L. Pinto, Jeffri Murrugarra-Llerena,
    and Cláudio R. Jung. 3d scene geometry estimation from 360° imagery: A survey.
    ACM Comput. Surv., 55(4), nov 2022.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Thiago L. T. da Silveira, Paulo G. L. Pinto, Jeffri Murrugarra-Llerena,
    和 Cláudio R. Jung。从360°图像中估计3D场景几何：综述。ACM计算机调查，55(4)，2022年11月。'
- en: '[20] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via
    region-based fully convolutional networks. In D. Lee, M. Sugiyama, U. Luxburg,
    I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems,
    volume 29. Curran Associates, Inc., 2016.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Jifeng Dai, Yi Li, Kaiming He 和 Jian Sun. R-fcn：基于区域的全卷积网络的物体检测。发表于D.
    Lee, M. Sugiyama, U. Luxburg, I. Guyon 和 R. Garnett 主编的《神经信息处理系统进展》, 第29卷。Curran
    Associates, Inc., 2016年。'
- en: '[21] Carlos R del Blanco, Pablo Carballeira, Fernando Jaureguizar, and Narciso
    García. Robust people indoor localization with omnidirectional cameras using a
    grid of spatial-aware classifiers. Signal Processing: Image Communication, 93:116135,
    2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Carlos R del Blanco, Pablo Carballeira, Fernando Jaureguizar 和 Narciso
    García. 使用空间感知分类器网格的全向摄像机进行鲁棒的室内人物定位。信号处理：图像通信，93：116135，2021年。'
- en: '[22] Konstantinos K Delibasis, Spiros V Georgakopoulos, Konstantina Kottari,
    Vassilis P Plagianakos, and Ilias Maglogiannis. Geodesically-corrected zernike
    descriptors for pose recognition in omni-directional images. Integrated Computer-Aided
    Engineering, 23(2):185–199, 2016.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Konstantinos K Delibasis, Spiros V Georgakopoulos, Konstantina Kottari,
    Vassilis P Plagianakos 和 Ilias Maglogiannis. 用于全向图像中的姿态识别的地理校正Zernike描述符。集成计算机辅助工程，23(2)：185–199，2016年。'
- en: '[23] Konstantinos K. Delibasis and Ilias Maglogiannis. A fall detection algorithm
    for indoor video sequences captured by fish-eye camera. In 2015 IEEE 15th International
    Conference on Bioinformatics and Bioengineering (BIBE), pages 1–5, 2015.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Konstantinos K. Delibasis 和 Ilias Maglogiannis. 用于鱼眼摄像机捕捉的室内视频序列的跌倒检测算法。发表于2015年IEEE第15届国际生物信息学与生物工程会议（BIBE），页码1–5，2015年。'
- en: '[24] Banş Evrim Demiröz, Ismail Ari, Orhan Eroğlu, Albert Ali Salah, and Laie
    Akarun. Feature-based tracking on a multi-omnidirectional camera dataset. In 2012
    5th International Symposium on Communications, Control and Signal Processing,
    pages 1–5\. IEEE, 2012.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Banş Evrim Demiröz, Ismail Ari, Orhan Eroğlu, Albert Ali Salah 和 Laie
    Akarun. 基于特征的多全向摄像机数据集跟踪。发表于2012年第5届国际通信、控制与信号处理研讨会，页码1–5。IEEE，2012年。'
- en: '[25] Julia Denecke and Christian Jauch. Verification and regularization method
    for 3d-human body pose estimation based on prior knowledge. Electronic Imaging,
    33:1–8, 2021.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Julia Denecke 和 Christian Jauch. 基于先验知识的3D人体姿态估计的验证和正则化方法。电子成像，33：1–8，2021年。'
- en: '[26] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi
    Tian. Centernet: Keypoint triplets for object detection. In Proceedings of the
    IEEE/CVF international conference on computer vision, pages 6569–6578, 2019.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang 和 Qi Tian.
    Centernet：用于物体检测的关键点三元组。发表于IEEE/CVF国际计算机视觉会议论文集，页码6569–6578，2019年。'
- en: '[27] Zhihao Duan, Ozan Tezcan, Hayato Nakamura, Prakash Ishwar, and Janusz
    Konrad. Rapid: rotation-aware people detection in overhead fisheye images. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops, pages 636–637, 2020.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Zhihao Duan, Ozan Tezcan, Hayato Nakamura, Prakash Ishwar 和 Janusz Konrad.
    Rapid：在俯视鱼眼图像中进行旋转感知的人物检测。发表于IEEE/CVF计算机视觉与模式识别会议研讨会论文集，页码636–637，2020年。'
- en: '[28] Andrea Eichenseer and André Kaup. A data set providing synthetic and real-world
    fisheye video sequences. In 2016 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), pages 1541–1545, 2016.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Andrea Eichenseer 和 André Kaup. 提供合成和真实世界鱼眼视频序列的数据集。发表于2016年IEEE国际声学、语音与信号处理会议（ICASSP），页码1541–1545，2016年。'
- en: '[29] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and
    Andrew Zisserman. The pascal visual object classes (voc) challenge. International
    journal of computer vision, 88(2):303–338, 2010.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn 和 Andrew
    Zisserman. Pascal视觉对象类别（VOC）挑战。计算机视觉国际期刊，88(2)：303–338，2010年。'
- en: '[30] Dario Figueira, Matteo Taiana, Athira Nambiar, Jacinto Nascimento, and
    Alexandre Bernardino. The hda+ data set for research on fully automated re-identification
    systems. In European Conference on Computer Vision, pages 241–255. Springer, 2014.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Dario Figueira, Matteo Taiana, Athira Nambiar, Jacinto Nascimento 和 Alexandre
    Bernardino. HDA+数据集用于全自动重识别系统的研究。发表于欧洲计算机视觉会议，页码241–255。Springer，2014年。'
- en: '[31] Raspberry Pi Foundation. Raspberry pi 3 model b. [https://www.raspberrypi.com/products/raspberry-pi-3-model-b/](https://www.raspberrypi.com/products/raspberry-pi-3-model-b/),
    2016. [Online; accessed 11-April-2023].'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Raspberry Pi Foundation. Raspberry Pi 3 Model B. [https://www.raspberrypi.com/products/raspberry-pi-3-model-b/](https://www.raspberrypi.com/products/raspberry-pi-3-model-b/)，2016年。[在线；访问日期：2023年4月11日]。'
- en: '[32] Daniel Fuertes, Carlos R del Blanco, Pablo Carballeira, Fernando Jaureguizar,
    and Narciso García. People detection with omnidirectional cameras using a spatial
    grid of deep learning foveatic classifiers. Digital Signal Processing, 126:103473,
    2022.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Daniel Fuertes、Carlos R del Blanco、Pablo Carballeira、Fernando Jaureguizar
    和 Narciso García. 使用空间网格深度学习中心分类器的全向摄像头进行人员检测。数字信号处理，126：103473，2022年。'
- en: '[33] Nicola Garau, Niccolo Bisagno, Piotr Bródka, and Nicola Conci. Deca: Deep
    viewpoint-equivariant human pose estimation using capsule autoencoders. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 11677–11686,
    2021.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Nicola Garau、Niccolo Bisagno、Piotr Bródka 和 Nicola Conci. Deca：使用胶囊自编码器的深度视角不变人体姿态估计。发表于
    IEEE/CVF 国际计算机视觉会议论文集，第 11677–11686 页，2021年。'
- en: '[34] Nicola Garau, Giulia Martinelli, Piotr Bródka, Niccolò Bisagno, and Nicola
    Conci. Panoptop: A framework for generating viewpoint-invariant human pose estimation
    datasets. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision (ICCV) Workshops, pages 234–242, October 2021.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Nicola Garau、Giulia Martinelli、Piotr Bródka、Niccolò Bisagno 和 Nicola Conci.
    Panoptop：生成视角不变的人体姿态估计数据集的框架。发表于 IEEE/CVF 国际计算机视觉会议（ICCV）研讨会论文集，第 234–242 页，2021年10月。'
- en: '[35] José Gaspar, Niall Winters, and José Santos-Victor. Vision-based navigation
    and environmental representations with an omnidirectional camera. IEEE Transactions
    on robotics and automation, 16(6):890–898, 2000.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] José Gaspar、Niall Winters 和 José Santos-Victor. 基于视觉的导航和环境表征，使用全向摄像头。IEEE
    机器人与自动化汇刊，16(6)：890–898，2000年。'
- en: '[36] Spiros V Georgakopoulos, Konstantina Kottari, Kostas Delibasis, Vassilis P
    Plagianakos, and Ilias Maglogiannis. Pose recognition using convolutional neural
    networks on omni-directional images. Neurocomputing, 280:23–31, 2018.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Spiros V Georgakopoulos、Konstantina Kottari、Kostas Delibasis、Vassilis
    P Plagianakos 和 Ilias Maglogiannis. 基于卷积神经网络的全向图像姿态识别。Neurocomputing，280：23–31，2018年。'
- en: '[37] Olfa Haggui, Hamza Bayd, Baptiste Magnier, and Arezki Aberkane. Human
    detection in moving fisheye camera using an improved yolov3 framework. In 2021
    IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP), pages
    1–6\. IEEE, 2021.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Olfa Haggui、Hamza Bayd、Baptiste Magnier 和 Arezki Aberkane. 使用改进的 yolov3
    框架在移动鱼眼摄像头中进行人体检测。发表于 2021 IEEE 第23届国际多媒体信号处理研讨会（MMSP），第 1–6 页。IEEE，2021年。'
- en: '[38] Albert Haque, Alexandre Alahi, and Li Fei-Fei. Recurrent attention models
    for depth-based person identification. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 1229–1238, 2016.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Albert Haque、Alexandre Alahi 和 Li Fei-Fei. 基于深度的个体识别的递归注意力模型。发表于 IEEE
    计算机视觉与模式识别会议论文集，第 1229–1238 页，2016年。'
- en: '[39] Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Serena Yeung, and
    Li Fei-Fei. Towards viewpoint invariant 3d human pose estimation. In Computer
    Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October
    11–14, 2016, Proceedings, Part I 14, pages 160–177\. Springer, 2016.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Albert Haque、Boya Peng、Zelun Luo、Alexandre Alahi、Serena Yeung 和 Li Fei-Fei.
    朝着视角不变的 3D 人体姿态估计。发表于 计算机视觉–ECCV 2016：第 14 届欧洲会议，荷兰阿姆斯特丹，2016年10月11–14日，论文集第一部分第
    14 卷，第 160–177 页。Springer，2016年。'
- en: '[40] Richard I Hartley, Rajiv Gupta, and Tom Chang. Stereo from uncalibrated
    cameras. In CVPR, volume 92, pages 761–764, 1992.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Richard I Hartley、Rajiv Gupta 和 Tom Chang. 来自未标定摄像机的立体视觉。发表于 CVPR，第 92
    卷，第 761–764 页，1992年。'
- en: '[41] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn.
    In Proceedings of the IEEE International Conference on Computer Vision (ICCV),
    Oct 2017.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Kaiming He、Georgia Gkioxari、Piotr Dollar 和 Ross Girshick. Mask R-CNN。发表于
    IEEE 国际计算机视觉会议（ICCV），2017年10月。'
- en: '[42] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Kaiming He、Xiangyu Zhang、Shaoqing Ren 和 Jian Sun. 深度残差学习用于图像识别。发表于 IEEE
    计算机视觉与模式识别会议论文集，第 770–778 页，2016年。'
- en: '[43] Christoph Heindl, Thomas Pönitz, Andreas Pichler, and Josef Scharinger.
    Large area 3d human pose detection via stereo reconstruction in panoramic cameras.
    arXiv preprint arXiv:1907.00534, 2019.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Christoph Heindl、Thomas Pönitz、Andreas Pichler 和 Josef Scharinger. 通过全景摄像机的立体重建进行大范围
    3D 人体姿态检测。arXiv 预印本 arXiv:1907.00534，2019年。'
- en: '[44] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    Computation, 9(8):1735–1780, 1997.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Sepp Hochreiter 和 Jürgen Schmidhuber. 长短期记忆。神经计算，9(8)：1735–1780，1997年。'
- en: '[45] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
    Human3.6m: Large scale datasets and predictive methods for 3d human sensing in
    natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence,
    36(7):1325–1339, jul 2014.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Catalin Ionescu, Dragos Papava, Vlad Olaru 和 Cristian Sminchisescu. Human3.6m：用于自然环境中3D人类感知的大规模数据集和预测方法。IEEE模式分析与机器智能汇刊，36(7):1325–1339，2014年7月。'
- en: '[46] Sam Johnson and Mark Everingham. Learning effective human pose estimation
    from inaccurate annotation. In CVPR 2011, pages 1465–1472\. IEEE, 2011.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Sam Johnson 和 Mark Everingham. 从不准确标注中学习有效的人体姿态估计。发表于CVPR 2011，第1465–1472页。IEEE，2011年。'
- en: '[47] J. Kannala and S.S. Brandt. A generic camera model and calibration method
    for conventional, wide-angle, and fish-eye lenses. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 28(8):1335–1340, 2006.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Kannala 和 S.S. Brandt. 适用于常规、广角和鱼眼镜头的通用相机模型和标定方法。IEEE模式分析与机器智能汇刊，28(8):1335–1340，2006年。'
- en: '[48] Sung-Tae Kim, Ming Fan, Seung-Won Jung, and Sung-Jea Ko. External vehicle
    positioning system using multiple fish-eye surveillance cameras for indoor parking
    lots. IEEE Systems Journal, 15(4):5107–5118, 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Sung-Tae Kim, Ming Fan, Seung-Won Jung 和 Sung-Jea Ko. 使用多个鱼眼监控相机的外部车辆定位系统，用于室内停车场。IEEE系统学报，15(4):5107–5118，2020年。'
- en: '[49] Konstantina N Kottari, Konstantinos K Delibasis, and Ilias G Maglogiannis.
    Real-time fall detection using uncalibrated fisheye cameras. IEEE Transactions
    on Cognitive and Developmental Systems, 12(3):588–600, 2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Konstantina N Kottari, Konstantinos K Delibasis 和 Ilias G Maglogiannis.
    使用未标定的鱼眼相机进行实时跌倒检测。IEEE认知与发展系统汇刊，12(3):588–600，2019年。'
- en: '[50] Varun Ravi Kumar, Ciarán Eising, Christian Witt, and Senthil Yogamani.
    Surround-view fisheye camera perception for automated driving: Overview, survey
    & challenges. IEEE Transactions on Intelligent Transportation Systems, pages 1–22,
    2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Varun Ravi Kumar, Ciarán Eising, Christian Witt 和 Senthil Yogamani. 自动驾驶的全景鱼眼相机感知：概述、调查与挑战。IEEE智能交通系统汇刊，第1–22页，2023年。'
- en: '[51] Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Stefan Milz, Tim
    Fingscheidt, and Patrick Mader. Syndistnet: Self-supervised monocular fisheye
    camera distance estimation synergized with semantic segmentation for autonomous
    driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision (WACV), pages 61–71, January 2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Stefan Milz, Tim
    Fingscheidt 和 Patrick Mader. Syndistnet：自监督的单目鱼眼相机距离估计与语义分割协同用于自动驾驶。发表于IEEE/CVF冬季计算机视觉应用会议论文集（WACV），第61–71页，2021年1月。'
- en: '[52] Olivier Laurendin, Sébastien Ambellouis, Anthony Fleury, Ankur Mahtani,
    Sanaa Chafik, and Clément Strauss. Hazardous events detection in automatic train
    doors vicinity using deep neural networks. In 2021 17th IEEE International Conference
    on Advanced Video and Signal Based Surveillance (AVSS), pages 1–7\. IEEE, 2021.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Olivier Laurendin, Sébastien Ambellouis, Anthony Fleury, Ankur Mahtani,
    Sanaa Chafik 和 Clément Strauss. 使用深度神经网络检测自动列车门附近的危险事件。发表于2021年第17届IEEE国际先进视频与信号监控会议（AVSS），第1–7页。IEEE，2021年。'
- en: '[53] Junnan Li, Jianquan Liu, Wong Yongkang, Shoji Nishimura, and Mohan Kankanhalli.
    Weakly-supervised multi-person action recognition in 360° videos. In Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 508–516,
    2020.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Junnan Li, Jianquan Liu, Wong Yongkang, Shoji Nishimura 和 Mohan Kankanhalli.
    在360°视频中进行弱监督的多人动作识别。发表于IEEE/CVF冬季计算机视觉应用会议论文集，第508–516页，2020年。'
- en: '[54] Jia Li, Kaiwen Yu, Yifan Zhao, Yu Zhang, and Long Xu. Cross-reference
    stitching quality assessment for 360° omnidirectional images. In Proceedings of
    the 27th ACM International Conference on Multimedia, MM ’19, page 2360–2368, New
    York, NY, USA, 2019\. Association for Computing Machinery.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Jia Li, Kaiwen Yu, Yifan Zhao, Yu Zhang 和 Long Xu. 360°全景图像的交叉参考拼接质量评估。发表于第27届ACM国际多媒体会议（MM
    ’19），第2360–2368页，美国纽约，2019年。计算机协会。'
- en: '[55] Mingyang Li, Yi Zhou, Ming Meng, Yuehua Wang, and Zhong Zhou. 3d room
    reconstruction from a single fisheye image. In 2019 International Joint Conference
    on Neural Networks (IJCNN), pages 1–8, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Mingyang Li, Yi Zhou, Ming Meng, Yuehua Wang 和 Zhong Zhou. 从单张鱼眼图像重建3D房间。发表于2019年国际神经网络联合会议（IJCNN），第1–8页，2019年。'
- en: '[56] Shengye Li, M. Ozan Tezcan, Prakash Ishwar, and Janusz Konrad. Supervised
    people counting using an overhead fisheye camera. In 2019 16th IEEE International
    Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1–8,
    2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Shengye Li, M. Ozan Tezcan, Prakash Ishwar 和 Janusz Konrad. 使用上方鱼眼相机进行有监督的人群计数。发表于2019年第16届IEEE国际先进视频与信号监控会议（AVSS），第1–8页，2019年。'
- en: '[57] Daniele Liciotti, Marina Paolanti, Emanuele Frontoni, Adriano Mancini,
    and Primo Zingaretti. Person Re-identification Dataset with RGB-D Camera in a
    Top-View Configuration, pages 1–11. Springer International Publishing, Cham, 2017.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Daniele Liciotti, Marina Paolanti, Emanuele Frontoni, Adriano Mancini,
    和 Primo Zingaretti. 使用 RGB-D 摄像机的行人重识别数据集，页面 1–11。施普林格国际出版公司，Cham，2017年。'
- en: '[58] Daniele Liciotti, Marina Paolanti, Emanuele Frontoni, and Primo Zingaretti.
    People detection and tracking from an rgb-d camera in top-view configuration:
    review of challenges and applications. In New Trends in Image Analysis and Processing–ICIAP
    2017: ICIAP International Workshops, WBICV, SSPandBE, 3AS, RGBD, NIVAR, IWBAAS,
    and MADiMa 2017, Catania, Italy, September 11-15, 2017, Revised Selected Papers
    19, pages 207–218\. Springer, 2017.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Daniele Liciotti, Marina Paolanti, Emanuele Frontoni, 和 Primo Zingaretti.
    从 RGB-D 摄像机的顶视配置中进行行人检测和跟踪：挑战与应用的回顾。发表于《图像分析与处理的新趋势–ICIAP 2017: ICIAP 国际研讨会, WBICV,
    SSPandBE, 3AS, RGBD, NIVAR, IWBAAS 和 MADiMa 2017》，意大利卡塔尼亚，2017年9月11-15日，修订选集 19，页面
    207–218。施普林格，2017年。'
- en: '[59] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path
    refinement networks for high-resolution semantic segmentation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July
    2017.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Guosheng Lin, Anton Milan, Chunhua Shen, 和 Ian Reid. Refinenet: 多路径精细化网络用于高分辨率语义分割。发表于
    IEEE 计算机视觉与模式识别会议 (CVPR) 论文集，2017年7月。'
- en: '[60] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona,
    Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects
    in context. In European conference on computer vision, pages 740–755. Springer,
    2014.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona,
    Deva Ramanan, Piotr Dollár, 和 C Lawrence Zitnick. Microsoft coco: 上下文中的常见物体。发表于欧洲计算机视觉会议论文集，页面
    740–755。施普林格，2014年。'
- en: '[61] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, and Alexander C. Berg. Ssd: Single shot multibox detector. In Bastian
    Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision – ECCV
    2016, pages 21–37, Cham, 2016\. Springer International Publishing.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, 和 Alexander C. Berg. SSD: 单次多框检测器。由 Bastian Leibe, Jiri Matas,
    Nicu Sebe, 和 Max Welling 编辑，《计算机视觉 – ECCV 2016》，页面 21–37，Cham，2016年。施普林格国际出版公司。'
- en: '[62] Zhangchi Lu., Mertcan Cokbas., Prakash Ishwar., and Janusz Konrad. Estimating
    distances between people using a single overhead fisheye camera with application
    to social-distancing oversight. In Proceedings of the 18th International Joint
    Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications
    - Volume 5: VISAPP,, pages 528–535\. INSTICC, SciTePress, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Zhangchi Lu., Mertcan Cokbas., Prakash Ishwar., 和 Janusz Konrad. 使用单个顶视鱼眼摄像机估计人际距离，应用于社交距离监督。发表于第18届国际计算机视觉、图像和计算机图形理论与应用联合会议论文集
    - 第5卷: VISAPP，页面 528–535。INSTICC, SciTePress，2023年。'
- en: '[63] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and
    Michael J Black. Amass: Archive of motion capture as surface shapes. In Proceedings
    of the IEEE/CVF international conference on computer vision, pages 5442–5451,
    2019.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, 和
    Michael J Black. Amass: 运动捕捉表面形状档案。发表于 IEEE/CVF 国际计算机视觉会议论文集，页面 5442–5451，2019年。'
- en: '[64] Quan Nguyen Minh, Bang Le Van, Can Nguyen, Anh Le, and Viet Dung Nguyen.
    Arpd: Anchor-free rotation-aware people detection using topview fisheye camera.
    In 2021 17th IEEE International Conference on Advanced Video and Signal Based
    Surveillance (AVSS), pages 1–8\. IEEE, 2021.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Quan Nguyen Minh, Bang Le Van, Can Nguyen, Anh Le, 和 Viet Dung Nguyen.
    Arpd: 使用顶视鱼眼摄像机的无锚点旋转感知行人检测。发表于 2021年第17届 IEEE 国际高级视频与信号监控会议 (AVSS) 论文集，页面 1–8。IEEE，2021年。'
- en: '[65] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
    Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,
    et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533,
    2015.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
    Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski
    等。通过深度强化学习实现人类水平的控制。《自然》，518(7540):529–533，2015年。'
- en: '[66] S.K. Nayar. Catadioptric omnidirectional camera. In Proceedings of IEEE
    Computer Society Conference on Computer Vision and Pattern Recognition, pages
    482–488, 1997.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S.K. Nayar. Catadioptric 全向摄像机。发表于 IEEE 计算机学会计算机视觉与模式识别会议论文集，页面 482–488，1997年。'
- en: '[67] Thanh Binh Nguyen, Van Tuan Nguyen, Sun-Tae Chung, and Seongwon Cho. Real-time
    human detection under omni-directional camera based on cnn with unified detection
    and agmm for visual surveillance. Journal of Korea Multimedia Society, 19(8):1345–1360,
    2016.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Thanh Binh Nguyen、Van Tuan Nguyen、Sun-Tae Chung 和 Seongwon Cho。基于 CNN
    的全向相机实时人类检测，结合统一检测和 AGMM 用于视觉监控。韩国多媒体学报，19(8)：1345–1360，2016 年。'
- en: '[68] Trong-Nguyen Nguyen and Jean Meunier. Anomaly detection in video sequence
    with appearance-motion correspondence. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 1273–1283, 2019.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Trong-Nguyen Nguyen 和 Jean Meunier。视频序列中的异常检测，具有外观-运动对应。 在 IEEE/CVF 计算机视觉国际会议论文集，页码
    1273–1283，2019 年。'
- en: '[69] Viet Dung Nguyen, Phuc Ngoc Pham, Xuan Bach Nguyen, Thi Men Tran, and
    Minh Quan Nguyen. Incorporation of panoramic view in fall detection using omnidirectional
    camera. In The International Conference on Intelligent Systems & Networks, pages
    313–318\. Springer, 2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Viet Dung Nguyen、Phuc Ngoc Pham、Xuan Bach Nguyen、Thi Men Tran 和 Minh Quan
    Nguyen。在跌倒检测中加入全景视图，使用全向相机。在国际智能系统与网络会议，页码 313–318。Springer，2021 年。'
- en: '[70] Kazuhiro Otsuka, Shoko Araki, Dan Mikami, Kentaro Ishizuka, Masakiyo Fujimoto,
    and Junji Yamato. Realtime meeting analysis and 3d meeting viewer based on omnidirectional
    multimodal sensors. In Proceedings of the 2009 international conference on Multimodal
    interfaces, pages 219–220, 2009.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Kazuhiro Otsuka、Shoko Araki、Dan Mikami、Kentaro Ishizuka、Masakiyo Fujimoto
    和 Junji Yamato。基于全向多模态传感器的实时会议分析和 3D 会议查看器。在 2009 年国际多模态界面会议论文集，页码 219–220，2009
    年。'
- en: '[71] G Dias Pais, Tiago J Dias, Jacinto C Nascimento, and Pedro Miraldo. Omnidrl:
    Robust pedestrian detection using deep reinforcement learning on omnidirectional
    cameras. In 2019 International Conference on Robotics and Automation (ICRA), pages
    4782–4789\. IEEE, 2019.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] G Dias Pais、Tiago J Dias、Jacinto C Nascimento 和 Pedro Miraldo。Omnidrl：使用深度强化学习在全向相机上进行鲁棒的行人检测。在
    2019 年国际机器人与自动化会议（ICRA），页码 4782–4789。IEEE，2019 年。'
- en: '[72] Hazem Rashed, Eslam Mohamed, Ganesh Sistu, Varun Ravi Kumar, Ciaran Eising,
    Ahmad El-Sallab, and Senthil Yogamani. Generalized object detection on fisheye
    cameras for autonomous driving: Dataset, representations and baseline. In Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages
    2272–2280, January 2021.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Hazem Rashed、Eslam Mohamed、Ganesh Sistu、Varun Ravi Kumar、Ciaran Eising、Ahmad
    El-Sallab 和 Senthil Yogamani。针对自动驾驶的鱼眼相机上广义目标检测：数据集、表示和基线。在 IEEE/CVF 计算机视觉应用冬季会议（WACV）论文集，页码
    2272–2280，2021 年 1 月。'
- en: '[73] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only
    look once: Unified, real-time object detection. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 779–788, 2016.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Joseph Redmon、Santosh Divvala、Ross Girshick 和 Ali Farhadi。你只看一次：统一的实时目标检测。在
    IEEE 计算机视觉与模式识别会议论文集，页码 779–788，2016 年。'
- en: '[74] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 7263–7271, 2017.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Joseph Redmon 和 Ali Farhadi。Yolo9000：更好、更快、更强。在 IEEE 计算机视觉与模式识别会议论文集，页码
    7263–7271，2017 年。'
- en: '[75] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Joseph Redmon 和 Ali Farhadi。Yolov3：增量改进。arXiv 预印本 arXiv:1804.02767，2018
    年。'
- en: '[76] Naveed ur Rehman. Hemispherical photographs: A review of acquisition methods
    and applications in the context of urban energy and environment assessments. ASME
    Open Journal of Engineering, 1:010801, 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Naveed ur Rehman。半球形照片：获取方法和在城市能源与环境评估中的应用综述。ASME 开放工程期刊，1:010801，2022
    年。'
- en: '[77] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards
    real-time object detection with region proposal networks. Advances in neural information
    processing systems, 28, 2015.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Shaoqing Ren、Kaiming He、Ross Girshick 和 Jian Sun。Faster R-CNN：实现实时目标检测的区域提议网络。神经信息处理系统进展，28，2015
    年。'
- en: '[78] Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafutdinov, Mohammad
    Shafiei, Hans-Peter Seidel, Bernt Schiele, and Christian Theobalt. Egocap: egocentric
    marker-less motion capture with two fisheye cameras. ACM Transactions on Graphics
    (TOG), 35(6):1–11, 2016.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Helge Rhodin、Christian Richardt、Dan Casas、Eldar Insafutdinov、Mohammad
    Shafiei、Hans-Peter Seidel、Bernt Schiele 和 Christian Theobalt。Egocap：基于两个鱼眼相机的自我中心无标记动作捕捉。ACM
    图形学学报（TOG），35(6)：1–11，2016 年。'
- en: '[79] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
    Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International
    Journal of Computer Vision (IJCV), 115(3):211–252, 2015.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Olga Russakovsky、Jia Deng、Hao Su、Jonathan Krause、Sanjeev Satheesh、Sean
    Ma、Zhiheng Huang、Andrej Karpathy、Aditya Khosla、Michael Bernstein、Alexander C.
    Berg 和 Li Fei-Fei。ImageNet大规模视觉识别挑战。国际计算机视觉杂志（IJCV），115(3):211–252，2015年。'
- en: '[80] Tobias Scheck, Ana Grassi, and Gangolf Hirtz. Unsupervised domain adaptation
    from synthetic to real images for anchorless object detection. In Proceedings
    of the 16th International Joint Conference on Computer Vision, Imaging and Computer
    Graphics Theory and Applications - Volume 4: VISAPP,, pages 319–327\. INSTICC,
    SciTePress, 2021.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Tobias Scheck、Ana Grassi 和 Gangolf Hirtz。从合成到真实图像的无监督领域适应，用于无锚点物体检测。发表于第16届国际计算机视觉、图像和计算机图形理论与应用联合会议论文集
    - 第4卷：VISAPP，第319–327页。INSTICC，SciTePress，2021年。'
- en: '[81] T. Scheck, R. Seidel, and G. Hirtz. Learning from theodore: A synthetic
    omnidirectional top-view indoor dataset for deep transfer learning. In 2020 IEEE
    Winter Conference on Applications of Computer Vision (WACV), pages 932–941, Los
    Alamitos, CA, USA, mar 2020\. IEEE Computer Society.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] T. Scheck、R. Seidel 和 G. Hirtz。来自Theodore的学习：一个用于深度迁移学习的合成全向顶视图室内数据集。发表于2020年IEEE计算机视觉应用冬季会议（WACV），第932–941页，美国洛杉矶，2020年3月。IEEE计算机学会。'
- en: '[82] Miriam Schönbein and Andreas Geiger. Omnidirectional 3d reconstruction
    in augmented manhattan worlds. In 2014 IEEE/RSJ International Conference on Intelligent
    Robots and Systems, pages 716–723, 2014.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Miriam Schönbein 和 Andreas Geiger。增强型曼哈顿世界中的全向3D重建。发表于2014年IEEE/RSJ国际智能机器人与系统会议，第716–723页，2014年。'
- en: '[83] Roman Seidel, André Apitzsch, and Gangolf Hirtz. Improved person detection
    on omnidirectional images with non-maxima supression. In Proceedings of the 14th
    International Joint Conference on Computer Vision, Imaging and Computer Graphics
    Theory and Applications - Volume 5: VISAPP,, pages 474–481\. INSTICC, SciTePress,
    2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Roman Seidel、André Apitzsch 和 Gangolf Hirtz。在全向图像中使用非极大值抑制改进的人物检测。发表于第14届国际计算机视觉、图像和计算机图形理论与应用联合会议论文集
    - 第5卷：VISAPP，第474–481页。INSTICC，SciTePress，2019年。'
- en: '[84] Roman Seidel, André Apitzsch, Jingrui Yu, Julian Seuffert, Norbert Nestler,
    Danny Heinz, Anne Goy, and Gangolf Hirtz. Auxilia: Nutzerzentriertes assistenz-und
    sicherheitssystem zur unterstützung von menschen mit demenz auf basis intelligenter
    verhaltensanalyse. In Innteract Conference, volume 2018, page 48, 2018.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Roman Seidel、André Apitzsch、Jingrui Yu、Julian Seuffert、Norbert Nestler、Danny
    Heinz、Anne Goy 和 Gangolf Hirtz。Auxilia：以智能行为分析为基础的用户中心化辅助与安全系统，用于支持痴呆症患者。发表于Innteract会议，第2018卷，第48页，2018年。'
- en: '[85] Ahmed Rida Sekkat, Yohan Dupuis, Varun Ravi Kumar, Hazem Rashed, Senthil
    Yogamani, Pascal Vasseur, and Paul Honeine. Synwoodscape: Synthetic surround-view
    fisheye camera dataset for autonomous driving. IEEE Robotics and Automation Letters,
    7(3):8502–8509, 2022.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Ahmed Rida Sekkat、Yohan Dupuis、Varun Ravi Kumar、Hazem Rashed、Senthil Yogamani、Pascal
    Vasseur 和 Paul Honeine。Synwoodscape：用于自动驾驶的合成环视鱼眼相机数据集。IEEE机器人与自动化通讯，7(3):8502–8509，2022年。'
- en: '[86] Julian Bruno Seuffert, Ana Cecilia Perez Grassi, Tobias Scheck, and Gangolf
    Hirtz. A Study on the Influence of Omnidirectional Distortion on CNN-based Stereo
    Vision. In Proceedings of the 16th International Joint Conference on Computer
    Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2021,
    Volume 5: VISAPP, pages 809–816, Online Conference, 2 2021\. SciTePress.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Julian Bruno Seuffert、Ana Cecilia Perez Grassi、Tobias Scheck 和 Gangolf
    Hirtz。关于全向畸变对基于CNN的立体视觉影响的研究。发表于第16届国际计算机视觉、图像和计算机图形理论与应用联合会议论文集，VISIGRAPP 2021，第5卷：VISAPP，第809–816页，在线会议，2021年2月。SciTePress。'
- en: '[87] Roman Siedel, Tobias Scheck, Ana C Perez Grassi, Julian B Seuffert, André
    Apitzsch, Jingrui Yu, Norbert Nestler, Danny Heinz, Lars Lehmann, Anne Goy, et al.
    Contactless interactive fall detection and sleep quality estimation for supporting
    elderly with incipient dementia. Current Directions in Biomedical Engineering,
    6(3):388–391, 2020.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Roman Siedel、Tobias Scheck、Ana C Perez Grassi、Julian B Seuffert、André
    Apitzsch、Jingrui Yu、Norbert Nestler、Danny Heinz、Lars Lehmann、Anne Goy 等。无接触互动式跌倒检测和睡眠质量评估，用于支持早期痴呆症老人。生物医学工程当前方向，6(3):388–391，2020年。'
- en: '[88] Karen Stephen, Jianquan Liu, and Vivek Barsopia. A hybrid two-stream approach
    for multi-person action recognition in top-view 360° videos. In 2021 IEEE International
    Conference on Image Processing (ICIP), pages 3418–3422\. IEEE, 2021.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Karen Stephen, Jianquan Liu, 和 Vivek Barsopia. 一种用于360°俯视视频中的多人物动作识别的混合双流方法。发表于2021
    IEEE国际图像处理会议（ICIP），页面3418–3422。IEEE，2021年。'
- en: '[89] Shih-Yang Su, Frank Yu, Michael Zollhöfer, and Helge Rhodin. A-nerf: Articulated
    neural radiance fields for learning human shape, appearance, and pose. In Advances
    in Neural Information Processing Systems, 2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Shih-Yang Su, Frank Yu, Michael Zollhöfer, 和 Helge Rhodin. A-nerf: 用于学习人体形状、外观和姿态的关节神经辐射场。发表于神经信息处理系统进展，2021年。'
- en: '[90] Masato Tamura, Shota Horiguchi, and Tomokazu Murakami. Omnidirectional
    pedestrian detection by rotation invariant training. In 2019 IEEE Winter Conference
    on Applications of Computer Vision (WACV), pages 1989–1998, 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Masato Tamura, Shota Horiguchi, 和 Tomokazu Murakami. 通过旋转不变训练进行全向行人检测。发表于2019
    IEEE冬季计算机视觉应用会议（WACV），页面1989–1998，2019年。'
- en: '[91] Ozan Tezcan, Zhihao Duan, Mertcan Cokbas, Prakash Ishwar, and Janusz Konrad.
    Wepdtof: A dataset and benchmark algorithms for in-the-wild people detection and
    tracking from overhead fisheye cameras. In Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, pages 503–512, 2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Ozan Tezcan, Zhihao Duan, Mertcan Cokbas, Prakash Ishwar, 和 Janusz Konrad.
    Wepdtof: 用于野外人物检测和跟踪的头顶鱼眼相机数据集及基准算法。发表于IEEE/CVF冬季计算机视觉应用会议论文集，页面503–512，2022年。'
- en: '[92] Denis Tome, Thiemo Alldieck, Patrick Peluse, Gerard Pons-Moll, Lourdes
    Agapito, Hernan Badino, and Fernando De la Torre. Selfpose: 3d egocentric pose
    estimation from a headset mounted camera. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, pages 1–1, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Denis Tome, Thiemo Alldieck, Patrick Peluse, Gerard Pons-Moll, Lourdes
    Agapito, Hernan Badino, 和 Fernando De la Torre. Selfpose: 从头戴式相机中估计3D自视姿态。IEEE模式分析与机器智能汇刊，页面1–1，2020年。'
- en: '[93] Denis Tome, Patrick Peluse, Lourdes Agapito, and Hernan Badino. xr-egopose:
    Egocentric 3d human pose from an hmd camera. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 7728–7738, 2019.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Denis Tome, Patrick Peluse, Lourdes Agapito, 和 Hernan Badino. xr-egopose:
    来自HMD相机的自视3D人体姿态。发表于IEEE/CVF国际计算机视觉会议论文集，页面7728–7738，2019年。'
- en: '[94] Carl Vondrick, Donald Patterson, and Deva Ramanan. Efficiently scaling
    up crowdsourced video annotation. International journal of computer vision, 101(1):184–204,
    2013.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Carl Vondrick, Donald Patterson, 和 Deva Ramanan. 高效扩展众包视频注释。国际计算机视觉杂志，101(1):184–204，2013年。'
- en: '[95] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick
    Pérez. Advent: Adversarial entropy minimization for domain adaptation in semantic
    segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pages 2517–2526, 2019.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, 和 Patrick Pérez.
    Advent: 用于语义分割领域适应的对抗熵最小化。发表于IEEE/CVF计算机视觉与模式识别会议论文集，页面2517–2526，2019年。'
- en: '[96] Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Diogo Luvizon,
    and Christian Theobalt. Estimating egocentric 3d human pose in the wild with external
    weak supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), pages 13157–13166, June 2022.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Diogo Luvizon,
    和 Christian Theobalt. 在野外使用外部弱监督估计自视3D人体姿态。发表于IEEE/CVF计算机视觉与模式识别会议论文集（CVPR），页面13157–13166，2022年6月。'
- en: '[97] Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, and Christian
    Theobalt. Estimating egocentric 3d human pose in global space. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 11500–11509,
    2021.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, 和 Christian Theobalt.
    在全球空间中估计自视3D人体姿态。发表于IEEE/CVF国际计算机视觉会议论文集，页面11500–11509，2021年。'
- en: '[98] Tsaipei Wang, Yun-Yi Hsieh, Fong-Wen Wong, and Yi-Fu Chen. Mask-rcnn based
    people detection using a top-view fisheye camera. In 2019 International Conference
    on Technologies and Applications of Artificial Intelligence (TAAI), pages 1–4\.
    IEEE, 2019.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Tsaipei Wang, Yun-Yi Hsieh, Fong-Wen Wong, 和 Yi-Fu Chen. 基于Mask-rcnn的俯视鱼眼相机人物检测。发表于2019国际人工智能技术与应用会议（TAAI），页面1–4。IEEE，2019年。'
- en: '[99] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron,
    and Ira Kemelmacher-Shlizerman. HumanNeRF: Free-viewpoint rendering of moving
    people from monocular video. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 16210–16220, June 2022.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron,
    和 Ira Kemelmacher-Shlizerman. HumanNeRF: 从单目视频中渲染移动人物的自由视点. 见于 IEEE/CVF 计算机视觉与模式识别会议
    (CVPR) 论文集, 页码 16210–16220, 2022年6月.'
- en: '[100] Thaddäus Wiedemer, Stefan Wolf, Arne Schumann, Kaisheng Ma, and Jürgen
    Beyerer. Few-shot supervised prototype alignment for pedestrian detection on fisheye
    images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR) Workshops, pages 4142–4153, June 2022.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Thaddäus Wiedemer, Stefan Wolf, Arne Schumann, Kaisheng Ma, 和 Jürgen
    Beyerer. 基于鱼眼图像的行人检测的少样本监督原型对齐. 见于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 页码 4142–4153,
    2022年6月.'
- en: '[101] N. Winters, J. Gaspar, G. Lacey, and J. Santos-Victor. Omni-directional
    vision for robot navigation. In Proceedings IEEE Workshop on Omnidirectional Vision
    (Cat. No.PR00704), pages 21–28, 2000.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] N. Winters, J. Gaspar, G. Lacey, 和 J. Santos-Victor. 用于机器人导航的全向视觉. 见于
    IEEE 全向视觉研讨会论文集 (Cat. No.PR00704), 页码 21–28, 2000.'
- en: '[102] Changhee Won, Hochang Seok, Zhaopeng Cui, Marc Pollefeys, and Jongwoo
    Lim. Omnislam: Omnidirectional localization and dense mapping for wide-baseline
    multi-camera systems. In 2020 IEEE International Conference on Robotics and Automation
    (ICRA), pages 559–566\. IEEE, 2020.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Changhee Won, Hochang Seok, Zhaopeng Cui, Marc Pollefeys, 和 Jongwoo Lim.
    Omnislam: 用于宽基线多摄像头系统的全景定位和密集地图构建. 见于 2020 IEEE 国际机器人与自动化会议 (ICRA), 页码 559–566\.
    IEEE, 2020.'
- en: '[103] Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge Rhodin, Pascal
    Fua, Hans-Peter Seidel, and Christian Theobalt. Mo 2 cap 2: Real-time mobile 3d
    motion capture with a cap-mounted fisheye camera. IEEE transactions on visualization
    and computer graphics, 25(5):2093–2101, 2019.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge Rhodin, Pascal
    Fua, Hans-Peter Seidel, 和 Christian Theobalt. Mo 2 cap 2: 实时移动 3D 动作捕捉与帽子安装的鱼眼摄像头.
    《IEEE 视觉化与计算机图形学交易》，25(5):2093–2101, 2019.'
- en: '[104] Marie Yahiaoui, Hazem Rashed, Letizia Mariotti, Ganesh Sistu, Ian Clancy,
    Lucie Yahiaoui, Varun Ravi Kumar, and Senthil Yogamani. Fisheyemodnet: Moving
    object detection on surround-view cameras for autonomous driving. arXiv preprint
    arXiv:1908.11789, 2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Marie Yahiaoui, Hazem Rashed, Letizia Mariotti, Ganesh Sistu, Ian Clancy,
    Lucie Yahiaoui, Varun Ravi Kumar, 和 Senthil Yogamani. Fisheyemodnet: 用于自动驾驶的环视摄像头中的移动物体检测.
    arXiv 预印本 arXiv:1908.11789, 2019.'
- en: '[105] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky.
    Towards fairer datasets: Filtering and balancing the distribution of the people
    subtree in the imagenet hierarchy. In Conference on Fairness, Accountability,
    and Transparency, 2020.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, 和 Olga Russakovsky. 更公平的数据集:
    过滤和均衡 ImageNet 层级中人员子树的分布. 见于 公平性、问责制和透明度会议, 2020.'
- en: '[106] Senthil Yogamani, Ciaran Hughes, Jonathan Horgan, Ganesh Sistu, Padraig
    Varley, Derek O’Dea, Michal Uricar, Stefan Milz, Martin Simon, Karl Amende, Christian
    Witt, Hazem Rashed, Sumanth Chennupati, Sanjaya Nayak, Saquib Mansoor, Xavier
    Perrotton, and Patrick Perez. Woodscape: A multi-task, multi-camera fisheye dataset
    for autonomous driving. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV), October 2019.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Senthil Yogamani, Ciaran Hughes, Jonathan Horgan, Ganesh Sistu, Padraig
    Varley, Derek O’Dea, Michal Uricar, Stefan Milz, Martin Simon, Karl Amende, Christian
    Witt, Hazem Rashed, Sumanth Chennupati, Sanjaya Nayak, Saquib Mansoor, Xavier
    Perrotton, 和 Patrick Perez. Woodscape: 一个多任务、多摄像头鱼眼数据集用于自动驾驶. 见于 IEEE/CVF 国际计算机视觉会议
    (ICCV) 论文集, 2019年10月.'
- en: '[107] Jingrui Yu, Roman Seidel, and Gangolf Hirtz. Omnipd: One-step person
    detection in top-view omnidirectional indoor scenes. Current Directions in Biomedical
    Engineering, 5(1):239–244, 2019.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Jingrui Yu, Roman Seidel, 和 Gangolf Hirtz. Omnipd: 顶视角全景室内场景中的一步人检测.
    《生物医学工程当前方向》，5(1):239–244, 2019.'
- en: '[108] Dongxu Zhao, Zhen Wei, Jisan Mahmud, and Jan-Michael Frahm. Egoglass:
    Egocentric-view human pose estimation from an eyeglass frame. In 2021 International
    Conference on 3D Vision (3DV), pages 32–41\. IEEE, 2021.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Dongxu Zhao, Zhen Wei, Jisan Mahmud, 和 Jan-Michael Frahm. Egoglass: 从眼镜框架中进行自我中心视角的人体姿态估计.
    见于 2021 国际 3D 视觉会议 (3DV), 页码 32–41\. IEEE, 2021.'
