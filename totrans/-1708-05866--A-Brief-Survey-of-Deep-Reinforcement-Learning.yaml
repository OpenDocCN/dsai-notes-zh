- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:08:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:08:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1708.05866] A Brief Survey of Deep Reinforcement Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1708.05866] 深度强化学习的简要调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1708.05866](https://ar5iv.labs.arxiv.org/html/1708.05866)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1708.05866](https://ar5iv.labs.arxiv.org/html/1708.05866)
- en: A Brief Survey of Deep Reinforcement Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习的简要调查
- en: Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep reinforcement learning is poised to revolutionise the field of AI and represents
    a step towards building autonomous systems with a higher level understanding of
    the visual world. Currently, deep learning is enabling reinforcement learning
    to scale to problems that were previously intractable, such as learning to play
    video games directly from pixels. Deep reinforcement learning algorithms are also
    applied to robotics, allowing control policies for robots to be learned directly
    from camera inputs in the real world. In this survey, we begin with an introduction
    to the general field of reinforcement learning, then progress to the main streams
    of value-based and policy-based methods. Our survey will cover central algorithms
    in deep reinforcement learning, including the deep $Q$-network, trust region policy
    optimisation, and asynchronous advantage actor-critic. In parallel, we highlight
    the unique advantages of deep neural networks, focusing on visual understanding
    via reinforcement learning. To conclude, we describe several current areas of
    research within the field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习有望彻底改变人工智能领域，并代表了构建具有更高视觉世界理解能力的自主系统的一步。当前，深度学习使得强化学习能够扩展到以前难以解决的问题，例如直接从像素中学习玩视频游戏。深度强化学习算法也应用于机器人技术，使得机器人控制策略可以直接从现实世界的相机输入中学习。在这项调查中，我们首先介绍了强化学习的通用领域，然后逐步介绍了基于价值的方法和基于策略的方法。我们的调查将涵盖深度强化学习中的核心算法，包括深度$Q$-网络、信任区域策略优化和异步优势演员-评论家。同时，我们突出深度神经网络的独特优势，重点关注通过强化学习进行视觉理解。最后，我们描述了该领域内几个当前的研究方向。
- en: I Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'One of the primary goals of the field of artificial intelligence (AI) is to
    produce fully autonomous agents that interact with their environments to learn
    optimal behaviours, improving over time through trial and error. Crafting AI systems
    that are responsive and can effectively learn has been a long-standing challenge,
    ranging from robots, which can sense and react to the world around them, to purely
    software-based agents, which can interact with natural language and multimedia.
    A principled mathematical framework for experience-driven autonomous learning
    is reinforcement learning (RL) [[135](#bib.bib135)]. Although RL had some successes
    in the past [[141](#bib.bib141), [129](#bib.bib129), [62](#bib.bib62), [93](#bib.bib93)],
    previous approaches lacked scalablity and were inherently limited to fairly low-dimensional
    problems. These limitations exist because RL algorithms share the same complexity
    issues as other algorithms: memory complexity, computational complexity, and in
    the case of machine learning algorithms, sample complexity [[133](#bib.bib133)].
    What we have witnessed in recent years—the rise of deep learning, relying on the
    powerful *function approximation* and *representation learning* properties of
    deep neural networks—has provided us with new tools to overcoming these problems.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）领域的主要目标之一是生产完全自主的智能体，使其能够与环境互动，以学习最佳行为，通过试错不断改进。设计响应性强且能够有效学习的AI系统一直是一个长期挑战，这包括能够感知和反应周围世界的机器人，以及能够与自然语言和多媒体交互的纯软件代理。一个原则性的经验驱动的自主学习数学框架是强化学习（RL）[[135](#bib.bib135)]。虽然RL在过去取得了一些成功[[141](#bib.bib141),
    [129](#bib.bib129), [62](#bib.bib62), [93](#bib.bib93)]，但之前的方法缺乏可扩展性，且固有地局限于较低维度的问题。这些限制的存在是因为RL算法与其他算法共享相同的复杂性问题：内存复杂性、计算复杂性，以及在机器学习算法中，样本复杂性[[133](#bib.bib133)]。我们近年来所见到的——深度学习的兴起，依赖于深度神经网络强大的*函数逼近*和*表示学习*属性——为我们提供了克服这些问题的新工具。
- en: The advent of deep learning has had a significant impact on many areas in machine
    learning, dramatically improving the state-of-the-art in tasks such as object
    detection, speech recognition, and language translation [[70](#bib.bib70)]. The
    most important property of deep learning is that deep neural networks can automatically
    find compact low-dimensional representations (features) of high-dimensional data
    (e.g., images, text and audio). Through crafting inductive biases into neural
    network architectures, particularly that of hierarchical representations, machine
    learning practitioners have made effective progress in addressing the curse of
    dimensionality [[15](#bib.bib15)]. Deep learning has similarly accelerated progress
    in RL, with the use of deep learning algorithms within RL defining the field of
    “deep reinforcement learning” (DRL). The aim of this survey is to cover both seminal
    and recent developments in DRL, conveying the innovative ways in which neural
    networks can be used to bring us closer towards developing autonomous agents.
    For a more comprehensive survey of recent efforts in DRL, including applications
    of DRL to areas such as natural language processing [[106](#bib.bib106), [5](#bib.bib5)],
    we refer readers to the overview by Li [[78](#bib.bib78)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的出现对机器学习的许多领域产生了重大影响，显著提高了物体检测、语音识别和语言翻译等任务的最先进水平[[70](#bib.bib70)]。深度学习最重要的特性是深度神经网络能够自动找到高维数据（例如图像、文本和音频）的紧凑低维表示（特征）。通过在神经网络架构中引入归纳偏置，特别是层级表示，机器学习从业者在解决维度诅咒问题上取得了有效进展[[15](#bib.bib15)]。深度学习同样加速了强化学习的进展，深度学习算法在强化学习中的应用定义了“深度强化学习”（DRL）领域。本次综述旨在涵盖深度强化学习的基础和最新发展，传达神经网络如何以创新方式推动我们接近开发自主代理的目标。有关深度强化学习近期努力的更全面的综述，包括DRL在自然语言处理等领域的应用[[106](#bib.bib106),
    [5](#bib.bib5)]，我们推荐读者参考Li的概述[[78](#bib.bib78)]。
- en: Deep learning enables RL to scale to decision-making problems that were previously
    intractable, i.e., settings with high-dimensional state and action spaces. Amongst
    recent work in the field of DRL, there have been two outstanding success stories.
    The first, kickstarting the revolution in DRL, was the development of an algorithm
    that could learn to play a range of Atari 2600 video games at a superhuman level,
    directly from image pixels [[84](#bib.bib84)]. Providing solutions for the instability
    of function approximation techniques in RL, this work was the first to convincingly
    demonstrate that RL agents could be trained on raw, high-dimensional observations,
    solely based on a reward signal. The second standout success was the development
    of a hybrid DRL system, AlphaGo, that defeated a human world champion in Go [[128](#bib.bib128)],
    paralleling the historic achievement of IBM’s Deep Blue in chess two decades earlier
    [[19](#bib.bib19)] and IBM’s Watson DeepQA system that beat the best human Jeopardy!
    players [[31](#bib.bib31)]. Unlike the handcrafted rules that have dominated chess-playing
    systems, AlphaGo was composed of neural networks that were trained using supervised
    and reinforcement learning, in combination with a traditional heuristic search
    algorithm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习使得强化学习能够扩展到以前难以处理的决策问题，即具有高维状态和动作空间的设置。在深度强化学习领域的近期工作中，有两个突出的成功案例。第一个是启动了深度强化学习革命的算法，这个算法能够从图像像素中直接学习并以超人类水平玩各种Atari
    2600视频游戏[[84](#bib.bib84)]。这个工作提供了解决强化学习中函数逼近技术不稳定性的问题的方案，并首次令人信服地展示了强化学习代理可以仅根据奖励信号在原始的高维观察数据上进行训练。第二个突出成功是混合深度强化学习系统AlphaGo的开发，它在围棋中击败了人类世界冠军[[128](#bib.bib128)]，这一成就与20年前IBM的深蓝在国际象棋中的历史性胜利[[19](#bib.bib19)]以及IBM的Watson
    DeepQA系统战胜顶级人类《危险边缘!》选手[[31](#bib.bib31)]相媲美。与主导棋类游戏系统的手工规则不同，AlphaGo由神经网络组成，这些网络结合了监督学习和强化学习，并配合传统启发式搜索算法进行训练。
- en: DRL algorithms have already been applied to a wide range of problems, such as
    robotics, where control policies for robots can now be learned directly from camera
    inputs in the real world [[74](#bib.bib74), [75](#bib.bib75)], succeeding controllers
    that used to be hand-engineered or learned from low-dimensional features of the
    robot’s state. In a step towards even more capable agents, DRL has been used to
    create agents that can meta-learn (“learn to learn”) [[29](#bib.bib29), [156](#bib.bib156)],
    allowing them to generalise to complex visual environments they have never seen
    before [[29](#bib.bib29)]. In Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A
    Brief Survey of Deep Reinforcement Learning"), we showcase just some of the domains
    that DRL has been applied to, ranging from playing video games [[84](#bib.bib84)]
    to indoor navigation [[167](#bib.bib167)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 算法已经被应用于广泛的问题领域，例如机器人技术，现在可以直接从现实世界的相机输入中学习机器人的控制策略[[74](#bib.bib74), [75](#bib.bib75)]，取代了以前由手工设计或从机器人状态的低维特征中学习的控制器。为了创建更强大的智能体，DRL
    被用来创建能够元学习（“学习如何学习”）的智能体[[29](#bib.bib29), [156](#bib.bib156)]，使它们能够泛化到以前从未见过的复杂视觉环境[[29](#bib.bib29)]。在图
    [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Brief Survey of Deep Reinforcement Learning")
    中，我们展示了 DRL 应用的一些领域，从玩视频游戏[[84](#bib.bib84)]到室内导航[[167](#bib.bib167)]。
- en: '![Refer to caption](img/cd860ccfaef9e4f3ec046dd54e2cae1b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cd860ccfaef9e4f3ec046dd54e2cae1b.png)'
- en: 'Figure 1: A range of visual RL domains. (a) Two classic Atari 2600 video games,
    “Freeway” and “Seaquest”, from the Arcade Learning Environment (ALE) [[10](#bib.bib10)].
    Due to the range of supported games that vary in genre, visuals and difficulty,
    the ALE has become a standard testbed for DRL algorithms [[84](#bib.bib84), [95](#bib.bib95),
    [44](#bib.bib44), [122](#bib.bib122), [132](#bib.bib132), [157](#bib.bib157),
    [85](#bib.bib85)]. As we will discuss later, the ALE is one of several benchmarks
    that are now being used to standardise evaluation in RL. (b) The TORCS car racing
    simulator, which has been used to test DRL algorithms that can output continuous
    actions [[64](#bib.bib64), [79](#bib.bib79), [85](#bib.bib85)] (as the games from
    the ALE only support discrete actions). (c) Utilising the potentially unlimited
    amount of training data that can be amassed in robotic simulators, several methods
    aim to transfer knowledge from the simulator to the real world [[22](#bib.bib22),
    [115](#bib.bib115), [146](#bib.bib146)]. (d) Two of the four robotic tasks designed
    by Levine et al. [[74](#bib.bib74)]: screwing on a bottle cap and placing a shaped
    block in the correct hole. Levine et al. [[74](#bib.bib74)] were able to train
    visuomotor policies in an end-to-end fashion, showing that visual servoing could
    be learned directly from raw camera inputs by using deep neural networks. (e)
    A real room, in which a wheeled robot trained to navigate the building is given
    a visual cue as input, and must find the corresponding location [[167](#bib.bib167)].
    (f) A natural image being captioned by a neural network that uses reinforcement
    learning to choose where to look [[166](#bib.bib166)]. By processing a small portion
    of the image for every word generated, the network can focus its attention on
    the most salient points. Figures reproduced from [[10](#bib.bib10), [79](#bib.bib79),
    [146](#bib.bib146), [74](#bib.bib74), [167](#bib.bib167), [166](#bib.bib166)],
    respectively.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 一系列视觉 RL 领域。 (a) 两款经典的 Atari 2600 视频游戏，“Freeway”和“Seaquest”，来自于 Arcade
    Learning Environment (ALE) [[10](#bib.bib10)]。由于支持的游戏范围涵盖不同的类型、视觉效果和难度，ALE 已成为
    DRL 算法的标准测试平台[[84](#bib.bib84), [95](#bib.bib95), [44](#bib.bib44), [122](#bib.bib122),
    [132](#bib.bib132), [157](#bib.bib157), [85](#bib.bib85)]。正如我们稍后将讨论的那样，ALE 是现在用于标准化
    RL 评估的几个基准之一。 (b) TORCS 赛车模拟器，已被用于测试可以输出连续动作的 DRL 算法[[64](#bib.bib64), [79](#bib.bib79),
    [85](#bib.bib85)]（因为 ALE 的游戏仅支持离散动作）。 (c) 利用在机器人模拟器中可以积累的潜在无限的训练数据，一些方法旨在将知识从模拟器转移到现实世界[[22](#bib.bib22),
    [115](#bib.bib115), [146](#bib.bib146)]。 (d) Levine 等人设计的四个机器人任务中的两个[[74](#bib.bib74)]：拧瓶盖和将形状块放入正确的孔中。Levine
    等人[[74](#bib.bib74)] 能够以端到端的方式训练视觉运动策略，展示了通过使用深度神经网络，可以直接从原始相机输入中学习视觉伺服。 (e) 一个真实的房间，其中一个训练过的轮式机器人在导航建筑物时接收视觉线索作为输入，必须找到相应的位置[[167](#bib.bib167)]。
    (f) 一个自然图像，通过一个使用强化学习来选择查看位置的神经网络进行描述[[166](#bib.bib166)]。通过处理图像中的小部分来生成每个词，网络可以将注意力集中在最突出的点上。图像转载自[[10](#bib.bib10),
    [79](#bib.bib79), [146](#bib.bib146), [74](#bib.bib74), [167](#bib.bib167), [166](#bib.bib166)]。'
- en: Video games may be an interesting challenge, but learning how to play them is
    not the end goal of DRL. One of the driving forces behind DRL is the vision of
    creating systems that are capable of learning how to adapt in the real world.
    From managing power consumption [[142](#bib.bib142)] to picking and stowing objects
    [[75](#bib.bib75)], DRL stands to increase the amount of physical tasks that can
    be automated by learning. However, DRL does not stop there, as RL is a general
    way of approaching optimisation problems by trial and error. From designing state-of-the-art
    machine translation models [[168](#bib.bib168)] to constructing new optimisation
    functions [[76](#bib.bib76)], DRL has already been used to approach all manner
    of machine learning tasks. And, in the same way that deep learning has been utilised
    across many branches of machine learning, it seems likely that in the future,
    DRL will be an important component in constructing general AI systems [[68](#bib.bib68)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 视频游戏可能是一个有趣的挑战，但学习如何玩游戏并不是DRL的终极目标。DRL的推动力之一是创建能够在现实世界中自适应学习的系统。从管理电力消耗[[142](#bib.bib142)]到挑选和存放物体[[75](#bib.bib75)]，DRL有望通过学习增加可以自动化的物理任务的数量。然而，DRL不仅仅止步于此，因为RL是一种通过试错方法解决优化问题的一般方式。从设计最先进的机器翻译模型[[168](#bib.bib168)]到构建新的优化函数[[76](#bib.bib76)]，DRL已经被用于处理各种机器学习任务。正如深度学习在许多机器学习分支中得到应用一样，未来DRL也可能成为构建通用AI系统的重要组成部分[[68](#bib.bib68)]。
- en: II Reward-driven Behaviour
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 奖励驱动的行为
- en: '![Refer to caption](img/275cef3317678694e02aeed283b0638a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/275cef3317678694e02aeed283b0638a.png)'
- en: 'Figure 2: The perception-action-learning loop. At time $t$, the agent receives
    state $\mathbf{s}_{t}$ from the environment. The agent uses its policy to choose
    an action $\mathbf{a}_{t}$. Once the action is executed, the environment transitions
    a step, providing the next state $\mathbf{s}_{t+1}$ as well as feedback in the
    form of a reward $r_{t+1}$. The agent uses knowledge of state transitions, of
    the form $(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{s}_{t+1},r_{t+1})$, in order
    to learn and improve its policy.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：感知-行动-学习循环。在时间$t$，智能体从环境中接收状态$\mathbf{s}_{t}$。智能体使用其策略选择一个动作$\mathbf{a}_{t}$。一旦动作被执行，环境转移一步，提供下一个状态$\mathbf{s}_{t+1}$以及奖励$r_{t+1}$形式的反馈。智能体利用状态转移的知识，如$(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{s}_{t+1},r_{t+1})$，以学习和改进其策略。
- en: Before examining the contributions of deep neural networks to RL, we will introduce
    the field of RL in general. The essence of RL is learning through *interaction*.
    An RL agent interacts with its environment and, upon observing the consequences
    of its actions, can learn to alter its own behaviour in response to rewards received.
    This paradigm of trial-and error-learning has its roots in behaviourist psychology,
    and is one of the main foundations of RL [[135](#bib.bib135)]. The other key influence
    on RL is optimal control, which has lent the mathematical formalisms (most notably
    dynamic programming [[13](#bib.bib13)]) that underpin the field.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察深度神经网络对强化学习（RL）的贡献之前，我们将首先介绍强化学习的基本领域。强化学习的本质是通过*互动*进行学习。RL智能体与环境互动，并在观察到其行为的后果后，能够根据获得的奖励调整自己的行为。这种试错学习的范式根源于行为心理学，是RL的主要基础之一[[135](#bib.bib135)]。另一个对RL有关键影响的领域是最优控制，它提供了支撑这一领域的数学形式化方法（尤其是动态规划[[13](#bib.bib13)]）。
- en: In the RL set-up, an autonomous *agent*, controlled by a machine learning algorithm,
    observes a *state* $\mathbf{s}_{t}$ from its *environment* at timestep $t$. The
    agent interacts with the environment by taking an *action* $\mathbf{a}_{t}$ in
    state $\mathbf{s}_{t}$. When the agent takes an action, the environment and the
    agent transition to a new state $\mathbf{s}_{t+1}$ based on the current state
    and the chosen action. The state is a sufficient statistic of the environment
    and thereby comprises all the necessary information for the agent to take the
    best action, which can include parts of the agent, such as the position of its
    actuators and sensors. In the optimal control literature, states and actions are
    often denoted by $\mathbf{x}_{t}$ and $\mathbf{u}_{t}$, respectively.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习的设置中，一个由机器学习算法控制的自主*智能体*在时间步 $t$ 从其*环境*中观察一个*状态* $\mathbf{s}_{t}$。智能体通过在状态
    $\mathbf{s}_{t}$ 下采取一个*动作* $\mathbf{a}_{t}$ 来与环境互动。当智能体采取一个动作时，环境和智能体根据当前状态和选择的动作过渡到一个新状态
    $\mathbf{s}_{t+1}$。状态是环境的充分统计量，因此包含了智能体采取最佳动作所需的所有信息，这可能包括智能体的部分内容，如其执行器和传感器的位置。在最优控制文献中，状态和动作通常分别用
    $\mathbf{x}_{t}$ 和 $\mathbf{u}_{t}$ 表示。
- en: The best sequence of actions is determined by the *rewards* provided by the
    environment. Every time the environment transitions to a new state, it also provides
    a scalar reward $r_{t+1}$ to the agent as feedback. The goal of the agent is to
    learn a *policy* (control strategy) $\pi$ that maximises the expected *return*
    (cumulative, discounted reward). Given a state, a policy returns an action to
    perform; an *optimal policy* is any policy that maximises the expected return
    in the environment. In this respect, RL aims to solve the same problem as optimal
    control. However, the challenge in RL is that the agent needs to learn about the
    consequences of actions in the environment by trial and error, as, unlike in optimal
    control, a model of the state transition dynamics is not available to the agent.
    Every interaction with the environment yields information, which the agent uses
    to update its knowledge. This *perception-action-learning loop* is illustrated
    in Figure [2](#S2.F2 "Figure 2 ‣ II Reward-driven Behaviour ‣ A Brief Survey of
    Deep Reinforcement Learning").
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳动作序列由环境提供的*奖励*来决定。每次环境过渡到新状态时，它还向智能体提供一个标量奖励 $r_{t+1}$ 作为反馈。智能体的目标是学习一个*策略*（控制策略）
    $\pi$，以最大化预期的*回报*（累积的折扣奖励）。给定一个状态，策略返回一个要执行的动作；*最优策略*是任何最大化环境中预期回报的策略。在这方面，强化学习旨在解决与最优控制相同的问题。然而，在强化学习中，挑战在于智能体需要通过试错学习了解动作在环境中的后果，因为与最优控制不同，智能体没有状态转移动态的模型。每次与环境的互动都提供了信息，智能体利用这些信息更新其知识。这个*感知-动作-学习循环*在图
    [2](#S2.F2 "Figure 2 ‣ II Reward-driven Behaviour ‣ A Brief Survey of Deep Reinforcement
    Learning")中有所示。
- en: II-A Markov Decision Processes
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 马尔可夫决策过程
- en: 'Formally, RL can be described as a Markov decision process (MDP), which consists
    of:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，强化学习（RL）可以被描述为一个马尔可夫决策过程（MDP），该过程包括：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A set of states $\mathcal{S}$, plus a distribution of starting states $p(\mathbf{s}_{0})$.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一组状态 $\mathcal{S}$，以及一个初始状态分布 $p(\mathbf{s}_{0})$。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A set of actions $\mathcal{A}$.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一组动作 $\mathcal{A}$。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transition dynamics $\mathcal{T}(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})$
    that map a state-action pair at time $t$ onto a distribution of states at time
    $t+1$.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移动态 $\mathcal{T}(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a}_{t})$，将时间 $t$ 的状态-动作对映射到时间
    $t+1$ 的状态分布上。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An immediate/instantaneous reward function $\mathcal{R}(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{s}_{t+1})$.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个即时/瞬时奖励函数 $\mathcal{R}(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{s}_{t+1})$。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A discount factor $\gamma\in[0,1]$, where lower values place more emphasis on
    immediate rewards.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个折扣因子 $\gamma\in[0,1]$，较低的值更强调即时奖励。
- en: 'In general, the policy $\pi$ is a mapping from states to a probability distribution
    over actions: $\pi:\mathcal{S}\rightarrow p(\mathcal{A}=\mathbf{a}|\mathcal{S})$.
    If the MDP is *episodic*, i.e., the state is reset after each episode of length
    $T$, then the sequence of states, actions and rewards in an episode constitutes
    a *trajectory* or *rollout* of the policy. Every rollout of a policy accumulates
    rewards from the environment, resulting in the return $R=\sum_{t=0}^{T-1}\gamma^{t}r_{t+1}$.
    The goal of RL is to find an optimal policy, $\pi^{*}$, which achieves the maximum
    expected return from all states:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，策略 $\pi$ 是从状态到动作概率分布的映射：$\pi:\mathcal{S}\rightarrow p(\mathcal{A}=\mathbf{a}|\mathcal{S})$。如果
    MDP 是*episodic*的，即每一集的状态在长度为 $T$ 后会被重置，则一集中的状态、动作和奖励序列构成了策略的*轨迹*或*滚动*。每一次策略的滚动都会从环境中累积奖励，导致回报
    $R=\sum_{t=0}^{T-1}\gamma^{t}r_{t+1}$。强化学习的目标是找到一个最优策略 $\pi^{*}$，它能够从所有状态中实现最大的期望回报：
- en: '|  | $\pi^{*}=\operatorname*{argmax}_{\pi}\mathbb{E}[R&#124;\pi].$ |  | (1)
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=\operatorname*{argmax}_{\pi}\mathbb{E}[R&#124;\pi].$ |  | (1)
    |'
- en: It is also possible to consider non-episodic MDPs, where $T=\infty$. In this
    situation, $\gamma<1$ prevents an infinite sum of rewards from being accumulated.
    Furthermore, methods that rely on complete trajectories are no longer applicable,
    but those that use a finite set of transitions still are.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以考虑非*episodic* MDP，其中 $T=\infty$。在这种情况下，$\gamma<1$ 防止奖励的无限累积。此外，依赖于完整轨迹的方法不再适用，但使用有限转移集合的方法仍然有效。
- en: A key concept underlying RL is the Markov property—only the current state affects
    the next state, or in other words, the future is conditionally independent of
    the past given the present state. This means that any decisions made at $\mathbf{s}_{t}$
    can be based solely on $\mathbf{s}_{t-1}$, rather than $\{\mathbf{s}_{0},\mathbf{s}_{1},\ldots,\mathbf{s}_{t-1}\}$.
    Although this assumption is held by the majority of RL algorithms, it is somewhat
    unrealistic, as it requires the states to be *fully observable*. A generalisation
    of MDPs are partially observable MDPs (POMDPs), in which the agent receives an
    observation $\mathbf{o}_{t}\in\Omega$, where the distribution of the observation
    $p(\mathbf{o}_{t+1}|\mathbf{s}_{t+1},\mathbf{a}_{t})$ is dependent on the current
    state and the previous action [[56](#bib.bib56)]. In a control and signal processing
    context, the observation would be described by a measurement/observation mapping
    in a state-space-model that depends on the current state and the previously applied
    action.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个关键概念是马尔可夫性质——只有当前状态影响下一个状态，换句话说，给定当前状态，未来与过去是条件独立的。这意味着在 $\mathbf{s}_{t}$
    时做出的任何决策都可以仅基于 $\mathbf{s}_{t-1}$，而不是 $\{\mathbf{s}_{0},\mathbf{s}_{1},\ldots,\mathbf{s}_{t-1}\}$。虽然大多数强化学习算法都遵循这一假设，但它在某种程度上是不现实的，因为它要求状态是*完全可观测的*。MDP
    的一种推广是部分可观测 MDP（POMDP），其中代理接收观察 $\mathbf{o}_{t}\in\Omega$，观察的分布 $p(\mathbf{o}_{t+1}|\mathbf{s}_{t+1},\mathbf{a}_{t})$
    依赖于当前状态和之前的动作 [[56](#bib.bib56)]。在控制和信号处理的背景下，观察会由一个状态空间模型中的测量/观察映射描述，该映射依赖于当前状态和先前应用的动作。
- en: POMDP algorithms typically maintain a *belief* over the current state given
    the previous belief state, the action taken and the current observation. A more
    common approach in deep learning is to utilise recurrent neural networks (RNNs)
    [[163](#bib.bib163), [44](#bib.bib44), [45](#bib.bib45), [85](#bib.bib85), [96](#bib.bib96)],
    which, unlike feedforward neural networks, are dynamical systems. This approach
    to solving POMDPs is related to other problems using dynamical systems and state
    space models, where the true state can only be estimated [[16](#bib.bib16)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: POMDP 算法通常在给定之前的信念状态、采取的动作和当前观察的情况下，维护对当前状态的*信念*。在深度学习中，更常见的方法是利用递归神经网络（RNNs）[[163](#bib.bib163),
    [44](#bib.bib44), [45](#bib.bib45), [85](#bib.bib85), [96](#bib.bib96)]，与前馈神经网络不同，递归神经网络是动态系统。这种解决
    POMDP 的方法与其他使用动态系统和状态空间模型的问题相关，其中真实状态只能被估计 [[16](#bib.bib16)]。
- en: II-B Challenges in RL
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 强化学习中的挑战
- en: 'It is instructive to emphasise some challenges faced in RL:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 强调一些强化学习中面临的挑战是很有启发性的：
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The optimal policy must be inferred by trial-and-error interaction with the
    environment. The only learning signal the agent receives is the reward.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最优策略必须通过与环境的试错交互来推断。代理接收到的唯一学习信号是奖励。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The observations of the agent depend on its actions and can contain strong temporal
    correlations.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理的观察依赖于其动作，并可能包含强烈的时间相关性。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Agents must deal with long-range time dependencies: Often the consequences
    of an action only materialise after many transitions of the environment. This
    is known as the (temporal) *credit assignment problem* [[135](#bib.bib135)].'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理必须处理长期时间依赖性：通常，一个行动的后果只有在环境经过多次转变后才会显现。这被称为（时间）*信用分配问题* [[135](#bib.bib135)]。
- en: 'We will illustrate these challenges in the context of an indoor robotic visual
    navigation task: if the goal location is specified, we may be able to estimate
    the distance remaining (and use it as a reward signal), but it is unlikely that
    we will know exactly what series of actions the robot needs to take to reach the
    goal. As the robot must choose where to go as it navigates the building, its decisions
    influence which rooms it sees and, hence, the statistics of the visual sequence
    captured. Finally, after navigating several junctions, the robot may find itself
    in a dead end. There is a range of problems, from learning the consequences of
    actions to balancing exploration against exploitation, but ultimately these can
    all be addressed formally within the framework of RL.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在室内机器人视觉导航任务的背景下说明这些挑战：如果目标位置已指定，我们可能能够估计剩余距离（并将其用作奖励信号），但不太可能确切知道机器人需要采取什么系列动作才能到达目标。由于机器人在导航建筑物时必须选择去哪里，它的决策会影响它看到哪些房间，从而影响捕获的视觉序列的统计数据。最后，在经过几个交叉口后，机器人可能会发现自己处于死胡同。存在从学习行动后果到在探索与利用之间平衡的一系列问题，但最终这些都可以在
    RL 框架内正式解决。
- en: III Reinforcement Learning Algorithms
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 强化学习算法
- en: 'So far, we have introduced the key formalism used in RL, the MDP, and briefly
    noted some challenges in RL. In the following, we will distinguish between different
    classes of RL algorithms. There are two main approaches to solving RL problems:
    methods based on *value functions* and methods based on *policy search*. There
    is also a hybrid, *actor-critic* approach, which employs both value functions
    and policy search. We will now explain these approaches and other useful concepts
    for solving RL problems.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了 RL 中使用的关键形式化工具，即 MDP，并简要提到了一些 RL 中的挑战。接下来，我们将区分不同类别的 RL 算法。解决
    RL 问题的主要方法有两种：基于*价值函数*的方法和基于*策略搜索*的方法。还有一种混合的*演员-评论家*方法，它同时采用价值函数和策略搜索。我们现在将解释这些方法和其他解决
    RL 问题的有用概念。
- en: III-A Value Functions
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 价值函数
- en: 'Value function methods are based on estimating the value (expected return)
    of being in a given state. The *state-value function* $V^{\pi}(\mathbf{s})$ is
    the expected return when starting in state $\mathbf{s}$ and following $\pi$ henceforth:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数方法基于估计处于给定状态的价值（期望回报）。*状态价值函数* $V^{\pi}(\mathbf{s})$ 是从状态 $\mathbf{s}$ 开始并遵循
    $\pi$ 的期望回报：
- en: '|  | $\displaystyle V^{\pi}(\mathbf{s})=\mathbb{E}[R&#124;\mathbf{s},\pi]$
    |  | (2) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{\pi}(\mathbf{s})=\mathbb{E}[R| \mathbf{s},\pi]$ |  |
    (2) |'
- en: The optimal policy, $\pi^{*}$, has a corresponding state-value function $V^{*}(\mathbf{s})$,
    and vice-versa, the optimal state-value function can be defined as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略 $\pi^{*}$ 有一个对应的状态价值函数 $V^{*}(\mathbf{s})$，反之亦然，最优状态价值函数可以定义为
- en: '|  | $\displaystyle V^{*}(\mathbf{s})=\max_{\pi}V^{\pi}(\mathbf{s})\quad\forall\mathbf{s}\in\mathcal{S}.$
    |  | (3) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V^{*}(\mathbf{s})=\max_{\pi}V^{\pi}(\mathbf{s})\quad\forall\mathbf{s}\in\mathcal{S}.$
    |  | (3) |'
- en: If we had $V^{*}(\mathbf{s})$ available, the optimal policy could be retrieved
    by choosing among all actions available at $\mathbf{s}_{t}$ and picking the action
    $\mathbf{a}$ that maximises $\mathbb{E}_{\mathbf{s}_{t+1}\sim\mathcal{T}(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a})}[V^{*}(\mathbf{s}_{t+1})]$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有 $V^{*}(\mathbf{s})$ 可用，最优策略可以通过在 $\mathbf{s}_{t}$ 处选择所有可用动作中的最大值，并选择使
    $\mathbb{E}_{\mathbf{s}_{t+1}\sim\mathcal{T}(\mathbf{s}_{t+1}|\mathbf{s}_{t},\mathbf{a})}[V^{*}(\mathbf{s}_{t+1})]$
    最大化的动作 $\mathbf{a}$ 来检索。
- en: 'In the RL setting, the transition dynamics $\mathcal{T}$ are unavailable. Therefore,
    we construct another function, the *state-action-value* or *quality function*
    $Q^{\pi}(\mathbf{s},\mathbf{a})$, which is similar to $V^{\pi}$, except that the
    initial action $\mathbf{a}$ is provided, and $\pi$ is only followed from the succeeding
    state onwards:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RL 设置中，转移动态 $\mathcal{T}$ 是不可用的。因此，我们构造了另一个函数，*状态-动作值*或*质量函数* $Q^{\pi}(\mathbf{s},\mathbf{a})$，它类似于
    $V^{\pi}$，只是提供了初始动作 $\mathbf{a}$，并且 $\pi$ 仅从后续状态开始遵循：
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})=\mathbb{E}[R&#124;\mathbf{s},\mathbf{a},\pi].$
    |  | (4) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s},\mathbf{a})=\mathbb{E}[R| \mathbf{s},\mathbf{a},\pi].$
    |  | (4) |'
- en: 'The best policy, given $Q^{\pi}(\mathbf{s},\mathbf{a})$, can be found by choosing
    $\mathbf{a}$ greedily at every state: $\operatorname*{argmax}_{\mathbf{a}}Q^{\pi}(\mathbf{s},\mathbf{a})$.
    Under this policy, we can also define $V^{\pi}(\mathbf{s})$ by maximising $Q^{\pi}(\mathbf{s},\mathbf{a})$:
    $V^{\pi}(\mathbf{s})=\max_{\mathbf{a}}Q^{\pi}(\mathbf{s},\mathbf{a})$.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 给定$Q^{\pi}(\mathbf{s},\mathbf{a})$，可以通过在每个状态下贪婪地选择$\mathbf{a}$来找到最佳策略：$\operatorname*{argmax}_{\mathbf{a}}Q^{\pi}(\mathbf{s},\mathbf{a})$。在此策略下，我们还可以通过最大化$Q^{\pi}(\mathbf{s},\mathbf{a})$来定义$V^{\pi}(\mathbf{s})$：$V^{\pi}(\mathbf{s})=\max_{\mathbf{a}}Q^{\pi}(\mathbf{s},\mathbf{a})$。
- en: 'Dynamic Programming: To actually learn $Q^{\pi}$, we exploit the Markov property
    and define the function as a Bellman equation [[13](#bib.bib13)], which has the
    following recursive form:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划：为了实际学习$Q^{\pi}$，我们利用马尔可夫性质，并将函数定义为Bellman方程[[13](#bib.bib13)]，其递归形式如下：
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})=\mathbb{E}_{\mathbf{s}_{t+1}}[r_{t+1}+\gamma
    Q^{\pi}(\mathbf{s}_{t+1},\pi(\mathbf{s}_{t+1}))].$ |  | (5) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})=\mathbb{E}_{\mathbf{s}_{t+1}}[r_{t+1}+\gamma
    Q^{\pi}(\mathbf{s}_{t+1},\pi(\mathbf{s}_{t+1}))].$ |  | (5) |'
- en: 'This means that $Q^{\pi}$ can be improved by *bootstrapping*, i.e., we can
    use the current values of our estimate of $Q^{\pi}$ to improve our estimate. This
    is the foundation of $Q$-learning [[159](#bib.bib159)] and the state-action-reward-state-action
    (SARSA) algorithm [[112](#bib.bib112)]:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着$Q^{\pi}$可以通过*自助法*得到改进，即我们可以利用当前$Q^{\pi}$估计的值来改进我们的估计。这是$Q$-学习[[159](#bib.bib159)]和状态-动作-奖励-状态-动作（SARSA）算法[[112](#bib.bib112)]的基础：
- en: '|  | $\displaystyle Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})\leftarrow Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})+\alpha\delta,$
    |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})\leftarrow Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})+\alpha\delta,$
    |  | (6) |'
- en: where $\alpha$ is the learning rate and $\delta=Y-Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})$
    the temporal difference (TD) error; here, $Y$ is a target as in a standard regression
    problem. SARSA, an *on-policy* learning algorithm, is used to improve the estimate
    of $Q^{\pi}$ by using transitions generated by the behavioural policy (the policy
    derived from $Q^{\pi}$), which results in setting $Y=r_{t}+\gamma Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1})$.
    $Q$-learning is *off-policy*, as $Q^{\pi}$ is instead updated by transitions that
    were not necessarily generated by the derived policy. Instead, $Q$-learning uses
    $Y=r_{t}+\gamma\max_{\mathbf{a}}Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a})$, which directly
    approximates $Q^{*}$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\alpha$是学习率，$\delta=Y-Q^{\pi}(\mathbf{s}_{t},\mathbf{a}_{t})$是时间差（TD）误差；这里，$Y$是标准回归问题中的目标。SARSA是一种*在策略*学习算法，用于通过使用行为策略（从$Q^{\pi}$推导出的策略）生成的过渡来改进$Q^{\pi}$的估计，这导致设置$Y=r_{t}+\gamma
    Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1})$。$Q$-学习是*离策略*的，因为$Q^{\pi}$是通过不一定由推导策略生成的过渡来更新的。相反，$Q$-学习使用$Y=r_{t}+\gamma\max_{\mathbf{a}}Q^{\pi}(\mathbf{s}_{t+1},\mathbf{a})$，直接逼近$Q^{*}$。
- en: To find $Q^{*}$ from an arbitrary $Q^{\pi}$, we use *generalised policy iteration*,
    where policy iteration consists of *policy evaluation* and *policy improvement*.
    Policy evaluation improves the estimate of the value function, which can be achieved
    by minimising TD errors from trajectories experienced by following the policy.
    As the estimate improves, the policy can naturally be improved by choosing actions
    greedily based on the updated value function. Instead of performing these steps
    separately to convergence (as in policy iteration), generalised policy iteration
    allows for interleaving the steps, such that progress can be made more rapidly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要从任意的$Q^{\pi}$中找到$Q^{*}$，我们使用*广义策略迭代*，其中策略迭代包括*策略评估*和*策略改进*。策略评估通过最小化跟随策略的轨迹中的TD误差来改进价值函数的估计。随着估计的改进，策略可以通过基于更新后的价值函数贪婪地选择动作自然地得到改进。与单独执行这些步骤直至收敛（如在策略迭代中）不同，广义策略迭代允许交替执行这些步骤，从而使进展更快。
- en: III-B Sampling
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 采样
- en: Instead of bootstrapping value functions using dynamic programming methods,
    Monte Carlo methods estimate the expected return ([2](#S3.E2 "In III-A Value Functions
    ‣ III Reinforcement Learning Algorithms ‣ A Brief Survey of Deep Reinforcement
    Learning")) from a state by averaging the return from multiple rollouts of a policy.
    Because of this, pure Monte Carlo methods can also be applied in non-Markovian
    environments. On the other hand, they can only be used in episodic MDPs, as a
    rollout has to terminate for the return to be calculated. It is possible to get
    the best of both methods by combining TD learning and Monte Carlo policy evaluation,
    as in done in the TD($\lambda$) algorithm [[135](#bib.bib135)]. Similarly to the
    discount factor, the $\lambda$ in TD($\lambda$) is used to interpolate between
    Monte Carlo evaluation and bootstrapping. As demonstrated in Figure [4](#S3.F4
    "Figure 4 ‣ III-B Sampling ‣ III Reinforcement Learning Algorithms ‣ A Brief Survey
    of Deep Reinforcement Learning"), this results in an entire spectrum of RL methods
    based around the amount of sampling utilised.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用动态规划方法引导价值函数不同，蒙特卡罗方法通过平均多个策略回合的回报来估计来自状态的预期回报（[2](#S3.E2 "在 III-A 价值函数 ‣
    III 强化学习算法 ‣ 深度强化学习简要调查")）。因此，纯蒙特卡罗方法也可以应用于非马尔科夫环境。另一方面，它们只能用于回合式 MDP，因为必须终止回合才能计算回报。通过将
    TD 学习和蒙特卡罗策略评估结合在一起，例如在 TD($\lambda$) 算法中[[135](#bib.bib135)]，可以获得两种方法的最佳效果。与折扣因子类似，TD($\lambda$)
    中的 $\lambda$ 用于在蒙特卡罗评估和引导法之间进行插值。如图 [4](#S3.F4 "图 4 ‣ III-B 采样 ‣ III 强化学习算法 ‣
    深度强化学习简要调查") 所示，这导致了围绕采样量的整个 RL 方法谱。
- en: '![Refer to caption](img/f7d5c5b6c76ddf435bb42049afd78035.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f7d5c5b6c76ddf435bb42049afd78035.png)'
- en: 'Figure 3: Two dimensions of RL algorithms, based on the *backups* used to learn
    or construct a policy. At the extremes of these dimensions are (a) dynamic programming,
    (b) exhaustive search, (c) one-step TD learning and (d) pure Monte Carlo approaches.
    Bootstrapping extends from (c) 1-step TD learning to $n$-step TD learning methods
    [[135](#bib.bib135)], with (d) pure Monte Carlo approaches not relying on bootstrapping
    at all. Another possible dimension of variation is choosing to (c, d) sample actions
    versus (a, b) taking the expectation over all choices. Recreated from [[135](#bib.bib135)].'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：RL 算法的两个维度，基于用于学习或构建策略的*备份*方法。这些维度的极端情况是（a）动态规划，（b）穷举搜索，（c）一步 TD 学习和（d）纯蒙特卡罗方法。引导法从（c）1
    步 TD 学习扩展到 $n$ 步 TD 学习方法[[135](#bib.bib135)]，而（d）纯蒙特卡罗方法完全不依赖于引导法。另一种可能的变化维度是选择（c，d）采样动作与（a，b）对所有选择的期望。重新创建自[[135](#bib.bib135)]。
- en: '![Refer to caption](img/d25f4f2534139582e49dd6f2531f0c88.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d25f4f2534139582e49dd6f2531f0c88.png)'
- en: 'Figure 4: Actor-critic set-up. The actor (policy) receives a state from the
    environment and chooses an action to perform. At the same time, the critic (value
    function) receives the state and reward resulting from the previous interaction.
    The critic uses the TD error calculated from this information to update itself
    and the actor. Recreated from [[135](#bib.bib135)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：演员-评论家设置。演员（策略）从环境中接收一个状态，并选择一个动作来执行。与此同时，评论家（价值函数）接收来自先前交互的状态和奖励。评论家利用从这些信息中计算出的
    TD 误差来更新自己和演员。重新创建自[[135](#bib.bib135)]。
- en: Another major value-function based method relies on learning the *advantage*
    function $A^{\pi}(\mathbf{s},\mathbf{a})$ [[6](#bib.bib6), [43](#bib.bib43)].
    Unlike producing absolute state-action values, as with $Q^{\pi}$, $A^{\pi}$ instead
    represents relative state-action values. Learning relative values is akin to removing
    a baseline or average level of a signal; more intuitively, it is easier to learn
    that one action has better consequences than another, than it is to learn the
    actual return from taking the action. $A^{\pi}$ represents a relative advantage
    of actions through the simple relationship $A^{\pi}=Q^{\pi}-V^{\pi}$, and is also
    closely related to the baseline method of variance reduction within gradient-based
    policy search methods [[164](#bib.bib164)]. The idea of advantage updates has
    been utilised in many recent DRL algorithms [[157](#bib.bib157), [40](#bib.bib40),
    [85](#bib.bib85), [123](#bib.bib123)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要的基于价值函数的方法依赖于学习*优势*函数 $A^{\pi}(\mathbf{s},\mathbf{a})$ [[6](#bib.bib6),
    [43](#bib.bib43)]。与产生绝对状态-动作值（如 $Q^{\pi}$）不同，$A^{\pi}$ 表示的是相对状态-动作值。学习相对值类似于去除信号的基线或平均水平；更直观地说，学习一个动作比另一个动作有更好的后果比学习采取该动作的实际回报要容易。$A^{\pi}$
    通过简单关系 $A^{\pi}=Q^{\pi}-V^{\pi}$ 表示动作的相对优势，并且与梯度基础的策略搜索方法中的方差减少基线方法紧密相关[[164](#bib.bib164)]。优势更新的思想已被许多近期的深度强化学习算法所利用[[157](#bib.bib157),
    [40](#bib.bib40), [85](#bib.bib85), [123](#bib.bib123)]。
- en: III-C Policy Search
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 策略搜索
- en: Policy search methods do not need to maintain a value function model, but directly
    search for an optimal policy $\pi^{*}$. Typically, a parameterised policy $\pi_{\theta}$
    is chosen, whose parameters are updated to maximise the expected return $\mathbb{E}[R|\theta]$
    using either gradient-based or gradient-free optimisation [[26](#bib.bib26)].
    Neural networks that encode policies have been successfully trained using both
    gradient-free [[37](#bib.bib37), [23](#bib.bib23), [64](#bib.bib64)] and gradient-based
    [[164](#bib.bib164), [163](#bib.bib163), [46](#bib.bib46), [79](#bib.bib79), [122](#bib.bib122),
    [123](#bib.bib123), [74](#bib.bib74)] methods. Gradient-free optimisation can
    effectively cover low-dimensional parameter spaces, but despite some successes
    in applying them to large networks [[64](#bib.bib64)], gradient-based training
    remains the method of choice for most DRL algorithms, being more sample-efficient
    when policies possess a large number of parameters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 策略搜索方法不需要维持价值函数模型，而是直接搜索最优策略 $\pi^{*}$。通常，选择一个参数化策略 $\pi_{\theta}$，其参数通过使用梯度基础或无梯度优化来最大化期望回报
    $\mathbb{E}[R|\theta]$ [[26](#bib.bib26)]。编码策略的神经网络已经成功地使用无梯度[[37](#bib.bib37),
    [23](#bib.bib23), [64](#bib.bib64)] 和梯度基础[[164](#bib.bib164), [163](#bib.bib163),
    [46](#bib.bib46), [79](#bib.bib79), [122](#bib.bib122), [123](#bib.bib123), [74](#bib.bib74)]
    方法进行训练。无梯度优化可以有效覆盖低维参数空间，但尽管在将其应用于大网络[[64](#bib.bib64)]方面取得了一些成功，梯度基础训练仍然是大多数深度强化学习算法的首选方法，因为它在策略参数较多时更具样本效率。
- en: When constructing the policy directly, it is common to output parameters for
    a probability distribution; for continuous actions, this could be the mean and
    standard deviations of Gaussian distributions, whilst for discrete actions this
    could be the individual probabilities of a multinomial distribution. The result
    is a stochastic policy from which we can directly sample actions. With gradient-free
    methods, finding better policies requires a heuristic search across a predefined
    class of models. Methods such as evolution strategies essentially perform hill-climbing
    in a subspace of policies [[116](#bib.bib116)], whilst more complex methods, such
    as compressed network search, impose additional inductive biases [[64](#bib.bib64)].
    Perhaps the greatest advantage of gradient-free policy search is that they can
    also optimise non-differentiable policies.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接构建策略时，常常会输出概率分布的参数；对于连续动作，这可能是高斯分布的均值和标准差，而对于离散动作，这可能是多项式分布的各个概率。结果是一个随机策略，我们可以直接从中采样动作。使用无梯度方法时，寻找更好的策略需要在预定义的模型类别中进行启发式搜索。诸如进化策略的方法本质上在策略的一个子空间中进行爬升[[116](#bib.bib116)]，而更复杂的方法，如压缩网络搜索，则施加额外的归纳偏差[[64](#bib.bib64)]。无梯度策略搜索的最大优势之一是它们还可以优化不可微分的策略。
- en: 'Policy Gradients: Gradients can provide a strong learning signal as to how
    to improve a parameterised policy. However, to compute the expected return ([1](#S2.E1
    "In II-A Markov Decision Processes ‣ II Reward-driven Behaviour ‣ A Brief Survey
    of Deep Reinforcement Learning")) we need to average over plausible trajectories
    induced by the current policy parameterisation. This averaging requires either
    deterministic approximations (e.g., linearisation) or stochastic approximations
    via sampling [[26](#bib.bib26)]. Deterministic approximations can only be applied
    in a model-based setting where a model of the underlying transition dynamics is
    available. In the more common model-free RL setting, a Monte Carlo estimate of
    the expected return is determined. For gradient-based learning, this Monte Carlo
    approximation poses a challenge since gradients cannot pass through these samples
    of a stochastic function. Therefore, we turn to an estimator of the gradient,
    known in RL as the REINFORCE rule [[164](#bib.bib164)], elsewhere known as the
    score function [[34](#bib.bib34)] or likelihood-ratio estimator [[36](#bib.bib36)].
    The latter name is telling as using the estimator is similar to the practice of
    optimising the log-likelihood in supervised learning. Intuitively, gradient ascent
    using the estimator increases the log probability of the sampled action, weighted
    by the return. More formally, the REINFORCE rule can be used to compute the gradient
    of an expectation over a function $f$ of a random variable $X$ with respect to
    parameters $\theta$:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 政策梯度：梯度可以提供强大的学习信号，指导如何改进参数化政策。然而，为了计算期望回报（[1](#S2.E1 "In II-A Markov Decision
    Processes ‣ II Reward-driven Behaviour ‣ A Brief Survey of Deep Reinforcement
    Learning")），我们需要对当前政策参数化诱导的合理轨迹进行平均。这种平均需要确定性近似（例如，线性化）或通过采样的随机近似[[26](#bib.bib26)]。确定性近似只能在模型基础的设置中应用，其中需要一个底层转移动态的模型。在更常见的无模型RL设置中，确定期望回报的蒙特卡罗估计被使用。对于基于梯度的学习，这种蒙特卡罗近似带来了挑战，因为梯度无法通过这些随机函数的样本。因此，我们转向梯度的估计器，在RL中称为REINFORCE规则[[164](#bib.bib164)]，在其他地方称为得分函数[[34](#bib.bib34)]或似然比估计器[[36](#bib.bib36)]。后一个名称很有说明性，因为使用估计器类似于在监督学习中优化对数似然。直观地说，使用估计器的梯度上升增加了采样动作的对数概率，加权由回报。更正式地，REINFORCE规则可以用来计算对随机变量$X$的函数$f$的期望的梯度，相对于参数$\theta$：
- en: '|  | $\displaystyle\nabla_{\theta}\mathbb{E}_{X}[f(X;\theta)]=\mathbb{E}_{X}[f(X;\theta)\nabla_{\theta}\log
    p(X)].$ |  | (7) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}\mathbb{E}_{X}[f(X;\theta)]=\mathbb{E}_{X}[f(X;\theta)\nabla_{\theta}\log
    p(X)].$ |  | (7) |'
- en: As this computation relies on the empirical return of a trajectory, the resulting
    gradients possess a high variance. By introducing unbiased estimates that are
    less noisy it is possible to reduce the variance. The general methodology for
    performing this is to subtract a baseline, which means weighting updates by an
    advantage rather than the pure return. The simplest baseline is the average return
    taken over several episodes [[164](#bib.bib164)], but many more options are available
    [[123](#bib.bib123)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该计算依赖于轨迹的经验回报，结果梯度具有较高的方差。通过引入噪声较少的无偏估计，可以减少方差。执行这一过程的一般方法是减去基线，这意味着通过优势而不是纯回报来加权更新。最简单的基线是取多个回合的平均回报[[164](#bib.bib164)]，但还有许多其他选项[[123](#bib.bib123)]。
- en: 'Actor-critic Methods: It is possible to combine value functions with an explicit
    representation of the policy, resulting in actor-critic methods, as shown in Figure
    [4](#S3.F4 "Figure 4 ‣ III-B Sampling ‣ III Reinforcement Learning Algorithms
    ‣ A Brief Survey of Deep Reinforcement Learning"). The “actor” (policy) learns
    by using feedback from the “critic” (value function). In doing so, these methods
    trade off variance reduction of policy gradients with bias introduction from value
    function methods [[63](#bib.bib63), [123](#bib.bib123)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家方法：可以将价值函数与政策的显式表示结合，形成演员-评论家方法，如图[4](#S3.F4 "Figure 4 ‣ III-B Sampling
    ‣ III Reinforcement Learning Algorithms ‣ A Brief Survey of Deep Reinforcement
    Learning")所示。“演员”（政策）通过使用来自“评论家”（价值函数）的反馈进行学习。这样，这些方法在政策梯度的方差减少与来自价值函数方法的偏差引入之间进行权衡[[63](#bib.bib63),
    [123](#bib.bib123)]。
- en: Actor-critic methods use the value function as a baseline for policy gradients,
    such that the only fundamental difference between actor-critic methods and other
    baseline methods are that actor-critic methods utilise a *learned* value function.
    For this reason, we will later discuss actor-critic methods as a subset of policy
    gradient methods.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家方法使用价值函数作为策略梯度的基准，因此演员-评论家方法与其他基准方法的唯一根本区别在于演员-评论家方法利用了 *学习到的* 价值函数。因此，稍后我们将把演员-评论家方法作为策略梯度方法的一个子集来讨论。
- en: III-D Planning and Learning
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 规划与学习
- en: Given a model of the environment, it is possible to use dynamic programming
    over all possible actions (Figure [4](#S3.F4 "Figure 4 ‣ III-B Sampling ‣ III
    Reinforcement Learning Algorithms ‣ A Brief Survey of Deep Reinforcement Learning")
    (a)), sample trajectories for heuristic search (as was done by AlphaGo [[128](#bib.bib128)]),
    or even perform an exhaustive search (Figure [4](#S3.F4 "Figure 4 ‣ III-B Sampling
    ‣ III Reinforcement Learning Algorithms ‣ A Brief Survey of Deep Reinforcement
    Learning") (b)). Sutton and Barto [[135](#bib.bib135)] define *planning* as any
    method which utilises a model to produce or improve a policy. This includes *distribution
    models*, which include $\mathcal{T}$ and $\mathcal{R}$, and *sample models*, from
    which only samples of transitions can be drawn.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 给定环境模型，可以利用动态规划对所有可能的动作进行计算（图 [4](#S3.F4 "Figure 4 ‣ III-B Sampling ‣ III Reinforcement
    Learning Algorithms ‣ A Brief Survey of Deep Reinforcement Learning") (a)），进行启发式搜索（如AlphaGo
    [[128](#bib.bib128)]所做），甚至进行穷尽性搜索（图 [4](#S3.F4 "Figure 4 ‣ III-B Sampling ‣ III
    Reinforcement Learning Algorithms ‣ A Brief Survey of Deep Reinforcement Learning")
    (b)）。Sutton 和 Barto [[135](#bib.bib135)] 定义 *规划* 为任何利用模型来生成或改进策略的方法。这包括 *分布模型*，包括
    $\mathcal{T}$ 和 $\mathcal{R}$，以及 *样本模型*，从中只能抽取状态转移样本。
- en: In RL, we focus on learning without access to the underlying model of the environment.
    However, interactions with the environment could be used to learn value functions,
    policies, and also a model. Model-free RL methods learn directly from interactions
    with the environment, but model-based RL methods can simulate transitions using
    the learned model, resulting in increased sample efficiency. This is particularly
    important in domains where each interaction with the environment is expensive.
    However, learning a model introduces extra complexities, and there is always the
    danger of suffering from model errors, which in turn affects the learned policy;
    a common but partial solution in this latter scenario is to use model predictive
    control, where planning is repeated after small sequences of actions in the real
    environment [[16](#bib.bib16)]. Although deep neural networks can potentially
    produce very complex and rich models [[95](#bib.bib95), [132](#bib.bib132), [32](#bib.bib32)],
    sometimes simpler, more data-efficient methods are preferable [[40](#bib.bib40)].
    These considerations also play a role in actor-critic methods with learned value
    functions [[63](#bib.bib63), [123](#bib.bib123)].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，我们专注于在没有环境基本模型的情况下进行学习。然而，与环境的交互可以用来学习价值函数、策略，以及模型。无模型的强化学习方法直接从与环境的交互中学习，但基于模型的强化学习方法可以使用学习到的模型模拟状态转移，从而提高样本效率。这在每次与环境的交互成本很高的领域尤为重要。然而，学习一个模型会引入额外的复杂性，并且总是存在遭受模型误差的风险，这反过来会影响学习到的策略；在这种情况下，一种常见但部分的解决方案是使用模型预测控制，即在实际环境中经过小序列动作后重复规划[[16](#bib.bib16)]。尽管深度神经网络有可能产生非常复杂和丰富的模型[[95](#bib.bib95),
    [132](#bib.bib132), [32](#bib.bib32)]，有时更简单、更数据高效的方法更为可取[[40](#bib.bib40)]。这些考虑因素在使用学习到的价值函数的演员-评论家方法中也起着作用[[63](#bib.bib63),
    [123](#bib.bib123)]。
- en: III-E The Rise of DRL
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 深度强化学习的兴起
- en: Many of the successes in DRL have been based on scaling up prior work in RL
    to high-dimensional problems. This is due to the learning of low-dimensional feature
    representations and the powerful function approximation properties of neural networks.
    By means of representation learning, DRL can deal efficiently with the curse of
    dimensionality, unlike tabular and traditional non-parametric methods [[15](#bib.bib15)].
    For instance, convolutional neural networks (CNNs) can be used as components of
    RL agents, allowing them to learn directly from raw, high-dimensional visual inputs.
    In general, DRL is based on training deep neural networks to approximate the optimal
    policy $\pi^{*}$, and/or the optimal value functions $V^{*}$, $Q^{*}$ and $A^{*}$.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度强化学习（DRL）中，许多成功的案例基于将先前的强化学习（RL）工作扩展到高维问题。这是由于神经网络在低维特征表示学习和强大的函数逼近特性上的优势。通过表示学习，DRL能够有效处理维度灾难，这与表格型和传统的非参数方法不同[[15](#bib.bib15)]。例如，卷积神经网络（CNNs）可以作为RL代理的组成部分，使其能够直接从原始的高维视觉输入中学习。一般而言，DRL基于训练深度神经网络来逼近最优策略$\pi^{*}$和/或最优值函数$V^{*}$、$Q^{*}$和$A^{*}$。
- en: Although there have been DRL successes with gradient-free methods [[37](#bib.bib37),
    [23](#bib.bib23), [64](#bib.bib64)], the vast majority of current works rely on
    gradients and hence the backpropagation algorithm [[162](#bib.bib162), [111](#bib.bib111)].
    The primary motivation is that when available, gradients provide a strong learning
    signal. In reality, these gradients are estimated based on approximations, through
    sampling or otherwise, and as such we have to craft algorithms with useful inductive
    biases in order for them to be tractable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经有使用无梯度方法取得成功的DRL案例[[37](#bib.bib37), [23](#bib.bib23), [64](#bib.bib64)]，但绝大多数当前的工作依赖于梯度，因此使用反向传播算法[[162](#bib.bib162),
    [111](#bib.bib111)]。主要原因是，当可用时，梯度提供了强大的学习信号。实际上，这些梯度是基于近似值、采样或其他方式估算的，因此我们必须设计具有有用归纳偏差的算法，使其可处理。
- en: The other benefit of backpropagation is to view the optimisation of the expected
    return as the optimisation of a stochastic function [[121](#bib.bib121), [46](#bib.bib46)].
    This function can comprise of several parts—models, policies and value functions—which
    can be combined in various ways. The individual parts, such as value functions,
    may not directly optimise the expected return, but can instead embody useful information
    about the RL domain. For example, using a differentiable model and policy, it
    is possible to forward propagate and backpropagate through entire rollouts; on
    the other hand, innacuracies can accumulate over long time steps, and it may be
    be pertinent to instead use a value function to summarise the statistics of the
    rollouts [[46](#bib.bib46)]. We have previously mentioned that representation
    learning and function approximation are key to the success of DRL, but it is also
    true to say that the field of deep learning has inspired new ways of thinking
    about RL.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的另一个好处是将期望回报的优化视为随机函数的优化[[121](#bib.bib121), [46](#bib.bib46)]。这个函数可以包含多个部分——模型、策略和价值函数——可以以各种方式组合。个别部分，如价值函数，可能不会直接优化期望回报，但可以包含关于RL领域的有用信息。例如，使用可微分的模型和策略，可以在整个回合中进行前向传播和反向传播；另一方面，随着时间步长的延长，误差可能会累积，因此可能需要使用价值函数来总结回合的统计数据[[46](#bib.bib46)]。我们之前提到表示学习和函数逼近是DRL成功的关键，但也可以说，深度学习领域激发了对RL的新思维方式。
- en: Following our review of RL, we will now partition the next part of the survey
    into value function and policy search methods in DRL, starting with the well-known
    deep $Q$-network (DQN) [[84](#bib.bib84)]. In these sections, we will focus on
    state-of-the-art techniques, as well as the historical works they are built upon.
    The focus of the state-of-the-art techniques will be on those for which the state
    space is conveyed through visual inputs, e.g., images and video. To conclude,
    we will examine ongoing research areas and open challenges.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了RL后，我们将把调查的下一部分分为DRL中的值函数和策略搜索方法，从著名的深度$Q$-网络（DQN）[[84](#bib.bib84)]开始。在这些部分中，我们将关注最先进的技术，以及它们所建立的历史工作。最先进技术的重点将是那些通过视觉输入传达状态空间的技术，例如图像和视频。最后，我们将考察正在进行的研究领域和开放挑战。
- en: IV Value Functions
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 值函数
- en: The well-known function approximation properties of neural networks led naturally
    to the use of deep learning to regress functions for use in RL agents. Indeed,
    one of the earliest success stories in RL is TD-Gammon, a neural network that
    reached expert-level performance in Backgammon in the early 90s [[141](#bib.bib141)].
    Using TD methods, the network took in the state of the board to predict the probability
    of black or white winning. Although this simple idea has been echoed in later
    work [[128](#bib.bib128)], progress in RL research has favoured the explicit use
    of value functions, which can capture the structure underlying the environment.
    From early value function methods in DRL, which took simple states as input [[109](#bib.bib109)],
    current methods are now able to tackle visually and conceptually complex environments
    [[84](#bib.bib84), [122](#bib.bib122), [85](#bib.bib85), [96](#bib.bib96), [167](#bib.bib167)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的众所周知的函数近似特性自然导致了深度学习在RL代理中回归函数的应用。事实上，RL中的早期成功案例之一是TD-Gammon，它在90年代初期在飞行棋中达到了专家级别的表现
    [[141](#bib.bib141)]。使用TD方法，网络接受棋盘状态以预测黑白方获胜的概率。尽管这个简单的想法在后来的工作中有所延续 [[128](#bib.bib128)]，但RL研究的进展更倾向于明确使用价值函数，这些函数可以捕捉环境的底层结构。从早期在DRL中的简单状态输入的价值函数方法
    [[109](#bib.bib109)] 到现在能够处理视觉和概念上复杂环境的现有方法 [[84](#bib.bib84), [122](#bib.bib122),
    [85](#bib.bib85), [96](#bib.bib96), [167](#bib.bib167)]。
- en: IV-A Function Approximation and the DQN
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 函数近似与DQN
- en: We begin our survey of value-function-based DRL algorithms with the DQN [[84](#bib.bib84)],
    pictured in Figure [5](#S4.F5 "Figure 5 ‣ IV-A Function Approximation and the
    DQN ‣ IV Value Functions ‣ A Brief Survey of Deep Reinforcement Learning"), which
    achieved scores across a wide range of classic Atari 2600 video games [[10](#bib.bib10)]
    that were comparable to that of a professional video games tester. The inputs
    to the DQN are four greyscale frames of the game, concatenated over time, which
    are initially processed by several convolutional layers in order to extract spatiotemporal
    features, such as the movement of the ball in “Pong” or “Breakout.” The final
    feature map from the convolutional layers is processed by several fully connected
    layers, which more implicitly encode the effects of actions. This contrasts with
    more traditional controllers that use fixed preprocessing steps, which are therefore
    unable to adapt their processing of the state in response to the learning signal.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始调查基于价值函数的深度强化学习（DRL）算法，其中包括DQN [[84](#bib.bib84)]，如图[5](#S4.F5 "图 5 ‣ IV-A
    函数近似与DQN ‣ IV 价值函数 ‣ 深度强化学习简要调查")所示。DQN在多个经典的Atari 2600视频游戏[[10](#bib.bib10)]中的得分与专业视频游戏测试员相当。DQN的输入是四帧时间上串联的灰度图像，这些图像最初由多个卷积层处理，以提取时空特征，例如“乒乓球”或“打砖块”中的球的运动。卷积层的最终特征图通过几个全连接层处理，这些层更隐式地编码了动作的效果。这与使用固定预处理步骤的传统控制器形成对比，后者无法根据学习信号调整其状态处理方式。
- en: '![Refer to caption](img/48b5357b99861716a1e2125932e9533a.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/48b5357b99861716a1e2125932e9533a.png)'
- en: 'Figure 5: The deep $Q$-network [[84](#bib.bib84)]. The network takes the state—a
    stack of greyscale frames from the video game—and processes it with convolutional
    and fully connected layers, with ReLU nonlinearities in between each layer. At
    the final layer, the network outputs a discrete action, which corresponds to one
    of the possible control inputs for the game. Given the current state and chosen
    action, the game returns a new score. The DQN uses the reward—the difference between
    the new score and the previous one—to learn from its decision. More precisely,
    the reward is used to update its estimate of $Q$, and the error between its previous
    estimate and its new estimate is backpropagated through the network.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 深度 $Q$-网络 [[84](#bib.bib84)]。该网络接收状态——来自视频游戏的灰度帧堆栈——并通过卷积层和全连接层进行处理，每层之间有ReLU非线性激活。在最终层，网络输出一个离散动作，该动作对应于游戏的一个可能控制输入。给定当前状态和选择的动作，游戏返回一个新的得分。DQN使用奖励——即新得分与之前得分的差值——来从其决策中学习。更准确地说，奖励用于更新其
    $Q$ 的估计值，并将其之前估计与新估计之间的误差通过网络反向传播。'
- en: A forerunner of the DQN—neural fitted $Q$ iteration (NFQ)—involved training
    a neural network to return the $Q$-value given a state-action pair [[109](#bib.bib109)].
    NFQ was later extended to train a network to drive a slot car using raw visual
    inputs from a camera over the race track, by combining a deep autoencoder to reduce
    the dimensionality of the inputs with a separate branch to predict $Q$-values
    [[69](#bib.bib69)]. Although the previous network could have been trained for
    both reconstruction and RL tasks simultaneously, it was both more reliable and
    computationally efficient to train the two parts of the network sequentially.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: DQN的前身——神经网络拟合$Q$迭代（NFQ）——涉及训练一个神经网络，以在给定状态-动作对时返回$Q$值[[109](#bib.bib109)]。NFQ后来被扩展为训练一个网络来驾驶小车，利用来自摄像头的原始视觉输入，并通过结合深度自编码器来减少输入的维度，以及一个单独的分支来预测$Q$值[[69](#bib.bib69)]。尽管之前的网络可以同时训练重建和强化学习任务，但分开训练网络的两个部分在可靠性和计算效率上都更优。
- en: The DQN [[84](#bib.bib84)] is closely related to the model proposed by Lange
    et al. [[69](#bib.bib69)], but was the first RL algorithm that was demonstrated
    to work directly from raw visual inputs and on a wide variety of environments.
    It was designed such that the final fully connected layer outputs $Q^{\pi}(\mathbf{s},\cdot)$
    for all action values in a discrete set of actions—in this case, the various directions
    of the joystick and the fire button. This not only enables the best action, $\operatorname*{argmax}_{\mathbf{a}}Q^{\pi}(\mathbf{s},\mathbf{a})$,
    to be chosen after a single forward pass of the network, but also allows the network
    to more easily encode action-independent knowledge in the lower, convolutional
    layers. With merely the goal of maximising its score on a video game, the DQN
    learns to extract salient visual features, jointly encoding objects, their movements,
    and, most importantly, their interactions. Using techniques originally developed
    for explaining the behaviour of CNNs in object recognition tasks, we can also
    inspect what parts of its view the agent considers important (see Figure [6](#S4.F6
    "Figure 6 ‣ IV-A Function Approximation and the DQN ‣ IV Value Functions ‣ A Brief
    Survey of Deep Reinforcement Learning")).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DQN[[84](#bib.bib84)]与Lange等人提出的模型[[69](#bib.bib69)]密切相关，但它是第一个被证明可以直接从原始视觉输入和在各种环境中工作的强化学习算法。它被设计成最终的全连接层输出离散动作集合中所有动作的$Q^{\pi}(\mathbf{s},\cdot)$——在这种情况下，是摇杆的各种方向和火按钮。这不仅使得在网络的单次前向传递后可以选择最佳动作$\operatorname*{argmax}_{\mathbf{a}}Q^{\pi}(\mathbf{s},\mathbf{a})$，还使网络能够更容易地在较低的卷积层中编码与动作无关的知识。DQN仅以最大化其在视频游戏中的得分为目标，学习提取显著的视觉特征，联合编码对象、其运动，最重要的是它们的交互。利用最初为解释CNN在对象识别任务中的行为而开发的技术，我们还可以检查代理认为重要的视图部分（见图[6](#S4.F6
    "Figure 6 ‣ IV-A Function Approximation and the DQN ‣ IV Value Functions ‣ A Brief
    Survey of Deep Reinforcement Learning")）。
- en: '![Refer to caption](img/3035db54b6ab409ad7a2c13017976fa3.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3035db54b6ab409ad7a2c13017976fa3.png)'
- en: 'Figure 6: Saliency map of a trained DQN [[84](#bib.bib84)] playing “Space Invaders”
    [[10](#bib.bib10)]. By backpropagating the training signal to the image space,
    it is possible to see what a neural-network-based agent is attending to. In this
    frame, the most salient points—shown with the red overlay—are the laser that the
    agent recently fired, and also the enemy that it anticipates hitting in a few
    time steps.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：训练后的DQN[[84](#bib.bib84)]玩“太空侵略者”[[10](#bib.bib10)]的显著性图。通过将训练信号反向传播到图像空间，可以看到基于神经网络的代理正在关注什么。在这个帧中，最显著的点——以红色覆盖显示——是代理最近发射的激光，以及它预期在几个时间步骤后击中的敌人。
- en: 'The true underlying state of the game is contained within 128 bytes of Atari
    2600 RAM. However, the DQN was designed to directly learn from visual inputs ($210\times
    160\text{pixel}$ 8-bit RGB images), which it takes as the state $\mathbf{s}$.
    It is impractical to represent $Q^{\pi}(\mathbf{s},\mathbf{a})$ exactly as a lookup
    table: When combined with 18 possible actions, we obtain a $Q$-table of size $|\mathcal{S}|\times|\mathcal{A}|=18\times
    256^{3\times 210\times 160}$. Even if it were feasible to create such a table,
    it would be sparsely populated, and information gained from one state-action pair
    cannot be propagated to other state-action pairs. The strength of the DQN lies
    in its ability to compactly represent both high-dimensional observations and the
    $Q$-function using deep neural networks. Without this ability, tackling the discrete
    Atari domain from raw visual inputs would be impractical.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的真实底层状态包含在 128 字节的 Atari 2600 RAM 中。然而，DQN 设计为直接从视觉输入 ($210\times 160\text{pixel}$
    8 位 RGB 图像) 中学习，将其作为状态 $\mathbf{s}$。准确地将 $Q^{\pi}(\mathbf{s},\mathbf{a})$ 表示为查找表是不切实际的：与
    18 种可能的动作结合时，我们得到一个大小为 $|\mathcal{S}|\times|\mathcal{A}|=18\times 256^{3\times
    210\times 160}$ 的 $Q$ 表。即使能够创建这样的表，它也会稀疏填充，并且从一个状态-动作对获得的信息无法传播到其他状态-动作对。DQN 的优势在于其利用深度神经网络紧凑地表示高维观察和
    $Q$-函数的能力。如果没有这种能力，从原始视觉输入中处理离散的 Atari 领域将是不可行的。
- en: 'The DQN addressed the fundamental instability problem of using function approximation
    in RL [[145](#bib.bib145)] by the use of two techniques: experience replay [[80](#bib.bib80)]
    and target networks. Experience replay memory stores transitions of the form $(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{s}_{t+1},r_{t+1})$
    in a cyclic buffer, enabling the RL agent to sample from and train on previously
    observed data offline. Not only does this massively reduce the amount of interactions
    needed with the environment, but batches of experience can be sampled, reducing
    the variance of learning updates. Furthermore, by sampling uniformly from a large
    memory, the temporal correlations that can adversely affect RL algorithms are
    broken. Finally, from a practical perspective, batches of data can be efficiently
    processed in parallel by modern hardware, increasing throughput. Whilst the original
    DQN algorithm used uniform sampling [[84](#bib.bib84)], later work showed that
    prioritising samples based on TD errors is more effective for learning [[118](#bib.bib118)].
    We note that although experience replay is typically thought of as a model-free
    technique, it could actually be considered a simple model [[150](#bib.bib150)].'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 通过使用两种技术来解决 RL 中使用函数逼近的基本不稳定性问题 [[145](#bib.bib145)]：经验回放 [[80](#bib.bib80)]
    和目标网络。经验回放内存在循环缓冲区中存储形式为 $(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{s}_{t+1},r_{t+1})$
    的转换，使 RL 代理可以从先前观察到的数据中进行离线采样和训练。这不仅大大减少了与环境的交互量，而且可以从批量经验中采样，降低学习更新的方差。此外，通过从大型内存中均匀采样，打破了可能对
    RL 算法产生不利影响的时间相关性。最后，从实际角度来看，现代硬件可以高效地并行处理数据批次，提高吞吐量。虽然原始 DQN 算法使用了均匀采样 [[84](#bib.bib84)]，但后来的工作表明，基于
    TD 误差优先采样对学习更有效 [[118](#bib.bib118)]。我们注意到，尽管经验回放通常被认为是一种无模型技术，但它实际上可以被视为一种简单的模型
    [[150](#bib.bib150)]。
- en: The second stabilising method, introduced by Mnih et al. [[84](#bib.bib84)],
    is the use of a target network that initially contains the weights of the network
    enacting the policy, but is kept frozen for a large period of time. Rather than
    having to calculate the TD error based on its own rapidly fluctuating estimates
    of the $Q$-values, the policy network uses the fixed target network. During training,
    the weights of the target network are updated to match the policy network after
    a fixed number of steps. Both experience replay and target networks have gone
    on to be used in subsequent DRL works [[40](#bib.bib40), [79](#bib.bib79), [158](#bib.bib158),
    [89](#bib.bib89)].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种稳定化方法由 Mnih 等人提出 [[84](#bib.bib84)]，即使用目标网络，该网络最初包含实施策略的网络的权重，但在较长时间内保持不变。与其根据自身快速波动的
    $Q$ 值估计来计算 TD 误差不同，策略网络使用固定的目标网络。在训练过程中，目标网络的权重在固定的步骤数后更新以匹配策略网络。经验回放和目标网络已被后续的
    DRL 工作所采用 [[40](#bib.bib40), [79](#bib.bib79), [158](#bib.bib158), [89](#bib.bib89)]。
- en: IV-B $Q$-Function Modifications
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B $Q$-函数修改
- en: Considering that one of the key components of the DQN is a function approximator
    for the $Q$-function, it can benefit from fundamental advances in RL. van Hasselt
    [[148](#bib.bib148)] showed that the single estimator used in the $Q$-learning
    update rule overestimates the expected return due to the use of the maximum action
    value as an approximation of the maximum *expected* action value. Double-$Q$ learning
    provides a better estimate through the use of a double estimator [[148](#bib.bib148)].
    Whilst double-$Q$ learning requires an additional function to be learned, later
    work proposed using the already available target network from the DQN algorithm,
    resulting in significantly better results with only a small change in the update
    step [[149](#bib.bib149)]. A more radical proposal by Bellemare et al. [[12](#bib.bib12)]
    was to actually learn the full *value distribution*, rather than just the expectation;
    this provides additional information, such as whether the potential rewards come
    from a skewed or multimodal distribution. Although the resulting algorithm—based
    on learning categorical distributions—was used to construct the Categorical DQN,
    the benefits can potentially be applied to any RL algorithm that utilises learned
    value functions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 DQN 的关键组成部分之一是 $Q$-函数的函数近似器，它可以从 RL 的基本进展中获益。van Hasselt [[148](#bib.bib148)]
    显示，$Q$-学习更新规则中使用的单一估计器由于使用最大动作值作为最大 *期望* 动作值的近似，导致过度估计预期回报。双重 $Q$ 学习通过使用双重估计器
    [[148](#bib.bib148)] 提供了更好的估计。尽管双重 $Q$ 学习需要额外的函数来学习，但后来的一项工作建议使用 DQN 算法中已经存在的目标网络，这使得更新步骤只需进行小幅修改就能获得显著更好的结果
    [[149](#bib.bib149)]。Bellemare 等人 [[12](#bib.bib12)] 的更激进的提议是实际学习完整的 *价值分布*，而不仅仅是期望值；这提供了额外的信息，例如潜在奖励是否来自于偏斜或多峰分布。尽管基于学习分类分布的结果算法用于构建分类
    DQN，但其优势可能适用于任何利用学习价值函数的 RL 算法。
- en: Yet another way to adjust the DQN architecture is to decompose the $Q$-function
    into meaningful functions, such as constructing $Q^{\pi}$ by adding together separate
    layers that compute the state-value function $V^{\pi}$ and advantage function
    $A^{\pi}$ [[157](#bib.bib157)]. Rather than having to come up with accurate $Q$-values
    for all actions, the duelling DQN [[157](#bib.bib157)] benefits from a single
    baseline for the state in the form of $V^{\pi}$, and easier-to-learn relative
    values in the form of $A^{\pi}$. The combination of the duelling DQN with prioritised
    experience replay [[118](#bib.bib118)] is one of the state-of-the-art techniques
    in discrete action settings. Further insight into the properties of $A^{\pi}$
    by Gu et al. [[40](#bib.bib40)] led them to modify the DQN with a convex advantage
    layer that extended the algorithm to work over sets of continuous actions, creating
    the normalised advantage function (NAF) algorithm. Benefiting from experience
    replay, target networks and advantage updates, NAF is one of several state-of-the-art
    techniques in continuous control problems [[40](#bib.bib40)].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 调整 DQN 架构的另一种方法是将 $Q$-函数分解为有意义的函数，例如通过将计算状态值函数 $V^{\pi}$ 和优势函数 $A^{\pi}$ 的不同层相加来构造
    $Q^{\pi}$ [[157](#bib.bib157)]。与其对所有动作计算准确的 $Q$-值，双重 DQN [[157](#bib.bib157)]
    从 $V^{\pi}$ 形式的状态基准中受益，并且在 $A^{\pi}$ 形式的相对值中更容易学习。将双重 DQN 与优先经验回放 [[118](#bib.bib118)]
    结合，是离散动作设置中的最先进技术之一。Gu 等人对 $A^{\pi}$ 属性的进一步洞察 [[40](#bib.bib40)] 使他们修改了 DQN，引入了一个凸优势层，将算法扩展到连续动作集上，从而创建了归一化优势函数
    (NAF) 算法。NAF 通过经验回放、目标网络和优势更新获益，是解决连续控制问题的几种最先进技术之一 [[40](#bib.bib40)]。
- en: Some RL domains, such as recommender systems, have very large discrete action
    spaces, and hence may be difficult to directly deal with. Dulac-Arnold et al.
    [[30](#bib.bib30)] proposed learning “action embeddings” over the large set of
    original actions, and then using $k$-nearest neighbors to produce “proto-actions”
    which can be used with traditional RL methods. The idea of using representation
    learning to create distributed embeddings is a particular strength of DRL, and
    has been successfully utilised for other purposes [[161](#bib.bib161), [100](#bib.bib100)].
    Another related scenario in RL is when many actions need to be made simultaneously,
    such as specifying the torques in a many-jointed robot, which results in the action
    space growing exponentially. A naive but reasonable approach is to factorise the
    policy, treating each action independently [[115](#bib.bib115)]. An alternative
    is to construct an autoregressive policy, where each action in a single timestep
    is predicted conditionally on the state and previously chosen actions from the
    same timestep [[106](#bib.bib106), [5](#bib.bib5), [168](#bib.bib168)]. Metz et
    al. [[81](#bib.bib81)] used this idea in order to construct the sequential DQN,
    allowing them to discretise a large action space and outperform NAF—which is limited
    by its quadratic advantage function—in continous control problems. In a broader
    context, rather than dealing directly with primitive actions directly, one may
    choose to invoke “subpolicies” from higher-level policies [[136](#bib.bib136)];
    this concept, known as hierarchical reinforcement learning (HRL), will be discussed
    later.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一些强化学习领域，如推荐系统，具有非常大的离散动作空间，因此可能很难直接处理。Dulac-Arnold等人[[30](#bib.bib30)]提出在原始动作的大集合上学习“动作嵌入”，然后使用$k$-最近邻生成可以与传统强化学习方法一起使用的“原型动作”。使用表示学习创建分布式嵌入是深度强化学习的一个特殊优势，并已成功用于其他目的[[161](#bib.bib161),
    [100](#bib.bib100)]。强化学习中的另一个相关场景是需要同时进行许多动作，例如在多关节机器人中指定扭矩，这会导致动作空间呈指数级增长。一种简单但合理的方法是对策略进行因子分解，将每个动作独立处理[[115](#bib.bib115)]。另一种选择是构建自回归策略，其中在一个时间步中每个动作是条件预测的，基于状态和同一时间步中之前选择的动作[[106](#bib.bib106),
    [5](#bib.bib5), [168](#bib.bib168)]。Metz等人[[81](#bib.bib81)]利用这个想法构建了顺序DQN，使他们能够对大型动作空间进行离散化，并在连续控制问题中超越了NAF——其受限于其二次优势函数。在更广泛的背景下，与其直接处理原始动作，不如选择从更高层次的策略中调用“子策略”[[136](#bib.bib136)]；这一概念，称为层次强化学习（HRL），将在后续讨论中涉及。
- en: V Policy Search
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 策略搜索
- en: Policy search methods aim to directly find policies by means of gradient-free
    or gradient-based methods. Prior to the current surge of interest in DRL, several
    successful methods in DRL eschewed the commonly used backpropagation algorithm
    in favour of evolutionary algorithms [[37](#bib.bib37), [23](#bib.bib23), [64](#bib.bib64)],
    which are gradient-free policy search algorithms. Evolutionary methods rely on
    evaluating the performance of a population of agents. Hence, they are expensive
    for large populations or agents with many parameters. However, as black-box optimisation
    methods they can be used to optimise arbitrary, non-differentiable models and
    naturally allow for more exploration in parameter space. In combination with a
    compressed representation of neural network weights, evolutionary algorithms can
    even be used to train large networks; such a technique resulted in the first deep
    neural network to learn an RL task, straight from high-dimensional visual inputs
    [[64](#bib.bib64)]. Recent work has reignited interest in evolutionary methods
    for RL as they can potentially be distributed at larger scales than techniques
    that rely on gradients [[116](#bib.bib116)].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 策略搜索方法旨在通过无梯度或基于梯度的方法直接找到策略。在当前对深度强化学习（DRL）的关注激增之前，DRL中有几个成功的方法避开了常用的反向传播算法，而选择了进化算法[[37](#bib.bib37),
    [23](#bib.bib23), [64](#bib.bib64)]，这些都是无梯度的策略搜索算法。进化方法依赖于评估一组代理的表现。因此，对于大规模的人群或具有许多参数的代理来说，它们是昂贵的。然而，作为黑箱优化方法，它们可以用于优化任意的、不可微分的模型，并自然地允许更多的参数空间探索。结合神经网络权重的压缩表示，进化算法甚至可以用于训练大型网络；这种技术产生了第一个能够直接从高维视觉输入中学习强化学习任务的深度神经网络[[64](#bib.bib64)]。最近的研究重新点燃了对进化方法在强化学习中的兴趣，因为它们可以比依赖梯度的技术在更大规模上分布[[116](#bib.bib116)]。
- en: V-A Backpropagation through Stochastic Functions
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 通过随机函数的反向传播
- en: The workhorse of DRL, however, remains backpropagation [[162](#bib.bib162),
    [111](#bib.bib111)]. The previously discussed REINFORCE rule [[164](#bib.bib164)]
    allows neural networks to learn stochastic policies in a task-dependent manner,
    such as deciding where to look in an image to track [[120](#bib.bib120)], classify
    [[83](#bib.bib83)] or caption objects [[166](#bib.bib166)]. In these cases, the
    stochastic variable would determine the coordinates of a small crop of the image,
    and hence reduce the amount of computation needed. This usage of RL to make discrete,
    stochastic decisions over inputs is known in the deep learning literature as *hard
    attention*, and is one of the more compelling uses of basic policy search methods
    in recent years, having many applications outside of traditional RL domains. More
    generally, the ability to backpropagate through stochastic functions, using techniques
    such as REINFORCE [[164](#bib.bib164)] or the “reparameterisation trick” [[61](#bib.bib61),
    [108](#bib.bib108)], allows neural networks to be treated as stochastic computation
    graphs that can be optimised over [[121](#bib.bib121)], which is a key concept
    in algorithms such as stochastic value gradients (SVGs) [[46](#bib.bib46)].
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DRL的主力依然是反向传播[[162](#bib.bib162), [111](#bib.bib111)]。前面讨论的REINFORCE规则[[164](#bib.bib164)]允许神经网络以任务相关的方式学习随机策略，例如决定在图像中哪里查看以进行跟踪[[120](#bib.bib120)]、分类[[83](#bib.bib83)]或对对象进行描述[[166](#bib.bib166)]。在这些情况下，随机变量将决定图像的一个小区域的坐标，从而减少所需的计算量。使用RL对输入进行离散的、随机的决策被称为*硬注意力*，这是近年来基本策略搜索方法的一个更具吸引力的应用，有许多应用在传统RL领域之外。更一般地，能够通过随机函数进行反向传播，使用如REINFORCE[[164](#bib.bib164)]或“重参数化技巧”[[61](#bib.bib61),
    [108](#bib.bib108)]等技术，使神经网络可以被视为可以优化的随机计算图[[121](#bib.bib121)]，这是诸如随机值梯度（SVGs）[[46](#bib.bib46)]等算法的关键概念。
- en: V-B Compounding Errors
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 累积误差
- en: Searching directly for a policy represented by a neural network with very many
    parameters can be difficult and can suffer from severe local minima. One way around
    this is to use guided policy search (GPS), which takes a few sequences of actions
    from another controller (which could be constructed using a separate method, such
    as optimal control). GPS learns from them by using supervised learning in combination
    with importance sampling, which corrects for off-policy samples [[73](#bib.bib73)].
    This approach effectively biases the search towards a good (local) optimum. GPS
    works in a loop, by optimising policies to match sampled trajectories, and optimising
    trajectory distributions to match the policy and minimise costs. Initially, GPS
    was used to train neural networks on simulated continuous RL problems [[72](#bib.bib72)],
    but was later utilised to train a policy for a real robot based on visual inputs
    [[74](#bib.bib74)]. This research by Levine et al. [[74](#bib.bib74)] showed that
    it was possible to train visuomotor policies for a robot “end-to-end”, straight
    from the RGB pixels of the camera to motor torques, and, hence, is one of the
    seminal works in DRL.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直接搜索由参数非常多的神经网络表示的策略可能会很困难，并且可能会遭遇严重的局部最小值。一种解决方法是使用引导策略搜索（GPS），它利用来自其他控制器（可以使用如最优控制等单独的方法构建）的几个动作序列。GPS通过结合监督学习和重要性采样从这些序列中学习，重要性采样纠正了偏离策略的样本[[73](#bib.bib73)]。这种方法有效地将搜索偏向于一个良好的（局部）最优解。GPS通过优化策略以匹配采样的轨迹，并优化轨迹分布以匹配策略和最小化成本来工作。最初，GPS用于在模拟的连续RL问题上训练神经网络[[72](#bib.bib72)]，但后来被用于基于视觉输入训练真实机器人策略[[74](#bib.bib74)]。Levine等人[[74](#bib.bib74)]的研究表明，可以从相机的RGB像素到电机扭矩“端到端”地训练机器人视觉运动策略，因此，这项研究是DRL领域的重要工作之一。
- en: A more commonly used method is to use a trust region, in which optimisation
    steps are restricted to lie within a region where the approximation of the true
    cost function still holds. By preventing updated policies from deviating too wildly
    from previous policies, the chance of a catastrophically bad update is lessened,
    and many algorithms that use trust regions guarantee or practically result in
    monotonic improvement in policy performance. The idea of constraining each policy
    gradient update, as measured by the Kullback-Leibler (KL) divergence between the
    current and proposed policy, has a long history in RL [[57](#bib.bib57), [4](#bib.bib4),
    [59](#bib.bib59), [103](#bib.bib103)]. One of the newer algorithms in this line
    of work, trust region policy optimisation (TRPO), has been shown to be relatively
    robust and applicable to domains with high-dimensional inputs [[122](#bib.bib122)].
    To achieve this, TRPO optimises a *surrogate* objective function—specifically,
    it optimises an (importance sampled) advantage estimate, constrained using a quadratic
    approximation of the KL divergence. Whilst TRPO can be used as a pure policy gradient
    method with a simple baseline, later work by Schulman et al. [[123](#bib.bib123)]
    introduced generalised advantage estimation (GAE), which proposed several, more
    advanced variance reduction baselines. The combination of TRPO and GAE remains
    one of the state-of-the-art RL techniques in continuous control. However, the
    constrained optimisation of TRPO requires calculating second-order gradients,
    limiting its applicability. In contrast, the newer proximal policy optimisation
    (PPO) algorithm performs unconstrained optimisation, requiring only first-order
    gradient information [[1](#bib.bib1), [47](#bib.bib47), [125](#bib.bib125)]. The
    two main variants include an adaptive penalty on the KL divergence, and a heuristic
    clipped objective which is independent of the KL divergence [[125](#bib.bib125)].
    Being less expensive whilst retaining the performance of TRPO means that PPO (with
    or without GAE) is gaining popularity for a range of RL tasks [[47](#bib.bib47),
    [125](#bib.bib125)].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更常用的方法是使用信任区域，其中优化步骤被限制在一个近似真实成本函数仍然有效的区域内。通过防止更新的策略过于偏离先前的策略，可以降低灾难性坏更新的机会，并且许多使用信任区域的算法保证或实际上导致策略性能的单调改善。将每次策略梯度更新限制在当前策略与建议策略之间的Kullback-Leibler
    (KL)散度的历史在强化学习中已有悠久的历史[[57](#bib.bib57), [4](#bib.bib4), [59](#bib.bib59), [103](#bib.bib103)]。这一研究方向中的一种较新算法，信任区域策略优化（TRPO），已被证明在高维输入领域中相对鲁棒且适用[[122](#bib.bib122)]。为实现这一点，TRPO优化一个*代理*目标函数——具体来说，它优化一个（重要性抽样的）优势估计，并使用KL散度的二次近似进行约束。虽然TRPO可以作为一种纯粹的策略梯度方法与简单基线一起使用，但Schulman等人后来的工作[[123](#bib.bib123)]引入了广义优势估计（GAE），提出了几种更先进的方差减少基线。TRPO与GAE的结合仍然是连续控制中的最先进的强化学习技术之一。然而，TRPO的约束优化需要计算二阶梯度，这限制了它的适用性。相比之下，较新的近端策略优化（PPO）算法执行无约束优化，仅需要一阶梯度信息[[1](#bib.bib1),
    [47](#bib.bib47), [125](#bib.bib125)]。主要的两个变体包括对KL散度的自适应惩罚，以及一个与KL散度无关的启发式剪切目标[[125](#bib.bib125)]。由于PPO在保留TRPO性能的同时成本较低，因此在各种强化学习任务中（无论是否使用GAE）越来越受欢迎[[47](#bib.bib47),
    [125](#bib.bib125)]。
- en: V-C Actor-Critic Methods
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C演员-评论家方法
- en: Instead of utilising the average of several Monte Carlo returns as the baseline
    for policy gradient methods, actor-critic approaches have grown in popularity
    as an effective means of combining the benefits of policy search methods with
    learned value functions, which are able to learn from full returns and/or TD errors.
    They can benefit from improvements in both policy gradient methods, such as GAE
    [[123](#bib.bib123)], and value function methods, such as target networks [[84](#bib.bib84)].
    In the last few years, DRL actor-critic methods have been scaled up from learning
    simulated physics tasks [[46](#bib.bib46), [79](#bib.bib79)] to real robotic visual
    navigation tasks [[167](#bib.bib167)], directly from image pixels.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于利用多个蒙特卡罗回报的平均值作为策略梯度方法的基线，演员-评论家方法作为一种有效结合策略搜索方法和学习值函数的手段变得越来越受欢迎，后者能够从完整的回报和/或时序差分（TD）误差中学习。它们可以从策略梯度方法（如GAE[[123](#bib.bib123)]）和值函数方法（如目标网络[[84](#bib.bib84)]）的改进中受益。在过去几年里，深度强化学习（DRL）演员-评论家方法已经从学习模拟物理任务[[46](#bib.bib46),
    [79](#bib.bib79)]扩展到实际机器人视觉导航任务[[167](#bib.bib167)]，直接从图像像素中学习。
- en: One recent development in the context of actor-critic algorithms are deterministic
    policy gradients (DPGs) [[127](#bib.bib127)], which extend the standard policy
    gradient theorems for stochastic policies [[164](#bib.bib164)] to deterministic
    policies. One of the major advantages of DPGs is that, whilst stochastic policy
    gradients integrate over both state and action spaces, DPGs only integrate over
    the state space, requiring fewer samples in problems with large action spaces.
    In the initial work on DPGs, Silver et al. [[127](#bib.bib127)] introduced and
    demonstrated an off-policy actor-critic algorithm that vastly improved upon a
    stochastic policy gradient equivalent in high-dimensional continuous control problems.
    Later work introduced deep DPG (DDPG), which utilised neural networks to operate
    on high-dimensional, visual state spaces [[79](#bib.bib79)]. In the same vein
    as DPGs, Heess et al. [[46](#bib.bib46)] devised a method for calculating gradients
    to optimise stochastic policies, by “reparameterising” [[61](#bib.bib61), [108](#bib.bib108)]
    the stochasticity away from the network, thereby allowing standard gradients to
    be used (instead of the high-variance REINFORCE estimator [[164](#bib.bib164)]).
    The resulting SVG methods are flexible, and can be used both with (SVG(0) and
    SVG(1)) and without (SVG($\infty$)) value function critics, and with (SVG($\infty$)
    and SVG(1)) and without (SVG(0)) models. Later work proceeded to integrate DPGs
    and SVGs with RNNs, allowing them to solve continuous control problems in POMDPs,
    learning directly from pixels [[45](#bib.bib45)].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Value functions introduce a broadly applicable benefit in actor-critic methods—the
    ability to use off-policy data. On-policy methods can be more stable, whilst off-policy
    methods can be more data efficient, and hence there have been several attempts
    to merge the two [[158](#bib.bib158), [94](#bib.bib94), [41](#bib.bib41), [39](#bib.bib39),
    [42](#bib.bib42)]. Earlier work has either utilised a mix of on-policy and off-policy
    gradient updates [[158](#bib.bib158), [94](#bib.bib94), [39](#bib.bib39)], or
    used the off-policy data to train a value function in order to reduce the variance
    of on-policy gradient updates [[41](#bib.bib41)]. The more recent work by Gu et
    al. [[42](#bib.bib42)] unified these methods under interpolated policy gradients
    (IPGs), resulting in one of the newest state-of-the-art continuous DRL algorithms,
    and also providing insights for future research in this area. Together, the ideas
    behind IPGs and SVGs (of which DPGs can be considered a special case) form algorithmic
    approaches for improving learning efficiency in DRL.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: An orthogonal approach to speeding up learning is to exploit parallel computation.
    In particular, methods for training networks through asynchronous gradient updates
    have been developed for use on both single machines [[107](#bib.bib107)] and distributed
    systems [[25](#bib.bib25)]. By keeping a canonical set of parameters that are
    read by and updated in an asynchronous fashion by multiple copies of a single
    network, computation can be efficiently distributed over both processing cores
    in a single CPU, and across CPUs in a cluster of machines. Using a distributed
    system, Nair et al. [[91](#bib.bib91)] developed a framework for training multiple
    DQNs in parallel, achieving both better performance and a reduction in training
    time. However, the simpler asynchronous advantage actor-critic (A3C) algorithm
    [[85](#bib.bib85)], developed for both single and distributed machine settings,
    has become one of the most popular DRL techniques in recent times. A3C combines
    advantage updates with the actor-critic formulation, and relies on asynchronously
    updated policy and value function networks trained in parallel over several processing
    threads. The use of multiple agents, situated in their own, independent environments,
    not only stabilises improvements in the parameters, but conveys an additional
    benefit in allowing for more exploration to occur. A3C has been used as a standard
    starting point in many subsequent works, including the work of Zhu et al. [[167](#bib.bib167)],
    who applied it to robotic navigation in the real world through visual inputs.
    For simplicity, the underlying algorithm may be used with just one agent, termed
    advantage actor-critic (A2C) [[156](#bib.bib156)]. Alternatively, segments from
    the trajectories of multiple agents can be collected and processed together in
    a batch, with batch processing more efficiently enabled by GPUs; this synchronous
    version also goes by the name of A2C [[125](#bib.bib125)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 提高学习速度的另一个正交方法是利用并行计算。特别是，已经开发出用于单台机器 [[107](#bib.bib107)] 和分布式系统 [[25](#bib.bib25)]
    的异步梯度更新训练网络的方法。通过保持一个规范的参数集，该参数集由多个相同网络的副本以异步方式读取和更新，计算可以高效地分布在单个 CPU 的处理核心上以及机器集群中的
    CPU 上。使用分布式系统，Nair 等人 [[91](#bib.bib91)] 开发了一个用于并行训练多个 DQN 的框架，实现了更好的性能和训练时间的减少。然而，简化的异步优势
    actor-critic (A3C) 算法 [[85](#bib.bib85)]，旨在单机和分布式机器设置下使用，已经成为最近最流行的 DRL 技术之一。A3C
    结合了优势更新和 actor-critic 表述，依赖于异步更新的策略和价值函数网络，这些网络在多个处理线程上并行训练。多个代理在各自独立的环境中，不仅稳定了参数的改进，还带来了额外的好处，使更多的探索得以进行。A3C
    已作为许多后续工作的标准起点，包括 Zhu 等人 [[167](#bib.bib167)] 的工作，他们通过视觉输入将其应用于现实世界的机器人导航。为了简便，底层算法可以只用一个代理，即优势
    actor-critic (A2C) [[156](#bib.bib156)]。或者，可以收集和处理多个代理的轨迹片段，批处理可以通过 GPU 更高效地实现；这种同步版本也被称为
    A2C [[125](#bib.bib125)]。
- en: There have been several major advancements on the original A3C algorithm that
    reflect various motivations in the field of DRL. The first is actor-critic with
    experience replay [[158](#bib.bib158), [39](#bib.bib39)], which adds Retrace($\lambda$)
    off-policy bias correction [[88](#bib.bib88)] to a $Q$-value-based A3C, allowing
    it to use experience replay in order to improve sample complexity. Others have
    attempted to bridge the gap between value and policy-based RL, utilising theoretical
    advancements to improve upon the original A3C [[89](#bib.bib89), [94](#bib.bib94),
    [124](#bib.bib124)]. Finally, there is a growing trend towards exploiting auxiliary
    tasks to improve the representations learned by DRL agents, and, hence, improve
    both the learning speed and final performance of these agents [[77](#bib.bib77),
    [54](#bib.bib54), [82](#bib.bib82)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对原始 A3C 算法的若干重大改进反映了 DRL 领域中的各种动机。首先是带有经验回放的 actor-critic [[158](#bib.bib158),
    [39](#bib.bib39)]，该方法在基于 $Q$ 值的 A3C 中添加了 Retrace($\lambda$) 离线策略偏差修正 [[88](#bib.bib88)]，使其能够利用经验回放以提高样本复杂度。其他研究者尝试弥合基于值和基于策略的
    RL 之间的差距，利用理论进展改进原始的 A3C [[89](#bib.bib89), [94](#bib.bib94), [124](#bib.bib124)]。最后，越来越多的趋势是利用辅助任务来改善
    DRL 代理学习的表示，从而提高这些代理的学习速度和最终性能 [[77](#bib.bib77), [54](#bib.bib54), [82](#bib.bib82)]。
- en: VI Current Research and Challenges
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 当前研究与挑战
- en: To conclude, we will highlight some current areas of research in DRL, and the
    challenges that still remain. Previously, we have focused mainly on model-free
    methods, but we will now examine a few model-based DRL algorithms in more detail.
    Model-based RL algorithms play an important role in making RL data-efficient and
    in trading off exploration and exploitation. After tackling exploration strategies,
    we shall then address HRL, which imposes an inductive bias on the final policy
    by explicitly factorising it into several levels. When available, trajectories
    from other controllers can be used to bootstrap the learning process, leading
    us to imitation learning and inverse RL (IRL). For the final topic specific to
    RL, we will look at multi-agent systems, which have their own special considerations.
    We then bring to attention two broader areas—the use of RNNs, and transfer learning—in
    the context of DRL. We then examine the issue of evaluating RL, and current benchmarks
    for DRL.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Model-based RL
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key idea behind model-based RL is to learn a transition model that allows
    for simulation of the environment without interacting with the environment directly.
    Model-based RL does not assume specific prior knowledge. However, in practice,
    we can incorporate prior knowledge (e.g., physics-based models [[58](#bib.bib58)])
    to speed up learning. Model learning plays an important role in reducing the amount
    of required interactions with the (real) environment, which may be limited in
    practice. For example, it is unrealistic to perform millions of experiments with
    a robot in a reasonable amount of time and without significant hardware wear and
    tear. There are various approaches to learn predictive models of dynamical systems
    using pixel information. Based on the deep dynamical model [[154](#bib.bib154)],
    where high-dimensional observations are embedded into a lower-dimensional space
    using autoencoders, several model-based DRL algorithms have been proposed for
    learning models and policies from pixel information [[95](#bib.bib95), [160](#bib.bib160),
    [155](#bib.bib155)]. If a sufficiently accurate model of the environment can be
    learned, then even simple controllers can be used to control a robot directly
    from camera images [[32](#bib.bib32)]. Learned models can also be used to guide
    exploration purely based on simulation of the environment, with deep models allowing
    these techniques to be scaled up to high-dimensional visual domains [[132](#bib.bib132)].
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: A compelling insight on the benefits of neural-network-based models is that
    they can overcome some of the problems incurred by planning with imperfect models;
    in effect, by *embedding* the activations and predictions (outputs) of these models
    into a vector, a DRL agent can not only obtain more information than just the
    final result of any model rollouts, but it can also learn to downplay this information
    if it believes that the model is inaccurate [[161](#bib.bib161)]. This can be
    more efficient, though less principled, than Bayesian methods for propagating
    uncertainty [[52](#bib.bib52)]. Another way to make use of the flexiblity of neural-network-based
    models is to let them decide when to plan, that is, given a finite amount of computation,
    whether it is worth modelling one long trajectory, several short trajectories,
    anything in-between, or simply to take an action in the real environment [[100](#bib.bib100)].
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络模型的一个引人注目的好处是它们可以克服使用不完美模型进行规划时遇到的一些问题；实际上，通过*嵌入*这些模型的激活和预测（输出）到一个向量中，DRL代理不仅可以获得比任何模型模拟的最终结果更多的信息，而且如果它认为模型不准确，它还可以学会贬低这些信息[[161](#bib.bib161)]。这可能比贝叶斯方法传播不确定性[[52](#bib.bib52)]更高效，但原理性较差。利用神经网络模型的灵活性的一种方法是让它们决定何时进行规划，即在有限的计算量下，是否值得建模一个长的轨迹、几个短的轨迹、介于两者之间的轨迹，还是仅仅在真实环境中采取行动[[100](#bib.bib100)]。
- en: Although deep neural networks can make reasonable predictions in simulated environments
    over hundreds of timesteps [[21](#bib.bib21)], they typically require many samples
    to tune the large amount of parameters they contain. Training these models often
    requires more samples (interaction with the environment) than simpler models.
    For this reason, Gu et al. [[40](#bib.bib40)] train locally linear models for
    use with the NAF algorithm—the continuous equivalent of the DQN [[84](#bib.bib84)]—to
    improve the algorithm’s sample complexity in the robotic domain where samples
    are expensive. In order to spur the adoption of deep models in model-based DRL,
    it is necessary to find strategies that can be used in order to improve their
    data efficiency [[90](#bib.bib90)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度神经网络可以在模拟环境中对数百个时间步做出合理预测[[21](#bib.bib21)]，它们通常需要许多样本来调整其包含的大量参数。训练这些模型通常需要比简单模型更多的样本（与环境的互动）。因此，Gu等人[[40](#bib.bib40)]为NAF算法（DQN[[84](#bib.bib84)]的连续等效物）训练了局部线性模型，以提高在样本昂贵的机器人领域中算法的样本复杂性。为了促进深度模型在基于模型的DRL中的应用，有必要找到可以提高其数据效率的策略[[90](#bib.bib90)]。
- en: A less common but potentially useful paradigm exists between model-free and
    model-based methods—the successor representation (SR) [[24](#bib.bib24)]. Rather
    than picking actions directly or performing planning with models, learning $\mathcal{T}$
    is replaced with learning expected (discounted) future occupancies (SRs), which
    can be linearly combined with $\mathcal{R}$ in order to calculate the optimal
    action; this decomposition makes SRs more robust than model-free methods when
    the reward structure changes (but still fallible when $\mathcal{T}$ changes).
    Work extending SRs to deep neural networks has demonstrated its usefulness in
    multi-task settings, whilst within a complex visual environment [[66](#bib.bib66)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一种较少见但可能有用的范式存在于无模型和基于模型的方法之间——后继表示（SR）[[24](#bib.bib24)]。与直接选择行动或使用模型进行规划不同，学习$\mathcal{T}$被替换为学习预期的（折扣）未来占用率（SR），这些可以与$\mathcal{R}$线性结合以计算最佳行动；这种分解使SR在奖励结构变化时比无模型方法更为稳健（但在$\mathcal{T}$变化时仍然可能出错）。扩展SR到深度神经网络的研究已经证明了它在多任务环境中的有用性，同时在复杂视觉环境中[[66](#bib.bib66)]。
- en: VI-B Exploration vs. Exploitation
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 探索与利用
- en: 'One of the greatest difficulties in RL is the fundamental dilemma of *exploration
    versus exploitation*: When should the agent try out (perceived) non-optimal actions
    in order to explore the environment (and potentially improve the model), and when
    should it exploit the optimal action in order to make useful progress? Off-policy
    algorithms, such as the DQN [[84](#bib.bib84)], typically use the simple $\epsilon$-greedy
    exploration policy, which chooses a random action with probability $\epsilon\in[0,1]$,
    and the optimal action otherwise. By decreasing $\epsilon$ over time, the agent
    progresses towards exploitation. Although adding independent noise for exploration
    is usable in continuous control problems, more sophisticated strategies inject
    noise that is correlated over time (e.g., from stochastic processes) in order
    to better preserve momentum [[79](#bib.bib79)].'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的一个最大困难是*探索与开发*的基本困境：代理何时应该尝试（感知的）非最优动作以探索环境（并可能改善模型），以及何时应该利用最优动作以取得有用的进展？离线策略算法，例如
    DQN [[84](#bib.bib84)]，通常使用简单的 $\epsilon$-贪婪探索策略，该策略以概率 $\epsilon\in[0,1]$ 选择一个随机动作，否则选择最优动作。通过随着时间减少
    $\epsilon$，代理向开发方向发展。尽管在连续控制问题中添加独立噪声以进行探索是可用的，但更复杂的策略注入随时间相关的噪声（例如来自随机过程）以更好地保持动量
    [[79](#bib.bib79)]。
- en: The observation that temporal correlation is important led Osband et al. [[97](#bib.bib97)]
    to propose the bootstrapped DQN, which maintains several $Q$-value “heads” that
    learn different values through a combination of different weight initialisations
    and bootstrapped sampling from experience replay memory. At the beginning of each
    training episode, a different head is chosen, leading to temporally-extended exploration.
    Usunier et al. [[147](#bib.bib147)] later proposed a similar method that performed
    exploration in policy space by adding noise to a single output head, using zero-order
    gradient estimates to allow backpropagation through the policy.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 时间相关性的重要性观察促使 Osband 等人 [[97](#bib.bib97)] 提出了引导式 DQN，该方法维护多个 $Q$-值“头”，通过不同权重初始化和从经验回放记忆中引导式采样的组合来学习不同的值。在每次训练开始时，选择不同的头，从而实现时间上扩展的探索。Usunier
    等人 [[147](#bib.bib147)] 后来提出了一种类似的方法，该方法通过向单一输出头添加噪声，在策略空间中进行探索，使用零阶梯度估计来允许通过策略进行反向传播。
- en: One of the main principled exploration strategies is the *upper confidence bound*
    (UCB) algorithm, based on the principle of “optimism in the face of uncertainty”
    [[67](#bib.bib67)]. The idea behind UCB is to pick actions that maximise $\mathbb{E[R]}+\kappa\sigma[R]$,
    where $\sigma[R]$ is the standard deviation of the return and $\kappa>0$. UCB
    therefore encourages exploration in regions with high uncertainty and moderate
    expected return. Whilst easily achievable in small tabular cases, the use of powerful
    density models [[11](#bib.bib11)], or conversely, hashing [[139](#bib.bib139)],
    has allowed this algorithm to scale to high-dimensional visual domains with DRL.
    UCB is only one technique for trading off exploration and exploitation in the
    context of Bayesian optimisation [[126](#bib.bib126)]; future work in DRL may
    benefit from investigating other successful techniques that are used in Bayesian
    optimisation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的原则性探索策略之一是*上置信界*（UCB）算法，该算法基于“在不确定性面前的乐观”原则 [[67](#bib.bib67)]。UCB 的思想是选择最大化
    $\mathbb{E[R]}+\kappa\sigma[R]$ 的动作，其中 $\sigma[R]$ 是回报的标准差，$\kappa>0$。因此，UCB 鼓励在高不确定性和适度预期回报的区域进行探索。虽然在小型表格案例中很容易实现，但使用强大的密度模型
    [[11](#bib.bib11)]，或者相反，哈希 [[139](#bib.bib139)]，使得该算法能够扩展到具有 DRL 的高维视觉领域。UCB 只是贝叶斯优化上下文中探索与开发权衡的一种技术
    [[126](#bib.bib126)]；未来在 DRL 的研究中可能会受益于调查其他在贝叶斯优化中成功使用的技术。
- en: UCB can also be considered one way of implementing *intrinsic motivation*, which
    is a general concept that advocates decreasing uncertainty/making progress in
    learning about the environment [[119](#bib.bib119)]. There have been several DRL
    algorithms that try to implement intrinsic motivation via minimising model prediction
    error [[132](#bib.bib132), [101](#bib.bib101)] or maximising information gain
    [[86](#bib.bib86), [52](#bib.bib52)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: UCB 也可以被视为实现*内在动机*的一种方式，这是一个提倡减少不确定性/在学习环境中取得进展的普遍概念 [[119](#bib.bib119)]。已经有一些
    DRL 算法尝试通过最小化模型预测误差 [[132](#bib.bib132), [101](#bib.bib101)] 或最大化信息增益 [[86](#bib.bib86),
    [52](#bib.bib52)] 来实现内在动机。
- en: VI-C Hierarchical RL
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 层次化强化学习
- en: In the same way that deep learning relies on hierarchies of features, HRL relies
    on hierarchies of policies. Early work in this area introduced *options*, in which,
    apart from *primitive actions* (single-timestep actions), policies could also
    run other policies (multi-timestep “actions”) [[136](#bib.bib136)]. This approach
    allows top-level policies to focus on higher-level *goals*, whilst *subpolicies*
    are responsible for fine control. Several works in DRL have attempted HRL by using
    one top-level policy that chooses between subpolicies, where the division of states
    or goals in to subpolicies is achieved either manually [[2](#bib.bib2), [143](#bib.bib143),
    [65](#bib.bib65)] or automatically [[3](#bib.bib3), [151](#bib.bib151), [152](#bib.bib152)].
    One way to help construct subpolicies is to focus on discovering and reaching
    goals, which are specific states in the environment; they may often be locations,
    which an agent should navigate to. Whether utilised with HRL or not, the discovery
    and generalisation of goals is also an important area of ongoing research [[117](#bib.bib117),
    [66](#bib.bib66), [152](#bib.bib152)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 就像深度学习依赖于特征的层次结构一样，HRL（层次化强化学习）依赖于策略的层次结构。早期的工作引入了*选项*，在这种情况下，除了*基本动作*（单时间步动作），策略还可以运行其他策略（多时间步“动作”）[[136](#bib.bib136)]。这种方法允许顶层策略专注于更高层次的*目标*，而*子策略*负责细致控制。DRL（深度强化学习）中的几项工作尝试了HRL，通过使用一个顶层策略在子策略之间进行选择，状态或目标的划分到子策略要么是手动完成[[2](#bib.bib2),
    [143](#bib.bib143), [65](#bib.bib65)]，要么是自动完成[[3](#bib.bib3), [151](#bib.bib151),
    [152](#bib.bib152)]。构建子策略的一种方法是专注于发现和实现目标，这些目标是环境中的特定状态；它们通常是位置，智能体应当导航到这些位置。无论是否利用HRL，目标的发现和泛化也是一个重要的研究领域[[117](#bib.bib117),
    [66](#bib.bib66), [152](#bib.bib152)]。
- en: VI-D Imitation Learning and Inverse RL
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 模仿学习与逆向强化学习
- en: One may ask why, if given a sequence of “optimal” actions from expert demonstrations,
    it is not possible to use supervised learning in a straightforward manner—a case
    of “learning from demonstration”. This is indeed possible, and is known as *behavioural
    cloning* in traditional RL literature. Taking advantage of the stronger signals
    available in supervised learning problems, behavioural cloning enjoyed success
    in earlier neural network research, with the most notable success being ALVINN,
    one of the earliest autonomous cars [[104](#bib.bib104)]. However, behavioural
    cloning cannot adapt to new situations, and small deviations from the demonstration
    during the execution of the learned policy can compound and lead to scenarios
    where the policy is unable to recover. A more generalisable solution is to use
    provided trajectories to guide the learning of suitable state-action pairs, but
    fine-tune the agent using RL [[49](#bib.bib49)]. Alternatively, if the expert
    is still available to query during training, the agent can use active learning
    to gather extra data when it is unsure, allowing it to learn from states away
    from the optimal trajectories [[110](#bib.bib110)]. This has been applied to a
    deep learning setting, where a CNN trained in a visual navigation task with active
    learning significantly improved upon a pure imitation learning baseline [[53](#bib.bib53)].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会问，如果给定来自专家演示的“最佳”动作序列，为什么无法以直截了当的方式使用监督学习——即“从示范中学习”的情况。这确实是可能的，并且在传统RL文献中被称为*行为克隆*。利用监督学习问题中可用的强信号，行为克隆在早期神经网络研究中取得了成功，其中最著名的成功是ALVINN，它是最早的自动驾驶汽车之一[[104](#bib.bib104)]。然而，行为克隆不能适应新情况，学习的策略在执行过程中从示范中产生的小偏差可能会积累，导致策略无法恢复。一个更具泛化性的解决方案是使用提供的轨迹来指导适合的状态-动作对的学习，但使用RL进行微调[[49](#bib.bib49)]。或者，如果在训练过程中专家仍然可以查询，智能体可以使用主动学习在不确定时收集额外数据，从而允许它从远离最佳轨迹的状态中学习[[110](#bib.bib110)]。这已被应用于深度学习设置，其中在视觉导航任务中使用主动学习训练的CNN显著改善了纯模仿学习基线[[53](#bib.bib53)]。
- en: The goal of IRL is to estimate an unknown reward function from observed trajectories
    that characterise a desired solution [[92](#bib.bib92)]; IRL can be used in combination
    with RL to improve upon demonstrated behaviour. Using the power of deep neural
    networks, it is now possible to learn complex, nonlinear reward functions for
    IRL [[165](#bib.bib165)]. Ho and Ermon [[51](#bib.bib51)] showed that policies
    are uniquely characterised by their *occupancies* (visited state and action distributions)
    allowing IRL to be reduced to the problem of measure matching. With this insight,
    they were able to use generative adversarial training [[38](#bib.bib38)] to facilitate
    reward function learning in a more flexible manner, resulting in the generative
    adversarial imitation learning (GAIL) algorithm. GAIL was later extended to allow
    IRL to be applied even when receiving expert trajectories from a different visual
    viewpoint to that of the RL agent [[131](#bib.bib131)]. In complementary work,
    Baram et al. [[7](#bib.bib7)] exploit gradient information that was not used in
    GAIL to learn models within the IRL process.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 逆强化学习（IRL）的目标是从观察到的轨迹中估计一个未知的奖励函数，这些轨迹特征化了期望的解决方案[[92](#bib.bib92)]；IRL可以与强化学习结合使用，以改善展示的行为。利用深度神经网络的强大功能，现在可以学习复杂的、非线性的奖励函数用于IRL[[165](#bib.bib165)]。Ho
    和 Ermon [[51](#bib.bib51)] 显示了策略可以通过其*占据度*（访问的状态和动作分布）独特地表征，从而将IRL简化为度量匹配的问题。凭借这一见解，他们能够使用生成对抗训练[[38](#bib.bib38)]以更灵活的方式促进奖励函数学习，从而得到了生成对抗模仿学习（GAIL）算法。GAIL后来被扩展，使得IRL即便在接收到来自与RL代理不同视觉视角的专家轨迹时也能应用[[131](#bib.bib131)]。在互补的工作中，Baram
    等人[[7](#bib.bib7)]利用了在GAIL中未使用的梯度信息来学习IRL过程中的模型。
- en: VI-E Multi-agent RL
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E 多智能体强化学习
- en: Usually, RL considers a single learning agent in a stationary environment. In
    contrast, multi-agent RL (MARL) considers multiple agents learning through RL,
    and often the non-stationarity introduced by other agents changing their behaviours
    as they learn [[18](#bib.bib18)]. In DRL, the focus has been on enabling (differentiable)
    communication between agents, which allows them to co-operate. Several approaches
    have been proposed for this purpose, including passing messages to agents sequentially
    [[33](#bib.bib33)], using a bidirectional channel (providing ordering with less
    signal loss) [[102](#bib.bib102)], and an all-to-all channel [[134](#bib.bib134)].
    The addition of communication channels is a natural strategy to apply to MARL
    in complex scenarios and does not preclude the usual practice of modelling co-operative
    or competing agents as applied elsewhere in the MARL literature [[18](#bib.bib18)].
    Other DRL works of note in MARL investigate the effects of learning and sequential
    decision making in game theory [[48](#bib.bib48), [71](#bib.bib71)].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，强化学习（RL）考虑的是在静态环境中单一学习代理。与此相对，多智能体强化学习（MARL）则涉及多个代理通过强化学习进行学习，并且通常会因为其他代理在学习过程中行为变化而引入非静态性[[18](#bib.bib18)]。在深度强化学习（DRL）中，重点是使代理之间能够进行（可微分的）通信，这允许它们进行合作。为此，提出了几种方法，包括按顺序向代理传递消息[[33](#bib.bib33)]，使用双向通道（提供有序性且信号损失较少）[[102](#bib.bib102)]，以及全对全通道[[134](#bib.bib134)]。增加通信通道是将其应用于复杂场景中的MARL的自然策略，并不排除在MARL文献中应用于建模合作或竞争代理的常规做法[[18](#bib.bib18)]。其他在MARL中值得注意的DRL工作探讨了在博弈论中学习和顺序决策的影响[[48](#bib.bib48),
    [71](#bib.bib71)]。
- en: VI-F Memory and Attention
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-F 记忆与注意力
- en: As one of the earliest works in DRL the DQN spawned many extensions. One of
    the first extensions was converting the DQN into an RNN, which allows the network
    to better deal with POMDPs by integrating information over long time periods.
    Like recursive filters, recurrent connections provide an efficient means of acting
    conditionally on temporally distant prior observations. By using recurrent connections
    between its hidden units, the deep recurrent $Q$-network (DRQN) introduced by
    Hausknecht and Stone [[44](#bib.bib44)] was able to successfully infer the velocity
    of the ball in the game “Pong,” even when frames of the game were randomly blanked
    out. Further improvements were gained by introducing *attention*—a technique where
    additional connections are added from the recurrent units to lower layers—to the
    DRQN, resulting in the deep attention recurrent $Q$-network (DARQN) [[130](#bib.bib130)].
    Attention gives a network the ability to choose which part of its next input to
    focus on, and allowed the DARQN to beat both the DQN and DRQN on games, which
    require longer-term planning. However, the DQN outperformed the DRQN and DARQN
    on games requiring quick reactions, where $Q$-values can fluctuate more rapidly.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 DRL 的最早工作之一，DQN 产生了许多扩展。最早的扩展之一是将 DQN 转换为 RNN，这使得网络能够通过整合长期信息来更好地处理 POMDPs。像递归滤波器一样，递归连接提供了一种有效的手段，基于时间上遥远的先前观测进行条件操作。通过在隐藏单元之间使用递归连接，Hausknecht
    和 Stone[[44](#bib.bib44)] 提出的深度递归 $Q$-网络（DRQN）能够成功推断游戏“Pong”中的球速，即使游戏的帧被随机遮蔽。进一步的改进通过引入
    *注意力*——一种从递归单元到较低层添加额外连接的技术——到 DRQN，形成了深度注意力递归 $Q$-网络（DARQN）[[130](#bib.bib130)]。注意力使网络能够选择下一个输入的哪个部分进行关注，并使
    DARQN 在需要长期规划的游戏中击败了 DQN 和 DRQN。然而，在需要快速反应的游戏中，DQN 超过了 DRQN 和 DARQN，因为 $Q$-值可以更快速地波动。
- en: Taking recurrent processing further, it is possible to add a differentiable
    memory to the DQN, which allows it to more flexibly process information in its
    “working memory” [[96](#bib.bib96)]. In traditional RNNs, recurrent units are
    responsible for both performing calculations and storing information. Differentiable
    memories add large matrices that are purely used for storing information, and
    can be accessed using differentiable read and write operations, analagously to
    computer memory. With their key-value-based memory $Q$-network (MQN), Oh et al.
    [[96](#bib.bib96)] constructed an agent that could solve a simple maze built in
    Minecraft, where the correct goal in each episode was indicated by a coloured
    block shown near the start of the maze. The MQN, and especially its more sophisticated
    variants, significantly outperformed both DQN and DRQN baselines, highlighting
    the importance of using decoupled memory storage. More recent work, where the
    memory was given a 2D structure in order to resemble a spatial map, hints at future
    research where more specialised memory structures will be developed to address
    specific problems, such as 2D or 3D navigation [[98](#bib.bib98)]. Alternatively,
    differentiable memories can be used as approximate hash tables, allowing DRL algorithms
    to store and retrieve successful experiences to facilitate rapid learning [[105](#bib.bib105)].
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归处理的基础上，可以向 DQN 添加可微记忆，这使得它能够更灵活地处理其“工作记忆”中的信息[[96](#bib.bib96)]。在传统的 RNN
    中，递归单元负责执行计算和存储信息。可微记忆添加了用于存储信息的大矩阵，并可以通过可微读写操作访问，类似于计算机内存。Oh 等人[[96](#bib.bib96)]
    使用基于键值的记忆 $Q$-网络（MQN）构建了一个代理，能够解决在 Minecraft 中构建的简单迷宫，其中每个回合的正确目标由迷宫开始处附近显示的彩色块指示。MQN，尤其是其更复杂的变体，显著优于
    DQN 和 DRQN 基线，突显了使用解耦记忆存储的重要性。最近的工作将记忆赋予 2D 结构，以类似于空间地图的方式，暗示了未来研究将开发更专门的记忆结构以解决特定问题，如
    2D 或 3D 导航[[98](#bib.bib98)]。或者，可微记忆可以用作近似哈希表，允许 DRL 算法存储和检索成功经验，以促进快速学习[[105](#bib.bib105)]。
- en: Note that RNNs are not restricted to value-function-based methods but have also
    been successfully applied to policy search [[163](#bib.bib163)] and actor-critic
    methods [[45](#bib.bib45), [85](#bib.bib85)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，RNN 不仅限于基于价值函数的方法，还成功应用于策略搜索[[163](#bib.bib163)]和演员-评论家方法[[45](#bib.bib45),
    [85](#bib.bib85)]。
- en: VI-G Transfer Learning
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-G 迁移学习
- en: 'Even though DRL algorithms can process high-dimensional inputs, it is rarely
    feasible to train RL agents directly on visual inputs in the real world, due to
    the large number of samples required. To speed up learning in DRL, it is possible
    to exploit previously acquired knowledge from related tasks, which comes in several
    guises: transfer learning, multitask learning [[20](#bib.bib20)] and curriculum
    learning [[14](#bib.bib14)] to name a few. There is much interest in transferring
    learning from one task to another, particularly from training in physics simulators
    with visual renderers and fine-tuning the models in the real world. This can be
    achieved in a naive fashion, directly using the same network in both the simulated
    and real phases [[167](#bib.bib167)], or with more sophisticated training procedures
    that directly try to mitigate the problem of neural networks “catastrophically
    forgetting” old knowledge by adding extra layers when transferring domain [[114](#bib.bib114),
    [115](#bib.bib115)]. Other approaches involve directly learning an alignment between
    simulated and real visuals [[146](#bib.bib146)], or even between two different
    camera viewpoints [[131](#bib.bib131)].'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DRL 算法可以处理高维输入，但由于所需样本量庞大，直接在现实世界的视觉输入上训练 RL 代理几乎不可行。为了加速 DRL 学习，可以利用从相关任务中获得的先前知识，这有几种形式：迁移学习、多任务学习
    [[20](#bib.bib20)] 和课程学习 [[14](#bib.bib14)] 等。人们对将学习从一个任务迁移到另一个任务特别感兴趣，尤其是从物理模拟器中的视觉渲染器训练和在现实世界中微调模型。这可以以朴素的方式实现，即在模拟和真实阶段直接使用相同的网络
    [[167](#bib.bib167)]，也可以采用更复杂的训练程序，通过在迁移领域时添加额外的层来直接尝试减轻神经网络“灾难性遗忘”旧知识的问题 [[114](#bib.bib114),
    [115](#bib.bib115)]。其他方法则涉及直接学习模拟和真实视觉之间的对齐 [[146](#bib.bib146)]，甚至是两个不同相机视点之间的对齐
    [[131](#bib.bib131)]。
- en: A different form of transfer can be utilised to help RL in the form of multitask
    training [[77](#bib.bib77), [54](#bib.bib54), [82](#bib.bib82)]. Especially with
    neural networks, supervised and unsupervised learning tasks can help train features
    that can be used by RL agents, making optimising the RL objective easier to achieve.
    For example, the “unsupervised reinforcement and auxiliary learning” A3C-based
    agent is additionally trained with “pixel control” (maximally changing pixel inputs),
    plus reward prediction and value function learning from experience replay [[54](#bib.bib54)].
    Meanwhile, the A3C-based agent of Mirowski et al. [[82](#bib.bib82)] was additionally
    trained to construct a depth map given RGB inputs, which helps it in its task
    of learning to navigate a 3D environment. In an ablation study, Mirowski et al.
    [[82](#bib.bib82)] showed the predicting depth was more useful than receiving
    depth as an extra input, lending further support to the idea that gradients induced
    by auxiliary tasks can be extremely effective at boosting DRL.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 可以利用不同形式的迁移来帮助 RL，如多任务训练 [[77](#bib.bib77), [54](#bib.bib54), [82](#bib.bib82)]。尤其是在神经网络中，监督学习和无监督学习任务可以帮助训练
    RL 代理所需的特征，从而使优化 RL 目标变得更容易实现。例如，基于 A3C 的“无监督强化学习和辅助学习”代理还使用了“像素控制”（最大限度地改变像素输入），加上从经验回放中进行的奖励预测和价值函数学习
    [[54](#bib.bib54)]。与此同时，Mirowski 等人的 A3C 基于代理 [[82](#bib.bib82)] 额外训练了在给定 RGB
    输入的情况下构建深度图，这有助于其学习在 3D 环境中导航的任务。在一次消融研究中，Mirowski 等人 [[82](#bib.bib82)] 表明，预测深度比将深度作为额外输入更有用，这进一步支持了辅助任务引发的梯度在提升
    DRL 方面的极端有效性。
- en: Transfer learning can also be used to construct more data- and parameter-efficient
    policies. In the student-teacher paradigm in machine learning, one can first train
    a more powerful “teacher” model, and then use it to guide the training of a less
    powerful “student” model. Whilst originally applied to supervised learning, the
    neural network knowledge transfer technique known as *distillation* [[50](#bib.bib50)]
    has been utilised to both transfer policies learned by large DQNs to smaller DQNs,
    and transfer policies learned by several DQNs trained on separate games to one
    single DQN [[99](#bib.bib99), [113](#bib.bib113)]. Together, the combination of
    multitask and transfer learning can improve the sample efficiency and robustness
    of current DRL algorithms [[140](#bib.bib140)]. These are important topics if
    we wish to construct agents that can accomplish a wide range of tasks, since naively
    training on multiple RL objectives at once may be infeasible.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习还可以用来构建更加数据和参数高效的策略。在机器学习中的师生范式中，可以先训练一个更强大的“教师”模型，然后利用它来指导训练一个能力较弱的“学生”模型。虽然这一技术最初应用于监督学习，但被称为*蒸馏*的神经网络知识迁移技术
    [[50](#bib.bib50)] 已被用于将大型 DQN 学到的策略转移到较小的 DQN 中，以及将多个在不同游戏中训练的 DQN 学到的策略转移到一个单一的
    DQN 中 [[99](#bib.bib99), [113](#bib.bib113)]。结合多任务学习和迁移学习可以提高当前 DRL 算法的样本效率和鲁棒性
    [[140](#bib.bib140)]。这些都是重要的话题，因为如果我们希望构建能够完成多种任务的智能体，简单地同时在多个 RL 目标上进行训练可能是不切实际的。
- en: VI-H Benchmarks
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-H 基准测试
- en: One of the challenges in any field in machine learning is developing a standardised
    way to evaluate new techniques. Although much early work focused on simple, custom
    MDPs, there shortly emerged control problems that could be used as standard benchmarks
    for testing new algorithms, such as the Cartpole [[8](#bib.bib8)] and Mountain
    Car [[87](#bib.bib87)] domains.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的任何领域中，开发标准化的评估新技术的方法是一个挑战。虽然早期的工作主要集中在简单的自定义 MDP 上，但很快出现了可以作为测试新算法的标准基准的问题，例如
    Cartpole [[8](#bib.bib8)] 和 Mountain Car [[87](#bib.bib87)] 领域。
- en: However, these problems were limited to relatively small state spaces, and therefore
    failed to capture the complexities that would be encountered in most realistic
    scenarios. Arguably the initial driver of DRL, the ALE provided an interface to
    Atari 2600 video games, with code to access over 50 games provided with the initial
    release [[10](#bib.bib10)]. As video games can vary greatly, but still present
    interesting and challenging objectives for humans, they provide an excellent testbed
    for RL agents. As the first algorithm to successfully play a range of these games
    directly from their visuals, the DQN [[84](#bib.bib84)] has secured its place
    as a milestone in the development of RL algorithms. This success story has started
    a trend of using video games as standardised RL testbeds, with several interesting
    options now available. ViZDoom provides an interface to the Doom first-person
    shooter [[60](#bib.bib60)], and echoing the popularity of e-sports competitions,
    ViZDoom competitions are now held at the yearly IEEE Conference on Computational
    Intelligence in Games. Facebook’s TorchCraft [[137](#bib.bib137)] and DeepMind’s
    StarCraft II Learning Environment [[153](#bib.bib153)] respectively provide interfaces
    to the StarCraft and StarCraft II real-time strategy games, presenting challenges
    in both micromanagement and long-term planning. In an aim to provide more flexible
    environments, DeepMind Lab was developed on top of the Quake III Arena first-person
    shooter engine [[9](#bib.bib9)], and Microsoft’s Project Malmo exposed an interface
    to the Minecraft sandbox game [[55](#bib.bib55)]. Both environments provide customisable
    platforms for RL agents in 3D environments.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些问题限制在相对较小的状态空间，因此未能捕捉到大多数现实场景中的复杂性。可以说，最初推动 DRL 的 ALE 提供了一个 Atari 2600
    视频游戏的接口，最初版本中提供了访问超过 50 款游戏的代码 [[10](#bib.bib10)]。由于视频游戏可以大相径庭，但仍为人类提供有趣且具有挑战性的目标，因此它们为
    RL 代理提供了一个绝佳的测试平台。作为第一个成功直接从视觉信息中玩这些游戏的算法，DQN [[84](#bib.bib84)] 已经在 RL 算法的发展中确立了里程碑地位。这个成功故事引发了使用视频游戏作为标准化
    RL 测试平台的趋势，现在有几个有趣的选项可供选择。ViZDoom 提供了一个 Doom 第一人称射击游戏的接口 [[60](#bib.bib60)]，并且呼应了电子竞技比赛的流行，ViZDoom
    比赛现在在每年的 IEEE 计算智能与游戏会议上举行。Facebook 的 TorchCraft [[137](#bib.bib137)] 和 DeepMind
    的 StarCraft II Learning Environment [[153](#bib.bib153)] 分别提供了 StarCraft 和 StarCraft
    II 实时战略游戏的接口，在微观管理和长期规划方面都提出了挑战。为了提供更灵活的环境，DeepMind Lab 在 Quake III Arena 第一人称射击引擎
    [[9](#bib.bib9)] 的基础上开发，微软的 Project Malmo 暴露了 Minecraft 沙盒游戏 [[55](#bib.bib55)]
    的接口。这两个环境为 3D 环境中的 RL 代理提供了可定制的平台。
- en: Most DRL approaches focus on discrete actions, but some solutions have also
    been developed for continuous control problems. Many DRL papers in continuous
    control [[122](#bib.bib122), [46](#bib.bib46), [79](#bib.bib79), [85](#bib.bib85),
    [7](#bib.bib7), [131](#bib.bib131)] have used the MuJoCo physics engine to obtain
    relatively realistic dynamics for multi-joint continuous control problems [[144](#bib.bib144)],
    and there has now been some effort to standardise these problems [[28](#bib.bib28)].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度强化学习（DRL）方法集中于离散动作，但也有一些解决方案被开发用于连续控制问题。许多关于连续控制的 DRL 论文 [[122](#bib.bib122),
    [46](#bib.bib46), [79](#bib.bib79), [85](#bib.bib85), [7](#bib.bib7), [131](#bib.bib131)]
    使用了 MuJoCo 物理引擎来获得相对真实的多关节连续控制问题的动态 [[144](#bib.bib144)]，现在也有一些努力在于标准化这些问题 [[28](#bib.bib28)]。
- en: To help with standardisation and reproducibility, most of the aforementioned
    RL domains and more have been made available in the OpenAI Gym, a library and
    online service that allows people to easily interface with and publicly share
    the results of RL algorithms on these domains [[17](#bib.bib17)].
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助标准化和可重复性，大多数上述的强化学习（RL）领域及其他更多领域已被整合进 OpenAI Gym，这是一种库和在线服务，允许人们轻松地与这些领域的
    RL 算法接口并公开分享结果 [[17](#bib.bib17)]。
- en: 'VII Conclusion: Beyond Pattern Recognition'
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论：超越模式识别
- en: Despite the successes of DRL, many problems need to be addressed before these
    techniques can be applied to a wide range of complex real-world problems [[68](#bib.bib68)].
    Recent work with (non-deep) generative causal models demonstrated superior generalisation
    over standard DRL algorithms [[85](#bib.bib85), [114](#bib.bib114)] in some benchmarks
    [[10](#bib.bib10)], achieved by reasoning about causes and effects in the environment
    [[58](#bib.bib58)]. For example, the schema networks of Kanksy et al. [[58](#bib.bib58)]
    trained on the game “Breakout” immediately adapted to a variant where a small
    wall was placed in front of the target blocks, whilst progressive (A3C) networks
    [[114](#bib.bib114)] failed to match the performance of the schema networks even
    after training on the new domain. Although DRL has already been combined with
    AI techniques, such as search [[128](#bib.bib128)] and planning [[138](#bib.bib138)],
    a deeper integration with other traditional AI approaches promises benefits such
    as better sample complexity, generalisation and interpretability [[35](#bib.bib35)].
    In time, we also hope that our theoretical understanding of the properties of
    neural networks (particularly within DRL) will improve, as it currently lags far
    behind practice.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DRL取得了成功，但在这些技术能够应用于广泛复杂的现实世界问题之前，仍需解决许多问题[[68](#bib.bib68)]。最近在（非深度）生成因果模型上的研究在一些基准测试中显示出比标准DRL算法更优的泛化能力[[85](#bib.bib85),
    [114](#bib.bib114)]，这是通过对环境中因果关系的推理实现的[[58](#bib.bib58)]。例如，Kanksy等人的模式网络[[58](#bib.bib58)]在“Breakout”游戏上训练后，能立即适应一个在目标块前放置了小墙的变体，而渐进（A3C）网络[[114](#bib.bib114)]即便在新领域中训练后仍未能匹配模式网络的表现。尽管DRL已与AI技术，如搜索[[128](#bib.bib128)]和规划[[138](#bib.bib138)]相结合，但与其他传统AI方法的更深度整合承诺带来如更好的样本复杂性、泛化能力和可解释性[[35](#bib.bib35)]等好处。我们也希望，随着时间的推移，我们对神经网络（特别是DRL中）的理论理解能有所提高，因为目前其理论水平远远落后于实践。
- en: 'To conclude, it is worth revisiting the overarching goal of all of this research:
    the creation of general-purpose AI systems that can interact with and learn from
    the world around them. Interaction with the environment is simultaneously the
    advantage and disadvantage of RL. Whilst there are many challenges in seeking
    to understand our complex and ever-changing world, RL allows us to choose how
    we explore it. In effect, RL endows agents with the ability to perform experiments
    to better understand their surroundings, enabling them to learn even high-level
    causal relationships. The availability of high-quality visual renderers and physics
    engines now enables us to take steps in this direction, with works that try to
    learn intuitive models of physics in visual environments [[27](#bib.bib27)]. Challenges
    remain before this will be possible in the real world, but steady progress is
    being made in agents that learn the fundamental principles of the world through
    observation and action. Perhaps, then, we are not too far away from AI systems
    that learn and act in more human-like ways in increasingly complex environments.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，值得重新审视所有这些研究的总体目标：创建能够与周围世界互动并从中学习的通用AI系统。与环境的互动既是RL的优势也是劣势。尽管理解我们复杂而不断变化的世界面临许多挑战，但RL允许我们选择探索的方式。实际上，RL赋予代理进行实验的能力，以更好地理解其周围环境，使其能够学习甚至是高层次的因果关系。高质量的视觉渲染器和物理引擎的可用性现在使我们能够朝这个方向迈进，其中包括尝试在视觉环境中学习直观物理模型的工作[[27](#bib.bib27)]。尽管在现实世界中实现这一目标仍面临挑战，但通过观察和行动来学习世界基本原则的代理正在稳步进展。因此，也许我们离能够在日益复杂的环境中以更类似于人类的方式学习和行动的AI系统并不远。
- en: Acknowledgments
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank the reviewers and broader community for their
    feedback on this survey; in particular, we would like to thank Nicolas Heess for
    clarifications on several points. Kai Arulkumaran would like to acknowledge PhD
    funding from the Department of Bioengineering, Imperial College London. This research
    has been partially funded by a Google Faculty Research Award to Marc Deisenroth.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢审稿人和更广泛社区对本调查的反馈；特别感谢Nicolas Heess对若干点的澄清。Kai Arulkumaran感谢帝国理工学院生物工程系的博士资助。该研究部分由Google
    Faculty Research Award资助给Marc Deisenroth。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abbeel and Schulman [2016] Pieter Abbeel and John Schulman. Deep Reinforcement
    Learning through Policy Optimization, 2016. Tutorial at NIPS 2016.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbeel和Schulman [2016] Pieter Abbeel和John Schulman。通过策略优化的深度强化学习，2016年。NIPS
    2016的教程。
- en: 'Arulkumaran et al. [2016] Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan,
    and Anil Anthony Bharath. Classifying Options for Deep Reinforcement Learning.
    In *IJCAI Workshop on Deep Reinforcement Learning: Frontiers and Challenges*,
    2016.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arulkumaran 等 [2016] Kai Arulkumaran、Nat Dilokthanakul、Murray Shanahan 和 Anil
    Anthony Bharath。深度强化学习的选项分类。发表于 *IJCAI Workshop on Deep Reinforcement Learning:
    Frontiers and Challenges*，2016。'
- en: Bacon et al. [2017] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The Option-Critic
    Architecture. In *AAAI*, 2017.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bacon 等 [2017] Pierre-Luc Bacon、Jean Harb 和 Doina Precup。选项-评论员架构。发表于 *AAAI*，2017。
- en: Bagnell and Schneider [2003] J Andrew Bagnell and Jeff Schneider. Covariant
    Policy Search. In *IJCAI*, 2003.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagnell 和 Schneider [2003] J Andrew Bagnell 和 Jeff Schneider。协方差策略搜索。发表于 *IJCAI*，2003。
- en: Bahdanau et al. [2017] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh
    Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An Actor-Critic
    Algorithm for Sequence Prediction. In *ICLR*, 2017.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等 [2017] Dzmitry Bahdanau、Philemon Brakel、Kelvin Xu、Anirudh Goyal、Ryan
    Lowe、Joelle Pineau、Aaron Courville 和 Yoshua Bengio。用于序列预测的演员-评论员算法。发表于 *ICLR*，2017。
- en: Baird III [1993] Leemon C Baird III. Advantage Updating. Technical report, DTIC,
    1993.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baird III [1993] Leemon C Baird III。优势更新。技术报告，DTIC，1993。
- en: Baram et al. [2016] Nir Baram, Oron Anschel, and Shie Mannor. Model-Based Adversarial
    Imitation Learning. In *NIPS Workshop on Deep Reinforcement Learning*, 2016.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baram 等 [2016] Nir Baram、Oron Anschel 和 Shie Mannor。基于模型的对抗模仿学习。发表于 *NIPS Workshop
    on Deep Reinforcement Learning*，2016。
- en: Barto et al. [1983] Andrew G Barto, Richard S Sutton, and Charles W Anderson.
    Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.
    *IEEE Trans. on Systems, Man, and Cybernetics*, (5):834–846, 1983.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barto 等 [1983] Andrew G Barto、Richard S Sutton 和 Charles W Anderson。类似神经元的自适应元素能够解决困难的学习控制问题。*IEEE
    Trans. on Systems, Man, and Cybernetics*，(5):834–846，1983。
- en: Beattie et al. [2016] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward,
    Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés,
    Amir Sadik, et al. DeepMind Lab. *arXiv:1612.03801*, 2016.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beattie 等 [2016] Charles Beattie、Joel Z Leibo、Denis Teplyashin、Tom Ward、Marcus
    Wainwright、Heinrich Küttler、Andrew Lefrancq、Simon Green、Víctor Valdés、Amir Sadik
    等。DeepMind Lab。*arXiv:1612.03801*，2016。
- en: 'Bellemare et al. [2015] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael
    Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents.
    In *IJCAI*, 2015.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等 [2015] Marc G Bellemare、Yavar Naddaf、Joel Veness 和 Michael Bowling。街机学习环境：通用智能体的评估平台。发表于
    *IJCAI*，2015。
- en: Bellemare et al. [2016] Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski,
    Tom Schaul, David Saxton, and Rémi Munos. Unifying Count-Based Exploration and
    Intrinsic Motivation. In *NIPS*, 2016.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等 [2016] Marc G Bellemare、Sriram Srinivasan、Georg Ostrovski、Tom Schaul、David
    Saxton 和 Rémi Munos。统一基于计数的探索与内在动机。发表于 *NIPS*，2016。
- en: Bellemare et al. [2017] Marc G Bellemare, Will Dabney, and Rémi Munos. A Distributional
    Perspective on Reinforcement Learning. In *ICML*, 2017.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等 [2017] Marc G Bellemare、Will Dabney 和 Rémi Munos。强化学习的分布视角。发表于 *ICML*，2017。
- en: Bellman [1952] Richard Bellman. On the Theory of Dynamic Programming. *PNAS*,
    38(8):716–719, 1952.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman [1952] Richard Bellman。动态规划理论。*PNAS*，38(8):716–719，1952。
- en: Bengio et al. [2009] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason
    Weston. Curriculum Learning. In *ICML*, 2009.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等 [2009] Yoshua Bengio、Jérôme Louradour、Ronan Collobert 和 Jason Weston。课程学习。发表于
    *ICML*，2009。
- en: 'Bengio et al. [2013] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation
    Learning: A Review and New Perspectives. *IEEE Trans. on Pattern Analysis and
    Machine Intelligence*, 35(8):1798–1828, 2013.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等 [2013] Yoshua Bengio、Aaron Courville 和 Pascal Vincent。表示学习：综述与新视角。*IEEE
    Trans. on Pattern Analysis and Machine Intelligence*，35(8):1798–1828，2013。
- en: 'Bertsekas [2005] Dimitri P Bertsekas. Dynamic Programming and Suboptimal Control:
    A Survey from ADP to MPC. *European Journal of Control*, 11(4-5):310–334, 2005.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertsekas [2005] Dimitri P Bertsekas。动态规划与次优控制：从 ADP 到 MPC 的调查。*European Journal
    of Control*，11(4-5):310–334，2005。
- en: Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
    Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. *arXiv:1606.01540*,
    2016.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockman 等 [2016] Greg Brockman、Vicki Cheung、Ludwig Pettersson、Jonas Schneider、John
    Schulman、Jie Tang 和 Wojciech Zaremba。OpenAI Gym。*arXiv:1606.01540*，2016。
- en: Busoniu et al. [2008] Lucian Busoniu, Robert Babuska, and Bart De Schutter.
    A Comprehensive survey of Multiagent Reinforcement Learning. *IEEE Trans. on Systems,
    Man, And Cybernetics*, 2008.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Busoniu 等 [2008] Lucian Busoniu、Robert Babuska 和 Bart De Schutter。多智能体强化学习的综合调查。*IEEE
    Trans. on Systems, Man, And Cybernetics*，2008。
- en: Campbell et al. [2002] Murray Campbell, A Joseph Hoane, and Feng-hsiung Hsu.
    Deep Blue. *Artificial Intelligence*, 134(1-2):57–83, 2002.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Campbell et al. [2002] Murray Campbell, A Joseph Hoane 和 Feng-hsiung Hsu. 深蓝。*人工智能*，134(1-2):57–83，2002年。
- en: Caruana [1997] Rich Caruana. Multitask Learning. *Machine Learning*, 28(1):41–75,
    1997.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caruana [1997] Rich Caruana. 多任务学习。*机器学习*，28(1):41–75，1997年。
- en: Chiappa et al. [2017] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and
    Shakir Mohamed. Recurrent Environment Simulators. In *ICLR*, 2017.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiappa et al. [2017] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra 和 Shakir
    Mohamed. 循环环境模拟器。在 *ICLR*，2017年。
- en: Christiano et al. [2016] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider,
    Trevor Blackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba. Transfer
    from Simulation to Real World through Learning Deep Inverse Dynamics Model. *arXiv:1610.03518*,
    2016.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano et al. [2016] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider,
    Trevor Blackwell, Joshua Tobin, Pieter Abbeel 和 Wojciech Zaremba. 通过学习深度逆动态模型实现从模拟到现实世界的迁移。*arXiv:1610.03518*，2016年。
- en: Cuccu et al. [2011] Giuseppe Cuccu, Matthew Luciw, Jürgen Schmidhuber, and Faustino
    Gomez. Intrinsically Motivated Neuroevolution for Vision-Based Reinforcement Learning.
    In *ICDL*, volume 2, 2011.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cuccu et al. [2011] Giuseppe Cuccu, Matthew Luciw, Jürgen Schmidhuber 和 Faustino
    Gomez. 基于视觉的强化学习的内在动机神经进化。在 *ICDL*，第2卷，2011年。
- en: 'Dayan [1993] Peter Dayan. Improving Generalization for Temporal Difference
    Learning: The Successor Representation. *Neural Computation*, 5(4):613–624, 1993.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dayan [1993] Peter Dayan. 改进时间差分学习的泛化：继任者表示。*神经计算*，5(4):613–624，1993年。
- en: Dean et al. [2012] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu
    Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large
    Scale Distributed Deep Networks. In *NIPS*, 2012.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dean et al. [2012] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu
    Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le 等。大规模分布式深度网络。在
    *NIPS*，2012年。
- en: Deisenroth et al. [2013] Marc P Deisenroth, Gerhard Neumann, and Jan Peters.
    A Survey on Policy Search for Robotics. *Foundations and Trends® in Robotics*,
    2(1–2), 2013.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deisenroth et al. [2013] Marc P Deisenroth, Gerhard Neumann 和 Jan Peters. 机器人政策搜索综述。*机器人学基础与趋势®*，2(1–2)，2013年。
- en: Denil et al. [2017] Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez,
    Peter Battaglia, and Nando de Freitas. Learning to Perform Physics Experiments
    via Deep Reinforcement Learning. In *ICLR*, 2017.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denil et al. [2017] Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez,
    Peter Battaglia 和 Nando de Freitas. 通过深度强化学习进行物理实验学习。在 *ICLR*，2017年。
- en: Duan et al. [2016a] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter
    Abbeel. Benchmarking Deep Reinforcement Learning for Continuous Control. In *ICML*,
    2016a.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan et al. [2016a] Yan Duan, Xi Chen, Rein Houthooft, John Schulman 和 Pieter
    Abbeel. 连续控制的深度强化学习基准测试。在 *ICML*，2016年。
- en: 'Duan et al. [2016b] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya
    Sutskever, and Pieter Abbeel. RL²: Fast Reinforcement Learning via Slow Reinforcement
    Learning. In *NIPS Workshop on Deep Reinforcement Learning*, 2016b.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan et al. [2016b] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya
    Sutskever 和 Pieter Abbeel. RL²：通过慢强化学习实现快速强化学习。在 *NIPS Workshop on Deep Reinforcement
    Learning*，2016年。
- en: Dulac-Arnold et al. [2015] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt,
    Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber,
    Thomas Degris, and Ben Coppin. Deep Reinforcement Learning in Large Discrete Action
    Spaces. *arXiv:1512.07679*, 2015.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dulac-Arnold et al. [2015] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt,
    Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber,
    Thomas Degris 和 Ben Coppin. 大规模离散动作空间中的深度强化学习。*arXiv:1512.07679*，2015年。
- en: 'Ferrucci et al. [2010] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
    Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg,
    John Prager, et al. Building Watson: An Overview of the DeepQA Project. *AI Magazine*,
    31(3):59–79, 2010.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrucci et al. [2010] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
    Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg,
    John Prager 等。构建Watson：DeepQA项目概述。*AI Magazine*，31(3):59–79，2010年。
- en: Finn et al. [2016] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey
    Levine, and Pieter Abbeel. Deep Spatial Autoencoders for Visuomotor Learning.
    In *ICRA*, 2016.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. [2016] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey
    Levine 和 Pieter Abbeel. 用于视觉运动学习的深度空间自编码器。在 *ICRA*，2016年。
- en: Foerster et al. [2016] Jakob Foerster, Yannis M Assael, Nando de Freitas, and
    Shimon Whiteson. Learning to Communicate with Deep Multi-Agent Reinforcement Learning.
    In *NIPS*, 2016.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster et al. [2016] Jakob Foerster, Yannis M Assael, Nando de Freitas 和 Shimon
    Whiteson. 利用深度多智能体强化学习进行通信学习。在 *NIPS*，2016年。
- en: Fu [2006] Michael C Fu. Gradient Estimation. *Handbooks in Operations Research
    and Management Science*, 13:575–616, 2006.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu [2006] Michael C Fu. 梯度估计。*运筹学与管理科学手册*，13:575–616，2006年。
- en: Garnelo et al. [2016] Marta Garnelo, Kai Arulkumaran, and Murray Shanahan. Towards
    Deep Symbolic Reinforcement Learning. In *NIPS Workshop on Deep Reinforcement
    Learning*, 2016.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garnelo 等 [2016] Marta Garnelo, Kai Arulkumaran 和 Murray Shanahan. 朝向深度符号强化学习。在
    *NIPS 深度强化学习研讨会*，2016年。
- en: Glynn [1990] Peter W Glynn. Likelihood Ratio Gradient Estimation for Stochastic
    Systems. *Communications of the ACM*, 33(10):75–84, 1990.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glynn [1990] Peter W Glynn. 随机系统的似然比梯度估计。*ACM 通讯*，33(10):75–84，1990年。
- en: Gomez and Schmidhuber [2005] Faustino Gomez and Jürgen Schmidhuber. Evolving
    Modular Fast-Weight Networks for Control. In *ICANN*, 2005.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gomez 和 Schmidhuber [2005] Faustino Gomez 和 Jürgen Schmidhuber. 进化模块化快速权重网络用于控制。在
    *ICANN*，2005年。
- en: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    Adversarial Nets. In *NIPS*, 2014.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
    David Warde-Farley, Sherjil Ozair, Aaron Courville 和 Yoshua Bengio. 生成对抗网络。在 *NIPS*，2014年。
- en: 'Gruslys et al. [2017] Audrunas Gruslys, Mohammad Gheshlaghi Azar, Marc G Bellemare,
    and Rémi Munos. The Reactor: A Sample-Efficient Actor-Critic Architecture. *arXiv:1704.04651*,
    2017.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gruslys 等 [2017] Audrunas Gruslys, Mohammad Gheshlaghi Azar, Marc G Bellemare
    和 Rémi Munos. Reactor：一种样本高效的演员-评论员架构。*arXiv:1704.04651*，2017年。
- en: Gu et al. [2016] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey
    Levine. Continuous Deep Q-Learning with Model-Based Acceleration. In *ICLR*, 2016.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 [2016] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever 和 Sergey Levine.
    使用基于模型的加速的连续深度 Q 学习。在 *ICLR*，2016年。
- en: 'Gu et al. [2017a] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E
    Turner, and Sergey Levine. Q-Prop: Sample-Efficient Policy Gradient with an Off-Policy
    Critic. In *ICLR*, 2017a.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 [2017a] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner
    和 Sergey Levine. Q-Prop：使用离策略评论员的样本高效策略梯度。在 *ICLR*，2017a年。
- en: 'Gu et al. [2017b] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E
    Turner, Bernhard Schölkopf, and Sergey Levine. Interpolated Policy Gradient: Merging
    On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning.
    In *NIPS*, 2017b.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 [2017b] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner,
    Bernhard Schölkopf 和 Sergey Levine. 插值策略梯度：合并策略梯度和离策略梯度估计用于深度强化学习。在 *NIPS*，2017b年。
- en: Harmon and Baird III [1996] Mance E Harmon and Leemon C Baird III. Multi-Player
    Residual Advantage Learning with General Function Approximation. Technical report,
    DTIC, 1996.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harmon 和 Baird III [1996] Mance E Harmon 和 Leemon C Baird III. 使用通用函数近似的多玩家残差优势学习。技术报告，DTIC，1996年。
- en: Hausknecht and Stone [2015] Matthew Hausknecht and Peter Stone. Deep Recurrent
    Q-Learning for Partially Observable MDPs. In *AAAI Fall Symposium Series*, 2015.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hausknecht 和 Stone [2015] Matthew Hausknecht 和 Peter Stone. 部分可观察 MDP 的深度递归
    Q 学习。在 *AAAI 秋季研讨会系列*，2015年。
- en: Heess et al. [2015a] Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, and
    David Silver. Memory-Based Control with Recurrent Neural Networks. In *NIPS Workshop
    on Deep Reinforcement Learning*, 2015a.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess 等 [2015a] Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap 和 David
    Silver. 基于记忆的控制与递归神经网络。在 *NIPS 深度强化学习研讨会*，2015a年。
- en: Heess et al. [2015b] Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap,
    Tom Erez, and Yuval Tassa. Learning Continuous Control Policies by Stochastic
    Value Gradients. In *NIPS*, 2015b.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess 等 [2015b] Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom
    Erez 和 Yuval Tassa. 通过随机值梯度学习连续控制策略。在 *NIPS*，2015b年。
- en: Heess et al. [2017] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel,
    Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et al.
    Emergence of Locomotion Behaviours in Rich Environments. *arXiv:1707.02286*, 2017.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess 等 [2017] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg
    Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin Riedmiller 等. 在丰富环境中行为的出现。*arXiv:1707.02286*，2017年。
- en: Heinrich and Silver [2016] Johannes Heinrich and David Silver. Deep Reinforcement
    Learning from Self-Play in Imperfect-Information Games. 2016.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heinrich 和 Silver [2016] Johannes Heinrich 和 David Silver. 从自我对弈中学习深度强化学习。2016年。
- en: Hester et al. [2017] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot,
    Tom Schaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John
    Agapiou, et al. Learning from Demonstrations for Real World Reinforcement Learning.
    *arXiv:1704.03732*, 2017.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hester 等 [2017] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot,
    Tom Schaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John
    Agapiou 等. 从示范中学习以应对现实世界的强化学习。*arXiv:1704.03732*，2017年。
- en: Hinton et al. [2014] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the Knowledge in a Neural Network. 2014.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等 [2014] Geoffrey Hinton, Oriol Vinyals 和 Jeff Dean. 提炼神经网络中的知识。2014年。
- en: Ho and Ermon [2016] Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation
    Learning. In *NIPS*, 2016.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho and Ermon [2016] 乔纳森·霍和斯特凡诺·厄蒙。生成对抗模仿学习。在*NIPS*，2016年。
- en: 'Houthooft et al. [2016] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip
    De Turck, and Pieter Abbeel. VIME: Variational Information Maximizing Exploration.
    In *NIPS*, 2016.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houthooft et al. [2016] 莱因·侯思霍夫特、习·陈、燕·段、约翰·舒尔曼、菲利普·德·图尔克和皮特·阿比尔。VIME：变分信息最大化探索。在*NIPS*，2016年。
- en: Hussein et al. [2016] Ahmed Hussein, Mohamed Medhat Gaber, and Eyad Elyan. Deep
    Active Learning for Autonomous Navigation. In *EANN*, 2016.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hussein et al. [2016] 艾哈迈德·侯赛因、穆罕默德·梅赫拉特·加贝尔和艾亚德·埃利安。用于自主导航的深度主动学习。在*EANN*，2016年。
- en: Jaderberg et al. [2017] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki,
    Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement Learning
    with Unsupervised Auxiliary Tasks. In *ICLR*, 2017.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg et al. [2017] 马克斯·贾德伯格、弗拉基米尔·米赫、沃伊切赫·玛丽安·察尔内基、汤姆·绍尔、乔尔·Z·雷博、大卫·银和科雷·卡武克丘奥卢。具有无监督辅助任务的强化学习。在*ICLR*，2017年。
- en: Johnson et al. [2016] Matthew Johnson, Katja Hofmann, Tim Hutton, and David
    Bignell. The Malmo Platform for Artificial Intelligence Experimentation. In *IJCAI*,
    2016.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson et al. [2016] 马修·约翰逊、卡特娅·霍夫曼、蒂姆·赫顿和大卫·比格内尔。马尔默人工智能实验平台。在*IJCAI*，2016年。
- en: Kaelbling et al. [1998] Leslie P Kaelbling, Michael L Littman, and Anthony R
    Cassandra. Planning and Acting in Partially Observable Stochastic Domains. *Artificial
    Intelligence*, 101(1):99–134, 1998.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaelbling et al. [1998] 莱斯利·P·凯尔布林、迈克尔·L·利特曼和安东尼·R·坎德拉。部分可观察随机领域中的规划与行动。*人工智能*，101(1):99–134，1998年。
- en: Kakade [2002] Sham M Kakade. A Natural Policy Gradient. In *NIPS*, 2002.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kakade [2002] 沙姆·M·卡卡德。一种自然策略梯度。在*NIPS*，2002年。
- en: 'Kansky et al. [2017] Ken Kansky, Tom Silver, David A Mély, Mohamed Eldawy,
    Miguel Lázaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix,
    and Dileep George. Schema Networks: Zero-Shot Transfer with a Generative Causal
    Model of Intuitive Physics. In *ICML*, 2017.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kansky et al. [2017] 肯·坎斯基、汤姆·西尔弗、大卫·A·梅利、穆罕默德·埃尔达维、米格尔·拉萨罗-格雷迪利亚、邢华·楼、尼姆罗德·多夫曼、斯蒙·西多尔、斯科特·菲尼克斯和迪利普·乔治。模式网络：使用直观物理的生成因果模型进行零样本迁移。在*ICML*，2017年。
- en: Kappen [2005] Hilbert J Kappen. Path Integrals and Symmetry Breaking for Optimal
    Control Theory. *JSTAT*, 2005(11):P11011, 2005.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kappen [2005] 希尔伯特·J·卡彭。路径积分和对称破缺的最优控制理论。*JSTAT*，2005年(11):P11011，2005年。
- en: 'Kempka et al. [2016] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek,
    and Wojciech Jaśkowski. ViZDoom: A Doom-Based AI Research Platform for Visual
    Reinforcement Learning. In *CIG*, 2016.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kempka et al. [2016] 米哈乌·凯姆普卡、马雷克·维德穆赫、格热戈日·伦茨、雅库布·托切克和沃伊切赫·雅斯科夫斯基。ViZDoom：基于Doom的视觉强化学习AI研究平台。在*CIG*，2016年。
- en: Kingma and Welling [2014] Diederik P Kingma and Max Welling. Auto-Encoding Variational
    Bayes. In *ICLR*, 2014.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Welling [2014] 迪德里克·P·金马和马克斯·韦林。自动编码变分贝叶斯。在*ICLR*，2014年。
- en: Kohl and Stone [2004] Nate Kohl and Peter Stone. Policy Gradient Reinforcement
    Learning for Fast Quadrupedal Locomotion. In *ICRA*, volume 3, 2004.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohl and Stone [2004] 内特·科尔和彼得·斯通。用于快速四足运动的策略梯度强化学习。在*ICRA*，第3卷，2004年。
- en: Konda and Tsitsiklis [2003] Vijay R Konda and John N Tsitsiklis. On Actor-Critic
    Algorithms. *SICON*, 42(4):1143–1166, 2003.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konda and Tsitsiklis [2003] 维贾伊·R·孔达和约翰·N·齐齐克利斯。关于演员-评论员算法。*SICON*，42(4):1143–1166，2003年。
- en: Koutník et al. [2013] Jan Koutník, Giuseppe Cuccu, Jürgen Schmidhuber, and Faustino
    Gomez. Evolving Large-Scale Neural Networks for Vision-Based Reinforcement Learning.
    In *GECCO*, 2013.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koutník et al. [2013] 扬·库特尼克、朱塞佩·库丘、于尔根·施密德胡伯和福斯蒂诺·戈麦斯。为基于视觉的强化学习进化大规模神经网络。在*GECCO*，2013年。
- en: 'Kulkarni et al. [2016a] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi,
    and Josh Tenenbaum. Hierarchical Deep Reinforcement Learning: Integrating Temporal
    Abstraction and Intrinsic Motivation. In *NIPS*, 2016a.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni et al. [2016a] 泰贾斯·D·库尔卡尼、卡尔蒂克·纳拉辛汉、阿尔达万·赛义迪和乔什·特嫩鲍姆。层次化深度强化学习：整合时间抽象和内在动机。在*NIPS*，2016a年。
- en: Kulkarni et al. [2016b] Tejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and
    Samuel J Gershman. Deep Successor Reinforcement Learning. In *NIPS Workshop on
    Deep Reinforcement Learning*, 2016b.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni et al. [2016b] 泰贾斯·D·库尔卡尼、阿尔达万·赛义迪、希曼塔·高塔姆和塞缪尔·J·格尔斯曼。深度后继强化学习。在*NIPS
    深度强化学习研讨会*，2016b年。
- en: Lai and Robbins [1985] Tze Leung Lai and Herbert Robbins. Asymptotically Efficient
    Adaptive Allocation Rules. *Advances in Applied Mathematics*, 6(1):4–22, 1985.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai and Robbins [1985] 李子良和赫伯特·罗宾斯。渐近有效的自适应分配规则。*应用数学进展*，6(1):4–22，1985年。
- en: Lake et al. [2016] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J
    Gershman. Building Machines That Learn and Think Like People. *The Behavioral
    and Brain Sciences*, page 1, 2016.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lange et al. [2012] Sascha Lange, Martin Riedmiller, and Arne Voigtlander. Autonomous
    Reinforcement Learning on Raw Visual Input Data in a Real World Application. In
    *IJCNN*, 2012.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [2015] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep Learning.
    *Nature*, 521(7553):436–444, 2015.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leibo et al. [2017] Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki,
    and Thore Graepel. Multi-Agent Reinforcement Learning in Sequential Social Dilemmas.
    In *AAMAS*, 2017.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levine and Abbeel [2014] Sergey Levine and Pieter Abbeel. Learning Neural Network
    Policies with Guided Policy Search under Unknown Dynamics. In *NIPS*, 2014.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levine and Koltun [2013] Sergey Levine and Vladlen Koltun. Guided Policy Search.
    In *ICLR*, 2013.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levine et al. [2016a] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter
    Abbeel. End-to-End Training of Deep Visuomotor Policies. *JMLR*, 17(39):1–40,
    2016a.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levine et al. [2016b] Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre
    Quillen. Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning
    and Large-Scale Data Collection. In *ISER*, 2016b.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Malik [2017] Ke Li and Jitendra Malik. Learning to Optimize. 2017.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2015] Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen,
    Li Deng, and Ji He. Recurrent Reinforcement Learning: A Hybrid Approach. *arXiv:1509.03044*,
    2015.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li [2017] Yuxi Li. Deep Reinforcement Learning: An Overview. *arXiv:1701.07274*,
    2017.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillicrap et al. [2016] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous
    Control with Deep Reinforcement Learning. In *ICLR*, 2016.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin [1992] Long-Ji Lin. Self-Improving Reactive Agents Based on Reinforcement
    Learning, Planning and Teaching. *Machine Learning*, 8(3–4):293–321, 1992.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metz et al. [2017] Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson.
    Discrete Sequential Prediction of Continuous Actions for Deep RL. *arXiv:1705.05035*,
    2017.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirowski et al. [2017] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer,
    Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray
    Kavukcuoglu, et al. Learning to Navigate in Complex Environments. In *ICLR*, 2017.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. [2014] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu.
    Recurrent Models of Visual Attention. In *NIPS*, 2014.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
    Fidjeland, Georg Ostrovski, et al. Human-Level Control through Deep Reinforcement
    Learning. *Nature*, 518(7540):529–533, 2015.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
    Graves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
    Asynchronous Methods for Deep Reinforcement Learning. In *ICLR*, 2016.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等人 [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves,
    Timothy P Lillicrap, Tim Harley, David Silver 和 Koray Kavukcuoglu。深度强化学习的异步方法。发表于
    *ICLR*，2016 年。
- en: Mohamed and Jimenez Rezende [2015] Shakir Mohamed and Danilo Jimenez Rezende.
    Variational Information Maximisation for Intrinsically Motivated Reinforcement
    Learning. In *NIPS*, 2015.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohamed 和 Jimenez Rezende [2015] Shakir Mohamed 和 Danilo Jimenez Rezende。用于内在激励强化学习的变分信息最大化。发表于
    *NIPS*，2015 年。
- en: Moore [1990] Andrew William Moore. Efficient Memory-Based Learning for Robot
    Control. Technical report, University of Cambridge, Computer Laboratory, 1990.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moore [1990] Andrew William Moore。机器人控制的高效记忆基础学习。技术报告，剑桥大学计算机实验室，1990 年。
- en: Munos et al. [2016] Rémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G
    Bellemare. Safe and Efficient Off-Policy Reinforcement Learning. In *NIPS*, 2016.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munos 等人 [2016] Rémi Munos, Tom Stepleton, Anna Harutyunyan 和 Marc G Bellemare。安全高效的离策略强化学习。发表于
    *NIPS*，2016 年。
- en: Nachum et al. [2017] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
    Bridging the Gap Between Value and Policy Based Reinforcement Learning. *arXiv:1702.08892*,
    2017.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nachum 等人 [2017] Ofir Nachum, Mohammad Norouzi, Kelvin Xu 和 Dale Schuurmans。弥合价值和策略基础强化学习之间的差距。*arXiv:1702.08892*，2017
    年。
- en: Nagabandi et al. [2017] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and
    Sergey Levine. Neural Network Dynamics for Model-Based Deep Reinforcement Learning
    with Model-Free Fine-Tuning. *arXiv:1708.02596*, 2017.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagabandi 等人 [2017] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing 和 Sergey
    Levine。基于模型的深度强化学习中的神经网络动态与无模型微调。*arXiv:1708.02596*，2017 年。
- en: Nair et al. [2015] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek,
    Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles
    Beattie, Stig Petersen, et al. Massively Parallel Methods for Deep Reinforcement
    Learning. In *ICML Workshop on Deep Learning*, 2015.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 等人 [2015] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek,
    Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles
    Beattie, Stig Petersen 等。大规模并行方法用于深度强化学习。发表于 *ICML Workshop on Deep Learning*，2015
    年。
- en: Ng and Russell [2000] Andrew Y Ng and Stuart J Russell. Algorithms for Inverse
    Reinforcement Learning. In *ICML*, 2000.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 和 Russell [2000] Andrew Y Ng 和 Stuart J Russell。逆向强化学习算法。发表于 *ICML*，2000
    年。
- en: Ng et al. [2006] Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie
    Schulte, Ben Tse, Eric Berger, and Eric Liang. Autonomous Inverted Helicopter
    Flight via Reinforcement Learning. *Experimental Robotics*, pages 363–372, 2006.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 等人 [2006] Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte,
    Ben Tse, Eric Berger 和 Eric Liang。通过强化学习实现自主倒置直升机飞行。*Experimental Robotics*，第
    363–372 页，2006 年。
- en: 'O’Donoghue et al. [2017] Brendan O’Donoghue, Rémi Munos, Koray Kavukcuoglu,
    and Volodymyr Mnih. PGQ: Combining Policy Gradient and Q-Learning. In *ICLR*,
    2017.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Donoghue 等人 [2017] Brendan O’Donoghue, Rémi Munos, Koray Kavukcuoglu 和 Volodymyr
    Mnih。PGQ：结合策略梯度和 Q 学习。发表于 *ICLR*，2017 年。
- en: Oh et al. [2015] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and
    Satinder Singh. Action-Conditional Video Prediction using Deep Networks in Atari
    Games. In *NIPS*, 2015.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等人 [2015] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis 和 Satinder
    Singh。在 Atari 游戏中使用深度网络进行动作条件视频预测。发表于 *NIPS*，2015 年。
- en: Oh et al. [2016] Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak
    Lee. Control of Memory, Active Perception, and Action in Minecraft. In *ICLR*,
    2016.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等人 [2016] Junhyuk Oh, Valliappa Chockalingam, Satinder Singh 和 Honglak Lee。Minecraft
    中的记忆控制、主动感知和行动。发表于 *ICLR*，2016 年。
- en: Osband et al. [2016] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin
    Van Roy. Deep Exploration via Bootstrapped DQN. In *NIPS*, 2016.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband 等人 [2016] Ian Osband, Charles Blundell, Alexander Pritzel 和 Benjamin
    Van Roy。通过自助法 DQN 进行深度探索。发表于 *NIPS*，2016 年。
- en: 'Parisotto and Salakhutdinov [2017] Emilio Parisotto and Ruslan Salakhutdinov.
    Neural Map: Structured Memory for Deep Reinforcement Learning. *arXiv:1702.08360*,
    2017.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto 和 Salakhutdinov [2017] Emilio Parisotto 和 Ruslan Salakhutdinov。神经地图：用于深度强化学习的结构化记忆。*arXiv:1702.08360*，2017
    年。
- en: 'Parisotto et al. [2016] Emilio Parisotto, Jimmy L Ba, and Ruslan Salakhutdinov.
    Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning. In *ICLR*, 2016.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto 等人 [2016] Emilio Parisotto, Jimmy L Ba 和 Ruslan Salakhutdinov。Actor-Mimic：深度多任务与迁移强化学习。发表于
    *ICLR*，2016 年。
- en: Pascanu et al. [2017] Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess,
    Lars Buesing, Sebastien Racanière, David Reichert, Théophane Weber, Daan Wierstra,
    and Peter Battaglia. Learning Model-Based Planning from Scratch. *arXiv:1707.06170*,
    2017.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pathak et al. [2017] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor
    Darrell. Curiosity-Driven Exploration by Self-supervised Prediction. In *ICML*,
    2017.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. [2017] Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang,
    Haitao Long, and Jun Wang. Multiagent Bidirectionally-Coordinated Nets: Emergence
    of Human-level Coordination in Learning to Play StarCraft Combat Games. *arXiv:1703.10069*,
    2017.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. [2010] Jan Peters, Katharina Mülling, and Yasemin Altun. Relative
    Entropy Policy Search. In *AAAI*, 2010.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pomerleau [1989] Dean A Pomerleau. ALVINN, an Autonomous Land Vehicle in a Neural
    Network. Technical report, Carnegie Mellon University, Computer Science Department,
    1989.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pritzel et al. [2017] Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adrià
    Puigdomènech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell.
    Neural Episodic Control. In *ICML*, 2017.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranzato et al. [2016] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and
    Wojciech Zaremba. Sequence Level Training with Recurrent Neural Networks. In *ICLR*,
    2016.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recht et al. [2011] Benjamin Recht, Christopher Re, Stephen Wright, and Feng
    Niu. Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.
    In *NIPS*, 2011.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende et al. [2014] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
    Stochastic Backpropagation and Approximate Inference in Deep Generative Models.
    In *ICML*, 2014.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riedmiller [2005] Martin Riedmiller. Neural Fitted Q Iteration—First Experiences
    with a Data Efficient Neural Reinforcement Learning Method. In *ECML*, 2005.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ross et al. [2011] Stéphane Ross, Geoffrey J Gordon, and Drew Bagnell. A Reduction
    of Imitation Learning and Structured Prediction to No-Regret Online Learning.
    In *AISTATS*, 2011.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. [1988] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    Learning Representations by Back-Propagating Errors. *Cognitive Modeling*, 5(3):1,
    1988.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rummery and Niranjan [1994] Gavin A Rummery and Mahesan Niranjan. *On-line Q-learning
    using Connectionist Systems*. University of Cambridge, Department of Engineering,
    1994.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rusu et al. [2016a] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre,
    Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray
    Kavukcuoglu, and Raia Hadsell. Policy Distillation. In *ICLR*, 2016a.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rusu et al. [2016b] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
    Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
    Progressive Neural Networks. *arXiv:1606.04671*, 2016b.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rusu et al. [2017] Andrei A Rusu, Matej Vecerik, Thomas Rothörl, Nicolas Heess,
    Razvan Pascanu, and Raia Hadsell. Sim-to-Real Robot Learning from Pixels with
    Progressive Nets. In *CoRL*, 2017.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salimans et al. [2017] Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever.
    Evolution Strategies as a Scalable Alternative to Reinforcement Learning. *arXiv:1703.03864*,
    2017.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaul et al. [2015] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.
    Universal Value Function Approximators. In *ICML*, 2015.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaul et al. [2016] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
    Prioritized Experience Replay. In *ICLR*, 2016.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidhuber [1991] Jürgen Schmidhuber. A Possibility for Implementing Curiosity
    and Boredom in Model-Building Neural Controllers. In *SAB*, 1991.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidhuber and Huber [1991] Jürgen Schmidhuber and Rudolf Huber. Learning to
    Generate Artificial Fovea Trajectories for Target Detection. *IJNS*, 2(01n02):125–134,
    1991.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2015a] John Schulman, Nicolas Heess, Theophane Weber, and Pieter
    Abbeel. Gradient Estimation using Stochastic Computation Graphs. In *NIPS*, 2015a.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2015b] John Schulman, Sergey Levine, Pieter Abbeel, Michael
    Jordan, and Philipp Moritz. Trust Region Policy Optimization. In *ICML*, 2015b.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2016] John Schulman, Philipp Moritz, Sergey Levine, Michael
    Jordan, and Pieter Abbeel. High-Dimensional Continuous Control using Generalized
    Advantage Estimation. In *ICLR*, 2016.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2017a] John Schulman, Pieter Abbeel, and Xi Chen. Equivalence
    Between Policy Gradients and Soft Q-Learning. *arXiv:1704.06440*, 2017a.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2017b] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. *arXiv:1707.06347*,
    2017b.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shahriari et al. [2016] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams,
    and Nando de Freitas. Taking the Human out of the Loop: A Review of Bayesian Optimization.
    *Proc. of the IEEE*, 104(1):148–175, 2016.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. [2014] David Silver, Guy Lever, Nicolas Heess, Thomas Degris,
    Daan Wierstra, and Martin Riedmiller. Deterministic Policy Gradient Algorithms.
    In *ICML*, 2014.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez,
    Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
    Veda Panneershelvam, Marc Lanctot, et al. Mastering the Game of Go with Deep Neural
    Networks and Tree Search. *Nature*, 529(7587):484–489, 2016.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. [2002] Satinder Singh, Diane Litman, Michael Kearns, and Marilyn
    Walker. Optimizing Dialogue Management with Reinforcement Learning: Experiments
    with the NJFun System. *JAIR*, 16:105–133, 2002.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorokin et al. [2015] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr
    Fedorov, and Anastasiia Ignateva. Deep Attention Recurrent Q-Network. In *NIPS
    Workshop on Deep Reinforcement Learning*, 2015.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stadie et al. [2017] Bradley C Stadie, Pieter Abbeel, and Ilya Sutskever. Third
    Person Imitation Learning. In *ICLR*, 2017.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stadie et al. [2015] Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing
    Exploration in Reinforcement Learning with Deep Predictive Models. In *NIPS Workshop
    on Deep Reinforcement Learning*, 2015.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strehl et al. [2006] Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford,
    and Michael L Littman. PAC Model-Free Reinforcement Learning. In *ICML*, 2006.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sukhbaatar et al. [2016] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.
    Learning Multiagent Communication with Backpropagation. In *NIPS*, 2016.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto [1998] Richard S Sutton and Andrew G Barto. *Reinforcement
    Learning: An Introduction*. MIT Press, 1998.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton et al. [1999] Richard S Sutton, Doina Precup, and Satinder Singh. Between
    MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning.
    *Artificial Intelligence*, 112(1–2):181–211, 1999.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Synnaeve et al. [2016] Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith
    Chintala, Timothée Lacroix, Zeming Lin, Florian Richoux, and Nicolas Usunier.
    TorchCraft: A Library for Machine Learning Research on Real-Time Strategy Games.
    *arXiv:1611.00625*, 2016.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tamar et al. [2016] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter
    Abbeel. Value Iteration Networks. In *NIPS*, 2016.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. [2017] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen,
    Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #Exploration: A Study
    of Count-Based Exploration for Deep Reinforcement Learning. In *NIPS*, 2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teh et al. [2017] Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John
    Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral:
    Robust Multitask Reinforcement Learning. In *NIPS*, 2017.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tesauro [1995] Gerald Tesauro. Temporal Difference Learning and TD-Gammon. *Communications
    of the ACM*, 38(3):58–68, 1995.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tesauro et al. [2008] Gerald Tesauro, Rajarshi Das, Hoi Chan, Jeffrey Kephart,
    David Levine, Freeman Rawson, and Charles Lefurgy. Managing Power Consumption
    and Performance of Computing Systems using Reinforcement Learning. In *NIPS*,
    2008.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tessler et al. [2017] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz,
    and Shie Mannor. A Deep Hierarchical Approach to Lifelong Learning in Minecraft.
    In *AAAI*, 2017.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Todorov et al. [2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A
    Physics Engine for Model-Based Control. In *IROS*, 2012.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsitsiklis and Van Roy [1997] John N Tsitsiklis and Benjamin Van Roy. Analysis
    of Temporal-Difference Learning with Function Approximation. In *NIPS*, 1997.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tzeng et al. [2016] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao
    Peng, Sergey Levine, Kate Saenko, and Trevor Darrell. Towards Adapting Deep Visuomotor
    Representations from Simulated to Real Environments. In *WAFR*, 2016.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usunier et al. [2017] Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith
    Chintala. Episodic Exploration for Deep Deterministic Policies: An Application
    to StarCraft Micromanagement Tasks. In *ICLR*, 2017.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Hasselt [2010] Hado van Hasselt. Double Q-Learning. In *NIPS*, 2010.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Hasselt et al. [2016] Hado van Hasselt, Arthur Guez, and David Silver. Deep
    Reinforcement Learning with Double Q-Learning. In *AAAI*, 2016.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanseijen and Sutton [2015] Harm Vanseijen and Rich Sutton. A Deeper Look at
    Planning as Learning from Replay. In *ICML*, 2015.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vezhnevets et al. [2016] Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero,
    Alex Graves, Oriol Vinyals, John Agapiou, and Koray Kavukcuoglu. Strategic Attentive
    Writer for Learning Macro-Actions. In *NIPS*, 2016.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vezhnevets et al. [2017] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul,
    Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. FeUdal Networks
    for Hierarchical Reinforcement Learning. In *ICML*, 2017.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. [2017] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev,
    Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler,
    John Agapiou, Julian Schrittwieser, et al. StarCraft II: A New Challenge for Reinforcement
    Learning. *arXiv:1708.04782*, 2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wahlström et al. [2015a] Niklas Wahlström, Thomas B Schön, and Marc P Deisenroth.
    Learning Deep Dynamical Models from Image Pixels. *IFAC SYSID*, 48(28), 2015a.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wahlström et al. [2015b] Niklas Wahlström, Thomas B Schön, and Marc P Deisenroth.
    From Pixels to Torques: Policy Learning with Deep Dynamical Models. In *ICML Workshop
    on Deep Learning*, 2015b.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2017a] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer,
    Joel Z Leibo, Rémi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.
    Learning to Reinforcement Learn. In *CogSci*, 2017a.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2016] Ziyu Wang, Nando de Freitas, and Marc Lanctot. Dueling Network
    Architectures for Deep Reinforcement Learning. In *ICLR*, 2016.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2017b] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih,
    Rémi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample Efficient Actor-Critic
    with Experience Replay. In *ICLR*, 2017b.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watkins and Dayan [1992] Christopher JCH Watkins and Peter Dayan. Q-Learning.
    *Machine Learning*, 8(3-4):279–292, 1992.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Watter et al. [2015] Manuel Watter, Jost Springenberg, Joschka Boedecker, and
    Martin Riedmiller. Embed to Control: A Locally Linear Latent Dynamics Model for
    Control from Raw Images. In *NIPS*, 2015.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weber et al. [2017] Théophane Weber, Sébastien Racanière, David P Reichert,
    Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomènech Badia, Oriol
    Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-Augmented Agents for Deep
    Reinforcement Learning. In *NIPS*, 2017.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Werbos [1974] Paul John Werbos. Beyond Regression: New Tools for Prediction
    and Analysis in the Behavioral Sciences. Technical report, Harvard University,
    Applied Mathematics, 1974.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wierstra et al. [2010] Daan Wierstra, Alexander Förster, Jan Peters, and Jürgen
    Schmidhuber. Recurrent Policy Gradients. *Logic Journal of the IGPL*, 18(5):620–634,
    2010.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams [1992] Ronald J Williams. Simple Statistical Gradient-Following Algorithms
    for Connectionist Reinforcement Learning. *Machine Learning*, 8(3-4):229–256,
    1992.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wulfmeier et al. [2015] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner.
    Maximum Entropy Deep Inverse Reinforcement Learning. In *NIPS Workshop on Deep
    Reinforcement Learning*, 2015.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville,
    Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio. Show, Attend and Tell:
    Neural Image Caption Generation with Visual Attention. In *ICML*, volume 14, 2015.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2017] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav
    Gupta, Li Fei-Fei, and Ali Farhadi. Target-Driven Visual Navigation in Indoor
    Scenes using Deep Reinforcement Learning. In *ICRA*, 2017.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph and Le [2017] Barret Zoph and Quoc V Le. Neural Architecture Search with
    Reinforcement Learning. In *ICLR*, 2017.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Kai Arulkumaran (ka709@imperial.ac.uk) is a Ph.D. candidate in the Department
    of Bioengineering at Imperial College London. He received a B.A. in Computer Science
    at the University of Cambridge in 2012, and an M.Sc. in Biomedical Engineering
    at Imperial College London in 2014\. He was a Research Intern in Twitter Magic
    Pony and Microsoft Research in 2017. His research focus is deep reinforcement
    learning and transfer learning for visuomotor control. |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| Marc Peter Deisenroth (m.deisenroth@imperial.ac.uk) is a Lecturer in Statistical
    Machine Learning in the Department of Computing at Imperial College London and
    with PROWLER.io. He received an M.Eng. in Computer Science at the University of
    Karlsruhe in 2006 and a Ph.D. in Machine Learning at the Karlsruhe Institute of
    Technology in 2009\. He has been awarded an Imperial College Research Fellowship
    in 2014 and received Best Paper Awards at ICRA 2014 and ICCAS 2016\. He is a recipient
    of a Google Faculty Research Award and a Microsoft Ph.D. Scholarship. His research
    is centred around data-efficient machine learning for autonomous decision making.
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| Miles Brundage (miles.brundage@philosophy.ox.ac.uk) is a Ph.D. candidate
    in Human and Social Dimensions of Science and Technology at Arizona State University,
    and a Research Fellow at the University of Oxford’s Future of Humanity Institute.
    He received a B.A. in Political Science at George Washington University in 2010\.
    His research focuses on governance issues related to artificial intelligence.
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| Anil Anthony Bharath (a.bharath@imperial.ac.uk) is a Reader in the Department
    of Bioengineering at Imperial College London and a Fellow of the Institution of
    Engineering and Technology. He received a B.Eng. in Electronic and Electrical
    Engineering from University College London in 1988, and a Ph.D. in Signal Processing
    from Imperial College London in 1993\. He was an academic visitor in the Signal
    Processing Group at the University of Cambridge in 2006\. He is a co-founder of
    Cortexica Vision Systems. His research interests are in deep architectures for
    visual inference. |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
