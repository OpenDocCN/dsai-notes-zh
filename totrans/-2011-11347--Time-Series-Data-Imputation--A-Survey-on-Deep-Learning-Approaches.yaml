- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:58:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:58:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2011.11347] Time Series Data Imputation: A Survey on Deep Learning Approaches'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2011.11347] æ—¶é—´åºåˆ—æ•°æ®æ’è¡¥ï¼šæ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2011.11347](https://ar5iv.labs.arxiv.org/html/2011.11347)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2011.11347](https://ar5iv.labs.arxiv.org/html/2011.11347)
- en: 'Time Series Data Imputation: A Survey on Deep Learning Approaches'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—æ•°æ®æ’è¡¥ï¼šæ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°
- en: Chenguang Fang [fcg19@mails.tsinghua.edu.cn](mailto:fcg19@mails.tsinghua.edu.cn)
    Chen Wang [wangË™chen@tsinghua.edu.cn](mailto:wang%CB%99chen@tsinghua.edu.cn) Tsinghua
    University, Beijing
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å†¯æˆå…‰ [fcg19@mails.tsinghua.edu.cn](mailto:fcg19@mails.tsinghua.edu.cn) ç‹æ™¨ [wangË™chen@tsinghua.edu.cn](mailto:wang%CB%99chen@tsinghua.edu.cn)
    æ¸…åå¤§å­¦ï¼ŒåŒ—äº¬
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Time series are all around in real-world applications. However, unexpected accidents
    for example broken sensors or missing of the signals will cause missing values
    in time series, making the data hard to be utilized. It then does harm to the
    downstream applications such as traditional classification or regression, sequential
    data integration and forecasting tasks, thus raising the demand for data imputation.
    Currently, time series data imputation is a well-studied problem with different
    categories of methods. However, these works rarely take the temporal relations
    among the observations and treat the time series as normal structured data, losing
    the information from the time data. In recent, deep learning models have raised
    great attention. Time series methods based on deep learning have made progress
    with the usage of models like RNN, since it captures time information from data.
    In this paper, we mainly focus on time series imputation technique with deep learning
    methods, which recently made progress in this field. We will review and discuss
    their model architectures, their pros and cons as well as their effects to show
    the development of the time series imputation methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­æ— å¤„ä¸åœ¨ã€‚ç„¶è€Œï¼Œæ„å¤–äº‹æ•…ï¼Œä¾‹å¦‚ä¼ æ„Ÿå™¨æ•…éšœæˆ–ä¿¡å·ä¸¢å¤±ï¼Œä¼šå¯¼è‡´æ—¶é—´åºåˆ—ä¸­å‡ºç°ç¼ºå¤±å€¼ï¼Œä»è€Œä½¿æ•°æ®éš¾ä»¥åˆ©ç”¨ã€‚è¿™å¯¹ä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚ä¼ ç»Ÿçš„åˆ†ç±»æˆ–å›å½’ã€åºåˆ—æ•°æ®é›†æˆå’Œé¢„æµ‹ä»»åŠ¡ï¼Œé€ æˆäº†ä¼¤å®³ï¼Œå› æ­¤å¯¹æ•°æ®æ’è¡¥çš„éœ€æ±‚å¢åŠ ã€‚ç›®å‰ï¼Œæ—¶é—´åºåˆ—æ•°æ®æ’è¡¥æ˜¯ä¸€ä¸ªç ”ç©¶å……åˆ†çš„é—®é¢˜ï¼Œå…·æœ‰ä¸åŒç±»åˆ«çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾ˆå°‘è€ƒè™‘è§‚å¯Ÿæ•°æ®ä¹‹é—´çš„æ—¶é—´å…³ç³»ï¼Œå¹¶å°†æ—¶é—´åºåˆ—è§†ä¸ºæ™®é€šçš„ç»“æ„åŒ–æ•°æ®ï¼Œå¯¼è‡´ä¸§å¤±æ—¶é—´æ•°æ®ä¸­çš„ä¿¡æ¯ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ—¶é—´åºåˆ—æ–¹æ³•åœ¨ä½¿ç”¨å¦‚RNNç­‰æ¨¡å‹æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œå› ä¸ºå®ƒæ•æ‰äº†æ•°æ®ä¸­çš„æ—¶é—´ä¿¡æ¯ã€‚æœ¬æ–‡ä¸»è¦å…³æ³¨ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œæ—¶é—´åºåˆ—æ’è¡¥æŠ€æœ¯ï¼Œæœ€è¿‘åœ¨è¿™ä¸€é¢†åŸŸå–å¾—äº†è¿›å±•ã€‚æˆ‘ä»¬å°†å›é¡¾å’Œè®¨è®ºå®ƒä»¬çš„æ¨¡å‹æ¶æ„ã€ä¼˜ç¼ºç‚¹åŠå…¶æ•ˆæœï¼Œä»¥å±•ç¤ºæ—¶é—´åºåˆ—æ’è¡¥æ–¹æ³•çš„å‘å±•ã€‚
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å…³é”®è¯ï¼š
- en: Time Series Imputation , Deep Learning , GAN , RNN
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—æ’è¡¥ï¼Œæ·±åº¦å­¦ä¹ ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: Time series are vital in real-world applications. However, due to unexpected
    accidents, for example broken sensors or missing of the signals, missing values
    are everywhere in time series. In some datasets, the missing rate can reach 90%,
    which makes the data hard to be utilized [[12](#bib.bib12)]. The missing values
    significantly do harm to the downstream applications such as traditional classification
    or regression, sequential data integration [[21](#bib.bib21)] and forecasting
    tasks [[18](#bib.bib18)], leading to high demand for data imputation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ„å¤–äº‹æ•…ï¼Œä¾‹å¦‚ä¼ æ„Ÿå™¨æ•…éšœæˆ–ä¿¡å·ä¸¢å¤±ï¼Œæ—¶é—´åºåˆ—ä¸­åˆ°å¤„éƒ½æ˜¯ç¼ºå¤±å€¼ã€‚åœ¨ä¸€äº›æ•°æ®é›†ä¸­ï¼Œç¼ºå¤±ç‡å¯ä»¥è¾¾åˆ°90%ï¼Œè¿™ä½¿å¾—æ•°æ®éš¾ä»¥åˆ©ç”¨[[12](#bib.bib12)]ã€‚ç¼ºå¤±å€¼å¯¹ä¸‹æ¸¸åº”ç”¨é€ æˆäº†æ˜¾è‘—çš„ä¼¤å®³ï¼Œä¾‹å¦‚ä¼ ç»Ÿçš„åˆ†ç±»æˆ–å›å½’ã€åºåˆ—æ•°æ®é›†æˆ[[21](#bib.bib21)]å’Œé¢„æµ‹ä»»åŠ¡[[18](#bib.bib18)]ï¼Œå¯¼è‡´å¯¹æ•°æ®æ’è¡¥çš„éœ€æ±‚å¾ˆé«˜ã€‚
- en: Our preliminary study [[11](#bib.bib11)] shows that imputing the missing values
    indeed helps significantly the prediction of fuel consumption. In the scenarios
    of fuel consumption prediction, missing values happen due to the errors of sensors.
    We propose an imputation approach named FuelNet to deal with such errors. The
    FuelNet generates proper values to impute missing data. With imputed data, the
    fuel consumption can be reduced by around 45.5%.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„åˆæ­¥ç ”ç©¶[[11](#bib.bib11)]è¡¨æ˜ï¼Œæ’è¡¥ç¼ºå¤±å€¼ç¡®å®æ˜¾è‘—æé«˜äº†ç‡ƒæ–™æ¶ˆè€—çš„é¢„æµ‹ã€‚åœ¨ç‡ƒæ–™æ¶ˆè€—é¢„æµ‹çš„åœºæ™¯ä¸­ï¼Œç¼ºå¤±å€¼å‘ç”Ÿæ˜¯ç”±äºä¼ æ„Ÿå™¨çš„é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFuelNetçš„æ’è¡¥æ–¹æ³•æ¥å¤„ç†è¿™äº›é”™è¯¯ã€‚FuelNetç”Ÿæˆé€‚å½“çš„å€¼æ¥æ’è¡¥ç¼ºå¤±æ•°æ®ã€‚ä½¿ç”¨æ’è¡¥æ•°æ®ï¼Œç‡ƒæ–™æ¶ˆè€—å¯ä»¥å‡å°‘çº¦45.5%ã€‚
- en: In current stages, time series data imputation is a well studied problem with
    different categories of methods including deletion methods, simple imputation
    methods and learning based methods. However, these works rarely take the temporal
    relations among the observations and treat the time series as normal structured
    data, thus losing the information from the time data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰é˜¶æ®µï¼Œæ—¶é—´åºåˆ—æ•°æ®æ’è¡¥æ˜¯ä¸€ä¸ªç ”ç©¶è¾ƒå¤šçš„é—®é¢˜ï¼Œæ¶µç›–äº†ä¸åŒç±»åˆ«çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ é™¤æ–¹æ³•ã€ç®€å•æ’è¡¥æ–¹æ³•å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›å·¥ä½œå¾ˆå°‘è€ƒè™‘è§‚æµ‹å€¼ä¹‹é—´çš„æ—¶é—´å…³ç³»ï¼Œè€Œæ˜¯å°†æ—¶é—´åºåˆ—è§†ä¸ºæ™®é€šçš„ç»“æ„åŒ–æ•°æ®ï¼Œä»è€Œä¸§å¤±äº†æ—¶é—´æ•°æ®çš„ä¿¡æ¯ã€‚
- en: Fortunately, with the increasing development of deep learning, a large quantity
    of deep learning methods are researched, among which RNN is one of the typical
    methods to handle sequence data. The intuition on why deep learning models could
    advance imputation tasks is that, they are proven to have the ability to mine
    information hidden in the time series. These characteristics could enable them
    to impute missing values with such models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„ä¸æ–­å‘å±•ï¼Œå¤§é‡æ·±åº¦å­¦ä¹ æ–¹æ³•è¢«ç ”ç©¶ï¼Œå…¶ä¸­RNNæ˜¯å¤„ç†åºåˆ—æ•°æ®çš„å…¸å‹æ–¹æ³•ä¹‹ä¸€ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿæ¨åŠ¨æ’è¡¥ä»»åŠ¡çš„ç›´è§‚åŸå› æ˜¯ï¼Œå®ƒä»¬è¢«è¯æ˜èƒ½å¤ŸæŒ–æ˜æ—¶é—´åºåˆ—ä¸­éšè—çš„ä¿¡æ¯ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿä½¿ç”¨è¿™æ ·çš„æ¨¡å‹æ¥æ’è¡¥ç¼ºå¤±å€¼ã€‚
- en: Recently, deep learning methods have been applied to multivariable time series
    imputation and show positive progress in imputing the missing values. In this
    paper, we mainly survey three papers about time series imputation with deep learning
    methods [[7](#bib.bib7), [27](#bib.bib27), [6](#bib.bib6), [28](#bib.bib28), [25](#bib.bib25)]
    among which RNN, GRU and GAN are adopted separately or in combination. We will
    review these papers about their model structure, the common parts they all adopted
    and the advantages and disadvantages through comparison.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•å·²è¢«åº”ç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥ï¼Œå¹¶åœ¨æ’è¡¥ç¼ºå¤±å€¼æ–¹é¢æ˜¾ç¤ºå‡ºç§¯æè¿›å±•ã€‚æœ¬æ–‡ä¸»è¦è°ƒæŸ¥äº†ä¸‰ç¯‡å…³äºæ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ—¶é—´åºåˆ—æ’è¡¥è®ºæ–‡ [[7](#bib.bib7),
    [27](#bib.bib27), [6](#bib.bib6), [28](#bib.bib28), [25](#bib.bib25)]ï¼Œå…¶ä¸­RNNã€GRUå’ŒGANè¢«å•ç‹¬æˆ–ç»„åˆä½¿ç”¨ã€‚æˆ‘ä»¬å°†å›é¡¾è¿™äº›è®ºæ–‡çš„æ¨¡å‹ç»“æ„ã€å®ƒä»¬å…±åŒé‡‡ç”¨çš„éƒ¨åˆ†ä»¥åŠé€šè¿‡æ¯”è¾ƒå¾—å‡ºçš„ä¼˜ç¼ºç‚¹ã€‚
- en: The remainder of the paper is organized as follows. In the next section, we
    categorize existing data imputation methods and mainly give an introduction to
    deep learning imputation methods. Section 3 will show the definition of the problems
    and the symbols. Section 4 will give a detailed discussion of deep learning methods,
    mainly about their concrete structure, advantages and disadvantages. And finally
    in Section 5 we summarize the survey and give our conclusions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ã€‚ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ç°æœ‰çš„æ•°æ®æ’è¡¥æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä¸»è¦ä»‹ç»æ·±åº¦å­¦ä¹ æ’è¡¥æ–¹æ³•ã€‚ç¬¬3èŠ‚å°†å±•ç¤ºé—®é¢˜çš„å®šä¹‰å’Œç¬¦å·ã€‚ç¬¬4èŠ‚å°†è¯¦ç»†è®¨è®ºæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä¸»è¦æ¶‰åŠå…¶å…·ä½“ç»“æ„ã€ä¼˜ç¼ºç‚¹ã€‚æœ€åï¼Œç¬¬5èŠ‚å°†æ€»ç»“è°ƒç ”å¹¶ç»™å‡ºæˆ‘ä»¬çš„ç»“è®ºã€‚
- en: 'Table 1: Comparison of different methods addressing time series imputation'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨1ï¼šä¸åŒæ—¶é—´åºåˆ—æ’è¡¥æ–¹æ³•çš„æ¯”è¾ƒ
- en: '| Methodologies | Sample approaches from the literature | Time interval | Value
    type | Time series dimension |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³•è®º | æ–‡çŒ®ä¸­çš„ç¤ºä¾‹æ–¹æ³• | æ—¶é—´é—´éš” | å€¼ç±»å‹ | æ—¶é—´åºåˆ—ç»´åº¦ |'
- en: '| Deletion | Listwise Deletion [[51](#bib.bib51)] | regular/irregular | qualitative
    | multidimensional |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| åˆ é™¤ | åˆ—åˆ é™¤ [[51](#bib.bib51)] | å¸¸è§„/éè§„ | å®šæ€§ | å¤šç»´ |'
- en: '| Pairwise Deletion [[29](#bib.bib29)] | regular/irregular | qualitative |
    multidimensional |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| é…å¯¹åˆ é™¤ [[29](#bib.bib29)] | å¸¸è§„/éè§„ | å®šæ€§ | å¤šç»´ |'
- en: '| Neighbor Based | QDORC [[41](#bib.bib41)] | regular/irregular | quantitative/qualitative
    | multidimensional |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäºé‚»åŸŸçš„ | QDORC [[41](#bib.bib41)] | å¸¸è§„/éè§„ | å®šé‡/å®šæ€§ | å¤šç»´ |'
- en: '| SRKN [[46](#bib.bib46)] | regular/irregular | quantitative/qualitative |
    multidimensional |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| SRKN [[46](#bib.bib46)] | å¸¸è§„/éè§„ | å®šé‡/å®šæ€§ | å¤šç»´ |'
- en: '| Constraint Based | DERAND [[43](#bib.bib43), [42](#bib.bib42)] | regular/irregular
    | quantitative/qualitative | multidimensional |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäºçº¦æŸ | DERAND [[43](#bib.bib43), [42](#bib.bib42)] | å¸¸è§„/éè§„ | å®šé‡/å®šæ€§ | å¤šç»´ |'
- en: '| SCREEN [[44](#bib.bib44)] | regular/irregular | qualitative | single dimensional
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| SCREEN [[44](#bib.bib44)] | å¸¸è§„/éè§„ | å®šæ€§ | å•ç»´ |'
- en: '| Regression Based | ARX [[5](#bib.bib5)] | regular | qualitative | single
    dimensional |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäºå›å½’ | ARX [[5](#bib.bib5)] | å¸¸è§„ | å®šæ€§ | å•ç»´ |'
- en: '| IMR [[55](#bib.bib55)] | regular | qualitative | single dimensional |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| IMR [[55](#bib.bib55)] | å¸¸è§„ | å®šæ€§ | å•ç»´ |'
- en: '| Statistical | DPC [[54](#bib.bib54)] | regular | qualitative | single dimensional
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| ç»Ÿè®¡å­¦ | DPC [[54](#bib.bib54)] | å¸¸è§„ | å®šæ€§ | å•ç»´ |'
- en: '| IIM [[53](#bib.bib53)] | regular | qualitative | multidimensional |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| IIM [[53](#bib.bib53)] | å¸¸è§„ | å®šæ€§ | å¤šç»´ |'
- en: '| MF Based | TRMF [[52](#bib.bib52)] | regular | qualitative | multidimensional
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäº MF | TRMF [[52](#bib.bib52)] | å¸¸è§„ | å®šæ€§ | å¤šç»´ |'
- en: '| NMF [[30](#bib.bib30)] | regular | qualitative | multidimensional |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| NMF [[30](#bib.bib30)] | å¸¸è§„ | å®šæ€§ | å¤šç»´ |'
- en: '| EM Based | EM [[14](#bib.bib14)] | regular | qualitative | multidimensional
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäº EM | EM [[14](#bib.bib14)] | å¸¸è§„ | å®šæ€§ | å¤šç»´ |'
- en: '| EM-GMM [[32](#bib.bib32)] | regular | qualitative | multidimensional |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| EM-GMM [[32](#bib.bib32)] | å¸¸è§„ | å®šæ€§ | å¤šç»´ |'
- en: '| MLP Based | MLP [[35](#bib.bib35)] | regular | qualitative | single dimensional
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäº MLP | MLP [[35](#bib.bib35)] | å¸¸è§„ | å®šæ€§ | å•ç»´ |'
- en: '| ANN [[33](#bib.bib33)] | regular | qualitative | single dimensional |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ANN [[33](#bib.bib33)] | å¸¸è§„ | å®šæ€§ | å•ç»´ |'
- en: '| DL Based | GRU-D [[7](#bib.bib7)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäº DL | GRU-D [[7](#bib.bib7)] | å¸¸è§„/ä¸è§„åˆ™ | å®šæ€§ | å¤šç»´ |'
- en: '| GRUI-GAN [[27](#bib.bib27)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| GRUI-GAN [[27](#bib.bib27)] | å¸¸è§„/ä¸è§„åˆ™ | å®šæ€§ | å¤šç»´ |'
- en: '| BRITS [[6](#bib.bib6)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| BRITS [[6](#bib.bib6)] | å¸¸è§„/ä¸è§„åˆ™ | å®šæ€§ | å¤šç»´ |'
- en: '| E2GAN [[28](#bib.bib28)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| E2GAN [[28](#bib.bib28)] | å¸¸è§„/ä¸è§„åˆ™ | å®šæ€§ | å¤šç»´ |'
- en: '| NAOMI [[25](#bib.bib25)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| NAOMI [[25](#bib.bib25)] | å¸¸è§„/ä¸è§„åˆ™ | å®šæ€§ | å¤šç»´ |'
- en: 2 Categorization
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 åˆ†ç±»
- en: In this section, we will give a brief introduction of the major approaches to
    time series imputation. Moreover, we will classify existing time series imputation
    methods according to the principles and techniques they rely on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç®€è¦ä»‹ç»ä¸»è¦çš„æ—¶é—´åºåˆ—å¡«è¡¥æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ ¹æ®è¿™äº›æ–¹æ³•æ‰€ä¾èµ–çš„åŸåˆ™å’ŒæŠ€æœ¯å¯¹ç°æœ‰çš„æ—¶é—´åºåˆ—å¡«è¡¥æ–¹æ³•è¿›è¡Œåˆ†ç±»ã€‚
- en: 'In order to impute the missing values, researchers have proposed many imputation
    methods to handle the missing values in time series. In this paper, we mainly
    conclude 8 kinds of the missing value imputation methods including deletion methods,
    neighbor based methods, constraint based methods, regression based methods, statistical
    based methods, MF based methods, EM based mathods, MLP based mathods and DL based
    methods. TableÂ [1](#S1.T1 "Table 1 â€£ 1 Introduction â€£ Time Series Data Imputation:
    A Survey on Deep Learning Approaches") shows the comparison of these methods we
    conclude. We will introduce each kind of method respectively as follows.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†å¡«è¡¥ç¼ºå¤±å€¼ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†è®¸å¤šå¡«è¡¥æ–¹æ³•æ¥å¤„ç†æ—¶é—´åºåˆ—ä¸­çš„ç¼ºå¤±å€¼ã€‚æœ¬æ–‡ä¸»è¦æ€»ç»“äº†åŒ…æ‹¬åˆ é™¤æ–¹æ³•ã€åŸºäºé‚»åŸŸçš„æ–¹æ³•ã€åŸºäºçº¦æŸçš„æ–¹æ³•ã€åŸºäºå›å½’çš„æ–¹æ³•ã€åŸºäºç»Ÿè®¡çš„æ–¹æ³•ã€åŸºäº
    MF çš„æ–¹æ³•ã€åŸºäº EM çš„æ–¹æ³•ã€åŸºäº MLP çš„æ–¹æ³•å’ŒåŸºäº DL çš„æ–¹æ³•åœ¨å†…çš„ 8 ç§ç¼ºå¤±å€¼å¡«è¡¥æ–¹æ³•ã€‚è¡¨ [1](#S1.T1 "Table 1 â€£ 1
    Introduction â€£ Time Series Data Imputation: A Survey on Deep Learning Approaches")
    æ˜¾ç¤ºäº†æˆ‘ä»¬æ€»ç»“çš„è¿™äº›æ–¹æ³•çš„æ¯”è¾ƒã€‚æˆ‘ä»¬å°†åˆ†åˆ«ä»‹ç»æ¯ç§æ–¹æ³•ã€‚'
- en: Deletion methods take a simple strategy that they directly erase the observations
    that contain missing values from the raw data [[29](#bib.bib29), [51](#bib.bib51)].
    It is also a commonly adopted strategy when the missing value is not high and
    the deletion of the missing values will not influence the downstream applications.
    However, when the missing rate reaches some level (in [[16](#bib.bib16)], it is
    5%), ignoring the missing values and deleting them make the data incomplete and
    not suitable for downstream applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ é™¤æ–¹æ³•é‡‡ç”¨ç®€å•çš„ç­–ç•¥ï¼Œå³ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è§‚æµ‹ [[29](#bib.bib29), [51](#bib.bib51)]ã€‚å½“ç¼ºå¤±å€¼ä¸å¤šä¸”åˆ é™¤ç¼ºå¤±å€¼ä¸ä¼šå½±å“åç»­åº”ç”¨æ—¶ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§å¸¸ç”¨çš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œå½“ç¼ºå¤±ç‡è¾¾åˆ°æŸä¸ªæ°´å¹³æ—¶ï¼ˆåœ¨
    [[16](#bib.bib16)] ä¸­ä¸º 5%ï¼‰ï¼Œå¿½ç•¥ç¼ºå¤±å€¼å¹¶åˆ é™¤å®ƒä»¬ä¼šä½¿æ•°æ®ä¸å®Œæ•´ï¼Œä¸é€‚åˆåç»­åº”ç”¨ã€‚
- en: Neighbor based methods [[3](#bib.bib3), [41](#bib.bib41)] find out the imputation
    value from neighbors, e.g., identified by clustering methods like KNN or DBSCAN.
    They first find the nearest neighbors of the missing values through other attributes,
    and then update the missing values with the mean value of these neighbors. Moreover,
    considering the local similarity, some methods take the last observed valid value
    to replace the blank [[2](#bib.bib2)]. SRKN (Swapping Repair with K Neighbors)
    [[46](#bib.bib46)] in our preliminary study could also be adapted to impute the
    missing values that are misplaced in other dimensions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºé‚»åŸŸçš„æ–¹æ³• [[3](#bib.bib3), [41](#bib.bib41)] ä»é‚»åŸŸä¸­æ‰¾å‡ºå¡«è¡¥å€¼ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡ KNN æˆ– DBSCAN ç­‰èšç±»æ–¹æ³•è¿›è¡Œè¯†åˆ«ã€‚å®ƒä»¬é¦–å…ˆé€šè¿‡å…¶ä»–å±æ€§æ‰¾åˆ°ç¼ºå¤±å€¼çš„æœ€è¿‘é‚»ï¼Œç„¶åç”¨è¿™äº›é‚»åŸŸçš„å‡å€¼æ¥æ›´æ–°ç¼ºå¤±å€¼ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°å±€éƒ¨ç›¸ä¼¼æ€§ï¼Œä¸€äº›æ–¹æ³•ä½¿ç”¨æœ€åè§‚å¯Ÿåˆ°çš„æœ‰æ•ˆå€¼æ¥æ›¿ä»£ç©ºç™½
    [[2](#bib.bib2)]ã€‚åœ¨æˆ‘ä»¬çš„åˆæ­¥ç ”ç©¶ä¸­ï¼ŒSRKNï¼ˆç”¨ K ä¸ªé‚»å±…è¿›è¡Œäº¤æ¢ä¿®å¤ï¼‰ [[46](#bib.bib46)] ä¹Ÿå¯ä»¥é€‚ç”¨äºå¡«è¡¥åœ¨å…¶ä»–ç»´åº¦ä¸Šé”™ä½çš„ç¼ºå¤±å€¼ã€‚
- en: Constraint based methods [[43](#bib.bib43), [42](#bib.bib42)] discover the rules
    in dataset, and take advantage of these rules to impute. To apply to time series
    data, similarity rules such as differential dependencies [[37](#bib.bib37), [38](#bib.bib38)]
    or comparable dependencies [[39](#bib.bib39), [40](#bib.bib40)] could be employed
    that study the distances or similarities of timestamps as well as values [[45](#bib.bib45)].
    More advanced constraints could be specified in a graph structure [[48](#bib.bib48),
    [57](#bib.bib57)], such as Petri net, and employed to impute the qualitative values
    of events in time series [[49](#bib.bib49), [50](#bib.bib50)]. These methods are
    effective when the data is highly continuous or satisfies certain patterns. For
    example, when the data is increasing linearly, it is effective and efficient to
    take simple methods or clustering methods. And when the rules or constraints are
    satisfied, constraints based methods outperform others in both time and accuracy
    [[44](#bib.bib44)]. However, multivariable time series in the real world are not
    usually satisfied with such rules, thus more general methods are required and
    learning based methods are researched to impute the time series automatically.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºçº¦æŸçš„æ–¹æ³• [[43](#bib.bib43), [42](#bib.bib42)] å‘ç°æ•°æ®é›†ä¸­çš„è§„åˆ™ï¼Œå¹¶åˆ©ç”¨è¿™äº›è§„åˆ™è¿›è¡Œæ’è¡¥ã€‚è¦åº”ç”¨äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨ç±»ä¼¼çš„è§„åˆ™ï¼Œä¾‹å¦‚å·®åˆ†ä¾èµ–
    [[37](#bib.bib37), [38](#bib.bib38)] æˆ–å¯æ¯”ä¾èµ– [[39](#bib.bib39), [40](#bib.bib40)]ï¼Œè¿™äº›è§„åˆ™ç ”ç©¶æ—¶é—´æˆ³åŠå…¶å€¼çš„è·ç¦»æˆ–ç›¸ä¼¼æ€§
    [[45](#bib.bib45)]ã€‚æ›´å…ˆè¿›çš„çº¦æŸå¯ä»¥åœ¨å›¾ç»“æ„ [[48](#bib.bib48), [57](#bib.bib57)] ä¸­æŒ‡å®šï¼Œä¾‹å¦‚Petriç½‘ï¼Œå¹¶ç”¨äºæ’è¡¥æ—¶é—´åºåˆ—ä¸­äº‹ä»¶çš„å®šæ€§å€¼
    [[49](#bib.bib49), [50](#bib.bib50)]ã€‚å½“æ•°æ®é«˜åº¦è¿ç»­æˆ–ç¬¦åˆæŸäº›æ¨¡å¼æ—¶ï¼Œè¿™äº›æ–¹æ³•éå¸¸æœ‰æ•ˆã€‚ä¾‹å¦‚ï¼Œå½“æ•°æ®çº¿æ€§å¢é•¿æ—¶ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•æˆ–èšç±»æ–¹æ³•æ—¢æœ‰æ•ˆåˆé«˜æ•ˆã€‚å½“è§„åˆ™æˆ–çº¦æŸå¾—åˆ°æ»¡è¶³æ—¶ï¼ŒåŸºäºçº¦æŸçš„æ–¹æ³•åœ¨æ—¶é—´å’Œå‡†ç¡®æ€§ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•
    [[44](#bib.bib44)]ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„å¤šå˜é‡æ—¶é—´åºåˆ—é€šå¸¸ä¸ç¬¦åˆè¿™äº›è§„åˆ™ï¼Œå› æ­¤éœ€è¦æ›´é€šç”¨çš„æ–¹æ³•ï¼Œç ”ç©¶è€…ä»¬æ­£åœ¨ç ”ç©¶åŸºäºå­¦ä¹ çš„æ–¹æ³•ä»¥è‡ªåŠ¨æ’è¡¥æ—¶é—´åºåˆ—æ•°æ®ã€‚
- en: Regression based methods LOESSÂ [[9](#bib.bib9)] learns a regression model from
    nearest neighbors for predicting the missing value referring to the complete attributes.
    For time series data, autoregressive (AR) models (e.g., ARX [[5](#bib.bib5)] and
    ARIMA [[56](#bib.bib56)]) try to predict missing values from historical data.
    More advanced IMR (iterative minimum repairing [[55](#bib.bib55)]) provides both
    anomaly detection and data repair for both anomalies and missing values. These
    methods mostly benefit from historical data as well as the accuracy of the nearest
    neighbors. Thus they could be applied when neighbors are reliable and the time
    series are highly relative.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå›å½’çš„æ–¹æ³• LOESS [[9](#bib.bib9)] ä»æœ€è¿‘é‚»ä¸­å­¦ä¹ å›å½’æ¨¡å‹ï¼Œä»¥é¢„æµ‹ç¼ºå¤±å€¼ï¼Œå‚è€ƒå®Œæ•´å±æ€§ã€‚å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ï¼ˆå¦‚
    ARX [[5](#bib.bib5)] å’Œ ARIMA [[56](#bib.bib56)]ï¼‰å°è¯•ä»å†å²æ•°æ®ä¸­é¢„æµ‹ç¼ºå¤±å€¼ã€‚æ›´å…ˆè¿›çš„ IMRï¼ˆè¿­ä»£æœ€å°ä¿®å¤
    [[55](#bib.bib55)]ï¼‰åŒæ—¶æä¾›å¼‚å¸¸æ£€æµ‹å’Œæ•°æ®ä¿®å¤ï¼Œç”¨äºå¤„ç†å¼‚å¸¸å’Œç¼ºå¤±å€¼ã€‚è¿™äº›æ–¹æ³•å¤§å¤šä¾èµ–äºå†å²æ•°æ®ä»¥åŠæœ€è¿‘é‚»çš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œå½“é‚»å±…å¯é ä¸”æ—¶é—´åºåˆ—é«˜åº¦ç›¸å…³æ—¶ï¼Œå®ƒä»¬å¯ä»¥åº”ç”¨ã€‚
- en: Statistical based methods rely on statistical models to impute the missing values
    [[24](#bib.bib24)]. Simple statistical methods just utilize the data in the original
    data to impute the missing values, such as take the mean value or median value
    of the attribute to impute [[1](#bib.bib1), [20](#bib.bib20)]. [[54](#bib.bib54)]
    estimates probability values by statistics on speeds as well as the changes. Recently,
    more advanced IIM (Imputation via Individual models) [[53](#bib.bib53)] adaptively
    learns individual models for various number of neighbors. Unlike regression based
    methods which based on just historical data, statistical based models are learned
    from the whole dataset, including historical data and future data. Therefore,
    they may capture more information from raw data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºç»Ÿè®¡çš„æ–¹æ³•ä¾èµ–äºç»Ÿè®¡æ¨¡å‹æ¥æ’è¡¥ç¼ºå¤±å€¼ [[24](#bib.bib24)]ã€‚ç®€å•çš„ç»Ÿè®¡æ–¹æ³•ä»…åˆ©ç”¨åŸå§‹æ•°æ®ä¸­çš„æ•°æ®æ¥æ’è¡¥ç¼ºå¤±å€¼ï¼Œä¾‹å¦‚å–å±æ€§çš„å‡å€¼æˆ–ä¸­ä½æ•°è¿›è¡Œæ’è¡¥
    [[1](#bib.bib1), [20](#bib.bib20)]ã€‚[[54](#bib.bib54)] é€šè¿‡ç»Ÿè®¡é€Ÿåº¦å’Œå˜åŒ–æ¥ä¼°è®¡æ¦‚ç‡å€¼ã€‚è¿‘å¹´æ¥ï¼Œæ›´å…ˆè¿›çš„
    IIMï¼ˆé€šè¿‡ä¸ªä½“æ¨¡å‹æ’è¡¥ï¼‰ [[53](#bib.bib53)] è‡ªé€‚åº”åœ°å­¦ä¹ å„ç§é‚»å±…æ•°é‡çš„ä¸ªä½“æ¨¡å‹ã€‚ä¸ä»…åŸºäºå†å²æ•°æ®çš„å›å½’æ–¹æ³•ä¸åŒï¼ŒåŸºäºç»Ÿè®¡çš„æ¨¡å‹æ˜¯ä»æ•´ä¸ªæ•°æ®é›†å­¦ä¹ çš„ï¼ŒåŒ…æ‹¬å†å²æ•°æ®å’Œæœªæ¥æ•°æ®ã€‚å› æ­¤ï¼Œå®ƒä»¬å¯èƒ½ä¼šä»åŸå§‹æ•°æ®ä¸­æ•è·æ›´å¤šä¿¡æ¯ã€‚
- en: Matrix Factorization based methods The Matrix Factorization (MF) algorithm tries
    to impute the value with the Matrix Factorization and reconstruction to find the
    correlations among the data and complete the missing values which is a classical
    method of collaborative filtering [[26](#bib.bib26)]. In recent years MF based
    approaches are introduced into time series imputation fields [[52](#bib.bib52),
    [30](#bib.bib30)]. In general, MF based approaches decompose the data matrix into
    2 low-dimensional matrices in the meantime extracting the features from original
    data. And then they try to reconstruct the original matrix and in this processing,
    missing values are imputed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºçŸ©é˜µåˆ†è§£çš„æ–¹æ³• çŸ©é˜µåˆ†è§£ï¼ˆMFï¼‰ç®—æ³•å°è¯•é€šè¿‡çŸ©é˜µåˆ†è§£å’Œé‡æ„æ¥å¡«è¡¥ç¼ºå¤±å€¼ï¼Œä»¥æ‰¾å‡ºæ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§å¹¶å®Œæˆç¼ºå¤±å€¼çš„å¡«è¡¥ï¼Œè¿™æ˜¯ååŒè¿‡æ»¤çš„ç»å…¸æ–¹æ³•[[26](#bib.bib26)]ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºMFçš„æ–¹æ³•è¢«å¼•å…¥æ—¶é—´åºåˆ—æ’è¡¥é¢†åŸŸ[[52](#bib.bib52),
    [30](#bib.bib30)]ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒMFæ–¹æ³•å°†æ•°æ®çŸ©é˜µåˆ†è§£ä¸º2ä¸ªä½ç»´çŸ©é˜µï¼ŒåŒæ—¶ä»åŸå§‹æ•°æ®ä¸­æå–ç‰¹å¾ã€‚ç„¶åï¼Œå®ƒä»¬å°è¯•é‡æ„åŸå§‹çŸ©é˜µï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­å¡«è¡¥ç¼ºå¤±å€¼ã€‚
- en: Expectation-Maximization based methods Expectation-Maximization (EM) based methods
    have been successfully applied to missing data imputation problems [[32](#bib.bib32),
    [13](#bib.bib13), [14](#bib.bib14)]. EM based methods follow a two-stage strategy
    consisting of the E (Expectation) step and the M (Maximization) step which iteratively
    imputes the missing values with the statistical model parameters and then updates
    the statistical model parameters to maximize the possibility of the distribution
    of the filled data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæœŸæœ›æœ€å¤§åŒ–çš„æ–¹æ³• æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰åŸºäºçš„æ–¹æ³•å·²ç»æˆåŠŸåº”ç”¨äºç¼ºå¤±æ•°æ®æ’è¡¥é—®é¢˜[[32](#bib.bib32), [13](#bib.bib13),
    [14](#bib.bib14)]ã€‚EMåŸºäºçš„æ–¹æ³•éµå¾ªä¸€ä¸ªç”±Eï¼ˆæœŸæœ›ï¼‰æ­¥éª¤å’ŒMï¼ˆæœ€å¤§åŒ–ï¼‰æ­¥éª¤ç»„æˆçš„ä¸¤é˜¶æ®µç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£åœ°ç”¨ç»Ÿè®¡æ¨¡å‹å‚æ•°å¡«è¡¥ç¼ºå¤±å€¼ï¼Œç„¶åæ›´æ–°ç»Ÿè®¡æ¨¡å‹å‚æ•°ï¼Œä»¥æœ€å¤§åŒ–å¡«è¡¥æ•°æ®åˆ†å¸ƒçš„å¯èƒ½æ€§ã€‚
- en: 'Multi-Layer Perceptron based methods Multi-Layer Perceptron (MLP) based methods
    employee MLP, which is also called fully connected networks. MLP tries to predict
    missing value by complete values. It can be divided into 3 parts: input layers,
    hidden layers and output layers. In this approach, by minimizing the loss function,
    the perceptron learns a function to impute missing values by input variables.
    In [[35](#bib.bib35)], MLP is used to predict missing values in neural network-based
    diagnostic systems. And in [[33](#bib.bib33)], MLP is employed to impute Population
    Census.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå¤šå±‚æ„ŸçŸ¥å™¨çš„æ–¹æ³• å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰åŸºäºçš„æ–¹æ³•ä½¿ç”¨MLPï¼Œä¹Ÿç§°ä¸ºå…¨è¿æ¥ç½‘ç»œã€‚MLPè¯•å›¾é€šè¿‡å®Œæ•´å€¼æ¥é¢„æµ‹ç¼ºå¤±å€¼ã€‚å®ƒå¯ä»¥åˆ†ä¸º3éƒ¨åˆ†ï¼šè¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œé€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œæ„ŸçŸ¥å™¨å­¦ä¹ ä¸€ä¸ªå‡½æ•°æ¥æ ¹æ®è¾“å…¥å˜é‡å¡«è¡¥ç¼ºå¤±å€¼ã€‚åœ¨[[35](#bib.bib35)]ä¸­ï¼ŒMLPè¢«ç”¨äºé¢„æµ‹ç¥ç»ç½‘ç»œè¯Šæ–­ç³»ç»Ÿä¸­çš„ç¼ºå¤±å€¼ã€‚åœ¨[[33](#bib.bib33)]ä¸­ï¼ŒMLPè¢«ç”¨äºå¡«è¡¥äººå£æ™®æŸ¥æ•°æ®ã€‚
- en: Recently, deep learning based methods [[7](#bib.bib7), [27](#bib.bib27), [28](#bib.bib28),
    [6](#bib.bib6)] mainly deploy Recurrent Neural Network (RNN), since RNN is capable
    of capturing the time information. In these papers, time information is handled
    separately and attached with more importance. To impute the time series, not only
    RNN is used, they also combine the models like Gated Recurrent Unit (GRU) [[7](#bib.bib7),
    [27](#bib.bib27), [28](#bib.bib28)] to extract the long-term information, Generative
    Adversarial Networks (GAN) [[27](#bib.bib27), [28](#bib.bib28)] to generate the
    imputed values and Bidirectional Recurrent Networks to improve the accuracy [[6](#bib.bib6)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•[[7](#bib.bib7), [27](#bib.bib27), [28](#bib.bib28), [6](#bib.bib6)]ä¸»è¦ä½¿ç”¨é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œå› ä¸ºRNNèƒ½å¤Ÿæ•æ‰æ—¶é—´ä¿¡æ¯ã€‚åœ¨è¿™äº›è®ºæ–‡ä¸­ï¼Œæ—¶é—´ä¿¡æ¯è¢«å•ç‹¬å¤„ç†ï¼Œå¹¶èµ‹äºˆæ›´å¤šçš„é‡è¦æ€§ã€‚ä¸ºäº†å¡«è¡¥æ—¶é—´åºåˆ—ï¼Œä¸ä»…ä½¿ç”¨äº†RNNï¼Œè¿˜ç»“åˆäº†å¦‚é—¨æ§é€’å½’å•å…ƒï¼ˆGRUï¼‰[[7](#bib.bib7),
    [27](#bib.bib27), [28](#bib.bib28)]æ¥æå–é•¿æœŸä¿¡æ¯ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰[[27](#bib.bib27), [28](#bib.bib28)]æ¥ç”Ÿæˆå¡«è¡¥å€¼ï¼Œä»¥åŠåŒå‘é€’å½’ç½‘ç»œæ¥æé«˜å‡†ç¡®æ€§[[6](#bib.bib6)]ã€‚
- en: According to the above classification, due to the length, the methods for time
    series imputation are too many to give a detailed introduction. Since among these
    methods, deep learning based ones are the latest and most powerful, we will discuss
    3 latest deep learning methods for time series imputation, find the connections
    and the differences among them.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä¸Šè¿°åˆ†ç±»ï¼Œç”±äºç¯‡å¹…é™åˆ¶ï¼Œæ—¶é—´åºåˆ—æ’è¡¥æ–¹æ³•ä¼—å¤šï¼Œæ— æ³•è¯¦ç»†ä»‹ç»ã€‚ç”±äºè¿™äº›æ–¹æ³•ä¸­ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æœ€æ–°ä¸”æœ€å¼ºå¤§ï¼Œæˆ‘ä»¬å°†è®¨è®º3ç§æœ€æ–°çš„æ·±åº¦å­¦ä¹ æ—¶é—´åºåˆ—æ’è¡¥æ–¹æ³•ï¼Œæ‰¾å‡ºå®ƒä»¬ä¹‹é—´çš„è”ç³»å’Œå·®å¼‚ã€‚
- en: 3 Preliminary
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 åˆæ­¥æ¦‚è¿°
- en: In this section, we first give our formalization of the imputation tasks. It
    is because when introducing the aforesaid deep learning methods, they formalize
    the imputation tasks with different symbols and formulas. And in our research,
    we review them and explain their methods with uniform definitions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç»™å‡ºæ’è¡¥ä»»åŠ¡çš„å½¢å¼åŒ–å®šä¹‰ã€‚è¿™æ˜¯å› ä¸ºåœ¨å¼•å…¥ä¸Šè¿°æ·±åº¦å­¦ä¹ æ–¹æ³•æ—¶ï¼Œå®ƒä»¬ç”¨ä¸åŒçš„ç¬¦å·å’Œå…¬å¼æ¥å½¢å¼åŒ–æ’è¡¥ä»»åŠ¡ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†è¿™äº›æ–¹æ³•ï¼Œå¹¶ç”¨ç»Ÿä¸€çš„å®šä¹‰æ¥è§£é‡Šå®ƒä»¬ã€‚
- en: Definition 1  (Multivariable Time Series).
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 1ï¼ˆå¤šå˜é‡æ—¶é—´åºåˆ—ï¼‰ã€‚
- en: We first denote a timestamp lists $\mathbf{T}=(t_{0},t_{1},...,t_{n-1})$, and
    the time series $\mathbf{X}=\{\mathbf{x_{t_{0}}},\mathbf{x_{t_{1}}},...,\mathbf{x_{t_{n-1}}}\}^{T}$
    as a sequence of $n$ observations. The $i$-th observation of $\ \mathbf{X}$ is
    $\mathbf{x_{t_{i}}}$, which consists of $d$ attributes $\{x_{t_{i}}^{0},x_{t_{i}}^{1},...,x_{t_{i}}^{d}\}$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆè¡¨ç¤ºä¸€ä¸ªæ—¶é—´æˆ³åˆ—è¡¨ $\mathbf{T}=(t_{0},t_{1},...,t_{n-1})$ï¼Œä»¥åŠæ—¶é—´åºåˆ— $\mathbf{X}=\{\mathbf{x_{t_{0}}},\mathbf{x_{t_{1}}},...,\mathbf{x_{t_{n-1}}}\}^{T}$
    ä½œä¸º $n$ æ¬¡è§‚æµ‹çš„åºåˆ—ã€‚$\ \mathbf{X}$ çš„ç¬¬ $i$ æ¬¡è§‚æµ‹æ˜¯ $\mathbf{x_{t_{i}}}$ï¼Œç”± $d$ ä¸ªå±æ€§ $\{x_{t_{i}}^{0},x_{t_{i}}^{1},...,x_{t_{i}}^{d}\}$
    ç»„æˆã€‚
- en: After defining the multivariable time series, we use mask matrix $\mathbf{M}$
    to denote the missing values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®šä¹‰äº†å¤šå˜é‡æ—¶é—´åºåˆ—ä¹‹åï¼Œæˆ‘ä»¬ä½¿ç”¨æ©ç çŸ©é˜µ $\mathbf{M}$ æ¥è¡¨ç¤ºç¼ºå¤±å€¼ã€‚
- en: Definition 2  (Mask Matrix).
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 2ï¼ˆæ©ç çŸ©é˜µï¼‰ã€‚
- en: Mask Matrix $\mathbf{M}$ represents the missing values in $\mathbf{X}$, i.e.,
    $\mathbf{M}\in\mathbb{R}^{n\times d}$. And each element of $\mathbf{M}$ is defined
    as below
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ©ç çŸ©é˜µ $\mathbf{M}$ è¡¨ç¤º $\mathbf{X}$ ä¸­çš„ç¼ºå¤±å€¼ï¼Œå³ $\mathbf{M}\in\mathbb{R}^{n\times
    d}$ã€‚$\mathbf{M}$ çš„æ¯ä¸ªå…ƒç´ å®šä¹‰å¦‚ä¸‹ã€‚
- en: '|  | $\mathbf{M}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{0}&amp;{\text{ if }x_{t_{i}}^{j}\text{
    is not observed, i.e. }x_{t}^{j}=\text{None}}\\ {1}&amp;{\text{ otherwise }}\end{array}\right.$
    |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{M}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{0}&amp;{\text{ å¦‚æœ }x_{t_{i}}^{j}\text{
    æœªè¢«è§‚å¯Ÿåˆ°ï¼Œå³ }x_{t}^{j}=\text{None}}\\ {1}&amp;{\text{ å¦åˆ™ }}\end{array}\right.$ |  |
    (1) |'
- en: To utilize the time information, the time intervals should be recorded with
    an extra structure. Therefore, we introduce the time lag, a matrix to represent
    the time intervals between two adjacent observed values of $\mathbf{X}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ©ç”¨æ—¶é—´ä¿¡æ¯ï¼Œæ—¶é—´é—´éš”åº”è¯¥ç”¨é¢å¤–çš„ç»“æ„è®°å½•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶é—´æ»åï¼Œä¸€ä¸ªçŸ©é˜µç”¨æ¥è¡¨ç¤º $\mathbf{X}$ ä¸­ä¸¤ä¸ªç›¸é‚»è§‚æµ‹å€¼ä¹‹é—´çš„æ—¶é—´é—´éš”ã€‚
- en: Definition 3  (Time Lag).
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 3ï¼ˆæ—¶é—´æ»åï¼‰ã€‚
- en: We use $\mathbf{\delta}\in\mathbb{R}^{n\times d}$ to record the time lag, and
    we calculate it in an iterative way as follows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ $\mathbf{\delta}\in\mathbb{R}^{n\times d}$ æ¥è®°å½•æ—¶é—´æ»åï¼Œå¹¶ä»¥å¦‚ä¸‹è¿­ä»£æ–¹å¼è®¡ç®—å®ƒã€‚
- en: '|  | <math   alttext="\mathbf{\delta}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{t_{i}-t_{i-1},}&amp;{\mathbf{M}_{t_{i-1}}^{j}=1}\\
    {\mathbf{\delta}_{t_{i-1}}^{j}+t_{i}-t_{i-1},}&amp;{\mathbf{M}_{t_{i-1}}^{j-1}==0\&amp;i>0}\\'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{\delta}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{t_{i}-t_{i-1},}&amp;{\mathbf{M}_{t_{i-1}}^{j}=1}\\
    {\mathbf{\delta}_{t_{i-1}}^{j}+t_{i}-t_{i-1},}&amp;{\mathbf{M}_{t_{i-1}}^{j-1}==0\&amp;i>0}\\'
- en: '{0,}&amp;{i==0}\end{array}\right." display="block"><semantics ><mrow ><msubsup
    ><mi >Î´</mi><msub ><mi >t</mi><mi >i</mi></msub><mi >j</mi></msubsup><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="2.0pt"
    ><mtr ><mtd columnalign="left" ><mrow ><mrow ><msub ><mi >t</mi><mi >i</mi></msub><mo
    >âˆ’</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub></mrow><mo
    >,</mo></mrow></mtd><mtd  columnalign="left" ><mrow ><msubsup ><mi >ğŒ</mi><msub
    ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub><mi >j</mi></msubsup><mo
    >=</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left" ><mrow ><mrow
    ><mrow ><msubsup ><mi >Î´</mi><msub ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn
    >1</mn></mrow></msub><mi >j</mi></msubsup><mo >+</mo><msub ><mi >t</mi><mi >i</mi></msub></mrow><mo
    >âˆ’</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub></mrow><mo
    >,</mo></mrow></mtd><mtd  columnalign="left" ><mrow ><msubsup ><mi >ğŒ</mi><msub
    ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub><mrow ><mi >j</mi><mo
    >âˆ’</mo><mn >1</mn></mrow></msubsup><mo rspace="0em" >=</mo><mo lspace="0em" >=</mo><mn
    >0</mn><mo lspace="0.222em" rspace="0.222em" >&</mo><mi >i</mi><mo >></mo><mn
    >0</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mn >0</mn><mo
    >,</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mi >i</mi><mo rspace="0em"
    >=</mo><mo lspace="0em" >=</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\mathbf{\delta}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{t_{i}-t_{i-1},}&{\mathbf{M}_{t_{i-1}}^{j}=1}\\
    {\mathbf{\delta}_{t_{i-1}}^{j}+t_{i}-t_{i-1},}&{\mathbf{M}_{t_{i-1}}^{j-1}==0\&i>0}\\
    {0,}&{i==0}\end{array}\right.</annotation></semantics></math> |  | (2) |'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '{0,}&amp;{i==0}\end{array}\right." display="block"><semantics ><mrow ><msubsup
    ><mi >Î´</mi><msub ><mi >t</mi><mi >i</mi></msub><mi >j</mi></msubsup><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="2.0pt"
    ><mtr ><mtd columnalign="left" ><mrow ><mrow ><msub ><mi >t</mi><mi >i</mi></msub><mo
    >âˆ’</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub></mrow><mo
    >,</mo></mrow></mtd><mtd  columnalign="left" ><mrow ><msubsup ><mi >ğŒ</mi><msub
    ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub><mi >j</mi></msubsup><mo
    >=</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left" ><mrow ><mrow
    ><mrow ><msubsup ><mi >Î´</mi><msub ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn
    >1</mn></mrow></msub><mi >j</mi></msubsup><mo >+</mo><msub ><mi >t</mi><mi >i</mi></msub></mrow><mo
    >âˆ’</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub></mrow><mo
    >,</mo></mrow></mtd><mtd  columnalign="left" ><mrow ><msubsup ><mi >ğŒ</mi><msub
    ><mi >t</mi><mrow ><mi >i</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub><mrow ><mi >j</mi><mo
    >âˆ’</mo><mn >1</mn></mrow></msubsup><mo rspace="0em" >=</mo><mo lspace="0em" >=</mo><mn
    >0</mn><mo lspace="0.222em" rspace="0.222em" >&</mo><mi >i</mi><mo >></mo><mn
    >0</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mn >0</mn><mo
    >,</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mi >i</mi><mo rspace="0em"
    >=</mo><mo lspace="0em" >=</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\mathbf{\delta}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{t_{i}-t_{i-1},}&{\mathbf{M}_{t_{i-1}}^{j}=1}\\
    {\mathbf{\delta}_{t_{i-1}}^{j}+t_{i}-t_{i-1},}&{\mathbf{M}_{t_{i-1}}^{j-1}==0\&i>0}\\
    {0,}&{i==0}\end{array}\right.</annotation></semantics></math> |  | (2) |'
- en: Example 1.
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ 1ã€‚
- en: We now give an example of the time series $\mathbf{X}$, and corresponding timestamp
    lists $\mathbf{T}$
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç»™å‡ºæ—¶é—´åºåˆ— $\mathbf{X}$ çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œä»¥åŠå¯¹åº”çš„æ—¶é—´æˆ³åˆ—è¡¨ $\mathbf{T}$
- en: '|  | <math   alttext="\mathbf{X}=\left[\begin{array}[]{cccc}{1}&amp;{6}&amp;{\text{
    None }}&amp;{9}\\ {7}&amp;{\text{ None }}&amp;{7}&amp;{\text{ None }}\\'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\mathbf{X}=\left[\begin{array}[]{cccc}{1}&amp;{6}&amp;{\text{
    None }}&amp;{9}\\ {7}&amp;{\text{ None }}&amp;{7}&amp;{\text{ None }}\\'
- en: '{9}&amp;{\text{ None }}&amp;{\text{ None }}&amp;{79}\end{array}\right],\mathbf{T}=\left[\begin{array}[]{c}{0}\\'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '{9}&amp;{\text{ None }}&amp;{\text{ None }}&amp;{79}\end{array}\right],\mathbf{T}=\left[\begin{array}[]{c}{0}\\'
- en: '{5}\\'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '{5}\\'
- en: '{13}\end{array}\right]" display="block"><semantics ><mrow ><mrow  ><mi >ğ—</mi><mo
    >=</mo><mrow  ><mo >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="2.0pt"  ><mtr
    ><mtd  ><mn >1</mn></mtd><mtd ><mn  >6</mn></mtd><mtd ><mtext  >Â NoneÂ </mtext></mtd><mtd
    ><mn  >9</mn></mtd></mtr><mtr ><mtd  ><mn >7</mn></mtd><mtd ><mtext  >Â NoneÂ </mtext></mtd><mtd
    ><mn  >7</mn></mtd><mtd ><mtext  >Â NoneÂ </mtext></mtd></mtr><mtr ><mtd  ><mn >9</mn></mtd><mtd
    ><mtext  >Â NoneÂ </mtext></mtd><mtd ><mtext  >Â NoneÂ </mtext></mtd><mtd ><mn  >79</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><mo >,</mo><mrow ><mi  >ğ“</mi><mo >=</mo><mrow ><mo  >[</mo><mtable
    displaystyle="true" rowspacing="2.0pt"  ><mtr ><mtd  ><mn >0</mn></mtd></mtr><mtr
    ><mtd  ><mn >5</mn></mtd></mtr><mtr ><mtd  ><mn >13</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><ci >ğ—</ci><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><cn type="integer"  >1</cn><cn
    type="integer"  >6</cn><ci ><mtext >Â NoneÂ </mtext></ci><cn type="integer" >9</cn></matrixrow><matrixrow
    ><cn type="integer"  >7</cn><ci ><mtext >Â NoneÂ </mtext></ci><cn type="integer"
    >7</cn><ci  ><mtext >Â NoneÂ </mtext></ci></matrixrow><matrixrow ><cn type="integer"
    >9</cn><ci  ><mtext >Â NoneÂ </mtext></ci><ci ><mtext >Â NoneÂ </mtext></ci><cn type="integer"
    >79</cn></matrixrow></matrix></apply></apply><apply ><ci  >ğ“</ci><apply ><csymbol
    cd="latexml" >delimited-[]</csymbol><matrix ><matrixrow ><cn type="integer"  >0</cn></matrixrow><matrixrow
    ><cn type="integer" >5</cn></matrixrow><matrixrow ><cn type="integer"  >13</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{X}=\left[\begin{array}[]{cccc}{1}&{6}&{\text{
    None }}&{9}\\ {7}&{\text{ None }}&{7}&{\text{ None }}\\ {9}&{\text{ None }}&{\text{
    None }}&{79}\end{array}\right],\mathbf{T}=\left[\begin{array}[]{c}{0}\\ {5}\\
    {13}\end{array}\right]</annotation></semantics></math> |  | (3) |'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: And we can thus compute the mask matrix $\mathbf{M}$ and the time lag $\mathbf{\delta}$.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="{\mathbf{M}}=\left[\begin{array}[]{cccc}{0}&amp;{0}&amp;{1}&amp;{0}\\
    {0}&amp;{1}&amp;{0}&amp;{1}\\'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '{0}&amp;{1}&amp;{1}&amp;{0}\end{array}\right],\delta=\left[\begin{array}[]{cccc}{0}&amp;{0}&amp;{0}&amp;{0}\\'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '{5}&amp;{5}&amp;{5}&amp;{5}\\'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '{8}&amp;{13}&amp;{8}&amp;{13}\end{array}\right]" display="block"><semantics
    ><mrow  ><mrow ><mi  >ğŒ</mi><mo >=</mo><mrow ><mo  >[</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="2.0pt"  ><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd
    ><mn  >1</mn></mtd><mtd ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd
    ><mn  >1</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >1</mn></mtd></mtr><mtr
    ><mtd  ><mn >0</mn></mtd><mtd ><mn  >1</mn></mtd><mtd ><mn  >1</mn></mtd><mtd
    ><mn  >0</mn></mtd></mtr></mtable><mo >]</mo></mrow></mrow><mo >,</mo><mrow ><mi  >Î´</mi><mo
    >=</mo><mrow ><mo  >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="2.0pt"  ><mtr
    ><mtd  ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd
    ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >5</mn></mtd><mtd ><mn  >5</mn></mtd><mtd
    ><mn  >5</mn></mtd><mtd ><mn  >5</mn></mtd></mtr><mtr ><mtd  ><mn >8</mn></mtd><mtd
    ><mn  >13</mn></mtd><mtd ><mn  >8</mn></mtd><mtd ><mn  >13</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><ci >ğŒ</ci><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><cn type="integer"  >0</cn><cn
    type="integer"  >0</cn><cn type="integer"  >1</cn><cn type="integer"  >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn><cn type="integer" >1</cn><cn type="integer" >0</cn><cn
    type="integer" >1</cn></matrixrow><matrixrow ><cn type="integer"  >0</cn><cn type="integer"  >1</cn><cn
    type="integer"  >1</cn><cn type="integer"  >0</cn></matrixrow></matrix></apply></apply><apply
    ><ci >ğ›¿</ci><apply  ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow
    ><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn
    type="integer"  >0</cn></matrixrow><matrixrow ><cn type="integer" >5</cn><cn type="integer"
    >5</cn><cn type="integer" >5</cn><cn type="integer" >5</cn></matrixrow><matrixrow
    ><cn type="integer"  >8</cn><cn type="integer"  >13</cn><cn type="integer"  >8</cn><cn
    type="integer"  >13</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >{\mathbf{M}}=\left[\begin{array}[]{cccc}{0}&{0}&{1}&{0}\\
    {0}&{1}&{0}&{1}\\ {0}&{1}&{1}&{0}\end{array}\right],\delta=\left[\begin{array}[]{cccc}{0}&{0}&{0}&{0}\\
    {5}&{5}&{5}&{5}\\ {8}&{13}&{8}&{13}\end{array}\right]</annotation></semantics></math>
    |  | (4) |'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \(\mathbf{M}=\left[\begin{array}{cccc}{0}&{0}&{1}&{0}\\ {0}&{1}&{0}&{1}\\ {0}&{1}&{1}&{0}\end{array}\right],
    \delta=\left[\begin{array}{cccc}{0}&{0}&{0}&{0}\\ {5}&{5}&{5}&{5}\\ {8}&{13}&{8}&{13}\end{array}\right]\)
- en: 4 Methods
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 æ–¹æ³•
- en: In this section, we will first give an overall review of the relationships among
    the given approaches and comparisons of them and then discuss them individually
    with details. The main deep learning methods we researched for time series imputation
    are GRU-D [[7](#bib.bib7)], GRUI-GAN [[27](#bib.bib27)], EÂ²GAN [[28](#bib.bib28)],
    BRITS [[6](#bib.bib6)] and NAOMI [[25](#bib.bib25)]. All of them are deep learning
    approaches published recently for time series imputation tasks. Among these methods,
    recurrent neural network (RNN) and generative adversarial network (GAN) are main
    architectures that are adopted. The reason is that RNN and its variations (e.g.,
    LSTM, GRU) have been proven powerful in modeling sequence data, while GAN has
    been successfully applied to generation and imputation tasks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†é¦–å…ˆå¯¹ç»™å®šæ–¹æ³•ä¹‹é—´çš„å…³ç³»å’Œæ¯”è¾ƒè¿›è¡Œæ€»ä½“å›é¡¾ï¼Œç„¶åè¯¦ç»†è®¨è®ºå®ƒä»¬ã€‚æˆ‘ä»¬ç ”ç©¶çš„ä¸»è¦æ·±åº¦å­¦ä¹ æ–¹æ³•ç”¨äºæ—¶é—´åºåˆ—æ’è¡¥åŒ…æ‹¬ GRU-D [[7](#bib.bib7)]ã€GRUI-GAN
    [[27](#bib.bib27)]ã€EÂ²GAN [[28](#bib.bib28)]ã€BRITS [[6](#bib.bib6)] å’Œ NAOMI [[25](#bib.bib25)]ã€‚è¿™äº›éƒ½æ˜¯æœ€è¿‘å‘è¡¨çš„ç”¨äºæ—¶é—´åºåˆ—æ’è¡¥ä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ˜¯ä¸»è¦é‡‡ç”¨çš„æ¶æ„ã€‚åŸå› æ˜¯
    RNN åŠå…¶å˜ä½“ï¼ˆä¾‹å¦‚ LSTMã€GRUï¼‰åœ¨å»ºæ¨¡åºåˆ—æ•°æ®æ–¹é¢å·²è¢«è¯æ˜éå¸¸æœ‰æ•ˆï¼Œè€Œ GAN å·²æˆåŠŸåº”ç”¨äºç”Ÿæˆå’Œæ’è¡¥ä»»åŠ¡ã€‚
- en: 'To describe the relationships among these methods, we illustrate the dependencies
    and common structures of them in FigureÂ [1](#S4.F1 "Figure 1 â€£ 4 Methods â€£ Time
    Series Data Imputation: A Survey on Deep Learning Approaches"). In FigureÂ [1](#S4.F1
    "Figure 1 â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep Learning
    Approaches"), we use arrows to describe the dependencies, for example GRUI-GAN
    improves the work by using GAN while EÂ²GAN is the updated version of GRUI-GAN.
    And we use boxes to describe the common structures among the methods, for example
    GRU-D and BRITS are both pure RNN models and BRITS and NAOMI both adopt bidirectional
    RNN structures. This can help us to understand how the time series imputation
    task is systematically modeled, how the solutions are developed and what progress
    people make in this process. In the following sections, we will take a progressive
    order to review them.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æè¿°è¿™äº›æ–¹æ³•ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬åœ¨å›¾[1](#S4.F1 "å›¾ 1 â€£ 4 æ–¹æ³• â€£ æ—¶é—´åºåˆ—æ•°æ®æ’è¡¥ï¼šæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç»¼è¿°")ä¸­è¯´æ˜äº†å®ƒä»¬çš„ä¾èµ–å…³ç³»å’Œå…±é€šç»“æ„ã€‚åœ¨å›¾[1](#S4.F1
    "å›¾ 1 â€£ 4 æ–¹æ³• â€£ æ—¶é—´åºåˆ—æ•°æ®æ’è¡¥ï¼šæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç»¼è¿°")ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®­å¤´æ¥æè¿°ä¾èµ–å…³ç³»ï¼Œä¾‹å¦‚ GRUI-GAN é€šè¿‡ä½¿ç”¨ GAN æ”¹è¿›äº†å·¥ä½œï¼Œè€Œ
    EÂ²GAN æ˜¯ GRUI-GAN çš„æ›´æ–°ç‰ˆæœ¬ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æ¡†æ¥æè¿°æ–¹æ³•ä¹‹é—´çš„å…±é€šç»“æ„ï¼Œä¾‹å¦‚ GRU-D å’Œ BRITS éƒ½æ˜¯çº¯ RNN æ¨¡å‹ï¼ŒBRITS å’Œ
    NAOMI éƒ½é‡‡ç”¨åŒå‘ RNN ç»“æ„ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬ç†è§£æ—¶é—´åºåˆ—æ’è¡¥ä»»åŠ¡å¦‚ä½•ç³»ç»Ÿåœ°å»ºæ¨¡ï¼Œè§£å†³æ–¹æ¡ˆæ˜¯å¦‚ä½•å¼€å‘çš„ï¼Œä»¥åŠäººä»¬åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å–å¾—äº†ä»€ä¹ˆè¿›å±•ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æŒ‰æ¸è¿›çš„é¡ºåºè¿›è¡Œå®¡æŸ¥ã€‚
- en: '![Refer to caption](img/fecac3d9d86e0edc2d7899848b8038e9.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/fecac3d9d86e0edc2d7899848b8038e9.png)'
- en: 'Figure 1: The relationships among methods we mainly surveyed.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šæˆ‘ä»¬ä¸»è¦è°ƒæŸ¥çš„æ–¹æ³•ä¹‹é—´çš„å…³ç³»ã€‚
- en: 'Table 2: Characteristics of the chosen methods'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼ 2ï¼šæ‰€é€‰æ–¹æ³•çš„ç‰¹å¾
- en: '| Methodologies | Model Prototype | Specific Models |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³•è®º | æ¨¡å‹åŸå‹ | å…·ä½“æ¨¡å‹ |'
- en: '&#124; Auto-Encoder &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; è‡ªç¼–ç å™¨ &#124;'
- en: '&#124; Enhanced &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å¢å¼º &#124;'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Adversarial Training &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å¯¹æŠ—è®­ç»ƒ &#124;'
- en: '&#124; Enhancednced &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å¢å¼º &#124;'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Bidirectional &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; åŒå‘ &#124;'
- en: '&#124; Enhanced &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å¢å¼º &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GRU-D | RNN | GRU | â€“ | â€“ | â€“ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| GRU-D | RNN | GRU | â€“ | â€“ | â€“ |'
- en: '| GRUI-GAN | Hybrid | GRU+GAN | â€“ | yes | â€“ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GRUI-GAN | æ··åˆ | GRU+GAN | â€“ | æ˜¯ | â€“ |'
- en: '| E2GAN | Hybrid | GRU+GAN | yes | yes | â€“ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| E2GAN | æ··åˆ | GRU+GAN | æ˜¯ | æ˜¯ | â€“ |'
- en: '| BRITS | RNN | Bidirectional RNN | â€“ | â€“ | yes |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| BRITS | RNN | åŒå‘ RNN | â€“ | â€“ | æ˜¯ |'
- en: '| NAOMI | Hybrid | RNN+GAN | â€“ | yes | yes |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| NAOMI | æ··åˆ | RNN+GAN | â€“ | æ˜¯ | æ˜¯ |'
- en: 4.1 Characteristics of Chosen Methods
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 æ‰€é€‰æ–¹æ³•çš„ç‰¹å¾
- en: 'In this section, we give the characteristics of the chosen methods in TableÂ [2](#S4.T2
    "Table 2 â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep Learning
    Approaches") to give a brief introduction and a taxonomy of the chosen methods
    we reviewed. We consider the following criteria:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æä¾›äº†è¡¨æ ¼[2](#S4.T2 "è¡¨æ ¼ 2 â€£ 4 æ–¹æ³• â€£ æ—¶é—´åºåˆ—æ•°æ®æ’è¡¥ï¼šæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç»¼è¿°")ä¸­æ‰€é€‰æ–¹æ³•çš„ç‰¹å¾ï¼Œä»¥ç®€è¦ä»‹ç»å’Œåˆ†ç±»æˆ‘ä»¬å®¡æŸ¥çš„æ‰€é€‰æ–¹æ³•ã€‚æˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹æ ‡å‡†ï¼š
- en: â€¢
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*Irregular Time Series Awareness*: time series including regular time series
    with fixed time interval and irregular time series. Both of them are common kinds
    which are important for classifying the using condition of the methodsÂ [[54](#bib.bib54),
    [44](#bib.bib44)].'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ä¸è§„åˆ™æ—¶é—´åºåˆ—æ„è¯†*ï¼šæ—¶é—´åºåˆ—åŒ…æ‹¬å…·æœ‰å›ºå®šæ—¶é—´é—´éš”çš„è§„åˆ™æ—¶é—´åºåˆ—å’Œä¸è§„åˆ™æ—¶é—´åºåˆ—ã€‚è¿™ä¸¤ç§éƒ½æ˜¯å¸¸è§çš„ç±»å‹ï¼Œå¯¹äºåˆ†ç±»æ–¹æ³•çš„ä½¿ç”¨æ¡ä»¶éå¸¸é‡è¦[[54](#bib.bib54),
    [44](#bib.bib44)]ã€‚'
- en: â€¢
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*Model Prototype*: model prototype concludes the overall kind of model in the
    methods, e.g., RNN, GAN and CNN. It is a basic information to classify the model
    type. If the model prototype is hybrid, it means more than 1 kind of prototype
    is employed.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*æ¨¡å‹åŸå‹*ï¼šæ¨¡å‹åŸå‹æ€»ç»“äº†æ–¹æ³•ä¸­çš„æ•´ä½“æ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚RNNã€GANå’ŒCNNã€‚è¿™æ˜¯å¯¹æ¨¡å‹ç±»å‹è¿›è¡Œåˆ†ç±»çš„åŸºæœ¬ä¿¡æ¯ã€‚å¦‚æœæ¨¡å‹åŸå‹æ˜¯æ··åˆçš„ï¼Œåˆ™æ„å‘³ç€ä½¿ç”¨äº†å¤šç§åŸå‹ã€‚'
- en: â€¢
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*Specific Models*: specific models introduce the specific kinds of model adopted
    in the methods. The specific models may relate to the basic idea of the methods.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*å…·ä½“æ¨¡å‹*ï¼šå…·ä½“æ¨¡å‹ä»‹ç»äº†æ–¹æ³•ä¸­é‡‡ç”¨çš„å…·ä½“æ¨¡å‹ç±»å‹ã€‚è¿™äº›å…·ä½“æ¨¡å‹å¯èƒ½ä¸æ–¹æ³•çš„åŸºæœ¬æ€æƒ³æœ‰å…³ã€‚'
- en: â€¢
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*Auto-Encoder Enhanced*: auto-encoder structure is an approach that can be
    applied in the imputation of the data. With the structure of encoder and decoder,
    it extracts the features from low-dimensional layers and recovery missing values
    by decoder. Therefore, it can serve as a feature of methods.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*è‡ªåŠ¨ç¼–ç å™¨å¢å¼º*ï¼šè‡ªåŠ¨ç¼–ç å™¨ç»“æ„æ˜¯ä¸€ç§å¯ä»¥åº”ç”¨äºæ•°æ®å¡«è¡¥çš„æ–¹æ³•ã€‚é€šè¿‡ç¼–ç å™¨å’Œè§£ç å™¨çš„ç»“æ„ï¼Œå®ƒä»ä½ç»´å±‚ä¸­æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡è§£ç å™¨æ¢å¤ç¼ºå¤±å€¼ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥ä½œä¸ºæ–¹æ³•çš„ä¸€ç§ç‰¹å¾ã€‚'
- en: â€¢
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*Adversarial Training Enhanced*: adversarial training adopts adversarial structure
    (e.g., GAN [[15](#bib.bib15)] and CGAN [[31](#bib.bib31)]) to enhance the model.
    It takes the idea of generative adversarial structure with generator and discriminator.
    Large amount of models can be enhanced with such idea.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*å¯¹æŠ—è®­ç»ƒå¢å¼º*ï¼šå¯¹æŠ—è®­ç»ƒé‡‡ç”¨å¯¹æŠ—ç»“æ„ï¼ˆä¾‹å¦‚ï¼ŒGAN [[15](#bib.bib15)] å’Œ CGAN [[31](#bib.bib31)]ï¼‰æ¥å¢å¼ºæ¨¡å‹ã€‚å®ƒé‡‡ç”¨ç”Ÿæˆå¯¹æŠ—ç»“æ„çš„æ€æƒ³ï¼ŒåŒ…æ‹¬ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ã€‚å¤§é‡æ¨¡å‹å¯ä»¥é€šè¿‡è¿™ç§æ€æƒ³å¾—åˆ°å¢å¼ºã€‚'
- en: â€¢
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*Bidirectional Enhanced*: Bidirectional RNN trains 2 models in forward direction
    and backward direction respectively with RNN and then combines them into the same
    loss function [[17](#bib.bib17)]. This idea is vital in data imputation tasks
    since both previous series and future series of missing values are known. Therefore,
    bidirectional structure benefits from both backward and forward training processing.
    Such idea is adopted in [[25](#bib.bib25), [6](#bib.bib6)].'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*åŒå‘å¢å¼º*ï¼šåŒå‘RNNåˆ†åˆ«åœ¨å‰å‘å’Œåå‘æ–¹å‘è®­ç»ƒä¸¤ä¸ªæ¨¡å‹ï¼Œç„¶åå°†å®ƒä»¬ç»“åˆåˆ°ç›¸åŒçš„æŸå¤±å‡½æ•°ä¸­[[17](#bib.bib17)]ã€‚è¿™ä¸ªæƒ³æ³•åœ¨æ•°æ®å¡«è¡¥ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºç¼ºå¤±å€¼çš„å‰åºç³»åˆ—å’Œååºç³»åˆ—éƒ½æ˜¯å·²çŸ¥çš„ã€‚å› æ­¤ï¼ŒåŒå‘ç»“æ„å—ç›Šäºå‰å‘å’Œåå‘è®­ç»ƒè¿‡ç¨‹ã€‚è¿™ç§æ€æƒ³è¢«é‡‡ç”¨åœ¨[[25](#bib.bib25)ã€[6](#bib.bib6)]ä¸­ã€‚'
- en: 4.2 GRU-D
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 GRU-D
- en: GRU-D is proposed by [[7](#bib.bib7)] as one of the early attempts to impute
    time series with deep learning models. It is the first one among the 5 researched
    paper to systematically model missing patterns into RNN for time series classification
    problems. It is also the first research to exploit that, RNN can model multivariable
    time series with the informativeness from the time series. Former works like [[23](#bib.bib23),
    [8](#bib.bib8)] attempted to impute missing values with RNN by concatenating timestamps
    and raw data, i.e., regard timestamps as one attribute of raw data. But in [[7](#bib.bib7)],
    the concept time lag is first proposed. In this paper, Gated Recurrent Unit (GRU)
    is first adopted to generate missing values. In each layer of GRU, since the input
    can contain missing values, they replace the input $x_{t_{i}}^{j}$ with a combination
    of the existing values $x_{t_{i}}^{j}$ and statistical values, element-wise multiplied
    with $\mathbf{M}$ and $\mathbf{1}-\mathbf{M}$ respectively.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GRU-D æ˜¯ç”± [[7](#bib.bib7)] æå‡ºçš„ï¼Œä½œä¸ºç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å¡«è¡¥æ—¶é—´åºåˆ—çš„æ—©æœŸå°è¯•ä¹‹ä¸€ã€‚å®ƒæ˜¯åœ¨5ç¯‡ç ”ç©¶è®ºæ–‡ä¸­é¦–ä¸ªç³»ç»Ÿåœ°å°†ç¼ºå¤±æ¨¡å¼å»ºæ¨¡ä¸ºRNNç”¨äºæ—¶é—´åºåˆ—åˆ†ç±»é—®é¢˜çš„ç ”ç©¶ã€‚å®ƒä¹Ÿæ˜¯é¦–ä¸ªåˆ©ç”¨RNNæ ¹æ®æ—¶é—´åºåˆ—çš„ä¿¡æ¯å»ºæ¨¡å¤šå˜é‡æ—¶é—´åºåˆ—çš„ç ”ç©¶ã€‚ä»¥å‰çš„å·¥ä½œå¦‚
    [[23](#bib.bib23)ã€[8](#bib.bib8)] å°è¯•é€šè¿‡å°†æ—¶é—´æˆ³å’ŒåŸå§‹æ•°æ®æ‹¼æ¥æ¥ç”¨RNNå¡«è¡¥ç¼ºå¤±å€¼ï¼Œå³å°†æ—¶é—´æˆ³è§†ä¸ºåŸå§‹æ•°æ®çš„ä¸€ä¸ªå±æ€§ã€‚ä½†åœ¨
    [[7](#bib.bib7)] ä¸­ï¼Œé¦–æ¬¡æå‡ºäº†æ—¶é—´æ»åæ¦‚å¿µã€‚æœ¬æ–‡é¦–æ¬¡é‡‡ç”¨é—¨æ§é€’å½’å•å…ƒï¼ˆGRUï¼‰æ¥ç”Ÿæˆç¼ºå¤±å€¼ã€‚åœ¨æ¯ä¸€å±‚GRUä¸­ï¼Œç”±äºè¾“å…¥å¯èƒ½åŒ…å«ç¼ºå¤±å€¼ï¼Œä»–ä»¬ç”¨ç°æœ‰å€¼
    $x_{t_{i}}^{j}$ å’Œç»Ÿè®¡å€¼çš„ç»„åˆæ›¿æ¢è¾“å…¥ $x_{t_{i}}^{j}$ï¼Œåˆ†åˆ«ä¸ $\mathbf{M}$ å’Œ $\mathbf{1}-\mathbf{M}$
    é€å…ƒç´ ç›¸ä¹˜ã€‚
- en: '|  | $x_{t_{i}}^{j}\leftarrow m_{t_{i}}^{j}x_{t_{i}}^{j}+\left(1-m_{t_{i}}^{j}\right)\tilde{x}^{j}$
    |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t_{i}}^{j}\leftarrow m_{t_{i}}^{j}x_{t_{i}}^{j}+\left(1-m_{t_{i}}^{j}\right)\tilde{x}^{j}$
    |  |'
- en: where $\tilde{x}$ can be one of the mean value, last observed value or concatenation
    of $\left[\mathbf{x_{i}};\mathbf{m_{i}};\delta_{i}\right]$.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\tilde{x}$ å¯ä»¥æ˜¯å‡å€¼ã€æœ€åè§‚å¯Ÿå€¼æˆ– $\left[\mathbf{x_{i}};\mathbf{m_{i}};\delta_{i}\right]$
    çš„æ‹¼æ¥ã€‚
- en: The main contribution of this paper is the GRU based model GRU-D and the proposition
    of decay rate. To address the imputation of the missing values, they discover
    that
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯åŸºäºGRUçš„æ¨¡å‹GRU-Då’Œè¡°å‡ç‡çš„æå‡ºã€‚ä¸ºäº†å¤„ç†ç¼ºå¤±å€¼çš„å¡«è¡¥ï¼Œä»–ä»¬å‘ç°
- en: â€¢
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: The missing variables tend to be close to some default value if its last observation
    happens a long time ago.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœç¼ºå¤±å˜é‡çš„æœ€åè§‚å¯Ÿå‘ç”Ÿåœ¨å¾ˆä¹…ä»¥å‰ï¼Œé‚£ä¹ˆè¿™äº›ç¼ºå¤±å˜é‡å¾€å¾€æ¥è¿‘æŸä¸ªé»˜è®¤å€¼ã€‚
- en: â€¢
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: The influence of the input variables will fade away over time if the variable
    has been missing for a while.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœå˜é‡ç¼ºå¤±äº†ä¸€æ®µæ—¶é—´ï¼Œé‚£ä¹ˆè¾“å…¥å˜é‡çš„å½±å“ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œé€æ¸æ¶ˆå¤±ã€‚
- en: And then they propose decay rate $\gamma$, which is defined as below
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä»–ä»¬æå‡ºäº†è¡°å‡ç‡$\gamma$ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š
- en: '|  | $\gamma_{t_{i}}=\exp({-\max{(\mathbf{0},\mathbf{W}_{\gamma}\mathbf{\delta_{t_{i}}})}})$
    |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\gamma_{t_{i}}=\exp({-\max{(\mathbf{0},\mathbf{W}_{\gamma}\mathbf{\delta_{t_{i}}})}})$
    |  |'
- en: The decay rate tries to model the impact of the other values have on the missing
    values. In brief, it guarantees that the larger the time intervals are, the less
    their influence on imputing the missing values. And then they replace the input
    variable as
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¡°å‡ç‡å°è¯•å»ºæ¨¡å…¶ä»–å€¼å¯¹ç¼ºå¤±å€¼çš„å½±å“ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå®ƒä¿è¯äº†æ—¶é—´é—´éš”è¶Šé•¿ï¼Œå®ƒä»¬å¯¹ç¼ºå¤±å€¼çš„å½±å“å°±è¶Šå°ã€‚ç„¶åä»–ä»¬å°†è¾“å…¥å˜é‡æ›¿æ¢ä¸ºï¼š
- en: '|  | $x_{t_{i}}^{j}\leftarrow m_{t_{i}}^{j}x_{t_{j}}^{j}+\left(1-m_{t_{i}}^{j}\right)\gamma_{\boldsymbol{x}_{t_{i}}}^{j}x_{t_{i}^{\prime}}^{j}+\left(1-m_{t_{i}}^{j}\right)\left(1-\gamma_{\boldsymbol{x}_{t_{i}}}^{j}\right)\tilde{x}^{j}$
    |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t_{i}}^{j}\leftarrow m_{t_{i}}^{j}x_{t_{j}}^{j}+\left(1-m_{t_{i}}^{j}\right)\gamma_{\boldsymbol{x}_{t_{i}}}^{j}x_{t_{i}^{\prime}}^{j}+\left(1-m_{t_{i}}^{j}\right)\left(1-\gamma_{\boldsymbol{x}_{t_{i}}}^{j}\right)\tilde{x}^{j}$
    |  |'
- en: 'Therefore, as illustrated in FigureÂ [2](#S4.F2 "Figure 2 â€£ 4.2 GRU-D â€£ 4 Methods
    â€£ Time Series Data Imputation: A Survey on Deep Learning Approaches"), the GRU-D
    model is proposed with 2 different trainable decays $\gamma_{\boldsymbol{x}}$
    and $\gamma_{\boldsymbol{h}}$, where $\gamma_{\boldsymbol{x}}$ is the input decay
    rate and the $\gamma_{\boldsymbol{h}}$ is the decay rate for the hidden state.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'å› æ­¤ï¼Œå¦‚å›¾[2](#S4.F2 "Figure 2 â€£ 4.2 GRU-D â€£ 4 Methods â€£ Time Series Data Imputation:
    A Survey on Deep Learning Approaches")æ‰€ç¤ºï¼ŒGRU-Dæ¨¡å‹æå‡ºäº†ä¸¤ä¸ªä¸åŒçš„å¯è®­ç»ƒè¡°å‡å› å­$\gamma_{\boldsymbol{x}}$å’Œ$\gamma_{\boldsymbol{h}}$ï¼Œå…¶ä¸­$\gamma_{\boldsymbol{x}}$æ˜¯è¾“å…¥è¡°å‡ç‡ï¼Œ$\gamma_{\boldsymbol{h}}$æ˜¯éšè—çŠ¶æ€çš„è¡°å‡ç‡ã€‚'
- en: '![Refer to caption](img/3c61c7aaf1f07b04f443ed06054d0b08.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§å›¾ä¾‹](img/3c61c7aaf1f07b04f443ed06054d0b08.png)'
- en: (a) GRU
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GRU
- en: '![Refer to caption](img/ff1741a5be6147658fecc52be8eb3f17.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§å›¾ä¾‹](img/ff1741a5be6147658fecc52be8eb3f17.png)'
- en: (b) GRU-D
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GRU-D
- en: 'Figure 2: Model of GRU and GRU-D. Images extracted from [[7](#bib.bib7)].'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šGRUå’ŒGRU-Dæ¨¡å‹ã€‚å›¾åƒæ‘˜è‡ª[[7](#bib.bib7)]ã€‚
- en: 4.3 GRUI-GAN
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 GRUI-GAN
- en: 'In [[27](#bib.bib27)], GRU-I is proposed as the recurrent unit to capture the
    time information. As FigureÂ [3](#S4.F3 "Figure 3 â€£ 4.3 GRUI-GAN â€£ 4 Methods â€£
    Time Series Data Imputation: A Survey on Deep Learning Approaches") illustrates,
    it follows the structure of GRU-D in SectionÂ [4.2](#S4.SS2 "4.2 GRU-D â€£ 4 Methods
    â€£ Time Series Data Imputation: A Survey on Deep Learning Approaches") with the
    removal of the input decay. Therefore, there is no innovation in the RNN part
    as well as the decay rate.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨[[27](#bib.bib27)]ä¸­ï¼ŒGRU-Iè¢«æå‡ºä½œä¸ºæ•æ‰æ—¶é—´ä¿¡æ¯çš„é€’å½’å•å…ƒã€‚å¦‚å›¾[3](#S4.F3 "Figure 3 â€£ 4.3 GRUI-GAN
    â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep Learning Approaches")æ‰€ç¤ºï¼Œå®ƒéµå¾ªäº†ç¬¬[4.2](#S4.SS2
    "4.2 GRU-D â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep Learning
    Approaches")èŠ‚ä¸­GRU-Dçš„ç»“æ„ï¼Œä½†ç§»é™¤äº†è¾“å…¥è¡°å‡ã€‚å› æ­¤ï¼ŒRNNéƒ¨åˆ†åŠè¡°å‡ç‡æ²¡æœ‰åˆ›æ–°ã€‚'
- en: 'The main contribution of this paper locates in the GAN structure. FigureÂ [4](#S4.F4
    "Figure 4 â€£ 4.3 GRUI-GAN â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on
    Deep Learning Approaches") shows the structure. The Generative Adversarial Network
    (GAN) structure is made up of a generator (G) and a discriminator (D). The G learns
    a mapping $G(z)$ that tries to map the random noise vector $z$ to realistic time
    series. The D tries to find a mapping $D(.)$ that tells us the input dataâ€™s probability
    of being real. Therefore, in this paper, the model takes a random noise as the
    input of the GAN model, which means the generating is a random process. Both G
    and D are based on GRU-I, and it takes lots of time to train the model to get
    the data imputed.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åœ¨äºGANç»“æ„ã€‚å›¾[4](#S4.F4 "Figure 4 â€£ 4.3 GRUI-GAN â€£ 4 Methods â€£ Time Series
    Data Imputation: A Survey on Deep Learning Approaches")å±•ç¤ºäº†è¯¥ç»“æ„ã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç»“æ„ç”±ç”Ÿæˆå™¨ï¼ˆGï¼‰å’Œé‰´åˆ«å™¨ï¼ˆDï¼‰ç»„æˆã€‚Gå­¦ä¹ ä¸€ä¸ªæ˜ å°„$G(z)$ï¼Œè¯•å›¾å°†éšæœºå™ªå£°å‘é‡$z$æ˜ å°„åˆ°çœŸå®çš„æ—¶é—´åºåˆ—ã€‚Dè¯•å›¾æ‰¾åˆ°ä¸€ä¸ªæ˜ å°„$D(.)$ï¼Œå‘Šè¯‰æˆ‘ä»¬è¾“å…¥æ•°æ®çš„çœŸå®æ€§æ¦‚ç‡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ¨¡å‹å°†éšæœºå™ªå£°ä½œä¸ºGANæ¨¡å‹çš„è¾“å…¥ï¼Œè¿™æ„å‘³ç€ç”Ÿæˆæ˜¯ä¸€ä¸ªéšæœºè¿‡ç¨‹ã€‚Gå’ŒDéƒ½åŸºäºGRU-Iï¼Œå¹¶ä¸”è®­ç»ƒæ¨¡å‹ä»¥è·å¾—æ•°æ®å¡«è¡¥éœ€è¦å¤§é‡æ—¶é—´ã€‚'
- en: The GRUI-GAN takes advantage of the ability of GAN in imputation, which has
    been proven powerful in image imputation such as [[34](#bib.bib34)]. And the adversarial
    structure improves accuracy. Moreover, the paper adopts a WGAN structure, which
    improves the stability of the learning stage, get out of the problem of mode collapse
    and makes it easy for the optimization of the GAN model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: GRUI-GAN åˆ©ç”¨äº† GAN åœ¨æ’è¡¥æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿™åœ¨å›¾åƒæ’è¡¥ä¸­å·²è¢«è¯æ˜æœ‰æ•ˆï¼Œå¦‚ [[34](#bib.bib34)]ã€‚å¯¹æŠ—ç»“æ„æé«˜äº†å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡é‡‡ç”¨äº†
    WGAN ç»“æ„ï¼Œæ”¹å–„äº†å­¦ä¹ é˜¶æ®µçš„ç¨³å®šæ€§ï¼Œè§£å†³äº†æ¨¡å¼å´©æºƒçš„é—®é¢˜ï¼Œå¹¶ç®€åŒ–äº† GAN æ¨¡å‹çš„ä¼˜åŒ–ã€‚
- en: However, this model is not practical since the accuracy of the generative model
    seems not stable with a random noise input. And it also makes the model hard to
    converge.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ä¸ªæ¨¡å‹å¹¶ä¸å®ç”¨ï¼Œå› ä¸ºç”Ÿæˆæ¨¡å‹çš„å‡†ç¡®æ€§åœ¨éšæœºå™ªå£°è¾“å…¥ä¸‹ä¼¼ä¹ä¸ç¨³å®šï¼Œè€Œä¸”ä½¿å¾—æ¨¡å‹éš¾ä»¥æ”¶æ•›ã€‚
- en: '![Refer to caption](img/53eb5aed6bc610fff50df774918704cb.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/53eb5aed6bc610fff50df774918704cb.png)'
- en: (a) GRU
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GRU
- en: '![Refer to caption](img/ff81fe650de8f326e56d7bb532b315e6.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/ff81fe650de8f326e56d7bb532b315e6.png)'
- en: (b) GRU-I
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GRU-I
- en: 'Figure 3: Model of GRU and GRU-I. Images extracted from [[27](#bib.bib27)].'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šGRU å’Œ GRU-I çš„æ¨¡å‹ã€‚å›¾åƒæ‘˜è‡ª [[27](#bib.bib27)]ã€‚
- en: '![Refer to caption](img/9bd2a0bb0203767ee55d8adcb0938bb9.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/9bd2a0bb0203767ee55d8adcb0938bb9.png)'
- en: 'Figure 4: The structure of the GRUI-GAN. Image extracted from [[27](#bib.bib27)].'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šGRUI-GAN çš„ç»“æ„ã€‚å›¾åƒæ‘˜è‡ª [[27](#bib.bib27)]ã€‚
- en: '![Refer to caption](img/56f0b4658042ad96fadb2b36a1738342.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/56f0b4658042ad96fadb2b36a1738342.png)'
- en: 'Figure 5: The structure of the BRITS. Image extracted from [[6](#bib.bib6)].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šBRITS çš„ç»“æ„ã€‚å›¾åƒæ‘˜è‡ª [[6](#bib.bib6)]ã€‚
- en: 4.4 BRITS
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 BRITS
- en: 'Unlike former methods, BRITS [[6](#bib.bib6)] is totally based on RNN structure
    and proposes imputation with unidirectional dynamics. Time lag (corresponding
    to â€time gapsâ€ in [[6](#bib.bib6)]) is also employed since the time series may
    be irregular. Similar to the idea of decay rate $\gamma$ from GRU-D introduced
    in SectionÂ [4.2](#S4.SS2 "4.2 GRU-D â€£ 4 Methods â€£ Time Series Data Imputation:
    A Survey on Deep Learning Approaches"), they propose temporal decay factor $\gamma_{t}=\exp{(-max\left(0,\mathbf{W}_{\gamma}\delta_{t}+\mathbf{b}_{\gamma}\right))}$.
    Compared to GRU-D where the time lags are considered in input and serve as the
    decay rate, in BRITS the hidden states update with the decay rate $\gamma$. It
    means when updating the hidden state, the old hidden state decays according to
    the time duration recorded in the time lags. Hence, the model is updated by:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒBRITS [[6](#bib.bib6)] å®Œå…¨åŸºäº RNN ç»“æ„ï¼Œå¹¶æå‡ºäº†å…·æœ‰å•å‘åŠ¨æ€çš„æ’è¡¥ã€‚ç”±äºæ—¶é—´åºåˆ—å¯èƒ½æ˜¯ä¸è§„åˆ™çš„ï¼Œè¿˜é‡‡ç”¨äº†æ—¶é—´æ»åï¼ˆå¯¹åº”äº
    [[6](#bib.bib6)] ä¸­çš„â€œæ—¶é—´é—´éš™â€ï¼‰ã€‚ç±»ä¼¼äºç¬¬ [4.2](#S4.SS2 "4.2 GRU-D â€£ 4 Methods â€£ Time Series
    Data Imputation: A Survey on Deep Learning Approaches") èŠ‚ä¸­ä»‹ç»çš„ GRU-D çš„è¡°å‡ç‡ $\gamma$
    æ¦‚å¿µï¼Œä»–ä»¬æå‡ºäº†æ—¶é—´è¡°å‡å› å­ $\gamma_{t}=\exp{(-\max\left(0,\mathbf{W}_{\gamma}\delta_{t}+\mathbf{b}_{\gamma}\right))}$ã€‚ä¸åœ¨
    BRITS ä¸­æ—¶é—´æ»åè¢«è§†ä¸ºè¾“å…¥å¹¶å……å½“è¡°å‡ç‡ä¸åŒï¼ŒGRU-D å°†æ—¶é—´æ»åè€ƒè™‘åœ¨è¾“å…¥ä¸­ã€‚åœ¨ BRITS ä¸­ï¼Œéšè—çŠ¶æ€çš„æ›´æ–°é‡‡ç”¨äº†è¡°å‡ç‡ $\gamma$ã€‚è¿™æ„å‘³ç€åœ¨æ›´æ–°éšè—çŠ¶æ€æ—¶ï¼Œæ—§çš„éšè—çŠ¶æ€æ ¹æ®è®°å½•åœ¨æ—¶é—´æ»åçš„æ—¶é—´æŒç»­æ—¶é—´è¿›è¡Œè¡°å‡ã€‚å› æ­¤ï¼Œæ¨¡å‹çš„æ›´æ–°æ–¹å¼ä¸ºï¼š'
- en: '|  | $\displaystyle\hat{\mathbf{x}}_{t}$ | $\displaystyle=\mathbf{W}_{x}\mathbf{h}_{t-1}+\mathbf{b}_{x}$
    |  | (5) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\mathbf{x}}_{t}$ | $\displaystyle=\mathbf{W}_{x}\mathbf{h}_{t-1}+\mathbf{b}_{x}$
    |  | (5) |'
- en: '|  | $\displaystyle\mathbf{x}_{t}^{c}$ | $\displaystyle=\mathbf{m}_{t}\odot\mathbf{x}_{t}+\left(1-\mathbf{m}_{t}\right)\odot\hat{\mathbf{x}}_{t}$
    |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{x}_{t}^{c}$ | $\displaystyle=\mathbf{m}_{t}\odot\mathbf{x}_{t}+\left(1-\mathbf{m}_{t}\right)\odot\hat{\mathbf{x}}_{t}$
    |  |'
- en: '|  | $\displaystyle\gamma_{t}$ | $\displaystyle=\exp\left\{-\max\left(0,\mathbf{W}_{\gamma}\delta_{t}+\mathbf{b}_{\gamma}\right)\right\}$
    |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\gamma_{t}$ | $\displaystyle=\exp\left\{-\max\left(0,\mathbf{W}_{\gamma}\delta_{t}+\mathbf{b}_{\gamma}\right)\right\}$
    |  |'
- en: '|  | $\displaystyle\mathbf{h}_{t}$ | $\displaystyle=\sigma\left(\mathbf{W}_{h}\left[\mathbf{h}_{t-1}\odot\gamma_{t}\right]+\mathbf{U}_{h}\left[\mathbf{x}_{t}^{c}\circ\mathbf{m}_{t}\right]+\mathbf{b}_{h}\right)$
    |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}_{t}$ | $\displaystyle=\sigma\left(\mathbf{W}_{h}\left[\mathbf{h}_{t-1}\odot\gamma_{t}\right]+\mathbf{U}_{h}\left[\mathbf{x}_{t}^{c}\circ\mathbf{m}_{t}\right]+\mathbf{b}_{h}\right)$
    |  |'
- en: '|  | $\displaystyle\ell_{t}$ | $\displaystyle=\left\langle\mathbf{m}_{t},\mathcal{L}_{e}\left(\mathbf{x}_{t},\hat{\mathbf{x}}_{t}\right)\right\rangle$
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ell_{t}$ | $\displaystyle=\left\langle\mathbf{m}_{t},\mathcal{L}_{e}\left(\mathbf{x}_{t},\hat{\mathbf{x}}_{t}\right)\right\rangle$
    |  |'
- en: The former model named RITS is the unidirectional version of the proposed methods
    in [[6](#bib.bib6)]. As the bidirectional version, BRITS employs bidirectional
    RNN by utilizing the bidirectional recurrent dynamics, i.e., they train 2 models
    in forward direction and backward direction respectively [[17](#bib.bib17)]. Thus
    consistency loss is introduced to take the losses of both directions into consideration.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰åä¸ºRITSçš„æ¨¡å‹æ˜¯[[6](#bib.bib6)]ä¸­æå‡ºçš„æ–¹æ³•çš„å•å‘ç‰ˆæœ¬ã€‚ä½œä¸ºåŒå‘ç‰ˆæœ¬ï¼ŒBRITSåˆ©ç”¨åŒå‘é€’å½’åŠ¨æ€ï¼Œå³åˆ†åˆ«è®­ç»ƒä¸¤ä¸ªå‰å‘å’Œåå‘æ¨¡å‹[[17](#bib.bib17)]ã€‚å› æ­¤ï¼Œå¼•å…¥äº†ä¸€è‡´æ€§æŸå¤±æ¥è€ƒè™‘ä¸¤ä¸ªæ–¹å‘çš„æŸå¤±ã€‚
- en: To conclude, in BRITS, time lags are still adopted to deal with irregular time
    series. Only RNN is used to model the time series. We can also conclude from the
    model and the experiments that bidirectional RNN contributes to a higher performance
    since the unidirectional model may suffer from bias exploding problem [[4](#bib.bib4)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œåœ¨BRITSä¸­ï¼Œæ—¶é—´æ»åä»ç„¶è¢«ç”¨æ¥å¤„ç†ä¸è§„åˆ™æ—¶é—´åºåˆ—ã€‚ä»…ä½¿ç”¨RNNæ¥å»ºæ¨¡æ—¶é—´åºåˆ—ã€‚ä»æ¨¡å‹å’Œå®éªŒä¸­æˆ‘ä»¬è¿˜å¯ä»¥å¾—å‡ºç»“è®ºï¼ŒåŒå‘RNNæœ‰åŠ©äºæé«˜æ€§èƒ½ï¼Œå› ä¸ºå•å‘æ¨¡å‹å¯èƒ½ä¼šé­é‡æ¢¯åº¦çˆ†ç‚¸é—®é¢˜
    [[4](#bib.bib4)]ã€‚
- en: '![Refer to caption](img/e02494da76278d94536185e6a859e02c.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/e02494da76278d94536185e6a859e02c.png)'
- en: 'Figure 6: The structure of the EÂ²GAN. Image extracted from [[28](#bib.bib28)].'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šEÂ²GANçš„ç»“æ„ã€‚å›¾åƒæ‘˜è‡ª[[28](#bib.bib28)]ã€‚
- en: 4.5 EÂ²GAN
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 EÂ²GAN
- en: 'EÂ²GAN [[28](#bib.bib28)] is another work based on GAN. While the GRUI-GAN in
    SectionÂ [4.3](#S4.SS3 "4.3 GRUI-GAN â€£ 4 Methods â€£ Time Series Data Imputation:
    A Survey on Deep Learning Approaches") takes a random noise vector as input, which
    takes lots of time to train, EÂ²GAN adopts an auto-encoder structure based on GRUI
    to form the generator. The overall structure of their model is in FigureÂ [6](#S4.F6
    "Figure 6 â€£ 4.4 BRITS â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep
    Learning Approaches").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'EÂ²GAN [[28](#bib.bib28)] æ˜¯å¦ä¸€ä¸ªåŸºäºGANçš„å·¥ä½œã€‚è™½ç„¶ç¬¬[4.3èŠ‚](#S4.SS3 "4.3 GRUI-GAN â€£ 4 Methods
    â€£ Time Series Data Imputation: A Survey on Deep Learning Approaches")ä¸­çš„GRUI-GANé‡‡ç”¨éšæœºå™ªå£°å‘é‡ä½œä¸ºè¾“å…¥ï¼Œè¿™éœ€è¦å¤§é‡æ—¶é—´è¿›è¡Œè®­ç»ƒï¼Œä½†EÂ²GANé‡‡ç”¨åŸºäºGRUIçš„è‡ªç¼–ç å™¨ç»“æ„æ¥å½¢æˆç”Ÿæˆå™¨ã€‚ä»–ä»¬æ¨¡å‹çš„æ•´ä½“ç»“æ„è§å›¾[6](#S4.F6
    "Figure 6 â€£ 4.4 BRITS â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep
    Learning Approaches")ã€‚'
- en: In EÂ²GAN, concepts including mask, time lag, decay rate and GRUI are all reserved
    without improvement, thus there is no innovation in the GRUI structure. The main
    contribution is the auto-encoder structure they adopt in the generator. This is
    a common strategy taken by image generation and imputation such as Context-Encoder
    [[34](#bib.bib34)], PixelGANs [[19](#bib.bib19)], but not a common strategy in
    RNN based GAN. Since the input of the model is the original time series, the model
    compresses the input incomplete time series $\mathbf{X}$ into a low-dimensional
    vector $z$ with the help of the GRUI. And then the reconstructing part will reconstruct
    the complete time series $\mathbf{X^{\prime}}$ to fool the discriminator. And
    the discriminator of the method attempts to distinguish actual incomplete time
    series $\mathbf{X}$ and the fake but complete sample $\mathbf{X^{\prime}}$ through
    the adoption of recursive neural network. The framework of the discriminator is
    also an encoder.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨EÂ²GANä¸­ï¼Œè¯¸å¦‚æ©ç ã€æ—¶é—´æ»åã€è¡°å‡ç‡å’ŒGRUIç­‰æ¦‚å¿µéƒ½ä¿ç•™äº†ï¼Œæ²¡æœ‰æ”¹è¿›ï¼Œå› æ­¤GRUIç»“æ„æ²¡æœ‰åˆ›æ–°ã€‚ä¸»è¦è´¡çŒ®æ˜¯ä»–ä»¬åœ¨ç”Ÿæˆå™¨ä¸­é‡‡ç”¨çš„è‡ªç¼–ç å™¨ç»“æ„ã€‚è¿™æ˜¯ä¸€ç§å›¾åƒç”Ÿæˆå’Œå¡«è¡¥å¸¸ç”¨çš„ç­–ç•¥ï¼Œå¦‚Context-Encoder
    [[34](#bib.bib34)]ã€PixelGANs [[19](#bib.bib19)]ï¼Œä½†åœ¨åŸºäºRNNçš„GANä¸­å¹¶ä¸å¸¸è§ã€‚ç”±äºæ¨¡å‹çš„è¾“å…¥æ˜¯åŸå§‹æ—¶é—´åºåˆ—ï¼Œæ¨¡å‹é€šè¿‡GRUIå°†è¾“å…¥çš„ä¸å®Œæ•´æ—¶é—´åºåˆ—$\mathbf{X}$å‹ç¼©ä¸ºä¸€ä¸ªä½ç»´å‘é‡$z$ã€‚ç„¶åï¼Œé‡å»ºéƒ¨åˆ†ä¼šé‡å»ºå®Œæ•´æ—¶é—´åºåˆ—$\mathbf{X^{\prime}}$ä»¥æ¬ºéª—åˆ¤åˆ«å™¨ã€‚è¯¥æ–¹æ³•çš„åˆ¤åˆ«å™¨è¯•å›¾é€šè¿‡é‡‡ç”¨é€’å½’ç¥ç»ç½‘ç»œæ¥åŒºåˆ†å®é™…çš„ä¸å®Œæ•´æ—¶é—´åºåˆ—$\mathbf{X}$å’Œå‡ä½†å®Œæ•´çš„æ ·æœ¬$\mathbf{X^{\prime}}$ã€‚åˆ¤åˆ«å™¨çš„æ¡†æ¶ä¹Ÿæ˜¯ä¸€ä¸ªç¼–ç å™¨ã€‚
- en: EÂ²GAN takes an encoder-decoder RNN based structure as the generator, which tackles
    the difficulty of training the model and the accuracy. So far, according to the
    experiments in the paper, EÂ²GAN has achieved state-of-the-art and outperforms
    other existing methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: EÂ²GANé‡‡ç”¨äº†åŸºäºç¼–ç å™¨-è§£ç å™¨RNNçš„ç»“æ„ä½œä¸ºç”Ÿæˆå™¨ï¼Œè¿™è§£å†³äº†è®­ç»ƒæ¨¡å‹çš„éš¾åº¦å’Œå‡†ç¡®æ€§ã€‚ç›®å‰ï¼Œæ ¹æ®è®ºæ–‡ä¸­çš„å®éªŒï¼ŒEÂ²GANå·²ç»è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚
- en: 4.6 NAOMI
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 NAOMI
- en: 'NAOMI (Non-AutOregressive Multiresolution Imputation [[25](#bib.bib25)]) proposes
    a non-autoregressive model which conditions both previous values but also future
    values, i.e., equipped with bidirectional RNN like BRITS introduced in SectionÂ [4.4](#S4.SS4
    "4.4 BRITS â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep Learning
    Approaches"). Since in the imputation tasks, future values and historical values
    are both observed, the intuition is to take advantage of both values and train
    bidirectional models for them. As illustrated in FigureÂ [7](#S4.F7 "Figure 7 â€£
    4.6 NAOMI â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep Learning
    Approaches"), $f_{f}$ and $f_{b}$ are forward and backward RNN respectively, thus
    the hidden state $h_{t}$ is a joint hidden state concatenated by $h^{f}_{t}$ and
    $h^{b}_{t}$.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'NAOMIï¼ˆéè‡ªå›å½’å¤šåˆ†è¾¨ç‡æ’è¡¥[[25](#bib.bib25)]ï¼‰æå‡ºäº†ä¸€ç§éè‡ªå›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä»…æ¡ä»¶åŒ–äºä¹‹å‰çš„å€¼ï¼Œè¿˜æ¡ä»¶åŒ–äºæœªæ¥çš„å€¼ï¼Œå³ï¼Œé…å¤‡äº†åŒå‘RNNï¼Œå¦‚ç¬¬[4.4](#S4.SS4
    "4.4 BRITS â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep Learning
    Approaches")èŠ‚ä¸­ä»‹ç»çš„BRITSã€‚ç”±äºåœ¨æ’è¡¥ä»»åŠ¡ä¸­ï¼Œæœªæ¥å€¼å’Œå†å²å€¼éƒ½è¢«è§‚å¯Ÿåˆ°ï¼Œå› æ­¤ç›´è§‰æ˜¯åˆ©ç”¨è¿™ä¸¤è€…ï¼Œå¹¶ä¸ºå®ƒä»¬è®­ç»ƒåŒå‘æ¨¡å‹ã€‚å¦‚å›¾[7](#S4.F7
    "Figure 7 â€£ 4.6 NAOMI â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep
    Learning Approaches")æ‰€ç¤ºï¼Œ$f_{f}$å’Œ$f_{b}$åˆ†åˆ«æ˜¯å‰å‘å’Œåå‘RNNï¼Œå› æ­¤éšè—çŠ¶æ€$h_{t}$æ˜¯ç”±$h^{f}_{t}$å’Œ$h^{b}_{t}$æ‹¼æ¥è€Œæˆçš„è”åˆéšè—çŠ¶æ€ã€‚'
- en: 'Moreover, a special predicting strategy is performed in this paper. They adopt
    a *divide and conquer strategy*. As it is shown in FigureÂ [7](#S4.F7 "Figure 7
    â€£ 4.6 NAOMI â€£ 4 Methods â€£ Time Series Data Imputation: A Survey on Deep Learning
    Approaches"), with 2 known values $x_{1}$ and $x_{5}$, they first predict the
    midpoint $x_{3}$ by $x_{1}$ and $x_{5}$ with proposed bidirectional RNN models,
    and then $x_{3}$ is updated and utilized to predict $x_{2}$ and $x_{4}$ respectively.
    Thus a fine-grained prediction is performed. Finally, adversarial training is
    taken to enhance the model.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­¤è®ºæ–‡ä¸­æ‰§è¡Œäº†ä¸€ç§ç‰¹æ®Šçš„é¢„æµ‹ç­–ç•¥ã€‚ä»–ä»¬é‡‡ç”¨äº†*åˆ†è€Œæ²»ä¹‹ç­–ç•¥*ã€‚å¦‚å›¾[7](#S4.F7 "Figure 7 â€£ 4.6 NAOMI â€£ 4 Methods
    â€£ Time Series Data Imputation: A Survey on Deep Learning Approaches")æ‰€ç¤ºï¼Œç»™å®šä¸¤ä¸ªå·²çŸ¥å€¼$x_{1}$å’Œ$x_{5}$ï¼Œä»–ä»¬é¦–å…ˆåˆ©ç”¨æå‡ºçš„åŒå‘RNNæ¨¡å‹é¢„æµ‹ä¸­ç‚¹$x_{3}$ï¼Œç„¶åæ›´æ–°$x_{3}$å¹¶åˆ©ç”¨å®ƒåˆ†åˆ«é¢„æµ‹$x_{2}$å’Œ$x_{4}$ã€‚å› æ­¤ï¼Œè¿›è¡Œäº†ç²¾ç»†çš„é¢„æµ‹ã€‚æœ€åï¼Œé‡‡ç”¨å¯¹æŠ—è®­ç»ƒä»¥å¢å¼ºæ¨¡å‹ã€‚'
- en: However, in NAOMI, time gaps are ignored and the data is injected into the RNN
    model without timestamps. It suggests the model is not aware of irregular time
    series although we can still take them as input by removing their timestamps directly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨NAOMIä¸­ï¼Œæ—¶é—´é—´éš”è¢«å¿½ç•¥ï¼Œæ•°æ®è¢«æ³¨å…¥åˆ°æ²¡æœ‰æ—¶é—´æˆ³çš„RNNæ¨¡å‹ä¸­ã€‚è¿™è¡¨æ˜è¯¥æ¨¡å‹è™½ç„¶å¯ä»¥é€šè¿‡ç›´æ¥ç§»é™¤æ—¶é—´æˆ³æ¥å°†å…¶ä½œä¸ºè¾“å…¥ï¼Œä½†å¹¶ä¸æ„è¯†åˆ°ä¸è§„åˆ™æ—¶é—´åºåˆ—ã€‚
- en: '![Refer to caption](img/8b7e6e36cd299753cb0a9c296a54019b.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/8b7e6e36cd299753cb0a9c296a54019b.png)'
- en: 'Figure 7: The structure of the NAOMI. Image extracted from [[25](#bib.bib25)].'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šNAOMIçš„ç»“æ„ã€‚å›¾åƒæ‘˜è‡ª[[25](#bib.bib25)]ã€‚
- en: 5 Conclusion
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ç»“è®º
- en: 'In this paper, we give a brief introduction to the imputation methods for time
    series. We propose that existing methods can be classified into 3 main classed:
    deletion methods, traditional methods, and learning based methods. And we introduce
    our classification in detail. Moreover, we investigate existing deep learning
    methods for time series imputation, since they outperform others and make great
    progress recently. We mainly researched 3 deep learning methods including GRU-D,
    GRUI-GAN, and EÂ²GAN. All of them based on RNN, and the latter two also adopt GAN
    for more accurate imputation. We also find the relationships among them: GRUI-GAN
    is based on the definitions from GRU-D, and EÂ²GAN improves the generator of the
    GRUI-GAN with auto-encoder. And so far, EÂ²GAN achieves state-of-the-art.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç®€è¦ä»‹ç»äº†æ—¶é—´åºåˆ—çš„æ’è¡¥æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºç°æœ‰çš„æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸‰å¤§ç±»ï¼šåˆ é™¤æ–¹æ³•ã€ä¼ ç»Ÿæ–¹æ³•å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ã€‚å¹¶è¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ—¶é—´åºåˆ—æ’è¡¥æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬åœ¨æœ€è¿‘çš„è¿›å±•ä¸­è¡¨ç°ä¼˜è¶Šã€‚æˆ‘ä»¬ä¸»è¦ç ”ç©¶äº†ä¸‰ç§æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬GRU-Dã€GRUI-GANå’ŒEÂ²GANã€‚å®ƒä»¬éƒ½åŸºäºRNNï¼Œåä¸¤è€…è¿˜é‡‡ç”¨äº†GANä»¥å®ç°æ›´å‡†ç¡®çš„æ’è¡¥ã€‚æˆ‘ä»¬è¿˜å‘ç°å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼šGRUI-GANåŸºäºGRU-Dçš„å®šä¹‰ï¼ŒEÂ²GANé€šè¿‡è‡ªç¼–ç å™¨æ”¹è¿›äº†GRUI-GANçš„ç”Ÿæˆå™¨ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒEÂ²GANå®ç°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚
- en: Since the imputation problem is fundamental, we believe with these methods,
    the filled data would benefit downstream applications in many aspects. And as
    we observed, most of the techniques in other fields can be adopted in this task
    since time series data is everywhere. In the future, we would like to see the
    time information can be utilized properly, and the methods can be more general
    and accurate so that we would not need to choose the best one from too many methods,
    and the missing data of the time series would not be a problem.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ’è¡¥é—®é¢˜æ˜¯åŸºç¡€æ€§çš„ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™äº›æ–¹æ³•å¯ä»¥ä½¿å¡«å……æ•°æ®åœ¨è®¸å¤šæ–¹é¢æœ‰åˆ©äºä¸‹æ¸¸åº”ç”¨ã€‚æ­£å¦‚æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„é‚£æ ·ï¼Œå…¶ä»–é¢†åŸŸä¸­çš„å¤§å¤šæ•°æŠ€æœ¯å¯ä»¥è¢«é‡‡ç”¨äºæ­¤ä»»åŠ¡ï¼Œå› ä¸ºæ—¶é—´åºåˆ—æ•°æ®æ— å¤„ä¸åœ¨ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¸Œæœ›çœ‹åˆ°æ—¶é—´ä¿¡æ¯èƒ½å¾—åˆ°å¦¥å–„åˆ©ç”¨ï¼Œæ–¹æ³•èƒ½æ›´åŠ é€šç”¨å’Œå‡†ç¡®ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸éœ€è¦ä»ä¼—å¤šæ–¹æ³•ä¸­é€‰æ‹©æœ€ä½³æ–¹æ³•ï¼Œæ—¶é—´åºåˆ—çš„ç¼ºå¤±æ•°æ®ä¹Ÿä¸ä¼šæˆä¸ºé—®é¢˜ã€‚
- en: 6 Future Research Opportunities
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 æœªæ¥ç ”ç©¶æœºä¼š
- en: Based on our observation from surveying the development of time series imputation
    methodologies, we try to highlight some potential research opportunities in this
    field. Most existing researches mainly focus on the structure of RNN and try to
    use bidirectional RNN, Auto-Encoder structure and GAN to enhance the model. With
    the rapid development in the deep learning society (especially Natural Language
    Processing (NLP) where time series are also highly concerned), some techniques
    have reached better performance (e.g., attention models). These models can be
    considered to enhance the imputation models. Further, most existing methods ignore
    the missing of timestamps which can also appear obscurely [[36](#bib.bib36)].
    Therefore, there is still demand for such techniques. Existing methods can be
    extended to impute missing timestamps. Moreover, query answering without directly
    imputing missing values is another perspective of dealing with missing values.
    Under such scenarios, specific values do not need imputation, and consistent queries
    in inconsistent probabilistic databases should be generated.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæˆ‘ä»¬å¯¹æ—¶é—´åºåˆ—æ’è¡¥æ–¹æ³•å‘å±•æƒ…å†µçš„è§‚å¯Ÿï¼Œæˆ‘ä»¬å°è¯•çªæ˜¾è¯¥é¢†åŸŸçš„ä¸€äº›æ½œåœ¨ç ”ç©¶æœºä¼šã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨RNNçš„ç»“æ„ï¼Œå¹¶å°è¯•ä½¿ç”¨åŒå‘RNNã€Auto-Encoderç»“æ„å’ŒGANæ¥å¢å¼ºæ¨¡å‹ã€‚éšç€æ·±åº¦å­¦ä¹ ç¤¾ä¼šï¼ˆå°¤å…¶æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼Œå…¶ä¸­æ—¶é—´åºåˆ—ä¹Ÿå—åˆ°é«˜åº¦å…³æ³¨ï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä¸€äº›æŠ€æœ¯å·²è¾¾åˆ°äº†æ›´å¥½çš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œæ³¨æ„åŠ›æ¨¡å‹ï¼‰ã€‚è¿™äº›æ¨¡å‹å¯ä»¥è¢«è®¤ä¸ºç”¨äºå¢å¼ºæ’è¡¥æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å¿½ç•¥äº†ç¼ºå¤±æ—¶é—´æˆ³çš„é—®é¢˜ï¼Œè¿™å¯èƒ½ä¹Ÿä¼šæ˜¾å¾—æ¨¡ç³Š[[36](#bib.bib36)]ã€‚å› æ­¤ï¼Œå¯¹æ­¤ç±»æŠ€æœ¯ä»æœ‰éœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°æ’è¡¥ç¼ºå¤±çš„æ—¶é—´æˆ³ã€‚æ­¤å¤–ï¼Œä¸ç›´æ¥æ’è¡¥ç¼ºå¤±å€¼çš„æŸ¥è¯¢å›ç­”æ˜¯å¤„ç†ç¼ºå¤±å€¼çš„å¦ä¸€ç§è§†è§’ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç‰¹å®šå€¼ä¸éœ€è¦æ’è¡¥ï¼Œè€Œåœ¨ä¸ä¸€è‡´çš„æ¦‚ç‡æ•°æ®åº“ä¸­åº”ç”Ÿæˆä¸€è‡´çš„æŸ¥è¯¢ã€‚
- en: 6.1 Attention Mechanism Enhanced
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 æ³¨æ„åŠ›æœºåˆ¶å¢å¼º
- en: In recent years, the attention mechanism has been shown successful in deep learning
    society, especially in NLP fields. When adopted in RNN, the attention mechanism
    allocates weights for each hidden state to draw information from the sequence.
    With such mechanism, the model is improved to capture latent patterns in historical
    data, thus may benefit time series imputation. Compared to existing RNN models
    (e.g., LSTM and GRU) which already take long-term dependencies into consideration,
    the attention mechanism for instance temporal attention enables the model to see
    features and status globally. However, LSTM and GRU will still lose long-term
    information due to the forget gate unit.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œæ³¨æ„åŠ›æœºåˆ¶åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸç‰¹åˆ«æ˜¯åœ¨NLPé¢†åŸŸè¡¨ç°æˆåŠŸã€‚å½“åœ¨RNNä¸­é‡‡ç”¨æ—¶ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¸ºæ¯ä¸ªéšè—çŠ¶æ€åˆ†é…æƒé‡ï¼Œä»¥ä»åºåˆ—ä¸­æå–ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æœºåˆ¶ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ•æ‰å†å²æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼ï¼Œä»è€Œå¯èƒ½æœ‰åˆ©äºæ—¶é—´åºåˆ—æ’è¡¥ã€‚ä¸å·²ç»è€ƒè™‘é•¿æœŸä¾èµ–çš„ç°æœ‰RNNæ¨¡å‹ï¼ˆä¾‹å¦‚LSTMå’ŒGRUï¼‰ç›¸æ¯”ï¼Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¾‹å¦‚æ—¶é—´æ³¨æ„åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…¨çƒæ€§åœ°çœ‹åˆ°ç‰¹å¾å’ŒçŠ¶æ€ã€‚ç„¶è€Œï¼ŒLSTMå’ŒGRUç”±äºé—å¿˜é—¨å•å…ƒä»ç„¶ä¼šä¸¢å¤±é•¿æœŸä¿¡æ¯ã€‚
- en: 'Recently, pure attention models are proposed without RNN. The Transformer proposed
    in [[47](#bib.bib47)] is one of the popular frameworks. In the proposed Transformer
    framework, it only adopts an attention layer called self-attention, which is computed
    as:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œçº¯æ³¨æ„åŠ›æ¨¡å‹è¢«æå‡ºè€Œæ²¡æœ‰ä½¿ç”¨RNNã€‚[[47](#bib.bib47)]ä¸­æå‡ºçš„Transformeræ˜¯å…¶ä¸­ä¸€ä¸ªæµè¡Œçš„æ¡†æ¶ã€‚åœ¨æ‰€æå‡ºçš„Transformeræ¡†æ¶ä¸­ï¼Œå®ƒåªé‡‡ç”¨äº†ä¸€ä¸ªç§°ä¸ºè‡ªæ³¨æ„åŠ›çš„æ³¨æ„åŠ›å±‚ï¼Œå…¶è®¡ç®—æ–¹å¼ä¸ºï¼š
- en: '|  | $\operatorname{Attention}(Q,K,V)=\operatorname{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$
    |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{Attention}(Q,K,V)=\operatorname{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$
    |  |'
- en: where $Q,K,V$ are queries, keys and values respectively, and $d_{k}$ is the
    dimension of the input.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $Q,K,V$ åˆ†åˆ«æ˜¯æŸ¥è¯¢ã€é”®å’Œå€¼ï¼Œè€Œ $d_{k}$ æ˜¯è¾“å…¥çš„ç»´åº¦ã€‚
- en: 'Accepting a single sequence as input, the self-attention mechanism relates
    different positions of the input and tries to compute a representation of the
    sequence. Without applying RNN, the Transformer relies entirely on the self-attention
    layers to former an encoder-decoder structure, which is similar to the auto-encoder
    introduced in SectionÂ [4.1](#S4.SS1 "4.1 Characteristics of Chosen Methods â€£ 4
    Methods â€£ Time Series Data Imputation: A Survey on Deep Learning Approaches").
    Such a structure provides the ability to extract high-dimensional features for
    reconstructing, which benefits tasks like machine translation introduced in [[47](#bib.bib47)].'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥å—å•ä¸ªåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶å°†è¾“å…¥çš„ä¸åŒä½ç½®è¿›è¡Œå…³è”ï¼Œå¹¶å°è¯•è®¡ç®—åºåˆ—çš„è¡¨ç¤ºã€‚æ— éœ€åº”ç”¨ RNNï¼ŒTransformer å®Œå…¨ä¾èµ–äºè‡ªæ³¨æ„åŠ›å±‚å½¢æˆç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œè¿™ç±»ä¼¼äºç¬¬[4.1](#S4.SS1
    "4.1 Characteristics of Chosen Methods â€£ 4 Methods â€£ Time Series Data Imputation:
    A Survey on Deep Learning Approaches")èŠ‚ä¸­ä»‹ç»çš„è‡ªç¼–ç å™¨ã€‚è¿™ç§ç»“æ„æä¾›äº†æå–é«˜ç»´ç‰¹å¾ä»¥è¿›è¡Œé‡å»ºçš„èƒ½åŠ›ï¼Œæœ‰åˆ©äºæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ï¼Œå¦‚[[47](#bib.bib47)]ä¸­ä»‹ç»çš„ä»»åŠ¡ã€‚'
- en: For improving the performance of data imputation, due to the effectiveness of
    the attention mechanisms, models based on attention mechanisms may also address
    the time series imputation problems. And two aforementioned categories of the
    attention mechanisms including temporal attention and self-attention are both
    potential techniques which may benefit the time series imputation. Moreover, with
    the idea of removing RNN and leveraging only attention mechanisms, structures
    like the Transformer may contribute to a new framework for the imputation tasks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜æ•°æ®è¡¥å…¨çš„æ€§èƒ½ï¼Œç”±äºæ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ä¹Ÿå¯èƒ½è§£å†³æ—¶é—´åºåˆ—è¡¥å…¨é—®é¢˜ã€‚ä¸Šè¿°ä¸¤ç±»æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬æ—¶é—´æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›ï¼Œéƒ½æ˜¯å¯èƒ½å¯¹æ—¶é—´åºåˆ—è¡¥å…¨æœ‰ç›Šçš„æ½œåœ¨æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨å»é™¤
    RNN ä»…ä¾é æ³¨æ„åŠ›æœºåˆ¶çš„æ€æƒ³ï¼Œåƒ Transformer è¿™æ ·çš„ç»“æ„å¯èƒ½ä¸ºè¡¥å…¨ä»»åŠ¡æä¾›æ–°çš„æ¡†æ¶ã€‚
- en: To summary, two categories of attention mechanisms including temporal attention
    and self-attention may bring future opportunities on time series imputation. And
    the pure attention frameworks are also new directions to model time series.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œä¸¤ç±»æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬æ—¶é—´æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›ï¼Œå¯èƒ½ä¼šä¸ºæ—¶é—´åºåˆ—è¡¥å…¨å¸¦æ¥æœªæ¥çš„æœºé‡ã€‚è€Œçº¯ç²¹çš„æ³¨æ„åŠ›æ¡†æ¶ä¹Ÿæ˜¯å»ºæ¨¡æ—¶é—´åºåˆ—çš„æ–°æ–¹å‘ã€‚
- en: 6.2 Imputing Missing Timestamps
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 è¡¥å…¨ç¼ºå¤±çš„æ—¶é—´æˆ³
- en: Missing timestamps often appear obscurely [[36](#bib.bib36)], e.g., denoted
    by 00:00:00. Most of existing methods mainly focus on the missing values of the
    time series. However, once timestamps are missing, these methods may fail to capture
    the information of time and unable to obtain accurate imputation results. Thus,
    an extension of existing methods to impute missing timestamps is potentially appropriate
    direction to deal with such scenarios.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼ºå¤±çš„æ—¶é—´æˆ³å¾€å¾€è¡¨ç°å¾—å¾ˆéšè”½[[36](#bib.bib36)]ï¼Œä¾‹å¦‚ï¼Œè¢«æ ‡è®°ä¸º00:00:00ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä¸»è¦å…³æ³¨æ—¶é—´åºåˆ—ä¸­çš„ç¼ºå¤±å€¼ã€‚ç„¶è€Œï¼Œä¸€æ—¦æ—¶é—´æˆ³ç¼ºå¤±ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½æ— æ³•æ•æ‰æ—¶é—´ä¿¡æ¯ï¼Œä»è€Œæ— æ³•è·å¾—å‡†ç¡®çš„è¡¥å…¨ç»“æœã€‚å› æ­¤ï¼Œæ‰©å±•ç°æœ‰æ–¹æ³•ä»¥è¡¥å…¨ç¼ºå¤±çš„æ—¶é—´æˆ³æ˜¯å¤„ç†æ­¤ç±»æƒ…å†µçš„æ½œåœ¨åˆé€‚æ–¹å‘ã€‚
- en: 6.3 Consistent Query Answering
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 ä¸€è‡´æ€§æŸ¥è¯¢å›ç­”
- en: Following [[22](#bib.bib22)], query answering without determining the specific
    imputation of each missing value is crucial in probabilistic databases [[10](#bib.bib10)],
    when data from many sources can be inconsistent and uncertain. Therefore, consistent
    query answering (CQA) is needed. Missing values data in CQA problem increase the
    difficulty of answering the query consistently. Both the inconsistent data from
    different sources and missing values should be considered. Therefore, a combination
    of data imputation methods and CQA methods can be a potential approach.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®[[22](#bib.bib22)]ï¼Œåœ¨æ¦‚ç‡æ•°æ®åº“[[10](#bib.bib10)]ä¸­è¿›è¡ŒæŸ¥è¯¢å›ç­”æ—¶ï¼Œä¸ç¡®å®šæ¯ä¸ªç¼ºå¤±å€¼çš„å…·ä½“è¡¥å…¨æ–¹å¼æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºæ¥è‡ªå¤šä¸ªæ¥æºçš„æ•°æ®å¯èƒ½ä¸ä¸€è‡´ä¸”ä¸ç¡®å®šã€‚å› æ­¤ï¼Œéœ€è¦ä¸€è‡´æ€§æŸ¥è¯¢å›ç­”ï¼ˆCQAï¼‰ã€‚CQA
    é—®é¢˜ä¸­çš„ç¼ºå¤±å€¼æ•°æ®å¢åŠ äº†ä¸€è‡´å›ç­”æŸ¥è¯¢çš„éš¾åº¦ã€‚åº”è€ƒè™‘æ¥è‡ªä¸åŒæ¥æºçš„ä¸ä¸€è‡´æ•°æ®å’Œç¼ºå¤±å€¼ã€‚å› æ­¤ï¼Œæ•°æ®è¡¥å…¨æ–¹æ³•å’Œ CQA æ–¹æ³•çš„ç»“åˆå¯èƒ½æ˜¯ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚
- en: References
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] E.Â Acuna and C.Â Rodriguez. The treatment of missing values and its effect
    on classifier accuracy. In Classification, clustering, and data mining applications,
    pages 639â€“647\. Springer, 2004.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] E. Acuna å’Œ C. Rodriguez. ç¼ºå¤±å€¼å¤„ç†åŠå…¶å¯¹åˆ†ç±»å™¨å‡†ç¡®æ€§çš„å½±å“ã€‚åœ¨ Classification, clustering,
    and data mining applications, é¡µç  639â€“647. Springer, 2004.'
- en: '[2] M.Â Amiri and R.Â Jensen. Missing data imputation using fuzzy-rough methods.
    Neurocomputing, 205:152â€“164, 2016.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. Amiri å’Œ R. Jensen. ä½¿ç”¨æ¨¡ç³Š-ç²—ç³™æ–¹æ³•è¿›è¡Œç¼ºå¤±æ•°æ®è¡¥å…¨ã€‚Neurocomputing, 205:152â€“164, 2016.'
- en: '[3] G.Â E. Batista, M.Â C. Monard, etÂ al. A study of k-nearest neighbour as an
    imputation method. HIS, 87(251-260):48, 2002.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] G. E. Batista, M. C. Monard ç­‰äºº. k-æœ€è¿‘é‚»ä½œä¸ºè¡¥å…¨æ–¹æ³•çš„ç ”ç©¶ã€‚HIS, 87(251-260):48, 2002.'
- en: '[4] S.Â Bengio, O.Â Vinyals, N.Â Jaitly, and N.Â Shazeer. Scheduled sampling for
    sequence prediction with recurrent neural networks. In Advances in Neural Information
    Processing Systems 28: Annual Conference on Neural Information Processing Systems
    2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1171â€“1179, 2015.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] G.Â E. Box, G.Â M. Jenkins, G.Â C. Reinsel, and G.Â M. Ljung. Time series analysis:
    forecasting and control. John Wiley & Sons, 2015.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] W.Â Cao, D.Â Wang, J.Â Li, H.Â Zhou, L.Â Li, and Y.Â Li. BRITS: bidirectional
    recurrent imputation for time series. In Advances in Neural Information Processing
    Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS
    2018, 3-8 December 2018, MontrÃ©al, Canada, pages 6776â€“6786, 2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Z.Â Che, S.Â Purushotham, K.Â Cho, D.Â Sontag, and Y.Â Liu. Recurrent neural
    networks for multivariate time series with missing values. Scientific reports,
    8(1):6085, 2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] E.Â Choi, M.Â T. Bahadori, A.Â Schuetz, W.Â F. Stewart, and J.Â Sun. Doctor
    ai: Predicting clinical events via recurrent neural networks. In Machine Learning
    for Healthcare Conference, pages 301â€“318, 2016.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] W.Â S. Cleveland and C.Â Loader. Smoothing by Local Regression: Principles
    and Methods, pages 10â€“49. Physica-Verlag HD, Heidelberg, 1996.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] N.Â N. Dalvi and D.Â Suciu. Efficient query evaluation on probabilistic
    databases. VLDB J., 16(4):523â€“544, 2007.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] C.Â Fang, S.Â Song, Z.Â Chen, and A.Â Gui. Fine-grained fuel consumption prediction.
    In Proceedings of the 28th ACM International Conference on Information and Knowledge
    Management, CIKM 2019, Beijing, China, November 3-7, 2019, pages 2783â€“2791, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] P.Â J. GarcÃ­a-Laencina, P.Â H. Abreu, M.Â H. Abreu, and N.Â Afonoso. Missing
    data imputation on the 5-year survival prediction of breast cancer patients with
    unknown discrete values. Comp. in Bio. and Med., 59:125â€“133, 2015.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P.Â J. GarcÃ­a-Laencina, J.Â Sancho-GÃ³mez, and A.Â R. Figueiras-Vidal. Pattern
    classification with missing data: a review. Neural Computing and Applications,
    19(2):263â€“282, 2010.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Z.Â Ghahramani and M.Â I. Jordan. Supervised learning from incomplete data
    via an EM approach. In J.Â D. Cowan, G.Â Tesauro, and J.Â Alspector, editors, Advances
    in Neural Information Processing Systems 6, [7th NIPS Conference, Denver, Colorado,
    USA, 1993], pages 120â€“127\. Morgan Kaufmann, 1993.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] I.Â J. Goodfellow, J.Â Pouget-Abadie, M.Â Mirza, B.Â Xu, D.Â Warde-Farley,
    S.Â Ozair, A.Â C. Courville, and Y.Â Bengio. Generative adversarial nets. In Advances
    in Neural Information Processing Systems 27: Annual Conference on Neural Information
    Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2672â€“2680,
    2014.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J.Â W. Graham. Missing data analysis: Making it work in the real world.
    Annual review of psychology, 60:549â€“576, 2009.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A.Â Graves and J.Â Schmidhuber. Framewise phoneme classification with bidirectional
    LSTM and other neural network architectures. Neural Networks, 18(5-6):602â€“610,
    2005.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T.Â Hsieh, H.Â Hsiao, and W.Â Yeh. Forecasting stock markets using wavelet
    transforms and recurrent neural networks: An integrated system based on artificial
    bee colony algorithm. Appl. Soft Comput., 11(2):2510â€“2525, 2011.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] P.Â Isola, J.-Y. Zhu, T.Â Zhou, and A.Â A. Efros. Image-to-image translation
    with conditional adversarial networks. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 1125â€“1134, 2017.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] M.Â Kantardzic. Data mining: concepts, models, methods, and algorithms.
    John Wiley & Sons, 2011.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y.Â Li, Y.Â Wang, Z.Â Zhang, Y.Â Wang, D.Â Ma, and J.Â Huang. A novel fast and
    memory efficient parallel MLCS algorithm for long and large-scale sequences alignments.
    In 32nd IEEE International Conference on Data Engineering, ICDE 2016, Helsinki,
    Finland, May 16-20, 2016, pages 1170â€“1181\. IEEE Computer Society, 2016.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X.Â Lian, L.Â Chen, and S.Â Song. Consistent query answers in inconsistent
    probabilistic databases. In Proceedings of the ACM SIGMOD International Conference
    on Management of Data, SIGMOD 2010, Indianapolis, Indiana, USA, June 6-10, 2010,
    pages 303â€“314, 2010.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Z.Â C. Lipton, D.Â Kale, and R.Â Wetzel. Directly modeling missing data in
    sequences with rnns: Improved classification of clinical time series. In Machine
    Learning for Healthcare Conference, pages 253â€“270, 2016.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R.Â J. Little and D.Â B. Rubin. Statistical analysis with missing data,
    volume 793. John Wiley & Sons, 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y.Â Liu, R.Â Yu, S.Â Zheng, E.Â Zhan, and Y.Â Yue. NAOMI: non-autoregressive
    multiresolution sequence imputation. In Advances in Neural Information Processing
    Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
    2019, 8-14 December 2019, Vancouver, BC, Canada, pages 11236â€“11246, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] X.Â Luo, M.Â Zhou, H.Â Leung, Y.Â Xia, Q.Â Zhu, Z.Â You, and S.Â Li. An incremental-and-static-combined
    scheme for matrix-factorization-based collaborative filtering. IEEE Transactions
    on Automation Science and Engineering, 13(1):333â€“343, 2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y.Â Luo, X.Â Cai, Y.Â Zhang, J.Â Xu, and X.Â Yuan. Multivariate time series
    imputation with generative adversarial networks. In Advances in Neural Information
    Processing Systems 31: Annual Conference on Neural Information Processing Systems
    2018, NeurIPS 2018, 3-8 December 2018, MontrÃ©al, Canada, pages 1603â€“1614, 2018.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y.Â Luo, Y.Â Zhang, X.Â Cai, and X.Â Yuan. EÂ²gan: End-to-end generative adversarial
    network for multivariate time series imputation. In Proceedings of the Twenty-Eighth
    International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
    China, August 10-16, 2019, pages 3094â€“3100\. ijcai.org, 2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] P.Â E. McKnight, K.Â M. McKnight, S.Â Sidani, and A.Â J. Figueredo. Missing
    data: A gentle introduction. Guilford Press, 2007.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J.Â Mei, Y.Â deÂ Castro, Y.Â Goude, and G.Â HÃ©brail. Nonnegative matrix factorization
    for time series recovery from a few temporal aggregates. In Proceedings of the
    34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,
    6-11 August 2017, volumeÂ 70 of Proceedings of Machine Learning Research, pages
    2382â€“2390\. PMLR, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] M.Â Mirza and S.Â Osindero. Conditional generative adversarial nets. CoRR,
    abs/1411.1784, 2014.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] F.Â V. Nelwamondo, S.Â Mohamed, and T.Â Marwala. Missing data: A comparison
    of neural network and expectation maximization techniques. Current Science, pages
    1514â€“1521, 2007.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S.Â Nordbotten. Neural network imputation applied to the norwegian 1990
    population census data. JOURNAL OF OFFICIAL STATISTICS-STOCKHOLM-, 12:385â€“402,
    1996.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] D.Â Pathak, P.Â Krahenbuhl, J.Â Donahue, T.Â Darrell, and A.Â A. Efros. Context
    encoders: Feature learning by inpainting. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 2536â€“2544, 2016.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] P.Â K. Sharpe and R.Â J. Solly. Dealing with missing values in neural network-based
    diagnostic systems. Neural Computing and Applications, 3(2):73â€“77, 1995.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S.Â Song, Y.Â Cao, and J.Â Wang. Cleaning timestamps with temporal constraints.
    PVLDB, 9(10):708â€“719, 2016.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S.Â Song and L.Â Chen. Differential dependencies: Reasoning and discovery.
    ACM Trans. Database Syst., 36(3):16:1â€“16:41, 2011.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] S.Â Song, L.Â Chen, and H.Â Cheng. Efficient determination of distance thresholds
    for differential dependencies. IEEE Trans. Knowl. Data Eng., 26(9):2179â€“2192,
    2014.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S.Â Song, L.Â Chen, and P.Â S. Yu. On data dependencies in dataspaces. In
    Proceedings of the 27th International Conference on Data Engineering, ICDE 2011,
    April 11-16, 2011, Hannover, Germany, pages 470â€“481, 2011.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S.Â Song, L.Â Chen, and P.Â S. Yu. Comparable dependencies over heterogeneous
    data. VLDB J., 22(2):253â€“274, 2013.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S.Â Song, C.Â Li, and X.Â Zhang. Turn waste into wealth: On simultaneous
    clustering and cleaning over dirty data. In Proceedings of the 21th ACM SIGKDD
    International Conference on Knowledge Discovery and Data Mining, Sydney, NSW,
    Australia, August 10-13, 2015, pages 1115â€“1124, 2015.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S.Â Song, Y.Â Sun, A.Â Zhang, L.Â Chen, and J.Â Wang. Enriching data imputation
    under similarity rule constraints. IEEE Trans. Knowl. Data Eng., 32(2):275â€“287,
    2020.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] S.Â Song, A.Â Zhang, L.Â Chen, and J.Â Wang. Enriching data imputation with
    extensive similarity neighbors. PVLDB, 8(11):1286â€“1297, 2015.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S.Â Song, A.Â Zhang, J.Â Wang, and P.Â S. Yu. SCREEN: stream data cleaning
    under speed constraints. In Proceedings of the 2015 ACM SIGMOD International Conference
    on Management of Data, Melbourne, Victoria, Australia, May 31 - June 4, 2015,
    pages 827â€“841, 2015.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S.Â Song, H.Â Zhu, and L.Â Chen. Probabilistic correlation-based similarity
    measure on text records. Inf. Sci., 289:8â€“24, 2014.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y.Â Sun, S.Â Song, C.Â Wang, and J.Â Wang. Swapping repair for misplaced attribute
    values. In 36th IEEE International Conference on Data Engineering, ICDE 2020.
    IEEE, 2020.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez,
    L.Â Kaiser, and I.Â Polosukhin. Attention is all you need. In Advances in Neural
    Information Processing Systems 30: Annual Conference on Neural Information Processing
    Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998â€“6008, 2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J.Â Wang, S.Â Song, X.Â Lin, X.Â Zhu, and J.Â Pei. Cleaning structured event
    logs: A graph repair approach. In 31st IEEE International Conference on Data Engineering,
    ICDE 2015, Seoul, South Korea, April 13-17, 2015, pages 30â€“41, 2015.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J.Â Wang, S.Â Song, X.Â Zhu, and X.Â Lin. Efficient recovery of missing events.
    PVLDB, 6(10):841â€“852, 2013.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J.Â Wang, S.Â Song, X.Â Zhu, X.Â Lin, and J.Â Sun. Efficient recovery of missing
    events. IEEE Trans. Knowl. Data Eng., 28(11):2943â€“2957, 2016.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] W.Â Wothke. Longitudinal and multigroup modeling with missing data. 2000.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H.Â Yu, N.Â Rao, and I.Â S. Dhillon. Temporal regularized matrix factorization
    for high-dimensional time series prediction. In Advances in Neural Information
    Processing Systems 29: Annual Conference on Neural Information Processing Systems
    2016, December 5-10, 2016, Barcelona, Spain, pages 847â€“855, 2016.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A.Â Zhang, S.Â Song, Y.Â Sun, and J.Â Wang. Learning individual models for
    imputation. In 35th IEEE International Conference on Data Engineering, ICDE 2019,
    Macao, China, April 8-11, 2019, pages 160â€“171\. IEEE, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A.Â Zhang, S.Â Song, and J.Â Wang. Sequential data cleaning: A statistical
    approach. In Proceedings of the 2016 International Conference on Management of
    Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016,
    pages 909â€“924\. ACM, 2016.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A.Â Zhang, S.Â Song, J.Â Wang, and P.Â S. Yu. Time series data cleaning: From
    anomaly detection to anomaly repairing. PVLDB, 10(10):1046â€“1057, 2017.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] G.Â P. Zhang. Time series forecasting using a hybrid ARIMA and neural network
    model. Neurocomputing, 50:159â€“175, 2003.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X.Â Zhu, S.Â Song, X.Â Lian, J.Â Wang, and L.Â Zou. Matching heterogeneous
    event data. In International Conference on Management of Data, SIGMOD 2014, Snowbird,
    UT, USA, June 22-27, 2014, pages 1211â€“1222, 2014.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
