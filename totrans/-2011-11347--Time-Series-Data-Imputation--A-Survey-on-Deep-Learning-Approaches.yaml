- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:58:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2011.11347] Time Series Data Imputation: A Survey on Deep Learning Approaches'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2011.11347] 时间序列数据插补：深度学习方法综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.11347](https://ar5iv.labs.arxiv.org/html/2011.11347)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2011.11347](https://ar5iv.labs.arxiv.org/html/2011.11347)
- en: 'Time Series Data Imputation: A Survey on Deep Learning Approaches'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列数据插补：深度学习方法综述
- en: Chenguang Fang [fcg19@mails.tsinghua.edu.cn](mailto:fcg19@mails.tsinghua.edu.cn)
    Chen Wang [wang˙chen@tsinghua.edu.cn](mailto:wang%CB%99chen@tsinghua.edu.cn) Tsinghua
    University, Beijing
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 冯成光 [fcg19@mails.tsinghua.edu.cn](mailto:fcg19@mails.tsinghua.edu.cn) 王晨 [wang˙chen@tsinghua.edu.cn](mailto:wang%CB%99chen@tsinghua.edu.cn)
    清华大学，北京
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Time series are all around in real-world applications. However, unexpected accidents
    for example broken sensors or missing of the signals will cause missing values
    in time series, making the data hard to be utilized. It then does harm to the
    downstream applications such as traditional classification or regression, sequential
    data integration and forecasting tasks, thus raising the demand for data imputation.
    Currently, time series data imputation is a well-studied problem with different
    categories of methods. However, these works rarely take the temporal relations
    among the observations and treat the time series as normal structured data, losing
    the information from the time data. In recent, deep learning models have raised
    great attention. Time series methods based on deep learning have made progress
    with the usage of models like RNN, since it captures time information from data.
    In this paper, we mainly focus on time series imputation technique with deep learning
    methods, which recently made progress in this field. We will review and discuss
    their model architectures, their pros and cons as well as their effects to show
    the development of the time series imputation methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列在现实世界应用中无处不在。然而，意外事故，例如传感器故障或信号丢失，会导致时间序列中出现缺失值，从而使数据难以利用。这对下游应用，如传统的分类或回归、序列数据集成和预测任务，造成了伤害，因此对数据插补的需求增加。目前，时间序列数据插补是一个研究充分的问题，具有不同类别的方法。然而，这些研究很少考虑观察数据之间的时间关系，并将时间序列视为普通的结构化数据，导致丧失时间数据中的信息。近年来，深度学习模型引起了极大的关注。基于深度学习的时间序列方法在使用如RNN等模型方面取得了进展，因为它捕捉了数据中的时间信息。本文主要关注使用深度学习方法进行时间序列插补技术，最近在这一领域取得了进展。我们将回顾和讨论它们的模型架构、优缺点及其效果，以展示时间序列插补方法的发展。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Time Series Imputation , Deep Learning , GAN , RNN
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列插补，深度学习，生成对抗网络（GAN），递归神经网络（RNN）
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Time series are vital in real-world applications. However, due to unexpected
    accidents, for example broken sensors or missing of the signals, missing values
    are everywhere in time series. In some datasets, the missing rate can reach 90%,
    which makes the data hard to be utilized [[12](#bib.bib12)]. The missing values
    significantly do harm to the downstream applications such as traditional classification
    or regression, sequential data integration [[21](#bib.bib21)] and forecasting
    tasks [[18](#bib.bib18)], leading to high demand for data imputation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列在现实世界应用中至关重要。然而，由于意外事故，例如传感器故障或信号丢失，时间序列中到处都是缺失值。在一些数据集中，缺失率可以达到90%，这使得数据难以利用[[12](#bib.bib12)]。缺失值对下游应用造成了显著的伤害，例如传统的分类或回归、序列数据集成[[21](#bib.bib21)]和预测任务[[18](#bib.bib18)]，导致对数据插补的需求很高。
- en: Our preliminary study [[11](#bib.bib11)] shows that imputing the missing values
    indeed helps significantly the prediction of fuel consumption. In the scenarios
    of fuel consumption prediction, missing values happen due to the errors of sensors.
    We propose an imputation approach named FuelNet to deal with such errors. The
    FuelNet generates proper values to impute missing data. With imputed data, the
    fuel consumption can be reduced by around 45.5%.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初步研究[[11](#bib.bib11)]表明，插补缺失值确实显著提高了燃料消耗的预测。在燃料消耗预测的场景中，缺失值发生是由于传感器的错误。我们提出了一种名为FuelNet的插补方法来处理这些错误。FuelNet生成适当的值来插补缺失数据。使用插补数据，燃料消耗可以减少约45.5%。
- en: In current stages, time series data imputation is a well studied problem with
    different categories of methods including deletion methods, simple imputation
    methods and learning based methods. However, these works rarely take the temporal
    relations among the observations and treat the time series as normal structured
    data, thus losing the information from the time data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 目前阶段，时间序列数据插补是一个研究较多的问题，涵盖了不同类别的方法，包括删除方法、简单插补方法和基于学习的方法。然而，这些工作很少考虑观测值之间的时间关系，而是将时间序列视为普通的结构化数据，从而丧失了时间数据的信息。
- en: Fortunately, with the increasing development of deep learning, a large quantity
    of deep learning methods are researched, among which RNN is one of the typical
    methods to handle sequence data. The intuition on why deep learning models could
    advance imputation tasks is that, they are proven to have the ability to mine
    information hidden in the time series. These characteristics could enable them
    to impute missing values with such models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，随着深度学习的不断发展，大量深度学习方法被研究，其中RNN是处理序列数据的典型方法之一。深度学习模型能够推动插补任务的直观原因是，它们被证明能够挖掘时间序列中隐藏的信息。这些特性使得它们能够使用这样的模型来插补缺失值。
- en: Recently, deep learning methods have been applied to multivariable time series
    imputation and show positive progress in imputing the missing values. In this
    paper, we mainly survey three papers about time series imputation with deep learning
    methods [[7](#bib.bib7), [27](#bib.bib27), [6](#bib.bib6), [28](#bib.bib28), [25](#bib.bib25)]
    among which RNN, GRU and GAN are adopted separately or in combination. We will
    review these papers about their model structure, the common parts they all adopted
    and the advantages and disadvantages through comparison.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习方法已被应用于多变量时间序列插补，并在插补缺失值方面显示出积极进展。本文主要调查了三篇关于深度学习方法的时间序列插补论文 [[7](#bib.bib7),
    [27](#bib.bib27), [6](#bib.bib6), [28](#bib.bib28), [25](#bib.bib25)]，其中RNN、GRU和GAN被单独或组合使用。我们将回顾这些论文的模型结构、它们共同采用的部分以及通过比较得出的优缺点。
- en: The remainder of the paper is organized as follows. In the next section, we
    categorize existing data imputation methods and mainly give an introduction to
    deep learning imputation methods. Section 3 will show the definition of the problems
    and the symbols. Section 4 will give a detailed discussion of deep learning methods,
    mainly about their concrete structure, advantages and disadvantages. And finally
    in Section 5 we summarize the survey and give our conclusions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。下一节中，我们将对现有的数据插补方法进行分类，并主要介绍深度学习插补方法。第3节将展示问题的定义和符号。第4节将详细讨论深度学习方法，主要涉及其具体结构、优缺点。最后，第5节将总结调研并给出我们的结论。
- en: 'Table 1: Comparison of different methods addressing time series imputation'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同时间序列插补方法的比较
- en: '| Methodologies | Sample approaches from the literature | Time interval | Value
    type | Time series dimension |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 方法论 | 文献中的示例方法 | 时间间隔 | 值类型 | 时间序列维度 |'
- en: '| Deletion | Listwise Deletion [[51](#bib.bib51)] | regular/irregular | qualitative
    | multidimensional |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 删除 | 列删除 [[51](#bib.bib51)] | 常规/非规 | 定性 | 多维 |'
- en: '| Pairwise Deletion [[29](#bib.bib29)] | regular/irregular | qualitative |
    multidimensional |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 配对删除 [[29](#bib.bib29)] | 常规/非规 | 定性 | 多维 |'
- en: '| Neighbor Based | QDORC [[41](#bib.bib41)] | regular/irregular | quantitative/qualitative
    | multidimensional |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 基于邻域的 | QDORC [[41](#bib.bib41)] | 常规/非规 | 定量/定性 | 多维 |'
- en: '| SRKN [[46](#bib.bib46)] | regular/irregular | quantitative/qualitative |
    multidimensional |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| SRKN [[46](#bib.bib46)] | 常规/非规 | 定量/定性 | 多维 |'
- en: '| Constraint Based | DERAND [[43](#bib.bib43), [42](#bib.bib42)] | regular/irregular
    | quantitative/qualitative | multidimensional |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 基于约束 | DERAND [[43](#bib.bib43), [42](#bib.bib42)] | 常规/非规 | 定量/定性 | 多维 |'
- en: '| SCREEN [[44](#bib.bib44)] | regular/irregular | qualitative | single dimensional
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| SCREEN [[44](#bib.bib44)] | 常规/非规 | 定性 | 单维 |'
- en: '| Regression Based | ARX [[5](#bib.bib5)] | regular | qualitative | single
    dimensional |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 基于回归 | ARX [[5](#bib.bib5)] | 常规 | 定性 | 单维 |'
- en: '| IMR [[55](#bib.bib55)] | regular | qualitative | single dimensional |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| IMR [[55](#bib.bib55)] | 常规 | 定性 | 单维 |'
- en: '| Statistical | DPC [[54](#bib.bib54)] | regular | qualitative | single dimensional
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 统计学 | DPC [[54](#bib.bib54)] | 常规 | 定性 | 单维 |'
- en: '| IIM [[53](#bib.bib53)] | regular | qualitative | multidimensional |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| IIM [[53](#bib.bib53)] | 常规 | 定性 | 多维 |'
- en: '| MF Based | TRMF [[52](#bib.bib52)] | regular | qualitative | multidimensional
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 基于 MF | TRMF [[52](#bib.bib52)] | 常规 | 定性 | 多维 |'
- en: '| NMF [[30](#bib.bib30)] | regular | qualitative | multidimensional |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| NMF [[30](#bib.bib30)] | 常规 | 定性 | 多维 |'
- en: '| EM Based | EM [[14](#bib.bib14)] | regular | qualitative | multidimensional
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 基于 EM | EM [[14](#bib.bib14)] | 常规 | 定性 | 多维 |'
- en: '| EM-GMM [[32](#bib.bib32)] | regular | qualitative | multidimensional |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| EM-GMM [[32](#bib.bib32)] | 常规 | 定性 | 多维 |'
- en: '| MLP Based | MLP [[35](#bib.bib35)] | regular | qualitative | single dimensional
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 基于 MLP | MLP [[35](#bib.bib35)] | 常规 | 定性 | 单维 |'
- en: '| ANN [[33](#bib.bib33)] | regular | qualitative | single dimensional |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ANN [[33](#bib.bib33)] | 常规 | 定性 | 单维 |'
- en: '| DL Based | GRU-D [[7](#bib.bib7)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 基于 DL | GRU-D [[7](#bib.bib7)] | 常规/不规则 | 定性 | 多维 |'
- en: '| GRUI-GAN [[27](#bib.bib27)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| GRUI-GAN [[27](#bib.bib27)] | 常规/不规则 | 定性 | 多维 |'
- en: '| BRITS [[6](#bib.bib6)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| BRITS [[6](#bib.bib6)] | 常规/不规则 | 定性 | 多维 |'
- en: '| E2GAN [[28](#bib.bib28)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| E2GAN [[28](#bib.bib28)] | 常规/不规则 | 定性 | 多维 |'
- en: '| NAOMI [[25](#bib.bib25)] | regular/irregular | qualitative | multidimensional
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| NAOMI [[25](#bib.bib25)] | 常规/不规则 | 定性 | 多维 |'
- en: 2 Categorization
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 分类
- en: In this section, we will give a brief introduction of the major approaches to
    time series imputation. Moreover, we will classify existing time series imputation
    methods according to the principles and techniques they rely on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍主要的时间序列填补方法。此外，我们将根据这些方法所依赖的原则和技术对现有的时间序列填补方法进行分类。
- en: 'In order to impute the missing values, researchers have proposed many imputation
    methods to handle the missing values in time series. In this paper, we mainly
    conclude 8 kinds of the missing value imputation methods including deletion methods,
    neighbor based methods, constraint based methods, regression based methods, statistical
    based methods, MF based methods, EM based mathods, MLP based mathods and DL based
    methods. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Time Series Data Imputation:
    A Survey on Deep Learning Approaches") shows the comparison of these methods we
    conclude. We will introduce each kind of method respectively as follows.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '为了填补缺失值，研究人员提出了许多填补方法来处理时间序列中的缺失值。本文主要总结了包括删除方法、基于邻域的方法、基于约束的方法、基于回归的方法、基于统计的方法、基于
    MF 的方法、基于 EM 的方法、基于 MLP 的方法和基于 DL 的方法在内的 8 种缺失值填补方法。表 [1](#S1.T1 "Table 1 ‣ 1
    Introduction ‣ Time Series Data Imputation: A Survey on Deep Learning Approaches")
    显示了我们总结的这些方法的比较。我们将分别介绍每种方法。'
- en: Deletion methods take a simple strategy that they directly erase the observations
    that contain missing values from the raw data [[29](#bib.bib29), [51](#bib.bib51)].
    It is also a commonly adopted strategy when the missing value is not high and
    the deletion of the missing values will not influence the downstream applications.
    However, when the missing rate reaches some level (in [[16](#bib.bib16)], it is
    5%), ignoring the missing values and deleting them make the data incomplete and
    not suitable for downstream applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 删除方法采用简单的策略，即直接从原始数据中删除包含缺失值的观测 [[29](#bib.bib29), [51](#bib.bib51)]。当缺失值不多且删除缺失值不会影响后续应用时，这也是一种常用的策略。然而，当缺失率达到某个水平时（在
    [[16](#bib.bib16)] 中为 5%），忽略缺失值并删除它们会使数据不完整，不适合后续应用。
- en: Neighbor based methods [[3](#bib.bib3), [41](#bib.bib41)] find out the imputation
    value from neighbors, e.g., identified by clustering methods like KNN or DBSCAN.
    They first find the nearest neighbors of the missing values through other attributes,
    and then update the missing values with the mean value of these neighbors. Moreover,
    considering the local similarity, some methods take the last observed valid value
    to replace the blank [[2](#bib.bib2)]. SRKN (Swapping Repair with K Neighbors)
    [[46](#bib.bib46)] in our preliminary study could also be adapted to impute the
    missing values that are misplaced in other dimensions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于邻域的方法 [[3](#bib.bib3), [41](#bib.bib41)] 从邻域中找出填补值，例如，通过 KNN 或 DBSCAN 等聚类方法进行识别。它们首先通过其他属性找到缺失值的最近邻，然后用这些邻域的均值来更新缺失值。此外，考虑到局部相似性，一些方法使用最后观察到的有效值来替代空白
    [[2](#bib.bib2)]。在我们的初步研究中，SRKN（用 K 个邻居进行交换修复） [[46](#bib.bib46)] 也可以适用于填补在其他维度上错位的缺失值。
- en: Constraint based methods [[43](#bib.bib43), [42](#bib.bib42)] discover the rules
    in dataset, and take advantage of these rules to impute. To apply to time series
    data, similarity rules such as differential dependencies [[37](#bib.bib37), [38](#bib.bib38)]
    or comparable dependencies [[39](#bib.bib39), [40](#bib.bib40)] could be employed
    that study the distances or similarities of timestamps as well as values [[45](#bib.bib45)].
    More advanced constraints could be specified in a graph structure [[48](#bib.bib48),
    [57](#bib.bib57)], such as Petri net, and employed to impute the qualitative values
    of events in time series [[49](#bib.bib49), [50](#bib.bib50)]. These methods are
    effective when the data is highly continuous or satisfies certain patterns. For
    example, when the data is increasing linearly, it is effective and efficient to
    take simple methods or clustering methods. And when the rules or constraints are
    satisfied, constraints based methods outperform others in both time and accuracy
    [[44](#bib.bib44)]. However, multivariable time series in the real world are not
    usually satisfied with such rules, thus more general methods are required and
    learning based methods are researched to impute the time series automatically.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于约束的方法 [[43](#bib.bib43), [42](#bib.bib42)] 发现数据集中的规则，并利用这些规则进行插补。要应用于时间序列数据，可以使用类似的规则，例如差分依赖
    [[37](#bib.bib37), [38](#bib.bib38)] 或可比依赖 [[39](#bib.bib39), [40](#bib.bib40)]，这些规则研究时间戳及其值的距离或相似性
    [[45](#bib.bib45)]。更先进的约束可以在图结构 [[48](#bib.bib48), [57](#bib.bib57)] 中指定，例如Petri网，并用于插补时间序列中事件的定性值
    [[49](#bib.bib49), [50](#bib.bib50)]。当数据高度连续或符合某些模式时，这些方法非常有效。例如，当数据线性增长时，使用简单方法或聚类方法既有效又高效。当规则或约束得到满足时，基于约束的方法在时间和准确性上均优于其他方法
    [[44](#bib.bib44)]。然而，现实世界中的多变量时间序列通常不符合这些规则，因此需要更通用的方法，研究者们正在研究基于学习的方法以自动插补时间序列数据。
- en: Regression based methods LOESS [[9](#bib.bib9)] learns a regression model from
    nearest neighbors for predicting the missing value referring to the complete attributes.
    For time series data, autoregressive (AR) models (e.g., ARX [[5](#bib.bib5)] and
    ARIMA [[56](#bib.bib56)]) try to predict missing values from historical data.
    More advanced IMR (iterative minimum repairing [[55](#bib.bib55)]) provides both
    anomaly detection and data repair for both anomalies and missing values. These
    methods mostly benefit from historical data as well as the accuracy of the nearest
    neighbors. Thus they could be applied when neighbors are reliable and the time
    series are highly relative.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于回归的方法 LOESS [[9](#bib.bib9)] 从最近邻中学习回归模型，以预测缺失值，参考完整属性。对于时间序列数据，自回归（AR）模型（如
    ARX [[5](#bib.bib5)] 和 ARIMA [[56](#bib.bib56)]）尝试从历史数据中预测缺失值。更先进的 IMR（迭代最小修复
    [[55](#bib.bib55)]）同时提供异常检测和数据修复，用于处理异常和缺失值。这些方法大多依赖于历史数据以及最近邻的准确性。因此，当邻居可靠且时间序列高度相关时，它们可以应用。
- en: Statistical based methods rely on statistical models to impute the missing values
    [[24](#bib.bib24)]. Simple statistical methods just utilize the data in the original
    data to impute the missing values, such as take the mean value or median value
    of the attribute to impute [[1](#bib.bib1), [20](#bib.bib20)]. [[54](#bib.bib54)]
    estimates probability values by statistics on speeds as well as the changes. Recently,
    more advanced IIM (Imputation via Individual models) [[53](#bib.bib53)] adaptively
    learns individual models for various number of neighbors. Unlike regression based
    methods which based on just historical data, statistical based models are learned
    from the whole dataset, including historical data and future data. Therefore,
    they may capture more information from raw data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于统计的方法依赖于统计模型来插补缺失值 [[24](#bib.bib24)]。简单的统计方法仅利用原始数据中的数据来插补缺失值，例如取属性的均值或中位数进行插补
    [[1](#bib.bib1), [20](#bib.bib20)]。[[54](#bib.bib54)] 通过统计速度和变化来估计概率值。近年来，更先进的
    IIM（通过个体模型插补） [[53](#bib.bib53)] 自适应地学习各种邻居数量的个体模型。与仅基于历史数据的回归方法不同，基于统计的模型是从整个数据集学习的，包括历史数据和未来数据。因此，它们可能会从原始数据中捕获更多信息。
- en: Matrix Factorization based methods The Matrix Factorization (MF) algorithm tries
    to impute the value with the Matrix Factorization and reconstruction to find the
    correlations among the data and complete the missing values which is a classical
    method of collaborative filtering [[26](#bib.bib26)]. In recent years MF based
    approaches are introduced into time series imputation fields [[52](#bib.bib52),
    [30](#bib.bib30)]. In general, MF based approaches decompose the data matrix into
    2 low-dimensional matrices in the meantime extracting the features from original
    data. And then they try to reconstruct the original matrix and in this processing,
    missing values are imputed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于矩阵分解的方法 矩阵分解（MF）算法尝试通过矩阵分解和重构来填补缺失值，以找出数据之间的相关性并完成缺失值的填补，这是协同过滤的经典方法[[26](#bib.bib26)]。近年来，基于MF的方法被引入时间序列插补领域[[52](#bib.bib52),
    [30](#bib.bib30)]。一般来说，MF方法将数据矩阵分解为2个低维矩阵，同时从原始数据中提取特征。然后，它们尝试重构原始矩阵，在此过程中填补缺失值。
- en: Expectation-Maximization based methods Expectation-Maximization (EM) based methods
    have been successfully applied to missing data imputation problems [[32](#bib.bib32),
    [13](#bib.bib13), [14](#bib.bib14)]. EM based methods follow a two-stage strategy
    consisting of the E (Expectation) step and the M (Maximization) step which iteratively
    imputes the missing values with the statistical model parameters and then updates
    the statistical model parameters to maximize the possibility of the distribution
    of the filled data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 基于期望最大化的方法 期望最大化（EM）基于的方法已经成功应用于缺失数据插补问题[[32](#bib.bib32), [13](#bib.bib13),
    [14](#bib.bib14)]。EM基于的方法遵循一个由E（期望）步骤和M（最大化）步骤组成的两阶段策略，通过迭代地用统计模型参数填补缺失值，然后更新统计模型参数，以最大化填补数据分布的可能性。
- en: 'Multi-Layer Perceptron based methods Multi-Layer Perceptron (MLP) based methods
    employee MLP, which is also called fully connected networks. MLP tries to predict
    missing value by complete values. It can be divided into 3 parts: input layers,
    hidden layers and output layers. In this approach, by minimizing the loss function,
    the perceptron learns a function to impute missing values by input variables.
    In [[35](#bib.bib35)], MLP is used to predict missing values in neural network-based
    diagnostic systems. And in [[33](#bib.bib33)], MLP is employed to impute Population
    Census.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基于多层感知器的方法 多层感知器（MLP）基于的方法使用MLP，也称为全连接网络。MLP试图通过完整值来预测缺失值。它可以分为3部分：输入层、隐藏层和输出层。在这种方法中，通过最小化损失函数，感知器学习一个函数来根据输入变量填补缺失值。在[[35](#bib.bib35)]中，MLP被用于预测神经网络诊断系统中的缺失值。在[[33](#bib.bib33)]中，MLP被用于填补人口普查数据。
- en: Recently, deep learning based methods [[7](#bib.bib7), [27](#bib.bib27), [28](#bib.bib28),
    [6](#bib.bib6)] mainly deploy Recurrent Neural Network (RNN), since RNN is capable
    of capturing the time information. In these papers, time information is handled
    separately and attached with more importance. To impute the time series, not only
    RNN is used, they also combine the models like Gated Recurrent Unit (GRU) [[7](#bib.bib7),
    [27](#bib.bib27), [28](#bib.bib28)] to extract the long-term information, Generative
    Adversarial Networks (GAN) [[27](#bib.bib27), [28](#bib.bib28)] to generate the
    imputed values and Bidirectional Recurrent Networks to improve the accuracy [[6](#bib.bib6)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于深度学习的方法[[7](#bib.bib7), [27](#bib.bib27), [28](#bib.bib28), [6](#bib.bib6)]主要使用递归神经网络（RNN），因为RNN能够捕捉时间信息。在这些论文中，时间信息被单独处理，并赋予更多的重要性。为了填补时间序列，不仅使用了RNN，还结合了如门控递归单元（GRU）[[7](#bib.bib7),
    [27](#bib.bib27), [28](#bib.bib28)]来提取长期信息，生成对抗网络（GAN）[[27](#bib.bib27), [28](#bib.bib28)]来生成填补值，以及双向递归网络来提高准确性[[6](#bib.bib6)]。
- en: According to the above classification, due to the length, the methods for time
    series imputation are too many to give a detailed introduction. Since among these
    methods, deep learning based ones are the latest and most powerful, we will discuss
    3 latest deep learning methods for time series imputation, find the connections
    and the differences among them.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述分类，由于篇幅限制，时间序列插补方法众多，无法详细介绍。由于这些方法中，基于深度学习的方法最新且最强大，我们将讨论3种最新的深度学习时间序列插补方法，找出它们之间的联系和差异。
- en: 3 Preliminary
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步概述
- en: In this section, we first give our formalization of the imputation tasks. It
    is because when introducing the aforesaid deep learning methods, they formalize
    the imputation tasks with different symbols and formulas. And in our research,
    we review them and explain their methods with uniform definitions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先给出插补任务的形式化定义。这是因为在引入上述深度学习方法时，它们用不同的符号和公式来形式化插补任务。在我们的研究中，我们回顾了这些方法，并用统一的定义来解释它们。
- en: Definition 1  (Multivariable Time Series).
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1（多变量时间序列）。
- en: We first denote a timestamp lists $\mathbf{T}=(t_{0},t_{1},...,t_{n-1})$, and
    the time series $\mathbf{X}=\{\mathbf{x_{t_{0}}},\mathbf{x_{t_{1}}},...,\mathbf{x_{t_{n-1}}}\}^{T}$
    as a sequence of $n$ observations. The $i$-th observation of $\ \mathbf{X}$ is
    $\mathbf{x_{t_{i}}}$, which consists of $d$ attributes $\{x_{t_{i}}^{0},x_{t_{i}}^{1},...,x_{t_{i}}^{d}\}$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先表示一个时间戳列表 $\mathbf{T}=(t_{0},t_{1},...,t_{n-1})$，以及时间序列 $\mathbf{X}=\{\mathbf{x_{t_{0}}},\mathbf{x_{t_{1}}},...,\mathbf{x_{t_{n-1}}}\}^{T}$
    作为 $n$ 次观测的序列。$\ \mathbf{X}$ 的第 $i$ 次观测是 $\mathbf{x_{t_{i}}}$，由 $d$ 个属性 $\{x_{t_{i}}^{0},x_{t_{i}}^{1},...,x_{t_{i}}^{d}\}$
    组成。
- en: After defining the multivariable time series, we use mask matrix $\mathbf{M}$
    to denote the missing values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了多变量时间序列之后，我们使用掩码矩阵 $\mathbf{M}$ 来表示缺失值。
- en: Definition 2  (Mask Matrix).
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2（掩码矩阵）。
- en: Mask Matrix $\mathbf{M}$ represents the missing values in $\mathbf{X}$, i.e.,
    $\mathbf{M}\in\mathbb{R}^{n\times d}$. And each element of $\mathbf{M}$ is defined
    as below
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码矩阵 $\mathbf{M}$ 表示 $\mathbf{X}$ 中的缺失值，即 $\mathbf{M}\in\mathbb{R}^{n\times
    d}$。$\mathbf{M}$ 的每个元素定义如下。
- en: '|  | $\mathbf{M}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{0}&amp;{\text{ if }x_{t_{i}}^{j}\text{
    is not observed, i.e. }x_{t}^{j}=\text{None}}\\ {1}&amp;{\text{ otherwise }}\end{array}\right.$
    |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{M}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{0}&amp;{\text{ 如果 }x_{t_{i}}^{j}\text{
    未被观察到，即 }x_{t}^{j}=\text{None}}\\ {1}&amp;{\text{ 否则 }}\end{array}\right.$ |  |
    (1) |'
- en: To utilize the time information, the time intervals should be recorded with
    an extra structure. Therefore, we introduce the time lag, a matrix to represent
    the time intervals between two adjacent observed values of $\mathbf{X}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用时间信息，时间间隔应该用额外的结构记录。因此，我们引入了时间滞后，一个矩阵用来表示 $\mathbf{X}$ 中两个相邻观测值之间的时间间隔。
- en: Definition 3  (Time Lag).
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3（时间滞后）。
- en: We use $\mathbf{\delta}\in\mathbb{R}^{n\times d}$ to record the time lag, and
    we calculate it in an iterative way as follows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 $\mathbf{\delta}\in\mathbb{R}^{n\times d}$ 来记录时间滞后，并以如下迭代方式计算它。
- en: '|  | <math   alttext="\mathbf{\delta}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{t_{i}-t_{i-1},}&amp;{\mathbf{M}_{t_{i-1}}^{j}=1}\\
    {\mathbf{\delta}_{t_{i-1}}^{j}+t_{i}-t_{i-1},}&amp;{\mathbf{M}_{t_{i-1}}^{j-1}==0\&amp;i>0}\\'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{\delta}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{t_{i}-t_{i-1},}&amp;{\mathbf{M}_{t_{i-1}}^{j}=1}\\
    {\mathbf{\delta}_{t_{i-1}}^{j}+t_{i}-t_{i-1},}&amp;{\mathbf{M}_{t_{i-1}}^{j-1}==0\&amp;i>0}\\'
- en: '{0,}&amp;{i==0}\end{array}\right." display="block"><semantics ><mrow ><msubsup
    ><mi >δ</mi><msub ><mi >t</mi><mi >i</mi></msub><mi >j</mi></msubsup><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="2.0pt"
    ><mtr ><mtd columnalign="left" ><mrow ><mrow ><msub ><mi >t</mi><mi >i</mi></msub><mo
    >−</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo
    >,</mo></mrow></mtd><mtd  columnalign="left" ><mrow ><msubsup ><mi >𝐌</mi><msub
    ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub><mi >j</mi></msubsup><mo
    >=</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left" ><mrow ><mrow
    ><mrow ><msubsup ><mi >δ</mi><msub ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn
    >1</mn></mrow></msub><mi >j</mi></msubsup><mo >+</mo><msub ><mi >t</mi><mi >i</mi></msub></mrow><mo
    >−</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo
    >,</mo></mrow></mtd><mtd  columnalign="left" ><mrow ><msubsup ><mi >𝐌</mi><msub
    ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub><mrow ><mi >j</mi><mo
    >−</mo><mn >1</mn></mrow></msubsup><mo rspace="0em" >=</mo><mo lspace="0em" >=</mo><mn
    >0</mn><mo lspace="0.222em" rspace="0.222em" >&</mo><mi >i</mi><mo >></mo><mn
    >0</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mn >0</mn><mo
    >,</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mi >i</mi><mo rspace="0em"
    >=</mo><mo lspace="0em" >=</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\mathbf{\delta}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{t_{i}-t_{i-1},}&{\mathbf{M}_{t_{i-1}}^{j}=1}\\
    {\mathbf{\delta}_{t_{i-1}}^{j}+t_{i}-t_{i-1},}&{\mathbf{M}_{t_{i-1}}^{j-1}==0\&i>0}\\
    {0,}&{i==0}\end{array}\right.</annotation></semantics></math> |  | (2) |'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '{0,}&amp;{i==0}\end{array}\right." display="block"><semantics ><mrow ><msubsup
    ><mi >δ</mi><msub ><mi >t</mi><mi >i</mi></msub><mi >j</mi></msubsup><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="2.0pt"
    ><mtr ><mtd columnalign="left" ><mrow ><mrow ><msub ><mi >t</mi><mi >i</mi></msub><mo
    >−</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo
    >,</mo></mrow></mtd><mtd  columnalign="left" ><mrow ><msubsup ><mi >𝐌</mi><msub
    ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub><mi >j</mi></msubsup><mo
    >=</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd columnalign="left" ><mrow ><mrow
    ><mrow ><msubsup ><mi >δ</mi><msub ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn
    >1</mn></mrow></msub><mi >j</mi></msubsup><mo >+</mo><msub ><mi >t</mi><mi >i</mi></msub></mrow><mo
    >−</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo
    >,</mo></mrow></mtd><mtd  columnalign="left" ><mrow ><msubsup ><mi >𝐌</mi><msub
    ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub><mrow ><mi >j</mi><mo
    >−</mo><mn >1</mn></mrow></msubsup><mo rspace="0em" >=</mo><mo lspace="0em" >=</mo><mn
    >0</mn><mo lspace="0.222em" rspace="0.222em" >&</mo><mi >i</mi><mo >></mo><mn
    >0</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mn >0</mn><mo
    >,</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mi >i</mi><mo rspace="0em"
    >=</mo><mo lspace="0em" >=</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\mathbf{\delta}_{t_{i}}^{j}=\left\{\begin{array}[]{ll}{t_{i}-t_{i-1},}&{\mathbf{M}_{t_{i-1}}^{j}=1}\\
    {\mathbf{\delta}_{t_{i-1}}^{j}+t_{i}-t_{i-1},}&{\mathbf{M}_{t_{i-1}}^{j-1}==0\&i>0}\\
    {0,}&{i==0}\end{array}\right.</annotation></semantics></math> |  | (2) |'
- en: Example 1.
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 1。
- en: We now give an example of the time series $\mathbf{X}$, and corresponding timestamp
    lists $\mathbf{T}$
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们给出时间序列 $\mathbf{X}$ 的一个示例，以及对应的时间戳列表 $\mathbf{T}$
- en: '|  | <math   alttext="\mathbf{X}=\left[\begin{array}[]{cccc}{1}&amp;{6}&amp;{\text{
    None }}&amp;{9}\\ {7}&amp;{\text{ None }}&amp;{7}&amp;{\text{ None }}\\'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\mathbf{X}=\left[\begin{array}[]{cccc}{1}&amp;{6}&amp;{\text{
    None }}&amp;{9}\\ {7}&amp;{\text{ None }}&amp;{7}&amp;{\text{ None }}\\'
- en: '{9}&amp;{\text{ None }}&amp;{\text{ None }}&amp;{79}\end{array}\right],\mathbf{T}=\left[\begin{array}[]{c}{0}\\'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '{9}&amp;{\text{ None }}&amp;{\text{ None }}&amp;{79}\end{array}\right],\mathbf{T}=\left[\begin{array}[]{c}{0}\\'
- en: '{5}\\'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '{5}\\'
- en: '{13}\end{array}\right]" display="block"><semantics ><mrow ><mrow  ><mi >𝐗</mi><mo
    >=</mo><mrow  ><mo >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="2.0pt"  ><mtr
    ><mtd  ><mn >1</mn></mtd><mtd ><mn  >6</mn></mtd><mtd ><mtext  > None </mtext></mtd><mtd
    ><mn  >9</mn></mtd></mtr><mtr ><mtd  ><mn >7</mn></mtd><mtd ><mtext  > None </mtext></mtd><mtd
    ><mn  >7</mn></mtd><mtd ><mtext  > None </mtext></mtd></mtr><mtr ><mtd  ><mn >9</mn></mtd><mtd
    ><mtext  > None </mtext></mtd><mtd ><mtext  > None </mtext></mtd><mtd ><mn  >79</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><mo >,</mo><mrow ><mi  >𝐓</mi><mo >=</mo><mrow ><mo  >[</mo><mtable
    displaystyle="true" rowspacing="2.0pt"  ><mtr ><mtd  ><mn >0</mn></mtd></mtr><mtr
    ><mtd  ><mn >5</mn></mtd></mtr><mtr ><mtd  ><mn >13</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><ci >𝐗</ci><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><cn type="integer"  >1</cn><cn
    type="integer"  >6</cn><ci ><mtext > None </mtext></ci><cn type="integer" >9</cn></matrixrow><matrixrow
    ><cn type="integer"  >7</cn><ci ><mtext > None </mtext></ci><cn type="integer"
    >7</cn><ci  ><mtext > None </mtext></ci></matrixrow><matrixrow ><cn type="integer"
    >9</cn><ci  ><mtext > None </mtext></ci><ci ><mtext > None </mtext></ci><cn type="integer"
    >79</cn></matrixrow></matrix></apply></apply><apply ><ci  >𝐓</ci><apply ><csymbol
    cd="latexml" >delimited-[]</csymbol><matrix ><matrixrow ><cn type="integer"  >0</cn></matrixrow><matrixrow
    ><cn type="integer" >5</cn></matrixrow><matrixrow ><cn type="integer"  >13</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{X}=\left[\begin{array}[]{cccc}{1}&{6}&{\text{
    None }}&{9}\\ {7}&{\text{ None }}&{7}&{\text{ None }}\\ {9}&{\text{ None }}&{\text{
    None }}&{79}\end{array}\right],\mathbf{T}=\left[\begin{array}[]{c}{0}\\ {5}\\
    {13}\end{array}\right]</annotation></semantics></math> |  | (3) |'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: And we can thus compute the mask matrix $\mathbf{M}$ and the time lag $\mathbf{\delta}$.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="{\mathbf{M}}=\left[\begin{array}[]{cccc}{0}&amp;{0}&amp;{1}&amp;{0}\\
    {0}&amp;{1}&amp;{0}&amp;{1}\\'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '{0}&amp;{1}&amp;{1}&amp;{0}\end{array}\right],\delta=\left[\begin{array}[]{cccc}{0}&amp;{0}&amp;{0}&amp;{0}\\'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '{5}&amp;{5}&amp;{5}&amp;{5}\\'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '{8}&amp;{13}&amp;{8}&amp;{13}\end{array}\right]" display="block"><semantics
    ><mrow  ><mrow ><mi  >𝐌</mi><mo >=</mo><mrow ><mo  >[</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="2.0pt"  ><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd
    ><mn  >1</mn></mtd><mtd ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd
    ><mn  >1</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >1</mn></mtd></mtr><mtr
    ><mtd  ><mn >0</mn></mtd><mtd ><mn  >1</mn></mtd><mtd ><mn  >1</mn></mtd><mtd
    ><mn  >0</mn></mtd></mtr></mtable><mo >]</mo></mrow></mrow><mo >,</mo><mrow ><mi  >δ</mi><mo
    >=</mo><mrow ><mo  >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="2.0pt"  ><mtr
    ><mtd  ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd
    ><mn  >0</mn></mtd></mtr><mtr ><mtd  ><mn >5</mn></mtd><mtd ><mn  >5</mn></mtd><mtd
    ><mn  >5</mn></mtd><mtd ><mn  >5</mn></mtd></mtr><mtr ><mtd  ><mn >8</mn></mtd><mtd
    ><mn  >13</mn></mtd><mtd ><mn  >8</mn></mtd><mtd ><mn  >13</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><ci >𝐌</ci><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><cn type="integer"  >0</cn><cn
    type="integer"  >0</cn><cn type="integer"  >1</cn><cn type="integer"  >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn><cn type="integer" >1</cn><cn type="integer" >0</cn><cn
    type="integer" >1</cn></matrixrow><matrixrow ><cn type="integer"  >0</cn><cn type="integer"  >1</cn><cn
    type="integer"  >1</cn><cn type="integer"  >0</cn></matrixrow></matrix></apply></apply><apply
    ><ci >𝛿</ci><apply  ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow
    ><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn type="integer"  >0</cn><cn
    type="integer"  >0</cn></matrixrow><matrixrow ><cn type="integer" >5</cn><cn type="integer"
    >5</cn><cn type="integer" >5</cn><cn type="integer" >5</cn></matrixrow><matrixrow
    ><cn type="integer"  >8</cn><cn type="integer"  >13</cn><cn type="integer"  >8</cn><cn
    type="integer"  >13</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >{\mathbf{M}}=\left[\begin{array}[]{cccc}{0}&{0}&{1}&{0}\\
    {0}&{1}&{0}&{1}\\ {0}&{1}&{1}&{0}\end{array}\right],\delta=\left[\begin{array}[]{cccc}{0}&{0}&{0}&{0}\\
    {5}&{5}&{5}&{5}\\ {8}&{13}&{8}&{13}\end{array}\right]</annotation></semantics></math>
    |  | (4) |'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \(\mathbf{M}=\left[\begin{array}{cccc}{0}&{0}&{1}&{0}\\ {0}&{1}&{0}&{1}\\ {0}&{1}&{1}&{0}\end{array}\right],
    \delta=\left[\begin{array}{cccc}{0}&{0}&{0}&{0}\\ {5}&{5}&{5}&{5}\\ {8}&{13}&{8}&{13}\end{array}\right]\)
- en: 4 Methods
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: In this section, we will first give an overall review of the relationships among
    the given approaches and comparisons of them and then discuss them individually
    with details. The main deep learning methods we researched for time series imputation
    are GRU-D [[7](#bib.bib7)], GRUI-GAN [[27](#bib.bib27)], E²GAN [[28](#bib.bib28)],
    BRITS [[6](#bib.bib6)] and NAOMI [[25](#bib.bib25)]. All of them are deep learning
    approaches published recently for time series imputation tasks. Among these methods,
    recurrent neural network (RNN) and generative adversarial network (GAN) are main
    architectures that are adopted. The reason is that RNN and its variations (e.g.,
    LSTM, GRU) have been proven powerful in modeling sequence data, while GAN has
    been successfully applied to generation and imputation tasks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将首先对给定方法之间的关系和比较进行总体回顾，然后详细讨论它们。我们研究的主要深度学习方法用于时间序列插补包括 GRU-D [[7](#bib.bib7)]、GRUI-GAN
    [[27](#bib.bib27)]、E²GAN [[28](#bib.bib28)]、BRITS [[6](#bib.bib6)] 和 NAOMI [[25](#bib.bib25)]。这些都是最近发表的用于时间序列插补任务的深度学习方法。在这些方法中，递归神经网络（RNN）和生成对抗网络（GAN）是主要采用的架构。原因是
    RNN 及其变体（例如 LSTM、GRU）在建模序列数据方面已被证明非常有效，而 GAN 已成功应用于生成和插补任务。
- en: 'To describe the relationships among these methods, we illustrate the dependencies
    and common structures of them in Figure [1](#S4.F1 "Figure 1 ‣ 4 Methods ‣ Time
    Series Data Imputation: A Survey on Deep Learning Approaches"). In Figure [1](#S4.F1
    "Figure 1 ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep Learning
    Approaches"), we use arrows to describe the dependencies, for example GRUI-GAN
    improves the work by using GAN while E²GAN is the updated version of GRUI-GAN.
    And we use boxes to describe the common structures among the methods, for example
    GRU-D and BRITS are both pure RNN models and BRITS and NAOMI both adopt bidirectional
    RNN structures. This can help us to understand how the time series imputation
    task is systematically modeled, how the solutions are developed and what progress
    people make in this process. In the following sections, we will take a progressive
    order to review them.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述这些方法之间的关系，我们在图[1](#S4.F1 "图 1 ‣ 4 方法 ‣ 时间序列数据插补：深度学习方法的综述")中说明了它们的依赖关系和共通结构。在图[1](#S4.F1
    "图 1 ‣ 4 方法 ‣ 时间序列数据插补：深度学习方法的综述")中，我们使用箭头来描述依赖关系，例如 GRUI-GAN 通过使用 GAN 改进了工作，而
    E²GAN 是 GRUI-GAN 的更新版本。我们还使用框来描述方法之间的共通结构，例如 GRU-D 和 BRITS 都是纯 RNN 模型，BRITS 和
    NAOMI 都采用双向 RNN 结构。这有助于我们理解时间序列插补任务如何系统地建模，解决方案是如何开发的，以及人们在这个过程中取得了什么进展。在接下来的部分，我们将按渐进的顺序进行审查。
- en: '![Refer to caption](img/fecac3d9d86e0edc2d7899848b8038e9.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fecac3d9d86e0edc2d7899848b8038e9.png)'
- en: 'Figure 1: The relationships among methods we mainly surveyed.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们主要调查的方法之间的关系。
- en: 'Table 2: Characteristics of the chosen methods'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：所选方法的特征
- en: '| Methodologies | Model Prototype | Specific Models |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 方法论 | 模型原型 | 具体模型 |'
- en: '&#124; Auto-Encoder &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自编码器 &#124;'
- en: '&#124; Enhanced &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强 &#124;'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Adversarial Training &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗训练 &#124;'
- en: '&#124; Enhancednced &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强 &#124;'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Bidirectional &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 双向 &#124;'
- en: '&#124; Enhanced &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GRU-D | RNN | GRU | – | – | – |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| GRU-D | RNN | GRU | – | – | – |'
- en: '| GRUI-GAN | Hybrid | GRU+GAN | – | yes | – |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GRUI-GAN | 混合 | GRU+GAN | – | 是 | – |'
- en: '| E2GAN | Hybrid | GRU+GAN | yes | yes | – |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| E2GAN | 混合 | GRU+GAN | 是 | 是 | – |'
- en: '| BRITS | RNN | Bidirectional RNN | – | – | yes |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| BRITS | RNN | 双向 RNN | – | – | 是 |'
- en: '| NAOMI | Hybrid | RNN+GAN | – | yes | yes |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| NAOMI | 混合 | RNN+GAN | – | 是 | 是 |'
- en: 4.1 Characteristics of Chosen Methods
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 所选方法的特征
- en: 'In this section, we give the characteristics of the chosen methods in Table [2](#S4.T2
    "Table 2 ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep Learning
    Approaches") to give a brief introduction and a taxonomy of the chosen methods
    we reviewed. We consider the following criteria:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提供了表格[2](#S4.T2 "表格 2 ‣ 4 方法 ‣ 时间序列数据插补：深度学习方法的综述")中所选方法的特征，以简要介绍和分类我们审查的所选方法。我们考虑以下标准：
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Irregular Time Series Awareness*: time series including regular time series
    with fixed time interval and irregular time series. Both of them are common kinds
    which are important for classifying the using condition of the methods [[54](#bib.bib54),
    [44](#bib.bib44)].'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*不规则时间序列意识*：时间序列包括具有固定时间间隔的规则时间序列和不规则时间序列。这两种都是常见的类型，对于分类方法的使用条件非常重要[[54](#bib.bib54),
    [44](#bib.bib44)]。'
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Model Prototype*: model prototype concludes the overall kind of model in the
    methods, e.g., RNN, GAN and CNN. It is a basic information to classify the model
    type. If the model prototype is hybrid, it means more than 1 kind of prototype
    is employed.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*模型原型*：模型原型总结了方法中的整体模型类型，例如RNN、GAN和CNN。这是对模型类型进行分类的基本信息。如果模型原型是混合的，则意味着使用了多种原型。'
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Specific Models*: specific models introduce the specific kinds of model adopted
    in the methods. The specific models may relate to the basic idea of the methods.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*具体模型*：具体模型介绍了方法中采用的具体模型类型。这些具体模型可能与方法的基本思想有关。'
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Auto-Encoder Enhanced*: auto-encoder structure is an approach that can be
    applied in the imputation of the data. With the structure of encoder and decoder,
    it extracts the features from low-dimensional layers and recovery missing values
    by decoder. Therefore, it can serve as a feature of methods.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*自动编码器增强*：自动编码器结构是一种可以应用于数据填补的方法。通过编码器和解码器的结构，它从低维层中提取特征，并通过解码器恢复缺失值。因此，它可以作为方法的一种特征。'
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Adversarial Training Enhanced*: adversarial training adopts adversarial structure
    (e.g., GAN [[15](#bib.bib15)] and CGAN [[31](#bib.bib31)]) to enhance the model.
    It takes the idea of generative adversarial structure with generator and discriminator.
    Large amount of models can be enhanced with such idea.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*对抗训练增强*：对抗训练采用对抗结构（例如，GAN [[15](#bib.bib15)] 和 CGAN [[31](#bib.bib31)]）来增强模型。它采用生成对抗结构的思想，包括生成器和判别器。大量模型可以通过这种思想得到增强。'
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Bidirectional Enhanced*: Bidirectional RNN trains 2 models in forward direction
    and backward direction respectively with RNN and then combines them into the same
    loss function [[17](#bib.bib17)]. This idea is vital in data imputation tasks
    since both previous series and future series of missing values are known. Therefore,
    bidirectional structure benefits from both backward and forward training processing.
    Such idea is adopted in [[25](#bib.bib25), [6](#bib.bib6)].'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*双向增强*：双向RNN分别在前向和后向方向训练两个模型，然后将它们结合到相同的损失函数中[[17](#bib.bib17)]。这个想法在数据填补任务中至关重要，因为缺失值的前序系列和后序系列都是已知的。因此，双向结构受益于前向和后向训练过程。这种思想被采用在[[25](#bib.bib25)、[6](#bib.bib6)]中。'
- en: 4.2 GRU-D
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 GRU-D
- en: GRU-D is proposed by [[7](#bib.bib7)] as one of the early attempts to impute
    time series with deep learning models. It is the first one among the 5 researched
    paper to systematically model missing patterns into RNN for time series classification
    problems. It is also the first research to exploit that, RNN can model multivariable
    time series with the informativeness from the time series. Former works like [[23](#bib.bib23),
    [8](#bib.bib8)] attempted to impute missing values with RNN by concatenating timestamps
    and raw data, i.e., regard timestamps as one attribute of raw data. But in [[7](#bib.bib7)],
    the concept time lag is first proposed. In this paper, Gated Recurrent Unit (GRU)
    is first adopted to generate missing values. In each layer of GRU, since the input
    can contain missing values, they replace the input $x_{t_{i}}^{j}$ with a combination
    of the existing values $x_{t_{i}}^{j}$ and statistical values, element-wise multiplied
    with $\mathbf{M}$ and $\mathbf{1}-\mathbf{M}$ respectively.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GRU-D 是由 [[7](#bib.bib7)] 提出的，作为用深度学习模型填补时间序列的早期尝试之一。它是在5篇研究论文中首个系统地将缺失模式建模为RNN用于时间序列分类问题的研究。它也是首个利用RNN根据时间序列的信息建模多变量时间序列的研究。以前的工作如
    [[23](#bib.bib23)、[8](#bib.bib8)] 尝试通过将时间戳和原始数据拼接来用RNN填补缺失值，即将时间戳视为原始数据的一个属性。但在
    [[7](#bib.bib7)] 中，首次提出了时间滞后概念。本文首次采用门控递归单元（GRU）来生成缺失值。在每一层GRU中，由于输入可能包含缺失值，他们用现有值
    $x_{t_{i}}^{j}$ 和统计值的组合替换输入 $x_{t_{i}}^{j}$，分别与 $\mathbf{M}$ 和 $\mathbf{1}-\mathbf{M}$
    逐元素相乘。
- en: '|  | $x_{t_{i}}^{j}\leftarrow m_{t_{i}}^{j}x_{t_{i}}^{j}+\left(1-m_{t_{i}}^{j}\right)\tilde{x}^{j}$
    |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t_{i}}^{j}\leftarrow m_{t_{i}}^{j}x_{t_{i}}^{j}+\left(1-m_{t_{i}}^{j}\right)\tilde{x}^{j}$
    |  |'
- en: where $\tilde{x}$ can be one of the mean value, last observed value or concatenation
    of $\left[\mathbf{x_{i}};\mathbf{m_{i}};\delta_{i}\right]$.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{x}$ 可以是均值、最后观察值或 $\left[\mathbf{x_{i}};\mathbf{m_{i}};\delta_{i}\right]$
    的拼接。
- en: The main contribution of this paper is the GRU based model GRU-D and the proposition
    of decay rate. To address the imputation of the missing values, they discover
    that
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献是基于GRU的模型GRU-D和衰减率的提出。为了处理缺失值的填补，他们发现
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The missing variables tend to be close to some default value if its last observation
    happens a long time ago.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果缺失变量的最后观察发生在很久以前，那么这些缺失变量往往接近某个默认值。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The influence of the input variables will fade away over time if the variable
    has been missing for a while.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果变量缺失了一段时间，那么输入变量的影响会随着时间的推移而逐渐消失。
- en: And then they propose decay rate $\gamma$, which is defined as below
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后他们提出了衰减率$\gamma$，其定义如下：
- en: '|  | $\gamma_{t_{i}}=\exp({-\max{(\mathbf{0},\mathbf{W}_{\gamma}\mathbf{\delta_{t_{i}}})}})$
    |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\gamma_{t_{i}}=\exp({-\max{(\mathbf{0},\mathbf{W}_{\gamma}\mathbf{\delta_{t_{i}}})}})$
    |  |'
- en: The decay rate tries to model the impact of the other values have on the missing
    values. In brief, it guarantees that the larger the time intervals are, the less
    their influence on imputing the missing values. And then they replace the input
    variable as
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减率尝试建模其他值对缺失值的影响。简而言之，它保证了时间间隔越长，它们对缺失值的影响就越小。然后他们将输入变量替换为：
- en: '|  | $x_{t_{i}}^{j}\leftarrow m_{t_{i}}^{j}x_{t_{j}}^{j}+\left(1-m_{t_{i}}^{j}\right)\gamma_{\boldsymbol{x}_{t_{i}}}^{j}x_{t_{i}^{\prime}}^{j}+\left(1-m_{t_{i}}^{j}\right)\left(1-\gamma_{\boldsymbol{x}_{t_{i}}}^{j}\right)\tilde{x}^{j}$
    |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t_{i}}^{j}\leftarrow m_{t_{i}}^{j}x_{t_{j}}^{j}+\left(1-m_{t_{i}}^{j}\right)\gamma_{\boldsymbol{x}_{t_{i}}}^{j}x_{t_{i}^{\prime}}^{j}+\left(1-m_{t_{i}}^{j}\right)\left(1-\gamma_{\boldsymbol{x}_{t_{i}}}^{j}\right)\tilde{x}^{j}$
    |  |'
- en: 'Therefore, as illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4.2 GRU-D ‣ 4 Methods
    ‣ Time Series Data Imputation: A Survey on Deep Learning Approaches"), the GRU-D
    model is proposed with 2 different trainable decays $\gamma_{\boldsymbol{x}}$
    and $\gamma_{\boldsymbol{h}}$, where $\gamma_{\boldsymbol{x}}$ is the input decay
    rate and the $\gamma_{\boldsymbol{h}}$ is the decay rate for the hidden state.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，如图[2](#S4.F2 "Figure 2 ‣ 4.2 GRU-D ‣ 4 Methods ‣ Time Series Data Imputation:
    A Survey on Deep Learning Approaches")所示，GRU-D模型提出了两个不同的可训练衰减因子$\gamma_{\boldsymbol{x}}$和$\gamma_{\boldsymbol{h}}$，其中$\gamma_{\boldsymbol{x}}$是输入衰减率，$\gamma_{\boldsymbol{h}}$是隐藏状态的衰减率。'
- en: '![Refer to caption](img/3c61c7aaf1f07b04f443ed06054d0b08.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/3c61c7aaf1f07b04f443ed06054d0b08.png)'
- en: (a) GRU
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GRU
- en: '![Refer to caption](img/ff1741a5be6147658fecc52be8eb3f17.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/ff1741a5be6147658fecc52be8eb3f17.png)'
- en: (b) GRU-D
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GRU-D
- en: 'Figure 2: Model of GRU and GRU-D. Images extracted from [[7](#bib.bib7)].'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：GRU和GRU-D模型。图像摘自[[7](#bib.bib7)]。
- en: 4.3 GRUI-GAN
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 GRUI-GAN
- en: 'In [[27](#bib.bib27)], GRU-I is proposed as the recurrent unit to capture the
    time information. As Figure [3](#S4.F3 "Figure 3 ‣ 4.3 GRUI-GAN ‣ 4 Methods ‣
    Time Series Data Imputation: A Survey on Deep Learning Approaches") illustrates,
    it follows the structure of GRU-D in Section [4.2](#S4.SS2 "4.2 GRU-D ‣ 4 Methods
    ‣ Time Series Data Imputation: A Survey on Deep Learning Approaches") with the
    removal of the input decay. Therefore, there is no innovation in the RNN part
    as well as the decay rate.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[27](#bib.bib27)]中，GRU-I被提出作为捕捉时间信息的递归单元。如图[3](#S4.F3 "Figure 3 ‣ 4.3 GRUI-GAN
    ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep Learning Approaches")所示，它遵循了第[4.2](#S4.SS2
    "4.2 GRU-D ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep Learning
    Approaches")节中GRU-D的结构，但移除了输入衰减。因此，RNN部分及衰减率没有创新。'
- en: 'The main contribution of this paper locates in the GAN structure. Figure [4](#S4.F4
    "Figure 4 ‣ 4.3 GRUI-GAN ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on
    Deep Learning Approaches") shows the structure. The Generative Adversarial Network
    (GAN) structure is made up of a generator (G) and a discriminator (D). The G learns
    a mapping $G(z)$ that tries to map the random noise vector $z$ to realistic time
    series. The D tries to find a mapping $D(.)$ that tells us the input data’s probability
    of being real. Therefore, in this paper, the model takes a random noise as the
    input of the GAN model, which means the generating is a random process. Both G
    and D are based on GRU-I, and it takes lots of time to train the model to get
    the data imputed.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的主要贡献在于GAN结构。图[4](#S4.F4 "Figure 4 ‣ 4.3 GRUI-GAN ‣ 4 Methods ‣ Time Series
    Data Imputation: A Survey on Deep Learning Approaches")展示了该结构。生成对抗网络（GAN）结构由生成器（G）和鉴别器（D）组成。G学习一个映射$G(z)$，试图将随机噪声向量$z$映射到真实的时间序列。D试图找到一个映射$D(.)$，告诉我们输入数据的真实性概率。因此，本文模型将随机噪声作为GAN模型的输入，这意味着生成是一个随机过程。G和D都基于GRU-I，并且训练模型以获得数据填补需要大量时间。'
- en: The GRUI-GAN takes advantage of the ability of GAN in imputation, which has
    been proven powerful in image imputation such as [[34](#bib.bib34)]. And the adversarial
    structure improves accuracy. Moreover, the paper adopts a WGAN structure, which
    improves the stability of the learning stage, get out of the problem of mode collapse
    and makes it easy for the optimization of the GAN model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: GRUI-GAN 利用了 GAN 在插补方面的能力，这在图像插补中已被证明有效，如 [[34](#bib.bib34)]。对抗结构提高了准确性。此外，论文采用了
    WGAN 结构，改善了学习阶段的稳定性，解决了模式崩溃的问题，并简化了 GAN 模型的优化。
- en: However, this model is not practical since the accuracy of the generative model
    seems not stable with a random noise input. And it also makes the model hard to
    converge.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个模型并不实用，因为生成模型的准确性在随机噪声输入下似乎不稳定，而且使得模型难以收敛。
- en: '![Refer to caption](img/53eb5aed6bc610fff50df774918704cb.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/53eb5aed6bc610fff50df774918704cb.png)'
- en: (a) GRU
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GRU
- en: '![Refer to caption](img/ff81fe650de8f326e56d7bb532b315e6.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ff81fe650de8f326e56d7bb532b315e6.png)'
- en: (b) GRU-I
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GRU-I
- en: 'Figure 3: Model of GRU and GRU-I. Images extracted from [[27](#bib.bib27)].'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GRU 和 GRU-I 的模型。图像摘自 [[27](#bib.bib27)]。
- en: '![Refer to caption](img/9bd2a0bb0203767ee55d8adcb0938bb9.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bd2a0bb0203767ee55d8adcb0938bb9.png)'
- en: 'Figure 4: The structure of the GRUI-GAN. Image extracted from [[27](#bib.bib27)].'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：GRUI-GAN 的结构。图像摘自 [[27](#bib.bib27)]。
- en: '![Refer to caption](img/56f0b4658042ad96fadb2b36a1738342.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56f0b4658042ad96fadb2b36a1738342.png)'
- en: 'Figure 5: The structure of the BRITS. Image extracted from [[6](#bib.bib6)].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：BRITS 的结构。图像摘自 [[6](#bib.bib6)]。
- en: 4.4 BRITS
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 BRITS
- en: 'Unlike former methods, BRITS [[6](#bib.bib6)] is totally based on RNN structure
    and proposes imputation with unidirectional dynamics. Time lag (corresponding
    to ”time gaps” in [[6](#bib.bib6)]) is also employed since the time series may
    be irregular. Similar to the idea of decay rate $\gamma$ from GRU-D introduced
    in Section [4.2](#S4.SS2 "4.2 GRU-D ‣ 4 Methods ‣ Time Series Data Imputation:
    A Survey on Deep Learning Approaches"), they propose temporal decay factor $\gamma_{t}=\exp{(-max\left(0,\mathbf{W}_{\gamma}\delta_{t}+\mathbf{b}_{\gamma}\right))}$.
    Compared to GRU-D where the time lags are considered in input and serve as the
    decay rate, in BRITS the hidden states update with the decay rate $\gamma$. It
    means when updating the hidden state, the old hidden state decays according to
    the time duration recorded in the time lags. Hence, the model is updated by:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '与以往方法不同，BRITS [[6](#bib.bib6)] 完全基于 RNN 结构，并提出了具有单向动态的插补。由于时间序列可能是不规则的，还采用了时间滞后（对应于
    [[6](#bib.bib6)] 中的“时间间隙”）。类似于第 [4.2](#S4.SS2 "4.2 GRU-D ‣ 4 Methods ‣ Time Series
    Data Imputation: A Survey on Deep Learning Approaches") 节中介绍的 GRU-D 的衰减率 $\gamma$
    概念，他们提出了时间衰减因子 $\gamma_{t}=\exp{(-\max\left(0,\mathbf{W}_{\gamma}\delta_{t}+\mathbf{b}_{\gamma}\right))}$。与在
    BRITS 中时间滞后被视为输入并充当衰减率不同，GRU-D 将时间滞后考虑在输入中。在 BRITS 中，隐藏状态的更新采用了衰减率 $\gamma$。这意味着在更新隐藏状态时，旧的隐藏状态根据记录在时间滞后的时间持续时间进行衰减。因此，模型的更新方式为：'
- en: '|  | $\displaystyle\hat{\mathbf{x}}_{t}$ | $\displaystyle=\mathbf{W}_{x}\mathbf{h}_{t-1}+\mathbf{b}_{x}$
    |  | (5) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\mathbf{x}}_{t}$ | $\displaystyle=\mathbf{W}_{x}\mathbf{h}_{t-1}+\mathbf{b}_{x}$
    |  | (5) |'
- en: '|  | $\displaystyle\mathbf{x}_{t}^{c}$ | $\displaystyle=\mathbf{m}_{t}\odot\mathbf{x}_{t}+\left(1-\mathbf{m}_{t}\right)\odot\hat{\mathbf{x}}_{t}$
    |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{x}_{t}^{c}$ | $\displaystyle=\mathbf{m}_{t}\odot\mathbf{x}_{t}+\left(1-\mathbf{m}_{t}\right)\odot\hat{\mathbf{x}}_{t}$
    |  |'
- en: '|  | $\displaystyle\gamma_{t}$ | $\displaystyle=\exp\left\{-\max\left(0,\mathbf{W}_{\gamma}\delta_{t}+\mathbf{b}_{\gamma}\right)\right\}$
    |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\gamma_{t}$ | $\displaystyle=\exp\left\{-\max\left(0,\mathbf{W}_{\gamma}\delta_{t}+\mathbf{b}_{\gamma}\right)\right\}$
    |  |'
- en: '|  | $\displaystyle\mathbf{h}_{t}$ | $\displaystyle=\sigma\left(\mathbf{W}_{h}\left[\mathbf{h}_{t-1}\odot\gamma_{t}\right]+\mathbf{U}_{h}\left[\mathbf{x}_{t}^{c}\circ\mathbf{m}_{t}\right]+\mathbf{b}_{h}\right)$
    |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}_{t}$ | $\displaystyle=\sigma\left(\mathbf{W}_{h}\left[\mathbf{h}_{t-1}\odot\gamma_{t}\right]+\mathbf{U}_{h}\left[\mathbf{x}_{t}^{c}\circ\mathbf{m}_{t}\right]+\mathbf{b}_{h}\right)$
    |  |'
- en: '|  | $\displaystyle\ell_{t}$ | $\displaystyle=\left\langle\mathbf{m}_{t},\mathcal{L}_{e}\left(\mathbf{x}_{t},\hat{\mathbf{x}}_{t}\right)\right\rangle$
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ell_{t}$ | $\displaystyle=\left\langle\mathbf{m}_{t},\mathcal{L}_{e}\left(\mathbf{x}_{t},\hat{\mathbf{x}}_{t}\right)\right\rangle$
    |  |'
- en: The former model named RITS is the unidirectional version of the proposed methods
    in [[6](#bib.bib6)]. As the bidirectional version, BRITS employs bidirectional
    RNN by utilizing the bidirectional recurrent dynamics, i.e., they train 2 models
    in forward direction and backward direction respectively [[17](#bib.bib17)]. Thus
    consistency loss is introduced to take the losses of both directions into consideration.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 之前名为RITS的模型是[[6](#bib.bib6)]中提出的方法的单向版本。作为双向版本，BRITS利用双向递归动态，即分别训练两个前向和后向模型[[17](#bib.bib17)]。因此，引入了一致性损失来考虑两个方向的损失。
- en: To conclude, in BRITS, time lags are still adopted to deal with irregular time
    series. Only RNN is used to model the time series. We can also conclude from the
    model and the experiments that bidirectional RNN contributes to a higher performance
    since the unidirectional model may suffer from bias exploding problem [[4](#bib.bib4)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在BRITS中，时间滞后仍然被用来处理不规则时间序列。仅使用RNN来建模时间序列。从模型和实验中我们还可以得出结论，双向RNN有助于提高性能，因为单向模型可能会遭遇梯度爆炸问题
    [[4](#bib.bib4)]。
- en: '![Refer to caption](img/e02494da76278d94536185e6a859e02c.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e02494da76278d94536185e6a859e02c.png)'
- en: 'Figure 6: The structure of the E²GAN. Image extracted from [[28](#bib.bib28)].'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：E²GAN的结构。图像摘自[[28](#bib.bib28)]。
- en: 4.5 E²GAN
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 E²GAN
- en: 'E²GAN [[28](#bib.bib28)] is another work based on GAN. While the GRUI-GAN in
    Section [4.3](#S4.SS3 "4.3 GRUI-GAN ‣ 4 Methods ‣ Time Series Data Imputation:
    A Survey on Deep Learning Approaches") takes a random noise vector as input, which
    takes lots of time to train, E²GAN adopts an auto-encoder structure based on GRUI
    to form the generator. The overall structure of their model is in Figure [6](#S4.F6
    "Figure 6 ‣ 4.4 BRITS ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep
    Learning Approaches").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'E²GAN [[28](#bib.bib28)] 是另一个基于GAN的工作。虽然第[4.3节](#S4.SS3 "4.3 GRUI-GAN ‣ 4 Methods
    ‣ Time Series Data Imputation: A Survey on Deep Learning Approaches")中的GRUI-GAN采用随机噪声向量作为输入，这需要大量时间进行训练，但E²GAN采用基于GRUI的自编码器结构来形成生成器。他们模型的整体结构见图[6](#S4.F6
    "Figure 6 ‣ 4.4 BRITS ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep
    Learning Approaches")。'
- en: In E²GAN, concepts including mask, time lag, decay rate and GRUI are all reserved
    without improvement, thus there is no innovation in the GRUI structure. The main
    contribution is the auto-encoder structure they adopt in the generator. This is
    a common strategy taken by image generation and imputation such as Context-Encoder
    [[34](#bib.bib34)], PixelGANs [[19](#bib.bib19)], but not a common strategy in
    RNN based GAN. Since the input of the model is the original time series, the model
    compresses the input incomplete time series $\mathbf{X}$ into a low-dimensional
    vector $z$ with the help of the GRUI. And then the reconstructing part will reconstruct
    the complete time series $\mathbf{X^{\prime}}$ to fool the discriminator. And
    the discriminator of the method attempts to distinguish actual incomplete time
    series $\mathbf{X}$ and the fake but complete sample $\mathbf{X^{\prime}}$ through
    the adoption of recursive neural network. The framework of the discriminator is
    also an encoder.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在E²GAN中，诸如掩码、时间滞后、衰减率和GRUI等概念都保留了，没有改进，因此GRUI结构没有创新。主要贡献是他们在生成器中采用的自编码器结构。这是一种图像生成和填补常用的策略，如Context-Encoder
    [[34](#bib.bib34)]、PixelGANs [[19](#bib.bib19)]，但在基于RNN的GAN中并不常见。由于模型的输入是原始时间序列，模型通过GRUI将输入的不完整时间序列$\mathbf{X}$压缩为一个低维向量$z$。然后，重建部分会重建完整时间序列$\mathbf{X^{\prime}}$以欺骗判别器。该方法的判别器试图通过采用递归神经网络来区分实际的不完整时间序列$\mathbf{X}$和假但完整的样本$\mathbf{X^{\prime}}$。判别器的框架也是一个编码器。
- en: E²GAN takes an encoder-decoder RNN based structure as the generator, which tackles
    the difficulty of training the model and the accuracy. So far, according to the
    experiments in the paper, E²GAN has achieved state-of-the-art and outperforms
    other existing methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: E²GAN采用了基于编码器-解码器RNN的结构作为生成器，这解决了训练模型的难度和准确性。目前，根据论文中的实验，E²GAN已经达到了最先进的水平，并且优于其他现有方法。
- en: 4.6 NAOMI
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 NAOMI
- en: 'NAOMI (Non-AutOregressive Multiresolution Imputation [[25](#bib.bib25)]) proposes
    a non-autoregressive model which conditions both previous values but also future
    values, i.e., equipped with bidirectional RNN like BRITS introduced in Section [4.4](#S4.SS4
    "4.4 BRITS ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep Learning
    Approaches"). Since in the imputation tasks, future values and historical values
    are both observed, the intuition is to take advantage of both values and train
    bidirectional models for them. As illustrated in Figure [7](#S4.F7 "Figure 7 ‣
    4.6 NAOMI ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep Learning
    Approaches"), $f_{f}$ and $f_{b}$ are forward and backward RNN respectively, thus
    the hidden state $h_{t}$ is a joint hidden state concatenated by $h^{f}_{t}$ and
    $h^{b}_{t}$.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'NAOMI（非自回归多分辨率插补[[25](#bib.bib25)]）提出了一种非自回归模型，该模型不仅条件化于之前的值，还条件化于未来的值，即，配备了双向RNN，如第[4.4](#S4.SS4
    "4.4 BRITS ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep Learning
    Approaches")节中介绍的BRITS。由于在插补任务中，未来值和历史值都被观察到，因此直觉是利用这两者，并为它们训练双向模型。如图[7](#S4.F7
    "Figure 7 ‣ 4.6 NAOMI ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep
    Learning Approaches")所示，$f_{f}$和$f_{b}$分别是前向和后向RNN，因此隐藏状态$h_{t}$是由$h^{f}_{t}$和$h^{b}_{t}$拼接而成的联合隐藏状态。'
- en: 'Moreover, a special predicting strategy is performed in this paper. They adopt
    a *divide and conquer strategy*. As it is shown in Figure [7](#S4.F7 "Figure 7
    ‣ 4.6 NAOMI ‣ 4 Methods ‣ Time Series Data Imputation: A Survey on Deep Learning
    Approaches"), with 2 known values $x_{1}$ and $x_{5}$, they first predict the
    midpoint $x_{3}$ by $x_{1}$ and $x_{5}$ with proposed bidirectional RNN models,
    and then $x_{3}$ is updated and utilized to predict $x_{2}$ and $x_{4}$ respectively.
    Thus a fine-grained prediction is performed. Finally, adversarial training is
    taken to enhance the model.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '此论文中执行了一种特殊的预测策略。他们采用了*分而治之策略*。如图[7](#S4.F7 "Figure 7 ‣ 4.6 NAOMI ‣ 4 Methods
    ‣ Time Series Data Imputation: A Survey on Deep Learning Approaches")所示，给定两个已知值$x_{1}$和$x_{5}$，他们首先利用提出的双向RNN模型预测中点$x_{3}$，然后更新$x_{3}$并利用它分别预测$x_{2}$和$x_{4}$。因此，进行了精细的预测。最后，采用对抗训练以增强模型。'
- en: However, in NAOMI, time gaps are ignored and the data is injected into the RNN
    model without timestamps. It suggests the model is not aware of irregular time
    series although we can still take them as input by removing their timestamps directly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在NAOMI中，时间间隔被忽略，数据被注入到没有时间戳的RNN模型中。这表明该模型虽然可以通过直接移除时间戳来将其作为输入，但并不意识到不规则时间序列。
- en: '![Refer to caption](img/8b7e6e36cd299753cb0a9c296a54019b.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8b7e6e36cd299753cb0a9c296a54019b.png)'
- en: 'Figure 7: The structure of the NAOMI. Image extracted from [[25](#bib.bib25)].'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：NAOMI的结构。图像摘自[[25](#bib.bib25)]。
- en: 5 Conclusion
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this paper, we give a brief introduction to the imputation methods for time
    series. We propose that existing methods can be classified into 3 main classed:
    deletion methods, traditional methods, and learning based methods. And we introduce
    our classification in detail. Moreover, we investigate existing deep learning
    methods for time series imputation, since they outperform others and make great
    progress recently. We mainly researched 3 deep learning methods including GRU-D,
    GRUI-GAN, and E²GAN. All of them based on RNN, and the latter two also adopt GAN
    for more accurate imputation. We also find the relationships among them: GRUI-GAN
    is based on the definitions from GRU-D, and E²GAN improves the generator of the
    GRUI-GAN with auto-encoder. And so far, E²GAN achieves state-of-the-art.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们简要介绍了时间序列的插补方法。我们提出现有的方法可以分为三大类：删除方法、传统方法和基于学习的方法。并详细介绍了我们的分类。此外，我们调查了现有的深度学习时间序列插补方法，因为它们在最近的进展中表现优越。我们主要研究了三种深度学习方法，包括GRU-D、GRUI-GAN和E²GAN。它们都基于RNN，后两者还采用了GAN以实现更准确的插补。我们还发现它们之间的关系：GRUI-GAN基于GRU-D的定义，E²GAN通过自编码器改进了GRUI-GAN的生成器。到目前为止，E²GAN实现了最先进的技术。
- en: Since the imputation problem is fundamental, we believe with these methods,
    the filled data would benefit downstream applications in many aspects. And as
    we observed, most of the techniques in other fields can be adopted in this task
    since time series data is everywhere. In the future, we would like to see the
    time information can be utilized properly, and the methods can be more general
    and accurate so that we would not need to choose the best one from too many methods,
    and the missing data of the time series would not be a problem.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于插补问题是基础性的，我们相信这些方法可以使填充数据在许多方面有利于下游应用。正如我们观察到的那样，其他领域中的大多数技术可以被采用于此任务，因为时间序列数据无处不在。未来，我们希望看到时间信息能得到妥善利用，方法能更加通用和准确，这样我们就不需要从众多方法中选择最佳方法，时间序列的缺失数据也不会成为问题。
- en: 6 Future Research Opportunities
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来研究机会
- en: Based on our observation from surveying the development of time series imputation
    methodologies, we try to highlight some potential research opportunities in this
    field. Most existing researches mainly focus on the structure of RNN and try to
    use bidirectional RNN, Auto-Encoder structure and GAN to enhance the model. With
    the rapid development in the deep learning society (especially Natural Language
    Processing (NLP) where time series are also highly concerned), some techniques
    have reached better performance (e.g., attention models). These models can be
    considered to enhance the imputation models. Further, most existing methods ignore
    the missing of timestamps which can also appear obscurely [[36](#bib.bib36)].
    Therefore, there is still demand for such techniques. Existing methods can be
    extended to impute missing timestamps. Moreover, query answering without directly
    imputing missing values is another perspective of dealing with missing values.
    Under such scenarios, specific values do not need imputation, and consistent queries
    in inconsistent probabilistic databases should be generated.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们对时间序列插补方法发展情况的观察，我们尝试突显该领域的一些潜在研究机会。大多数现有研究主要关注RNN的结构，并尝试使用双向RNN、Auto-Encoder结构和GAN来增强模型。随着深度学习社会（尤其是自然语言处理（NLP），其中时间序列也受到高度关注）的快速发展，一些技术已达到了更好的性能（例如，注意力模型）。这些模型可以被认为用于增强插补模型。此外，大多数现有方法忽略了缺失时间戳的问题，这可能也会显得模糊[[36](#bib.bib36)]。因此，对此类技术仍有需求。现有方法可以扩展到插补缺失的时间戳。此外，不直接插补缺失值的查询回答是处理缺失值的另一种视角。在这种情况下，特定值不需要插补，而在不一致的概率数据库中应生成一致的查询。
- en: 6.1 Attention Mechanism Enhanced
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 注意力机制增强
- en: In recent years, the attention mechanism has been shown successful in deep learning
    society, especially in NLP fields. When adopted in RNN, the attention mechanism
    allocates weights for each hidden state to draw information from the sequence.
    With such mechanism, the model is improved to capture latent patterns in historical
    data, thus may benefit time series imputation. Compared to existing RNN models
    (e.g., LSTM and GRU) which already take long-term dependencies into consideration,
    the attention mechanism for instance temporal attention enables the model to see
    features and status globally. However, LSTM and GRU will still lose long-term
    information due to the forget gate unit.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，注意力机制在深度学习领域特别是在NLP领域表现成功。当在RNN中采用时，注意力机制为每个隐藏状态分配权重，以从序列中提取信息。通过这种机制，模型可以更好地捕捉历史数据中的潜在模式，从而可能有利于时间序列插补。与已经考虑长期依赖的现有RNN模型（例如LSTM和GRU）相比，注意力机制，例如时间注意力，使模型能够全球性地看到特征和状态。然而，LSTM和GRU由于遗忘门单元仍然会丢失长期信息。
- en: 'Recently, pure attention models are proposed without RNN. The Transformer proposed
    in [[47](#bib.bib47)] is one of the popular frameworks. In the proposed Transformer
    framework, it only adopts an attention layer called self-attention, which is computed
    as:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，纯注意力模型被提出而没有使用RNN。[[47](#bib.bib47)]中提出的Transformer是其中一个流行的框架。在所提出的Transformer框架中，它只采用了一个称为自注意力的注意力层，其计算方式为：
- en: '|  | $\operatorname{Attention}(Q,K,V)=\operatorname{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$
    |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{Attention}(Q,K,V)=\operatorname{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$
    |  |'
- en: where $Q,K,V$ are queries, keys and values respectively, and $d_{k}$ is the
    dimension of the input.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q,K,V$ 分别是查询、键和值，而 $d_{k}$ 是输入的维度。
- en: 'Accepting a single sequence as input, the self-attention mechanism relates
    different positions of the input and tries to compute a representation of the
    sequence. Without applying RNN, the Transformer relies entirely on the self-attention
    layers to former an encoder-decoder structure, which is similar to the auto-encoder
    introduced in Section [4.1](#S4.SS1 "4.1 Characteristics of Chosen Methods ‣ 4
    Methods ‣ Time Series Data Imputation: A Survey on Deep Learning Approaches").
    Such a structure provides the ability to extract high-dimensional features for
    reconstructing, which benefits tasks like machine translation introduced in [[47](#bib.bib47)].'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '接受单个序列作为输入，自注意力机制将输入的不同位置进行关联，并尝试计算序列的表示。无需应用 RNN，Transformer 完全依赖于自注意力层形成编码器-解码器结构，这类似于第[4.1](#S4.SS1
    "4.1 Characteristics of Chosen Methods ‣ 4 Methods ‣ Time Series Data Imputation:
    A Survey on Deep Learning Approaches")节中介绍的自编码器。这种结构提供了提取高维特征以进行重建的能力，有利于机器翻译等任务，如[[47](#bib.bib47)]中介绍的任务。'
- en: For improving the performance of data imputation, due to the effectiveness of
    the attention mechanisms, models based on attention mechanisms may also address
    the time series imputation problems. And two aforementioned categories of the
    attention mechanisms including temporal attention and self-attention are both
    potential techniques which may benefit the time series imputation. Moreover, with
    the idea of removing RNN and leveraging only attention mechanisms, structures
    like the Transformer may contribute to a new framework for the imputation tasks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高数据补全的性能，由于注意力机制的有效性，基于注意力机制的模型也可能解决时间序列补全问题。上述两类注意力机制，包括时间注意力和自注意力，都是可能对时间序列补全有益的潜在技术。此外，利用去除
    RNN 仅依靠注意力机制的思想，像 Transformer 这样的结构可能为补全任务提供新的框架。
- en: To summary, two categories of attention mechanisms including temporal attention
    and self-attention may bring future opportunities on time series imputation. And
    the pure attention frameworks are also new directions to model time series.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，两类注意力机制，包括时间注意力和自注意力，可能会为时间序列补全带来未来的机遇。而纯粹的注意力框架也是建模时间序列的新方向。
- en: 6.2 Imputing Missing Timestamps
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 补全缺失的时间戳
- en: Missing timestamps often appear obscurely [[36](#bib.bib36)], e.g., denoted
    by 00:00:00. Most of existing methods mainly focus on the missing values of the
    time series. However, once timestamps are missing, these methods may fail to capture
    the information of time and unable to obtain accurate imputation results. Thus,
    an extension of existing methods to impute missing timestamps is potentially appropriate
    direction to deal with such scenarios.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失的时间戳往往表现得很隐蔽[[36](#bib.bib36)]，例如，被标记为00:00:00。现有的大多数方法主要关注时间序列中的缺失值。然而，一旦时间戳缺失，这些方法可能无法捕捉时间信息，从而无法获得准确的补全结果。因此，扩展现有方法以补全缺失的时间戳是处理此类情况的潜在合适方向。
- en: 6.3 Consistent Query Answering
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 一致性查询回答
- en: Following [[22](#bib.bib22)], query answering without determining the specific
    imputation of each missing value is crucial in probabilistic databases [[10](#bib.bib10)],
    when data from many sources can be inconsistent and uncertain. Therefore, consistent
    query answering (CQA) is needed. Missing values data in CQA problem increase the
    difficulty of answering the query consistently. Both the inconsistent data from
    different sources and missing values should be considered. Therefore, a combination
    of data imputation methods and CQA methods can be a potential approach.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[22](#bib.bib22)]，在概率数据库[[10](#bib.bib10)]中进行查询回答时，不确定每个缺失值的具体补全方式是至关重要的，因为来自多个来源的数据可能不一致且不确定。因此，需要一致性查询回答（CQA）。CQA
    问题中的缺失值数据增加了一致回答查询的难度。应考虑来自不同来源的不一致数据和缺失值。因此，数据补全方法和 CQA 方法的结合可能是一种潜在的解决方案。
- en: References
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] E. Acuna and C. Rodriguez. The treatment of missing values and its effect
    on classifier accuracy. In Classification, clustering, and data mining applications,
    pages 639–647\. Springer, 2004.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] E. Acuna 和 C. Rodriguez. 缺失值处理及其对分类器准确性的影响。在 Classification, clustering,
    and data mining applications, 页码 639–647. Springer, 2004.'
- en: '[2] M. Amiri and R. Jensen. Missing data imputation using fuzzy-rough methods.
    Neurocomputing, 205:152–164, 2016.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. Amiri 和 R. Jensen. 使用模糊-粗糙方法进行缺失数据补全。Neurocomputing, 205:152–164, 2016.'
- en: '[3] G. E. Batista, M. C. Monard, et al. A study of k-nearest neighbour as an
    imputation method. HIS, 87(251-260):48, 2002.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] G. E. Batista, M. C. Monard 等人. k-最近邻作为补全方法的研究。HIS, 87(251-260):48, 2002.'
- en: '[4] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for
    sequence prediction with recurrent neural networks. In Advances in Neural Information
    Processing Systems 28: Annual Conference on Neural Information Processing Systems
    2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1171–1179, 2015.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung. Time series analysis:
    forecasting and control. John Wiley & Sons, 2015.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] W. Cao, D. Wang, J. Li, H. Zhou, L. Li, and Y. Li. BRITS: bidirectional
    recurrent imputation for time series. In Advances in Neural Information Processing
    Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS
    2018, 3-8 December 2018, Montréal, Canada, pages 6776–6786, 2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural
    networks for multivariate time series with missing values. Scientific reports,
    8(1):6085, 2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] E. Choi, M. T. Bahadori, A. Schuetz, W. F. Stewart, and J. Sun. Doctor
    ai: Predicting clinical events via recurrent neural networks. In Machine Learning
    for Healthcare Conference, pages 301–318, 2016.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] W. S. Cleveland and C. Loader. Smoothing by Local Regression: Principles
    and Methods, pages 10–49. Physica-Verlag HD, Heidelberg, 1996.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] N. N. Dalvi and D. Suciu. Efficient query evaluation on probabilistic
    databases. VLDB J., 16(4):523–544, 2007.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] C. Fang, S. Song, Z. Chen, and A. Gui. Fine-grained fuel consumption prediction.
    In Proceedings of the 28th ACM International Conference on Information and Knowledge
    Management, CIKM 2019, Beijing, China, November 3-7, 2019, pages 2783–2791, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] P. J. García-Laencina, P. H. Abreu, M. H. Abreu, and N. Afonoso. Missing
    data imputation on the 5-year survival prediction of breast cancer patients with
    unknown discrete values. Comp. in Bio. and Med., 59:125–133, 2015.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P. J. García-Laencina, J. Sancho-Gómez, and A. R. Figueiras-Vidal. Pattern
    classification with missing data: a review. Neural Computing and Applications,
    19(2):263–282, 2010.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Z. Ghahramani and M. I. Jordan. Supervised learning from incomplete data
    via an EM approach. In J. D. Cowan, G. Tesauro, and J. Alspector, editors, Advances
    in Neural Information Processing Systems 6, [7th NIPS Conference, Denver, Colorado,
    USA, 1993], pages 120–127\. Morgan Kaufmann, 1993.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. C. Courville, and Y. Bengio. Generative adversarial nets. In Advances
    in Neural Information Processing Systems 27: Annual Conference on Neural Information
    Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2672–2680,
    2014.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. W. Graham. Missing data analysis: Making it work in the real world.
    Annual review of psychology, 60:549–576, 2009.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Graves and J. Schmidhuber. Framewise phoneme classification with bidirectional
    LSTM and other neural network architectures. Neural Networks, 18(5-6):602–610,
    2005.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T. Hsieh, H. Hsiao, and W. Yeh. Forecasting stock markets using wavelet
    transforms and recurrent neural networks: An integrated system based on artificial
    bee colony algorithm. Appl. Soft Comput., 11(2):2510–2525, 2011.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation
    with conditional adversarial networks. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 1125–1134, 2017.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] M. Kantardzic. Data mining: concepts, models, methods, and algorithms.
    John Wiley & Sons, 2011.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Li, Y. Wang, Z. Zhang, Y. Wang, D. Ma, and J. Huang. A novel fast and
    memory efficient parallel MLCS algorithm for long and large-scale sequences alignments.
    In 32nd IEEE International Conference on Data Engineering, ICDE 2016, Helsinki,
    Finland, May 16-20, 2016, pages 1170–1181\. IEEE Computer Society, 2016.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Lian, L. Chen, and S. Song. Consistent query answers in inconsistent
    probabilistic databases. In Proceedings of the ACM SIGMOD International Conference
    on Management of Data, SIGMOD 2010, Indianapolis, Indiana, USA, June 6-10, 2010,
    pages 303–314, 2010.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Z. C. Lipton, D. Kale, and R. Wetzel. Directly modeling missing data in
    sequences with rnns: Improved classification of clinical time series. In Machine
    Learning for Healthcare Conference, pages 253–270, 2016.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. J. Little and D. B. Rubin. Statistical analysis with missing data,
    volume 793. John Wiley & Sons, 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Liu, R. Yu, S. Zheng, E. Zhan, and Y. Yue. NAOMI: non-autoregressive
    multiresolution sequence imputation. In Advances in Neural Information Processing
    Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
    2019, 8-14 December 2019, Vancouver, BC, Canada, pages 11236–11246, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] X. Luo, M. Zhou, H. Leung, Y. Xia, Q. Zhu, Z. You, and S. Li. An incremental-and-static-combined
    scheme for matrix-factorization-based collaborative filtering. IEEE Transactions
    on Automation Science and Engineering, 13(1):333–343, 2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Luo, X. Cai, Y. Zhang, J. Xu, and X. Yuan. Multivariate time series
    imputation with generative adversarial networks. In Advances in Neural Information
    Processing Systems 31: Annual Conference on Neural Information Processing Systems
    2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 1603–1614, 2018.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Luo, Y. Zhang, X. Cai, and X. Yuan. E²gan: End-to-end generative adversarial
    network for multivariate time series imputation. In Proceedings of the Twenty-Eighth
    International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
    China, August 10-16, 2019, pages 3094–3100\. ijcai.org, 2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] P. E. McKnight, K. M. McKnight, S. Sidani, and A. J. Figueredo. Missing
    data: A gentle introduction. Guilford Press, 2007.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Mei, Y. de Castro, Y. Goude, and G. Hébrail. Nonnegative matrix factorization
    for time series recovery from a few temporal aggregates. In Proceedings of the
    34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,
    6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages
    2382–2390\. PMLR, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] M. Mirza and S. Osindero. Conditional generative adversarial nets. CoRR,
    abs/1411.1784, 2014.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] F. V. Nelwamondo, S. Mohamed, and T. Marwala. Missing data: A comparison
    of neural network and expectation maximization techniques. Current Science, pages
    1514–1521, 2007.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S. Nordbotten. Neural network imputation applied to the norwegian 1990
    population census data. JOURNAL OF OFFICIAL STATISTICS-STOCKHOLM-, 12:385–402,
    1996.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context
    encoders: Feature learning by inpainting. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 2536–2544, 2016.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] P. K. Sharpe and R. J. Solly. Dealing with missing values in neural network-based
    diagnostic systems. Neural Computing and Applications, 3(2):73–77, 1995.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Song, Y. Cao, and J. Wang. Cleaning timestamps with temporal constraints.
    PVLDB, 9(10):708–719, 2016.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Song and L. Chen. Differential dependencies: Reasoning and discovery.
    ACM Trans. Database Syst., 36(3):16:1–16:41, 2011.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] S. Song, L. Chen, and H. Cheng. Efficient determination of distance thresholds
    for differential dependencies. IEEE Trans. Knowl. Data Eng., 26(9):2179–2192,
    2014.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Song, L. Chen, and P. S. Yu. On data dependencies in dataspaces. In
    Proceedings of the 27th International Conference on Data Engineering, ICDE 2011,
    April 11-16, 2011, Hannover, Germany, pages 470–481, 2011.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Song, L. Chen, and P. S. Yu. Comparable dependencies over heterogeneous
    data. VLDB J., 22(2):253–274, 2013.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Song, C. Li, and X. Zhang. Turn waste into wealth: On simultaneous
    clustering and cleaning over dirty data. In Proceedings of the 21th ACM SIGKDD
    International Conference on Knowledge Discovery and Data Mining, Sydney, NSW,
    Australia, August 10-13, 2015, pages 1115–1124, 2015.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S. Song, Y. Sun, A. Zhang, L. Chen, and J. Wang. Enriching data imputation
    under similarity rule constraints. IEEE Trans. Knowl. Data Eng., 32(2):275–287,
    2020.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] S. Song, A. Zhang, L. Chen, and J. Wang. Enriching data imputation with
    extensive similarity neighbors. PVLDB, 8(11):1286–1297, 2015.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Song, A. Zhang, J. Wang, and P. S. Yu. SCREEN: stream data cleaning
    under speed constraints. In Proceedings of the 2015 ACM SIGMOD International Conference
    on Management of Data, Melbourne, Victoria, Australia, May 31 - June 4, 2015,
    pages 827–841, 2015.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S. Song, H. Zhu, and L. Chen. Probabilistic correlation-based similarity
    measure on text records. Inf. Sci., 289:8–24, 2014.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Sun, S. Song, C. Wang, and J. Wang. Swapping repair for misplaced attribute
    values. In 36th IEEE International Conference on Data Engineering, ICDE 2020.
    IEEE, 2020.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural
    Information Processing Systems 30: Annual Conference on Neural Information Processing
    Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998–6008, 2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Wang, S. Song, X. Lin, X. Zhu, and J. Pei. Cleaning structured event
    logs: A graph repair approach. In 31st IEEE International Conference on Data Engineering,
    ICDE 2015, Seoul, South Korea, April 13-17, 2015, pages 30–41, 2015.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Wang, S. Song, X. Zhu, and X. Lin. Efficient recovery of missing events.
    PVLDB, 6(10):841–852, 2013.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Wang, S. Song, X. Zhu, X. Lin, and J. Sun. Efficient recovery of missing
    events. IEEE Trans. Knowl. Data Eng., 28(11):2943–2957, 2016.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] W. Wothke. Longitudinal and multigroup modeling with missing data. 2000.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. Yu, N. Rao, and I. S. Dhillon. Temporal regularized matrix factorization
    for high-dimensional time series prediction. In Advances in Neural Information
    Processing Systems 29: Annual Conference on Neural Information Processing Systems
    2016, December 5-10, 2016, Barcelona, Spain, pages 847–855, 2016.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. Zhang, S. Song, Y. Sun, and J. Wang. Learning individual models for
    imputation. In 35th IEEE International Conference on Data Engineering, ICDE 2019,
    Macao, China, April 8-11, 2019, pages 160–171\. IEEE, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Zhang, S. Song, and J. Wang. Sequential data cleaning: A statistical
    approach. In Proceedings of the 2016 International Conference on Management of
    Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016,
    pages 909–924\. ACM, 2016.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Zhang, S. Song, J. Wang, and P. S. Yu. Time series data cleaning: From
    anomaly detection to anomaly repairing. PVLDB, 10(10):1046–1057, 2017.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] G. P. Zhang. Time series forecasting using a hybrid ARIMA and neural network
    model. Neurocomputing, 50:159–175, 2003.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X. Zhu, S. Song, X. Lian, J. Wang, and L. Zou. Matching heterogeneous
    event data. In International Conference on Management of Data, SIGMOD 2014, Snowbird,
    UT, USA, June 22-27, 2014, pages 1211–1222, 2014.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
