- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:44:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2208.10498] Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10498](https://ar5iv.labs.arxiv.org/html/2208.10498)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dalin Zhang,  Kaixuan Chen,  Yan Zhao,  Bin Yang,  Lina Yao,  and Christian
    S. Jensen Dalin Zhang, Kaixuan Chen, Yan Zhao, Bin Yang, and Christian S. Jensen
    is with the Department of Computer Science, Aalborg University, Aalborg Øst 9220,
    Denmark (e-mail: dalinz@cs.aau.dk, kchen@cs.aau.dk, yanz@cs.aau.dk, byang@cs.aau.dk,
    and csj@cs.aau.dk).Lina Yao is with the School of Computer Science and Engineering,
    University of New South Wales, UNSW Sydney 2052, Australia (e-mail: lina.yao@unsw.edu.au).Manuscript
    received March 10, 2022'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Deep learning technologies have demonstrated remarkable effectiveness in a
    wide range of tasks, and deep learning holds the potential to advance a multitude
    of applications, including in edge computing, where deep models are deployed on
    edge devices to enable instant data processing and response. A key challenge is
    that while the application of deep models often incurs substantial memory and
    computational costs, edge devices typically offer only very limited storage and
    computational capabilities that may vary substantially across devices. These characteristics
    make it difficult to build deep learning solutions that unleash the potential
    of edge devices while complying with their constraints. A promising approach to
    addressing this challenge is to automate the design of effective deep learning
    models that are lightweight, require only a little storage, and incur only low
    computational overheads. This survey offers comprehensive coverage of studies
    of design automation techniques for deep learning models targeting edge computing.
    It offers an overview and comparison of key metrics that are used commonly to
    quantify the proficiency of models in terms of effectiveness, lightness, and computational
    costs. The survey then proceeds to cover three categories of the state-of-the-art
    of deep model design automation techniques: automated neural architecture search,
    automated model compression, and joint automated design and compression. Finally,
    the survey covers open issues and directions for future research.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: deep learning, neural architecture search, lightweight model, model compression
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I-A Background
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep learning has achieved state-of-the-art performance at a multitude of tasks
    and has affected people’s lives in myriad areas, including recommendation systems
    [[1](#bib.bib1)], natural language understanding [[2](#bib.bib2)], and biomedical
    engineering [[3](#bib.bib3)]. Deep learning frees researchers from manually designing
    purposeful feature representations of objects by introducing multi-layer neural
    architectures capable of automatic feature extraction. This enables researchers
    to work at a higher level of abstraction, focusing on architecture engineering
    rather than on feature engineering and model building. The neural architectures
    of deep models tend to be increasingly intricate and complex, thus requiring substantial
    hardware resources for deployment. This may not be a problem when a powerful server
    is available, but important settings occur where this is not the case:: 1) computing
    on mobile hardware, e.g., smartphones and tablet PCs; 2) computing on industrial
    hardware optimized for low deployment cost and low power consumption. Furthermore,
    although cloud computing may be available, it is often fundamentally unattractive
    to transfer data from edge devices to the cloud. On the one hand, such transfer
    may incur privacy, ownership, and consequent regulatory concerns, including for
    human-related data like audio and video data and utility consumption data [[4](#bib.bib4)];
    on the other hand, data transfer incurs substantial latency due to low bandwidth
    “last mile” connectivity. Last but not least, deployment of complex models is
    expensive, due to the cost of specialized hardware and energy consumption, and
    it is also bad for the environment due to the carbon footprint of producing the
    required electricity [[5](#bib.bib5), [6](#bib.bib6)].'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, research has demonstrated that deep learning models generally have
    large numbers of redundant parameters and computations that contribute to their
    performance [[7](#bib.bib7), [8](#bib.bib8)], so there is considerable room for
    reducing redundancies without compromising model accuracy. Hence, it is highly
    desirable and completely possible to design simplified deep learning models with
    reduced computational complexity, thus achieving lighter weight and more efficient
    deep models¹¹1By efficient deep learning models we mean deep learning models with
    low memory usage or low inference cost or latency..
  prefs: []
  type: TYPE_NORMAL
- en: I-B Design of Efficient Deep Learning Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Research on building efficient deep learning models can be categorized into
    two categories: efficient network architecture design and model compression. In
    efficient network architecture design, the aim is to create compact neural modules
    and to connect these according to a carefully designed topology. The objective
    is to achieve efficient deep learning models with acceptable accuracy but small
    structures (low memory requirement) and low computational complexity (high speed).
    MobileNet [[9](#bib.bib9)] proposes an efficient network structure using a depthwise
    separable convolution module as the basic building block, and superior size, speed,
    and accuracy characteristics over a variety of computer vision tasks are documented.
    A second version, called MobileNetV2, incorporates a new basic building block,
    bottleneck depth-separable convolution with residuals/inverted residual [[10](#bib.bib10)].
    As a result, MobileNetV2 generally needs 30% fewer parameters, requires two times
    fewer operations and is about 30–40% faster on a Google Pixel phone while achieving
    higher accuracy than its predecessor. In ShuffleNetV1 [[11](#bib.bib11)], bottleneck-like
    structures with pointwise group convolutions and ”channel shuffle” operations
    are the basic building blocks that are used to achieve efficient neural structures.
    Like in the MobileNet case, ShuffleNet also has a second generation, called ShuffleNetV2
    [[12](#bib.bib12)]. Here a new “channel split” operation is introduced in order
    to further improve speed and accuracy. In addition, four practical guidelines
    for efficient network architecture design are provided.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to directly designing efficient architecture from a pool of basic
    building blocks, model compression aims at modifying a given neural model to reduce
    its memory and computational cost. Pruning is one such powerful technique that
    tries to remove unimportant components from a model [[13](#bib.bib13), [14](#bib.bib14)].
    It is flexible in that it is possible to remove layers, neurons, connections,
    or channels. While pruning shrinks a model by removing redundant parts, quantization
    aims to reduce the number of bits required to represent model parameters [[15](#bib.bib15)].
    Most processors use 32 bits or more to store the parameters of a deep model. However,
    research estimates that the human brain stores information in a discrete format
    that uses 4–7 bits [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]. Indeed,
    many efforts have been devoted to investigating using fewer bits to store model
    parameters to reduce memory and computational cost [[19](#bib.bib19), [20](#bib.bib20)].
    Other techniques like knowledge distillation [[21](#bib.bib21)] and tensor decomposition
    [[22](#bib.bib22)] are also popular and effective at compressing deep models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/27bac87c51e16b3a7255e722adb0bc28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Design Automation for Efficient Deep Learning Models. The automatic
    designer takes the hardware constraints into its search strategy to explore a
    predefined deep learning architecture design space which could be either an architecture
    space or a compression space; with the explored design choices, a new deep learning
    model from scratch or a compressed deep learning model is derived and deployed
    on the target device; the model’s performance including both accuracy and hardware
    consumption is finally estimated and fed into the automatic designer.'
  prefs: []
  type: TYPE_NORMAL
- en: I-C Design Automation for Efficient Deep Learning Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although remarkable progress has been achieved in building efficient deep learning
    models, initial proposals were heuristic rule-based and hand-tuned with inevitable
    limitations. First, building an efficient deep learning model still requires advanced
    prior knowledge and experience, making it difficult for beginners and even deep
    learning experts without domain knowledge to develop specialized models that meet
    given requirements. Second, as it is impossible to apply the same uniform model
    across diverse mobile platforms and tasks, enabling specializations that address
    such diversity is essential. Yet, it remains excessively time-consuming and inconvenient.
    Third, hand-crafted rules offer limited capabilities at utilizing hardware potentials
    fully, while satisfying the size and latency requirements. Different deep learning
    models can satisfy the same hardware constraints, and it is impractical to manually
    exhaust all possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations such as the above have prompted a multitude of studies of design
    automation techniques for efficient deep learning models. Fig. [1](#S1.F1 "Figure
    1 ‣ I-B Design of Efficient Deep Learning Models ‣ I Introduction ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") shows a
    general automated design process. The design automation algorithm (i.e., Automatic
    Designer) applies a search strategy to find network architectures and compression
    in the predefined Deep Learning Architecture Design Space; next, a specific deep
    learning model is Derived through executing the identified operations and is then
    Deployed on target devices (e.g., CPU, GPU, or IoT devices); lastly, model performance
    metrics such as accuracy, latency, and memory use are estimated and provided to
    the Automatic Designer. In the next iteration, the Automatic Designer considers
    both the feedback and specialized constraints and takes a new design action to
    find a better model in the design space. This process is repeated until a satisfactory
    model is achieved.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural architecture search (NAS) aims to automate the design of neural networks
    that achieve the best possible accuracy. Next, more targeted studies that aim
    to automate the design of efficient neural networks build on generic NAS and involve
    the design of search spaces and the modification of the optimization objective
    from sole accuracy to both accuracy and efficiency. Cai et al. [[23](#bib.bib23)]
    incorporate model latency into the optimization goal of their binarized design
    automation framework (i.e., ProxylessNAS) by means of a differentiable loss. Due
    to targeting optimized inference latency directly, ProxylessNAS can achieve efficient
    neural architectures 1.83 times faster than MobileNetV2 with the same level of
    top-1 accuracy. Notably, the search space of ProxylessNAS is based on the inverted
    residual blocks of different convolution sizes, as proposed by MobleNetV2\. This
    implies that design automation is not only able to reduce human labor but even
    enables architectures that surpass handcrafted architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to studies that target the direct design of efficient neural networks,
    other studies target the automated compression of deep neural networks [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)]. For example, Xiao et al. [[24](#bib.bib24)]
    design an automatic pruning approach that is based on learnable pruning indicators
    instead of pruning rules designed individually for specific architectures and
    datasets. This approach achieves superior compression performance on different
    widely-used neural models (e.g., AlexNet, ResNet, and MobileNet). Considering
    the progress in, and promise of, design automation for efficient deep learning
    models, it is a comprehensive and systematic survey is called for and holds the
    potential to accelerate future research.
  prefs: []
  type: TYPE_NORMAL
- en: I-D Key Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, this is the first survey of state-of-the-art
    design automation methods that target fast, lightweight, and effective deep learning
    models. The key contributions of the survey are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a comprehensive review of design automation techniques targeting
    fast, lightweight, and effective deep learning models. In doing so, more than
    150 papers are covered, analyzed, and compared.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We propose a new taxonomy of deep design automation methods from the perspectives
    of how to design, i.e., by search (Section [III](#S3 "III Search for Efficient
    Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey")), by compression (Section [IV](#S4 "IV Automated
    Compression of Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")), or by joint search and compression
    (Section [V](#S5 "V Joint Automated Design Strategies ‣ Design Automation for
    Fast, Lightweight, and Effective Deep Learning Models: A Survey")); and by what
    to design, i.e., the search space, the search strategy, and the performance estimation
    strategy; and by what to compress, e.g., tensors, knowledge, and representation.
    The detailed categories provide convenience to the readers in obtaining an overview
    of the literature and identifying a direction of interest.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize and compare the evaluation metrics that are used for both the obtained
    models and the design approaches. By means of the comparison, we emphasize the
    role of each metric and explicate the associated pros and cons.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss open issues and identify future directions on automated design and
    compression.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of the survey is organized as follows: Section [II](#S2 "II Evaluation
    Metrics ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey") summarizes the evaluation metrics of efficiency (i.e., speed
    and lightness) and effectiveness of deep learning models. Section [III](#S3 "III
    Search for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey") covers studies of searching for
    efficient deep models. Section [IV](#S4 "IV Automated Compression of Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey") then covers research on automated compression, and Section [V](#S5
    "V Joint Automated Design Strategies ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey") considers studies of joint automated
    search and compression. Section [VI](#S6 "VI Future Directions ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") presents
    research directions, and Section [VII](#S7 "VII Conclusion ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") concludes
    the survey.'
  prefs: []
  type: TYPE_NORMAL
- en: II Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Different evaluation metrics or objectives may lead to different or even opposite
    conclusions. Thus, it is indispensable to introduce and distinguish the relevant
    evaluation metrics before diving into the details of efficient models. In this
    section, we will introduce the evaluation metrics for measuring the efficiency
    and effectiveness of the obtained efficient models. These metrics are also critical
    to evaluating the effectiveness of a design automation approach. Furthermore,
    we introduce metrics for evaluating the cost of a design automation approach as
    well. We briefly summarize the characteristics including advantages and limitations
    of the commonly used evaluation metrics in Table [I](#S2.T1 "TABLE I ‣ II Evaluation
    Metrics ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Evaluation Metrics for Measuring the Efficiency and Effectiveness
    of Both the Obtained Efficient Model and the Design Automation Approach'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Metrics | Characteristics |'
  prefs: []
  type: TYPE_TB
- en: '| Advantages | Limitations |'
  prefs: []
  type: TYPE_TB
- en: '| Device-agnostic Efficiency Evaluation Metrics | FLOPs | • each to obtain
    • coarse approximation of latency | • inconsistent definitions • omitting the
    degree of computational parallelism • omitting the memory access cost • omitting
    the implementation library |'
  prefs: []
  type: TYPE_TB
- en: '| Number of Parameters | • easy to obtain • precise when measuring storage
    requirement | • omitting the computation caching for measuring memory • unable
    to reflect peak memory usage |'
  prefs: []
  type: TYPE_TB
- en: '| Device-aware Efficiency Evaluation Metrics | Latency | • the real criterion
    that we care about • obtain through implementation on real hardware or prediction
    models • depend on different hardware platforms and implementation libraries |'
  prefs: []
  type: TYPE_TB
- en: '| Peak Memory Usage | • the real criterion that we care about • obtain through
    implementation on real hardware or prediction models • reflect the peak usage
    that really matters |'
  prefs: []
  type: TYPE_TB
- en: '| Effectiveness Evaluation Metrics | Accuracy/mAP/mIOU etc. | • diverse and
    dependent on the targeting tasks • identical to those for evaluating normal deep
    learning models |'
  prefs: []
  type: TYPE_TB
- en: '| Design Automation Cost Metrics | GPU Hours | • used to evaluate the speed
    of a design automation method • depend on the number of GPUs used |'
  prefs: []
  type: TYPE_TB
- en: '| GPU Memory | • used to evaluate the memory requirement of a design automation
    • grows linearly w.r.t. the size of the candidate set |'
  prefs: []
  type: TYPE_TB
- en: II-A Device-agnostic Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The device-agnostic metrics can be calculated directly from the model architecture
    without real-world implementation on hardware. These metrics do not essentially
    reflect a model’s real performance that we care about [[12](#bib.bib12), [27](#bib.bib27),
    [28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 FLoating-point OPerations (FLOPs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'FLOPs is generally defined as the number of floating-point multiplication-add
    operations in a model for approximating the latency/speed or computation complexity
    [[11](#bib.bib11), [12](#bib.bib12), [29](#bib.bib29)]. There are also other commonly
    used analogous metrics, such as the number of multiply-add operations (MAdds)
    and the number of multiply-accumulate operations (MACs) [[30](#bib.bib30), [27](#bib.bib27),
    [31](#bib.bib31)]. However, one contention remains regarding the definition: whether
    multiplication-add should be considered as one or two operations. Some researchers
    argue that in many recent deep learning models, convolutions are bias-free and
    it makes sense to count multiplication and add as separate FLOPs [[32](#bib.bib32)].
    Moreover, some non-multiplication or non-add operations require FLOPs in some
    implementations as well, such as an activation layer [[33](#bib.bib33), [27](#bib.bib27)].
    Whether such operations should be counted into total FLOPs is also a dispute.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the inconsistency of the definition, there exist three main issues
    that induce the discrepancy between FLOPs and real latency. First, counting only
    FLOPs ignores some decisive factors that affect latency remarkably. One such factor
    is parallelism. With the same FLOPs, a model with a high degree of parallelism
    may be much faster than another model with a low degree of parallelism [[34](#bib.bib34),
    [35](#bib.bib35)]. Another important factor arises from the memory access cost.
    Since the on-device RAM is usually limited, data cannot be entirely loaded at
    one time and thus reading data from external memory is required. It has an increasing
    impact on the latency as the computation unit is getting stronger recently, thus
    becoming the bottleneck for latency. This intrinsic factor should not be simply
    neglected. Third, the implementation library of a deep learning model significantly
    influences its latency as well. For example, NVIDIA’s cuDNN library provides different
    implementations of a convolution operation [[36](#bib.bib36)] that clearly require
    different amounts of FLOPs although the network architecture is identical. Some
    works also found that a smaller number of FLOPs could be even slower due to the
    library abstractions [[12](#bib.bib12), [13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Number of Parameters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is usually required to fit all parameters of a neural network within on-chip
    memory to execute the model fast [[37](#bib.bib37)]. Thus, the number of parameters
    mainly constrains the memory requirement of a deep learning model. Although some
    mobile devices like smartphones have abundant memory, there are still a few mobile
    devices that have particularly scarce memory, such as microcontrollers that typically
    have 10’s-100’s of KB [[38](#bib.bib38)]. These memory-scarce devices are in demand
    in many fields due to their low prices. Thus, it is desirable to design a small-sized
    model that can fit into “tiny” memory and the number of parameters is a common
    device-agnostic metric for evaluating such a requirement. Nevertheless, the number
    of parameters only accounts for a portion of memory usage; input data, computation
    caching (i.e., intermediate tensors produced at runtime), and network structure
    information take over a relatively larger portion of memory [[39](#bib.bib39),
    [40](#bib.bib40)]. In some extreme scenarios, only computation caching and active
    parameters (i.e., the parameters used for current computation) occupy memory.
    Some researchers propose to optimize the in-memory computation caching to reduce
    memory consumption but their performance largely depends on the network structure
    [[41](#bib.bib41)]. Thus, a model with a larger number of parameters is not certainly
    more memory-hungry than a model with a smaller number of parameters. On the other
    hand, model parameters account for a major component of storage/external memory
    (e.g., FLASH memory) usage.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Device-aware Efficiency Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike device-agnostic metrics, device-aware metrics can reflect the computational
    cost that we really care about. They can be collected on real and target hardware
    platforms or approximated.
  prefs: []
  type: TYPE_NORMAL
- en: II-B1 Latency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The latency is used to evaluate the running speed of a deep model making inferences.
    It is usually measured in the form of running time per inference (e.g., millisecond)
    [[42](#bib.bib42)] or inferences per unit time (e.g., batches/s or images/s) [[12](#bib.bib12)].
    In practice, the precise latency is an average value computed on a large batch
    or several bathes [[42](#bib.bib42)]. Some works collect this information through
    implementations on real devices including GPUs, TPUs, and CPUs [[25](#bib.bib25),
    [12](#bib.bib12), [43](#bib.bib43), [42](#bib.bib42)]. It should be noted that
    the reliability is unknown when evaluating a deep model’s latency not on its target
    mobile devices (e.g., smartphone CPUs) but on non-mobile devices (e.g., GPUs).
    In addition to directly measuring latency, some researchers try to approximate
    it [[23](#bib.bib23), [44](#bib.bib44)]. The lookup table is a latency approximation
    method that enumerates all the possible layers that a family of models can have
    along with the latency of each of the layers [[44](#bib.bib44)]. It is hardware
    and model family-specific and requires a significant amount of time to maintain
    a large and dynamic database. In addition, this simple summation of the latency
    of an individual does not take memory access cost and parallelism into consideration,
    and thus shows low precision. A latency prediction model is another latency approximation
    method that models the network latency as a function of network structures and/or
    hardware parameters [[23](#bib.bib23), [45](#bib.bib45), [44](#bib.bib44)]. This
    approach can make latency differentiable to be directly involved in an objective
    function for gradient-based optimization [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B2 Memory Usage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different from latency where we care about the total inference time of a sample/batch,
    the peak memory usage during inference is our major concern [[37](#bib.bib37),
    [46](#bib.bib46)]. As long as the peak memory usage is lower than the memory capacity,
    a model is able to run on the device. It is not necessary to keep memory usage
    as low as possible. Extreme scenarios with maximal memory saving are considered.
    The peak memory usage is dominated by the intermediate tensors (so-called activation
    metrics), so a small model (i.e., a small number of model parameters) doesn’t
    guarantee a low peak memory usage. For example, at similar ImageNet accuracy (70%),
    even though MobileNetV2 [[10](#bib.bib10)] reduces its model size by 4.6$\times$
    compared to ResNet-18 [[47](#bib.bib47)], the peak memory requirement increases
    by 1.8$\times$ [[46](#bib.bib46)]. Thus, it is highly recommended to evaluate
    peak memory usage when targeting a device with quite constrained memory resources.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the peak memory usage, which considers the on-chip memory, storage/external-memory
    usage is also an essential evaluation metric. It mainly restricts the model size
    of which model parameters occupy the largest proportion. Therefore, the bit-precision
    of parameters has a crucial impact on the external-memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Effectiveness Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The metrics for evaluating the effectiveness of an efficient model are quite
    diverse and mainly dependent on the targeting tasks. Vision tasks, such as image
    recognition, object detection, and semantic segmentation, are such commonly used
    benchmark tasks [[12](#bib.bib12), [10](#bib.bib10), [48](#bib.bib48)]. Different
    tasks have different evaluation metrics: top-1 or top 5 accuracy for image recognition
    [[48](#bib.bib48)], mAP for object detection [[48](#bib.bib48)], and mIOU for
    semantic segmentation [[10](#bib.bib10)]. In addition to vision tasks, audio tasks,
    such as keyword spotting, are leveraged as an evaluation task [[46](#bib.bib46)].
    However, accuracy is also used as the evaluation metric in this case.'
  prefs: []
  type: TYPE_NORMAL
- en: II-D Design Automation Cost Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the price of freeing human efforts, design automation normally demands an
    excessive computational cost that prohibits its wide deployment.
  prefs: []
  type: TYPE_NORMAL
- en: II-D1 GPU Hours
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GPU hours/days are metrics used to evaluate the time cost of a design automation
    method especially a NAS-based method [[49](#bib.bib49), [50](#bib.bib50)]. The
    GPU days can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{GPU days}=N\times t,$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $N$ denotes the number of GPUs, and $t$ denotes the number of days that
    are used for searching [[51](#bib.bib51)]. GPU hours have a similar definition.
    At the early stage, it requires several or even tens of days for searching [[52](#bib.bib52),
    [49](#bib.bib49)], while currently researchers have pushed the time to the magnitude
    of multiple hours [[53](#bib.bib53)].
  prefs: []
  type: TYPE_NORMAL
- en: II-D2 GPU Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although differentiable neural architecture search has reduced the cost of GPU
    hours considerably, it suffers from an intensive GPU memory cost. The consumption
    of GPU memory depends on the size of the candidate set for searching. Specifically,
    the required memory grows linearly w.r.t. the number of choices in a candidate
    set [[54](#bib.bib54)]. This issue restricts the search space size that prevents
    the capability of discovering novel and strong models. Some works have targeted
    this issue and achieved impressive progress [[23](#bib.bib23)]. Thus, it is important
    to involve GPU memory for a thorough evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: III Search for Efficient Deep Learning Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Driven by the growing demand for mobile applications, efficient deep learning
    models have gained explosive attention. A tremendous number of studies have been
    proposed to explore manually designed efficient neural architectures or modules
    and achieved impressive progress [[29](#bib.bib29), [12](#bib.bib12), [10](#bib.bib10),
    [55](#bib.bib55), [56](#bib.bib56)]. Though the notable success, it is challenging
    for human engineers to heuristically exhaust the design space to trade off accuracy
    and hardware constraints. Hardware-aware neural architecture search plays an influential
    role in advancing this field as it automates the design process to find an optimal
    solution. Similar to regular NAS [[57](#bib.bib57)], the hardware-aware NAS also
    has three components, i.e., search space, search strategy, and performance estimation
    strategy, but with additional freedom or constraints (Fig. [1](#S1.F1 "Figure
    1 ‣ I-B Design of Efficient Deep Learning Models ‣ I Introduction ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") left bottom).
    In this section, we review recent achievements from these three aspects and summarize
    the main results in TABLE [II](#S3.T2 "TABLE II ‣ III-C2 Performance Predictor
    ‣ III-C Performance Estimation Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Efficient Search Space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A search space is the basis of NAS and determines what architectures NAS can
    discover in principle and their performance upper-limit [[57](#bib.bib57)]. A
    well-defined search space can not only accelerate the search process but also
    promote the searched model’s performance [[49](#bib.bib49), [58](#bib.bib58)].
    Since some manually explored efficient neural architectures have achieved considerable
    advances, it is an intuitive yet practical idea to construct a search space with
    the heuristics of these hand-crafted efficient structures, and leverage the NAS
    technology to automatically discover novel efficient models upon this search space.
    There are usually two components that define a search space: (i) operators that
    each layer executes, and (ii) a backbone that decides the topological connections
    of these layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c3f4693cf41dd2c7211eac76a56be5c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) DSConv
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3687b427cd1f1df1a5632d37361331fb.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) MBConv
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29be66dd4c821b387af03911937f87b3.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) MBConv+SE
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/471d66b894a27d6e5e631326262aad3e.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) GConv+channel shuffle
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Schematics of efficient operators. (a) DSConv: depthwise separable
    convolution; (b) MBConv: mobile inverted bottleneck convolution; (c) MBConv+SE:
    MBConv with the squeeze and excitation module; (d) GConv+channel shuffle: group
    convolution with channel shuffle.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Operators
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Operators, such as convolutional layers or fully connected layers, are basic
    building components for a deep learning model. A simple way to build a deep learning
    model is to directly stack several operators, such as VGG Net [[59](#bib.bib59)].
    Other work connects several neural network layers to construct a motif (e.g.,
    a residual block) and builds a model by repeating and arbitrarily connecting these
    motifs [[60](#bib.bib60), [47](#bib.bib47), [57](#bib.bib57)]. In this paper,
    we use the term ”operator” to also represent a motif, which is usually used as
    a whole and regarded as a basic component of an end-to-end model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As early-developed and widely-demonstrated neural architectures, the convolutional
    layer and its variants are the dominant operators of many deep learning models,
    especially those in the computer vision area. A standard convolution can be denoted
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Conv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{m,n,k}^{M,N,K}{\textbf{w}}_{(m,\;n,\;k)}\cdot\bm{x}_{(i+m,\;j+n,\;k)},$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where W is the convolution kernel weight, $\bm{X}$ is the input feature maps
    to a convolution layer, $(i,j)$ is the coordinate of an output feature map, and
    $(m,n,k)$ is the coordinate of the convolutional kernel. NSGA-Net [[61](#bib.bib61)]
    utilizes standard convolutions as the operator and devotes to automatically determining
    their connections in a block-based manner. As it only seeks to optimize the connection
    of operators, limited efficiency is gained. Scheidegger et al. [[62](#bib.bib62)]
    and Lu et al. [[63](#bib.bib63)] also consider standard convolutions but, instead
    of optimizing the connections, they allow to search the convolution hyperparameters
    such as the filter size, the stride and the number of filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the standard convolutions, extensive manually-designed variants
    have been demonstrated both efficient and effective. Therefore, it is intuitive
    to employ these architectures to construct an efficient search space. The depthwise
    separable convolution (DSConv), which can significantly reduce the number of parameters
    and computation and works as the primary module in MobileNet [[9](#bib.bib9)],
    is such a widely used efficient operator. As shown in Fig [2a](#S3.F2.sf1 "In
    Figure 2 ‣ III-A Efficient Search Space ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey"), a DSConv block is made up of two components: a depthwise convolution
    and a pointwise convolution. The depthwise convolutions (DWConv) applies a single
    filter per each input feature map (input depth) for spatial filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{DWConv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{m,n}^{M,N}{\textbf{w}}_{(m,\;n)}\cdot\bm{x}_{(i+m,\;j+n)}.$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'The pointwise convolution (PWConv), a simple $1\times 1$ convolution, is used
    to create a linear combination of the output of the depthwise convolutions for
    feature fusion:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{PWConv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{k}^{K}{\textbf{w}}_{k}\cdot\bm{x}_{(i,\;j)}.$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Thus, by combining DWConv and PWConv, the DSConv is denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\text{DSConv}(\textbf{W}_{p},\textbf{W}_{d},\bm{X})_{(i,j)}=$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\;\;\;\;\;\;\;\;\;\;\text{PWConv}(\textbf{W}_{p},\text{DWConv}(\textbf{W}_{d},\bm{X})_{(i,j)})_{(i,j)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: LEMONADE [[64](#bib.bib64)] adopts both DSConvs and standard convolutions as
    the basic operators of its search space and supports increasing the number of
    filters and pruning filters to search efficient neural architectures. RENA [[65](#bib.bib65)]
    also includes DSConvs but allows to automatically decide not only their hyperparameters
    but also whether they should be used in each layer. In addition to the standard
    DSConv, ProxylessNAS [[23](#bib.bib23)] includes a variant, namely dilated depthwise
    separable convolution, in its search space as well. The search engine can choose
    between the standard one and the variant.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MobileNetV2, a more advanced mobile convolution block, mobile inverted bottleneck
    convolutions (MBConv), is proposed [[10](#bib.bib10)] and soon becomes a popular
    operator in favour of an efficient search space. Fig. [2b](#S3.F2.sf2 "In Figure
    2 ‣ III-A Efficient Search Space ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey") illustrates the structure of MBConv. It is made up of three convolutions
    and one residual connection. First, a PWConv is applied to expand the input feature
    map to a higher-dimensional space so that non-linear activations (ReLU6) can better
    extract information. Then, a depthwise convolution is performed with $3\times
    3$ kernels and ReLU6 activations to achieve spatial filtering of the higher-dimensional
    feature maps. Furthermore, the spatially-filtered feature maps are projected back
    to a low-dimensional space with another pointwise convolution. Since the low-dimensional
    projection results in loss of information, linear activation is used after pointwise
    convolution. Finally, an optional residual connection (depending on whether the
    stride of the depthwise layer is 1) is added to combine the original input and
    the output of the low-dimensional projection. Note that the last two convolutions
    (depthwise convolution and pointwise convolution) are essentially a DSConv with
    dimension reduction. There are multiple works on purely using MBConv to construct
    an efficient search space but with different backbones or search strategies [[66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)]. Xiong et al. [[69](#bib.bib69)] argue that
    DSConv is inexpensive based on FLOPs or the number of parameters, which are not
    necessarily correlated with the inference efficiency, so they propose a fused
    MBConv layer that fuses together the first pointwise convolution and the subsequent
    depthwise convolution into a single standard convolution. They achieve higher
    mAP and lower latency on EdgeTPU and DSP than the pure MBConv search space. Li
    et al. [[70](#bib.bib70)] provide an in-depth comparison between fused MBConv
    and MBConv and summarize that fused MBConv has higher operational intensity but
    higher FLOPs than MBConv depending on the shape and size of filters and activations.
    Therefore, they add both fused MBConv and MBConv into the search space to let
    the search strategy determine automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a more popular manner, MnasNet [[48](#bib.bib48)] upgrades MBConv with a
    squeeze and excitation (SE) module[[71](#bib.bib71)] after the DWConv for attentions
    on feature maps (as shown in Fig. [2c](#S3.F2.sf3 "In Figure 2 ‣ III-A Efficient
    Search Space ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey")). Specifically,
    the S (squeeze) procedure first converts each individual feature map into a scalar
    descriptor using global average pooling so the input feature maps are converted
    into a vector $\bm{z}\in\mathbb{R}^{n}$ with its $k$-th element calculated by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z_{k}=\text{GlobalArgPool}(\bm{x}_{k})=\frac{1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W}\bm{x}_{(i,j,k)},$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\bm{x}_{(i,j)}$ is the $(i,j)$-th element of the $k$-th input feature
    map, which is of size $H\times W$. The E (excitation) procedure converts the S
    procedure’s output $\bm{z}$ into a vector of activations $\bm{s}$ using the gating
    mechanism of two fully connected layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{s}=\sigma(\textbf{W}_{2}\cdot\text{ReLU}(\textbf{W}_{1}\cdot\bm{z})),$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\sigma(\cdot)$ is the sigmoid activation function. The final output
    of the SE module is $\bm{\tilde{X}}$ with its $k$-th feature map denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\tilde{x}}_{k}=\text{SE}(\bm{x}_{k})=s_{k}\cdot\bm{x}_{k}.$ |  |
    (8) |'
  prefs: []
  type: TYPE_TB
- en: MobileNetV3 [[72](#bib.bib72)], the latest version of the MobileNet series,
    also adopts the MBConv plus SE operator and further enhances it by using the hard-swish
    (HS) [[73](#bib.bib73)] nonlinearities instead of ReLU and replacing the sigmoid
    function in SE with hard sigmoid [[74](#bib.bib74)]. The authors also point out
    that HS in deeper layers is more beneficial. This choice is based on the fact
    that the sigmoid function is expensive to deploy on mobile devices. Thereafter,
    many more works embrace the MBConv plus SE operator in their efficient search
    space [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)]. However, SE is not always paired with MBConv due to either
    not being supported by hardware [[31](#bib.bib31)] or unfavourable performance
    [[80](#bib.bib80)]. Some other works maintain diverse convolution operators (i.e.,
    standard convolution, DSConv, MBConv, and MBConv+SE) in the search space and let
    the search strategy choose automatically [[81](#bib.bib81), [48](#bib.bib48)].
    The commonly used searchable parameters of the convolution operator family are
    kernel sizes, the number of output channels, expansion ratios (MBConv, MBConv+SE),
    and SE ratio (MBConv+SE). Although different works do not have exactly the same
    searching ranges, they are basically similar. For example, Stamoulis et al. [[66](#bib.bib66)]
    consider kernel sizes of $\{3,5\}$ and expansion ratios of $\{3,6\}$; Fang et
    al. [[67](#bib.bib67)] consider kernel sizes of $\{3,5,7\}$ and expansion ratios
    of $\{3,6\}$; Cai et al. [[77](#bib.bib77)] consider kernel sizes of $\{3,5,7\}$
    and expansion ratios of $\{3,4,6\}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the above convolution operators of the MobileNet family, the group
    convolution (GConv) and its variants [[82](#bib.bib82), [83](#bib.bib83), [11](#bib.bib11)]
    are also considered to be significant operators for constructing an efficient
    search space [[84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88)]. Fig. [2d](#S3.F2.sf4 "In Figure 2 ‣ III-A Efficient Search
    Space ‣ III Search for Efficient Deep Learning Models ‣ Design Automation for
    Fast, Lightweight, and Effective Deep Learning Models: A Survey") illustrates
    the structure of GConv, where we take into account the feature map dimension of
    convolutional layers. The feature maps and filters are divided into $G$ ($G=3$
    in Fig. [2d](#S3.F2.sf4 "In Figure 2 ‣ III-A Efficient Search Space ‣ III Search
    for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")) groups respectively: $\bm{X}=\{\bm{X}_{1},\bm{X}_{2},...,\bm{X}_{G}\}$
    and $\textbf{W}=\{\textbf{W}_{1}\,\textbf{W}_{2}\,...,\textbf{W}_{G}\}$. In GConv,
    the convolution is only performed within each group so the output $\bm{\tilde{X}}$
    is denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\tilde{X}}=\{\textbf{W}_{1}\otimes\bm{X}_{1},\textbf{W}_{2}\otimes\bm{X}_{2},...,\textbf{W}_{G}\otimes\bm{X}_{G}\},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\otimes$ is the convolution operation between two sets. In this way,
    GConv can not only reduce parameters and computation but also provide a simple
    way to model parallelism. Therefore, AlexNet [[83](#bib.bib83)] can be trained
    on multiple GPUs with only 3GB RAM each. Note that depthwise convolution is a
    special case of GConv with the number of groups being the same as the number of
    channels. A channel shuffle usually comes after GConv to enable inter-group communications
    [[11](#bib.bib11)]. Xu et al. [[84](#bib.bib84)] include GConv in their search
    space with the number of groups as searchable from 1 (standard convolution) to
    N (the number of input channels). FBNet [[86](#bib.bib86)] and RCAS [[87](#bib.bib87)]
    encompasses a new convolution operator in their search space by replacing the
    first and last pointwise convolution in MBConv with $1\times 1$ GConv. This design
    expands MBConv but also allows the search strategy to automatically determine
    whether this expansion is needed. However, their allowed maximum group amount
    is quite small (2 for FBNet and 4 for RCAS). DPP-Net [[85](#bib.bib85)] and MONAS
    [[88](#bib.bib88)] also contain a variant of GConv, Learned Group Convolution
    (LGConv), which is the key operator of CondenseNet [[89](#bib.bib89)], in their
    search space. The LGConv prunes away unimportant filters with low magnitude weights
    further reducing the computational complexity on top of the GConv [[82](#bib.bib82)].
  prefs: []
  type: TYPE_NORMAL
- en: Some other studies use customized operators or non-convolution operators in
    their efficient search spaces. FasterSeg [[90](#bib.bib90)] proposes a zoomed
    convolution, where the input is sequentially processed with bilinear downsampling,
    standard convolution, and bilinear upsampling. The authors demonstrate that the
    zoomed convolution has 40% latency trim compared to a standard convolution on
    a GTX 1080i GPU. Different from previous works, which focus on 2D processing,
    Tang et al. [[91](#bib.bib91)] target 3D scenes. They propose sparse point-voxel
    convolutions and allow to search for channel numbers and network depth. However,
    as the computation of 3D CNN increases more significantly with increasing kernel
    sizes than 2D CNN, the authors keep the kernel size as a constant of 3\. HR-NAS
    [[92](#bib.bib92)] also involves Transformer [[93](#bib.bib93), [94](#bib.bib94)]
    in addition to convolutions due to its recent success in computer vision [[95](#bib.bib95),
    [96](#bib.bib96)]. The authors design a lightweight Transformer that requires
    less computation when facing high-resolution images. Convolutional channels and
    Transformer queries are progressively reduced during the search. In order to facilitate
    high parallelism of convolution operators on TPUs and GPUs, Li et al. [[70](#bib.bib70)]
    add space-to-depth/batch into the search space to increase the depth and batch
    dimensions. The results show that even without the lowest FLOPs, their searched
    EfficientNet-X models are the fastest among compared model families on TPUs and
    GPUs. The main reason is that EfficientNet-X models strike a balance between FLOPs
    (lower is good for speed) and operational intensity (higher is good for speed).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to parametric operators, non-parametric operators are critical
    components of an efficient search space as well. Pooling is usually coupled with
    convolutions in hand-crafted CNN model family, and so does in convolution-based
    search space [[87](#bib.bib87), [80](#bib.bib80)]. It can help to reduce redundant
    information (favourable for accuracy) and computation (favourable for speed) without
    requiring additional parameters. Skip is another widely adopted operator that
    impacts the topology of achieved models. It can have two concepts: 1) drop skip
    directly feeding input to output without any actual computations, i.e., the entire
    layer is dropped and thus the model depth is reduced [[66](#bib.bib66), [86](#bib.bib86),
    [67](#bib.bib67)]; 2) residual skip providing an identity residual connection
    in parallel with another computation operator [[64](#bib.bib64), [80](#bib.bib80)].
    The residual connection can be achieved by either concatenation or by addition
    [[64](#bib.bib64)]. Activation functions are another searchable non-parametric
    operators that impact both accuracy and speed [[87](#bib.bib87), [70](#bib.bib70)].
    The computation cost of activation functions decreases when going deeper since
    the resolution of feature maps decreases [[72](#bib.bib72)]. In some research,
    the non-linearity is associated with convolution operators as a whole, and the
    operator together with its activation function is determined automatically [[79](#bib.bib79)].'
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Backbones
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After defining the operators that a model can have, it is essential to determine
    how many operators there are and how these operators are connected, i.e., the
    backbone of a model. However, operators and backbones are not completely isolated,
    such as the skip operator, which removes layers or adds connections and can change
    a model’s backbone. Depending on the connection topology, backbones can be roughly
    classified into two categories: chain-structured and multi-branch.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/22cfdcb6232686bf2fa2894c0ab2b7a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Schematics of different backbones. Each block represents an operator
    $O_{i}$ in (a) and (b), and represents a cell $C_{i}$ in (c), and different colours
    indicate different operator/cell types. The arrows indicate the information flow
    direction. In the macro-micro backbone, the macro-structure describes the topology
    of multiple cells, each of which has a micro-structure of operators.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chain-structured Backbones. The chain-structured backbone [[57](#bib.bib57)]
    is the earliest and simplest topological structure of a neural network. It directly
    stacks multiple operators in sequence. As shown in Fig. [3](#S3.F3 "Figure 3 ‣
    III-A2 Backbones ‣ III-A Efficient Search Space ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey") (a), the $i$th operator ($O_{i}$) takes the output
    of the $i$-1th operator ($O_{i-1}$) as input and its output serves as the input
    of the operator ($O_{i+1}$). Therefore, the topological connection of different
    operators is determined and not searchable. However, it is possible to search
    how many operators there are, e.g., via the drop skip. This simple backbone is
    a favourable practice of many hand-crafted CNN models [[83](#bib.bib83), [11](#bib.bib11),
    [9](#bib.bib9), [10](#bib.bib10)].'
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive implementation is to stack operators layer by layer [[87](#bib.bib87),
    [77](#bib.bib77)]. Motivated by the design principle of existing success [[10](#bib.bib10),
    [11](#bib.bib11)], the chain-structured backbone is more commonly implemented
    in a cell-based, a.k.a. block- or stage-based, manner [[68](#bib.bib68), [81](#bib.bib81),
    [79](#bib.bib79), [91](#bib.bib91), [86](#bib.bib86), [67](#bib.bib67)], where
    a backbone is composed of multiple chain-structured cells, each of which contains
    multiple chain-structured operators. The operators in the same cell can be either
    identical or diverse in hyperparameters but the same in type. In the sequence
    of cells, it is a usual principle that the input resolutions are reduced and widths
    are increased gradually. To achieve the reduced resolutions, the first or last
    operator in a cell usually has manually set strides and widths. This cell-based
    implementation is not only effective but also reduces the search space. Wu et
    al. [[86](#bib.bib86)] design a chain-structured backbone consisting of four searchable
    cells. There are 8 candidate convolution operators with various expansion rates,
    kernel sizes and numbers of groups, and a drop skip operator for searching. Different
    cells are separated by their input resolutions and widths, which are determined
    by manually set parameters. Yan et al. [[81](#bib.bib81)] also follow the same
    practice to construct their backbone but with more flexibility in searchable parameters.
    In these studies, it is usually required to predefine some parameters of the backbone,
    like the number of cells.
  prefs: []
  type: TYPE_NORMAL
- en: In comparison with setting backbone structure empirically, some work directly
    embraces the backbone of existing models as a starting point and then searches
    beyond that [[76](#bib.bib76), [69](#bib.bib69), [88](#bib.bib88), [23](#bib.bib23),
    [66](#bib.bib66), [69](#bib.bib69)]. This design principle relieves experts’ burden
    of search space design by reusing prior knowledge, which is important in NAS [[97](#bib.bib97)].
    MONAS [[88](#bib.bib88)] relies on the backbone of a simplified version of AlexNet
    [[83](#bib.bib83)] to search the convolution filter sizes and amounts. Scheidegger
    et al. [[62](#bib.bib62)] investigate the backbone of MobileNetV2 [[10](#bib.bib10)]
    and allows to search the number of operators in each cell but all operators in
    a cell have the same settings except for the stride, which is used to modify the
    output resolution. The complexity reduction is mainly obtained by lowering the
    channel widths and reducing the number of topological replications. In addition
    to following the backbone of hand-crafted models, there is a trend to using the
    backbone of existing searched efficient models. MOGA [[76](#bib.bib76)] adopts
    MobileNetV3-large [[72](#bib.bib72)] as its backbone and keeps the same number
    and type of operators. It only searches the parameters of operators, like kernel
    sizes, expansion ratios for MBConv, and whether SE is enabled or not. Cai et al.
    [[77](#bib.bib77)] also adopts MobileNetV3, but they additionally provide the
    flexibility of searching the number of operators in each cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mutli-branch Backbones. While the chain-structured backbone is simple, it restricts
    the information flow to be sequential and single-path. Current hand-crafted architectures
    have suggested that a multi-branch architecture, which allows multi-path information
    flow and residual skip connections, works impressively better than a chain-structured
    architecture [[98](#bib.bib98), [47](#bib.bib47)]. As illustrated in Fig. [3](#S3.F3
    "Figure 3 ‣ III-A2 Backbones ‣ III-A Efficient Search Space ‣ III Search for Efficient
    Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey") (b), in the multi-branch backbone, an operator
    $O_{i}$ is allowed to accept the outputs from some of its previous operators (i.e.,
    $O_{1}$, $O_{2}$ … $O_{i-1}$) as the input but not necessarily takes all. This
    setting provides more degrees of freedom on network topology. Note that a chain-structured
    backbone with residual skip connections, where an operator $O_{i}$ must receive
    the output of its immediate previous operator $O_{i-1}$, is a special case of
    the multi-branch backbone.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The multi-branch backbone can be achieved by using an insertion operation in
    the search space [[65](#bib.bib65)]. However, searching for a multi-branch backbone
    as a whole is time-consuming and difficult to find an optimal structure. Similar
    to the chain-structured backbone, the cell-based principle is also widely adopted
    in designing a multi-branch backbone [[80](#bib.bib80), [61](#bib.bib61), [68](#bib.bib68)],
    where the topology within a cell has a multi-branch structure. This results in
    a Macro-micro Backbone (Fig. [3](#S3.F3 "Figure 3 ‣ III-A2 Backbones ‣ III-A Efficient
    Search Space ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") (c)) [[48](#bib.bib48),
    [61](#bib.bib61), [92](#bib.bib92), [84](#bib.bib84)]. In the macro-structure,
    residual skips are optional to provide the flexibility of multi-path information
    flow; in the micro-structure, operators are connected in the multi-branch fashion
    with the last operator (either parametric or non-parametric) assembling a single
    output of the cell. For example, MnasNet [[48](#bib.bib48)] predefines a backbone
    of 7 sequentially stacked cells and each cell contains sequentially stacked operators
    with a searchable amount, types, and parameters. It further allows searchable
    residual skip connections among operators within a cell. This multi-branch backbone
    simply augments the chain-structured backbone with residual skips, resulting in
    more flexibility of multi-path information flow but marginal research space expansion.
    NSGA-Net [[61](#bib.bib61)] and HR-NAS [[92](#bib.bib92)], in contrast, allow
    more general multi-branch connections within a cell. NSGA-Net supports searchable
    topology while HR-NAS sets fixed multi-branch connections and searches for discarded
    convolutional channels and transformer queries.'
  prefs: []
  type: TYPE_NORMAL
- en: Different from searching for a customized structure, following existing success
    is also a favoured choice in designing multi-branch backbones [[70](#bib.bib70),
    [31](#bib.bib31), [80](#bib.bib80), [72](#bib.bib72)]. Li et al. [[70](#bib.bib70)]
    adopt the multi-branch backbone of the EfficientNet and only search operators.
    MnasFPN [[80](#bib.bib80)] constructs its search space based on the NAS-FPN(Lite)
    backbone and searches both multi-branch structures and operators in a cell for
    merging various resolutions. However, to reduce the search burden, it does not
    allow general connectivity patterns and applies limited merging connections. MobileNetV3
    [[9](#bib.bib9)] uses the backbone of MnasNet [[48](#bib.bib48)] as the seed and
    proceeds layer-wise search on it. Scheidegger et al. [[62](#bib.bib62)] investigate
    the backbone of several existing models (DenseNet121 [[60](#bib.bib60)], MobileNetV2
    [[10](#bib.bib10)], GoogLeNet [[99](#bib.bib99)], PNASNet [[100](#bib.bib100)],
    and ResNeXt [[82](#bib.bib82)]) and demonstrate that their approach is able to
    provide improved accuracy with hardware constraints and different backbones. While
    considering that searching both macro and micro structures overburdens the searching
    process, most works utilize the multi-branch macro backbone of existing models
    and only search the micro-structures [[23](#bib.bib23), [85](#bib.bib85), [88](#bib.bib88)].
    ProxylessNAS [[23](#bib.bib23)] accepts the backbone of the residual PyramidNet
    [[101](#bib.bib101)], which has a residual skip connection every two operators,
    and replaces the original operators with their own tree-structured cells [[102](#bib.bib102)].
    DPP-Net [[85](#bib.bib85)] selects the backbone of CondenseNet [[89](#bib.bib89)],
    which repeats an identical cell abundant times with both residual skip and chain
    connections, and only searches the operators in the cell. MONAS [[88](#bib.bib88)]
    also reuses the backbone of CondenseNet but it uses the same cell structure and
    searches the number of stages and growth rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the macro-micro backbone provides the highest flexibility and complexity
    among the above three categories, there is no evidence that it is the best choice.
    However, we note that no matter in which backbone category, cell-based implementation
    is the most common practice. This is due to the following considerations: i) an
    effective network usually has gradually shrinking resolutions as going deep, and
    a cell can have multiple operators on the same resolution to strengthen feature
    extraction; ii) previous experience with the manually designed network indicates
    that the cell-based structure is effective for deep learning; iii) the cell-based
    backbone provides high search efficiency by limiting the search space (e.g., the
    whole network can stack the same cell and a cell can have the same operator);
    iv) the cell-based backbone simplifies the network topology with inter- and intra-cell
    connectivity instead of random edges among all operators. Some recent works reveal
    that search performance may be impeded by these search space design biases [[103](#bib.bib103)].
    Nevertheless, the choice of the search space principally regulates the difficulty
    of the search process. Current NAS algorithms are imperfect so more freedoms in
    the search space do not always lead to better resultant models but inversely overburden
    the search algorithms. With the continued improvement of NAS algorithms, it is
    essential to reduce design biases and construct a more general search space.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Search Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The search strategy describes how to explore a search space to find an optimal
    efficient network. Essentially, the search process is to find the top candidate
    networks regarding some evaluation metrics (e.g., accuracy and latency). However,
    it is computationally prohibitive to achieve these metrics of all candidate networks
    since it requires fully training a network to obtain its metrics. Therefore, the
    aim of a search strategy is to efficiently find top-ranking networks without exhaustively
    examining all candidate networks. In this section, we first conclude the search
    algorithms that describe how the search proceeds and then summarize how the hardware
    constraint is incorporated into the search process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/10b2cd79f02abca56f1fdfe9dd1f5ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Bayesian optimization
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2699d5d565557c0871b5ed13f8c07c2f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Evolutionary search
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/457e52c36c078233b34112cb7e97f317.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bfcabb5188d6d5f386cdc286d1f383bf.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Differentiable search
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Schematics of different search algorithms. (a) Bayesian optimization;
    (b) Evolutionary search; (c) Reinforcement learning; (d) Differentiable search.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Search Algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most intuitive and easiest search algorithm is the grid search, which exhaustively
    explores the search space and evaluates every possible architecture. This approach
    works well for a small space [[68](#bib.bib68)] but is terribly inefficient for
    a large space due to the exponentially increased number of evaluations. Another
    problem with this type of method is that it only supports a bounded and discrete
    space and needs careful selection of the grid interval. Random search [[104](#bib.bib104)],
    on the other hand, samples neural architectures randomly from the search space.
    It can be used not only for a discrete space but also for a continuous space with
    a predefined distribution. This algorithm is superior to the grid search also
    when the search dimensions (e.g., kernel sizes, expansion ratios, and network
    depths) have different effects on the final performance. Random search and its
    simple variants are usually used when the performance of a candidate model is
    easy to obtain [[84](#bib.bib84), [75](#bib.bib75)]. Advanced search algorithms,
    like reinforcement learning and evolutionary search, are widely demonstrated better
    performance than random search [[49](#bib.bib49), [105](#bib.bib105), [106](#bib.bib106)].
    In the following, we summarize the advanced search algorithms, including Bayesian
    optimization, evolutionary search, reinforcement learning, and differentiable
    methods, regarding design automation for efficient deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian Optimization (BayesOpt) is an important approach for automated hyperparameter
    tuning and architecture search. Given a search space $\mathbb{S}$ that contains
    a large set of neural architectures, and a black-box objective function $f(\cdot)$
    from an neural architecture to an evaluation metric (e.g., accuracy), the goal
    of BayesOpt is to find an architecture $a\in\mathbb{S}$ that maximizes $f(\textbf{a})$:
    $\textbf{a}^{\ast}=\operatorname*{argmax}_{\textbf{a}\in\mathbb{S}}f(\textbf{a})$.
    However, it is expensive to achieve $f(\textbf{a})$ because it requires fully
    training the architecture a from scratch. Therefore, a statistical model, which
    is invariably a Gaussian process (GP), is used as a surrogate model. The more
    neural architectures (i.e., points of $f(\cdot)$) are evaluated, the less uncertainty
    the surrogate model has. Another important component of BayesOpt is the acquisition
    function for deciding which architectures are sampled and then evaluated at each
    iteration, which is often expected improvement (EI):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\text{EI}(\textbf{a})=\mathbb{E}_{max}(f(\textbf{a})-f(\textbf{a}^{+}),\;0),$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\hskip 85.35826pt\textbf{a}^{+}=\operatorname*{argmax}_{\textbf{a}_{i}\in\textbf{a}_{1:t}}f(\textbf{a}_{i}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where a is a sampling architecture and $f(\textbf{a}^{+})$ is the evaluation
    metric of the best architecture $\textbf{a}^{+}$ so far, that is until $t$-th
    exploration. The general process of BayesOpt is shown in Fig. [4a](#S3.F4.sf1
    "In Figure 4 ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey"), where multiple architectures are first randomly sampled from
    the efficient search space; the sampled architectures are then trained and evaluated
    for updating a GP model; the acquisition function is computed based on the updated
    GP model to decide which neural architectures should be sampled next; the process
    proceeds iteratively until certain conditions are met. The best architecture among
    all sampled architectures is usually output as the final model.'
  prefs: []
  type: TYPE_NORMAL
- en: When additionally considering the hardware constraints, Multi-Objective Bayesian
    Optimization (MOBO) is commonly used [[107](#bib.bib107), [108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)], where GP is fitted for each objective
    independently, and Pareto-frontier is identified as the set of optimal trade-off
    solutions of the multiple objectives. Different MOBO approaches mainly differ
    in how to achieve the Pareto-frontier during the acquisition process. For example,
    Parsa et al. [[109](#bib.bib109)] use a Gaussian distribution to estimate the
    Pareto-frontier function; Eriksson et al. [[110](#bib.bib110)] use the Noisy Expected
    Hypervolume Improvement (NEHVI) acquisition function, which is a noise-tolerant
    and extended version of EI for the multi-objective setting, to sample intermediate
    Pareto-frontiers. Although BayesOpt has shown promising performance, especially
    in hyperparameter optimization, it is computationally expensive and struggling
    when handling a high-dimensional search space and multiple objectives [[111](#bib.bib111),
    [112](#bib.bib112)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Evolutionary Search (ES) is a long-thriving NAS approach, which is based on
    the concept of biological evolution. It manages to find an optimal solution by
    iteratively improving upon a population of candidate solutions according to a
    fitness function. As illustrated in Fig. [4b](#S3.F4.sf2 "In Figure 4 ‣ III-B
    Search Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") (the schematic
    of the basic ES process), a population of candidate networks is first initialized
    by randomly sampled from the efficient search space; then evolutionary operations,
    such as crossover (combination of two parents) and mutation (arbitrarily mutating
    operators with new ones from the search space), are applied to current population
    to generate next generation; lastly, all offsprings need to be tested against
    a fitness function, which is to select the most powerful candidate networks and
    update the population; the process proceeds repeatedly until stopping criteria
    are met. The best network in the last population is the final achieved model.
    The major benefit of evolutionary search is its flexibility in directly controlling
    the offspring generation process and population updating process [[105](#bib.bib105)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The hardware constraints are usually considered in the fitness function of
    ES in two ways: hard-constraint and soft-constraint, which we will present concretely
    in the flowing section [III-B2](#S3.SS2.SSS2 "III-B2 Hardware-constraint Incorporation
    Strategy ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey"). Thus, most studies directly use existing evolutionary algorithms with
    customised fitness function to incorporate hardware constraints [[78](#bib.bib78),
    [91](#bib.bib91), [81](#bib.bib81), [113](#bib.bib113), [77](#bib.bib77), [114](#bib.bib114),
    [105](#bib.bib105), [115](#bib.bib115)]. For example, FBNetV3 [[78](#bib.bib78)]
    uses the adaptive genetic algorithms [[116](#bib.bib116)] to realize adaptive
    probabilities of crossover and mutation, and customizes the fitness function with
    its proposed accuracy predictor and hard hardware constraints; OFA [[77](#bib.bib77)]
    adopts the regularized evolutionary search [[117](#bib.bib117)], which introduces
    an age property to favor younger generation during search; DONNA [[113](#bib.bib113)],
    FairNAS [[118](#bib.bib118)], and MOGA [[76](#bib.bib76)] appeal to the famous
    NSGA-\@slowromancapii@ [[119](#bib.bib119)], which is a multi-objective evolutionary
    algorithm, to find the Pareto-optimal solution instead of the solution under hard
    constraints in previous studies. In contrast to directly using off-the-shelf ES
    algorithms, some researchers develop ES algorithms specific to the hardware efficient
    application [[64](#bib.bib64), [61](#bib.bib61), [62](#bib.bib62)]. LEMONADE [[64](#bib.bib64)]
    is such a representative work, which handles various objectives differently considering
    that different objectives have different evaluation costs. Specifically, the efficiency
    objective (e.g., FLOPs) is cheap to evaluate while the accuracy objective is much
    more expensive as it requires fully training the model at each iteration. The
    authors propose to first select the architectures that fulfil the Pareto front
    for the cheap objectives and then only train and evaluate these networks. As traditional
    ES can only sample a limited amount of networks that satisfy hardware constraints,
    Scheidegger et al. [[62](#bib.bib62)] use ES algorithms to search sampling laws
    that can better cover the sub search space under specific constraints. Thus, better
    Pareto frontiers can be achieved. NSAG-Net [[61](#bib.bib61)] designs an additional
    exploitation step after the traditional evolutionary operating to learn and leverage
    the history of evaluated populations. Since the evaluated populations at each
    iteration rank relatively high regarding the fitness score, it is quite possible
    that the optimal model has similarities to the evaluated populations and thus
    extracting common patterns from the evaluated populations can accelerate convergence.
    Although the above studies manage to lighten the search process from the algorithm
    perspective, it is still costly due to the requirement of training each network
    of the new generation at each interaction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinforcement Learning (RL) has achieved notable success and grabbed great
    attention in the NAS community since Zoph and Le’s work in 2017 [[52](#bib.bib52)].
    The general framework of RL for NAS is illustrated in Fig. [4c](#S3.F4.sf3 "In
    Figure 4 ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey"): the controller, usually an RNN network, is the core component, which
    generates actions to sample operators or hyperparameters from the efficient search
    space; a neural network will then be constructed using the sampled options and
    trained and evaluated to achieve a reward, which will be used to update the controller
    using a policy gradient method [[120](#bib.bib120)]. This process will be performed
    repeatedly until stopping criteria are met.'
  prefs: []
  type: TYPE_NORMAL
- en: The hardware information is often considered in the RL reward function. Some
    studies thus directly use the existing RL-based NAS approach [[52](#bib.bib52)],
    but optimize with a customized reward function [[88](#bib.bib88), [65](#bib.bib65)].
    Furthermore, most recent work mainly follows the advanced RL search algorithms
    in two representative studies, MnasNet [[48](#bib.bib48)] and TuNAS [[106](#bib.bib106)].
    Specifically, [[80](#bib.bib80), [72](#bib.bib72)] follow MnasNet, and [[69](#bib.bib69),
    [31](#bib.bib31)] follow TuNAS. Similar to [[52](#bib.bib52)], MnasNet [[48](#bib.bib48)]
    also uses RNN as the controller while maximizing the expected reward using an
    advanced policy gradient method, Proximal Policy Optimization (PPO) [[121](#bib.bib121)],
    to alleviate the high gradient variance of the vanilla policy gradient method.
    TuNAS is based on ENAS [[122](#bib.bib122)] and ProxylessNAS [[23](#bib.bib23)].
    It improves with warmup and channel masking techniques for better search robustness
    and scalability. Instead of building one candidate network at each training step,
    TuNAS encompasses all candidate networks into a supernet. Each path of the supernet
    represents a candidate network. In addition, TuNAS is not a sequential decision-making
    process and does not rely on an RNN controller; alternatively, it uses learnable
    probability distribution that spans over architectural choices as the RL controller.
    At each step, a candidate network is sampled from the distribution, and then the
    portion of the supernet correlated with the sampled network is trained; a reward
    is calculated with the sampled network to update the probability distribution
    i.e., the RL controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'Differentiable Search is the most recently developed search paradigm that finds
    an optimal network by gradient descent [[54](#bib.bib54)]. As shown in Figure
    [4d](#S3.F4.sf4 "In Figure 4 ‣ III-B Search Strategy ‣ III Search for Efficient
    Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey"), a network is as a directed acyclic graph consisting
    of an ordered sequence of $N$ nodes ($N=4$ in Figure [4d](#S3.F4.sf4 "In Figure
    4 ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design
    Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey"):
    $X_{0},X_{1},X_{2},X_{3}$). Each node $X_{i}$ is a latent representation and each
    directed edge represents a candidate operator $O_{i}$ that is applied to $X_{i}$.
    For example, in Figure [4d](#S3.F4.sf4 "In Figure 4 ‣ III-B Search Strategy ‣
    III Search for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey"), there are three candidate operators
    (i.e., $O_{1}$, $O_{2}$, and $O_{3}$) between node $X_{0}$ and $X_{3}$. In this
    way, the search process is formulated as an optimal path-finding problem. All
    outputs of the candidate operators from a node’s predecessors are summed with
    weights $\alpha_{i}$ to achieve the node representation: $X_{j}=\sum_{i=1}^{m}\alpha_{i}O_{i}(X_{i})$,
    subject to $\alpha_{i}\geq 0,\sum_{i=1}^{m}\alpha_{i}=1$. In Figure [4d](#S3.F4.sf4
    "In Figure 4 ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey"), edges with darker colours indicate larger weights. The weights
    $\alpha_{i}$ can be represented with a softmax function of architectural parameters
    $\beta_{i}$: $\alpha_{i}=\frac{exp(\beta_{i})}{\sum_{i=1}^{m}exp(\beta_{i})}$.
    The network weights ($w$) and architectural parameters ($\beta_{i}$) are trained
    alternatively with training data and validation data, respectively. This induces
    a bi-level optimization problem [[54](#bib.bib54)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\alpha}\min_{w_{\alpha}}\mathbfcal{L}(\alpha,w_{\alpha}).$ |  |
    (11) |'
  prefs: []
  type: TYPE_TB
- en: Therefore, unlike ES and RL, a differentiable search unifies model training
    and search into a joint procedure. As the whole network contains multiple parallel
    operators in each layer, it is called a supernet, and its subnet with one operator
    in each layer is a candidate network. The candidate operator with the highest
    associated weight is chosen to construct the final model. The training of the
    supernet is also the search process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hardware information can be considered in the loss function and optimized
    when training the architectural parameters. Various differentiable search algorithms
    are proposed to achieve better search efficiency and performance [[79](#bib.bib79),
    [86](#bib.bib86), [23](#bib.bib23), [66](#bib.bib66), [92](#bib.bib92), [67](#bib.bib67)].
    FBNetV1 [[86](#bib.bib86)] replaces the softmax function with the Gumbel softmax
    function [[123](#bib.bib123)] to better represents the subnet sampling process,
    and thus reduce the performance gap between the supernet and subnet. HR-NAS [[92](#bib.bib92)]
    progressively discards the paths with low path weights during the search to increase
    the search efficiency. DenseNAS [[67](#bib.bib67)] splits the search procedure
    into two stages: the first stage optimizes the model weights only for enough epochs
    and the second stage alternatively optimizes the model weights and architectural
    parameters. This strategy alleviates the bias of fast convergence operators. Considering
    the small size of the search space of the conventional differentiable search due
    to its requirement of loading the whole supernet into memory, many studies propose
    novel differentiable search algorithms to reduce the memory footprint [[23](#bib.bib23),
    [66](#bib.bib66), [79](#bib.bib79), [67](#bib.bib67)]. ProxylessNAS [[23](#bib.bib23)]
    proposes to factorize the task of training of all paths into training two sampled
    paths, which have the highest sampling probabilities, and discard the other paths
    temporarily at each iteration. The architectural parameters of the two selected
    paths are updated during training and then rescaled after training to keep the
    path weights of unsampled paths unchanged. In this way, the memory requirement
    is decreased to the level of training two subnets. Different from previous papers,
    which use multi-path supernets and thereby have memory issues, Single-Path NAS
    [[66](#bib.bib66)] and FBNetV2 [[79](#bib.bib79)] develop masking mechanisms that
    train a supernet with the largest hyperparameters (e.g., $7\times 7$ kernel) and
    a mask/indicator function that determines whether to just use a small part (e.g.,
    $5\times 5$ or $3\times 3$ kernel) of the largest hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Other types of search algorithms are also reported [[87](#bib.bib87), [85](#bib.bib85)].
    Xiong et al. [[87](#bib.bib87)] propose a modified Cost-Effective Greedy algorithm
    for the submodular NAS process, where starting from an empty network, each block
    is filled iteratively with the highest marginal gain ratio regarding both accuracy
    and cost. DPP-Net [[85](#bib.bib85)] follows the progressive search algorithm
    in [[100](#bib.bib100)] to find the optimal operator layer by layer. Different
    from searching for a new model for each new resource budget, another way is to
    build a baseline model first and then scale it to obtain a family of models for
    various budgets [[68](#bib.bib68), [70](#bib.bib70)]. Specifically, they adopt
    the conventional RL-based hardware-aware search algorithm [[48](#bib.bib48)] to
    search for a small baseline model, and use a simple grid search to determine the
    best scaling coefficient (i.e., $\alpha$, $\beta$, $\gamma$) for network depth,
    width, and input size. A family of specialized models for different budgets can
    be obtained by scaling up the baseline model by $\alpha^{N}$, $\beta^{N}$, and
    $\gamma^{N}$ with $2^{N}$ times increased resource.
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Hardware-constraint Incorporation Strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite some studies only using an efficient search space without considering
    hardware constraint [[49](#bib.bib49)], there are two strategies commonly used
    to consider hardware budget for finding optimal compact models. The first considers
    a specific hardware platform and treats its resources as hard constraint to build
    a specialized model that is the most accurate under the fixed constraint. In contrast,
    the second strategy is to directly search networks without considering specific
    hardware constraint. This strategy, dubbed soft-constraint incorporation, treats
    the model efficiency as an additional optimization objective and tries to find
    the Pareto frontiers. In this section, we summarize the design automation techniques
    from the above two perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hard-constraint Incorporation can be easily adopted by evolutionary search
    [[81](#bib.bib81), [91](#bib.bib91), [77](#bib.bib77), [78](#bib.bib78), [62](#bib.bib62)]
    since the offspring generation and selection step can be directly controlled.
    The incorporation process is quite straightforward: when a set of candidate networks
    is generated (e.g., by evolutionary operating), their hardware costs are then
    tested, and the networks that do not meet the constraint are simply discarded.
    This strategy is also easy to implement for other search algorithms, which generate
    a set of candidate networks one time or at each iteration, such as random search
    [[84](#bib.bib84), [75](#bib.bib75)], grid search [[68](#bib.bib68)], and greedy
    search [[87](#bib.bib87)]. The RL-based search algorithms can also embrace the
    hard-constraint incorporation strategy but in a different way [[65](#bib.bib65),
    [48](#bib.bib48), [88](#bib.bib88)]. The general idea is to have only the accuracy
    in the reward function when the constraint is met; otherwise, both accuracy and
    the hardware objective are considered in the reward function. A typical reward
    formula $\mathbfcal{R}(\cdot)$ is proposed in MnasNet [[48](#bib.bib48)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbfcal{R}(a)=\begin{cases}ACC(a),&amp;\text{if}\;\;C(a)\leq C_{0}\\
    ACC(a)\times(C(a)/C_{0})^{\beta},&amp;\text{if}\;\;C(a)>C_{0}\end{cases}$ |  |
    (12) |'
  prefs: []
  type: TYPE_TB
- en: where $ACC(a)$ and $C(a)$ denote the accuracy and hardware cost (e.g., latency
    in [[48](#bib.bib48)]) of model $a$, respectively, and $C_{0}$ denotes the target
    cost constraint. $\beta<0$ is the only tunable hyperparameter that controls the
    convergence speed. In [[106](#bib.bib106)], the authors empirically find that
    if $\beta$ is too large, the RL-controller will prefer to sample the architectures
    whose cost is significantly smaller than the target cost constraint. This will
    result in suboptimal models regarding accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Soft-constraint Incorporation does not target any specific constraint and is
    realized by adding an additional optimization objective to the accuracy objective.
    Since these objectives are competing, no unique optimal solution exists in the
    multi-objective space. Thus, Pareto frontiers are sought, especially by multi-objective
    BayesOpt and multi-objective evolutionary algorithms, and one specific model can
    be identified according to different application requirements [[61](#bib.bib61),
    [109](#bib.bib109), [113](#bib.bib113), [76](#bib.bib76), [107](#bib.bib107),
    [64](#bib.bib64), [108](#bib.bib108), [115](#bib.bib115), [118](#bib.bib118)].
    For example, ChamNet [[115](#bib.bib115)] designs a multi-objective fitness function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbfcal{F}(a)=ACC(a)-[\alpha H(C(a)-C_{0})]^{\beta},$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'where $C(a)$ and $C_{0}$ denote the hardware cost of model $a$ and the target
    constraint, respectively, $H$ is the Heaviside step function, and $\alpha$ and
    $\beta$ are positive constants. The searching objective is to maximize the fitness
    function: $a^{*}=\operatorname*{argmax}_{a}(\mathbfcal{F}(a))$. This fitness function
    guides the search process to find a model approaching the hardware target but
    without a hard guarantee. Some work also pursues the soft-constraint objective
    under a hard constraint with a two-phase approach, which first filters out models
    that do not satisfy the hard constraint and then performs multi-objective optimization
    [[85](#bib.bib85), [62](#bib.bib62)]. Instead of finding a set of solutions, some
    studies attempt to find the best tradeoff between efficiency and effectiveness.
    Two schemes are commonly used in this regard and mainly for the RL-based and differentiable
    algorithms:multiplication and linear combination. For the multiplication scheme,
    the effectiveness objective (e.g., accuracy) and efficiency objective (e.g., latency)
    are multiplied to construct the loss function [[86](#bib.bib86)] or reward function
    [[72](#bib.bib72), [48](#bib.bib48), [80](#bib.bib80), [70](#bib.bib70)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{loss function: }\mathbfcal{L}(a)=\mathbfcal{L}_{CE}(w&#124;a)\times\text{log}(C(a))^{\beta},$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{reward function: }\mathbfcal{R}(a)=ACC(a)\times(C(a)/C_{0})^{\beta}.$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: 'The $\mathbfcal{L}_{CE}(w|a)$ is the cross-entropy loss of model $a$ with parameter
    $w$. The exponent coefficient ($\beta>0$ for the loss function; $\beta<0$ for
    the reward function) modulates the trade-off between effectiveness and efficiency.
    Note that although there is a target constraint term $C_{0}$ in the reward function,
    this is still a soft constraint since this just pushes the cost to be lower than
    the target but without any guarantee. Alternatively, it is possible to keep running
    the search process until the hardware constraint is satisfied [[72](#bib.bib72)].
    For the linear combination scheme, the two competing objectives are linearly combined
    to construct the loss function [[66](#bib.bib66), [67](#bib.bib67), [79](#bib.bib79),
    [92](#bib.bib92), [23](#bib.bib23)] or reward function [[69](#bib.bib69), [106](#bib.bib106),
    [31](#bib.bib31)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{loss function: }\mathbfcal{L}(a)=\mathbfcal{L}_{CE}(w&#124;a)+\beta\>\text{log}(C(a)),$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\text{reward function: }\mathbfcal{R}(a)=ACC(a)+\beta&#124;C(a)/C_{0}-1&#124;.$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: Similar to the multiplication scheme, the $\beta$ coefficient balances the effectiveness
    and efficiency. The log($\cdot$) function in the hardware-related term in the
    loss function is used to scale the cost and are omitted in some studies [[79](#bib.bib79),
    [23](#bib.bib23), [92](#bib.bib92)]. TuNAS [[106](#bib.bib106)] empirically demonstrates
    that search results are robust to the exact value of $\beta$. It shows that the
    same $\beta$ value works great for different search spaces and hardware constraints.
    This $\beta$ value-invariance alleviates the tedious tuning of $\beta$ for new
    scenarios (e.g., new devices).
  prefs: []
  type: TYPE_NORMAL
- en: 'For non-differentiable search algorithms, the hardware constraint can be smoothly
    incorporated; by contrast, differentiable search algorithms require the cost term
    to be differentiable, which is intrinsically not. Therefore, specific cost estimation
    strategies are expected to resolve this contradiction, which we will introduce
    in Section [III-C2](#S3.SS3.SSS2 "III-C2 Performance Predictor ‣ III-C Performance
    Estimation Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Performance Estimation Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The search process is managed by search strategies, which we have discussed
    in Section [III-B](#S3.SS2 "III-B Search Strategy ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey"), and guided by the performance of candidate models,
    which we will discuss in this section. The performance in the context of hardware-efficient
    models has two aspects: task-related performance (e.g., accuracy) and hardware-related
    performance (e.g., latency). The most natural way to obtain a model’s performance
    is to fully train the model and evaluate it on validation data for task-related
    performance and deploy it on the target hardware for hardware-related performance
    (for direct metrics, e.g., latency) [[107](#bib.bib107), [61](#bib.bib61)]. However,
    it demands massive computation and time to fully train each candidate model from
    scratch and deploy them on target hardware. Therefore, efforts are made toward
    alleviating the performance estimation process. We categorize and discuss these
    efforts into two routes: one is to speed up the training process (section [III-C1](#S3.SS3.SSS1
    "III-C1 Training Strategy ‣ III-C Performance Estimation Strategy ‣ III Search
    for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")), and the other is to directly
    predict the performance without training (section [III-C2](#S3.SS3.SSS2 "III-C2
    Performance Predictor ‣ III-C Performance Estimation Strategy ‣ III Search for
    Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight, and
    Effective Deep Learning Models: A Survey")). Since the hardware cost does not
    require training, studies of estimating hardware-related performance are reviewed
    in section [III-C2](#S3.SS3.SSS2 "III-C2 Performance Predictor ‣ III-C Performance
    Estimation Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") as well.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Training Strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The hardware-aware NAS does not distinguish from conventional NAS regarding
    the training strategies of candidate models so the training strategy summarized
    in this section can also be applied to conventional NAS and vice versa. The simplest
    training strategy is to train each candidate model from scratch and use the sample-eval-update
    loop [[52](#bib.bib52)] to achieve an effective search agent. This strategy is
    not only widely adopted by RL-based search [[48](#bib.bib48), [80](#bib.bib80),
    [72](#bib.bib72), [65](#bib.bib65), [49](#bib.bib49)], but also by BayesOpt [[107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109)], evolutionary search [[61](#bib.bib61),
    [64](#bib.bib64), [62](#bib.bib62)], and other algorithms [[70](#bib.bib70), [68](#bib.bib68)].
    To alleviate the drawback of the straightforward training approach, some studies
    [[80](#bib.bib80), [48](#bib.bib48), [87](#bib.bib87)] follow Zoph et al. [[49](#bib.bib49)]
    to first train and search on proxy tasks, such as smaller datasets or fewer epochs,
    then transfer to the large-scale target task. The proxy task is a reduced version
    of the target task. Xiong et al. [[87](#bib.bib87)] also propose lazy evaluation
    to further allow fewer evaluation. However, this is specific to its search strategy.
    An inevitable shortcoming of using a proxy task is that the model optimized on
    proxy tasks is not guaranteed to be favourable on the target task.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of speeding up the training process is parameter sharing [[124](#bib.bib124)].
    The search space is represented as a single super Directed Acyclic Graph (DAG),
    dubbed a supernet, whose nodes are computation operators and edges illustrate
    the flow of information. Each candidate model is a subgraph/subnet of the supernet
    and candidate models can share their parameters if they share common operators.
    When the supernet is well trained (e.g., trained for enough epochs), a candidate
    model can directly inherit parameters from the supernet without any separate training.
    A more widely used supernet is to have each node represent a latent representation
    (e.g., feature maps of a CONV operator) and each directed edge represents some
    operation (e.g., a CONV operator). There are multiple edges (i.e., different operators)
    between every pair of nodes, and the search process is to determine which edge/edges
    should be retained between each pair of nodes. The common retained edges share
    parameters between different candidate models. Different studies differ in training
    the supernet. Some work [[88](#bib.bib88), [106](#bib.bib106), [114](#bib.bib114)]
    first pre-trains the whole supernet, and then only trains a part (e.g. a subnet)
    of the supernet, which is selected by search algorithms. Specifically, TuNAS [[106](#bib.bib106)]
    disables the RL controller at the first 25% of the search and only trains the
    parameters of the supernet. It randomly selects CNN filters and candidate operators
    to train with some probability $p$, which is linearly decreased from 1 to 0 over
    the first 25% search. During the parameter sharing, TuNAS further shares the “submodule”
    parameters. For example, MBConv operators with different DWSConv kernel sizes
    are in different paths/edges, but they can share the PWConv weights in MBConv
    regardless of which DWConv kernel is selected. In addition, TuNAS also applies
    channel masking for parameter sharing; it creates a convolution with the largest
    possible number of channels while simulating smaller channel numbers by choosing
    the first $N$ channels while zeroing out the remaining ones. These techniques
    are well recognized and followed by many researchers [[31](#bib.bib31), [69](#bib.bib69)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Differentiable NAS naturally uses a parameter-sharing training strategy. It
    jointly trains the supernet’s parameters and the importance weights of each path/edge
    [[86](#bib.bib86)]. Furthermore, some researchers [[67](#bib.bib67), [92](#bib.bib92)]
    propose to progressively discard the paths/edges with low importance during training
    to further accelerate the training process. This progressive shrinking strategy
    speeds up the training process significantly. A defect of the differentiable parameter
    sharing strategy is that the supernet consumes high GPU memory, which would grow
    linearly with regard to the number of candidate models. To fill this gap, the
    single-path training strategy is proposed to reduce the memory cost to the same
    level as training a single candidate model. The conventional parameter sharing
    strategy has different operators on different paths, even though these operators
    are of the same type but with different hyperparameters (e.g., different kernel
    sizes). In light of this observation, Single-Path NAS [[66](#bib.bib66)] designs
    a superkernel $\textbf{w}_{k}$ for the DWConv inside a MBConv to choose between
    a $3\times 3$ or a $5\times 5$ kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{w}_{k}=\textbf{w}_{3\times 3}+\mathbbm{1}(&#124;&#124;\textbf{w}_{5\times
    5\setminus 3\times 3}&#124;&#124;^{2}>t_{k})\cdot\textbf{w}_{5\times 5\setminus
    3\times 3}.$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'It views the $3\times 3$ kernel as the inner core of the $5\times 5$ kernel,
    while zeroing out the weights of the outer shell. The $5\times 5$ kernel can be
    viewed as the summation of this inner core $\textbf{w}_{3\times 3}$ and the outer
    shell $\textbf{w}_{5\times 5\setminus 3\times 3}$: $\textbf{w}_{5\times 5}=\textbf{w}_{3\times
    3}+\textbf{w}_{5\times 5\setminus 3\times 3}$. The choice of the kernel size can
    be done using the indicator function $\mathbbm{1}(\cdot)\in\{0,1\}$ in Equation
    ([18](#S3.E18 "In III-C1 Training Strategy ‣ III-C Performance Estimation Strategy
    ‣ III Search for Efficient Deep Learning Models ‣ Design Automation for Fast,
    Lightweight, and Effective Deep Learning Models: A Survey")), which is relaxed
    to be a sigmoid function $\sigma(\cdot)$ to compute gradients. The $t_{k}$ is
    a learned threshold. Likewise, the NAS decision of the expansion ratio $e\in\{3,6\}$
    of MBConv can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\textbf{w}_{k,e}=\mathbbm{1}(&#124;&#124;\textbf{w}_{k,3}&#124;&#124;^{2}>t_{e=3})\cdot(\textbf{w}_{k,3}+\mathbbm{1}(&#124;&#124;\textbf{w}_{k,6\setminus
    3}&#124;&#124;^{2}>t_{e=6})\cdot\textbf{w}_{k,6\setminus 3}),$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\textbf{w}_{k,3}$ is the channels with expansion ratio $e=3$, and can
    be viewed as the first half of the channels of the MBConv with expansion ratio
    $e=6$, while zeroing out the second half of the channels $\textbf{w}_{k,6\setminus
    3}$. The first indicator function in the Equation ([19](#S3.E19 "In III-C1 Training
    Strategy ‣ III-C Performance Estimation Strategy ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey")) is used to decide whether to skip the MBConv layer.
    This NAS problem can then be formulated as a common single-level optimization
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\textbf{w}}\mathbfcal{L}(\textbf{w}&#124;t_{k},t_{e}),$ |  | (20)
    |'
  prefs: []
  type: TYPE_TB
- en: 'as opposed to the bi-level optimization in Equation ([11](#S3.E11 "In III-B1
    Search Algorithms ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey")). This optimization is solved in a differentiable way. The
    memory consumption of the single-path NAS is of the same level as the largest
    candidate model. Nevertheless, this single-path strategy only validates searching
    for different hyperparameters of the same type of operators, especially for convolution-based
    operators. ProxylessNAS [[23](#bib.bib23)], on the other hand, still relies on
    a multi-path structure but only activates two paths at each training iteration,
    which are sampled with the highest learnable probabilities, and mask out all the
    other paths. FBNetV2 [[79](#bib.bib79)] combines the single-path and multi-path
    strategies. It also considers the smaller number of channels as part of the larger
    volume of channels via vector masks, each of which has ones in the first entries
    and zeros in the remaining entries; different vector masks are selected with Gumbel
    softmax weights; in this way, only the largest set of filters needs to be trained.
    For the resolution search, the authors propose to subsample smaller feature maps
    from the largest feature map, perform convolution, and enlarge with inserted zeros
    to the largest size. This design resolves the resolution mismatch problem during
    single-path training. Although FBNetV2 is still a multi-path structure, it reduces
    the network paths with the single-path strategy thus accelerating the training
    and reducing the memory cost.'
  prefs: []
  type: TYPE_NORMAL
- en: Different from the joint optimization of gradient-based methods, evolutionary
    search or RL-based search decouples the training and search process into two sequential
    steps. Thus, it is unnecessary to use indicator functions or learnable weights
    in the training. The single-path training strategy becomes simpler. For example,
    SPOS [[105](#bib.bib105)] proposes to uniformly sample a single path from a supernet
    and makes it well trained. As a result, all candidate models are trained evenly
    and simultaneously. This strategy alleviates the co-adaptation problem of shared
    weights [[125](#bib.bib125)]. After training, subnets can be directly sampled
    from the supernet for search evaluation. Many papers [[81](#bib.bib81), [91](#bib.bib91),
    [118](#bib.bib118), [76](#bib.bib76)] then follow this simple yet effective strategy,
    and design novel techniques to improve fair sampling and training of candidate
    models [[91](#bib.bib91), [118](#bib.bib118), [76](#bib.bib76)]. A comprehensive
    study is FairNAS [[118](#bib.bib118), [76](#bib.bib76)] that ensures the parameters
    of every candidate operator be updated the same amount of times at any training
    iteration. Specifically, similar to SPOS, it randomly samples a candidate operator
    at each layer to form a subnet but differently, the chosen operators are not put
    back at the next sampling step. The sampling process continues until all operators
    are sampled. The sampled models are trained individually with back-propagation,
    but their gradients are accumulated to update the supernet’s parameters. This
    strategy can alleviate the ordering issue [[118](#bib.bib118)], where candidate
    models are trained with an inherent training order in SPOS. Though SPOS and FairNAS
    accelerate the training process and reduce the memory cost, it is conventionally
    required to retrain the searched model before deployment because the inherited
    parameters from the supernet are not specialized for a specific model. Several
    studies [[75](#bib.bib75), [84](#bib.bib84), [77](#bib.bib77)] attempt to form
    a single-training strategy that does not require any post training after searching.
    BigNAS [[75](#bib.bib75), [84](#bib.bib84)] employs a bunch of training techniques
    (e.g., sandwich rule, in place distillation, and batch norm calibration) [[126](#bib.bib126),
    [75](#bib.bib75)] to achieve a high quality supernet so the inherited parameters
    can work well for any subnets. OFA [[77](#bib.bib77)] is a more promising strategy
    that prevents interference between subnets and thus a derived model can be directly
    deployed. It trains candidate models from the largest size (i.e., largest kernel
    size, depth, and width) to the smallest size (i.e., smallest kernel size, depth,
    and width) progressively. When training smaller subnets, the authors keep the
    last layers or channels untouched and finetune the early ones from shared parameters.
    As for elastic kernel size, the authors train kernel transformation matrices to
    transform the center of the larger kernel into the smaller kernel. The OFA strategy
    ensures each candidate model has a specifically trained or finetuned part so that
    mitigates their interference.
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 Performance Predictor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another strategy to speed up the training process is to estimate the accuracy
    of candidate models using an accuracy predictor [[77](#bib.bib77), [78](#bib.bib78),
    [85](#bib.bib85), [115](#bib.bib115), [113](#bib.bib113)]. Although it requires
    substantial [model, accuracy] pairs thus computation and time to construct an
    accuracy predictor, it is a one-time cost as the predictor can be re-used for
    multiple hardware constraints. In addition, the accuracy predictor can also be
    easily finetuned for new datasets. The input into the accuracy predictor is the
    representation of candidate models, which is often one-hot encoding of candidate
    operators and hyperparameters [[77](#bib.bib77), [85](#bib.bib85)] or continuous
    values of hyperparameters (e.g., for channel counts) [[78](#bib.bib78), [115](#bib.bib115)].
    A different study is DONNA [[113](#bib.bib113)], which uses block-quality metrics
    derived from blockwise knowledge distillation as the input into the accuracy predictor.
    For the predictor architecture, some studies use neural networks (e.g., MLP [[78](#bib.bib78),
    [77](#bib.bib77)] or RNN [[85](#bib.bib85)]) while others use conventional learning
    models (e.g., linear regressor [[113](#bib.bib113)] or Gaussian Process regressor
    [[115](#bib.bib115)]). ChamNet [[115](#bib.bib115)] additionally adopts Bayesian
    optimization for model sampling to achieve better sampling efficiency and reliable
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to an accuracy predictor, predictors for hardware cost are also
    widely studied to reduce the huge communication expense between the target device
    and the model training machine. Furthermore, the hardware cost predictor is demonstrated
    high fidelity across platforms ($r^{2}\geq 0.99$) [[69](#bib.bib69)]. Therefore,
    the cost predictor is popular and necessary. A broadly investigated approach is
    the latency Lookup Table (LUT) [[106](#bib.bib106), [80](#bib.bib80), [86](#bib.bib86),
    [67](#bib.bib67), [66](#bib.bib66), [114](#bib.bib114), [76](#bib.bib76), [115](#bib.bib115),
    [77](#bib.bib77)], which records the runtime of each operator in the search space.
    The basic assumption is that the runtime of each operator is independent of other
    operators [[86](#bib.bib86), [23](#bib.bib23)], so that the latency of an entire
    model $a$ can be estimated as the sum of the latency of each individual operator
    $O_{i}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $LAT(a)=\sum_{i}LAT(O_{i}).$ |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: Researchers profile the target hardware and record the runtime for each candidate
    operator to estimate the latency of candidate models. Some research also considers
    the connectivity of operators and the communication overheads in sequential layers
    [[67](#bib.bib67), [114](#bib.bib114), [80](#bib.bib80)]. Another possible way
    is to construct a regressor to estimate the hardware cost based on critical features
    of a candidate model [[107](#bib.bib107), [23](#bib.bib23), [115](#bib.bib115),
    [69](#bib.bib69), [31](#bib.bib31)]. The performance regressor can be a linear
    regressor [[69](#bib.bib69), [31](#bib.bib31)], a Gaussian Process regressor [[115](#bib.bib115)],
    or a neural network [[107](#bib.bib107), [23](#bib.bib23)]. Different from directly
    measuring the hardware cost and training a black-box performance regressor, two
    papers examine specific devices and derive the runtime [[108](#bib.bib108)] or
    power consumption [[109](#bib.bib109)] through theoretical analysis. Nevertheless,
    this approach requires the knowledge of specific devices and is inflexible to
    different devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hardware cost predictor is essential for differentiable search algorithms.
    Since each candidate operator is to be selected by a binary indicator in a differentiable
    supernet, the latency of it can be the weighted sum (i.e., with binary indicators
    {0,1}) of the latency of each candidate operator [[86](#bib.bib86)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $LAT(a)=\sum_{l}\sum_{i}\mathbb{I}_{l,i}\cdot LAT(O_{l,i}),\;\mathbb{I}_{l,i}\in\{0,1\},$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'where $O_{l,i}$ and $\mathbb{I}_{l,i}$ are the $i$th operator of the $l$th
    layer and its associated binary indicator, respectively. $LAT(O_{l,i})$ can be
    achieved through either LUT [[86](#bib.bib86), [66](#bib.bib66), [67](#bib.bib67)]
    or performance regressor [[23](#bib.bib23)]. However, the loss function ([14](#S3.E14
    "In III-B2 Hardware-constraint Incorporation Strategy ‣ III-B Search Strategy
    ‣ III Search for Efficient Deep Learning Models ‣ Design Automation for Fast,
    Lightweight, and Effective Deep Learning Models: A Survey")) and ([16](#S3.E16
    "In III-B2 Hardware-constraint Incorporation Strategy ‣ III-B Search Strategy
    ‣ III Search for Efficient Deep Learning Models ‣ Design Automation for Fast,
    Lightweight, and Effective Deep Learning Models: A Survey")) with indicator $\mathbb{I}_{l,i}$
    is not directly differentiable. To sidestep this problem, the indicator is relaxed
    to be a continuous variable computed via the Gumbel Softmax function [[123](#bib.bib123)]
    with learnable parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{I}_{l,i}=\frac{exp[(\theta_{l,i}+g_{l,i})/\tau]}{\sum_{i}exp[(\theta_{l,i}+g_{l,i})/\tau]},$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\tau\in(0,1)$ is the temperature parameter that controls the search
    efficiency and efficacy. Larger $\tau$ makes the indicator distribution smoother;
    smaller $\tau$ approaches discrete categorical sampling. Despite the value of
    $\tau$, the loss function ([14](#S3.E14 "In III-B2 Hardware-constraint Incorporation
    Strategy ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey")) and ([16](#S3.E16 "In III-B2 Hardware-constraint Incorporation Strategy
    ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design
    Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey"))
    with the latency calculation ([22](#S3.E22 "In III-C2 Performance Predictor ‣
    III-C Performance Estimation Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey")) and the relaxed indicator ([23](#S3.E23 "In III-C2 Performance
    Predictor ‣ III-C Performance Estimation Strategy ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey")) are differentiable. For single-path training, the
    latency calculation ([22](#S3.E22 "In III-C2 Performance Predictor ‣ III-C Performance
    Estimation Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey")) can be
    simplified [[23](#bib.bib23)] as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $LAT(a)=\sum_{l}\mathbb{I}_{l}\cdot LAT(O_{l}),$ |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: where $O_{l}$ is the selected operator at layer $l$ and $\mathbb{I}_{l}$ is
    the path probability of operator $O_{l}$.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of preparing a hardware cost predictor is minimal since it does not
    require training models. Only one forward pass of a test model is sufficient to
    record its cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Summary of Searching for Efficient Deep Learning Models. The “hardware”
    column indicates on which the achieved models are evaluated. The latency is reported
    per input; otherwise is specified in the relevant table cells (e.g., FPS). $\sim$
    indicates the exact data is not reported in the paper and thus estimated from
    the reported figures. ‘-’ indicates unavailable records.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Search strategy | Reference | Dataset | Hardware | Accuracy | Latency | FLOPs
    | # of Parameters |'
  prefs: []
  type: TYPE_TB
- en: '| BayesOpt+Soft | [[108](#bib.bib108)] | MNIST | Xilinx Zynq-7000 SoC ZC70
    | acc: 99.49 | 64.74ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Xilinx Virtex-7 FPGA VC707 | acc: 99.46 | 105.54ms |'
  prefs: []
  type: TYPE_TB
- en: '|  | CIFAR10 | Xilinx Zynq-7000 SoC ZC70 | acc: 88.25 | 136.15ms |'
  prefs: []
  type: TYPE_TB
- en: '|  | Xilinx Virtex-7 FPGA VC707 | acc: 88.52 | 160.75ms |'
  prefs: []
  type: TYPE_TB
- en: '| ES+Soft | [[61](#bib.bib61)] | CIFAR10 | - | error: 3.85/2.75/2.50 | - |
    1290M/535M/4147M | 3.3M/3.3M/26.8M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[64](#bib.bib64)] | CIFAR10 | - | error: 4.57/3.69/3.05/2.58 | - | -
    | 0.5M/1.1M/4.7M/13.1M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[113](#bib.bib113)] | ImageNet | NVIDIA V100/Samsung S20 | Top1 acc:
    $\sim$79/78.5 | $\sim$20ms/16.25ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[76](#bib.bib76)] | ImageNet | SNPE (Mi MIX3)/ | Top1/Top5 acc: 75.9/92.8
    | 11.8ms/11.1ms/101ms | 304M | 5.1M |'
  prefs: []
  type: TYPE_TB
- en: '|  | MACE (Mi MIX3)/ | Top1/Top5 acc: 75.5/92.6 | 10.3ms/10.0ms/81ms | 248M
    | 5.5M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Google Pixel 1 | Top1/Top5 acc: 75.3/92.5 | 9.6ms/8.8ms/71ms | 221M |
    5.4M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[115](#bib.bib115)] | ImageNet | Snapdragon 835 | Top1 acc: 75.4/73.8/71.6/69.1/64.2
    | 29.8ms/19.9ms/15.0ms/10.0ms/6.1ms | 553M/323M/212M/120M/54M | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[114](#bib.bib114)] | ImageNet | NVIDIA GV100/ | Top1/Top5 error: 23.6/6.9
    | 12.0ms/31.6ms/76.9ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Intel Xeon Gold 6136/ | Top1/Top5 error: 23.5/6.8 | 13.4ms/26.4ms/69.1ms
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | NVIDIA Jetson Xavier | Top1/Top5 error: 23.8/6.9 | 12.9ms/31.8ms/52.7ms
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[62](#bib.bib62)] | CIFAR10 | Raspberry Pi 3(B+) | acc: $\sim$81/$\sim$91/$\sim$94
    | 10ms/100ms/1000ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[118](#bib.bib118)] | ImageNet | - | Top1 acc: 75.3/77.5 | - | 388M/392M
    | 4.6M/5.9M |'
  prefs: []
  type: TYPE_TB
- en: '| RL+Soft | [[48](#bib.bib48)] | ImageNet | Google Pixel 1 | Top1/Top5 acc:
    75.2/92.5 | 78ms | 312M | 3.9M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 75.6/92.7 | 84ms | 340M | 4.8M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 76.7/93.3 | 103ms | 403M | 5.2M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[72](#bib.bib72)] | CIFAR10 | - | error: 2.82 | - | - | 2.5M |'
  prefs: []
  type: TYPE_TB
- en: '|  | CIFAR100 | error: 18.13 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | Top1/Top5 error: 27.5/9.1 | 497M | 4.4M |'
  prefs: []
  type: TYPE_TB
- en: '|  | PTB | Perplexity: 57.5 | - | 23M |'
  prefs: []
  type: TYPE_TB
- en: '|  | WT2 | Perplexity: 69.4 | 33M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[80](#bib.bib80)] | COCO | Google Pixel 1 | mAP: 24.9/25.5 | 187ms/196ms
    | 900M/940M | 2.5M/2.6M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[106](#bib.bib106)] | ImageNet | - | Top1 acc: 75.4 | 57.1ms | - | -
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | COCO | mAP: 22.5 | 106ms |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[31](#bib.bib31)] | ImageNet | Google Pixel 4(CPU/GPU/ | Top1 acc: 74.9
    | 25.2ms/4.47ms/3.38ms/2.22ms | 349M | 4.39M |'
  prefs: []
  type: TYPE_TB
- en: '|  | DSP/EdgeTPU) | Top1 acc: 75.8 | 31.0ms/5.40ms/3.81ms/2.40ms | 433M | 4.91M
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[69](#bib.bib69)] | COCO | Google Pixel 1 CPU | mAP: 23.7/22.7/23.4 |
    122ms/107ms/113ms | 510M/390M/450M | 3.85M/2.57M/4.21M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Google Pixel 4 EdgeTPU | mAP: 25.5/25.4/24.7 | 6.9ms/6.8ms/7.4ms | 1530M/1760M/970M
    | 4.20M/4.79M/4.17M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Google Pixel 4 DSP | mAP: 28.5/28.5/26.9 | 12.3ms/11.9ms/12.2ms | 2820M/3220M/1430M
    | 7.16M/9.15M/4.85M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[23](#bib.bib23)] | CIFAR10 | - | error: 2.30/2.08 | - | - | 5.8M/5.7M
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | Google Pixel 1 | Top1/Top5 acc: 74.6/92.2 | 78ms | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Tesla V100 | Top1/Top5 acc: 75.1/92.5 | 5.1ms |'
  prefs: []
  type: TYPE_TB
- en: '| Differentiable+Soft | [[86](#bib.bib86)] | ImageNet | Samsung Galexy S8 |
    Top1 acc: 73.0/74.1/74.9 | 19.8ms/23.1ms/28.1ms | 249M/295M/375M | 4.3M/4.5M/5.5M
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[79](#bib.bib79)] | ImageNet | - | Top1 acc: 68.3/73.2/76.0/77.2 | -
    | 56M/126M/238M/325M | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[66](#bib.bib66)] | ImageNet | Google Pixel 1 | Top1/Top5 acc:74.96/92.21
    | 79.48ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[67](#bib.bib67)] | ImageNet | TITAN-XP | Top1 acc: 76.1/73.1/74.6/75.3
    | 28.9ms/13.6ms/15.4ms/17.9ms (per batch) | 479M/251M/314M/361M | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[23](#bib.bib23)] | ImageNet | Google Pixel 1 | Top1/Top5 acc: 74.2/91.7
    | 79ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[92](#bib.bib92)] | ImageNet | - | Top1 acc: 76.6/77.3 | - | 267M/325M
    | 5.5M/6.4M |'
  prefs: []
  type: TYPE_TB
- en: '|  | COCO key-point | AP/$\text{AP}^{\text{M}}$/$\text{AP}^{\text{L}}$/AR:
    75.5/72.6/81.7/79.4 | 3720M | 6.6M |'
  prefs: []
  type: TYPE_TB
- en: '|  | AP/$\text{AP}^{\text{M}}$/$\text{AP}^{\text{L}}$/AR: 65.7/62.5/72.1/71.4
    | 350M | 1.1M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cityscapes | mIoU: 74.26/75.90 | 1910M/4660M | 2.20M/3.85M |'
  prefs: []
  type: TYPE_TB
- en: '|  | ADE20K | mIoU: 33.22/34.92 | 1420M/2190M | 2.49M/3.86M |'
  prefs: []
  type: TYPE_TB
- en: '|  | KITTI | AP moderate/easy/hard: 78.49/87.62/75.53 | 15650M | 4.74M |'
  prefs: []
  type: TYPE_TB
- en: '|  | AP moderate/easy/hard: 69.74/83.09/74.89 | 3220M | 2.13M |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: Summary of Searching for Efficient Deep Learning Models (cont.).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Search strategy | Reference | Dataset | Hardware | Accuracy | Latency | FLOPs
    | # of Parameters |'
  prefs: []
  type: TYPE_TB
- en: '| Other+Soft | [[85](#bib.bib85)] | CIFAR10 |  | error: 4.62 | 9ms/82ms/149ms
    | 63.5M | 0.52M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Titan X/ | error: 4.36 | 13ms/62ms/912ms | 1364M | 11.39M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Maxwell 256/ | error: 4.78 | 6ms/75ms/210ms | 137M | 1.00M |'
  prefs: []
  type: TYPE_TB
- en: '|  | ARM Cortex53 | error: 4.93 | 7ms/44ms/381ms | 270M | 2.04M |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | error: 4.84 | 8ms/65ms/145ms | 59.27M | 0.45M |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | Maxwell 256/ | Top1/Top5 error: 24.16/7.13 | 218ms/5421ms |
    9276M | 77.16M |'
  prefs: []
  type: TYPE_TB
- en: '|  | ARM Cortex53 | Top1/Top5 error: 25.98/8.21 | 69ms/676ms | 523M | 4.8M
    |'
  prefs: []
  type: TYPE_TB
- en: '| ES+Hard | [[81](#bib.bib81)] | VOT-19 | A10 Fusion PowerVR GPU/ | EAO/acc/robustness:
    0.333/0.590/0.376 |  | 530M | 1.97M |'
  prefs: []
  type: TYPE_TB
- en: '|  | GOT-10K | Kirin 985 Mali-G77 GPU/ | AO/SR0.5: 0.611/0.710 | 52.6FPS/27.4FPS/
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | TrackingNet | Snapdragon 845 GPU/ | P/$\text{P}_{\text{norm}}$/AUC: 69.5/77.9/72.5
    | 38.4FPS/43.5FPS |'
  prefs: []
  type: TYPE_TB
- en: '|  | Snapdragon 845 DSP |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[91](#bib.bib91)] | SemanticKITTI | NVIDIA GTX1080Ti | mIoU: 60.3/63.7/66.4
    | 89ms/110ms/259ms | 8900M/15000M/73800M | 1.1M/2.6M/12.5M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[77](#bib.bib77)] | ImageNet | Google Pixel 1 | Top1 acc: 76.9/80.0 |
    58ms/- | 230M/595M | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[78](#bib.bib78)] | ImageNet | - | Top1/Top5 acc: 79.1/94.5 | - | 357M
    | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 80.5/95.1 |  | 557M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 81.3/95.5 |  | 762M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 82.8/96.3 |  | 2100M |'
  prefs: []
  type: TYPE_TB
- en: '|  | COCO | mAP: 30.5/33 |  | 2900M/5300M | 5.3M/10.6M |'
  prefs: []
  type: TYPE_TB
- en: '| RL+Hard | [[88](#bib.bib88)] | CIFAR10 | - | acc: $\sim$0.925/$\sim$0.85
    | - | $\sim$20M/$\sim$2M | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[65](#bib.bib65)] | CIFAR10 | - | error: 3.48/3.87/2.95/3.98/3.22 | -
    | - | 7.7M/3.4M/29M/2.2M/4.0M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Google speech | acc: 95.81/94.04/94.82/93.16/ |  | 3390M/1400M/6530M/890M/
    | 0.143M/0.047M/0.067M/0.425M/ |'
  prefs: []
  type: TYPE_TB
- en: '|  | commands | 95.02/95.64/95.18/93.65/93.07 |  | 3300M/13590M/210120M/12570M/1000M
    | 0.171M/0.733M/2.626M/0.074M/0.035M |'
  prefs: []
  type: TYPE_TB
- en: '| Other+Hard | [[84](#bib.bib84)] | PoseTrack2018 | - | AP: 83.2/78.2 | - |
    1440M/370M | 7.3M/2.5M |'
  prefs: []
  type: TYPE_TB
- en: '|  | COCO2017 | AP: 73.9 |  | 5640M | 16.3M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[75](#bib.bib75)] | ImageNet | - | Top1 acc: 76.5/78.9/79.5/80.9 | -
    | 242M/418M/586M/1040M | 4.5M/5.5M/6.4M/9.5M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[68](#bib.bib68)] | ImageNet | - | Top1/Top5 acc: 76.3/93.2 | - | 390M
    | 5.3M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 78.8/94.4 |  | 700M | 7.8M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 79.8/94.9 |  | 1000M | 9.2M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 81.1/95.5 |  | 1800M | 12M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 82.6/96.3 |  | 4200M | 19M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 83.3/96.7 |  | 9900M | 30M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 84.0/96.9 |  | 19000M | 43M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 84.4/97.1 |  | 37000M | 66M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[70](#bib.bib70)] | ImageNet | TPUv3/GPUv100 | Top1 acc: 77.3 | 8.71ms/22.5ms
    | 910M | 7.6M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1 acc: 79.4 | 13.6ms/34.4ms | 1580M | 10.4M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1 acc: 80.0 | 15.7ms/45.5ms | 1890M | 11.5M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1 acc: 81.4 | 31.9ms/66.6ms | 4300M | 16M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1 acc: 83.0 | 64.9ms/149.2ms | 10400M | 34M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1 acc: 83.7 | 125.9ms/290.2ms | 22200M | 60M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1 acc: 84.4 | 258.1ms/467.2ms | 52000M | 137M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1 acc: 84.7 | 396.1ms/847.7ms | 93000M | 199M |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[87](#bib.bib87)] | CIFAR100 | - | Top1/Top5 acc: 76.1/94.0 | - | 87.3M
    | 1.92M |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | Top1/Top5 acc: 72.2/91.0 |  | 294M | 3.4M |'
  prefs: []
  type: TYPE_TB
- en: '|  | Top1/Top5 acc: 74.7/92.0 |  | 471M | 4.7M |'
  prefs: []
  type: TYPE_TB
- en: IV Automated Compression of Deep Learning Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since deep learning models have proved remarkable performance in a wide spectrum
    of problems, it is a natural idea to compress these well-established models for
    memory saving and compute acceleration and thus hardware-efficiency [[127](#bib.bib127)].
    This idea is fervently supported by the fact that the hand-crafted deep learning
    models are often over-parameterized [[128](#bib.bib128), [7](#bib.bib7), [8](#bib.bib8)].
    The goal of deep learning compression is to modify well-trained models for efficient
    execution without significantly compromising accuracy. Although related works
    are quite divergent, they can be broadly summarized into four categories [[128](#bib.bib128),
    [127](#bib.bib127)]: tensor decomposition, knowledge distillation, pruning, and
    quantization. In the past few years, the above compression techniques have achieved
    great success while they crucially rely on domain knowledge, hand-crafted designs,
    and tremendous efforts for tuning. Recently, there is a growing demand and trend
    for automating the compression process on all the above four compression categories.
    In this section, we aim to provide a comprehensive review of recent research studies
    on automated compression of neural architectures with regard to the four compression
    categories.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Automated Tensor Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As the computation in neural networks is based on tensor operations, it is
    intuitive to compress tensors to squeeze and accelerate a neural network. The
    basic operation of tensor decomposition is the mode-$i$ product of a tensor with
    a matrix. For a $d$th-order tensor ${\mathbfcal{X}}\in\mathbb{R}^{n_{1}\times
    n_{2}\times...\times n_{d}}$ and a matrix $\textit{{B}}\in\mathbb{R}^{m\times
    n_{i}}(i\in\{1,2,...,d\})$, the mode-$i$ product between $\mathbfcal{X}$ and B
    is $\mathbfcal{R}=\mathbfcal{X}\times_{i}\textit{{B}}$, where $\mathbfcal{R}\in\mathbb{R}^{n_{1}\times...\times
    n_{i-1}\times m\times n_{i+1}...\times n_{d}}$ is also a $d$th-order tensor. Given
    this definition, a $d$th-tensor $\mathbfcal{A}\in\mathbb{R}^{n_{1}\times n_{2}\times...\times
    n_{d}}$ can be decomposed into one core $d$th-order tensor $\mathbfcal{G}\in\mathbb{R}^{r_{1}\times
    r_{2}\times...\times r_{d}}$ and $d$ factor matrices $\textbf{{U}}^{(i)}\in\mathbb{R}^{n_{i}\times
    r_{i}}(i\in{1,2,...,d})$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbfcal{A}\approx\mathbfcal{G}\times_{1}\textbf{{U}}^{(1)}\times_{2}\textbf{{U}}^{(2)}\times_{3}...\times_{d}\textbf{{U}}^{(d)}.$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'This decomposition is called Tucker decomposition [[129](#bib.bib129)], and
    the tuple $(r_{1},r_{2},...,r_{d})$ is called the Tucker rank. By selecting proper
    low ranks (i.e., $r_{i}<n_{i}$), the original tensor can be represented by lightweight
    decomposed pieces. The parameter compression ratio is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P=\frac{\prod_{i=1}^{d}n_{i}}{\sum_{i=1}^{d}r_{i}n_{i}+\prod_{i=1}^{d}r_{i}}.$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'Given that the parameters of a neural network are in the form of tensors (e.g.,
    the filter in a convolutional layer is a 4-way tensor), tensor decomposition can
    be naturally applied to model the parameters in a more efficient way. The speed-up
    ratio of a convolution layer is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\scalebox{1.0}{$S=\frac{n_{1}n_{2}n_{3}n_{4}H^{\prime}W^{\prime}}{n_{1}r_{1}H^{\prime}W+n_{2}r_{2}HW^{\prime}+n_{3}r_{3}HW+n_{4}r_{4}H^{\prime}W^{\prime}+r_{1}r_{2}r_{3}r_{4}H^{\prime}W^{\prime}}$},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $H\times W$ is the input feature map size, $H^{\prime}\times W^{\prime}$
    is the output feature map size, and $n_{1}\times n_{2}\times n_{3}\times n_{4}$
    is the kernel size. Tucker decomposition is a widely applied approach to this
    aim by tensorization of neural network layers [[130](#bib.bib130), [131](#bib.bib131)].
    Another commonly used tensorization approach is the Canonical Polyadic (CP) decomposition
    [[132](#bib.bib132)], which is a special case of the Tucker decomposition [[133](#bib.bib133),
    [134](#bib.bib134)]. After decomposition, it is usually required to fine-tune
    the tensorized network to recover the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: When tensorizing neural networks, the decomposition rank is the most important
    hyperparameter that needs to be carefully selected since it controls the compression-accuracy
    trade-off. A typical selection method is cross-validation, which is quite cumbersome
    for selecting a diverse range of ranks, so the common practice is to set the ranks
    of different layers to be the same. This simplification is coarse and sub-optimal.
    Therefore, automating the rank-selection process is crucial to determining optimal
    decomposition ranks. Kim et al. [[130](#bib.bib130)] propose to employ global
    analytic solutions for variational Bayesian matrix factorization (VBMF) [[135](#bib.bib135)]
    for automatic rank selection of Tucker decomposition. Publicly available tools
    can easily implement the VBMF. They perform full model compression including fully
    connected and convolutional layers and show that the accuracy degradation after
    compression can be well recovered by fine-tuning. However, Gusak et al. [[136](#bib.bib136)]
    claim that they find it difficult to restore the initial accuracy by fine-tuning
    with the global analytic VBMF ranks. Therefore, they use the global analytic solution
    of Empirical VBMF (EVBMF) to automatically select ranks. Instead of directly using
    the ranks achieved from EVBMF, the authors design a weakened rank for decomposition,
    which is larger than the extreme rank (i.e., obtained by EVBMF) and thus not optimal
    with a certain amount of redundancy after decomposition. The reason for this design
    is that this work proposes an iterative compression and fine-tuning strategy that
    gradually compresses the original model and restores the initial accuracy. Different
    from tensor decomposition on a well-trained network followed by fine-tuning in
    [[130](#bib.bib130), [136](#bib.bib136)], Hawkins et al. [[137](#bib.bib137),
    [138](#bib.bib138)] operate automatic rank determination and tensor decomposition
    along with the model training process. They propose a prior distribution mask,
    which can be optimized during training, over the decomposition core tensor to
    control the rank. If an element of the resultant posterior mask is smaller than
    a threshold, it will be regarded as zero and the rank will be automatically selected.
    The authors use a Bayesian inference method to train this low-rank tensorized
    model and prove its efficacy for various decomposition schemes. MARS [[139](#bib.bib139)]
    sets a factorized Bernoulli prior and optimizes the model via relaxed MAP (maximum
    a posteriori) estimation to achieve a binary mask to control the rank so the manually
    set threshold used in [[137](#bib.bib137), [138](#bib.bib138)] is avoided. It
    shows slightly worse in compression but better accuracy than [[138](#bib.bib138)].
    Inspired by the success of reinforcement learning in making decisions, Javaheripi
    et al. [[140](#bib.bib140), [141](#bib.bib141)] propose a state-action-reward
    system to automatically select the optimal rank for each layer. The hardware cost
    and accuracy are both considered in the reward and the action of decomposing a
    layer with the highest rewarded rank is selected. Bayesian optimization can also
    utilized to achieve the optimal ranks [[142](#bib.bib142)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE [III](#S4.T3 "TABLE III ‣ IV-A Automated Tensor Decomposition ‣ IV Automated
    Compression of Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey") summarizes the main results of
    automated tensor decomposition studies. Note that the latency is achieved with
    the hardware listed in the table, which is different from the hardware for training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Summary of Automated Tensor Decomposition Studies. The “hardware”
    column indicates on which the achieved models are evaluated. The model name (e.g.,
    AlexNet) in the “Accuracy” column indicates the original model before tensor decomposition.
    Some studies only report relative performance changes which are denoted as $n\times$
    in the table, indicating the original model is $n$ times its compressed counterpart.
    The latency is reported per input. ‘-’ indicates unavailable records.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters |'
  prefs: []
  type: TYPE_TB
- en: '| Automated Tensor Decomposition | [[130](#bib.bib130)] | ImageNet | Nvidia
    Titan X | AlexNet/VGG-S/GoogleNet/VGG-16 (Top5 acc): | 272M/549M/760M/3139M |
    0.30ms/0.92ms/1.48ms/4.58ms | 11M/14M/4.7M/127M |'
  prefs: []
  type: TYPE_TB
- en: '| Samsung Galaxy S6 | 78.33/84.05/88.66/89.40 | 43ms/97ms/192ms/576ms |'
  prefs: []
  type: TYPE_TB
- en: '| [[136](#bib.bib136)] | VOC2007 | - | Faster R-CNN (mAP): 69.2/68.3/77.0/75.0
    | 10.49$\times$/13.95$\times$/1.57$\times$/1.49$\times$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| COCO2014 | Faster R-CNN FPN (mAP/mAP.50): 35.4/56.2 | 1.8$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Faster R-CNN FPN (mAP/mAP.50): 36.2/57.1 | 1.7$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| ARD-LU[[137](#bib.bib137)] | MNIST | - | MLP (Acc): 98.06/98.30/96.28/98.24
    | - | - | 7k/101k/4k/6k |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB | DLRM (Acc): 87.61/87.79/85.33/88.93 | 6k/62k/23k/16k |'
  prefs: []
  type: TYPE_TB
- en: '| Criteo Ad Kaggle | BiLSTM (Acc): 78.61/78.64/78.67/78.72 | 564k/437k/154k/200k
    |'
  prefs: []
  type: TYPE_TB
- en: '| ARD-HU[[137](#bib.bib137)] | MNIST | - | MLP (Acc): 97.98/98.30/97.04/98.23
    | - | - | 7k/91k/4k/5k |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB | DLRM (Acc): 87.54/88.01/85.82/88.78 | 6k/58k/19k/14k |'
  prefs: []
  type: TYPE_TB
- en: '| Criteo Ad Kaggle | BiLSTM (Acc): 78.57/78.62/78.63/78.73 | 571k/402k/160k/164k
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[138](#bib.bib138)] | CIFAR-10 | - | ResNet-110 (Acc): 90.4 | - | - | 7.4$\times$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[139](#bib.bib139)] | MNIST | - | LeNet-5 (Acc): 99.0 | 1.19$\times$ | -
    | 10$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 | ResNet-110 (Acc): 90.7/91.1 | - | 7$\times$/5.5$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[140](#bib.bib140), [141](#bib.bib141)] | ImageNet | ARM-A57 | AlexNet (Top5
    acc): 81.01/80.37/79.98 | 543M/349M/277M | 4.39s/2.21s/1.61s | - |'
  prefs: []
  type: TYPE_TB
- en: '| VGG-16 (Top5 acc): 90.05/89.61/88.89 | 4950M/3170M/2360M | 72.15s/42.55s/30.48s
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[142](#bib.bib142)] | ImageNet | - | ResNet18(Top1/Top5 acc): 68.16/88.15
    | - | - | 3.15M |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet50(Top1/Top5 acc): 74.83/92.28 | 4.49M |'
  prefs: []
  type: TYPE_TB
- en: IV-B Automated Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Knowledge distillation (KD) is extended from knowledge transfer (KT) [[143](#bib.bib143)]
    by Ba and Caruana [[144](#bib.bib144)] to compress a cumbersome network (teacher)
    into a smaller and simpler network (student). This is done by making the student
    model mimic the function learned by the teacher model in order to achieve a competitive
    accuracy. It is later formally popularized by Hinton et al. [[21](#bib.bib21)]
    as a student-teacher paradigm, where the knowledge is transferred from the teacher
    to the student by minimizing the difference between the logits (features before
    the final softmax) of the teacher and student. In many situations, the performance
    of the teacher is almost perfect with a very high classification probability for
    the correct class and flat probabilities for the other classes. Therefore, the
    teacher is not able to provide much more information than the ground truth labels.
    Hinton et al. [[21](#bib.bib21)] introduce the concept of softmax temperature
    to transfer knowledge, which can better deliver the information of which classes
    the teacher find similar to the correct class. Formally, given the logits of the
    teacher model, the classification probability $p_{i}$ of the class $i$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{i}=\frac{exp(\frac{z_{i}}{\tau})}{\sum_{j}exp(\frac{z_{i}}{\tau})},$
    |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$ is the temperature parameter. It controls how soft the labels from
    the teacher are. The soft labels together with the ground truth labels are used
    to supervise a compact student model.
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla knowledge distillation mostly focuses on transferring knowledge to a
    student model with a fixed small architecture, which is manually designed in advance.
    However, different teachers and tasks favour different student architectures,
    and hand-crafted architectures are prone to be sub-optimal. Considering these
    limitations, there is a growing trend to automate the architecture design of a
    student model [[145](#bib.bib145), [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148),
    [149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151)]. The ground-truth
    labels are combined with the distillation labels to guide the automatic design
    process. AKDNet [[145](#bib.bib145)] proposes to search optimal student architectures
    for distilling a given teacher by RL-based NAS. It adopts the efficient search
    space of [[48](#bib.bib48)] and designs a KD-guided reward with a teacher network.
    KDAS-ReID [[147](#bib.bib147)], AdaRec [[150](#bib.bib150)], and NAS-KD [[151](#bib.bib151)]
    leverage the differential NAS [[54](#bib.bib54)] to determine the optimal student
    structures. In addition, Bayesian optimization [[149](#bib.bib149), [148](#bib.bib148)]
    and evolutionary search [[146](#bib.bib146)] are also popular search strategies
    in the context of student architecture search.
  prefs: []
  type: TYPE_NORMAL
- en: Although the soft labels from the final output of a teacher have been demonstrated
    effective for transferring knowledge, some works argue that it is also helpful
    to mimic the teacher from the intermediate layers. AdaRec [[150](#bib.bib150)]
    and NAS-KD [[151](#bib.bib151)] try to minimize the difference between the intermediate
    features of the student and the teacher. Similarly, Li et al. [[152](#bib.bib152)]
    and MFAGAN [[153](#bib.bib153)] compress the student generator in Generative Adversarial
    Networks (GANs) via intermediate feature distillation and once-for-all NAS [[77](#bib.bib77)].
    PPCD-GAN [[154](#bib.bib154)] also compresses the generator but with a teacher-guided
    learnable mask to automatically reduce the number of channels.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike searching for how to reduce, Kang et al. [[155](#bib.bib155)] and Mitsuno
    et al. [[156](#bib.bib156)] propose to search on how to increase. They set an
    extremely small student backbone at the start point, and augment operations [[155](#bib.bib155)]
    or additional layer channels [[156](#bib.bib156)] to the backbone during the search
    procedure. A large pre-trained teacher [[156](#bib.bib156)] or an ensemble teacher
    [[155](#bib.bib155)] is used to guide the search process. The motivation of these
    works is to alleviate the search burden with a start point and to maximize the
    distilled knowledge by optimally reducing the capacity gap between teacher and
    student.
  prefs: []
  type: TYPE_NORMAL
- en: A different direction is NAS-BERT [[157](#bib.bib157)], where the block-wise
    KD [[158](#bib.bib158)] is applied to train a supernet composing a bunch of candidate
    compact subnets. Then, the meta information (e.g., parameter, latency) of the
    candidate networks is summarized in a lookup table. Given certain hardware constraints,
    all candidate networks that satisfy the constraints are evaluated and the best
    performing one is selected as the final compressed model. The benefit of this
    strategy is that no retraining and researching are required for new hardware constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not only the student architecture is searchable, but also related hyperparameters
    (e.g., the temperature $\tau$ in Equation ([27](#S4.E27 "In IV-B Automated Knowledge
    Distillation ‣ IV Automated Compression of Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey"))) [[148](#bib.bib148)]
    can be included in the search process. AutoKD [[148](#bib.bib148)] uses the BayesOpt
    to simultaneously explore the optimal student structure and KD hyperparameters,
    temperature $\tau$ and loss weight $\alpha$, during the KD process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We outline the major results of automated knowledge distillation papers in
    TABLE [IV](#S4.T4 "TABLE IV ‣ IV-B Automated Knowledge Distillation ‣ IV Automated
    Compression of Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Summary of Automated Knowledge Distillation Studies. The “hardware”
    column indicates on which the achieved models are evaluated. The model name (e.g.,
    Inception-ResNet-V2) in the “Accuracy” column indicates the teacher model; if
    the teacher is not specified, the teacher is searched during training. The latency
    is reported per input; otherwise is specified in the relevant table cells (e.g.,
    s/batch). ‘-’ indicates unavailable records.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters | Memory |'
  prefs: []
  type: TYPE_TB
- en: '| Automated Knowledge Distillation | [[145](#bib.bib145)] | ImageNet | Pixel
    1 phone | Inception-ResNet-v2 (Top5 acc): 87.5/89.1/93.1 | - | 15ms/25ms/75ms
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Inception-ResNet-v2 (Top1 acc): 66.5/69.6/75.5 |'
  prefs: []
  type: TYPE_TB
- en: '| [[146](#bib.bib146)] | SST-2 | - | BERT (Acc): 84.1 | - | - | 24M | - |'
  prefs: []
  type: TYPE_TB
- en: '| Ag News | BERT (Acc): 90.3 | 16.9M |'
  prefs: []
  type: TYPE_TB
- en: '| [[147](#bib.bib147)] | Market-1501 | - | ResNet50 (mAP/Rank): 94.7/95.6 |
    - | - | 14.3M | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[148](#bib.bib148)] | CIFAR100 | - | Inception-Resnet-V2 (Acc): 81.2 | -
    | - | 4M | - |'
  prefs: []
  type: TYPE_TB
- en: '| MIT67 | DARTS (Acc): 76.0 | 6M |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | Inception-Resnet-V2 (Acc): 78.0 | 6M |'
  prefs: []
  type: TYPE_TB
- en: '| [[149](#bib.bib149)] | GLUE | TPUv4i | IB-$\text{BERT}_{\text{LARGE}}$ (AvgAcc):
    80.38/81.69 | - | 0.45ms/0.58ms | 20.6M/28.5M | - |'
  prefs: []
  type: TYPE_TB
- en: '| SQuAD | IB-$\text{BERT}_{\text{LARGE}}$ (F1): 88.4/88.1 | 0.59ms/0.49ms |
    22.8M/20.6M |'
  prefs: []
  type: TYPE_TB
- en: '| [[150](#bib.bib150)] | RetailRocket | - | NextItNet (MRR@5/HR@5/NDCG@5):
    0.7345/0.7964/0.7500 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 30Music | NextItNet (MRR@5/HR@5/NDCG@5): 0.6343/0.7151/0.6544 |'
  prefs: []
  type: TYPE_TB
- en: '| ML-2K | NextItNet (MRR@5/HR@5/NDCG@5): 0.4489/0.6519/0.4995 |'
  prefs: []
  type: TYPE_TB
- en: '| [[151](#bib.bib151)] | GLUE | - | $\text{BERT}_{\text{base}}$ (SST-2/MRPC/QQP/MNLI-m/MNLI-mm/QNLI/RTE):
    | - | - |  | - |'
  prefs: []
  type: TYPE_TB
- en: '| 92.2/86.3/70.4/81.0/80.2/88.6/65.9 | 42.4M |'
  prefs: []
  type: TYPE_TB
- en: '| 86.9/79.3/67.5/76.1/75.5/83.9/58.9 | 33.2M |'
  prefs: []
  type: TYPE_TB
- en: '| [[152](#bib.bib152)] | Cityscapes | - | Pix2Pix (mIoU): 41.71 | 5450M | -
    | 0.89M | - |'
  prefs: []
  type: TYPE_TB
- en: '| GauGAN (mIoU): 61.17 | 31200M | 20.2M |'
  prefs: []
  type: TYPE_TB
- en: '| COCO-Stuff |  | GauGAN (mIoU/FID): 35.34/25.06 | 35400M | 26M |'
  prefs: []
  type: TYPE_TB
- en: '| [[153](#bib.bib153)] | Set5 | NVIDIA V100 | MFANet (PSNR/LPIPS): 30.16/0.0571
    | 8410M | 21.9ms | 0.55M | 0.52G |'
  prefs: []
  type: TYPE_TB
- en: '| Set14 | MFANet (PSNR/LPIPS): 26.69/0.113 |'
  prefs: []
  type: TYPE_TB
- en: '| B100 | MFANet (PSNR/LPIPS): 25.33/0.1332 |'
  prefs: []
  type: TYPE_TB
- en: '| Urban100 | MFANet (PSNR/LPIPS): 24.23/0.1132 |'
  prefs: []
  type: TYPE_TB
- en: '| [[154](#bib.bib154)] | ImageNet | AMD Ryzen 9 3900X | BigGAN (IS/FID/LPIPS):
    83.13/12.76/0.62 | 1600M | 2.05s/batch | 13.6M | - |'
  prefs: []
  type: TYPE_TB
- en: '| NVIDIA GTX3090 | 1.19s/batch |'
  prefs: []
  type: TYPE_TB
- en: '| [[157](#bib.bib157)] | GLUE | - | $\text{BERT}_{\text{base}}$ (MNLI/QQP/QNLI/CoLA/SST-2/STS-B/RTE/MRPC)
    | - | - | 60M | - |'
  prefs: []
  type: TYPE_TB
- en: '| 84.1/88.8/91.2/50.5/92.6/86.9/72.7/86.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SQuADv1.1 | $\text{BERT}_{\text{base}}$(EM/F1): 81.2/88.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SQuADv2.0 | $\text{BERT}_{\text{base}}$(EM/F1): 73.9/77.1 |'
  prefs: []
  type: TYPE_TB
- en: IV-C Automated Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural network pruning, a.k.a. sparsification, attempts to reduce memory storage
    and computation cost by removing unimportant weights or neurons from the base
    network. According to the granularity, pruning can be roughly categorized into
    structured pruning and unstructured pruning. Unstructured pruning enforces weights
    or layers to be sparse by removing individual connections or neurons. Structured
    pruning, on the other hand, targets discarding the entire channels or layers.
    Although unstructured pruning can theoretically achieve a better accuracy-compression
    tradeoff, it is less compatible with existing deep learning platforms and hardware
    than structured pruning. Thus, structured pruning attracts more research focus.
  prefs: []
  type: TYPE_NORMAL
- en: A typical pruning procedure is to first identify and cut away unimportant weighs
    or layers from an existing over-parameterized network and then fine-tune the pruned
    network to restore the accuracy. The first step is the key and largely relies
    on expert knowledge and hand-crafted heuristics, such as manually setting the
    criterion of parameter importance [[159](#bib.bib159)] and the pruning rate [[160](#bib.bib160)].
    However, heuristic settings are typically based on trial and error and are sub-optimal
    so researchers actively seek automated solutions to alleviate human intervention
    during pruning. These efforts can roughly be categorized into three directions.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Reducing the number of hyperparameters (i.e., per-layer pruning rates) needed
    to be tuned [[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163)]. Zeng
    et al. [[161](#bib.bib161)] compute the importance of model parameters using the
    Taylor expansion of the loss function and prune unimportant ones. They introduce
    an auxiliary hyperparameter that controls the model shrinking proportion at each
    pruning iteration and show that this hyperparameter is insensitive to the final
    performance. In this way, they avoid carefully tuning the overall pruning rate.
    Li et al. [[162](#bib.bib162)] use the regularization approach, Alternating Direction
    Method Multipliers (ADMM), as the core pruning algorithm but substitute its hard
    constraint with the soft constraint-based formulation and solve the optimization
    problem with the Primal-Proximal solution. Their approach naturally does not require
    predefining the per-layer pruning rates and thus reduces the number of hyperparameters.
    Zheng et al. [[163](#bib.bib163)] introduce the normalized Hilbert-Schmidt Independence
    Criterion (nHSIC) from the information theory to measure the per-layer importance
    and derive the compression problem with constraints into a linear programming
    problem, which is convex and easily solved. In such a manner, only two hyperparameters
    are required and demonstrated robust to different values.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Searching for optimal pruning rates or magnitude thresholds for pruning [[26](#bib.bib26),
    [164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167),
    [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171),
    [172](#bib.bib172), [173](#bib.bib173)]. Note that the magnitude thresholds of
    weights can be naturally derived from pruning rates. AMC [[26](#bib.bib26)] and
    Auto-Prune [[165](#bib.bib165)] leverage deep reinforcement learning (DRL) to
    automatically determine the pruning rate of each layer and empirically prune the
    weights with the least magnitude. Arguing that DRL has an inherited incompatibility
    with the pruning problem, Liu et al. [[164](#bib.bib164)] suggest using the heuristic
    search technique, simulated annealing (SA), to search for the per-layer pruning
    rates and then use the ADMM to dynamically regularize network weights for structural
    pruning. They hypothesize that the layers with more weights can have higher compression
    rates. The authors also apply the ADMM to searching for the magnitude thresholds
    for the second-phase unstructured pruning after the first-phase structural pruning.
    Tung et al. [[167](#bib.bib167)] propose to employ a Bayesian optimization framework
    to search the pruning hyperparameters including the magnitude threshold. Some
    studies [[168](#bib.bib168), [169](#bib.bib169)] follow this work to make it constraint-aware
    [[168](#bib.bib168)] and applicable to pruning deeper networks [[169](#bib.bib169)].
    Other than pruning full-precision neural networks, Guerra and Drummond [[171](#bib.bib171)]
    aim at automatically pruning quantized neural networks by a customized, rule-based
    pruning strategy with Bayesian optimization of layer-wise pruning ratios.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Learning the weight/channel importance [[174](#bib.bib174), [24](#bib.bib24),
    [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)]. Different from relying
    on heuristic weight/channel importance measurement like the weight magnitude,
    this direction struggles to learn the importance measurement through optimizing
    the final objective. Liu et al. [[175](#bib.bib175)] propose a slimming scheme
    that regularizes the scaling factors in batch normalization (BN) layers as a channel
    selection indicator to identify unimportant channels (or neurons). Then, they
    apply a threshold to the trained scaling factors for channel pruning. AutoPrune
    [[24](#bib.bib24)] and DAIS [[174](#bib.bib174)] are following works that rely
    on auxiliary learnable channel selection indicator for pruning. AutoPrune argues
    that decoupling the channel selection indicators and network parameters will help
    to stabilize the weight learning and make the pruning insensitive to hyperparameters.
    DHP [[176](#bib.bib176)] introduces an additional hypernetwork that can generate
    weights for the backbone network (i.e., the network to be pruned). The input of
    the hypernetwork is the latent vectors, which are attached to each layer of the
    backbone network. Binary channel masks will then be achieved by setting those
    latent vector elements, which are smaller than a threshold, to be zero otherwise
    to be one. In contrast to learning channel importance, ASBP [[177](#bib.bib177)]
    targets to automatically decide the binary importance of each bit of weight. This
    bit-level pruning has the finest granularity but is impractical to achieve a real
    reduction of resource consumption due to poor hardware support. To solve this
    problem, the authors target a specific sort of hardware, RRAM, which assembles
    multiple low-precision cells together as a crossbar to represent a high-precision
    data value. They then prune at the granularity of crossbar size, that is all cells
    in a crossbar share the same pruning strategy. The DL algorithm, deep deterministic
    policy gradient (DDPG), is used to search for the optimal set of bit-pruning strategies.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C1 Unifying knowledge distillation and pruning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since pruning and knowledge distillation both require a pre-trained model (i.e.,
    the base model in pruning; teacher in KD) to guide the compression process, researchers
    naturally explore ways to unify these two techniques. One route is the two-step
    unifying scheme [[178](#bib.bib178), [179](#bib.bib179)]. TAS [[178](#bib.bib178)]
    is an early and representative work. It first searches for the shrunken width
    and depth of the pruned network with the help of learnable probability from the
    base model. The searched architecture is then trained from scratch as the student
    of a simple KD approach with the base model serving as the teacher. The other
    route is to unify pruning and KD in a single step [[180](#bib.bib180), [181](#bib.bib181)].
    Gu and Tresp [[181](#bib.bib181)] introduce learnable channel gates into the base
    model and train the network weights and channel gates with a distillation-aware
    loss function. Thus, this scheme prunes the base model and distils its knowledge
    simultaneously and automatically. Yao et al. additionally search for both teacher
    and student to pursue the optimal distillation pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Summary of Automated Pruning Studies. The “hardware” column indicates
    on which the achieved models are evaluated. The model name (e.g., LeNet-300-100)
    in the “Accuracy” column indicates the original model before pruning; if the original
    model is not specified, it is searched during training. The latency is reported
    per input; otherwise is specified in the relevant table cells (e.g., FPS (frame
    per second)). $n\%$ indicates the compressed model is $n$ per cent of its original
    model; $\downarrow n\%$ indicates the compressed model decreases $n$ per cent
    compared to its original model. $n\times$ indicates that the original model is
    $n$ times its compressed counterpart. ‘-’ indicates unavailable records.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters | Memory |'
  prefs: []
  type: TYPE_TB
- en: '| Automated Pruning | [[161](#bib.bib161)] | MNIST | - | LeNet-300-100/LeNet-5(Top1
    error): $\uparrow$0.08/$\uparrow$0.04 | - | - | 77$\times$/200$\times$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 |  | CifarNet(Top1 error):$\uparrow$ 0.17 | 15.6$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet |  | VGG16(Top1/Top5 error): $\uparrow$2.3/$\uparrow$1.2 | 5.4$\times$
    | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ResNet50(Top1/Top5 error): $\uparrow$2.0/$\uparrow$1.1 | 2.3$\times$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[162](#bib.bib162)] | CIFAR-10 | Qualcomm Kryo 485 Octacore | VGG16/ResNet-18/MobileNetV2(acc):
    93.5/94.2/94.6 | - | 2.52ms/3.39ms/2.98ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Qualcomm Adreno 640 | 2.32ms/3.28ms/2.97ms |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 | Qualcomm Kryo 485 Octacore | VGG16/ResNet-18/MobileNetV2(acc):
    72.2/75.3/78.7 | 2.98ms/3.91ms/3.63ms |'
  prefs: []
  type: TYPE_TB
- en: '|  | Qualcomm Adreno 640 | 3.07ms/3.70ms/3.50ms |'
  prefs: []
  type: TYPE_TB
- en: '| [[163](#bib.bib163)] | CIFAR-10 | - | VGG/ResNet-20/ResNet56(Top1 acc): 94.00/92.01/94.05
    | 98.8M/20.8M/59.5M | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | Google Pixel 2 | MobleNetV1/MobileNetV2(Top1 acc): 68.06/69.13
    | 149M/149M | 10ms/13ms |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobleNetV1/MobileNetV2(Top1 acc): 70.92/71.54 | 283M/219M | 15ms/17ms
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[26](#bib.bib26)] | CIFAR-10 | - | Plain-20/ResNet56(acc): 90.2/91.9 | 50%/50%
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet |  | VGG16/MobileNetV1/MobileNetV2(acc): $\downarrow$1.4/$\downarrow$1.7/$\downarrow$1.0
    | 20%/40%/50% |'
  prefs: []
  type: TYPE_TB
- en: '|  | Google Pixel 1 | MobileNetV1(Top1/Top5 acc): 70.2/89.2 | 272M | 16.0FPS
    | 13.2MB |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobileNetV1(Top1/Top5 acc): 70.5/89.3 | 285M | 14.6FPS | 14.3MB |'
  prefs: []
  type: TYPE_TB
- en: '| [[164](#bib.bib164)] | CIFAR-10 | Qualcomm Adreno 640 | VGG16/ReNet-18(Acc):
    93.21/93.81 | 8.8 $\times$/12.2 $\times$ | 2.7ms/1.45ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | - | VGG16/ReNet-18(Top5 acc): $\downarrow$0.6/$\downarrow$0.1
    | - | - | 6.4$\times$/3.3$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[165](#bib.bib165)] | CIFAR-10 | ReRAM simulator | AlexNet/VGG16/Plain20(Acc5):
    99.10/98.62/98.29 | - | - | - | 14.3 $\times$/11.9$\times$/10.3$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST |  | AlexNet/VGG16/Plain20(Acc1): 98.49/98.63/98.00 | 21.4$\times$/19.3$\times$/6.2$\times$
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[166](#bib.bib166)] | CIFAR-10 | - | ResNet20/ResNet32/ResNet56(Top1 acc):
    92.06/92.60/93.66 | $\downarrow$48.35%/$\downarrow$45.05%/$\downarrow$52.00% |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 |  | ResNet20/ResNet32/ResNet56(Top1 acc): 66.93/68.98/70.50 |
    $\downarrow$46.35%/$\downarrow$55.17%/$\downarrow$55.29% |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet |  | ResNet18/ResNet34/ResNet50/MobileNetV2(Top1 acc): | $\downarrow$46.70%/$\downarrow$48.72%/
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 69.65/73.56/76.84/71.60 | $\downarrow$50.60%/$\downarrow$51.20% |'
  prefs: []
  type: TYPE_TB
- en: '| [[167](#bib.bib167)] | UCMerced Land Use | - | AlexNet(Acc): 94.1 | - | -
    | 1.17M | - |'
  prefs: []
  type: TYPE_TB
- en: '| Describable Textures |  | AlexNet(Acc): 52.8 | 2.41M |'
  prefs: []
  type: TYPE_TB
- en: '| [[168](#bib.bib168)] | ImageNet | Intel Core(TM) i7-4790 | CaffeNet(Top1
    acc): 53.70 | - | 69.7ms/batch | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Describable Textures | Intel Core(TM) i7-7700 | CaffeNet(Top1 acc): 59.00
    | 58.2ms/batch |'
  prefs: []
  type: TYPE_TB
- en: '| [[169](#bib.bib169)] | ImageNet | - | MobileNetV1(Top1/Top5 acc): 49.34/74.52
    | 50% | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobileNetV2(Top1/Top5 acc): 52.86/78.57 | 60% |'
  prefs: []
  type: TYPE_TB
- en: '| [[170](#bib.bib170)] | UCSD | NVIDIA MX250 | YOLOv3(mAP): 69.6 | 4458M |
    16ms | 4.685M | - |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile Robot Detection |  | YOLOv3(mAP): 92.1 | 327M | 3ms | 0.299M |'
  prefs: []
  type: TYPE_TB
- en: '| Sim2real Detection (sim) |  | YOLOv3(mAP): 98.0 | 1581M | 8ms | 2.545M |'
  prefs: []
  type: TYPE_TB
- en: '| Sim2real Detection (realr) |  | YOLOv3(mAP): 76.1 |'
  prefs: []
  type: TYPE_TB
- en: '| [[171](#bib.bib171)] | CIFAR-10 | - | VGG11/ResNet-14(Top1 acc): 86.30/89.84
    | - | 1.4$\times$/1.2$\times$ | - | 0.94MB/0.73MB |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet |  | ResNet-18(Top1 acc): 59.30 | 1.4$\times$ | 4.36MB |'
  prefs: []
  type: TYPE_TB
- en: '| [[172](#bib.bib172)] | ImageNet | Google Pixel 1 | MobileNetV1(Top1 acc):
    46.3/69.1 | - | 6.01ms/74.9ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobileNetV2(Top1/Top5 acc): 70.9/70.0 | 61.6ms/53.5ms |'
  prefs: []
  type: TYPE_TB
- en: '| [[173](#bib.bib173)] | ImageNet | Google Pixel 1 | MobileNetV3(Top1 acc):
    77.0/78.5 | 225M/314M | 51ms/- | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Summary of Automated Pruning Studies (cont.).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters | Memory |'
  prefs: []
  type: TYPE_TB
- en: '| Automated Pruning | [[174](#bib.bib174)] | ImageNet | Galaxy S9 | ResNet18/ResNet34/ResNet50(Top1
    acc): 67.56/72.77/74.45 | 1030M/2130M/1830M | 190ms/310ms/310ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ResNet18/ResNet34/ResNet50(Top5 acc): 87.90/90.99/92.21 |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 | - | ResNet32/ResNet56/ResNet110(acc): 72.20/72.57/74.69 | 39.4M/58.4M/114M
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 |  | ResNet20/ResNet32/ResNet56/ResNet110/MobileNetV1(acc): | 19.1M/31.9M/36.4M/
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 91.87/93.49/93.53/95.02/91.87 | 101M/115M |'
  prefs: []
  type: TYPE_TB
- en: '| [[175](#bib.bib175)] | CIFAR-10 | - | VGGNet/DenseNet40/ResNet164(error):
    6.20/5.65/5.27 | 391M/240M/275M | - | 2.30M/0.35M/1.10M | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 |  | VGGNet/DenseNet40/ResNet164(error): 26.52/25.72/23.91 | 501M/281M/247M
    | 5.00M/0.46M/1.21M |'
  prefs: []
  type: TYPE_TB
- en: '| SVHN |  | VGGNet/DenseNet40/ResNet164(error): 2.06/1.81/1.81 | 398M/267M/225M
    | 3.04M/0.44M/1.12M |'
  prefs: []
  type: TYPE_TB
- en: '| [[24](#bib.bib24)] | ImageNet | - | MobileNetV2(Top1 acc): 66.83/73.32/74.0
    | 102M/209M/305M | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[176](#bib.bib176)] | CIFAR-10 | - | ResNet20/ResNet56/ResNet110/ | 51.80%/39.07%/21.63%/
    | - | 56.13%/41.10%/22.40%/ | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ResNet164/DenseNet-12-40(Top1 error): | 21.78%/29.52% | 20.46%/26.01%
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 8.46/7.06/6.61/6.30/6.51 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Tiny-ImageNet |  | MobileNetV1/MobileNetV2(Top1 error): 51.63/52.43 | 51.91%/11.92%
    | 36.95%/6.50% |'
  prefs: []
  type: TYPE_TB
- en: '| [[178](#bib.bib178)] | CIFAR-10 | - | ResNet20/ResNet32/ResNet56/ResNet110/ResNet164(acc):
    | 22.4M/35.0M/59.5M/ | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 92.88/93.16/93.69/94.33/94.00 | 119M/178M |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 |  | ResNet20/ResNet32/ResNet56/ResNet110/ResNet164(acc): | 22.4M/42.5M/61.2M/
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 68.90/72.41/72.25/73.16/77.76 | 120M/171M |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet |  | ResNet18/ResNet50(Top1 acc): 69.15/76.20 | 1210M/2310M |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ResNet18/ResNet50(Top5 acc): 89.19/93.07 |'
  prefs: []
  type: TYPE_TB
- en: '| [[179](#bib.bib179)] | CIFAR-10 | - | ResNet164(test error): 4.58/5.01 |
    190M/133M | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFRA-100 |  | ResNet164(test error): 21.63/22.38 | 178M/123M |'
  prefs: []
  type: TYPE_TB
- en: '| [[180](#bib.bib180)] | MS COCO | NVIDIA V100 | (AP/AP.5/AP.7/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$):
    | - | 25.4 FPS | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 42.3/62.6/46.2/26.2/45.1/50.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 43.9/63.8/47.9/27.0/46.8/52.8 | 23.3 FPS |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 50.7/69.6/55.4/31.3/53.8/64.0 | 10.1 FPS |'
  prefs: []
  type: TYPE_TB
- en: IV-D Automated Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike the above three compression techniques, which struggle to optimize network
    architectures, quantization appeals to reduce the representation precision of
    network weights and intermediate activation tensors. Neural networks are generally
    trained using the 32-bit floating-point precision; if we were to perform network
    inference in the 32-bit floating-point as well, MAC operations, data transfer,
    and data saving would have to be done all in 32-bit floating-point. Hence, using
    lower bit precision would substantially reduce the hardware overhead, including
    communication bandwidth, computation and memory usage [[182](#bib.bib182)]. For
    example, when moving from 32 to 8 bits, the memory cost and computation would
    decrease by a factor of 4 and 16 respectively. In addition, fixed-point computation
    is more efficient than its floating-point counterpart [[182](#bib.bib182)]. The
    most commonly explored quantization scheme, uniform quantization, converts a floating
    point tensor $\boldsymbol{x}=\{x_{1},...,x_{N}\}$ with range $(x_{min},x_{max})$
    into its integer coding $\boldsymbol{x}_{q}$ with range $[n,p]=[0,2^{b}-1]$ via
    the following definition [[183](#bib.bib183)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{x}_{q}=\lfloor\text{clamp}(\frac{\boldsymbol{x}}{\Delta}+z;n,p)\rceil,$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\Delta=\frac{x_{max}-x_{min}}{p-n},\>z=\text{clamp}(-\lfloor\frac{x_{min}}{\Delta}\rceil+z;n,p),$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\boldsymbol{\hat{x}}=(\boldsymbol{x}_{q}-z)\Delta,$ |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\Delta$ is the scale factor that specifies the step size of the quantizer;
    $z$ is the zero-point that represents the real value zero without any precision
    loss; $b$ is the quantization bitwidth; $\lfloor\cdot\rceil$ is the round-to-nearest
    operation. The $clamp(\cdot)$ function is to truncate all values to fall between
    $n$ and $p$. Through this procedure, two sorts of errors are induced: a clipping
    error induced by the clamp function and a rounding error induced by the round-to-nearest
    operation $\lfloor\cdot\rceil$. The quantized tensor $\boldsymbol{\hat{x}}$ is
    then used for efficient but low precision computation. Since there are no theoretical
    correlations between the model accuracy and $b$, simple try-and-error is only
    feasible in the scenario where all weights and activation tensors have the same
    precision. However, different layers have different redundancy and behaviours
    on both the hardware and the task. Therefore, automatically determining the bitwidth
    of weights and activations for each layer is stunningly attractive, and this mixed-precision
    feature has recently been supported by hardware manufacturers [[184](#bib.bib184),
    [185](#bib.bib185)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'One direction is to model the layer-wise bit-precision assignment as a reinforcement
    learning problem, where the action is to determine the bitwidth values [[25](#bib.bib25),
    [186](#bib.bib186)]. HAQ [[25](#bib.bib25), [186](#bib.bib186)] is such a representative
    work. It utilizes the DDPG agent to explore a continuous action space of [0, 1],
    from which selected actions (i.e., real numbers) can be rounded into discrete
    bitwidth values for the weights and activations of each layer. The use of a continuous
    action space is based on the consideration that a discrete action space is not
    able to represent the relative order of bitwidth. The quantized model is then
    retrained one more epoch to recover the accuracy. Since the authors induce the
    hardware constraints by limiting the search space, they only consider minimizing
    the accuracy drop after quantization in the reward function. Another direction
    is to develop a differentiable framework that searches for the quantization bitwidth
    in the same way as searching for the network architecture [[187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191),
    [192](#bib.bib192)]. Given a network to be quantized, a supernet is first constructed
    by inserting multiple parallel quantization operations into every pair of adjacent
    layers of the network. Each inserted operation constitutes an edge from the $i$-th
    layer to the ($i+1$)-th layer and is associated with a learnable parameter. Multiple
    parallel edges/operations on the same level are of different quantization bitwidths
    and summarized by assembling all edges with the weights derived from the learnable
    parameters. Then, the network parameters and the quantization weights are jointly
    trained with the combination of accuracy and model size as the target. Wu et al.
    [[192](#bib.bib192)] and Xu et al. [[189](#bib.bib189)] simply try to diminish
    the model size while Yu et al. [[188](#bib.bib188)] and Wei et al. [[191](#bib.bib191)]
    consider hardware constraints in the training target. SSPS [[190](#bib.bib190)]
    further borrows the idea from [[23](#bib.bib23)] to use a one-hot mask instead
    of continuous weights to select the bitwidth. In this way, only one path is active
    during training so the search cost is considerably reduced. The bitwidth search
    space is usually below eight since a previous study has shown that simple 8-bit
    post-training quantization is able to achieve marginal accuracy degradation [[193](#bib.bib193)].
    We summarize the main studies of automated quantization in TABLE [VI](#S4.T6 "TABLE
    VI ‣ IV-D Automated Quantization ‣ IV Automated Compression of Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Summary of Automated Quantization Studies. The “hardware” column
    indicates on which the achieved models are evaluated. The model name (e.g., MobileNetV1)
    in the “Accuracy” column indicates the original model before quantization. The
    latency is reported per input. $n\times$ indicates that the original model is
    $n$ times its quantized counterpart. ‘-’ indicates unavailable records.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Reference | Dataset | Hardware | Accuracy | Bitwdith | Latency |
    Model Size |'
  prefs: []
  type: TYPE_TB
- en: '| Automated Quantization | [[25](#bib.bib25), [186](#bib.bib186)] | ImageNet
    |  | MobileNetV1 (Top1 acc): 67.40/70.58/71.20 | - | 45.51ms/57.70ms/70.35ms |
    - |'
  prefs: []
  type: TYPE_TB
- en: '|  | BISMO on | MobileNetV1 (Top5 acc): 87.90/89.77/90.19 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Xilinx Zynq-7020 | MobileNetV2 (Top1 acc): 66.99/70.90/71.89 | 52.12ms/66.92ms/82.34ms
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobileNetV2 (Top5 acc): 87.33/89.91/80.36 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobileNetV1 (Top1 acc): 65.33/69.97/71.20 | 57.40ms/77.49ms/99.66ms
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | BISMO on | MobileNetV1 (Top5 acc): 86.60/89.37/90.08 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Xilinx VU9P | MobileNetV2 (Top1 acc): 67.01/69.45/71.85 | 73.97ms/99.07ms/127.03ms
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobileNetV2 (Top5 acc): 87.46/88.94/90.24 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BitFusion | MobileNetV1 (Top1 acc): 67.45/70.40/70.90 | 7.86ms/11.09ms/19.98ms
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | MobileNetV1 (Top5 acc): 87.85/89.69/89.95 |'
  prefs: []
  type: TYPE_TB
- en: '|  | - | MobileNetV1 (Top1 acc): 57.14/67.66/71.74 | - | 1.09MB/1.58MB/2.07MB
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | MobileNetV1 (Top5 acc): 81.87/88.21/90.36 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MobileNetV2 (Top1 acc): 66.75/70.90/71.47 | 0.95MB/1.38MB/1.79MB |'
  prefs: []
  type: TYPE_TB
- en: '|  | MobileNetV2 (Top5 acc): 87.32/89.76/90.23 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ResNet50 (Top1 acc): 70.63/75.30/76.14 | 6.30MB/9.22MB/12.14MB |'
  prefs: []
  type: TYPE_TB
- en: '|  | ResNet50 (Top5 acc): 89.93/92.45/92.89 |'
  prefs: []
  type: TYPE_TB
- en: '| [[187](#bib.bib187)] | MNIST | - | LeNet5 (Val. error): 1.14/1.69/2.32 |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | AlexNet (Val. error): 47.46/47.69/48.54 |'
  prefs: []
  type: TYPE_TB
- en: '| [[188](#bib.bib188)] | CIFAR-10 | - | ResNet20 (Top1 acc): 92.30/92.12/92.04
    | Avg: 3.5/3.3/2.9 | - | 10.19$\times$/10.74$\times$/12.08$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | ResNet50 (Top1 acc): 76.67/75.71 | Avg: 3.8/2.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | ResNet50 (Top5 acc): 93.55/92.83 |'
  prefs: []
  type: TYPE_TB
- en: '| [[189](#bib.bib189)] | Penn Treebank Corpus | - | Transformer (Perplexity):
    56.82/58.23 | Avg: 2.0/2.2 | - | 6.5MB/4.8MB |'
  prefs: []
  type: TYPE_TB
- en: '| Conversational Telephone Speech | Transformer (Perplexity): 42.39/42.75 |
    Avg: 1.9/2.5 | 8.0MB/9.1MB |'
  prefs: []
  type: TYPE_TB
- en: '| [[190](#bib.bib190)] | CIFAR-10 | - | ResNet20 (Top1 acc): 92.54 | Avg: 3.04
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | ResNet18/ResNet34/ResNet50/MobileNetV2 (Top1 acc): | Avg: |'
  prefs: []
  type: TYPE_TB
- en: '|  | 70.70/74.30/76.22/69.10 | 3.95/4.01/3.98/4.02 |'
  prefs: []
  type: TYPE_TB
- en: '| COCO | Faster R-CNN (AP/AP.5/AP.75/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$)
    | Avg: 4.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 37.4/58.1/40.6/22.1/40.4/47.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RetinaNet (AP/AP.5/AP.75/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$)
    | Avg: 4.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 36.4/55.8/38.6/20.8/39.9/47.6 |'
  prefs: []
  type: TYPE_TB
- en: '| [[191](#bib.bib191)] | NWPU-RESISC45 | - | ResNet34 (acc): 92.66/92.75/92.73
    | - | - | 13.17MB/16.50MB/18.86MB |'
  prefs: []
  type: TYPE_TB
- en: '|  | SqueezeNet (acc): 88.45/88.59/88.61 | 0.45MB/0.55MB/0.69MB |'
  prefs: []
  type: TYPE_TB
- en: '| [[192](#bib.bib192)] | CIFAR-10 | - | ResNet20/ResNet56/ResNet110 (acc):
    92.00/94.12/94.39 | - | - | 16.6$\times$/18.93$\times$/20.3$\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | ResNet18/ResNet34 (acc): 69.58/73.37 | 21.1$\times$/19.0$\times$
    |'
  prefs: []
  type: TYPE_TB
- en: V Joint Automated Design Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural architecture design and compression are divergent routines to achieve
    efficient deep learning models and a combination of these two strategies is an
    intuitive way to achieve even more efficiency than merely relying on either technique
    individually. The most natural way in this regard is a sequential pipeline: design
    then compress. Under the context of design automation, the pipeline is commonly
    recognized as a search-compress scheme. There are bare studies on combining tensor
    decomposition and NAS. We will thus concentrate on the other three compression
    techniques in this section. TABLE [VII](#S5.T7 "TABLE VII ‣ V-C2 Optimizing search
    and quantization jointly ‣ V-C Joint Search and Quantization ‣ V Joint Automated
    Design Strategies ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey") summarizes main references on joint automated design
    work.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A Joint Search and Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For knowledge distillation, the intuitive joint automated design strategy is
    to first search for a powerful teacher model and then achieve a compressed student
    model by automated knowledge distillation. An improved strategy is FasterSeg [[90](#bib.bib90)],
    which searches for student and teacher alternatively so as to ensure optimal teacher-student
    matching pairs and effective knowledge distillation. Instead of searching for
    the entire teacher model, Guan et al. [[194](#bib.bib194)] target the feature
    distillation and search for the optimal teacher’s feature aggregation for distillation.
    For each layer group in the teacher model, the authors try to search for the aggregation
    weights of each layer’s feature in that group; then the features are weighted
    sum to guide the knowledge distillation of the corresponding student layer. This
    work mimics the multi-teacher distillation scheme but with a single-teacher, multi-feature
    distillation proposal and automated teacher knowledge aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Joint Search and Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Different from the search-compress scheme, pruning can be incorporated into
    neural architecture search as integrity. The aforementioned automated pruning
    approaches (in Section [IV-C](#S4.SS3 "IV-C Automated Pruning ‣ IV Automated Compression
    of Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey")) decide the pruning strategy based on the weights
    of trained networks. However, a recent study [[195](#bib.bib195)] indicates that
    the essence of network pruning is the pruned structure instead of the weights
    inherited from the base network, which means that the pruning strategy can be
    determined before the model is trained. In light of this critical conclusion,
    the research direction turns to finding optimal pruned structures, e.g., the channel
    number in each layer, rather than to determining important channels from the base
    network. This objective is basically the same as NAS so numerous works emerge
    to use NAS to achieve pruning, where the pruning strategy and network structure
    are sampled together before training in the search process [[196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201)]. However, different from pure NAS, the NAS-based pruning 1)
    requires an existing state-of-the-art model as the pruning base, 2) only searches
    the network hyperparameters (e.g., channel number, layer number) instead of the
    type of operators, and 3) is bounded by searching smaller hyperparameters than
    the base network. MetaPruning [[196](#bib.bib196)] is an early work in this direction,
    which attempts to find the optimal number of channels for each layer. It first
    constructs a PruningNet to generate weights for pruned networks so that fine-tuning
    is not required during search time; then it utilizes an evolutionary algorithm
    to search for the optimal combination of layers with different numbers of channels.
    The search procedure is highly efficient and no fine-tuning is obliged after searching.
    However, the search space is large especially when there are many channels in
    the base network. Some following works thus attempt to shrink the search space
    by only searching channel number ratios [[198](#bib.bib198)] or using a clustering
    algorithm to cluster channels in each layer as the initial number of channels
    in the pruned network for later search [[199](#bib.bib199)]. Other works [[200](#bib.bib200),
    [201](#bib.bib201)], on the other hand, seek to develop a performance estimator
    to accelerate the searching process. Overall, most NAS-based pruning process is
    analogous to searching optimal pruning rates but without pruning low magnitude
    channels at each iteration, instead, it trains from scratch with the searched
    structure or quickly derives the performance with an additional performance estimator.'
  prefs: []
  type: TYPE_NORMAL
- en: Alternative to unifying NAS and pruning by sampling the pruning strategy and
    model architecture before training, SpArSe [[202](#bib.bib202)] selects the pruning
    strategy based on a trained model but optimizes the pruning hyperparameters and
    model architecture jointly. Specifically, it first samples a configuration of
    the model architecture (e.g., operators and connectivity) and hyperparameters
    of pruning algorithms (i.e., Sparse Variational Dropout (SpVD) and Bayesian Compression
    (BC)); it then trains the model and prunes it. After evaluation, it optimizes
    the sampling configuration with multi-objective Bayesian optimization. In this
    way, both structured and unstructured pruning strategies can be optimized together
    with the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Joint Search and Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-C1 Optimizing Search and quantization separately
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some researchers consider optimizing the two components separately and particularly
    focus on automating the architecture design process. For example, Cai et al. use
    the ProxylessNAS [[23](#bib.bib23)] framework to firstly search for an efficient
    architecture and then perform quantization-aware fine-tuning to further compress
    the model [[203](#bib.bib203)]; Liu et al. [[204](#bib.bib204)] include the quantization-friendly
    activation function ReLU6 in the search space to facilitate the following quantization;
    Peter et al. [[205](#bib.bib205)] compare the performance of post-training quantization
    (PTQ) and quantization-aware training (QAT) and indicate that the PTQ performance
    drops rapidly when the bitwidth is smaller than 4 while QAT only has marginal
    drop even with 1-bit precision. This is due to the incompatibility between quantization
    and the network structure, which is primarily optimized for full precision.
  prefs: []
  type: TYPE_NORMAL
- en: In light of this, attentions are drawn to quantization-aware NAS, where the
    search space is based on quantized operators [[206](#bib.bib206), [207](#bib.bib207),
    [208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210)]. Most of these works
    are for binary neural networks (BNNs) because of their extreme computation and
    memory savings [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209),
    [211](#bib.bib211)]. Shen et al. [[209](#bib.bib209)] apply an evolutionary search
    to optimize the layer-wise channel expansion ratio for binarized VGG and ResNet.
    Phan et al. [[207](#bib.bib207)] discover that binary DWConv has limited representation
    capability and thus attempt to search for the group number for each layer of the
    MobileNet via an evolutionary algorithm as well. BATS [[206](#bib.bib206)] and
    BNAS [[208](#bib.bib208)] concurrently emerge in the sense of differentiable quantization-aware
    NAS. They both design a BNNs-oriented search space discarding the separable convolution
    due to its failure during quantization; BNAS keeps convolutions and dilated convolutions,
    and BATS keeps group convolutions and dilated group convolutions. It is interesting
    to note that the BATS binarizes only activations during training but keeps weights
    at full precision and binarizes the weights after training with a marginal drop
    in accuracy ($\sim$1%). Instead of optimizing the architecture as a whole, Xu
    et al. [[211](#bib.bib211)] probe layer-wise searching under the KD guidance for
    BNN object detectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above work only automatically optimizes the neural architectures while
    using fixed quantization policies. As a result, the final model has a large possibility
    to be sub-optimal: e.g., the BNN with the searched architecture is not necessarily
    smaller and more accurate than a mixed-precision model.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C2 Optimizing search and quantization jointly
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Jointly optimizing the neural architecture and quantization policy can find
    the best combination of both worlds. The most intuitive joint approach is to construct
    a new search space that contains both structure and bitwidth choices, and the
    search process is the same as for traditional hardware-aware NAS [[212](#bib.bib212),
    [213](#bib.bib213)]. For example, JASQ [[212](#bib.bib212)] employs a classical
    evolutionary search algorithm to explore both operators and cell-wise quantization
    bitwidths; Gong et al. [[213](#bib.bib213)], instead, engage differentiable neural
    architecture search to explore a mixed-precision supernet of MBConv hyperparameters
    and bitwidths. Though simple and effective, this strategy is inefficient: the
    search space is enlarged by many times that JASQ requires three GPU days to search
    for a suitable model for each given resource budget.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this issue, some studies decouple the supernet training and search
    process so that no training is required in the search process [[105](#bib.bib105),
    [214](#bib.bib214), [215](#bib.bib215)]. The weight-sharing training paradigm
    is further adopted to reduce the training cost. SPOS [[105](#bib.bib105)] designs
    a single path weight-sharing strategy that trains a supernet independently to
    the search process. In each iteration of the supernet training, only one single
    path of the supernet is activated and trained. The single-path activation is realized
    by uniformly sampling one candidate block at each block level of the supernet.
    The weights of the same block are shared among different supernet training iterations
    (i.e., different paths). In the search process, an evolutionary algorithm is performed
    on randomly sampled paths of the pre-trained supernet to find the optimal combination
    of architecture and block-wise bitwidths. As the supernet is well pre-trained,
    each architecture only performs inference during the evolutionary search. This
    SPOS strategy improves the efficiency of both the supernet training and search.
    APQ [[214](#bib.bib214)] adopts the once-for-all [[77](#bib.bib77)] supernet training
    strategy, which requires little time for retraining the searched model but is
    inefficient to support different bitwidths in the supernet. Therefore, the authors
    devise a quantization-aware accuracy predictor so that they can train the once-for-all
    supernet in full precision and derive the accuracy of a quantized network via
    the accuracy predictor. However, it is hard and time-consuming to collect a large
    volume of [quantized network, accuracy] pairs for preparing the quantization-aware
    accuracy predictor due to lengthy quantization-aware training. To alleviate this
    issue, the authors first train a full precision accuracy predictor with abundant
    [full-precision network, accuracy] pairs from the supernet, and then fine-tune
    it with only a few [quantized network, accuracy] pairs to obtain its quantization-aware
    counterpart. Next, they then do a simple evolutionary search with the accuracy
    predictor to achieve a new model for a given resource budget. Instead of using
    the accuracy predictor to deal with the different bitwidths problem, Shen et al.
    [[215](#bib.bib215)] conceive the once quantization-aware (OQA) training strategy,
    where a set of quantized supernets with sequential bitwidths are created via the
    bit inheritance scheme: the $k-1$ bit supernet uses double quantization step size
    of its $k$ bit counterpart. The authors experimentally show that with just one
    epoch fine-tuning the lower-bit network with bit inheritance outperforms its counterpart
    with QAT from scratch. The final architecture is derived by a simple coarse-to-fine
    architecture selection procedure without any retraining. However, OQA only supports
    fixed precision quantization policies as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The most recent work, QFA [[183](#bib.bib183)], manages to allow joint mixed-precision
    quantization and architecture search without retraining. The authors propose a
    new quantization strategy, batch quantization, to stabilize the estimation of
    the scale factor so the mixed-precision supernet can be stably trained. They replace
    the progressive shrinking training strategy in the work [[215](#bib.bib215)] with
    a newly proposed two-stage training strategy reducing more than half training
    epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Summary of Joint Automated Design Strategies. The “hardware” column
    indicates on which the achieved models are evaluated. The model name (e.g., MobileNetV1)
    in the “Accuracy” column indicates the base model used for search; if the original
    model is not specified, it is searched during training. The latency is reported
    per input; otherwise is specified in the relevant table cells (e.g., FPS). $\downarrow
    n\%$ indicates the compressed model decreases $n$ per cent compared to its original
    model. $\sim$ indicates the exact data is not reported in the paper and thus estimated
    from the reported figures. ‘-’ indicates unavailable records.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters | Memory |'
  prefs: []
  type: TYPE_TB
- en: '| Joint Search and KD | [[194](#bib.bib194)] | CIFAR100 | - | (Acc): 79.74/78.18/75.85
    | - | - | 2.77M/1.47M/0.7M | - |'
  prefs: []
  type: TYPE_TB
- en: '| CINIC-10 |  | (Acc): 85.41/79.45/79.38 | 1.27M/0.36M/0.36M |'
  prefs: []
  type: TYPE_TB
- en: '| [[90](#bib.bib90)] | Cityscapes |  | (mIoU): 71.5 | - | 163.9 FPS | - | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| CamVid | Nvidia Geforce GTX1080Ti | (mIoU): 71.1 | 398.1 FPS |'
  prefs: []
  type: TYPE_TB
- en: '| BDD |  | (mIoU): 55.1 | 318.0 FPS |'
  prefs: []
  type: TYPE_TB
- en: '| Joint Search and Pruning | [[196](#bib.bib196)] | ImageNet | - | MobileNetV1(Top1
    acc): 70.9/66.1/57.2 | 324M/149M/41M | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobileNetV2(Top1 acc): 72.7/68.2/67.3/ | 291M/140M/124M/ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 65.0/63.8/58.3 | 105M/84M/43M |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ResNet50(Top1 error/Top5 error): 76.2/75.4/73.4 | 3000M/2000M/1000M
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | NVIDIA Titan Xp | MobileNetV1(Top1 acc): 71.0/67.4/59.6 | - | 0.48ms/0.30ms/0.17ms
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MobileNetV2(Top1 acc): 73.2/71.7/64.5 | 0.67ms/0.47ms/0.29ms |'
  prefs: []
  type: TYPE_TB
- en: '| [[197](#bib.bib197)] | CIFRA-10 |  | VGG16/ResNet18(acc): 91.62/94.37 | $\downarrow$85.04%/$\downarrow$41.15%
    | $\downarrow$59.56%/$\downarrow$21.3% | $\downarrow$94.46%/$\downarrow$61.71%
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 | NVIDIA GeForce RTX 2080 Ti | VGG16/ResNet34(acc): 69.42/74.59
    | $\downarrow$69.03%/$\downarrow$56.91% | $\downarrow$49.4%/$\downarrow$27.91%
    | $\downarrow$81.14%/$\downarrow$69.94% |'
  prefs: []
  type: TYPE_TB
- en: '| Tiny-ImageNet |  | ResNet18/ResNet34(acc): 56.87/58.64 | $\downarrow$50.16%/$\downarrow$61.89%
    | $\downarrow$22.15%/$\downarrow$27.12% | $\downarrow$24.67%/$\downarrow$31.58%
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[198](#bib.bib198)] | CIFAR-10 | - | VGGNet16/GoogLeNet/ResNet56/ResNet110(Top1
    acc): | 82.81M/513.19M/ | - | 1.67M/2.46M/0.39M/0.56M | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 93.08/94.84/93.23/93.58 | 58.54M/89.87M |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet |  | ResNet18/ResNet34/ResNet50/ResNet101/ResNet152(Top1 acc): |
    968.13M/2170.77M/1890.60M/ | 9.50M/10.12M/11.75M/ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 67.80/70.98/73.86/75.82/77.12 | 3164.91M/4309.52M | 17.72M/24.07M |'
  prefs: []
  type: TYPE_TB
- en: '| [[199](#bib.bib199)] | CIFAR-10 | - | ResNet56/ResNet110(acc): 93.39/93.65
    | $\downarrow$54.43%/$\downarrow$78.32% | - | $\downarrow$58.82%/$\downarrow$80.92%
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 |  | ResNet56(acc): 71.15 | $\downarrow$52.21 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet |  | ResNet34(Top1/Top5 acc): $\downarrow$1.32/$\downarrow$0.87
    | $\downarrow$55.73% | $\downarrow$58.07% |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ResNet50(Top1/Top5 acc): $\downarrow$0.89/$\downarrow$0.53 | $\downarrow$63.34%
    | $\downarrow$64.36% |'
  prefs: []
  type: TYPE_TB
- en: '| PASCAL VOC |  | SSD(mAP): 74.92/74.80/73.73 | $\downarrow$35.85%/$\downarrow$49.51%/$\downarrow$66.44%
    | $\downarrow$35.41%/$\downarrow$50.55%/$\downarrow$61.13% |'
  prefs: []
  type: TYPE_TB
- en: '| [[200](#bib.bib200)] | CIFAR-10 | - | VGG16/GoogLeNet(acc): 93.78/93.18 |
    81.19M/680M | - | 1.64M/2.76M | - |'
  prefs: []
  type: TYPE_TB
- en: '| Carvana | NVIDIA Tesla P40 | Unet(Dice score): 0.9944/0.9896/0.9926 | 81110M/162230M/237260M
    | 190ms/210ms/220ms | 4.3M/5.88M/12.74M |'
  prefs: []
  type: TYPE_TB
- en: '| [[201](#bib.bib201)] | CIFAR-10 | - | VGG16/ResNet56/ResNet110(acc): 93.57/93.31/93.56
    | $\downarrow$70%/$\downarrow$50%/$\downarrow$65% | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet |  | ResNet50/MobileNetV2(Top1 acc): 75.46/71.1 | $\downarrow$51.7%/$\downarrow$30.2%
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[202](#bib.bib202)] | MNIST |  | (acc): 96.97/95.76 | - | 285.82ms/27.06ms
    | - | 1.32KB/0.71KB |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR10-binary | STM32F413 MCUs | (acc): 73.4/70.48 | 2529.84ms/298.57ms
    | 2.4KB/2.12KB |'
  prefs: []
  type: TYPE_TB
- en: '| CUReT-binary |  | (acc): 73.22 | 103.67ms | 2.06KB |'
  prefs: []
  type: TYPE_TB
- en: '| Chars4K-binary |  | (acc): 74.87 | 77.89ms | 1.87KB |'
  prefs: []
  type: TYPE_TB
- en: '| Reference | Dataset | Hardware | Accuracy | Bitwdith | Latency | FLOPs |
    Model Size |'
  prefs: []
  type: TYPE_TB
- en: '| Joint Search and Quantization | [[203](#bib.bib203)] | ImageNet | Google
    Pixel 2 | ProxylessNAS(Top1 acc): 69.2 | 8 | 35ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[204](#bib.bib204)] | ImageNet | ARM Cortex-A72 | Top-1 acc: 75.1 | 8 |
    $\sim$100ms | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Coral USB TPU | Top-1 acc: 76.3 | 8 | $\sim$5ms |'
  prefs: []
  type: TYPE_TB
- en: '|  | Nvidia 2080Ti | Top1 acc: 76.3 | 32 | 22ms |'
  prefs: []
  type: TYPE_TB
- en: '| [[205](#bib.bib205)] | Google Speech | - | acc: 96 | 7 | - | 19.6M | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[206](#bib.bib206)] | ImageNet | - | Top1/Top5 acc: 66.1/87.0 | 1 | - |
    121M | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[207](#bib.bib207)] | ImageNet | - | Top1/Top5 acc: 60.90/82.60 | 1 | -
    | 154M | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: Summary of Joint Automated Design Strategies (cont.).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Reference | Dataset | Hardware | Accuracy | Bitwdith | Latency |
    FLOPs | Model Size |'
  prefs: []
  type: TYPE_TB
- en: '| Joint Search and Quantization | [[208](#bib.bib208)] | ImageNet | - | Top1/Top5
    acc: 63.51/83.91 | 1 | - | 90M | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR10 | Top1 acc: 94.43 | 656M |'
  prefs: []
  type: TYPE_TB
- en: '| [[209](#bib.bib209)] | ImageNet | - | ResNet18(Top1/Top5 acc): 69.65/89.08
    | 1 | - | 660M | - |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR10 | VGG-small(Top1 acc): 93.06 | 59.3M |'
  prefs: []
  type: TYPE_TB
- en: '| [[210](#bib.bib210)] | ImageNet | AMD Ryzen Threadripper 3960X | Top1 acc:
    77.8/77.0 | 32/8 | 297ms/100ms | 717M | - |'
  prefs: []
  type: TYPE_TB
- en: '| Top1 acc: 69.8/67.7 | 151ms/53ms | 141M |'
  prefs: []
  type: TYPE_TB
- en: '| [[211](#bib.bib211)] | PASCAL VOC | - | ResNet18/ResNet34/ResNet50(mAP):
    73.2/75.8/76.9 | 1 | - | 18.49G/21.49G/21.95G | 16.61MB/24.68MB/29.61MB |'
  prefs: []
  type: TYPE_TB
- en: '| [[212](#bib.bib212)] | ImageNet | - | error: 27.22/34.10 | - | - | - | 4.9MB/2.5MB
    |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR10 | error: 2.90/2.97 | 2.5MB/0.9MB |'
  prefs: []
  type: TYPE_TB
- en: '| [[213](#bib.bib213)] | ImgeNet |  | Top1/Top5 error: 31.62/11.56 | mixed
    | 21.19ms | - | 1.44MB |'
  prefs: []
  type: TYPE_TB
- en: '| NVIDIA Tesla P100 GPU | Top1/Top5 error: 28.23/9.94 | 24.71ms | 2.06MB |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR100 |  | error: 22.16/21.27 | 2.53ms/2.98ms | 0.6MB/0.8MB |'
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | ImageNet | - | ResNet18(Top1 acc): 66.4/69.4/7 | mixed
    | - | BitOps: 6.21G/13.49G/24.31G | - |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet34(Top1 acc): 71.5/73.9/74.6 | BitOps: 13.11G/28.78G/51.92G |'
  prefs: []
  type: TYPE_TB
- en: '| [[214](#bib.bib214)] | ImageNet | BitFusion | Top1 acc: 75.1/74.1 | mixed
    | 12.17ms/8.40ms | BitOps: 16.5G/23.6G | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[215](#bib.bib215)] | ImageNet | - | Top1 acc: 61.7/71.3/74.1 | 2/3/4 |
    - | BitOPs: 1.21G/3.07G/9.28G | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[183](#bib.bib183)] | ImageNet | - | Top1 acc: $\sim$71.5 | mixed | - |
    $\sim$200M | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[46](#bib.bib46)] | ImageNet | STM32F746 | Top1 acc: 49.9/40.5 | 8 | 5FPS/10FPS
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| STM32F412 | Top1 acc: 62.0 | 4 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| STM32F746 | Top1 acc: 63.5 |'
  prefs: []
  type: TYPE_TB
- en: '| STM32F765 | Top1 acc: 65.9 |'
  prefs: []
  type: TYPE_TB
- en: '| STM32H743 | Top1 acc: 70.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Pascal VOC | STM32H743 | mAP: 51.4 | 8 | 168M | peak SRAM: 466KB |'
  prefs: []
  type: TYPE_TB
- en: VI Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The design automation for efficient deep learning models is an exciting area
    yet still in its infant research phase. Existing work has covered almost its every
    research aspect and laid firm foundations for its future evolution. In this section,
    we discuss several future directions that are worthy to delve into.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Beyond Computer Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most current works validate their performance on computer vision (CV) tasks,
    e.g., image classification and object detection. This is ascribable to two reasons:
    1) the CV area is the most developed area regarding deep learning research that
    abundant resources are available and experimental protocol is well standardized;
    thus it requires only bare efforts to prepare comparison benchmarks; 2) the CV
    models are mainly based on the CNN family and much manual engineering has been
    devoted to devising efficient CNN models so it is relatively easy to establish
    an efficient search space or base models for compression. Nevertheless, CV is
    not the only field that expects efficient deep learning models; applications like
    IoT are more resource-constrained but seldom investigated regarding the design
    automation for efficient deep learning. Since these domains have dissimilar characteristics
    to CV, specific research is desired to go beyond the CV. Though some works have
    already been presented, like [[146](#bib.bib146), [149](#bib.bib149), [189](#bib.bib189)]
    on automatically compressing language models, these are very limited and more
    room for different applications and the deeper investigation remains to be explored.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Accelerating Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While searched models are driven to be efficient, the search process is often
    time- and computation-intensive because vast numbers of candidate models need
    to be evaluated before choosing the best one. This is problematic as the search
    process is task- and/or hardware-specific and thus a new search is required when
    a new task or hardware arises. Accelerating the search process can not only facilitate
    the task/hardware adaptation but also save the energy consumed during the search.
    The weight-sharing paradigm [[124](#bib.bib124), [54](#bib.bib54)] can alleviate
    this problem but this is still too slow, especially for the hardware-aware settings.
    Using proxies is another strategy to accelerate the search process, which approximates
    different models’ performance without fully training them [[117](#bib.bib117)].
    Most recently, zero-cost proxies for NAS have attained great interest and shown
    outstanding performance on some NAS benchmarks [[216](#bib.bib216)]. This strategy
    aims to rank network architectures without any training or even without seeing
    data [[217](#bib.bib217), [216](#bib.bib216)]. Though remarkable efforts have
    been made in accelerating conventional NAS, how these approaches can be efficiently
    and effectively applied to the hardware-aware NAS and automated compression is
    still an open question. For example, which proxies can be used for automated quantization?
    Or can we use the same proxies for different quantization bitwidths?
  prefs: []
  type: TYPE_NORMAL
- en: Another direction for accelerating the search is to reduce the search space.
    For example, EfficientMet [[68](#bib.bib68)] fixes a baseline model and only searches
    the constant scale ratios of width/depth/resolution. Only a small grid search
    can produce extraordinary performance. An effective and reduced search space is
    based on extensive investigation and human knowledge and thus demands considerable
    effort in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Hardware Simulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The feedback (e.g., latency and memory usage) from target devices is a necessity
    for exploring efficient deep learning models automatically. However, running an
    enormous amount of models on target devices is labour- and time-intensive, especially
    when the devices are general-purpose computing systems (e.g., MCU) instead of
    specific NN accelerators (e.g., TPUs). Furthermore, the frequent communication
    between the target devices and the devices, on which the design program runs,
    takes even more time. Therefore, it is desirable to have an accurate hardware
    simulator that simulates the behaviour of DNN hardware implementations. This simulator
    would accelerate the automated design process by running all models and obtaining
    all feedback on a powerful server. In addition, with a hardware simulator, beginners
    do not need to have cross-disciplinary knowledge in the hardware setting and compilation
    to collect hardware-cost data; the design process becomes easy to get started
    with. Although some works have reported hardware simulators like [[218](#bib.bib218),
    [219](#bib.bib219)], the precision and hardware information that can be provided
    are far from satisfactory. It is also attractive to have a general software platform
    that offers the simulation of deploying DNNs on diverse and configurable devices.
  prefs: []
  type: TYPE_NORMAL
- en: VI-D Benchmarking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite tremendous results that have been reported to achieve state-of-the-art
    performance, most of these works are not based on the same protocol thus resulting
    in skewed comparison. First, some literature uses direct evaluation metrics while
    others use indirect ones. Superiority on indirect metrics does not necessarily
    mean more efficiency on real devices. Second, the comparison results can vary
    on different hardware or different implementation details on the same hardware.
    Finally, the dataset/task settings also influence the performance significantly.
    For example, even using the same dataset, different input resolutions would lead
    to different hardware consumption. It is thus imperative to benchmark these evaluation
    settings to resolve unfair comparisons and make this domain more reproducible.
    In addition, a unified benchmark dataset can also facilitate the comparison of
    design automation algorithms. Li et al. [[220](#bib.bib220)] recently develop
    a comprehensive benchmark dataset of hardware-aware NAS on three categories of
    hardware (e.g., commercial edge devices, FPGA, and ASIC). However, this benchmark
    dataset primarily aims to facilitate the development of search strategies, and
    therefore is relatively small and based on only two spaces. There so far lacks
    well-recognized benchmark settings and a thorough dataset in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the above less noticed and underdeveloped directions, other directions
    like developing novel efficient operators and competent design algorithms have
    been widely researched, but there is still large room to improve.
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This survey provides a thorough review of the design automation techniques
    for fast, lightweight and effective deep learning models. We analyze and summarize
    current studies into three categories: 1) by searching, 2) by compressing, and
    3) by jointly. In addition, we conclude the evaluation metrics specific to efficient
    deep learning models. At the end of this work, we afford discussion of existing
    issues and future directions for both novices and experienced researchers.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] S. Zhang, L. Yao, A. Sun, and Y. Tay, “Deep learning based recommender
    system: A survey and new perspectives,” *ACM Computing Surveys*, vol. 52, no. 1,
    pp. 5:1–5:38, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of
    deep bidirectional transformers for language understanding,” in *NAACL-HLT*, 2019,
    pp. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. Zhang, K. Chen, D. Jian, L. Yao, S. Wang, and P. Li, “Learning attentional
    temporal cues of brainwaves with spatial embedding for motion intent detection,”
    in *ICDM*, 2019, pp. 1450–1455.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. Chen and H. Zhao, “Data security and privacy protection issues in cloud
    computing,” in *ICCSEE*, 2012, pp. 647–651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations
    for deep learning in NLP,” in *ACL*, 2019, pp. 3645–3650.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” *Communications
    of the ACM*, vol. 63, no. 12, pp. 54–63, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas, “Predicting
    parameters in deep learning,” in *NIPS*, 2013, pp. 2148–2156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in *NIPS*,
    1989, pp. 598–605.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” *arXiv preprint arXiv:1704.04861*, pp. 1–9, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *CVPR*, 2018, pp. 4510–4520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient
    convolutional neural network for mobile devices,” in *CVPR*, 2018, pp. 6848–6856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical guidelines
    for efficient cnn architecture design,” in *ECCV*, 2018, pp. 122–138.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating very deep
    neural networks,” in *ICCV*, 2017, pp. 1398–1406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Luo, J. Wu, and W. Lin, “Thinet: A filter level pruning method for
    deep neural network compression,” in *ICCV*, 2017, pp. 5068–5076.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Courbariaux, Y. Bengio, and J.-P. David, “Training deep neural networks
    with low precision multiplications,” in *ICLR Workshop*, 2015, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent
    in nervous activity,” *The Bulletin of Mathematical Biophysics*, vol. 5, no. 4,
    pp. 115–133, 1943.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] T. M. Bartol, C. Bromer, J. Kinney, M. A. Chirillo, J. N. Bourne, K. M.
    Harris, and T. J. Sejnowski, “Hippocampal spine head sizes are highly precise,”
    *bioRxiv*, p. 016329, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] D. J. Linden, *Think Tank: Forty Neuroscientists Explore the Biological
    Roots of Human Experience*.   Yale University Press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. Lin, S. Talathi, and S. Annapureddy, “Fixed point quantization of deep
    convolutional networks,” in *ICML*, 2016, pp. 2849–2858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning
    with limited numerical precision,” in *ICML*, 2015, pp. 1737–1746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” in *NIPS Workshop*, 2014, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, and Z. Xu, “Learning compact
    recurrent neural networks with block-term tensor decomposition,” in *CVPR*, 2018,
    pp. 9378–9387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural architecture
    search on target task and hardware,” in *ICLR*, 2019, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] X. Xiao, Z. Wang, and S. Rajasekaran, “Autoprune: Automatic network pruning
    by regularizing auxiliary parameters,” in *NeurIPS*, 2019, pp. 13 681–13 691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “HAQ: hardware-aware automated
    quantization with mixed precision,” in *CVPR*, 2019, pp. 8612–8620.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. He, J. Lin, Z. Liu, H. Wang, L. Li, and S. Han, “AMC: automl for model
    compression and acceleration on mobile devices,” in *ECCV*, 2018, pp. 815–832.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] D. Langerman, A. Johnson, K. Buettner, and A. D. George, “Beyond floating-point
    ops: CNN performance prediction with critical datapath length,” in *HPEC*, 2020,
    pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Yao, Y. Zhao, H. Shao, S. Liu, D. Liu, L. Su, and T. Abdelzaher, “Fastdeepiot:
    Towards understanding and optimizing neural network execution time on mobile and
    embedded devices,” in *SenSys*, 2018, pp. 278–291.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang, and Q. Tian, “Condensenet
    v2: Sparse feature reactivation for deep networks,” in *CVPR*, 2021, pp. 3569–3578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] V. Camus, C. Enz, and M. Verhelst, “Survey of precision-scalable multiply-accumulate
    units for neural-network processing,” in *AICAS*, 2019, pp. 57–61.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] G. Chu, O. Arikan, G. Bender, W. Wang, A. Brighton, P.-J. Kindermans,
    H. Liu, B. Akin, S. Gupta, and A. Howard, “Discovering multi-hardware mobile models
    via architecture search,” in *CVPR Workshops*, 2021, pp. 3022–3031.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Benchmark analysis
    of representative deep neural network architectures,” *IEEE Access*, vol. 6, pp.
    64 270–64 277, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] C. Zhengbo, T. Lei, and C. Zuoning, “Research and design of activation
    function hardware implementation methods,” in *Journal of Physics: Conference
    Series*, vol. 1684, no. 1, 2020, p. 012111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] N. Dryden, N. Maruyama, T. Benson, T. Moon, M. Snir, and B. Van Essen,
    “Improving strong-scaling of cnn training by exploiting finer-grained parallelism,”
    in *IPDPS*, 2019, pp. 210–220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] W. Niu, M. Sun, Z. Li, J. Chen, J. Guan, X. Shen, Y. Wang, S. Liu, X. Lin,
    and B. Ren, “Rt3d: Achieving real-time execution of 3d convolutional neural networks
    on mobile devices,” in *AAAI*, 2021, pp. 9179–9187.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
    and E. Shelhamer, “cudnn: Efficient primitives for deep learning,” *arXiv preprint
    arXiv:1410.0759*, pp. 1–9, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] E. Liberis and N. D. Lane, “Neural networks on microcontrollers: saving
    memory at inference via operator reordering,” *arXiv preprint arXiv:1910.05110*,
    pp. 1–8, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] N. Suda and D. Loh, “Machine learning on arm cortex-m microcontrollers,”
    *Arm Ltd.: Cambridge, UK*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] K. Siu, D. M. Stuart, M. Mahmoud, and A. Moshovos, “Memory requirements
    for convolutional neural network hardware accelerators,” in *IISWC*, 2018, pp.
    111–121.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Xu, K. Ota, and M. Dong, “Saving energy on the edge: In-memory caching
    for multi-tier heterogeneous networks,” *IEEE Communications Magazine*, vol. 56,
    no. 5, pp. 102–107, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] H. Unlu, “Efficient neural network deployment for microcontroller,” *arXiv
    preprint arXiv:2007.01348*, pp. 1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Ioannou, D. Robertson, R. Cipolla, and A. Criminisi, “Deep roots: Improving
    cnn efficiency with hierarchical filter groups,” in *CVPR*, 2017, pp. 1231–1240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] L. Yang, B. Lu, and S. Ren, “A note on latency variability of deep neural
    networks for mobile inference,” *arXiv preprint arXiv:2003.00138*, pp. 1–5, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] M. Syed and A. A. Srinivasan, “Generalized latency performance estimation
    for once-for-all neural architecture search,” *arXiv preprint arXiv:2101.00732*,
    pp. 1–12, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Q. Wang and X. Chu, “Gpgpu performance estimation with core and memory
    frequency scaling,” *IEEE Transactions on Parallel and Distributed Systems*, vol. 31,
    no. 12, pp. 2865–2881, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Lin, W. Chen, Y. Lin, J. Cohn, C. Gan, and S. Han, “Mcunet: Tiny deep
    learning on iot devices,” in *NeurIPS*, 2020, p. 11711–11722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V.
    Le, “Mnasnet: Platform-aware neural architecture search for mobile,” in *CVPR*,
    2019, pp. 2820–2828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable
    architectures for scalable image recognition,” in *CVPR*, 2018, pp. 8697–8710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. He, K. Zhao, and X. Chu, “Automl: A survey of the state-of-the-art,”
    *Knowledge-Based Systems*, vol. 212, p. 106622, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen, and K. C. Tan, “A survey
    on evolutionary neural architecture search,” *IEEE Transactions on Neural Networks
    and Learning Systems*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement learning,”
    in *ICLR*, 2017, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] X. Dong and Y. Yang, “Searching for a robust neural architecture in four
    GPU hours,” in *CVPR*, 2019, pp. 1761–1770.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture
    search,” in *ICLR*, 2019, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] C. Xu, B. Wu, Z. Wang, W. Zhan, P. Vajda, K. Keutzer, and M. Tomizuka,
    “Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation,”
    in *ECCV*, 2020, pp. 1–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Qiu, Y. Liu, S. Li, and J. Xu, “Miniseg: An extremely minimum network
    for efficient covid-19 segmentation,” in *AAAI*, 2021, pp. 4846–4854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A
    survey,” *The Journal of Machine Learning Research*, vol. 20, pp. 55:1–55:21,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu, “Hierarchical
    representations for efficient architecture search,” in *ICLR*, 2018, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *CVPR*, 2017, pp. 2261–2269.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Z. Lu, I. Whalen, V. Boddeti, Y. D. Dhebar, K. Deb, E. D. Goodman, and
    W. Banzhaf, “Nsga-net: neural architecture search using multi-objective genetic
    algorithm,” in *GECCO*, 2019, pp. 419–427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] F. Scheidegger, L. Benini, C. Bekas, and A. C. I. Malossi, “Constrained
    deep neural network architecture search for iot devices accounting for hardware
    calibration,” in *NeurIPS*, 2019, pp. 6054–6064.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Q. Lu, W. Jiang, X. Xu, Y. Shi, and J. Hu, “On neural architecture search
    for resource-constrained hardware platforms,” *arXiv preprint arXiv:1911.00105*,
    pp. 1–8, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] T. Elsken, J. H. Metzen, and F. Hutter, “Efficient multi-objective neural
    architecture search via lamarckian evolution,” in *ICLR*, 2019, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Zhou, S. Ebrahimi, S. Ö. Arık, H. Yu, H. Liu, and G. Diamos, “Resource-efficient
    neural architect,” *arXiv preprint arXiv:1806.07912*, pp. 1–14, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu,
    and D. Marculescu, “Single-path NAS: designing hardware-efficient convnets in
    less than 4 hours,” in *ECML-PKDD*, 2019, pp. 481–497.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. Fang, Y. Sun, Q. Zhang, Y. Li, W. Liu, and X. Wang, “Densely connected
    search space for more flexible neural architecture search,” in *CVPR*, 2020, pp.
    10 625–10 634.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] M. Tan and Q. V. Le, “Efficientnet: Rethinking model scaling for convolutional
    neural networks,” in *ICML*, 2019, pp. 6105–6114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Xiong, H. Liu, S. Gupta, B. Akin, G. Bender, Y. Wang, P. Kindermans,
    M. Tan, V. Singh, and B. Chen, “Mobiledets: Searching for object detection architectures
    for mobile accelerators,” in *CVPR*, 2021, pp. 3825–3834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] S. Li, M. Tan, R. Pang, A. Li, L. Cheng, Q. V. Le, and N. P. Jouppi, “Searching
    for fast model families on datacenter accelerators,” in *CVPR*, 2021, pp. 8085–8095.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *CVPR*,
    2018, pp. 7132–7141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Howard, R. Pang, H. Adam, Q. V. Le, M. Sandler, B. Chen, W. Wang, L. Chen,
    M. Tan, G. Chu, V. Vasudevan, and Y. Zhu, “Searching for mobilenetv3,” in *ICCV*,
    2019, pp. 1314–1324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] R. Avenash and P. Viswanath, “Semantic segmentation of satellite images
    using a modified CNN with hard-swish activation function,” in *VISIGRAPP*, 2019,
    pp. 413–420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Courbariaux, Y. Bengio, and J. David, “Binaryconnect: Training deep
    neural networks with binary weights during propagations,” in *NIPS*, 2015, pp.
    3123–3131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. Yu, P. Jin, H. Liu, G. Bender, P. Kindermans, M. Tan, T. S. Huang,
    X. Song, R. Pang, and Q. Le, “Bignas: Scaling up neural architecture search with
    big single-stage models,” in *ECCV*, 2020, pp. 702–717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Chu, B. Zhang, and R. Xu, “Moga: Searching beyond mobilenetv3,” in
    *ICASSP*, 2020, pp. 4042–4046.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train one
    network and specialize it for efficient deployment,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen, Y. Tian, M. Yu,
    P. Vajda, and J. E. Gonzalez, “Fbnetv3: Joint architecture-recipe search using
    predictor pretraining,” in *CVPR*, 2021, pp. 16 276–16 285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu, T. Xu,
    K. Chen, P. Vajda, and J. E. Gonzalez, “Fbnetv2: Differentiable neural architecture
    search for spatial and channel dimensions,” in *CVPR*, 2020, pp. 12 962–12 971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] B. Chen, G. Ghiasi, H. Liu, T. Lin, D. Kalenichenko, H. Adam, and Q. V.
    Le, “Mnasfpn: Learning latency-aware pyramid architecture for object detection
    on mobile devices,” in *CVPR*, 2020, pp. 13 604–13 613.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] B. Yan, H. Peng, K. Wu, D. Wang, J. Fu, and H. Lu, “Lighttrack: Finding
    lightweight neural networks for object tracking via one-shot architecture search,”
    in *CVPR*, 2021, pp. 15 180–15 189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. Xie, R. B. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in *CVPR*, 2017, pp. 5987–5995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NIPS*, 2012, pp. 1106–1114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] L. Xu, Y. Guan, S. Jin, W. Liu, C. Qian, P. Luo, W. Ouyang, and X. Wang,
    “Vipnas: Efficient video pose estimation via neural architecture search,” in *CVPR*,
    2021, pp. 16 072–16 081.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Dong, A. Cheng, D. Juan, W. Wei, and M. Sun, “Dpp-net: Device-aware
    progressive search for pareto-optimal neural architectures,” in *ECCV*, 2018,
    pp. 540–555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, Y. Jia,
    and K. Keutzer, “Fbnet: Hardware-aware efficient convnet design via differentiable
    neural architecture search,” in *CVPR*, 2019, pp. 10 734–10 742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Xiong, R. Mehta, and V. Singh, “Resource constrained neural network
    architecture search: Will a submodularity assumption help?” in *ICCV*, 2019, pp.
    1901–1910.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C.-H. Hsu, S.-H. Chang, J.-H. Liang, H.-P. Chou, C.-H. Liu, S.-C. Chang,
    J.-Y. Pan, Y.-T. Chen, W. Wei, and D.-C. Juan, “Monas: Multi-objective neural
    architecture search,” *arXiv preprint arXiv:1806.10332*, pp. 1–8, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] G. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger, “Condensenet:
    An efficient densenet using learned group convolutions,” in *CVPR*, 2018, pp.
    2752–2761.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] W. Chen, X. Gong, X. Liu, Q. Zhang, Y. Li, and Z. Wang, “Fasterseg: Searching
    for faster real-time semantic segmentation,” in *ICLR*, 2020, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han, “Searching
    efficient 3d architectures with sparse point-voxel convolution,” in *ECCV*, 2020,
    pp. 685–702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] M. Ding, X. Lian, L. Yang, P. Wang, X. Jin, Z. Lu, and P. Luo, “HR-NAS:
    searching efficient high-resolution neural architectures with lightweight transformers,”
    in *CVPR*, 2021, pp. 2982–2992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NIPS*, 2017, pp.
    5998–6008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] D. R. So, Q. V. Le, and C. Liang, “The evolved transformer,” in *ICML*,
    2019, pp. 5877–5886.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
    “End-to-end object detection with transformers,” in *ECCV*, 2020, pp. 213–229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    in *ICLR*, 2021, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Y. Chen, R. Gao, F. Liu, and D. Zhao, “Modulenet: Knowledge-inherited
    neural architecture search,” *IEEE Transactions on Cybernetics*, pp. 1–11, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *CVPR*, 2016, pp. 2818–2826.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L. Li, L. Fei-Fei, A. L.
    Yuille, J. Huang, and K. Murphy, “Progressive neural architecture search,” in
    *ECCV*, 2018, pp. 19–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] D. Han, J. Kim, and J. Kim, “Deep pyramidal residual networks,” in *CVPR*,
    2017, pp. 6307–6315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] H. Cai, J. Yang, W. Zhang, S. Han, and Y. Yu, “Path-level network transformation
    for efficient architecture search,” in *ICML*, 2018, pp. 677–686.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Xie, A. Kirillov, R. B. Girshick, and K. He, “Exploring randomly wired
    neural networks for image recognition,” in *ICCV*, 2019, pp. 1284–1293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,”
    *Journal of Machine Learning Research*, vol. 13, pp. 281–305, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, “Single
    path one-shot neural architecture search with uniform sampling,” in *ECCV*, 2020,
    pp. 544–560.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] G. Bender, H. Liu, B. Chen, G. Chu, S. Cheng, P. Kindermans, and Q. V.
    Le, “Can weight sharing outperform random architecture search? an investigation
    with tunas,” in *CVPR*, 2020, pp. 14 311–14 320.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M. Odema, N. Rashid, B. U. Demirel, and M. A. A. Faruque, “Lens: Layer
    distribution enabled neural architecture search in edge-cloud hierarchies,” in
    *DAC*, 2021, pp. 403–408.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Z. Yang, S. Zhang, R. Li, C. Li, M. Wang, D. Wang, and M. Zhang, “Efficient
    resource-aware convolutional neural architecture search for edge computing with
    pareto-bayesian optimization,” *Sensors*, vol. 21, no. 2, p. 444, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Parsa, J. P. Mitchell, C. D. Schuman, R. M. Patton, T. E. Potok, and
    K. Roy, “Bayesian multi-objective hyperparameter optimization for accurate, fast,
    and efficient neural network accelerator design,” *Frontiers in Neuroscience*,
    p. 667, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] D. Eriksson, I. Pierce, J. Chuang, S. Daulton, P. Xia, A. Shrivastava,
    A. Babu, S. Zhao, A. A. Aly, G. Venkatesh *et al.*, “Latency-aware neural architecture
    search with multi-objective bayesian optimization,” in *ICML Workshop on Automated
    Machine Learning (AutoML)*, 2021, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] D. Gaudrie, “High-dimensional bayesian multi-objective optimization,”
    Ph.D. dissertation, Lyon, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. Daulton, D. Eriksson, M. Balandat, and E. Bakshy, “Multi-objective
    bayesian optimization over high-dimensional search spaces,” *arXiv preprint arXiv:2109.10964*,
    pp. 1–11, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] B. Moons, P. Noorzad, A. Skliar, G. Mariani, D. Mehta, C. Lott, and T. Blankevoort,
    “Distilling optimal neural networks: Rapid search in diverse spaces,” in *ICCV*,
    2021, pp. 12 209–12 218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] X. Luo, D. Liu, S. Huai, and W. Liu, “Hsconas: Hardware-software co-design
    of efficient dnns via neural architecture search,” in *DATE*, 2021, pp. 418–421.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] X. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y. Wang, M. Dukhan, Y. Hu, Y. Wu,
    Y. Jia, P. Vajda, M. Uyttendaele, and N. K. Jha, “Chamnet: Towards efficient network
    design through platform-aware model adaptation,” in *CVPR*, 2019, pp. 11 398–11 407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] M. Srinivas and L. M. Patnaik, “Adaptive probabilities of crossover and
    mutation in genetic algorithms,” *IEEE Transactions on Systems, Man, and Cybernetics*,
    vol. 24, no. 4, pp. 656–667, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, “Regularized evolution
    for image classifier architecture search,” in *AAAI*, 2019, pp. 4780–4789.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] X. Chu, B. Zhang, and R. Xu, “Fairnas: Rethinking evaluation fairness
    of weight sharing neural architecture search,” in *ICCV*, 2021, pp. 12 219–12 228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan, “A fast and elitist
    multiobjective genetic algorithm: NSGA-II,” *IEEE Transactions on Evolutionary
    Computing*, vol. 6, no. 2, pp. 182–197, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine Learning*, vol. 8, pp. 229–256,
    1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, pp. 1–9, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, “Efficient neural architecture
    search via parameters sharing,” in *ICML*, 2018, pp. 4095–4104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with gumbel-softmax,”
    in *ICLR*, 2017, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efficient neural
    architecture search via parameter sharing,” in *ICML*, 2018, pp. 4092–4101.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] G. Bender, P. Kindermans, B. Zoph, V. Vasudevan, and Q. V. Le, “Understanding
    and simplifying one-shot architecture search,” in *ICML*, 2018, pp. 550–559.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Yu and T. S. Huang, “Universally slimmable networks and improved training
    techniques,” in *ICCV*, 2019, pp. 1803–1811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] L. Deng, G. Li, S. Han, L. Shi, and Y. Xie, “Model compression and hardware
    acceleration for neural networks: A comprehensive survey,” *Proceedings of the
    IEEE*, vol. 108, no. 4, pp. 485–532, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] T. Choudhary, V. Mishra, A. Goswami, and J. Sarangapani, “A comprehensive
    survey on model compression and acceleration,” *Artificial Intelligence Review*,
    vol. 53, no. 7, pp. 5113–5155, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] L. R. Tucker, “Some mathematical notes on three-mode factor analysis,”
    *Psychometrika*, vol. 31, no. 3, pp. 279–311, 1966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Y. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin, “Compression
    of deep convolutional neural networks for fast and low power mobile applications,”
    in *ICLR*, 2016, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Kossaifi, A. Khanna, Z. Lipton, T. Furlanello, and A. Anandkumar,
    “Tensor contraction layers for parsimonious deep nets,” in *CVPR Workshops*, 2017,
    pp. 1940–1946.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] J. D. Carroll and J.-J. Chang, “Analysis of individual differences in
    multidimensional scaling via an n-way generalization of “eckart-young” decomposition,”
    *Psychometrika*, vol. 35, no. 3, pp. 283–319, 1970.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] M. Astrid and S. Lee, “Cp-decomposition with tensor power method for
    convolutional neural networks compression,” in *BigComp*, 2017, pp. 115–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lempitsky,
    “Speeding-up convolutional neural networks using fine-tuned cp-decomposition,”
    in *ICLR*, 2015, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Nakajima, M. Sugiyama, S. D. Babacan, and R. Tomioka, “Global analytic
    solution of fully-observed variational bayesian matrix factorization,” *Journal
    of Machine Learning Research*, vol. 14, no. 1, pp. 1–37, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. Gusak, M. Kholyavchenko, E. Ponomarev, L. Markeeva, P. Blagoveschensky,
    A. Cichocki, and I. V. Oseledets, “Automated multi-stage compression of neural
    networks,” in *ICCVW*, 2019, pp. 2501–2508.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] C. Hawkins, X. Liu, and Z. Zhang, “Towards compact neural networks via
    end-to-end training: A bayesian tensor approach with automatic rank determination,”
    *SIAM Journal on Mathematics of Data Science*, vol. 4, no. 1, pp. 46–71, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] C. Hawkins and Z. Zhang, “Bayesian tensorized neural networks with automatic
    rank selection,” *Neurocomputing*, vol. 453, pp. 172–180, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] M. Kodryan, D. Kropotov, and D. Vetrov, “Mars: Masked automatic ranks
    selection in tensor decompositions,” *arXiv preprint arXiv:2006.10859*, pp. 1–12,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] M. Javaheripi, M. Samragh, and F. Koushanfar, “Autorank: Automated rank
    selection for effective neural network customization,” *IEEE Journal on Emerging
    and Selected Topics in Circuits and Systems*, vol. 11, no. 4, pp. 611–619, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M. Samragh, M. Javaheripi, and F. Koushanfar, “Autorank: Automated rank
    selection for effective neural network customization,” in *ISCA*, 2019, pp. 611–619.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] X. Ma, A. R. Triki, M. Berman, C. Sagonas, J. Calì, and M. B. Blaschko,
    “A bayesian optimization framework for neural network compression,” in *ICCV*,
    2019, pp. 10 273–10 282.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] C. Bucila, R. Caruana, and A. Niculescu-Mizil, “Model compression,” in
    *KDD*, 2006, pp. 535–541.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in *NIPS*,
    2014, pp. 2654–2662.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Y. Liu, X. Jia, M. Tan, R. Vemulapalli, Y. Zhu, B. Green, and X. Wang,
    “Search to distill: Pearls are everywhere but not the eyes,” in *CVPR*, 2020,
    pp. 7536–7545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] X. Guo, J. Yang, H. Zhou, X. Ye, and J. Li, “Rosearch: Search for robust
    student architectures when distilling pre-trained language models,” *arXiv preprint
    arXiv:2106.03613*, pp. 1–10, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Z. Lei, K. Yang, K. Jiang, and S. Chen, “Kdas-reid: Architecture search
    for person re-identification via distilled knowledge with dynamic temperature,”
    *Algorithms*, vol. 14, no. 5, p. 137, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] R. H. Eyono, F. M. Carlucci, P. M. Esperança, B. Ru, and P. Torr, “Autokd:
    Automatic knowledge distillation into a student architecture family,” *arXiv preprint
    arXiv:2111.03555*, pp. 1–12, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] X. Zhang, Z. Zhou, D. Chen, and Y. E. Wang, “Autodistill: an end-to-end
    framework to explore and distill hardware-efficient language models,” *arXiv preprint
    arXiv:2201.08539*, pp. 1–14, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] L. Chen, F. Yuan, J. Yang, M. Yang, and C. Li, “Scene-adaptive knowledge
    distillation for sequential recommendation via differentiable architecture search,”
    *arXiv preprint arXiv:2107.07173*, pp. 1–11, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Z. Zhang, W. Zhu, J. Yan, P. Gao, and G. Xie, “Automatic student network
    search for knowledge distillation,” in *ICPR*, 2020, pp. 2446–2453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] M. Li, J. Lin, Y. Ding, Z. Liu, J. Zhu, and S. Han, “Gan compression:
    Efficient architectures for interactive conditional gans,” in *CVPR*, 2020, pp.
    5283–5293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] W. Cheng, M. Zhao, Z. Ye, and S. Gu, “Mfagan: A compression framework
    for memory-efficient on-device super-resolution gan,” *arXiv preprint arXiv:2107.12679*,
    pp. 1–10, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] D. M. Vo, A. Sugimoto, and H. Nakayama, “PPCD-GAN: progressive pruning
    and class-aware distillation for large-scale conditional gans compression,” in
    *WACV*, 2022, pp. 1422–1430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] M. Kang, J. Mun, and B. Han, “Towards oracle knowledge distillation with
    neural architecture search,” in *AAAI*, 2020, pp. 4404–4411.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] K. Mitsuno, Y. Nomura, and T. Kurita, “Channel planting for deep neural
    networks using knowledge distillation,” in *ICPR*, 2020, pp. 7573–7579.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, and T. Liu, “NAS-BERT:
    task-agnostic and adaptive-size BERT compression with neural architecture search,”
    in *KDD*, 2021, pp. 1933–1943.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] C. Li, J. Peng, L. Yuan, G. Wang, X. Liang, L. Lin, and X. Chang, “Block-wisely
    supervised neural architecture search with knowledge distillation,” in *CVPR*,
    2020, pp. 1986–1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] S. Park, J. Lee, S. Mo, and J. Shin, “Lookahead: A far-sighted alternative
    of magnitude-based pruning,” in *ICLR*, 2020, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] T. Zhang, S. Ye, Y. Zhang, Y. Wang, and M. Fardad, “A systematic weight
    pruning framework of dnns using alternating direction method of multipliers,”
    in *ECCV*, 2018, p. 191–207.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] W. Zeng, Y. Xiong, and R. Urtasun, “Network automatic pruning: Start
    nap and take a nap,” *arXiv preprint arXiv:2101.06608*, pp. 1–14, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Z. Li, Y. Gong, X. Ma, S. Liu, M. Sun, Z. Zhan, Z. Kong, G. Yuan, and
    Y. Wang, “Ss-auto: A single-shot, automatic structured weight pruning framework
    of dnns with ultra-high efficiency,” *arXiv preprint arXiv:2001.08839*, pp. 1–8,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] X. Zheng, Y. Ma, T. Xi, G. Zhang, E. Ding, Y. Li, J. Chen, Y. Tian, and
    R. Ji, “An information theory-inspired strategy for automatic network pruning,”
    *arXiv preprint arXiv:2108.08532*, pp. 1–10, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] N. Liu, X. Ma, Z. Xu, Y. Wang, J. Tang, and J. Ye, “Autocompress: An
    automatic DNN structured pruning framework for ultra-high compression rates,”
    in *AAAI*, 2020, pp. 4876–4883.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] S. Yang, W. Chen, X. Zhang, S. He, Y. Yin, and X. Sun, “Auto-prune: automated
    dnn pruning and mapping for reram-based accelerator,” in *ICS*, 2021, pp. 304–315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] B. Li, Y. Fan, Z. Pan, Y. Bian, and G. Zhang, “Automatic channel pruning
    with hyper-parameter search and dynamic masking,” in *MM*, 2021, pp. 2121–2129.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] F. Tung, S. Muralidharan, and G. Mori, “Fine-pruning: Joint fine-tuning
    and compression of a convolutional network with bayesian optimization,” in *BMVC*,
    2017, pp. 115.1–115.12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] C. Chen, F. Tung, N. Vedula, and G. Mori, “Constraint-aware deep neural
    network compression,” in *ECCV*, 2018, pp. 409–424.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] J. Mu, H. Fan, and W. Zhang, “High-dimensional bayesian optimization
    for cnn auto pruning with clustering and rollback,” in *ECCV*, 2022, pp. 1–17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] J. Li, H. Li, Y. Chen, Z. Ding, N. Li, M. Ma, Z. Duan, and D. Zhao, “Abcp:
    Automatic block-wise and channel-wise network pruning via joint search,” *arXiv
    preprint arXiv:2110.03858*, pp. 1–12, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] L. Guerra and T. Drummond, “Automatic pruning for quantized neural networks,”
    in *DICTA*, 2021, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] T. Yang, A. G. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze,
    and H. Adam, “Netadapt: Platform-aware neural network adaptation for mobile applications,”
    in *ECCV*, 2018, pp. 289–304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] T. Yang, Y. Liao, and V. Sze, “Netadaptv2: Efficient neural architecture
    search with fast super-network training and architecture optimization,” in *CVPR*,
    2021, pp. 2402–2411.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Y. Guan, N. Liu, P. Zhao, Z. Che, K. Bian, Y. Wang, and J. Tang, “Dais:
    Automatic channel pruning via differentiable annealing indicator search,” *IEEE
    Transactions on Neural Network and Learning Systems*, pp. 1–12, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning efficient
    convolutional networks through network slimming,” in *ICCV*, 2017, pp. 2755–2763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Y. Li, S. Gu, K. Zhang, L. V. Gool, and R. Timofte, “DHP: differentiable
    meta pruning via hypernetworks,” in *ECCV*, 2020, pp. 608–624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] S. Qu, B. Li, Y. Wang, and L. Zhang, “ASBP: automatic structured bit-pruning
    for rram-based NN accelerator,” in *DAC*, 2021, pp. 745–750.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] X. Dong and Y. Yang, “Network pruning via transformable architecture
    search,” in *NeurIPS*, 2019, pp. 759–770.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] F. Xue and J. Xin, “Network compression via cooperative architecture
    search and distillation,” in *AI4I*, 2021, pp. 42–43.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] L. Yao, R. Pi, H. Xu, W. Zhang, Z. Li, and T. Zhang, “Joint-detnas: Upgrade
    your detector with nas, pruning and dynamic distillation,” in *CVPR*, 2021, pp.
    10 175–10 184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] J. Gu and V. Tresp, “Search for better students to learn distilled knowledge,”
    in *ECAI*, 2020, pp. 1159–1165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] M. Horowitz, “1.1 computing’s energy problem (and what we can do about
    it),” in *ISSCC*, 2014, pp. 10–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] H. Bai, M. Cao, P. Huang, and J. Shan, “Batchquant: Quantized-for-all
    architecture search with robust quantizer,” in *NeurIPS*, 2021, pp. 1074–1085.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] EENews, “Apple describes 7nm a12 bionic chips,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Nvidia, “Nvidia tensor cores,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “Hardware-centric automl
    for mixed-precision quantization,” *International Journal of Computer Vision*,
    vol. 128, no. 8, pp. 2035–2048, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] G. Lacey, G. W. Taylor, and S. Areibi, “Stochastic layer-wise precision
    in deep neural networks,” in *UAI*, 2018, pp. 663–672.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. Yu, Q. Han, J. Li, J. Shi, G. Cheng, and B. Fan, “Search what you
    want: Barrier panelty NAS for mixed precision quantization,” in *ECCV*, 2020,
    pp. 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] J. Xu, S. Hu, J. Yu, X. Liu, and H. Meng, “Mixed precision quantization
    of transformer language models for speech recognition,” in *ICASSP*, 2021, pp.
    7383–7387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Q. Sun, L. Jiao, Y. Ren, X. Li, F. Shang, and F. Liu, “Effective and
    fast: A novel sequential single path search for mixed-precision quantization,”
    *IEEE Transactions on Cybernetics*, pp. 1–13, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] X. Wei, H. Chen, W. Liu, and Y. Xie, “Mixed-precision quantization for
    cnn-based remote sensing scene classification,” *IEEE Geoscience and Remote Sensing
    Letters*, vol. 18, no. 10, pp. 1721–1725, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] B. Wu, Y. Wang, P. Zhang, Y. Tian, P. Vajda, and K. Keutzer, “Mixed precision
    quantization of convnets via differentiable neural architecture search,” *arXiv
    preprint arXiv:1812.00090*, pp. 1–11, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen,
    and T. Blankevoort, “A white paper on neural network quantization,” *arXiv preprint
    arXiv:2106.08295*, pp. 1–27, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Y. Guan, P. Zhao, B. Wang, Y. Zhang, C. Yao, K. Bian, and J. Tang, “Differentiable
    feature aggregation search for knowledge distillation,” in *ECCV*, 2020, pp. 469–484.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value
    of network pruning,” in *ICLR*, 2019, pp. 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K. Cheng, and J. Sun, “Metapruning:
    Meta learning for automatic neural network channel pruning,” in *ICCV*, 2019,
    pp. 3295–3304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] J. Liu, J. Sun, Z. Xu, and G. Sun, “Latency-aware automatic cnn channel
    pruning with gpu runtime analysis,” *BenchCouncil Transactions on Benchmarks,
    Standards and Evaluations*, vol. 1, no. 1, p. 100009, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] M. Lin, R. Ji, Y. Zhang, B. Zhang, Y. Wu, and Y. Tian, “Channel pruning
    via automatic structure search,” in *IJCAI*, 2020, pp. 673–679.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Chang, Y. Lu, P. Xue, Y. Xu, and Z. Wei, “Acp: Automatic channel pruning
    via clustering and swarm intelligence optimization for cnn,” *arXiv preprint arXiv:2101.06407*,
    pp. 1–10, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Y. Liu, Y. Wang, H. Qi, and X. Ju, “Superpruner: Automatic neural network
    pruning via super network,” *Scientific Programming*, vol. 2021, pp. 1–11, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] L. Lin, Y. Yang, and Z. Guo, “Aacp: Model compression by accurate and
    automatic channel pruning,” *arXiv preprint arXiv:2102.00390*, pp. 1–10, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] I. Fedorov, R. P. Adams, M. Mattina, and P. N. Whatmough, “Sparse: Sparse
    architecture search for cnns on resource-constrained microcontrollers,” in *NeurIPS*,
    2019, pp. 4978–4990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] H. Cai, T. Wang, Z. Wu, K. Wang, J. Lin, and S. Han, “On-device image
    classification with proxyless neural architecture search and quantization-aware
    fine-tuning,” in *ICCVW*, 2019, pp. 2509–2513.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] C. Liu, Y. Han, Y. Sung, Y. Lee, H. Chiang, and K. Wu, “FOX-NAS: fast,
    on-device and explainable neural architecture search,” in *ICCVW*, 2021, pp. 789–797.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] D. Peter, W. Roth, and F. Pernkopf, “Resource-efficient dnns for keyword
    spotting using neural architecture search and quantization,” in *ICPR*, 2020,
    pp. 9273–9279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] A. Bulat, B. Martínez, and G. Tzimiropoulos, “BATS: binary architecture
    search,” in *ECCV*, 2020, pp. 309–325.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] H. Phan, Z. Liu, D. Huynh, M. Savvides, K. Cheng, and Z. Shen, “Binarizing
    mobilenet via evolution-based searching,” in *CVPR*, 2020, pp. 13 417–13 426.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] D. Kim, K. P. Singh, and J. Choi, “Learning architectures for binary
    networks,” in *ECCV*, 2020, pp. 575–591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] M. Shen, K. Han, C. Xu, and Y. Wang, “Searching for accurate binary neural
    architectures,” in *ICCVW*, 2019, pp. 2041–2044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] T. Kim, Y. Yoo, and J. Yang, “Frostnet: Towards quantization-awareof
    network architecture search,” *arXiv preprint arXiv:2006.09679*, pp. 1–18, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] S. Xu, J. Zhao, J. Lu, B. Zhang, S. Han, and D. S. Doermann, “Layer-wise
    searching for 1-bit detectors,” in *CVPR*, 2021, pp. 5682–5691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Y. Chen, G. Meng, Q. Zhang, X. Zhang, L. Song, S. Xiang, and C. Pan,
    “Joint neural architecture search and quantization,” *arXiv preprint arXiv:1811.09426*,
    pp. 1–10, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] C. Gong, Z. Jiang, D. Wang, Y. Lin, Q. Liu, and D. Z. Pan, “Mixed precision
    neural architecture search for energy efficient deep learning,” in *ICCAD*, 2019,
    pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] T. Wang, K. Wang, H. Cai, J. Lin, Z. Liu, H. Wang, Y. Lin, and S. Han,
    “APQ: joint search for network architecture, pruning and quantization policy,”
    in *CVPR*, 2020, pp. 2075–2084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] M. Shen, F. Liang, R. Gong, Y. Li, C. Li, C. Lin, F. Yu, J. Yan, and
    W. Ouyang, “Once quantization-aware training: High performance extremely low-bit
    architecture search,” in *ICCV*, 2021, pp. 5320–5329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] M. S. Abdelfattah, A. Mehrotra, L. Dudziak, and N. D. Lane, “Zero-cost
    proxies for lightweight NAS,” in *ICLR*, 2021, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] M. Lin, P. Wang, Z. Sun, H. Chen, X. Sun, Q. Qian, H. Li, and R. Jin,
    “Zen-nas: A zero-shot NAS for high-performance image recognition,” in *ICCV*,
    2021, pp. 337–346.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] A. Gholami, K. Kwon, B. Wu, Z. Tai, X. Yue, P. H. Jin, S. Zhao, and K. Keutzer,
    “Squeezenext: Hardware-aware neural network design,” in *CVPR Workshops*, 2018,
    pp. 1638–1647.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] M. Sun, P. Zhao, Y. Wang, N. Chang, and X. Lin, “HSIM-DNN: hardware simulator
    for computation-, storage- and power-efficient deep neural networks,” in *GLSVLSI*,
    2019, pp. 81–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao,
    and Y. Lin, “Hw-nas-bench: Hardware-aware neural architecture search benchmark,”
    in *ICLR*, 2019, pp. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
