- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:44:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:44:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2208.10498] Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2208.10498] 快速、轻量且高效的深度学习模型设计自动化：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10498](https://ar5iv.labs.arxiv.org/html/2208.10498)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10498](https://ar5iv.labs.arxiv.org/html/2208.10498)
- en: 'Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速、轻量且高效的深度学习模型设计自动化：综述
- en: 'Dalin Zhang,  Kaixuan Chen,  Yan Zhao,  Bin Yang,  Lina Yao,  and Christian
    S. Jensen Dalin Zhang, Kaixuan Chen, Yan Zhao, Bin Yang, and Christian S. Jensen
    is with the Department of Computer Science, Aalborg University, Aalborg Øst 9220,
    Denmark (e-mail: dalinz@cs.aau.dk, kchen@cs.aau.dk, yanz@cs.aau.dk, byang@cs.aau.dk,
    and csj@cs.aau.dk).Lina Yao is with the School of Computer Science and Engineering,
    University of New South Wales, UNSW Sydney 2052, Australia (e-mail: lina.yao@unsw.edu.au).Manuscript
    received March 10, 2022'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 达林·张、开轩·陈、燕·赵、宾·杨、丽娜·姚和克里斯蒂安·S·詹森 达林·张、开轩·陈、燕·赵、宾·杨和克里斯蒂安·S·詹森均在丹麦奥尔堡大学计算机科学系，地址：Aalborg
    Øst 9220, Denmark（电子邮件：dalinz@cs.aau.dk、kchen@cs.aau.dk、yanz@cs.aau.dk、byang@cs.aau.dk
    和 csj@cs.aau.dk）。丽娜·姚在新南威尔士大学计算机科学与工程学院，地址：UNSW Sydney 2052, Australia（电子邮件：lina.yao@unsw.edu.au）。稿件收到于2022年3月10日
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning technologies have demonstrated remarkable effectiveness in a
    wide range of tasks, and deep learning holds the potential to advance a multitude
    of applications, including in edge computing, where deep models are deployed on
    edge devices to enable instant data processing and response. A key challenge is
    that while the application of deep models often incurs substantial memory and
    computational costs, edge devices typically offer only very limited storage and
    computational capabilities that may vary substantially across devices. These characteristics
    make it difficult to build deep learning solutions that unleash the potential
    of edge devices while complying with their constraints. A promising approach to
    addressing this challenge is to automate the design of effective deep learning
    models that are lightweight, require only a little storage, and incur only low
    computational overheads. This survey offers comprehensive coverage of studies
    of design automation techniques for deep learning models targeting edge computing.
    It offers an overview and comparison of key metrics that are used commonly to
    quantify the proficiency of models in terms of effectiveness, lightness, and computational
    costs. The survey then proceeds to cover three categories of the state-of-the-art
    of deep model design automation techniques: automated neural architecture search,
    automated model compression, and joint automated design and compression. Finally,
    the survey covers open issues and directions for future research.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术在广泛任务中展示了显著的效果，深度学习有潜力推动众多应用的发展，包括边缘计算，其中深度模型被部署在边缘设备上以实现即时的数据处理和响应。一个主要挑战是，虽然深度模型的应用通常会带来相当大的内存和计算成本，但边缘设备通常仅提供非常有限的存储和计算能力，这些能力在不同设备之间可能差异很大。这些特点使得构建能够充分发挥边缘设备潜力的深度学习解决方案，同时符合其限制条件变得困难。应对这一挑战的一个有前景的方法是自动化设计轻量级、仅需少量存储且计算开销低的有效深度学习模型。这项综述全面覆盖了针对边缘计算的深度学习模型设计自动化技术的研究。它提供了常用的关键指标的概述和比较，这些指标用于量化模型在有效性、轻量性和计算成本方面的能力。然后，综述覆盖了深度模型设计自动化技术的三类最新技术：自动神经架构搜索、自动模型压缩和联合自动设计与压缩。最后，综述涵盖了开放性问题和未来研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: deep learning, neural architecture search, lightweight model, model compression
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，神经架构搜索，轻量级模型，模型压缩
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: I-A Background
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 背景
- en: 'Deep learning has achieved state-of-the-art performance at a multitude of tasks
    and has affected people’s lives in myriad areas, including recommendation systems
    [[1](#bib.bib1)], natural language understanding [[2](#bib.bib2)], and biomedical
    engineering [[3](#bib.bib3)]. Deep learning frees researchers from manually designing
    purposeful feature representations of objects by introducing multi-layer neural
    architectures capable of automatic feature extraction. This enables researchers
    to work at a higher level of abstraction, focusing on architecture engineering
    rather than on feature engineering and model building. The neural architectures
    of deep models tend to be increasingly intricate and complex, thus requiring substantial
    hardware resources for deployment. This may not be a problem when a powerful server
    is available, but important settings occur where this is not the case:: 1) computing
    on mobile hardware, e.g., smartphones and tablet PCs; 2) computing on industrial
    hardware optimized for low deployment cost and low power consumption. Furthermore,
    although cloud computing may be available, it is often fundamentally unattractive
    to transfer data from edge devices to the cloud. On the one hand, such transfer
    may incur privacy, ownership, and consequent regulatory concerns, including for
    human-related data like audio and video data and utility consumption data [[4](#bib.bib4)];
    on the other hand, data transfer incurs substantial latency due to low bandwidth
    “last mile” connectivity. Last but not least, deployment of complex models is
    expensive, due to the cost of specialized hardware and energy consumption, and
    it is also bad for the environment due to the carbon footprint of producing the
    required electricity [[5](#bib.bib5), [6](#bib.bib6)].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多任务上实现了最先进的性能，并在推荐系统 [[1](#bib.bib1)]、自然语言理解 [[2](#bib.bib2)] 和生物医学工程
    [[3](#bib.bib3)] 等多个领域影响了人们的生活。深度学习通过引入能够自动特征提取的多层神经网络架构，解放了研究人员从手动设计有目的的特征表示中，从而使他们可以在更高的抽象层次上工作，专注于架构工程而不是特征工程和模型构建。深度模型的神经架构往往越来越复杂，因此需要大量的硬件资源进行部署。这在拥有强大服务器时可能不是问题，但在以下重要场景中则可能存在问题：1)
    在移动硬件上进行计算，例如智能手机和平板电脑；2) 在优化低部署成本和低功耗的工业硬件上进行计算。此外，尽管可能有云计算可用，但从边缘设备向云端传输数据通常从根本上是不吸引人的。一方面，这种传输可能会引发隐私、所有权和相关的监管问题，包括音频、视频数据和公用事业消费数据
    [[4](#bib.bib4)]；另一方面，由于“最后一公里”连接的低带宽，数据传输会产生大量延迟。最后但同样重要的是，复杂模型的部署成本昂贵，这包括专用硬件的费用和能源消耗，并且由于生产所需电力的碳足迹，对环境也不利
    [[5](#bib.bib5), [6](#bib.bib6)]。
- en: Fortunately, research has demonstrated that deep learning models generally have
    large numbers of redundant parameters and computations that contribute to their
    performance [[7](#bib.bib7), [8](#bib.bib8)], so there is considerable room for
    reducing redundancies without compromising model accuracy. Hence, it is highly
    desirable and completely possible to design simplified deep learning models with
    reduced computational complexity, thus achieving lighter weight and more efficient
    deep models¹¹1By efficient deep learning models we mean deep learning models with
    low memory usage or low inference cost or latency..
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，研究表明深度学习模型通常有大量冗余的参数和计算，这些冗余对模型的性能有贡献 [[7](#bib.bib7), [8](#bib.bib8)]，因此在不影响模型准确性的前提下，有很大的空间来减少这些冗余。因此，设计具有简化计算复杂度的深度学习模型是非常可取的，并且完全可能，从而实现更轻量、更高效的深度模型¹¹1高效的深度学习模型是指那些具有低内存使用、低推理成本或低延迟的深度学习模型。
- en: I-B Design of Efficient Deep Learning Models
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 高效深度学习模型的设计
- en: 'Research on building efficient deep learning models can be categorized into
    two categories: efficient network architecture design and model compression. In
    efficient network architecture design, the aim is to create compact neural modules
    and to connect these according to a carefully designed topology. The objective
    is to achieve efficient deep learning models with acceptable accuracy but small
    structures (low memory requirement) and low computational complexity (high speed).
    MobileNet [[9](#bib.bib9)] proposes an efficient network structure using a depthwise
    separable convolution module as the basic building block, and superior size, speed,
    and accuracy characteristics over a variety of computer vision tasks are documented.
    A second version, called MobileNetV2, incorporates a new basic building block,
    bottleneck depth-separable convolution with residuals/inverted residual [[10](#bib.bib10)].
    As a result, MobileNetV2 generally needs 30% fewer parameters, requires two times
    fewer operations and is about 30–40% faster on a Google Pixel phone while achieving
    higher accuracy than its predecessor. In ShuffleNetV1 [[11](#bib.bib11)], bottleneck-like
    structures with pointwise group convolutions and ”channel shuffle” operations
    are the basic building blocks that are used to achieve efficient neural structures.
    Like in the MobileNet case, ShuffleNet also has a second generation, called ShuffleNetV2
    [[12](#bib.bib12)]. Here a new “channel split” operation is introduced in order
    to further improve speed and accuracy. In addition, four practical guidelines
    for efficient network architecture design are provided.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关于构建高效深度学习模型的研究可以分为两类：高效网络架构设计和模型压缩。在高效网络架构设计中，目标是创建紧凑的神经模块，并根据精心设计的拓扑结构将这些模块连接起来。其目的是实现具有可接受精度但结构小（低内存需求）且计算复杂度低（高速）的高效深度学习模型。MobileNet
    [[9](#bib.bib9)] 提出了使用深度可分离卷积模块作为基本构建块的高效网络结构，并且在各种计算机视觉任务中表现出了优越的尺寸、速度和准确性特征。第二版，即
    MobileNetV2，引入了一种新的基本构建块，即带有残差/反向残差的瓶颈深度可分离卷积 [[10](#bib.bib10)]。因此，MobileNetV2
    通常需要少 30% 的参数，所需的操作次数减少了一半，并且在 Google Pixel 手机上运行速度快约 30-40%，同时比前代具有更高的准确性。在 ShuffleNetV1
    [[11](#bib.bib11)] 中，类似瓶颈的结构与逐点分组卷积和“通道洗牌”操作是实现高效神经结构的基本构建块。与 MobileNet 的情况类似，ShuffleNet
    也有第二代，称为 ShuffleNetV2 [[12](#bib.bib12)]。这里引入了新的“通道拆分”操作，以进一步提高速度和准确性。此外，还提供了四条高效网络架构设计的实际指南。
- en: In contrast to directly designing efficient architecture from a pool of basic
    building blocks, model compression aims at modifying a given neural model to reduce
    its memory and computational cost. Pruning is one such powerful technique that
    tries to remove unimportant components from a model [[13](#bib.bib13), [14](#bib.bib14)].
    It is flexible in that it is possible to remove layers, neurons, connections,
    or channels. While pruning shrinks a model by removing redundant parts, quantization
    aims to reduce the number of bits required to represent model parameters [[15](#bib.bib15)].
    Most processors use 32 bits or more to store the parameters of a deep model. However,
    research estimates that the human brain stores information in a discrete format
    that uses 4–7 bits [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]. Indeed,
    many efforts have been devoted to investigating using fewer bits to store model
    parameters to reduce memory and computational cost [[19](#bib.bib19), [20](#bib.bib20)].
    Other techniques like knowledge distillation [[21](#bib.bib21)] and tensor decomposition
    [[22](#bib.bib22)] are also popular and effective at compressing deep models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接从基本构建块池中设计高效架构不同，模型压缩旨在修改给定的神经模型，以减少其内存和计算成本。剪枝是一种强大的技术，试图从模型中移除不重要的组件 [[13](#bib.bib13),
    [14](#bib.bib14)]。它具有灵活性，可以移除层、神经元、连接或通道。虽然剪枝通过移除冗余部分来缩小模型，但量化旨在减少表示模型参数所需的位数
    [[15](#bib.bib15)]。大多数处理器使用 32 位或更多位来存储深度模型的参数。然而，研究估计人脑以离散格式存储信息，使用 4-7 位 [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18)]。确实，许多研究已致力于使用更少的位数来存储模型参数，以减少内存和计算成本 [[19](#bib.bib19),
    [20](#bib.bib20)]。其他技术如知识蒸馏 [[21](#bib.bib21)] 和张量分解 [[22](#bib.bib22)] 也在压缩深度模型方面非常流行和有效。
- en: '![Refer to caption](img/27bac87c51e16b3a7255e722adb0bc28.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/27bac87c51e16b3a7255e722adb0bc28.png)'
- en: 'Figure 1: Design Automation for Efficient Deep Learning Models. The automatic
    designer takes the hardware constraints into its search strategy to explore a
    predefined deep learning architecture design space which could be either an architecture
    space or a compression space; with the explored design choices, a new deep learning
    model from scratch or a compressed deep learning model is derived and deployed
    on the target device; the model’s performance including both accuracy and hardware
    consumption is finally estimated and fed into the automatic designer.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：高效深度学习模型的设计自动化。自动设计器将硬件约束纳入其搜索策略，以探索预定义的深度学习架构设计空间，这可能是一个架构空间或压缩空间；通过探索的设计选择，从头开始或压缩的深度学习模型被派生并部署到目标设备上；模型的性能，包括准确性和硬件消耗，最终被估计并反馈给自动设计器。
- en: I-C Design Automation for Efficient Deep Learning Models
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-C 高效深度学习模型的设计自动化
- en: Although remarkable progress has been achieved in building efficient deep learning
    models, initial proposals were heuristic rule-based and hand-tuned with inevitable
    limitations. First, building an efficient deep learning model still requires advanced
    prior knowledge and experience, making it difficult for beginners and even deep
    learning experts without domain knowledge to develop specialized models that meet
    given requirements. Second, as it is impossible to apply the same uniform model
    across diverse mobile platforms and tasks, enabling specializations that address
    such diversity is essential. Yet, it remains excessively time-consuming and inconvenient.
    Third, hand-crafted rules offer limited capabilities at utilizing hardware potentials
    fully, while satisfying the size and latency requirements. Different deep learning
    models can satisfy the same hardware constraints, and it is impractical to manually
    exhaust all possibilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在构建高效深度学习模型方面取得了显著进展，但最初的提案仍是启发式规则驱动和手工调优的，存在不可避免的局限性。首先，构建一个高效的深度学习模型仍然需要先进的先验知识和经验，这使得初学者甚至没有领域知识的深度学习专家难以开发出符合特定要求的专业模型。其次，由于不可能在各种移动平台和任务上应用相同的统一模型，启用能够应对这种多样性的专业化是至关重要的，但这仍然极其耗时且不便。第三，手工设计的规则在充分利用硬件潜力方面能力有限，同时满足大小和延迟要求。不同的深度学习模型可以满足相同的硬件约束，手动穷尽所有可能性是不切实际的。
- en: 'Observations such as the above have prompted a multitude of studies of design
    automation techniques for efficient deep learning models. Fig. [1](#S1.F1 "Figure
    1 ‣ I-B Design of Efficient Deep Learning Models ‣ I Introduction ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") shows a
    general automated design process. The design automation algorithm (i.e., Automatic
    Designer) applies a search strategy to find network architectures and compression
    in the predefined Deep Learning Architecture Design Space; next, a specific deep
    learning model is Derived through executing the identified operations and is then
    Deployed on target devices (e.g., CPU, GPU, or IoT devices); lastly, model performance
    metrics such as accuracy, latency, and memory use are estimated and provided to
    the Automatic Designer. In the next iteration, the Automatic Designer considers
    both the feedback and specialized constraints and takes a new design action to
    find a better model in the design space. This process is repeated until a satisfactory
    model is achieved.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '如上所述的观察促使了大量针对高效深度学习模型的设计自动化技术的研究。图[1](#S1.F1 "Figure 1 ‣ I-B Design of Efficient
    Deep Learning Models ‣ I Introduction ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")展示了一个通用的自动化设计过程。设计自动化算法（即自动设计器）应用搜索策略来寻找网络架构和在预定义的深度学习架构设计空间中的压缩；接着，通过执行识别的操作派生出特定的深度学习模型，并在目标设备（如CPU、GPU或IoT设备）上部署；最后，模型性能指标如准确性、延迟和内存使用情况被估计并提供给自动设计器。在下一次迭代中，自动设计器考虑反馈和专门的约束，并采取新的设计行动以在设计空间中寻找更好的模型。这个过程会重复进行，直到获得满意的模型。'
- en: Neural architecture search (NAS) aims to automate the design of neural networks
    that achieve the best possible accuracy. Next, more targeted studies that aim
    to automate the design of efficient neural networks build on generic NAS and involve
    the design of search spaces and the modification of the optimization objective
    from sole accuracy to both accuracy and efficiency. Cai et al. [[23](#bib.bib23)]
    incorporate model latency into the optimization goal of their binarized design
    automation framework (i.e., ProxylessNAS) by means of a differentiable loss. Due
    to targeting optimized inference latency directly, ProxylessNAS can achieve efficient
    neural architectures 1.83 times faster than MobileNetV2 with the same level of
    top-1 accuracy. Notably, the search space of ProxylessNAS is based on the inverted
    residual blocks of different convolution sizes, as proposed by MobleNetV2\. This
    implies that design automation is not only able to reduce human labor but even
    enables architectures that surpass handcrafted architectures.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索（NAS）旨在自动化设计神经网络，以实现最佳可能的准确性。接下来，旨在自动化设计高效神经网络的更具针对性的研究在通用 NAS 的基础上进行，涉及搜索空间的设计以及将优化目标从单一准确性修改为同时考虑准确性和效率。Cai
    等人 [[23](#bib.bib23)] 通过可微分损失将模型延迟纳入其二值化设计自动化框架（即 ProxylessNAS）的优化目标中。由于直接针对优化推理延迟，ProxylessNAS
    能够以比 MobileNetV2 快 1.83 倍的速度实现高效神经架构，同时保持相同水平的 top-1 准确性。值得注意的是，ProxylessNAS 的搜索空间基于
    MobleNetV2 提出的不同卷积尺寸的反向残差块。这意味着设计自动化不仅能够减少人工劳动，还能实现超越手工设计架构的架构。
- en: In addition to studies that target the direct design of efficient neural networks,
    other studies target the automated compression of deep neural networks [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)]. For example, Xiao et al. [[24](#bib.bib24)]
    design an automatic pruning approach that is based on learnable pruning indicators
    instead of pruning rules designed individually for specific architectures and
    datasets. This approach achieves superior compression performance on different
    widely-used neural models (e.g., AlexNet, ResNet, and MobileNet). Considering
    the progress in, and promise of, design automation for efficient deep learning
    models, it is a comprehensive and systematic survey is called for and holds the
    potential to accelerate future research.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接设计高效神经网络的研究，其他研究还关注深度神经网络的自动化压缩 [[24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]。例如，Xiao
    等人 [[24](#bib.bib24)] 设计了一种基于可学习剪枝指示器的自动剪枝方法，而不是为特定架构和数据集单独设计剪枝规则。这种方法在不同的广泛使用的神经模型（如
    AlexNet、ResNet 和 MobileNet）上实现了卓越的压缩性能。考虑到高效深度学习模型设计自动化的进展和前景，呼吁进行全面而系统的调查，并有望加速未来的研究。
- en: I-D Key Contributions
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-D 主要贡献
- en: 'To the best of our knowledge, this is the first survey of state-of-the-art
    design automation methods that target fast, lightweight, and effective deep learning
    models. The key contributions of the survey are summarized as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这是首个针对快速、轻量级和高效深度学习模型的最先进设计自动化方法的综述。该综述的主要贡献总结如下：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive review of design automation techniques targeting
    fast, lightweight, and effective deep learning models. In doing so, more than
    150 papers are covered, analyzed, and compared.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了一份针对快速、轻量级和高效深度学习模型的设计自动化技术的全面综述。在此过程中，涵盖、分析和比较了 150 多篇论文。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We propose a new taxonomy of deep design automation methods from the perspectives
    of how to design, i.e., by search (Section [III](#S3 "III Search for Efficient
    Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey")), by compression (Section [IV](#S4 "IV Automated
    Compression of Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")), or by joint search and compression
    (Section [V](#S5 "V Joint Automated Design Strategies ‣ Design Automation for
    Fast, Lightweight, and Effective Deep Learning Models: A Survey")); and by what
    to design, i.e., the search space, the search strategy, and the performance estimation
    strategy; and by what to compress, e.g., tensors, knowledge, and representation.
    The detailed categories provide convenience to the readers in obtaining an overview
    of the literature and identifying a direction of interest.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们从设计方法的角度提出了一种新的深度设计自动化方法的分类法，即通过搜索（第[III](#S3 "III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey")节）、压缩（第[IV](#S4 "IV Automated Compression of Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey")节）或联合搜索和压缩（第[V](#S5 "V Joint Automated Design Strategies ‣ Design
    Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey")节）进行设计；以及设计什么，即搜索空间、搜索策略和性能估计策略；以及压缩什么，例如张量、知识和表示。详细的分类为读者提供了获取文献概览和确定感兴趣方向的便利。'
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We summarize and compare the evaluation metrics that are used for both the obtained
    models and the design approaches. By means of the comparison, we emphasize the
    role of each metric and explicate the associated pros and cons.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结并比较了用于获得模型和设计方法的评估指标。通过比较，我们强调了每个指标的作用，并阐明了相关的优缺点。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss open issues and identify future directions on automated design and
    compression.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了开放问题并确定了自动化设计和压缩的未来方向。
- en: 'The remainder of the survey is organized as follows: Section [II](#S2 "II Evaluation
    Metrics ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey") summarizes the evaluation metrics of efficiency (i.e., speed
    and lightness) and effectiveness of deep learning models. Section [III](#S3 "III
    Search for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey") covers studies of searching for
    efficient deep models. Section [IV](#S4 "IV Automated Compression of Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey") then covers research on automated compression, and Section [V](#S5
    "V Joint Automated Design Strategies ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey") considers studies of joint automated
    search and compression. Section [VI](#S6 "VI Future Directions ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") presents
    research directions, and Section [VII](#S7 "VII Conclusion ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") concludes
    the survey.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '该调查的其余部分组织如下：第[II](#S2 "II Evaluation Metrics ‣ Design Automation for Fast,
    Lightweight, and Effective Deep Learning Models: A Survey")节总结了深度学习模型效率（即速度和轻量化）和有效性的评估指标。第[III](#S3
    "III Search for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")节涵盖了搜索高效深度模型的研究。第[IV](#S4 "IV Automated
    Compression of Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")节则涵盖了自动压缩的研究，第[V](#S5 "V Joint Automated
    Design Strategies ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey")节考虑了联合自动搜索和压缩的研究。第[VI](#S6 "VI Future Directions ‣
    Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A
    Survey")节介绍了研究方向，第[VII](#S7 "VII Conclusion ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")节总结了调查。'
- en: II Evaluation Metrics
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 评估指标
- en: 'Different evaluation metrics or objectives may lead to different or even opposite
    conclusions. Thus, it is indispensable to introduce and distinguish the relevant
    evaluation metrics before diving into the details of efficient models. In this
    section, we will introduce the evaluation metrics for measuring the efficiency
    and effectiveness of the obtained efficient models. These metrics are also critical
    to evaluating the effectiveness of a design automation approach. Furthermore,
    we introduce metrics for evaluating the cost of a design automation approach as
    well. We briefly summarize the characteristics including advantages and limitations
    of the commonly used evaluation metrics in Table [I](#S2.T1 "TABLE I ‣ II Evaluation
    Metrics ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '不同的评估指标或目标可能会导致不同甚至相反的结论。因此，在深入研究高效模型的细节之前，介绍和区分相关评估指标是不可或缺的。在本节中，我们将介绍用于衡量获得的高效模型的效率和有效性的评估指标。这些指标对于评估设计自动化方法的有效性也至关重要。此外，我们还介绍了评估设计自动化方法成本的指标。我们在表格[I](#S2.T1
    "TABLE I ‣ II Evaluation Metrics ‣ Design Automation for Fast, Lightweight, and
    Effective Deep Learning Models: A Survey")中简要总结了常用评估指标的特点，包括优点和局限性。'
- en: 'TABLE I: Evaluation Metrics for Measuring the Efficiency and Effectiveness
    of Both the Obtained Efficient Model and the Design Automation Approach'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 用于测量所获得的高效模型和设计自动化方法的效率和有效性的评估指标'
- en: '|  | Metrics | Characteristics |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | 指标 | 特征 |'
- en: '| Advantages | Limitations |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 局限性 |'
- en: '| Device-agnostic Efficiency Evaluation Metrics | FLOPs | • each to obtain
    • coarse approximation of latency | • inconsistent definitions • omitting the
    degree of computational parallelism • omitting the memory access cost • omitting
    the implementation library |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 设备无关效率评估指标 | FLOPs | • 每个获取 • 延迟的粗略近似 | • 定义不一致 • 忽略计算并行度 • 忽略内存访问成本 • 忽略实现库
    |'
- en: '| Number of Parameters | • easy to obtain • precise when measuring storage
    requirement | • omitting the computation caching for measuring memory • unable
    to reflect peak memory usage |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 参数数量 | • 容易获得 • 测量存储需求时精确 | • 遗漏计算缓存的内存测量 • 无法反映峰值内存使用 |'
- en: '| Device-aware Efficiency Evaluation Metrics | Latency | • the real criterion
    that we care about • obtain through implementation on real hardware or prediction
    models • depend on different hardware platforms and implementation libraries |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 设备感知效率评估指标 | 延迟 | • 我们关心的真实标准 • 通过在实际硬件上实现或预测模型获得 • 依赖于不同的硬件平台和实现库 |'
- en: '| Peak Memory Usage | • the real criterion that we care about • obtain through
    implementation on real hardware or prediction models • reflect the peak usage
    that really matters |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 峰值内存使用 | • 我们关心的真实标准 • 通过在实际硬件上实现或预测模型获得 • 反映真正重要的峰值使用 |'
- en: '| Effectiveness Evaluation Metrics | Accuracy/mAP/mIOU etc. | • diverse and
    dependent on the targeting tasks • identical to those for evaluating normal deep
    learning models |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 效果评估指标 | 准确度/mAP/mIOU 等 | • 多样且依赖于目标任务 • 与评估普通深度学习模型的指标相同 |'
- en: '| Design Automation Cost Metrics | GPU Hours | • used to evaluate the speed
    of a design automation method • depend on the number of GPUs used |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 设计自动化成本指标 | GPU 小时 | • 用于评估设计自动化方法的速度 • 依赖于使用的 GPU 数量 |'
- en: '| GPU Memory | • used to evaluate the memory requirement of a design automation
    • grows linearly w.r.t. the size of the candidate set |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| GPU 内存 | • 用于评估设计自动化的内存需求 • 随候选集大小线性增长 |'
- en: II-A Device-agnostic Evaluation Metrics
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 设备无关评估指标
- en: The device-agnostic metrics can be calculated directly from the model architecture
    without real-world implementation on hardware. These metrics do not essentially
    reflect a model’s real performance that we care about [[12](#bib.bib12), [27](#bib.bib27),
    [28](#bib.bib28)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 设备无关的指标可以直接从模型架构计算，而无需在硬件上进行实际实现。这些指标本质上无法反映我们关心的模型真实性能 [[12](#bib.bib12), [27](#bib.bib27),
    [28](#bib.bib28)]。
- en: II-A1 FLoating-point OPerations (FLOPs)
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 浮点运算（FLOPs）
- en: 'FLOPs is generally defined as the number of floating-point multiplication-add
    operations in a model for approximating the latency/speed or computation complexity
    [[11](#bib.bib11), [12](#bib.bib12), [29](#bib.bib29)]. There are also other commonly
    used analogous metrics, such as the number of multiply-add operations (MAdds)
    and the number of multiply-accumulate operations (MACs) [[30](#bib.bib30), [27](#bib.bib27),
    [31](#bib.bib31)]. However, one contention remains regarding the definition: whether
    multiplication-add should be considered as one or two operations. Some researchers
    argue that in many recent deep learning models, convolutions are bias-free and
    it makes sense to count multiplication and add as separate FLOPs [[32](#bib.bib32)].
    Moreover, some non-multiplication or non-add operations require FLOPs in some
    implementations as well, such as an activation layer [[33](#bib.bib33), [27](#bib.bib27)].
    Whether such operations should be counted into total FLOPs is also a dispute.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: FLOPs通常定义为模型中浮点乘加操作的数量，用于近似延迟/速度或计算复杂度[[11](#bib.bib11)，[12](#bib.bib12)，[29](#bib.bib29)]。还有其他常用的类似指标，如乘加操作数量（MAdds）和乘累加操作数量（MACs）[[30](#bib.bib30)，[27](#bib.bib27)，[31](#bib.bib31)]。然而，对于定义仍然存在一个争议：乘法加法应被视为一个还是两个操作。一些研究人员认为，在许多最近的深度学习模型中，卷积是无偏的，因此将乘法和加法视为独立的FLOPs是有意义的[[32](#bib.bib32)]。此外，一些非乘法或非加法操作在某些实现中也需要FLOPs，例如激活层[[33](#bib.bib33)，[27](#bib.bib27)]。是否应将这些操作计算到总FLOPs中也是一个争论点。
- en: In addition to the inconsistency of the definition, there exist three main issues
    that induce the discrepancy between FLOPs and real latency. First, counting only
    FLOPs ignores some decisive factors that affect latency remarkably. One such factor
    is parallelism. With the same FLOPs, a model with a high degree of parallelism
    may be much faster than another model with a low degree of parallelism [[34](#bib.bib34),
    [35](#bib.bib35)]. Another important factor arises from the memory access cost.
    Since the on-device RAM is usually limited, data cannot be entirely loaded at
    one time and thus reading data from external memory is required. It has an increasing
    impact on the latency as the computation unit is getting stronger recently, thus
    becoming the bottleneck for latency. This intrinsic factor should not be simply
    neglected. Third, the implementation library of a deep learning model significantly
    influences its latency as well. For example, NVIDIA’s cuDNN library provides different
    implementations of a convolution operation [[36](#bib.bib36)] that clearly require
    different amounts of FLOPs although the network architecture is identical. Some
    works also found that a smaller number of FLOPs could be even slower due to the
    library abstractions [[12](#bib.bib12), [13](#bib.bib13)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定义的不一致之外，还有三个主要问题导致FLOPs与实际延迟之间的差异。首先，仅计算FLOPs忽略了一些显著影响延迟的决定性因素。其中一个因素是并行性。在相同的FLOPs下，高度并行的模型可能比低并行度的模型快得多[[34](#bib.bib34)，[35](#bib.bib35)]。另一个重要因素是内存访问成本。由于设备上的RAM通常有限，数据不能一次性完全加载，因此需要从外部内存读取数据。随着计算单元的日益强大，这种影响越来越大，从而成为延迟的瓶颈。这一内在因素不应被简单忽略。第三，深度学习模型的实现库也显著影响其延迟。例如，NVIDIA的cuDNN库提供了不同的卷积操作实现[[36](#bib.bib36)]，虽然网络架构相同，但这些实现明显需要不同数量的FLOPs。一些研究还发现，由于库的抽象，FLOPs较少的情况有时反而更慢[[12](#bib.bib12)，[13](#bib.bib13)]。
- en: II-A2 Number of Parameters
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 参数数量
- en: It is usually required to fit all parameters of a neural network within on-chip
    memory to execute the model fast [[37](#bib.bib37)]. Thus, the number of parameters
    mainly constrains the memory requirement of a deep learning model. Although some
    mobile devices like smartphones have abundant memory, there are still a few mobile
    devices that have particularly scarce memory, such as microcontrollers that typically
    have 10’s-100’s of KB [[38](#bib.bib38)]. These memory-scarce devices are in demand
    in many fields due to their low prices. Thus, it is desirable to design a small-sized
    model that can fit into “tiny” memory and the number of parameters is a common
    device-agnostic metric for evaluating such a requirement. Nevertheless, the number
    of parameters only accounts for a portion of memory usage; input data, computation
    caching (i.e., intermediate tensors produced at runtime), and network structure
    information take over a relatively larger portion of memory [[39](#bib.bib39),
    [40](#bib.bib40)]. In some extreme scenarios, only computation caching and active
    parameters (i.e., the parameters used for current computation) occupy memory.
    Some researchers propose to optimize the in-memory computation caching to reduce
    memory consumption but their performance largely depends on the network structure
    [[41](#bib.bib41)]. Thus, a model with a larger number of parameters is not certainly
    more memory-hungry than a model with a smaller number of parameters. On the other
    hand, model parameters account for a major component of storage/external memory
    (e.g., FLASH memory) usage.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通常要求将神经网络的所有参数适配到片上内存中，以便快速执行模型[[37](#bib.bib37)]。因此，参数数量主要限制了深度学习模型的内存需求。尽管像智能手机这样的移动设备拥有丰富的内存，但仍有一些移动设备的内存特别稀缺，如通常只有10到100KB内存的微控制器[[38](#bib.bib38)]。这些内存稀缺的设备因其低价格在许多领域有需求。因此，设计一个能够适配“微小”内存的小型模型是理想的，参数数量是评估这种需求的常见设备无关度量标准。然而，参数数量仅占内存使用的一部分；输入数据、计算缓存（即运行时产生的中间张量）和网络结构信息占据了相对较大的内存部分[[39](#bib.bib39),
    [40](#bib.bib40)]。在一些极端场景中，仅计算缓存和活动参数（即用于当前计算的参数）占用内存。一些研究人员提议优化内存中的计算缓存以减少内存消耗，但它们的性能在很大程度上依赖于网络结构[[41](#bib.bib41)]。因此，参数更多的模型不一定比参数较少的模型更耗费内存。另一方面，模型参数占存储/外部内存（例如，FLASH内存）使用的主要部分。
- en: II-B Device-aware Efficiency Evaluation Metrics
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 设备感知效率评估指标
- en: Unlike device-agnostic metrics, device-aware metrics can reflect the computational
    cost that we really care about. They can be collected on real and target hardware
    platforms or approximated.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与设备无关的指标不同，设备感知指标能够反映我们真正关心的计算成本。它们可以在真实的目标硬件平台上收集或进行近似评估。
- en: II-B1 Latency
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 延迟
- en: The latency is used to evaluate the running speed of a deep model making inferences.
    It is usually measured in the form of running time per inference (e.g., millisecond)
    [[42](#bib.bib42)] or inferences per unit time (e.g., batches/s or images/s) [[12](#bib.bib12)].
    In practice, the precise latency is an average value computed on a large batch
    or several bathes [[42](#bib.bib42)]. Some works collect this information through
    implementations on real devices including GPUs, TPUs, and CPUs [[25](#bib.bib25),
    [12](#bib.bib12), [43](#bib.bib43), [42](#bib.bib42)]. It should be noted that
    the reliability is unknown when evaluating a deep model’s latency not on its target
    mobile devices (e.g., smartphone CPUs) but on non-mobile devices (e.g., GPUs).
    In addition to directly measuring latency, some researchers try to approximate
    it [[23](#bib.bib23), [44](#bib.bib44)]. The lookup table is a latency approximation
    method that enumerates all the possible layers that a family of models can have
    along with the latency of each of the layers [[44](#bib.bib44)]. It is hardware
    and model family-specific and requires a significant amount of time to maintain
    a large and dynamic database. In addition, this simple summation of the latency
    of an individual does not take memory access cost and parallelism into consideration,
    and thus shows low precision. A latency prediction model is another latency approximation
    method that models the network latency as a function of network structures and/or
    hardware parameters [[23](#bib.bib23), [45](#bib.bib45), [44](#bib.bib44)]. This
    approach can make latency differentiable to be directly involved in an objective
    function for gradient-based optimization [[23](#bib.bib23)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟用于评估深度模型推理的运行速度。通常以每次推理的运行时间（例如，毫秒）[[42](#bib.bib42)] 或每单位时间的推理次数（例如，批次/s
    或图像/s）[[12](#bib.bib12)] 来衡量。实际中，精确的延迟是通过在大批量或多个批量上计算的平均值[[42](#bib.bib42)]。一些研究通过在实际设备上（包括GPU、TPU和CPU）实现来收集这些信息[[25](#bib.bib25),
    [12](#bib.bib12), [43](#bib.bib43), [42](#bib.bib42)]。需要注意的是，当在非目标移动设备（例如，智能手机CPU）而不是移动设备上（例如，GPU）评估深度模型的延迟时，其可靠性尚不可知。除了直接测量延迟外，一些研究人员还尝试进行近似[[23](#bib.bib23),
    [44](#bib.bib44)]。查找表是一种延迟近似方法，通过列举一系列模型可以具有的所有可能层以及每层的延迟[[44](#bib.bib44)]。这种方法依赖于硬件和模型系列特定，维护一个大且动态的数据库需要大量时间。此外，这种对个体延迟的简单求和没有考虑内存访问成本和并行性，因此精度较低。延迟预测模型是另一种延迟近似方法，它将网络延迟建模为网络结构和/或硬件参数的函数[[23](#bib.bib23),
    [45](#bib.bib45), [44](#bib.bib44)]。这种方法可以使延迟变得可微分，从而直接参与梯度优化的目标函数[[23](#bib.bib23)]。
- en: II-B2 Memory Usage
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 内存使用
- en: Different from latency where we care about the total inference time of a sample/batch,
    the peak memory usage during inference is our major concern [[37](#bib.bib37),
    [46](#bib.bib46)]. As long as the peak memory usage is lower than the memory capacity,
    a model is able to run on the device. It is not necessary to keep memory usage
    as low as possible. Extreme scenarios with maximal memory saving are considered.
    The peak memory usage is dominated by the intermediate tensors (so-called activation
    metrics), so a small model (i.e., a small number of model parameters) doesn’t
    guarantee a low peak memory usage. For example, at similar ImageNet accuracy (70%),
    even though MobileNetV2 [[10](#bib.bib10)] reduces its model size by 4.6$\times$
    compared to ResNet-18 [[47](#bib.bib47)], the peak memory requirement increases
    by 1.8$\times$ [[46](#bib.bib46)]. Thus, it is highly recommended to evaluate
    peak memory usage when targeting a device with quite constrained memory resources.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们关心的样本/批次的总推理时间不同，推理过程中的峰值内存使用是我们主要关注的问题[[37](#bib.bib37), [46](#bib.bib46)]。只要峰值内存使用低于内存容量，模型就可以在设备上运行。没有必要将内存使用保持在尽可能低的水平。考虑到极端的最大内存节省场景。峰值内存使用由中间张量（即所谓的激活指标）主导，因此一个小模型（即，较少的模型参数）并不保证低峰值内存使用。例如，在类似的ImageNet准确率（70%）下，即使MobileNetV2
    [[10](#bib.bib10)] 相比ResNet-18 [[47](#bib.bib47)] 将模型大小减少了4.6$\times$，峰值内存需求却增加了1.8$\times$
    [[46](#bib.bib46)]。因此，强烈建议在针对内存资源非常有限的设备时评估峰值内存使用。
- en: In addition to the peak memory usage, which considers the on-chip memory, storage/external-memory
    usage is also an essential evaluation metric. It mainly restricts the model size
    of which model parameters occupy the largest proportion. Therefore, the bit-precision
    of parameters has a crucial impact on the external-memory usage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了考虑片上内存的峰值内存使用外，存储/外部内存使用也是一个重要的评估指标。它主要限制了模型的大小，而模型参数占据了最大比例。因此，参数的位精度对外部内存使用有着重要影响。
- en: II-C Effectiveness Evaluation Metrics
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 效果评估指标
- en: 'The metrics for evaluating the effectiveness of an efficient model are quite
    diverse and mainly dependent on the targeting tasks. Vision tasks, such as image
    recognition, object detection, and semantic segmentation, are such commonly used
    benchmark tasks [[12](#bib.bib12), [10](#bib.bib10), [48](#bib.bib48)]. Different
    tasks have different evaluation metrics: top-1 or top 5 accuracy for image recognition
    [[48](#bib.bib48)], mAP for object detection [[48](#bib.bib48)], and mIOU for
    semantic segmentation [[10](#bib.bib10)]. In addition to vision tasks, audio tasks,
    such as keyword spotting, are leveraged as an evaluation task [[46](#bib.bib46)].
    However, accuracy is also used as the evaluation metric in this case.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 评估高效模型效果的指标种类繁多，主要依赖于目标任务。视觉任务，如图像识别、物体检测和语义分割，是常用的基准任务[[12](#bib.bib12), [10](#bib.bib10),
    [48](#bib.bib48)]。不同任务有不同的评估指标：图像识别的top-1或top-5准确率[[48](#bib.bib48)]，物体检测的mAP[[48](#bib.bib48)]，以及语义分割的mIOU[[10](#bib.bib10)]。除了视觉任务，音频任务，如关键词检测，也被用作评估任务[[46](#bib.bib46)]。然而，在这种情况下，准确率也作为评估指标。
- en: II-D Design Automation Cost Metrics
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 设计自动化成本指标
- en: As the price of freeing human efforts, design automation normally demands an
    excessive computational cost that prohibits its wide deployment.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解放人力成本的代价，设计自动化通常需要过高的计算成本，从而限制了其广泛应用。
- en: II-D1 GPU Hours
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D1 GPU小时
- en: 'GPU hours/days are metrics used to evaluate the time cost of a design automation
    method especially a NAS-based method [[49](#bib.bib49), [50](#bib.bib50)]. The
    GPU days can be defined as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: GPU小时/天是评估设计自动化方法，特别是基于NAS的方法的时间成本的指标[[49](#bib.bib49), [50](#bib.bib50)]。GPU天可以定义为：
- en: '|  | $\text{GPU days}=N\times t,$ |  | (1) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{GPU days}=N\times t,$ |  | (1) |'
- en: where $N$ denotes the number of GPUs, and $t$ denotes the number of days that
    are used for searching [[51](#bib.bib51)]. GPU hours have a similar definition.
    At the early stage, it requires several or even tens of days for searching [[52](#bib.bib52),
    [49](#bib.bib49)], while currently researchers have pushed the time to the magnitude
    of multiple hours [[53](#bib.bib53)].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N$表示GPU的数量，$t$表示用于搜索的天数[[51](#bib.bib51)]。GPU小时具有类似的定义。在早期阶段，搜索需要几天甚至几十天，而目前研究人员已将时间缩短至数小时的量级[[52](#bib.bib52),
    [49](#bib.bib49)]。
- en: II-D2 GPU Memory
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D2 GPU内存
- en: Although differentiable neural architecture search has reduced the cost of GPU
    hours considerably, it suffers from an intensive GPU memory cost. The consumption
    of GPU memory depends on the size of the candidate set for searching. Specifically,
    the required memory grows linearly w.r.t. the number of choices in a candidate
    set [[54](#bib.bib54)]. This issue restricts the search space size that prevents
    the capability of discovering novel and strong models. Some works have targeted
    this issue and achieved impressive progress [[23](#bib.bib23)]. Thus, it is important
    to involve GPU memory for a thorough evaluation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可微分神经架构搜索显著降低了GPU小时的成本，但它仍然面临高GPU内存成本的问题。GPU内存的消耗依赖于候选集的大小。具体而言，所需内存随着候选集中选择数量的增加而线性增长[[54](#bib.bib54)]。这个问题限制了搜索空间的大小，妨碍了发现新颖而强大的模型的能力。一些研究已经针对这一问题取得了显著进展[[23](#bib.bib23)]。因此，涉及GPU内存以进行彻底评估是重要的。
- en: III Search for Efficient Deep Learning Models
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 寻找高效深度学习模型
- en: 'Driven by the growing demand for mobile applications, efficient deep learning
    models have gained explosive attention. A tremendous number of studies have been
    proposed to explore manually designed efficient neural architectures or modules
    and achieved impressive progress [[29](#bib.bib29), [12](#bib.bib12), [10](#bib.bib10),
    [55](#bib.bib55), [56](#bib.bib56)]. Though the notable success, it is challenging
    for human engineers to heuristically exhaust the design space to trade off accuracy
    and hardware constraints. Hardware-aware neural architecture search plays an influential
    role in advancing this field as it automates the design process to find an optimal
    solution. Similar to regular NAS [[57](#bib.bib57)], the hardware-aware NAS also
    has three components, i.e., search space, search strategy, and performance estimation
    strategy, but with additional freedom or constraints (Fig. [1](#S1.F1 "Figure
    1 ‣ I-B Design of Efficient Deep Learning Models ‣ I Introduction ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") left bottom).
    In this section, we review recent achievements from these three aspects and summarize
    the main results in TABLE [II](#S3.T2 "TABLE II ‣ III-C2 Performance Predictor
    ‣ III-C Performance Estimation Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 受到对移动应用日益增长的需求的推动，高效深度学习模型获得了极大的关注。大量研究提出了手动设计的高效神经架构或模块，并取得了令人印象深刻的进展[[29](#bib.bib29),
    [12](#bib.bib12), [10](#bib.bib10), [55](#bib.bib55), [56](#bib.bib56)]。尽管取得了显著成功，但对于人工工程师来说，要在准确性和硬件限制之间权衡设计空间是具有挑战性的。硬件感知的神经架构搜索在推动这一领域的发展中发挥了重要作用，因为它自动化了设计过程，以找到最佳解决方案。与常规NAS[[57](#bib.bib57)]类似，硬件感知NAS也有三个组成部分，即搜索空间、搜索策略和性能评估策略，但具有额外的自由度或约束（图[1](#S1.F1
    "图 1 ‣ I-B 高效深度学习模型的设计 ‣ I 引言 ‣ 快速、轻量化且有效的深度学习模型的设计自动化：综述")左下角）。在本节中，我们回顾了这三个方面的近期成果，并在表[II](#S3.T2
    "表 II ‣ III-C2 性能预测器 ‣ III-C 性能评估策略 ‣ III 高效深度学习模型的搜索 ‣ 快速、轻量化且有效的深度学习模型的设计自动化：综述")中总结了主要结果。
- en: III-A Efficient Search Space
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 高效搜索空间
- en: 'A search space is the basis of NAS and determines what architectures NAS can
    discover in principle and their performance upper-limit [[57](#bib.bib57)]. A
    well-defined search space can not only accelerate the search process but also
    promote the searched model’s performance [[49](#bib.bib49), [58](#bib.bib58)].
    Since some manually explored efficient neural architectures have achieved considerable
    advances, it is an intuitive yet practical idea to construct a search space with
    the heuristics of these hand-crafted efficient structures, and leverage the NAS
    technology to automatically discover novel efficient models upon this search space.
    There are usually two components that define a search space: (i) operators that
    each layer executes, and (ii) a backbone that decides the topological connections
    of these layers.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间是NAS的基础，决定了NAS在原则上可以发现哪些架构及其性能上限[[57](#bib.bib57)]。一个明确定义的搜索空间不仅可以加速搜索过程，还能提高搜索模型的性能[[49](#bib.bib49),
    [58](#bib.bib58)]。由于一些手动探索的高效神经架构已经取得了显著的进展，基于这些手工制作的高效结构的启发来构建搜索空间，并利用NAS技术在此搜索空间上自动发现新颖高效模型，是一个直观且实用的想法。定义搜索空间通常有两个组成部分：（i）每层执行的操作符，和（ii）决定这些层的拓扑连接的骨干网络。
- en: '![Refer to caption](img/3c3f4693cf41dd2c7211eac76a56be5c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3c3f4693cf41dd2c7211eac76a56be5c.png)'
- en: (a) DSConv
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (a) DSConv
- en: '![Refer to caption](img/3687b427cd1f1df1a5632d37361331fb.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3687b427cd1f1df1a5632d37361331fb.png)'
- en: (b) MBConv
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MBConv
- en: '![Refer to caption](img/29be66dd4c821b387af03911937f87b3.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/29be66dd4c821b387af03911937f87b3.png)'
- en: (c) MBConv+SE
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (c) MBConv+SE
- en: '![Refer to caption](img/471d66b894a27d6e5e631326262aad3e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/471d66b894a27d6e5e631326262aad3e.png)'
- en: (d) GConv+channel shuffle
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (d) GConv+通道洗牌
- en: 'Figure 2: Schematics of efficient operators. (a) DSConv: depthwise separable
    convolution; (b) MBConv: mobile inverted bottleneck convolution; (c) MBConv+SE:
    MBConv with the squeeze and excitation module; (d) GConv+channel shuffle: group
    convolution with channel shuffle.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：高效操作符的示意图。 (a) DSConv: 深度可分离卷积；(b) MBConv: 移动反向瓶颈卷积；(c) MBConv+SE: 带有压缩和激励模块的MBConv；(d)
    GConv+通道洗牌: 带有通道洗牌的组卷积。'
- en: III-A1 Operators
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 操作符
- en: Operators, such as convolutional layers or fully connected layers, are basic
    building components for a deep learning model. A simple way to build a deep learning
    model is to directly stack several operators, such as VGG Net [[59](#bib.bib59)].
    Other work connects several neural network layers to construct a motif (e.g.,
    a residual block) and builds a model by repeating and arbitrarily connecting these
    motifs [[60](#bib.bib60), [47](#bib.bib47), [57](#bib.bib57)]. In this paper,
    we use the term ”operator” to also represent a motif, which is usually used as
    a whole and regarded as a basic component of an end-to-end model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层或全连接层等操作符是深度学习模型的基本构建组件。构建深度学习模型的一种简单方法是直接堆叠几个操作符，如 VGG Net [[59](#bib.bib59)]。其他工作则连接几个神经网络层来构建一个模块（例如残差块），通过重复和任意连接这些模块来构建模型
    [[60](#bib.bib60), [47](#bib.bib47), [57](#bib.bib57)]。在本文中，我们使用“操作符”一词来表示一个模块，它通常作为一个整体使用，被视为端到端模型的基本组件。
- en: 'As early-developed and widely-demonstrated neural architectures, the convolutional
    layer and its variants are the dominant operators of many deep learning models,
    especially those in the computer vision area. A standard convolution can be denoted
    as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作为早期开发并广泛展示的神经网络结构，卷积层及其变体是许多深度学习模型中的主要操作符，特别是在计算机视觉领域。标准卷积可以表示为：
- en: '|  | $\text{Conv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{m,n,k}^{M,N,K}{\textbf{w}}_{(m,\;n,\;k)}\cdot\bm{x}_{(i+m,\;j+n,\;k)},$
    |  | (2) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Conv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{m,n,k}^{M,N,K}{\textbf{w}}_{(m,\;n,\;k)}\cdot\bm{x}_{(i+m,\;j+n,\;k)},$
    |  | (2) |'
- en: where W is the convolution kernel weight, $\bm{X}$ is the input feature maps
    to a convolution layer, $(i,j)$ is the coordinate of an output feature map, and
    $(m,n,k)$ is the coordinate of the convolutional kernel. NSGA-Net [[61](#bib.bib61)]
    utilizes standard convolutions as the operator and devotes to automatically determining
    their connections in a block-based manner. As it only seeks to optimize the connection
    of operators, limited efficiency is gained. Scheidegger et al. [[62](#bib.bib62)]
    and Lu et al. [[63](#bib.bib63)] also consider standard convolutions but, instead
    of optimizing the connections, they allow to search the convolution hyperparameters
    such as the filter size, the stride and the number of filters.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 W 是卷积核权重，$\bm{X}$ 是卷积层的输入特征图，$(i,j)$ 是输出特征图的坐标，$(m,n,k)$ 是卷积核的坐标。NSGA-Net
    [[61](#bib.bib61)] 利用标准卷积作为操作符，并致力于以块状方式自动确定其连接。由于它仅寻求优化操作符的连接，效率有限。Scheidegger
    等人 [[62](#bib.bib62)] 和 Lu 等人 [[63](#bib.bib63)] 也考虑了标准卷积，但它们不是优化连接，而是允许搜索卷积超参数，如滤波器大小、步幅和滤波器数量。
- en: 'In addition to the standard convolutions, extensive manually-designed variants
    have been demonstrated both efficient and effective. Therefore, it is intuitive
    to employ these architectures to construct an efficient search space. The depthwise
    separable convolution (DSConv), which can significantly reduce the number of parameters
    and computation and works as the primary module in MobileNet [[9](#bib.bib9)],
    is such a widely used efficient operator. As shown in Fig [2a](#S3.F2.sf1 "In
    Figure 2 ‣ III-A Efficient Search Space ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey"), a DSConv block is made up of two components: a depthwise convolution
    and a pointwise convolution. The depthwise convolutions (DWConv) applies a single
    filter per each input feature map (input depth) for spatial filtering:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准卷积，大量手工设计的变体已被证明既高效又有效。因此，利用这些架构构建高效的搜索空间是直观的。深度可分离卷积（DSConv）可以显著减少参数数量和计算量，并作为
    MobileNet [[9](#bib.bib9)] 中的主要模块，是一种广泛使用的高效操作符。如图 [2a](#S3.F2.sf1 "图 2 ‣ III-A
    高效搜索空间 ‣ III 搜索高效深度学习模型 ‣ 设计自动化以实现快速、轻量和有效的深度学习模型：综述") 所示，DSConv 块由两个组件组成：深度卷积和点卷积。深度卷积（DWConv）对每个输入特征图（输入深度）应用单个滤波器进行空间滤波：
- en: '|  | $\text{DWConv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{m,n}^{M,N}{\textbf{w}}_{(m,\;n)}\cdot\bm{x}_{(i+m,\;j+n)}.$
    |  | (3) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{DWConv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{m,n}^{M,N}{\textbf{w}}_{(m,\;n)}\cdot\bm{x}_{(i+m,\;j+n)}.$
    |  | (3) |'
- en: 'The pointwise convolution (PWConv), a simple $1\times 1$ convolution, is used
    to create a linear combination of the output of the depthwise convolutions for
    feature fusion:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 点卷积（PWConv），一种简单的 $1\times 1$ 卷积，用于创建深度卷积输出的线性组合以进行特征融合：
- en: '|  | $\text{PWConv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{k}^{K}{\textbf{w}}_{k}\cdot\bm{x}_{(i,\;j)}.$
    |  | (4) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{PWConv}(\textbf{W},\bm{X})_{(i,j)}=\sum_{k}^{K}{\textbf{w}}_{k}\cdot\bm{x}_{(i,\;j)}.$
    |  | (4) |'
- en: 'Thus, by combining DWConv and PWConv, the DSConv is denoted as:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过结合DWConv和PWConv，DSConv被表示为：
- en: '|  |  | $\displaystyle\text{DSConv}(\textbf{W}_{p},\textbf{W}_{d},\bm{X})_{(i,j)}=$
    |  | (5) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{DSConv}(\textbf{W}_{p},\textbf{W}_{d},\bm{X})_{(i,j)}=$
    |  | (5) |'
- en: '|  |  | $\displaystyle\;\;\;\;\;\;\;\;\;\;\text{PWConv}(\textbf{W}_{p},\text{DWConv}(\textbf{W}_{d},\bm{X})_{(i,j)})_{(i,j)}.$
    |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\;\;\;\;\;\;\;\;\;\;\text{PWConv}(\textbf{W}_{p},\text{DWConv}(\textbf{W}_{d},\bm{X})_{(i,j)})_{(i,j)}.$
    |  |'
- en: LEMONADE [[64](#bib.bib64)] adopts both DSConvs and standard convolutions as
    the basic operators of its search space and supports increasing the number of
    filters and pruning filters to search efficient neural architectures. RENA [[65](#bib.bib65)]
    also includes DSConvs but allows to automatically decide not only their hyperparameters
    but also whether they should be used in each layer. In addition to the standard
    DSConv, ProxylessNAS [[23](#bib.bib23)] includes a variant, namely dilated depthwise
    separable convolution, in its search space as well. The search engine can choose
    between the standard one and the variant.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: LEMONADE [[64](#bib.bib64)] 采用了DSConvs和标准卷积作为其搜索空间的基本操作符，并支持增加滤波器数量和剪枝滤波器以搜索高效的神经网络架构。RENA
    [[65](#bib.bib65)] 也包含DSConvs，但允许自动决定不仅其超参数，还决定它们是否在每一层中使用。除了标准DSConv外，ProxylessNAS
    [[23](#bib.bib23)] 还在其搜索空间中包括了一种变体，即膨胀深度可分离卷积。搜索引擎可以在标准版本和变体之间进行选择。
- en: 'In MobileNetV2, a more advanced mobile convolution block, mobile inverted bottleneck
    convolutions (MBConv), is proposed [[10](#bib.bib10)] and soon becomes a popular
    operator in favour of an efficient search space. Fig. [2b](#S3.F2.sf2 "In Figure
    2 ‣ III-A Efficient Search Space ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey") illustrates the structure of MBConv. It is made up of three convolutions
    and one residual connection. First, a PWConv is applied to expand the input feature
    map to a higher-dimensional space so that non-linear activations (ReLU6) can better
    extract information. Then, a depthwise convolution is performed with $3\times
    3$ kernels and ReLU6 activations to achieve spatial filtering of the higher-dimensional
    feature maps. Furthermore, the spatially-filtered feature maps are projected back
    to a low-dimensional space with another pointwise convolution. Since the low-dimensional
    projection results in loss of information, linear activation is used after pointwise
    convolution. Finally, an optional residual connection (depending on whether the
    stride of the depthwise layer is 1) is added to combine the original input and
    the output of the low-dimensional projection. Note that the last two convolutions
    (depthwise convolution and pointwise convolution) are essentially a DSConv with
    dimension reduction. There are multiple works on purely using MBConv to construct
    an efficient search space but with different backbones or search strategies [[66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)]. Xiong et al. [[69](#bib.bib69)] argue that
    DSConv is inexpensive based on FLOPs or the number of parameters, which are not
    necessarily correlated with the inference efficiency, so they propose a fused
    MBConv layer that fuses together the first pointwise convolution and the subsequent
    depthwise convolution into a single standard convolution. They achieve higher
    mAP and lower latency on EdgeTPU and DSP than the pure MBConv search space. Li
    et al. [[70](#bib.bib70)] provide an in-depth comparison between fused MBConv
    and MBConv and summarize that fused MBConv has higher operational intensity but
    higher FLOPs than MBConv depending on the shape and size of filters and activations.
    Therefore, they add both fused MBConv and MBConv into the search space to let
    the search strategy determine automatically.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MobileNetV2 中，提出了一种更先进的移动卷积块——移动反向瓶颈卷积（MBConv）[[10](#bib.bib10)]，并迅速成为高效搜索空间的热门操作符。图
    [2b](#S3.F2.sf2 "在图2 ‣ III-A 高效搜索空间 ‣ III 搜索高效深度学习模型 ‣ 快速、轻量且有效的深度学习模型设计自动化：综述")
    展示了 MBConv 的结构。它由三个卷积和一个残差连接组成。首先，应用一个 PWConv 将输入特征图扩展到更高维空间，以便非线性激活（ReLU6）可以更好地提取信息。然后，使用
    $3\times 3$ 核和 ReLU6 激活执行深度卷积，以实现对高维特征图的空间滤波。此外，空间滤波后的特征图通过另一个逐点卷积投影回低维空间。由于低维投影会导致信息丢失，逐点卷积后使用线性激活。最后，根据深度卷积层的步幅是否为
    1，添加一个可选的残差连接，将原始输入和低维投影的输出结合起来。注意，最后两个卷积（深度卷积和逐点卷积）本质上是具有维度减少的 DSConv。许多研究工作使用
    MBConv 构建高效搜索空间，但使用了不同的骨干网络或搜索策略 [[66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]。Xiong
    等人 [[69](#bib.bib69)] 认为 DSConv 在 FLOPs 或参数数量上成本低，但这些指标不一定与推理效率相关，因此他们提出了一种融合
    MBConv 层，将第一个逐点卷积和随后的深度卷积融合成一个标准卷积。他们在 EdgeTPU 和 DSP 上实现了比纯 MBConv 搜索空间更高的 mAP
    和更低的延迟。Li 等人 [[70](#bib.bib70)] 对融合 MBConv 和 MBConv 进行了深入比较，总结出融合 MBConv 的操作强度更高，但
    FLOPs 比 MBConv 更高，这取决于滤波器和激活的形状和大小。因此，他们将融合 MBConv 和 MBConv 都加入到搜索空间中，以让搜索策略自动确定。
- en: 'In a more popular manner, MnasNet [[48](#bib.bib48)] upgrades MBConv with a
    squeeze and excitation (SE) module[[71](#bib.bib71)] after the DWConv for attentions
    on feature maps (as shown in Fig. [2c](#S3.F2.sf3 "In Figure 2 ‣ III-A Efficient
    Search Space ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey")). Specifically,
    the S (squeeze) procedure first converts each individual feature map into a scalar
    descriptor using global average pooling so the input feature maps are converted
    into a vector $\bm{z}\in\mathbb{R}^{n}$ with its $k$-th element calculated by:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的方式是，MnasNet [[48](#bib.bib48)] 在 DWConv 后升级了 MBConv，引入了 squeeze 和 excitation
    (SE) 模块[[71](#bib.bib71)]，以便对特征图进行关注（如图 [2c](#S3.F2.sf3 "在图 2 ‣ III-A 高效搜索空间 ‣
    III 搜索高效深度学习模型 ‣ 设计自动化以实现快速、轻量和有效的深度学习模型：综述")所示）。具体来说，S (squeeze) 过程首先使用全局平均池化将每个特征图转换为标量描述符，因此输入特征图被转换为一个向量
    $\bm{z}\in\mathbb{R}^{n}$，其第 $k$ 个元素计算公式为：
- en: '|  | $z_{k}=\text{GlobalArgPool}(\bm{x}_{k})=\frac{1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W}\bm{x}_{(i,j,k)},$
    |  | (6) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{k}=\text{GlobalArgPool}(\bm{x}_{k})=\frac{1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W}\bm{x}_{(i,j,k)},$
    |  | (6) |'
- en: 'where $\bm{x}_{(i,j)}$ is the $(i,j)$-th element of the $k$-th input feature
    map, which is of size $H\times W$. The E (excitation) procedure converts the S
    procedure’s output $\bm{z}$ into a vector of activations $\bm{s}$ using the gating
    mechanism of two fully connected layers:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{x}_{(i,j)}$ 是第 $k$ 个输入特征图中 $(i,j)$ 位置的元素，特征图的大小为 $H\times W$。E (excitation)
    过程使用两个全连接层的门控机制将 S 过程的输出 $\bm{z}$ 转换为激活向量 $\bm{s}$：
- en: '|  | $\bm{s}=\sigma(\textbf{W}_{2}\cdot\text{ReLU}(\textbf{W}_{1}\cdot\bm{z})),$
    |  | (7) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{s}=\sigma(\textbf{W}_{2}\cdot\text{ReLU}(\textbf{W}_{1}\cdot\bm{z})),$
    |  | (7) |'
- en: 'where $\sigma(\cdot)$ is the sigmoid activation function. The final output
    of the SE module is $\bm{\tilde{X}}$ with its $k$-th feature map denoted as:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma(\cdot)$ 是 sigmoid 激活函数。SE 模块的最终输出是 $\bm{\tilde{X}}$，其第 $k$ 个特征图表示为：
- en: '|  | $\bm{\tilde{x}}_{k}=\text{SE}(\bm{x}_{k})=s_{k}\cdot\bm{x}_{k}.$ |  |
    (8) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\tilde{x}}_{k}=\text{SE}(\bm{x}_{k})=s_{k}\cdot\bm{x}_{k}.$ |  |
    (8) |'
- en: MobileNetV3 [[72](#bib.bib72)], the latest version of the MobileNet series,
    also adopts the MBConv plus SE operator and further enhances it by using the hard-swish
    (HS) [[73](#bib.bib73)] nonlinearities instead of ReLU and replacing the sigmoid
    function in SE with hard sigmoid [[74](#bib.bib74)]. The authors also point out
    that HS in deeper layers is more beneficial. This choice is based on the fact
    that the sigmoid function is expensive to deploy on mobile devices. Thereafter,
    many more works embrace the MBConv plus SE operator in their efficient search
    space [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)]. However, SE is not always paired with MBConv due to either
    not being supported by hardware [[31](#bib.bib31)] or unfavourable performance
    [[80](#bib.bib80)]. Some other works maintain diverse convolution operators (i.e.,
    standard convolution, DSConv, MBConv, and MBConv+SE) in the search space and let
    the search strategy choose automatically [[81](#bib.bib81), [48](#bib.bib48)].
    The commonly used searchable parameters of the convolution operator family are
    kernel sizes, the number of output channels, expansion ratios (MBConv, MBConv+SE),
    and SE ratio (MBConv+SE). Although different works do not have exactly the same
    searching ranges, they are basically similar. For example, Stamoulis et al. [[66](#bib.bib66)]
    consider kernel sizes of $\{3,5\}$ and expansion ratios of $\{3,6\}$; Fang et
    al. [[67](#bib.bib67)] consider kernel sizes of $\{3,5,7\}$ and expansion ratios
    of $\{3,6\}$; Cai et al. [[77](#bib.bib77)] consider kernel sizes of $\{3,5,7\}$
    and expansion ratios of $\{3,4,6\}$.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetV3 [[72](#bib.bib72)]，作为 MobileNet 系列的最新版本，也采用了 MBConv 加 SE 操作符，并通过使用
    hard-swish (HS) [[73](#bib.bib73)] 非线性函数代替 ReLU，并将 SE 中的 sigmoid 函数替换为 hard sigmoid
    [[74](#bib.bib74)] 进一步增强了性能。作者还指出，在更深的层中，HS 更具优势。这一选择基于 sigmoid 函数在移动设备上部署成本高的事实。因此，许多其他工作也在其高效搜索空间中采纳了
    MBConv 加 SE 操作符 [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)]。然而，由于硬件 [[31](#bib.bib31)] 不支持或性能不佳 [[80](#bib.bib80)]，SE 并不总是与
    MBConv 配对。一些其他工作保持了多样的卷积操作符（即标准卷积、DSConv、MBConv 和 MBConv+SE）在搜索空间中，并让搜索策略自动选择
    [[81](#bib.bib81), [48](#bib.bib48)]。卷积操作符家族的常用可搜索参数包括内核大小、输出通道数量、扩展比（MBConv，MBConv+SE）和
    SE 比率（MBConv+SE）。虽然不同的工作没有完全相同的搜索范围，但基本相似。例如，Stamoulis 等 [[66](#bib.bib66)] 考虑了
    $\{3,5\}$ 的内核大小和 $\{3,6\}$ 的扩展比；Fang 等 [[67](#bib.bib67)] 考虑了 $\{3,5,7\}$ 的内核大小和
    $\{3,6\}$ 的扩展比；Cai 等 [[77](#bib.bib77)] 考虑了 $\{3,5,7\}$ 的内核大小和 $\{3,4,6\}$ 的扩展比。
- en: 'Besides the above convolution operators of the MobileNet family, the group
    convolution (GConv) and its variants [[82](#bib.bib82), [83](#bib.bib83), [11](#bib.bib11)]
    are also considered to be significant operators for constructing an efficient
    search space [[84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88)]. Fig. [2d](#S3.F2.sf4 "In Figure 2 ‣ III-A Efficient Search
    Space ‣ III Search for Efficient Deep Learning Models ‣ Design Automation for
    Fast, Lightweight, and Effective Deep Learning Models: A Survey") illustrates
    the structure of GConv, where we take into account the feature map dimension of
    convolutional layers. The feature maps and filters are divided into $G$ ($G=3$
    in Fig. [2d](#S3.F2.sf4 "In Figure 2 ‣ III-A Efficient Search Space ‣ III Search
    for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")) groups respectively: $\bm{X}=\{\bm{X}_{1},\bm{X}_{2},...,\bm{X}_{G}\}$
    and $\textbf{W}=\{\textbf{W}_{1}\,\textbf{W}_{2}\,...,\textbf{W}_{G}\}$. In GConv,
    the convolution is only performed within each group so the output $\bm{\tilde{X}}$
    is denoted as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 MobileNet 家族的上述卷积操作符，组卷积（GConv）及其变体 [[82](#bib.bib82), [83](#bib.bib83),
    [11](#bib.bib11)] 也被认为是构建高效搜索空间的重要操作符 [[84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88)]。图 [2d](#S3.F2.sf4 "在图 2 ‣ III-A 高效搜索空间 ‣ III
    搜索高效深度学习模型 ‣ 快速、轻量级和有效深度学习模型的设计自动化：调查") 说明了 GConv 的结构，其中我们考虑了卷积层的特征图维度。特征图和滤波器分别被分成
    $G$ （图 [2d](#S3.F2.sf4 "在图 2 ‣ III-A 高效搜索空间 ‣ III 搜索高效深度学习模型 ‣ 快速、轻量级和有效深度学习模型的设计自动化：调查")
    中 $G=3$） 组：$\bm{X}=\{\bm{X}_{1},\bm{X}_{2},...,\bm{X}_{G}\}$ 和 $\textbf{W}=\{\textbf{W}_{1}\,\textbf{W}_{2}\,...,\textbf{W}_{G}\}$。在
    GConv 中，卷积仅在每个组内执行，因此输出 $\bm{\tilde{X}}$ 表示为：
- en: '|  | $\bm{\tilde{X}}=\{\textbf{W}_{1}\otimes\bm{X}_{1},\textbf{W}_{2}\otimes\bm{X}_{2},...,\textbf{W}_{G}\otimes\bm{X}_{G}\},$
    |  | (9) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\tilde{X}}=\{\textbf{W}_{1}\otimes\bm{X}_{1},\textbf{W}_{2}\otimes\bm{X}_{2},...,\textbf{W}_{G}\otimes\bm{X}_{G}\},$
    |  | (9) |'
- en: where $\otimes$ is the convolution operation between two sets. In this way,
    GConv can not only reduce parameters and computation but also provide a simple
    way to model parallelism. Therefore, AlexNet [[83](#bib.bib83)] can be trained
    on multiple GPUs with only 3GB RAM each. Note that depthwise convolution is a
    special case of GConv with the number of groups being the same as the number of
    channels. A channel shuffle usually comes after GConv to enable inter-group communications
    [[11](#bib.bib11)]. Xu et al. [[84](#bib.bib84)] include GConv in their search
    space with the number of groups as searchable from 1 (standard convolution) to
    N (the number of input channels). FBNet [[86](#bib.bib86)] and RCAS [[87](#bib.bib87)]
    encompasses a new convolution operator in their search space by replacing the
    first and last pointwise convolution in MBConv with $1\times 1$ GConv. This design
    expands MBConv but also allows the search strategy to automatically determine
    whether this expansion is needed. However, their allowed maximum group amount
    is quite small (2 for FBNet and 4 for RCAS). DPP-Net [[85](#bib.bib85)] and MONAS
    [[88](#bib.bib88)] also contain a variant of GConv, Learned Group Convolution
    (LGConv), which is the key operator of CondenseNet [[89](#bib.bib89)], in their
    search space. The LGConv prunes away unimportant filters with low magnitude weights
    further reducing the computational complexity on top of the GConv [[82](#bib.bib82)].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\otimes$ 是两个集合之间的卷积操作。这样，GConv 不仅可以减少参数和计算，还能提供一种简单的并行建模方式。因此，AlexNet [[83](#bib.bib83)]
    可以在每个仅有 3GB RAM 的多个 GPU 上进行训练。注意，depthwise 卷积是 GConv 的一个特殊情况，其组数与通道数相同。GConv 通常后接一个通道混洗操作，以实现组间通信
    [[11](#bib.bib11)]。徐等人 [[84](#bib.bib84)] 将 GConv 包含在他们的搜索空间中，其中组数从 1（标准卷积）到 N（输入通道数）都是可搜索的。FBNet
    [[86](#bib.bib86)] 和 RCAS [[87](#bib.bib87)] 在其搜索空间中包含了一种新的卷积操作符，通过用 $1\times
    1$ GConv 替换 MBConv 中的第一和最后一个点卷积。这一设计扩展了 MBConv，同时也允许搜索策略自动确定是否需要这种扩展。然而，它们允许的最大组数较小（FBNet
    为 2，RCAS 为 4）。DPP-Net [[85](#bib.bib85)] 和 MONAS [[88](#bib.bib88)] 也在其搜索空间中包含了
    GConv 的一种变体，学习型组卷积（LGConv），这是 CondenseNet [[89](#bib.bib89)] 的关键操作符。LGConv 通过剪除低幅度权重的不重要滤波器，进一步减少了
    GConv 上的计算复杂度 [[82](#bib.bib82)]。
- en: Some other studies use customized operators or non-convolution operators in
    their efficient search spaces. FasterSeg [[90](#bib.bib90)] proposes a zoomed
    convolution, where the input is sequentially processed with bilinear downsampling,
    standard convolution, and bilinear upsampling. The authors demonstrate that the
    zoomed convolution has 40% latency trim compared to a standard convolution on
    a GTX 1080i GPU. Different from previous works, which focus on 2D processing,
    Tang et al. [[91](#bib.bib91)] target 3D scenes. They propose sparse point-voxel
    convolutions and allow to search for channel numbers and network depth. However,
    as the computation of 3D CNN increases more significantly with increasing kernel
    sizes than 2D CNN, the authors keep the kernel size as a constant of 3\. HR-NAS
    [[92](#bib.bib92)] also involves Transformer [[93](#bib.bib93), [94](#bib.bib94)]
    in addition to convolutions due to its recent success in computer vision [[95](#bib.bib95),
    [96](#bib.bib96)]. The authors design a lightweight Transformer that requires
    less computation when facing high-resolution images. Convolutional channels and
    Transformer queries are progressively reduced during the search. In order to facilitate
    high parallelism of convolution operators on TPUs and GPUs, Li et al. [[70](#bib.bib70)]
    add space-to-depth/batch into the search space to increase the depth and batch
    dimensions. The results show that even without the lowest FLOPs, their searched
    EfficientNet-X models are the fastest among compared model families on TPUs and
    GPUs. The main reason is that EfficientNet-X models strike a balance between FLOPs
    (lower is good for speed) and operational intensity (higher is good for speed).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他研究在其高效搜索空间中使用了定制的操作符或非卷积操作符。FasterSeg [[90](#bib.bib90)] 提出了一个放大卷积，其中输入依次通过双线性下采样、标准卷积和双线性上采样处理。作者展示了相较于标准卷积，放大卷积在GTX
    1080i GPU上的延迟减少了40%。不同于以往专注于2D处理的工作，Tang等人 [[91](#bib.bib91)] 以3D场景为目标。他们提出了稀疏点-体素卷积，并允许搜索通道数和网络深度。然而，由于3D
    CNN的计算量随着卷积核尺寸增加比2D CNN显著增加，作者将卷积核大小保持为常数3。HR-NAS [[92](#bib.bib92)] 除了卷积外，还涉及了Transformer
    [[93](#bib.bib93), [94](#bib.bib94)]，这是由于其在计算机视觉 [[95](#bib.bib95), [96](#bib.bib96)]
    中的近期成功。作者设计了一个轻量级Transformer，当面对高分辨率图像时需要更少的计算。卷积通道和Transformer查询在搜索过程中逐步减少。为了促进TPUs和GPUs上卷积操作符的高度并行性，Li等人
    [[70](#bib.bib70)] 将空间到深度/批次添加到搜索空间中，以增加深度和批次维度。结果表明，即使没有最低的FLOPs，他们搜索的EfficientNet-X模型在TPUs和GPUs上的速度也最快。主要原因是EfficientNet-X模型在FLOPs（较低有利于速度）和操作强度（较高有利于速度）之间达到了平衡。
- en: 'In addition to parametric operators, non-parametric operators are critical
    components of an efficient search space as well. Pooling is usually coupled with
    convolutions in hand-crafted CNN model family, and so does in convolution-based
    search space [[87](#bib.bib87), [80](#bib.bib80)]. It can help to reduce redundant
    information (favourable for accuracy) and computation (favourable for speed) without
    requiring additional parameters. Skip is another widely adopted operator that
    impacts the topology of achieved models. It can have two concepts: 1) drop skip
    directly feeding input to output without any actual computations, i.e., the entire
    layer is dropped and thus the model depth is reduced [[66](#bib.bib66), [86](#bib.bib86),
    [67](#bib.bib67)]; 2) residual skip providing an identity residual connection
    in parallel with another computation operator [[64](#bib.bib64), [80](#bib.bib80)].
    The residual connection can be achieved by either concatenation or by addition
    [[64](#bib.bib64)]. Activation functions are another searchable non-parametric
    operators that impact both accuracy and speed [[87](#bib.bib87), [70](#bib.bib70)].
    The computation cost of activation functions decreases when going deeper since
    the resolution of feature maps decreases [[72](#bib.bib72)]. In some research,
    the non-linearity is associated with convolution operators as a whole, and the
    operator together with its activation function is determined automatically [[79](#bib.bib79)].'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 除了参数化操作符，非参数化操作符也是高效搜索空间的关键组成部分。池化通常与卷积结合使用，在手工制作的 CNN 模型家族中是这样，在基于卷积的搜索空间中也是如此[[87](#bib.bib87),
    [80](#bib.bib80)]。它可以帮助减少冗余信息（有利于准确性）和计算（有利于速度），而无需额外的参数。跳跃是另一种广泛采用的操作符，它会影响所得到模型的拓扑。它可以有两个概念：1）跳过操作符直接将输入传递到输出，没有任何实际计算，即整个层被跳过，从而减少模型深度[[66](#bib.bib66),
    [86](#bib.bib86), [67](#bib.bib67)]; 2）残差跳跃提供一个与另一个计算操作符并行的身份残差连接[[64](#bib.bib64),
    [80](#bib.bib80)]。残差连接可以通过拼接或加法实现[[64](#bib.bib64)]。激活函数是另一种可搜索的非参数化操作符，它影响准确性和速度[[87](#bib.bib87),
    [70](#bib.bib70)]。随着深度的增加，激活函数的计算成本减少，因为特征图的分辨率降低[[72](#bib.bib72)]。在一些研究中，非线性与卷积操作符整体相关，操作符及其激活函数会自动确定[[79](#bib.bib79)]。
- en: III-A2 Backbones
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 骨干
- en: 'After defining the operators that a model can have, it is essential to determine
    how many operators there are and how these operators are connected, i.e., the
    backbone of a model. However, operators and backbones are not completely isolated,
    such as the skip operator, which removes layers or adds connections and can change
    a model’s backbone. Depending on the connection topology, backbones can be roughly
    classified into two categories: chain-structured and multi-branch.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型可以拥有的操作符之后，确定有多少个操作符以及这些操作符如何连接，即模型的骨干，至关重要。然而，操作符和骨干并不是完全独立的，例如跳跃操作符，它会移除层或添加连接，可能会改变模型的骨干。根据连接拓扑，骨干大致可以分为两类：链式结构和多分支结构。
- en: '![Refer to caption](img/22cfdcb6232686bf2fa2894c0ab2b7a2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22cfdcb6232686bf2fa2894c0ab2b7a2.png)'
- en: 'Figure 3: Schematics of different backbones. Each block represents an operator
    $O_{i}$ in (a) and (b), and represents a cell $C_{i}$ in (c), and different colours
    indicate different operator/cell types. The arrows indicate the information flow
    direction. In the macro-micro backbone, the macro-structure describes the topology
    of multiple cells, each of which has a micro-structure of operators.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同骨干的示意图。每个块在 (a) 和 (b) 中代表一个操作符 $O_{i}$，在 (c) 中代表一个单元 $C_{i}$，不同的颜色表示不同的操作符/单元类型。箭头表示信息流动方向。在宏观-微观骨干中，宏观结构描述多个单元的拓扑结构，每个单元都有一个操作符的微观结构。
- en: 'Chain-structured Backbones. The chain-structured backbone [[57](#bib.bib57)]
    is the earliest and simplest topological structure of a neural network. It directly
    stacks multiple operators in sequence. As shown in Fig. [3](#S3.F3 "Figure 3 ‣
    III-A2 Backbones ‣ III-A Efficient Search Space ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey") (a), the $i$th operator ($O_{i}$) takes the output
    of the $i$-1th operator ($O_{i-1}$) as input and its output serves as the input
    of the operator ($O_{i+1}$). Therefore, the topological connection of different
    operators is determined and not searchable. However, it is possible to search
    how many operators there are, e.g., via the drop skip. This simple backbone is
    a favourable practice of many hand-crafted CNN models [[83](#bib.bib83), [11](#bib.bib11),
    [9](#bib.bib9), [10](#bib.bib10)].'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '链式结构骨干网络。链式结构骨干网络[[57](#bib.bib57)]是最早和最简单的神经网络拓扑结构。它直接按顺序堆叠多个操作符。如图[3](#S3.F3
    "Figure 3 ‣ III-A2 Backbones ‣ III-A Efficient Search Space ‣ III Search for Efficient
    Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey") (a)所示，第$i$个操作符($O_{i}$)以第$i$-1个操作符($O_{i-1}$)的输出作为输入，其输出作为第$i$+1个操作符($O_{i+1}$)的输入。因此，不同操作符的拓扑连接是确定的，无法搜索。然而，可以搜索操作符的数量，例如，通过跳过操作符。这个简单的骨干网络是许多手工制作的CNN模型的一个优良实践[[83](#bib.bib83),
    [11](#bib.bib11), [9](#bib.bib9), [10](#bib.bib10)]。'
- en: An intuitive implementation is to stack operators layer by layer [[87](#bib.bib87),
    [77](#bib.bib77)]. Motivated by the design principle of existing success [[10](#bib.bib10),
    [11](#bib.bib11)], the chain-structured backbone is more commonly implemented
    in a cell-based, a.k.a. block- or stage-based, manner [[68](#bib.bib68), [81](#bib.bib81),
    [79](#bib.bib79), [91](#bib.bib91), [86](#bib.bib86), [67](#bib.bib67)], where
    a backbone is composed of multiple chain-structured cells, each of which contains
    multiple chain-structured operators. The operators in the same cell can be either
    identical or diverse in hyperparameters but the same in type. In the sequence
    of cells, it is a usual principle that the input resolutions are reduced and widths
    are increased gradually. To achieve the reduced resolutions, the first or last
    operator in a cell usually has manually set strides and widths. This cell-based
    implementation is not only effective but also reduces the search space. Wu et
    al. [[86](#bib.bib86)] design a chain-structured backbone consisting of four searchable
    cells. There are 8 candidate convolution operators with various expansion rates,
    kernel sizes and numbers of groups, and a drop skip operator for searching. Different
    cells are separated by their input resolutions and widths, which are determined
    by manually set parameters. Yan et al. [[81](#bib.bib81)] also follow the same
    practice to construct their backbone but with more flexibility in searchable parameters.
    In these studies, it is usually required to predefine some parameters of the backbone,
    like the number of cells.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 直观的实现方式是逐层堆叠操作符[[87](#bib.bib87), [77](#bib.bib77)]。受现有成功设计原则的启发[[10](#bib.bib10),
    [11](#bib.bib11)]，链式结构的骨干网络通常以基于单元的方式实现，也就是基于块或阶段的方式[[68](#bib.bib68), [81](#bib.bib81),
    [79](#bib.bib79), [91](#bib.bib91), [86](#bib.bib86), [67](#bib.bib67)]，其中一个骨干网络由多个链式结构单元组成，每个单元包含多个链式结构操作符。同一单元中的操作符可以在超参数上相同或不同，但类型相同。在单元序列中，通常的原则是输入分辨率逐渐减少，宽度逐渐增加。为了实现减少的分辨率，单元中的第一个或最后一个操作符通常具有手动设置的步幅和宽度。这种基于单元的实现方式不仅有效，而且减少了搜索空间。Wu等人[[86](#bib.bib86)]设计了一个由四个可搜索单元组成的链式结构骨干网络。存在8个候选卷积操作符，具有不同的扩展率、核大小和组数，以及一个用于搜索的跳过操作符。不同的单元通过其输入分辨率和宽度来分隔，这些参数是由手动设置的。Yan等人[[81](#bib.bib81)]也遵循相同的做法来构建他们的骨干网络，但在可搜索参数方面具有更大的灵活性。在这些研究中，通常需要预定义一些骨干网络的参数，如单元的数量。
- en: In comparison with setting backbone structure empirically, some work directly
    embraces the backbone of existing models as a starting point and then searches
    beyond that [[76](#bib.bib76), [69](#bib.bib69), [88](#bib.bib88), [23](#bib.bib23),
    [66](#bib.bib66), [69](#bib.bib69)]. This design principle relieves experts’ burden
    of search space design by reusing prior knowledge, which is important in NAS [[97](#bib.bib97)].
    MONAS [[88](#bib.bib88)] relies on the backbone of a simplified version of AlexNet
    [[83](#bib.bib83)] to search the convolution filter sizes and amounts. Scheidegger
    et al. [[62](#bib.bib62)] investigate the backbone of MobileNetV2 [[10](#bib.bib10)]
    and allows to search the number of operators in each cell but all operators in
    a cell have the same settings except for the stride, which is used to modify the
    output resolution. The complexity reduction is mainly obtained by lowering the
    channel widths and reducing the number of topological replications. In addition
    to following the backbone of hand-crafted models, there is a trend to using the
    backbone of existing searched efficient models. MOGA [[76](#bib.bib76)] adopts
    MobileNetV3-large [[72](#bib.bib72)] as its backbone and keeps the same number
    and type of operators. It only searches the parameters of operators, like kernel
    sizes, expansion ratios for MBConv, and whether SE is enabled or not. Cai et al.
    [[77](#bib.bib77)] also adopts MobileNetV3, but they additionally provide the
    flexibility of searching the number of operators in each cell.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过经验设置骨干结构相比，一些工作直接采用现有模型的骨干作为起点，然后在此基础上进行搜索[[76](#bib.bib76), [69](#bib.bib69),
    [88](#bib.bib88), [23](#bib.bib23), [66](#bib.bib66), [69](#bib.bib69)]。这一设计原则通过重用先前的知识，减轻了专家在搜索空间设计上的负担，这在NAS中是重要的[[97](#bib.bib97)]。MONAS[[88](#bib.bib88)]依赖于AlexNet[[83](#bib.bib83)]的简化版本的骨干来搜索卷积滤波器的尺寸和数量。Scheidegger等人[[62](#bib.bib62)]研究了MobileNetV2[[10](#bib.bib10)]的骨干，并允许搜索每个单元中的操作符数量，但单元中的所有操作符具有相同的设置，除了用于修改输出分辨率的步幅。复杂度的降低主要是通过减少通道宽度和降低拓扑复制的数量来实现的。除了遵循手工设计的模型的骨干网络外，还有一种趋势是使用现有高效模型的骨干。MOGA[[76](#bib.bib76)]采用MobileNetV3-large[[72](#bib.bib72)]作为其骨干，并保持相同数量和类型的操作符。它仅搜索操作符的参数，如内核尺寸、MBConv的扩展比例以及是否启用SE。Cai等人[[77](#bib.bib77)]也采用了MobileNetV3，但他们额外提供了搜索每个单元中操作符数量的灵活性。
- en: 'Mutli-branch Backbones. While the chain-structured backbone is simple, it restricts
    the information flow to be sequential and single-path. Current hand-crafted architectures
    have suggested that a multi-branch architecture, which allows multi-path information
    flow and residual skip connections, works impressively better than a chain-structured
    architecture [[98](#bib.bib98), [47](#bib.bib47)]. As illustrated in Fig. [3](#S3.F3
    "Figure 3 ‣ III-A2 Backbones ‣ III-A Efficient Search Space ‣ III Search for Efficient
    Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey") (b), in the multi-branch backbone, an operator
    $O_{i}$ is allowed to accept the outputs from some of its previous operators (i.e.,
    $O_{1}$, $O_{2}$ … $O_{i-1}$) as the input but not necessarily takes all. This
    setting provides more degrees of freedom on network topology. Note that a chain-structured
    backbone with residual skip connections, where an operator $O_{i}$ must receive
    the output of its immediate previous operator $O_{i-1}$, is a special case of
    the multi-branch backbone.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '多分支骨干网络。虽然链式结构的骨干网络简单，但它限制了信息流的顺序和单路径。当前的手工设计架构表明，多分支架构允许多路径信息流和残差跳跃连接，比链式结构架构表现得更为出色[[98](#bib.bib98),
    [47](#bib.bib47)]。如图 [3](#S3.F3 "Figure 3 ‣ III-A2 Backbones ‣ III-A Efficient
    Search Space ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") (b)所示，在多分支骨干网络中，操作符$O_{i}$可以接受来自某些先前操作符（即$O_{1}$,
    $O_{2}$ … $O_{i-1}$）的输出作为输入，但不一定接受全部。这种设置提供了更多的网络拓扑自由度。注意，带有残差跳跃连接的链式结构骨干网络，其中操作符$O_{i}$必须接收其直接前驱操作符$O_{i-1}$的输出，是多分支骨干网络的一个特例。'
- en: 'The multi-branch backbone can be achieved by using an insertion operation in
    the search space [[65](#bib.bib65)]. However, searching for a multi-branch backbone
    as a whole is time-consuming and difficult to find an optimal structure. Similar
    to the chain-structured backbone, the cell-based principle is also widely adopted
    in designing a multi-branch backbone [[80](#bib.bib80), [61](#bib.bib61), [68](#bib.bib68)],
    where the topology within a cell has a multi-branch structure. This results in
    a Macro-micro Backbone (Fig. [3](#S3.F3 "Figure 3 ‣ III-A2 Backbones ‣ III-A Efficient
    Search Space ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") (c)) [[48](#bib.bib48),
    [61](#bib.bib61), [92](#bib.bib92), [84](#bib.bib84)]. In the macro-structure,
    residual skips are optional to provide the flexibility of multi-path information
    flow; in the micro-structure, operators are connected in the multi-branch fashion
    with the last operator (either parametric or non-parametric) assembling a single
    output of the cell. For example, MnasNet [[48](#bib.bib48)] predefines a backbone
    of 7 sequentially stacked cells and each cell contains sequentially stacked operators
    with a searchable amount, types, and parameters. It further allows searchable
    residual skip connections among operators within a cell. This multi-branch backbone
    simply augments the chain-structured backbone with residual skips, resulting in
    more flexibility of multi-path information flow but marginal research space expansion.
    NSGA-Net [[61](#bib.bib61)] and HR-NAS [[92](#bib.bib92)], in contrast, allow
    more general multi-branch connections within a cell. NSGA-Net supports searchable
    topology while HR-NAS sets fixed multi-branch connections and searches for discarded
    convolutional channels and transformer queries.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '多分支骨干网络可以通过在搜索空间中使用插入操作来实现[[65](#bib.bib65)]。然而，整体搜索多分支骨干网络是耗时的，并且很难找到**最优**结构。类似于链结构骨干网络，基于单元的原理在设计多分支骨干网络时也被广泛采用[[80](#bib.bib80),
    [61](#bib.bib61), [68](#bib.bib68)]，其中单元内的拓扑结构具有多分支结构。这就产生了宏观-微观骨干网络（图 [3](#S3.F3
    "Figure 3 ‣ III-A2 Backbones ‣ III-A Efficient Search Space ‣ III Search for Efficient
    Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey") (c)）[[48](#bib.bib48), [61](#bib.bib61), [92](#bib.bib92),
    [84](#bib.bib84)]。在宏观结构中，残差跳跃是可选的，以提供多路径信息流的灵活性；在微观结构中，操作符以多分支方式连接，最后的操作符（无论是参数化还是非参数化）将单元的输出汇总。例如，MnasNet
    [[48](#bib.bib48)] 预定义了一个由 7 个顺序堆叠的单元组成的骨干网络，每个单元包含具有可搜索数量、类型和参数的顺序堆叠操作符。它进一步允许单元内操作符之间可搜索的残差跳跃连接。这种多分支骨干网络通过增加残差跳跃来增强链结构骨干网络，从而提供了更多的多路径信息流的灵活性，但边际上扩展了研究空间。相比之下，NSGA-Net
    [[61](#bib.bib61)] 和 HR-NAS [[92](#bib.bib92)] 允许在单元内有更一般的多分支连接。NSGA-Net 支持可搜索的拓扑结构，而
    HR-NAS 设置固定的多分支连接并搜索被丢弃的卷积通道和变换器查询。'
- en: Different from searching for a customized structure, following existing success
    is also a favoured choice in designing multi-branch backbones [[70](#bib.bib70),
    [31](#bib.bib31), [80](#bib.bib80), [72](#bib.bib72)]. Li et al. [[70](#bib.bib70)]
    adopt the multi-branch backbone of the EfficientNet and only search operators.
    MnasFPN [[80](#bib.bib80)] constructs its search space based on the NAS-FPN(Lite)
    backbone and searches both multi-branch structures and operators in a cell for
    merging various resolutions. However, to reduce the search burden, it does not
    allow general connectivity patterns and applies limited merging connections. MobileNetV3
    [[9](#bib.bib9)] uses the backbone of MnasNet [[48](#bib.bib48)] as the seed and
    proceeds layer-wise search on it. Scheidegger et al. [[62](#bib.bib62)] investigate
    the backbone of several existing models (DenseNet121 [[60](#bib.bib60)], MobileNetV2
    [[10](#bib.bib10)], GoogLeNet [[99](#bib.bib99)], PNASNet [[100](#bib.bib100)],
    and ResNeXt [[82](#bib.bib82)]) and demonstrate that their approach is able to
    provide improved accuracy with hardware constraints and different backbones. While
    considering that searching both macro and micro structures overburdens the searching
    process, most works utilize the multi-branch macro backbone of existing models
    and only search the micro-structures [[23](#bib.bib23), [85](#bib.bib85), [88](#bib.bib88)].
    ProxylessNAS [[23](#bib.bib23)] accepts the backbone of the residual PyramidNet
    [[101](#bib.bib101)], which has a residual skip connection every two operators,
    and replaces the original operators with their own tree-structured cells [[102](#bib.bib102)].
    DPP-Net [[85](#bib.bib85)] selects the backbone of CondenseNet [[89](#bib.bib89)],
    which repeats an identical cell abundant times with both residual skip and chain
    connections, and only searches the operators in the cell. MONAS [[88](#bib.bib88)]
    also reuses the backbone of CondenseNet but it uses the same cell structure and
    searches the number of stages and growth rate.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于搜索定制化结构，沿用已有的成功经验也是设计多分支骨干网络的一种受欢迎的选择[[70](#bib.bib70), [31](#bib.bib31),
    [80](#bib.bib80), [72](#bib.bib72)]。李等人[[70](#bib.bib70)]采用了EfficientNet的多分支骨干网络，并只进行操作符搜索。MnasFPN
    [[80](#bib.bib80)]基于NAS-FPN(Lite)骨干网络构建其搜索空间，并在单元中同时搜索多分支结构和操作符以合并不同分辨率。然而，为了减少搜索负担，它不允许一般的连接模式，并应用有限的合并连接。MobileNetV3
    [[9](#bib.bib9)]以MnasNet [[48](#bib.bib48)]作为基础，并在其上进行逐层搜索。Scheidegger等人[[62](#bib.bib62)]调查了几个现有模型的骨干网络（DenseNet121
    [[60](#bib.bib60)], MobileNetV2 [[10](#bib.bib10)], GoogLeNet [[99](#bib.bib99)],
    PNASNet [[100](#bib.bib100)], 和ResNeXt [[82](#bib.bib82)]），并证明他们的方法在硬件约束和不同骨干网络下能够提供更好的准确性。考虑到同时搜索宏观和微观结构会加重搜索过程，大多数研究利用现有模型的多分支宏观骨干网络，仅搜索微观结构[[23](#bib.bib23),
    [85](#bib.bib85), [88](#bib.bib88)]。ProxylessNAS [[23](#bib.bib23)]接受了残差PyramidNet
    [[101](#bib.bib101)]的骨干网络，该网络在每两个操作符之间有一个残差跳跃连接，并用其自己的树结构单元[[102](#bib.bib102)]替换了原始操作符。DPP-Net
    [[85](#bib.bib85)]选择了CondenseNet [[89](#bib.bib89)]的骨干网络，该网络在具有残差跳跃和链式连接的相同单元上重复多个相同的单元，并仅搜索单元中的操作符。MONAS
    [[88](#bib.bib88)]也重用了CondenseNet的骨干网络，但它使用了相同的单元结构，并搜索阶段数和增长率。
- en: 'Although the macro-micro backbone provides the highest flexibility and complexity
    among the above three categories, there is no evidence that it is the best choice.
    However, we note that no matter in which backbone category, cell-based implementation
    is the most common practice. This is due to the following considerations: i) an
    effective network usually has gradually shrinking resolutions as going deep, and
    a cell can have multiple operators on the same resolution to strengthen feature
    extraction; ii) previous experience with the manually designed network indicates
    that the cell-based structure is effective for deep learning; iii) the cell-based
    backbone provides high search efficiency by limiting the search space (e.g., the
    whole network can stack the same cell and a cell can have the same operator);
    iv) the cell-based backbone simplifies the network topology with inter- and intra-cell
    connectivity instead of random edges among all operators. Some recent works reveal
    that search performance may be impeded by these search space design biases [[103](#bib.bib103)].
    Nevertheless, the choice of the search space principally regulates the difficulty
    of the search process. Current NAS algorithms are imperfect so more freedoms in
    the search space do not always lead to better resultant models but inversely overburden
    the search algorithms. With the continued improvement of NAS algorithms, it is
    essential to reduce design biases and construct a more general search space.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管宏微骨干在上述三种类别中提供了最高的灵活性和复杂性，但没有证据表明它是最佳选择。然而，我们注意到无论是哪个骨干类别，基于单元的实现都是最常见的做法。这是由于以下考虑：i)
    一个有效的网络通常具有逐渐缩小的分辨率，随着深度增加，单元可以在相同分辨率上有多个操作符来加强特征提取；ii) 以往手动设计网络的经验表明，基于单元的结构对深度学习是有效的；iii)
    基于单元的骨干通过限制搜索空间（例如，整个网络可以堆叠相同的单元，单元可以有相同的操作符）提供了高效的搜索；iv) 基于单元的骨干通过单元间和单元内的连接简化了网络拓扑，而不是所有操作符之间的随机边。一些近期的工作揭示了这些搜索空间设计偏差可能会阻碍搜索性能[[103](#bib.bib103)]。尽管如此，搜索空间的选择主要调节了搜索过程的难度。当前的NAS算法不完美，因此搜索空间中的更多自由度并不总是能导致更好的结果模型，反而会过度负担搜索算法。随着NAS算法的不断改进，减少设计偏差和构建更通用的搜索空间是至关重要的。
- en: III-B Search Strategy
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 搜索策略
- en: The search strategy describes how to explore a search space to find an optimal
    efficient network. Essentially, the search process is to find the top candidate
    networks regarding some evaluation metrics (e.g., accuracy and latency). However,
    it is computationally prohibitive to achieve these metrics of all candidate networks
    since it requires fully training a network to obtain its metrics. Therefore, the
    aim of a search strategy is to efficiently find top-ranking networks without exhaustively
    examining all candidate networks. In this section, we first conclude the search
    algorithms that describe how the search proceeds and then summarize how the hardware
    constraint is incorporated into the search process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索策略描述了如何探索搜索空间以找到最佳高效网络。实际上，搜索过程是根据一些评估指标（例如，准确性和延迟）找到顶级候选网络。然而，由于需要完全训练一个网络以获得其指标，因此计算上是不可行的。因此，搜索策略的目标是高效地找到排名靠前的网络，而不是对所有候选网络进行详尽检查。在本节中，我们首先总结描述搜索过程的搜索算法，然后总结如何将硬件约束纳入搜索过程。
- en: '![Refer to caption](img/10b2cd79f02abca56f1fdfe9dd1f5ea5.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10b2cd79f02abca56f1fdfe9dd1f5ea5.png)'
- en: (a) Bayesian optimization
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 贝叶斯优化
- en: '![Refer to caption](img/2699d5d565557c0871b5ed13f8c07c2f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2699d5d565557c0871b5ed13f8c07c2f.png)'
- en: (b) Evolutionary search
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 进化搜索
- en: '![Refer to caption](img/457e52c36c078233b34112cb7e97f317.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/457e52c36c078233b34112cb7e97f317.png)'
- en: (c) Reinforcement learning
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 强化学习
- en: '![Refer to caption](img/bfcabb5188d6d5f386cdc286d1f383bf.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bfcabb5188d6d5f386cdc286d1f383bf.png)'
- en: (d) Differentiable search
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 可微分搜索
- en: 'Figure 4: Schematics of different search algorithms. (a) Bayesian optimization;
    (b) Evolutionary search; (c) Reinforcement learning; (d) Differentiable search.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不同搜索算法的示意图。(a) 贝叶斯优化；(b) 进化搜索；(c) 强化学习；(d) 可微分搜索。
- en: III-B1 Search Algorithms
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 搜索算法
- en: The most intuitive and easiest search algorithm is the grid search, which exhaustively
    explores the search space and evaluates every possible architecture. This approach
    works well for a small space [[68](#bib.bib68)] but is terribly inefficient for
    a large space due to the exponentially increased number of evaluations. Another
    problem with this type of method is that it only supports a bounded and discrete
    space and needs careful selection of the grid interval. Random search [[104](#bib.bib104)],
    on the other hand, samples neural architectures randomly from the search space.
    It can be used not only for a discrete space but also for a continuous space with
    a predefined distribution. This algorithm is superior to the grid search also
    when the search dimensions (e.g., kernel sizes, expansion ratios, and network
    depths) have different effects on the final performance. Random search and its
    simple variants are usually used when the performance of a candidate model is
    easy to obtain [[84](#bib.bib84), [75](#bib.bib75)]. Advanced search algorithms,
    like reinforcement learning and evolutionary search, are widely demonstrated better
    performance than random search [[49](#bib.bib49), [105](#bib.bib105), [106](#bib.bib106)].
    In the following, we summarize the advanced search algorithms, including Bayesian
    optimization, evolutionary search, reinforcement learning, and differentiable
    methods, regarding design automation for efficient deep learning models.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观且最简单的搜索算法是网格搜索，它穷尽地探索搜索空间并评估每一个可能的架构。这种方法在小空间中表现良好[[68](#bib.bib68)]，但对于大空间则效率极低，因为评估数量呈指数增加。此类方法的另一个问题是它仅支持有界的离散空间，并且需要仔细选择网格间隔。另一方面，随机搜索[[104](#bib.bib104)]则是从搜索空间中随机采样神经架构。它不仅可以用于离散空间，还可以用于具有预定义分布的连续空间。当搜索维度（例如，内核大小、扩展比例和网络深度）对最终性能有不同影响时，这种算法优于网格搜索。随机搜索及其简单变体通常在候选模型性能容易获得时使用[[84](#bib.bib84),
    [75](#bib.bib75)]。高级搜索算法，如强化学习和进化搜索，通常表现出比随机搜索更好的性能[[49](#bib.bib49), [105](#bib.bib105),
    [106](#bib.bib106)]。以下，我们总结了用于高效深度学习模型设计自动化的高级搜索算法，包括贝叶斯优化、进化搜索、强化学习和可微方法。
- en: 'Bayesian Optimization (BayesOpt) is an important approach for automated hyperparameter
    tuning and architecture search. Given a search space $\mathbb{S}$ that contains
    a large set of neural architectures, and a black-box objective function $f(\cdot)$
    from an neural architecture to an evaluation metric (e.g., accuracy), the goal
    of BayesOpt is to find an architecture $a\in\mathbb{S}$ that maximizes $f(\textbf{a})$:
    $\textbf{a}^{\ast}=\operatorname*{argmax}_{\textbf{a}\in\mathbb{S}}f(\textbf{a})$.
    However, it is expensive to achieve $f(\textbf{a})$ because it requires fully
    training the architecture a from scratch. Therefore, a statistical model, which
    is invariably a Gaussian process (GP), is used as a surrogate model. The more
    neural architectures (i.e., points of $f(\cdot)$) are evaluated, the less uncertainty
    the surrogate model has. Another important component of BayesOpt is the acquisition
    function for deciding which architectures are sampled and then evaluated at each
    iteration, which is often expected improvement (EI):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化（BayesOpt）是一种用于自动超参数调优和架构搜索的重要方法。给定一个包含大量神经网络架构的搜索空间$\mathbb{S}$，以及一个从神经架构到评估指标（如准确率）的黑箱目标函数$f(\cdot)$，BayesOpt的目标是找到一个架构$a\in\mathbb{S}$，使得$f(\textbf{a})$最大化：$\textbf{a}^{\ast}=\operatorname*{argmax}_{\textbf{a}\in\mathbb{S}}f(\textbf{a})$。然而，获得$f(\textbf{a})$是昂贵的，因为这需要从头开始完全训练架构a。因此，使用了一个统计模型，通常是高斯过程（GP），作为替代模型。评估的神经架构（即$f(\cdot)$的点）越多，替代模型的不确定性就越小。BayesOpt的另一个重要组件是决策哪些架构在每次迭代中被采样和评估的获取函数，通常是期望改进（EI）：
- en: '|  |  | $\displaystyle\text{EI}(\textbf{a})=\mathbb{E}_{max}(f(\textbf{a})-f(\textbf{a}^{+}),\;0),$
    |  | (10) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{EI}(\textbf{a})=\mathbb{E}_{max}(f(\textbf{a})-f(\textbf{a}^{+}),\;0),$
    |  | (10) |'
- en: '|  |  | $\displaystyle\hskip 85.35826pt\textbf{a}^{+}=\operatorname*{argmax}_{\textbf{a}_{i}\in\textbf{a}_{1:t}}f(\textbf{a}_{i}),$
    |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\hskip 85.35826pt\textbf{a}^{+}=\operatorname*{argmax}_{\textbf{a}_{i}\in\textbf{a}_{1:t}}f(\textbf{a}_{i}),$
    |  |'
- en: 'where a is a sampling architecture and $f(\textbf{a}^{+})$ is the evaluation
    metric of the best architecture $\textbf{a}^{+}$ so far, that is until $t$-th
    exploration. The general process of BayesOpt is shown in Fig. [4a](#S3.F4.sf1
    "In Figure 4 ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey"), where multiple architectures are first randomly sampled from
    the efficient search space; the sampled architectures are then trained and evaluated
    for updating a GP model; the acquisition function is computed based on the updated
    GP model to decide which neural architectures should be sampled next; the process
    proceeds iteratively until certain conditions are met. The best architecture among
    all sampled architectures is usually output as the final model.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 a 是一个采样架构，$f(\textbf{a}^{+})$ 是迄今为止最佳架构 $\textbf{a}^{+}$ 的评估指标，即直到 $t$ 次探索。BayesOpt
    的一般过程如图 [4a](#S3.F4.sf1 "在图 4 ‣ III-B 搜索策略 ‣ III 搜索高效深度学习模型 ‣ 设计自动化以实现快速、轻量和高效的深度学习模型：综述")
    所示，其中多个架构首先从高效搜索空间中随机采样；然后对采样架构进行训练和评估以更新 GP 模型；基于更新的 GP 模型计算获取函数，以决定接下来应采样哪些神经架构；该过程迭代进行，直到满足某些条件。所有采样架构中表现最好的架构通常作为最终模型输出。
- en: When additionally considering the hardware constraints, Multi-Objective Bayesian
    Optimization (MOBO) is commonly used [[107](#bib.bib107), [108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)], where GP is fitted for each objective
    independently, and Pareto-frontier is identified as the set of optimal trade-off
    solutions of the multiple objectives. Different MOBO approaches mainly differ
    in how to achieve the Pareto-frontier during the acquisition process. For example,
    Parsa et al. [[109](#bib.bib109)] use a Gaussian distribution to estimate the
    Pareto-frontier function; Eriksson et al. [[110](#bib.bib110)] use the Noisy Expected
    Hypervolume Improvement (NEHVI) acquisition function, which is a noise-tolerant
    and extended version of EI for the multi-objective setting, to sample intermediate
    Pareto-frontiers. Although BayesOpt has shown promising performance, especially
    in hyperparameter optimization, it is computationally expensive and struggling
    when handling a high-dimensional search space and multiple objectives [[111](#bib.bib111),
    [112](#bib.bib112)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当额外考虑硬件约束时，多目标贝叶斯优化（MOBO）通常被使用 [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110)]，其中 GP 被独立地为每个目标进行拟合，帕累托前沿被识别为多个目标的最优折衷解集。不同的 MOBO 方法主要在于如何在获取过程中实现帕累托前沿。例如，Parsa
    等人 [[109](#bib.bib109)] 使用高斯分布来估计帕累托前沿函数；Eriksson 等人 [[110](#bib.bib110)] 使用噪声容忍和扩展版的
    EI 的多目标设置下的期望超体积改进（NEHVI）获取函数来采样中间帕累托前沿。尽管 BayesOpt 在超参数优化中表现出了良好的前景，但它在处理高维搜索空间和多个目标时计算开销较大，效果不佳
    [[111](#bib.bib111), [112](#bib.bib112)]。
- en: 'Evolutionary Search (ES) is a long-thriving NAS approach, which is based on
    the concept of biological evolution. It manages to find an optimal solution by
    iteratively improving upon a population of candidate solutions according to a
    fitness function. As illustrated in Fig. [4b](#S3.F4.sf2 "In Figure 4 ‣ III-B
    Search Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") (the schematic
    of the basic ES process), a population of candidate networks is first initialized
    by randomly sampled from the efficient search space; then evolutionary operations,
    such as crossover (combination of two parents) and mutation (arbitrarily mutating
    operators with new ones from the search space), are applied to current population
    to generate next generation; lastly, all offsprings need to be tested against
    a fitness function, which is to select the most powerful candidate networks and
    update the population; the process proceeds repeatedly until stopping criteria
    are met. The best network in the last population is the final achieved model.
    The major benefit of evolutionary search is its flexibility in directly controlling
    the offspring generation process and population updating process [[105](#bib.bib105)].'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 进化搜索（ES）是一种长期发展的神经架构搜索（NAS）方法，它基于生物进化的概念。它通过根据适应度函数迭代地改进一组候选解，来寻找最佳解决方案。如图 [4b](#S3.F4.sf2
    "在图 4 ‣ III-B 搜索策略 ‣ III 高效深度学习模型的搜索 ‣ 快速、轻量级和有效深度学习模型的设计自动化：一项调查")（基本ES过程的示意图）所示，首先通过在高效搜索空间中随机抽样来初始化一组候选网络；然后，对当前种群应用进化操作，例如交叉（两个父母的组合）和变异（用来自搜索空间的新操作员任意变异），以生成下一代；最后，所有后代需要根据适应度函数进行测试，以选择最强的候选网络并更新种群；这个过程不断重复，直到满足停止准则。最后一代中的最佳网络就是最终得到的模型。进化搜索的主要优点在于其直接控制后代生成过程和种群更新过程的灵活性
    [[105](#bib.bib105)]。
- en: 'The hardware constraints are usually considered in the fitness function of
    ES in two ways: hard-constraint and soft-constraint, which we will present concretely
    in the flowing section [III-B2](#S3.SS2.SSS2 "III-B2 Hardware-constraint Incorporation
    Strategy ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey"). Thus, most studies directly use existing evolutionary algorithms with
    customised fitness function to incorporate hardware constraints [[78](#bib.bib78),
    [91](#bib.bib91), [81](#bib.bib81), [113](#bib.bib113), [77](#bib.bib77), [114](#bib.bib114),
    [105](#bib.bib105), [115](#bib.bib115)]. For example, FBNetV3 [[78](#bib.bib78)]
    uses the adaptive genetic algorithms [[116](#bib.bib116)] to realize adaptive
    probabilities of crossover and mutation, and customizes the fitness function with
    its proposed accuracy predictor and hard hardware constraints; OFA [[77](#bib.bib77)]
    adopts the regularized evolutionary search [[117](#bib.bib117)], which introduces
    an age property to favor younger generation during search; DONNA [[113](#bib.bib113)],
    FairNAS [[118](#bib.bib118)], and MOGA [[76](#bib.bib76)] appeal to the famous
    NSGA-\@slowromancapii@ [[119](#bib.bib119)], which is a multi-objective evolutionary
    algorithm, to find the Pareto-optimal solution instead of the solution under hard
    constraints in previous studies. In contrast to directly using off-the-shelf ES
    algorithms, some researchers develop ES algorithms specific to the hardware efficient
    application [[64](#bib.bib64), [61](#bib.bib61), [62](#bib.bib62)]. LEMONADE [[64](#bib.bib64)]
    is such a representative work, which handles various objectives differently considering
    that different objectives have different evaluation costs. Specifically, the efficiency
    objective (e.g., FLOPs) is cheap to evaluate while the accuracy objective is much
    more expensive as it requires fully training the model at each iteration. The
    authors propose to first select the architectures that fulfil the Pareto front
    for the cheap objectives and then only train and evaluate these networks. As traditional
    ES can only sample a limited amount of networks that satisfy hardware constraints,
    Scheidegger et al. [[62](#bib.bib62)] use ES algorithms to search sampling laws
    that can better cover the sub search space under specific constraints. Thus, better
    Pareto frontiers can be achieved. NSAG-Net [[61](#bib.bib61)] designs an additional
    exploitation step after the traditional evolutionary operating to learn and leverage
    the history of evaluated populations. Since the evaluated populations at each
    iteration rank relatively high regarding the fitness score, it is quite possible
    that the optimal model has similarities to the evaluated populations and thus
    extracting common patterns from the evaluated populations can accelerate convergence.
    Although the above studies manage to lighten the search process from the algorithm
    perspective, it is still costly due to the requirement of training each network
    of the new generation at each interaction.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件约束通常以两种方式在ES的适应函数中考虑：硬约束和软约束，在下一节[III-B2](#S3.SS2.SSS2 "III-B2硬件约束融合策略 ‣ III-B搜索策略
    ‣ III寻找高效深度学习模型 ‣ 快速、轻量和有效深度学习模型的设计自动化：一项调查")中我们将具体介绍。因此，大多数研究直接使用现有的带有定制适应函数的进化算法来融合硬件约束[[78](#bib.bib78),
    [91](#bib.bib91), [81](#bib.bib81), [113](#bib.bib113), [77](#bib.bib77), [114](#bib.bib114),
    [105](#bib.bib105), [115](#bib.bib115)]。例如，FBNetV3[[78](#bib.bib78)]使用自适应遗传算法[[116](#bib.bib116)]实现交配和突变的自适应概率，并根据其提出的准确度预测器和硬件约束定制适应函数；OFA[[77](#bib.bib77)]采用正则化进化搜索[[117](#bib.bib117)]，在搜索过程中引入了年龄属性，以偏爱更年轻的代；DONNA[[113](#bib.bib113)]，FairNAS[[118](#bib.bib118)]和MOGA[[76](#bib.bib76)]采用了著名的多目标进化算法NSGA-\@slowromancapii@[[119](#bib.bib119)]，而不是以前的研究中在硬约束下找到解决方案。与直接使用现成的ES算法不同，一些研究人员开发了针对硬件高效应用的ES算法[[64](#bib.bib64),
    [61](#bib.bib61), [62](#bib.bib62)]。LEMONADE[[64](#bib.bib64)]就是这样一个代表作，考虑到不同的目标有不同的评估成本，该算法以不同的方式处理各种目标。特别是，效率目标（如FLOPs）便宜评估，而准确度目标则需要完全训练模型，成本更高。作者建议先选择满足便宜目标Pareto前沿的架构，然后仅训练和评估这些网络。由于传统的ES只能对满足硬件约束的有限数量的网络进行抽样，Scheidegger等人[[62](#bib.bib62)]使用ES算法搜索能更好地覆盖特定约束下的子搜索空间的抽样规律。因此可以获得更好的Pareto前沿。NSAG-Net[[61](#bib.bib61)]在传统的进化操作后设计了额外的开发步骤，以学习并利用已评估种群的历史。由于每次迭代的评估种群在健身得分上排名相对较高，所以最优模型很可能与评估种群具有相似之处，因此从评估种群中提取共同模式可以加速收敛。尽管上述研究从算法角度成功地使搜索过程变得更轻松，但由于需要在每次交互中训练新一代的每个网络，仍然成本很高。
- en: 'Reinforcement Learning (RL) has achieved notable success and grabbed great
    attention in the NAS community since Zoph and Le’s work in 2017 [[52](#bib.bib52)].
    The general framework of RL for NAS is illustrated in Fig. [4c](#S3.F4.sf3 "In
    Figure 4 ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey"): the controller, usually an RNN network, is the core component, which
    generates actions to sample operators or hyperparameters from the efficient search
    space; a neural network will then be constructed using the sampled options and
    trained and evaluated to achieve a reward, which will be used to update the controller
    using a policy gradient method [[120](#bib.bib120)]. This process will be performed
    repeatedly until stopping criteria are met.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Zoph和Le在2017年的工作[[52](#bib.bib52)]以来，强化学习（RL）在NAS社区取得了显著成功，并引起了广泛关注。RL用于NAS的总体框架如图[4c](#S3.F4.sf3
    "在图4 ‣ III-B搜索策略 ‣ III高效深度学习模型的搜索 ‣ 快速、轻量和有效深度学习模型的设计自动化：一项调查")所示：控制器，通常是RNN网络，是核心组件，它生成动作以从高效搜索空间中采样操作符或超参数；然后将使用采样选项构建神经网络，并进行训练和评估以获得奖励，该奖励将用于通过策略梯度方法[[120](#bib.bib120)]更新控制器。这个过程将重复进行，直到满足停止标准。
- en: The hardware information is often considered in the RL reward function. Some
    studies thus directly use the existing RL-based NAS approach [[52](#bib.bib52)],
    but optimize with a customized reward function [[88](#bib.bib88), [65](#bib.bib65)].
    Furthermore, most recent work mainly follows the advanced RL search algorithms
    in two representative studies, MnasNet [[48](#bib.bib48)] and TuNAS [[106](#bib.bib106)].
    Specifically, [[80](#bib.bib80), [72](#bib.bib72)] follow MnasNet, and [[69](#bib.bib69),
    [31](#bib.bib31)] follow TuNAS. Similar to [[52](#bib.bib52)], MnasNet [[48](#bib.bib48)]
    also uses RNN as the controller while maximizing the expected reward using an
    advanced policy gradient method, Proximal Policy Optimization (PPO) [[121](#bib.bib121)],
    to alleviate the high gradient variance of the vanilla policy gradient method.
    TuNAS is based on ENAS [[122](#bib.bib122)] and ProxylessNAS [[23](#bib.bib23)].
    It improves with warmup and channel masking techniques for better search robustness
    and scalability. Instead of building one candidate network at each training step,
    TuNAS encompasses all candidate networks into a supernet. Each path of the supernet
    represents a candidate network. In addition, TuNAS is not a sequential decision-making
    process and does not rely on an RNN controller; alternatively, it uses learnable
    probability distribution that spans over architectural choices as the RL controller.
    At each step, a candidate network is sampled from the distribution, and then the
    portion of the supernet correlated with the sampled network is trained; a reward
    is calculated with the sampled network to update the probability distribution
    i.e., the RL controller.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件信息通常会考虑到RL奖励函数中。因此，一些研究直接使用现有的基于RL的NAS方法[[52](#bib.bib52)]，但通过定制的奖励函数进行优化[[88](#bib.bib88),
    [65](#bib.bib65)]。此外，大多数最新工作主要遵循两个具有代表性的研究中的高级RL搜索算法，即MnasNet[[48](#bib.bib48)]和TuNAS[[106](#bib.bib106)]。具体而言，[[80](#bib.bib80),
    [72](#bib.bib72)]遵循MnasNet，而[[69](#bib.bib69), [31](#bib.bib31)]遵循TuNAS。类似于[[52](#bib.bib52)]，MnasNet[[48](#bib.bib48)]也使用RNN作为控制器，同时利用先进的策略梯度方法Proximal
    Policy Optimization (PPO) [[121](#bib.bib121)]来最大化期望奖励，以减轻普通策略梯度方法的高梯度方差。TuNAS基于ENAS[[122](#bib.bib122)]和ProxylessNAS[[23](#bib.bib23)]。它通过预热和通道掩蔽技术来提高搜索的鲁棒性和可扩展性。与在每个训练步骤中构建一个候选网络不同，TuNAS将所有候选网络纳入一个超网。超网的每条路径代表一个候选网络。此外，TuNAS不是一个顺序决策过程，也不依赖于RNN控制器；相反，它使用跨越架构选择的可学习概率分布作为RL控制器。在每一步，从分布中采样一个候选网络，然后训练与采样网络相关的超网部分；使用采样网络计算奖励，以更新概率分布，即RL控制器。
- en: 'Differentiable Search is the most recently developed search paradigm that finds
    an optimal network by gradient descent [[54](#bib.bib54)]. As shown in Figure
    [4d](#S3.F4.sf4 "In Figure 4 ‣ III-B Search Strategy ‣ III Search for Efficient
    Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey"), a network is as a directed acyclic graph consisting
    of an ordered sequence of $N$ nodes ($N=4$ in Figure [4d](#S3.F4.sf4 "In Figure
    4 ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design
    Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey"):
    $X_{0},X_{1},X_{2},X_{3}$). Each node $X_{i}$ is a latent representation and each
    directed edge represents a candidate operator $O_{i}$ that is applied to $X_{i}$.
    For example, in Figure [4d](#S3.F4.sf4 "In Figure 4 ‣ III-B Search Strategy ‣
    III Search for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey"), there are three candidate operators
    (i.e., $O_{1}$, $O_{2}$, and $O_{3}$) between node $X_{0}$ and $X_{3}$. In this
    way, the search process is formulated as an optimal path-finding problem. All
    outputs of the candidate operators from a node’s predecessors are summed with
    weights $\alpha_{i}$ to achieve the node representation: $X_{j}=\sum_{i=1}^{m}\alpha_{i}O_{i}(X_{i})$,
    subject to $\alpha_{i}\geq 0,\sum_{i=1}^{m}\alpha_{i}=1$. In Figure [4d](#S3.F4.sf4
    "In Figure 4 ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey"), edges with darker colours indicate larger weights. The weights
    $\alpha_{i}$ can be represented with a softmax function of architectural parameters
    $\beta_{i}$: $\alpha_{i}=\frac{exp(\beta_{i})}{\sum_{i=1}^{m}exp(\beta_{i})}$.
    The network weights ($w$) and architectural parameters ($\beta_{i}$) are trained
    alternatively with training data and validation data, respectively. This induces
    a bi-level optimization problem [[54](#bib.bib54)]:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 可微分搜索是最近开发的搜索范式，通过梯度下降找到最优网络[[54](#bib.bib54)]。如图 [4d](#S3.F4.sf4 "在图 4 ‣ III-B
    搜索策略 ‣ III 寻找高效深度学习模型 ‣ 快速、轻量和有效深度学习模型设计自动化：综述") 所示，网络作为一个有向无环图，由有序的 $N$ 个节点组成（图
    [4d](#S3.F4.sf4 "在图 4 ‣ III-B 搜索策略 ‣ III 寻找高效深度学习模型 ‣ 快速、轻量和有效深度学习模型设计自动化：综述")
    中 $N=4$）：$X_{0},X_{1},X_{2},X_{3}$）。每个节点 $X_{i}$ 是一个潜在表示，每个有向边表示一个应用于 $X_{i}$
    的候选运算符 $O_{i}$。例如，在图 [4d](#S3.F4.sf4 "在图 4 ‣ III-B 搜索策略 ‣ III 寻找高效深度学习模型 ‣ 快速、轻量和有效深度学习模型设计自动化：综述")
    中，节点 $X_{0}$ 和 $X_{3}$ 之间有三个候选运算符（即 $O_{1}$、$O_{2}$ 和 $O_{3}$）。通过这种方式，搜索过程被表述为一个最优路径寻址问题。来自节点前驱的候选运算符的所有输出与权重
    $\alpha_{i}$ 相加以实现节点表示：$X_{j}=\sum_{i=1}^{m}\alpha_{i}O_{i}(X_{i})$，其中 $\alpha_{i}\geq
    0,\sum_{i=1}^{m}\alpha_{i}=1$。在图 [4d](#S3.F4.sf4 "在图 4 ‣ III-B 搜索策略 ‣ III 寻找高效深度学习模型
    ‣ 快速、轻量和有效深度学习模型设计自动化：综述") 中，颜色较深的边表示权重较大。权重 $\alpha_{i}$ 可以用建筑参数 $\beta_{i}$
    的 softmax 函数表示：$\alpha_{i}=\frac{exp(\beta_{i})}{\sum_{i=1}^{m}exp(\beta_{i})}$。网络权重（$w$）和建筑参数（$\beta_{i}$）分别使用训练数据和验证数据交替训练。这引发了一个双层优化问题
    [[54](#bib.bib54)]。
- en: '|  | $\min_{\alpha}\min_{w_{\alpha}}\mathbfcal{L}(\alpha,w_{\alpha}).$ |  |
    (11) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\alpha}\min_{w_{\alpha}}\mathbfcal{L}(\alpha,w_{\alpha}).$ |  |
    (11) |'
- en: Therefore, unlike ES and RL, a differentiable search unifies model training
    and search into a joint procedure. As the whole network contains multiple parallel
    operators in each layer, it is called a supernet, and its subnet with one operator
    in each layer is a candidate network. The candidate operator with the highest
    associated weight is chosen to construct the final model. The training of the
    supernet is also the search process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与ES和RL不同，可微分搜索将模型训练和搜索统一为一个联合过程。由于整个网络在每一层都包含多个并行运算符，它被称为超网络，而其每层只有一个运算符的子网络是候选网络。选择具有最高相关权重的候选运算符来构建最终模型。超网络的训练也是搜索过程。
- en: 'The hardware information can be considered in the loss function and optimized
    when training the architectural parameters. Various differentiable search algorithms
    are proposed to achieve better search efficiency and performance [[79](#bib.bib79),
    [86](#bib.bib86), [23](#bib.bib23), [66](#bib.bib66), [92](#bib.bib92), [67](#bib.bib67)].
    FBNetV1 [[86](#bib.bib86)] replaces the softmax function with the Gumbel softmax
    function [[123](#bib.bib123)] to better represents the subnet sampling process,
    and thus reduce the performance gap between the supernet and subnet. HR-NAS [[92](#bib.bib92)]
    progressively discards the paths with low path weights during the search to increase
    the search efficiency. DenseNAS [[67](#bib.bib67)] splits the search procedure
    into two stages: the first stage optimizes the model weights only for enough epochs
    and the second stage alternatively optimizes the model weights and architectural
    parameters. This strategy alleviates the bias of fast convergence operators. Considering
    the small size of the search space of the conventional differentiable search due
    to its requirement of loading the whole supernet into memory, many studies propose
    novel differentiable search algorithms to reduce the memory footprint [[23](#bib.bib23),
    [66](#bib.bib66), [79](#bib.bib79), [67](#bib.bib67)]. ProxylessNAS [[23](#bib.bib23)]
    proposes to factorize the task of training of all paths into training two sampled
    paths, which have the highest sampling probabilities, and discard the other paths
    temporarily at each iteration. The architectural parameters of the two selected
    paths are updated during training and then rescaled after training to keep the
    path weights of unsampled paths unchanged. In this way, the memory requirement
    is decreased to the level of training two subnets. Different from previous papers,
    which use multi-path supernets and thereby have memory issues, Single-Path NAS
    [[66](#bib.bib66)] and FBNetV2 [[79](#bib.bib79)] develop masking mechanisms that
    train a supernet with the largest hyperparameters (e.g., $7\times 7$ kernel) and
    a mask/indicator function that determines whether to just use a small part (e.g.,
    $5\times 5$ or $3\times 3$ kernel) of the largest hyperparameters.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件信息可以被考虑到损失函数中，并在训练架构参数时进行优化。提出了各种可微分的搜索算法以实现更好的搜索效率和性能 [[79](#bib.bib79),
    [86](#bib.bib86), [23](#bib.bib23), [66](#bib.bib66), [92](#bib.bib92), [67](#bib.bib67)]。FBNetV1
    [[86](#bib.bib86)] 用 Gumbel softmax 函数 [[123](#bib.bib123)] 替代了 softmax 函数，以更好地表示子网采样过程，从而减少了超网和子网之间的性能差距。HR-NAS
    [[92](#bib.bib92)] 在搜索过程中逐步丢弃低路径权重的路径，以提高搜索效率。DenseNAS [[67](#bib.bib67)] 将搜索过程分为两个阶段：第一阶段仅优化模型权重足够的轮次，第二阶段交替优化模型权重和架构参数。这一策略缓解了快速收敛操作的偏差。考虑到传统可微分搜索因需要将整个超网加载到内存中而导致的搜索空间较小，许多研究提出了新型可微分搜索算法以减少内存占用
    [[23](#bib.bib23), [66](#bib.bib66), [79](#bib.bib79), [67](#bib.bib67)]。ProxylessNAS
    [[23](#bib.bib23)] 提出了将所有路径的训练任务分解为训练两个采样概率最高的路径，并在每次迭代中暂时丢弃其他路径的方案。这两个选择路径的架构参数在训练过程中进行更新，训练后重新缩放，以保持未采样路径的路径权重不变。这样，内存需求降低到训练两个子网的水平。与以往使用多路径超网并因此存在内存问题的论文不同，Single-Path
    NAS [[66](#bib.bib66)] 和 FBNetV2 [[79](#bib.bib79)] 开发了掩码机制，训练具有最大超参数（例如 $7\times
    7$ 核）的超网，并使用掩码/指示函数决定是否仅使用最大超参数的一小部分（例如 $5\times 5$ 或 $3\times 3$ 核）。
- en: Other types of search algorithms are also reported [[87](#bib.bib87), [85](#bib.bib85)].
    Xiong et al. [[87](#bib.bib87)] propose a modified Cost-Effective Greedy algorithm
    for the submodular NAS process, where starting from an empty network, each block
    is filled iteratively with the highest marginal gain ratio regarding both accuracy
    and cost. DPP-Net [[85](#bib.bib85)] follows the progressive search algorithm
    in [[100](#bib.bib100)] to find the optimal operator layer by layer. Different
    from searching for a new model for each new resource budget, another way is to
    build a baseline model first and then scale it to obtain a family of models for
    various budgets [[68](#bib.bib68), [70](#bib.bib70)]. Specifically, they adopt
    the conventional RL-based hardware-aware search algorithm [[48](#bib.bib48)] to
    search for a small baseline model, and use a simple grid search to determine the
    best scaling coefficient (i.e., $\alpha$, $\beta$, $\gamma$) for network depth,
    width, and input size. A family of specialized models for different budgets can
    be obtained by scaling up the baseline model by $\alpha^{N}$, $\beta^{N}$, and
    $\gamma^{N}$ with $2^{N}$ times increased resource.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的搜索算法也有报道[[87](#bib.bib87), [85](#bib.bib85)]。Xiong等人[[87](#bib.bib87)]提出了一种改进的成本效益贪心算法，用于子模块化NAS过程，其中从空网络开始，每个块逐步填充，以获得关于准确性和成本的最高边际收益比。DPP-Net
    [[85](#bib.bib85)]遵循了[[100](#bib.bib100)]中的渐进式搜索算法，逐层寻找最佳操作符。与为每个新的资源预算搜索新模型不同，另一种方法是先建立一个基线模型，然后通过缩放它来获得不同预算的模型系列[[68](#bib.bib68),
    [70](#bib.bib70)]。具体来说，他们采用传统的基于RL的硬件感知搜索算法[[48](#bib.bib48)]来搜索一个小型基线模型，并使用简单的网格搜索来确定最佳的缩放系数（即，$\alpha$，$\beta$，$\gamma$）用于网络深度、宽度和输入大小。通过将基线模型按$\alpha^{N}$，$\beta^{N}$和$\gamma^{N}$的比例缩放，并使用$2^{N}$倍的资源，可以获得不同预算的专业化模型系列。
- en: III-B2 Hardware-constraint Incorporation Strategy
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 硬件约束纳入策略
- en: Despite some studies only using an efficient search space without considering
    hardware constraint [[49](#bib.bib49)], there are two strategies commonly used
    to consider hardware budget for finding optimal compact models. The first considers
    a specific hardware platform and treats its resources as hard constraint to build
    a specialized model that is the most accurate under the fixed constraint. In contrast,
    the second strategy is to directly search networks without considering specific
    hardware constraint. This strategy, dubbed soft-constraint incorporation, treats
    the model efficiency as an additional optimization objective and tries to find
    the Pareto frontiers. In this section, we summarize the design automation techniques
    from the above two perspectives.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些研究仅使用有效的搜索空间而不考虑硬件约束[[49](#bib.bib49)]，但通常有两种策略用于考虑硬件预算以找到最佳紧凑模型。第一种是考虑特定的硬件平台，将其资源视为硬约束，以建立在固定约束下最准确的专业模型。相反，第二种策略是直接搜索网络，而不考虑特定的硬件约束。这种策略被称为软约束纳入，视模型效率为附加的优化目标，并试图找到帕累托前沿。在本节中，我们从上述两个角度总结了设计自动化技术。
- en: 'Hard-constraint Incorporation can be easily adopted by evolutionary search
    [[81](#bib.bib81), [91](#bib.bib91), [77](#bib.bib77), [78](#bib.bib78), [62](#bib.bib62)]
    since the offspring generation and selection step can be directly controlled.
    The incorporation process is quite straightforward: when a set of candidate networks
    is generated (e.g., by evolutionary operating), their hardware costs are then
    tested, and the networks that do not meet the constraint are simply discarded.
    This strategy is also easy to implement for other search algorithms, which generate
    a set of candidate networks one time or at each iteration, such as random search
    [[84](#bib.bib84), [75](#bib.bib75)], grid search [[68](#bib.bib68)], and greedy
    search [[87](#bib.bib87)]. The RL-based search algorithms can also embrace the
    hard-constraint incorporation strategy but in a different way [[65](#bib.bib65),
    [48](#bib.bib48), [88](#bib.bib88)]. The general idea is to have only the accuracy
    in the reward function when the constraint is met; otherwise, both accuracy and
    the hardware objective are considered in the reward function. A typical reward
    formula $\mathbfcal{R}(\cdot)$ is proposed in MnasNet [[48](#bib.bib48)]:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 硬约束整合可以很容易地被进化搜索采用[[81](#bib.bib81), [91](#bib.bib91), [77](#bib.bib77), [78](#bib.bib78),
    [62](#bib.bib62)]，因为后代生成和选择步骤可以直接控制。整合过程非常简单：当生成一组候选网络（例如，通过进化操作）时，测试它们的硬件成本，不符合约束的网络会被直接丢弃。这个策略对于其他生成候选网络的搜索算法也很容易实现，例如随机搜索[[84](#bib.bib84),
    [75](#bib.bib75)]，网格搜索[[68](#bib.bib68)]和贪婪搜索[[87](#bib.bib87)]。基于强化学习的搜索算法也可以采用硬约束整合策略，但方式有所不同[[65](#bib.bib65),
    [48](#bib.bib48), [88](#bib.bib88)]。一般的思路是，当约束满足时，奖励函数中仅包含准确度；否则，奖励函数中同时考虑准确度和硬件目标。MnasNet[[48](#bib.bib48)]中提出了一个典型的奖励公式$\mathbfcal{R}(\cdot)$：
- en: '|  | $\mathbfcal{R}(a)=\begin{cases}ACC(a),&amp;\text{if}\;\;C(a)\leq C_{0}\\
    ACC(a)\times(C(a)/C_{0})^{\beta},&amp;\text{if}\;\;C(a)>C_{0}\end{cases}$ |  |
    (12) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbfcal{R}(a)=\begin{cases}ACC(a),&\text{if}\;\;C(a)\leq C_{0}\\ ACC(a)\times(C(a)/C_{0})^{\beta},&\text{if}\;\;C(a)>C_{0}\end{cases}$
    |  | (12) |'
- en: where $ACC(a)$ and $C(a)$ denote the accuracy and hardware cost (e.g., latency
    in [[48](#bib.bib48)]) of model $a$, respectively, and $C_{0}$ denotes the target
    cost constraint. $\beta<0$ is the only tunable hyperparameter that controls the
    convergence speed. In [[106](#bib.bib106)], the authors empirically find that
    if $\beta$ is too large, the RL-controller will prefer to sample the architectures
    whose cost is significantly smaller than the target cost constraint. This will
    result in suboptimal models regarding accuracy.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$ACC(a)$和$C(a)$分别表示模型$a$的准确度和硬件成本（例如[[48](#bib.bib48)]中的延迟），$C_{0}$表示目标成本约束。$\beta<0$是唯一可调的超参数，用于控制收敛速度。在[[106](#bib.bib106)]中，作者通过实验证明，如果$\beta$过大，RL控制器会倾向于选择成本远低于目标成本约束的架构，这将导致准确度方面的次优模型。
- en: 'Soft-constraint Incorporation does not target any specific constraint and is
    realized by adding an additional optimization objective to the accuracy objective.
    Since these objectives are competing, no unique optimal solution exists in the
    multi-objective space. Thus, Pareto frontiers are sought, especially by multi-objective
    BayesOpt and multi-objective evolutionary algorithms, and one specific model can
    be identified according to different application requirements [[61](#bib.bib61),
    [109](#bib.bib109), [113](#bib.bib113), [76](#bib.bib76), [107](#bib.bib107),
    [64](#bib.bib64), [108](#bib.bib108), [115](#bib.bib115), [118](#bib.bib118)].
    For example, ChamNet [[115](#bib.bib115)] designs a multi-objective fitness function:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 软约束整合并不针对任何特定约束，而是通过将额外的优化目标添加到准确度目标中实现。由于这些目标是竞争的，因此在多目标空间中不存在唯一的最优解。因此，特别是通过多目标贝叶斯优化和多目标进化算法来寻找帕累托前沿，并根据不同的应用需求确定特定模型[[61](#bib.bib61),
    [109](#bib.bib109), [113](#bib.bib113), [76](#bib.bib76), [107](#bib.bib107),
    [64](#bib.bib64), [108](#bib.bib108), [115](#bib.bib115), [118](#bib.bib118)]。例如，ChamNet[[115](#bib.bib115)]设计了一个多目标适应度函数：
- en: '|  | $\mathbfcal{F}(a)=ACC(a)-[\alpha H(C(a)-C_{0})]^{\beta},$ |  | (13) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbfcal{F}(a)=ACC(a)-[\alpha H(C(a)-C_{0})]^{\beta},$ |  | (13) |'
- en: 'where $C(a)$ and $C_{0}$ denote the hardware cost of model $a$ and the target
    constraint, respectively, $H$ is the Heaviside step function, and $\alpha$ and
    $\beta$ are positive constants. The searching objective is to maximize the fitness
    function: $a^{*}=\operatorname*{argmax}_{a}(\mathbfcal{F}(a))$. This fitness function
    guides the search process to find a model approaching the hardware target but
    without a hard guarantee. Some work also pursues the soft-constraint objective
    under a hard constraint with a two-phase approach, which first filters out models
    that do not satisfy the hard constraint and then performs multi-objective optimization
    [[85](#bib.bib85), [62](#bib.bib62)]. Instead of finding a set of solutions, some
    studies attempt to find the best tradeoff between efficiency and effectiveness.
    Two schemes are commonly used in this regard and mainly for the RL-based and differentiable
    algorithms:multiplication and linear combination. For the multiplication scheme,
    the effectiveness objective (e.g., accuracy) and efficiency objective (e.g., latency)
    are multiplied to construct the loss function [[86](#bib.bib86)] or reward function
    [[72](#bib.bib72), [48](#bib.bib48), [80](#bib.bib80), [70](#bib.bib70)]:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C(a)$ 和 $C_{0}$ 分别表示模型 $a$ 的硬件成本和目标约束，$H$ 是海维赛德阶跃函数，$\alpha$ 和 $\beta$ 是正的常数。搜索目标是最大化适应度函数：$a^{*}=\operatorname*{argmax}_{a}(\mathbfcal{F}(a))$。该适应度函数指导搜索过程以寻找接近硬件目标的模型，但没有硬性保证。一些工作也在硬约束下追求软约束目标，采用两阶段方法，首先筛选出不满足硬约束的模型，然后进行多目标优化
    [[85](#bib.bib85), [62](#bib.bib62)]。而不是寻找一组解决方案，一些研究尝试在效率和效果之间找到最佳折衷。为此，通常使用两种方案，主要用于基于
    RL 和可微分算法：乘法和线性组合。对于乘法方案，效果目标（例如准确性）和效率目标（例如延迟）被相乘以构建损失函数 [[86](#bib.bib86)] 或奖励函数
    [[72](#bib.bib72), [48](#bib.bib48), [80](#bib.bib80), [70](#bib.bib70)]：
- en: '|  | $\text{loss function: }\mathbfcal{L}(a)=\mathbfcal{L}_{CE}(w&#124;a)\times\text{log}(C(a))^{\beta},$
    |  | (14) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{损失函数: }\mathbfcal{L}(a)=\mathbfcal{L}_{CE}(w&#124;a)\times\text{log}(C(a))^{\beta},$
    |  | (14) |'
- en: '|  | $\text{reward function: }\mathbfcal{R}(a)=ACC(a)\times(C(a)/C_{0})^{\beta}.$
    |  | (15) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{奖励函数: }\mathbfcal{R}(a)=ACC(a)\times(C(a)/C_{0})^{\beta}.$ |  |
    (15) |'
- en: 'The $\mathbfcal{L}_{CE}(w|a)$ is the cross-entropy loss of model $a$ with parameter
    $w$. The exponent coefficient ($\beta>0$ for the loss function; $\beta<0$ for
    the reward function) modulates the trade-off between effectiveness and efficiency.
    Note that although there is a target constraint term $C_{0}$ in the reward function,
    this is still a soft constraint since this just pushes the cost to be lower than
    the target but without any guarantee. Alternatively, it is possible to keep running
    the search process until the hardware constraint is satisfied [[72](#bib.bib72)].
    For the linear combination scheme, the two competing objectives are linearly combined
    to construct the loss function [[66](#bib.bib66), [67](#bib.bib67), [79](#bib.bib79),
    [92](#bib.bib92), [23](#bib.bib23)] or reward function [[69](#bib.bib69), [106](#bib.bib106),
    [31](#bib.bib31)]:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbfcal{L}_{CE}(w|a)$ 是模型 $a$ 在参数 $w$ 下的交叉熵损失。指数系数（对于损失函数 $\beta>0$；对于奖励函数
    $\beta<0$）调节效果与效率之间的权衡。注意，虽然奖励函数中存在一个目标约束项 $C_{0}$，但这仍然是一个软约束，因为它只是将成本推到低于目标的水平，但没有任何保证。或者，也可以继续进行搜索过程，直到满足硬件约束
    [[72](#bib.bib72)]。对于线性组合方案，这两个竞争目标被线性组合以构建损失函数 [[66](#bib.bib66), [67](#bib.bib67),
    [79](#bib.bib79), [92](#bib.bib92), [23](#bib.bib23)] 或奖励函数 [[69](#bib.bib69),
    [106](#bib.bib106), [31](#bib.bib31)]：
- en: '|  | $\text{loss function: }\mathbfcal{L}(a)=\mathbfcal{L}_{CE}(w&#124;a)+\beta\>\text{log}(C(a)),$
    |  | (16) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{损失函数: }\mathbfcal{L}(a)=\mathbfcal{L}_{CE}(w&#124;a)+\beta\>\text{log}(C(a)),$
    |  | (16) |'
- en: '|  | $\text{reward function: }\mathbfcal{R}(a)=ACC(a)+\beta&#124;C(a)/C_{0}-1&#124;.$
    |  | (17) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{奖励函数: }\mathbfcal{R}(a)=ACC(a)+\beta&#124;C(a)/C_{0}-1&#124;.$
    |  | (17) |'
- en: Similar to the multiplication scheme, the $\beta$ coefficient balances the effectiveness
    and efficiency. The log($\cdot$) function in the hardware-related term in the
    loss function is used to scale the cost and are omitted in some studies [[79](#bib.bib79),
    [23](#bib.bib23), [92](#bib.bib92)]. TuNAS [[106](#bib.bib106)] empirically demonstrates
    that search results are robust to the exact value of $\beta$. It shows that the
    same $\beta$ value works great for different search spaces and hardware constraints.
    This $\beta$ value-invariance alleviates the tedious tuning of $\beta$ for new
    scenarios (e.g., new devices).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于乘法方案，$\beta$系数在效果和效率之间进行平衡。在损失函数中与硬件相关的项使用了log($\cdot$)函数来缩放成本，在一些研究中被省略了[[79](#bib.bib79),
    [23](#bib.bib23), [92](#bib.bib92)]。TuNAS [[106](#bib.bib106)] 实证表明，搜索结果对$\beta$的精确值具有鲁棒性。它表明，相同的$\beta$值在不同的搜索空间和硬件约束下效果都很好。这种$\beta$值的不变性减轻了在新场景（例如新设备）下对$\beta$的繁琐调优。
- en: 'For non-differentiable search algorithms, the hardware constraint can be smoothly
    incorporated; by contrast, differentiable search algorithms require the cost term
    to be differentiable, which is intrinsically not. Therefore, specific cost estimation
    strategies are expected to resolve this contradiction, which we will introduce
    in Section [III-C2](#S3.SS3.SSS2 "III-C2 Performance Predictor ‣ III-C Performance
    Estimation Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey").'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '对于非可微分的搜索算法，硬件约束可以顺利地融入其中；相比之下，可微分的搜索算法需要成本项是可微分的，而这在本质上是不可能的。因此，预计会有特定的成本估计策略来解决这一矛盾，我们将在第[III-C2](#S3.SS3.SSS2
    "III-C2 Performance Predictor ‣ III-C Performance Estimation Strategy ‣ III Search
    for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")节介绍。'
- en: III-C Performance Estimation Strategy
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 性能估计策略
- en: 'The search process is managed by search strategies, which we have discussed
    in Section [III-B](#S3.SS2 "III-B Search Strategy ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey"), and guided by the performance of candidate models,
    which we will discuss in this section. The performance in the context of hardware-efficient
    models has two aspects: task-related performance (e.g., accuracy) and hardware-related
    performance (e.g., latency). The most natural way to obtain a model’s performance
    is to fully train the model and evaluate it on validation data for task-related
    performance and deploy it on the target hardware for hardware-related performance
    (for direct metrics, e.g., latency) [[107](#bib.bib107), [61](#bib.bib61)]. However,
    it demands massive computation and time to fully train each candidate model from
    scratch and deploy them on target hardware. Therefore, efforts are made toward
    alleviating the performance estimation process. We categorize and discuss these
    efforts into two routes: one is to speed up the training process (section [III-C1](#S3.SS3.SSS1
    "III-C1 Training Strategy ‣ III-C Performance Estimation Strategy ‣ III Search
    for Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey")), and the other is to directly
    predict the performance without training (section [III-C2](#S3.SS3.SSS2 "III-C2
    Performance Predictor ‣ III-C Performance Estimation Strategy ‣ III Search for
    Efficient Deep Learning Models ‣ Design Automation for Fast, Lightweight, and
    Effective Deep Learning Models: A Survey")). Since the hardware cost does not
    require training, studies of estimating hardware-related performance are reviewed
    in section [III-C2](#S3.SS3.SSS2 "III-C2 Performance Predictor ‣ III-C Performance
    Estimation Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey") as well.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索过程由搜索策略管理，我们在[III-B](#S3.SS2 "III-B 搜索策略 ‣ III 高效深度学习模型的搜索 ‣ 快速、轻量且有效的深度学习模型设计自动化：综述")节中讨论过这些策略，并且由候选模型的性能指导，我们将在本节中讨论。在硬件高效模型的背景下，性能有两个方面：与任务相关的性能（例如，准确率）和与硬件相关的性能（例如，延迟）。获得模型性能的最自然方式是对模型进行完全训练，然后在验证数据上评估其任务相关性能，并在目标硬件上部署以评估其与硬件相关的性能（例如，延迟的直接指标）[[107](#bib.bib107),
    [61](#bib.bib61)]。然而，这需要大量的计算和时间来从头开始完全训练每个候选模型并将其部署到目标硬件上。因此，努力减少性能估计过程。我们将这些努力分类并讨论为两种途径：一种是加速训练过程（见[III-C1](#S3.SS3.SSS1
    "III-C1 训练策略 ‣ III-C 性能估计策略 ‣ III 高效深度学习模型的搜索 ‣ 快速、轻量且有效的深度学习模型设计自动化：综述")节），另一种是直接预测性能而无需训练（见[III-C2](#S3.SS3.SSS2
    "III-C2 性能预测器 ‣ III-C 性能估计策略 ‣ III 高效深度学习模型的搜索 ‣ 快速、轻量且有效的深度学习模型设计自动化：综述")节）。由于硬件成本不需要训练，硬件相关性能的估计研究也在[III-C2](#S3.SS3.SSS2
    "III-C2 性能预测器 ‣ III-C 性能估计策略 ‣ III 高效深度学习模型的搜索 ‣ 快速、轻量且有效的深度学习模型设计自动化：综述")节中进行回顾。
- en: III-C1 Training Strategy
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 训练策略
- en: The hardware-aware NAS does not distinguish from conventional NAS regarding
    the training strategies of candidate models so the training strategy summarized
    in this section can also be applied to conventional NAS and vice versa. The simplest
    training strategy is to train each candidate model from scratch and use the sample-eval-update
    loop [[52](#bib.bib52)] to achieve an effective search agent. This strategy is
    not only widely adopted by RL-based search [[48](#bib.bib48), [80](#bib.bib80),
    [72](#bib.bib72), [65](#bib.bib65), [49](#bib.bib49)], but also by BayesOpt [[107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109)], evolutionary search [[61](#bib.bib61),
    [64](#bib.bib64), [62](#bib.bib62)], and other algorithms [[70](#bib.bib70), [68](#bib.bib68)].
    To alleviate the drawback of the straightforward training approach, some studies
    [[80](#bib.bib80), [48](#bib.bib48), [87](#bib.bib87)] follow Zoph et al. [[49](#bib.bib49)]
    to first train and search on proxy tasks, such as smaller datasets or fewer epochs,
    then transfer to the large-scale target task. The proxy task is a reduced version
    of the target task. Xiong et al. [[87](#bib.bib87)] also propose lazy evaluation
    to further allow fewer evaluation. However, this is specific to its search strategy.
    An inevitable shortcoming of using a proxy task is that the model optimized on
    proxy tasks is not guaranteed to be favourable on the target task.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知的神经架构搜索（NAS）在候选模型的训练策略上与传统的NAS没有区别，因此本节总结的训练策略也可以应用于传统的NAS，反之亦然。最简单的训练策略是从头开始训练每个候选模型，并使用样本-评估-更新循环[[52](#bib.bib52)]来实现有效的搜索代理。该策略不仅被基于强化学习的搜索[[48](#bib.bib48)、[80](#bib.bib80)、[72](#bib.bib72)、[65](#bib.bib65)、[49](#bib.bib49)]广泛采用，还被贝叶斯优化[[107](#bib.bib107)、[108](#bib.bib108)、[109](#bib.bib109)]、进化搜索[[61](#bib.bib61)、[64](#bib.bib64)、[62](#bib.bib62)]和其他算法[[70](#bib.bib70)、[68](#bib.bib68)]所使用。为了缓解直接训练方法的缺点，一些研究[[80](#bib.bib80)、[48](#bib.bib48)、[87](#bib.bib87)]跟随Zoph等[[49](#bib.bib49)]的做法，首先在代理任务上进行训练和搜索，例如使用较小的数据集或较少的训练周期，然后转移到大规模的目标任务。代理任务是目标任务的简化版本。Xiong等人[[87](#bib.bib87)]还提出了懒惰评估，以进一步减少评估次数。然而，这一方法特定于其搜索策略。使用代理任务的一个不可避免的缺点是，在代理任务上优化的模型并不一定在目标任务上表现良好。
- en: Another way of speeding up the training process is parameter sharing [[124](#bib.bib124)].
    The search space is represented as a single super Directed Acyclic Graph (DAG),
    dubbed a supernet, whose nodes are computation operators and edges illustrate
    the flow of information. Each candidate model is a subgraph/subnet of the supernet
    and candidate models can share their parameters if they share common operators.
    When the supernet is well trained (e.g., trained for enough epochs), a candidate
    model can directly inherit parameters from the supernet without any separate training.
    A more widely used supernet is to have each node represent a latent representation
    (e.g., feature maps of a CONV operator) and each directed edge represents some
    operation (e.g., a CONV operator). There are multiple edges (i.e., different operators)
    between every pair of nodes, and the search process is to determine which edge/edges
    should be retained between each pair of nodes. The common retained edges share
    parameters between different candidate models. Different studies differ in training
    the supernet. Some work [[88](#bib.bib88), [106](#bib.bib106), [114](#bib.bib114)]
    first pre-trains the whole supernet, and then only trains a part (e.g. a subnet)
    of the supernet, which is selected by search algorithms. Specifically, TuNAS [[106](#bib.bib106)]
    disables the RL controller at the first 25% of the search and only trains the
    parameters of the supernet. It randomly selects CNN filters and candidate operators
    to train with some probability $p$, which is linearly decreased from 1 to 0 over
    the first 25% search. During the parameter sharing, TuNAS further shares the “submodule”
    parameters. For example, MBConv operators with different DWSConv kernel sizes
    are in different paths/edges, but they can share the PWConv weights in MBConv
    regardless of which DWConv kernel is selected. In addition, TuNAS also applies
    channel masking for parameter sharing; it creates a convolution with the largest
    possible number of channels while simulating smaller channel numbers by choosing
    the first $N$ channels while zeroing out the remaining ones. These techniques
    are well recognized and followed by many researchers [[31](#bib.bib31), [69](#bib.bib69)].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种加速训练过程的方法是参数共享[[124](#bib.bib124)]。搜索空间被表示为一个超级有向无环图（DAG），称为超网，其节点是计算操作符，边缘展示了信息流动。每个候选模型是超网的一个子图/子网，如果它们共享共同的操作符，候选模型可以共享它们的参数。当超网训练得很好（例如，训练了足够的轮次）时，候选模型可以直接从超网继承参数，无需单独训练。一个更广泛使用的超网是让每个节点表示一个潜在表示（例如，CONV操作符的特征图），每个有向边表示某种操作（例如，CONV操作符）。每对节点之间有多个边（即不同的操作符），搜索过程是确定每对节点之间应该保留哪些边。保留的共同边在不同的候选模型之间共享参数。不同的研究在训练超网时有所不同。一些工作[[88](#bib.bib88),
    [106](#bib.bib106), [114](#bib.bib114)]首先对整个超网进行预训练，然后仅训练超网的一部分（例如，子网），这部分是由搜索算法选择的。具体来说，TuNAS[[106](#bib.bib106)]在搜索的前25%禁用RL控制器，仅训练超网的参数。它随机选择CNN滤波器和候选操作符进行训练，选择的概率$p$在前25%的搜索过程中线性地从1降低到0。在参数共享期间，TuNAS进一步共享“子模块”参数。例如，不同DWSConv内核大小的MBConv操作符处于不同的路径/边缘，但无论选择哪个DWConv内核，它们都可以共享MBConv中的PWConv权重。此外，TuNAS还应用了通道掩码进行参数共享；它创建了一个具有最大可能通道数的卷积，同时通过选择前$N$个通道并将其余通道置为零来模拟较小的通道数。这些技术得到了许多研究人员的认可和遵循[[31](#bib.bib31),
    [69](#bib.bib69)]。
- en: 'Differentiable NAS naturally uses a parameter-sharing training strategy. It
    jointly trains the supernet’s parameters and the importance weights of each path/edge
    [[86](#bib.bib86)]. Furthermore, some researchers [[67](#bib.bib67), [92](#bib.bib92)]
    propose to progressively discard the paths/edges with low importance during training
    to further accelerate the training process. This progressive shrinking strategy
    speeds up the training process significantly. A defect of the differentiable parameter
    sharing strategy is that the supernet consumes high GPU memory, which would grow
    linearly with regard to the number of candidate models. To fill this gap, the
    single-path training strategy is proposed to reduce the memory cost to the same
    level as training a single candidate model. The conventional parameter sharing
    strategy has different operators on different paths, even though these operators
    are of the same type but with different hyperparameters (e.g., different kernel
    sizes). In light of this observation, Single-Path NAS [[66](#bib.bib66)] designs
    a superkernel $\textbf{w}_{k}$ for the DWConv inside a MBConv to choose between
    a $3\times 3$ or a $5\times 5$ kernel:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 可微分神经架构搜索（Differentiable NAS）自然采用了参数共享的训练策略。它共同训练超网（supernet）的参数以及每条路径/边的权重
    [[86](#bib.bib86)]。此外，一些研究人员 [[67](#bib.bib67), [92](#bib.bib92)] 提出了在训练过程中逐步丢弃重要性较低的路径/边，以进一步加速训练过程。这种逐步收缩策略显著加快了训练过程。可微分参数共享策略的一个缺陷是超网消耗大量
    GPU 内存，这会随着候选模型数量的增加而线性增长。为弥补这一缺陷，提出了单路径训练策略，将内存消耗降低到与训练单一候选模型相同的水平。传统的参数共享策略在不同路径上使用不同的操作符，即使这些操作符类型相同但超参数（例如不同的卷积核大小）不同。鉴于这一观察，Single-Path
    NAS [[66](#bib.bib66)] 设计了一个超核 $\textbf{w}_{k}$ 用于选择 MBConv 内部的 DWConv 是 $3\times
    3$ 还是 $5\times 5$ 卷积核：
- en: '|  | $\textbf{w}_{k}=\textbf{w}_{3\times 3}+\mathbbm{1}(&#124;&#124;\textbf{w}_{5\times
    5\setminus 3\times 3}&#124;&#124;^{2}>t_{k})\cdot\textbf{w}_{5\times 5\setminus
    3\times 3}.$ |  | (18) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{w}_{k}=\textbf{w}_{3\times 3}+\mathbbm{1}(&#124;&#124;\textbf{w}_{5\times
    5\setminus 3\times 3}&#124;&#124;^{2}>t_{k})\cdot\textbf{w}_{5\times 5\setminus
    3\times 3}.$ |  | (18) |'
- en: 'It views the $3\times 3$ kernel as the inner core of the $5\times 5$ kernel,
    while zeroing out the weights of the outer shell. The $5\times 5$ kernel can be
    viewed as the summation of this inner core $\textbf{w}_{3\times 3}$ and the outer
    shell $\textbf{w}_{5\times 5\setminus 3\times 3}$: $\textbf{w}_{5\times 5}=\textbf{w}_{3\times
    3}+\textbf{w}_{5\times 5\setminus 3\times 3}$. The choice of the kernel size can
    be done using the indicator function $\mathbbm{1}(\cdot)\in\{0,1\}$ in Equation
    ([18](#S3.E18 "In III-C1 Training Strategy ‣ III-C Performance Estimation Strategy
    ‣ III Search for Efficient Deep Learning Models ‣ Design Automation for Fast,
    Lightweight, and Effective Deep Learning Models: A Survey")), which is relaxed
    to be a sigmoid function $\sigma(\cdot)$ to compute gradients. The $t_{k}$ is
    a learned threshold. Likewise, the NAS decision of the expansion ratio $e\in\{3,6\}$
    of MBConv can be represented as:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '它将 $3\times 3$ 卷积核视为 $5\times 5$ 卷积核的内部核心，同时将外壳部分的权重置为零。$5\times 5$ 卷积核可以视为内部核心
    $\textbf{w}_{3\times 3}$ 和外壳 $\textbf{w}_{5\times 5\setminus 3\times 3}$ 的总和：$\textbf{w}_{5\times
    5}=\textbf{w}_{3\times 3}+\textbf{w}_{5\times 5\setminus 3\times 3}$。卷积核大小的选择可以通过方程中的指示函数
    $\mathbbm{1}(\cdot)\in\{0,1\}$ 来完成 ([18](#S3.E18 "In III-C1 Training Strategy
    ‣ III-C Performance Estimation Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey"))，该函数放松为 sigmoid 函数 $\sigma(\cdot)$ 以计算梯度。$t_{k}$ 是学习到的阈值。同样，MBConv
    的扩展比率 $e\in\{3,6\}$ 的 NAS 决策可以表示为：'
- en: '|  | $\small\textbf{w}_{k,e}=\mathbbm{1}(&#124;&#124;\textbf{w}_{k,3}&#124;&#124;^{2}>t_{e=3})\cdot(\textbf{w}_{k,3}+\mathbbm{1}(&#124;&#124;\textbf{w}_{k,6\setminus
    3}&#124;&#124;^{2}>t_{e=6})\cdot\textbf{w}_{k,6\setminus 3}),$ |  | (19) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\textbf{w}_{k,e}=\mathbbm{1}(&#124;&#124;\textbf{w}_{k,3}&#124;&#124;^{2}>t_{e=3})\cdot(\textbf{w}_{k,3}+\mathbbm{1}(&#124;&#124;\textbf{w}_{k,6\setminus
    3}&#124;&#124;^{2}>t_{e=6})\cdot\textbf{w}_{k,6\setminus 3}),$ |  | (19) |'
- en: 'where $\textbf{w}_{k,3}$ is the channels with expansion ratio $e=3$, and can
    be viewed as the first half of the channels of the MBConv with expansion ratio
    $e=6$, while zeroing out the second half of the channels $\textbf{w}_{k,6\setminus
    3}$. The first indicator function in the Equation ([19](#S3.E19 "In III-C1 Training
    Strategy ‣ III-C Performance Estimation Strategy ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey")) is used to decide whether to skip the MBConv layer.
    This NAS problem can then be formulated as a common single-level optimization
    problem:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{w}_{k,3}$ 是扩展比 $e=3$ 的通道，可以视为扩展比 $e=6$ 的 MBConv 通道的前半部分，同时将通道 $\textbf{w}_{k,6\setminus
    3}$ 的后半部分置为零。方程中（[19](#S3.E19 "在 III-C1 训练策略 ‣ III-C 性能评估策略 ‣ III 高效深度学习模型搜索 ‣
    快速、轻量且有效的深度学习模型设计自动化：综述")）的第一个指示函数用于决定是否跳过 MBConv 层。然后，这个 NAS 问题可以表述为一个通用的单层优化问题：
- en: '|  | $\min_{\textbf{w}}\mathbfcal{L}(\textbf{w}&#124;t_{k},t_{e}),$ |  | (20)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\textbf{w}}\mathbfcal{L}(\textbf{w}&#124;t_{k},t_{e}),$ |  | (20)
    |'
- en: 'as opposed to the bi-level optimization in Equation ([11](#S3.E11 "In III-B1
    Search Algorithms ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey")). This optimization is solved in a differentiable way. The
    memory consumption of the single-path NAS is of the same level as the largest
    candidate model. Nevertheless, this single-path strategy only validates searching
    for different hyperparameters of the same type of operators, especially for convolution-based
    operators. ProxylessNAS [[23](#bib.bib23)], on the other hand, still relies on
    a multi-path structure but only activates two paths at each training iteration,
    which are sampled with the highest learnable probabilities, and mask out all the
    other paths. FBNetV2 [[79](#bib.bib79)] combines the single-path and multi-path
    strategies. It also considers the smaller number of channels as part of the larger
    volume of channels via vector masks, each of which has ones in the first entries
    and zeros in the remaining entries; different vector masks are selected with Gumbel
    softmax weights; in this way, only the largest set of filters needs to be trained.
    For the resolution search, the authors propose to subsample smaller feature maps
    from the largest feature map, perform convolution, and enlarge with inserted zeros
    to the largest size. This design resolves the resolution mismatch problem during
    single-path training. Although FBNetV2 is still a multi-path structure, it reduces
    the network paths with the single-path strategy thus accelerating the training
    and reducing the memory cost.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于方程中（[11](#S3.E11 "在 III-B1 搜索算法 ‣ III-B 搜索策略 ‣ III 高效深度学习模型搜索 ‣ 快速、轻量且有效的深度学习模型设计自动化：综述")）的双层优化，这种优化是以可微分的方式解决的。单路径
    NAS 的内存消耗与最大候选模型相同。然而，这种单路径策略仅验证了搜索相同类型操作符的不同超参数，特别是基于卷积的操作符。另一方面，ProxylessNAS
    [[23](#bib.bib23)] 仍然依赖于多路径结构，但在每次训练迭代中仅激活两个路径，这些路径是以最高可学习概率进行采样的，其余路径被屏蔽。FBNetV2
    [[79](#bib.bib79)] 结合了单路径和多路径策略。它还通过向量掩码将较少数量的通道视为较大体积通道的一部分，每个掩码的第一个条目为1，其余条目为0；不同的向量掩码是通过
    Gumbel softmax 权重选择的；这样，只需要训练最大的过滤器集合。对于分辨率搜索，作者建议从最大的特征图中子采样较小的特征图，进行卷积，然后通过插入零扩展到最大尺寸。这种设计解决了单路径训练中的分辨率不匹配问题。尽管
    FBNetV2 仍是多路径结构，但它通过单路径策略减少了网络路径，从而加快了训练并降低了内存成本。
- en: Different from the joint optimization of gradient-based methods, evolutionary
    search or RL-based search decouples the training and search process into two sequential
    steps. Thus, it is unnecessary to use indicator functions or learnable weights
    in the training. The single-path training strategy becomes simpler. For example,
    SPOS [[105](#bib.bib105)] proposes to uniformly sample a single path from a supernet
    and makes it well trained. As a result, all candidate models are trained evenly
    and simultaneously. This strategy alleviates the co-adaptation problem of shared
    weights [[125](#bib.bib125)]. After training, subnets can be directly sampled
    from the supernet for search evaluation. Many papers [[81](#bib.bib81), [91](#bib.bib91),
    [118](#bib.bib118), [76](#bib.bib76)] then follow this simple yet effective strategy,
    and design novel techniques to improve fair sampling and training of candidate
    models [[91](#bib.bib91), [118](#bib.bib118), [76](#bib.bib76)]. A comprehensive
    study is FairNAS [[118](#bib.bib118), [76](#bib.bib76)] that ensures the parameters
    of every candidate operator be updated the same amount of times at any training
    iteration. Specifically, similar to SPOS, it randomly samples a candidate operator
    at each layer to form a subnet but differently, the chosen operators are not put
    back at the next sampling step. The sampling process continues until all operators
    are sampled. The sampled models are trained individually with back-propagation,
    but their gradients are accumulated to update the supernet’s parameters. This
    strategy can alleviate the ordering issue [[118](#bib.bib118)], where candidate
    models are trained with an inherent training order in SPOS. Though SPOS and FairNAS
    accelerate the training process and reduce the memory cost, it is conventionally
    required to retrain the searched model before deployment because the inherited
    parameters from the supernet are not specialized for a specific model. Several
    studies [[75](#bib.bib75), [84](#bib.bib84), [77](#bib.bib77)] attempt to form
    a single-training strategy that does not require any post training after searching.
    BigNAS [[75](#bib.bib75), [84](#bib.bib84)] employs a bunch of training techniques
    (e.g., sandwich rule, in place distillation, and batch norm calibration) [[126](#bib.bib126),
    [75](#bib.bib75)] to achieve a high quality supernet so the inherited parameters
    can work well for any subnets. OFA [[77](#bib.bib77)] is a more promising strategy
    that prevents interference between subnets and thus a derived model can be directly
    deployed. It trains candidate models from the largest size (i.e., largest kernel
    size, depth, and width) to the smallest size (i.e., smallest kernel size, depth,
    and width) progressively. When training smaller subnets, the authors keep the
    last layers or channels untouched and finetune the early ones from shared parameters.
    As for elastic kernel size, the authors train kernel transformation matrices to
    transform the center of the larger kernel into the smaller kernel. The OFA strategy
    ensures each candidate model has a specifically trained or finetuned part so that
    mitigates their interference.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于梯度的方法的联合优化不同，进化搜索或基于 RL 的搜索将训练和搜索过程解耦为两个连续的步骤。因此，在训练中不需要使用指示函数或可学习权重。单路径训练策略变得更简单。例如，SPOS
    [[105](#bib.bib105)] 提出了从超网络中均匀采样单个路径并对其进行充分训练。因此，所有候选模型都被均匀且同时地训练。这一策略缓解了共享权重的共同适应问题
    [[125](#bib.bib125)]。训练后，可以直接从超网络中采样子网络进行搜索评估。许多论文 [[81](#bib.bib81), [91](#bib.bib91),
    [118](#bib.bib118), [76](#bib.bib76)] 随后采用了这种简单而有效的策略，并设计了新技术以改善候选模型的公平采样和训练 [[91](#bib.bib91),
    [118](#bib.bib118), [76](#bib.bib76)]。一个全面的研究是 FairNAS [[118](#bib.bib118), [76](#bib.bib76)]，它确保每个候选操作符的参数在任何训练迭代中更新的次数相同。具体而言，类似于
    SPOS，它在每一层随机采样候选操作符以形成一个子网络，但不同的是，选择的操作符在下一次采样步骤中不会被放回。采样过程持续进行，直到所有操作符都被采样。采样模型通过反向传播单独训练，但它们的梯度会累积更新超网络的参数。这一策略可以缓解排序问题
    [[118](#bib.bib118)]，而在 SPOS 中，候选模型以固有的训练顺序进行训练。尽管 SPOS 和 FairNAS 加快了训练过程并减少了内存成本，但通常需要在部署前重新训练搜索到的模型，因为从超网络继承的参数并不针对特定模型进行专业化。一些研究
    [[75](#bib.bib75), [84](#bib.bib84), [77](#bib.bib77)] 尝试形成一种单一训练策略，旨在搜索后不需要任何后续训练。BigNAS
    [[75](#bib.bib75), [84](#bib.bib84)] 采用了一系列训练技术（例如，三明治规则、原位蒸馏和批量规范校准） [[126](#bib.bib126),
    [75](#bib.bib75)] 来实现高质量的超网络，以便继承的参数可以很好地适用于任何子网络。OFA [[77](#bib.bib77)] 是一种更有前途的策略，它防止了子网络之间的干扰，因此派生模型可以直接部署。它从最大尺寸（即最大卷积核尺寸、深度和宽度）逐渐训练到最小尺寸（即最小卷积核尺寸、深度和宽度）。在训练较小的子网络时，作者保持最后的层或通道不变，并从共享参数中微调早期的层。至于弹性卷积核尺寸，作者训练卷积核变换矩阵，将较大卷积核的中心转换为较小卷积核。OFA
    策略确保每个候选模型都有专门训练或微调的部分，从而减轻它们之间的干扰。
- en: III-C2 Performance Predictor
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 性能预测器
- en: Another strategy to speed up the training process is to estimate the accuracy
    of candidate models using an accuracy predictor [[77](#bib.bib77), [78](#bib.bib78),
    [85](#bib.bib85), [115](#bib.bib115), [113](#bib.bib113)]. Although it requires
    substantial [model, accuracy] pairs thus computation and time to construct an
    accuracy predictor, it is a one-time cost as the predictor can be re-used for
    multiple hardware constraints. In addition, the accuracy predictor can also be
    easily finetuned for new datasets. The input into the accuracy predictor is the
    representation of candidate models, which is often one-hot encoding of candidate
    operators and hyperparameters [[77](#bib.bib77), [85](#bib.bib85)] or continuous
    values of hyperparameters (e.g., for channel counts) [[78](#bib.bib78), [115](#bib.bib115)].
    A different study is DONNA [[113](#bib.bib113)], which uses block-quality metrics
    derived from blockwise knowledge distillation as the input into the accuracy predictor.
    For the predictor architecture, some studies use neural networks (e.g., MLP [[78](#bib.bib78),
    [77](#bib.bib77)] or RNN [[85](#bib.bib85)]) while others use conventional learning
    models (e.g., linear regressor [[113](#bib.bib113)] or Gaussian Process regressor
    [[115](#bib.bib115)]). ChamNet [[115](#bib.bib115)] additionally adopts Bayesian
    optimization for model sampling to achieve better sampling efficiency and reliable
    prediction.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种加速训练过程的策略是使用精度预测器估算候选模型的精度[[77](#bib.bib77), [78](#bib.bib78), [85](#bib.bib85),
    [115](#bib.bib115), [113](#bib.bib113)]。尽管它需要大量的[模型，精度]对，因此需要计算和时间来构建精度预测器，但这是一项一次性成本，因为预测器可以在多个硬件约束下重复使用。此外，精度预测器也可以很容易地针对新数据集进行微调。精度预测器的输入是候选模型的表示，这通常是候选操作符和超参数的一热编码[[77](#bib.bib77),
    [85](#bib.bib85)]或超参数的连续值（例如，通道数量）[[78](#bib.bib78), [115](#bib.bib115)]。另一项研究是DONNA[[113](#bib.bib113)]，它使用从块状知识蒸馏中得出的块质量指标作为精度预测器的输入。对于预测器架构，一些研究使用神经网络（例如，MLP
    [[78](#bib.bib78), [77](#bib.bib77)]或RNN [[85](#bib.bib85)]），而其他研究使用传统学习模型（例如，线性回归器[[113](#bib.bib113)]或高斯过程回归器[[115](#bib.bib115)]）。ChamNet
    [[115](#bib.bib115)]还采用了贝叶斯优化来进行模型采样，以实现更好的采样效率和可靠的预测。
- en: 'In addition to an accuracy predictor, predictors for hardware cost are also
    widely studied to reduce the huge communication expense between the target device
    and the model training machine. Furthermore, the hardware cost predictor is demonstrated
    high fidelity across platforms ($r^{2}\geq 0.99$) [[69](#bib.bib69)]. Therefore,
    the cost predictor is popular and necessary. A broadly investigated approach is
    the latency Lookup Table (LUT) [[106](#bib.bib106), [80](#bib.bib80), [86](#bib.bib86),
    [67](#bib.bib67), [66](#bib.bib66), [114](#bib.bib114), [76](#bib.bib76), [115](#bib.bib115),
    [77](#bib.bib77)], which records the runtime of each operator in the search space.
    The basic assumption is that the runtime of each operator is independent of other
    operators [[86](#bib.bib86), [23](#bib.bib23)], so that the latency of an entire
    model $a$ can be estimated as the sum of the latency of each individual operator
    $O_{i}$:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 除了精度预测器，硬件成本预测器也广泛研究，以减少目标设备与模型训练机之间的巨大通信开销。此外，硬件成本预测器在各个平台上表现出高保真度（$r^{2}\geq
    0.99$）[[69](#bib.bib69)]。因此，成本预测器很受欢迎且必要。一种广泛研究的方法是延迟查找表（LUT）[[106](#bib.bib106),
    [80](#bib.bib80), [86](#bib.bib86), [67](#bib.bib67), [66](#bib.bib66), [114](#bib.bib114),
    [76](#bib.bib76), [115](#bib.bib115), [77](#bib.bib77)]，它记录了搜索空间中每个操作符的运行时间。基本假设是每个操作符的运行时间与其他操作符无关[[86](#bib.bib86),
    [23](#bib.bib23)]，因此整个模型$a$的延迟可以估算为每个单独操作符$O_{i}$的延迟之和：
- en: '|  | $LAT(a)=\sum_{i}LAT(O_{i}).$ |  | (21) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | $LAT(a)=\sum_{i}LAT(O_{i}).$ |  | (21) |'
- en: Researchers profile the target hardware and record the runtime for each candidate
    operator to estimate the latency of candidate models. Some research also considers
    the connectivity of operators and the communication overheads in sequential layers
    [[67](#bib.bib67), [114](#bib.bib114), [80](#bib.bib80)]. Another possible way
    is to construct a regressor to estimate the hardware cost based on critical features
    of a candidate model [[107](#bib.bib107), [23](#bib.bib23), [115](#bib.bib115),
    [69](#bib.bib69), [31](#bib.bib31)]. The performance regressor can be a linear
    regressor [[69](#bib.bib69), [31](#bib.bib31)], a Gaussian Process regressor [[115](#bib.bib115)],
    or a neural network [[107](#bib.bib107), [23](#bib.bib23)]. Different from directly
    measuring the hardware cost and training a black-box performance regressor, two
    papers examine specific devices and derive the runtime [[108](#bib.bib108)] or
    power consumption [[109](#bib.bib109)] through theoretical analysis. Nevertheless,
    this approach requires the knowledge of specific devices and is inflexible to
    different devices.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员对目标硬件进行分析，并记录每个候选操作符的运行时间，以估算候选模型的延迟。一些研究还考虑了操作符的连接性以及顺序层中的通信开销[[67](#bib.bib67),
    [114](#bib.bib114), [80](#bib.bib80)]。另一种可能的方法是构建一个回归模型，根据候选模型的关键特征来估算硬件成本[[107](#bib.bib107),
    [23](#bib.bib23), [115](#bib.bib115), [69](#bib.bib69), [31](#bib.bib31)]。性能回归模型可以是线性回归模型[[69](#bib.bib69),
    [31](#bib.bib31)]，高斯过程回归模型[[115](#bib.bib115)]，或神经网络[[107](#bib.bib107), [23](#bib.bib23)]。不同于直接测量硬件成本并训练一个黑箱性能回归模型，两篇论文通过理论分析检查特定设备，并推导运行时间[[108](#bib.bib108)]或功耗[[109](#bib.bib109)]。然而，这种方法需要对特定设备有了解，并且对不同设备的适应性不强。
- en: 'The hardware cost predictor is essential for differentiable search algorithms.
    Since each candidate operator is to be selected by a binary indicator in a differentiable
    supernet, the latency of it can be the weighted sum (i.e., with binary indicators
    {0,1}) of the latency of each candidate operator [[86](#bib.bib86)]:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件成本预测器对可微搜索算法至关重要。由于每个候选操作符由可微超级网络中的二进制指示器选择，其延迟可以是每个候选操作符延迟的加权和（即，使用二进制指示器
    {0,1}）[[86](#bib.bib86)]：
- en: '|  | $LAT(a)=\sum_{l}\sum_{i}\mathbb{I}_{l,i}\cdot LAT(O_{l,i}),\;\mathbb{I}_{l,i}\in\{0,1\},$
    |  | (22) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $LAT(a)=\sum_{l}\sum_{i}\mathbb{I}_{l,i}\cdot LAT(O_{l,i}),\;\mathbb{I}_{l,i}\in\{0,1\},$
    |  | (22) |'
- en: 'where $O_{l,i}$ and $\mathbb{I}_{l,i}$ are the $i$th operator of the $l$th
    layer and its associated binary indicator, respectively. $LAT(O_{l,i})$ can be
    achieved through either LUT [[86](#bib.bib86), [66](#bib.bib66), [67](#bib.bib67)]
    or performance regressor [[23](#bib.bib23)]. However, the loss function ([14](#S3.E14
    "In III-B2 Hardware-constraint Incorporation Strategy ‣ III-B Search Strategy
    ‣ III Search for Efficient Deep Learning Models ‣ Design Automation for Fast,
    Lightweight, and Effective Deep Learning Models: A Survey")) and ([16](#S3.E16
    "In III-B2 Hardware-constraint Incorporation Strategy ‣ III-B Search Strategy
    ‣ III Search for Efficient Deep Learning Models ‣ Design Automation for Fast,
    Lightweight, and Effective Deep Learning Models: A Survey")) with indicator $\mathbb{I}_{l,i}$
    is not directly differentiable. To sidestep this problem, the indicator is relaxed
    to be a continuous variable computed via the Gumbel Softmax function [[123](#bib.bib123)]
    with learnable parameters:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $O_{l,i}$ 和 $\mathbb{I}_{l,i}$ 分别是第 $l$ 层的第 $i$ 个操作符及其相关的二进制指示器。$LAT(O_{l,i})$
    可以通过 LUT [[86](#bib.bib86), [66](#bib.bib66), [67](#bib.bib67)] 或性能回归模型 [[23](#bib.bib23)]
    来获得。然而，带有指示器 $\mathbb{I}_{l,i}$ 的损失函数 ([14](#S3.E14 "In III-B2 Hardware-constraint
    Incorporation Strategy ‣ III-B Search Strategy ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey")) 和 ([16](#S3.E16 "In III-B2 Hardware-constraint Incorporation
    Strategy ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey")) 并不可直接微分。为避免这个问题，指示器被放松为通过 Gumbel Softmax 函数 [[123](#bib.bib123)] 计算的连续变量，该函数具有可学习的参数：'
- en: '|  | $\mathbb{I}_{l,i}=\frac{exp[(\theta_{l,i}+g_{l,i})/\tau]}{\sum_{i}exp[(\theta_{l,i}+g_{l,i})/\tau]},$
    |  | (23) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{I}_{l,i}=\frac{exp[(\theta_{l,i}+g_{l,i})/\tau]}{\sum_{i}exp[(\theta_{l,i}+g_{l,i})/\tau]},$
    |  | (23) |'
- en: 'where $\tau\in(0,1)$ is the temperature parameter that controls the search
    efficiency and efficacy. Larger $\tau$ makes the indicator distribution smoother;
    smaller $\tau$ approaches discrete categorical sampling. Despite the value of
    $\tau$, the loss function ([14](#S3.E14 "In III-B2 Hardware-constraint Incorporation
    Strategy ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey")) and ([16](#S3.E16 "In III-B2 Hardware-constraint Incorporation Strategy
    ‣ III-B Search Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design
    Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey"))
    with the latency calculation ([22](#S3.E22 "In III-C2 Performance Predictor ‣
    III-C Performance Estimation Strategy ‣ III Search for Efficient Deep Learning
    Models ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning
    Models: A Survey")) and the relaxed indicator ([23](#S3.E23 "In III-C2 Performance
    Predictor ‣ III-C Performance Estimation Strategy ‣ III Search for Efficient Deep
    Learning Models ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey")) are differentiable. For single-path training, the
    latency calculation ([22](#S3.E22 "In III-C2 Performance Predictor ‣ III-C Performance
    Estimation Strategy ‣ III Search for Efficient Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey")) can be
    simplified [[23](#bib.bib23)] as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau\in(0,1)$ 是控制搜索效率和效果的温度参数。较大的 $\tau$ 使指示器分布更加平滑；较小的 $\tau$ 接近离散类别采样。尽管
    $\tau$ 的值如何，损失函数 ([14](#S3.E14 "在 III-B2 硬件约束整合策略 ‣ III-B 搜索策略 ‣ III 高效深度学习模型搜索
    ‣ 设计自动化：快速、轻量且高效的深度学习模型的调查")) 和 ([16](#S3.E16 "在 III-B2 硬件约束整合策略 ‣ III-B 搜索策略
    ‣ III 高效深度学习模型搜索 ‣ 设计自动化：快速、轻量且高效的深度学习模型的调查")) 以及延迟计算 ([22](#S3.E22 "在 III-C2
    性能预测器 ‣ III-C 性能估计策略 ‣ III 高效深度学习模型搜索 ‣ 设计自动化：快速、轻量且高效的深度学习模型的调查")) 和放松指示器 ([23](#S3.E23
    "在 III-C2 性能预测器 ‣ III-C 性能估计策略 ‣ III 高效深度学习模型搜索 ‣ 设计自动化：快速、轻量且高效的深度学习模型的调查"))
    都是可微分的。对于单路径训练，延迟计算 ([22](#S3.E22 "在 III-C2 性能预测器 ‣ III-C 性能估计策略 ‣ III 高效深度学习模型搜索
    ‣ 设计自动化：快速、轻量且高效的深度学习模型的调查")) 可以简化为 [[23](#bib.bib23)]：
- en: '|  | $LAT(a)=\sum_{l}\mathbb{I}_{l}\cdot LAT(O_{l}),$ |  | (24) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $LAT(a)=\sum_{l}\mathbb{I}_{l}\cdot LAT(O_{l}),$ |  | (24) |'
- en: where $O_{l}$ is the selected operator at layer $l$ and $\mathbb{I}_{l}$ is
    the path probability of operator $O_{l}$.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $O_{l}$ 是层 $l$ 上选择的操作符，$\mathbb{I}_{l}$ 是操作符 $O_{l}$ 的路径概率。
- en: The cost of preparing a hardware cost predictor is minimal since it does not
    require training models. Only one forward pass of a test model is sufficient to
    record its cost.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 准备硬件成本预测器的成本很低，因为它不需要训练模型。只需对测试模型进行一次前向传递即可记录其成本。
- en: 'TABLE II: Summary of Searching for Efficient Deep Learning Models. The “hardware”
    column indicates on which the achieved models are evaluated. The latency is reported
    per input; otherwise is specified in the relevant table cells (e.g., FPS). $\sim$
    indicates the exact data is not reported in the paper and thus estimated from
    the reported figures. ‘-’ indicates unavailable records.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：高效深度学习模型搜索总结。“硬件”列指示了所评估模型的硬件。延迟按每个输入报告；否则在相关表格单元格中指定（例如，FPS）。$\sim$ 表示论文中没有报告确切数据，因此从报告的数据中估算。‘-’
    表示记录不可用。
- en: '| Search strategy | Reference | Dataset | Hardware | Accuracy | Latency | FLOPs
    | # of Parameters |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 搜索策略 | 参考文献 | 数据集 | 硬件 | 准确率 | 延迟 | FLOPs | 参数数量 |'
- en: '| BayesOpt+Soft | [[108](#bib.bib108)] | MNIST | Xilinx Zynq-7000 SoC ZC70
    | acc: 99.49 | 64.74ms | - | - |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| BayesOpt+Soft | [[108](#bib.bib108)] | MNIST | Xilinx Zynq-7000 SoC ZC70
    | 准确率：99.49 | 64.74ms | - | - |'
- en: '|  | Xilinx Virtex-7 FPGA VC707 | acc: 99.46 | 105.54ms |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | Xilinx Virtex-7 FPGA VC707 | 准确率：99.46 | 105.54ms |'
- en: '|  | CIFAR10 | Xilinx Zynq-7000 SoC ZC70 | acc: 88.25 | 136.15ms |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | CIFAR10 | Xilinx Zynq-7000 SoC ZC70 | 准确率：88.25 | 136.15ms |'
- en: '|  | Xilinx Virtex-7 FPGA VC707 | acc: 88.52 | 160.75ms |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | Xilinx Virtex-7 FPGA VC707 | 准确率：88.52 | 160.75ms |'
- en: '| ES+Soft | [[61](#bib.bib61)] | CIFAR10 | - | error: 3.85/2.75/2.50 | - |
    1290M/535M/4147M | 3.3M/3.3M/26.8M |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| ES+Soft | [[61](#bib.bib61)] | CIFAR10 | - | 错误率：3.85/2.75/2.50 | - | 1290M/535M/4147M
    | 3.3M/3.3M/26.8M |'
- en: '|  | [[64](#bib.bib64)] | CIFAR10 | - | error: 4.57/3.69/3.05/2.58 | - | -
    | 0.5M/1.1M/4.7M/13.1M |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | [[64](#bib.bib64)] | CIFAR10 | - | 错误率：4.57/3.69/3.05/2.58 | - | - | 0.5M/1.1M/4.7M/13.1M
    |'
- en: '|  | [[113](#bib.bib113)] | ImageNet | NVIDIA V100/Samsung S20 | Top1 acc:
    $\sim$79/78.5 | $\sim$20ms/16.25ms | - | - |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | [[113](#bib.bib113)] | ImageNet | NVIDIA V100/Samsung S20 | Top1 精度: $\sim$79/78.5
    | $\sim$20ms/16.25ms | - | - |'
- en: '|  | [[76](#bib.bib76)] | ImageNet | SNPE (Mi MIX3)/ | Top1/Top5 acc: 75.9/92.8
    | 11.8ms/11.1ms/101ms | 304M | 5.1M |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | [[76](#bib.bib76)] | ImageNet | SNPE (Mi MIX3)/ | Top1/Top5 精度: 75.9/92.8
    | 11.8ms/11.1ms/101ms | 304M | 5.1M |'
- en: '|  | MACE (Mi MIX3)/ | Top1/Top5 acc: 75.5/92.6 | 10.3ms/10.0ms/81ms | 248M
    | 5.5M |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | MACE (Mi MIX3)/ | Top1/Top5 精度: 75.5/92.6 | 10.3ms/10.0ms/81ms | 248M
    | 5.5M |'
- en: '|  | Google Pixel 1 | Top1/Top5 acc: 75.3/92.5 | 9.6ms/8.8ms/71ms | 221M |
    5.4M |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | Google Pixel 1 | Top1/Top5 精度: 75.3/92.5 | 9.6ms/8.8ms/71ms | 221M | 5.4M
    |'
- en: '|  | [[115](#bib.bib115)] | ImageNet | Snapdragon 835 | Top1 acc: 75.4/73.8/71.6/69.1/64.2
    | 29.8ms/19.9ms/15.0ms/10.0ms/6.1ms | 553M/323M/212M/120M/54M | - |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | [[115](#bib.bib115)] | ImageNet | Snapdragon 835 | Top1 精度: 75.4/73.8/71.6/69.1/64.2
    | 29.8ms/19.9ms/15.0ms/10.0ms/6.1ms | 553M/323M/212M/120M/54M | - |'
- en: '|  | [[114](#bib.bib114)] | ImageNet | NVIDIA GV100/ | Top1/Top5 error: 23.6/6.9
    | 12.0ms/31.6ms/76.9ms | - | - |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | [[114](#bib.bib114)] | ImageNet | NVIDIA GV100/ | Top1/Top5 错误率: 23.6/6.9
    | 12.0ms/31.6ms/76.9ms | - | - |'
- en: '|  | Intel Xeon Gold 6136/ | Top1/Top5 error: 23.5/6.8 | 13.4ms/26.4ms/69.1ms
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | Intel Xeon Gold 6136/ | Top1/Top5 错误率: 23.5/6.8 | 13.4ms/26.4ms/69.1ms
    |'
- en: '|  | NVIDIA Jetson Xavier | Top1/Top5 error: 23.8/6.9 | 12.9ms/31.8ms/52.7ms
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | NVIDIA Jetson Xavier | Top1/Top5 错误率: 23.8/6.9 | 12.9ms/31.8ms/52.7ms
    |'
- en: '|  | [[62](#bib.bib62)] | CIFAR10 | Raspberry Pi 3(B+) | acc: $\sim$81/$\sim$91/$\sim$94
    | 10ms/100ms/1000ms | - | - |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | [[62](#bib.bib62)] | CIFAR10 | Raspberry Pi 3(B+) | 精度: $\sim$81/$\sim$91/$\sim$94
    | 10ms/100ms/1000ms | - | - |'
- en: '|  | [[118](#bib.bib118)] | ImageNet | - | Top1 acc: 75.3/77.5 | - | 388M/392M
    | 4.6M/5.9M |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | [[118](#bib.bib118)] | ImageNet | - | Top1 精度: 75.3/77.5 | - | 388M/392M
    | 4.6M/5.9M |'
- en: '| RL+Soft | [[48](#bib.bib48)] | ImageNet | Google Pixel 1 | Top1/Top5 acc:
    75.2/92.5 | 78ms | 312M | 3.9M |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| RL+软 | [[48](#bib.bib48)] | ImageNet | Google Pixel 1 | Top1/Top5 精度: 75.2/92.5
    | 78ms | 312M | 3.9M |'
- en: '|  | Top1/Top5 acc: 75.6/92.7 | 84ms | 340M | 4.8M |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 精度: 75.6/92.7 | 84ms | 340M | 4.8M |'
- en: '|  | Top1/Top5 acc: 76.7/93.3 | 103ms | 403M | 5.2M |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 精度: 76.7/93.3 | 103ms | 403M | 5.2M |'
- en: '|  | [[72](#bib.bib72)] | CIFAR10 | - | error: 2.82 | - | - | 2.5M |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | [[72](#bib.bib72)] | CIFAR10 | - | 错误率: 2.82 | - | - | 2.5M |'
- en: '|  | CIFAR100 | error: 18.13 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | CIFAR100 | 错误率: 18.13 |'
- en: '|  | ImageNet | Top1/Top5 error: 27.5/9.1 | 497M | 4.4M |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | ImageNet | Top1/Top5 错误率: 27.5/9.1 | 497M | 4.4M |'
- en: '|  | PTB | Perplexity: 57.5 | - | 23M |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | PTB | 困惑度: 57.5 | - | 23M |'
- en: '|  | WT2 | Perplexity: 69.4 | 33M |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | WT2 | 困惑度: 69.4 | 33M |'
- en: '|  | [[80](#bib.bib80)] | COCO | Google Pixel 1 | mAP: 24.9/25.5 | 187ms/196ms
    | 900M/940M | 2.5M/2.6M |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | [[80](#bib.bib80)] | COCO | Google Pixel 1 | mAP: 24.9/25.5 | 187ms/196ms
    | 900M/940M | 2.5M/2.6M |'
- en: '|  | [[106](#bib.bib106)] | ImageNet | - | Top1 acc: 75.4 | 57.1ms | - | -
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | [[106](#bib.bib106)] | ImageNet | - | Top1 精度: 75.4 | 57.1ms | - | - |'
- en: '|  | COCO | mAP: 22.5 | 106ms |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | COCO | mAP: 22.5 | 106ms |'
- en: '|  | [[31](#bib.bib31)] | ImageNet | Google Pixel 4(CPU/GPU/ | Top1 acc: 74.9
    | 25.2ms/4.47ms/3.38ms/2.22ms | 349M | 4.39M |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | [[31](#bib.bib31)] | ImageNet | Google Pixel 4(CPU/GPU/ | Top1 精度: 74.9
    | 25.2ms/4.47ms/3.38ms/2.22ms | 349M | 4.39M |'
- en: '|  | DSP/EdgeTPU) | Top1 acc: 75.8 | 31.0ms/5.40ms/3.81ms/2.40ms | 433M | 4.91M
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | DSP/EdgeTPU) | Top1 精度: 75.8 | 31.0ms/5.40ms/3.81ms/2.40ms | 433M | 4.91M
    |'
- en: '|  | [[69](#bib.bib69)] | COCO | Google Pixel 1 CPU | mAP: 23.7/22.7/23.4 |
    122ms/107ms/113ms | 510M/390M/450M | 3.85M/2.57M/4.21M |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | [[69](#bib.bib69)] | COCO | Google Pixel 1 CPU | mAP: 23.7/22.7/23.4 |
    122ms/107ms/113ms | 510M/390M/450M | 3.85M/2.57M/4.21M |'
- en: '|  | Google Pixel 4 EdgeTPU | mAP: 25.5/25.4/24.7 | 6.9ms/6.8ms/7.4ms | 1530M/1760M/970M
    | 4.20M/4.79M/4.17M |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | Google Pixel 4 EdgeTPU | mAP: 25.5/25.4/24.7 | 6.9ms/6.8ms/7.4ms | 1530M/1760M/970M
    | 4.20M/4.79M/4.17M |'
- en: '|  | Google Pixel 4 DSP | mAP: 28.5/28.5/26.9 | 12.3ms/11.9ms/12.2ms | 2820M/3220M/1430M
    | 7.16M/9.15M/4.85M |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | Google Pixel 4 DSP | mAP: 28.5/28.5/26.9 | 12.3ms/11.9ms/12.2ms | 2820M/3220M/1430M
    | 7.16M/9.15M/4.85M |'
- en: '|  | [[23](#bib.bib23)] | CIFAR10 | - | error: 2.30/2.08 | - | - | 5.8M/5.7M
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | [[23](#bib.bib23)] | CIFAR10 | - | 错误率: 2.30/2.08 | - | - | 5.8M/5.7M
    |'
- en: '|  | ImageNet | Google Pixel 1 | Top1/Top5 acc: 74.6/92.2 | 78ms | - |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | ImageNet | Google Pixel 1 | Top1/Top5 精度: 74.6/92.2 | 78ms | - |'
- en: '|  | Tesla V100 | Top1/Top5 acc: 75.1/92.5 | 5.1ms |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | Tesla V100 | Top1/Top5 精度: 75.1/92.5 | 5.1ms |'
- en: '| Differentiable+Soft | [[86](#bib.bib86)] | ImageNet | Samsung Galexy S8 |
    Top1 acc: 73.0/74.1/74.9 | 19.8ms/23.1ms/28.1ms | 249M/295M/375M | 4.3M/4.5M/5.5M
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 可微分+软 | [[86](#bib.bib86)] | ImageNet | Samsung Galexy S8 | Top1 精度: 73.0/74.1/74.9
    | 19.8ms/23.1ms/28.1ms | 249M/295M/375M | 4.3M/4.5M/5.5M |'
- en: '|  | [[79](#bib.bib79)] | ImageNet | - | Top1 acc: 68.3/73.2/76.0/77.2 | -
    | 56M/126M/238M/325M | - |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | [[79](#bib.bib79)] | ImageNet | - | Top1 精度: 68.3/73.2/76.0/77.2 | - |
    56M/126M/238M/325M | - |'
- en: '|  | [[66](#bib.bib66)] | ImageNet | Google Pixel 1 | Top1/Top5 acc:74.96/92.21
    | 79.48ms | - | - |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | [[66](#bib.bib66)] | ImageNet | Google Pixel 1 | Top1/Top5 精度: 74.96/92.21
    | 79.48ms | - | - |'
- en: '|  | [[67](#bib.bib67)] | ImageNet | TITAN-XP | Top1 acc: 76.1/73.1/74.6/75.3
    | 28.9ms/13.6ms/15.4ms/17.9ms (per batch) | 479M/251M/314M/361M | - |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | [[67](#bib.bib67)] | ImageNet | TITAN-XP | Top1 精度: 76.1/73.1/74.6/75.3
    | 28.9ms/13.6ms/15.4ms/17.9ms（每批次） | 479M/251M/314M/361M | - |'
- en: '|  | [[23](#bib.bib23)] | ImageNet | Google Pixel 1 | Top1/Top5 acc: 74.2/91.7
    | 79ms | - | - |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | [[23](#bib.bib23)] | ImageNet | Google Pixel 1 | Top1/Top5 准确率: 74.2/91.7
    | 79ms | - | - |'
- en: '|  | [[92](#bib.bib92)] | ImageNet | - | Top1 acc: 76.6/77.3 | - | 267M/325M
    | 5.5M/6.4M |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | [[92](#bib.bib92)] | ImageNet | - | Top1 准确率: 76.6/77.3 | - | 267M/325M
    | 5.5M/6.4M |'
- en: '|  | COCO key-point | AP/$\text{AP}^{\text{M}}$/$\text{AP}^{\text{L}}$/AR:
    75.5/72.6/81.7/79.4 | 3720M | 6.6M |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | COCO 关键点 | AP/$\text{AP}^{\text{M}}$/$\text{AP}^{\text{L}}$/AR: 75.5/72.6/81.7/79.4
    | 3720M | 6.6M |'
- en: '|  | AP/$\text{AP}^{\text{M}}$/$\text{AP}^{\text{L}}$/AR: 65.7/62.5/72.1/71.4
    | 350M | 1.1M |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | AP/$\text{AP}^{\text{M}}$/$\text{AP}^{\text{L}}$/AR: 65.7/62.5/72.1/71.4
    | 350M | 1.1M |'
- en: '|  | Cityscapes | mIoU: 74.26/75.90 | 1910M/4660M | 2.20M/3.85M |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | Cityscapes | mIoU: 74.26/75.90 | 1910M/4660M | 2.20M/3.85M |'
- en: '|  | ADE20K | mIoU: 33.22/34.92 | 1420M/2190M | 2.49M/3.86M |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | ADE20K | mIoU: 33.22/34.92 | 1420M/2190M | 2.49M/3.86M |'
- en: '|  | KITTI | AP moderate/easy/hard: 78.49/87.62/75.53 | 15650M | 4.74M |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | KITTI | AP 中等/简单/困难: 78.49/87.62/75.53 | 15650M | 4.74M |'
- en: '|  | AP moderate/easy/hard: 69.74/83.09/74.89 | 3220M | 2.13M |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | AP 中等/简单/困难: 69.74/83.09/74.89 | 3220M | 2.13M |'
- en: 'TABLE II: Summary of Searching for Efficient Deep Learning Models (cont.).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 高效深度学习模型搜索总结（续）。'
- en: '| Search strategy | Reference | Dataset | Hardware | Accuracy | Latency | FLOPs
    | # of Parameters |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 搜索策略 | 参考文献 | 数据集 | 硬件 | 准确率 | 延迟 | FLOPs | 参数数量 |'
- en: '| Other+Soft | [[85](#bib.bib85)] | CIFAR10 |  | error: 4.62 | 9ms/82ms/149ms
    | 63.5M | 0.52M |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Other+Soft | [[85](#bib.bib85)] | CIFAR10 |  | error: 4.62 | 9ms/82ms/149ms
    | 63.5M | 0.52M |'
- en: '|  | Titan X/ | error: 4.36 | 13ms/62ms/912ms | 1364M | 11.39M |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | Titan X/ | error: 4.36 | 13ms/62ms/912ms | 1364M | 11.39M |'
- en: '|  | Maxwell 256/ | error: 4.78 | 6ms/75ms/210ms | 137M | 1.00M |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | Maxwell 256/ | error: 4.78 | 6ms/75ms/210ms | 137M | 1.00M |'
- en: '|  | ARM Cortex53 | error: 4.93 | 7ms/44ms/381ms | 270M | 2.04M |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | ARM Cortex53 | error: 4.93 | 7ms/44ms/381ms | 270M | 2.04M |'
- en: '|  |  | error: 4.84 | 8ms/65ms/145ms | 59.27M | 0.45M |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  |  | error: 4.84 | 8ms/65ms/145ms | 59.27M | 0.45M |'
- en: '|  | ImageNet | Maxwell 256/ | Top1/Top5 error: 24.16/7.13 | 218ms/5421ms |
    9276M | 77.16M |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | ImageNet | Maxwell 256/ | Top1/Top5 error: 24.16/7.13 | 218ms/5421ms |
    9276M | 77.16M |'
- en: '|  | ARM Cortex53 | Top1/Top5 error: 25.98/8.21 | 69ms/676ms | 523M | 4.8M
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | ARM Cortex53 | Top1/Top5 error: 25.98/8.21 | 69ms/676ms | 523M | 4.8M
    |'
- en: '| ES+Hard | [[81](#bib.bib81)] | VOT-19 | A10 Fusion PowerVR GPU/ | EAO/acc/robustness:
    0.333/0.590/0.376 |  | 530M | 1.97M |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ES+Hard | [[81](#bib.bib81)] | VOT-19 | A10 Fusion PowerVR GPU/ | EAO/准确率/鲁棒性:
    0.333/0.590/0.376 |  | 530M | 1.97M |'
- en: '|  | GOT-10K | Kirin 985 Mali-G77 GPU/ | AO/SR0.5: 0.611/0.710 | 52.6FPS/27.4FPS/
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | GOT-10K | Kirin 985 Mali-G77 GPU/ | AO/SR0.5: 0.611/0.710 | 52.6FPS/27.4FPS/
    |'
- en: '|  | TrackingNet | Snapdragon 845 GPU/ | P/$\text{P}_{\text{norm}}$/AUC: 69.5/77.9/72.5
    | 38.4FPS/43.5FPS |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | TrackingNet | Snapdragon 845 GPU/ | P/$\text{P}_{\text{norm}}$/AUC: 69.5/77.9/72.5
    | 38.4FPS/43.5FPS |'
- en: '|  | Snapdragon 845 DSP |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | Snapdragon 845 DSP |  |'
- en: '|  | [[91](#bib.bib91)] | SemanticKITTI | NVIDIA GTX1080Ti | mIoU: 60.3/63.7/66.4
    | 89ms/110ms/259ms | 8900M/15000M/73800M | 1.1M/2.6M/12.5M |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | [[91](#bib.bib91)] | SemanticKITTI | NVIDIA GTX1080Ti | mIoU: 60.3/63.7/66.4
    | 89ms/110ms/259ms | 8900M/15000M/73800M | 1.1M/2.6M/12.5M |'
- en: '|  | [[77](#bib.bib77)] | ImageNet | Google Pixel 1 | Top1 acc: 76.9/80.0 |
    58ms/- | 230M/595M | - |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | [[77](#bib.bib77)] | ImageNet | Google Pixel 1 | Top1 准确率: 76.9/80.0 |
    58ms/- | 230M/595M | - |'
- en: '|  | [[78](#bib.bib78)] | ImageNet | - | Top1/Top5 acc: 79.1/94.5 | - | 357M
    | - |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | [[78](#bib.bib78)] | ImageNet | - | Top1/Top5 准确率: 79.1/94.5 | - | 357M
    | - |'
- en: '|  | Top1/Top5 acc: 80.5/95.1 |  | 557M |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 80.5/95.1 |  | 557M |'
- en: '|  | Top1/Top5 acc: 81.3/95.5 |  | 762M |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 81.3/95.5 |  | 762M |'
- en: '|  | Top1/Top5 acc: 82.8/96.3 |  | 2100M |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 82.8/96.3 |  | 2100M |'
- en: '|  | COCO | mAP: 30.5/33 |  | 2900M/5300M | 5.3M/10.6M |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | COCO | mAP: 30.5/33 |  | 2900M/5300M | 5.3M/10.6M |'
- en: '| RL+Hard | [[88](#bib.bib88)] | CIFAR10 | - | acc: $\sim$0.925/$\sim$0.85
    | - | $\sim$20M/$\sim$2M | - |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| RL+Hard | [[88](#bib.bib88)] | CIFAR10 | - | 准确率: $\sim$0.925/$\sim$0.85
    | - | $\sim$20M/$\sim$2M | - |'
- en: '|  | [[65](#bib.bib65)] | CIFAR10 | - | error: 3.48/3.87/2.95/3.98/3.22 | -
    | - | 7.7M/3.4M/29M/2.2M/4.0M |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | [[65](#bib.bib65)] | CIFAR10 | - | error: 3.48/3.87/2.95/3.98/3.22 | -
    | - | 7.7M/3.4M/29M/2.2M/4.0M |'
- en: '|  | Google speech | acc: 95.81/94.04/94.82/93.16/ |  | 3390M/1400M/6530M/890M/
    | 0.143M/0.047M/0.067M/0.425M/ |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | Google 语音 | 准确率: 95.81/94.04/94.82/93.16/ |  | 3390M/1400M/6530M/890M/
    | 0.143M/0.047M/0.067M/0.425M/ |'
- en: '|  | commands | 95.02/95.64/95.18/93.65/93.07 |  | 3300M/13590M/210120M/12570M/1000M
    | 0.171M/0.733M/2.626M/0.074M/0.035M |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | commands | 95.02/95.64/95.18/93.65/93.07 |  | 3300M/13590M/210120M/12570M/1000M
    | 0.171M/0.733M/2.626M/0.074M/0.035M |'
- en: '| Other+Hard | [[84](#bib.bib84)] | PoseTrack2018 | - | AP: 83.2/78.2 | - |
    1440M/370M | 7.3M/2.5M |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Other+Hard | [[84](#bib.bib84)] | PoseTrack2018 | - | AP: 83.2/78.2 | - |
    1440M/370M | 7.3M/2.5M |'
- en: '|  | COCO2017 | AP: 73.9 |  | 5640M | 16.3M |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | COCO2017 | AP: 73.9 |  | 5640M | 16.3M |'
- en: '|  | [[75](#bib.bib75)] | ImageNet | - | Top1 acc: 76.5/78.9/79.5/80.9 | -
    | 242M/418M/586M/1040M | 4.5M/5.5M/6.4M/9.5M |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | [[75](#bib.bib75)] | ImageNet | - | Top1 准确率: 76.5/78.9/79.5/80.9 | -
    | 242M/418M/586M/1040M | 4.5M/5.5M/6.4M/9.5M |'
- en: '|  | [[68](#bib.bib68)] | ImageNet | - | Top1/Top5 acc: 76.3/93.2 | - | 390M
    | 5.3M |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | [[68](#bib.bib68)] | ImageNet | - | Top1/Top5 准确率: 76.3/93.2 | - | 390M
    | 5.3M |'
- en: '|  | Top1/Top5 acc: 78.8/94.4 |  | 700M | 7.8M |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 78.8/94.4 |  | 700M | 7.8M |'
- en: '|  | Top1/Top5 acc: 79.8/94.9 |  | 1000M | 9.2M |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 79.8/94.9 |  | 1000M | 9.2M |'
- en: '|  | Top1/Top5 acc: 81.1/95.5 |  | 1800M | 12M |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 81.1/95.5 |  | 1800M | 12M |'
- en: '|  | Top1/Top5 acc: 82.6/96.3 |  | 4200M | 19M |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 82.6/96.3 |  | 4200M | 19M |'
- en: '|  | Top1/Top5 acc: 83.3/96.7 |  | 9900M | 30M |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 83.3/96.7 |  | 9900M | 30M |'
- en: '|  | Top1/Top5 acc: 84.0/96.9 |  | 19000M | 43M |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 84.0/96.9 |  | 19000M | 43M |'
- en: '|  | Top1/Top5 acc: 84.4/97.1 |  | 37000M | 66M |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 84.4/97.1 |  | 37000M | 66M |'
- en: '|  | [[70](#bib.bib70)] | ImageNet | TPUv3/GPUv100 | Top1 acc: 77.3 | 8.71ms/22.5ms
    | 910M | 7.6M |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | [[70](#bib.bib70)] | ImageNet | TPUv3/GPUv100 | Top1 准确率: 77.3 | 8.71ms/22.5ms
    | 910M | 7.6M |'
- en: '|  | Top1 acc: 79.4 | 13.6ms/34.4ms | 1580M | 10.4M |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1 准确率: 79.4 | 13.6ms/34.4ms | 1580M | 10.4M |'
- en: '|  | Top1 acc: 80.0 | 15.7ms/45.5ms | 1890M | 11.5M |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1 准确率: 80.0 | 15.7ms/45.5ms | 1890M | 11.5M |'
- en: '|  | Top1 acc: 81.4 | 31.9ms/66.6ms | 4300M | 16M |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1 准确率: 81.4 | 31.9ms/66.6ms | 4300M | 16M |'
- en: '|  | Top1 acc: 83.0 | 64.9ms/149.2ms | 10400M | 34M |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1 准确率: 83.0 | 64.9ms/149.2ms | 10400M | 34M |'
- en: '|  | Top1 acc: 83.7 | 125.9ms/290.2ms | 22200M | 60M |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1 准确率: 83.7 | 125.9ms/290.2ms | 22200M | 60M |'
- en: '|  | Top1 acc: 84.4 | 258.1ms/467.2ms | 52000M | 137M |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1 准确率: 84.4 | 258.1ms/467.2ms | 52000M | 137M |'
- en: '|  | Top1 acc: 84.7 | 396.1ms/847.7ms | 93000M | 199M |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1 准确率: 84.7 | 396.1ms/847.7ms | 93000M | 199M |'
- en: '|  | [[87](#bib.bib87)] | CIFAR100 | - | Top1/Top5 acc: 76.1/94.0 | - | 87.3M
    | 1.92M |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | [[87](#bib.bib87)] | CIFAR100 | - | Top1/Top5 准确率: 76.1/94.0 | - | 87.3M
    | 1.92M |'
- en: '|  | ImageNet | Top1/Top5 acc: 72.2/91.0 |  | 294M | 3.4M |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | ImageNet | Top1/Top5 准确率: 72.2/91.0 |  | 294M | 3.4M |'
- en: '|  | Top1/Top5 acc: 74.7/92.0 |  | 471M | 4.7M |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | Top1/Top5 准确率: 74.7/92.0 |  | 471M | 4.7M |'
- en: IV Automated Compression of Deep Learning Models
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 深度学习模型的自动压缩
- en: 'Since deep learning models have proved remarkable performance in a wide spectrum
    of problems, it is a natural idea to compress these well-established models for
    memory saving and compute acceleration and thus hardware-efficiency [[127](#bib.bib127)].
    This idea is fervently supported by the fact that the hand-crafted deep learning
    models are often over-parameterized [[128](#bib.bib128), [7](#bib.bib7), [8](#bib.bib8)].
    The goal of deep learning compression is to modify well-trained models for efficient
    execution without significantly compromising accuracy. Although related works
    are quite divergent, they can be broadly summarized into four categories [[128](#bib.bib128),
    [127](#bib.bib127)]: tensor decomposition, knowledge distillation, pruning, and
    quantization. In the past few years, the above compression techniques have achieved
    great success while they crucially rely on domain knowledge, hand-crafted designs,
    and tremendous efforts for tuning. Recently, there is a growing demand and trend
    for automating the compression process on all the above four compression categories.
    In this section, we aim to provide a comprehensive review of recent research studies
    on automated compression of neural architectures with regard to the four compression
    categories.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习模型在广泛的问题上表现出显著的性能，将这些成熟的模型进行压缩以节省内存和加速计算，从而提高硬件效率，是一个自然的想法 [[127](#bib.bib127)]。这一想法得到了强烈支持，因为手工设计的深度学习模型通常存在过度参数化的情况
    [[128](#bib.bib128), [7](#bib.bib7), [8](#bib.bib8)]。深度学习压缩的目标是对经过良好训练的模型进行修改，以实现高效执行而不显著影响准确性。尽管相关工作存在很大差异，但可以大致总结为四类
    [[128](#bib.bib128), [127](#bib.bib127)]：张量分解、知识蒸馏、剪枝和量化。在过去几年中，上述压缩技术取得了巨大成功，但它们严重依赖领域知识、手工设计和大量的调优工作。最近，对所有四类压缩方法自动化过程的需求和趋势日益增长。本节旨在全面回顾关于神经架构自动压缩的最新研究，涉及四类压缩方法。
- en: IV-A Automated Tensor Decomposition
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 自动化张量分解
- en: 'As the computation in neural networks is based on tensor operations, it is
    intuitive to compress tensors to squeeze and accelerate a neural network. The
    basic operation of tensor decomposition is the mode-$i$ product of a tensor with
    a matrix. For a $d$th-order tensor ${\mathbfcal{X}}\in\mathbb{R}^{n_{1}\times
    n_{2}\times...\times n_{d}}$ and a matrix $\textit{{B}}\in\mathbb{R}^{m\times
    n_{i}}(i\in\{1,2,...,d\})$, the mode-$i$ product between $\mathbfcal{X}$ and B
    is $\mathbfcal{R}=\mathbfcal{X}\times_{i}\textit{{B}}$, where $\mathbfcal{R}\in\mathbb{R}^{n_{1}\times...\times
    n_{i-1}\times m\times n_{i+1}...\times n_{d}}$ is also a $d$th-order tensor. Given
    this definition, a $d$th-tensor $\mathbfcal{A}\in\mathbb{R}^{n_{1}\times n_{2}\times...\times
    n_{d}}$ can be decomposed into one core $d$th-order tensor $\mathbfcal{G}\in\mathbb{R}^{r_{1}\times
    r_{2}\times...\times r_{d}}$ and $d$ factor matrices $\textbf{{U}}^{(i)}\in\mathbb{R}^{n_{i}\times
    r_{i}}(i\in{1,2,...,d})$:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络中的计算基于张量操作，因此压缩张量以挤压和加速神经网络是直观的。张量分解的基本操作是张量与矩阵的 mode-$i$ 乘积。对于一个 $d$
    阶张量 ${\mathbfcal{X}}\in\mathbb{R}^{n_{1}\times n_{2}\times...\times n_{d}}$ 和一个矩阵
    $\textit{{B}}\in\mathbb{R}^{m\times n_{i}}(i\in\{1,2,...,d\})$，张量 $\mathbfcal{X}$
    和 B 之间的 mode-$i$ 乘积是 $\mathbfcal{R}=\mathbfcal{X}\times_{i}\textit{{B}}$，其中 $\mathbfcal{R}\in\mathbb{R}^{n_{1}\times...\times
    n_{i-1}\times m\times n_{i+1}...\times n_{d}}$ 也是一个 $d$ 阶张量。根据这个定义，一个 $d$ 阶张量
    $\mathbfcal{A}\in\mathbb{R}^{n_{1}\times n_{2}\times...\times n_{d}}$ 可以被分解成一个核心
    $d$ 阶张量 $\mathbfcal{G}\in\mathbb{R}^{r_{1}\times r_{2}\times...\times r_{d}}$
    和 $d$ 个因子矩阵 $\textbf{{U}}^{(i)}\in\mathbb{R}^{n_{i}\times r_{i}}(i\in{1,2,...,d})$：
- en: '|  | $\mathbfcal{A}\approx\mathbfcal{G}\times_{1}\textbf{{U}}^{(1)}\times_{2}\textbf{{U}}^{(2)}\times_{3}...\times_{d}\textbf{{U}}^{(d)}.$
    |  | (25) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbfcal{A}\approx\mathbfcal{G}\times_{1}\textbf{{U}}^{(1)}\times_{2}\textbf{{U}}^{(2)}\times_{3}...\times_{d}\textbf{{U}}^{(d)}.$
    |  | (25) |'
- en: 'This decomposition is called Tucker decomposition [[129](#bib.bib129)], and
    the tuple $(r_{1},r_{2},...,r_{d})$ is called the Tucker rank. By selecting proper
    low ranks (i.e., $r_{i}<n_{i}$), the original tensor can be represented by lightweight
    decomposed pieces. The parameter compression ratio is:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分解称为 Tucker 分解 [[129](#bib.bib129)]，元组 $(r_{1},r_{2},...,r_{d})$ 被称为 Tucker
    秩。通过选择合适的低秩（即 $r_{i}<n_{i}$），原始张量可以由轻量级的分解部分表示。参数压缩比为：
- en: '|  | $P=\frac{\prod_{i=1}^{d}n_{i}}{\sum_{i=1}^{d}r_{i}n_{i}+\prod_{i=1}^{d}r_{i}}.$
    |  | (26) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $P=\frac{\prod_{i=1}^{d}n_{i}}{\sum_{i=1}^{d}r_{i}n_{i}+\prod_{i=1}^{d}r_{i}}.$
    |  | (26) |'
- en: 'Given that the parameters of a neural network are in the form of tensors (e.g.,
    the filter in a convolutional layer is a 4-way tensor), tensor decomposition can
    be naturally applied to model the parameters in a more efficient way. The speed-up
    ratio of a convolution layer is:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络的参数以张量的形式存在（例如，卷积层中的滤波器是一个 4 维张量），张量分解可以自然地应用于以更高效的方式建模参数。卷积层的加速比为：
- en: '|  | $\displaystyle\scalebox{1.0}{$S=\frac{n_{1}n_{2}n_{3}n_{4}H^{\prime}W^{\prime}}{n_{1}r_{1}H^{\prime}W+n_{2}r_{2}HW^{\prime}+n_{3}r_{3}HW+n_{4}r_{4}H^{\prime}W^{\prime}+r_{1}r_{2}r_{3}r_{4}H^{\prime}W^{\prime}}$},$
    |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\scalebox{1.0}{$S=\frac{n_{1}n_{2}n_{3}n_{4}H^{\prime}W^{\prime}}{n_{1}r_{1}H^{\prime}W+n_{2}r_{2}HW^{\prime}+n_{3}r_{3}HW+n_{4}r_{4}H^{\prime}W^{\prime}+r_{1}r_{2}r_{3}r_{4}H^{\prime}W^{\prime}}$},$
    |  |'
- en: where $H\times W$ is the input feature map size, $H^{\prime}\times W^{\prime}$
    is the output feature map size, and $n_{1}\times n_{2}\times n_{3}\times n_{4}$
    is the kernel size. Tucker decomposition is a widely applied approach to this
    aim by tensorization of neural network layers [[130](#bib.bib130), [131](#bib.bib131)].
    Another commonly used tensorization approach is the Canonical Polyadic (CP) decomposition
    [[132](#bib.bib132)], which is a special case of the Tucker decomposition [[133](#bib.bib133),
    [134](#bib.bib134)]. After decomposition, it is usually required to fine-tune
    the tensorized network to recover the accuracy.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H\times W$ 是输入特征图大小，$H^{\prime}\times W^{\prime}$ 是输出特征图大小，而 $n_{1}\times
    n_{2}\times n_{3}\times n_{4}$ 是核大小。Tucker 分解是一种广泛应用的张量化神经网络层的方法 [[130](#bib.bib130),
    [131](#bib.bib131)]。另一种常用的张量化方法是典型多重分解（CP） [[132](#bib.bib132)]，它是 Tucker 分解的特例
    [[133](#bib.bib133), [134](#bib.bib134)]。分解后，通常需要对张量化网络进行微调以恢复准确性。
- en: When tensorizing neural networks, the decomposition rank is the most important
    hyperparameter that needs to be carefully selected since it controls the compression-accuracy
    trade-off. A typical selection method is cross-validation, which is quite cumbersome
    for selecting a diverse range of ranks, so the common practice is to set the ranks
    of different layers to be the same. This simplification is coarse and sub-optimal.
    Therefore, automating the rank-selection process is crucial to determining optimal
    decomposition ranks. Kim et al. [[130](#bib.bib130)] propose to employ global
    analytic solutions for variational Bayesian matrix factorization (VBMF) [[135](#bib.bib135)]
    for automatic rank selection of Tucker decomposition. Publicly available tools
    can easily implement the VBMF. They perform full model compression including fully
    connected and convolutional layers and show that the accuracy degradation after
    compression can be well recovered by fine-tuning. However, Gusak et al. [[136](#bib.bib136)]
    claim that they find it difficult to restore the initial accuracy by fine-tuning
    with the global analytic VBMF ranks. Therefore, they use the global analytic solution
    of Empirical VBMF (EVBMF) to automatically select ranks. Instead of directly using
    the ranks achieved from EVBMF, the authors design a weakened rank for decomposition,
    which is larger than the extreme rank (i.e., obtained by EVBMF) and thus not optimal
    with a certain amount of redundancy after decomposition. The reason for this design
    is that this work proposes an iterative compression and fine-tuning strategy that
    gradually compresses the original model and restores the initial accuracy. Different
    from tensor decomposition on a well-trained network followed by fine-tuning in
    [[130](#bib.bib130), [136](#bib.bib136)], Hawkins et al. [[137](#bib.bib137),
    [138](#bib.bib138)] operate automatic rank determination and tensor decomposition
    along with the model training process. They propose a prior distribution mask,
    which can be optimized during training, over the decomposition core tensor to
    control the rank. If an element of the resultant posterior mask is smaller than
    a threshold, it will be regarded as zero and the rank will be automatically selected.
    The authors use a Bayesian inference method to train this low-rank tensorized
    model and prove its efficacy for various decomposition schemes. MARS [[139](#bib.bib139)]
    sets a factorized Bernoulli prior and optimizes the model via relaxed MAP (maximum
    a posteriori) estimation to achieve a binary mask to control the rank so the manually
    set threshold used in [[137](#bib.bib137), [138](#bib.bib138)] is avoided. It
    shows slightly worse in compression but better accuracy than [[138](#bib.bib138)].
    Inspired by the success of reinforcement learning in making decisions, Javaheripi
    et al. [[140](#bib.bib140), [141](#bib.bib141)] propose a state-action-reward
    system to automatically select the optimal rank for each layer. The hardware cost
    and accuracy are both considered in the reward and the action of decomposing a
    layer with the highest rewarded rank is selected. Bayesian optimization can also
    utilized to achieve the optimal ranks [[142](#bib.bib142)].
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在张量化神经网络时，分解秩是最重要的超参数，需要仔细选择，因为它控制了压缩与精度的权衡。一个典型的选择方法是交叉验证，但这种方法对于选择不同范围的秩相当繁琐，因此常见的做法是将不同层的秩设置为相同。这种简化方法较为粗糙且次优。因此，自动化秩选择过程对于确定最佳分解秩至关重要。Kim
    等人 [[130](#bib.bib130)] 提出使用变分贝叶斯矩阵分解 (VBMF) [[135](#bib.bib135)] 的全局解析解来自动选择
    Tucker 分解的秩。公开的工具可以轻松实现 VBMF。它们执行包括全连接层和卷积层在内的完整模型压缩，并显示出压缩后的精度下降可以通过微调得到很好的恢复。然而，Gusak
    等人 [[136](#bib.bib136)] 声称他们发现通过使用全局解析 VBMF 秩进行微调很难恢复初始精度。因此，他们使用经验性 VBMF (EVBMF)
    的全局解析解来自动选择秩。与直接使用 EVBMF 获得的秩不同，作者设计了一种较弱的分解秩，该秩大于极端秩（即，通过 EVBMF 获得的秩），因此在分解后具有一定的冗余。这种设计的原因是，该工作提出了一种迭代压缩和微调策略，逐渐压缩原始模型并恢复初始精度。与
    [[130](#bib.bib130), [136](#bib.bib136)] 中在经过良好训练的网络上进行张量分解然后微调不同，Hawkins 等人 [[137](#bib.bib137),
    [138](#bib.bib138)] 在模型训练过程中进行自动秩确定和张量分解。他们提出了一种可以在训练过程中优化的先验分布掩码，用于控制分解核心张量的秩。如果结果后验掩码的某个元素小于阈值，则将其视为零，秩将被自动选择。作者使用贝叶斯推断方法来训练这个低秩张量化模型，并证明其在各种分解方案中的有效性。MARS
    [[139](#bib.bib139)] 设置了一个分解伯努利先验，并通过放宽的 MAP (最大后验估计) 优化模型，以实现控制秩的二进制掩码，从而避免了
    [[137](#bib.bib137), [138](#bib.bib138)] 中手动设置的阈值。它在压缩方面略逊一筹，但比 [[138](#bib.bib138)]
    的精度更高。受到强化学习在决策中取得成功的启发，Javaheripi 等人 [[140](#bib.bib140), [141](#bib.bib141)]
    提出了一个状态-行动-奖励系统，以自动选择每层的最佳秩。奖励中考虑了硬件成本和精度，选择具有最高奖励秩的层进行分解。贝叶斯优化也可以用于实现最佳秩 [[142](#bib.bib142)]。
- en: 'TABLE [III](#S4.T3 "TABLE III ‣ IV-A Automated Tensor Decomposition ‣ IV Automated
    Compression of Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey") summarizes the main results of
    automated tensor decomposition studies. Note that the latency is achieved with
    the hardware listed in the table, which is different from the hardware for training.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [III](#S4.T3 "TABLE III ‣ IV-A Automated Tensor Decomposition ‣ IV Automated
    Compression of Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey") 总结了自动张量分解研究的主要结果。请注意，延迟是使用表中列出的硬件实现的，这与训练所用的硬件不同。'
- en: 'TABLE III: Summary of Automated Tensor Decomposition Studies. The “hardware”
    column indicates on which the achieved models are evaluated. The model name (e.g.,
    AlexNet) in the “Accuracy” column indicates the original model before tensor decomposition.
    Some studies only report relative performance changes which are denoted as $n\times$
    in the table, indicating the original model is $n$ times its compressed counterpart.
    The latency is reported per input. ‘-’ indicates unavailable records.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 自动张量分解研究总结。 “硬件”列指示了所获得模型的评估平台。 “准确率”列中的模型名称（例如，AlexNet）表示张量分解之前的原始模型。一些研究仅报告相对性能变化，这在表中用
    $n\times$ 表示，表示原始模型是其压缩版本的 $n$ 倍。延迟按输入报告。‘-’ 表示记录不可用。'
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参考文献 | 数据集 | 硬件 | 准确率 | FLOPs | 延迟 | 参数数量 |'
- en: '| Automated Tensor Decomposition | [[130](#bib.bib130)] | ImageNet | Nvidia
    Titan X | AlexNet/VGG-S/GoogleNet/VGG-16 (Top5 acc): | 272M/549M/760M/3139M |
    0.30ms/0.92ms/1.48ms/4.58ms | 11M/14M/4.7M/127M |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 自动张量分解 | [[130](#bib.bib130)] | ImageNet | Nvidia Titan X | AlexNet/VGG-S/GoogleNet/VGG-16
    (Top5 acc): | 272M/549M/760M/3139M | 0.30ms/0.92ms/1.48ms/4.58ms | 11M/14M/4.7M/127M
    |'
- en: '| Samsung Galaxy S6 | 78.33/84.05/88.66/89.40 | 43ms/97ms/192ms/576ms |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Samsung Galaxy S6 | 78.33/84.05/88.66/89.40 | 43ms/97ms/192ms/576ms |'
- en: '| [[136](#bib.bib136)] | VOC2007 | - | Faster R-CNN (mAP): 69.2/68.3/77.0/75.0
    | 10.49$\times$/13.95$\times$/1.57$\times$/1.49$\times$ | - | - |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| [[136](#bib.bib136)] | VOC2007 | - | Faster R-CNN (mAP): 69.2/68.3/77.0/75.0
    | 10.49$\times$/13.95$\times$/1.57$\times$/1.49$\times$ | - | - |'
- en: '| COCO2014 | Faster R-CNN FPN (mAP/mAP.50): 35.4/56.2 | 1.8$\times$ |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| COCO2014 | Faster R-CNN FPN (mAP/mAP.50): 35.4/56.2 | 1.8$\times$ |'
- en: '| Faster R-CNN FPN (mAP/mAP.50): 36.2/57.1 | 1.7$\times$ |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN FPN (mAP/mAP.50): 36.2/57.1 | 1.7$\times$ |'
- en: '| ARD-LU[[137](#bib.bib137)] | MNIST | - | MLP (Acc): 98.06/98.30/96.28/98.24
    | - | - | 7k/101k/4k/6k |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| ARD-LU[[137](#bib.bib137)] | MNIST | - | MLP (Acc): 98.06/98.30/96.28/98.24
    | - | - | 7k/101k/4k/6k |'
- en: '| IMDB | DLRM (Acc): 87.61/87.79/85.33/88.93 | 6k/62k/23k/16k |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| IMDB | DLRM (Acc): 87.61/87.79/85.33/88.93 | 6k/62k/23k/16k |'
- en: '| Criteo Ad Kaggle | BiLSTM (Acc): 78.61/78.64/78.67/78.72 | 564k/437k/154k/200k
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Criteo Ad Kaggle | BiLSTM (Acc): 78.61/78.64/78.67/78.72 | 564k/437k/154k/200k
    |'
- en: '| ARD-HU[[137](#bib.bib137)] | MNIST | - | MLP (Acc): 97.98/98.30/97.04/98.23
    | - | - | 7k/91k/4k/5k |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| ARD-HU[[137](#bib.bib137)] | MNIST | - | MLP (Acc): 97.98/98.30/97.04/98.23
    | - | - | 7k/91k/4k/5k |'
- en: '| IMDB | DLRM (Acc): 87.54/88.01/85.82/88.78 | 6k/58k/19k/14k |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| IMDB | DLRM (Acc): 87.54/88.01/85.82/88.78 | 6k/58k/19k/14k |'
- en: '| Criteo Ad Kaggle | BiLSTM (Acc): 78.57/78.62/78.63/78.73 | 571k/402k/160k/164k
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Criteo Ad Kaggle | BiLSTM (Acc): 78.57/78.62/78.63/78.73 | 571k/402k/160k/164k
    |'
- en: '| [[138](#bib.bib138)] | CIFAR-10 | - | ResNet-110 (Acc): 90.4 | - | - | 7.4$\times$
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | CIFAR-10 | - | ResNet-110 (Acc): 90.4 | - | - | 7.4$\times$
    |'
- en: '| [[139](#bib.bib139)] | MNIST | - | LeNet-5 (Acc): 99.0 | 1.19$\times$ | -
    | 10$\times$ |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| [[139](#bib.bib139)] | MNIST | - | LeNet-5 (Acc): 99.0 | 1.19$\times$ | -
    | 10$\times$ |'
- en: '| CIFAR-10 | ResNet-110 (Acc): 90.7/91.1 | - | 7$\times$/5.5$\times$ |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-10 | ResNet-110 (Acc): 90.7/91.1 | - | 7$\times$/5.5$\times$ |'
- en: '| [[140](#bib.bib140), [141](#bib.bib141)] | ImageNet | ARM-A57 | AlexNet (Top5
    acc): 81.01/80.37/79.98 | 543M/349M/277M | 4.39s/2.21s/1.61s | - |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| [[140](#bib.bib140), [141](#bib.bib141)] | ImageNet | ARM-A57 | AlexNet (Top5
    acc): 81.01/80.37/79.98 | 543M/349M/277M | 4.39s/2.21s/1.61s | - |'
- en: '| VGG-16 (Top5 acc): 90.05/89.61/88.89 | 4950M/3170M/2360M | 72.15s/42.55s/30.48s
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| VGG-16 (Top5 acc): 90.05/89.61/88.89 | 4950M/3170M/2360M | 72.15s/42.55s/30.48s
    |'
- en: '| [[142](#bib.bib142)] | ImageNet | - | ResNet18(Top1/Top5 acc): 68.16/88.15
    | - | - | 3.15M |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| [[142](#bib.bib142)] | ImageNet | - | ResNet18(Top1/Top5 acc): 68.16/88.15
    | - | - | 3.15M |'
- en: '| ResNet50(Top1/Top5 acc): 74.83/92.28 | 4.49M |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| ResNet50(Top1/Top5 acc): 74.83/92.28 | 4.49M |'
- en: IV-B Automated Knowledge Distillation
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 自动知识蒸馏
- en: 'Knowledge distillation (KD) is extended from knowledge transfer (KT) [[143](#bib.bib143)]
    by Ba and Caruana [[144](#bib.bib144)] to compress a cumbersome network (teacher)
    into a smaller and simpler network (student). This is done by making the student
    model mimic the function learned by the teacher model in order to achieve a competitive
    accuracy. It is later formally popularized by Hinton et al. [[21](#bib.bib21)]
    as a student-teacher paradigm, where the knowledge is transferred from the teacher
    to the student by minimizing the difference between the logits (features before
    the final softmax) of the teacher and student. In many situations, the performance
    of the teacher is almost perfect with a very high classification probability for
    the correct class and flat probabilities for the other classes. Therefore, the
    teacher is not able to provide much more information than the ground truth labels.
    Hinton et al. [[21](#bib.bib21)] introduce the concept of softmax temperature
    to transfer knowledge, which can better deliver the information of which classes
    the teacher find similar to the correct class. Formally, given the logits of the
    teacher model, the classification probability $p_{i}$ of the class $i$ is:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏（KD）由 Ba 和 Caruana [[144](#bib.bib144)] 从知识转移（KT）[[143](#bib.bib143)] 扩展而来，旨在将一个繁重的网络（教师）压缩成一个更小、更简单的网络（学生）。这通过使学生模型模仿教师模型所学的功能来实现竞争力的准确性。随后，Hinton
    等人 [[21](#bib.bib21)] 正式推广为学生-教师范式，其中知识通过最小化教师和学生之间的 logits（最终 softmax 之前的特征）差异从教师转移到学生。在许多情况下，教师的表现几乎是完美的，对正确类别的分类概率非常高，而对其他类别的概率则很平坦。因此，教师不能提供比真实标签更多的信息。Hinton
    等人 [[21](#bib.bib21)] 引入了 softmax 温度的概念来转移知识，这可以更好地传递教师认为与正确类别相似的类别的信息。形式上，给定教师模型的
    logits，类别 $i$ 的分类概率 $p_{i}$ 为：
- en: '|  | $p_{i}=\frac{exp(\frac{z_{i}}{\tau})}{\sum_{j}exp(\frac{z_{i}}{\tau})},$
    |  | (27) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}=\frac{exp(\frac{z_{i}}{\tau})}{\sum_{j}exp(\frac{z_{i}}{\tau})},$
    |  | (27) |'
- en: where $\tau$ is the temperature parameter. It controls how soft the labels from
    the teacher are. The soft labels together with the ground truth labels are used
    to supervise a compact student model.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau$ 是温度参数。它控制教师标签的柔软程度。软标签与真实标签一起用于监督紧凑的学生模型。
- en: Vanilla knowledge distillation mostly focuses on transferring knowledge to a
    student model with a fixed small architecture, which is manually designed in advance.
    However, different teachers and tasks favour different student architectures,
    and hand-crafted architectures are prone to be sub-optimal. Considering these
    limitations, there is a growing trend to automate the architecture design of a
    student model [[145](#bib.bib145), [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148),
    [149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151)]. The ground-truth
    labels are combined with the distillation labels to guide the automatic design
    process. AKDNet [[145](#bib.bib145)] proposes to search optimal student architectures
    for distilling a given teacher by RL-based NAS. It adopts the efficient search
    space of [[48](#bib.bib48)] and designs a KD-guided reward with a teacher network.
    KDAS-ReID [[147](#bib.bib147)], AdaRec [[150](#bib.bib150)], and NAS-KD [[151](#bib.bib151)]
    leverage the differential NAS [[54](#bib.bib54)] to determine the optimal student
    structures. In addition, Bayesian optimization [[149](#bib.bib149), [148](#bib.bib148)]
    and evolutionary search [[146](#bib.bib146)] are also popular search strategies
    in the context of student architecture search.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 香草知识蒸馏主要集中于将知识转移到一个预先设计的固定小型架构的学生模型中。然而，不同的教师和任务偏好不同的学生架构，而手工设计的架构往往不够优化。考虑到这些限制，自动化学生模型架构设计的趋势越来越明显[[145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151)]。真实标签与蒸馏标签相结合，以指导自动设计过程。AKDNet [[145](#bib.bib145)]
    提出了通过基于 RL 的 NAS 搜索最优学生架构以蒸馏给定教师模型。它采用了[[48](#bib.bib48)]的高效搜索空间，并设计了一个带有教师网络的
    KD 引导奖励。KDAS-ReID [[147](#bib.bib147)]、AdaRec [[150](#bib.bib150)] 和 NAS-KD [[151](#bib.bib151)]
    利用差分 NAS [[54](#bib.bib54)] 确定最优的学生结构。此外，贝叶斯优化 [[149](#bib.bib149), [148](#bib.bib148)]
    和进化搜索 [[146](#bib.bib146)] 也是学生架构搜索中的热门策略。
- en: Although the soft labels from the final output of a teacher have been demonstrated
    effective for transferring knowledge, some works argue that it is also helpful
    to mimic the teacher from the intermediate layers. AdaRec [[150](#bib.bib150)]
    and NAS-KD [[151](#bib.bib151)] try to minimize the difference between the intermediate
    features of the student and the teacher. Similarly, Li et al. [[152](#bib.bib152)]
    and MFAGAN [[153](#bib.bib153)] compress the student generator in Generative Adversarial
    Networks (GANs) via intermediate feature distillation and once-for-all NAS [[77](#bib.bib77)].
    PPCD-GAN [[154](#bib.bib154)] also compresses the generator but with a teacher-guided
    learnable mask to automatically reduce the number of channels.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管教师的最终输出中的软标签已被证明对知识迁移有效，但一些研究认为模拟教师的中间层也是有帮助的。AdaRec [[150](#bib.bib150)]
    和 NAS-KD [[151](#bib.bib151)] 试图最小化学生与教师在中间特征上的差异。类似地，Li 等人 [[152](#bib.bib152)]
    和 MFAGAN [[153](#bib.bib153)] 通过中间特征蒸馏和一次性 NAS [[77](#bib.bib77)] 压缩生成对抗网络 (GANs)
    中的学生生成器。PPCD-GAN [[154](#bib.bib154)] 也压缩生成器，但使用教师指导的可学习掩码来自动减少通道数量。
- en: Unlike searching for how to reduce, Kang et al. [[155](#bib.bib155)] and Mitsuno
    et al. [[156](#bib.bib156)] propose to search on how to increase. They set an
    extremely small student backbone at the start point, and augment operations [[155](#bib.bib155)]
    or additional layer channels [[156](#bib.bib156)] to the backbone during the search
    procedure. A large pre-trained teacher [[156](#bib.bib156)] or an ensemble teacher
    [[155](#bib.bib155)] is used to guide the search process. The motivation of these
    works is to alleviate the search burden with a start point and to maximize the
    distilled knowledge by optimally reducing the capacity gap between teacher and
    student.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 与寻找如何减少不同，Kang 等人 [[155](#bib.bib155)] 和 Mitsuno 等人 [[156](#bib.bib156)] 提出了寻找如何增加的方案。他们在起始点设置了一个极小的学生骨干，并在搜索过程中向骨干添加增强操作
    [[155](#bib.bib155)] 或额外的层通道 [[156](#bib.bib156)]。使用一个大型预训练教师 [[156](#bib.bib156)]
    或一个集成教师 [[155](#bib.bib155)] 来指导搜索过程。这些工作的动机是通过设定起始点来减轻搜索负担，并通过最优化地减少教师与学生之间的容量差距来最大化蒸馏知识。
- en: A different direction is NAS-BERT [[157](#bib.bib157)], where the block-wise
    KD [[158](#bib.bib158)] is applied to train a supernet composing a bunch of candidate
    compact subnets. Then, the meta information (e.g., parameter, latency) of the
    candidate networks is summarized in a lookup table. Given certain hardware constraints,
    all candidate networks that satisfy the constraints are evaluated and the best
    performing one is selected as the final compressed model. The benefit of this
    strategy is that no retraining and researching are required for new hardware constraints.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方向是 NAS-BERT [[157](#bib.bib157)]，其中应用了块级 KD [[158](#bib.bib158)] 来训练一个由多个候选紧凑子网组成的超网络。然后，将候选网络的元信息（例如参数、延迟）汇总在一个查找表中。根据特定的硬件约束，评估所有满足约束的候选网络，并选择性能最佳的作为最终压缩模型。这种策略的好处在于不需要为新的硬件约束进行重新训练和研究。
- en: 'Not only the student architecture is searchable, but also related hyperparameters
    (e.g., the temperature $\tau$ in Equation ([27](#S4.E27 "In IV-B Automated Knowledge
    Distillation ‣ IV Automated Compression of Deep Learning Models ‣ Design Automation
    for Fast, Lightweight, and Effective Deep Learning Models: A Survey"))) [[148](#bib.bib148)]
    can be included in the search process. AutoKD [[148](#bib.bib148)] uses the BayesOpt
    to simultaneously explore the optimal student structure and KD hyperparameters,
    temperature $\tau$ and loss weight $\alpha$, during the KD process.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 学生架构不仅可以被搜索，相关的超参数（例如方程 ([27](#S4.E27 "在 IV-B 自动化知识蒸馏 ‣ IV 深度学习模型的自动压缩 ‣ 设计自动化以实现快速、轻量级且有效的深度学习模型：综述"))
    [[148](#bib.bib148)] 中的温度 $\tau$）也可以包括在搜索过程中。AutoKD [[148](#bib.bib148)] 使用 BayesOpt
    同时探索最佳学生结构和 KD 超参数，包括温度 $\tau$ 和损失权重 $\alpha$，在 KD 过程中。
- en: 'We outline the major results of automated knowledge distillation papers in
    TABLE [IV](#S4.T4 "TABLE IV ‣ IV-B Automated Knowledge Distillation ‣ IV Automated
    Compression of Deep Learning Models ‣ Design Automation for Fast, Lightweight,
    and Effective Deep Learning Models: A Survey").'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格 [IV](#S4.T4 "TABLE IV ‣ IV-B 自动化知识蒸馏 ‣ IV 深度学习模型的自动压缩 ‣ 设计自动化以实现快速、轻量级且有效的深度学习模型：综述")
    中概述了自动化知识蒸馏论文的主要结果。
- en: 'TABLE IV: Summary of Automated Knowledge Distillation Studies. The “hardware”
    column indicates on which the achieved models are evaluated. The model name (e.g.,
    Inception-ResNet-V2) in the “Accuracy” column indicates the teacher model; if
    the teacher is not specified, the teacher is searched during training. The latency
    is reported per input; otherwise is specified in the relevant table cells (e.g.,
    s/batch). ‘-’ indicates unavailable records.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：自动化知识蒸馏研究总结。“硬件”列指明了模型评估的硬件平台。“准确率”列中的模型名称（例如 Inception-ResNet-V2）表示教师模型；如果未指定教师，则在训练过程中搜索教师。延迟是按输入报告的；否则在相关表格单元格中指定（例如，s/batch）。‘-’
    表示记录不可用。
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters | Memory |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参考文献 | 数据集 | 硬件 | 准确率 | FLOPs | 延迟 | 参数数量 | 内存 |'
- en: '| Automated Knowledge Distillation | [[145](#bib.bib145)] | ImageNet | Pixel
    1 phone | Inception-ResNet-v2 (Top5 acc): 87.5/89.1/93.1 | - | 15ms/25ms/75ms
    | - | - |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 自动化知识蒸馏 | [[145](#bib.bib145)] | ImageNet | Pixel 1 phone | Inception-ResNet-v2
    (Top5 acc): 87.5/89.1/93.1 | - | 15ms/25ms/75ms | - | - |'
- en: '| Inception-ResNet-v2 (Top1 acc): 66.5/69.6/75.5 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Inception-ResNet-v2 (Top1 acc): 66.5/69.6/75.5 |'
- en: '| [[146](#bib.bib146)] | SST-2 | - | BERT (Acc): 84.1 | - | - | 24M | - |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| [[146](#bib.bib146)] | SST-2 | - | BERT (Acc): 84.1 | - | - | 24M | - |'
- en: '| Ag News | BERT (Acc): 90.3 | 16.9M |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Ag News | BERT (Acc): 90.3 | 16.9M |'
- en: '| [[147](#bib.bib147)] | Market-1501 | - | ResNet50 (mAP/Rank): 94.7/95.6 |
    - | - | 14.3M | - |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| [[147](#bib.bib147)] | Market-1501 | - | ResNet50 (mAP/Rank): 94.7/95.6 |
    - | - | 14.3M | - |'
- en: '| [[148](#bib.bib148)] | CIFAR100 | - | Inception-Resnet-V2 (Acc): 81.2 | -
    | - | 4M | - |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| [[148](#bib.bib148)] | CIFAR100 | - | Inception-Resnet-V2 (Acc): 81.2 | -
    | - | 4M | - |'
- en: '| MIT67 | DARTS (Acc): 76.0 | 6M |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| MIT67 | DARTS (Acc): 76.0 | 6M |'
- en: '| ImageNet | Inception-Resnet-V2 (Acc): 78.0 | 6M |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | Inception-Resnet-V2 (Acc): 78.0 | 6M |'
- en: '| [[149](#bib.bib149)] | GLUE | TPUv4i | IB-$\text{BERT}_{\text{LARGE}}$ (AvgAcc):
    80.38/81.69 | - | 0.45ms/0.58ms | 20.6M/28.5M | - |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| [[149](#bib.bib149)] | GLUE | TPUv4i | IB-$\text{BERT}_{\text{LARGE}}$ (AvgAcc):
    80.38/81.69 | - | 0.45ms/0.58ms | 20.6M/28.5M | - |'
- en: '| SQuAD | IB-$\text{BERT}_{\text{LARGE}}$ (F1): 88.4/88.1 | 0.59ms/0.49ms |
    22.8M/20.6M |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | IB-$\text{BERT}_{\text{LARGE}}$ (F1): 88.4/88.1 | 0.59ms/0.49ms |
    22.8M/20.6M |'
- en: '| [[150](#bib.bib150)] | RetailRocket | - | NextItNet (MRR@5/HR@5/NDCG@5):
    0.7345/0.7964/0.7500 | - | - | - | - |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| [[150](#bib.bib150)] | RetailRocket | - | NextItNet (MRR@5/HR@5/NDCG@5):
    0.7345/0.7964/0.7500 | - | - | - | - |'
- en: '| 30Music | NextItNet (MRR@5/HR@5/NDCG@5): 0.6343/0.7151/0.6544 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 30Music | NextItNet (MRR@5/HR@5/NDCG@5): 0.6343/0.7151/0.6544 |'
- en: '| ML-2K | NextItNet (MRR@5/HR@5/NDCG@5): 0.4489/0.6519/0.4995 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| ML-2K | NextItNet (MRR@5/HR@5/NDCG@5): 0.4489/0.6519/0.4995 |'
- en: '| [[151](#bib.bib151)] | GLUE | - | $\text{BERT}_{\text{base}}$ (SST-2/MRPC/QQP/MNLI-m/MNLI-mm/QNLI/RTE):
    | - | - |  | - |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| [[151](#bib.bib151)] | GLUE | - | $\text{BERT}_{\text{base}}$ (SST-2/MRPC/QQP/MNLI-m/MNLI-mm/QNLI/RTE):
    | - | - |  | - |'
- en: '| 92.2/86.3/70.4/81.0/80.2/88.6/65.9 | 42.4M |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 92.2/86.3/70.4/81.0/80.2/88.6/65.9 | 42.4M |'
- en: '| 86.9/79.3/67.5/76.1/75.5/83.9/58.9 | 33.2M |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 86.9/79.3/67.5/76.1/75.5/83.9/58.9 | 33.2M |'
- en: '| [[152](#bib.bib152)] | Cityscapes | - | Pix2Pix (mIoU): 41.71 | 5450M | -
    | 0.89M | - |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| [[152](#bib.bib152)] | Cityscapes | - | Pix2Pix (mIoU): 41.71 | 5450M | -
    | 0.89M | - |'
- en: '| GauGAN (mIoU): 61.17 | 31200M | 20.2M |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| GauGAN (mIoU): 61.17 | 31200M | 20.2M |'
- en: '| COCO-Stuff |  | GauGAN (mIoU/FID): 35.34/25.06 | 35400M | 26M |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| COCO-Stuff |  | GauGAN (mIoU/FID): 35.34/25.06 | 35400M | 26M |'
- en: '| [[153](#bib.bib153)] | Set5 | NVIDIA V100 | MFANet (PSNR/LPIPS): 30.16/0.0571
    | 8410M | 21.9ms | 0.55M | 0.52G |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| [[153](#bib.bib153)] | Set5 | NVIDIA V100 | MFANet (PSNR/LPIPS): 30.16/0.0571
    | 8410M | 21.9ms | 0.55M | 0.52G |'
- en: '| Set14 | MFANet (PSNR/LPIPS): 26.69/0.113 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Set14 | MFANet (PSNR/LPIPS): 26.69/0.113 |'
- en: '| B100 | MFANet (PSNR/LPIPS): 25.33/0.1332 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| B100 | MFANet (PSNR/LPIPS): 25.33/0.1332 |'
- en: '| Urban100 | MFANet (PSNR/LPIPS): 24.23/0.1132 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Urban100 | MFANet (PSNR/LPIPS): 24.23/0.1132 |'
- en: '| [[154](#bib.bib154)] | ImageNet | AMD Ryzen 9 3900X | BigGAN (IS/FID/LPIPS):
    83.13/12.76/0.62 | 1600M | 2.05s/batch | 13.6M | - |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| [[154](#bib.bib154)] | ImageNet | AMD Ryzen 9 3900X | BigGAN (IS/FID/LPIPS):
    83.13/12.76/0.62 | 1600M | 2.05s/batch | 13.6M | - |'
- en: '| NVIDIA GTX3090 | 1.19s/batch |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| NVIDIA GTX3090 | 1.19s/batch |'
- en: '| [[157](#bib.bib157)] | GLUE | - | $\text{BERT}_{\text{base}}$ (MNLI/QQP/QNLI/CoLA/SST-2/STS-B/RTE/MRPC)
    | - | - | 60M | - |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| [[157](#bib.bib157)] | GLUE | - | $\text{BERT}_{\text{base}}$ (MNLI/QQP/QNLI/CoLA/SST-2/STS-B/RTE/MRPC)
    | - | - | 60M | - |'
- en: '| 84.1/88.8/91.2/50.5/92.6/86.9/72.7/86.4 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 84.1/88.8/91.2/50.5/92.6/86.9/72.7/86.4 |'
- en: '| SQuADv1.1 | $\text{BERT}_{\text{base}}$(EM/F1): 81.2/88.3 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| SQuADv1.1 | $\text{BERT}_{\text{base}}$(EM/F1): 81.2/88.3 |'
- en: '| SQuADv2.0 | $\text{BERT}_{\text{base}}$(EM/F1): 73.9/77.1 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| SQuADv2.0 | $\text{BERT}_{\text{base}}$(EM/F1): 73.9/77.1 |'
- en: IV-C Automated Pruning
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 自动化剪枝
- en: Neural network pruning, a.k.a. sparsification, attempts to reduce memory storage
    and computation cost by removing unimportant weights or neurons from the base
    network. According to the granularity, pruning can be roughly categorized into
    structured pruning and unstructured pruning. Unstructured pruning enforces weights
    or layers to be sparse by removing individual connections or neurons. Structured
    pruning, on the other hand, targets discarding the entire channels or layers.
    Although unstructured pruning can theoretically achieve a better accuracy-compression
    tradeoff, it is less compatible with existing deep learning platforms and hardware
    than structured pruning. Thus, structured pruning attracts more research focus.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络剪枝，即稀疏化，试图通过从基础网络中移除不重要的权重或神经元来减少内存存储和计算成本。根据粒度，剪枝大致可以分为结构化剪枝和非结构化剪枝。非结构化剪枝通过移除个别连接或神经元来强制权重或层变得稀疏。而结构化剪枝则针对丢弃整个通道或层。虽然非结构化剪枝理论上可以实现更好的准确性-压缩权衡，但与现有的深度学习平台和硬件的兼容性较差。因此，结构化剪枝吸引了更多的研究关注。
- en: A typical pruning procedure is to first identify and cut away unimportant weighs
    or layers from an existing over-parameterized network and then fine-tune the pruned
    network to restore the accuracy. The first step is the key and largely relies
    on expert knowledge and hand-crafted heuristics, such as manually setting the
    criterion of parameter importance [[159](#bib.bib159)] and the pruning rate [[160](#bib.bib160)].
    However, heuristic settings are typically based on trial and error and are sub-optimal
    so researchers actively seek automated solutions to alleviate human intervention
    during pruning. These efforts can roughly be categorized into three directions.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的剪枝过程是首先从现有的过参数化网络中识别并剪除不重要的权重或层，然后对剪枝后的网络进行微调以恢复准确性。第一步是关键，主要依赖于专家知识和手工制定的启发式规则，如手动设置参数重要性标准[[159](#bib.bib159)]和剪枝率[[160](#bib.bib160)]。然而，启发式设置通常基于试验和错误，并且是次优的，因此研究人员积极寻求自动化解决方案以减轻剪枝过程中对人工干预的依赖。这些努力大致可以分为三个方向。
- en: 1) Reducing the number of hyperparameters (i.e., per-layer pruning rates) needed
    to be tuned [[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163)]. Zeng
    et al. [[161](#bib.bib161)] compute the importance of model parameters using the
    Taylor expansion of the loss function and prune unimportant ones. They introduce
    an auxiliary hyperparameter that controls the model shrinking proportion at each
    pruning iteration and show that this hyperparameter is insensitive to the final
    performance. In this way, they avoid carefully tuning the overall pruning rate.
    Li et al. [[162](#bib.bib162)] use the regularization approach, Alternating Direction
    Method Multipliers (ADMM), as the core pruning algorithm but substitute its hard
    constraint with the soft constraint-based formulation and solve the optimization
    problem with the Primal-Proximal solution. Their approach naturally does not require
    predefining the per-layer pruning rates and thus reduces the number of hyperparameters.
    Zheng et al. [[163](#bib.bib163)] introduce the normalized Hilbert-Schmidt Independence
    Criterion (nHSIC) from the information theory to measure the per-layer importance
    and derive the compression problem with constraints into a linear programming
    problem, which is convex and easily solved. In such a manner, only two hyperparameters
    are required and demonstrated robust to different values.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 减少需要调优的超参数数量（即，每层剪枝率）[[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163)]。曾等人[[161](#bib.bib161)]通过使用损失函数的泰勒展开来计算模型参数的重要性，并剪除不重要的参数。他们引入了一个辅助超参数，控制每次剪枝迭代中的模型收缩比例，并展示了该超参数对最终性能的不敏感性。通过这种方式，他们避免了仔细调整整体剪枝率。李等人[[162](#bib.bib162)]使用正则化方法，即交替方向乘子法（ADMM），作为核心剪枝算法，但用基于软约束的形式替代其硬约束，并用原始-近似解法解决优化问题。他们的方法自然不需要预定义每层剪枝率，从而减少了超参数的数量。郑等人[[163](#bib.bib163)]引入了信息理论中的归一化希尔伯特-施密特独立准则（nHSIC）来衡量每层的重要性，并将压缩问题及约束推导为线性规划问题，这个问题是凸的且容易解决。通过这种方式，只需两个超参数，并且在不同值下表现稳健。
- en: 2) Searching for optimal pruning rates or magnitude thresholds for pruning [[26](#bib.bib26),
    [164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167),
    [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170), [171](#bib.bib171),
    [172](#bib.bib172), [173](#bib.bib173)]. Note that the magnitude thresholds of
    weights can be naturally derived from pruning rates. AMC [[26](#bib.bib26)] and
    Auto-Prune [[165](#bib.bib165)] leverage deep reinforcement learning (DRL) to
    automatically determine the pruning rate of each layer and empirically prune the
    weights with the least magnitude. Arguing that DRL has an inherited incompatibility
    with the pruning problem, Liu et al. [[164](#bib.bib164)] suggest using the heuristic
    search technique, simulated annealing (SA), to search for the per-layer pruning
    rates and then use the ADMM to dynamically regularize network weights for structural
    pruning. They hypothesize that the layers with more weights can have higher compression
    rates. The authors also apply the ADMM to searching for the magnitude thresholds
    for the second-phase unstructured pruning after the first-phase structural pruning.
    Tung et al. [[167](#bib.bib167)] propose to employ a Bayesian optimization framework
    to search the pruning hyperparameters including the magnitude threshold. Some
    studies [[168](#bib.bib168), [169](#bib.bib169)] follow this work to make it constraint-aware
    [[168](#bib.bib168)] and applicable to pruning deeper networks [[169](#bib.bib169)].
    Other than pruning full-precision neural networks, Guerra and Drummond [[171](#bib.bib171)]
    aim at automatically pruning quantized neural networks by a customized, rule-based
    pruning strategy with Bayesian optimization of layer-wise pruning ratios.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索最优的剪枝率或幅度阈值 [[26](#bib.bib26), [164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166),
    [167](#bib.bib167), [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170),
    [171](#bib.bib171), [172](#bib.bib172), [173](#bib.bib173)]。需要注意的是，权重的幅度阈值可以自然地从剪枝率中推导出来。AMC
    [[26](#bib.bib26)] 和 Auto-Prune [[165](#bib.bib165)] 利用深度强化学习（DRL）自动确定每层的剪枝率，并经验性地剪去幅度最小的权重。刘等
    [[164](#bib.bib164)] 认为 DRL 与剪枝问题存在固有的不兼容性，建议使用启发式搜索技术——模拟退火（SA）来搜索每层的剪枝率，然后使用
    ADMM 动态地对网络权重进行结构剪枝。他们假设，权重更多的层可以具有更高的压缩率。作者们还将 ADMM 应用于在第一阶段结构剪枝后搜索第二阶段非结构化剪枝的幅度阈值。Tung
    等 [[167](#bib.bib167)] 提出采用贝叶斯优化框架来搜索包括幅度阈值在内的剪枝超参数。一些研究 [[168](#bib.bib168),
    [169](#bib.bib169)] 在此基础上使其具有约束意识 [[168](#bib.bib168)] 并适用于剪枝更深的网络 [[169](#bib.bib169)]。除了剪枝全精度神经网络，Guerra
    和 Drummond [[171](#bib.bib171)] 旨在通过定制的基于规则的剪枝策略，并利用贝叶斯优化进行层级剪枝比例的优化，自动剪枝量化神经网络。
- en: 3) Learning the weight/channel importance [[174](#bib.bib174), [24](#bib.bib24),
    [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)]. Different from relying
    on heuristic weight/channel importance measurement like the weight magnitude,
    this direction struggles to learn the importance measurement through optimizing
    the final objective. Liu et al. [[175](#bib.bib175)] propose a slimming scheme
    that regularizes the scaling factors in batch normalization (BN) layers as a channel
    selection indicator to identify unimportant channels (or neurons). Then, they
    apply a threshold to the trained scaling factors for channel pruning. AutoPrune
    [[24](#bib.bib24)] and DAIS [[174](#bib.bib174)] are following works that rely
    on auxiliary learnable channel selection indicator for pruning. AutoPrune argues
    that decoupling the channel selection indicators and network parameters will help
    to stabilize the weight learning and make the pruning insensitive to hyperparameters.
    DHP [[176](#bib.bib176)] introduces an additional hypernetwork that can generate
    weights for the backbone network (i.e., the network to be pruned). The input of
    the hypernetwork is the latent vectors, which are attached to each layer of the
    backbone network. Binary channel masks will then be achieved by setting those
    latent vector elements, which are smaller than a threshold, to be zero otherwise
    to be one. In contrast to learning channel importance, ASBP [[177](#bib.bib177)]
    targets to automatically decide the binary importance of each bit of weight. This
    bit-level pruning has the finest granularity but is impractical to achieve a real
    reduction of resource consumption due to poor hardware support. To solve this
    problem, the authors target a specific sort of hardware, RRAM, which assembles
    multiple low-precision cells together as a crossbar to represent a high-precision
    data value. They then prune at the granularity of crossbar size, that is all cells
    in a crossbar share the same pruning strategy. The DL algorithm, deep deterministic
    policy gradient (DDPG), is used to search for the optimal set of bit-pruning strategies.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 学习权重/通道重要性 [[174](#bib.bib174), [24](#bib.bib24), [175](#bib.bib175), [176](#bib.bib176),
    [177](#bib.bib177)]。与依赖于启发式权重/通道重要性测量（如权重大小）不同，这一方向通过优化最终目标来学习重要性测量。Liu 等人 [[175](#bib.bib175)]
    提出了一种收缩方案，该方案将批归一化（BN）层中的缩放因子作为通道选择指标来识别不重要的通道（或神经元）。然后，他们对训练后的缩放因子应用阈值进行通道剪枝。AutoPrune
    [[24](#bib.bib24)] 和 DAIS [[174](#bib.bib174)] 是依赖于辅助可学习通道选择指标进行剪枝的后续工作。AutoPrune
    认为，解耦通道选择指标和网络参数有助于稳定权重学习，使剪枝对超参数不敏感。DHP [[176](#bib.bib176)] 引入了一个额外的超网络，它可以生成主网络（即待剪枝的网络）的权重。超网络的输入是潜在向量，这些向量附加在主网络的每一层上。通过将那些小于阈值的潜在向量元素设置为零，其他元素设置为一，从而获得二值通道掩码。与学习通道重要性不同，ASBP
    [[177](#bib.bib177)] 旨在自动决定每个位权重的二值重要性。这种位级剪枝具有最精细的粒度，但由于硬件支持不佳，实际实现资源消耗的减少是不可行的。为解决这一问题，作者针对特定类型的硬件
    RRAM，该硬件将多个低精度单元组装在一起形成交叉开关，以表示高精度数据值。他们在交叉开关大小的粒度上进行剪枝，即所有交叉开关中的单元共享相同的剪枝策略。使用深度确定性策略梯度（DDPG）算法来搜索最优的位剪枝策略集合。
- en: IV-C1 Unifying knowledge distillation and pruning
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 知识蒸馏与剪枝的统一
- en: Since pruning and knowledge distillation both require a pre-trained model (i.e.,
    the base model in pruning; teacher in KD) to guide the compression process, researchers
    naturally explore ways to unify these two techniques. One route is the two-step
    unifying scheme [[178](#bib.bib178), [179](#bib.bib179)]. TAS [[178](#bib.bib178)]
    is an early and representative work. It first searches for the shrunken width
    and depth of the pruned network with the help of learnable probability from the
    base model. The searched architecture is then trained from scratch as the student
    of a simple KD approach with the base model serving as the teacher. The other
    route is to unify pruning and KD in a single step [[180](#bib.bib180), [181](#bib.bib181)].
    Gu and Tresp [[181](#bib.bib181)] introduce learnable channel gates into the base
    model and train the network weights and channel gates with a distillation-aware
    loss function. Thus, this scheme prunes the base model and distils its knowledge
    simultaneously and automatically. Yao et al. additionally search for both teacher
    and student to pursue the optimal distillation pair.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 由于剪枝和知识蒸馏都需要预训练模型（即剪枝中的基础模型；KD中的教师）来指导压缩过程，研究人员自然会探索将这两种技术统一的方法。一种途径是两步统一方案
    [[178](#bib.bib178)，[179](#bib.bib179)]。TAS [[178](#bib.bib178)] 是一个早期且具有代表性的工作。它首先借助基础模型中的可学习概率来搜索剪枝网络的缩小宽度和深度。然后从头开始训练搜索得到的架构，作为简单
    KD 方法中的学生，基础模型作为教师。另一种途径是将剪枝和 KD 统一到一步 [[180](#bib.bib180)，[181](#bib.bib181)]。Gu
    和 Tresp [[181](#bib.bib181)] 将可学习的通道门引入基础模型，并用蒸馏感知的损失函数训练网络权重和通道门。因此，该方案同时自动地对基础模型进行剪枝并蒸馏其知识。Yao
    等人进一步搜索教师和学生，以追求最佳的蒸馏配对。
- en: 'TABLE V: Summary of Automated Pruning Studies. The “hardware” column indicates
    on which the achieved models are evaluated. The model name (e.g., LeNet-300-100)
    in the “Accuracy” column indicates the original model before pruning; if the original
    model is not specified, it is searched during training. The latency is reported
    per input; otherwise is specified in the relevant table cells (e.g., FPS (frame
    per second)). $n\%$ indicates the compressed model is $n$ per cent of its original
    model; $\downarrow n\%$ indicates the compressed model decreases $n$ per cent
    compared to its original model. $n\times$ indicates that the original model is
    $n$ times its compressed counterpart. ‘-’ indicates unavailable records.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：自动剪枝研究总结。“硬件”列表示实现的模型所评估的平台。“准确率”列中的模型名称（例如 LeNet-300-100）表示剪枝前的原始模型；如果未指定原始模型，则在训练过程中进行搜索。延迟是每个输入的报告；否则在相关表格单元中指定（例如，FPS（每秒帧数））。$n\%$
    表示压缩后的模型是其原始模型的 $n$ 百分比；$\downarrow n\%$ 表示压缩后的模型比其原始模型减少了 $n$ 百分比。$n\times$ 表示原始模型是其压缩模型的
    $n$ 倍。“-”表示记录不可用。
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters | Memory |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参考文献 | 数据集 | 硬件 | 准确率 | FLOPs | 延迟 | 参数数量 | 内存 |'
- en: '| Automated Pruning | [[161](#bib.bib161)] | MNIST | - | LeNet-300-100/LeNet-5(Top1
    error): $\uparrow$0.08/$\uparrow$0.04 | - | - | 77$\times$/200$\times$ | - |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 自动剪枝 | [[161](#bib.bib161)] | MNIST | - | LeNet-300-100/LeNet-5（Top1 错误率）：$\uparrow$0.08/$\uparrow$0.04
    | - | - | 77$\times$/200$\times$ | - |'
- en: '| CIFAR-10 |  | CifarNet(Top1 error):$\uparrow$ 0.17 | 15.6$\times$ |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-10 |  | CifarNet（Top1 错误率）：$\uparrow$ 0.17 | 15.6$\times$ |'
- en: '| ImageNet |  | VGG16(Top1/Top5 error): $\uparrow$2.3/$\uparrow$1.2 | 5.4$\times$
    | - |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  | VGG16（Top1/Top5 错误率）：$\uparrow$2.3/$\uparrow$1.2 | 5.4$\times$
    | - |'
- en: '|  |  | ResNet50(Top1/Top5 error): $\uparrow$2.0/$\uparrow$1.1 | 2.3$\times$
    |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ResNet50（Top1/Top5 错误率）：$\uparrow$2.0/$\uparrow$1.1 | 2.3$\times$ |'
- en: '| [[162](#bib.bib162)] | CIFAR-10 | Qualcomm Kryo 485 Octacore | VGG16/ResNet-18/MobileNetV2(acc):
    93.5/94.2/94.6 | - | 2.52ms/3.39ms/2.98ms | - | - |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| [[162](#bib.bib162)] | CIFAR-10 | Qualcomm Kryo 485 Octacore | VGG16/ResNet-18/MobileNetV2（准确率）：93.5/94.2/94.6
    | - | 2.52ms/3.39ms/2.98ms | - | - |'
- en: '|  | Qualcomm Adreno 640 | 2.32ms/3.28ms/2.97ms |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | Qualcomm Adreno 640 | 2.32ms/3.28ms/2.97ms |'
- en: '| CIFAR-100 | Qualcomm Kryo 485 Octacore | VGG16/ResNet-18/MobileNetV2(acc):
    72.2/75.3/78.7 | 2.98ms/3.91ms/3.63ms |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 | Qualcomm Kryo 485 Octacore | VGG16/ResNet-18/MobileNetV2（准确率）：72.2/75.3/78.7
    | 2.98ms/3.91ms/3.63ms |'
- en: '|  | Qualcomm Adreno 640 | 3.07ms/3.70ms/3.50ms |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | Qualcomm Adreno 640 | 3.07ms/3.70ms/3.50ms |'
- en: '| [[163](#bib.bib163)] | CIFAR-10 | - | VGG/ResNet-20/ResNet56(Top1 acc): 94.00/92.01/94.05
    | 98.8M/20.8M/59.5M | - | - | - |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| [[163](#bib.bib163)] | CIFAR-10 | - | VGG/ResNet-20/ResNet56（Top1 准确率）：94.00/92.01/94.05
    | 98.8M/20.8M/59.5M | - | - | - |'
- en: '| ImageNet | Google Pixel 2 | MobleNetV1/MobileNetV2(Top1 acc): 68.06/69.13
    | 149M/149M | 10ms/13ms |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | Google Pixel 2 | MobleNetV1/MobileNetV2（Top1 准确率）：68.06/69.13
    | 149M/149M | 10ms/13ms |'
- en: '|  |  | MobleNetV1/MobileNetV2(Top1 acc): 70.92/71.54 | 283M/219M | 15ms/17ms
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobleNetV1/MobileNetV2（Top1 acc）：70.92/71.54 | 283M/219M | 15ms/17ms
    |'
- en: '| [[26](#bib.bib26)] | CIFAR-10 | - | Plain-20/ResNet56(acc): 90.2/91.9 | 50%/50%
    | - | - | - |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] | CIFAR-10 | - | Plain-20/ResNet56（acc）：90.2/91.9 | 50%/50%
    | - | - | - |'
- en: '| ImageNet |  | VGG16/MobileNetV1/MobileNetV2(acc): $\downarrow$1.4/$\downarrow$1.7/$\downarrow$1.0
    | 20%/40%/50% |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  | VGG16/MobileNetV1/MobileNetV2（acc）：$\downarrow$1.4/$\downarrow$1.7/$\downarrow$1.0
    | 20%/40%/50% |'
- en: '|  | Google Pixel 1 | MobileNetV1(Top1/Top5 acc): 70.2/89.2 | 272M | 16.0FPS
    | 13.2MB |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|  | Google Pixel 1 | MobileNetV1（Top1/Top5 acc）：70.2/89.2 | 272M | 16.0FPS
    | 13.2MB |'
- en: '|  |  | MobileNetV1(Top1/Top5 acc): 70.5/89.3 | 285M | 14.6FPS | 14.3MB |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileNetV1（Top1/Top5 acc）：70.5/89.3 | 285M | 14.6FPS | 14.3MB |'
- en: '| [[164](#bib.bib164)] | CIFAR-10 | Qualcomm Adreno 640 | VGG16/ReNet-18(Acc):
    93.21/93.81 | 8.8 $\times$/12.2 $\times$ | 2.7ms/1.45ms | - | - |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| [[164](#bib.bib164)] | CIFAR-10 | Qualcomm Adreno 640 | VGG16/ReNet-18（Acc）：93.21/93.81
    | 8.8 $\times$/12.2 $\times$ | 2.7ms/1.45ms | - | - |'
- en: '| ImageNet | - | VGG16/ReNet-18(Top5 acc): $\downarrow$0.6/$\downarrow$0.1
    | - | - | 6.4$\times$/3.3$\times$ |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | - | VGG16/ReNet-18（Top5 acc）：$\downarrow$0.6/$\downarrow$0.1 |
    - | - | 6.4$\times$/3.3$\times$ |'
- en: '| [[165](#bib.bib165)] | CIFAR-10 | ReRAM simulator | AlexNet/VGG16/Plain20(Acc5):
    99.10/98.62/98.29 | - | - | - | 14.3 $\times$/11.9$\times$/10.3$\times$ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| [[165](#bib.bib165)] | CIFAR-10 | ReRAM模拟器 | AlexNet/VGG16/Plain20（Acc5）：99.10/98.62/98.29
    | - | - | - | 14.3 $\times$/11.9$\times$/10.3$\times$ |'
- en: '| MNIST |  | AlexNet/VGG16/Plain20(Acc1): 98.49/98.63/98.00 | 21.4$\times$/19.3$\times$/6.2$\times$
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| MNIST |  | AlexNet/VGG16/Plain20（Acc1）：98.49/98.63/98.00 | 21.4$\times$/19.3$\times$/6.2$\times$
    |'
- en: '| [[166](#bib.bib166)] | CIFAR-10 | - | ResNet20/ResNet32/ResNet56(Top1 acc):
    92.06/92.60/93.66 | $\downarrow$48.35%/$\downarrow$45.05%/$\downarrow$52.00% |
    - | - | - |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| [[166](#bib.bib166)] | CIFAR-10 | - | ResNet20/ResNet32/ResNet56（Top1 acc）：92.06/92.60/93.66
    | $\downarrow$48.35%/$\downarrow$45.05%/$\downarrow$52.00% | - | - | - |'
- en: '| CIFAR-100 |  | ResNet20/ResNet32/ResNet56(Top1 acc): 66.93/68.98/70.50 |
    $\downarrow$46.35%/$\downarrow$55.17%/$\downarrow$55.29% |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 |  | ResNet20/ResNet32/ResNet56（Top1 acc）：66.93/68.98/70.50 | $\downarrow$46.35%/$\downarrow$55.17%/$\downarrow$55.29%
    |'
- en: '| ImageNet |  | ResNet18/ResNet34/ResNet50/MobileNetV2(Top1 acc): | $\downarrow$46.70%/$\downarrow$48.72%/
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  | ResNet18/ResNet34/ResNet50/MobileNetV2（Top1 acc）： | $\downarrow$46.70%/$\downarrow$48.72%/
    |'
- en: '|  |  | 69.65/73.56/76.84/71.60 | $\downarrow$50.60%/$\downarrow$51.20% |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 69.65/73.56/76.84/71.60 | $\downarrow$50.60%/$\downarrow$51.20% |'
- en: '| [[167](#bib.bib167)] | UCMerced Land Use | - | AlexNet(Acc): 94.1 | - | -
    | 1.17M | - |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| [[167](#bib.bib167)] | UCMerced Land Use | - | AlexNet（Acc）：94.1 | - | -
    | 1.17M | - |'
- en: '| Describable Textures |  | AlexNet(Acc): 52.8 | 2.41M |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Describable Textures |  | AlexNet（Acc）：52.8 | 2.41M |'
- en: '| [[168](#bib.bib168)] | ImageNet | Intel Core(TM) i7-4790 | CaffeNet(Top1
    acc): 53.70 | - | 69.7ms/batch | - | - |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| [[168](#bib.bib168)] | ImageNet | Intel Core(TM) i7-4790 | CaffeNet（Top1
    acc）：53.70 | - | 69.7ms/batch | - | - |'
- en: '| Describable Textures | Intel Core(TM) i7-7700 | CaffeNet(Top1 acc): 59.00
    | 58.2ms/batch |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| Describable Textures | Intel Core(TM) i7-7700 | CaffeNet（Top1 acc）：59.00
    | 58.2ms/batch |'
- en: '| [[169](#bib.bib169)] | ImageNet | - | MobileNetV1(Top1/Top5 acc): 49.34/74.52
    | 50% | - | - | - |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| [[169](#bib.bib169)] | ImageNet | - | MobileNetV1（Top1/Top5 acc）：49.34/74.52
    | 50% | - | - | - |'
- en: '|  |  | MobileNetV2(Top1/Top5 acc): 52.86/78.57 | 60% |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileNetV2（Top1/Top5 acc）：52.86/78.57 | 60% |'
- en: '| [[170](#bib.bib170)] | UCSD | NVIDIA MX250 | YOLOv3(mAP): 69.6 | 4458M |
    16ms | 4.685M | - |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| [[170](#bib.bib170)] | UCSD | NVIDIA MX250 | YOLOv3（mAP）：69.6 | 4458M | 16ms
    | 4.685M | - |'
- en: '| Mobile Robot Detection |  | YOLOv3(mAP): 92.1 | 327M | 3ms | 0.299M |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 移动机器人检测 |  | YOLOv3（mAP）：92.1 | 327M | 3ms | 0.299M |'
- en: '| Sim2real Detection (sim) |  | YOLOv3(mAP): 98.0 | 1581M | 8ms | 2.545M |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| Sim2real Detection (sim) |  | YOLOv3（mAP）：98.0 | 1581M | 8ms | 2.545M |'
- en: '| Sim2real Detection (realr) |  | YOLOv3(mAP): 76.1 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Sim2real Detection (realr) |  | YOLOv3（mAP）：76.1 |'
- en: '| [[171](#bib.bib171)] | CIFAR-10 | - | VGG11/ResNet-14(Top1 acc): 86.30/89.84
    | - | 1.4$\times$/1.2$\times$ | - | 0.94MB/0.73MB |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| [[171](#bib.bib171)] | CIFAR-10 | - | VGG11/ResNet-14（Top1 acc）：86.30/89.84
    | - | 1.4$\times$/1.2$\times$ | - | 0.94MB/0.73MB |'
- en: '| ImageNet |  | ResNet-18(Top1 acc): 59.30 | 1.4$\times$ | 4.36MB |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  | ResNet-18（Top1 acc）：59.30 | 1.4$\times$ | 4.36MB |'
- en: '| [[172](#bib.bib172)] | ImageNet | Google Pixel 1 | MobileNetV1(Top1 acc):
    46.3/69.1 | - | 6.01ms/74.9ms | - | - |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| [[172](#bib.bib172)] | ImageNet | Google Pixel 1 | MobileNetV1（Top1 acc）：46.3/69.1
    | - | 6.01ms/74.9ms | - | - |'
- en: '|  |  | MobileNetV2(Top1/Top5 acc): 70.9/70.0 | 61.6ms/53.5ms |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileNetV2（Top1/Top5 acc）：70.9/70.0 | 61.6ms/53.5ms |'
- en: '| [[173](#bib.bib173)] | ImageNet | Google Pixel 1 | MobileNetV3(Top1 acc):
    77.0/78.5 | 225M/314M | 51ms/- | - | - |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| [[173](#bib.bib173)] | ImageNet | Google Pixel 1 | MobileNetV3（Top1 acc）：77.0/78.5
    | 225M/314M | 51ms/- | - | - |'
- en: 'TABLE V: Summary of Automated Pruning Studies (cont.).'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE V: 自动剪枝研究总结（续）。'
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters | Memory |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参考文献 | 数据集 | 硬件 | 准确率 | FLOPs | 延迟 | 参数数量 | 内存 |'
- en: '| Automated Pruning | [[174](#bib.bib174)] | ImageNet | Galaxy S9 | ResNet18/ResNet34/ResNet50(Top1
    acc): 67.56/72.77/74.45 | 1030M/2130M/1830M | 190ms/310ms/310ms | - | - |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 自动修剪 | [[174](#bib.bib174)] | ImageNet | Galaxy S9 | ResNet18/ResNet34/ResNet50(Top1准确率):
    67.56/72.77/74.45 | 1030M/2130M/1830M | 190ms/310ms/310ms | - | - |'
- en: '|  |  | ResNet18/ResNet34/ResNet50(Top5 acc): 87.90/90.99/92.21 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ResNet18/ResNet34/ResNet50(Top5准确率): 87.90/90.99/92.21 |'
- en: '| CIFAR-100 | - | ResNet32/ResNet56/ResNet110(acc): 72.20/72.57/74.69 | 39.4M/58.4M/114M
    | - |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 | - | ResNet32/ResNet56/ResNet110(准确率): 72.20/72.57/74.69 | 39.4M/58.4M/114M
    | - |'
- en: '| CIFAR-10 |  | ResNet20/ResNet32/ResNet56/ResNet110/MobileNetV1(acc): | 19.1M/31.9M/36.4M/
    |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-10 |  | ResNet20/ResNet32/ResNet56/ResNet110/MobileNetV1(准确率): | 19.1M/31.9M/36.4M/
    |'
- en: '|  |  | 91.87/93.49/93.53/95.02/91.87 | 101M/115M |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 91.87/93.49/93.53/95.02/91.87 | 101M/115M |'
- en: '| [[175](#bib.bib175)] | CIFAR-10 | - | VGGNet/DenseNet40/ResNet164(error):
    6.20/5.65/5.27 | 391M/240M/275M | - | 2.30M/0.35M/1.10M | - |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| [[175](#bib.bib175)] | CIFAR-10 | - | VGGNet/DenseNet40/ResNet164(错误率): 6.20/5.65/5.27
    | 391M/240M/275M | - | 2.30M/0.35M/1.10M | - |'
- en: '| CIFAR-100 |  | VGGNet/DenseNet40/ResNet164(error): 26.52/25.72/23.91 | 501M/281M/247M
    | 5.00M/0.46M/1.21M |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 |  | VGGNet/DenseNet40/ResNet164(错误率): 26.52/25.72/23.91 | 501M/281M/247M
    | 5.00M/0.46M/1.21M |'
- en: '| SVHN |  | VGGNet/DenseNet40/ResNet164(error): 2.06/1.81/1.81 | 398M/267M/225M
    | 3.04M/0.44M/1.12M |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| SVHN |  | VGGNet/DenseNet40/ResNet164(错误率): 2.06/1.81/1.81 | 398M/267M/225M
    | 3.04M/0.44M/1.12M |'
- en: '| [[24](#bib.bib24)] | ImageNet | - | MobileNetV2(Top1 acc): 66.83/73.32/74.0
    | 102M/209M/305M | - | - | - |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| [[24](#bib.bib24)] | ImageNet | - | MobileNetV2(Top1准确率): 66.83/73.32/74.0
    | 102M/209M/305M | - | - | - |'
- en: '| [[176](#bib.bib176)] | CIFAR-10 | - | ResNet20/ResNet56/ResNet110/ | 51.80%/39.07%/21.63%/
    | - | 56.13%/41.10%/22.40%/ | - |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| [[176](#bib.bib176)] | CIFAR-10 | - | ResNet20/ResNet56/ResNet110/ | 51.80%/39.07%/21.63%/
    | - | 56.13%/41.10%/22.40%/ | - |'
- en: '|  |  | ResNet164/DenseNet-12-40(Top1 error): | 21.78%/29.52% | 20.46%/26.01%
    |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ResNet164/DenseNet-12-40(Top1错误率): | 21.78%/29.52% | 20.46%/26.01%
    |'
- en: '|  |  | 8.46/7.06/6.61/6.30/6.51 |  |  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 8.46/7.06/6.61/6.30/6.51 |  |  |'
- en: '| Tiny-ImageNet |  | MobileNetV1/MobileNetV2(Top1 error): 51.63/52.43 | 51.91%/11.92%
    | 36.95%/6.50% |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| Tiny-ImageNet |  | MobileNetV1/MobileNetV2(Top1错误率): 51.63/52.43 | 51.91%/11.92%
    | 36.95%/6.50% |'
- en: '| [[178](#bib.bib178)] | CIFAR-10 | - | ResNet20/ResNet32/ResNet56/ResNet110/ResNet164(acc):
    | 22.4M/35.0M/59.5M/ | - | - | - |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| [[178](#bib.bib178)] | CIFAR-10 | - | ResNet20/ResNet32/ResNet56/ResNet110/ResNet164(准确率):
    | 22.4M/35.0M/59.5M/ | - | - | - |'
- en: '|  |  | 92.88/93.16/93.69/94.33/94.00 | 119M/178M |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 92.88/93.16/93.69/94.33/94.00 | 119M/178M |'
- en: '| CIFAR-100 |  | ResNet20/ResNet32/ResNet56/ResNet110/ResNet164(acc): | 22.4M/42.5M/61.2M/
    |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 |  | ResNet20/ResNet32/ResNet56/ResNet110/ResNet164(准确率): | 22.4M/42.5M/61.2M/
    |'
- en: '|  |  | 68.90/72.41/72.25/73.16/77.76 | 120M/171M |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 68.90/72.41/72.25/73.16/77.76 | 120M/171M |'
- en: '| ImageNet |  | ResNet18/ResNet50(Top1 acc): 69.15/76.20 | 1210M/2310M |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  | ResNet18/ResNet50(Top1准确率): 69.15/76.20 | 1210M/2310M |'
- en: '|  |  | ResNet18/ResNet50(Top5 acc): 89.19/93.07 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ResNet18/ResNet50(Top5准确率): 89.19/93.07 |'
- en: '| [[179](#bib.bib179)] | CIFAR-10 | - | ResNet164(test error): 4.58/5.01 |
    190M/133M | - | - | - |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| [[179](#bib.bib179)] | CIFAR-10 | - | ResNet164(测试错误率): 4.58/5.01 | 190M/133M
    | - | - | - |'
- en: '| CIFRA-100 |  | ResNet164(test error): 21.63/22.38 | 178M/123M |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 |  | ResNet164(测试错误率): 21.63/22.38 | 178M/123M |'
- en: '| [[180](#bib.bib180)] | MS COCO | NVIDIA V100 | (AP/AP.5/AP.7/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$):
    | - | 25.4 FPS | - | - |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| [[180](#bib.bib180)] | MS COCO | NVIDIA V100 | (AP/AP.5/AP.7/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$):
    | - | 25.4 FPS | - | - |'
- en: '|  |  | 42.3/62.6/46.2/26.2/45.1/50.6 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 42.3/62.6/46.2/26.2/45.1/50.6 |'
- en: '|  |  | 43.9/63.8/47.9/27.0/46.8/52.8 | 23.3 FPS |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 43.9/63.8/47.9/27.0/46.8/52.8 | 23.3 FPS |'
- en: '|  |  | 50.7/69.6/55.4/31.3/53.8/64.0 | 10.1 FPS |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 50.7/69.6/55.4/31.3/53.8/64.0 | 10.1 FPS |'
- en: IV-D Automated Quantization
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 自动量化
- en: 'Unlike the above three compression techniques, which struggle to optimize network
    architectures, quantization appeals to reduce the representation precision of
    network weights and intermediate activation tensors. Neural networks are generally
    trained using the 32-bit floating-point precision; if we were to perform network
    inference in the 32-bit floating-point as well, MAC operations, data transfer,
    and data saving would have to be done all in 32-bit floating-point. Hence, using
    lower bit precision would substantially reduce the hardware overhead, including
    communication bandwidth, computation and memory usage [[182](#bib.bib182)]. For
    example, when moving from 32 to 8 bits, the memory cost and computation would
    decrease by a factor of 4 and 16 respectively. In addition, fixed-point computation
    is more efficient than its floating-point counterpart [[182](#bib.bib182)]. The
    most commonly explored quantization scheme, uniform quantization, converts a floating
    point tensor $\boldsymbol{x}=\{x_{1},...,x_{N}\}$ with range $(x_{min},x_{max})$
    into its integer coding $\boldsymbol{x}_{q}$ with range $[n,p]=[0,2^{b}-1]$ via
    the following definition [[183](#bib.bib183)]:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述三种压缩技术不同，量化旨在降低网络权重和中间激活张量的表示精度。神经网络通常使用32位浮点精度进行训练；如果我们在32位浮点中进行网络推理，那么MAC操作、数据传输和数据保存都必须在32位浮点中完成。因此，使用较低的位精度可以大幅减少硬件开销，包括通信带宽、计算和内存使用[[182](#bib.bib182)]。例如，从32位转换到8位时，内存成本和计算量分别会减少4倍和16倍。此外，定点计算比浮点计算更高效[[182](#bib.bib182)]。最常见的量化方案，均匀量化，将范围为$(x_{min},x_{max})$的浮点张量$\boldsymbol{x}=\{x_{1},...,x_{N}\}$转换为范围为$[n,p]=[0,2^{b}-1]$的整数编码$\boldsymbol{x}_{q}$，通过以下定义[[183](#bib.bib183)]：
- en: '|  | $\boldsymbol{x}_{q}=\lfloor\text{clamp}(\frac{\boldsymbol{x}}{\Delta}+z;n,p)\rceil,$
    |  | (28) |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{x}_{q}=\lfloor\text{clamp}(\frac{\boldsymbol{x}}{\Delta}+z;n,p)\rceil,$
    |  | (28) |'
- en: '|  | $\Delta=\frac{x_{max}-x_{min}}{p-n},\>z=\text{clamp}(-\lfloor\frac{x_{min}}{\Delta}\rceil+z;n,p),$
    |  | (29) |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta=\frac{x_{max}-x_{min}}{p-n},\>z=\text{clamp}(-\lfloor\frac{x_{min}}{\Delta}\rceil+z;n,p),$
    |  | (29) |'
- en: '|  | $\boldsymbol{\hat{x}}=(\boldsymbol{x}_{q}-z)\Delta,$ |  | (30) |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\hat{x}}=(\boldsymbol{x}_{q}-z)\Delta,$ |  | (30) |'
- en: 'where $\Delta$ is the scale factor that specifies the step size of the quantizer;
    $z$ is the zero-point that represents the real value zero without any precision
    loss; $b$ is the quantization bitwidth; $\lfloor\cdot\rceil$ is the round-to-nearest
    operation. The $clamp(\cdot)$ function is to truncate all values to fall between
    $n$ and $p$. Through this procedure, two sorts of errors are induced: a clipping
    error induced by the clamp function and a rounding error induced by the round-to-nearest
    operation $\lfloor\cdot\rceil$. The quantized tensor $\boldsymbol{\hat{x}}$ is
    then used for efficient but low precision computation. Since there are no theoretical
    correlations between the model accuracy and $b$, simple try-and-error is only
    feasible in the scenario where all weights and activation tensors have the same
    precision. However, different layers have different redundancy and behaviours
    on both the hardware and the task. Therefore, automatically determining the bitwidth
    of weights and activations for each layer is stunningly attractive, and this mixed-precision
    feature has recently been supported by hardware manufacturers [[184](#bib.bib184),
    [185](#bib.bib185)].'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\Delta$是指定量化器步长的缩放因子；$z$是表示真实值零而没有任何精度损失的零点；$b$是量化位宽；$\lfloor\cdot\rceil$是四舍五入操作。$clamp(\cdot)$函数用于将所有值截断到$n$和$p$之间。通过这个过程，产生了两种误差：由clamp函数引起的裁剪误差和由四舍五入操作$\lfloor\cdot\rceil$引起的舍入误差。然后，量化张量$\boldsymbol{\hat{x}}$用于高效但低精度的计算。由于模型准确度与$b$之间没有理论上的关联，只有在所有权重和激活张量具有相同精度的情况下，简单的试错法才是可行的。然而，不同的层在硬件和任务上具有不同的冗余和行为。因此，自动确定每层权重和激活的位宽是极具吸引力的，这种混合精度特性最近得到了硬件制造商的支持[[184](#bib.bib184),
    [185](#bib.bib185)]。
- en: 'One direction is to model the layer-wise bit-precision assignment as a reinforcement
    learning problem, where the action is to determine the bitwidth values [[25](#bib.bib25),
    [186](#bib.bib186)]. HAQ [[25](#bib.bib25), [186](#bib.bib186)] is such a representative
    work. It utilizes the DDPG agent to explore a continuous action space of [0, 1],
    from which selected actions (i.e., real numbers) can be rounded into discrete
    bitwidth values for the weights and activations of each layer. The use of a continuous
    action space is based on the consideration that a discrete action space is not
    able to represent the relative order of bitwidth. The quantized model is then
    retrained one more epoch to recover the accuracy. Since the authors induce the
    hardware constraints by limiting the search space, they only consider minimizing
    the accuracy drop after quantization in the reward function. Another direction
    is to develop a differentiable framework that searches for the quantization bitwidth
    in the same way as searching for the network architecture [[187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191),
    [192](#bib.bib192)]. Given a network to be quantized, a supernet is first constructed
    by inserting multiple parallel quantization operations into every pair of adjacent
    layers of the network. Each inserted operation constitutes an edge from the $i$-th
    layer to the ($i+1$)-th layer and is associated with a learnable parameter. Multiple
    parallel edges/operations on the same level are of different quantization bitwidths
    and summarized by assembling all edges with the weights derived from the learnable
    parameters. Then, the network parameters and the quantization weights are jointly
    trained with the combination of accuracy and model size as the target. Wu et al.
    [[192](#bib.bib192)] and Xu et al. [[189](#bib.bib189)] simply try to diminish
    the model size while Yu et al. [[188](#bib.bib188)] and Wei et al. [[191](#bib.bib191)]
    consider hardware constraints in the training target. SSPS [[190](#bib.bib190)]
    further borrows the idea from [[23](#bib.bib23)] to use a one-hot mask instead
    of continuous weights to select the bitwidth. In this way, only one path is active
    during training so the search cost is considerably reduced. The bitwidth search
    space is usually below eight since a previous study has shown that simple 8-bit
    post-training quantization is able to achieve marginal accuracy degradation [[193](#bib.bib193)].
    We summarize the main studies of automated quantization in TABLE [VI](#S4.T6 "TABLE
    VI ‣ IV-D Automated Quantization ‣ IV Automated Compression of Deep Learning Models
    ‣ Design Automation for Fast, Lightweight, and Effective Deep Learning Models:
    A Survey").'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '一种方法是将逐层位宽分配建模为强化学习问题，其中动作是确定位宽值[[25](#bib.bib25), [186](#bib.bib186)]。HAQ
    [[25](#bib.bib25), [186](#bib.bib186)]就是一个代表性工作。它利用DDPG代理探索连续动作空间[0, 1]，从中选择的动作（即实数）可以被四舍五入成每层的权重和激活的离散位宽值。使用连续动作空间是基于这样一种考虑：离散动作空间无法表示位宽的相对顺序。然后，对量化模型进行再训练一个周期以恢复准确性。由于作者通过限制搜索空间来引入硬件约束，他们只在奖励函数中考虑最小化量化后的准确性下降。另一种方法是开发一个可微分框架，按搜索网络架构的方式来搜索量化位宽[[187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191),
    [192](#bib.bib192)]。给定一个需要量化的网络，首先通过在网络的每对相邻层中插入多个并行量化操作来构建一个超网络。每个插入的操作构成从第$i$层到第($i+1$)层的边，并且与一个可学习的参数相关联。同一层上的多个并行边/操作具有不同的量化位宽，并通过汇总所有边及其从可学习参数得出的权重来总结。然后，网络参数和量化权重一起训练，以准确性和模型大小的组合作为目标。Wu等[[192](#bib.bib192)]和Xu等[[189](#bib.bib189)]只是简单地尝试减小模型大小，而Yu等[[188](#bib.bib188)]和Wei等[[191](#bib.bib191)]在训练目标中考虑了硬件约束。SSPS
    [[190](#bib.bib190)]进一步借鉴了[[23](#bib.bib23)]的思想，使用一热掩码代替连续权重来选择位宽。这样，训练过程中只有一条路径是活跃的，因此搜索成本大大降低。位宽搜索空间通常低于八，因为先前的研究表明，简单的8位后训练量化能够实现轻微的准确性下降[[193](#bib.bib193)]。我们在TABLE
    [VI](#S4.T6 "TABLE VI ‣ IV-D Automated Quantization ‣ IV Automated Compression
    of Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey")中总结了自动化量化的主要研究。'
- en: 'TABLE VI: Summary of Automated Quantization Studies. The “hardware” column
    indicates on which the achieved models are evaluated. The model name (e.g., MobileNetV1)
    in the “Accuracy” column indicates the original model before quantization. The
    latency is reported per input. $n\times$ indicates that the original model is
    $n$ times its quantized counterpart. ‘-’ indicates unavailable records.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 自动量化研究总结。“硬件”列指示了模型评估所使用的硬件。 “准确度”列中的模型名称（例如，MobileNetV1）指的是量化前的原始模型。延迟是针对每个输入报告的。
    $n\times$ 表示原始模型是其量化对应物的 $n$ 倍。‘-’ 表示记录不可用。'
- en: '| Method | Reference | Dataset | Hardware | Accuracy | Bitwdith | Latency |
    Model Size |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参考文献 | 数据集 | 硬件 | 准确度 | 位宽 | 延迟 | 模型大小 |'
- en: '| Automated Quantization | [[25](#bib.bib25), [186](#bib.bib186)] | ImageNet
    |  | MobileNetV1 (Top1 acc): 67.40/70.58/71.20 | - | 45.51ms/57.70ms/70.35ms |
    - |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 自动量化 | [[25](#bib.bib25), [186](#bib.bib186)] | ImageNet |  | MobileNetV1
    (Top1 acc): 67.40/70.58/71.20 | - | 45.51ms/57.70ms/70.35ms | - |'
- en: '|  | BISMO on | MobileNetV1 (Top5 acc): 87.90/89.77/90.19 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  | BISMO on | MobileNetV1 (Top5 acc): 87.90/89.77/90.19 |'
- en: '|  | Xilinx Zynq-7020 | MobileNetV2 (Top1 acc): 66.99/70.90/71.89 | 52.12ms/66.92ms/82.34ms
    |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|  | Xilinx Zynq-7020 | MobileNetV2 (Top1 acc): 66.99/70.90/71.89 | 52.12ms/66.92ms/82.34ms
    |'
- en: '|  |  | MobileNetV2 (Top5 acc): 87.33/89.91/80.36 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileNetV2 (Top5 acc): 87.33/89.91/80.36 |'
- en: '|  |  | MobileNetV1 (Top1 acc): 65.33/69.97/71.20 | 57.40ms/77.49ms/99.66ms
    |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileNetV1 (Top1 acc): 65.33/69.97/71.20 | 57.40ms/77.49ms/99.66ms
    |'
- en: '|  | BISMO on | MobileNetV1 (Top5 acc): 86.60/89.37/90.08 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|  | BISMO on | MobileNetV1 (Top5 acc): 86.60/89.37/90.08 |'
- en: '|  | Xilinx VU9P | MobileNetV2 (Top1 acc): 67.01/69.45/71.85 | 73.97ms/99.07ms/127.03ms
    |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  | Xilinx VU9P | MobileNetV2 (Top1 acc): 67.01/69.45/71.85 | 73.97ms/99.07ms/127.03ms
    |'
- en: '|  |  | MobileNetV2 (Top5 acc): 87.46/88.94/90.24 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileNetV2 (Top5 acc): 87.46/88.94/90.24 |'
- en: '|  | BitFusion | MobileNetV1 (Top1 acc): 67.45/70.40/70.90 | 7.86ms/11.09ms/19.98ms
    |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  | BitFusion | MobileNetV1 (Top1 acc): 67.45/70.40/70.90 | 7.86ms/11.09ms/19.98ms
    |'
- en: '|  | MobileNetV1 (Top5 acc): 87.85/89.69/89.95 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  | MobileNetV1 (Top5 acc): 87.85/89.69/89.95 |'
- en: '|  | - | MobileNetV1 (Top1 acc): 57.14/67.66/71.74 | - | 1.09MB/1.58MB/2.07MB
    |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  | - | MobileNetV1 (Top1 acc): 57.14/67.66/71.74 | - | 1.09MB/1.58MB/2.07MB
    |'
- en: '|  | MobileNetV1 (Top5 acc): 81.87/88.21/90.36 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '|  | MobileNetV1 (Top5 acc): 81.87/88.21/90.36 |'
- en: '|  | MobileNetV2 (Top1 acc): 66.75/70.90/71.47 | 0.95MB/1.38MB/1.79MB |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '|  | MobileNetV2 (Top1 acc): 66.75/70.90/71.47 | 0.95MB/1.38MB/1.79MB |'
- en: '|  | MobileNetV2 (Top5 acc): 87.32/89.76/90.23 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  | MobileNetV2 (Top5 acc): 87.32/89.76/90.23 |'
- en: '|  | ResNet50 (Top1 acc): 70.63/75.30/76.14 | 6.30MB/9.22MB/12.14MB |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNet50 (Top1 acc): 70.63/75.30/76.14 | 6.30MB/9.22MB/12.14MB |'
- en: '|  | ResNet50 (Top5 acc): 89.93/92.45/92.89 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNet50 (Top5 acc): 89.93/92.45/92.89 |'
- en: '| [[187](#bib.bib187)] | MNIST | - | LeNet5 (Val. error): 1.14/1.69/2.32 |
    - | - | - |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| [[187](#bib.bib187)] | MNIST | - | LeNet5 (验证错误): 1.14/1.69/2.32 | - | -
    | - |'
- en: '| ImageNet | AlexNet (Val. error): 47.46/47.69/48.54 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | AlexNet (验证错误): 47.46/47.69/48.54 |'
- en: '| [[188](#bib.bib188)] | CIFAR-10 | - | ResNet20 (Top1 acc): 92.30/92.12/92.04
    | Avg: 3.5/3.3/2.9 | - | 10.19$\times$/10.74$\times$/12.08$\times$ |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| [[188](#bib.bib188)] | CIFAR-10 | - | ResNet20 (Top1 acc): 92.30/92.12/92.04
    | 平均: 3.5/3.3/2.9 | - | 10.19$\times$/10.74$\times$/12.08$\times$ |'
- en: '| ImageNet | ResNet50 (Top1 acc): 76.67/75.71 | Avg: 3.8/2.9 | - |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | ResNet50 (Top1 acc): 76.67/75.71 | 平均: 3.8/2.9 | - |'
- en: '|  | ResNet50 (Top5 acc): 93.55/92.83 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNet50 (Top5 acc): 93.55/92.83 |'
- en: '| [[189](#bib.bib189)] | Penn Treebank Corpus | - | Transformer (Perplexity):
    56.82/58.23 | Avg: 2.0/2.2 | - | 6.5MB/4.8MB |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| [[189](#bib.bib189)] | Penn Treebank Corpus | - | Transformer (困惑度): 56.82/58.23
    | 平均: 2.0/2.2 | - | 6.5MB/4.8MB |'
- en: '| Conversational Telephone Speech | Transformer (Perplexity): 42.39/42.75 |
    Avg: 1.9/2.5 | 8.0MB/9.1MB |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 对话电话语音 | Transformer (困惑度): 42.39/42.75 | 平均: 1.9/2.5 | 8.0MB/9.1MB |'
- en: '| [[190](#bib.bib190)] | CIFAR-10 | - | ResNet20 (Top1 acc): 92.54 | Avg: 3.04
    | - | - |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| [[190](#bib.bib190)] | CIFAR-10 | - | ResNet20 (Top1 acc): 92.54 | 平均: 3.04
    | - | - |'
- en: '| ImageNet | ResNet18/ResNet34/ResNet50/MobileNetV2 (Top1 acc): | Avg: |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | ResNet18/ResNet34/ResNet50/MobileNetV2 (Top1 acc): | 平均: |'
- en: '|  | 70.70/74.30/76.22/69.10 | 3.95/4.01/3.98/4.02 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '|  | 70.70/74.30/76.22/69.10 | 3.95/4.01/3.98/4.02 |'
- en: '| COCO | Faster R-CNN (AP/AP.5/AP.75/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$)
    | Avg: 4.00 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| COCO | Faster R-CNN (AP/AP.5/AP.75/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$)
    | 平均: 4.00 |'
- en: '|  | 37.4/58.1/40.6/22.1/40.4/47.9 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|  | 37.4/58.1/40.6/22.1/40.4/47.9 |'
- en: '|  | RetinaNet (AP/AP.5/AP.75/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$)
    | Avg: 4.00 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  | RetinaNet (AP/AP.5/AP.75/$\text{AP}_{\text{S}}$/$\text{AP}_{\text{M}}$/$\text{AP}_{\text{L}}$)
    | 平均: 4.00 |'
- en: '|  | 36.4/55.8/38.6/20.8/39.9/47.6 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|  | 36.4/55.8/38.6/20.8/39.9/47.6 |'
- en: '| [[191](#bib.bib191)] | NWPU-RESISC45 | - | ResNet34 (acc): 92.66/92.75/92.73
    | - | - | 13.17MB/16.50MB/18.86MB |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| [[191](#bib.bib191)] | NWPU-RESISC45 | - | ResNet34 (准确度): 92.66/92.75/92.73
    | - | - | 13.17MB/16.50MB/18.86MB |'
- en: '|  | SqueezeNet (acc): 88.45/88.59/88.61 | 0.45MB/0.55MB/0.69MB |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  | SqueezeNet (准确度): 88.45/88.59/88.61 | 0.45MB/0.55MB/0.69MB |'
- en: '| [[192](#bib.bib192)] | CIFAR-10 | - | ResNet20/ResNet56/ResNet110 (acc):
    92.00/94.12/94.39 | - | - | 16.6$\times$/18.93$\times$/20.3$\times$ |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| [[192](#bib.bib192)] | CIFAR-10 | - | ResNet20/ResNet56/ResNet110 (acc):
    92.00/94.12/94.39 | - | - | 16.6$\times$/18.93$\times$/20.3$\times$ |'
- en: '| ImageNet | ResNet18/ResNet34 (acc): 69.58/73.37 | 21.1$\times$/19.0$\times$
    |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | ResNet18/ResNet34 (acc): 69.58/73.37 | 21.1$\times$/19.0$\times$
    |'
- en: V Joint Automated Design Strategies
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 共同自动化设计策略
- en: 'Neural architecture design and compression are divergent routines to achieve
    efficient deep learning models and a combination of these two strategies is an
    intuitive way to achieve even more efficiency than merely relying on either technique
    individually. The most natural way in this regard is a sequential pipeline: design
    then compress. Under the context of design automation, the pipeline is commonly
    recognized as a search-compress scheme. There are bare studies on combining tensor
    decomposition and NAS. We will thus concentrate on the other three compression
    techniques in this section. TABLE [VII](#S5.T7 "TABLE VII ‣ V-C2 Optimizing search
    and quantization jointly ‣ V-C Joint Search and Quantization ‣ V Joint Automated
    Design Strategies ‣ Design Automation for Fast, Lightweight, and Effective Deep
    Learning Models: A Survey") summarizes main references on joint automated design
    work.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构设计和压缩是实现高效深度学习模型的不同途径，这两种策略的结合是实现更高效率的直观方法，而不仅仅依赖于单一技术。在这方面，最自然的方法是顺序流程：首先设计然后压缩。在设计自动化的背景下，这一流程通常被称为搜索-压缩方案。目前关于结合张量分解和
    NAS 的研究较少。因此，我们将在本节中重点讨论其他三种压缩技术。表 [VII](#S5.T7 "TABLE VII ‣ V-C2 优化搜索与量化的联合 ‣
    V-C 共同搜索与量化 ‣ V 共同自动化设计策略 ‣ 设计自动化用于快速、轻量级和高效的深度学习模型：综述") 总结了主要的共同自动化设计工作参考文献。
- en: V-A Joint Search and Knowledge Distillation
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 共同搜索与知识蒸馏
- en: For knowledge distillation, the intuitive joint automated design strategy is
    to first search for a powerful teacher model and then achieve a compressed student
    model by automated knowledge distillation. An improved strategy is FasterSeg [[90](#bib.bib90)],
    which searches for student and teacher alternatively so as to ensure optimal teacher-student
    matching pairs and effective knowledge distillation. Instead of searching for
    the entire teacher model, Guan et al. [[194](#bib.bib194)] target the feature
    distillation and search for the optimal teacher’s feature aggregation for distillation.
    For each layer group in the teacher model, the authors try to search for the aggregation
    weights of each layer’s feature in that group; then the features are weighted
    sum to guide the knowledge distillation of the corresponding student layer. This
    work mimics the multi-teacher distillation scheme but with a single-teacher, multi-feature
    distillation proposal and automated teacher knowledge aggregation.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 对于知识蒸馏，直观的共同自动化设计策略是首先搜索一个强大的教师模型，然后通过自动化知识蒸馏实现一个压缩的学生模型。一种改进策略是 FasterSeg [[90](#bib.bib90)]，它交替搜索学生和教师，以确保最佳的教师-学生匹配对和有效的知识蒸馏。Guan
    等人 [[194](#bib.bib194)] 的研究并不是搜索整个教师模型，而是针对特征蒸馏，搜索最佳的教师特征聚合进行蒸馏。对于教师模型中的每个层组，作者尝试搜索该组中每个层特征的聚合权重；然后，将这些特征加权求和以指导相应学生层的知识蒸馏。这项工作模拟了多教师蒸馏方案，但提出了单教师、多特征蒸馏和自动化教师知识聚合。
- en: V-B Joint Search and Pruning
  id: totrans-481
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 共同搜索与剪枝
- en: 'Different from the search-compress scheme, pruning can be incorporated into
    neural architecture search as integrity. The aforementioned automated pruning
    approaches (in Section [IV-C](#S4.SS3 "IV-C Automated Pruning ‣ IV Automated Compression
    of Deep Learning Models ‣ Design Automation for Fast, Lightweight, and Effective
    Deep Learning Models: A Survey")) decide the pruning strategy based on the weights
    of trained networks. However, a recent study [[195](#bib.bib195)] indicates that
    the essence of network pruning is the pruned structure instead of the weights
    inherited from the base network, which means that the pruning strategy can be
    determined before the model is trained. In light of this critical conclusion,
    the research direction turns to finding optimal pruned structures, e.g., the channel
    number in each layer, rather than to determining important channels from the base
    network. This objective is basically the same as NAS so numerous works emerge
    to use NAS to achieve pruning, where the pruning strategy and network structure
    are sampled together before training in the search process [[196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201)]. However, different from pure NAS, the NAS-based pruning 1)
    requires an existing state-of-the-art model as the pruning base, 2) only searches
    the network hyperparameters (e.g., channel number, layer number) instead of the
    type of operators, and 3) is bounded by searching smaller hyperparameters than
    the base network. MetaPruning [[196](#bib.bib196)] is an early work in this direction,
    which attempts to find the optimal number of channels for each layer. It first
    constructs a PruningNet to generate weights for pruned networks so that fine-tuning
    is not required during search time; then it utilizes an evolutionary algorithm
    to search for the optimal combination of layers with different numbers of channels.
    The search procedure is highly efficient and no fine-tuning is obliged after searching.
    However, the search space is large especially when there are many channels in
    the base network. Some following works thus attempt to shrink the search space
    by only searching channel number ratios [[198](#bib.bib198)] or using a clustering
    algorithm to cluster channels in each layer as the initial number of channels
    in the pruned network for later search [[199](#bib.bib199)]. Other works [[200](#bib.bib200),
    [201](#bib.bib201)], on the other hand, seek to develop a performance estimator
    to accelerate the searching process. Overall, most NAS-based pruning process is
    analogous to searching optimal pruning rates but without pruning low magnitude
    channels at each iteration, instead, it trains from scratch with the searched
    structure or quickly derives the performance with an additional performance estimator.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '与搜索-压缩方案不同，剪枝可以作为完整性纳入神经架构搜索。上述自动化剪枝方法（在[IV-C](#S4.SS3 "IV-C Automated Pruning
    ‣ IV Automated Compression of Deep Learning Models ‣ Design Automation for Fast,
    Lightweight, and Effective Deep Learning Models: A Survey")部分）基于训练网络的权重决定剪枝策略。然而，最近的研究[[195](#bib.bib195)]表明，网络剪枝的本质是剪枝后的结构，而不是从基础网络继承的权重，这意味着剪枝策略可以在模型训练之前确定。鉴于这一关键结论，研究方向转向寻找最优剪枝结构，例如每层的通道数量，而不是从基础网络中确定重要的通道。这个目标基本上与NAS相同，因此出现了大量工作使用NAS来实现剪枝，其中剪枝策略和网络结构在训练之前的搜索过程中一起被采样[[196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201)]。然而，与纯NAS不同，基于NAS的剪枝1)需要现有的最先进模型作为剪枝基础，2)仅搜索网络超参数（例如通道数量、层数）而不是操作符的类型，并且3)受限于比基础网络更小的超参数搜索。MetaPruning
    [[196](#bib.bib196)]是这一方向的早期工作，尝试为每层找到最优的通道数量。它首先构建一个PruningNet来生成剪枝网络的权重，因此在搜索过程中无需微调；然后利用进化算法搜索具有不同通道数量的层的最佳组合。搜索过程非常高效，搜索后无需额外的微调。然而，尤其当基础网络中有许多通道时，搜索空间非常大。因此，一些后续工作尝试通过仅搜索通道数量比例[[198](#bib.bib198)]或使用聚类算法将每层的通道聚类为剪枝网络的初始通道数量来缩小搜索空间[[199](#bib.bib199)]。另一方面，其他工作[[200](#bib.bib200),
    [201](#bib.bib201)]则试图开发性能评估器以加速搜索过程。总体而言，大多数基于NAS的剪枝过程类似于搜索最优剪枝率，但在每次迭代中不进行低幅度通道的剪枝，而是用搜索到的结构从头开始训练或使用额外的性能评估器快速推导性能。'
- en: Alternative to unifying NAS and pruning by sampling the pruning strategy and
    model architecture before training, SpArSe [[202](#bib.bib202)] selects the pruning
    strategy based on a trained model but optimizes the pruning hyperparameters and
    model architecture jointly. Specifically, it first samples a configuration of
    the model architecture (e.g., operators and connectivity) and hyperparameters
    of pruning algorithms (i.e., Sparse Variational Dropout (SpVD) and Bayesian Compression
    (BC)); it then trains the model and prunes it. After evaluation, it optimizes
    the sampling configuration with multi-objective Bayesian optimization. In this
    way, both structured and unstructured pruning strategies can be optimized together
    with the model architecture.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过在训练前采样剪枝策略和模型架构来统一 NAS 和剪枝不同，SpArSe [[202](#bib.bib202)] 基于训练好的模型选择剪枝策略，但联合优化剪枝超参数和模型架构。具体来说，它首先对模型架构（例如，运算符和连接）和剪枝算法的超参数（即稀疏变分
    Dropout (SpVD) 和贝叶斯压缩 (BC)）进行配置采样；然后训练模型并进行剪枝。评估后，使用多目标贝叶斯优化优化采样配置。通过这种方式，可以同时优化结构化和非结构化剪枝策略以及模型架构。
- en: V-C Joint Search and Quantization
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C联合搜索与量化
- en: V-C1 Optimizing Search and quantization separately
  id: totrans-485
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C1 分开优化搜索和量化
- en: Some researchers consider optimizing the two components separately and particularly
    focus on automating the architecture design process. For example, Cai et al. use
    the ProxylessNAS [[23](#bib.bib23)] framework to firstly search for an efficient
    architecture and then perform quantization-aware fine-tuning to further compress
    the model [[203](#bib.bib203)]; Liu et al. [[204](#bib.bib204)] include the quantization-friendly
    activation function ReLU6 in the search space to facilitate the following quantization;
    Peter et al. [[205](#bib.bib205)] compare the performance of post-training quantization
    (PTQ) and quantization-aware training (QAT) and indicate that the PTQ performance
    drops rapidly when the bitwidth is smaller than 4 while QAT only has marginal
    drop even with 1-bit precision. This is due to the incompatibility between quantization
    and the network structure, which is primarily optimized for full precision.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究者考虑将两个组件分别优化，特别关注自动化架构设计过程。例如，Cai 等人使用 ProxylessNAS [[23](#bib.bib23)] 框架首先搜索高效的架构，然后进行量化感知微调以进一步压缩模型
    [[203](#bib.bib203)]；Liu 等人 [[204](#bib.bib204)] 在搜索空间中包含了量化友好的激活函数 ReLU6，以便于后续的量化；Peter
    等人 [[205](#bib.bib205)] 比较了后训练量化（PTQ）和量化感知训练（QAT）的性能，并指出当位宽小于 4 时，PTQ 性能会急剧下降，而
    QAT 即使在 1 位精度下也只有微小的下降。这是由于量化与网络结构的不兼容，后者主要针对全精度进行优化。
- en: In light of this, attentions are drawn to quantization-aware NAS, where the
    search space is based on quantized operators [[206](#bib.bib206), [207](#bib.bib207),
    [208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210)]. Most of these works
    are for binary neural networks (BNNs) because of their extreme computation and
    memory savings [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209),
    [211](#bib.bib211)]. Shen et al. [[209](#bib.bib209)] apply an evolutionary search
    to optimize the layer-wise channel expansion ratio for binarized VGG and ResNet.
    Phan et al. [[207](#bib.bib207)] discover that binary DWConv has limited representation
    capability and thus attempt to search for the group number for each layer of the
    MobileNet via an evolutionary algorithm as well. BATS [[206](#bib.bib206)] and
    BNAS [[208](#bib.bib208)] concurrently emerge in the sense of differentiable quantization-aware
    NAS. They both design a BNNs-oriented search space discarding the separable convolution
    due to its failure during quantization; BNAS keeps convolutions and dilated convolutions,
    and BATS keeps group convolutions and dilated group convolutions. It is interesting
    to note that the BATS binarizes only activations during training but keeps weights
    at full precision and binarizes the weights after training with a marginal drop
    in accuracy ($\sim$1%). Instead of optimizing the architecture as a whole, Xu
    et al. [[211](#bib.bib211)] probe layer-wise searching under the KD guidance for
    BNN object detectors.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，注意力转向了量化感知NAS，其中搜索空间基于量化操作符[[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209), [210](#bib.bib210)]。这些研究大多针对二进制神经网络（BNNs），因为它们在计算和内存上具有极大的节省[[206](#bib.bib206),
    [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209), [211](#bib.bib211)]。Shen
    等人 [[209](#bib.bib209)] 应用进化搜索来优化二值化 VGG 和 ResNet 的层级通道扩展比。Phan 等人 [[207](#bib.bib207)]
    发现二值化 DWConv 的表示能力有限，因此也尝试通过进化算法搜索 MobileNet 每层的组数。BATS [[206](#bib.bib206)] 和
    BNAS [[208](#bib.bib208)] 在可微分量化感知NAS方面同时出现。它们都设计了一个面向BNNs的搜索空间，舍弃了因量化失败的可分离卷积；BNAS
    保留卷积和扩张卷积，BATS 保留组卷积和扩张组卷积。有趣的是，BATS 只在训练过程中将激活量化，而保持权重的全精度，并在训练后将权重二值化，准确度下降很小（$\sim$1%）。Xu
    等人 [[211](#bib.bib211)] 则在KD指导下对BNN目标检测器进行逐层搜索，而不是整体优化架构。
- en: 'The above work only automatically optimizes the neural architectures while
    using fixed quantization policies. As a result, the final model has a large possibility
    to be sub-optimal: e.g., the BNN with the searched architecture is not necessarily
    smaller and more accurate than a mixed-precision model.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 以上工作仅自动优化神经架构，同时使用固定量化策略。因此，最终模型很可能不是最优的：例如，经过搜索的BNN不一定比混合精度模型更小或更准确。
- en: V-C2 Optimizing search and quantization jointly
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C2 联合优化搜索和量化
- en: 'Jointly optimizing the neural architecture and quantization policy can find
    the best combination of both worlds. The most intuitive joint approach is to construct
    a new search space that contains both structure and bitwidth choices, and the
    search process is the same as for traditional hardware-aware NAS [[212](#bib.bib212),
    [213](#bib.bib213)]. For example, JASQ [[212](#bib.bib212)] employs a classical
    evolutionary search algorithm to explore both operators and cell-wise quantization
    bitwidths; Gong et al. [[213](#bib.bib213)], instead, engage differentiable neural
    architecture search to explore a mixed-precision supernet of MBConv hyperparameters
    and bitwidths. Though simple and effective, this strategy is inefficient: the
    search space is enlarged by many times that JASQ requires three GPU days to search
    for a suitable model for each given resource budget.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 联合优化神经网络架构和量化策略可以找到两者的最佳组合。最直观的联合方法是构造一个包含结构和位宽选择的新搜索空间，搜索过程与传统的硬件感知NAS相同[[212](#bib.bib212),
    [213](#bib.bib213)]。例如，JASQ [[212](#bib.bib212)] 使用经典的进化搜索算法来探索操作符和单元量化位宽；而 Gong
    等人 [[213](#bib.bib213)] 则采用可微分神经架构搜索来探索MBConv超参数和位宽的混合精度超网。尽管这种策略简单有效，但效率低下：搜索空间扩大了很多倍，JASQ
    需要三天 GPU 时间来搜索适合的模型以满足给定的资源预算。
- en: 'To tackle this issue, some studies decouple the supernet training and search
    process so that no training is required in the search process [[105](#bib.bib105),
    [214](#bib.bib214), [215](#bib.bib215)]. The weight-sharing training paradigm
    is further adopted to reduce the training cost. SPOS [[105](#bib.bib105)] designs
    a single path weight-sharing strategy that trains a supernet independently to
    the search process. In each iteration of the supernet training, only one single
    path of the supernet is activated and trained. The single-path activation is realized
    by uniformly sampling one candidate block at each block level of the supernet.
    The weights of the same block are shared among different supernet training iterations
    (i.e., different paths). In the search process, an evolutionary algorithm is performed
    on randomly sampled paths of the pre-trained supernet to find the optimal combination
    of architecture and block-wise bitwidths. As the supernet is well pre-trained,
    each architecture only performs inference during the evolutionary search. This
    SPOS strategy improves the efficiency of both the supernet training and search.
    APQ [[214](#bib.bib214)] adopts the once-for-all [[77](#bib.bib77)] supernet training
    strategy, which requires little time for retraining the searched model but is
    inefficient to support different bitwidths in the supernet. Therefore, the authors
    devise a quantization-aware accuracy predictor so that they can train the once-for-all
    supernet in full precision and derive the accuracy of a quantized network via
    the accuracy predictor. However, it is hard and time-consuming to collect a large
    volume of [quantized network, accuracy] pairs for preparing the quantization-aware
    accuracy predictor due to lengthy quantization-aware training. To alleviate this
    issue, the authors first train a full precision accuracy predictor with abundant
    [full-precision network, accuracy] pairs from the supernet, and then fine-tune
    it with only a few [quantized network, accuracy] pairs to obtain its quantization-aware
    counterpart. Next, they then do a simple evolutionary search with the accuracy
    predictor to achieve a new model for a given resource budget. Instead of using
    the accuracy predictor to deal with the different bitwidths problem, Shen et al.
    [[215](#bib.bib215)] conceive the once quantization-aware (OQA) training strategy,
    where a set of quantized supernets with sequential bitwidths are created via the
    bit inheritance scheme: the $k-1$ bit supernet uses double quantization step size
    of its $k$ bit counterpart. The authors experimentally show that with just one
    epoch fine-tuning the lower-bit network with bit inheritance outperforms its counterpart
    with QAT from scratch. The final architecture is derived by a simple coarse-to-fine
    architecture selection procedure without any retraining. However, OQA only supports
    fixed precision quantization policies as well.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，一些研究将超网训练和搜索过程分离，使得搜索过程不需要训练[[105](#bib.bib105), [214](#bib.bib214),
    [215](#bib.bib215)]。进一步采用了权重共享训练范式以降低训练成本。SPOS [[105](#bib.bib105)] 设计了一种单路径权重共享策略，独立训练超网与搜索过程。在每次超网训练的迭代中，仅激活并训练超网的一个单路径。单路径激活通过在超网的每个块级别均匀采样一个候选块来实现。相同块的权重在不同的超网训练迭代（即不同路径）之间共享。在搜索过程中，对预训练超网的随机采样路径执行进化算法，以寻找架构和块级位宽的最佳组合。由于超网已得到良好的预训练，每种架构仅在进化搜索期间执行推理。该SPOS策略提高了超网训练和搜索的效率。APQ
    [[214](#bib.bib214)] 采用了“一次性全部”[[77](#bib.bib77)] 超网训练策略，该策略需要较少的时间来重新训练搜索到的模型，但在超网中支持不同位宽的效率较低。因此，作者设计了一个量化感知准确性预测器，以便他们可以在全精度下训练一次性超网，并通过准确性预测器推导量化网络的准确性。然而，由于量化感知训练过程漫长，收集大量[量化网络，准确性]对以准备量化感知准确性预测器是困难且耗时的。为了解决这个问题，作者首先使用来自超网的大量[全精度网络，准确性]对来训练一个全精度准确性预测器，然后用少量[量化网络，准确性]对进行微调，以获得其量化感知版本。接下来，他们使用准确性预测器进行简单的进化搜索，以在给定资源预算下获得一个新模型。Shen等人[[215](#bib.bib215)]则提出了一次性量化感知（OQA）训练策略，通过位继承方案创建一组具有顺序位宽的量化超网：$k-1$
    位超网使用其 $k$ 位对应体的两倍量化步长。作者通过实验表明，仅用一个时代微调具有位继承的低位网络，其性能优于从头开始进行QAT的对应网络。最终架构通过简单的粗到精的架构选择程序得到，无需任何重新训练。然而，OQA
    也仅支持固定精度量化策略。
- en: The most recent work, QFA [[183](#bib.bib183)], manages to allow joint mixed-precision
    quantization and architecture search without retraining. The authors propose a
    new quantization strategy, batch quantization, to stabilize the estimation of
    the scale factor so the mixed-precision supernet can be stably trained. They replace
    the progressive shrinking training strategy in the work [[215](#bib.bib215)] with
    a newly proposed two-stage training strategy reducing more than half training
    epochs.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的工作，QFA [[183](#bib.bib183)]，成功实现了联合混合精度量化和架构搜索，而无需重新训练。作者提出了一种新的量化策略——批量量化，以稳定比例因子的估计，从而使混合精度超网络能够稳定训练。他们用新提出的两阶段训练策略替代了[[215](#bib.bib215)]中的渐进收缩训练策略，减少了超过一半的训练周期。
- en: 'TABLE VII: Summary of Joint Automated Design Strategies. The “hardware” column
    indicates on which the achieved models are evaluated. The model name (e.g., MobileNetV1)
    in the “Accuracy” column indicates the base model used for search; if the original
    model is not specified, it is searched during training. The latency is reported
    per input; otherwise is specified in the relevant table cells (e.g., FPS). $\downarrow
    n\%$ indicates the compressed model decreases $n$ per cent compared to its original
    model. $\sim$ indicates the exact data is not reported in the paper and thus estimated
    from the reported figures. ‘-’ indicates unavailable records.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：联合自动设计策略总结。“硬件”列指示了模型评估的设备。“准确率”列中的模型名称（例如，MobileNetV1）表示用于搜索的基础模型；如果未指定原始模型，则在训练过程中进行搜索。延迟按输入报告；否则在相关表格单元中指定（例如，FPS）。$\downarrow
    n\%$ 表示压缩模型比原始模型减少了$n$百分比。$\sim$ 表示论文中未报告确切数据，因此从报告的数字中估算。“-” 表示记录不可用。
- en: '| Method | Reference | Dataset | Hardware | Accuracy | FLOPs | Latency | #
    of Parameters | Memory |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参考文献 | 数据集 | 硬件 | 准确率 | FLOPs | 延迟 | 参数数量 | 内存 |'
- en: '| Joint Search and KD | [[194](#bib.bib194)] | CIFAR100 | - | (Acc): 79.74/78.18/75.85
    | - | - | 2.77M/1.47M/0.7M | - |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 联合搜索与KD | [[194](#bib.bib194)] | CIFAR100 | - | (Acc)：79.74/78.18/75.85 |
    - | - | 2.77M/1.47M/0.7M | - |'
- en: '| CINIC-10 |  | (Acc): 85.41/79.45/79.38 | 1.27M/0.36M/0.36M |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| CINIC-10 |  | (Acc)：85.41/79.45/79.38 | 1.27M/0.36M/0.36M |'
- en: '| [[90](#bib.bib90)] | Cityscapes |  | (mIoU): 71.5 | - | 163.9 FPS | - | -
    |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| [[90](#bib.bib90)] | Cityscapes |  | (mIoU)：71.5 | - | 163.9 FPS | - | -
    |'
- en: '| CamVid | Nvidia Geforce GTX1080Ti | (mIoU): 71.1 | 398.1 FPS |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| CamVid | Nvidia Geforce GTX1080Ti | (mIoU)：71.1 | 398.1 FPS |'
- en: '| BDD |  | (mIoU): 55.1 | 318.0 FPS |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| BDD |  | (mIoU)：55.1 | 318.0 FPS |'
- en: '| Joint Search and Pruning | [[196](#bib.bib196)] | ImageNet | - | MobileNetV1(Top1
    acc): 70.9/66.1/57.2 | 324M/149M/41M | - | - | - |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 联合搜索与剪枝 | [[196](#bib.bib196)] | ImageNet | - | MobileNetV1（Top1 acc）：70.9/66.1/57.2
    | 324M/149M/41M | - | - | - |'
- en: '|  |  | MobileNetV2(Top1 acc): 72.7/68.2/67.3/ | 291M/140M/124M/ |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileNetV2（Top1 acc）：72.7/68.2/67.3/ | 291M/140M/124M/ |'
- en: '|  |  | 65.0/63.8/58.3 | 105M/84M/43M |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 65.0/63.8/58.3 | 105M/84M/43M |'
- en: '|  |  | ResNet50(Top1 error/Top5 error): 76.2/75.4/73.4 | 3000M/2000M/1000M
    |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ResNet50（Top1 error/Top5 error）：76.2/75.4/73.4 | 3000M/2000M/1000M
    |'
- en: '|  | NVIDIA Titan Xp | MobileNetV1(Top1 acc): 71.0/67.4/59.6 | - | 0.48ms/0.30ms/0.17ms
    |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | NVIDIA Titan Xp | MobileNetV1（Top1 acc）：71.0/67.4/59.6 | - | 0.48ms/0.30ms/0.17ms
    |'
- en: '|  |  | MobileNetV2(Top1 acc): 73.2/71.7/64.5 | 0.67ms/0.47ms/0.29ms |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileNetV2（Top1 acc）：73.2/71.7/64.5 | 0.67ms/0.47ms/0.29ms |'
- en: '| [[197](#bib.bib197)] | CIFRA-10 |  | VGG16/ResNet18(acc): 91.62/94.37 | $\downarrow$85.04%/$\downarrow$41.15%
    | $\downarrow$59.56%/$\downarrow$21.3% | $\downarrow$94.46%/$\downarrow$61.71%
    | - |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| [[197](#bib.bib197)] | CIFAR-10 |  | VGG16/ResNet18（acc）：91.62/94.37 | $\downarrow$85.04%/$\downarrow$41.15%
    | $\downarrow$59.56%/$\downarrow$21.3% | $\downarrow$94.46%/$\downarrow$61.71%
    | - |'
- en: '| CIFAR-100 | NVIDIA GeForce RTX 2080 Ti | VGG16/ResNet34(acc): 69.42/74.59
    | $\downarrow$69.03%/$\downarrow$56.91% | $\downarrow$49.4%/$\downarrow$27.91%
    | $\downarrow$81.14%/$\downarrow$69.94% |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 | NVIDIA GeForce RTX 2080 Ti | VGG16/ResNet34（acc）：69.42/74.59
    | $\downarrow$69.03%/$\downarrow$56.91% | $\downarrow$49.4%/$\downarrow$27.91%
    | $\downarrow$81.14%/$\downarrow$69.94% |'
- en: '| Tiny-ImageNet |  | ResNet18/ResNet34(acc): 56.87/58.64 | $\downarrow$50.16%/$\downarrow$61.89%
    | $\downarrow$22.15%/$\downarrow$27.12% | $\downarrow$24.67%/$\downarrow$31.58%
    |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| Tiny-ImageNet |  | ResNet18/ResNet34（acc）：56.87/58.64 | $\downarrow$50.16%/$\downarrow$61.89%
    | $\downarrow$22.15%/$\downarrow$27.12% | $\downarrow$24.67%/$\downarrow$31.58%
    |'
- en: '| [[198](#bib.bib198)] | CIFAR-10 | - | VGGNet16/GoogLeNet/ResNet56/ResNet110(Top1
    acc): | 82.81M/513.19M/ | - | 1.67M/2.46M/0.39M/0.56M | - |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| [[198](#bib.bib198)] | CIFAR-10 | - | VGGNet16/GoogLeNet/ResNet56/ResNet110（Top1
    acc）： | 82.81M/513.19M/ | - | 1.67M/2.46M/0.39M/0.56M | - |'
- en: '|  |  | 93.08/94.84/93.23/93.58 | 58.54M/89.87M |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 93.08/94.84/93.23/93.58 | 58.54M/89.87M |'
- en: '| ImageNet |  | ResNet18/ResNet34/ResNet50/ResNet101/ResNet152(Top1 acc): |
    968.13M/2170.77M/1890.60M/ | 9.50M/10.12M/11.75M/ |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  | ResNet18/ResNet34/ResNet50/ResNet101/ResNet152（Top1 acc）： |
    968.13M/2170.77M/1890.60M/ | 9.50M/10.12M/11.75M/ |'
- en: '|  |  | 67.80/70.98/73.86/75.82/77.12 | 3164.91M/4309.52M | 17.72M/24.07M |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 67.80/70.98/73.86/75.82/77.12 | 3164.91M/4309.52M | 17.72M/24.07M |'
- en: '| [[199](#bib.bib199)] | CIFAR-10 | - | ResNet56/ResNet110(acc): 93.39/93.65
    | $\downarrow$54.43%/$\downarrow$78.32% | - | $\downarrow$58.82%/$\downarrow$80.92%
    | - |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| [[199](#bib.bib199)] | CIFAR-10 | - | ResNet56/ResNet110(准确率): 93.39/93.65
    | $\downarrow$54.43%/$\downarrow$78.32% | - | $\downarrow$58.82%/$\downarrow$80.92%
    | - |'
- en: '| CIFAR-100 |  | ResNet56(acc): 71.15 | $\downarrow$52.21 | - |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 |  | ResNet56(准确率): 71.15 | $\downarrow$52.21 | - |'
- en: '| ImageNet |  | ResNet34(Top1/Top5 acc): $\downarrow$1.32/$\downarrow$0.87
    | $\downarrow$55.73% | $\downarrow$58.07% |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  | ResNet34(Top1/Top5 准确率): $\downarrow$1.32/$\downarrow$0.87
    | $\downarrow$55.73% | $\downarrow$58.07% |'
- en: '|  |  | ResNet50(Top1/Top5 acc): $\downarrow$0.89/$\downarrow$0.53 | $\downarrow$63.34%
    | $\downarrow$64.36% |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ResNet50(Top1/Top5 准确率): $\downarrow$0.89/$\downarrow$0.53 | $\downarrow$63.34%
    | $\downarrow$64.36% |'
- en: '| PASCAL VOC |  | SSD(mAP): 74.92/74.80/73.73 | $\downarrow$35.85%/$\downarrow$49.51%/$\downarrow$66.44%
    | $\downarrow$35.41%/$\downarrow$50.55%/$\downarrow$61.13% |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL VOC |  | SSD(mAP): 74.92/74.80/73.73 | $\downarrow$35.85%/$\downarrow$49.51%/$\downarrow$66.44%
    | $\downarrow$35.41%/$\downarrow$50.55%/$\downarrow$61.13% |'
- en: '| [[200](#bib.bib200)] | CIFAR-10 | - | VGG16/GoogLeNet(acc): 93.78/93.18 |
    81.19M/680M | - | 1.64M/2.76M | - |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| [[200](#bib.bib200)] | CIFAR-10 | - | VGG16/GoogLeNet(准确率): 93.78/93.18 |
    81.19M/680M | - | 1.64M/2.76M | - |'
- en: '| Carvana | NVIDIA Tesla P40 | Unet(Dice score): 0.9944/0.9896/0.9926 | 81110M/162230M/237260M
    | 190ms/210ms/220ms | 4.3M/5.88M/12.74M |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| Carvana | NVIDIA Tesla P40 | Unet(Dice 分数): 0.9944/0.9896/0.9926 | 81110M/162230M/237260M
    | 190ms/210ms/220ms | 4.3M/5.88M/12.74M |'
- en: '| [[201](#bib.bib201)] | CIFAR-10 | - | VGG16/ResNet56/ResNet110(acc): 93.57/93.31/93.56
    | $\downarrow$70%/$\downarrow$50%/$\downarrow$65% | - | - | - |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| [[201](#bib.bib201)] | CIFAR-10 | - | VGG16/ResNet56/ResNet110(准确率): 93.57/93.31/93.56
    | $\downarrow$70%/$\downarrow$50%/$\downarrow$65% | - | - | - |'
- en: '| ImageNet |  | ResNet50/MobileNetV2(Top1 acc): 75.46/71.1 | $\downarrow$51.7%/$\downarrow$30.2%
    |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet |  | ResNet50/MobileNetV2(Top1 准确率): 75.46/71.1 | $\downarrow$51.7%/$\downarrow$30.2%
    |'
- en: '| [[202](#bib.bib202)] | MNIST |  | (acc): 96.97/95.76 | - | 285.82ms/27.06ms
    | - | 1.32KB/0.71KB |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| [[202](#bib.bib202)] | MNIST |  | (准确率): 96.97/95.76 | - | 285.82ms/27.06ms
    | - | 1.32KB/0.71KB |'
- en: '| CIFAR10-binary | STM32F413 MCUs | (acc): 73.4/70.48 | 2529.84ms/298.57ms
    | 2.4KB/2.12KB |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR10-binary | STM32F413 MCUs | (准确率): 73.4/70.48 | 2529.84ms/298.57ms
    | 2.4KB/2.12KB |'
- en: '| CUReT-binary |  | (acc): 73.22 | 103.67ms | 2.06KB |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| CUReT-binary |  | (准确率): 73.22 | 103.67ms | 2.06KB |'
- en: '| Chars4K-binary |  | (acc): 74.87 | 77.89ms | 1.87KB |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| Chars4K-binary |  | (准确率): 74.87 | 77.89ms | 1.87KB |'
- en: '| Reference | Dataset | Hardware | Accuracy | Bitwdith | Latency | FLOPs |
    Model Size |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 数据集 | 硬件 | 准确率 | 位宽 | 延迟 | FLOPs | 模型大小 |'
- en: '| Joint Search and Quantization | [[203](#bib.bib203)] | ImageNet | Google
    Pixel 2 | ProxylessNAS(Top1 acc): 69.2 | 8 | 35ms | - | - |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 联合搜索与量化 | [[203](#bib.bib203)] | ImageNet | Google Pixel 2 | ProxylessNAS(Top1
    准确率): 69.2 | 8 | 35ms | - | - |'
- en: '| [[204](#bib.bib204)] | ImageNet | ARM Cortex-A72 | Top-1 acc: 75.1 | 8 |
    $\sim$100ms | - | - |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| [[204](#bib.bib204)] | ImageNet | ARM Cortex-A72 | Top-1 准确率: 75.1 | 8 |
    $\sim$100ms | - | - |'
- en: '|  | Coral USB TPU | Top-1 acc: 76.3 | 8 | $\sim$5ms |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '|  | Coral USB TPU | Top-1 准确率: 76.3 | 8 | $\sim$5ms |'
- en: '|  | Nvidia 2080Ti | Top1 acc: 76.3 | 32 | 22ms |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | Nvidia 2080Ti | Top1 准确率: 76.3 | 32 | 22ms |'
- en: '| [[205](#bib.bib205)] | Google Speech | - | acc: 96 | 7 | - | 19.6M | - |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| [[205](#bib.bib205)] | Google Speech | - | 准确率: 96 | 7 | - | 19.6M | - |'
- en: '| [[206](#bib.bib206)] | ImageNet | - | Top1/Top5 acc: 66.1/87.0 | 1 | - |
    121M | - |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| [[206](#bib.bib206)] | ImageNet | - | Top1/Top5 准确率: 66.1/87.0 | 1 | - |
    121M | - |'
- en: '| [[207](#bib.bib207)] | ImageNet | - | Top1/Top5 acc: 60.90/82.60 | 1 | -
    | 154M | - |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| [[207](#bib.bib207)] | ImageNet | - | Top1/Top5 准确率: 60.90/82.60 | 1 | -
    | 154M | - |'
- en: 'TABLE VII: Summary of Joint Automated Design Strategies (cont.).'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VII: 联合自动化设计策略的总结（续）。'
- en: '| Method | Reference | Dataset | Hardware | Accuracy | Bitwdith | Latency |
    FLOPs | Model Size |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参考 | 数据集 | 硬件 | 准确率 | 位宽 | 延迟 | FLOPs | 模型大小 |'
- en: '| Joint Search and Quantization | [[208](#bib.bib208)] | ImageNet | - | Top1/Top5
    acc: 63.51/83.91 | 1 | - | 90M | - |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 联合搜索与量化 | [[208](#bib.bib208)] | ImageNet | - | Top1/Top5 准确率: 63.51/83.91
    | 1 | - | 90M | - |'
- en: '| CIFAR10 | Top1 acc: 94.43 | 656M |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR10 | Top1 准确率: 94.43 | 656M |'
- en: '| [[209](#bib.bib209)] | ImageNet | - | ResNet18(Top1/Top5 acc): 69.65/89.08
    | 1 | - | 660M | - |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| [[209](#bib.bib209)] | ImageNet | - | ResNet18(Top1/Top5 准确率): 69.65/89.08
    | 1 | - | 660M | - |'
- en: '| CIFAR10 | VGG-small(Top1 acc): 93.06 | 59.3M |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR10 | VGG-small(Top1 准确率): 93.06 | 59.3M |'
- en: '| [[210](#bib.bib210)] | ImageNet | AMD Ryzen Threadripper 3960X | Top1 acc:
    77.8/77.0 | 32/8 | 297ms/100ms | 717M | - |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| [[210](#bib.bib210)] | ImageNet | AMD Ryzen Threadripper 3960X | Top1 准确率:
    77.8/77.0 | 32/8 | 297ms/100ms | 717M | - |'
- en: '| Top1 acc: 69.8/67.7 | 151ms/53ms | 141M |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| Top1 准确率: 69.8/67.7 | 151ms/53ms | 141M |'
- en: '| [[211](#bib.bib211)] | PASCAL VOC | - | ResNet18/ResNet34/ResNet50(mAP):
    73.2/75.8/76.9 | 1 | - | 18.49G/21.49G/21.95G | 16.61MB/24.68MB/29.61MB |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| [[211](#bib.bib211)] | PASCAL VOC | - | ResNet18/ResNet34/ResNet50(mAP):
    73.2/75.8/76.9 | 1 | - | 18.49G/21.49G/21.95G | 16.61MB/24.68MB/29.61MB |'
- en: '| [[212](#bib.bib212)] | ImageNet | - | error: 27.22/34.10 | - | - | - | 4.9MB/2.5MB
    |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| [[212](#bib.bib212)] | ImageNet | - | 错误率: 27.22/34.10 | - | - | - | 4.9MB/2.5MB
    |'
- en: '| CIFAR10 | error: 2.90/2.97 | 2.5MB/0.9MB |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR10 | 错误率: 2.90/2.97 | 2.5MB/0.9MB |'
- en: '| [[213](#bib.bib213)] | ImgeNet |  | Top1/Top5 error: 31.62/11.56 | mixed
    | 21.19ms | - | 1.44MB |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| [[213](#bib.bib213)] | ImageNet |  | Top1/Top5 错误率: 31.62/11.56 | 混合 | 21.19ms
    | - | 1.44MB |'
- en: '| NVIDIA Tesla P100 GPU | Top1/Top5 error: 28.23/9.94 | 24.71ms | 2.06MB |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| NVIDIA Tesla P100 GPU | Top1/Top5 错误率: 28.23/9.94 | 24.71ms | 2.06MB |'
- en: '| CIFAR100 |  | error: 22.16/21.27 | 2.53ms/2.98ms | 0.6MB/0.8MB |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR100 |  | 错误率: 22.16/21.27 | 2.53ms/2.98ms | 0.6MB/0.8MB |'
- en: '| [[105](#bib.bib105)] | ImageNet | - | ResNet18(Top1 acc): 66.4/69.4/7 | mixed
    | - | BitOps: 6.21G/13.49G/24.31G | - |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| [[105](#bib.bib105)] | ImageNet | - | ResNet18(Top1 准确率): 66.4/69.4/7 | 混合
    | - | BitOps: 6.21G/13.49G/24.31G | - |'
- en: '| ResNet34(Top1 acc): 71.5/73.9/74.6 | BitOps: 13.11G/28.78G/51.92G |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| ResNet34(Top1 准确率): 71.5/73.9/74.6 | BitOps: 13.11G/28.78G/51.92G |'
- en: '| [[214](#bib.bib214)] | ImageNet | BitFusion | Top1 acc: 75.1/74.1 | mixed
    | 12.17ms/8.40ms | BitOps: 16.5G/23.6G | - |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| [[214](#bib.bib214)] | ImageNet | BitFusion | Top1 准确率: 75.1/74.1 | 混合 |
    12.17ms/8.40ms | BitOps: 16.5G/23.6G | - |'
- en: '| [[215](#bib.bib215)] | ImageNet | - | Top1 acc: 61.7/71.3/74.1 | 2/3/4 |
    - | BitOPs: 1.21G/3.07G/9.28G | - |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| [[215](#bib.bib215)] | ImageNet | - | Top1 准确率: 61.7/71.3/74.1 | 2/3/4 |
    - | BitOPs: 1.21G/3.07G/9.28G | - |'
- en: '| [[183](#bib.bib183)] | ImageNet | - | Top1 acc: $\sim$71.5 | mixed | - |
    $\sim$200M | - |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| [[183](#bib.bib183)] | ImageNet | - | Top1 准确率: $\sim$71.5 | 混合 | - | $\sim$200M
    | - |'
- en: '| [[46](#bib.bib46)] | ImageNet | STM32F746 | Top1 acc: 49.9/40.5 | 8 | 5FPS/10FPS
    |  |  |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| [[46](#bib.bib46)] | ImageNet | STM32F746 | Top1 准确率: 49.9/40.5 | 8 | 5FPS/10FPS
    |  |  |'
- en: '| STM32F412 | Top1 acc: 62.0 | 4 | - | - | - |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| STM32F412 | Top1 准确率: 62.0 | 4 | - | - | - |'
- en: '| STM32F746 | Top1 acc: 63.5 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| STM32F746 | Top1 准确率: 63.5 |'
- en: '| STM32F765 | Top1 acc: 65.9 |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| STM32F765 | Top1 准确率: 65.9 |'
- en: '| STM32H743 | Top1 acc: 70.7 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| STM32H743 | Top1 准确率: 70.7 |'
- en: '| Pascal VOC | STM32H743 | mAP: 51.4 | 8 | 168M | peak SRAM: 466KB |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| Pascal VOC | STM32H743 | mAP: 51.4 | 8 | 168M | 峰值 SRAM: 466KB |'
- en: VI Future Directions
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 未来方向
- en: The design automation for efficient deep learning models is an exciting area
    yet still in its infant research phase. Existing work has covered almost its every
    research aspect and laid firm foundations for its future evolution. In this section,
    we discuss several future directions that are worthy to delve into.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 高效深度学习模型的设计自动化是一个令人兴奋但仍处于初期研究阶段的领域。现有的工作几乎涵盖了它的每一个研究方面，并为其未来的发展奠定了坚实的基础。在这一部分，我们讨论了几个值得深入研究的未来方向。
- en: VI-A Beyond Computer Vision
  id: totrans-561
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 计算机视觉之外
- en: 'Most current works validate their performance on computer vision (CV) tasks,
    e.g., image classification and object detection. This is ascribable to two reasons:
    1) the CV area is the most developed area regarding deep learning research that
    abundant resources are available and experimental protocol is well standardized;
    thus it requires only bare efforts to prepare comparison benchmarks; 2) the CV
    models are mainly based on the CNN family and much manual engineering has been
    devoted to devising efficient CNN models so it is relatively easy to establish
    an efficient search space or base models for compression. Nevertheless, CV is
    not the only field that expects efficient deep learning models; applications like
    IoT are more resource-constrained but seldom investigated regarding the design
    automation for efficient deep learning. Since these domains have dissimilar characteristics
    to CV, specific research is desired to go beyond the CV. Though some works have
    already been presented, like [[146](#bib.bib146), [149](#bib.bib149), [189](#bib.bib189)]
    on automatically compressing language models, these are very limited and more
    room for different applications and the deeper investigation remains to be explored.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 当前大多数工作在计算机视觉（CV）任务上验证其性能，例如图像分类和目标检测。这可以归因于两个原因：1) CV 领域是深度学习研究中最发达的领域，拥有丰富的资源和标准化的实验协议；因此，准备对比基准只需要付出少量努力；2)
    CV 模型主要基于 CNN 家族，很多手工工程致力于设计高效的 CNN 模型，因此建立高效的搜索空间或基础模型相对容易。然而，CV 并不是唯一需要高效深度学习模型的领域；例如物联网（IoT）的应用虽然资源受限，但在高效深度学习设计自动化方面的研究却很少。由于这些领域的特征与
    CV 不同，因此需要特定的研究来超越 CV。尽管已经有一些工作出现，如[[146](#bib.bib146), [149](#bib.bib149), [189](#bib.bib189)]
    关于自动压缩语言模型，这些工作非常有限，不同应用和更深层次的研究还有待探索。
- en: VI-B Accelerating Search
  id: totrans-563
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 加速搜索
- en: While searched models are driven to be efficient, the search process is often
    time- and computation-intensive because vast numbers of candidate models need
    to be evaluated before choosing the best one. This is problematic as the search
    process is task- and/or hardware-specific and thus a new search is required when
    a new task or hardware arises. Accelerating the search process can not only facilitate
    the task/hardware adaptation but also save the energy consumed during the search.
    The weight-sharing paradigm [[124](#bib.bib124), [54](#bib.bib54)] can alleviate
    this problem but this is still too slow, especially for the hardware-aware settings.
    Using proxies is another strategy to accelerate the search process, which approximates
    different models’ performance without fully training them [[117](#bib.bib117)].
    Most recently, zero-cost proxies for NAS have attained great interest and shown
    outstanding performance on some NAS benchmarks [[216](#bib.bib216)]. This strategy
    aims to rank network architectures without any training or even without seeing
    data [[217](#bib.bib217), [216](#bib.bib216)]. Though remarkable efforts have
    been made in accelerating conventional NAS, how these approaches can be efficiently
    and effectively applied to the hardware-aware NAS and automated compression is
    still an open question. For example, which proxies can be used for automated quantization?
    Or can we use the same proxies for different quantization bitwidths?
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管搜索模型的驱动目标是提高效率，但搜索过程通常是时间和计算密集型的，因为需要评估大量候选模型才能选择最佳模型。这是一个问题，因为搜索过程通常是任务和/或硬件特定的，因此当出现新任务或新硬件时，需要重新进行搜索。加速搜索过程不仅可以促进任务/硬件适配，还可以节省搜索过程中消耗的能源。**权重共享范式**[[124](#bib.bib124),
    [54](#bib.bib54)] 可以缓解这一问题，但对于硬件感知设置来说，这仍然太慢。使用代理是加速搜索过程的另一种策略，它通过不完全训练不同模型来近似其性能[[117](#bib.bib117)]。最近，**零成本代理**在NAS中引起了极大的兴趣，并在一些NAS基准测试中表现出色[[216](#bib.bib216)]。这一策略旨在对网络架构进行排名，而无需任何训练甚至不需要查看数据[[217](#bib.bib217),
    [216](#bib.bib216)]。尽管在加速传统NAS方面已经做出了显著努力，但如何有效和高效地将这些方法应用于硬件感知NAS和自动化压缩仍然是一个未解之谜。例如，哪些代理可以用于自动量化？或者我们是否可以对不同的量化位宽使用相同的代理？
- en: Another direction for accelerating the search is to reduce the search space.
    For example, EfficientMet [[68](#bib.bib68)] fixes a baseline model and only searches
    the constant scale ratios of width/depth/resolution. Only a small grid search
    can produce extraordinary performance. An effective and reduced search space is
    based on extensive investigation and human knowledge and thus demands considerable
    effort in this direction.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 加速搜索的另一个方向是减少搜索空间。例如，EfficientMet [[68](#bib.bib68)] 固定了基线模型，只搜索宽度/深度/分辨率的常量比例。仅通过小范围的网格搜索即可产生非凡的性能。有效且减少的搜索空间基于广泛的调查和人类知识，因此在这一方向上需要付出相当大的努力。
- en: VI-C Hardware Simulation
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 硬件模拟
- en: The feedback (e.g., latency and memory usage) from target devices is a necessity
    for exploring efficient deep learning models automatically. However, running an
    enormous amount of models on target devices is labour- and time-intensive, especially
    when the devices are general-purpose computing systems (e.g., MCU) instead of
    specific NN accelerators (e.g., TPUs). Furthermore, the frequent communication
    between the target devices and the devices, on which the design program runs,
    takes even more time. Therefore, it is desirable to have an accurate hardware
    simulator that simulates the behaviour of DNN hardware implementations. This simulator
    would accelerate the automated design process by running all models and obtaining
    all feedback on a powerful server. In addition, with a hardware simulator, beginners
    do not need to have cross-disciplinary knowledge in the hardware setting and compilation
    to collect hardware-cost data; the design process becomes easy to get started
    with. Although some works have reported hardware simulators like [[218](#bib.bib218),
    [219](#bib.bib219)], the precision and hardware information that can be provided
    are far from satisfactory. It is also attractive to have a general software platform
    that offers the simulation of deploying DNNs on diverse and configurable devices.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 从目标设备获得的反馈（例如，延迟和内存使用）对于自动探索高效的深度学习模型是必要的。然而，在目标设备上运行大量模型劳动强度大且耗时，特别是当设备是通用计算系统（例如，MCU）而非特定的神经网络加速器（例如，TPU）时。此外，目标设备与运行设计程序的设备之间的频繁通信还需要更多时间。因此，拥有一个准确的硬件模拟器来模拟DNN硬件实现的行为是理想的。这个模拟器可以通过在强大的服务器上运行所有模型并获得所有反馈来加速自动设计过程。此外，使用硬件模拟器，初学者无需在硬件设置和编译中具备跨学科知识即可收集硬件成本数据；设计过程变得更容易入手。尽管一些研究报道了硬件模拟器，如[[218](#bib.bib218),
    [219](#bib.bib219)]，但其提供的精确度和硬件信息仍远未令人满意。拥有一个通用的软件平台，提供在多样化和可配置设备上部署DNN的模拟，也是很有吸引力的。
- en: VI-D Benchmarking
  id: totrans-568
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 基准测试
- en: Despite tremendous results that have been reported to achieve state-of-the-art
    performance, most of these works are not based on the same protocol thus resulting
    in skewed comparison. First, some literature uses direct evaluation metrics while
    others use indirect ones. Superiority on indirect metrics does not necessarily
    mean more efficiency on real devices. Second, the comparison results can vary
    on different hardware or different implementation details on the same hardware.
    Finally, the dataset/task settings also influence the performance significantly.
    For example, even using the same dataset, different input resolutions would lead
    to different hardware consumption. It is thus imperative to benchmark these evaluation
    settings to resolve unfair comparisons and make this domain more reproducible.
    In addition, a unified benchmark dataset can also facilitate the comparison of
    design automation algorithms. Li et al. [[220](#bib.bib220)] recently develop
    a comprehensive benchmark dataset of hardware-aware NAS on three categories of
    hardware (e.g., commercial edge devices, FPGA, and ASIC). However, this benchmark
    dataset primarily aims to facilitate the development of search strategies, and
    therefore is relatively small and based on only two spaces. There so far lacks
    well-recognized benchmark settings and a thorough dataset in this domain.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已有大量研究报道取得了最先进的性能，大多数这些工作并不基于相同的协议，从而导致了比较偏差。首先，一些文献使用直接评估指标，而其他文献使用间接指标。间接指标上的优越性并不一定意味着在实际设备上的效率更高。其次，相同硬件上的不同实现细节或不同硬件上的比较结果可能有所不同。最后，数据集/任务设置也会显著影响性能。例如，即使使用相同的数据集，不同的输入分辨率也会导致不同的硬件消耗。因此，对这些评估设置进行基准测试以解决不公平的比较，并使该领域更具可重复性是迫切的。此外，统一的基准数据集也可以促进设计自动化算法的比较。Li等人[[220](#bib.bib220)]最近开发了一个全面的硬件感知NAS基准数据集，涵盖三类硬件（例如，商业边缘设备、FPGA和ASIC）。然而，这个基准数据集主要旨在促进搜索策略的发展，因此相对较小且仅基于两个空间。目前，该领域缺乏公认的基准设置和全面的数据集。
- en: Apart from the above less noticed and underdeveloped directions, other directions
    like developing novel efficient operators and competent design algorithms have
    been widely researched, but there is still large room to improve.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述不太被注意和发展不足的方向，其他方向如开发新型高效操作符和高效设计算法已被广泛研究，但仍有很大的改进空间。
- en: VII Conclusion
  id: totrans-571
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: 'This survey provides a thorough review of the design automation techniques
    for fast, lightweight and effective deep learning models. We analyze and summarize
    current studies into three categories: 1) by searching, 2) by compressing, and
    3) by jointly. In addition, we conclude the evaluation metrics specific to efficient
    deep learning models. At the end of this work, we afford discussion of existing
    issues and future directions for both novices and experienced researchers.'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查提供了对快速、轻量级和高效深度学习模型的设计自动化技术的详细回顾。我们将当前研究分析和总结为三类: 1) 通过搜索，2) 通过压缩，3) 通过联合。此外，我们总结了针对高效深度学习模型的评估指标。最后，我们讨论了现有问题和未来方向，以便于新手和有经验的研究人员。'
- en: References
  id: totrans-573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Zhang, L. Yao, A. Sun, and Y. Tay, “Deep learning based recommender
    system: A survey and new perspectives,” *ACM Computing Surveys*, vol. 52, no. 1,
    pp. 5:1–5:38, 2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Zhang, L. Yao, A. Sun 和 Y. Tay, “基于深度学习的推荐系统: 调查与新视角,” *ACM Computing
    Surveys*, 第52卷，第1期, 页码 5:1–5:38, 2019。'
- en: '[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of
    deep bidirectional transformers for language understanding,” in *NAACL-HLT*, 2019,
    pp. 4171–4186.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Devlin, M.-W. Chang, K. Lee 和 K. Toutanova, “BERT: 深度双向变换器的预训练用于语言理解,”
    收录于 *NAACL-HLT*, 2019, 页码 4171–4186。'
- en: '[3] D. Zhang, K. Chen, D. Jian, L. Yao, S. Wang, and P. Li, “Learning attentional
    temporal cues of brainwaves with spatial embedding for motion intent detection,”
    in *ICDM*, 2019, pp. 1450–1455.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D. Zhang, K. Chen, D. Jian, L. Yao, S. Wang 和 P. Li, “利用空间嵌入学习脑电波的注意力时间线索以进行运动意图检测,”
    收录于 *ICDM*, 2019, 页码 1450–1455。'
- en: '[4] D. Chen and H. Zhao, “Data security and privacy protection issues in cloud
    computing,” in *ICCSEE*, 2012, pp. 647–651.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. Chen 和 H. Zhao, “云计算中的数据安全与隐私保护问题,” 收录于 *ICCSEE*, 2012, 页码 647–651。'
- en: '[5] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations
    for deep learning in NLP,” in *ACL*, 2019, pp. 3645–3650.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] E. Strubell, A. Ganesh 和 A. McCallum, “深度学习在 NLP 中的能源和政策考虑,” 收录于 *ACL*,
    2019, 页码 3645–3650。'
- en: '[6] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” *Communications
    of the ACM*, vol. 63, no. 12, pp. 54–63, 2020.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] R. Schwartz, J. Dodge, N. A. Smith 和 O. Etzioni, “绿色 AI,” *Communications
    of the ACM*, 第63卷，第12期, 页码 54–63, 2020。'
- en: '[7] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas, “Predicting
    parameters in deep learning,” in *NIPS*, 2013, pp. 2148–2156.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Denil, B. Shakibi, L. Dinh, M. Ranzato 和 N. de Freitas, “预测深度学习中的参数,”
    收录于 *NIPS*, 2013, 页码 2148–2156。'
- en: '[8] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in *NIPS*,
    1989, pp. 598–605.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. LeCun, J. S. Denker 和 S. A. Solla, “最佳脑损伤,” 收录于 *NIPS*, 1989, 页码 598–605。'
- en: '[9] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” *arXiv preprint arXiv:1704.04861*, pp. 1–9, 2017.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto 和 H. Adam, “Mobilenets: 高效的卷积神经网络用于移动视觉应用,” *arXiv 预印本 arXiv:1704.04861*,
    页码 1–9, 2017。'
- en: '[10] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *CVPR*, 2018, pp. 4510–4520.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov 和 L. Chen, “Mobilenetv2:
    反向残差和线性瓶颈,” 收录于 *CVPR*, 2018, 页码 4510–4520。'
- en: '[11] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient
    convolutional neural network for mobile devices,” in *CVPR*, 2018, pp. 6848–6856.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] X. Zhang, X. Zhou, M. Lin 和 J. Sun, “Shufflenet: 一种极其高效的移动设备卷积神经网络,” 收录于
    *CVPR*, 2018, 页码 6848–6856。'
- en: '[12] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical guidelines
    for efficient cnn architecture design,” in *ECCV*, 2018, pp. 122–138.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] N. Ma, X. Zhang, H.-T. Zheng 和 J. Sun, “Shufflenet v2: 高效 CNN 架构设计的实用指南,”
    收录于 *ECCV*, 2018, 页码 122–138。'
- en: '[13] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating very deep
    neural networks,” in *ICCV*, 2017, pp. 1398–1406.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. He, X. Zhang 和 J. Sun, “加速极深神经网络的通道剪枝,” 收录于 *ICCV*, 2017, 页码 1398–1406。'
- en: '[14] J. Luo, J. Wu, and W. Lin, “Thinet: A filter level pruning method for
    deep neural network compression,” in *ICCV*, 2017, pp. 5068–5076.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Luo, J. Wu 和 W. Lin, “Thinet: 一种用于深度神经网络压缩的滤波器级剪枝方法,” 收录于 *ICCV*, 2017,
    页码 5068–5076。'
- en: '[15] M. Courbariaux, Y. Bengio, and J.-P. David, “Training deep neural networks
    with low precision multiplications,” in *ICLR Workshop*, 2015, pp. 1–10.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Courbariaux, Y. Bengio 和 J.-P. David, “使用低精度乘法训练深度神经网络,” 收录于 *ICLR
    Workshop*, 2015, 页码 1–10。'
- en: '[16] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent
    in nervous activity,” *The Bulletin of Mathematical Biophysics*, vol. 5, no. 4,
    pp. 115–133, 1943.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. S. McCulloch 和 W. Pitts, “神经活动中固有思想的逻辑演算,” *The Bulletin of Mathematical
    Biophysics*, 第5卷，第4期, 页码 115–133, 1943。'
- en: '[17] T. M. Bartol, C. Bromer, J. Kinney, M. A. Chirillo, J. N. Bourne, K. M.
    Harris, and T. J. Sejnowski, “Hippocampal spine head sizes are highly precise,”
    *bioRxiv*, p. 016329, 2015.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] T. M. Bartol, C. Bromer, J. Kinney, M. A. Chirillo, J. N. Bourne, K. M.
    Harris, 和 T. J. Sejnowski，“海马体脊柱头的尺寸非常精确”，*bioRxiv*，页码 016329，2015年。'
- en: '[18] D. J. Linden, *Think Tank: Forty Neuroscientists Explore the Biological
    Roots of Human Experience*.   Yale University Press, 2018.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] D. J. Linden，*思想库：四十位神经科学家探讨人类体验的生物学根源*。耶鲁大学出版社，2018年。'
- en: '[19] D. Lin, S. Talathi, and S. Annapureddy, “Fixed point quantization of deep
    convolutional networks,” in *ICML*, 2016, pp. 2849–2858.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] D. Lin, S. Talathi, 和 S. Annapureddy，“深度卷积网络的定点量化”，发表于 *ICML*，2016年，页码
    2849–2858。'
- en: '[20] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning
    with limited numerical precision,” in *ICML*, 2015, pp. 1737–1746.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Gupta, A. Agrawal, K. Gopalakrishnan, 和 P. Narayanan，“有限数值精度下的深度学习”，发表于
    *ICML*，2015年，页码 1737–1746。'
- en: '[21] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” in *NIPS Workshop*, 2014, pp. 1–9.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] G. Hinton, O. Vinyals, 和 J. Dean，“提炼神经网络中的知识”，发表于 *NIPS Workshop*，2014年，页码
    1–9。'
- en: '[22] J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, and Z. Xu, “Learning compact
    recurrent neural networks with block-term tensor decomposition,” in *CVPR*, 2018,
    pp. 9378–9387.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, 和 Z. Xu，“利用块项张量分解学习紧凑的递归神经网络”，发表于
    *CVPR*，2018年，页码 9378–9387。'
- en: '[23] H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural architecture
    search on target task and hardware,” in *ICLR*, 2019, pp. 1–11.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. Cai, L. Zhu, 和 S. Han，“Proxylessnas：在目标任务和硬件上的直接神经架构搜索”，发表于 *ICLR*，2019年，页码
    1–11。'
- en: '[24] X. Xiao, Z. Wang, and S. Rajasekaran, “Autoprune: Automatic network pruning
    by regularizing auxiliary parameters,” in *NeurIPS*, 2019, pp. 13 681–13 691.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] X. Xiao, Z. Wang, 和 S. Rajasekaran，“Autoprune：通过正则化辅助参数自动剪枝网络”，发表于 *NeurIPS*，2019年，页码
    13 681–13 691。'
- en: '[25] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “HAQ: hardware-aware automated
    quantization with mixed precision,” in *CVPR*, 2019, pp. 8612–8620.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] K. Wang, Z. Liu, Y. Lin, J. Lin, 和 S. Han，“HAQ：基于硬件的自动量化与混合精度”，发表于 *CVPR*，2019年，页码
    8612–8620。'
- en: '[26] Y. He, J. Lin, Z. Liu, H. Wang, L. Li, and S. Han, “AMC: automl for model
    compression and acceleration on mobile devices,” in *ECCV*, 2018, pp. 815–832.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. He, J. Lin, Z. Liu, H. Wang, L. Li, 和 S. Han，“AMC：用于移动设备的模型压缩和加速的自动机器学习”，发表于
    *ECCV*，2018年，页码 815–832。'
- en: '[27] D. Langerman, A. Johnson, K. Buettner, and A. D. George, “Beyond floating-point
    ops: CNN performance prediction with critical datapath length,” in *HPEC*, 2020,
    pp. 1–9.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] D. Langerman, A. Johnson, K. Buettner, 和 A. D. George，“超越浮点操作：基于关键数据路径长度的CNN性能预测”，发表于
    *HPEC*，2020年，页码 1–9。'
- en: '[28] S. Yao, Y. Zhao, H. Shao, S. Liu, D. Liu, L. Su, and T. Abdelzaher, “Fastdeepiot:
    Towards understanding and optimizing neural network execution time on mobile and
    embedded devices,” in *SenSys*, 2018, pp. 278–291.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Yao, Y. Zhao, H. Shao, S. Liu, D. Liu, L. Su, 和 T. Abdelzaher，“Fastdeepiot：理解和优化移动和嵌入式设备上的神经网络执行时间”，发表于
    *SenSys*，2018年，页码 278–291。'
- en: '[29] L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang, and Q. Tian, “Condensenet
    v2: Sparse feature reactivation for deep networks,” in *CVPR*, 2021, pp. 3569–3578.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] L. Yang, H. Jiang, R. Cai, Y. Wang, S. Song, G. Huang, 和 Q. Tian，“Condensenet
    v2：用于深度网络的稀疏特征再激活”，发表于 *CVPR*，2021年，页码 3569–3578。'
- en: '[30] V. Camus, C. Enz, and M. Verhelst, “Survey of precision-scalable multiply-accumulate
    units for neural-network processing,” in *AICAS*, 2019, pp. 57–61.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] V. Camus, C. Enz, 和 M. Verhelst，“用于神经网络处理的精度可扩展乘加单元调查”，发表于 *AICAS*，2019年，页码
    57–61。'
- en: '[31] G. Chu, O. Arikan, G. Bender, W. Wang, A. Brighton, P.-J. Kindermans,
    H. Liu, B. Akin, S. Gupta, and A. Howard, “Discovering multi-hardware mobile models
    via architecture search,” in *CVPR Workshops*, 2021, pp. 3022–3031.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] G. Chu, O. Arikan, G. Bender, W. Wang, A. Brighton, P.-J. Kindermans,
    H. Liu, B. Akin, S. Gupta, 和 A. Howard，“通过架构搜索发现多硬件移动模型”，发表于 *CVPR Workshops*，2021年，页码
    3022–3031。'
- en: '[32] S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Benchmark analysis
    of representative deep neural network architectures,” *IEEE Access*, vol. 6, pp.
    64 270–64 277, 2018.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Bianco, R. Cadene, L. Celona, 和 P. Napoletano，“代表性深度神经网络架构的基准分析”，*IEEE
    Access*，第6卷，页码 64 270–64 277，2018年。'
- en: '[33] C. Zhengbo, T. Lei, and C. Zuoning, “Research and design of activation
    function hardware implementation methods,” in *Journal of Physics: Conference
    Series*, vol. 1684, no. 1, 2020, p. 012111.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C. Zhengbo, T. Lei, 和 C. Zuoning，“激活函数硬件实现方法的研究与设计”，发表于 *Journal of Physics:
    Conference Series*，第1684卷，第1期，2020年，页码 012111。'
- en: '[34] N. Dryden, N. Maruyama, T. Benson, T. Moon, M. Snir, and B. Van Essen,
    “Improving strong-scaling of cnn training by exploiting finer-grained parallelism,”
    in *IPDPS*, 2019, pp. 210–220.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] N. Dryden, N. Maruyama, T. Benson, T. Moon, M. Snir 和 B. Van Essen, “通过利用更细粒度的并行性改善
    CNN 训练的强扩展性，” 在 *IPDPS*，2019 年，第 210–220 页。'
- en: '[35] W. Niu, M. Sun, Z. Li, J. Chen, J. Guan, X. Shen, Y. Wang, S. Liu, X. Lin,
    and B. Ren, “Rt3d: Achieving real-time execution of 3d convolutional neural networks
    on mobile devices,” in *AAAI*, 2021, pp. 9179–9187.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] W. Niu, M. Sun, Z. Li, J. Chen, J. Guan, X. Shen, Y. Wang, S. Liu, X.
    Lin 和 B. Ren, “RT3D: 实现移动设备上 3D 卷积神经网络的实时执行，” 在 *AAAI*，2021 年，第 9179–9187 页。'
- en: '[36] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
    and E. Shelhamer, “cudnn: Efficient primitives for deep learning,” *arXiv preprint
    arXiv:1410.0759*, pp. 1–9, 2014.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro
    和 E. Shelhamer, “cudnn: 深度学习的高效原语，” *arXiv 预印本 arXiv:1410.0759*，第 1–9 页，2014 年。'
- en: '[37] E. Liberis and N. D. Lane, “Neural networks on microcontrollers: saving
    memory at inference via operator reordering,” *arXiv preprint arXiv:1910.05110*,
    pp. 1–8, 2019.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] E. Liberis 和 N. D. Lane, “微控制器上的神经网络: 通过操作符重排序节省推理内存，” *arXiv 预印本 arXiv:1910.05110*，第
    1–8 页，2019 年。'
- en: '[38] N. Suda and D. Loh, “Machine learning on arm cortex-m microcontrollers,”
    *Arm Ltd.: Cambridge, UK*, 2019.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] N. Suda 和 D. Loh, “在 ARM Cortex-M 微控制器上的机器学习，” *Arm Ltd.: 剑桥，英国*，2019
    年。'
- en: '[39] K. Siu, D. M. Stuart, M. Mahmoud, and A. Moshovos, “Memory requirements
    for convolutional neural network hardware accelerators,” in *IISWC*, 2018, pp.
    111–121.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. Siu, D. M. Stuart, M. Mahmoud 和 A. Moshovos, “卷积神经网络硬件加速器的内存需求，” 在
    *IISWC*，2018 年，第 111–121 页。'
- en: '[40] J. Xu, K. Ota, and M. Dong, “Saving energy on the edge: In-memory caching
    for multi-tier heterogeneous networks,” *IEEE Communications Magazine*, vol. 56,
    no. 5, pp. 102–107, 2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Xu, K. Ota 和 M. Dong, “边缘节能: 针对多层异构网络的内存缓存，” *IEEE 通信杂志*，第 56 卷，第 5
    期，第 102–107 页，2018 年。'
- en: '[41] H. Unlu, “Efficient neural network deployment for microcontroller,” *arXiv
    preprint arXiv:2007.01348*, pp. 1–6, 2020.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Unlu, “针对微控制器的高效神经网络部署，” *arXiv 预印本 arXiv:2007.01348*，第 1–6 页，2020
    年。'
- en: '[42] Y. Ioannou, D. Robertson, R. Cipolla, and A. Criminisi, “Deep roots: Improving
    cnn efficiency with hierarchical filter groups,” in *CVPR*, 2017, pp. 1231–1240.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Ioannou, D. Robertson, R. Cipolla 和 A. Criminisi, “深层根基: 通过层次化滤波器组提高
    CNN 效率，” 在 *CVPR*，2017 年，第 1231–1240 页。'
- en: '[43] L. Yang, B. Lu, and S. Ren, “A note on latency variability of deep neural
    networks for mobile inference,” *arXiv preprint arXiv:2003.00138*, pp. 1–5, 2020.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] L. Yang, B. Lu 和 S. Ren, “关于移动推断中深度神经网络延迟变异性的笔记，” *arXiv 预印本 arXiv:2003.00138*，第
    1–5 页，2020 年。'
- en: '[44] M. Syed and A. A. Srinivasan, “Generalized latency performance estimation
    for once-for-all neural architecture search,” *arXiv preprint arXiv:2101.00732*,
    pp. 1–12, 2021.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. Syed 和 A. A. Srinivasan, “一次性神经网络架构搜索的广义延迟性能估计，” *arXiv 预印本 arXiv:2101.00732*，第
    1–12 页，2021 年。'
- en: '[45] Q. Wang and X. Chu, “Gpgpu performance estimation with core and memory
    frequency scaling,” *IEEE Transactions on Parallel and Distributed Systems*, vol. 31,
    no. 12, pp. 2865–2881, 2020.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Q. Wang 和 X. Chu, “GPGPU 性能估计与核心和内存频率缩放，” *IEEE 并行与分布式系统汇刊*，第 31 卷，第 12
    期，第 2865–2881 页，2020 年。'
- en: '[46] J. Lin, W. Chen, Y. Lin, J. Cohn, C. Gan, and S. Han, “Mcunet: Tiny deep
    learning on iot devices,” in *NeurIPS*, 2020, p. 11711–11722.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Lin, W. Chen, Y. Lin, J. Cohn, C. Gan 和 S. Han, “MCUNet: 在物联网设备上的小型深度学习，”
    在 *NeurIPS*，2020 年，第 11711–11722 页。'
- en: '[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] K. He, X. Zhang, S. Ren 和 J. Sun, “用于图像识别的深度残差学习，” 在 *CVPR*，2016 年，第 770–778
    页。'
- en: '[48] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V.
    Le, “Mnasnet: Platform-aware neural architecture search for mobile,” in *CVPR*,
    2019, pp. 2820–2828.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard 和 Q. V.
    Le, “MnasNet: 面向移动平台的神经架构搜索，” 在 *CVPR*，2019 年，第 2820–2828 页。'
- en: '[49] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable
    architectures for scalable image recognition,” in *CVPR*, 2018, pp. 8697–8710.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] B. Zoph, V. Vasudevan, J. Shlens 和 Q. V. Le, “学习可转移的架构以进行可扩展图像识别，” 在 *CVPR*，2018
    年，第 8697–8710 页。'
- en: '[50] X. He, K. Zhao, and X. Chu, “Automl: A survey of the state-of-the-art,”
    *Knowledge-Based Systems*, vol. 212, p. 106622, 2021.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. He, K. Zhao 和 X. Chu, “AutoML: 最前沿的综述，” *知识基础系统*，第 212 卷，第 106622 页，2021
    年。'
- en: '[51] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen, and K. C. Tan, “A survey
    on evolutionary neural architecture search,” *IEEE Transactions on Neural Networks
    and Learning Systems*, 2021.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen 和 K. C. Tan, “关于进化神经架构搜索的综述，”
    *IEEE 神经网络与学习系统汇刊*，2021 年。'
- en: '[52] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement learning,”
    in *ICLR*, 2017, pp. 1–11.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] B. Zoph 和 Q. V. Le, “基于强化学习的神经网络架构搜索，” 见 *ICLR*，2017年，第1–11页。'
- en: '[53] X. Dong and Y. Yang, “Searching for a robust neural architecture in four
    GPU hours,” in *CVPR*, 2019, pp. 1761–1770.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] X. Dong 和 Y. Yang, “在四个GPU小时内搜索鲁棒的神经网络架构，” 见 *CVPR*，2019年，第1761–1770页。'
- en: '[54] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture
    search,” in *ICLR*, 2019, pp. 1–11.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Liu, K. Simonyan, 和 Y. Yang, “Darts：可微分架构搜索，” 见 *ICLR*，2019年，第1–11页。'
- en: '[55] C. Xu, B. Wu, Z. Wang, W. Zhan, P. Vajda, K. Keutzer, and M. Tomizuka,
    “Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation,”
    in *ECCV*, 2020, pp. 1–19.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] C. Xu, B. Wu, Z. Wang, W. Zhan, P. Vajda, K. Keutzer, 和 M. Tomizuka, “Squeezesegv3：用于高效点云分割的空间自适应卷积，”
    见 *ECCV*，2020年，第1–19页。'
- en: '[56] Y. Qiu, Y. Liu, S. Li, and J. Xu, “Miniseg: An extremely minimum network
    for efficient covid-19 segmentation,” in *AAAI*, 2021, pp. 4846–4854.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Qiu, Y. Liu, S. Li, 和 J. Xu, “Miniseg：用于高效COVID-19分割的极简网络，” 见 *AAAI*，2021年，第4846–4854页。'
- en: '[57] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A
    survey,” *The Journal of Machine Learning Research*, vol. 20, pp. 55:1–55:21,
    2019.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] T. Elsken, J. H. Metzen, 和 F. Hutter, “神经网络架构搜索：综述，” *机器学习研究期刊*，第20卷，第55:1–55:21页，2019年。'
- en: '[58] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu, “Hierarchical
    representations for efficient architecture search,” in *ICLR*, 2018, pp. 1–11.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, 和 K. Kavukcuoglu, “高效架构搜索的分层表示，”
    见 *ICLR*，2018年，第1–11页。'
- en: '[59] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015, pp. 1–10.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络，” 见 *ICLR*，2015年，第1–10页。'
- en: '[60] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *CVPR*, 2017, pp. 2261–2269.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] G. Huang, Z. Liu, L. van der Maaten, 和 K. Q. Weinberger, “密集连接卷积网络，” 见
    *CVPR*，2017年，第2261–2269页。'
- en: '[61] Z. Lu, I. Whalen, V. Boddeti, Y. D. Dhebar, K. Deb, E. D. Goodman, and
    W. Banzhaf, “Nsga-net: neural architecture search using multi-objective genetic
    algorithm,” in *GECCO*, 2019, pp. 419–427.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Z. Lu, I. Whalen, V. Boddeti, Y. D. Dhebar, K. Deb, E. D. Goodman, 和 W.
    Banzhaf, “Nsga-net：使用多目标遗传算法的神经网络架构搜索，” 见 *GECCO*，2019年，第419–427页。'
- en: '[62] F. Scheidegger, L. Benini, C. Bekas, and A. C. I. Malossi, “Constrained
    deep neural network architecture search for iot devices accounting for hardware
    calibration,” in *NeurIPS*, 2019, pp. 6054–6064.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] F. Scheidegger, L. Benini, C. Bekas, 和 A. C. I. Malossi, “针对物联网设备的受限深度神经网络架构搜索，考虑硬件校准，”
    见 *NeurIPS*，2019年，第6054–6064页。'
- en: '[63] Q. Lu, W. Jiang, X. Xu, Y. Shi, and J. Hu, “On neural architecture search
    for resource-constrained hardware platforms,” *arXiv preprint arXiv:1911.00105*,
    pp. 1–8, 2019.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Q. Lu, W. Jiang, X. Xu, Y. Shi, 和 J. Hu, “面向资源受限硬件平台的神经网络架构搜索，” *arXiv
    预印本 arXiv:1911.00105*，第1–8页，2019年。'
- en: '[64] T. Elsken, J. H. Metzen, and F. Hutter, “Efficient multi-objective neural
    architecture search via lamarckian evolution,” in *ICLR*, 2019, pp. 1–11.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] T. Elsken, J. H. Metzen, 和 F. Hutter, “通过拉马克进化的高效多目标神经网络架构搜索，” 见 *ICLR*，2019年，第1–11页。'
- en: '[65] Y. Zhou, S. Ebrahimi, S. Ö. Arık, H. Yu, H. Liu, and G. Diamos, “Resource-efficient
    neural architect,” *arXiv preprint arXiv:1806.07912*, pp. 1–14, 2018.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Zhou, S. Ebrahimi, S. Ö. Arık, H. Yu, H. Liu, 和 G. Diamos, “资源高效的神经网络架构，”
    *arXiv 预印本 arXiv:1806.07912*，第1–14页，2018年。'
- en: '[66] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu,
    and D. Marculescu, “Single-path NAS: designing hardware-efficient convnets in
    less than 4 hours,” in *ECML-PKDD*, 2019, pp. 481–497.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu,
    和 D. Marculescu, “单路径 NAS：在不到4小时内设计硬件高效的卷积网络，” 见 *ECML-PKDD*，2019年，第481–497页。'
- en: '[67] J. Fang, Y. Sun, Q. Zhang, Y. Li, W. Liu, and X. Wang, “Densely connected
    search space for more flexible neural architecture search,” in *CVPR*, 2020, pp.
    10 625–10 634.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Fang, Y. Sun, Q. Zhang, Y. Li, W. Liu, 和 X. Wang, “用于更灵活神经网络架构搜索的密集连接搜索空间，”
    见 *CVPR*，2020年，第10,625–10,634页。'
- en: '[68] M. Tan and Q. V. Le, “Efficientnet: Rethinking model scaling for convolutional
    neural networks,” in *ICML*, 2019, pp. 6105–6114.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] M. Tan 和 Q. V. Le, “Efficientnet：重新思考卷积神经网络的模型缩放，” 见 *ICML*，2019年，第6105–6114页。'
- en: '[69] Y. Xiong, H. Liu, S. Gupta, B. Akin, G. Bender, Y. Wang, P. Kindermans,
    M. Tan, V. Singh, and B. Chen, “Mobiledets: Searching for object detection architectures
    for mobile accelerators,” in *CVPR*, 2021, pp. 3825–3834.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Xiong, H. Liu, S. Gupta, B. Akin, G. Bender, Y. Wang, P. Kindermans,
    M. Tan, V. Singh, 和 B. Chen, “Mobiledets：为移动加速器搜索目标检测架构，” 见 *CVPR*，2021年，第3825–3834页。'
- en: '[70] S. Li, M. Tan, R. Pang, A. Li, L. Cheng, Q. V. Le, and N. P. Jouppi, “Searching
    for fast model families on datacenter accelerators,” in *CVPR*, 2021, pp. 8085–8095.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *CVPR*,
    2018, pp. 7132–7141.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Howard, R. Pang, H. Adam, Q. V. Le, M. Sandler, B. Chen, W. Wang, L. Chen,
    M. Tan, G. Chu, V. Vasudevan, and Y. Zhu, “Searching for mobilenetv3,” in *ICCV*,
    2019, pp. 1314–1324.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] R. Avenash and P. Viswanath, “Semantic segmentation of satellite images
    using a modified CNN with hard-swish activation function,” in *VISIGRAPP*, 2019,
    pp. 413–420.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Courbariaux, Y. Bengio, and J. David, “Binaryconnect: Training deep
    neural networks with binary weights during propagations,” in *NIPS*, 2015, pp.
    3123–3131.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. Yu, P. Jin, H. Liu, G. Bender, P. Kindermans, M. Tan, T. S. Huang,
    X. Song, R. Pang, and Q. Le, “Bignas: Scaling up neural architecture search with
    big single-stage models,” in *ECCV*, 2020, pp. 702–717.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Chu, B. Zhang, and R. Xu, “Moga: Searching beyond mobilenetv3,” in
    *ICASSP*, 2020, pp. 4042–4046.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train one
    network and specialize it for efficient deployment,” in *ICLR*, 2020.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen, Y. Tian, M. Yu,
    P. Vajda, and J. E. Gonzalez, “Fbnetv3: Joint architecture-recipe search using
    predictor pretraining,” in *CVPR*, 2021, pp. 16 276–16 285.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu, T. Xu,
    K. Chen, P. Vajda, and J. E. Gonzalez, “Fbnetv2: Differentiable neural architecture
    search for spatial and channel dimensions,” in *CVPR*, 2020, pp. 12 962–12 971.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] B. Chen, G. Ghiasi, H. Liu, T. Lin, D. Kalenichenko, H. Adam, and Q. V.
    Le, “Mnasfpn: Learning latency-aware pyramid architecture for object detection
    on mobile devices,” in *CVPR*, 2020, pp. 13 604–13 613.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] B. Yan, H. Peng, K. Wu, D. Wang, J. Fu, and H. Lu, “Lighttrack: Finding
    lightweight neural networks for object tracking via one-shot architecture search,”
    in *CVPR*, 2021, pp. 15 180–15 189.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. Xie, R. B. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in *CVPR*, 2017, pp. 5987–5995.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NIPS*, 2012, pp. 1106–1114.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] L. Xu, Y. Guan, S. Jin, W. Liu, C. Qian, P. Luo, W. Ouyang, and X. Wang,
    “Vipnas: Efficient video pose estimation via neural architecture search,” in *CVPR*,
    2021, pp. 16 072–16 081.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Dong, A. Cheng, D. Juan, W. Wei, and M. Sun, “Dpp-net: Device-aware
    progressive search for pareto-optimal neural architectures,” in *ECCV*, 2018,
    pp. 540–555.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, Y. Jia,
    and K. Keutzer, “Fbnet: Hardware-aware efficient convnet design via differentiable
    neural architecture search,” in *CVPR*, 2019, pp. 10 734–10 742.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Xiong, R. Mehta, and V. Singh, “Resource constrained neural network
    architecture search: Will a submodularity assumption help?” in *ICCV*, 2019, pp.
    1901–1910.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C.-H. Hsu, S.-H. Chang, J.-H. Liang, H.-P. Chou, C.-H. Liu, S.-C. Chang,
    J.-Y. Pan, Y.-T. Chen, W. Wei, and D.-C. Juan, “Monas: Multi-objective neural
    architecture search,” *arXiv preprint arXiv:1806.10332*, pp. 1–8, 2018.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] G. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger, “Condensenet:
    An efficient densenet using learned group convolutions,” in *CVPR*, 2018, pp.
    2752–2761.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] W. Chen, X. Gong, X. Liu, Q. Zhang, Y. Li, and Z. Wang, “Fasterseg: Searching
    for faster real-time semantic segmentation,” in *ICLR*, 2020, pp. 1–11.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han, “Searching
    efficient 3d architectures with sparse point-voxel convolution,” in *ECCV*, 2020,
    pp. 685–702.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] M. Ding, X. Lian, L. Yang, P. Wang, X. Jin, Z. Lu, and P. Luo, “HR-NAS:
    searching efficient high-resolution neural architectures with lightweight transformers,”
    in *CVPR*, 2021, pp. 2982–2992.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NIPS*, 2017, pp.
    5998–6008.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] D. R. So, Q. V. Le, and C. Liang, “The evolved transformer,” in *ICML*,
    2019, pp. 5877–5886.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
    “End-to-end object detection with transformers,” in *ECCV*, 2020, pp. 213–229.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    in *ICLR*, 2021, pp. 1–11.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Y. Chen, R. Gao, F. Liu, and D. Zhao, “Modulenet: Knowledge-inherited
    neural architecture search,” *IEEE Transactions on Cybernetics*, pp. 1–11, 2021.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015, pp. 1–9.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *CVPR*, 2016, pp. 2818–2826.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L. Li, L. Fei-Fei, A. L.
    Yuille, J. Huang, and K. Murphy, “Progressive neural architecture search,” in
    *ECCV*, 2018, pp. 19–35.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] D. Han, J. Kim, and J. Kim, “Deep pyramidal residual networks,” in *CVPR*,
    2017, pp. 6307–6315.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] H. Cai, J. Yang, W. Zhang, S. Han, and Y. Yu, “Path-level network transformation
    for efficient architecture search,” in *ICML*, 2018, pp. 677–686.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Xie, A. Kirillov, R. B. Girshick, and K. He, “Exploring randomly wired
    neural networks for image recognition,” in *ICCV*, 2019, pp. 1284–1293.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] S. Xie, A. Kirillov, R. B. Girshick, 和 K. He，“探索随机连接的神经网络进行图像识别，” *ICCV*，2019，第1284–1293页。'
- en: '[104] J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,”
    *Journal of Machine Learning Research*, vol. 13, pp. 281–305, 2012.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Bergstra 和 Y. Bengio，“超参数优化的随机搜索，” *Journal of Machine Learning Research*，第13卷，第281–305页，2012年。'
- en: '[105] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, “Single
    path one-shot neural architecture search with uniform sampling,” in *ECCV*, 2020,
    pp. 544–560.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, 和 J. Sun，“单路径一次性神经架构搜索与均匀采样，”
    *ECCV*，2020，第544–560页。'
- en: '[106] G. Bender, H. Liu, B. Chen, G. Chu, S. Cheng, P. Kindermans, and Q. V.
    Le, “Can weight sharing outperform random architecture search? an investigation
    with tunas,” in *CVPR*, 2020, pp. 14 311–14 320.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] G. Bender, H. Liu, B. Chen, G. Chu, S. Cheng, P. Kindermans, 和 Q. V.
    Le，“权重共享能否超越随机架构搜索？使用TUNAS的研究，” *CVPR*，2020，第14 311–14 320页。'
- en: '[107] M. Odema, N. Rashid, B. U. Demirel, and M. A. A. Faruque, “Lens: Layer
    distribution enabled neural architecture search in edge-cloud hierarchies,” in
    *DAC*, 2021, pp. 403–408.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M. Odema, N. Rashid, B. U. Demirel, 和 M. A. A. Faruque，“Lens：在边缘-云层次结构中启用层分布的神经架构搜索，”
    *DAC*，2021，第403–408页。'
- en: '[108] Z. Yang, S. Zhang, R. Li, C. Li, M. Wang, D. Wang, and M. Zhang, “Efficient
    resource-aware convolutional neural architecture search for edge computing with
    pareto-bayesian optimization,” *Sensors*, vol. 21, no. 2, p. 444, 2021.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Z. Yang, S. Zhang, R. Li, C. Li, M. Wang, D. Wang, 和 M. Zhang，“面向边缘计算的高效资源感知卷积神经架构搜索，结合帕累托-贝叶斯优化，”
    *Sensors*，第21卷，第2期，第444页，2021年。'
- en: '[109] M. Parsa, J. P. Mitchell, C. D. Schuman, R. M. Patton, T. E. Potok, and
    K. Roy, “Bayesian multi-objective hyperparameter optimization for accurate, fast,
    and efficient neural network accelerator design,” *Frontiers in Neuroscience*,
    p. 667, 2020.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. Parsa, J. P. Mitchell, C. D. Schuman, R. M. Patton, T. E. Potok, 和
    K. Roy，“用于精确、快速和高效神经网络加速器设计的贝叶斯多目标超参数优化，” *Frontiers in Neuroscience*，第667页，2020年。'
- en: '[110] D. Eriksson, I. Pierce, J. Chuang, S. Daulton, P. Xia, A. Shrivastava,
    A. Babu, S. Zhao, A. A. Aly, G. Venkatesh *et al.*, “Latency-aware neural architecture
    search with multi-objective bayesian optimization,” in *ICML Workshop on Automated
    Machine Learning (AutoML)*, 2021, pp. 1–9.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] D. Eriksson, I. Pierce, J. Chuang, S. Daulton, P. Xia, A. Shrivastava,
    A. Babu, S. Zhao, A. A. Aly, G. Venkatesh *等*，“基于延迟的神经架构搜索与多目标贝叶斯优化，” *ICML Workshop
    on Automated Machine Learning (AutoML)*，2021，第1–9页。'
- en: '[111] D. Gaudrie, “High-dimensional bayesian multi-objective optimization,”
    Ph.D. dissertation, Lyon, 2019.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] D. Gaudrie，“高维贝叶斯多目标优化，” 博士学位论文，里昂，2019年。'
- en: '[112] S. Daulton, D. Eriksson, M. Balandat, and E. Bakshy, “Multi-objective
    bayesian optimization over high-dimensional search spaces,” *arXiv preprint arXiv:2109.10964*,
    pp. 1–11, 2021.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] S. Daulton, D. Eriksson, M. Balandat, 和 E. Bakshy，“高维搜索空间上的多目标贝叶斯优化，”
    *arXiv preprint arXiv:2109.10964*，第1–11页，2021年。'
- en: '[113] B. Moons, P. Noorzad, A. Skliar, G. Mariani, D. Mehta, C. Lott, and T. Blankevoort,
    “Distilling optimal neural networks: Rapid search in diverse spaces,” in *ICCV*,
    2021, pp. 12 209–12 218.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] B. Moons, P. Noorzad, A. Skliar, G. Mariani, D. Mehta, C. Lott, 和 T.
    Blankevoort，“优化神经网络的蒸馏：在多样空间中的快速搜索，” *ICCV*，2021，第12 209–12 218页。'
- en: '[114] X. Luo, D. Liu, S. Huai, and W. Liu, “Hsconas: Hardware-software co-design
    of efficient dnns via neural architecture search,” in *DATE*, 2021, pp. 418–421.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] X. Luo, D. Liu, S. Huai, 和 W. Liu，“Hsconas：通过神经架构搜索设计高效的硬件-软件协同深度神经网络，”
    *DATE*，2021，第418–421页。'
- en: '[115] X. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y. Wang, M. Dukhan, Y. Hu, Y. Wu,
    Y. Jia, P. Vajda, M. Uyttendaele, and N. K. Jha, “Chamnet: Towards efficient network
    design through platform-aware model adaptation,” in *CVPR*, 2019, pp. 11 398–11 407.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] X. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y. Wang, M. Dukhan, Y. Hu, Y.
    Wu, Y. Jia, P. Vajda, M. Uyttendaele, 和 N. K. Jha，“Chamnet：通过平台感知模型适应实现高效的网络设计，”
    *CVPR*，2019，第11 398–11 407页。'
- en: '[116] M. Srinivas and L. M. Patnaik, “Adaptive probabilities of crossover and
    mutation in genetic algorithms,” *IEEE Transactions on Systems, Man, and Cybernetics*,
    vol. 24, no. 4, pp. 656–667, 1994.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] M. Srinivas 和 L. M. Patnaik，“遗传算法中的交叉和变异的自适应概率，” *IEEE Transactions on
    Systems, Man, and Cybernetics*，第24卷，第4期，第656–667页，1994年。'
- en: '[117] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, “Regularized evolution
    for image classifier architecture search,” in *AAAI*, 2019, pp. 4780–4789.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] E. Real, A. Aggarwal, Y. Huang, 和 Q. V. Le，“图像分类器架构搜索的正则化进化，” *AAAI*，2019，第4780–4789页。'
- en: '[118] X. Chu, B. Zhang, and R. Xu, “Fairnas: Rethinking evaluation fairness
    of weight sharing neural architecture search,” in *ICCV*, 2021, pp. 12 219–12 228.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] X. Chu, B. Zhang 和 R. Xu，“Fairnas：重新思考权重共享神经架构搜索的评估公平性”，发表于 *ICCV*，2021年，第12 219–12 228页。'
- en: '[119] K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan, “A fast and elitist
    multiobjective genetic algorithm: NSGA-II,” *IEEE Transactions on Evolutionary
    Computing*, vol. 6, no. 2, pp. 182–197, 2002.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] K. Deb, S. Agrawal, A. Pratap 和 T. Meyarivan，“一种快速且精英的多目标遗传算法：NSGA-II”，*IEEE进化计算汇刊*，第6卷，第2期，第182–197页，2002年。'
- en: '[120] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine Learning*, vol. 8, pp. 229–256,
    1992.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] R. J. Williams，“用于连接主义强化学习的简单统计梯度跟踪算法”，*机器学习*，第8卷，第229–256页，1992年。'
- en: '[121] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, pp. 1–9, 2017.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 和 O. Klimov，“近端策略优化算法”，*arXiv
    预印本 arXiv:1707.06347*，第1–9页，2017年。'
- en: '[122] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, “Efficient neural architecture
    search via parameters sharing,” in *ICML*, 2018, pp. 4095–4104.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] H. Pham, M. Guan, B. Zoph, Q. Le 和 J. Dean，“通过参数共享的高效神经架构搜索”，发表于 *ICML*，2018年，第4095–4104页。'
- en: '[123] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with gumbel-softmax,”
    in *ICLR*, 2017, pp. 1–11.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] E. Jang, S. Gu 和 B. Poole，“使用gumbel-softmax的分类重参数化”，发表于 *ICLR*，2017年，第1–11页。'
- en: '[124] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efficient neural
    architecture search via parameter sharing,” in *ICML*, 2018, pp. 4092–4101.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le 和 J. Dean，“通过参数共享的高效神经架构搜索”，发表于
    *ICML*，2018年，第4092–4101页。'
- en: '[125] G. Bender, P. Kindermans, B. Zoph, V. Vasudevan, and Q. V. Le, “Understanding
    and simplifying one-shot architecture search,” in *ICML*, 2018, pp. 550–559.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] G. Bender, P. Kindermans, B. Zoph, V. Vasudevan 和 Q. V. Le，“理解和简化一键架构搜索”，发表于
    *ICML*，2018年，第550–559页。'
- en: '[126] J. Yu and T. S. Huang, “Universally slimmable networks and improved training
    techniques,” in *ICCV*, 2019, pp. 1803–1811.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Yu 和 T. S. Huang，“普适可缩网络与改进的训练技术”，发表于 *ICCV*，2019年，第1803–1811页。'
- en: '[127] L. Deng, G. Li, S. Han, L. Shi, and Y. Xie, “Model compression and hardware
    acceleration for neural networks: A comprehensive survey,” *Proceedings of the
    IEEE*, vol. 108, no. 4, pp. 485–532, 2020.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] L. Deng, G. Li, S. Han, L. Shi 和 Y. Xie，“神经网络的模型压缩与硬件加速：综合调查”，*IEEE汇刊*，第108卷，第4期，第485–532页，2020年。'
- en: '[128] T. Choudhary, V. Mishra, A. Goswami, and J. Sarangapani, “A comprehensive
    survey on model compression and acceleration,” *Artificial Intelligence Review*,
    vol. 53, no. 7, pp. 5113–5155, 2020.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] T. Choudhary, V. Mishra, A. Goswami 和 J. Sarangapani，“关于模型压缩和加速的综合调查”，*人工智能评论*，第53卷，第7期，第5113–5155页，2020年。'
- en: '[129] L. R. Tucker, “Some mathematical notes on three-mode factor analysis,”
    *Psychometrika*, vol. 31, no. 3, pp. 279–311, 1966.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] L. R. Tucker，“三模因子分析的一些数学笔记”，*Psychometrika*，第31卷，第3期，第279–311页，1966年。'
- en: '[130] Y. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin, “Compression
    of deep convolutional neural networks for fast and low power mobile applications,”
    in *ICLR*, 2016, pp. 1–11.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. Kim, E. Park, S. Yoo, T. Choi, L. Yang 和 D. Shin，“用于快速和低功耗移动应用的深度卷积神经网络压缩”，发表于
    *ICLR*，2016年，第1–11页。'
- en: '[131] J. Kossaifi, A. Khanna, Z. Lipton, T. Furlanello, and A. Anandkumar,
    “Tensor contraction layers for parsimonious deep nets,” in *CVPR Workshops*, 2017,
    pp. 1940–1946.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Kossaifi, A. Khanna, Z. Lipton, T. Furlanello 和 A. Anandkumar，“用于节约深度网络的张量收缩层”，发表于
    *CVPR Workshops*，2017年，第1940–1946页。'
- en: '[132] J. D. Carroll and J.-J. Chang, “Analysis of individual differences in
    multidimensional scaling via an n-way generalization of “eckart-young” decomposition,”
    *Psychometrika*, vol. 35, no. 3, pp. 283–319, 1970.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] J. D. Carroll 和 J.-J. Chang，“通过“eckart-young”分解的n维推广分析多维标度中的个体差异”，*Psychometrika*，第35卷，第3期，第283–319页，1970年。'
- en: '[133] M. Astrid and S. Lee, “Cp-decomposition with tensor power method for
    convolutional neural networks compression,” in *BigComp*, 2017, pp. 115–118.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] M. Astrid 和 S. Lee，“用于卷积神经网络压缩的张量幂法Cp分解”，发表于 *BigComp*，2017年，第115–118页。'
- en: '[134] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets, and V. S. Lempitsky,
    “Speeding-up convolutional neural networks using fine-tuned cp-decomposition,”
    in *ICLR*, 2015, pp. 1–11.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] V. Lebedev, Y. Ganin, M. Rakhuba, I. V. Oseledets 和 V. S. Lempitsky，“通过精细调整的cp分解加速卷积神经网络”，发表于
    *ICLR*，2015年，第1–11页。'
- en: '[135] S. Nakajima, M. Sugiyama, S. D. Babacan, and R. Tomioka, “Global analytic
    solution of fully-observed variational bayesian matrix factorization,” *Journal
    of Machine Learning Research*, vol. 14, no. 1, pp. 1–37, 2013.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J. Gusak, M. Kholyavchenko, E. Ponomarev, L. Markeeva, P. Blagoveschensky,
    A. Cichocki, and I. V. Oseledets, “Automated multi-stage compression of neural
    networks,” in *ICCVW*, 2019, pp. 2501–2508.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] C. Hawkins, X. Liu, and Z. Zhang, “Towards compact neural networks via
    end-to-end training: A bayesian tensor approach with automatic rank determination,”
    *SIAM Journal on Mathematics of Data Science*, vol. 4, no. 1, pp. 46–71, 2022.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] C. Hawkins and Z. Zhang, “Bayesian tensorized neural networks with automatic
    rank selection,” *Neurocomputing*, vol. 453, pp. 172–180, 2021.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] M. Kodryan, D. Kropotov, and D. Vetrov, “Mars: Masked automatic ranks
    selection in tensor decompositions,” *arXiv preprint arXiv:2006.10859*, pp. 1–12,
    2020.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] M. Javaheripi, M. Samragh, and F. Koushanfar, “Autorank: Automated rank
    selection for effective neural network customization,” *IEEE Journal on Emerging
    and Selected Topics in Circuits and Systems*, vol. 11, no. 4, pp. 611–619, 2021.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M. Samragh, M. Javaheripi, and F. Koushanfar, “Autorank: Automated rank
    selection for effective neural network customization,” in *ISCA*, 2019, pp. 611–619.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] X. Ma, A. R. Triki, M. Berman, C. Sagonas, J. Calì, and M. B. Blaschko,
    “A bayesian optimization framework for neural network compression,” in *ICCV*,
    2019, pp. 10 273–10 282.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] C. Bucila, R. Caruana, and A. Niculescu-Mizil, “Model compression,” in
    *KDD*, 2006, pp. 535–541.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in *NIPS*,
    2014, pp. 2654–2662.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Y. Liu, X. Jia, M. Tan, R. Vemulapalli, Y. Zhu, B. Green, and X. Wang,
    “Search to distill: Pearls are everywhere but not the eyes,” in *CVPR*, 2020,
    pp. 7536–7545.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] X. Guo, J. Yang, H. Zhou, X. Ye, and J. Li, “Rosearch: Search for robust
    student architectures when distilling pre-trained language models,” *arXiv preprint
    arXiv:2106.03613*, pp. 1–10, 2021.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Z. Lei, K. Yang, K. Jiang, and S. Chen, “Kdas-reid: Architecture search
    for person re-identification via distilled knowledge with dynamic temperature,”
    *Algorithms*, vol. 14, no. 5, p. 137, 2021.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] R. H. Eyono, F. M. Carlucci, P. M. Esperança, B. Ru, and P. Torr, “Autokd:
    Automatic knowledge distillation into a student architecture family,” *arXiv preprint
    arXiv:2111.03555*, pp. 1–12, 2021.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] X. Zhang, Z. Zhou, D. Chen, and Y. E. Wang, “Autodistill: an end-to-end
    framework to explore and distill hardware-efficient language models,” *arXiv preprint
    arXiv:2201.08539*, pp. 1–14, 2022.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] L. Chen, F. Yuan, J. Yang, M. Yang, and C. Li, “Scene-adaptive knowledge
    distillation for sequential recommendation via differentiable architecture search,”
    *arXiv preprint arXiv:2107.07173*, pp. 1–11, 2021.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Z. Zhang, W. Zhu, J. Yan, P. Gao, and G. Xie, “Automatic student network
    search for knowledge distillation,” in *ICPR*, 2020, pp. 2446–2453.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Z. Zhang, W. Zhu, J. Yan, P. Gao, 和 G. Xie，“用于知识蒸馏的自动学生网络搜索”，见于 *ICPR*，2020年，页码2446–2453。'
- en: '[152] M. Li, J. Lin, Y. Ding, Z. Liu, J. Zhu, and S. Han, “Gan compression:
    Efficient architectures for interactive conditional gans,” in *CVPR*, 2020, pp.
    5283–5293.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] M. Li, J. Lin, Y. Ding, Z. Liu, J. Zhu, 和 S. Han，“Gan压缩：用于交互式条件GAN的高效架构”，见于
    *CVPR*，2020年，页码5283–5293。'
- en: '[153] W. Cheng, M. Zhao, Z. Ye, and S. Gu, “Mfagan: A compression framework
    for memory-efficient on-device super-resolution gan,” *arXiv preprint arXiv:2107.12679*,
    pp. 1–10, 2021.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] W. Cheng, M. Zhao, Z. Ye, 和 S. Gu，“Mfagan：一种内存高效的设备超分辨率GAN压缩框架”，*arXiv预印本
    arXiv:2107.12679*，页码1–10，2021年。'
- en: '[154] D. M. Vo, A. Sugimoto, and H. Nakayama, “PPCD-GAN: progressive pruning
    and class-aware distillation for large-scale conditional gans compression,” in
    *WACV*, 2022, pp. 1422–1430.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] D. M. Vo, A. Sugimoto, 和 H. Nakayama，“PPCD-GAN：用于大规模条件GAN压缩的渐进式修剪和类别感知蒸馏”，见于
    *WACV*，2022年，页码1422–1430。'
- en: '[155] M. Kang, J. Mun, and B. Han, “Towards oracle knowledge distillation with
    neural architecture search,” in *AAAI*, 2020, pp. 4404–4411.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] M. Kang, J. Mun, 和 B. Han，“通过神经架构搜索实现神谕知识蒸馏”，见于 *AAAI*，2020年，页码4404–4411。'
- en: '[156] K. Mitsuno, Y. Nomura, and T. Kurita, “Channel planting for deep neural
    networks using knowledge distillation,” in *ICPR*, 2020, pp. 7573–7579.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] K. Mitsuno, Y. Nomura, 和 T. Kurita，“使用知识蒸馏的深度神经网络通道植入”，见于 *ICPR*，2020年，页码7573–7579。'
- en: '[157] J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, and T. Liu, “NAS-BERT:
    task-agnostic and adaptive-size BERT compression with neural architecture search,”
    in *KDD*, 2021, pp. 1933–1943.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, 和 T. Liu，“NAS-BERT：任务无关和自适应大小的BERT压缩与神经架构搜索”，见于
    *KDD*，2021年，页码1933–1943。'
- en: '[158] C. Li, J. Peng, L. Yuan, G. Wang, X. Liang, L. Lin, and X. Chang, “Block-wisely
    supervised neural architecture search with knowledge distillation,” in *CVPR*,
    2020, pp. 1986–1995.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] C. Li, J. Peng, L. Yuan, G. Wang, X. Liang, L. Lin, 和 X. Chang，“基于知识蒸馏的块级监督神经架构搜索”，见于
    *CVPR*，2020年，页码1986–1995。'
- en: '[159] S. Park, J. Lee, S. Mo, and J. Shin, “Lookahead: A far-sighted alternative
    of magnitude-based pruning,” in *ICLR*, 2020, pp. 1–10.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] S. Park, J. Lee, S. Mo, 和 J. Shin，“Lookahead：一种基于幅度的修剪的远见替代方案”，见于 *ICLR*，2020年，页码1–10。'
- en: '[160] T. Zhang, S. Ye, Y. Zhang, Y. Wang, and M. Fardad, “A systematic weight
    pruning framework of dnns using alternating direction method of multipliers,”
    in *ECCV*, 2018, p. 191–207.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] T. Zhang, S. Ye, Y. Zhang, Y. Wang, 和 M. Fardad，“使用交替方向乘子法的DNN系统权重修剪框架”，见于
    *ECCV*，2018年，页码191–207。'
- en: '[161] W. Zeng, Y. Xiong, and R. Urtasun, “Network automatic pruning: Start
    nap and take a nap,” *arXiv preprint arXiv:2101.06608*, pp. 1–14, 2021.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] W. Zeng, Y. Xiong, 和 R. Urtasun，“网络自动修剪：开始打盹和小憩”，*arXiv预印本 arXiv:2101.06608*，页码1–14，2021年。'
- en: '[162] Z. Li, Y. Gong, X. Ma, S. Liu, M. Sun, Z. Zhan, Z. Kong, G. Yuan, and
    Y. Wang, “Ss-auto: A single-shot, automatic structured weight pruning framework
    of dnns with ultra-high efficiency,” *arXiv preprint arXiv:2001.08839*, pp. 1–8,
    2020.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Z. Li, Y. Gong, X. Ma, S. Liu, M. Sun, Z. Zhan, Z. Kong, G. Yuan, 和 Y.
    Wang，“Ss-auto：一种超高效的单次自动结构化权重修剪DNN框架”，*arXiv预印本 arXiv:2001.08839*，页码1–8，2020年。'
- en: '[163] X. Zheng, Y. Ma, T. Xi, G. Zhang, E. Ding, Y. Li, J. Chen, Y. Tian, and
    R. Ji, “An information theory-inspired strategy for automatic network pruning,”
    *arXiv preprint arXiv:2108.08532*, pp. 1–10, 2021.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] X. Zheng, Y. Ma, T. Xi, G. Zhang, E. Ding, Y. Li, J. Chen, Y. Tian, 和
    R. Ji，“一种基于信息理论的自动网络修剪策略”，*arXiv预印本 arXiv:2108.08532*，页码1–10，2021年。'
- en: '[164] N. Liu, X. Ma, Z. Xu, Y. Wang, J. Tang, and J. Ye, “Autocompress: An
    automatic DNN structured pruning framework for ultra-high compression rates,”
    in *AAAI*, 2020, pp. 4876–4883.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] N. Liu, X. Ma, Z. Xu, Y. Wang, J. Tang, 和 J. Ye，“Autocompress：用于超高压缩率的自动DNN结构修剪框架”，见于
    *AAAI*，2020年，页码4876–4883。'
- en: '[165] S. Yang, W. Chen, X. Zhang, S. He, Y. Yin, and X. Sun, “Auto-prune: automated
    dnn pruning and mapping for reram-based accelerator,” in *ICS*, 2021, pp. 304–315.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. Yang, W. Chen, X. Zhang, S. He, Y. Yin, 和 X. Sun，“Auto-prune：基于RERAM的加速器的自动化DNN修剪和映射”，见于
    *ICS*，2021年，页码304–315。'
- en: '[166] B. Li, Y. Fan, Z. Pan, Y. Bian, and G. Zhang, “Automatic channel pruning
    with hyper-parameter search and dynamic masking,” in *MM*, 2021, pp. 2121–2129.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] B. Li, Y. Fan, Z. Pan, Y. Bian, 和 G. Zhang，“自动通道修剪与超参数搜索及动态掩码”，见于 *MM*，2021年，页码2121–2129。'
- en: '[167] F. Tung, S. Muralidharan, and G. Mori, “Fine-pruning: Joint fine-tuning
    and compression of a convolutional network with bayesian optimization,” in *BMVC*,
    2017, pp. 115.1–115.12.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] F. Tung, S. Muralidharan, 和 G. Mori，“Fine-pruning：通过贝叶斯优化联合微调和压缩卷积网络”，见于
    *BMVC*，2017年，页码115.1–115.12。'
- en: '[168] C. Chen, F. Tung, N. Vedula, and G. Mori, “Constraint-aware deep neural
    network compression,” in *ECCV*, 2018, pp. 409–424.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] C. Chen, F. Tung, N. Vedula 和 G. Mori，“约束感知的深度神经网络压缩，”发表于 *ECCV*，2018年，第409–424页。'
- en: '[169] J. Mu, H. Fan, and W. Zhang, “High-dimensional bayesian optimization
    for cnn auto pruning with clustering and rollback,” in *ECCV*, 2022, pp. 1–17.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] J. Mu, H. Fan 和 W. Zhang，“高维贝叶斯优化用于具有聚类和回滚的CNN自动修剪，”发表于 *ECCV*，2022年，第1–17页。'
- en: '[170] J. Li, H. Li, Y. Chen, Z. Ding, N. Li, M. Ma, Z. Duan, and D. Zhao, “Abcp:
    Automatic block-wise and channel-wise network pruning via joint search,” *arXiv
    preprint arXiv:2110.03858*, pp. 1–12, 2021.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] J. Li, H. Li, Y. Chen, Z. Ding, N. Li, M. Ma, Z. Duan 和 D. Zhao，“ABCP:
    通过联合搜索进行自动块级和通道级网络修剪，” *arXiv预印本arXiv:2110.03858*，第1–12页，2021年。'
- en: '[171] L. Guerra and T. Drummond, “Automatic pruning for quantized neural networks,”
    in *DICTA*, 2021, pp. 1–8.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] L. Guerra 和 T. Drummond，“量化神经网络的自动修剪，”发表于 *DICTA*，2021年，第1–8页。'
- en: '[172] T. Yang, A. G. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze,
    and H. Adam, “Netadapt: Platform-aware neural network adaptation for mobile applications,”
    in *ECCV*, 2018, pp. 289–304.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] T. Yang, A. G. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze 和
    H. Adam，“Netadapt: 面向移动应用的平台感知神经网络适配，”发表于 *ECCV*，2018年，第289–304页。'
- en: '[173] T. Yang, Y. Liao, and V. Sze, “Netadaptv2: Efficient neural architecture
    search with fast super-network training and architecture optimization,” in *CVPR*,
    2021, pp. 2402–2411.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] T. Yang, Y. Liao 和 V. Sze，“Netadaptv2: 通过快速超网络训练和架构优化进行高效神经架构搜索，”发表于
    *CVPR*，2021年，第2402–2411页。'
- en: '[174] Y. Guan, N. Liu, P. Zhao, Z. Che, K. Bian, Y. Wang, and J. Tang, “Dais:
    Automatic channel pruning via differentiable annealing indicator search,” *IEEE
    Transactions on Neural Network and Learning Systems*, pp. 1–12, 2020.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Y. Guan, N. Liu, P. Zhao, Z. Che, K. Bian, Y. Wang 和 J. Tang，“Dais: 通过可微分退火指示器搜索进行自动通道修剪，”
    *IEEE神经网络与学习系统汇刊*，第1–12页，2020年。'
- en: '[175] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning efficient
    convolutional networks through network slimming,” in *ICCV*, 2017, pp. 2755–2763.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan 和 C. Zhang，“通过网络瘦身学习高效卷积网络，”发表于
    *ICCV*，2017年，第2755–2763页。'
- en: '[176] Y. Li, S. Gu, K. Zhang, L. V. Gool, and R. Timofte, “DHP: differentiable
    meta pruning via hypernetworks,” in *ECCV*, 2020, pp. 608–624.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Y. Li, S. Gu, K. Zhang, L. V. Gool 和 R. Timofte，“DHP: 通过超网络进行可微分元修剪，”发表于
    *ECCV*，2020年，第608–624页。'
- en: '[177] S. Qu, B. Li, Y. Wang, and L. Zhang, “ASBP: automatic structured bit-pruning
    for rram-based NN accelerator,” in *DAC*, 2021, pp. 745–750.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] S. Qu, B. Li, Y. Wang 和 L. Zhang，“ASBP: 基于RRAM的NN加速器的自动结构化比特修剪，”发表于 *DAC*，2021年，第745–750页。'
- en: '[178] X. Dong and Y. Yang, “Network pruning via transformable architecture
    search,” in *NeurIPS*, 2019, pp. 759–770.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] X. Dong 和 Y. Yang，“通过可变结构搜索进行网络修剪，”发表于 *NeurIPS*，2019年，第759–770页。'
- en: '[179] F. Xue and J. Xin, “Network compression via cooperative architecture
    search and distillation,” in *AI4I*, 2021, pp. 42–43.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] F. Xue 和 J. Xin，“通过合作架构搜索和蒸馏进行网络压缩，”发表于 *AI4I*，2021年，第42–43页。'
- en: '[180] L. Yao, R. Pi, H. Xu, W. Zhang, Z. Li, and T. Zhang, “Joint-detnas: Upgrade
    your detector with nas, pruning and dynamic distillation,” in *CVPR*, 2021, pp.
    10 175–10 184.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] L. Yao, R. Pi, H. Xu, W. Zhang, Z. Li 和 T. Zhang，“Joint-detnas: 使用NAS、修剪和动态蒸馏升级你的检测器，”发表于
    *CVPR*，2021年，第10,175–10,184页。'
- en: '[181] J. Gu and V. Tresp, “Search for better students to learn distilled knowledge,”
    in *ECAI*, 2020, pp. 1159–1165.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] J. Gu 和 V. Tresp，“寻找更优秀的学生以学习蒸馏知识，”发表于 *ECAI*，2020年，第1159–1165页。'
- en: '[182] M. Horowitz, “1.1 computing’s energy problem (and what we can do about
    it),” in *ISSCC*, 2014, pp. 10–14.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] M. Horowitz，“1.1 计算的能源问题（以及我们可以做些什么），”发表于 *ISSCC*，2014年，第10–14页。'
- en: '[183] H. Bai, M. Cao, P. Huang, and J. Shan, “Batchquant: Quantized-for-all
    architecture search with robust quantizer,” in *NeurIPS*, 2021, pp. 1074–1085.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] H. Bai, M. Cao, P. Huang 和 J. Shan，“Batchquant: 具有鲁棒量化器的全量化架构搜索，”发表于
    *NeurIPS*，2021年，第1074–1085页。'
- en: '[184] EENews, “Apple describes 7nm a12 bionic chips,” 2018.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] EENews，“苹果描述了7nm的A12仿生芯片，”2018年。'
- en: '[185] Nvidia, “Nvidia tensor cores,” 2018.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Nvidia，“Nvidia张量核心，”2018年。'
- en: '[186] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “Hardware-centric automl
    for mixed-precision quantization,” *International Journal of Computer Vision*,
    vol. 128, no. 8, pp. 2035–2048, 2020.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] K. Wang, Z. Liu, Y. Lin, J. Lin 和 S. Han，“面向硬件的自动机器学习用于混合精度量化，” *国际计算机视觉杂志*，第128卷，第8期，第2035–2048页，2020年。'
- en: '[187] G. Lacey, G. W. Taylor, and S. Areibi, “Stochastic layer-wise precision
    in deep neural networks,” in *UAI*, 2018, pp. 663–672.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] G. Lacey, G. W. Taylor 和 S. Areibi，“深度神经网络中的随机层级精度，”发表于 *UAI*，2018年，第663–672页。'
- en: '[188] H. Yu, Q. Han, J. Li, J. Shi, G. Cheng, and B. Fan, “Search what you
    want: Barrier panelty NAS for mixed precision quantization,” in *ECCV*, 2020,
    pp. 1–16.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] H. Yu, Q. Han, J. Li, J. Shi, G. Cheng, 和 B. Fan，“搜索你想要的：混合精度量化的障碍面惩罚
    NAS，”在 *ECCV*，2020 年，第 1–16 页。'
- en: '[189] J. Xu, S. Hu, J. Yu, X. Liu, and H. Meng, “Mixed precision quantization
    of transformer language models for speech recognition,” in *ICASSP*, 2021, pp.
    7383–7387.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] J. Xu, S. Hu, J. Yu, X. Liu, 和 H. Meng，“用于语音识别的变换器语言模型的混合精度量化，”在 *ICASSP*，2021
    年，第 7383–7387 页。'
- en: '[190] Q. Sun, L. Jiao, Y. Ren, X. Li, F. Shang, and F. Liu, “Effective and
    fast: A novel sequential single path search for mixed-precision quantization,”
    *IEEE Transactions on Cybernetics*, pp. 1–13, 2022.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Q. Sun, L. Jiao, Y. Ren, X. Li, F. Shang, 和 F. Liu，“有效且快速：一种新颖的序列单路径搜索用于混合精度量化，”
    *IEEE Transactions on Cybernetics*，第 1–13 页，2022 年。'
- en: '[191] X. Wei, H. Chen, W. Liu, and Y. Xie, “Mixed-precision quantization for
    cnn-based remote sensing scene classification,” *IEEE Geoscience and Remote Sensing
    Letters*, vol. 18, no. 10, pp. 1721–1725, 2021.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] X. Wei, H. Chen, W. Liu, 和 Y. Xie，“基于 CNN 的遥感场景分类的混合精度量化，” *IEEE Geoscience
    and Remote Sensing Letters*，第 18 卷，第 10 期，第 1721–1725 页，2021 年。'
- en: '[192] B. Wu, Y. Wang, P. Zhang, Y. Tian, P. Vajda, and K. Keutzer, “Mixed precision
    quantization of convnets via differentiable neural architecture search,” *arXiv
    preprint arXiv:1812.00090*, pp. 1–11, 2018.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] B. Wu, Y. Wang, P. Zhang, Y. Tian, P. Vajda, 和 K. Keutzer，“通过可微分神经网络架构搜索进行卷积网络的混合精度量化，”
    *arXiv 预印本 arXiv:1812.00090*，第 1–11 页，2018 年。'
- en: '[193] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen,
    and T. Blankevoort, “A white paper on neural network quantization,” *arXiv preprint
    arXiv:2106.08295*, pp. 1–27, 2021.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen,
    和 T. Blankevoort，“关于神经网络量化的白皮书，” *arXiv 预印本 arXiv:2106.08295*，第 1–27 页，2021 年。'
- en: '[194] Y. Guan, P. Zhao, B. Wang, Y. Zhang, C. Yao, K. Bian, and J. Tang, “Differentiable
    feature aggregation search for knowledge distillation,” in *ECCV*, 2020, pp. 469–484.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Y. Guan, P. Zhao, B. Wang, Y. Zhang, C. Yao, K. Bian, 和 J. Tang，“用于知识蒸馏的可微分特征聚合搜索，”在
    *ECCV*，2020 年，第 469–484 页。'
- en: '[195] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value
    of network pruning,” in *ICLR*, 2019, pp. 1–16.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Z. Liu, M. Sun, T. Zhou, G. Huang, 和 T. Darrell，“重新思考网络剪枝的价值，”在 *ICLR*，2019
    年，第 1–16 页。'
- en: '[196] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K. Cheng, and J. Sun, “Metapruning:
    Meta learning for automatic neural network channel pruning,” in *ICCV*, 2019,
    pp. 3295–3304.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K. Cheng, 和 J. Sun，“Metapruning：用于自动神经网络通道剪枝的元学习，”在
    *ICCV*，2019 年，第 3295–3304 页。'
- en: '[197] J. Liu, J. Sun, Z. Xu, and G. Sun, “Latency-aware automatic cnn channel
    pruning with gpu runtime analysis,” *BenchCouncil Transactions on Benchmarks,
    Standards and Evaluations*, vol. 1, no. 1, p. 100009, 2021.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] J. Liu, J. Sun, Z. Xu, 和 G. Sun，“基于 GPU 运行时分析的延迟感知自动 CNN 通道剪枝，” *BenchCouncil
    Transactions on Benchmarks, Standards and Evaluations*，第 1 卷，第 1 期，第 100009 页，2021
    年。'
- en: '[198] M. Lin, R. Ji, Y. Zhang, B. Zhang, Y. Wu, and Y. Tian, “Channel pruning
    via automatic structure search,” in *IJCAI*, 2020, pp. 673–679.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] M. Lin, R. Ji, Y. Zhang, B. Zhang, Y. Wu, 和 Y. Tian，“通过自动结构搜索进行通道剪枝，”在
    *IJCAI*，2020 年，第 673–679 页。'
- en: '[199] J. Chang, Y. Lu, P. Xue, Y. Xu, and Z. Wei, “Acp: Automatic channel pruning
    via clustering and swarm intelligence optimization for cnn,” *arXiv preprint arXiv:2101.06407*,
    pp. 1–10, 2021.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Chang, Y. Lu, P. Xue, Y. Xu, 和 Z. Wei，“Acp：通过聚类和群体智能优化进行 CNN 的自动通道剪枝，”
    *arXiv 预印本 arXiv:2101.06407*，第 1–10 页，2021 年。'
- en: '[200] Y. Liu, Y. Wang, H. Qi, and X. Ju, “Superpruner: Automatic neural network
    pruning via super network,” *Scientific Programming*, vol. 2021, pp. 1–11, 2021.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Y. Liu, Y. Wang, H. Qi, 和 X. Ju，“Superpruner：通过超级网络自动化神经网络剪枝，” *Scientific
    Programming*，第 2021 卷，第 1–11 页，2021 年。'
- en: '[201] L. Lin, Y. Yang, and Z. Guo, “Aacp: Model compression by accurate and
    automatic channel pruning,” *arXiv preprint arXiv:2102.00390*, pp. 1–10, 2021.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] L. Lin, Y. Yang, 和 Z. Guo，“Aacp：通过精确和自动化通道剪枝进行模型压缩，” *arXiv 预印本 arXiv:2102.00390*，第
    1–10 页，2021 年。'
- en: '[202] I. Fedorov, R. P. Adams, M. Mattina, and P. N. Whatmough, “Sparse: Sparse
    architecture search for cnns on resource-constrained microcontrollers,” in *NeurIPS*,
    2019, pp. 4978–4990.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] I. Fedorov, R. P. Adams, M. Mattina, 和 P. N. Whatmough，“Sparse：用于资源受限微控制器的稀疏架构搜索，”在
    *NeurIPS*，2019 年，第 4978–4990 页。'
- en: '[203] H. Cai, T. Wang, Z. Wu, K. Wang, J. Lin, and S. Han, “On-device image
    classification with proxyless neural architecture search and quantization-aware
    fine-tuning,” in *ICCVW*, 2019, pp. 2509–2513.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] H. Cai, T. Wang, Z. Wu, K. Wang, J. Lin, 和 S. Han，“基于无代理神经架构搜索和量化感知微调的设备端图像分类，”在
    *ICCVW*，2019 年，第 2509–2513 页。'
- en: '[204] C. Liu, Y. Han, Y. Sung, Y. Lee, H. Chiang, and K. Wu, “FOX-NAS: fast,
    on-device and explainable neural architecture search,” in *ICCVW*, 2021, pp. 789–797.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] D. Peter, W. Roth, and F. Pernkopf, “Resource-efficient dnns for keyword
    spotting using neural architecture search and quantization,” in *ICPR*, 2020,
    pp. 9273–9279.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] A. Bulat, B. Martínez, and G. Tzimiropoulos, “BATS: binary architecture
    search,” in *ECCV*, 2020, pp. 309–325.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] H. Phan, Z. Liu, D. Huynh, M. Savvides, K. Cheng, and Z. Shen, “Binarizing
    mobilenet via evolution-based searching,” in *CVPR*, 2020, pp. 13 417–13 426.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] D. Kim, K. P. Singh, and J. Choi, “Learning architectures for binary
    networks,” in *ECCV*, 2020, pp. 575–591.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] M. Shen, K. Han, C. Xu, and Y. Wang, “Searching for accurate binary neural
    architectures,” in *ICCVW*, 2019, pp. 2041–2044.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] T. Kim, Y. Yoo, and J. Yang, “Frostnet: Towards quantization-awareof
    network architecture search,” *arXiv preprint arXiv:2006.09679*, pp. 1–18, 2020.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] S. Xu, J. Zhao, J. Lu, B. Zhang, S. Han, and D. S. Doermann, “Layer-wise
    searching for 1-bit detectors,” in *CVPR*, 2021, pp. 5682–5691.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Y. Chen, G. Meng, Q. Zhang, X. Zhang, L. Song, S. Xiang, and C. Pan,
    “Joint neural architecture search and quantization,” *arXiv preprint arXiv:1811.09426*,
    pp. 1–10, 2018.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] C. Gong, Z. Jiang, D. Wang, Y. Lin, Q. Liu, and D. Z. Pan, “Mixed precision
    neural architecture search for energy efficient deep learning,” in *ICCAD*, 2019,
    pp. 1–7.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] T. Wang, K. Wang, H. Cai, J. Lin, Z. Liu, H. Wang, Y. Lin, and S. Han,
    “APQ: joint search for network architecture, pruning and quantization policy,”
    in *CVPR*, 2020, pp. 2075–2084.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] M. Shen, F. Liang, R. Gong, Y. Li, C. Li, C. Lin, F. Yu, J. Yan, and
    W. Ouyang, “Once quantization-aware training: High performance extremely low-bit
    architecture search,” in *ICCV*, 2021, pp. 5320–5329.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] M. S. Abdelfattah, A. Mehrotra, L. Dudziak, and N. D. Lane, “Zero-cost
    proxies for lightweight NAS,” in *ICLR*, 2021, pp. 1–11.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] M. Lin, P. Wang, Z. Sun, H. Chen, X. Sun, Q. Qian, H. Li, and R. Jin,
    “Zen-nas: A zero-shot NAS for high-performance image recognition,” in *ICCV*,
    2021, pp. 337–346.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] A. Gholami, K. Kwon, B. Wu, Z. Tai, X. Yue, P. H. Jin, S. Zhao, and K. Keutzer,
    “Squeezenext: Hardware-aware neural network design,” in *CVPR Workshops*, 2018,
    pp. 1638–1647.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] M. Sun, P. Zhao, Y. Wang, N. Chang, and X. Lin, “HSIM-DNN: hardware simulator
    for computation-, storage- and power-efficient deep neural networks,” in *GLSVLSI*,
    2019, pp. 81–86.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao,
    and Y. Lin, “Hw-nas-bench: Hardware-aware neural architecture search benchmark,”
    in *ICLR*, 2019, pp. 1–14.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
