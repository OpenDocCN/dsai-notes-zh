- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:57:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:57:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2012.13392] Deep Learning-Based Human Pose Estimation: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2012.13392] 基于深度学习的人体姿态估计：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2012.13392](https://ar5iv.labs.arxiv.org/html/2012.13392)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2012.13392](https://ar5iv.labs.arxiv.org/html/2012.13392)
- en: 'Deep Learning-Based Human Pose Estimation: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的人体姿态估计：综述
- en: Ce Zheng [cezheng@knights.ucf.edu](mailto:cezheng@knights.ucf.edu) University
    of Central Florida4328 Scorpius St, Suite 245OrlandoFloridaUSA32816 ,  Wenhan
    Wu [wwu25@uncc.edu](mailto:wwu25@uncc.edu) University of North Carolina at Charlotte9201
    University City BlvdCharlotteNorth CarolinaUSA28223 ,  Chen Chen [chen.chen@crcv.ucf.edu](mailto:chen.chen@crcv.ucf.edu)
    University of Central Florida4328 Scorpius St, Suite 245OrlandoFloridaUSA32816
    ,  Taojiannan Yang [taoyang1122@knights.ucf.edu](mailto:taoyang1122@knights.ucf.edu)
    University of Central Florida4328 Scorpius St, Suite 245OrlandoFloridaUSA32816
    ,  Sijie Zhu [sizhu@knights.ucf.edu](mailto:sizhu@knights.ucf.edu) University
    of Central Florida4328 Scorpius St, Suite 245OrlandoFloridaUSA32816 ,  Ju Shen
    [jshen1@udayton.edu](mailto:jshen1@udayton.edu) University of Dayton300 College
    ParkDaytonOhioUSA45469 ,  Nasser Kehtarnavaz [kehtar@utdallas.edu](mailto:kehtar@utdallas.edu)
    University of Texas at Dallas 800 W. Campbell RoadRichardsonTexasUSA75080  and 
    Mubarak Shah [shah@crcv.ucf.edu](mailto:shah@crcv.ucf.edu) University of Central
    Florida4328 Scorpius St, Suite 245OrlandoFloridaUSA32816(2018)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ce Zheng [cezheng@knights.ucf.edu](mailto:cezheng@knights.ucf.edu) 中佛罗里达大学4328
    Scorpius St, Suite 245奥兰多佛罗里达美国32816，Wenhan Wu [wwu25@uncc.edu](mailto:wwu25@uncc.edu)
    北卡罗来纳大学夏洛特分校9201 University City Blvd夏洛特北卡罗来纳美国28223，Chen Chen [chen.chen@crcv.ucf.edu](mailto:chen.chen@crcv.ucf.edu)
    中佛罗里达大学4328 Scorpius St, Suite 245奥兰多佛罗里达美国32816，Taojiannan Yang [taoyang1122@knights.ucf.edu](mailto:taoyang1122@knights.ucf.edu)
    中佛罗里达大学4328 Scorpius St, Suite 245奥兰多佛罗里达美国32816，Sijie Zhu [sizhu@knights.ucf.edu](mailto:sizhu@knights.ucf.edu)
    中佛罗里达大学4328 Scorpius St, Suite 245奥兰多佛罗里达美国32816，Ju Shen [jshen1@udayton.edu](mailto:jshen1@udayton.edu)
    代顿大学300 College Park代顿俄亥俄美国45469，Nasser Kehtarnavaz [kehtar@utdallas.edu](mailto:kehtar@utdallas.edu)
    德克萨斯大学达拉斯分校800 W. Campbell Road理查森德克萨斯美国75080 和 Mubarak Shah [shah@crcv.ucf.edu](mailto:shah@crcv.ucf.edu)
    中佛罗里达大学4328 Scorpius St, Suite 245奥兰多佛罗里达美国32816 (2018)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Human pose estimation aims to locate the human body parts and build human body
    representation (e.g., body skeleton) from input data such as images and videos.
    It has drawn increasing attention during the past decade and has been utilized
    in a wide range of applications including human-computer interaction, motion analysis,
    augmented reality, and virtual reality. Although the recently developed deep learning-based
    solutions have achieved high performance in human pose estimation, there still
    remain challenges due to insufficient training data, depth ambiguities, and occlusion.
    The goal of this survey paper is to provide a comprehensive review of recent deep
    learning-based solutions for both 2D and 3D pose estimation via a systematic analysis
    and comparison of these solutions based on their input data and inference procedures.
    More than 260 research papers since 2014 are covered in this survey. Furthermore,
    2D and 3D human pose estimation datasets and evaluation metrics are included.
    Quantitative performance comparisons of the reviewed methods on popular datasets
    are summarized and discussed. Finally, the challenges involved, applications,
    and future research directions are concluded. A regularly updated project page
    is provided: [https://github.com/zczcwh/DL-HPE](https://github.com/zczcwh/DL-HPE)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计旨在从输入数据（如图像和视频）中定位人体部位并构建人体表示（例如，身体骨架）。在过去十年中，它引起了越来越多的关注，并已被广泛应用于包括人机交互、动作分析、增强现实和虚拟现实在内的各种应用。尽管最近开发的基于深度学习的解决方案在人类姿态估计方面取得了高性能，但由于训练数据不足、深度模糊和遮挡等问题，仍然存在挑战。本文综述的目标是通过系统分析和比较这些解决方案，提供对近期深度学习解决方案在2D和3D姿态估计中的全面回顾。该综述涵盖了自2014年以来的260多篇研究论文。此外，还包括2D和3D人体姿态估计数据集和评估指标。总结并讨论了在流行数据集上审阅方法的定量性能比较。最后，总结了涉及的挑战、应用和未来研究方向。提供了一个定期更新的项目页面：[https://github.com/zczcwh/DL-HPE](https://github.com/zczcwh/DL-HPE)
- en: 'Survey of human pose estimation, 2D and 3D pose estimation, deep learning-based
    pose estimation, pose estimation datasets, pose estimation metrics^†^†copyright:
    acmcopyright^†^†journalyear: 2018^†^†doi: 10.1145/1122445.1122456^†^†journal:
    JACM^†^†journalvolume: 37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth:
    8^†^†ccs: Computing methodologies Computer vision^†^†ccs: General and reference Surveys
    and overviews'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '人体姿态估计的调查、2D 和 3D 姿态估计、基于深度学习的姿态估计、姿态估计数据集、姿态估计指标^†^†版权：acmcopyright^†^†期刊年份：2018^†^†doi:
    10.1145/1122445.1122456^†^†期刊：JACM^†^†期刊卷号：37^†^†期刊期号：4^†^†文章：111^†^†出版月份：8^†^†ccs:
    计算方法 计算机视觉^†^†ccs: 综合和参考 调查和概述'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Human pose estimation (HPE), which has been extensively studied in computer
    vision literature, involves estimating the configuration of human body parts from
    input data captured by sensors, in particular images and videos. HPE provides
    geometric and motion information about the human body which has been applied to
    a wide range of applications (e.g., human-computer interaction, motion analysis,
    augmented reality (AR), virtual reality (VR), healthcare, etc.). With the rapid
    development of deep learning solutions in recent years, such solutions have been
    shown to outperform classical computer vision methods in various tasks including
    image classification (Krizhevsky et al., [2012](#bib.bib106)), semantic segmentation
    (Long et al., [2015](#bib.bib143)), and object detection (Ren et al., [2015](#bib.bib205)).
    Significant progress and remarkable performance have already been made by employing
    deep learning techniques in HPE tasks. However, challenges such as occlusion,
    insufficient training data, and depth ambiguity still pose difficulties to be
    overcome. 2D HPE from images and videos with 2D pose annotations is easily achievable
    and high performance has been reached for the human pose estimation of a single
    person using deep learning techniques. More recently, attention has been paid
    to highly occluded multi-person HPE in complex scenes. In contrast, for 3D HPE,
    obtaining accurate 3D pose annotations is much more difficult than its 2D counterpart.
    Motion capture systems can collect 3D pose annotation in controlled lab environments;
    however, they have limitations for in-the-wild environments. For 3D HPE from monocular
    RGB images and videos, the main challenge is depth ambiguities. In a multi-view
    setting, viewpoints association is the key issue that needs to be addressed. Some
    works have utilized sensors such as depth sensors, inertial measurement units
    (IMUs), and radio frequency devices, but these approaches are usually not cost-effective
    and require special-purpose hardware.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计（HPE）在计算机视觉领域中已经被广泛研究，它涉及从传感器（尤其是图像和视频）捕获的输入数据中估计人体部位的配置。HPE 提供了有关人体的几何和运动信息，这些信息已经应用于广泛的领域（例如，人机交互、运动分析、增强现实（AR）、虚拟现实（VR）、医疗保健等）。随着近年来深度学习解决方案的快速发展，这些解决方案已被证明在图像分类（Krizhevsky
    et al., [2012](#bib.bib106)）、语义分割（Long et al., [2015](#bib.bib143)）和目标检测（Ren et
    al., [2015](#bib.bib205)）等各种任务中优于经典的计算机视觉方法。通过采用深度学习技术，HPE 任务已经取得了显著进展和显著性能。然而，遮挡、训练数据不足和深度模糊等挑战仍然存在，需要克服。使用深度学习技术进行的单人2D
    HPE 从图像和视频中获得高性能是容易实现的。近年来，研究重点转向复杂场景中高度遮挡的多人体 HPE。相比之下，获取准确的3D姿态标注要比2D 任务困难得多。运动捕捉系统可以在受控实验室环境中收集3D姿态标注；然而，它们在自然环境中的应用有限。对于从单目
    RGB 图像和视频中进行的3D HPE，主要挑战是深度模糊。在多视角设置中，视角关联是需要解决的关键问题。一些研究利用了深度传感器、惯性测量单元（IMUs）和射频设备等传感器，但这些方法通常成本较高且需要特殊用途的硬件。
- en: Given the rapid progress in HPE research, this article attempts to track recent
    advances and summarize their achievements in order to provide a clear picture
    of current research on deep learning-based 2D and 3D HPE.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 HPE 研究的快速进展，本文试图追踪近期的进展并总结其成果，以提供当前基于深度学习的2D和3D HPE研究的清晰概况。
- en: '![Refer to caption](img/42abfdca0eddba74d30eab07f600b0aa.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/42abfdca0eddba74d30eab07f600b0aa.png)'
- en: Figure 1\. Taxonomy of this survey.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 本调查的分类
- en: 1.1\. Previous surveys and our contributions
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 以前的调查及我们的贡献
- en: There are several related surveys and reviews previously reported on HPE. Among
    them, (Moeslund and Granum, [2001](#bib.bib164); Moeslund et al., [2006](#bib.bib165);
    Poppe, [2007](#bib.bib195); Ji and Liu, [2009](#bib.bib86)) focus on the general
    field of visual-based human motion capture including pose estimation, tracking,
    and action recognition. Therefore, pose estimation is only one of the topics covered
    in these surveys. The research works on 3D HPE before 2012 are reviewed in (Holte
    et al., [2012](#bib.bib68)). The body parts parsing-based methods for single-view
    and multi-view HPE are reported in (Liu et al., [2015](#bib.bib142)). These surveys
    published during 2001-2015 mainly focused on conventional methods without deep
    learning. A survey on both traditional and deep learning-based methods related
    to HPE is presented in (Gong et al., [2016](#bib.bib61)). However, only a handful
    of deep learning-based approaches are included. The survey in (Sarafianos et al.,
    [2016](#bib.bib215)) covers 3D HPE methods with RGB inputs, while the survey in
    (Munea et al., [2020](#bib.bib170)) only reviews 2D HPE methods. Monocular HPE
    from the classical to recent deep learning-based methods (till 2019, less than
    100 papers) is summarized in (Chen et al., [2020f](#bib.bib29)). However, it only
    covers 2D HPE and 3D single-view HPE from monocular cameras. 3D multi-view HPE
    from monocular cameras and 3D HPE from other sensors are ignored. Also, no extensive
    performance comparisons or in-depth analyses are given, and the discussion on
    existing challenges and future directions is relatively short.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前有几篇相关的调查和综述文章报告了HPE。其中，（Moeslund和Granum, [2001](#bib.bib164)；Moeslund等, [2006](#bib.bib165)；Poppe,
    [2007](#bib.bib195)；Ji和Liu, [2009](#bib.bib86)）关注于基于视觉的人体运动捕捉的一般领域，包括姿态估计、跟踪和动作识别。因此，姿态估计只是这些调查涵盖的主题之一。2012年前关于3D
    HPE的研究工作在（Holte等, [2012](#bib.bib68)）中进行了回顾。基于身体部位解析的单视角和多视角HPE方法在（Liu等, [2015](#bib.bib142)）中进行了报告。这些在2001-2015年间发布的调查主要集中于传统方法，没有涉及深度学习。关于传统和深度学习方法的HPE调查见（Gong等,
    [2016](#bib.bib61)）。然而，其中仅包括少数深度学习方法。（Sarafianos等, [2016](#bib.bib215)）的调查涵盖了RGB输入的3D
    HPE方法，而（Munea等, [2020](#bib.bib170)）的调查仅回顾了2D HPE方法。关于从经典到最近的深度学习方法（截至2019年，不到100篇论文）的单目HPE总结见（Chen等,
    [2020f](#bib.bib29)）。然而，这只涵盖了2D HPE和来自单目摄像头的3D单视角HPE。来自单目摄像头的3D多视角HPE以及来自其他传感器的3D
    HPE被忽略。此外，没有进行广泛的性能比较或深入分析，关于现有挑战和未来方向的讨论也相对较短。
- en: 'This survey aims to address the shortcomings of the previous surveys in terms
    of providing a systematic review of the recent deep learning-based solutions to
    2D and 3D HPE but also covering other aspects of HPE including the performance
    evaluation of (2D and 3D) HPE methods on popular datasets, their applications,
    and comprehensive discussion. The key points that distinguish this survey from
    the previous ones are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查旨在弥补以往调查在系统回顾基于深度学习的2D和3D人体姿态估计（HPE）解决方案方面的不足，同时涵盖HPE的其他方面，包括（2D和3D）HPE方法在常用数据集上的性能评估、其应用以及综合讨论。本调查与以往调查的关键区别在于：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A comprehensive review of recent deep learning-based 2D and 3D HPE methods (up
    to 2022 with more than 260 papers) is provided by categorizing them according
    to 2D or 3D scenarios, single-view or multi-view, from monocular images/videos
    or other sources, and learning paradigm.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了对最新深度学习方法的2D和3D HPE（截至2022年，超过260篇论文）的全面综述，按照2D或3D场景、单视角或多视角、来自单目图像/视频或其他来源、以及学习范式进行分类。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extensive performance evaluation of 2D and 3D HPE methods. We summarize and
    compare reported performances of promising methods on common datasets based on
    their categories. The comparison of results provides cues for the strengths and
    weaknesses of different methods, revealing the research trends and future directions
    of HPE.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对2D和3D HPE方法进行广泛的性能评估。我们根据方法的类别总结和比较了在常用数据集上报告的有前景方法的性能。这些结果的比较提供了不同方法的优缺点线索，揭示了HPE的研究趋势和未来方向。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An overview of a wide range of HPE applications, such as surveillance, AR/VR,
    and healthcare.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对HPE应用的广泛概述，如监控、增强现实/虚拟现实和医疗保健。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An thorough discussion of 2D and 3D HPE is presented in terms of key challenges
    in HPE pointing to potential future research toward improving performance.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对2D和3D HPE进行深入讨论，重点是HPE中的关键挑战，并指向潜在的未来研究方向以改善性能。
- en: 1.2\. Paper organization
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2. 论文组织
- en: 'HPE is divided into two main categories: 2D HPE (§ [2](#S2 "2\. 2D human pose
    estimation ‣ Deep Learning-Based Human Pose Estimation: A Survey")) and 3D HPE
    (§ [3](#S3 "3\. 3D human pose estimation ‣ Deep Learning-Based Human Pose Estimation:
    A Survey")). Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Deep Learning-Based
    Human Pose Estimation: A Survey") shows the taxonomy of deep learning methods
    for HPE. According to the number of people, 2D HPE methods are categorized into
    single-person and multi-person settings. For single-person methods (§ [2.1](#S2.SS1
    "2.1\. 2D single-person pose estimation ‣ 2\. 2D human pose estimation ‣ Deep
    Learning-Based Human Pose Estimation: A Survey")), there are two categories: regression
    methods and heatmap-based methods. For multi-person methods (§ [2.2](#S2.SS2 "2.2\.
    2D multi-person pose estimation ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based
    Human Pose Estimation: A Survey")), there are also two types of methods: top-down
    methods and bottom-up methods.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: HPE 分为两个主要类别：2D HPE (§ [2](#S2 "2\. 2D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述")) 和 3D HPE
    (§ [3](#S3 "3\. 3D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述"))。图 [1](#S1.F1 "图 1 ‣ 1\. 引言 ‣ 基于深度学习的人体姿态估计：综述")
    展示了 HPE 的深度学习方法分类。根据人数，2D HPE 方法分为单人和多人设置。对于单人方法 (§ [2.1](#S2.SS1 "2.1\. 2D 单人姿态估计
    ‣ 2\. 2D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述"))，分为回归方法和基于热图的方法。对于多人方法 (§ [2.2](#S2.SS2 "2.2\.
    2D 多人姿态估计 ‣ 2\. 2D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述"))，也有两种方法：自上而下的方法和自下而上的方法。
- en: '3D HPE methods are classified according to the input source types: monocular
    RGB images and videos (§ [3.1](#S3.SS1 "3.1\. 3D HPE from monocular RGB images
    and videos ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based Human Pose Estimation:
    A Survey")), or other sensors (e.g., inertial measurement unit sensors, § [3.2](#S3.SS2
    "3.2\. 3D HPE from other sources ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based
    Human Pose Estimation: A Survey")). The majority of these methods use monocular
    RGB images and videos, and they are further divided into single-view single-person
    (§ [3.1.1](#S3.SS1.SSS1 "3.1.1\. Single-view single person 3D HPE ‣ 3.1\. 3D HPE
    from monocular RGB images and videos ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based
    Human Pose Estimation: A Survey")); single-view multi-person (§ [3.1.2](#S3.SS1.SSS2
    "3.1.2\. Single-view multi-person 3D HPE ‣ 3.1\. 3D HPE from monocular RGB images
    and videos ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based Human Pose Estimation:
    A Survey")); and multi-view methods (§ [3.1.3](#S3.SS1.SSS3 "3.1.3\. Multi-view
    3D HPE ‣ 3.1\. 3D HPE from monocular RGB images and videos ‣ 3\. 3D human pose
    estimation ‣ Deep Learning-Based Human Pose Estimation: A Survey")). Multi-view
    settings are deployed mainly for multi-person pose estimation. Hence, single-person
    or multi-person is not specified in this category.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 3D HPE 方法根据输入源类型分类：单目 RGB 图像和视频 (§ [3.1](#S3.SS1 "3.1\. 单目 RGB 图像和视频的 3D HPE
    ‣ 3\. 3D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述"))，或其他传感器（例如，惯性测量单元传感器，§ [3.2](#S3.SS2 "3.2\.
    其他来源的 3D HPE ‣ 3\. 3D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述")）。这些方法中的大多数使用单目 RGB 图像和视频，并且进一步分为单视角单人
    (§ [3.1.1](#S3.SS1.SSS1 "3.1.1\. 单视角单人 3D HPE ‣ 3.1\. 单目 RGB 图像和视频的 3D HPE ‣ 3\.
    3D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述"))；单视角多人 (§ [3.1.2](#S3.SS1.SSS2 "3.1.2\. 单视角多人 3D
    HPE ‣ 3.1\. 单目 RGB 图像和视频的 3D HPE ‣ 3\. 3D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述"))；和多视角方法
    (§ [3.1.3](#S3.SS1.SSS3 "3.1.3\. 多视角 3D HPE ‣ 3.1\. 单目 RGB 图像和视频的 3D HPE ‣ 3\.
    3D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述"))。多视角设置主要用于多人姿态估计。因此，在这一类别中未指定单人或多人。
- en: 'Next, depending on the 2D and 3D HPE pipelines, the datasets and evaluation
    metrics commonly used are summarized followed by a comparison of results of the
    promising methods (§ [4](#S4 "4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based
    Human Pose Estimation: A Survey")). In addition, various applications of HPE such
    as AR/VR are mentioned (§ [5](#S5 "5\. Applications ‣ Deep Learning-Based Human
    Pose Estimation: A Survey")). Finally, the paper ends by an thorough discussion
    of some promising directions for future research (§ [6](#S6 "6\. Conclusion and
    Future Directions ‣ Deep Learning-Based Human Pose Estimation: A Survey")).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，根据 2D 和 3D HPE 管道，总结了常用的数据集和评估指标，并对有前景的方法的结果进行比较（§ [4](#S4 "4\. Datasets
    and Evaluation Metrics ‣ Deep Learning-Based Human Pose Estimation: A Survey")）。此外，还提到了
    HPE 的各种应用，如 AR/VR（§ [5](#S5 "5\. Applications ‣ Deep Learning-Based Human Pose
    Estimation: A Survey")）。最后，本文通过对未来研究的一些有前途的方向进行深入讨论（§ [6](#S6 "6\. Conclusion
    and Future Directions ‣ Deep Learning-Based Human Pose Estimation: A Survey")）。'
- en: 2\. 2D human pose estimation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 2D 人体姿态估计
- en: 2D HPE methods estimate the 2D position or spatial location of human body keypoints
    from images or videos. Traditional 2D HPE methods adopt different hand-crafted
    feature extraction techniques for body parts, and these early works describe the
    human body as a stick figure to obtain global pose structures. Recently, deep
    learning-based approaches have achieved a major breakthrough in HPE by improving
    the results significantly. In the following, we review deep learning-based 2D
    HPE methods with respect to single-person and multi-person scenarios.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 2D HPE 方法从图像或视频中估计人体关键点的 2D 位置或空间位置。传统的 2D HPE 方法采用不同的手工特征提取技术来处理身体部位，这些早期的工作通过将人体描述为一个棍子图形来获得全局姿态结构。最近，基于深度学习的方法在
    HPE 中取得了重大突破，显著提高了结果。接下来，我们将回顾基于深度学习的 2D HPE 方法，涵盖单人和多人场景。
- en: 2.1\. 2D single-person pose estimation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 2D 单人姿态估计
- en: '2D single-person pose estimation is used to localize human body joint positions
    when the input is a single-person image. If there are several people, the input
    image is cropped first so that there is only one person in each cropped patch
    (or sub-image). This process can be achieved automatically by an upper-body detector
    (Micilotta et al., [2006](#bib.bib162)) or a full-body detector (Ren et al., [2015](#bib.bib205)).
    In general, there are two categories for single-person pipelines that employ deep
    learning techniques: regression methods and heatmap-based methods. Regression
    methods apply an end-to-end framework to learn a mapping from the input image
    to the positions of body joints or parameters of human body models (Toshev and
    Szegedy, [2014](#bib.bib234)). The goal of heatmap-based methods is to predict
    approximate locations of body parts and joints (Chen and Yuille, [2014](#bib.bib27))
    (Newell et al., [2016](#bib.bib172)), which are supervised by heatmaps representation
    (Tompson et al., [2015](#bib.bib232); Wei et al., [2016](#bib.bib255)). Heatmap-based
    frameworks are now widely used in 2D HPE tasks. The general frameworks of 2D single-person
    HPE methods are depicted in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1\. 2D single-person
    pose estimation ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based Human Pose
    Estimation: A Survey").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '2D 单人姿态估计用于定位单张图像中的人体关节点。如果图像中有多个人，则首先裁剪图像，使每个裁剪的图像块（或子图像）中只有一个人。这个过程可以通过上半身检测器（Micilotta
    et al., [2006](#bib.bib162)）或全身检测器（Ren et al., [2015](#bib.bib205)）自动完成。通常，使用深度学习技术的单人管道分为两类：回归方法和基于热图的方法。回归方法应用端到端框架来学习从输入图像到人体关节位置或人体模型参数的映射（Toshev
    and Szegedy, [2014](#bib.bib234)）。基于热图的方法的目标是预测身体部位和关节的大致位置（Chen and Yuille, [2014](#bib.bib27)）（Newell
    et al., [2016](#bib.bib172)），这些位置通过热图表示来进行监督（Tompson et al., [2015](#bib.bib232)；Wei
    et al., [2016](#bib.bib255)）。基于热图的框架现在在 2D HPE 任务中得到了广泛应用。2D 单人 HPE 方法的一般框架如图
    [2](#S2.F2 "Figure 2 ‣ 2.1\. 2D single-person pose estimation ‣ 2\. 2D human pose
    estimation ‣ Deep Learning-Based Human Pose Estimation: A Survey") 所示。'
- en: '![Refer to caption](img/9a055d5279952a2ee8cb3414891d6c32.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/9a055d5279952a2ee8cb3414891d6c32.png)'
- en: Figure 2\. Single-person 2D HPE frameworks. (a) Regression methods directly
    learn a mapping (via a deep neural network) from the original image to the kinematic
    body model and produce joint coordinates. (b) Given the ground-truth 2D pose,
    the ground-truth heatmaps of each joint are generated by applying a Gaussian kernel
    to each joint’s location. Then, heatmap-based methods utilize a model to predict
    the heatmap of each joint.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 单人 2D HPE 框架。（a）回归方法通过深度神经网络直接学习从原始图像到运动体模型的映射，并生成关节坐标。（b）给定真实的 2D 姿态，通过对每个关节的位置应用高斯核来生成每个关节的真实热图。然后，基于热图的方法利用模型预测每个关节的热图。
- en: 2.1.1\. Regression methods
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 回归方法
- en: 'There are many works based on the regression framework (e.g., (Toshev and Szegedy,
    [2014](#bib.bib234); Pfister et al., [2014](#bib.bib191); Carreira et al., [2016](#bib.bib18);
    Sun et al., [2017](#bib.bib223); Luvizon et al., [2019](#bib.bib149); Nibali et al.,
    [2018](#bib.bib173); Li et al., [2014](#bib.bib119); Fan et al., [2015](#bib.bib55);
    Luvizon et al., [2018](#bib.bib148); Zhang et al., [2019b](#bib.bib285); Li et al.,
    [2021c](#bib.bib117); Panteleris and Argyros, [2021](#bib.bib179); Mao et al.,
    [2021](#bib.bib154); Mao et al., [2022](#bib.bib155))) to predict joint coordinates
    from images as shown in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1\. 2D single-person pose
    estimation ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based Human Pose Estimation:
    A Survey") (a). Using AlexNet (Krizhevsky et al., [2012](#bib.bib106)) as the
    backbone, Toshev and Szegedy (Toshev and Szegedy, [2014](#bib.bib234)) proposed
    a cascaded deep neural network regressor named DeepPose to learn keypoints from
    images. Due to the impressive performance of DeepPose, the research paradigm of
    HPE began to shift from classic approaches to deep learning, in particular convolutional
    neural networks (CNNs). Sun et al. (Sun et al., [2017](#bib.bib223)) introduced
    a structure-aware regression method called ”compositional pose regression” based
    on ResNet-50 (He et al., [2016](#bib.bib67)). This method adopts a re-parameterized
    and bone-based representation that contains human body information and pose structure,
    instead of the traditional joint-based representation. Luvizon et al. (Luvizon
    et al., [2019](#bib.bib149)) proposed an end-to-end regression approach for HPE
    using soft-argmax function to convert feature maps into joint coordinates in a
    fully differentiable framework. Li et al. (Li et al., [2021c](#bib.bib117)) first
    designed a transformer-based cascade network for regressing human keypoints. The
    spatial correlation of joints and appearance is captured by self-attention mechanism.
    Different from the previous methods, Li et al. (Li et al., [2021a](#bib.bib113))
    proposed a normalizing flow model named RLE (Log-likelihood Estimation) to capture
    the distribution of joint location, aiming for finding the optimized parameters
    by residual log-likelihood estimation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '基于回归框架的研究有很多（例如，（Toshev 和 Szegedy，[2014](#bib.bib234)；Pfister 等，[2014](#bib.bib191)；Carreira
    等，[2016](#bib.bib18)；Sun 等，[2017](#bib.bib223)；Luvizon 等，[2019](#bib.bib149)；Nibali
    等，[2018](#bib.bib173)；Li 等，[2014](#bib.bib119)；Fan 等，[2015](#bib.bib55)；Luvizon
    等，[2018](#bib.bib148)；Zhang 等，[2019b](#bib.bib285)；Li 等，[2021c](#bib.bib117)；Panteleris
    和 Argyros，[2021](#bib.bib179)；Mao 等，[2021](#bib.bib154)；Mao 等，[2022](#bib.bib155)）用于从图像中预测关节坐标，如图[2](#S2.F2
    "Figure 2 ‣ 2.1\. 2D single-person pose estimation ‣ 2\. 2D human pose estimation
    ‣ Deep Learning-Based Human Pose Estimation: A Survey")（a）所示。Toshev 和 Szegedy（Toshev
    和 Szegedy，[2014](#bib.bib234)）使用 AlexNet（Krizhevsky 等，[2012](#bib.bib106)）作为骨干网络，提出了一种名为
    DeepPose 的级联深度神经网络回归器，用于从图像中学习关键点。由于 DeepPose 的出色表现，人类姿态估计的研究范式开始从经典方法转向深度学习，特别是卷积神经网络（CNN）。Sun
    等（Sun 等，[2017](#bib.bib223)）提出了一种结构感知回归方法，称为“组合姿态回归”，基于 ResNet-50（He 等，[2016](#bib.bib67)）。该方法采用了一种重新参数化和基于骨架的表示，包含了人体信息和姿态结构，而不是传统的基于关节的表示。Luvizon
    等（Luvizon 等，[2019](#bib.bib149)）提出了一种端到端的回归方法，用于 HPE，使用 soft-argmax 函数将特征图转换为关节坐标，在完全可微的框架中。Li
    等（Li 等，[2021c](#bib.bib117)）首次设计了一种基于变压器的级联网络来回归人体关键点。自注意力机制捕捉关节和外观的空间相关性。与之前的方法不同，Li
    等（Li 等，[2021a](#bib.bib113)）提出了一种名为 RLE（对数似然估计）的归一化流模型，以捕捉关节位置的分布，旨在通过残差对数似然估计找到优化的参数。'
- en: 'A good feature that encodes rich pose information is critical for regression-based
    methods. One popular strategy to learn better feature representation is multi-task
    learning (Ruder, [2017](#bib.bib210)). By sharing representations between related
    tasks (e.g., pose estimation and pose-based action recognition), the model can
    generalize better on the original task (pose estimation). Following this direction,
    Li et al. (Li et al., [2014](#bib.bib119)) proposed a heterogeneous multi-task
    framework that consists of two tasks: predicting joint coordinates from full images
    by a regressor and detecting body parts from image patches using a sliding window.
    Fan et al. (Fan et al., [2015](#bib.bib55)) proposed a dual-source (i.e., image
    patches and full images) CNN for two tasks: joint detection which determines whether
    a patch contains a body joint, and joint localization which finds the exact location
    of the joint in the patch. Each task corresponds to a loss function, and the combination
    of two tasks leads to improved results.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个能够编码丰富姿态信息的良好特征对于基于回归的方法至关重要。一种流行的学习更好特征表示的策略是多任务学习（Ruder，[2017](#bib.bib210)）。通过在相关任务（例如，姿态估计和基于姿态的动作识别）之间共享表示，模型可以在原始任务（姿态估计）上有更好的泛化能力。沿着这个方向，Li等人（Li
    et al., [2014](#bib.bib119)）提出了一个异构多任务框架，该框架由两个任务组成：通过回归器从完整图像中预测关节坐标，以及使用滑动窗口从图像块中检测身体部位。Fan等人（Fan
    et al., [2015](#bib.bib55)）提出了一种双源（即图像块和完整图像）CNN，用于两个任务：关节检测，即确定一个图像块是否包含身体关节，以及关节定位，即找到关节在图像块中的确切位置。每个任务对应一个损失函数，两个任务的结合可以提高结果。
- en: 2.1.2\. Heatmap-based methods
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 基于热图的方法
- en: 'Instead of estimating the 2D coordinates of human joints directly, heatmap-based
    methods for HPE aim to estimate the 2D heatmaps which are generated by adding
    2D Gaussian kernels on each joint’s location as shown in Fig. [2](#S2.F2 "Figure
    2 ‣ 2.1\. 2D single-person pose estimation ‣ 2\. 2D human pose estimation ‣ Deep
    Learning-Based Human Pose Estimation: A Survey"). Concretely, the goal is to estimate
    $K$ heatmaps $\{H_{1},H_{2},...,H_{K}\}$ for a total of $K$ keypoints. The pixel
    value $H_{i}(x,y)$ in each keypoint heatmap indicates the probability that the
    keypoint lies in the position $(x,y)$ (see Fig. [2](#S2.F2 "Figure 2 ‣ 2.1\. 2D
    single-person pose estimation ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based
    Human Pose Estimation: A Survey") (b)). The target (or ground-truth) heatmap is
    generated by a 2D Gaussian centered at the ground-truth joint location (Tompson
    et al., [2015](#bib.bib232))(Tompson et al., [2014](#bib.bib233)). Thus pose estimation
    networks are trained by minimizing the discrepancy (e.g., the Mean Squared-Error
    (MSE)) between the predicted heatmaps and target heatmaps. Compared with joint
    coordinates, heatmaps preserve the spatial location information while it can make
    the training process smoother.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '与其直接估计人体关节的2D坐标，不如采用基于热图的方法来进行HPE，这些方法旨在估计2D热图，这些热图是通过在每个关节的位置上添加2D高斯核生成的，如图[2](#S2.F2
    "Figure 2 ‣ 2.1\. 2D single-person pose estimation ‣ 2\. 2D human pose estimation
    ‣ Deep Learning-Based Human Pose Estimation: A Survey")所示。具体来说，目标是估计总共$K$个关键点的$K$个热图$\{H_{1},H_{2},...,H_{K}\}$。每个关键点热图中的像素值$H_{i}(x,y)$表示该关键点位于位置$(x,y)$的概率（见图[2](#S2.F2
    "Figure 2 ‣ 2.1\. 2D single-person pose estimation ‣ 2\. 2D human pose estimation
    ‣ Deep Learning-Based Human Pose Estimation: A Survey") (b)）。目标（或真实）热图是由一个以真实关节位置为中心的2D高斯生成的（Tompson
    et al., [2015](#bib.bib232)）（Tompson et al., [2014](#bib.bib233)）。因此，姿态估计网络通过最小化预测热图与目标热图之间的差异（例如，均方误差（MSE））来进行训练。与关节坐标相比，热图保留了空间位置信息，同时可以使训练过程更加平滑。'
- en: Therefore, there is a recent growing interest in leveraging heatmaps to represent
    the joint locations and developing effective CNN architectures for HPE, e.g.,
    (Tompson et al., [2014](#bib.bib233); Ramakrishna et al., [2014](#bib.bib201);
    Tompson et al., [2015](#bib.bib232); Lifshitz et al., [2016](#bib.bib130); Bulat
    and Tzimiropoulos, [2016](#bib.bib12); Newell et al., [2016](#bib.bib172); Wei
    et al., [2016](#bib.bib255); Gkioxari et al., [2016](#bib.bib60); Belagiannis
    and Zisserman, [2017](#bib.bib8); Yang et al., [2017](#bib.bib270); Luo et al.,
    [2018](#bib.bib146); Debnath et al., [2018](#bib.bib47); Xiao et al., [2018](#bib.bib260);
    Zhang et al., [2019a](#bib.bib286); Artacho and Savakis, [2020](#bib.bib7); Li
    et al., [2022b](#bib.bib125)). As one of the fundamental works, Wei et al. (Wei
    et al., [2016](#bib.bib255)) introduced a convolutional networks-based sequential
    framework named Convolutional Pose Machines (CPM) to predict the locations of
    keypoints with multi-stage processing (the convolutional networks in each stage
    utilize the 2D belief maps generated from previous stages and produce the increasingly
    refined predictions of body part locations). At the same time, Newell et al. (Newell
    et al., [2016](#bib.bib172)) proposed an encoder-decoder network named ”stacked
    hourglass” to repeat bottom-up and top-down processing with intermediate supervision.
    In this work, the encoder squeezes features through the bottleneck and then the
    decoder expands them for the substage. The stacked hourglass (SHG) network consists
    of consecutive steps of pooling and upsampling layers to capture information at
    every scale. Since then, complex variations of the SHG architecture were developed
    for HPE. Following (Newell et al., [2016](#bib.bib172)), Chu et al. (Chu et al.,
    [2017](#bib.bib42)) designed novel Hourglass Residual Units (HRUs), which extend
    the residual units with a side branch of filters with larger receptive fields,
    to capture features from various scales. Yang et al. (Yang et al., [2017](#bib.bib270))
    designed a multi-branch Pyramid Residual Module (PRM) to replace the residual
    unit in SHG, leading to enhanced invariance in scales of deep CNNs. Sun et al.
    (Sun et al., [2019](#bib.bib222)) presented a novel High-Resolution Net (HRNet)
    to learn reliable high-resolution representations by connecting multi-resolution
    subnetworks in parallel and conducting repeated multi-scale fusions, which results
    in more accurate keypoint heatmap prediction. Inspired by HRNet, Yu et al. (Yu
    et al., [2021](#bib.bib275)) introduced a light-weighted HRNet named Lite-HRNet,
    which designed conditional channel weighting blocks to exchange information between
    channels and resolutions. Recently, due to the superior performance, the HRNet
    (Sun et al., [2019](#bib.bib222)) and its variations (Cheng et al., [2020](#bib.bib32);
    Yuan et al., [2021](#bib.bib278); Yu et al., [2021](#bib.bib275)) have been widely
    adopted in HPE and other pose-related tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最近对利用热图表示关节位置以及开发有效的CNN架构用于人体姿态估计（HPE）表现出越来越大的兴趣，例如（Tompson et al., [2014](#bib.bib233);
    Ramakrishna et al., [2014](#bib.bib201); Tompson et al., [2015](#bib.bib232);
    Lifshitz et al., [2016](#bib.bib130); Bulat and Tzimiropoulos, [2016](#bib.bib12);
    Newell et al., [2016](#bib.bib172); Wei et al., [2016](#bib.bib255); Gkioxari
    et al., [2016](#bib.bib60); Belagiannis and Zisserman, [2017](#bib.bib8); Yang
    et al., [2017](#bib.bib270); Luo et al., [2018](#bib.bib146); Debnath et al.,
    [2018](#bib.bib47); Xiao et al., [2018](#bib.bib260); Zhang et al., [2019a](#bib.bib286);
    Artacho and Savakis, [2020](#bib.bib7); Li et al., [2022b](#bib.bib125))。作为基础工作之一，Wei
    et al. (Wei et al., [2016](#bib.bib255)) 提出了基于卷积网络的顺序框架，称为卷积姿态机器（CPM），用于预测关键点的位置，通过多阶段处理（每个阶段的卷积网络利用前一阶段生成的2D信念图，并产生越来越精细的身体部位位置预测）。与此同时，Newell
    et al. (Newell et al., [2016](#bib.bib172)) 提出了一个名为“堆叠沙漏”（stacked hourglass）的编码器-解码器网络，通过中间监督重复底向上和顶向下处理。在这项工作中，编码器通过瓶颈压缩特征，然后解码器将其扩展到子阶段。堆叠沙漏（SHG）网络由连续的池化和上采样层步骤组成，以捕捉每个尺度的信息。此后，针对HPE开发了SHG架构的复杂变体。根据
    (Newell et al., [2016](#bib.bib172))，Chu et al. (Chu et al., [2017](#bib.bib42))
    设计了新颖的沙漏残差单元（HRUs），通过带有更大感受野的侧支滤波器扩展了残差单元，以捕捉来自不同尺度的特征。Yang et al. (Yang et al.,
    [2017](#bib.bib270)) 设计了多分支金字塔残差模块（PRM）来替代SHG中的残差单元，从而增强了深度CNN在尺度上的不变性。Sun et
    al. (Sun et al., [2019](#bib.bib222)) 提出了新颖的高分辨率网络（HRNet），通过将多分辨率子网络并行连接并进行重复的多尺度融合，从而学习可靠的高分辨率表示，这使得关键点热图预测更加准确。受到HRNet的启发，Yu
    et al. (Yu et al., [2021](#bib.bib275)) 介绍了一种轻量级的HRNet，名为Lite-HRNet，设计了条件通道加权块，以在通道和分辨率之间交换信息。近期，由于卓越的性能，HRNet
    (Sun et al., [2019](#bib.bib222)) 及其变体（Cheng et al., [2020](#bib.bib32); Yuan
    et al., [2021](#bib.bib278); Yu et al., [2021](#bib.bib275)) 已被广泛应用于HPE和其他姿态相关任务。
- en: With the emergence of Generative Adversarial Networks (GANs) (Goodfellow et al.,
    [2014](#bib.bib62)), they are explored in HPE to generate biologically plausible
    pose configurations and to discriminate the predictions with high confidence from
    those with low confidence, which could infer the poses of the occluded body parts.
    Inspired by hourglass architecture which efficiently refines joints, Chen et al.
    (Chen et al., [2017](#bib.bib28)) constructed a structure-aware conditional adversarial
    network–Adversarial PoseNet–which contains an hourglass network-based pose generator
    and two discriminators to discriminate reasonable body poses from unreasonable
    ones. Chou et al. (Chou et al., [2018](#bib.bib40)) built an adversarial learning-based
    network with two stacked hourglass networks sharing the same structure as the
    discriminator and generator, respectively. The generator estimates the location
    of each joint, and the discriminator distinguishes between the ground-truth heatmaps
    and predicted ones. Unlike GANs-based methods that take the HPE network as the
    generator and utilize the discriminator to provide supervision, Peng et al. (Peng
    et al., [2018](#bib.bib189)) developed an adversarial data augmentation network
    to optimize data augmentation and network training by treating the HPE network
    as a discriminator and using augmentation network as a generator to perform adversarial
    augmentations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成对抗网络（GANs）的出现（Goodfellow et al., [2014](#bib.bib62)），它们在HPE中被用于生成生物学上合理的姿态配置，并用高置信度的预测与低置信度的预测进行区分，从而推断被遮挡身体部位的姿态。受到有效细化关节的hourglass架构的启发，Chen
    et al.（Chen et al., [2017](#bib.bib28)）构建了一个结构感知的条件对抗网络——对抗姿态网络（Adversarial PoseNet），它包含一个基于hourglass网络的姿态生成器和两个判别器，用于区分合理的身体姿态与不合理的姿态。Chou
    et al.（Chou et al., [2018](#bib.bib40)）构建了一个基于对抗学习的网络，该网络有两个堆叠的hourglass网络，分别共享与判别器和生成器相同的结构。生成器估计每个关节的位置，而判别器区分真实热图与预测热图。与以GANs为基础的方法不同，这些方法将HPE网络视为生成器，并利用判别器提供监督，Peng
    et al.（Peng et al., [2018](#bib.bib189)）开发了一个对抗数据增强网络，通过将HPE网络视为判别器，并使用增强网络作为生成器来执行对抗性增强，从而优化数据增强和网络训练。
- en: Besides these efforts in designing effective networks for HPE, body structure
    information is also investigated to provide more and better supervision information
    for building HPE networks. Yang et al. (Yang et al., [2016](#bib.bib271)) designed
    an end-to-end CNN framework for HPE, which can find hard negatives by incorporating
    the spatial and appearance consistency among human body parts. A structured feature-level
    learning framework was proposed in (Chu et al., [2016](#bib.bib41)) for reasoning
    the correlations among human body joints in HPE, which captures richer information
    of human body joints and improves the learning results. Ke et al. (Ke et al.,
    [2018](#bib.bib97)) designed a multi-scale structure-aware neural network, which
    combines multi-scale supervision, multi-scale feature combination, structure-aware
    loss information scheme, and a keypoint masking training method to improve HPE
    models in complex scenarios. Tang et al. (Tang et al., [2018](#bib.bib225)) built
    an hourglass-based supervision network, termed as Deeply Learned Compositional
    Model, to describe the complex and realistic relationships among body parts and
    learn the compositional pattern information (the orientation, scale, and shape
    information of each body part) in human bodies. Different from the previous approaches
    which consider all body parts, Tang and Wu (Tang and Wu, [2019](#bib.bib224))
    revealed that not all parts are related to each other, therefore introducing a
    Part-based Branches Network to learn representations specific to each part group
    rather than a shared representation for all parts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设计有效的 HPE 网络的努力外，体结构信息也被研究，以提供更多更好的监督信息来构建 HPE 网络。Yang 等人（Yang et al., [2016](#bib.bib271)）设计了一个端到端
    CNN 框架用于 HPE，该框架通过结合人体部位之间的空间和外观一致性来发现困难的负例。在（Chu et al., [2016](#bib.bib41)）中提出了一个结构化特征级学习框架，用于推理
    HPE 中人体关节之间的相关性，这捕捉了更丰富的人体关节信息并改善了学习结果。Ke 等人（Ke et al., [2018](#bib.bib97)）设计了一个多尺度结构感知神经网络，结合了多尺度监督、多尺度特征组合、结构感知损失信息方案以及关键点掩膜训练方法，以改进复杂场景中的
    HPE 模型。Tang 等人（Tang et al., [2018](#bib.bib225)）建立了一个基于沙漏的监督网络，称为**深度学习组合模型**，用于描述身体部位之间复杂而真实的关系，并学习人体中的组合模式信息（每个身体部位的方向、尺度和形状信息）。与以前考虑所有身体部位的方法不同，Tang
    和 Wu（Tang and Wu, [2019](#bib.bib224)）揭示了并非所有部位都相互关联，因此引入了基于部件的分支网络，以学习特定于每个部件组的表示，而不是所有部位共享的表示。
- en: Human poses in video sequences are (3D) spatio-temporal signals. Therefore,
    modeling spatio-temporal information is important for HPE from videos. Jain et
    al. (Jain et al., [2014](#bib.bib82)) designed a two-branch CNN framework to incorporate
    both color and motion features within frame pairs to build an expressive temporal-spatial
    model in HPE. Pfister et al. (Pfister et al., [2015](#bib.bib190)) proposed a
    CNN that can utilize temporal context information from multiple frames by using
    optical flow to align predicted heatmaps from neighboring frames. Different from
    the previous video-based methods which are computationally intensive, Luo et al.
    (Luo et al., [2018](#bib.bib146)) introduced a recurrent structure with Long Short-Term
    Memory to capture temporal geometric consistency and dependency from different
    frames. This method results in faster training time for the HPE network for videos.
    Zhang et al. (Zhang et al., [2020e](#bib.bib294)) introduced a keyframe proposal
    network for capturing spatial and temporal information from frames, and a human
    pose interpolation module for efficient video-based HPE.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 视频序列中的人体姿势是（3D）时空信号。因此，建模时空信息对于从视频中进行**人体姿势估计（HPE）**至关重要。Jain 等人（Jain et al.,
    [2014](#bib.bib82)）设计了一个双分支 CNN 框架，将颜色和运动特征结合在帧对中，以建立一个具有表现力的时空模型用于 HPE。Pfister
    等人（Pfister et al., [2015](#bib.bib190)）提出了一个 CNN，利用光流对齐来自相邻帧的预测热图，从多个帧中利用时间上下文信息。与以前计算密集的视频方法不同，Luo
    等人（Luo et al., [2018](#bib.bib146)）引入了一个带有长短期记忆的递归结构，以捕捉来自不同帧的时间几何一致性和依赖性。这种方法导致了视频
    HPE 网络的更快训练时间。Zhang 等人（Zhang et al., [2020e](#bib.bib294)）引入了一个关键帧提议网络，用于捕捉帧中的时空信息，并且有一个**人体姿势插值模块**以实现高效的视频
    HPE。
- en: 2.2\. 2D multi-person pose estimation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 2D 多人姿势估计
- en: 'Compared to single-person HPE, multi-person HPE is more difficult and challenging
    because it needs to figure out the number of people and their positions, and how
    to group keypoints for different people. In order to solve these problems, multi-person
    HPE methods can be classified into top-down and bottom-up methods. Top-down methods
    employ off-the-shelf person detectors to obtain a set of boxes (each corresponding
    to one person) from the input images and then apply single-person pose estimators
    to each person box to generate multi-person poses. Different from top-down methods,
    bottom-up methods locate all the body joints in one image first and then group
    them into individual subjects. In the top-down pipeline, the number of people
    in the input image will directly affect the computing time. The computing speed
    for bottom-up methods is usually faster than top-down methods since they do not
    need to detect the pose for each person separately. Fig. [3](#S2.F3 "Figure 3
    ‣ 2.2\. 2D multi-person pose estimation ‣ 2\. 2D human pose estimation ‣ Deep
    Learning-Based Human Pose Estimation: A Survey") shows the general frameworks
    for 2D multi-person HPE methods.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '与单人姿态估计相比，多人姿态估计更加困难和具有挑战性，因为它需要确定人数及其位置，并且如何将关键点归类到不同的人身上。为了应对这些问题，多人姿态估计方法可以分为自上而下和自下而上两种方法。自上而下的方法使用现成的人体检测器从输入图像中获取一组框（每个框对应一个人），然后对每个框应用单人姿态估计器来生成多人姿态。与自上而下的方法不同，自下而上的方法首先在一张图像中定位所有身体关节，然后将它们分组到各个个体中。在自上而下的管道中，输入图像中的人数将直接影响计算时间。自下而上的方法的计算速度通常比自上而下的方法更快，因为它们不需要单独检测每个人的姿态。图[3](#S2.F3
    "Figure 3 ‣ 2.2\. 2D multi-person pose estimation ‣ 2\. 2D human pose estimation
    ‣ Deep Learning-Based Human Pose Estimation: A Survey")展示了2D多人姿态估计方法的一般框架。'
- en: '![Refer to caption](img/40c5231cf697a0d08279f4a55a50c7fd.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/40c5231cf697a0d08279f4a55a50c7fd.png)'
- en: 'Figure 3\. Illustration of the multi-person 2D HPE frameworks. (a) Top-down
    approaches have two sub-tasks: (1) human detection and (2) pose estimation in
    the region of a single human; (b) Bottom-up approaches also have two sub-tasks:
    (1) detect all keypoints candidates of body parts and (2) associate body parts
    in different human bodies and assemble them into individual pose representations.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 多人2D人体姿态估计框架的示意图。(a) 自上而下的方法包含两个子任务：(1) 人体检测和(2) 在单个人体区域中的姿态估计；(b) 自下而上的方法也有两个子任务：(1)
    检测所有身体部位的关键点候选点和(2) 将不同人体的身体部位关联起来，并将它们组装成各自的姿态表示。
- en: 2.2.1\. Top-down pipeline
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 自上而下的管道
- en: 'In the top-down pipeline as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. 2D multi-person
    pose estimation ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based Human Pose
    Estimation: A Survey") (a), there are two important parts: a human body detector
    to obtain person bounding boxes and a single-person pose estimator to predict
    the locations of keypoints within these bounding boxes. A line of works focus
    on designing and improving the modules in HPE networks, e.g., (Papandreou et al.,
    [2017](#bib.bib181); Huang et al., [2017](#bib.bib73); Xiao et al., [2018](#bib.bib260);
    Sun et al., [2019](#bib.bib222); Li et al., [2019a](#bib.bib122); Moon et al.,
    [2019b](#bib.bib167); Wang et al., [2020b](#bib.bib247); Huang et al., [2020d](#bib.bib72);
    Cai et al., [2020](#bib.bib15); Zhang et al., [2020h](#bib.bib284); Liu et al.,
    [2021b](#bib.bib135)). To answer the question ”how good could a simple method
    be” in building an HPE network, Xiao et al. (Xiao et al., [2018](#bib.bib260))
    added a few deconvolutional layers in the ResNet (backbone network) to build a
    simple yet effective structure to produce heatmaps for high-resolution representations.
    To improve the accuracy of keypoint localization, Wang et al. (Wang et al., [2020b](#bib.bib247))
    introduced a two-stage graph-based and model-agnostic framework, called Graph-PCNN.
    It consists of a localization subnet to obtain rough keypoint locations and a
    graph pose refinement module to get refined keypoints localization representations.
    Cai et al. (Cai et al., [2020](#bib.bib15)) introduced a multi-stage network with
    a Residual Steps Network (RSN) module to learn delicate local representations
    by efficient intra-level feature fusion strategies, and a Pose Refine Machine
    (PRM) module to find a trade-off between local and global representations in the
    features.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '在如图[3](#S2.F3 "Figure 3 ‣ 2.2\. 2D multi-person pose estimation ‣ 2\. 2D human
    pose estimation ‣ Deep Learning-Based Human Pose Estimation: A Survey")（a）所示的自顶向下管道中，有两个重要部分：一个人体检测器用于获取人物边界框，以及一个单人姿态估计器用于预测这些边界框内关键点的位置。一系列工作集中于设计和改进HPE网络中的模块，例如（Papandreou等，[2017](#bib.bib181)；Huang等，[2017](#bib.bib73)；Xiao等，[2018](#bib.bib260)；Sun等，[2019](#bib.bib222)；Li等，[2019a](#bib.bib122)；Moon等，[2019b](#bib.bib167)；Wang等，[2020b](#bib.bib247)；Huang等，[2020d](#bib.bib72)；Cai等，[2020](#bib.bib15)；Zhang等，[2020h](#bib.bib284)；Liu等，[2021b](#bib.bib135)）。为了回答“简单方法能有多好”以构建HPE网络的问题，Xiao等（Xiao等，[2018](#bib.bib260)）在ResNet（骨干网络）中添加了一些反卷积层，建立了一个简单而有效的结构来生成高分辨率表示的热图。为了提高关键点定位的准确性，Wang等（Wang等，[2020b](#bib.bib247)）引入了一个两阶段的图基和模型无关的框架，称为Graph-PCNN。它包括一个定位子网以获得粗略的关键点位置和一个图姿态细化模块以获得细化的关键点定位表示。Cai等（Cai等，[2020](#bib.bib15)）引入了一个多阶段网络，具有Residual
    Steps Network（RSN）模块，通过高效的层内特征融合策略学习精细的局部表示，以及一个Pose Refine Machine（PRM）模块，以在特征中的局部和全局表示之间找到权衡。'
- en: 'Estimating poses under occlusion and truncation scenes often occurs in multi-person
    settings since the overlapping of limbs is inevitable. Human detectors may fail
    in the first step of top-down pipeline due to occlusion. Thus, robustness to occlusion
    or truncation is an important aspect of the multi-person HPE approaches. Towards
    this goal, Iqbal and Gall (Iqbal and Gall, [2016](#bib.bib79)) built a convolutional
    pose machine-based pose estimator to estimate the joint candidates. Then they
    used integer linear programming to solve the joint-to-person association problem
    and obtain human body poses even in presence of severe occlusions. Fang et al.
    (Fang et al., [2017](#bib.bib56)) designed a regional multi-person pose estimation
    (RMPE) approach to improve the performance of HPE in complex scenes. The RMPE
    framework has three parts: Symmetric Spatial Transformer Network (to detect single
    person region within an inaccurate bounding box), Parametric Pose Non-Maximum-Suppression
    (to solve the redundant detection problem), and Pose-Guided Proposals Generator
    (to augment training data). Papandreou et al. (Papandreou et al., [2017](#bib.bib181))
    proposed a two-stage architecture with a Faster R-CNN person detector to create
    bounding boxes for candidate human bodies and a keypoint estimator to predict
    the locations of keypoints by using a form of heatmap-offset aggregation. The
    method works well in occluded and cluttered scenes. To alleviate the occlusion
    problem in HPE, Chen et al. (Chen et al., [2018b](#bib.bib30)) presented a Cascade
    Pyramid Network (CPN) which includes two parts: GlobalNet (a feature pyramid network
    to predict the invisible keypoints) and RefineNet (a network to integrate all
    levels of features from the GlobalNet with a keypoint mining loss). Their results
    reveal that CPN has a good performance in predicting occluded keypoints. Su et
    al. (Su et al., [2019](#bib.bib220)) designed two modules, the Channel Shuffle
    Module and the Spatial & Channel-wise Attention Residual Bottleneck, to achieve
    channel-wise and spatial information enhancement for better multi-person HPE under
    occluded scenes. Qiu et al. (Qiu et al., [2020](#bib.bib200)) developed an Occluded
    Pose Estimation and Correction module and an occluded pose dataset to solve the
    occlusion problem in crowd pose estimation. Umer et al. (Umer et al., [2020](#bib.bib238))
    proposed a keypoint correspondence framework to recover missed poses using temporal
    information of the previous frame in occluded scenes. The network is trained using
    self-supervision to improve the pose estimation results on sparsely annotated
    video datasets.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在遮挡和截断场景下的姿态估计经常发生在多人设置中，因为肢体重叠是不可避免的。由于遮挡，人类检测器可能在自上而下的流程的第一步中失败。因此，对遮挡或截断的鲁棒性是多人人体姿态估计（HPE）方法中的一个重要方面。为此，Iqbal
    和 Gall（Iqbal 和 Gall，[2016](#bib.bib79)）建立了一个基于卷积姿态机的姿态估计器来估计关节候选点。然后，他们使用整数线性规划解决了关节与个体的关联问题，并在严重遮挡的情况下获得了人体姿态。Fang
    等人（Fang 等人，[2017](#bib.bib56)）设计了一种区域多人姿态估计（RMPE）方法，以提高复杂场景下 HPE 的性能。RMPE 框架分为三部分：对称空间变换网络（用于检测不准确边界框内的单人区域）、参数化姿态非极大值抑制（用于解决冗余检测问题）和姿态引导提议生成器（用于增强训练数据）。Papandreou
    等人（Papandreou 等人，[2017](#bib.bib181)）提出了一种两阶段架构，结合了 Faster R-CNN 人体检测器用于创建候选人体的边界框，以及关键点估计器通过热图偏移聚合预测关键点位置。该方法在遮挡和杂乱场景中表现良好。为缓解
    HPE 中的遮挡问题，Chen 等人（Chen 等人，[2018b](#bib.bib30)）提出了一个级联金字塔网络（CPN），包括两个部分：GlobalNet（一个特征金字塔网络，用于预测不可见的关键点）和
    RefineNet（一个网络，用于整合 GlobalNet 的所有层级特征，并通过关键点挖掘损失进行优化）。他们的结果表明，CPN 在预测遮挡关键点方面表现良好。Su
    等人（Su 等人，[2019](#bib.bib220)）设计了两个模块，即通道洗牌模块和空间及通道注意残差瓶颈，以实现通道级和空间信息增强，从而在遮挡场景下更好地进行多人
    HPE。Qiu 等人（Qiu 等人，[2020](#bib.bib200)）开发了一个遮挡姿态估计和修正模块以及一个遮挡姿态数据集，以解决人群姿态估计中的遮挡问题。Umer
    等人（Umer 等人，[2020](#bib.bib238)）提出了一个关键点对应框架，以利用上一帧的时间信息恢复错过的姿态。该网络通过自监督训练，以改善在稀疏标注视频数据集上的姿态估计结果。
- en: Recently, transformer-based methods have attracted more attention (Li et al.,
    [2021b](#bib.bib123), [e](#bib.bib126); Yang et al., [2021](#bib.bib269); Yuan
    et al., [2021](#bib.bib278); Shi et al., [2022](#bib.bib217); Ma et al., [2022](#bib.bib151))
    since the attention modules in transformer can obtain long-range dependencies
    and global evidence of the predicted keypoints, which are more powerful than CNNs.
    The early exploration (Yang et al., [2021](#bib.bib269)) proposed a transformer-based
    model for 2D HPE named TransPose, which utilizes the attention layers to predict
    the heatmaps of the keypoints and learn the fine-grained evidence for HPE in occlusion
    scenarios. Following (Yang et al., [2021](#bib.bib269)), Li et al. (Li et al.,
    [2021e](#bib.bib126)) built a pure transformer-based model named TokenPose to
    capture the constraint cues and visual appearance relationships by using token
    representation. In contrast to the methods based on the vision transformer which
    learn the representations in low resolution, Yuan et al. (Yuan et al., [2021](#bib.bib278))
    presented a high-resolution transformer named HRFormer by exchanging the blocks
    in HRNet (Cheng et al., [2020](#bib.bib32)) with transformer modules, which improves
    the memory and computing efficiency. Ma et al. (Ma et al., [2022](#bib.bib151))
    applied the token-Pruned Pose Transformer (PPT) for locating the areas of the
    human body, which enables the model to estimate the multi-view pose efficiently.
    Different from the traditional two-step structures in HPE, Shi et al. (Shi et al.,
    [2022](#bib.bib217)) proposed a fully end-to-end framework based on the attention
    mechanism, which directly estimates the instance-aware body poses.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于变换器的方法吸引了更多关注（Li et al., [2021b](#bib.bib123), [e](#bib.bib126); Yang et
    al., [2021](#bib.bib269); Yuan et al., [2021](#bib.bib278); Shi et al., [2022](#bib.bib217);
    Ma et al., [2022](#bib.bib151)），因为变换器中的注意力模块能够获得长距离依赖和全局证据，这些比CNN更强大。早期的探索（Yang
    et al., [2021](#bib.bib269)）提出了一种名为TransPose的2D HPE变换器模型，它利用注意力层来预测关键点的热图，并学习遮挡场景下HPE的细粒度证据。继（Yang
    et al., [2021](#bib.bib269)）之后，Li et al.（Li et al., [2021e](#bib.bib126)）构建了一个名为TokenPose的纯变换器模型，通过使用令牌表示来捕获约束线索和视觉外观关系。与基于视觉变换器的方法学习低分辨率表示不同，Yuan
    et al.（Yuan et al., [2021](#bib.bib278)）提出了一种名为HRFormer的高分辨率变换器，通过将HRNet（Cheng
    et al., [2020](#bib.bib32)）中的块替换为变换器模块，从而提高了内存和计算效率。Ma et al.（Ma et al., [2022](#bib.bib151)）应用了令牌修剪姿态变换器（PPT）来定位人体部位，从而使模型能够高效地估计多视角姿态。不同于传统的两步结构HPE，Shi
    et al.（Shi et al., [2022](#bib.bib217)）提出了一种基于注意力机制的完全端到端框架，直接估计实例感知的身体姿态。
- en: Besides the image-based works introduced above, multi-frame pose estimation
    in videos is also popular in multi-person 2D HPE (Guo et al., [2018](#bib.bib64);
    Bertasius et al., [2019](#bib.bib10); Liu et al., [2021a](#bib.bib140); Xu et al.,
    [2021](#bib.bib265); Liu et al., [2022](#bib.bib141)), which leverages the temporal
    information in video sequences to facilitate the pose estimation. In order to
    reduce the cost of labeling frames in the video, Bertasius et al.(Bertasius et al.,
    [2019](#bib.bib10)) proposed a network named PoseWarper, which improves the label
    propagation between frames and benefits the training with the sparse annotations.
    To alleviate the motion blur and pose occlusions among video frames, Liu et al.
    (Liu et al., [2021a](#bib.bib140)) designed a network named DCpose for multi-frame
    HPE, which contains three modules (Pose Temporal Merger, Pose Residual Fusion,
    and Pose Correction Network) to exploit the temporal information between frames
    for keypoint detection. Nevertheless, these methods failed to fully utilize the
    information from neighboring frames. To solve this issue, Liu et al. (Liu et al.,
    [2022](#bib.bib141)) introduced a hierarchical alignment framework for alleviating
    the aggregation of unaligned contexts between two frames.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述基于图像的工作，多帧视频中的姿态估计在多人人体2D HPE中也很受欢迎（Guo et al., [2018](#bib.bib64); Bertasius
    et al., [2019](#bib.bib10); Liu et al., [2021a](#bib.bib140); Xu et al., [2021](#bib.bib265);
    Liu et al., [2022](#bib.bib141)），它利用视频序列中的时间信息来促进姿态估计。为了减少标记视频帧的成本，Bertasius et
    al.（Bertasius et al., [2019](#bib.bib10)）提出了一个名为PoseWarper的网络，改善了帧间的标签传播，并使稀疏注释的训练受益。为了解决视频帧中的运动模糊和姿态遮挡问题，Liu
    et al.（Liu et al., [2021a](#bib.bib140)）设计了一个名为DCpose的网络，用于多帧HPE，该网络包含三个模块（Pose
    Temporal Merger、Pose Residual Fusion和Pose Correction Network），以利用帧间的时间信息进行关键点检测。然而，这些方法未能充分利用邻近帧的信息。为了解决这一问题，Liu
    et al.（Liu et al., [2022](#bib.bib141)）引入了一个分层对齐框架，以缓解两帧之间未对齐上下文的聚合。
- en: 2.2.2\. Bottom-up pipeline
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 自下而上的流程
- en: 'The bottom-up pipeline (e.g., (Insafutdinov et al., [2017](#bib.bib76); Cao
    et al., [2017](#bib.bib17); Newell et al., [2017](#bib.bib171); Fieraru et al.,
    [2018](#bib.bib57); Tian et al., [2019](#bib.bib229); Kreiss et al., [2019](#bib.bib105);
    Nie et al., [2019](#bib.bib176); Jin et al., [2020a](#bib.bib89); Cheng et al.,
    [2020](#bib.bib32); Wang et al., [2022a](#bib.bib253), [c](#bib.bib245))) has
    two main steps including body joint detection (i.e., extracting local features
    and predicting body joint candidates) and joint candidates assembling for individual
    bodies (i.e., grouping joint candidates to build pose representations with part
    association strategies) as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2\. 2D multi-person
    pose estimation ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based Human Pose
    Estimation: A Survey") (b).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 自下而上的流程（例如，（Insafutdinov 等， [2017](#bib.bib76)；Cao 等， [2017](#bib.bib17)；Newell
    等， [2017](#bib.bib171)；Fieraru 等， [2018](#bib.bib57)；Tian 等， [2019](#bib.bib229)；Kreiss
    等， [2019](#bib.bib105)；Nie 等， [2019](#bib.bib176)；Jin 等， [2020a](#bib.bib89)；Cheng
    等， [2020](#bib.bib32)；Wang 等， [2022a](#bib.bib253)， [c](#bib.bib245))) 包括两个主要步骤：身体关节检测（即提取局部特征和预测身体关节候选）和个体身体的关节候选组装（即将关节候选分组以使用部分关联策略构建姿态表示），如图
    [3](#S2.F3 "Figure 3 ‣ 2.2\. 2D 多人姿态估计 ‣ 2\. 2D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述")（b）所示。
- en: Pishchulin et al. (Pishchulin et al., [2016](#bib.bib193)) proposed a Fast R-CNN-based
    body part detector named DeepCut, which is one of the earliest two-stage bottom-up
    approaches. It first detects all the body part candidates, then labels each part
    and assembles these parts using integer linear programming (ILP) to a final pose.
    However, DeepCut is computationally expensive. To this end, Insafutdinov et al.(Insafutdinov
    et al., [2016](#bib.bib77)) introduced DeeperCut to improve DeepCut by applying
    a stronger body part detector with a better incremental optimization strategy
    and image-conditioned pairwise terms to group body parts, leading to improved
    performance as well as a faster speed. Later, Cao et al. (Cao et al., [2017](#bib.bib17))
    built a detector named OpenPose, which uses Convolutional Pose Machines (Wei et al.,
    [2016](#bib.bib255)) to predict keypoint coordinates via heatmaps and Part Affinity
    Fields (PAFs, a set of 2D vector fields with vector maps that encode the position
    and orientation of limbs) to associate the keypoints to each person. OpenPose
    largely accelerates the speed of bottom-up multi-person HPE. Based on the OpenPose
    framework, Zhu et al. (Zhu et al., [2017](#bib.bib316)) improved the OpenPose
    structure by adding redundant edges to increase the connections between joints
    in PAFs and obtained better performance than the baseline approach. Although OpenPose-based
    methods have achieved impressive results on high-resolution images, they have
    poor performance with low-resolution images and occlusions. To address this problem,
    Kreiss et al. (Kreiss et al., [2019](#bib.bib105)) proposed a bottom-up method
    called PifPaf that uses a Part Intensity Field to predict the locations of body
    parts and a Part Association Field to represent the joints association. This method
    outperformed previous OpenPose-based approaches on low-resolution and occluded
    scenes. Motivated by OpenPose (Cao et al., [2017](#bib.bib17)) and stacked hourglass
    structure (Newell et al., [2016](#bib.bib172)), Newell et al. (Newell et al.,
    [2017](#bib.bib171)) introduced a single-stage deep network to simultaneously
    obtain pose detection and group assignments. Following (Newell et al., [2017](#bib.bib171)),
    Jin et al. (Jin et al., [2020a](#bib.bib89)) proposed a new differentiable Hierarchical
    Graph Grouping method to learn the human part grouping. Based on (Newell et al.,
    [2017](#bib.bib171)) and (Sun et al., [2019](#bib.bib222)), Cheng et al. (Cheng
    et al., [2020](#bib.bib32)) proposed an extension of HRNet, named Higher Resolution
    Network, which deconvolves the high-resolution heatmaps generated by HRNet to
    solve the scale variation challenge in bottom-up multi-person HPE.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Pishchulin 等人（Pishchulin et al., [2016](#bib.bib193)）提出了一种基于 Fast R-CNN 的身体部位检测器，命名为
    DeepCut，这是一种最早的两阶段自下而上的方法之一。它首先检测所有身体部位候选区域，然后对每个部位进行标记，并通过整数线性规划（ILP）将这些部位组装成最终的姿势。然而，DeepCut
    计算开销较大。为此，Insafutdinov 等人（Insafutdinov et al., [2016](#bib.bib77)）引入了 DeeperCut，通过应用更强的身体部位检测器、改进的增量优化策略和图像条件下的成对项来提高
    DeepCut，从而提高了性能并加快了速度。随后，Cao 等人（Cao et al., [2017](#bib.bib17)）构建了一种名为 OpenPose
    的检测器，它利用卷积姿态网络（Wei et al., [2016](#bib.bib255)）通过热图预测关键点坐标，并使用部位关联字段（PAFs，一组 2D
    向量场与向量图，编码了四肢的位置和方向）将关键点与每个人关联起来。OpenPose 在自下而上的多人姿态估计中显著加快了速度。在 OpenPose 框架的基础上，Zhu
    等人（Zhu et al., [2017](#bib.bib316)）通过添加冗余边来增加 PAFs 中关节之间的连接，从而改进了 OpenPose 结构，并取得了比基准方法更好的性能。尽管基于
    OpenPose 的方法在高分辨率图像上取得了令人印象深刻的结果，但在低分辨率图像和遮挡情况下表现较差。为了解决这个问题，Kreiss 等人（Kreiss
    et al., [2019](#bib.bib105)）提出了一种名为 PifPaf 的自下而上方法，利用部位强度场预测身体部位的位置，并使用部位关联场表示关节的关联。该方法在低分辨率和遮挡场景下优于之前的基于
    OpenPose 的方法。受到 OpenPose（Cao et al., [2017](#bib.bib17)）和堆叠沙漏结构（Newell et al.,
    [2016](#bib.bib172)）的启发，Newell 等人（Newell et al., [2017](#bib.bib171)）引入了一种单阶段深度网络，旨在同时获得姿态检测和分组分配。继（Newell
    et al., [2017](#bib.bib171)）之后，Jin 等人（Jin et al., [2020a](#bib.bib89)）提出了一种新的可微分层次图分组方法，以学习人体部位分组。基于（Newell
    et al., [2017](#bib.bib171)）和（Sun et al., [2019](#bib.bib222)），Cheng 等人（Cheng
    et al., [2020](#bib.bib32)）提出了 HRNet 的扩展版本，命名为 Higher Resolution Network，该网络通过解卷积
    HRNet 生成的高分辨率热图来解决自下而上的多人姿态估计中的尺度变化挑战。
- en: Multi-task structures are also employed in bottom-up HPE methods. Papandreou
    et al. (Papandreou et al., [2018](#bib.bib180)) introduced PersonLab to combine
    the pose estimation module and the person segmentation module for keypoints detection
    and association. PersonLab consists of short-range offsets (for refining heatmaps),
    mid-range offsets (for predicting the keypoints), and long-range offsets (for
    grouping keypoints into instances). Kocabas et al. (Kocabas et al., [2018](#bib.bib100))
    presented a multi-task learning model with a pose residual net, named MultiPoseNet,
    which can perform keypoint prediction, human detection, and semantic segmentation
    tasks altogether. However, these methods are struggling in dealing with the variance
    of human scales, to address this problem, Luo et al. (Luo et al., [2021](#bib.bib147))
    introduced a method named SAHR (scale-adaptive heatmap regression) to optimize
    the joint standard deviation adaptively, which improved the tolerance of various
    human scales and labeling ambiguities in an efficient way.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务结构也被用于自下而上的 HPE 方法中。Papandreou 等人 (Papandreou 等, [2018](#bib.bib180)) 介绍了
    PersonLab，将姿态估计模块和人体分割模块结合起来进行关键点检测和关联。PersonLab 由短距离偏移（用于精细化热图）、中距离偏移（用于预测关键点）和长距离偏移（用于将关键点分组为实例）组成。Kocabas
    等人 (Kocabas 等, [2018](#bib.bib100)) 提出了一个多任务学习模型，名为 MultiPoseNet，能够同时执行关键点预测、人体检测和语义分割任务。然而，这些方法在处理人体尺度变化时仍然面临困难。为了解决这个问题，Luo
    等人 (Luo 等, [2021](#bib.bib147)) 引入了一种名为 SAHR（尺度自适应热图回归）的方法，能够自适应地优化关节标准差，提高了对各种人体尺度和标注模糊的容忍度。
- en: 2.3\. 2D HPE Summary
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 2D HPE 概述
- en: In summary, the performance of 2D HPE has been significantly improved with the
    blooming of deep learning techniques. In recent years, deeper and more powerful
    networks have enhanced the performance of 2D single-person HPE methods such as
    DeepPose (Toshev and Szegedy, [2014](#bib.bib234)) and Stacked Hourglass Network
    (Newell et al., [2016](#bib.bib172)), as well as in 2D multi-person HPE like AlphaPose
    (Fang et al., [2017](#bib.bib56)) and OpenPose (Cao et al., [2017](#bib.bib17)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，随着深度学习技术的蓬勃发展，2D HPE 的性能有了显著提升。近年来，更深层次和更强大的网络提升了2D单人HPE方法的性能，如 DeepPose
    (Toshev 和 Szegedy, [2014](#bib.bib234)) 和 Stacked Hourglass Network (Newell 等,
    [2016](#bib.bib172))，以及2D多人体HPE方法，如 AlphaPose (Fang 等, [2017](#bib.bib56)) 和 OpenPose
    (Cao 等, [2017](#bib.bib17))。
- en: Although promising performance has been achieved, there are several challenges
    in 2D HPE that need to be further addressed in future research. First is the reliable
    detection of individuals under significant occlusion (Chen et al., [2018b](#bib.bib30)),
    e.g., in crowd scenarios. Person detectors in top-down 2D HPE methods may fail
    to identify the boundaries of largely overlapped human bodies. Similarly, the
    difficulty of keypoint association is more pronounced for bottom-up approaches
    in occluded scenes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经取得了有希望的性能，但在 2D HPE 中仍存在一些挑战，需要在未来的研究中进一步解决。首先是对显著遮挡下个体的可靠检测 (Chen 等, [2018b](#bib.bib30))，例如在人群场景中。自顶向下的
    2D HPE 方法中的人体检测器可能无法识别大量重叠的人体边界。类似地，自下而上的方法在遮挡场景中面临的关键点关联难度更为突出。
- en: The second challenge is computation efficiency. Although some methods like OpenPose
    (Cao et al., [2017](#bib.bib17)) can achieve near real-time processing on special
    hardware with moderate computing power (e.g., 22 FPS with an Nvidia GTX 1080 Ti
    GPU), it is still difficult to implement the networks on resource-constrained
    devices. Real-world applications (e.g., gaming, AR, and VR) require more efficient
    HPE methods on commercial devices which can bring better interactive experiences
    for users.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个挑战是计算效率。尽管一些方法如 OpenPose (Cao 等, [2017](#bib.bib17)) 可以在具有中等计算能力的特定硬件上实现接近实时处理（例如，使用
    Nvidia GTX 1080 Ti GPU 时达到 22 FPS），但在资源受限的设备上实现这些网络仍然很困难。现实世界应用（例如，游戏、AR 和 VR）需要在商业设备上实现更高效的
    HPE 方法，以便为用户提供更好的互动体验。
- en: Another challenge lies in the limited data for rare poses. Although the size
    of current datasets for 2D HPE is large enough (e.g., COCO dataset (Lin et al.,
    [2014](#bib.bib133))) for normal pose estimation (e.g., standing, walking, running),
    these datasets have limited training data for unusual poses, e.g., falling. The
    data imbalance may cause model bias, resulting in poor performance on those poses.
    It would be useful to develop effective data generation or augmentation techniques
    to generate extra pose data for training more robust models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战在于稀有姿态的数据有限。尽管目前2D HPE数据集的规模足够大（例如，COCO数据集（Lin et al., [2014](#bib.bib133)））以用于正常姿态估计（例如，站立、行走、跑步），这些数据集对不寻常姿态（例如，跌倒）的训练数据有限。数据不平衡可能导致模型偏差，从而在这些姿态上表现较差。开发有效的数据生成或增强技术以生成额外的姿态数据以训练更强健的模型将是有用的。
- en: 3\. 3D human pose estimation
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 3D人体姿态估计
- en: 3D HPE, which aims to predict the locations of body joints in 3D space, has
    attracted much interest in recent years since it can provide extensive 3D structure
    information related to the human body. It can be applied to various applications
    (e.g., 3D movie and animation industries, virtual reality, and sports analysis).
    Although significant improvements have been achieved in 2D HPE, 3D HPE still remains
    a challenging task. Most existing works tackle 3D HPE from monocular images or
    videos, which is an ill-posed and inverse problem due to projection of 3D to 2D
    where one dimension is lost. When multiple views are available or other sensors
    such as IMU and LiDAR are deployed, 3D HPE can be a well-posed problem employing
    information fusion techniques. Another limitation is that deep learning models
    are data-hungry and sensitive to the data collection environment. Unlike 2D HPE
    datasets where accurate 2D pose annotation can be easily obtained, collecting
    accurate 3D pose annotation is time-consuming and manual labeling is not practical.
    Also, datasets are usually collected from indoor environments with selected daily
    actions. Recent works (Zhou et al., [2017](#bib.bib309); Yang et al., [2018](#bib.bib272);
    Wandt and Rosenhahn, [2019](#bib.bib243)) revealed the poor generalization of
    models trained with biased datasets by cross-dataset inference. In this section,
    we first focus on 3D HPE from monocular RGB images and videos and then cover 3D
    HPE based on other sensors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 3D HPE（3D人体关键点估计）旨在预测三维空间中身体关节的位置，近年来受到广泛关注，因为它可以提供与人体相关的广泛三维结构信息。它可以应用于各种领域（例如，3D电影和动画产业、虚拟现实以及运动分析）。尽管2D
    HPE已经取得了显著进展，但3D HPE仍然是一项具有挑战性的任务。大多数现有工作从单目图像或视频中处理3D HPE，由于三维到二维的投影丢失了一个维度，这是一种不适定的逆问题。当有多个视角可用或部署其他传感器如IMU和LiDAR时，通过信息融合技术，3D
    HPE可以成为一个适定的问题。另一个限制是深度学习模型对数据需求量大且对数据收集环境敏感。与2D HPE数据集可以轻松获得准确的2D姿态标注不同，收集准确的3D姿态标注既耗时又手动标注不切实际。此外，数据集通常来自于室内环境中选择的日常动作。近期的研究（Zhou
    et al., [2017](#bib.bib309); Yang et al., [2018](#bib.bib272); Wandt and Rosenhahn,
    [2019](#bib.bib243)）通过跨数据集推理揭示了在偏置数据集上训练的模型的较差泛化能力。本节首先关注基于单目RGB图像和视频的3D HPE，然后介绍基于其他传感器的3D
    HPE。
- en: 3.1\. 3D HPE from monocular RGB images and videos
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 基于单目RGB图像和视频的3D HPE
- en: The monocular camera is the most widely used sensor for HPE in both 2D and 3D
    scenarios. Recent progress in deep learning-based 2D HPE from monocular images
    and videos has enabled researchers to extend their works to 3D HPE.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 单目相机是2D和3D场景中最广泛使用的HPE传感器。基于深度学习的单目图像和视频2D HPE的最新进展使研究人员能够将他们的工作扩展到3D HPE。
- en: The reconstruction of 3D human poses from a single view of monocular images
    and videos is a nontrivial task that suffers from self-occlusions and other object
    occlusions, depth ambiguities, and insufficient training data. It is a severely
    ill-posed problem because different 3D human poses can be projected to a similar
    2D pose projection.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从单目图像和视频的单一视角重建3D人体姿态是一项非平凡的任务，容易受到自遮挡和其他物体遮挡、深度模糊以及训练数据不足的影响。这是一个严重的不适定问题，因为不同的3D人体姿态可能投影到相似的2D姿态投影中。
- en: 'The problem of occlusion can be alleviated by estimating 3D human pose from
    multi-view cameras. In a multi-view setting, the viewpoints association needs
    to be addressed. Thus deep learning-based 3D HPE methods are divided into three
    categories: single-view single-person 3D HPE, single-view multi-person 3D HPE,
    and multi-view 3D HPE.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从多视角相机估计 3D 人体姿态，可以缓解遮挡问题。在多视角设置中，需要解决视点关联的问题。因此，基于深度学习的 3D 人体姿态估计方法分为三类：单视角单人
    3D 人体姿态估计、单视角多人 3D 人体姿态估计和多视角 3D 人体姿态估计。
- en: 3.1.1\. Single-view single person 3D HPE
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 单视角单人 3D 人体姿态估计
- en: Single-person 3D HPE approaches can be classified into skeleton-only and human
    mesh recovery (HMR) categories based on whether to reconstruct 3D human skeleton
    or to recover 3D human mesh by employing a human body model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 单人 3D 人体姿态估计方法可以根据是否重建 3D 人体骨架或通过使用人体模型恢复 3D 人体网格，分为仅骨架和人体网格恢复 (HMR) 两类。
- en: A. Skeleton-only. The skeleton-only methods estimate 3D human joints as the
    final output. They do not employ human body models to reconstruct 3D human mesh
    representation. These methods can be further divided into direct estimation approaches
    and 2D to 3D lifting approaches.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: A. 仅骨架。仅骨架方法估计 3D 人体关节作为最终输出。它们不使用人体模型来重建 3D 人体网格表示。这些方法可以进一步分为直接估计方法和 2D 到
    3D 提升方法。
- en: '![Refer to caption](img/45939dd40b73ed9f75cb18cca0bd092c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/45939dd40b73ed9f75cb18cca0bd092c.png)'
- en: Figure 4\. Single-person 3D HPE frameworks. (a) Direct estimation approaches
    directly estimate the 3D human pose from 2D images. (b) 2D to 3D lifting approaches
    leverage the predicted 2D human pose (intermediate representation) for 3D pose
    estimation. (c) Human mesh recovery methods incorporate parametric body models
    to recover a high-quality 3D human mesh. The 3D pose and shape parameters inferred
    by the 3D pose and shape network are fed into the model regressor to reconstruct
    3D human mesh. Part of the figure is from (Arnab et al., [2019](#bib.bib6)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 单人 3D 人体姿态估计框架。 (a) 直接估计方法直接从 2D 图像中估计 3D 人体姿态。 (b) 2D 到 3D 提升方法利用预测的
    2D 人体姿态（中间表示）进行 3D 姿态估计。 (c) 人体网格恢复方法结合参数化的身体模型来恢复高质量的 3D 人体网格。由 3D 姿态和形状网络推断的
    3D 姿态和形状参数被输入到模型回归器中以重建 3D 人体网格。部分图像来源于 (Arnab et al., [2019](#bib.bib6))。
- en: 'Direct estimation: As shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1\. Single-view
    single person 3D HPE ‣ 3.1\. 3D HPE from monocular RGB images and videos ‣ 3\.
    3D human pose estimation ‣ Deep Learning-Based Human Pose Estimation: A Survey")(a),
    direct estimation methods infer the 3D human pose from 2D images without intermediately
    estimating 2D pose representation, e.g., (Li et al., [2015](#bib.bib120); Tekin
    et al., [2016a](#bib.bib226); Sun et al., [2017](#bib.bib223); Pavlakos et al.,
    [2017a](#bib.bib185); Pavlakos et al., [2018a](#bib.bib184)). Li and Chan (Li
    and Chan, [2014](#bib.bib118)) employed a shallow network to train the body part
    detector with sliding windows and the pose coordinate regression synchronously.
    Sun et al. (Sun et al., [2017](#bib.bib223)) proposed a structure-aware regression
    approach. Instead of using a joint-based representation, they adopted a bone-based
    representation with more stability. A compositional loss was defined by exploiting
    the 3D bone structure with bone-based representation that encodes long-range interactions
    between the bones. Pavlakos et al. (Pavlakos et al., [2017a](#bib.bib185); Pavlakos
    et al., [2018a](#bib.bib184)) introduced a volumetric representation to convert
    the highly non-linear 3D coordinate regression problem to a manageable form in
    a discretized space. The voxel likelihoods for each joint in the volume were predicted
    by a convolutional network. Ordinal depth relations of human joints were used
    to alleviate the need for accurate 3D ground truth poses.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 直接估计：如图 [4](#S3.F4 "图 4 ‣ 3.1.1\. 单视图单人 3D HPE ‣ 3.1\. 从单目 RGB 图像和视频中获取 3D HPE
    ‣ 3\. 3D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述")(a) 所示，直接估计方法从 2D 图像中推断 3D 人体姿态，而不需要中间估计 2D
    姿态表示，例如，（Li 等人，[2015](#bib.bib120)；Tekin 等人，[2016a](#bib.bib226)；Sun 等人，[2017](#bib.bib223)；Pavlakos
    等人，[2017a](#bib.bib185)；Pavlakos 等人，[2018a](#bib.bib184)）。Li 和 Chan（Li 和 Chan，[2014](#bib.bib118)）采用浅层网络同时训练身体部位检测器与姿态坐标回归。Sun
    等人（Sun 等人，[2017](#bib.bib223)）提出了一种结构感知回归方法。他们没有使用基于关节的表示，而是采用了基于骨骼的表示，具有更高的稳定性。通过利用基于骨骼的
    3D 骨架结构定义了组合损失，该表示编码了骨骼之间的长程交互。Pavlakos 等人（Pavlakos 等人，[2017a](#bib.bib185)；Pavlakos
    等人，[2018a](#bib.bib184)）引入了体积表示，将高度非线性的 3D 坐标回归问题转化为离散空间中可管理的形式。体积中每个关节的体素可能性由卷积网络预测。使用了人类关节的序数深度关系，以减轻对准确
    3D 实际姿态的需求。
- en: '2D to 3D lifting: Motivated by the recent success of 2D HPE, 2D to 3D lifting
    approaches that infer 3D human pose from the intermediately estimated 2D human
    pose have become a popular 3D HPE solution as illustrated in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1.1\. Single-view single person 3D HPE ‣ 3.1\. 3D HPE from monocular RGB
    images and videos ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based Human Pose
    Estimation: A Survey") (b). In the first stage, off-the-shelf 2D HPE models are
    employed to estimate 2D pose. Then 2D to 3D lifting is used to obtain 3D pose
    in the second stage, e.g., (Chen and Ramanan, [2017](#bib.bib19); Martinez et al.,
    [2017](#bib.bib157); Tekin et al., [2017](#bib.bib227); Zhou et al., [2019](#bib.bib308);
    Moreno-Noguer, [2017](#bib.bib169); Li and Lee, [2019](#bib.bib112)). Benefiting
    from the excellent performance of state-of-the-art 2D pose detectors, 2D to 3D
    lifting approaches generally outperform direct estimation approaches. Martinez
    et al. (Martinez et al., [2017](#bib.bib157)) proposed a fully connected residual
    network to regress 3D joint locations based on the 2D joint locations. Despite
    achieving state-of-the-art results at that time, the method could fail due to
    reconstruction ambiguity of over-reliance on the 2D pose detector. Tekin et al.
    (Tekin et al., [2017](#bib.bib227)) and Zhou et al. (Zhou et al., [2019](#bib.bib308))
    adopted 2D heatmaps instead of 2D pose as intermediate representations for estimating
    3D pose. Wang et al. (Wang et al., [2018](#bib.bib251)) developed a pairwise ranking
    CNN to predict the depth ranking of pairwise human joints. Then, a coarse-to-fine
    pose estimator was used to regress the 3D pose from 2D joints and the depth ranking
    matrix. Jahangiri and Yuille (Jahangiri and Yuille, [2017](#bib.bib81)), Sharma
    et al. (Sharma et al., [2019](#bib.bib216)), and Li and Lee (Li and Lee, [2019](#bib.bib112))
    first generated multiple diverse 3D pose hypotheses then applied ranking networks
    to select the best 3D pose.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 到 3D 的提升：受到最近 2D HPE 成功的启发，从中间估计的 2D 人体姿态推断 3D 人体姿态的 2D 到 3D 提升方法已经成为一种流行的
    3D HPE 解决方案，如图 [4](#S3.F4 "图 4 ‣ 3.1.1\. 单视角单人 3D HPE ‣ 3.1\. 从单目 RGB 图像和视频中进行
    3D HPE ‣ 3\. 3D 人体姿态估计 ‣ 基于深度学习的人体姿态估计：综述") (b) 所示。在第一阶段，使用现成的 2D HPE 模型来估计 2D
    姿态。然后在第二阶段使用 2D 到 3D 提升方法来获得 3D 姿态，例如 (Chen and Ramanan, [2017](#bib.bib19); Martinez
    et al., [2017](#bib.bib157); Tekin et al., [2017](#bib.bib227); Zhou et al., [2019](#bib.bib308);
    Moreno-Noguer, [2017](#bib.bib169); Li and Lee, [2019](#bib.bib112))。得益于最先进的 2D
    姿态检测器的优异性能，2D 到 3D 提升方法通常优于直接估计方法。Martinez et al. (Martinez et al., [2017](#bib.bib157))
    提出了一个完全连接的残差网络，根据 2D 关节位置回归 3D 关节位置。尽管当时取得了最先进的结果，但该方法由于过度依赖 2D 姿态检测器而可能因重建模糊性而失败。Tekin
    et al. (Tekin et al., [2017](#bib.bib227)) 和 Zhou et al. (Zhou et al., [2019](#bib.bib308))
    采用了 2D 热图而不是 2D 姿态作为中间表示来估计 3D 姿态。Wang et al. (Wang et al., [2018](#bib.bib251))
    开发了一种成对排序 CNN 来预测成对人体关节的深度排序。然后，使用粗到细的姿态估计器从 2D 关节和深度排序矩阵回归 3D 姿态。Jahangiri 和
    Yuille (Jahangiri and Yuille, [2017](#bib.bib81))、Sharma et al. (Sharma et al.,
    [2019](#bib.bib216)) 和 Li 和 Lee (Li and Lee, [2019](#bib.bib112)) 首先生成多个多样化的 3D
    姿态假设，然后应用排序网络来选择最佳的 3D 姿态。
- en: Given that a human pose can be represented as a graph where the joints are the
    nodes and the bones are the edges, Graph Convolutional Networks (GCNs) have been
    applied to the 2D-to-3D pose lifting problem by showing promising performance
    (Ci et al., [2019](#bib.bib43); Zhao et al., [2019b](#bib.bib297); Choi et al.,
    [2020](#bib.bib39); Liu et al., [2020a](#bib.bib138); Zeng et al., [2020](#bib.bib282)).
    Ci et al. (Ci et al., [2019](#bib.bib43)) proposed a Locally Connected Network
    (LCN), which leverages both a fully connected network and GCN to encode the relationship
    between local joint neighborhoods. LCN can overcome the limitations of GCN that
    the weight-sharing scheme harms the pose estimation model’s representation ability,
    and the structure matrix lacks the flexibility to support customized node dependence.
    Zhao et al. (Zhao et al., [2019b](#bib.bib297)) also tackled the limitation of
    the shared weight matrix of convolution filters for all the nodes in GCN. A Semantic-GCN
    was proposed to investigate the semantic information and relationship, which is
    not explicitly represented in the graph. The semantic graph convolution (SemGConv)
    operation is used to learn channel-wise weights for edges. Both local and global
    relationships among nodes are captured since SemGConv and non-local layers are
    interleaved. Zhou et al. (Zou and Tang, [2021](#bib.bib317)) further introduced
    a novel modulated GCN network which consists of weight modulation and affinity
    modulation. The weight modulation exploits different modulation vectors for different
    nodes that disentangles the feature transformations. The affinity modulation explores
    additional joint correlations beyond the defined human skeleton.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人体姿态可以表示为一个图，其中关节是节点，骨骼是边，图卷积网络（GCNs）已被应用于二维到三维姿态提升问题，并取得了有希望的表现（Ci 等， [2019](#bib.bib43)；赵等，
    [2019b](#bib.bib297)；崔等， [2020](#bib.bib39)；刘等， [2020a](#bib.bib138)；曾等， [2020](#bib.bib282)）。
    Ci 等（Ci 等， [2019](#bib.bib43)）提出了一种局部连接网络（LCN），它结合了全连接网络和 GCN 来编码局部关节邻域之间的关系。LCN
    可以克服 GCN 的局限性，即权重共享机制损害了姿态估计模型的表示能力，且结构矩阵缺乏支持自定义节点依赖性的灵活性。赵等（赵等， [2019b](#bib.bib297)）还解决了
    GCN 中所有节点卷积滤波器共享权重矩阵的限制。提出了一种语义-GCN，以探究图中未显式表示的语义信息和关系。使用语义图卷积（SemGConv）操作来学习边的通道级权重。由于
    SemGConv 和非局部层交替，因此捕捉了节点之间的局部和全局关系。周等（邹和唐， [2021](#bib.bib317)）进一步介绍了一种新颖的调制 GCN
    网络，该网络由权重调制和亲和调制组成。权重调制利用不同的调制向量为不同的节点解开特征变换。亲和调制探索了超出定义的人体骨架的附加关节相关性。
- en: The kinematic model is an articulated body representation by connected bones
    and joints with kinematic constraints, which has gained increasing attention in
    3D HPE in recent years. Many methods leverage prior knowledge based on the kinematic
    model such as skeletal joint connectivity information, joint rotation properties,
    and fixed bone-length ratios for plausible pose estimation, e.g., (Zhou et al.,
    [2016a](#bib.bib310); Mehta et al., [2017](#bib.bib161); Nie et al., [2017](#bib.bib174);
    Wang et al., [2019a](#bib.bib246); Kundu et al., [2020c](#bib.bib109); Xu et al.,
    [2020c](#bib.bib264); Nie et al., [2020](#bib.bib175); Georgakis et al., [2020](#bib.bib59)).
    Zhou et al. (Zhou et al., [2016a](#bib.bib310)) embedded a kinematic model into
    a network as kinematic layers to enforce the orientation and rotation constraints.
    Nie et al. (Nie et al., [2017](#bib.bib174)) and Lee et al. (Lee et al., [2018](#bib.bib111))
    employed a skeleton-LSTM network to leverage joint relations and connectivity.
    Observing that human body parts have a distinct degree of freedom (DOF) based
    on the kinematic structure, Wang et al. (Wang et al., [2019a](#bib.bib246)) and
    Nie et al. (Nie et al., [2020](#bib.bib175)) proposed bidirectional networks to
    model the kinematic and geometric dependencies of the human skeleton. Kundu et
    al. (Kundu et al., [2020c](#bib.bib109)) (Kundu et al., [2020b](#bib.bib108))
    designed a kinematic structure preservation approach by inferring local-kinematic
    parameters with energy-based loss and explored 2D part segments based on the parent-relative
    local limb kinematic model. Xu et al. (Xu et al., [2020c](#bib.bib264)) demonstrated
    that noise in the 2D joint is one of the key obstacles for accurate 3D pose estimation.
    Hence a 2D pose correction module was employed to refine unreliable 2D joints
    based on the kinematic structure. Zanfir et al. (Zanfir et al., [2020](#bib.bib279))
    introduced a kinematic latent normalizing flow representation (a sequence of invertible
    transformations applied to the original distribution) with differentiable semantic
    body part alignment loss functions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 动力学模型是通过连接的骨骼和关节以及动力学约束来表示的可动体模型，近年来在3D HPE中获得了越来越多的关注。许多方法利用基于动力学模型的先验知识，如骨骼关节连通信息、关节旋转属性和固定骨长比率，以进行合理的姿势估计，例如（Zhou
    et al., [2016a](#bib.bib310); Mehta et al., [2017](#bib.bib161); Nie et al., [2017](#bib.bib174);
    Wang et al., [2019a](#bib.bib246); Kundu et al., [2020c](#bib.bib109); Xu et al.,
    [2020c](#bib.bib264); Nie et al., [2020](#bib.bib175); Georgakis et al., [2020](#bib.bib59)）。Zhou
    et al. (Zhou et al., [2016a](#bib.bib310)) 将动力学模型嵌入网络中作为动力学层，以强制执行方向和旋转约束。Nie
    et al. (Nie et al., [2017](#bib.bib174)) 和 Lee et al. (Lee et al., [2018](#bib.bib111))
    使用了骨骼-LSTM 网络，以利用关节关系和连通性。Wang et al. (Wang et al., [2019a](#bib.bib246)) 和 Nie
    et al. (Nie et al., [2020](#bib.bib175)) 观察到人体部位根据动力学结构具有不同的自由度 (DOF)，因此提出了双向网络来建模人体骨架的动力学和几何依赖性。Kundu
    et al. (Kundu et al., [2020c](#bib.bib109)) (Kundu et al., [2020b](#bib.bib108))
    设计了一种通过推断局部动力学参数和基于能量的损失的动力学结构保留方法，并基于父体相对局部肢体动力学模型探索了2D部分分段。Xu et al. (Xu et
    al., [2020c](#bib.bib264)) 证明了2D关节中的噪声是准确的3D姿势估计的关键障碍之一。因此，采用了一个2D姿势校正模块来基于动力学结构细化不可靠的2D关节。Zanfir
    et al. (Zanfir et al., [2020](#bib.bib279)) 引入了一种动力学潜在正则化流表示（对原始分布应用的一系列可逆变换），并结合可微的语义身体部位对齐损失函数。
- en: 3D HPE datasets are usually collected from controlled environments with selected
    common motions. It is difficult to obtain accurate 3D pose annotations for in-the-wild
    data. Thus 3D HPE for in-the-wild data with unusual poses and occlusions is still
    a challenge. To this end, a group of 2D to 3D lifting methods estimate the 3D
    human pose from in-the-wild images without 3D pose annotations such as (Zhou et al.,
    [2017](#bib.bib309); Habibie et al., [2019](#bib.bib65); Chen et al., [2019](#bib.bib20);
    Yang et al., [2018](#bib.bib272); Wandt and Rosenhahn, [2019](#bib.bib243)). Zhou
    et al. (Zhou et al., [2017](#bib.bib309)) proposed a weakly supervised transfer
    learning method that uses 2D annotations of in-the-wild images as weak labels.
    A 3D pose estimation module was connected with intermediate layers of the 2D pose
    estimation module. For in-the-wild images, 2D pose estimation module performed
    a supervised 2D heatmap regression and a 3D bone length constraint-induced loss
    was applied in the weakly supervised 3D pose estimation module. Habibie et al.
    (Habibie et al., [2019](#bib.bib65)) tailored a projection loss to refine the
    3D human pose without 3D annotation. A 3D-2D projection module was designed to
    estimate the 2D body joint locations with the predicted 3D pose from the earlier
    network layer. The projection loss was used to update the 3D human pose without
    requiring 3D annotations. Inspired by (Drover et al., [2018](#bib.bib51)), Chen
    et al. (Chen et al., [2019](#bib.bib20)) proposed an unsupervised lifting network
    based on the closure and invariance lifting properties with a geometric self-consistency
    loss for the lift-reproject-lift process. Closure means for a lifted 3D skeleton,
    after random rotation and re-projection, the resulting 2D skeleton will lie within
    the distribution of valid 2D poses. Invariance means when changing the viewpoint
    of 2D projection from a 3D skeleton, the re-lifted 3D skeleton should be the same.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 3D HPE 数据集通常是在控制环境中收集的，具有选择性的常见动作。获取实际场景数据的准确 3D 姿态标注是困难的。因此，对于具有不寻常姿势和遮挡的实际场景数据的
    3D HPE 仍然是一个挑战。为此，一些 2D 到 3D 提升方法从实际场景图像中估计 3D 人体姿态，而无需 3D 姿态标注，例如 (Zhou et al.,
    [2017](#bib.bib309); Habibie et al., [2019](#bib.bib65); Chen et al., [2019](#bib.bib20);
    Yang et al., [2018](#bib.bib272); Wandt and Rosenhahn, [2019](#bib.bib243))。Zhou
    et al. (Zhou et al., [2017](#bib.bib309)) 提出了一个弱监督迁移学习方法，该方法使用实际场景图像的 2D 注释作为弱标签。一个
    3D 姿态估计模块与 2D 姿态估计模块的中间层连接。对于实际场景图像，2D 姿态估计模块执行了监督的 2D 热图回归，并在弱监督 3D 姿态估计模块中应用了
    3D 骨长约束引起的损失。Habibie et al. (Habibie et al., [2019](#bib.bib65)) 定制了一种投影损失，以在没有
    3D 注释的情况下细化 3D 人体姿态。设计了一个 3D-2D 投影模块，用于估计具有先前网络层预测的 3D 姿态的 2D 身体关节位置。投影损失用于更新
    3D 人体姿态，而无需 3D 注释。受到 (Drover et al., [2018](#bib.bib51)) 的启发，Chen et al. (Chen
    et al., [2019](#bib.bib20)) 提出了一个基于闭合性和不变性提升特性的无监督提升网络，并为提升-重投影-提升过程引入了几何自一致性损失。闭合性意味着对于一个提升的
    3D 骨架，在随机旋转和重投影后，得到的 2D 骨架将位于有效 2D 姿态的分布范围内。不变性意味着当从 3D 骨架的 2D 投影视点发生变化时，重新提升的
    3D 骨架应该保持不变。
- en: Instead of estimating 3D human pose from monocular images, videos can provide
    temporal information to improve accuracy and robustness of 3D HPE, e.g., (Zhou
    et al., [2016b](#bib.bib312); Zhou et al., [2018](#bib.bib313); Dabral et al.,
    [2018](#bib.bib45); Pavllo et al., [2019](#bib.bib188); Cheng et al., [2019](#bib.bib35);
    Cai et al., [2019](#bib.bib14); Wang et al., [2020d](#bib.bib249); Tekin et al.,
    [2016b](#bib.bib228)). Hossain and Little (Rayat Imtiaz Hossain and Little, [2018](#bib.bib202))
    proposed a recurrent neural network using a Long Short-Term Memory (LSTM) unit
    with shortcut connections to exploit temporal information from sequences of human
    pose. Their method exploits the past events in a sequence-to-sequence network
    to predict temporally consistent 3D pose. Noticing that the complementary property
    between spatial constraints and temporal correlations is usually ignored by prior
    work, Dabral et al. (Dabral et al., [2018](#bib.bib45)), Cai et al. (Cai et al.,
    [2019](#bib.bib14)), and Li et al. (Li et al., [2019](#bib.bib128)) exploited
    the spatial-temporal relationships and constraints (e.g., bone-length constraint
    and left-right symmetry constraint) to improve 3D HPE performance from sequential
    frames. Pavllo et al. (Pavllo et al., [2019](#bib.bib188)) proposed a temporal
    convolution network to estimate 3D pose over 2D keypoints from consecutive 2D
    sequences. However, their method is based on the assumption that prediction errors
    are temporally non-continuous and independent, which may not hold in the presence
    of occlusions (Cheng et al., [2019](#bib.bib35)). Based on (Pavllo et al., [2019](#bib.bib188)),
    Chen et al. (Chen et al., [2021](#bib.bib24)) added a bone direction module and
    bone length module to ensure human anatomy temporal consistency across video frames,
    while Liu et al. (Liu et al., [2020c](#bib.bib139)) utilized the attention mechanism
    to recognize significant frames and model long-range dependencies in large temporal
    receptive fields. Zeng et al. (Zeng et al., [2020](#bib.bib282)) employed the
    split-and-recombine strategy to address the rare and unseen pose problem. The
    human body is first split into local regions for processing through separate temporal
    convolutional network branches, then the low-dimensional global context obtained
    from each branch is combined for maintaining global coherence.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于从单目图像中估计3D人体姿势，视频可以提供时间信息以提高3D人体姿态估计的准确性和鲁棒性，例如，（Zhou et al., [2016b](#bib.bib312);
    Zhou et al., [2018](#bib.bib313); Dabral et al., [2018](#bib.bib45); Pavllo et
    al., [2019](#bib.bib188); Cheng et al., [2019](#bib.bib35); Cai et al., [2019](#bib.bib14);
    Wang et al., [2020d](#bib.bib249); Tekin et al., [2016b](#bib.bib228)）。Hossain和Little（Rayat
    Imtiaz Hossain和Little, [2018](#bib.bib202)）提出了一种使用带有快捷连接的长短期记忆（LSTM）单元的递归神经网络，以利用人体姿势序列中的时间信息。他们的方法利用序列到序列网络中的过去事件来预测时间上一致的3D姿势。Dabral等人（Dabral
    et al., [2018](#bib.bib45)）、Cai等人（Cai et al., [2019](#bib.bib14)）和Li等人（Li et al.,
    [2019](#bib.bib128)）注意到先前的工作通常忽视了空间约束和时间相关性之间的互补特性，因此他们利用了空间-时间关系和约束（例如骨长约束和左右对称约束）来提高从序列帧中获得的3D姿态估计性能。Pavllo等人（Pavllo
    et al., [2019](#bib.bib188)）提出了一种时间卷积网络，以估计连续2D序列中2D关键点的3D姿势。然而，他们的方法基于预测误差在时间上是非连续和独立的假设，这在存在遮挡的情况下可能不成立（Cheng
    et al., [2019](#bib.bib35)）。基于（Pavllo et al., [2019](#bib.bib188)），Chen等人（Chen
    et al., [2021](#bib.bib24)）添加了骨方向模块和骨长模块，以确保视频帧中的人体解剖结构时间一致性，而Liu等人（Liu et al.,
    [2020c](#bib.bib139)）利用了注意力机制来识别重要帧，并在大时间接收场中建模长距离依赖关系。Zeng等人（Zeng et al., [2020](#bib.bib282)）采用了分割和重组合策略来解决稀有和未见过的姿势问题。首先，将人体分割为局部区域，通过不同的时间卷积网络分支进行处理，然后将从每个分支获得的低维全局上下文结合起来，以保持全局一致性。
- en: Transformer architectures have become the model of choice in natural language
    processing due to the self-attention mechanism, and now are developing rapidly
    in the field of computer vision. Recent works have demonstrated the powerful global
    representation ability of transformer attention mechanism in various vision tasks
    (Khan et al., [2021](#bib.bib98)). Zheng et al. (Zheng et al., [2021](#bib.bib305))
    presented the first purely transformer-based approach, named PoseFormer, for 3D
    HPE without convolutional architectures involved. The spatial transformer module
    encodes local relationships between human body joints, and the temporal transformer
    module captures the global dependencies across frames in the entire sequence.
    Li et al. (Li et al., [2022a](#bib.bib121)) further designed a multi-hypothesis
    transformer to exploit spatial-temporal representations of multiple pose hypotheses.
    Zhao et al. (Zhao et al., [2023](#bib.bib300)) further proposed PoseFormerV2,
    which exploits a compact representation of lengthy skeleton sequences in the frequency
    domain to efficiently scale up the receptive field and boost robustness to noisy
    2D joint detection.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构由于自注意力机制已成为自然语言处理中的首选模型，现在在计算机视觉领域也在迅速发展。最近的研究展示了 transformer
    注意力机制在各种视觉任务中的强大全局表示能力（Khan 等， [2021](#bib.bib98)）。Zheng 等（Zheng 等， [2021](#bib.bib305)）提出了第一个纯粹基于
    transformer 的方法，称为 PoseFormer，用于 3D HPE，而不涉及卷积架构。空间 transformer 模块编码了人体关节之间的局部关系，时间
    transformer 模块捕捉了整个序列中帧之间的全局依赖关系。Li 等（Li 等， [2022a](#bib.bib121)）进一步设计了一个多假设 transformer，以利用多个姿态假设的时空表示。Zhao
    等（Zhao 等， [2023](#bib.bib300)）进一步提出了 PoseFormerV2，该方法利用频域中长骨架序列的紧凑表示，以有效地扩大感受野并提高对噪声
    2D 关节检测的鲁棒性。
- en: 'B. Human Mesh Recovery (HMR). HMR methods incorporate parametric body models
    such as SMPL (Loper et al., [2015](#bib.bib144)) to recovery human mesh as shown
    in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1\. Single-view single person 3D HPE ‣ 3.1\.
    3D HPE from monocular RGB images and videos ‣ 3\. 3D human pose estimation ‣ Deep
    Learning-Based Human Pose Estimation: A Survey")(c). The SMPL (Skinned Multi-Person
    Linear) model (Loper et al., [2015](#bib.bib144)) is a widely used model in 3D
    HPE, which can be modeled with natural pose-dependent deformations exhibiting
    soft-tissue dynamics. To learn how people deform with poses, there are 1786 high-resolution
    3D scans of different subjects of poses with template mesh in SMPL to optimize
    the blend weights (Kavan, [2014](#bib.bib96)), pose-dependent blend shapes, the
    mean template shape, and the regressor from vertices to joint locations. The 3D
    pose can be obtained by using the model-defined joint regression matrix (Kolotouros
    et al., [2019](#bib.bib102)). There are also other popular volumetric models such
    as DYNA (Pons-Moll et al., [2015](#bib.bib194)), Stitched Puppet model (Zuffi
    and Black, [2015](#bib.bib318)), Frankenstein & Adam (Joo et al., [2018](#bib.bib94)),
    and GHUM & GHUML(ite) (Xu et al., [2020a](#bib.bib263)).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'B. 人体网格恢复（HMR）。HMR 方法结合了参数化体模，例如 SMPL（Loper 等， [2015](#bib.bib144)），以恢复人体网格，如图
    [4](#S3.F4 "Figure 4 ‣ 3.1.1\. Single-view single person 3D HPE ‣ 3.1\. 3D HPE
    from monocular RGB images and videos ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based
    Human Pose Estimation: A Survey")(c) 所示。SMPL（Skinned Multi-Person Linear）模型（Loper
    等， [2015](#bib.bib144)）是 3D HPE 中广泛使用的模型，可以通过自然姿态依赖的变形来建模，表现出软组织动态。为了了解人们如何随着姿势发生变形，SMPL
    中有 1786 个不同姿势的高分辨率 3D 扫描与模板网格一起使用，以优化混合权重（Kavan， [2014](#bib.bib96)）、姿势依赖的混合形状、平均模板形状以及从顶点到关节位置的回归器。可以使用模型定义的关节回归矩阵（Kolotouros
    等， [2019](#bib.bib102)）来获取 3D 姿态。此外，还有其他流行的体积模型，例如 DYNA（Pons-Moll 等， [2015](#bib.bib194)）、拼接木偶模型（Zuffi
    和 Black， [2015](#bib.bib318)）、Frankenstein & Adam（Joo 等， [2018](#bib.bib94)）以及
    GHUM & GHUML(ite)（Xu 等， [2020a](#bib.bib263)）。'
- en: Volumetric models are used to recover high-quality human mesh, providing extra
    shape information of the human body. As one of the most popular volumetric models,
    the SMPL model (Loper et al., [2015](#bib.bib144)) has been widely used in 3D
    HPE, e.g., (Bogo et al., [2016](#bib.bib11); Kolotouros et al., [2019](#bib.bib102);
    Zhu et al., [2019](#bib.bib314); Arnab et al., [2019](#bib.bib6); Moon and Lee,
    [2020](#bib.bib168); Zhang et al., [2020c](#bib.bib290); Li et al., [2021d](#bib.bib116);
    Zhang et al., [2021](#bib.bib288)), because it is compatible with existing rendering
    engines. Tan et al. (Vince Tan and Cipolla, [2017](#bib.bib240)), Tung et al.
    (Tung et al., [2017](#bib.bib237)), Pavlakos et al. (Pavlakos et al., [2018b](#bib.bib187)),
    and Omran et al. (Omran et al., [2018](#bib.bib177)) regressed SMPL parameters
    to reconstruct 3D human mesh. Instead of predicting SMPL parameters, Kolotouros
    et al. (Kolotouros et al., [2019](#bib.bib103)) regressed the locations of the
    SMPL mesh vertices using a Graph-CNN architecture. Kocabas et al. (Kocabas et al.,
    [2020](#bib.bib99)) included the large-scale motion capture dataset AMASS (Mahmood
    et al., [2019](#bib.bib153)) for adversarial training of their SMPL-based method
    named VIBE (Video Inference for Body Pose and Shape Estimation). VIBE leveraged
    AMASS to discriminate between real human motions and those predicted by the pose
    regression module. Since low-resolution visual content is more common in real-world
    scenarios than high-resolution visual content, existing well-trained models may
    fail when the resolution is degraded. Xu et al. (Xu et al., [2020b](#bib.bib267))
    introduced the contrastive learning scheme into a self-supervised resolution-aware
    SMPL-based network. The self-supervised contrastive learning scheme uses a self-supervision
    loss and a contrastive feature loss to enforce feature and scale consistency.
    Choi et al. (Choi et al., [2021](#bib.bib38)) presented a temporally consistent
    mesh recovery system (named TCMR) to smooth 3D human motion output using a bi-directional
    gated recurrent unit. Kolotouros et al. (Kolotouros et al., [2021](#bib.bib104))
    proposed a probabilistic model using conditional normalizing flow for 3D human
    mesh recovery from 2D evidence. Zheng et al. (Zheng et al., [2022](#bib.bib303))
    designed a lightweight transformer-based method that can reconstruct human mesh
    from 2D human pose with a significant computation and memory cost reduction, while
    the performance is competitive with Pose2Mesh (Choi et al., [2020](#bib.bib39)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 体积模型用于恢复高质量的人体网格，提供额外的人体形状信息。作为最流行的体积模型之一，SMPL模型（Loper et al., [2015](#bib.bib144)）在3D
    HPE中被广泛使用，例如（Bogo et al., [2016](#bib.bib11); Kolotouros et al., [2019](#bib.bib102);
    Zhu et al., [2019](#bib.bib314); Arnab et al., [2019](#bib.bib6); Moon and Lee,
    [2020](#bib.bib168); Zhang et al., [2020c](#bib.bib290); Li et al., [2021d](#bib.bib116);
    Zhang et al., [2021](#bib.bib288)），因为它与现有的渲染引擎兼容。Tan et al.（Vince Tan and Cipolla,
    [2017](#bib.bib240)）、Tung et al.（Tung et al., [2017](#bib.bib237)）、Pavlakos et
    al.（Pavlakos et al., [2018b](#bib.bib187)）和Omran et al.（Omran et al., [2018](#bib.bib177)）回归SMPL参数以重建3D人体网格。Kolotouros
    et al.（Kolotouros et al., [2019](#bib.bib103)）没有预测SMPL参数，而是使用Graph-CNN架构回归SMPL网格顶点的位置。Kocabas
    et al.（Kocabas et al., [2020](#bib.bib99)）将大规模动作捕捉数据集AMASS（Mahmood et al., [2019](#bib.bib153)）包含在其基于SMPL的方法VIBE（Video
    Inference for Body Pose and Shape Estimation）的对抗训练中。VIBE利用AMASS区分真实的人体动作与由姿态回归模块预测的动作。由于低分辨率的视觉内容在现实场景中比高分辨率的视觉内容更为常见，现有的训练良好的模型可能会在分辨率降低时失败。Xu
    et al.（Xu et al., [2020b](#bib.bib267)）将对比学习方案引入到一个自监督的分辨率感知SMPL网络中。自监督对比学习方案使用自监督损失和对比特征损失来强制执行特征和尺度一致性。Choi
    et al.（Choi et al., [2021](#bib.bib38)）提出了一种时间一致的网格恢复系统（命名为TCMR），使用双向门控递归单元来平滑3D人体运动输出。Kolotouros
    et al.（Kolotouros et al., [2021](#bib.bib104)）提出了一种使用条件归一化流的概率模型，以从2D证据中恢复3D人体网格。Zheng
    et al.（Zheng et al., [2022](#bib.bib303)）设计了一种轻量级的基于变换器的方法，可以从2D人体姿态中重建人体网格，显著降低计算和内存成本，同时性能与Pose2Mesh（Choi
    et al., [2020](#bib.bib39)）竞争。
- en: There are a few recent attempts to utilize transformer in HMR (Lin et al., [2021a](#bib.bib131);
    Zheng et al., [2022](#bib.bib303), [2023a](#bib.bib302), [2023b](#bib.bib304)).
    Lin et al. proposed METRO (Lin et al., [2021a](#bib.bib131)) and MeshGraphormer
    (Lin et al., [2021b](#bib.bib132)) that combine CNNs with transformer networks
    to regress SMPL mesh vertices from a single image. However, they pursued higher
    accuracy while sacrificing computation and memory. FeatER (Zheng et al., [2023b](#bib.bib304))
    and POTTER (Zheng et al., [2023a](#bib.bib302)) reduced the computational and
    memory cost by proposing lightweight transformer architectures, while both of
    them outperform METRO by only requiring less than 10%of total parameters and 15%
    MACs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 近期有一些尝试在HMR中利用变换器（Lin et al., [2021a](#bib.bib131); Zheng et al., [2022](#bib.bib303),
    [2023a](#bib.bib302), [2023b](#bib.bib304)）。Lin et al. 提出了METRO（Lin et al., [2021a](#bib.bib131)）和MeshGraphormer（Lin
    et al., [2021b](#bib.bib132)），它们将CNN与变换器网络结合，从单幅图像中回归SMPL网格顶点。然而，他们在追求更高精度的同时牺牲了计算和内存。FeatER（Zheng
    et al., [2023b](#bib.bib304)）和POTTER（Zheng et al., [2023a](#bib.bib302)）通过提出轻量级变换器架构降低了计算和内存成本，同时它们都优于METRO，所需参数少于总参数的10%和MACs的15%。
- en: There are several extended SMPL-based models to address the limitations of the
    SMPL model such as high computational complexity, and lack of hands and facial
    landmarks. SMPLify (Lassner et al., [2017](#bib.bib110)) is an optimization method
    that fits the SMPL model to the detected 2D joints and minimizes the re-projection
    error. Pavlakos et al. (Pavlakos et al., [2019](#bib.bib183)) introduced a new
    model, named SMPL-X, that can also predict fully articulated hands and facial
    landmarks. Following the SMPLify method, they also proposed SMPLify-X, which is
    an improved version learned from AMASS dataset (Mahmood et al., [2019](#bib.bib153)).
    Hassan et al. (Hassan et al., [2019](#bib.bib66)) further extended SMPLify-X to
    PROX – a method enforcing Proximal Relationships with Object eXclusion by adding
    3D environmental constraints. Kolotouros et al. (Kolotouros et al., [2019](#bib.bib102))
    integrated the regression-based and optimization-based SMPL parameter estimation
    methods to a new one named SPIN (SMPL oPtimization IN the loop) while employing
    SMPLify in the training loop. The estimated 2D pose served as the initialization
    in an iterative optimization routine for producing a more accurate 3D pose and
    shape.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个扩展的基于SMPL的模型用于解决SMPL模型的局限性，如高计算复杂度以及缺乏手部和面部标记。SMPLify（Lassner et al., [2017](#bib.bib110)）是一种优化方法，通过将SMPL模型拟合到检测到的2D关节上，并最小化重投影误差。Pavlakos
    et al.（Pavlakos et al., [2019](#bib.bib183)）介绍了一种新模型，命名为SMPL-X，它还可以预测完全可动的手部和面部标记。继SMPLify方法之后，他们还提出了SMPLify-X，这是一个从AMASS数据集（Mahmood
    et al., [2019](#bib.bib153)）学习得到的改进版本。Hassan et al.（Hassan et al., [2019](#bib.bib66)）进一步扩展了SMPLify-X到PROX——一种通过添加3D环境约束来强制执行对象排除的近端关系的方法。Kolotouros
    et al.（Kolotouros et al., [2019](#bib.bib102)）将基于回归和优化的SMPL参数估计方法整合到一个新的方法中，名为SPIN（SMPL
    oPtimization IN the loop），同时在训练循环中使用SMPLify。估计的2D姿态作为迭代优化程序中的初始化，用于生成更准确的3D姿态和形状。
- en: Instead of using the SMPL-based model, other models have also been used for
    recovering 3D human pose or mesh, e.g.,(Qammaz and Argyros, [2019](#bib.bib196);
    Saito et al., [2020](#bib.bib213); Wang et al., [2020a](#bib.bib244)). Chen et
    al. (Cheng et al., [2019](#bib.bib36)) introduced a Cylinder Man Model to generate
    occlusion labels for 3D data and performed data augmentation. A pose regularization
    term was introduced to penalize wrong estimated occlusion labels. Xiang et al.
    (Xiang et al., [2019](#bib.bib259)) utilized the Adam model (Joo et al., [2018](#bib.bib94))
    to reconstruct the 3D motions. A 3D human representation, named 3D Part Orientation
    Fields (POFs), was introduced to encode the 3D orientation of human body parts
    in the 2D space. Wang et al. (Wang et al., [2020a](#bib.bib244)) presented a new
    Bone-level Skinned Model of human mesh, which decouples bone modeling and identity-specific
    variations by setting bone lengths and joint angles. Fisch and Clark (Fisch and
    Clark, [2020](#bib.bib58)) introduced an orientation keypoints model which can
    compute full 3-axis joint rotations including yaw, pitch, and roll for 6D HPE.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用基于SMPL的模型外，其他模型也被用于恢复3D人体姿态或网格，例如，（Qammaz和Argyros，[2019](#bib.bib196)；Saito等，[2020](#bib.bib213)；Wang等，[2020a](#bib.bib244)）。Chen等（Cheng等，[2019](#bib.bib36)）介绍了一种Cylinder
    Man模型，用于为3D数据生成遮挡标签并进行数据增强。引入了一种姿态正则化项来惩罚错误估计的遮挡标签。Xiang等（Xiang等，[2019](#bib.bib259)）利用Adam模型（Joo等，[2018](#bib.bib94)）来重建3D运动。介绍了一种名为3D
    Part Orientation Fields（POFs）的3D人体表示，用于在2D空间中编码人体部位的3D方向。Wang等（Wang等，[2020a](#bib.bib244)）提出了一种新的骨骼级Skinned模型，通过设置骨骼长度和关节角度来解耦骨骼建模和特定身份的变化。Fisch和Clark（Fisch和Clark，[2020](#bib.bib58)）介绍了一种方向关键点模型，该模型可以计算完整的3轴关节旋转，包括偏航、俯仰和滚转，用于6D
    HPE。
- en: 3.1.2\. Single-view multi-person 3D HPE
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 单视图多人物3D HPE
- en: 'For 3D multi-person HPE from monocular RGB images or videos, similar categories
    as 2D multi-person HPE are noted here: top-down approaches and bottom-up approaches
    as shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.1.2\. Single-view multi-person 3D HPE
    ‣ 3.1\. 3D HPE from monocular RGB images and videos ‣ 3\. 3D human pose estimation
    ‣ Deep Learning-Based Human Pose Estimation: A Survey") (a) and Fig. [5](#S3.F5
    "Figure 5 ‣ 3.1.2\. Single-view multi-person 3D HPE ‣ 3.1\. 3D HPE from monocular
    RGB images and videos ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based Human
    Pose Estimation: A Survey") (b), respectively. The comparison between 2D top-down
    and bottom-up approaches in Section [2.2](#S2.SS2 "2.2\. 2D multi-person pose
    estimation ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based Human Pose Estimation:
    A Survey") is applicable to the 3D case.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '对于从单目RGB图像或视频中进行的3D多人物HPE，注意到与2D多人物HPE类似的类别：自上而下的方法和自下而上的方法，如图[5](#S3.F5 "Figure
    5 ‣ 3.1.2\. Single-view multi-person 3D HPE ‣ 3.1\. 3D HPE from monocular RGB
    images and videos ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based Human Pose
    Estimation: A Survey") (a)和图[5](#S3.F5 "Figure 5 ‣ 3.1.2\. Single-view multi-person
    3D HPE ‣ 3.1\. 3D HPE from monocular RGB images and videos ‣ 3\. 3D human pose
    estimation ‣ Deep Learning-Based Human Pose Estimation: A Survey") (b)所示。第[2.2](#S2.SS2
    "2.2\. 2D multi-person pose estimation ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based
    Human Pose Estimation: A Survey")节中2D自上而下和自下而上的方法的比较同样适用于3D情况。'
- en: '![Refer to caption](img/16e73e095948be0d384e3dc09edde286.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/16e73e095948be0d384e3dc09edde286.png)'
- en: Figure 5\. Illustration of the multi-person 3D HPE frameworks. (a) Top-Down
    methods first detect single-person regions by human detection network. For each
    single-person region, individual 3D poses can be estimated by 3D pose network.
    Then all 3D poses are aligned to the world coordinate. (b) Bottom-Up methods first
    estimate all body joints and depth maps, then associate body parts to each person
    according to the root depth and part relative depth. Part of the figure is from
    (Zhen et al., [2020](#bib.bib301)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 多人物3D HPE框架的示意图。 (a) 自上而下的方法首先通过人体检测网络检测单人物区域。对于每个单人物区域，可以通过3D姿态网络估计各自的3D姿态。然后，所有3D姿态都对齐到世界坐标系。
    (b) 自下而上的方法首先估计所有身体关节和深度图，然后根据根深度和部件相对深度将身体部位关联到每个人。部分图像来自（Zhen等，[2020](#bib.bib301)）。
- en: Top-down approaches. Top-down approaches of 3D multi-person HPE first perform
    human detection to detect each individual person. Then for each detected person,
    the absolute root (center joint of the human) coordinate and 3D root-relative
    pose are estimated by 3D pose networks. Based on the absolute root coordinate
    of each person and their root-relative pose, all poses are aligned to the world
    coordinate. Rogez et al. (Rogez et al., [2017](#bib.bib208)) localized candidate
    regions of each person to generate potential poses, and used a regressor to jointly
    refine the pose proposals. This localization-classification-regression method,
    named LCR-Net, performed well on the controlled environment datasets but could
    not generalize well to in-the-wild images. To address this issue, Rogez et al.
    (Rogez et al., [2019](#bib.bib209)) proposed LCR-Net++ by using synthetic data
    augmentation for the training data to improve performance. Zanfir et al. (Zanfir
    et al., [2018](#bib.bib280)) added semantic segmentation to the 3D multi-person
    HPE module with scene constraints. Additionally, the 3D temporal assignment problem
    was tackled by the Hungarian matching method for video-based multi-person 3D HPE.
    Moon et al. (Moon et al., [2019a](#bib.bib166)) introduced a camera distance-aware
    approach under top-down pipeline. The RootNet estimated the camera-centered coordinates
    of human body’s roots. Then the root-relative 3D pose of each cropped human was
    estimated by the proposed PoseNet. Benzine et al. (Benzine et al., [2020](#bib.bib9))
    proposed a single-shot approach named PandaNet (Pose estimAtioN and Detection
    Anchor-based Network). A low-resolution anchor-based representation was introduced
    to avoid the occlusion problem. A pose-aware anchor selection module was developed
    to address the overlapping problem by removing ambiguous anchors. An automatic
    weighting of losses associated with different scales was used to handle the imbalance
    issue of different sizes of people. Li et al. (Li et al., [2020b](#bib.bib114))
    tackled the lack of global information in the top-down approaches. They adopted
    a Hierarchical Multi-person Ordinal Relations method to leverage body-level semantic
    and global consistency for encoding the interaction information hierarchically.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 自顶向下的方法。3D 多人姿态估计的自顶向下方法首先进行人体检测，以检测每个个体。然后，对于每个检测到的人，3D 姿态网络估计绝对根部（人体中心关节）的坐标和
    3D 根部相对姿态。基于每个人的绝对根部坐标和他们的根部相对姿态，所有姿态都对齐到世界坐标。Rogez 等人（Rogez et al., [2017](#bib.bib208)）定位每个人的候选区域以生成潜在姿态，并使用回归器联合优化姿态提案。这种定位-分类-回归方法，名为
    LCR-Net，在受控环境数据集上表现良好，但无法很好地推广到实际环境图像。为解决这一问题，Rogez 等人（Rogez et al., [2019](#bib.bib209)）通过使用合成数据增强训练数据来提高性能，提出了
    LCR-Net++。Zanfir 等人（Zanfir et al., [2018](#bib.bib280)）在 3D 多人姿态估计模块中添加了带有场景约束的语义分割。此外，使用匈牙利匹配方法解决了视频基础上的
    3D 时间分配问题。Moon 等人（Moon et al., [2019a](#bib.bib166)）在自顶向下的流程下引入了一个相机距离感知的方法。RootNet
    估计了人体根部的相机中心坐标。然后，PoseNet 估计了每个人体裁剪图像的根部相对 3D 姿态。Benzine 等人（Benzine et al., [2020](#bib.bib9)）提出了一种名为
    PandaNet（Pose estimAtioN and Detection Anchor-based Network）的单次检测方法。引入了低分辨率的基于锚点的表示，以避免遮挡问题。开发了一个姿态感知的锚点选择模块，通过移除模糊锚点来解决重叠问题。通过自动加权与不同尺度相关的损失，以处理不同人群大小的不平衡问题。Li
    等人（Li et al., [2020b](#bib.bib114)）解决了自顶向下方法中缺乏全局信息的问题。他们采用了分层多人人体序关系方法，以层次化方式利用身体级语义和全局一致性来编码交互信息。
- en: Bottom-up approaches. In contrast to top-down approaches, bottom-up approaches
    first produce all body joint locations and depth maps, then associate body parts
    to each person according to the root depth and part relative depth. A key challenge
    of bottom-up approaches is how to group human body joints belonging to each person.
    Zanfir et al. (Zanfir et al., [2018](#bib.bib281)) formulated the person grouping
    problem as a binary integer programming (BIP) problem. A limb scoring module was
    used to estimate candidate kinematic connections of detected joints and a skeleton
    grouping module assembled limbs into skeletons by solving the BIP problem. Nie
    et al. (Nie et al., [2019](#bib.bib176)) proposed a Single-stage multi-person
    Pose Machine (SPM) to define the unique identity root joint for each person. The
    body joints were aligned to each root joint by using dense displacement maps.
    However, this method is limited in that only paired 2D images and 3D pose annotations
    can be used for supervised learning. Without paired 2D images and 3D pose annotations,
    Kundu et al. (Kundu et al., [2020a](#bib.bib107)) proposed a frozen network to
    exploit the shared latent space between two diverse modalities under a practical
    deployment paradigm such that the learning could be cast as a cross-model alignment
    problem. Fabbri et al. (Fabbri et al., [2020](#bib.bib54)) developed a distance-based
    heuristic for linking joints in a multi-person setting. Specifically, starting
    from detected heads (i.e., the joint with the highest confidence), the remaining
    joints are connected by selecting the closest ones in terms of 3D Euclidean distance.
    Chen et al. (Cheng et al., [2021b](#bib.bib34)) integrated top-down and bottom-up
    approaches in their method. A top-down network first estimates joint heatmaps
    inside each bounding box, then a bottom-up network incorporates estimated joint
    heatmaps to handle the scale variation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自下而上的方法。与自上而下的方法相比，自下而上的方法首先生成所有身体关节位置和深度图，然后根据根深度和部位相对深度将身体部位与每个人关联起来。自下而上的方法的一个关键挑战是如何将属于每个人的人体关节分组。Zanfir
    等人（Zanfir et al., [2018](#bib.bib281)）将人员分组问题表述为二进制整数规划（BIP）问题。一个肢体评分模块用于估计检测到的关节的候选运动学连接，一个骨架分组模块通过解决
    BIP 问题将肢体组装成骨架。Nie 等人（Nie et al., [2019](#bib.bib176)）提出了一个单阶段多人的姿态机器（SPM），为每个人定义唯一的身份根关节。通过使用密集位移图将身体关节对齐到每个根关节。然而，这种方法的限制在于只能使用配对的
    2D 图像和 3D 姿态注释进行监督学习。没有配对的 2D 图像和 3D 姿态注释，Kundu 等人（Kundu et al., [2020a](#bib.bib107)）提出了一种冻结网络，以利用两种不同模态之间的共享潜在空间，实现在实际部署范式下的学习，从而将学习任务视为跨模型对齐问题。Fabbri
    等人（Fabbri et al., [2020](#bib.bib54)）开发了一种基于距离的启发式方法，用于在多人物设置中链接关节。具体来说，从检测到的头部（即置信度最高的关节）开始，通过选择在
    3D 欧几里得距离上最接近的关节来连接剩余的关节。Chen 等人（Cheng et al., [2021b](#bib.bib34)）在他们的方法中整合了自上而下和自下而上的方法。自上而下的网络首先在每个边界框内估计关节热图，然后自下而上的网络结合估计的关节热图来处理尺度变化。
- en: Another challenge of bottom-up approaches is occlusion. To cope with this challenge,
    Metha et al. (Mehta et al., [2018](#bib.bib160)) developed an Occlusion-Robust
    Pose-Maps (ORPM) approach to incorporate redundancy into the location-maps formulation,
    which facilitates person association in the heatmaps, especially for occluded
    scenes. Zhen et al. (Zhen et al., [2020](#bib.bib301)) leveraged a depth-aware
    part association algorithm to assign joints to individuals by reasoning about
    inter-person occlusion and bone-length constraints. Mehta et al. (Mehta et al.,
    [2020](#bib.bib159)) quickly inferred intermediate 3D pose of visible body joints
    regardless of the accuracy. Then the completed 3D pose is reconstructed by inferring
    occluded joints using learned pose priors and global context. The final 3D pose
    was refined by applying temporal coherence and fitting the kinematic skeletal
    model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 自下而上的方法的另一个挑战是遮挡。为了应对这一挑战，Metha 等人（Mehta et al., [2018](#bib.bib160)）开发了一种遮挡鲁棒姿态图（ORPM）方法，将冗余纳入位置图的公式中，从而有助于在热图中进行人员关联，特别是对于被遮挡的场景。Zhen
    等人（Zhen et al., [2020](#bib.bib301)）利用一种深度感知部位关联算法，通过推理人员间遮挡和骨长约束将关节分配给个体。Mehta
    等人（Mehta et al., [2020](#bib.bib159)）迅速推断出可见身体关节的中间 3D 姿态，而不考虑准确性。然后通过推断遮挡关节，使用学习到的姿态先验和全局上下文来重建完成的
    3D 姿态。最终的 3D 姿态通过应用时间一致性和拟合运动学骨架模型来进行优化。
- en: Comparison of top-down and bottom-up approaches. Top-down approaches usually
    achieve promising results by relying on state-of-the-art person detection methods
    and single-person HPE methods. But the computational complexity and the inference
    time may become excessive with the increase in the number of humans, especially
    in crowded scenes. Moreover, since top-down approaches first detect the bounding
    box for each person, global information in the scene may get neglected. The estimated
    depth of cropped region may be inconsistent with the actual depth ordering and
    the predicted human bodies may be placed in overlapping positions. On the contrary,
    the bottom-up approaches enjoy linear computation and time complexity. However,
    if the goal is to recover 3D body mesh, it is not straightforward for the bottom-up
    approaches to reconstruct human body meshes. For top-down approaches, after detecting
    each individual person, human body mesh of each person can be easily recovered
    by incorporating the 3D single-person human mesh recovery method. While for the
    bottom-up approaches, an additional model regressor module is needed to reconstruct
    human body meshes based on the final 3D poses.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层方法与底层方法的比较。顶层方法通常通过依赖最先进的人体检测方法和单人 HPE 方法来获得令人满意的结果。但随着人类数量的增加，计算复杂度和推理时间可能会过高，特别是在拥挤的场景中。此外，由于顶层方法首先检测每个人的边界框，场景中的全局信息可能会被忽视。裁剪区域的估计深度可能与实际深度顺序不一致，预测的人体可能会被放置在重叠的位置。相反，底层方法具有线性计算和时间复杂度。然而，如果目标是恢复
    3D 身体网格，底层方法在重建人体网格时并不直接。对于顶层方法，检测到每个单独的人后，可以通过结合 3D 单人网格恢复方法来轻松恢复每个人的身体网格。而对于底层方法，需要额外的模型回归模块来基于最终的
    3D 姿态重建人体网格。
- en: 3.1.3\. Multi-view 3D HPE
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3. 多视角 3D HPE
- en: Partial occlusion is a challenging problem for 3D HPE in the single-view setting.
    The natural solution to overcome this problem is to estimate 3D human pose from
    multiple views, since the occluded part in one view may become visible in other
    views. In order to reconstruct the 3D pose from multiple views, the association
    of corresponding locations between different cameras needs to be resolved. We
    do not specify single-person or multi-person in this category since the multi-view
    setting is deployed mainly for multi-person pose estimation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 部分遮挡是单视角设置中 3D HPE 的一个挑战性问题。克服这个问题的自然解决方案是从多个视角估计 3D 人体姿态，因为一个视角中的遮挡部分在其他视角中可能会变得可见。为了从多个视角重建
    3D 姿态，需要解决不同摄像机之间的对应位置关联问题。我们在这一类别中没有指定单人还是多人，因为多视角设置主要用于多人姿态估计。
- en: A group of methods (Qiu et al., [2019](#bib.bib199); Liang and Lin, [2019](#bib.bib129);
    Dong et al., [2019](#bib.bib49); Tu et al., [2020](#bib.bib236); Dong et al.,
    [2021](#bib.bib50)) used body models to tackle the association problem by optimizing
    model parameters to match the model projection with the 2D pose. The widely used
    3D pictorial structure model (Burenius et al., [2013](#bib.bib13)) is such a model.
    However, these methods usually need large memory and expensive computational cost,
    especially for multi-person 3D HPE under multi-view settings. Rhodin et al. (Rhodin
    et al., [2018b](#bib.bib207)) employed a multi-view consistency constraint in
    the network, however it requires a large amount of 3D ground-truth training data.
    To overcome this limitation, Rhodin et al. (Rhodin et al., [2018a](#bib.bib206))
    further proposed an encoder-decoder framework to learn the geometry-aware 3D latent
    representation from multi-view images and background segmentation without 3D annotations.
    Chen et al. (Chen et al., [2020b](#bib.bib21)), Mitra et al. (Mitra et al., [2020](#bib.bib163)),
    Zhang et al.(Zhang et al., [2020a](#bib.bib293)), and Huang et al. (Huang et al.,
    [2020b](#bib.bib70)) proposed multi-view matching frameworks to reconstruct 3D
    human pose across all viewpoints with consistency constraints. Pavlakos et al.
    (Pavlakos et al., [2017b](#bib.bib186)) and Zhang et al. (Zhang et al., [2020g](#bib.bib296))
    aggregated the 2D keypoint heatmaps of multi-view images into a 3D pictorial structure
    model based on all the calibrated camera parameters. However, when multi-view
    camera environments change, the model needs to be retrained. Qiu et al. (Qiu et al.,
    [2019](#bib.bib199)), and Kocabas et al. (Kocabas et al., [2019](#bib.bib101))
    employed epipolar geometry to match paired multi-view poses for 3D pose reconstruction
    and generalized their methods to new multi-view camera environments. It should
    be noted that matching each pair of views separately without the cycle consistency
    constraint may lead to incorrect 3D pose reconstructions (Dong et al., [2019](#bib.bib49)).
    Tu et al. (Tu et al., [2020](#bib.bib236)) aggregated all the features in each
    camera view in the 3D voxel space to avoid incorrect estimation in each camera
    view. A cuboid proposal network and a pose regression network were designed to
    localize all people and to estimate the 3D pose, respectively. When given sufficient
    viewpoints (more than ten), it is not practical to use all viewpoints for 3D pose
    estimation. Pirinen et al. (Pirinen et al., [2019](#bib.bib192)) proposed a self-supervised
    reinforcement learning approach to select a small set of viewpoints to reconstruct
    the 3D pose via triangulation. Wang et al. (Wang et al., [2021](#bib.bib252))
    introduced a transformer-based model that directly regresses 3D poses from multi-view
    images without relying on any intermediate task. The proposed Multi-view Pose
    transformer (MvP) was designed to represent query embedding of multi-person joints.
    The multi-view information was fused by a novel geometrically guided attention
    mechanism.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一组方法（Qiu et al., [2019](#bib.bib199); Liang and Lin, [2019](#bib.bib129); Dong
    et al., [2019](#bib.bib49); Tu et al., [2020](#bib.bib236); Dong et al., [2021](#bib.bib50)）使用身体模型通过优化模型参数来解决关联问题，以使模型投影与2D姿态匹配。广泛使用的3D图像结构模型（Burenius
    et al., [2013](#bib.bib13)）就是这样的模型。然而，这些方法通常需要大量内存和昂贵的计算成本，特别是在多视角设置下进行多人3D HPE时。Rhodin
    et al.（Rhodin et al., [2018b](#bib.bib207)）在网络中采用了多视角一致性约束，但这需要大量的3D真实数据来进行训练。为了克服这一限制，Rhodin
    et al.（Rhodin et al., [2018a](#bib.bib206)）进一步提出了一种编码器-解码器框架，以从多视角图像和背景分割中学习几何感知的3D潜在表示，而无需3D注释。Chen
    et al.（Chen et al., [2020b](#bib.bib21)）、Mitra et al.（Mitra et al., [2020](#bib.bib163)）、Zhang
    et al.（Zhang et al., [2020a](#bib.bib293)）和Huang et al.（Huang et al., [2020b](#bib.bib70)）提出了多视角匹配框架，以在所有视点之间重建3D人体姿态并施加一致性约束。Pavlakos
    et al.（Pavlakos et al., [2017b](#bib.bib186)）和Zhang et al.（Zhang et al., [2020g](#bib.bib296)）将多视角图像的2D关键点热图汇总到一个基于所有校准相机参数的3D图像结构模型中。然而，当多视角相机环境发生变化时，模型需要重新训练。Qiu
    et al.（Qiu et al., [2019](#bib.bib199)）和Kocabas et al.（Kocabas et al., [2019](#bib.bib101)）使用极几何来匹配配对的多视角姿态进行3D姿态重建，并将他们的方法推广到新的多视角相机环境中。需要注意的是，没有循环一致性约束的情况下分别匹配每对视图可能会导致不正确的3D姿态重建（Dong
    et al., [2019](#bib.bib49)）。Tu et al.（Tu et al., [2020](#bib.bib236)）将每个相机视图中的所有特征聚合到3D体素空间中，以避免每个相机视图中的不正确估计。设计了一个立方体提议网络和一个姿态回归网络，分别用于定位所有人物和估计3D姿态。当提供足够的视点（超过十个）时，使用所有视点进行3D姿态估计是不实际的。Pirinen
    et al.（Pirinen et al., [2019](#bib.bib192)）提出了一种自监督强化学习方法，通过三角测量选择一小组视点来重建3D姿态。Wang
    et al.（Wang et al., [2021](#bib.bib252)）引入了一种基于变压器的模型，直接从多视角图像中回归3D姿态，而无需依赖任何中间任务。提出的多视角姿态变压器（MvP）被设计为表示多人的关节查询嵌入。通过一种新颖的几何引导注意机制融合了多视角信息。
- en: Besides accuracy, lightweight architecture, fast inference time, and efficient
    adaptation to new camera settings also need to be taken into consideration in
    multi-view HPE. In contrast to (Dong et al., [2019](#bib.bib49)) which matched
    all view inputs together, Chen et al. (Chen et al., [2020a](#bib.bib23)) applied
    an iterative processing strategy to match 2D poses of each view with the 3D pose
    while the 3D pose was updated iteratively. Compared to previous methods whose
    running time may explode with the increase in the number of cameras, their method
    has linear time complexity. Remelli et al. (Remelli et al., [2020](#bib.bib204))
    encoded images of each view into a unified latent representation so that the feature
    maps were disentangled from camera viewpoints. As a lightweight canonical fusion,
    these 2D representations are lifted to the 3D pose using a GPU-based Direct Linear
    Transform to accelerate processing. In order to improve the generalization ability
    of the multi-view fusion scheme, Xie et al. (Xie et al., [2020](#bib.bib261))
    proposed a pre-trained multi-view fusion model (MetaFuse), which can be efficiently
    adapted to new camera settings with few labeled data. They deployed the model-agnostic
    meta-learning framework to learn the optimal initialization of the generic fusion
    model for adaptation. To reduce the computational cost of the VoxelPose (Tu et al.,
    [2020](#bib.bib236)), Ye et al. presented Faster VoxelPose (Ye et al., [2022](#bib.bib274))
    which re-projects the feature volume to three two-dimensional coordinate planes
    for estimating X, Y, Z coordinates from them separately. The fps of Faster VoxelPose
    is 31.1, which is almost 10 $\times$ speed up compared to VoxelPose.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确性，多视角HPE还需要考虑轻量级架构、快速推理时间和对新相机设置的高效适应性。与（Dong等人，[2019](#bib.bib49)）将所有视图输入匹配在一起不同，陈等人（Chen等人，[2020a](#bib.bib23)）采用了迭代处理策略，将每个视图的2D姿势与3D姿势匹配，同时迭代更新3D姿势。与之前的方法相比，随着相机数量的增加，其运行时间可能会爆炸性增长，而他们的方法具有线性时间复杂度。雷梅利等人（Remelli等人，[2020](#bib.bib204)）将每个视图的图像编码为统一的潜在表示，以便特征图脱离摄像机视角。作为轻量级的规范融合，这些2D表示使用基于GPU的直接线性变换被提升到3D姿势以加速处理。为了提高多视角融合方案的泛化能力，谢等人（Xie等人，[2020](#bib.bib261)）提出了预训练的多视角融合模型（MetaFuse），能够通过少量标记数据有效地适应新相机设置。他们采用了模型无关的元学习框架来学习通用融合模型的最佳初始化以用于适应。为了减少VoxelPose（Tu等人，[2020](#bib.bib236)）的计算成本，叶等人提出了更快速的VoxelPose（Ye等人，[2022](#bib.bib274)），它将特征体重新投影到三个二维坐标平面上，分别从中估计X、Y、Z坐标。更快速的VoxelPose的帧率为31.1，与VoxelPose相比几乎提速了10倍。
- en: 3.2\. 3D HPE from other sources
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 其他来源的3D HPE
- en: Although a monocular RGB camera is the most common device used for 3D HPE, other
    sensors (e.g., depth sensor, IMUs, and radio frequency device) are also used for
    this purpose.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然单目RGB相机是用于3D HPE最常见的设备，其他传感器（例如，深度传感器、IMU和射频设备）也被用于此目的。
- en: 'Depth and point cloud sensors: Depth sensors have gained more attention recently
    for 3D computer vision tasks due to their low-cost and increased utilization.
    As one of the key challenges in 3D HPE, the depth ambiguity problem can be alleviated
    by using depth sensors. Yu et al. (Yu et al., [2019](#bib.bib276)), Xiong et al.
    (Xiong et al., [2019](#bib.bib262)), Kadkhodamohammadi et al. (Kadkhodamohammadi
    et al., [2017](#bib.bib95)), and Zhi et al. (Zhi et al., [2020](#bib.bib306))
    utilized depth images to estimate 3D human pose.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 深度和点云传感器：由于低成本和增加的利用率，深度传感器最近在3D计算机视觉任务中受到了更多关注。作为3D HPE中的关键挑战之一，深度歧义问题可以通过使用深度传感器来缓解。于等人（Yu等人，[2019](#bib.bib276)）、熊等人（Xiong等人，[2019](#bib.bib262)）、卡德霍达莫哈迈迪等人（Kadkhodamohammadi等人，[2017](#bib.bib95)）和志等人（Zhi等人，[2020](#bib.bib306)）利用深度图像来估计3D人体姿势。
- en: Compared with depth images, point clouds can provide more information. The state-of-the-art
    point cloud feature extraction techniques, PointNet (Qi et al., [2017a](#bib.bib197))
    and PointNet++ (Qi et al., [2017b](#bib.bib198)), have demonstrated excellent
    performance for classification and segmentation tasks. Jiang et al. (Jiang et al.,
    [2019](#bib.bib87)) and Wang et al. (Wang et al., [2020c](#bib.bib250)) combined
    PointNet++ with the 3D human body model to recover 3D human mesh.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度图像相比，点云可以提供更多信息。当代点云特征提取技术PointNet（Qi等人，[2017a](#bib.bib197)）和PointNet++（Qi等人，[2017b](#bib.bib198)）在分类和分割任务中表现出卓越的性能。江等人（Jiang等人，[2019](#bib.bib87)）和王等人（Wang等人，[2020c](#bib.bib250)）将PointNet++与3D人体模型结合起来，实现了3D人体网格的还原。
- en: 'IMUs with monocular images: Wearable Inertial Measurement Units (IMUs) can
    track the orientation and acceleration of human body parts by recording motions
    without object occlusions and clothes obstructions. Marcard et al.(Von Marcard
    et al., [2017](#bib.bib242); von Marcard et al., [2018](#bib.bib241)), Huang et
    al. (Huang et al., [2018](#bib.bib75)), Zhang et al. (Zhang et al., [2020f](#bib.bib295)),
    and Huang et al. (Huang et al., [2020c](#bib.bib71)) proposed IMU-based HPE methods
    to reconstruct 3D human pose. However, drifting may occur over time when using
    IMUs.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 单目图像的惯性测量单元（IMUs）：可穿戴的惯性测量单元（IMUs）可以通过记录运动来跟踪人体部位的方向和加速度，而不受物体遮挡和衣物阻碍的影响。Marcard等（Von
    Marcard et al., [2017](#bib.bib242); von Marcard et al., [2018](#bib.bib241)）、Huang等（Huang
    et al., [2018](#bib.bib75)）、Zhang等（Zhang et al., [2020f](#bib.bib295)）以及Huang等（Huang
    et al., [2020c](#bib.bib71)）提出了基于IMU的HPE方法来重建3D人体姿势。然而，使用IMU时可能会随着时间发生漂移。
- en: 'Radio frequency device: Radio Frequency (RF) based sensing technology has also
    been used for 3D HPE, e.g., (Zhao et al., [2018](#bib.bib299)) and (Zhao et al.,
    [2019a](#bib.bib298)). The ability to traverse walls and bounce off human bodies
    in the WiFi range without carrying wireless transmitters is the major advantage
    of deploying an RF-based sensing system. Also, privacy can be preserved due to
    non-visual data. However, RF signals have a relatively low spatial resolution
    compared to visual camera images, and RF systems have been shown to generate coarse
    3D pose estimation.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 射频设备：射频（RF）基础的传感技术也被用于3D HPE，例如（Zhao et al., [2018](#bib.bib299)）和（Zhao et al.,
    [2019a](#bib.bib298)）。在WiFi范围内穿越墙壁和从人体反射而无需携带无线发射器是部署RF基础传感系统的主要优势。此外，由于数据非视觉化，隐私可以得到保护。然而，与视觉摄像机图像相比，RF信号的空间分辨率相对较低，并且RF系统已被证明生成粗糙的3D姿势估计。
- en: 'Other sensors/sources: Besides using the aforementioned sensors, Isogawa et
    al. (Isogawa et al., [2020](#bib.bib80)) estimated 3D human pose from the 3D spatio-temporal
    histogram of photons captured by a non-line-of-sight (NLOS) imaging system. Some
    methods (Tome et al., [2019](#bib.bib231); Tome et al., [2020](#bib.bib230); Xu
    et al., [2019](#bib.bib266)) tackled the egocentric 3D pose estimation via a fish-eye
    camera. Saini et al. (Saini et al., [2019](#bib.bib211)) estimated human motion
    using images captured by multiple Autonomous Micro Aerial Vehicles (MAVs). Clever
    et al. (Clever et al., [2020](#bib.bib44)) focused on the HPE of the rest position
    in bed from pressure images which were collected by a pressure sensing mat.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其他传感器/来源：除了使用上述传感器外，Isogawa等（Isogawa et al., [2020](#bib.bib80)）通过非视线（NLOS）成像系统捕获的光子3D时空直方图来估计3D人体姿势。一些方法（Tome
    et al., [2019](#bib.bib231); Tome et al., [2020](#bib.bib230); Xu et al., [2019](#bib.bib266)）通过鱼眼摄像头处理自我中心的3D姿势估计。Saini等（Saini
    et al., [2019](#bib.bib211)）利用多台自主微型航空器（MAVs）捕获的图像来估计人体运动。Clever等（Clever et al.,
    [2020](#bib.bib44)）专注于从压力感应垫收集的压力图像中进行床上休息姿势的HPE。
- en: 3.3\. 3D HPE Summary
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 3D HPE总结
- en: 3D HPE has made significant advancements in recent years. Since a large number
    of 3D HPE methods apply the 2D to 3D lifting strategy, the performance of 3D HPE
    has been improved considerably due to the progress made in 2D HPE. Some 2D HPE
    methods such as OpenPose (Cao et al., [2017](#bib.bib17)), AlphaPose (Fang et al.,
    [2017](#bib.bib56)), and HRNet (Sun et al., [2019](#bib.bib222)) have been extensively
    used as 2D pose detectors in 3D HPE methods. Besides the 3D pose, some methods
    also recover 3D human mesh from images or videos, e.g., (Kolotouros et al., [2019](#bib.bib102);
    Kocabas et al., [2020](#bib.bib99); Zeng et al., [2020](#bib.bib283); Zhou et al.,
    [2020](#bib.bib307)). However, despite the progress made so far, there are still
    several challenges.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 3D HPE在近年来取得了显著的进展。由于大量3D HPE方法应用了2D到3D的提升策略，3D HPE的性能由于2D HPE的进步而得到了显著改善。一些2D
    HPE方法如OpenPose（Cao et al., [2017](#bib.bib17)）、AlphaPose（Fang et al., [2017](#bib.bib56)）和HRNet（Sun
    et al., [2019](#bib.bib222)）已广泛用作3D HPE方法中的2D姿势检测器。除了3D姿势外，一些方法还从图像或视频中恢复3D人体网格，例如（Kolotouros
    et al., [2019](#bib.bib102); Kocabas et al., [2020](#bib.bib99); Zeng et al.,
    [2020](#bib.bib283); Zhou et al., [2020](#bib.bib307)）。然而，尽管取得了进展，仍面临若干挑战。
- en: One challenge is model generalization. High-quality 3D ground truth pose annotations
    depend on motion capture systems that cannot be easily deployed in a random environment.
    Therefore, the existing datasets are mainly captured in constrained scenes. The
    state-of-the-art methods can achieve promising results on these datasets, but
    their performance degrades when applied to in-the-wild data. It is possible to
    leverage gaming engines to generate synthetic datasets with diverse poses and
    complex scenes, e.g., SURREAL dataset (Varol et al., [2017](#bib.bib239)) and
    GTA-IM dataset (Cao et al., [2020](#bib.bib16)). However, learning from synthetic
    data may not achieve the desired performance due to a gap between synthetic and
    real data distributions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个挑战是模型的泛化能力。高质量的3D真实姿态标注依赖于难以在随机环境中部署的动作捕捉系统。因此，现有的数据集主要在受限场景中捕获。最先进的方法在这些数据集上能够取得令人满意的结果，但在应用于实际环境数据时，性能会下降。可以利用游戏引擎生成具有多样姿态和复杂场景的合成数据集，例如SURREAL数据集（Varol
    等，[2017](#bib.bib239)）和GTA-IM数据集（Cao 等，[2020](#bib.bib16)）。然而，由于合成数据和真实数据分布之间存在差距，从合成数据中学习可能无法达到预期的性能。
- en: Like 2D HPE, robustness to occlusion and computational efficiency are two key
    challenges for 3D HPE as well. The performance of current 3D HPE methods drops
    considerably in crowded scenarios due to severe mutual occlusion and possibly
    low-resolution content of each person. 3D HPE is more computation-demanding than
    2D HPE. For example, 2D to 3D lifting approaches rely on 2D poses as intermediate
    representations for inferring 3D poses. It is critical to develop computationally
    efficient 2D HPE pipelines while maintaining high accuracy for pose estimation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与2D HPE类似，3D HPE也面临遮挡鲁棒性和计算效率两个关键挑战。由于严重的相互遮挡和每个人可能存在的低分辨率内容，当前3D HPE方法在拥挤场景中的性能会显著下降。3D
    HPE比2D HPE更具计算需求。例如，2D到3D的提升方法依赖于2D姿态作为推断3D姿态的中间表示。在保持高精度姿态估计的同时，开发计算效率高的2D HPE管道至关重要。
- en: 4\. Datasets and Evaluation Metrics
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 数据集和评估指标
- en: Datasets are very much needed in conducting HPE. They are also necessary to
    provide a fair comparison among different algorithms. In this section, we present
    the most widely used datasets and evaluation metrics for evaluating 2D and 3D
    deep learning-based HPE methods. The results achieved by existing methods on these
    popular datasets are summarized.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在进行HPE时是非常必要的。它们还用于提供不同算法之间的公平比较。在本节中，我们介绍了用于评估2D和3D深度学习基础的HPE方法的最广泛使用的数据集和评估指标。总结了现有方法在这些热门数据集上的结果。
- en: 4.1\. Datasets for 2D HPE
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 2D HPE的数据集
- en: Although there are several datasets used for 2D HPE tasks before 2014, only
    a few recent works use these datasets because they have several shortcomings such
    as a lack of diverse object movements and limited data. Since deep learning-based
    approaches are fueled by a large amount of training data, this section mainly
    discusses the recent and large-scale 2D human pose datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在2014年之前已经有几个数据集用于2D HPE任务，但由于这些数据集存在一些缺陷，如缺乏多样的物体运动和数据有限，最近只有少数几项工作使用了这些数据集。由于基于深度学习的方法依赖于大量的训练数据，本节主要讨论了最近和大规模的2D人体姿态数据集。
- en: '[Max Planck Institute for Informatics (MPII) Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/#)
    (Andriluka et al., [2014](#bib.bib4)) is a popular dataset for evaluation of articulated
    HPE which includes around 25,000 images containing over 40,000 individuals with
    annotated body joints. Rich annotations including body part occlusions, 3D torso,
    and head orientations are labeled by workers on Amazon Mechanical Turk. Images
    in MPII are suitable for 2D single-person or multi-person HPE.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[马克斯·普朗克信息学研究所（MPII）人体姿态数据集](http://human-pose.mpi-inf.mpg.de/#)（Andriluka
    等，[2014](#bib.bib4)）是一个用于评估关节人体姿态估计（HPE）的热门数据集，包括约25,000张图像，涵盖了40,000多个个体及其标注的身体关节。数据集中的丰富标注由亚马逊机械土耳其的工人提供，包括身体部位遮挡、3D躯干和头部朝向等。MPII中的图像适用于2D单人或多人HPE。'
- en: '[Microsoft Common Objects in Context (COCO) Dataset](https://cocodataset.org/#home)
    (Lin et al., [2014](#bib.bib133)) is the most widely used large-scale dataset.
    It has more than 330,000 images and 200,000 labeled subjects with keypoints, and
    each individual person is labeled with 17 joints. In addition, Jin et al. (Jin
    et al., [2020b](#bib.bib90)) proposed COCO-WholeBody Dataset with whole-body annotations
    for HPE.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[Microsoft Common Objects in Context (COCO) 数据集](https://cocodataset.org/#home)
    (Lin et al., [2014](#bib.bib133)) 是使用最广泛的大规模数据集。它包含超过330,000张图像和200,000个带有关键点的标注主体，每个人体被标注了17个关节。此外，Jin
    et al. (Jin et al., [2020b](#bib.bib90)) 提出了带有全身注释的 COCO-WholeBody 数据集，用于HPE。'
- en: '[PoseTrack Dataset](https://posetrack.net/) (Andriluka et al., [2018a](#bib.bib2))
    is a large-scale dataset for HPE and articulated tracking in videos, including
    body part occlusion and truncation in crowded environments. There are two versions
    for PoseTrack dataset: PoseTrack2017 (Andriluka et al., [2018a](#bib.bib2)) contains
    514 video sequences with 16,219 pose annotations, which are split into 250 (training),
    50 (validation), and 214 (testing) sequences. PoseTrack2018 (Andriluka et al.,
    [2018b](#bib.bib3)) has 1,138 video sequences with 153,615 pose annotations, which
    are divided into 593 for training, 170 for validation, and 375 for testing. Each
    person in PoseTrack is labeled with 15 joints and an additional label for keypoint
    visibility.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[PoseTrack 数据集](https://posetrack.net/) (Andriluka et al., [2018a](#bib.bib2))
    是一个用于HPE和视频中关节跟踪的大规模数据集，包括身体部位遮挡和拥挤环境中的截断。PoseTrack 数据集有两个版本：PoseTrack2017 (Andriluka
    et al., [2018a](#bib.bib2)) 包含514个视频序列和16,219个姿态注释，分为250（训练）、50（验证）和214（测试）序列。PoseTrack2018
    (Andriluka et al., [2018b](#bib.bib3)) 包含1,138个视频序列和153,615个姿态注释，分为593个用于训练、170个用于验证和375个用于测试。每个人在
    PoseTrack 中标注了15个关节，并附加了关键点可见性的标签。'
- en: 'We refer the readers to the original references for details about other datasets
    including LSP (Johnson and Everingham, [2010](#bib.bib91)), FLIC (Sapp and Taskar,
    [2013](#bib.bib214)), AIC-HKD (Wu et al., [2017](#bib.bib258)), CrowdPose (Li
    et al., [2019b](#bib.bib115)), Penn Action (Zhang et al., [2013](#bib.bib292)),
    J-HMDB (Jhuang et al., [2013](#bib.bib84)), and HiEve (Lin et al., [2020](#bib.bib134)).
    A summary of these datasets is presented in Table [1](#S4.T1 "Table 1 ‣ 4.1\.
    Datasets for 2D HPE ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based
    Human Pose Estimation: A Survey").'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '我们建议读者查阅原始文献，以获取有关其他数据集的详细信息，包括 LSP (Johnson and Everingham, [2010](#bib.bib91))，FLIC
    (Sapp and Taskar, [2013](#bib.bib214))，AIC-HKD (Wu et al., [2017](#bib.bib258))，CrowdPose
    (Li et al., [2019b](#bib.bib115))，Penn Action (Zhang et al., [2013](#bib.bib292))，J-HMDB
    (Jhuang et al., [2013](#bib.bib84)) 和 HiEve (Lin et al., [2020](#bib.bib134))。这些数据集的总结见表
    [1](#S4.T1 "Table 1 ‣ 4.1\. Datasets for 2D HPE ‣ 4\. Datasets and Evaluation
    Metrics ‣ Deep Learning-Based Human Pose Estimation: A Survey")。'
- en: Table 1\. Datasets for 2D HPE.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 2D HPE 数据集。
- en: '|         Image-based datasets |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|         基于图像的数据集 |'
- en: '| --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|         Name |         Year |          Single-Person         /Multi-Person
    |         Joints |         Number of images |          Evaluation         protocol
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|         名称 |         年份 |          单人/多人 |         关节 |         图像数量 |         
    评价协议 |'
- en: '|         Train |         Val |         Test |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|         训练 |         验证 |         测试 |'
- en: '|         LSP (Johnson and Everingham, [2010](#bib.bib91)) |         2010 |
            Single |         14 |         1k |         - |         1k |         PCP
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|         LSP (Johnson and Everingham, [2010](#bib.bib91)) |         2010 |
            单人 |         14 |         1k |         - |         1k |         PCP |'
- en: '|         LSP-extended (Johnson and Everingham, [2011](#bib.bib92)) |         2011
    |         Single |         14 |         10k |         - |         - |         PCP
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|         LSP-extended (Johnson and Everingham, [2011](#bib.bib92)) |         2011
    |         单人 |         14 |         10k |         - |         - |         PCP
    |'
- en: '|         FLIC (Sapp and Taskar, [2013](#bib.bib214)) |         2013 |         Single
    |         10 |         5k |         - |         1k |         PCP&PCK |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|         FLIC (Sapp and Taskar, [2013](#bib.bib214)) |         2013 |         单人
    |         10 |         5k |         - |         1k |         PCP&PCK |'
- en: '|         FLIC-full (Sapp and Taskar, [2013](#bib.bib214)) |         2013 |
            Single |         10 |         20k |         - |         - |         PCP&PCK
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|         FLIC-full (Sapp and Taskar, [2013](#bib.bib214)) |         2013 |
            单人 |         10 |         20k |         - |         - |         PCP&PCK
    |'
- en: '|         FLIC-plus (Tompson et al., [2014](#bib.bib233)) |         2014 |
            Single |         10 |         17k |         - |         - |         PCP&PCK
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|         FLIC-plus (Tompson et al., [2014](#bib.bib233)) |         2014 |
            单人 |         10 |         17k |         - |         - |         PCP&PCK
    |'
- en: '|         MPII (Andriluka et al., [2014](#bib.bib4)) |         2014 |         Single
    |         16 |         29k |         - |         12k |         PCPm/PCKh |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|         MPII (Andriluka等人，[2014](#bib.bib4)) |         2014 |         单人
    |         16 |         29k |         - |         12k |         PCPm/PCKh |'
- en: '|         Multiple |         16 |         3.8k |         - |         1.7k |
            mAp |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|         多人 |         16 |         3.8k |         - |         1.7k |         mAp
    |'
- en: '|         COCO2016 (Lin et al., [2014](#bib.bib133)) |         2016 |         Multiple
    |         17 |         45k |         22k |         80k |         AP |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|         COCO2016（Lin等人，[2014](#bib.bib133)） |         2016 |         多人 |
            17 |         45k |         22k |         80k |         AP |'
- en: '|         COCO2017 (Lin et al., [2014](#bib.bib133)) |         2017 |         Multiple
    |         17 |         64k |         2.7k |         40k |         AP |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|         COCO2017（Lin等人，[2014](#bib.bib133)） |         2017 |         多人 |
            17 |         64k |         2.7k |         40k |         AP |'
- en: '|         AIC-HKD (Wu et al., [2017](#bib.bib258)) |         2017 |         Multiple
    |         14 |         210k |         30k |         60k |         AP |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|         AIC-HKD（Wu等人，[2017](#bib.bib258)） |         2017 |         多人 |         14
    |         210k |         30k |         60k |         AP |'
- en: '|         CrowdPose (Li et al., [2019b](#bib.bib115)) |         2019 |         Multiple
    |         14 |         10k |         2k |         8k |         mAP |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|         CrowdPose (Li等人，[2019b](#bib.bib115)) |         2019 |         多人
    |         14 |         10k |         2k |         8k |         mAP |'
- en: '|         Video-based datasets |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|         基于视频的数据集 |'
- en: '|         Name |         Year |          Single-Person         /Multi-Person
    |         Joints |         Number of videos |          Evaluation         protocol
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|         名称 |         年份 |         单人 / 多人 |         关节数 |         视频数量 |
            评估协议 |'
- en: '|         Train |         Val |         Test |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|         训练集 |         验证集 |         测试集 |'
- en: '|         Penn Action (Zhang et al., [2013](#bib.bib292)) |         2013 |
            Single |         13 |         1k |         - |         1k |         -
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|         Penn Action（Zhang等人，[2013](#bib.bib292)） |         2013 |         单人
    |         13 |         1k |         - |         1k |         - |'
- en: '|         J-HMDB (Jhuang et al., [2013](#bib.bib84)) |         2013 |         Single
    |         15 |         0.6k |         - |         0.3k |         - |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|         J-HMDB (Jhuang等人，[2013](#bib.bib84)) |         2013 |         单人
    |         15 |         0.6k |         - |         0.3k |         - |'
- en: '|         PoseTrack2017 (Andriluka et al., [2018a](#bib.bib2)) |         2017
    |         Multiple |         15 |         250 |         50 |         214 |         mAP
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|         PoseTrack2017（Andriluka等人，[2018a](#bib.bib2)） |         2017 |         多人
    |         15 |         250 |         50 |         214 |         mAP |'
- en: '|         PoseTrack2018 (Andriluka et al., [2018b](#bib.bib3)) |         2018
    |         Multiple |         15 |         593 |         170 |         375 |         mAP
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|         PoseTrack2018 (Andriluka等人，[2018b](#bib.bib3)) |         2018 |         多人
    |         15 |         593 |         170 |         375 |         mAP |'
- en: '|         HiEve (Lin et al., [2020](#bib.bib134)) |         2020 |         Multiple
    |         14 |         19 |         - |         13 |         mAP |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|         HiEve（Lin等人，[2020](#bib.bib134)） |         2020 |         多人 |         14
    |         19 |         - |         13 |         mAP |'
- en: 4.2\. Evaluation Metrics for 2D HPE
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 2D HPE的评估指标
- en: It is difficult to precisely evaluate the performance of HPE because there are
    many features and requirements that need to be considered (e.g., upper/full human
    body, single/multiple pose estimation, the size of human body). As a result, many
    evaluation metrics have been used for 2D HPE. Here we summarize the commonly used
    ones.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要考虑许多特征和要求（例如，上半身/全身的人体，单人/多人姿势估计，人体的大小），因此很难准确评估2D HPE的性能。因此，常用许多评估指标来对其进行评估。在这里，我们总结了常用的评估指标。
- en: Percentage of Correct Parts (PCP) (Eichner et al., [2012](#bib.bib52)) is a
    measure commonly used in early works on 2D HPE, which evaluates stick predictions
    to report the localization accuracy for limbs. The localization of limbs is determined
    when the distance between the predicted joint and ground truth joint is less than
    a fraction of the limb length (between 0.1 to 0.5). In some works, the PCP measure
    is also referred to as PCP@0.5, where the threshold is 0.5\. This measure is used
    for single-person HPE evaluation. However, PCP has not been widely implemented
    in the latest works because it penalizes the limbs with short lengths which are
    hard to detect. The performance of a model is considered better when it has a
    higher PCP measure. In order to address the drawbacks of PCP, Percentage of Detected
    Joints (PDJ) is introduced, where a predicted joint is considered as detected
    if the distance between predicted joints and true joints is within a certain fraction
    of the torso diameter(Toshev and Szegedy, [2014](#bib.bib234)).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正确部位百分比（PCP）（艾希纳等，[2012](#bib.bib52)）是早期2D姿态估计工作中常用的度量，它评估骨架预测以报告四肢的定位准确性。当预测关节与真实关节之间的距离小于四肢长度的一部分（0.1到0.5之间）时，确定四肢的定位。在一些工作中，PCP度量也称为PCP@0.5，其中阈值为0.5。此度量用于单人姿态估计评估。然而，由于PCP对难以检测的短长度四肢给予惩罚，因此在最新的工作中并未广泛应用。模型的性能被认为更好，当其PCP度量更高。为了解决PCP的缺点，引入了检测到的关节百分比（PDJ），如果预测关节与真实关节之间的距离在躯干直径的某一部分内，则认为预测关节被检测到（托谢夫和塞格迪，[2014](#bib.bib234)）。
- en: Percentage of Correct Keypoints (PCK) (Yang and Ramanan, [2012](#bib.bib273))
    is also used to measure the accuracy of localization of different keypoints within
    a given threshold. The threshold is set to 50 percent of the head segment length
    of each test image and it is denoted as PCKh@0.5\. PCK is referred to as PCK@0.2
    when the distance between detected joints and true joints is less than 0.2 times
    the torso diameter. The higher the PCK value, the better model performance is
    regarded.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正确关键点百分比（PCK）（杨和拉马南，[2012](#bib.bib273)）也用于测量在给定阈值内不同关键点的定位准确性。阈值设置为每个测试图像头部段长度的50%，表示为PCKh@0.5。PCK被称为PCK@0.2，当检测到的关节与真实关节之间的距离小于躯干直径的0.2倍时。PCK值越高，模型性能越好。
- en: Average Precision (AP) and Average Recall (AR). AP measure is an index to measure
    the accuracy of keypoints detection according to precision (the ratio of true
    positive results to the total positive results) and recall (the ratio of true
    positive results to the total number of ground truth positives). AP computes the
    average precision value for recall over 0 to 1\. AP has several similar variants.
    For example, Average Precision of Keypoints (APK) is introduced in (Yang and Ramanan,
    [2012](#bib.bib273)). Mean Average Precision (mAP), which is the mean of average
    precision over all classes, is a widely used metric on the MPII and PoseTrack
    datasets. Average Recall (AR) is another metric used in the COCO keypoint evaluation
    (Lin et al., [2014](#bib.bib133)). Object Keypoint Similarity (OKS) plays a similar
    role as the Intersection over Union (IoU) in object detection and is used for
    AP or AR. This measure is computed from the scale of the subject and the distance
    between predicted points and ground truth points. The COCO evaluation usually
    uses mAP across 10 OKS thresholds as the evaluation metric.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度（AP）和平均召回率（AR）。AP度量是根据精度（真实正结果与总正结果的比例）和召回率（真实正结果与地面真实正结果的比例）来衡量关键点检测准确性的指标。AP计算0到1之间召回率的平均精度值。AP有几个类似的变体。例如，关键点平均精度（APK）在（杨和拉马南，[2012](#bib.bib273)）中介绍。均值平均精度（mAP），即所有类别的平均精度的均值，是在MPII和PoseTrack数据集上广泛使用的度量。平均召回率（AR）是COCO关键点评估（林等，[2014](#bib.bib133)）中使用的另一种度量。对象关键点相似度（OKS）在对象检测中发挥类似于交并比（IoU）的作用，并用于AP或AR。此度量从主体的尺度和预测点与地面真实点之间的距离计算。COCO评估通常使用10个OKS阈值的mAP作为评估指标。
- en: 'Table 2\. Comparison of different methods on the MPII dataset for 2D single-person
    HPE using PCKh@0.5 measure (i.e., the threshold is equal to 50 percent of the
    head segment length of each test image). H: Heatmap; R: Regression. (Red: best;
    Blue: second best)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '表2\. 使用PCKh@0.5度量（即阈值等于每个测试图像头部段长度的50%）对MPII数据集进行2D单人姿态估计的不同方法比较。H: 热图；R: 回归。（红色：最佳；蓝色：第二最佳）'
- en: '|         Max Planck Institute for Informatics (MPII) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|         马普朗克信息学研究所（MPII） |'
- en: '| --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|  |         Year |         Method |         Head |         Shoulder |         Elbow
    |         Wrist |         Hip |         Knee |         Ankle |         Mean |
            Params(M) |         FLOPs(G) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  |         Year |         Method |         Head |         Shoulder |         Elbow
    |         Wrist |         Hip |         Knee |         Ankle |         Mean |
            Params(M) |         FLOPs(G) |'
- en: '|  |         2017 |         (Chu et al., [2017](#bib.bib42)) |         98.5
    |         96.3 |         91.9 |         88.1 |         90.6 |         88.0 |         85.0
    |         91.5 |         - |         - |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |         2017 |         (Chu et al., [2017](#bib.bib42)) |         98.5
    |         96.3 |         91.9 |         88.1 |         90.6 |         88.0 |         85.0
    |         91.5 |         - |         - |'
- en: '|  |         2017 |         (Yang et al., [2017](#bib.bib270)) |         98.5
    |         96.7 |         92.5 |         88.7 |         91.1 |         88.6 |         86.0
    |         92.0 |         7.3 |         14.7 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  |         2017 |         (Yang et al., [2017](#bib.bib270)) |         98.5
    |         96.7 |         92.5 |         88.7 |         91.1 |         88.6 |         86.0
    |         92.0 |         7.3 |         14.7 |'
- en: '|  |         2018 |         (Ke et al., [2018](#bib.bib97)) |         98.5
    |         96.8 |         92.7 |         88.4 |         90.6 |         89.3 |         86.3
    |         92.1 |         - |         - |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  |         2018 |         (Ke et al., [2018](#bib.bib97)) |         98.5
    |         96.8 |         92.7 |         88.4 |         90.6 |         89.3 |         86.3
    |         92.1 |         - |         - |'
- en: '|  |         2018 |         (Tang et al., [2018](#bib.bib225)) |         98.4
    |         96.9 |         92.6 |         88.7 |         91.8 |         89.4 |         86.2
    |         92.3 |         15.5 |         33.6 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  |         2018 |         (Tang et al., [2018](#bib.bib225)) |         98.4
    |         96.9 |         92.6 |         88.7 |         91.8 |         89.4 |         86.2
    |         92.3 |         15.5 |         33.6 |'
- en: '|  |         2018 |         (Xiao et al., [2018](#bib.bib260)) |         98.5
    |         96.6 |         91.9 |         87.6 |         91.1 |         88.1 |         84.1
    |         91.5 |         - |         - |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |         2018 |         (Xiao et al., [2018](#bib.bib260)) |         98.5
    |         96.6 |         91.9 |         87.6 |         91.1 |         88.1 |         84.1
    |         91.5 |         - |         - |'
- en: '|  |         2019 |         (Sun et al., [2019](#bib.bib222)) |         98.6
    |         96.9 |         92.8 |         89.0 |         91.5 |         89.0 |         85.7
    |         92.3 |         28.5 |         9.5 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         (Sun et al., [2019](#bib.bib222)) |         98.6
    |         96.9 |         92.8 |         89.0 |         91.5 |         89.0 |         85.7
    |         92.3 |         28.5 |         9.5 |'
- en: '|  |         2019 |         (Zhang et al., [2019a](#bib.bib286)) |         98.6
    |         97.0 |         92.8 |         88.8 |         91.7 |         89.8 |         86.6
    |         92.5 |         23.9 |         41.4 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         (Zhang et al., [2019a](#bib.bib286)) |         98.6
    |         97.0 |         92.8 |         88.8 |         91.7 |         89.8 |         86.6
    |         92.5 |         23.9 |         41.4 |'
- en: '|  |         2019 |         (Li et al., [2019a](#bib.bib122)) |         98.4
    |         97.1 |         93.2 |         89.2 |         92.0 |         90.1 |         85.5
    |         92.6 |         - |         - |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         (Li et al., [2019a](#bib.bib122)) |         98.4
    |         97.1 |         93.2 |         89.2 |         92.0 |         90.1 |         85.5
    |         92.6 |         - |         - |'
- en: '|  |         2020 |         (Artacho and Savakis, [2020](#bib.bib7)) |         -
    |         - |         - |         - |         - |         - |         - |         92.7
    |         - |         - |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  |         2020 |         (Artacho and Savakis, [2020](#bib.bib7)) |         -
    |         - |         - |         - |         - |         - |         - |         92.7
    |         - |         - |'
- en: '|  |         2020 |         (Cai et al., [2020](#bib.bib15)) |         98.5
    |         97.3 |         93.9 |         89.9 |         92.0 |         90.6 |         86.8
    |         93.0 |         - |         - |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  |         2020 |         (Cai et al., [2020](#bib.bib15)) |         98.5
    |         97.3 |         93.9 |         89.9 |         92.0 |         90.6 |         86.8
    |         93.0 |         - |         - |'
- en: '|  |         2021 |         (Li et al., [2021e](#bib.bib126)) |         97.1
    |         95.9 |         90.4 |         86.0 |         89.3 |         87.1 |         82.5
    |         90.2 |         28.1 |         - |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |         2021 |         (Li et al., [2021e](#bib.bib126)) |         97.1
    |         95.9 |         90.4 |         86.0 |         89.3 |         87.1 |         82.5
    |         90.2 |         28.1 |         - |'
- en: '|         H |         2022 |         (Li et al., [2022b](#bib.bib125)) |         97.2
    |         96.0 |         90.4 |         85.6 |         89.5 |         85.8 |         81.8
    |         90.0 |         - |         - |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|         H |         2022 |         (Li et al., [2022b](#bib.bib125)) |         97.2
    |         96.0 |         90.4 |         85.6 |         89.5 |         85.8 |         81.8
    |         90.0 |         - |         - |'
- en: '|  |         2017 |         (Sun et al., [2017](#bib.bib223)) |         97.5
    |         94.3 |         87.0 |         81.2 |         86.5 |         78.5 |         75.4
    |         86.4 |         - |         - |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  |         2017 |         (Sun et al., [2017](#bib.bib223)) |         97.5
    |         94.3 |         87.0 |         81.2 |         86.5 |         78.5 |         75.4
    |         86.4 |         - |         - |'
- en: '|  |         2019 |         (Zhang et al., [2019b](#bib.bib285)) |         98.3
    |         96.4 |         91.5 |         87.4 |         90.0 |         87.1 |         83.7
    |         91.1 |         3.0 |         9.0 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         （Zhang 等，[2019b](#bib.bib285)） |         98.3 |
            96.4 |         91.5 |         87.4 |         90.0 |         87.1 |         83.7
    |         91.1 |         3.0 |         9.0 |'
- en: '|  |         2019 |         (Luvizon et al., [2019](#bib.bib149)) |         98.1
    |         96.6 |         92.0 |         87.5 |         90.6 |         88 |         82.7
    |         91.2 |         - |         - |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         （Luvizon 等，[2019](#bib.bib149)） |         98.1
    |         96.6 |         92.0 |         87.5 |         90.6 |         88 |         82.7
    |         91.2 |         - |         - |'
- en: '|  |         2021 |         (Li et al., [2021c](#bib.bib117)) |         97.3
    |         96.0 |         90.6 |         84.5 |         89.7 |         85.5 |         79.0
    |         89.5 |         - |         - |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |         2021 |         （Li 等，[2021c](#bib.bib117)） |         97.3 |         96.0
    |         90.6 |         84.5 |         89.7 |         85.5 |         79.0 |         89.5
    |         - |         - |'
- en: '|  |         2021 |         (Mao et al., [2021](#bib.bib154)) |         98.0
    |         95.9 |         91.0 |         86.0 |         89.8 |         86.6 |         82.6
    |         90.4 |         - |         - |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  |         2021 |         （Mao 等，[2021](#bib.bib154)） |         98.0 |         95.9
    |         91.0 |         86.0 |         89.8 |         86.6 |         82.6 |         90.4
    |         - |         - |'
- en: '|         R |         2022 |         (Mao et al., [2022](#bib.bib155)) |         -
    |         - |         - |         - |         - |         - |         - |         90.5
    |         - |         - |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|         R |         2022 |         （Mao 等，[2022](#bib.bib155)） |         -
    |         - |         - |         - |         - |         - |         - |         90.5
    |         - |         - |'
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Note: (Insafutdinov et al., [2016](#bib.bib77)), (Sun et al., [2019](#bib.bib222)),
    (Li et al., [2019a](#bib.bib122)), (Cai et al., [2020](#bib.bib15)), (Rogez et al.,
    [2017](#bib.bib208)),(Li et al., [2021e](#bib.bib126)) are 2D multi-person HPE
    methods, which are also applied to the single-person case here.'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：（Insafutdinov 等，[2016](#bib.bib77)）、（Sun 等，[2019](#bib.bib222)）、（Li 等，[2019a](#bib.bib122)）、（Cai
    等，[2020](#bib.bib15)）、（Rogez 等，[2017](#bib.bib208)）、（Li 等，[2021e](#bib.bib126)）是
    2D 多人 HPE 方法，这里也应用于单人情况。
- en: 'Table 3\. Comparison of different 2D multi-person HPE methods on the test-dev
    set of the COCO dataset using AP measure (AP.5: AP at OKS = 0.50; AP.75: AP at
    OKS = 0.75; AP(M) is used for medium objects; AP(L) is used for large objects).
    Extra: extra data is used for training. T: Top-down; B: Bottom-up'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3\. 在 COCO 数据集的 test-dev 集上，不同 2D 多人 HPE 方法的比较，使用 AP 测量（AP.5: OKS = 0.50
    时的 AP；AP.75: OKS = 0.75 时的 AP；AP(M) 用于中等对象；AP(L) 用于大型对象）。额外：额外数据用于训练。T: 自顶向下；B:
    自底向上'
- en: '|         Microsoft Common Objects in Context (COCO) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|         Microsoft Common Objects in Context (COCO) |'
- en: '| --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|  |         Year |         Method |         Extra |         Backbone |         Input
    size |         AP |         AP.5 |         AP.75 |         AP(M) |         AP(L)
    |         Params(M) |         FLOPs(G) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  |         年份 |         方法 |         额外 |         骨干网络 |         输入大小 |         AP
    |         AP.5 |         AP.75 |         AP(M) |         AP(L) |         参数(M)
    |         FLOPs(G) |'
- en: '|  |         2019 |         (Sun et al., [2019](#bib.bib222)) |         no
    |         HRNet-W32 |         384$\times$288 |         74.9 |         92.5 |         82.8
    |         71.3 |         80.9 |         28.5 |         16.0 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         （Sun 等，[2019](#bib.bib222)） |         no |         HRNet-W32
    |         384$\times$288 |         74.9 |         92.5 |         82.8 |         71.3
    |         80.9 |         28.5 |         16.0 |'
- en: '|  |         2019 |         (Sun et al., [2019](#bib.bib222)) |         no
    |         HRNet-W48 |         384$\times$288 |         75.5 |         92.5 |         83.3
    |         71.9 |         81.5 |         63.6 |         32.9 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         （Sun 等，[2019](#bib.bib222)） |         no |         HRNet-W48
    |         384$\times$288 |         75.5 |         92.5 |         83.3 |         71.9
    |         81.5 |         63.6 |         32.9 |'
- en: '|  |         2019 |         (Sun et al., [2019](#bib.bib222)) |         yes
    |         HRNet-W48 |         384$\times$288 |         77.0 |         92.7 |         84.5
    |         73.4 |         83.1 |         63.6 |         32.9 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         （Sun 等，[2019](#bib.bib222)） |         yes |         HRNet-W48
    |         384$\times$288 |         77.0 |         92.7 |         84.5 |         73.4
    |         83.1 |         63.6 |         32.9 |'
- en: '|  |         2019 |         (Li et al., [2019a](#bib.bib122)) |         yes
    |         4xResNet-50 |         384$\times$288 |         77.1 |         93.8 |
            84.6 |         73.4 |         82.3 |         - |         - |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         （Li 等，[2019a](#bib.bib122)） |         yes |         4xResNet-50
    |         384$\times$288 |         77.1 |         93.8 |         84.6 |         73.4
    |         82.3 |         - |         - |'
- en: '|  |         2020 |         (Zhang et al., [2020h](#bib.bib284)) |         no
    |         HRNet-W48 |         384$\times$288 |         76.2 |         92.5 |         83.6
    |         72.5 |         82.4 |         63.6 |         32.9 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  |         2020 |         (张等, [2020h](#bib.bib284)) |         no |         HRNet-W48
    |         384$\times$288 |         76.2 |         92.5 |         83.6 |         72.5
    |         82.4 |         63.6 |         32.9 |'
- en: '|  |         2020 |         (Zhang et al., [2020h](#bib.bib284)) |         yes
    |         HRNet-W48 |         384$\times$288 |         77.4 |         92.6 |         84.6
    |         73.6 |         83.7 |         63.6 |         32.9 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  |         2020 |         (张等, [2020h](#bib.bib284)) |         yes |         HRNet-W48
    |         384$\times$288 |         77.4 |         92.6 |         84.6 |         73.6
    |         83.7 |         63.6 |         32.9 |'
- en: '|  |         2020 |         (Cai et al., [2020](#bib.bib15)) |         no |
            4xRSN-50 |         384$\times$288 |         78.6 |         94.3 |         86.6
    |         75.5 |         83.3 |         111.8 |         65.9 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  |         2020 |         (蔡等, [2020](#bib.bib15)) |         no |         4xRSN-50
    |         384$\times$288 |         78.6 |         94.3 |         86.6 |         75.5
    |         83.3 |         111.8 |         65.9 |'
- en: '|  |         2021 |         (Yang et al., [2021](#bib.bib269)) |         no
    |         HRNet-W48 |         256$\times$192 |         75.0 |         92.2 |         82.3
    |         71.3 |         81.1 |         17.5 |         21.8 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  |         2021 |         (杨等, [2021](#bib.bib269)) |         no |         HRNet-W48
    |         256$\times$192 |         75.0 |         92.2 |         82.3 |         71.3
    |         81.1 |         17.5 |         21.8 |'
- en: '|  |         2021 |         (Li et al., [2021e](#bib.bib126)) |         no
    |         HRNet-W48 |         384$\times$288 |         75.9 |         92.3 |         83.4
    |         72.2 |         82.1 |         29.8 |         22.1 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  |         2021 |         (李等, [2021e](#bib.bib126)) |         no |         HRNet-W48
    |         384$\times$288 |         75.9 |         92.3 |         83.4 |         72.2
    |         82.1 |         29.8 |         22.1 |'
- en: '|  |         2021 |         (Liu et al., [2021b](#bib.bib135)) |         no
    |         HRNet-W48 |         384$\times$288 |         79.5 |         93.6 |         85.9
    |         76.3 |         84.3 |         35.4 |         70.1 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  |         2021 |         (刘等, [2021b](#bib.bib135)) |         no |         HRNet-W48
    |         384$\times$288 |         79.5 |         93.6 |         85.9 |         76.3
    |         84.3 |         35.4 |         70.1 |'
- en: '|         T |         2022 |         (Ma et al., [2022](#bib.bib151)) |         no
    |         Transformer |         256$\times$192 |         75.2 |         89.8 |
            81.7 |         71.7 |         82.1 |         20.8 |         8.7 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|         T |         2022 |         (马等, [2022](#bib.bib151)) |         no
    |         Transformer |         256$\times$192 |         75.2 |         89.8 |
            81.7 |         71.7 |         82.1 |         20.8 |         8.7 |'
- en: '|  |         2017 |         (Newell et al., [2017](#bib.bib171)) |         no
    |         Hourglass |         512$\times$512 |         65.5 |         86.8 |         72.3
    |         60.6 |         72.6 |         - |         - |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  |         2017 |         (纽厄尔等, [2017](#bib.bib171)) |         no |         Hourglass
    |         512$\times$512 |         65.5 |         86.8 |         72.3 |         60.6
    |         72.6 |         - |         - |'
- en: '|  |         2018 |         (Papandreou et al., [2018](#bib.bib180)) |         no
    |         ResNet-152 |         1401$\times$1401 |         68.7 |         89.0
    |         75.4 |         64.1 |         75.5 |         68.7 |         405.5 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |         2018 |         (帕潘德里欧等, [2018](#bib.bib180)) |         no |         ResNet-152
    |         1401$\times$1401 |         68.7 |         89.0 |         75.4 |         64.1
    |         75.5 |         68.7 |         405.5 |'
- en: '|  |         2019 |         (Tian et al., [2019](#bib.bib229)) |         no
    |         ResNet-101 |         800$\times$800 |         64.8 |         87.8 |
            71.1 |         60.4 |         71.5 |         - |         - |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |         2019 |         (田等, [2019](#bib.bib229)) |         no |         ResNet-101
    |         800$\times$800 |         64.8 |         87.8 |         71.1 |         60.4
    |         71.5 |         - |         - |'
- en: '|  |         2020 |         (Jin et al., [2020a](#bib.bib89)) |         no
    |         Hourglass |         512$\times$512 |         67.6 |         85.1 |         73.7
    |         62.7 |         74.6 |         - |         - |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |         2020 |         (晋等, [2020a](#bib.bib89)) |         no |         Hourglass
    |         512$\times$512 |         67.6 |         85.1 |         73.7 |         62.7
    |         74.6 |         - |         - |'
- en: '|  |         2020 |         (Cheng et al., [2020](#bib.bib32)) |         no
    |         HRNet-W48 |         640$\times$640 |         70.5 |         89.3 |         77.2
    |         66.6 |         75.8 |         63.8 |         154.3 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  |         2020 |         (程等, [2020](#bib.bib32)) |         no |         HRNet-W48
    |         640$\times$640 |         70.5 |         89.3 |         77.2 |         66.6
    |         75.8 |         63.8 |         154.3 |'
- en: '|  |         2021 |         (Luo et al., [2021](#bib.bib147)) |         no
    |         HRNet-W48 |         640$\times$640 |         72.0 |         90.7 |         78.8
    |         67.8 |         77.7 |         63.8 |         154.6 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |         2021 |         (罗等, [2021](#bib.bib147)) |         no |         HRNet-W48
    |         640$\times$640 |         72.0 |         90.7 |         78.8 |         67.8
    |         77.7 |         63.8 |         154.6 |'
- en: '|         B |         2022 |         (Wang et al., [2022c](#bib.bib245)) |
            no |         HRNet-W32 |         640$\times$640 |         72.8 |         91.2
    |         79.9 |         68.3 |         79.3 |         - |         - |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|         B |         2022 |         (Wang et al., [2022c](#bib.bib245)) |
            no |         HRNet-W32 |         640$\times$640 |         72.8 |         91.2
    |         79.9 |         68.3 |         79.3 |         - |         - |'
- en: Table 4\. Comparison of different 2D video-based HPE methods on the PoseTrack2017
    test set and PoseTrack2018 test set
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 不同 2D 视频基础 HPE 方法在 PoseTrack2017 测试集和 PoseTrack2018 测试集上的比较
- en: '|         PoseTrack2017 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|         PoseTrack2017 |'
- en: '|         Year |         Method |         Backbone |         Head |         Shoulder
    |         Elbow |         Wrist |         Hip |         Knee |         Ankle |
            Total |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|         年份 |         方法 |         骨干网络 |         头部 |         肩膀 |         肘部
    |         手腕 |         臀部 |         膝盖 |         踝部 |         总计 |'
- en: '|         2018 |         (Doering et al., [2018](#bib.bib48)) |         VGG
    |         - |         - |         - |         53.1 |         - |         - |         50.4
    |         63.4 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|         2018 |         (Doering et al., [2018](#bib.bib48)) |         VGG
    |         - |         - |         - |         53.1 |         - |         - |         50.4
    |         63.4 |'
- en: '|         2019 |         (Bertasius et al., [2019](#bib.bib10)) |         HRNet-W48
    |         79.5 |         84.3 |         80.1 |         75.8 |         77.6 |         76.8
    |         70.8 |         77.9 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|         2019 |         (Bertasius et al., [2019](#bib.bib10)) |         HRNet-W48
    |         79.5 |         84.3 |         80.1 |         75.8 |         77.6 |         76.8
    |         70.8 |         77.9 |'
- en: '|         2020 |         (Snower et al., [2020](#bib.bib219)) |         Transformer
    |         - |         - |         - |         71.9 |         - |         - |         65.0
    |         74.0 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|         2020 |         (Snower et al., [2020](#bib.bib219)) |         Transformer
    |         - |         - |         - |         71.9 |         - |         - |         65.0
    |         74.0 |'
- en: '|         2021 |         (Liu et al., [2021a](#bib.bib140)) |         HRNet-W48
    |         84.3 |         84.9 |         80.5 |         76.1 |         77.9 |         77.1
    |         71.2 |         79.2 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|         2021 |         (Liu et al., [2021a](#bib.bib140)) |         HRNet-W48
    |         84.3 |         84.9 |         80.5 |         76.1 |         77.9 |         77.1
    |         71.2 |         79.2 |'
- en: '|         2022 |         (Liu et al., [2022](#bib.bib141)) |         HRNet-W48
    |         86.1 |         86.1 |         81.8 |         77.4 |         79.5 |         79.1
    |         73.6 |         80.9 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|         2022 |         (Liu et al., [2022](#bib.bib141)) |         HRNet-W48
    |         86.1 |         86.1 |         81.8 |         77.4 |         79.5 |         79.1
    |         73.6 |         80.9 |'
- en: '|         PoseTrack2018 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|         PoseTrack2018 |'
- en: '|         2018 |         (Guo et al., [2018](#bib.bib64)) |         ResNet-152
    |         - |         - |         - |         74.5 |         - |         - |         69.0
    |         76.4 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|         2018 |         (Guo et al., [2018](#bib.bib64)) |         ResNet-152
    |         - |         - |         - |         74.5 |         - |         - |         69.0
    |         76.4 |'
- en: '|         2019 |         (Bertasius et al., [2019](#bib.bib10)) |         HRNet-W48
    |         78.9 |         84.4 |         80.9 |         76.8 |         75.6 |         77.5
    |         71.8 |         78.0 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|         2019 |         (Bertasius et al., [2019](#bib.bib10)) |         HRNet-W48
    |         78.9 |         84.4 |         80.9 |         76.8 |         75.6 |         77.5
    |         71.8 |         78.0 |'
- en: '|         2021 |         (Liu et al., [2021a](#bib.bib140)) |         HRNet-W48
    |         82.8 |         84.0 |         80.8 |         77.2 |         76.1 |         77.6
    |         72.3 |         79.0 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|         2021 |         (Liu et al., [2021a](#bib.bib140)) |         HRNet-W48
    |         82.8 |         84.0 |         80.8 |         77.2 |         76.1 |         77.6
    |         72.3 |         79.0 |'
- en: '|         2022 |         (Liu et al., [2022](#bib.bib141)) |         HRNet-W48
    |         83.6 |         84.5 |         81.4 |         77.9 |         76.8 |         78.3
    |         72.9 |         79.6 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|         2022 |         (Liu et al., [2022](#bib.bib141)) |         HRNet-W48
    |         83.6 |         84.5 |         81.4 |         77.9 |         76.8 |         78.3
    |         72.9 |         79.6 |'
- en: Table 5\. Datasets for 3D HPE.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 3D HPE 数据集
- en: '| Dataset | Year | Capture system | Environment | Size | Single person | Multi-person
    | Single view | Multi-view |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 捕捉系统 | 环境 | 大小 | 单人 | 多人 | 单视角 | 多视角 |'
- en: '| HumanEva (Sigal et al., [2010](#bib.bib218)) | 2010 | Marker-based MoCap
    | Indoor | 6 subject, 7 actions, 40k frames | Yes | No | Yes | Yes |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| HumanEva (Sigal et al., [2010](#bib.bib218)) | 2010 | 基于标记的 MoCap | 室内 |
    6 个受试者，7 种动作，40k 帧 | 是 | 否 | 是 | 是 |'
- en: '| Human3.6M (Ionescu et al., [2014](#bib.bib78)) | 2014 | Marker-based MoCap
    | Indoor | 11 subjects, 17 actions, 3.6M frames | Yes | No | Yes | Yes |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Human3.6M (Ionescu et al., [2014](#bib.bib78)) | 2014 | 基于标记的 MoCap | 室内
    | 11 个受试者，17 种动作，3.6M 帧 | 是 | 否 | 是 | 是 |'
- en: '| CMU Panoptic (Joo et al., [2017](#bib.bib93)) | 2016 | Marker-less MoCap
    | Indoor | 8 subjects, 1.5M frames | Yes | Yes | Yes | Yes |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| CMU Panoptic (Joo et al., [2017](#bib.bib93)) | 2016 | 无标记 MoCap | 室内 | 8
    个受试者，1.5M 帧 | 是 | 是 | 是 | 是 |'
- en: '| MPI-INF-3DHP (Mehta et al., [2017](#bib.bib158)) | 2017 | Marker-less MoCap
    | Indoor and outdoor | 8 subjects, 8 actions, 1.3M frames | Yes | No | Yes | Yes
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| MPI-INF-3DHP (Mehta et al., [2017](#bib.bib158)) | 2017 | 无标记运动捕捉 | 室内和室外
    | 8个受试者，8个动作，1.3M帧 | 是 | 否 | 是 | 是 |'
- en: '| TotalCapture (Trumble et al., [2017](#bib.bib235)) | 2017 | Marker-based
    MoCap with IMUs | Indoor | 5 subjects, 5 actions, 1.9M frames | Yes | No | Yes
    | Yes |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| TotalCapture (Trumble et al., [2017](#bib.bib235)) | 2017 | 带IMU的标记式运动捕捉
    | 室内 | 5个受试者，5个动作，1.9M帧 | 是 | 否 | 是 | 是 |'
- en: '| 3DPW (von Marcard et al., [2018](#bib.bib241)) | 2018 | Hand-held cameras
    with IMUs | Indoor and outdoor | 7 subjects, 51k frames | Yes | Yes | Yes | No
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 3DPW (von Marcard et al., [2018](#bib.bib241)) | 2018 | 带IMU的手持摄像机 | 室内和室外
    | 7个受试者，51k帧 | 是 | 是 | 是 | 否 |'
- en: '| MuPoTS-3D (Mehta et al., [2018](#bib.bib160)) | 2018 | Marker-less MoCap
    | Indoor and outdoor | 8 subjects, 8k frames | Yes | Yes | Yes | Yes |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| MuPoTS-3D (Mehta et al., [2018](#bib.bib160)) | 2018 | 无标记运动捕捉 | 室内和室外 |
    8个受试者，8k帧 | 是 | 是 | 是 | 是 |'
- en: '| AMASS (Mahmood et al., [2019](#bib.bib153)) | 2019 | Marker-based MoCap |
    Indoor and outdoor | 300 subjects, 9M frames | Yes | No | Yes | Yes |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| AMASS (Mahmood et al., [2019](#bib.bib153)) | 2019 | 标记式运动捕捉 | 室内和室外 | 300个受试者，9M帧
    | 是 | 否 | 是 | 是 |'
- en: '| NBA2K (Zhu et al., [2020](#bib.bib315)) | 2020 | NBA2K19 game engine | Indoor
    | 27 subjects, 27k poses | Yes | No | Yes | No |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| NBA2K (Zhu et al., [2020](#bib.bib315)) | 2020 | NBA2K19游戏引擎 | 室内 | 27个受试者，27k姿势
    | 是 | 否 | 是 | 否 |'
- en: '| GTA-IM (Cao et al., [2020](#bib.bib16)) | 2020 | GTA game engine | Indoor
    | 1M frames | Yes | No | Yes | No |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| GTA-IM (Cao et al., [2020](#bib.bib16)) | 2020 | GTA游戏引擎 | 室内 | 1M帧 | 是 |
    否 | 是 | 否 |'
- en: '| Occlusion-Person (Zhang et al., [2020g](#bib.bib296)) | 2020 | Unreal Engine
    4 game engine | Indoor | 73k frames | Yes | No | Yes | Yes |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Occlusion-Person (Zhang et al., [2020g](#bib.bib296)) | 2020 | Unreal Engine
    4游戏引擎 | 室内 | 73k帧 | 是 | 否 | 是 | 是 |'
- en: 4.3\. Performance Comparison of 2D HPE Methods
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 2D HPE方法的性能比较
- en: 'Single-person 2D HPE: Table [2](#S4.T2 "Table 2 ‣ 4.2\. Evaluation Metrics
    for 2D HPE ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based Human Pose
    Estimation: A Survey") shows the comparison results for different 2D single-person
    HPE methods on the MPII dataset using PCKh@0.5 measure. Although both heatmap-based
    and regression-based methods have impressive results, they have their own limitations
    in 2D single-person HPE. Regression methods can learn a nonlinear mapping from
    input images to keypoint coordinates with an end-to-end framework, which offers
    a fast learning paradigm and a sub-pixel level prediction accuracy. However, they
    usually give sub-optimal solutions (Luvizon et al., [2019](#bib.bib149)) due to
    the highly nonlinear problem. Heatmap-based methods outperform regression-based
    approaches and are more widely used in 2D HPE(Li et al., [2019a](#bib.bib122))(Cai
    et al., [2020](#bib.bib15))(Luvizon et al., [2019](#bib.bib149)) since (1) the
    probabilistic prediction of each pixel in a heatmap can improve the accuracy of
    locating the keypoints, and (2) heatmaps provide richer supervision information
    by preserving the spatial location information. However, the precision of the
    predicted keypoints is dependent on the resolution of heatmaps. The computational
    cost and memory footprint are significantly increased when using high-resolution
    heatmaps(Sun et al., [2019](#bib.bib222)).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '单人2D HPE：表[2](#S4.T2 "Table 2 ‣ 4.2\. Evaluation Metrics for 2D HPE ‣ 4\. Datasets
    and Evaluation Metrics ‣ Deep Learning-Based Human Pose Estimation: A Survey")显示了在MPII数据集上使用PCKh@0.5度量的不同2D单人HPE方法的比较结果。虽然基于热图的方法和回归方法都具有令人印象深刻的结果，但它们在2D单人HPE中各有其局限性。回归方法可以通过端到端的框架学习从输入图像到关键点坐标的非线性映射，提供快速学习模式和亚像素级的预测精度。然而，由于高度非线性的问题，它们通常提供次优解（Luvizon
    et al., [2019](#bib.bib149)）。基于热图的方法优于基于回归的方法，并且在2D HPE中更为广泛使用（Li et al., [2019a](#bib.bib122)）（Cai
    et al., [2020](#bib.bib15)）（Luvizon et al., [2019](#bib.bib149)），因为（1）热图中每个像素的概率预测可以提高关键点定位的准确性，（2）热图通过保留空间位置信息提供了更丰富的监督信息。然而，预测关键点的精度依赖于热图的分辨率。使用高分辨率热图时，计算成本和内存占用显著增加（Sun
    et al., [2019](#bib.bib222)）。'
- en: 'Multi-person 2D HPE: Table [3](#S4.T3 "Table 3 ‣ 4.2\. Evaluation Metrics for
    2D HPE ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based Human Pose
    Estimation: A Survey") presents the experimental results of different 2D HPE methods
    on the test-dev set of the COCO dataset, together with a summary of the experiment
    settings (extra data, backbones in models, input image size) and AP scores for
    each approach. The comparison experiments highlight the significant results of
    both top-down and bottom-up methods in multi-person HPE. Presumably, the top-down
    pipeline yields better results because it first detects each individual from the
    image using detection methods, then predicts the locations of keypoints using
    single-person HPE approaches. In this case, the keypoint estimation for each detected
    person is made easier, as the background is largely removed. But on the other
    hand, bottom-up methods are generally faster than top-down methods, because they
    directly detect all the keypoints and group them into individual poses using keypoint
    association strategies such as affinity linking (Cao et al., [2017](#bib.bib17)),
    associative embedding (Newell et al., [2017](#bib.bib171)), and pixel-wise keypoint
    regression (Zhou et al., [2019](#bib.bib311)). Besides the image-based methods
    listed above, Table [4](#S4.T4 "Table 4 ‣ 4.2\. Evaluation Metrics for 2D HPE
    ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based Human Pose Estimation:
    A Survey") also illustrates the comparisons of the recent video-based works on
    PoseTrack2017 and PoseTrack2018 datasets. The detailed results of the test sets
    are summarized.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 多人 2D HPE：表格 [3](#S4.T3 "表 3 ‣ 4.2\. 2D HPE 评价指标 ‣ 4\. 数据集和评价指标 ‣ 基于深度学习的人体姿态估计：综述")
    展示了不同 2D HPE 方法在 COCO 数据集的 test-dev 集上的实验结果，并总结了实验设置（额外数据、模型中的骨干网络、输入图像大小）和每种方法的
    AP 分数。比较实验突显了顶端到底端和自底向上的方法在多人 HPE 中的显著结果。可以推测，顶端到底端的流程效果更好，因为它首先使用检测方法从图像中检测每个人，然后使用单人
    HPE 方法预测关键点的位置。在这种情况下，由于背景被大部分去除，检测到的每个人的关键点估计变得更加容易。但另一方面，自底向上的方法通常比顶端到底端的方法更快，因为它们直接检测所有关键点，并使用关键点关联策略（如亲和连接（Cao
    et al., [2017](#bib.bib17)）、关联嵌入（Newell et al., [2017](#bib.bib171)）和像素级关键点回归（Zhou
    et al., [2019](#bib.bib311)））将其分组到个体姿态中。除了上述基于图像的方法，表格 [4](#S4.T4 "表 4 ‣ 4.2\.
    2D HPE 评价指标 ‣ 4\. 数据集和评价指标 ‣ 基于深度学习的人体姿态估计：综述") 还展示了近期在 PoseTrack2017 和 PoseTrack2018
    数据集上的视频基方法的比较。测试集的详细结果已总结。
- en: 4.4\. Datasets for 3D HPE
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 3D HPE 数据集
- en: In contrast to numerous 2D human pose datasets with high-quality annotation,
    acquiring accurate 3D annotation for 3D HPE datasets is a challenging task that
    requires motion capture systems such as MoCap and wearable IMUs. Due to the page
    limit, we only review several widely used large-scale 3D pose datasets for deep
    learning-based 3D HPE in the following.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与众多高质量注释的 2D 人体姿态数据集相比，获得准确的 3D 注释用于 3D HPE 数据集是一项具有挑战性的任务，需要使用诸如 MoCap 和可穿戴
    IMU 等运动捕捉系统。由于页面限制，以下仅回顾了几个广泛使用的大规模 3D 姿态数据集，用于基于深度学习的 3D HPE。
- en: '[Human3.6M](http://vision.imar.ro/human3.6m/) (Ionescu et al., [2014](#bib.bib78))
    is the most widely used indoor dataset for 3D HPE from monocular images and videos.
    There are 11 professional actors performing 17 activities from 4 different views
    in an indoor laboratory environment. This dataset contains 3.6 million 3D human
    poses with 3D ground truth annotation captured by accurate marker-based MoCap
    systems. Protocol #1 uses images of subjects S1, S5, S6, and S7 for training,
    and images of subjects S9 and S11 for testing.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[Human3.6M](http://vision.imar.ro/human3.6m/) (Ionescu et al., [2014](#bib.bib78))
    是从单目图像和视频中进行 3D HPE 的最广泛使用的室内数据集。该数据集包含 11 位专业演员在室内实验室环境中执行 17 种活动，从 4 个不同的视角拍摄。数据集中包含
    360 万个带有准确基于标记的 MoCap 系统捕捉的 3D 人体姿态的 3D 基准注释。协议 #1 使用 S1、S5、S6 和 S7 的图像进行训练，使用
    S9 和 S11 的图像进行测试。'
- en: '[MuPoTS-3D](http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/) (Mehta
    et al., [2018](#bib.bib160)) is a multi-person 3D test set and its ground-truth
    3D poses were captured by a multi-view marker-less MoCap system containing 20
    real-world scenes (5 indoor and 15 outdoor). There are challenging samples with
    occlusions, drastic illumination changes, and lens flares in some of the outdoor
    footage. More than 8,000 frames were collected in the 20 sequences by 8 subjects.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[MuPoTS-3D](http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/) (Mehta
    等，[2018](#bib.bib160)) 是一个多人的 3D 测试集，其真实 3D 姿态由包含 20 个真实场景（5 个室内和 15 个室外）的多视角无标记
    MoCap 系统捕获。部分室外镜头中包含具有挑战性的样本，如遮挡、剧烈的光照变化和镜头光晕。在 20 个序列中收集了超过 8,000 帧，由 8 个受试者完成。'
- en: 'Readers are referred to the original references for details about other datasets
    including MPI-INF-3DHP(Mehta et al., [2017](#bib.bib158)), HumanEva (Sigal et al.,
    [2010](#bib.bib218)), CMU Panoptic Dataset (Joo et al., [2017](#bib.bib93)), TotalCapture
    (Trumble et al., [2017](#bib.bib235)),MuCo-3DHP Dataset (Mehta et al., [2018](#bib.bib160)),
    3DPW (von Marcard et al., [2018](#bib.bib241)), AMASS (Mahmood et al., [2019](#bib.bib153)),
    NBA2K (Zhu et al., [2020](#bib.bib315)), GTA-IM (Cao et al., [2020](#bib.bib16)),
    and Occlusion-Person (Zhang et al., [2020g](#bib.bib296)). A summary of these
    datasets is shown in Table [5](#S4.T5 "Table 5 ‣ 4.2\. Evaluation Metrics for
    2D HPE ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based Human Pose
    Estimation: A Survey").'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 有关其他数据集的详细信息，请参见原始参考文献，包括 MPI-INF-3DHP (Mehta 等，[2017](#bib.bib158))、HumanEva
    (Sigal 等，[2010](#bib.bib218))、CMU Panoptic 数据集 (Joo 等，[2017](#bib.bib93))、TotalCapture
    (Trumble 等，[2017](#bib.bib235))、MuCo-3DHP 数据集 (Mehta 等，[2018](#bib.bib160))、3DPW
    (von Marcard 等，[2018](#bib.bib241))、AMASS (Mahmood 等，[2019](#bib.bib153))、NBA2K
    (Zhu 等，[2020](#bib.bib315))、GTA-IM (Cao 等，[2020](#bib.bib16)) 和 Occlusion-Person
    (Zhang 等，[2020g](#bib.bib296))。这些数据集的总结见表 [5](#S4.T5 "表 5 ‣ 4.2\. 2D HPE 的评估指标
    ‣ 4\. 数据集和评估指标 ‣ 基于深度学习的人体姿态估计：综述")。
- en: Table 6\. Comparison of different 3D single-view single-person HPE approaches
    on the Human3.6M dataset (Protocol 1). In skeleton-only approaches, “Direct” indicates
    the methods directly estimating 3D pose without 2D pose representation. “Lifting”
    denotes the methods lifting the 2D pose representation to the 3D space. The reported
    total parameters (Params) and FLOPs for 2D-3D lifting methods are computed without
    including the params and FLOPs of the external 2D pose detector.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 比较不同 3D 单视角单人 HPE 方法在 Human3.6M 数据集上的表现（协议 1）。在仅骨架的方法中，“Direct”表示直接估计
    3D 姿态的方法，无需 2D 姿态表示。“Lifting”表示将 2D 姿态表示提升到 3D 空间的方法。报告的总参数（Params）和 FLOPs 仅包括
    2D-3D 提升方法的参数和 FLOPs，不包括外部 2D 姿态检测器的参数和 FLOPs。
- en: '| Skeleton-only methods | MPJPE | PA-MPJPE |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 仅骨架方法 | MPJPE | PA-MPJPE |'
- en: '| Approaches | Year | Methods | 2D pose detector | Input | Params(M) | FLOPs(G)
    | Dir. | Disc. | Eat. | Greet | Phone | Photo | Pose | Purch. | Sit | SitD. |
    Somke | Wait | WalkD. | Walk | WalkT. | Average | Average |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 方法 | 2D 姿态检测器 | 输入 | 参数（M） | FLOPs（G） | Dir. | Disc. | Eat. | Greet
    | Phone | Photo | Pose | Purch. | Sit | SitD. | Somke | Wait | WalkD. | Walk |
    WalkT. | 平均值 | 平均值 |'
- en: '|  | 2017 | (Pavlakos et al., [2017a](#bib.bib185)) | - | image | - | - | 67.4
    | 71.9 | 66.7 | 69.1 | 72.0 | 77.0 | 65.0 | 68.3 | 83.7 | 96.5 | 71.7 | 65.8 |
    74.9 | 59.1 | 63.2 | 71.9 | 51.9 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | (Pavlakos 等，[2017a](#bib.bib185)) | - | image | - | - | 67.4 |
    71.9 | 66.7 | 69.1 | 72.0 | 77.0 | 65.0 | 68.3 | 83.7 | 96.5 | 71.7 | 65.8 | 74.9
    | 59.1 | 63.2 | 71.9 | 51.9 |'
- en: '| Direct | 2018 | (Pavlakos et al., [2018a](#bib.bib184)) | - | image | - |
    - | 48.5 | 54.4 | 54.4 | 52.0 | 59.4 | 65.3 | 49.9 | 52.9 | 65.8 | 71.1 | 56.6
    | 52.9 | 60.9 | 44.7 | 47.8 | 56.2 | 41.8 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 直接 | 2018 | (Pavlakos 等，[2018a](#bib.bib184)) | - | image | - | - | 48.5
    | 54.4 | 54.4 | 52.0 | 59.4 | 65.3 | 49.9 | 52.9 | 65.8 | 71.1 | 56.6 | 52.9 |
    60.9 | 44.7 | 47.8 | 56.2 | 41.8 |'
- en: '|  | 2019 | (Li and Lee, [2019](#bib.bib112)) | Hourglass | image | - | - |
    43.8 | 48.6 | 49.1 | 49.8 | 57.6 | 61.5 | 45.9 | 48.3 | 62.0 | 73.4 | 54.8 | 50.6
    | 56.0 | 43.4 | 45.5 | 52.7 | 42.6 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Li 和 Lee，[2019](#bib.bib112)) | Hourglass | image | - | - | 43.8
    | 48.6 | 49.1 | 49.8 | 57.6 | 61.5 | 45.9 | 48.3 | 62.0 | 73.4 | 54.8 | 50.6 |
    56.0 | 43.4 | 45.5 | 52.7 | 42.6 |'
- en: '|  | 2019 | (Zhao et al., [2019b](#bib.bib297)) | own design | image | - |
    - | 47.3 | 60.7 | 51.4 | 60.5 | 61.1 | 49.9 | 47.3 | 68.1 | 86.2 | 55.0 | 67.8
    | 61.0 | 42.1 | 60.6 | 45.3 | 57.6 | - |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Zhao 等，[2019b](#bib.bib297)) | 自设计 | image | - | - | 47.3 | 60.7
    | 51.4 | 60.5 | 61.1 | 49.9 | 47.3 | 68.1 | 86.2 | 55.0 | 67.8 | 61.0 | 42.1 |
    60.6 | 45.3 | 57.6 | - |'
- en: '|  | 2021 | (Zou and Tang, [2021](#bib.bib317)) | CPN | image | 0.3 | - | 45.4
    | 49.2 | 45.7 | 49.4 | 50.4 | 58.2 | 47.9 | 46.0 | 57.5 | 63.0 | 49.7 | 46.6 |
    52.2 | 38.9 | 40.8 | 49.4 | 39.1 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | (Zou 和 Tang，[2021](#bib.bib317)) | CPN | image | 0.3 | - | 45.4
    | 49.2 | 45.7 | 49.4 | 50.4 | 58.2 | 47.9 | 46.0 | 57.5 | 63.0 | 49.7 | 46.6 |
    52.2 | 38.9 | 40.8 | 49.4 | 39.1 |'
- en: '|  | 2019 | (Pavllo et al., [2019](#bib.bib188)) | CPN | Video | 17 | 0.03
    | 45.2 | 46.7 | 43.3 | 45.6 | 48.1 | 55.1 | 44.6 | 44.3 | 57.3 | 65.8 | 47.1 |
    44.0 | 49.0 | 32.8 | 33.9 | 46.8 | 36.5 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Pavllo et al., [2019](#bib.bib188)) | CPN | 视频 | 17 | 0.03 | 45.2
    | 46.7 | 43.3 | 45.6 | 48.1 | 55.1 | 44.6 | 44.3 | 57.3 | 65.8 | 47.1 | 44.0 |
    49.0 | 32.8 | 33.9 | 46.8 | 36.5 |'
- en: '|  | 2019 | (Cai et al., [2019](#bib.bib14)) | CPN | Video | - | - | 44.6 |
    47.4 | 45.6 | 48.8 | 50.8 | 59.0 | 47.2 | 43.9 | 57.9 | 61.9 | 49.7 | 46.6 | 51.3
    | 37.1 | 39.4 | 48.8 | 39.0 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Cai et al., [2019](#bib.bib14)) | CPN | 视频 | - | - | 44.6 | 47.4
    | 45.6 | 48.8 | 50.8 | 59.0 | 47.2 | 43.9 | 57.9 | 61.9 | 49.7 | 46.6 | 51.3 |
    37.1 | 39.4 | 48.8 | 39.0 |'
- en: '|  | 2020 | (Liu et al., [2020c](#bib.bib139)) | CPN | Video | 11 | - | 41.8
    | 44.8 | 41.1 | 44.9 | 47.4 | 54.1 | 43.4 | 42.2 | 56.2 | 63.6 | 45.3 | 43.5 |
    45.3 | 31.3 | 32.2 | 45.1 | 35.6 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Liu et al., [2020c](#bib.bib139)) | CPN | 视频 | 11 | - | 41.8 |
    44.8 | 41.1 | 44.9 | 47.4 | 54.1 | 43.4 | 42.2 | 56.2 | 63.6 | 45.3 | 43.5 | 45.3
    | 31.3 | 32.2 | 45.1 | 35.6 |'
- en: '|  | 2020 | (Zeng et al., [2020](#bib.bib282)) | CPN | Video | - | - | 46.6
    | 47.1 | 43.9 | 41.6 | 45.8 | 49.6 | 46.5 | 40.0 | 53.4 | 61.1 | 46.1 | 42.6 |
    43.1 | 31.5 | 32.6 | 44.8 | - |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Zeng et al., [2020](#bib.bib282)) | CPN | 视频 | - | - | 46.6 |
    47.1 | 43.9 | 41.6 | 45.8 | 49.6 | 46.5 | 40.0 | 53.4 | 61.1 | 46.1 | 42.6 | 43.1
    | 31.5 | 32.6 | 44.8 | - |'
- en: '|  | 2020 | (Wang et al., [2020d](#bib.bib249)) | HRNet | Video | - | - | 38.2
    | 41.0 | 45.9 | 39.7 | 41.4 | 51.4 | 41.6 | 41.4 | 52.0 | 57.4 | 41.8 | 44.4 |
    41.6 | 33.1 | 30.0 | 42.6 | 32.7 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Wang et al., [2020d](#bib.bib249)) | HRNet | 视频 | - | - | 38.2
    | 41.0 | 45.9 | 39.7 | 41.4 | 51.4 | 41.6 | 41.4 | 52.0 | 57.4 | 41.8 | 44.4 |
    41.6 | 33.1 | 30.0 | 42.6 | 32.7 |'
- en: '|  | 2020 | (Chen et al., [2021](#bib.bib24)) | CPN | Video | 59 | 0.1 | 41.4
    | 43.5 | 40.1 | 42.9 | 46.6 | 51.9 | 41.7 | 42.3 | 53.9 | 60.2 | 45.4 | 41.7 |
    46.0 | 31.5 | 32.7 | 44.1 | 35.0 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Chen et al., [2021](#bib.bib24)) | CPN | 视频 | 59 | 0.1 | 41.4
    | 43.5 | 40.1 | 42.9 | 46.6 | 51.9 | 41.7 | 42.3 | 53.9 | 60.2 | 45.4 | 41.7 |
    46.0 | 31.5 | 32.7 | 44.1 | 35.0 |'
- en: '|  | 2021 | (Zheng et al., [2021](#bib.bib305)) | CPN | Video | 10 | 1.4 |
    41.5 | 44.8 | 39.8 | 42.5 | 46.5 | 51.6 | 42.1 | 42.0 | 53.3 | 60.7 | 45.5 | 43.3
    | 46.1 | 31.8 | 32.2 | 44.3 | 34.6 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | (Zheng et al., [2021](#bib.bib305)) | CPN | 视频 | 10 | 1.4 | 41.5
    | 44.8 | 39.8 | 42.5 | 46.5 | 51.6 | 42.1 | 42.0 | 53.3 | 60.7 | 45.5 | 43.3 |
    46.1 | 31.8 | 32.2 | 44.3 | 34.6 |'
- en: '|  | 2022 | (Li et al., [2022a](#bib.bib121)) | CPN | Video | 32 | 7.0 | 39.2
    | 43.1 | 40.1 | 40.9 | 44.9 | 51.2 | 40.6 | 41.3 | 53.5 | 60.3 | 43.7 | 41.1 |
    43.8 | 29.8 | 30.6 | 43.0 | - |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | (Li et al., [2022a](#bib.bib121)) | CPN | 视频 | 32 | 7.0 | 39.2
    | 43.1 | 40.1 | 40.9 | 44.9 | 51.2 | 40.6 | 41.3 | 53.5 | 60.3 | 43.7 | 41.1 |
    43.8 | 29.8 | 30.6 | 43.0 | - |'
- en: '| Lifting | 2022 | (Zhang et al., [2022](#bib.bib289)) | CPN | Video | 41 |
    0.6 | 37.6 | 40.9 | 37.3 | 39.7 | 42.3 | 49.9 | 40.1 | 39.8 | 51.7 | 55.0 | 42.1
    | 39.8 | 41.0 | 27.9 | 27.9 | 40.9 | 32.6 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Lifting | 2022 | (Zhang et al., [2022](#bib.bib289)) | CPN | 视频 | 41 | 0.6
    | 37.6 | 40.9 | 37.3 | 39.7 | 42.3 | 49.9 | 40.1 | 39.8 | 51.7 | 55.0 | 42.1 |
    39.8 | 41.0 | 27.9 | 27.9 | 40.9 | 32.6 |'
- en: '| Human mesh recovery methods | MPJPE | PA-MPJPE |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 人体网格恢复方法 | MPJPE | PA-MPJPE |'
- en: '| output | Year | Methods | Model | Input | Params(M) | FLOPs(G) | Dir. | Disc.
    | Eat. | Greet | Phone | Photo | Pose | Purch. | Sit | SitD. | Somke | Wait |
    WalkD. | Walk | WalkT. | Average | Average |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 年份 | 方法 | 模型 | 输入 | 参数(M) | FLOPs(G) | Dir. | Disc. | Eat. | Greet |
    Phone | Photo | Pose | Purch. | Sit | SitD. | Somke | Wait | WalkD. | Walk | WalkT.
    | 平均 | 平均 |'
- en: '|  | 2019 | (Kolotouros et al., [2019](#bib.bib103)) | SMPL | Image | - | -
    | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 50.1 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Kolotouros et al., [2019](#bib.bib103)) | SMPL | 图像 | - | - |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 50.1 |'
- en: '|  | 2019 | (Kolotouros et al., [2019](#bib.bib102)) | SMPL | Image | - | -
    | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 41.1 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Kolotouros et al., [2019](#bib.bib102)) | SMPL | 图像 | - | - |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 41.1 |'
- en: '|  | 2019 | (Xiang et al., [2019](#bib.bib259)) | Adam | Image | - | - | -
    | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 58.3 | - |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Xiang et al., [2019](#bib.bib259)) | Adam | 图像 | - | - | - | -
    | - | - | - | - | - | - | - | - | - | - | - | - | - | 58.3 | - |'
- en: '|  | 2020 | (Choi et al., [2020](#bib.bib39)) | - | Image | 140 | 11 | - |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | 64.9 | 47.0 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Choi et al., [2020](#bib.bib39)) | - | 图像 | 140 | 11 | - | - |
    - | - | - | - | - | - | - | - | - | - | - | - | - | 64.9 | 47.0 |'
- en: '|  | 2021 | (Zhang et al., [2021](#bib.bib288)) | SMPL | Image | 45.2 | 11
    | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 57.7 | 40.5 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | (Zhang et al., [2021](#bib.bib288)) | SMPL | 图像 | 45.2 | 11 | -
    | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 57.7 | 40.5 |'
- en: '|  | 2021 | (Lin et al., [2021a](#bib.bib131)) | SMPL | Image | 230 | 57 |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 54.0 | 36.7 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | (Lin et al., [2021a](#bib.bib131)) | SMPL | 图像 | 230 | 57 | - |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | 54.0 | 36.7 |'
- en: '|  | 2021 | (Lin et al., [2021b](#bib.bib132)) | SMPL | Image | 230 | 57 |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 51.2 | 34.5 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | (Lin等，[2021b](#bib.bib132)) | SMPL | Image | 230 | 57 | - | - |
    - | - | - | - | - | - | - | - | - | - | - | 51.2 | 34.5 |'
- en: '|  | 2022 | (Cho et al., [2022](#bib.bib37)) | SMPL | Image | 49 | 16 | - |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | 52.2 | 33.7 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | (Cho等，[2022](#bib.bib37)) | SMPL | Image | 49 | 16 | - | - | -
    | - | - | - | - | - | - | - | - | - | - | - | - | 52.2 | 33.7 |'
- en: '|  | 2019 | (Arnab et al., [2019](#bib.bib6)) | SMPL | Video | - | - | - |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | 77.8 | 54.3 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Arnab等，[2019](#bib.bib6)) | SMPL | Video | - | - | - | - | - |
    - | - | - | - | - | - | - | - | - | - | - | - | 77.8 | 54.3 |'
- en: '|  | 2020 | (Kocabas et al., [2020](#bib.bib99)) | SMPL | Video | 59 | 10 |
    - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 65.6 | 41.4 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Kocabas等，[2020](#bib.bib99)) | SMPL | Video | 59 | 10 | - | -
    | - | - | - | - | - | - | - | - | - | - | - | - | - | 65.6 | 41.4 |'
- en: '| Mesh | 2021 | (Choi et al., [2021](#bib.bib38)) | SMPL | Video | 123 | 10
    | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | 62.3 | 41.1 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 网格 | 2021 | (Choi等，[2021](#bib.bib38)) | SMPL | Video | 123 | 10 | - | -
    | - | - | - | - | - | - | - | - | - | - | - | - | - | 62.3 | 41.1 |'
- en: 4.5\. Evaluation Metrics for 3D HPE
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 3D HPE的评估指标
- en: 'MPJPE (Mean Per Joint Position Error) is the most widely used metric to evaluate
    the performance of 3D HPE. MPJPE is computed by using the Euclidean distance between
    the estimated 3D joints and the ground truth positions as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: MPJPE（每关节位置误差均值）是评估3D HPE性能的最常用指标。MPJPE通过使用估计的3D关节与地面真实位置之间的欧氏距离计算，如下所示：
- en: '| (1) |  | $\displaystyle\small{MPJPE}=\frac{1}{N}\sum_{i=1}^{N}\&#124;J_{i}-J_{i}^{*}\&#124;_{2},$
    |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle\small{MPJPE}=\frac{1}{N}\sum_{i=1}^{N}\&#124;J_{i}-J_{i}^{*}\&#124;_{2},$
    |  |'
- en: where $N$ is the number of joints, $J_{i}$ and $J^{*}_{i}$ are the ground truth
    position and the estimated position of the $i_{th}$ joint.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N$是关节数量，$J_{i}$和$J^{*}_{i}$分别是第$i_{th}$关节的地面真实位置和估计位置。
- en: PA-MPJPE, also called Reconstruction Error, is the MPJPE after rigid alignment
    by post-processing between the estimated pose and the ground truth pose.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: PA-MPJPE，也称为重建误差，是通过后处理在估计姿态和地面真实姿态之间的刚性对齐后的MPJPE。
- en: NMPJPE is defined as the MPJPE after normalizing the predicted positions in
    scale to the reference (Rhodin et al., [2018b](#bib.bib207)).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: NMPJPE被定义为在将预测位置按比例标准化到参考位置后的MPJPE（Rhodin等，[2018b](#bib.bib207)）。
- en: 'MPVE (Mean Per Vertex Error) (Pavlakos et al., [2018b](#bib.bib187)) measures
    the Euclidean distances between the ground truth vertices and the predicted vertices:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: MPVE（每顶点误差均值）（Pavlakos等，[2018b](#bib.bib187)）测量的是地面真实顶点与预测顶点之间的欧氏距离：
- en: '| (2) |  | $\displaystyle\small{MPVE}=\frac{1}{N}\sum_{i=1}^{N}\&#124;V_{i}-V_{i}^{*}\&#124;_{2},$
    |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle\small{MPVE}=\frac{1}{N}\sum_{i=1}^{N}\&#124;V_{i}-V_{i}^{*}\&#124;_{2},$
    |  |'
- en: where $N$ is the number of vertices, $V$ is the ground truth vertices, and $V^{*}$
    is the estimated vertices.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N$是顶点数量，$V$是地面真实顶点，$V^{*}$是估计顶点。
- en: 3DPCK is a 3D extended version of the Percentage of Correct Keypoints (PCK)
    metric used in 2D HPE evaluation. An estimated joint is considered as correct
    if the distance between the estimation and the ground truth is within a certain
    threshold. Generally, the threshold is set to 150$mm$.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 3DPCK是用于2D HPE评估的关键点正确百分比（PCK）指标的3D扩展版。估计的关节被认为是正确的，如果估计值与地面真实值之间的距离在某个阈值以内。通常，阈值设置为150$mm$。
- en: Table 7\. Comparison of different 3D single-view multi-person HPE approaches
    on the MuPoTS-3D dataset. The reported fps is taken from (Wang et al., [2022b](#bib.bib254)).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表7\. 在MuPoTS-3D数据集上不同的3D单视角多人物HPE方法的比较。报告的fps取自（Wang等，[2022b](#bib.bib254)）。
- en: '| MuPoTS-3D |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| MuPoTS-3D |'
- en: '| --- |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|  |  |  | 3DPCK $\uparrow$ | fps |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 3DPCK $\uparrow$ | fps |'
- en: '|  | Year | Method | All people | Matched people |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | 年份 | 方法 | 所有人 | 匹配人员 |'
- en: '|  | 2019 | (Moon et al., [2019a](#bib.bib166)) | 81.8 | 82.5 | 9.3 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Moon等，[2019a](#bib.bib166)) | 81.8 | 82.5 | 9.3 |'
- en: '|  | 2020 | (Jiang et al., [2020](#bib.bib88)) | 69.1 | 72.2 | - |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Jiang等，[2020](#bib.bib88)) | 69.1 | 72.2 | - |'
- en: '|  | 2020 | (Li et al., [2020b](#bib.bib114)) | 82.0 | - | - |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Li等，[2020b](#bib.bib114)) | 82.0 | - | - |'
- en: '| Top down | 2021 | (Cheng et al., [2021a](#bib.bib33)) | 87.5 | - | - |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下 | 2021 | (Cheng等，[2021a](#bib.bib33)) | 87.5 | - | - |'
- en: '|  | 2018 | (Mehta et al., [2018](#bib.bib160)) | 65.0 | 69.8 | - |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | (Mehta等，[2018](#bib.bib160)) | 65.0 | 69.8 | - |'
- en: '|  | 2019 | (Mehta et al., [2020](#bib.bib159)) | 70.4 | - | - |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | (Mehta等，[2020](#bib.bib159)) | 70.4 | - | - |'
- en: '|  | 2020 | (Benzine et al., [2020](#bib.bib9)) | 72.0 | - | - |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | (Benzine等，[2020](#bib.bib9)) | 72.0 | - | - |'
- en: '| Bottom up | 2020 | (Zhen et al., [2020](#bib.bib301)) | 73.5 | 80.5 | 9.3
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上 | 2020 | (Zhen 等，[2020](#bib.bib301)) | 73.5 | 80.5 | 9.3 |'
- en: '|  | 2021 | (Cheng et al., [2021b](#bib.bib34)) | 89.6 | - | - |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | (Cheng 等，[2021b](#bib.bib34)) | 89.6 | - | - |'
- en: '| Integrated | 2022 | (Wang et al., [2022b](#bib.bib254)) | 82.7 | - | 13.3
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 综合 | 2022 | (Wang 等，[2022b](#bib.bib254)) | 82.7 | - | 13.3 |'
- en: Table 8\. Comparison of different 3D multi-view HPE approaches on the Human3.6M
    dataset.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8\. 在 Human3.6M 数据集上不同 3D 多视角 HPE 方法的比较。
- en: '| Human3.6M |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Human3.6M |'
- en: '|  |  |  | Protocol 1 | Protocol 1 | FLOPs(G) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 协议 1 | 协议 1 | FLOPs(G) |'
- en: '| Year | Method | Use extra 3D data | MPJPE $\downarrow$ | Normalized MPJPE
    $\downarrow$ | PA-MPJPE $\downarrow$ |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 年 | 方法 | 是否使用额外的3D数据 | MPJPE $\downarrow$ | 归一化MPJPE $\downarrow$ | PA-MPJPE
    $\downarrow$ |'
- en: '| 2019 | (Liang and Lin, [2019](#bib.bib129)) | Yes | 79.9 | - | 45.1 | - |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | (Liang 和 Lin，[2019](#bib.bib129)) | 是 | 79.9 | - | 45.1 | - |'
- en: '| 2019 | (Kocabas et al., [2019](#bib.bib101)) | No | 60.6 | 60.0 | 47.5 |
    - |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | (Kocabas 等，[2019](#bib.bib101)) | 否 | 60.6 | 60.0 | 47.5 | - |'
- en: '| 2019 | (Qiu et al., [2019](#bib.bib199)) | No | 31.2 | - | - | 55 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | (Qiu 等，[2019](#bib.bib199)) | 否 | 31.2 | - | - | 55 |'
- en: '| 2019 | (Qiu et al., [2019](#bib.bib199)) | Yes | 26.2 | - | - | 55 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | (Qiu 等，[2019](#bib.bib199)) | 是 | 26.2 | - | - | 55 |'
- en: '| 2020 | (Remelli et al., [2020](#bib.bib204)) | No | 30.2 | - | - | - |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | (Remelli 等，[2020](#bib.bib204)) | 否 | 30.2 | - | - | - |'
- en: '| 2020 | (Xie et al., [2020](#bib.bib261)) | No | 29.3 | - | - | - |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | (Xie 等，[2020](#bib.bib261)) | 否 | 29.3 | - | - | - |'
- en: '| 2020 | (Zhang et al., [2020g](#bib.bib296)) | Yes | 19.5 | - | - | - |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | (Zhang 等，[2020g](#bib.bib296)) | 是 | 19.5 | - | - | - |'
- en: '| 2021 | (Ma et al., [2021](#bib.bib150)) | No | 25.8 | - | - | 50 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | (Ma 等，[2021](#bib.bib150)) | 否 | 25.8 | - | - | 50 |'
- en: '| 2021 | (Reddy et al., [2021](#bib.bib203)) | Yes | 18.7 | - | - | - |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | (Reddy 等，[2021](#bib.bib203)) | 是 | 18.7 | - | - | - |'
- en: '| 2022 | (Ma et al., [2022](#bib.bib151)) | Yes | 24.4 | - | - | 15 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | (Ma 等，[2022](#bib.bib151)) | 是 | 24.4 | - | - | 15 |'
- en: Summary. As pointed out by Ji et al. (JI et al., [2020](#bib.bib85)), low MPJPE
    does not always indicate an accurate pose estimation as it depends on the predicted
    scale of human shape and skeleton. Although 3DPCK is more robust to incorrect
    joints, it cannot evaluate the precision of correct joints. Existing metrics are
    designed to evaluate the precision of an estimated pose in a single frame. However,
    the temporal consistency and smoothness of the reconstructed human pose cannot
    be examined over continuous frames by existing evaluation metrics. Designing frame-level
    evaluation metrics that can evaluate 3D HPE performance with temporal consistency
    and smoothness remains an open problem.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。正如 Ji 等（JI 等，[2020](#bib.bib85)）所指出的，低 MPJPE 并不总是表示准确的姿态估计，因为它取决于预测的人体形状和骨架的尺度。尽管
    3DPCK 对错误关节更具鲁棒性，但它无法评估正确关节的精确度。现有指标旨在评估单帧中估计姿态的精度。然而，现有评估指标无法检查连续帧上的重建人体姿态的时间一致性和光滑度。设计能够评估具有时间一致性和光滑度的
    3D HPE 性能的帧级评估指标仍然是一个未解的问题。
- en: 4.6\. Performance Comparison of 3D HPE Methods
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6\. 3D HPE 方法的性能比较
- en: 'Single-view single-person: In Table [6](#S4.T6 "Table 6 ‣ 4.4\. Datasets for
    3D HPE ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based Human Pose
    Estimation: A Survey"), it is seen that most 3D single-view single-person HPE
    methods estimate 3D human pose with remarkable precision on the Human3.6M dataset.
    However, despite the fact that the Human3.6M dataset has a large size of training
    and testing data, it only contains 11 actors performing 17 activities in lab environments.
    When estimating 3D pose on in-the-wild data with more complex scenarios, the performance
    of these methods degrades quickly. Estimating 3D pose from videos can achieve
    better performance than from a single image because the temporal consistency is
    preserved.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 单视角单人物：在表 [6](#S4.T6 "表 6 ‣ 4.4\. 3D HPE 数据集 ‣ 4\. 数据集和评估指标 ‣ 基于深度学习的人体姿态估计：综述")
    中可以看到，大多数 3D 单视角单人物 HPE 方法在 Human3.6M 数据集上能以显著精度估计 3D 人体姿态。然而，尽管 Human3.6M 数据集有大量的训练和测试数据，但它仅包含
    11 名演员在实验室环境中执行的 17 种活动。在处理具有更复杂场景的野外数据时，这些方法的性能迅速下降。从视频中估计 3D 姿态能比从单张图像中获得更好的性能，因为时间一致性得以保留。
- en: For skeleton-only methods, 2D-to-3D lifting approaches generally outperform
    direct estimation approaches due to the excellent performance of state-of-the-art
    2D pose detectors. Beyond estimated 3D coordinates of joints, a group of methods
    utilized volumetric models such as SMPL to recover human mesh. These methods still
    reported MPJPE of joints since the datasets do not provide the ground truth mesh
    vertices. However, the reported MPJPE is higher than those methods that only estimate
    3D joints. One of the reasons is that these methods regressed both pose parameters
    and shape parameters, then fed in the volumetric model for mesh reconstruction,
    only evaluating MPJPE of joints can not show their strength.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 对于仅使用骨架的方法，由于最先进的2D姿态检测器的出色表现，2D到3D的提升方法通常优于直接估计方法。除了估计的3D关节坐标外，一些方法利用了像SMPL这样的体积模型来恢复人体网格。这些方法仍然报告了关节的MPJPE，因为数据集没有提供真实的网格顶点。然而，报告的MPJPE高于仅估计3D关节的方法。其中一个原因是这些方法回归了姿态参数和形状参数，然后将其输入体积模型进行网格重建，仅评估关节的MPJPE无法展示它们的优势。
- en: 'Single-view multi-person: Estimating 3D poses in the multi-person setting is
    a harder task than in single-person due to more severe occlusion. As shown in
    Tables [8](#S4.T8 "Table 8 ‣ 4.5\. Evaluation Metrics for 3D HPE ‣ 4\. Datasets
    and Evaluation Metrics ‣ Deep Learning-Based Human Pose Estimation: A Survey"),
    good progress has been made in the single-view multi-person HPE methods in recent
    years. The Top-Down methods perform better than Bottom-Up methods due to the state-of-the-art
    person detection methods and single-person HPE methods. On the other hand, Bottom-Up
    methods are more computationally and time efficient.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '单视角多人的情况：在多人人员设置中估计3D姿态比在单人人员情况下更困难，因为遮挡问题更严重。如表格[8](#S4.T8 "Table 8 ‣ 4.5\.
    Evaluation Metrics for 3D HPE ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based
    Human Pose Estimation: A Survey")所示，近年来单视角多人人员HPE方法取得了显著进展。Top-Down方法由于最先进的人物检测方法和单人人员HPE方法表现更好。另一方面，Bottom-Up方法在计算和时间效率上更具优势。'
- en: 'Multi-view: By comparing the results from Table [6](#S4.T6 "Table 6 ‣ 4.4\.
    Datasets for 3D HPE ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based
    Human Pose Estimation: A Survey") and Table [8](#S4.T8 "Table 8 ‣ 4.5\. Evaluation
    Metrics for 3D HPE ‣ 4\. Datasets and Evaluation Metrics ‣ Deep Learning-Based
    Human Pose Estimation: A Survey"), it is evident that the performance (e.g., MPJPE
    under Protocol 1) of multi-view 3D HPE methods has improved compared to single-view
    3D HPE methods using the same dataset and evaluation metric. Occlusion and depth
    ambiguity can be alleviated through the multi-view setting.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '多视角：通过比较表格[6](#S4.T6 "Table 6 ‣ 4.4\. Datasets for 3D HPE ‣ 4\. Datasets and
    Evaluation Metrics ‣ Deep Learning-Based Human Pose Estimation: A Survey")和表格[8](#S4.T8
    "Table 8 ‣ 4.5\. Evaluation Metrics for 3D HPE ‣ 4\. Datasets and Evaluation Metrics
    ‣ Deep Learning-Based Human Pose Estimation: A Survey")中的结果，可以明显看出，相比于使用相同数据集和评估指标的单视角3D
    HPE方法，多视角3D HPE方法的性能（例如在Protocol 1下的MPJPE）有所提升。多视角设置可以缓解遮挡和深度模糊问题。'
- en: 5\. Applications
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 应用
- en: In this section, we review related works of exploring HPE for a few popular
    applications.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们回顾了一些流行应用中的HPE相关研究。
- en: 'Action recognition, prediction, detection, and tracking: Pose information has
    been utilized as cues for various applications such as action recognition, prediction,
    detection, and tracking. Angelini et al. (Angelini et al., [2018](#bib.bib5))
    proposed a real-time action recognition method using a pose-based algorithm. Yan
    et al. (Yan et al., [2018](#bib.bib268)) leveraged the dynamic skeleton modality
    of pose for action recognition. Markovitz et al. (Markovitz et al., [2020](#bib.bib156))
    studied human pose graphs for anomaly detection of human actions in videos. Cao
    et al. (Cao et al., [2020](#bib.bib16)) used the predicted 3D pose for long-term
    human motion prediction. Sun et al. (Sun et al., [2020](#bib.bib221)) proposed
    a view-invariant probabilistic pose embedding for video alignment. Hua et al.
    (Hua et al., [2023](#bib.bib69)) proposed an attention-based contrastive learning
    framework that integrates local similarity and global features for skeleton-based
    action representations.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 动作识别、预测、检测和跟踪：姿势信息被用作各种应用的线索，如动作识别、预测、检测和跟踪。Angelini等人（Angelini et al., [2018](#bib.bib5)）提出了一种基于姿势的实时动作识别方法。颜等人（Yan
    et al., [2018](#bib.bib268)）利用姿势的动态骨架模态进行动作识别。Markovitz等人（Markovitz et al., [2020](#bib.bib156)）研究了人体姿势图以检测视频中的动作异常。曹等人（Cao
    et al., [2020](#bib.bib16)）使用预测的3D姿势进行长期的人体运动预测。孙等人（Sun et al., [2020](#bib.bib221)）提出了一种用于视频对齐的视图不变概率姿势嵌入。华等人（Hua
    et al., [2023](#bib.bib69)）提出了一种基于注意力的对比学习框架，整合了局部相似性和全局特征，用于骨架基础的动作表示。
- en: Pose-based video surveillance enjoys the advantage of preserving privacy by
    monitoring through pose and human mesh representation instead of human sensitive
    identities. Das et al. (Das et al., [2020](#bib.bib46)) embedded video with poses
    to identify activities of daily living for monitoring human behavior.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 基于姿势的视频监控具有通过姿势和人体网格表示而不是人类敏感身份来保护隐私的优势。Das等人（Das et al., [2020](#bib.bib46)）将视频嵌入姿势中，以识别日常活动，从而监控人类行为。
- en: 'Action correction and online coaching: Some activities such as dancing, sporting,
    and professional training require precise human body control guidance. Normally
    personal trainers are responsible for pose correction and action guidance in a
    face-to-face manner. With the help of 3D HPE and action detection, AI personal
    trainers can make coaching more convenient by simply setting up cameras without
    a personal trainer present. Wang et al. (Wang et al., [2019b](#bib.bib248)) designed
    an AI coaching system with a pose estimation module for personalized athletic
    training assistance.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 动作纠正和在线辅导：一些活动如舞蹈、体育和专业培训需要精确的人体控制指导。通常，私人教练负责面对面的姿势纠正和动作指导。借助3D HPE和动作检测，AI私人教练可以通过设置摄像头而无需私人教练在场，使辅导更加便利。王等人（Wang
    et al., [2019b](#bib.bib248)）设计了一种具有姿势估计模块的AI辅导系统，用于个性化的运动训练辅助。
- en: 'Clothes parsing: The e-commerce trends have brought about a noticeable impact
    on various aspects including clothes purchases. Clothing products in pictures
    can no longer satisfy customers’ demands, and customers hope to see reliable appearances
    as they wear their selected clothes. Clothes parsing (Yu et al., [2019](#bib.bib277))
    (Saito et al., [2019](#bib.bib212)) and pose transfer (Li et al., [2019](#bib.bib124))
    make it possible by inferring the 3D appearance of a person wearing specific clothes.
    HPE can provide plausible human body regions for cloth parsing. Moreover, the
    recommendation system can be upgraded by evaluating appropriateness based on the
    inferred reliable 3D appearance of customers with selected items. Patel et al.
    (Patel et al., [2020](#bib.bib182)) achieved clothing prediction from 3D pose,
    shape, and garment style.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 服装解析：电子商务趋势对各种方面产生了明显影响，包括服装购买。图片中的服装产品已无法满足顾客的需求，顾客希望看到他们所选服装的真实穿着效果。服装解析（Yu
    et al., [2019](#bib.bib277)）（Saito et al., [2019](#bib.bib212)）和姿势转移（Li et al.,
    [2019](#bib.bib124)）通过推断穿着特定服装的人的3D外观使其成为可能。HPE可以为服装解析提供可信的人体区域。此外，通过基于推断的顾客与选定商品的可信3D外观评估适宜性，可以升级推荐系统。Patel等人（Patel
    et al., [2020](#bib.bib182)）实现了从3D姿势、形状和服装风格中进行服装预测。
- en: 'Animation, movie, and gaming: Motion capture is the key component to present
    characters with complex movements and realistic physical interactions in industries
    of animation, movies, and gaming. Equipments are usually expensive and complicated
    to set up. HPE can provide realistic pose information while alleviating the demand
    for professional high-cost equipment (Willett et al., [2020](#bib.bib257))(Liu
    et al., [2020b](#bib.bib137)).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 动画、电影和游戏：动作捕捉是动画、电影和游戏行业中展示具有复杂动作和现实物理互动角色的关键组件。设备通常昂贵且设置复杂。HPE 可以提供逼真的姿态信息，同时减轻对专业高成本设备的需求（Willett
    et al., [2020](#bib.bib257)）（Liu et al., [2020b](#bib.bib137)）。
- en: 'AR and VR: Augmented Reality (AR) technology aims to enhance the interactive
    experience of digital objects in the real-world environment. The objective of
    Virtual Reality (VR) technology is to provide an immersive experience for the
    users. AR and VR devices use human pose information as input to achieve their
    goals of different applications. A cartoon character can be generated in real-world
    scenes to replace a real person. Weng et al. (Weng et al., [2019](#bib.bib256))
    created 3D character animation from a single photo with the help of 3D pose estimation
    and human mesh recovery. Zhang et al. (Zhang et al., [2020d](#bib.bib287)) presented
    a pose-based system that converts broadcast tennis match videos into interactive
    and controllable video sprites.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: AR 和 VR：增强现实（AR）技术旨在提升数字对象在现实环境中的互动体验。虚拟现实（VR）技术的目标是为用户提供沉浸式体验。AR 和 VR 设备使用人体姿势信息作为输入，以实现其不同应用的目标。可以在现实场景中生成卡通角色来替代真实人物。Weng
    等人（Weng et al., [2019](#bib.bib256)）利用 3D 姿态估计和人体网格恢复技术，从单张照片中创建了 3D 角色动画。Zhang
    等人（Zhang et al., [2020d](#bib.bib287)）提出了一种基于姿态的系统，将广播网球比赛视频转换为互动和可控的视频精灵。
- en: 'Healthcare: HPE provides quantitative human motion information that physicians
    can use to diagnose some complex diseases, create rehabilitation training, and
    operate physical therapy. Lu et al. (Lu et al., [2020](#bib.bib145)) designed
    a pose-based estimation system for assessing Parkinson’s disease motor severity.
    Gu et al. (Gu et al., [2019](#bib.bib63)) developed a pose-based physical therapy
    system that patients can be evaluated and advised at their homes. Furthermore,
    such a system can be established to detect abnormal actions and predict subsequent
    actions ahead of time. Alerts are sent immediately if the system determines that
    danger may occur. Chen et al. (Chen et al., [2020d](#bib.bib26)) used HPE algorithms
    for fall detection monitoring in order to provide immediate assistance. Also,
    HPE methods can provide reliable posture labels of patients in hospital environments
    to augment research on neural correlates to natural behaviors (Chen et al., [2018a](#bib.bib22)).'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗保健：HPE 提供的定量人体运动信息可供医生用于诊断一些复杂疾病、制定康复训练计划和进行物理治疗。Lu 等人（Lu et al., [2020](#bib.bib145)）设计了一个基于姿态的估计系统，用于评估帕金森病的运动严重程度。Gu
    等人（Gu et al., [2019](#bib.bib63)）开发了一个基于姿态的物理治疗系统，使患者可以在家中进行评估和建议。此外，还可以建立这样的系统来检测异常动作并预测后续动作。如果系统判断可能会发生危险，会立即发出警报。Chen
    等人（Chen et al., [2020d](#bib.bib26)）使用 HPE 算法进行跌倒检测监控，以提供即时帮助。此外，HPE 方法可以在医院环境中提供患者的可靠姿势标签，以增强对自然行为的神经相关研究（Chen
    et al., [2018a](#bib.bib22)）。
- en: 6\. Conclusion and Future Directions
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论和未来方向
- en: 'In this survey, we have presented a systematic overview of recent deep learning-based
    2D and 3D HPE methods. A comprehensive taxonomy and performance comparison of
    these methods have been covered. Despite great success, there are still many challenges
    as discussed in Sections [2.3](#S2.SS3 "2.3\. 2D HPE Summary ‣ 2\. 2D human pose
    estimation ‣ Deep Learning-Based Human Pose Estimation: A Survey") and [3.3](#S3.SS3
    "3.3\. 3D HPE Summary ‣ 3\. 3D human pose estimation ‣ Deep Learning-Based Human
    Pose Estimation: A Survey"). We further point out a few promising future directions
    to promote advances in HPE research.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '在本次调查中，我们系统地概述了近期基于深度学习的 2D 和 3D HPE 方法。涵盖了这些方法的综合分类和性能比较。尽管取得了巨大成功，但仍存在许多挑战，如第
    [2.3](#S2.SS3 "2.3\. 2D HPE Summary ‣ 2\. 2D human pose estimation ‣ Deep Learning-Based
    Human Pose Estimation: A Survey") 和 [3.3](#S3.SS3 "3.3\. 3D HPE Summary ‣ 3\.
    3D human pose estimation ‣ Deep Learning-Based Human Pose Estimation: A Survey")
    节所讨论。我们进一步指出了一些有前途的未来方向，以推动 HPE 研究的进展。'
- en: •
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Domain adaptation for HPE. For some applications such as estimating human pose
    from infant images (Huang et al., [2020a](#bib.bib74)) or artwork collections(Madhu
    et al., [2020](#bib.bib152)), there are not enough training data with ground truth
    annotations. Moreover, data for these applications exhibit different distributions
    from that of the standard pose datasets. HPE methods trained on existing standard
    datasets may not generalize well across different domains. The recent trend to
    alleviate the domain gap is utilizing GAN-based learning approaches. Nonetheless,
    how to effectively transfer the human pose knowledge to bridge domain gaps remains
    unaddressed.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HPE 的领域自适应。例如，在从婴儿图像（Huang 等，[2020a](#bib.bib74)）或艺术作品集合（Madhu 等，[2020](#bib.bib152)）中估计人体姿势的应用中，缺乏带有真实标注的训练数据。此外，这些应用的数据与标准姿势数据集的分布不同。基于现有标准数据集训练的
    HPE 方法可能无法在不同领域之间良好泛化。当前的趋势是利用基于 GAN 的学习方法来缓解领域差距。然而，如何有效地将人体姿势知识迁移以弥合领域差距仍未得到解决。
- en: •
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Human body models such as SMPL, SMPL-X, GHUM & GHUML, and Adam are used to model
    human mesh representation. However, these models have a huge number of parameters.
    How to reduce the number of parameters while preserving the reconstructed mesh
    quality is an intriguing problem. Also, different people have various deformations
    of body shape. A more effective human body model may utilize other information
    such as BMI (Osman et al., [2020](#bib.bib178)) and silhouette (Li et al., [2020a](#bib.bib127))
    for better generalization.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人体模型如 SMPL、SMPL-X、GHUM & GHUML 和 Adam 被用于建模人体网格表示。然而，这些模型有大量的参数。如何在保持重建网格质量的同时减少参数数量是一个有趣的问题。此外，不同的人体形状有各种变形。更有效的人体模型可能利用其他信息如
    BMI（Osman 等，[2020](#bib.bib178)）和轮廓（Li 等，[2020a](#bib.bib127)）以获得更好的泛化能力。
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most existing methods ignore human interaction with 3D scenes. There are strong
    human-scene relationship constraints that can be explored such as a human subject
    cannot be simultaneously present in the locations of other objects in the scene.
    The physical constraints with semantic cues can provide reliable and realistic
    3D HPE.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数现有方法忽略了人类与 3D 场景的互动。可以探索强的人物与场景关系约束，例如人类主体不能同时出现在场景中其他物体的位置。结合语义提示的物理约束可以提供可靠且真实的
    3D HPE。
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 3D HPE is employed in visual tracking and analysis. Existing 3D HPE from videos
    are not smooth and continuous. One reason is that the evaluation metrics such
    as MPJPE cannot evaluate the smoothness and the degree of realisticness. Appropriate
    frame-level evaluation metrics focusing on temporal consistency and motion smoothness
    should be developed.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3D HPE 被应用于视觉追踪和分析。现有的从视频中获取的 3D HPE 不够平滑和连续。一个原因是像 MPJPE 这样的评估指标无法评估平滑度和真实度。应开发适当的帧级评估指标，重点关注时间一致性和运动平滑性。
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Existing well-trained networks pay less attention to resolution mismatch. The
    training data of HPE networks are usually high-resolution images or videos, which
    may lead to inaccurate estimation when predicting human pose from low-resolution
    input. The contrastive learning scheme (Chen et al., [2020e](#bib.bib25)) (e.g.,
    the original image and its low-resolution version as a positive pair) might be
    helpful for building resolution-aware HPE networks.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有的训练良好的网络对于分辨率不匹配关注较少。HPE 网络的训练数据通常是高分辨率图像或视频，这可能导致在从低分辨率输入中预测人体姿势时估计不准确。对比学习方案（Chen
    等，[2020e](#bib.bib25)）（例如，将原始图像及其低分辨率版本作为正样本对）可能有助于构建分辨率感知的 HPE 网络。
- en: •
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Deep neural networks in vision tasks are vulnerable to adversarial attacks.
    Imperceptible noise can significantly affect the performance of HPE. There are
    few works (Liu et al., [2019](#bib.bib136)) (Jain et al., [2019](#bib.bib83))
    that consider adversarial attacks for HPE. The study of defense against adversarial
    attacks can improve the robustness of HPE networks and facilitate real-world pose-based
    applications.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视觉任务中的深度神经网络易受到对抗性攻击。不可察觉的噪声可以显著影响 HPE 的性能。关于 HPE 的对抗攻击研究较少（Liu 等，[2019](#bib.bib136)）（Jain
    等，[2019](#bib.bib83)）。研究对抗攻击的防御措施可以提高 HPE 网络的鲁棒性，并促进基于姿势的现实应用。
- en: •
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Human body parts may have different movement patterns and shapes due to the
    heterogeneity of the human body. A single shared network architecture may not
    be optimal for estimating all body parts with varying degrees of freedom. Neural
    Architecture Search (NAS) (Elsken et al., [2019](#bib.bib53)) can search the optimal
    architecture for estimating each body part (Chen et al., [2020c](#bib.bib31)).
    Also, NAS can be used for discovering efficient HPE network architectures to reduce
    the computational cost (Zhang et al., [2020b](#bib.bib291)). It is also worth
    exploring multi-objective NAS in HPE when multiple objectives (e.g, latency, accuracy,
    and energy consumption) have to be met.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人体部位由于人体的异质性可能具有不同的运动模式和形状。单一的共享网络架构可能不适合估计所有具有不同自由度的身体部位。神经架构搜索（NAS）(Elsken
    et al., [2019](#bib.bib53)) 可以为每个身体部位搜索最佳架构 (Chen et al., [2020c](#bib.bib31))。此外，NAS
    还可以用于发现高效的HPE网络架构，以降低计算成本 (Zhang et al., [2020b](#bib.bib291))。当需要满足多个目标（例如，延迟、准确性和能耗）时，探索多目标NAS在HPE中的应用也是值得的。
- en: 'HPE workshops and challenges: Finally, we list the HPE workshops and challenges
    (2017-2022) on our project page ([https://github.com/zczcwh/DL-HPE](https://github.com/zczcwh/DL-HPE))
    to facilitate research in this field.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: HPE工作坊和挑战：最后，我们在我们的项目页面上列出了HPE工作坊和挑战（2017-2022）（[https://github.com/zczcwh/DL-HPE](https://github.com/zczcwh/DL-HPE)），以促进该领域的研究。
- en: References
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Andriluka et al. (2018a) M. Andriluka, U. Iqbal, E. Ensafutdinov, L. Pishchulin,
    A. Milan, J. Gall, and Schiele B. 2018a. PoseTrack: A Benchmark for Human Pose
    Estimation and Tracking. In *CVPR*.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Andriluka et al. (2018a) M. Andriluka, U. Iqbal, E. Ensafutdinov, L. Pishchulin,
    A. Milan, J. Gall, 和 Schiele B. 2018a. PoseTrack: 人类姿态估计和跟踪的基准。在*CVPR*中。'
- en: 'Andriluka et al. (2018b) Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov,
    Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. 2018b. Posetrack:
    A benchmark for human pose estimation and tracking. In *CVPR*.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Andriluka et al. (2018b) Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov,
    Leonid Pishchulin, Anton Milan, Juergen Gall, 和 Bernt Schiele. 2018b. Posetrack:
    人类姿态估计和跟踪的基准。在*CVPR*中。'
- en: 'Andriluka et al. (2014) Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler,
    and Bernt Schiele. 2014. 2d human pose estimation: New benchmark and state of
    the art analysis. In *CVPR*.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andriluka et al. (2014) Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler,
    和 Bernt Schiele. 2014. 2D人类姿态估计：新的基准和最新分析。在*CVPR*中。
- en: 'Angelini et al. (2018) Federico Angelini, Zeyu Fu, Yang Long, Ling Shao, and
    Syed Mohsen Naqvi. 2018. Actionxpose: A novel 2d multi-view pose-based algorithm
    for real-time human action recognition. In *arXiv preprint arXiv:1810.12126*.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Angelini et al. (2018) Federico Angelini, Zeyu Fu, Yang Long, Ling Shao, 和
    Syed Mohsen Naqvi. 2018. Actionxpose: 一种新颖的基于2D多视角姿态的实时人类动作识别算法。在*arXiv预印本 arXiv:1810.12126*中。'
- en: Arnab et al. (2019) Anurag Arnab, Carl Doersch, and Andrew Zisserman. 2019.
    Exploiting Temporal Context for 3D Human Pose Estimation in the Wild. In *CVPR*.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arnab et al. (2019) Anurag Arnab, Carl Doersch, 和 Andrew Zisserman. 2019. 利用时间上下文进行野外3D人类姿态估计。在*CVPR*中。
- en: 'Artacho and Savakis (2020) Bruno Artacho and Andreas Savakis. 2020. UniPose:
    Unified Human Pose Estimation in Single Images and Videos. In *CVPR*.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Artacho and Savakis (2020) Bruno Artacho 和 Andreas Savakis. 2020. UniPose：单图像和视频中的统一人类姿态估计。在*CVPR*中。
- en: Belagiannis and Zisserman (2017) Vasileios Belagiannis and Andrew Zisserman.
    2017. Recurrent human pose estimation. In *FG*.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belagiannis and Zisserman (2017) Vasileios Belagiannis 和 Andrew Zisserman. 2017.
    递归人类姿态估计。在*FG*中。
- en: 'Benzine et al. (2020) Abdallah Benzine, Florian Chabot, Bertrand Luvison, Quoc Cuong
    Pham, and Catherine Achard. 2020. PandaNet: Anchor-Based Single-Shot Multi-Person
    3D Pose Estimation. In *CVPR*.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Benzine et al. (2020) Abdallah Benzine, Florian Chabot, Bertrand Luvison, Quoc
    Cuong Pham, 和 Catherine Achard. 2020. PandaNet: 基于锚点的单次多人物3D姿态估计。在*CVPR*中。'
- en: Bertasius et al. (2019) Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo
    Shi, and Lorenzo Torresani. 2019. Learning temporal pose estimation from sparsely-labeled
    videos. In *NeurIPS*.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertasius et al. (2019) Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo
    Shi, 和 Lorenzo Torresani. 2019. 从稀疏标注的视频中学习时间姿态估计。在*NeurIPS*中。
- en: 'Bogo et al. (2016) Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
    Gehler, Javier Romero, and Michael J. Black. 2016. Keep it SMPL: Automatic Estimation
    of 3D Human Pose and Shape from a Single Image. In *ECCV*.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogo et al. (2016) Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
    Gehler, Javier Romero, 和 Michael J. Black. 2016. 保持SMPL：从单张图像自动估计3D人类姿态和形状。在*ECCV*中。
- en: Bulat and Tzimiropoulos (2016) Adrian Bulat and Georgios Tzimiropoulos. 2016.
    Human pose estimation via convolutional part heatmap regression. In *ECCV*.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulat and Tzimiropoulos (2016) Adrian Bulat 和 Georgios Tzimiropoulos. 2016.
    通过卷积部件热图回归进行人类姿态估计。在*ECCV*中。
- en: Burenius et al. (2013) M. Burenius, J. Sullivan, and S. Carlsson. 2013. 3D Pictorial
    Structures for Multiple View Articulated Pose Estimation. In *CVPR*.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burenius等（2013）M. Burenius, J. Sullivan, 和 S. Carlsson。2013。用于多视角关节姿态估计的3D图形结构。在*CVPR*上。
- en: Cai et al. (2019) Y. Cai, L. Ge, J. Liu, J. Cai, T. Cham, J. Yuan, and N. M.
    Thalmann. 2019. Exploiting Spatial-Temporal Relationships for 3D Pose Estimation
    via Graph Convolutional Networks. In *ICCV*.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai等（2019）Y. Cai, L. Ge, J. Liu, J. Cai, T. Cham, J. Yuan, 和 N. M. Thalmann。2019。通过图卷积网络利用空间-时间关系进行3D姿态估计。在*ICCV*上。
- en: Cai et al. (2020) Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin, Angang
    Du, Haoqian Wang, Xinyu Zhou, Erjin Zhou, Xiangyu Zhang, and Jian Sun. 2020. Learning
    Delicate Local Representations for Multi-Person Pose Estimation. In *arXiv preprint
    arXiv:2003.04030*.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai等（2020）Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin, Angang Du,
    Haoqian Wang, Xinyu Zhou, Erjin Zhou, Xiangyu Zhang, 和 Jian Sun。2020。学习细腻的局部表示以进行多人姿态估计。在*arXiv预印本arXiv:2003.04030*上。
- en: Cao et al. (2020) Zhe Cao, Hang Gao, Karttikeya Mangalam, Qizhi Cai, Minh Vo,
    and Jitendra Malik. 2020. Long-term human motion prediction with scene context.
    In *ECCV*.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等（2020）Zhe Cao, Hang Gao, Karttikeya Mangalam, Qizhi Cai, Minh Vo, 和 Jitendra
    Malik。2020。具有场景上下文的长期人类运动预测。在*ECCV*上。
- en: Cao et al. (2017) Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017.
    Realtime multi-person 2d pose estimation using part affinity fields. In *CVPR*.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等（2017）Zhe Cao, Tomas Simon, Shih-En Wei, 和 Yaser Sheikh。2017。使用部件亲和场的实时多人2D姿态估计。在*CVPR*上。
- en: Carreira et al. (2016) Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki,
    and Jitendra Malik. 2016. Human pose estimation with iterative error feedback.
    In *CVPR*.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carreira等（2016）Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, 和 Jitendra
    Malik。2016。带有迭代误差反馈的人体姿态估计。在*CVPR*上。
- en: Chen and Ramanan (2017) Ching-Hang Chen and Deva Ramanan. 2017. 3D Human Pose
    Estimation = 2D Pose Estimation + Matching. In *CVPR*.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen和Ramanan（2017）Ching-Hang Chen和Deva Ramanan。2017。3D人体姿态估计 = 2D姿态估计 + 匹配。在*CVPR*上。
- en: Chen et al. (2019) Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dylan Drover,
    Rohith MV, Stefan Stojanov, and James M. Rehg. 2019. Unsupervised 3D Pose Estimation
    With Geometric Self-Supervision. In *CVPR*.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2019）Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dylan Drover, Rohith
    MV, Stefan Stojanov, 和 James M. Rehg。2019。通过几何自监督进行无监督3D姿态估计。在*CVPR*上。
- en: Chen et al. (2020b) He Chen, Pengfei Guo, Pengfei Li, Gim Hee Lee, and Gregory
    Chirikjian. 2020b. Multi-person 3D Pose Estimation in Crowded Scenes Based on
    Multi-View Geometry. In *ECCV*.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2020b）He Chen, Pengfei Guo, Pengfei Li, Gim Hee Lee, 和 Gregory Chirikjian。2020b。在拥挤场景中基于多视角几何的多人3D姿态估计。在*ECCV*上。
- en: Chen et al. (2018a) Kenny Chen, Paolo Gabriel, Abdulwahab Alasfour, Chenghao
    Gong, Werner K Doyle, Orrin Devinsky, Daniel Friedman, Patricia Dugan, Lucia Melloni,
    Thomas Thesen, et al. 2018a. Patient-specific pose estimation in clinical environments.
    In *JTEHM*, Vol. 6\. 1–11.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2018a）Kenny Chen, Paolo Gabriel, Abdulwahab Alasfour, Chenghao Gong, Werner
    K Doyle, Orrin Devinsky, Daniel Friedman, Patricia Dugan, Lucia Melloni, Thomas
    Thesen, 等。2018a。临床环境中的患者特定姿态估计。在*JTEHM*，第6卷，1–11页。
- en: Chen et al. (2020a) Long Chen, Haizhou Ai, Rui Chen, Zijie Zhuang, and Shuang
    Liu. 2020a. Cross-View Tracking for Multi-Human 3D Pose Estimation at Over 100
    FPS. In *CVPR*.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2020a）Long Chen, Haizhou Ai, Rui Chen, Zijie Zhuang, 和 Shuang Liu。2020a。以超过100FPS进行多人3D姿态估计的跨视角跟踪。在*CVPR*上。
- en: Chen et al. (2021) Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili
    Chen, and Jiebo Luo. 2021. Anatomy-aware 3D Human Pose Estimation with Bone-based
    Pose Decomposition. In *TCSVT*, Vol. 32\. 198–209.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2021）Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili Chen, 和
    Jiebo Luo。2021。基于骨骼的姿态分解的解剖学意识3D人体姿态估计。在*TCSVT*，第32卷，198–209页。
- en: Chen et al. (2020e) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. 2020e. A simple framework for contrastive learning of visual representations.
    In *arXiv preprint arXiv:2002.05709*.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2020e）Ting Chen, Simon Kornblith, Mohammad Norouzi, 和 Geoffrey Hinton。2020e。一个用于对比学习视觉表征的简单框架。在*arXiv预印本arXiv:2002.05709*上。
- en: Chen et al. (2020d) Weiming Chen, Zijie Jiang, Hailin Guo, and Xiaoyang Ni.
    2020d. Fall Detection Based on Key Points of Human-Skeleton Using OpenPose. In
    *Symmetry*, Vol. 12\. 744.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2020d）Weiming Chen, Zijie Jiang, Hailin Guo, 和 Xiaoyang Ni。2020d。基于OpenPose的人体骨架关键点的跌倒检测。在*Symmetry*，第12卷，744页。
- en: Chen and Yuille (2014) Xianjie Chen and Alan L Yuille. 2014. Articulated pose
    estimation by a graphical model with image dependent pairwise relations. In *NeurIPS*.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen和Yuille（2014）Xianjie Chen 和 Alan L Yuille。2014。通过具有图像依赖的成对关系的图模型进行关节姿态估计。在*NeurIPS*上。
- en: 'Chen et al. (2017) Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and Jian
    Yang. 2017. Adversarial posenet: A structure-aware convolutional network for human
    pose estimation. In *ICCV*.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2017）Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, 和 Jian Yang. 2017.
    对抗姿态网络：一种结构感知卷积网络用于人体姿态估计。发表于*ICCV*。
- en: 'Chen et al. (2020f) Yucheng Chen, Yingli Tian, and Mingyi He. 2020f. Monocular
    human pose estimation: A survey of deep learning-based methods. In *CVIU*, Vol. 192\.
    102897.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2020f）Yucheng Chen, Yingli Tian, 和 Mingyi He. 2020f. 单目人体姿态估计：基于深度学习的方法综述。发表于*CVIU*，第192卷，102897。
- en: Chen et al. (2018b) Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang,
    Gang Yu, and Jian Sun. 2018b. Cascaded pyramid network for multi-person pose estimation.
    In *CVPR*.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2018b）Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu,
    和 Jian Sun. 2018b. 用于多人的姿态估计的级联金字塔网络。发表于*CVPR*。
- en: 'Chen et al. (2020c) Zerui Chen, Yan Huang, Hongyuan Yu, Bin Xue, Ke Han, Yiru
    Guo, and Liang Wang. 2020c. Towards Part-aware Monocular 3D Human Pose Estimation:
    An Architecture Search Approach. In *ECCV*.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2020c）Zerui Chen, Yan Huang, Hongyuan Yu, Bin Xue, Ke Han, Yiru Guo, 和
    Liang Wang. 2020c. 迈向部分感知的单目3D人体姿态估计：一种架构搜索方法。发表于*ECCV*。
- en: 'Cheng et al. (2020) Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S
    Huang, and Lei Zhang. 2020. HigherHRNet: Scale-Aware Representation Learning for
    Bottom-Up Human Pose Estimation. In *CVPR*.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等（2020）Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S Huang,
    和 Lei Zhang. 2020. HigherHRNet：面向尺度的表示学习用于自下而上的人体姿态估计。发表于*CVPR*。
- en: Cheng et al. (2021a) Yu Cheng, Bo Wang, Bo Yang, and Robby T. Tan. 2021a. Graph
    and Temporal Convolutional Networks for 3D Multi-person Pose Estimation in Monocular
    Videos. In *AAAI*.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等（2021a）Yu Cheng, Bo Wang, Bo Yang, 和 Robby T. Tan. 2021a. 图神经网络和时序卷积网络用于单目视频中的3D多人体姿态估计。发表于*AAAI*。
- en: Cheng et al. (2021b) Yu Cheng, Bo Wang, Bo Yang, and Robby T. Tan. 2021b. Monocular
    3D Multi-Person Pose Estimation by Integrating Top-Down and Bottom-Up Networks.
    In *CVPR*.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等（2021b）Yu Cheng, Bo Wang, Bo Yang, 和 Robby T. Tan. 2021b. 通过集成自上而下和自下而上的网络进行单目3D多人体姿态估计。发表于*CVPR*。
- en: Cheng et al. (2019) Y. Cheng, B. Yang, B. Wang, Y. Wending, and R. Tan. 2019.
    Occlusion-Aware Networks for 3D Human Pose Estimation in Video. In *ICCV*.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等（2019）Y. Cheng, B. Yang, B. Wang, Y. Wending, 和 R. Tan. 2019. 具有遮挡感知的网络用于视频中的3D人类姿态估计。发表于*ICCV*。
- en: Cheng et al. (2019) Yu Cheng, Bo Yang, Bo Wang, Wending Yan, and Robby T. Tan.
    2019. Occlusion-Aware Networks for 3D Human Pose Estimation in Video. In *ICCV*.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等（2019）Yu Cheng, Bo Yang, Bo Wang, Wending Yan, 和 Robby T. Tan. 2019. 具有遮挡感知的网络用于视频中的3D人类姿态估计。发表于*ICCV*。
- en: Cho et al. (2022) Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh. 2022. Cross-Attention
    of Disentangled Modalities for 3D Human Mesh Recovery with Transformers. In *ECCV*.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho等（2022）Junhyeong Cho, Kim Youwang, 和 Tae-Hyun Oh. 2022. 采用跨模态注意力的3D人类网格恢复，基于变换器技术。发表于*ECCV*。
- en: Choi et al. (2021) Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu
    Lee. 2021. Beyond Static Features for Temporally Consistent 3D Human Pose and
    Shape From a Video. In *CVPR*.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi等（2021）Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, 和 Kyoung Mu Lee. 2021.
    超越静态特征的时间一致性3D人体姿态和形状估计。发表于*CVPR*。
- en: 'Choi et al. (2020) Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. 2020. Pose2Mesh:
    Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human
    Pose. In *ECCV*.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi等（2020）Hongsuk Choi, Gyeongsik Moon, 和 Kyoung Mu Lee. 2020. Pose2Mesh：用于从2D人体姿态恢复3D人体姿态和网格的图卷积网络。发表于*ECCV*。
- en: Chou et al. (2018) Chia-Jung Chou, Jui-Ting Chien, and Hwann-Tzong Chen. 2018.
    Self adversarial training for human pose estimation. In *APSIPA ASC*.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chou等（2018）Chia-Jung Chou, Jui-Ting Chien, 和 Hwann-Tzong Chen. 2018. 用于人体姿态估计的自对抗训练。发表于*APSIPA
    ASC*。
- en: Chu et al. (2016) Xiao Chu, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. 2016.
    Structured feature learning for pose estimation. In *CVPR*.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu等（2016）Xiao Chu, Wanli Ouyang, Hongsheng Li, 和 Xiaogang Wang. 2016. 结构化特征学习用于姿态估计。发表于*CVPR*。
- en: Chu et al. (2017) Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille,
    and Xiaogang Wang. 2017. Multi-context attention for human pose estimation. In
    *CVPR*.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu等（2017）Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille, 和 Xiaogang
    Wang. 2017. 人体姿态估计的多上下文注意力机制。发表于*CVPR*。
- en: Ci et al. (2019) H. Ci, C. Wang, X. Ma, and Y. Wang. 2019. Optimizing Network
    Structure for 3D Human Pose Estimation. In *ICCV*.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ci等（2019）H. Ci, C. Wang, X. Ma, 和 Y. Wang. 2019. 优化网络结构以进行3D人体姿态估计。发表于*ICCV*。
- en: 'Clever et al. (2020) Henry M. Clever, Zackory Erickson, Ariel Kapusta, Greg
    Turk, Karen Liu, and Charles C. Kemp. 2020. Bodies at Rest: 3D Human Pose and
    Shape Estimation From a Pressure Image Using Synthetic Data. In *CVPR*.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clever等（2020）Henry M. Clever, Zackory Erickson, Ariel Kapusta, Greg Turk, Karen
    Liu, 和 Charles C. Kemp. 2020. 静态身体：利用合成数据从压力图像中估计3D人体姿态和形状。发表于*CVPR*。
- en: Dabral et al. (2018) Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer
    Afaque, Abhishek Sharma, and Arjun Jain. 2018. Learning 3D Human Pose from Structure
    and Motion. In *ECCV*.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dabral et al. (2018) Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer
    Afaque, Abhishek Sharma 和 Arjun Jain. 2018. 从结构和运动中学习 3D 人体姿态。发表于 *ECCV*。
- en: 'Das et al. (2020) Srijan Das, Saurav Sharma, Rui Dai, François Brémond, and
    Monique Thonnat. 2020. VPN: Learning Video-Pose Embedding for Activities of Daily
    Living. In *ECCV*.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Das et al. (2020) Srijan Das, Saurav Sharma, Rui Dai, François Brémond 和 Monique
    Thonnat. 2020. VPN: 学习视频姿态嵌入以进行日常活动。发表于 *ECCV*。'
- en: Debnath et al. (2018) Bappaditya Debnath, Mary O’Brien, Motonori Yamaguchi,
    and Ardhendu Behera. 2018. Adapting MobileNets for mobile based upper body pose
    estimation. In *AVSS*.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debnath et al. (2018) Bappaditya Debnath, Mary O’Brien, Motonori Yamaguchi 和
    Ardhendu Behera. 2018. 为移动设备适配 MobileNets 以进行上半身姿态估计。发表于 *AVSS*。
- en: 'Doering et al. (2018) Andreas Doering, Umar Iqbal, and Juergen Gall. 2018.
    Joint flow: Temporal flow fields for multi person tracking. In *arXiv preprint
    arXiv:1805.04596*.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doering et al. (2018) Andreas Doering, Umar Iqbal 和 Juergen Gall. 2018. 联合流：用于多人人物跟踪的时间流场。发表于
    *arXiv preprint arXiv:1805.04596*。
- en: Dong et al. (2019) Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao, and Xiaowei
    Zhou. 2019. Fast and Robust Multi-Person 3D Pose Estimation From Multiple Views.
    In *CVPR*.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2019) Junting Dong, Wen Jiang, Qixing Huang, Hujun Bao 和 Xiaowei
    Zhou. 2019. 从多个视角快速而稳健的多人人体 3D 姿态估计。发表于 *CVPR*。
- en: Dong et al. (2021) Zijian Dong, Jie Song, Xu Chen, Chen Guo, and Otmar Hilliges.
    2021. Shape-aware Multi-Person Pose Estimation from Multi-view Images. In *ICCV*.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2021) Zijian Dong, Jie Song, Xu Chen, Chen Guo 和 Otmar Hilliges.
    2021. 形状感知的多人人体姿态估计：来自多视角图像。发表于 *ICCV*。
- en: Drover et al. (2018) Dylan Drover, Ching-Hang Chen, Amit Agrawal, Ambrish Tyagi,
    and Cong Phuoc Huynh. 2018. Can 3d pose be learned from 2d projections alone?.
    In *ECCV*.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drover et al. (2018) Dylan Drover, Ching-Hang Chen, Amit Agrawal, Ambrish Tyagi
    和 Cong Phuoc Huynh. 2018. 3D 姿态能否仅通过 2D 投影学习？发表于 *ECCV*。
- en: Eichner et al. (2012) Marcin Eichner, Manuel Marin-Jimenez, Andrew Zisserman,
    and Vittorio Ferrari. 2012. 2d articulated human pose estimation and retrieval
    in (almost) unconstrained still images. In *IJCV*, Vol. 99\. 190–214.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eichner et al. (2012) Marcin Eichner, Manuel Marin-Jimenez, Andrew Zisserman
    和 Vittorio Ferrari. 2012. 在（几乎）不受限制的静态图像中进行 2D 关节人体姿态估计和检索。发表于 *IJCV*，第 99 卷，第
    190–214 页。
- en: 'Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019.
    Neural Architecture Search: A Survey. In *JMLR*, Vol. 20. 1997–2017.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen 和 Frank Hutter. 2019.
    神经架构搜索：综述。发表于 *JMLR*，第 20 卷，第 1997–2017 页。
- en: Fabbri et al. (2020) Matteo Fabbri, Fabio Lanzi, Simone Calderara, Stefano Alletto,
    and Rita Cucchiara. 2020. Compressed Volumetric Heatmaps for Multi-Person 3D Pose
    Estimation. In *CVPR*.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fabbri et al. (2020) Matteo Fabbri, Fabio Lanzi, Simone Calderara, Stefano Alletto
    和 Rita Cucchiara. 2020. 用于多人人体 3D 姿态估计的压缩体积热图。发表于 *CVPR*。
- en: 'Fan et al. (2015) Xiaochuan Fan, Kang Zheng, Yuewei Lin, and Song Wang. 2015.
    Combining local appearance and holistic view: Dual-source deep neural networks
    for human pose estimation. In *CVPR*.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan et al. (2015) Xiaochuan Fan, Kang Zheng, Yuewei Lin 和 Song Wang. 2015. 结合局部外观和整体视角：用于人体姿态估计的双源深度神经网络。发表于
    *CVPR*。
- en: 'Fang et al. (2017) Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. 2017.
    Rmpe: Regional multi-person pose estimation. In *ICCV*.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang et al. (2017) Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai 和 Cewu Lu. 2017. RMPE:
    区域多人体姿态估计。发表于 *ICCV*。'
- en: Fieraru et al. (2018) Mihai Fieraru, Anna Khoreva, Leonid Pishchulin, and Bernt
    Schiele. 2018. Learning to refine human pose estimation. In *CVPR Workshops*.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fieraru et al. (2018) Mihai Fieraru, Anna Khoreva, Leonid Pishchulin 和 Bernt
    Schiele. 2018. 学习优化人体姿态估计。发表于 *CVPR Workshops*。
- en: Fisch and Clark (2020) Martin Fisch and Ronald Clark. 2020. Orientation Keypoints
    for 6D Human Pose Estimation. In *arXiv preprint arXiv:2009.04930*.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fisch 和 Clark (2020) Martin Fisch 和 Ronald Clark. 2020. 用于 6D 人体姿态估计的方向关键点。发表于
    *arXiv preprint arXiv:2009.04930*。
- en: Georgakis et al. (2020) Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence
    Chen, Jana Kosecka, and Ziyan Wu. 2020. Hierarchical Kinematic Human Mesh Recovery.
    In *ECCV*.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Georgakis et al. (2020) Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence
    Chen, Jana Kosecka 和 Ziyan Wu. 2020. 分层运动学人体网格恢复。发表于 *ECCV*。
- en: Gkioxari et al. (2016) Georgia Gkioxari, Alexander Toshev, and Navdeep Jaitly.
    2016. Chained predictions using convolutional neural networks. In *ECCV*.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gkioxari et al. (2016) Georgia Gkioxari, Alexander Toshev 和 Navdeep Jaitly.
    2016. 使用卷积神经网络的链式预测。发表于 *ECCV*。
- en: 'Gong et al. (2016) Wenjuan Gong, Xuena Zhang, Jordi Gonzàlez, Andrews Sobral,
    Thierry Bouwmans, Changhe Tu, and El-hadi Zahzah. 2016. Human pose estimation
    from monocular images: A comprehensive survey. In *Sensors*, Vol. 16\. 1966.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. (2016) Wenjuan Gong, Xuena Zhang, Jordi Gonzàlez, Andrews Sobral,
    Thierry Bouwmans, Changhe Tu 和 El-hadi Zahzah. 2016. 从单目图像中进行人体姿态估计：一个全面的综述。发表于
    *Sensors*，第 16 卷，第 1966 页。
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. In *NeurIPS*.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 2014.
    生成对抗网络。在 *NeurIPS*。
- en: Gu et al. (2019) Yiwen Gu, Shreya Pandit, Elham Saraee, Timothy Nordahl, Terry
    Ellis, and Margrit Betke. 2019. Home-based physical therapy with an interactive
    computer vision system. In *ICCV Workshops*.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2019) Yiwen Gu, Shreya Pandit, Elham Saraee, Timothy Nordahl, Terry
    Ellis, 和 Margrit Betke. 2019. 基于家庭的物理治疗与互动计算机视觉系统。在 *ICCV Workshops*。
- en: Guo et al. (2018) Hengkai Guo, Tang Tang, Guozhong Luo, Riwei Chen, Yongchen
    Lu, and Linfu Wen. 2018. Multi-domain pose network for multi-person pose estimation
    and tracking. In *ECCV Workshops*.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2018) Hengkai Guo, Tang Tang, Guozhong Luo, Riwei Chen, Yongchen
    Lu, 和 Linfu Wen. 2018. 用于多人体姿态估计和跟踪的多领域姿态网络。在 *ECCV Workshops*。
- en: Habibie et al. (2019) I. Habibie, W. Xu, D. Mehta, G. Pons-Moll, and C. Theobalt.
    2019. In the Wild Human Pose Estimation Using Explicit 2D Features and Intermediate
    3D Representations. In *CVPR*.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Habibie et al. (2019) I. Habibie, W. Xu, D. Mehta, G. Pons-Moll, 和 C. Theobalt.
    2019. 使用显式 2D 特征和中间 3D 表征进行野外人体姿态估计。在 *CVPR*。
- en: Hassan et al. (2019) Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and
    Michael J. Black. 2019. Resolving 3D Human Pose Ambiguities with 3D Scene Constraints.
    In *ICCV*.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassan et al. (2019) Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, 和
    Michael J. Black. 2019. 通过 3D 场景约束解决 3D 人体姿态模糊问题。在 *ICCV*。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *CVPR*.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 2016.
    图像识别的深度残差学习。在 *CVPR*。
- en: 'Holte et al. (2012) Michael B Holte, Cuong Tran, Mohan M Trivedi, and Thomas B
    Moeslund. 2012. Human pose estimation and activity recognition from multi-view
    videos: Comparative explorations of recent developments. In *IEEE J Sel Top Signal
    Process*, Vol. 6\. 538–552.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holte et al. (2012) Michael B Holte, Cuong Tran, Mohan M Trivedi, 和 Thomas B
    Moeslund. 2012. 从多视角视频中进行人体姿态估计和活动识别：对近期发展的比较探索。在 *IEEE J Sel Top Signal Process*,
    第 6 卷. 538–552。
- en: Hua et al. (2023) Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen
    Chen, and Shiqian Wu. 2023. Part Aware Contrastive Learning for Self-Supervised
    Action Recognition. In *IJCAI*.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua et al. (2023) Yilei Hua, Wenhan Wu, Ce Zheng, Aidong Lu, Mengyuan Liu, Chen
    Chen, 和 Shiqian Wu. 2023. 面向部分的对比学习用于自监督动作识别。在 *IJCAI*。
- en: Huang et al. (2020b) Congzhentao Huang, Shuai Jiang, Yang Li, Ziyue Zhang, Jason
    Traish, Chen Deng, Sam Ferguson, and Richard Yi Da Xu. 2020b. End-to-end Dynamic
    Matching Network for Multi-view Multi-person 3d Pose Estimation. In *ECCV*.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2020b) Congzhentao Huang, Shuai Jiang, Yang Li, Ziyue Zhang, Jason
    Traish, Chen Deng, Sam Ferguson, 和 Richard Yi Da Xu. 2020b. 用于多视角多人体 3D 姿态估计的端到端动态匹配网络。在
    *ECCV*。
- en: 'Huang et al. (2020c) Fuyang Huang, Ailing Zeng, Minhao Liu, Qiuxia Lai, and
    Qiang Xu. 2020c. DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation
    from Multi-View Image. In *WACV*.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2020c) Fuyang Huang, Ailing Zeng, Minhao Liu, Qiuxia Lai, 和 Qiang
    Xu. 2020c. DeepFuse: 一种 IMU 感知网络，用于从多视角图像实时估计 3D 人体姿态。在 *WACV*。'
- en: 'Huang et al. (2020d) Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang. 2020d.
    The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose
    Estimation. In *CVPR*.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2020d) Junjie Huang, Zheng Zhu, Feng Guo, 和 Guan Huang. 2020d.
    魔鬼在细节中：深入探讨无偏数据处理在人体姿态估计中的应用。在 *CVPR*。
- en: Huang et al. (2017) Shaoli Huang, Mingming Gong, and Dacheng Tao. 2017. A coarse-fine
    network for keypoint localization. In *ICCV*.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2017) Shaoli Huang, Mingming Gong, 和 Dacheng Tao. 2017. 用于关键点定位的粗细网络。在
    *ICCV*。
- en: Huang et al. (2020a) Xiaofei Huang, Nihang Fu, Shuangjun Liu, Kathan Vyas, Amirreza
    Farnoosh, and Sarah Ostadabbas. 2020a. Invariant Representation Learning for Infant
    Pose Estimation with Small Data. In *arXiv preprint arXiv:2010.06100*.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2020a) Xiaofei Huang, Nihang Fu, Shuangjun Liu, Kathan Vyas, Amirreza
    Farnoosh, 和 Sarah Ostadabbas. 2020a. 用于婴儿姿态估计的具有小数据的不可变表示学习。在 *arXiv preprint
    arXiv:2010.06100*。
- en: 'Huang et al. (2018) Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J.
    Black, Otmar Hilliges, and Gerard Pons-Moll. 2018. Deep Inertial Poser: Learning
    to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time. In *ACM
    TOG*.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2018) Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J.
    Black, Otmar Hilliges, 和 Gerard Pons-Moll. 2018. Deep Inertial Poser: 学习从稀疏惯性测量中实时重建人体姿态。在
    *ACM TOG*。'
- en: 'Insafutdinov et al. (2017) Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin,
    Siyu Tang, Evgeny Levinkov, Bjoern Andres, and Bernt Schiele. 2017. Arttrack:
    Articulated multi-person tracking in the wild. In *CVPR*.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Insafutdinov 等人 (2017) Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin,
    Siyu Tang, Evgeny Levinkov, Bjoern Andres 和 Bernt Schiele。2017。Arttrack：在野外的关节化多人体跟踪。在
    *CVPR*。
- en: 'Insafutdinov et al. (2016) Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres,
    Mykhaylo Andriluka, and Bernt Schiele. 2016. Deepercut: A deeper, stronger, and
    faster multi-person pose estimation model. In *ECCV*.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Insafutdinov 等人 (2016) Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres,
    Mykhaylo Andriluka 和 Bernt Schiele。2016。Deepercut：更深、更强、更快的多人体姿态估计模型。在 *ECCV*。
- en: 'Ionescu et al. (2014) C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu.
    2014. Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing
    in Natural Environments. In *IEEE TPAMI*, Vol. 36. 1325–1339.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ionescu 等人 (2014) C. Ionescu, D. Papava, V. Olaru 和 C. Sminchisescu。2014。Human3.6M：自然环境中3D人体感知的大规模数据集和预测方法。在
    *IEEE TPAMI*，第36卷。1325–1339。
- en: Iqbal and Gall (2016) Umar Iqbal and Juergen Gall. 2016. Multi-person pose estimation
    with local joint-to-person associations. In *ECCV*.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iqbal 和 Gall (2016) Umar Iqbal 和 Juergen Gall。2016。带有局部关节到人体关联的多人体姿态估计。在 *ECCV*。
- en: Isogawa et al. (2020) Mariko Isogawa, Ye Yuan, Matthew O’Toole, and Kris M.
    Kitani. 2020. Optical Non-Line-of-Sight Physics-Based 3D Human Pose Estimation.
    In *CVPR*.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isogawa 等人 (2020) Mariko Isogawa, Ye Yuan, Matthew O’Toole 和 Kris M. Kitani。2020。基于光学非视线物理的3D人体姿态估计。在
    *CVPR*。
- en: Jahangiri and Yuille (2017) Ehsan Jahangiri and Alan L Yuille. 2017. Generating
    multiple diverse hypotheses for human 3d pose consistent with 2d joint detections.
    In *ICCV Workshops*.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jahangiri 和 Yuille (2017) Ehsan Jahangiri 和 Alan L Yuille。2017。生成与2D关节检测一致的多种不同假设的人体3D姿态。在
    *ICCV Workshops*。
- en: 'Jain et al. (2014) Arjun Jain, Jonathan Tompson, Yann LeCun, and Christoph
    Bregler. 2014. Modeep: A deep learning framework using motion features for human
    pose estimation. In *ACCV*.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等人 (2014) Arjun Jain, Jonathan Tompson, Yann LeCun 和 Christoph Bregler。2014。Modeep：一个使用运动特征进行人体姿态估计的深度学习框架。在
    *ACCV*。
- en: Jain et al. (2019) Naman Jain, Sahil Shah, Abhishek Kumar, and Arjun Jain. 2019.
    On the Robustness of Human Pose Estimation. In *CVPR Workshops*.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等人 (2019) Naman Jain, Sahil Shah, Abhishek Kumar 和 Arjun Jain。2019。人体姿态估计的鲁棒性。在
    *CVPR Workshops*。
- en: Jhuang et al. (2013) H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black.
    2013. Towards understanding action recognition. In *ICCV*.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jhuang 等人 (2013) H. Jhuang, J. Gall, S. Zuffi, C. Schmid 和 M. J. Black。2013。朝向理解动作识别。在
    *ICCV*。
- en: JI et al. (2020) Xiaopeng JI, Qi FANG, Junting DONG, Qing SHUAI, Wen JIANG,
    and Xiaowei ZHOU. 2020. A survey on monocular 3D human pose estimation. In *Virtual
    Reality $\&amp;$ Intelligent Hardware*, Vol. 2\. 471–500.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JI 等人 (2020) Xiaopeng JI, Qi FANG, Junting DONG, Qing SHUAI, Wen JIANG 和 Xiaowei
    ZHOU。2020。单目3D人体姿态估计综述。在 *Virtual Reality $\&amp;$ Intelligent Hardware*，第2卷。471–500。
- en: 'Ji and Liu (2009) Xiaofei Ji and Honghai Liu. 2009. Advances in view-invariant
    human motion analysis: a review. In *IEEE TSMC*, Vol. 40. 13–24.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 和 Liu (2009) Xiaofei Ji 和 Honghai Liu。2009。视图不变人体运动分析的进展：综述。在 *IEEE TSMC*，第40卷。13–24。
- en: Jiang et al. (2019) Haiyong Jiang, Jianfei Cai, and Jianmin Zheng. 2019. Skeleton-Aware
    3D Human Shape Reconstruction From Point Clouds. In *ICCV*.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 (2019) Haiyong Jiang, Jianfei Cai 和 Jianmin Zheng。2019。基于骨架的3D人体形状重建从点云中。在
    *ICCV*。
- en: Jiang et al. (2020) Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei
    Zhou, and Kostas Daniilidis. 2020. Coherent Reconstruction of Multiple Humans
    From a Single Image. In *CVPR*.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 (2020) Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei Zhou
    和 Kostas Daniilidis。2020。基于单张图像的多人体一致重建。在 *CVPR*。
- en: Jin et al. (2020a) Sheng Jin, Wentao Liu, Enze Xie, Wenhai Wang, Chen Qian,
    Wanli Ouyang, and Ping Luo. 2020a. Differentiable Hierarchical Graph Grouping
    for Multi-Person Pose Estimation. In *arXiv preprint arXiv:2007.11864*.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 (2020a) Sheng Jin, Wentao Liu, Enze Xie, Wenhai Wang, Chen Qian, Wanli
    Ouyang 和 Ping Luo。2020a。用于多人体姿态估计的可微分层次图分组。在 *arXiv preprint arXiv:2007.11864*。
- en: Jin et al. (2020b) Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian,
    Wanli Ouyang, and Ping Luo. 2020b. Whole-Body Human Pose Estimation in the Wild.
    In *arXiv preprint arXiv:2007.11858*.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 (2020b) Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian,
    Wanli Ouyang 和 Ping Luo。2020b。在野外的全身人体姿态估计。在 *arXiv preprint arXiv:2007.11858*。
- en: Johnson and Everingham (2010) Sam Johnson and Mark Everingham. 2010. Clustered
    Pose and Nonlinear Appearance Models for Human Pose Estimation. In *BMVC*.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 和 Everingham (2010) Sam Johnson 和 Mark Everingham。2010。用于人体姿态估计的聚类姿态和非线性外观模型。在
    *BMVC*。
- en: Johnson and Everingham (2011) Sam Johnson and Mark Everingham. 2011. Learning
    effective human pose estimation from inaccurate annotation. In *CVPR*.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson and Everingham (2011) Sam Johnson 和 Mark Everingham. 2011. 从不准确的注释中学习有效的人体姿态估计。在*CVPR*。
- en: 'Joo et al. (2017) Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin
    Gui, Sean Banerjee, Timothy Scott Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade,
    Shohei Nobuhara, and Yaser Sheikh. 2017. Panoptic Studio: A Massively Multiview
    System for Social Interaction Capture. In *IEEE TPAMI*. 3334–3342.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joo et al. (2017) Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin
    Gui, Sean Banerjee, Timothy Scott Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade,
    Shohei Nobuhara, 和 Yaser Sheikh. 2017. Panoptic Studio：一个大规模多视角系统用于社交互动捕捉。在*IEEE
    TPAMI*。3334–3342。
- en: 'Joo et al. (2018) H. Joo, T. Simon, and Y. Sheikh. 2018. Total Capture: A 3D
    Deformation Model for Tracking Faces, Hands, and Bodies. In *CVPR*.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joo et al. (2018) H. Joo, T. Simon, 和 Y. Sheikh. 2018. Total Capture：用于追踪面部、手部和身体的3D变形模型。在*CVPR*。
- en: Kadkhodamohammadi et al. (2017) A. Kadkhodamohammadi, A. Gangi, M. de Mathelin,
    and N. Padoy. 2017. A Multi-view RGB-D Approach for Human Pose Estimation in Operating
    Rooms. In *WACV*.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kadkhodamohammadi et al. (2017) A. Kadkhodamohammadi, A. Gangi, M. de Mathelin,
    和 N. Padoy. 2017. 用于手术室中人体姿态估计的多视角RGB-D方法。在*WACV*。
- en: 'Kavan (2014) Ladislav Kavan. 2014. Part I: direct skinning methods and deformation
    primitives. In *ACM SIGGRAPH*.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kavan (2014) Ladislav Kavan. 2014. 第一部分：直接皮肤映射方法和变形原语。在*ACM SIGGRAPH*。
- en: Ke et al. (2018) Lipeng Ke, Ming-Ching Chang, Honggang Qi, and Siwei Lyu. 2018.
    Multi-scale structure-aware network for human pose estimation. In *ECCV*.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke et al. (2018) Lipeng Ke, Ming-Ching Chang, Honggang Qi, 和 Siwei Lyu. 2018.
    用于人体姿态估计的多尺度结构感知网络。在*ECCV*。
- en: 'Khan et al. (2021) Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas
    Zamir, Fahad Shahbaz Khan, and Mubarak Shah. 2021. Transformers in vision: A survey.
    In *arXiv preprint arXiv:2101.01169*.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan et al. (2021) Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir,
    Fahad Shahbaz Khan, 和 Mubarak Shah. 2021. 视觉中的变换器：综述。在*arXiv preprint arXiv:2101.01169*。
- en: 'Kocabas et al. (2020) Muhammed Kocabas, Nikos Athanasiou, and Michael J Black.
    2020. VIBE: Video inference for human body pose and shape estimation. In *CVPR*.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocabas et al. (2020) Muhammed Kocabas, Nikos Athanasiou, 和 Michael J Black.
    2020. VIBE：用于人体姿态和形状估计的视频推断。在*CVPR*。
- en: 'Kocabas et al. (2018) Muhammed Kocabas, Salih Karagoz, and Emre Akbas. 2018.
    Multiposenet: Fast multi-person pose estimation using pose residual network. In
    *ECCV*.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocabas et al. (2018) Muhammed Kocabas, Salih Karagoz, 和 Emre Akbas. 2018. Multiposenet：使用姿态残差网络的快速多人体姿态估计。在*ECCV*。
- en: Kocabas et al. (2019) Muhammed Kocabas, Salih Karagoz, and Emre Akbas. 2019.
    Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry. In *CVPR*.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocabas et al. (2019) Muhammed Kocabas, Salih Karagoz, 和 Emre Akbas. 2019. 使用多视角几何进行的3D人体姿态自监督学习。在*CVPR*。
- en: Kolotouros et al. (2019) N. Kolotouros, G. Pavlakos, M. Black, and K. Daniilidis.
    2019. Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the
    Loop. In *ICCV*.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolotouros et al. (2019) N. Kolotouros, G. Pavlakos, M. Black, 和 K. Daniilidis.
    2019. 通过模型拟合循环学习重建3D人体姿态和形状。在*ICCV*。
- en: Kolotouros et al. (2019) Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis.
    2019. Convolutional Mesh Regression for Single-Image Human Shape Reconstruction.
    In *CVPR*.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolotouros et al. (2019) Nikos Kolotouros, Georgios Pavlakos, 和 Kostas Daniilidis.
    2019. 用于单张图像人体形状重建的卷积网格回归。在*CVPR*。
- en: Kolotouros et al. (2021) Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman,
    and Kostas Daniilidis. 2021. Probabilistic Modeling for Human Mesh Recovery. In
    *ICCV*.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolotouros et al. (2021) Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman,
    和 Kostas Daniilidis. 2021. 人体网格恢复的概率建模。在*ICCV*。
- en: 'Kreiss et al. (2019) Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. 2019.
    Pifpaf: Composite fields for human pose estimation. In *CVPR*.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kreiss et al. (2019) Sven Kreiss, Lorenzo Bertoni, 和 Alexandre Alahi. 2019.
    Pifpaf：用于人体姿态估计的复合场。在*CVPR*。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. In *NeurIPS*.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E Hinton.
    2012. 使用深度卷积神经网络进行Imagenet分类。在*NeurIPS*。
- en: Kundu et al. (2020a) Jogendra Nath Kundu, Ambareesh Revanur, Govind Vitthal
    Waghmare, Rahul Mysore Venkatesh, and R. Venkatesh Babu. 2020a. Unsupervised Cross-Modal
    Alignment for Multi-Person 3D Pose Estimation. In *ECCV*.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kundu et al. (2020a) Jogendra Nath Kundu, Ambareesh Revanur, Govind Vitthal
    Waghmare, Rahul Mysore Venkatesh, 和 R. Venkatesh Babu. 2020a. 用于多人3D姿态估计的无监督跨模态对齐。在*ECCV*。
- en: Kundu et al. (2020b) Jogendra Nath Kundu, Siddharth Seth, Varun Jampani, Mugalodi
    Rakesh, R. Venkatesh Babu, and Anirban Chakraborty. 2020b. Self-Supervised 3D
    Human Pose Estimation via Part Guided Novel Image Synthesis. In *CVPR*.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kundu et al. (2020b) Jogendra Nath Kundu, Siddharth Seth, Varun Jampani, Mugalodi
    Rakesh, R. Venkatesh Babu, 和 Anirban Chakraborty. 2020b. 通过部件引导的新颖图像合成进行自监督3D人体姿态估计.
    发表在*CVPR*。
- en: Kundu et al. (2020c) Jogendra Nath Kundu, Siddharth Seth, MV Rahul, Mugalodi
    Rakesh, Venkatesh Babu Radhakrishnan, and Anirban Chakraborty. 2020c. Kinematic-Structure-Preserved
    Representation for Unsupervised 3D Human Pose Estimation. In *AAAI*.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kundu et al. (2020c) Jogendra Nath Kundu, Siddharth Seth, MV Rahul, Mugalodi
    Rakesh, Venkatesh Babu Radhakrishnan, 和 Anirban Chakraborty. 2020c. 保持运动学结构不变的无监督3D人体姿态估计表示.
    发表在*AAAI*。
- en: 'Lassner et al. (2017) Christoph Lassner, Javier Romero, Martin Kiefel, Federica
    Bogo, Michael J. Black, and Peter V. Gehler. 2017. Unite the People: Closing the
    Loop Between 3D and 2D Human Representations. In *CVPR*.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lassner et al. (2017) Christoph Lassner, Javier Romero, Martin Kiefel, Federica
    Bogo, Michael J. Black, 和 Peter V. Gehler. 2017. 联合人群：弥合3D和2D人体表示之间的差距. 发表在*CVPR*。
- en: 'Lee et al. (2018) Kyoungoh Lee, Inwoong Lee, and Sanghoon Lee. 2018. Propagating
    LSTM: 3D Pose Estimation based on Joint Interdependency. In *ECCV*.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2018) Kyoungoh Lee, Inwoong Lee, 和 Sanghoon Lee. 2018. 传播LSTM：基于关节互依性的3D姿态估计.
    发表在*ECCV*。
- en: Li and Lee (2019) Chen Li and Gim Hee Lee. 2019. Generating Multiple Hypotheses
    for 3D Human Pose Estimation With Mixture Density Network. In *CVPR*.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Lee (2019) Chen Li 和 Gim Hee Lee. 2019. 利用混合密度网络生成多种假设以进行3D人体姿态估计. 发表在*CVPR*。
- en: Li et al. (2021a) Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao
    Liu, and Cewu Lu. 2021a. Human pose regression with residual log-likelihood estimation.
    In *ICCV*.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021a) Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao
    Liu, 和 Cewu Lu. 2021a. 基于残差对数似然估计的人体姿态回归. 发表在*ICCV*。
- en: 'Li et al. (2020b) Jiefeng Li, Can Wang, Wentao Liu, Chen Qian, and Cewu Lu.
    2020b. HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person
    3D Pose Estimation. In *ECCV*.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020b) Jiefeng Li, Can Wang, Wentao Liu, Chen Qian, 和 Cewu Lu. 2020b.
    HMOR：用于单目多人3D姿态估计的分层多人人次关系. 发表在*ECCV*。
- en: 'Li et al. (2019b) Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang,
    and Cewu Lu. 2019b. Crowdpose: Efficient crowded scenes pose estimation and a
    new benchmark. In *ICCV*.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019b) Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, 和
    Cewu Lu. 2019b. Crowdpose：高效的拥挤场景姿态估计及新基准. 发表在*ICCV*。
- en: 'Li et al. (2021d) Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,
    and Cewu Lu. 2021d. HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution
    for 3D Human Pose and Shape Estimation. In *CVPR*.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021d) Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,
    和 Cewu Lu. 2021d. HybrIK：一种用于3D人体姿态和形状估计的混合解析-神经逆运动学解决方案. 发表在*CVPR*。
- en: Li et al. (2021c) Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, and
    Zhuowen Tu. 2021c. Pose Recognition with Cascade Transformers. In *CVPR*.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021c) Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, 和 Zhuowen
    Tu. 2021c. 使用级联变换器的姿态识别. 发表在*CVPR*。
- en: Li and Chan (2014) Sijin Li and Antoni B. Chan. 2014. 3D Human Pose Estimation
    from Monocular Images with Deep Convolutional Neural Network. In *ACCV*.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Chan (2014) Sijin Li 和 Antoni B. Chan. 2014. 基于深度卷积神经网络的单目图像3D人体姿态估计.
    发表在*ACCV*。
- en: Li et al. (2014) Sijin Li, Zhi-Qiang Liu, and Antoni B Chan. 2014. Heterogeneous
    multi-task learning for human pose estimation with deep convolutional neural network.
    In *CVPR Workshops*.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2014) Sijin Li, Zhi-Qiang Liu, 和 Antoni B Chan. 2014. 基于深度卷积神经网络的异质多任务学习用于人体姿态估计.
    发表在*CVPR Workshops*。
- en: Li et al. (2015) Sijin Li, Weichen Zhang, and Antoni B. Chan. 2015. Maximum-Margin
    Structured Learning With Deep Networks for 3D Human Pose Estimation. In *ICCV*.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2015) Sijin Li, Weichen Zhang, 和 Antoni B. Chan. 2015. 基于深度网络的最大间隔结构学习用于3D人体姿态估计.
    发表在*ICCV*。
- en: 'Li et al. (2022a) Wenhao Li, Hong Liu, Hao Tang, Pichao Wang, and Luc Van Gool.
    2022a. MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. In
    *CVPR*.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022a) Wenhao Li, Hong Liu, Hao Tang, Pichao Wang, 和 Luc Van Gool.
    2022a. MHFormer：用于3D人体姿态估计的多假设变换器. 发表在*CVPR*。
- en: Li et al. (2019a) Wenbo Li, Zhicheng Wang, Binyi Yin, Qixiang Peng, Yuming Du,
    Tianzi Xiao, Gang Yu, Hongtao Lu, Yichen Wei, and Jian Sun. 2019a. Rethinking
    on multi-stage networks for human pose estimation. In *arXiv preprint arXiv:1901.00148*.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019a) Wenbo Li, Zhicheng Wang, Binyi Yin, Qixiang Peng, Yuming Du,
    Tianzi Xiao, Gang Yu, Hongtao Lu, Yichen Wei, 和 Jian Sun. 2019a. 重新思考用于人体姿态估计的多阶段网络.
    发表在*arXiv preprint arXiv:1901.00148*。
- en: Li et al. (2021b) Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gundavarapu,
    and Xiaolong Wang. 2021b. Test-time personalization with a transformer for human
    pose estimation. In *NeurIPS*.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021b) Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gundavarapu,
    和 Xiaolong Wang. 2021b. 使用变换器进行人体姿态估计的测试时个性化。发表于*NeurIPS*。
- en: Li et al. (2019) Yining Li, Chen Huang, and Chen Change Loy. 2019. Dense Intrinsic
    Appearance Flow for Human Pose Transfer. In *CVPR*.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019) Yining Li, Chen Huang, 和 Chen Change Loy. 2019. 用于人体姿态转移的稠密内在外观流。发表于*CVPR*。
- en: 'Li et al. (2022b) Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao
    Wang, Zhicheng Wang, Wankou Yang, and Shu-Tao Xia. 2022b. SimCC: A Simple Coordinate
    Classification Perspective for Human Pose Estimation. In *ECCV*.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022b) Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao Wang,
    Zhicheng Wang, Wankou Yang, 和 Shu-Tao Xia. 2022b. SimCC：一种简单的坐标分类视角用于人体姿态估计。发表于*ECCV*。
- en: 'Li et al. (2021e) Yanjie Li, Shoukui Zhang, Zhicheng Wang, Sen Yang, Wankou
    Yang, Shu-Tao Xia, and Erjin Zhou. 2021e. TokenPose: Learning Keypoint Tokens
    for Human Pose Estimation. In *ICCV*.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021e) Yanjie Li, Shoukui Zhang, Zhicheng Wang, Sen Yang, Wankou
    Yang, Shu-Tao Xia, 和 Erjin Zhou. 2021e. TokenPose：为人体姿态估计学习关键点令牌。发表于*ICCV*。
- en: Li et al. (2020a) Zhongguo Li, Anders Heyden, and Magnus Oskarsson. 2020a. A
    novel joint points and silhouette-based method to estimate 3D human pose and shape.
    In *arXiv preprint arXiv:2012.06109*.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020a) Zhongguo Li, Anders Heyden, 和 Magnus Oskarsson. 2020a. 一种新颖的基于关节点和轮廓的方法来估计3D人体姿态和形状。发表于*arXiv
    preprint arXiv:2012.06109*。
- en: Li et al. (2019) Z. Li, X. Wang, F. Wang, and P. Jiang. 2019. On Boosting Single-Frame
    3D Human Pose Estimation via Monocular Videos. In *ICCV*.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019) Z. Li, X. Wang, F. Wang, 和 P. Jiang. 2019. 通过单目视频提升单帧3D人体姿态估计。发表于*ICCV*。
- en: Liang and Lin (2019) Junbang Liang and Ming C. Lin. 2019. Shape-Aware Human
    Pose and Shape Reconstruction Using Multi-View Images. In *ICCV*.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 和 Lin (2019) Junbang Liang 和 Ming C. Lin. 2019. 使用多视角图像的形状感知人体姿态和形状重建。发表于*ICCV*。
- en: Lifshitz et al. (2016) Ita Lifshitz, Ethan Fetaya, and Shimon Ullman. 2016.
    Human pose estimation using deep consensus voting. In *ECCV*.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lifshitz et al. (2016) Ita Lifshitz, Ethan Fetaya, 和 Shimon Ullman. 2016. 使用深度共识投票进行人体姿态估计。发表于*ECCV*。
- en: Lin et al. (2021a) Kevin Lin, Lijuan Wang, and Zicheng Liu. 2021a. End-to-end
    human pose and mesh reconstruction with transformers. In *CVPR*.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2021a) Kevin Lin, Lijuan Wang, 和 Zicheng Liu. 2021a. 使用变换器的端到端人体姿态和网格重建。发表于*CVPR*。
- en: Lin et al. (2021b) Kevin Lin, Lijuan Wang, and Zicheng Liu. 2021b. Mesh Graphormer.
    In *ICCV*.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2021b) Kevin Lin, Lijuan Wang, 和 Zicheng Liu. 2021b. Mesh Graphormer。发表于*ICCV*。
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft
    coco: Common objects in context. In *ECCV*.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
    Perona, Deva Ramanan, Piotr Dollár, 和 C Lawrence Zitnick. 2014. Microsoft coco：上下文中的常见物体。发表于*ECCV*。
- en: 'Lin et al. (2020) Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao
    Wang, Ning Xu, Hongkai Xiong, Guo-Jun Qi, and Nicu Sebe. 2020. Human in events:
    A large-scale benchmark for human-centric video analysis in complex events. In
    *arXiv preprint arXiv:2005.04490*.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2020) Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao
    Wang, Ning Xu, Hongkai Xiong, Guo-Jun Qi, 和 Nicu Sebe. 2020. 事件中的人：复杂事件中以人为中心的视频分析大规模基准。发表于*arXiv
    preprint arXiv:2005.04490*。
- en: 'Liu et al. (2021b) Huajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang. 2021b.
    Polarized self-attention: towards high-quality pixel-wise regression. In *arXiv
    preprint arXiv:2107.00782*.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021b) Huajun Liu, Fuqiang Liu, Xinyi Fan, 和 Dong Huang. 2021b.
    极化自注意力：朝向高质量像素级回归。发表于*arXiv preprint arXiv:2107.00782*。
- en: Liu et al. (2019) Jian Liu, Naveed Akhtar, and Ajmal Mian. 2019. Adversarial
    Attack on Skeleton-based Human Action Recognition. In *arXiv preprint arXiv:1909.06500*.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) Jian Liu, Naveed Akhtar, 和 Ajmal Mian. 2019. 对基于骨架的人体动作识别的对抗攻击。发表于*arXiv
    preprint arXiv:1909.06500*。
- en: 'Liu et al. (2020b) Jingyuan Liu, Hongbo Fu, and Chiew-Lan Tai. 2020b. PoseTween:
    Pose-driven Tween Animation. In *ACM UIST*.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020b) Jingyuan Liu, Hongbo Fu, 和 Chiew-Lan Tai. 2020b. PoseTween：基于姿态的Tween动画。发表于*ACM
    UIST*。
- en: Liu et al. (2020a) Kenkun Liu, Rongqi Ding, Zhiming Zou, Le Wang, and Wei Tang.
    2020a. A Comprehensive Study of Weight Sharing in Graph Networks for 3D Human
    Pose Estimation. In *ECCV*.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020a) Kenkun Liu, Rongqi Ding, Zhiming Zou, Le Wang, 和 Wei Tang.
    2020a. 图网络中权重共享的综合研究，用于3D人体姿态估计。发表于*ECCV*。
- en: 'Liu et al. (2020c) Ruixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung,
    and Vijayan Asari. 2020c. Attention Mechanism Exploits Temporal Contexts: Real-Time
    3D Human Pose Reconstruction. In *CVPR*.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020c) Ruixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung,
    和 Vijayan Asari. 2020c. 注意力机制利用时间上下文：实时3D人体姿态重建。发表于*CVPR*。
- en: Liu et al. (2021a) Zhenguang Liu, Haoming Chen, Runyang Feng, Shuang Wu, Shouling
    Ji, Bailin Yang, and Xun Wang. 2021a. Deep dual consecutive network for human
    pose estimation. In *CVPR*.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Zhenguang Liu, Runyang Feng, Haoming Chen, Shuang Wu, Yixing
    Gao, Yunjun Gao, and Xiang Wang. 2022. Temporal Feature Alignment and Mutual Information
    Maximization for Video-Based Human Pose Estimation. In *CVPR*.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2015) Zhao Liu, Jianke Zhu, Jiajun Bu, and Chun Chen. 2015. A survey
    of human pose estimation: the body parts parsing based methods. In *JVCIR*, Vol. 32\.
    10–19.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2015) Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015.
    Fully Convolutional Networks for Semantic Segmentation. In *CVPR*.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loper et al. (2015) Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll,
    and Michael J. Black. 2015. SMPL: A Skinned Multi-Person Linear Model. In *ACM
    TOG*, Vol. 34. 1–16.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2020) Mandy Lu, Kathleen Poston, Adolf Pfefferbaum, Edith V Sullivan,
    Li Fei-Fei, Kilian M Pohl, Juan Carlos Niebles, and Ehsan Adeli. 2020. Vision-based
    Estimation of MDS-UPDRS Gait Scores for Assessing Parkinson’s Disease Motor Severity.
    In *arXiv preprint arXiv:2007.08920*.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2018) Yue Luo, Jimmy Ren, Zhouxia Wang, Wenxiu Sun, Jinshan Pan,
    Jianbo Liu, Jiahao Pang, and Liang Lin. 2018. Lstm pose machines. In *CVPR*.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2021) Zhengxiong Luo, Zhicheng Wang, Yan Huang, Liang Wang, Tieniu
    Tan, and Erjin Zhou. 2021. Rethinking the heatmap regression for bottom-up human
    pose estimation. In *CVPR*.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luvizon et al. (2018) Diogo C Luvizon, David Picard, and Hedi Tabia. 2018. 2d/3d
    pose estimation and action recognition using multitask deep learning. In *CVPR*.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luvizon et al. (2019) Diogo C Luvizon, Hedi Tabia, and David Picard. 2019. Human
    pose regression by combining indirect part detection and contextual information.
    In *Computers & Graphics*, Vol. 85\. 15–22.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2021) Haoyu Ma, Liangjian Chen, Deying Kong, Zhe Wang, Xingwei Liu,
    Hao Tang, Xiangyi Yan, Yusheng Xie, Shih-Yao Lin, and Xiaohui Xie. 2021. Transfusion:
    Cross-view fusion with transformer for 3d human pose estimation. In *BMVC*.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2022) Haoyu Ma, Zhe Wang, Yifei Chen, Deying Kong, Liangjian Chen,
    Xingwei Liu, Xiangyi Yan, Hao Tang, and Xiaohui Xie. 2022. PPT: token-Pruned Pose
    Transformer for monocular and multi-view human pose estimation. In *ECCV*.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madhu et al. (2020) Prathmesh Madhu, Angel Villar-Corrales, Ronak Kosti, Torsten
    Bendschus, Corinna Reinhardt, Peter Bell, Andreas Maier, and Vincent Christlein.
    2020. Enhancing Human Pose Estimation in Ancient Vase Paintings via Perceptually-grounded
    Style Transfer Learning. In *arXiv preprint arXiv:2012.05616*.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahmood et al. (2019) Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard
    Pons-Moll, and Michael J. Black. 2019. AMASS: Archive of Motion Capture as Surface
    Shapes. In *ICCV*.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. (2021) Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang,
    and Zhibin Wang. 2021. Tfpose: Direct human pose estimation with transformers.
    In *arXiv preprint arXiv:2103.15320*.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao et al. (2021) Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang,
    和 Zhibin Wang. 2021. Tfpose：使用变换器的直接人体姿态估计。在*arXiv 预印本 arXiv:2103.15320*中。
- en: 'Mao et al. (2022) Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang,
    Zhibin Wang, and Anton van den Hengel. 2022. Poseur: Direct Human Pose Regression
    with Transformers. In *ECCV*.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao et al. (2022) Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang,
    Zhibin Wang, 和 Anton van den Hengel. 2022. Poseur：使用变换器的直接人体姿态回归。在*ECCV*中。
- en: Markovitz et al. (2020) Amir Markovitz, Gilad Sharir, Itamar Friedman, Lihi
    Zelnik-Manor, and Shai Avidan. 2020. Graph Embedded Pose Clustering for Anomaly
    Detection. In *CVPR*.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Markovitz et al. (2020) Amir Markovitz, Gilad Sharir, Itamar Friedman, Lihi
    Zelnik-Manor, 和 Shai Avidan. 2020. 图嵌入姿态聚类用于异常检测。在*CVPR*中。
- en: Martinez et al. (2017) Julieta Martinez, Rayat Hossain, Javier Romero, and James J.
    Little. 2017. A simple yet effective baseline for 3d human pose estimation. In
    *ICCV*.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martinez et al. (2017) Julieta Martinez, Rayat Hossain, Javier Romero, 和 James
    J. Little. 2017. 一种简单而有效的 3D 人体姿态估计基线。在*ICCV*中。
- en: Mehta et al. (2017) D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W.
    Xu, and C. Theobalt. 2017. Monocular 3D Human Pose Estimation in the Wild Using
    Improved CNN Supervision. In *3DV*.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2017) D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W.
    Xu, 和 C. Theobalt. 2017. 使用改进的 CNN 监督进行单目 3D 人体姿态估计。在*3DV*中。
- en: 'Mehta et al. (2020) Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller,
    Weipeng Xu, Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard
    Pons-Moll, and Christian Theobalt. 2020. XNect: Real-time Multi-Person 3D Motion
    Capture with a Single RGB Camera. In *ACM TOG*, Vol. 39\. 82–1.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2020) Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller,
    Weipeng Xu, Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard
    Pons-Moll, 和 Christian Theobalt. 2020. XNect：使用单个 RGB 相机的实时多人体 3D 运动捕捉。在*ACM TOG*中，第39卷，82–1页。
- en: Mehta et al. (2018) Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller,
    Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll, and Christian Theobalt. 2018. Single-Shot
    Multi-Person 3D Pose Estimation From Monocular RGB. In *3DV*.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2018) Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller,
    Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll, 和 Christian Theobalt. 2018. 从单目
    RGB 图像中进行单次多人体 3D 姿态估计。在*3DV*中。
- en: 'Mehta et al. (2017) Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,
    Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, et al. 2017. Vnect: Real-time
    3d human pose estimation with a single rgb camera. In *ACM TOG*, Vol. 36\. 1–14.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta et al. (2017) Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,
    Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, 等. 2017. Vnect：使用单个 RGB 相机的实时
    3D 人体姿态估计。在*ACM TOG*中，第36卷，1–14页。
- en: Micilotta et al. (2006) Antonio S Micilotta, Eng-Jon Ong, and Richard Bowden.
    2006. Real-time upper body detection and 3D pose estimation in monoscopic images.
    In *ECCV*.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micilotta et al. (2006) Antonio S Micilotta, Eng-Jon Ong, 和 Richard Bowden.
    2006. 在单目图像中进行实时上半身检测和 3D 姿态估计。在*ECCV*中。
- en: Mitra et al. (2020) Rahul Mitra, Nitesh B. Gundavarapu, Abhishek Sharma, and
    Arjun Jain. 2020. Multiview-Consistent Semi-Supervised Learning for 3D Human Pose
    Estimation. In *CVPR*.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitra et al. (2020) Rahul Mitra, Nitesh B. Gundavarapu, Abhishek Sharma, 和 Arjun
    Jain. 2020. 多视角一致的半监督学习用于 3D 人体姿态估计。在*CVPR*中。
- en: Moeslund and Granum (2001) Thomas B Moeslund and Erik Granum. 2001. A survey
    of computer vision-based human motion capture. In *CVIU*, Vol. 81. 231–268.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moeslund and Granum (2001) Thomas B Moeslund 和 Erik Granum. 2001. 基于计算机视觉的人体运动捕捉调查。在*CVIU*中，第81卷，231–268页。
- en: Moeslund et al. (2006) Thomas B Moeslund, Adrian Hilton, and Volker Krüger.
    2006. A survey of advances in vision-based human motion capture and analysis.
    In *CVIU*, Vol. 104\. 90–126.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moeslund et al. (2006) Thomas B Moeslund, Adrian Hilton, 和 Volker Krüger. 2006.
    基于视觉的人体运动捕捉和分析的进展调查。在*CVIU*中，第104卷，90–126页。
- en: Moon et al. (2019a) Gyeongsik Moon, Juyong Chang, and Kyoung Mu Lee. 2019a.
    Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from
    a Single RGB Image. In *ICCV*.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon et al. (2019a) Gyeongsik Moon, Juyong Chang, 和 Kyoung Mu Lee. 2019a. 一种考虑相机距离的自上而下方法，用于从单个
    RGB 图像中进行 3D 多人姿态估计。在*ICCV*中。
- en: 'Moon et al. (2019b) Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. 2019b.
    Posefix: Model-agnostic general human pose refinement network. In *CVPR*.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon et al. (2019b) Gyeongsik Moon, Ju Yong Chang, 和 Kyoung Mu Lee. 2019b. Posefix：模型无关的通用人体姿态优化网络。在*CVPR*中。
- en: 'Moon and Lee (2020) Gyeongsik Moon and Kyoung Mu Lee. 2020. I2L-MeshNet: Image-to-Lixel
    Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single
    RGB Image. In *ECCV*.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon and Lee (2020) Gyeongsik Moon 和 Kyoung Mu Lee. 2020. I2L-MeshNet：用于从单个
    RGB 图像中准确估计 3D 人体姿态和网格的图像到 Lixel 预测网络。在*ECCV*中。
- en: Moreno-Noguer (2017) Francesc Moreno-Noguer. 2017. 3d human pose estimation
    from a single image via distance matrix regression. In *CVPR*.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moreno-Noguer (2017) Francesc Moreno-Noguer。2017年。通过距离矩阵回归从单张图像中进行3D人体姿态估计。发表于
    *CVPR*。
- en: 'Munea et al. (2020) Tewodros Legesse Munea, Yalew Zelalem Jembre, Halefom Tekle
    Weldegebriel, Longbiao Chen, Chenxi Huang, and Chenhui Yang. 2020. The Progress
    of Human Pose Estimation: A Survey and Taxonomy of Models Applied in 2D Human
    Pose Estimation. In *IEEE Access*, Vol. 8. 133330–133348.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munea 等 (2020) Tewodros Legesse Munea, Yalew Zelalem Jembre, Halefom Tekle Weldegebriel,
    Longbiao Chen, Chenxi Huang 和 Chenhui Yang。2020年。人体姿态估计的进展：2D人体姿态估计模型的调查和分类。发表于
    *IEEE Access*, 第8卷，133330–133348。
- en: 'Newell et al. (2017) Alejandro Newell, Zhiao Huang, and Jia Deng. 2017. Associative
    embedding: End-to-end learning for joint detection and grouping. In *NeurIPS*.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell 等 (2017) Alejandro Newell, Zhiao Huang 和 Jia Deng。2017年。关联嵌入：用于联合检测和分组的端到端学习。发表于
    *NeurIPS*。
- en: Newell et al. (2016) Alejandro Newell, Kaiyu Yang, and Jia Deng. 2016. Stacked
    hourglass networks for human pose estimation. In *ECCV*.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell 等 (2016) Alejandro Newell, Kaiyu Yang 和 Jia Deng。2016年。用于人体姿态估计的堆叠小时glass网络。发表于
    *ECCV*。
- en: Nibali et al. (2018) Aiden Nibali, Zhen He, Stuart Morgan, and Luke Prendergast.
    2018. Numerical coordinate regression with convolutional neural networks. In *arXiv
    preprint arXiv:1801.07372*.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nibali 等 (2018) Aiden Nibali, Zhen He, Stuart Morgan 和 Luke Prendergast。2018年。使用卷积神经网络的数值坐标回归。发表于
    *arXiv preprint arXiv:1801.07372*。
- en: Nie et al. (2017) B. X. Nie, P. Wei, and S. Zhu. 2017. Monocular 3D Human Pose
    Estimation by Predicting Depth on Joints. In *ICCV*.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等 (2017) B. X. Nie, P. Wei 和 S. Zhu。2017年。通过预测关节深度的单目3D人体姿态估计。发表于 *ICCV*。
- en: Nie et al. (2020) Qiang Nie, Ziwei Liu, and Yunhui Liu. 2020. Unsupervised Human
    3D Pose Representation with Viewpoint and Pose Disentanglement. In *ECCV*.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等 (2020) Qiang Nie, Ziwei Liu 和 Yunhui Liu。2020年。无监督的人体3D姿态表示与视角和姿态的解耦。发表于
    *ECCV*。
- en: Nie et al. (2019) Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, and Shuicheng Yan.
    2019. Single-Stage Multi-Person Pose Machines. In *ICCV*.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等 (2019) Xuecheng Nie, Jiashi Feng, Jianfeng Zhang 和 Shuicheng Yan。2019年。单阶段多人体姿态机器。发表于
    *ICCV*。
- en: 'Omran et al. (2018) Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter V.
    Gehler, and Bernt Schiele. 2018. Neural Body Fitting: Unifying Deep Learning and
    Model-Based Human Pose and Shape Estimation. In *3DV*.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Omran 等 (2018) Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter V.
    Gehler 和 Bernt Schiele。2018年。神经体拟合：统一深度学习与基于模型的人体姿态和形状估计。发表于 *3DV*。
- en: 'Osman et al. (2020) Ahmed A A Osman, Timo Bolkart, and Michael J. Black. 2020.
    STAR: A Spare Trained Articulated Human Body Regressor. In *ECCV*.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Osman 等 (2020) Ahmed A A Osman, Timo Bolkart 和 Michael J. Black。2020年。STAR:
    一种备用训练的关节化人体回归模型。发表于 *ECCV*。'
- en: 'Panteleris and Argyros (2021) Paschalis Panteleris and Antonis Argyros. 2021.
    PE-former: Pose Estimation Transformer. In *arXiv preprint arXiv:2112.04981*.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Panteleris 和 Argyros (2021) Paschalis Panteleris 和 Antonis Argyros。2021年。PE-former:
    姿态估计变压器。发表于 *arXiv preprint arXiv:2112.04981*。'
- en: 'Papandreou et al. (2018) George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros
    Gidaris, Jonathan Tompson, and Kevin Murphy. 2018. Personlab: Person pose estimation
    and instance segmentation with a bottom-up, part-based, geometric embedding model.
    In *ECCV*.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papandreou 等 (2018) George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros
    Gidaris, Jonathan Tompson 和 Kevin Murphy。2018年。Personlab: 基于自下而上的、基于部分的几何嵌入模型进行的人体姿态估计和实例分割。发表于
    *ECCV*。'
- en: Papandreou et al. (2017) George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander
    Toshev, Jonathan Tompson, Chris Bregler, and Kevin Murphy. 2017. Towards Accurate
    Multi-Person Pose Estimation in the Wild. In *CVPR*.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papandreou 等 (2017) George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev,
    Jonathan Tompson, Chris Bregler 和 Kevin Murphy。2017年。朝向准确的野外多人体姿态估计。发表于 *CVPR*。
- en: 'Patel et al. (2020) Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll.
    2020. TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape
    and Garment Style. In *CVPR*.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patel 等 (2020) Chaitanya Patel, Zhouyingcheng Liao 和 Gerard Pons-Moll。2020年。TailorNet:
    作为人体姿态、形状和服装风格函数的3D服装预测。发表于 *CVPR*。'
- en: 'Pavlakos et al. (2019) Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
    Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019.
    Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In *CVPR*.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pavlakos 等 (2019) Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo
    Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas 和 Michael J. Black。2019年。Expressive
    Body Capture: 从单张图像中获取3D手部、面部和身体。发表于 *CVPR*。'
- en: Pavlakos et al. (2018a) Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.
    2018a. Ordinal Depth Supervision for 3D Human Pose Estimation. In *CVPR*.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlakos 等 (2018a) Georgios Pavlakos, Xiaowei Zhou 和 Kostas Daniilidis。2018a年。用于3D人体姿态估计的序数深度监督。发表于
    *CVPR*。
- en: Pavlakos et al. (2017a) Georgios Pavlakos, Xiaowei Zhou, Konstantinos G Derpanis,
    and Kostas Daniilidis. 2017a. Coarse-to-Fine Volumetric Prediction for Single-Image
    3D Human Pose. In *CVPR*.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlakos et al. (2017a) Georgios Pavlakos, Xiaowei Zhou, Konstantinos G Derpanis,
    和 Kostas Daniilidis. 2017a. 从粗到细的体积预测用于单幅图像的3D人体姿态。发表于*CVPR*。
- en: Pavlakos et al. (2017b) Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis,
    and Kostas Daniilidis. 2017b. Harvesting Multiple Views for Marker-Less 3D Human
    Pose Annotations. In *CVPR*.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlakos et al. (2017b) Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis,
    和 Kostas Daniilidis. 2017b. 收集多视角用于无标记3D人体姿态标注。发表于*CVPR*。
- en: Pavlakos et al. (2018b) Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas
    Daniilidis. 2018b. Learning to Estimate 3D Human Pose and Shape from a Single
    Color Image. In *CVPR*.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlakos et al. (2018b) Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, 和 Kostas
    Daniilidis. 2018b. 从单幅彩色图像中学习估计3D人体姿态和形状。发表于*CVPR*。
- en: Pavllo et al. (2019) Dario Pavllo, Christoph Feichtenhofer, David Grangier,
    and Michael Auli. 2019. 3D human pose estimation in video with temporal convolutions
    and semi-supervised training. In *CVPR*.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavllo et al. (2019) Dario Pavllo, Christoph Feichtenhofer, David Grangier,
    和 Michael Auli. 2019. 使用时间卷积和半监督训练的视频中的3D人体姿态估计。发表于*CVPR*。
- en: 'Peng et al. (2018) Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S Feris, and Dimitris
    Metaxas. 2018. Jointly optimize data augmentation and network training: Adversarial
    data augmentation in human pose estimation. In *CVPR*.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2018) Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S Feris, 和 Dimitris
    Metaxas. 2018. 联合优化数据增强和网络训练：人体姿态估计中的对抗性数据增强。发表于*CVPR*。
- en: Pfister et al. (2015) Tomas Pfister, James Charles, and Andrew Zisserman. 2015.
    Flowing convnets for human pose estimation in videos. In *ICCV*.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfister et al. (2015) Tomas Pfister, James Charles, 和 Andrew Zisserman. 2015.
    用于视频中人体姿态估计的流动卷积网络。发表于*ICCV*。
- en: Pfister et al. (2014) Tomas Pfister, Karen Simonyan, James Charles, and Andrew
    Zisserman. 2014. Deep convolutional neural networks for efficient pose estimation
    in gesture videos. In *ACCV*.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfister et al. (2014) Tomas Pfister, Karen Simonyan, James Charles, 和 Andrew
    Zisserman. 2014. 用于手势视频中高效姿态估计的深度卷积神经网络。发表于*ACCV*。
- en: 'Pirinen et al. (2019) Aleksis Pirinen, Erik Gärtner, and Cristian Sminchisescu.
    2019. Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose
    Reconstruction. In *NeurIPS*.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pirinen et al. (2019) Aleksis Pirinen, Erik Gärtner, 和 Cristian Sminchisescu.
    2019. 从圆顶到无人机：用于3D人体姿态重建的自监督主动三角测量。发表于*NeurIPS*。
- en: 'Pishchulin et al. (2016) Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang,
    Bjoern Andres, Mykhaylo Andriluka, Peter V Gehler, and Bernt Schiele. 2016. Deepcut:
    Joint subset partition and labeling for multi person pose estimation. In *CVPR*.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pishchulin et al. (2016) Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern
    Andres, Mykhaylo Andriluka, Peter V Gehler, 和 Bernt Schiele. 2016. Deepcut：多人体姿态估计的联合子集划分和标注。发表于*CVPR*。
- en: 'Pons-Moll et al. (2015) Gerard Pons-Moll, Javier Romero, Naureen Mahmood, and
    Michael J. Black. 2015. Dyna: A Model of Dynamic Human Shape in Motion. In *ACM
    TOG*, Vol. 34. 1–14.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pons-Moll et al. (2015) Gerard Pons-Moll, Javier Romero, Naureen Mahmood, 和
    Michael J. Black. 2015. Dyna：动态人体形状模型。发表于*ACM TOG*，第34卷，1–14页。
- en: 'Poppe (2007) Ronald Poppe. 2007. Vision-based human motion analysis: An overview.
    In *CVIU*, Vol. 108. 4–18.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poppe (2007) Ronald Poppe. 2007. 基于视觉的人体运动分析：概述。发表于*CVIU*，第108卷，4–18页。
- en: 'Qammaz and Argyros (2019) Ammar Qammaz and Antonis A Argyros. 2019. MocapNET:
    Ensemble of SNN Encoders for 3D Human Pose Estimation in RGB Images. In *BMVC*.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qammaz and Argyros (2019) Ammar Qammaz 和 Antonis A Argyros. 2019. MocapNET：用于RGB图像中的3D人体姿态估计的SNN编码器集成。发表于*BMVC*。
- en: 'Qi et al. (2017a) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
    2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation.
    In *CVPR*.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi et al. (2017a) Charles R Qi, Hao Su, Kaichun Mo, 和 Leonidas J Guibas. 2017a.
    Pointnet：用于3D分类和分割的点集深度学习。发表于*CVPR*。
- en: 'Qi et al. (2017b) Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.
    2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric
    space. In *NeurIPS*.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi et al. (2017b) Charles Ruizhongtai Qi, Li Yi, Hao Su, 和 Leonidas J Guibas.
    2017b. Pointnet++：在度量空间中对点集进行深度分层特征学习。发表于*NeurIPS*。
- en: Qiu et al. (2019) Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, and Wenjun
    Zeng. 2019. Cross View Fusion for 3D Human Pose Estimation. In *ICCV*.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu et al. (2019) Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, 和 Wenjun
    Zeng. 2019. 用于3D人体姿态估计的视角融合。发表于*ICCV*。
- en: 'Qiu et al. (2020) Lingteng Qiu, Xuanye Zhang, Yanran Li, Guanbin Li, Xiaojun
    Wu, Zixiang Xiong, Xiaoguang Han, and Shuguang Cui. 2020. Peeking into occluded
    joints: A novel framework for crowd pose estimation. In *arXiv preprint arXiv:2003.10506*.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu et al. (2020) Lingteng Qiu, Xuanye Zhang, Yanran Li, Guanbin Li, Xiaojun
    Wu, Zixiang Xiong, Xiaoguang Han, 和 Shuguang Cui. 2020. 观察遮挡关节：用于人群姿态估计的新框架。发表于*arXiv
    preprint arXiv:2003.10506*。
- en: 'Ramakrishna et al. (2014) Varun Ramakrishna, Daniel Munoz, Martial Hebert,
    James Andrew Bagnell, and Yaser Sheikh. 2014. Pose machines: Articulated pose
    estimation via inference machines. In *ECCV*.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rayat Imtiaz Hossain and Little (2018) Mir Rayat Imtiaz Hossain and James J.
    Little. 2018. Exploiting temporal information for 3D human pose estimation. In
    *ECCV*.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reddy et al. (2021) N Dinesh Reddy, Laurent Guigues, Leonid Pishchulin, Jayan
    Eledath, and Srinivasa G Narasimhan. 2021. TesseTrack: End-to-End Learnable Multi-Person
    Articulated 3D Pose Tracking. In *CVPR*.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remelli et al. (2020) Edoardo Remelli, Shangchen Han, Sina Honari, Pascal Fua,
    and Robert Wang. 2020. Lightweight Multi-View 3D Pose Estimation Through Camera-Disentangled
    Representation. In *CVPR*.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster r-cnn: Towards real-time object detection with region proposal networks.
    In *NeurIPS*.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rhodin et al. (2018a) Helge Rhodin, Mathieu Salzmann, and Pascal Fua. 2018a.
    Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation. In *ECCV*.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rhodin et al. (2018b) Helge Rhodin, Jörg Spörri, Isinsu Katircioglu, Victor
    Constantin, Frédéric Meyer, Erich Müller, Mathieu Salzmann, and Pascal Fua. 2018b.
    Learning Monocular 3D Human Pose Estimation From Multi-View Images. In *CVPR*.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rogez et al. (2017) G. Rogez, P. Weinzaepfel, and C. Schmid. 2017. LCR-Net:
    Localization-Classification-Regression for Human Pose. In *CVPR*.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rogez et al. (2019) Grégory Rogez, Philippe Weinzaepfel, and Cordelia Schmid.
    2019. LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images. In *IEEE
    TPAMI*, Vol. 42\. 1146–1161.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder (2017) Sebastian Ruder. 2017. An overview of multi-task learning in deep
    neural networks. In *arXiv preprint arXiv:1706.05098*.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saini et al. (2019) Nitin Saini, Eric Price, Rahul Tallamraju, Raffi Enficiaud,
    Roman Ludwig, Igor Martinović, Aamir Ahmad, and Michael Black. 2019. Markerless
    Outdoor Human Motion Capture Using Multiple Autonomous Micro Aerial Vehicles.
    In *ICCV*.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saito et al. (2019) Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima,
    Angjoo Kanazawa, and Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution
    Clothed Human Digitization. In *ICCV*.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saito et al. (2020) Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
    Joo. 2020. PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution
    3D Human Digitization. In *CVPR*.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sapp and Taskar (2013) Ben Sapp and Ben Taskar. 2013. Modec: Multimodal decomposable
    models for human pose estimation. In *CVPR*.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarafianos et al. (2016) Nikolaos Sarafianos, Bogdan Boteanu, Bogdan Ionescu,
    and Ioannis A Kakadiaris. 2016. 3d human pose estimation: A review of the literature
    and analysis of covariates. In *CVIU*, Vol. 152\. 1–20.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2019) Saurabh Sharma, Pavan Teja Varigonda, Prashast Bindal,
    Abhishek Sharma, and Arjun Jain. 2019. Monocular 3D Human Pose Estimation by Generation
    and Ordinal Ranking. In *ICCV*.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2022) Dahu Shi, Xing Wei, Liangqi Li, Ye Ren, and Wenming Tan. 2022.
    End-to-End Multi-Person Pose Estimation With Transformers. In *CVPR*.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sigal et al. (2010) L. Sigal, A. Balan, and M. J. Black. 2010. HumanEva: Synchronized
    video and motion capture dataset and baseline algorithm for evaluation of articulated
    human motion. In *IJCV*, Vol. 87. 4.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snower et al. (2020) Michael Snower, Asim Kadav, Farley Lai, and Hans Peter
    Graf. 2020. 15 keypoints is all you need. In *CVPR*.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2019) Kai Su, Dongdong Yu, Zhenqi Xu, Xin Geng, and Changhu Wang.
    2019. Multi-person pose estimation with enhanced channel-wise and spatial information.
    In *CVPR*.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2020) Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff,
    Hartwig Adam, and Ting Liu. 2020. View-Invariant Probabilistic Embedding for Human
    Pose. In *ECCV*.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. 2019. Deep
    high-resolution representation learning for human pose estimation. In *CVPR*.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2017) Xiao Sun, Jiaxiang Shang, Shuang Liang, and Yichen Wei. 2017.
    Compositional human pose regression. In *ICCV*.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang and Wu (2019) Wei Tang and Ying Wu. 2019. Does Learning Specific Features
    for Related Parts Help Human Pose Estimation?. In *CVPR*.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2018) Wei Tang, Pei Yu, and Ying Wu. 2018. Deeply learned compositional
    models for human pose estimation. In *ECCV*.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tekin et al. (2016a) Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent
    Lepetit, and Pascal Fua. 2016a. Structured Prediction of 3D Human Pose with Deep
    Neural Networks. In *BMVC*.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tekin et al. (2017) Bugra Tekin, Pablo Márquez-Neila, Mathieu Salzmann, and
    Pascal Fua. 2017. Learning to fuse 2d and 3d image cues for monocular body pose
    estimation. In *ICCV*.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tekin et al. (2016b) Bugra Tekin, Artem Rozantsev, Vincent Lepetit, and Pascal
    Fua. 2016b. Direct Prediction of 3D Body Poses From Motion Compensated Sequences.
    In *CVPR*.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2019) Zhi Tian, Hao Chen, and Chunhua Shen. 2019. DirectPose:
    Direct End-to-End Multi-Person Pose Estimation. In *arXiv preprint arXiv:1911.07451*.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tome et al. (2020) Denis Tome, Thiemo Alldieck, Patrick Peluse, Gerard Pons-Moll,
    Lourdes Agapito, Hernan Badino, and Fernando De la Torre. 2020. SelfPose: 3D Egocentric
    Pose Estimation from a Headset Mounted Camera. In *arXiv preprint arXiv:2011.01519*.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tome et al. (2019) Denis Tome, Patrick Peluse, Lourdes Agapito, and Hernan
    Badino. 2019. xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera. In *ICCV*.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tompson et al. (2015) Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun,
    and Christoph Bregler. 2015. Efficient object localization using convolutional
    networks. In *CVPR*.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tompson et al. (2014) Jonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph
    Bregler. 2014. Joint training of a convolutional network and a graphical model
    for human pose estimation. In *NeurIPS*.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Toshev and Szegedy (2014) Alexander Toshev and Christian Szegedy. 2014. Deeppose:
    Human pose estimation via deep neural networks. In *CVPR*.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trumble et al. (2017) Matt Trumble, Andrew Gilbert, Charles Malleson, Adrian
    Hilton, and John Collomosse. 2017. Total Capture: 3D Human Pose Estimation Fusing
    Video and Inertial Sensors. In *BMVC*.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. (2020) Hanyue Tu, Chunyu Wang, and Wenjun Zeng. 2020. VoxelPose:
    Towards Multi-Camera 3D Human Pose Estimation in Wild Environment. In *ECCV*.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tung et al. (2017) Hsiao-Yu Fish Tung, Hsiao-Wei Tung, Ersin Yumer, and Katerina
    Fragkiadaki. 2017. Self-Supervised Learning of Motion Capture. In *NeurIPS*.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Umer et al. (2020) Rafi Umer, Andreas Doering, Bastian Leibe, and Juergen Gall.
    2020. Self-supervised Keypoint Correspondences for Multi-Person Pose Estimation
    and Tracking in Videos. In *arXiv preprint arXiv:2004.12652*.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varol et al. (2017) Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood,
    Michael J Black, Ivan Laptev, and Cordelia Schmid. 2017. Learning from synthetic
    humans. In *CVPR*.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vince Tan and Cipolla (2017) Ignas Budvytis Vince Tan and Roberto Cipolla. 2017.
    Indirect deep structured learning for 3D human body shape and pose prediction.
    In *BMVC*.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: von Marcard et al. (2018) Timo von Marcard, Roberto Henschel, Michael J. Black,
    Bodo Rosenhahn, and Gerard Pons-Moll. 2018. Recovering Accurate 3D Human Pose
    in The Wild Using IMUs and a Moving Camera. In *ECCV*.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Von Marcard et al. (2017) Timo Von Marcard, Bodo Rosenhahn, Michael J Black,
    and Gerard Pons-Moll. 2017. Sparse inertial poser: Automatic 3d human pose estimation
    from sparse imus. In *Computer Graphics Forum*.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wandt and Rosenhahn (2019) Bastian Wandt and Bodo Rosenhahn. 2019. RepNet:
    Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human
    Pose Estimation. In *CVPR*.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Haoyang Wang, Riza Alp Guler, Iasonas Kokkinos, George
    Papandreou, and Stefanos Zafeiriou. 2020a. BLSM: A Bone-Level Skinned Model of
    the Human Mesh. In *ECCV*.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022c) Haixin Wang, Lu Zhou, Yingying Chen, Ming Tang, and Jinqiao
    Wang. 2022c. Regularizing Vector Embedding in Bottom-Up Human Pose Estimation.
    In *ECCV*.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Jue Wang, Shaoli Huang, Xinchao Wang, and Dacheng Tao.
    2019a. Not All Parts Are Created Equal: 3D Pose Estimation by Modeling Bi-Directional
    Dependencies of Body Parts. In *ICCV*.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Jian Wang, Xiang Long, Yuan Gao, Errui Ding, and Shilei
    Wen. 2020b. Graph-PCNN: Two Stage Human Pose Estimation with Graph Pose Refinement.
    In *arXiv preprint arXiv:2007.10599*.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Jianbo Wang, Kai Qiu, Houwen Peng, Jianlong Fu, and Jianke
    Zhu. 2019b. AI Coach: Deep Human Pose Estimation and Analysis for Personalized
    Athletic Training Assistance. In *ACM MM*.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020d) Jingbo Wang, Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2020d.
    Motion Guided 3D Pose Estimation from Videos. In *ECCV*.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Kangkan Wang, Jin Xie, Guofeng Zhang, Lei Liu, and Jian
    Yang. 2020c. Sequential 3D Human Pose and Shape Estimation From Point Clouds.
    In *CVPR*.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Min Wang, Xipeng Chen, Wentao Liu, Chen Qian, Liang Lin,
    and Lizhuang Ma. 2018. DRPose3D: Depth Ranking in 3D Human Pose Estimation. In
    *IJCAI*.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Tao Wang, Jianfeng Zhang, Yujun Cai, Shuicheng Yan, and Jiashi
    Feng. 2021. Direct Multi-view Multi-person 3D Human Pose Estimation. In *NeurIPS*.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) Yihan Wang, Muyang Li, Han Cai, Wei-Ming Chen, and Song
    Han. 2022a. Lite pose: Efficient architecture design for 2d human pose estimation.
    In *CVPR*.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Zitian Wang, Xuecheng Nie, Xiaochao Qu, Yunpeng Chen, and
    Si Liu. 2022b. Distribution-Aware Single-Stage Models for Multi-Person 3D Pose
    Estimation. In *CVPR*.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2016) Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh.
    2016. Convolutional pose machines. In *CVPR*.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng et al. (2019) C. Weng, B. Curless, and I. Kemelmacher-Shlizerman. 2019.
    Photo Wake-Up: 3D Character Animation From a Single Photo. In *CVPR*.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Willett et al. (2020) Nora S Willett, Hijung Valentina Shin, Zeyu Jin, Wilmot
    Li, and Adam Finkelstein. 2020. Pose2Pose: pose selection and transfer for 2D
    character animation. In *IUI*.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2017) Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui
    Liang, Wenjia Wang, Shipei Zhou, et al. 2017. Ai challenger: A large-scale dataset
    for going deeper in image understanding. In *arXiv preprint arXiv:1711.06475*.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiang et al. (2019) Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. 2019. Monocular
    total capture: Posing face, body, and hands in the wild. In *CVPR*.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2018) Bin Xiao, Haiping Wu, and Yichen Wei. 2018. Simple Baselines
    for Human Pose Estimation and Tracking. In *ECCV*.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2020) Rongchang Xie, Chunyu Wang, and Yizhou Wang. 2020. MetaFuse:
    A Pre-trained Fusion Model for Human Pose Estimation. In *CVPR*.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2019) Fu Xiong, Boshen Zhang, Yang Xiao, Zhiguo Cao, Taidong
    Yu, Joey Zhou Tianyi, and Junsong Yuan. 2019. A2J: Anchor-to-Joint Regression
    Network for 3D Articulated Pose Estimation from a Single Depth Image. In *ICCV*.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020a) Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William T.
    Freeman, Rahul Sukthankar, and Cristian Sminchisescu. 2020a. GHUM $\&amp;$ GHUML:
    Generative 3D Human Shape and Articulated Pose Models. In *CVPR*.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020c) Jingwei Xu, Zhenbo Yu, Bingbing Ni, Jiancheng Yang, Xiaokang
    Yang, and Wenjun Zhang. 2020c. Deep Kinematics Analysis for Monocular 3D Human
    Pose Estimation. In *CVPR*.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Lumin Xu, Yingda Guan, Sheng Jin, Wentao Liu, Chen Qian, Ping
    Luo, Wanli Ouyang, and Xiaogang Wang. 2021. Vipnas: Efficient video pose estimation
    via neural architecture search. In *CVPR*.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2019) Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge
    Rhodin, Pascal Fua, Hans-Peter Seidel, and Christian Theobalt. 2019. Mo 2 cap
    2: Real-time mobile 3d motion capture with a cap-mounted fisheye camera. In *IEEE
    TVCG Proc. VR*, Vol. 25\. 2093–2101.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020b) Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni,
    and Fernando De la Torre. 2020b. 3D Human Shape and Pose from a Single Low-Resolution
    Image with Self-Supervised Learning. In *ECCV*.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2018) Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial temporal
    graph convolutional networks for skeleton-based action recognition. In *AAAI*.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021) Sen Yang, Zhibin Quan, Mu Nie, and Wankou Yang. 2021. Transpose:
    Keypoint localization via transformer. In *ICCV*.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2017) Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, and Xiaogang
    Wang. 2017. Learning feature pyramids for human pose estimation. In *ICCV*.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2016) Wei Yang, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang.
    2016. End-to-end learning of deformable mixture of parts and deep convolutional
    neural networks for human pose estimation. In *CVPR*.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2018) Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy Ren, Hongsheng
    Li, and Xiaogang Wang. 2018. 3D Human Pose Estimation in the Wild by Adversarial
    Learning. In *CVPR*.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Ramanan (2012) Yi Yang and Deva Ramanan. 2012. Articulated human detection
    with flexible mixtures of parts. In *IEEE TPAMI*, Vol. 35\. 2878–2890.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2022) Hang Ye, Wentao Zhu, Chunyu Wang, Rujie Wu, and Yizhou Wang.
    2022. Faster VoxelPose: Real-time 3D Human Pose Estimation by Orthographic Projection.
    In *ECCV*.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021) Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei Zhang,
    Nong Sang, and Jingdong Wang. 2021. Lite-hrnet: A lightweight high-resolution
    network. In *CVPR*.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019) T. Yu, J. Zhao, Z. Zheng, K. Guo, Q. Dai, H. Li, G. Pons-Moll,
    and Y. Liu. 2019. DoubleFusion: Real-time Capture of Human Performances with Inner
    Body Shapes from a Single Depth Sensor. In *IEEE TPAMI*. 7287–7296.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019) Tao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qionghai Dai,
    Gerard Pons-Moll, and Yebin Liu. 2019. Simulcap: Single-view human performance
    capture with cloth simulation. In *CVPR*.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2021) Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang,
    Xilin Chen, and Jingdong Wang. 2021. Hrformer: High-resolution vision transformer
    for dense predict. In *NeurIPS*.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zanfir et al. (2020) Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, Bill
    Freeman, Rahul Sukthankar, and Cristian Sminchisescu. 2020. Weakly Supervised
    3D Human Pose and Shape Reconstruction with Normalizing Flows. In *ECCV*.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zanfir et al. (2018) A. Zanfir, E. Marinoiu, and C. Sminchisescu. 2018. Monocular
    3D Pose and Shape Estimation of Multiple People in Natural Scenes: The Importance
    of Multiple Scene Constraints. In *CVPR*.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zanfir et al. (2018) Andrei Zanfir, Elisabeta Marinoiu, Mihai Zanfir, Alin-Ionut
    Popa, and Cristian Sminchisescu. 2018. Deep Network for the Integrated 3D Sensing
    of Multiple People in Natural Images. In *NeurIPS*.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2020) Ailing Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, Qiang Xu,
    and Stephen Lin. 2020. SRNet: Improving Generalization in 3D Human Pose Estimation
    with a Split-and-Recombine Approach. In *ECCV*.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2020) W. Zeng, W. Ouyang, P. Luo, W. Liu, and X. Wang. 2020. 3D
    Human Mesh Regression With Dense Correspondence. In *CVPR*.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020h) Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu.
    2020h. Distribution-aware coordinate representation for human pose estimation.
    In *CVPR*.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Feng Zhang, Xiatian Zhu, and Mao Ye. 2019b. Fast human
    pose estimation. In *CVPR*.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) Hong Zhang, Hao Ouyang, Shu Liu, Xiaojuan Qi, Xiaoyong
    Shen, Ruigang Yang, and Jiaya Jia. 2019a. Human pose estimation with spatial contextual
    information. In *arXiv preprint arXiv:1901.01760*.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020d) Haotian Zhang, Cristobal Sciutto, Maneesh Agrawala, and
    Kayvon Fatahalian. 2020d. Vid2Player: Controllable Video Sprites that Behave and
    Appear like Professional Tennis Players. In *arXiv preprint arXiv:2008.04524*.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,
    Yebin Liu, Limin Wang, and Zhenan Sun. 2021. PyMAF: 3D Human Pose and Shape Regression
    with Pyramidal Mesh Alignment Feedback Loop. In *ICCV*.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Jinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, and Junsong
    Yuan. 2022. MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation
    in Video. In *CVPR*.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020c) Tianshu Zhang, Buzhen Huang, and Yangang Wang. 2020c. Object-Occluded
    Human Shape and Pose Estimation From a Single Color Image. In *CVPR*.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Wenqiang Zhang, Jiemin Fang, Xinggang Wang, and Wenyu
    Liu. 2020b. EfficientPose: Efficient Human Pose Estimation with Neural Architecture
    Search. In *arXiv preprint arXiv:2012.07086*.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2013) Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis.
    2013. From actemes to action: A strongly-supervised representation for detailed
    action understanding. In *ICCV*.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Yuxiang Zhang, Liang An, Tao Yu, xiu Li, Kun Li, and Yebin
    Liu. 2020a. 4D Association Graph for Realtime Multi-person Motion Capture Using
    Multiple Video Cameras. In *CVPR*.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020e) Yuexi Zhang, Yin Wang, Octavia Camps, and Mario Sznaier.
    2020e. Key Frame Proposal Network for Efficient Pose Estimation in Videos. In
    *arXiv preprint arXiv:2007.15217*.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020f) Zhe Zhang, Chunyu Wang, Wenhu Qin, and Wenjun Zeng. 2020f.
    Fusing Wearable IMUs With Multi-View Images for Human Pose Estimation: A Geometric
    Approach. In *CVPR*.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020g) Zhe Zhang, Chunyu Wang, Weichao Qiu, Wenhu Qin, and Wenjun
    Zeng. 2020g. AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation
    in the Wild. In *IJCV*, Vol. 129\. 703–718.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019b) Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris N.
    Metaxas. 2019b. Semantic Graph Convolutional Networks for 3D Human Pose Regression.
    In *CVPR*.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019a) Mingmin Zhao, Yingcheng Liu, Aniruddh Raghu, Tianhong Li,
    Hang Zhao, Antonio Torralba, and Dina Katabi. 2019a. Through-Wall Human Mesh Recovery
    Using Radio Signals. In *ICCV*.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Mingmin Zhao, Yonglong Tian, Hang Zhao, Mohammad Abu Alsheikh,
    Tianhong Li, Rumen Hristov, Zachary Kabelac, Dina Katabi, and Antonio Torralba.
    2018. RF-based 3D skeletons. In *SIGCOMM*.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023) Qitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, and Chen
    Chen. 2023. PoseFormerV2: Exploring Frequency Domain for Efficient and Robust
    3D Human Pose Estimation. In *CVPR*.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhen et al. (2020) Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang,
    Hujun Bao, and Xiaowei Zhou. 2020. SMAP: Single-Shot Multi-Person Absolute 3D
    Pose Estimation. In *ECCV*.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023a) Ce Zheng, Xianpeng Liu, Guo-Jun Qi, and Chen Chen. 2023a.
    POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery. In *CVPR*.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2022) Ce Zheng, Matias Mendieta, Pu Wang, Aidong Lu, and Chen
    Chen. 2022. A Lightweight Graph Transformer Network for Human Mesh Reconstruction
    from 2D Human Pose. In *ACM Multimedia*.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023b) Ce Zheng, Matias Mendieta, Taojiannan Yang, Guo-Jun Qi,
    and Chen Chen. 2023b. FeatER: An Efficient Network for Human Reconstruction via
    Feature Map-Based TransformER. In *CVPR*.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2021) Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen
    Chen, and Zhengming Ding. 2021. 3D Human Pose Estimation with Spatial and Temporal
    Transformers. In *ICCV*.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhi et al. (2020) Tiancheng Zhi, Christoph Lassner, Tony Tung, Carsten Stoll,
    Srinivasa G. Narasimhan, and Minh Vo. 2020. TexMesh: Reconstructing Detailed Human
    Texture and Geometry from RGB-D Video. In *ECCV*.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020) Keyang Zhou, Bharat Lal Bhatnagar, and Gerard Pons-Moll.
    2020. Unsupervised Shape and Pose Disentanglement for 3D Meshes. In *arXiv preprint
    arXiv:2007.11341*.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2019) K. Zhou, X. Han, N. Jiang, K. Jia, and J. Lu. 2019. HEMlets
    Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation.
    In *ICCV*.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2017) Xingyi Zhou, Qixing Huang, Xiao Sun, Xiangyang Xue, and
    Yichen Wei. 2017. Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised
    Approach. In *ICCV*.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016a) Xingyi Zhou, Xiao Sun, Wei Zhang, Shuang Liang, and Yichen
    Wei. 2016a. Deep kinematic pose regression. In *ECCV*.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2019) Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. 2019. Objects
    as points. In *arXiv preprint arXiv:1904.07850*.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2016b) Xiaowei Zhou, Menglong Zhu, Kosta Derpanis, and Kostas
    Daniilidis. 2016b. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular
    Video. In *CVPR*.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2016b) Xiaowei Zhou, Menglong Zhu, Kosta Derpanis, 和 Kostas Daniilidis.
    2016b. 稀疏性遇上深度：从单目视频中进行3D人体姿势估计。在 *CVPR*。
- en: 'Zhou et al. (2018) Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyridon
    Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. 2018. Monocap: Monocular
    human motion capture using a cnn coupled with a geometric prior. In *IEEE TPAMI*,
    Vol. 41\. 901–914.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2018) Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyridon Leonardos,
    Konstantinos G Derpanis, 和 Kostas Daniilidis. 2018. Monocap：使用与几何先验结合的CNN进行单目人体运动捕捉。在
    *IEEE TPAMI*, 第41卷. 901–914。
- en: Zhu et al. (2019) Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang Yang.
    2019. Detailed human shape estimation from a single image by hierarchical mesh
    deformation. In *CVPR*.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2019) Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, 和 Ruigang Yang. 2019.
    通过分层网格变形从单张图像中详细估计人体形状。在 *CVPR*。
- en: Zhu et al. (2020) Luyang Zhu, Konstantinos Rematas, Brian Curless, Steve Seitz,
    and Ira Kemelmacher-Shlizerman. 2020. Reconstructing NBA players. In *ECCV*.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2020) Luyang Zhu, Konstantinos Rematas, Brian Curless, Steve Seitz,
    和 Ira Kemelmacher-Shlizerman. 2020. 重建NBA球员。在 *ECCV*。
- en: Zhu et al. (2017) Xiangyu Zhu, Yingying Jiang, and Zhenbo Luo. 2017. Multi-person
    pose estimation for posetrack with enhanced part affinity fields. In *ICCV PoseTrack
    Workshop*.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2017) Xiangyu Zhu, Yingying Jiang, 和 Zhenbo Luo. 2017. 用于Posetrack的多人的姿势估计，结合增强的部分关联场。在
    *ICCV PoseTrack Workshop*。
- en: Zou and Tang (2021) Zhiming Zou and Wei Tang. 2021. Modulated Graph Convolutional
    Network for 3D Human Pose Estimation. In *ICCV*.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou and Tang (2021) Zhiming Zou 和 Wei Tang. 2021. 调制图卷积网络用于3D人体姿势估计。在 *ICCV*。
- en: 'Zuffi and Black (2015) Silvia Zuffi and Michael J. Black. 2015. The Stitched
    Puppet: A Graphical Model of 3D Human Shape and Pose. In *CVPR*.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zuffi and Black (2015) Silvia Zuffi 和 Michael J. Black. 2015. 缝合木偶：3D人体形状和姿势的图形模型。在
    *CVPR*。
