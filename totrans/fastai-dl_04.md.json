["```py\nlearn = ConvLearner.pretrained(arch, data, ps=0.5, precompute=True)\n```", "```py\nlearn *Sequential(\n  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n  (1): Dropout(p=0.5)\n  (2): Linear(in_features=1024, out_features=512)\n  (3): ReLU()\n  (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n  (5): Dropout(p=0.5)\n  (6): Linear(in_features=512, out_features=120)\n  (7): LogSoftmax()\n)*\n```", "```py\n*Dropout(p=0.5)*\n```", "```py\nlearn = ConvLearner.pretrained(arch, data, **ps=0.5**, precompute=True)\n```", "```py\n[2\\.      **0.3521**   **0.55247**  0.84189]\n```", "```py\nSequential(\n  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n  (1): Linear(in_features=4096, out_features=512)\n  (2): ReLU()\n  (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n  (4): Linear(in_features=512, out_features=120)\n  (5): LogSoftmax()\n)\n```", "```py\nlearn = ConvLearner.pretrained(arch, data, ps=0., precompute=True, \n            **xtra_fc=[]**); learn *Sequential(\n  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n  (1): Linear(in_features=1024, out_features=120)\n  (2): LogSoftmax()\n)*learn = ConvLearner.pretrained(arch, data, ps=0., precompute=True, \n            **xtra_fc=[700, 300]**); learn*Sequential(\n  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n  (1): Linear(in_features=1024, out_features=****700****)\n  (2): ReLU()\n  (3): BatchNorm1d(700, eps=1e-05, momentum=0.1, affine=True)\n  (4): Linear(in_features=700, out_features=****300****)\n  (5): ReLU()\n  (6): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True)\n  (7): Linear(in_features=300, out_features=120)\n  (8): LogSoftmax()\n)*\n```", "```py\nlearn = ConvLearner.pretrained(arch, data, ps=[0., 0.2],\n            precompute=True, xtra_fc=[512]); learn*Sequential(\n  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)\n  (1): Linear(in_features=4096, out_features=512)\n  (2): ReLU()\n  (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)\n  (4): Dropout(p=0.2)\n  (5): Linear(in_features=512, out_features=120)\n  (6): LogSoftmax()\n)*\n```", "```py\ncat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day',\n            'StateHoliday', 'CompetitionMonthsOpen', 'Promo2Weeks',\n            'StoreType', 'Assortment', 'PromoInterval', \n            'CompetitionOpenSinceYear', 'Promo2SinceYear', 'State',\n            'Week', 'Events', 'Promo_fw', 'Promo_bw', \n            'StateHoliday_fw', 'StateHoliday_bw', \n            'SchoolHoliday_fw', 'SchoolHoliday_bw']contin_vars = ['CompetitionDistance', 'Max_TemperatureC', \n               'Mean_TemperatureC', 'Min_TemperatureC', \n               'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', \n               'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', \n               'CloudCover', 'trend', 'trend_DE', \n               'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', \n               'SchoolHoliday']n = len(joined); n\n```", "```py\nfor v in cat_vars: \n    joined[v] = joined[v].astype('category').cat.as_ordered()for v in contin_vars:\n    joined[v] = joined[v].astype('float32')dep = 'Sales'\njoined = joined[cat_vars+contin_vars+[dep, 'Date']].copy()\n```", "```py\nidxs = get_cv_idxs(n, val_pct=150000/n) \njoined_samp = joined.iloc[idxs].set_index(\"Date\") \nsamp_size = len(joined_samp); samp_size\n```", "```py\ndf, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True)\nyl = np.log(y)\n```", "```py\nval_idx = np.flatnonzero((df.index<=datetime.datetime(2014,9,17)) &\n              (df.index>=datetime.datetime(2014,8,1)))\n```", "```py\ndef inv_y(a): return np.exp(a)def exp_rmspe(y_pred, targ):\n    targ = inv_y(targ)\n    pct_var = (targ - inv_y(y_pred))/targ\n    return math.sqrt((pct_var**2).mean())max_log_y = np.max(yl)\ny_range = (0, max_log_y*1.2)\n```", "```py\nmd = **ColumnarModelData.from_data_frame**(PATH, val_idx, df, \n         yl.astype(np.float32), cat_flds=cat_vars, bs=128, \n         test_df=df_test)\n```", "```py\nm = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n                   0.04, 1, [1000,500], [0.001,0.01], \n                   y_range=y_range)\n```", "```py\ncat_sz = [(c, len(joined_samp[c].cat.categories)+1) \n             **for** c **in** cat_vars]\ncat_sz*[('Store', 1116),\n ('DayOfWeek', 8),\n ('Year', 4),\n ('Month', 13),\n ('Day', 32),\n ('StateHoliday', 3),\n ('CompetitionMonthsOpen', 26),\n ('Promo2Weeks', 27),\n ('StoreType', 5),\n ('Assortment', 4),\n ('PromoInterval', 4),\n ('CompetitionOpenSinceYear', 24),\n ('Promo2SinceYear', 9),\n ('State', 13),\n ('Week', 53),\n ('Events', 22),\n ('Promo_fw', 7),\n ('Promo_bw', 7),\n ('StateHoliday_fw', 4),\n ('StateHoliday_bw', 4),\n ('SchoolHoliday_fw', 9),\n ('SchoolHoliday_bw', 9)]*\n```", "```py\nemb_szs = [(c, min(50, (c+1)//2)) **for** _,c **in** cat_sz]\nemb_szs*[(1116, 50),\n (8, 4),\n (4, 2),\n (13, 7),\n (32, 16),\n (3, 2),\n (26, 13),\n (27, 14),\n (5, 3),\n (4, 2),\n (4, 2),\n (24, 12),\n (9, 5),\n (13, 7),\n (53, 27),\n (22, 11),\n (7, 4),\n (7, 4),\n (4, 2),\n (4, 2),\n (9, 5),\n (9, 5)]*\n```", "```py\nm = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,\n                   [1000,500], [0.001,0.01], y_range=y_range)\n```", "```py\nadd_datepart(weather, \"Date\", drop=False)\nadd_datepart(googletrend, \"Date\", drop=False)\nadd_datepart(train, \"Date\", drop=False)\nadd_datepart(test, \"Date\", drop=False)\n```", "```py\nm = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,\n                   [1000,500], [0.001,0.01], y_range=y_range)\nlr = 1e-3\n```", "```py\nm.fit(lr, 3, metrics=[exp_rmspe])*A Jupyter Widget**[ 0\\.       0.02479  0.02205* ***0.19309****]                          \n[ 1\\.       0.02044  0.01751* ***0.18301****]                          \n[ 2\\.       0.01598  0.01571* ***0.17248****]*\n```", "```py\nm.fit(lr, 1, metrics=[exp_rmspe], cycle_len=1)*[ 0\\.       0.00676  0.01041  0.09711]* \n```", "```py\nlearn = ConvLearner.pretrained(arch, data, ps=0., precompute=True)\n```", "```py\nm = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,\n                   [1000,500], [0.001,0.01], y_range=y_range)\n```", "```py\nmd = ColumnarModelData.from_data_frame(PATH, val_idx, df, \n         yl.astype(np.float32), cat_flds=cat_vars, bs=128, \n         test_df=df_test)\n```", "```py\nm = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,\n                   [1000,500], [0.001,0.01], y_range=y_range)\n```", "```py\n' '.join(md.trn_ds[0].text[:150])*'<cat> csni <summ> the exploitation of mm - wave bands is one of the key - enabler for 5 g mobile \\n radio networks . however , the introduction of mm - wave technologies in cellular \\n networks is not straightforward due to harsh propagation conditions that limit \\n the mm - wave access availability . mm - wave technologies require high - gain antenna \\n systems to compensate for high path loss and limited power . as a consequence , \\n directional transmissions must be used for cell discovery and synchronization \\n processes : this can lead to a non - negligible access delay caused by the \\n exploration of the cell area with multiple transmissions along different \\n directions . \\n    the integration of mm - wave technologies and conventional wireless access \\n networks with the objective of speeding up the cell search process requires new \\n'*\n```", "```py\nsample_model(m, \"<CAT> csni <SUMM> algorithms that\")*...use the same network as a single node are not able to achieve the same performance as the traditional network - based routing algorithms . in this paper , we propose a novel routing scheme for routing protocols in wireless networks . the proposed scheme is based ...*\n```", "```py\nsample_model(m, \"<CAT> cscv <SUMM> algorithms that\")*...use the same data to perform image classification are increasingly being used to improve the performance of image classification algorithms . in this paper , we propose a novel method for image classification using a deep convolutional neural network ( cnn ) . the proposed method is ...*\n```", "```py\nsample_model(m,\"<CAT> cscv <SUMM> algorithms. <TITLE> on \")*...the performance of deep learning for image classification <eos>*sample_model(m,\"<CAT> csni <SUMM> algorithms. <TITLE> on \")*...the performance of wireless networks <eos>*sample_model(m,\"<CAT> cscv <SUMM> algorithms. <TITLE> towards \")*...a new approach to image classification <eos>*sample_model(m,\"<CAT> csni <SUMM> algorithms. <TITLE> towards \")*...a new approach to the analysis of wireless networks <eos>*\n```", "```py\nfrom fastai.learner import *import torchtext\nfrom torchtext import vocab, data\nfrom torchtext.datasets import language_modelingfrom fastai.rnn_reg import *\nfrom fastai.rnn_train import *\nfrom fastai.nlp import *\nfrom fastai.lm_rnn import *import dill as pickle\n```", "```py\nPATH = 'data/aclImdb/'TRN_PATH = 'train/all/'\nVAL_PATH = 'test/all/'\nTRN = f'{PATH}{TRN_PATH}'\nVAL = f'{PATH}{VAL_PATH}'%ls {PATH}*imdbEr.txt  imdb.vocab  models/  README  test/  tmp/  train/*\n```", "```py\ntrn_files = !ls {TRN}\ntrn_files[:10]\n\n*['0_0.txt',\n '0_3.txt',\n '0_9.txt',\n '10000_0.txt',\n '10000_4.txt',\n '10000_8.txt',\n '1000_0.txt',\n '10001_0.txt',\n '10001_10.txt',\n '10001_4.txt']*review = !cat {TRN}{trn_files[6]}\nreview[0]*\"I have to say when a name like Zombiegeddon and an atom bomb on the front cover I was expecting a flat out chop-socky fung-ku, but what I got instead was a comedy. So, it wasn't quite was I was expecting, but I really liked it anyway! The best scene ever was the main cop dude pulling those kids over and pulling a Bad Lieutenant on them!! I was laughing my ass off. I mean, the cops were just so bad! And when I say bad, I mean The Shield Vic Macky bad. But unlike that show I was laughing when they shot people and smoked dope.<br /><br />Felissa Rose...man, oh man. What can you say about that hottie. She was great and put those other actresses to shame. She should work more often!!!!! I also really liked the fight scene outside of the building. That was done really well. Lots of fighting and people getting their heads banged up. FUN! Last, but not least Joe Estevez and William Smith were great as the...well, I wasn't sure what they were, but they seemed to be having fun and throwing out lines. I mean, some of it didn't make sense with the rest of the flick, but who cares when you're laughing so hard! All in all the film wasn't the greatest thing since sliced bread, but I wasn't expecting that. It was a Troma flick so I figured it would totally suck. It's nice when something surprises you but not totally sucking.<br /><br />Rent it if you want to get stoned on a Friday night and laugh with your buddies. Don't rent it if you are an uptight weenie or want a zombie movie with lots of flesh eating.<br /><br />P.S. Uwe Boil was a nice touch.\"*\n```", "```py\n!find {TRN} -name '*.txt' | xargs cat | wc -w*17486581*!find {VAL} -name '*.txt' | xargs cat | wc -w*5686719*\n```", "```py\nTEXT = data.Field(lower=**True**, tokenize=spacy_tok)\n```", "```py\nbs=64; bptt=70FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\nmd = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, \n                                       bptt=bptt, min_freq=10)\n```", "```py\n*# 'itos': 'int-to-string'* \nTEXT.vocab.itos[:12]*['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'it', 'in']**# 'stoi': 'string to int'*\nTEXT.vocab.stoi['the']*2*\n```", "```py\nmd.trn_ds[0].text[:12]*['i',\n 'have',\n 'always',\n 'loved',\n 'this',\n 'story',\n '-',\n 'the',\n 'hopeful',\n 'theme',\n ',',\n 'the']*TEXT.numericalize([md.trn_ds[0].text[:12]])*Variable containing:\n   12\n   35\n  227\n  480\n   13\n   76\n   17\n    2\n 7319\n  769\n    3\n    2\n[torch.cuda.LongTensor of size 12x1 (GPU 0)]*\n```", "```py\nnext(iter(md.trn_dl))*(Variable containing:\n     12    567      3  ...    2118      4   2399\n    *** *35      7     33*** *...       6    148     55\n    227    103    533  ...    4892     31     10\n         ...            \u22f1           ...         \n     19   8879     33  ...      41     24    733\n    552   8250     57  ...     219     57   1777\n      5     19      2  ...    3099      8     48\n [torch.cuda.LongTensor of size 75x64 (GPU 0)], Variable containing:* ***35******7******33*** *\u22ee   \n     22\n   3885\n  21587\n [torch.cuda.LongTensor of size 4800 (GPU 0)])*\n```", "```py\nlen(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)*(4602, 34945, 1, 20621966)*\n```", "```py\nem_sz = 200  *# size of each embedding vector*\nnh = 500     *# number of hidden activations per layer*\nnl = 3       *# number of layers*\n```", "```py\nopt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n```", "```py\nlearner = md.get_model(opt_fn, em_sz, nh, nl, dropouti=0.05,\n                       dropout=0.05, wdrop=0.1, dropoute=0.02, \n                       dropouth=0.05)\nlearner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\nlearner.clip=0.3\n```", "```py\nlearner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)learner.save_encoder('adam1_enc')learner.fit(3e-3, 4, wds=1e-6, cycle_len=10, \n            cycle_save_name='adam3_10')learner.save_encoder('adam3_10_enc')learner.fit(3e-3, 1, wds=1e-6, cycle_len=20, \n            cycle_save_name='adam3_20')learner.load_cycle('adam3_20',0)\n```", "```py\nlearner.save_encoder('adam3_20_enc')learner.load_encoder('adam3_20_enc')\n```", "```py\nmath.exp(4.165)*64.3926824434624*pickle.dump(TEXT, open(f'**{PATH}**models/TEXT.pkl','wb'))\n```", "```py\nm=learner.model\nss=\"\"\". So, it wasn't quite was I was expecting, but I really liked it anyway! The best\"\"\"s = [spacy_tok(ss)]\nt=TEXT.numericalize(s)\n' '.join(s[0])*\". So , it was n't quite was I was expecting , but I really liked it anyway ! The best\"*\n```", "```py\n*# Set batch size to 1*\nm[0].bs=1\n*# Turn off dropout*\nm.eval()\n*# Reset hidden state*\nm.reset()\n*# Get predictions from model*\nres,*_ = m(t)\n*# Put the batch size back to what it was*\nm[0].bs=bs\n```", "```py\nnexts = torch.topk(res[-1], 10)[1]\n[TEXT.vocab.itos[o] **for** o **in** to_np(nexts)]*['film',\n 'movie',\n 'of',\n 'thing',\n 'part',\n '<unk>',\n 'performance',\n 'scene',\n ',',\n 'actor']*\n```", "```py\nprint(ss,\"**\\n**\")\n**for** i **in** range(50):\n    n=res[-1].topk(2)[1]\n    n = n[1] **if** n.data[0]==0 **else** n[0]\n    print(TEXT.vocab.itos[n.data[0]], end=' ')\n    res,*_ = m(n[0].unsqueeze(0))\nprint('...')*. So, it wasn't quite was I was expecting, but I really liked it anyway! The best* *film ever ! <eos> i saw this movie at the toronto international film festival . i was very impressed . i was very impressed with the acting . i was very impressed with the acting . i was surprised to see that the actors were not in the movie . ...*\n```", "```py\nTEXT = pickle.load(open(f'**{PATH}**models/TEXT.pkl','rb'))\n```", "```py\nIMDB_LABEL = data.Field(sequential=**False**)\n```", "```py\n**splits** = torchtext.datasets.IMDB.splits(TEXT, IMDB_LABEL, 'data/')t = splits[0].examples[0]t.label, ' '.join(t.text[:16])*('pos', 'ashanti is a very 70s sort of film ( 1979 , to be precise ) .')*\n```", "```py\nmd2 = TextData.from_splits(PATH, splits, bs)\n```", "```py\nm3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, \n                   n_layers=nl, dropout=0.1, dropouti=0.4,\n                   wdrop=0.5, dropoute=0.05, dropouth=0.3)m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)m3.**load_encoder**(f'adam3_20_enc')\n```", "```py\nm3.clip=25.\nlrs=np.array([1e-4,1e-3,1e-2])m3.freeze_to(-1)\nm3.fit(lrs/2, 1, metrics=[accuracy])\nm3.unfreeze()\nm3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)*[ 0\\.       0.45074  0.28424  0.88458]**[ 0\\.       0.29202  0.19023  0.92768]*\n```", "```py\nm3.fit(lrs, 7, metrics=[accuracy], cycle_len=2, \n       cycle_save_name='imdb2')[ 0\\.       0.29053  0.18292  0.93241]                        \n[ 1\\.       0.24058  0.18233  0.93313]                        \n[ 2\\.       0.24244  0.17261  0.93714]                        \n[ 3\\.       0.21166  0.17143  0.93866]                        \n[ 4\\.       0.2062   0.17143  0.94042]                        \n[ 5\\.       0.18951  0.16591  0.94083]                        \n[ 6\\.       0.20527  0.16631  0.9393 ]                        \n[ 7\\.       0.17372  0.16162  0.94159]                        \n[ 8\\.       0.17434  0.17213  0.94063]                        \n[ 9\\.       0.16285  0.16073  0.94311]                        \n[ 10\\.        0.16327   0.17851   0.93998]                    \n[ 11\\.        0.15795   0.16042   0.94267]                    \n[ 12\\.        0.1602    0.16015   0.94199]                    \n[ 13\\.        0.15503   0.1624    0.94171]m3.load_cycle('imdb2', 4)accuracy(*m3.predict_with_targs())*0.94310897435897434*\n```", "```py\npath='data/ml-latest-small/'ratings = pd.read_csv(path+'ratings.csv')\nratings.head()\n```", "```py\nmovies = pd.read_csv(path+'movies.csv')\nmovies.head()\n```", "```py\ng=ratings.groupby('userId')['rating'].count()\ntopUsers=g.sort_values(ascending=False)[:15]g=ratings.groupby('movieId')['rating'].count()\ntopMovies=g.sort_values(ascending=False)[:15]top_r = ratings.join(topUsers, rsuffix='_r', how='inner', \n                     on='userId')\ntop_r = top_r.join(topMovies, rsuffix='_r', how='inner', \n                   on='movieId')pd.crosstab(top_r.userId, top_r.movieId, top_r.rating, \n            aggfunc=np.sum)\n```"]