- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:49:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:49:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2111.06119] Fine-Grained Image Analysis with Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2111.06119] 基于深度学习的细粒度图像分析：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.06119](https://ar5iv.labs.arxiv.org/html/2111.06119)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2111.06119](https://ar5iv.labs.arxiv.org/html/2111.06119)
- en: 'Fine-Grained Image Analysis with Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的细粒度图像分析：综述
- en: Xiu-Shen Wei, , Yi-Zhe Song, , Oisin Mac Aodha, Jianxin Wu, , Yuxin Peng, ,
    Jinhui Tang, , Jian Yang, , Serge Belongie X.-S. Wei and J. Yang are with PCA
    Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information
    of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding
    for Social Security, School of Computer Science and Engineering, Nanjing University
    of Science and Technology, China. Y.-Z. Song is with University of Surrey, UK.
    O. Mac Aodha is with the University of Edinburgh, UK. J. Wu is the State Key Laboratory
    for Novel Software Technology, Nanjing University, China. Y. Peng is with Peking
    University, China. J. Tang is with Nanjing University of Science and Technology,
    China. S. Belongie is with the University of Copenhagen and the Pioneer Centre
    for AI, Denmark.X.-S. Wei and J. Yang are the corresponding authors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xiu-Shen Wei, , Yi-Zhe Song, , Oisin Mac Aodha, Jianxin Wu, , Yuxin Peng, ,
    Jinhui Tang, , Jian Yang, , Serge Belongie X.-S. Wei 和 J. Yang 现任职于中国南京理工大学计算机科学与工程学院，教育部高维信息智能感知与系统重点实验室，以及江苏省社会安全图像与视频理解重点实验室。Y.-Z.
    Song 现任职于英国萨里大学。O. Mac Aodha 现任职于英国爱丁堡大学。J. Wu 是中国南京大学新型软件技术国家重点实验室的成员。Y. Peng
    现任职于中国北京大学。J. Tang 现任职于中国南京理工大学。S. Belongie 现任职于丹麦哥本哈根大学和先锋AI中心。X.-S. Wei 和 J.
    Yang 是通讯作者。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-grained image analysis (FGIA) is a longstanding and fundamental problem
    in computer vision and pattern recognition, and underpins a diverse set of real-world
    applications. The task of FGIA targets analyzing visual objects from subordinate
    categories, *e.g.*, species of birds or models of cars. The small inter-class
    and large intra-class variation inherent to fine-grained image analysis makes
    it a challenging problem. Capitalizing on advances in deep learning, in recent
    years we have witnessed remarkable progress in deep learning powered FGIA. In
    this paper we present a systematic survey of these advances, where we attempt
    to re-define and broaden the field of FGIA by consolidating two fundamental fine-grained
    research areas – fine-grained image recognition and fine-grained image retrieval.
    In addition, we also review other key issues of FGIA, such as publicly available
    benchmark datasets and related domain-specific applications. We conclude by highlighting
    several research directions and open problems which need further exploration from
    the community.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度图像分析（FGIA）是计算机视觉和模式识别领域中的一个长期存在且基础性的问题，并支撑着多种现实世界的应用。FGIA 任务的目标是分析来自下属类别的视觉对象，例如鸟类的不同物种或汽车的不同型号。细粒度图像分析固有的小类间变异和大类内变异，使其成为一个具有挑战性的问题。凭借深度学习的进步，近年来我们见证了基于深度学习的
    FGIA 取得了显著的进展。在本文中，我们系统地回顾了这些进展，尝试通过整合两个基础性的细粒度研究领域——细粒度图像识别和细粒度图像检索，来重新定义和拓宽
    FGIA 领域。此外，我们还审视了 FGIA 的其他关键问题，如公开可用的基准数据集和相关的领域特定应用。最后，我们强调了几个需要社区进一步探索的研究方向和未解的问题。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Fine-Grained Images Analysis; Deep Learning; Fine-Grained Image Recognition;
    Fine-Grained Image Retrieval.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度图像分析；深度学习；细粒度图像识别；细粒度图像检索。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'The human visual system is inherently capable of fine-grained image reasoning
    – we are not only able to tell a dog from a bird, but also know the difference
    between a Siberian Husky and an Alaskan Malamute (see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")).
    Fine-grained image analysis (FGIA) was introduced to the academic community for
    the very same purpose, *i.e.*, to teach machine to “see” in a fine-grained manner.
    FGIA approaches are present in a wide-range of applications in both industry and
    research, with examples including automatic biodiversity monitoring [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)], intelligent retail [[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6)], and intelligent transportation [[7](#bib.bib7), [8](#bib.bib8)],
    and have resulted in a positive impact in areas such as conservation [[9](#bib.bib9)]
    and commerce [[10](#bib.bib10)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类视觉系统天生具备细粒度图像推理能力——我们不仅能够区分狗和鸟，还能分辨西伯利亚哈士奇和阿拉斯加马拉穆特（见图[1](#S1.F1 "图 1 ‣ 1
    引言 ‣ 基于深度学习的细粒度图像分析：综述")）。细粒度图像分析（FGIA）正是为了同样的目的而引入学术界的，即教会机器以细粒度的方式“看”。FGIA 方法在工业和研究的广泛应用中都存在，例子包括自动生物多样性监测 [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]，智能零售 [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]，和智能交通 [[7](#bib.bib7),
    [8](#bib.bib8)]，并在保护 [[9](#bib.bib9)]和商业 [[10](#bib.bib10)]等领域产生了积极的影响。
- en: The goal of FGIA in computer vision is to retrieve and recognize images belonging
    to multiple subordinate categories of a super-category (*aka* a meta-category
    or a basic-level category), *e.g.*, different species of animals/plants, different
    models of cars, different kinds of retail products, etc. The key challenge therefore
    lies with understanding fine-grained visual differences that sufficiently discriminate
    between objects that are highly similar in overall appearance, but differ in *fine-grained*
    features. Great strides has been made since its inception almost two decades ago [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]. Deep learning [[14](#bib.bib14)] in particular
    has emerged as a powerful method for discriminative feature learning, and has
    led to remarkable breakthroughs in the field of FGIA. Deep learning enabled FGIA
    has greatly advanced the practical deployment of these methods in a diverse set
    of application scenarios [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [5](#bib.bib5)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: FGIA 在计算机视觉中的目标是检索和识别属于多个子类别的图像（*即*一个超类别的子类别，或称为元类别或基础级类别），*例如*，不同种类的动物/植物，不同型号的汽车，不同种类的零售产品等。因此，关键挑战在于理解细粒度的视觉差异，这些差异足以区分在整体外观上非常相似但在
    *细粒度* 特征上有所不同的对象。自近二十年前其创立以来，已经取得了重大进展 [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]。特别是深度学习 [[14](#bib.bib14)]
    作为一种强大的特征学习方法，已经在 FGIA 领域取得了显著突破。深度学习使得 FGIA 的实际应用在各种应用场景中得到了极大推进 [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [5](#bib.bib5)]。
- en: '![Refer to caption](img/92883f479697bb651a8cfc37ad6a2d82.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/92883f479697bb651a8cfc37ad6a2d82.png)'
- en: 'Figure 1: Fine-grained image analysis *vs*. generic image analysis (using visual
    classification as an example).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：细粒度图像分析 *vs*. 通用图像分析（以视觉分类为例）。
- en: '![Refer to caption](img/d7f9a195b9ca52777aa212fc79eb83b4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d7f9a195b9ca52777aa212fc79eb83b4.png)'
- en: 'Figure 2: Overview of the landscape of deep learning based fine-grained image
    analysis (FGIA), as well as future directions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度学习基础的细粒度图像分析（FGIA）领域概览，以及未来的方向。
- en: There has been significant interest in FGIA from both the computer vision and
    machine learning research communities in recent years. Rough statistics indicate
    an average of $>10$ conference papers on deep learning based FGIA are published
    every year in each of the premium vision and machine learning conferences. There
    have also been a number of special issues addressing FGIA [[15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]. Additionally, a number
    of influential competitions on FGIA are frequently held on online platforms. Representatives
    include the series of iNaturalist Competitions (for large numbers of natural species) [[20](#bib.bib20)],
    the Nature Conservancy Fisheries Monitoring (for fish species categorization) [[21](#bib.bib21)],
    Humpback Whale Identification (for whale identity categorization) [[22](#bib.bib22)],
    among others. Each competition attracted hundreds of participants from around
    the world, and some even exceeding 2,000 teams. Specific tutorials and workshops
    aimed at FGIA topics have also been held at top-tier international conferences,
    *e.g.*, [[23](#bib.bib23), [24](#bib.bib24)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Despite such salient interest, the study of FGIA with deep learning remains
    fragmented. It is therefore the purpose of this survey to (i) provide a comprehensive
    survey of recent achievements in FGIA, especially those brought by deep learning
    techniques, and more importantly (ii) to propose a unified research front by consolidating
    research from different aspects of FGIA. Our approach is significantly different
    to existing surveys [[25](#bib.bib25), [26](#bib.bib26)] that focus solely on
    the problem of fine-grained *recognition/classification*, which as we argue only
    constitutes part of the larger study of FGIA. In particular, we attempt to re-define
    and broaden the field of fine-grained image analysis, by highlighting the synergy
    between fine-grained recognition, and the parallel but complementary task of fine-grained
    image *retrieval*, which is also an integral part of FGIA.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Our survey takes a unique deep learning based perspective to review recent
    advances in FGIA in a broad, systematic, and comprehensive manner. Our main contributions
    are summarized as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We broaden the field of FGIA, by offering a consolidated landscape that promotes
    synergies between related problems in fine-grained image analysis.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We provide a comprehensive review of FGIA techniques based on deep learning,
    including commonly accepted problem definitions, benchmark datasets, different
    families of FGIA methods, along with covering domain-specific FGIA applications.
    Particularly, we organize these approaches taxonomically (see Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Fine-Grained Image Analysis with Deep Learning: A
    Survey")) to provide readers with a quick snapshot of the state-of-the-art in
    this area.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We consolidate the performance of existing methods on several publicly available
    datasets and provide discussion and insights to inform future research.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们整合了现有方法在几个公开数据集上的表现，并提供讨论和见解，以指导未来的研究。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We finish by discussing existing challenges and open issues, and identify new
    trends and future directions to provide a plausible road map for the community
    to address these problems.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们最后讨论现有的挑战和开放问题，并确定新的趋势和未来方向，以为社区提供一个可行的路线图，以应对这些问题。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Finally, in an attempt to continuously track recent developments in this fast
    advancing field, we provide an accompanying webpage which catalogs papers addressing
    FGIA problems, according to our problem-based taxonomy: http://www.weixiushen.com/project/Awesome_FGIA/Awesome_FGIA.html.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，为了持续跟踪这个快速发展的领域的最新进展，我们提供了一个附属网页，按我们基于问题的分类法对解决 FGIA 问题的论文进行分类：http://www.weixiushen.com/project/Awesome_FGIA/Awesome_FGIA.html。
- en: 2 Recognition vs. Retrieval
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 识别与检索
- en: 'Previous surveys of FGIA, *e.g.*, [[25](#bib.bib25), [26](#bib.bib26)], predominately
    focused on fine-grained recognition, and as a result do not expose all facets
    of the FGIA problem. In this survey, we cover two fundamental areas of fine-grained
    image analysis for the first time (*i.e.*, recognition and retrieval) in order
    to give a comprehensive review of recent advances in deep learning based FGIA
    techniques. In Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Fine-Grained Image
    Analysis with Deep Learning: A Survey"), we provide a new taxonomy that reflects
    the current FGIA landscape.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '之前关于 FGIA 的调查，如[[25](#bib.bib25), [26](#bib.bib26)]，主要集中在细粒度识别上，因此没有暴露 FGIA
    问题的所有方面。在本次调查中，我们首次涵盖了细粒度图像分析的两个基本领域（*即*，识别和检索），以全面回顾基于深度学习的 FGIA 技术的最新进展。在图 [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Fine-Grained Image Analysis with Deep Learning: A
    Survey")中，我们提供了一种新的分类法，反映了当前 FGIA 的格局。'
- en: 'Fine-Grained Recognition: We organize the different families of fine-grained
    *recognition* approaches into three paradigms, *i.e.*, 1) recognition by localization-classification
    subnetworks, 2) recognition by end-to-end feature encoding, and 3) recognition
    with external information. Fine-grained recognition is the most studied area in
    FGIA, since recognition is a fundamental ability of most visual systems and is
    thus worthy of long-term continuous research.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度识别：我们将不同的细粒度*识别*方法组织为三种范式，即 1) 通过定位-分类子网络进行识别，2) 通过端到端特征编码进行识别，3) 利用外部信息进行识别。细粒度识别是
    FGIA 中研究最为广泛的领域，因为识别是大多数视觉系统的基本能力，因此值得长期持续研究。
- en: 'Fine-Grained Retrieval: Based on the type of query image, we separate fine-grained
    *retrieval* methods into two groups, *i.e.*, 1) content-based fine-grained image
    retrieval and 2) sketch-based fine-grained image retrieval. Compared with fine-grained
    recognition, fine-grained retrieval is an emerging area of FGIA in recent years,
    one that is attracting more and more attention from both academia and industry.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度检索：根据查询图像的类型，我们将细粒度*检索*方法分为两组，即 1) 基于内容的细粒度图像检索和 2) 基于草图的细粒度图像检索。与细粒度识别相比，细粒度检索近年来是
    FGIA 中一个新兴领域，越来越受到学术界和工业界的关注。
- en: 'Recognition and Retrieval Differences: Both fine-grained recognition and retrieval
    aim to identify the discriminative, but subtle, differences between different
    fine-grained objects. However, fine-grained recognition is a closed-world task
    with a *fixed* number of subordinate categories. In contrast, fine-grained retrieval
    extends the problem to an open-world setting with unlimited sub-categories. Furthermore,
    fine-grained retrieval also aims to rank all the instances so that images depicting
    the concept of interest (*e.g.*, the same sub-category label) are ranked highest
    based on the fine-grained details in the query.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 识别与检索的差异：细粒度识别和检索都旨在识别不同细粒度对象之间的辨别性但微妙的差异。然而，细粒度识别是一个封闭世界任务，具有*固定*数量的从属类别。相比之下，细粒度检索将问题扩展到具有无限子类别的开放世界环境。此外，细粒度检索还旨在对所有实例进行排序，以便基于查询中的细粒度细节，将展示感兴趣概念（*例如*，相同子类别标签）的图像排名最高。
- en: 'Recognition and Retrieval Synergies: Advances in fine-grained recognition and
    retrieval have commonalities and can benefit each other. Many common techniques
    are shared by both fine-grained recognition and retrieval, *e.g.*, deep metric
    learning methods [[27](#bib.bib27), [28](#bib.bib28)], multi-modal matching methods [[29](#bib.bib29),
    [30](#bib.bib30)], and the basic ideas of selecting useful deep descriptors [[31](#bib.bib31),
    [32](#bib.bib32)], etc. Detailed discussions are elaborated in Section [7](#S7
    "7 Common Techniques Shared by both Fine-Grained Recognition and Retrieval ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"). Furthermore, in real-world applications,
    fine-grained recognition and retrieval also compliment each other, *e.g.*, retrieval
    techniques are able to support novel sub-category recognition by utilizing learned
    representations from a fine-grained recognition model [[33](#bib.bib33), [5](#bib.bib5)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '识别与检索的协同作用：细粒度识别和检索的进展有共通之处，且可以相互受益。细粒度识别和检索共享许多常见技术，如深度度量学习方法[[27](#bib.bib27),
    [28](#bib.bib28)]、多模态匹配方法[[29](#bib.bib29), [30](#bib.bib30)]以及选择有用的深度描述符的基本理念[[31](#bib.bib31),
    [32](#bib.bib32)]等。详细讨论见第[7](#S7 "7 Common Techniques Shared by both Fine-Grained
    Recognition and Retrieval ‣ Fine-Grained Image Analysis with Deep Learning: A
    Survey")节。此外，在实际应用中，细粒度识别和检索也相互补充，如检索技术能够通过利用从细粒度识别模型中学到的表示来支持新子类别识别[[33](#bib.bib33),
    [5](#bib.bib5)]。'
- en: '3 Background: Problem and Challenges'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 背景：问题与挑战
- en: 'Fine-grained image analysis (FGIA) focuses on dealing with objects belonging
    to multiple *subordinate categories* of the same meta-category (*e.g.*, different
    species of birds or different models of cars), and generally involves two central
    tasks: fine-grained image recognition and fine-grained image retrieval. As illustrated
    in Figure [3](#S3.F3 "Figure 3 ‣ 3 Background: Problem and Challenges ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"), fine-grained analysis lies in the
    continuum between basic-level category analysis (*i.e.*, generic image analysis)
    and instance-level analysis (*e.g.*, the identification of individuals).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '细粒度图像分析（FGIA）关注处理属于同一元类别的多个*下属类别*的对象（*例如*，不同鸟类的物种或不同型号的汽车），通常涉及两个核心任务：细粒度图像识别和细粒度图像检索。如图[3](#S3.F3
    "Figure 3 ‣ 3 Background: Problem and Challenges ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey")所示，细粒度分析处于基本级别类别分析（*即*，通用图像分析）和实例级别分析（*例如*，个体识别）之间的连续体上。'
- en: 'Specifically, what distinguishes FGIA from generic image analysis is that in
    generic image analysis, target objects belong to coarse-grained meta-categories
    (*i.e.*, basic-level categories) and are thus visually quite different (*e.g.*,
    determining if an image contains a bird, a fruit, or a dog). However, in FGIA,
    since objects typically come from sub-categories of the same meta-category, the
    fine-grained nature of the problem causes them to be visually similar. As an example
    of fine-grained recognition, in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Fine-Grained Image Analysis with Deep Learning: A Survey"), the task is to classify
    different breeds of dogs. For accurate image recognition, it is necessary to capture
    the subtle visual differences (*e.g.*, discriminative features such as ears, noses,
    or tails). Characterizing such features is also desirable for other FGIA tasks
    (*e.g.*, retrieval). Furthermore, as noted earlier, the fine-grained nature of
    the problem is challenging because of the *small inter-class variations* caused
    by highly similar sub-categories, and the *large intra-class variations* in poses,
    scales and rotations (see Figure [4](#S3.F4 "Figure 4 ‣ 3 Background: Problem
    and Challenges ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")).
    It is as such the opposite of generic image analysis (*i.e.*, the small intra-class
    variations and the large inter-class variations), and what makes FGIA a unique
    and challenging problem.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'While instance-level analysis typically targets a *specific instance* of an
    object not just object categories or even object sub-categories, if we move down
    the spectrum of granularity, in the extreme, individual identification (*e.g.*,
    face identification) can be viewed as a special instance of fine-grained recognition,
    where the granularity is at the individual identity level. For instance, person/vehicle
    re-identification [[34](#bib.bib34), [7](#bib.bib7)] can be considered a fine-grained
    task, which aims to determine whether two images are taken of the same specific
    person/vehicle. In practice, these works solve the corresponding domain-specific
    problems using related methods to FGIA, *e.g.*, by capturing the discriminative
    parts of objects (faces, people, and vehicles) [[35](#bib.bib35), [8](#bib.bib8),
    [36](#bib.bib36)], discovering coarse-to-fine structural information [[37](#bib.bib37)],
    developing attribute-based models [[38](#bib.bib38), [39](#bib.bib39)], and so
    on. Research in these instance-level problems is also very active. However, since
    such problems are not within the scope of classical FGIA (see Figure [3](#S3.F3
    "Figure 3 ‣ 3 Background: Problem and Challenges ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey")), for more information, we refer readers to survey
    papers of these specific topics, *e.g.*, [[34](#bib.bib34), [7](#bib.bib7), [40](#bib.bib40)].
    In the following, we start by formulating our definition of classical FGIA.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11ee2fa96655b9a399d5c60c250e7f4a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An illustration of fine-grained image analysis which lies in the
    continuum between the basic-level category analysis (*i.e.*, generic image analysis)
    and the instance-level analysis (*e.g.*, car identification).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b87dc972e883cda1297d839f00ac0222.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Key challenges of fine-grained image analysis, *i.e.*, small inter-class
    variations and large intra-class variations. Here we present four different Tern
    species from [[13](#bib.bib13)], one species per row, with different instances
    in the columns.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Formulation: In generic image recognition, we are given a training dataset
    $\mathcal{D}=\left\{\left(\bm{x}^{(n)},y^{(n)}\right)|i=1,...,N\right\}$, containing
    multiple images and associated class labels (*i.e.*, $\bm{x}$ and $y$), where
    $y\in[1,...,C]$. Each instance $\left(\bm{x},y\right)$ belongs to the joint space
    of both the image and label spaces (*i.e.*, $\mathcal{X}$ and $\mathcal{Y}$, respectively),
    according to the distribution of $p_{r}(\bm{x},y)$'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left(\bm{x},y\right)\in\mathcal{X}\times\mathcal{Y}\,.$ |  | (1) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: In particular, the label space $\mathcal{Y}$ is the union space of all the $C$
    subspaces corresponding to the $C$ categories, *i.e.*, $\mathcal{Y}=\mathcal{Y}_{1}\cup\mathcal{Y}_{2}\cup\cdots\cup\mathcal{Y}_{c}\cup\cdots\cup\mathcal{Y}_{C}$.
    Then, we can train a predictive/recognition deep network $f(\bm{x};\theta)$ parameterized
    by $\theta$ for generic image recognition by minimizing the expected risk
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathbb{E}_{(\bm{x},y)\sim p_{r}(\bm{x},y)}\left[\mathcal{L}(y,f(\bm{x};\theta))\right]\,,$
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}(\cdot,\cdot)$ is a loss function that measures the match
    between the true labels and those predicted by $f(\cdot;\theta)$. While, as aforementioned,
    fine-grained recognition aims to accurately classify instances of different subordinate
    categories from a certain meta-category, *i.e.*,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\left(\bm{x},y^{\prime}\right)\in\mathcal{X}\times\mathcal{Y}_{c}\,,$
    |  | (3) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: where $y^{\prime}$ denotes the fine-grained label and $\mathcal{Y}_{c}$ represents
    the label space of class $c$ as the meta-category. Therefore, the optimization
    objective of fine-grained recognition is as
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\mathbb{E}_{(\bm{x},y^{\prime})\sim p^{\prime}_{r}(\bm{x},y^{\prime})}\left[\mathcal{L}(y^{\prime},f(\bm{x};\theta))\right]\,.$
    |  | (4) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: Compared with fine-grained recognition, in addition to getting the sub-category
    correct, fine-grained retrieval also must rank all the instances so that images
    belonging to the same sub-category are ranked highest based on the fine-grained
    details in the query of retrieval tasks. Given an input query $\bm{x}^{q}$, the
    goal of a fine-grained retrieval system is to rank all instances in a retrieval
    set $\Omega=\{\bm{x}^{(i)}\}_{i=1}^{M}$ (whose label $y^{\prime}\in\mathcal{Y}_{c}$)
    based on their fine-grained relevance to the query. Let $\mathcal{S}_{\Omega}=\{s^{(i)}\}_{i=1}^{M}$
    represent the similarity between $\bm{x}^{q}$ and each $\bm{x}^{(i)}$ measured
    via a pre-defined metric applied to the corresponding fine-grained representations,
    *i.e.*, $h(\bm{x}^{q};\delta)$ and $h(\bm{x}^{(i)};\delta)$. Here, $\delta$ denotes
    the parameters of a retrieval model $h$. For the instances whose labels are consistent
    with the fine-grained category of $\bm{x}^{q}$, we form them into a positive set
    $\mathcal{P}_{q}$ and obtain the corresponding $\mathcal{S}_{P}$. Then, the retrieval
    model $h(\cdot;\delta)$ can be trained by maximizing the ranking based score
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\delta}\frac{\mathcal{R}(i,\mathcal{S}_{P})}{\mathcal{R}(i,\mathcal{S}_{\Omega})}\,,$
    |  | (5) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: w.r.t. all the query images, where $\mathcal{R}(i,\mathcal{S}_{P})$ and $\mathcal{R}(i,\mathcal{S}_{\Omega})$
    refer to the rankings of the instance $i$ in $\mathcal{P}_{q}$ and $\Omega$, respectively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 4 Benchmark Datasets
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, the vision community has released many fine-grained benchmark
    datasets covering diverse domains, *e.g.*, birds [[13](#bib.bib13), [41](#bib.bib41),
    [1](#bib.bib1)], dogs [[42](#bib.bib42), [27](#bib.bib27)], cars [[43](#bib.bib43)],
    airplanes [[44](#bib.bib44)], flowers [[45](#bib.bib45)], vegetables [[46](#bib.bib46)],
    fruits [[46](#bib.bib46)], foods [[47](#bib.bib47)], fashion [[38](#bib.bib38),
    [33](#bib.bib33), [6](#bib.bib6)], retail products [[5](#bib.bib5), [48](#bib.bib48)],
    etc. Additionally, it is worth noting that even the most popular large-scale image
    classification dataset, *i.e.*, ImageNet [[49](#bib.bib49)], also contains fine-grained
    classes covering a lot of dog and bird sub-categories.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99ae8d22514058d13024b92e1c502c9b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Examples of fine-grained images belonging to different species of
    flowers/vegetables [[46](#bib.bib46)], different models of cars [[43](#bib.bib43)]
    and aircraft [[44](#bib.bib44)] and different kinds of retail products [[5](#bib.bib5)].
    Accurate identification of these fine-grained objects requires the extraction
    of discriminative, but subtle, object parts or image regions. (Best viewed in
    color and zoomed in.)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Table I: Summary of popular fine-grained image datasets organized by their
    major applicable topics and sorted by their release time. Note that, “$\sharp$
    images” means the total number of images of these datasets. “BBox” indicates whether
    this dataset provides object bounding box supervisions. “Part anno.” means that
    key parts annotations are provided. “HRCHY” corresponds to hierarchical labels.
    “ATR” represents attribute labels (*e.g.*, wing color, male, female, etc). “Texts”
    indicates whether fine-grained text descriptions of images are supplied. Several
    datasets are listed here twice since they are commonly used in both recognition
    and retrieval tasks.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '| Topic | Dataset name | Year | Meta-class | $\sharp$ images | $\sharp$ categories
    | BBox | Part anno. | HRCHY | ATR | Texts |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| Recog. | Oxford Flowers [[45](#bib.bib45)] | 2008 | Flowers |     8,189 |
        102 |  |  |  |  | ✓ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| CUB200-2011 [[13](#bib.bib13)] | 2011 | Birds |   11,788 |     200 | ✓ |
    ✓ |  | ✓ | ✓ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| Stanford Dogs [[42](#bib.bib42)] | 2011 | Dogs |   20,580 |     120 | ✓ |  |  |  |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| Stanford Cars [[43](#bib.bib43)] | 2013 | Cars |   16,185 |     196 | ✓ |  |  |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| FGVC Aircraft [[44](#bib.bib44)] | 2013 | Aircrafts |   10,000 |     100
    | ✓ |  | ✓ |  |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| Birdsnap [[41](#bib.bib41)] | 2014 | Birds |   49,829 |     500 | ✓ | ✓ |  |
    ✓ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| Food101 [[47](#bib.bib47)] | 2014 | Food dishes | 101,000 |     101 |  |  |  |  |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| NABirds [[1](#bib.bib1)] | 2015 | Birds |   48,562 |     555 | ✓ | ✓ |  |  |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| Food-975 [[50](#bib.bib50)] | 2016 | Foods |   37,885 |     975 |  |  |  |
    ✓ |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| DeepFashion [[38](#bib.bib38)] | 2016 | Clothes | 800,000 |   1,050 | ✓ |
    ✓ |  | ✓ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| Fru92 [[46](#bib.bib46)] | 2017 | Fruits |   69,614 |       92 |  |  | ✓
    |  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| Veg200 [[46](#bib.bib46)] | 2017 | Vegetable |   91,117 |     200 |  |  |
    ✓ |  |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| iNat2017 [[2](#bib.bib2)] | 2017 | Plants & Animals | 857,877 |   5,089 |
    ✓ |  | ✓ |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| Dogs-in-the-Wild [[27](#bib.bib27)] | 2018 | Dogs | 299,458 |     362 |  |  |  |  |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| RPC [[5](#bib.bib5)] | 2019 | Retail products |   83,739 |     200 | ✓ |  |
    ✓ |  |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '|  | Products-10K [[48](#bib.bib48)] | 2020 | Retail products |  150,000 |
    10,000 | ✓ |  | ✓ |  |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '|  | iNat2021 [[3](#bib.bib3)] | 2021 | Plants & Animals | 3,286,843 | 10,000
    |  |  | ✓ |  |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| Retriev. | Oxford Flowers [[45](#bib.bib45)] | 2008 | Flowers |     8,189
    |     102 |  |  |  |  | ✓ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| CUB200-2011 [[13](#bib.bib13)] | 2011 | Birds |   11,788 |     200 | ✓ |
    ✓ |  | ✓ | ✓ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| Stanford Cars [[43](#bib.bib43)] | 2013 | Cars |   16,185 |     196 | ✓ |  |  |  |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| SBIR2014^∗ [[51](#bib.bib51)] | 2014 | Multiple |    1,120/7,267 |      14
    | ✓ | ✓ |  | ✓ |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| DeepFashion [[38](#bib.bib38)] | 2016 | Clothes | 800,000 |   1,050 | ✓ |
    ✓ |  | ✓ |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| QMUL-Shoe^∗ [[52](#bib.bib52)] | 2016 | Shoes | 419/419 |         1 |  |  |  |
    ✓ |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| QMUL-Chair^∗ [[52](#bib.bib52)] | 2016 | Chairs | 297/297 |         1 |  |  |  |
    ✓ |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| Sketchy^∗ [[53](#bib.bib53)] | 2016 | Multiple |    75,471/12,500 |     125
    |  |  |  |  |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| QMUL-Handbag^∗ [[54](#bib.bib54)] | 2017 | Handbags |    568/568 |         1
    |  |  |  |  |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| SBIR2017^∗ [[55](#bib.bib55)] | 2017 | Shoes |   912/304 |         1 |  |
    ✓ |  | ✓ |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| QMUL-Shoe-V2^∗ [[56](#bib.bib56)] | 2019 | Shoes |   6,730/2,000 |         1
    |  |  |  |  |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '|  | FG-Xmedia^† [[57](#bib.bib57)] | 2019 | Birds | 11,788 |     200 |  |  |  |  |
    ✓ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: ^∗ For these fine-grained sketch-based image retrieval datasets, normally they
    have sketch-and-image pairs (*i.e.*, not only images). Thus, we present the numbers
    of sketches and images separately (the numbers of sketches first). Regarding “$\sharp$
    categories”, we report the number of meta-categories in these datasets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: ^† Except for text descriptions, *FG-Xmedia* also contains multiple other modalities,
    *e.g.*, videos and audios.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/22fbae48e0e1eb6d86ace906aeb80c97.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An example image from *CUB200-2011* [[13](#bib.bib13)] with multiple
    different types of annotations *e.g.*, category label, part annotations (*aka*
    key point locations), object bounding box shown in green, attribute labels (*i.e.*,
    “ATR”), and a text description.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Representative images from some of these fine-grained benchmark datasets can
    be found in Figure [5](#S4.F5 "Figure 5 ‣ 4 Benchmark Datasets ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"). In Table [I](#S4.T1 "Table I ‣
    4 Benchmark Datasets ‣ Fine-Grained Image Analysis with Deep Learning: A Survey"),
    we summarize the most commonly used image datasets, and indicate their meta-category,
    the amount of images, the number of categories, their main task, and additional
    available supervision, *e.g.*, bounding boxes, part annotations, hierarchical
    labels, attribute labels, and text descriptions (cf. Figure [6](#S4.F6 "Figure
    6 ‣ 4 Benchmark Datasets ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")).
    These datasets have been one of the most important factors for the considerable
    progress in the field, not only as a common ground for measuring and comparing
    performance of competing approaches, but also pushing this field towards increasingly
    complex, practical, and challenging problems.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64f896df72ead86fcf61830436dcc5f9.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Chronological overview of representative deep learning based fine-grained
    recognition methods which are categorized by different learning approaches. (Best
    viewed in color.)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'The fine-grained bird classification dataset *CUB200-2011* [[13](#bib.bib13)]
    is one of the most popular fine-grained datasets. The majority of FGIA approaches
    choose it for comparisons with the state-of-the-art. Moreover, continuous contributions
    are made upon *CUB200-2011* for advanced tasks, *e.g.*, collecting text descriptions
    of the fine-grained images for multi-modal analysis, cf. [[58](#bib.bib58), [59](#bib.bib59)]
    and Section [5.3.2](#S5.SS3.SSS2 "5.3.2 Multi-Modal Data ‣ 5.3 Recognition with
    External Information ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, more challenging and practical fine-grained datasets have
    been proposed, *e.g.*, *iNat2017* containing different species of plants and animals [[2](#bib.bib2)],
    and *RPC* for retail products [[5](#bib.bib5)]. Novel properties of these datasets
    include the fact that they are large-scale, have a hierarchical structure, exhibit
    a domain gap, and form a long-tailed distribution. These challenges illustrate
    the practical requirements of FGIA in the real-world and motivate new interesting
    research challenges (cf. Section [8](#S8 "8 Future Directions ‣ Fine-Grained Image
    Analysis with Deep Learning: A Survey")).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond that, a series of fine-grained sketch-based image retrieval datasets,
    *e.g.*, *QMUL-Shoe* [[52](#bib.bib52)], *QMUL-Chair* [[52](#bib.bib52)], *QMUL-handbag* [[54](#bib.bib54)],
    *SBIR2014* [[51](#bib.bib51)], *SBIR2017* [[55](#bib.bib55)], *Sketchy* [[53](#bib.bib53)],
    *QMUL-Shoe-V2* [[56](#bib.bib56)], were constructed to further advance the development
    of fine-grained retrieval, cf. Section [6.2](#S6.SS2 "6.2 Sketch-based Fine-Grained
    Image Retrieval ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey"). Furthermore, some novel datasets and benchmarks,
    such as *FG-Xmedia* [[57](#bib.bib57)], were constructed to expand fine-grained
    image retrieval to fine-grained cross-media retrieval.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 5 Fine-Grained Image Recognition
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-grained image recognition has been by far the most active research area
    of FGIA in the past decade. Fine-grained recognition aims to discriminate numerous
    visually similar subordinate categories that belong to the same basic category,
    such as the fine distinction of animal species [[2](#bib.bib2)], cars [[43](#bib.bib43)],
    fruits [[46](#bib.bib46)], aircraft models [[44](#bib.bib44)], and so on. It has
    been frequently applied in real-world tasks, *e.g.*, ecosystem conservation (recognizing
    biological species) [[9](#bib.bib9)], intelligent retail systems [[5](#bib.bib5),
    [10](#bib.bib10)], etc. Recognizing fine-grained categories is difficult due to
    the challenges of discriminative region localization and fine-grained feature
    learning. Researchers have attempted to deal with these challenges from diverse
    perspectives. In this section, we review the main fine-grained recognition approaches
    since the advent of deep learning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly, existing fine-grained recognition approaches can be organized into
    the following three main paradigms:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛地说，现有的细粒度识别方法可以组织成以下三种主要范式：
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recognition by localization-classification subnetworks;
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于定位-分类子网络的识别；
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recognition by end-to-end feature encoding;
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于端到端特征编码的识别；
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recognition with external information.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用外部信息的识别。
- en: 'Among them, the first two paradigms restrict themselves by only utilizing the
    supervisions associated with fine-grained images, such as image labels, bounding
    boxes, part annotations, etc. To further resolve ambiguous fine-grained problems,
    there is a body of work that uses additional information such as where and when
    the image was taken [[60](#bib.bib60), [61](#bib.bib61)], web images [[62](#bib.bib62),
    [63](#bib.bib63)], or text description [[58](#bib.bib58), [59](#bib.bib59)]. In
    order to present these representative deep learning based fine-grained recognition
    methods intuitively, we show a chronological overview in Figure [7](#S4.F7 "Figure
    7 ‣ 4 Benchmark Datasets ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")
    by organizing them into the three aforementioned paradigms.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，前两种范式通过仅利用与细粒度图像相关的监督信息，如图像标签、边界框、部件注释等，来限制自己。为了进一步解决模糊的细粒度问题，有一系列工作使用了额外的信息，例如图像拍摄的时间和地点[[60](#bib.bib60),
    [61](#bib.bib61)]，网页图像[[62](#bib.bib62), [63](#bib.bib63)]，或文本描述[[58](#bib.bib58),
    [59](#bib.bib59)]。为了直观地展示这些具有代表性的深度学习基础的细粒度识别方法，我们在图[7](#S4.F7 "Figure 7 ‣ 4 Benchmark
    Datasets ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")中通过将它们组织成三种上述范式，提供了一个时间顺序的概述。'
- en: For performance evaluation, when the test set is balanced (*i.e.*, there is
    a similar number test examples from each class), the most commonly used metric
    in fine-grained recognition is classification *accuracy* across all subordinate
    categories of the datasets. It is defined as
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于性能评估，当测试集是平衡的（*即*，每个类别的测试样本数量相似）时，细粒度识别中最常用的指标是所有子类别数据集上的分类*准确率*。它被定义为
- en: '|  | ${\rm Accuracy}=\frac{&#124;I_{\rm correct}&#124;}{&#124;I_{\rm total}&#124;}\,,$
    |  | (6) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm Accuracy}=\frac{|I_{\rm correct}|}{|I_{\rm total}|}\,,$ |  | (6)
    |'
- en: where $|I_{\rm total}|$ represents the number of images across all sub-categories
    in the test set and $|I_{\rm correct}|$ represents the number of images which
    are correctly categorized by the model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|I_{\rm total}|$ 代表测试集中所有子类别的图像数量，而 $|I_{\rm correct}|$ 代表被模型正确分类的图像数量。
- en: 5.1 Recognition by Localization-Classification Subnetworks
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于定位-分类子网络的识别
- en: '![Refer to caption](img/287c40206a84295b6bbc6d74e3f893ba.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/287c40206a84295b6bbc6d74e3f893ba.png)'
- en: 'Figure 8: Illustration of the high-level pipeline of the fine-grained recognition
    by localization-classification subnetworks paradigm.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：基于定位-分类子网络范式的细粒度识别高层次流程图示。
- en: 'Researchers have attempted to create models that capture the discriminative
    semantic parts of fine-grained objects and then construct a mid-level representation
    corresponding to these parts for the final classification, cf. Figure [8](#S5.F8
    "Figure 8 ‣ 5.1 Recognition by Localization-Classification Subnetworks ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey").
    More specifically, a localization subnetwork is designed for locating key parts,
    and then the corresponding part-level (local) feature vectors are obtained. This
    is usually combined with object-level (global) image features for representing
    fine-grained objects. This is followed by a classification subnetwork which performs
    recognition. The framework of such two collaborative subnetworks forms the first
    paradigm, *i.e.*, fine-grained recognition with *localization-classification subnetworks*.
    The motivation for these models is to first find the corresponding parts and then
    compare their appearance. Concretely, it is desirable to capture semantic parts
    (*e.g.*, heads and torsos) that are shared across fine-grained categories and
    for discovering the subtle differences between these part representations.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing methods in this paradigm can be divided into four broad types: 1)
    employing detection or segmentation techniques, 2) utilizing deep filters, 3)
    leveraging attention mechanisms, and 4) other methods.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Table II: Comparative fine-grained recognition results of two learning paradigms
    (cf. Section [5.1](#S5.SS1 "5.1 Recognition by Localization-Classification Subnetworks
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey") and Section [5.2](#S5.SS2 "5.2 Recognition by End-to-End Feature Encoding
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey")) on the fine-grained benchmark datasets, *i.e.*, Birds (*CUB200-2011* [[13](#bib.bib13)]),
    Dogs (*Stanford Dogs* [[42](#bib.bib42)]), Cars (*Stanford Cars* [[43](#bib.bib43)]),
    and Aircrafts (*FGVC Aircraft* [[44](#bib.bib44)]). Note that, “Train anno.” and
    “Test anno.” mean which supervised signals used in the training and test phases,
    respectively. The symbol “–” means the results are unavailable.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Published in | Train anno. | Test anno. | Backbones | Img. resolution
    | Accuracy |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Birds | Dogs | Cars | Aircrafts |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| Fine-grained recognition by localization-classification subnetworks | Employing
    detection or segmentation techniques | Branson *et al.* [[64](#bib.bib64)] | BMVC
    2014 | BBox+Parts |  | CaffeNet | $224\times 224$ | 75.7% | – | – | – |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| PB R-CNN [[65](#bib.bib65)] | ECCV 2014 | BBox+Parts | BBox | Alex-Net |
    $224\times 224$ | 76.4% | – | – | – |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| Krause *et al.* [[66](#bib.bib66)] | CVPR 2015 | BBox |  | CaffeNet | $224\times
    224$ | 82.0% | – | 92.6% | – |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| Deep LAC [[67](#bib.bib67)] | CVPR 2015 | BBox+Parts | BBox | Alex-Net |
    $227\times 227$ | 80.3% | – | – | – |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| PS-CNN [[68](#bib.bib68)] | CVPR 2016 | BBox+Parts | BBox | CaffeNet | $227\times
    227$ | 76.6% | – | – | – |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| SPDA-CNN [[69](#bib.bib69)] | CVPR 2016 | BBox+Parts | BBox | CaffeNet |
    Longer side to 800px | 81.0% | – | – | – |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| SPDA-CNN [[69](#bib.bib69)] | CVPR 2016 | BBox+Parts | BBox | VGG-16 | Longer
    side to 800px | 85.1% | – | – | – |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[70](#bib.bib70)] | IEEE TIP 2016 |  |  | Alex-Net | $224\times
    224$ | 78.9% | 80.4% | – | – |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| HSnet [[71](#bib.bib71)] | CVPR 2017 | Parts |  | GoogLeNet | $224\times
    224$ | 87.5% | – | 93.9% | – |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| TSC [[72](#bib.bib72)] | AAAI 2017 |  |  | VGG-16 | Not given | 84.7% | –
    | – | – |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| Mask-CNN [[31](#bib.bib31)] | PR 2018 | Parts |  | VGG-16 | $448\times 448$
    | 85.7% | – | – | – |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| Ge *et al.* [[73](#bib.bib73)] | CVPR 2019 |  |  | GoogLeNet+BN | Shorter
    side to 800px | 90.3% | 93.9% | – | – |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| GCL [[74](#bib.bib74)] | AAAI 2020 |  |  | ResNet-50+BN | $448\times 448$
    | 88.3% | – | 94.0% | 93.2% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| FDL [[75](#bib.bib75)] | AAAI 2020 |  |  | ResNet-50 | $448\times 448$ |
    88.6% | 85.0% | 94.3% | 93.4% |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| Utilizing deep filters | Two-level atten. [[76](#bib.bib76)] | CVPR 2015
    |  |  | VGG-16 | Not given | 77.9% | – | – | – |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| CL [[77](#bib.bib77)] | CVPR 2015 |  |  | Alex-Net | $448\times 448$ | 73.5%
    | – | – | – |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| NAC [[78](#bib.bib78)] | ICCV 2015 |  |  | VGG-19 | Not given | 81.0% | –
    | – | – |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| PDFS [[79](#bib.bib79)] | CVPR 2016 |  |  | VGG-16 | Not given | 84.5% |
    72.0% | – | – |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| DFL-CNN [[80](#bib.bib80)] | CVPR 2018 |  |  | VGG-16 | $448\times 448$ |
    86.7% | – | 93.8% | 92.0% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| S3N [[81](#bib.bib81)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$ |
    88.5% | – | 94.7% | 92.8% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| Huang *et al.* [[82](#bib.bib82)] | CVPR 2020 |  |  | ResNet-101 | Shorter
    side to 448px | 87.3% | – | – | – |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Attention mechanisms | RA-CNN [[83](#bib.bib83)] | CVPR 2017 |  |  | VGG-19
    | $448\times 448$ | 85.3% | 87.3% | 92.5% | – |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| MA-CNN [[84](#bib.bib84)] | ICCV 2017 |  |  | VGG-19 | $448\times 448$ |
    86.5% | – | 92.8% | 89.9% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Liu *et al.* [[39](#bib.bib39)] | AAAI 2017 | Parts+Attr. |  | ResNet-50
    | $448\times 448$ | 85.4% | – | – | – |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| Sun *et al.* [[27](#bib.bib27)] | ECCV 2018 |  |  | ResNet-50 | $448\times
    448$ | 86.5% | 84.8% | 93.0% | – |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| OPAM [[85](#bib.bib85)] | IEEE TIP 2018 |  |  | VGG-16 | Not given | 85.8%
    | – | 92.2% | – |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| MGE-CNN [[86](#bib.bib86)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$
    | 88.5% | – | 93.9% | – |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| TASN [[87](#bib.bib87)] | CVPR 2019 |  |  | ResNet-50 | $224\times 224$ |
    87.9% | – | 93.8% | – |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| PA-CNN [[88](#bib.bib88)] | IEEE TIP 2020 |  |  | VGG-19 | $448\times 448$
    | 87.8% | – | 93.3% | 91.0% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| Ji *et al.* [[89](#bib.bib89)] | CVPR 2020 |  |  | ResNet-50 | $448\times
    448$ | 88.1% | – | 94.6% | 92.4% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| Others | STN [[90](#bib.bib90)] | NeurIPS 2015 |  |  | GoogLeNet+BN | $448\times
    448$ | 84.1% | – | – | – |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| BoT [[91](#bib.bib91)] | CVPR 2016 |  |  | Alex-Net | Not given | – | – |
    92.5% | 88.4% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| NTS-Net [[92](#bib.bib92)] | ECCV 2018 |  |  | ResNet-50 | $448\times 448$
    | 87.5% | – | 93.9% | 91.4% |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| M2DRL [[93](#bib.bib93)] | IJCV 2019 |  |  | VGG-16 | $448\times 448$ | 87.2%
    | – | 93.3% | – |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| DF-GMM [[94](#bib.bib94)] | CVPR 2020 |  |  | ResNet-50 | $448\times 448$
    | 88.8% | – | 94.8% | 93.8% |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Fine-grained recognition by end-to-end feature encoding | High-order feature
    interactions | Bilinear CNN [[95](#bib.bib95)] | ICCV 2015 |  |  | VGG-16+VGG-M
    | $448\times 448$ | 84.1% | – | 91.3% | 84.1% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| C-BCNN [[96](#bib.bib96)] | CVPR 2016 |  |  | VGG-16 | $448\times 448$ |
    84.3% | – | 91.2% | 84.1% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| KP [[97](#bib.bib97)] | CVPR 2017 |  |  | VGG-16 | $224\times 224$ | 86.2%
    | – | 92.4% | 86.9% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| LRBP [[98](#bib.bib98)] | CVPR 2017 |  |  | VGG-16 | $224\times 224$ | 84.2%
    | – | 90.0% | 87.3% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| G²DeNet [[99](#bib.bib99)] | CVPR 2017 |  |  | VGG-16 | Longer side to 200px
    | 87.1% | – | 92.5% | 89.0% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Cai *et al.* [[100](#bib.bib100)] | ICCV 2017 |  |  | VGG-16 | $448\times
    448$ | 85.3% | – | 91.7% | 88.3% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| iSQRT-COV [[101](#bib.bib101)] | CVPR 2018 |  |  | ResNet-101 | $224\times
    224$ | 88.7% | – | 93.3% | 91.4% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| DeepKSPD [[102](#bib.bib102)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$
    | 86.5% | – | 93.2% | 91.0% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| HBP [[103](#bib.bib103)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$ | 87.1%
    | – | 93.7% | 90.3% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| GP [[104](#bib.bib104)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$ | 85.8%
    | – | 92.8% | 89.8% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| DBTNet-50 [[105](#bib.bib105)] | NeurIPS 2019 |  |  | VGG-16 | $448\times
    448$ | 87.5% | – | 94.1% | 91.2% |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '|  | MOMN [[106](#bib.bib106)] | IEEE TIP 2020 |  |  | VGG-16 | $448\times
    448$ | 87.3% | – | 92.8% | 90.4% |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| Specific loss functions | MaxEnt [[107](#bib.bib107)] | NeurIPS 2018 |  |  |
    VGG-16 | $224\times 224$ | 77.0% | 65.4% | 83.9% | 78.1% |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| MaxEnt [[107](#bib.bib107)] | NeurIPS 2018 |  |  | Bilinear CNN | $224\times
    224$ | 85.3% | 83.2% | 92.8% | 86.1% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| PC [[108](#bib.bib108)] | ECCV 2018 |  |  | Bilinear CNN | $224\times 224$
    | 85.6% | 83.0% | 92.4% | 85.7% |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Sun *et al.* [[27](#bib.bib27)] | ECCV 2018 |  |  | ResNet-50 | $448\times
    448$ | 86.5% | 84.8% | 93.0% | – |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| CIN [[109](#bib.bib109)] | AAAI 2020 |  |  | ResNet-101 | $448\times 448$
    | 88.1% | – | 94.5% | 92.8% |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| Sun *et al.* [[110](#bib.bib110)] | AAAI 2020 |  |  | ResNet-50 | $448\times
    448$ | 88.6% | 87.7% | 94.9% | 93.5% |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| API-Net [[111](#bib.bib111)] | AAAI 2020 |  |  | ResNet-50 | $448\times 448$
    | 87.7% | 88.3% | 94.8% | 93.0% |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| API-Net [[111](#bib.bib111)] | AAAI 2020 |  |  | DenseNet-161 | $448\times
    448$ | 90.0% | 89.4% | 95.3% | 93.9% |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| MC-Loss [[112](#bib.bib112)] | IEEE TIP 2020 |  |  | Bilinear CNN | $448\times
    448$ | 86.4% | – | 94.4% | 92.9% |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| Others | BGL [[50](#bib.bib50)] | CVPR 2016 |  |  | VGG-16 | $224\times 224$
    | 75.9% | – | 86.0% | – |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| BGL [[50](#bib.bib50)] | CVPR 2016 | BBox |  | VGG-16 | $224\times 224$ |
    80.4% | – | 90.5% | – |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| DCL [[113](#bib.bib113)] | CVPR 2019 |  |  | ResNet-50 | $448\times 448$
    | 87.8% | – | 94.5% | 93.0% |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| Cross-X [[114](#bib.bib114)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$
    | 87.7% | 88.9% | 94.6% | 92.6% |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '|  | PMG [[115](#bib.bib115)] | ECCV 2020 |  |  | ResNet-50 | $448\times 448$
    | 89.6% | – | 95.1% | 93.4% |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: 5.1.1 Employing Detection or Segmentation Techniques
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is straightforward to employ detection or segmentation techniques [[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118)] to locate key image regions corresponding
    to fine-grained object parts, *e.g.*, bird heads, bird tails, car lights, dog
    ears, dog torsos, etc. Thanks to localization information, *i.e.*, part-level
    bounding boxes or segmentation masks, the model can obtain more discriminative
    mid-level (part-level) representations w.r.t. these parts. Thus, it could further
    enhance the learning capability of the classification subnetwork, thus significantly
    boost the final recognition accuracy.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier works in this paradigm made use of additional dense part annotations
    (*aka* key point localization, cf. Figure [6](#S4.F6 "Figure 6 ‣ 4 Benchmark Datasets
    ‣ Fine-Grained Image Analysis with Deep Learning: A Survey") on the left) to locate
    semantic key parts of objects. For example, Branson *et al.* [[64](#bib.bib64)]
    proposed to use groups of detected part keypoints to compute multiple warped image
    regions and further obtained the corresponding part-level features by pose normalization.
    In the same period, Zhang *et al.* [[65](#bib.bib65)] first generated part-level
    bounding boxes based on ground truth part annotations, and then trained a R-CNN [[118](#bib.bib118)]
    model to perform part detection. Di *et al.* [[67](#bib.bib67)] further proposed
    a Valve Linkage Function, which not only connected all subnetworks, but also refined
    localization according to the *part alignment* results. In order to integrate
    both semantic part detection and abstraction, SPDA-CNN [[69](#bib.bib69)] designed
    a top-down method to generate *part-level* proposals by inheriting prior geometric
    constraints and then used a Faster R-CNN [[116](#bib.bib116)] to return part localization
    predictions. Other approaches made use of segmentation information. PS-CNN [[68](#bib.bib68)]
    and Mask-CNN [[31](#bib.bib31)] employed segmentation models to get part/object
    masks to aid part/object localization. Compared with detection techniques, segmentation
    can result in more accurate part localization [[31](#bib.bib31)] as segmentation
    focuses on the finer pixel-level targets, instead of just coarse bounding boxes.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'However, employing traditional detectors or segmentation models requires dense
    part annotations for training, which is labor-intensive and would limit both scalability
    and practicality of real-world fine-grained applications. Therefore, it is desirable
    to accurately locate fine-grained parts by only using image level labels [[70](#bib.bib70),
    [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75)]. These
    set of approaches are referred to as “weakly-supervised” as they only use image
    level labels. It is interesting to note that since 2016 there is an apparent trend
    in developing fine-grained methods in this weakly-supervised setting, rather than
    the strong-supervised setting (*i.e.*, using part annotations and bounding boxes),
    cf. Table [II](#S5.T2 "Table II ‣ 5.1 Recognition by Localization-Classification
    Subnetworks ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with
    Deep Learning: A Survey").'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Recognition methods in the weakly-supervised localization based classification
    setting always rely on unsupervised approaches to obtain semantic groups which
    correspond to object parts. Specifically, Zhang *et al.* [[70](#bib.bib70)] adopted
    the spatial pyramid strategy [[119](#bib.bib119)] to generate part proposals from
    object proposals. Then, by using a clustering approach, they generated part proposal
    prototype clusters and further selected useful clusters to get discriminative
    part-level features. Co-segmentation [[120](#bib.bib120)] based methods are also
    commonly used in this weakly supervised case. One approach is to use co-segmentation
    to obtain object masks without supervision, and then perform heuristic strategies,
    *e.g.*, part constraints [[72](#bib.bib72)] or part alignment [[66](#bib.bib66)],
    to locate fine-grained parts.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the majority of previous work overlooks the internal
    semantic correlation among discriminative part-level features. Concretely, the
    aforementioned methods pick out the discriminative regions independently and utilize
    their features directly, while neglecting the fact that an object’s features are
    mutually semantic correlated and region groups can be more discriminative. Therefore,
    very recently, some methods attempt to jointly learn the interdependencies among
    part-level features to obtain more universal and powerful fine-grained image representations.
    By performing different feature fusion strategies (*e.g.*, LSTMs [[71](#bib.bib71),
    [73](#bib.bib73)], graphs [[74](#bib.bib74)], or knowledge distilling [[75](#bib.bib75)])
    these joint part feature learning methods yield significantly higher recognition
    accuracy over previous independent part feature learning methods.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Utilizing Deep Filters
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In deep convolutional neural networks (CNNs), deep filters (*i.e.*, CNN filters)
    refer to the learned weights of the convolution layers [[14](#bib.bib14)]. The
    responses/activations from these deep filters can be viewed as localized descriptors.
    The deep descriptors have the following properties [[121](#bib.bib121)]: 1) Locality:
    they describe and correspond to local image regions w.r.t. the whole input image
    and 2) Spatiality: they are also able to encode spatial information. As works
    started exploring the use of CNNs in computer vision, researchers gradually discovered
    that intermediate CNN outputs (*e.g.*, local deep descriptors) could be linked
    to semantic parts of common objects [[122](#bib.bib122)]. Therefore, the fine-grained
    community attempted to employ these filter outputs as part detectors [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82)], and thus rely on them to conduct localization-classification
    fine-grained recognition. One of the main advantages here is that this does not
    require any part-level annotations.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Xiao *et al.* [[76](#bib.bib76)] performed spectral clustering [[123](#bib.bib123)]
    on these deep filters to form groups and then used the filter groups to serve
    as part detectors. Similarly, NAC [[78](#bib.bib78)] exploits the channels of
    a CNN as part detectors. Liu *et al.* [[77](#bib.bib77)] developed a cross-layer
    pooling method by aggregating the part-based deep descriptors using activations
    from two successive convolutional layers as guidance. PDFS [[79](#bib.bib79)]
    was proposed to select deep filters corresponding to parts and then iteratively
    update the learned “detectors” which resulted in the discovery of discriminative
    and consistent part-based image regions. For these aforementioned methods, after
    obtaining detected parts using deep filters from pre-trained classification CNNs,
    they typically trained off-line classifiers, *e.g.*, SVMs or decision trees [[123](#bib.bib123)],
    using the part-based feature vectors to conduct the final recognition task.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate the learning of both part detection and part-based classification,
    unified end-to-end trained fine-grained models [[80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82)] were developed. As a result, significant recognition improvements
    were observed, cf. Table [II](#S5.T2 "Table II ‣ 5.1 Recognition by Localization-Classification
    Subnetworks ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with
    Deep Learning: A Survey"). Wang *et al.* [[80](#bib.bib80)] utilized an additional
    learnable $1\times 1$ convolutional filter as a small patch (*i.e.*, part) detector.
    This was followed by a global max-pooling to keep the highest activations w.r.t.
    that filter for the final classification. Later, based on class response maps [[124](#bib.bib124)],
    S3N [[81](#bib.bib81)] leveraged the class peak responses, *i.e.*, local maximums,
    as the basis of part localization. Similar to [[71](#bib.bib71), [73](#bib.bib73),
    [74](#bib.bib74)], S3N also considers part correlations in a mutual part learning
    way.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Leveraging Attention Mechanisms
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even though the previous localization-classification fine-grained methods have
    shown strong classification performance, one of their major drawbacks is that
    they require meaningful definitions of the object parts. In many applications
    however, it may be hard to represent or even define common parts of some object
    classes, *e.g.*, non-structured objects like food dishes [[47](#bib.bib47)] or
    flowers with repeating parts [[45](#bib.bib45)]. Compared to these localization-classification
    methods, a more natural solution of finding parts is to leverage attention mechanisms [[125](#bib.bib125)]
    as sub-modules. This enables CNNs to attend to loosely defined regions for fine-grained
    objects and as a result have emerged as a promising direction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'It is common knowledge that attention plays an important role in human perception [[125](#bib.bib125),
    [126](#bib.bib126)]. Humans exploit a sequence of partial glimpses and selectively
    focus on salient parts of an object or a scene in order to better capture visual
    structure [[127](#bib.bib127)]. Inspired by this, Fu *et al.* and Zheng *et al.* [[83](#bib.bib83),
    [84](#bib.bib84)] were the first to incorporate attention processing to improve
    the fine-grained recognition accuracy of CNNs. Specifically, RA-CNN [[83](#bib.bib83)]
    uses a recurrent visual attention model to select a sequence of attention regions
    (corresponding to object “parts”¹¹1Note that here “parts” refers to the loosely
    defined attention regions for fine-grained objects, which is different from the
    clearly defined object parts from manual annotations, cf. Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Employing Detection or Segmentation Techniques ‣ 5.1 Recognition by Localization-Classification
    Subnetworks ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with
    Deep Learning: A Survey").). RA-CNN iteratively generates region attention maps
    in a coarse to fine fashion by taking previous predictions as a reference. MA-CNN [[84](#bib.bib84)]
    is equipped with a multi-attention CNN, and can return multiple region attentions
    in parallel. Subsequently, Peng *et al.* [[85](#bib.bib85)] and Zheng *et al.* [[88](#bib.bib88)]
    proposed multi-level attention models to obtain hierarchical attention information
    (*i.e.*, both object- and part-level). He *et al.* [[128](#bib.bib128)] applied
    multi-level attention to localize multiple discriminative regions simultaneously
    for each image via an $n$-pathway end-to-end discriminative localization network
    that simultaneously localizes discriminative regions and encodes their features.
    This multi-level attention can result in diverse and complementary information
    compared to the aforementioned single-level attention methods. Sun *et al.* [[27](#bib.bib27)]
    incorporated channel attentions [[129](#bib.bib129)] and metric learning [[130](#bib.bib130)]
    to enforce the correlations among different attended regions. Zheng *et al.* [[87](#bib.bib87)]
    developed a trilinear attention sampling network to learn fine-grained details
    from hundreds of part proposals and efficiently distill the learned features into
    a single CNN. Recently, Ji *et al.* [[89](#bib.bib89)] presented an attention
    based convolutional binary neural tree, which incorporates attention mechanisms
    with a tree structure to facilitate coarse-to-fine hierarchical fine-grained feature
    learning. Although the attention mechanism achieves strong accuracy in fine-grained
    recognition, it tends to overfit in the case of small-scale data.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4 Other Methods
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many other approaches in the localization-classification paradigm have also
    been proposed for fine-grained recognition. Spatial Transformer Networks (STN) [[90](#bib.bib90)]
    were originally introduced to explicitly perform spatial transformations in an
    end-to-end learnable way. They can also be equipped with multiple transformers
    in parallel to conduct fine-grained recognition. Each transformer in an STN can
    correspond to a part detector with spatial transformation capabilities. Later,
    Wang *et al.* [[91](#bib.bib91)] developed a triplet of patches with geometric
    constraints as a template to automatically mine discriminative triplets and then
    generated mid-level representations for classification with the mined triplets.
    In addition, other methods have achieved better accuracy by introducing feedback
    mechanisms. Specifically, NTS-Net [[92](#bib.bib92)] employs a multi-agent cooperative
    learning scheme to address the core problem of fine-grained recognition, *i.e.*,
    accurately identifying informative regions in an image. M2DRL [[131](#bib.bib131),
    [93](#bib.bib93)] was the first to utilize deep reinforcement learning [[132](#bib.bib132)]
    at both the object- and part-level to capture multi-granularity discriminative
    localization and multi-scale representations using their tailored reward functions.
    Inspired by low-rank mechanisms in natural language processing [[133](#bib.bib133)],
    Wang *et al.* [[94](#bib.bib94)] proposed the DF-GMM framework to alleviate the
    region diffusion problem in high-level feature maps for fine-grained part localization.
    DF-GMM first selects discriminative regions from the high-level feature maps by
    constructing low-rank bases, and then applies spatial information of the low-rank
    bases to reconstruct low-rank feature maps. Part correlations can also be modeled
    by reorganization processing, which brings accuracy improvements.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Recognition by End-to-End Feature Encoding
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second learning paradigm of fine-grained recognition is *end-to-end feature
    encoding*. As with other vision tasks, feature learning also plays a fundamental
    role in fine-grained recognition. Since the differences between sub-categories
    are typically very subtle and local, capturing global semantic information using
    only fully connected layers limits the representation capacity of a fine-grained
    model, and hence restricts further improvements in final recognition performance.
    Therefore, methods have been developed that aim to learn a unified, yet discriminative,
    image representation for modeling subtle differences between fine-grained categories
    in the following ways: 1) by performing high-order feature interactions, 2) by
    designing novel loss functions, and 3) through other means.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Performing High-Order Feature Interactions
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Feature learning plays a crucial role in almost all vision tasks such as retrieval,
    detection, tracking, etc. The success of deep convolutional networks is mainly
    due to the learned discriminative deep features. In the initial era of deep learning,
    the features (*i.e.*, activations) of fully connected layers were commonly used
    as image representations. Later, the feature maps of deeper convolutional layers
    were discovered to contain mid- and high-level information, *e.g.*, object parts
    or complete objects [[134](#bib.bib134)], which led to the widespread use of convolutional
    features/descriptors [[77](#bib.bib77), [135](#bib.bib135)], cf. Figure [9](#S5.F9
    "Figure 9 ‣ 5.2.1 Performing High-Order Feature Interactions ‣ 5.2 Recognition
    by End-to-End Feature Encoding ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"). Additionally, applying encoding
    techniques for these local convolutional descriptors has resulted in significant
    improvements compared with fully-connected outputs [[136](#bib.bib136), [135](#bib.bib135),
    [137](#bib.bib137)]. To some extent, these improvements in encoding techniques
    come from the *higher-order* statistics encoded in the final features. Particularly
    for fine-grained recognition, where the need for end-to-end modeling of higher-order
    statistics became evident when the Fisher Vector encodings of SIFT features outperformed
    a fine-tuned AlexNet in several fine-grained tasks [[138](#bib.bib138)].'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9227fb49f16b8478afb4f20e82b66c9a.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Illustration of feature maps and deep descriptors in CNNs.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The covariance matrix based representation [[139](#bib.bib139), [140](#bib.bib140)]
    is a representative higher-order (*i.e.*, second-order) feature interaction technique,
    which has been used in computer vision and machine learning. Let $\bm{V}_{d\times
    n}=\left[\bm{v}_{1},\bm{v}_{2},\ldots,\bm{v}_{n}\right]$ denote a data matrix,
    in which each column contains a local descriptor $\bm{v}_{i}\in\mathcal{R}^{d}$,
    extracted from an image. The corresponding $d\times d$ sample covariance matrix
    over $\bm{V}$ is denoted as $\bm{\Sigma}=\bar{\bm{V}}\bar{\bm{V}}^{\top}$ (or
    simply ${\bm{V}}{\bm{V}}^{\top}$), where $\bar{\bm{V}}$ denotes the centered $\bm{V}$.
    Originally, this covariance matrix is proposed as a region descriptor, *e.g.*,
    characterizing the covariance of the color intensities of pixels in an image patch.
    In recent years, it has been used as a promising second-order pooled image representation
    for visual recognition.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: By integrating the covariance matrix based representation with deep descriptors,
    a series of methods showed promising accuracy in fine-grained recognition in the
    past few years. The most representative method among them is Bilinear CNNs [[95](#bib.bib95),
    [141](#bib.bib141)], which represents an image as a pooled outer product of features
    derived from two deep CNNs, and thus encodes second-order statistics of convolutional
    activations, resulting in clear improvements in fine-grained recognition. This
    outer product essentially leads to a covariance matrix (in the form of $\bm{V}\bm{V}^{\top}$)
    when the two CNNs are set as the same. However, the outer product operation results
    in extremely high dimensional features, *i.e.*, the bilinear pooled feature is
    reshaped into a vector $\bm{z}={\rm vec}(\bm{V}\bm{V}^{\top})\in\mathcal{R}^{d^{2}}$.
    This results in a large increase in the number of parameters in the classification
    module of the deep network, which can cause overfitting and make it impractical
    for realistic applications, especially for large-scale ones. To address this problem,
    Gao *et al.* [[96](#bib.bib96)] applied Tensor Sketch [[142](#bib.bib142)] to
    both approximate the second-order statistics of the original bilinear pooling
    operation and reduce feature dimensions. Kong *et al.* [[98](#bib.bib98)] adopted
    a low-rank approximation to the covariance matrix and further learned a low-rank
    bilinear classifier. The resulting classifier can be evaluated without explicitly
    computing the bilinear feature matrix which results in a large reduction on the
    parameter size. Li *et al.* [[143](#bib.bib143)] also modeled pairwise feature
    interaction by performing a quadratic transformation with a low-rank constraint.
    Yu *et al.* [[103](#bib.bib103)] used a dimension reduction projection before
    bilinear pooling to alleviate dimension explosion. Zheng *et al.* [[105](#bib.bib105)]
    applied bilinear pooling to feature channel groups where the bilinear transformation
    is represented by calculating pairwise interactions within each group. This also
    results in large saving in computation cost.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these approaches, some methods attempt to capture much higher-order (more
    than second-order) interactions of features to generate stronger and more discriminative
    feature representations. Cui *et al.* [[97](#bib.bib97)] introduced a kernel pooling
    method that captures arbitrarily ordered and non-linear features via compact feature
    mapping. Cai *et al.* [[100](#bib.bib100)] proposed a polynomial kernel based
    predictor to model higher-order statistics of convolutional activations across
    multiple layers for modeling part interactions. Subsequently, DeepKSPD [[102](#bib.bib102)]
    was developed to jointly learn the deep local descriptors and the kernel-matrix
    based covariance representation in an end-to-end trainable manner.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: As $\ell_{2}$ feature normalization can suppress the common patterns of high
    response and thereby enhance those discriminative features (*i.e.*, the visual
    burstiness problem [[104](#bib.bib104), [144](#bib.bib144)]), the aforementioned
    bilinear pooling based methods typically perform element-wise square root normalization
    followed by $\ell_{2}$-normalization on covariance matrix to improve performance.
    However, merely employing $\ell_{2}$-normalization can cause unstable high-order
    information and also lead to slow convergence. To this end, many methods have
    explored non-linearly scaling based on the singular value decomposition (SVD)
    or eigendecomposition (EIG) to obtain more stability for second-order representations.
    Specifically, Li *et al.* [[145](#bib.bib145)] proposed to apply the power exponent
    to the eigenvalues of bilinear features to achieve better recognition accuracy.
    G²DeNet [[99](#bib.bib99)] further combined complementary first-order and second-order
    information via a Gaussian embedding and matrix square root normalization. iSQRT-COV [[101](#bib.bib101)]
    and the improved B-CNN [[146](#bib.bib146)] used the Newton-Schulz iteration to
    approximate matrix square-root normalization with only matrix multiplication to
    decrease training time. Recently, MOMN [[106](#bib.bib106)] was proposed to simultaneously
    normalize a bilinear representation in terms of square-root, low-rank, and sparsity
    all within a multi-objective optimization framework.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Designing Specific Loss Functions
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Loss functions play an important role in the construction of deep networks.
    They can directly affect both the learned classifiers and features. Thus, designing
    fine-grained tailored loss functions is an important direction for fine-grained
    image recognition.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Distinct from generic image recognition, in fine-grained classification, where
    samples that belong to different classes can be visually very similar, it is reasonable
    to prevent the classifier from being too confident in its outputs (*i.e.*, discourage
    low entropy). Following this intuition, [[107](#bib.bib107)] also maximized the
    entropy of the output probability distribution when training networks on fine-grained
    tasks. Similarly, Dubey *et al.* [[108](#bib.bib108)] used a pairwise confusion
    optimization procedure to solve both overfitting and sample-specific artifacts
    in fine-grained recognition by bringing the different class-conditional probability
    distributions closer together and confusing the deep network. This allows a reduction
    in prediction over-confidence, therefore resulting in improved generalization
    performance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Humans can effectively identify contrastive clues by comparing image pairs,
    and this type of metric / contrastive learning is also common in fine-grained
    recognition. Specifically, Sun *et al.* [[27](#bib.bib27)] first learned multiple
    part-corresponding attention regions and then leveraged metric learning for pulling
    same-attention same-class features closer, while pushing different-attention or
    different-class features away. Furthermore, their approach can coherently enforce
    the correlations among different object parts during training. CIN [[109](#bib.bib109)]
    pulls positive pairs closer while pushing negative pairs away via a contrastive
    channel interaction module which also exploits channel correlations between samples.
    API-Net [[111](#bib.bib111)] was also built upon a metric learning framework,
    and can adaptively discover contrastive cues from a pair of images and distinguish
    them via pairwise attention based interactions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Designing a single loss function for localizing part-level patterns and further
    aggregating image-level representations has also been explored in the literature.
    Specifically, Sun *et al.* [[110](#bib.bib110)] developed a gradient-boosting
    based loss function along with a diversification block to force the network to
    move swiftly to discriminate the hard classes. Concretely, the diversification
    block suppresses the discriminative regions of the class activation maps, and
    hence the network is forced to find alternative informative features. The gradient-booting
    loss focuses on difficult (*i.e.*, confusing) classes for each image and boosts
    their gradient. MC-Loss [[112](#bib.bib112)] encourages the feature channels to
    be more discriminative by focusing on various part-level regions. They propose
    a single loss that does not require any specific network modifications for partial
    localization of fine-grained objects.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: These aforementioned loss function based fine-grained recognition methods are
    backbone-agnostic and their performance can typically be improved by using more
    powerful backbone network architectures.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Other Methods
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Beyond modelling the interactions between higher-order features and designing
    novel loss functions, another set of approaches involve constructing fine-grained
    tailored auxiliary tasks for obtaining unified and discriminative image representations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: BGL [[50](#bib.bib50)] was proposed to incorporate rich bipartite-graph labels
    into CNN training to model the important relationships among fine-grained classes.
    DCL [[113](#bib.bib113)] performed a “destruction and construction” process to
    enhance the difficulty of recognition to guide the network to focus on discriminative
    parts for fine-grained recognition (*i.e.*, by destruction learning) and then
    model the semantic correlation among parts of the object (*i.e.*, by construction
    learning). Similar to DCL, Du *et al.* [[115](#bib.bib115)] tackled fine-grained
    representation learning using a jigsaw puzzle generator proxy task to encourage
    the network to learn at different levels of granularity and simultaneously fuse
    features at these levels together. Recently, a more direct fine-grained feature
    learning method [[147](#bib.bib147)] was formulated with the goal of generating
    *identity-preserved* fine-grained images in an adversarial learning manner to
    directly obtain a unified fine-grained image representation. The authors showed
    that this direct feature learning approach not only preserved the identity of
    the generated images, but also significantly boosted the visual recognition performance
    in other challenging tasks like fine-grained few-shot learning [[148](#bib.bib148)].
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Recognition with External Information
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table III: Comparison of fine-grained “recognition with external information”
    (cf. Section [5.3](#S5.SS3 "5.3 Recognition with External Information ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey"))
    on multiple fine-grained benchmark datasets, including Birds (*CUB200-2011* [[13](#bib.bib13)]),
    Dogs (*Stanford Dogs* [[42](#bib.bib42)]), Cars (*Stanford Cars* [[43](#bib.bib43)]),
    and Aircrafts (*FGVC Aircraft* [[44](#bib.bib44)]). “External info.” denotes which
    kind of external information is used by the respective approach. “Train anno.”
    and “Test anno.” indicate the supervision used during training and testing, and
    “–” means the results are unavailable.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Published in | Train anno. | Test anno. | External info. | Backbones
    | Img. resolution | Accuracy |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| Birds | Dogs | Cars | Aircrafts |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| Fine-grained recognition with external information | With web / auxiliary
    data | HAR-CNN [[149](#bib.bib149)] | CVPR 2015 | BBox | BBox | Web data | Alex-Net
    | $224\times 224$ | – | 49.4% | 80.8% | – |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| Xu *et al.* [[150](#bib.bib150)] | ICCV 2015 | BBox+Parts |  | Web data |
    CaffeNet | $224\times 224$ | 84.6% | – | – | – |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| Krause *et al.* [[151](#bib.bib151)] | ECCV 2016 |  |  | Web data | Inception-v3
    | $224\times 224$ | 92.3% | 80.8% | – | 93.4% |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| Niu *et al.* [[152](#bib.bib152)] | CVPR 2018 |  |  | Web data | VGG-16 |
    $224\times 224$ | 76.5% | 85.2% | – | – |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| MetaFGNet [[153](#bib.bib153)] | ECCV 2018 |  |  | Auxiliary data | ResNet-34
    | $224\times 224$ | 87.6% | 96.7% | – | – |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| Xu *et al.* [[154](#bib.bib154)] | IEEE TPAMI 2018 | BBox+Parts |  | Web
    data | Alex-Net | $224\times 224$ | 84.6% | – | – | – |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| Yang *et al.* [[155](#bib.bib155)] | IEEE TIP 2018 |  |  | Web data | ResNet-50
    | $224\times 224$ | – | 87.4% | – | – |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| Sun *et al.* [[62](#bib.bib62)] | AAAI 2019 |  |  | Web data | ResNet-50
    | $224\times 224$ | – | 87.1% | – | – |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '|  | Zhang *et al.* [[156](#bib.bib156)] | AAAI 2020 |  |  | Web data | VGG-16
    | $224\times 224$ | 77.2% | – | 78.7% | 72.9% |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| With multi-modal data | CVL [[59](#bib.bib59)] | CVPR 2017 |  |  | Language
    texts | VGG-16 | Not given | 85.6% | – | – | – |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[157](#bib.bib157)] | AAAI 2018 | BBox |  | Audio | VGG-16
    | $227\times 227$ | 85.6% | – | – | – |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[157](#bib.bib157)] | AAAI 2018 | BBox | BBox | Audio | VGG-16
    | $227\times 227$ | 86.6% | – | – | – |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| T-CNN [[158](#bib.bib158)] | IJCAI 2018 | BBox |  | Knowledge base + Texts
    | ResNet-50 | $224\times 224$ | 86.5% | – | – | – |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| KERL [[159](#bib.bib159)] | IJCAI 2018 |  |  | Knowledge base | VGG-16 |
    $224\times 224$ | 87.0% | – | – | – |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| PMA [[160](#bib.bib160)] | IEEE TIP 2020 |  |  | Language texts | VGG-16
    | $448\times 448$ | 88.2% | – | – | – |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| PMA [[160](#bib.bib160)] | IEEE TIP 2020 |  |  | Language texts | ResNet-50
    | $448\times 448$ | 88.7% | – | – | – |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: Beyond the conventional recognition paradigms, which are restricted to using
    supervision associated with the images themselves, another paradigm is to leverage
    external information, *e.g.*, web data, multi-modal data, or human-computer interactions,
    to further assist fine-grained recognition.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Noisy Web Data
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Large and well-labeled training datasets are necessary in order to identify
    subtle differences between various fine-grained categories. However, acquiring
    accurate human labels for fine-grained categories is difficult due to the need
    for domain expertise and the myriads of fine-grained categories (*e.g.*, potentially
    more than tens of thousands of subordinate categories in a meta-category). As
    a result, some fine-grained recognition methods seek to utilize freely available,
    but noisy, web data to boost recognition performance. The majority of existing
    work in this line can be roughly grouped into two directions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The first direction involves scraping noisy labeled web data for the categories
    of interest as training data, which is regarded as *webly supervised learning* [[161](#bib.bib161),
    [62](#bib.bib62), [154](#bib.bib154)]. These approaches typically concentrate
    on: 1) overcoming the domain gap between easily acquired web images and the well-labeled
    data from standard datasets; and 2) reducing the negative effects caused by the
    noisy data. For instance, HAR-CNN [[149](#bib.bib149)] utilized easily annotated
    meta-classes inherent in the fine-grained data and also acquired a large number
    of meta-class-labeled images from the web to regularize the models for improving
    recognition accuracy in a multi-task manner (*i.e.*, for both the fine-grained
    and the meta-class data recognition task). Xu *et al.* [[150](#bib.bib150)] investigated
    if fine-grained web images could provide weakly-labeled information to augment
    deep features and thus contribute to robust object classifiers by building a multi-instance
    (MI) learner, *i.e.*, treating the image as the MI bag and the proposal part bounding
    boxes as the instances of MI. Krause *et al.* [[151](#bib.bib151)] introduced
    an alternative approach to combine a generic classification model with web data
    by excluding images that appear in search results for more than one category to
    combat cross-domain noise. Inspired by adversarial learning [[162](#bib.bib162)],
    [[62](#bib.bib62)] proposed an adversarial discriminative loss to encourage representation
    coherence between standard and web data.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The second direction is to transfer the knowledge from auxiliary categories
    with well-labeled training data to the test categories, which usually employs
    zero-shot learning [[63](#bib.bib63)] or meta learning [[163](#bib.bib163)]. Niu *et
    al.* [[63](#bib.bib63)] exploited zero-shot learning to transfer knowledge from
    annotated fine-grained categories to other fine-grained categories. Subsequently,
    Zhang *et al.* [[153](#bib.bib153)], Yang *et al.* [[155](#bib.bib155)], and Zhang *et
    al.* [[156](#bib.bib156)] investigated different approaches for selecting high-quality
    web training images to expand the training set. Zhang *et al.* [[153](#bib.bib153)]
    proposed a novel regularized meta-learning objective to guide the learning of
    network parameters so they are optimal for adapting to the target fine-grained
    categories. Yang *et al.* [[155](#bib.bib155)] designed an iterative method that
    progressively selects useful images by modifying the label assignment using multiple
    labels to lessen the impact of the labels from the noisy web data. Zhang *et al.* [[156](#bib.bib156)]
    leveraged the prediction scores in different training epochs to supervise the
    separation of useful and irrelevant noisy web images.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Multi-Modal Data
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-modal analysis has attracted a lot of attention with the rapid growth
    of multi-media data, *e.g.*, image, text, knowledge bases, etc. In fine-grained
    recognition, multi-modal data can be used to establish joint-representations/embeddings
    by incorporating multi-modal data in order to boost fine-grained recognition accuracy.
    Compared with strong semantic supervision from fine-grained images (*e.g.*, part
    annotations), text descriptions are a weak form of supervision (*i.e.*, they only
    provide image-level supervision). One advantage however, is that text descriptions
    can be relatively accurately generated by non-experts. Thus, they are both easy
    and cheap to be collected. In addition, high-level knowledge graphs, when available,
    can contain rich knowledge (*e.g.*, *DBpedia* [[164](#bib.bib164)]). In practice,
    both text descriptions and knowledge bases are useful extra guidance for advancing
    fine-grained image representation learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ca6d1c1715fce9b694281c505b51370.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: An example knowledge graph for modeling category-attribute correlations
    in *CUB200-2011* [[13](#bib.bib13)].'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Reed *et al.* [[58](#bib.bib58)] collected text descriptions, and introduced
    a structured joint embedding for zero-shot fine-grained image recognition by combining
    text and images. Later, He and Peng [[59](#bib.bib59)] combined vision and language
    bi-streams in a joint end-to-end fashion to preserve the intra-modality and inter-modality
    information for generating complementary fine-grained representations. Later,
    PMA [[160](#bib.bib160)] proposed a mask-based self-attention mechanism to capture
    the most discriminative parts in the visual modality. In addition, they explored
    using out-of-visual-domain knowledge using language with query-relational attention.
    Multiple PMA blocks for the vision and language modalities were aggregated and
    stacked using the proposed progressive mask strategy. For fine-grained recognition
    with knowledge bases, some existing works  [[159](#bib.bib159), [158](#bib.bib158)]
    have introduced knowledge base information (using attribute label associations,
    cf. Figure [10](#S5.F10 "Figure 10 ‣ 5.3.2 Multi-Modal Data ‣ 5.3 Recognition
    with External Information ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image
    Analysis with Deep Learning: A Survey")) to implicitly enrich the embedding space,
    while also reasoning about the discriminative attributes of fine-grained objects.
    Concretely, T-CNN [[158](#bib.bib158)] explored using semantic embeddings from
    knowledge bases and text, and then trained a CNN to linearly map image features
    to the semantic embedding space to aggregate multi-modal information. To incorporate
    the knowledge representation into image features, KERL [[159](#bib.bib159)] employed
    a gated graph network to propagate node messages through the graph to generate
    the knowledge representation. Finally, [[157](#bib.bib157)] incorporated audio
    information related to the fine-grained visual categories of interest to boost
    recognition accuracy.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Humans-in-the-Loop
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Human-in-the-loop methods [[165](#bib.bib165)] combine the complementary strengths
    of human knowledge with computer vision algorithms. Fine-grained recognition with
    humans-in-the-loop is typically posed in an iterative fashion and requires the
    vision system to be intelligent about when it queries the human for assistance.
    Generally, for these kinds of recognition methods, in each round the system is
    seeking to understand how humans perform recognition, *e.g.*, by asking expert
    humans to label the image class [[166](#bib.bib166)], or by identifying key part
    localization and selecting discriminative features [[167](#bib.bib167)] for fine-grained
    recognition.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Summary and Discussion
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *CUB200-2011* [[13](#bib.bib13)], *Stanford Dogs* [[42](#bib.bib42)], *Stanford
    Cars* [[43](#bib.bib43)], and *FGVC Aircraft* [[44](#bib.bib44)] benchmarks are
    among the most influential datasets in fine-grained recognition. Tables [II](#S5.T2
    "Table II ‣ 5.1 Recognition by Localization-Classification Subnetworks ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")
    and [III](#S5.T3 "Table III ‣ 5.3 Recognition with External Information ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")
    summarize results achieved by the fine-grained methods belonging to three recognition
    learning paradigms outlined above, *i.e.*, “recognition by localization-classification
    subnetworks”, “recognition by end-to-end feature encoding”, and “recognition with
    external information”. A chronological overview can be seen in Figure [7](#S4.F7
    "Figure 7 ‣ 4 Benchmark Datasets ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey"). The main observations can be summarized as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is an explicit correspondence between the reviewed methods and the aforementioned
    challenges of fine-grained recognition. Specifically, the challenge of capturing
    subtle visual differences can be overcome by localization-classification methods
    (cf. Section [5.1](#S5.SS1 "5.1 Recognition by Localization-Classification Subnetworks
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey")) or via specific construction-based tasks [[113](#bib.bib113), [115](#bib.bib115),
    [147](#bib.bib147)], as well as human-in-the-loop methods. The challenge of characterizing
    fine-grained tailored features is alleviated by performing high-order feature
    interactions or by leveraging multi-modality data. Finally, the challenging nature
    of FGIA can be somewhat addressed by designing specific loss functions [[107](#bib.bib107),
    [108](#bib.bib108), [110](#bib.bib110)] for achieving better accuracy.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table IV: Comparative fine-grained recognition results on *CUB200-2011* using
    different input image resolutions. The results in this table are conducted based
    on a vanilla ResNet-50 trained at the respective resolution.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| Resolution | $224\times 224$ | $280\times 280$ | $336\times 336$ | $392\times
    392$ |'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Accuracy | 81.6% | 83.3% | 85.0% | 85.6% |'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the different learning paradigms, the “recognition by localization-classification
    subnetworks” and “recognition by end-to-end feature encoding” paradigms are the
    two most frequently investigated ones.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part-level reasoning of fine-grained object categories boosts fine-grained recognition
    accuracy especially for non-rigid objects, *e.g.*, birds. Modeling the internal
    semantic interactions/correlations among discriminative parts has attracted increased
    attention in recent years, cf. [[71](#bib.bib71), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [81](#bib.bib81), [27](#bib.bib27), [94](#bib.bib94), [109](#bib.bib109)].
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-rigid fine-grained object recognition (*e.g.*, birds or dogs) is more challenging
    than rigid fine-grained objects (*e.g.*, cars or aircrafts), which is partly due
    to the variation on object appearance.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-grained image recognition performance improves as image resolution increases
    [[168](#bib.bib168)]. Comparison results on *CUB200-2011* of different image resolutions
    are reported in Table [IV](#S5.T4 "Table IV ‣ 1st item ‣ 5.4 Summary and Discussion
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey").'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a trade-off between recognition and localization ability for the “recognition
    by localization-classification subnetworks” paradigm, which might impact a single
    integrated network’s recognition accuracy. Such a trade-off is also reflected
    in practice when trying to achieve better recognition results, in that training
    usually involves alternating optimization of the two networks or separately training
    the two followed by joint tuning. Alternating or multistage strategies complicate
    the tuning of the integrated network.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While effective, most end-to-end encoding networks are less human-interpretable
    and less consistent in their accuracy across non-rigid and rigid visual domains
    compared to localization-classification subnetworks. Recently, it has been observed
    that several higher-order pooling methods attempt to understand such kind of methods
    by presenting visual interpretation [[141](#bib.bib141)] or from an optimization
    perspective [[169](#bib.bib169)].
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Recognition by localization-classification subnetworks” based methods are challenging
    to apply when the fine-grained parts are not consistent across the meta-categories
    (*e.g.*, *iNaturalist* [[2](#bib.bib2)]). Here, unified end-to-end feature encoding
    methods are more appropriate.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6 Fine-Grained Image Retrieval
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-grained retrieval is another fundamental aspect of FGIA that has gained
    more traction in recent years. What distinguishes fine-grained retrieval from
    fine-grained recognition is that in addition to estimating the sub-category correctly,
    it is also necessary to rank all the instances so that images belonging to the
    same sub-category are ranked highest based on the fine-grained details in the
    query. Specifically, in fine-grained retrieval we are given a database of images
    of the same meta-category (*e.g.*, birds or cars) and a query, and the goal is
    to return images related to the query based on relevant fine-grained features.
    Compared to generic image retrieval, which focuses on retrieving near-duplicate
    images based on similarities in their content (*e.g.*, texture, color, and shapes),
    fine-grained retrieval focuses on retrieving the images of the same category type
    (*e.g.*, the same subordinate species of animal or the same model of vehicle).
    What makes it more challenging is that objects of fine-grained categories have
    exhibit subtle differences, and can vary in pose, scale, and orientation or can
    contain large cross-modal differences (*e.g.*, in the case of sketch-based retrieval).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d08cb2791e26120676333e2f14984fa.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An illustration of fine-grained content-based image retrieval (FG-CBIR).
    Given a query image (*aka* probe) depicting a “Dodge Charger Sedan 2012”, fine-grained
    retrieval is required to return images of the same car model from a car database
    (*aka* galaxy). In this figure, the fourth returned image, marked with a red outline,
    is incorrect as it is a different car model, it is a “Dodge Caliber Wagon 2012”.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Table V: Comparison of recent fine-grained content-based image retrieval methods
    on CUB200-2011 [[13](#bib.bib13)] and Stanford Cars [[43](#bib.bib43)]. *Recall*@$K$
    is the average recall over all query images in the test set.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Published in | Supervised | Backbones | Img. Resolution | Recall@$K$
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| Birds | Cars |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 4 | 8 | 1 | 2 | 4 | 8 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| SCDA [[32](#bib.bib32)] | TIP 2017 |  | VGG-16 | $224\times 224$ | 62.2%
    | 74.2% | 83.2% | 90.1% | 58.5% | 69.8% | 79.1% | 86.2% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| CRL-WSL [[170](#bib.bib170)] | IJCAI 2018 | $\checkmark$ | VGG-16 | $224\times
    224$ | 65.9% | 76.5% | 85.3% | 90.3% | 63.9% | 73.7% | 82.1% | 89.2% |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| DGCRL [[28](#bib.bib28)] | AAAI 2019 | $\checkmark$ | ResNet-50 | Not given
    | 67.9% | 79.1% | 86.2% | 91.8% | 75.9% | 83.9% | 89.7% | 94.0% |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| Zeng *et al.* [[171](#bib.bib171)] | Image and Vis. Comp. 2020 | $\checkmark$
    | ResNet-50 | $224\times 224$ | 70.1% | 79.8% | 86.9% | 92.0% | 86.7% | 91.7%
    | 95.2% | 97.0% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: 'Fine-grained retrieval techniques have been widely used in commercial applications,
    *e.g.*, e-commerce (searching fine-grained products [[172](#bib.bib172)]), touch-screen
    devices (searching fine-grained objects by sketches [[53](#bib.bib53)]), crime
    prevention (searching face photos [[173](#bib.bib173)]), among others. Depending
    on the type of query image, the most studied areas of fine-grained image retrieval
    can be separated into two groups: fine-grained content-based image retrieval (FG-CBIR,
    cf. Figure [11](#S6.F11 "Figure 11 ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey")) and fine-grained sketch-based image
    retrieval (FG-SBIR, cf. Figure [12](#S6.F12 "Figure 12 ‣ 6.1 Content-based Fine-Grained
    Image Retrieval ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey")). Fine-grained image retrieval can also be expanded
    into fine-grained cross-media retrieval [[57](#bib.bib57)], which can utilize
    one media type to retrieve any media types, for example using an image to retrieve
    relevant text, video, or audio.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: For performance evaluation, following the standard convention, FG-CBIR performance
    is typically measured using *Recall@*$K$ [[174](#bib.bib174)] which is the average
    recall score over all $M$ query images in the test set. For each query, the top
    $K$ relevant images are returned. The recall score will be $1$ if there are at
    least one positive image in the top $K$ returned images, and $0$ otherwise. By
    formulation, the definition of *Recall@*$K$ is as follows
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\emph{Recall@}K=\frac{1}{M}\sum_{i=1}^{M}{\rm score_{i}}\,.$ |  | (7)
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: 'For measuring FG-SBIR performance, *Accuracy@*$K$ is commonly used, which is
    the percentage of sketches whose true-match photos are ranked in the top $K$:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\emph{Accuracy@}K=\frac{&#124;I_{\rm correct}^{K}&#124;}{K}\,,$ |  |
    (8) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: where $|I_{\rm correct}^{K}|$ is the number of true-match photos in top $K$.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Content-based Fine-Grained Image Retrieval
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SCDA [[32](#bib.bib32)] is one of the earliest examples of fine-grained image
    retrieval that used deep learning. It employs a pre-trained CNN to select meaningful
    deep descriptors by localizing the main object in an image without using explicit
    localization supervision. Unsurprisingly, they show that selecting only useful
    deep descriptors, by removing background features, can significantly benefit retrieval
    performance in such an unsupervised retrieval setting (*i.e.*, requiring no image
    labels). Recently, supervised metric learning based approaches (*e.g.*, [[170](#bib.bib170),
    [28](#bib.bib28)]) have been proposed to overcome the retrieval accuracy limitations
    of unsupervised retrieval. These methods still include additional sub-modules
    specifically tailored for fine-grained objects, *e.g.*, the weakly-supervised
    localization module proposed in [[170](#bib.bib170)], which is in turn inspired
    by  [[32](#bib.bib32)]. CRL-WSL [[170](#bib.bib170)] employed a centralized ranking
    loss with a weakly-supervised localization approach to train their feature extractor.
    DGCRL [[28](#bib.bib28)] eliminated the gap between inner-product and the Euclidean
    distance in the training and test stages by adding a Normalize-Scale layer to
    enhance the intra-class separability and inter-class compactness with their Decorrelated
    Global-aware Centralized Ranking Loss. Recently, the Piecewise Cross Entropy loss [[171](#bib.bib171)]
    was proposed by modifying the traditional cross entropy function by reducing the
    confidence of the fine-grained model, which is similar to the basic idea of following
    the natural prediction confidence scores for fine-grained categories [[107](#bib.bib107),
    [108](#bib.bib108)].
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance of recent fine-grained content-based image retrieval approaches
    are reported in Table [V](#S6.T5 "Table V ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"). Although supervised metric learning
    based retrieval methods outperform their unsupervised counterparts, the absolute
    recall scores (*i.e.*, *Recall@*$K$) of the retrieval task still has room for
    further improvement. One promising direction is to integrate advanced techniques,
    *e.g.*, attention mechanisms or higher-order feature interactions, which are successful
    for fine-grained recognition into FG-CBIR to achieve better retrieval accuracy.
    However, new large-scale FG-CBIR datasets are required to drive future progress,
    which are also desirable to be associated with other characteristics or challenges,
    *e.g.*, open-world sub-category retrieval (cf. Section [2](#S2 "2 Recognition
    vs. Retrieval ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")).'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78fc7084b930197f8ade6e20f8821f8c.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: An illustration of fine-grained sketch-based image retrieval (FG-SBIR),
    where a free-hand human sketch serves as the query for instance-level retrieval
    of images. FG-SBIR is challenging due to 1) the fine-grained and cross-domain
    nature of the task and 2) free-hand sketches are highly abstract, making fine-grained
    matching even more difficult.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Table VI: Comparison of fine-grained sketch-based image retrieval methods on
    QMUL-Shoe [[52](#bib.bib52)], QMUL-Chair [[52](#bib.bib52)], QMUL-Handbag [[54](#bib.bib54)],
    Sketchy [[53](#bib.bib53)], and QMUL-Shoe-V2 [[56](#bib.bib56)]. *Accuracy*@$K$
    is the percentage of sketches whose true-match photos are ranked in the top $K$.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Published in | Accuracy@$K$ |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| *QMUL-Shoe* | *QMUL-Chair* | *QMUL-Handbag* | *Sketchy* | *QMUL-Shoe-V2*
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| 1 | 10 | 1 | 10 | 1 | 10 | 1 | 10 | 1 | 10 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| Yu *et al.* [[52](#bib.bib52)] | CVPR 2016 | 39.1% | 87.8% | 69.1% | 97.9%
    | – | – | – | – | – | – |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| Song *et al.* [[175](#bib.bib175)] | BMVC 2016 | 50.5% | 91.3% | 78.3% |
    98.9% | – | – | – | – | – | – |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| Song *et al.* [[54](#bib.bib54)] | ICCV 2017 | 61.7% | 94.8% | 81.4% | 95.9%
    | 49.4% | 82.7% | – | – | – | – |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| Li *et al.* [[55](#bib.bib55)] | IEEE TIP 2017 | 51.3% | 90.4% | 76.3% |
    97.9% | – | – | 45.3% | 98.2% | – | – |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| Radenovic *et al.* [[176](#bib.bib176)] | ECCV 2018 | 54.8% | 92.2% | 85.7%
    | 97.9% | 51.2% | 85.7% | – | – | – | – |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[177](#bib.bib177)] | ECCV 2018 | 35.7% | 84.3% | 67.1% |
    99.0% | – | – | – | – | – | – |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| Pang *et al.* [[178](#bib.bib178)] | CVPR 2020 | 56.5% | – | 85.9% | – |
    62.9% | – | – | – | 36.5% | – |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| Bhunia *et al.* [[179](#bib.bib179)] | CVPR 2020 | – | – | – | – | – | –
    | – | – | – | 79.6% |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| Sain *et al.* [[180](#bib.bib180)] | BMVC 2020 | – | – | – | – | – | – |
    – | – | 36.3% | 80.6% |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: 6.2 Sketch-based Fine-Grained Image Retrieval
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of fine-grained sketch-based image retrieval (FG-SBIR) is to match
    specific photo instances using a free-hand sketch as the query modality. Distinct
    from the previously discussed content-based approach, the additional sketch-photo
    domain gap lies at the centre of FG-SBIR. Thus, the key is introducing a cross-modal
    representation that not only captures fine-grained characteristics present in
    the sketches, but also possesses the ability to traverse the sketch and image
    domain gap.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Existing FG-SBIR solutions generally aim to train a joint embedding space where
    sketches and photos can be compared in a nearest neighbor fashion. Specifically,
    Li *et al.* [[51](#bib.bib51)] proposed the first method for FG-SBIR, where they
    learned a deformable part-based models (DPM)  [[181](#bib.bib181)] as a mid-level
    representation to discover and encode the various poses in the sketch and image
    domains independently. This was followed by a graph matching operation on the
    DPM to establish pose correspondences across the two domains. Yu *et al.* [[52](#bib.bib52)]
    first employed deep learning for this task, leveraging a triplet ranking model
    with a staged pre-training strategy to learn a joint embedding space for the two
    domains. This triplet ranking model was further augmented with auxiliary semantic
    attribute prediction and attribute ranking layers [[175](#bib.bib175)] to improve
    generalization [[52](#bib.bib52)]. The implicit challenge encountered due to the
    large sketch-photo domain gap in FG-SBIR was tackled via a discriminative-generative
    hybrid model [[182](#bib.bib182)] using cross-domain translation. Inspired on
    these approaches, Song *et al.* [[54](#bib.bib54)] introduced an attention module
    and encoded the spatial position of visual features and combined both coarse and
    fine-grained semantic knowledge. Li *et al.* [[55](#bib.bib55)] introduced a part-aware
    learning approach to reduce the instance-level domain-gap by performing subspace
    alignment with fine-grained attributes. While research on fine-grained SBIR has
    flourished over the years, a dilemma still remains – whether sketching is a better
    input modality for fine-grained image retrieval compared to other alternatives,
    *e.g.*, texts or attributes. To answer this question, Song *et al.* [[183](#bib.bib183)]
    showed the superiority of sketch over text for fine-grained retrieval, illustrating
    that each modality can complement the other when they are jointly modeled.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to these earlier works that were mostly based on Siamese-triplet
    networks, there have been several recent attempts to advance FG-SBIR performance.
    For instance, Pang *et al.* [[56](#bib.bib56)] identified that a baseline triplet
    based model fails to generalize when exposed to unseen categories. Thus, cross-category
    generalization was introduced through a domain generalization setup, where a universal
    manifold of prototypical visual sketches was modeled to dynamically represent
    the sketch/photo. On the other hand, Bhunia *et al.* [[179](#bib.bib179)] developed
    an on-the-fly retrieval setup via reinforcement learning that begins retrieving
    photos as soon as the user starts drawing. ImageNet pre-trained weights have been
    considered as the de-facto choice for initializing sketch/photo embedding networks
    for FG-SBIR. However, following the recent progress in self-supervised learning [[184](#bib.bib184),
    [185](#bib.bib185), [186](#bib.bib186)], Pang *et al.* [[178](#bib.bib178)] performed
    a jigsaw solving strategy over mixed-modal patches between a particular photo
    and its edge-map. They showed this to be an effective pre-text task for self-supervised
    pre-training strategy and it improves FG-SBIR performance. Instead of performing
    independent sketch and photo embeddings, as in almost all previous works, Sain *et
    al.*[[180](#bib.bib180)] used paired-embeddings by employing cross-modal co-attention
    and hierarchical stroke/region-wise feature fusion in order to deal with varying
    levels of sketch detail. In spite of a significant performance boost, it is noteworthy
    that paired-embeddings incur a significant computational overhead, by a multiple
    of at least the number of gallery photos, compared to other state-of-the-art methods.
    Recently, a new scene-level fine-grained SBIR task [[187](#bib.bib187)] was explored.
    Liu *et al.* [[187](#bib.bib187)] proposed to utilize local features such as object
    instances and their visual detail, as well as global context (*e.g.*, the scene
    layout), to deal with such a task.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the previous FG-SBIR methods, Zhang *et al.* [[177](#bib.bib177)]
    and Radenovic *et al.* [[176](#bib.bib176)] also evaluated on popular FG-SBIR
    datasets, in addition to category-level SBIR ones. We compare the results of recent
    FG-SBIR methods in Table [VI](#S6.T6 "Table VI ‣ 6.1 Content-based Fine-Grained
    Image Retrieval ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey"). Note that we only present methods whose evaluation
    strategies are uniform and consistent, and thus exclude those that involve problem
    specific evaluation protocols, such as zero-shot FG-SBIR [[56](#bib.bib56)].'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 7 Common Techniques Shared by both Fine-Grained Recognition and Retrieval
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tasks of fine-grained image recognition and retrieval are complementary.
    As a result, there exists common techniques shared by both problems in the FGIA
    literature. In this section, we discuss these common techniques, in terms of methods
    and basic ideas, with the aim of motivating further work in these areas.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Common Methods: In the context of deep learning, both fine-grained recognition
    and retrieval tasks require discriminative deep feature embeddings to distinguish
    subtle differences between fine-grained objects. While recognition aims to distinguish
    category labels, retrieval aims to return an accurate ranking. To achieve these
    goals, deep metric learning and multi-modal matching can be viewed as two common
    sets of techniques that are applicable for both fine-grained recognition and retrieval.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, deep metric learning [[188](#bib.bib188)] attempts to map image
    data to an embedding space, where similar fine-grained images are close together
    and dissimilar images are far apart. In general, fine-grained recognition realizes
    metric learning by classification losses, where it includes a weight matrix to
    transform the embedding space into a vector of fine-grained class logits, *e.g.*, [[27](#bib.bib27),
    [109](#bib.bib109), [111](#bib.bib111)]. While, metric learning on fine-grained
    retrieval tasks (most cases without explicit image labels) is always achieved
    by means of embedding losses which operate on the relationships between fine-grained
    samples in a batch, *e.g.*, [[170](#bib.bib170), [28](#bib.bib28)].
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Recently, multi-modal matching methods [[29](#bib.bib29), [30](#bib.bib30)]
    have emerged as powerful representation learning approaches to simultaneously
    boost fine-grained recognition and retrieval tasks. In particularly, Mafla *et
    al.* [[29](#bib.bib29)] leveraged textual information along with visual cues to
    comprehend the existing intrinsic relation between the two modalities. [[30](#bib.bib30)]
    employed a graph convolutional network to perform multi-modal reasoning and obtain
    relationship-enhanced multi-modal features. Such kind of methods reveal a new
    development trend of multi-modal learning in FGIA.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Common Basic Ideas: Beyond these common methods, many basic ideas are shared
    by both fine-grained recognition and retrieval, *e.g.*, selecting useful deep
    descriptors [[31](#bib.bib31), [32](#bib.bib32)], reducing the uncertainty of
    fine-grained predictions [[107](#bib.bib107), [108](#bib.bib108), [171](#bib.bib171)],
    and deconstructing/constructing fine-grained images for learning fine-grained
    patterns [[113](#bib.bib113), [178](#bib.bib178)]. These observations further
    illustrate the benefit of consolidating work in fine-grained recognition and fine-grained
    retrieval in this survey paper.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 8 Future Directions
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Advances in deep learning have enabled significant progress in fine-grained
    image analysis (FGIA). Despite the success however, there are still many unsolved
    problems. Thus, in this section, we aim to explicitly point out these problems,
    and highlight some open questions to motivate future progression of the field.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'Precise Definition of “Fine-Grained”: Although FGIA has existed for many years
    as a thriving sub-field of computer vision and pattern recognition, one fundamental
    problem in FGIA persists, *i.e.*, the designation lacks a precise definition [[189](#bib.bib189),
    [190](#bib.bib190)]. Specifically, the community always *qualitatively* describes
    a so-called fine-grained dataset/problem as being “fine-grained” by roughly stating
    that its target objects belong to one meta-category. However, a precise definition
    of “fine-grained” could enable us to *quantitatively* evaluate the granularity
    of a dataset. In addition, it would not only provide a better understanding of
    current algorithmic performance on tasks of different granularities, but also
    bring additional insights to the fine-grained community.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Next-Generation Fine-Grained Datasets: Classic fine-grained datasets such as
    *CUB200-2011*, *Stanford Dogs*, *Stanford Cars*, and *Oxford Flowers*, are overwhelmingly
    used for benchmarking performance in FGIA. However, by modern standards these
    datasets are relatively small-scale and are largely saturated in terms of performance.
    In the future, it would be valuable to see additional large-scale fine-grained
    datasets being promoted to replace these existing benchmarks, *e.g.*, *iNat2017* [[2](#bib.bib2)],
    *Dogs-in-the-wild* [[27](#bib.bib27)], *RPC* [[5](#bib.bib5)]. State-of-the-art
    results on these datasets are 72.6% [[191](#bib.bib191)], 78.5% [[27](#bib.bib27)],
    80.5% [[192](#bib.bib192)], respectively, which reveals the substantial room for
    further improvement. Moreover, these new benchmarks should embrace and embody
    all the challenges associated with fine-grained learning in addition to being
    large-scale (in terms of both the number of sub-categories and images), contain
    diverse images captured in realistic settings, and include rich annotations. However,
    high-quality fine-grained datasets usually require domain-experts to annotate.
    This limits the development of fine-grained datasets to a certain extent. As a
    potential solution, constructing an unlabeled large-scale fine-grained database
    and employing unsupervised feature learning techniques (*e.g.*, self-supervised
    learning [[193](#bib.bib193)]) could benefit discriminative features learning
    and further promote unsupervised downstream tasks [[3](#bib.bib3)]. Also, synthetic
    data [[194](#bib.bib194)] is an increasingly popular tool for training deep models,
    and offers promising opportunities to be explored further for FGIA.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'Application to 3D Fine-Grained Tasks: Most existing FGIA approaches target
    2D fine-grained images and the value of 3D information (*e.g.*, 3D pose labels
    or 3D shape information) is under-explored. How 3D object representations boost
    the performance of 2D FGIA approaches is an interesting and important problem.
    On the other hand, how 2D FGIA approaches generalize to 3D fine-grained applications [[195](#bib.bib195)],
    *e.g.*, robotic bin picking, robot perception or augmented reality, is also worthy
    of future attention. To make progress in this area there are open questions associated
    with difficulties in obtaining accurate 3D annotations for many object categories,
    *e.g.*, animals or other non-rigid objects.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Robust Fine-Grained Representations: One important factor which makes FGIA
    uniquely challenging is the overwhelming variability in real-world fine-grained
    images, including changes in viewpoint, object scale, pose, and factors such as
    part deformations, background clutter, and so on. Meanwhile, FGIA often dictates
    fine-grained patterns (derived from discriminative but subtle parts) to be identified
    to drive predictions, which makes it more sensitive to image resolutions, corruptions
    and perturbations, and adversarial examples. Despite advances in deep learning,
    current models are still exhibit a lack of robustness to these variations or disturbances,
    and this significantly constrains their usability in many real-world applications
    where high accuracy is essential. Therefore, how to effectively obtain robust
    fine-grained representations (*i.e.*, not only containing discriminative fine-grained
    visual clues, but also resisting the interference of irrelevant information) requires
    further in-depth exploration. As discussed above, when coupled with next generation
    fine-grained datasets, there are questions related to the utility of self-supervised
    learning in these domains [[196](#bib.bib196)]. For example, will self-supervised
    learning help improving FGIA, and do we perform self-supervision on fine-grained
    data to generate robust fine-grained representations?'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretable Fine-Grained Learning: Unilaterally achieving higher accuracy
    compared to non-expert humans may no longer be the primary goal of FGIA. It is
    very important for fine-grained models to not only result in high accuracy but
    also to be interpretable [[197](#bib.bib197)]. More interpretable FGIA could help
    the community to address several challenges when dealing with fine-grained tasks
    using deep learning, *e.g.*, semantically debugging network representations, learning
    via human-computer communications at the semantic level, designing more effective,
    task-specific deep models, etc.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-Grained Few-Shot Learning: Humans are capable of learning a new fine-grained
    concept with very little supervision (*e.g.*, with a few image of a new species
    of bird), yet our best deep learning fine-grained systems need hundreds or thousands
    of labeled examples. Even worse, the supervision of fine-grained images are both
    time-consuming and expensive to obtain, as fine-grained objects need to be accurately
    labeled by domain experts. Thus, it is desirable to develop fine-grained few-shot
    learning (FGFS) methods [[148](#bib.bib148), [198](#bib.bib198), [199](#bib.bib199),
    [200](#bib.bib200)]. The task of FGFS requires the learning system to build classifiers
    for novel fine-grained categories from few examples (*e.g.*, less than 10). Robust
    FGFS methods could significantly strengthen the usability and scalability of fine-grained
    recognition.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-Grained Hashing: Recently, larger-scale fine-grained datasets have been
    released, *e.g.*, [[41](#bib.bib41), [38](#bib.bib38), [46](#bib.bib46), [2](#bib.bib2),
    [5](#bib.bib5), [3](#bib.bib3)]. In real applications like fine-grained image
    retrieval, the computational cost of finding the exact nearest neighbor can be
    prohibitively high in cases where the reference database is very large. Image
    hashing [[201](#bib.bib201), [202](#bib.bib202)] is a popular and effective technique
    for approximate nearest neighbor search, and has the potential to help with large-scale
    fine-grained data too. Therefore, targeting the big data challenge, fine-grained
    hashing [[203](#bib.bib203), [204](#bib.bib204)] is a promising direction worthy
    of further explorations.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic Fine-Grained Models: Automated machine learning (AutoML) [[205](#bib.bib205)]
    and neural architecture search (NAS) [[206](#bib.bib206)] have attracted growing
    attention of late. AutoML targets automating the process of applying machine learning
    to real-world tasks and NAS is the process of automating neural network architecture
    design. Recent methods for AutoML and NAS could be comparable or even outperform
    hand-designed architectures in various computer vision applications. Thus, it
    is also logical that AutoML and NAS techniques could find better, and more tailor-made
    deep models for FGIA.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-Grained Analysis in More Realistic Settings: In the past decade, FGIA
    related techniques have been developed and have achieved good performance in standard
    computer vision benchmarks, *e.g.*, [[13](#bib.bib13), [42](#bib.bib42), [43](#bib.bib43)].
    The vast majority of these existing FGIA benchmarks are however defined in a static
    and closed environment. One other big limitation of current FGIA datasets is that
    they typically contain large instances of the object (*i.e.*, the objects of interest
    occupy most of the image frame). However, these settings are not representative
    of many real-world applications, *e.g.*, recognizing retail products in storage
    racks by models trained with images collected in controlled environments [[5](#bib.bib5)]
    or recognizing/detecting tends of thousands of natural species in the wild [[2](#bib.bib2)].
    More research is needed in areas such as domain adaptation [[207](#bib.bib207),
    [208](#bib.bib208), [209](#bib.bib209)], long-tailed distributions [[210](#bib.bib210),
    [211](#bib.bib211)], open world settings [[212](#bib.bib212)], scale variations [[2](#bib.bib2)],
    fine-grained video understanding [[213](#bib.bib213), [214](#bib.bib214)], knowledge
    transfer, and resource constrained embedded deployment, to name a few.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have presented a comprehensive survey on recent advances in deep learning
    based fine-grained image analysis (FGIA). Specifically, we advocated a broadened
    definition of FGIA, by consolidating work in fine-grained recognition and fine-grained
    retrieval. We enumerated gaps in existing research, pointed out a series of emerging
    topics, highlighted important future research directions, and illustrated that
    the problem of FGIA is still far from solved. However, given the significant improvements
    in performance over the past decade, we remain optimistic about future progress
    as we move towards more realistic and impactful applications.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank the editor and the anonymous reviewers for their
    constructive comments. This work was supported by Natural Science Foundation of
    Jiangsu Province of China under Grant (BK20210340), National Natural Science Foundation
    of China under Grant (61925201, 62132001, 61772256), the Fundamental Research
    Funds for the Central Universities (No. 30920041111), CAAI-Huawei MindSpore Open
    Fund, and “111” Program B13022.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis,
    P. Perona, and S. Belongie, “Building a bird recognition app and large scale dataset
    with citizen scientists: The fine print in fine-grained dataset collection,” in
    *CVPR*, 2015, pp. 595–604.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam,
    P. Perona, and S. Belongie, “The iNaturalist species classification and detection
    dataset,” in *CVPR*, 2017, pp. 8769–8778.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] G. Van Horn, E. Cole, S. Beery, K. Wilber, S. Belongie, and O. Mac Aodha,
    “Benchmarking representation learning for natural world image collections,” in
    *CVPR*, 2021.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] L. Karlinsky, J. Shtok, Y. Tzur, and A. Tzadok, “Fine-grained recognition
    of thousands of object categories with single-example training,” in *CVPR*, 2017,
    pp. 4113–4122.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] X.-S. Wei, Q. Cui, L. Yang, P. Wang, and L. Liu, “RPC: A large-scale retail
    product checkout dataset,” *arXiv preprint arXiv:1901.07249*, 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. Jia, M. Shi, M. Sirotenko, Y. Cui, C. Cardie, B. Hariharan, H. Adam,
    and S. Belongie, “Fashionpedia: Ontology, segmentation, and an attribute localization
    dataset,” in *ECCV*, 2020, pp. 316–332.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. D. Khan and H. Ullah, “A survey of advances in vision-based vehicle
    re-identification,” *CVIU*, vol. 182, pp. 50–63, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Yin, A. Wu, and W.-S. Zheng, “Fine-grained person re-identification,”
    *IJCV*, vol. 128, pp. 1654–1672, 2020.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] ICCV 2019 Workshop on Computer Vision for Wildlife Conservation. [Online].
    Available: https://openaccess.thecvf.com/ICCV2019_workshops/ICCV2019_CVWC'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Wei, S. Tran, S. Xu, B. Kang, and M. Springer, “Deep learning for retail
    product recognition: Challenges and techniques,” *Computational Intelligence and
    Neuroscience*, vol. 128, pp. 1–23, 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] K. E. Johnson and A. T. Eilers, “Effects of knowledge and development
    on subordinate level categorization,” *Cognitive Dev.*, pp. 515–545, 1998.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] B. Yao, A. Khosla, and L. Fei-Fei, “Combining randomization and discrimination
    for fine-grained image categorization,” in *CVPR*, 2011, pp. 1577–1584.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The Caltech-UCSD
    birds-200-2011 dataset,” *Tech. Report CNS-TR-2011-001*, 2011.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    pp. 436–444, 2015.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] IEEE TPAMI Special Issue on Fine-Grained Visual Categorization. [Online].
    Available: https://www.computer.org/digital-library/journals/tp/fine-grained-visual-categorization'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Pattern Recognition Special Issue on Fine-Grained Object Retrieval, Matching
    and Ranking. [Online]. Available: https://www.journals.elsevier.com/pattern-recognition/call-for-papers/fine-grained-object-retrieval-matching-and-ranking'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] ACM TOMM Special Issue on Fine-Grained Visual Recognition and Re-Identification.
    [Online]. Available: https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/CFP_FGVRreID-1592406610240.pdf'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Pattern Recognition Letters Special Issue on Fine-Grained Categorization
    in Ecological Multimedia. [Online]. Available: https://www.sciencedirect.com/journal/pattern-recognition-letters/special-issue/10Z519ZW5DJ'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Neurocomputing Special Issue on Fine-grained Visual Understanding and
    Reasoning. [Online]. Available: https://www.journals.elsevier.com/neurocomputing/call-for-papers/fine-grained-visual-understanding-and-reasoning'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] The competition homepage of “iNaturalist”. [Online]. Available: https://www.kaggle.com/c/inaturalist-2019-fgvc6/overview'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] The competition homepage of “Nature Conservancy Fisheries Monitoring”.
    [Online]. Available: https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] The competition homepage of “Humpback Whale Identification”. [Online].
    Available: https://www.kaggle.com/c/humpback-whale-identification'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] CVPR 2021 Tutorial on Fine-Grained Visual Analysis with Deep Learning.
    [Online]. Available: https://fgva-cvpr21.github.io/'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] The Eight Workshop on Fine-Grained Visual Categorization. [Online]. Available:
    https://sites.google.com/view/fgvc8/home'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] B. Zhao, J. Feng, X. Wu, and S. Yan, “A survey on deep learning-based
    fine-grained object classification and semantic segmentation,” *Int. J. Autom.
    and Comput.*, vol. 14, no. 2, pp. 119–135, 2017.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Zheng, Q. Li, Y. Geng, H. Yu, J. Wang, J. Gan, and W. Xue, “A survey
    of fine-grained image categorization,” in *Proc. IEEE Conf. Signal Process.*,
    2018, pp. 533–538.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Sun, Y. Yuan, F. Zhou, and E. Ding, “Multi-attention multi-class constraint
    for fine-grained image recognition,” in *ECCV*, 2018, pp. 834–850.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] X. Zheng, R. Ji, X. Sun, B. Zhang, Y. Wu, and F. Huang, “Towards optimal
    fine grained retrieval via decorrelated centralized loss with normalize-scale
    layer,” in *AAAI*, 2019, pp. 9291–9298.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Mafla, S. Dey, A. F. Biten, L. Gomez, and D. Karatzas, “Fine-grained
    image classification and retrieval by combining visual and locally pooled textual
    features,” in *WACV*, 2020, pp. 2950–2959.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] ——, “Multi-modal reasoning graph for scene-text based fine-grained image
    classification and retrieval,” in *WACV*, 2020, pp. 4023–4033.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] X.-S. Wei, C.-W. Xie, J. Wu, and C. Shen, “Mask-CNN: Localizing parts
    and selecting descriptors for fine-grained bird species categorization,” *PR*,
    vol. 76, pp. 704–714, 2018.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] X.-S. Wei, J.-H. Luo, J. Wu, and Z.-H. Zhou, “Selective convolutional
    descriptor aggregation for fine-grained image retrieval,” *IEEE TIP*, vol. 26,
    no. 6, pp. 2868–2881, 2017.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Y. Ge, R. Zhang, L. Wu, X. Wang, X. Tang, and P. Luo, “A versatile benchmark
    for detection, pose estimation, segmentation and re-identification of clothing
    images,” in *CVPR*, 2019, pp. 5337–5345.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, and S. C. H. Hoi, “Deep learning
    for person re-identification: A survey and outlook,” *arXiv preprint arXiv:2001.04193*,
    2020.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] F. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet: A unified embedding
    for face recognition and clustering,” in *CVPR*, 2015, pp. 815–823.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Suh, J. Wang, S. Tang, T. Mei, and K. M. Lee, “Part-aligned bilinear
    representations for person re-identification,” in *ECCV*, 2018, pp. 402–419.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] X.-S. Wei, C.-L. Zhang, L. Liu, C. Shen, and J. Wu., “Coarse-to-fine:
    A RNN-based hierarchical attention model for vehicle re-identification,” in *ACCV*,
    2018, pp. 575–591.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang, “DeepFashion: Powering robust
    clothes recognition and retrieval with rich annotations,” in *CVPR*, 2016, pp.
    1096–1104.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. Liu, J. Wang, S. Wen, E. Ding, and Y. Lin, “Localizing by describing:
    Attribute-guided attention localization for fine-grained recognition,” in *AAAI*,
    2017, pp. 4190–4196.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Wang and W. Deng, “Deep face recognition: A survey,” *Neurocomputing*,
    vol. 429, pp. 215–244, 2021.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, and P. N. Belhumeur,
    “Birdsnap: Large-scale fine-grained visual categorization of birds,” in *CVPR*,
    2014, pp. 2019–2026.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei, “Novel dataset
    for fine-grained image categorization,” in *CVPR Workshop on Fine-Grained Visual
    Categorization*, 2011, pp. 806–813.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3D object representations
    for fine-grained categorization,” in *ICCV Workshop on 3D Representation and Recognition*,
    2013.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi, “Fine-grained
    visual classification of aircraft,” *arXiv preprint arXiv:1306.5151*, 2013.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] M.-E. Nilsback and A. Zisserman, “Automated flower classification over
    a large number of classes,” in *Indian Conf. on Comput. Vision, Graph. and Image
    Process.*, 2008, pp. 722–729.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Hou, Y. Feng, and Z. Wang, “VegFru: A domain-specific dataset for fine-grained
    visual categorization,” in *ICCV*, 2017, pp. 541–549.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] L. Bossard, M. Guillaumin, and L. V. Gool, “Food-101 – mining discriminative
    components with random forests,” in *ECCV*, 2014, pp. 446–461.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Bai, Y. Chen, W. Yu, L. Wang, and W. Zhang, “Products-10K: A large-scale
    product recognition dataset,” *arXiv preprint arXiv:2008.10545*, 2020.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet large
    scale visual recognition challenge,” *IJCV*, vol. 115, pp. 211–252, 2015.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] F. Zhou and Y. Lin, “Fine-grained image classification by exploring bipartite-graph
    labels,” in *CVPR*, 2016, pp. 1124–1133.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Li, T. M. Hospedales, Y.-Z. Song, and S. Gong, “Fine-grained sketch-based
    image retrieval by matching deformable part models,” in *BMVC*, 2014, pp. 1–12.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, and C. C. Loy,
    “Sketch me that shoe,” in *CVPR*, 2016, pp. 799–807.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] P. Sangkloy, N. Burnell, C. Ham, and J. Hays, “The sketchy database: learning
    to retrieve badly drawn bunnies,” *ACM Trans. on Graphics*, vol. 35, no. 4, pp.
    1–12, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Deep spatial-semantic
    attention for fine-grained sketch-based image retrieval,” in *ICCV*, 2017, pp.
    5551–5560.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] K. Li, K. Pang, Y.-Z. Song, T. M. Hospedales, T. Xiang, and H. Zhang,
    “Synergistic instance-level subspace alignment for fine-grained sketch-based image
    retrieval,” *IEEE TIP*, vol. 26, no. 12, pp. 5908–5921, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] K. Pang, K. Li, Y. Yang, H. Zhang, T. M. Hospedales, T. Xiang, and Y.-Z.
    Song, “Generalising fine-grained sketch-based image retrieval,” in *CVPR*, 2019,
    pp. 677–686.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X. He, Y. Peng, and X. Liu, “A new benchmark and approach for fine-grained
    cross-media retrieval,” in *ACM MM*, 2019, pp. 1740–1748.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S. Reed, Z. Akata, H. Lee, and B. Schiele, “Learning deep representations
    of fine-grained visual descriptions,” in *CVPR*, 2016, pp. 49–58.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] X. He and Y. Peng, “Fine-grained image classification via combining vision
    and language,” in *CVPR*, 2017, pp. 5994–6002.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] O. Mac Aodha, E. Cole, and P. Perona, “Presence-only geographical priors
    for fine-grained image classification,” in *ICCV*, 2019, pp. 9596–9606.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] G. Chu, B. Potetz, W. Wang, A. Howard, Y. Song, F. Brucher, T. Leung,
    and H. Adam, “Geo-aware networks for fine-grained recognition,” in *ICCV Workshops*,
    2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] X. Sun, L. Chen, and J. Yang, “Learning from web data using adversarial
    discriminative neural networks for fine-grained classification,” in *AAAI*, 2019,
    pp. 273–280.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] L. Niu, A. Veeraraghavan, and A. Sabharwal, “Webly supervised learning
    meets zero-shot learning: A hybrid approach for fine-grained classification,”
    in *CVPR*, 2018, pp. 7171–7180.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] S. Branson, G. Van Horn, S. Belongie, and P. Perona, “Bird species categorization
    using pose normalized deep convolutional nets,” in *BMVC*, 2014, pp. 1–14.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] N. Zhang, J. Donahue, R. Girshick, and T. Darrell, “Part-based R-CNNs
    for fine-grained category detection,” in *ECCV*, 2014, pp. 834–849.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Krause, H. Jin, J. Yang, and L. Fei-Fei, “Fine-grained recognition
    without part annotations,” in *CVPR*, 2015, pp. 5546–5555.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] D. Lin, X. Shen, C. Lu, and J. Jia, “Deep LAC: Deep localization, alignment
    and classification for fine-grained recognition,” in *CVPR*, 2015, pp. 1666–1674.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] S. Huang, Z. Xu, D. Tao, and Y. Zhang, “Part-stacked CNN for fine-grained
    visual categorization,” in *CVPR*, 2016, pp. 1173–1182.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] H. Zhang, T. Xu, M. Elhoseiny, X. Huang, S. Zhang, A. Elgammal, and D. Metaxas,
    “SPDA-CNN: Unifying semantic part detection and abstraction for fine-grained recognition,”
    in *CVPR*, 2016, pp. 1143–1152.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Y. Zhang, X.-S. Wei, J. Wu, J. Cai, J. Lu, V.-A. Nguyen, and M. N. Do,
    “Weakly supervised fine-grained categorization with part-based image representation,”
    *IEEE TIP*, vol. 25, no. 4, pp. 1713–1725, 2016.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Lam, B. Mahasseni, and S. Todorovic, “Fine-grained recognition as HSnet
    search for informative image parts,” in *CVPR*, 2017, pp. 2520–2529.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] X. He and Y. Peng, “Weakly supervised learning of part selection model
    with spatial constraints for fine-grained image classification,” in *AAAI*, 2017,
    pp. 4075–4081.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] W. Ge, X. Lin, and Y. Yu, “Weakly supervised complementary parts models
    for fine-grained image classification from the bottom up,” in *CVPR*, 2019, pp.
    3034–3043.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Z. Wang, S. Wang, H. Li, Z. Dou, and J. Li, “Graph-propagation based correlation
    learning for weakly supervised fine-grained image classification,” in *AAAI*,
    2020, pp. 12 289–12 296.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] C. Liu, H. Xie, Z.-J. Zha, L. Ma, L. Yu, and Y. Zhang, “Filtration and
    distillation: Enhancing region attention for fine-grained visual categorization,”
    in *AAAI*, 2020, pp. 11 555–11 562.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, and Z. Zhang, “The application
    of two-level attention models in deep convolutional neural network for fine-grained
    image classification,” in *CVPR*, 2015, pp. 842–850.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] L. Liu, C. Shen, and A. van den Hengel, “The treasure beneath convolutional
    layers: Cross-convolutional-layer pooling for image classification,” in *CVPR*,
    2015, pp. 4749–4757.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Simon and E. Rodner, “Neural activation constellations: Unsupervised
    part model discovery with convolutional networks,” in *ICCV*, 2015, pp. 1143–1151.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] X. Zhang, H. Xiong, W. Zhou, W. Lin, and Q. Tian, “Picking deep filter
    responses for fine-grained image recognition,” in *CVPR*, 2016, pp. 1134–1142.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Y. Wang, V. I. Morariu, and L. S. Davis, “Learning a discriminative filter
    bank within a CNN for fine-grained recognition,” in *CVPR*, 2018, pp. 4148–4157.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Ding, Y. Zhou, Y. Zhu, Q. Ye, and J. Jiao, “Selective sparse sampling
    for fine-grained image recognition,” in *ICCV*, 2019, pp. 6599–6608.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Z. Huang and Y. Li, “Interpretable and accurate fine-grained recognition
    via region grouping,” in *CVPR*, 2020, pp. 8662–8672.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] J. Fu, H. Zheng, and T. Mei, “Look closer to see better: Recurrent attention
    convolutional neural network for fine-grained image recognition,” in *CVPR*, 2017,
    pp. 4438–4446.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] H. Zheng, J. Fu, T. Mei, and J. Luo, “Learning multi-attention convolutional
    neural network for fine-grained image recognition,” in *ICCV*, 2017, pp. 5209–5217.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. Peng, X. He, and J. Zhao, “Object-part attention model for fine-grained
    image classification,” *IEEE TIP*, vol. 27, no. 3, pp. 1487–1500, 2018.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] L. Zhang, S. Huang, W. Liu, and D. Tao, “Learning a mixture of granularity-specific
    experts for fine-grained categorization,” in *ICCV*, 2019, pp. 8331–8340.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. Zheng, J. Fu, Z.-J. Zha, and J. Luo, “Looking for the devil in the
    details: Learning trilinear attention sampling network for fine-grained image
    recognition,” in *CVPR*, 2019, pp. 5012–5021.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] H. Zheng, J. Fu, Z.-J. Zha, J. Luo, and T. Mei, “Learning rich part hierarchies
    with progressive attention networks for fine-grained image recognition,” *IEEE
    TIP*, vol. 29, pp. 476–488, 2020.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] R. Ji, L. Wen, L. Zhang, D. Du, Y. Wu, C. Zhao, X. Liu, and F. Huang,
    “Attention convolutional binary neural tree for fine-grained visual categorization,”
    in *CVPR*, 2020, pp. 10 468–10 477.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu, “Spatial
    transformer networks,” in *NIPS*, 2015, pp. 2017–2025.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Wang, J. Choi, V. I. Morariu, and L. S. Davis, “Mining discriminative
    triplets of patches for fine-grained classification,” in *CVPR*, 2016, pp. 1163–1172.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Z. Yang, T. Luo, D. Wang, Z. Hu, J. Gao, and L. Wang, “Learning to navigate
    for fine-grained classification,” in *ECCV*, 2018, pp. 438–454.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] X. He, Y. Peng, and J. Zhao, “Which and how many regions to gaze: Focus
    discriminative regions for fine-grained visual categorization,” *IJCV*, vol. 127,
    pp. 1235–1255, 2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Wang, S. Wang, S. Yang, H. Li, J. Li, and Z. Li, “Weakly supervised
    fine-grained image classification via guassian mixture model oriented discriminative
    learning,” in *CVPR*, 2020, pp. 9749–9758.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear CNN models for fine-grained
    visual recognition,” in *ICCV*, 2015, pp. 1449–1457.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell, “Compact bilinear pooling,”
    in *CVPR*, 2016, pp. 317–326.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, and S. Belongie, “Kernel pooling
    for convolutional neural networks,” in *CVPR*, 2017, pp. 2921–2930.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Kong and C. Fowlkes, “Low-rank bilinear pooling for fine-grained classification,”
    in *CVPR*, 2017, pp. 365–374.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Q. Wang, P. Li, and L. Zhang, “G²DeNet: Global Gaussian distribution embedding
    network and its application to visual recognition,” in *CVPR*, 2017, pp. 2730–2739.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Cai, W. Zuo, and L. Zhang, “Higher-order integration of hierarchical
    convolutional activations for fine-grained visual categorization,” in *ICCV*,
    2017, pp. 511–520.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] P. Li, J. Xie, Q. Wang, and Z. Gao, “Towards faster training of global
    covariance pooling networks by iterative matrix square root normalization,” in
    *CVPR*, 2018, pp. 947–955.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Engin, L. Wang, L. Zhou, and X. Liu, “DeepKSPD: Learning kernel-matrix-based
    SPD representation for fine-grained image recognition,” in *ECCV*, 2018, pp. 629–645.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] C. Yu, X. Zhao, Q. Zheng, P. Zhang, and X. You, “Hierarchical bilinear
    pooling for fine-grained visual recognition,” in *ECCV*, 2018, pp. 595–610.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] X. Wei, Y. Zhang, Y. Gong, J. Zhang, and N. Zheng, “Grassmann pooling
    as compact homogeneous bilinear pooling for fine-grained visual classification,”
    in *ECCV*, 2018, pp. 365–380.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] H. Zheng, J. Fu, Z.-J. Zha, and J. Luo, “Learning deep bilinear transformation
    for fine-grained image representation,” in *NIPS*, 2019, pp. 4277–4286.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] S. Min, H. Yao, H. Xie, Z.-J. Zha, and Y. Zhang, “Multi-objective matrix
    normalization for fine-grained visual recognition,” *IEEE TIP*, vol. 29, pp. 4996–5009,
    2020.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] A. Dubey, O. Gupta, R. Raskar, and N. Naik, “Maximum entropy fine-grained
    classification,” in *NIPS*, 2018, pp. 637–647.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell, and N. Naik, “Pairwise
    confusion for fine-grained visual classification,” in *ECCV*, 2018, pp. 71–88.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Y. Gao, X. Han, X. Wang, W. Huang, and M. R. Scott, “Channel interaction
    networks for fine-grained image categorization,” in *AAAI*, 2020, pp. 10 818–10 825.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] G. Sun, H. Cholakkal, S. Khan, F. S. Khan, and L. Shao, “Fine-grained
    recognition: Accounting for subtle differences between similar classes,” in *AAAI*,
    2020, pp. 12 047–12 054.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] P. Zhuang, Y. Wang, and Y. Qiao, “Learning attentive pairwise interaction
    for fine-grained classification,” in *AAAI*, 2020, pp. 2457–2463.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] D. Chang, Y. Ding, J. Xie, A. K. Bhunia, X. Li, Z. Ma, M. Wu, J. Guo,
    and Y.-Z. Song, “The devil is in the channels: Mutual-channel loss for fine-grained
    image classification,” *IEEE TIP*, vol. 29, pp. 4683–4695, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Y. Chen, Y. Bai, W. Zhang, and T. Mei, “Destruction and construction
    learning for fine-grained image recognition,” in *CVPR*, 2019, pp. 5157–5166.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] W. Luo, X. Yang, X. Mo, Y. Lu, L. S. Davis, J. Li, J. Yang, and S.-N.
    Lim, “Cross-X learning for fine-grained visual categorization,” in *ICCV*, 2019,
    pp. 8242–8251.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] R. Du, D. Chang, A. K. Bhunia, J. Xie, Y.-Z. Song, Z. Ma, and J. Guo,
    “Fine-grained visual classification via progressive multi-granularity training
    of Jigsaw patches,” in *ECCV*, 2020, pp. 153–168.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
    object detection with region proposal networks,” in *NIPS*, 2015, pp. 91–99.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” in *CVPR*, 2015, pp. 3431–3440.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *CVPR*, 2014, pp.
    580–587.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial
    pyramid matching for recognizing natural scene categories,” in *CVPR*, 2006, pp.
    1–8.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] M. Guillaumin, D. Küttel, and V. Ferrari, “ImageNet auto-annotation with
    segmentation propagation,” *IJCV*, vol. 110, pp. 328–348, 2014.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] X.-S. Wei, C.-L. Zhang, J. Wu, C. Shen, and Z.-H. Zhou, “Unsupervised
    object discovery and co-localization by deep descriptor transformation,” *PR*,
    vol. 88, pp. 113–126, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] M. Zeiler and R. Fergus, “Visualizing and understanding convolutional
    networks,” in *ECCV*, 2014, pp. 818–833.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] C. M. Bishop, *Pattern recognition and machine learning*.   Springer,
    2006, vol. 1.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *CVPR*, 2016, pp. 2921–2929.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention
    for rapid scene analysis,” *IEEE TPAMI*, vol. 20, no. 11, pp. 1254–1259, 1998.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Corbetta and G. L. Shulman, “Control of goal-directed and stimulus-driven
    attention in the brain,” *Nature Reviews Neuroscience*, vol. 3, pp. 201–215, 2002.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] H. Larochelle and G. E. Hinton, “Learning to combine foveal glimpses
    with a third-order boltzmann machine,” in *NIPS*, 2010, pp. 1243–1251.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] X. He, Y. Peng, and J. Zhao, “Fast fine-grained image classification
    via weakly supervised discriminative localization,” *IEEE TCSVT*, vol. 29, no. 5,
    pp. 1394–1407, 2019.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *CVPR*,
    2018, pp. 7132–7141.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah, “Signature
    verification using a “siamese” time delay neural network,” in *NIPS*, 1993, pp.
    737–744.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] X. He, Y. Peng, and J. Zhao, “StackDRL: Stacked deep reinforcement learning
    for fine-grained visual categorization,” in *IJCAI*, 2018, pp. 741–747.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning:
    A survey,” *JAIR*, vol. 4, pp. 237–285, 1996.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Mu, S. Bhat, and P. Viswanath, “Representing sentences as low-rank
    subspaces,” in *ACL*, 2017, pp. 629–634.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] M. D. Zeiler, G. W. Taylor, and R. Fergus, “Adaptive deconvolutional
    networks for mid and high level feature learning,” in *ICCV*, 2011, pp. 2018–2025.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Z. Xu, Y. Yang, and A. G. Hauptmann, “A discriminative CNN video representation
    for event detection,” in *CVPR*, 2015, pp. 1798–1807.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] M. Cimpoi, S. Maji, and A. Vedaldi, “Deep filter banks for texture recognition
    and segmentation,” in *CVPR*, 2015, pp. 3828–3836.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] B.-B. Gao, X.-S. Wei, J. Wu, and W. Lin, “Deep spatial pyramid: The devil
    is once again in the details,” *arXiv preprint arXiv:1504.05277*, 2015.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] P. H. Gosselin, N. Murray, H. Jégou, and F. Perronnin, “Revisiting the
    fisher vector for fine-grained classification,” in *Pattern Recogn. Letters*,
    no. 49, 2015, pp. 92–98.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] L. Wang, J. Zhang, L. Zhou, C. Tang, and W. Li, “Beyond covariance: Feature
    representation with nonlinear kernel matrices,” in *ICCV*, 2015, pp. 4570–4578.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Q. Wang, J. Xie, W. Zuo, L. Zhang, and P. Li, “Deep CNNs meet global
    covariance pooling: Better representation and generalization,” *IEEE TPAMI*, 2020.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear convolutional neural
    networks for fine-grained visual recognition,” *IEEE TPAMI*, vol. 40, no. 6, pp.
    1309–1322, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] N. Pham and R. Pagh, “Fast and scalable polynomial kernels via explicit
    feature maps,” in *KDD*, 2013, pp. 239–247.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Y. Li, N. Wang, J. Liu, and X. Hou, “Factorized bilinear models for image
    recognition,” in *ICCV*, 2017, pp. 2079–2087.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Sánchez, F. Perronnin, T. Mensink, and J. Verbeek, “Image classification
    with the Fisher vector: Theory and practice,” *IJCV*, vol. 105, no. 3, pp. 222–245,
    2013.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] P. Li, J. Xie, Q. Wang, and W. Zuo, “Is second-order information helpful
    for large-scale visual recognition?” in *ICCV*, 2017, pp. 2070–2078.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] T.-Y. Lin and S. Maji, “Improved bilinear pooling with CNNs,” in *BMVC*,
    2017, pp. 1–12.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] W. Xiong, Y. He, Y. Zhang, W. Luo, L. Ma, and J. Luo, “Fine-grained image-to-image
    transformation towards visual recognition,” in *CVPR*, 2020, pp. 5840–5849.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] X.-S. Wei, P. Wang, L. Liu, C. Shen, and J. Wu, “Piecewise classifier
    mappings: Learning fine-grained learners for novel categories with few examples,”
    *IEEE TIP*, vol. 28, no. 12, pp. 6116–6125, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] S. Xie, T. Yang, X. Wang, and Y. Lin, “Hyper-class augmented and regularized
    deep learning for fine-grained image classification,” in *CVPR*, 2015, pp. 2645–2654.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Z. Xu, S. Huang, Y. Zhang, and D. Tao, “Augmenting strong supervision
    using web data for fine-grained categorization,” in *CVPR*, 2015, pp. 2524–2532.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev, T. Duerig, J. Philbin,
    and L. Fei-Fei, “The unreasonable effectiveness of noisy data for fine-grained
    recognition,” in *ECCV*, 2016, pp. 301–320.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] L. Niu, A. Veeraraghavan, and A. Sabharwal, “Webly supervised learning
    meets zero-shot learning: A hybrid approach for fine-grained classification,”
    in *CVPR*, 2018, pp. 7171–7180.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Y. Zhang, H. Tang, and K. Jia, “Fine-grained visual categorization using
    meta-learning optimization with sample selection of auxiliary data,” in *ECCV*,
    2018, pp. 241–256.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Z. Xu, S. Huang, Y. Zhang, and D. Tao, “Webly-supervised fine-grained
    visual categorization via deep domain adaptation,” *IEEE TPAMI*, vol. 40, no. 4,
    pp. 769–790, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] J. Yang, X. Sun, Y.-K. Lai, L. Zheng, and M.-M. Cheng, “Recognition from
    web data: A progressive filtering approach,” *IEEE TIP*, vol. 27, no. 11, pp.
    5303–5315, 2018.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] C. Zhang, Y. Yao, H. Liu, G.-S. Xie, X. Shu, T. Zhou, Z. Zhang, F. Shen,
    and Z. Tang, “Web-supervised network with softly update-drop training for fine-grained
    visual classification,” in *AAAI*, 2020, pp. 12 781–12 788.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] H. Zhang, X. Cao, and R. Wang, “Audio visual attribute discovery for
    fine-grained object recognition,” in *AAAI*, 2018, pp. 7542–7549.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Xu, G. Qi, J. Li, M. Wang, K. Xu, and H. Gao, “Fine-grained image
    classification by visual-semantic embedding,” in *IJCAI*, 2018, pp. 1043–1049.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] T. Chen, L. Lin, R. Chen, Y. Wu, and X. Luo, “Knowledge-embedded representation
    learning for fine-grained image recognition,” in *IJCAI*, 2018, pp. 627–634.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] K. Song, X.-S. Wei, X. Shu, R.-J. Song, and J. Lu, “Bi-modal progressive
    mask attention for fine-grained recognition,” *IEEE TIP*, vol. 29, pp. 7006–7018,
    2020.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] B. Zhuang, L. Liu, Y. Li, C. Shen, and I. Reid, “Attend in groups: a
    weakly-supervised deep learning framework for learning from web data,” in *CVPR*,
    2017, pp. 1878–1887.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NIPS*, 2014, pp.
    2672–2680.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning
    in neural networks: A survey,” *arXiv preprint arXiv:2004.05439*, 2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes,
    S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer, “DBpedia - A large-scale,
    multilingual knowledge base extracted from Wikipedia,” *Semantic Web Journal*,
    pp. 167–195, 2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] F. M. Zanzotto, “Viewpoint: Human-in-the-loop artificial intelligence,”
    *JAIR*, vol. 64, pp. 243–252, 2019.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Y. Cui, F. Zhou, Y. Lin, and S. Belongie, “Fine-grained categorization
    and dataset bootstrapping using deep metric learning with humans in the loop,”
    in *CVPR*, 2016, pp. 1153–1162.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Deng, J. Krause, M. Stark, and L. Fei-Fei, “Leveraging the wisdom
    of the crowd for fine-grained recognition,” *IEEE TPAMI*, vol. 38, no. 4, pp.
    666–676, 2016.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Cui, Y. Song, C. Sun, A. Howard, and S. Belongie, “Large scale fine-grained
    categorization and domain-specific transfer learning,” in *CVPR*, 2018, pp. 4109–4118.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Q. Wang, L. Zhang, B. Wu, D. Ren, P. Li, W. Zuo, and Q. Hu, “What deep
    CNNs benefit from global covariance pooling: An optimization perspective,” in
    *CVPR*, 2020, pp. 10 771–10 780.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] X. Zheng, R. Ji, X. Sun, Y. Wu, F. Huang, and Y. Yang, “Centralized ranking
    loss with weakly supervised localization for fine-grained object retrieval,” in
    *IJCAI*, 2018, pp. 1226–1233.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] X. Zeng, Y. Zhang, X. Wang, K. Chen, D. Li, and W. Yang, “Fine-grained
    image retrieval via piecewise cross entropy loss,” *Image and Vision Computing*,
    vol. 93, pp. 2868–2881, 2020.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] CVPR 2020 Workshop on RetailVision. [Online]. Available: https://retailvisionworkshop.github.io/'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] H. Idrees, M. Shah, and R. Surette, “Enhancing camera surveillance using
    computer vision: a research note,” *arXiv preprint arXiv:1808.03998*, 2018.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese, “Deep metric learning
    via lifted structured feature embedding,” in *CVPR*, 2016, pp. 4004–4012.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Song, Y.-Z. Song, T. Xiang, T. M. Hospedales, and X. Ruan, “Deep multi-task
    attribute-driven ranking for fine-grained sketch-based image retrieval,” in *BMVC*,
    2016, pp. 1–11.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] F. Radenovic, G. Tolias, and O. Chum, “Deep shape matching,” in *ECCV*,
    2018, pp. 751–767.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] J. Zhang, F. Shen, L. Liu, F. Zhu, M. Yu, L. Shao, H. Tao Shen, and L. Van Gool,
    “Generative domain-migration hashing for sketch-to-image retrieval,” in *ECCV*,
    2018, pp. 297–314.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] K. Pang, Y. Yang, T. M. Hospedales, T. Xiang, and Y.-Z. Song, “Solving
    mixed-modal jigsaw puzzle for fine-grained sketch-based image retrieval,” in *CVPR*,
    2020, pp. 10 347–10 355.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] A. K. Bhunia, Y. Yang, T. M. Hospedales, T. Xiang, and Y.-Z. Song, “Sketch
    less for more: On-the-fly fine-grained sketch-based image retrieval,” in *CVPR*,
    2020, pp. 9779–9788.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] A. Sain, A. K. Bhunia, Y. Yang, T. Xiang, and Y.-Z. Song, “Cross-modal
    hierarchical modelling for fine-grained sketch based image retrieval,” in *BMVC*,
    2020, pp. 1–14.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] R. Girshick, F. Iandola, T. Darrell, and J. Malik, “Deformable part models
    are convolutional neural networks,” in *CVPR*, 2015, pp. 437–446.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] K. Pang, Y.-Z. Song, T. Xiang, and T. M. Hospdales, “Cross-domain generative
    learning for fine-grained sketch-based image retrieval,” in *BMVC*, 2017, pp.
    1–12.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] J. Song, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Fine-grained image
    retrieval: the text/sketch input dilemma,” in *BMVC*, 2017, pp. 1–12.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation
    learning by predicting image rotations,” in *ICLR*, 2018, pp. 1–16.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
    unsupervised visual representation learning,” in *CVPR*, 2020, pp. 9729–9738.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
    for contrastive learning of visual representations,” in *ICML*, 2020, pp. 1597–1607.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] F. Liu, C. Zou, X. Deng, R. Zuo, Y.-K. Lai, C. Ma, Y.-J. Liu, and H. Wang,
    “SceneSketcher: Fine-grained image retrieval with scene sketches,” in *ECCV*,
    2020, pp. 718–734.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] K. Musgrave, S. Belongie, and S.-N. Lim, “A metric learning reality check,”
    in *ECCV*, 2020, pp. 681–699.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Y. Cui, Z. Gu, D. Mahajan, L. van der Maaten, S. Belongie, and S.-N.
    Lim, “Measuring dataset granularity,” *arXiv preprint arXiv:1912.10154*, 2019.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] D. Chang, K. Pang, Y. Zheng, Z. Ma, Y.-Z. Song, and J. Guo, “Your “flamingo”
    is my “bird”: Fine-grained, or not,” *arXiv preprint arXiv:2011.09040*, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] X. Wang, L. Lian, Z. Miao, Z. Liu, and S. X. Yu, “Long-tailed recognition
    by routing diverse distribution-aware experts,” in *ICLR*, 2021, pp. 1–15.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] C. Li, D. Du, L. Zhang, T. Luo, Y. Wu, Q. Tian, L. Wen, and S. Lyu, “Data
    priming network for automatic check-out,” in *ACM MM*, 2019, pp. 2152–2160.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE TPAMI*, 2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] S. I. Nikolenko, “Synthetic data for deep learning,” *arXiv preprint
    arXiv:1909.11512*, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker, “Fine-grained 3D shape classification
    with hierarchical part-view attention,” *IEEE TIP*, vol. 30, pp. 1744–1758, 2021.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] E. Cole, X. Yang, K. Wilber, O. Mac Aodha, and S. Belongie, “When does
    contrastive visual representation learning work?” *arXiv preprint arXiv:2105.05837*,
    2021.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Q. Zhang and S.-C. Zhu, “Visual interpretability for deep learning: a
    survey,” *arXiv preprint arXiv:1802.00614*, 2018.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] S. Tsutsui, Y. Fu, and D. Crandall, “Meta-reinforced synthetic data for
    one-shot fine-grained visual recognition,” in *NIPS*, 2019, pp. 3063–3072.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] L. Tang, D. Wertheimer, and B. Hariharan, “Revisiting pose-normalization
    for fine-grained few-shot recognition,” in *CVPR*, 2020.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] J.-C. Su, S. Maji, and B. Hariharan, “When does self-supervision improve
    few-shot learning?” in *ECCV*, 2020, pp. 645–666.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] J. Wang, T. Zhang, J. Song, N. Sebe, and H. T. Shen, “A survey on learning
    to hash,” *IEEE TPAMI*, vol. 40, no. 4, pp. 769–790, 2018.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep supervised
    hashing with pairwise labels,” in *IJCAI*, 2016, pp. 1711–1717.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Q. Cui, Q.-Y. Jiang, X.-S. Wei, W.-J. Li, and O. Yoshie, “ExchNet: A
    unified hashing network for large-scale fine-grained image retrieval,” in *ECCV*,
    2020, pp. 189–205.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] S. Jin, H. Yao, X. Sun, S. Zhou, L. Zhang, and X. Hua, “Deep saliency
    hashing for fine-grained retrieval,” *IEEE TIP*, vol. 29, pp. 5336–5351, 2020.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter,
    “Efficient and robust automated machine learning,” in *NIPS*, 2015, pp. 2962–2970.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search:
    A survey,” *arXiv preprint arXiv:1808.05377*, 2018.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] T. Gebru, J. Hoffman, and L. Fei-Fei, “Fine-grained recognition in the
    wild: A multi-task domain adaptation approach,” in *ICCV*, 2017, pp. 1349–1358.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] S. Wang, X. Chen, Y. Wang, M. Long, and J. Wang, “Progressive adversarial
    networks for fine-grained domain adaptation,” in *CVPR*, 2020, pp. 9213–9222.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Y. Wang, R.-J. Song, X.-S. Wei, and L. Zhang, “An adversarial domain
    adaptation network for cross-domain fine-grained recognition,” in *WACV*, 2020,
    pp. 1228–1236.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, “Class-balanced
    loss based on effective number of samples,” in *CVPR*, 2019, pp. 9268–9277.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] B. Zhou, Q. Cui, X.-S. Wei, and Z.-M. Chen, “BBN: Bilateral-branch network
    with cumulative learning for long-tailed visual recognition,” in *CVPR*, 2020,
    pp. 9719–9728.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, and S. X. Yu, “Large-scale
    long-tailed recognition in an open world,” in *CVPR*, 2019, pp. 2537–2546.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] C. Zhu, X. Tan, F. Zhou, X. Liu, K. Yue, E. Ding, and Y. Ma, “Fine-grained
    video categorization with redundancy reduction attention,” in *ECCV*, 2018, pp.
    136–152.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] J. Munro and D. Damen, “Multi-modal domain adaptation for fine-grained
    action recognition,” in *CVPR*, 2020, pp. 122–132.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/05ad5090690cea4eb72178ef5bdef883.png) | Xiu-Shen
    Wei is a Professor with the School of Computer Science and Engineering, Nanjing
    University of Science and Technology, China. He was a Program Chair for the workshops
    associated with ICCV, IJCAI, ACM Multimedia, etc. He has also served as a Guest
    Editor of Pattern Recognition Journal, and a Tutorial Chair for Asian Conference
    on Computer Vision (ACCV) 2022. |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4ef43107113907e8467948811b5f5f64.png) | Yi-Zhe
    Song is a Chair Professor of Computer Vision and Machine Learning, and Director
    of SketchX Lab at the Centre for Vision Speech and Signal Processing (CVSSP),
    University of Surrey, UK. He is an Associate Editor of the IEEE Transactions on
    Pattern Analysis and Machine Intelligence (TPAMI), and a Program Chair for British
    Machine Vision Conference (BMVC) 2021. |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/83dcfd08794eeab2ddfc055e91b98ec4.png) | Oisin
    Mac Aodha is a Lecturer in Machine Learning at the University of Edinburgh, UK.
    He is a Fellow of the Alan Turing Institute and a European Laboratory for Learning
    and Intelligent Systems (ELLIS) Scholar. He was a Program Chair for the British
    Machine Vision Conference (BMVC) 2020. |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/657be029ff811b9c25127daae75da0ea.png) | Jianxin
    Wu is currently a professor in the Department of Computer Science and Technology
    at Nanjing University, China, and is associated with the State Key Laboratory
    for Novel Software Technology, China. |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/0754ae2d34ca88929d69cd3613b04e27.png) | Yuxin
    Peng is currently the Boya Distinguished Professor with Wangxuan Institute of
    Computer Technology, Peking University, China. |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/d9734939cd452c0579a4295b236f8b05.png) | Jinhui
    Tang is a Professor with the School of Computer Science and Engineering, Nanjing
    University of Science and Technology, China. |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/8b428719a1bae07dff747a7a160e0ac6.png) | Jian Yang
    is a Chang-Jiang professor in the School of Computer Science and Technology of
    Nanjing University of Science and Technology, China. |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/da39ca57717b2963e3d0dfd587a080a3.png) | Serge
    Belongie is a professor of Computer Science at the University of Copenhagen (DIKU)
    and the director of the Pioneer Centre for AI, Denmark. |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
