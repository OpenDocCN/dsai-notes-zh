- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:49:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:49:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2111.06119] Fine-Grained Image Analysis with Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2111.06119] 基于深度学习的细粒度图像分析：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.06119](https://ar5iv.labs.arxiv.org/html/2111.06119)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2111.06119](https://ar5iv.labs.arxiv.org/html/2111.06119)
- en: 'Fine-Grained Image Analysis with Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的细粒度图像分析：综述
- en: Xiu-Shen Wei, , Yi-Zhe Song, , Oisin Mac Aodha, Jianxin Wu, , Yuxin Peng, ,
    Jinhui Tang, , Jian Yang, , Serge Belongie X.-S. Wei and J. Yang are with PCA
    Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information
    of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding
    for Social Security, School of Computer Science and Engineering, Nanjing University
    of Science and Technology, China. Y.-Z. Song is with University of Surrey, UK.
    O. Mac Aodha is with the University of Edinburgh, UK. J. Wu is the State Key Laboratory
    for Novel Software Technology, Nanjing University, China. Y. Peng is with Peking
    University, China. J. Tang is with Nanjing University of Science and Technology,
    China. S. Belongie is with the University of Copenhagen and the Pioneer Centre
    for AI, Denmark.X.-S. Wei and J. Yang are the corresponding authors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xiu-Shen Wei, , Yi-Zhe Song, , Oisin Mac Aodha, Jianxin Wu, , Yuxin Peng, ,
    Jinhui Tang, , Jian Yang, , Serge Belongie X.-S. Wei 和 J. Yang 现任职于中国南京理工大学计算机科学与工程学院，教育部高维信息智能感知与系统重点实验室，以及江苏省社会安全图像与视频理解重点实验室。Y.-Z.
    Song 现任职于英国萨里大学。O. Mac Aodha 现任职于英国爱丁堡大学。J. Wu 是中国南京大学新型软件技术国家重点实验室的成员。Y. Peng
    现任职于中国北京大学。J. Tang 现任职于中国南京理工大学。S. Belongie 现任职于丹麦哥本哈根大学和先锋AI中心。X.-S. Wei 和 J.
    Yang 是通讯作者。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-grained image analysis (FGIA) is a longstanding and fundamental problem
    in computer vision and pattern recognition, and underpins a diverse set of real-world
    applications. The task of FGIA targets analyzing visual objects from subordinate
    categories, *e.g.*, species of birds or models of cars. The small inter-class
    and large intra-class variation inherent to fine-grained image analysis makes
    it a challenging problem. Capitalizing on advances in deep learning, in recent
    years we have witnessed remarkable progress in deep learning powered FGIA. In
    this paper we present a systematic survey of these advances, where we attempt
    to re-define and broaden the field of FGIA by consolidating two fundamental fine-grained
    research areas – fine-grained image recognition and fine-grained image retrieval.
    In addition, we also review other key issues of FGIA, such as publicly available
    benchmark datasets and related domain-specific applications. We conclude by highlighting
    several research directions and open problems which need further exploration from
    the community.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度图像分析（FGIA）是计算机视觉和模式识别领域中的一个长期存在且基础性的问题，并支撑着多种现实世界的应用。FGIA 任务的目标是分析来自下属类别的视觉对象，例如鸟类的不同物种或汽车的不同型号。细粒度图像分析固有的小类间变异和大类内变异，使其成为一个具有挑战性的问题。凭借深度学习的进步，近年来我们见证了基于深度学习的
    FGIA 取得了显著的进展。在本文中，我们系统地回顾了这些进展，尝试通过整合两个基础性的细粒度研究领域——细粒度图像识别和细粒度图像检索，来重新定义和拓宽
    FGIA 领域。此外，我们还审视了 FGIA 的其他关键问题，如公开可用的基准数据集和相关的领域特定应用。最后，我们强调了几个需要社区进一步探索的研究方向和未解的问题。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Fine-Grained Images Analysis; Deep Learning; Fine-Grained Image Recognition;
    Fine-Grained Image Retrieval.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度图像分析；深度学习；细粒度图像识别；细粒度图像检索。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'The human visual system is inherently capable of fine-grained image reasoning
    – we are not only able to tell a dog from a bird, but also know the difference
    between a Siberian Husky and an Alaskan Malamute (see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")).
    Fine-grained image analysis (FGIA) was introduced to the academic community for
    the very same purpose, *i.e.*, to teach machine to “see” in a fine-grained manner.
    FGIA approaches are present in a wide-range of applications in both industry and
    research, with examples including automatic biodiversity monitoring [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)], intelligent retail [[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6)], and intelligent transportation [[7](#bib.bib7), [8](#bib.bib8)],
    and have resulted in a positive impact in areas such as conservation [[9](#bib.bib9)]
    and commerce [[10](#bib.bib10)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类视觉系统天生具备细粒度图像推理能力——我们不仅能够区分狗和鸟，还能分辨西伯利亚哈士奇和阿拉斯加马拉穆特（见图[1](#S1.F1 "图 1 ‣ 1
    引言 ‣ 基于深度学习的细粒度图像分析：综述")）。细粒度图像分析（FGIA）正是为了同样的目的而引入学术界的，即教会机器以细粒度的方式“看”。FGIA 方法在工业和研究的广泛应用中都存在，例子包括自动生物多样性监测 [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]，智能零售 [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]，和智能交通 [[7](#bib.bib7),
    [8](#bib.bib8)]，并在保护 [[9](#bib.bib9)]和商业 [[10](#bib.bib10)]等领域产生了积极的影响。
- en: The goal of FGIA in computer vision is to retrieve and recognize images belonging
    to multiple subordinate categories of a super-category (*aka* a meta-category
    or a basic-level category), *e.g.*, different species of animals/plants, different
    models of cars, different kinds of retail products, etc. The key challenge therefore
    lies with understanding fine-grained visual differences that sufficiently discriminate
    between objects that are highly similar in overall appearance, but differ in *fine-grained*
    features. Great strides has been made since its inception almost two decades ago [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]. Deep learning [[14](#bib.bib14)] in particular
    has emerged as a powerful method for discriminative feature learning, and has
    led to remarkable breakthroughs in the field of FGIA. Deep learning enabled FGIA
    has greatly advanced the practical deployment of these methods in a diverse set
    of application scenarios [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [5](#bib.bib5)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: FGIA 在计算机视觉中的目标是检索和识别属于多个子类别的图像（*即*一个超类别的子类别，或称为元类别或基础级类别），*例如*，不同种类的动物/植物，不同型号的汽车，不同种类的零售产品等。因此，关键挑战在于理解细粒度的视觉差异，这些差异足以区分在整体外观上非常相似但在
    *细粒度* 特征上有所不同的对象。自近二十年前其创立以来，已经取得了重大进展 [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]。特别是深度学习 [[14](#bib.bib14)]
    作为一种强大的特征学习方法，已经在 FGIA 领域取得了显著突破。深度学习使得 FGIA 的实际应用在各种应用场景中得到了极大推进 [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [5](#bib.bib5)]。
- en: '![Refer to caption](img/92883f479697bb651a8cfc37ad6a2d82.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/92883f479697bb651a8cfc37ad6a2d82.png)'
- en: 'Figure 1: Fine-grained image analysis *vs*. generic image analysis (using visual
    classification as an example).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：细粒度图像分析 *vs*. 通用图像分析（以视觉分类为例）。
- en: '![Refer to caption](img/d7f9a195b9ca52777aa212fc79eb83b4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d7f9a195b9ca52777aa212fc79eb83b4.png)'
- en: 'Figure 2: Overview of the landscape of deep learning based fine-grained image
    analysis (FGIA), as well as future directions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度学习基础的细粒度图像分析（FGIA）领域概览，以及未来的方向。
- en: There has been significant interest in FGIA from both the computer vision and
    machine learning research communities in recent years. Rough statistics indicate
    an average of $>10$ conference papers on deep learning based FGIA are published
    every year in each of the premium vision and machine learning conferences. There
    have also been a number of special issues addressing FGIA [[15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]. Additionally, a number
    of influential competitions on FGIA are frequently held on online platforms. Representatives
    include the series of iNaturalist Competitions (for large numbers of natural species) [[20](#bib.bib20)],
    the Nature Conservancy Fisheries Monitoring (for fish species categorization) [[21](#bib.bib21)],
    Humpback Whale Identification (for whale identity categorization) [[22](#bib.bib22)],
    among others. Each competition attracted hundreds of participants from around
    the world, and some even exceeding 2,000 teams. Specific tutorials and workshops
    aimed at FGIA topics have also been held at top-tier international conferences,
    *e.g.*, [[23](#bib.bib23), [24](#bib.bib24)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机视觉和机器学习研究社区对FGIA表现出显著的兴趣。粗略统计表明，每年在顶级视觉和机器学习会议上平均有$>10$篇基于深度学习的FGIA会议论文发表。也有许多特别期刊专门讨论FGIA [[15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]。此外，一些有影响力的FGIA竞赛经常在在线平台上举行。代表性赛事包括iNaturalist竞赛系列（用于大量自然物种）[[20](#bib.bib20)]、自然保护协会渔业监测（用于鱼类物种分类）[[21](#bib.bib21)]、座头鲸识别（用于鲸鱼身份分类）[[22](#bib.bib22)]等。每个竞赛都吸引了来自世界各地的数百名参赛者，有些甚至超过了2,000支队伍。针对FGIA主题的特定教程和研讨会也曾在顶级国际会议上举行，*例如*[[23](#bib.bib23),
    [24](#bib.bib24)]。
- en: Despite such salient interest, the study of FGIA with deep learning remains
    fragmented. It is therefore the purpose of this survey to (i) provide a comprehensive
    survey of recent achievements in FGIA, especially those brought by deep learning
    techniques, and more importantly (ii) to propose a unified research front by consolidating
    research from different aspects of FGIA. Our approach is significantly different
    to existing surveys [[25](#bib.bib25), [26](#bib.bib26)] that focus solely on
    the problem of fine-grained *recognition/classification*, which as we argue only
    constitutes part of the larger study of FGIA. In particular, we attempt to re-define
    and broaden the field of fine-grained image analysis, by highlighting the synergy
    between fine-grained recognition, and the parallel but complementary task of fine-grained
    image *retrieval*, which is also an integral part of FGIA.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此显著的兴趣，FGIA的深度学习研究仍然显得零散。因此，本调查的目的在于（i）提供对FGIA最新成就的全面调查，尤其是深度学习技术带来的成就，更重要的是（ii）通过整合FGIA不同方面的研究，提出一个统一的研究前沿。我们的方法与现有的调查[[25](#bib.bib25),
    [26](#bib.bib26)]显著不同，这些调查仅关注细粒度*识别/分类*问题，而我们认为这只是FGIA更大研究的一部分。特别是，我们试图重新定义和拓宽细粒度图像分析领域，通过突显细粒度识别与细粒度图像*检索*这两个平行但互补的任务之间的协同效应，而细粒度图像检索也是FGIA的一个重要组成部分。
- en: 'Our survey takes a unique deep learning based perspective to review recent
    advances in FGIA in a broad, systematic, and comprehensive manner. Our main contributions
    are summarized as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查从深度学习的独特视角系统、全面地回顾了FGIA领域的最新进展。我们的主要贡献总结如下：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We broaden the field of FGIA, by offering a consolidated landscape that promotes
    synergies between related problems in fine-grained image analysis.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过提供一个整合的视野来拓宽FGIA的领域，这个视野促进了细粒度图像分析中相关问题之间的协同效应。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We provide a comprehensive review of FGIA techniques based on deep learning,
    including commonly accepted problem definitions, benchmark datasets, different
    families of FGIA methods, along with covering domain-specific FGIA applications.
    Particularly, we organize these approaches taxonomically (see Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Fine-Grained Image Analysis with Deep Learning: A
    Survey")) to provide readers with a quick snapshot of the state-of-the-art in
    this area.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了基于深度学习的FGIA技术的全面综述，包括普遍接受的问题定义、基准数据集、不同类别的FGIA方法，并覆盖了领域特定的FGIA应用。特别地，我们将这些方法按类别组织（见图 [2](#S1.F2
    "图 2 ‣ 1 介绍 ‣ 基于深度学习的细粒度图像分析：一项调查")），以便读者快速了解该领域的最新进展。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We consolidate the performance of existing methods on several publicly available
    datasets and provide discussion and insights to inform future research.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们整合了现有方法在几个公开数据集上的表现，并提供讨论和见解，以指导未来的研究。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We finish by discussing existing challenges and open issues, and identify new
    trends and future directions to provide a plausible road map for the community
    to address these problems.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们最后讨论现有的挑战和开放问题，并确定新的趋势和未来方向，以为社区提供一个可行的路线图，以应对这些问题。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Finally, in an attempt to continuously track recent developments in this fast
    advancing field, we provide an accompanying webpage which catalogs papers addressing
    FGIA problems, according to our problem-based taxonomy: http://www.weixiushen.com/project/Awesome_FGIA/Awesome_FGIA.html.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，为了持续跟踪这个快速发展的领域的最新进展，我们提供了一个附属网页，按我们基于问题的分类法对解决 FGIA 问题的论文进行分类：http://www.weixiushen.com/project/Awesome_FGIA/Awesome_FGIA.html。
- en: 2 Recognition vs. Retrieval
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 识别与检索
- en: 'Previous surveys of FGIA, *e.g.*, [[25](#bib.bib25), [26](#bib.bib26)], predominately
    focused on fine-grained recognition, and as a result do not expose all facets
    of the FGIA problem. In this survey, we cover two fundamental areas of fine-grained
    image analysis for the first time (*i.e.*, recognition and retrieval) in order
    to give a comprehensive review of recent advances in deep learning based FGIA
    techniques. In Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Fine-Grained Image
    Analysis with Deep Learning: A Survey"), we provide a new taxonomy that reflects
    the current FGIA landscape.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '之前关于 FGIA 的调查，如[[25](#bib.bib25), [26](#bib.bib26)]，主要集中在细粒度识别上，因此没有暴露 FGIA
    问题的所有方面。在本次调查中，我们首次涵盖了细粒度图像分析的两个基本领域（*即*，识别和检索），以全面回顾基于深度学习的 FGIA 技术的最新进展。在图 [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Fine-Grained Image Analysis with Deep Learning: A
    Survey")中，我们提供了一种新的分类法，反映了当前 FGIA 的格局。'
- en: 'Fine-Grained Recognition: We organize the different families of fine-grained
    *recognition* approaches into three paradigms, *i.e.*, 1) recognition by localization-classification
    subnetworks, 2) recognition by end-to-end feature encoding, and 3) recognition
    with external information. Fine-grained recognition is the most studied area in
    FGIA, since recognition is a fundamental ability of most visual systems and is
    thus worthy of long-term continuous research.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度识别：我们将不同的细粒度*识别*方法组织为三种范式，即 1) 通过定位-分类子网络进行识别，2) 通过端到端特征编码进行识别，3) 利用外部信息进行识别。细粒度识别是
    FGIA 中研究最为广泛的领域，因为识别是大多数视觉系统的基本能力，因此值得长期持续研究。
- en: 'Fine-Grained Retrieval: Based on the type of query image, we separate fine-grained
    *retrieval* methods into two groups, *i.e.*, 1) content-based fine-grained image
    retrieval and 2) sketch-based fine-grained image retrieval. Compared with fine-grained
    recognition, fine-grained retrieval is an emerging area of FGIA in recent years,
    one that is attracting more and more attention from both academia and industry.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度检索：根据查询图像的类型，我们将细粒度*检索*方法分为两组，即 1) 基于内容的细粒度图像检索和 2) 基于草图的细粒度图像检索。与细粒度识别相比，细粒度检索近年来是
    FGIA 中一个新兴领域，越来越受到学术界和工业界的关注。
- en: 'Recognition and Retrieval Differences: Both fine-grained recognition and retrieval
    aim to identify the discriminative, but subtle, differences between different
    fine-grained objects. However, fine-grained recognition is a closed-world task
    with a *fixed* number of subordinate categories. In contrast, fine-grained retrieval
    extends the problem to an open-world setting with unlimited sub-categories. Furthermore,
    fine-grained retrieval also aims to rank all the instances so that images depicting
    the concept of interest (*e.g.*, the same sub-category label) are ranked highest
    based on the fine-grained details in the query.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 识别与检索的差异：细粒度识别和检索都旨在识别不同细粒度对象之间的辨别性但微妙的差异。然而，细粒度识别是一个封闭世界任务，具有*固定*数量的从属类别。相比之下，细粒度检索将问题扩展到具有无限子类别的开放世界环境。此外，细粒度检索还旨在对所有实例进行排序，以便基于查询中的细粒度细节，将展示感兴趣概念（*例如*，相同子类别标签）的图像排名最高。
- en: 'Recognition and Retrieval Synergies: Advances in fine-grained recognition and
    retrieval have commonalities and can benefit each other. Many common techniques
    are shared by both fine-grained recognition and retrieval, *e.g.*, deep metric
    learning methods [[27](#bib.bib27), [28](#bib.bib28)], multi-modal matching methods [[29](#bib.bib29),
    [30](#bib.bib30)], and the basic ideas of selecting useful deep descriptors [[31](#bib.bib31),
    [32](#bib.bib32)], etc. Detailed discussions are elaborated in Section [7](#S7
    "7 Common Techniques Shared by both Fine-Grained Recognition and Retrieval ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"). Furthermore, in real-world applications,
    fine-grained recognition and retrieval also compliment each other, *e.g.*, retrieval
    techniques are able to support novel sub-category recognition by utilizing learned
    representations from a fine-grained recognition model [[33](#bib.bib33), [5](#bib.bib5)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '识别与检索的协同作用：细粒度识别和检索的进展有共通之处，且可以相互受益。细粒度识别和检索共享许多常见技术，如深度度量学习方法[[27](#bib.bib27),
    [28](#bib.bib28)]、多模态匹配方法[[29](#bib.bib29), [30](#bib.bib30)]以及选择有用的深度描述符的基本理念[[31](#bib.bib31),
    [32](#bib.bib32)]等。详细讨论见第[7](#S7 "7 Common Techniques Shared by both Fine-Grained
    Recognition and Retrieval ‣ Fine-Grained Image Analysis with Deep Learning: A
    Survey")节。此外，在实际应用中，细粒度识别和检索也相互补充，如检索技术能够通过利用从细粒度识别模型中学到的表示来支持新子类别识别[[33](#bib.bib33),
    [5](#bib.bib5)]。'
- en: '3 Background: Problem and Challenges'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 背景：问题与挑战
- en: 'Fine-grained image analysis (FGIA) focuses on dealing with objects belonging
    to multiple *subordinate categories* of the same meta-category (*e.g.*, different
    species of birds or different models of cars), and generally involves two central
    tasks: fine-grained image recognition and fine-grained image retrieval. As illustrated
    in Figure [3](#S3.F3 "Figure 3 ‣ 3 Background: Problem and Challenges ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"), fine-grained analysis lies in the
    continuum between basic-level category analysis (*i.e.*, generic image analysis)
    and instance-level analysis (*e.g.*, the identification of individuals).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '细粒度图像分析（FGIA）关注处理属于同一元类别的多个*下属类别*的对象（*例如*，不同鸟类的物种或不同型号的汽车），通常涉及两个核心任务：细粒度图像识别和细粒度图像检索。如图[3](#S3.F3
    "Figure 3 ‣ 3 Background: Problem and Challenges ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey")所示，细粒度分析处于基本级别类别分析（*即*，通用图像分析）和实例级别分析（*例如*，个体识别）之间的连续体上。'
- en: 'Specifically, what distinguishes FGIA from generic image analysis is that in
    generic image analysis, target objects belong to coarse-grained meta-categories
    (*i.e.*, basic-level categories) and are thus visually quite different (*e.g.*,
    determining if an image contains a bird, a fruit, or a dog). However, in FGIA,
    since objects typically come from sub-categories of the same meta-category, the
    fine-grained nature of the problem causes them to be visually similar. As an example
    of fine-grained recognition, in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Fine-Grained Image Analysis with Deep Learning: A Survey"), the task is to classify
    different breeds of dogs. For accurate image recognition, it is necessary to capture
    the subtle visual differences (*e.g.*, discriminative features such as ears, noses,
    or tails). Characterizing such features is also desirable for other FGIA tasks
    (*e.g.*, retrieval). Furthermore, as noted earlier, the fine-grained nature of
    the problem is challenging because of the *small inter-class variations* caused
    by highly similar sub-categories, and the *large intra-class variations* in poses,
    scales and rotations (see Figure [4](#S3.F4 "Figure 4 ‣ 3 Background: Problem
    and Challenges ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")).
    It is as such the opposite of generic image analysis (*i.e.*, the small intra-class
    variations and the large inter-class variations), and what makes FGIA a unique
    and challenging problem.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '具体而言，将FGIA与通用图像分析区分开的因素在于，通用图像分析中，目标对象属于粗粒度的元类别（*即*，基本层级类别），因此在视觉上非常不同（*例如*，确定图像是否包含鸟类、水果或狗）。然而，在FGIA中，由于对象通常来自相同元类别的子类别，问题的精细化特性导致它们在视觉上相似。作为精细识别的一个例子，在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Fine-Grained Image Analysis with Deep Learning: A
    Survey")中，任务是分类不同品种的狗。为了准确的图像识别，需要捕捉微妙的视觉差异（*例如*，耳朵、鼻子或尾巴等区分特征）。对这些特征进行表征对于其他FGIA任务（*例如*，检索）也是非常有益的。此外，如前所述，问题的精细化特性具有挑战性，因为由高度相似的子类别造成的*类间小变异*，以及姿势、尺度和旋转中的*类内大变异*（参见图[4](#S3.F4
    "Figure 4 ‣ 3 Background: Problem and Challenges ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey")）。这与通用图像分析（*即*，小的类内变异和大的类间变异）正好相反，使得FGIA成为一个独特且具有挑战性的问题。'
- en: 'While instance-level analysis typically targets a *specific instance* of an
    object not just object categories or even object sub-categories, if we move down
    the spectrum of granularity, in the extreme, individual identification (*e.g.*,
    face identification) can be viewed as a special instance of fine-grained recognition,
    where the granularity is at the individual identity level. For instance, person/vehicle
    re-identification [[34](#bib.bib34), [7](#bib.bib7)] can be considered a fine-grained
    task, which aims to determine whether two images are taken of the same specific
    person/vehicle. In practice, these works solve the corresponding domain-specific
    problems using related methods to FGIA, *e.g.*, by capturing the discriminative
    parts of objects (faces, people, and vehicles) [[35](#bib.bib35), [8](#bib.bib8),
    [36](#bib.bib36)], discovering coarse-to-fine structural information [[37](#bib.bib37)],
    developing attribute-based models [[38](#bib.bib38), [39](#bib.bib39)], and so
    on. Research in these instance-level problems is also very active. However, since
    such problems are not within the scope of classical FGIA (see Figure [3](#S3.F3
    "Figure 3 ‣ 3 Background: Problem and Challenges ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey")), for more information, we refer readers to survey
    papers of these specific topics, *e.g.*, [[34](#bib.bib34), [7](#bib.bib7), [40](#bib.bib40)].
    In the following, we start by formulating our definition of classical FGIA.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然实例级分析通常针对对象的*特定实例*，而不仅仅是对象类别甚至对象子类别，如果我们进一步深入粒度范围，在极端情况下，个体识别（*例如*，面部识别）可以被视为精细识别的特殊实例，其中粒度水平是个体身份级别。例如，人/车辆重识别[[34](#bib.bib34),
    [7](#bib.bib7)]可以被认为是一个精细的任务，其目的是确定两张图像是否拍摄了同一个特定的人/车辆。在实践中，这些工作通过使用与FGIA相关的方法来解决相应的领域特定问题，*例如*，通过捕捉对象（面部、人员和车辆）的区分部分[[35](#bib.bib35),
    [8](#bib.bib8), [36](#bib.bib36)]，发现从粗到细的结构信息[[37](#bib.bib37)]，开发基于属性的模型[[38](#bib.bib38),
    [39](#bib.bib39)]，等等。这些实例级问题的研究也非常活跃。然而，由于这些问题不在经典FGIA的范围内（参见图[3](#S3.F3 "Figure
    3 ‣ 3 Background: Problem and Challenges ‣ Fine-Grained Image Analysis with Deep
    Learning: A Survey")），有关更多信息，我们请读者参考这些特定主题的调查论文，*例如*，[[34](#bib.bib34), [7](#bib.bib7),
    [40](#bib.bib40)]。接下来，我们将开始制定经典FGIA的定义。'
- en: '![Refer to caption](img/11ee2fa96655b9a399d5c60c250e7f4a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/11ee2fa96655b9a399d5c60c250e7f4a.png)'
- en: 'Figure 3: An illustration of fine-grained image analysis which lies in the
    continuum between the basic-level category analysis (*i.e.*, generic image analysis)
    and the instance-level analysis (*e.g.*, car identification).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：细粒度图像分析的示意图，位于基础类别分析（*即*，通用图像分析）和实例级分析（*例如*，汽车识别）之间的连续体。
- en: '![Refer to caption](img/b87dc972e883cda1297d839f00ac0222.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b87dc972e883cda1297d839f00ac0222.png)'
- en: 'Figure 4: Key challenges of fine-grained image analysis, *i.e.*, small inter-class
    variations and large intra-class variations. Here we present four different Tern
    species from [[13](#bib.bib13)], one species per row, with different instances
    in the columns.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：细粒度图像分析的关键挑战，*即*，小的类间变异和大的类内变异。这里我们展示了来自[[13](#bib.bib13)]的四种不同的Tern物种，每行一种，每列展示不同实例。
- en: 'Formulation: In generic image recognition, we are given a training dataset
    $\mathcal{D}=\left\{\left(\bm{x}^{(n)},y^{(n)}\right)|i=1,...,N\right\}$, containing
    multiple images and associated class labels (*i.e.*, $\bm{x}$ and $y$), where
    $y\in[1,...,C]$. Each instance $\left(\bm{x},y\right)$ belongs to the joint space
    of both the image and label spaces (*i.e.*, $\mathcal{X}$ and $\mathcal{Y}$, respectively),
    according to the distribution of $p_{r}(\bm{x},y)$'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 公式：在通用图像识别中，我们给定一个训练数据集$\mathcal{D}=\left\{\left(\bm{x}^{(n)},y^{(n)}\right)|i=1,...,N\right\}$，包含多个图像和相关的类别标签（*即*，$\bm{x}$和$y$），其中$y\in[1,...,C]$。每个实例$\left(\bm{x},y\right)$属于图像和标签空间的联合空间（*即*，$\mathcal{X}$和$\mathcal{Y}$），根据$p_{r}(\bm{x},y)$的分布
- en: '|  | $\left(\bm{x},y\right)\in\mathcal{X}\times\mathcal{Y}\,.$ |  | (1) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left(\bm{x},y\right)\in\mathcal{X}\times\mathcal{Y}\,.$ |  | (1) |'
- en: In particular, the label space $\mathcal{Y}$ is the union space of all the $C$
    subspaces corresponding to the $C$ categories, *i.e.*, $\mathcal{Y}=\mathcal{Y}_{1}\cup\mathcal{Y}_{2}\cup\cdots\cup\mathcal{Y}_{c}\cup\cdots\cup\mathcal{Y}_{C}$.
    Then, we can train a predictive/recognition deep network $f(\bm{x};\theta)$ parameterized
    by $\theta$ for generic image recognition by minimizing the expected risk
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，标签空间$\mathcal{Y}$是所有$C$个子空间的并集，对应于$C$个类别，*即*，$\mathcal{Y}=\mathcal{Y}_{1}\cup\mathcal{Y}_{2}\cup\cdots\cup\mathcal{Y}_{c}\cup\cdots\cup\mathcal{Y}_{C}$。然后，我们可以通过最小化期望风险来训练一个由$\theta$参数化的预测/识别深度网络$f(\bm{x};\theta)$，用于通用图像识别。
- en: '|  | $\min_{\theta}\mathbb{E}_{(\bm{x},y)\sim p_{r}(\bm{x},y)}\left[\mathcal{L}(y,f(\bm{x};\theta))\right]\,,$
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathbb{E}_{(\bm{x},y)\sim p_{r}(\bm{x},y)}\left[\mathcal{L}(y,f(\bm{x};\theta))\right]\,,$
    |  | (2) |'
- en: where $\mathcal{L}(\cdot,\cdot)$ is a loss function that measures the match
    between the true labels and those predicted by $f(\cdot;\theta)$. While, as aforementioned,
    fine-grained recognition aims to accurately classify instances of different subordinate
    categories from a certain meta-category, *i.e.*,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{L}(\cdot,\cdot)$是一个损失函数，用于衡量真实标签与$f(\cdot;\theta)$预测标签之间的匹配程度。正如前面提到的，细粒度识别旨在准确分类来自某一元类别的不同子类别的实例，*即*，
- en: '|  | $\left(\bm{x},y^{\prime}\right)\in\mathcal{X}\times\mathcal{Y}_{c}\,,$
    |  | (3) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left(\bm{x},y^{\prime}\right)\in\mathcal{X}\times\mathcal{Y}_{c}\,,$
    |  | (3) |'
- en: where $y^{\prime}$ denotes the fine-grained label and $\mathcal{Y}_{c}$ represents
    the label space of class $c$ as the meta-category. Therefore, the optimization
    objective of fine-grained recognition is as
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$y^{\prime}$表示细粒度标签，$\mathcal{Y}_{c}$表示类别$c$的标签空间作为元类别。因此，细粒度识别的优化目标为
- en: '|  | $\min_{\theta}\mathbb{E}_{(\bm{x},y^{\prime})\sim p^{\prime}_{r}(\bm{x},y^{\prime})}\left[\mathcal{L}(y^{\prime},f(\bm{x};\theta))\right]\,.$
    |  | (4) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathbb{E}_{(\bm{x},y^{\prime})\sim p^{\prime}_{r}(\bm{x},y^{\prime})}\left[\mathcal{L}(y^{\prime},f(\bm{x};\theta))\right]\,.$
    |  | (4) |'
- en: Compared with fine-grained recognition, in addition to getting the sub-category
    correct, fine-grained retrieval also must rank all the instances so that images
    belonging to the same sub-category are ranked highest based on the fine-grained
    details in the query of retrieval tasks. Given an input query $\bm{x}^{q}$, the
    goal of a fine-grained retrieval system is to rank all instances in a retrieval
    set $\Omega=\{\bm{x}^{(i)}\}_{i=1}^{M}$ (whose label $y^{\prime}\in\mathcal{Y}_{c}$)
    based on their fine-grained relevance to the query. Let $\mathcal{S}_{\Omega}=\{s^{(i)}\}_{i=1}^{M}$
    represent the similarity between $\bm{x}^{q}$ and each $\bm{x}^{(i)}$ measured
    via a pre-defined metric applied to the corresponding fine-grained representations,
    *i.e.*, $h(\bm{x}^{q};\delta)$ and $h(\bm{x}^{(i)};\delta)$. Here, $\delta$ denotes
    the parameters of a retrieval model $h$. For the instances whose labels are consistent
    with the fine-grained category of $\bm{x}^{q}$, we form them into a positive set
    $\mathcal{P}_{q}$ and obtain the corresponding $\mathcal{S}_{P}$. Then, the retrieval
    model $h(\cdot;\delta)$ can be trained by maximizing the ranking based score
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与精细化识别相比，除了正确获取子类别外，精细化检索还必须对所有实例进行排序，使得属于同一子类别的图像根据检索任务中的精细化细节排在最高位置。给定一个输入查询
    $\bm{x}^{q}$，精细化检索系统的目标是基于它们与查询的精细化相关性，对检索集 $\Omega=\{\bm{x}^{(i)}\}_{i=1}^{M}$（其标签
    $y^{\prime}\in\mathcal{Y}_{c}$）中的所有实例进行排序。让 $\mathcal{S}_{\Omega}=\{s^{(i)}\}_{i=1}^{M}$
    代表 $\bm{x}^{q}$ 和每个 $\bm{x}^{(i)}$ 之间的相似性，通过应用于相应精细化表示的预定义度量来测量，即 $h(\bm{x}^{q};\delta)$
    和 $h(\bm{x}^{(i)};\delta)$。这里，$\delta$ 表示检索模型 $h$ 的参数。对于标签与 $\bm{x}^{q}$ 的精细化类别一致的实例，我们将其形成正集
    $\mathcal{P}_{q}$ 并获得相应的 $\mathcal{S}_{P}$。然后，检索模型 $h(\cdot;\delta)$ 可以通过最大化排序分数来训练。
- en: '|  | $\max_{\delta}\frac{\mathcal{R}(i,\mathcal{S}_{P})}{\mathcal{R}(i,\mathcal{S}_{\Omega})}\,,$
    |  | (5) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\delta}\frac{\mathcal{R}(i,\mathcal{S}_{P})}{\mathcal{R}(i,\mathcal{S}_{\Omega})}\,,$
    |  | (5) |'
- en: w.r.t. all the query images, where $\mathcal{R}(i,\mathcal{S}_{P})$ and $\mathcal{R}(i,\mathcal{S}_{\Omega})$
    refer to the rankings of the instance $i$ in $\mathcal{P}_{q}$ and $\Omega$, respectively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于所有查询图像，其中 $\mathcal{R}(i,\mathcal{S}_{P})$ 和 $\mathcal{R}(i,\mathcal{S}_{\Omega})$
    分别指示实例 $i$ 在 $\mathcal{P}_{q}$ 和 $\Omega$ 中的排名。
- en: 4 Benchmark Datasets
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基准数据集
- en: In recent years, the vision community has released many fine-grained benchmark
    datasets covering diverse domains, *e.g.*, birds [[13](#bib.bib13), [41](#bib.bib41),
    [1](#bib.bib1)], dogs [[42](#bib.bib42), [27](#bib.bib27)], cars [[43](#bib.bib43)],
    airplanes [[44](#bib.bib44)], flowers [[45](#bib.bib45)], vegetables [[46](#bib.bib46)],
    fruits [[46](#bib.bib46)], foods [[47](#bib.bib47)], fashion [[38](#bib.bib38),
    [33](#bib.bib33), [6](#bib.bib6)], retail products [[5](#bib.bib5), [48](#bib.bib48)],
    etc. Additionally, it is worth noting that even the most popular large-scale image
    classification dataset, *i.e.*, ImageNet [[49](#bib.bib49)], also contains fine-grained
    classes covering a lot of dog and bird sub-categories.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，视觉领域发布了许多涵盖不同领域的精细化基准数据集，例如，鸟类 [[13](#bib.bib13), [41](#bib.bib41), [1](#bib.bib1)],
    狗类 [[42](#bib.bib42), [27](#bib.bib27)], 汽车 [[43](#bib.bib43)], 飞机 [[44](#bib.bib44)],
    花卉 [[45](#bib.bib45)], 蔬菜 [[46](#bib.bib46)], 水果 [[46](#bib.bib46)], 食物 [[47](#bib.bib47)],
    时尚 [[38](#bib.bib38), [33](#bib.bib33), [6](#bib.bib6)], 零售产品 [[5](#bib.bib5),
    [48](#bib.bib48)]，等等。此外，值得注意的是，即使是最流行的大规模图像分类数据集，例如，ImageNet [[49](#bib.bib49)]，也包含了覆盖大量狗类和鸟类子类别的精细化类别。
- en: '![Refer to caption](img/99ae8d22514058d13024b92e1c502c9b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/99ae8d22514058d13024b92e1c502c9b.png)'
- en: 'Figure 5: Examples of fine-grained images belonging to different species of
    flowers/vegetables [[46](#bib.bib46)], different models of cars [[43](#bib.bib43)]
    and aircraft [[44](#bib.bib44)] and different kinds of retail products [[5](#bib.bib5)].
    Accurate identification of these fine-grained objects requires the extraction
    of discriminative, but subtle, object parts or image regions. (Best viewed in
    color and zoomed in.)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同种类的花卉/蔬菜 [[46](#bib.bib46)]、不同车型的汽车 [[43](#bib.bib43)] 和飞机 [[44](#bib.bib44)]
    以及不同种类的零售产品 [[5](#bib.bib5)] 的精细化图像示例。准确识别这些精细化对象需要提取具有辨别力但细微的对象部分或图像区域。（最佳查看效果为彩色和放大显示。）
- en: 'Table I: Summary of popular fine-grained image datasets organized by their
    major applicable topics and sorted by their release time. Note that, “$\sharp$
    images” means the total number of images of these datasets. “BBox” indicates whether
    this dataset provides object bounding box supervisions. “Part anno.” means that
    key parts annotations are provided. “HRCHY” corresponds to hierarchical labels.
    “ATR” represents attribute labels (*e.g.*, wing color, male, female, etc). “Texts”
    indicates whether fine-grained text descriptions of images are supplied. Several
    datasets are listed here twice since they are commonly used in both recognition
    and retrieval tasks.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 总结了按主要应用主题组织并按发布顺序排序的流行细粒度图像数据集。请注意，“$\sharp$ 图像”表示这些数据集的总图像数量。“BBox”表示该数据集是否提供了物体边界框的监督。“部分注释”意味着提供了关键部件的注释。“HRCHY”对应于层次标签。“ATR”表示属性标签（*例如*，翅膀颜色，雄性，雌性等）。
    “文本”表示是否提供了图像的细粒度文本描述。一些数据集在此列出两次，因为它们在识别和检索任务中都被广泛使用。'
- en: '| Topic | Dataset name | Year | Meta-class | $\sharp$ images | $\sharp$ categories
    | BBox | Part anno. | HRCHY | ATR | Texts |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 数据集名称 | 年份 | 元类别 | $\sharp$ 图像 | $\sharp$ 类别 | BBox | 部分注释 | HRCHY |
    ATR | 文本 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Recog. | Oxford Flowers [[45](#bib.bib45)] | 2008 | Flowers |     8,189 |
        102 |  |  |  |  | ✓ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 识别 | Oxford Flowers [[45](#bib.bib45)] | 2008 | 花卉 | 8,189 | 102 |  |  |  |  |
    ✓ |'
- en: '| CUB200-2011 [[13](#bib.bib13)] | 2011 | Birds |   11,788 |     200 | ✓ |
    ✓ |  | ✓ | ✓ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| CUB200-2011 [[13](#bib.bib13)] | 2011 | 鸟类 | 11,788 | 200 | ✓ | ✓ |  | ✓
    | ✓ |'
- en: '| Stanford Dogs [[42](#bib.bib42)] | 2011 | Dogs |   20,580 |     120 | ✓ |  |  |  |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Stanford Dogs [[42](#bib.bib42)] | 2011 | 狗 | 20,580 | 120 | ✓ |  |  |  |  |'
- en: '| Stanford Cars [[43](#bib.bib43)] | 2013 | Cars |   16,185 |     196 | ✓ |  |  |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Stanford Cars [[43](#bib.bib43)] | 2013 | 汽车 | 16,185 | 196 | ✓ |  |  |  |  |'
- en: '| FGVC Aircraft [[44](#bib.bib44)] | 2013 | Aircrafts |   10,000 |     100
    | ✓ |  | ✓ |  |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| FGVC Aircraft [[44](#bib.bib44)] | 2013 | 飞机 | 10,000 | 100 | ✓ |  | ✓ |  |  |'
- en: '| Birdsnap [[41](#bib.bib41)] | 2014 | Birds |   49,829 |     500 | ✓ | ✓ |  |
    ✓ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Birdsnap [[41](#bib.bib41)] | 2014 | 鸟类 | 49,829 | 500 | ✓ | ✓ |  | ✓ |  |'
- en: '| Food101 [[47](#bib.bib47)] | 2014 | Food dishes | 101,000 |     101 |  |  |  |  |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Food101 [[47](#bib.bib47)] | 2014 | 食物菜肴 | 101,000 | 101 |  |  |  |  |  |'
- en: '| NABirds [[1](#bib.bib1)] | 2015 | Birds |   48,562 |     555 | ✓ | ✓ |  |  |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| NABirds [[1](#bib.bib1)] | 2015 | 鸟类 | 48,562 | 555 | ✓ | ✓ |  |  |  |'
- en: '| Food-975 [[50](#bib.bib50)] | 2016 | Foods |   37,885 |     975 |  |  |  |
    ✓ |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Food-975 [[50](#bib.bib50)] | 2016 | 食品 | 37,885 | 975 |  |  |  | ✓ |  |'
- en: '| DeepFashion [[38](#bib.bib38)] | 2016 | Clothes | 800,000 |   1,050 | ✓ |
    ✓ |  | ✓ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| DeepFashion [[38](#bib.bib38)] | 2016 | 服装 | 800,000 | 1,050 | ✓ | ✓ |  |
    ✓ |  |'
- en: '| Fru92 [[46](#bib.bib46)] | 2017 | Fruits |   69,614 |       92 |  |  | ✓
    |  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Fru92 [[46](#bib.bib46)] | 2017 | 水果 | 69,614 | 92 |  |  | ✓ |  |  |'
- en: '| Veg200 [[46](#bib.bib46)] | 2017 | Vegetable |   91,117 |     200 |  |  |
    ✓ |  |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Veg200 [[46](#bib.bib46)] | 2017 | 蔬菜 | 91,117 | 200 |  |  | ✓ |  |  |'
- en: '| iNat2017 [[2](#bib.bib2)] | 2017 | Plants & Animals | 857,877 |   5,089 |
    ✓ |  | ✓ |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| iNat2017 [[2](#bib.bib2)] | 2017 | 植物与动物 | 857,877 | 5,089 | ✓ |  | ✓ |  |  |'
- en: '| Dogs-in-the-Wild [[27](#bib.bib27)] | 2018 | Dogs | 299,458 |     362 |  |  |  |  |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Dogs-in-the-Wild [[27](#bib.bib27)] | 2018 | 狗 | 299,458 | 362 |  |  |  |  |  |'
- en: '| RPC [[5](#bib.bib5)] | 2019 | Retail products |   83,739 |     200 | ✓ |  |
    ✓ |  |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| RPC [[5](#bib.bib5)] | 2019 | 零售产品 | 83,739 | 200 | ✓ |  | ✓ |  |  |'
- en: '|  | Products-10K [[48](#bib.bib48)] | 2020 | Retail products |  150,000 |
    10,000 | ✓ |  | ✓ |  |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | Products-10K [[48](#bib.bib48)] | 2020 | 零售产品 | 150,000 | 10,000 | ✓ |  |
    ✓ |  |  |'
- en: '|  | iNat2021 [[3](#bib.bib3)] | 2021 | Plants & Animals | 3,286,843 | 10,000
    |  |  | ✓ |  |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | iNat2021 [[3](#bib.bib3)] | 2021 | 植物与动物 | 3,286,843 | 10,000 |  |  |
    ✓ |  |  |'
- en: '| Retriev. | Oxford Flowers [[45](#bib.bib45)] | 2008 | Flowers |     8,189
    |     102 |  |  |  |  | ✓ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 检索 | Oxford Flowers [[45](#bib.bib45)] | 2008 | 花卉 | 8,189 | 102 |  |  |  |  |
    ✓ |'
- en: '| CUB200-2011 [[13](#bib.bib13)] | 2011 | Birds |   11,788 |     200 | ✓ |
    ✓ |  | ✓ | ✓ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| CUB200-2011 [[13](#bib.bib13)] | 2011 | 鸟类 | 11,788 | 200 | ✓ | ✓ |  | ✓
    | ✓ |'
- en: '| Stanford Cars [[43](#bib.bib43)] | 2013 | Cars |   16,185 |     196 | ✓ |  |  |  |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Stanford Cars [[43](#bib.bib43)] | 2013 | 汽车 | 16,185 | 196 | ✓ |  |  |  |  |'
- en: '| SBIR2014^∗ [[51](#bib.bib51)] | 2014 | Multiple |    1,120/7,267 |      14
    | ✓ | ✓ |  | ✓ |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| SBIR2014^∗ [[51](#bib.bib51)] | 2014 | 多类别 | 1,120/7,267 | 14 | ✓ | ✓ |  |
    ✓ |  |'
- en: '| DeepFashion [[38](#bib.bib38)] | 2016 | Clothes | 800,000 |   1,050 | ✓ |
    ✓ |  | ✓ |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| DeepFashion [[38](#bib.bib38)] | 2016 | 服装 | 800,000 | 1,050 | ✓ | ✓ |  |
    ✓ |  |'
- en: '| QMUL-Shoe^∗ [[52](#bib.bib52)] | 2016 | Shoes | 419/419 |         1 |  |  |  |
    ✓ |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| QMUL-Shoe^∗ [[52](#bib.bib52)] | 2016 | 鞋子 | 419/419 | 1 |  |  |  | ✓ |  |'
- en: '| QMUL-Chair^∗ [[52](#bib.bib52)] | 2016 | Chairs | 297/297 |         1 |  |  |  |
    ✓ |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| QMUL-Chair^∗ [[52](#bib.bib52)] | 2016 | 椅子 | 297/297 |         1 |  |  |  |
    ✓ |  |'
- en: '| Sketchy^∗ [[53](#bib.bib53)] | 2016 | Multiple |    75,471/12,500 |     125
    |  |  |  |  |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Sketchy^∗ [[53](#bib.bib53)] | 2016 | 多种 |    75,471/12,500 |     125 |  |  |  |  |  |'
- en: '| QMUL-Handbag^∗ [[54](#bib.bib54)] | 2017 | Handbags |    568/568 |         1
    |  |  |  |  |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| QMUL-Handbag^∗ [[54](#bib.bib54)] | 2017 | 手袋 |    568/568 |         1 |  |  |  |  |  |'
- en: '| SBIR2017^∗ [[55](#bib.bib55)] | 2017 | Shoes |   912/304 |         1 |  |
    ✓ |  | ✓ |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SBIR2017^∗ [[55](#bib.bib55)] | 2017 | 鞋子 |   912/304 |         1 |  | ✓
    |  | ✓ |  |'
- en: '| QMUL-Shoe-V2^∗ [[56](#bib.bib56)] | 2019 | Shoes |   6,730/2,000 |         1
    |  |  |  |  |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| QMUL-Shoe-V2^∗ [[56](#bib.bib56)] | 2019 | 鞋子 |   6,730/2,000 |         1
    |  |  |  |  |  |'
- en: '|  | FG-Xmedia^† [[57](#bib.bib57)] | 2019 | Birds | 11,788 |     200 |  |  |  |  |
    ✓ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | FG-Xmedia^† [[57](#bib.bib57)] | 2019 | 鸟类 | 11,788 |     200 |  |  |  |  |
    ✓ |'
- en: ^∗ For these fine-grained sketch-based image retrieval datasets, normally they
    have sketch-and-image pairs (*i.e.*, not only images). Thus, we present the numbers
    of sketches and images separately (the numbers of sketches first). Regarding “$\sharp$
    categories”, we report the number of meta-categories in these datasets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗ 对于这些基于草图的细粒度图像检索数据集，通常它们包含草图和图像对（*即*，不仅仅是图像）。因此，我们分别列出了草图和图像的数量（草图数量在前）。关于“$\sharp$
    类别”，我们报告了这些数据集中的元类别数量。
- en: ^† Except for text descriptions, *FG-Xmedia* also contains multiple other modalities,
    *e.g.*, videos and audios.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ^† 除了文本描述，*FG-Xmedia* 还包含多种其他模态，如视频和音频。
- en: '![Refer to caption](img/22fbae48e0e1eb6d86ace906aeb80c97.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22fbae48e0e1eb6d86ace906aeb80c97.png)'
- en: 'Figure 6: An example image from *CUB200-2011* [[13](#bib.bib13)] with multiple
    different types of annotations *e.g.*, category label, part annotations (*aka*
    key point locations), object bounding box shown in green, attribute labels (*i.e.*,
    “ATR”), and a text description.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：来自 *CUB200-2011* [[13](#bib.bib13)] 的示例图像，具有多种不同类型的注释，如类别标签、部件注释（*即* 关键点位置）、用绿色显示的对象边界框、属性标签（*即*，“ATR”）以及文本描述。
- en: 'Representative images from some of these fine-grained benchmark datasets can
    be found in Figure [5](#S4.F5 "Figure 5 ‣ 4 Benchmark Datasets ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"). In Table [I](#S4.T1 "Table I ‣
    4 Benchmark Datasets ‣ Fine-Grained Image Analysis with Deep Learning: A Survey"),
    we summarize the most commonly used image datasets, and indicate their meta-category,
    the amount of images, the number of categories, their main task, and additional
    available supervision, *e.g.*, bounding boxes, part annotations, hierarchical
    labels, attribute labels, and text descriptions (cf. Figure [6](#S4.F6 "Figure
    6 ‣ 4 Benchmark Datasets ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")).
    These datasets have been one of the most important factors for the considerable
    progress in the field, not only as a common ground for measuring and comparing
    performance of competing approaches, but also pushing this field towards increasingly
    complex, practical, and challenging problems.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一些细粒度基准数据集的代表性图像可以在图 [5](#S4.F5 "图 5 ‣ 4 基准数据集 ‣ 基于深度学习的细粒度图像分析：综述") 中找到。在表 [I](#S4.T1
    "表 I ‣ 4 基准数据集 ‣ 基于深度学习的细粒度图像分析：综述") 中，我们总结了最常用的图像数据集，并指出了它们的元类别、图像数量、类别数量、主要任务以及额外的可用监督信息，如边界框、部件注释、层级标签、属性标签和文本描述（参见图 [6](#S4.F6
    "图 6 ‣ 4 基准数据集 ‣ 基于深度学习的细粒度图像分析：综述")）。这些数据集是该领域取得显著进展的重要因素之一，不仅作为衡量和比较竞争方法性能的共同基础，也推动了该领域向日益复杂、实际和具有挑战性的问题发展。
- en: '![Refer to caption](img/64f896df72ead86fcf61830436dcc5f9.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/64f896df72ead86fcf61830436dcc5f9.png)'
- en: 'Figure 7: Chronological overview of representative deep learning based fine-grained
    recognition methods which are categorized by different learning approaches. (Best
    viewed in color.)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于深度学习的代表性细粒度识别方法的时间顺序概览，这些方法按照不同的学习方式进行分类。（最佳效果请查看彩色图像。）
- en: 'The fine-grained bird classification dataset *CUB200-2011* [[13](#bib.bib13)]
    is one of the most popular fine-grained datasets. The majority of FGIA approaches
    choose it for comparisons with the state-of-the-art. Moreover, continuous contributions
    are made upon *CUB200-2011* for advanced tasks, *e.g.*, collecting text descriptions
    of the fine-grained images for multi-modal analysis, cf. [[58](#bib.bib58), [59](#bib.bib59)]
    and Section [5.3.2](#S5.SS3.SSS2 "5.3.2 Multi-Modal Data ‣ 5.3 Recognition with
    External Information ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化鸟类分类数据集*CUB200-2011* [[13](#bib.bib13)] 是最受欢迎的精细化数据集之一。大多数FGIA方法选择它与最先进的方法进行比较。此外，*CUB200-2011*
    还不断在高级任务上做出贡献，例如为多模态分析收集精细化图像的文本描述，参见 [[58](#bib.bib58), [59](#bib.bib59)] 和第[5.3.2](#S5.SS3.SSS2
    "5.3.2 多模态数据 ‣ 5.3 外部信息的识别 ‣ 5 精细化图像识别 ‣ 精细化图像分析与深度学习：综述")节。
- en: 'In recent years, more challenging and practical fine-grained datasets have
    been proposed, *e.g.*, *iNat2017* containing different species of plants and animals [[2](#bib.bib2)],
    and *RPC* for retail products [[5](#bib.bib5)]. Novel properties of these datasets
    include the fact that they are large-scale, have a hierarchical structure, exhibit
    a domain gap, and form a long-tailed distribution. These challenges illustrate
    the practical requirements of FGIA in the real-world and motivate new interesting
    research challenges (cf. Section [8](#S8 "8 Future Directions ‣ Fine-Grained Image
    Analysis with Deep Learning: A Survey")).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，提出了更多具有挑战性和实际应用的精细化数据集，例如包含不同植物和动物物种的*iNat2017* [[2](#bib.bib2)] 和用于零售产品的*RPC*
    [[5](#bib.bib5)]。这些数据集的新特性包括它们是大规模的、具有层次结构的、存在领域差距并形成长尾分布。这些挑战展示了FGIA在现实世界中的实际需求，并激发了新的有趣的研究挑战（参见第[8](#S8
    "8 未来方向 ‣ 精细化图像分析与深度学习：综述")节）。
- en: 'Beyond that, a series of fine-grained sketch-based image retrieval datasets,
    *e.g.*, *QMUL-Shoe* [[52](#bib.bib52)], *QMUL-Chair* [[52](#bib.bib52)], *QMUL-handbag* [[54](#bib.bib54)],
    *SBIR2014* [[51](#bib.bib51)], *SBIR2017* [[55](#bib.bib55)], *Sketchy* [[53](#bib.bib53)],
    *QMUL-Shoe-V2* [[56](#bib.bib56)], were constructed to further advance the development
    of fine-grained retrieval, cf. Section [6.2](#S6.SS2 "6.2 Sketch-based Fine-Grained
    Image Retrieval ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey"). Furthermore, some novel datasets and benchmarks,
    such as *FG-Xmedia* [[57](#bib.bib57)], were constructed to expand fine-grained
    image retrieval to fine-grained cross-media retrieval.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了进一步推动精细化检索的发展，构建了一系列基于草图的精细化图像检索数据集，例如*QMUL-Shoe* [[52](#bib.bib52)]、*QMUL-Chair*
    [[52](#bib.bib52)]、*QMUL-handbag* [[54](#bib.bib54)]、*SBIR2014* [[51](#bib.bib51)]、*SBIR2017*
    [[55](#bib.bib55)]、*Sketchy* [[53](#bib.bib53)]、*QMUL-Shoe-V2* [[56](#bib.bib56)]。另外，还构建了一些新颖的数据集和基准，例如*FG-Xmedia*
    [[57](#bib.bib57)]，以扩展精细化图像检索到精细化跨媒体检索。
- en: 5 Fine-Grained Image Recognition
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 精细化图像识别
- en: Fine-grained image recognition has been by far the most active research area
    of FGIA in the past decade. Fine-grained recognition aims to discriminate numerous
    visually similar subordinate categories that belong to the same basic category,
    such as the fine distinction of animal species [[2](#bib.bib2)], cars [[43](#bib.bib43)],
    fruits [[46](#bib.bib46)], aircraft models [[44](#bib.bib44)], and so on. It has
    been frequently applied in real-world tasks, *e.g.*, ecosystem conservation (recognizing
    biological species) [[9](#bib.bib9)], intelligent retail systems [[5](#bib.bib5),
    [10](#bib.bib10)], etc. Recognizing fine-grained categories is difficult due to
    the challenges of discriminative region localization and fine-grained feature
    learning. Researchers have attempted to deal with these challenges from diverse
    perspectives. In this section, we review the main fine-grained recognition approaches
    since the advent of deep learning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化图像识别在过去十年中一直是FGIA最活跃的研究领域。精细化识别旨在区分属于同一基本类别的众多视觉上相似的子类别，例如动物物种的精细区分 [[2](#bib.bib2)]、汽车
    [[43](#bib.bib43)]、水果 [[46](#bib.bib46)]、飞机型号 [[44](#bib.bib44)] 等。它已经在现实世界任务中得到广泛应用，例如生态系统保护（识别生物物种）[[9](#bib.bib9)]、智能零售系统
    [[5](#bib.bib5), [10](#bib.bib10)] 等。由于区分区域定位和精细特征学习的挑战，识别精细化类别是困难的。研究人员尝试从不同角度解决这些挑战。本节回顾了自深度学习出现以来的主要精细化识别方法。
- en: 'Broadly, existing fine-grained recognition approaches can be organized into
    the following three main paradigms:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛地说，现有的细粒度识别方法可以组织成以下三种主要范式：
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recognition by localization-classification subnetworks;
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于定位-分类子网络的识别；
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recognition by end-to-end feature encoding;
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于端到端特征编码的识别；
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recognition with external information.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用外部信息的识别。
- en: 'Among them, the first two paradigms restrict themselves by only utilizing the
    supervisions associated with fine-grained images, such as image labels, bounding
    boxes, part annotations, etc. To further resolve ambiguous fine-grained problems,
    there is a body of work that uses additional information such as where and when
    the image was taken [[60](#bib.bib60), [61](#bib.bib61)], web images [[62](#bib.bib62),
    [63](#bib.bib63)], or text description [[58](#bib.bib58), [59](#bib.bib59)]. In
    order to present these representative deep learning based fine-grained recognition
    methods intuitively, we show a chronological overview in Figure [7](#S4.F7 "Figure
    7 ‣ 4 Benchmark Datasets ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")
    by organizing them into the three aforementioned paradigms.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，前两种范式通过仅利用与细粒度图像相关的监督信息，如图像标签、边界框、部件注释等，来限制自己。为了进一步解决模糊的细粒度问题，有一系列工作使用了额外的信息，例如图像拍摄的时间和地点[[60](#bib.bib60),
    [61](#bib.bib61)]，网页图像[[62](#bib.bib62), [63](#bib.bib63)]，或文本描述[[58](#bib.bib58),
    [59](#bib.bib59)]。为了直观地展示这些具有代表性的深度学习基础的细粒度识别方法，我们在图[7](#S4.F7 "Figure 7 ‣ 4 Benchmark
    Datasets ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")中通过将它们组织成三种上述范式，提供了一个时间顺序的概述。'
- en: For performance evaluation, when the test set is balanced (*i.e.*, there is
    a similar number test examples from each class), the most commonly used metric
    in fine-grained recognition is classification *accuracy* across all subordinate
    categories of the datasets. It is defined as
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于性能评估，当测试集是平衡的（*即*，每个类别的测试样本数量相似）时，细粒度识别中最常用的指标是所有子类别数据集上的分类*准确率*。它被定义为
- en: '|  | ${\rm Accuracy}=\frac{&#124;I_{\rm correct}&#124;}{&#124;I_{\rm total}&#124;}\,,$
    |  | (6) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm Accuracy}=\frac{|I_{\rm correct}|}{|I_{\rm total}|}\,,$ |  | (6)
    |'
- en: where $|I_{\rm total}|$ represents the number of images across all sub-categories
    in the test set and $|I_{\rm correct}|$ represents the number of images which
    are correctly categorized by the model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|I_{\rm total}|$ 代表测试集中所有子类别的图像数量，而 $|I_{\rm correct}|$ 代表被模型正确分类的图像数量。
- en: 5.1 Recognition by Localization-Classification Subnetworks
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于定位-分类子网络的识别
- en: '![Refer to caption](img/287c40206a84295b6bbc6d74e3f893ba.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/287c40206a84295b6bbc6d74e3f893ba.png)'
- en: 'Figure 8: Illustration of the high-level pipeline of the fine-grained recognition
    by localization-classification subnetworks paradigm.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：基于定位-分类子网络范式的细粒度识别高层次流程图示。
- en: 'Researchers have attempted to create models that capture the discriminative
    semantic parts of fine-grained objects and then construct a mid-level representation
    corresponding to these parts for the final classification, cf. Figure [8](#S5.F8
    "Figure 8 ‣ 5.1 Recognition by Localization-Classification Subnetworks ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey").
    More specifically, a localization subnetwork is designed for locating key parts,
    and then the corresponding part-level (local) feature vectors are obtained. This
    is usually combined with object-level (global) image features for representing
    fine-grained objects. This is followed by a classification subnetwork which performs
    recognition. The framework of such two collaborative subnetworks forms the first
    paradigm, *i.e.*, fine-grained recognition with *localization-classification subnetworks*.
    The motivation for these models is to first find the corresponding parts and then
    compare their appearance. Concretely, it is desirable to capture semantic parts
    (*e.g.*, heads and torsos) that are shared across fine-grained categories and
    for discovering the subtle differences between these part representations.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员尝试创建能够捕捉细粒度对象的判别性语义部分的模型，然后构建一个对应于这些部分的中级表示用于最终分类，详见图 [8](#S5.F8 "图 8 ‣
    5.1 基于定位-分类子网络的识别 ‣ 5 细粒度图像识别 ‣ 使用深度学习进行细粒度图像分析：综述")。更具体地说，设计了一个定位子网络用于定位关键部分，然后获取相应的部分级（局部）特征向量。这通常与对象级（全局）图像特征结合以表示细粒度对象。接着是一个分类子网络进行识别。这种两种协作子网络的框架形成了第一个范式，即细粒度识别与*定位-分类子网络*。这些模型的动机是首先找到相应的部分，然后比较它们的外观。具体来说，理想的是捕捉在细粒度类别中共享的语义部分（*例如*，头部和躯干），以及发现这些部分表示之间的微妙差异。
- en: 'Existing methods in this paradigm can be divided into four broad types: 1)
    employing detection or segmentation techniques, 2) utilizing deep filters, 3)
    leveraging attention mechanisms, and 4) other methods.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该范式下的现有方法可以分为四种大类：1) 使用检测或分割技术，2) 利用深度滤波器，3) 利用注意力机制，和 4) 其他方法。
- en: 'Table II: Comparative fine-grained recognition results of two learning paradigms
    (cf. Section [5.1](#S5.SS1 "5.1 Recognition by Localization-Classification Subnetworks
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey") and Section [5.2](#S5.SS2 "5.2 Recognition by End-to-End Feature Encoding
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey")) on the fine-grained benchmark datasets, *i.e.*, Birds (*CUB200-2011* [[13](#bib.bib13)]),
    Dogs (*Stanford Dogs* [[42](#bib.bib42)]), Cars (*Stanford Cars* [[43](#bib.bib43)]),
    and Aircrafts (*FGVC Aircraft* [[44](#bib.bib44)]). Note that, “Train anno.” and
    “Test anno.” mean which supervised signals used in the training and test phases,
    respectively. The symbol “–” means the results are unavailable.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 两种学习范式在细粒度基准数据集上的对比细粒度识别结果（详见第 [5.1](#S5.SS1 "5.1 基于定位-分类子网络的识别 ‣ 5 细粒度图像识别
    ‣ 使用深度学习进行细粒度图像分析：综述") 和第 [5.2](#S5.SS2 "5.2 基于端到端特征编码的识别 ‣ 5 细粒度图像识别 ‣ 使用深度学习进行细粒度图像分析：综述")
    节）。即，鸟类（*CUB200-2011* [[13](#bib.bib13)]）、狗类（*斯坦福狗* [[42](#bib.bib42)]）、汽车（*斯坦福汽车* [[43](#bib.bib43)]）和飞机（*FGVC
    飞机* [[44](#bib.bib44)]）。注意，“Train anno.” 和 “Test anno.” 表示训练和测试阶段使用的监督信号。符号“–”表示结果不可用。'
- en: '| Methods | Published in | Train anno. | Test anno. | Backbones | Img. resolution
    | Accuracy |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表在 | 训练标注 | 测试标注 | 主干网络 | 图像分辨率 | 准确率 |'
- en: '| Birds | Dogs | Cars | Aircrafts |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 鸟类 | 狗类 | 汽车 | 飞机 |'
- en: '| Fine-grained recognition by localization-classification subnetworks | Employing
    detection or segmentation techniques | Branson *et al.* [[64](#bib.bib64)] | BMVC
    2014 | BBox+Parts |  | CaffeNet | $224\times 224$ | 75.7% | – | – | – |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 通过定位-分类子网络进行的细粒度识别 | 使用检测或分割技术 | Branson *等* [[64](#bib.bib64)] | BMVC 2014
    | BBox+Parts |  | CaffeNet | $224\times 224$ | 75.7% | – | – | – |'
- en: '| PB R-CNN [[65](#bib.bib65)] | ECCV 2014 | BBox+Parts | BBox | Alex-Net |
    $224\times 224$ | 76.4% | – | – | – |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| PB R-CNN [[65](#bib.bib65)] | ECCV 2014 | BBox+Parts | BBox | Alex-Net |
    $224\times 224$ | 76.4% | – | – | – |'
- en: '| Krause *et al.* [[66](#bib.bib66)] | CVPR 2015 | BBox |  | CaffeNet | $224\times
    224$ | 82.0% | – | 92.6% | – |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Krause *等* [[66](#bib.bib66)] | CVPR 2015 | BBox |  | CaffeNet | $224\times
    224$ | 82.0% | – | 92.6% | – |'
- en: '| Deep LAC [[67](#bib.bib67)] | CVPR 2015 | BBox+Parts | BBox | Alex-Net |
    $227\times 227$ | 80.3% | – | – | – |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Deep LAC [[67](#bib.bib67)] | CVPR 2015 | BBox+Parts | BBox | Alex-Net |
    $227\times 227$ | 80.3% | – | – | – |'
- en: '| PS-CNN [[68](#bib.bib68)] | CVPR 2016 | BBox+Parts | BBox | CaffeNet | $227\times
    227$ | 76.6% | – | – | – |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| PS-CNN [[68](#bib.bib68)] | CVPR 2016 | BBox+Parts | BBox | CaffeNet | $227\times
    227$ | 76.6% | – | – | – |'
- en: '| SPDA-CNN [[69](#bib.bib69)] | CVPR 2016 | BBox+Parts | BBox | CaffeNet |
    Longer side to 800px | 81.0% | – | – | – |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| SPDA-CNN [[69](#bib.bib69)] | CVPR 2016 | BBox+Parts | BBox | CaffeNet |
    Longer side to 800px | 81.0% | – | – | – |'
- en: '| SPDA-CNN [[69](#bib.bib69)] | CVPR 2016 | BBox+Parts | BBox | VGG-16 | Longer
    side to 800px | 85.1% | – | – | – |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SPDA-CNN [[69](#bib.bib69)] | CVPR 2016 | BBox+Parts | BBox | VGG-16 | Longer
    side to 800px | 85.1% | – | – | – |'
- en: '| Zhang *et al.* [[70](#bib.bib70)] | IEEE TIP 2016 |  |  | Alex-Net | $224\times
    224$ | 78.9% | 80.4% | – | – |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *等* [[70](#bib.bib70)] | IEEE TIP 2016 |  |  | Alex-Net | $224\times
    224$ | 78.9% | 80.4% | – | – |'
- en: '| HSnet [[71](#bib.bib71)] | CVPR 2017 | Parts |  | GoogLeNet | $224\times
    224$ | 87.5% | – | 93.9% | – |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| HSnet [[71](#bib.bib71)] | CVPR 2017 | Parts |  | GoogLeNet | $224\times
    224$ | 87.5% | – | 93.9% | – |'
- en: '| TSC [[72](#bib.bib72)] | AAAI 2017 |  |  | VGG-16 | Not given | 84.7% | –
    | – | – |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| TSC [[72](#bib.bib72)] | AAAI 2017 |  |  | VGG-16 | Not given | 84.7% | –
    | – | – |'
- en: '| Mask-CNN [[31](#bib.bib31)] | PR 2018 | Parts |  | VGG-16 | $448\times 448$
    | 85.7% | – | – | – |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Mask-CNN [[31](#bib.bib31)] | PR 2018 | Parts |  | VGG-16 | $448\times 448$
    | 85.7% | – | – | – |'
- en: '| Ge *et al.* [[73](#bib.bib73)] | CVPR 2019 |  |  | GoogLeNet+BN | Shorter
    side to 800px | 90.3% | 93.9% | – | – |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Ge *等* [[73](#bib.bib73)] | CVPR 2019 |  |  | GoogLeNet+BN | Shorter side
    to 800px | 90.3% | 93.9% | – | – |'
- en: '| GCL [[74](#bib.bib74)] | AAAI 2020 |  |  | ResNet-50+BN | $448\times 448$
    | 88.3% | – | 94.0% | 93.2% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GCL [[74](#bib.bib74)] | AAAI 2020 |  |  | ResNet-50+BN | $448\times 448$
    | 88.3% | – | 94.0% | 93.2% |'
- en: '| FDL [[75](#bib.bib75)] | AAAI 2020 |  |  | ResNet-50 | $448\times 448$ |
    88.6% | 85.0% | 94.3% | 93.4% |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| FDL [[75](#bib.bib75)] | AAAI 2020 |  |  | ResNet-50 | $448\times 448$ |
    88.6% | 85.0% | 94.3% | 93.4% |'
- en: '| Utilizing deep filters | Two-level atten. [[76](#bib.bib76)] | CVPR 2015
    |  |  | VGG-16 | Not given | 77.9% | – | – | – |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 利用深度滤波器 | Two-level atten. [[76](#bib.bib76)] | CVPR 2015 |  |  | VGG-16
    | Not given | 77.9% | – | – | – |'
- en: '| CL [[77](#bib.bib77)] | CVPR 2015 |  |  | Alex-Net | $448\times 448$ | 73.5%
    | – | – | – |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| CL [[77](#bib.bib77)] | CVPR 2015 |  |  | Alex-Net | $448\times 448$ | 73.5%
    | – | – | – |'
- en: '| NAC [[78](#bib.bib78)] | ICCV 2015 |  |  | VGG-19 | Not given | 81.0% | –
    | – | – |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| NAC [[78](#bib.bib78)] | ICCV 2015 |  |  | VGG-19 | Not given | 81.0% | –
    | – | – |'
- en: '| PDFS [[79](#bib.bib79)] | CVPR 2016 |  |  | VGG-16 | Not given | 84.5% |
    72.0% | – | – |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| PDFS [[79](#bib.bib79)] | CVPR 2016 |  |  | VGG-16 | Not given | 84.5% |
    72.0% | – | – |'
- en: '| DFL-CNN [[80](#bib.bib80)] | CVPR 2018 |  |  | VGG-16 | $448\times 448$ |
    86.7% | – | 93.8% | 92.0% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| DFL-CNN [[80](#bib.bib80)] | CVPR 2018 |  |  | VGG-16 | $448\times 448$ |
    86.7% | – | 93.8% | 92.0% |'
- en: '| S3N [[81](#bib.bib81)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$ |
    88.5% | – | 94.7% | 92.8% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| S3N [[81](#bib.bib81)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$ |
    88.5% | – | 94.7% | 92.8% |'
- en: '| Huang *et al.* [[82](#bib.bib82)] | CVPR 2020 |  |  | ResNet-101 | Shorter
    side to 448px | 87.3% | – | – | – |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Huang *等* [[82](#bib.bib82)] | CVPR 2020 |  |  | ResNet-101 | Shorter side
    to 448px | 87.3% | – | – | – |'
- en: '| Attention mechanisms | RA-CNN [[83](#bib.bib83)] | CVPR 2017 |  |  | VGG-19
    | $448\times 448$ | 85.3% | 87.3% | 92.5% | – |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 注意力机制 | RA-CNN [[83](#bib.bib83)] | CVPR 2017 |  |  | VGG-19 | $448\times
    448$ | 85.3% | 87.3% | 92.5% | – |'
- en: '| MA-CNN [[84](#bib.bib84)] | ICCV 2017 |  |  | VGG-19 | $448\times 448$ |
    86.5% | – | 92.8% | 89.9% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| MA-CNN [[84](#bib.bib84)] | ICCV 2017 |  |  | VGG-19 | $448\times 448$ |
    86.5% | – | 92.8% | 89.9% |'
- en: '| Liu *et al.* [[39](#bib.bib39)] | AAAI 2017 | Parts+Attr. |  | ResNet-50
    | $448\times 448$ | 85.4% | – | – | – |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Liu *等* [[39](#bib.bib39)] | AAAI 2017 | Parts+Attr. |  | ResNet-50 | $448\times
    448$ | 85.4% | – | – | – |'
- en: '| Sun *et al.* [[27](#bib.bib27)] | ECCV 2018 |  |  | ResNet-50 | $448\times
    448$ | 86.5% | 84.8% | 93.0% | – |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Sun *等* [[27](#bib.bib27)] | ECCV 2018 |  |  | ResNet-50 | $448\times 448$
    | 86.5% | 84.8% | 93.0% | – |'
- en: '| OPAM [[85](#bib.bib85)] | IEEE TIP 2018 |  |  | VGG-16 | Not given | 85.8%
    | – | 92.2% | – |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| OPAM [[85](#bib.bib85)] | IEEE TIP 2018 |  |  | VGG-16 | Not given | 85.8%
    | – | 92.2% | – |'
- en: '| MGE-CNN [[86](#bib.bib86)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$
    | 88.5% | – | 93.9% | – |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| MGE-CNN [[86](#bib.bib86)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$
    | 88.5% | – | 93.9% | – |'
- en: '| TASN [[87](#bib.bib87)] | CVPR 2019 |  |  | ResNet-50 | $224\times 224$ |
    87.9% | – | 93.8% | – |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| TASN [[87](#bib.bib87)] | CVPR 2019 |  |  | ResNet-50 | $224\times 224$ |
    87.9% | – | 93.8% | – |'
- en: '| PA-CNN [[88](#bib.bib88)] | IEEE TIP 2020 |  |  | VGG-19 | $448\times 448$
    | 87.8% | – | 93.3% | 91.0% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| PA-CNN [[88](#bib.bib88)] | IEEE TIP 2020 |  |  | VGG-19 | $448\times 448$
    | 87.8% | – | 93.3% | 91.0% |'
- en: '| Ji *et al.* [[89](#bib.bib89)] | CVPR 2020 |  |  | ResNet-50 | $448\times
    448$ | 88.1% | – | 94.6% | 92.4% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Ji *等* [[89](#bib.bib89)] | CVPR 2020 |  |  | ResNet-50 | $448\times 448$
    | 88.1% | – | 94.6% | 92.4% |'
- en: '| Others | STN [[90](#bib.bib90)] | NeurIPS 2015 |  |  | GoogLeNet+BN | $448\times
    448$ | 84.1% | – | – | – |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Others | STN [[90](#bib.bib90)] | NeurIPS 2015 |  |  | GoogLeNet+BN | $448\times
    448$ | 84.1% | – | – | – |'
- en: '| BoT [[91](#bib.bib91)] | CVPR 2016 |  |  | Alex-Net | Not given | – | – |
    92.5% | 88.4% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| BoT [[91](#bib.bib91)] | CVPR 2016 |  |  | Alex-Net | Not given | – | – |
    92.5% | 88.4% |'
- en: '| NTS-Net [[92](#bib.bib92)] | ECCV 2018 |  |  | ResNet-50 | $448\times 448$
    | 87.5% | – | 93.9% | 91.4% |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| NTS-Net [[92](#bib.bib92)] | ECCV 2018 |  |  | ResNet-50 | $448\times 448$
    | 87.5% | – | 93.9% | 91.4% |'
- en: '| M2DRL [[93](#bib.bib93)] | IJCV 2019 |  |  | VGG-16 | $448\times 448$ | 87.2%
    | – | 93.3% | – |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| M2DRL [[93](#bib.bib93)] | IJCV 2019 |  |  | VGG-16 | $448\times 448$ | 87.2%
    | – | 93.3% | – |'
- en: '| DF-GMM [[94](#bib.bib94)] | CVPR 2020 |  |  | ResNet-50 | $448\times 448$
    | 88.8% | – | 94.8% | 93.8% |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| DF-GMM [[94](#bib.bib94)] | CVPR 2020 |  |  | ResNet-50 | $448\times 448$
    | 88.8% | – | 94.8% | 93.8% |'
- en: '| Fine-grained recognition by end-to-end feature encoding | High-order feature
    interactions | Bilinear CNN [[95](#bib.bib95)] | ICCV 2015 |  |  | VGG-16+VGG-M
    | $448\times 448$ | 84.1% | – | 91.3% | 84.1% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 通过端到端特征编码的细粒度识别 | 高阶特征交互 | 双线性 CNN [[95](#bib.bib95)] | ICCV 2015 |  |  |
    VGG-16+VGG-M | $448\times 448$ | 84.1% | – | 91.3% | 84.1% |'
- en: '| C-BCNN [[96](#bib.bib96)] | CVPR 2016 |  |  | VGG-16 | $448\times 448$ |
    84.3% | – | 91.2% | 84.1% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| C-BCNN [[96](#bib.bib96)] | CVPR 2016 |  |  | VGG-16 | $448\times 448$ |
    84.3% | – | 91.2% | 84.1% |'
- en: '| KP [[97](#bib.bib97)] | CVPR 2017 |  |  | VGG-16 | $224\times 224$ | 86.2%
    | – | 92.4% | 86.9% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| KP [[97](#bib.bib97)] | CVPR 2017 |  |  | VGG-16 | $224\times 224$ | 86.2%
    | – | 92.4% | 86.9% |'
- en: '| LRBP [[98](#bib.bib98)] | CVPR 2017 |  |  | VGG-16 | $224\times 224$ | 84.2%
    | – | 90.0% | 87.3% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| LRBP [[98](#bib.bib98)] | CVPR 2017 |  |  | VGG-16 | $224\times 224$ | 84.2%
    | – | 90.0% | 87.3% |'
- en: '| G²DeNet [[99](#bib.bib99)] | CVPR 2017 |  |  | VGG-16 | Longer side to 200px
    | 87.1% | – | 92.5% | 89.0% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| G²DeNet [[99](#bib.bib99)] | CVPR 2017 |  |  | VGG-16 | 较长边 200px | 87.1%
    | – | 92.5% | 89.0% |'
- en: '| Cai *et al.* [[100](#bib.bib100)] | ICCV 2017 |  |  | VGG-16 | $448\times
    448$ | 85.3% | – | 91.7% | 88.3% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Cai *等人* [[100](#bib.bib100)] | ICCV 2017 |  |  | VGG-16 | $448\times 448$
    | 85.3% | – | 91.7% | 88.3% |'
- en: '| iSQRT-COV [[101](#bib.bib101)] | CVPR 2018 |  |  | ResNet-101 | $224\times
    224$ | 88.7% | – | 93.3% | 91.4% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| iSQRT-COV [[101](#bib.bib101)] | CVPR 2018 |  |  | ResNet-101 | $224\times
    224$ | 88.7% | – | 93.3% | 91.4% |'
- en: '| DeepKSPD [[102](#bib.bib102)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$
    | 86.5% | – | 93.2% | 91.0% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DeepKSPD [[102](#bib.bib102)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$
    | 86.5% | – | 93.2% | 91.0% |'
- en: '| HBP [[103](#bib.bib103)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$ | 87.1%
    | – | 93.7% | 90.3% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| HBP [[103](#bib.bib103)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$ | 87.1%
    | – | 93.7% | 90.3% |'
- en: '| GP [[104](#bib.bib104)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$ | 85.8%
    | – | 92.8% | 89.8% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| GP [[104](#bib.bib104)] | ECCV 2018 |  |  | VGG-16 | $448\times 448$ | 85.8%
    | – | 92.8% | 89.8% |'
- en: '| DBTNet-50 [[105](#bib.bib105)] | NeurIPS 2019 |  |  | VGG-16 | $448\times
    448$ | 87.5% | – | 94.1% | 91.2% |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| DBTNet-50 [[105](#bib.bib105)] | NeurIPS 2019 |  |  | VGG-16 | $448\times
    448$ | 87.5% | – | 94.1% | 91.2% |'
- en: '|  | MOMN [[106](#bib.bib106)] | IEEE TIP 2020 |  |  | VGG-16 | $448\times
    448$ | 87.3% | – | 92.8% | 90.4% |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | MOMN [[106](#bib.bib106)] | IEEE TIP 2020 |  |  | VGG-16 | $448\times
    448$ | 87.3% | – | 92.8% | 90.4% |'
- en: '| Specific loss functions | MaxEnt [[107](#bib.bib107)] | NeurIPS 2018 |  |  |
    VGG-16 | $224\times 224$ | 77.0% | 65.4% | 83.9% | 78.1% |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 特定损失函数 | MaxEnt [[107](#bib.bib107)] | NeurIPS 2018 |  |  | VGG-16 | $224\times
    224$ | 77.0% | 65.4% | 83.9% | 78.1% |'
- en: '| MaxEnt [[107](#bib.bib107)] | NeurIPS 2018 |  |  | Bilinear CNN | $224\times
    224$ | 85.3% | 83.2% | 92.8% | 86.1% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| MaxEnt [[107](#bib.bib107)] | NeurIPS 2018 |  |  | 双线性 CNN | $224\times 224$
    | 85.3% | 83.2% | 92.8% | 86.1% |'
- en: '| PC [[108](#bib.bib108)] | ECCV 2018 |  |  | Bilinear CNN | $224\times 224$
    | 85.6% | 83.0% | 92.4% | 85.7% |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| PC [[108](#bib.bib108)] | ECCV 2018 |  |  | 双线性 CNN | $224\times 224$ | 85.6%
    | 83.0% | 92.4% | 85.7% |'
- en: '| Sun *et al.* [[27](#bib.bib27)] | ECCV 2018 |  |  | ResNet-50 | $448\times
    448$ | 86.5% | 84.8% | 93.0% | – |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Sun *等人* [[27](#bib.bib27)] | ECCV 2018 |  |  | ResNet-50 | $448\times 448$
    | 86.5% | 84.8% | 93.0% | – |'
- en: '| CIN [[109](#bib.bib109)] | AAAI 2020 |  |  | ResNet-101 | $448\times 448$
    | 88.1% | – | 94.5% | 92.8% |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| CIN [[109](#bib.bib109)] | AAAI 2020 |  |  | ResNet-101 | $448\times 448$
    | 88.1% | – | 94.5% | 92.8% |'
- en: '| Sun *et al.* [[110](#bib.bib110)] | AAAI 2020 |  |  | ResNet-50 | $448\times
    448$ | 88.6% | 87.7% | 94.9% | 93.5% |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Sun *等人* [[110](#bib.bib110)] | AAAI 2020 |  |  | ResNet-50 | $448\times
    448$ | 88.6% | 87.7% | 94.9% | 93.5% |'
- en: '| API-Net [[111](#bib.bib111)] | AAAI 2020 |  |  | ResNet-50 | $448\times 448$
    | 87.7% | 88.3% | 94.8% | 93.0% |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| API-Net [[111](#bib.bib111)] | AAAI 2020 |  |  | ResNet-50 | $448\times 448$
    | 87.7% | 88.3% | 94.8% | 93.0% |'
- en: '| API-Net [[111](#bib.bib111)] | AAAI 2020 |  |  | DenseNet-161 | $448\times
    448$ | 90.0% | 89.4% | 95.3% | 93.9% |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| API-Net [[111](#bib.bib111)] | AAAI 2020 |  |  | DenseNet-161 | $448\times
    448$ | 90.0% | 89.4% | 95.3% | 93.9% |'
- en: '| MC-Loss [[112](#bib.bib112)] | IEEE TIP 2020 |  |  | Bilinear CNN | $448\times
    448$ | 86.4% | – | 94.4% | 92.9% |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| MC-Loss [[112](#bib.bib112)] | IEEE TIP 2020 |  |  | 双线性 CNN | $448\times
    448$ | 86.4% | – | 94.4% | 92.9% |'
- en: '| Others | BGL [[50](#bib.bib50)] | CVPR 2016 |  |  | VGG-16 | $224\times 224$
    | 75.9% | – | 86.0% | – |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | BGL [[50](#bib.bib50)] | CVPR 2016 |  |  | VGG-16 | $224\times 224$
    | 75.9% | – | 86.0% | – |'
- en: '| BGL [[50](#bib.bib50)] | CVPR 2016 | BBox |  | VGG-16 | $224\times 224$ |
    80.4% | – | 90.5% | – |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| BGL [[50](#bib.bib50)] | CVPR 2016 | 边界框 |  | VGG-16 | $224\times 224$ |
    80.4% | – | 90.5% | – |'
- en: '| DCL [[113](#bib.bib113)] | CVPR 2019 |  |  | ResNet-50 | $448\times 448$
    | 87.8% | – | 94.5% | 93.0% |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| DCL [[113](#bib.bib113)] | CVPR 2019 |  |  | ResNet-50 | $448\times 448$
    | 87.8% | – | 94.5% | 93.0% |'
- en: '| Cross-X [[114](#bib.bib114)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$
    | 87.7% | 88.9% | 94.6% | 92.6% |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Cross-X [[114](#bib.bib114)] | ICCV 2019 |  |  | ResNet-50 | $448\times 448$
    | 87.7% | 88.9% | 94.6% | 92.6% |'
- en: '|  | PMG [[115](#bib.bib115)] | ECCV 2020 |  |  | ResNet-50 | $448\times 448$
    | 89.6% | – | 95.1% | 93.4% |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | PMG [[115](#bib.bib115)] | ECCV 2020 |  |  | ResNet-50 | $448\times 448$
    | 89.6% | – | 95.1% | 93.4% |'
- en: 5.1.1 Employing Detection or Segmentation Techniques
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 采用检测或分割技术
- en: It is straightforward to employ detection or segmentation techniques [[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118)] to locate key image regions corresponding
    to fine-grained object parts, *e.g.*, bird heads, bird tails, car lights, dog
    ears, dog torsos, etc. Thanks to localization information, *i.e.*, part-level
    bounding boxes or segmentation masks, the model can obtain more discriminative
    mid-level (part-level) representations w.r.t. these parts. Thus, it could further
    enhance the learning capability of the classification subnetwork, thus significantly
    boost the final recognition accuracy.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 采用检测或分割技术 [[116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118)] 来定位对应于细粒度物体部件的关键图像区域是直接的，*例如*，鸟的头部、鸟的尾部、汽车灯、狗的耳朵、狗的躯干等。得益于定位信息，*即*
    部件级别的边界框或分割掩模，模型可以获得关于这些部件的更具区分性的中级（部件级别）表示。因此，它可以进一步增强分类子网络的学习能力，从而显著提升最终的识别准确性。
- en: 'Earlier works in this paradigm made use of additional dense part annotations
    (*aka* key point localization, cf. Figure [6](#S4.F6 "Figure 6 ‣ 4 Benchmark Datasets
    ‣ Fine-Grained Image Analysis with Deep Learning: A Survey") on the left) to locate
    semantic key parts of objects. For example, Branson *et al.* [[64](#bib.bib64)]
    proposed to use groups of detected part keypoints to compute multiple warped image
    regions and further obtained the corresponding part-level features by pose normalization.
    In the same period, Zhang *et al.* [[65](#bib.bib65)] first generated part-level
    bounding boxes based on ground truth part annotations, and then trained a R-CNN [[118](#bib.bib118)]
    model to perform part detection. Di *et al.* [[67](#bib.bib67)] further proposed
    a Valve Linkage Function, which not only connected all subnetworks, but also refined
    localization according to the *part alignment* results. In order to integrate
    both semantic part detection and abstraction, SPDA-CNN [[69](#bib.bib69)] designed
    a top-down method to generate *part-level* proposals by inheriting prior geometric
    constraints and then used a Faster R-CNN [[116](#bib.bib116)] to return part localization
    predictions. Other approaches made use of segmentation information. PS-CNN [[68](#bib.bib68)]
    and Mask-CNN [[31](#bib.bib31)] employed segmentation models to get part/object
    masks to aid part/object localization. Compared with detection techniques, segmentation
    can result in more accurate part localization [[31](#bib.bib31)] as segmentation
    focuses on the finer pixel-level targets, instead of just coarse bounding boxes.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '早期在这一范式中的工作利用了额外的密集部分注释（*即* 关键点定位，参见图 [6](#S4.F6 "Figure 6 ‣ 4 Benchmark Datasets
    ‣ Fine-Grained Image Analysis with Deep Learning: A Survey") 的左侧）来定位物体的语义关键部位。例如，Branson
    *等* [[64](#bib.bib64)] 提出了使用检测到的部分关键点组来计算多个变形图像区域，并通过姿态归一化进一步获得相应的部分级别特征。在同一时期，Zhang
    *等* [[65](#bib.bib65)] 首先基于真实标注生成了部分级别的边界框，然后训练了一个 R-CNN [[118](#bib.bib118)]
    模型进行部分检测。Di *等* [[67](#bib.bib67)] 进一步提出了一种阀门联动功能，该功能不仅连接了所有子网络，还根据 *部件对齐* 结果细化了定位。为了整合语义部分检测和抽象，SPDA-CNN
    [[69](#bib.bib69)] 设计了一种自上而下的方法，通过继承先验几何约束生成 *部分级别* 提案，并使用 Faster R-CNN [[116](#bib.bib116)]
    返回部分定位预测。其他方法利用了分割信息。PS-CNN [[68](#bib.bib68)] 和 Mask-CNN [[31](#bib.bib31)] 使用分割模型获取部件/物体掩模以帮助部件/物体定位。与检测技术相比，分割可以得到更准确的部件定位
    [[31](#bib.bib31)]，因为分割关注的是更细的像素级目标，而不仅仅是粗略的边界框。'
- en: 'However, employing traditional detectors or segmentation models requires dense
    part annotations for training, which is labor-intensive and would limit both scalability
    and practicality of real-world fine-grained applications. Therefore, it is desirable
    to accurately locate fine-grained parts by only using image level labels [[70](#bib.bib70),
    [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75)]. These
    set of approaches are referred to as “weakly-supervised” as they only use image
    level labels. It is interesting to note that since 2016 there is an apparent trend
    in developing fine-grained methods in this weakly-supervised setting, rather than
    the strong-supervised setting (*i.e.*, using part annotations and bounding boxes),
    cf. Table [II](#S5.T2 "Table II ‣ 5.1 Recognition by Localization-Classification
    Subnetworks ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with
    Deep Learning: A Survey").'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，采用传统的检测器或分割模型需要密集的部分注释进行训练，这既费力又限制了实际细粒度应用的可扩展性和实用性。因此，仅通过使用图像级标签来准确定位细粒度部分是理想的 [[70](#bib.bib70),
    [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75)]。这些方法被称为“弱监督”，因为它们只使用图像级标签。值得注意的是，自2016年以来，开发这种弱监督设置下的细粒度方法成为了一种明显的趋势，而不是强监督设置（*即*，使用部分注释和边界框），参见表 [II](#S5.T2
    "Table II ‣ 5.1 Recognition by Localization-Classification Subnetworks ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")。'
- en: Recognition methods in the weakly-supervised localization based classification
    setting always rely on unsupervised approaches to obtain semantic groups which
    correspond to object parts. Specifically, Zhang *et al.* [[70](#bib.bib70)] adopted
    the spatial pyramid strategy [[119](#bib.bib119)] to generate part proposals from
    object proposals. Then, by using a clustering approach, they generated part proposal
    prototype clusters and further selected useful clusters to get discriminative
    part-level features. Co-segmentation [[120](#bib.bib120)] based methods are also
    commonly used in this weakly supervised case. One approach is to use co-segmentation
    to obtain object masks without supervision, and then perform heuristic strategies,
    *e.g.*, part constraints [[72](#bib.bib72)] or part alignment [[66](#bib.bib66)],
    to locate fine-grained parts.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在弱监督定位分类设置中的识别方法总是依赖于无监督方法来获取与对象部分对应的语义组。具体来说，Zhang *等* [[70](#bib.bib70)] 采用了空间金字塔策略 [[119](#bib.bib119)]
    来从对象提议中生成部分提议。然后，通过使用聚类方法，他们生成了部分提议原型簇，并进一步选择有用的簇以获得区分性部分特征。在这种弱监督情况下，共同分割 [[120](#bib.bib120)]
    方法也被广泛使用。一种方法是使用共同分割在没有监督的情况下获取对象掩膜，然后执行启发式策略，*例如*，部分约束 [[72](#bib.bib72)] 或部分对齐 [[66](#bib.bib66)]，以定位细粒度部分。
- en: It is worth noting that the majority of previous work overlooks the internal
    semantic correlation among discriminative part-level features. Concretely, the
    aforementioned methods pick out the discriminative regions independently and utilize
    their features directly, while neglecting the fact that an object’s features are
    mutually semantic correlated and region groups can be more discriminative. Therefore,
    very recently, some methods attempt to jointly learn the interdependencies among
    part-level features to obtain more universal and powerful fine-grained image representations.
    By performing different feature fusion strategies (*e.g.*, LSTMs [[71](#bib.bib71),
    [73](#bib.bib73)], graphs [[74](#bib.bib74)], or knowledge distilling [[75](#bib.bib75)])
    these joint part feature learning methods yield significantly higher recognition
    accuracy over previous independent part feature learning methods.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，大多数以前的工作忽视了区分性部分特征之间的内部语义关联。具体而言，前述方法独立地挑选出区分性区域并直接利用其特征，同时忽视了一个对象的特征是相互语义相关的，并且区域组可能更具区分性。因此，最近一些方法尝试联合学习部分特征之间的相互依赖关系，以获得更通用和强大的细粒度图像表示。通过执行不同的特征融合策略（*例如*，LSTM [[71](#bib.bib71),
    [73](#bib.bib73)]，图谱 [[74](#bib.bib74)]，或知识蒸馏 [[75](#bib.bib75)]），这些联合部分特征学习方法在识别准确性上显著高于以前的独立部分特征学习方法。
- en: 5.1.2 Utilizing Deep Filters
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 利用深度滤波器
- en: 'In deep convolutional neural networks (CNNs), deep filters (*i.e.*, CNN filters)
    refer to the learned weights of the convolution layers [[14](#bib.bib14)]. The
    responses/activations from these deep filters can be viewed as localized descriptors.
    The deep descriptors have the following properties [[121](#bib.bib121)]: 1) Locality:
    they describe and correspond to local image regions w.r.t. the whole input image
    and 2) Spatiality: they are also able to encode spatial information. As works
    started exploring the use of CNNs in computer vision, researchers gradually discovered
    that intermediate CNN outputs (*e.g.*, local deep descriptors) could be linked
    to semantic parts of common objects [[122](#bib.bib122)]. Therefore, the fine-grained
    community attempted to employ these filter outputs as part detectors [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82)], and thus rely on them to conduct localization-classification
    fine-grained recognition. One of the main advantages here is that this does not
    require any part-level annotations.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '在深度卷积神经网络（CNN）中，深度滤波器（*即*，CNN 滤波器）指的是卷积层的学习权重 [[14](#bib.bib14)]。这些深度滤波器的响应/激活可以视为局部描述符。深度描述符具有以下属性
    [[121](#bib.bib121)]: 1) 局部性：它们描述并对应于相对于整个输入图像的局部图像区域；2) 空间性：它们还能编码空间信息。随着研究开始探索
    CNN 在计算机视觉中的应用，研究人员逐渐发现中间 CNN 输出（*例如*，局部深度描述符）可以与常见物体的语义部件相关联 [[122](#bib.bib122)]。因此，细粒度社区尝试将这些滤波器输出用作部件检测器
    [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82)]，并依赖它们进行定位-分类的细粒度识别。其中一个主要优势是这不需要任何部件级的注释。'
- en: Xiao *et al.* [[76](#bib.bib76)] performed spectral clustering [[123](#bib.bib123)]
    on these deep filters to form groups and then used the filter groups to serve
    as part detectors. Similarly, NAC [[78](#bib.bib78)] exploits the channels of
    a CNN as part detectors. Liu *et al.* [[77](#bib.bib77)] developed a cross-layer
    pooling method by aggregating the part-based deep descriptors using activations
    from two successive convolutional layers as guidance. PDFS [[79](#bib.bib79)]
    was proposed to select deep filters corresponding to parts and then iteratively
    update the learned “detectors” which resulted in the discovery of discriminative
    and consistent part-based image regions. For these aforementioned methods, after
    obtaining detected parts using deep filters from pre-trained classification CNNs,
    they typically trained off-line classifiers, *e.g.*, SVMs or decision trees [[123](#bib.bib123)],
    using the part-based feature vectors to conduct the final recognition task.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao *等人* [[76](#bib.bib76)] 对这些深度滤波器进行了谱聚类 [[123](#bib.bib123)] 以形成组，然后使用这些滤波器组作为部件检测器。类似地，NAC
    [[78](#bib.bib78)] 利用 CNN 的通道作为部件检测器。Liu *等人* [[77](#bib.bib77)] 通过使用来自两个连续卷积层的激活作为指导，开发了一种跨层池化方法，通过聚合基于部件的深度描述符。PDFS
    [[79](#bib.bib79)] 被提出用来选择对应于部件的深度滤波器，然后迭代更新学习到的“检测器”，从而发现具有判别性和一致性的基于部件的图像区域。对于这些前述方法，在从预训练分类
    CNN 中获取检测到的部件后，它们通常会训练离线分类器，*例如*，SVM 或决策树 [[123](#bib.bib123)]，使用基于部件的特征向量进行最终识别任务。
- en: 'To facilitate the learning of both part detection and part-based classification,
    unified end-to-end trained fine-grained models [[80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82)] were developed. As a result, significant recognition improvements
    were observed, cf. Table [II](#S5.T2 "Table II ‣ 5.1 Recognition by Localization-Classification
    Subnetworks ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with
    Deep Learning: A Survey"). Wang *et al.* [[80](#bib.bib80)] utilized an additional
    learnable $1\times 1$ convolutional filter as a small patch (*i.e.*, part) detector.
    This was followed by a global max-pooling to keep the highest activations w.r.t.
    that filter for the final classification. Later, based on class response maps [[124](#bib.bib124)],
    S3N [[81](#bib.bib81)] leveraged the class peak responses, *i.e.*, local maximums,
    as the basis of part localization. Similar to [[71](#bib.bib71), [73](#bib.bib73),
    [74](#bib.bib74)], S3N also considers part correlations in a mutual part learning
    way.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '为了促进部分检测和基于部分的分类的学习，开发了统一的端到端训练的细粒度模型[[80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82)]。因此，观察到了显著的识别改善，见表[II](#S5.T2
    "Table II ‣ 5.1 Recognition by Localization-Classification Subnetworks ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")。王*等人*[[80](#bib.bib80)]利用额外的可学习的$1\times
    1$卷积滤波器作为小块（即部分）检测器。接着通过全局最大池化来保持相对于该滤波器的最高激活值以进行最终分类。后来，基于类别响应图[[124](#bib.bib124)]，S3N[[81](#bib.bib81)]利用类别峰值响应，即局部最大值，作为部分定位的基础。类似于[[71](#bib.bib71),
    [73](#bib.bib73), [74](#bib.bib74)]，S3N也以互助部分学习的方式考虑部分之间的关联。'
- en: 5.1.3 Leveraging Attention Mechanisms
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 利用注意力机制
- en: Even though the previous localization-classification fine-grained methods have
    shown strong classification performance, one of their major drawbacks is that
    they require meaningful definitions of the object parts. In many applications
    however, it may be hard to represent or even define common parts of some object
    classes, *e.g.*, non-structured objects like food dishes [[47](#bib.bib47)] or
    flowers with repeating parts [[45](#bib.bib45)]. Compared to these localization-classification
    methods, a more natural solution of finding parts is to leverage attention mechanisms [[125](#bib.bib125)]
    as sub-modules. This enables CNNs to attend to loosely defined regions for fine-grained
    objects and as a result have emerged as a promising direction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前的定位-分类细粒度方法展示了强大的分类性能，但其主要缺陷之一是需要对物体部分进行有意义的定义。然而，在许多应用中，可能很难表示或甚至定义某些物体类别的常见部分，例如，像食物盘子这类非结构化物体[[47](#bib.bib47)]或具有重复部分的花朵[[45](#bib.bib45)]。与这些定位-分类方法相比，利用注意力机制[[125](#bib.bib125)]作为子模块来寻找部分是一种更自然的解决方案。这使得卷积神经网络（CNN）能够关注于定义较松散的区域，以应对细粒度物体，因此成为一个有前景的方向。
- en: 'It is common knowledge that attention plays an important role in human perception [[125](#bib.bib125),
    [126](#bib.bib126)]. Humans exploit a sequence of partial glimpses and selectively
    focus on salient parts of an object or a scene in order to better capture visual
    structure [[127](#bib.bib127)]. Inspired by this, Fu *et al.* and Zheng *et al.* [[83](#bib.bib83),
    [84](#bib.bib84)] were the first to incorporate attention processing to improve
    the fine-grained recognition accuracy of CNNs. Specifically, RA-CNN [[83](#bib.bib83)]
    uses a recurrent visual attention model to select a sequence of attention regions
    (corresponding to object “parts”¹¹1Note that here “parts” refers to the loosely
    defined attention regions for fine-grained objects, which is different from the
    clearly defined object parts from manual annotations, cf. Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Employing Detection or Segmentation Techniques ‣ 5.1 Recognition by Localization-Classification
    Subnetworks ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with
    Deep Learning: A Survey").). RA-CNN iteratively generates region attention maps
    in a coarse to fine fashion by taking previous predictions as a reference. MA-CNN [[84](#bib.bib84)]
    is equipped with a multi-attention CNN, and can return multiple region attentions
    in parallel. Subsequently, Peng *et al.* [[85](#bib.bib85)] and Zheng *et al.* [[88](#bib.bib88)]
    proposed multi-level attention models to obtain hierarchical attention information
    (*i.e.*, both object- and part-level). He *et al.* [[128](#bib.bib128)] applied
    multi-level attention to localize multiple discriminative regions simultaneously
    for each image via an $n$-pathway end-to-end discriminative localization network
    that simultaneously localizes discriminative regions and encodes their features.
    This multi-level attention can result in diverse and complementary information
    compared to the aforementioned single-level attention methods. Sun *et al.* [[27](#bib.bib27)]
    incorporated channel attentions [[129](#bib.bib129)] and metric learning [[130](#bib.bib130)]
    to enforce the correlations among different attended regions. Zheng *et al.* [[87](#bib.bib87)]
    developed a trilinear attention sampling network to learn fine-grained details
    from hundreds of part proposals and efficiently distill the learned features into
    a single CNN. Recently, Ji *et al.* [[89](#bib.bib89)] presented an attention
    based convolutional binary neural tree, which incorporates attention mechanisms
    with a tree structure to facilitate coarse-to-fine hierarchical fine-grained feature
    learning. Although the attention mechanism achieves strong accuracy in fine-grained
    recognition, it tends to overfit in the case of small-scale data.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '众所周知，注意力在人的感知中发挥着重要作用[[125](#bib.bib125), [126](#bib.bib126)]。人类利用一系列部分的瞥见，选择性地关注对象或场景中的显著部分，以便更好地捕捉视觉结构[[127](#bib.bib127)]。受到这一点的启发，Fu
    *et al.* 和 Zheng *et al.* [[83](#bib.bib83), [84](#bib.bib84)] 首次将注意力处理技术引入到 CNN
    中，以提高细粒度识别的准确性。具体来说，RA-CNN [[83](#bib.bib83)] 使用递归视觉注意力模型选择一系列注意力区域（对应于对象的“部分”¹¹1
    请注意，这里的“部分”指的是用于细粒度对象的松散定义的注意力区域，这与手动标注的明确对象部分不同，参见第 [5.1.1](#S5.SS1.SSS1 "5.1.1
    Employing Detection or Segmentation Techniques ‣ 5.1 Recognition by Localization-Classification
    Subnetworks ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with
    Deep Learning: A Survey") 节）。RA-CNN 通过将先前的预测作为参考，迭代生成从粗到细的区域注意力图。MA-CNN [[84](#bib.bib84)]
    配备了多重注意力 CNN，并能够并行返回多个区域注意力。随后，Peng *et al.* [[85](#bib.bib85)] 和 Zheng *et al.*
    [[88](#bib.bib88)] 提出了多级注意力模型，以获得分层的注意力信息（*即*，包括对象级别和部分级别）。He *et al.* [[128](#bib.bib128)]
    应用了多级注意力，以通过一个 $n$-路径端到端的辨别定位网络同时定位每张图像中的多个辨别区域，并编码其特征。这种多级注意力可以产生与上述单级注意力方法相比的多样和互补的信息。Sun
    *et al.* [[27](#bib.bib27)] 结合了通道注意力 [[129](#bib.bib129)] 和度量学习 [[130](#bib.bib130)]，以加强不同关注区域之间的相关性。Zheng
    *et al.* [[87](#bib.bib87)] 开发了一个三线性注意力采样网络，从数百个部分提议中学习细粒度细节，并高效地将学习到的特征提炼到一个单一的
    CNN 中。最近，Ji *et al.* [[89](#bib.bib89)] 提出了一个基于注意力的卷积二叉神经树，它将注意力机制与树结构相结合，以促进从粗到细的分层细粒度特征学习。尽管注意力机制在细粒度识别中取得了强大的准确性，但在小规模数据的情况下，它倾向于过拟合。'
- en: 5.1.4 Other Methods
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 其他方法
- en: Many other approaches in the localization-classification paradigm have also
    been proposed for fine-grained recognition. Spatial Transformer Networks (STN) [[90](#bib.bib90)]
    were originally introduced to explicitly perform spatial transformations in an
    end-to-end learnable way. They can also be equipped with multiple transformers
    in parallel to conduct fine-grained recognition. Each transformer in an STN can
    correspond to a part detector with spatial transformation capabilities. Later,
    Wang *et al.* [[91](#bib.bib91)] developed a triplet of patches with geometric
    constraints as a template to automatically mine discriminative triplets and then
    generated mid-level representations for classification with the mined triplets.
    In addition, other methods have achieved better accuracy by introducing feedback
    mechanisms. Specifically, NTS-Net [[92](#bib.bib92)] employs a multi-agent cooperative
    learning scheme to address the core problem of fine-grained recognition, *i.e.*,
    accurately identifying informative regions in an image. M2DRL [[131](#bib.bib131),
    [93](#bib.bib93)] was the first to utilize deep reinforcement learning [[132](#bib.bib132)]
    at both the object- and part-level to capture multi-granularity discriminative
    localization and multi-scale representations using their tailored reward functions.
    Inspired by low-rank mechanisms in natural language processing [[133](#bib.bib133)],
    Wang *et al.* [[94](#bib.bib94)] proposed the DF-GMM framework to alleviate the
    region diffusion problem in high-level feature maps for fine-grained part localization.
    DF-GMM first selects discriminative regions from the high-level feature maps by
    constructing low-rank bases, and then applies spatial information of the low-rank
    bases to reconstruct low-rank feature maps. Part correlations can also be modeled
    by reorganization processing, which brings accuracy improvements.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在定位分类范式中，也提出了许多其他方法用于细粒度识别。空间变换网络（STN）[[90](#bib.bib90)]最初是为了以端到端可学习的方式显式地执行空间变换而引入的。它们也可以配备多个变换器以并行进行细粒度识别。STN中的每个变换器可以对应于具有空间变换能力的部件检测器。后来，Wang
    *et al.* [[91](#bib.bib91)]开发了一组具有几何约束的补丁作为模板，以自动挖掘具有区分性的三元组，然后生成中级表示以进行分类。除此之外，其他方法通过引入反馈机制实现了更好的准确性。具体而言，NTS-Net
    [[92](#bib.bib92)]采用多代理协作学习方案来解决细粒度识别的核心问题，即准确识别图像中的信息区域。M2DRL [[131](#bib.bib131),
    [93](#bib.bib93)]首次在对象和部件层面上利用深度强化学习 [[132](#bib.bib132)] 捕捉多粒度区分定位和多尺度表示，并使用其量身定制的奖励函数。受到自然语言处理中的低秩机制
    [[133](#bib.bib133)] 的启发，Wang *et al.* [[94](#bib.bib94)] 提出了DF-GMM框架，以缓解细粒度部件定位中高层特征图的区域扩散问题。DF-GMM首先通过构造低秩基来从高层特征图中选择具有区分性的区域，然后应用低秩基的空间信息来重建低秩特征图。部件相关性也可以通过重组处理来建模，从而提高准确性。
- en: 5.2 Recognition by End-to-End Feature Encoding
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 通过端到端特征编码进行识别
- en: 'The second learning paradigm of fine-grained recognition is *end-to-end feature
    encoding*. As with other vision tasks, feature learning also plays a fundamental
    role in fine-grained recognition. Since the differences between sub-categories
    are typically very subtle and local, capturing global semantic information using
    only fully connected layers limits the representation capacity of a fine-grained
    model, and hence restricts further improvements in final recognition performance.
    Therefore, methods have been developed that aim to learn a unified, yet discriminative,
    image representation for modeling subtle differences between fine-grained categories
    in the following ways: 1) by performing high-order feature interactions, 2) by
    designing novel loss functions, and 3) through other means.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度识别的第二种学习范式是*端到端特征编码*。与其他视觉任务一样，特征学习在细粒度识别中也发挥着基础作用。由于子类别之间的差异通常非常微妙和局部，单靠全连接层来捕捉全局语义信息限制了细粒度模型的表示能力，因此限制了最终识别性能的进一步提升。因此，已经开发出旨在学习统一但具有区分性的图像表示的方法，以建模细粒度类别之间的微妙差异，具体方法包括：1)
    进行高阶特征交互，2) 设计新颖的损失函数，以及3) 通过其他手段。
- en: 5.2.1 Performing High-Order Feature Interactions
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 进行高阶特征交互
- en: 'Feature learning plays a crucial role in almost all vision tasks such as retrieval,
    detection, tracking, etc. The success of deep convolutional networks is mainly
    due to the learned discriminative deep features. In the initial era of deep learning,
    the features (*i.e.*, activations) of fully connected layers were commonly used
    as image representations. Later, the feature maps of deeper convolutional layers
    were discovered to contain mid- and high-level information, *e.g.*, object parts
    or complete objects [[134](#bib.bib134)], which led to the widespread use of convolutional
    features/descriptors [[77](#bib.bib77), [135](#bib.bib135)], cf. Figure [9](#S5.F9
    "Figure 9 ‣ 5.2.1 Performing High-Order Feature Interactions ‣ 5.2 Recognition
    by End-to-End Feature Encoding ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"). Additionally, applying encoding
    techniques for these local convolutional descriptors has resulted in significant
    improvements compared with fully-connected outputs [[136](#bib.bib136), [135](#bib.bib135),
    [137](#bib.bib137)]. To some extent, these improvements in encoding techniques
    come from the *higher-order* statistics encoded in the final features. Particularly
    for fine-grained recognition, where the need for end-to-end modeling of higher-order
    statistics became evident when the Fisher Vector encodings of SIFT features outperformed
    a fine-tuned AlexNet in several fine-grained tasks [[138](#bib.bib138)].'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 特征学习在几乎所有视觉任务中扮演着至关重要的角色，如检索、检测、跟踪等。深度卷积网络的成功主要归功于所学习的区分性深层特征。在深度学习的初期阶段，通常使用全连接层的特征（*即*，激活）作为图像表示。后来发现，较深卷积层的特征图包含了中层和高层信息，例如物体部件或完整物体[[134](#bib.bib134)]，这导致了卷积特征/描述符的广泛使用[[77](#bib.bib77),
    [135](#bib.bib135)]，见图[9](#S5.F9 "图 9 ‣ 5.2.1 执行高阶特征交互 ‣ 5.2 端到端特征编码的识别 ‣ 5 精细图像识别
    ‣ 深度学习中的精细图像分析：综述")。此外，应用这些局部卷积描述符的编码技术相比于全连接输出显著提高了性能[[136](#bib.bib136), [135](#bib.bib135),
    [137](#bib.bib137)]。在某种程度上，这些编码技术的改进来自于最终特征中编码的*高阶*统计信息。特别是在精细识别中，当SIFT特征的Fisher向量编码在多个精细任务中优于精调后的AlexNet时，*高阶*统计信息的端到端建模需求变得显而易见[[138](#bib.bib138)]。
- en: '![Refer to caption](img/9227fb49f16b8478afb4f20e82b66c9a.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9227fb49f16b8478afb4f20e82b66c9a.png)'
- en: 'Figure 9: Illustration of feature maps and deep descriptors in CNNs.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：CNN 中特征图和深层描述符的示意图。
- en: The covariance matrix based representation [[139](#bib.bib139), [140](#bib.bib140)]
    is a representative higher-order (*i.e.*, second-order) feature interaction technique,
    which has been used in computer vision and machine learning. Let $\bm{V}_{d\times
    n}=\left[\bm{v}_{1},\bm{v}_{2},\ldots,\bm{v}_{n}\right]$ denote a data matrix,
    in which each column contains a local descriptor $\bm{v}_{i}\in\mathcal{R}^{d}$,
    extracted from an image. The corresponding $d\times d$ sample covariance matrix
    over $\bm{V}$ is denoted as $\bm{\Sigma}=\bar{\bm{V}}\bar{\bm{V}}^{\top}$ (or
    simply ${\bm{V}}{\bm{V}}^{\top}$), where $\bar{\bm{V}}$ denotes the centered $\bm{V}$.
    Originally, this covariance matrix is proposed as a region descriptor, *e.g.*,
    characterizing the covariance of the color intensities of pixels in an image patch.
    In recent years, it has been used as a promising second-order pooled image representation
    for visual recognition.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 基于协方差矩阵的表示[[139](#bib.bib139), [140](#bib.bib140)]是一种具有代表性的高阶（*即*，二阶）特征交互技术，已在计算机视觉和机器学习中使用。设$\bm{V}_{d\times
    n}=\left[\bm{v}_{1},\bm{v}_{2},\ldots,\bm{v}_{n}\right]$为数据矩阵，其中每一列包含从图像中提取的局部描述符$\bm{v}_{i}\in\mathcal{R}^{d}$。相应的$d\times
    d$样本协方差矩阵记作$\bm{\Sigma}=\bar{\bm{V}}\bar{\bm{V}}^{\top}$（或简单地记作${\bm{V}}{\bm{V}}^{\top}$），其中$\bar{\bm{V}}$表示中心化后的$\bm{V}$。最初，这个协方差矩阵被提议作为区域描述符，例如，表征图像块中像素颜色强度的协方差。近年来，它被用作视觉识别的有前途的二阶池化图像表示。
- en: By integrating the covariance matrix based representation with deep descriptors,
    a series of methods showed promising accuracy in fine-grained recognition in the
    past few years. The most representative method among them is Bilinear CNNs [[95](#bib.bib95),
    [141](#bib.bib141)], which represents an image as a pooled outer product of features
    derived from two deep CNNs, and thus encodes second-order statistics of convolutional
    activations, resulting in clear improvements in fine-grained recognition. This
    outer product essentially leads to a covariance matrix (in the form of $\bm{V}\bm{V}^{\top}$)
    when the two CNNs are set as the same. However, the outer product operation results
    in extremely high dimensional features, *i.e.*, the bilinear pooled feature is
    reshaped into a vector $\bm{z}={\rm vec}(\bm{V}\bm{V}^{\top})\in\mathcal{R}^{d^{2}}$.
    This results in a large increase in the number of parameters in the classification
    module of the deep network, which can cause overfitting and make it impractical
    for realistic applications, especially for large-scale ones. To address this problem,
    Gao *et al.* [[96](#bib.bib96)] applied Tensor Sketch [[142](#bib.bib142)] to
    both approximate the second-order statistics of the original bilinear pooling
    operation and reduce feature dimensions. Kong *et al.* [[98](#bib.bib98)] adopted
    a low-rank approximation to the covariance matrix and further learned a low-rank
    bilinear classifier. The resulting classifier can be evaluated without explicitly
    computing the bilinear feature matrix which results in a large reduction on the
    parameter size. Li *et al.* [[143](#bib.bib143)] also modeled pairwise feature
    interaction by performing a quadratic transformation with a low-rank constraint.
    Yu *et al.* [[103](#bib.bib103)] used a dimension reduction projection before
    bilinear pooling to alleviate dimension explosion. Zheng *et al.* [[105](#bib.bib105)]
    applied bilinear pooling to feature channel groups where the bilinear transformation
    is represented by calculating pairwise interactions within each group. This also
    results in large saving in computation cost.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将协方差矩阵表示与深度描述符相结合，近年来出现了一系列在细粒度识别中表现出良好准确率的方法。其中最具代表性的方法是双线性卷积神经网络（Bilinear
    CNNs）[[95](#bib.bib95), [141](#bib.bib141)]，该方法将图像表示为从两个深度卷积神经网络提取的特征的外积池化，从而编码卷积激活的二阶统计量，带来了细粒度识别的显著改进。这个外积在两个卷积神经网络设置相同时，实际上会导致协方差矩阵（形式为$\bm{V}\bm{V}^{\top}$）。然而，外积操作会导致特征维度极高，*即*，双线性池化特征被重塑为向量$\bm{z}={\rm
    vec}(\bm{V}\bm{V}^{\top})\in\mathcal{R}^{d^{2}}$。这导致深度网络分类模块中参数数量的大幅增加，这可能会导致过拟合，并使其在现实应用中不切实际，尤其是在大规模应用中。为了解决这个问题，Gao
    *et al.* [[96](#bib.bib96)] 将张量草图（Tensor Sketch）[[142](#bib.bib142)]应用于近似原始双线性池化操作的二阶统计量并减少特征维度。Kong
    *et al.* [[98](#bib.bib98)] 采用协方差矩阵的低秩近似，并进一步学习低秩双线性分类器。得到的分类器可以在不显式计算双线性特征矩阵的情况下进行评估，从而大幅减少了参数规模。Li
    *et al.* [[143](#bib.bib143)] 还通过进行带低秩约束的二次变换来建模成对特征交互。Yu *et al.* [[103](#bib.bib103)]
    在双线性池化之前使用降维投影以缓解维度爆炸。Zheng *et al.* [[105](#bib.bib105)] 将双线性池化应用于特征通道组，其中双线性变换通过计算每个组内的成对交互来表示。这也带来了计算成本的大幅节省。
- en: Beyond these approaches, some methods attempt to capture much higher-order (more
    than second-order) interactions of features to generate stronger and more discriminative
    feature representations. Cui *et al.* [[97](#bib.bib97)] introduced a kernel pooling
    method that captures arbitrarily ordered and non-linear features via compact feature
    mapping. Cai *et al.* [[100](#bib.bib100)] proposed a polynomial kernel based
    predictor to model higher-order statistics of convolutional activations across
    multiple layers for modeling part interactions. Subsequently, DeepKSPD [[102](#bib.bib102)]
    was developed to jointly learn the deep local descriptors and the kernel-matrix
    based covariance representation in an end-to-end trainable manner.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些方法之外，一些方法试图捕捉更高阶（超过二阶）特征交互，以生成更强大和更具区分性的特征表示。Cui *et al.* [[97](#bib.bib97)]
    引入了一种核池化方法，通过紧凑的特征映射捕捉任意阶次和非线性特征。Cai *et al.* [[100](#bib.bib100)] 提出了基于多项式核的预测器，以建模多层卷积激活的高阶统计量，用于建模部分交互。随后，DeepKSPD
    [[102](#bib.bib102)] 被开发出来，以端到端可训练的方式联合学习深度局部描述符和基于核矩阵的协方差表示。
- en: As $\ell_{2}$ feature normalization can suppress the common patterns of high
    response and thereby enhance those discriminative features (*i.e.*, the visual
    burstiness problem [[104](#bib.bib104), [144](#bib.bib144)]), the aforementioned
    bilinear pooling based methods typically perform element-wise square root normalization
    followed by $\ell_{2}$-normalization on covariance matrix to improve performance.
    However, merely employing $\ell_{2}$-normalization can cause unstable high-order
    information and also lead to slow convergence. To this end, many methods have
    explored non-linearly scaling based on the singular value decomposition (SVD)
    or eigendecomposition (EIG) to obtain more stability for second-order representations.
    Specifically, Li *et al.* [[145](#bib.bib145)] proposed to apply the power exponent
    to the eigenvalues of bilinear features to achieve better recognition accuracy.
    G²DeNet [[99](#bib.bib99)] further combined complementary first-order and second-order
    information via a Gaussian embedding and matrix square root normalization. iSQRT-COV [[101](#bib.bib101)]
    and the improved B-CNN [[146](#bib.bib146)] used the Newton-Schulz iteration to
    approximate matrix square-root normalization with only matrix multiplication to
    decrease training time. Recently, MOMN [[106](#bib.bib106)] was proposed to simultaneously
    normalize a bilinear representation in terms of square-root, low-rank, and sparsity
    all within a multi-objective optimization framework.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\ell_{2}$ 特征归一化可以抑制高响应的常见模式，从而增强那些判别特征（*即*，视觉突发性问题 [[104](#bib.bib104),
    [144](#bib.bib144)]），上述基于双线性池化的方法通常对协方差矩阵进行逐元素平方根归一化，然后进行 $\ell_{2}$-归一化以提高性能。然而，仅仅采用
    $\ell_{2}$-归一化可能会导致高阶信息不稳定，并且导致收敛速度缓慢。为此，许多方法探索了基于奇异值分解（SVD）或特征分解（EIG）的非线性缩放，以获得更稳定的二阶表示。具体而言，Li
    *et al.* [[145](#bib.bib145)] 提出了对双线性特征的特征值应用幂指数，以获得更好的识别准确性。G²DeNet [[99](#bib.bib99)]
    进一步通过高斯嵌入和矩阵平方根归一化结合了互补的一级和二级信息。iSQRT-COV [[101](#bib.bib101)] 和改进的 B-CNN [[146](#bib.bib146)]
    使用 Newton-Schulz 迭代来通过仅矩阵乘法来近似矩阵平方根归一化，以减少训练时间。最近，MOMN [[106](#bib.bib106)] 被提出，在一个多目标优化框架中，同时对双线性表示进行平方根、低秩和稀疏归一化。
- en: 5.2.2 Designing Specific Loss Functions
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 设计特定的损失函数
- en: Loss functions play an important role in the construction of deep networks.
    They can directly affect both the learned classifiers and features. Thus, designing
    fine-grained tailored loss functions is an important direction for fine-grained
    image recognition.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数在深度网络的构建中起着重要作用。它们可以直接影响学习到的分类器和特征。因此，设计针对细粒度图像识别的定制损失函数是一个重要方向。
- en: Distinct from generic image recognition, in fine-grained classification, where
    samples that belong to different classes can be visually very similar, it is reasonable
    to prevent the classifier from being too confident in its outputs (*i.e.*, discourage
    low entropy). Following this intuition, [[107](#bib.bib107)] also maximized the
    entropy of the output probability distribution when training networks on fine-grained
    tasks. Similarly, Dubey *et al.* [[108](#bib.bib108)] used a pairwise confusion
    optimization procedure to solve both overfitting and sample-specific artifacts
    in fine-grained recognition by bringing the different class-conditional probability
    distributions closer together and confusing the deep network. This allows a reduction
    in prediction over-confidence, therefore resulting in improved generalization
    performance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通的图像识别不同，在细粒度分类中，当属于不同类别的样本在视觉上非常相似时，合理的做法是防止分类器对其输出过于自信（*即*，避免低熵）。遵循这一直觉，[[107](#bib.bib107)]
    在训练细粒度任务的网络时也最大化了输出概率分布的熵。类似地，Dubey *et al.* [[108](#bib.bib108)] 采用了一种成对混淆优化过程，通过将不同类别条件概率分布拉得更近并混淆深度网络来解决细粒度识别中的过拟合和样本特定伪影。这使得预测过度自信得以减少，从而改善了泛化性能。
- en: Humans can effectively identify contrastive clues by comparing image pairs,
    and this type of metric / contrastive learning is also common in fine-grained
    recognition. Specifically, Sun *et al.* [[27](#bib.bib27)] first learned multiple
    part-corresponding attention regions and then leveraged metric learning for pulling
    same-attention same-class features closer, while pushing different-attention or
    different-class features away. Furthermore, their approach can coherently enforce
    the correlations among different object parts during training. CIN [[109](#bib.bib109)]
    pulls positive pairs closer while pushing negative pairs away via a contrastive
    channel interaction module which also exploits channel correlations between samples.
    API-Net [[111](#bib.bib111)] was also built upon a metric learning framework,
    and can adaptively discover contrastive cues from a pair of images and distinguish
    them via pairwise attention based interactions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 人类可以通过比较图像对有效地识别对比线索，这种度量/对比学习在细粒度识别中也很常见。具体来说，Sun *et al.* [[27](#bib.bib27)]
    首先学习了多个部件对应的注意区域，然后利用度量学习将相同注意、相同类别的特征拉得更近，同时将不同注意或不同类别的特征推得更远。此外，他们的方法可以在训练过程中连贯地强制不同物体部件之间的相关性。CIN
    [[109](#bib.bib109)] 通过对比通道交互模块拉近正样本对的距离，同时推远负样本对的距离，这也利用了样本之间的通道相关性。API-Net [[111](#bib.bib111)]
    也建立在度量学习框架上，可以自适应地从一对图像中发现对比线索，并通过成对注意基的交互来区分它们。
- en: Designing a single loss function for localizing part-level patterns and further
    aggregating image-level representations has also been explored in the literature.
    Specifically, Sun *et al.* [[110](#bib.bib110)] developed a gradient-boosting
    based loss function along with a diversification block to force the network to
    move swiftly to discriminate the hard classes. Concretely, the diversification
    block suppresses the discriminative regions of the class activation maps, and
    hence the network is forced to find alternative informative features. The gradient-booting
    loss focuses on difficult (*i.e.*, confusing) classes for each image and boosts
    their gradient. MC-Loss [[112](#bib.bib112)] encourages the feature channels to
    be more discriminative by focusing on various part-level regions. They propose
    a single loss that does not require any specific network modifications for partial
    localization of fine-grained objects.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中也探讨了为定位局部模式和进一步聚合图像级表示设计单一损失函数的方法。具体来说，Sun *et al.* [[110](#bib.bib110)]
    开发了一种基于梯度提升的损失函数，并配备了一个多样化模块，以迫使网络迅速区分困难的类别。具体而言，多样化模块抑制了类别激活图的判别区域，因此网络被迫寻找替代的有信息特征。梯度提升损失函数关注每张图像中的困难（*即*，混淆）类别，并提升其梯度。MC-Loss
    [[112](#bib.bib112)] 通过关注各种局部区域来鼓励特征通道更加具有判别性。他们提出了一个单一的损失函数，不需要对网络进行任何特定的修改即可进行细粒度物体的部分定位。
- en: These aforementioned loss function based fine-grained recognition methods are
    backbone-agnostic and their performance can typically be improved by using more
    powerful backbone network architectures.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些前述的基于损失函数的细粒度识别方法与骨干网络无关，它们的性能通常可以通过使用更强大的骨干网络架构来提高。
- en: 5.2.3 Other Methods
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 其他方法
- en: Beyond modelling the interactions between higher-order features and designing
    novel loss functions, another set of approaches involve constructing fine-grained
    tailored auxiliary tasks for obtaining unified and discriminative image representations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 除了建模高阶特征之间的交互和设计新颖的损失函数外，另一组方法涉及构建细粒度定制的辅助任务，以获得统一和有判别力的图像表示。
- en: BGL [[50](#bib.bib50)] was proposed to incorporate rich bipartite-graph labels
    into CNN training to model the important relationships among fine-grained classes.
    DCL [[113](#bib.bib113)] performed a “destruction and construction” process to
    enhance the difficulty of recognition to guide the network to focus on discriminative
    parts for fine-grained recognition (*i.e.*, by destruction learning) and then
    model the semantic correlation among parts of the object (*i.e.*, by construction
    learning). Similar to DCL, Du *et al.* [[115](#bib.bib115)] tackled fine-grained
    representation learning using a jigsaw puzzle generator proxy task to encourage
    the network to learn at different levels of granularity and simultaneously fuse
    features at these levels together. Recently, a more direct fine-grained feature
    learning method [[147](#bib.bib147)] was formulated with the goal of generating
    *identity-preserved* fine-grained images in an adversarial learning manner to
    directly obtain a unified fine-grained image representation. The authors showed
    that this direct feature learning approach not only preserved the identity of
    the generated images, but also significantly boosted the visual recognition performance
    in other challenging tasks like fine-grained few-shot learning [[148](#bib.bib148)].
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: BGL [[50](#bib.bib50)] 提出了将丰富的二分图标签融入 CNN 训练中，以建模细粒度类别间的重要关系。DCL [[113](#bib.bib113)]
    执行了“破坏与重建”过程，以增强识别难度，引导网络关注细粒度识别的判别部分（*即*，通过破坏学习），然后建模对象部分间的语义关联（*即*，通过重建学习）。类似于
    DCL，Du *et al.* [[115](#bib.bib115)] 使用拼图生成器代理任务来处理细粒度表示学习，以鼓励网络在不同层次上学习，并同时融合这些层次的特征。最近，提出了一种更直接的细粒度特征学习方法
    [[147](#bib.bib147)]，其目标是以对抗学习方式生成*身份保留*的细粒度图像，以直接获得统一的细粒度图像表示。作者表明，这种直接特征学习方法不仅保留了生成图像的身份，还显著提升了在细粒度少样本学习
    [[148](#bib.bib148)] 等其他具有挑战性任务中的视觉识别性能。
- en: 5.3 Recognition with External Information
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 外部信息识别
- en: 'Table III: Comparison of fine-grained “recognition with external information”
    (cf. Section [5.3](#S5.SS3 "5.3 Recognition with External Information ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey"))
    on multiple fine-grained benchmark datasets, including Birds (*CUB200-2011* [[13](#bib.bib13)]),
    Dogs (*Stanford Dogs* [[42](#bib.bib42)]), Cars (*Stanford Cars* [[43](#bib.bib43)]),
    and Aircrafts (*FGVC Aircraft* [[44](#bib.bib44)]). “External info.” denotes which
    kind of external information is used by the respective approach. “Train anno.”
    and “Test anno.” indicate the supervision used during training and testing, and
    “–” means the results are unavailable.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III：细粒度“外部信息识别”的比较（参见第 [5.3](#S5.SS3 "5.3 Recognition with External Information
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey") 节），在多个细粒度基准数据集上，包括鸟类（*CUB200-2011* [[13](#bib.bib13)]）、狗（*Stanford
    Dogs* [[42](#bib.bib42)]）、汽车（*Stanford Cars* [[43](#bib.bib43)]）和飞机（*FGVC Aircraft*
    [[44](#bib.bib44)]）。“外部信息”表示各自方法使用的外部信息类型。“训练标注”和“测试标注”表示训练和测试过程中使用的监督， “–” 表示结果不可用。'
- en: '| Methods | Published in | Train anno. | Test anno. | External info. | Backbones
    | Img. resolution | Accuracy |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表在 | 训练标注 | 测试标注 | 外部信息 | 主干网络 | 图像分辨率 | 准确率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Birds | Dogs | Cars | Aircrafts |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 鸟类 | 狗 | 汽车 | 飞机 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Fine-grained recognition with external information | With web / auxiliary
    data | HAR-CNN [[149](#bib.bib149)] | CVPR 2015 | BBox | BBox | Web data | Alex-Net
    | $224\times 224$ | – | 49.4% | 80.8% | – |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 外部信息的细粒度识别 | 通过网页 / 辅助数据 | HAR-CNN [[149](#bib.bib149)] | CVPR 2015 | BBox
    | BBox | 网页数据 | Alex-Net | $224\times 224$ | – | 49.4% | 80.8% | – |'
- en: '| Xu *et al.* [[150](#bib.bib150)] | ICCV 2015 | BBox+Parts |  | Web data |
    CaffeNet | $224\times 224$ | 84.6% | – | – | – |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Xu *et al.* [[150](#bib.bib150)] | ICCV 2015 | BBox+Parts |  | 网页数据 | CaffeNet
    | $224\times 224$ | 84.6% | – | – | – |'
- en: '| Krause *et al.* [[151](#bib.bib151)] | ECCV 2016 |  |  | Web data | Inception-v3
    | $224\times 224$ | 92.3% | 80.8% | – | 93.4% |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Krause *et al.* [[151](#bib.bib151)] | ECCV 2016 |  |  | 网页数据 | Inception-v3
    | $224\times 224$ | 92.3% | 80.8% | – | 93.4% |'
- en: '| Niu *et al.* [[152](#bib.bib152)] | CVPR 2018 |  |  | Web data | VGG-16 |
    $224\times 224$ | 76.5% | 85.2% | – | – |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Niu *et al.* [[152](#bib.bib152)] | CVPR 2018 |  |  | 网页数据 | VGG-16 | $224\times
    224$ | 76.5% | 85.2% | – | – |'
- en: '| MetaFGNet [[153](#bib.bib153)] | ECCV 2018 |  |  | Auxiliary data | ResNet-34
    | $224\times 224$ | 87.6% | 96.7% | – | – |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| MetaFGNet [[153](#bib.bib153)] | ECCV 2018 |  |  | 辅助数据 | ResNet-34 | $224\times
    224$ | 87.6% | 96.7% | – | – |'
- en: '| Xu *et al.* [[154](#bib.bib154)] | IEEE TPAMI 2018 | BBox+Parts |  | Web
    data | Alex-Net | $224\times 224$ | 84.6% | – | – | – |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Xu *et al.* [[154](#bib.bib154)] | IEEE TPAMI 2018 | BBox+Parts |  | 网络数据
    | Alex-Net | $224\times 224$ | 84.6% | – | – | – |'
- en: '| Yang *et al.* [[155](#bib.bib155)] | IEEE TIP 2018 |  |  | Web data | ResNet-50
    | $224\times 224$ | – | 87.4% | – | – |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Yang *et al.* [[155](#bib.bib155)] | IEEE TIP 2018 |  |  | 网络数据 | ResNet-50
    | $224\times 224$ | – | 87.4% | – | – |'
- en: '| Sun *et al.* [[62](#bib.bib62)] | AAAI 2019 |  |  | Web data | ResNet-50
    | $224\times 224$ | – | 87.1% | – | – |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Sun *et al.* [[62](#bib.bib62)] | AAAI 2019 |  |  | 网络数据 | ResNet-50 | $224\times
    224$ | – | 87.1% | – | – |'
- en: '|  | Zhang *et al.* [[156](#bib.bib156)] | AAAI 2020 |  |  | Web data | VGG-16
    | $224\times 224$ | 77.2% | – | 78.7% | 72.9% |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhang *et al.* [[156](#bib.bib156)] | AAAI 2020 |  |  | 网络数据 | VGG-16
    | $224\times 224$ | 77.2% | – | 78.7% | 72.9% |'
- en: '| With multi-modal data | CVL [[59](#bib.bib59)] | CVPR 2017 |  |  | Language
    texts | VGG-16 | Not given | 85.6% | – | – | – |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 利用多模态数据 | CVL [[59](#bib.bib59)] | CVPR 2017 |  |  | 语言文本 | VGG-16 | 未提供
    | 85.6% | – | – | – |'
- en: '| Zhang *et al.* [[157](#bib.bib157)] | AAAI 2018 | BBox |  | Audio | VGG-16
    | $227\times 227$ | 85.6% | – | – | – |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *et al.* [[157](#bib.bib157)] | AAAI 2018 | BBox |  | 音频 | VGG-16 |
    $227\times 227$ | 85.6% | – | – | – |'
- en: '| Zhang *et al.* [[157](#bib.bib157)] | AAAI 2018 | BBox | BBox | Audio | VGG-16
    | $227\times 227$ | 86.6% | – | – | – |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *et al.* [[157](#bib.bib157)] | AAAI 2018 | BBox | BBox | 音频 | VGG-16
    | $227\times 227$ | 86.6% | – | – | – |'
- en: '| T-CNN [[158](#bib.bib158)] | IJCAI 2018 | BBox |  | Knowledge base + Texts
    | ResNet-50 | $224\times 224$ | 86.5% | – | – | – |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| T-CNN [[158](#bib.bib158)] | IJCAI 2018 | BBox |  | 知识库 + 文本 | ResNet-50
    | $224\times 224$ | 86.5% | – | – | – |'
- en: '| KERL [[159](#bib.bib159)] | IJCAI 2018 |  |  | Knowledge base | VGG-16 |
    $224\times 224$ | 87.0% | – | – | – |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| KERL [[159](#bib.bib159)] | IJCAI 2018 |  |  | 知识库 | VGG-16 | $224\times
    224$ | 87.0% | – | – | – |'
- en: '| PMA [[160](#bib.bib160)] | IEEE TIP 2020 |  |  | Language texts | VGG-16
    | $448\times 448$ | 88.2% | – | – | – |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| PMA [[160](#bib.bib160)] | IEEE TIP 2020 |  |  | 语言文本 | VGG-16 | $448\times
    448$ | 88.2% | – | – | – |'
- en: '| PMA [[160](#bib.bib160)] | IEEE TIP 2020 |  |  | Language texts | ResNet-50
    | $448\times 448$ | 88.7% | – | – | – |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| PMA [[160](#bib.bib160)] | IEEE TIP 2020 |  |  | 语言文本 | ResNet-50 | $448\times
    448$ | 88.7% | – | – | – |'
- en: Beyond the conventional recognition paradigms, which are restricted to using
    supervision associated with the images themselves, another paradigm is to leverage
    external information, *e.g.*, web data, multi-modal data, or human-computer interactions,
    to further assist fine-grained recognition.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的识别范式（这些范式限制于使用与图像本身相关的监督信息）之外，另一种范式是利用外部信息，*例如*，网络数据、多模态数据或人机交互，来进一步辅助细粒度识别。
- en: 5.3.1 Noisy Web Data
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 噪声网络数据
- en: Large and well-labeled training datasets are necessary in order to identify
    subtle differences between various fine-grained categories. However, acquiring
    accurate human labels for fine-grained categories is difficult due to the need
    for domain expertise and the myriads of fine-grained categories (*e.g.*, potentially
    more than tens of thousands of subordinate categories in a meta-category). As
    a result, some fine-grained recognition methods seek to utilize freely available,
    but noisy, web data to boost recognition performance. The majority of existing
    work in this line can be roughly grouped into two directions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模且标注良好的训练数据集对于识别各种细粒度类别之间的微小差异是必要的。然而，由于需要领域专业知识以及细粒度类别的数量庞大（*例如*，一个元类别中可能有数万种从属类别），因此获取准确的人类标签是困难的。因此，一些细粒度识别方法寻求利用免费的但噪声较多的网络数据来提高识别性能。现有的工作大致可以分为两个方向。
- en: 'The first direction involves scraping noisy labeled web data for the categories
    of interest as training data, which is regarded as *webly supervised learning* [[161](#bib.bib161),
    [62](#bib.bib62), [154](#bib.bib154)]. These approaches typically concentrate
    on: 1) overcoming the domain gap between easily acquired web images and the well-labeled
    data from standard datasets; and 2) reducing the negative effects caused by the
    noisy data. For instance, HAR-CNN [[149](#bib.bib149)] utilized easily annotated
    meta-classes inherent in the fine-grained data and also acquired a large number
    of meta-class-labeled images from the web to regularize the models for improving
    recognition accuracy in a multi-task manner (*i.e.*, for both the fine-grained
    and the meta-class data recognition task). Xu *et al.* [[150](#bib.bib150)] investigated
    if fine-grained web images could provide weakly-labeled information to augment
    deep features and thus contribute to robust object classifiers by building a multi-instance
    (MI) learner, *i.e.*, treating the image as the MI bag and the proposal part bounding
    boxes as the instances of MI. Krause *et al.* [[151](#bib.bib151)] introduced
    an alternative approach to combine a generic classification model with web data
    by excluding images that appear in search results for more than one category to
    combat cross-domain noise. Inspired by adversarial learning [[162](#bib.bib162)],
    [[62](#bib.bib62)] proposed an adversarial discriminative loss to encourage representation
    coherence between standard and web data.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方向涉及从带噪声的标注网页数据中抓取感兴趣的类别作为训练数据，这被视为*网络监督学习*[[161](#bib.bib161), [62](#bib.bib62),
    [154](#bib.bib154)]。这些方法通常集中于：1）克服易获取的网络图像与标准数据集中良好标注的数据之间的领域差距；2）减少由噪声数据引起的负面影响。例如，HAR-CNN[[149](#bib.bib149)]利用了细粒度数据中固有的易标注元类别，并从网络上获取了大量标注了元类别的图像，以通过多任务方式正则化模型，从而提高识别准确性（*即*，用于细粒度和元类别数据识别任务）。Xu
    *et al.*[[150](#bib.bib150)]研究了细粒度网络图像是否可以提供弱标注信息来增强深度特征，从而通过构建多实例（MI）学习器来贡献于稳健的对象分类器，*即*，将图像视为MI包，将建议的部分边界框视为MI的实例。Krause
    *et al.*[[151](#bib.bib151)]介绍了一种替代方法，通过排除在搜索结果中出现多于一个类别的图像，将通用分类模型与网络数据结合起来，以对抗跨领域噪声。受对抗学习[[162](#bib.bib162)]的启发，[62](#bib.bib62)提出了一种对抗性判别损失，以鼓励标准数据和网络数据之间的表示一致性。
- en: The second direction is to transfer the knowledge from auxiliary categories
    with well-labeled training data to the test categories, which usually employs
    zero-shot learning [[63](#bib.bib63)] or meta learning [[163](#bib.bib163)]. Niu *et
    al.* [[63](#bib.bib63)] exploited zero-shot learning to transfer knowledge from
    annotated fine-grained categories to other fine-grained categories. Subsequently,
    Zhang *et al.* [[153](#bib.bib153)], Yang *et al.* [[155](#bib.bib155)], and Zhang *et
    al.* [[156](#bib.bib156)] investigated different approaches for selecting high-quality
    web training images to expand the training set. Zhang *et al.* [[153](#bib.bib153)]
    proposed a novel regularized meta-learning objective to guide the learning of
    network parameters so they are optimal for adapting to the target fine-grained
    categories. Yang *et al.* [[155](#bib.bib155)] designed an iterative method that
    progressively selects useful images by modifying the label assignment using multiple
    labels to lessen the impact of the labels from the noisy web data. Zhang *et al.* [[156](#bib.bib156)]
    leveraged the prediction scores in different training epochs to supervise the
    separation of useful and irrelevant noisy web images.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个方向是将从辅助类别中获得的知识转移到测试类别，这通常采用零样本学习[[63](#bib.bib63)]或元学习[[163](#bib.bib163)]。Niu
    *et al.*[[63](#bib.bib63)]利用零样本学习将知识从注释的细粒度类别转移到其他细粒度类别。随后，Zhang *et al.*[[153](#bib.bib153)]、Yang
    *et al.*[[155](#bib.bib155)]和Zhang *et al.*[[156](#bib.bib156)]探讨了选择高质量网络训练图像以扩展训练集的不同方法。Zhang
    *et al.*[[153](#bib.bib153)]提出了一种新颖的正则化元学习目标，以指导网络参数的学习，使其在适应目标细粒度类别时达到最佳。Yang
    *et al.*[[155](#bib.bib155)]设计了一种迭代方法，通过使用多个标签修改标签分配来逐步选择有用的图像，从而减少来自噪声网络数据的标签影响。Zhang
    *et al.*[[156](#bib.bib156)]利用不同训练时期的预测分数来监督有用和无关的噪声网络图像的分离。
- en: 5.3.2 Multi-Modal Data
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 多模态数据
- en: Multi-modal analysis has attracted a lot of attention with the rapid growth
    of multi-media data, *e.g.*, image, text, knowledge bases, etc. In fine-grained
    recognition, multi-modal data can be used to establish joint-representations/embeddings
    by incorporating multi-modal data in order to boost fine-grained recognition accuracy.
    Compared with strong semantic supervision from fine-grained images (*e.g.*, part
    annotations), text descriptions are a weak form of supervision (*i.e.*, they only
    provide image-level supervision). One advantage however, is that text descriptions
    can be relatively accurately generated by non-experts. Thus, they are both easy
    and cheap to be collected. In addition, high-level knowledge graphs, when available,
    can contain rich knowledge (*e.g.*, *DBpedia* [[164](#bib.bib164)]). In practice,
    both text descriptions and knowledge bases are useful extra guidance for advancing
    fine-grained image representation learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 随着多媒体数据的快速增长，多模态分析引起了大量关注，*例如*，图像、文本、知识库等。在细粒度识别中，通过结合多模态数据来建立联合表示/嵌入，以提高细粒度识别的准确性。与来自细粒度图像的强语义监督（*例如*，部件注释）相比，文本描述是一种较弱的监督形式（*即*，它们仅提供图像级别的监督）。然而，一个优势是，文本描述可以相对准确地由非专家生成。因此，它们既容易又便宜收集。此外，当可用时，高级知识图谱可以包含丰富的知识（*例如*，*DBpedia*[[164](#bib.bib164)]）。在实际应用中，文本描述和知识库都是推动细粒度图像表示学习的有用额外指导。
- en: '![Refer to caption](img/6ca6d1c1715fce9b694281c505b51370.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6ca6d1c1715fce9b694281c505b51370.png)'
- en: 'Figure 10: An example knowledge graph for modeling category-attribute correlations
    in *CUB200-2011* [[13](#bib.bib13)].'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：一个用于建模*CUB200-2011*中类别-属性相关性的知识图谱示例[[13](#bib.bib13)]。
- en: 'Reed *et al.* [[58](#bib.bib58)] collected text descriptions, and introduced
    a structured joint embedding for zero-shot fine-grained image recognition by combining
    text and images. Later, He and Peng [[59](#bib.bib59)] combined vision and language
    bi-streams in a joint end-to-end fashion to preserve the intra-modality and inter-modality
    information for generating complementary fine-grained representations. Later,
    PMA [[160](#bib.bib160)] proposed a mask-based self-attention mechanism to capture
    the most discriminative parts in the visual modality. In addition, they explored
    using out-of-visual-domain knowledge using language with query-relational attention.
    Multiple PMA blocks for the vision and language modalities were aggregated and
    stacked using the proposed progressive mask strategy. For fine-grained recognition
    with knowledge bases, some existing works  [[159](#bib.bib159), [158](#bib.bib158)]
    have introduced knowledge base information (using attribute label associations,
    cf. Figure [10](#S5.F10 "Figure 10 ‣ 5.3.2 Multi-Modal Data ‣ 5.3 Recognition
    with External Information ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image
    Analysis with Deep Learning: A Survey")) to implicitly enrich the embedding space,
    while also reasoning about the discriminative attributes of fine-grained objects.
    Concretely, T-CNN [[158](#bib.bib158)] explored using semantic embeddings from
    knowledge bases and text, and then trained a CNN to linearly map image features
    to the semantic embedding space to aggregate multi-modal information. To incorporate
    the knowledge representation into image features, KERL [[159](#bib.bib159)] employed
    a gated graph network to propagate node messages through the graph to generate
    the knowledge representation. Finally, [[157](#bib.bib157)] incorporated audio
    information related to the fine-grained visual categories of interest to boost
    recognition accuracy.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reed *等* [[58](#bib.bib58)] 收集了文本描述，并通过结合文本和图像引入了结构化的联合嵌入来实现零样本细粒度图像识别。后来，He
    和 Peng [[59](#bib.bib59)] 将视觉和语言双流结合在一个端到端的方式中，以保留内模态和跨模态信息，用于生成互补的细粒度表示。随后，PMA
    [[160](#bib.bib160)] 提出了基于掩码的自注意力机制，以捕捉视觉模态中最具辨别力的部分。此外，他们探索了使用语言与查询关系注意力来利用超视觉领域的知识。使用所提出的渐进掩码策略将多个
    PMA 块用于视觉和语言模态进行聚合和堆叠。对于具有知识库的细粒度识别，一些现有的工作 [[159](#bib.bib159), [158](#bib.bib158)]
    引入了知识库信息（使用属性标签关联，参见图 [10](#S5.F10 "Figure 10 ‣ 5.3.2 Multi-Modal Data ‣ 5.3 Recognition
    with External Information ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image
    Analysis with Deep Learning: A Survey")）以隐式丰富嵌入空间，同时也推理细粒度对象的辨别属性。具体而言，T-CNN [[158](#bib.bib158)]
    探索了使用来自知识库和文本的语义嵌入，然后训练 CNN 将图像特征线性映射到语义嵌入空间，以汇总多模态信息。为了将知识表示融入图像特征，KERL [[159](#bib.bib159)]
    采用了一个门控图网络，通过图中的节点消息传播来生成知识表示。最后，[[157](#bib.bib157)] 纳入了与感兴趣的细粒度视觉类别相关的音频信息，以提高识别准确性。'
- en: 5.3.3 Humans-in-the-Loop
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 人机协作
- en: Human-in-the-loop methods [[165](#bib.bib165)] combine the complementary strengths
    of human knowledge with computer vision algorithms. Fine-grained recognition with
    humans-in-the-loop is typically posed in an iterative fashion and requires the
    vision system to be intelligent about when it queries the human for assistance.
    Generally, for these kinds of recognition methods, in each round the system is
    seeking to understand how humans perform recognition, *e.g.*, by asking expert
    humans to label the image class [[166](#bib.bib166)], or by identifying key part
    localization and selecting discriminative features [[167](#bib.bib167)] for fine-grained
    recognition.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 人机协作方法 [[165](#bib.bib165)] 结合了人类知识和计算机视觉算法的互补优势。人机协作的细粒度识别通常以迭代的方式进行，要求视觉系统在何时向人类寻求帮助时具备智能。一般而言，对于这些识别方法，在每一轮中系统都在寻求理解人类如何进行识别，*例如*，通过要求专家对图像类别进行标注
    [[166](#bib.bib166)]，或通过识别关键部位定位并选择具有辨别力的特征 [[167](#bib.bib167)] 来进行细粒度识别。
- en: 5.4 Summary and Discussion
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 摘要与讨论
- en: 'The *CUB200-2011* [[13](#bib.bib13)], *Stanford Dogs* [[42](#bib.bib42)], *Stanford
    Cars* [[43](#bib.bib43)], and *FGVC Aircraft* [[44](#bib.bib44)] benchmarks are
    among the most influential datasets in fine-grained recognition. Tables [II](#S5.T2
    "Table II ‣ 5.1 Recognition by Localization-Classification Subnetworks ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")
    and [III](#S5.T3 "Table III ‣ 5.3 Recognition with External Information ‣ 5 Fine-Grained
    Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")
    summarize results achieved by the fine-grained methods belonging to three recognition
    learning paradigms outlined above, *i.e.*, “recognition by localization-classification
    subnetworks”, “recognition by end-to-end feature encoding”, and “recognition with
    external information”. A chronological overview can be seen in Figure [7](#S4.F7
    "Figure 7 ‣ 4 Benchmark Datasets ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey"). The main observations can be summarized as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*CUB200-2011* [[13](#bib.bib13)]、*斯坦福犬* [[42](#bib.bib42)]、*斯坦福汽车* [[43](#bib.bib43)]
    和 *FGVC 飞机* [[44](#bib.bib44)] 基准是细粒度识别中最有影响力的数据集之一。表 [II](#S5.T2 "Table II ‣
    5.1 Recognition by Localization-Classification Subnetworks ‣ 5 Fine-Grained Image
    Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey") 和 [III](#S5.T3
    "Table III ‣ 5.3 Recognition with External Information ‣ 5 Fine-Grained Image
    Recognition ‣ Fine-Grained Image Analysis with Deep Learning: A Survey") 总结了属于上述三种识别学习范式的细粒度方法所取得的结果，*即*，“通过定位-分类子网络进行识别”、“通过端到端特征编码进行识别”和“通过外部信息进行识别”。时间顺序的概述可以在图
    [7](#S4.F7 "Figure 7 ‣ 4 Benchmark Datasets ‣ Fine-Grained Image Analysis with
    Deep Learning: A Survey") 中看到。主要观察结果总结如下：'
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'There is an explicit correspondence between the reviewed methods and the aforementioned
    challenges of fine-grained recognition. Specifically, the challenge of capturing
    subtle visual differences can be overcome by localization-classification methods
    (cf. Section [5.1](#S5.SS1 "5.1 Recognition by Localization-Classification Subnetworks
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey")) or via specific construction-based tasks [[113](#bib.bib113), [115](#bib.bib115),
    [147](#bib.bib147)], as well as human-in-the-loop methods. The challenge of characterizing
    fine-grained tailored features is alleviated by performing high-order feature
    interactions or by leveraging multi-modality data. Finally, the challenging nature
    of FGIA can be somewhat addressed by designing specific loss functions [[107](#bib.bib107),
    [108](#bib.bib108), [110](#bib.bib110)] for achieving better accuracy.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '回顾的方法与上述细粒度识别挑战之间有明确的对应关系。具体而言，通过定位-分类方法（参见第 [5.1](#S5.SS1 "5.1 Recognition
    by Localization-Classification Subnetworks ‣ 5 Fine-Grained Image Recognition
    ‣ Fine-Grained Image Analysis with Deep Learning: A Survey) 节）或通过特定的构造任务 [[113](#bib.bib113),
    [115](#bib.bib115), [147](#bib.bib147)] 以及人机协作方法，可以克服捕捉细微视觉差异的挑战。通过执行高阶特征交互或利用多模态数据，可以缓解特征细粒度定制的挑战。最后，通过设计特定的损失函数
    [[107](#bib.bib107), [108](#bib.bib108), [110](#bib.bib110)] 来实现更好的准确性，从而在一定程度上解决
    FGIA 的挑战性。'
- en: 'Table IV: Comparative fine-grained recognition results on *CUB200-2011* using
    different input image resolutions. The results in this table are conducted based
    on a vanilla ResNet-50 trained at the respective resolution.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 IV：在使用不同输入图像分辨率的情况下，对*CUB200-2011*的细粒度识别结果进行比较。这张表中的结果是基于在相应分辨率下训练的原始 ResNet-50
    进行的。
- en: '| Resolution | $224\times 224$ | $280\times 280$ | $336\times 336$ | $392\times
    392$ |'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 分辨率 | $224\times 224$ | $280\times 280$ | $336\times 336$ | $392\times 392$
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Accuracy | 81.6% | 83.3% | 85.0% | 85.6% |'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 准确率 | 81.6% | 83.3% | 85.0% | 85.6% |'
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Among the different learning paradigms, the “recognition by localization-classification
    subnetworks” and “recognition by end-to-end feature encoding” paradigms are the
    two most frequently investigated ones.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在不同的学习范式中，“通过定位-分类子网络进行识别”和“通过端到端特征编码进行识别”是最常被研究的两种方法。
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part-level reasoning of fine-grained object categories boosts fine-grained recognition
    accuracy especially for non-rigid objects, *e.g.*, birds. Modeling the internal
    semantic interactions/correlations among discriminative parts has attracted increased
    attention in recent years, cf. [[71](#bib.bib71), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [81](#bib.bib81), [27](#bib.bib27), [94](#bib.bib94), [109](#bib.bib109)].
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对细粒度物体类别的部件级推理特别是对非刚性物体（*例如*，鸟类）能提升细粒度识别的准确性。对区分部件之间内部语义互动/关联的建模近年来受到越来越多的关注，参见[[71](#bib.bib71),
    [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [81](#bib.bib81), [27](#bib.bib27),
    [94](#bib.bib94), [109](#bib.bib109)]。
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Non-rigid fine-grained object recognition (*e.g.*, birds or dogs) is more challenging
    than rigid fine-grained objects (*e.g.*, cars or aircrafts), which is partly due
    to the variation on object appearance.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非刚性细粒度物体识别（*例如*，鸟类或狗）比刚性细粒度物体（*例如*，汽车或飞机）更具挑战性，这部分是由于物体外观的变化。
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-grained image recognition performance improves as image resolution increases
    [[168](#bib.bib168)]. Comparison results on *CUB200-2011* of different image resolutions
    are reported in Table [IV](#S5.T4 "Table IV ‣ 1st item ‣ 5.4 Summary and Discussion
    ‣ 5 Fine-Grained Image Recognition ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey").'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '随着图像分辨率的提高，细粒度图像识别性能也在提升[[168](#bib.bib168)]。不同图像分辨率在*CUB200-2011*上的对比结果见表[IV](#S5.T4
    "Table IV ‣ 1st item ‣ 5.4 Summary and Discussion ‣ 5 Fine-Grained Image Recognition
    ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")。'
- en: •
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: There is a trade-off between recognition and localization ability for the “recognition
    by localization-classification subnetworks” paradigm, which might impact a single
    integrated network’s recognition accuracy. Such a trade-off is also reflected
    in practice when trying to achieve better recognition results, in that training
    usually involves alternating optimization of the two networks or separately training
    the two followed by joint tuning. Alternating or multistage strategies complicate
    the tuning of the integrated network.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于“定位-分类子网络”范式，识别能力和定位能力之间存在权衡，这可能影响单一集成网络的识别准确性。这种权衡在实践中也有所体现，例如，在尝试获得更好的识别结果时，训练通常涉及交替优化两个网络或分别训练两个网络后进行联合调优。交替或多阶段策略使得集成网络的调优更加复杂。
- en: •
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While effective, most end-to-end encoding networks are less human-interpretable
    and less consistent in their accuracy across non-rigid and rigid visual domains
    compared to localization-classification subnetworks. Recently, it has been observed
    that several higher-order pooling methods attempt to understand such kind of methods
    by presenting visual interpretation [[141](#bib.bib141)] or from an optimization
    perspective [[169](#bib.bib169)].
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管有效，但大多数端到端编码网络在解释性上不如定位-分类子网络，并且在非刚性和刚性视觉领域中的准确性一致性较差。最近观察到，几个高阶池化方法尝试通过呈现视觉解释[[141](#bib.bib141)]或从优化角度[[169](#bib.bib169)]来理解此类方法。
- en: •
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: “Recognition by localization-classification subnetworks” based methods are challenging
    to apply when the fine-grained parts are not consistent across the meta-categories
    (*e.g.*, *iNaturalist* [[2](#bib.bib2)]). Here, unified end-to-end feature encoding
    methods are more appropriate.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当细粒度部分在元类别之间不一致时（*例如*，*iNaturalist* [[2](#bib.bib2)]），基于“定位-分类子网络”方法的应用面临挑战。在这种情况下，统一的端到端特征编码方法更为合适。
- en: 6 Fine-Grained Image Retrieval
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 细粒度图像检索
- en: Fine-grained retrieval is another fundamental aspect of FGIA that has gained
    more traction in recent years. What distinguishes fine-grained retrieval from
    fine-grained recognition is that in addition to estimating the sub-category correctly,
    it is also necessary to rank all the instances so that images belonging to the
    same sub-category are ranked highest based on the fine-grained details in the
    query. Specifically, in fine-grained retrieval we are given a database of images
    of the same meta-category (*e.g.*, birds or cars) and a query, and the goal is
    to return images related to the query based on relevant fine-grained features.
    Compared to generic image retrieval, which focuses on retrieving near-duplicate
    images based on similarities in their content (*e.g.*, texture, color, and shapes),
    fine-grained retrieval focuses on retrieving the images of the same category type
    (*e.g.*, the same subordinate species of animal or the same model of vehicle).
    What makes it more challenging is that objects of fine-grained categories have
    exhibit subtle differences, and can vary in pose, scale, and orientation or can
    contain large cross-modal differences (*e.g.*, in the case of sketch-based retrieval).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度检索是 FGIA 另一个基本方面，近年来获得了更多关注。细粒度检索与细粒度识别的区别在于，除了正确估计子类别外，还需要对所有实例进行排序，使得属于同一子类别的图像在基于查询的细粒度细节的基础上排在最前面。具体而言，在细粒度检索中，我们给定一个同一元类别（*例如*，鸟类或汽车）的图像数据库和一个查询，目标是根据相关的细粒度特征返回与查询相关的图像。与关注于根据内容相似性（*例如*，纹理、颜色和形状）检索近似重复图像的通用图像检索相比，细粒度检索关注的是检索同一类别类型的图像（*例如*，相同的动物下属物种或相同的车辆型号）。更具挑战性的是，细粒度类别的对象具有微妙的差异，可能在姿态、尺度和方向上有所不同，或者可能存在较大的跨模态差异（*例如*，在基于草图的检索中）。
- en: '![Refer to caption](img/8d08cb2791e26120676333e2f14984fa.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d08cb2791e26120676333e2f14984fa.png)'
- en: 'Figure 11: An illustration of fine-grained content-based image retrieval (FG-CBIR).
    Given a query image (*aka* probe) depicting a “Dodge Charger Sedan 2012”, fine-grained
    retrieval is required to return images of the same car model from a car database
    (*aka* galaxy). In this figure, the fourth returned image, marked with a red outline,
    is incorrect as it is a different car model, it is a “Dodge Caliber Wagon 2012”.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：细粒度内容基图像检索（FG-CBIR）的示意图。给定一个描绘“Dodge Charger Sedan 2012”的查询图像（*即*探针），细粒度检索需要从汽车数据库（*即*银河）中返回相同车型的图像。在此图中，第四个返回的图像用红色轮廓标记，为错误图像，因为它是不同的车型，是“Dodge
    Caliber Wagon 2012”。
- en: 'Table V: Comparison of recent fine-grained content-based image retrieval methods
    on CUB200-2011 [[13](#bib.bib13)] and Stanford Cars [[43](#bib.bib43)]. *Recall*@$K$
    is the average recall over all query images in the test set.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：最近的细粒度内容基图像检索方法在 CUB200-2011 [[13](#bib.bib13)] 和 Stanford Cars [[43](#bib.bib43)]
    的比较。*召回率*@$K$ 是测试集中所有查询图像的平均召回率。
- en: '| Methods | Published in | Supervised | Backbones | Img. Resolution | Recall@$K$
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表时间 | 监督学习 | 主干网络 | 图像分辨率 | 召回率@$K$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Birds | Cars |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 鸟类 | 汽车 |'
- en: '| --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | 2 | 4 | 8 | 1 | 2 | 4 | 8 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 4 | 8 | 1 | 2 | 4 | 8 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SCDA [[32](#bib.bib32)] | TIP 2017 |  | VGG-16 | $224\times 224$ | 62.2%
    | 74.2% | 83.2% | 90.1% | 58.5% | 69.8% | 79.1% | 86.2% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| SCDA [[32](#bib.bib32)] | TIP 2017 |  | VGG-16 | $224\times 224$ | 62.2%
    | 74.2% | 83.2% | 90.1% | 58.5% | 69.8% | 79.1% | 86.2% |'
- en: '| CRL-WSL [[170](#bib.bib170)] | IJCAI 2018 | $\checkmark$ | VGG-16 | $224\times
    224$ | 65.9% | 76.5% | 85.3% | 90.3% | 63.9% | 73.7% | 82.1% | 89.2% |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| CRL-WSL [[170](#bib.bib170)] | IJCAI 2018 | $\checkmark$ | VGG-16 | $224\times
    224$ | 65.9% | 76.5% | 85.3% | 90.3% | 63.9% | 73.7% | 82.1% | 89.2% |'
- en: '| DGCRL [[28](#bib.bib28)] | AAAI 2019 | $\checkmark$ | ResNet-50 | Not given
    | 67.9% | 79.1% | 86.2% | 91.8% | 75.9% | 83.9% | 89.7% | 94.0% |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| DGCRL [[28](#bib.bib28)] | AAAI 2019 | $\checkmark$ | ResNet-50 | 未提供 | 67.9%
    | 79.1% | 86.2% | 91.8% | 75.9% | 83.9% | 89.7% | 94.0% |'
- en: '| Zeng *et al.* [[171](#bib.bib171)] | Image and Vis. Comp. 2020 | $\checkmark$
    | ResNet-50 | $224\times 224$ | 70.1% | 79.8% | 86.9% | 92.0% | 86.7% | 91.7%
    | 95.2% | 97.0% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Zeng *等人* [[171](#bib.bib171)] | Image and Vis. Comp. 2020 | $\checkmark$
    | ResNet-50 | $224\times 224$ | 70.1% | 79.8% | 86.9% | 92.0% | 86.7% | 91.7%
    | 95.2% | 97.0% |'
- en: 'Fine-grained retrieval techniques have been widely used in commercial applications,
    *e.g.*, e-commerce (searching fine-grained products [[172](#bib.bib172)]), touch-screen
    devices (searching fine-grained objects by sketches [[53](#bib.bib53)]), crime
    prevention (searching face photos [[173](#bib.bib173)]), among others. Depending
    on the type of query image, the most studied areas of fine-grained image retrieval
    can be separated into two groups: fine-grained content-based image retrieval (FG-CBIR,
    cf. Figure [11](#S6.F11 "Figure 11 ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey")) and fine-grained sketch-based image
    retrieval (FG-SBIR, cf. Figure [12](#S6.F12 "Figure 12 ‣ 6.1 Content-based Fine-Grained
    Image Retrieval ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey")). Fine-grained image retrieval can also be expanded
    into fine-grained cross-media retrieval [[57](#bib.bib57)], which can utilize
    one media type to retrieve any media types, for example using an image to retrieve
    relevant text, video, or audio.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度检索技术已广泛应用于商业领域，例如，电子商务（搜索细粒度产品[[172](#bib.bib172)]），触摸屏设备（通过草图搜索细粒度对象[[53](#bib.bib53)]），犯罪预防（搜索人脸照片[[173](#bib.bib173)]）等。根据查询图像的类型，细粒度图像检索的主要研究领域可以分为两组：基于内容的细粒度图像检索（FG-CBIR，参见图[11](#S6.F11
    "图 11 ‣ 6 细粒度图像检索 ‣ 深度学习的细粒度图像分析：综述")）和基于草图的细粒度图像检索（FG-SBIR，参见图[12](#S6.F12 "图
    12 ‣ 6.1 基于内容的细粒度图像检索 ‣ 6 细粒度图像检索 ‣ 深度学习的细粒度图像分析：综述")）。细粒度图像检索还可以扩展到细粒度跨媒体检索[[57](#bib.bib57)]，它可以利用一种媒体类型检索任何媒体类型，例如使用图像来检索相关的文本、视频或音频。
- en: For performance evaluation, following the standard convention, FG-CBIR performance
    is typically measured using *Recall@*$K$ [[174](#bib.bib174)] which is the average
    recall score over all $M$ query images in the test set. For each query, the top
    $K$ relevant images are returned. The recall score will be $1$ if there are at
    least one positive image in the top $K$ returned images, and $0$ otherwise. By
    formulation, the definition of *Recall@*$K$ is as follows
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估性能，按照标准惯例，FG-CBIR性能通常使用*Recall@*$K$ [[174](#bib.bib174)]进行衡量，这是一组测试集中所有$M$个查询图像的平均召回率。对于每个查询，返回前$K$个相关图像。如果在前$K$个返回的图像中至少有一个正例图像，则召回率为$1$，否则为$0$。根据公式，*Recall@*$K$的定义如下：
- en: '|  | $\emph{Recall@}K=\frac{1}{M}\sum_{i=1}^{M}{\rm score_{i}}\,.$ |  | (7)
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $\emph{Recall@}K=\frac{1}{M}\sum_{i=1}^{M}{\rm score_{i}}\,.$ |  | (7)
    |'
- en: 'For measuring FG-SBIR performance, *Accuracy@*$K$ is commonly used, which is
    the percentage of sketches whose true-match photos are ranked in the top $K$:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量FG-SBIR的性能，通常使用*Accuracy@*$K$，这是指真实匹配的照片在前$K$中排名的草图的百分比：
- en: '|  | $\emph{Accuracy@}K=\frac{&#124;I_{\rm correct}^{K}&#124;}{K}\,,$ |  |
    (8) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $\emph{Accuracy@}K=\frac{&#124;I_{\rm correct}^{K}&#124;}{K}\,,$ |  |
    (8) |'
- en: where $|I_{\rm correct}^{K}|$ is the number of true-match photos in top $K$.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$|I_{\rm correct}^{K}|$是前$K$中的真实匹配照片的数量。
- en: 6.1 Content-based Fine-Grained Image Retrieval
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 基于内容的细粒度图像检索
- en: SCDA [[32](#bib.bib32)] is one of the earliest examples of fine-grained image
    retrieval that used deep learning. It employs a pre-trained CNN to select meaningful
    deep descriptors by localizing the main object in an image without using explicit
    localization supervision. Unsurprisingly, they show that selecting only useful
    deep descriptors, by removing background features, can significantly benefit retrieval
    performance in such an unsupervised retrieval setting (*i.e.*, requiring no image
    labels). Recently, supervised metric learning based approaches (*e.g.*, [[170](#bib.bib170),
    [28](#bib.bib28)]) have been proposed to overcome the retrieval accuracy limitations
    of unsupervised retrieval. These methods still include additional sub-modules
    specifically tailored for fine-grained objects, *e.g.*, the weakly-supervised
    localization module proposed in [[170](#bib.bib170)], which is in turn inspired
    by  [[32](#bib.bib32)]. CRL-WSL [[170](#bib.bib170)] employed a centralized ranking
    loss with a weakly-supervised localization approach to train their feature extractor.
    DGCRL [[28](#bib.bib28)] eliminated the gap between inner-product and the Euclidean
    distance in the training and test stages by adding a Normalize-Scale layer to
    enhance the intra-class separability and inter-class compactness with their Decorrelated
    Global-aware Centralized Ranking Loss. Recently, the Piecewise Cross Entropy loss [[171](#bib.bib171)]
    was proposed by modifying the traditional cross entropy function by reducing the
    confidence of the fine-grained model, which is similar to the basic idea of following
    the natural prediction confidence scores for fine-grained categories [[107](#bib.bib107),
    [108](#bib.bib108)].
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: SCDA [[32](#bib.bib32)] 是最早采用深度学习进行细粒度图像检索的实例之一。它利用预训练的CNN，通过在图像中定位主要物体来选择有意义的深度描述符，而无需显式的定位监督。不出所料，他们展示了通过去除背景特征，仅选择有用的深度描述符，可以显著提高这种无监督检索设置下的检索性能（*即*，无需图像标签）。最近，基于监督度量学习的方法（*例如*，[[170](#bib.bib170),
    [28](#bib.bib28)]）被提出，以克服无监督检索的准确性限制。这些方法仍然包含专门针对细粒度对象的附加子模块，*例如*，[[170](#bib.bib170)]中提出的弱监督定位模块，灵感来源于[[32](#bib.bib32)]。CRL-WSL
    [[170](#bib.bib170)] 采用集中排名损失与弱监督定位方法来训练其特征提取器。DGCRL [[28](#bib.bib28)] 通过添加Normalize-Scale层，消除了训练和测试阶段内积与欧几里得距离之间的差距，从而通过其去相关全球感知集中排名损失提高了类内可分性和类间紧凑性。最近，Piecewise
    Cross Entropy损失[[171](#bib.bib171)] 通过降低细粒度模型的置信度来修改传统的交叉熵函数，这与跟随自然预测置信度评分的基本思想类似，适用于细粒度类别[[107](#bib.bib107),
    [108](#bib.bib108)]。
- en: 'The performance of recent fine-grained content-based image retrieval approaches
    are reported in Table [V](#S6.T5 "Table V ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained
    Image Analysis with Deep Learning: A Survey"). Although supervised metric learning
    based retrieval methods outperform their unsupervised counterparts, the absolute
    recall scores (*i.e.*, *Recall@*$K$) of the retrieval task still has room for
    further improvement. One promising direction is to integrate advanced techniques,
    *e.g.*, attention mechanisms or higher-order feature interactions, which are successful
    for fine-grained recognition into FG-CBIR to achieve better retrieval accuracy.
    However, new large-scale FG-CBIR datasets are required to drive future progress,
    which are also desirable to be associated with other characteristics or challenges,
    *e.g.*, open-world sub-category retrieval (cf. Section [2](#S2 "2 Recognition
    vs. Retrieval ‣ Fine-Grained Image Analysis with Deep Learning: A Survey")).'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '最近细粒度内容基础图像检索方法的性能如表[V](#S6.T5 "Table V ‣ 6 Fine-Grained Image Retrieval ‣
    Fine-Grained Image Analysis with Deep Learning: A Survey")所示。尽管基于监督度量学习的检索方法优于其无监督对手，但检索任务的绝对召回率（*即*，*Recall@*$K$）仍有进一步提升的空间。一个有前途的方向是将成功应用于细粒度识别的先进技术，*例如*，注意机制或高阶特征交互，集成到FG-CBIR中以实现更好的检索准确性。然而，需要新的大规模FG-CBIR数据集来推动未来的进展，这些数据集还需与其他特性或挑战相关联，*例如*，开放世界子类别检索（参见第[2](#S2
    "2 Recognition vs. Retrieval ‣ Fine-Grained Image Analysis with Deep Learning:
    A Survey")节）。'
- en: '![Refer to caption](img/78fc7084b930197f8ade6e20f8821f8c.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/78fc7084b930197f8ade6e20f8821f8c.png)'
- en: 'Figure 12: An illustration of fine-grained sketch-based image retrieval (FG-SBIR),
    where a free-hand human sketch serves as the query for instance-level retrieval
    of images. FG-SBIR is challenging due to 1) the fine-grained and cross-domain
    nature of the task and 2) free-hand sketches are highly abstract, making fine-grained
    matching even more difficult.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 细粒度基于草图的图像检索（FG-SBIR）的示意图，其中手绘草图作为实例级图像检索的查询。FG-SBIR具有挑战性，因为1）任务的细粒度和跨领域性质，以及2）手绘草图高度抽象，使得细粒度匹配更加困难。'
- en: 'Table VI: Comparison of fine-grained sketch-based image retrieval methods on
    QMUL-Shoe [[52](#bib.bib52)], QMUL-Chair [[52](#bib.bib52)], QMUL-Handbag [[54](#bib.bib54)],
    Sketchy [[53](#bib.bib53)], and QMUL-Shoe-V2 [[56](#bib.bib56)]. *Accuracy*@$K$
    is the percentage of sketches whose true-match photos are ranked in the top $K$.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 基于草图的细粒度图像检索方法在 QMUL-Shoe [[52](#bib.bib52)]、QMUL-Chair [[52](#bib.bib52)]、QMUL-Handbag
    [[54](#bib.bib54)]、Sketchy [[53](#bib.bib53)] 和 QMUL-Shoe-V2 [[56](#bib.bib56)]
    上的比较。*精度*@$K$ 是草图的真实匹配照片排名在前 $K$ 的百分比。'
- en: '| Methods | Published in | Accuracy@$K$ |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表在 | 精度@$K$ |'
- en: '| --- | --- | --- |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *QMUL-Shoe* | *QMUL-Chair* | *QMUL-Handbag* | *Sketchy* | *QMUL-Shoe-V2*
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| *QMUL-Shoe* | *QMUL-Chair* | *QMUL-Handbag* | *Sketchy* | *QMUL-Shoe-V2*
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 10 | 1 | 10 | 1 | 10 | 1 | 10 | 1 | 10 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10 | 1 | 10 | 1 | 10 | 1 | 10 | 1 | 10 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Yu *et al.* [[52](#bib.bib52)] | CVPR 2016 | 39.1% | 87.8% | 69.1% | 97.9%
    | – | – | – | – | – | – |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Yu *等* [[52](#bib.bib52)] | CVPR 2016 | 39.1% | 87.8% | 69.1% | 97.9% | –
    | – | – | – | – | – |'
- en: '| Song *et al.* [[175](#bib.bib175)] | BMVC 2016 | 50.5% | 91.3% | 78.3% |
    98.9% | – | – | – | – | – | – |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Song *等* [[175](#bib.bib175)] | BMVC 2016 | 50.5% | 91.3% | 78.3% | 98.9%
    | – | – | – | – | – | – |'
- en: '| Song *et al.* [[54](#bib.bib54)] | ICCV 2017 | 61.7% | 94.8% | 81.4% | 95.9%
    | 49.4% | 82.7% | – | – | – | – |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Song *等* [[54](#bib.bib54)] | ICCV 2017 | 61.7% | 94.8% | 81.4% | 95.9% |
    49.4% | 82.7% | – | – | – | – |'
- en: '| Li *et al.* [[55](#bib.bib55)] | IEEE TIP 2017 | 51.3% | 90.4% | 76.3% |
    97.9% | – | – | 45.3% | 98.2% | – | – |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Li *等* [[55](#bib.bib55)] | IEEE TIP 2017 | 51.3% | 90.4% | 76.3% | 97.9%
    | – | – | 45.3% | 98.2% | – | – |'
- en: '| Radenovic *et al.* [[176](#bib.bib176)] | ECCV 2018 | 54.8% | 92.2% | 85.7%
    | 97.9% | 51.2% | 85.7% | – | – | – | – |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Radenovic *等* [[176](#bib.bib176)] | ECCV 2018 | 54.8% | 92.2% | 85.7% |
    97.9% | 51.2% | 85.7% | – | – | – | – |'
- en: '| Zhang *et al.* [[177](#bib.bib177)] | ECCV 2018 | 35.7% | 84.3% | 67.1% |
    99.0% | – | – | – | – | – | – |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *等* [[177](#bib.bib177)] | ECCV 2018 | 35.7% | 84.3% | 67.1% | 99.0%
    | – | – | – | – | – | – |'
- en: '| Pang *et al.* [[178](#bib.bib178)] | CVPR 2020 | 56.5% | – | 85.9% | – |
    62.9% | – | – | – | 36.5% | – |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Pang *等* [[178](#bib.bib178)] | CVPR 2020 | 56.5% | – | 85.9% | – | 62.9%
    | – | – | – | 36.5% | – |'
- en: '| Bhunia *et al.* [[179](#bib.bib179)] | CVPR 2020 | – | – | – | – | – | –
    | – | – | – | 79.6% |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Bhunia *等* [[179](#bib.bib179)] | CVPR 2020 | – | – | – | – | – | – | – |
    – | – | 79.6% |'
- en: '| Sain *et al.* [[180](#bib.bib180)] | BMVC 2020 | – | – | – | – | – | – |
    – | – | 36.3% | 80.6% |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Sain *等* [[180](#bib.bib180)] | BMVC 2020 | – | – | – | – | – | – | – | –
    | 36.3% | 80.6% |'
- en: 6.2 Sketch-based Fine-Grained Image Retrieval
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 基于草图的细粒度图像检索
- en: The goal of fine-grained sketch-based image retrieval (FG-SBIR) is to match
    specific photo instances using a free-hand sketch as the query modality. Distinct
    from the previously discussed content-based approach, the additional sketch-photo
    domain gap lies at the centre of FG-SBIR. Thus, the key is introducing a cross-modal
    representation that not only captures fine-grained characteristics present in
    the sketches, but also possesses the ability to traverse the sketch and image
    domain gap.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 基于草图的细粒度图像检索（FG-SBIR）的目标是使用手绘草图作为查询模态来匹配特定的照片实例。与之前讨论的基于内容的方法不同，FG-SBIR的核心在于额外的草图-照片领域差距。因此，关键是引入一种跨模态表示，这种表示不仅能够捕捉草图中存在的细粒度特征，而且还具备跨越草图和图像领域差距的能力。
- en: Existing FG-SBIR solutions generally aim to train a joint embedding space where
    sketches and photos can be compared in a nearest neighbor fashion. Specifically,
    Li *et al.* [[51](#bib.bib51)] proposed the first method for FG-SBIR, where they
    learned a deformable part-based models (DPM)  [[181](#bib.bib181)] as a mid-level
    representation to discover and encode the various poses in the sketch and image
    domains independently. This was followed by a graph matching operation on the
    DPM to establish pose correspondences across the two domains. Yu *et al.* [[52](#bib.bib52)]
    first employed deep learning for this task, leveraging a triplet ranking model
    with a staged pre-training strategy to learn a joint embedding space for the two
    domains. This triplet ranking model was further augmented with auxiliary semantic
    attribute prediction and attribute ranking layers [[175](#bib.bib175)] to improve
    generalization [[52](#bib.bib52)]. The implicit challenge encountered due to the
    large sketch-photo domain gap in FG-SBIR was tackled via a discriminative-generative
    hybrid model [[182](#bib.bib182)] using cross-domain translation. Inspired on
    these approaches, Song *et al.* [[54](#bib.bib54)] introduced an attention module
    and encoded the spatial position of visual features and combined both coarse and
    fine-grained semantic knowledge. Li *et al.* [[55](#bib.bib55)] introduced a part-aware
    learning approach to reduce the instance-level domain-gap by performing subspace
    alignment with fine-grained attributes. While research on fine-grained SBIR has
    flourished over the years, a dilemma still remains – whether sketching is a better
    input modality for fine-grained image retrieval compared to other alternatives,
    *e.g.*, texts or attributes. To answer this question, Song *et al.* [[183](#bib.bib183)]
    showed the superiority of sketch over text for fine-grained retrieval, illustrating
    that each modality can complement the other when they are jointly modeled.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的FG-SBIR解决方案通常旨在训练一个联合嵌入空间，以便可以通过最近邻的方式比较草图和照片。具体来说，Li *et al.* [[51](#bib.bib51)]
    提出了FG-SBIR的第一个方法，他们学习了变形部件基础模型（DPM）[[181](#bib.bib181)]，作为中级表示来独立发现和编码草图和图像域中的各种姿势。随后对DPM进行了图匹配操作，以在两个域之间建立姿势对应关系。Yu
    *et al.* [[52](#bib.bib52)] 首次采用深度学习解决这一任务，利用具有阶段性预训练策略的三元组排序模型来学习两个领域的联合嵌入空间。这个三元组排序模型进一步通过辅助语义属性预测和属性排序层[[175](#bib.bib175)]
    来增强，以提高泛化能力[[52](#bib.bib52)]。由于FG-SBIR中草图-照片领域间隙较大所遇到的隐性挑战，通过使用跨域翻译的判别生成混合模型[[182](#bib.bib182)]
    进行了处理。受到这些方法的启发，Song *et al.* [[54](#bib.bib54)] 引入了一个注意力模块，并编码了视觉特征的空间位置，同时结合了粗粒度和细粒度的语义知识。Li
    *et al.* [[55](#bib.bib55)] 引入了一种基于部件的学习方法，通过对细粒度属性进行子空间对齐来减少实例级域间隙。尽管细粒度SBIR的研究多年来蓬勃发展，但仍然存在一个困境——草图是否比其他替代方式，例如文本或属性，作为细粒度图像检索的更好输入模态。为了解答这个问题，Song
    *et al.* [[183](#bib.bib183)] 显示了草图在细粒度检索中的优越性，说明当这些模态联合建模时，每种模态可以相互补充。
- en: In contrast to these earlier works that were mostly based on Siamese-triplet
    networks, there have been several recent attempts to advance FG-SBIR performance.
    For instance, Pang *et al.* [[56](#bib.bib56)] identified that a baseline triplet
    based model fails to generalize when exposed to unseen categories. Thus, cross-category
    generalization was introduced through a domain generalization setup, where a universal
    manifold of prototypical visual sketches was modeled to dynamically represent
    the sketch/photo. On the other hand, Bhunia *et al.* [[179](#bib.bib179)] developed
    an on-the-fly retrieval setup via reinforcement learning that begins retrieving
    photos as soon as the user starts drawing. ImageNet pre-trained weights have been
    considered as the de-facto choice for initializing sketch/photo embedding networks
    for FG-SBIR. However, following the recent progress in self-supervised learning [[184](#bib.bib184),
    [185](#bib.bib185), [186](#bib.bib186)], Pang *et al.* [[178](#bib.bib178)] performed
    a jigsaw solving strategy over mixed-modal patches between a particular photo
    and its edge-map. They showed this to be an effective pre-text task for self-supervised
    pre-training strategy and it improves FG-SBIR performance. Instead of performing
    independent sketch and photo embeddings, as in almost all previous works, Sain *et
    al.*[[180](#bib.bib180)] used paired-embeddings by employing cross-modal co-attention
    and hierarchical stroke/region-wise feature fusion in order to deal with varying
    levels of sketch detail. In spite of a significant performance boost, it is noteworthy
    that paired-embeddings incur a significant computational overhead, by a multiple
    of at least the number of gallery photos, compared to other state-of-the-art methods.
    Recently, a new scene-level fine-grained SBIR task [[187](#bib.bib187)] was explored.
    Liu *et al.* [[187](#bib.bib187)] proposed to utilize local features such as object
    instances and their visual detail, as well as global context (*e.g.*, the scene
    layout), to deal with such a task.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些大多基于 Siamese-triplet 网络的早期工作相比，近期有几个尝试提升 FG-SBIR 性能的研究。例如，Pang*等人*[[56](#bib.bib56)]
    发现基线 triplet 基于模型在遇到未见过的类别时无法泛化。因此，引入了通过领域泛化设置进行跨类别泛化，其中建模了一个通用的原型视觉草图流形，以动态表示草图/照片。另一方面，Bhunia*等人*[[179](#bib.bib179)]
    开发了通过强化学习的即刻检索设置，用户开始绘图时即开始检索照片。ImageNet 预训练权重已被视为初始化 FG-SBIR 草图/照片嵌入网络的事实上的选择。然而，随着自监督学习的最新进展
    [[184](#bib.bib184), [185](#bib.bib185), [186](#bib.bib186)]，Pang*等人*[[178](#bib.bib178)]
    在特定照片及其边缘图的混合模态补丁上执行了拼图求解策略。他们展示了这是一种有效的自监督预训练策略，并提高了 FG-SBIR 性能。与几乎所有以前的工作中独立进行的草图和照片嵌入不同，Sain*等人*[[180](#bib.bib180)]
    通过采用跨模态共同注意力和分层笔划/区域特征融合使用了配对嵌入，以处理不同级别的草图细节。尽管性能显著提升，但值得注意的是，与其他最先进的方法相比，配对嵌入带来了显著的计算开销，至少是画廊照片数量的多倍。最近，探索了一种新的场景级细粒度
    SBIR 任务 [[187](#bib.bib187)]。Liu*等人*[[187](#bib.bib187)] 提出了利用局部特征如物体实例及其视觉细节，以及全局上下文（*例如*，场景布局）来处理此类任务。
- en: 'In addition to the previous FG-SBIR methods, Zhang *et al.* [[177](#bib.bib177)]
    and Radenovic *et al.* [[176](#bib.bib176)] also evaluated on popular FG-SBIR
    datasets, in addition to category-level SBIR ones. We compare the results of recent
    FG-SBIR methods in Table [VI](#S6.T6 "Table VI ‣ 6.1 Content-based Fine-Grained
    Image Retrieval ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained Image Analysis
    with Deep Learning: A Survey"). Note that we only present methods whose evaluation
    strategies are uniform and consistent, and thus exclude those that involve problem
    specific evaluation protocols, such as zero-shot FG-SBIR [[56](#bib.bib56)].'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '除了之前的 FG-SBIR 方法，张*等人*[[177](#bib.bib177)] 和 Radenovic*等人*[[176](#bib.bib176)]
    还在流行的 FG-SBIR 数据集上进行了评估，此外还有类别级别的 SBIR 数据集。我们在表格 [VI](#S6.T6 "Table VI ‣ 6.1 Content-based
    Fine-Grained Image Retrieval ‣ 6 Fine-Grained Image Retrieval ‣ Fine-Grained Image
    Analysis with Deep Learning: A Survey") 中比较了近期 FG-SBIR 方法的结果。请注意，我们仅展示评估策略一致且统一的方法，因此排除了涉及特定问题评估协议的方法，如零-shot
    FG-SBIR [[56](#bib.bib56)]。'
- en: 7 Common Techniques Shared by both Fine-Grained Recognition and Retrieval
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 细粒度识别和检索中共有的 7 种常见技术
- en: The tasks of fine-grained image recognition and retrieval are complementary.
    As a result, there exists common techniques shared by both problems in the FGIA
    literature. In this section, we discuss these common techniques, in terms of methods
    and basic ideas, with the aim of motivating further work in these areas.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 精细图像识别和检索任务是互补的。因此，FGIA文献中存在两者共享的共同技术。在这一部分，我们讨论这些共同技术，涉及方法和基本思想，旨在激励进一步在这些领域的工作。
- en: 'Common Methods: In the context of deep learning, both fine-grained recognition
    and retrieval tasks require discriminative deep feature embeddings to distinguish
    subtle differences between fine-grained objects. While recognition aims to distinguish
    category labels, retrieval aims to return an accurate ranking. To achieve these
    goals, deep metric learning and multi-modal matching can be viewed as two common
    sets of techniques that are applicable for both fine-grained recognition and retrieval.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 常见方法：在深度学习的背景下，精细识别和检索任务都需要辨别性的深度特征嵌入，以区分精细对象之间的微妙差异。虽然识别旨在区分类别标签，但检索则旨在返回准确的排名。为了实现这些目标，深度度量学习和多模态匹配可以被视为适用于精细识别和检索的两组常见技术。
- en: Specifically, deep metric learning [[188](#bib.bib188)] attempts to map image
    data to an embedding space, where similar fine-grained images are close together
    and dissimilar images are far apart. In general, fine-grained recognition realizes
    metric learning by classification losses, where it includes a weight matrix to
    transform the embedding space into a vector of fine-grained class logits, *e.g.*, [[27](#bib.bib27),
    [109](#bib.bib109), [111](#bib.bib111)]. While, metric learning on fine-grained
    retrieval tasks (most cases without explicit image labels) is always achieved
    by means of embedding losses which operate on the relationships between fine-grained
    samples in a batch, *e.g.*, [[170](#bib.bib170), [28](#bib.bib28)].
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，深度度量学习 [[188](#bib.bib188)] 尝试将图像数据映射到嵌入空间，其中相似的精细图像彼此接近，不相似的图像则远离。通常，精细识别通过分类损失实现度量学习，其中包括一个权重矩阵来将嵌入空间转换为精细类别对数向量，例如 [[27](#bib.bib27),
    [109](#bib.bib109), [111](#bib.bib111)]。而，精细检索任务上的度量学习（大多数情况下没有明确的图像标签）总是通过嵌入损失来实现，这些损失作用于批次中精细样本之间的关系，例如 [[170](#bib.bib170),
    [28](#bib.bib28)]。
- en: Recently, multi-modal matching methods [[29](#bib.bib29), [30](#bib.bib30)]
    have emerged as powerful representation learning approaches to simultaneously
    boost fine-grained recognition and retrieval tasks. In particularly, Mafla *et
    al.* [[29](#bib.bib29)] leveraged textual information along with visual cues to
    comprehend the existing intrinsic relation between the two modalities. [[30](#bib.bib30)]
    employed a graph convolutional network to perform multi-modal reasoning and obtain
    relationship-enhanced multi-modal features. Such kind of methods reveal a new
    development trend of multi-modal learning in FGIA.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，多模态匹配方法 [[29](#bib.bib29), [30](#bib.bib30)] 作为强大的表示学习方法应运而生，能够同时提升精细识别和检索任务。特别是，Mafla
    *et al.* [[29](#bib.bib29)] 利用文本信息以及视觉线索来理解两种模态之间的内在关系。[[30](#bib.bib30)] 采用图卷积网络进行多模态推理，并获得关系增强的多模态特征。这些方法揭示了多模态学习在精细图像分析中的新发展趋势。
- en: 'Common Basic Ideas: Beyond these common methods, many basic ideas are shared
    by both fine-grained recognition and retrieval, *e.g.*, selecting useful deep
    descriptors [[31](#bib.bib31), [32](#bib.bib32)], reducing the uncertainty of
    fine-grained predictions [[107](#bib.bib107), [108](#bib.bib108), [171](#bib.bib171)],
    and deconstructing/constructing fine-grained images for learning fine-grained
    patterns [[113](#bib.bib113), [178](#bib.bib178)]. These observations further
    illustrate the benefit of consolidating work in fine-grained recognition and fine-grained
    retrieval in this survey paper.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的基本思想：除了这些常见方法外，精细识别和检索共享许多基本思想，例如，选择有用的深度描述符 [[31](#bib.bib31), [32](#bib.bib32)]，减少精细预测的不确定性 [[107](#bib.bib107),
    [108](#bib.bib108), [171](#bib.bib171)]，以及拆解/构建精细图像以学习精细模式 [[113](#bib.bib113),
    [178](#bib.bib178)]。这些观察进一步说明了在这篇综述论文中整合精细识别和精细检索工作的好处。
- en: 8 Future Directions
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 未来方向
- en: Advances in deep learning have enabled significant progress in fine-grained
    image analysis (FGIA). Despite the success however, there are still many unsolved
    problems. Thus, in this section, we aim to explicitly point out these problems,
    and highlight some open questions to motivate future progression of the field.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的进步促进了细粒度图像分析（FGIA）的显著发展。然而，尽管取得了成功，但仍然存在许多未解决的问题。因此，在这一部分，我们旨在明确指出这些问题，并突出一些未解之谜，以激励该领域的未来进展。
- en: 'Precise Definition of “Fine-Grained”: Although FGIA has existed for many years
    as a thriving sub-field of computer vision and pattern recognition, one fundamental
    problem in FGIA persists, *i.e.*, the designation lacks a precise definition [[189](#bib.bib189),
    [190](#bib.bib190)]. Specifically, the community always *qualitatively* describes
    a so-called fine-grained dataset/problem as being “fine-grained” by roughly stating
    that its target objects belong to one meta-category. However, a precise definition
    of “fine-grained” could enable us to *quantitatively* evaluate the granularity
    of a dataset. In addition, it would not only provide a better understanding of
    current algorithmic performance on tasks of different granularities, but also
    bring additional insights to the fine-grained community.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: “细粒度”的精确定义：虽然细粒度图像分析（FGIA）作为计算机视觉和模式识别的一个繁荣子领域已经存在多年，但在细粒度图像分析中仍存在一个基本问题，*即*，该术语缺乏精确定义 [[189](#bib.bib189),
    [190](#bib.bib190)]。具体来说，社区总是*定性*地将所谓的细粒度数据集/问题描述为“细粒度”，大致说明其目标对象属于一个元类别。然而，对“细粒度”的精确定义可以使我们*定量*地评估数据集的粒度。此外，它不仅可以提供对当前算法在不同粒度任务上表现的更好理解，还可以为细粒度社区带来额外的见解。
- en: 'Next-Generation Fine-Grained Datasets: Classic fine-grained datasets such as
    *CUB200-2011*, *Stanford Dogs*, *Stanford Cars*, and *Oxford Flowers*, are overwhelmingly
    used for benchmarking performance in FGIA. However, by modern standards these
    datasets are relatively small-scale and are largely saturated in terms of performance.
    In the future, it would be valuable to see additional large-scale fine-grained
    datasets being promoted to replace these existing benchmarks, *e.g.*, *iNat2017* [[2](#bib.bib2)],
    *Dogs-in-the-wild* [[27](#bib.bib27)], *RPC* [[5](#bib.bib5)]. State-of-the-art
    results on these datasets are 72.6% [[191](#bib.bib191)], 78.5% [[27](#bib.bib27)],
    80.5% [[192](#bib.bib192)], respectively, which reveals the substantial room for
    further improvement. Moreover, these new benchmarks should embrace and embody
    all the challenges associated with fine-grained learning in addition to being
    large-scale (in terms of both the number of sub-categories and images), contain
    diverse images captured in realistic settings, and include rich annotations. However,
    high-quality fine-grained datasets usually require domain-experts to annotate.
    This limits the development of fine-grained datasets to a certain extent. As a
    potential solution, constructing an unlabeled large-scale fine-grained database
    and employing unsupervised feature learning techniques (*e.g.*, self-supervised
    learning [[193](#bib.bib193)]) could benefit discriminative features learning
    and further promote unsupervised downstream tasks [[3](#bib.bib3)]. Also, synthetic
    data [[194](#bib.bib194)] is an increasingly popular tool for training deep models,
    and offers promising opportunities to be explored further for FGIA.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代细粒度数据集：经典细粒度数据集如*CUB200-2011*、*Stanford Dogs*、*Stanford Cars*和*Oxford Flowers*，在FGIA性能基准测试中被广泛使用。然而，按现代标准，这些数据集相对较小，并且在性能上已基本饱和。未来，推动更多大型细粒度数据集以替代现有基准数据集将是有价值的，*例如*，*iNat2017* [[2](#bib.bib2)]、*Dogs-in-the-wild* [[27](#bib.bib27)]、*RPC* [[5](#bib.bib5)]。这些数据集上的最新结果分别为72.6% [[191](#bib.bib191)]、78.5% [[27](#bib.bib27)]、80.5% [[192](#bib.bib192)]，这显示了进一步改进的巨大空间。此外，这些新基准数据集应当包括并体现与细粒度学习相关的所有挑战，除了要大规模（包括子类别和图像数量）外，还应包含在现实环境中拍摄的多样化图像，并包含丰富的注释。然而，高质量的细粒度数据集通常需要领域专家进行标注，这在一定程度上限制了细粒度数据集的发展。作为潜在解决方案，构建一个未标注的大规模细粒度数据库并采用无监督特征学习技术（*例如*，自监督学习 [[193](#bib.bib193)]）可能有助于判别特征学习，并进一步促进无监督下游任务 [[3](#bib.bib3)]。此外，合成数据 [[194](#bib.bib194)]
    是一个越来越受欢迎的深度模型训练工具，并为FGIA提供了进一步探索的有希望机会。
- en: 'Application to 3D Fine-Grained Tasks: Most existing FGIA approaches target
    2D fine-grained images and the value of 3D information (*e.g.*, 3D pose labels
    or 3D shape information) is under-explored. How 3D object representations boost
    the performance of 2D FGIA approaches is an interesting and important problem.
    On the other hand, how 2D FGIA approaches generalize to 3D fine-grained applications [[195](#bib.bib195)],
    *e.g.*, robotic bin picking, robot perception or augmented reality, is also worthy
    of future attention. To make progress in this area there are open questions associated
    with difficulties in obtaining accurate 3D annotations for many object categories,
    *e.g.*, animals or other non-rigid objects.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于 3D 细粒度任务：大多数现有的 FGIA 方法针对的是 2D 细粒度图像，而 3D 信息（*例如*，3D 姿态标签或 3D 形状信息）的价值尚未充分探讨。3D
    物体表示如何提升 2D FGIA 方法的性能是一个有趣且重要的问题。另一方面，2D FGIA 方法如何推广到 3D 细粒度应用中[[195](#bib.bib195)]，*例如*，机器人拾取、机器人感知或增强现实，也值得未来关注。为了在这一领域取得进展，需要解决与获取准确
    3D 注释相关的开放问题，例如动物或其他非刚性物体。
- en: 'Robust Fine-Grained Representations: One important factor which makes FGIA
    uniquely challenging is the overwhelming variability in real-world fine-grained
    images, including changes in viewpoint, object scale, pose, and factors such as
    part deformations, background clutter, and so on. Meanwhile, FGIA often dictates
    fine-grained patterns (derived from discriminative but subtle parts) to be identified
    to drive predictions, which makes it more sensitive to image resolutions, corruptions
    and perturbations, and adversarial examples. Despite advances in deep learning,
    current models are still exhibit a lack of robustness to these variations or disturbances,
    and this significantly constrains their usability in many real-world applications
    where high accuracy is essential. Therefore, how to effectively obtain robust
    fine-grained representations (*i.e.*, not only containing discriminative fine-grained
    visual clues, but also resisting the interference of irrelevant information) requires
    further in-depth exploration. As discussed above, when coupled with next generation
    fine-grained datasets, there are questions related to the utility of self-supervised
    learning in these domains [[196](#bib.bib196)]. For example, will self-supervised
    learning help improving FGIA, and do we perform self-supervision on fine-grained
    data to generate robust fine-grained representations?'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 强健的细粒度表示：使 FGIA 独特具有挑战性的一个重要因素是现实世界细粒度图像中的极大变异性，包括视点、物体尺度、姿态以及部件变形、背景杂乱等因素。同时，FGIA
    通常要求识别细粒度模式（源自具有区分性的但细微的部分）以驱动预测，这使得它对图像分辨率、损坏和扰动以及对抗性样本更加敏感。尽管深度学习取得了进展，但当前模型仍表现出对这些变化或干扰的鲁棒性不足，这在许多高准确性要求的现实应用中显著限制了它们的可用性。因此，如何有效地获得强健的细粒度表示（*即*，不仅包含具有区分性的细粒度视觉线索，还能抵御无关信息的干扰）需要进一步深入探索。如上所述，当与下一代细粒度数据集结合时，存在与自监督学习在这些领域的有效性相关的问题[[196](#bib.bib196)]。例如，自监督学习是否有助于改善
    FGIA，我们是否应该对细粒度数据进行自监督以生成强健的细粒度表示？
- en: 'Interpretable Fine-Grained Learning: Unilaterally achieving higher accuracy
    compared to non-expert humans may no longer be the primary goal of FGIA. It is
    very important for fine-grained models to not only result in high accuracy but
    also to be interpretable [[197](#bib.bib197)]. More interpretable FGIA could help
    the community to address several challenges when dealing with fine-grained tasks
    using deep learning, *e.g.*, semantically debugging network representations, learning
    via human-computer communications at the semantic level, designing more effective,
    task-specific deep models, etc.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的细粒度学习：与非专家人类相比，单方面实现更高的准确率可能不再是 FGIA 的主要目标。细粒度模型不仅需要实现高准确率，还要具备可解释性[[197](#bib.bib197)]。更具可解释性的
    FGIA 有助于社区解决在使用深度学习处理细粒度任务时面临的多个挑战，例如语义调试网络表示、通过人机通信在语义层面进行学习、设计更有效的任务特定深度模型等。
- en: 'Fine-Grained Few-Shot Learning: Humans are capable of learning a new fine-grained
    concept with very little supervision (*e.g.*, with a few image of a new species
    of bird), yet our best deep learning fine-grained systems need hundreds or thousands
    of labeled examples. Even worse, the supervision of fine-grained images are both
    time-consuming and expensive to obtain, as fine-grained objects need to be accurately
    labeled by domain experts. Thus, it is desirable to develop fine-grained few-shot
    learning (FGFS) methods [[148](#bib.bib148), [198](#bib.bib198), [199](#bib.bib199),
    [200](#bib.bib200)]. The task of FGFS requires the learning system to build classifiers
    for novel fine-grained categories from few examples (*e.g.*, less than 10). Robust
    FGFS methods could significantly strengthen the usability and scalability of fine-grained
    recognition.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度少样本学习：人类能够通过很少的监督（*例如*，仅凭几张新鸟类的图像）学习一个新的细粒度概念，而我们目前最好的深度学习细粒度系统则需要数百或数千个标注示例。更糟糕的是，细粒度图像的监督既耗时又昂贵，因为细粒度物体需要由领域专家准确标注。因此，开发细粒度少样本学习（FGFS）方法[[148](#bib.bib148),
    [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200)]是值得期望的。FGFS任务要求学习系统从少量示例（*例如*，少于10个）中为新颖的细粒度类别构建分类器。强大的FGFS方法可以显著增强细粒度识别的可用性和可扩展性。
- en: 'Fine-Grained Hashing: Recently, larger-scale fine-grained datasets have been
    released, *e.g.*, [[41](#bib.bib41), [38](#bib.bib38), [46](#bib.bib46), [2](#bib.bib2),
    [5](#bib.bib5), [3](#bib.bib3)]. In real applications like fine-grained image
    retrieval, the computational cost of finding the exact nearest neighbor can be
    prohibitively high in cases where the reference database is very large. Image
    hashing [[201](#bib.bib201), [202](#bib.bib202)] is a popular and effective technique
    for approximate nearest neighbor search, and has the potential to help with large-scale
    fine-grained data too. Therefore, targeting the big data challenge, fine-grained
    hashing [[203](#bib.bib203), [204](#bib.bib204)] is a promising direction worthy
    of further explorations.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度哈希：最近，已经发布了更大规模的细粒度数据集，*例如*，[[41](#bib.bib41), [38](#bib.bib38), [46](#bib.bib46),
    [2](#bib.bib2), [5](#bib.bib5), [3](#bib.bib3)]。在细粒度图像检索等实际应用中，当参考数据库非常庞大时，找到精确的最近邻的计算成本可能非常高。图像哈希[[201](#bib.bib201),
    [202](#bib.bib202)]是一种流行且有效的近似最近邻搜索技术，也有助于大规模细粒度数据。因此，针对大数据挑战，细粒度哈希[[203](#bib.bib203),
    [204](#bib.bib204)]是一个值得进一步探索的有前途的方向。
- en: 'Automatic Fine-Grained Models: Automated machine learning (AutoML) [[205](#bib.bib205)]
    and neural architecture search (NAS) [[206](#bib.bib206)] have attracted growing
    attention of late. AutoML targets automating the process of applying machine learning
    to real-world tasks and NAS is the process of automating neural network architecture
    design. Recent methods for AutoML and NAS could be comparable or even outperform
    hand-designed architectures in various computer vision applications. Thus, it
    is also logical that AutoML and NAS techniques could find better, and more tailor-made
    deep models for FGIA.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化细粒度模型：自动化机器学习（AutoML）[[205](#bib.bib205)]和神经网络架构搜索（NAS）[[206](#bib.bib206)]近期受到了越来越多的关注。AutoML旨在自动化将机器学习应用于现实世界任务的过程，而NAS则是自动化神经网络架构设计的过程。最近的AutoML和NAS方法在各种计算机视觉应用中可以与手工设计的架构相媲美，甚至超越它们。因此，AutoML和NAS技术也可能找到更好、更量身定制的细粒度智能（FGIA）深度模型。
- en: 'Fine-Grained Analysis in More Realistic Settings: In the past decade, FGIA
    related techniques have been developed and have achieved good performance in standard
    computer vision benchmarks, *e.g.*, [[13](#bib.bib13), [42](#bib.bib42), [43](#bib.bib43)].
    The vast majority of these existing FGIA benchmarks are however defined in a static
    and closed environment. One other big limitation of current FGIA datasets is that
    they typically contain large instances of the object (*i.e.*, the objects of interest
    occupy most of the image frame). However, these settings are not representative
    of many real-world applications, *e.g.*, recognizing retail products in storage
    racks by models trained with images collected in controlled environments [[5](#bib.bib5)]
    or recognizing/detecting tends of thousands of natural species in the wild [[2](#bib.bib2)].
    More research is needed in areas such as domain adaptation [[207](#bib.bib207),
    [208](#bib.bib208), [209](#bib.bib209)], long-tailed distributions [[210](#bib.bib210),
    [211](#bib.bib211)], open world settings [[212](#bib.bib212)], scale variations [[2](#bib.bib2)],
    fine-grained video understanding [[213](#bib.bib213), [214](#bib.bib214)], knowledge
    transfer, and resource constrained embedded deployment, to name a few.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 更现实环境中的细粒度分析：在过去十年中，FGIA相关技术得到了发展，并在标准计算机视觉基准测试中取得了良好的表现，*例如*，[[13](#bib.bib13),
    [42](#bib.bib42), [43](#bib.bib43)]。然而，现有的FGIA基准测试绝大多数都是在静态和封闭的环境中定义的。当前FGIA数据集的另一个大限制是，它们通常包含对象的大实例（*即*，感兴趣的对象占据了大部分图像框架）。然而，这些设置并不能代表许多现实世界应用的情况，*例如*，识别通过在受控环境中收集的图像训练的模型在存储架中的零售产品[[5](#bib.bib5)]，或识别/检测野外成千上万种自然物种[[2](#bib.bib2)]。需要在领域适应[[207](#bib.bib207),
    [208](#bib.bib208), [209](#bib.bib209)]、长尾分布[[210](#bib.bib210), [211](#bib.bib211)]、开放世界设置[[212](#bib.bib212)]、尺度变化[[2](#bib.bib2)]、细粒度视频理解[[213](#bib.bib213),
    [214](#bib.bib214)]、知识转移和资源受限的嵌入式部署等领域进行更多研究。
- en: 9 Conclusion
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: We have presented a comprehensive survey on recent advances in deep learning
    based fine-grained image analysis (FGIA). Specifically, we advocated a broadened
    definition of FGIA, by consolidating work in fine-grained recognition and fine-grained
    retrieval. We enumerated gaps in existing research, pointed out a series of emerging
    topics, highlighted important future research directions, and illustrated that
    the problem of FGIA is still far from solved. However, given the significant improvements
    in performance over the past decade, we remain optimistic about future progress
    as we move towards more realistic and impactful applications.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了对深度学习基础的细粒度图像分析（FGIA）最近进展的全面调查。具体而言，我们提倡通过整合细粒度识别和细粒度检索的工作，扩展FGIA的定义。我们列举了现有研究中的空白，指出了一系列新兴话题，强调了重要的未来研究方向，并说明了FGIA问题仍然远未解决。然而，考虑到过去十年中的显著性能提升，我们对未来进展仍持乐观态度，因为我们向更现实和有影响力的应用迈进。
- en: Acknowledgments
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank the editor and the anonymous reviewers for their
    constructive comments. This work was supported by Natural Science Foundation of
    Jiangsu Province of China under Grant (BK20210340), National Natural Science Foundation
    of China under Grant (61925201, 62132001, 61772256), the Fundamental Research
    Funds for the Central Universities (No. 30920041111), CAAI-Huawei MindSpore Open
    Fund, and “111” Program B13022.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢编辑和匿名审稿人提供的建设性意见。本研究得到了江苏省自然科学基金（资助号 BK20210340）、中国国家自然科学基金（资助号 61925201,
    62132001, 61772256）、中央高校基础研究基金（编号 30920041111）、CAA-Huawei MindSpore开放基金和“111”计划B13022的资助。
- en: References
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis,
    P. Perona, and S. Belongie, “Building a bird recognition app and large scale dataset
    with citizen scientists: The fine print in fine-grained dataset collection,” in
    *CVPR*, 2015, pp. 595–604.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis,
    P. Perona, 和 S. Belongie, “与公民科学家共同构建鸟类识别应用和大规模数据集：细粒度数据集收集中的细节，” 见于*CVPR*，2015，第595–604页。'
- en: '[2] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam,
    P. Perona, and S. Belongie, “The iNaturalist species classification and detection
    dataset,” in *CVPR*, 2017, pp. 8769–8778.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam,
    P. Perona, 和 S. Belongie, “iNaturalist物种分类和检测数据集，” 见于*CVPR*，2017，第8769–8778页。'
- en: '[3] G. Van Horn, E. Cole, S. Beery, K. Wilber, S. Belongie, and O. Mac Aodha,
    “Benchmarking representation learning for natural world image collections,” in
    *CVPR*, 2021.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] G. Van Horn, E. Cole, S. Beery, K. Wilber, S. Belongie, 和 O. Mac Aodha,
    “自然世界图像集合的表示学习基准测试，” 在 *CVPR*，2021年。'
- en: '[4] L. Karlinsky, J. Shtok, Y. Tzur, and A. Tzadok, “Fine-grained recognition
    of thousands of object categories with single-example training,” in *CVPR*, 2017,
    pp. 4113–4122.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] L. Karlinsky, J. Shtok, Y. Tzur, 和 A. Tzadok, “使用单例训练进行数千个物体类别的细粒度识别，”
    在 *CVPR*，2017年，第4113–4122页。'
- en: '[5] X.-S. Wei, Q. Cui, L. Yang, P. Wang, and L. Liu, “RPC: A large-scale retail
    product checkout dataset,” *arXiv preprint arXiv:1901.07249*, 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X.-S. Wei, Q. Cui, L. Yang, P. Wang, 和 L. Liu, “RPC：一个大规模零售产品结账数据集，” *arXiv
    预印本 arXiv:1901.07249*，2019年。'
- en: '[6] M. Jia, M. Shi, M. Sirotenko, Y. Cui, C. Cardie, B. Hariharan, H. Adam,
    and S. Belongie, “Fashionpedia: Ontology, segmentation, and an attribute localization
    dataset,” in *ECCV*, 2020, pp. 316–332.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Jia, M. Shi, M. Sirotenko, Y. Cui, C. Cardie, B. Hariharan, H. Adam,
    和 S. Belongie, “Fashionpedia：本体、分割和属性定位数据集，” 在 *ECCV*，2020年，第316–332页。'
- en: '[7] S. D. Khan and H. Ullah, “A survey of advances in vision-based vehicle
    re-identification,” *CVIU*, vol. 182, pp. 50–63, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] S. D. Khan 和 H. Ullah, “基于视觉的车辆再识别进展综述，” *CVIU*，第182卷，第50–63页，2019年。'
- en: '[8] J. Yin, A. Wu, and W.-S. Zheng, “Fine-grained person re-identification,”
    *IJCV*, vol. 128, pp. 1654–1672, 2020.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Yin, A. Wu, 和 W.-S. Zheng, “细粒度人物再识别，” *IJCV*，第128卷，第1654–1672页，2020年。'
- en: '[9] ICCV 2019 Workshop on Computer Vision for Wildlife Conservation. [Online].
    Available: https://openaccess.thecvf.com/ICCV2019_workshops/ICCV2019_CVWC'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] ICCV 2019 野生动物保护计算机视觉研讨会。 [在线]。 可用链接：https://openaccess.thecvf.com/ICCV2019_workshops/ICCV2019_CVWC'
- en: '[10] Y. Wei, S. Tran, S. Xu, B. Kang, and M. Springer, “Deep learning for retail
    product recognition: Challenges and techniques,” *Computational Intelligence and
    Neuroscience*, vol. 128, pp. 1–23, 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Wei, S. Tran, S. Xu, B. Kang, 和 M. Springer, “零售产品识别的深度学习：挑战与技术，” *计算智能与神经科学*，第128卷，第1–23页，2020年。'
- en: '[11] K. E. Johnson and A. T. Eilers, “Effects of knowledge and development
    on subordinate level categorization,” *Cognitive Dev.*, pp. 515–545, 1998.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] K. E. Johnson 和 A. T. Eilers, “知识和发展对下级类别化的影响，” *认知发展*，第515–545页，1998年。'
- en: '[12] B. Yao, A. Khosla, and L. Fei-Fei, “Combining randomization and discrimination
    for fine-grained image categorization,” in *CVPR*, 2011, pp. 1577–1584.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] B. Yao, A. Khosla, 和 L. Fei-Fei, “结合随机化和区分性进行细粒度图像分类，” 在 *CVPR*，2011年，第1577–1584页。'
- en: '[13] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The Caltech-UCSD
    birds-200-2011 dataset,” *Tech. Report CNS-TR-2011-001*, 2011.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. Wah, S. Branson, P. Welinder, P. Perona, 和 S. Belongie, “加州理工-加州大学圣地亚哥分校鸟类数据集-2011，”
    *技术报告 CNS-TR-2011-001*，2011年。'
- en: '[14] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    pp. 436–444, 2015.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习，” *Nature*，第521卷，第436–444页，2015年。'
- en: '[15] IEEE TPAMI Special Issue on Fine-Grained Visual Categorization. [Online].
    Available: https://www.computer.org/digital-library/journals/tp/fine-grained-visual-categorization'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] IEEE TPAMI 细粒度视觉分类特刊。 [在线]。 可用链接：https://www.computer.org/digital-library/journals/tp/fine-grained-visual-categorization'
- en: '[16] Pattern Recognition Special Issue on Fine-Grained Object Retrieval, Matching
    and Ranking. [Online]. Available: https://www.journals.elsevier.com/pattern-recognition/call-for-papers/fine-grained-object-retrieval-matching-and-ranking'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 《模式识别》细粒度物体检索、匹配和排序的特刊。 [在线]。 可用链接：https://www.journals.elsevier.com/pattern-recognition/call-for-papers/fine-grained-object-retrieval-matching-and-ranking'
- en: '[17] ACM TOMM Special Issue on Fine-Grained Visual Recognition and Re-Identification.
    [Online]. Available: https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/CFP_FGVRreID-1592406610240.pdf'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] ACM TOMM 关于细粒度视觉识别和再识别的特刊。 [在线]。 可用链接：https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/CFP_FGVRreID-1592406610240.pdf'
- en: '[18] Pattern Recognition Letters Special Issue on Fine-Grained Categorization
    in Ecological Multimedia. [Online]. Available: https://www.sciencedirect.com/journal/pattern-recognition-letters/special-issue/10Z519ZW5DJ'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 《模式识别快报》关于生态多媒体中细粒度分类的特刊。 [在线]。 可用链接：https://www.sciencedirect.com/journal/pattern-recognition-letters/special-issue/10Z519ZW5DJ'
- en: '[19] Neurocomputing Special Issue on Fine-grained Visual Understanding and
    Reasoning. [Online]. Available: https://www.journals.elsevier.com/neurocomputing/call-for-papers/fine-grained-visual-understanding-and-reasoning'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 《神经计算》关于细粒度视觉理解和推理的特刊。 [在线]。 可用链接：https://www.journals.elsevier.com/neurocomputing/call-for-papers/fine-grained-visual-understanding-and-reasoning'
- en: '[20] The competition homepage of “iNaturalist”. [Online]. Available: https://www.kaggle.com/c/inaturalist-2019-fgvc6/overview'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] “iNaturalist” 竞赛主页。 [在线]. 可用： https://www.kaggle.com/c/inaturalist-2019-fgvc6/overview'
- en: '[21] The competition homepage of “Nature Conservancy Fisheries Monitoring”.
    [Online]. Available: https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] “自然保护协会渔业监测”竞赛主页。 [在线]. 可用： https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring'
- en: '[22] The competition homepage of “Humpback Whale Identification”. [Online].
    Available: https://www.kaggle.com/c/humpback-whale-identification'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] “座头鲸识别”竞赛主页。 [在线]. 可用： https://www.kaggle.com/c/humpback-whale-identification'
- en: '[23] CVPR 2021 Tutorial on Fine-Grained Visual Analysis with Deep Learning.
    [Online]. Available: https://fgva-cvpr21.github.io/'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] CVPR 2021 深度学习细粒度视觉分析教程。 [在线]. 可用： https://fgva-cvpr21.github.io/'
- en: '[24] The Eight Workshop on Fine-Grained Visual Categorization. [Online]. Available:
    https://sites.google.com/view/fgvc8/home'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 第八届细粒度视觉分类研讨会。 [在线]. 可用： https://sites.google.com/view/fgvc8/home'
- en: '[25] B. Zhao, J. Feng, X. Wu, and S. Yan, “A survey on deep learning-based
    fine-grained object classification and semantic segmentation,” *Int. J. Autom.
    and Comput.*, vol. 14, no. 2, pp. 119–135, 2017.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] B. Zhao, J. Feng, X. Wu, 和 S. Yan, “基于深度学习的细粒度物体分类和语义分割调查，” *Int. J. Autom.
    and Comput.*, vol. 14, no. 2, pp. 119–135, 2017.'
- en: '[26] M. Zheng, Q. Li, Y. Geng, H. Yu, J. Wang, J. Gan, and W. Xue, “A survey
    of fine-grained image categorization,” in *Proc. IEEE Conf. Signal Process.*,
    2018, pp. 533–538.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Zheng, Q. Li, Y. Geng, H. Yu, J. Wang, J. Gan, 和 W. Xue, “细粒度图像分类的调查，”
    见 *Proc. IEEE Conf. Signal Process.*, 2018, pp. 533–538.'
- en: '[27] M. Sun, Y. Yuan, F. Zhou, and E. Ding, “Multi-attention multi-class constraint
    for fine-grained image recognition,” in *ECCV*, 2018, pp. 834–850.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Sun, Y. Yuan, F. Zhou, 和 E. Ding, “用于细粒度图像识别的多重注意力多类约束，” 见 *ECCV*,
    2018, pp. 834–850.'
- en: '[28] X. Zheng, R. Ji, X. Sun, B. Zhang, Y. Wu, and F. Huang, “Towards optimal
    fine grained retrieval via decorrelated centralized loss with normalize-scale
    layer,” in *AAAI*, 2019, pp. 9291–9298.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] X. Zheng, R. Ji, X. Sun, B. Zhang, Y. Wu, 和 F. Huang, “通过去相关中心损失和归一化尺度层实现最佳细粒度检索，”
    见 *AAAI*, 2019, pp. 9291–9298.'
- en: '[29] A. Mafla, S. Dey, A. F. Biten, L. Gomez, and D. Karatzas, “Fine-grained
    image classification and retrieval by combining visual and locally pooled textual
    features,” in *WACV*, 2020, pp. 2950–2959.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. Mafla, S. Dey, A. F. Biten, L. Gomez, 和 D. Karatzas, “通过结合视觉和局部池化文本特征进行细粒度图像分类和检索，”
    见 *WACV*, 2020, pp. 2950–2959.'
- en: '[30] ——, “Multi-modal reasoning graph for scene-text based fine-grained image
    classification and retrieval,” in *WACV*, 2020, pp. 4023–4033.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] ——, “用于场景文本细粒度图像分类和检索的多模态推理图，” 见 *WACV*, 2020, pp. 4023–4033.'
- en: '[31] X.-S. Wei, C.-W. Xie, J. Wu, and C. Shen, “Mask-CNN: Localizing parts
    and selecting descriptors for fine-grained bird species categorization,” *PR*,
    vol. 76, pp. 704–714, 2018.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] X.-S. Wei, C.-W. Xie, J. Wu, 和 C. Shen, “Mask-CNN: 本地化部件和选择描述符用于细粒度鸟类分类，”
    *PR*, vol. 76, pp. 704–714, 2018.'
- en: '[32] X.-S. Wei, J.-H. Luo, J. Wu, and Z.-H. Zhou, “Selective convolutional
    descriptor aggregation for fine-grained image retrieval,” *IEEE TIP*, vol. 26,
    no. 6, pp. 2868–2881, 2017.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] X.-S. Wei, J.-H. Luo, J. Wu, 和 Z.-H. Zhou, “用于细粒度图像检索的选择性卷积描述符聚合，” *IEEE
    TIP*, vol. 26, no. 6, pp. 2868–2881, 2017.'
- en: '[33] Y. Ge, R. Zhang, L. Wu, X. Wang, X. Tang, and P. Luo, “A versatile benchmark
    for detection, pose estimation, segmentation and re-identification of clothing
    images,” in *CVPR*, 2019, pp. 5337–5345.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Ge, R. Zhang, L. Wu, X. Wang, X. Tang, 和 P. Luo, “用于衣物图像检测、姿态估计、分割和重新识别的多功能基准，”
    见 *CVPR*, 2019, pp. 5337–5345.'
- en: '[34] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, and S. C. H. Hoi, “Deep learning
    for person re-identification: A survey and outlook,” *arXiv preprint arXiv:2001.04193*,
    2020.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, 和 S. C. H. Hoi, “用于人员重新识别的深度学习：调查与展望，”
    *arXiv 预印本 arXiv:2001.04193*, 2020.'
- en: '[35] F. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet: A unified embedding
    for face recognition and clustering,” in *CVPR*, 2015, pp. 815–823.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] F. Schroff, D. Kalenichenko, 和 J. Philbin, “FaceNet：一种用于面部识别和聚类的统一嵌入，”
    见 *CVPR*, 2015, pp. 815–823.'
- en: '[36] Y. Suh, J. Wang, S. Tang, T. Mei, and K. M. Lee, “Part-aligned bilinear
    representations for person re-identification,” in *ECCV*, 2018, pp. 402–419.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Y. Suh, J. Wang, S. Tang, T. Mei, 和 K. M. Lee, “用于人员重新识别的部分对齐双线性表示，” 见
    *ECCV*, 2018, pp. 402–419.'
- en: '[37] X.-S. Wei, C.-L. Zhang, L. Liu, C. Shen, and J. Wu., “Coarse-to-fine:
    A RNN-based hierarchical attention model for vehicle re-identification,” in *ACCV*,
    2018, pp. 575–591.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] X.-S. Wei, C.-L. Zhang, L. Liu, C. Shen, 和 J. Wu, “粗到细：一种基于 RNN 的层次注意力模型用于车辆重新识别，”
    见 *ACCV*, 2018, pp. 575–591.'
- en: '[38] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang, “DeepFashion: Powering robust
    clothes recognition and retrieval with rich annotations,” in *CVPR*, 2016, pp.
    1096–1104.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Z. Liu, P. Luo, S. Qiu, X. Wang, 和 X. Tang，“DeepFashion：通过丰富的注释支持强大的服装识别和检索”，发表于
    *CVPR*，2016年，第1096–1104页。'
- en: '[39] X. Liu, J. Wang, S. Wen, E. Ding, and Y. Lin, “Localizing by describing:
    Attribute-guided attention localization for fine-grained recognition,” in *AAAI*,
    2017, pp. 4190–4196.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] X. Liu, J. Wang, S. Wen, E. Ding, 和 Y. Lin，“通过描述定位：基于属性的注意力定位用于精细化识别”，发表于
    *AAAI*，2017年，第4190–4196页。'
- en: '[40] M. Wang and W. Deng, “Deep face recognition: A survey,” *Neurocomputing*,
    vol. 429, pp. 215–244, 2021.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Wang 和 W. Deng，“深度人脸识别：综述”，*Neurocomputing*，第429卷，第215–244页，2021年。'
- en: '[41] T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, and P. N. Belhumeur,
    “Birdsnap: Large-scale fine-grained visual categorization of birds,” in *CVPR*,
    2014, pp. 2019–2026.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, 和 P. N. Belhumeur，“Birdsnap：大规模精细化鸟类视觉分类”，发表于
    *CVPR*，2014年，第2019–2026页。'
- en: '[42] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei, “Novel dataset
    for fine-grained image categorization,” in *CVPR Workshop on Fine-Grained Visual
    Categorization*, 2011, pp. 806–813.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Khosla, N. Jayadevaprakash, B. Yao, 和 L. Fei-Fei，“用于精细化图像分类的新数据集”，发表于
    *CVPR Workshop on Fine-Grained Visual Categorization*，2011年，第806–813页。'
- en: '[43] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3D object representations
    for fine-grained categorization,” in *ICCV Workshop on 3D Representation and Recognition*,
    2013.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Krause, M. Stark, J. Deng, 和 L. Fei-Fei，“用于精细化分类的3D对象表示”，发表于 *ICCV
    Workshop on 3D Representation and Recognition*，2013年。'
- en: '[44] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi, “Fine-grained
    visual classification of aircraft,” *arXiv preprint arXiv:1306.5151*, 2013.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, 和 A. Vedaldi，“飞机的精细化视觉分类”，*arXiv预印本
    arXiv:1306.5151*，2013年。'
- en: '[45] M.-E. Nilsback and A. Zisserman, “Automated flower classification over
    a large number of classes,” in *Indian Conf. on Comput. Vision, Graph. and Image
    Process.*, 2008, pp. 722–729.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M.-E. Nilsback 和 A. Zisserman，“在大量类别下的自动化花卉分类”，发表于 *Indian Conf. on Comput.
    Vision, Graph. and Image Process.*，2008年，第722–729页。'
- en: '[46] S. Hou, Y. Feng, and Z. Wang, “VegFru: A domain-specific dataset for fine-grained
    visual categorization,” in *ICCV*, 2017, pp. 541–549.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Hou, Y. Feng, 和 Z. Wang，“VegFru：用于精细化视觉分类的领域特定数据集”，发表于 *ICCV*，2017年，第541–549页。'
- en: '[47] L. Bossard, M. Guillaumin, and L. V. Gool, “Food-101 – mining discriminative
    components with random forests,” in *ECCV*, 2014, pp. 446–461.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] L. Bossard, M. Guillaumin, 和 L. V. Gool，“Food-101 – 利用随机森林挖掘辨别性组件”，发表于
    *ECCV*，2014年，第446–461页。'
- en: '[48] Y. Bai, Y. Chen, W. Yu, L. Wang, and W. Zhang, “Products-10K: A large-scale
    product recognition dataset,” *arXiv preprint arXiv:2008.10545*, 2020.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Bai, Y. Chen, W. Yu, L. Wang, 和 W. Zhang，“Products-10K：大规模产品识别数据集”，*arXiv预印本
    arXiv:2008.10545*，2020年。'
- en: '[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet large
    scale visual recognition challenge,” *IJCV*, vol. 115, pp. 211–252, 2015.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, 和 L. Fei-Fei，“ImageNet大规模视觉识别挑战”，*IJCV*，第115卷，第211–252页，2015年。'
- en: '[50] F. Zhou and Y. Lin, “Fine-grained image classification by exploring bipartite-graph
    labels,” in *CVPR*, 2016, pp. 1124–1133.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] F. Zhou 和 Y. Lin，“通过探索二分图标签进行精细化图像分类”，发表于 *CVPR*，2016年，第1124–1133页。'
- en: '[51] Y. Li, T. M. Hospedales, Y.-Z. Song, and S. Gong, “Fine-grained sketch-based
    image retrieval by matching deformable part models,” in *BMVC*, 2014, pp. 1–12.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Li, T. M. Hospedales, Y.-Z. Song, 和 S. Gong，“通过匹配可变形部件模型进行精细化草图图像检索”，发表于
    *BMVC*，2014年，第1–12页。'
- en: '[52] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, and C. C. Loy,
    “Sketch me that shoe,” in *CVPR*, 2016, pp. 799–807.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, 和 C. C. Loy，“给我那双鞋”，发表于
    *CVPR*，2016年，第799–807页。'
- en: '[53] P. Sangkloy, N. Burnell, C. Ham, and J. Hays, “The sketchy database: learning
    to retrieve badly drawn bunnies,” *ACM Trans. on Graphics*, vol. 35, no. 4, pp.
    1–12, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] P. Sangkloy, N. Burnell, C. Ham, 和 J. Hays，“草图数据库：学习检索绘制不佳的小兔子”，*ACM Trans.
    on Graphics*，第35卷，第4期，第1–12页，2016年。'
- en: '[54] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Deep spatial-semantic
    attention for fine-grained sketch-based image retrieval,” in *ICCV*, 2017, pp.
    5551–5560.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“用于精细化草图图像检索的深度空间语义注意力”，发表于
    *ICCV*，2017年，第5551–5560页。'
- en: '[55] K. Li, K. Pang, Y.-Z. Song, T. M. Hospedales, T. Xiang, and H. Zhang,
    “Synergistic instance-level subspace alignment for fine-grained sketch-based image
    retrieval,” *IEEE TIP*, vol. 26, no. 12, pp. 5908–5921, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] K. Li, K. Pang, Y.-Z. Song, T. M. Hospedales, T. Xiang, 和 H. Zhang，“协同实例级子空间对齐用于细粒度基于草图的图像检索，”*IEEE
    TIP*，第 26 卷，第 12 期，页码 5908–5921，2017。'
- en: '[56] K. Pang, K. Li, Y. Yang, H. Zhang, T. M. Hospedales, T. Xiang, and Y.-Z.
    Song, “Generalising fine-grained sketch-based image retrieval,” in *CVPR*, 2019,
    pp. 677–686.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] K. Pang, K. Li, Y. Yang, H. Zhang, T. M. Hospedales, T. Xiang, 和 Y.-Z.
    Song，“细粒度基于草图的图像检索的概括，”在 *CVPR*，2019，页码 677–686。'
- en: '[57] X. He, Y. Peng, and X. Liu, “A new benchmark and approach for fine-grained
    cross-media retrieval,” in *ACM MM*, 2019, pp. 1740–1748.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] X. He, Y. Peng, 和 X. Liu，“用于细粒度跨媒体检索的新基准和方法，”在 *ACM MM*，2019，页码 1740–1748。'
- en: '[58] S. Reed, Z. Akata, H. Lee, and B. Schiele, “Learning deep representations
    of fine-grained visual descriptions,” in *CVPR*, 2016, pp. 49–58.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] S. Reed, Z. Akata, H. Lee, 和 B. Schiele，“学习细粒度视觉描述的深度表示，”在 *CVPR*，2016，页码
    49–58。'
- en: '[59] X. He and Y. Peng, “Fine-grained image classification via combining vision
    and language,” in *CVPR*, 2017, pp. 5994–6002.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] X. He 和 Y. Peng，“通过结合视觉和语言进行细粒度图像分类，”在 *CVPR*，2017，页码 5994–6002。'
- en: '[60] O. Mac Aodha, E. Cole, and P. Perona, “Presence-only geographical priors
    for fine-grained image classification,” in *ICCV*, 2019, pp. 9596–9606.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] O. Mac Aodha, E. Cole, 和 P. Perona，“用于细粒度图像分类的仅存在地理先验，”在 *ICCV*，2019，页码
    9596–9606。'
- en: '[61] G. Chu, B. Potetz, W. Wang, A. Howard, Y. Song, F. Brucher, T. Leung,
    and H. Adam, “Geo-aware networks for fine-grained recognition,” in *ICCV Workshops*,
    2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] G. Chu, B. Potetz, W. Wang, A. Howard, Y. Song, F. Brucher, T. Leung,
    和 H. Adam，“用于细粒度识别的地理感知网络，”在 *ICCV Workshops*，2019。'
- en: '[62] X. Sun, L. Chen, and J. Yang, “Learning from web data using adversarial
    discriminative neural networks for fine-grained classification,” in *AAAI*, 2019,
    pp. 273–280.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] X. Sun, L. Chen, 和 J. Yang，“使用对抗性判别神经网络从网络数据中学习以进行细粒度分类，”在 *AAAI*，2019，页码
    273–280。'
- en: '[63] L. Niu, A. Veeraraghavan, and A. Sabharwal, “Webly supervised learning
    meets zero-shot learning: A hybrid approach for fine-grained classification,”
    in *CVPR*, 2018, pp. 7171–7180.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] L. Niu, A. Veeraraghavan, 和 A. Sabharwal，“Webly 监督学习与零样本学习相结合：用于细粒度分类的混合方法，”在
    *CVPR*，2018，页码 7171–7180。'
- en: '[64] S. Branson, G. Van Horn, S. Belongie, and P. Perona, “Bird species categorization
    using pose normalized deep convolutional nets,” in *BMVC*, 2014, pp. 1–14.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S. Branson, G. Van Horn, S. Belongie, 和 P. Perona，“使用姿态归一化深度卷积网络的鸟类物种分类，”在
    *BMVC*，2014，页码 1–14。'
- en: '[65] N. Zhang, J. Donahue, R. Girshick, and T. Darrell, “Part-based R-CNNs
    for fine-grained category detection,” in *ECCV*, 2014, pp. 834–849.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] N. Zhang, J. Donahue, R. Girshick, 和 T. Darrell，“基于部件的 R-CNN 用于细粒度类别检测，”在
    *ECCV*，2014，页码 834–849。'
- en: '[66] J. Krause, H. Jin, J. Yang, and L. Fei-Fei, “Fine-grained recognition
    without part annotations,” in *CVPR*, 2015, pp. 5546–5555.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Krause, H. Jin, J. Yang, 和 L. Fei-Fei，“无部件注释的细粒度识别，”在 *CVPR*，2015，页码
    5546–5555。'
- en: '[67] D. Lin, X. Shen, C. Lu, and J. Jia, “Deep LAC: Deep localization, alignment
    and classification for fine-grained recognition,” in *CVPR*, 2015, pp. 1666–1674.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] D. Lin, X. Shen, C. Lu, 和 J. Jia，“Deep LAC：细粒度识别的深度定位、对齐和分类，”在 *CVPR*，2015，页码
    1666–1674。'
- en: '[68] S. Huang, Z. Xu, D. Tao, and Y. Zhang, “Part-stacked CNN for fine-grained
    visual categorization,” in *CVPR*, 2016, pp. 1173–1182.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] S. Huang, Z. Xu, D. Tao, 和 Y. Zhang，“用于细粒度视觉分类的部件堆叠 CNN，”在 *CVPR*，2016，页码
    1173–1182。'
- en: '[69] H. Zhang, T. Xu, M. Elhoseiny, X. Huang, S. Zhang, A. Elgammal, and D. Metaxas,
    “SPDA-CNN: Unifying semantic part detection and abstraction for fine-grained recognition,”
    in *CVPR*, 2016, pp. 1143–1152.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] H. Zhang, T. Xu, M. Elhoseiny, X. Huang, S. Zhang, A. Elgammal, 和 D. Metaxas，“SPDA-CNN：统一语义部件检测和抽象以进行细粒度识别，”在
    *CVPR*，2016，页码 1143–1152。'
- en: '[70] Y. Zhang, X.-S. Wei, J. Wu, J. Cai, J. Lu, V.-A. Nguyen, and M. N. Do,
    “Weakly supervised fine-grained categorization with part-based image representation,”
    *IEEE TIP*, vol. 25, no. 4, pp. 1713–1725, 2016.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Y. Zhang, X.-S. Wei, J. Wu, J. Cai, J. Lu, V.-A. Nguyen, 和 M. N. Do，“基于部件的图像表示的弱监督细粒度分类，”*IEEE
    TIP*，第 25 卷，第 4 期，页码 1713–1725，2016。'
- en: '[71] M. Lam, B. Mahasseni, and S. Todorovic, “Fine-grained recognition as HSnet
    search for informative image parts,” in *CVPR*, 2017, pp. 2520–2529.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Lam, B. Mahasseni, 和 S. Todorovic，“作为 HSnet 搜索信息图像部件的细粒度识别，”在 *CVPR*，2017，页码
    2520–2529。'
- en: '[72] X. He and Y. Peng, “Weakly supervised learning of part selection model
    with spatial constraints for fine-grained image classification,” in *AAAI*, 2017,
    pp. 4075–4081.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] X. He 和 Y. Peng，“带有空间约束的部件选择模型的弱监督学习用于细粒度图像分类，”在 *AAAI*，2017，页码 4075–4081。'
- en: '[73] W. Ge, X. Lin, and Y. Yu, “Weakly supervised complementary parts models
    for fine-grained image classification from the bottom up,” in *CVPR*, 2019, pp.
    3034–3043.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] W. Ge, X. Lin, 和 Y. Yu，“从底层开始的弱监督互补部分模型用于精细图像分类”，在 *CVPR*，2019年，页码3034–3043。'
- en: '[74] Z. Wang, S. Wang, H. Li, Z. Dou, and J. Li, “Graph-propagation based correlation
    learning for weakly supervised fine-grained image classification,” in *AAAI*,
    2020, pp. 12 289–12 296.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Z. Wang, S. Wang, H. Li, Z. Dou, 和 J. Li，“基于图传播的相关学习用于弱监督精细图像分类”，在 *AAAI*，2020年，页码12 289–12 296。'
- en: '[75] C. Liu, H. Xie, Z.-J. Zha, L. Ma, L. Yu, and Y. Zhang, “Filtration and
    distillation: Enhancing region attention for fine-grained visual categorization,”
    in *AAAI*, 2020, pp. 11 555–11 562.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] C. Liu, H. Xie, Z.-J. Zha, L. Ma, L. Yu, 和 Y. Zhang，“过滤与蒸馏：增强精细视觉分类的区域注意力”，在
    *AAAI*，2020年，页码11 555–11 562。'
- en: '[76] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, and Z. Zhang, “The application
    of two-level attention models in deep convolutional neural network for fine-grained
    image classification,” in *CVPR*, 2015, pp. 842–850.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, 和 Z. Zhang，“在深度卷积神经网络中应用两级注意力模型用于精细图像分类”，在
    *CVPR*，2015年，页码842–850。'
- en: '[77] L. Liu, C. Shen, and A. van den Hengel, “The treasure beneath convolutional
    layers: Cross-convolutional-layer pooling for image classification,” in *CVPR*,
    2015, pp. 4749–4757.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] L. Liu, C. Shen, 和 A. van den Hengel，“卷积层下的宝藏：用于图像分类的跨卷积层池化”，在 *CVPR*，2015年，页码4749–4757。'
- en: '[78] M. Simon and E. Rodner, “Neural activation constellations: Unsupervised
    part model discovery with convolutional networks,” in *ICCV*, 2015, pp. 1143–1151.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Simon 和 E. Rodner，“神经激活星座：使用卷积网络进行无监督的部分模型发现”，在 *ICCV*，2015年，页码1143–1151。'
- en: '[79] X. Zhang, H. Xiong, W. Zhou, W. Lin, and Q. Tian, “Picking deep filter
    responses for fine-grained image recognition,” in *CVPR*, 2016, pp. 1134–1142.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] X. Zhang, H. Xiong, W. Zhou, W. Lin, 和 Q. Tian，“选择深度滤波响应用于精细图像识别”，在 *CVPR*，2016年，页码1134–1142。'
- en: '[80] Y. Wang, V. I. Morariu, and L. S. Davis, “Learning a discriminative filter
    bank within a CNN for fine-grained recognition,” in *CVPR*, 2018, pp. 4148–4157.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Wang, V. I. Morariu, 和 L. S. Davis，“在CNN中学习一个判别性滤波器组用于精细识别”，在 *CVPR*，2018年，页码4148–4157。'
- en: '[81] Y. Ding, Y. Zhou, Y. Zhu, Q. Ye, and J. Jiao, “Selective sparse sampling
    for fine-grained image recognition,” in *ICCV*, 2019, pp. 6599–6608.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Ding, Y. Zhou, Y. Zhu, Q. Ye, 和 J. Jiao，“用于精细图像识别的选择性稀疏采样”，在 *ICCV*，2019年，页码6599–6608。'
- en: '[82] Z. Huang and Y. Li, “Interpretable and accurate fine-grained recognition
    via region grouping,” in *CVPR*, 2020, pp. 8662–8672.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Z. Huang 和 Y. Li，“通过区域分组进行可解释且准确的精细识别”，在 *CVPR*，2020年，页码8662–8672。'
- en: '[83] J. Fu, H. Zheng, and T. Mei, “Look closer to see better: Recurrent attention
    convolutional neural network for fine-grained image recognition,” in *CVPR*, 2017,
    pp. 4438–4446.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. Fu, H. Zheng, 和 T. Mei，“更近距离观察以获得更好的效果：用于精细图像识别的递归注意力卷积神经网络”，在 *CVPR*，2017年，页码4438–4446。'
- en: '[84] H. Zheng, J. Fu, T. Mei, and J. Luo, “Learning multi-attention convolutional
    neural network for fine-grained image recognition,” in *ICCV*, 2017, pp. 5209–5217.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] H. Zheng, J. Fu, T. Mei, 和 J. Luo，“学习多重注意力卷积神经网络用于精细图像识别”，在 *ICCV*，2017年，页码5209–5217。'
- en: '[85] Y. Peng, X. He, and J. Zhao, “Object-part attention model for fine-grained
    image classification,” *IEEE TIP*, vol. 27, no. 3, pp. 1487–1500, 2018.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Peng, X. He, 和 J. Zhao，“用于精细图像分类的对象部分注意力模型”，*IEEE TIP*，第27卷，第3期，页码1487–1500，2018年。'
- en: '[86] L. Zhang, S. Huang, W. Liu, and D. Tao, “Learning a mixture of granularity-specific
    experts for fine-grained categorization,” in *ICCV*, 2019, pp. 8331–8340.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] L. Zhang, S. Huang, W. Liu, 和 D. Tao，“学习针对特定粒度的专家混合用于精细分类”，在 *ICCV*，2019年，页码8331–8340。'
- en: '[87] H. Zheng, J. Fu, Z.-J. Zha, and J. Luo, “Looking for the devil in the
    details: Learning trilinear attention sampling network for fine-grained image
    recognition,” in *CVPR*, 2019, pp. 5012–5021.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. Zheng, J. Fu, Z.-J. Zha, 和 J. Luo，“在细节中寻找魔鬼：学习三线性注意力采样网络用于精细图像识别”，在
    *CVPR*，2019年，页码5012–5021。'
- en: '[88] H. Zheng, J. Fu, Z.-J. Zha, J. Luo, and T. Mei, “Learning rich part hierarchies
    with progressive attention networks for fine-grained image recognition,” *IEEE
    TIP*, vol. 29, pp. 476–488, 2020.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] H. Zheng, J. Fu, Z.-J. Zha, J. Luo, 和 T. Mei，“通过渐进注意力网络学习丰富的部分层次结构用于精细图像识别”，*IEEE
    TIP*，第29卷，页码476–488，2020年。'
- en: '[89] R. Ji, L. Wen, L. Zhang, D. Du, Y. Wu, C. Zhao, X. Liu, and F. Huang,
    “Attention convolutional binary neural tree for fine-grained visual categorization,”
    in *CVPR*, 2020, pp. 10 468–10 477.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] R. Ji, L. Wen, L. Zhang, D. Du, Y. Wu, C. Zhao, X. Liu, 和 F. Huang，“用于精细视觉分类的注意力卷积二元神经树”，在
    *CVPR*，2020年，页码10 468–10 477。'
- en: '[90] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu, “Spatial
    transformer networks,” in *NIPS*, 2015, pp. 2017–2025.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Jaderberg, K. Simonyan, A. Zisserman, 和 K. Kavukcuoglu, “空间变换网络”，发表于
    *NIPS*，2015年，第2017–2025页。'
- en: '[91] Y. Wang, J. Choi, V. I. Morariu, and L. S. Davis, “Mining discriminative
    triplets of patches for fine-grained classification,” in *CVPR*, 2016, pp. 1163–1172.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. Wang, J. Choi, V. I. Morariu, 和 L. S. Davis, “挖掘细粒度分类的判别三元组”，发表于 *CVPR*，2016年，第1163–1172页。'
- en: '[92] Z. Yang, T. Luo, D. Wang, Z. Hu, J. Gao, and L. Wang, “Learning to navigate
    for fine-grained classification,” in *ECCV*, 2018, pp. 438–454.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Z. Yang, T. Luo, D. Wang, Z. Hu, J. Gao, 和 L. Wang, “学习导航以进行细粒度分类”，发表于
    *ECCV*，2018年，第438–454页。'
- en: '[93] X. He, Y. Peng, and J. Zhao, “Which and how many regions to gaze: Focus
    discriminative regions for fine-grained visual categorization,” *IJCV*, vol. 127,
    pp. 1235–1255, 2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] X. He, Y. Peng, 和 J. Zhao, “哪个以及多少个区域需要关注：聚焦于细粒度视觉分类的判别区域”，*IJCV*，第127卷，第1235–1255页，2019年。'
- en: '[94] Z. Wang, S. Wang, S. Yang, H. Li, J. Li, and Z. Li, “Weakly supervised
    fine-grained image classification via guassian mixture model oriented discriminative
    learning,” in *CVPR*, 2020, pp. 9749–9758.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Z. Wang, S. Wang, S. Yang, H. Li, J. Li, 和 Z. Li, “通过高斯混合模型导向的判别学习进行弱监督细粒度图像分类”，发表于
    *CVPR*，2020年，第9749–9758页。'
- en: '[95] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear CNN models for fine-grained
    visual recognition,” in *ICCV*, 2015, pp. 1449–1457.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] T.-Y. Lin, A. RoyChowdhury, 和 S. Maji, “用于细粒度视觉识别的双线性 CNN 模型”，发表于 *ICCV*，2015年，第1449–1457页。'
- en: '[96] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell, “Compact bilinear pooling,”
    in *CVPR*, 2016, pp. 317–326.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Y. Gao, O. Beijbom, N. Zhang, 和 T. Darrell, “紧凑的双线性池化”，发表于 *CVPR*，2016年，第317–326页。'
- en: '[97] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, and S. Belongie, “Kernel pooling
    for convolutional neural networks,” in *CVPR*, 2017, pp. 2921–2930.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, 和 S. Belongie, “用于卷积神经网络的核池化”，发表于
    *CVPR*，2017年，第2921–2930页。'
- en: '[98] S. Kong and C. Fowlkes, “Low-rank bilinear pooling for fine-grained classification,”
    in *CVPR*, 2017, pp. 365–374.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] S. Kong 和 C. Fowlkes, “用于细粒度分类的低秩双线性池化”，发表于 *CVPR*，2017年，第365–374页。'
- en: '[99] Q. Wang, P. Li, and L. Zhang, “G²DeNet: Global Gaussian distribution embedding
    network and its application to visual recognition,” in *CVPR*, 2017, pp. 2730–2739.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Q. Wang, P. Li, 和 L. Zhang, “G²DeNet：全局高斯分布嵌入网络及其在视觉识别中的应用”，发表于 *CVPR*，2017年，第2730–2739页。'
- en: '[100] S. Cai, W. Zuo, and L. Zhang, “Higher-order integration of hierarchical
    convolutional activations for fine-grained visual categorization,” in *ICCV*,
    2017, pp. 511–520.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Cai, W. Zuo, 和 L. Zhang, “用于细粒度视觉分类的层次卷积激活的高阶融合”，发表于 *ICCV*，2017年，第511–520页。'
- en: '[101] P. Li, J. Xie, Q. Wang, and Z. Gao, “Towards faster training of global
    covariance pooling networks by iterative matrix square root normalization,” in
    *CVPR*, 2018, pp. 947–955.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] P. Li, J. Xie, Q. Wang, 和 Z. Gao, “通过迭代矩阵平方根归一化加速全局协方差池化网络的训练”，发表于 *CVPR*，2018年，第947–955页。'
- en: '[102] M. Engin, L. Wang, L. Zhou, and X. Liu, “DeepKSPD: Learning kernel-matrix-based
    SPD representation for fine-grained image recognition,” in *ECCV*, 2018, pp. 629–645.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. Engin, L. Wang, L. Zhou, 和 X. Liu, “DeepKSPD：学习基于核矩阵的 SPD 表示用于细粒度图像识别”，发表于
    *ECCV*，2018年，第629–645页。'
- en: '[103] C. Yu, X. Zhao, Q. Zheng, P. Zhang, and X. You, “Hierarchical bilinear
    pooling for fine-grained visual recognition,” in *ECCV*, 2018, pp. 595–610.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] C. Yu, X. Zhao, Q. Zheng, P. Zhang, 和 X. You, “用于细粒度视觉识别的层次双线性池化”，发表于
    *ECCV*，2018年，第595–610页。'
- en: '[104] X. Wei, Y. Zhang, Y. Gong, J. Zhang, and N. Zheng, “Grassmann pooling
    as compact homogeneous bilinear pooling for fine-grained visual classification,”
    in *ECCV*, 2018, pp. 365–380.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] X. Wei, Y. Zhang, Y. Gong, J. Zhang, 和 N. Zheng, “Grassmann 池化作为紧凑的同质双线性池化用于细粒度视觉分类”，发表于
    *ECCV*，2018年，第365–380页。'
- en: '[105] H. Zheng, J. Fu, Z.-J. Zha, and J. Luo, “Learning deep bilinear transformation
    for fine-grained image representation,” in *NIPS*, 2019, pp. 4277–4286.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] H. Zheng, J. Fu, Z.-J. Zha, 和 J. Luo, “学习深度双线性变换以用于细粒度图像表示”，发表于 *NIPS*，2019年，第4277–4286页。'
- en: '[106] S. Min, H. Yao, H. Xie, Z.-J. Zha, and Y. Zhang, “Multi-objective matrix
    normalization for fine-grained visual recognition,” *IEEE TIP*, vol. 29, pp. 4996–5009,
    2020.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] S. Min, H. Yao, H. Xie, Z.-J. Zha, 和 Y. Zhang, “用于细粒度视觉识别的多目标矩阵归一化”，*IEEE
    TIP*，第29卷，第4996–5009页，2020年。'
- en: '[107] A. Dubey, O. Gupta, R. Raskar, and N. Naik, “Maximum entropy fine-grained
    classification,” in *NIPS*, 2018, pp. 637–647.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Dubey, O. Gupta, R. Raskar, 和 N. Naik, “最大熵细粒度分类”，发表于 *NIPS*，2018年，第637–647页。'
- en: '[108] A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell, and N. Naik, “Pairwise
    confusion for fine-grained visual classification,” in *ECCV*, 2018, pp. 71–88.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell, 和 N. Naik, “对细粒度视觉分类的成对混淆”，发表于
    *ECCV*，2018年，第71–88页。'
- en: '[109] Y. Gao, X. Han, X. Wang, W. Huang, and M. R. Scott, “Channel interaction
    networks for fine-grained image categorization,” in *AAAI*, 2020, pp. 10 818–10 825.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Y. Gao, X. Han, X. Wang, W. Huang, 和 M. R. Scott，“用于细粒度图像分类的通道交互网络，”发表于
    *AAAI*，2020年，第10 818–10 825页。'
- en: '[110] G. Sun, H. Cholakkal, S. Khan, F. S. Khan, and L. Shao, “Fine-grained
    recognition: Accounting for subtle differences between similar classes,” in *AAAI*,
    2020, pp. 12 047–12 054.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] G. Sun, H. Cholakkal, S. Khan, F. S. Khan, 和 L. Shao，“细粒度识别：考虑相似类别之间的细微差异，”发表于
    *AAAI*，2020年，第12 047–12 054页。'
- en: '[111] P. Zhuang, Y. Wang, and Y. Qiao, “Learning attentive pairwise interaction
    for fine-grained classification,” in *AAAI*, 2020, pp. 2457–2463.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] P. Zhuang, Y. Wang, 和 Y. Qiao，“学习注意的成对交互以进行细粒度分类，”发表于 *AAAI*，2020年，第2457–2463页。'
- en: '[112] D. Chang, Y. Ding, J. Xie, A. K. Bhunia, X. Li, Z. Ma, M. Wu, J. Guo,
    and Y.-Z. Song, “The devil is in the channels: Mutual-channel loss for fine-grained
    image classification,” *IEEE TIP*, vol. 29, pp. 4683–4695, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] D. Chang, Y. Ding, J. Xie, A. K. Bhunia, X. Li, Z. Ma, M. Wu, J. Guo,
    和 Y.-Z. Song，“魔鬼藏在通道中：用于细粒度图像分类的互通道损失，”*IEEE TIP*，第29卷，第4683–4695页，2020年。'
- en: '[113] Y. Chen, Y. Bai, W. Zhang, and T. Mei, “Destruction and construction
    learning for fine-grained image recognition,” in *CVPR*, 2019, pp. 5157–5166.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Y. Chen, Y. Bai, W. Zhang, 和 T. Mei，“细粒度图像识别的破坏与重建学习，”发表于 *CVPR*，2019年，第5157–5166页。'
- en: '[114] W. Luo, X. Yang, X. Mo, Y. Lu, L. S. Davis, J. Li, J. Yang, and S.-N.
    Lim, “Cross-X learning for fine-grained visual categorization,” in *ICCV*, 2019,
    pp. 8242–8251.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] W. Luo, X. Yang, X. Mo, Y. Lu, L. S. Davis, J. Li, J. Yang, 和 S.-N. Lim，“细粒度视觉分类的跨领域学习，”发表于
    *ICCV*，2019年，第8242–8251页。'
- en: '[115] R. Du, D. Chang, A. K. Bhunia, J. Xie, Y.-Z. Song, Z. Ma, and J. Guo,
    “Fine-grained visual classification via progressive multi-granularity training
    of Jigsaw patches,” in *ECCV*, 2020, pp. 153–168.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] R. Du, D. Chang, A. K. Bhunia, J. Xie, Y.-Z. Song, Z. Ma, 和 J. Guo，“通过逐步多粒度训练的拼图块进行细粒度视觉分类，”发表于
    *ECCV*，2020年，第153–168页。'
- en: '[116] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
    object detection with region proposal networks,” in *NIPS*, 2015, pp. 91–99.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] S. Ren, K. He, R. Girshick, 和 J. Sun，“Faster R-CNN：基于区域提议网络的实时目标检测，”发表于
    *NIPS*，2015年，第91–99页。'
- en: '[117] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” in *CVPR*, 2015, pp. 3431–3440.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] J. Long, E. Shelhamer, 和 T. Darrell，“用于语义分割的全卷积网络，”发表于 *CVPR*，2015年，第3431–3440页。'
- en: '[118] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *CVPR*, 2014, pp.
    580–587.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] R. Girshick, J. Donahue, T. Darrell, 和 J. Malik，“用于准确目标检测和语义分割的丰富特征层次结构，”发表于
    *CVPR*，2014年，第580–587页。'
- en: '[119] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial
    pyramid matching for recognizing natural scene categories,” in *CVPR*, 2006, pp.
    1–8.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] S. Lazebnik, C. Schmid, 和 J. Ponce，“超越特征袋：用于识别自然场景类别的空间金字塔匹配，”发表于 *CVPR*，2006年，第1–8页。'
- en: '[120] M. Guillaumin, D. Küttel, and V. Ferrari, “ImageNet auto-annotation with
    segmentation propagation,” *IJCV*, vol. 110, pp. 328–348, 2014.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] M. Guillaumin, D. Küttel, 和 V. Ferrari，“ImageNet自动标注与分割传播，”*IJCV*，第110卷，第328–348页，2014年。'
- en: '[121] X.-S. Wei, C.-L. Zhang, J. Wu, C. Shen, and Z.-H. Zhou, “Unsupervised
    object discovery and co-localization by deep descriptor transformation,” *PR*,
    vol. 88, pp. 113–126, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] X.-S. Wei, C.-L. Zhang, J. Wu, C. Shen, 和 Z.-H. Zhou，“通过深度描述符变换进行无监督目标发现和共同定位，”*PR*，第88卷，第113–126页，2019年。'
- en: '[122] M. Zeiler and R. Fergus, “Visualizing and understanding convolutional
    networks,” in *ECCV*, 2014, pp. 818–833.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] M. Zeiler 和 R. Fergus，“可视化和理解卷积网络，”发表于 *ECCV*，2014年，第818–833页。'
- en: '[123] C. M. Bishop, *Pattern recognition and machine learning*.   Springer,
    2006, vol. 1.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] C. M. Bishop, *模式识别与机器学习*。   Springer，2006年，第1卷。'
- en: '[124] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *CVPR*, 2016, pp. 2921–2929.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, 和 A. Torralba，“用于区分定位的深度特征学习，”发表于
    *CVPR*，2016年，第2921–2929页。'
- en: '[125] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention
    for rapid scene analysis,” *IEEE TPAMI*, vol. 20, no. 11, pp. 1254–1259, 1998.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] L. Itti, C. Koch, 和 E. Niebur，“基于显著性的视觉注意力模型用于快速场景分析，”*IEEE TPAMI*，第20卷，第11期，第1254–1259页，1998年。'
- en: '[126] M. Corbetta and G. L. Shulman, “Control of goal-directed and stimulus-driven
    attention in the brain,” *Nature Reviews Neuroscience*, vol. 3, pp. 201–215, 2002.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] M. Corbetta 和 G. L. Shulman，“大脑中目标导向和刺激驱动注意力的控制，”*Nature Reviews Neuroscience*，第3卷，第201–215页，2002年。'
- en: '[127] H. Larochelle and G. E. Hinton, “Learning to combine foveal glimpses
    with a third-order boltzmann machine,” in *NIPS*, 2010, pp. 1243–1251.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] H. Larochelle 和 G. E. Hinton, “学习将中心视点与三阶玻尔兹曼机相结合，” 见 *NIPS*，2010 年，页
    1243–1251。'
- en: '[128] X. He, Y. Peng, and J. Zhao, “Fast fine-grained image classification
    via weakly supervised discriminative localization,” *IEEE TCSVT*, vol. 29, no. 5,
    pp. 1394–1407, 2019.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] X. He, Y. Peng 和 J. Zhao, “通过弱监督判别定位进行快速细粒度图像分类，” *IEEE TCSVT*，第 29 卷，第
    5 期，页 1394–1407，2019 年。'
- en: '[129] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *CVPR*,
    2018, pp. 7132–7141.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Hu, L. Shen 和 G. Sun, “挤压与激励网络，” 见 *CVPR*，2018 年，页 7132–7141。'
- en: '[130] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah, “Signature
    verification using a “siamese” time delay neural network,” in *NIPS*, 1993, pp.
    737–744.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger 和 R. Shah, “使用“孪生”时延神经网络进行签名验证，”
    见 *NIPS*，1993 年，页 737–744。'
- en: '[131] X. He, Y. Peng, and J. Zhao, “StackDRL: Stacked deep reinforcement learning
    for fine-grained visual categorization,” in *IJCAI*, 2018, pp. 741–747.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] X. He, Y. Peng 和 J. Zhao, “StackDRL：用于细粒度视觉分类的堆叠深度强化学习，” 见 *IJCAI*，2018
    年，页 741–747。'
- en: '[132] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning:
    A survey,” *JAIR*, vol. 4, pp. 237–285, 1996.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] L. P. Kaelbling, M. L. Littman 和 A. W. Moore, “强化学习：综述，” *JAIR*，第 4 卷，页
    237–285，1996 年。'
- en: '[133] J. Mu, S. Bhat, and P. Viswanath, “Representing sentences as low-rank
    subspaces,” in *ACL*, 2017, pp. 629–634.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Mu, S. Bhat 和 P. Viswanath, “将句子表示为低秩子空间，” 见 *ACL*，2017 年，页 629–634。'
- en: '[134] M. D. Zeiler, G. W. Taylor, and R. Fergus, “Adaptive deconvolutional
    networks for mid and high level feature learning,” in *ICCV*, 2011, pp. 2018–2025.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] M. D. Zeiler, G. W. Taylor 和 R. Fergus, “用于中高层特征学习的自适应反卷积网络，” 见 *ICCV*，2011
    年，页 2018–2025。'
- en: '[135] Z. Xu, Y. Yang, and A. G. Hauptmann, “A discriminative CNN video representation
    for event detection,” in *CVPR*, 2015, pp. 1798–1807.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Z. Xu, Y. Yang 和 A. G. Hauptmann, “用于事件检测的判别性 CNN 视频表示，” 见 *CVPR*，2015
    年，页 1798–1807。'
- en: '[136] M. Cimpoi, S. Maji, and A. Vedaldi, “Deep filter banks for texture recognition
    and segmentation,” in *CVPR*, 2015, pp. 3828–3836.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] M. Cimpoi, S. Maji 和 A. Vedaldi, “用于纹理识别与分割的深度滤波器组，” 见 *CVPR*，2015 年，页
    3828–3836。'
- en: '[137] B.-B. Gao, X.-S. Wei, J. Wu, and W. Lin, “Deep spatial pyramid: The devil
    is once again in the details,” *arXiv preprint arXiv:1504.05277*, 2015.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] B.-B. Gao, X.-S. Wei, J. Wu 和 W. Lin, “深度空间金字塔：魔鬼再一次藏在细节中，” *arXiv preprint
    arXiv:1504.05277*，2015 年。'
- en: '[138] P. H. Gosselin, N. Murray, H. Jégou, and F. Perronnin, “Revisiting the
    fisher vector for fine-grained classification,” in *Pattern Recogn. Letters*,
    no. 49, 2015, pp. 92–98.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] P. H. Gosselin, N. Murray, H. Jégou 和 F. Perronnin, “重新审视 Fisher 向量在细粒度分类中的应用，”
    见 *Pattern Recogn. Letters*，第 49 期，2015 年，页 92–98。'
- en: '[139] L. Wang, J. Zhang, L. Zhou, C. Tang, and W. Li, “Beyond covariance: Feature
    representation with nonlinear kernel matrices,” in *ICCV*, 2015, pp. 4570–4578.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] L. Wang, J. Zhang, L. Zhou, C. Tang 和 W. Li, “超越协方差：具有非线性核矩阵的特征表示，” 见
    *ICCV*，2015 年，页 4570–4578。'
- en: '[140] Q. Wang, J. Xie, W. Zuo, L. Zhang, and P. Li, “Deep CNNs meet global
    covariance pooling: Better representation and generalization,” *IEEE TPAMI*, 2020.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Q. Wang, J. Xie, W. Zuo, L. Zhang 和 P. Li, “深度 CNN 遇上全局协方差池化：更好的表示与泛化，”
    *IEEE TPAMI*，2020 年。'
- en: '[141] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear convolutional neural
    networks for fine-grained visual recognition,” *IEEE TPAMI*, vol. 40, no. 6, pp.
    1309–1322, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] T.-Y. Lin, A. RoyChowdhury 和 S. Maji, “用于细粒度视觉识别的双线性卷积神经网络，” *IEEE TPAMI*，第
    40 卷，第 6 期，页 1309–1322，2018 年。'
- en: '[142] N. Pham and R. Pagh, “Fast and scalable polynomial kernels via explicit
    feature maps,” in *KDD*, 2013, pp. 239–247.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] N. Pham 和 R. Pagh, “通过显式特征映射实现快速且可扩展的多项式核，” 见 *KDD*，2013 年，页 239–247。'
- en: '[143] Y. Li, N. Wang, J. Liu, and X. Hou, “Factorized bilinear models for image
    recognition,” in *ICCV*, 2017, pp. 2079–2087.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Y. Li, N. Wang, J. Liu 和 X. Hou, “图像识别的因式分解双线性模型，” 见 *ICCV*，2017 年，页
    2079–2087。'
- en: '[144] J. Sánchez, F. Perronnin, T. Mensink, and J. Verbeek, “Image classification
    with the Fisher vector: Theory and practice,” *IJCV*, vol. 105, no. 3, pp. 222–245,
    2013.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. Sánchez, F. Perronnin, T. Mensink 和 J. Verbeek, “使用 Fisher 向量进行图像分类：理论与实践，”
    *IJCV*，第 105 卷，第 3 期，页 222–245，2013 年。'
- en: '[145] P. Li, J. Xie, Q. Wang, and W. Zuo, “Is second-order information helpful
    for large-scale visual recognition?” in *ICCV*, 2017, pp. 2070–2078.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] P. Li, J. Xie, Q. Wang 和 W. Zuo, “二阶信息对大规模视觉识别是否有帮助？” 见 *ICCV*，2017 年，页
    2070–2078。'
- en: '[146] T.-Y. Lin and S. Maji, “Improved bilinear pooling with CNNs,” in *BMVC*,
    2017, pp. 1–12.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] T.-Y. Lin 和 S. Maji, “改进的双线性池化与 CNNs，” 见 *BMVC*，2017 年，页 1–12。'
- en: '[147] W. Xiong, Y. He, Y. Zhang, W. Luo, L. Ma, and J. Luo, “Fine-grained image-to-image
    transformation towards visual recognition,” in *CVPR*, 2020, pp. 5840–5849.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] W. Xiong, Y. He, Y. Zhang, W. Luo, L. Ma, 和 J. Luo, “面向视觉识别的细粒度图像到图像转换”，发表于
    *CVPR*，2020，第5840–5849页。'
- en: '[148] X.-S. Wei, P. Wang, L. Liu, C. Shen, and J. Wu, “Piecewise classifier
    mappings: Learning fine-grained learners for novel categories with few examples,”
    *IEEE TIP*, vol. 28, no. 12, pp. 6116–6125, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] X.-S. Wei, P. Wang, L. Liu, C. Shen, 和 J. Wu, “分段分类器映射：为新类别学习细粒度学习者以应对少量示例”，*IEEE
    TIP*，第28卷，第12期，第6116–6125页，2019。'
- en: '[149] S. Xie, T. Yang, X. Wang, and Y. Lin, “Hyper-class augmented and regularized
    deep learning for fine-grained image classification,” in *CVPR*, 2015, pp. 2645–2654.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] S. Xie, T. Yang, X. Wang, 和 Y. Lin, “超类增强和正则化的深度学习用于细粒度图像分类”，发表于 *CVPR*，2015，第2645–2654页。'
- en: '[150] Z. Xu, S. Huang, Y. Zhang, and D. Tao, “Augmenting strong supervision
    using web data for fine-grained categorization,” in *CVPR*, 2015, pp. 2524–2532.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Z. Xu, S. Huang, Y. Zhang, 和 D. Tao, “利用网络数据增强强监督以进行细粒度分类”，发表于 *CVPR*，2015，第2524–2532页。'
- en: '[151] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev, T. Duerig, J. Philbin,
    and L. Fei-Fei, “The unreasonable effectiveness of noisy data for fine-grained
    recognition,” in *ECCV*, 2016, pp. 301–320.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev, T. Duerig, J. Philbin,
    和 L. Fei-Fei, “嘈杂数据在细粒度识别中的不合理有效性”，发表于 *ECCV*，2016，第301–320页。'
- en: '[152] L. Niu, A. Veeraraghavan, and A. Sabharwal, “Webly supervised learning
    meets zero-shot learning: A hybrid approach for fine-grained classification,”
    in *CVPR*, 2018, pp. 7171–7180.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] L. Niu, A. Veeraraghavan, 和 A. Sabharwal, “网络监督学习与零样本学习相结合：一种用于细粒度分类的混合方法”，发表于
    *CVPR*，2018，第7171–7180页。'
- en: '[153] Y. Zhang, H. Tang, and K. Jia, “Fine-grained visual categorization using
    meta-learning optimization with sample selection of auxiliary data,” in *ECCV*,
    2018, pp. 241–256.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Y. Zhang, H. Tang, 和 K. Jia, “使用元学习优化与辅助数据样本选择的细粒度视觉分类”，发表于 *ECCV*，2018，第241–256页。'
- en: '[154] Z. Xu, S. Huang, Y. Zhang, and D. Tao, “Webly-supervised fine-grained
    visual categorization via deep domain adaptation,” *IEEE TPAMI*, vol. 40, no. 4,
    pp. 769–790, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Z. Xu, S. Huang, Y. Zhang, 和 D. Tao, “通过深度领域适应进行网络监督的细粒度视觉分类”，*IEEE TPAMI*，第40卷，第4期，第769–790页，2018。'
- en: '[155] J. Yang, X. Sun, Y.-K. Lai, L. Zheng, and M.-M. Cheng, “Recognition from
    web data: A progressive filtering approach,” *IEEE TIP*, vol. 27, no. 11, pp.
    5303–5315, 2018.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Yang, X. Sun, Y.-K. Lai, L. Zheng, 和 M.-M. Cheng, “来自网络数据的识别：一种渐进过滤方法”，*IEEE
    TIP*，第27卷，第11期，第5303–5315页，2018。'
- en: '[156] C. Zhang, Y. Yao, H. Liu, G.-S. Xie, X. Shu, T. Zhou, Z. Zhang, F. Shen,
    and Z. Tang, “Web-supervised network with softly update-drop training for fine-grained
    visual classification,” in *AAAI*, 2020, pp. 12 781–12 788.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] C. Zhang, Y. Yao, H. Liu, G.-S. Xie, X. Shu, T. Zhou, Z. Zhang, F. Shen,
    和 Z. Tang, “具有柔性更新-丢弃训练的网络监督用于细粒度视觉分类”，发表于 *AAAI*，2020，第12 781–12 788页。'
- en: '[157] H. Zhang, X. Cao, and R. Wang, “Audio visual attribute discovery for
    fine-grained object recognition,” in *AAAI*, 2018, pp. 7542–7549.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] H. Zhang, X. Cao, 和 R. Wang, “用于细粒度对象识别的音频视觉属性发现”，发表于 *AAAI*，2018，第7542–7549页。'
- en: '[158] H. Xu, G. Qi, J. Li, M. Wang, K. Xu, and H. Gao, “Fine-grained image
    classification by visual-semantic embedding,” in *IJCAI*, 2018, pp. 1043–1049.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] H. Xu, G. Qi, J. Li, M. Wang, K. Xu, 和 H. Gao, “通过视觉-语义嵌入进行细粒度图像分类”，发表于
    *IJCAI*，2018，第1043–1049页。'
- en: '[159] T. Chen, L. Lin, R. Chen, Y. Wu, and X. Luo, “Knowledge-embedded representation
    learning for fine-grained image recognition,” in *IJCAI*, 2018, pp. 627–634.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] T. Chen, L. Lin, R. Chen, Y. Wu, 和 X. Luo, “用于细粒度图像识别的知识嵌入表示学习”，发表于 *IJCAI*，2018，第627–634页。'
- en: '[160] K. Song, X.-S. Wei, X. Shu, R.-J. Song, and J. Lu, “Bi-modal progressive
    mask attention for fine-grained recognition,” *IEEE TIP*, vol. 29, pp. 7006–7018,
    2020.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] K. Song, X.-S. Wei, X. Shu, R.-J. Song, 和 J. Lu, “用于细粒度识别的双模态渐进掩码注意力”，*IEEE
    TIP*，第29卷，第7006–7018页，2020。'
- en: '[161] B. Zhuang, L. Liu, Y. Li, C. Shen, and I. Reid, “Attend in groups: a
    weakly-supervised deep learning framework for learning from web data,” in *CVPR*,
    2017, pp. 1878–1887.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] B. Zhuang, L. Liu, Y. Li, C. Shen, 和 I. Reid, “组内注意：一种从网络数据学习的弱监督深度学习框架”，发表于
    *CVPR*，2017，第1878–1887页。'
- en: '[162] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NIPS*, 2014, pp.
    2672–2680.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio, “生成对抗网络”，发表于 *NIPS*，2014，第2672–2680页。'
- en: '[163] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning
    in neural networks: A survey,” *arXiv preprint arXiv:2004.05439*, 2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] T. Hospedales, A. Antoniou, P. Micaelli, 和 A. Storkey, “神经网络中的元学习：综述”，*arXiv
    preprint arXiv:2004.05439*，2020。'
- en: '[164] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes,
    S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer, “DBpedia - A large-scale,
    multilingual knowledge base extracted from Wikipedia,” *Semantic Web Journal*,
    pp. 167–195, 2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes,
    S. Hellmann, M. Morsey, P. van Kleef, S. Auer, 和 C. Bizer，“DBpedia - 从维基百科提取的大规模多语言知识库”，*Semantic
    Web Journal*，第167–195页，2015年。'
- en: '[165] F. M. Zanzotto, “Viewpoint: Human-in-the-loop artificial intelligence,”
    *JAIR*, vol. 64, pp. 243–252, 2019.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] F. M. Zanzotto，“观点：人机协作的人工智能”，*JAIR*，第64卷，第243–252页，2019年。'
- en: '[166] Y. Cui, F. Zhou, Y. Lin, and S. Belongie, “Fine-grained categorization
    and dataset bootstrapping using deep metric learning with humans in the loop,”
    in *CVPR*, 2016, pp. 1153–1162.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Y. Cui, F. Zhou, Y. Lin, 和 S. Belongie，“使用人机协作的深度度量学习进行细粒度分类和数据集自举”，发表于
    *CVPR*，2016年，第1153–1162页。'
- en: '[167] J. Deng, J. Krause, M. Stark, and L. Fei-Fei, “Leveraging the wisdom
    of the crowd for fine-grained recognition,” *IEEE TPAMI*, vol. 38, no. 4, pp.
    666–676, 2016.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Deng, J. Krause, M. Stark, 和 L. Fei-Fei，“利用大众智慧进行细粒度识别”，*IEEE TPAMI*，第38卷，第4期，第666–676页，2016年。'
- en: '[168] Y. Cui, Y. Song, C. Sun, A. Howard, and S. Belongie, “Large scale fine-grained
    categorization and domain-specific transfer learning,” in *CVPR*, 2018, pp. 4109–4118.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Cui, Y. Song, C. Sun, A. Howard, 和 S. Belongie，“大规模细粒度分类和领域特定迁移学习”，发表于
    *CVPR*，2018年，第4109–4118页。'
- en: '[169] Q. Wang, L. Zhang, B. Wu, D. Ren, P. Li, W. Zuo, and Q. Hu, “What deep
    CNNs benefit from global covariance pooling: An optimization perspective,” in
    *CVPR*, 2020, pp. 10 771–10 780.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Q. Wang, L. Zhang, B. Wu, D. Ren, P. Li, W. Zuo, 和 Q. Hu，“深度卷积神经网络如何从全局协方差池化中受益：一种优化视角”，发表于
    *CVPR*，2020年，第10 771–10 780页。'
- en: '[170] X. Zheng, R. Ji, X. Sun, Y. Wu, F. Huang, and Y. Yang, “Centralized ranking
    loss with weakly supervised localization for fine-grained object retrieval,” in
    *IJCAI*, 2018, pp. 1226–1233.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] X. Zheng, R. Ji, X. Sun, Y. Wu, F. Huang, 和 Y. Yang，“带弱监督定位的集中式排名损失用于细粒度目标检索”，发表于
    *IJCAI*，2018年，第1226–1233页。'
- en: '[171] X. Zeng, Y. Zhang, X. Wang, K. Chen, D. Li, and W. Yang, “Fine-grained
    image retrieval via piecewise cross entropy loss,” *Image and Vision Computing*,
    vol. 93, pp. 2868–2881, 2020.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] X. Zeng, Y. Zhang, X. Wang, K. Chen, D. Li, 和 W. Yang，“通过逐片交叉熵损失进行细粒度图像检索”，*Image
    and Vision Computing*，第93卷，第2868–2881页，2020年。'
- en: '[172] CVPR 2020 Workshop on RetailVision. [Online]. Available: https://retailvisionworkshop.github.io/'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] CVPR 2020零售视觉研讨会。[在线]. 可用链接：https://retailvisionworkshop.github.io/'
- en: '[173] H. Idrees, M. Shah, and R. Surette, “Enhancing camera surveillance using
    computer vision: a research note,” *arXiv preprint arXiv:1808.03998*, 2018.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] H. Idrees, M. Shah, 和 R. Surette，“使用计算机视觉增强摄像头监控：研究笔记”，*arXiv预印本 arXiv:1808.03998*，2018年。'
- en: '[174] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese, “Deep metric learning
    via lifted structured feature embedding,” in *CVPR*, 2016, pp. 4004–4012.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] H. O. Song, Y. Xiang, S. Jegelka, 和 S. Savarese，“通过提升结构特征嵌入进行深度度量学习”，发表于
    *CVPR*，2016年，第4004–4012页。'
- en: '[175] J. Song, Y.-Z. Song, T. Xiang, T. M. Hospedales, and X. Ruan, “Deep multi-task
    attribute-driven ranking for fine-grained sketch-based image retrieval,” in *BMVC*,
    2016, pp. 1–11.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] J. Song, Y.-Z. Song, T. Xiang, T. M. Hospedales, 和 X. Ruan，“深度多任务属性驱动排序用于细粒度基于素描的图像检索”，发表于
    *BMVC*，2016年，第1–11页。'
- en: '[176] F. Radenovic, G. Tolias, and O. Chum, “Deep shape matching,” in *ECCV*,
    2018, pp. 751–767.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] F. Radenovic, G. Tolias, 和 O. Chum，“深度形状匹配”，发表于 *ECCV*，2018年，第751–767页。'
- en: '[177] J. Zhang, F. Shen, L. Liu, F. Zhu, M. Yu, L. Shao, H. Tao Shen, and L. Van Gool,
    “Generative domain-migration hashing for sketch-to-image retrieval,” in *ECCV*,
    2018, pp. 297–314.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] J. Zhang, F. Shen, L. Liu, F. Zhu, M. Yu, L. Shao, H. Tao Shen, 和 L.
    Van Gool，“用于素描到图像检索的生成领域迁移哈希”，发表于 *ECCV*，2018年，第297–314页。'
- en: '[178] K. Pang, Y. Yang, T. M. Hospedales, T. Xiang, and Y.-Z. Song, “Solving
    mixed-modal jigsaw puzzle for fine-grained sketch-based image retrieval,” in *CVPR*,
    2020, pp. 10 347–10 355.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] K. Pang, Y. Yang, T. M. Hospedales, T. Xiang, 和 Y.-Z. Song，“解决混合模态拼图以进行细粒度基于素描的图像检索”，发表于
    *CVPR*，2020年，第10 347–10 355页。'
- en: '[179] A. K. Bhunia, Y. Yang, T. M. Hospedales, T. Xiang, and Y.-Z. Song, “Sketch
    less for more: On-the-fly fine-grained sketch-based image retrieval,” in *CVPR*,
    2020, pp. 9779–9788.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] A. K. Bhunia, Y. Yang, T. M. Hospedales, T. Xiang, 和 Y.-Z. Song，“减少素描以获得更多：即时细粒度基于素描的图像检索”，发表于
    *CVPR*，2020年，第9779–9788页。'
- en: '[180] A. Sain, A. K. Bhunia, Y. Yang, T. Xiang, and Y.-Z. Song, “Cross-modal
    hierarchical modelling for fine-grained sketch based image retrieval,” in *BMVC*,
    2020, pp. 1–14.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] A. Sain, A. K. Bhunia, Y. Yang, T. Xiang, 和 Y.-Z. Song，“用于细粒度基于素描的图像检索的跨模态分层建模”，发表于
    *BMVC*，2020年，第1–14页。'
- en: '[181] R. Girshick, F. Iandola, T. Darrell, and J. Malik, “Deformable part models
    are convolutional neural networks,” in *CVPR*, 2015, pp. 437–446.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] R. Girshick, F. Iandola, T. Darrell, 和 J. Malik，“可变形部件模型即卷积神经网络，”发表在
    *CVPR*，2015年，页码 437–446。'
- en: '[182] K. Pang, Y.-Z. Song, T. Xiang, and T. M. Hospdales, “Cross-domain generative
    learning for fine-grained sketch-based image retrieval,” in *BMVC*, 2017, pp.
    1–12.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] K. Pang, Y.-Z. Song, T. Xiang, 和 T. M. Hospdales，“跨领域生成学习用于细粒度基于草图的图像检索，”发表在
    *BMVC*，2017年，页码 1–12。'
- en: '[183] J. Song, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Fine-grained image
    retrieval: the text/sketch input dilemma,” in *BMVC*, 2017, pp. 1–12.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] J. Song, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“细粒度图像检索：文本/草图输入困境，”发表在
    *BMVC*，2017年，页码 1–12。'
- en: '[184] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation
    learning by predicting image rotations,” in *ICLR*, 2018, pp. 1–16.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] S. Gidaris, P. Singh, 和 N. Komodakis，“通过预测图像旋转进行无监督表示学习，”发表在 *ICLR*，2018年，页码
    1–16。'
- en: '[185] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
    unsupervised visual representation learning,” in *CVPR*, 2020, pp. 9729–9738.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] K. He, H. Fan, Y. Wu, S. Xie, 和 R. Girshick，“用于无监督视觉表示学习的动量对比，”发表在 *CVPR*，2020年，页码
    9729–9738。'
- en: '[186] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
    for contrastive learning of visual representations,” in *ICML*, 2020, pp. 1597–1607.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] T. Chen, S. Kornblith, M. Norouzi, 和 G. Hinton，“用于对比学习视觉表示的简单框架，”发表在
    *ICML*，2020年，页码 1597–1607。'
- en: '[187] F. Liu, C. Zou, X. Deng, R. Zuo, Y.-K. Lai, C. Ma, Y.-J. Liu, and H. Wang,
    “SceneSketcher: Fine-grained image retrieval with scene sketches,” in *ECCV*,
    2020, pp. 718–734.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] F. Liu, C. Zou, X. Deng, R. Zuo, Y.-K. Lai, C. Ma, Y.-J. Liu, 和 H. Wang，“SceneSketcher：基于场景草图的细粒度图像检索，”发表在
    *ECCV*，2020年，页码 718–734。'
- en: '[188] K. Musgrave, S. Belongie, and S.-N. Lim, “A metric learning reality check,”
    in *ECCV*, 2020, pp. 681–699.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] K. Musgrave, S. Belongie, 和 S.-N. Lim，“度量学习的现实检验，”发表在 *ECCV*，2020年，页码
    681–699。'
- en: '[189] Y. Cui, Z. Gu, D. Mahajan, L. van der Maaten, S. Belongie, and S.-N.
    Lim, “Measuring dataset granularity,” *arXiv preprint arXiv:1912.10154*, 2019.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Y. Cui, Z. Gu, D. Mahajan, L. van der Maaten, S. Belongie, 和 S.-N. Lim，“测量数据集的粒度，”
    *arXiv 预印本 arXiv:1912.10154*，2019年。'
- en: '[190] D. Chang, K. Pang, Y. Zheng, Z. Ma, Y.-Z. Song, and J. Guo, “Your “flamingo”
    is my “bird”: Fine-grained, or not,” *arXiv preprint arXiv:2011.09040*, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] D. Chang, K. Pang, Y. Zheng, Z. Ma, Y.-Z. Song, 和 J. Guo，“你的“火烈鸟”是我的“鸟”：细粒度，还是不是，”
    *arXiv 预印本 arXiv:2011.09040*，2020年。'
- en: '[191] X. Wang, L. Lian, Z. Miao, Z. Liu, and S. X. Yu, “Long-tailed recognition
    by routing diverse distribution-aware experts,” in *ICLR*, 2021, pp. 1–15.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] X. Wang, L. Lian, Z. Miao, Z. Liu, 和 S. X. Yu，“通过路由多样化分布感知专家实现长尾识别，”发表在
    *ICLR*，2021年，页码 1–15。'
- en: '[192] C. Li, D. Du, L. Zhang, T. Luo, Y. Wu, Q. Tian, L. Wen, and S. Lyu, “Data
    priming network for automatic check-out,” in *ACM MM*, 2019, pp. 2152–2160.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] C. Li, D. Du, L. Zhang, T. Luo, Y. Wu, Q. Tian, L. Wen, 和 S. Lyu，“用于自动结账的数据引导网络，”发表在
    *ACM MM*，2019年，页码 2152–2160。'
- en: '[193] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE TPAMI*, 2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] L. Jing 和 Y. Tian，“基于深度神经网络的自监督视觉特征学习：综述，” *IEEE TPAMI*，2020年。'
- en: '[194] S. I. Nikolenko, “Synthetic data for deep learning,” *arXiv preprint
    arXiv:1909.11512*, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] S. I. Nikolenko，“深度学习的合成数据，” *arXiv 预印本 arXiv:1909.11512*，2019年。'
- en: '[195] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker, “Fine-grained 3D shape classification
    with hierarchical part-view attention,” *IEEE TIP*, vol. 30, pp. 1744–1758, 2021.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] X. Liu, Z. Han, Y.-S. Liu, 和 M. Zwicker，“基于分层部件视图注意力的细粒度3D形状分类，” *IEEE
    TIP*，第30卷，页码 1744–1758，2021年。'
- en: '[196] E. Cole, X. Yang, K. Wilber, O. Mac Aodha, and S. Belongie, “When does
    contrastive visual representation learning work?” *arXiv preprint arXiv:2105.05837*,
    2021.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] E. Cole, X. Yang, K. Wilber, O. Mac Aodha, 和 S. Belongie，“对比视觉表示学习何时有效？”
    *arXiv 预印本 arXiv:2105.05837*，2021年。'
- en: '[197] Q. Zhang and S.-C. Zhu, “Visual interpretability for deep learning: a
    survey,” *arXiv preprint arXiv:1802.00614*, 2018.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Q. Zhang 和 S.-C. Zhu，“深度学习的视觉可解释性：综述，” *arXiv 预印本 arXiv:1802.00614*，2018年。'
- en: '[198] S. Tsutsui, Y. Fu, and D. Crandall, “Meta-reinforced synthetic data for
    one-shot fine-grained visual recognition,” in *NIPS*, 2019, pp. 3063–3072.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] S. Tsutsui, Y. Fu, 和 D. Crandall，“用于一-shot细粒度视觉识别的元增强合成数据，”发表在 *NIPS*，2019年，页码
    3063–3072。'
- en: '[199] L. Tang, D. Wertheimer, and B. Hariharan, “Revisiting pose-normalization
    for fine-grained few-shot recognition,” in *CVPR*, 2020.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] L. Tang, D. Wertheimer, 和 B. Hariharan，“重新审视姿态归一化以实现细粒度少样本识别，”发表在 *CVPR*，2020年。'
- en: '[200] J.-C. Su, S. Maji, and B. Hariharan, “When does self-supervision improve
    few-shot learning?” in *ECCV*, 2020, pp. 645–666.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] J.-C. Su, S. Maji, 和 B. Hariharan，“自监督何时改善少样本学习？”发表在 *ECCV*，2020年，页码
    645–666。'
- en: '[201] J. Wang, T. Zhang, J. Song, N. Sebe, and H. T. Shen, “A survey on learning
    to hash,” *IEEE TPAMI*, vol. 40, no. 4, pp. 769–790, 2018.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] J. Wang, T. Zhang, J. Song, N. Sebe, 和 H. T. Shen, “关于学习哈希的综述,” *IEEE
    TPAMI*, 第 40 卷, 第 4 期, 第 769–790 页, 2018。'
- en: '[202] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep supervised
    hashing with pairwise labels,” in *IJCAI*, 2016, pp. 1711–1717.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] W.-J. Li, S. Wang, 和 W.-C. Kang, “基于对偶标签的深度监督哈希特征学习,” 见 *IJCAI*, 2016,
    第 1711–1717 页。'
- en: '[203] Q. Cui, Q.-Y. Jiang, X.-S. Wei, W.-J. Li, and O. Yoshie, “ExchNet: A
    unified hashing network for large-scale fine-grained image retrieval,” in *ECCV*,
    2020, pp. 189–205.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Q. Cui, Q.-Y. Jiang, X.-S. Wei, W.-J. Li, 和 O. Yoshie, “ExchNet: 用于大规模细粒度图像检索的统一哈希网络,”
    见 *ECCV*, 2020, 第 189–205 页。'
- en: '[204] S. Jin, H. Yao, X. Sun, S. Zhou, L. Zhang, and X. Hua, “Deep saliency
    hashing for fine-grained retrieval,” *IEEE TIP*, vol. 29, pp. 5336–5351, 2020.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] S. Jin, H. Yao, X. Sun, S. Zhou, L. Zhang, 和 X. Hua, “用于细粒度检索的深度显著性哈希,”
    *IEEE TIP*, 第 29 卷, 第 5336–5351 页, 2020。'
- en: '[205] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter,
    “Efficient and robust automated machine learning,” in *NIPS*, 2015, pp. 2962–2970.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, 和 F.
    Hutter, “高效且鲁棒的自动化机器学习,” 见 *NIPS*, 2015, 第 2962–2970 页。'
- en: '[206] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search:
    A survey,” *arXiv preprint arXiv:1808.05377*, 2018.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] T. Elsken, J. H. Metzen, 和 F. Hutter, “神经架构搜索: 一项综述,” *arXiv 预印本 arXiv:1808.05377*,
    2018。'
- en: '[207] T. Gebru, J. Hoffman, and L. Fei-Fei, “Fine-grained recognition in the
    wild: A multi-task domain adaptation approach,” in *ICCV*, 2017, pp. 1349–1358.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] T. Gebru, J. Hoffman, 和 L. Fei-Fei, “野外细粒度识别: 一种多任务领域适应方法,” 见 *ICCV*,
    2017, 第 1349–1358 页。'
- en: '[208] S. Wang, X. Chen, Y. Wang, M. Long, and J. Wang, “Progressive adversarial
    networks for fine-grained domain adaptation,” in *CVPR*, 2020, pp. 9213–9222.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] S. Wang, X. Chen, Y. Wang, M. Long, 和 J. Wang, “用于细粒度领域适应的渐进对抗网络,” 见
    *CVPR*, 2020, 第 9213–9222 页。'
- en: '[209] Y. Wang, R.-J. Song, X.-S. Wei, and L. Zhang, “An adversarial domain
    adaptation network for cross-domain fine-grained recognition,” in *WACV*, 2020,
    pp. 1228–1236.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Y. Wang, R.-J. Song, X.-S. Wei, 和 L. Zhang, “一种用于跨域细粒度识别的对抗领域适应网络,” 见
    *WACV*, 2020, 第 1228–1236 页。'
- en: '[210] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, “Class-balanced
    loss based on effective number of samples,” in *CVPR*, 2019, pp. 9268–9277.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, 和 S. Belongie, “基于有效样本数量的类别平衡损失,”
    见 *CVPR*, 2019, 第 9268–9277 页。'
- en: '[211] B. Zhou, Q. Cui, X.-S. Wei, and Z.-M. Chen, “BBN: Bilateral-branch network
    with cumulative learning for long-tailed visual recognition,” in *CVPR*, 2020,
    pp. 9719–9728.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] B. Zhou, Q. Cui, X.-S. Wei, 和 Z.-M. Chen, “BBN: 用于长尾视觉识别的双侧分支网络与累积学习,”
    见 *CVPR*, 2020, 第 9719–9728 页。'
- en: '[212] Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, and S. X. Yu, “Large-scale
    long-tailed recognition in an open world,” in *CVPR*, 2019, pp. 2537–2546.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, 和 S. X. Yu, “开放世界中的大规模长尾识别,”
    见 *CVPR*, 2019, 第 2537–2546 页。'
- en: '[213] C. Zhu, X. Tan, F. Zhou, X. Liu, K. Yue, E. Ding, and Y. Ma, “Fine-grained
    video categorization with redundancy reduction attention,” in *ECCV*, 2018, pp.
    136–152.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] C. Zhu, X. Tan, F. Zhou, X. Liu, K. Yue, E. Ding, 和 Y. Ma, “具有冗余减少注意力的细粒度视频分类,”
    见 *ECCV*, 2018, 第 136–152 页。'
- en: '[214] J. Munro and D. Damen, “Multi-modal domain adaptation for fine-grained
    action recognition,” in *CVPR*, 2020, pp. 122–132.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] J. Munro 和 D. Damen, “用于细粒度动作识别的多模态领域适应,” 见 *CVPR*, 2020, 第 122–132 页。'
- en: '| ![[Uncaptioned image]](img/05ad5090690cea4eb72178ef5bdef883.png) | Xiu-Shen
    Wei is a Professor with the School of Computer Science and Engineering, Nanjing
    University of Science and Technology, China. He was a Program Chair for the workshops
    associated with ICCV, IJCAI, ACM Multimedia, etc. He has also served as a Guest
    Editor of Pattern Recognition Journal, and a Tutorial Chair for Asian Conference
    on Computer Vision (ACCV) 2022. |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| ![[无说明图片]](img/05ad5090690cea4eb72178ef5bdef883.png) | 魏修申是中国南京理工大学计算机科学与工程学院的教授。他曾担任与
    ICCV、IJCAI、ACM Multimedia 等相关的研讨会程序主席。他还曾担任《模式识别期刊》的客座编辑，以及 2022 年亚洲计算机视觉会议（ACCV）的教程主席。
    |'
- en: '| ![[Uncaptioned image]](img/4ef43107113907e8467948811b5f5f64.png) | Yi-Zhe
    Song is a Chair Professor of Computer Vision and Machine Learning, and Director
    of SketchX Lab at the Centre for Vision Speech and Signal Processing (CVSSP),
    University of Surrey, UK. He is an Associate Editor of the IEEE Transactions on
    Pattern Analysis and Machine Intelligence (TPAMI), and a Program Chair for British
    Machine Vision Conference (BMVC) 2021. |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/4ef43107113907e8467948811b5f5f64.png) | 宋一哲是英国萨里大学视觉、语音与信号处理中心（CVSSP）的计算机视觉与机器学习讲席教授，SketchX实验室主任。他是《IEEE模式分析与机器智能汇刊》（TPAMI）的副主编，以及2021年英国机器视觉会议（BMVC）的程序主席。
    |'
- en: '| ![[Uncaptioned image]](img/83dcfd08794eeab2ddfc055e91b98ec4.png) | Oisin
    Mac Aodha is a Lecturer in Machine Learning at the University of Edinburgh, UK.
    He is a Fellow of the Alan Turing Institute and a European Laboratory for Learning
    and Intelligent Systems (ELLIS) Scholar. He was a Program Chair for the British
    Machine Vision Conference (BMVC) 2020. |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/83dcfd08794eeab2ddfc055e91b98ec4.png) | 奥伊辛·麦克·奥赫是英国爱丁堡大学机器学习讲师。他是艾伦·图灵研究所的研究员，以及欧洲学习与智能系统实验室（ELLIS）学者。他曾是2020年英国机器视觉会议（BMVC）的程序主席。
    |'
- en: '| ![[Uncaptioned image]](img/657be029ff811b9c25127daae75da0ea.png) | Jianxin
    Wu is currently a professor in the Department of Computer Science and Technology
    at Nanjing University, China, and is associated with the State Key Laboratory
    for Novel Software Technology, China. |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/657be029ff811b9c25127daae75da0ea.png) | 吴建新目前是中国南京大学计算机科学与技术系的教授，并且与中国新型软件技术国家重点实验室相关联。
    |'
- en: '| ![[Uncaptioned image]](img/0754ae2d34ca88929d69cd3613b04e27.png) | Yuxin
    Peng is currently the Boya Distinguished Professor with Wangxuan Institute of
    Computer Technology, Peking University, China. |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/0754ae2d34ca88929d69cd3613b04e27.png) | 彭宇新目前是中国北京大学王选计算机技术研究所的博雅特聘教授。
    |'
- en: '| ![[Uncaptioned image]](img/d9734939cd452c0579a4295b236f8b05.png) | Jinhui
    Tang is a Professor with the School of Computer Science and Engineering, Nanjing
    University of Science and Technology, China. |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/d9734939cd452c0579a4295b236f8b05.png) | 唐金辉是中国南京理工大学计算机科学与工程学院的教授。
    |'
- en: '| ![[Uncaptioned image]](img/8b428719a1bae07dff747a7a160e0ac6.png) | Jian Yang
    is a Chang-Jiang professor in the School of Computer Science and Technology of
    Nanjing University of Science and Technology, China. |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/8b428719a1bae07dff747a7a160e0ac6.png) | 杨健是中国南京理工大学计算机科学与技术学院的长江教授。
    |'
- en: '| ![[Uncaptioned image]](img/da39ca57717b2963e3d0dfd587a080a3.png) | Serge
    Belongie is a professor of Computer Science at the University of Copenhagen (DIKU)
    and the director of the Pioneer Centre for AI, Denmark. |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/da39ca57717b2963e3d0dfd587a080a3.png) | 塞尔日·贝隆吉是哥本哈根大学（DIKU）计算机科学教授，也是丹麦先锋人工智能中心的主任。
    |'
