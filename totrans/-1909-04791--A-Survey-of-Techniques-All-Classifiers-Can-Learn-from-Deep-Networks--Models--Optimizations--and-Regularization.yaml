- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:04:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1909.04791] A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1909.04791](https://ar5iv.labs.arxiv.org/html/1909.04791)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alireza Ghods
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Washington State University
  prefs: []
  type: TYPE_NORMAL
- en: Pullman, WA 99163
  prefs: []
  type: TYPE_NORMAL
- en: alireza.ghods@wsu.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Diane J. Cook'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: Washington State University
  prefs: []
  type: TYPE_NORMAL
- en: Pullman, WA 99163
  prefs: []
  type: TYPE_NORMAL
- en: djcook@wsu.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep neural networks have introduced novel and useful tools to the machine learning
    community. Other types of classifiers can potentially make use of these tools
    as well to improve their performance and generality. This paper reviews the current
    state of the art for deep learning classifier technologies that are being used
    outside of deep neural networks. Non-network classifiers can employ many components
    found in deep neural network architectures. In this paper, we review the feature
    learning, optimization, and regularization methods that form a core of deep network
    technologies. We then survey non-neural network learning algorithms that make
    innovative use of these methods to improve classification. Because many opportunities
    and challenges still exist, we discuss directions that can be pursued to expand
    the area of deep learning for a variety of classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '*K*eywords Deep Learning  $\cdot$ Deep Neural Networks,  $\cdot$ ,  $\cdot$
    Regularization'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of supervised learning algorithms is to identify an optimal mapping
    between input features and output values based on a given training dataset. A
    supervised learning method that is attracting substantial research and industry
    attention is Deep Neural Networks (DNN). DNNs have a profound effect on our daily
    lives; they are found in search engines [[1](#bib.bib1)] [[2](#bib.bib2)], self-driving
    cars [[3](#bib.bib3)] [[4](#bib.bib4)] [[5](#bib.bib5)], health care systems [[6](#bib.bib6)],
    and consumer devices such as smart-phones and cameras [[7](#bib.bib7)]. Convolutional
    Neural Networks (CNN) have become the standard for processing images [[8](#bib.bib8)]
    [[9](#bib.bib9)] [[10](#bib.bib10)], whereas Recurrent Neural Networks (RNN) dominate
    the processing of sequential data such as text and voice [[11](#bib.bib11)] [[12](#bib.bib12)]
    [[13](#bib.bib13)] [[14](#bib.bib14)]. DNNs allow machines to automatically discover
    the representations needed for detection or classification of raw input [[15](#bib.bib15)].
    Additionally, the neural network community developed unsupervised algorithms to
    help with the learning of unlabeled data. These unsupervised methods have found
    their way to real-world applications, such as creating generative adversarial
    networks (GANs) that design clothes [[16](#bib.bib16)]. The term deep has been
    used to distinguish these networks from shallow networks which have only one hidden
    layer; in contrast, DNNs have multiple hidden layers. The two terms deep learning
    and deep neural networks have been used synonymously. However, we observe that
    deep leaning itself conveys a broader meaning, which can also shape the field
    of machine learning outside the realm of neural network algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The remarkable recent DNN advances were made possible by the availability of
    massive amounts of computational power and labeled data. However, these advances
    do not overcome all of the difficulties associated with DNNs. For example, there
    are many real-world scenarios, such as analyzing power distribution data [[17](#bib.bib17)],
    for which large annotated datasets do not exist due to the complexity and expense
    of collecting data. While applications like clinical interpretation of medical
    diagnoses require that the learned model be understandable, most DNNs resist interpretation
    due to their complexity [[18](#bib.bib18)]. DNNs can be insensitive to noisy training
    data [[19](#bib.bib19)] [[20](#bib.bib20)] [[21](#bib.bib21)], and they also require
    appropriate parameter initialization to converge [[22](#bib.bib22)] [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite these shortcomings, DNNs have reported higher predictive accuracy than
    other supervised learning methods for many datasets, given enough supervised data
    and computational resources. Deep models offer structural advantages that may
    improve the quality of learning in complex datasets as empirically shown by Bengio
    [[24](#bib.bib24)]. Recently, researchers have designed hybrid methods which combine
    unique DNN techniques with other classifiers to address some of these identified
    problems or to boost other classifiers. This survey paper investigates these methods,
    reviewing classifiers which have adapted DNN techniques to alternative classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Research Objectives and Outline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While DNN research is growing rapidly, this paper aims to draw a broader picture
    of deep learning methods. Although some studies provide evidence that DNN models
    offer greater generalization than classic machine learning algorithms for complex
    data [[25](#bib.bib25)] [[26](#bib.bib26)] [[27](#bib.bib27)] [[28](#bib.bib28)]
    [[29](#bib.bib29)], there is no “silver bullet” approach to concept learning [[30](#bib.bib30)].
    Numerous studies comparing DNNs and other supervised learning algorithms [[31](#bib.bib31)]
    [[32](#bib.bib32)] [[33](#bib.bib33)] [[34](#bib.bib34)] [[35](#bib.bib35)] observe
    that the choice of algorithm depends on the data - no ideal algorithm exists which
    generalizes optimally on all types of data. Recognizing the unique and important
    role other classifiers thus play, we aim to investigate how non-network machine
    learning algorithms can benefit from the advances in deep neural networks. Many
    deep learning survey papers have been published that provide a primer on the topic
    [[36](#bib.bib36)] or highlight diverse applications such as object detection
    [[37](#bib.bib37)], medical record analysis [[38](#bib.bib38)], activity recognition
    [[39](#bib.bib39)], and natural language processing [[40](#bib.bib40)]. In this
    survey, we do not focus solely on deep neural network models but rather on how
    deep learning can inspire a broader range of classifiers. We concentrate on research
    breakthroughs that transform non-network classifiers into deep learners. Further,
    we review deep network techniques such as stochastic gradient descent that can
    be used more broadly, and we discuss ways in which non-network models can benefit
    from network-inspired deep learning innovations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The literature provides evidence that non-network models may offer improved
    generalizability over deep networks, depending on the amount and type of data
    that is available. By surveying methods for transforming non-network classifiers
    into deep learners, these approaches can become stronger learners. To provide
    evidence of the need for continued research on this topic, we also implement a
    collection of shallow and deep learners surveyed in this paper, both network and
    non-network classifiers, to compare their performance. Figure [1](#S1.F1 "Figure
    1 ‣ 1.1 Research Objectives and Outline ‣ 1 Introduction ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization")
    highlights deep learning components that we discuss in this survey. This graph
    also summarizes the deep classifiers that we survey and the relationships that
    we highlight between techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: for tree= font=, draw, semithick, rounded corners, align = center, inner sep
    = 1mm, edge = semithick, -stealth, l sep=0.5cm, s sep=0.2cm, fork sep=1mm, parent
    anchor=south, child anchor=north, edge path= [-Stealth[], \forestoptionedge, thin]
    (!u.parent anchor) – +(0,-5pt) -| (.child anchor)\forestoptionedge label; , /tikz/>=LaTeX,
    , [Deep Learning, [Training Methodology, for tree=fill=gray!20 [Optimization,
    for tree=folder,grow’=0 [Gradient Decent [DNDF] [mGBDT] [ML-SVM] ] ] [Regularization,
    for tree=folder,grow’=0 [Feature Penalty [GRRF] [NLP-SVM] [RRF] [SCAD-SVM] ] [Dropout
    [DART] ] ] ] [Classifiers, for tree=fill=pink!20 [Network-Based, for tree=folder,grow’=0
    [AE [VAE] ] [CNN] [GAN] [MLP] [SNN] [RNN [LSTM] [GRU] ] ] [Tree-Based, for tree=folder,grow’=0
    [ANT] [DF] [DNDF] [eForest] [FSDT] [GAF] [mGBDT] [SDF] [DNDT] ] [SVM-Based, for
    tree=folder,grow’=0 [Deep SVM] [DTA-LS-SVM] [ML-SVM] [R2SVM] ] [Statistical-Based,
    for tree=folder,grow’=0 [DBN] [DGP] [DKF] ] [Hybrid, for tree=folder,grow’=0 [CondNN]
    [DBT] [DCCA] [Deep PCA] [DNDF] [LMM] ] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Content map of the methods covered in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Brief Overview of Deep Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 The Origin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In 1985, Rosenblatt introduced the Perceptron [[41](#bib.bib41)], an online
    binary classifier which flows input through a weight vector to an output layer.
    Perceptron learning uses a form of gradient descent to adjust the weights between
    the input and output layers to optimize a loss function [[42](#bib.bib42)]. A
    few years later, Minsky proved that a single-layer Perceptron is unable to learn
    nonlinear functions, including the XOR function [[43](#bib.bib43)]. Multilayer
    perceptrons (MLPs, see Table [4](#S8.T4 "Table 4 ‣ 8 Conclusions and Directions
    for Ongoing Research ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization") for a complete list of abbreviations)
    addressed the nonlinearity problem by adding layers of hidden units to the networks
    and applying alternative differentiable activation functions, such as sigmoid,
    to each node. Stochastic gradient descent was then applied to MLPs to determine
    the weights between layers that minimize function approximation errors [[44](#bib.bib44)].
    However, the lack of computational power caused DNN research to stagnate for decades,
    and other classifiers rose in popularity. In 2006, a renaissance began in DNN
    research, spurred by the introduction of Deep Belief Networks (DBNs) [[45](#bib.bib45)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Deep Neural Network Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the increasing popularity of deep learning, many DNN architectures have
    been introduced with variations such as Neural Turing Machines [[46](#bib.bib46)]
    and Capsule Neural Networks [[47](#bib.bib47)]. In this paper, we summarize the
    general form of DNNs together with architectural components that not only appear
    in DNNs but can be incorporated into other models. We start by reviewing popular
    types of DNNs that have been introduced and that play complementary learning roles.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.3.1 Multilayer Perceptron
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A multilayer perceptron (MLP) is one of the essential bases of many deep learning
    algorithms. The goal of a MLP is to map input $X$ to class $y$ by learning a function
    $y=f(X,\theta)$, where $\theta$ represents the best possible function approximation.
    For example, in Figure [2](#S2.F2 "Figure 2 ‣ 2.3.1 Multilayer Perceptron ‣ 2.3
    Supervised Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization")
    the MLP maps input $X$ to $y$ using function $f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$,
    where $f^{(1)}$ is the first layer, $f^{(2)}$ is the second layer, and $f^{(3)}$
    represents the third, output layer. This chain structure is a common component
    of many DNN architectures. The network depth is equal to the length of the chain,
    and the width of each layer represents the number of nodes in that layer [[48](#bib.bib48)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In networks such as the MLP, the connections are not cyclic and thus belong
    to a class of DNNs called feedforward networks. Feedforward networks move information
    in only one direction, from the input to the output layer. Figure [2](#S2.F2 "Figure
    2 ‣ 2.3.1 Multilayer Perceptron ‣ 2.3 Supervised Learning ‣ 2 Brief Overview of
    Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization") depicts a particular type
    of feedforward network which is a fully-connected multilayer perceptron because
    each node at one layer is connected to all of the nodes at the next layer. Special
    cases of feedforward networks and MLPs have drawn considerable recent attention,
    which we describe next.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F2.pic1" class="ltx_picture ltx_centering" height="179.43" overflow="visible"
    version="1.1" width="300.08"><g transform="translate(0,179.43) matrix(1 0 0 -1
    0 0) translate(39.65,0) translate(0,83.66)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.8 0.0 0.0 1.8 -9.34 -53.6)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.8 0.0 0.0 1.8 69.4 -16.69)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.8 0.0 0.0 1.8 148.14 -16.69)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.8 0.0 0.0 1.8 226.88 -18.17)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 51.68)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 22.15)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 -7.38)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.1 -67.26)" fill="#000000" stroke="#000000"><foreignobject
    width="14.71" height="7.63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 70.49 39.65)" fill="#000000" stroke="#000000"><foreignobject
    width="16.5" height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 70.29 -36.93)" fill="#000000" stroke="#000000"><foreignobject
    width="16.9" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{k}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 149.23 39.65)" fill="#000000" stroke="#000000"><foreignobject
    width="16.5" height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 149.35 -35.56)" fill="#000000" stroke="#000000"><foreignobject
    width="16.26" height="13.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{j}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 238.95 29.69)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 238.56 -29.36)" fill="#000000" stroke="#000000"><foreignobject
    width="11.93" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{n}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -16.72 63.94)" fill="#000000" stroke="#000000"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 21.06)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 14.91)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><g class="ltx_tikzmatrix" transform="matrix(1
    0 0 -1 0 8.835)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 12.23)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    0 0)"><text transform="matrix(1 0 0 -1 0 0)">Input</text></g></g></g></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 24.52)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 2.29 0)"><text transform="matrix(1
    0 0 -1 0 0)">layer</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0
    57.02 63.94)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1
    0 0 -1 0 18.53)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 12.38)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    0 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 7.575)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 12.38)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Hidden</text></g></g></g></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 21.99)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 7.28 0)"><text transform="matrix(1
    0 0 -1 0 0)">layer</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0
    135.76 63.94)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1
    0 0 -1 0 18.53)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 12.38)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    0 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 7.575)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 12.38)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Hidden</text></g></g></g></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 21.99)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 7.28 0)"><text transform="matrix(1
    0 0 -1 0 0)">layer</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0
    216.62 63.94)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1
    0 0 -1 0 21.06)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 14.91)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    0 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.835)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 12.23)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Ouput</text></g></g></g></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 24.52)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 5.17 0)"><text transform="matrix(1
    0 0 -1 0 0)">layer</text></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: An illustration of a three-layered MLP with $j$ nodes at the first
    hidden layer and $k$ at the second layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Deep Convolutional Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A convolutional neural network (CNN) [[49](#bib.bib49)] is a specialized class
    of feedforward DNNs for processing data that can be discretely presented. Examples
    of data that can benefit from CNNs include time series data that can be presented
    as samples of discrete regular time intervals and image data presented as samples
    of 2-D pixels at discrete locations. Most CNNs involve three stages: a convolution
    operation; an activation function, such as the rectified linear activation (ReLU)
    function [[50](#bib.bib50)]; and a pooling function, such as max pooling [[51](#bib.bib51)].
    A convolution operation is a weighted average or smooth estimation of a windowed
    input. One of the strengths of the convolution operation is that the connections
    between nodes in a network become sparser by learning a small kernel for unimportant
    features. Another benefit of convolution is parameter sharing. A CNN makes an
    assumption that a kernel learned for one input position can be used at every position,
    in contrast to a MLP which deploys a separate element of a weight matrix for each
    connection. Applying the convolution operator frequently improves the network’s
    learning ability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A pooling function replaces the output of specific nearby nodes by their statistical
    summary. For example, the max-pooling function returns the maximum of a rectangular
    neighborhood. The motivation behind adding a pooling layer is that statistically
    down-sampling the number of features makes the representation approximately invariant
    to small translations of the input by maintaining the essential features. The
    final output of the learner is generated via a Fully-Connected (FC) layer that
    appears after the convolutional and max-pooling layers (see Figure [3](#S2.F3
    "Figure 3 ‣ 2.3.2 Deep Convolutional Neural Network ‣ 2.3 Supervised Learning
    ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization") for
    an illustration of the process).'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F3.pic1" class="ltx_picture ltx_centering" height="177.15" overflow="visible"
    version="1.1" width="360.61"><g transform="translate(0,177.15) matrix(1 0 0 -1
    0 0) translate(13.89,0) translate(0,49.2)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -9.27 -44.59)" fill="#000000" stroke="#000000"><foreignobject
    width="50.04" height="49.81" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">|
    Input |
  prefs: []
  type: TYPE_NORMAL
- en: '| layer |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 28.33 -44.59)"
    fill="#000000" stroke="#000000"><foreignobject width="100.82" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| Convolutional |'
  prefs: []
  type: TYPE_TB
- en: '| layer + ReLU |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0
    130.33 73.52)" fill="#000000" stroke="#000000"><foreignobject width="93.67" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Max Pooling |'
  prefs: []
  type: TYPE_TB
- en: '| layer |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 166.13
    -44.59)" fill="#000000" stroke="#000000"><foreignobject width="100.82" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Convolutional |'
  prefs: []
  type: TYPE_TB
- en: '| layer + ReLU |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0
    248.44 73.52)" fill="#000000" stroke="#000000"><foreignobject width="93.67" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Max Pooling |'
  prefs: []
  type: TYPE_TB
- en: '| layer |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 252.86
    -44.59)" fill="#000000" stroke="#000000"><foreignobject width="45.47" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| FC |'
  prefs: []
  type: TYPE_TB
- en: '| layer |</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: An illustration of a three-layered CNN made of six convolution filters
    followed by six max pooling filters at the first layer, and eight convolution
    filters followed by seven max pooling filters at the second layer. The last layer
    is a fully connected layer (FC).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Recurrent Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A recurrent Neural Network (RNN) is a sequential model that can capture the
    relationship between items in a sequence. Unlike traditional neural networks,
    wherein all inputs are independent of each other, RNNs contain artificial neurons
    with one or more feedback loops. Feedback loops are recurrent cycles over time
    or sequence, as shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.3.3 Recurrent Neural
    Network ‣ 2.3 Supervised Learning ‣ 2 Brief Overview of Deep Neural Networks ‣
    A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"). An established RNN problem is exploding or vanishing gradients.
    For a long data sequence, the gradient could become increasingly smaller or increasingly
    larger, which halts the learning. To address this issue, Hochreiter et al. [[52](#bib.bib52)]
    introduced a long short-term memory (LSTM) model and Cho et al. [[53](#bib.bib53)]
    proposed a gated recurrent unit (GRU) model. Both of these networks allow the
    gradient to flow unchanged in the network, thus preventing exploding or vanishing
    gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F4.pic1" class="ltx_picture ltx_centering" height="157.48" overflow="visible"
    version="1.1" width="349.36"><g transform="translate(0,157.48) matrix(1 0 0 -1
    0 0) translate(14.76,0) translate(0,78.74)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.27 -63.78)" fill="#000000" stroke="#000000"><foreignobject
    width="12.55" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$X$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.99 -4.8)" fill="#000000" stroke="#000000"><foreignobject
    width="7.97" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.55 54.33)" fill="#000000" stroke="#000000"><foreignobject
    width="11.11" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Y$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 46.56 -4.8)" fill="#000000" stroke="#000000"><foreignobject
    width="40.74" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Unfold</foreignobject></g><g
    transform="matrix(1.2 0.0 0.0 1.2 107.95 -1.78)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\cdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.71 -60.79)" fill="#000000" stroke="#000000"><foreignobject
    width="17.16" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.68 -3.56)" fill="#000000" stroke="#000000"><foreignobject
    width="17.23" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 161.02 57.42)" fill="#000000" stroke="#000000"><foreignobject
    width="16.54" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.12 -60.84)" fill="#000000" stroke="#000000"><foreignobject
    width="10.71" height="8.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.09 -3.61)" fill="#000000" stroke="#000000"><foreignobject
    width="10.77" height="11.99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.43 57.42)" fill="#000000" stroke="#000000"><foreignobject
    width="10.08" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.35 -60.46)" fill="#000000" stroke="#000000"><foreignobject
    width="20.61" height="9.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.32 -3.23)" fill="#000000" stroke="#000000"><foreignobject
    width="20.67" height="12.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.66 57.65)" fill="#000000" stroke="#000000"><foreignobject
    width="19.98" height="9.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t+1}$</foreignobject></g><g
    transform="matrix(1.2 0.0 0.0 1.2 316.61 -1.78)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\cdots$</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: An illustration of a simple RNN and its unfolded structure through
    time $t$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4 Siamese Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are settings in which the number of training samples is limited, such
    as in facial recognition scenarios where only one image is available per person.
    When there is a limited number of examples for each class, DNNs struggle with
    generalizing the model. One strategy for addressing this problem is to learn a
    similarity function. This function computes the degree of difference between two
    samples, instead of learning each class. As an example, let $x_{1}$ represent
    one facial image and $x_{2}$ represent a second. If $d(x1,x2)\leq\tau$, we can
    conclude that the images are of the same person while $d(x_{1},x_{2})>\tau$ implies
    that they are different people. Siamese Neural Networks (SNN) [[54](#bib.bib54)]
    build on this idea by encoding examples $x_{i}$ and $x_{j}$ on two separate DNNs
    with shared parameters. The SNN learns a function $d$ using encoded features as
    shown in Figure [5](#S2.F5 "Figure 5 ‣ 2.3.4 Siamese Neural Network ‣ 2.3 Supervised
    Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization").
    The network then outputs $y>0$ for similar objects (i.e., when $d$ is less then
    a threshold value) and $y<0$ otherwise. Thus, SNNs can be used for similarity
    learning by learning a distance function over objects. In addition to their value
    for supervised learning from limited samples, SNNs are also beneficial for unsupervised
    learning tasks [[55](#bib.bib55)] [[56](#bib.bib56)].'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F5.pic1" class="ltx_picture ltx_centering" height="177.72" overflow="visible"
    version="1.1" width="349.35"><g transform="translate(0,177.72) matrix(1 0 0 -1
    0 0) translate(83.32,0) translate(0,98.7)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -24.97 46.81)" fill="#000000" stroke="#000000"><foreignobject
    width="10.58" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{i}$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -0.08 26.92)" fill="#000000" stroke="#000000"><foreignobject
    width="78.91" height="24.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">|
    Network-1 |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -25.46 -69.9)"
    fill="#000000" stroke="#000000"><foreignobject width="11.54" height="10.02" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$x_{j}$</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 -0.08 -71.51)" fill="#000000" stroke="#000000"><foreignobject width="78.91"
    height="24.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Network-2
    |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -78.71 -13.3)" fill="#000000"
    stroke="#000000"><foreignobject width="113.19" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Shared parameters</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 102.95 46.81)" fill="#000000" stroke="#000000"><foreignobject width="10.64"
    height="12.16" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{i}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 102.46 -73.55)" fill="#000000" stroke="#000000"><foreignobject
    width="11.61" height="13.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{j}$</foreignobject></g><g
    stroke-opacity="1" fill="#000000" fill-opacity="1" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 123 -13)"><foreignobject width="46.36" height="14.44" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$d(h_{i},h_{j})$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 186.3 -0.89)" fill="#000000" stroke="#000000"><foreignobject width="70.32"
    height="12.98" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-1\leq
    y_{i,j}\leq 1$</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: An illustration of an SNN. In this figure, $x_{i}$ and $x_{j}$ are
    two data vectors corresponding to a pair of instances from the training set. Both
    networks share the same weights and map the input to a new representation. By
    comparing the outputs of the networks using a distance measure such as Euclidean,
    we can determine the compatibility between instances $x_{i}$ and $x_{j}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.4.1 Generative Adversarial Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Until this point in the survey, we have focused on deep learning for its power
    in classifying data points. However, researchers have exploited deep learning
    for other uses as well, such as generating synthetic data that shares characteristics
    with known real data. One way to create synthetic data is to learn a generative
    model. A generative model learns the parameters that govern a distribution based
    on observation of real data points from that distribution. The learned model can
    then be used to create arbitrary amounts of synthetic data that emulate the real
    data observations. Recently, researchers have found a way to exploit multiplayer
    games for the purpose of improving generative machine learning algorithms. In
    the adversarial training scenario, two agents compete against each other, as inspired
    by Samuel [[57](#bib.bib57)] who designed a computer program to play checkers
    against itself. Goodfellow et al. [[58](#bib.bib58)] put this idea to use in developing
    Generative Adversarial Networks (GANs), in which a DNN (generator) tries to generate
    synthetic data that is so similar to real data that it fools its opponent DNN
    (discriminator), whose job is to distinguish real from fake data (see Figure [6](#S2.F6
    "Figure 6 ‣ 2.4.1 Generative Adversarial Network ‣ 2.4 Unsupervised Learning ‣
    2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization") for
    an illustration). The goal of GANs is to simultaneously improve the ability of
    the generator to produce realistic data and of the discriminator to distinguish
    synthetic from real data. GANs have found successful application in diverse tasks
    including translating text to images [[59](#bib.bib59)], discovering drugs [[60](#bib.bib60)],
    and transforming sketches to images [[61](#bib.bib61)] [[62](#bib.bib62)].'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F6.pic1" class="ltx_picture ltx_centering" height="177.72" overflow="visible"
    version="1.1" width="335.2"><g transform="translate(0,177.72) matrix(1 0 0 -1
    0 0) translate(98.7,0) translate(0,98.7)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 47.99 44.26)" fill="#000000" stroke="#000000"><foreignobject
    width="61.5" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Real
    Data</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 107.69 14.46)" fill="#000000"
    stroke="#000000"><foreignobject width="99.59" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| Discriminator |
  prefs: []
  type: TYPE_NORMAL
- en: '| Network |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 202.99
    44.26)" fill="#000000" stroke="#000000"><foreignobject width="27.1" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Real</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 202.6 24.87)" fill="#000000" stroke="#000000"><foreignobject
    width="27.87" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Fake</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -93.36 -54.17)" fill="#000000" stroke="#000000"><foreignobject
    width="88.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Random
    Noise</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 0.62 -83.96)" fill="#000000"
    stroke="#000000"><foreignobject width="77.51" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| Generator |'
  prefs: []
  type: TYPE_TB
- en: '| Network |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 134.28
    -83.96)" fill="#000000" stroke="#000000"><foreignobject width="46.39" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Fake |'
  prefs: []
  type: TYPE_TB
- en: '| Data |</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: An illustration of a GAN. The goal of the discriminator network is
    to distinguish real data from fake data, and the goal of the generator network
    is to use the feedback from the discriminator to generate data that the discriminator
    cannot distinguish from real.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Yet another purpose for deep neural networks is to provide data compression
    and dimensionality reduction. An Autoencoder (AE) is a DNN that accomplishes this
    goal by creating an output layer that resembles the input layer, using a reduced
    set of terms represented by the middle layers [[48](#bib.bib48)]. Architecturally,
    an AE combines two networks. The first network, called the encoder, learns a new
    representation of input $x$ with fewer features $h=f(x)$; the second part, called
    the decoder, maps $h$ onto a reconstruction of the input space $\hat{y}=g(h)$,
    as shown in Figure [7](#S2.F7 "Figure 7 ‣ 2.4.2 Autoencoder ‣ 2.4 Unsupervised
    Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization").
    The goal of an AE is not simply to recreate the input features. Instead, an AE
    learns an approximation of the input features to identify useful properties of
    the data. AEs are vital tools for dimensionality reduction [[63](#bib.bib63)],
    feature learning [[64](#bib.bib64)], image colorization [[65](#bib.bib65)], higher-resolution
    data generation [[66](#bib.bib66)], and latent space clustering [[67](#bib.bib67)].
    Additionally, other versions of AEs such as variational autoencoders (VAEs) [[68](#bib.bib68)]
    can be used as generative models.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S2.F7.pic1" class="ltx_picture ltx_centering" height="167.68" overflow="visible"
    version="1.1" width="305.94"><g transform="translate(0,167.68) matrix(1 0 0 -1
    0 0) translate(39.65,0) translate(0,83.66)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.8 0.0 0.0 1.8 -9.34 -53.6)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.8 0.0 0.0 1.8 226.88 -53.6)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 51.87)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 22.34)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 -7.18)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 247.6 -66.24)" fill="#000000" stroke="#000000"><foreignobject
    width="14.08" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 51.68)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 22.15)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 -7.38)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.1 -67.26)" fill="#000000" stroke="#000000"><foreignobject
    width="14.71" height="7.63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -24.91 66.9)" fill="#000000" stroke="#000000"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 7.705)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 12.52)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Encoder</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 211.51 66.9)" fill="#000000" stroke="#000000"><g
    class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 7.705)"><g class="ltx_tikzmatrix_row"
    transform="matrix(1 0 0 1 0 12.52)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Decoder</text></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: An illustration of an AE. The first part of the network, called the
    encoder, compresses input into a latent-space by learning the function $h=f(x)$.
    The second part, called the decoder, reconstructs the input from the latent-space
    representation by learning the function $\hat{y}=g(h)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Optimization for Training Deep Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we described common DNN architecture components. In
    this section, we offer a brief overview of optimization approaches for training
    DNNs. Learning methods may optimize a function $f(x)$ (e.g., minimize a loss function)
    by modifying model parameters (e.g., changing DNN weights). However, as Bengio
    et al. [[69](#bib.bib69)] point out, DNN optimization during training may be further
    complicated by local minima and ill-conditioning (see Figure [8](#S2.F8 "Figure
    8 ‣ 2.5 Optimization for Training Deep Neural Networks ‣ 2 Brief Overview of Deep
    Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization") for an illustration of an ill-condition).'
  prefs: []
  type: TYPE_NORMAL
- en: The most common type of optimization strategy employed by DNNs is gradient descent.
    This intuitive approach to learns the weights of connections between layers which
    reduce the network’s objective function by computing the error derivative with
    respect to a Ir-level layer of the network. Input $x$ is fed forward through a
    network to predict $\hat{y}$. A cost function $J(\theta)$ measures the error of
    the network at the output layer. Gradient descent then directs the cost value
    to flow backward through the network by computing the gradient of the objective
    function $\nabla_{\theta}J(\theta)$. This process is sometimes alternatively referred
    to as backpropagation because the training error propagates backward through the
    network from output to input layers. Many variations of gradient descent have
    been tested for DNN optimization, such as stochastic gradient descent, mini-batch
    gradient descent, momentum [[70](#bib.bib70)], Ada-Grad [[71](#bib.bib71)], and
    Adam [[72](#bib.bib72)].
  prefs: []
  type: TYPE_NORMAL
- en: Deep network optimization is an active area of research. Along with gradient
    descent, many other algorithms such as derivative-free optimization [[73](#bib.bib73)]
    and feedback-alignment [[74](#bib.bib74)] have appeared. However, none of these
    algorithms are as popular as the gradient descent algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: The left-hand side loss surface depicts a well-conditioned model
    where local minima can be reached from all directions. The right-hand side loss
    surface depicts an ill-conditioned model where there are several ways to overshoot
    or never reach the minima.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regularization was an optimization staple for decades prior to the development
    of DNNs. The rationale behind adding a regularizer to a classifier is to avoid
    the overfitting problem, where the classifier fits the training set too closely
    instead of generalizing to the entire data space. Goodfellow et al. [[48](#bib.bib48)]
    defined regularization as “any modification to a learning algorithm that is intended
    to reduce its generalization error but not its training error”. While regularization
    methods such as bagging have been popular for neural networks and other classifiers,
    recently the DNN community has developed novel regularization methods that are
    unique to deep neural networks. In some cases, backpropagation training of fully-connected
    DNNs results in poorer performance than shallow structures because the deeper
    structure is prone to being trapped in local minima and overfitting the training
    data. To improve the generalizability of DNNs, regularization methods have thus
    been adopted during training. Here we review the intuition behind the most frequent
    regularization methods that are currently found in DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.1 Parameter Norm Penalty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A conventional method for avoiding overfitting is to penalize large weights
    by adding a p-norm penalty function to the optimization function of the form $f(x)+$
    p-norm$(x)$, where the p-norm $p$ for weights $w$ is denoted as $||w||_{p}=(\sum_{i}|w_{i}|^{p})^{\frac{1}{p}}$.
    Popular p-norms are the $L_{1}$ and $L_{2}$ norms which have been used by other
    classifiers such as logistic regression and SVMs prior to the introduction of
    DNNs. $L_{1}$ adds a regularization term $\Omega(\theta)=||w||_{1}$ to the objective
    function for weights $w$, while $L_{2}$ adds a regularization term $\Omega(\theta)=||w||_{2}$.
    The difference between the $L_{1}$ and $L_{2}$ norm penalty functions is that
    $L_{1}$ penalizes features more heavily by setting the corresponding edge weights
    to zero compared to $L_{2}$. Therefore, a classifier with the $L_{1}$ norm penalty
    tends to prefer a sparse model. The $L_{2}$ norm penalty is more common than the
    $L_{1}$ norm penalty. However, it is often advised to use the $L_{1}$ norm penalty
    when the amount of training data is small and the number of features is large
    to avoid noisy and less-important features. Because of its sparsity property,
    the $L_{1}$ penalty function is a key component of LASSO feature selection [[75](#bib.bib75)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.2 Dropout
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A powerful method to reduce generalization error is to create an ensemble of
    classifiers. Multiple models are trained separately, then as an ensemble they
    output a combination of the models’ predictions on test points. Some examples
    of ensemble methods included bagging [[76](#bib.bib76)], which trains $k$ models
    on $k$ different folds of random samples with replacement, and boosting [[77](#bib.bib77)],
    which applies a similar process to weighted data. A variety of DNNs use boosting
    to achieve lower generalization error [[45](#bib.bib45)] [[78](#bib.bib78)] [[79](#bib.bib79)].
  prefs: []
  type: TYPE_NORMAL
- en: Dropout [[80](#bib.bib80)] is a popular regularization method for DNNs which
    can be viewed as a computationally-inexpensive application of bagging to deep
    networks. A common way to apply dropout to a DNN is to deactivate a randomly-selected
    50% of the hidden nodes and a randomly-selected 20% of the input nodes for each
    mini-batch of data. The difference between bagging and dropout is that in bagging
    the models are independent of each other, while in dropout each model inherits
    a subset of parameters from the parent deep network.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.3 Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DNNs can generalize better when they have more training data; however, the
    amount of available data is often limited. One way to circumvent this limitation
    is to generate artificial data from the same distribution as the training set.
    Data augmentation has been particularly effective when used in the context of
    classification. The goal of data augmentation is to generate new training samples
    from the original training set $(X,y)$ by transforming the $X$ inputs. Data augmentation
    may include generating noisy data to improve robustness (denoising) or creating
    additional training data for the purpose of regularization (synthetic data generation).
    Dataset augmentation has been adopted for a variety of tasks such as image recognition
    [[81](#bib.bib81)] [[82](#bib.bib82)], speech recognition [[83](#bib.bib83)],
    and activity recognition [[84](#bib.bib84)]. Additionally, GANs [[85](#bib.bib85)]
    [[86](#bib.bib86)] and AEs [[87](#bib.bib87)] [[88](#bib.bib88)], described in
    Sections [2.4.1](#S2.SS4.SSS1 "2.4.1 Generative Adversarial Network ‣ 2.4 Unsupervised
    Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization")
    and [2.4.2](#S2.SS4.SSS2 "2.4.2 Autoencoder ‣ 2.4 Unsupervised Learning ‣ 2 Brief
    Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can
    Learn from Deep Networks: Models, Optimizations, and Regularization"), can be
    employed to generate such new examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Injecting noise into a copy of the input is another data augmentation method.
    Although DNNs are not consistently robust to noise [[89](#bib.bib89)], Poole et
    al. [[90](#bib.bib90)] show that DNNs can benefit from carefully-tuned noise.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning Architectures Outside of Deep Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent research has introduced numerous enhancements to the basic neural network
    architecture that enhance network classification power, particularly for deep
    networks. In this section, we survey non-network classifiers that also make use
    of these advances.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 Feedforward Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A DNN involves multiple layers of operations that are performed sequentially.
    The idea of creating a sequence of operations, each of which manipulates the data
    before passing them to the next operator, may be used to improve many types of
    classifiers. One way to construct a model with a deep feedforward architecture
    is to use stacked generalization [[91](#bib.bib91)] [[92](#bib.bib92)]. Stacked
    generalization classifiers are comprised of multiple layers of classifiers stacked
    on top of each other as found in DNNs. In stacked generalization classifiers,
    one layer generates the next layer’s input by concatenating its own input to its
    output. Stacked generalization classifiers typically only implement forward propagation,
    in contrast to DNNs which propagate information both forward and backward through
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: In general, learning methods that employ stacked generalization can be categorized
    into two strategies. In the first stacked generalization strategy, the new feature
    space for the current layer comes from the concatenation of the predicted output
    of the previous layer with the original feature vector. Here, layers refer not
    to layers of neural network operations, but instead refer to sequences of other
    types of operations. Examples of this strategy include Deep Forest (DF) [[93](#bib.bib93)]
    and the Deep Transfer Additive Kernel Least Square SVM (DTA-LS-SVM) [[94](#bib.bib94)].
    At any given layer, for each instance $x$, DF extends $x$’s previous feature vector
    to include the previous layer’s predicted class value for the instance. The prediction
    represents a distribution over class values, averaged over all trees in the forest.
    Furthermore, Zhou et al. [[93](#bib.bib93)] introduce a method called Multi-Grained
    Scanning for improving the accuracy of DFs. Inspired by CNNs and RNNs where spatial
    relationships between features are critical, Multi-Grained Scanning splits a $D$-dimensional
    feature vector into smaller segments by moving a window by moving a window over
    the features. For example, given 400 features and a window size of 100, the original
    features convert to 301 features of length 100, $\{<1-100>,<2-101>,\ldots,<301-400>\}$,
    where the new instances have the same labels as the original instances. The new
    samples which are described by a subset of the original features might have incorrectly-associated
    labels. At a first glance, it seems these noisy data could hurt the generalization.
    But as Breiman illustrates [[95](#bib.bib95)], perturbing a percentage of the
    training labels can actually help generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Ho [[96](#bib.bib96)] demonstrates that feature sub-sampling can
    enhance the generalization capability for RFs. Zhou et al. [[93](#bib.bib93)]
    tested three different window sizes ($D/4$, $D/8$, and $D/16$), where data from
    each different window size fits a different level of a DF model. Then the newly-learned
    representation from these three layers are fed to a multilayer DF, applying subsampling
    when the transformed features are too long. Multi-Grained Scanning can improve
    the performance of a DF model for continuous data, as Zhou et al. [[93](#bib.bib93)]
    report that accuracy increased by 1.24% on the MNIST [[97](#bib.bib97)] dataset.
    An alternative method, DTA-LS-SVM, applies an Additive Kernel Least Squares SVM
    (AK-LS-SVM) [[98](#bib.bib98)] [[99](#bib.bib99)] at each layer and concatenates
    the original feature vector $x$ with the prediction of the previous level to feed
    to the next layer. In addition, DTA-LS-SVM incorporates a parameter-transfer approach
    between the source (previous-layer learner) and target (next-layer learner) to
    enhance the classification capability of the higher level.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second stacked generalization strategy, the current layer’s new feature
    space comes from the concatenation of predictions from all previous layers with
    the original input feature vector. Examples of this strategy include the Deep
    SVM (D-SVM) [[100](#bib.bib100)] and the Random Recursive SVM (R2-SVM) [[101](#bib.bib101)].
    The D-SVM contains multiple layers of SVMs, where the first layer is trained in
    the normal fashion. Following this step, each successive layer employs the kernel
    activation from the previous layer with the desired labels. The R2-SVM is a multilayer
    SVM model which at each layer transforms the data based on the sigmoid of a projection
    of all previous layers’ outputs. For the data $(X,Y)$ where $X\in R^{D}$ and $Y\in
    R^{C}$, the random projection matrix is $W\in R^{D\times C}$, where each element
    is sampled from $N(0,1)$. The input data for the next layer is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X_{l+1}=\sigma(d+\beta W_{l+1}[o_{1}^{T},o_{2}^{T},...,o_{l}^{T}]^{T}),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\beta$ is a weight parameter that controls the degree with which a data
    sample in $X_{l+1}$ moves from the previous layer, $\sigma(.)$ is the sigmoid
    function, $W_{l+1}$ is the concatenation of $l$ random projection matrices $[W_{l+1,1},W_{l+1,2},...,W_{l+1,l}]$,
    one for each previous layer, and $o$ is the output of each layer. Addition of
    a sigmoid function to the recursive model prevents deterioration to a trivial
    linear model in a similar fashion as MLPs. The purpose of the random projection
    is to push data from different classes in different directions.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note here that stacked generalization can be found in DNNs
    as well as non-network classifiers. Examples of DNNs with stacked generalization
    include Deep Stacking Networks [[102](#bib.bib102)] [[103](#bib.bib103)] and Convex
    Stacking Architectures [[104](#bib.bib104)] [[102](#bib.bib102)]. This is clearly
    one enhancement that benefits all types of classifier strategies. However, there
    is no evidence that stack generalization could add nonlinearity to the model.
  prefs: []
  type: TYPE_NORMAL
- en: DNN classifiers learn a new representation of data at each layer with a goal
    that the newly-learned representation maximally separate the classes. Unsupervised
    DNNs often share this goal. As an example, Deep PCA’s model [[105](#bib.bib105)]
    is made of two layers that each learn a new data representation by applying a
    Zero Components Analysis (ZCA) whitening filter [[106](#bib.bib106)] followed
    by a principal components analysis (PCA) [[107](#bib.bib107)]. The final data
    representation is derived from concatenating the output of the two layers. The
    motivation behind applying a ZCA whitening filter is to force the model to focus
    on higher-order correlations. One motivation for combining output from the first
    and second layers could be to preserve the learned representation from the first
    layer and to prevent feature loss after applying PCA at each layer. Experiments
    demonstrate that Deep PCA exhibits superior performance for face recognition tasks
    compared to standard PCA and a two-layer PCA without a whitening filter. However,
    as empirically confirmed by Damianou et al. [[108](#bib.bib108)], stacking PCAs
    does not necessarily result in an improved representation of the data because
    Deep PCA is unable to learn a nonlinear representation of data at each layer.
    Damianou et al. [[108](#bib.bib108)] fed a Gaussian to a Deep PCA and observed
    that the model learned just a lower rank of the input Gaussian at each layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As pointed out earlier in this survey, the invention of the deep belief net
    (DBN) [[45](#bib.bib45)] drew the attention of researchers to developing deep
    models. A DBN can be viewed as a stacked restricted Boltzmann machine (RBM), where
    each layer is trained separately and alternates functionality between hidden and
    input units. In this model, features learned at hidden layers then represent inputs
    to the next layer. A RBM is a generative model that contains a single hidden layer.
    Unlike the Boltzmann machine, hidden units in the restricted model are not connected
    to each other and contain undirected, symmetrical connections from a layer of
    visible units (inputs). All of the units in each layer of a RBM are updated in
    parallel by inputting the current state of the unit to the other layer. This updating
    process repeats until the system is sampling from an equilibrium distribution.
    The RBM learning rule is shown in Equation [2](#S3.E2 "In 3.1.1 Feedforward Learning
    ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures Outside of Deep Neural
    Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial\log P(v)}{\partial W_{ij}}\approx<v_{i}h_{j}>_{data}-<v_{i}h_{j}>_{reconstruction}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: In this equation, $W_{ij}$ represents the weight vector between a visible unit
    $v_{i}$ and a hidden unit $h_{j}$, and $<.>$ is the average value over all training
    samples. Since the introduction of DBNs, many other different variations of Deep
    RBMs have been proposed such as temporal RBMs [[109](#bib.bib109)], gated RBMs
    [[110](#bib.bib110)], and cardinality RBMs [[111](#bib.bib111)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Another novel form of a deep belief net is a deep Gaussian process (DGP) model
    [[108](#bib.bib108)]. DGP is a deep directed graph where multiple layers of Gaussian
    processes map the original features to a series of latent spaces. DGPs offer a
    more general form of Gaussian Processes (GPs) [[112](#bib.bib112)] where a one-layer
    DGP consists of a single GP, $f$. In a multilayer DGP, each GP, $f_{l}$, maps
    data from one latent space to the next. As shown in Equation [3](#S3.E3 "In 3.1.1
    Feedforward Learning ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization"), each data point
    $Y$ is generated from the corresponding function $f_{l}$ with $\epsilon$ Guassian
    noise applied to data $X_{l}$ that is obtained from a previous layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Y=f_{l}(X_{l})+\epsilon_{l},\indent\epsilon_{l}\sim\mathcal{N}(0,\sigma^{2}_{l}I)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure [9](#S3.F9 "Figure 9 ‣ 3.1.1 Feedforward Learning ‣ 3.1 Supervised Learning
    ‣ 3 Deep Learning Architectures Outside of Deep Neural Networks ‣ A Survey of
    Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization") illustrates a DGP expressed as a series of Gaussian processes
    mapping data from one latent space to the next. Functions $f_{l}$ are drawn from
    a Gaussian process, i.e. $f(x)\sim\mathcal{GP}(0,k(x,x^{\prime}))$. In this setting,
    the covariance function $k$ defines the properties of the mapping function. DGP
    can be utilized for both supervised and unsupervised learning. In the supervised
    setting, the top hidden layer is observed, whereas in the unsupervised setting,
    the top hidden layer is set to a unit Gaussian as a fairly uninformative prior.
    DGP is a powerful non-parametric model but it has only been tested on small datasets.
    Also, we note that researchers have developed deep Gaussian process models with
    alternative architectures such as recurrent Gaussian processes [[113](#bib.bib113)],
    convolutional Gaussian processes [[114](#bib.bib114)] and variational auto-encoded
    deep Gaussian processes [[115](#bib.bib115)]. There exists a vast amount of literature
    on this topic that provides additional insights on deep Gaussian processes [[116](#bib.bib116)]
    [[117](#bib.bib117)] [[118](#bib.bib118)].'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.F9.pic1" class="ltx_picture ltx_centering" height="90.85" overflow="visible"
    version="1.1" width="349.6"><g transform="translate(0,90.85) matrix(1 0 0 -1 0
    0) translate(98.62,0) translate(0,2.26)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -94.01 69.09)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">X</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -6.07 70.36)" fill="#000000" stroke="#000000"><foreignobject
    width="12.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 72.67 70.36)" fill="#000000" stroke="#000000"><foreignobject
    width="12.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 151.41 70.36)" fill="#000000" stroke="#000000"><foreignobject
    width="12.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 230.67 69.09)" fill="#000000" stroke="#000000"><foreignobject
    width="11.11" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Y$</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: A deep Gaussian process with two hidden layers.'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, non-network classifiers have been designed that contain multiple
    layers of operations, similar to a DNN. We observe that a common strategy for
    creating a deep non-network model is to add the prediction of the previous layer
    or layers to the original input feature. Likewise, novel methods can be applied
    to learn a new representation of data at each layer. We discuss these methods
    next.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Siamese Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As discussed in Section [2.3.4](#S2.SS3.SSS4 "2.3.4 Siamese Neural Network
    ‣ 2.3 Supervised Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"), a SNN represents a powerful method for similarity learning.
    However, one problem with SNNs is overfitting when there is a small number of
    training examples. The Siamese Deep Forest (SDF) [[119](#bib.bib119)] is a method
    based on DF which offers an alternative to a standard SNN. The SDF, unlike the
    SNN, uses only one DF. The first step in training a SDF is to modify the training
    examples. The training set consists of the concatenation of each pair of samples
    in the original set. If sample points $x_{i}$ and $x_{j}$ are semantically similar,
    the corresponding class label is set to zero; otherwise, the class label is set
    to one. The difference between the SDF and the DF in training is that the Siamese
    Deep Forest concatenates the original feature vector with a weighted sum of the
    tree class probabilities. Training of SDF is similar to DF; the primary difference
    is that SDF learns the class probability weights $w$ for each forest separately
    at each layer. Learning the weights for each forest can be accomplished by minimizing
    the function in Equation [4](#S3.E4 "In 3.1.2 Siamese Model ‣ 3.1 Supervised Learning
    ‣ 3 Deep Learning Architectures Outside of Deep Neural Networks ‣ A Survey of
    Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min\limits_{w}J_{q}(w)=\min\limits_{w}\sum_{i,j}l(x_{i},x_{j},y_{ij},w)+\lambda
    R(w)$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $w$ represents a concatenation of vectors $w^{k}$, $k=1,...,M$, $q$ is
    the SDF layer, $R(w)$ is a regularization term, and $\lambda$ is a hyper-parameter
    to control regularization. Detailed instructions on minimizing Equation [4](#S3.E4
    "In 3.1.2 Siamese Model ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization") are found in the
    literature [[119](#bib.bib119)]. The results of SDF experiments indicate that
    the SDF can achieve better classification accuracy than DF for small datasets.
    In general, all non-network models that learn data representations can take advantage
    of the Siamese architecture like SDF.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1 Generative Adversarial Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A common element found in GANs is inclusion of a FC layer in the discriminator.
    One issue with the FC layer is that it cannot deal with the ill-condition in which
    local minima are not surrounded by spherical wells as shown in Figure [8](#S2.F8
    "Figure 8 ‣ 2.5 Optimization for Training Deep Neural Networks ‣ 2 Brief Overview
    of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from
    Deep Networks: Models, Optimizations, and Regularization"). The Generative Adversarial
    Forest (GAF) [[120](#bib.bib120)] replaces the FC layer of the discriminator with
    a deep neural decision forest (DNDF), which is discussed in Section [4](#S4 "4
    Deep Learning Optimization Outside of Deep Neural Networks ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization").
    GAF and DNDF are distinguished based on how leaf node values are learned. Instead
    of learning leaf node values iteratively, as DNDF does, GAF learns them in parallel
    across the ensemble members. The strong discriminatory power of the decision forest
    is the reason the authors recommend this method in lieu of the fully-connected
    discriminator layer.'
  prefs: []
  type: TYPE_NORMAL
- en: In this previous work, the discriminator is replaced by an unconventional model.
    We hypothesize that replacing the discriminator with other classifiers such as
    Random Forest, SVM, of K nearest neighbor based on the data could result in a
    diverse GAN strategies, each of which may offer benefits for alternative learning
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As we discussed in Section [2.4.2](#S2.SS4.SSS2 "2.4.2 Autoencoder ‣ 2.4 Unsupervised
    Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization"),
    AEs offer strategies for dimensionality reduction and data reconstruction from
    compressed information. The autoencoding methodology can be found in neural networks,
    non-networks, and hybrid methods. As an example, the multilayer SVM (ML-SVM) autoencoder
    is a variation of ML-SVM with the same number of output nodes as input features
    and a single hidden layer that consists of fewer nodes than the input features.
    ML-SVM is a model with the same structure as a MLP. The distinction here is that
    the network contains SVM models as its nodes. A review of ML-SVM is discussed
    in Section [4](#S4 "4 Deep Learning Optimization Outside of Deep Neural Networks
    ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization"). The outputs of hidden nodes are fed as input
    to each SVM output node $c$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $g_{c}(f(X&#124;\theta))=\sum_{i=1}^{l}(\alpha_{i}^{c*}-\alpha_{i}^{c})K_{o}(f(x_{i}&#124;\theta),f(x&#124;\theta))+b_{c},$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{i}^{c*}$ and $\alpha_{i}^{c}$ are the support vector coefficients,
    $K_{o}$ is the kernel function, and $b_{c}$ is their bias. The error backpropagates
    through the network to update the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another exciting emerging research area is the combination of Kalman filters
    with deep networks. A Kalman filter is a well-known algorithm that estimates the
    optimal state of a system from a series of noisy observations. The classical Kalman
    filter [[121](#bib.bib121)] is a linear dynamical system and therefore is unable
    to model complex phenomena. For this reason, researchers developed nonlinear versions
    of Kalman filters. In a seminal contribution, Krishnan et al. [[122](#bib.bib122)]
    introduced a model that combines a variational autoencoder with Kalman filters
    for counterfactual inference of patient information. In a standard autoencoder,
    the model learns a latent space that represents the original data minus extraneous
    information or “signal noise”. In contrast, a variational autoencoder (VAE) [[68](#bib.bib68)]
    adds a constraint to the encoder that it learn a Gaussian distribution of the
    original input data. Therefore, a VAE is able to generate a latent vector by sampling
    from the learned Gaussian distribution. Deep Kalman filters (DKF) learn a generative
    model from observed sequences $\vec{x}=(x_{1},\cdots,x_{T})$ and actions $\vec{u}=(u_{1},\cdots
    u_{T-1})$, with a corresponding latent space $\vec{z}=(z_{1},\cdots,z_{T})$, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.E6.m1.61" class="ltx_Math" alttext="\begin{split}&amp;z_{1}\sim\mathcal{N}(\mu_{0},\Sigma_{0})\\
    &amp;z_{t}\sim\mathcal{N}(G_{\alpha}(z_{t-1},u_{t-1},\Delta_{t}),S_{\beta}(z_{t-1},y_{t-1},\Delta_{t}))\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;x_{t}\sim\Pi(F_{k}(z_{t})),\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics id="S3.E6.m1.61a"><mtable columnspacing="0pt"
    displaystyle="true" rowspacing="0pt" id="S3.E6.m1.61.61.6"><mtr id="S3.E6.m1.61.61.6a"><mtd
    class="ltx_align_left" columnalign="left" id="S3.E6.m1.61.61.6c"><mrow id="S3.E6.m1.58.58.3.57.13.13"><msub
    id="S3.E6.m1.58.58.3.57.13.13.14"><mi id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml">z</mi><mn
    id="S3.E6.m1.2.2.2.2.2.2.1" xref="S3.E6.m1.2.2.2.2.2.2.1.cmml">1</mn></msub><mo
    id="S3.E6.m1.3.3.3.3.3.3" xref="S3.E6.m1.3.3.3.3.3.3.cmml">∼</mo><mrow id="S3.E6.m1.58.58.3.57.13.13.13"><mi
    class="ltx_font_mathcaligraphic" id="S3.E6.m1.4.4.4.4.4.4" xref="S3.E6.m1.4.4.4.4.4.4.cmml">𝒩</mi><mo
    lspace="0em" rspace="0em" id="S3.E6.m1.58.58.3.57.13.13.13.3" xref="S3.E6.m1.56.56.1.1.1.cmml">​</mo><mrow
    id="S3.E6.m1.58.58.3.57.13.13.13.2.2"><mo stretchy="false" id="S3.E6.m1.5.5.5.5.5.5"
    xref="S3.E6.m1.56.56.1.1.1.cmml">(</mo><msub id="S3.E6.m1.57.57.2.56.12.12.12.1.1.1"><mi
    id="S3.E6.m1.6.6.6.6.6.6" xref="S3.E6.m1.6.6.6.6.6.6.cmml">μ</mi><mn id="S3.E6.m1.7.7.7.7.7.7.1"
    xref="S3.E6.m1.7.7.7.7.7.7.1.cmml">0</mn></msub><mo id="S3.E6.m1.8.8.8.8.8.8"
    xref="S3.E6.m1.56.56.1.1.1.cmml">,</mo><msub id="S3.E6.m1.58.58.3.57.13.13.13.2.2.2"><mi
    mathvariant="normal" id="S3.E6.m1.9.9.9.9.9.9" xref="S3.E6.m1.9.9.9.9.9.9.cmml">Σ</mi><mn
    id="S3.E6.m1.10.10.10.10.10.10.1" xref="S3.E6.m1.10.10.10.10.10.10.1.cmml">0</mn></msub><mo
    stretchy="false" id="S3.E6.m1.11.11.11.11.11.11" xref="S3.E6.m1.56.56.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    id="S3.E6.m1.61.61.6d"><mtd class="ltx_align_left" columnalign="left" id="S3.E6.m1.61.61.6f"><mrow
    id="S3.E6.m1.60.60.5.59.33.33"><msub id="S3.E6.m1.60.60.5.59.33.33.34"><mi id="S3.E6.m1.12.12.12.1.1.1"
    xref="S3.E6.m1.12.12.12.1.1.1.cmml">z</mi><mi id="S3.E6.m1.13.13.13.2.2.2.1" xref="S3.E6.m1.13.13.13.2.2.2.1.cmml">t</mi></msub><mo
    id="S3.E6.m1.14.14.14.3.3.3" xref="S3.E6.m1.14.14.14.3.3.3.cmml">∼</mo><mrow id="S3.E6.m1.60.60.5.59.33.33.33"><mi
    class="ltx_font_mathcaligraphic" id="S3.E6.m1.15.15.15.4.4.4" xref="S3.E6.m1.15.15.15.4.4.4.cmml">𝒩</mi><mo
    lspace="0em" rspace="0em" id="S3.E6.m1.60.60.5.59.33.33.33.3" xref="S3.E6.m1.56.56.1.1.1.cmml">​</mo><mrow
    id="S3.E6.m1.60.60.5.59.33.33.33.2.2"><mo stretchy="false" id="S3.E6.m1.16.16.16.5.5.5"
    xref="S3.E6.m1.56.56.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.59.59.4.58.32.32.32.1.1.1"><msub
    id="S3.E6.m1.59.59.4.58.32.32.32.1.1.1.5"><mi id="S3.E6.m1.17.17.17.6.6.6" xref="S3.E6.m1.17.17.17.6.6.6.cmml">G</mi><mi
    id="S3.E6.m1.18.18.18.7.7.7.1" xref="S3.E6.m1.18.18.18.7.7.7.1.cmml">α</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E6.m1.59.59.4.58.32.32.32.1.1.1.4" xref="S3.E6.m1.56.56.1.1.1.cmml">​</mo><mrow
    id="S3.E6.m1.59.59.4.58.32.32.32.1.1.1.3.3"><mo stretchy="false" id="S3.E6.m1.19.19.19.8.8.8"
    xref="S3.E6.m1.56.56.1.1.1.cmml">(</mo><msub id="S3.E6.m1.59.59.4.58.32.32.32.1.1.1.1.1.1"><mi
    id="S3.E6.m1.20.20.20.9.9.9" xref="S3.E6.m1.20.20.20.9.9.9.cmml">z</mi><mrow id="S3.E6.m1.21.21.21.10.10.10.1"
    xref="S3.E6.m1.21.21.21.10.10.10.1.cmml"><mi id="S3.E6.m1.21.21.21.10.10.10.1.2"
    xref="S3.E6.m1.21.21.21.10.10.10.1.2.cmml">t</mi><mo id="S3.E6.m1.21.21.21.10.10.10.1.1"
    xref="S3.E6.m1.21.21.21.10.10.10.1.1.cmml">−</mo><mn id="S3.E6.m1.21.21.21.10.10.10.1.3"
    xref="S3.E6.m1.21.21.21.10.10.10.1.3.cmml">1</mn></mrow></msub><mo id="S3.E6.m1.22.22.22.11.11.11"
    xref="S3.E6.m1.56.56.1.1.1.cmml">,</mo><msub id="S3.E6.m1.59.59.4.58.32.32.32.1.1.1.2.2.2"><mi
    id="S3.E6.m1.23.23.23.12.12.12" xref="S3.E6.m1.23.23.23.12.12.12.cmml">u</mi><mrow
    id="S3.E6.m1.24.24.24.13.13.13.1" xref="S3.E6.m1.24.24.24.13.13.13.1.cmml"><mi
    id="S3.E6.m1.24.24.24.13.13.13.1.2" xref="S3.E6.m1.24.24.24.13.13.13.1.2.cmml">t</mi><mo
    id="S3.E6.m1.24.24.24.13.13.13.1.1" xref="S3.E6.m1.24.24.24.13.13.13.1.1.cmml">−</mo><mn
    id="S3.E6.m1.24.24.24.13.13.13.1.3" xref="S3.E6.m1.24.24.24.13.13.13.1.3.cmml">1</mn></mrow></msub><mo
    id="S3.E6.m1.25.25.25.14.14.14" xref="S3.E6.m1.56.56.1.1.1.cmml">,</mo><msub id="S3.E6.m1.59.59.4.58.32.32.32.1.1.1.3.3.3"><mi
    mathvariant="normal" id="S3.E6.m1.26.26.26.15.15.15" xref="S3.E6.m1.26.26.26.15.15.15.cmml">Δ</mi><mi
    id="S3.E6.m1.27.27.27.16.16.16.1" xref="S3.E6.m1.27.27.27.16.16.16.1.cmml">t</mi></msub><mo
    stretchy="false" id="S3.E6.m1.28.28.28.17.17.17" xref="S3.E6.m1.56.56.1.1.1.cmml">)</mo></mrow></mrow><mo
    id="S3.E6.m1.29.29.29.18.18.18" xref="S3.E6.m1.56.56.1.1.1.cmml">,</mo><mrow id="S3.E6.m1.60.60.5.59.33.33.33.2.2.2"><msub
    id="S3.E6.m1.60.60.5.59.33.33.33.2.2.2.5"><mi id="S3.E6.m1.30.30.30.19.19.19"
    xref="S3.E6.m1.30.30.30.19.19.19.cmml">S</mi><mi id="S3.E6.m1.31.31.31.20.20.20.1"
    xref="S3.E6.m1.31.31.31.20.20.20.1.cmml">β</mi></msub><mo lspace="0em" rspace="0em"
    id="S3.E6.m1.60.60.5.59.33.33.33.2.2.2.4" xref="S3.E6.m1.56.56.1.1.1.cmml">​</mo><mrow
    id="S3.E6.m1.60.60.5.59.33.33.33.2.2.2.3.3"><mo stretchy="false" id="S3.E6.m1.32.32.32.21.21.21"
    xref="S3.E6.m1.56.56.1.1.1.cmml">(</mo><msub id="S3.E6.m1.60.60.5.59.33.33.33.2.2.2.1.1.1"><mi
    id="S3.E6.m1.33.33.33.22.22.22" xref="S3.E6.m1.33.33.33.22.22.22.cmml">z</mi><mrow
    id="S3.E6.m1.34.34.34.23.23.23.1" xref="S3.E6.m1.34.34.34.23.23.23.1.cmml"><mi
    id="S3.E6.m1.34.34.34.23.23.23.1.2" xref="S3.E6.m1.34.34.34.23.23.23.1.2.cmml">t</mi><mo
    id="S3.E6.m1.34.34.34.23.23.23.1.1" xref="S3.E6.m1.34.34.34.23.23.23.1.1.cmml">−</mo><mn
    id="S3.E6.m1.34.34.34.23.23.23.1.3" xref="S3.E6.m1.34.34.34.23.23.23.1.3.cmml">1</mn></mrow></msub><mo
    id="S3.E6.m1.35.35.35.24.24.24" xref="S3.E6.m1.56.56.1.1.1.cmml">,</mo><msub id="S3.E6.m1.60.60.5.59.33.33.33.2.2.2.2.2.2"><mi
    id="S3.E6.m1.36.36.36.25.25.25" xref="S3.E6.m1.36.36.36.25.25.25.cmml">y</mi><mrow
    id="S3.E6.m1.37.37.37.26.26.26.1" xref="S3.E6.m1.37.37.37.26.26.26.1.cmml"><mi
    id="S3.E6.m1.37.37.37.26.26.26.1.2" xref="S3.E6.m1.37.37.37.26.26.26.1.2.cmml">t</mi><mo
    id="S3.E6.m1.37.37.37.26.26.26.1.1" xref="S3.E6.m1.37.37.37.26.26.26.1.1.cmml">−</mo><mn
    id="S3.E6.m1.37.37.37.26.26.26.1.3" xref="S3.E6.m1.37.37.37.26.26.26.1.3.cmml">1</mn></mrow></msub><mo
    id="S3.E6.m1.38.38.38.27.27.27" xref="S3.E6.m1.56.56.1.1.1.cmml">,</mo><msub id="S3.E6.m1.60.60.5.59.33.33.33.2.2.2.3.3.3"><mi
    mathvariant="normal" id="S3.E6.m1.39.39.39.28.28.28" xref="S3.E6.m1.39.39.39.28.28.28.cmml">Δ</mi><mi
    id="S3.E6.m1.40.40.40.29.29.29.1" xref="S3.E6.m1.40.40.40.29.29.29.1.cmml">t</mi></msub><mo
    stretchy="false" id="S3.E6.m1.41.41.41.30.30.30" xref="S3.E6.m1.56.56.1.1.1.cmml">)</mo></mrow></mrow><mo
    stretchy="false" id="S3.E6.m1.42.42.42.31.31.31" xref="S3.E6.m1.56.56.1.1.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    id="S3.E6.m1.61.61.6g"><mtd class="ltx_align_left" columnalign="left" id="S3.E6.m1.61.61.6i"><mrow
    id="S3.E6.m1.61.61.6.60.14.14.14"><mrow id="S3.E6.m1.61.61.6.60.14.14.14.1"><msub
    id="S3.E6.m1.61.61.6.60.14.14.14.1.2"><mi id="S3.E6.m1.43.43.43.1.1.1" xref="S3.E6.m1.43.43.43.1.1.1.cmml">x</mi><mi
    id="S3.E6.m1.44.44.44.2.2.2.1" xref="S3.E6.m1.44.44.44.2.2.2.1.cmml">t</mi></msub><mo
    id="S3.E6.m1.45.45.45.3.3.3" xref="S3.E6.m1.45.45.45.3.3.3.cmml">∼</mo><mrow id="S3.E6.m1.61.61.6.60.14.14.14.1.1"><mi
    mathvariant="normal" id="S3.E6.m1.46.46.46.4.4.4" xref="S3.E6.m1.46.46.46.4.4.4.cmml">Π</mi><mo
    lspace="0em" rspace="0em" id="S3.E6.m1.61.61.6.60.14.14.14.1.1.2" xref="S3.E6.m1.56.56.1.1.1.cmml">​</mo><mrow
    id="S3.E6.m1.61.61.6.60.14.14.14.1.1.1.1"><mo stretchy="false" id="S3.E6.m1.47.47.47.5.5.5"
    xref="S3.E6.m1.56.56.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.61.61.6.60.14.14.14.1.1.1.1.1"><msub
    id="S3.E6.m1.61.61.6.60.14.14.14.1.1.1.1.1.3"><mi id="S3.E6.m1.48.48.48.6.6.6"
    xref="S3.E6.m1.48.48.48.6.6.6.cmml">F</mi><mi id="S3.E6.m1.49.49.49.7.7.7.1" xref="S3.E6.m1.49.49.49.7.7.7.1.cmml">k</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E6.m1.61.61.6.60.14.14.14.1.1.1.1.1.2" xref="S3.E6.m1.56.56.1.1.1.cmml">​</mo><mrow
    id="S3.E6.m1.61.61.6.60.14.14.14.1.1.1.1.1.1.1"><mo stretchy="false" id="S3.E6.m1.50.50.50.8.8.8"
    xref="S3.E6.m1.56.56.1.1.1.cmml">(</mo><msub id="S3.E6.m1.61.61.6.60.14.14.14.1.1.1.1.1.1.1.1"><mi
    id="S3.E6.m1.51.51.51.9.9.9" xref="S3.E6.m1.51.51.51.9.9.9.cmml">z</mi><mi id="S3.E6.m1.52.52.52.10.10.10.1"
    xref="S3.E6.m1.52.52.52.10.10.10.1.cmml">t</mi></msub><mo stretchy="false" id="S3.E6.m1.53.53.53.11.11.11"
    xref="S3.E6.m1.56.56.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E6.m1.54.54.54.12.12.12"
    xref="S3.E6.m1.56.56.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6.m1.55.55.55.13.13.13"
    xref="S3.E6.m1.56.56.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S3.E6.m1.61b"><apply id="S3.E6.m1.56.56.1.1.1.cmml"
    xref="S3.E6.m1.61.61.6b"><apply id="S3.E6.m1.56.56.1.1.1b.cmml" xref="S3.E6.m1.61.61.6b"><csymbol
    cd="latexml" id="S3.E6.m1.3.3.3.3.3.3.cmml" xref="S3.E6.m1.3.3.3.3.3.3">similar-to</csymbol><apply
    id="S3.E6.m1.56.56.1.1.1.7.cmml" xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous"
    id="S3.E6.m1.56.56.1.1.1.7.1.cmml" xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci
    id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1">𝑧</ci><cn type="integer"
    id="S3.E6.m1.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.1">1</cn></apply><apply
    id="S3.E6.m1.56.56.1.1.1.2.cmml" xref="S3.E6.m1.61.61.6b"><ci id="S3.E6.m1.4.4.4.4.4.4.cmml"
    xref="S3.E6.m1.4.4.4.4.4.4">𝒩</ci><interval closure="open" id="S3.E6.m1.56.56.1.1.1.2.2.3.cmml"
    xref="S3.E6.m1.61.61.6b"><apply id="S3.E6.m1.56.56.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.61.61.6b"><csymbol
    cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci
    id="S3.E6.m1.6.6.6.6.6.6.cmml" xref="S3.E6.m1.6.6.6.6.6.6">𝜇</ci><cn type="integer"
    id="S3.E6.m1.7.7.7.7.7.7.1.cmml" xref="S3.E6.m1.7.7.7.7.7.7.1">0</cn></apply><apply
    id="S3.E6.m1.56.56.1.1.1.2.2.2.2.cmml" xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous"
    id="S3.E6.m1.56.56.1.1.1.2.2.2.2.1.cmml" xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci
    id="S3.E6.m1.9.9.9.9.9.9.cmml" xref="S3.E6.m1.9.9.9.9.9.9">Σ</ci><cn type="integer"
    id="S3.E6.m1.10.10.10.10.10.10.1.cmml" xref="S3.E6.m1.10.10.10.10.10.10.1">0</cn></apply></interval><apply
    id="S3.E6.m1.56.56.1.1.1.2.5.cmml" xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous"
    id="S3.E6.m1.56.56.1.1.1.2.5.1.cmml" xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci
    id="S3.E6.m1.12.12.12.1.1.1.cmml" xref="S3.E6.m1.12.12.12.1.1.1">𝑧</ci><ci id="S3.E6.m1.13.13.13.2.2.2.1.cmml"
    xref="S3.E6.m1.13.13.13.2.2.2.1">𝑡</ci></apply></apply></apply><apply id="S3.E6.m1.56.56.1.1.1c.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="latexml" id="S3.E6.m1.14.14.14.3.3.3.cmml"
    xref="S3.E6.m1.14.14.14.3.3.3">similar-to</csymbol><apply id="S3.E6.m1.56.56.1.1.1.4.cmml"
    xref="S3.E6.m1.61.61.6b"><ci id="S3.E6.m1.15.15.15.4.4.4.cmml" xref="S3.E6.m1.15.15.15.4.4.4">𝒩</ci><interval
    closure="open" id="S3.E6.m1.56.56.1.1.1.4.2.3.cmml" xref="S3.E6.m1.61.61.6b"><apply
    id="S3.E6.m1.56.56.1.1.1.3.1.1.1.cmml" xref="S3.E6.m1.61.61.6b"><apply id="S3.E6.m1.56.56.1.1.1.3.1.1.1.5.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.3.1.1.1.5.1.cmml"
    xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci id="S3.E6.m1.17.17.17.6.6.6.cmml"
    xref="S3.E6.m1.17.17.17.6.6.6">𝐺</ci><ci id="S3.E6.m1.18.18.18.7.7.7.1.cmml" xref="S3.E6.m1.18.18.18.7.7.7.1">𝛼</ci></apply><vector
    id="S3.E6.m1.56.56.1.1.1.3.1.1.1.3.4.cmml" xref="S3.E6.m1.61.61.6b"><apply id="S3.E6.m1.56.56.1.1.1.3.1.1.1.1.1.1.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.3.1.1.1.1.1.1.1.cmml"
    xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci id="S3.E6.m1.20.20.20.9.9.9.cmml"
    xref="S3.E6.m1.20.20.20.9.9.9">𝑧</ci><apply id="S3.E6.m1.21.21.21.10.10.10.1.cmml"
    xref="S3.E6.m1.21.21.21.10.10.10.1"><ci id="S3.E6.m1.21.21.21.10.10.10.1.2.cmml"
    xref="S3.E6.m1.21.21.21.10.10.10.1.2">𝑡</ci><cn type="integer" id="S3.E6.m1.21.21.21.10.10.10.1.3.cmml"
    xref="S3.E6.m1.21.21.21.10.10.10.1.3">1</cn></apply></apply><apply id="S3.E6.m1.56.56.1.1.1.3.1.1.1.2.2.2.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.3.1.1.1.2.2.2.1.cmml"
    xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci id="S3.E6.m1.23.23.23.12.12.12.cmml"
    xref="S3.E6.m1.23.23.23.12.12.12">𝑢</ci><apply id="S3.E6.m1.24.24.24.13.13.13.1.cmml"
    xref="S3.E6.m1.24.24.24.13.13.13.1"><ci id="S3.E6.m1.24.24.24.13.13.13.1.2.cmml"
    xref="S3.E6.m1.24.24.24.13.13.13.1.2">𝑡</ci><cn type="integer" id="S3.E6.m1.24.24.24.13.13.13.1.3.cmml"
    xref="S3.E6.m1.24.24.24.13.13.13.1.3">1</cn></apply></apply><apply id="S3.E6.m1.56.56.1.1.1.3.1.1.1.3.3.3.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.3.1.1.1.3.3.3.1.cmml"
    xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci id="S3.E6.m1.26.26.26.15.15.15.cmml"
    xref="S3.E6.m1.26.26.26.15.15.15">Δ</ci><ci id="S3.E6.m1.27.27.27.16.16.16.1.cmml"
    xref="S3.E6.m1.27.27.27.16.16.16.1">𝑡</ci></apply></vector></apply><apply id="S3.E6.m1.56.56.1.1.1.4.2.2.2.cmml"
    xref="S3.E6.m1.61.61.6b"><apply id="S3.E6.m1.56.56.1.1.1.4.2.2.2.5.cmml" xref="S3.E6.m1.61.61.6b"><csymbol
    cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.4.2.2.2.5.1.cmml" xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci
    id="S3.E6.m1.30.30.30.19.19.19.cmml" xref="S3.E6.m1.30.30.30.19.19.19">𝑆</ci><ci
    id="S3.E6.m1.31.31.31.20.20.20.1.cmml" xref="S3.E6.m1.31.31.31.20.20.20.1">𝛽</ci></apply><vector
    id="S3.E6.m1.56.56.1.1.1.4.2.2.2.3.4.cmml" xref="S3.E6.m1.61.61.6b"><apply id="S3.E6.m1.56.56.1.1.1.4.2.2.2.1.1.1.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.4.2.2.2.1.1.1.1.cmml"
    xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci id="S3.E6.m1.33.33.33.22.22.22.cmml"
    xref="S3.E6.m1.33.33.33.22.22.22">𝑧</ci><apply id="S3.E6.m1.34.34.34.23.23.23.1.cmml"
    xref="S3.E6.m1.34.34.34.23.23.23.1"><ci id="S3.E6.m1.34.34.34.23.23.23.1.2.cmml"
    xref="S3.E6.m1.34.34.34.23.23.23.1.2">𝑡</ci><cn type="integer" id="S3.E6.m1.34.34.34.23.23.23.1.3.cmml"
    xref="S3.E6.m1.34.34.34.23.23.23.1.3">1</cn></apply></apply><apply id="S3.E6.m1.56.56.1.1.1.4.2.2.2.2.2.2.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.4.2.2.2.2.2.2.1.cmml"
    xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci id="S3.E6.m1.36.36.36.25.25.25.cmml"
    xref="S3.E6.m1.36.36.36.25.25.25">𝑦</ci><apply id="S3.E6.m1.37.37.37.26.26.26.1.cmml"
    xref="S3.E6.m1.37.37.37.26.26.26.1"><ci id="S3.E6.m1.37.37.37.26.26.26.1.2.cmml"
    xref="S3.E6.m1.37.37.37.26.26.26.1.2">𝑡</ci><cn type="integer" id="S3.E6.m1.37.37.37.26.26.26.1.3.cmml"
    xref="S3.E6.m1.37.37.37.26.26.26.1.3">1</cn></apply></apply><apply id="S3.E6.m1.56.56.1.1.1.4.2.2.2.3.3.3.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.4.2.2.2.3.3.3.1.cmml"
    xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci id="S3.E6.m1.39.39.39.28.28.28.cmml"
    xref="S3.E6.m1.39.39.39.28.28.28">Δ</ci><ci id="S3.E6.m1.40.40.40.29.29.29.1.cmml"
    xref="S3.E6.m1.40.40.40.29.29.29.1">𝑡</ci></apply></vector></apply></interval><apply
    id="S3.E6.m1.56.56.1.1.1.4.5.cmml" xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous"
    id="S3.E6.m1.56.56.1.1.1.4.5.1.cmml" xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci
    id="S3.E6.m1.43.43.43.1.1.1.cmml" xref="S3.E6.m1.43.43.43.1.1.1">𝑥</ci><ci id="S3.E6.m1.44.44.44.2.2.2.1.cmml"
    xref="S3.E6.m1.44.44.44.2.2.2.1">𝑡</ci></apply></apply></apply><apply id="S3.E6.m1.56.56.1.1.1e.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="latexml" id="S3.E6.m1.45.45.45.3.3.3.cmml"
    xref="S3.E6.m1.45.45.45.3.3.3">similar-to</csymbol><apply id="S3.E6.m1.56.56.1.1.1.5.cmml"
    xref="S3.E6.m1.61.61.6b"><ci id="S3.E6.m1.46.46.46.4.4.4.cmml" xref="S3.E6.m1.46.46.46.4.4.4">Π</ci><apply
    id="S3.E6.m1.56.56.1.1.1.5.1.1.1.cmml" xref="S3.E6.m1.61.61.6b"><apply id="S3.E6.m1.56.56.1.1.1.5.1.1.1.3.cmml"
    xref="S3.E6.m1.61.61.6b"><csymbol cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.5.1.1.1.3.1.cmml"
    xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci id="S3.E6.m1.48.48.48.6.6.6.cmml"
    xref="S3.E6.m1.48.48.48.6.6.6">𝐹</ci><ci id="S3.E6.m1.49.49.49.7.7.7.1.cmml" xref="S3.E6.m1.49.49.49.7.7.7.1">𝑘</ci></apply><apply
    id="S3.E6.m1.56.56.1.1.1.5.1.1.1.1.1.1.cmml" xref="S3.E6.m1.61.61.6b"><csymbol
    cd="ambiguous" id="S3.E6.m1.56.56.1.1.1.5.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.61.61.6b">subscript</csymbol><ci
    id="S3.E6.m1.51.51.51.9.9.9.cmml" xref="S3.E6.m1.51.51.51.9.9.9">𝑧</ci><ci id="S3.E6.m1.52.52.52.10.10.10.1.cmml"
    xref="S3.E6.m1.52.52.52.10.10.10.1">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E6.m1.61c">\begin{split}&z_{1}\sim\mathcal{N}(\mu_{0},\Sigma_{0})\\
    &z_{t}\sim\mathcal{N}(G_{\alpha}(z_{t-1},u_{t-1},\Delta_{t}),S_{\beta}(z_{t-1},y_{t-1},\Delta_{t}))\\
    &x_{t}\sim\Pi(F_{k}(z_{t})),\\ \end{split}</annotation></semantics></math> |  |
    (6) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\mu_{0}=0$ and $\Sigma_{0}=I_{d}$, $\Delta_{t}$ represents the difference
    between times $t$ and $t-1$, and $\Pi$ represents a distribution (e.g., Bernoulli
    for binary data) over observation $x_{t}$. The functions $G_{\alpha}$, $S_{\beta}$,
    $F_{k}$ are parameterized by a neural net. As a result, the autoencoder will learn
    $\theta=\{\alpha,\beta,k\}$ parameters. Additionally, Shashua et al. [[123](#bib.bib123)]
    introduced deep Q-learning with Kalman filters and Lu et al. [[124](#bib.bib124)]
    presented a deep Kalman filter model for video compression.
  prefs: []
  type: TYPE_NORMAL
- en: As we highlighted in this section, non-network methods have been designed that
    are inspired by AEs. Although ML-SVM mimics the architecture of AEs, its computational
    cost prevents the algorithm from being a practical choice. DKF takes advantage
    of the VAE idea by learning a Kalman Filter in its middle layer. Additionally,
    Feng et al. [[125](#bib.bib125)] introduced an encoder forest, a model inspired
    by the DNN autoencoder. Because the encoder forest is not a deep model, we do
    not include the details of this algorithm in our survey.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Learning Optimization Outside of Deep Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed in Section [2.5](#S2.SS5 "2.5 Optimization for Training Deep Neural
    Networks ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization"),
    gradient descent has been a prominent optimization algorithm for DNNs; however,
    it has been underutilized by non-network classifiers. Some notable exceptions
    are found in the literature. We discuss these here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A resourceful method for constructing a deep model is to start with a DNN architecture
    and then replace nodes with non-network classifiers. As an example, the multilayer
    SVM (ML-SVM) [[126](#bib.bib126)] replaces nodes in a MLP with standard SVMs.
    ML-SVM is a multiclass classifier which contains SVMs within the network. At the
    output layer, the ML-SVM contains the same number of SVMs as the number of classes
    learned at the perceptron output layer. Each SVM at the ML-SVM output layer is
    trained in a one-versus-all fashion for one of the classes. When observing a new
    data point, ML-SVM outputs the class label corresponding to the SVM that generates
    the highest confidence. At each hidden layer, SVMs are associated with each node
    that learn latent variables. These variables are then fed to the output layer.
    At hidden layer $f(X|\theta)$ where $X$ is the training set and $\theta$ denotes
    the trainable parameters of the SVM, ML-SVM maps the hidden layer features to
    an output value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $g(f(X&#124;\theta))=\sum^{l}_{i=1}y^{c}_{i}\alpha^{c}_{i}K_{o}(f(x_{i}&#124;\theta),f(X&#124;\theta))+b_{c},$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'where $g$ is the output layer function, $y_{i}^{c}\in\{-1,1\}$ for each class
    $c$, $K_{o}$ is the kernel function for the output layer, $\alpha_{i}^{c}$ are
    the support vector coefficients for SVM nodes of the output layer, and $b_{c}$
    is their bias. The goal of ML-SVM is to learn the maximum support vector coefficient
    of each SVM at the output layer with respect to the objective function $J_{c}(.)$,
    as shown in Equation [8](#S4.E8 "In 4 Deep Learning Optimization Outside of Deep
    Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{w^{c},b,\xi,\theta}J_{c}=\frac{1}{2}&#124;&#124;w^{c}&#124;&#124;^{2}+C\sum_{i}^{l}\xi_{i}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Here, $w^{c}$ represents the set of weights for class $c$, $C$ represents a
    trade-off between margin width and misclassification risk and $\xi_{i}$ are slack
    variables. ML-SVM applies gradient ascent to adapt its support vector coefficient
    towards a local maximum of $J_{c}(.)$. The support vector coefficient is defined
    as zero for values less than zero and is assigned to $C$ for values larger than
    $C$. The data is backpropagated through the network similar to traditional MLPs
    by calculating the gradient of the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: The SVMs in the hidden layer are identical. Given the same inputs, they would
    thus generate the same outputs. To diversity the SVMs, the hidden layers train
    on a perturbed version of the training set to eliminate producing similar outputs
    before training the combined ML-SVM model. The outputs of hidden layer nodes are
    constrained to generate values in the range $[-1\ldots 1]$. Despite the effort
    of ML-SVMs to learn a multi-layer data representation, this approach is currently
    not practical because adding a new node incurs a dramatic computational expense
    for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Kontschieder et al. [[127](#bib.bib127)] further incorporate gradient descent
    into a Random Forest (RF), which is a popular classification method. One of the
    drawbacks of a RF is that does not traditionally learn new internal representations
    like DNNs. The Deep Network Decision Forest (DNDF) [[127](#bib.bib127)] integrates
    a DNN into each decision tree within the forest to reduce the uncertainty at each
    decision node. In DNDF, the result of a decision node $d_{n}(x,\Theta)$ corresponds
    to the output of a DNN $f_{n}(x,\Theta)$, where $x$ is an input and $\Theta$ is
    the parameter of a decision node. DNDF must have differentiable decision trees
    to be able to apply gradient descent to the process of updating decision nodes.
    In a standard decision tree, the result of a decision node $d_{n}(x,\Theta)$ is
    deterministic. DNDF replaces the traditional decision node with a sigmoid function
    $d_{n}(x,\Theta)=\sigma(f_{n}(x;\Theta))$ to create a stochastic decision node.
    The probability of reaching a leaf node $l$ is calculated as the product of all
    decision node outputs from the root to the leaf $l$, which is expressed as $\mu_{l}$
    in this context. The set of leaf nodes $\mathcal{L}$ learns the class distribution
    $\pi$, and the class with the highest probability is the prediction of the tree.
    The aim of DNDF is to minimize its empirical risk with respect to the decision
    node parameter $\Theta$ and the class distribution $\pi$ of $\mathcal{L}$ under
    the log-loss function for a given data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization of the empirical risk is a two-step process which is executed
    iteratively. The first step is to optimize the class distribution of leaf nodes
    $\pi_{\mathcal{L}}$ while fixing the decision node parameters and the corresponding
    DNN. At the start of optimization (iteration 0), class distribution $\pi^{{0}}$
    is set to a uniform distribution across all leaves. DNDF then iteratively updates
    the class distribution across the leaf nodes as follows for iteration t+1:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi_{l_{y}}^{(t+1)}=\frac{1}{Z_{l}^{(t)}}\sum_{(x,y^{\prime})\in\mathcal{T}}\frac{\mathbbm{1}_{y=y^{\prime}}\pi_{l_{y}^{(t)}}\mu_{l}(x&#124;\Theta)}{\mathbb{P}_{T}[y&#124;x,\Theta,\pi^{(t)}]},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $Z_{l}^{(t)}$ is a normalization factor ensuring that $\sum_{y}\pi^{t+1}_{l_{y}}=1$,
    $\mathbbm{1}_{q}$ is the indicator function on the argument $q$, and $\mathbb{P}_{T}$
    is the prediction of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is to optimize decision node parameters $\Theta$ while fixing
    the class distribution $\pi_{\mathcal{L}}$. DNDF employs gradient descent to minimize
    log-loss with respect to $\Theta$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial L}{\partial\Theta}(\Theta,\pi;x,y)=\sum_{n\in\mathcal{N}}\frac{\partial
    L(\Theta,\pi;x,y)}{\partial f_{n}(x;\Theta)}\frac{\partial f_{n}(x;\Theta)}{\partial\Theta}.$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'The second term in Equation [10](#S4.E10 "In 4 Deep Learning Optimization Outside
    of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from
    Deep Networks: Models, Optimizations, and Regularization") is the gradient of
    the DNN. Because this is commonly known, we only discuss calculating the gradient
    of the differentiable decision tree. Here, the gradient of the differentiable
    decision tree is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial L(\Theta,\pi;x,y)}{\partial f_{n}(x;\Theta)}=d_{n}(x;\Theta)A_{n_{r}}-\bar{d}_{n}(x;\Theta)A_{n_{l}},$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'where $d_{n}$ is the probability of transitioning to the left child, $\bar{d}_{n}=1-d_{n}$
    is the probability of transitioning to the right child calculated by a forward
    pass through the DNN, and $n_{l}$ and $n_{r}$ indicate the left and right children
    of node $n$. To calculate the term $A$ in Equation [11](#S4.E11 "In 4 Deep Learning
    Optimization Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization"), DNDF
    performs one forward pass and one backward pass through the differentiable decision
    tree. Upon completing the forward pass, a value $A_{l}$ can be initially computed
    for each leaf node as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A_{l}=\frac{\pi_{l_{y}}\mu_{l}}{\sum_{l}\pi_{l_{y}}\mu_{l}}.$ |  | (12)
    |'
  prefs: []
  type: TYPE_TB
- en: Next, the values of $A_{l}$ for each leaf node are used to compute the values
    of $A_{m}$ for each internal node $m$. To do this, a backward pass is made through
    the decision tree, during which the values are calculated as $A_{m}=A_{n_{l}}+A_{n_{r}}$,
    where $n_{l}$ and $n_{r}$ represent the left and the right children of node $m$,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer of a standard DNN produces the output $o_{i}$ at layer $i$. As mentioned
    earlier, the goal of the DNN is to learn a mapping function $F_{i}:o_{i-1}\rightarrow
    o_{i}$ that minimizes the empirical loss at the last layer of DNN on a training
    set. Because each $F_{i}$ is differentiable, a DNN updates its parameters efficiently
    by applying gradient descent to reduce the empirical loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adopting a different methodology, Frosst et al. [[128](#bib.bib128)] distill
    a neural network into a soft decision tree. This model benefits from both neural
    network-based representation learning and decision tree-based concept explainability.
    In the Frosst soft decision tree (FSDT), each tree’s inner node learns a filter
    $w_{i}$ and a bias $b_{i}$, and leaf nodes $l$ learn a distribution of classes.
    Like the hidden units of a neural network, each inner node of the tree determines
    the probability of input $x$ at node $i$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{i}(x)=\sigma(\beta(xw_{i}+b_{i}))$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\sigma$ represents the sigmoid function and $\beta$ represents an inverse
    temperature whose function is to avoid soft decisions in the tree. Filter activation
    routes the sample $x$ to the left branch for values of $p_{i}$ less than $0.5$,
    and to the right branch otherwise. The probability distribution $Q^{l}$ for each
    leaf node $l$ represents the learned parameter $\phi^{l}$ at that leaf over the
    possible $k$ output classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q_{k}^{l}=\frac{\exp(\phi_{k}^{l})}{\sum_{k^{\prime}}\exp(\phi_{k^{\prime}}^{l})}.$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'The predictive distribution over classes is calculated by traversing the greatest-probability
    path. To train this soft decision tree, Frosst et al. [[128](#bib.bib128)] calculate
    a loss function $L$ that minimizes the cross entropy between each leaf, weighted
    by input vector $x$ path probability and target distribution $T$, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L(x)=-\log\Big{(}\sum_{l\in LeafNodes}P^{l}(x)\sum_{k}T_{k}\log Q_{k}^{l}\Big{)}$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: 'where $P^{l}(x)$ is the probability of reaching leaf node $l$ given input $x$.
    Frosst et al. [[128](#bib.bib128)] also introduce a regularization term to avoid
    internal nodes routing all data points on one particular path and encourage them
    to equally route data along the left and right branches. The penalty function
    calculates a sum over all internal nodes from the root to node $i$, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C=-\lambda\sum_{i\in InnerNodes}0.5\log(\alpha_{i})+0.5\log(1-\alpha_{i})$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\lambda$ is a hyper-parameter set prior to training to determine the
    effect of the penalty. The cross entropy $\alpha$ for a node $i$ is the sum of
    the path probability $P^{i}(x)$ from the root to node $i$ multiplied by the probability
    of that node $p_{i}$ divided by the path probability, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha_{i}=\frac{\sum_{x}P^{i}(x)p_{i}(x)}{\sum_{x}P^{i}(x)}.$ |  | (17)
    |'
  prefs: []
  type: TYPE_TB
- en: Because the probability distribution is not uniform across nodes in the penultimate
    level, this penalty function could actually hurt the generalization. The authors
    address this problem by decaying the strength of penalty function $\lambda$ exponentially
    with the depth $d$ of the node to $2^{d}$. Another challenge is that in any given
    batch of data, as the data descends the tree, the number of samples decreases
    exponentially. Therefore, the estimated probability loses accuracy further down
    the tree. Frosst et al. recommend addressing this problem by decaying a running
    average of the actual probabilities with a time window that is exponentially proportional
    to the depth of the nodes [[128](#bib.bib128)]. Although the authors report that
    the accuracy of this model was less than the deep neural network, the model offers
    an advantage of concept interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Both DNDF and the soft decision tree fix the depth of the learned tree to a
    predefined value. In contrast, Tanno et al. [[129](#bib.bib129)] introduced the
    Adaptive Neural Tree (ANT) which can grow to any arbitrary depth. The ANT architecture
    is similar to a decision tree but at each internal node and edge, ANT learns a
    new data representation. For example, an ANT may contain one or more convolution
    layers followed by a fully-connected layer at each inner node, one or more convolution
    layers followed by an activation function such as ReLU or tanh at each edge, and
    a linear classifier at each leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training an ANT requires two phases: growth and refinement. In the growth phase,
    starting from the root in breadth-first order one of the nodes is selected. The
    learner then evaluates three choices: 1) split the node and add a sub-tree, 2)
    deepen edge transformation by adding another layer of convolution, or 3) keep
    the current model. The model optimizes the parameters of the newly-added components
    by minimizing log likelihood via gradient descent while fixing the parameters
    of the previous portion of the tree. Eventually, the model selects the choice
    that yields the lowest log likelihood. This process repeats until the model converges.
    In the refinement phase, the model performs gradient descent on the final architecture.
    The purpose of the refinement phase is to correct suboptimal decisions that may
    have occurred during the growth phase. The authors evaluate their method on several
    standard testbeds and the results indicate that ANT is competitive with many deep
    network and non-network learners for these tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yang et al. [[130](#bib.bib130)] took a different approach; instead of integrating
    artificial neurons into the tree, they obtained a decision tree using a neural
    network. The Deep Neural Decision Tree (DNDT) employs a soft binning function
    to learn the split rules of the tree. DNDT construct a one-layer neural network
    with softmax as its activation function. The objective function of this network
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{softmax}\big{(}\frac{wx+b}{\tau}\big{)}.$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, for a continuous variable $x$, we want to bin it to $n+1$, $w=[1,2,\cdots,n+1]$
    is an untrainable constant, $b$ is a learnable bin or the cutting rule in the
    tree, and $\tau$ is a temperature variable. After training this model, the decision
    tree is constructed via the Kronecker product $\otimes$. Given an input $x\in
    R^{D}$ with $D$ features, the tree rule to reach a leaf node is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z=f_{1}(x_{1})\otimes f_{2}(x_{2})\otimes\cdots\otimes f_{D}(x_{D})$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: Here, $z$ is an almost-one-hot encoded vector that indicates the index of leaf
    node. One of the shortcomings of this method is that it cannot handle a high-dimensional
    dataset because the cost of calculating the Kronecker product becomes prohibitive.
    To overcome this problem, authors learn a classifier forest by training each tree
    on a random subset of features.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the mapping function is not differentiable. Feng et al. [[131](#bib.bib131)]
    propose a new learning paradigm for training a multilayer Gradient Boosting decision
    tree (mGBDT) [[131](#bib.bib131)] where $F_{i}$ is not differentiable. Gradient
    boosting decision tree (GBDT) is an iterative method which learns an ensemble
    of regression predictors. In GBDT, a decision tree first learns a model on a training
    set, then it computes the corresponding error residual for each training sample.
    A new tree learns a model on the error residuals, and by combining these two trees
    GBDT is able to learn a more complex model. The algorithm follows this procedure
    iteratively until it meets a prespecified number of trees for training.
  prefs: []
  type: TYPE_NORMAL
- en: Since gradient descent is not applicable to mGBDT, Feng et al. [[131](#bib.bib131)]
    obtain a “pseudo-inverse” mapping. In this mapping, $G^{t}_{i}$ represents the
    pseudo-inverse of $F^{t-1}_{i}$ at iteration $t$, such that $G^{t}_{i}(F^{t-1}_{i}(o_{i-1}))\sim
    o_{i-1}$. After performing backward propagation and calculating $G^{t}_{i}$, forward
    propagation is performed by fitting a pseudo-label $z^{t}_{i-1}$ from $G^{t}_{i}$
    to $F^{t-1}_{i}$. The last layer $F_{m}$ computes $z^{t}_{m}$ based on the true
    labels at iteration $t$, where $i\in\{2\ldots m\}$. After this step, pseudo-labels
    for previous layers are computed via pseudo-inverse mapping. To initialize mGBDT
    at iteration $t=0$, each intermediate (hidden) layer outputs Gaussian noise and
    $F^{0}_{i}$ represent depth-constrained trees that will later be refined. Feng
    et al. [[131](#bib.bib131)] thus create a method that is inspired by gradient
    descent yet is applicable in situations where true gradient descent cannot be
    effectively applied.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we examine methods that apply gradient descent to non-network
    models. As we observed, one way of utilizing gradient descent is to replace the
    hidden units in a network with a differentiable algorithm like SVM. Another common
    theme we recognized was to transform deterministic decision-tree nodes into stochastic
    versions that offer greater representational power. Alternatively, trees or other
    ruled-based models can be built using neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Deep Learning Regularization Outside of Deep Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have discussed some of the common regularization methods used by DNNs in
    Section [2.6](#S2.SS6 "2.6 Regularization ‣ 2 Brief Overview of Deep Neural Networks
    ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization"). Now we focus on how these methods have been
    applied to non-network classifiers in the literature. It is worth mentioning that
    while most models introduced in this section are not deep models, we investigate
    how non-network models can improve their performance by applying regularization
    methods typically associated with the deep operations found in DNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Parameter Norm Penalty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Problems arise when a model is learned from data that contain a large number
    of redundant features. For example, selecting relevant genes associated with different
    types of cancer is challenging because of a large number of redundancies may exist
    in the gene’s long string of features. There are two common ways to eliminate
    redundant features: the first way is to perform feature selection and then train
    a classifier from the selected features; the second way is to simultaneously perform
    feature selection and classification. As we discussed in Section [2.6.1](#S2.SS6.SSS1
    "2.6.1 Parameter Norm Penalty ‣ 2.6 Regularization ‣ 2 Brief Overview of Deep
    Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization"), DNNs apply a $L_{1}$ or $L_{2}$ penalty
    function to penalize large weights. In this section, we investigate how the traditional
    DNN idea of penalizing features can be applied to non-network classifiers to simultaneously
    select high-ranked features and perform classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard SVMs employ the $L_{2}$ norm penalty to penalize weights in a manner
    similar to DNNs. However, the Newton Linear Programming SVM (NLP-SVM) [[132](#bib.bib132)]
    replaces the $L_{2}$ norm penalty with the $L_{1}$ norm penalty. This has the
    effect of setting small hyperparameter coefficients to zero, thus enabling NLP-SVM
    to select important features automatically. A different way to penalize non-important
    features in SVMs is to employ a Smoothly Clipped Absolute Deviation (SCAD) [[133](#bib.bib133)]
    function. The $L_{1}$ penalty function can be biased because it imposes a larger
    penalty on large coefficients; in contrast, SCAD can give a nearly unbiased estimation
    of large coefficients. SCAD learns a non-convex penalty function as shown in Equation [20](#S5.E20
    "In 5.1 Parameter Norm Penalty ‣ 5 Deep Learning Regularization Outside of Deep
    Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S5.E20.m1.8" class="ltx_Math" alttext="p_{\lambda}(&#124;w&#124;)=\begin{cases}\lambda&#124;w&#124;&amp;\text{if
    }&#124;w&#124;\leq\lambda\\ -\frac{(&#124;w&#124;^{2}-2a\lambda&#124;w&#124;+\lambda^{2})}{2(a-1)}&amp;\text{if
    }\lambda<&#124;w&#124;\leq a\lambda\\'
  prefs: []
  type: TYPE_NORMAL
- en: \frac{(a+1)\lambda^{2}}{2}&amp;\text{if }&#124;w&#124;>a\lambda\end{cases}"
    display="block"><semantics id="S5.E20.m1.8a"><mrow id="S5.E20.m1.8.8" xref="S5.E20.m1.8.8.cmml"><mrow
    id="S5.E20.m1.8.8.1" xref="S5.E20.m1.8.8.1.cmml"><msub id="S5.E20.m1.8.8.1.3"
    xref="S5.E20.m1.8.8.1.3.cmml"><mi id="S5.E20.m1.8.8.1.3.2" xref="S5.E20.m1.8.8.1.3.2.cmml">p</mi><mi
    id="S5.E20.m1.8.8.1.3.3" xref="S5.E20.m1.8.8.1.3.3.cmml">λ</mi></msub><mo lspace="0em"
    rspace="0em" id="S5.E20.m1.8.8.1.2" xref="S5.E20.m1.8.8.1.2.cmml">​</mo><mrow
    id="S5.E20.m1.8.8.1.1.1" xref="S5.E20.m1.8.8.1.cmml"><mo stretchy="false" id="S5.E20.m1.8.8.1.1.1.2"
    xref="S5.E20.m1.8.8.1.cmml">(</mo><mrow id="S5.E20.m1.8.8.1.1.1.1.2" xref="S5.E20.m1.8.8.1.1.1.1.1.cmml"><mo
    stretchy="false" id="S5.E20.m1.8.8.1.1.1.1.2.1" xref="S5.E20.m1.8.8.1.1.1.1.1.1.cmml">&#124;</mo><mi
    id="S5.E20.m1.7.7" xref="S5.E20.m1.7.7.cmml">w</mi><mo stretchy="false" id="S5.E20.m1.8.8.1.1.1.1.2.2"
    xref="S5.E20.m1.8.8.1.1.1.1.1.1.cmml">&#124;</mo></mrow><mo stretchy="false" id="S5.E20.m1.8.8.1.1.1.3"
    xref="S5.E20.m1.8.8.1.cmml">)</mo></mrow></mrow><mo id="S5.E20.m1.8.8.2" xref="S5.E20.m1.8.8.2.cmml">=</mo><mrow
    id="S5.E20.m1.6.6" xref="S5.E20.m1.8.8.3.1.cmml"><mo id="S5.E20.m1.6.6.7" xref="S5.E20.m1.8.8.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S5.E20.m1.6.6.6"
    xref="S5.E20.m1.8.8.3.1.cmml"><mtr id="S5.E20.m1.6.6.6a" xref="S5.E20.m1.8.8.3.1.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S5.E20.m1.6.6.6b" xref="S5.E20.m1.8.8.3.1.cmml"><mrow
    id="S5.E20.m1.1.1.1.1.1.1" xref="S5.E20.m1.1.1.1.1.1.1.cmml"><mi id="S5.E20.m1.1.1.1.1.1.1.3"
    xref="S5.E20.m1.1.1.1.1.1.1.3.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S5.E20.m1.1.1.1.1.1.1.2"
    xref="S5.E20.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S5.E20.m1.1.1.1.1.1.1.4.2"
    xref="S5.E20.m1.1.1.1.1.1.1.4.1.cmml"><mo stretchy="false" id="S5.E20.m1.1.1.1.1.1.1.4.2.1"
    xref="S5.E20.m1.1.1.1.1.1.1.4.1.1.cmml">&#124;</mo><mi id="S5.E20.m1.1.1.1.1.1.1.1"
    xref="S5.E20.m1.1.1.1.1.1.1.1.cmml">w</mi><mo stretchy="false" id="S5.E20.m1.1.1.1.1.1.1.4.2.2"
    xref="S5.E20.m1.1.1.1.1.1.1.4.1.1.cmml">&#124;</mo></mrow></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S5.E20.m1.6.6.6c" xref="S5.E20.m1.8.8.3.1.cmml"><mrow id="S5.E20.m1.2.2.2.2.2.1"
    xref="S5.E20.m1.2.2.2.2.2.1.cmml"><mrow id="S5.E20.m1.2.2.2.2.2.1.3" xref="S5.E20.m1.2.2.2.2.2.1.3.cmml"><mtext
    id="S5.E20.m1.2.2.2.2.2.1.3.2" xref="S5.E20.m1.2.2.2.2.2.1.3.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.2.2.2.2.2.1.3.1" xref="S5.E20.m1.2.2.2.2.2.1.3.1.cmml">​</mo><mrow
    id="S5.E20.m1.2.2.2.2.2.1.3.3.2" xref="S5.E20.m1.2.2.2.2.2.1.3.3.1.cmml"><mo stretchy="false"
    id="S5.E20.m1.2.2.2.2.2.1.3.3.2.1" xref="S5.E20.m1.2.2.2.2.2.1.3.3.1.1.cmml">&#124;</mo><mi
    id="S5.E20.m1.2.2.2.2.2.1.1" xref="S5.E20.m1.2.2.2.2.2.1.1.cmml">w</mi><mo stretchy="false"
    id="S5.E20.m1.2.2.2.2.2.1.3.3.2.2" xref="S5.E20.m1.2.2.2.2.2.1.3.3.1.1.cmml">&#124;</mo></mrow></mrow><mo
    id="S5.E20.m1.2.2.2.2.2.1.2" xref="S5.E20.m1.2.2.2.2.2.1.2.cmml">≤</mo><mi id="S5.E20.m1.2.2.2.2.2.1.4"
    xref="S5.E20.m1.2.2.2.2.2.1.4.cmml">λ</mi></mrow></mtd></mtr><mtr id="S5.E20.m1.6.6.6d"
    xref="S5.E20.m1.8.8.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.E20.m1.6.6.6e"
    xref="S5.E20.m1.8.8.3.1.cmml"><mrow id="S5.E20.m1.3.3.3.3.1.1" xref="S5.E20.m1.3.3.3.3.1.1.cmml"><mo
    id="S5.E20.m1.3.3.3.3.1.1a" xref="S5.E20.m1.3.3.3.3.1.1.cmml">−</mo><mstyle displaystyle="false"
    id="S5.E20.m1.3.3.3.3.1.1.4" xref="S5.E20.m1.3.3.3.3.1.1.4.cmml"><mfrac id="S5.E20.m1.3.3.3.3.1.1.4a"
    xref="S5.E20.m1.3.3.3.3.1.1.4.cmml"><mrow id="S5.E20.m1.3.3.3.3.1.1.3.3.3" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.cmml"><mo
    stretchy="false" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.cmml">(</mo><mrow
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.cmml"><mrow
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.cmml"><msup
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.cmml"><mrow
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.2.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.2.1.cmml"><mo
    stretchy="false" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.2.2.1" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.2.1.1.cmml">&#124;</mo><mi
    id="S5.E20.m1.3.3.3.3.1.1.1.1.1" xref="S5.E20.m1.3.3.3.3.1.1.1.1.1.cmml">w</mi><mo
    stretchy="false" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.2.2.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.2.1.1.cmml">&#124;</mo></mrow><mn
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.3" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.3.cmml">2</mn></msup><mo
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.1" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.1.cmml">−</mo><mrow
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.cmml"><mn
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.2.cmml">2</mn><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.1" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.1.cmml">​</mo><mi
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.3" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.3.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.1a" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.1.cmml">​</mo><mi
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.4" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.4.cmml">λ</mi><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.1b" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.1.cmml">​</mo><mrow
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.5.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.5.1.cmml"><mo
    stretchy="false" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.5.2.1" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.5.1.1.cmml">&#124;</mo><mi
    id="S5.E20.m1.3.3.3.3.1.1.2.2.2" xref="S5.E20.m1.3.3.3.3.1.1.2.2.2.cmml">w</mi><mo
    stretchy="false" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.5.2.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.5.1.1.cmml">&#124;</mo></mrow></mrow></mrow><mo
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.1" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.1.cmml">+</mo><msup
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.cmml"><mi
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.2" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.2.cmml">λ</mi><mn
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.3" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.3.cmml">2</mn></msup></mrow><mo
    stretchy="false" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.3" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.cmml">)</mo></mrow><mrow
    id="S5.E20.m1.3.3.3.3.1.1.4.4" xref="S5.E20.m1.3.3.3.3.1.1.4.4.cmml"><mn id="S5.E20.m1.3.3.3.3.1.1.4.4.3"
    xref="S5.E20.m1.3.3.3.3.1.1.4.4.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S5.E20.m1.3.3.3.3.1.1.4.4.2"
    xref="S5.E20.m1.3.3.3.3.1.1.4.4.2.cmml">​</mo><mrow id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1"
    xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.cmml"><mo stretchy="false" id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.2"
    xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.cmml">(</mo><mrow id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1"
    xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.cmml"><mi id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.2"
    xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.2.cmml">a</mi><mo id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.1"
    xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.1.cmml">−</mo><mn id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.3"
    xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false"
    id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.3" xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mstyle></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S5.E20.m1.6.6.6f" xref="S5.E20.m1.8.8.3.1.cmml"><mrow
    id="S5.E20.m1.4.4.4.4.2.1" xref="S5.E20.m1.4.4.4.4.2.1.cmml"><mrow id="S5.E20.m1.4.4.4.4.2.1.3"
    xref="S5.E20.m1.4.4.4.4.2.1.3.cmml"><mtext id="S5.E20.m1.4.4.4.4.2.1.3.2" xref="S5.E20.m1.4.4.4.4.2.1.3.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.4.4.4.4.2.1.3.1" xref="S5.E20.m1.4.4.4.4.2.1.3.1.cmml">​</mo><mi
    id="S5.E20.m1.4.4.4.4.2.1.3.3" xref="S5.E20.m1.4.4.4.4.2.1.3.3.cmml">λ</mi></mrow><mo
    id="S5.E20.m1.4.4.4.4.2.1.4" xref="S5.E20.m1.4.4.4.4.2.1.4.cmml"><</mo><mrow id="S5.E20.m1.4.4.4.4.2.1.5.2"
    xref="S5.E20.m1.4.4.4.4.2.1.5.1.cmml"><mo stretchy="false" id="S5.E20.m1.4.4.4.4.2.1.5.2.1"
    xref="S5.E20.m1.4.4.4.4.2.1.5.1.1.cmml">&#124;</mo><mi id="S5.E20.m1.4.4.4.4.2.1.1"
    xref="S5.E20.m1.4.4.4.4.2.1.1.cmml">w</mi><mo stretchy="false" id="S5.E20.m1.4.4.4.4.2.1.5.2.2"
    xref="S5.E20.m1.4.4.4.4.2.1.5.1.1.cmml">&#124;</mo></mrow><mo id="S5.E20.m1.4.4.4.4.2.1.6"
    xref="S5.E20.m1.4.4.4.4.2.1.6.cmml">≤</mo><mrow id="S5.E20.m1.4.4.4.4.2.1.7" xref="S5.E20.m1.4.4.4.4.2.1.7.cmml"><mi
    id="S5.E20.m1.4.4.4.4.2.1.7.2" xref="S5.E20.m1.4.4.4.4.2.1.7.2.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.4.4.4.4.2.1.7.1" xref="S5.E20.m1.4.4.4.4.2.1.7.1.cmml">​</mo><mi
    id="S5.E20.m1.4.4.4.4.2.1.7.3" xref="S5.E20.m1.4.4.4.4.2.1.7.3.cmml">λ</mi></mrow></mrow></mtd></mtr><mtr
    id="S5.E20.m1.6.6.6g" xref="S5.E20.m1.8.8.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S5.E20.m1.6.6.6h" xref="S5.E20.m1.8.8.3.1.cmml"><mstyle
    displaystyle="false" id="S5.E20.m1.5.5.5.5.1.1" xref="S5.E20.m1.5.5.5.5.1.1.cmml"><mfrac
    id="S5.E20.m1.5.5.5.5.1.1a" xref="S5.E20.m1.5.5.5.5.1.1.cmml"><mrow id="S5.E20.m1.5.5.5.5.1.1.1.1"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.cmml"><mrow id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.2"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.cmml"><mi id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.2"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.2.cmml">a</mi><mo id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.1"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.3"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false"
    id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.3" xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.5.5.5.5.1.1.1.1.2" xref="S5.E20.m1.5.5.5.5.1.1.1.1.2.cmml">​</mo><msup
    id="S5.E20.m1.5.5.5.5.1.1.1.1.3" xref="S5.E20.m1.5.5.5.5.1.1.1.1.3.cmml"><mi id="S5.E20.m1.5.5.5.5.1.1.1.1.3.2"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.3.2.cmml">λ</mi><mn id="S5.E20.m1.5.5.5.5.1.1.1.1.3.3"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.3.3.cmml">2</mn></msup></mrow><mn id="S5.E20.m1.5.5.5.5.1.1.3"
    xref="S5.E20.m1.5.5.5.5.1.1.3.cmml">2</mn></mfrac></mstyle></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S5.E20.m1.6.6.6i" xref="S5.E20.m1.8.8.3.1.cmml"><mrow id="S5.E20.m1.6.6.6.6.2.1"
    xref="S5.E20.m1.6.6.6.6.2.1.cmml"><mrow id="S5.E20.m1.6.6.6.6.2.1.3" xref="S5.E20.m1.6.6.6.6.2.1.3.cmml"><mtext
    id="S5.E20.m1.6.6.6.6.2.1.3.2" xref="S5.E20.m1.6.6.6.6.2.1.3.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.6.6.6.6.2.1.3.1" xref="S5.E20.m1.6.6.6.6.2.1.3.1.cmml">​</mo><mrow
    id="S5.E20.m1.6.6.6.6.2.1.3.3.2" xref="S5.E20.m1.6.6.6.6.2.1.3.3.1.cmml"><mo stretchy="false"
    id="S5.E20.m1.6.6.6.6.2.1.3.3.2.1" xref="S5.E20.m1.6.6.6.6.2.1.3.3.1.1.cmml">&#124;</mo><mi
    id="S5.E20.m1.6.6.6.6.2.1.1" xref="S5.E20.m1.6.6.6.6.2.1.1.cmml">w</mi><mo stretchy="false"
    id="S5.E20.m1.6.6.6.6.2.1.3.3.2.2" xref="S5.E20.m1.6.6.6.6.2.1.3.3.1.1.cmml">&#124;</mo></mrow></mrow><mo
    id="S5.E20.m1.6.6.6.6.2.1.2" xref="S5.E20.m1.6.6.6.6.2.1.2.cmml">></mo><mrow id="S5.E20.m1.6.6.6.6.2.1.4"
    xref="S5.E20.m1.6.6.6.6.2.1.4.cmml"><mi id="S5.E20.m1.6.6.6.6.2.1.4.2" xref="S5.E20.m1.6.6.6.6.2.1.4.2.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S5.E20.m1.6.6.6.6.2.1.4.1" xref="S5.E20.m1.6.6.6.6.2.1.4.1.cmml">​</mo><mi
    id="S5.E20.m1.6.6.6.6.2.1.4.3" xref="S5.E20.m1.6.6.6.6.2.1.4.3.cmml">λ</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S5.E20.m1.8b"><apply id="S5.E20.m1.8.8.cmml" xref="S5.E20.m1.8.8"><apply
    id="S5.E20.m1.8.8.1.cmml" xref="S5.E20.m1.8.8.1"><apply id="S5.E20.m1.8.8.1.3.cmml"
    xref="S5.E20.m1.8.8.1.3"><csymbol cd="ambiguous" id="S5.E20.m1.8.8.1.3.1.cmml"
    xref="S5.E20.m1.8.8.1.3">subscript</csymbol><ci id="S5.E20.m1.8.8.1.3.2.cmml"
    xref="S5.E20.m1.8.8.1.3.2">𝑝</ci><ci id="S5.E20.m1.8.8.1.3.3.cmml" xref="S5.E20.m1.8.8.1.3.3">𝜆</ci></apply><apply
    id="S5.E20.m1.8.8.1.1.1.1.1.cmml" xref="S5.E20.m1.8.8.1.1.1.1.2"><ci id="S5.E20.m1.7.7.cmml"
    xref="S5.E20.m1.7.7">𝑤</ci></apply></apply><apply id="S5.E20.m1.8.8.3.1.cmml"
    xref="S5.E20.m1.6.6"><csymbol cd="latexml" id="S5.E20.m1.8.8.3.1.1.cmml" xref="S5.E20.m1.6.6.7">cases</csymbol><apply
    id="S5.E20.m1.1.1.1.1.1.1.cmml" xref="S5.E20.m1.1.1.1.1.1.1"><ci id="S5.E20.m1.1.1.1.1.1.1.3.cmml"
    xref="S5.E20.m1.1.1.1.1.1.1.3">𝜆</ci><apply id="S5.E20.m1.1.1.1.1.1.1.4.1.cmml"
    xref="S5.E20.m1.1.1.1.1.1.1.4.2"><ci id="S5.E20.m1.1.1.1.1.1.1.1.cmml" xref="S5.E20.m1.1.1.1.1.1.1.1">𝑤</ci></apply></apply><apply
    id="S5.E20.m1.2.2.2.2.2.1.cmml" xref="S5.E20.m1.2.2.2.2.2.1"><apply id="S5.E20.m1.2.2.2.2.2.1.3.cmml"
    xref="S5.E20.m1.2.2.2.2.2.1.3"><ci id="S5.E20.m1.2.2.2.2.2.1.3.2a.cmml" xref="S5.E20.m1.2.2.2.2.2.1.3.2"><mtext
    id="S5.E20.m1.2.2.2.2.2.1.3.2.cmml" xref="S5.E20.m1.2.2.2.2.2.1.3.2">if </mtext></ci><apply
    id="S5.E20.m1.2.2.2.2.2.1.3.3.1.cmml" xref="S5.E20.m1.2.2.2.2.2.1.3.3.2"><ci id="S5.E20.m1.2.2.2.2.2.1.1.cmml"
    xref="S5.E20.m1.2.2.2.2.2.1.1">𝑤</ci></apply></apply><ci id="S5.E20.m1.2.2.2.2.2.1.4.cmml"
    xref="S5.E20.m1.2.2.2.2.2.1.4">𝜆</ci></apply><apply id="S5.E20.m1.3.3.3.3.1.1.cmml"
    xref="S5.E20.m1.3.3.3.3.1.1"><apply id="S5.E20.m1.3.3.3.3.1.1.4.cmml" xref="S5.E20.m1.3.3.3.3.1.1.4"><apply
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3"><apply
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2"><apply
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2"><csymbol
    cd="ambiguous" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.1.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2">superscript</csymbol><apply
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.2.1.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.2.2"><ci
    id="S5.E20.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S5.E20.m1.3.3.3.3.1.1.1.1.1">𝑤</ci></apply><cn
    type="integer" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.3.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.2.3">2</cn></apply><apply
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3"><cn
    type="integer" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.2.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.2">2</cn><ci
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.3.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.3">𝑎</ci><ci
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.4.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.4">𝜆</ci><apply
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.5.1.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.2.3.5.2"><ci
    id="S5.E20.m1.3.3.3.3.1.1.2.2.2.cmml" xref="S5.E20.m1.3.3.3.3.1.1.2.2.2">𝑤</ci></apply></apply></apply><apply
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3"><csymbol
    cd="ambiguous" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.1.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3">superscript</csymbol><ci
    id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.2.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.2">𝜆</ci><cn
    type="integer" id="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.3.cmml" xref="S5.E20.m1.3.3.3.3.1.1.3.3.3.1.3.3">2</cn></apply></apply><apply
    id="S5.E20.m1.3.3.3.3.1.1.4.4.cmml" xref="S5.E20.m1.3.3.3.3.1.1.4.4"><cn type="integer"
    id="S5.E20.m1.3.3.3.3.1.1.4.4.3.cmml" xref="S5.E20.m1.3.3.3.3.1.1.4.4.3">2</cn><apply
    id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.cmml" xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1"><ci
    id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.2.cmml" xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.2">𝑎</ci><cn
    type="integer" id="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.3.cmml" xref="S5.E20.m1.3.3.3.3.1.1.4.4.1.1.1.3">1</cn></apply></apply></apply></apply><apply
    id="S5.E20.m1.4.4.4.4.2.1.cmml" xref="S5.E20.m1.4.4.4.4.2.1"><apply id="S5.E20.m1.4.4.4.4.2.1b.cmml"
    xref="S5.E20.m1.4.4.4.4.2.1"><apply id="S5.E20.m1.4.4.4.4.2.1.3.cmml" xref="S5.E20.m1.4.4.4.4.2.1.3"><ci
    id="S5.E20.m1.4.4.4.4.2.1.3.2a.cmml" xref="S5.E20.m1.4.4.4.4.2.1.3.2"><mtext id="S5.E20.m1.4.4.4.4.2.1.3.2.cmml"
    xref="S5.E20.m1.4.4.4.4.2.1.3.2">if </mtext></ci><ci id="S5.E20.m1.4.4.4.4.2.1.3.3.cmml"
    xref="S5.E20.m1.4.4.4.4.2.1.3.3">𝜆</ci></apply><apply id="S5.E20.m1.4.4.4.4.2.1.5.1.cmml"
    xref="S5.E20.m1.4.4.4.4.2.1.5.2"><ci id="S5.E20.m1.4.4.4.4.2.1.1.cmml" xref="S5.E20.m1.4.4.4.4.2.1.1">𝑤</ci></apply></apply><apply
    id="S5.E20.m1.4.4.4.4.2.1c.cmml" xref="S5.E20.m1.4.4.4.4.2.1"><apply id="S5.E20.m1.4.4.4.4.2.1.7.cmml"
    xref="S5.E20.m1.4.4.4.4.2.1.7"><ci id="S5.E20.m1.4.4.4.4.2.1.7.2.cmml" xref="S5.E20.m1.4.4.4.4.2.1.7.2">𝑎</ci><ci
    id="S5.E20.m1.4.4.4.4.2.1.7.3.cmml" xref="S5.E20.m1.4.4.4.4.2.1.7.3">𝜆</ci></apply></apply></apply><apply
    id="S5.E20.m1.5.5.5.5.1.1.cmml" xref="S5.E20.m1.5.5.5.5.1.1"><apply id="S5.E20.m1.5.5.5.5.1.1.1.1.cmml"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1"><apply id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.cmml"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1"><ci id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.2.cmml"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.2">𝑎</ci><cn type="integer" id="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.3.cmml"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S5.E20.m1.5.5.5.5.1.1.1.1.3.cmml"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E20.m1.5.5.5.5.1.1.1.1.3.1.cmml"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.3">superscript</csymbol><ci id="S5.E20.m1.5.5.5.5.1.1.1.1.3.2.cmml"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.3.2">𝜆</ci><cn type="integer" id="S5.E20.m1.5.5.5.5.1.1.1.1.3.3.cmml"
    xref="S5.E20.m1.5.5.5.5.1.1.1.1.3.3">2</cn></apply></apply><cn type="integer"
    id="S5.E20.m1.5.5.5.5.1.1.3.cmml" xref="S5.E20.m1.5.5.5.5.1.1.3">2</cn></apply><apply
    id="S5.E20.m1.6.6.6.6.2.1.cmml" xref="S5.E20.m1.6.6.6.6.2.1"><apply id="S5.E20.m1.6.6.6.6.2.1.3.cmml"
    xref="S5.E20.m1.6.6.6.6.2.1.3"><ci id="S5.E20.m1.6.6.6.6.2.1.3.2a.cmml" xref="S5.E20.m1.6.6.6.6.2.1.3.2"><mtext
    id="S5.E20.m1.6.6.6.6.2.1.3.2.cmml" xref="S5.E20.m1.6.6.6.6.2.1.3.2">if </mtext></ci><apply
    id="S5.E20.m1.6.6.6.6.2.1.3.3.1.cmml" xref="S5.E20.m1.6.6.6.6.2.1.3.3.2"><ci id="S5.E20.m1.6.6.6.6.2.1.1.cmml"
    xref="S5.E20.m1.6.6.6.6.2.1.1">𝑤</ci></apply></apply><apply id="S5.E20.m1.6.6.6.6.2.1.4.cmml"
    xref="S5.E20.m1.6.6.6.6.2.1.4"><ci id="S5.E20.m1.6.6.6.6.2.1.4.2.cmml" xref="S5.E20.m1.6.6.6.6.2.1.4.2">𝑎</ci><ci
    id="S5.E20.m1.6.6.6.6.2.1.4.3.cmml" xref="S5.E20.m1.6.6.6.6.2.1.4.3">𝜆</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S5.E20.m1.8c">p_{\lambda}(&#124;w&#124;)=\begin{cases}\lambda&#124;w&#124;&\text{if
    }&#124;w&#124;\leq\lambda\\ -\frac{(&#124;w&#124;^{2}-2a\lambda&#124;w&#124;+\lambda^{2})}{2(a-1)}&\text{if
    }\lambda<&#124;w&#124;\leq a\lambda\\ \frac{(a+1)\lambda^{2}}{2}&\text{if }&#124;w&#124;>a\lambda\end{cases}</annotation></semantics></math>
    |  | (20) |
  prefs: []
  type: TYPE_NORMAL
- en: SCAD equates with $L_{1}$ penalty function until $|w|=\lambda$, then smoothly
    transitions to a quadratic function until $|w|=a\lambda$, after which it remains
    a constant for all $|w|>a\lambda$. As shown by Fan et al. [[134](#bib.bib134)],
    SCAD has better theoretical properties than the $L_{1}$ function.
  prefs: []
  type: TYPE_NORMAL
- en: 'One limitation of decision tree classifiers is that the number of training
    instances that can be selected at each branch in the tree decreases with the tree
    depth. This downward sampling may cause less relevant or redundant features to
    be selected near the bottom of the tree. To address this issue, Dang et al. [[135](#bib.bib135)]
    proposed to penalize features that were never selected in the process of making
    a tree. In a Regularized Random Forest (RRF) [[135](#bib.bib135)], the information
    gain for a feature $j$ is specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Gain(j)=\begin{cases}\lambda.Gain(j)&amp;\text{$j\not\in F$}\\ Gain(f_{i})&amp;\text{$j\in
    F$}\end{cases}$ |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where $F$ is the set of features used earlier in the path, $f_{i}\in F$, and
    $\lambda\in[0,1]$ is the penalty coefficient. RRF avoids including a new feature
    $j$, except when the value of $Gain(j)$ is greater than $\max\limits_{i}\big{(}Gain(f_{i})\big{)}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve RRF, Guided RRF (GRRF) [[136](#bib.bib136)] assigns a different
    penalty coefficient $\lambda_{j}$ to each feature instead of assigning the same
    penalty coefficient to all features. GRRF employs the importance score from a
    pre-trained RF on the training set to refine the selection of features at a given
    node. The importance score of feature $j$ in an RF with $T$ trees is the mean
    of gain for features in the RF. The important scores evaluate the contribution
    of features for predicting classes. The GRRF uses the normalized importance score
    to control the degree of regularization of the penalty coefficient as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lambda_{j}=(1-\gamma)\lambda_{0}+\gamma Imp^{\prime}_{j},$ |  | (22)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{0}\in(0,1]$ is the base penalty coefficient and $\gamma\in[0,1]$
    controls the weight of the normalized importance score. The GRRF and RRF are computationally
    inexpensive methods that are able to select stronger features and avoid redundant
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As detailed in Section [2.6.2](#S2.SS6.SSS2 "2.6.2 Dropout ‣ 2.6 Regularization
    ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization"), dropout
    is a method that prevents DNNs from overfitting by randomly dropping nodes during
    the training. Dropout can be added to other machine learning algorithms through
    two methods: by dropping features or by dropping models in the case of ensemble
    methods. Dropout has also been employed by dropping input features during training
    [[137](#bib.bib137)] [[138](#bib.bib138)]. Here we look at techniques that have
    been investigated for dropping input features, particularly in non-network classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: Rashmi et al. [[139](#bib.bib139)] applied dropout to Multiple Additive Regression
    Trees (MART) [[140](#bib.bib140)] [[141](#bib.bib141)]. MART is a regression tree
    ensemble which iteratively refines its model by continually adding trees that
    fit the loss function derivatives from the previous version of the ensemble. Because
    trees added at later iterations may only impact a small fraction of the training
    set and thus over-specialize, researchers previously used shrinkage to exclude
    a random subset of leaf nodes during each tree-adding step. More recently, Rashmi
    et al. integrated the deep-learning idea of dropout into MART. Using dropout,
    a subset of the trees are temporarily dropped. A new tree is created based on
    the loss function for the on-dropped trees. This new tree is combined with the
    previously-dropped trees into a new ensemble. This method, Dropout Multiple Additive
    Regression Trees (DART) [[139](#bib.bib139)], weights the votes for the new and
    re-integrated trees to have the same effect on the final model output as the original
    set of trees. Other researchers have experimented with permanently removing a
    strategic subset of the dropped trees as well [[142](#bib.bib142)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Early Stopping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core concept of early stopping is to terminate DNN training once performance
    on the validation set is not improving. One potential advantage of Deep Forest
    [[93](#bib.bib93)] over DNNs is that DF can determine the depth of a model automatically.
    In DF, if the model performance does not increase on the validation set after
    adding a new layer, the learning terminates. Unlike DNNs, DF may avoid the tendency
    to overfit as more layers are added. Thus, while early stopping does not necessarily
    enjoy the primary outcome of preventing such overfitting, it can provide additional
    benefits such as shortening the validation cycle in the search for the optimal
    tree depth.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in Section [2.6.3](#S2.SS6.SSS3 "2.6.3 Data Augmentation ‣ 2.6
    Regularization ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization"),
    data augmentation is a powerful method for improving DNN generalization. However,
    little research has investigated the effects of data augmentation methods on non-network
    classifiers. As demonstrated by Wong et al. [[143](#bib.bib143)], the SVM classifier
    does not always benefit from data augmentation, in contrast to DNNs. However,
    Xu [[144](#bib.bib144)] ran several data augmentation experiments on synthetic
    datasets and observed that data augmentation did enhance the performance of random
    forest classifiers. Offering explanations for the circumstances in which such
    augmentation is beneficial is a needed area for future research.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Hybrid Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hybrid models can be defined as a combination of two or more classes of models.
    There are many ways to construct hybrid models, such as DNDF [[127](#bib.bib127)]
    which integrates a deep network into a decision forest as explained in Section
    [4](#S4 "4 Deep Learning Optimization Outside of Deep Neural Networks ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"). In this section we discuss other examples of hybrid models.'
  prefs: []
  type: TYPE_NORMAL
- en: One motivation for combining aspects of multiple models is to find a balance
    between classification accuracy and computational cost. Energy consumption by
    mobile devices and cloud servers is an increasing concern for responsive applications
    and green computing. Decision forests are computationally inexpensive models because
    of the conditional property of decision trees. Conversely, while CNNs are less
    efficient, they can achieve higher accuracy because of their representation-learning
    capabilities. Ioannou et al. [[145](#bib.bib145)] introduced the Conditional Neural
    Network (CondNN) to reduce computation in a CNN model by introducing a routing
    method similar to that found in decision trees. In CondNN, each node in layer
    $l$ is connected to a subset of nodes from the previous layer, $l-1$. Given a
    fully trained network, for every two consecutive layers a matrix $\Lambda_{(l-1,l)}$
    stores the activation values of these two layers. By rearranging elements of $\Lambda_{(l-1,l)}$
    based on highly-active pairs for each class in the diagonal and zeroing out off-diagonal
    elements, the CondNN develops explicit routes $\Lambda_{(l,l-1)}^{route}$ through
    nodes in the network. CondNN incurs profoundly lower computation cost compared
    to other DNNs at test time; whereas, CondNN’s accuracy remains similar to larger
    models. We note that DNN size can be also be reduced by employing Bayesian optimization,
    as investigated by Blundell et al. [[146](#bib.bib146)] and by Fortunato et al.
    [[147](#bib.bib147)]. These earlier efforts provide evidence that Bayesian neural
    networks are able to decrease network size even more than CondNNs while maintaining
    a similar level of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Another direction for blending a deep network with a non-network classifier
    is to improve the non-network model by learning a better representation of data
    via a deep network. Zoran et al. [[148](#bib.bib148)] introduce the differentiable
    boundary tree (DBT) in order to integrate a DNN into the boundary tree [[149](#bib.bib149)]
    to learn a better representation of data. The newly-learned data representation
    leads to a simpler boundary tree because the classes are well separated. The boundary
    tree is an online algorithm in which each node in the tree corresponds to a sample
    in the training set. The first sample together with its label are established
    as the tree root. Given a new query sample $z$, the sample traverses through the
    tree from the root to find the closest node $n$ based on some distance function
    like the Euclidean distance function. If the label of the nearest node in the
    tree is different from the query sample, a new node containing the query $z$ is
    added as a child of the closest node $n$ in the tree; otherwise the query node
    $z$ is discarded. Therefore, each edge in the tree marks the boundary between
    two classes and each node tends to be close to these boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transitions between nodes in a standard boundary tree are deterministic. DBT
    combines a SoftMax cost function with a boundary tree, resulting in stochastic
    transitions. Let $x$ be a training sample and $c$ be the one-hot encoding label
    of that sample. Given the current node $x_{i}$ in the tree and a query node $z$,
    the transition probability from node $x_{i}$ to node $x_{j}$, where $x_{j}\in\{child(x_{i}),x_{i}\}$
    is the SoftMax of the negative distance between $x_{j}$ and $z$. This is shown
    in Equation [23](#S6.E23 "In 6 Hybrid Models ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}p(x_{i}\rightarrow x_{j}&#124;z)=\underset{i,j\in child(i)}{\operatorname{SoftMax}}(-d(x_{j},z))\\
    =\frac{\exp(-d(x_{j},z))}{\underset{j^{\prime}\in\{i,j\in child(i)\}}{\sum}\exp(-d(x_{j},z))}\end{gathered}$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: 'The probability of traversing a particular path in the boundary tree, given
    a query node $z$, is the product of the probability of each transition along the
    path from the root to the final node $x_{final^{*}}$ in the tree. The final class
    log probability of DBT is computed by summing the probabilities of all transitions
    to the parent of $x_{final^{*}}$ together with the probabilities of the final
    node and its siblings. The set $sibling(x_{i})$ consists of all nodes sharing
    the same parent with node $x_{i}$ and the node $x_{i}$ itself. As discussed earlier,
    a DNN $f_{\theta}(x)$ transforms the inputs to learn a better representation.
    The final class log probabilities for the query node $z$ are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\log p(c&#124;f_{\theta}(z))=\smashoperator[]{\sum_{x_{i}\rightarrow
    x_{j}\in path^{\dagger}&#124;f_{\theta}(z)}^{}}\log p(f_{\theta}(x_{i})\rightarrow
    f_{\theta}(x_{j})&#124;f_{\theta}(z))\\ +\log\smashoperator[]{\sum_{x_{k}\in sibling(x_{final^{*}})}^{}}p(parent(f_{\theta}(x_{k}))\rightarrow
    f_{\theta}(x_{k})&#124;f_{\theta}(z))c(x_{k}).\end{gathered}$ |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: 'In Equation [24](#S6.E24 "In 6 Hybrid Models ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization"), $path^{\dagger}$
    denotes $path^{*}$ (the path to the final node $x_{final^{*}}$) without the last
    transition, and $sibling(x)$ represents node $x$ and all other nodes sharing the
    same parent with node $x$. The gradient descent algorithm can be applied to Equation
    [24](#S6.E24 "In 6 Hybrid Models ‣ A Survey of Techniques All Classifiers Can
    Learn from Deep Networks: Models, Optimizations, and Regularization") by plugging
    in a loss function to learn parameter $\theta$ of the DNN. However, gradient descent
    cannot be applied easily to DBT because of the node and edge manipulations in
    the graph. To address this issue, DBT transforms a small subset of training examples
    via a DNN and builds a boundary tree based on the transformed examples. Next,
    DBT transforms a query node $z$ via the same DNN and calculates the log probability
    of a class according to Equation [24](#S6.E24 "In 6 Hybrid Models ‣ A Survey of
    Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"). The DNN employs gradient descent to update its parameters
    by propagating the gradient of log loss probability. DBT discards this boundary
    tree and iteratively builds a new boundary tree as described until a convergence
    criteria is met. In the described method, the authors set a specific threshold
    for the loss value to terminate the training. DBT is able to achieve greater accuracy
    with a simpler tree than original boundary tree as shown by the authors on the
    MNIST dataset [[97](#bib.bib97)]. One of the biggest advantages of DBT is its
    interpretability. However, DBT is computationally an expensive method because
    a new computation graph needs to be built, which makes batching inefficient. Another
    limitation is that the algorithm needs to switch between building the tree and
    updating the tree. Therefore, scaling to large datasets is fairly prohibitive.'
  prefs: []
  type: TYPE_NORMAL
- en: Yet another way of building a hybrid model is to learn a new representation
    of data with a DNN, then hand the resulting feature vectors off to other classifiers
    to learn a model. Tang [[150](#bib.bib150)] explored replacing the last layer
    of DNNs with a linear SVM for classification tasks. The activation values of the
    penultimate layer are fed as input to an SVM with a $L_{2}$ regularizer. The weights
    of the lower layer are learned through momentum gradient descent by differentiating
    the SVM objective function with respect to activation of the penultimate layer.
    The author’s experiments on the MNIST [[97](#bib.bib97)] and CIFAR-10 [[151](#bib.bib151)]
    datasets demonstrate that replacing a CNN’s SoftMax output layer with SVM yields
    a lower test error. Tang et al. [[150](#bib.bib150)] postulate that the performance
    gain is due to the superior regularization effect of the SVM loss function.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that in their experiment on MNIST [[97](#bib.bib97)],
    Tang first used PCA to reduce the features and then fed the reduced feature vectors
    as input to their model. Also, Niu et al. [[152](#bib.bib152)] replaced the last
    layer of a CNN with an SVM which similarly resulted in lowering test error of
    the model compare to a CNN on the MNIST dataset. Similar to these methods, Zareapoor
    et al. [[153](#bib.bib153)], Nagi et al. [[154](#bib.bib154)], Bellili et al.
    [[155](#bib.bib155)], and Azevedo et al. [[156](#bib.bib156)] replace the last
    layer of a DNN with an SVM. In these cases, their results from multiple datasets
    reveal that employing a SVM as the last layer of a neural network can improve
    the generalization of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhao et al. [[157](#bib.bib157)] replace the last layer of a deep network with
    a visual hierarchical tree to learn a better solution for image classification
    problems. A visual hierarchical tree with $L$ levels organizes $N$ objects classes
    based on their visual similarities in its nodes. Deeper in the tree, groups become
    more separated wherein each leaf node should contain instances of one class. The
    class similarity between the class $c_{i}$ and $c_{j}$ is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S_{i,j}=S(c_{i},c_{j})=\exp\Big{(}-\frac{d(x_{i},x_{j})}{\sigma}\Big{)}.$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: Here, $d(x_{i},x_{j})$ represents the distance between the deep representation
    of instances of classes $c_{i}$ and $c_{j}$, and $\sigma$ is automatically determined
    by a self-tuning technique. After calculating matrix $S$, hierarchical clustering
    is employed to learn a visual hierarchical tree.
  prefs: []
  type: TYPE_NORMAL
- en: In a traditional visual hierarchical tree, some objects might be assigned to
    incorrect groups. A level-wise mixture model (LMM) [[157](#bib.bib157)] aims to
    improve this visual hierarchical tree by learning a new representation of data
    via a DNN then updating the tree during training. For a given tree, matrix $\Psi_{y_{i},t_{i}}$
    denotes the probability of objects with label $y$ belonging to group $t$ in the
    tree. First, LMM updates the DNN parameters and the visual hierarchical tree as
    is done with a traditional DNN. The only difference is a calculation of two gradients,
    one based on the parameters of the DNN and other one based on the parameters of
    the tree. Second, LMM updates the matrix $\Psi_{y_{i},t_{i}}$ for each training
    sample separately and updates the parameters of the DNN and the tree afterwards.
    To update the $\Psi$, the posterior probability of the assigning group $t_{i}$
    for the object $x_{i}$ is calculated based on the number of samples having the
    same label $y$ as the label of $x_{i}$ in group $t$. For a given test image, LMM
    learns a new representation of the image based on the DNN and then obtains a prediction
    by traversing the tree. One of the advantages of a LMM is that over time, by learning
    a better representation of data via DNN, the algorithm can update the visual hierarchical
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, two different data views are available. As an example, one view
    might contain video and the another sound. Canonical correlation analysis (CCA)
    [[158](#bib.bib158)] and kernel canonical correlation analysis (KCCA) [[159](#bib.bib159)]
    offer standard statistical methods for learning view representations that each
    the most predictable by the other view. Nonlinear representations learned by KCCA
    can achieve a higher correlation than linear representations learned by CCA. Despite
    the advantages of KCCA, the kernel function faces some drawbacks. Specifically,
    the representation is bound to the fixed kernel. Furthermore, because is model
    is nonparametric, training time as well as the time to compute the new data representation
    scales poorly with the size of a training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Andrews et al. [[160](#bib.bib160)] proposed to apply deep networks to learn
    a nonlinear data representation instead of employing a kernel function. Their
    resulting deep canonical correlation analysis (DCCA) consists of two separate
    deep networks for learning a new representation for each view. The new representation
    learned by the final layer of networks $H_{1}$ and $H_{2}$ is fed to CCA. To compute
    the objective gradient of DCCA, the gradient of the output of the correlation
    objective with respect to the new representation can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial_{corr}(H_{1},H_{2})}{\partial H_{1}}$ |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'After this computation, backpropagation is applied to find the gradient with
    respect to all parameters. The details of calculating the gradient in Equation
    [26](#S6.E26 "In 6 Hybrid Models ‣ A Survey of Techniques All Classifiers Can
    Learn from Deep Networks: Models, Optimizations, and Regularization") are provided
    by the authors [[160](#bib.bib160)].'
  prefs: []
  type: TYPE_NORMAL
- en: While researchers have also created LSTM methods that employ tree structures
    [[161](#bib.bib161)] [[162](#bib.bib162)], these methods utilize the data structure
    to improve a network model rather than employing tree-based learning algorithms.
    Similarly, other researches integrate non-network classifiers into a network structure.
    Cimino et al. [[163](#bib.bib163)] and Agarap [[164](#bib.bib164)] introduce hybrid
    models. These two methods apply LSTM and GRU, respectively, to learn a network
    representation. Unlike traditional DNNs, the last layer employs a SVM for classification.
  prefs: []
  type: TYPE_NORMAL
- en: The work surveyed in this section provides evidence that deep neural nets are
    capable methods for learning high-level features. These features, in turn, can
    be used to improve the modeling capability for many types of supervised classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of classifiers which integrate deep network components into
    non-network classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Methods | Classifiers |'
  prefs: []
  type: TYPE_TB
- en: '| Architecture | Feedforward | ANT [[129](#bib.bib129)], DNDT [[130](#bib.bib130)],
    DBN [[45](#bib.bib45)], Deep PCA [[105](#bib.bib105)], DF [[93](#bib.bib93)],
    DPG [[116](#bib.bib116)], R2-SVM [[101](#bib.bib101)], D-SVM [[100](#bib.bib100)],
    DTA-LS-SVM [[94](#bib.bib94)], SFDT [[128](#bib.bib128)] |'
  prefs: []
  type: TYPE_TB
- en: '| Autoencoder | DKF [[122](#bib.bib122)], eForest [[125](#bib.bib125)], ML-SVM
    [[126](#bib.bib126)] |'
  prefs: []
  type: TYPE_TB
- en: '| Siamese Model | SDF [[119](#bib.bib119)] |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Adversarial Model | GAF [[120](#bib.bib120)] |'
  prefs: []
  type: TYPE_TB
- en: '| Optimization | Gradient Decent | DNDF [[127](#bib.bib127)], mGBDT [[131](#bib.bib131)],
    ML-SVM [[126](#bib.bib126)] |'
  prefs: []
  type: TYPE_TB
- en: '| Regularization | Parameter Norm Penalty | NLP-SVM [[132](#bib.bib132)], GRRF
    [[136](#bib.bib136)], RRF [[135](#bib.bib135)] , SCAD-SVM [[133](#bib.bib133)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | DART [[139](#bib.bib139)] |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid Model |  | CondCNN [[145](#bib.bib145)], DBT [[148](#bib.bib148)],
    DCCA [[160](#bib.bib160)], DNDF [[127](#bib.bib127)], DNN+SVM [[150](#bib.bib150)]
    [[152](#bib.bib152)] [[153](#bib.bib153)] [[154](#bib.bib154)] [[155](#bib.bib155)]
    [[156](#bib.bib156)], LMM [[157](#bib.bib157)] |'
  prefs: []
  type: TYPE_TB
- en: 'In this survey, we aim to provide a thorough review of non-network models that
    utilize the unique features of deep network models. Table [1](#S6.T1 "Table 1
    ‣ 6 Hybrid Models ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization") provides a summary of such
    non-network models, organized based on four aspects of deep networks: model architecture,
    optimization, regularization, and hybrid model fusing. A known advantage of traditional
    deep networks compared with non-network models has been the ability to learn a
    better representation of input features. Inspired by various deep network architectures,
    deep learning of non-network classifiers has resulted in methods to also learn
    new feature representations. Another area where non-network classifiers have benefited
    from recent deep network research is applying backpropagation optimization to
    improve generalization. This table summarizes published efforts to apply regularization
    techniques that improve neural network generalization. The last category of models
    combines deep network classifiers and non-network classifiers to increase overall
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we survey a wide variety of models and methods. Our goal is to
    demonstrate that diverse types of models can benefit from deep learning techniques.
    To highlight this point, we empirically compare the performance of many techniques
    described in this survey. This comparison includes deep and shallow networks as
    well as and non-network learning algorithms. Because of the variety of classifiers
    that are surveyed, we organize the comparison based on the learned model structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compare the models that are most similar to DNNs. These models should
    be able to learn a better representation of data when a large dataset is available.
    We report the test error provided by the authors for MNIST and CIFAR-10 dataset
    in Table [2](#S7.T2 "Table 2 ‣ 7 Experiments ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization"). If
    the performance of a model was not available for any of these datasets, we ran
    that experiment with the authors’ code. Default parameters are employed for parameter
    values that are not specified in the original papers. In the event that the authors
    did not provide their code, we did not report any results. These omissions prevent
    the report of erroneous performances that result from implementation differences.'
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset has been a popular testbed dataset for comparing model choices
    within the computer vision and deep network communities. MNIST instances contain
    $28\times 28$ pixel grayscale images of handwritten digits and their labels. The
    MNIST labels are drawn from 10 object classes, with a total of 6000 training samples
    and 1000 testing samples. The CIFAR-10 is also a well-known dataset containing
    10 object classes with approximately 5000 examples per class, where each sample
    is a $32\times 32$ pixel RGB image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Classification error rate ($\%$) comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset/Models | RF | ANT$\dagger$ [[129](#bib.bib129)] | DF [[93](#bib.bib93)]
    | R2SVM [[101](#bib.bib101)] | SFDT [[128](#bib.bib128)] | DNDF [[127](#bib.bib127)]
    | CondCNN [[145](#bib.bib145)] | DBT [[148](#bib.bib148)] | DNN+SVM [[150](#bib.bib150)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST | 3.21 | 0.29 | 0.74 | 4.03^∗ | 5.55 | 0.7 | - | 1.85 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 | 50.17 | 7.71 | 31.0 | 20.3 | - | - | 10.12 | 13.06 | 11.9 |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* Based on the authors’ code.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\dagger$ The reported result reflects an ANT ensemble.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the next set of experiments, we compare models designed to handle small
    or structured datasets, as shown in Table [3](#S7.T3 "Table 3 ‣ 7 Experiments
    ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization"). The authors of these methods tested a wide
    range of datasets to evaluate their their approach. In this survey, we conducted
    a series of experiments on the UCI human activity recognition (HAR) dataset [[165](#bib.bib165)].
    Here, human activity recognition data were collected from 30 participants performing
    six scripted activities (walking, walking upstairs, walking downstairs, sitting,
    standing, and laying) while wearing smartphones. The dataset contains 561 features
    extracted from sensors including an accelerometer and a gyroscope. The training
    set contains 7352 samples from 70% of the volunteers and the testing set contains
    2947 samples from the remaining 30% of volunteers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Classification error rate ($\%$) comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset/Models | RF | SVM | MLP | DART [[139](#bib.bib139)] | RRF [[135](#bib.bib135)]
    | GRRF [[136](#bib.bib136)] | mGBDT [[131](#bib.bib131)] |'
  prefs: []
  type: TYPE_TB
- en: '| HAR | 6.96 | 4.69 | 4.69 | 6.55 | 3.77 | 3.74 | 7.68 |'
  prefs: []
  type: TYPE_TB
- en: From the table, we observe that models representing multiple layers of machine
    learning models such as DF, R2SVM, and mGBDT did not perform well on the MNIST
    and CIFAR-10 datasets. Compared to DNNs, these models are computationally more
    expensive, require an excessive amount of resources, and do not offer advantages
    of interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Another category of models utilizes a neural network to learn a better representation
    of data. These models such as DNDF, and DNN+SVM applied a more traditional machine
    learning model on the newly-learned representation. This technique could be beneficial
    when the neural network has been trained on a large dataset. For example, DNDF
    utilized GoogLeNet for extracting features, and subsequently achieved a $6.38\%$
    error rate on the ImageNet testset. In contrast, the GoogLeNet error rate is $10.02\%$.
    Another class of models enhances the tree model by integrating artificial neurons
    such as ANT, SFDT, and DBT. These models cleverly combine neural networks with
    decision trees that improve interpretation while offering representation-learning
    benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Another hybrid strategy focused on decreasing the computational complexity of
    the DNNs. CondCNN is such a neural network that employs a routing mechanism similar
    to a decision tree to achieve this goal. Another successful line of research is
    to add regularizers frequently used by the neural network to other classifiers
    similar to DART, RRF, and GRRF.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results from our experiments reveal that both network classifiers and non-network
    classifiers benefit from deep learning. The methods surveyed in this paper and
    evaluated in these experiments demonstrate that non-network machine learning models
    do improve performance by incorporating DNN components into their algorithms.
    Whereas models without feature learning such as RF usually do not perform well
    on unstructured data such as images, we observe that adding deep learning to these
    models drastically improve their performance, as shown in Table [2](#S7.T2 "Table
    2 ‣ 7 Experiments ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization"). Additionally, non-deep
    models may achieve improved performance on structured data by adding regularizers,
    as shown in Table [3](#S7.T3 "Table 3 ‣ 7 Experiments ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization").
    The methods surveyed in this paper demonstrate that deep learning components can
    be added to any type of machine learning model, and are not specific to DNNs.
    The incorporation of deep learning strategies is a promising direction for all
    types of classifiers, both network and non-network methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions and Directions for Ongoing Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DNNs have emerged as a powerful force in the machine learning field for the
    past few years. This survey paper reviews the latest attempts to incorporate methods
    that are traditionally found in DNNs into other learning algorithms. DNNs work
    well when there is a large body of training data and available computational power.
    DNNs have consistently yielded strong results for a variety of datasets and competitions,
    such as winning the Large Scale Visual Recognition Challenge [[166](#bib.bib166)]
    and achieving strong results for energy demand prediction [[167](#bib.bib167)],
    identifying gender of a text author [[168](#bib.bib168)], stroke prediction [[169](#bib.bib169)],
    network intrusion detection [[170](#bib.bib170)], speech emotion recognition [[171](#bib.bib171)],
    and taxi destination prediction [[172](#bib.bib172)]. Since there are many applications
    which lack large amounts of training data or for which the interpretability of
    a learned model is important, there is a need to integrate the benefits of DNNs
    with other classifier algorithms. Other classifiers have demonstrated improved
    performance on some types of data, therefore the field can benefit from examining
    ways of combining deep learning elements between network and non-network methods.
  prefs: []
  type: TYPE_NORMAL
- en: Although some work to date provides evidence that DNN techniques can be used
    effectively by other classifiers, there are still many challenges that researchers
    need to address, both to improve DNNs and to extend deep learning to other types
    of classifiers. Based on our survey of existing work, some related areas where
    supervised learners can benefit from unique DNN methods are outlined below.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most characteristic feature of DNNs is a deep architecture and its ability
    to learn a new representation of data. A variety of stacked generalization methods
    have been developed to allow other machine learning methods to utilize deep architectures
    as well. These methods incorporate multiple classification steps in which the
    input of the next layer represents the concatenation of the output of the previous
    layer and the original feature vector as discussed in Section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Feedforward Learning ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization"). Future work can
    explore the many other possibilities that exist for refining the input features
    to each layer to better separate instances of each class at each layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies provide evidence that DNNs are effective data generators [[173](#bib.bib173)]
    [[174](#bib.bib174)], while in some cases non-network classifiers may actually
    be the better discriminators. Future research can consider using a DNN as a generator
    and an alternative classifier as a discriminator in generative adversarial models.
    Incorporating this type of model diversity could improve the robustness of the
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent can be applied to any differentiable algorithm. We observed
    that Kontschieder et al. [[127](#bib.bib127)], Frosst et al. [[128](#bib.bib128)],
    Tanno et al. [[129](#bib.bib129)], and Zoran et al. [[148](#bib.bib148)] all applied
    gradient descent to two different tree-based algorithms by making them differentiable.
    In the future, additional classifiers can be altered to be differentiable. Applying
    gradient descent to other algorithms could be an effective way to adjust the probability
    distribution of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another area which is vital to investigate is the application of network-customized
    regularization methods unique to non-network classifiers. As discussed in Section
    [5](#S5 "5 Deep Learning Regularization Outside of Deep Neural Networks ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"), the non-network classifiers can benefit from the regularization
    methods that are unique to DNNs. However, there exist many different ways that
    these regularization methods can be adapted by non-network classifiers to improve
    model generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: An important area of research is interpretable models. There exist applications
    such as credit score, insurance risk, health status because of their sensitivity,
    models need to be interpretable. Further research needs to exploit the use of
    DNNs in interpretable models such as DNDT [[130](#bib.bib130)].
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in this survey, an emerging area of research is to combine the
    complementary benefits of statistical models with neural networks. Statistical
    models offer mathematical formalisms as well as possible explanatory power. This
    combination may provide a more effective model than either approach used in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: There are cases in which the amount of ground truth-labeled data is limited,
    but a large body of labeled data from the same or similar distribution is available.
    One possible area of ongoing exploration is to couple the use of DNNs for learning
    from unlabeled data with the use of other classifier strategies for learning from
    labeled data. The simple model learned from labeled data can be exploited to further
    tune and improve learned representation patterns in the DNN.
  prefs: []
  type: TYPE_NORMAL
- en: We observe that currently, there is a general interest among the machine learning
    community to transfer new deep network developments to other classifiers. While
    a substantial effort has been made to incorporate deep learning ideas into the
    general machine learning field, continuing this work may spark the creation of
    new learning paradigms. However, the benefit between network-based learners and
    non-network learners can be bi-directional. Because a tremendous variety of classifiers
    has shown superior performance for a wide range of applications, future research
    can focus not only on how DNN techniques can improve non-network classifiers but
    on how DNNs can incorporate and benefit from non-network learning ideas as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The list of abbreviations and their descriptions utilized in this
    survey.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Abbreviation | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AE | Autoencoder |'
  prefs: []
  type: TYPE_TB
- en: '| ANT | Adaptive Neural Tree |'
  prefs: []
  type: TYPE_TB
- en: '| CNN | Convolutional Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| CondNN | Conditional Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| DART | Dropout Multiple Additive Regression Trees |'
  prefs: []
  type: TYPE_TB
- en: '| DBT | Differentiable Boundary Tree |'
  prefs: []
  type: TYPE_TB
- en: '| DBN | Deep Belief Network |'
  prefs: []
  type: TYPE_TB
- en: '| DCCA | Deep Canonical Correlation Analysis |'
  prefs: []
  type: TYPE_TB
- en: '| Deep PCA | Deep principal components analysis |'
  prefs: []
  type: TYPE_TB
- en: '| DF | Deep Forest |'
  prefs: []
  type: TYPE_TB
- en: '| DGP | Deep Gaussian Processes |'
  prefs: []
  type: TYPE_TB
- en: '| DKF | Deep Kalman Filters |'
  prefs: []
  type: TYPE_TB
- en: '| DNDT | Deep Network Decision Tree |'
  prefs: []
  type: TYPE_TB
- en: '| DNDF | Deep Network Decision Forest |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | Deep Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| DSVM | Deep SVM |'
  prefs: []
  type: TYPE_TB
- en: '| DTA-LS-SVM | Deep Transfer Additive Kernel Least Square SVM |'
  prefs: []
  type: TYPE_TB
- en: '| eForest | Encoder Forest |'
  prefs: []
  type: TYPE_TB
- en: '| FC | Fully Connected |'
  prefs: []
  type: TYPE_TB
- en: '| GAF | Generative Adversarial Forest |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | Generative Adversarial Network |'
  prefs: []
  type: TYPE_TB
- en: '| GRRF | Guided Regularized Random Forest |'
  prefs: []
  type: TYPE_TB
- en: '| LMM | Level-wise Mixture Model |'
  prefs: []
  type: TYPE_TB
- en: '| mGBDT | Multilayer Gradient Decision Tree |'
  prefs: []
  type: TYPE_TB
- en: '| ML-SVM | Multilayer SVM |'
  prefs: []
  type: TYPE_TB
- en: '| MLP | Multilayer perceptron |'
  prefs: []
  type: TYPE_TB
- en: '| NLP-SVM | Newton Linear Programming SVM |'
  prefs: []
  type: TYPE_TB
- en: '| R2-SVM | Random Recursive SVM |'
  prefs: []
  type: TYPE_TB
- en: '| RBM | Restricted Boltzmann Machine |'
  prefs: []
  type: TYPE_TB
- en: '| RNN | Recurrent Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| RRF | Regularized Random Forest |'
  prefs: []
  type: TYPE_TB
- en: '| SCAD-SVM | Smoothly Clipped Absolute Deviation SVM |'
  prefs: []
  type: TYPE_TB
- en: '| SDF | Siamese Deep Forest |'
  prefs: []
  type: TYPE_TB
- en: '| SNN | Siamese Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| FSDT | Frosst Soft Decision Tree |'
  prefs: []
  type: TYPE_TB
- en: '| VAE | Variational Autoencoder |'
  prefs: []
  type: TYPE_TB
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank Tharindu Adikari, Chris Choy, Ji Feng, Yani
    Ioannou, Stanislaw Jastrzebski and Marco A. Wiering for their valuable assistance
    in providing code and additional implementation details of the algorithms that
    were evaluated in this paper. We would also like to thank Samaneh Aminikhanghahi
    and Tinghui Wang for their feedback and guidance on the methods described in this
    survey. This material is based upon work supported by the National Science Foundation
    under Grant No. 1543656.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Cade Metz. AI is transforming google search. the rest of the web is next,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Nathan Sikes. Deep learning and the future of search engine optimization,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Alex Davies. The numbers don’t lie: Self-driving cars are getting good,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Neal Boudette. Tesla’s self-driving system cleared in deadly crash, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Autonomous vehicle disengagement reports 2017, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Fei Jiang, Yong Jiang, Hui Zhi, Yi Dong, Hao Li, Sufeng Ma, Yilong Wang,
    Qiang Dong, Haipeng Shen, and Yongjun Wang. Artificial intelligence in healthcare:
    past, present and future. Stroke and vascular neurology, 2(4):230–243, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Liang-Chieh Chen and Yukun Zhu. Semantic image segmentation with deeplab
    in tensorflow, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization
    of neural networks using dropconnect. In International Conference on Machine Learning,
    pages 1058–1066, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Benjamin Graham. Fractional max-pooling. arXiv preprint arXiv:1412.6071,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and
    accurate deep network learning by exponential linear units (elus). arXiv preprint
    arXiv:1511.07289, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt,
    Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.
    In Advances in Neural Information Processing Systems, pages 1693–1701, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,
    and Chris Dyer. Neural architectures for named entity recognition. arXiv preprint
    arXiv:1603.01360, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and
    Yoshua Bengio. End-to-end attention-based large vocabulary speech recognition.
    In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference
    on, pages 4945–4949\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai,
    Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang
    Chen, et al. Deep speech 2: End-to-end speech recognition in english and mandarin.
    In International Conference on Machine Learning, pages 173–182, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Thuy Ong. Amazon’s new algorithm designs clothing by analyzing a bunch
    of pictures, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Chenghui Tang, Yishen Wang, Jian Xu, Yuanzhang Sun, and Baosen Zhang.
    Efficient scenario generation of multiple renewable power plants considering spatial
    and temporal correlations. Applied Energy, 221:348–357, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie
    Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital
    30-day readmission. In Proceedings of the 21th ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining, pages 1721–1730\. ACM, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily
    fooled: High confidence predictions for unrecognizable images. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427–436,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
    Understanding deep learning requires rethinking generalization. arXiv preprint
    arXiv:1611.03530, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] David Krueger, Nicolas Ballas, Stanislaw Jastrzebski, Devansh Arpit, Maxinder S
    Kanwal, Tegan Maharaj, Emmanuel Bengio, Asja Fischer, and Aaron Courville. Deep
    nets don’t learn via memorization. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint
    arXiv:1511.06422, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Siddharth Krishna Kumar. On weight initialization in deep neural networks.
    arXiv preprint arXiv:1704.08863, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and
    trends® in Machine Learning, 2(1):1–127, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1–9, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,
    Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s
    neural machine translation system: Bridging the gap between human and machine
    translation. arXiv preprint arXiv:1609.08144, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui
    Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition
    with deep recurrent neural networks. In 2013 IEEE international conference on
    acoustics, speech and signal processing, pages 6645–6649\. IEEE, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks
    for human action recognition. IEEE transactions on pattern analysis and machine
    intelligence, 35(1):221–231, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] David H Wolpert, William G Macready, et al. No free lunch theorems for
    optimization. IEEE transactions on evolutionary computation, 1(1):67–82, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Ross D. King, Cao Feng, and Alistair Sutherland. Statlog: comparison of
    classification algorithms on large real-world problems. Applied Artificial Intelligence
    an International Journal, 9(3):289–333, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Tjen-Sien Lim, Wei-Yin Loh, and Yu-Shan Shih. A comparison of prediction
    accuracy, complexity, and training time of thirty-three old and new classification
    algorithms. Machine learning, 40(3):203–228, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of
    supervised learning algorithms. In Proceedings of the 23rd international conference
    on Machine learning, pages 161–168\. ACM, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Rich Caruana, Nikos Karampatziakis, and Ainur Yessenalina. An empirical
    evaluation of supervised learning in high dimensions. In Proceedings of the 25th
    international conference on Machine learning, pages 96–103\. ACM, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Philipp Baumann, DS Hochbaum, and YT Yang. A comparative study of the
    leading machine learning techniques and two new optimization algorithms. European
    journal of operational research, 272(3):1041–1057, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa
    Reyes, Mei-Ling Shyu, Shu-Ching Chen, and SS Iyengar. A survey on deep learning:
    Algorithms, techniques, and applications. ACM Computing Surveys (CSUR), 51(5):92,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Benjamin Shickel, Patrick James Tighe, Azra Bihorac, and Parisa Rashidi.
    Deep ehr: A survey of recent advances in deep learning techniques for electronic
    health record (ehr) analysis. IEEE journal of biomedical and health informatics,
    22(5):1589–1604, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Junwei Han, Dingwen Zhang, Gong Cheng, Nian Liu, and Dong Xu. Advanced
    deep-learning techniques for salient and category-specific object detection: a
    survey. IEEE Signal Processing Magazine, 35(1):84–100, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, and Lisha Hu. Deep
    learning for sensor-based activity recognition: A survey. Pattern Recognition
    Letters, 119:3–11, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] William Grant Hatcher and Wei Yu. A survey of deep learning: platforms,
    applications and emerging research trends. IEEE Access, 6:24411–24432, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Frank Rosenblatt. The perceptron: a probabilistic model for information
    storage and organization in the brain. Psychological review, 65(6):386, 1958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. Technical
    report, Stanford Univ Ca Stanford Electronics Labs, 1960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Marvin Minsky and Seymour A Papert. Perceptrons: An introduction to computational
    geometry. MIT press, 1969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning
    internal representations by error propagation. Technical report, California Univ
    San Diego La Jolla Inst for Cognitive Science, 1985.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm
    for deep belief nets. Neural computation, 18(7):1527–1554, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv
    preprint arXiv:1410.5401, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between
    capsules. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett, editors, Advances in Neural Information Processing Systems 30,
    pages 3856–3866\. Curran Associates, Inc., 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep
    learning, volume 1. MIT press Cambridge, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Yann LeCun et al. Generalization and network design strategies. Connectionism
    in perspective, pages 143–155, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou,
    and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems
    25, pages 1097–1105\. Curran Associates, Inc., 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Yi-Tong Zhou and Rama Chellappa. Computation of optical flow using a neural
    network. In IEEE International Conference on Neural Networks, volume 1998, pages
    71–78, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using rnn encoder-decoder for statistical machine translation. arXiv preprint
    arXiv:1406.1078, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface:
    Closing the gap to human-level performance in face verification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 1701–1708,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Rachid Riad, Corentin Dancette, Julien Karadayi, Neil Zeghidour, Thomas
    Schatz, and Emmanuel Dupoux. Sampling strategies in siamese networks for unsupervised
    speech representation learning. arXiv preprint arXiv:1804.11297, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Zara Alaverdyan, Julien Jung, Romain Bouet, and Carole Lartizien. Regularized
    siamese neural network for unsupervised outlier detection on brain multiparametric
    magnetic resonance imaging: application to epilepsy lesion screening. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Arthur L Samuel. Some studies in machine learning using the game of checkers.
    IBM Journal of research and development, 3(3):210–229, 1959.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,
    editors, Advances in Neural Information Processing Systems 27, pages 2672–2680\.
    Curran Associates, Inc., 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele,
    and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint
    arXiv:1605.05396, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Artur Kadurin, Alexander Aliper, Andrey Kazennov, Polina Mamoshina, Quentin
    Vanhaelen, Kuzma Khrabrov, and Alex Zhavoronkov. The cornucopia of meaningful
    leads: Applying deep adversarial autoencoders for new molecule development in
    oncology. Oncotarget, 8(7):10883, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Wengling Chen and James Hays. Sketchygan: Towards diverse and realistic
    sketch to image synthesis. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 9416–9425, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Gaugan: semantic
    image synthesis with spatially adaptive normalization. In ACM SIGGRAPH 2019 Real-Time
    Live!, page 2\. ACM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality
    of data with neural networks. science, 313(5786):504–507, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
    Extracting and composing robust features with denoising autoencoders. In Proceedings
    of the 25th international conference on Machine learning, pages 1096–1103\. ACM,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization.
    In European Conference on Computer Vision, pages 649–666. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Detian Huang, Weiqin Huang, Zhenguo Yuan, Yanming Lin, Jian Zhang, and
    Lixin Zheng. Image super-resolution algorithm based on an improved sparse autoencoder.
    Information, 9(1):11, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko, and Yu-Chiang Frank Wang. Learning
    deep latent space for multi-label classification. In AAAI, pages 2838–2844, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv
    preprint arXiv:1312.6114, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Yoshua Bengio. Deep learning of representations: Looking forward. In International
    Conference on Statistical Language and Speech Processing, pages 1–37\. Springer,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the
    importance of initialization and momentum in deep learning. In International conference
    on machine learning, pages 1139–1147, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods
    for online learning and stochastic optimization. Journal of Machine Learning Research,
    12(Jul):2121–2159, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization:
    a review of algorithms and comparison of software implementations. Journal of
    Global Optimization, 56(3):1247–1293, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Arild Nøkland. Direct feedback alignment provides learning in deep neural
    networks. In Advances in neural information processing systems, pages 1037–1045,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal
    of the Royal Statistical Society. Series B (Methodological), pages 267–288, 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Yoav Freund. Boosting a weak learning algorithm by majority. Information
    and computation, 121(2):256–285, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Mohammad Moghimi, Serge J Belongie, Mohammad J Saberian, Jian Yang, Nuno
    Vasconcelos, and Li-Jia Li. Boosted convolutional neural networks. In BMVC, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Jesse Eickholt and Jianlin Cheng. Dndisorder: predicting protein disorder
    using boosting and deep networks. BMC bioinformatics, 14(1):88, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and
    Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting.
    The Journal of Machine Learning Research, 15(1):1929–1958, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Luis Perez and Jason Wang. The effectiveness of data augmentation in image
    classification using deep learning. arXiv preprint arXiv:1712.04621, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V
    Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Navdeep Jaitly and Geoffrey E Hinton. Vocal tract length perturbation
    (vtlp) improves speech recognition. In Proc. ICML Workshop on Deep Learning for
    Audio, Speech and Language, volume 117, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Hiroki OHASHI, Mohammad AL-NASER, Sheraz AHMED, Takayuki AKIYAMA, Takuto
    SATO, Phong NGUYEN, Katsuyuki NAKAMURA, and Andreas DENGEL. Augmenting wearable
    sensor data with physical constraint for dnn-based human-action recognition. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger
    Gunn, Alexander Hammers, David Alexander Dickie, Maria Valdés Hernández, Joanna
    Wardlaw, and Daniel Rueckert. Gan augmentation: Augmenting training data using
    generative adversarial networks. arXiv preprint arXiv:1810.10863, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation
    generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Javier Jorge, Jesús Vieco, Roberto Paredes, Joan-Andreu Sánchez, and José-Miguel
    Benedí. Empirical evaluation of variational autoencoders for data augmentation.
    In VISIGRAPP (5: VISAPP), pages 96–104, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun
    Wang, Site Li, Ping Jia, and Jane You. Data augmentation via latent space interpolation
    for image classification. In 2018 24th International Conference on Pattern Recognition
    (ICPR), pages 728–733\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Yichuan Tang and Chris Eliasmith. Deep networks for robust visual recognition.
    In Proceedings of the 27th International Conference on Machine Learning (ICML-10),
    pages 1055–1062\. Citeseer, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli. Analyzing noise in
    autoencoders and deep networks. arXiv preprint arXiv:1406.1831, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] David H Wolpert. Stacked generalization. Neural networks, 5(2):241–259,
    1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Kai Ming Ting and Ian H Witten. Issues in stacked generalization. Journal
    of artificial intelligence research, 10:271–289, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep
    neural networks. arXiv preprint arXiv:1702.08835, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Guanjin Wang, Guangquan Zhang, Kup-Sze Choi, and Jie Lu. Deep additive
    least squares support vector machines for classification with model transfer.
    IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Leo Breiman. Randomizing outputs to increase prediction accuracy. Machine
    Learning, 40(3):229–242, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Tin Kam Ho. Random decision forests. In Document analysis and recognition,
    1995., proceedings of the third international conference on, volume 1, pages 278–282\.
    IEEE, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
    com/exdb/mnist/, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Gavin C Cawley. Leave-one-out cross-validation based model selection criteria
    for weighted ls-svms. In Neural Networks, 2006\. IJCNN’06\. International Joint
    Conference on, pages 1661–1668\. IEEE, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Hao Yang and Jianxin Wu. Practical large scale classification with additive
    kernels. In Asian Conference on Machine Learning, pages 523–538, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Azizi Abdullah, Remco C Veltkamp, and Marco A Wiering. An ensemble of
    deep support vector machines for image categorization. In Soft Computing and Pattern
    Recognition, 2009\. SOCPAR’09. International Conference of, pages 301–306\. IEEE,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Oriol Vinyals, Yangqing Jia, Li Deng, and Trevor Darrell. Learning with
    recursive perceptual representations. In Advances in Neural Information Processing
    Systems, pages 2825–2833, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Li Deng, Dong Yu, and John Platt. Scalable stacking and learning for
    building deep architectures. In Acoustics, Speech and Signal Processing (ICASSP),
    2012 IEEE International Conference on, pages 2133–2136\. IEEE, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Brian Hutchinson, Li Deng, and Dong Yu. Tensor deep stacking networks.
    IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1944–1957,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Li Deng and Dong Yu. Deep convex net: A scalable architecture for speech
    pattern classification. In Twelfth Annual Conference of the International Speech
    Communication Association, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Venice Erin Liong, Jiwen Lu, and Gang Wang. Face recognition using deep
    pca. In Information, Communications and Signal Processing (ICICS) 2013 9th International
    Conference on, pages 1–5\. IEEE, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features
    from tiny images. Technical report, Citeseer, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Jonathon Shlens. A tutorial on principal component analysis. arXiv preprint
    arXiv:1404.1100, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial
    Intelligence and Statistics, pages 207–215, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Ilya Sutskever and Geoffrey Hinton. Learning multilevel distributed representations
    for high-dimensional sequences. In Artificial intelligence and statistics, pages
    548–555, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Roland Memisevic and Geoffrey Hinton. Unsupervised learning of image
    transformations. In 2007 IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1–8\. IEEE, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Kevin Swersky, Ilya Sutskever, Daniel Tarlow, Richard S Zemel, Ruslan R
    Salakhutdinov, and Ryan P Adams. Cardinality restricted boltzmann machines. In
    Advances in neural information processing systems, pages 3293–3301, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer
    School on Machine Learning, pages 63–71\. Springer, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] César Lincoln C Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth,
    Guilherme A Barreto, and Neil D Lawrence. Recurrent gaussian processes. arXiv
    preprint arXiv:1511.06644, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Mark Van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional
    gaussian processes. In Advances in Neural Information Processing Systems, pages
    2849–2858, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Zhenwen Dai, Andreas Damianou, Javier González, and Neil Lawrence. Variational
    auto-encoded deep gaussian processes. arXiv preprint arXiv:1511.06455, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Andreas Damianou. Deep Gaussian processes and variational propagation
    of uncertainty. PhD thesis, University of Sheffield, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Matthew M Dunlop, Mark A Girolami, Andrew M Stuart, and Aretha L Teckentrup.
    How deep are deep gaussian processes? The Journal of Machine Learning Research,
    19(1):2100–2145, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding
    pathologies in very deep networks. In Artificial Intelligence and Statistics,
    pages 202–210, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Lev V Utkin and Mikhail A Ryabinin. A siamese deep forest. arXiv preprint
    arXiv:1704.08715, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Yan Zuo, Gil Avraham, and Tom Drummond. Generative adversarial forests
    for better conditioned adversarial learning. arXiv preprint arXiv:1805.05185,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Rudolph Emil Kalman. A new approach to linear filtering and prediction
    problems. Journal of basic Engineering, 82(1):35–45, 1960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Rahul G. Krishnan, Uri Shalit, and David Sontag. Deep kalman filters.
    arXiv preprint arXiv:1511.05121, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Shirli Di-Castro Shashua and Shie Mannor. Deep robust kalman filter.
    arXiv preprint arXiv:1703.02310, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Zhiyong Gao, and Ming-Ting
    Sun. Deep kalman filtering network for video compression artifact reduction. In
    Proceedings of the European Conference on Computer Vision (ECCV), pages 568–584,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Ji Feng and Zhi-Hua Zhou. Autoencoder by forest. arXiv preprint arXiv:1709.09018,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Marco A Wiering and Lambert RB Schomaker. Multi-layer support vector
    machines. Regularization, optimization, kernels, and support vector machines,
    page 457, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota
    Bulo. Deep neural decision forests. In Computer Vision (ICCV), 2015 IEEE International
    Conference on, pages 1467–1475\. IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into
    a soft decision tree. arXiv preprint arXiv:1711.09784, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Ryutaro Tanno, Kai Arulkumaran, Daniel C Alexander, Antonio Criminisi,
    and Aditya Nori. Adaptive neural trees. arXiv preprint arXiv:1807.06699, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Yongxin Yang, Irene Garcia Morillo, and Timothy M Hospedales. Deep neural
    decision trees. arXiv preprint arXiv:1806.06988, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Ji Feng, Yang Yu, and Zhi-Hua Zhou. Multi-layered gradient boosting decision
    trees. arXiv preprint arXiv:1806.00007, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Glenn M Fung and Olvi L Mangasarian. A feature selection newton method
    for support vector machine classification. Computational optimization and applications,
    28(2):185–202, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Hao Helen Zhang, Jeongyoun Ahn, Xiaodong Lin, and Cheolwoo Park. Gene
    selection using support vector machines with non-convex penalty. bioinformatics,
    22(1):88–95, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized
    likelihood and its oracle properties. Journal of the American statistical Association,
    96(456):1348–1360, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Houtao Deng and George Runger. Feature selection via regularized trees.
    In Neural Networks (IJCNN), The 2012 International Joint Conference on, pages
    1–8\. IEEE, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Houtao Deng and George Runger. Gene selection with guided regularized
    random forest. Pattern Recognition, 46(12):3483–3489, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good
    sentiment and topic classification. In Proceedings of the 50th Annual Meeting
    of the Association for Computational Linguistics: Short Papers-Volume 2, pages
    90–94\. Association for Computational Linguistics, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Sida Wang and Christopher Manning. Fast dropout training. In international
    conference on machine learning, pages 118–126, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Rashmi Korlakai Vinayak and Ran Gilad-Bachrach. Dart: Dropouts meet multiple
    additive regression trees. In Artificial Intelligence and Statistics, pages 489–497,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Jerome H Friedman. Greedy function approximation: a gradient boosting
    machine. Annals of statistics, pages 1189–1232, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Jerome H Friedman. Stochastic gradient boosting. Computational Statistics
    & Data Analysis, 38(4):367–378, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego,
    and Salvatore Trani. X-dart: Blending dropout and pruning for efficient learning
    to rank. In Proceedings of the 40th International ACM SIGIR Conference on Research
    and Development in Information Retrieval, pages 1077–1080\. ACM, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Sebastien C Wong, Adam Gatt, Victor Stamatescu, and Mark D McDonnell.
    Understanding data augmentation for classification: when to warp? arXiv preprint
    arXiv:1609.08764, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Ruo Xu. Improvements to random forest methodology. 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie
    Shotton, Matthew Brown, and Antonio Criminisi. Decision forests, convolutional
    networks and the models in-between. arXiv preprint arXiv:1603.01250, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
    Weight uncertainty in neural networks. arXiv preprint arXiv:1505.05424, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Meire Fortunato, Charles Blundell, and Oriol Vinyals. Bayesian recurrent
    neural networks. CoRR, abs/1704.02798, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Daniel Zoran, Balaji Lakshminarayanan, and Charles Blundell. Learning
    deep nearest neighbor representations using differentiable boundary trees. arXiv
    preprint arXiv:1702.08833, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Charles Mathy, Nate Derbinsky, José Bento, Jonathan Rosenthal, and Jonathan S
    Yedidia. The boundary forest algorithm for online supervised and unsupervised
    learning. In AAAI, pages 2864–2870, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Yichuan Tang. Deep learning using linear support vector machines. arXiv
    preprint arXiv:1306.0239, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian
    institute for advanced research).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Xiao-Xiao Niu and Ching Y Suen. A novel hybrid cnn–svm classifier for
    recognizing handwritten digits. Pattern Recognition, 45(4):1318–1325, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Masoumeh Zareapoor, Pourya Shamsolmoali, Deepak Kumar Jain, Haoxiang
    Wang, and Jie Yang. Kernelized support vector machine with deep learning: an efficient
    approach for extreme multiclass dataset. Pattern Recognition Letters, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Jawad Nagi, Gianni A Di Caro, Alessandro Giusti, Farrukh Nagi, Luca Maria
    Gambardella, et al. Convolutional neural support vector machines: Hybrid visual
    pattern classifiers for multi-robot systems. In ICMLA (1), pages 27–32, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Abdel Bellili, Michel Gilloux, and Patrick Gallinari. An hybrid mlp-svm
    handwritten digit recognizer. In Document Analysis and Recognition, 2001\. Proceedings.
    Sixth International Conference on, pages 28–32\. IEEE, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Washington W Azevedo and Cleber Zanchet. A mlp-svm hybrid model for cursive
    handwriting recognition. In Neural Networks (IJCNN), The 2011 International Joint
    Conference on, pages 843–850\. IEEE, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Tianyi Zhao, Baopeng Zhang, Ming He, Wei Zhanga, Ning Zhou, Jun Yu, and
    Jianping Fan. Embedding visual hierarchy with deep networks for large-scale visual
    recognition. IEEE Transactions on Image Processing, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Harold Hotelling. Relations between two sets of variates. In Breakthroughs
    in statistics, pages 162–190\. Springer, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] David R Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical correlation
    analysis: An overview with application to learning methods. Neural computation,
    16(12):2639–2664, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical
    correlation analysis. In International conference on machine learning, pages 1247–1255,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic
    representations from tree-structured long short-term memory networks. arXiv preprint
    arXiv:1503.00075, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] David Alvarez-Melis and Tommi S Jaakkola. Tree-structured decoding with
    doubly-recurrent neural networks. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Andrea Cimino and Felice Dell’Orletta. Tandem lstm-svm approach for sentiment
    analysis. In of the Final Workshop 7 December 2016, Naples, page 172, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Abien Fred M Agarap. A neural network architecture combining gated recurrent
    unit (gru) and support vector machine (svm) for intrusion detection in network
    traffic data. In Proceedings of the 2018 10th International Conference on Machine
    Learning and Computing, pages 26–30\. ACM, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis
    Reyes-Ortiz. A public domain dataset for human activity recognition using smartphones.
    In ESANN, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Large scale visual recognition challenge 2017 (ilsvrc2017), 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Nikolaos G Paterakis, Elena Mocanu, Madeleine Gibescu, Bart Stappers,
    and Walter van Alst. Deep learning versus traditional machine learning methods
    for aggregated energy demand prediction. In Innovative Smart Grid Technologies
    Conference Europe (ISGT-Europe), 2017 IEEE PES, pages 1–6\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Alexander Sboev, Ivan Moloshnikov, Dmitry Gudovskikh, Anton Selivanov,
    Roman Rybka, and Tatiana Litvinova. Deep learning neural nets versus traditional
    machine learning in gender identification of authors of rusprofiling texts. Procedia
    Computer Science, 123:424–431, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Chen-Ying Hung, Wei-Chen Chen, Po-Tsun Lai, Ching-Heng Lin, and Chi-Chun
    Lee. Comparing deep neural network and other machine learning algorithms for stroke
    prediction in a large-scale population-based electronic medical claims database.
    In Engineering in Medicine and Biology Society (EMBC), 2017 39th Annual International
    Conference of the IEEE, pages 3110–3113\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Chuanlong Yin, Yuefei Zhu, Jinlong Fei, and Xinzheng He. A deep learning
    approach for intrusion detection using recurrent neural networks. IEEE Access,
    5:21954–21961, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Haytham M Fayek, Margaret Lech, and Lawrence Cavedon. Evaluating deep
    learning architectures for speech emotion recognition. Neural Networks, 92:60–68,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Alexandre De Brébisson, Étienne Simon, Alex Auvolat, Pascal Vincent,
    and Yoshua Bengio. Artificial neural networks applied to taxi destination prediction.
    arXiv preprint arXiv:1508.00021, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation
    learning with deep convolutional generative adversarial networks. arXiv preprint
    arXiv:1511.06434, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate
    Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial
    domain adaptation. arXiv preprint arXiv:1711.03213, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
