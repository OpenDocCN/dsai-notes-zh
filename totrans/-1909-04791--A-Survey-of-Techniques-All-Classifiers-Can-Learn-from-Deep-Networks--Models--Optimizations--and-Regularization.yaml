- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:04:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:04:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1909.04791] A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1909.04791] 从深度网络中所有分类器可以学习的技术调查：模型、优化和正则化'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1909.04791](https://ar5iv.labs.arxiv.org/html/1909.04791)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1909.04791](https://ar5iv.labs.arxiv.org/html/1909.04791)
- en: 'A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从深度网络中所有分类器可以学习的技术调查：模型、优化和正则化
- en: Alireza Ghods
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Alireza Ghods
- en: Department of Computer Science
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Washington State University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿州立大学
- en: Pullman, WA 99163
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 普尔曼, WA 99163
- en: alireza.ghods@wsu.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: alireza.ghods@wsu.edu
- en: '&Diane J. Cook'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Diane J. Cook'
- en: Department of Computer Science
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Washington State University
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿州立大学
- en: Pullman, WA 99163
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 普尔曼, WA 99163
- en: djcook@wsu.edu
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: djcook@wsu.edu
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep neural networks have introduced novel and useful tools to the machine learning
    community. Other types of classifiers can potentially make use of these tools
    as well to improve their performance and generality. This paper reviews the current
    state of the art for deep learning classifier technologies that are being used
    outside of deep neural networks. Non-network classifiers can employ many components
    found in deep neural network architectures. In this paper, we review the feature
    learning, optimization, and regularization methods that form a core of deep network
    technologies. We then survey non-neural network learning algorithms that make
    innovative use of these methods to improve classification. Because many opportunities
    and challenges still exist, we discuss directions that can be pursued to expand
    the area of deep learning for a variety of classification algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络为机器学习社区引入了新颖且有用的工具。其他类型的分类器也可以利用这些工具来提升其性能和泛化能力。本文回顾了当前在深度神经网络之外使用的深度学习分类器技术的最新进展。非网络分类器可以利用深度神经网络架构中发现的许多组件。本文回顾了构成深度网络技术核心的特征学习、优化和正则化方法。接着，我们调查了非神经网络学习算法如何创新性地使用这些方法来改善分类。由于仍然存在许多机会和挑战，我们讨论了可以探索的方向，以扩展深度学习在各种分类算法中的应用领域。
- en: '*K*eywords Deep Learning  $\cdot$ Deep Neural Networks,  $\cdot$ ,  $\cdot$
    Regularization'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 深度学习  $\cdot$ 深度神经网络  $\cdot$  $\cdot$ 正则化'
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The objective of supervised learning algorithms is to identify an optimal mapping
    between input features and output values based on a given training dataset. A
    supervised learning method that is attracting substantial research and industry
    attention is Deep Neural Networks (DNN). DNNs have a profound effect on our daily
    lives; they are found in search engines [[1](#bib.bib1)] [[2](#bib.bib2)], self-driving
    cars [[3](#bib.bib3)] [[4](#bib.bib4)] [[5](#bib.bib5)], health care systems [[6](#bib.bib6)],
    and consumer devices such as smart-phones and cameras [[7](#bib.bib7)]. Convolutional
    Neural Networks (CNN) have become the standard for processing images [[8](#bib.bib8)]
    [[9](#bib.bib9)] [[10](#bib.bib10)], whereas Recurrent Neural Networks (RNN) dominate
    the processing of sequential data such as text and voice [[11](#bib.bib11)] [[12](#bib.bib12)]
    [[13](#bib.bib13)] [[14](#bib.bib14)]. DNNs allow machines to automatically discover
    the representations needed for detection or classification of raw input [[15](#bib.bib15)].
    Additionally, the neural network community developed unsupervised algorithms to
    help with the learning of unlabeled data. These unsupervised methods have found
    their way to real-world applications, such as creating generative adversarial
    networks (GANs) that design clothes [[16](#bib.bib16)]. The term deep has been
    used to distinguish these networks from shallow networks which have only one hidden
    layer; in contrast, DNNs have multiple hidden layers. The two terms deep learning
    and deep neural networks have been used synonymously. However, we observe that
    deep leaning itself conveys a broader meaning, which can also shape the field
    of machine learning outside the realm of neural network algorithms.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法的目标是根据给定的训练数据集，识别输入特征与输出值之间的最佳映射。一个正在引起大量研究和工业关注的监督学习方法是**深度神经网络**（DNN）。DNN
    对我们的日常生活产生了深远的影响；它们被应用于搜索引擎 [[1](#bib.bib1)] [[2](#bib.bib2)]、自动驾驶汽车 [[3](#bib.bib3)]
    [[4](#bib.bib4)] [[5](#bib.bib5)]、医疗保健系统 [[6](#bib.bib6)]，以及智能手机和相机等消费设备 [[7](#bib.bib7)]。卷积神经网络（CNN）已成为处理图像的标准
    [[8](#bib.bib8)] [[9](#bib.bib9)] [[10](#bib.bib10)]，而递归神经网络（RNN）主导了对文本和语音等序列数据的处理
    [[11](#bib.bib11)] [[12](#bib.bib12)] [[13](#bib.bib13)] [[14](#bib.bib14)]。DNN
    使机器能够自动发现用于检测或分类原始输入所需的表示 [[15](#bib.bib15)]。此外，神经网络社区开发了无监督算法以帮助学习未标记的数据。这些无监督方法已经在实际应用中找到了用武之地，例如创建生成对抗网络（GANs）来设计服装
    [[16](#bib.bib16)]。术语“深度”被用来区分这些网络与只有一个隐藏层的浅层网络；相比之下，DNN 具有多个隐藏层。术语“深度学习”和“深度神经网络”通常是同义的。然而，我们观察到“深度学习”本身具有更广泛的含义，这也可以塑造神经网络算法之外的机器学习领域。
- en: The remarkable recent DNN advances were made possible by the availability of
    massive amounts of computational power and labeled data. However, these advances
    do not overcome all of the difficulties associated with DNNs. For example, there
    are many real-world scenarios, such as analyzing power distribution data [[17](#bib.bib17)],
    for which large annotated datasets do not exist due to the complexity and expense
    of collecting data. While applications like clinical interpretation of medical
    diagnoses require that the learned model be understandable, most DNNs resist interpretation
    due to their complexity [[18](#bib.bib18)]. DNNs can be insensitive to noisy training
    data [[19](#bib.bib19)] [[20](#bib.bib20)] [[21](#bib.bib21)], and they also require
    appropriate parameter initialization to converge [[22](#bib.bib22)] [[23](#bib.bib23)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最近深度神经网络的显著进展得益于海量计算能力和标记数据的可用性。然而，这些进展并未解决与 DNN 相关的所有困难。例如，在许多现实场景中，例如分析电力分配数据
    [[17](#bib.bib17)]，由于数据收集的复杂性和成本，存在大量的注释数据集。而像临床医学诊断的解读这类应用需要学习的模型是可理解的，但由于复杂性，大多数
    DNN 仍然难以解释 [[18](#bib.bib18)]。DNN 对噪声训练数据可能不够敏感 [[19](#bib.bib19)] [[20](#bib.bib20)]
    [[21](#bib.bib21)]，同时它们还需要适当的参数初始化以便收敛 [[22](#bib.bib22)] [[23](#bib.bib23)]。
- en: Despite these shortcomings, DNNs have reported higher predictive accuracy than
    other supervised learning methods for many datasets, given enough supervised data
    and computational resources. Deep models offer structural advantages that may
    improve the quality of learning in complex datasets as empirically shown by Bengio
    [[24](#bib.bib24)]. Recently, researchers have designed hybrid methods which combine
    unique DNN techniques with other classifiers to address some of these identified
    problems or to boost other classifiers. This survey paper investigates these methods,
    reviewing classifiers which have adapted DNN techniques to alternative classifiers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些不足，但 DNN 在许多数据集上报告的预测准确率高于其他监督学习方法，只要有足够的监督数据和计算资源。深度模型提供了结构上的优势，这可能提高在复杂数据集上的学习质量，正如
    Bengio 通过实证研究所示[[24](#bib.bib24)]。最近，研究人员设计了混合方法，将独特的 DNN 技术与其他分类器相结合，以解决一些已识别的问题或提升其他分类器。本文综述了这些方法，回顾了将
    DNN 技术适应到其他分类器的分类器。
- en: 1.1 Research Objectives and Outline
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 研究目标与大纲
- en: While DNN research is growing rapidly, this paper aims to draw a broader picture
    of deep learning methods. Although some studies provide evidence that DNN models
    offer greater generalization than classic machine learning algorithms for complex
    data [[25](#bib.bib25)] [[26](#bib.bib26)] [[27](#bib.bib27)] [[28](#bib.bib28)]
    [[29](#bib.bib29)], there is no “silver bullet” approach to concept learning [[30](#bib.bib30)].
    Numerous studies comparing DNNs and other supervised learning algorithms [[31](#bib.bib31)]
    [[32](#bib.bib32)] [[33](#bib.bib33)] [[34](#bib.bib34)] [[35](#bib.bib35)] observe
    that the choice of algorithm depends on the data - no ideal algorithm exists which
    generalizes optimally on all types of data. Recognizing the unique and important
    role other classifiers thus play, we aim to investigate how non-network machine
    learning algorithms can benefit from the advances in deep neural networks. Many
    deep learning survey papers have been published that provide a primer on the topic
    [[36](#bib.bib36)] or highlight diverse applications such as object detection
    [[37](#bib.bib37)], medical record analysis [[38](#bib.bib38)], activity recognition
    [[39](#bib.bib39)], and natural language processing [[40](#bib.bib40)]. In this
    survey, we do not focus solely on deep neural network models but rather on how
    deep learning can inspire a broader range of classifiers. We concentrate on research
    breakthroughs that transform non-network classifiers into deep learners. Further,
    we review deep network techniques such as stochastic gradient descent that can
    be used more broadly, and we discuss ways in which non-network models can benefit
    from network-inspired deep learning innovations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度神经网络（DNN）研究发展迅速，但本文旨在对深度学习方法进行更广泛的概述。尽管一些研究提供了证据，表明 DNN 模型在处理复杂数据时比经典机器学习算法具有更好的泛化能力[[25](#bib.bib25)]
    [[26](#bib.bib26)] [[27](#bib.bib27)] [[28](#bib.bib28)] [[29](#bib.bib29)]，但并没有一种“万灵药”式的方法来进行概念学习[[30](#bib.bib30)]。大量比较
    DNN 和其他监督学习算法的研究[[31](#bib.bib31)] [[32](#bib.bib32)] [[33](#bib.bib33)] [[34](#bib.bib34)]
    [[35](#bib.bib35)]观察到，算法的选择依赖于数据——没有一种理想的算法可以在所有类型的数据上都能做到最佳泛化。因此，我们认识到其他分类器所扮演的独特而重要的角色，旨在研究非网络机器学习算法如何从深度神经网络的进展中受益。许多深度学习综述论文已经出版，提供了关于这一主题的入门介绍[[36](#bib.bib36)]，或强调了如目标检测[[37](#bib.bib37)]、医疗记录分析[[38](#bib.bib38)]、活动识别[[39](#bib.bib39)]和自然语言处理[[40](#bib.bib40)]等多种应用。在这项综述中，我们不仅关注深度神经网络模型，还关注深度学习如何激发更广泛的分类器。我们专注于将非网络分类器转变为深度学习者的研究突破。此外，我们回顾了诸如随机梯度下降等深度网络技术，这些技术可以被更广泛地使用，并讨论了非网络模型如何受益于受网络启发的深度学习创新。
- en: 'The literature provides evidence that non-network models may offer improved
    generalizability over deep networks, depending on the amount and type of data
    that is available. By surveying methods for transforming non-network classifiers
    into deep learners, these approaches can become stronger learners. To provide
    evidence of the need for continued research on this topic, we also implement a
    collection of shallow and deep learners surveyed in this paper, both network and
    non-network classifiers, to compare their performance. Figure [1](#S1.F1 "Figure
    1 ‣ 1.1 Research Objectives and Outline ‣ 1 Introduction ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization")
    highlights deep learning components that we discuss in this survey. This graph
    also summarizes the deep classifiers that we survey and the relationships that
    we highlight between techniques.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 文献提供的证据表明，与深度网络相比，非网络模型可能在泛化能力上有所改善，具体取决于可用数据的数量和类型。通过调查将非网络分类器转变为深度学习者的方法，这些方法可以成为更强的学习者。为了提供持续研究这一主题的必要性的证据，我们还实现了一组浅层和深层学习者，这些学习者包括网络和非网络分类器，以比较它们的性能。图[1](#S1.F1
    "图1 ‣ 1.1 研究目标和大纲 ‣ 1 介绍 ‣ 关于深度网络的所有分类器学习的技术调查：模型、优化和正则化")突出了我们在本次调查中讨论的深度学习组件。该图还总结了我们调查的深度分类器以及我们强调的技术之间的关系。
- en: '{forest}'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: for tree= font=, draw, semithick, rounded corners, align = center, inner sep
    = 1mm, edge = semithick, -stealth, l sep=0.5cm, s sep=0.2cm, fork sep=1mm, parent
    anchor=south, child anchor=north, edge path= [-Stealth[], \forestoptionedge, thin]
    (!u.parent anchor) – +(0,-5pt) -| (.child anchor)\forestoptionedge label; , /tikz/>=LaTeX,
    , [Deep Learning, [Training Methodology, for tree=fill=gray!20 [Optimization,
    for tree=folder,grow’=0 [Gradient Decent [DNDF] [mGBDT] [ML-SVM] ] ] [Regularization,
    for tree=folder,grow’=0 [Feature Penalty [GRRF] [NLP-SVM] [RRF] [SCAD-SVM] ] [Dropout
    [DART] ] ] ] [Classifiers, for tree=fill=pink!20 [Network-Based, for tree=folder,grow’=0
    [AE [VAE] ] [CNN] [GAN] [MLP] [SNN] [RNN [LSTM] [GRU] ] ] [Tree-Based, for tree=folder,grow’=0
    [ANT] [DF] [DNDF] [eForest] [FSDT] [GAF] [mGBDT] [SDF] [DNDT] ] [SVM-Based, for
    tree=folder,grow’=0 [Deep SVM] [DTA-LS-SVM] [ML-SVM] [R2SVM] ] [Statistical-Based,
    for tree=folder,grow’=0 [DBN] [DGP] [DKF] ] [Hybrid, for tree=folder,grow’=0 [CondNN]
    [DBT] [DCCA] [Deep PCA] [DNDF] [LMM] ] ] ]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树形图：字体=，绘制，半粗，圆角，居中对齐，内边距=1mm，边缘=半粗，-stealth，l sep=0.5cm，s sep=0.2cm，fork
    sep=1mm，父锚点=south，子锚点=north，边缘路径= [-Stealth[], \forestoptionedge，细] (!u.parent
    anchor) – +(0,-5pt) -| (.child anchor)\forestoptionedge 标签；，/tikz/>=LaTeX，，[深度学习，[训练方法，树形图=填充=gray!20
    [优化，树形图=文件夹，grow’=0 [梯度下降 [DNDF] [mGBDT] [ML-SVM] ] ] [正则化，树形图=文件夹，grow’=0 [特征惩罚
    [GRRF] [NLP-SVM] [RRF] [SCAD-SVM] ] [丢弃 [DART] ] ] ] [分类器，树形图=填充=pink!20 [基于网络的，树形图=文件夹，grow’=0
    [AE [VAE] ] [CNN] [GAN] [MLP] [SNN] [RNN [LSTM] [GRU] ] ] [基于树的，树形图=文件夹，grow’=0
    [ANT] [DF] [DNDF] [eForest] [FSDT] [GAF] [mGBDT] [SDF] [DNDT] ] [基于SVM的，树形图=文件夹，grow’=0
    [深度SVM] [DTA-LS-SVM] [ML-SVM] [R2SVM] ] [基于统计的，树形图=文件夹，grow’=0 [DBN] [DGP] [DKF]
    ] [混合，树形图=文件夹，grow’=0 [CondNN] [DBT] [DCCA] [深度PCA] [DNDF] [LMM] ] ] ]
- en: 'Figure 1: Content map of the methods covered in this survey.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本次调查中涵盖的方法内容图。
- en: 2 Brief Overview of Deep Neural Networks
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度神经网络的简要概述
- en: 2.1 The Origin
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 起源
- en: 'In 1985, Rosenblatt introduced the Perceptron [[41](#bib.bib41)], an online
    binary classifier which flows input through a weight vector to an output layer.
    Perceptron learning uses a form of gradient descent to adjust the weights between
    the input and output layers to optimize a loss function [[42](#bib.bib42)]. A
    few years later, Minsky proved that a single-layer Perceptron is unable to learn
    nonlinear functions, including the XOR function [[43](#bib.bib43)]. Multilayer
    perceptrons (MLPs, see Table [4](#S8.T4 "Table 4 ‣ 8 Conclusions and Directions
    for Ongoing Research ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization") for a complete list of abbreviations)
    addressed the nonlinearity problem by adding layers of hidden units to the networks
    and applying alternative differentiable activation functions, such as sigmoid,
    to each node. Stochastic gradient descent was then applied to MLPs to determine
    the weights between layers that minimize function approximation errors [[44](#bib.bib44)].
    However, the lack of computational power caused DNN research to stagnate for decades,
    and other classifiers rose in popularity. In 2006, a renaissance began in DNN
    research, spurred by the introduction of Deep Belief Networks (DBNs) [[45](#bib.bib45)].'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '1985年，Rosenblatt 介绍了感知机 [[41](#bib.bib41)]，这是一种在线二分类器，通过权重向量将输入流向输出层。感知机学习使用一种梯度下降形式来调整输入层和输出层之间的权重，以优化损失函数
    [[42](#bib.bib42)]。几年后，Minsky 证明了单层感知机无法学习非线性函数，包括 XOR 函数 [[43](#bib.bib43)]。多层感知机（MLPs，完整的缩写列表见表
    [4](#S8.T4 "Table 4 ‣ 8 Conclusions and Directions for Ongoing Research ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization")）通过在网络中添加隐藏单元的层，并对每个节点应用替代的可微激活函数，如 sigmoid，来解决非线性问题。然后，随机梯度下降被应用于MLP，以确定各层之间的权重，从而最小化函数近似误差
    [[44](#bib.bib44)]。然而，计算能力的不足使得DNN研究在几十年内停滞不前，其他分类器逐渐受到欢迎。2006年，DNN研究迎来了复兴，受到深度信念网络（DBNs）
    [[45](#bib.bib45)] 引入的推动。'
- en: 2.2 Deep Neural Network Architectures
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度神经网络架构
- en: Due to the increasing popularity of deep learning, many DNN architectures have
    been introduced with variations such as Neural Turing Machines [[46](#bib.bib46)]
    and Capsule Neural Networks [[47](#bib.bib47)]. In this paper, we summarize the
    general form of DNNs together with architectural components that not only appear
    in DNNs but can be incorporated into other models. We start by reviewing popular
    types of DNNs that have been introduced and that play complementary learning roles.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的日益流行，许多DNN架构被引入，其中包括神经图灵机 [[46](#bib.bib46)] 和胶囊神经网络 [[47](#bib.bib47)]
    等变体。在本文中，我们总结了DNN的一般形式以及不仅出现在DNN中的建筑组件，还可以被纳入其他模型。我们首先回顾了已引入的流行DNN类型，这些类型在学习中扮演着互补的角色。
- en: 2.3 Supervised Learning
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 监督学习
- en: 2.3.1 Multilayer Perceptron
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 多层感知器
- en: 'A multilayer perceptron (MLP) is one of the essential bases of many deep learning
    algorithms. The goal of a MLP is to map input $X$ to class $y$ by learning a function
    $y=f(X,\theta)$, where $\theta$ represents the best possible function approximation.
    For example, in Figure [2](#S2.F2 "Figure 2 ‣ 2.3.1 Multilayer Perceptron ‣ 2.3
    Supervised Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization")
    the MLP maps input $X$ to $y$ using function $f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$,
    where $f^{(1)}$ is the first layer, $f^{(2)}$ is the second layer, and $f^{(3)}$
    represents the third, output layer. This chain structure is a common component
    of many DNN architectures. The network depth is equal to the length of the chain,
    and the width of each layer represents the number of nodes in that layer [[48](#bib.bib48)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '多层感知器（MLP）是许多深度学习算法的基本组成部分之一。MLP的目标是通过学习函数 $y=f(X,\theta)$ 将输入 $X$ 映射到类别 $y$，其中
    $\theta$ 代表最佳的函数近似。例如，在图 [2](#S2.F2 "Figure 2 ‣ 2.3.1 Multilayer Perceptron ‣
    2.3 Supervised Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization") 中，MLP 使用函数 $f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$ 将输入 $X$ 映射到
    $y$，其中 $f^{(1)}$ 是第一层，$f^{(2)}$ 是第二层，$f^{(3)}$ 代表第三层，即输出层。这种链式结构是许多DNN架构的常见组件。网络深度等于链的长度，而每一层的宽度表示该层中的节点数量
    [[48](#bib.bib48)]。'
- en: 'In networks such as the MLP, the connections are not cyclic and thus belong
    to a class of DNNs called feedforward networks. Feedforward networks move information
    in only one direction, from the input to the output layer. Figure [2](#S2.F2 "Figure
    2 ‣ 2.3.1 Multilayer Perceptron ‣ 2.3 Supervised Learning ‣ 2 Brief Overview of
    Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization") depicts a particular type
    of feedforward network which is a fully-connected multilayer perceptron because
    each node at one layer is connected to all of the nodes at the next layer. Special
    cases of feedforward networks and MLPs have drawn considerable recent attention,
    which we describe next.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '在像MLP这样的网络中，连接不是循环的，因此属于一种叫做前馈网络的DNN类别。前馈网络中的信息仅沿一个方向流动，从输入层到输出层。图[2](#S2.F2
    "Figure 2 ‣ 2.3.1 Multilayer Perceptron ‣ 2.3 Supervised Learning ‣ 2 Brief Overview
    of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from
    Deep Networks: Models, Optimizations, and Regularization") 描述了一种特定类型的前馈网络，即完全连接的多层感知器，因为每一层的每个节点都与下一层的所有节点相连。前馈网络和MLP的特殊情况最近引起了相当大的关注，我们接下来将进行描述。'
- en: <svg   height="179.43" overflow="visible" version="1.1" width="300.08"><g transform="translate(0,179.43)
    matrix(1 0 0 -1 0 0) translate(39.65,0) translate(0,83.66)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.8 0.0 0.0 1.8 -9.34 -53.6)" fill="#000000"
    stroke="#000000"><foreignobject width="10.38" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g transform="matrix(1.8
    0.0 0.0 1.8 69.4 -16.69)" fill="#000000" stroke="#000000"><foreignobject width="10.38"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.8 0.0 0.0 1.8 148.14 -16.69)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.8 0.0 0.0 1.8 226.88 -18.17)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 51.68)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 22.15)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 -7.38)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.1 -67.26)" fill="#000000" stroke="#000000"><foreignobject
    width="14.71" height="7.63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 70.49 39.65)" fill="#000000" stroke="#000000"><foreignobject
    width="16.5" height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 70.29 -36.93)" fill="#000000" stroke="#000000"><foreignobject
    width="16.9" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{k}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 149.23 39.65)" fill="#000000" stroke="#000000"><foreignobject
    width="16.5" height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 149.35 -35.56)" fill="#000000" stroke="#000000"><foreignobject
    width="16.26" height="13.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{j}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 238.95 29.69)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 238.56 -29.36)" fill="#000000" stroke="#000000"><foreignobject
    width="11.93" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{n}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -16.72 63.94)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 21.06)"><g transform="matrix(1 0 0 1 0 14.91)"><g
    transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1 0 0 -1 0 8.835)"><g  transform="matrix(1
    0 0 1 0 12.23)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1
    0 0 -1 0 0)">Input</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0 24.52)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    2.29 0)"><text transform="matrix(1 0 0 -1 0 0)">layer</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 57.02 63.94)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 18.53)"><g  transform="matrix(1 0 0 1 0 12.38)"><g transform="matrix(1
    0 0 -1 0 0)"><g  transform="matrix(1 0 0 -1 0 7.575)"><g transform="matrix(1 0
    0 1 0 12.38)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0
    0 -1 0 0)">Hidden</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0 21.99)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    7.28 0)"><text transform="matrix(1 0 0 -1 0 0)">layer</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 135.76 63.94)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 18.53)"><g  transform="matrix(1 0 0 1 0 12.38)"><g transform="matrix(1
    0 0 -1 0 0)"><g  transform="matrix(1 0 0 -1 0 7.575)"><g transform="matrix(1 0
    0 1 0 12.38)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0
    0 -1 0 0)">Hidden</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0 21.99)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    7.28 0)"><text transform="matrix(1 0 0 -1 0 0)">layer</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 216.62 63.94)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 21.06)"><g  transform="matrix(1 0 0 1 0 14.91)"><g transform="matrix(1
    0 0 -1 0 0)"><g  transform="matrix(1 0 0 -1 0 8.835)"><g transform="matrix(1 0
    0 1 0 12.23)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0
    0 -1 0 0)">Ouput</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0 24.52)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    5.17 0)"><text transform="matrix(1 0 0 -1 0 0)">layer</text></g></g></g></g></g></svg>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="179.43" overflow="visible" version="1.1" width="300.08"><g transform="translate(0,179.43)
    matrix(1 0 0 -1 0 0) translate(39.65,0) translate(0,83.66)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.8 0.0 0.0 1.8 -9.34 -53.6)" fill="#000000"
    stroke="#000000"><foreignobject width="10.38" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g transform="matrix(1.8
    0.0 0.0 1.8 69.4 -16.69)" fill="#000000" stroke="#000000"><foreignobject width="10.38"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.8 0.0 0.0 1.8 148.14 -16.69)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.8 0.0 0.0 1.8 226.88 -18.17)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 51.68)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 22.15)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 -7.38)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.1 -67.26)" fill="#000000" stroke="#000000"><foreignobject
    width="14.71" height="7.63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 70.49 39.65)" fill="#000000" stroke="#000000"><foreignobject
    width="16.5" height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 70.29 -36.93)" fill="#000000" stroke="#000000"><foreignobject
    width="16.9" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{k}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 149.23 39.65)" fill="#000000" stroke="#000000"><foreignobject
    width="16.5" height="11.95" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 149.35 -35.56)" fill="#000000" stroke="#000000"><foreignobject
    width="16.26" height="13.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$H_{j}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 238.95 29.69)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 238.56 -29.36)" fill="#000000" stroke="#000000"><foreignobject
    width="11.93" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{n}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -16.72 63.94)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 21.06)"><g transform="matrix(1 0 0 1 0 14.91)"><g
    transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1 0 0 -1 0 8.835)"><g  transform="matrix(1
    0 0 1 0 12.23)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1
    0 0 -1 0 0)">输入</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0 24.52)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    2.29 0)"><text transform="matrix(1 0 0 -1 0 0)">层</text></g></g></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 57.02 63.94)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 18.53)"><g  transform="matrix(1 0 0 1 0 12.38)"><g transform="matrix(1
    0 0 -1 0 0)"><g  transform="matrix(1 0 0 -1 0 7.575)"><g transform="matrix(1 0
    0 1 0 12.38)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0
    0 -1 0 0)">隐藏</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0 21.99)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    7.28 0)"><text transform="matrix(1 0 0 -1 0 0)">层</text></g></g></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 135.76 63.94)" fill="#000000" stroke="#000000"><g  transform="matrix(1
    0 0 -1 0 18.53)"><g  transform="matrix(1 0 0 1 0 12.38)"><g transform="matrix(1
    0 0 -1 0 0)"><g  transform="matrix(1 0 0 -1 0 7.575)"><g transform="matrix(1 0
    0 1 0 12.38)"><g transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0
    0 -1 0 0)">隐藏</text></g></g></g></g></g><g transform="matrix(1 0 0 1 0 21.99)"><g
    class="
- en: 'Figure 2: An illustration of a three-layered MLP with $j$ nodes at the first
    hidden layer and $k$ at the second layer.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：具有$j$个节点的第一隐藏层和具有$k$个节点的第二层的三层MLP示意图。
- en: 2.3.2 Deep Convolutional Neural Network
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 深度卷积神经网络
- en: 'A convolutional neural network (CNN) [[49](#bib.bib49)] is a specialized class
    of feedforward DNNs for processing data that can be discretely presented. Examples
    of data that can benefit from CNNs include time series data that can be presented
    as samples of discrete regular time intervals and image data presented as samples
    of 2-D pixels at discrete locations. Most CNNs involve three stages: a convolution
    operation; an activation function, such as the rectified linear activation (ReLU)
    function [[50](#bib.bib50)]; and a pooling function, such as max pooling [[51](#bib.bib51)].
    A convolution operation is a weighted average or smooth estimation of a windowed
    input. One of the strengths of the convolution operation is that the connections
    between nodes in a network become sparser by learning a small kernel for unimportant
    features. Another benefit of convolution is parameter sharing. A CNN makes an
    assumption that a kernel learned for one input position can be used at every position,
    in contrast to a MLP which deploys a separate element of a weight matrix for each
    connection. Applying the convolution operator frequently improves the network’s
    learning ability.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）[[49](#bib.bib49)] 是一种专门用于处理可以离散呈现的数据的前馈深度神经网络（DNN）。可以从CNN中受益的数据示例包括可以作为离散规律时间间隔样本呈现的时间序列数据以及作为离散位置的二维像素样本呈现的图像数据。大多数CNN涉及三个阶段：卷积操作；一个激活函数，如修正线性激活（ReLU）函数[[50](#bib.bib50)]；以及一个池化函数，如最大池化[[51](#bib.bib51)]。卷积操作是一个加权平均或平滑估计的窗口输入。卷积操作的一个优势是通过学习一个小内核来忽略不重要的特征，从而使网络中节点之间的连接变得更加稀疏。卷积的另一个好处是参数共享。CNN假设一个为某一输入位置学习的内核可以在每个位置上使用，与每个连接部署一个权重矩阵的单独元素的多层感知器（MLP）相对。频繁应用卷积操作通常会提高网络的学习能力。
- en: 'A pooling function replaces the output of specific nearby nodes by their statistical
    summary. For example, the max-pooling function returns the maximum of a rectangular
    neighborhood. The motivation behind adding a pooling layer is that statistically
    down-sampling the number of features makes the representation approximately invariant
    to small translations of the input by maintaining the essential features. The
    final output of the learner is generated via a Fully-Connected (FC) layer that
    appears after the convolutional and max-pooling layers (see Figure [3](#S2.F3
    "Figure 3 ‣ 2.3.2 Deep Convolutional Neural Network ‣ 2.3 Supervised Learning
    ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization") for
    an illustration of the process).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 池化函数通过统计汇总替换特定附近节点的输出。例如，最大池化函数返回矩形邻域中的最大值。添加池化层的动机是，统计下采样特征数量使表示在小的输入平移下大致不变，通过保持基本特征。学习者的最终输出通过卷积层和最大池化层后出现的全连接（FC）层生成（参见图[3](#S2.F3
    "图 3 ‣ 2.3.2 深度卷积神经网络 ‣ 2.3 监督学习 ‣ 2 深度神经网络简要概述 ‣ 通过深度网络可以学习的所有分类器的技术调查：模型、优化和正则化")以获取过程示意）。
- en: <svg   height="177.15" overflow="visible" version="1.1" width="360.61"><g transform="translate(0,177.15)
    matrix(1 0 0 -1 0 0) translate(13.89,0) translate(0,49.2)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -9.27 -44.59)" fill="#000000"
    stroke="#000000"><foreignobject width="50.04" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| Input |
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="177.15" overflow="visible" version="1.1" width="360.61"><g transform="translate(0,177.15)
    matrix(1 0 0 -1 0 0) translate(13.89,0) translate(0,49.2)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -9.27 -44.59)" fill="#000000"
    stroke="#000000"><foreignobject width="50.04" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 输入 |
- en: '| layer |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 28.33 -44.59)"
    fill="#000000" stroke="#000000"><foreignobject width="100.82" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| Convolutional |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 层 |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 28.33 -44.59)"
    fill="#000000" stroke="#000000"><foreignobject width="100.82" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 卷积 |'
- en: '| layer + ReLU |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0
    130.33 73.52)" fill="#000000" stroke="#000000"><foreignobject width="93.67" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Max Pooling |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 层 + ReLU |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 130.33
    73.52)" fill="#000000" stroke="#000000"><foreignobject width="93.67" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| 最大池化 |'
- en: '| layer |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 166.13
    -44.59)" fill="#000000" stroke="#000000"><foreignobject width="100.82" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Convolutional |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 层 |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 166.13 -44.59)"
    fill="#000000" stroke="#000000"><foreignobject width="100.82" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 卷积 |'
- en: '| layer + ReLU |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0
    248.44 73.52)" fill="#000000" stroke="#000000"><foreignobject width="93.67" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Max Pooling |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 层 + ReLU |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 248.44
    73.52)" fill="#000000" stroke="#000000"><foreignobject width="93.67" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| 最大池化 |'
- en: '| layer |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 252.86
    -44.59)" fill="#000000" stroke="#000000"><foreignobject width="45.47" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| FC |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 层 |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 252.86 -44.59)"
    fill="#000000" stroke="#000000"><foreignobject width="45.47" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 全连接 |'
- en: '| layer |</foreignobject></g></g></svg>'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '| 层 |</foreignobject></g></g></svg>'
- en: 'Figure 3: An illustration of a three-layered CNN made of six convolution filters
    followed by six max pooling filters at the first layer, and eight convolution
    filters followed by seven max pooling filters at the second layer. The last layer
    is a fully connected layer (FC).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：一个三层卷积神经网络（CNN）的示意图，其中包括六个卷积滤波器，后跟六个最大池化滤波器作为第一层，第二层包括八个卷积滤波器，后跟七个最大池化滤波器。最后一层是一个全连接层（FC）。
- en: 2.3.3 Recurrent Neural Network
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 循环神经网络
- en: 'A recurrent Neural Network (RNN) is a sequential model that can capture the
    relationship between items in a sequence. Unlike traditional neural networks,
    wherein all inputs are independent of each other, RNNs contain artificial neurons
    with one or more feedback loops. Feedback loops are recurrent cycles over time
    or sequence, as shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.3.3 Recurrent Neural
    Network ‣ 2.3 Supervised Learning ‣ 2 Brief Overview of Deep Neural Networks ‣
    A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"). An established RNN problem is exploding or vanishing gradients.
    For a long data sequence, the gradient could become increasingly smaller or increasingly
    larger, which halts the learning. To address this issue, Hochreiter et al. [[52](#bib.bib52)]
    introduced a long short-term memory (LSTM) model and Cho et al. [[53](#bib.bib53)]
    proposed a gated recurrent unit (GRU) model. Both of these networks allow the
    gradient to flow unchanged in the network, thus preventing exploding or vanishing
    gradients.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是一种序列模型，可以捕捉序列中各项之间的关系。与传统神经网络不同，传统神经网络中的所有输入彼此独立，而RNN包含一个或多个反馈回路的人工神经元。反馈回路是时间或序列上的递归循环，如图
    [4](#S2.F4 "图 4 ‣ 2.3.3 循环神经网络 ‣ 2.3 监督学习 ‣ 2 深度神经网络概述 ‣ 所有分类器可以从深度网络中学习的技术：模型、优化和正则化")
    所示。一个已知的RNN问题是梯度爆炸或梯度消失。对于长数据序列，梯度可能变得越来越小或越来越大，从而阻碍学习。为了解决这个问题，Hochreiter 等人
    [[52](#bib.bib52)] 提出了长短期记忆（LSTM）模型，而 Cho 等人 [[53](#bib.bib53)] 提出了门控循环单元（GRU）模型。这两种网络都允许梯度在网络中不变地流动，从而防止梯度爆炸或梯度消失。
- en: <svg   height="157.48" overflow="visible" version="1.1" width="349.36"><g transform="translate(0,157.48)
    matrix(1 0 0 -1 0 0) translate(14.76,0) translate(0,78.74)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -6.27 -63.78)" fill="#000000"
    stroke="#000000"><foreignobject width="12.55" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$X$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -3.99 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="7.97"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.55 54.33)" fill="#000000" stroke="#000000"><foreignobject
    width="11.11" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Y$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 46.56 -4.8)" fill="#000000" stroke="#000000"><foreignobject
    width="40.74" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Unfold</foreignobject></g><g
    transform="matrix(1.2 0.0 0.0 1.2 107.95 -1.78)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\cdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.71 -60.79)" fill="#000000" stroke="#000000"><foreignobject
    width="17.16" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.68 -3.56)" fill="#000000" stroke="#000000"><foreignobject
    width="17.23" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 161.02 57.42)" fill="#000000" stroke="#000000"><foreignobject
    width="16.54" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.12 -60.84)" fill="#000000" stroke="#000000"><foreignobject
    width="10.71" height="8.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.09 -3.61)" fill="#000000" stroke="#000000"><foreignobject
    width="10.77" height="11.99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.43 57.42)" fill="#000000" stroke="#000000"><foreignobject
    width="10.08" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.35 -60.46)" fill="#000000" stroke="#000000"><foreignobject
    width="20.61" height="9.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.32 -3.23)" fill="#000000" stroke="#000000"><foreignobject
    width="20.67" height="12.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.66 57.65)" fill="#000000" stroke="#000000"><foreignobject
    width="19.98" height="9.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t+1}$</foreignobject></g><g
    transform="matrix(1.2 0.0 0.0 1.2 316.61 -1.78)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\cdots$</foreignobject></g></g></svg>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="157.48" overflow="visible" version="1.1" width="349.36"><g transform="translate(0,157.48)
    matrix(1 0 0 -1 0 0) translate(14.76,0) translate(0,78.74)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -6.27 -63.78)" fill="#000000"
    stroke="#000000"><foreignobject width="12.55" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$X$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -3.99 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="7.97"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.55 54.33)" fill="#000000" stroke="#000000"><foreignobject
    width="11.11" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Y$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 46.56 -4.8)" fill="#000000" stroke="#000000"><foreignobject
    width="40.74" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">展开</foreignobject></g><g
    transform="matrix(1.2 0.0 0.0 1.2 107.95 -1.78)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\cdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.71 -60.79)" fill="#000000" stroke="#000000"><foreignobject
    width="17.16" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.68 -3.56)" fill="#000000" stroke="#000000"><foreignobject
    width="17.23" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 161.02 57.42)" fill="#000000" stroke="#000000"><foreignobject
    width="16.54" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t-1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.12 -60.84)" fill="#000000" stroke="#000000"><foreignobject
    width="10.71" height="8.34" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.09 -3.61)" fill="#000000" stroke="#000000"><foreignobject
    width="10.77" height="11.99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 215.43 57.42)" fill="#000000" stroke="#000000"><foreignobject
    width="10.08" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.35 -60.46)" fill="#000000" stroke="#000000"><foreignobject
    width="20.61" height="9.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.32 -3.23)" fill="#000000" stroke="#000000"><foreignobject
    width="20.67" height="12.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{t+1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.66 57.65)" fill="#000000" stroke="#000000"><foreignobject
    width="19.98" height="9.1" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{t+1}$</foreignobject></g><g
    transform="matrix(1.2 0.0 0.0 1.2 316.61 -1.78)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\cdots$</foreignobject></g></g></svg>
- en: 'Figure 4: An illustration of a simple RNN and its unfolded structure through
    time $t$.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：一个简单的RNN及其在时间$t$上的展开结构的示意图。
- en: 2.3.4 Siamese Neural Network
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 Siamese 神经网络
- en: 'There are settings in which the number of training samples is limited, such
    as in facial recognition scenarios where only one image is available per person.
    When there is a limited number of examples for each class, DNNs struggle with
    generalizing the model. One strategy for addressing this problem is to learn a
    similarity function. This function computes the degree of difference between two
    samples, instead of learning each class. As an example, let $x_{1}$ represent
    one facial image and $x_{2}$ represent a second. If $d(x1,x2)\leq\tau$, we can
    conclude that the images are of the same person while $d(x_{1},x_{2})>\tau$ implies
    that they are different people. Siamese Neural Networks (SNN) [[54](#bib.bib54)]
    build on this idea by encoding examples $x_{i}$ and $x_{j}$ on two separate DNNs
    with shared parameters. The SNN learns a function $d$ using encoded features as
    shown in Figure [5](#S2.F5 "Figure 5 ‣ 2.3.4 Siamese Neural Network ‣ 2.3 Supervised
    Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization").
    The network then outputs $y>0$ for similar objects (i.e., when $d$ is less then
    a threshold value) and $y<0$ otherwise. Thus, SNNs can be used for similarity
    learning by learning a distance function over objects. In addition to their value
    for supervised learning from limited samples, SNNs are also beneficial for unsupervised
    learning tasks [[55](#bib.bib55)] [[56](#bib.bib56)].'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '在一些设置中，训练样本的数量有限，例如在面部识别场景中，每个人只有一张图片。在每个类别的样本数量有限时，深度神经网络（DNN）在模型泛化方面会遇到困难。解决这个问题的一种策略是学习相似度函数。该函数计算两个样本之间的差异程度，而不是学习每个类别。例如，设$x_{1}$代表一张面部图像，$x_{2}$代表另一张。如果$d(x_{1},x_{2})\leq\tau$，我们可以得出这些图像属于同一个人，而$d(x_{1},x_{2})>\tau$则意味着他们是不同的人。Siamese
    神经网络（SNN）[[54](#bib.bib54)]基于这一思想，通过在两个具有共享参数的DNN上编码样本$x_{i}$和$x_{j}$来进行。SNN学习一个函数$d$，使用编码特征，如图[5](#S2.F5
    "Figure 5 ‣ 2.3.4 Siamese Neural Network ‣ 2.3 Supervised Learning ‣ 2 Brief Overview
    of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from
    Deep Networks: Models, Optimizations, and Regularization")所示。网络然后输出$y>0$用于相似对象（即，当$d$小于阈值时），否则输出$y<0$。因此，SNN可以通过学习对象间的距离函数用于相似度学习。除了对有限样本的监督学习有价值外，SNN对于无监督学习任务[[55](#bib.bib55)]
    [[56](#bib.bib56)]也有益处。'
- en: <svg   height="177.72" overflow="visible" version="1.1" width="349.35"><g transform="translate(0,177.72)
    matrix(1 0 0 -1 0 0) translate(83.32,0) translate(0,98.7)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -24.97 46.81)" fill="#000000"
    stroke="#000000"><foreignobject width="10.58" height="8.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$x_{i}$</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 -0.08 26.92)" fill="#000000" stroke="#000000"><foreignobject width="78.91"
    height="24.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Network-1
    |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -25.46 -69.9)" fill="#000000"
    stroke="#000000"><foreignobject width="11.54" height="10.02" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$x_{j}$</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 -0.08 -71.51)" fill="#000000" stroke="#000000"><foreignobject width="78.91"
    height="24.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Network-2
    |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -78.71 -13.3)" fill="#000000"
    stroke="#000000"><foreignobject width="113.19" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Shared parameters</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 102.95 46.81)" fill="#000000" stroke="#000000"><foreignobject width="10.64"
    height="12.16" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{i}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 102.46 -73.55)" fill="#000000" stroke="#000000"><foreignobject
    width="11.61" height="13.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{j}$</foreignobject></g><g
    stroke-opacity="1" fill="#000000" fill-opacity="1" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 123 -13)"><foreignobject width="46.36" height="14.44" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$d(h_{i},h_{j})$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 186.3 -0.89)" fill="#000000" stroke="#000000"><foreignobject width="70.32"
    height="12.98" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-1\leq
    y_{i,j}\leq 1$</foreignobject></g></g></svg>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="177.72" overflow="visible" version="1.1" width="349.35"><g transform="translate(0,177.72)
    matrix(1 0 0 -1 0 0) translate(83.32,0) translate(0,98.7)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -24.97 46.81)" fill="#000000"
    stroke="#000000"><foreignobject width="10.58" height="8.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$x_{i}$</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 -0.08 26.92)" fill="#000000" stroke="#000000"><foreignobject width="78.91"
    height="24.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Network-1
    |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -25.46 -69.9)" fill="#000000"
    stroke="#000000"><foreignobject width="11.54" height="10.02" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$x_{j}$</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 -0.08 -71.51)" fill="#000000" stroke="#000000"><foreignobject width="78.91"
    height="24.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Network-2
    |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -78.71 -13.3)" fill="#000000"
    stroke="#000000"><foreignobject width="113.19" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Shared parameters</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 102.95 46.81)" fill="#000000" stroke="#000000"><foreignobject width="10.64"
    height="12.16" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{i}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 102.46 -73.55)" fill="#000000" stroke="#000000"><foreignobject
    width="11.61" height="13.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h_{j}$</foreignobject></g><g
    stroke-opacity="1" fill="#000000" fill-opacity="1" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 123 -13)"><foreignobject width="46.36" height="14.44" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$d(h_{i},h_{j})$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 186.3 -0.89)" fill="#000000" stroke="#000000"><foreignobject width="70.32"
    height="12.98" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-1\leq
    y_{i,j}\leq 1$</foreignobject></g></g></svg>
- en: 'Figure 5: An illustration of an SNN. In this figure, $x_{i}$ and $x_{j}$ are
    two data vectors corresponding to a pair of instances from the training set. Both
    networks share the same weights and map the input to a new representation. By
    comparing the outputs of the networks using a distance measure such as Euclidean,
    we can determine the compatibility between instances $x_{i}$ and $x_{j}$.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：SNN的示意图。在这个图中，$x_{i}$和$x_{j}$是来自训练集的一对数据向量。两个网络共享相同的权重，并将输入映射到新的表示中。通过使用诸如欧几里得距离等距离度量比较网络的输出，我们可以确定实例$x_{i}$和$x_{j}$之间的兼容性。
- en: 2.4 Unsupervised Learning
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 无监督学习
- en: 2.4.1 Generative Adversarial Network
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 生成对抗网络
- en: 'Until this point in the survey, we have focused on deep learning for its power
    in classifying data points. However, researchers have exploited deep learning
    for other uses as well, such as generating synthetic data that shares characteristics
    with known real data. One way to create synthetic data is to learn a generative
    model. A generative model learns the parameters that govern a distribution based
    on observation of real data points from that distribution. The learned model can
    then be used to create arbitrary amounts of synthetic data that emulate the real
    data observations. Recently, researchers have found a way to exploit multiplayer
    games for the purpose of improving generative machine learning algorithms. In
    the adversarial training scenario, two agents compete against each other, as inspired
    by Samuel [[57](#bib.bib57)] who designed a computer program to play checkers
    against itself. Goodfellow et al. [[58](#bib.bib58)] put this idea to use in developing
    Generative Adversarial Networks (GANs), in which a DNN (generator) tries to generate
    synthetic data that is so similar to real data that it fools its opponent DNN
    (discriminator), whose job is to distinguish real from fake data (see Figure [6](#S2.F6
    "Figure 6 ‣ 2.4.1 Generative Adversarial Network ‣ 2.4 Unsupervised Learning ‣
    2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization") for
    an illustration). The goal of GANs is to simultaneously improve the ability of
    the generator to produce realistic data and of the discriminator to distinguish
    synthetic from real data. GANs have found successful application in diverse tasks
    including translating text to images [[59](#bib.bib59)], discovering drugs [[60](#bib.bib60)],
    and transforming sketches to images [[61](#bib.bib61)] [[62](#bib.bib62)].'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 直到调查的这一点，我们一直关注于深度学习在分类数据点方面的强大能力。然而，研究人员也利用深度学习进行其他用途，例如生成具有已知真实数据特征的合成数据。创建合成数据的一种方法是学习生成模型。生成模型根据对真实数据点的观察，学习支配某一分布的参数。然后可以使用学习到的模型生成任意数量的合成数据，这些数据模拟真实数据的观察。最近，研究人员发现了一种利用多人游戏来改进生成机器学习算法的方法。在对抗训练场景中，两个代理相互竞争，灵感来自于Samuel
    [[57](#bib.bib57)]，他设计了一个计算机程序让其与自己下跳棋。Goodfellow等人[[58](#bib.bib58)]将这一思想应用于开发生成对抗网络（GANs），在这种网络中，一个DNN（生成器）试图生成与真实数据非常相似的合成数据，从而欺骗其对手DNN（判别器），判别器的工作是区分真实数据和虚假数据（参见图
    [6](#S2.F6 "图 6 ‣ 2.4.1 生成对抗网络 ‣ 2.4 无监督学习 ‣ 2 深度神经网络简要概述 ‣ 深度网络的所有分类器可学习的技术调查：模型、优化和正则化")）。GANs的目标是同时提高生成器产生逼真数据的能力和判别器区分合成数据与真实数据的能力。GANs在包括将文本翻译为图像[[59](#bib.bib59)]、发现药物[[60](#bib.bib60)]、以及将草图转化为图像[[61](#bib.bib61)]
    [[62](#bib.bib62)]等各种任务中取得了成功的应用。
- en: <svg   height="177.72" overflow="visible" version="1.1" width="335.2"><g transform="translate(0,177.72)
    matrix(1 0 0 -1 0 0) translate(98.7,0) translate(0,98.7)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 47.99 44.26)" fill="#000000"
    stroke="#000000"><foreignobject width="61.5" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Real Data</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 107.69 14.46)" fill="#000000" stroke="#000000"><foreignobject width="99.59"
    height="49.81" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Discriminator
    |
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="177.72" overflow="visible" version="1.1" width="335.2"><g transform="translate(0,177.72)
    matrix(1 0 0 -1 0 0) translate(98.7,0) translate(0,98.7)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 47.99 44.26)" fill="#000000"
    stroke="#000000"><foreignobject width="61.5" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">真实数据</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 107.69 14.46)" fill="#000000" stroke="#000000"><foreignobject width="99.59"
    height="49.81" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| 判别器 |
- en: '| Network |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 202.99
    44.26)" fill="#000000" stroke="#000000"><foreignobject width="27.1" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Real</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 202.6 24.87)" fill="#000000" stroke="#000000"><foreignobject
    width="27.87" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Fake</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -93.36 -54.17)" fill="#000000" stroke="#000000"><foreignobject
    width="88.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Random
    Noise</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 0.62 -83.96)" fill="#000000"
    stroke="#000000"><foreignobject width="77.51" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| Generator |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 网络 |</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 202.99 44.26)"
    fill="#000000" stroke="#000000"><foreignobject width="27.1" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">真实</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 202.6 24.87)" fill="#000000" stroke="#000000"><foreignobject width="27.87"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">虚假</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -93.36 -54.17)" fill="#000000" stroke="#000000"><foreignobject
    width="88.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">随机噪声</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 0.62 -83.96)" fill="#000000" stroke="#000000"><foreignobject
    width="77.51" height="49.81" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">|
    生成器 |'
- en: '| Network |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 134.28
    -83.96)" fill="#000000" stroke="#000000"><foreignobject width="46.39" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| Fake |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 网络 |</foreignobject></g>  <g transform="matrix(1.0 0.0 0.0 1.0 134.28 -83.96)"
    fill="#000000" stroke="#000000"><foreignobject width="46.39" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| 虚假 |'
- en: '| Data |</foreignobject></g></g></svg>'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '| 数据 |</foreignobject></g></g></svg>'
- en: 'Figure 6: An illustration of a GAN. The goal of the discriminator network is
    to distinguish real data from fake data, and the goal of the generator network
    is to use the feedback from the discriminator to generate data that the discriminator
    cannot distinguish from real.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：GAN的示意图。判别网络的目标是区分真实数据和虚假数据，而生成网络的目标是利用来自判别器的反馈生成判别器无法区分的真实数据。
- en: 2.4.2 Autoencoder
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 自编码器
- en: 'Yet another purpose for deep neural networks is to provide data compression
    and dimensionality reduction. An Autoencoder (AE) is a DNN that accomplishes this
    goal by creating an output layer that resembles the input layer, using a reduced
    set of terms represented by the middle layers [[48](#bib.bib48)]. Architecturally,
    an AE combines two networks. The first network, called the encoder, learns a new
    representation of input $x$ with fewer features $h=f(x)$; the second part, called
    the decoder, maps $h$ onto a reconstruction of the input space $\hat{y}=g(h)$,
    as shown in Figure [7](#S2.F7 "Figure 7 ‣ 2.4.2 Autoencoder ‣ 2.4 Unsupervised
    Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization").
    The goal of an AE is not simply to recreate the input features. Instead, an AE
    learns an approximation of the input features to identify useful properties of
    the data. AEs are vital tools for dimensionality reduction [[63](#bib.bib63)],
    feature learning [[64](#bib.bib64)], image colorization [[65](#bib.bib65)], higher-resolution
    data generation [[66](#bib.bib66)], and latent space clustering [[67](#bib.bib67)].
    Additionally, other versions of AEs such as variational autoencoders (VAEs) [[68](#bib.bib68)]
    can be used as generative models.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '深度神经网络的另一个目的就是提供数据压缩和维度减少。自编码器（AE）是一种通过创建与输入层类似的输出层、使用由中间层表示的减少术语集来实现这一目标的DNN
    [[48](#bib.bib48)]。从架构上看，自编码器结合了两个网络。第一个网络称为编码器，它学习具有较少特征的输入 $x$ 的新表示 $h=f(x)$；第二部分称为解码器，它将
    $h$ 映射到输入空间的重构 $\hat{y}=g(h)$，如图 [7](#S2.F7 "Figure 7 ‣ 2.4.2 Autoencoder ‣ 2.4
    Unsupervised Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of
    Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization") 所示。自编码器的目标不仅仅是重建输入特征。相反，自编码器学习输入特征的近似，以识别数据的有用属性。自编码器是维度减少
    [[63](#bib.bib63)]、特征学习 [[64](#bib.bib64)]、图像着色 [[65](#bib.bib65)]、高分辨率数据生成 [[66](#bib.bib66)]
    和潜在空间聚类 [[67](#bib.bib67)] 的重要工具。此外，变分自编码器（VAEs） [[68](#bib.bib68)] 等其他版本的自编码器也可以用作生成模型。'
- en: <svg   height="167.68" overflow="visible" version="1.1" width="305.94"><g transform="translate(0,167.68)
    matrix(1 0 0 -1 0 0) translate(39.65,0) translate(0,83.66)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.8 0.0 0.0 1.8 -9.34 -53.6)" fill="#000000"
    stroke="#000000"><foreignobject width="10.38" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g transform="matrix(1.8
    0.0 0.0 1.8 226.88 -53.6)" fill="#000000" stroke="#000000"><foreignobject width="10.38"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 51.87)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 22.34)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 -7.18)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 247.6 -66.24)" fill="#000000" stroke="#000000"><foreignobject
    width="14.08" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 51.68)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 22.15)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 -7.38)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.1 -67.26)" fill="#000000" stroke="#000000"><foreignobject
    width="14.71" height="7.63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -24.91 66.9)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 7.705)"><g transform="matrix(1 0 0 1 0 12.52)"><g
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Encoder</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 211.51 66.9)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 7.705)"><g transform="matrix(1 0 0 1 0 12.52)"><g
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Decoder</text></g></g></g></g></g></svg>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg height="167.68" overflow="visible" version="1.1" width="305.94"><g transform="translate(0,167.68)
    matrix(1 0 0 -1 0 0) translate(39.65,0) translate(0,83.66)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.8 0.0 0.0 1.8 -9.34 -53.6)" fill="#000000"
    stroke="#000000"><foreignobject width="10.38" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g transform="matrix(1.8
    0.0 0.0 1.8 226.88 -53.6)" fill="#000000" stroke="#000000"><foreignobject width="10.38"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 51.87)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 22.34)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.06 -7.18)" fill="#000000" stroke="#000000"><foreignobject
    width="11.15" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 247.6 -66.24)" fill="#000000" stroke="#000000"><foreignobject
    width="14.08" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$y_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 51.68)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 22.15)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.64 -7.38)" fill="#000000" stroke="#000000"><foreignobject
    width="11.78" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -32.1 -67.26)" fill="#000000" stroke="#000000"><foreignobject
    width="14.71" height="7.63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$x_{m}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -24.91 66.9)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 7.705)"><g transform="matrix(1 0 0 1 0 12.52)"><g
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">编码器</text></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 211.51 66.9)" fill="#000000" stroke="#000000"><g
    transform="matrix(1 0 0 -1 0 7.705)"><g transform="matrix(1 0 0 1 0 12.52)"><g
    transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">解码器</text></g></g></g></g></g></svg>`'
- en: 'Figure 7: An illustration of an AE. The first part of the network, called the
    encoder, compresses input into a latent-space by learning the function $h=f(x)$.
    The second part, called the decoder, reconstructs the input from the latent-space
    representation by learning the function $\hat{y}=g(h)$.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：自编码器的示意图。网络的第一部分称为编码器，通过学习函数 $h=f(x)$ 将输入压缩到潜在空间。第二部分称为解码器，通过学习函数 $\hat{y}=g(h)$
    从潜在空间表示中重建输入。
- en: 2.5 Optimization for Training Deep Neural Networks
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 深度神经网络的训练优化
- en: 'In the previous section, we described common DNN architecture components. In
    this section, we offer a brief overview of optimization approaches for training
    DNNs. Learning methods may optimize a function $f(x)$ (e.g., minimize a loss function)
    by modifying model parameters (e.g., changing DNN weights). However, as Bengio
    et al. [[69](#bib.bib69)] point out, DNN optimization during training may be further
    complicated by local minima and ill-conditioning (see Figure [8](#S2.F8 "Figure
    8 ‣ 2.5 Optimization for Training Deep Neural Networks ‣ 2 Brief Overview of Deep
    Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization") for an illustration of an ill-condition).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '在前一节中，我们描述了常见的 DNN 架构组件。在本节中，我们简要概述了训练 DNN 的优化方法。学习方法可以通过修改模型参数（例如，更改 DNN 权重）来优化函数
    $f(x)$（例如，最小化损失函数）。然而，正如 Bengio 等人 [[69](#bib.bib69)] 所指出的那样，训练过程中的 DNN 优化可能会因局部极小值和不良条件而变得更加复杂（请参见图
    [8](#S2.F8 "Figure 8 ‣ 2.5 Optimization for Training Deep Neural Networks ‣ 2
    Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization") 以查看不良条件的插图）。'
- en: The most common type of optimization strategy employed by DNNs is gradient descent.
    This intuitive approach to learns the weights of connections between layers which
    reduce the network’s objective function by computing the error derivative with
    respect to a Ir-level layer of the network. Input $x$ is fed forward through a
    network to predict $\hat{y}$. A cost function $J(\theta)$ measures the error of
    the network at the output layer. Gradient descent then directs the cost value
    to flow backward through the network by computing the gradient of the objective
    function $\nabla_{\theta}J(\theta)$. This process is sometimes alternatively referred
    to as backpropagation because the training error propagates backward through the
    network from output to input layers. Many variations of gradient descent have
    been tested for DNN optimization, such as stochastic gradient descent, mini-batch
    gradient descent, momentum [[70](#bib.bib70)], Ada-Grad [[71](#bib.bib71)], and
    Adam [[72](#bib.bib72)].
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 最常用的优化策略是梯度下降。这种直观的方法通过计算相对于网络中 Ir 级层的误差导数来学习层间连接的权重，从而减少网络的目标函数。输入 $x$
    被前向传递通过网络以预测 $\hat{y}$。成本函数 $J(\theta)$ 衡量网络在输出层的误差。然后，梯度下降通过计算目标函数的梯度 $\nabla_{\theta}J(\theta)$
    来将成本值反向传递通过网络。这个过程有时被称为反向传播，因为训练误差从输出层向输入层反向传播。许多梯度下降的变种已被用于 DNN 优化，如随机梯度下降、迷你批量梯度下降、动量
    [[70](#bib.bib70)]、Ada-Grad [[71](#bib.bib71)] 和 Adam [[72](#bib.bib72)]。
- en: Deep network optimization is an active area of research. Along with gradient
    descent, many other algorithms such as derivative-free optimization [[73](#bib.bib73)]
    and feedback-alignment [[74](#bib.bib74)] have appeared. However, none of these
    algorithms are as popular as the gradient descent algorithms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络优化是一个活跃的研究领域。除了梯度下降，许多其他算法如无导数优化 [[73](#bib.bib73)] 和反馈对齐 [[74](#bib.bib74)]
    也已经出现。然而，这些算法都没有梯度下降算法那么流行。
- en: 'Figure 8: The left-hand side loss surface depicts a well-conditioned model
    where local minima can be reached from all directions. The right-hand side loss
    surface depicts an ill-conditioned model where there are several ways to overshoot
    or never reach the minima.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：左侧的损失表面描绘了一个良好条件的模型，其中局部极小值可以从所有方向达到。右侧的损失表面描绘了一个不良条件的模型，其中有多种方式可以超越或永远无法到达极小值。
- en: 2.6 Regularization
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 正则化
- en: Regularization was an optimization staple for decades prior to the development
    of DNNs. The rationale behind adding a regularizer to a classifier is to avoid
    the overfitting problem, where the classifier fits the training set too closely
    instead of generalizing to the entire data space. Goodfellow et al. [[48](#bib.bib48)]
    defined regularization as “any modification to a learning algorithm that is intended
    to reduce its generalization error but not its training error”. While regularization
    methods such as bagging have been popular for neural networks and other classifiers,
    recently the DNN community has developed novel regularization methods that are
    unique to deep neural networks. In some cases, backpropagation training of fully-connected
    DNNs results in poorer performance than shallow structures because the deeper
    structure is prone to being trapped in local minima and overfitting the training
    data. To improve the generalizability of DNNs, regularization methods have thus
    been adopted during training. Here we review the intuition behind the most frequent
    regularization methods that are currently found in DNNs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化在DNNs发展之前几十年一直是优化的基础。将正则化项添加到分类器中的理由是为了避免过拟合问题，即分类器过于贴合训练集而不能很好地推广到整个数据空间。Goodfellow等人[[48](#bib.bib48)]将正则化定义为“对学习算法的任何修改，其目的是减少其泛化误差而不是训练误差”。虽然像bagging这样的正则化方法在神经网络和其他分类器中很受欢迎，但最近DNN社区开发了一些独特于深度神经网络的新型正则化方法。在某些情况下，完全连接的DNN的反向传播训练效果不如浅层结构，因为更深的结构容易陷入局部最小值并对训练数据进行过拟合。为了提高DNN的泛化能力，因此在训练过程中采用了正则化方法。在这里，我们回顾了目前在DNN中最常见的正则化方法的直觉。
- en: 2.6.1 Parameter Norm Penalty
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.1 参数范数惩罚
- en: A conventional method for avoiding overfitting is to penalize large weights
    by adding a p-norm penalty function to the optimization function of the form $f(x)+$
    p-norm$(x)$, where the p-norm $p$ for weights $w$ is denoted as $||w||_{p}=(\sum_{i}|w_{i}|^{p})^{\frac{1}{p}}$.
    Popular p-norms are the $L_{1}$ and $L_{2}$ norms which have been used by other
    classifiers such as logistic regression and SVMs prior to the introduction of
    DNNs. $L_{1}$ adds a regularization term $\Omega(\theta)=||w||_{1}$ to the objective
    function for weights $w$, while $L_{2}$ adds a regularization term $\Omega(\theta)=||w||_{2}$.
    The difference between the $L_{1}$ and $L_{2}$ norm penalty functions is that
    $L_{1}$ penalizes features more heavily by setting the corresponding edge weights
    to zero compared to $L_{2}$. Therefore, a classifier with the $L_{1}$ norm penalty
    tends to prefer a sparse model. The $L_{2}$ norm penalty is more common than the
    $L_{1}$ norm penalty. However, it is often advised to use the $L_{1}$ norm penalty
    when the amount of training data is small and the number of features is large
    to avoid noisy and less-important features. Because of its sparsity property,
    the $L_{1}$ penalty function is a key component of LASSO feature selection [[75](#bib.bib75)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过拟合的传统方法是通过将p-范数惩罚函数添加到形式为$f(x)+$ p-norm$(x)$的优化函数中来惩罚大权重，其中权重$w$的p-范数$p$表示为$||w||_{p}=(\sum_{i}|w_{i}|^{p})^{\frac{1}{p}}$。流行的p-范数包括$L_{1}$和$L_{2}$范数，这些范数在DNNs出现之前已被其他分类器（如逻辑回归和支持向量机）使用。$L_{1}$在目标函数中添加一个正则化项$\Omega(\theta)=||w||_{1}$，而$L_{2}$则添加一个正则化项$\Omega(\theta)=||w||_{2}$。$L_{1}$和$L_{2}$范数惩罚函数的区别在于，$L_{1}$通过将相应的边权重设置为零来对特征施加更严格的惩罚，相比之下$L_{2}$则较轻。因此，使用$L_{1}$范数惩罚的分类器倾向于选择稀疏模型。$L_{2}$范数惩罚比$L_{1}$范数惩罚更为常见。然而，当训练数据量较小而特征数量较大时，建议使用$L_{1}$范数惩罚，以避免噪声和不重要的特征。由于其稀疏性，$L_{1}$惩罚函数是LASSO特征选择的关键组成部分[[75](#bib.bib75)]。
- en: 2.6.2 Dropout
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.2 Dropout
- en: A powerful method to reduce generalization error is to create an ensemble of
    classifiers. Multiple models are trained separately, then as an ensemble they
    output a combination of the models’ predictions on test points. Some examples
    of ensemble methods included bagging [[76](#bib.bib76)], which trains $k$ models
    on $k$ different folds of random samples with replacement, and boosting [[77](#bib.bib77)],
    which applies a similar process to weighted data. A variety of DNNs use boosting
    to achieve lower generalization error [[45](#bib.bib45)] [[78](#bib.bib78)] [[79](#bib.bib79)].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 减少泛化误差的一种强大方法是创建分类器的集成。多个模型分别训练，然后作为一个集成输出模型对测试点的预测组合。一些集成方法的例子包括bagging [[76](#bib.bib76)]，它在具有替换的$k$个不同随机样本折叠上训练$k$个模型，以及boosting
    [[77](#bib.bib77)]，它对加权数据应用类似过程。各种DNN使用boosting来实现较低的泛化误差[[45](#bib.bib45)] [[78](#bib.bib78)]
    [[79](#bib.bib79)]。
- en: Dropout [[80](#bib.bib80)] is a popular regularization method for DNNs which
    can be viewed as a computationally-inexpensive application of bagging to deep
    networks. A common way to apply dropout to a DNN is to deactivate a randomly-selected
    50% of the hidden nodes and a randomly-selected 20% of the input nodes for each
    mini-batch of data. The difference between bagging and dropout is that in bagging
    the models are independent of each other, while in dropout each model inherits
    a subset of parameters from the parent deep network.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout [[80](#bib.bib80)] 是一种流行的DNN正则化方法，可以看作是对深度网络的计算上廉价的bagging应用。对DNN应用dropout的一种常见方式是对每个小批量数据禁用随机选择的50%的隐藏节点和随机选择的20%的输入节点。bagging与dropout的区别在于，bagging中的模型彼此独立，而在dropout中，每个模型从父深度网络继承了一部分参数。
- en: 2.6.3 Data Augmentation
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.3 数据增强
- en: 'DNNs can generalize better when they have more training data; however, the
    amount of available data is often limited. One way to circumvent this limitation
    is to generate artificial data from the same distribution as the training set.
    Data augmentation has been particularly effective when used in the context of
    classification. The goal of data augmentation is to generate new training samples
    from the original training set $(X,y)$ by transforming the $X$ inputs. Data augmentation
    may include generating noisy data to improve robustness (denoising) or creating
    additional training data for the purpose of regularization (synthetic data generation).
    Dataset augmentation has been adopted for a variety of tasks such as image recognition
    [[81](#bib.bib81)] [[82](#bib.bib82)], speech recognition [[83](#bib.bib83)],
    and activity recognition [[84](#bib.bib84)]. Additionally, GANs [[85](#bib.bib85)]
    [[86](#bib.bib86)] and AEs [[87](#bib.bib87)] [[88](#bib.bib88)], described in
    Sections [2.4.1](#S2.SS4.SSS1 "2.4.1 Generative Adversarial Network ‣ 2.4 Unsupervised
    Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization")
    and [2.4.2](#S2.SS4.SSS2 "2.4.2 Autoencoder ‣ 2.4 Unsupervised Learning ‣ 2 Brief
    Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can
    Learn from Deep Networks: Models, Optimizations, and Regularization"), can be
    employed to generate such new examples.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '深度神经网络（DNNs）在拥有更多训练数据时能够更好地泛化；然而，可用的数据量通常有限。解决这个限制的一种方法是从与训练集相同的分布中生成人工数据。数据增强在分类任务中尤其有效。数据增强的目标是通过转换$X$输入，从原始训练集$(X,y)$生成新的训练样本。数据增强可能包括生成噪声数据以提高鲁棒性（去噪）或创建额外的训练数据以进行正则化（合成数据生成）。数据集增强已被应用于图像识别[[81](#bib.bib81)]
    [[82](#bib.bib82)]、语音识别[[83](#bib.bib83)]和活动识别[[84](#bib.bib84)]等各种任务。此外，GANs
    [[85](#bib.bib85)] [[86](#bib.bib86)]和AEs [[87](#bib.bib87)] [[88](#bib.bib88)]，在[2.4.1](#S2.SS4.SSS1
    "2.4.1 Generative Adversarial Network ‣ 2.4 Unsupervised Learning ‣ 2 Brief Overview
    of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from
    Deep Networks: Models, Optimizations, and Regularization")和[2.4.2](#S2.SS4.SSS2
    "2.4.2 Autoencoder ‣ 2.4 Unsupervised Learning ‣ 2 Brief Overview of Deep Neural
    Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization")节中描述的，可以用来生成这样的新示例。'
- en: Injecting noise into a copy of the input is another data augmentation method.
    Although DNNs are not consistently robust to noise [[89](#bib.bib89)], Poole et
    al. [[90](#bib.bib90)] show that DNNs can benefit from carefully-tuned noise.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将噪声注入输入副本是另一种数据增强方法。尽管DNN对噪声的鲁棒性并不一致[[89](#bib.bib89)]，但Poole等人[[90](#bib.bib90)]表明，DNN可以从精心调整的噪声中受益。
- en: 3 Deep Learning Architectures Outside of Deep Neural Networks
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种深度学习架构，超越深度神经网络
- en: Recent research has introduced numerous enhancements to the basic neural network
    architecture that enhance network classification power, particularly for deep
    networks. In this section, we survey non-network classifiers that also make use
    of these advances.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究引入了许多对基础神经网络架构的增强，特别是对于深度网络，提升了网络的分类能力。在这一部分，我们调查了那些也利用这些进展的非网络分类器。
- en: 3.1 Supervised Learning
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 监督学习
- en: 3.1.1 Feedforward Learning
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 前馈学习
- en: A DNN involves multiple layers of operations that are performed sequentially.
    The idea of creating a sequence of operations, each of which manipulates the data
    before passing them to the next operator, may be used to improve many types of
    classifiers. One way to construct a model with a deep feedforward architecture
    is to use stacked generalization [[91](#bib.bib91)] [[92](#bib.bib92)]. Stacked
    generalization classifiers are comprised of multiple layers of classifiers stacked
    on top of each other as found in DNNs. In stacked generalization classifiers,
    one layer generates the next layer’s input by concatenating its own input to its
    output. Stacked generalization classifiers typically only implement forward propagation,
    in contrast to DNNs which propagate information both forward and backward through
    the model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个深度神经网络（DNN）涉及多个层的操作，这些操作是顺序执行的。创建一个操作序列的想法，每个操作在将数据传递给下一个操作符之前对数据进行处理，可以用于改进许多类型的分类器。构建深度前馈架构模型的一种方法是使用堆叠泛化
    [[91](#bib.bib91)] [[92](#bib.bib92)]。堆叠泛化分类器由多个层叠在一起的分类器组成，如同在深度神经网络（DNNs）中所发现的那样。在堆叠泛化分类器中，一层通过将其自己的输入与其输出连接起来来生成下一层的输入。与同时前向和后向传播信息的DNNs不同，堆叠泛化分类器通常只实现前向传播。
- en: In general, learning methods that employ stacked generalization can be categorized
    into two strategies. In the first stacked generalization strategy, the new feature
    space for the current layer comes from the concatenation of the predicted output
    of the previous layer with the original feature vector. Here, layers refer not
    to layers of neural network operations, but instead refer to sequences of other
    types of operations. Examples of this strategy include Deep Forest (DF) [[93](#bib.bib93)]
    and the Deep Transfer Additive Kernel Least Square SVM (DTA-LS-SVM) [[94](#bib.bib94)].
    At any given layer, for each instance $x$, DF extends $x$’s previous feature vector
    to include the previous layer’s predicted class value for the instance. The prediction
    represents a distribution over class values, averaged over all trees in the forest.
    Furthermore, Zhou et al. [[93](#bib.bib93)] introduce a method called Multi-Grained
    Scanning for improving the accuracy of DFs. Inspired by CNNs and RNNs where spatial
    relationships between features are critical, Multi-Grained Scanning splits a $D$-dimensional
    feature vector into smaller segments by moving a window by moving a window over
    the features. For example, given 400 features and a window size of 100, the original
    features convert to 301 features of length 100, $\{<1-100>,<2-101>,\ldots,<301-400>\}$,
    where the new instances have the same labels as the original instances. The new
    samples which are described by a subset of the original features might have incorrectly-associated
    labels. At a first glance, it seems these noisy data could hurt the generalization.
    But as Breiman illustrates [[95](#bib.bib95)], perturbing a percentage of the
    training labels can actually help generalization.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用堆叠泛化的学习方法可以分为两种策略。在第一种堆叠泛化策略中，当前层的新特征空间来自于将上一层的预测输出与原始特征向量连接。在这里，层不是指神经网络操作的层，而是指其他类型操作的序列。这种策略的例子包括深度森林（DF）
    [[93](#bib.bib93)] 和深度传输加性核最小二乘支持向量机（DTA-LS-SVM） [[94](#bib.bib94)]。在任何给定的层中，对于每个实例
    $x$，DF 将 $x$ 的先前特征向量扩展为包括先前层对该实例的预测类别值。该预测表示所有森林中的树的类别值分布的平均值。此外，Zhou 等人 [[93](#bib.bib93)]
    提出了一种称为多粒度扫描的方法，以提高 DF 的准确性。受 CNNs 和 RNNs 启发，这些网络中特征之间的空间关系至关重要，多粒度扫描通过在特征上移动窗口，将
    $D$ 维特征向量拆分为更小的片段。例如，给定 400 个特征和 100 的窗口大小，原始特征转换为 301 个长度为 100 的特征，$\{<1-100>,<2-101>,\ldots,<301-400>\}$，其中新的实例与原始实例具有相同的标签。这些由原始特征子集描述的新样本可能具有错误关联的标签。乍一看，这些噪声数据似乎可能会影响泛化。但正如
    Breiman 所示 [[95](#bib.bib95)]，扰动一部分训练标签实际上可能有助于泛化。
- en: Furthermore, Ho [[96](#bib.bib96)] demonstrates that feature sub-sampling can
    enhance the generalization capability for RFs. Zhou et al. [[93](#bib.bib93)]
    tested three different window sizes ($D/4$, $D/8$, and $D/16$), where data from
    each different window size fits a different level of a DF model. Then the newly-learned
    representation from these three layers are fed to a multilayer DF, applying subsampling
    when the transformed features are too long. Multi-Grained Scanning can improve
    the performance of a DF model for continuous data, as Zhou et al. [[93](#bib.bib93)]
    report that accuracy increased by 1.24% on the MNIST [[97](#bib.bib97)] dataset.
    An alternative method, DTA-LS-SVM, applies an Additive Kernel Least Squares SVM
    (AK-LS-SVM) [[98](#bib.bib98)] [[99](#bib.bib99)] at each layer and concatenates
    the original feature vector $x$ with the prediction of the previous level to feed
    to the next layer. In addition, DTA-LS-SVM incorporates a parameter-transfer approach
    between the source (previous-layer learner) and target (next-layer learner) to
    enhance the classification capability of the higher level.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Ho [[96](#bib.bib96)] 证明特征子采样可以增强随机森林的泛化能力。Zhou等人[[93](#bib.bib93)]测试了三种不同的窗口大小（$D/4$，$D/8$，和$D/16$），每种窗口大小的数据适合DF模型的不同层级。然后，这三层中新学习到的表示被输入到一个多层DF中，当转换后的特征过长时应用子采样。多粒度扫描可以提高DF模型在连续数据上的性能，因为Zhou等人[[93](#bib.bib93)]报告称，MNIST
    [[97](#bib.bib97)]数据集上的准确率提高了1.24%。另一种方法，DTA-LS-SVM，在每一层应用加性核最小二乘支持向量机（AK-LS-SVM）[[98](#bib.bib98)]
    [[99](#bib.bib99)]，并将原始特征向量$x$与前一层的预测连接起来，以输入到下一层。此外，DTA-LS-SVM在源（前一层学习者）和目标（下一层学习者）之间引入了参数转移方法，以增强较高层次的分类能力。
- en: 'In the second stacked generalization strategy, the current layer’s new feature
    space comes from the concatenation of predictions from all previous layers with
    the original input feature vector. Examples of this strategy include the Deep
    SVM (D-SVM) [[100](#bib.bib100)] and the Random Recursive SVM (R2-SVM) [[101](#bib.bib101)].
    The D-SVM contains multiple layers of SVMs, where the first layer is trained in
    the normal fashion. Following this step, each successive layer employs the kernel
    activation from the previous layer with the desired labels. The R2-SVM is a multilayer
    SVM model which at each layer transforms the data based on the sigmoid of a projection
    of all previous layers’ outputs. For the data $(X,Y)$ where $X\in R^{D}$ and $Y\in
    R^{C}$, the random projection matrix is $W\in R^{D\times C}$, where each element
    is sampled from $N(0,1)$. The input data for the next layer is:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种堆叠泛化策略中，当前层的新特征空间来源于所有前一层的预测与原始输入特征向量的连接。该策略的例子包括深度支持向量机（D-SVM）[[100](#bib.bib100)]和随机递归支持向量机（R2-SVM）[[101](#bib.bib101)]。D-SVM包含多个支持向量机层，其中第一层以正常方式训练。之后，每一层都使用来自前一层的核激活与期望标签。R2-SVM是一个多层支持向量机模型，在每一层根据所有前一层输出的投影的sigmoid来转换数据。对于数据$(X,Y)$，其中$X\in
    R^{D}$和$Y\in R^{C}$，随机投影矩阵是$W\in R^{D\times C}$，其中每个元素从$N(0,1)$中抽样。下一层的输入数据为：
- en: '|  | $X_{l+1}=\sigma(d+\beta W_{l+1}[o_{1}^{T},o_{2}^{T},...,o_{l}^{T}]^{T}),$
    |  | (1) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $X_{l+1}=\sigma(d+\beta W_{l+1}[o_{1}^{T},o_{2}^{T},...,o_{l}^{T}]^{T}),$
    |  | (1) |'
- en: where $\beta$ is a weight parameter that controls the degree with which a data
    sample in $X_{l+1}$ moves from the previous layer, $\sigma(.)$ is the sigmoid
    function, $W_{l+1}$ is the concatenation of $l$ random projection matrices $[W_{l+1,1},W_{l+1,2},...,W_{l+1,l}]$,
    one for each previous layer, and $o$ is the output of each layer. Addition of
    a sigmoid function to the recursive model prevents deterioration to a trivial
    linear model in a similar fashion as MLPs. The purpose of the random projection
    is to push data from different classes in different directions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\beta$是一个权重参数，控制数据样本在$X_{l+1}$中从上一层移动的程度，$\sigma(.)$是sigmoid函数，$W_{l+1}$是$l$个随机投影矩阵的连接$[W_{l+1,1},W_{l+1,2},...,W_{l+1,l}]$，每个矩阵对应于前一层，每一层的输出为$o$。将sigmoid函数添加到递归模型中可以防止模型退化为类似于多层感知机（MLPs）的平凡线性模型。随机投影的目的是将来自不同类别的数据推向不同的方向。
- en: It is important to note here that stacked generalization can be found in DNNs
    as well as non-network classifiers. Examples of DNNs with stacked generalization
    include Deep Stacking Networks [[102](#bib.bib102)] [[103](#bib.bib103)] and Convex
    Stacking Architectures [[104](#bib.bib104)] [[102](#bib.bib102)]. This is clearly
    one enhancement that benefits all types of classifier strategies. However, there
    is no evidence that stack generalization could add nonlinearity to the model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，堆叠泛化可以在深度神经网络（DNNs）以及非网络分类器中找到。具有堆叠泛化的DNN示例包括深度堆叠网络 [[102](#bib.bib102)]
    [[103](#bib.bib103)] 和凸堆叠架构 [[104](#bib.bib104)] [[102](#bib.bib102)]。这显然是对所有类型分类器策略的一个增强。然而，目前没有证据表明堆叠泛化可以为模型增加非线性。
- en: DNN classifiers learn a new representation of data at each layer with a goal
    that the newly-learned representation maximally separate the classes. Unsupervised
    DNNs often share this goal. As an example, Deep PCA’s model [[105](#bib.bib105)]
    is made of two layers that each learn a new data representation by applying a
    Zero Components Analysis (ZCA) whitening filter [[106](#bib.bib106)] followed
    by a principal components analysis (PCA) [[107](#bib.bib107)]. The final data
    representation is derived from concatenating the output of the two layers. The
    motivation behind applying a ZCA whitening filter is to force the model to focus
    on higher-order correlations. One motivation for combining output from the first
    and second layers could be to preserve the learned representation from the first
    layer and to prevent feature loss after applying PCA at each layer. Experiments
    demonstrate that Deep PCA exhibits superior performance for face recognition tasks
    compared to standard PCA and a two-layer PCA without a whitening filter. However,
    as empirically confirmed by Damianou et al. [[108](#bib.bib108)], stacking PCAs
    does not necessarily result in an improved representation of the data because
    Deep PCA is unable to learn a nonlinear representation of data at each layer.
    Damianou et al. [[108](#bib.bib108)] fed a Gaussian to a Deep PCA and observed
    that the model learned just a lower rank of the input Gaussian at each layer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DNN分类器在每一层学习数据的新表示，目标是使新学习的表示能最大限度地分隔类别。无监督的DNN通常也有这个目标。例如，深度主成分分析模型 [[105](#bib.bib105)]
    由两层组成，每层通过应用零分量分析（ZCA）白化滤波器 [[106](#bib.bib106)]，然后进行主成分分析（PCA） [[107](#bib.bib107)]
    来学习新的数据表示。最终的数据表示是由两层的输出拼接而成。应用ZCA白化滤波器的动机是迫使模型关注高阶相关性。结合第一层和第二层的输出的一个动机可能是为了保留第一层学到的表示，并在每层应用PCA后防止特征丢失。实验表明，相比于标准PCA和没有白化滤波器的两层PCA，深度PCA在面部识别任务中表现更佳。然而，正如Damianou等人
    [[108](#bib.bib108)] 实证确认的那样，堆叠PCA不一定会导致数据表示的改进，因为深度PCA无法在每层学习非线性数据表示。Damianou等人
    [[108](#bib.bib108)] 将高斯分布输入深度PCA，并观察到模型在每一层仅学习了输入高斯分布的较低秩。
- en: 'As pointed out earlier in this survey, the invention of the deep belief net
    (DBN) [[45](#bib.bib45)] drew the attention of researchers to developing deep
    models. A DBN can be viewed as a stacked restricted Boltzmann machine (RBM), where
    each layer is trained separately and alternates functionality between hidden and
    input units. In this model, features learned at hidden layers then represent inputs
    to the next layer. A RBM is a generative model that contains a single hidden layer.
    Unlike the Boltzmann machine, hidden units in the restricted model are not connected
    to each other and contain undirected, symmetrical connections from a layer of
    visible units (inputs). All of the units in each layer of a RBM are updated in
    parallel by inputting the current state of the unit to the other layer. This updating
    process repeats until the system is sampling from an equilibrium distribution.
    The RBM learning rule is shown in Equation [2](#S3.E2 "In 3.1.1 Feedforward Learning
    ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures Outside of Deep Neural
    Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '正如本调查中早先指出的，深度置信网络（DBN）的发明[[45](#bib.bib45)]引起了研究人员对开发深度模型的关注。DBN可以视为一个堆叠的限制玻尔兹曼机（RBM），每一层单独训练，并在隐藏单元和输入单元之间交替功能。在这个模型中，隐藏层学习到的特征表示为下一层的输入。RBM是一种生成模型，包含一个隐藏层。与玻尔兹曼机不同，限制模型中的隐藏单元彼此不连接，并且包含来自可见单元（输入）的无向对称连接。RBM中的每一层单元通过将当前单元状态输入到另一层来并行更新。这个更新过程会重复，直到系统从平衡分布中采样。RBM的学习规则如方程[2](#S3.E2
    "In 3.1.1 Feedforward Learning ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization")所示。'
- en: '|  | $\frac{\partial\log P(v)}{\partial W_{ij}}\approx<v_{i}h_{j}>_{data}-<v_{i}h_{j}>_{reconstruction}$
    |  | (2) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial\log P(v)}{\partial W_{ij}}\approx<v_{i}h_{j}>_{data}-<v_{i}h_{j}>_{reconstruction}$
    |  | (2) |'
- en: In this equation, $W_{ij}$ represents the weight vector between a visible unit
    $v_{i}$ and a hidden unit $h_{j}$, and $<.>$ is the average value over all training
    samples. Since the introduction of DBNs, many other different variations of Deep
    RBMs have been proposed such as temporal RBMs [[109](#bib.bib109)], gated RBMs
    [[110](#bib.bib110)], and cardinality RBMs [[111](#bib.bib111)].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，$W_{ij}$表示可见单元$v_{i}$和隐藏单元$h_{j}$之间的权重向量，$<.>$是所有训练样本的平均值。自DBN引入以来，已经提出了许多不同变体的深度RBM，例如时间RBM[[109](#bib.bib109)]、门控RBM[[110](#bib.bib110)]和基数RBM[[111](#bib.bib111)]。
- en: 'Another novel form of a deep belief net is a deep Gaussian process (DGP) model
    [[108](#bib.bib108)]. DGP is a deep directed graph where multiple layers of Gaussian
    processes map the original features to a series of latent spaces. DGPs offer a
    more general form of Gaussian Processes (GPs) [[112](#bib.bib112)] where a one-layer
    DGP consists of a single GP, $f$. In a multilayer DGP, each GP, $f_{l}$, maps
    data from one latent space to the next. As shown in Equation [3](#S3.E3 "In 3.1.1
    Feedforward Learning ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization"), each data point
    $Y$ is generated from the corresponding function $f_{l}$ with $\epsilon$ Guassian
    noise applied to data $X_{l}$ that is obtained from a previous layer.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '深度置信网络的另一种新颖形式是深度高斯过程（DGP）模型[[108](#bib.bib108)]。DGP是一个深度有向图，其中多层高斯过程将原始特征映射到一系列潜在空间。DGP提供了高斯过程（GP）[[112](#bib.bib112)]的更一般形式，其中单层DGP由一个GP组成，$f$。在多层DGP中，每个GP，$f_{l}$，将数据从一个潜在空间映射到下一个。如方程[3](#S3.E3
    "In 3.1.1 Feedforward Learning ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization")所示，每个数据点$Y$是由对应的函数$f_{l}$生成的，$\epsilon$
    高斯噪声施加在从上一层获得的数据$X_{l}$上。'
- en: '|  | $Y=f_{l}(X_{l})+\epsilon_{l},\indent\epsilon_{l}\sim\mathcal{N}(0,\sigma^{2}_{l}I)$
    |  | (3) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y=f_{l}(X_{l})+\epsilon_{l},\indent\epsilon_{l}\sim\mathcal{N}(0,\sigma^{2}_{l}I)$
    |  | (3) |'
- en: 'Figure [9](#S3.F9 "Figure 9 ‣ 3.1.1 Feedforward Learning ‣ 3.1 Supervised Learning
    ‣ 3 Deep Learning Architectures Outside of Deep Neural Networks ‣ A Survey of
    Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization") illustrates a DGP expressed as a series of Gaussian processes
    mapping data from one latent space to the next. Functions $f_{l}$ are drawn from
    a Gaussian process, i.e. $f(x)\sim\mathcal{GP}(0,k(x,x^{\prime}))$. In this setting,
    the covariance function $k$ defines the properties of the mapping function. DGP
    can be utilized for both supervised and unsupervised learning. In the supervised
    setting, the top hidden layer is observed, whereas in the unsupervised setting,
    the top hidden layer is set to a unit Gaussian as a fairly uninformative prior.
    DGP is a powerful non-parametric model but it has only been tested on small datasets.
    Also, we note that researchers have developed deep Gaussian process models with
    alternative architectures such as recurrent Gaussian processes [[113](#bib.bib113)],
    convolutional Gaussian processes [[114](#bib.bib114)] and variational auto-encoded
    deep Gaussian processes [[115](#bib.bib115)]. There exists a vast amount of literature
    on this topic that provides additional insights on deep Gaussian processes [[116](#bib.bib116)]
    [[117](#bib.bib117)] [[118](#bib.bib118)].'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S3.F9 "图 9 ‣ 3.1.1 前馈学习 ‣ 3.1 监督学习 ‣ 3 深度学习架构超越深度神经网络 ‣ 深度网络模型、优化和正则化：所有分类器可以学习的技术调查")展示了一个DGP，它被表示为一系列高斯过程，将数据从一个潜在空间映射到下一个。函数
    $f_{l}$ 来自一个高斯过程，即 $f(x)\sim\mathcal{GP}(0,k(x,x^{\prime}))$。在这种设置下，协方差函数 $k$
    定义了映射函数的属性。DGP 可以用于监督学习和无监督学习。在监督学习中，顶层隐藏层是观察到的，而在无监督学习中，顶层隐藏层被设置为单位高斯作为一种相对不具信息量的先验。DGP
    是一个强大的非参数模型，但它仅在小型数据集上进行了测试。此外，我们注意到研究人员开发了具有替代架构的深度高斯过程模型，例如递归高斯过程 [[113](#bib.bib113)]、卷积高斯过程
    [[114](#bib.bib114)] 和变分自编码深度高斯过程 [[115](#bib.bib115)]。关于这一主题存在大量文献，提供了关于深度高斯过程的额外见解
    [[116](#bib.bib116)] [[117](#bib.bib117)] [[118](#bib.bib118)]。
- en: <svg   height="90.85" overflow="visible" version="1.1" width="349.6"><g transform="translate(0,90.85)
    matrix(1 0 0 -1 0 0) translate(98.62,0) translate(0,2.26)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -94.01 69.09)" fill="#000000"
    stroke="#000000"><foreignobject width="10.38" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">X</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -6.07 70.36)" fill="#000000" stroke="#000000"><foreignobject width="12.14"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 72.67 70.36)" fill="#000000" stroke="#000000"><foreignobject
    width="12.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 151.41 70.36)" fill="#000000" stroke="#000000"><foreignobject
    width="12.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 230.67 69.09)" fill="#000000" stroke="#000000"><foreignobject
    width="11.11" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Y$</foreignobject></g></g></svg>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="90.85" overflow="visible" version="1.1" width="349.6"><g transform="translate(0,90.85)
    matrix(1 0 0 -1 0 0) translate(98.62,0) translate(0,2.26)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -94.01 69.09)" fill="#000000"
    stroke="#000000"><foreignobject width="10.38" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">X</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -6.07 70.36)" fill="#000000" stroke="#000000"><foreignobject width="12.14"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 72.67 70.36)" fill="#000000" stroke="#000000"><foreignobject
    width="12.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 151.41 70.36)" fill="#000000" stroke="#000000"><foreignobject
    width="12.14" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$f_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 230.67 69.09)" fill="#000000" stroke="#000000"><foreignobject
    width="11.11" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$Y$</foreignobject></g></g></svg>
- en: 'Figure 9: A deep Gaussian process with two hidden layers.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：具有两个隐藏层的深度高斯过程。
- en: As we discussed, non-network classifiers have been designed that contain multiple
    layers of operations, similar to a DNN. We observe that a common strategy for
    creating a deep non-network model is to add the prediction of the previous layer
    or layers to the original input feature. Likewise, novel methods can be applied
    to learn a new representation of data at each layer. We discuss these methods
    next.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的那样，已经设计出包含多个操作层的非网络分类器，类似于DNN。我们观察到，创建深度非网络模型的一种常见策略是将前一层或几层的预测添加到原始输入特征中。同样，可以应用新方法在每一层学习数据的新表示。接下来我们讨论这些方法。
- en: 3.1.2 Siamese Model
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 孪生模型
- en: 'As discussed in Section [2.3.4](#S2.SS3.SSS4 "2.3.4 Siamese Neural Network
    ‣ 2.3 Supervised Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"), a SNN represents a powerful method for similarity learning.
    However, one problem with SNNs is overfitting when there is a small number of
    training examples. The Siamese Deep Forest (SDF) [[119](#bib.bib119)] is a method
    based on DF which offers an alternative to a standard SNN. The SDF, unlike the
    SNN, uses only one DF. The first step in training a SDF is to modify the training
    examples. The training set consists of the concatenation of each pair of samples
    in the original set. If sample points $x_{i}$ and $x_{j}$ are semantically similar,
    the corresponding class label is set to zero; otherwise, the class label is set
    to one. The difference between the SDF and the DF in training is that the Siamese
    Deep Forest concatenates the original feature vector with a weighted sum of the
    tree class probabilities. Training of SDF is similar to DF; the primary difference
    is that SDF learns the class probability weights $w$ for each forest separately
    at each layer. Learning the weights for each forest can be accomplished by minimizing
    the function in Equation [4](#S3.E4 "In 3.1.2 Siamese Model ‣ 3.1 Supervised Learning
    ‣ 3 Deep Learning Architectures Outside of Deep Neural Networks ‣ A Survey of
    Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization").'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.3.4](#S2.SS3.SSS4 "2.3.4 Siamese Neural Network ‣ 2.3 Supervised Learning
    ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization")节讨论的，SNN（孪生神经网络）代表了一种强大的相似性学习方法。然而，SNN的一个问题是当训练样本数量较少时会过拟合。Siamese
    Deep Forest (SDF) [[119](#bib.bib119)] 是一种基于DF（深度森林）的方法，提供了一个替代标准SNN的选项。与SNN不同，SDF仅使用一个DF。训练SDF的第一步是修改训练样本。训练集由原始集合中每对样本的串联组成。如果样本点
    $x_{i}$ 和 $x_{j}$ 在语义上相似，则相应的类别标签设置为零；否则，类别标签设置为一。SDF和DF在训练中的区别在于，Siamese Deep
    Forest 将原始特征向量与树类概率的加权和串联在一起。SDF的训练类似于DF；主要区别在于SDF在每层单独为每个森林学习类别概率权重 $w$。通过最小化方程[4](#S3.E4
    "In 3.1.2 Siamese Model ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization")中的函数可以完成对每个森林权重的学习。'
- en: '|  | $\min\limits_{w}J_{q}(w)=\min\limits_{w}\sum_{i,j}l(x_{i},x_{j},y_{ij},w)+\lambda
    R(w)$ |  | (4) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min\limits_{w}J_{q}(w)=\min\limits_{w}\sum_{i,j}l(x_{i},x_{j},y_{ij},w)+\lambda
    R(w)$ |  | (4) |'
- en: 'Here, $w$ represents a concatenation of vectors $w^{k}$, $k=1,...,M$, $q$ is
    the SDF layer, $R(w)$ is a regularization term, and $\lambda$ is a hyper-parameter
    to control regularization. Detailed instructions on minimizing Equation [4](#S3.E4
    "In 3.1.2 Siamese Model ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization") are found in the
    literature [[119](#bib.bib119)]. The results of SDF experiments indicate that
    the SDF can achieve better classification accuracy than DF for small datasets.
    In general, all non-network models that learn data representations can take advantage
    of the Siamese architecture like SDF.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '这里，$w$ 代表向量 $w^{k}$ 的串联，$k=1,...,M$，$q$ 是SDF层，$R(w)$ 是正则化项，而 $\lambda$ 是控制正则化的超参数。最小化方程
    [4](#S3.E4 "In 3.1.2 Siamese Model ‣ 3.1 Supervised Learning ‣ 3 Deep Learning
    Architectures Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization") 的详细说明可以在文献中找到
    [[119](#bib.bib119)]。SDF实验的结果表明，SDF可以在小数据集上实现比DF更好的分类准确性。一般来说，所有学习数据表示的非网络模型都可以利用类似SDF的Siamese架构。'
- en: 3.2 Unsupervised Learning
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 无监督学习
- en: 3.2.1 Generative Adversarial Model
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 生成对抗模型
- en: 'A common element found in GANs is inclusion of a FC layer in the discriminator.
    One issue with the FC layer is that it cannot deal with the ill-condition in which
    local minima are not surrounded by spherical wells as shown in Figure [8](#S2.F8
    "Figure 8 ‣ 2.5 Optimization for Training Deep Neural Networks ‣ 2 Brief Overview
    of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from
    Deep Networks: Models, Optimizations, and Regularization"). The Generative Adversarial
    Forest (GAF) [[120](#bib.bib120)] replaces the FC layer of the discriminator with
    a deep neural decision forest (DNDF), which is discussed in Section [4](#S4 "4
    Deep Learning Optimization Outside of Deep Neural Networks ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization").
    GAF and DNDF are distinguished based on how leaf node values are learned. Instead
    of learning leaf node values iteratively, as DNDF does, GAF learns them in parallel
    across the ensemble members. The strong discriminatory power of the decision forest
    is the reason the authors recommend this method in lieu of the fully-connected
    discriminator layer.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'GANs 中一个常见的元素是鉴别器中包含一个全连接层（FC）。全连接层的问题之一是它无法处理局部最小值周围没有球形井的恶劣条件，如图 [8](#S2.F8
    "Figure 8 ‣ 2.5 Optimization for Training Deep Neural Networks ‣ 2 Brief Overview
    of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from
    Deep Networks: Models, Optimizations, and Regularization") 所示。生成对抗森林（GAF）[[120](#bib.bib120)]
    用深度神经决策森林（DNDF）替代鉴别器的全连接层，这在第 [4](#S4 "4 Deep Learning Optimization Outside of
    Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization") 节中讨论。GAF 和 DNDF 的区别在于叶节点值的学习方式。与
    DNDF 迭代学习叶节点值不同，GAF 在集成成员之间并行学习这些值。决策森林的强大区分能力是作者推荐这种方法代替全连接鉴别器层的原因。'
- en: In this previous work, the discriminator is replaced by an unconventional model.
    We hypothesize that replacing the discriminator with other classifiers such as
    Random Forest, SVM, of K nearest neighbor based on the data could result in a
    diverse GAN strategies, each of which may offer benefits for alternative learning
    problems.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项早期工作中，鉴别器被替换为一种非常规模型。我们假设，将鉴别器替换为其他分类器，如随机森林、支持向量机（SVM）或基于数据的 K 近邻，可能会导致多样化的
    GAN 策略，每种策略可能对不同的学习问题提供好处。
- en: 3.2.2 Autoencoder
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 自动编码器
- en: 'As we discussed in Section [2.4.2](#S2.SS4.SSS2 "2.4.2 Autoencoder ‣ 2.4 Unsupervised
    Learning ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization"),
    AEs offer strategies for dimensionality reduction and data reconstruction from
    compressed information. The autoencoding methodology can be found in neural networks,
    non-networks, and hybrid methods. As an example, the multilayer SVM (ML-SVM) autoencoder
    is a variation of ML-SVM with the same number of output nodes as input features
    and a single hidden layer that consists of fewer nodes than the input features.
    ML-SVM is a model with the same structure as a MLP. The distinction here is that
    the network contains SVM models as its nodes. A review of ML-SVM is discussed
    in Section [4](#S4 "4 Deep Learning Optimization Outside of Deep Neural Networks
    ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization"). The outputs of hidden nodes are fed as input
    to each SVM output node $c$ as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在第 [2.4.2](#S2.SS4.SSS2 "2.4.2 Autoencoder ‣ 2.4 Unsupervised Learning
    ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization") 节中讨论的，自动编码器（AE）提供了从压缩信息中进行降维和数据重建的策略。自动编码方法可以在神经网络、非网络和混合方法中找到。例如，多层
    SVM（ML-SVM）自动编码器是具有与输入特征相同数量的输出节点和单一隐藏层（该层的节点数少于输入特征）的 ML-SVM 的一种变体。ML-SVM 是一个具有与
    MLP 相同结构的模型。不同之处在于，该网络包含 SVM 模型作为其节点。ML-SVM 的综述讨论在第 [4](#S4 "4 Deep Learning Optimization
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization") 节中。隐藏节点的输出被作为输入传递到每个
    SVM 输出节点 $c$ 中，如下所示：'
- en: '|  | $g_{c}(f(X&#124;\theta))=\sum_{i=1}^{l}(\alpha_{i}^{c*}-\alpha_{i}^{c})K_{o}(f(x_{i}&#124;\theta),f(x&#124;\theta))+b_{c},$
    |  | (5) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $g_{c}(f(X&#124;\theta))=\sum_{i=1}^{l}(\alpha_{i}^{c*}-\alpha_{i}^{c})K_{o}(f(x_{i}&#124;\theta),f(x&#124;\theta))+b_{c},$
    |  | (5) |'
- en: where $\alpha_{i}^{c*}$ and $\alpha_{i}^{c}$ are the support vector coefficients,
    $K_{o}$ is the kernel function, and $b_{c}$ is their bias. The error backpropagates
    through the network to update the parameters.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{i}^{c*}$ 和 $\alpha_{i}^{c}$ 是支持向量系数，$K_{o}$ 是核函数，$b_{c}$ 是它们的偏置。误差通过网络反向传播以更新参数。
- en: 'Another exciting emerging research area is the combination of Kalman filters
    with deep networks. A Kalman filter is a well-known algorithm that estimates the
    optimal state of a system from a series of noisy observations. The classical Kalman
    filter [[121](#bib.bib121)] is a linear dynamical system and therefore is unable
    to model complex phenomena. For this reason, researchers developed nonlinear versions
    of Kalman filters. In a seminal contribution, Krishnan et al. [[122](#bib.bib122)]
    introduced a model that combines a variational autoencoder with Kalman filters
    for counterfactual inference of patient information. In a standard autoencoder,
    the model learns a latent space that represents the original data minus extraneous
    information or “signal noise”. In contrast, a variational autoencoder (VAE) [[68](#bib.bib68)]
    adds a constraint to the encoder that it learn a Gaussian distribution of the
    original input data. Therefore, a VAE is able to generate a latent vector by sampling
    from the learned Gaussian distribution. Deep Kalman filters (DKF) learn a generative
    model from observed sequences $\vec{x}=(x_{1},\cdots,x_{T})$ and actions $\vec{u}=(u_{1},\cdots
    u_{T-1})$, with a corresponding latent space $\vec{z}=(z_{1},\cdots,z_{T})$, as
    follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个令人兴奋的新兴研究领域是卡尔曼滤波器与深度网络的结合。卡尔曼滤波器是一个著名的算法，它通过一系列噪声观察来估计系统的最优状态。经典的卡尔曼滤波器
    [[121](#bib.bib121)] 是一个线性动态系统，因此无法建模复杂现象。正因为如此，研究人员开发了非线性版本的卡尔曼滤波器。在一个开创性的贡献中，Krishnan
    等人 [[122](#bib.bib122)] 提出了一个模型，将变分自编码器与卡尔曼滤波器结合，用于患者信息的反事实推断。在标准自编码器中，模型学习一个潜在空间，该空间表示原始数据减去多余的信息或“信号噪声”。相比之下，变分自编码器（VAE）
    [[68](#bib.bib68)] 为编码器增加了一个约束，要求其学习原始输入数据的高斯分布。因此，VAE 能够通过从学习到的高斯分布中采样生成潜在向量。深度卡尔曼滤波器（DKF）从观察到的序列
    $\vec{x}=(x_{1},\cdots,x_{T})$ 和动作 $\vec{u}=(u_{1},\cdots u_{T-1})$ 学习一个生成模型，并具有一个相应的潜在空间
    $\vec{z}=(z_{1},\cdots,z_{T})$，具体如下：
- en: '|  | <math   alttext="\begin{split}&amp;z_{1}\sim\mathcal{N}(\mu_{0},\Sigma_{0})\\
    &amp;z_{t}\sim\mathcal{N}(G_{\alpha}(z_{t-1},u_{t-1},\Delta_{t}),S_{\beta}(z_{t-1},y_{t-1},\Delta_{t}))\\'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}&amp;z_{1}\sim\mathcal{N}(\mu_{0},\Sigma_{0})\\
    &amp;z_{t}\sim\mathcal{N}(G_{\alpha}(z_{t-1},u_{t-1},\Delta_{t}),S_{\beta}(z_{t-1},y_{t-1},\Delta_{t}))\\'
- en: '&amp;x_{t}\sim\Pi(F_{k}(z_{t})),\\'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;x_{t}\sim\Pi(F_{k}(z_{t})),\\'
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd columnalign="left" ><mrow ><msub ><mi  >z</mi><mn
    >1</mn></msub><mo >∼</mo><mrow ><mi >𝒩</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub ><mi >μ</mi><mn >0</mn></msub><mo >,</mo><msub
    ><mi mathvariant="normal"  >Σ</mi><mn >0</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><msub ><mi >z</mi><mi  >t</mi></msub><mo >∼</mo><mrow
    ><mi >𝒩</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><msub ><mi  >G</mi><mi >α</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub ><mi >z</mi><mrow ><mi >t</mi><mo >−</mo><mn
    >1</mn></mrow></msub><mo >,</mo><msub ><mi >u</mi><mrow ><mi >t</mi><mo >−</mo><mn
    >1</mn></mrow></msub><mo >,</mo><msub ><mi mathvariant="normal"  >Δ</mi><mi >t</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >,</mo><mrow ><msub ><mi >S</mi><mi
    >β</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >z</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow></msub><mo
    >,</mo><msub ><mi >y</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow></msub><mo
    >,</mo><msub ><mi mathvariant="normal"  >Δ</mi><mi >t</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow ><msub ><mi  >x</mi><mi >t</mi></msub><mo >∼</mo><mrow ><mi mathvariant="normal"  >Π</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub
    ><mi >F</mi><mi  >k</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><msub ><mi >z</mi><mi >t</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><csymbol cd="latexml"  >similar-to</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑧</ci><cn type="integer" >1</cn></apply><apply
    ><ci >𝒩</ci><interval closure="open" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜇</ci><cn type="integer" >0</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >Σ</ci><cn type="integer" >0</cn></apply></interval><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑧</ci><ci >𝑡</ci></apply></apply></apply><apply ><csymbol
    cd="latexml" >similar-to</csymbol><apply ><ci  >𝒩</ci><interval closure="open"  ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐺</ci><ci  >𝛼</ci></apply><vector
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑧</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑢</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >Δ</ci><ci >𝑡</ci></apply></vector></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑆</ci><ci >𝛽</ci></apply><vector
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑧</ci><apply ><ci >𝑡</ci><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑦</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >Δ</ci><ci >𝑡</ci></apply></vector></apply></interval><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><csymbol cd="latexml" >similar-to</csymbol><apply ><ci  >Π</ci><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐹</ci><ci  >𝑘</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑧</ci><ci >𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&z_{1}\sim\mathcal{N}(\mu_{0},\Sigma_{0})\\
    &z_{t}\sim\mathcal{N}(G_{\alpha}(z_{t-1},u_{t-1},\Delta_{t}),S_{\beta}(z_{t-1},y_{t-1},\Delta_{t}))\\
    &x_{t}\sim\Pi(F_{k}(z_{t})),\\ \end{split}</annotation></semantics></math> |  |
    (6) |
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: where $\mu_{0}=0$ and $\Sigma_{0}=I_{d}$, $\Delta_{t}$ represents the difference
    between times $t$ and $t-1$, and $\Pi$ represents a distribution (e.g., Bernoulli
    for binary data) over observation $x_{t}$. The functions $G_{\alpha}$, $S_{\beta}$,
    $F_{k}$ are parameterized by a neural net. As a result, the autoencoder will learn
    $\theta=\{\alpha,\beta,k\}$ parameters. Additionally, Shashua et al. [[123](#bib.bib123)]
    introduced deep Q-learning with Kalman filters and Lu et al. [[124](#bib.bib124)]
    presented a deep Kalman filter model for video compression.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mu_{0}=0$和$\Sigma_{0}=I_{d}$，$\Delta_{t}$表示时间$t$与$t-1$之间的差异，$\Pi$表示观测$x_{t}$上的分布（例如，二值数据的伯努利分布）。函数$G_{\alpha}$、$S_{\beta}$、$F_{k}$由神经网络参数化。因此，自编码器将学习$\theta=\{\alpha,\beta,k\}$参数。此外，Shashua等人[[123](#bib.bib123)]引入了带有卡尔曼滤波器的深度Q学习，Lu等人[[124](#bib.bib124)]提出了一种用于视频压缩的深度卡尔曼滤波器模型。
- en: As we highlighted in this section, non-network methods have been designed that
    are inspired by AEs. Although ML-SVM mimics the architecture of AEs, its computational
    cost prevents the algorithm from being a practical choice. DKF takes advantage
    of the VAE idea by learning a Kalman Filter in its middle layer. Additionally,
    Feng et al. [[125](#bib.bib125)] introduced an encoder forest, a model inspired
    by the DNN autoencoder. Because the encoder forest is not a deep model, we do
    not include the details of this algorithm in our survey.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节中强调的，已经设计了受自编码器（AEs）启发的非网络方法。尽管ML-SVM模仿了AEs的架构，但其计算成本使得该算法无法成为实际的选择。DKF利用变分自编码器（VAE）的思想，在其中间层学习卡尔曼滤波器。此外，Feng等人[[125](#bib.bib125)]引入了一种编码器森林模型，该模型受到DNN自编码器的启发。由于编码器森林不是深度模型，因此我们没有在本次调查中包含该算法的细节。
- en: 4 Deep Learning Optimization Outside of Deep Neural Networks
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习优化：超越深度神经网络
- en: 'As discussed in Section [2.5](#S2.SS5 "2.5 Optimization for Training Deep Neural
    Networks ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All
    Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization"),
    gradient descent has been a prominent optimization algorithm for DNNs; however,
    it has been underutilized by non-network classifiers. Some notable exceptions
    are found in the literature. We discuss these here.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '正如[2.5节](#S2.SS5 "2.5 Optimization for Training Deep Neural Networks ‣ 2 Brief
    Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can
    Learn from Deep Networks: Models, Optimizations, and Regularization")中讨论的那样，梯度下降一直是DNN的主要优化算法；然而，它在非网络分类器中的应用不足。文献中找到了一些显著的例外情况。我们在这里进行讨论。'
- en: 'A resourceful method for constructing a deep model is to start with a DNN architecture
    and then replace nodes with non-network classifiers. As an example, the multilayer
    SVM (ML-SVM) [[126](#bib.bib126)] replaces nodes in a MLP with standard SVMs.
    ML-SVM is a multiclass classifier which contains SVMs within the network. At the
    output layer, the ML-SVM contains the same number of SVMs as the number of classes
    learned at the perceptron output layer. Each SVM at the ML-SVM output layer is
    trained in a one-versus-all fashion for one of the classes. When observing a new
    data point, ML-SVM outputs the class label corresponding to the SVM that generates
    the highest confidence. At each hidden layer, SVMs are associated with each node
    that learn latent variables. These variables are then fed to the output layer.
    At hidden layer $f(X|\theta)$ where $X$ is the training set and $\theta$ denotes
    the trainable parameters of the SVM, ML-SVM maps the hidden layer features to
    an output value as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 构建深度模型的一种资源丰富的方法是从DNN架构开始，然后用非网络分类器替换节点。例如，多层SVM（ML-SVM）[[126](#bib.bib126)]用标准SVM替换了MLP中的节点。ML-SVM是一种包含网络内SVM的多类分类器。在输出层，ML-SVM包含的SVM数量与感知器输出层学习的类别数量相同。ML-SVM输出层中的每个SVM都以一对多的方式为其中一个类别进行训练。当观察到一个新数据点时，ML-SVM输出对应于生成最高置信度的SVM的类别标签。在每个隐藏层中，SVM与学习潜在变量的每个节点相关联。这些变量随后被送到输出层。在隐藏层$f(X|\theta)$中，其中$X$是训练集，$\theta$表示SVM的可训练参数，ML-SVM将隐藏层特征映射到输出值，如下所示：
- en: '|  | $g(f(X&#124;\theta))=\sum^{l}_{i=1}y^{c}_{i}\alpha^{c}_{i}K_{o}(f(x_{i}&#124;\theta),f(X&#124;\theta))+b_{c},$
    |  | (7) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $g(f(X&#124;\theta))=\sum^{l}_{i=1}y^{c}_{i}\alpha^{c}_{i}K_{o}(f(x_{i}&#124;\theta),f(X&#124;\theta))+b_{c},$
    |  | (7) |'
- en: 'where $g$ is the output layer function, $y_{i}^{c}\in\{-1,1\}$ for each class
    $c$, $K_{o}$ is the kernel function for the output layer, $\alpha_{i}^{c}$ are
    the support vector coefficients for SVM nodes of the output layer, and $b_{c}$
    is their bias. The goal of ML-SVM is to learn the maximum support vector coefficient
    of each SVM at the output layer with respect to the objective function $J_{c}(.)$,
    as shown in Equation [8](#S4.E8 "In 4 Deep Learning Optimization Outside of Deep
    Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization").'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$g$是输出层函数，$y_{i}^{c}\in\{-1,1\}$表示每个类别$c$，$K_{o}$是输出层的核函数，$\alpha_{i}^{c}$是输出层SVM节点的支持向量系数，$b_{c}$是它们的偏差。ML-SVM的目标是学习每个SVM在输出层的最大支持向量系数，目标函数为$J_{c}(.)$，如方程[8](#S4.E8
    "在4 深度学习优化中 ‣ 深度网络能学习的所有分类器的技术概述：模型、优化和正则化")所示。
- en: '|  | $\min_{w^{c},b,\xi,\theta}J_{c}=\frac{1}{2}&#124;&#124;w^{c}&#124;&#124;^{2}+C\sum_{i}^{l}\xi_{i}$
    |  | (8) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{w^{c},b,\xi,\theta}J_{c}=\frac{1}{2}&#124;&#124;w^{c}&#124;&#124;^{2}+C\sum_{i}^{l}\xi_{i}$
    |  | (8) |'
- en: Here, $w^{c}$ represents the set of weights for class $c$, $C$ represents a
    trade-off between margin width and misclassification risk and $\xi_{i}$ are slack
    variables. ML-SVM applies gradient ascent to adapt its support vector coefficient
    towards a local maximum of $J_{c}(.)$. The support vector coefficient is defined
    as zero for values less than zero and is assigned to $C$ for values larger than
    $C$. The data is backpropagated through the network similar to traditional MLPs
    by calculating the gradient of the objective function.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$w^{c}$表示类别$c$的权重集，$C$表示边界宽度和误分类风险之间的权衡，$\xi_{i}$是松弛变量。ML-SVM通过梯度上升来调整其支持向量系数，以趋向于$J_{c}(.)$的局部最大值。支持向量系数对于小于零的值定义为零，对于大于$C$的值则赋值为$C$。数据通过计算目标函数的梯度类似于传统的MLPs在网络中反向传播。
- en: The SVMs in the hidden layer are identical. Given the same inputs, they would
    thus generate the same outputs. To diversity the SVMs, the hidden layers train
    on a perturbed version of the training set to eliminate producing similar outputs
    before training the combined ML-SVM model. The outputs of hidden layer nodes are
    constrained to generate values in the range $[-1\ldots 1]$. Despite the effort
    of ML-SVMs to learn a multi-layer data representation, this approach is currently
    not practical because adding a new node incurs a dramatic computational expense
    for large datasets.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中的SVMs是相同的。给定相同的输入，它们会生成相同的输出。为了使SVMs具有多样性，隐藏层在训练集的扰动版本上进行训练，以消除在训练组合的ML-SVM模型之前生成相似输出的问题。隐藏层节点的输出被限制在$[-1\ldots
    1]$范围内。尽管ML-SVMs努力学习多层数据表示，但由于添加新节点对大数据集来说会带来巨大的计算开销，这种方法目前并不实用。
- en: Kontschieder et al. [[127](#bib.bib127)] further incorporate gradient descent
    into a Random Forest (RF), which is a popular classification method. One of the
    drawbacks of a RF is that does not traditionally learn new internal representations
    like DNNs. The Deep Network Decision Forest (DNDF) [[127](#bib.bib127)] integrates
    a DNN into each decision tree within the forest to reduce the uncertainty at each
    decision node. In DNDF, the result of a decision node $d_{n}(x,\Theta)$ corresponds
    to the output of a DNN $f_{n}(x,\Theta)$, where $x$ is an input and $\Theta$ is
    the parameter of a decision node. DNDF must have differentiable decision trees
    to be able to apply gradient descent to the process of updating decision nodes.
    In a standard decision tree, the result of a decision node $d_{n}(x,\Theta)$ is
    deterministic. DNDF replaces the traditional decision node with a sigmoid function
    $d_{n}(x,\Theta)=\sigma(f_{n}(x;\Theta))$ to create a stochastic decision node.
    The probability of reaching a leaf node $l$ is calculated as the product of all
    decision node outputs from the root to the leaf $l$, which is expressed as $\mu_{l}$
    in this context. The set of leaf nodes $\mathcal{L}$ learns the class distribution
    $\pi$, and the class with the highest probability is the prediction of the tree.
    The aim of DNDF is to minimize its empirical risk with respect to the decision
    node parameter $\Theta$ and the class distribution $\pi$ of $\mathcal{L}$ under
    the log-loss function for a given data set.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Kontschieder等人[[127](#bib.bib127)]进一步将梯度下降算法融入随机森林（RF）中，RF是一种流行的分类方法。RF的一个缺点是它不像深度神经网络（DNNs）那样传统地学习新的内部表示。深度网络决策森林（DNDF）[[127](#bib.bib127)]将DNN集成到森林中的每棵决策树内，以减少每个决策节点的不确定性。在DNDF中，决策节点$d_{n}(x,\Theta)$的结果对应于DNN
    $f_{n}(x,\Theta)$的输出，其中$x$是输入，$\Theta$是决策节点的参数。DNDF必须具有可微分的决策树，以便能够将梯度下降应用于更新决策节点的过程。在标准决策树中，决策节点$d_{n}(x,\Theta)$的结果是确定性的。DNDF用一个sigmoid函数$d_{n}(x,\Theta)=\sigma(f_{n}(x;\Theta))$替代传统的决策节点，以创建一个随机决策节点。从根节点到叶节点$l$的所有决策节点输出的乘积计算到达叶节点$l$的概率，在这种情况下表示为$\mu_{l}$。叶节点集合$\mathcal{L}$学习类分布$\pi$，其中概率最高的类是树的预测结果。DNDF的目标是最小化相对于决策节点参数$\Theta$和叶节点集合$\mathcal{L}$的类分布$\pi$的经验风险，使用对数损失函数进行优化。
- en: 'The optimization of the empirical risk is a two-step process which is executed
    iteratively. The first step is to optimize the class distribution of leaf nodes
    $\pi_{\mathcal{L}}$ while fixing the decision node parameters and the corresponding
    DNN. At the start of optimization (iteration 0), class distribution $\pi^{{0}}$
    is set to a uniform distribution across all leaves. DNDF then iteratively updates
    the class distribution across the leaf nodes as follows for iteration t+1:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 经验风险的优化是一个两步过程，迭代执行。第一步是优化叶节点的类分布$\pi_{\mathcal{L}}$，同时固定决策节点参数和相应的DNN。在优化开始时（迭代0），类分布$\pi^{{0}}$被设为所有叶节点上的均匀分布。DNDF随后迭代更新叶节点上的类分布，更新过程如下所示，适用于迭代t+1：
- en: '|  | $\pi_{l_{y}}^{(t+1)}=\frac{1}{Z_{l}^{(t)}}\sum_{(x,y^{\prime})\in\mathcal{T}}\frac{\mathbbm{1}_{y=y^{\prime}}\pi_{l_{y}^{(t)}}\mu_{l}(x&#124;\Theta)}{\mathbb{P}_{T}[y&#124;x,\Theta,\pi^{(t)}]},$
    |  | (9) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{l_{y}}^{(t+1)}=\frac{1}{Z_{l}^{(t)}}\sum_{(x,y^{\prime})\in\mathcal{T}}\frac{\mathbbm{1}_{y=y^{\prime}}\pi_{l_{y}^{(t)}}\mu_{l}(x&#124;\Theta)}{\mathbb{P}_{T}[y&#124;x,\Theta,\pi^{(t)}]},$
    |  | (9) |'
- en: where $Z_{l}^{(t)}$ is a normalization factor ensuring that $\sum_{y}\pi^{t+1}_{l_{y}}=1$,
    $\mathbbm{1}_{q}$ is the indicator function on the argument $q$, and $\mathbb{P}_{T}$
    is the prediction of the tree.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Z_{l}^{(t)}$是一个归一化因子，确保$\sum_{y}\pi^{t+1}_{l_{y}}=1$，$\mathbbm{1}_{q}$是对参数$q$的指示函数，$\mathbb{P}_{T}$是树的预测。
- en: 'The second step is to optimize decision node parameters $\Theta$ while fixing
    the class distribution $\pi_{\mathcal{L}}$. DNDF employs gradient descent to minimize
    log-loss with respect to $\Theta$ as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是在固定类分布$\pi_{\mathcal{L}}$的情况下优化决策节点参数$\Theta$。DNDF使用梯度下降来最小化相对于$\Theta$的对数损失，具体如下：
- en: '|  | $\frac{\partial L}{\partial\Theta}(\Theta,\pi;x,y)=\sum_{n\in\mathcal{N}}\frac{\partial
    L(\Theta,\pi;x,y)}{\partial f_{n}(x;\Theta)}\frac{\partial f_{n}(x;\Theta)}{\partial\Theta}.$
    |  | (10) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial L}{\partial\Theta}(\Theta,\pi;x,y)=\sum_{n\in\mathcal{N}}\frac{\partial
    L(\Theta,\pi;x,y)}{\partial f_{n}(x;\Theta)}\frac{\partial f_{n}(x;\Theta)}{\partial\Theta}.$
    |  | (10) |'
- en: 'The second term in Equation [10](#S4.E10 "In 4 Deep Learning Optimization Outside
    of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from
    Deep Networks: Models, Optimizations, and Regularization") is the gradient of
    the DNN. Because this is commonly known, we only discuss calculating the gradient
    of the differentiable decision tree. Here, the gradient of the differentiable
    decision tree is given by:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [10](#S4.E10 "在 4 深度学习优化：深度神经网络之外 ‣ 所有分类器可以从深度网络学习的技术综述：模型、优化和正则化") 中的第二项是
    DNN 的梯度。由于这是广为人知的，我们仅讨论可微决策树的梯度计算。这里，可微决策树的梯度由下式给出：
- en: '|  | $\frac{\partial L(\Theta,\pi;x,y)}{\partial f_{n}(x;\Theta)}=d_{n}(x;\Theta)A_{n_{r}}-\bar{d}_{n}(x;\Theta)A_{n_{l}},$
    |  | (11) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial L(\Theta,\pi;x,y)}{\partial f_{n}(x;\Theta)}=d_{n}(x;\Theta)A_{n_{r}}-\bar{d}_{n}(x;\Theta)A_{n_{l}},$
    |  | (11) |'
- en: 'where $d_{n}$ is the probability of transitioning to the left child, $\bar{d}_{n}=1-d_{n}$
    is the probability of transitioning to the right child calculated by a forward
    pass through the DNN, and $n_{l}$ and $n_{r}$ indicate the left and right children
    of node $n$. To calculate the term $A$ in Equation [11](#S4.E11 "In 4 Deep Learning
    Optimization Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization"), DNDF
    performs one forward pass and one backward pass through the differentiable decision
    tree. Upon completing the forward pass, a value $A_{l}$ can be initially computed
    for each leaf node as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{n}$ 是转到左子节点的概率，$\bar{d}_{n}=1-d_{n}$ 是通过 DNN 前向传播计算的转到右子节点的概率，$n_{l}$
    和 $n_{r}$ 表示节点 $n$ 的左子节点和右子节点。为了计算方程 [11](#S4.E11 "在 4 深度学习优化：深度神经网络之外 ‣ 所有分类器可以从深度网络学习的技术综述：模型、优化和正则化")
    中的项 $A$，DNDF 通过可微决策树执行一次前向传播和一次反向传播。在完成前向传播后，可以初步计算每个叶子节点的 $A_{l}$ 值，如下所示：
- en: '|  | $A_{l}=\frac{\pi_{l_{y}}\mu_{l}}{\sum_{l}\pi_{l_{y}}\mu_{l}}.$ |  | (12)
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{l}=\frac{\pi_{l_{y}}\mu_{l}}{\sum_{l}\pi_{l_{y}}\mu_{l}}.$ |  | (12)
    |'
- en: Next, the values of $A_{l}$ for each leaf node are used to compute the values
    of $A_{m}$ for each internal node $m$. To do this, a backward pass is made through
    the decision tree, during which the values are calculated as $A_{m}=A_{n_{l}}+A_{n_{r}}$,
    where $n_{l}$ and $n_{r}$ represent the left and the right children of node $m$,
    respectively.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，利用每个叶子节点的 $A_{l}$ 值来计算每个内部节点 $m$ 的 $A_{m}$ 值。为此，通过决策树进行反向传播，在此过程中，值计算为 $A_{m}=A_{n_{l}}+A_{n_{r}}$，其中
    $n_{l}$ 和 $n_{r}$ 分别表示节点 $m$ 的左子节点和右子节点。
- en: Each layer of a standard DNN produces the output $o_{i}$ at layer $i$. As mentioned
    earlier, the goal of the DNN is to learn a mapping function $F_{i}:o_{i-1}\rightarrow
    o_{i}$ that minimizes the empirical loss at the last layer of DNN on a training
    set. Because each $F_{i}$ is differentiable, a DNN updates its parameters efficiently
    by applying gradient descent to reduce the empirical loss.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 DNN 的每一层在层 $i$ 产生输出 $o_{i}$。如前所述，DNN 的目标是学习一个映射函数 $F_{i}:o_{i-1}\rightarrow
    o_{i}$，以最小化训练集上 DNN 最后一层的经验损失。由于每个 $F_{i}$ 是可微的，DNN 通过应用梯度下降法来高效地更新其参数，以减少经验损失。
- en: 'Adopting a different methodology, Frosst et al. [[128](#bib.bib128)] distill
    a neural network into a soft decision tree. This model benefits from both neural
    network-based representation learning and decision tree-based concept explainability.
    In the Frosst soft decision tree (FSDT), each tree’s inner node learns a filter
    $w_{i}$ and a bias $b_{i}$, and leaf nodes $l$ learn a distribution of classes.
    Like the hidden units of a neural network, each inner node of the tree determines
    the probability of input $x$ at node $i$ as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 采用不同的方法论，Frosst 等人 [[128](#bib.bib128)] 将神经网络提炼成一种软决策树。这个模型结合了基于神经网络的表示学习和基于决策树的概念解释。在
    Frosst 软决策树（FSDT）中，每棵树的内部节点学习一个滤波器 $w_{i}$ 和一个偏置 $b_{i}$，叶子节点 $l$ 学习一个类别分布。类似于神经网络的隐藏单元，树的每个内部节点通过以下方式确定输入
    $x$ 在节点 $i$ 的概率：
- en: '|  | $p_{i}(x)=\sigma(\beta(xw_{i}+b_{i}))$ |  | (13) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}(x)=\sigma(\beta(xw_{i}+b_{i}))$ |  | (13) |'
- en: 'where $\sigma$ represents the sigmoid function and $\beta$ represents an inverse
    temperature whose function is to avoid soft decisions in the tree. Filter activation
    routes the sample $x$ to the left branch for values of $p_{i}$ less than $0.5$,
    and to the right branch otherwise. The probability distribution $Q^{l}$ for each
    leaf node $l$ represents the learned parameter $\phi^{l}$ at that leaf over the
    possible $k$ output classes:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 代表 sigmoid 函数，$\beta$ 代表一个逆温度，其功能是避免树中的软决策。滤波器激活将样本 $x$ 对于 $p_{i}$
    小于 $0.5$ 的值路由到左支，其他情况则路由到右支。每个叶子节点 $l$ 的概率分布 $Q^{l}$ 代表该叶子节点上的学习参数 $\phi^{l}$
    对于可能的 $k$ 输出类别：
- en: '|  | $Q_{k}^{l}=\frac{\exp(\phi_{k}^{l})}{\sum_{k^{\prime}}\exp(\phi_{k^{\prime}}^{l})}.$
    |  | (14) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q_{k}^{l}=\frac{\exp(\phi_{k}^{l})}{\sum_{k^{\prime}}\exp(\phi_{k^{\prime}}^{l})}.$
    |  | (14) |'
- en: 'The predictive distribution over classes is calculated by traversing the greatest-probability
    path. To train this soft decision tree, Frosst et al. [[128](#bib.bib128)] calculate
    a loss function $L$ that minimizes the cross entropy between each leaf, weighted
    by input vector $x$ path probability and target distribution $T$, as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 类别上的预测分布通过遍历最大概率路径进行计算。为了训练这个软决策树，Frosst 等人 [[128](#bib.bib128)] 计算了一个损失函数 $L$，该函数最小化每个叶子之间的交叉熵，按输入向量
    $x$ 的路径概率和目标分布 $T$ 加权，如下所示：
- en: '|  | $L(x)=-\log\Big{(}\sum_{l\in LeafNodes}P^{l}(x)\sum_{k}T_{k}\log Q_{k}^{l}\Big{)}$
    |  | (15) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $L(x)=-\log\Big{(}\sum_{l\in LeafNodes}P^{l}(x)\sum_{k}T_{k}\log Q_{k}^{l}\Big{)}$
    |  | (15) |'
- en: 'where $P^{l}(x)$ is the probability of reaching leaf node $l$ given input $x$.
    Frosst et al. [[128](#bib.bib128)] also introduce a regularization term to avoid
    internal nodes routing all data points on one particular path and encourage them
    to equally route data along the left and right branches. The penalty function
    calculates a sum over all internal nodes from the root to node $i$, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P^{l}(x)$ 是给定输入 $x$ 到达叶子节点 $l$ 的概率。Frosst 等人 [[128](#bib.bib128)] 还引入了一个正则化项，以避免内部节点将所有数据点路由到特定路径，并鼓励它们沿左支和右支均匀路由数据。惩罚函数计算从根节点到节点
    $i$ 的所有内部节点的总和，如下所示：
- en: '|  | $C=-\lambda\sum_{i\in InnerNodes}0.5\log(\alpha_{i})+0.5\log(1-\alpha_{i})$
    |  | (16) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $C=-\lambda\sum_{i\in InnerNodes}0.5\log(\alpha_{i})+0.5\log(1-\alpha_{i})$
    |  | (16) |'
- en: 'where $\lambda$ is a hyper-parameter set prior to training to determine the
    effect of the penalty. The cross entropy $\alpha$ for a node $i$ is the sum of
    the path probability $P^{i}(x)$ from the root to node $i$ multiplied by the probability
    of that node $p_{i}$ divided by the path probability, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是在训练前设置的超参数，用于确定惩罚的效果。节点 $i$ 的交叉熵 $\alpha$ 是从根节点到节点 $i$ 的路径概率 $P^{i}(x)$
    乘以该节点的概率 $p_{i}$ 除以路径概率的总和，如下所示：
- en: '|  | $\alpha_{i}=\frac{\sum_{x}P^{i}(x)p_{i}(x)}{\sum_{x}P^{i}(x)}.$ |  | (17)
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{i}=\frac{\sum_{x}P^{i}(x)p_{i}(x)}{\sum_{x}P^{i}(x)}.$ |  | (17)
    |'
- en: Because the probability distribution is not uniform across nodes in the penultimate
    level, this penalty function could actually hurt the generalization. The authors
    address this problem by decaying the strength of penalty function $\lambda$ exponentially
    with the depth $d$ of the node to $2^{d}$. Another challenge is that in any given
    batch of data, as the data descends the tree, the number of samples decreases
    exponentially. Therefore, the estimated probability loses accuracy further down
    the tree. Frosst et al. recommend addressing this problem by decaying a running
    average of the actual probabilities with a time window that is exponentially proportional
    to the depth of the nodes [[128](#bib.bib128)]. Although the authors report that
    the accuracy of this model was less than the deep neural network, the model offers
    an advantage of concept interpretability.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于倒数第二层节点的概率分布不均匀，这种惩罚函数可能实际上会伤害到泛化能力。作者通过将惩罚函数 $\lambda$ 的强度以节点深度 $d$ 指数级衰减到
    $2^{d}$ 来解决这个问题。另一个挑战是，在任何给定的数据批次中，随着数据下沉到树中，样本数量呈指数级减少。因此，估计的概率在树的更深层次上准确性进一步降低。Frosst
    等人建议通过使用时间窗口对实际概率的移动平均进行衰减，该窗口指数级地与节点深度成正比 [[128](#bib.bib128)]。尽管作者报告该模型的准确性低于深度神经网络，但该模型提供了概念可解释性的优势。
- en: Both DNDF and the soft decision tree fix the depth of the learned tree to a
    predefined value. In contrast, Tanno et al. [[129](#bib.bib129)] introduced the
    Adaptive Neural Tree (ANT) which can grow to any arbitrary depth. The ANT architecture
    is similar to a decision tree but at each internal node and edge, ANT learns a
    new data representation. For example, an ANT may contain one or more convolution
    layers followed by a fully-connected layer at each inner node, one or more convolution
    layers followed by an activation function such as ReLU or tanh at each edge, and
    a linear classifier at each leaf node.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Training an ANT requires two phases: growth and refinement. In the growth phase,
    starting from the root in breadth-first order one of the nodes is selected. The
    learner then evaluates three choices: 1) split the node and add a sub-tree, 2)
    deepen edge transformation by adding another layer of convolution, or 3) keep
    the current model. The model optimizes the parameters of the newly-added components
    by minimizing log likelihood via gradient descent while fixing the parameters
    of the previous portion of the tree. Eventually, the model selects the choice
    that yields the lowest log likelihood. This process repeats until the model converges.
    In the refinement phase, the model performs gradient descent on the final architecture.
    The purpose of the refinement phase is to correct suboptimal decisions that may
    have occurred during the growth phase. The authors evaluate their method on several
    standard testbeds and the results indicate that ANT is competitive with many deep
    network and non-network learners for these tasks.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Yang et al. [[130](#bib.bib130)] took a different approach; instead of integrating
    artificial neurons into the tree, they obtained a decision tree using a neural
    network. The Deep Neural Decision Tree (DNDT) employs a soft binning function
    to learn the split rules of the tree. DNDT construct a one-layer neural network
    with softmax as its activation function. The objective function of this network
    is:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{softmax}\big{(}\frac{wx+b}{\tau}\big{)}.$ |  | (18) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: 'Here, for a continuous variable $x$, we want to bin it to $n+1$, $w=[1,2,\cdots,n+1]$
    is an untrainable constant, $b$ is a learnable bin or the cutting rule in the
    tree, and $\tau$ is a temperature variable. After training this model, the decision
    tree is constructed via the Kronecker product $\otimes$. Given an input $x\in
    R^{D}$ with $D$ features, the tree rule to reach a leaf node is:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z=f_{1}(x_{1})\otimes f_{2}(x_{2})\otimes\cdots\otimes f_{D}(x_{D})$
    |  | (19) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: Here, $z$ is an almost-one-hot encoded vector that indicates the index of leaf
    node. One of the shortcomings of this method is that it cannot handle a high-dimensional
    dataset because the cost of calculating the Kronecker product becomes prohibitive.
    To overcome this problem, authors learn a classifier forest by training each tree
    on a random subset of features.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the mapping function is not differentiable. Feng et al. [[131](#bib.bib131)]
    propose a new learning paradigm for training a multilayer Gradient Boosting decision
    tree (mGBDT) [[131](#bib.bib131)] where $F_{i}$ is not differentiable. Gradient
    boosting decision tree (GBDT) is an iterative method which learns an ensemble
    of regression predictors. In GBDT, a decision tree first learns a model on a training
    set, then it computes the corresponding error residual for each training sample.
    A new tree learns a model on the error residuals, and by combining these two trees
    GBDT is able to learn a more complex model. The algorithm follows this procedure
    iteratively until it meets a prespecified number of trees for training.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，映射函数不可微分。Feng 等人 [[131](#bib.bib131)] 提出了一个新的学习范式来训练多层梯度提升决策树 (mGBDT)
    [[131](#bib.bib131)]，其中 $F_{i}$ 是不可微分的。梯度提升决策树 (GBDT) 是一种迭代方法，它学习一个回归预测器的集成。在
    GBDT 中，决策树首先在训练集上学习一个模型，然后计算每个训练样本的相应误差残差。一个新树在误差残差上学习一个模型，通过结合这两棵树，GBDT 能够学习一个更复杂的模型。该算法按照此过程迭代，直到满足预设的训练树的数量。
- en: Since gradient descent is not applicable to mGBDT, Feng et al. [[131](#bib.bib131)]
    obtain a “pseudo-inverse” mapping. In this mapping, $G^{t}_{i}$ represents the
    pseudo-inverse of $F^{t-1}_{i}$ at iteration $t$, such that $G^{t}_{i}(F^{t-1}_{i}(o_{i-1}))\sim
    o_{i-1}$. After performing backward propagation and calculating $G^{t}_{i}$, forward
    propagation is performed by fitting a pseudo-label $z^{t}_{i-1}$ from $G^{t}_{i}$
    to $F^{t-1}_{i}$. The last layer $F_{m}$ computes $z^{t}_{m}$ based on the true
    labels at iteration $t$, where $i\in\{2\ldots m\}$. After this step, pseudo-labels
    for previous layers are computed via pseudo-inverse mapping. To initialize mGBDT
    at iteration $t=0$, each intermediate (hidden) layer outputs Gaussian noise and
    $F^{0}_{i}$ represent depth-constrained trees that will later be refined. Feng
    et al. [[131](#bib.bib131)] thus create a method that is inspired by gradient
    descent yet is applicable in situations where true gradient descent cannot be
    effectively applied.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度下降不适用于 mGBDT，Feng 等人 [[131](#bib.bib131)] 获取了一个“伪逆”映射。在这个映射中，$G^{t}_{i}$
    代表第 $t$ 次迭代中 $F^{t-1}_{i}$ 的伪逆，使得 $G^{t}_{i}(F^{t-1}_{i}(o_{i-1}))\sim o_{i-1}$。在执行反向传播并计算
    $G^{t}_{i}$ 后，通过从 $G^{t}_{i}$ 拟合伪标签 $z^{t}_{i-1}$ 到 $F^{t-1}_{i}$ 来执行前向传播。最后一层
    $F_{m}$ 根据第 $t$ 次迭代中的真实标签计算 $z^{t}_{m}$，其中 $i\in\{2\ldots m\}$。在此步骤之后，通过伪逆映射计算前层的伪标签。为了在第
    $t=0$ 次迭代中初始化 mGBDT，每个中间（隐藏）层输出高斯噪声，$F^{0}_{i}$ 代表将来会被细化的深度约束树。因此，Feng 等人 [[131](#bib.bib131)]
    创造了一种受梯度下降启发但适用于无法有效应用真正梯度下降的情况的方法。
- en: In this section, we examine methods that apply gradient descent to non-network
    models. As we observed, one way of utilizing gradient descent is to replace the
    hidden units in a network with a differentiable algorithm like SVM. Another common
    theme we recognized was to transform deterministic decision-tree nodes into stochastic
    versions that offer greater representational power. Alternatively, trees or other
    ruled-based models can be built using neural networks.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了将梯度下降应用于非网络模型的方法。正如我们所观察到的，利用梯度下降的一种方式是用像 SVM 这样的可微分算法替代网络中的隐藏单元。另一个我们认识到的常见主题是将确定性决策树节点转变为提供更大表征能力的随机版本。或者，也可以使用神经网络构建树或其他基于规则的模型。
- en: 5 Deep Learning Regularization Outside of Deep Neural Networks
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 深度学习在深度神经网络之外的正则化
- en: 'We have discussed some of the common regularization methods used by DNNs in
    Section [2.6](#S2.SS6 "2.6 Regularization ‣ 2 Brief Overview of Deep Neural Networks
    ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization"). Now we focus on how these methods have been
    applied to non-network classifiers in the literature. It is worth mentioning that
    while most models introduced in this section are not deep models, we investigate
    how non-network models can improve their performance by applying regularization
    methods typically associated with the deep operations found in DNNs.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第 [2.6](#S2.SS6 "2.6 Regularization ‣ 2 Brief Overview of Deep Neural Networks
    ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization") 节中讨论了 DNNs 使用的一些常见正则化方法。现在我们关注这些方法如何在文献中应用于非网络分类器。值得一提的是，尽管本节介绍的大多数模型不是深度模型，但我们研究了非网络模型如何通过应用通常与
    DNNs 中深度操作相关的正则化方法来提高其性能。'
- en: 5.1 Parameter Norm Penalty
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 参数范数惩罚
- en: 'Problems arise when a model is learned from data that contain a large number
    of redundant features. For example, selecting relevant genes associated with different
    types of cancer is challenging because of a large number of redundancies may exist
    in the gene’s long string of features. There are two common ways to eliminate
    redundant features: the first way is to perform feature selection and then train
    a classifier from the selected features; the second way is to simultaneously perform
    feature selection and classification. As we discussed in Section [2.6.1](#S2.SS6.SSS1
    "2.6.1 Parameter Norm Penalty ‣ 2.6 Regularization ‣ 2 Brief Overview of Deep
    Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization"), DNNs apply a $L_{1}$ or $L_{2}$ penalty
    function to penalize large weights. In this section, we investigate how the traditional
    DNN idea of penalizing features can be applied to non-network classifiers to simultaneously
    select high-ranked features and perform classification.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard SVMs employ the $L_{2}$ norm penalty to penalize weights in a manner
    similar to DNNs. However, the Newton Linear Programming SVM (NLP-SVM) [[132](#bib.bib132)]
    replaces the $L_{2}$ norm penalty with the $L_{1}$ norm penalty. This has the
    effect of setting small hyperparameter coefficients to zero, thus enabling NLP-SVM
    to select important features automatically. A different way to penalize non-important
    features in SVMs is to employ a Smoothly Clipped Absolute Deviation (SCAD) [[133](#bib.bib133)]
    function. The $L_{1}$ penalty function can be biased because it imposes a larger
    penalty on large coefficients; in contrast, SCAD can give a nearly unbiased estimation
    of large coefficients. SCAD learns a non-convex penalty function as shown in Equation [20](#S5.E20
    "In 5.1 Parameter Norm Penalty ‣ 5 Deep Learning Regularization Outside of Deep
    Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks:
    Models, Optimizations, and Regularization").'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="p_{\lambda}(&#124;w&#124;)=\begin{cases}\lambda&#124;w&#124;&amp;\text{if
    }&#124;w&#124;\leq\lambda\\ -\frac{(&#124;w&#124;^{2}-2a\lambda&#124;w&#124;+\lambda^{2})}{2(a-1)}&amp;\text{if
    }\lambda<&#124;w&#124;\leq a\lambda\\'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: \frac{(a+1)\lambda^{2}}{2}&amp;\text{if }&#124;w&#124;>a\lambda\end{cases}"
    display="block"><semantics ><mrow  ><mrow ><msub ><mi  >p</mi><mi >λ</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mrow  ><mo
    stretchy="false"  >&#124;</mo><mi >w</mi><mo stretchy="false" >&#124;</mo></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mi
    >λ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >&#124;</mo><mi
    >w</mi><mo stretchy="false" >&#124;</mo></mrow></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow  ><mtext >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >&#124;</mo><mi >w</mi><mo stretchy="false" >&#124;</mo></mrow></mrow><mo >≤</mo><mi
    >λ</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow  ><mo >−</mo><mstyle
    displaystyle="false" ><mfrac ><mrow  ><mo stretchy="false"  >(</mo><mrow ><mrow
    ><msup ><mrow ><mo stretchy="false"  >&#124;</mo><mi >w</mi><mo stretchy="false"  >&#124;</mo></mrow><mn
    >2</mn></msup><mo >−</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em"  >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >λ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >&#124;</mo><mi >w</mi><mo stretchy="false"  >&#124;</mo></mrow></mrow></mrow><mo
    >+</mo><msup ><mi >λ</mi><mn >2</mn></msup></mrow><mo stretchy="false"  >)</mo></mrow><mrow
    ><mn >2</mn><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow
    ><mi >a</mi><mo >−</mo><mn >1</mn></mrow><mo stretchy="false" >)</mo></mrow></mrow></mfrac></mstyle></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >λ</mi></mrow><mo ><</mo><mrow ><mo stretchy="false" >&#124;</mo><mi >w</mi><mo
    stretchy="false" >&#124;</mo></mrow><mo >≤</mo><mrow  ><mi >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >λ</mi></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mstyle
    displaystyle="false"  ><mfrac ><mrow ><mrow ><mo stretchy="false" >(</mo><mrow
    ><mi >a</mi><mo >+</mo><mn >1</mn></mrow><mo stretchy="false" >)</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><msup ><mi >λ</mi><mn >2</mn></msup></mrow><mn
    >2</mn></mfrac></mstyle></mtd><mtd columnalign="left"  ><mrow ><mrow  ><mtext
    >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >&#124;</mo><mi >w</mi><mo stretchy="false" >&#124;</mo></mrow></mrow><mo >></mo><mrow
    ><mi  >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >λ</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑝</ci><ci  >𝜆</ci></apply><apply ><ci >𝑤</ci></apply></apply><apply ><csymbol
    cd="latexml"  >cases</csymbol><apply ><ci >𝜆</ci><apply ><ci  >𝑤</ci></apply></apply><apply
    ><apply ><ci  ><mtext >if </mtext></ci><apply ><ci >𝑤</ci></apply></apply><ci
    >𝜆</ci></apply><apply ><apply  ><apply ><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><ci >𝑤</ci></apply><cn type="integer"  >2</cn></apply><apply ><cn type="integer"  >2</cn><ci
    >𝑎</ci><ci >𝜆</ci><apply ><ci >𝑤</ci></apply></apply></apply><apply ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci >𝜆</ci><cn type="integer"  >2</cn></apply></apply><apply
    ><cn type="integer" >2</cn><apply ><ci >𝑎</ci><cn type="integer"  >1</cn></apply></apply></apply></apply><apply
    ><apply ><apply  ><ci ><mtext >if </mtext></ci><ci >𝜆</ci></apply><apply ><ci  >𝑤</ci></apply></apply><apply
    ><apply ><ci  >𝑎</ci><ci >𝜆</ci></apply></apply></apply><apply ><apply ><apply
    ><ci >𝑎</ci><cn type="integer" >1</cn></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝜆</ci><cn type="integer" >2</cn></apply></apply><cn
    type="integer" >2</cn></apply><apply ><apply ><ci  ><mtext >if </mtext></ci><apply
    ><ci >𝑤</ci></apply></apply><apply ><ci  >𝑎</ci><ci >𝜆</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >p_{\lambda}(&#124;w&#124;)=\begin{cases}\lambda&#124;w&#124;&\text{if
    }&#124;w&#124;\leq\lambda\\ -\frac{(&#124;w&#124;^{2}-2a\lambda&#124;w&#124;+\lambda^{2})}{2(a-1)}&\text{if
    }\lambda<&#124;w&#124;\leq a\lambda\\ \frac{(a+1)\lambda^{2}}{2}&\text{if }&#124;w&#124;>a\lambda\end{cases}</annotation></semantics></math>
    |  | (20) |
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: SCAD equates with $L_{1}$ penalty function until $|w|=\lambda$, then smoothly
    transitions to a quadratic function until $|w|=a\lambda$, after which it remains
    a constant for all $|w|>a\lambda$. As shown by Fan et al. [[134](#bib.bib134)],
    SCAD has better theoretical properties than the $L_{1}$ function.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'One limitation of decision tree classifiers is that the number of training
    instances that can be selected at each branch in the tree decreases with the tree
    depth. This downward sampling may cause less relevant or redundant features to
    be selected near the bottom of the tree. To address this issue, Dang et al. [[135](#bib.bib135)]
    proposed to penalize features that were never selected in the process of making
    a tree. In a Regularized Random Forest (RRF) [[135](#bib.bib135)], the information
    gain for a feature $j$ is specified as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Gain(j)=\begin{cases}\lambda.Gain(j)&amp;\text{$j\not\in F$}\\ Gain(f_{i})&amp;\text{$j\in
    F$}\end{cases}$ |  | (21) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: where $F$ is the set of features used earlier in the path, $f_{i}\in F$, and
    $\lambda\in[0,1]$ is the penalty coefficient. RRF avoids including a new feature
    $j$, except when the value of $Gain(j)$ is greater than $\max\limits_{i}\big{(}Gain(f_{i})\big{)}$.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve RRF, Guided RRF (GRRF) [[136](#bib.bib136)] assigns a different
    penalty coefficient $\lambda_{j}$ to each feature instead of assigning the same
    penalty coefficient to all features. GRRF employs the importance score from a
    pre-trained RF on the training set to refine the selection of features at a given
    node. The importance score of feature $j$ in an RF with $T$ trees is the mean
    of gain for features in the RF. The important scores evaluate the contribution
    of features for predicting classes. The GRRF uses the normalized importance score
    to control the degree of regularization of the penalty coefficient as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lambda_{j}=(1-\gamma)\lambda_{0}+\gamma Imp^{\prime}_{j},$ |  | (22)
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{0}\in(0,1]$ is the base penalty coefficient and $\gamma\in[0,1]$
    controls the weight of the normalized importance score. The GRRF and RRF are computationally
    inexpensive methods that are able to select stronger features and avoid redundant
    features.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Dropout
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As detailed in Section [2.6.2](#S2.SS6.SSS2 "2.6.2 Dropout ‣ 2.6 Regularization
    ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization"), dropout
    is a method that prevents DNNs from overfitting by randomly dropping nodes during
    the training. Dropout can be added to other machine learning algorithms through
    two methods: by dropping features or by dropping models in the case of ensemble
    methods. Dropout has also been employed by dropping input features during training
    [[137](#bib.bib137)] [[138](#bib.bib138)]. Here we look at techniques that have
    been investigated for dropping input features, particularly in non-network classifiers.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Rashmi et al. [[139](#bib.bib139)] applied dropout to Multiple Additive Regression
    Trees (MART) [[140](#bib.bib140)] [[141](#bib.bib141)]. MART is a regression tree
    ensemble which iteratively refines its model by continually adding trees that
    fit the loss function derivatives from the previous version of the ensemble. Because
    trees added at later iterations may only impact a small fraction of the training
    set and thus over-specialize, researchers previously used shrinkage to exclude
    a random subset of leaf nodes during each tree-adding step. More recently, Rashmi
    et al. integrated the deep-learning idea of dropout into MART. Using dropout,
    a subset of the trees are temporarily dropped. A new tree is created based on
    the loss function for the on-dropped trees. This new tree is combined with the
    previously-dropped trees into a new ensemble. This method, Dropout Multiple Additive
    Regression Trees (DART) [[139](#bib.bib139)], weights the votes for the new and
    re-integrated trees to have the same effect on the final model output as the original
    set of trees. Other researchers have experimented with permanently removing a
    strategic subset of the dropped trees as well [[142](#bib.bib142)].
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Early Stopping
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core concept of early stopping is to terminate DNN training once performance
    on the validation set is not improving. One potential advantage of Deep Forest
    [[93](#bib.bib93)] over DNNs is that DF can determine the depth of a model automatically.
    In DF, if the model performance does not increase on the validation set after
    adding a new layer, the learning terminates. Unlike DNNs, DF may avoid the tendency
    to overfit as more layers are added. Thus, while early stopping does not necessarily
    enjoy the primary outcome of preventing such overfitting, it can provide additional
    benefits such as shortening the validation cycle in the search for the optimal
    tree depth.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Data Augmentation
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in Section [2.6.3](#S2.SS6.SSS3 "2.6.3 Data Augmentation ‣ 2.6
    Regularization ‣ 2 Brief Overview of Deep Neural Networks ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization"),
    data augmentation is a powerful method for improving DNN generalization. However,
    little research has investigated the effects of data augmentation methods on non-network
    classifiers. As demonstrated by Wong et al. [[143](#bib.bib143)], the SVM classifier
    does not always benefit from data augmentation, in contrast to DNNs. However,
    Xu [[144](#bib.bib144)] ran several data augmentation experiments on synthetic
    datasets and observed that data augmentation did enhance the performance of random
    forest classifiers. Offering explanations for the circumstances in which such
    augmentation is beneficial is a needed area for future research.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 6 Hybrid Models
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hybrid models can be defined as a combination of two or more classes of models.
    There are many ways to construct hybrid models, such as DNDF [[127](#bib.bib127)]
    which integrates a deep network into a decision forest as explained in Section
    [4](#S4 "4 Deep Learning Optimization Outside of Deep Neural Networks ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"). In this section we discuss other examples of hybrid models.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '混合模型可以定义为两种或多种类别模型的组合。有许多方法来构建混合模型，例如 DNDF [[127](#bib.bib127)]，它将深度网络集成到决策森林中，如第
    [4](#S4 "4 Deep Learning Optimization Outside of Deep Neural Networks ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization") 节所解释。在本节中，我们讨论其他混合模型的示例。'
- en: One motivation for combining aspects of multiple models is to find a balance
    between classification accuracy and computational cost. Energy consumption by
    mobile devices and cloud servers is an increasing concern for responsive applications
    and green computing. Decision forests are computationally inexpensive models because
    of the conditional property of decision trees. Conversely, while CNNs are less
    efficient, they can achieve higher accuracy because of their representation-learning
    capabilities. Ioannou et al. [[145](#bib.bib145)] introduced the Conditional Neural
    Network (CondNN) to reduce computation in a CNN model by introducing a routing
    method similar to that found in decision trees. In CondNN, each node in layer
    $l$ is connected to a subset of nodes from the previous layer, $l-1$. Given a
    fully trained network, for every two consecutive layers a matrix $\Lambda_{(l-1,l)}$
    stores the activation values of these two layers. By rearranging elements of $\Lambda_{(l-1,l)}$
    based on highly-active pairs for each class in the diagonal and zeroing out off-diagonal
    elements, the CondNN develops explicit routes $\Lambda_{(l,l-1)}^{route}$ through
    nodes in the network. CondNN incurs profoundly lower computation cost compared
    to other DNNs at test time; whereas, CondNN’s accuracy remains similar to larger
    models. We note that DNN size can be also be reduced by employing Bayesian optimization,
    as investigated by Blundell et al. [[146](#bib.bib146)] and by Fortunato et al.
    [[147](#bib.bib147)]. These earlier efforts provide evidence that Bayesian neural
    networks are able to decrease network size even more than CondNNs while maintaining
    a similar level of accuracy.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 结合多种模型的一个动机是为了在分类准确性和计算成本之间找到平衡。移动设备和云服务器的能耗对响应式应用程序和绿色计算日益成为关注点。决策森林由于决策树的条件属性，计算成本低廉。相对而言，尽管卷积神经网络（CNN）效率较低，但由于其表示学习能力，它们能够实现更高的准确性。Ioannou
    等人[[145](#bib.bib145)] 引入了条件神经网络（CondNN），通过引入类似于决策树的路由方法来减少 CNN 模型中的计算。在 CondNN
    中，层 $l$ 中的每个节点连接到上一层 $l-1$ 的一个子集节点。给定一个完全训练好的网络，对于每两个连续层，矩阵 $\Lambda_{(l-1,l)}$
    存储这两层的激活值。通过根据对角线中每个类别的高活跃对来重新排列 $\Lambda_{(l-1,l)}$ 的元素，并将非对角线元素置零，CondNN 通过网络中的节点开发出明确的路由
    $\Lambda_{(l,l-1)}^{route}$。与其他深度神经网络（DNN）相比，CondNN 在测试时的计算成本显著降低；而且，CondNN 的准确性保持与更大模型相似。我们注意到，通过使用贝叶斯优化（如
    Blundell 等人[[146](#bib.bib146)] 和 Fortunato 等人[[147](#bib.bib147)] 研究的那样），DNN
    的规模也可以被缩小。这些早期的努力提供了证据，表明贝叶斯神经网络能够在保持类似准确性水平的同时进一步减小网络规模。
- en: Another direction for blending a deep network with a non-network classifier
    is to improve the non-network model by learning a better representation of data
    via a deep network. Zoran et al. [[148](#bib.bib148)] introduce the differentiable
    boundary tree (DBT) in order to integrate a DNN into the boundary tree [[149](#bib.bib149)]
    to learn a better representation of data. The newly-learned data representation
    leads to a simpler boundary tree because the classes are well separated. The boundary
    tree is an online algorithm in which each node in the tree corresponds to a sample
    in the training set. The first sample together with its label are established
    as the tree root. Given a new query sample $z$, the sample traverses through the
    tree from the root to find the closest node $n$ based on some distance function
    like the Euclidean distance function. If the label of the nearest node in the
    tree is different from the query sample, a new node containing the query $z$ is
    added as a child of the closest node $n$ in the tree; otherwise the query node
    $z$ is discarded. Therefore, each edge in the tree marks the boundary between
    two classes and each node tends to be close to these boundaries.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Transitions between nodes in a standard boundary tree are deterministic. DBT
    combines a SoftMax cost function with a boundary tree, resulting in stochastic
    transitions. Let $x$ be a training sample and $c$ be the one-hot encoding label
    of that sample. Given the current node $x_{i}$ in the tree and a query node $z$,
    the transition probability from node $x_{i}$ to node $x_{j}$, where $x_{j}\in\{child(x_{i}),x_{i}\}$
    is the SoftMax of the negative distance between $x_{j}$ and $z$. This is shown
    in Equation [23](#S6.E23 "In 6 Hybrid Models ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization").'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}p(x_{i}\rightarrow x_{j}&#124;z)=\underset{i,j\in child(i)}{\operatorname{SoftMax}}(-d(x_{j},z))\\
    =\frac{\exp(-d(x_{j},z))}{\underset{j^{\prime}\in\{i,j\in child(i)\}}{\sum}\exp(-d(x_{j},z))}\end{gathered}$
    |  | (23) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: 'The probability of traversing a particular path in the boundary tree, given
    a query node $z$, is the product of the probability of each transition along the
    path from the root to the final node $x_{final^{*}}$ in the tree. The final class
    log probability of DBT is computed by summing the probabilities of all transitions
    to the parent of $x_{final^{*}}$ together with the probabilities of the final
    node and its siblings. The set $sibling(x_{i})$ consists of all nodes sharing
    the same parent with node $x_{i}$ and the node $x_{i}$ itself. As discussed earlier,
    a DNN $f_{\theta}(x)$ transforms the inputs to learn a better representation.
    The final class log probabilities for the query node $z$ are calculated as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}\log p(c&#124;f_{\theta}(z))=\smashoperator[]{\sum_{x_{i}\rightarrow
    x_{j}\in path^{\dagger}&#124;f_{\theta}(z)}^{}}\log p(f_{\theta}(x_{i})\rightarrow
    f_{\theta}(x_{j})&#124;f_{\theta}(z))\\ +\log\smashoperator[]{\sum_{x_{k}\in sibling(x_{final^{*}})}^{}}p(parent(f_{\theta}(x_{k}))\rightarrow
    f_{\theta}(x_{k})&#124;f_{\theta}(z))c(x_{k}).\end{gathered}$ |  | (24) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: 'In Equation [24](#S6.E24 "In 6 Hybrid Models ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization"), $path^{\dagger}$
    denotes $path^{*}$ (the path to the final node $x_{final^{*}}$) without the last
    transition, and $sibling(x)$ represents node $x$ and all other nodes sharing the
    same parent with node $x$. The gradient descent algorithm can be applied to Equation
    [24](#S6.E24 "In 6 Hybrid Models ‣ A Survey of Techniques All Classifiers Can
    Learn from Deep Networks: Models, Optimizations, and Regularization") by plugging
    in a loss function to learn parameter $\theta$ of the DNN. However, gradient descent
    cannot be applied easily to DBT because of the node and edge manipulations in
    the graph. To address this issue, DBT transforms a small subset of training examples
    via a DNN and builds a boundary tree based on the transformed examples. Next,
    DBT transforms a query node $z$ via the same DNN and calculates the log probability
    of a class according to Equation [24](#S6.E24 "In 6 Hybrid Models ‣ A Survey of
    Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"). The DNN employs gradient descent to update its parameters
    by propagating the gradient of log loss probability. DBT discards this boundary
    tree and iteratively builds a new boundary tree as described until a convergence
    criteria is met. In the described method, the authors set a specific threshold
    for the loss value to terminate the training. DBT is able to achieve greater accuracy
    with a simpler tree than original boundary tree as shown by the authors on the
    MNIST dataset [[97](#bib.bib97)]. One of the biggest advantages of DBT is its
    interpretability. However, DBT is computationally an expensive method because
    a new computation graph needs to be built, which makes batching inefficient. Another
    limitation is that the algorithm needs to switch between building the tree and
    updating the tree. Therefore, scaling to large datasets is fairly prohibitive.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Yet another way of building a hybrid model is to learn a new representation
    of data with a DNN, then hand the resulting feature vectors off to other classifiers
    to learn a model. Tang [[150](#bib.bib150)] explored replacing the last layer
    of DNNs with a linear SVM for classification tasks. The activation values of the
    penultimate layer are fed as input to an SVM with a $L_{2}$ regularizer. The weights
    of the lower layer are learned through momentum gradient descent by differentiating
    the SVM objective function with respect to activation of the penultimate layer.
    The author’s experiments on the MNIST [[97](#bib.bib97)] and CIFAR-10 [[151](#bib.bib151)]
    datasets demonstrate that replacing a CNN’s SoftMax output layer with SVM yields
    a lower test error. Tang et al. [[150](#bib.bib150)] postulate that the performance
    gain is due to the superior regularization effect of the SVM loss function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that in their experiment on MNIST [[97](#bib.bib97)],
    Tang first used PCA to reduce the features and then fed the reduced feature vectors
    as input to their model. Also, Niu et al. [[152](#bib.bib152)] replaced the last
    layer of a CNN with an SVM which similarly resulted in lowering test error of
    the model compare to a CNN on the MNIST dataset. Similar to these methods, Zareapoor
    et al. [[153](#bib.bib153)], Nagi et al. [[154](#bib.bib154)], Bellili et al.
    [[155](#bib.bib155)], and Azevedo et al. [[156](#bib.bib156)] replace the last
    layer of a DNN with an SVM. In these cases, their results from multiple datasets
    reveal that employing a SVM as the last layer of a neural network can improve
    the generalization of the network.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhao et al. [[157](#bib.bib157)] replace the last layer of a deep network with
    a visual hierarchical tree to learn a better solution for image classification
    problems. A visual hierarchical tree with $L$ levels organizes $N$ objects classes
    based on their visual similarities in its nodes. Deeper in the tree, groups become
    more separated wherein each leaf node should contain instances of one class. The
    class similarity between the class $c_{i}$ and $c_{j}$ is defined as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S_{i,j}=S(c_{i},c_{j})=\exp\Big{(}-\frac{d(x_{i},x_{j})}{\sigma}\Big{)}.$
    |  | (25) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: Here, $d(x_{i},x_{j})$ represents the distance between the deep representation
    of instances of classes $c_{i}$ and $c_{j}$, and $\sigma$ is automatically determined
    by a self-tuning technique. After calculating matrix $S$, hierarchical clustering
    is employed to learn a visual hierarchical tree.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: In a traditional visual hierarchical tree, some objects might be assigned to
    incorrect groups. A level-wise mixture model (LMM) [[157](#bib.bib157)] aims to
    improve this visual hierarchical tree by learning a new representation of data
    via a DNN then updating the tree during training. For a given tree, matrix $\Psi_{y_{i},t_{i}}$
    denotes the probability of objects with label $y$ belonging to group $t$ in the
    tree. First, LMM updates the DNN parameters and the visual hierarchical tree as
    is done with a traditional DNN. The only difference is a calculation of two gradients,
    one based on the parameters of the DNN and other one based on the parameters of
    the tree. Second, LMM updates the matrix $\Psi_{y_{i},t_{i}}$ for each training
    sample separately and updates the parameters of the DNN and the tree afterwards.
    To update the $\Psi$, the posterior probability of the assigning group $t_{i}$
    for the object $x_{i}$ is calculated based on the number of samples having the
    same label $y$ as the label of $x_{i}$ in group $t$. For a given test image, LMM
    learns a new representation of the image based on the DNN and then obtains a prediction
    by traversing the tree. One of the advantages of a LMM is that over time, by learning
    a better representation of data via DNN, the algorithm can update the visual hierarchical
    tree.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, two different data views are available. As an example, one view
    might contain video and the another sound. Canonical correlation analysis (CCA)
    [[158](#bib.bib158)] and kernel canonical correlation analysis (KCCA) [[159](#bib.bib159)]
    offer standard statistical methods for learning view representations that each
    the most predictable by the other view. Nonlinear representations learned by KCCA
    can achieve a higher correlation than linear representations learned by CCA. Despite
    the advantages of KCCA, the kernel function faces some drawbacks. Specifically,
    the representation is bound to the fixed kernel. Furthermore, because is model
    is nonparametric, training time as well as the time to compute the new data representation
    scales poorly with the size of a training set.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Andrews et al. [[160](#bib.bib160)] proposed to apply deep networks to learn
    a nonlinear data representation instead of employing a kernel function. Their
    resulting deep canonical correlation analysis (DCCA) consists of two separate
    deep networks for learning a new representation for each view. The new representation
    learned by the final layer of networks $H_{1}$ and $H_{2}$ is fed to CCA. To compute
    the objective gradient of DCCA, the gradient of the output of the correlation
    objective with respect to the new representation can be calculated as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\partial_{corr}(H_{1},H_{2})}{\partial H_{1}}$ |  | (26) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: 'After this computation, backpropagation is applied to find the gradient with
    respect to all parameters. The details of calculating the gradient in Equation
    [26](#S6.E26 "In 6 Hybrid Models ‣ A Survey of Techniques All Classifiers Can
    Learn from Deep Networks: Models, Optimizations, and Regularization") are provided
    by the authors [[160](#bib.bib160)].'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: While researchers have also created LSTM methods that employ tree structures
    [[161](#bib.bib161)] [[162](#bib.bib162)], these methods utilize the data structure
    to improve a network model rather than employing tree-based learning algorithms.
    Similarly, other researches integrate non-network classifiers into a network structure.
    Cimino et al. [[163](#bib.bib163)] and Agarap [[164](#bib.bib164)] introduce hybrid
    models. These two methods apply LSTM and GRU, respectively, to learn a network
    representation. Unlike traditional DNNs, the last layer employs a SVM for classification.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The work surveyed in this section provides evidence that deep neural nets are
    capable methods for learning high-level features. These features, in turn, can
    be used to improve the modeling capability for many types of supervised classifiers.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of classifiers which integrate deep network components into
    non-network classifiers.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Methods | Classifiers |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Architecture | Feedforward | ANT [[129](#bib.bib129)], DNDT [[130](#bib.bib130)],
    DBN [[45](#bib.bib45)], Deep PCA [[105](#bib.bib105)], DF [[93](#bib.bib93)],
    DPG [[116](#bib.bib116)], R2-SVM [[101](#bib.bib101)], D-SVM [[100](#bib.bib100)],
    DTA-LS-SVM [[94](#bib.bib94)], SFDT [[128](#bib.bib128)] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| Autoencoder | DKF [[122](#bib.bib122)], eForest [[125](#bib.bib125)], ML-SVM
    [[126](#bib.bib126)] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| Siamese Model | SDF [[119](#bib.bib119)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| Generative Adversarial Model | GAF [[120](#bib.bib120)] |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| Optimization | Gradient Decent | DNDF [[127](#bib.bib127)], mGBDT [[131](#bib.bib131)],
    ML-SVM [[126](#bib.bib126)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| Regularization | Parameter Norm Penalty | NLP-SVM [[132](#bib.bib132)], GRRF
    [[136](#bib.bib136)], RRF [[135](#bib.bib135)] , SCAD-SVM [[133](#bib.bib133)]
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| Dropout | DART [[139](#bib.bib139)] |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| Hybrid Model |  | CondCNN [[145](#bib.bib145)], DBT [[148](#bib.bib148)],
    DCCA [[160](#bib.bib160)], DNDF [[127](#bib.bib127)], DNN+SVM [[150](#bib.bib150)]
    [[152](#bib.bib152)] [[153](#bib.bib153)] [[154](#bib.bib154)] [[155](#bib.bib155)]
    [[156](#bib.bib156)], LMM [[157](#bib.bib157)] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: 'In this survey, we aim to provide a thorough review of non-network models that
    utilize the unique features of deep network models. Table [1](#S6.T1 "Table 1
    ‣ 6 Hybrid Models ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization") provides a summary of such
    non-network models, organized based on four aspects of deep networks: model architecture,
    optimization, regularization, and hybrid model fusing. A known advantage of traditional
    deep networks compared with non-network models has been the ability to learn a
    better representation of input features. Inspired by various deep network architectures,
    deep learning of non-network classifiers has resulted in methods to also learn
    new feature representations. Another area where non-network classifiers have benefited
    from recent deep network research is applying backpropagation optimization to
    improve generalization. This table summarizes published efforts to apply regularization
    techniques that improve neural network generalization. The last category of models
    combines deep network classifiers and non-network classifiers to increase overall
    performance.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 7 Experiments
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we survey a wide variety of models and methods. Our goal is to
    demonstrate that diverse types of models can benefit from deep learning techniques.
    To highlight this point, we empirically compare the performance of many techniques
    described in this survey. This comparison includes deep and shallow networks as
    well as and non-network learning algorithms. Because of the variety of classifiers
    that are surveyed, we organize the comparison based on the learned model structure.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compare the models that are most similar to DNNs. These models should
    be able to learn a better representation of data when a large dataset is available.
    We report the test error provided by the authors for MNIST and CIFAR-10 dataset
    in Table [2](#S7.T2 "Table 2 ‣ 7 Experiments ‣ A Survey of Techniques All Classifiers
    Can Learn from Deep Networks: Models, Optimizations, and Regularization"). If
    the performance of a model was not available for any of these datasets, we ran
    that experiment with the authors’ code. Default parameters are employed for parameter
    values that are not specified in the original papers. In the event that the authors
    did not provide their code, we did not report any results. These omissions prevent
    the report of erroneous performances that result from implementation differences.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset has been a popular testbed dataset for comparing model choices
    within the computer vision and deep network communities. MNIST instances contain
    $28\times 28$ pixel grayscale images of handwritten digits and their labels. The
    MNIST labels are drawn from 10 object classes, with a total of 6000 training samples
    and 1000 testing samples. The CIFAR-10 is also a well-known dataset containing
    10 object classes with approximately 5000 examples per class, where each sample
    is a $32\times 32$ pixel RGB image.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Classification error rate ($\%$) comparison.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset/Models | RF | ANT$\dagger$ [[129](#bib.bib129)] | DF [[93](#bib.bib93)]
    | R2SVM [[101](#bib.bib101)] | SFDT [[128](#bib.bib128)] | DNDF [[127](#bib.bib127)]
    | CondCNN [[145](#bib.bib145)] | DBT [[148](#bib.bib148)] | DNN+SVM [[150](#bib.bib150)]
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| MNIST | 3.21 | 0.29 | 0.74 | 4.03^∗ | 5.55 | 0.7 | - | 1.85 | 0.87 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 | 50.17 | 7.71 | 31.0 | 20.3 | - | - | 10.12 | 13.06 | 11.9 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: •
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* Based on the authors’ code.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\dagger$ The reported result reflects an ANT ensemble.
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the next set of experiments, we compare models designed to handle small
    or structured datasets, as shown in Table [3](#S7.T3 "Table 3 ‣ 7 Experiments
    ‣ A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models,
    Optimizations, and Regularization"). The authors of these methods tested a wide
    range of datasets to evaluate their their approach. In this survey, we conducted
    a series of experiments on the UCI human activity recognition (HAR) dataset [[165](#bib.bib165)].
    Here, human activity recognition data were collected from 30 participants performing
    six scripted activities (walking, walking upstairs, walking downstairs, sitting,
    standing, and laying) while wearing smartphones. The dataset contains 561 features
    extracted from sensors including an accelerometer and a gyroscope. The training
    set contains 7352 samples from 70% of the volunteers and the testing set contains
    2947 samples from the remaining 30% of volunteers.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Classification error rate ($\%$) comparison.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset/Models | RF | SVM | MLP | DART [[139](#bib.bib139)] | RRF [[135](#bib.bib135)]
    | GRRF [[136](#bib.bib136)] | mGBDT [[131](#bib.bib131)] |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| HAR | 6.96 | 4.69 | 4.69 | 6.55 | 3.77 | 3.74 | 7.68 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: From the table, we observe that models representing multiple layers of machine
    learning models such as DF, R2SVM, and mGBDT did not perform well on the MNIST
    and CIFAR-10 datasets. Compared to DNNs, these models are computationally more
    expensive, require an excessive amount of resources, and do not offer advantages
    of interpretability.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Another category of models utilizes a neural network to learn a better representation
    of data. These models such as DNDF, and DNN+SVM applied a more traditional machine
    learning model on the newly-learned representation. This technique could be beneficial
    when the neural network has been trained on a large dataset. For example, DNDF
    utilized GoogLeNet for extracting features, and subsequently achieved a $6.38\%$
    error rate on the ImageNet testset. In contrast, the GoogLeNet error rate is $10.02\%$.
    Another class of models enhances the tree model by integrating artificial neurons
    such as ANT, SFDT, and DBT. These models cleverly combine neural networks with
    decision trees that improve interpretation while offering representation-learning
    benefits.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Another hybrid strategy focused on decreasing the computational complexity of
    the DNNs. CondCNN is such a neural network that employs a routing mechanism similar
    to a decision tree to achieve this goal. Another successful line of research is
    to add regularizers frequently used by the neural network to other classifiers
    similar to DART, RRF, and GRRF.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The results from our experiments reveal that both network classifiers and non-network
    classifiers benefit from deep learning. The methods surveyed in this paper and
    evaluated in these experiments demonstrate that non-network machine learning models
    do improve performance by incorporating DNN components into their algorithms.
    Whereas models without feature learning such as RF usually do not perform well
    on unstructured data such as images, we observe that adding deep learning to these
    models drastically improve their performance, as shown in Table [2](#S7.T2 "Table
    2 ‣ 7 Experiments ‣ A Survey of Techniques All Classifiers Can Learn from Deep
    Networks: Models, Optimizations, and Regularization"). Additionally, non-deep
    models may achieve improved performance on structured data by adding regularizers,
    as shown in Table [3](#S7.T3 "Table 3 ‣ 7 Experiments ‣ A Survey of Techniques
    All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization").
    The methods surveyed in this paper demonstrate that deep learning components can
    be added to any type of machine learning model, and are not specific to DNNs.
    The incorporation of deep learning strategies is a promising direction for all
    types of classifiers, both network and non-network methods.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions and Directions for Ongoing Research
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DNNs have emerged as a powerful force in the machine learning field for the
    past few years. This survey paper reviews the latest attempts to incorporate methods
    that are traditionally found in DNNs into other learning algorithms. DNNs work
    well when there is a large body of training data and available computational power.
    DNNs have consistently yielded strong results for a variety of datasets and competitions,
    such as winning the Large Scale Visual Recognition Challenge [[166](#bib.bib166)]
    and achieving strong results for energy demand prediction [[167](#bib.bib167)],
    identifying gender of a text author [[168](#bib.bib168)], stroke prediction [[169](#bib.bib169)],
    network intrusion detection [[170](#bib.bib170)], speech emotion recognition [[171](#bib.bib171)],
    and taxi destination prediction [[172](#bib.bib172)]. Since there are many applications
    which lack large amounts of training data or for which the interpretability of
    a learned model is important, there is a need to integrate the benefits of DNNs
    with other classifier algorithms. Other classifiers have demonstrated improved
    performance on some types of data, therefore the field can benefit from examining
    ways of combining deep learning elements between network and non-network methods.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Although some work to date provides evidence that DNN techniques can be used
    effectively by other classifiers, there are still many challenges that researchers
    need to address, both to improve DNNs and to extend deep learning to other types
    of classifiers. Based on our survey of existing work, some related areas where
    supervised learners can benefit from unique DNN methods are outlined below.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The most characteristic feature of DNNs is a deep architecture and its ability
    to learn a new representation of data. A variety of stacked generalization methods
    have been developed to allow other machine learning methods to utilize deep architectures
    as well. These methods incorporate multiple classification steps in which the
    input of the next layer represents the concatenation of the output of the previous
    layer and the original feature vector as discussed in Section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Feedforward Learning ‣ 3.1 Supervised Learning ‣ 3 Deep Learning Architectures
    Outside of Deep Neural Networks ‣ A Survey of Techniques All Classifiers Can Learn
    from Deep Networks: Models, Optimizations, and Regularization"). Future work can
    explore the many other possibilities that exist for refining the input features
    to each layer to better separate instances of each class at each layer.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies provide evidence that DNNs are effective data generators [[173](#bib.bib173)]
    [[174](#bib.bib174)], while in some cases non-network classifiers may actually
    be the better discriminators. Future research can consider using a DNN as a generator
    and an alternative classifier as a discriminator in generative adversarial models.
    Incorporating this type of model diversity could improve the robustness of the
    models.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent can be applied to any differentiable algorithm. We observed
    that Kontschieder et al. [[127](#bib.bib127)], Frosst et al. [[128](#bib.bib128)],
    Tanno et al. [[129](#bib.bib129)], and Zoran et al. [[148](#bib.bib148)] all applied
    gradient descent to two different tree-based algorithms by making them differentiable.
    In the future, additional classifiers can be altered to be differentiable. Applying
    gradient descent to other algorithms could be an effective way to adjust the probability
    distribution of parameters.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Another area which is vital to investigate is the application of network-customized
    regularization methods unique to non-network classifiers. As discussed in Section
    [5](#S5 "5 Deep Learning Regularization Outside of Deep Neural Networks ‣ A Survey
    of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations,
    and Regularization"), the non-network classifiers can benefit from the regularization
    methods that are unique to DNNs. However, there exist many different ways that
    these regularization methods can be adapted by non-network classifiers to improve
    model generalization.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: An important area of research is interpretable models. There exist applications
    such as credit score, insurance risk, health status because of their sensitivity,
    models need to be interpretable. Further research needs to exploit the use of
    DNNs in interpretable models such as DNDT [[130](#bib.bib130)].
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in this survey, an emerging area of research is to combine the
    complementary benefits of statistical models with neural networks. Statistical
    models offer mathematical formalisms as well as possible explanatory power. This
    combination may provide a more effective model than either approach used in isolation.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: There are cases in which the amount of ground truth-labeled data is limited,
    but a large body of labeled data from the same or similar distribution is available.
    One possible area of ongoing exploration is to couple the use of DNNs for learning
    from unlabeled data with the use of other classifier strategies for learning from
    labeled data. The simple model learned from labeled data can be exploited to further
    tune and improve learned representation patterns in the DNN.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: We observe that currently, there is a general interest among the machine learning
    community to transfer new deep network developments to other classifiers. While
    a substantial effort has been made to incorporate deep learning ideas into the
    general machine learning field, continuing this work may spark the creation of
    new learning paradigms. However, the benefit between network-based learners and
    non-network learners can be bi-directional. Because a tremendous variety of classifiers
    has shown superior performance for a wide range of applications, future research
    can focus not only on how DNN techniques can improve non-network classifiers but
    on how DNNs can incorporate and benefit from non-network learning ideas as well.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The list of abbreviations and their descriptions utilized in this
    survey.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '| Abbreviation | Description |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| AE | Autoencoder |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| ANT | Adaptive Neural Tree |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| CNN | Convolutional Neural Network |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| CondNN | Conditional Neural Network |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| DART | Dropout Multiple Additive Regression Trees |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| DBT | Differentiable Boundary Tree |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| DBN | Deep Belief Network |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| DCCA | Deep Canonical Correlation Analysis |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| Deep PCA | Deep principal components analysis |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| DF | Deep Forest |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| DGP | Deep Gaussian Processes |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| DKF | Deep Kalman Filters |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| DNDT | Deep Network Decision Tree |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| DNDF | Deep Network Decision Forest |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| DNN | Deep Neural Network |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| DSVM | Deep SVM |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| DTA-LS-SVM | Deep Transfer Additive Kernel Least Square SVM |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| eForest | Encoder Forest |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| FC | Fully Connected |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| GAF | Generative Adversarial Forest |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| GAN | Generative Adversarial Network |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| GRRF | Guided Regularized Random Forest |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| LMM | Level-wise Mixture Model |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| mGBDT | Multilayer Gradient Decision Tree |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| ML-SVM | Multilayer SVM |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| MLP | Multilayer perceptron |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| NLP-SVM | Newton Linear Programming SVM |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| R2-SVM | Random Recursive SVM |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| RBM | Restricted Boltzmann Machine |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| RNN | Recurrent Neural Network |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| RRF | Regularized Random Forest |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| SCAD-SVM | Smoothly Clipped Absolute Deviation SVM |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| SDF | Siamese Deep Forest |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| SNN | Siamese Neural Network |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| FSDT | Frosst Soft Decision Tree |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| VAE | Variational Autoencoder |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: Acknowledgment
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank Tharindu Adikari, Chris Choy, Ji Feng, Yani
    Ioannou, Stanislaw Jastrzebski and Marco A. Wiering for their valuable assistance
    in providing code and additional implementation details of the algorithms that
    were evaluated in this paper. We would also like to thank Samaneh Aminikhanghahi
    and Tinghui Wang for their feedback and guidance on the methods described in this
    survey. This material is based upon work supported by the National Science Foundation
    under Grant No. 1543656.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Cade Metz. AI is transforming google search. the rest of the web is next,
    2016.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Nathan Sikes. Deep learning and the future of search engine optimization,
    2015.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Alex Davies. The numbers don’t lie: Self-driving cars are getting good,
    2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Neal Boudette. Tesla’s self-driving system cleared in deadly crash, 2017.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Autonomous vehicle disengagement reports 2017, 2017.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Fei Jiang, Yong Jiang, Hui Zhi, Yi Dong, Hao Li, Sufeng Ma, Yilong Wang,
    Qiang Dong, Haipeng Shen, and Yongjun Wang. Artificial intelligence in healthcare:
    past, present and future. Stroke and vascular neurology, 2(4):230–243, 2017.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Liang-Chieh Chen and Yukun Zhu. Semantic image segmentation with deeplab
    in tensorflow, 2018.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization
    of neural networks using dropconnect. In International Conference on Machine Learning,
    pages 1058–1066, 2013.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Benjamin Graham. Fractional max-pooling. arXiv preprint arXiv:1412.6071,
    2014.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and
    accurate deep network learning by exponential linear units (elus). arXiv preprint
    arXiv:1511.07289, 2015.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt,
    Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.
    In Advances in Neural Information Processing Systems, pages 1693–1701, 2015.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,
    and Chris Dyer. Neural architectures for named entity recognition. arXiv preprint
    arXiv:1603.01360, 2016.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and
    Yoshua Bengio. End-to-end attention-based large vocabulary speech recognition.
    In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference
    on, pages 4945–4949\. IEEE, 2016.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai,
    Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang
    Chen, et al. Deep speech 2: End-to-end speech recognition in english and mandarin.
    In International Conference on Machine Learning, pages 173–182, 2016.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436, 2015.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Thuy Ong. Amazon’s new algorithm designs clothing by analyzing a bunch
    of pictures, 2017.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Chenghui Tang, Yishen Wang, Jian Xu, Yuanzhang Sun, and Baosen Zhang.
    Efficient scenario generation of multiple renewable power plants considering spatial
    and temporal correlations. Applied Energy, 221:348–357, 2018.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie
    Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital
    30-day readmission. In Proceedings of the 21th ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining, pages 1721–1730\. ACM, 2015.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily
    fooled: High confidence predictions for unrecognizable images. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427–436,
    2015.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
    Understanding deep learning requires rethinking generalization. arXiv preprint
    arXiv:1611.03530, 2016.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] David Krueger, Nicolas Ballas, Stanislaw Jastrzebski, Devansh Arpit, Maxinder S
    Kanwal, Tegan Maharaj, Emmanuel Bengio, Asja Fischer, and Aaron Courville. Deep
    nets don’t learn via memorization. 2017.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint
    arXiv:1511.06422, 2015.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Siddharth Krishna Kumar. On weight initialization in deep neural networks.
    arXiv preprint arXiv:1704.08863, 2017.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and
    trends® in Machine Learning, 2(1):1–127, 2009.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1–9, 2015.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,
    Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s
    neural machine translation system: Bridging the gap between human and machine
    translation. arXiv preprint arXiv:1609.08144, 2016.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui
    Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410,
    2016.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition
    with deep recurrent neural networks. In 2013 IEEE international conference on
    acoustics, speech and signal processing, pages 6645–6649\. IEEE, 2013.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks
    for human action recognition. IEEE transactions on pattern analysis and machine
    intelligence, 35(1):221–231, 2013.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] David H Wolpert, William G Macready, et al. No free lunch theorems for
    optimization. IEEE transactions on evolutionary computation, 1(1):67–82, 1997.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Ross D. King, Cao Feng, and Alistair Sutherland. Statlog: comparison of
    classification algorithms on large real-world problems. Applied Artificial Intelligence
    an International Journal, 9(3):289–333, 1995.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Tjen-Sien Lim, Wei-Yin Loh, and Yu-Shan Shih. A comparison of prediction
    accuracy, complexity, and training time of thirty-three old and new classification
    algorithms. Machine learning, 40(3):203–228, 2000.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of
    supervised learning algorithms. In Proceedings of the 23rd international conference
    on Machine learning, pages 161–168\. ACM, 2006.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Rich Caruana, Nikos Karampatziakis, and Ainur Yessenalina. An empirical
    evaluation of supervised learning in high dimensions. In Proceedings of the 25th
    international conference on Machine learning, pages 96–103\. ACM, 2008.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Philipp Baumann, DS Hochbaum, and YT Yang. A comparative study of the
    leading machine learning techniques and two new optimization algorithms. European
    journal of operational research, 272(3):1041–1057, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa
    Reyes, Mei-Ling Shyu, Shu-Ching Chen, and SS Iyengar. A survey on deep learning:
    Algorithms, techniques, and applications. ACM Computing Surveys (CSUR), 51(5):92,
    2018.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Benjamin Shickel, Patrick James Tighe, Azra Bihorac, and Parisa Rashidi.
    Deep ehr: A survey of recent advances in deep learning techniques for electronic
    health record (ehr) analysis. IEEE journal of biomedical and health informatics,
    22(5):1589–1604, 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Junwei Han, Dingwen Zhang, Gong Cheng, Nian Liu, and Dong Xu. Advanced
    deep-learning techniques for salient and category-specific object detection: a
    survey. IEEE Signal Processing Magazine, 35(1):84–100, 2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, and Lisha Hu. Deep
    learning for sensor-based activity recognition: A survey. Pattern Recognition
    Letters, 119:3–11, 2019.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] William Grant Hatcher and Wei Yu. A survey of deep learning: platforms,
    applications and emerging research trends. IEEE Access, 6:24411–24432, 2018.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Frank Rosenblatt. The perceptron: a probabilistic model for information
    storage and organization in the brain. Psychological review, 65(6):386, 1958.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. Technical
    report, Stanford Univ Ca Stanford Electronics Labs, 1960.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Marvin Minsky and Seymour A Papert. Perceptrons: An introduction to computational
    geometry. MIT press, 1969.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning
    internal representations by error propagation. Technical report, California Univ
    San Diego La Jolla Inst for Cognitive Science, 1985.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm
    for deep belief nets. Neural computation, 18(7):1527–1554, 2006.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv
    preprint arXiv:1410.5401, 2014.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between
    capsules. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett, editors, Advances in Neural Information Processing Systems 30,
    pages 3856–3866\. Curran Associates, Inc., 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep
    learning, volume 1. MIT press Cambridge, 2016.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Yann LeCun et al. Generalization and network design strategies. Connectionism
    in perspective, pages 143–155, 1989.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou,
    and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems
    25, pages 1097–1105\. Curran Associates, Inc., 2012.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Yi-Tong Zhou and Rama Chellappa. Computation of optical flow using a neural
    network. In IEEE International Conference on Neural Networks, volume 1998, pages
    71–78, 1988.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using rnn encoder-decoder for statistical machine translation. arXiv preprint
    arXiv:1406.1078, 2014.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface:
    Closing the gap to human-level performance in face verification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 1701–1708,
    2014.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Rachid Riad, Corentin Dancette, Julien Karadayi, Neil Zeghidour, Thomas
    Schatz, and Emmanuel Dupoux. Sampling strategies in siamese networks for unsupervised
    speech representation learning. arXiv preprint arXiv:1804.11297, 2018.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Zara Alaverdyan, Julien Jung, Romain Bouet, and Carole Lartizien. Regularized
    siamese neural network for unsupervised outlier detection on brain multiparametric
    magnetic resonance imaging: application to epilepsy lesion screening. 2018.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Arthur L Samuel. Some studies in machine learning using the game of checkers.
    IBM Journal of research and development, 3(3):210–229, 1959.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,
    editors, Advances in Neural Information Processing Systems 27, pages 2672–2680\.
    Curran Associates, Inc., 2014.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele,
    and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint
    arXiv:1605.05396, 2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Artur Kadurin, Alexander Aliper, Andrey Kazennov, Polina Mamoshina, Quentin
    Vanhaelen, Kuzma Khrabrov, and Alex Zhavoronkov. The cornucopia of meaningful
    leads: Applying deep adversarial autoencoders for new molecule development in
    oncology. Oncotarget, 8(7):10883, 2017.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Wengling Chen and James Hays. Sketchygan: Towards diverse and realistic
    sketch to image synthesis. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 9416–9425, 2018.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Gaugan: semantic
    image synthesis with spatially adaptive normalization. In ACM SIGGRAPH 2019 Real-Time
    Live!, page 2\. ACM, 2019.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality
    of data with neural networks. science, 313(5786):504–507, 2006.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
    Extracting and composing robust features with denoising autoencoders. In Proceedings
    of the 25th international conference on Machine learning, pages 1096–1103\. ACM,
    2008.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization.
    In European Conference on Computer Vision, pages 649–666. Springer, 2016.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Detian Huang, Weiqin Huang, Zhenguo Yuan, Yanming Lin, Jian Zhang, and
    Lixin Zheng. Image super-resolution algorithm based on an improved sparse autoencoder.
    Information, 9(1):11, 2018.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko, and Yu-Chiang Frank Wang. Learning
    deep latent space for multi-label classification. In AAAI, pages 2838–2844, 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv
    preprint arXiv:1312.6114, 2013.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Yoshua Bengio. Deep learning of representations: Looking forward. In International
    Conference on Statistical Language and Speech Processing, pages 1–37\. Springer,
    2013.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the
    importance of initialization and momentum in deep learning. In International conference
    on machine learning, pages 1139–1147, 2013.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods
    for online learning and stochastic optimization. Journal of Machine Learning Research,
    12(Jul):2121–2159, 2011.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization:
    a review of algorithms and comparison of software implementations. Journal of
    Global Optimization, 56(3):1247–1293, 2013.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Arild Nøkland. Direct feedback alignment provides learning in deep neural
    networks. In Advances in neural information processing systems, pages 1037–1045,
    2016.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal
    of the Royal Statistical Society. Series B (Methodological), pages 267–288, 1996.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Yoav Freund. Boosting a weak learning algorithm by majority. Information
    and computation, 121(2):256–285, 1995.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Mohammad Moghimi, Serge J Belongie, Mohammad J Saberian, Jian Yang, Nuno
    Vasconcelos, and Li-Jia Li. Boosted convolutional neural networks. In BMVC, 2016.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Jesse Eickholt and Jianlin Cheng. Dndisorder: predicting protein disorder
    using boosting and deep networks. BMC bioinformatics, 14(1):88, 2013.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and
    Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting.
    The Journal of Machine Learning Research, 15(1):1929–1958, 2014.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Luis Perez and Jason Wang. The effectiveness of data augmentation in image
    classification using deep learning. arXiv preprint arXiv:1712.04621, 2017.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V
    Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501,
    2018.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Navdeep Jaitly and Geoffrey E Hinton. Vocal tract length perturbation
    (vtlp) improves speech recognition. In Proc. ICML Workshop on Deep Learning for
    Audio, Speech and Language, volume 117, 2013.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Hiroki OHASHI, Mohammad AL-NASER, Sheraz AHMED, Takayuki AKIYAMA, Takuto
    SATO, Phong NGUYEN, Katsuyuki NAKAMURA, and Andreas DENGEL. Augmenting wearable
    sensor data with physical constraint for dnn-based human-action recognition. 2017.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger
    Gunn, Alexander Hammers, David Alexander Dickie, Maria Valdés Hernández, Joanna
    Wardlaw, and Daniel Rueckert. Gan augmentation: Augmenting training data using
    generative adversarial networks. arXiv preprint arXiv:1810.10863, 2018.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation
    generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Javier Jorge, Jesús Vieco, Roberto Paredes, Joan-Andreu Sánchez, and José-Miguel
    Benedí. Empirical evaluation of variational autoencoders for data augmentation.
    In VISIGRAPP (5: VISAPP), pages 96–104, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun
    Wang, Site Li, Ping Jia, and Jane You. Data augmentation via latent space interpolation
    for image classification. In 2018 24th International Conference on Pattern Recognition
    (ICPR), pages 728–733\. IEEE, 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Yichuan Tang and Chris Eliasmith. Deep networks for robust visual recognition.
    In Proceedings of the 27th International Conference on Machine Learning (ICML-10),
    pages 1055–1062\. Citeseer, 2010.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli. Analyzing noise in
    autoencoders and deep networks. arXiv preprint arXiv:1406.1831, 2014.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] David H Wolpert. Stacked generalization. Neural networks, 5(2):241–259,
    1992.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Kai Ming Ting and Ian H Witten. Issues in stacked generalization. Journal
    of artificial intelligence research, 10:271–289, 1999.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep
    neural networks. arXiv preprint arXiv:1702.08835, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Guanjin Wang, Guangquan Zhang, Kup-Sze Choi, and Jie Lu. Deep additive
    least squares support vector machines for classification with model transfer.
    IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Leo Breiman. Randomizing outputs to increase prediction accuracy. Machine
    Learning, 40(3):229–242, 2000.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Tin Kam Ho. Random decision forests. In Document analysis and recognition,
    1995., proceedings of the third international conference on, volume 1, pages 278–282\.
    IEEE, 1995.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
    com/exdb/mnist/, 1998.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Gavin C Cawley. Leave-one-out cross-validation based model selection criteria
    for weighted ls-svms. In Neural Networks, 2006\. IJCNN’06\. International Joint
    Conference on, pages 1661–1668\. IEEE, 2006.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Hao Yang and Jianxin Wu. Practical large scale classification with additive
    kernels. In Asian Conference on Machine Learning, pages 523–538, 2012.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Azizi Abdullah, Remco C Veltkamp, and Marco A Wiering. An ensemble of
    deep support vector machines for image categorization. In Soft Computing and Pattern
    Recognition, 2009\. SOCPAR’09. International Conference of, pages 301–306\. IEEE,
    2009.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Oriol Vinyals, Yangqing Jia, Li Deng, and Trevor Darrell. Learning with
    recursive perceptual representations. In Advances in Neural Information Processing
    Systems, pages 2825–2833, 2012.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Li Deng, Dong Yu, and John Platt. Scalable stacking and learning for
    building deep architectures. In Acoustics, Speech and Signal Processing (ICASSP),
    2012 IEEE International Conference on, pages 2133–2136\. IEEE, 2012.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Brian Hutchinson, Li Deng, and Dong Yu. Tensor deep stacking networks.
    IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1944–1957,
    2013.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Li Deng and Dong Yu. Deep convex net: A scalable architecture for speech
    pattern classification. In Twelfth Annual Conference of the International Speech
    Communication Association, 2011.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Venice Erin Liong, Jiwen Lu, and Gang Wang. Face recognition using deep
    pca. In Information, Communications and Signal Processing (ICICS) 2013 9th International
    Conference on, pages 1–5\. IEEE, 2013.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features
    from tiny images. Technical report, Citeseer, 2009.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Jonathon Shlens. A tutorial on principal component analysis. arXiv preprint
    arXiv:1404.1100, 2014.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Andreas Damianou and Neil Lawrence. Deep gaussian processes. In Artificial
    Intelligence and Statistics, pages 207–215, 2013.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Ilya Sutskever and Geoffrey Hinton. Learning multilevel distributed representations
    for high-dimensional sequences. In Artificial intelligence and statistics, pages
    548–555, 2007.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Roland Memisevic and Geoffrey Hinton. Unsupervised learning of image
    transformations. In 2007 IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1–8\. IEEE, 2007.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Kevin Swersky, Ilya Sutskever, Daniel Tarlow, Richard S Zemel, Ruslan R
    Salakhutdinov, and Ryan P Adams. Cardinality restricted boltzmann machines. In
    Advances in neural information processing systems, pages 3293–3301, 2012.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer
    School on Machine Learning, pages 63–71\. Springer, 2003.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] César Lincoln C Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth,
    Guilherme A Barreto, and Neil D Lawrence. Recurrent gaussian processes. arXiv
    preprint arXiv:1511.06644, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Mark Van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional
    gaussian processes. In Advances in Neural Information Processing Systems, pages
    2849–2858, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Zhenwen Dai, Andreas Damianou, Javier González, and Neil Lawrence. Variational
    auto-encoded deep gaussian processes. arXiv preprint arXiv:1511.06455, 2015.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Andreas Damianou. Deep Gaussian processes and variational propagation
    of uncertainty. PhD thesis, University of Sheffield, 2015.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Matthew M Dunlop, Mark A Girolami, Andrew M Stuart, and Aretha L Teckentrup.
    How deep are deep gaussian processes? The Journal of Machine Learning Research,
    19(1):2100–2145, 2018.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding
    pathologies in very deep networks. In Artificial Intelligence and Statistics,
    pages 202–210, 2014.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Lev V Utkin and Mikhail A Ryabinin. A siamese deep forest. arXiv preprint
    arXiv:1704.08715, 2017.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Yan Zuo, Gil Avraham, and Tom Drummond. Generative adversarial forests
    for better conditioned adversarial learning. arXiv preprint arXiv:1805.05185,
    2018.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Rudolph Emil Kalman. A new approach to linear filtering and prediction
    problems. Journal of basic Engineering, 82(1):35–45, 1960.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Rahul G. Krishnan, Uri Shalit, and David Sontag. Deep kalman filters.
    arXiv preprint arXiv:1511.05121, 2015.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Shirli Di-Castro Shashua and Shie Mannor. Deep robust kalman filter.
    arXiv preprint arXiv:1703.02310, 2017.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Zhiyong Gao, and Ming-Ting
    Sun. Deep kalman filtering network for video compression artifact reduction. In
    Proceedings of the European Conference on Computer Vision (ECCV), pages 568–584,
    2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Ji Feng and Zhi-Hua Zhou. Autoencoder by forest. arXiv preprint arXiv:1709.09018,
    2017.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Marco A Wiering and Lambert RB Schomaker. Multi-layer support vector
    machines. Regularization, optimization, kernels, and support vector machines,
    page 457, 2014.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota
    Bulo. Deep neural decision forests. In Computer Vision (ICCV), 2015 IEEE International
    Conference on, pages 1467–1475\. IEEE, 2015.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into
    a soft decision tree. arXiv preprint arXiv:1711.09784, 2017.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Ryutaro Tanno, Kai Arulkumaran, Daniel C Alexander, Antonio Criminisi,
    and Aditya Nori. Adaptive neural trees. arXiv preprint arXiv:1807.06699, 2018.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Yongxin Yang, Irene Garcia Morillo, and Timothy M Hospedales. Deep neural
    decision trees. arXiv preprint arXiv:1806.06988, 2018.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Ji Feng, Yang Yu, and Zhi-Hua Zhou. Multi-layered gradient boosting decision
    trees. arXiv preprint arXiv:1806.00007, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Glenn M Fung and Olvi L Mangasarian. A feature selection newton method
    for support vector machine classification. Computational optimization and applications,
    28(2):185–202, 2004.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Hao Helen Zhang, Jeongyoun Ahn, Xiaodong Lin, and Cheolwoo Park. Gene
    selection using support vector machines with non-convex penalty. bioinformatics,
    22(1):88–95, 2005.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized
    likelihood and its oracle properties. Journal of the American statistical Association,
    96(456):1348–1360, 2001.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Houtao Deng and George Runger. Feature selection via regularized trees.
    In Neural Networks (IJCNN), The 2012 International Joint Conference on, pages
    1–8\. IEEE, 2012.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Houtao Deng and George Runger. Gene selection with guided regularized
    random forest. Pattern Recognition, 46(12):3483–3489, 2013.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good
    sentiment and topic classification. In Proceedings of the 50th Annual Meeting
    of the Association for Computational Linguistics: Short Papers-Volume 2, pages
    90–94\. Association for Computational Linguistics, 2012.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Sida Wang and Christopher Manning. Fast dropout training. In international
    conference on machine learning, pages 118–126, 2013.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Rashmi Korlakai Vinayak and Ran Gilad-Bachrach. Dart: Dropouts meet multiple
    additive regression trees. In Artificial Intelligence and Statistics, pages 489–497,
    2015.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Jerome H Friedman. Greedy function approximation: a gradient boosting
    machine. Annals of statistics, pages 1189–1232, 2001.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Jerome H Friedman. Stochastic gradient boosting. Computational Statistics
    & Data Analysis, 38(4):367–378, 2002.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego,
    and Salvatore Trani. X-dart: Blending dropout and pruning for efficient learning
    to rank. In Proceedings of the 40th International ACM SIGIR Conference on Research
    and Development in Information Retrieval, pages 1077–1080\. ACM, 2017.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Sebastien C Wong, Adam Gatt, Victor Stamatescu, and Mark D McDonnell.
    Understanding data augmentation for classification: when to warp? arXiv preprint
    arXiv:1609.08764, 2016.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Ruo Xu. Improvements to random forest methodology. 2013.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie
    Shotton, Matthew Brown, and Antonio Criminisi. Decision forests, convolutional
    networks and the models in-between. arXiv preprint arXiv:1603.01250, 2016.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
    Weight uncertainty in neural networks. arXiv preprint arXiv:1505.05424, 2015.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Meire Fortunato, Charles Blundell, and Oriol Vinyals. Bayesian recurrent
    neural networks. CoRR, abs/1704.02798, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Daniel Zoran, Balaji Lakshminarayanan, and Charles Blundell. Learning
    deep nearest neighbor representations using differentiable boundary trees. arXiv
    preprint arXiv:1702.08833, 2017.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Charles Mathy, Nate Derbinsky, José Bento, Jonathan Rosenthal, and Jonathan S
    Yedidia. The boundary forest algorithm for online supervised and unsupervised
    learning. In AAAI, pages 2864–2870, 2015.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Yichuan Tang. Deep learning using linear support vector machines. arXiv
    preprint arXiv:1306.0239, 2013.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian
    institute for advanced research).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Xiao-Xiao Niu and Ching Y Suen. A novel hybrid cnn–svm classifier for
    recognizing handwritten digits. Pattern Recognition, 45(4):1318–1325, 2012.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Masoumeh Zareapoor, Pourya Shamsolmoali, Deepak Kumar Jain, Haoxiang
    Wang, and Jie Yang. Kernelized support vector machine with deep learning: an efficient
    approach for extreme multiclass dataset. Pattern Recognition Letters, 2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Jawad Nagi, Gianni A Di Caro, Alessandro Giusti, Farrukh Nagi, Luca Maria
    Gambardella, et al. Convolutional neural support vector machines: Hybrid visual
    pattern classifiers for multi-robot systems. In ICMLA (1), pages 27–32, 2012.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Abdel Bellili, Michel Gilloux, and Patrick Gallinari. An hybrid mlp-svm
    handwritten digit recognizer. In Document Analysis and Recognition, 2001\. Proceedings.
    Sixth International Conference on, pages 28–32\. IEEE, 2001.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Washington W Azevedo and Cleber Zanchet. A mlp-svm hybrid model for cursive
    handwriting recognition. In Neural Networks (IJCNN), The 2011 International Joint
    Conference on, pages 843–850\. IEEE, 2011.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Tianyi Zhao, Baopeng Zhang, Ming He, Wei Zhanga, Ning Zhou, Jun Yu, and
    Jianping Fan. Embedding visual hierarchy with deep networks for large-scale visual
    recognition. IEEE Transactions on Image Processing, 2018.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Harold Hotelling. Relations between two sets of variates. In Breakthroughs
    in statistics, pages 162–190\. Springer, 1992.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] David R Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical correlation
    analysis: An overview with application to learning methods. Neural computation,
    16(12):2639–2664, 2004.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical
    correlation analysis. In International conference on machine learning, pages 1247–1255,
    2013.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic
    representations from tree-structured long short-term memory networks. arXiv preprint
    arXiv:1503.00075, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] David Alvarez-Melis and Tommi S Jaakkola. Tree-structured decoding with
    doubly-recurrent neural networks. 2016.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Andrea Cimino and Felice Dell’Orletta. Tandem lstm-svm approach for sentiment
    analysis. In of the Final Workshop 7 December 2016, Naples, page 172, 2016.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Abien Fred M Agarap. A neural network architecture combining gated recurrent
    unit (gru) and support vector machine (svm) for intrusion detection in network
    traffic data. In Proceedings of the 2018 10th International Conference on Machine
    Learning and Computing, pages 26–30\. ACM, 2018.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis
    Reyes-Ortiz. A public domain dataset for human activity recognition using smartphones.
    In ESANN, 2013.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Large scale visual recognition challenge 2017 (ilsvrc2017), 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Nikolaos G Paterakis, Elena Mocanu, Madeleine Gibescu, Bart Stappers,
    and Walter van Alst. Deep learning versus traditional machine learning methods
    for aggregated energy demand prediction. In Innovative Smart Grid Technologies
    Conference Europe (ISGT-Europe), 2017 IEEE PES, pages 1–6\. IEEE, 2017.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Alexander Sboev, Ivan Moloshnikov, Dmitry Gudovskikh, Anton Selivanov,
    Roman Rybka, and Tatiana Litvinova. Deep learning neural nets versus traditional
    machine learning in gender identification of authors of rusprofiling texts. Procedia
    Computer Science, 123:424–431, 2018.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Chen-Ying Hung, Wei-Chen Chen, Po-Tsun Lai, Ching-Heng Lin, and Chi-Chun
    Lee. Comparing deep neural network and other machine learning algorithms for stroke
    prediction in a large-scale population-based electronic medical claims database.
    In Engineering in Medicine and Biology Society (EMBC), 2017 39th Annual International
    Conference of the IEEE, pages 3110–3113\. IEEE, 2017.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Chuanlong Yin, Yuefei Zhu, Jinlong Fei, and Xinzheng He. A deep learning
    approach for intrusion detection using recurrent neural networks. IEEE Access,
    5:21954–21961, 2017.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Haytham M Fayek, Margaret Lech, and Lawrence Cavedon. Evaluating deep
    learning architectures for speech emotion recognition. Neural Networks, 92:60–68,
    2017.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Alexandre De Brébisson, Étienne Simon, Alex Auvolat, Pascal Vincent,
    and Yoshua Bengio. Artificial neural networks applied to taxi destination prediction.
    arXiv preprint arXiv:1508.00021, 2015.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation
    learning with deep convolutional generative adversarial networks. arXiv preprint
    arXiv:1511.06434, 2015.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate
    Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial
    domain adaptation. arXiv preprint arXiv:1711.03213, 2017.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
