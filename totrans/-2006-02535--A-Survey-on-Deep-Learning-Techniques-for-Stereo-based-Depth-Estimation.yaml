- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:00:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2006.02535] A Survey on Deep Learning Techniques for Stereo-based Depth Estimation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2006.02535] 基于立体的深度估计技术综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2006.02535](https://ar5iv.labs.arxiv.org/html/2006.02535)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2006.02535](https://ar5iv.labs.arxiv.org/html/2006.02535)
- en: A Survey on Deep Learning Techniques
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于立体的深度估计技术综述
- en: for Stereo-based Depth Estimation
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于立体的深度估计
- en: 'Hamid Laga, Laurent Valentin Jospin, Farid Boussaid    Mohammed Bennamoun Hamid
    Laga is with the Information Technology, Mathematics and Statistics Discipline,
    Murdoch University (Australia), and with the Phenomics and Bioinformatics Research
    Centre, University of South Australia. Email: H.Laga@murdoch.edu.au Laurent Valentin
    Jospin is with the University of Western Australia, Perth, WA 6009, Australia.
    Email: laurent.jospin@research.uwa.edu.au Farid Boussaid is with the University
    of Western Australia, Perth, WA 6009, Australia. Email: farid.boussaid@uwa.edu.au
    Mohammed Bennamoun is with the University of Western Australia, Perth, WA 6009,
    Australia. Email: mohammed.bennamoun@uwa.edu.au Manuscript received June, 2020;
    revised June, 2020.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 哈米德·拉加、劳伦特·瓦伦丁·朱斯潘、法里德·布萨伊德    穆罕默德·本纳蒙 哈米德·拉加就职于澳大利亚默多克大学信息技术、数学与统计学学科，同时也在南澳大利亚大学的表型学与生物信息学研究中心工作。电子邮箱：H.Laga@murdoch.edu.au
    劳伦特·瓦伦丁·朱斯潘就职于澳大利亚西澳大学，珀斯，WA 6009。电子邮箱：laurent.jospin@research.uwa.edu.au 法里德·布萨伊德就职于澳大利亚西澳大学，珀斯，WA
    6009。电子邮箱：farid.boussaid@uwa.edu.au 穆罕默德·本纳蒙就职于澳大利亚西澳大学，珀斯，WA 6009。电子邮箱：mohammed.bennamoun@uwa.edu.au
    手稿收到日期：2020年6月；修订日期：2020年6月。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Estimating depth from RGB images is a long-standing ill-posed problem, which
    has been explored for decades by the computer vision, graphics, and machine learning
    communities. Among the existing techniques, stereo matching remains one of the
    most widely used in the literature due to its strong connection to the human binocular
    system. Traditionally, stereo-based depth estimation has been addressed through
    matching hand-crafted features across multiple images. Despite the extensive amount
    of research, these traditional techniques still suffer in the presence of highly
    textured areas, large uniform regions, and occlusions. Motivated by their growing
    success in solving various 2D and 3D vision problems, deep learning for stereo-based
    depth estimation has attracted a growing interest from the community, with more
    than 150 papers published in this area between 2014 and 2019\. This new generation
    of methods has demonstrated a significant leap in performance, enabling applications
    such as autonomous driving and augmented reality. In this article, we provide
    a comprehensive survey of this new and continuously growing field of research,
    summarize the most commonly used pipelines, and discuss their benefits and limitations.
    In retrospect of what has been achieved so far, we also conjecture what the future
    may hold for deep learning-based stereo for depth estimation research.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从RGB图像中估计深度是一个长期存在的难题，这一问题在计算机视觉、图形学和机器学习领域已被探索了数十年。在现有技术中，由于与人类双眼系统的强关联，立体匹配仍然是文献中使用最广泛的方法之一。传统上，基于立体的深度估计是通过在多幅图像之间匹配手工制作的特征来解决的。尽管进行了大量研究，但这些传统技术在处理高纹理区域、大面积均匀区域和遮挡物时仍然存在问题。受益于其在解决各种2D和3D视觉问题上的成功，基于深度学习的立体深度估计受到越来越多的关注，在2014年至2019年间，相关领域发表了超过150篇论文。这一新一代方法在性能上取得了显著的突破，使得诸如自动驾驶和增强现实等应用成为可能。本文提供了对这一新兴且持续增长的研究领域的全面综述，总结了最常用的流程，并讨论了其优缺点。在回顾迄今为止的成就时，我们还推测了基于深度学习的立体深度估计研究的未来发展方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: CNN, Deep Learning, 3D Reconstruction, Stereo Matching, Multi-view Stereo, Disparity
    Estimation, Feature Leaning, Feature Matching.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CNN，深度学习，3D重建，立体匹配，多视角立体，视差估计，特征学习，特征匹配。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Depth estimation from one or multiple RGB images is a long standing ill-posed
    problem, with applications in various domains such as robotics, autonomous driving,
    object recognition and scene understanding, 3D modeling and animation, augmented
    reality, industrial control, and medical diagnosis. This problem has been extensively
    investigated for many decades. Among all the techniques that have been proposed
    in the literature, stereo matching is traditionally the most explored one due
    to its strong connection to the human binocular system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从单张或多张RGB图像中进行深度估计是一个长期存在的难题，应用领域包括机器人技术、自动驾驶、物体识别和场景理解、3D建模与动画、增强现实、工业控制和医学诊断。这个问题已经被广泛研究了几十年。在所有提出的技术中，立体匹配由于其与人类双眼系统的紧密联系，一直是最被探索的技术之一。
- en: The first generation of stereo-based depth estimation methods relied typically
    on matching pixels across multiple images captured using accurately calibrated
    cameras. Although these techniques can achieve good results, they are still limited
    in many aspects. For instance, they are not suitable when dealing with occlusions,
    featureless regions, or highly textured regions with repetitive patterns. Interestingly,
    we, as humans, are good at solving such ill-posed inverse problems by leveraging
    prior knowledge. For example, we can easily infer the approximate sizes of objects,
    their relative locations, and even their approximate relative distance to our
    eye(s). We can do this because all the previously seen objects and scenes have
    enabled us to build prior knowledge and develop mental models of how the 3D world
    looks like. The second generation of methods tries to leverage this prior knowledge
    by formulating the problem as a learning task. The advent of deep learning techniques
    in computer vision [[1](#bib.bib1)] coupled with the increasing availability of
    large training datasets, have led to a third generation of methods that are able
    to recover the lost dimension. Despite being recent, these methods have demonstrated
    exciting and promising results on various tasks related to computer vision and
    graphics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一代基于立体视觉的深度估计方法通常依赖于在使用准确标定的相机拍摄的多张图像中匹配像素。虽然这些技术可以取得良好的结果，但在许多方面仍然有限。例如，当处理遮挡、无特征区域或具有重复纹理的高度纹理区域时，它们并不适用。有趣的是，作为人类，我们擅长通过利用先验知识来解决此类难解的逆问题。例如，我们可以轻松推测物体的近似大小、它们的相对位置，甚至它们与我们眼睛的相对距离。我们之所以能够做到这一点，是因为所有先前看到的物体和场景使我们能够建立先验知识，并形成3D世界的心理模型。第二代方法试图通过将问题表述为学习任务来利用这些先验知识。计算机视觉中深度学习技术的出现[[1](#bib.bib1)]加上大型训练数据集的日益普及，催生了第三代方法，这些方法能够恢复丢失的维度。尽管这些方法比较新，但在与计算机视觉和图形相关的各种任务中表现出了令人兴奋和有前途的结果。
- en: In this article, we provide a comprehensive and structured review of the recent
    advances in stereo image-based depth estimation using deep learning techniques.
    These methods use two or more images captured with spatially-distributed RGB cameras¹¹1Deep
    learning-based depth estimation from monocular images and videos is an emerging
    field and requires a separate survey.. We have gathered more than $150$ papers,
    which appeared between January $2014$ and December $2019$ in leading computer
    vision, computer graphics, and machine learning conferences and journals²²2At
    the time of writing this article.. The goal is to help the reader navigate in
    this emerging field, which has gained a significant momentum in the past few years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们提供了一个全面且结构化的综述，涵盖了基于立体图像的深度估计的最新进展，这些进展使用了深度学习技术。这些方法使用了由空间分布的RGB相机拍摄的两张或更多图像¹¹深度学习基于单目图像和视频的深度估计是一个新兴领域，需要单独的综述..
    我们收集了超过**150**篇论文，这些论文发表于**2014**年1月到**2019**年12月之间的主要计算机视觉、计算机图形学和机器学习会议及期刊²²在撰写本文时..
    其目标是帮助读者在这个新兴领域中进行导航，该领域在过去几年中获得了显著的动力。
- en: The major contributions of this article are as follows;
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献如下；
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first article that surveys stereo-based
    depth estimation using deep learning techniques. We present a comprehensive review
    of more than $150$ papers, which appeared in the past six years in leading conferences
    and journals.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一篇使用深度学习技术调查基于立体视觉的深度估计的文章。我们对过去六年在主要会议和期刊上发表的超过**150**篇论文进行了全面的综述。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive taxonomy of the state-of-the-art. We first describe
    the common pipelines and then discuss the similarities and differences between
    methods within each pipeline.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了最先进技术的全面分类。我们首先描述常见的处理流程，然后讨论每个流程内方法的相似性和差异。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive review and an insightful analysis on all the aspects
    of the problem, including the training data, the network architectures and their
    effect on the reconstruction performance, the training strategies, and the generalization
    ability.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对问题所有方面的全面回顾和深刻分析，包括训练数据、网络架构及其对重建性能的影响、训练策略和泛化能力。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comparative summary of the properties and performances of some
    key methods using publicly available datasets and in-house images. The latter
    have been chosen to test how these methods would perform on completely new scenarios.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了一个比较总结，涵盖了使用公开可用数据集和内部图像的某些关键方法的属性和性能。后者被选择用于测试这些方法在全新场景中的表现。
- en: The rest of this article is organized as follows; Section [2](#S2 "2 Scope and
    taxonomy ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    formulates the problem and lays down the taxonomy. Section [3](#S3 "3 Datasets
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation") surveys
    the various datasets which have been used to train and test stereo-based depth
    reconstruction algorithms. Section [4](#S4 "4 Depth by stereo matching ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation") focuses on the
    works that use deep learning architectures to learn how to match pixels across
    images. Section [5](#S5 "5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation") reviews the end-to-end methods
    for stereo matching, while Section [6](#S6 "6 Learning multiview stereo ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation") discusses how
    these methods have been extended to the multi-view stereo case. Section [7](#S7
    "7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques for
    Stereo-based Depth Estimation") focuses on the training procedures including the
    choice of the loss functions and the degree of supervision. Section [8](#S8 "8
    Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") discusses the performance of key methods. Finally, Section [9](#S9
    "9 Future research directions ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") discusses the potential future research directions, while Section [10](#S10
    "10 Conclusion ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    summarizes the main contributions of this article.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：第[2](#S2 "2 Scope and taxonomy ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation)节 formulates the problem and lays down the taxonomy.
    第[3](#S3 "3 Datasets ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation)节回顾了用于训练和测试基于立体的深度重建算法的各种数据集。第[4](#S4 "4 Depth by stereo matching ‣
    A Survey on Deep Learning Techniques for Stereo-based Depth Estimation)节重点介绍了使用深度学习架构学习如何在图像之间匹配像素的工作。第[5](#S5
    "5 End-to-end depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation)节回顾了立体匹配的端到端方法，第[6](#S6 "6 Learning multiview stereo ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation)节讨论了这些方法如何扩展到多视角立体的情况。第[7](#S7
    "7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques for
    Stereo-based Depth Estimation)节专注于包括损失函数选择和监督程度在内的训练过程。第[8](#S8 "8 Discussion
    and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation)节讨论了关键方法的性能。最后，第[9](#S9
    "9 Future research directions ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation)节讨论了潜在的未来研究方向，第[10](#S10 "10 Conclusion ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation)节总结了本文的主要贡献。
- en: 2 Scope and taxonomy
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 范围与分类
- en: Let $\textbf{I}=\{I_{k},k=1,\dots,n\}$ be a set of $n\geq 1$ RGB images of the
    same 3D scene, captured using cameras whose intrinsic and extrinsic parameters
    can be *known* or *unknown*. The goal is to estimate one or multiple depth maps,
    which can be from the same viewpoint as the input [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)], or from a new arbitrary viewpoint [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]. This article
    focuses on deep learning methods for stereo-based depth estimation, *i.e.,* $n=2$
    in the case of stereo matching, and $n>2$ for the case of Multi-View Stereo (MVS).
    Monocular and video-based depth estimation methods are beyond the scope of this
    article and require a separate survey.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\textbf{I}=\{I_{k},k=1,\dots,n\}$ 为一组 $n\geq 1$ 的 RGB 图像，这些图像来自同一 3D 场景，使用的相机的内参和外参可以是
    *已知的* 或 *未知的*。目标是估计一个或多个深度图，这些深度图可以与输入图像的视角相同 [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]，也可以是来自新的任意视角 [[6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]。本文重点讨论基于深度学习的立体视觉深度估计方法，即在立体匹配的情况下
    $n=2$，在多视角立体（MVS）的情况下 $n>2$。单目和基于视频的深度估计方法超出了本文的范围，需要另行调查。
- en: Learning-based depth reconstruction can be summarized as the process of learning
    a predictor $f_{\theta}$ that can infer from the set of images I, a depth map
    $\hat{D}$ that is as close as possible to the unknown depth map $D$. In other
    words, we seek to find a function $f_{\theta}$ such that $\mathcal{L}(\textbf{I})=d\left(f_{\theta}(\textbf{I}),D\right)$
    is minimized. Here, $\theta$ is a set of parameters, and $d(\cdot,\cdot)$ is a
    certain measure of distance between the real depth map $D$ and the reconstructed
    depth map $f_{\theta}(\textbf{I})$. The reconstruction objective $\mathcal{L}$
    is also known as the *loss function*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的深度重建可以总结为学习一个预测器 $f_{\theta}$ 的过程，该预测器可以从一组图像 I 推断出一个尽可能接近未知深度图 $D$ 的深度图
    $\hat{D}$。换句话说，我们寻求找到一个函数 $f_{\theta}$，使得 $\mathcal{L}(\textbf{I})=d\left(f_{\theta}(\textbf{I}),D\right)$
    被最小化。这里，$\theta$ 是一组参数，$d(\cdot,\cdot)$ 是实际深度图 $D$ 和重建深度图 $f_{\theta}(\textbf{I})$
    之间的某种距离度量。重建目标 $\mathcal{L}$ 也被称为 *损失函数*。
- en: 'We can distinguish two main categories of methods. Methods in the first class
    (Section [4](#S4 "4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")) mimic the traditional stereo-matching techniques [[11](#bib.bib11)]
    by explicitly learning how to match, or put in correspondence, pixels across the
    input images. Such correspondences can then be converted into an optical flow
    or a disparity map, which in turn can be converted into depth at each pixel in
    the reference image. The predictor $f$ is composed of three modules: a feature
    extraction module, a feature matching and cost aggregation module, and a disparity/depth
    estimation module. Each module is trained independently from the others.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以区分两大类方法。第一类方法（第 [4](#S4 "4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation) 节") 模仿传统的立体匹配技术 [[11](#bib.bib11)]，通过显式学习如何匹配或对应输入图像中的像素。这些对应关系可以被转换为光流或视差图，进而被转换为参考图像中每个像素的深度。预测器
    $f$ 由三个模块组成：特征提取模块、特征匹配和成本聚合模块，以及视差/深度估计模块。每个模块都是独立训练的。
- en: The second class of methods (Section [5](#S5 "5 End-to-end depth from stereo
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")) solves
    the stereo matching problem using a pipeline that is trainable end-to-end. Two
    main classes of methods have been proposed. Early methods formulated the depth
    estimation as a regression problem. In other words, the depth map is directly
    regressed from the input without explicitly matching features across the views.
    While these methods are simple and fast at runtime, they require a large amount
    of training data, which is hard to obtain. Methods in the second class mimic the
    traditional stereo matching pipeline by breaking the problem into stages composed
    of differentiable blocks and thus allowing end-to-end training. While a large
    body of the literature focused on pairwise stereo methods, several papers have
    also addressed the multi-view stereo case and these will be reviews in Section [6](#S6
    "6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation").
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类方法（第[5](#S5 "5 End-to-end depth from stereo ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation)节）通过一个可以端到端训练的管道解决立体匹配问题。已经提出了两类主要的方法。早期方法将深度估计公式化为回归问题。换句话说，深度图直接从输入中回归出来，而不需要在视图之间显式匹配特征。虽然这些方法在运行时简单且快速，但它们需要大量训练数据，这很难获得。第二类方法通过将问题分解为由可微分块组成的阶段，从而模仿传统的立体匹配管道，并允许端到端训练。虽然大量文献集中于成对立体方法，但也有几篇论文涉及到多视角立体问题，这些将在第[6](#S6
    "6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation)节中回顾。
- en: In all methods, the estimated depth maps can be further refined using refinement
    modules [[12](#bib.bib12), [2](#bib.bib2), [13](#bib.bib13), [3](#bib.bib3)] and/or
    progressive reconstruction strategies where the reconstruction is refined every
    time new images become available.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有方法中，估计的深度图可以通过使用细化模块 [[12](#bib.bib12), [2](#bib.bib2), [13](#bib.bib13),
    [3](#bib.bib3)] 和/或逐步重建策略进一步细化，其中重建会在每次新图像可用时进行改进。
- en: Finally, the performance of deep learning-based stereo methods depends not only
    on the network architecture but also on the datasets on which they have been trained
    (Section [3](#S3 "3 Datasets ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")) and on the training procedure used to optimise their parameters
    (Section [7](#S7 "7 Training end-to-end stereo methods ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")). The latter includes the choice
    of the loss functions and the supervision mode, which can be fully supervised
    with 3D annotations, weakly supervised, or self-supervised. We will discuss all
    these aspects in the subsequent sections.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，基于深度学习的立体方法的性能不仅依赖于网络架构，还依赖于其训练的数据集（第[3](#S3 "3 Datasets ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation)节）和用于优化其参数的训练过程（第[7](#S7
    "7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques for
    Stereo-based Depth Estimation)节）。后者包括损失函数的选择和监督模式，这可以是完全监督的3D注释、弱监督的或自监督的。我们将在后续部分讨论这些方面。
- en: 3 Datasets
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集
- en: 'TABLE I: Datasets for depth/disparity estimation. ”GT”: ground-truth, ”Tr.”:
    training, ”Ts.”: testing, ”fr.”: frames, ”Vol.”: volumetric, ”Eucl”: Euclidean,
    ”Ord”: ordinal, ”Int.”: intrinsic, ”Ext.”: extrinsic.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 深度/差异估计数据集。 "GT"：真实值，"Tr."：训练，"Ts."：测试，"fr."：帧，"Vol."：体积，"Eucl"：欧几里得，"Ord"：序数，"Int."：内部，"Ext."：外部。'
- en: '|  | Year | Type | Purpose | Images |  | Depth |  | Cam. params. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | 年份 | 类型 | 目的 | 图像 |  | 深度 |  | 相机参数 |'
- en: '|  | Resolution | # Scenes | # Views per scene | # Tr. scenes | # Ts. scenes
    |  | Resolution | #GT frames | Type | Depth range | Disparity range |  | Int.
    | Ext. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | 分辨率 | 场景数量 | 每场景视图数量 | 训练场景数量 | 测试场景数量 |  | 分辨率 | GT帧数量 | 类型 | 深度范围 |
    差异范围 |  | 内部 | 外部 |'
- en: '| Make3D [[14](#bib.bib14)] | 2009 | Real | Monocular depth | $2272\times 1704$
    | $534$ | monocular | $400$ | $134$ |  | $78\times 51$ | $534$ | Dense | $-$ |
    $-$ |  |  |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Make3D [[14](#bib.bib14)] | 2009 | 真实 | 单目深度 | $2272\times 1704$ | $534$
    | 单目 | $400$ | $134$ |  | $78\times 51$ | $534$ | 密集 | $-$ | $-$ |  |  |  |'
- en: '| KITTI2012 [[15](#bib.bib15)] | 2012 | Real | Stereo | $1240\times 376$ |
    $389$ | $2$ | $194$ | $195$ |  | $1226\times 370$ | $-$ | Sparse | $-$ | $-$ |  |
    Y | Y |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| KITTI2012 [[15](#bib.bib15)] | 2012 | 真实 | 立体 | $1240\times 376$ | $389$
    | $2$ | $194$ | $195$ |  | $1226\times 370$ | $-$ | 稀疏 | $-$ | $-$ |  | Y | Y
    |'
- en: '| MPI Sintel [[16](#bib.bib16)] | 2012 | Synthetic | Optical flow | $1024\times
    436$ | $35$ videos | $50$ | $23$ videos | $12$ videos |  | $-$ | $-$ | Dense |
    $-$ | $-$ |  |  |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| NYU2 [[17](#bib.bib17)] | 2012 | Real - indoor | Monocular depth, object
    segmentation | $640\times 480$ | $464$ videos, 100$+$ fr. per video | monocular
    | $-$ | $-$ |  | $-$ | $1,449$ | Kinect depth | $-$ | $-$ |  | N | N |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| RGB-D SLAM [[18](#bib.bib18)] | 2012 | Real | SLAM | $640\times 480$ | $19$
    videos |  | $15$ videos | $4$ videos |  | $-$ | $-$ | Dense | $-$ | $-$ |  | Y
    | Y |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| SUN3D [[19](#bib.bib19)] | 2013 | Real - rooms | Monocular video | $640\times
    480$ | $415$ videos, 10$-$1000$+$ fr. per video | $-$ | $-$ | $-$ |  | $-$ | $-$
    | Dense, SfM | $-$ | $-$ |  |  | Y |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Middleburry [[20](#bib.bib20)] | 2014 | Indoor | Stereo | $2948\times 1988$
    | $30$ | $2$ | $15$ | $15$ |  | $2948\times 1988$ | $30$ | Dense | $-$ | $260$
    |  | Y | Y |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| KITTI 2015 [[21](#bib.bib21)] | 2015 | Real | Stereo | $1242\times 375$ |
    $400$ | $4$ | $200$ | $200$ |  | $1242\times 375$ | $-$ | Sparse | $-$ | $-$ |  |
    Y | Y |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| KITTI-MVS2015 [[21](#bib.bib21)] | 2015 | Real | MVS | $1242\times 375$ |
    $400$ | $20$ | $200$ | $200$ |  | $-$ | $-$ | Sparse | $-$ | $-$ |  | Y | Y |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| FlyingThings3D, Monkaa, Driving [[22](#bib.bib22)] | 2016 | Synthetic | Stereo,
    Video, Optical flow | $960\times 540$ | $39$K frames | $2$ | $21,818$ | $4,248$
    |  | $384\times 192$ | $-$ | Dense | $-$ | $160$px |  | Y | Y |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| CityScapes [[23](#bib.bib23)] | 2016 | Street scenes | Semantic seg., dense
    labels | $2048\times 1024$ | $5$K | $2$ | $2975$ | $1525$ |  | $-$ | $-$ | NA
    | $-$ | $-$ |  | Ego-motion |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Semantic seg.,coarse labels | $2048\times 1024$ | $20$K | $2$ |  |  |  |
    NA | NA | NA | NA | NA |  | Ego-motion |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| DTU [[24](#bib.bib24)] | 2016 | Real, small objects | MVS | $1200\times 1600$
    | $80$ | $49-64$ | $-$ | $-$ |  | $-$ | $-$ | Structured light scans | $-$ | $-$
    |  | Y | Y |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| ETH3D [[25](#bib.bib25)] | 2017 | Real, in/outdoor | Low-res, Stereo | $940\times
    490$ | $47$ | $2$ | $27$ | $20$ |  | $-$ | $47$ | Dense | $-$ | $-$ |  | Y | Y
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Low-res, MVS on video | $940\times 490$ | $10$ videos | $4$ | $5$
    videos | $5$ videos |  | $-$ | $-$ | Dense | $-$ | $-$ |  | Y | Y |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | High-res, MVS on images from DSLR camera | $940\times 490$ | $25$
    | $14-76$ | $13$ | $12$ |  | $-$ | $25$ | Dense | $-$ | $-$ |  | Y | Y |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| SUNCG [[26](#bib.bib26)] | 2017 | Synthetic, indoor | Scene completion |
    $-$ | $45$K | $-$ | $-$ | $-$ |  | $640\times 480$ | $-$ | Depth and Vol. GT |
    $-$ | $-$ |  |  |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| MVS-Synth [[27](#bib.bib27)] | 2018 | Synth - urban | MVS | $1920\times 1080$
    | $120$ | $100$ |  |  |  | $-$ | $-$ | Dense | $-$ | $-$ |  | Y | Y |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| MegaDepth [[28](#bib.bib28)] | 2018 | Real (Internet images) | Monocular,
    Eucl. and ord. depth | $1600\times 1600$ | $130$K | monocular | $-$ | $-$ |  |
    $-$ | $100$K (Eucl.), $30$K (Ord.) | Dense, Eucl., Ord. | $-$ | $-$ |  |  |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| Jeon and Lee [[29](#bib.bib29)] | $2018$ | Real | Depth enhancement | $-$
    | $4$K images | $-$ | $-$ | $-$ |  | $640\times 480$ | $4,000$ | Dense | $0.01-30$m
    | $-$ |  | Y | Y |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Jeon and Lee [[29](#bib.bib29)] | $2018$ | 真实 | 深度增强 | $-$ | $4$K 图像 | $-$
    | $-$ | $-$ |  | $640\times 480$ | $4,000$ | 密集 | $0.01-30$m | $-$ |  | Y | Y
    |'
- en: '| OmniThings [[30](#bib.bib30), [31](#bib.bib31)] | 2019 | Synthetic, fisheye
    images | Omnidirectional MVS | $800\times 768$ | $10240$ | $4$ | $9216$ | $1024$
    |  | $640\times 320$ | $-$ | Dense | $-$ | $\leq 192$px |  |  |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| OmniThings [[30](#bib.bib30), [31](#bib.bib31)] | 2019 | 合成，鱼眼图像 | 全景MVS
    | $800\times 768$ | $10240$ | $4$ | $9216$ | $1024$ |  | $640\times 320$ | $-$
    | 密集 | $-$ | $\leq 192$px |  |  |  |'
- en: '| OmniHouse [[30](#bib.bib30), [31](#bib.bib31)] | 2019 | Synthetic, fisheye
    images | Omnidirectional MVS | $800\times 768$ | $2,560$ | $4$ | $2048$ | $512$
    |  | $640\times 320$ | $-$ | Dense | $-$ | $\leq 192$px |  |  |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| OmniHouse [[30](#bib.bib30), [31](#bib.bib31)] | 2019 | 合成，鱼眼图像 | 全景MVS |
    $800\times 768$ | $2,560$ | $4$ | $2048$ | $512$ |  | $640\times 320$ | $-$ |
    密集 | $-$ | $\leq 192$px |  |  |  |'
- en: '| HR-VS [[32](#bib.bib32)] | 2019 | Synthetic, outdoor | High res. stereo |
    $2056\times 2464$ | $780$ | $2$ | $-$ | $-$ |  | $1918\times 2424$ | 780 | Dense,
    Eucl. | $2.52$ to $200$m | $9.66$ to $768$px |  |  |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| HR-VS [[32](#bib.bib32)] | 2019 | 合成，户外 | 高分辨率立体 | $2056\times 2464$ | $780$
    | $2$ | $-$ | $-$ |  | $1918\times 2424$ | 780 | 密集，欧几里得 | $2.52$到$200$m | $9.66$到$768$px
    |  |  |  |'
- en: '|  |  | Real, outdoor | High res. stereo | $1918\times 2424$ | $33$ | $2$ |
    $-$ | $-$ |  | $1918\times 2424$ | 33 | Dense, Eucl. |  | $5.41$ to $182.3$px
    |  |  |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 真实，户外 | 高分辨率立体 | $1918\times 2424$ | $33$ | $2$ | $-$ | $-$ |  | $1918\times
    2424$ | 33 | 密集，欧几里得 |  | $5.41$到$182.3$px |  |  |  |'
- en: '| DrivingStereo [[33](#bib.bib33)] | 2019 | Driving | High res. stereo | $1762\times
    800$ | $182,188$ | $2$ | $174,437$ | $7,751$ |  | $1762\times 800$ | $182,188$
    | Sparse | up to $80$m |  |  |  |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| DrivingStereo [[33](#bib.bib33)] | 2019 | 驾驶 | 高分辨率立体 | $1762\times 800$
    | $182,188$ | $2$ | $174,437$ | $7,751$ |  | $1762\times 800$ | $182,188$ | 稀疏
    | 高达 $80$m |  |  |  |  |'
- en: '| ApolloScape [[34](#bib.bib34)] | 2019 | Auto. driving | High res. stereo
    | $3130\times 960$ | $5,165$ | $2$ | $4,156$ | $1,009$ |  | $-$ | $5165$ | LIDAR
    | $-$ to $-$m | $-$ |  | Y | $-$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ApolloScape [[34](#bib.bib34)] | 2019 | 自动驾驶 | 高分辨率立体 | $3130\times 960$
    | $5,165$ | $2$ | $4,156$ | $1,009$ |  | $-$ | $5165$ | LIDAR | $-$到$-$m | $-$
    |  | Y | $-$ |'
- en: '| A2D2 [[35](#bib.bib35)] | 2020 | Auto. driving | High res. stereo | $2.3$M
    pixel | $41,277$ | $6$ | $-$ | $-$ |  | $-$ | $-$ | LIDAR | up to $100$m | $-$
    |  | Y | Y |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| A2D2 [[35](#bib.bib35)] | 2020 | 自动驾驶 | 高分辨率立体 | $2.3$M 像素 | $41,277$ | $6$
    | $-$ | $-$ |  | $-$ | $-$ | LIDAR | 高达 $100$m | $-$ |  | Y | Y |'
- en: Table [I](#S3.T1 "TABLE I ‣ 3 Datasets ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") summarizes some of the datasets that have
    been used to train and test deep learning-based depth estimation algorithms. Below,
    we discuss these datasets based on their sizes, their spatial and depth resolution,
    the type of depth annotation they provide, and the domain gap (or shift) issue
    faced by many deep learning-based algorithms.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [I](#S3.T1 "TABLE I ‣ 3 Datasets ‣ A Survey on Deep Learning Techniques for
    Stereo-based Depth Estimation") 总结了用于训练和测试基于深度学习的深度估计算法的一些数据集。下面，我们根据这些数据集的大小、空间和深度分辨率、提供的深度标注类型以及许多基于深度学习的算法面临的领域差距（或偏移）问题进行讨论。
- en: (1) Dataset size. The first datasets, which appeared prior to $2016$, are of
    small scale due to the difficulty of creating ground-truth 3D annotations. An
    example is the two KITTI datasets [[15](#bib.bib15), [21](#bib.bib21)], which
    contain $200$ stereo pairs with their corresponding disparity ground-truth. They
    have been extensively used to train and test patch-based CNNs for stereo matching
    algorithms (see Section [4](#S4 "4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")), which have a small receptive
    field. As such a single stereo pair can result in thousands of training samples.
    However, in end-to-end architectures (Sections [5](#S5 "5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    and [6](#S6 "6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")), a stereo pair corresponds to only one sample.
    End-to-end networks have a large number of parameters, and thus require large
    datasets for efficient training. While collecting large image datasets is very
    easy, *e.g.,* by using video sequences as in *e.g.,* NYU2 [[17](#bib.bib17)],
    ETH3D [[25](#bib.bib25)], SUN3D [[19](#bib.bib19)], and ETH3D [[25](#bib.bib25)],
    annotating them with 3D labels is time consuming. Recent works, *e.g.,* the AppoloScape [[34](#bib.bib34)]
    and A2D2 [[35](#bib.bib35)], use LIDAR to acquire dense 3D annotations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 数据集大小。最早出现的几个数据集，出现在$2016$年前，由于创建地面真实3D注释的难度，规模较小。例如，两个KITTI数据集[[15](#bib.bib15),
    [21](#bib.bib21)]，包含$200$对立体图像及其对应的视差真实值。它们已被广泛用于训练和测试用于立体匹配算法的基于块的CNN（见第[4](#S4
    "4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation)节），这些算法具有较小的感受野。因此，一个立体图像对可以产生数千个训练样本。然而，在端到端架构（第[5](#S5 "5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation)节和第[6](#S6 "6 Learning multiview stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation)节），一个立体图像对仅对应一个样本。端到端网络具有大量参数，因此需要大型数据集以实现高效训练。虽然收集大型图像数据集非常容易，*例如*，使用视频序列，如*例如*，NYU2[[17](#bib.bib17)]、ETH3D[[25](#bib.bib25)]、SUN3D[[19](#bib.bib19)]和ETH3D[[25](#bib.bib25)]，但用3D标签进行注释是耗时的。最近的工作，*例如*，AppoloScape[[34](#bib.bib34)]和A2D2[[35](#bib.bib35)]，使用LIDAR获取密集的3D注释。
- en: Data augmentation strategies, *e.g.,* by applying geometric and photometric
    transformations to the images that are available, have been extensively used in
    the literature. There are, however, a few other strategies that are specific to
    depth estimation. This includes artificially synthesizing and rendering from 3D
    CAD models 2D and 2.5D views from various (random) viewpoints, poses, and lighting
    conditions. One can also overlay rendered 3D models on the top of real images.
    This approach has been used to generate the FlyingThings3D, Monkaa, and Driving
    datasets of [[22](#bib.bib22)], and the OmniThings and OmniHouse datasets for
    benchmarking MVS for omnidirectional images [[30](#bib.bib30), [31](#bib.bib31)].
    Huang *et al.* [[27](#bib.bib27)] followed a similar idea but used scenes from
    video games to generate MVS-Synth, a photo-realistic synthetic dataset prepared
    for learning-based Multi-View Stereo algorithms.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强策略，*例如*，通过对可用图像应用几何和光度变换，已经在文献中被广泛使用。然而，还有一些其他策略是特定于深度估计的。这包括从3D CAD模型人工合成和渲染各种（随机）视角、姿态和光照条件下的2D和2.5D视图。还可以将渲染的3D模型覆盖在真实图像上。这种方法已被用于生成FlyingThings3D、Monkaa和Driving数据集[[22](#bib.bib22)]，以及OmniThings和OmniHouse数据集，用于对全景图像的MVS进行基准测试[[30](#bib.bib30),
    [31](#bib.bib31)]。黄*等人*[[27](#bib.bib27)]遵循了类似的思路，但使用了视频游戏中的场景生成MVS-Synth，这是一个为基于学习的多视图立体算法准备的照片级真实合成数据集。
- en: The main challenge is that generating large amounts of synthetic data containing
    varied real-world appearance and motion is not trivial [[36](#bib.bib36)]. As
    a result, a number of works overcome the need for ground-truth depth information
    by training their deep networks without 3D supervision, see Section [7.1](#S7.SS1
    "7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation"). Others used traditional
    depth estimation and structure-from-motion (SfM) techniques to generate 3D annotations.
    For example, Li *et al.* [[28](#bib.bib28)] used modern structure-from-motion
    and multiview stereo (MVS) methods together with multiview Internet photo collections
    to create the large-scale MegaDepth dataset providing improved depth estimation
    accuracy via bigger training dataset sizes. This dataset has also been automatically
    augmented with ordinal depth relations generated using semantic segmentation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 主要挑战在于生成包含各种现实世界外观和运动的大量合成数据并非易事[[36](#bib.bib36)]。因此，一些研究通过训练其深度网络而无需3D监督来克服对真实深度信息的需求，见第[7.1节](#S7.SS1
    "7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")。其他研究使用传统的深度估计和运动结构（SfM）技术来生成3D标注。例如，李*等人*[[28](#bib.bib28)]结合现代运动结构和多视角立体（MVS）方法及多视角互联网照片集合创建了大规模的MegaDepth数据集，通过更大的训练数据集提高了深度估计的准确性。该数据集还通过语义分割自动增强了序数深度关系。
- en: (2) Spatial and depth resolutions. The disparity/depth information can be either
    in the form of maps of the same or lower resolution than the input images, or
    in the form of sparse depth values at some locations in the reference image. Most
    of the existing datasets are of low spatial resolution. In recent years, however,
    there has been a growing focus on stereo matching with high-resolution images.
    An example of a high-resolution dataset is the HR-VS and HR-RS of Yang *et al.* [[32](#bib.bib32)],
    where each RGB pair of resolution $1918\times 2424$ is annotated with a depth
    map of the same resolution. However, the dataset only contains $800$ pairs of
    stereo images, which is relatively small for end-to-end training. Other datasets
    such as the ApolloScape [[34](#bib.bib34)] and A2D2 [[35](#bib.bib35)] contain
    very high resolution images, of the order of $3130\times 960$, with more that
    $100+$ hours of stereo driving videos, in the case of ApolloScape, have been specifically
    designed to test autonomous driving algorithms.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 空间和深度分辨率。视差/深度信息可以是与输入图像相同或更低分辨率的地图形式，也可以是参考图像中某些位置的稀疏深度值。大多数现有数据集的空间分辨率较低。然而，近年来，越来越多地关注于高分辨率图像的立体匹配。一个高分辨率数据集的例子是杨*等人*[[32](#bib.bib32)]的HR-VS和HR-RS，其中每对分辨率为$1918\times
    2424$的RGB图像都标注有相同分辨率的深度图。然而，该数据集仅包含$800$对立体图像，对于端到端训练来说相对较小。其他数据集如ApolloScape[[34](#bib.bib34)]和A2D2[[35](#bib.bib35)]包含非常高分辨率的图像，约为$3130\times
    960$，其中ApolloScape的立体驾驶视频超过$100+$小时，专门设计用于测试自动驾驶算法。
- en: (3) Euclidean vs. ordinal depth. Instead of manually annotating images with
    exact, *i.e.,* Euclidean, depth values, some papers, *e.g.,* MegaDepth [[28](#bib.bib28)],
    provide ordinal annotations, *i.e.,* pixel $x_{1}$ is closer, farther, or at the
    same depth, as pixel $x_{2}$. Ordinal annotation is simpler and faster to achieve
    than Euclidean annotation. In fact, it can be accurately obtained using traditional
    stereo matching algorithms, since ordinal depth is less sensitive to innacuracies
    in depth estimation
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 欧几里得深度与序数深度。与其手动标注图像的确切*即*欧几里得深度值，不如一些论文*例如*MegaDepth[[28](#bib.bib28)]提供序数标注，即像素$x_{1}$距离更近、更远或与像素$x_{2}$在相同深度。序数标注比欧几里得标注更简单、更快捷。实际上，可以使用传统的立体匹配算法准确获得序数深度，因为序数深度对深度估计中的不准确性不那么敏感。
- en: (4) Domain gap. While artificially augmenting training datasets allows enriching
    existing ones, the domain shift caused by the very different conditions between
    real and synthetic data can result in a lower accuracy when applied to real-world
    environments. We will discuss, in Section [7.3](#S7.SS3 "7.3 Domain adaptation
    and transfer learning ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation"), how this domain shift
    issue has been addressed in the literature.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 域间差距。虽然通过人工扩充训练数据集可以丰富现有数据，但由于真实数据和合成数据之间条件的巨大差异，可能会导致在真实环境中准确性降低。我们将在第[7.3](#S7.SS3
    "7.3 域适应与迁移学习 ‣ 7 端到端立体方法训练 ‣ 深度学习技术在立体深度估计中的应用调查")节中讨论文献中如何解决这一域间差距问题。
- en: 4 Depth by stereo matching
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 通过立体匹配的深度
- en: 'TABLE II: Taxonomy and comparison of deep learning-based stereo matching techniques.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：基于深度学习的立体匹配技术的分类与比较。
- en: '| Method | Year | Feature computation |  | Similarity |  | Training |  | Regularization
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 特征计算 |  | 相似性 |  | 训练 |  | 正则化 |'
- en: '| Architectures | Dimension |  |  | Degree of supervision | Loss |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 维度 |  |  | 监督程度 | 损失 |  |'
- en: '| Zagoruyko [[37](#bib.bib37)] | $2015$ | ConvNet | Multiscale |  | FCN |  |
    Supervised with positive/negative samples | Hinge and squared $L_{2}$ |  | NA
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Zagoruyko [[37](#bib.bib37)] | $2015$ | ConvNet | 多尺度 |  | FCN |  | 有监督，正负样本
    | 挤压和平方 $L_{2}$ |  | NA |'
- en: '| Han [[38](#bib.bib38)] | $2015$ | ConvNet | Fixed scale |  | FCN |  | Supervised
    | Cross-entropy |  | NA |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Han [[38](#bib.bib38)] | $2015$ | ConvNet | 固定尺度 |  | FCN |  | 有监督 | 交叉熵
    |  | NA |'
- en: '| Zbontar [[39](#bib.bib39)] | $2015$ | ConvNet | Fixed scale |  | Hand-crafted
    |  | Triplet contrastive learning | $L_{1}$ |  | MRF |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Zbontar [[39](#bib.bib39)] | $2015$ | ConvNet | 固定尺度 |  | 手工制作 |  | 三元对比学习
    | $L_{1}$ |  | MRF |'
- en: '| Chen [[40](#bib.bib40)] | $2015$ | ConvNet | Multiscale |  | Correlation
    $+$ voting |  | Supervised with positive/negative samples | $L_{1}$ |  | MRF |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[40](#bib.bib40)] | $2015$ | ConvNet | 多尺度 |  | 相关性 $+$ 投票 |  | 有监督，正负样本
    | $L_{1}$ |  | MRF |'
- en: '| Simo [[41](#bib.bib41)] | $2015$ | ConvNet | Fixed scale |  | $L_{2}$ |  |
    Supervised with positive/negative samples | $L_{2}$ |  | NA |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Simo [[41](#bib.bib41)] | $2015$ | ConvNet | 固定尺度 |  | $L_{2}$ |  | 有监督，正负样本
    | $L_{2}$ |  | NA |'
- en: '| Zbontar [[42](#bib.bib42)] | $2016$ | ConvNet | Fixed scale |  | Hand-crafted,
    FCN |  | Supervised with known disparity | Hinge |  | Classic stereo |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Zbontar [[42](#bib.bib42)] | $2016$ | ConvNet | 固定尺度 |  | 手工制作, FCN |  |
    有监督，已知视差 | 挤压 |  | 经典立体 |'
- en: '| Balantas [[43](#bib.bib43)] | $2016$ | ConvNet | Fixe scale |  | $L_{2}$
    |  | Supervised, triplet contrastive learning | Soft-Positive-Negative (Soft-PN)
    |  | $-$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Balantas [[43](#bib.bib43)] | $2016$ | ConvNet | 固定尺度 |  | $L_{2}$ |  | 有监督，三元对比学习
    | 软正负样本 (Soft-PN) |  | $-$ |'
- en: '| Mayer [[22](#bib.bib22)] | $2016$ | ConvNet | Fixed-scale |  | Hand-crafted
    |  | Supervised | $-$ |  | Encoder-decoder |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Mayer [[22](#bib.bib22)] | $2016$ | ConvNet | 固定尺度 |  | 手工制作 |  | 有监督 | $-$
    |  | 编码器-解码器 |'
- en: '| Luo [[44](#bib.bib44)] | $2016$ | ConvNet | Fixed scale |  | Correlation
    |  | Supervised | Cross-entropy |  | MRF |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Luo [[44](#bib.bib44)] | $2016$ | ConvNet | 固定尺度 |  | 相关性 |  | 有监督 | 交叉熵
    |  | MRF |'
- en: '| Kumar [[45](#bib.bib45)] | 2016 | ConvNet | Fixed scale |  | ConvNet |  |
    Supervised, triplet contrastive learning | Maximise inter-class distance, |  |
    $-$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Kumar [[45](#bib.bib45)] | 2016 | ConvNet | 固定尺度 |  | ConvNet |  | 有监督，三元对比学习
    | 最大化类间距离 |  | $-$ |'
- en: '|  |  |  |  |  |  |  |  | minimize inter-class distance. |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | 最小化类间距离。 |  |  |'
- en: '| Shaked [[46](#bib.bib46)] | $2017$ | Highway network with | Fixed scale |  |
    FCN |  | Supervised | Hinge$+$cross-entropy |  | Classic$+$4Conv$+$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Shaked [[46](#bib.bib46)] | $2017$ | 高速网络 | 固定尺度 |  | FCN |  | 有监督 | 挤压$+$交叉熵
    |  | 经典$+$4Conv$+$ |'
- en: '|  |  | multilevel skip connections |  |  |  |  |  |  |  | 5FC |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 多层跳跃连接 |  |  |  |  |  |  |  | 5FC |'
- en: '| Hartmann [[47](#bib.bib47)] | $2017$ | ConvNet | Fixed scale |  | ConvNet
    |  | Supervised | Croos-entropy |  | Encoder |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Hartmann [[47](#bib.bib47)] | $2017$ | ConvNet | 固定尺度 |  | ConvNet |  | 有监督
    | 交叉熵 |  | 编码器 |'
- en: '| Park [[48](#bib.bib48)] | $2017$ | ConvNet | Fixed scale |  | $1\times 1$
    Convs, |  | Supervised | $-$ |  | NA |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Park [[48](#bib.bib48)] | $2017$ | ConvNet | 固定尺度 |  | $1\times 1$ 卷积 |  |
    有监督 | $-$ |  | NA |'
- en: '|  |  |  |  |  | ReLU, SPP |  |  |  |  |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | ReLU, SPP |  |  |  |  |  |'
- en: '| Ye [[49](#bib.bib49)] | $2017$ | ConvNet | Fixed scale |  | FCN |  | Supervised
    | $L_{1}$ |  | SGM |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Ye [[49](#bib.bib49)] | $2017$ | ConvNet | 固定尺度 |  | FCN |  | 有监督 | $L_{1}$
    |  | SGM |'
- en: '|  |  | Multisize pooling |  |  | ($1\times 1$ convs) |  |  |  |  |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 多尺寸池化 |  |  | ($1\times 1$ 卷积) |  |  |  |  |  |'
- en: '| Tulyakov [[50](#bib.bib50)] | $2017$ | Generic - independent of the network
    architecture |  | Weakly supervised | MIL, Contrastive, Contrastive-DP |  | $-$
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Tulyakov [[50](#bib.bib50)] | $2017$ | 通用 - 与网络架构无关 |  | 弱监督 | MIL, 对比, 对比-DP
    |  | $-$ |'
- en: 'Stereo-based depth reconstruction methods take $n=2$ RGB images and produce
    a disparity map $D$ that minimizes an energy function of the form:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于立体的深度重建方法采用 $n=2$ RGB 图像，并生成一个视差图 $D$，该视差图最小化形式为：
- en: '|  | $E(D)=\sum_{x}C(x,0pt_{x})+\sum_{x}\sum_{y\in\mathcal{N}_{x}}E_{s}(0pt_{x},0pt_{y}).$
    |  | (1) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(D)=\sum_{x}C(x,0pt_{x})+\sum_{x}\sum_{y\in\mathcal{N}_{x}}E_{s}(0pt_{x},0pt_{y}).$
    |  | (1) |'
- en: Here, $x$ and $y$ are image pixels, and $\mathcal{N}_{x}$ is the set of pixels
    that are within the neighborhood of $x$. The first term of Eqn. ([1](#S4.E1 "In
    4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")) is the matching cost. When using rectified stereo pairs, $C(x,0pt_{x})$
    measures the cost of matching the pixel $x=(i,j)$ of the left image with the pixel
    $y=(i,j-0pt_{x})$ of the right image. In this case, $0pt_{x}=D(x)\in[d_{min},d_{max}]$
    is the disparity at pixel $x$. Depth can then be inferred by triangulation. When
    the disparity range is discritized into $n_{d}$ disparity levels, $C$ becomes
    a 3D cost volume of size $0pt\times 0pt\times n_{d}$. In the more general multiview
    stereo case, *i.e.,* $n\geq 2$, the cost $C(x,0pt_{x})$ measures the inverse likelihood
    of $x$ on the reference image having depth $0pt_{x}$. The second term of Eqn. ([1](#S4.E1
    "In 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")) is a regularization term used to impose constraints such as
    smoothness and left-right consistency.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$x$ 和 $y$ 是图像像素，$\mathcal{N}_{x}$ 是位于 $x$ 邻域内的像素集合。公式 ([1](#S4.E1 "In 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")) 的第一项是匹配代价。在使用矫正后的立体对时，$C(x,0pt_{x})$ 测量将左图像的像素 $x=(i,j)$ 与右图像的像素
    $y=(i,j-0pt_{x})$ 匹配的代价。在这种情况下，$0pt_{x}=D(x)\in[d_{min},d_{max}]$ 是像素 $x$ 的视差。然后可以通过三角测量推断深度。当视差范围离散为
    $n_{d}$ 个视差级别时，$C$ 变成一个大小为 $0pt\times 0pt\times n_{d}$ 的 3D 代价体积。在更一般的多视角立体情况下，*即，*
    $n\geq 2$，代价 $C(x,0pt_{x})$ 测量 $x$ 在参考图像上具有深度 $0pt_{x}$ 的逆概率。公式 ([1](#S4.E1 "In
    4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")) 的第二项是一个正则化项，用于施加平滑性和左右一致性等约束。
- en: '![Refer to caption](img/e3d644b4ae621dd997815832d964a711.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/e3d644b4ae621dd997815832d964a711.png)'
- en: 'Figure 1: The building blocs of a stereo matching pipeline.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：立体匹配流水线的构建模块。
- en: 'Traditionally, this problem has been solved using a pipeline of four building
    blocks [[11](#bib.bib11)], see Fig. [1](#S4.F1 "Figure 1 ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation"): (1)
    feature extraction, (2) feature matching across images, (3) disparity computation,
    and (4) disparity refinement and post-processing. The first two blocks construct
    the cost volume $C$. The third block regularizes the cost volume and then finds,
    by minimizing Eqn. ([1](#S4.E1 "In 4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")), an initial estimate
    of the disparity map. The last block refines and post-processes the initial disparity
    map.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，这个问题是通过使用四个构建模块的流水线来解决的 [[11](#bib.bib11)]，见图 [1](#S4.F1 "图 1 ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")：（1）特征提取，（2）图像间特征匹配，（3）视差计算，以及（4）视差细化和后处理。前两个模块构建了代价体积 $C$。第三个模块对代价体积进行正则化，然后通过最小化公式 ([1](#S4.E1
    "In 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation"))，找到视差图的初步估计。最后一个模块对初步视差图进行细化和后处理。
- en: This section focuses on how these individual blocks have been implemented using
    deep learning-based methods. Table [II](#S4.T2 "TABLE II ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation") summarises
    the state-of-the-art methods.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍如何使用基于深度学习的方法实现这些单独的模块。表 [II](#S4.T2 "TABLE II ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation") 总结了最新的技术方法。
- en: 4.1 Learning feature extraction and matching
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 学习特征提取和匹配
- en: '| ![Refer to caption](img/8acf528e7280601c61cc431040a1f554.png) | ![Refer to
    caption](img/906a51c527f1892aee51d9afe7e07e17.png) | ![Refer to caption](img/06270467dce79b33d4ba68ed22252fa9.png)
    | ![Refer to caption](img/49200cc32256c862424df9ae36c6f0b1.png) | ![Refer to caption](img/228c9da17f799c9184c363bcc8c1d9ee.png)
    | ![Refer to caption](img/fad058a7f417212e4f18d90faef5745c.png) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ![Refer to caption](img/8acf528e7280601c61cc431040a1f554.png) | ![Refer to
    caption](img/906a51c527f1892aee51d9afe7e07e17.png) | ![Refer to caption](img/06270467dce79b33d4ba68ed22252fa9.png)
    | ![Refer to caption](img/49200cc32256c862424df9ae36c6f0b1.png) | ![Refer to caption](img/228c9da17f799c9184c363bcc8c1d9ee.png)
    | ![Refer to caption](img/fad058a7f417212e4f18d90faef5745c.png) |'
- en: '| (a) MC-CNN [[39](#bib.bib39), [42](#bib.bib42)]. | (b) [[37](#bib.bib37)]
    and [[38](#bib.bib38)]. | (c) [[37](#bib.bib37)]. | (d) LW-CNN [[48](#bib.bib48)].
    | (e) FED-D2DRR [[49](#bib.bib49)]. | (f) [[37](#bib.bib37)]. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (a) MC-CNN [[39](#bib.bib39), [42](#bib.bib42)]. | (b) [[37](#bib.bib37)]
    和 [[38](#bib.bib38)]. | (c) [[37](#bib.bib37)]. | (d) LW-CNN [[48](#bib.bib48)].
    | (e) FED-D2DRR [[49](#bib.bib49)]. | (f) [[37](#bib.bib37)]. |'
- en: 'Figure 2: Feature learning architectures.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：特征学习架构。
- en: Early deep learning techniques for stereo matching replace the hand-crafted
    features (block A of Fig. LABEL:ig:stereo_matching_pipeline) with learned features [[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [42](#bib.bib42)]. They take two patches,
    one centered at a pixel $x=(i,j)$ on the left image and another one centered at
    pixel $y=(i,j-d)$ on the right image (with $d\in\{0,\dots,n_{d}\}$), compute their
    corresponding feature vectors using a CNN, and then match them (block B of Fig. LABEL:ig:stereo_matching_pipeline),
    to produce a similarity score $C(x,d)$, using either standard similarity metrics
    such as the $L_{1}$, the $L_{2}$, and the correlation metric, or metrics learned
    using a top network. The two components can be trained either separately or jointly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的深度学习技术用于立体匹配，取代了手工制作的特征（图LABEL:ig:stereo_matching_pipeline的A块），采用了学习到的特征[[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [42](#bib.bib42)]。它们取两个图块，一个中心在左图中的像素$x=(i,j)$，另一个中心在右图中的像素$y=(i,j-d)$（其中$d\in\{0,\dots,n_{d}\}$），使用CNN计算它们的特征向量，然后匹配它们（图LABEL:ig:stereo_matching_pipeline的B块），生成相似度分数$C(x,d)$，使用标准的相似度度量如$L_{1}$、$L_{2}$和相关度量，或者使用顶级网络学习的度量。这两个组件可以分别训练，也可以联合训练。
- en: 4.1.1 The basic network architecture
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 基本网络架构
- en: 'The basic network architecture, introduced in [[37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39), [42](#bib.bib42)] and shown in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1
    Learning feature extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation")-(a), is composed
    of two CNN encoding branches, which act as descriptor computation modules. The
    first branch takes a patch around a pixel $x=(i,j)$ on the left image and outputs
    a feature vector that characterizes that patch. The second branch takes a patch
    around the pixel $y=(i,j-d)$, where $d\in[d_{min},d_{max}]$ is a candidate disparity.
    Zbontar and LeCun [[39](#bib.bib39)] and later Zbontar *et al.* [[42](#bib.bib42)]
    use an encoder composed of four convolutional layers, see Fig. [2](#S4.F2 "Figure
    2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(a).
    Each layer, except the last one, is followed by a ReLU unit. Zagoruyko and Komodakis [[37](#bib.bib37)]
    and Han *et al.* [[38](#bib.bib38)] use a similar architecture but add:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基本网络架构在[[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [42](#bib.bib42)]中介绍，并在图[2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(a)中展示，由两个CNN编码分支组成，这些分支作为描述符计算模块。第一个分支接收左图中一个像素$x=(i,j)$周围的图块，并输出一个特征向量来表征该图块。第二个分支接收围绕像素$y=(i,j-d)$的图块，其中$d\in[d_{min},d_{max}]$是一个候选视差。Zbontar和LeCun[[39](#bib.bib39)]及后来Zbontar
    *et al.*[[42](#bib.bib42)]使用由四个卷积层组成的编码器，见图[2](#S4.F2 "Figure 2 ‣ 4.1 Learning
    feature extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")-(a)。除了最后一层，每一层后面都跟着一个ReLU单元。Zagoruyko和Komodakis[[37](#bib.bib37)]以及Han
    *et al.*[[38](#bib.bib38)]使用类似的架构，但增加了：
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: max-pooling and subsampling after each layer, except the last one, see Fig. [2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(b).
    As such, the network is able to account for larger patch sizes and a larger variation
    in the viewpoint compared to [[39](#bib.bib39), [42](#bib.bib42)].
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每一层之后都进行最大池化和子采样，除了最后一层，见图 [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction
    and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")-(b)。因此，与 [[39](#bib.bib39), [42](#bib.bib42)]
    相比，网络能够处理更大的补丁尺寸和更大的视角变化。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a Spatial Pyramid Pooling (SPP) module at the end of each feature extraction
    branch [[37](#bib.bib37)] so that the network can process patches of arbitrary
    sizes while producing features of a fixed size, see Fig. [2](#S4.F2 "Figure 2
    ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching ‣
    A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(c).
    Its role is to aggregate the features of the last convolutional layer, through
    spatial pooling, into a feature grid of a fixed size. The module is designed in
    such a way that the size of the pooling regions varies with the size of the input
    to ensure that the output feature grid has a fixed size independently of the size
    of the input patch or image. Thus, the network is able to process patches/images
    of arbitrary sizes and compute feature vectors of the same dimension without changing
    its structure or retraining.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个特征提取分支的末尾添加一个空间金字塔池化 (SPP) 模块 [[37](#bib.bib37)]，以便网络能够处理任意大小的补丁，同时产生固定大小的特征，见图
    [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(c)。其作用是通过空间池化将最后一层卷积的特征汇总到一个固定大小的特征网格中。该模块的设计方式使得池化区域的大小随着输入的大小变化，以确保输出特征网格具有固定的大小，而不依赖于输入补丁或图像的大小。因此，网络能够处理任意大小的补丁/图像，并计算相同维度的特征向量，而无需改变其结构或重新训练。
- en: The learned features are then fed to a top module, which returns a similarity
    score. It can be implemented as a standard similarity metric, *e.g.,* the $L_{2}$
    distance, the cosine distance, and the (normalized) correlation distance (or inner
    product) as in the MC-CNN-fast (MC-CNN-fst) architecture of [[39](#bib.bib39),
    [42](#bib.bib42)]. The main advantage of the correlation over the $L_{2}$ distance
    is that it can be implemented using a layer of 2D [[51](#bib.bib51)] or 1D [[22](#bib.bib22)]
    convolutional operations, called *correlation layer*. A correlation layer does
    not require training since the filters are in fact the features computed by the
    second branch of the network. As such, correlation layers have been extensively
    used in the literature [[39](#bib.bib39), [41](#bib.bib41), [42](#bib.bib42),
    [22](#bib.bib22), [44](#bib.bib44)].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的特征然后被输入到一个顶部模块，该模块返回一个相似度评分。它可以实现为标准的相似度度量，*例如*，$L_{2}$ 距离、余弦距离和（归一化的）相关距离（或内积），如在
    [[39](#bib.bib39), [42](#bib.bib42)] 的 MC-CNN-fast (MC-CNN-fst) 架构中。相关距离相对于 $L_{2}$
    距离的主要优势在于，它可以通过 2D [[51](#bib.bib51)] 或 1D [[22](#bib.bib22)] 卷积操作层实现，称为 *相关层*。相关层不需要训练，因为滤波器实际上是由网络的第二分支计算的特征。因此，相关层在文献中得到了广泛应用
    [[39](#bib.bib39), [41](#bib.bib41), [42](#bib.bib42), [22](#bib.bib22), [44](#bib.bib44)]。
- en: Instead of using hand-crafted similarity measures, recent works use a decision
    network composed of fully-connected (FC) layers [[38](#bib.bib38), [37](#bib.bib37),
    [42](#bib.bib42), [46](#bib.bib46), [49](#bib.bib49)], which can be implemented
    as $1\times 1$ convolutions, fully convolutional layers [[47](#bib.bib47)], or
    convolutional layers followed by fully-connected layers. The decision network
    is trained jointly with the feature extraction module to assess the similarity
    between two image patches. Han *et al.* [[38](#bib.bib38)] use a top network composed
    of three fully-connected layers followed by a softmax. Zagoruyko and Komodakis [[37](#bib.bib37)]
    use two linear fully connected layers (each with $512$ hidden units) that are
    separated by a ReLU activation layer while the MC-CNN-acrt network of Zbontar
    *et al.* [[42](#bib.bib42)] use up to five fully-connected layers. In all cases,
    the features computed by the two branches of the feature encoding module are first
    concatenated and then fed to the top network. Hartmann *et al.* [[47](#bib.bib47)],
    on the other hand, aggregate the features coming from multiple patches using mean
    pooling before feeding them to a decision network. The main advantage of aggregation
    by pooling over concatenation is that the former can handle any arbitrary number
    of patches without changing the architecture of the network or re-training it.
    As such, it is suitable for computing multi-patch similarity.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 近期的研究使用了由全连接（FC）层组成的决策网络[[38](#bib.bib38), [37](#bib.bib37), [42](#bib.bib42),
    [46](#bib.bib46), [49](#bib.bib49)]，而不是手工制作的相似性度量，这些网络可以实现为$1\times 1$卷积、全卷积层[[47](#bib.bib47)]，或者是卷积层后接全连接层。决策网络与特征提取模块联合训练，以评估两个图像块之间的相似性。Han
    *et al.*[[38](#bib.bib38)]使用了由三个全连接层和一个softmax组成的顶层网络。Zagoruyko和Komodakis[[37](#bib.bib37)]使用两个线性全连接层（每个有$512$个隐藏单元），并由ReLU激活层分隔，而Zbontar
    *et al.*[[42](#bib.bib42)]的MC-CNN-acrt网络使用了多达五个全连接层。在所有情况下，由特征编码模块的两个分支计算出的特征首先被连接在一起，然后输入到顶层网络中。另一方面，Hartmann
    *et al.*[[47](#bib.bib47)]通过均值池化将来自多个图像块的特征进行聚合，然后将其输入到决策网络中。池化聚合的主要优点是它可以处理任意数量的图像块，而不需要更改网络架构或重新训练。因此，它适合于计算多图像块的相似性。
- en: Using a decision network instead of hand-crafted similarity measures enables
    learning, from data, the appropriate similarity measure instead of imposing one
    at the outset. It is more accurate than using a correlation layer but is significantly
    slower.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策网络代替手工制作的相似性度量方法可以从数据中学习到适当的相似性度量，而不是一开始就强加一个。这比使用相关层更准确，但速度明显较慢。
- en: 4.1.2 Network architecture variants
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 网络架构变体
- en: 'Since its introduction, the baseline architecture has been extended in several
    ways in order to: (1) improve training using residual networks (ResNet) [[46](#bib.bib46)],
    (2) enlarge the receptive field of the network without losing in resolution or
    in computation efficiency [[48](#bib.bib48), [49](#bib.bib49), [52](#bib.bib52)],
    (3) handling multiscale features [[37](#bib.bib37), [40](#bib.bib40)], (4) reducing
    the number of forward passes [[37](#bib.bib37), [44](#bib.bib44)], and (5) easing
    the training procedure by learning similarity without explicitly learning features [[37](#bib.bib37)].'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 自引入以来，基线架构已经在几个方面进行了扩展，以：(1) 使用残差网络（ResNet）[[46](#bib.bib46)]改进训练，(2) 扩大网络的接收域，同时不在分辨率或计算效率上妥协[[48](#bib.bib48),
    [49](#bib.bib49), [52](#bib.bib52)]，(3) 处理多尺度特征[[37](#bib.bib37), [40](#bib.bib40)]，(4)
    减少前向传播次数[[37](#bib.bib37), [44](#bib.bib44)]，以及(5) 通过学习相似性而非显式学习特征来简化训练过程[[37](#bib.bib37)]。
- en: 4.1.2.1 ConvNet vs. ResNet
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.1 ConvNet与ResNet
- en: While Zbontar *et al.* [[39](#bib.bib39), [42](#bib.bib42)] and Han *et al.* [[38](#bib.bib38)]
    use standard convolutional layers in the feature extraction block, Shaked and
    Wolf [[46](#bib.bib46)] add residual blocks with multilevel weighted residual
    connections to facilitate the training of very deep networks. Its particularity
    is that the network learns by itself how to adjust the contribution of the added
    skip connections. It was demonstrated that this architecture outperforms the base
    network of Zbontar *et al.* [[39](#bib.bib39)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Zbontar *et al.*[[39](#bib.bib39), [42](#bib.bib42)]和Han *et al.*[[38](#bib.bib38)]在特征提取块中使用了标准卷积层，但Shaked和Wolf[[46](#bib.bib46)]则添加了具有多级加权残差连接的残差块，以促进非常深层网络的训练。其特别之处在于网络能够自我调整跳跃连接的贡献。已经证明，这种架构优于Zbontar
    *et al.*[[39](#bib.bib39)]的基础网络。
- en: 4.1.2.2 Enlarging the receptive field of the network
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.2 扩大网络的接收域
- en: The scale of the learned features is defined by (1) the size of the input patches,
    (2) the receptive field of the network, and (3) the kernel size of the convolutional
    filters and pooling operations used in each layer. While increasing the kernel
    sizes allows the capture of more global interactions between the image pixels,
    it induces a high computational cost. Also, the conventional pooling, as used
    in [[39](#bib.bib39), [42](#bib.bib42)], reduces resolution and could cause the
    loss of fine details, which is not suitable for dense correspondence estimation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 学习特征的尺度由（1）输入补丁的大小，（2）网络的感受野，以及（3）每层中使用的卷积滤波器和池化操作的核大小定义。虽然增加核大小可以捕捉图像像素之间更多的全局交互，但这会导致高计算成本。此外，传统的池化，如在[[39](#bib.bib39)，[42](#bib.bib42)]中使用的，会降低分辨率并可能导致细节丢失，这不适合密集对应估计。
- en: To enlarge the receptive field without losing resolution or increasing the computation
    time, some techniques, *e.g.,*  [[52](#bib.bib52)], use dilated convolutions,
    *i.e.,* large convolutional filters but with holes and thus they are computationally
    efficient. Other techniques, *e.g.,*  [[48](#bib.bib48), [49](#bib.bib49)], use
    spatial pyramid pooling (SPP) modules placed at different locations in the network,
    see Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣
    4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(c-e). For instance, Park *et al.* [[48](#bib.bib48)], who
    introduced FW-CNN for stereo matching, append an SPP module at the end of the
    decision network, see Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction
    and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")-(d). As a result, the receptive field can
    be enlarged. However, for each pixel in the reference image, both the fully-connected
    layers and the pooling operations need to be computed $n_{d}$ times where $n_{d}$
    is the number of disparity levels. To avoid this, Ye *et al.* [[49](#bib.bib49)]
    place the SPP module at the end of each feature computation branch, see Figs. [2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(c)
    and (e). In this way, it is only computed once for each patch. Also, Ye *et al.* [[49](#bib.bib49)]
    employ multiple one-stride poolings, with different window sizes, to different
    layers and then concatenate their outputs to generate the feature maps, see Fig. [2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(e).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不降低分辨率或增加计算时间的情况下扩大感受野，一些技术，例如，[[52](#bib.bib52)]，使用扩张卷积，即大型卷积滤波器但带有孔，因此计算效率高。其他技术，例如，[[48](#bib.bib48)，[49](#bib.bib49)]，使用空间金字塔池化（SPP）模块，放置在网络中的不同位置，见图[2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(c-e)。例如，Park
    *等人* [[48](#bib.bib48)]，介绍了用于立体匹配的FW-CNN，在决策网络的末尾附加了一个SPP模块，见图[2](#S4.F2 "Figure
    2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(d)。结果，感受野可以扩大。然而，对于参考图像中的每个像素，完全连接层和池化操作需要计算$n_{d}$次，其中$n_{d}$是视差级别的数量。为了避免这种情况，Ye
    *等人* [[49](#bib.bib49)]将SPP模块放在每个特征计算分支的末尾，见图[2](#S4.F2 "Figure 2 ‣ 4.1 Learning
    feature extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")-(c)和(e)。这样，每个补丁只需计算一次。此外，Ye
    *等人* [[49](#bib.bib49)]采用多个单步池化，使用不同的窗口大小，作用于不同层，然后将它们的输出拼接以生成特征图，见图[2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(e)。
- en: 4.1.2.3 Learning multiscale features
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.3 学习多尺度特征
- en: '| ![Refer to caption](img/7f1b26a76303eb335624b4e0b061cd5e.png) | ![Refer to
    caption](img/1aa809e7b9418ebdaeae7a96afe38a2d.png) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/7f1b26a76303eb335624b4e0b061cd5e.png) | ![参见说明](img/1aa809e7b9418ebdaeae7a96afe38a2d.png)
    |'
- en: '| (a) Center-surround [[37](#bib.bib37)] | (b) Voting-based [[40](#bib.bib40)].
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| (a) 中心-环绕[[37](#bib.bib37)] | (b) 基于投票[[40](#bib.bib40)]。 |'
- en: 'Figure 3: Multiscale feature learning architectures.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '图3: 多尺度特征学习架构。'
- en: The methods described so far can be extended to learn features at multiple scales
    by using multi-stream networks, one stream per patch size [[37](#bib.bib37), [40](#bib.bib40)],
    see Fig. [3](#S4.F3 "Figure 3 ‣ 4.1.2.3 Learning multiscale features ‣ 4.1.2 Network
    architecture variants ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation"). Zagoruyko and Komodakis [[37](#bib.bib37)] propose a two-stream
    network, which is essentially a network composed of two siamese networks combined
    at the output by a top network, see Fig. [3](#S4.F3 "Figure 3 ‣ 4.1.2.3 Learning
    multiscale features ‣ 4.1.2 Network architecture variants ‣ 4.1 Learning feature
    extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(a). The first siamese network,
    called central high-resolution stream, receives as input two $32\times 32$ patches
    centered around the pixel of interest. The second network, called surround low-resolution
    stream, receives as input two $64\times 64$ patches but down-sampled to $32\times
    32$. The output of the two streams are then concatenated and fed to a top decision
    network, which returns a matching score. Chen *et al.* [[40](#bib.bib40)] use
    a similar approach but instead of aggregating the features computed by the two
    streams prior to feeding them to the top decision network, it appends a top network
    on each stream to produce a matching score. The two scores are then aggregated
    by voting, see Fig. [3](#S4.F3 "Figure 3 ‣ 4.1.2.3 Learning multiscale features
    ‣ 4.1.2 Network architecture variants ‣ 4.1 Learning feature extraction and matching
    ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(b).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 目前描述的方法可以通过使用多流网络扩展到在多个尺度上学习特征，每个补丁大小一个流[[37](#bib.bib37), [40](#bib.bib40)]，见图[3](#S4.F3
    "Figure 3 ‣ 4.1.2.3 Learning multiscale features ‣ 4.1.2 Network architecture
    variants ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")。Zagoruyko和Komodakis[[37](#bib.bib37)]提出了一种双流网络，本质上是由两个西阿摩斯网络组成，通过一个顶层网络在输出处结合，见图[3](#S4.F3
    "Figure 3 ‣ 4.1.2.3 Learning multiscale features ‣ 4.1.2 Network architecture
    variants ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(a)。第一个西阿摩斯网络，称为中央高分辨率流，接收两个$32\times
    32$的补丁，中心围绕感兴趣的像素。第二个网络，称为周围低分辨率流，接收两个$64\times 64$的补丁，但下采样到$32\times 32$。这两个流的输出然后被连接并送入一个顶层决策网络，该网络返回一个匹配分数。Chen
    *et al.* [[40](#bib.bib40)]使用了类似的方法，但不是在将两个流计算的特征送入顶层决策网络之前进行聚合，而是在每个流上附加一个顶层网络以产生匹配分数。然后通过投票聚合这两个分数，见图[3](#S4.F3
    "Figure 3 ‣ 4.1.2.3 Learning multiscale features ‣ 4.1.2 Network architecture
    variants ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(b)。
- en: The main advantage of the multi-stream architecture is that it can compute features
    at multiple scales in a single forward pass. It, however, requires one stream
    per scale, which is not practical if more than two scales are needed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 多流架构的主要优点是它可以在一次前向传递中计算多个尺度的特征。然而，它需要每个尺度一个流，如果需要多个尺度，这种方法就不太实际。
- en: 4.1.2.4 Reducing the number of forward passes
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.4 减少前向传递的次数
- en: Using the approaches described so far, inferring the raw cost volume from a
    pair of stereo images is performed using a moving window-like approach, which
    would require multiple forward passes, $n_{d}$ forward passes per pixel where
    $n_{d}$ is the number of disparity levels. However, since correlations are highly
    parallelizable, the number of forward passes can be significantly reduced. For
    instance, Luo *et al.* [[44](#bib.bib44)] reduce the number of forward passes
    to one pass per pixel by using a siamese network, whose first branch takes a patch
    around a pixel while the second branch takes a larger patch that expands over
    all possible disparities. The output is a single 64D representation for the left
    branch, and $n_{d}\times 64$ for the right branch. A correlation layer then computes
    a vector of length $n_{d}$, where its $0pt-$th element is the cost of matching
    the pixel $x$ on the left image with the pixel $x-0pt$ on the rectified right
    image.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迄今为止描述的方法，从一对立体图像推断原始成本体积是通过类似移动窗口的方法进行的，这需要每个像素进行多次前向传播，每个像素需要 $n_{d}$ 次前向传播，其中
    $n_{d}$ 是视差水平的数量。然而，由于相关性高度并行化，前向传播的次数可以显著减少。例如，Luo *et al.* [[44](#bib.bib44)]
    通过使用一个西格玛网络，将前向传播次数减少到每个像素一次，该网络的第一个分支处理像素周围的一个补丁，而第二个分支处理一个扩展到所有可能视差的大补丁。输出是左分支的一个
    64D 表示，以及右分支的 $n_{d}\times 64$。然后，相关层计算长度为 $n_{d}$ 的向量，其中其 $0pt-$th 元素是将左图像上的像素
    $x$ 与校正右图像上的像素 $x-0pt$ 匹配的成本。
- en: Zagoruyko and Komodakis [[37](#bib.bib37)] showed that the outputs of the two
    feature extraction sub-networks need to be computed only once per pixel, and do
    not need to be recomputed for every disparity under consideration. This can be
    done in a single forward pass, for the entire image, by propagating full-resolution
    images instead of small patches. Also, the output of the top network composed
    of fully-connected layers in the accurate architecture (*i.e.,* MC-CNN-Accr) can
    be computed in a single forward pass by replacing the fully-connected layers with
    convolutional layers of $1\times 1$ kernels. However, it still requires one forward
    pass for each disparity under consideration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Zagoruyko 和 Komodakis [[37](#bib.bib37)] 表明，两种特征提取子网络的输出只需每个像素计算一次，不需要对每个视差进行重新计算。这可以通过传播全分辨率图像而不是小补丁在整个图像中进行单次前向传播来完成。此外，由完全连接层组成的顶层网络（即
    MC-CNN-Accr）的输出可以通过用 $1\times 1$ 卷积层替代完全连接层来在单次前向传播中计算。然而，它仍然需要对每个考虑中的视差进行一次前向传播。
- en: 4.1.2.5 Learning similarity without feature learning
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.5 无需特征学习的相似性学习
- en: Joint training of feature extraction and similarity computation networks unifies
    the feature learning and the metric learning steps. Zagoruyko and Komodakis [[37](#bib.bib37)]
    propose another architecture that does not have a direct notion of features, see
    Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(f). In this architecture, the left and right patches are packed
    together and fed jointly into a two-channel network composed of convolution and
    ReLU layers followed by a set of fully connected layers. Instead of computing
    features, the network directly outputs the similarity between the input pair of
    patches. Zagoruyko and Komodakis [[37](#bib.bib37)] showed that this architecture
    is easy to train. However, it is expensive at runtime since the whole network
    needs to be run $n_{d}$ times per pixel.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取和相似性计算网络的联合训练将特征学习和度量学习步骤统一起来。Zagoruyko 和 Komodakis [[37](#bib.bib37)] 提出了另一种架构，该架构没有直接的特征概念，见图
    [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(f)。在这种架构中，左侧和右侧的补丁被一起打包，并共同输入到由卷积和 ReLU 层组成的双通道网络中，之后是完全连接层。网络直接输出输入补丁对之间的相似性，而不是计算特征。Zagoruyko
    和 Komodakis [[37](#bib.bib37)] 表明这种架构易于训练。然而，它在运行时成本较高，因为整个网络需要每个像素运行 $n_{d}$
    次。
- en: 4.1.3 Training procedures
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 训练程序
- en: The networks described in this section are composed of a feature extraction
    block and a feature matching block. Since the goal is to learn how to match patches,
    these two modules are jointly trained either in a supervised (Section [4.1.3.1](#S4.SS1.SSS3.P1
    "4.1.3.1 Supervised training ‣ 4.1.3 Training procedures ‣ 4.1 Learning feature
    extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")) or in a weakly supervised manner
    (Section [4.1.3.2](#S4.SS1.SSS3.P2 "4.1.3.2 Weakly supervised learning ‣ 4.1.3
    Training procedures ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by
    stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3.1 Supervised training
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Existing methods for supervised training use a training set composed of positive
    and negative examples. Each positive (respectively negative) example is a pair
    composed of a reference patch and its matching patch (respectively a non-matching
    one) from another image. Training either takes one example at a time, positive
    or negative, and adapts the similarity [[40](#bib.bib40), [41](#bib.bib41), [38](#bib.bib38),
    [37](#bib.bib37)], or takes at each step both a positive and a negative example,
    and maximizes the difference between the similarities, hence aiming at making
    the two patches from the positive pair more similar than the two patches from
    the negative pair [[43](#bib.bib43), [45](#bib.bib45), [39](#bib.bib39)]. This
    latter scheme is known as Triplet Contrastive learning.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Zbontar *et al.* [[39](#bib.bib39), [42](#bib.bib42)] use the ground-truth disparities
    of the KITTI2012 [[15](#bib.bib15)] or Middlebury [[20](#bib.bib20)] datasets.
    For each known disparity, the method extracts one negative pair and one positive
    pair as training examples. As such, the method is able to extract more than $25$
    million training samples from KITTI2012 [[15](#bib.bib15)] and more than $38$
    million from the Middlebury dataset [[20](#bib.bib20)]. This method has been also
    used by Chen *et al.* [[40](#bib.bib40)], Zagoruyku and Komodakis [[37](#bib.bib37)],
    and Han *et al.* [[38](#bib.bib38)]. The amount of training data can be further
    augmented by using data augmentation techniques, *e.g.,* flipping patches and
    rotating them in various directions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Although the supervised learning works very well, the complexity of the neural
    network models requires very large labeled training sets, which are hard or costly
    to collect for real applications (*e.g.,* consider the stereo reconstruction of
    the Mars landscape). Even when such large sets are available, the ground truth
    is usually produced from depth sensors and often contains noise that reduces the
    effectiveness of the supervised learning [[53](#bib.bib53)]. This can be mitigated
    by augmenting the training set with random perturbations [[39](#bib.bib39)] or
    synthetic data [[54](#bib.bib54), [22](#bib.bib22)]. However, synthesis procedures
    are hand-crafted and do not account for the regularities specific to the stereo
    system and target scene at hand.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions. Supervised stereo matching networks are trained to minimize
    a matching loss, which is a function that measures the discrepancy between the
    ground-truth and the predicted matching scores for each training sample. It can
    be defined using (1) the $L_{1}$ distance [[42](#bib.bib42), [40](#bib.bib40),
    [46](#bib.bib46)], (2) the hinge loss [[42](#bib.bib42), [46](#bib.bib46)], or
    (3) the cross-entropy loss [[44](#bib.bib44)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3.2 Weakly supervised learning
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Weakly supervised techniques exploit one or more stereo constraints to reduce
    the amount of manual labelling. Tulyakov *et al.* [[50](#bib.bib50)] consider
    Multi-Instance Learning (MIL) in conjunction with stereo constraints and coarse
    information about the scene to train stereo matching networks with datasets for
    which ground truth is not available. Unlike supervised techniques, which require
    pairs of matching and non-matching patches, the training set is composed of $N$
    triplets. Each triplet is composed of: (1) $W$ reference patches extracted on
    a horizontal line of the reference image, (2) $W$ positive patches extracted from
    the corresponding horizontal line on the right image, and (3) $W$ negative patches,
    *i.e.,* patches that do not match the reference patches, extracted from another
    horizontal line on the right image. As such, the training set can automatically
    be constructed from stereo pairs without manual labelling.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'The method is then trained by exploiting five constraints: the epipolar constraint,
    the disparity range constraint, the uniqueness constraint, the continuity (smoothness)
    constraint, and the ordering constraint. They then define three losses that use
    different subsets of these constraints, namely:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Multi Instance Learning (MIL) loss, which uses the epipolar and the disparity
    range constraints. From these two constraints, we know that every non-occluded
    reference patch has a matching positive patch in a known index interval, but does
    not have a matching negative patch. Therefore, for every reference patch, the
    similarity of the best reference-positive match should be greater than the similarity
    of the best reference-negative match.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The constractive loss, which adds to the MIL method the uniqueness constraint.
    It tells us that the matching positive patch is unique. Thus, for every patch,
    the similarity of the best match should be greater than the similarity of the
    second best match.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The constractive-DP uses all the constraints but finds the best match using
    dynamic programming.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The method has been applied to train a deep siamese neural-network that takes
    two patches as an input and predicts a similarity measure. Benchmarking on standard
    datasets shows that the performance is as good or better than the published results
    on MC-CNN-fst [[39](#bib.bib39)], which uses the same network architecture but
    trained using fully labeled data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Regularization and disparity estimation
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the raw cost volume is estimated, one can estimate the disparity by dropping
    the regularization term of Eqn. ([1](#S4.E1 "In 4 Depth by stereo matching ‣ A
    Survey on Deep Learning Techniques for Stereo-based Depth Estimation")), or equivalently
    block C of Fig. [1](#S4.F1 "Figure 1 ‣ 4 Depth by stereo matching ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation"), and taking the argmin,
    the softargmin, or the subpixel MAP approximation (block D of Fig. [1](#S4.F1
    "Figure 1 ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")). However, the raw cost volume computed from
    image features could be noise-contaminated, *e.g.,* due to the existence of non-Lambertian
    surfaces, object occlusions, or repetitive patterns. Thus, the estimated depth
    maps can be noisy. As such, some methods overcome this problem by using traditional
    MRF-based stereo framework for cost volume regularization [[40](#bib.bib40), [39](#bib.bib39),
    [44](#bib.bib44)]. In these methods, the initial cost volume $C$ is fed to a global [[11](#bib.bib11)]
    or a semi-global [[55](#bib.bib55)] matcher to compute the disparity map. Semi-global
    matching provides a good tradeoff between accuracy and computation requirements.
    In this method, the smoothness term of Eqn. ([1](#S4.E1 "In 4 Depth by stereo
    matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation"))
    is defined as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{E_{s}(0pt_{x},0pt_{y})=\alpha_{1}\delta(&#124;0pt_{xy}=1)+\alpha_{2}\delta(&#124;0pt_{xy}>1),}$
    |  | (2) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: 'where $0pt_{xy}=0pt_{x}-0pt_{y}$, $\alpha_{1}$ and $\alpha_{2}$ are positive
    weights chosen such that $\alpha_{2}>\alpha_{1}$, and $\delta$ is the Kronecker
    delta function, which gives $1$ when the condition in the bracket is satisfied,
    otherwise $0$. To solve this optimisation problem, the SGM energy is broken down
    into multiple energies $E_{s}$, each one defined along a path $s$. The energies
    are minimised independently and then aggregated. The disparity at $x$ is computed
    using the winner-takes-all strategy of the aggregated costs of all directions:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d_{x}=\arg\min_{d}\sum_{s}E_{s}(x,d).$ |  | (3) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: This method requires setting the two parameters $\alpha_{1}$ and $\alpha_{2}$
    of Eqn. ([2](#S4.E2 "In 4.2 Regularization and disparity estimation ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")). Instead of manually setting them, Seki *et al.* [[56](#bib.bib56)]
    proposed SGM-Net, a neural network trained to provide these parameters at each
    image pixel. They obtained better penalties than hand-tuned methods as in [[39](#bib.bib39)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'The SGM method, which uses an aggregated scheme to combine costs from multiple
    1D scanline optimizations, suffers from two major issues: (1) streaking artifacts
    caused by the scanline optimization approach, at the core of this algorithm, may
    lead to inaccurate results, and (2) the high memory footprint that may become
    prohibitive with high resolution images or devices with constrained resources.
    As such Schonberger *et al.* [[57](#bib.bib57)] reformulate the fusion step as
    the task of selecting the best amongst all the scanline optimization proposals
    at each pixel in the image. They solve this task using a per-pixel random forest
    classifier.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Poggi *et al.* [[58](#bib.bib58)] learn a weighted aggregation where the weight
    of each 1D scanline optimisation is defined using a confidence map computed using
    either traditional techniques [[59](#bib.bib59)] or deep neural networks, see
    Section [5.5](#S5.SS5 "5.5 Learning confidence maps ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation").
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 5 End-to-end depth from stereo
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/435c5cc5241b4fa1af61701caf0f8fba.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Taxonomy of the network architectures for stereo-based disparity
    estimation using end-to-end deep learning.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Recent works solve the stereo matching problem using a pipeline that is trained
    end-to-end. Two main classes of methods have been proposed. Early methods, *e.g.,*
    FlowNetSimple [[51](#bib.bib51)] and DispNetS [[22](#bib.bib22)], use a single
    encoder-decoder, which stacks together the left and right images into a 6D volume,
    and regresses the disparity map. These methods, which do not require an explicit
    feature matching module, are fast at runtime. They, however, require a large amount
    of training data, which is hard to obtain. Methods in the second class mimic the
    traditional stereo matching pipeline by breaking the problem into stages, each
    stage is composed of differentiable blocks and thus allowing end-to-end training.
    Below, we review in details these techniques. Fig. [4](#S5.F4 "Figure 4 ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation") provides a taxonomy of the state-of-the-art, while Table [III](#S5.T3
    "TABLE III ‣ 5.1 Feature learning ‣ 5 End-to-end depth from stereo ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation") compares $28$
    key methods based on this taxonomy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Feature learning
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Feature learning networks follow the same architectures as the ones described
    in Figs. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣
    4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") and [3](#S4.F3 "Figure 3 ‣ 4.1.2.3 Learning multiscale features
    ‣ 4.1.2 Network architecture variants ‣ 4.1 Learning feature extraction and matching
    ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation"). However, instead of processing individual patches, the entire
    images are processed in a single forward pass producing feature maps of the same
    or lower resolution as the input images. Two strategies have been used to enable
    matching features across the images:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: (1) Multi-branch networks composed of $n$ branches where $n$ is the number of
    input images. Each branch produces a feature map that characterizes its input
    image [[22](#bib.bib22), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)]. These techniques assume
    that the input images have been rectified so that the search for correspodnences
    is restricted to be along the horizontal scanlines.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: (2) Multi-branch networks composed of $n_{d}$ branches where $n_{d}$ is the
    number of disparity levels. The $d$-th branch, $1\leq d\leq n_{d}$, processes
    a stack of two images, as in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature
    extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(f); the first image is the reference
    image. The second one is the right image but re-projected to the $d$-th depth
    plane [[66](#bib.bib66)]. Each branch produces a similarity feature map that characterizes
    the similarity between the reference image and the right image re-projected onto
    a given depth plane. While these techniques do not rectify the images, they assume
    that the intrinsic and extrinsic camera parameters are known. Also, the number
    of disparity levels cannot be varied without updating the network architecture
    and retraining it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: In both methods, the feature extraction module uses either fully convolutional
    (ConvNet) networks such as VGG, or residual networks such as ResNets [[67](#bib.bib67)].
    The latter facilitates the training of very deep networks [[68](#bib.bib68)].
    They can also capture and incorporate more global context in the unary features
    by using either dilated convolutions (Section [4.1.2.2](#S4.SS1.SSS2.P2 "4.1.2.2
    Enlarging the receptive field of the network ‣ 4.1.2 Network architecture variants
    ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching ‣
    A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")) or multi-scale
    approaches. For instance, the PSM-Net of Chang and Chen [[64](#bib.bib64)] append
    a Spatial Pyramid Pooling (SPP) module in order to extract and aggregate features
    at multiple scales. Nie *et al.* [[65](#bib.bib65)] extended PSM-Net using a multi-level
    context aggregation pattern termed *Multi-Level Context Ultra-Aggregation (MLCUA)*.
    It encapsulates all convolutional features into a more discriminative representation
    by intra and inter-level features combination. It combines the features at the
    shallowest, smallest scale with features at deeper, larger scales using just shallow
    skip connections. This results in an improved performance, compared to PSM-Net [[64](#bib.bib64)],
    without significantly increasing the number of parameters in the network.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Taxonomy and comparison of $28$ end-to-end deep learning-based disparity
    estimation techniques. ”FCN”: Fully-Connected Network, ”SPN”: Spatial Propagation
    Network. ”LRCR”: Left-Right Comparative Recurrent model, ”MCUA”: Multi-Level Context
    Ultra-Aggregation for Stereo Matching. ”DLA”: Deep layer aggregation [[69](#bib.bib69)],
    ”VPP”: Volumetric Pyramid Pooling. The performance is measured on KITTI2015 test
    dataset.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year |  | Feature computation |  | Cost volume |  | Disparity |  |
    Refinement/post processing |  | Supervision | Performance |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '|  | Architecture | Dimension |  | Type | Construction | Regularization |  |  |
    Spatial/depth resolution | Completion/denoising |  | D1-all/est | D1-all/fg |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| FlowNetCorr [[51](#bib.bib51)] | 2015 |  | ConvNet | Single scale |  | 3D
    | Correlation | 2D ConvNet |  |  |  | Up-convolutions | Ad-hoc, variational |  |
    Supervised | $-$ | $-$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| DispNetC [[22](#bib.bib22)] | 2016 |  | ConvNet | Single scale |  | 3D |
    Correlation | 2D ConvNet |  | $-$ |  | $-$ | $-$ |  | Supervised | $4.34$ | $4.32$
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| Zhong *et al.* [[70](#bib.bib70)] | 2017 |  | ConvNet | Single scale |  |
    4D | Interleaved | 3D Conv, |  | Soft argmin |  | Self-improvement at runtime
    |  | Self-supervised | $3.57$ | $7.12$ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | with skip conn. |  |  |  | feature concat. | encoder-decoder |  |  |  |  |  |  |  |  |  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| Kendall *et al.* [[61](#bib.bib61)] | 2017 |  | ConvNet | Single scale |  |
    4D | Feature concat. | 3D Conv, encoder- |  | Soft argmax |  | $-$ | $-$ |  |
    Supervised | $2.87$ | $6.16$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | with skip conn. |  |  |  |  | decoder, hierarchical |  |  |  |  |  |  |  |  |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| Pang *et al.* [[62](#bib.bib62)] | 2017 |  | ConvNet | Single scale |  |
    3D | Correlation | 2D ConvNet |  |  |  | Upsampling$+$ residual learning |  |  |
    Supervised | $2.67$ | $3.59$ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| Knobelreiter *et al.* [[71](#bib.bib71)] | 2017 |  | ConvNet | Single scale
    |  | 3D | Correlation | Hybrid CNN-CRF |  |  |  | No post-processing |  | Supervised
    |  |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| Chang *et al.* [[64](#bib.bib64)] | 2018 |  | SPP | Multiscale |  | 4D |
    Feature concat. | 3D Conv, Stacked encoder-decoders |  | Soft argmin |  | Progressive
    refinement |  |  | Supervised | $2.32$ | $4.62$ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| Khamis *et al.* [[72](#bib.bib72)] | 2018 |  | ResNet | Single scale |  |
    3D | $L_{2}$ | 3D ConvNet |  | Soft argmin |  | Hierarchical, Upsamling$+$residual
    learning |  | Supervised | $4.83$ | $7.45$ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| Liang *et al.* [[63](#bib.bib63)] | 2018 |  | ConvNet | Multiscale |  | 3D
    | Correlation | 2D ConvNet |  | Encoder-decoder |  | Iterative upsampling$+$residual
    learning |  | Supervised | $2.67$ | $3.59$ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| Yang *et al.* [[68](#bib.bib68)] | 2018 |  | Shallow | Single scale |  |
    3D | Correlation, Left features, segmentation mask |  | Regression with |  | $-$
    | $-$ |  | Self-supervised | $2.25$ | $4.07$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '|  | ResNet |  |  | Encoder-decoder |  |  |  |  |  |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[73](#bib.bib73)] | 2018 |  | ConvNet | Single scale |  |
    3D | Hand-crafted | NA |  | Soft argmin |  | Upsampling$+$residual learning |  |
    Self-supervised | $-$ | $-$ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| Jie *et al.* [[74](#bib.bib74)] | 2018 |  | Constant highway net | Single
    scale |  | 3D | FCN | $-$ |  | RNN-based LRCR |  | $-$ | $-$ |  | Supervised |
    $3.03$ | $5.42$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| Ilg *et al.* [[75](#bib.bib75)] | 2018 |  | ConvNet | Single scale |  | 3D
    | Correlation | Encoder-decoder, joint disparity and occlusion |  | Cascade of
    encoder-decoders, residual learning |  | Supervised | $2.19$ | $-$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| Song *et al.* [[76](#bib.bib76)] | 2018 |  | SHallow ConvNet | Single scale
    |  | 3D | Correlation | Edge-guided, Context Pyramid, |  | Residual pyramid |  |
    $-$ | $-$ |  | Supervised | $2.59$ | $4.18$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  | Encoder |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| Yu *et al.* [[77](#bib.bib77)] | 2018 |  | ResNet | Single scale |  | 3D
    | Feature concatenation | 3D Conv $+$ SGM |  | Soft argmin |  | $-$ | $-$ |  |
    Supervised | $2.79$ | $5.46$ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  | Encoder-decoder |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| Tulyakov *et al.* [[78](#bib.bib78)] | 2018 |  | $-$ | Single scale |  |
    4D | Compressed | 3D Conv |  | Multimodal - |  | $-$ | $-$ |  | Supervised | $2.58$
    | $4.05$ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  | matching features |  |  | Sub-pixel MAP |  |  |  |  |  |  |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| EMCUA *et al.* [[65](#bib.bib65)] | 2019 |  | SPP | Multiscale |  | 4D |
    Feature concat. | 3D Conv, MCUA scheme |  | Arg softmin |  | $-$ | $-$ |  | Supervised
    | $2.09$ | $4.27$ |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| Yang *et al.* [[32](#bib.bib32)] | 2019 |  | SPP | Multiscale |  | Pyramidal
    4D | Feature difference | Conv3D blocks $+$ Volume Pyramid Pooling |  | Arg softmax
    |  | Spatial & depth res. by cost volume upsampling |  | Supervised | $2.14$ |
    $3.85$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| Wu *et al.* [[79](#bib.bib79)] | 2019 |  | ResNet50, SPP | Multiscale |  |
    Pyramidal 4D | Feature concat. | Encoder-decoder |  | 3D Conv, soft argmin |  |
    $-$ | $-$ |  | Supervised | $2.11$ | $3.89$ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  | $+$ Feature fusion |  |  |  |  |  |  | Disparity
    & boundary loss |  |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| Yin *et al.* [[80](#bib.bib80)] | 2019 |  | DLA net | Multiscale |  | 3D
    | Correlation | Density decoder |  | Outputs discrete |  | $-$ | $-$ |  | Supervised
    | 2.02 | $3.63$ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  |  |  | matching distribution |  |  |  |  |  |  |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| Chabra *et al.* [[81](#bib.bib81)] | 2019 |  | ConvNet $+$ | Multiscale |  |
    3D | $L_{1}$ | Dilated 3D ConvNet |  | Soft argmax |  | Upsampling$+$residual
    learning |  | Supervised | $2.26$ | $4.95$ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Vortex pooling [[82](#bib.bib82)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| Duggal *et al.* [[83](#bib.bib83)] | 2019 |  | ResNet, SPP | Multiscale |  |
    3D, sparse | Correlation, Adaptive | Encoder-decoder |  | Soft argmax |  | Encoder
    | $-$ |  | Supervised | $2.35$ | 3.43 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  | pruning with PatchMatch |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Tonioni *et al.* [[84](#bib.bib84)] | 2019 |  | ConvNet | Multiscale |  |
    3D | Correlation |  |  | Encoder |  | Recrusively upsampling $+$ residual learning
    |  | Online self-adaptive | $4.66$ | $-$ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| Yang *et al.* [[32](#bib.bib32)] | 2019 |  | ConvNet, SPP | Multiscale |  |
    Pyramid, 4D | Concatenation | Decoder, Residual blocs, |  | Conv3D block |  |
    $-$ | $-$ |  | Supervised | $2.14$ | $3.85$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  | VPP |  |  |  |  |  |  |  |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.* [[85](#bib.bib85)] | 2019 |  | Stacked hourglass | Single
    scale |  | 4D | Concatenation | Semi-global aggregation layers, |  | Soft argmax
    |  | $-$ | $-$ |  | Supervised | $2.03$ | $3.91$ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  | Local-guided aggregation layers |  |  |  |  |  |  |  |  |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| Guo *et al.* [[86](#bib.bib86)] | 2019 |  | SPP | Multiscale |  | Hybrid
    3D-4D | Group-wise correlation | Stacked hourglass nets |  | Soft argmin |  |
    $-$ | $-$ |  | Supervised | $2.11$ | $3.93$ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.* [[87](#bib.bib87)] | 2019 |  |  |  |  |  |  |  |  | Single-modal
    weighted avg |  |  |  |  | Supervised | $2.14$ | $4.33$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| Wang *et al.* [[88](#bib.bib88)] | 2019 |  | ConvNet | Multiresolution maps
    |  | 3D | $L_{1}$ | Progressive refinement |  | Soft argmin |  | Usampling, |
    Spatial propagation |  | Supervised | $-$ | $-$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  | (3D Conv) |  |  |  | residual learning | network
    |  |  |  |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: 5.2 Cost volume construction
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the features have been computed, the next step is to compute the matching
    scores, which will be fed, in the form of a cost volume, to a top network for
    regularization and disparity estimation. The cost volume can be three dimensional
    (3D) where the third dimension is the disparity level (Section [5.2.1](#S5.SS2.SSS1
    "5.2.1 3D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")),
    four dimensional (4D) where the third dimension is the feature dimension and the
    fourth one is the disparity level (Section [5.2.2](#S5.SS2.SSS2 "5.2.2 4D cost
    volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation")), or hybrid to
    benefit from the properties of the 3D and 4D cost volumes (Section [5.2.3](#S5.SS2.SSS3
    "5.2.3 Hybrid 3D-4D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")). In general, the cost volume is constructed at a lower resolution,
    *e.g.,* at $\nicefrac{{1}}{{8}}$-th, than the input [[72](#bib.bib72), [73](#bib.bib73)].
    It is then either subsequently upscaled and refined, or used as is to estimate
    a low resolution disparity map, which is then upscaled and refined using a refinement
    module.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 3D cost volumes
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 5.2.1.1 Construction
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A 3D cost volume can be simply built by taking the $L_{1}$, $L_{2}$, or correlation
    distance between the features of the left image and those of the right image that
    are within a pre-defined disparity range, see [[22](#bib.bib22), [73](#bib.bib73),
    [74](#bib.bib74), [72](#bib.bib72), [80](#bib.bib80), [81](#bib.bib81), [83](#bib.bib83)],
    and the FlowNetCorr of [[51](#bib.bib51)]. The advantage of correlation-based
    dissimilarities is that they can be implemented using a convolutional layer that
    does not require training (its filters are the features computed by the second
    branch of the network). Flow estimation networks such as FlowNetCorr [[51](#bib.bib51)]
    use 2D correlations. Disparity estimation networks, such as [[22](#bib.bib22),
    [68](#bib.bib68)], iResNet [[63](#bib.bib63)], DispNet3 [[75](#bib.bib75)], EdgeStereo [[76](#bib.bib76)],
    HD³ [[80](#bib.bib80)], and [[84](#bib.bib84), [83](#bib.bib83)], use 1D correlations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1.2 Regularization of 3D cost volumes
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once a cost volume is computed, an initial disparity map can be estimated using
    the argmin, the softargmin, or the subpixel MAP approximation over the depth dimension
    of the cost volume, see for example [[73](#bib.bib73)] and Fig. [5](#S5.F5 "Figure
    5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost
    volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(a). This is equivalent to dropping
    the regularization term of Eqn. ([1](#S4.E1 "In 4 Depth by stereo matching ‣ A
    Survey on Deep Learning Techniques for Stereo-based Depth Estimation")). In general,
    however, the raw cost volume is noise-contaminated (*e.g.,* due to the existence
    of non-Lambertian surfaces, object occlusions, and repetitive patterns). The goal
    of the regularization module is to leverage context along the spatial and/or disparity
    dimensions to refine the cost volume before estimating the initial disparity map.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: (1) Regularization using traditional methods. Early papers use traditional techniques,
    *e.g.,* Markov Random Fields (MRF), Conditional Random Fields (CRF), and Semi-Global
    Matching (SGM), to regularize the cost volume by explicitly incorporating spatial
    constraints, *e.g.,* smoothness, of the depth maps. Recent papers showed that
    deep learning networks can be used to fine-tune the parameters of these methods.
    For example, Knöbelreiter *et al.* [[71](#bib.bib71)] proposed a hybrid CNN-CRF.
    The CNN computes the matching term of Eqn. ([1](#S4.E1 "In 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")), which
    becomes the unary term of a CRF module. The pairwise term of the CRF is parameterized
    by edge weights computed using another CNN. The end-to-end trained CNN-CRF pipeline
    could achieve a competitive performance using much fewer parameters (thus a better
    utilization of the training data) than the earlier methods.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Zheng *et al.* [[89](#bib.bib89)] provide a way to model CRFs as recurrent neural
    networks (RNN) for segmentation tasks so that the entire pipeline can be trained
    end-to-end. Unlike segmentation, in depth estimation, the number of depth samples,
    whose counterparts are the semantic labels in segmentation tasks, is expected
    to vary for different scenarios. As such, Xue *et al.* [[90](#bib.bib90)] re-designed
    the RNN-formed CRF module so that the model parameters are independent of the
    number of depth samples. Paschalidou *et al.* [[91](#bib.bib91)] formulate the
    inference in a MRF as a differentiable function, hence allowing end-to-end training
    using back propagation. Note that Zheng *et al.* [[89](#bib.bib89)] and Paschalidou
    *et al.* [[91](#bib.bib91)] focus on multi-view stereo (Section [6](#S6 "6 Learning
    multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")). Their approaches, however, are generic and can be used to regularize
    3D cost volumes obtained using pairwise stereo networks.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0bbcfef77d5c62c3068a656a183106f2.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Cost volume regularization schemes [[92](#bib.bib92)]: (a) does not
    consider context, (b) captures context along the spatial dimensions using 2D convolutions,
    (c) captures context along the spatial and disparity dimensions by recurrent regularization
    using 2D convolutions, and (d) captures context in all dimensions by using 3D
    convolutions.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: (2) Regularization using 2D convolutions (2DConvNet), Figs. [5](#S5.F5 "Figure
    5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost
    volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(b) and (c). Another approach is
    to process the 3D cost volume using a series of 2D convolutional layers producing
    another 3D cost volume [[22](#bib.bib22), [51](#bib.bib51), [62](#bib.bib62),
    [63](#bib.bib63)]. 2D convolutions are computationally efficient. However, they
    only capture and aggregate context along the spatial dimensions, see Fig. [5](#S5.F5
    "Figure 5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes
    ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")-(b), and ignore context
    along the disparity dimension. Yao *et al.* [[92](#bib.bib92)] sequentially regularize
    the 2D cost maps along the depth direction via a Gated Recurrent Unit (GRU), see
    Fig. [5](#S5.F5 "Figure 5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1
    3D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(c).
    This reduces drastically the memory consumption, *e.g.,* from $15.4$GB in [[93](#bib.bib93)]
    to around $5$GB, making high-resolution reconstruction feasible, while capturing
    context along both the spatial and the disparity dimensions.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: (3) Regularization using 3D convolutions (3DConvNet), Fig. [5](#S5.F5 "Figure
    5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost
    volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(d). Khamis *et al.* [[72](#bib.bib72)]
    use the $L_{2}$ distance to compute an initial 3D cost volume and 3D convolutions
    to regularize it across both the spatial and disparity dimensions, see Fig. [5](#S5.F5
    "Figure 5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes
    ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")-(d). Due to its memory
    requirements, the approach first estimates a low-resolution disparity map, which
    is then progressively improved using residual learning. Zhang *et al.* [[73](#bib.bib73)]
    follow the same approach but the refinement block starts with separate convolution
    layers running on the upsampled disparity and input image respectively, and merge
    the features later to produce the residual. Chabra *et al.* [[81](#bib.bib81)]
    observe that the cost volume regularization step is the one that uses most of
    the computational resources. They then propose a regularization module that uses
    3D dilated convolutions in the width, height, and disparity dimesions, to reduce
    the computation time while capturing a wider context.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 4D cost volumes
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 5.2.2.1 Construction
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4D cost volumes to preserve the dimension of the features [[70](#bib.bib70),
    [61](#bib.bib61), [64](#bib.bib64), [65](#bib.bib65), [32](#bib.bib32), [79](#bib.bib79)].
    The rational behind 4D cost volumes is to let the top network learn the appropriate
    similarity measure for comparing the features instead of using hand-crafted ones
    as in Section [5.2.1](#S5.SS2.SSS1 "5.2.1 3D cost volumes ‣ 5.2 Cost volume construction
    ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation").
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 4D cost volumes can be constructed by feature differences across a pre-defined
    disparity range [[32](#bib.bib32)], which results in cost volume of size $0pt\times
    0pt\times 2n_{d}\times c$, or by concatenating the features computed by the different
    branches of the network [[61](#bib.bib61), [70](#bib.bib70), [64](#bib.bib64),
    [65](#bib.bib65), [79](#bib.bib79)]. Using this method, Kendall *et al.* [[61](#bib.bib61)]
    build a 4D volume of size $0pt\times 0pt\times(n_{d}+1)\times c$ ($c$ here is
    the dimension of the features). Zhong *et al.* [[70](#bib.bib70)] follow the same
    approach but concatenate the features in an interleaved manner. That is, if $\textbf{f}_{L}$
    is the feature map of the left image and $\textbf{f}_{R}$ the feature map of the
    right image, then the final feature volume is assembled in such a way that its
    $2i-$th slice holds the left feature map while the $(2i+1)-$th slice holds the
    right feature map but at disparity $d=i$. This results in a 4D cost volume that
    is twice larger than the cost volume of Kendall *et al.* [[61](#bib.bib61)]. To
    capture multi-scale context in the cost volume, Chang and Chen [[64](#bib.bib64)]
    generate for each input image a pyramid of features, upsamples them to the same
    dimension, and then builds a single 4D cost volume by concatenation. Wu *et al.* [[79](#bib.bib79)]
    build from the multiscale features (four scales) multiscale 4D cost volumes.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 4D cost volumes carry richer information compared to 3D cost volumes. Note,
    however, that volumes obtained by concatenation contain no information about the
    feature similarities, so more parameters are required in the subsequent modules
    to learn the similarity function.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2.2 Regularization of 4D cost volumes
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4D cost volumes are regularized with 3D convolutions, which exploit the correlation
    in height, width and disparity dimensions, to produce a 3D cost volume. Kendall
    *et al.* [[61](#bib.bib61)] use a U-net encoder-decoder with 3D convolutions and
    skip connections. Zhong *et al.* [[70](#bib.bib70)] use a similar approach but
    add residual connections from the contracting to the expanding parts of the regularization
    network. To take into account a large context without a significant additional
    computational burden, Kendall *et al.* [[61](#bib.bib61)] regularize the cost
    volume hierarchically, with four levels of subsampling, allowing to explicitly
    leverage context with a wide field of view. Muliscale 4D cost volumes [[79](#bib.bib79)]
    are aggregated into a single 3D cost volume using a 3D multi-cost aggregation
    module, which operates in a pairwise manner starting with the smallest volume.
    Each volume is processed with an encoder-decoder, upsampled to the next resolution
    in the pyramid, and then fused using a 3D feature fusion module.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, semi-global matching (SGM) techniques have been used to regularize the
    4D cost volume where their parameters are estimated using convolutional networks.
    In particular, Yu *et al.* [[77](#bib.bib77)] process the initial 4D cost volume
    with an encoder-decoder composed of 3D convolutions and upconvolutions, and produces
    another 3D cost volume. The subsequent aggregation step is performed using an
    end-to-end two-stream network: the first stream generates three cost aggregation
    proposals $C_{i}$, one along each of the tree dimensions, *i.e.,* the height,
    width, and disparity. The second stream is a guidance stream used to select the
    best proposals. It uses 2D convolutions to produce three guidance (confidence)
    maps $W_{i}$. The final 3D cost volume is produced as a weighted sum of the three
    proposals, *i.e.,* $\max_{i}(C_{i}*W_{i})$.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 3D convolutions are expensive in terms of memory requirements and computation
    time. As such, subsequent works that followed the seminal work of Kendall *et
    al.* [[61](#bib.bib61)] focused on (1) reducing the number of 3D convolutional
    layers [[85](#bib.bib85)], (2) progressively refining the cost volume and the
    disparity map [[64](#bib.bib64), [88](#bib.bib88)], and (3) compressing the 4D
    cost volume [[78](#bib.bib78)]. Below, we discuss these approaches.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: (1) Reducing the number of 3D convolutional layers. Zhang *et al.* [[85](#bib.bib85)]
    introduced GANet, which replaces a large number of the 3D convolutional layers
    in the regularization block with (1) two 3D convolutional layers, (2) a semi-global
    aggregation layer (SGA), and (3) a local guided aggregation layer (LGA). SGA is
    a differentiable approximation of the semi-global matching (SGM). Unlike SGM,
    in SGA the user-defined parameters are learnable. Moreover, they are added as
    penalty coefficients/weights of the matching cost terms. Thus, they are adaptive
    and more flexible at different locations for different situations. The LGA layer,
    on the other hand, is appended at the end and aims to refine the thin structures
    and object edges. The SGA and LGA layers, which are used to replace the costly
    3D convolutions, capture local and whole-image cost dependencies. They significantly
    improve the accuracy of the disparity estimation in challenging regions such as
    occlusions, large textureless/reflective regions, and thin structures.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: (2) Progressive approaches. Some techniques avoid directly regularizing high
    resolution 4D cost volumes using the expensive 3D convolutions. Instead, they
    operate in a progressive manner. For instance, Chang and Chen [[64](#bib.bib64)]
    introduced PSM-Net, which first estimates a low resolution 4D cost volume, and
    then regularizes it using stacked hourglass 3D encoder-decoder blocks. Each block
    returns a 3D cost volume, which is then upsampled and used to regress a high resolution
    disparity map using additional 3D convolutional layers followed by a softmax operator.
    As such, the stacked hourglass blocks can be seen as refinement modules.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Wang *et al.* [[88](#bib.bib88)] use a three-stage disparity estimation network,
    called AnyNet, which builds cost volumes in a coarse-to-fine manner. The first
    stage takes as input low resolution feature maps, builds a low resolution 4D cost
    volume and then uses 3D convolutions to estimate a low resolution disparity map
    by searching on a small disparity range. The prediction in the previous level
    is then upsampled and used to warp the input feature at the higher scale, with
    the same disparity estimation network used to estimate disparity residuals. The
    advantage is two-fold; first, at higher resolutions, the network only learns to
    predict residuals, which reduces the computation cost. Second, the approach is
    progressive and one can select to return the intermediate disparities, trading
    accuracy for speed.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: (3) 4D cost volume compression. Tulyakov *et al.* [[78](#bib.bib78)] reduce
    the memory usage, without having to sacrify accuracy, by compressing the features
    into compact matching signatures. As such, the memory footprint is significantly
    reduced. More importantly, it allows the network to handle an arbitrary number
    of multiview images and to vary the number of inputs at runtime without having
    to re-train the network.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Hybrid 3D-4D cost volumes
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The correlation layer provides an efficient way to measure feature similarities,
    but it loses much information because it produces only a single-channel map for
    each disparity level. On the other hand, 4D cost volumes obtained by feature concatenation
    carry more information but are resource-demanding. They also require more parameters
    in the subsequent aggregation network to learn the similarity function. To benefit
    from both, Guo *et al.* [[86](#bib.bib86)] propose a hybrid approach, which constructs
    two cost volumes; one by feature concatenation but compressed into $12$ channels
    using two convolutions. The second one is built by dividing the high-dimension
    feature maps into $N_{g}$ groups along the feature channel, computing correlations
    within each group at all disparity levels, and finally concatenating the correlation
    maps forming another 4D volume. The two volumes are then combined together and
    passed to a 3D regularization module composed of four 3D convolution layers followed
    by three stacked 3D hourglass networks. This approach results in a significant
    reduction of parameters compared to 4D cost volumes built by only feature concatenation,
    without losing too much information like full correlations.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Disparity computation
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The simplest way to estimate the disparity map from the regularized cost volume
    $C$ is by using the pixel-wise argmin, *i.e.,* $0pt_{x}=\arg\min_{0pt}C(x,0pt)$
    (or equivalently $\arg\max$ if the volume $C$ encodes the likelihood). However,
    the agrmin/argmax operator is unable to produce sub-pixel accuracy and cannot
    be trained with back-propagation due to its non-differentiability. Another approach
    is the differentiable soft argmin/max over disparity [[66](#bib.bib66), [61](#bib.bib61),
    [73](#bib.bib73), [72](#bib.bib72)]:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d^{*}=\frac{1}{\sum_{j=0}^{n_{d}}e^{-C(x,j)}}\sum_{d=0}^{n_{d}}d\times
    e^{-C(x,d)}.$ |  | (4) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: The soft argmin operator approximates the sub-pixel MAP solution when the distribution
    is unimodal and symmetric [[78](#bib.bib78)]. When this assumption is not fulfilled,
    the softargmin blends the modes and may produce a solution that is far from all
    the modes and may result in over smoothing. Chen *et al.* [[87](#bib.bib87)] observe
    that this is particularly the case at boundary pixels where the estimated disparities
    follow multimodal distributions. To address these issues, Chen *et al.* [[87](#bib.bib87)]
    only apply a weighted average operation on a window centered around the modal
    with the maximum probability, instead of using a full-band weighted average on
    the entire disparity range.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Tulyakov *et al.* [[78](#bib.bib78)] introduced the sub-pixel MAP approximation,
    which computes a weighted mean around the disparity with the maximum posterior
    probability as:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d^{*}=\sum_{d:&#124;\hat{d}-d&#124;\leq\delta}d\cdot\sigma(C(x,d)),$
    |  | (5) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: where $\delta$ is a meta parameter set to $4$ in [[78](#bib.bib78)], $\sigma(C(x,d))$
    is the probability of the pixel $x$ having a disparity $d$, and $\displaystyle\hat{d}=\arg\max_{d}C(x,d)$.
    The sub-pixel MAP is only used for inference. Tulyakov *et al.* [[78](#bib.bib78)]
    also showed that, unlike the softargmin/max, this approach allows changing the
    disparity range at runtime without re-training the network.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Variants
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pipeline described so far infers disparity maps that can be of low-resolution
    (along the width, height, and disparity dimensions), incomplete, noisy, missing
    fine details, and suffering from over-smoothing especially at object boundaries.
    As such, many variants have been introduced to (1) improve their resolution (Section [5.4.1](#S5.SS4.SSS1
    "5.4.1 Learning to infer high resolution disparity maps ‣ 5.4 Variants ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")), (2) improve the processing time, especially at runtime (Section [5.4.3](#S5.SS4.SSS3
    "5.4.3 Learning for realtime processing ‣ 5.4 Variants ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")),
    and (3) perform disparity completion and denoising (Section [5.4.2](#S5.SS4.SSS2
    "5.4.2 Learning for completion and denoising ‣ 5.4 Variants ‣ 5 End-to-end depth
    from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Learning to infer high resolution disparity maps
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Directly regressing high-resolution depth maps that contain fine details, *e.g.,*
    by adding further upconvolutional layers to upscale the cost volume, would require
    a large number of parameters and thus are computationally expensive and difficult
    to train. As such, state-of-the-art methods struggle to process high resolution
    imagery because of memory constraints or speed limitations. This has been addressed
    by using either bottom-up or top-down techniques.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Bottom-up techniques operate in a sliding window-like approach. They take small
    patches and estimate the refined disparity either for the entire patch or for
    the pixel at the center of the patch. Lee *et al.* [[94](#bib.bib94)] follow a
    split-and-merge approach. The input image is split into regions, and a depth is
    estimated for each region. The estimates are then merged using a fusion network,
    which operates in the Fourier domain so that depth maps with different cropping
    ratios can be handled. While both sliding window and split-and-merge approaches
    reduce memory requirements, they require multiple forward passes, and thus are
    not suitable for realtime applications. Also, these methods do not capture the
    global context, which can limit their performance.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Top-down techniques, on the other hand, operate on the disparity map estimates
    in a hierarchical manner. They first estimate a low-resolution disparity map and
    then upsample them to the desired resolution, *e.g.,* using bilinear upsampling,
    and further process them using residual learning to recover small details and
    thin structures [[72](#bib.bib72), [73](#bib.bib73), [81](#bib.bib81)]. This process
    can also be run progressively by cascading many of such refinement blocks, each
    block refines the estimate of the previous block [[62](#bib.bib62), [72](#bib.bib72)].
    Unlike upsampling cost volumes, refining disparity maps is computationally efficient
    since it only requires 2D convolutions. Existing methods mainly differ in the
    type of additional information that is appended to the upsampled disparity map
    for refinement. For instance:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khamis *et al.* [[72](#bib.bib72)] concatenate the upsampled disparity map with
    the original reference image.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang *et al.* [[63](#bib.bib63)] append to the initial disparity map the cost
    volume and the reconstruction error, defined as the difference between the left
    image and the right image but warped to the left image using the estimated disparity
    map.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chabra *et al.* [[81](#bib.bib81)] take the left image and the reconstruction
    error on one side, and the left disparity and the geometric error map, defined
    as the difference between the estimated left disparity and right disparity but
    warped onto the left view. These are independently filtered using one layer of
    convolutions followed by batch normalization. The results of the two streams are
    concatenated and then further processed using a series of convolutional layers
    to produce the refined disparity map.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These methods improve the spatial resolution but not the disparity resolution.
    To refine both the spatial and depth resolution, while operating on high resolution
    images, Yang *et al.* [[32](#bib.bib32)] propose to search for correspondences
    incrementally over a coarse-to-fine hierarchy. The approach constructs a pyramid
    of four 4D cost volumes, each with increasing spatial and depth resolutions. Each
    volume is filtered by six 3D convolution blocks, and further processed with a
    Volumetric Pyramid Pooling block, an extension of Spatial Pyramid Pooling to feature
    volumes, to generate features that capture sufficient global context for high
    resolution inputs. The output is then either (1) processed with another conv3D
    block to generate a 3D cost volume from which disparity can be directly regressed.
    This allows to report on-demand disparities computed from the current scale, or
    (2) tri-linearly-upsampled to a higher spatial and disparity resolution so that
    it can be fused with the next 4D volume in the pyramid. To minimise memory requirements,
    the approach uses striding along the disparity dimensions in the last and second
    last volumes of the pyramid. The network is trained end-to-end using a multi-scale
    loss. This hierarchical design also allows for anytime on-demand reports of disparity
    by capping intermediate coarse results, allowing accurate predictions for near-range
    structures with low latency (30ms).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: This approach shares some similarities with the approach of Kendall *et al.* [[61](#bib.bib61)],
    which constructs hierarchical 4D feature volumes and processes them from coarse
    to fine using 3D convolutions. Kendall *et al.*’s approach [[61](#bib.bib61)],
    however, has been used to leverage context with a wide field of view while Yang
    *et al.* [[32](#bib.bib32)] apply coarse-to-fine principles for high-resolution
    inputs and anytime, on-demand processing.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Learning for completion and denoising
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Raw disparities can be noisy and incomplete, especially near object boundaries
    where depth smearing between objects remains a challenge. Several techniques have
    been developed for denoising and completion. Some of them are ad-hoc, *i.e.,*
    post-process the noisy and uncomplete initial estimates to generate clean and
    complete depth maps. Other methods addressed the issue of the lack of training
    data for completion and denoising. Others proposed novel depth representations
    that are more suitable for this task, especially for solving the depth smearing
    between objects.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Ad-hoc methods process the initially estimated disparities a using variational
    approaches [[51](#bib.bib51), [95](#bib.bib95)], Fully-Connected CRFs (DenseCRF) [[27](#bib.bib27),
    [96](#bib.bib96)], hierarchical CRFs [[2](#bib.bib2)], and diffusion processes [[40](#bib.bib40)]
    guided by confidence maps [[97](#bib.bib97)]. They encourage pixels that are spatially
    close and with similar colors to have closer disparity predictions. They have
    been also explored by Liu *et al.* [[5](#bib.bib5)]. However, unlike Li *et al.* [[2](#bib.bib2)],
    Liu *et al.* [[5](#bib.bib5)] used a CNN to minimize the CRF energy. Convolutional
    Spatial Propagation Networks (CSPN) [[98](#bib.bib98), [99](#bib.bib99)], which
    implement an anisotropic diffusion process, are particularly suitable for depth
    completion since they predict the diffusion tensor using a deep CNN. This is then
    applied to the initial map to obtain the refined one.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: One of the main challenges of deep learning-based depth completion and denoising
    is the lack of labelled training data, *i.e.,* pairs of noisy, incomplete depth
    maps and their corresponding clean depth maps. To address this issue, Jeon and
    Lee [[29](#bib.bib29)] propose a pairwise depth image dataset generation method
    using dense 3D surface reconstruction with a filtering method to remove low quality
    pairs. They also present a multi-scale Laplacian pyramid based neural network
    and structure preserving loss functions to progressively reduce the noise and
    holes from coarse to fine scales. The approach first predicts the clean complete
    depth image at the coarsest scale, which has a quarter of the original resolution.
    The predicted depth map is then progressively upsampled through the pyramid to
    predict the half and original-sized image. At the coarse level, the approach captures
    global context while at finer scales it captures local information. In addition,
    the features extracted during the downsampling are passed to the upsampling pyramid
    with skip connections to prevent the loss of the original details in the input
    depth image during the upsampling.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Instead of operating on the network architecture, the loss function, or the
    training datasets, Imran *et al.* [[100](#bib.bib100)] propose a new representation
    for depth called Depth Coefficients (DC) to address the problem of depth smearing
    between objects. The representation enables convolutions to more easily avoid
    inter-object depth mixing. The representation uses a multi-channel image of the
    same size as the target depth map, with each channel representing a fixed depth.
    The depth values increase in even steps of size $b$. (The approach uses $80$ bins.)
    The choice of the number of bins trades-off memory vs. precision. The vector composed
    of all these values at a given pixel defines the depth coefficients for that pixel.
    For each pixel, these coefficients are constrained to be non-negative and sum
    to $1$. This representation of depth provides a much simpler way for CNNs to avoid
    depth mixing. First, CNNs can learn to avoid mixing depths in different channels
    as needed. Second, since convolutions apply to all channels simultaneously, depth
    dependencies, like occlusion effects, can be modelled and learned by neural networks.
    The main limitation, however, is that the depth range needs to be set in advance
    and cannot be changed at runtime without re-training the network. Imran *et al.* [[100](#bib.bib100)]
    also show that the standard Mean Squared Error (MSE) loss function can promote
    depth mixing, and thus propose to use cross-entropy loss for estimating the depth
    coefficients.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Learning for realtime processing
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal is to design efficient stereo algorithms that not only produce reliable
    and accurate estimations, but also run in realtime. For instance, in the PSMNet [[64](#bib.bib64)],
    the cost volume construction and aggregation takes more than $250$ms (on nNvidia
    Titan-Xp GPU). This renders realtime applications infeasible. To speed the process,
    Khamis *et al.* [[72](#bib.bib72)] first estimate a low resolution disparity map
    and then hierarchically refine it. Yin *et al.* [[80](#bib.bib80)] employ a fixed,
    coarse-to-fine procedure to iteratively find the match. Chabra *et al.* [[81](#bib.bib81)]
    use 3D dilated convolutions in the width, height, and disparity channels when
    filtering the cost volume. Duggal *et al.* [[83](#bib.bib83)] combine deep learning
    with PatchMatch [[101](#bib.bib101)] to adaptively prune out the potentially large
    search space and significantly speed up inference. PatchMatch-based pruner module
    is able to predict a confidence range for each pixel, and construct a sparse cost
    volume that requires significantly less operations. This also allows the model
    to focus only on regions with high likelihood and save computation and memory.
    To enable end-to-end training, Duggal *et al.* [[83](#bib.bib83)] unroll PatchMatch
    as an RNN where each unrolling step is equivalent to an iteration of the algorithm.
    This approach achieved a performance that is comparable to the state-of-the-art,
    *e.g.,*  [[64](#bib.bib64), [68](#bib.bib68)], while reducing the computation
    time from $600$ms to $60$ms per image in the KITTI2015 dataset.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Learning confidence maps
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ability to detect, and subsequently remedy to, failure cases is important
    for applications such as autonomous driving and medical imaging. Thus, a lot of
    research has been dedicated to estimating confidence or uncertainty maps, which
    are then used to sparsify the estimated disparities by removing potential errors
    and then replacing them from the reliable neighboring pixels. Disparity maps can
    also be incorporated in a disparity refinement pipeline to guide the refinement
    process [[102](#bib.bib102), [103](#bib.bib103), [74](#bib.bib74)]. Seki *et al.* [[102](#bib.bib102)],
    for example, incorporate the confidence map into a Semi-Global Matching (SGM)
    module for dense disparity estimation. Gidaris *et al.* [[103](#bib.bib103)] use
    confidence maps to detect the incorrect estimates, replace them with disparities
    from neighbouring regions, and then refine the disparity using a refinement network.
    Jie *et al.* [[74](#bib.bib74)], on the other hand, estimate two confidence maps,
    one for each of the input images, concatenate them with their associated cost
    volumes, and use them as input to a 3D convolutional LSTM to selectively focus
    in the subsequent step on the left-right mismatched regions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Conventional confidence estimation methods are mostly based on assumptions and
    heuristics on the matching cost volume analysis, see [[59](#bib.bib59)] for a
    review and evaluation of the early methods. Recent techniques are based on supervised
    learning [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106), [107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109)]. They estimate confidence maps directly
    from the disparity space either in an ad-hoc manner, or in an integrated fashion
    so that they can be trained end-to-end along with the disparity/depth estimation.
    Poggi *et al.* [[110](#bib.bib110)] provide a quantitative evaluation. Below,
    we discuss some of these techniques.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 Confidence from left-right consistency check
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Left-right consistency is one of the most commonly-used criteria for measuring
    confidence in disparity estimates. The idea is to estimate two disparity maps,
    one from the left image ($D_{left}$), and another from the right image ($D_{right}$).
    An error map can then be computed by taking a pixel-wise difference between $D_{left}$
    and $D_{right}$, but warped back onto the left image, and converting them into
    probabilities [[63](#bib.bib63)]. This measure is suitable for detecting occlusions,
    *i.e.,* regions that are visible in one view but not in the other.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Left-right consistency can also be learned using deep or shallow networks composed
    of fully convolutional layers [[102](#bib.bib102), [74](#bib.bib74)]. Seki *et
    al.* [[102](#bib.bib102)] propose a patch-based confidence prediction (PBCP) network,
    which requires two disparity maps, one estimated from the left image and the other
    one from the right image. PBCP uses a two-channel network. The first channel enforces
    left-right consistency while the second one enforces local consistency. The network
    is trained in a classifier manner. It outputs a label per pixel indicating whether
    the estimated disparity is correct.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Instead of treating left-right consistency check as an isolated post-processing
    step, Jie *et al.* [[74](#bib.bib74)] perform it jointly with disparity estimation,
    using a Left-Right Comparative Recurrent (LRCR) model. It consists of two parallel
    convolutional LSTM networks [[111](#bib.bib111)], which produce two error maps;
    one for the left disparity and another for the right disparity. The two error
    maps are then concatenated with their associated cost volumes and used as input
    to a 3D convolutional LSTM to selectively focus in the next step on the left-right
    mismatched regions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 Confidence from a single raw disparity map
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Left-right consistency checks estimate two disparity maps and thus are expensive
    at runtime. Shaked and Wolf [[46](#bib.bib46)] train, via the binary cross entropy
    loss, a network, composed of two fully-connected layers, to predict the correctness
    of an estimated disparity from only the reference image. Poggi and Mattoccia [[107](#bib.bib107)]
    pose the confidence estimation as a regression problem and solve it using a CNN
    trained on small patches. For each pixel, the approach extracts a square patch
    around the pixel and forwards it to a CNN trained to distinguish between patterns
    corresponding to correct and erroneous disparity assignments. It is a single channel
    network, designed for $9\times 9$ image patches. Zhang *et al.* [[73](#bib.bib73)]
    use a similar confidence map estimation network, called *invalidation network*.
    The key idea is to train the network to predict confidence using a pixel-wise
    error between the left disparity and the right disparity. At runtime, the network
    only requires the left disparity. Finally, Poggi and Mattoccia [[112](#bib.bib112)]
    show that one can improve the confidence maps estimated using previous algorithms
    by enforcing local consistency in the confidence estimates.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 Confidence map from matching densities
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traditional deep networks represent activations and outputs as deterministic
    point estimates. Gast and Roth [[113](#bib.bib113)] explore the possibility of
    replacing the deterministic outputs by probabilistic output layers. To go one
    step further, they replace all intermediate activations by distributions. As such,
    the network can be used to estimate the matching probability densities, hereinafter
    referred to as *matching densities*, which can then be converted into uncertainties
    (or confidence) at runtime. The main challenge of estimating matching densities
    is the computation time. To make it tractable, Gast and Roth [[113](#bib.bib113)]
    assume parametric distributions. Yin *et al.* [[80](#bib.bib80)] relax this assumption
    and propose a pyramidal architecture to make the computation cost sustainable
    and allow for the estimation of confidence at run time.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.4 Local vs. global reasoning
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some techniques, *e.g.,* Seki *et al.* [[102](#bib.bib102)]’s, reason locally
    by enforcing local consistency. Tosi *et al.* [[114](#bib.bib114)] introduced
    LGC-Net to move beyond local reasoning. The input reference image and its disparity
    map are forwarded to a local network, *e.g.,* C-CNN [[107](#bib.bib107)], and
    a global network, *e.g.,* an encoder/decoder architecture with a large receptive
    field. The output of the two networks and the initial disparity, concatenated
    with the reference image, are further processed with three independent convolutional
    towers whose outputs are concatenated and processed with three $1\times 1$ convolutional
    layers to finally infer the confidence map.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.5 Combining multiple estimators
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some papers combine the estimates of multiple algorithms to achieve a better
    accuracy. Haeusler *et al.* [[104](#bib.bib104)] fed a random forest with a pool
    of $23$ confidence maps, estimated using conventional techniques, yielding a much
    better accuracy compared to any confidence map in the pool. Batsos *et al.* [[109](#bib.bib109)]
    followed a similar idea but combine the strengths and mitigate the weaknesses
    of four basic stereo matchers in order to generate a robust matching volume for
    the subsequent optimization and regularization steps. Poggi and Mattoccia [[58](#bib.bib58)]
    train an ensemble regression trees classifier. These methods are independent of
    the disparity estimation module, and rely on the availability of the cost volume.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 6 Learning multiview stereo
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/bd0a040ac7e65662b0dc439907d960a2.png) | ![Refer to
    caption](img/312c709404753414edfe435f8901d026.png) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| (a) Hartmann *et al.* [[47](#bib.bib47)]. | (b) Flynn *et al.* [[66](#bib.bib66)].
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/e473e8d663929d1792abd90297ca50aa.png) |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| (c) Kar *et al.* [[60](#bib.bib60)] and Yao *et al.* [[93](#bib.bib93)].
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/080e2675293a6cd93572e935861c8e0b.png) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| (d) Huang *et al.* [[27](#bib.bib27)]. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: 'Figure 6: Taxonomy of multivew stereo methods. (a), (b), and (c) perform early
    fusion, while (d) performs early fusion by aggregating features across depth plans,
    and late fusion by aggregating cost volumes across views.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Multiview Stereo (MVS) methods follow the same pipeline as of depth-from-stereo.
    Early works focused on computing the similarity between multiple patches. For
    instance, Hartmann *et al.* [[47](#bib.bib47)] (Fig. [6](#S6.F6 "Figure 6 ‣ 6
    Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(a)) replace the pairwise correlation layer used in stereo
    matching by an average pooling layer to aggregate the learned features of $n\geq
    2$ input patches, and then feed the output to a top network, which returns a matching
    score. With this method, computing the best match for a pixel on the reference
    image requires $n_{d}^{n-1}$ forward passes. ($n_{d}$ is the number of depth levels
    and $n$ is the number of images.) This is computationally very expensive especially
    when dealing with high resolution images.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Techniques that compute depth maps in a single forward pass differ in the way
    the information from the multiple views is fed to the network and aggregated.
    We classify them into whether they are volumetric (Section [6.1](#S6.SS1 "6.1
    Volumetric representations ‣ 6 Learning multiview stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")) or Plane-Sweep Volume (PSV)-based
    (Section [6.2](#S6.SS2 "6.2 Plane-Sweep Volume representations ‣ 6 Learning multiview
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")).
    The latter does not rely on intermediate volumetric representations of the 3D
    geometry. The only exception is the approach of Hou *et al.* [[115](#bib.bib115)],
    which performs temporal fusion of the latent representations of the input images.
    The approach, however, requires temporally-ordered images. Table [IV](#S6.T4 "TABLE
    IV ‣ 6.2 Plane-Sweep Volume representations ‣ 6 Learning multiview stereo ‣ A
    Survey on Deep Learning Techniques for Stereo-based Depth Estimation") provides
    a taxonomy and compares $13$ state-of-the-art MVS techniques.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Volumetric representations
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the main issues for MVS reconstruction is how to match, in an efficient
    way, features across multiple images. Pairwise stereo methods rectify the images
    so that the search for correspondences is restricted to the horizontal epipolar
    lines. This is not possible with MVS due to the large view angle differences between
    the images. This has bee addressed using volumetric representations of the scene
    geometry [[116](#bib.bib116), [60](#bib.bib60)]. Depth maps are then generated
    by projection from the desired viewpoint. For a given input image, with known
    camera parameters, a ray from the viewpoint is cast through each image pixel.
    The voxels intersected by that ray are assigned the color [[116](#bib.bib116)]
    or the learned feature [[60](#bib.bib60)] of that pixel. Existing methods differ
    in the way information from multiple views are fused:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: (1) Fusing feature grids. Kar *et al.* [[60](#bib.bib60)] (Fig. [6](#S6.F6 "Figure
    6 ‣ 6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(c)) fuse, recursively, the back-projected 3D feature grids
    using a recurrent neural network (RNN). The produced 3D grid is regularized using
    an encoder-decoder. To avoid dependency on the order of the images, Kar *et al.* [[60](#bib.bib60)]
    randomly permute the input images during training while constraining the output
    to be the same.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: (2) Fusing pairwise cost volumes. Choi *et al.* [[117](#bib.bib117)] fuse the
    cost volumes, computed from each pair of images, using a weighted sum where the
    weight of each volume is the confidence map computed from that cost volume.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: (3) Fusing the reconstructed surfaces. Ji *et al.* [[116](#bib.bib116)] process
    each pair of volumetric grids using a 3D CNN, which classifies whether a voxel
    is a surface point or not. To avoid the exhaustive combination of every possible
    image pairs, Ji *et al.* [[116](#bib.bib116)] learn their relative importance,
    using a network composed of fully-connected layers, automatically select a few
    view pairs based on their relative importance to reconstruct multiple volumetric
    grids, and take their weighted sum to produce the final 3D reconstruction.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: To handle high resolution volumetric grids, Ji *et al.* [[116](#bib.bib116)]
    split the whole space into small Colored Voxel Cubes (CVCs) and regress the surface
    cube-by-cube. While this reduces the memory requirements, it requires multiple
    forward passes and thus increases the computation time. Paschalidou *et al.* [[91](#bib.bib91)]
    avoid the explicit use of the volumetric representation. Instead, each voxel of
    the grid is projected onto each of the input views, before computing the pairwise
    correlation between the corresponding learned features on each pair of views,
    and then averaging them over all pairs of views. Repeating this process for each
    depth value will result in the depth distribution on each pixel. This depth distribution
    is regularized using an MRF formulated as a differentiable function to enable
    end-to-end training.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: In terms of performance, the volumetric approach of Ji *et al.* [[116](#bib.bib116)]
    requires $4$ hours to obtain a full reconstruction of a typical scene in DTU dataset [[24](#bib.bib24)].
    The approach of Paschalidou *et al.* [[91](#bib.bib91)] takes approximately $25$mins,
    on an Intel i7 computer with an Nvidia GTX Titan X GPU, for the same task. Finally,
    methods that perform fusion post-reconstruction have higher reconstruction errors
    compared to those that perform early fusion.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Plane-Sweep Volume representations
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These methods directly estimate depth maps from the input without using intermediate
    volumetric representations of the 3D geometry. As such, they are computationally
    more efficient. The main challenge to address is how to efficiently match features
    across multiple views in a single forward pass. This is done by using the Plane-Sweep
    Volumes (PSV) [[66](#bib.bib66), [27](#bib.bib27), [118](#bib.bib118), [93](#bib.bib93),
    [119](#bib.bib119), [90](#bib.bib90)], *i.e.,* they back project the input images [[66](#bib.bib66),
    [27](#bib.bib27), [118](#bib.bib118)] or their learned features [[93](#bib.bib93),
    [119](#bib.bib119), [90](#bib.bib90)] into planes at different depth values, forming
    PSVs from which the depth map is estimated. Existing methods differ in the way
    the PSVs are processed with the feature extraction and feature matching blocks.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Flynn *et al.*’s network [[66](#bib.bib66)] (Fig. [6](#S6.F6 "Figure 6 ‣ 6 Learning
    multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(b)) is composed of $n_{d}$ branches, one for each depth plane. The
    $0pt-$th branch of the network takes as input the reference image and the planes
    of the PSVs of the other images which are located at depth $0pt$. These are packed
    together and fed to a two-stage network. The first stage computes matching features
    between the reference image and all the PSV planes located at depth $0pt$. The
    second stage models interactions across depth planes using convolutional layers.
    The final block of the network is a per-pixel softmax over depth, which returns
    the most probable depth value per pixel. The approach requires that the number
    of views and the camera parameters of each view to be known.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Huang *et al.* [[27](#bib.bib27)]’s approach (Fig. [6](#S6.F6 "Figure 6 ‣ 6
    Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(d)) starts with a pairwise matching step where a cost volume
    is computed between the reference image and each of the input images. For a given
    pair $(I_{1},I_{i}),i=2,\dots,n$, $I_{i}$ is first back-projected into a PSV.
    A siamese network then computes a matching cost volume between $I_{1}$ and each
    of the PSV planes. These volumes are aggregated into a single cost volume using
    an encoder-decoder network. This is referred to as intra-volume aggregation. Finally
    a max-pooling layer is used to aggregate the multi intra-volumes into a single
    inter-volume, which is then used to predict the depth map. Unlike Flynn *et al.* [[66](#bib.bib66)],
    Huang *et al.* [[27](#bib.bib27)]’s approach does not require a fixed number of
    input views since aggregation is performed using pooling. In fact, the number
    of views can vary between training and at runtime.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Unlike [[66](#bib.bib66), [27](#bib.bib27)], which back-project the input images,
    the MVSNet of Yao *et al.* [[93](#bib.bib93)] use the camera parameters to back-project
    the learned features into a 3D frustum of a reference camera sliced into parallel
    frontal planes, one for each depth value. The approach then generates the matching
    cost volume upon a pixel-wise variance-based metric, and finally a generic 3D
    U-Net is used to regularize the matching cost volume to estimate the depth maps.
    Luo *et al.* [[119](#bib.bib119)] extend MVSNet [[93](#bib.bib93)] to P-MVSNet
    in two ways. First, a raw cost volume is processed with a learnable patch-wise
    aggregation function before feeding it to the regularization network. This improves
    the matching robustness and accuracy for noisy data. Second, instead of using
    a generic 3D-UNet network for regularization, P-MVSNet uses a hybrid isotropic-anisotropic
    3D-UNet. The plane-sweep volumes are essentially anisotropic in depth and spatial
    directions, but they are often approximated by isotropic cost volumes, which could
    be detrimental. In fact, one can infer the corresponding depth map along the depth
    direction of the matching cost volume, but cannot get the same information along
    other directions. Luo *et al.* [[119](#bib.bib119)] exploit this fact, through
    the proposed hybrid 3D U-Net with isotropic and anisotropic 3D convolutions, to
    guide the regularization of matching confidence volume.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of using PSVs is that they eliminate the need to supply rectified
    images. In other words, the camera parameters are implicitly encoded. However,
    in order to compute the PSVs, the intrinsic and extrinsic camera parameters need
    to be either provided in advance or estimated using, for example, Structure-from-Motion
    techniques as in [[27](#bib.bib27)]. Also, these methods require setting in advance
    the disparity range and its discretisation. Moreover, they often result in a complex
    network architecture. Wang *et al.* [[120](#bib.bib120)] propose a light-weight
    architecture. It stacks together the reference image and the cost volume, computed
    using the absolute difference between the reference image and each other image
    but at different depth planes, and feeds them to an encoder-decoder network, with
    skip connections, to estimate the inverse depth at three different resolutions.
    Wang *et al.* [[120](#bib.bib120)] use a view selection rule, which selects the
    frames that have enough angle or translation difference and then use the selected
    frames to compute the cost volume.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that feature back-projection has been also used by Won *et al.* [[30](#bib.bib30)]
    for omnidirectional depth estimation from a wide-baseline multi-view stereo setup.
    The approach uses spherical maps and spherical cost volumes.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Taxonomy and comparison of $13$ deep learning-based MVS techniques.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Representation | Fusion | Training |  | Peformance on (DTU,
    SUN3D, ETH3D) |  | Complexity |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '|  | #images | Error ($mm$) | % $<1mm$ | % $<2mm$ |  | # Params | Memory |
    Complexity | Time (s) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| Kar *et al.* [[60](#bib.bib60)] | 2017 | Volumetric | Recurrent fusion |
    Supervised |  | Variable | $-$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | of 3D feature grids |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| Hartmann *et al.* [[47](#bib.bib47)] | 2017 | Replace correlation by pooling
    | Supervised |  | $5$ | $(1.356,-,-)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  | (can vary) |  |  |  |  |  |  |  |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| Ji *et al.* [[116](#bib.bib116)] | 2017 | Volumetric | Reconstructed surfaces
    | Supervised |  | $5$ | $(0.745,-,-)$ | $69.95$ | $74.4$ |  | $-$ | $-$ | $-$
    | $4$ hrs |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| Choi *et al.* [[117](#bib.bib117)] | 2018 | Volumetric | Pairwise cost volumes
    | Supervised |  | $5$ | $(0.6511,-,-)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| Huang *et al.* [[27](#bib.bib27)] | 2018 | PSV | Encoder-decoder for intra-volume,
    | Supervised |  | Variable | $(-,0.419,0.412)$ | $-$ | $-$ |  | $-$ | $-$ | $-$
    | $-$ |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Max pooling for inter-volume |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| Leroy *et al.* [[118](#bib.bib118)] | 2018 | PSV | Depth fusion | Supervised
    |  | Variable | $(0.599,-,-)$ | $-$ | $-$ |  | $72$K | $-$ | $-$ | $-$ |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| Paschalidou *et al.* [[91](#bib.bib91)] | 2018 | Depth-based | Avg. pooling
    over | Supervised |  | Variable | $(-,-,-)$ | $-$ | $-$ |  | $-$ | $7$GB | $-$
    | $25$ mins |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | pairwsie correlations |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| Yao *et al.* [[93](#bib.bib93)] | 2018 | PSV | Feature pooling by variance
    | Supervised |  | $5$ | $(0.462,0.397,0.470)$ | $75.69$ | $80.25$ |  | $363$K
    | $5.28$GB | $O(0pt\times 0pt\times n_{d})$ | $0.9$s |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| Wang *et al.* [[120](#bib.bib120)] | 2018 | PSV and abs. | Concatenation
    of pairwise | Supervised |  | Variable | $(-,0.114,0.257)$ | $-$ | $-$ |  | $33.9$M
    for | $-$ | $-$ | $0.04$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '|  |  | difference | cost volumes and ref. image |  |  |  |  |  |  |  | $n_{d}=64$
    |  |  |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| Hou *et al.* [[115](#bib.bib115)] | 2019 | $-$ | Temporal fusion of | Supervised
    |  | Variable | $(-,\textbf{0.101},0.229)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ |
    $-$ |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | the latent rep. |  |  | (video sequence) |  |  |  |  |  |  |  |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| Luo *et al.* [[119](#bib.bib119)] | 2019 | PSV | Feature pooling by variance
    | Supervised |  | Variable | $(0.406,-,-)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ |
    $-$ |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| Xue *et al.**et al.* [[90](#bib.bib90)] | 2019 | PSV | Cost volume | Supervised
    |  | $5$ | $(\textbf{0.398},-,-)$ | $80.02$ | $83.84$ |  | $571$K | $5.43$GB |
    $O(0pt\times 0pt\times n_{d})$ | $1.8$s |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | pooling by variance |  |  | (can vary) |  |  |  |  |  |  |  |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| Won *et al.* [[30](#bib.bib30)] | 2019 | Spherical PSV | Concatenation |
    Supervised |  | $-$ | $-$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: 7 Training end-to-end stereo methods
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training process aims to find the network parameters $W$ that minimize a
    loss function $\mathcal{L}(W;\hat{D},\Theta)$ where $\hat{D}$ is the estimated
    disparity, and $\Theta$ are the supervisory cues. The loss function is defined
    as the sum of a data term $\mathcal{L}_{1}(\hat{D},\Theta,W)$, which measures
    the discrepancy between the ground-truth and the estimated disparity, and a regularization
    or smoothness term $\mathcal{L}_{2}(\hat{D},W)$, which imposes local or global
    constraints on the solution. The type of supervisory cues defines the degree of
    supervision (Section [7.1](#S7.SS1 "7.1 Supervision methods ‣ 7 Training end-to-end
    stereo methods ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")),
    which can be supervised with 3D groundtruth (Section [7.1.1](#S7.SS1.SSS1 "7.1.1
    3D supervision methods ‣ 7.1 Supervision methods ‣ 7 Training end-to-end stereo
    methods ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")),
    self-supervised using auxiliary cues (Section [7.1.2](#S7.SS1.SSS2 "7.1.2 Self-supervised
    methods ‣ 7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation")), or weakly supervised
    (Section [7.1.3](#S7.SS1.SSS3 "7.1.3 Weakly supervised methods ‣ 7.1 Supervision
    methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")). Some methods use additional cues, in the
    form of constraints on the solution, to boost the accuracy and performance (Section [7.2](#S7.SS2
    "7.2 Incorporating additional cues ‣ 7 Training end-to-end stereo methods ‣ A
    Survey on Deep Learning Techniques for Stereo-based Depth Estimation")). One of
    the main challenges of deep learning-based techniques is their ability to generalize
    to new domains. Section [7.3](#S7.SS3 "7.3 Domain adaptation and transfer learning
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") reviews methods that addressed this issue.
    Finally, Section [7.4](#S7.SS4 "7.4 Learning the network architecture ‣ 7 Training
    end-to-end stereo methods ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") reviews methods that learn network architectures.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Supervision methods
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 7.1.1 3D supervision methods
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Supervised methods are trained to minimise a loss function that measures the
    error between the ground truth disparity and the estimated disparity. It is of
    the form:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum C(x)H(C(x)-\epsilon)\mathcal{D}\left(\Phi(d_{x}),\Phi(\hat{d}_{x})\right),$
    |  | (6) |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: 'where: $d_{x}$ a $\hat{d}_{x}$ are, respectively, the groundtruth and the estimated
    disparity at pixel $x$. $\mathcal{D}$ is a measure of distance, which can be the
    $L_{2}$, the $L_{1}$ [[61](#bib.bib61), [121](#bib.bib121), [62](#bib.bib62),
    [99](#bib.bib99)], the smooth $L_{1}$ [[64](#bib.bib64)], or the smooth $L_{1}$
    but approximated using the two-parameter robust function $\rho(\cdot)$ [[72](#bib.bib72),
    [122](#bib.bib122)]. $C(x)\in[0,1]$ is the confidence of the estimated disparity
    at $x$. Setting $C(x)=1$ and the threshold $\epsilon=0,\forall x$ is equivalent
    to ignoring the confidence map. $H(x)$ is the heavyside function, which is equal
    to $1$ if $x\geq 0$, and $0$ otherwise. $\Phi(\cdot)$ is either the identify or
    the log function. The latter avoids overfitting the network to large disparities.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Some papers restrict the sum in Eqn. ([6](#S7.E6 "In 7.1.1 3D supervision methods
    ‣ 7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")) to be over only
    the valid pixels or regions of interest, *e.g.,* foreground or visible pixels [[123](#bib.bib123)],
    to avoid outliers. Other, *e.g.,* Yao *et al.* [[93](#bib.bib93)], divide the
    loss into two parts, one over the initial disparity and the other one over the
    refined disparity. The overall loss is then defined as the weighted sum of the
    two losses.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Self-supervised methods
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Self-supervised methods, originally used in optical flow estimation [[124](#bib.bib124),
    [125](#bib.bib125)], have been proposed as a possible solution in the absence
    of sufficient ground-truth training data. These methods mainly rely on image reconstruction
    losses, taking advantage of the projective geometry, and the spatial and temporal
    coherence when multiple images of the same scene are available. The rationale
    is that if the estimated disparity map is as close as possible to the ground truth,
    then the discrepancy between the reference image and any of the other images but
    unprojected using the estimated depth map onto the reference image, is also minimized.
    The general loss function is of the form:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\mathcal{D}\left(\Phi\left(I_{ref}\right)(x)-\Phi\left(\tilde{I}_{ref}\right)(x)\right),$
    |  | (7) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: 'where $\tilde{I}_{ref}$, which is $I_{right}$ but unwarped onto $I_{ref}$ using
    the estimated disparity, and $\mathcal{D}$ is a measure of distance. The mapping
    function $\Phi$ can be:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The identity [[126](#bib.bib126), [127](#bib.bib127), [70](#bib.bib70), [68](#bib.bib68)].
    In this case, the loss of Eqn. ([7](#S7.E7 "In 7.1.2 Self-supervised methods ‣
    7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")) is called a photometric
    or image reconstruction loss.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mapping to the feature space [[68](#bib.bib68)], *i.e.,* $\Phi\left(I_{ref}\right)=\textbf{f}$
    where f is the learned feature map.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient of the image, *i.e.,* $\Phi\left(I_{ref}\right)=\nabla I_{ref}$,
    which is less sensitive to variations in lighting and acquisition conditions than
    the photometric loss.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The distance $\mathcal{D}$ can be the $L_{1}$ or $L_{2}$ distance. Some papers [[70](#bib.bib70)]
    also use more complex metrics such as the structural dissimilarity [[128](#bib.bib128)]
    between patches in $I_{ref}$ and in $\tilde{I}_{ref}$.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: While stereo-based supervision methods do not require ground-truth 3D labels,
    they rely on the availability of calibrated stereo pairs during training.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3 Weakly supervised methods
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Supervised methods for disparity estimation can achieve promising results if
    trained on large quantities of ground truth depth data. However, manually obtaining
    ground-truth depth data is extremely difficult and expensive, and is prone to
    noise and inaccuracies. Weakly supervised methods rely on auxiliary signals to
    reduce the amount of manual labelling. In particular, Tonioni *et al.*[[129](#bib.bib129)]
    used as a supervisory signal the depth estimated using traditional stereo matching
    techniques to fine-tune depth estimation networks. Since such depth data can be
    sparse, noisy, and prone to errors, they propose a confidence-guided loss that
    penalizes ground-truth depth values that are deemed not reliable. It is defined
    using Eqn. ([6](#S7.E6 "In 7.1.1 3D supervision methods ‣ 7.1 Supervision methods
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")) by setting $\mathcal{D}(\cdot)$ to be the
    $L_{1}$ distance, and $\epsilon>0$. Kuznietsov *et al.* [[130](#bib.bib130)] use
    sparse ground-truth depth for supervised learning, while enforcing the deep network
    to produce photo-consistent dense depth maps in a stereo setup using a direct
    image alignment/reprojection loss. These two methods rely on an ad-hoc disparity
    estimator. To avoid that, Zhou *et al.* [[131](#bib.bib131)] propose an iterative
    approach, which starts with a randomly initialized network. At each iteration,
    it computes matches from the left to the right images, and matches from the right
    to the left images. It then selects the high confidence matches and adds them
    as labelled data for further training in the subsequent iterations. The confidence
    is computed using the left-right consistency of Eqn. ([12](#S7.E12 "In 7.2 Incorporating
    additional cues ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Incorporating additional cues
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several works incorporate additional cues and constraints to improve the quality
    of the disparity estimates. Examples include smoothness [[70](#bib.bib70)], left-right
    consistency [[70](#bib.bib70)], maximum depth [[70](#bib.bib70)], and scale-invariant
    gradient loss [[121](#bib.bib121)]. Such cues can also be in the form of auxiliary
    information such as semantic cues used to guide the disparity estimation network.
    Below, we discuss a number of these works.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '(1) Smoothness. In general, one can assume that neighboring pixels have similar
    disparity values. Such smoothness constraint can be enforced by minimizing:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The absolute difference between the disparity predicted at $x$ and those predicted
    at each pixel $y$ within a certain predefined neighborhood $\mathcal{N}_{x}$ around
    $x$:'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\sum_{y\in\mathcal{N}_{x}}&#124;d_{x}-d_{y}&#124;.$
    |  | (8) |'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Here, $N$ is the total number of pixels.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The magnitude of the first-order gradient $\nabla$ of the estimated disparity
    map [[68](#bib.bib68)]:'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}d_{x})+(\nabla_{v}d_{x})\right\},x=(u,v).}$
    |  | (9) |'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The magnitude of the second-order gradient of the estimated disparity [[127](#bib.bib127),
    [132](#bib.bib132)]:'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}^{2}d_{x})^{2}+(\nabla_{v}^{2}d_{x})^{2}\right\}.$
    |  | (10) |'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second-order gradient of the estimated disparity map weighted by the image’s
    second-order gradients [[70](#bib.bib70)]:'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}=\frac{1}{N}\sum_{x}\left\{&#124;\nabla_{u}^{2}d_{x}&#124;e^{-&#124;\nabla_{u}^{2}I(x)&#124;}+&#124;\nabla_{v}^{2}d_{x}&#124;e^{-&#124;\nabla_{v}^{2}I(x)&#124;}\right\}.}$
    |  | (11) |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '(2) Consistency. Zhong *et al.* [[70](#bib.bib70)] introduced the loop-consistency
    loss, which is constructed as follows. Consider the left image $I_{left}$ and
    the synthesized image $\tilde{I}_{left}$ obtained by warping the right image to
    the left image coordinate using the disparity map defined on the right image.
    A second synthesized left image $\tilde{\tilde{I}}_{left}$ can also be generated
    by warping the left image to the right image coordinates, by using the disparities
    at the left image, and then warping it back to the left image using the disparity
    at the right image. The three versions of the left image provide two constraints:
    $I_{left}=\tilde{I}_{left}$ and $I_{left}=\tilde{\tilde{I}}_{left}$, which can
    be used to regularize the disparity maps. Godard *et al.* [[133](#bib.bib133)]
    introduce the left-right consistency term, which is a linear approximation of
    the loop consistency. The loss attempts to make the left-view disparity map equal
    to the projected right-view disparity map. It is defined as:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}&#124;d_{x}-\tilde{d}_{x}&#124;,$ |  |
    (12) |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: where $\tilde{d}$ is the disparity at the right image but reprojected onto the
    coordinates of the left image.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '(3) Maximum-depth heuristic. There may be multiple warping functions that achieve
    a similar warping loss, especially for textureless areas. To provide strong regularization
    in these areas, Zhong *et al.* [[70](#bib.bib70)] use the Maximum-Depth Heuristic
    (MDH) [[134](#bib.bib134)] defined as the sum of all depths/disparities:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}&#124;d_{x}&#124;.$ |  | (13) |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '(4) Scale-invariant gradient loss [[121](#bib.bib121)]. It is defined as:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\sum_{h\in A}\sum_{x}\&#124;g_{h}[D](x)-g_{h}[\hat{D}](x)\&#124;_{2},$
    |  | (14) |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: where $A=\{1,2,4,8,16\}$, $x=(i,j)$, $f_{i,j}\equiv f(i,j)$, and
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{g_{h}[f](i,j)=\left(\frac{f_{i+h,j}-f_{i,j}}{&#124;f_{i+h,j}-f_{i,j}&#124;},\frac{f_{i,j+h}-f_{i,j}}{&#124;f_{i,j+h}-f_{i,j}&#124;}\right)^{\top}.}$
    |  | (15) |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: This loss penalizes relative depth errors between neighbouring pixels. This
    loss stimulates the network to compare depth values within a local neighbourhood
    for each pixel. It emphasizes depth discontinuities, stimulates sharp edges, and
    increases smoothness within homogeneous regions.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '(5) Incorporating semantic cues. Some papers incorporate additional cues such
    as normal [[135](#bib.bib135)], segmentation [[68](#bib.bib68)], and edge [[76](#bib.bib76)]
    maps, to guide the disparity estimation. These can be either provided at the outset,
    *e.g.,* estimated with a separate method as in [[76](#bib.bib76)], or estimated
    jointly with the disparity map. Qi *et al.* [[135](#bib.bib135)] propose a mechanism
    that uses the depth map to refine the quality of the normal estimates, and the
    normal map to refine the quality of the depth estimates. This is done using a
    two-stream network: a depth-to-normal network for normal map refinement using
    the initial depth estimates, and a normal-to-depth network for depth refinement
    using the estimated normal map.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Yang *et al.* [[68](#bib.bib68)] and Song *et al.* [[76](#bib.bib76)] incorporate
    semantics by stacking semantic maps (segmentation masks in the case of [[68](#bib.bib68)]
    and edge features in the case of [[76](#bib.bib76)]) with the 3D cost volume.
    Yang *et al.* [[68](#bib.bib68)] train jointly a disparity estimation network
    and a segmentation network by using a loss function defined as a weighted sum
    of the reconstruction error, a smoothness term, and a segmentation error. Song
    *et al.* [[76](#bib.bib76)] further incorporate edge cues in the edge-aware smoothness
    loss to penalize drastic depth changes in flat regions. Also, to allow for depth
    discontinuities at object boundaries, the edge-aware smoothness loss is defined
    based on the gradient map obtained from the edge detection sub-network, which
    is more semantically meaningful than the variation in raw pixel intensities.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Wu *et al.* [[79](#bib.bib79)] introduced an approach that fuses multiscale
    4D cost volumes with semantic features obtained using a segmentation sub-network.
    The approach uses the features of the left and the right images as input to a
    semantic segmentation network similar to PSPNet [[136](#bib.bib136)]. Semantic
    features for each image are then obtained from the output of the classification
    layer of the segmentation network. A 4D semantic cost volume is obtained by concatenating
    each unary semantic feature with their corresponding unary from the opposite stereo
    image across each disparity level. Both the spatial pyramid cost volumes and the
    semantic cost volume are fed into a 3D multi-cost aggregation module, which aggregates
    them, using an encoder-decoder followed by a 3D feature fusion module, into a
    single 3D cost volume in a pairwise manner starting with the smallest volume.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: In summary, appending semantic features to the cost volume improves the reconstruction
    of fine details, especially near object boundaries.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Domain adaptation and transfer learning
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep architectures for depth estimation are severely affected by the domain
    shift issue, which hinders their effectiveness when performing inference on images
    significantly diverse from those used during the training stage. This can be observed,
    for instance, when moving between indoor and outdoor environments, from synthetic
    to real data, see Fig. [7](#S7.F7 "Figure 7 ‣ 7.3 Domain adaptation and transfer
    learning ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation"), or between different outdoor/indoor environments,
    and when changing the camera model/parameters. As such, deep learning networks
    trained on one domain, *e.g.,* by using synthetic data, suffer when applied to
    another domain, *e.g.,* real data, resulting in blurry object boundaries and errors
    in ill-posed regions such as object occlusions, repeated patterns, and textureless
    regions. These are referred to as *generalization glitches* [[137](#bib.bib137)].
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Several strategies have been proposed to adress this domain bias issue. They
    can be classified into two categories: adaptation by fine-tuning (Section [7.3.1](#S7.SS3.SSS1
    "7.3.1 Adaptation by fine-tuning ‣ 7.3 Domain adaptation and transfer learning
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")) and adaptation by data transformation (Section [7.3.2](#S7.SS3.SSS2
    "7.3.2 Adaptation by data transformation ‣ 7.3 Domain adaptation and transfer
    learning ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")). In both cases, the adaptation can be offline
    or online.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9287320ef827e6144299fb36b265bbdd.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Illustration of the domain gap between synthetic (left) and real
    (right) images. The left image is from the FlyingThings synthetic dataset [[22](#bib.bib22)].'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1 Adaptation by fine-tuning
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Methods in this category perform domain adaptation by first training a network
    on images from a certain domain, *e.g.,* synthetic images as in [[22](#bib.bib22)],
    and then fine-tuning it on images from a target domain. A major difficulty is
    to collect accurate ground-truth depth for stereo or multiview images from the
    target domain. Relying on active sensors (*e.g.,* LiDAR) to obtain such supervised
    labeled data is not feasible in practical applications. As such, recent works,
    *e.g.,*  [[129](#bib.bib129), [138](#bib.bib138), [137](#bib.bib137)] rely on
    off-the-shelf stereo algorithms to obtain ground-truth disparity/depth labels
    in an unsupervised manner, together with state-of-the-art confidence measures
    to ascertain the correctness of the measurements of the off-the-shelf stereo algorithms.
    The latter is used in [[129](#bib.bib129), [138](#bib.bib138)] to discriminate
    between reliable and unreliable disparity measurements, to select the former and
    fine tune a pre-trained model, *e.g.,* DispNet [[22](#bib.bib22)], using such
    smaller and sparse set of points as if they were ground-truth labels.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Pang *et al.* [[137](#bib.bib137)] also use a similar approach as in [[129](#bib.bib129),
    [138](#bib.bib138)] to address the generalization glitches. The approach, however,
    exploits the scale diversity, *i.e.,* up-sampling the stereo pairs enables the
    model to perform stereo matching in a localized manner with subpixel accuracy,
    by performing iterative optimisation of predictions obtained at multiple resolutions
    of the input.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Note that self-supervised and weakly supervised techniques for disparity estimation,
    *e.g.,*  [[139](#bib.bib139), [133](#bib.bib133), [140](#bib.bib140), [141](#bib.bib141)]
    can also be used for offline domain adaptation. In particular, if stereo pairs
    of the target domain are available, these techniques can be fine-tuned, in an
    unsupervised manner, using reprojection losses, see Sections [7.1.2](#S7.SS1.SSS2
    "7.1.2 Self-supervised methods ‣ 7.1 Supervision methods ‣ 7 Training end-to-end
    stereo methods ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    and [7.1.3](#S7.SS1.SSS3 "7.1.3 Weakly supervised methods ‣ 7.1 Supervision methods
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation").
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Although effective, these offline adaptation techniques reduce the usability
    of the methods since users are required to train the models every time they are
    exposed to a new domain. As a result, several recent papers developed online adaptation
    techniques. For example, Tonioni *et al.* [[84](#bib.bib84)] address the domain
    shift issue by casting adaptation as a continuous learning process whereby a stereo
    network can evolve online based on the images gathered by the camera during its
    real deployment. This is achieved in an unsupervised manner by computing error
    signals on the current frames, updating the whole network by a single back-propagation
    iteration, and moving to the next pair of input frames. To keep a high enough
    frame rate, Tonioni *et al.* [[84](#bib.bib84)] propose a lightweight, fast, and
    modular architecture, called MADNet, which allows training sub-portions of the
    whole network independently from each other. This allows adapting disparity estimation
    networks to unseen environments without supervision at approximately $25$ fps,
    while achieving an accuracy comparable to DispNetC [[22](#bib.bib22)]. Similarly,
    Zhong *et al.* [[142](#bib.bib142)] use video sequences to train a deep network
    online from a random initialization. They employ an LSTM in their model to leverage
    the temporal information during the prediction.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Zhong *et al.* [[142](#bib.bib142)] and Tonioni *et al.* [[84](#bib.bib84)]
    consider online adaptation separately from the initial training. Tonioni *et al.* [[143](#bib.bib143)],
    on the other hand, incorporate the adaptation procedure to the learning objective
    to obtain a set of initial parameters that are suitable for online adaptation,
    *i.e.,* they can be adapted quickly to unseen environments. This is implemented
    using the model agnostic meta-learning framework of [[144](#bib.bib144)], an explicit
    *learn-to-adapt* framework that enables stereo methods to adapt quickly and continuously
    to new target domains in an unsupervised manner.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2 Adaptation by data transformation
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Methods in this category transform the data of one domain to look similar in
    style to the data of the other domain. For example, Atapour-Abarghoue *et al.* [[145](#bib.bib145)]
    proposed a two-staged approach. The first stage trains a depth estimation model
    using synthetic data. The second stage is trained to transfer the style of synthetic
    images to real-world images. By doing so, the style of real images is first transformed
    to match the style of synthetic data and then fed into the depth estimation network,
    which has been trained on synthetic data. Zheng *et al.* [[146](#bib.bib146)]
    perform the opposite by transforming the synthetic images to become more realistic
    and using them to train the depth estimation network. Zhao *et al.* [[147](#bib.bib147)]
    consider both synthetic-to-real [[146](#bib.bib146)] and real-to-synthetic [[148](#bib.bib148),
    [145](#bib.bib145)] translations. The two translators are trained in an adversarial
    manner using an adversarial loss and a cycle-consistency loss. That is, a synthetic
    image when converted to a real image and converted back to the synthetic domain
    should look similar to the original one.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Although these methods have been used for monocular depth estimation, they are
    applicable to (multi-view) stereo matching methods.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Learning the network architecture
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Much research work in depth estimation is being spent on manually optimizing
    network architectures, but what about if the optimal network architecture, along
    with its parameters, could be also learnt from data? Saika *et al.* [[149](#bib.bib149)]
    show how to use and extend existing AutoML techniques [[150](#bib.bib150)] to
    efficiently optimize large-scale U-Net-like encoder-decoder architectures for
    stereo-based depth estimation. Traditional AutoML techniques have extreme computational
    demand limiting their usage to small-scale classification tasks. Saika *et al.* [[149](#bib.bib149)]
    applies Differentiable Architecture Search (DARTs) [[151](#bib.bib151)] to encoder-decoder
    architectures. Its main idea is to have a large network that includes all architectural
    choices and to select the best parts of this network by optimization. This can
    be relaxed to a continuous optimization problem, which, together with the regular
    network training, leads to a bilevel optimization problem. Experiments conducted
    on DispNet of [[75](#bib.bib75)], an improved version of [[22](#bib.bib22)], show
    that the automatically optimized DispNet (AutoDispNet) yields better performance
    compared to the baseline DispNet of [[75](#bib.bib75)], with about the same number
    of parameters. The paper also shows that the benefits of automated optimization
    carry over to large stacked networks.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 8 Discussion and comparison
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tables [III](#S5.T3 "TABLE III ‣ 5.1 Feature learning ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    and [IV](#S6.T4 "TABLE IV ‣ 6.2 Plane-Sweep Volume representations ‣ 6 Learning
    multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation"), respectively, compare the performance of the methods surveyed in
    this article on standard datasets such as KITTI2015 for pairwise stereo methods,
    and DTU, SUN3D and ETH3D for multiview stereo methods. Most of these methods have
    been trained on subsets of these publicly available datasets. A good disparity
    estimation method, once properly trained, should achieve good performance not
    only on publicly available benchmarks but on arbitrary novel images. They should
    not require re-training or fine-tuning every time the domain of usage changes.
    In this section, we will look at how some of these methods perform on novel unseen
    images. We will first describe in Section [8.1](#S8.SS1 "8.1 Evaluation protocol
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") the evaluation protocol, the images that will be used, and
    the evaluation metrics. We then discuss the performance of these methods in Sections [8.2](#S8.SS2
    "8.2 Computation time and memory footprint ‣ 8 Discussion and comparison ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation") and [8.3](#S8.SS3
    "8.3 Reconstruction accuracy ‣ 8 Discussion and comparison ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation").
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Evaluation protocol
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We consider several key methods and evaluate their performance on the stereo
    subset of the ApolloScape dataset [[34](#bib.bib34)], and on an in-house collected
    set of four images. The motivation behind this choice is two-fold. First, the
    ApolloScape dataset is composed of stereo images taken outdoor in autonomous driving
    setups. Thus, it exhibits several challenges related to uncontrolled complex and
    varying lighting conditions, and heavy occlusions. Second, the dataset is novel
    and existing methods have not been trained or exposed to this dataset. Thus, it
    can be used to assess how these methods generalize to novel scenarios. In this
    dataset, ground truth disparities have been acquired by accumulating 3D point
    clouds from Lidar and fitting 3D CAD models to individually moving cars. We also
    use four in-house images of size $0pt=640$ and $0pt=480$, see Fig. [9](#S8.F9
    "Figure 9 ‣ 8.1 Evaluation protocol ‣ 8 Discussion and comparison ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation"), specifically designed
    to challenge these methods. Two of the images are of real scenes: a Bicycles scene
    composed of bicycles in a parking, and an indoor Desk scene composed of office
    furnitures. We use a moving stereo camera to capture multiple stereo pairs, and
    Structure-from-Motion (SfM) to build a 3D model of the scenes. We then render
    depth maps from the real cameras’ viewpoints. Regions where depth is estimated
    with high confidence will be used as ground-truth. The remaining two images are
    synthetic, but real-looking. They include objects with complex structures, *e.g.,*
    thin structures such as plants, large surfaces with either uniform colors or textures
    and repetitive patterns, presenting several challenges to stereo-based depth estimation
    algorithms.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/d4e5fa875ce77113e45aa19a75208fae.png) | ![Refer to
    caption](img/69fb50d6f40a3c5a9403bb02d43acb47.png) | ![Refer to caption](img/9a63a621259d32226f508ec3059b8804.png)
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| (a) Baseline: images with good lighting conditions. |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/40b2b768f198eea2f28d2c8053c35323.png) | ![Refer to
    caption](img/f8eec6998abbc0e36d1baccff1fe0c4e.png) | ![Refer to caption](img/3e1085666e93c19f7f486b70d5c8398c.png)
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| (b) Challenge: images with challenging lighting conditions. |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: 'Figure 8: Examples of stereo pairs and their ground-truth disparity maps from
    the ApolloScape dataset [[34](#bib.bib34)].'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/b7681081c5af890f9d55a205db7d6642.png) | ![Refer to
    caption](img/61f108d235377a4c737baec5fe987e40.png) | ![Refer to caption](img/56528b0194b72940760e5fe83c12f49b.png)
    | ![Refer to caption](img/ce1c906a4c76a6940921b51dd1ce240c.png) |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| (a) Left image. |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/3251304de12bead33aabaaedc70c6a47.png) | ![Refer to
    caption](img/430afd96d07e6eef22b75c1fe2c03df2.png) | ![Refer to caption](img/de0e8acc261202f6fb59497bb9dd892e.png)
    | ![Refer to caption](img/dab16633f31bb813568d5b370e6c3106.png) |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| (b) Highlights of regions of interest where ground-truth disparity is |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| estimated with high confidence. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/0a93fafa45ddd6abea49e5897210c1a8.png) | ![Refer to
    caption](img/b4ab087f886eb30058f75a8140b34953.png) | ![Refer to caption](img/31b7633c422a5f48e795be9f06fb52a2.png)
    | ![Refer to caption](img/9c606a83f4766c7ccc7ab1be35b344e7.png) |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| (c) Right image. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/4c748812459cd7227a45fbde7eb25495.png) | ![Refer to
    caption](img/200e1efd1a15a78a22deb4733035fdfb.png) | ![Refer to caption](img/d5214a179752895f29e47a0f5015dc82.png)
    | ![Refer to caption](img/ac84a30b96faae181c5efeb83299854d.png) |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '| Disp. $\in[9.3,34.0]$ | Disp. $\in[18.7,29.9]$ | Disp. $\in[5.6,14.5]$ |
    Disp. $\in[5.5,13.2]$ |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| Depth $\in[2.1,7.8]$ | Depth $\in[2.4,3.3]$ | Depth $\in[7.8,25.0]$ | Depth
    $\in[10.8,25.6]$ |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| (d) Ground-truth disparity maps. |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: 'Figure 9: Four images, collected in-house and used to test $16$ state-of-the-art
    methods. The green masks on some of the left images highlight the pixels where
    the ground-truth disparity is available. The disparity range is shown in pixels
    while the depth range is in meters.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: We have tested $16$ stereo-based methods published in $9$ papers (between $2018$
    and $2019$), see below. We use the network weights as provided by the authors.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '(1) AnyNet [[88](#bib.bib88)]: It is a four-stages network, which builds 3D
    cost volumes in a coarse-to-fine manner. The first stage estimates a low resolution
    disparity map by searching on a small disparity range. The subsequent stages estimate
    refined disparity maps using residual learning.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '(2) DeepPruner [[83](#bib.bib83)]: It combines deep learning with PatchMatch [[101](#bib.bib101)]
    to speed up inference by adaptively pruning out the potentially large search space
    for correspondences. Two variants have been proposed: DeepPruner (Best), which
    downsamples the cost volume by a factor of $4$, and DeepPruner (Fast), which downsamples
    it by a factor of $8$.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: (3) DispNet3 [[75](#bib.bib75)], an improved version of DispNet [[22](#bib.bib22)]
    where occlusions and disparity maps are jointly estimated.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '(4) GANet [[85](#bib.bib85)]: It replaces a large number of the 3D convolutional
    layers in the regularization block with (1) two 3D convolutional layers, (2) a
    semi-global aggregation layer (SGA), and (3) a local guided aggregation layer
    (LGA). SGA and LGA layers capture local and whole-image cost dependencies. They
    are meant to improve the accuracy in challenging regions such as occlusions, large
    textureless/reflective regions, and thin structures.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '(5) HighResNet [[32](#bib.bib32)]: To refine both the spatial and the depth
    resolutions while operating on high resolution images, this method searches for
    correspondences incrementally using a coarse-to-fine hierarchy. Its hierarchical
    design also allows for anytime on-demand reports of disparity.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '(6) PSMNet [[64](#bib.bib64)]: It progressively regularizes a low resolution
    4D cost volume, estimated from a pyramid of features.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '(7) iResNet [[63](#bib.bib63)]: The initial disparity and the learned features
    are used to calculate a feature constancy map, which measures the correctness
    of the stereo matching. The initial disparity map and the feature constancy map
    are then fed into a sub-network for disparity refinement.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '(8) UnsupAdpt [[129](#bib.bib129)]: It is an unsupervised adaptation approach
    that enables fine-tuning without any ground-truth information. It first trains
    DispNet-Corr1D [[22](#bib.bib22)] using the KITTI 2012 training dataset and then
    adapts the network to KITTI2015 and Middlebury 2014.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '(9) SegStereo [[68](#bib.bib68)]: It is an unsupervised disparity estimation
    method, which uses segmentation masks to guide the disparity estimation. Both
    segmentation and disparity maps are jointly estimated with an end-to-end network.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods (1) to (7) are supervised with ground-truth depth maps while the
    methods (8) and (9) are self-supervised. We compare their accuracy at runtime
    using the overall Root Mean Square Error (RMSE) defined as:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{RMSE}^{2}_{\text{linear}}={\frac{1}{N}\sum_{N}{&#124;0pt_{i}-\hat{0}pt_{i}&#124;}^{2}},$
    |  | (16) |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: and the Bad-n error defined as the percentage of pixels whose estimated disparity
    deviates with more than $n$ pixels from the ground truth. We use $n\in\{0.5,1,2,3,4,5\}$.
    The Bad-n error considers the distribution and spread of the error and thus provides
    a better insight on the accuracy of the methods. In addition to accuracy, we also
    report the computation time and memory footprint at runtime.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Computation time and memory footprint
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE V: Computation time and memory consumption, at runtime, on images of
    size $640\times 480$. SegStereo [[68](#bib.bib68)] has been tested on a PC equipped
    with an Nvidia GeForce RTX 2080\. The other methods have been tested on a PC equipped
    with an Nvidia Tesla K40 GPU with a 12 Go graphic memory. See the Supplementary
    Material for a visual representation.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Supervision | Cost vol. | Time (s) | Memory (GB) | Training set
    | Baseline |  | Challenge |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| mode | Bkg | Fg | Bkg$+$Fg |  | Bkg | Fg | Bkg$+$Fg |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| AnyNet [[88](#bib.bib88)] | Supervised | 3D | $0.285$ | 0.232 | KITTI2015
    | 9.46 | 10.74 | 10.34 |  | 9.83 | 11.60 | 11.15 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | KITTI2012 | 9.80 | 10.29 | 10.20 |  | 9.34 | 10.62 | 10.61
    |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| DeepPruner (Best) [[83](#bib.bib83)] | Supervised | 3D | $8.430$ | $8.845$
    | KITTI2012$+$2015 | 9.64 | 9.43 | 9.46 |  | 12.38 | 8.74 | 10.48 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| DeepPruner (Fast) [[83](#bib.bib83)] | Supervised | 3D | $3.930$ | $6.166$
    | KITTI2012$+$2015 | 9.56 | 9.90 | 9.94 |  | 8.74 | 9.75 | 9.86 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| DispNet3 [[75](#bib.bib75)] | Supervised | 3D | $-$ | $10.953$ | CSS-ft-KITTI
    | 9.68 | 9.62 | 9.70 |  | 8.38 | 11.00 | 11.11 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | CSS-FlyingThings3D [[22](#bib.bib22)] | 9.11 | 9.64 | 9.54
    |  | 8.97 | 9.91 | 10.19 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | css-FlyingThings3D [[22](#bib.bib22)] | 9.29 | 9.98 | 9.87
    |  | 9.66 | 10.34 | 10.61 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| GANet [[85](#bib.bib85)] | Supervised | 4D | $8.336$ | 3.017 | KITTI2015
    | 9.55 | 9.38 | 9.39 |  | 9.37 | 9.50 | 9.89 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | KITTI2012 | 9.98 | 10.29 | 10.25 |  | 10.69 | 10.95 | 11.55
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: '| HighResNet [[32](#bib.bib32)] | Supervised | 4D | 0.037 | $0.474$ | Middleburry [[20](#bib.bib20)],
    KITTI2015 [[21](#bib.bib21)], | 9.47 | 9.91 | 9.94 |  | 8.58 | 9.64 | 9.78 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | ETH3D [[25](#bib.bib25)], HR-VS [[32](#bib.bib32)] |  |  |  |  |  |  |  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| PSMNet [[64](#bib.bib64)] | Supervised | 4D | $1.314$ | $1.900$ | KITTI2015
    | 9.88 | 9.81 | 9.80 |  | 10.10 | 9.42 | 9.93 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | KITTI2012 | 10.17 | 10.24 | 10.29 |  | 10.66 | 10.33 | 11.00
    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
- en: '| iResNet [[63](#bib.bib63)] | Supervised | 3D | $0.939$ | $7.656$ | KITTI2015
    | 60.04 | 61.72 | 60.54 |  | 45.87 | 46.85 | 47.86 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | ROB [[152](#bib.bib152)] | 22.08 | 17.16 | 18.08 |  | 23.01
    | 16.51 | 18.83 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| UnsupAdpt [[129](#bib.bib129)] | Self-supervised | 3D | $-$ | $-$ | KITTI2012
    adapted to KITTI2015 | 9.44 | 10.39 | 10.19 |  | 10.10 | 10.42 | 10.78 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | Shadow-on-Truck | 8.52 | 10.08 | 9.58 |  | 10.66 | 10.88 |
    10.27 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
- en: '| SegStereo [[68](#bib.bib68)] | Self-supervised | 3D | $0.195$ | $\sim 12.00$
    | CityScapes [[23](#bib.bib23)] | 9.26 | 10.30 | 10.17 |  | 9.03 | 10.49 | 10.54
    |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: 'From Table [V](#S8.T5 "TABLE V ‣ 8.2 Computation time and memory footprint
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation"), we can distinguish three types of methods; slow methods, *e.g.,*
    PSMNet [[64](#bib.bib64)], DeepPruner (Best) and (Fast) [[83](#bib.bib83)], and
    GANet [[85](#bib.bib85)], require more than $1$ second to estimate one disparity
    map. They also require between $3$GB and $10$GB (for DispNet3 [[75](#bib.bib75)])
    of memory at runtime. As such, these methods are very hard to deploy on mobile
    platforms. Average-speed methods, *e.g.,* AnyNet [[88](#bib.bib88)] and iResNet [[63](#bib.bib63)],
    produce a disparity map in around one second. Finally, fast methods, *e.g.,* HighResNet [[32](#bib.bib32)],
    require less than $0.1$ seconds. In general, methods that use 3D cost volumes
    are faster and less memory demanding than those that use 4D cost volumes. There
    are, however, two exceptions: iResNet [[63](#bib.bib63)] and DeepPruner [[83](#bib.bib83)],
    which use 3D cost volumes but require a large amount of memory at runtime. While
    iResNet requires less than a second to process images of size $0pt=640,0pt=480$,
    since it uses 2D convolutions to regularize the cost volume, DeepPruner [[83](#bib.bib83)]
    requires more than $3$ seconds. We also observe that HighResNet [[32](#bib.bib32)],
    which uses 4D cost volumes but adopts a hierarchical approach to produce disparity
    on demand, is very efficient in terms of computation time as it only requires
    $37$ms, which is almost $8$ times faster than AnyNet [[88](#bib.bib88)], which
    uses 3D cost volumes. Note also that AnyNet [[88](#bib.bib88)] can run on mobile
    devices due to its memory efficiency.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Reconstruction accuracy
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1316059820018b62c7c95008b95d2a09.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Overall Bad-n error, $n\in[0.5,5.0]$ on a selection of $141$ (baseline)
    images from the stereo vision challenge of ApolloScape dataset [[34](#bib.bib34)].
    A similar behaviour is observed on the challenge subset, see the supplementary
    material. The horizontal axis is the error $n$ while the vertical axis is the
    percentage of pixels whose estimated disparity deviates with more than $n$ pixels
    from the ground truth.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Table [V](#S8.T5 "TABLE V ‣ 8.2 Computation time and memory footprint ‣ 8 Discussion
    and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    shows the average RMSE of each of the methods described in Section [8.1](#S8.SS1
    "8.1 Evaluation protocol ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation"). We report the results on a baseline
    subset composed of $141$ images that look more or less like KITTI2012 images,
    hereinafter referred to as *baseline*, and on another subset composed of $33$
    images with challenging lighting conditions, hereinafter referred to as *challenge*.
    Here, we focus on the relative comparison across methods since some of the high
    errors observed might be attributed to the way the ground-truth has been acquired
    in ApolloScape [[34](#bib.bib34)] dataset, rather than to the methods themselves.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: We observe that these methods behave almost equally on the two subsets. However,
    the reconstruction error, is significantly important, $>8$ pixels, compared to
    the errors reported on standard datasets such as KITTI2012 and KITTI2015\. This
    suggests that, when there is a significant domain gap between training and testing
    then the reconstruction accuracy can be significantly affected.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: We also observe the same trend on the Bad-n curves of Fig. [10](#S8.F10 "Figure
    10 ‣ 8.3 Reconstruction accuracy ‣ 8 Discussion and comparison ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation") where, in all methods,
    more than $25\%$ of the pixels had a reconstruction error that is larger than
    $5$ pixels. The Bad-n curves show that the errors are large on the foreground
    pixels, *i.e.,* pixels that correspond to cars, with more than $55\%$ of the pixels
    having an error that is larger than $3$ pixels (against $35\%$ on the background
    pixels). Interestingly, Table [V](#S8.T5 "TABLE V ‣ 8.2 Computation time and memory
    footprint ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") and Fig. [10](#S8.F10 "Figure 10 ‣ 8.3 Reconstruction
    accuracy ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") show that most of the methods achieve similar
    reconstruction accuracies. The only exception is iResNet [[63](#bib.bib63)] trained
    on Kitti2015 and on ROB [[152](#bib.bib152)], which had more than $90\%$, respectively
    $55\%$, of pixels with an error that is larger than $5$ pixels. In all methods,
    less than $5\%$ of the pixels had an error that is less than $2$ pixels. This
    suggests that achieving sub-pixel accuracy remains an important challenge for
    future research.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: Note that SegStereo [[68](#bib.bib68)], which is self-supervised, achieves a
    similar or better performance than many of the supervised methods. Also, the unsupervised
    self-adaptation method of Tonioni *et al.* [[129](#bib.bib129)], which takes the
    baseline DispNet-Corr1D network [[22](#bib.bib22)] trained on KITTI 2012 and adapts
    it to KITTI2015 and Middlebury 2014, achieves one of the best performances on
    the foreground regions.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/ce7eb8921bd7d9014164e59d8522b055.png) | ![Refer to
    caption](img/a75310f3dd91c3d1680007cc9da1fadd.png) | ![Refer to caption](img/184bf118de495b271b498737930021a8.png)
    | ![Refer to caption](img/7bb0f3f9377a9116e885398debb15ba5.png) | ![Refer to caption](img/040764ab56d19d299b0a48e640e6fe50.png)
    | ![Refer to caption](img/12a94cf3469c6f6583735f3bbef27630.png) |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| AnyNet (Kitti2012) | AnyNet (Kitti2015) | DeepPruner (fast) | DispNet3 (css)
    | GANet (Kittit2015) | Unsup.Adapt (Kitti) |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/7bf58590d3eee444a74b1fee64ef90dc.png) | ![Refer to
    caption](img/a9d106d7a62b3f2a75297fc7679f43db.png) | ![Refer to caption](img/a3922c235ee795402226f7f815f08018.png)
    | ![Refer to caption](img/d1151e3ec50e7d1d8ec3a5f39050a5ff.png) | ![Refer to caption](img/f949650509aabf375bd4ba4f41026bbb.png)
    | ![Refer to caption](img/d0ce84bec16fccc1d77342dde6e63853.png) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| iResNet (Kitti2015) | iResNet (ROB) | PSMNet (Kitti2012) | PSMNet (Kitti2015)
    | SegStereo | Unsup.Adapt |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | (shadowsontruck) |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| (a) Results on the images of Fig. [8](#S8.F8 "Figure 8 ‣ 8.1 Evaluation protocol
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(a). |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/67a0d29509cdd07efde38d0677e8dd04.png) | ![Refer to
    caption](img/3774f5aab19b7b9e75fd558ef8c970fb.png) | ![Refer to caption](img/6d2c5dccb69cfbae1bad33ef48661acf.png)
    | ![Refer to caption](img/749d43771444e18c6c704f47c1cca416.png) | ![Refer to caption](img/23a56666ed3d929a3f60d98d6cfc1cd9.png)
    | ![Refer to caption](img/3bfbf8e8371d8e7a113dafe22a5cdfd6.png) |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| AnyNet (Kitti2012) | AnyNet (Kitti2015) | DeepPruner (fast) | DispNet3 (css)
    | GANet (Kittit2015) | Unsup.Adapt (Kitti) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/6fd1c2367f38981d4bf6c1f530e4a637.png) | ![Refer to
    caption](img/a14ea61e1f272980068d03bd31863822.png) | ![Refer to caption](img/14cba94081c3ddcfec657925edfbebe6.png)
    | ![Refer to caption](img/2ccf61259b936e000f474ff36c82c554.png) | ![Refer to caption](img/88583835d361a976e0b12266fcd5a4e0.png)
    | ![Refer to caption](img/efeac9434b2526ebf78e9310ddeadc03.png) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| iResNet (Kitti2015) | iResNet (ROB) | PSMNet (Kitti2012) | PSMNet (Kitti2015)
    | SegStereo | Unsup.Adapt |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  | (shadowsontruck) |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
- en: '| (b) Results on the images of Fig. [8](#S8.F8 "Figure 8 ‣ 8.1 Evaluation protocol
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(b). |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: 'Figure 11: Pixel-wise errors between the ground-truth disparities and the disparities
    estimated from the images of Fig. [8](#S8.F8 "Figure 8 ‣ 8.1 Evaluation protocol
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation").'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the visual quality of the estimated disparities, see Fig. [11](#S8.F11
    "Figure 11 ‣ 8.3 Reconstruction accuracy ‣ 8 Discussion and comparison ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation"), we observe that
    most of the methods were able to recover the overall shape of trees but fail to
    reconstruct the details especially the leaves. The reconstruction errors are high
    in flat areas and around object boundaries. Also, highly reflective materials
    and poor lighting conditions remain a big challenge to these methods as shown
    in Fig. [11](#S8.F11 "Figure 11 ‣ 8.3 Reconstruction accuracy ‣ 8 Discussion and
    comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(b).
    The supplementary material provides more results on the four stereo pairs of Fig. [9](#S8.F9
    "Figure 9 ‣ 8.1 Evaluation protocol ‣ 8 Discussion and comparison ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation").
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: 9 Future research directions
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning methods for stereo-based depth estimation have achieved promising
    results. The topic, however, is still in its infancy and further developments
    are yet to be expected. In this section, we present some of the current issues
    and highlight directions for future research.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: (1) Camera parameters. Most of the stereo-based techniques surveyed in this
    article require rectified images. Multi-view stereo techniques use Plane-Sweep
    Volumes or back-projected images/features. Both image rectification and PSVs require
    known camera parameters, which are challenging to estimate in the wild. Many papers
    attempted to address this problem for monocular depth estimation and for 3D shape
    reconstruction by jointly optimising for the camera parameters and the geometry
    of the 3D scene [[153](#bib.bib153)].
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: (2) Lighting conditions and complex material properties. Poor lighting conditions
    and complex materials properties remain a challenge to most of the current methods,
    see for example Fig. [11](#S8.F11 "Figure 11 ‣ 8.3 Reconstruction accuracy ‣ 8
    Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(b). Combining object recognition, high-level scene understanding,
    and low-level feature learning can be one promising avenue to address these issues.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: (3) Spatial and depth resolution. Most of the current techniques do not handle
    high resolution input images and generally produce depth maps of low spatial and
    depth resolution. Depth resolution is particularly limited, making the methods
    unable to reconstruct thin structures, *e.g.,* vegetation and hair, and structures
    located at a far distance from the camera. Although refinement modules can improve
    the resolution of the estimated depth maps, the gain is still small compared to
    the resolution of the input images. This has recently been addressed using hierarchical
    techniques, which allow on-demand reports of disparity by capping the resolution
    of the intermediate results [[32](#bib.bib32)]. In these methods, low resolution
    depth maps can be produced in realtime, and thus can be used on mobile platforms,
    while high resolution maps would require more computation time. Producing, in
    realtime, accurate maps of high spatial and depth resolutions remains a challenge
    for future research.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: (4) Realtime processing. Most deep learning methods for disparity estimation
    use 3D and 4D cost volumes, which are processed and regularized using 2D and 3D
    convolutions. They are expensive in terms of memory requirements and processing
    time. Developing lightweight, and subsequently fast, end-to-end deep networks
    remains a challenging avenue for future research.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: (5) Disparity range. Existing techniques uniformly discretize the disparity
    range. This results in multiple issues. In particular, although the reconstruction
    error can be small in the disparity space, it can result in an error of meters
    in the depth space, especially at far ranges. One way to mitigate this is by discritizing
    disparity and depth uniformly in the log space. Also, changing the disparity range
    requires retraining the networks. Treating depth as a continuum could be one promising
    avenue for future research.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: (6) Training. Deep networks heavily rely on the availability of training images
    annotated with ground-truth labels. This is very expensive and labor intensive
    for depth/disparity reconstruction. As such, the performance of the methods and
    their generalization ability can significantly be affected including the risk
    of overfitting the models to specific domains. Existing techniques mitigate this
    problem by either designing loss functions that do not require 3D annotations,
    or by using domain adaptation and transfer learning strategies. The former, however,
    requires calibrated cameras. Domain adaptation techniques, especially unsupervised
    ones [[138](#bib.bib138)], are recently attracting more attention since, with
    these techniques, one can train with both synthetic data, which are easy to obtain,
    and real-world data. They also adapt, in an unsupervised manner and at run-time
    to ever-changing environments as soon as new images are gathered. Early results
    are very encouraging and thus expect in the future to see the emergence of large
    datasets, similar to ImageNet but for 3D reconstruction.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: (7) Automatically learning the network architecture, its activation functions,
    and its parameters from data. Most existing research has focused on designing
    novel network architectures and novel training methods for optimizing their parameters.
    It is only recently that some papers started to focus on automatically learning
    optimal architectures. Early attempts, *e.g.,*  [[149](#bib.bib149)], focus on
    simple architectures. We expect in the future to see more research on automatically
    learning complex disparity estimation architectures and their activation functions,
    using, for example, the neuro-evolution theory [[154](#bib.bib154), [155](#bib.bib155)],
    which will free the need for manual network design.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: 10 Conclusion
  id: totrans-485
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper provides a comprehensive survey of the recent developments in stereo-based
    depth estimation using deep learning techniques. Despite their infancy, these
    techniques are achieving state-of-the-art results. Since 2014, we have entered
    a new era where data-driven and machine learning techniques play a central role
    in image-based depth reconstruction. We have seen that, from $2014$ to $2019$,
    more than $150$ papers on the topic have been published in the major computer
    vision, computer graphics, and machine learning conferences and journals. Even
    during the final stages of this submission, more new papers are being published
    making it difficult to keep track of the recent developments, and more importantly,
    understand their differences and similarities, especially for new comers to the
    field. This timely survey can thus serve as a guide to the reader to navigate
    this fast-growing field of research.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there are several related topics that have not been covered in this
    survey. Examples include image-based 3D object reconstruction using deep learning,
    which has been recently surveyed by Han *et al.* [[153](#bib.bib153)], and monocular
    and video-based depth estimation, which requires a separate survey paper given
    the large amount of papers that have been published on the topic in the past $5$
    to $6$ years. Other topics include photometric stereo and active stereo [[156](#bib.bib156),
    [157](#bib.bib157)], which are outside the scope of this paper.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements. We would like to thank all the authors of the reference papers
    who have made their codes and datasets publicly available. This work is supported
    in part by Murdoch University’s Vice Chancellor’s Small Steps of Innovation Funding
    Program, and by ARC DP150100294 and DP150104251.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] S. Khan, H. Rahmani, S. A. A. Shah, and M. Bennamoun, “A guide to convolutional
    neural networks for computer vision,” *Synthesis Lectures on Computer Vision*,
    vol. 8, no. 1, pp. 1–207, 2018.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] B. Li, C. Shen, Y. Dai, A. Van Den Hengel, and M. He, “Depth and surface
    normal estimation from monocular images using regression on deep features and
    hierarchical CRFs,” in *IEEE CVPR*, 2015, pp. 1119–1127.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic
    labels with a common multi-scale convolutional architecture,” in *IEEE ICCV*,
    2015, pp. 2650–2658.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] R. Garg, V. K. BG, G. Carneiro, and I. Reid, “Unsupervised CNN for single
    view depth estimation: Geometry to the rescue,” in *ECCV*, 2016, pp. 740–756.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] F. Liu, C. Shen, G. Lin, and I. D. Reid, “Learning Depth from Single Monocular
    Images Using Deep Convolutional Neural Fields,” *IEEE PAMI*, vol. 38, no. 10,
    pp. 2024–2039, 2016.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. Yang, S. E. Reed, M.-H. Yang, and H. Lee, “Weakly-supervised disentangling
    with recurrent transformations for 3D view synthesis,” in *NIPS*, 2015, pp. 1099–1107.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum, “Deep convolutional
    inverse graphics network,” in *NIPS*, 2015, pp. 2539–2547.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Multi-view 3D models from
    single images with a convolutional network,” in *ECCV*, 2016, pp. 322–337.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros, “View synthesis
    by appearance flow,” in *ECCV*, 2016, pp. 286–301.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C. Berg, “Transformation-grounded
    image generation network for novel 3D view synthesis,” in *IEEE CVPR*, 2017, pp.
    702–711.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame
    stereo correspondence algorithms,” *IJCV*, vol. 47, no. 1-3, pp. 7–42, 2002.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single
    image using a multi-scale deep network,” in *NIPS*, 2014, pp. 2366–2374.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] X. Wang, D. Fouhey, and A. Gupta, “Designing deep networks for surface
    normal estimation,” in *IEEE CVPR*, 2015, pp. 539–547.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. Saxena, M. Sun, and A. Y. Ng, “Make3d: Learning 3d scene structure
    from a single still image,” *IEEE PAMI*, vol. 31, no. 5, pp. 824–840, 2009.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *IEEE CVPR*, 2012, pp. 3354–3361.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic
    open source movie for optical flow evaluation,” in *ECCV*, 2012, pp. 611–625.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” *ECCV*, pp. 746–760, 2012.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark
    for the evaluation of rgb-d slam systems,” in *IROS*, 2012, pp. 573–580.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Xiao, A. Owens, and A. Torralba, “Sun3d: A database of big spaces reconstructed
    using sfm and object labels,” in *IEEE ICCV*, 2013, pp. 1625–1632.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] D. Scharstein, H. Hirschmüller, Y. Kitajima, G. Krathwohl, N. Nešić, X. Wang,
    and P. Westling, “High-resolution stereo datasets with subpixel-accurate ground
    truth,” in *German conference on pattern recognition*, 2014, pp. 31–42.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. Menze and A. Geiger, “Object scene flow for autonomous vehicles,” in
    *IEEE CVPR*, 2015, pp. 3061–3070.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
    and T. Brox, “A large dataset to train convolutional networks for disparity, optical
    flow, and scene flow estimation,” in *IEEE CVPR*, 2016, pp. 4040–4048.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *IEEE CVPR*, 2016, pp. 3213–3223.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, “Large-scale
    data for multiple-view stereopsis,” *IJCV*, vol. 120, no. 2, pp. 153–168, 2016.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] T. Schöps, J. L. Schönberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys,
    and A. Geiger, “A multi-view stereo benchmark with high-resolution images and
    multi-camera videos,” in *IEEE CVPR*, 2017, pp. 3260–3269.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, “Semantic
    scene completion from a single depth image,” in *IEEE CVPR*, 2017, pp. 1746–1754.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, “DeepMVS:
    Learning Multi-view Stereopsis,” in *IEEE CVPR*, 2018, pp. 2821–2830.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Z. Li and N. Snavely, “MegaDepth: Learning Single-View Depth Prediction
    From Internet Photos,” in *IEEE CVPR*, 2018, pp. 2041–2050.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Jeon and S. Lee, “Reconstruction-based Pairwise Depth Dataset for Depth
    Image Enhancement Using CNN,” in *ECCV*, 2018, pp. 422–438.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] C. Won, J. Ryu, and J. Lim, “OmniMVS: End-to-End Learning for Omnidirectional
    Stereo Matching,” in *IEEE ICCV*, 2019, pp. 8987–8996.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] ——, “End-to-End Learning for Omnidirectional Stereo Matching with Uncertainty
    Prior,” *IEEE PAMI*, 2020.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] G. Yang, J. Manela, M. Happold, and D. Ramanan, “Hierarchical Deep Stereo
    Matching on High-Resolution Images,” in *IEEE CVPR*, 2019, pp. 5515–5524.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] G. Yang, X. Song, C. Huang, Z. Deng, J. Shi, and B. Zhou, “Drivingstereo:
    A large-scale dataset for stereo matching in autonomous driving scenarios,” in
    *IEEE CVPR*, 2019, pp. 899–908.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, “The apolloscape
    open dataset for autonomous driving and its application,” *IEEE transactions on
    pattern analysis and machine intelligence*, 2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung,
    L. Hauswald, V. H. Pham, M. Mühlegg, S. Dorn *et al.*, “A2d2: Audi autonomous
    driving dataset,” *arXiv preprint arXiv:2004.06320*, 2020.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] N. Mayer, E. Ilg, P. Fischer, C. Hazirbas, D. Cremers, A. Dosovitskiy,
    and T. Brox, “What makes good synthetic training data for learning disparity and
    optical flow estimation?” *International Journal of Computer Vision*, vol. 126,
    no. 9, pp. 942–960, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Zagoruyko and N. Komodakis, “Learning to compare image patches via
    convolutional neural networks,” in *IEEE CVPR*, 2015, pp. 4353–4361.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg, “MatchNet: Unifying
    feature and metric learning for patch-based matching,” in *IEEE CVPR*, 2015, pp.
    3279–3286.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] J. Zbontar and Y. LeCun, “Computing the stereo matching cost with a convolutional
    neural network,” in *IEEE CVPR*, 2015, pp. 1592–1599.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] W. Chen, X. Sun, L. Wang, Y. Yu, and C. Huang, “A deep visual correspondence
    embedding model for stereo matching costs,” in *IEEE ICCV*, 2015, pp. 972–980.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer,
    “Discriminative learning of deep convolutional feature point descriptors,” in
    *IEEE ICCV*, 2015, pp. 118–126.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Zbontar and Y. LeCun, “Stereo matching by training a convolutional
    neural network to compare image patches,” *Journal of Machine Learning Research*,
    vol. 17, no. 1-32, p. 2, 2016.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] V. Balntas, E. Johns, L. Tang, and K. Mikolajczyk, “PN-Net: Conjoined
    Triple Deep Network for Learning Local Image Descriptors,” *CoRR*, 2016.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] W. Luo, A. G. Schwing, and R. Urtasun, “Efficient deep learning for stereo
    matching,” in *IEEE CVPR*, 2016, pp. 5695–5703.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] B. Kumar, G. Carneiro, I. Reid *et al.*, “Learning local image descriptors
    with deep siamese and triplet convolutional networks by minimising global loss
    functions,” in *IEEE CVPR*, 2016, pp. 5385–5394.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] A. Shaked and L. Wolf, “Improved stereo matching with constant highway
    networks and reflective confidence learning,” in *IEEE CVPR*, 2017, pp. 4641–4650.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] W. Hartmann, S. Galliani, M. Havlena, L. Van Gool, and K. Schindler, “Learned
    multi-patch similarity,” in *IEEE ICCV*, 2017, pp. 1595–1603.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] H. Park and K. M. Lee, “Look wider to match image patches with convolutional
    neural networks,” *IEEE Signal Processing Letters*, vol. 24, no. 12, pp. 1788–1792,
    2017.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. Ye, J. Li, H. Wang, H. Huang, and X. Zhang, “Efficient stereo matching
    leveraging deep local and context information,” *IEEE Access*, vol. 5, pp. 18 745–18 755,
    2017.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Tulyakov, A. Ivanov, and F. Fleuret, “Weakly supervised learning of
    deep metrics for stereo reconstruction,” in *IEEE ICCV*, 2017, pp. 1339–1348.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, and T. Brox, “FlowNet: Learning optical flow with
    convolutional networks,” in *IEEE ICCV*, 2015, pp. 2758–2766.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, “Deep Ordinal Regression
    Network for Monocular Depth Estimation,” in *IEEE CVPR*, June 2018.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Sukhbaatar and R. Fergus, “Learning from noisy labels with deep neural
    networks,” *ICLR Workshop*, 2014.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] P. Fischer, A. Dosovitskiy, and T. Brox, “Descriptor matching with convolutional
    neural networks: a comparison to sift,” *CoRR*, 2014.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] H. Hirschmuller, “Stereo processing by semiglobal matching and mutual
    information,” *IEEE PAMI*, vol. 30, no. 2, pp. 328–341, 2008.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Seki and M. Pollefeys, “SGM-Nets: Semi-global matching with neural
    networks,” in *IEEE CVPR Workshops*, 2017, pp. 21–26.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] J. L. Schönberger, S. N. Sinha, and M. Pollefeys, “Learning to Fuse Proposals
    from Multiple Scanline Optimizations in Semi-Global Matching,” in *Proceedings
    of ECCV*, 2018, pp. 739–755.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] M. Poggi and S. Mattoccia, “Learning a general-purpose confidence measure
    based on o(1) features and a smarter aggregation strategy for semi global matching,”
    in *3DV*, 2016, pp. 509–518.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] X. Hu and P. Mordohai, “A quantitative evaluation of confidence measures
    for stereo vision,” *IEEE PAMI*, vol. 34, no. 11, pp. 2121–2133, 2012.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] A. Kar, C. Häne, and J. Malik, “Learning a multi-view stereo machine,”
    in *NIPS*, 2017, pp. 364–375.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach,
    and A. Bry, “End-to-end learning of geometry and context for deep stereo regression,”
    *IEEE ICCV*, pp. 66–75, 2017.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade residual learning:
    A two-stage convolutional neural network for stereo matching,” in *ICCV Workshops*,
    vol. 7, no. 8, 2017.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Z. Liang, Y. Feng, Y. G. H. L. W. Chen, and L. Q. L. Z. J. Zhang, “Learning
    for Disparity Estimation Through Feature Constancy,” in *IEEE CVPR*, 2018, pp.
    2811–2820.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Chang and Y. Chen, “Pyramid Stereo Matching Network,” *IEEE CVPR*,
    pp. 5410–5418, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] G.-Y. Nie, M.-M. Cheng, Y. Liu, Z. Liang, D.-P. Fan, Y. Liu, and Y. Wang,
    “Multi-Level Context Ultra-Aggregation for Stereo Matching,” in *IEEE CVPR*, 2019,
    pp. 3283–3291.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Flynn, I. Neulander, J. Philbin, and N. Snavely, “DeepStereo: Learning
    to predict new views from the world’s imagery,” in *IEEE CVPR*, 2016, pp. 5515–5524.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *IEEE CVPR*, 2016, pp. 770–778.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, “SegStereo: Exploiting
    semantic information for disparity estimation,” in *ECCV*, 2018, pp. 636–651.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] F. Yu, D. Wang, E. Shelhamer, and T. Darrell, “Deep layer aggregation,”
    in *IEEE CVPR*, 2018, pp. 2403–2412.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Y. Zhong, Y. Dai, and H. Li, “Self-supervised learning for stereo matching
    with self-improving ability,” *arXiv:1709.00930*, 2017.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] P. Knöbelreiter, C. Reinbacher, A. Shekhovtsov, and T. Pock, “End-to-end
    training of hybrid CNN-CRF models for stereo,” in *IEEE CVPR*, 2017, pp. 1456–1465.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, and S. Izadi,
    “StereoNet: Guided hierarchical refinement for real-time edge-aware depth prediction,”
    *ECCV*, 2018.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich,
    M. Schoenberg, S. Izadi, T. Funkhouser, and S. Fanello, “ActiveStereoNet: end-to-end
    self-supervised learning for active stereo systems,” *ECCV*, pp. 784–801, 2018.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Z. Jie, P. Wang, Y. Ling, B. Zhao, Y. Wei, J. Feng, and W. Liu, “Left-Right
    Comparative Recurrent Model for Stereo Matching,” in *IEEE CVPR*, 2018, pp. 3838–3846.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] E. Ilg, T. Saikia, M. Keuper, and T. Brox, “Occlusions, Motion and Depth
    Boundaries with a Generic Network for Disparity, Optical Flow or Scene Flow Estimation,”
    in *ECCV*, 2018, pp. 614–630.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Song, X. Zhao, H. Hu, and L. Fang, “EdgeStereo: A Context Integrated
    Residual Pyramid Network for Stereo Matching,” *ACCV*, 2018.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] L. Yu, Y. Wang, Y. Wu, and Y. Jia, “Deep Stereo Matching with Explicit
    Cost Aaggregation Sub-architecture,” *AAAI*, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] S. Tulyakov, A. Ivanov, and F. Fleuret, “Practical Deep Stereo (PDS):
    Toward applications-friendly deep stereo matching,” *NIPS*, pp. 5871–5881, 2018.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Z. Wu, X. Wu, X. Zhang, S. Wang, and L. Ju, “Semantic Stereo Matching
    With Pyramid Cost Volumes,” in *IEEE ICCV*, 2019, pp. 7484–7493.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Z. Yin, T. Darrell, and F. Yu, “Hierarchical Discrete Distribution Decomposition
    for Match Density Estimation,” in *IEEE CVPR*, 2019, pp. 6044–6053.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] R. Chabra, J. Straub, C. Sweeney, R. Newcombe, and H. Fuchs, “StereoDRNet:
    Dilated Residual StereoNet,” in *IEEE CVPR*, 2019, pp. 11 786–11 795.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C.-W. Xie, H.-Y. Zhou, and J. Wu, “Vortex pooling: Improving context representation
    in semantic segmentation,” *arXiv:1804.06242*, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Duggal, S. Wang, W.-C. Ma, R. Hu, and R. Urtasun, “DeepPruner: Learning
    Efficient Stereo Matching via Differentiable PatchMatch,” in *IEEE ICCV*, 2019,
    pp. 4384–4393.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] A. Tonioni, F. Tosi, M. Poggi, S. Mattoccia, and L. D. Stefano, “Real-time
    self-adaptive deep stereo,” in *IEEE CVPR*, 2019, pp. 195–204.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, “GA-Net: Guided Aggregation
    Net for End-to-End Stereo Matching,” in *IEEE CVPR*, 2019, pp. 185–194.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, “Group-wise Correlation
    Stereo Network,” in *IEEE CVPR*, 2019, pp. 3273–3282.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] C. Chen, X. Chen, and H. Cheng, “On the Over-Smoothing Problem of CNN
    Based Disparity Estimation,” in *IEEE ICCV*, 2019, pp. 8997–9005.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Wang, Z. Lai, G. Huang, B. H. Wang, L. van der Maaten, M. Campbell,
    and K. Q. Weinberger, “Anytime stereo image depth estimation on mobile devices,”
    in *ICRA*, 2019, pp. 5893–5900.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang,
    and P. H. Torr, “Conditional random fields as recurrent neural networks,” in *IEEE
    CVPR*, 2015, pp. 1529–1537.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Y. Xue, J. Chen, W. Wan, Y. Huang, C. Yu, T. Li, and J. Bao, “MVSCRF:
    Learning Multi-View Stereo With Conditional Random Fields,” in *IEEE ICCV*, 2019,
    pp. 4312–4321.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] D. Paschalidou, O. Ulusoy, C. Schmitt, L. Van Gool, and A. Geiger, “RayNet:
    Learning Volumetric 3D Reconstruction With Ray Potentials,” in *IEEE CVPR*, June
    2018, pp. 3897–3906.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. Yao, Z. Luo, S. Li, T. Shen, T. Fang, and L. Quan, “Recurrent MVSNet
    for High-Resolution Multi-View Stereo Depth Inference,” in *IEEE CVPR*, 2019,
    pp. 5525–5534.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, “MVSNet: Depth Inference
    for Unstructured Multi-view Stereo,” *ECCV*, pp. 767–783, 2018.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] J.-H. Lee, M. Heo, K.-R. Kim, and C.-S. Kim, “Single-Image Depth Estimation
    Based on Fourier Domain Analysis,” in *IEEE CVPR*, June 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] T. Brox and J. Malik, “Large displacement optical flow: descriptor matching
    in variational motion estimation,” *IEEE PAMI*, vol. 33, no. 3, pp. 500–513, 2011.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] P. Krähenbühl and V. Koltun, “Efficient inference in fully connected crfs
    with gaussian edge potentials,” in *NIPS*, 2011, pp. 109–117.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] X. Sun, X. Mei, S. Jiao, M. Zhou, Z. Liu, and H. Wang, “Real-time local
    stereo via edge-aware disparity propagation,” *Pattern Recognition Letters*, vol. 49,
    pp. 201–206, 2014.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, and J. Kautz, “Learning
    affinity via spatial propagation networks,” in *NIPS*, 2017, pp. 1520–1530.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] X. Cheng, P. Wang, and R. Yang, “Depth Estimation via Affinity Learned
    with Convolutional Spatial Propagation Network,” in *ECCV*, 2018, pp. 103–119.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Imran, Y. Long, X. Liu, and D. Morris, “Depth Coefficients for Depth
    Completion,” in *IEEE CVPR*, June 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman, “Patchmatch:
    A randomized correspondence algorithm for structural image editing,” in *ACM TOG*,
    vol. 28, no. 3, 2009, p. 24.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. Seki and M. Pollefeys, “Patch Based Confidence Prediction for Dense
    Disparity Map,” in *BMVC*, vol. 2, no. 3, 2016, p. 4.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Gidaris and N. Komodakis, “Detect, Replace, Refine: Deep structured
    prediction for pixel wise labeling,” in *IEEE CVPR*, 2017, pp. 5248–5257.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] R. Haeusler, R. Nair, and D. Kondermann, “Ensemble learning for confidence
    measures in stereo vision,” in *IEEE CVPR*, 2013, pp. 305–312.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] A. Spyropoulos, N. Komodakis, and P. Mordohai, “Learning to detect ground
    control points for improving the accuracy of stereo matching,” in *IEEE CVPR*,
    2014, pp. 1621–1628.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] M.-G. Park and K.-J. Yoon, “Leveraging stereo matching with learning-based
    confidence measures,” in *IEEE CVPR*, 2015, pp. 101–109.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M. Poggi and S. Mattoccia, “Learning from scratch a confidence measure,”
    in *BMVC*, 2016.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] A. S. Wannenwetsch, M. Keuper, and S. Roth, “Probflow: Joint optical
    flow and uncertainty estimation,” in *IEEE ICCV*, 2017, pp. 1173–1182.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] K. Batsos, C. Cai, and P. Mordohai, “CBMV: A Coalesced Bidirectional
    Matching Volume for Disparity Estimation,” *IEEE CVPR*, 2018.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M. Poggi, F. Tosi, and S. Mattoccia, “Quantitative evaluation of confidence
    measures in a machine learning world,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2017, pp. 5228–5237.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo,
    “Convolutional lstm network: A machine learning approach for precipitation nowcasting,”
    in *NIPS*, 2015, pp. 802–810.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] M. Poggi and S. Mattoccia, “Learning to predict stereo reliability enforcing
    local consistency of confidence maps,” in *IEEE CVPR*, 2017, pp. 2452–2461.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] J. Gast and S. Roth, “Lightweight probabilistic deep networks,” in *IEEE
    CVPR*, 2018, pp. 3369–3378.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] F. Tosi, M. Poggi, A. Benincasa, and S. Mattoccia, “Beyond local reasoning
    for stereo confidence estimation with deep learning,” in *ECCV*, 2018, pp. 319–334.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Y. Hou, J. Kannala, and A. Solin, “Multi-View Stereo by Temporal Nonparametric
    Fusion,” in *IEEE ICCV*, 2019, pp. 2651–2660.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, “SurfaceNet: an end-to-end
    3D neural network for multiview stereopsis,” *IEEE ICCV*, pp. 2307–2315, 2017.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] S. Choi, S. Kim, K. Park, and K. Sohn, “Learning Descriptor, Confidence,
    and Depth Estimation in Multi-view Stereo,” in *IEEE CVPR Workshops*, 2018, pp.
    276–282.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] V. Leroy, J.-S. Franco, and E. Boyer, “Shape reconstruction using volume
    sweeping and learned photoconsistency,” in *ECCV*, 2018, pp. 781–796.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] K. Luo, T. Guan, L. Ju, H. Huang, and Y. Luo, “P-MVSNet: Learning Patch-Wise
    Matching Confidence Aggregation for Multi-View Stereo,” in *IEEE ICCV*, 2019,
    pp. 10 452–10 461.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] K. Wang and S. Shen, “Mvdepthnet: real-time multiview depth estimation
    neural network,” in *2018 3DV*, 2018, pp. 248–257.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and
    T. Brox, “Demon: Depth and motion network for learning monocular stereo,” in *IEEE
    CVPR*, vol. 5, 2017, p. 6.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] J. T. Barron, “A more general robust loss function,” *arXiv:1701.03077*,
    2017.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and A. A. Efros, “Learning
    dense correspondence via 3D-guided cycle consistency,” in *IEEE CVPR*, 2016, pp.
    117–126.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] A. Ahmadi and I. Patras, “Unsupervised convolutional neural networks
    for motion estimation,” in *ICIP*, 2016, pp. 1629–1633.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] J. Y. Jason, A. W. Harley, and K. G. Derpanis, “Back to basics: Unsupervised
    learning of optical flow via brightness constancy and motion smoothness,” in *ECCV*,
    2016, pp. 3–10.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Bai, W. Luo, K. Kundu, and R. Urtasun, “Exploiting semantic information
    and deep matching for optical flow,” in *ECCV*, 2016, pp. 154–170.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised learning
    of depth and ego-motion from video,” in *IEEE CVPR*, vol. 2, no. 6, 2017, p. 7.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE TIP*, vol. 13,
    no. 4, pp. 600–612, 2004.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] A. Tonioni, M. Poggi, S. Mattoccia, and L. Di Stefano, “Unsupervised
    adaptation for deep stereo,” in *IEEE ICCV*, 2017, pp. 1614–1622.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Y. Kuznietsov, J. Stuckler, and B. Leibe, “Semi-supervised deep learning
    for monocular depth map prediction,” in *IEEE CVPR*, 2017, pp. 6647–6655.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] C. Zhou, H. Zhang, X. Shen, and J. Jia, “Unsupervised learning of stereo
    matching,” in *IEEE ICCV*, 2017, pp. 1567–1575.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki,
    “Sfm-net: Learning of structure and motion from video,” *arXiv:1704.07804*, 2017.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth
    estimation with left-right consistency,” in *CVPR*, vol. 2, no. 6, 2017, p. 7.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] M. Perriollat, R. Hartley, and A. Bartoli, “Monocular template-based
    reconstruction of inextensible surfaces,” *IJCV*, vol. 95, no. 2, pp. 124–137,
    2011.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] X. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia, “GeoNet: Geometric Neural
    Network for Joint Depth and Surface Normal Estimation,” in *IEEE CVPR*, June 2018.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *IEEE CVPR*, 2017, pp. 2881–2890.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] J. Pang, W. Sun, C. Yang, J. Ren, R. Xiao, J. Zeng, and L. Lin, “Zoom
    and learn: Generalizing deep stereo matching to novel domains,” in *IEEE CVPR*,
    2018, pp. 2070–2079.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Tonioni, M. Poggi, S. Mattoccia, and L. Di Stefano, “Unsupervised
    domain adaptation for depth prediction from images,” *IEEE TPAMI*, 2019.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] X. Zhou, Q. Huang, X. Sun, X. Xue, and Y. Wei, “Weakly-supervised transfer
    for 3d human pose estimation in the wild,” *arXiv:1704.02447*, 2017.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Z. Zhang, C. Xu, J. Yang, Y. Tai, and L. Chen, “Deep hierarchical guidance
    and regularization learning for end-to-end depth estimation,” *Pattern Recognition*,
    vol. 83, pp. 430–442, 2018.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M. Poggi, F. Aleotti, F. Tosi, and S. Mattoccia, “Towards real-time unsupervised
    monocular depth estimation on CPU,” in *IROS*, 2018, pp. 5848–5854.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Y. Zhong, H. Li, and Y. Dai, “Open-world stereo video matching with deep
    RNN,” in *ECCV*, 2018, pp. 101–116.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Tonioni, O. Rahnama, T. Joy, L. D. Stefano, T. Ajanthan, and P. H.
    Torr, “Learning to Adapt for Stereo,” in *IEEE CVPR*, 2019, pp. 9661–9670.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *Proceedings of the 34th International Conference
    on Machine Learning-Volume 70*, 2017, pp. 1126–1135.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] A. Atapour-Abarghouei and T. P. Breckon, “Real-Time Monocular Depth Estimation
    Using Synthetic Data With Domain Adaptation via Image Style Transfer,” in *IEEE
    CVPR*, 2018, pp. 2800–2810.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] C. Zheng, T.-J. Cham, and J. Cai, “T2Net: Synthetic-to-Realistic Translation
    for Solving Single-Image Depth Estimation Tasks,” in *ECCV*, 2018, pp. 767–783.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] S. Zhao, H. Fu, M. Gong, and D. Tao, “Geometry-Aware Symmetric Domain
    Adaptation for Monocular Depth Estimation,” in *IEEE CVPR*, 2019, pp. 9788–9798.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Nath Kundu, P. Krishna Uppala, A. Pahuja, and R. Venkatesh Babu, “AdaDepth:
    Unsupervised Content Congruent Adaptation for Depth Estimation,” in *IEEE CVPR*,
    2018, pp. 2656–2665.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] T. Saikia, Y. Marrakchi, A. Zela, F. Hutter, and T. Brox, “AutoDispNet:
    Improving Disparity Estimation With AutoML,” in *IEEE ICCV*, 2019, pp. 1812–1823.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] F. Hutter, L. Kotthoff, and J. Vanschoren, “Automated machine learning-methods,
    systems, challenges,” 2019.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture
    search,” in *International Conference on Learning Representations*, 2019.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] “Robust vision challenge,” *http://www.robustvision.net/*.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] X. Han, H. Laga, and M. Bennamoun, “Image-based 3d object reconstruction:
    State-of-the-art and trends in the deep learning era,” *IEEE PAMI*, 2020.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] K. O. Stanley, J. Clune, J. Lehman, and R. Miikkulainen, “Designing neural
    networks through neuroevolution,” *Nature Machine Intelligence*, vol. 1, no. 1,
    pp. 24–35, 2019.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] G. Bingham, W. Macke, and R. Miikkulainen, “Evolutionary optimization
    of deep learning activation functions,” *arXiv preprint arXiv:2002.07224*, 2020.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] B. Haefner, Z. Ye, M. Gao, T. Wu, Y. Queau, and D. Cremers, “Variational
    Uncalibrated Photometric Stereo Under General Lighting,” in *IEEE ICCV*, 2019.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Q. Zheng, Y. Jia, B. Shi, X. Jiang, L.-Y. Duan, and A. C. Kot, “SPLINE-Net:
    Sparse Photometric Stereo Through Lighting Interpolation and Normal Estimation
    Networks,” in *IEEE ICCV*, 2019.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/16e086e850516e7c4f2e78643ba70a3f.png) | Hamid
    Laga received the PhD degrees in Computer Science from Tokyo Institute of Technology
    in 2006\. He is currently an Associate Professor at Murdoch University (Australia)
    and an Adjunct Associate Professor with the Phenomics and Bioinformatics Research
    Centre (PBRC) of the University of South Australia. His research interests span
    various fields of machine learning, computer vision, computer graphics, and pattern
    recognition, with a special focus on the 3D reconstruction, modeling and analysis
    of static and deformable 3D objects, and on image analysis and big data in agriculture
    and health. He is the recipient of the Best Paper Awards at SGP2017, DICTA2012,
    and SMI2006. |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4a3320fd363a27b311e54d30ed236cfa.png) | Laurent
    Valentin Jospin Laurent Valentin Jospin is a young research student in the field
    of computer vision. His main research interests include 3d reconstruction, sampling
    and image acquisition strategies, computer vision applied to robotic navigation
    and computer vision applied to environmental sciences. Holder of a master of science
    in environmental engineering from EPFL since 2017 with a thesis on accuracy prediction
    in aerial mapping, his research career started as intern in two EPFL labs, publishing
    his first three papers in the process, before starting a PhD in computer science
    at the University of Western Australia in 2019\. His thesis project focus on real
    time 3D reconstruction with different computer vision techniques. |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/57419594ca27d67ab20d45120fcb016a.png) | Faird
    Boussaid received the M.S. and Ph.D. degrees in microelectronics from the National
    Institute of Applied Science (INSA), Toulouse, France, in 1996 and 1999 respectively.
    He joined Edith Cowan University, Perth, Australia, as a Postdoctoral Research
    Fellow, and a Member of the Visual Information Processing Research Group in 2000\.
    He joined the University of Western Australia, Crawley, Australia, in 2005, where
    he is currently a Professor. His current research interests include neuromorphic
    engineering, smart sensors, and machine learning. |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/084657efb02b745d94d8b2dee70502dd.png) | Mohammed
    Bennamoun is Winthrop Professor in the Department of Computer Science and Software
    Engineering at UWA and is a researcher in computer vision, machine/deep learning,
    robotics, and signal/speech processing. He has published 4 books (available on
    Amazon, 1 edited book, 1 Encyclopedia article, 14 book chapters, 120+ journal
    papers, 250+ conference publications, 16 invited and keynote publications. His
    h-index is 47 and his number of citations is 10,000+ (Google Scholar). He was
    awarded 65+ competitive research grants, from the Australian Research Council,
    and numerous other Government, UWA and industry Research Grants. He successfully
    supervised +26 PhD students to completion. He won the Best Supervisor of the Year
    Award at QUT (1998), and received award for research supervision at UWA (2008
    and 2016) and Vice-Chancellor Award for mentorship (2016). He delivered conference
    tutorials at major conferences, including: IEEE CVPR 2016, Interspeech 2014, IEEE
    ICASSP, and ECCV. He was also invited to give a Tutorial at an International Summer
    School on Deep Learning (DeepLearn 2017). |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
