- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:00:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2006.02535] A Survey on Deep Learning Techniques for Stereo-based Depth Estimation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2006.02535] 基于立体的深度估计技术综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2006.02535](https://ar5iv.labs.arxiv.org/html/2006.02535)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2006.02535](https://ar5iv.labs.arxiv.org/html/2006.02535)
- en: A Survey on Deep Learning Techniques
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于立体的深度估计技术综述
- en: for Stereo-based Depth Estimation
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于立体的深度估计
- en: 'Hamid Laga, Laurent Valentin Jospin, Farid Boussaid    Mohammed Bennamoun Hamid
    Laga is with the Information Technology, Mathematics and Statistics Discipline,
    Murdoch University (Australia), and with the Phenomics and Bioinformatics Research
    Centre, University of South Australia. Email: H.Laga@murdoch.edu.au Laurent Valentin
    Jospin is with the University of Western Australia, Perth, WA 6009, Australia.
    Email: laurent.jospin@research.uwa.edu.au Farid Boussaid is with the University
    of Western Australia, Perth, WA 6009, Australia. Email: farid.boussaid@uwa.edu.au
    Mohammed Bennamoun is with the University of Western Australia, Perth, WA 6009,
    Australia. Email: mohammed.bennamoun@uwa.edu.au Manuscript received June, 2020;
    revised June, 2020.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 哈米德·拉加、劳伦特·瓦伦丁·朱斯潘、法里德·布萨伊德    穆罕默德·本纳蒙 哈米德·拉加就职于澳大利亚默多克大学信息技术、数学与统计学学科，同时也在南澳大利亚大学的表型学与生物信息学研究中心工作。电子邮箱：H.Laga@murdoch.edu.au
    劳伦特·瓦伦丁·朱斯潘就职于澳大利亚西澳大学，珀斯，WA 6009。电子邮箱：laurent.jospin@research.uwa.edu.au 法里德·布萨伊德就职于澳大利亚西澳大学，珀斯，WA
    6009。电子邮箱：farid.boussaid@uwa.edu.au 穆罕默德·本纳蒙就职于澳大利亚西澳大学，珀斯，WA 6009。电子邮箱：mohammed.bennamoun@uwa.edu.au
    手稿收到日期：2020年6月；修订日期：2020年6月。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Estimating depth from RGB images is a long-standing ill-posed problem, which
    has been explored for decades by the computer vision, graphics, and machine learning
    communities. Among the existing techniques, stereo matching remains one of the
    most widely used in the literature due to its strong connection to the human binocular
    system. Traditionally, stereo-based depth estimation has been addressed through
    matching hand-crafted features across multiple images. Despite the extensive amount
    of research, these traditional techniques still suffer in the presence of highly
    textured areas, large uniform regions, and occlusions. Motivated by their growing
    success in solving various 2D and 3D vision problems, deep learning for stereo-based
    depth estimation has attracted a growing interest from the community, with more
    than 150 papers published in this area between 2014 and 2019\. This new generation
    of methods has demonstrated a significant leap in performance, enabling applications
    such as autonomous driving and augmented reality. In this article, we provide
    a comprehensive survey of this new and continuously growing field of research,
    summarize the most commonly used pipelines, and discuss their benefits and limitations.
    In retrospect of what has been achieved so far, we also conjecture what the future
    may hold for deep learning-based stereo for depth estimation research.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从RGB图像中估计深度是一个长期存在的难题，这一问题在计算机视觉、图形学和机器学习领域已被探索了数十年。在现有技术中，由于与人类双眼系统的强关联，立体匹配仍然是文献中使用最广泛的方法之一。传统上，基于立体的深度估计是通过在多幅图像之间匹配手工制作的特征来解决的。尽管进行了大量研究，但这些传统技术在处理高纹理区域、大面积均匀区域和遮挡物时仍然存在问题。受益于其在解决各种2D和3D视觉问题上的成功，基于深度学习的立体深度估计受到越来越多的关注，在2014年至2019年间，相关领域发表了超过150篇论文。这一新一代方法在性能上取得了显著的突破，使得诸如自动驾驶和增强现实等应用成为可能。本文提供了对这一新兴且持续增长的研究领域的全面综述，总结了最常用的流程，并讨论了其优缺点。在回顾迄今为止的成就时，我们还推测了基于深度学习的立体深度估计研究的未来发展方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: CNN, Deep Learning, 3D Reconstruction, Stereo Matching, Multi-view Stereo, Disparity
    Estimation, Feature Leaning, Feature Matching.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CNN，深度学习，3D重建，立体匹配，多视角立体，视差估计，特征学习，特征匹配。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Depth estimation from one or multiple RGB images is a long standing ill-posed
    problem, with applications in various domains such as robotics, autonomous driving,
    object recognition and scene understanding, 3D modeling and animation, augmented
    reality, industrial control, and medical diagnosis. This problem has been extensively
    investigated for many decades. Among all the techniques that have been proposed
    in the literature, stereo matching is traditionally the most explored one due
    to its strong connection to the human binocular system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从单张或多张RGB图像中进行深度估计是一个长期存在的难题，应用领域包括机器人技术、自动驾驶、物体识别和场景理解、3D建模与动画、增强现实、工业控制和医学诊断。这个问题已经被广泛研究了几十年。在所有提出的技术中，立体匹配由于其与人类双眼系统的紧密联系，一直是最被探索的技术之一。
- en: The first generation of stereo-based depth estimation methods relied typically
    on matching pixels across multiple images captured using accurately calibrated
    cameras. Although these techniques can achieve good results, they are still limited
    in many aspects. For instance, they are not suitable when dealing with occlusions,
    featureless regions, or highly textured regions with repetitive patterns. Interestingly,
    we, as humans, are good at solving such ill-posed inverse problems by leveraging
    prior knowledge. For example, we can easily infer the approximate sizes of objects,
    their relative locations, and even their approximate relative distance to our
    eye(s). We can do this because all the previously seen objects and scenes have
    enabled us to build prior knowledge and develop mental models of how the 3D world
    looks like. The second generation of methods tries to leverage this prior knowledge
    by formulating the problem as a learning task. The advent of deep learning techniques
    in computer vision [[1](#bib.bib1)] coupled with the increasing availability of
    large training datasets, have led to a third generation of methods that are able
    to recover the lost dimension. Despite being recent, these methods have demonstrated
    exciting and promising results on various tasks related to computer vision and
    graphics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一代基于立体视觉的深度估计方法通常依赖于在使用准确标定的相机拍摄的多张图像中匹配像素。虽然这些技术可以取得良好的结果，但在许多方面仍然有限。例如，当处理遮挡、无特征区域或具有重复纹理的高度纹理区域时，它们并不适用。有趣的是，作为人类，我们擅长通过利用先验知识来解决此类难解的逆问题。例如，我们可以轻松推测物体的近似大小、它们的相对位置，甚至它们与我们眼睛的相对距离。我们之所以能够做到这一点，是因为所有先前看到的物体和场景使我们能够建立先验知识，并形成3D世界的心理模型。第二代方法试图通过将问题表述为学习任务来利用这些先验知识。计算机视觉中深度学习技术的出现[[1](#bib.bib1)]加上大型训练数据集的日益普及，催生了第三代方法，这些方法能够恢复丢失的维度。尽管这些方法比较新，但在与计算机视觉和图形相关的各种任务中表现出了令人兴奋和有前途的结果。
- en: In this article, we provide a comprehensive and structured review of the recent
    advances in stereo image-based depth estimation using deep learning techniques.
    These methods use two or more images captured with spatially-distributed RGB cameras¹¹1Deep
    learning-based depth estimation from monocular images and videos is an emerging
    field and requires a separate survey.. We have gathered more than $150$ papers,
    which appeared between January $2014$ and December $2019$ in leading computer
    vision, computer graphics, and machine learning conferences and journals²²2At
    the time of writing this article.. The goal is to help the reader navigate in
    this emerging field, which has gained a significant momentum in the past few years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们提供了一个全面且结构化的综述，涵盖了基于立体图像的深度估计的最新进展，这些进展使用了深度学习技术。这些方法使用了由空间分布的RGB相机拍摄的两张或更多图像¹¹深度学习基于单目图像和视频的深度估计是一个新兴领域，需要单独的综述..
    我们收集了超过**150**篇论文，这些论文发表于**2014**年1月到**2019**年12月之间的主要计算机视觉、计算机图形学和机器学习会议及期刊²²在撰写本文时..
    其目标是帮助读者在这个新兴领域中进行导航，该领域在过去几年中获得了显著的动力。
- en: The major contributions of this article are as follows;
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献如下；
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first article that surveys stereo-based
    depth estimation using deep learning techniques. We present a comprehensive review
    of more than $150$ papers, which appeared in the past six years in leading conferences
    and journals.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一篇使用深度学习技术调查基于立体视觉的深度估计的文章。我们对过去六年在主要会议和期刊上发表的超过**150**篇论文进行了全面的综述。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive taxonomy of the state-of-the-art. We first describe
    the common pipelines and then discuss the similarities and differences between
    methods within each pipeline.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了最先进技术的全面分类。我们首先描述常见的处理流程，然后讨论每个流程内方法的相似性和差异。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive review and an insightful analysis on all the aspects
    of the problem, including the training data, the network architectures and their
    effect on the reconstruction performance, the training strategies, and the generalization
    ability.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对问题所有方面的全面回顾和深刻分析，包括训练数据、网络架构及其对重建性能的影响、训练策略和泛化能力。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comparative summary of the properties and performances of some
    key methods using publicly available datasets and in-house images. The latter
    have been chosen to test how these methods would perform on completely new scenarios.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了一个比较总结，涵盖了使用公开可用数据集和内部图像的某些关键方法的属性和性能。后者被选择用于测试这些方法在全新场景中的表现。
- en: The rest of this article is organized as follows; Section [2](#S2 "2 Scope and
    taxonomy ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    formulates the problem and lays down the taxonomy. Section [3](#S3 "3 Datasets
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation") surveys
    the various datasets which have been used to train and test stereo-based depth
    reconstruction algorithms. Section [4](#S4 "4 Depth by stereo matching ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation") focuses on the
    works that use deep learning architectures to learn how to match pixels across
    images. Section [5](#S5 "5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation") reviews the end-to-end methods
    for stereo matching, while Section [6](#S6 "6 Learning multiview stereo ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation") discusses how
    these methods have been extended to the multi-view stereo case. Section [7](#S7
    "7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques for
    Stereo-based Depth Estimation") focuses on the training procedures including the
    choice of the loss functions and the degree of supervision. Section [8](#S8 "8
    Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") discusses the performance of key methods. Finally, Section [9](#S9
    "9 Future research directions ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") discusses the potential future research directions, while Section [10](#S10
    "10 Conclusion ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    summarizes the main contributions of this article.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：第[2](#S2 "2 Scope and taxonomy ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation)节 formulates the problem and lays down the taxonomy.
    第[3](#S3 "3 Datasets ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation)节回顾了用于训练和测试基于立体的深度重建算法的各种数据集。第[4](#S4 "4 Depth by stereo matching ‣
    A Survey on Deep Learning Techniques for Stereo-based Depth Estimation)节重点介绍了使用深度学习架构学习如何在图像之间匹配像素的工作。第[5](#S5
    "5 End-to-end depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation)节回顾了立体匹配的端到端方法，第[6](#S6 "6 Learning multiview stereo ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation)节讨论了这些方法如何扩展到多视角立体的情况。第[7](#S7
    "7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques for
    Stereo-based Depth Estimation)节专注于包括损失函数选择和监督程度在内的训练过程。第[8](#S8 "8 Discussion
    and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation)节讨论了关键方法的性能。最后，第[9](#S9
    "9 Future research directions ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation)节讨论了潜在的未来研究方向，第[10](#S10 "10 Conclusion ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation)节总结了本文的主要贡献。
- en: 2 Scope and taxonomy
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 范围与分类
- en: Let $\textbf{I}=\{I_{k},k=1,\dots,n\}$ be a set of $n\geq 1$ RGB images of the
    same 3D scene, captured using cameras whose intrinsic and extrinsic parameters
    can be *known* or *unknown*. The goal is to estimate one or multiple depth maps,
    which can be from the same viewpoint as the input [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)], or from a new arbitrary viewpoint [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]. This article
    focuses on deep learning methods for stereo-based depth estimation, *i.e.,* $n=2$
    in the case of stereo matching, and $n>2$ for the case of Multi-View Stereo (MVS).
    Monocular and video-based depth estimation methods are beyond the scope of this
    article and require a separate survey.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\textbf{I}=\{I_{k},k=1,\dots,n\}$ 为一组 $n\geq 1$ 的 RGB 图像，这些图像来自同一 3D 场景，使用的相机的内参和外参可以是
    *已知的* 或 *未知的*。目标是估计一个或多个深度图，这些深度图可以与输入图像的视角相同 [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]，也可以是来自新的任意视角 [[6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]。本文重点讨论基于深度学习的立体视觉深度估计方法，即在立体匹配的情况下
    $n=2$，在多视角立体（MVS）的情况下 $n>2$。单目和基于视频的深度估计方法超出了本文的范围，需要另行调查。
- en: Learning-based depth reconstruction can be summarized as the process of learning
    a predictor $f_{\theta}$ that can infer from the set of images I, a depth map
    $\hat{D}$ that is as close as possible to the unknown depth map $D$. In other
    words, we seek to find a function $f_{\theta}$ such that $\mathcal{L}(\textbf{I})=d\left(f_{\theta}(\textbf{I}),D\right)$
    is minimized. Here, $\theta$ is a set of parameters, and $d(\cdot,\cdot)$ is a
    certain measure of distance between the real depth map $D$ and the reconstructed
    depth map $f_{\theta}(\textbf{I})$. The reconstruction objective $\mathcal{L}$
    is also known as the *loss function*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的深度重建可以总结为学习一个预测器 $f_{\theta}$ 的过程，该预测器可以从一组图像 I 推断出一个尽可能接近未知深度图 $D$ 的深度图
    $\hat{D}$。换句话说，我们寻求找到一个函数 $f_{\theta}$，使得 $\mathcal{L}(\textbf{I})=d\left(f_{\theta}(\textbf{I}),D\right)$
    被最小化。这里，$\theta$ 是一组参数，$d(\cdot,\cdot)$ 是实际深度图 $D$ 和重建深度图 $f_{\theta}(\textbf{I})$
    之间的某种距离度量。重建目标 $\mathcal{L}$ 也被称为 *损失函数*。
- en: 'We can distinguish two main categories of methods. Methods in the first class
    (Section [4](#S4 "4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")) mimic the traditional stereo-matching techniques [[11](#bib.bib11)]
    by explicitly learning how to match, or put in correspondence, pixels across the
    input images. Such correspondences can then be converted into an optical flow
    or a disparity map, which in turn can be converted into depth at each pixel in
    the reference image. The predictor $f$ is composed of three modules: a feature
    extraction module, a feature matching and cost aggregation module, and a disparity/depth
    estimation module. Each module is trained independently from the others.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以区分两大类方法。第一类方法（第 [4](#S4 "4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation) 节") 模仿传统的立体匹配技术 [[11](#bib.bib11)]，通过显式学习如何匹配或对应输入图像中的像素。这些对应关系可以被转换为光流或视差图，进而被转换为参考图像中每个像素的深度。预测器
    $f$ 由三个模块组成：特征提取模块、特征匹配和成本聚合模块，以及视差/深度估计模块。每个模块都是独立训练的。
- en: The second class of methods (Section [5](#S5 "5 End-to-end depth from stereo
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")) solves
    the stereo matching problem using a pipeline that is trainable end-to-end. Two
    main classes of methods have been proposed. Early methods formulated the depth
    estimation as a regression problem. In other words, the depth map is directly
    regressed from the input without explicitly matching features across the views.
    While these methods are simple and fast at runtime, they require a large amount
    of training data, which is hard to obtain. Methods in the second class mimic the
    traditional stereo matching pipeline by breaking the problem into stages composed
    of differentiable blocks and thus allowing end-to-end training. While a large
    body of the literature focused on pairwise stereo methods, several papers have
    also addressed the multi-view stereo case and these will be reviews in Section [6](#S6
    "6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation").
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类方法（第[5](#S5 "5 End-to-end depth from stereo ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation)节）通过一个可以端到端训练的管道解决立体匹配问题。已经提出了两类主要的方法。早期方法将深度估计公式化为回归问题。换句话说，深度图直接从输入中回归出来，而不需要在视图之间显式匹配特征。虽然这些方法在运行时简单且快速，但它们需要大量训练数据，这很难获得。第二类方法通过将问题分解为由可微分块组成的阶段，从而模仿传统的立体匹配管道，并允许端到端训练。虽然大量文献集中于成对立体方法，但也有几篇论文涉及到多视角立体问题，这些将在第[6](#S6
    "6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation)节中回顾。
- en: In all methods, the estimated depth maps can be further refined using refinement
    modules [[12](#bib.bib12), [2](#bib.bib2), [13](#bib.bib13), [3](#bib.bib3)] and/or
    progressive reconstruction strategies where the reconstruction is refined every
    time new images become available.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有方法中，估计的深度图可以通过使用细化模块 [[12](#bib.bib12), [2](#bib.bib2), [13](#bib.bib13),
    [3](#bib.bib3)] 和/或逐步重建策略进一步细化，其中重建会在每次新图像可用时进行改进。
- en: Finally, the performance of deep learning-based stereo methods depends not only
    on the network architecture but also on the datasets on which they have been trained
    (Section [3](#S3 "3 Datasets ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")) and on the training procedure used to optimise their parameters
    (Section [7](#S7 "7 Training end-to-end stereo methods ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")). The latter includes the choice
    of the loss functions and the supervision mode, which can be fully supervised
    with 3D annotations, weakly supervised, or self-supervised. We will discuss all
    these aspects in the subsequent sections.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，基于深度学习的立体方法的性能不仅依赖于网络架构，还依赖于其训练的数据集（第[3](#S3 "3 Datasets ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation)节）和用于优化其参数的训练过程（第[7](#S7
    "7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques for
    Stereo-based Depth Estimation)节）。后者包括损失函数的选择和监督模式，这可以是完全监督的3D注释、弱监督的或自监督的。我们将在后续部分讨论这些方面。
- en: 3 Datasets
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集
- en: 'TABLE I: Datasets for depth/disparity estimation. ”GT”: ground-truth, ”Tr.”:
    training, ”Ts.”: testing, ”fr.”: frames, ”Vol.”: volumetric, ”Eucl”: Euclidean,
    ”Ord”: ordinal, ”Int.”: intrinsic, ”Ext.”: extrinsic.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 深度/差异估计数据集。 "GT"：真实值，"Tr."：训练，"Ts."：测试，"fr."：帧，"Vol."：体积，"Eucl"：欧几里得，"Ord"：序数，"Int."：内部，"Ext."：外部。'
- en: '|  | Year | Type | Purpose | Images |  | Depth |  | Cam. params. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | 年份 | 类型 | 目的 | 图像 |  | 深度 |  | 相机参数 |'
- en: '|  | Resolution | # Scenes | # Views per scene | # Tr. scenes | # Ts. scenes
    |  | Resolution | #GT frames | Type | Depth range | Disparity range |  | Int.
    | Ext. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | 分辨率 | 场景数量 | 每场景视图数量 | 训练场景数量 | 测试场景数量 |  | 分辨率 | GT帧数量 | 类型 | 深度范围 |
    差异范围 |  | 内部 | 外部 |'
- en: '| Make3D [[14](#bib.bib14)] | 2009 | Real | Monocular depth | $2272\times 1704$
    | $534$ | monocular | $400$ | $134$ |  | $78\times 51$ | $534$ | Dense | $-$ |
    $-$ |  |  |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Make3D [[14](#bib.bib14)] | 2009 | 真实 | 单目深度 | $2272\times 1704$ | $534$
    | 单目 | $400$ | $134$ |  | $78\times 51$ | $534$ | 密集 | $-$ | $-$ |  |  |  |'
- en: '| KITTI2012 [[15](#bib.bib15)] | 2012 | Real | Stereo | $1240\times 376$ |
    $389$ | $2$ | $194$ | $195$ |  | $1226\times 370$ | $-$ | Sparse | $-$ | $-$ |  |
    Y | Y |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| KITTI2012 [[15](#bib.bib15)] | 2012 | 真实 | 立体 | $1240\times 376$ | $389$
    | $2$ | $194$ | $195$ |  | $1226\times 370$ | $-$ | 稀疏 | $-$ | $-$ |  | Y | Y
    |'
- en: '| MPI Sintel [[16](#bib.bib16)] | 2012 | Synthetic | Optical flow | $1024\times
    436$ | $35$ videos | $50$ | $23$ videos | $12$ videos |  | $-$ | $-$ | Dense |
    $-$ | $-$ |  |  |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| MPI Sintel [[16](#bib.bib16)] | 2012 | 合成 | 光流 | $1024\times 436$ | $35$
    视频 | $50$ | $23$ 视频 | $12$ 视频 |  | $-$ | $-$ | 密集 | $-$ | $-$ |  |  |  |'
- en: '| NYU2 [[17](#bib.bib17)] | 2012 | Real - indoor | Monocular depth, object
    segmentation | $640\times 480$ | $464$ videos, 100$+$ fr. per video | monocular
    | $-$ | $-$ |  | $-$ | $1,449$ | Kinect depth | $-$ | $-$ |  | N | N |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| NYU2 [[17](#bib.bib17)] | 2012 | 实际 - 室内 | 单目深度，物体分割 | $640\times 480$ |
    $464$ 视频，100$+$ 帧每视频 | 单目 | $-$ | $-$ |  | $-$ | $1,449$ | Kinect 深度 | $-$ | $-$
    |  | 否 | 否 |'
- en: '| RGB-D SLAM [[18](#bib.bib18)] | 2012 | Real | SLAM | $640\times 480$ | $19$
    videos |  | $15$ videos | $4$ videos |  | $-$ | $-$ | Dense | $-$ | $-$ |  | Y
    | Y |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| RGB-D SLAM [[18](#bib.bib18)] | 2012 | 实际 | SLAM | $640\times 480$ | $19$
    视频 |  | $15$ 视频 | $4$ 视频 |  | $-$ | $-$ | 密集 | $-$ | $-$ |  | 是 | 是 |'
- en: '| SUN3D [[19](#bib.bib19)] | 2013 | Real - rooms | Monocular video | $640\times
    480$ | $415$ videos, 10$-$1000$+$ fr. per video | $-$ | $-$ | $-$ |  | $-$ | $-$
    | Dense, SfM | $-$ | $-$ |  |  | Y |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SUN3D [[19](#bib.bib19)] | 2013 | 实际 - 房间 | 单目视频 | $640\times 480$ | $415$
    视频，10$-$1000$+$ 帧每视频 | $-$ | $-$ | $-$ |  | $-$ | $-$ | 密集，SfM | $-$ | $-$ |  |  |
    是 |'
- en: '| Middleburry [[20](#bib.bib20)] | 2014 | Indoor | Stereo | $2948\times 1988$
    | $30$ | $2$ | $15$ | $15$ |  | $2948\times 1988$ | $30$ | Dense | $-$ | $260$
    |  | Y | Y |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Middleburry [[20](#bib.bib20)] | 2014 | 室内 | 立体 | $2948\times 1988$ | $30$
    | $2$ | $15$ | $15$ |  | $2948\times 1988$ | $30$ | 密集 | $-$ | $260$ |  | 是 |
    是 |'
- en: '| KITTI 2015 [[21](#bib.bib21)] | 2015 | Real | Stereo | $1242\times 375$ |
    $400$ | $4$ | $200$ | $200$ |  | $1242\times 375$ | $-$ | Sparse | $-$ | $-$ |  |
    Y | Y |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| KITTI 2015 [[21](#bib.bib21)] | 2015 | 实际 | 立体 | $1242\times 375$ | $400$
    | $4$ | $200$ | $200$ |  | $1242\times 375$ | $-$ | 稀疏 | $-$ | $-$ |  | 是 | 是
    |'
- en: '| KITTI-MVS2015 [[21](#bib.bib21)] | 2015 | Real | MVS | $1242\times 375$ |
    $400$ | $20$ | $200$ | $200$ |  | $-$ | $-$ | Sparse | $-$ | $-$ |  | Y | Y |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| KITTI-MVS2015 [[21](#bib.bib21)] | 2015 | 实际 | MVS | $1242\times 375$ | $400$
    | $20$ | $200$ | $200$ |  | $-$ | $-$ | 稀疏 | $-$ | $-$ |  | 是 | 是 |'
- en: '| FlyingThings3D, Monkaa, Driving [[22](#bib.bib22)] | 2016 | Synthetic | Stereo,
    Video, Optical flow | $960\times 540$ | $39$K frames | $2$ | $21,818$ | $4,248$
    |  | $384\times 192$ | $-$ | Dense | $-$ | $160$px |  | Y | Y |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| FlyingThings3D, Monkaa, Driving [[22](#bib.bib22)] | 2016 | 合成 | 立体，视频，光流
    | $960\times 540$ | $39$K 帧 | $2$ | $21,818$ | $4,248$ |  | $384\times 192$ |
    $-$ | 密集 | $-$ | $160$px |  | 是 | 是 |'
- en: '| CityScapes [[23](#bib.bib23)] | 2016 | Street scenes | Semantic seg., dense
    labels | $2048\times 1024$ | $5$K | $2$ | $2975$ | $1525$ |  | $-$ | $-$ | NA
    | $-$ | $-$ |  | Ego-motion |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| CityScapes [[23](#bib.bib23)] | 2016 | 街景 | 语义分割，密集标签 | $2048\times 1024$
    | $5$K | $2$ | $2975$ | $1525$ |  | $-$ | $-$ | NA | $-$ | $-$ |  | 自我运动 |'
- en: '|  |  |  | Semantic seg.,coarse labels | $2048\times 1024$ | $20$K | $2$ |  |  |  |
    NA | NA | NA | NA | NA |  | Ego-motion |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 语义分割，粗略标签 | $2048\times 1024$ | $20$K | $2$ |  |  |  | NA | NA |
    NA | NA | NA |  | 自我运动 |'
- en: '| DTU [[24](#bib.bib24)] | 2016 | Real, small objects | MVS | $1200\times 1600$
    | $80$ | $49-64$ | $-$ | $-$ |  | $-$ | $-$ | Structured light scans | $-$ | $-$
    |  | Y | Y |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| DTU [[24](#bib.bib24)] | 2016 | 实际，小物体 | MVS | $1200\times 1600$ | $80$ |
    $49-64$ | $-$ | $-$ |  | $-$ | $-$ | 结构光扫描 | $-$ | $-$ |  | 是 | 是 |'
- en: '| ETH3D [[25](#bib.bib25)] | 2017 | Real, in/outdoor | Low-res, Stereo | $940\times
    490$ | $47$ | $2$ | $27$ | $20$ |  | $-$ | $47$ | Dense | $-$ | $-$ |  | Y | Y
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ETH3D [[25](#bib.bib25)] | 2017 | 实际，室内/室外 | 低分辨率，立体 | $940\times 490$ |
    $47$ | $2$ | $27$ | $20$ |  | $-$ | $47$ | 密集 | $-$ | $-$ |  | 是 | 是 |'
- en: '|  |  |  | Low-res, MVS on video | $940\times 490$ | $10$ videos | $4$ | $5$
    videos | $5$ videos |  | $-$ | $-$ | Dense | $-$ | $-$ |  | Y | Y |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 低分辨率，视频上的 MVS | $940\times 490$ | $10$ 视频 | $4$ | $5$ 视频 | $5$ 视频
    |  | $-$ | $-$ | 密集 | $-$ | $-$ |  | 是 | 是 |'
- en: '|  |  |  | High-res, MVS on images from DSLR camera | $940\times 490$ | $25$
    | $14-76$ | $13$ | $12$ |  | $-$ | $25$ | Dense | $-$ | $-$ |  | Y | Y |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 高分辨率，使用 DSLR 相机的 MVS | $940\times 490$ | $25$ | $14-76$ | $13$ |
    $12$ |  | $-$ | $25$ | 密集 | $-$ | $-$ |  | 是 | 是 |'
- en: '| SUNCG [[26](#bib.bib26)] | 2017 | Synthetic, indoor | Scene completion |
    $-$ | $45$K | $-$ | $-$ | $-$ |  | $640\times 480$ | $-$ | Depth and Vol. GT |
    $-$ | $-$ |  |  |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| SUNCG [[26](#bib.bib26)] | 2017 | 合成，室内 | 场景完成 | $-$ | $45$K | $-$ | $-$
    | $-$ |  | $640\times 480$ | $-$ | 深度和体积 GT | $-$ | $-$ |  |  |  |'
- en: '| MVS-Synth [[27](#bib.bib27)] | 2018 | Synth - urban | MVS | $1920\times 1080$
    | $120$ | $100$ |  |  |  | $-$ | $-$ | Dense | $-$ | $-$ |  | Y | Y |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| MVS-Synth [[27](#bib.bib27)] | 2018 | 合成 - 城市 | MVS | $1920\times 1080$ |
    $120$ | $100$ |  |  |  | $-$ | $-$ | 密集 | $-$ | $-$ |  | 是 | 是 |'
- en: '| MegaDepth [[28](#bib.bib28)] | 2018 | Real (Internet images) | Monocular,
    Eucl. and ord. depth | $1600\times 1600$ | $130$K | monocular | $-$ | $-$ |  |
    $-$ | $100$K (Eucl.), $30$K (Ord.) | Dense, Eucl., Ord. | $-$ | $-$ |  |  |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| MegaDepth [[28](#bib.bib28)] | 2018 | 实际（互联网图片） | 单目，欧氏和有序深度 | $1600\times
    1600$ | $130$K | 单目 | $-$ | $-$ |  | $-$ | $100$K（欧氏），$30$K（有序） | 密集，欧氏，有序 | $-$
    | $-$ |  |  |  |'
- en: '| Jeon and Lee [[29](#bib.bib29)] | $2018$ | Real | Depth enhancement | $-$
    | $4$K images | $-$ | $-$ | $-$ |  | $640\times 480$ | $4,000$ | Dense | $0.01-30$m
    | $-$ |  | Y | Y |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Jeon and Lee [[29](#bib.bib29)] | $2018$ | 真实 | 深度增强 | $-$ | $4$K 图像 | $-$
    | $-$ | $-$ |  | $640\times 480$ | $4,000$ | 密集 | $0.01-30$m | $-$ |  | Y | Y
    |'
- en: '| OmniThings [[30](#bib.bib30), [31](#bib.bib31)] | 2019 | Synthetic, fisheye
    images | Omnidirectional MVS | $800\times 768$ | $10240$ | $4$ | $9216$ | $1024$
    |  | $640\times 320$ | $-$ | Dense | $-$ | $\leq 192$px |  |  |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| OmniThings [[30](#bib.bib30), [31](#bib.bib31)] | 2019 | 合成，鱼眼图像 | 全景MVS
    | $800\times 768$ | $10240$ | $4$ | $9216$ | $1024$ |  | $640\times 320$ | $-$
    | 密集 | $-$ | $\leq 192$px |  |  |  |'
- en: '| OmniHouse [[30](#bib.bib30), [31](#bib.bib31)] | 2019 | Synthetic, fisheye
    images | Omnidirectional MVS | $800\times 768$ | $2,560$ | $4$ | $2048$ | $512$
    |  | $640\times 320$ | $-$ | Dense | $-$ | $\leq 192$px |  |  |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| OmniHouse [[30](#bib.bib30), [31](#bib.bib31)] | 2019 | 合成，鱼眼图像 | 全景MVS |
    $800\times 768$ | $2,560$ | $4$ | $2048$ | $512$ |  | $640\times 320$ | $-$ |
    密集 | $-$ | $\leq 192$px |  |  |  |'
- en: '| HR-VS [[32](#bib.bib32)] | 2019 | Synthetic, outdoor | High res. stereo |
    $2056\times 2464$ | $780$ | $2$ | $-$ | $-$ |  | $1918\times 2424$ | 780 | Dense,
    Eucl. | $2.52$ to $200$m | $9.66$ to $768$px |  |  |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| HR-VS [[32](#bib.bib32)] | 2019 | 合成，户外 | 高分辨率立体 | $2056\times 2464$ | $780$
    | $2$ | $-$ | $-$ |  | $1918\times 2424$ | 780 | 密集，欧几里得 | $2.52$到$200$m | $9.66$到$768$px
    |  |  |  |'
- en: '|  |  | Real, outdoor | High res. stereo | $1918\times 2424$ | $33$ | $2$ |
    $-$ | $-$ |  | $1918\times 2424$ | 33 | Dense, Eucl. |  | $5.41$ to $182.3$px
    |  |  |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 真实，户外 | 高分辨率立体 | $1918\times 2424$ | $33$ | $2$ | $-$ | $-$ |  | $1918\times
    2424$ | 33 | 密集，欧几里得 |  | $5.41$到$182.3$px |  |  |  |'
- en: '| DrivingStereo [[33](#bib.bib33)] | 2019 | Driving | High res. stereo | $1762\times
    800$ | $182,188$ | $2$ | $174,437$ | $7,751$ |  | $1762\times 800$ | $182,188$
    | Sparse | up to $80$m |  |  |  |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| DrivingStereo [[33](#bib.bib33)] | 2019 | 驾驶 | 高分辨率立体 | $1762\times 800$
    | $182,188$ | $2$ | $174,437$ | $7,751$ |  | $1762\times 800$ | $182,188$ | 稀疏
    | 高达 $80$m |  |  |  |  |'
- en: '| ApolloScape [[34](#bib.bib34)] | 2019 | Auto. driving | High res. stereo
    | $3130\times 960$ | $5,165$ | $2$ | $4,156$ | $1,009$ |  | $-$ | $5165$ | LIDAR
    | $-$ to $-$m | $-$ |  | Y | $-$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ApolloScape [[34](#bib.bib34)] | 2019 | 自动驾驶 | 高分辨率立体 | $3130\times 960$
    | $5,165$ | $2$ | $4,156$ | $1,009$ |  | $-$ | $5165$ | LIDAR | $-$到$-$m | $-$
    |  | Y | $-$ |'
- en: '| A2D2 [[35](#bib.bib35)] | 2020 | Auto. driving | High res. stereo | $2.3$M
    pixel | $41,277$ | $6$ | $-$ | $-$ |  | $-$ | $-$ | LIDAR | up to $100$m | $-$
    |  | Y | Y |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| A2D2 [[35](#bib.bib35)] | 2020 | 自动驾驶 | 高分辨率立体 | $2.3$M 像素 | $41,277$ | $6$
    | $-$ | $-$ |  | $-$ | $-$ | LIDAR | 高达 $100$m | $-$ |  | Y | Y |'
- en: Table [I](#S3.T1 "TABLE I ‣ 3 Datasets ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") summarizes some of the datasets that have
    been used to train and test deep learning-based depth estimation algorithms. Below,
    we discuss these datasets based on their sizes, their spatial and depth resolution,
    the type of depth annotation they provide, and the domain gap (or shift) issue
    faced by many deep learning-based algorithms.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [I](#S3.T1 "TABLE I ‣ 3 Datasets ‣ A Survey on Deep Learning Techniques for
    Stereo-based Depth Estimation") 总结了用于训练和测试基于深度学习的深度估计算法的一些数据集。下面，我们根据这些数据集的大小、空间和深度分辨率、提供的深度标注类型以及许多基于深度学习的算法面临的领域差距（或偏移）问题进行讨论。
- en: (1) Dataset size. The first datasets, which appeared prior to $2016$, are of
    small scale due to the difficulty of creating ground-truth 3D annotations. An
    example is the two KITTI datasets [[15](#bib.bib15), [21](#bib.bib21)], which
    contain $200$ stereo pairs with their corresponding disparity ground-truth. They
    have been extensively used to train and test patch-based CNNs for stereo matching
    algorithms (see Section [4](#S4 "4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")), which have a small receptive
    field. As such a single stereo pair can result in thousands of training samples.
    However, in end-to-end architectures (Sections [5](#S5 "5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    and [6](#S6 "6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")), a stereo pair corresponds to only one sample.
    End-to-end networks have a large number of parameters, and thus require large
    datasets for efficient training. While collecting large image datasets is very
    easy, *e.g.,* by using video sequences as in *e.g.,* NYU2 [[17](#bib.bib17)],
    ETH3D [[25](#bib.bib25)], SUN3D [[19](#bib.bib19)], and ETH3D [[25](#bib.bib25)],
    annotating them with 3D labels is time consuming. Recent works, *e.g.,* the AppoloScape [[34](#bib.bib34)]
    and A2D2 [[35](#bib.bib35)], use LIDAR to acquire dense 3D annotations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 数据集大小。最早出现的几个数据集，出现在$2016$年前，由于创建地面真实3D注释的难度，规模较小。例如，两个KITTI数据集[[15](#bib.bib15),
    [21](#bib.bib21)]，包含$200$对立体图像及其对应的视差真实值。它们已被广泛用于训练和测试用于立体匹配算法的基于块的CNN（见第[4](#S4
    "4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation)节），这些算法具有较小的感受野。因此，一个立体图像对可以产生数千个训练样本。然而，在端到端架构（第[5](#S5 "5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation)节和第[6](#S6 "6 Learning multiview stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation)节），一个立体图像对仅对应一个样本。端到端网络具有大量参数，因此需要大型数据集以实现高效训练。虽然收集大型图像数据集非常容易，*例如*，使用视频序列，如*例如*，NYU2[[17](#bib.bib17)]、ETH3D[[25](#bib.bib25)]、SUN3D[[19](#bib.bib19)]和ETH3D[[25](#bib.bib25)]，但用3D标签进行注释是耗时的。最近的工作，*例如*，AppoloScape[[34](#bib.bib34)]和A2D2[[35](#bib.bib35)]，使用LIDAR获取密集的3D注释。
- en: Data augmentation strategies, *e.g.,* by applying geometric and photometric
    transformations to the images that are available, have been extensively used in
    the literature. There are, however, a few other strategies that are specific to
    depth estimation. This includes artificially synthesizing and rendering from 3D
    CAD models 2D and 2.5D views from various (random) viewpoints, poses, and lighting
    conditions. One can also overlay rendered 3D models on the top of real images.
    This approach has been used to generate the FlyingThings3D, Monkaa, and Driving
    datasets of [[22](#bib.bib22)], and the OmniThings and OmniHouse datasets for
    benchmarking MVS for omnidirectional images [[30](#bib.bib30), [31](#bib.bib31)].
    Huang *et al.* [[27](#bib.bib27)] followed a similar idea but used scenes from
    video games to generate MVS-Synth, a photo-realistic synthetic dataset prepared
    for learning-based Multi-View Stereo algorithms.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强策略，*例如*，通过对可用图像应用几何和光度变换，已经在文献中被广泛使用。然而，还有一些其他策略是特定于深度估计的。这包括从3D CAD模型人工合成和渲染各种（随机）视角、姿态和光照条件下的2D和2.5D视图。还可以将渲染的3D模型覆盖在真实图像上。这种方法已被用于生成FlyingThings3D、Monkaa和Driving数据集[[22](#bib.bib22)]，以及OmniThings和OmniHouse数据集，用于对全景图像的MVS进行基准测试[[30](#bib.bib30),
    [31](#bib.bib31)]。黄*等人*[[27](#bib.bib27)]遵循了类似的思路，但使用了视频游戏中的场景生成MVS-Synth，这是一个为基于学习的多视图立体算法准备的照片级真实合成数据集。
- en: The main challenge is that generating large amounts of synthetic data containing
    varied real-world appearance and motion is not trivial [[36](#bib.bib36)]. As
    a result, a number of works overcome the need for ground-truth depth information
    by training their deep networks without 3D supervision, see Section [7.1](#S7.SS1
    "7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation"). Others used traditional
    depth estimation and structure-from-motion (SfM) techniques to generate 3D annotations.
    For example, Li *et al.* [[28](#bib.bib28)] used modern structure-from-motion
    and multiview stereo (MVS) methods together with multiview Internet photo collections
    to create the large-scale MegaDepth dataset providing improved depth estimation
    accuracy via bigger training dataset sizes. This dataset has also been automatically
    augmented with ordinal depth relations generated using semantic segmentation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 主要挑战在于生成包含各种现实世界外观和运动的大量合成数据并非易事[[36](#bib.bib36)]。因此，一些研究通过训练其深度网络而无需3D监督来克服对真实深度信息的需求，见第[7.1节](#S7.SS1
    "7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")。其他研究使用传统的深度估计和运动结构（SfM）技术来生成3D标注。例如，李*等人*[[28](#bib.bib28)]结合现代运动结构和多视角立体（MVS）方法及多视角互联网照片集合创建了大规模的MegaDepth数据集，通过更大的训练数据集提高了深度估计的准确性。该数据集还通过语义分割自动增强了序数深度关系。
- en: (2) Spatial and depth resolutions. The disparity/depth information can be either
    in the form of maps of the same or lower resolution than the input images, or
    in the form of sparse depth values at some locations in the reference image. Most
    of the existing datasets are of low spatial resolution. In recent years, however,
    there has been a growing focus on stereo matching with high-resolution images.
    An example of a high-resolution dataset is the HR-VS and HR-RS of Yang *et al.* [[32](#bib.bib32)],
    where each RGB pair of resolution $1918\times 2424$ is annotated with a depth
    map of the same resolution. However, the dataset only contains $800$ pairs of
    stereo images, which is relatively small for end-to-end training. Other datasets
    such as the ApolloScape [[34](#bib.bib34)] and A2D2 [[35](#bib.bib35)] contain
    very high resolution images, of the order of $3130\times 960$, with more that
    $100+$ hours of stereo driving videos, in the case of ApolloScape, have been specifically
    designed to test autonomous driving algorithms.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 空间和深度分辨率。视差/深度信息可以是与输入图像相同或更低分辨率的地图形式，也可以是参考图像中某些位置的稀疏深度值。大多数现有数据集的空间分辨率较低。然而，近年来，越来越多地关注于高分辨率图像的立体匹配。一个高分辨率数据集的例子是杨*等人*[[32](#bib.bib32)]的HR-VS和HR-RS，其中每对分辨率为$1918\times
    2424$的RGB图像都标注有相同分辨率的深度图。然而，该数据集仅包含$800$对立体图像，对于端到端训练来说相对较小。其他数据集如ApolloScape[[34](#bib.bib34)]和A2D2[[35](#bib.bib35)]包含非常高分辨率的图像，约为$3130\times
    960$，其中ApolloScape的立体驾驶视频超过$100+$小时，专门设计用于测试自动驾驶算法。
- en: (3) Euclidean vs. ordinal depth. Instead of manually annotating images with
    exact, *i.e.,* Euclidean, depth values, some papers, *e.g.,* MegaDepth [[28](#bib.bib28)],
    provide ordinal annotations, *i.e.,* pixel $x_{1}$ is closer, farther, or at the
    same depth, as pixel $x_{2}$. Ordinal annotation is simpler and faster to achieve
    than Euclidean annotation. In fact, it can be accurately obtained using traditional
    stereo matching algorithms, since ordinal depth is less sensitive to innacuracies
    in depth estimation
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 欧几里得深度与序数深度。与其手动标注图像的确切*即*欧几里得深度值，不如一些论文*例如*MegaDepth[[28](#bib.bib28)]提供序数标注，即像素$x_{1}$距离更近、更远或与像素$x_{2}$在相同深度。序数标注比欧几里得标注更简单、更快捷。实际上，可以使用传统的立体匹配算法准确获得序数深度，因为序数深度对深度估计中的不准确性不那么敏感。
- en: (4) Domain gap. While artificially augmenting training datasets allows enriching
    existing ones, the domain shift caused by the very different conditions between
    real and synthetic data can result in a lower accuracy when applied to real-world
    environments. We will discuss, in Section [7.3](#S7.SS3 "7.3 Domain adaptation
    and transfer learning ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation"), how this domain shift
    issue has been addressed in the literature.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 域间差距。虽然通过人工扩充训练数据集可以丰富现有数据，但由于真实数据和合成数据之间条件的巨大差异，可能会导致在真实环境中准确性降低。我们将在第[7.3](#S7.SS3
    "7.3 域适应与迁移学习 ‣ 7 端到端立体方法训练 ‣ 深度学习技术在立体深度估计中的应用调查")节中讨论文献中如何解决这一域间差距问题。
- en: 4 Depth by stereo matching
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 通过立体匹配的深度
- en: 'TABLE II: Taxonomy and comparison of deep learning-based stereo matching techniques.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：基于深度学习的立体匹配技术的分类与比较。
- en: '| Method | Year | Feature computation |  | Similarity |  | Training |  | Regularization
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 特征计算 |  | 相似性 |  | 训练 |  | 正则化 |'
- en: '| Architectures | Dimension |  |  | Degree of supervision | Loss |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 维度 |  |  | 监督程度 | 损失 |  |'
- en: '| Zagoruyko [[37](#bib.bib37)] | $2015$ | ConvNet | Multiscale |  | FCN |  |
    Supervised with positive/negative samples | Hinge and squared $L_{2}$ |  | NA
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Zagoruyko [[37](#bib.bib37)] | $2015$ | ConvNet | 多尺度 |  | FCN |  | 有监督，正负样本
    | 挤压和平方 $L_{2}$ |  | NA |'
- en: '| Han [[38](#bib.bib38)] | $2015$ | ConvNet | Fixed scale |  | FCN |  | Supervised
    | Cross-entropy |  | NA |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Han [[38](#bib.bib38)] | $2015$ | ConvNet | 固定尺度 |  | FCN |  | 有监督 | 交叉熵
    |  | NA |'
- en: '| Zbontar [[39](#bib.bib39)] | $2015$ | ConvNet | Fixed scale |  | Hand-crafted
    |  | Triplet contrastive learning | $L_{1}$ |  | MRF |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Zbontar [[39](#bib.bib39)] | $2015$ | ConvNet | 固定尺度 |  | 手工制作 |  | 三元对比学习
    | $L_{1}$ |  | MRF |'
- en: '| Chen [[40](#bib.bib40)] | $2015$ | ConvNet | Multiscale |  | Correlation
    $+$ voting |  | Supervised with positive/negative samples | $L_{1}$ |  | MRF |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[40](#bib.bib40)] | $2015$ | ConvNet | 多尺度 |  | 相关性 $+$ 投票 |  | 有监督，正负样本
    | $L_{1}$ |  | MRF |'
- en: '| Simo [[41](#bib.bib41)] | $2015$ | ConvNet | Fixed scale |  | $L_{2}$ |  |
    Supervised with positive/negative samples | $L_{2}$ |  | NA |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Simo [[41](#bib.bib41)] | $2015$ | ConvNet | 固定尺度 |  | $L_{2}$ |  | 有监督，正负样本
    | $L_{2}$ |  | NA |'
- en: '| Zbontar [[42](#bib.bib42)] | $2016$ | ConvNet | Fixed scale |  | Hand-crafted,
    FCN |  | Supervised with known disparity | Hinge |  | Classic stereo |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Zbontar [[42](#bib.bib42)] | $2016$ | ConvNet | 固定尺度 |  | 手工制作, FCN |  |
    有监督，已知视差 | 挤压 |  | 经典立体 |'
- en: '| Balantas [[43](#bib.bib43)] | $2016$ | ConvNet | Fixe scale |  | $L_{2}$
    |  | Supervised, triplet contrastive learning | Soft-Positive-Negative (Soft-PN)
    |  | $-$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Balantas [[43](#bib.bib43)] | $2016$ | ConvNet | 固定尺度 |  | $L_{2}$ |  | 有监督，三元对比学习
    | 软正负样本 (Soft-PN) |  | $-$ |'
- en: '| Mayer [[22](#bib.bib22)] | $2016$ | ConvNet | Fixed-scale |  | Hand-crafted
    |  | Supervised | $-$ |  | Encoder-decoder |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Mayer [[22](#bib.bib22)] | $2016$ | ConvNet | 固定尺度 |  | 手工制作 |  | 有监督 | $-$
    |  | 编码器-解码器 |'
- en: '| Luo [[44](#bib.bib44)] | $2016$ | ConvNet | Fixed scale |  | Correlation
    |  | Supervised | Cross-entropy |  | MRF |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Luo [[44](#bib.bib44)] | $2016$ | ConvNet | 固定尺度 |  | 相关性 |  | 有监督 | 交叉熵
    |  | MRF |'
- en: '| Kumar [[45](#bib.bib45)] | 2016 | ConvNet | Fixed scale |  | ConvNet |  |
    Supervised, triplet contrastive learning | Maximise inter-class distance, |  |
    $-$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Kumar [[45](#bib.bib45)] | 2016 | ConvNet | 固定尺度 |  | ConvNet |  | 有监督，三元对比学习
    | 最大化类间距离 |  | $-$ |'
- en: '|  |  |  |  |  |  |  |  | minimize inter-class distance. |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | 最小化类间距离。 |  |  |'
- en: '| Shaked [[46](#bib.bib46)] | $2017$ | Highway network with | Fixed scale |  |
    FCN |  | Supervised | Hinge$+$cross-entropy |  | Classic$+$4Conv$+$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Shaked [[46](#bib.bib46)] | $2017$ | 高速网络 | 固定尺度 |  | FCN |  | 有监督 | 挤压$+$交叉熵
    |  | 经典$+$4Conv$+$ |'
- en: '|  |  | multilevel skip connections |  |  |  |  |  |  |  | 5FC |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 多层跳跃连接 |  |  |  |  |  |  |  | 5FC |'
- en: '| Hartmann [[47](#bib.bib47)] | $2017$ | ConvNet | Fixed scale |  | ConvNet
    |  | Supervised | Croos-entropy |  | Encoder |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Hartmann [[47](#bib.bib47)] | $2017$ | ConvNet | 固定尺度 |  | ConvNet |  | 有监督
    | 交叉熵 |  | 编码器 |'
- en: '| Park [[48](#bib.bib48)] | $2017$ | ConvNet | Fixed scale |  | $1\times 1$
    Convs, |  | Supervised | $-$ |  | NA |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Park [[48](#bib.bib48)] | $2017$ | ConvNet | 固定尺度 |  | $1\times 1$ 卷积 |  |
    有监督 | $-$ |  | NA |'
- en: '|  |  |  |  |  | ReLU, SPP |  |  |  |  |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | ReLU, SPP |  |  |  |  |  |'
- en: '| Ye [[49](#bib.bib49)] | $2017$ | ConvNet | Fixed scale |  | FCN |  | Supervised
    | $L_{1}$ |  | SGM |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Ye [[49](#bib.bib49)] | $2017$ | ConvNet | 固定尺度 |  | FCN |  | 有监督 | $L_{1}$
    |  | SGM |'
- en: '|  |  | Multisize pooling |  |  | ($1\times 1$ convs) |  |  |  |  |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 多尺寸池化 |  |  | ($1\times 1$ 卷积) |  |  |  |  |  |'
- en: '| Tulyakov [[50](#bib.bib50)] | $2017$ | Generic - independent of the network
    architecture |  | Weakly supervised | MIL, Contrastive, Contrastive-DP |  | $-$
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Tulyakov [[50](#bib.bib50)] | $2017$ | 通用 - 与网络架构无关 |  | 弱监督 | MIL, 对比, 对比-DP
    |  | $-$ |'
- en: 'Stereo-based depth reconstruction methods take $n=2$ RGB images and produce
    a disparity map $D$ that minimizes an energy function of the form:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于立体的深度重建方法采用 $n=2$ RGB 图像，并生成一个视差图 $D$，该视差图最小化形式为：
- en: '|  | $E(D)=\sum_{x}C(x,0pt_{x})+\sum_{x}\sum_{y\in\mathcal{N}_{x}}E_{s}(0pt_{x},0pt_{y}).$
    |  | (1) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(D)=\sum_{x}C(x,0pt_{x})+\sum_{x}\sum_{y\in\mathcal{N}_{x}}E_{s}(0pt_{x},0pt_{y}).$
    |  | (1) |'
- en: Here, $x$ and $y$ are image pixels, and $\mathcal{N}_{x}$ is the set of pixels
    that are within the neighborhood of $x$. The first term of Eqn. ([1](#S4.E1 "In
    4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")) is the matching cost. When using rectified stereo pairs, $C(x,0pt_{x})$
    measures the cost of matching the pixel $x=(i,j)$ of the left image with the pixel
    $y=(i,j-0pt_{x})$ of the right image. In this case, $0pt_{x}=D(x)\in[d_{min},d_{max}]$
    is the disparity at pixel $x$. Depth can then be inferred by triangulation. When
    the disparity range is discritized into $n_{d}$ disparity levels, $C$ becomes
    a 3D cost volume of size $0pt\times 0pt\times n_{d}$. In the more general multiview
    stereo case, *i.e.,* $n\geq 2$, the cost $C(x,0pt_{x})$ measures the inverse likelihood
    of $x$ on the reference image having depth $0pt_{x}$. The second term of Eqn. ([1](#S4.E1
    "In 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")) is a regularization term used to impose constraints such as
    smoothness and left-right consistency.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$x$ 和 $y$ 是图像像素，$\mathcal{N}_{x}$ 是位于 $x$ 邻域内的像素集合。公式 ([1](#S4.E1 "In 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")) 的第一项是匹配代价。在使用矫正后的立体对时，$C(x,0pt_{x})$ 测量将左图像的像素 $x=(i,j)$ 与右图像的像素
    $y=(i,j-0pt_{x})$ 匹配的代价。在这种情况下，$0pt_{x}=D(x)\in[d_{min},d_{max}]$ 是像素 $x$ 的视差。然后可以通过三角测量推断深度。当视差范围离散为
    $n_{d}$ 个视差级别时，$C$ 变成一个大小为 $0pt\times 0pt\times n_{d}$ 的 3D 代价体积。在更一般的多视角立体情况下，*即，*
    $n\geq 2$，代价 $C(x,0pt_{x})$ 测量 $x$ 在参考图像上具有深度 $0pt_{x}$ 的逆概率。公式 ([1](#S4.E1 "In
    4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")) 的第二项是一个正则化项，用于施加平滑性和左右一致性等约束。
- en: '![Refer to caption](img/e3d644b4ae621dd997815832d964a711.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/e3d644b4ae621dd997815832d964a711.png)'
- en: 'Figure 1: The building blocs of a stereo matching pipeline.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：立体匹配流水线的构建模块。
- en: 'Traditionally, this problem has been solved using a pipeline of four building
    blocks [[11](#bib.bib11)], see Fig. [1](#S4.F1 "Figure 1 ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation"): (1)
    feature extraction, (2) feature matching across images, (3) disparity computation,
    and (4) disparity refinement and post-processing. The first two blocks construct
    the cost volume $C$. The third block regularizes the cost volume and then finds,
    by minimizing Eqn. ([1](#S4.E1 "In 4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")), an initial estimate
    of the disparity map. The last block refines and post-processes the initial disparity
    map.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，这个问题是通过使用四个构建模块的流水线来解决的 [[11](#bib.bib11)]，见图 [1](#S4.F1 "图 1 ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")：（1）特征提取，（2）图像间特征匹配，（3）视差计算，以及（4）视差细化和后处理。前两个模块构建了代价体积 $C$。第三个模块对代价体积进行正则化，然后通过最小化公式 ([1](#S4.E1
    "In 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation"))，找到视差图的初步估计。最后一个模块对初步视差图进行细化和后处理。
- en: This section focuses on how these individual blocks have been implemented using
    deep learning-based methods. Table [II](#S4.T2 "TABLE II ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation") summarises
    the state-of-the-art methods.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍如何使用基于深度学习的方法实现这些单独的模块。表 [II](#S4.T2 "TABLE II ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation") 总结了最新的技术方法。
- en: 4.1 Learning feature extraction and matching
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 学习特征提取和匹配
- en: '| ![Refer to caption](img/8acf528e7280601c61cc431040a1f554.png) | ![Refer to
    caption](img/906a51c527f1892aee51d9afe7e07e17.png) | ![Refer to caption](img/06270467dce79b33d4ba68ed22252fa9.png)
    | ![Refer to caption](img/49200cc32256c862424df9ae36c6f0b1.png) | ![Refer to caption](img/228c9da17f799c9184c363bcc8c1d9ee.png)
    | ![Refer to caption](img/fad058a7f417212e4f18d90faef5745c.png) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ![Refer to caption](img/8acf528e7280601c61cc431040a1f554.png) | ![Refer to
    caption](img/906a51c527f1892aee51d9afe7e07e17.png) | ![Refer to caption](img/06270467dce79b33d4ba68ed22252fa9.png)
    | ![Refer to caption](img/49200cc32256c862424df9ae36c6f0b1.png) | ![Refer to caption](img/228c9da17f799c9184c363bcc8c1d9ee.png)
    | ![Refer to caption](img/fad058a7f417212e4f18d90faef5745c.png) |'
- en: '| (a) MC-CNN [[39](#bib.bib39), [42](#bib.bib42)]. | (b) [[37](#bib.bib37)]
    and [[38](#bib.bib38)]. | (c) [[37](#bib.bib37)]. | (d) LW-CNN [[48](#bib.bib48)].
    | (e) FED-D2DRR [[49](#bib.bib49)]. | (f) [[37](#bib.bib37)]. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (a) MC-CNN [[39](#bib.bib39), [42](#bib.bib42)]. | (b) [[37](#bib.bib37)]
    和 [[38](#bib.bib38)]. | (c) [[37](#bib.bib37)]. | (d) LW-CNN [[48](#bib.bib48)].
    | (e) FED-D2DRR [[49](#bib.bib49)]. | (f) [[37](#bib.bib37)]. |'
- en: 'Figure 2: Feature learning architectures.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：特征学习架构。
- en: Early deep learning techniques for stereo matching replace the hand-crafted
    features (block A of Fig. LABEL:ig:stereo_matching_pipeline) with learned features [[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [42](#bib.bib42)]. They take two patches,
    one centered at a pixel $x=(i,j)$ on the left image and another one centered at
    pixel $y=(i,j-d)$ on the right image (with $d\in\{0,\dots,n_{d}\}$), compute their
    corresponding feature vectors using a CNN, and then match them (block B of Fig. LABEL:ig:stereo_matching_pipeline),
    to produce a similarity score $C(x,d)$, using either standard similarity metrics
    such as the $L_{1}$, the $L_{2}$, and the correlation metric, or metrics learned
    using a top network. The two components can be trained either separately or jointly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的深度学习技术用于立体匹配，取代了手工制作的特征（图LABEL:ig:stereo_matching_pipeline的A块），采用了学习到的特征[[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [42](#bib.bib42)]。它们取两个图块，一个中心在左图中的像素$x=(i,j)$，另一个中心在右图中的像素$y=(i,j-d)$（其中$d\in\{0,\dots,n_{d}\}$），使用CNN计算它们的特征向量，然后匹配它们（图LABEL:ig:stereo_matching_pipeline的B块），生成相似度分数$C(x,d)$，使用标准的相似度度量如$L_{1}$、$L_{2}$和相关度量，或者使用顶级网络学习的度量。这两个组件可以分别训练，也可以联合训练。
- en: 4.1.1 The basic network architecture
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 基本网络架构
- en: 'The basic network architecture, introduced in [[37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39), [42](#bib.bib42)] and shown in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1
    Learning feature extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation")-(a), is composed
    of two CNN encoding branches, which act as descriptor computation modules. The
    first branch takes a patch around a pixel $x=(i,j)$ on the left image and outputs
    a feature vector that characterizes that patch. The second branch takes a patch
    around the pixel $y=(i,j-d)$, where $d\in[d_{min},d_{max}]$ is a candidate disparity.
    Zbontar and LeCun [[39](#bib.bib39)] and later Zbontar *et al.* [[42](#bib.bib42)]
    use an encoder composed of four convolutional layers, see Fig. [2](#S4.F2 "Figure
    2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(a).
    Each layer, except the last one, is followed by a ReLU unit. Zagoruyko and Komodakis [[37](#bib.bib37)]
    and Han *et al.* [[38](#bib.bib38)] use a similar architecture but add:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基本网络架构在[[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [42](#bib.bib42)]中介绍，并在图[2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(a)中展示，由两个CNN编码分支组成，这些分支作为描述符计算模块。第一个分支接收左图中一个像素$x=(i,j)$周围的图块，并输出一个特征向量来表征该图块。第二个分支接收围绕像素$y=(i,j-d)$的图块，其中$d\in[d_{min},d_{max}]$是一个候选视差。Zbontar和LeCun[[39](#bib.bib39)]及后来Zbontar
    *et al.*[[42](#bib.bib42)]使用由四个卷积层组成的编码器，见图[2](#S4.F2 "Figure 2 ‣ 4.1 Learning
    feature extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")-(a)。除了最后一层，每一层后面都跟着一个ReLU单元。Zagoruyko和Komodakis[[37](#bib.bib37)]以及Han
    *et al.*[[38](#bib.bib38)]使用类似的架构，但增加了：
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: max-pooling and subsampling after each layer, except the last one, see Fig. [2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(b).
    As such, the network is able to account for larger patch sizes and a larger variation
    in the viewpoint compared to [[39](#bib.bib39), [42](#bib.bib42)].
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每一层之后都进行最大池化和子采样，除了最后一层，见图 [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction
    and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")-(b)。因此，与 [[39](#bib.bib39), [42](#bib.bib42)]
    相比，网络能够处理更大的补丁尺寸和更大的视角变化。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a Spatial Pyramid Pooling (SPP) module at the end of each feature extraction
    branch [[37](#bib.bib37)] so that the network can process patches of arbitrary
    sizes while producing features of a fixed size, see Fig. [2](#S4.F2 "Figure 2
    ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching ‣
    A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(c).
    Its role is to aggregate the features of the last convolutional layer, through
    spatial pooling, into a feature grid of a fixed size. The module is designed in
    such a way that the size of the pooling regions varies with the size of the input
    to ensure that the output feature grid has a fixed size independently of the size
    of the input patch or image. Thus, the network is able to process patches/images
    of arbitrary sizes and compute feature vectors of the same dimension without changing
    its structure or retraining.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个特征提取分支的末尾添加一个空间金字塔池化 (SPP) 模块 [[37](#bib.bib37)]，以便网络能够处理任意大小的补丁，同时产生固定大小的特征，见图
    [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(c)。其作用是通过空间池化将最后一层卷积的特征汇总到一个固定大小的特征网格中。该模块的设计方式使得池化区域的大小随着输入的大小变化，以确保输出特征网格具有固定的大小，而不依赖于输入补丁或图像的大小。因此，网络能够处理任意大小的补丁/图像，并计算相同维度的特征向量，而无需改变其结构或重新训练。
- en: The learned features are then fed to a top module, which returns a similarity
    score. It can be implemented as a standard similarity metric, *e.g.,* the $L_{2}$
    distance, the cosine distance, and the (normalized) correlation distance (or inner
    product) as in the MC-CNN-fast (MC-CNN-fst) architecture of [[39](#bib.bib39),
    [42](#bib.bib42)]. The main advantage of the correlation over the $L_{2}$ distance
    is that it can be implemented using a layer of 2D [[51](#bib.bib51)] or 1D [[22](#bib.bib22)]
    convolutional operations, called *correlation layer*. A correlation layer does
    not require training since the filters are in fact the features computed by the
    second branch of the network. As such, correlation layers have been extensively
    used in the literature [[39](#bib.bib39), [41](#bib.bib41), [42](#bib.bib42),
    [22](#bib.bib22), [44](#bib.bib44)].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的特征然后被输入到一个顶部模块，该模块返回一个相似度评分。它可以实现为标准的相似度度量，*例如*，$L_{2}$ 距离、余弦距离和（归一化的）相关距离（或内积），如在
    [[39](#bib.bib39), [42](#bib.bib42)] 的 MC-CNN-fast (MC-CNN-fst) 架构中。相关距离相对于 $L_{2}$
    距离的主要优势在于，它可以通过 2D [[51](#bib.bib51)] 或 1D [[22](#bib.bib22)] 卷积操作层实现，称为 *相关层*。相关层不需要训练，因为滤波器实际上是由网络的第二分支计算的特征。因此，相关层在文献中得到了广泛应用
    [[39](#bib.bib39), [41](#bib.bib41), [42](#bib.bib42), [22](#bib.bib22), [44](#bib.bib44)]。
- en: Instead of using hand-crafted similarity measures, recent works use a decision
    network composed of fully-connected (FC) layers [[38](#bib.bib38), [37](#bib.bib37),
    [42](#bib.bib42), [46](#bib.bib46), [49](#bib.bib49)], which can be implemented
    as $1\times 1$ convolutions, fully convolutional layers [[47](#bib.bib47)], or
    convolutional layers followed by fully-connected layers. The decision network
    is trained jointly with the feature extraction module to assess the similarity
    between two image patches. Han *et al.* [[38](#bib.bib38)] use a top network composed
    of three fully-connected layers followed by a softmax. Zagoruyko and Komodakis [[37](#bib.bib37)]
    use two linear fully connected layers (each with $512$ hidden units) that are
    separated by a ReLU activation layer while the MC-CNN-acrt network of Zbontar
    *et al.* [[42](#bib.bib42)] use up to five fully-connected layers. In all cases,
    the features computed by the two branches of the feature encoding module are first
    concatenated and then fed to the top network. Hartmann *et al.* [[47](#bib.bib47)],
    on the other hand, aggregate the features coming from multiple patches using mean
    pooling before feeding them to a decision network. The main advantage of aggregation
    by pooling over concatenation is that the former can handle any arbitrary number
    of patches without changing the architecture of the network or re-training it.
    As such, it is suitable for computing multi-patch similarity.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 近期的研究使用了由全连接（FC）层组成的决策网络[[38](#bib.bib38), [37](#bib.bib37), [42](#bib.bib42),
    [46](#bib.bib46), [49](#bib.bib49)]，而不是手工制作的相似性度量，这些网络可以实现为$1\times 1$卷积、全卷积层[[47](#bib.bib47)]，或者是卷积层后接全连接层。决策网络与特征提取模块联合训练，以评估两个图像块之间的相似性。Han
    *et al.*[[38](#bib.bib38)]使用了由三个全连接层和一个softmax组成的顶层网络。Zagoruyko和Komodakis[[37](#bib.bib37)]使用两个线性全连接层（每个有$512$个隐藏单元），并由ReLU激活层分隔，而Zbontar
    *et al.*[[42](#bib.bib42)]的MC-CNN-acrt网络使用了多达五个全连接层。在所有情况下，由特征编码模块的两个分支计算出的特征首先被连接在一起，然后输入到顶层网络中。另一方面，Hartmann
    *et al.*[[47](#bib.bib47)]通过均值池化将来自多个图像块的特征进行聚合，然后将其输入到决策网络中。池化聚合的主要优点是它可以处理任意数量的图像块，而不需要更改网络架构或重新训练。因此，它适合于计算多图像块的相似性。
- en: Using a decision network instead of hand-crafted similarity measures enables
    learning, from data, the appropriate similarity measure instead of imposing one
    at the outset. It is more accurate than using a correlation layer but is significantly
    slower.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策网络代替手工制作的相似性度量方法可以从数据中学习到适当的相似性度量，而不是一开始就强加一个。这比使用相关层更准确，但速度明显较慢。
- en: 4.1.2 Network architecture variants
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 网络架构变体
- en: 'Since its introduction, the baseline architecture has been extended in several
    ways in order to: (1) improve training using residual networks (ResNet) [[46](#bib.bib46)],
    (2) enlarge the receptive field of the network without losing in resolution or
    in computation efficiency [[48](#bib.bib48), [49](#bib.bib49), [52](#bib.bib52)],
    (3) handling multiscale features [[37](#bib.bib37), [40](#bib.bib40)], (4) reducing
    the number of forward passes [[37](#bib.bib37), [44](#bib.bib44)], and (5) easing
    the training procedure by learning similarity without explicitly learning features [[37](#bib.bib37)].'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 自引入以来，基线架构已经在几个方面进行了扩展，以：(1) 使用残差网络（ResNet）[[46](#bib.bib46)]改进训练，(2) 扩大网络的接收域，同时不在分辨率或计算效率上妥协[[48](#bib.bib48),
    [49](#bib.bib49), [52](#bib.bib52)]，(3) 处理多尺度特征[[37](#bib.bib37), [40](#bib.bib40)]，(4)
    减少前向传播次数[[37](#bib.bib37), [44](#bib.bib44)]，以及(5) 通过学习相似性而非显式学习特征来简化训练过程[[37](#bib.bib37)]。
- en: 4.1.2.1 ConvNet vs. ResNet
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.1 ConvNet与ResNet
- en: While Zbontar *et al.* [[39](#bib.bib39), [42](#bib.bib42)] and Han *et al.* [[38](#bib.bib38)]
    use standard convolutional layers in the feature extraction block, Shaked and
    Wolf [[46](#bib.bib46)] add residual blocks with multilevel weighted residual
    connections to facilitate the training of very deep networks. Its particularity
    is that the network learns by itself how to adjust the contribution of the added
    skip connections. It was demonstrated that this architecture outperforms the base
    network of Zbontar *et al.* [[39](#bib.bib39)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Zbontar *et al.*[[39](#bib.bib39), [42](#bib.bib42)]和Han *et al.*[[38](#bib.bib38)]在特征提取块中使用了标准卷积层，但Shaked和Wolf[[46](#bib.bib46)]则添加了具有多级加权残差连接的残差块，以促进非常深层网络的训练。其特别之处在于网络能够自我调整跳跃连接的贡献。已经证明，这种架构优于Zbontar
    *et al.*[[39](#bib.bib39)]的基础网络。
- en: 4.1.2.2 Enlarging the receptive field of the network
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.2 扩大网络的接收域
- en: The scale of the learned features is defined by (1) the size of the input patches,
    (2) the receptive field of the network, and (3) the kernel size of the convolutional
    filters and pooling operations used in each layer. While increasing the kernel
    sizes allows the capture of more global interactions between the image pixels,
    it induces a high computational cost. Also, the conventional pooling, as used
    in [[39](#bib.bib39), [42](#bib.bib42)], reduces resolution and could cause the
    loss of fine details, which is not suitable for dense correspondence estimation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 学习特征的尺度由（1）输入补丁的大小，（2）网络的感受野，以及（3）每层中使用的卷积滤波器和池化操作的核大小定义。虽然增加核大小可以捕捉图像像素之间更多的全局交互，但这会导致高计算成本。此外，传统的池化，如在[[39](#bib.bib39)，[42](#bib.bib42)]中使用的，会降低分辨率并可能导致细节丢失，这不适合密集对应估计。
- en: To enlarge the receptive field without losing resolution or increasing the computation
    time, some techniques, *e.g.,*  [[52](#bib.bib52)], use dilated convolutions,
    *i.e.,* large convolutional filters but with holes and thus they are computationally
    efficient. Other techniques, *e.g.,*  [[48](#bib.bib48), [49](#bib.bib49)], use
    spatial pyramid pooling (SPP) modules placed at different locations in the network,
    see Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣
    4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(c-e). For instance, Park *et al.* [[48](#bib.bib48)], who
    introduced FW-CNN for stereo matching, append an SPP module at the end of the
    decision network, see Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction
    and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")-(d). As a result, the receptive field can
    be enlarged. However, for each pixel in the reference image, both the fully-connected
    layers and the pooling operations need to be computed $n_{d}$ times where $n_{d}$
    is the number of disparity levels. To avoid this, Ye *et al.* [[49](#bib.bib49)]
    place the SPP module at the end of each feature computation branch, see Figs. [2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(c)
    and (e). In this way, it is only computed once for each patch. Also, Ye *et al.* [[49](#bib.bib49)]
    employ multiple one-stride poolings, with different window sizes, to different
    layers and then concatenate their outputs to generate the feature maps, see Fig. [2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(e).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不降低分辨率或增加计算时间的情况下扩大感受野，一些技术，例如，[[52](#bib.bib52)]，使用扩张卷积，即大型卷积滤波器但带有孔，因此计算效率高。其他技术，例如，[[48](#bib.bib48)，[49](#bib.bib49)]，使用空间金字塔池化（SPP）模块，放置在网络中的不同位置，见图[2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(c-e)。例如，Park
    *等人* [[48](#bib.bib48)]，介绍了用于立体匹配的FW-CNN，在决策网络的末尾附加了一个SPP模块，见图[2](#S4.F2 "Figure
    2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(d)。结果，感受野可以扩大。然而，对于参考图像中的每个像素，完全连接层和池化操作需要计算$n_{d}$次，其中$n_{d}$是视差级别的数量。为了避免这种情况，Ye
    *等人* [[49](#bib.bib49)]将SPP模块放在每个特征计算分支的末尾，见图[2](#S4.F2 "Figure 2 ‣ 4.1 Learning
    feature extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")-(c)和(e)。这样，每个补丁只需计算一次。此外，Ye
    *等人* [[49](#bib.bib49)]采用多个单步池化，使用不同的窗口大小，作用于不同层，然后将它们的输出拼接以生成特征图，见图[2](#S4.F2
    "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(e)。
- en: 4.1.2.3 Learning multiscale features
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.3 学习多尺度特征
- en: '| ![Refer to caption](img/7f1b26a76303eb335624b4e0b061cd5e.png) | ![Refer to
    caption](img/1aa809e7b9418ebdaeae7a96afe38a2d.png) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/7f1b26a76303eb335624b4e0b061cd5e.png) | ![参见说明](img/1aa809e7b9418ebdaeae7a96afe38a2d.png)
    |'
- en: '| (a) Center-surround [[37](#bib.bib37)] | (b) Voting-based [[40](#bib.bib40)].
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| (a) 中心-环绕[[37](#bib.bib37)] | (b) 基于投票[[40](#bib.bib40)]。 |'
- en: 'Figure 3: Multiscale feature learning architectures.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '图3: 多尺度特征学习架构。'
- en: The methods described so far can be extended to learn features at multiple scales
    by using multi-stream networks, one stream per patch size [[37](#bib.bib37), [40](#bib.bib40)],
    see Fig. [3](#S4.F3 "Figure 3 ‣ 4.1.2.3 Learning multiscale features ‣ 4.1.2 Network
    architecture variants ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation"). Zagoruyko and Komodakis [[37](#bib.bib37)] propose a two-stream
    network, which is essentially a network composed of two siamese networks combined
    at the output by a top network, see Fig. [3](#S4.F3 "Figure 3 ‣ 4.1.2.3 Learning
    multiscale features ‣ 4.1.2 Network architecture variants ‣ 4.1 Learning feature
    extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(a). The first siamese network,
    called central high-resolution stream, receives as input two $32\times 32$ patches
    centered around the pixel of interest. The second network, called surround low-resolution
    stream, receives as input two $64\times 64$ patches but down-sampled to $32\times
    32$. The output of the two streams are then concatenated and fed to a top decision
    network, which returns a matching score. Chen *et al.* [[40](#bib.bib40)] use
    a similar approach but instead of aggregating the features computed by the two
    streams prior to feeding them to the top decision network, it appends a top network
    on each stream to produce a matching score. The two scores are then aggregated
    by voting, see Fig. [3](#S4.F3 "Figure 3 ‣ 4.1.2.3 Learning multiscale features
    ‣ 4.1.2 Network architecture variants ‣ 4.1 Learning feature extraction and matching
    ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(b).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 目前描述的方法可以通过使用多流网络扩展到在多个尺度上学习特征，每个补丁大小一个流[[37](#bib.bib37), [40](#bib.bib40)]，见图[3](#S4.F3
    "Figure 3 ‣ 4.1.2.3 Learning multiscale features ‣ 4.1.2 Network architecture
    variants ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")。Zagoruyko和Komodakis[[37](#bib.bib37)]提出了一种双流网络，本质上是由两个西阿摩斯网络组成，通过一个顶层网络在输出处结合，见图[3](#S4.F3
    "Figure 3 ‣ 4.1.2.3 Learning multiscale features ‣ 4.1.2 Network architecture
    variants ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(a)。第一个西阿摩斯网络，称为中央高分辨率流，接收两个$32\times
    32$的补丁，中心围绕感兴趣的像素。第二个网络，称为周围低分辨率流，接收两个$64\times 64$的补丁，但下采样到$32\times 32$。这两个流的输出然后被连接并送入一个顶层决策网络，该网络返回一个匹配分数。Chen
    *et al.* [[40](#bib.bib40)]使用了类似的方法，但不是在将两个流计算的特征送入顶层决策网络之前进行聚合，而是在每个流上附加一个顶层网络以产生匹配分数。然后通过投票聚合这两个分数，见图[3](#S4.F3
    "Figure 3 ‣ 4.1.2.3 Learning multiscale features ‣ 4.1.2 Network architecture
    variants ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(b)。
- en: The main advantage of the multi-stream architecture is that it can compute features
    at multiple scales in a single forward pass. It, however, requires one stream
    per scale, which is not practical if more than two scales are needed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 多流架构的主要优点是它可以在一次前向传递中计算多个尺度的特征。然而，它需要每个尺度一个流，如果需要多个尺度，这种方法就不太实际。
- en: 4.1.2.4 Reducing the number of forward passes
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.4 减少前向传递的次数
- en: Using the approaches described so far, inferring the raw cost volume from a
    pair of stereo images is performed using a moving window-like approach, which
    would require multiple forward passes, $n_{d}$ forward passes per pixel where
    $n_{d}$ is the number of disparity levels. However, since correlations are highly
    parallelizable, the number of forward passes can be significantly reduced. For
    instance, Luo *et al.* [[44](#bib.bib44)] reduce the number of forward passes
    to one pass per pixel by using a siamese network, whose first branch takes a patch
    around a pixel while the second branch takes a larger patch that expands over
    all possible disparities. The output is a single 64D representation for the left
    branch, and $n_{d}\times 64$ for the right branch. A correlation layer then computes
    a vector of length $n_{d}$, where its $0pt-$th element is the cost of matching
    the pixel $x$ on the left image with the pixel $x-0pt$ on the rectified right
    image.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迄今为止描述的方法，从一对立体图像推断原始成本体积是通过类似移动窗口的方法进行的，这需要每个像素进行多次前向传播，每个像素需要 $n_{d}$ 次前向传播，其中
    $n_{d}$ 是视差水平的数量。然而，由于相关性高度并行化，前向传播的次数可以显著减少。例如，Luo *et al.* [[44](#bib.bib44)]
    通过使用一个西格玛网络，将前向传播次数减少到每个像素一次，该网络的第一个分支处理像素周围的一个补丁，而第二个分支处理一个扩展到所有可能视差的大补丁。输出是左分支的一个
    64D 表示，以及右分支的 $n_{d}\times 64$。然后，相关层计算长度为 $n_{d}$ 的向量，其中其 $0pt-$th 元素是将左图像上的像素
    $x$ 与校正右图像上的像素 $x-0pt$ 匹配的成本。
- en: Zagoruyko and Komodakis [[37](#bib.bib37)] showed that the outputs of the two
    feature extraction sub-networks need to be computed only once per pixel, and do
    not need to be recomputed for every disparity under consideration. This can be
    done in a single forward pass, for the entire image, by propagating full-resolution
    images instead of small patches. Also, the output of the top network composed
    of fully-connected layers in the accurate architecture (*i.e.,* MC-CNN-Accr) can
    be computed in a single forward pass by replacing the fully-connected layers with
    convolutional layers of $1\times 1$ kernels. However, it still requires one forward
    pass for each disparity under consideration.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Zagoruyko 和 Komodakis [[37](#bib.bib37)] 表明，两种特征提取子网络的输出只需每个像素计算一次，不需要对每个视差进行重新计算。这可以通过传播全分辨率图像而不是小补丁在整个图像中进行单次前向传播来完成。此外，由完全连接层组成的顶层网络（即
    MC-CNN-Accr）的输出可以通过用 $1\times 1$ 卷积层替代完全连接层来在单次前向传播中计算。然而，它仍然需要对每个考虑中的视差进行一次前向传播。
- en: 4.1.2.5 Learning similarity without feature learning
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.2.5 无需特征学习的相似性学习
- en: Joint training of feature extraction and similarity computation networks unifies
    the feature learning and the metric learning steps. Zagoruyko and Komodakis [[37](#bib.bib37)]
    propose another architecture that does not have a direct notion of features, see
    Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(f). In this architecture, the left and right patches are packed
    together and fed jointly into a two-channel network composed of convolution and
    ReLU layers followed by a set of fully connected layers. Instead of computing
    features, the network directly outputs the similarity between the input pair of
    patches. Zagoruyko and Komodakis [[37](#bib.bib37)] showed that this architecture
    is easy to train. However, it is expensive at runtime since the whole network
    needs to be run $n_{d}$ times per pixel.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取和相似性计算网络的联合训练将特征学习和度量学习步骤统一起来。Zagoruyko 和 Komodakis [[37](#bib.bib37)] 提出了另一种架构，该架构没有直接的特征概念，见图
    [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(f)。在这种架构中，左侧和右侧的补丁被一起打包，并共同输入到由卷积和 ReLU 层组成的双通道网络中，之后是完全连接层。网络直接输出输入补丁对之间的相似性，而不是计算特征。Zagoruyko
    和 Komodakis [[37](#bib.bib37)] 表明这种架构易于训练。然而，它在运行时成本较高，因为整个网络需要每个像素运行 $n_{d}$
    次。
- en: 4.1.3 Training procedures
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 训练程序
- en: The networks described in this section are composed of a feature extraction
    block and a feature matching block. Since the goal is to learn how to match patches,
    these two modules are jointly trained either in a supervised (Section [4.1.3.1](#S4.SS1.SSS3.P1
    "4.1.3.1 Supervised training ‣ 4.1.3 Training procedures ‣ 4.1 Learning feature
    extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")) or in a weakly supervised manner
    (Section [4.1.3.2](#S4.SS1.SSS3.P2 "4.1.3.2 Weakly supervised learning ‣ 4.1.3
    Training procedures ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by
    stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述的网络由特征提取模块和特征匹配模块组成。由于目标是学习如何匹配图块，这两个模块要么以监督学习（第[4.1.3.1节](#S4.SS1.SSS3.P1
    "4.1.3.1 Supervised training ‣ 4.1.3 Training procedures ‣ 4.1 Learning feature
    extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")），要么以弱监督学习（第[4.1.3.2节](#S4.SS1.SSS3.P2
    "4.1.3.2 Weakly supervised learning ‣ 4.1.3 Training procedures ‣ 4.1 Learning
    feature extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")）的方式进行联合训练。
- en: 4.1.3.1 Supervised training
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.3.1 监督训练
- en: Existing methods for supervised training use a training set composed of positive
    and negative examples. Each positive (respectively negative) example is a pair
    composed of a reference patch and its matching patch (respectively a non-matching
    one) from another image. Training either takes one example at a time, positive
    or negative, and adapts the similarity [[40](#bib.bib40), [41](#bib.bib41), [38](#bib.bib38),
    [37](#bib.bib37)], or takes at each step both a positive and a negative example,
    and maximizes the difference between the similarities, hence aiming at making
    the two patches from the positive pair more similar than the two patches from
    the negative pair [[43](#bib.bib43), [45](#bib.bib45), [39](#bib.bib39)]. This
    latter scheme is known as Triplet Contrastive learning.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的监督训练方法使用由正负样本组成的训练集。每个正样本（或负样本）是由参考图块和其匹配图块（或不匹配图块）组成的对。训练过程要么一次使用一个样本，正样本或负样本，并调整相似度[[40](#bib.bib40),
    [41](#bib.bib41), [38](#bib.bib38), [37](#bib.bib37)]，要么每一步同时使用一个正样本和一个负样本，并最大化相似度之间的差异，从而使正样本对中的两个图块更相似于彼此，而比负样本对中的两个图块更相似[[43](#bib.bib43),
    [45](#bib.bib45), [39](#bib.bib39)]。这种方法被称为三元组对比学习。
- en: Zbontar *et al.* [[39](#bib.bib39), [42](#bib.bib42)] use the ground-truth disparities
    of the KITTI2012 [[15](#bib.bib15)] or Middlebury [[20](#bib.bib20)] datasets.
    For each known disparity, the method extracts one negative pair and one positive
    pair as training examples. As such, the method is able to extract more than $25$
    million training samples from KITTI2012 [[15](#bib.bib15)] and more than $38$
    million from the Middlebury dataset [[20](#bib.bib20)]. This method has been also
    used by Chen *et al.* [[40](#bib.bib40)], Zagoruyku and Komodakis [[37](#bib.bib37)],
    and Han *et al.* [[38](#bib.bib38)]. The amount of training data can be further
    augmented by using data augmentation techniques, *e.g.,* flipping patches and
    rotating them in various directions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Zbontar *等人* [[39](#bib.bib39), [42](#bib.bib42)] 使用了KITTI2012 [[15](#bib.bib15)]
    或 Middlebury [[20](#bib.bib20)] 数据集的真实视差。对于每一个已知视差，该方法提取一个负样本对和一个正样本对作为训练样本。因此，该方法能够从KITTI2012
    [[15](#bib.bib15)] 数据集中提取超过$25$百万个训练样本，从Middlebury数据集中提取超过$38$百万个样本。该方法也被Chen
    *等人* [[40](#bib.bib40)]，Zagoruyku 和 Komodakis [[37](#bib.bib37)]，以及Han *等人* [[38](#bib.bib38)]
    使用。通过使用数据增强技术，如翻转图块和在不同方向上旋转图块，可以进一步增加训练数据的数量。
- en: Although the supervised learning works very well, the complexity of the neural
    network models requires very large labeled training sets, which are hard or costly
    to collect for real applications (*e.g.,* consider the stereo reconstruction of
    the Mars landscape). Even when such large sets are available, the ground truth
    is usually produced from depth sensors and often contains noise that reduces the
    effectiveness of the supervised learning [[53](#bib.bib53)]. This can be mitigated
    by augmenting the training set with random perturbations [[39](#bib.bib39)] or
    synthetic data [[54](#bib.bib54), [22](#bib.bib22)]. However, synthesis procedures
    are hand-crafted and do not account for the regularities specific to the stereo
    system and target scene at hand.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督学习效果很好，但神经网络模型的复杂性需要非常大的标注训练集，这对于实际应用来说难以或成本高昂（*例如，* 考虑火星地形的立体重建）。即使有如此大规模的数据集，真实值通常是通过深度传感器生成的，并且通常包含噪声，从而降低了监督学习的有效性
    [[53](#bib.bib53)]。这可以通过用随机扰动 [[39](#bib.bib39)] 或合成数据 [[54](#bib.bib54), [22](#bib.bib22)]
    增强训练集来缓解。然而，合成过程是手工制作的，并未考虑立体系统和目标场景的特定规律。
- en: Loss functions. Supervised stereo matching networks are trained to minimize
    a matching loss, which is a function that measures the discrepancy between the
    ground-truth and the predicted matching scores for each training sample. It can
    be defined using (1) the $L_{1}$ distance [[42](#bib.bib42), [40](#bib.bib40),
    [46](#bib.bib46)], (2) the hinge loss [[42](#bib.bib42), [46](#bib.bib46)], or
    (3) the cross-entropy loss [[44](#bib.bib44)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数。监督立体匹配网络通过最小化匹配损失进行训练，该损失函数测量每个训练样本的真实值与预测匹配分数之间的差异。可以使用（1）$L_{1}$ 距离 [[42](#bib.bib42),
    [40](#bib.bib40), [46](#bib.bib46)]，（2）铰链损失 [[42](#bib.bib42), [46](#bib.bib46)]，或（3）交叉熵损失
    [[44](#bib.bib44)] 来定义。
- en: 4.1.3.2 Weakly supervised learning
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.1.3.2 弱监督学习
- en: 'Weakly supervised techniques exploit one or more stereo constraints to reduce
    the amount of manual labelling. Tulyakov *et al.* [[50](#bib.bib50)] consider
    Multi-Instance Learning (MIL) in conjunction with stereo constraints and coarse
    information about the scene to train stereo matching networks with datasets for
    which ground truth is not available. Unlike supervised techniques, which require
    pairs of matching and non-matching patches, the training set is composed of $N$
    triplets. Each triplet is composed of: (1) $W$ reference patches extracted on
    a horizontal line of the reference image, (2) $W$ positive patches extracted from
    the corresponding horizontal line on the right image, and (3) $W$ negative patches,
    *i.e.,* patches that do not match the reference patches, extracted from another
    horizontal line on the right image. As such, the training set can automatically
    be constructed from stereo pairs without manual labelling.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督技术利用一个或多个立体约束来减少手动标注的数量。Tulyakov *等* [[50](#bib.bib50)] 将多实例学习（MIL）与立体约束和场景的粗略信息结合起来，以训练用于没有真实值的数据集的立体匹配网络。与需要配对的匹配和不匹配补丁的监督技术不同，训练集由$N$个三元组组成。每个三元组包括：（1）从参考图像的水平线提取的$W$个参考补丁，（2）从右图像的相应水平线提取的$W$个正补丁，以及（3）从右图像的另一条水平线提取的$W$个负补丁，*即，*
    不匹配参考补丁的补丁。因此，训练集可以从立体对自动构建，无需手动标注。
- en: 'The method is then trained by exploiting five constraints: the epipolar constraint,
    the disparity range constraint, the uniqueness constraint, the continuity (smoothness)
    constraint, and the ordering constraint. They then define three losses that use
    different subsets of these constraints, namely:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过利用五个约束来训练该方法：极线约束、视差范围约束、唯一性约束、连续性（平滑性）约束和排序约束。他们定义了三种使用这些约束不同子集的损失函数，即：
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Multi Instance Learning (MIL) loss, which uses the epipolar and the disparity
    range constraints. From these two constraints, we know that every non-occluded
    reference patch has a matching positive patch in a known index interval, but does
    not have a matching negative patch. Therefore, for every reference patch, the
    similarity of the best reference-positive match should be greater than the similarity
    of the best reference-negative match.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多实例学习（MIL）损失函数利用了极线约束和视差范围约束。根据这两个约束，我们知道每个非遮挡的参考补丁在已知的索引区间内都有一个匹配的正补丁，但没有匹配的负补丁。因此，对于每个参考补丁，最佳参考正匹配的相似度应大于最佳参考负匹配的相似度。
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The constractive loss, which adds to the MIL method the uniqueness constraint.
    It tells us that the matching positive patch is unique. Thus, for every patch,
    the similarity of the best match should be greater than the similarity of the
    second best match.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Constractive** 损失为 MIL 方法增加了唯一性约束。它告诉我们匹配的正样本是唯一的。因此，对于每个补丁，最佳匹配的相似度应该大于第二佳匹配的相似度。'
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The constractive-DP uses all the constraints but finds the best match using
    dynamic programming.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Constractive-DP** 使用所有约束条件，但通过动态规划找到最佳匹配。'
- en: The method has been applied to train a deep siamese neural-network that takes
    two patches as an input and predicts a similarity measure. Benchmarking on standard
    datasets shows that the performance is as good or better than the published results
    on MC-CNN-fst [[39](#bib.bib39)], which uses the same network architecture but
    trained using fully labeled data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法已应用于训练一个深度**孪生神经网络**，该网络以两个补丁作为输入并预测相似度度量。在标准数据集上的基准测试表明，其性能与 MC-CNN-fst [[39](#bib.bib39)]
    上发布的结果一样好或更好，后者使用相同的网络架构，但使用完全标记的数据进行训练。
- en: 4.2 Regularization and disparity estimation
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 正则化和视差估计
- en: 'Once the raw cost volume is estimated, one can estimate the disparity by dropping
    the regularization term of Eqn. ([1](#S4.E1 "In 4 Depth by stereo matching ‣ A
    Survey on Deep Learning Techniques for Stereo-based Depth Estimation")), or equivalently
    block C of Fig. [1](#S4.F1 "Figure 1 ‣ 4 Depth by stereo matching ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation"), and taking the argmin,
    the softargmin, or the subpixel MAP approximation (block D of Fig. [1](#S4.F1
    "Figure 1 ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")). However, the raw cost volume computed from
    image features could be noise-contaminated, *e.g.,* due to the existence of non-Lambertian
    surfaces, object occlusions, or repetitive patterns. Thus, the estimated depth
    maps can be noisy. As such, some methods overcome this problem by using traditional
    MRF-based stereo framework for cost volume regularization [[40](#bib.bib40), [39](#bib.bib39),
    [44](#bib.bib44)]. In these methods, the initial cost volume $C$ is fed to a global [[11](#bib.bib11)]
    or a semi-global [[55](#bib.bib55)] matcher to compute the disparity map. Semi-global
    matching provides a good tradeoff between accuracy and computation requirements.
    In this method, the smoothness term of Eqn. ([1](#S4.E1 "In 4 Depth by stereo
    matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation"))
    is defined as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦原始成本体积被估计出来，可以通过去掉 Eqn. ([1](#S4.E1 "在 4 深度通过立体匹配 ‣ 深度学习技术在立体深度估计中的应用")）中的正则化项来估计视差，或者等效地，图
    Fig. [1](#S4.F1 "图 1 ‣ 4 深度通过立体匹配 ‣ 深度学习技术在立体深度估计中的应用") 中的块 C，然后取 argmin、softargmin
    或亚像素 MAP 近似（图 Fig. [1](#S4.F1 "图 1 ‣ 4 深度通过立体匹配 ‣ 深度学习技术在立体深度估计中的应用") 中的块 D）。然而，从图像特征计算出的原始成本体积可能会受到噪声污染，*例如*，由于非兰伯特表面、物体遮挡或重复图案的存在。因此，估计的深度图可能会很嘈杂。因此，一些方法通过使用传统的
    MRF 基于立体的框架来进行成本体积正则化来克服这个问题 [[40](#bib.bib40), [39](#bib.bib39), [44](#bib.bib44)]。在这些方法中，初始成本体积
    $C$ 被输入到全局 [[11](#bib.bib11)] 或半全局 [[55](#bib.bib55)] 匹配器中以计算视差图。半全局匹配提供了准确性和计算需求之间的良好折衷。在这种方法中，Eqn. ([1](#S4.E1
    "在 4 深度通过立体匹配 ‣ 深度学习技术在立体深度估计中的应用")) 的平滑项定义为：
- en: '|  | $\small{E_{s}(0pt_{x},0pt_{y})=\alpha_{1}\delta(&#124;0pt_{xy}=1)+\alpha_{2}\delta(&#124;0pt_{xy}>1),}$
    |  | (2) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{E_{s}(0pt_{x},0pt_{y})=\alpha_{1}\delta(&#124;0pt_{xy}=1)+\alpha_{2}\delta(&#124;0pt_{xy}>1),}$
    |  | (2) |'
- en: 'where $0pt_{xy}=0pt_{x}-0pt_{y}$, $\alpha_{1}$ and $\alpha_{2}$ are positive
    weights chosen such that $\alpha_{2}>\alpha_{1}$, and $\delta$ is the Kronecker
    delta function, which gives $1$ when the condition in the bracket is satisfied,
    otherwise $0$. To solve this optimisation problem, the SGM energy is broken down
    into multiple energies $E_{s}$, each one defined along a path $s$. The energies
    are minimised independently and then aggregated. The disparity at $x$ is computed
    using the winner-takes-all strategy of the aggregated costs of all directions:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $0pt_{xy}=0pt_{x}-0pt_{y}$，$\alpha_{1}$ 和 $\alpha_{2}$ 是选择的正权重，使得 $\alpha_{2}>\alpha_{1}$，$\delta$
    是克罗内克 δ 函数，当括号中的条件满足时，值为 $1$，否则为 $0$。为了求解这个优化问题，SGM 能量被分解为多个能量 $E_{s}$，每个能量沿路径
    $s$ 定义。这些能量被独立最小化，然后聚合。使用所有方向的聚合成本的赢家通吃策略计算 $x$ 处的视差：
- en: '|  | $d_{x}=\arg\min_{d}\sum_{s}E_{s}(x,d).$ |  | (3) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{x}=\arg\min_{d}\sum_{s}E_{s}(x,d).$ |  | (3) |'
- en: This method requires setting the two parameters $\alpha_{1}$ and $\alpha_{2}$
    of Eqn. ([2](#S4.E2 "In 4.2 Regularization and disparity estimation ‣ 4 Depth
    by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")). Instead of manually setting them, Seki *et al.* [[56](#bib.bib56)]
    proposed SGM-Net, a neural network trained to provide these parameters at each
    image pixel. They obtained better penalties than hand-tuned methods as in [[39](#bib.bib39)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法需要设置方程式 ([2](#S4.E2 "在 4.2 正则化和视差估计 ‣ 4 通过立体匹配获得深度 ‣ 基于深度学习的立体深度估计技术综述"))
    中的两个参数 $\alpha_{1}$ 和 $\alpha_{2}$。Seki *等人* [[56](#bib.bib56)] 提出了 SGM-Net，一种训练神经网络以在每个图像像素处提供这些参数的方法。他们获得了比手动调整方法
    [[39](#bib.bib39)] 更好的惩罚结果。
- en: 'The SGM method, which uses an aggregated scheme to combine costs from multiple
    1D scanline optimizations, suffers from two major issues: (1) streaking artifacts
    caused by the scanline optimization approach, at the core of this algorithm, may
    lead to inaccurate results, and (2) the high memory footprint that may become
    prohibitive with high resolution images or devices with constrained resources.
    As such Schonberger *et al.* [[57](#bib.bib57)] reformulate the fusion step as
    the task of selecting the best amongst all the scanline optimization proposals
    at each pixel in the image. They solve this task using a per-pixel random forest
    classifier.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: SGM 方法使用一种聚合方案来结合来自多个 1D 扫描线优化的成本，但存在两个主要问题：（1）扫描线优化方法引起的条纹伪影可能导致不准确的结果；（2）高内存占用可能在高分辨率图像或资源受限的设备上变得过于繁重。因此，Schonberger
    *等人* [[57](#bib.bib57)] 将融合步骤重新表述为在图像的每个像素中选择所有扫描线优化提议中的最佳值的任务。他们使用每像素随机森林分类器来解决这一任务。
- en: Poggi *et al.* [[58](#bib.bib58)] learn a weighted aggregation where the weight
    of each 1D scanline optimisation is defined using a confidence map computed using
    either traditional techniques [[59](#bib.bib59)] or deep neural networks, see
    Section [5.5](#S5.SS5 "5.5 Learning confidence maps ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation").
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Poggi *等人* [[58](#bib.bib58)] 研究了一种加权聚合，其中每个 1D 扫描线优化的权重是通过使用传统技术 [[59](#bib.bib59)]
    或深度神经网络计算的置信度图来定义的，详见第 [5.5](#S5.SS5 "5.5 学习置信度图 ‣ 5 从立体视觉端到端深度 ‣ 基于深度学习的立体深度估计技术综述")
    节。
- en: 5 End-to-end depth from stereo
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 从立体视觉端到端深度
- en: '![Refer to caption](img/435c5cc5241b4fa1af61701caf0f8fba.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/435c5cc5241b4fa1af61701caf0f8fba.png)'
- en: 'Figure 4: Taxonomy of the network architectures for stereo-based disparity
    estimation using end-to-end deep learning.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 基于立体视觉的视差估计的网络架构分类。'
- en: Recent works solve the stereo matching problem using a pipeline that is trained
    end-to-end. Two main classes of methods have been proposed. Early methods, *e.g.,*
    FlowNetSimple [[51](#bib.bib51)] and DispNetS [[22](#bib.bib22)], use a single
    encoder-decoder, which stacks together the left and right images into a 6D volume,
    and regresses the disparity map. These methods, which do not require an explicit
    feature matching module, are fast at runtime. They, however, require a large amount
    of training data, which is hard to obtain. Methods in the second class mimic the
    traditional stereo matching pipeline by breaking the problem into stages, each
    stage is composed of differentiable blocks and thus allowing end-to-end training.
    Below, we review in details these techniques. Fig. [4](#S5.F4 "Figure 4 ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation") provides a taxonomy of the state-of-the-art, while Table [III](#S5.T3
    "TABLE III ‣ 5.1 Feature learning ‣ 5 End-to-end depth from stereo ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation") compares $28$
    key methods based on this taxonomy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究通过使用端到端训练的管道解决立体匹配问题。已经提出了两类主要方法。早期方法，如 FlowNetSimple [[51](#bib.bib51)]
    和 DispNetS [[22](#bib.bib22)]，使用单个编码器-解码器，将左图和右图堆叠成一个 6D 体积，并回归视差图。这些方法不需要显式的特征匹配模块，运行速度快。然而，它们需要大量的训练数据，这些数据难以获得。第二类方法模仿传统的立体匹配管道，将问题分解为多个阶段，每个阶段由可微分的块组成，从而允许端到端训练。以下是对这些技术的详细回顾。图
    [4](#S5.F4 "图 4 ‣ 5 从立体视觉端到端深度 ‣ 基于深度学习的立体深度估计技术综述") 提供了最新技术的分类，而表 [III](#S5.T3
    "表 III ‣ 5.1 特征学习 ‣ 5 从立体视觉端到端深度 ‣ 基于深度学习的立体深度估计技术综述") 比较了基于该分类的 $28$ 种关键方法。
- en: 5.1 Feature learning
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 特征学习
- en: 'Feature learning networks follow the same architectures as the ones described
    in Figs. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature extraction and matching ‣
    4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") and [3](#S4.F3 "Figure 3 ‣ 4.1.2.3 Learning multiscale features
    ‣ 4.1.2 Network architecture variants ‣ 4.1 Learning feature extraction and matching
    ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation"). However, instead of processing individual patches, the entire
    images are processed in a single forward pass producing feature maps of the same
    or lower resolution as the input images. Two strategies have been used to enable
    matching features across the images:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 特征学习网络遵循与图 [2](#S4.F2 "图 2 ‣ 4.1 学习特征提取和匹配 ‣ 4 深度通过立体匹配 ‣ 立体深度估计的深度学习技术调查")
    和 [3](#S4.F3 "图 3 ‣ 4.1.2.3 学习多尺度特征 ‣ 4.1.2 网络架构变体 ‣ 4.1 学习特征提取和匹配 ‣ 4 深度通过立体匹配
    ‣ 立体深度估计的深度学习技术调查") 所描述的相同架构。然而，与处理单独的补丁不同，整个图像在一次前向传递中处理，生成与输入图像相同或较低分辨率的特征图。为了实现图像间的特征匹配，使用了两种策略：
- en: (1) Multi-branch networks composed of $n$ branches where $n$ is the number of
    input images. Each branch produces a feature map that characterizes its input
    image [[22](#bib.bib22), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)]. These techniques assume
    that the input images have been rectified so that the search for correspodnences
    is restricted to be along the horizontal scanlines.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 由 $n$ 个分支组成的多分支网络，其中 $n$ 是输入图像的数量。每个分支生成一个特征图，描述其输入图像 [[22](#bib.bib22),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65)]。这些技术假设输入图像已被矫正，因此对应点的搜索限制在水平扫描线上。
- en: (2) Multi-branch networks composed of $n_{d}$ branches where $n_{d}$ is the
    number of disparity levels. The $d$-th branch, $1\leq d\leq n_{d}$, processes
    a stack of two images, as in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Learning feature
    extraction and matching ‣ 4 Depth by stereo matching ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(f); the first image is the reference
    image. The second one is the right image but re-projected to the $d$-th depth
    plane [[66](#bib.bib66)]. Each branch produces a similarity feature map that characterizes
    the similarity between the reference image and the right image re-projected onto
    a given depth plane. While these techniques do not rectify the images, they assume
    that the intrinsic and extrinsic camera parameters are known. Also, the number
    of disparity levels cannot be varied without updating the network architecture
    and retraining it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 由 $n_{d}$ 个分支组成的多分支网络，其中 $n_{d}$ 是视差层级的数量。第 $d$ 个分支，$1\leq d\leq n_{d}$，处理一对图像的堆栈，如图 [2](#S4.F2
    "图 2 ‣ 4.1 学习特征提取和匹配 ‣ 4 深度通过立体匹配 ‣ 立体深度估计的深度学习技术调查")-(f) 所示；第一张图像是参考图像。第二张是右图像，但重新投影到第
    $d$ 个深度平面上 [[66](#bib.bib66)]。每个分支生成一个相似度特征图，描述参考图像与重新投影到给定深度平面上的右图像之间的相似性。虽然这些技术不对图像进行矫正，但假设已知内在和外在相机参数。此外，视差层级的数量不能在不更新网络架构和重新训练的情况下改变。
- en: In both methods, the feature extraction module uses either fully convolutional
    (ConvNet) networks such as VGG, or residual networks such as ResNets [[67](#bib.bib67)].
    The latter facilitates the training of very deep networks [[68](#bib.bib68)].
    They can also capture and incorporate more global context in the unary features
    by using either dilated convolutions (Section [4.1.2.2](#S4.SS1.SSS2.P2 "4.1.2.2
    Enlarging the receptive field of the network ‣ 4.1.2 Network architecture variants
    ‣ 4.1 Learning feature extraction and matching ‣ 4 Depth by stereo matching ‣
    A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")) or multi-scale
    approaches. For instance, the PSM-Net of Chang and Chen [[64](#bib.bib64)] append
    a Spatial Pyramid Pooling (SPP) module in order to extract and aggregate features
    at multiple scales. Nie *et al.* [[65](#bib.bib65)] extended PSM-Net using a multi-level
    context aggregation pattern termed *Multi-Level Context Ultra-Aggregation (MLCUA)*.
    It encapsulates all convolutional features into a more discriminative representation
    by intra and inter-level features combination. It combines the features at the
    shallowest, smallest scale with features at deeper, larger scales using just shallow
    skip connections. This results in an improved performance, compared to PSM-Net [[64](#bib.bib64)],
    without significantly increasing the number of parameters in the network.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种方法中，特征提取模块使用全卷积（ConvNet）网络，如VGG，或残差网络，如ResNets [[67](#bib.bib67)]。后者有助于训练非常深的网络
    [[68](#bib.bib68)]。它们还可以通过使用扩张卷积（第[4.1.2.2节](#S4.SS1.SSS2.P2 "4.1.2.2 扩大网络感受野
    ‣ 4.1.2 网络架构变种 ‣ 4.1 学习特征提取与匹配 ‣ 4 立体匹配的深度 ‣ 立体深度估计的深度学习技术综述")）或多尺度方法来捕捉并融入更多的全局上下文。例如，Chang和Chen
    [[64](#bib.bib64)] 的PSM-Net增加了一个空间金字塔池化（SPP）模块，以提取和汇聚多尺度的特征。Nie *et al.* [[65](#bib.bib65)]
    使用称为*多层上下文超聚合（MLCUA）*的模式扩展了PSM-Net。它通过层内和层间特征组合将所有卷积特征封装成更具区分性的表示。它使用浅层跳跃连接将最浅、最小尺度的特征与更深、大尺度的特征结合。这与PSM-Net
    [[64](#bib.bib64)] 相比，在不显著增加网络参数数量的情况下，性能有所提升。
- en: 'TABLE III: Taxonomy and comparison of $28$ end-to-end deep learning-based disparity
    estimation techniques. ”FCN”: Fully-Connected Network, ”SPN”: Spatial Propagation
    Network. ”LRCR”: Left-Right Comparative Recurrent model, ”MCUA”: Multi-Level Context
    Ultra-Aggregation for Stereo Matching. ”DLA”: Deep layer aggregation [[69](#bib.bib69)],
    ”VPP”: Volumetric Pyramid Pooling. The performance is measured on KITTI2015 test
    dataset.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：$28$种端到端深度学习基础的视差估计技术的分类与比较。“FCN”：全连接网络，“SPN”：空间传播网络。“LRCR”：左右对比递归模型，“MCUA”：用于立体匹配的多层上下文超聚合。“DLA”：深度层聚合
    [[69](#bib.bib69)]，“VPP”：体积金字塔池化。性能在KITTI2015测试数据集上进行测量。
- en: '| Method | Year |  | Feature computation |  | Cost volume |  | Disparity |  |
    Refinement/post processing |  | Supervision | Performance |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 |  | 特征计算 |  | 成本体积 |  | 差异 |  | 精细化/后处理 |  | 监督 | 性能 |'
- en: '|  | Architecture | Dimension |  | Type | Construction | Regularization |  |  |
    Spatial/depth resolution | Completion/denoising |  | D1-all/est | D1-all/fg |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | 架构 | 维度 |  | 类型 | 构造 | 正则化 |  |  | 空间/深度分辨率 | 完成/去噪 |  | D1-all/est |
    D1-all/fg |'
- en: '| FlowNetCorr [[51](#bib.bib51)] | 2015 |  | ConvNet | Single scale |  | 3D
    | Correlation | 2D ConvNet |  |  |  | Up-convolutions | Ad-hoc, variational |  |
    Supervised | $-$ | $-$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| FlowNetCorr [[51](#bib.bib51)] | 2015 |  | ConvNet | 单尺度 |  | 3D | 相关 | 2D
    ConvNet |  |  |  | 上采样卷积 | 临时，变分 |  | 监督 | $-$ | $-$ |'
- en: '| DispNetC [[22](#bib.bib22)] | 2016 |  | ConvNet | Single scale |  | 3D |
    Correlation | 2D ConvNet |  | $-$ |  | $-$ | $-$ |  | Supervised | $4.34$ | $4.32$
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| DispNetC [[22](#bib.bib22)] | 2016 |  | ConvNet | 单尺度 |  | 3D | 相关 | 2D ConvNet
    |  | $-$ |  | $-$ | $-$ |  | 监督 | $4.34$ | $4.32$ |'
- en: '| Zhong *et al.* [[70](#bib.bib70)] | 2017 |  | ConvNet | Single scale |  |
    4D | Interleaved | 3D Conv, |  | Soft argmin |  | Self-improvement at runtime
    |  | Self-supervised | $3.57$ | $7.12$ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Zhong *et al.* [[70](#bib.bib70)] | 2017 |  | ConvNet | 单尺度 |  | 4D | 交错
    | 3D Conv |  | Soft argmin |  | 运行时自我改进 |  | 自监督 | $3.57$ | $7.12$ |'
- en: '|  |  |  | with skip conn. |  |  |  | feature concat. | encoder-decoder |  |  |  |  |  |  |  |  |  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 带跳跃连接 |  |  |  | 特征拼接 | 编码器-解码器 |  |  |  |  |  |  |  |  |'
- en: '| Kendall *et al.* [[61](#bib.bib61)] | 2017 |  | ConvNet | Single scale |  |
    4D | Feature concat. | 3D Conv, encoder- |  | Soft argmax |  | $-$ | $-$ |  |
    Supervised | $2.87$ | $6.16$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Kendall *et al.* [[61](#bib.bib61)] | 2017 |  | ConvNet | 单尺度 |  | 4D | 特征拼接
    | 3D Conv，编码器- |  | Soft argmax |  | $-$ | $-$ |  | 监督 | $2.87$ | $6.16$ |'
- en: '|  |  |  | with skip conn. |  |  |  |  | decoder, hierarchical |  |  |  |  |  |  |  |  |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 带跳跃连接 |  |  |  |  | 解码器，分层 |  |  |  |  |  |  |  |  |  |'
- en: '| Pang *et al.* [[62](#bib.bib62)] | 2017 |  | ConvNet | Single scale |  |
    3D | Correlation | 2D ConvNet |  |  |  | Upsampling$+$ residual learning |  |  |
    Supervised | $2.67$ | $3.59$ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 庞 *等人* [[62](#bib.bib62)] | 2017 |  | ConvNet | 单尺度 |  | 3D | 相关性 | 2D ConvNet
    |  |  |  | 上采样$+$残差学习 |  |  | 有监督 | $2.67$ | $3.59$ |'
- en: '| Knobelreiter *et al.* [[71](#bib.bib71)] | 2017 |  | ConvNet | Single scale
    |  | 3D | Correlation | Hybrid CNN-CRF |  |  |  | No post-processing |  | Supervised
    |  |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 克诺贝尔赖特 *等人* [[71](#bib.bib71)] | 2017 |  | ConvNet | 单尺度 |  | 3D | 相关性 |
    混合CNN-CRF |  |  |  | 无后处理 |  | 有监督 |  |  |'
- en: '| Chang *et al.* [[64](#bib.bib64)] | 2018 |  | SPP | Multiscale |  | 4D |
    Feature concat. | 3D Conv, Stacked encoder-decoders |  | Soft argmin |  | Progressive
    refinement |  |  | Supervised | $2.32$ | $4.62$ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 常 *等人* [[64](#bib.bib64)] | 2018 |  | SPP | 多尺度 |  | 4D | 特征拼接 | 3D Conv,
    堆叠的编码器-解码器 |  | 软最小值 |  | 渐进细化 |  |  | 有监督 | $2.32$ | $4.62$ |'
- en: '| Khamis *et al.* [[72](#bib.bib72)] | 2018 |  | ResNet | Single scale |  |
    3D | $L_{2}$ | 3D ConvNet |  | Soft argmin |  | Hierarchical, Upsamling$+$residual
    learning |  | Supervised | $4.83$ | $7.45$ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 卡米斯 *等人* [[72](#bib.bib72)] | 2018 |  | ResNet | 单尺度 |  | 3D | $L_{2}$ |
    3D ConvNet |  | 软最小值 |  | 分层，上采样$+$残差学习 |  | 有监督 | $4.83$ | $7.45$ |'
- en: '| Liang *et al.* [[63](#bib.bib63)] | 2018 |  | ConvNet | Multiscale |  | 3D
    | Correlation | 2D ConvNet |  | Encoder-decoder |  | Iterative upsampling$+$residual
    learning |  | Supervised | $2.67$ | $3.59$ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 梁 *等人* [[63](#bib.bib63)] | 2018 |  | ConvNet | 多尺度 |  | 3D | 相关性 | 2D ConvNet
    |  | 编码器-解码器 |  | 迭代上采样$+$残差学习 |  | 有监督 | $2.67$ | $3.59$ |'
- en: '| Yang *et al.* [[68](#bib.bib68)] | 2018 |  | Shallow | Single scale |  |
    3D | Correlation, Left features, segmentation mask |  | Regression with |  | $-$
    | $-$ |  | Self-supervised | $2.25$ | $4.07$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 杨 *等人* [[68](#bib.bib68)] | 2018 |  | 浅层 | 单尺度 |  | 3D | 相关性，左侧特征，分割掩码 |  |
    回归与 |  | $-$ | $-$ |  | 自监督 | $2.25$ | $4.07$ |'
- en: '|  | ResNet |  |  | Encoder-decoder |  |  |  |  |  |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNet |  |  | 编码器-解码器 |  |  |  |  |  |  |  |'
- en: '| Zhang *et al.* [[73](#bib.bib73)] | 2018 |  | ConvNet | Single scale |  |
    3D | Hand-crafted | NA |  | Soft argmin |  | Upsampling$+$residual learning |  |
    Self-supervised | $-$ | $-$ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 张 *等人* [[73](#bib.bib73)] | 2018 |  | ConvNet | 单尺度 |  | 3D | 手工设计 | NA |  |
    软最小值 |  | 上采样$+$残差学习 |  | 自监督 | $-$ | $-$ |'
- en: '| Jie *et al.* [[74](#bib.bib74)] | 2018 |  | Constant highway net | Single
    scale |  | 3D | FCN | $-$ |  | RNN-based LRCR |  | $-$ | $-$ |  | Supervised |
    $3.03$ | $5.42$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 季 *等人* [[74](#bib.bib74)] | 2018 |  | 恒定高速公路网络 | 单尺度 |  | 3D | FCN | $-$
    |  | 基于RNN的LRCR |  | $-$ | $-$ |  | 有监督 | $3.03$ | $5.42$ |'
- en: '| Ilg *et al.* [[75](#bib.bib75)] | 2018 |  | ConvNet | Single scale |  | 3D
    | Correlation | Encoder-decoder, joint disparity and occlusion |  | Cascade of
    encoder-decoders, residual learning |  | Supervised | $2.19$ | $-$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 伊尔格 *等人* [[75](#bib.bib75)] | 2018 |  | ConvNet | 单尺度 |  | 3D | 相关性 | 编码器-解码器，联合视差和遮挡
    |  | 编码器-解码器级联，残差学习 |  | 有监督 | $2.19$ | $-$ |'
- en: '| Song *et al.* [[76](#bib.bib76)] | 2018 |  | SHallow ConvNet | Single scale
    |  | 3D | Correlation | Edge-guided, Context Pyramid, |  | Residual pyramid |  |
    $-$ | $-$ |  | Supervised | $2.59$ | $4.18$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 宋 *等人* [[76](#bib.bib76)] | 2018 |  | 浅层ConvNet | 单尺度 |  | 3D | 相关性 | 边缘引导,
    上下文金字塔 |  | 残差金字塔 |  | $-$ | $-$ |  | 有监督 | $2.59$ | $4.18$ |'
- en: '|  |  |  |  |  |  |  | Encoder |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  | 编码器 |  |  |  |  |  |  |  |  |  |'
- en: '| Yu *et al.* [[77](#bib.bib77)] | 2018 |  | ResNet | Single scale |  | 3D
    | Feature concatenation | 3D Conv $+$ SGM |  | Soft argmin |  | $-$ | $-$ |  |
    Supervised | $2.79$ | $5.46$ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 于 *等人* [[77](#bib.bib77)] | 2018 |  | ResNet | 单尺度 |  | 3D | 特征拼接 | 3D Conv
    $+$ SGM |  | 软最小值 |  | $-$ | $-$ |  | 有监督 | $2.79$ | $5.46$ |'
- en: '|  |  |  |  |  |  |  | Encoder-decoder |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  | 编码器-解码器 |  |  |  |  |  |  |  |  |  |  |'
- en: '| Tulyakov *et al.* [[78](#bib.bib78)] | 2018 |  | $-$ | Single scale |  |
    4D | Compressed | 3D Conv |  | Multimodal - |  | $-$ | $-$ |  | Supervised | $2.58$
    | $4.05$ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 图利亚科夫 *等人* [[78](#bib.bib78)] | 2018 |  | $-$ | 单尺度 |  | 4D | 压缩 | 3D Conv
    |  | 多模态 - |  | $-$ | $-$ |  | 有监督 | $2.58$ | $4.05$ |'
- en: '|  |  |  |  |  |  |  | matching features |  |  | Sub-pixel MAP |  |  |  |  |  |  |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  | 匹配特征 |  |  | 子像素MAP |  |  |  |  |  |  |  |'
- en: '| EMCUA *et al.* [[65](#bib.bib65)] | 2019 |  | SPP | Multiscale |  | 4D |
    Feature concat. | 3D Conv, MCUA scheme |  | Arg softmin |  | $-$ | $-$ |  | Supervised
    | $2.09$ | $4.27$ |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| EMCUA *等人* [[65](#bib.bib65)] | 2019 |  | SPP | 多尺度 |  | 4D | 特征拼接 | 3D Conv,
    MCUA方案 |  | 软最小值 |  | $-$ | $-$ |  | 有监督 | $2.09$ | $4.27$ |'
- en: '| Yang *et al.* [[32](#bib.bib32)] | 2019 |  | SPP | Multiscale |  | Pyramidal
    4D | Feature difference | Conv3D blocks $+$ Volume Pyramid Pooling |  | Arg softmax
    |  | Spatial & depth res. by cost volume upsampling |  | Supervised | $2.14$ |
    $3.85$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Yang *等* [[32](#bib.bib32)] | 2019 |  | SPP | 多尺度 |  | 金字塔4D | 特征差异 | Conv3D块
    $+$ 体积金字塔池化 |  | Arg softmax |  | 通过成本体积上采样的空间和深度分辨率 |  | 监督学习 | $2.14$ | $3.85$
    |'
- en: '| Wu *et al.* [[79](#bib.bib79)] | 2019 |  | ResNet50, SPP | Multiscale |  |
    Pyramidal 4D | Feature concat. | Encoder-decoder |  | 3D Conv, soft argmin |  |
    $-$ | $-$ |  | Supervised | $2.11$ | $3.89$ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Wu *等* [[79](#bib.bib79)] | 2019 |  | ResNet50, SPP | 多尺度 |  | 金字塔4D | 特征连接
    | 编码器-解码器 |  | 3D卷积, soft argmin |  | $-$ | $-$ |  | 监督学习 | $2.11$ | $3.89$ |'
- en: '|  |  |  |  |  |  |  |  | $+$ Feature fusion |  |  |  |  |  |  | Disparity
    & boundary loss |  |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | $+$ 特征融合 |  |  |  |  |  |  | 视差与边界损失 |  |  |'
- en: '| Yin *et al.* [[80](#bib.bib80)] | 2019 |  | DLA net | Multiscale |  | 3D
    | Correlation | Density decoder |  | Outputs discrete |  | $-$ | $-$ |  | Supervised
    | 2.02 | $3.63$ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Yin *等* [[80](#bib.bib80)] | 2019 |  | DLA网络 | 多尺度 |  | 3D | 相关 | 密度解码器 |  |
    输出离散值 |  | $-$ | $-$ |  | 监督学习 | 2.02 | $3.63$ |'
- en: '|  |  |  |  |  |  |  |  |  |  | matching distribution |  |  |  |  |  |  |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |  |  | 匹配分布 |  |  |  |  |  |  |  |'
- en: '| Chabra *et al.* [[81](#bib.bib81)] | 2019 |  | ConvNet $+$ | Multiscale |  |
    3D | $L_{1}$ | Dilated 3D ConvNet |  | Soft argmax |  | Upsampling$+$residual
    learning |  | Supervised | $2.26$ | $4.95$ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Chabra *等* [[81](#bib.bib81)] | 2019 |  | 卷积网络 $+$ | 多尺度 |  | 3D | $L_{1}$
    | 膨胀3D卷积网络 |  | Soft argmax |  | 上采样$+$残差学习 |  | 监督学习 | $2.26$ | $4.95$ |'
- en: '|  |  |  | Vortex pooling [[82](#bib.bib82)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 漩涡池化 [[82](#bib.bib82)] |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Duggal *et al.* [[83](#bib.bib83)] | 2019 |  | ResNet, SPP | Multiscale |  |
    3D, sparse | Correlation, Adaptive | Encoder-decoder |  | Soft argmax |  | Encoder
    | $-$ |  | Supervised | $2.35$ | 3.43 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Duggal *等* [[83](#bib.bib83)] | 2019 |  | ResNet, SPP | 多尺度 |  | 3D, 稀疏 |
    相关, 自适应 | 编码器-解码器 |  | Soft argmax |  | 编码器 | $-$ |  | 监督学习 | $2.35$ | 3.43 |'
- en: '|  |  |  |  |  |  |  | pruning with PatchMatch |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  | PatchMatch修剪 |  |  |  |  |  |  |  |  |  |  |'
- en: '| Tonioni *et al.* [[84](#bib.bib84)] | 2019 |  | ConvNet | Multiscale |  |
    3D | Correlation |  |  | Encoder |  | Recrusively upsampling $+$ residual learning
    |  | Online self-adaptive | $4.66$ | $-$ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Tonioni *等* [[84](#bib.bib84)] | 2019 |  | 卷积网络 | 多尺度 |  | 3D | 相关 |  |  |
    编码器 |  | 递归上采样 $+$ 残差学习 |  | 在线自适应 | $4.66$ | $-$ |'
- en: '| Yang *et al.* [[32](#bib.bib32)] | 2019 |  | ConvNet, SPP | Multiscale |  |
    Pyramid, 4D | Concatenation | Decoder, Residual blocs, |  | Conv3D block |  |
    $-$ | $-$ |  | Supervised | $2.14$ | $3.85$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Yang *等* [[32](#bib.bib32)] | 2019 |  | 卷积网络, SPP | 多尺度 |  | 金字塔, 4D | 连接
    | 解码器, 残差块 |  | Conv3D块 |  | $-$ | $-$ |  | 监督学习 | $2.14$ | $3.85$ |'
- en: '|  |  |  |  |  |  |  |  | VPP |  |  |  |  |  |  |  |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | VPP |  |  |  |  |  |  |  |  |'
- en: '| Zhang *et al.* [[85](#bib.bib85)] | 2019 |  | Stacked hourglass | Single
    scale |  | 4D | Concatenation | Semi-global aggregation layers, |  | Soft argmax
    |  | $-$ | $-$ |  | Supervised | $2.03$ | $3.91$ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Zhang *等* [[85](#bib.bib85)] | 2019 |  | 堆叠式沙漏 | 单尺度 |  | 4D | 连接 | 半全局聚合层
    |  | Soft argmax |  | $-$ | $-$ |  | 监督学习 | $2.03$ | $3.91$ |'
- en: '|  |  |  |  |  |  |  |  | Local-guided aggregation layers |  |  |  |  |  |  |  |  |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | 局部引导聚合层 |  |  |  |  |  |  |  |  |'
- en: '| Guo *et al.* [[86](#bib.bib86)] | 2019 |  | SPP | Multiscale |  | Hybrid
    3D-4D | Group-wise correlation | Stacked hourglass nets |  | Soft argmin |  |
    $-$ | $-$ |  | Supervised | $2.11$ | $3.93$ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Guo *等* [[86](#bib.bib86)] | 2019 |  | SPP | 多尺度 |  | 混合3D-4D | 分组相关 | 堆叠式沙漏网络
    |  | Soft argmin |  | $-$ | $-$ |  | 监督学习 | $2.11$ | $3.93$ |'
- en: '| Chen *et al.* [[87](#bib.bib87)] | 2019 |  |  |  |  |  |  |  |  | Single-modal
    weighted avg |  |  |  |  | Supervised | $2.14$ | $4.33$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Chen *等* [[87](#bib.bib87)] | 2019 |  |  |  |  |  |  |  |  | 单模态加权平均 |  |  |  |  |
    监督学习 | $2.14$ | $4.33$ |'
- en: '| Wang *et al.* [[88](#bib.bib88)] | 2019 |  | ConvNet | Multiresolution maps
    |  | 3D | $L_{1}$ | Progressive refinement |  | Soft argmin |  | Usampling, |
    Spatial propagation |  | Supervised | $-$ | $-$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Wang *等* [[88](#bib.bib88)] | 2019 |  | 卷积网络 | 多分辨率图 |  | 3D | $L_{1}$ |
    逐步细化 |  | Soft argmin |  | 上采样, | 空间传播 |  | 监督学习 | $-$ | $-$ |'
- en: '|  |  |  |  |  |  |  |  | (3D Conv) |  |  |  | residual learning | network
    |  |  |  |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  | (3D卷积) |  |  |  | 残差学习 | 网络 |  |  |  |  |'
- en: 5.2 Cost volume construction
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 成本体积构建
- en: Once the features have been computed, the next step is to compute the matching
    scores, which will be fed, in the form of a cost volume, to a top network for
    regularization and disparity estimation. The cost volume can be three dimensional
    (3D) where the third dimension is the disparity level (Section [5.2.1](#S5.SS2.SSS1
    "5.2.1 3D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")),
    four dimensional (4D) where the third dimension is the feature dimension and the
    fourth one is the disparity level (Section [5.2.2](#S5.SS2.SSS2 "5.2.2 4D cost
    volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation")), or hybrid to
    benefit from the properties of the 3D and 4D cost volumes (Section [5.2.3](#S5.SS2.SSS3
    "5.2.3 Hybrid 3D-4D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")). In general, the cost volume is constructed at a lower resolution,
    *e.g.,* at $\nicefrac{{1}}{{8}}$-th, than the input [[72](#bib.bib72), [73](#bib.bib73)].
    It is then either subsequently upscaled and refined, or used as is to estimate
    a low resolution disparity map, which is then upscaled and refined using a refinement
    module.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦特征被计算出来，下一步就是计算匹配分数，这些分数将以代价体积的形式输入到顶层网络中进行正则化和视差估计。代价体积可以是三维的 (3D)，其中第三维度是视差水平（见[5.2.1](#S5.SS2.SSS1
    "5.2.1 3D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")），四维的
    (4D)，其中第三维度是特征维度，第四维度是视差水平（见[5.2.2](#S5.SS2.SSS2 "5.2.2 4D cost volumes ‣ 5.2
    Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")），或混合型，以利用3D和4D 代价体积的特性（见[5.2.3](#S5.SS2.SSS3
    "5.2.3 Hybrid 3D-4D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")）。通常，代价体积在较低分辨率下构建，例如，$\nicefrac{{1}}{{8}}$-分辨率，而不是输入[[72](#bib.bib72)、[73](#bib.bib73)]。然后，它要么被后续地上采样和精化，要么按原样用于估计低分辨率视差图，再通过精化模块进行上采样和精化。
- en: 5.2.1 3D cost volumes
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 3D 代价体积
- en: 5.2.1.1 Construction
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.2.1.1 构建
- en: A 3D cost volume can be simply built by taking the $L_{1}$, $L_{2}$, or correlation
    distance between the features of the left image and those of the right image that
    are within a pre-defined disparity range, see [[22](#bib.bib22), [73](#bib.bib73),
    [74](#bib.bib74), [72](#bib.bib72), [80](#bib.bib80), [81](#bib.bib81), [83](#bib.bib83)],
    and the FlowNetCorr of [[51](#bib.bib51)]. The advantage of correlation-based
    dissimilarities is that they can be implemented using a convolutional layer that
    does not require training (its filters are the features computed by the second
    branch of the network). Flow estimation networks such as FlowNetCorr [[51](#bib.bib51)]
    use 2D correlations. Disparity estimation networks, such as [[22](#bib.bib22),
    [68](#bib.bib68)], iResNet [[63](#bib.bib63)], DispNet3 [[75](#bib.bib75)], EdgeStereo [[76](#bib.bib76)],
    HD³ [[80](#bib.bib80)], and [[84](#bib.bib84), [83](#bib.bib83)], use 1D correlations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 代价体积可以通过计算左图像和右图像在预定义视差范围内的特征之间的$L_{1}$、$L_{2}$或相关距离来简单地构建，参见[[22](#bib.bib22)、[73](#bib.bib73)、[74](#bib.bib74)、[72](#bib.bib72)、[80](#bib.bib80)、[81](#bib.bib81)、[83](#bib.bib83)]，以及[[51](#bib.bib51)]的FlowNetCorr。基于相关性的异质性优势在于它们可以通过一个不需要训练的卷积层来实现（其滤波器是网络第二分支计算的特征）。FlowNetCorr[[51](#bib.bib51)]等流估计网络使用2D
    相关性。视差估计网络，如[[22](#bib.bib22)、[68](#bib.bib68)]、iResNet[[63](#bib.bib63)]、DispNet3[[75](#bib.bib75)]、EdgeStereo[[76](#bib.bib76)]、HD³[[80](#bib.bib80)]和[[84](#bib.bib84)、[83](#bib.bib83)]，使用1D
    相关性。
- en: 5.2.1.2 Regularization of 3D cost volumes
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.2.1.2 3D 代价体积的正则化
- en: Once a cost volume is computed, an initial disparity map can be estimated using
    the argmin, the softargmin, or the subpixel MAP approximation over the depth dimension
    of the cost volume, see for example [[73](#bib.bib73)] and Fig. [5](#S5.F5 "Figure
    5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost
    volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(a). This is equivalent to dropping
    the regularization term of Eqn. ([1](#S4.E1 "In 4 Depth by stereo matching ‣ A
    Survey on Deep Learning Techniques for Stereo-based Depth Estimation")). In general,
    however, the raw cost volume is noise-contaminated (*e.g.,* due to the existence
    of non-Lambertian surfaces, object occlusions, and repetitive patterns). The goal
    of the regularization module is to leverage context along the spatial and/or disparity
    dimensions to refine the cost volume before estimating the initial disparity map.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出成本体积，可以使用argmin、softargmin或子像素MAP近似来估计初始视差图，详见例如[[73](#bib.bib73)]和图[5](#S5.F5
    "Figure 5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes
    ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")-(a)。这等同于去掉方程([1](#S4.E1
    "In 4 Depth by stereo matching ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation"))中的正则化项。然而，通常情况下，原始成本体积会被噪声污染（*例如，*由于存在非兰伯特表面、物体遮挡和重复模式）。正则化模块的目标是利用空间和/或视差维度的上下文来优化成本体积，然后再估计初始视差图。
- en: (1) Regularization using traditional methods. Early papers use traditional techniques,
    *e.g.,* Markov Random Fields (MRF), Conditional Random Fields (CRF), and Semi-Global
    Matching (SGM), to regularize the cost volume by explicitly incorporating spatial
    constraints, *e.g.,* smoothness, of the depth maps. Recent papers showed that
    deep learning networks can be used to fine-tune the parameters of these methods.
    For example, Knöbelreiter *et al.* [[71](#bib.bib71)] proposed a hybrid CNN-CRF.
    The CNN computes the matching term of Eqn. ([1](#S4.E1 "In 4 Depth by stereo matching
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")), which
    becomes the unary term of a CRF module. The pairwise term of the CRF is parameterized
    by edge weights computed using another CNN. The end-to-end trained CNN-CRF pipeline
    could achieve a competitive performance using much fewer parameters (thus a better
    utilization of the training data) than the earlier methods.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 使用传统方法的正则化。早期论文使用传统技术，*例如，*马尔可夫随机场（MRF）、条件随机场（CRF）和半全局匹配（SGM），通过显式地纳入深度图的空间约束（*例如，*平滑性）来正则化成本体积。最近的论文表明，深度学习网络可以用来微调这些方法的参数。例如，Knöbelreiter
    *et al.* [[71](#bib.bib71)] 提出了一个混合CNN-CRF。CNN计算方程([1](#S4.E1 "In 4 Depth by stereo
    matching ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation"))的匹配项，该项成为CRF模块的单项。CRF的对偶项通过另一个CNN计算的边缘权重进行参数化。经过端到端训练的CNN-CRF管道可以用更少的参数（从而更好地利用训练数据）实现具有竞争力的性能，优于早期方法。
- en: Zheng *et al.* [[89](#bib.bib89)] provide a way to model CRFs as recurrent neural
    networks (RNN) for segmentation tasks so that the entire pipeline can be trained
    end-to-end. Unlike segmentation, in depth estimation, the number of depth samples,
    whose counterparts are the semantic labels in segmentation tasks, is expected
    to vary for different scenarios. As such, Xue *et al.* [[90](#bib.bib90)] re-designed
    the RNN-formed CRF module so that the model parameters are independent of the
    number of depth samples. Paschalidou *et al.* [[91](#bib.bib91)] formulate the
    inference in a MRF as a differentiable function, hence allowing end-to-end training
    using back propagation. Note that Zheng *et al.* [[89](#bib.bib89)] and Paschalidou
    *et al.* [[91](#bib.bib91)] focus on multi-view stereo (Section [6](#S6 "6 Learning
    multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")). Their approaches, however, are generic and can be used to regularize
    3D cost volumes obtained using pairwise stereo networks.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 郑*等* [[89](#bib.bib89)] 提供了一种将条件随机场（CRFs）建模为递归神经网络（RNN）用于分割任务的方法，从而可以对整个管道进行端到端训练。与分割不同，在深度估计中，深度样本的数量（其对应物是分割任务中的语义标签）预计会因不同场景而异。因此，薛*等* [[90](#bib.bib90)]
    重新设计了RNN形成的CRF模块，使得模型参数与深度样本的数量无关。Paschalidou*等* [[91](#bib.bib91)] 将MRF中的推断公式化为可微分函数，从而允许使用反向传播进行端到端训练。需要注意的是，郑*等* [[89](#bib.bib89)]
    和Paschalidou*等* [[91](#bib.bib91)] 专注于多视角立体（第[6](#S6 "6 Learning multiview stereo
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation)节）。然而，他们的方法是通用的，可以用来对通过成对立体网络获得的3D代价体进行规范化。
- en: '![Refer to caption](img/0bbcfef77d5c62c3068a656a183106f2.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0bbcfef77d5c62c3068a656a183106f2.png)'
- en: 'Figure 5: Cost volume regularization schemes [[92](#bib.bib92)]: (a) does not
    consider context, (b) captures context along the spatial dimensions using 2D convolutions,
    (c) captures context along the spatial and disparity dimensions by recurrent regularization
    using 2D convolutions, and (d) captures context in all dimensions by using 3D
    convolutions.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '图5: 代价体正则化方案 [[92](#bib.bib92)]: (a) 不考虑上下文，(b) 使用2D卷积捕获空间维度上的上下文，(c) 通过2D卷积的递归正则化捕获空间和视差维度上的上下文，以及(d)
    使用3D卷积捕获所有维度上的上下文。'
- en: (2) Regularization using 2D convolutions (2DConvNet), Figs. [5](#S5.F5 "Figure
    5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost
    volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(b) and (c). Another approach is
    to process the 3D cost volume using a series of 2D convolutional layers producing
    another 3D cost volume [[22](#bib.bib22), [51](#bib.bib51), [62](#bib.bib62),
    [63](#bib.bib63)]. 2D convolutions are computationally efficient. However, they
    only capture and aggregate context along the spatial dimensions, see Fig. [5](#S5.F5
    "Figure 5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes
    ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")-(b), and ignore context
    along the disparity dimension. Yao *et al.* [[92](#bib.bib92)] sequentially regularize
    the 2D cost maps along the depth direction via a Gated Recurrent Unit (GRU), see
    Fig. [5](#S5.F5 "Figure 5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1
    3D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(c).
    This reduces drastically the memory consumption, *e.g.,* from $15.4$GB in [[93](#bib.bib93)]
    to around $5$GB, making high-resolution reconstruction feasible, while capturing
    context along both the spatial and the disparity dimensions.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 使用2D卷积（2DConvNet）的正则化，见图[5](#S5.F5 "Figure 5 ‣ 5.2.1.2 Regularization of
    3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(b)和(c)。另一种方法是使用一系列2D卷积层处理3D代价体，产生另一个3D代价体 [[22](#bib.bib22), [51](#bib.bib51),
    [62](#bib.bib62), [63](#bib.bib63)]。2D卷积计算效率高，但仅捕获和汇聚空间维度上的上下文，见图[5](#S5.F5 "Figure
    5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost
    volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(b)，忽略了视差维度上的上下文。姚*等* [[92](#bib.bib92)]
    通过Gated Recurrent Unit (GRU) 顺序地对2D代价图进行深度方向的正则化，见图[5](#S5.F5 "Figure 5 ‣ 5.2.1.2
    Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost volume construction
    ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(c)。这大幅降低了内存消耗，例如，从 [[93](#bib.bib93)] 中的 $15.4$GB 降至约 $5$GB，使得高分辨率重建成为可能，同时捕获了空间和视差维度上的上下文。
- en: (3) Regularization using 3D convolutions (3DConvNet), Fig. [5](#S5.F5 "Figure
    5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost
    volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")-(d). Khamis *et al.* [[72](#bib.bib72)]
    use the $L_{2}$ distance to compute an initial 3D cost volume and 3D convolutions
    to regularize it across both the spatial and disparity dimensions, see Fig. [5](#S5.F5
    "Figure 5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes
    ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")-(d). Due to its memory
    requirements, the approach first estimates a low-resolution disparity map, which
    is then progressively improved using residual learning. Zhang *et al.* [[73](#bib.bib73)]
    follow the same approach but the refinement block starts with separate convolution
    layers running on the upsampled disparity and input image respectively, and merge
    the features later to produce the residual. Chabra *et al.* [[81](#bib.bib81)]
    observe that the cost volume regularization step is the one that uses most of
    the computational resources. They then propose a regularization module that uses
    3D dilated convolutions in the width, height, and disparity dimesions, to reduce
    the computation time while capturing a wider context.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 使用3D卷积进行正则化（3DConvNet），见图[5](#S5.F5 "Figure 5 ‣ 5.2.1.2 Regularization of
    3D cost volumes ‣ 5.2.1 3D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(d)。Khamis *等人* [[72](#bib.bib72)] 使用$L_{2}$距离来计算初始的3D成本体积，并通过3D卷积对空间和视差维度进行正则化，见图[5](#S5.F5
    "Figure 5 ‣ 5.2.1.2 Regularization of 3D cost volumes ‣ 5.2.1 3D cost volumes
    ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from stereo ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")-(d)。由于其内存需求，该方法首先估计低分辨率的视差图，然后通过残差学习逐步改进。Zhang
    *等人* [[73](#bib.bib73)] 采用相同的方法，但细化块从分别在上采样的视差和输入图像上运行的卷积层开始，然后合并特征以产生残差。Chabra
    *等人* [[81](#bib.bib81)] 观察到成本体积正则化步骤是最消耗计算资源的步骤。他们提出了一种使用3D膨胀卷积在宽度、高度和视差维度上进行正则化的模块，以减少计算时间，同时捕捉更广泛的上下文。
- en: 5.2.2 4D cost volumes
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 4D成本体积
- en: 5.2.2.1 Construction
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.2.2.1 构建
- en: 4D cost volumes to preserve the dimension of the features [[70](#bib.bib70),
    [61](#bib.bib61), [64](#bib.bib64), [65](#bib.bib65), [32](#bib.bib32), [79](#bib.bib79)].
    The rational behind 4D cost volumes is to let the top network learn the appropriate
    similarity measure for comparing the features instead of using hand-crafted ones
    as in Section [5.2.1](#S5.SS2.SSS1 "5.2.1 3D cost volumes ‣ 5.2 Cost volume construction
    ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation").
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 4D成本体积用于保持特征的维度 [[70](#bib.bib70), [61](#bib.bib61), [64](#bib.bib64), [65](#bib.bib65),
    [32](#bib.bib32), [79](#bib.bib79)]。4D成本体积的原理是让顶层网络学习适当的相似性度量来比较特征，而不是像在第[5.2.1](#S5.SS2.SSS1
    "5.2.1 3D cost volumes ‣ 5.2 Cost volume construction ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")节中使用的手工设计的度量。
- en: 4D cost volumes can be constructed by feature differences across a pre-defined
    disparity range [[32](#bib.bib32)], which results in cost volume of size $0pt\times
    0pt\times 2n_{d}\times c$, or by concatenating the features computed by the different
    branches of the network [[61](#bib.bib61), [70](#bib.bib70), [64](#bib.bib64),
    [65](#bib.bib65), [79](#bib.bib79)]. Using this method, Kendall *et al.* [[61](#bib.bib61)]
    build a 4D volume of size $0pt\times 0pt\times(n_{d}+1)\times c$ ($c$ here is
    the dimension of the features). Zhong *et al.* [[70](#bib.bib70)] follow the same
    approach but concatenate the features in an interleaved manner. That is, if $\textbf{f}_{L}$
    is the feature map of the left image and $\textbf{f}_{R}$ the feature map of the
    right image, then the final feature volume is assembled in such a way that its
    $2i-$th slice holds the left feature map while the $(2i+1)-$th slice holds the
    right feature map but at disparity $d=i$. This results in a 4D cost volume that
    is twice larger than the cost volume of Kendall *et al.* [[61](#bib.bib61)]. To
    capture multi-scale context in the cost volume, Chang and Chen [[64](#bib.bib64)]
    generate for each input image a pyramid of features, upsamples them to the same
    dimension, and then builds a single 4D cost volume by concatenation. Wu *et al.* [[79](#bib.bib79)]
    build from the multiscale features (four scales) multiscale 4D cost volumes.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 4D 成本体积可以通过预定义视差范围内的特征差异构建 [[32](#bib.bib32)]，这会生成大小为 $0pt\times 0pt\times 2n_{d}\times
    c$ 的成本体积，或者通过将网络不同分支计算出的特征进行连接 [[61](#bib.bib61), [70](#bib.bib70), [64](#bib.bib64),
    [65](#bib.bib65), [79](#bib.bib79)]。使用这种方法，**Kendall et al.** [[61](#bib.bib61)]
    构建了一个大小为 $0pt\times 0pt\times(n_{d}+1)\times c$ 的 4D 体积（$c$ 是特征的维度）。**Zhong et
    al.** [[70](#bib.bib70)] 采用相同的方法，但以交错的方式连接特征。即，如果 $\textbf{f}_{L}$ 是左图像的特征图，而
    $\textbf{f}_{R}$ 是右图像的特征图，那么最终的特征体积是这样组装的：其 $2i-$th 切片保存左特征图，而 $(2i+1)-$th 切片保存右特征图，但视差为
    $d=i$。这会生成一个比 **Kendall et al.** [[61](#bib.bib61)] 的成本体积大两倍的 4D 成本体积。为了在成本体积中捕捉多尺度上下文，**Chang
    和 Chen** [[64](#bib.bib64)] 为每个输入图像生成特征金字塔，将它们上采样到相同维度，然后通过连接构建一个单一的 4D 成本体积。**Wu
    et al.** [[79](#bib.bib79)] 从多尺度特征（四个尺度）构建了多尺度 4D 成本体积。
- en: 4D cost volumes carry richer information compared to 3D cost volumes. Note,
    however, that volumes obtained by concatenation contain no information about the
    feature similarities, so more parameters are required in the subsequent modules
    to learn the similarity function.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 与 3D 成本体积相比，4D 成本体积包含更丰富的信息。然而，请注意，通过连接获得的体积不包含关于特征相似性的信息，因此后续模块中需要更多的参数来学习相似性函数。
- en: 5.2.2.2 Regularization of 4D cost volumes
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.2.2.2 4D 成本体积的正则化
- en: 4D cost volumes are regularized with 3D convolutions, which exploit the correlation
    in height, width and disparity dimensions, to produce a 3D cost volume. Kendall
    *et al.* [[61](#bib.bib61)] use a U-net encoder-decoder with 3D convolutions and
    skip connections. Zhong *et al.* [[70](#bib.bib70)] use a similar approach but
    add residual connections from the contracting to the expanding parts of the regularization
    network. To take into account a large context without a significant additional
    computational burden, Kendall *et al.* [[61](#bib.bib61)] regularize the cost
    volume hierarchically, with four levels of subsampling, allowing to explicitly
    leverage context with a wide field of view. Muliscale 4D cost volumes [[79](#bib.bib79)]
    are aggregated into a single 3D cost volume using a 3D multi-cost aggregation
    module, which operates in a pairwise manner starting with the smallest volume.
    Each volume is processed with an encoder-decoder, upsampled to the next resolution
    in the pyramid, and then fused using a 3D feature fusion module.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 4D 成本体积通过 3D 卷积进行正则化，这些卷积利用高度、宽度和视差维度之间的相关性，生成一个 3D 成本体积。**Kendall et al.**
    [[61](#bib.bib61)] 使用了一个带有 3D 卷积和跳跃连接的 U-net 编码器-解码器。**Zhong et al.** [[70](#bib.bib70)]
    采用了类似的方法，但在正则化网络的收缩部分和扩展部分之间增加了残差连接。为了在不显著增加计算负担的情况下考虑大范围的上下文，**Kendall et al.**
    [[61](#bib.bib61)] 通过四级子采样对成本体积进行分层正则化，从而可以明确地利用广阔视野的上下文。多尺度 4D 成本体积 [[79](#bib.bib79)]
    通过 3D 多成本聚合模块聚合成一个单一的 3D 成本体积，该模块以配对的方式操作，从最小的体积开始。每个体积通过编码器-解码器处理，向上采样到金字塔中的下一个分辨率，然后使用
    3D 特征融合模块进行融合。
- en: 'Also, semi-global matching (SGM) techniques have been used to regularize the
    4D cost volume where their parameters are estimated using convolutional networks.
    In particular, Yu *et al.* [[77](#bib.bib77)] process the initial 4D cost volume
    with an encoder-decoder composed of 3D convolutions and upconvolutions, and produces
    another 3D cost volume. The subsequent aggregation step is performed using an
    end-to-end two-stream network: the first stream generates three cost aggregation
    proposals $C_{i}$, one along each of the tree dimensions, *i.e.,* the height,
    width, and disparity. The second stream is a guidance stream used to select the
    best proposals. It uses 2D convolutions to produce three guidance (confidence)
    maps $W_{i}$. The final 3D cost volume is produced as a weighted sum of the three
    proposals, *i.e.,* $\max_{i}(C_{i}*W_{i})$.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，半全局匹配 (SGM) 技术也被用来正则化 4D 成本体，其参数使用卷积网络进行估计。特别地，Yu *等人* [[77](#bib.bib77)]
    使用由 3D 卷积和上卷积组成的编码器-解码器处理初始的 4D 成本体，并产生另一个 3D 成本体。随后的聚合步骤是通过端到端的双流网络完成的：第一流生成三个代价聚合提议
    $C_{i}$，每个提议沿着三个维度 *即* 高度、宽度和视差。第二流是一个引导流，用于选择最佳提议。它使用 2D 卷积生成三个引导（置信度）图 $W_{i}$。最终的
    3D 成本体作为三个提议的加权和生成，*即* $\max_{i}(C_{i}*W_{i})$。
- en: 3D convolutions are expensive in terms of memory requirements and computation
    time. As such, subsequent works that followed the seminal work of Kendall *et
    al.* [[61](#bib.bib61)] focused on (1) reducing the number of 3D convolutional
    layers [[85](#bib.bib85)], (2) progressively refining the cost volume and the
    disparity map [[64](#bib.bib64), [88](#bib.bib88)], and (3) compressing the 4D
    cost volume [[78](#bib.bib78)]. Below, we discuss these approaches.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 卷积在内存需求和计算时间方面成本高昂。因此，紧随 Kendall *等人* [[61](#bib.bib61)] 开创性工作的后续研究集中于 (1)
    减少 3D 卷积层的数量 [[85](#bib.bib85)]，(2) 渐进地细化成本体和视差图 [[64](#bib.bib64), [88](#bib.bib88)]，以及
    (3) 压缩 4D 成本体 [[78](#bib.bib78)]。以下，我们将深入讨论这些方法。
- en: (1) Reducing the number of 3D convolutional layers. Zhang *et al.* [[85](#bib.bib85)]
    introduced GANet, which replaces a large number of the 3D convolutional layers
    in the regularization block with (1) two 3D convolutional layers, (2) a semi-global
    aggregation layer (SGA), and (3) a local guided aggregation layer (LGA). SGA is
    a differentiable approximation of the semi-global matching (SGM). Unlike SGM,
    in SGA the user-defined parameters are learnable. Moreover, they are added as
    penalty coefficients/weights of the matching cost terms. Thus, they are adaptive
    and more flexible at different locations for different situations. The LGA layer,
    on the other hand, is appended at the end and aims to refine the thin structures
    and object edges. The SGA and LGA layers, which are used to replace the costly
    3D convolutions, capture local and whole-image cost dependencies. They significantly
    improve the accuracy of the disparity estimation in challenging regions such as
    occlusions, large textureless/reflective regions, and thin structures.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 减少 3D 卷积层的数量。张 *等人* [[85](#bib.bib85)] 提出了 GANet，该方法用 (1) 两个 3D 卷积层、(2)
    一个半全局聚合层 (SGA) 和 (3) 一个局部引导聚合层 (LGA) 替代了正则化块中的大量 3D 卷积层。SGA 是半全局匹配 (SGM) 的可微近似。与
    SGM 不同，SGA 中的用户定义参数是可学习的。此外，它们被作为匹配代价项的惩罚系数/权重添加。因此，它们在不同位置对不同情况具有自适应性和更大的灵活性。另一方面，LGA
    层则附加在末尾，旨在细化细结构和物体边缘。SGA 和 LGA 层用于替代高成本的 3D 卷积，捕捉局部和全图的代价依赖性。它们显著提高了在遮挡、大面积无纹理/反射区域和细结构等挑战区域的视差估计准确性。
- en: (2) Progressive approaches. Some techniques avoid directly regularizing high
    resolution 4D cost volumes using the expensive 3D convolutions. Instead, they
    operate in a progressive manner. For instance, Chang and Chen [[64](#bib.bib64)]
    introduced PSM-Net, which first estimates a low resolution 4D cost volume, and
    then regularizes it using stacked hourglass 3D encoder-decoder blocks. Each block
    returns a 3D cost volume, which is then upsampled and used to regress a high resolution
    disparity map using additional 3D convolutional layers followed by a softmax operator.
    As such, the stacked hourglass blocks can be seen as refinement modules.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 渐进方法。一些技术避免直接使用昂贵的 3D 卷积对高分辨率的 4D 成本体进行正则化。相反，它们以渐进的方式进行操作。例如，Chang 和 Chen
    [[64](#bib.bib64)] 提出了 PSM-Net，该方法首先估计一个低分辨率的 4D 成本体，然后使用堆叠的 hourglass 3D 编码器-解码器块对其进行正则化。每个块返回一个
    3D 成本体，然后进行上采样，并使用额外的 3D 卷积层和 softmax 操作器回归高分辨率视差图。因此，堆叠的 hourglass 块可以视为细化模块。
- en: Wang *et al.* [[88](#bib.bib88)] use a three-stage disparity estimation network,
    called AnyNet, which builds cost volumes in a coarse-to-fine manner. The first
    stage takes as input low resolution feature maps, builds a low resolution 4D cost
    volume and then uses 3D convolutions to estimate a low resolution disparity map
    by searching on a small disparity range. The prediction in the previous level
    is then upsampled and used to warp the input feature at the higher scale, with
    the same disparity estimation network used to estimate disparity residuals. The
    advantage is two-fold; first, at higher resolutions, the network only learns to
    predict residuals, which reduces the computation cost. Second, the approach is
    progressive and one can select to return the intermediate disparities, trading
    accuracy for speed.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Wang *等人* [[88](#bib.bib88)] 使用一个三阶段视差估计网络，称为AnyNet，该网络以粗到细的方式构建成本体积。第一阶段以低分辨率特征图作为输入，构建一个低分辨率的4D成本体积，然后使用3D卷积在小视差范围内估计低分辨率的视差图。上一层的预测结果随后被上采样并用于在更高尺度下扭曲输入特征，使用相同的视差估计网络来估计视差残差。这个方法有两个优点；首先，在更高分辨率下，网络只需学习预测残差，从而降低计算成本。其次，该方法是渐进的，可以选择返回中间视差，以在精度和速度之间进行权衡。
- en: (3) 4D cost volume compression. Tulyakov *et al.* [[78](#bib.bib78)] reduce
    the memory usage, without having to sacrify accuracy, by compressing the features
    into compact matching signatures. As such, the memory footprint is significantly
    reduced. More importantly, it allows the network to handle an arbitrary number
    of multiview images and to vary the number of inputs at runtime without having
    to re-train the network.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 4D成本体积压缩。Tulyakov *等人* [[78](#bib.bib78)] 通过将特征压缩成紧凑的匹配签名来减少内存使用，而不牺牲精度。因此，内存占用显著减少。更重要的是，它允许网络处理任意数量的多视角图像，并在运行时调整输入数量，而无需重新训练网络。
- en: 5.2.3 Hybrid 3D-4D cost volumes
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 混合3D-4D成本体积
- en: The correlation layer provides an efficient way to measure feature similarities,
    but it loses much information because it produces only a single-channel map for
    each disparity level. On the other hand, 4D cost volumes obtained by feature concatenation
    carry more information but are resource-demanding. They also require more parameters
    in the subsequent aggregation network to learn the similarity function. To benefit
    from both, Guo *et al.* [[86](#bib.bib86)] propose a hybrid approach, which constructs
    two cost volumes; one by feature concatenation but compressed into $12$ channels
    using two convolutions. The second one is built by dividing the high-dimension
    feature maps into $N_{g}$ groups along the feature channel, computing correlations
    within each group at all disparity levels, and finally concatenating the correlation
    maps forming another 4D volume. The two volumes are then combined together and
    passed to a 3D regularization module composed of four 3D convolution layers followed
    by three stacked 3D hourglass networks. This approach results in a significant
    reduction of parameters compared to 4D cost volumes built by only feature concatenation,
    without losing too much information like full correlations.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 相关层提供了一种高效的方法来衡量特征相似性，但由于它仅为每个视差级别生成一个单通道图，因此会丢失大量信息。另一方面，通过特征级联获得的4D成本体积包含了更多信息，但资源需求较高。这也需要在后续的聚合网络中使用更多的参数来学习相似性函数。为此，Guo
    *等人* [[86](#bib.bib86)] 提出了一个混合方法，该方法构建了两个成本体积；一个通过特征级联生成，但使用两个卷积压缩到$12$个通道。第二个成本体积是通过将高维特征图沿特征通道划分为$N_{g}$组，在每组内计算所有视差级别的相关性，然后将相关图拼接形成另一个4D体积。然后将两个体积结合起来，传递给一个由四个3D卷积层和三个堆叠的3D沙漏网络组成的3D正则化模块。与仅通过特征级联构建的4D成本体积相比，这种方法显著减少了参数数量，同时不会像完全相关性那样丢失太多信息。
- en: 5.3 Disparity computation
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 视差计算
- en: 'The simplest way to estimate the disparity map from the regularized cost volume
    $C$ is by using the pixel-wise argmin, *i.e.,* $0pt_{x}=\arg\min_{0pt}C(x,0pt)$
    (or equivalently $\arg\max$ if the volume $C$ encodes the likelihood). However,
    the agrmin/argmax operator is unable to produce sub-pixel accuracy and cannot
    be trained with back-propagation due to its non-differentiability. Another approach
    is the differentiable soft argmin/max over disparity [[66](#bib.bib66), [61](#bib.bib61),
    [73](#bib.bib73), [72](#bib.bib72)]:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从正则化成本体积 $C$ 中估计视差图的最简单方法是使用逐像素argmin，即 $0pt_{x}=\arg\min_{0pt}C(x,0pt)$（或者如果体积
    $C$ 编码了似然，则等效于 $\arg\max$）。然而，argmin/argmax 操作符无法产生子像素精度，并且由于其不可微性无法通过反向传播进行训练。另一种方法是对视差进行可微的软
    argmin/max [[66](#bib.bib66), [61](#bib.bib61), [73](#bib.bib73), [72](#bib.bib72)]。
- en: '|  | $d^{*}=\frac{1}{\sum_{j=0}^{n_{d}}e^{-C(x,j)}}\sum_{d=0}^{n_{d}}d\times
    e^{-C(x,d)}.$ |  | (4) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $d^{*}=\frac{1}{\sum_{j=0}^{n_{d}}e^{-C(x,j)}}\sum_{d=0}^{n_{d}}d\times
    e^{-C(x,d)}.$ |  | (4) |'
- en: The soft argmin operator approximates the sub-pixel MAP solution when the distribution
    is unimodal and symmetric [[78](#bib.bib78)]. When this assumption is not fulfilled,
    the softargmin blends the modes and may produce a solution that is far from all
    the modes and may result in over smoothing. Chen *et al.* [[87](#bib.bib87)] observe
    that this is particularly the case at boundary pixels where the estimated disparities
    follow multimodal distributions. To address these issues, Chen *et al.* [[87](#bib.bib87)]
    only apply a weighted average operation on a window centered around the modal
    with the maximum probability, instead of using a full-band weighted average on
    the entire disparity range.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当分布是单峰且对称时，软argmin操作符能够逼近子像素MAP解[[78](#bib.bib78)]。当这一假设不成立时，软argmin会混合各模式，可能会产生远离所有模式的解，导致过度平滑。Chen
    *et al.* [[87](#bib.bib87)] 观察到，特别是在边界像素处，估计的视差遵循多峰分布。为了解决这些问题，Chen *et al.* [[87](#bib.bib87)]
    仅在概率最大的模式中心的窗口上应用加权平均操作，而不是对整个视差范围使用全带宽加权平均。
- en: 'Tulyakov *et al.* [[78](#bib.bib78)] introduced the sub-pixel MAP approximation,
    which computes a weighted mean around the disparity with the maximum posterior
    probability as:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Tulyakov *et al.* [[78](#bib.bib78)] 引入了子像素MAP逼近方法，该方法计算围绕具有最大后验概率的视差的加权均值，如下所示：
- en: '|  | $d^{*}=\sum_{d:&#124;\hat{d}-d&#124;\leq\delta}d\cdot\sigma(C(x,d)),$
    |  | (5) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $d^{*}=\sum_{d:&#124;\hat{d}-d&#124;\leq\delta}d\cdot\sigma(C(x,d)),$
    |  | (5) |'
- en: where $\delta$ is a meta parameter set to $4$ in [[78](#bib.bib78)], $\sigma(C(x,d))$
    is the probability of the pixel $x$ having a disparity $d$, and $\displaystyle\hat{d}=\arg\max_{d}C(x,d)$.
    The sub-pixel MAP is only used for inference. Tulyakov *et al.* [[78](#bib.bib78)]
    also showed that, unlike the softargmin/max, this approach allows changing the
    disparity range at runtime without re-training the network.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\delta$ 是在 [[78](#bib.bib78)] 中设定为 $4$ 的元参数，$\sigma(C(x,d))$ 是像素 $x$ 具有视差
    $d$ 的概率，$\displaystyle\hat{d}=\arg\max_{d}C(x,d)$。子像素MAP仅用于推断。Tulyakov *et al.*
    [[78](#bib.bib78)] 还展示了，与软argmin/max不同，这种方法允许在运行时更改视差范围而无需重新训练网络。
- en: 5.4 Variants
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 变体
- en: The pipeline described so far infers disparity maps that can be of low-resolution
    (along the width, height, and disparity dimensions), incomplete, noisy, missing
    fine details, and suffering from over-smoothing especially at object boundaries.
    As such, many variants have been introduced to (1) improve their resolution (Section [5.4.1](#S5.SS4.SSS1
    "5.4.1 Learning to infer high resolution disparity maps ‣ 5.4 Variants ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")), (2) improve the processing time, especially at runtime (Section [5.4.3](#S5.SS4.SSS3
    "5.4.3 Learning for realtime processing ‣ 5.4 Variants ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")),
    and (3) perform disparity completion and denoising (Section [5.4.2](#S5.SS4.SSS2
    "5.4.2 Learning for completion and denoising ‣ 5.4 Variants ‣ 5 End-to-end depth
    from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止描述的流程推断出的视差图可能是低分辨率的（在宽度、高度和视差维度上）、不完整、噪声多、缺乏细节，并且特别是在物体边界处容易过度平滑。因此，已经引入了许多变体来（1）提高其分辨率（第[5.4.1节](#S5.SS4.SSS1
    "5.4.1 Learning to infer high resolution disparity maps ‣ 5.4 Variants ‣ 5 End-to-end
    depth from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")），（2）提高处理时间，尤其是在运行时（第[5.4.3节](#S5.SS4.SSS3 "5.4.3 Learning for realtime
    processing ‣ 5.4 Variants ‣ 5 End-to-end depth from stereo ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")），以及（3）进行视差完成和去噪（第[5.4.2节](#S5.SS4.SSS2
    "5.4.2 Learning for completion and denoising ‣ 5.4 Variants ‣ 5 End-to-end depth
    from stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")）。
- en: 5.4.1 Learning to infer high resolution disparity maps
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 学习推断高分辨率视差图
- en: Directly regressing high-resolution depth maps that contain fine details, *e.g.,*
    by adding further upconvolutional layers to upscale the cost volume, would require
    a large number of parameters and thus are computationally expensive and difficult
    to train. As such, state-of-the-art methods struggle to process high resolution
    imagery because of memory constraints or speed limitations. This has been addressed
    by using either bottom-up or top-down techniques.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 直接回归包含细节的高分辨率深度图，例如，通过添加更多的上卷积层来放大代价体积，需要大量的参数，因此计算开销大，训练困难。因此，最先进的方法在处理高分辨率图像时由于内存限制或速度限制而面临挑战。这通过使用自下而上或自上而下的技术来解决。
- en: Bottom-up techniques operate in a sliding window-like approach. They take small
    patches and estimate the refined disparity either for the entire patch or for
    the pixel at the center of the patch. Lee *et al.* [[94](#bib.bib94)] follow a
    split-and-merge approach. The input image is split into regions, and a depth is
    estimated for each region. The estimates are then merged using a fusion network,
    which operates in the Fourier domain so that depth maps with different cropping
    ratios can be handled. While both sliding window and split-and-merge approaches
    reduce memory requirements, they require multiple forward passes, and thus are
    not suitable for realtime applications. Also, these methods do not capture the
    global context, which can limit their performance.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 自下而上的技术采用类似滑动窗口的方法。它们取小块图像，并为整个小块或小块中心的像素估计细化的视差。Lee *et al.* [[94](#bib.bib94)]
    采用分裂和合并的方法。输入图像被分割成区域，每个区域估计一个深度。这些估计通过一个在傅里叶域中运行的融合网络进行合并，从而处理具有不同裁剪比的深度图。尽管滑动窗口和分裂合并方法减少了内存需求，但它们需要多次前向传递，因此不适合实时应用。此外，这些方法不能捕捉全局上下文，可能限制其性能。
- en: 'Top-down techniques, on the other hand, operate on the disparity map estimates
    in a hierarchical manner. They first estimate a low-resolution disparity map and
    then upsample them to the desired resolution, *e.g.,* using bilinear upsampling,
    and further process them using residual learning to recover small details and
    thin structures [[72](#bib.bib72), [73](#bib.bib73), [81](#bib.bib81)]. This process
    can also be run progressively by cascading many of such refinement blocks, each
    block refines the estimate of the previous block [[62](#bib.bib62), [72](#bib.bib72)].
    Unlike upsampling cost volumes, refining disparity maps is computationally efficient
    since it only requires 2D convolutions. Existing methods mainly differ in the
    type of additional information that is appended to the upsampled disparity map
    for refinement. For instance:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 相对而言，自顶向下的方法以分层的方式操作视差图估计。它们首先估计一个低分辨率的视差图，然后将其上采样到所需分辨率，*例如*，使用双线性上采样，并通过残差学习进一步处理，以恢复小细节和细微结构
    [[72](#bib.bib72), [73](#bib.bib73), [81](#bib.bib81)]。这个过程还可以通过级联许多这样的精细化块逐步进行，每个块都精细化前一个块的估计
    [[62](#bib.bib62), [72](#bib.bib72)]。与上采样代价体积不同，精细化视差图在计算上更高效，因为它仅需要2D卷积。现有方法主要在附加到上采样视差图的附加信息类型上有所不同。例如：
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Khamis *et al.* [[72](#bib.bib72)] concatenate the upsampled disparity map with
    the original reference image.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Khamis *等人* [[72](#bib.bib72)] 将上采样的视差图与原始参考图像进行连接。
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Liang *et al.* [[63](#bib.bib63)] append to the initial disparity map the cost
    volume and the reconstruction error, defined as the difference between the left
    image and the right image but warped to the left image using the estimated disparity
    map.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Liang *等人* [[63](#bib.bib63)] 将代价体积和重建误差附加到初始视差图中，重建误差定义为左侧图像与右侧图像之间的差异，但被变换到左侧图像中，使用估计的视差图。
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Chabra *et al.* [[81](#bib.bib81)] take the left image and the reconstruction
    error on one side, and the left disparity and the geometric error map, defined
    as the difference between the estimated left disparity and right disparity but
    warped onto the left view. These are independently filtered using one layer of
    convolutions followed by batch normalization. The results of the two streams are
    concatenated and then further processed using a series of convolutional layers
    to produce the refined disparity map.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Chabra *等人* [[81](#bib.bib81)] 将左侧图像和重建误差放在一侧，并将左侧视差与几何误差图进行比较，几何误差图定义为估计的左侧视差与右侧视差之间的差异，但被变换到左侧视图中。这些数据通过一层卷积和批量归一化独立过滤。两个流的结果被连接在一起，然后通过一系列卷积层进一步处理，生成精细化的视差图。
- en: These methods improve the spatial resolution but not the disparity resolution.
    To refine both the spatial and depth resolution, while operating on high resolution
    images, Yang *et al.* [[32](#bib.bib32)] propose to search for correspondences
    incrementally over a coarse-to-fine hierarchy. The approach constructs a pyramid
    of four 4D cost volumes, each with increasing spatial and depth resolutions. Each
    volume is filtered by six 3D convolution blocks, and further processed with a
    Volumetric Pyramid Pooling block, an extension of Spatial Pyramid Pooling to feature
    volumes, to generate features that capture sufficient global context for high
    resolution inputs. The output is then either (1) processed with another conv3D
    block to generate a 3D cost volume from which disparity can be directly regressed.
    This allows to report on-demand disparities computed from the current scale, or
    (2) tri-linearly-upsampled to a higher spatial and disparity resolution so that
    it can be fused with the next 4D volume in the pyramid. To minimise memory requirements,
    the approach uses striding along the disparity dimensions in the last and second
    last volumes of the pyramid. The network is trained end-to-end using a multi-scale
    loss. This hierarchical design also allows for anytime on-demand reports of disparity
    by capping intermediate coarse results, allowing accurate predictions for near-range
    structures with low latency (30ms).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法提高了空间分辨率，但没有提高视差分辨率。为了同时提高空间和深度分辨率，同时处理高分辨率图像，杨*等人* [[32](#bib.bib32)] 提出了在粗到细的层次结构上逐步搜索对应点的方法。这种方法构建了四个4D代价体积的金字塔，每个体积具有逐步增加的空间和深度分辨率。每个体积通过六个3D卷积块进行滤波，并进一步通过体积金字塔池化块处理，体积金字塔池化是对特征体积的空间金字塔池化的扩展，以生成捕获高分辨率输入的充分全局上下文的特征。然后，输出可以（1）通过另一个conv3D块处理，生成一个可以直接回归视差的3D代价体积。这允许报告从当前尺度计算的按需视差，或者（2）以三线性上采样到更高的空间和视差分辨率，以便与金字塔中的下一个4D体积融合。为了最小化内存需求，该方法在金字塔的最后一个和倒数第二个体积中沿视差维度进行步长处理。网络通过多尺度损失进行端到端训练。这种层次化设计还允许通过限制中间粗糙结果来随时按需报告视差，从而在低延迟（30毫秒）下进行准确的近距离结构预测。
- en: This approach shares some similarities with the approach of Kendall *et al.* [[61](#bib.bib61)],
    which constructs hierarchical 4D feature volumes and processes them from coarse
    to fine using 3D convolutions. Kendall *et al.*’s approach [[61](#bib.bib61)],
    however, has been used to leverage context with a wide field of view while Yang
    *et al.* [[32](#bib.bib32)] apply coarse-to-fine principles for high-resolution
    inputs and anytime, on-demand processing.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与肯德尔*等人* [[61](#bib.bib61)] 的方法有一些相似之处，肯德尔*等人*的办法构建了分层的4D特征体积，并使用3D卷积从粗到细进行处理。然而，肯德尔*等人*的办法[[61](#bib.bib61)]
    已被用于利用广阔视场的上下文，而杨*等人*[[32](#bib.bib32)] 则将粗到细的原则应用于高分辨率输入和随时按需处理。
- en: 5.4.2 Learning for completion and denoising
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 完成和去噪的学习
- en: Raw disparities can be noisy and incomplete, especially near object boundaries
    where depth smearing between objects remains a challenge. Several techniques have
    been developed for denoising and completion. Some of them are ad-hoc, *i.e.,*
    post-process the noisy and uncomplete initial estimates to generate clean and
    complete depth maps. Other methods addressed the issue of the lack of training
    data for completion and denoising. Others proposed novel depth representations
    that are more suitable for this task, especially for solving the depth smearing
    between objects.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 原始视差可能会很嘈杂且不完整，尤其是在物体边界附近，物体之间的深度模糊仍然是一个挑战。已经开发了几种去噪和补全技术。其中一些是专门的，即后处理嘈杂且不完整的初始估计以生成干净且完整的深度图。其他方法则解决了补全和去噪的训练数据不足的问题。还有一些方法提出了更适合此任务的新颖深度表示，特别是用于解决物体之间的深度模糊问题。
- en: Ad-hoc methods process the initially estimated disparities a using variational
    approaches [[51](#bib.bib51), [95](#bib.bib95)], Fully-Connected CRFs (DenseCRF) [[27](#bib.bib27),
    [96](#bib.bib96)], hierarchical CRFs [[2](#bib.bib2)], and diffusion processes [[40](#bib.bib40)]
    guided by confidence maps [[97](#bib.bib97)]. They encourage pixels that are spatially
    close and with similar colors to have closer disparity predictions. They have
    been also explored by Liu *et al.* [[5](#bib.bib5)]. However, unlike Li *et al.* [[2](#bib.bib2)],
    Liu *et al.* [[5](#bib.bib5)] used a CNN to minimize the CRF energy. Convolutional
    Spatial Propagation Networks (CSPN) [[98](#bib.bib98), [99](#bib.bib99)], which
    implement an anisotropic diffusion process, are particularly suitable for depth
    completion since they predict the diffusion tensor using a deep CNN. This is then
    applied to the initial map to obtain the refined one.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊方法处理最初估计的视差，采用变分方法[[51](#bib.bib51), [95](#bib.bib95)]，全连接条件随机场（DenseCRF）[[27](#bib.bib27),
    [96](#bib.bib96)]，分层条件随机场[[2](#bib.bib2)]，以及由置信度图引导的扩散过程[[40](#bib.bib40), [97](#bib.bib97)]。它们鼓励空间上相近且颜色相似的像素具有更接近的视差预测。刘*等人*[[5](#bib.bib5)]也对此进行了探讨。然而，与李*等人*[[2](#bib.bib2)]不同，刘*等人*[[5](#bib.bib5)]使用卷积神经网络来最小化条件随机场的能量。卷积空间传播网络（CSPN）[[98](#bib.bib98),
    [99](#bib.bib99)]，实现了各向异性扩散过程，特别适用于深度补全，因为它们通过深度卷积神经网络预测扩散张量。然后，这个张量应用于初始图以获得精细化的图。
- en: One of the main challenges of deep learning-based depth completion and denoising
    is the lack of labelled training data, *i.e.,* pairs of noisy, incomplete depth
    maps and their corresponding clean depth maps. To address this issue, Jeon and
    Lee [[29](#bib.bib29)] propose a pairwise depth image dataset generation method
    using dense 3D surface reconstruction with a filtering method to remove low quality
    pairs. They also present a multi-scale Laplacian pyramid based neural network
    and structure preserving loss functions to progressively reduce the noise and
    holes from coarse to fine scales. The approach first predicts the clean complete
    depth image at the coarsest scale, which has a quarter of the original resolution.
    The predicted depth map is then progressively upsampled through the pyramid to
    predict the half and original-sized image. At the coarse level, the approach captures
    global context while at finer scales it captures local information. In addition,
    the features extracted during the downsampling are passed to the upsampling pyramid
    with skip connections to prevent the loss of the original details in the input
    depth image during the upsampling.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的深度补全和去噪的主要挑战之一是缺乏标记的训练数据，即有噪声、不完整的深度图与其对应的干净深度图对。为了解决这个问题，Jeon 和 Lee
    [[29](#bib.bib29)] 提出了一种使用密集的三维表面重建和过滤方法生成成对深度图像数据集的方法，以去除低质量对。他们还提出了一种基于多尺度拉普拉斯金字塔的神经网络和结构保持损失函数，以逐步减少从粗到细尺度的噪声和孔洞。该方法首先在最粗尺度下预测干净的完整深度图，该尺度为原始分辨率的四分之一。然后，通过金字塔逐步上采样预测出半尺寸和原始尺寸的图像。在粗糙层面，该方法捕捉全球背景，而在更细的尺度上捕捉局部信息。此外，在下采样过程中提取的特征通过跳跃连接传递到上采样金字塔中，以防止在上采样过程中输入深度图像中原始细节的丢失。
- en: Instead of operating on the network architecture, the loss function, or the
    training datasets, Imran *et al.* [[100](#bib.bib100)] propose a new representation
    for depth called Depth Coefficients (DC) to address the problem of depth smearing
    between objects. The representation enables convolutions to more easily avoid
    inter-object depth mixing. The representation uses a multi-channel image of the
    same size as the target depth map, with each channel representing a fixed depth.
    The depth values increase in even steps of size $b$. (The approach uses $80$ bins.)
    The choice of the number of bins trades-off memory vs. precision. The vector composed
    of all these values at a given pixel defines the depth coefficients for that pixel.
    For each pixel, these coefficients are constrained to be non-negative and sum
    to $1$. This representation of depth provides a much simpler way for CNNs to avoid
    depth mixing. First, CNNs can learn to avoid mixing depths in different channels
    as needed. Second, since convolutions apply to all channels simultaneously, depth
    dependencies, like occlusion effects, can be modelled and learned by neural networks.
    The main limitation, however, is that the depth range needs to be set in advance
    and cannot be changed at runtime without re-training the network. Imran *et al.* [[100](#bib.bib100)]
    also show that the standard Mean Squared Error (MSE) loss function can promote
    depth mixing, and thus propose to use cross-entropy loss for estimating the depth
    coefficients.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Imran *等人* [[100](#bib.bib100)] 提出了一个新的深度表示方法，称为深度系数（DC），以解决对象之间深度模糊的问题。该表示方法使得卷积操作能够更容易地避免对象之间的深度混合。该表示方法使用一个与目标深度图大小相同的多通道图像，每个通道表示一个固定的深度。深度值以大小为
    $b$ 的均匀步长递增。（该方法使用了 $80$ 个区间。）区间数量的选择是在内存和精度之间权衡的结果。由这些值组成的向量在给定像素处定义了该像素的深度系数。对于每个像素，这些系数被限制为非负且总和为
    $1$。这种深度表示提供了一种更简单的方式，使 CNN 避免深度混合。首先，CNN 可以根据需要学习避免在不同通道中混合深度。其次，由于卷积对所有通道同时应用，深度依赖关系，如遮挡效应，可以通过神经网络进行建模和学习。然而，主要的限制是深度范围需要预先设定，并且在运行时不能更改而无需重新训练网络。Imran
    *等人* [[100](#bib.bib100)] 还展示了标准的均方误差（MSE）损失函数可能会促进深度混合，因此建议使用交叉熵损失来估计深度系数。
- en: 5.4.3 Learning for realtime processing
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3 实时处理的学习
- en: The goal is to design efficient stereo algorithms that not only produce reliable
    and accurate estimations, but also run in realtime. For instance, in the PSMNet [[64](#bib.bib64)],
    the cost volume construction and aggregation takes more than $250$ms (on nNvidia
    Titan-Xp GPU). This renders realtime applications infeasible. To speed the process,
    Khamis *et al.* [[72](#bib.bib72)] first estimate a low resolution disparity map
    and then hierarchically refine it. Yin *et al.* [[80](#bib.bib80)] employ a fixed,
    coarse-to-fine procedure to iteratively find the match. Chabra *et al.* [[81](#bib.bib81)]
    use 3D dilated convolutions in the width, height, and disparity channels when
    filtering the cost volume. Duggal *et al.* [[83](#bib.bib83)] combine deep learning
    with PatchMatch [[101](#bib.bib101)] to adaptively prune out the potentially large
    search space and significantly speed up inference. PatchMatch-based pruner module
    is able to predict a confidence range for each pixel, and construct a sparse cost
    volume that requires significantly less operations. This also allows the model
    to focus only on regions with high likelihood and save computation and memory.
    To enable end-to-end training, Duggal *et al.* [[83](#bib.bib83)] unroll PatchMatch
    as an RNN where each unrolling step is equivalent to an iteration of the algorithm.
    This approach achieved a performance that is comparable to the state-of-the-art,
    *e.g.,*  [[64](#bib.bib64), [68](#bib.bib68)], while reducing the computation
    time from $600$ms to $60$ms per image in the KITTI2015 dataset.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是设计高效的立体算法，这些算法不仅能够产生可靠和准确的估计，还能实时运行。例如，在PSMNet [[64](#bib.bib64)]中，成本体积的构建和聚合耗时超过$250$ms（在nNvidia
    Titan-Xp GPU上）。这使得实时应用变得不可行。为了加快这一过程，Khamis *et al.* [[72](#bib.bib72)] 首先估计一个低分辨率的视差图，然后对其进行分层细化。Yin
    *et al.* [[80](#bib.bib80)] 采用固定的粗到细程序来迭代地找到匹配。Chabra *et al.* [[81](#bib.bib81)]
    在过滤成本体积时使用宽度、高度和视差通道中的3D膨胀卷积。Duggal *et al.* [[83](#bib.bib83)] 将深度学习与PatchMatch [[101](#bib.bib101)]
    结合，以自适应地修剪可能很大的搜索空间，并显著加速推断。基于PatchMatch的修剪模块能够预测每个像素的置信度范围，并构建一个需要显著更少操作的稀疏成本体积。这也使得模型只关注高概率区域，从而节省计算和内存。为了实现端到端训练，Duggal
    *et al.* [[83](#bib.bib83)] 将PatchMatch展开为一个RNN，其中每一步展开等同于算法的一个迭代。这种方法实现了与最先进技术相当的性能，*例如*，[[64](#bib.bib64),
    [68](#bib.bib68)]，同时将KITTI2015数据集中每张图像的计算时间从$600$ms减少到$60$ms。
- en: 5.5 Learning confidence maps
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 学习置信度图
- en: The ability to detect, and subsequently remedy to, failure cases is important
    for applications such as autonomous driving and medical imaging. Thus, a lot of
    research has been dedicated to estimating confidence or uncertainty maps, which
    are then used to sparsify the estimated disparities by removing potential errors
    and then replacing them from the reliable neighboring pixels. Disparity maps can
    also be incorporated in a disparity refinement pipeline to guide the refinement
    process [[102](#bib.bib102), [103](#bib.bib103), [74](#bib.bib74)]. Seki *et al.* [[102](#bib.bib102)],
    for example, incorporate the confidence map into a Semi-Global Matching (SGM)
    module for dense disparity estimation. Gidaris *et al.* [[103](#bib.bib103)] use
    confidence maps to detect the incorrect estimates, replace them with disparities
    from neighbouring regions, and then refine the disparity using a refinement network.
    Jie *et al.* [[74](#bib.bib74)], on the other hand, estimate two confidence maps,
    one for each of the input images, concatenate them with their associated cost
    volumes, and use them as input to a 3D convolutional LSTM to selectively focus
    in the subsequent step on the left-right mismatched regions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 检测并随后修复失败案例的能力对自动驾驶和医疗成像等应用非常重要。因此，很多研究致力于估计置信度或不确定性图，这些图随后用于通过去除潜在错误来稀疏化估计的视差，并用可靠的邻近像素替换它们。视差图还可以被纳入视差细化管道，以指导细化过程 [[102](#bib.bib102),
    [103](#bib.bib103), [74](#bib.bib74)]。例如，Seki *et al.* [[102](#bib.bib102)] 将置信度图融入到一个半全局匹配（SGM）模块中进行密集视差估计。Gidaris
    *et al.* [[103](#bib.bib103)] 使用置信度图来检测不正确的估计，用邻近区域的视差替换它们，然后使用细化网络细化视差。另一方面，Jie
    *et al.* [[74](#bib.bib74)] 估计两个置信度图，每个输入图像一个，将它们与相关的成本体积连接起来，并将它们作为输入传递给3D卷积LSTM，以在随后的步骤中选择性地关注左-右不匹配区域。
- en: Conventional confidence estimation methods are mostly based on assumptions and
    heuristics on the matching cost volume analysis, see [[59](#bib.bib59)] for a
    review and evaluation of the early methods. Recent techniques are based on supervised
    learning [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106), [107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109)]. They estimate confidence maps directly
    from the disparity space either in an ad-hoc manner, or in an integrated fashion
    so that they can be trained end-to-end along with the disparity/depth estimation.
    Poggi *et al.* [[110](#bib.bib110)] provide a quantitative evaluation. Below,
    we discuss some of these techniques.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的置信度估计方法大多基于对匹配成本体积分析的假设和启发式方法，参见 [[59](#bib.bib59)] 以获取早期方法的综述和评估。最近的技术则基于监督学习
    [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106), [107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109)]。这些技术直接从视差空间中估计置信度图，或以特别的方式，或以集成的方式进行训练，从而能够与视差/深度估计一起端到端地训练。Poggi
    *等人* [[110](#bib.bib110)] 提供了定量评估。下面，我们将深入探讨这些技术的一些方面。
- en: 5.5.1 Confidence from left-right consistency check
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1 来自左右一致性检查的置信度
- en: Left-right consistency is one of the most commonly-used criteria for measuring
    confidence in disparity estimates. The idea is to estimate two disparity maps,
    one from the left image ($D_{left}$), and another from the right image ($D_{right}$).
    An error map can then be computed by taking a pixel-wise difference between $D_{left}$
    and $D_{right}$, but warped back onto the left image, and converting them into
    probabilities [[63](#bib.bib63)]. This measure is suitable for detecting occlusions,
    *i.e.,* regions that are visible in one view but not in the other.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 左右一致性是衡量视差估计置信度的最常用标准之一。其理念是估计两个视差图，一个来自左图像（$D_{left}$），另一个来自右图像（$D_{right}$）。然后，通过对
    $D_{left}$ 和 $D_{right}$ 进行逐像素差异计算来生成误差图，但将其重新映射回左图像，并将其转换为概率 [[63](#bib.bib63)]。这种度量适用于检测遮挡，即在一个视图中可见但在另一个视图中不可见的区域。
- en: Left-right consistency can also be learned using deep or shallow networks composed
    of fully convolutional layers [[102](#bib.bib102), [74](#bib.bib74)]. Seki *et
    al.* [[102](#bib.bib102)] propose a patch-based confidence prediction (PBCP) network,
    which requires two disparity maps, one estimated from the left image and the other
    one from the right image. PBCP uses a two-channel network. The first channel enforces
    left-right consistency while the second one enforces local consistency. The network
    is trained in a classifier manner. It outputs a label per pixel indicating whether
    the estimated disparity is correct.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 左右一致性也可以通过由全卷积层组成的深度或浅层网络来学习 [[102](#bib.bib102), [74](#bib.bib74)]。Seki *等人*
    [[102](#bib.bib102)] 提出了一个基于补丁的置信度预测（PBCP）网络，该网络需要两个视差图，一个是从左图像估计的，另一个是从右图像估计的。PBCP
    使用了一个双通道网络。第一个通道强制执行左右一致性，而第二个通道则强制执行局部一致性。该网络以分类器的方式进行训练，每个像素输出一个标签，指示估计的视差是否正确。
- en: Instead of treating left-right consistency check as an isolated post-processing
    step, Jie *et al.* [[74](#bib.bib74)] perform it jointly with disparity estimation,
    using a Left-Right Comparative Recurrent (LRCR) model. It consists of two parallel
    convolutional LSTM networks [[111](#bib.bib111)], which produce two error maps;
    one for the left disparity and another for the right disparity. The two error
    maps are then concatenated with their associated cost volumes and used as input
    to a 3D convolutional LSTM to selectively focus in the next step on the left-right
    mismatched regions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Jie *等人* [[74](#bib.bib74)] 并未将左右一致性检查视为一个孤立的后处理步骤，而是与视差估计联合进行，使用了一个左右比较递归（LRCR）模型。该模型由两个并行的卷积
    LSTM 网络 [[111](#bib.bib111)] 组成，产生两个误差图：一个用于左侧视差，另一个用于右侧视差。然后，将这两个误差图与它们关联的成本体积连接，并作为
    3D 卷积 LSTM 的输入，在下一步中选择性地聚焦于左右不匹配的区域。
- en: 5.5.2 Confidence from a single raw disparity map
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2 来自单个原始视差图的置信度
- en: Left-right consistency checks estimate two disparity maps and thus are expensive
    at runtime. Shaked and Wolf [[46](#bib.bib46)] train, via the binary cross entropy
    loss, a network, composed of two fully-connected layers, to predict the correctness
    of an estimated disparity from only the reference image. Poggi and Mattoccia [[107](#bib.bib107)]
    pose the confidence estimation as a regression problem and solve it using a CNN
    trained on small patches. For each pixel, the approach extracts a square patch
    around the pixel and forwards it to a CNN trained to distinguish between patterns
    corresponding to correct and erroneous disparity assignments. It is a single channel
    network, designed for $9\times 9$ image patches. Zhang *et al.* [[73](#bib.bib73)]
    use a similar confidence map estimation network, called *invalidation network*.
    The key idea is to train the network to predict confidence using a pixel-wise
    error between the left disparity and the right disparity. At runtime, the network
    only requires the left disparity. Finally, Poggi and Mattoccia [[112](#bib.bib112)]
    show that one can improve the confidence maps estimated using previous algorithms
    by enforcing local consistency in the confidence estimates.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 左右一致性检查估计两个视差图，因此在运行时开销较大。Shaked 和 Wolf [[46](#bib.bib46)] 通过二元交叉熵损失训练一个由两层全连接层组成的网络，仅从参考图像预测估计视差的正确性。Poggi
    和 Mattoccia [[107](#bib.bib107)] 将置信度估计视为回归问题，并使用在小补丁上训练的 CNN 解决它。对于每个像素，该方法提取一个像素周围的方形补丁，并将其输入到一个训练好的
    CNN 中，该 CNN 用于区分正确和错误的视差分配模式。这是一个单通道网络，设计用于 $9\times 9$ 图像补丁。Zhang *et al.* [[73](#bib.bib73)]
    使用类似的置信度图估计网络，称为 *无效网络*。其关键思想是训练网络通过计算左视差和右视差之间的像素级误差来预测置信度。在运行时，网络只需要左视差。最后，Poggi
    和 Mattoccia [[112](#bib.bib112)] 显示，可以通过在置信度估计中强制局部一致性来改善使用之前算法估计的置信度图。
- en: 5.5.3 Confidence map from matching densities
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3 来自匹配密度的置信度图
- en: Traditional deep networks represent activations and outputs as deterministic
    point estimates. Gast and Roth [[113](#bib.bib113)] explore the possibility of
    replacing the deterministic outputs by probabilistic output layers. To go one
    step further, they replace all intermediate activations by distributions. As such,
    the network can be used to estimate the matching probability densities, hereinafter
    referred to as *matching densities*, which can then be converted into uncertainties
    (or confidence) at runtime. The main challenge of estimating matching densities
    is the computation time. To make it tractable, Gast and Roth [[113](#bib.bib113)]
    assume parametric distributions. Yin *et al.* [[80](#bib.bib80)] relax this assumption
    and propose a pyramidal architecture to make the computation cost sustainable
    and allow for the estimation of confidence at run time.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的深度网络将激活和输出表示为确定性点估计。Gast 和 Roth [[113](#bib.bib113)] 探索了用概率输出层替代确定性输出的可能性。更进一步，他们将所有中间激活替换为分布。因此，该网络可以用于估计匹配概率密度，以下简称为
    *匹配密度*，这些密度可以在运行时转化为不确定性（或置信度）。估计匹配密度的主要挑战是计算时间。为了解决这个问题，Gast 和 Roth [[113](#bib.bib113)]
    假设了参数分布。Yin *et al.* [[80](#bib.bib80)] 放宽了这一假设，并提出了一种金字塔结构，以使计算成本可持续，并允许在运行时进行置信度估计。
- en: 5.5.4 Local vs. global reasoning
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.4 局部与全局推理
- en: Some techniques, *e.g.,* Seki *et al.* [[102](#bib.bib102)]’s, reason locally
    by enforcing local consistency. Tosi *et al.* [[114](#bib.bib114)] introduced
    LGC-Net to move beyond local reasoning. The input reference image and its disparity
    map are forwarded to a local network, *e.g.,* C-CNN [[107](#bib.bib107)], and
    a global network, *e.g.,* an encoder/decoder architecture with a large receptive
    field. The output of the two networks and the initial disparity, concatenated
    with the reference image, are further processed with three independent convolutional
    towers whose outputs are concatenated and processed with three $1\times 1$ convolutional
    layers to finally infer the confidence map.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 一些技术，如 Seki *et al.* [[102](#bib.bib102)] 的方法，通过强制局部一致性来进行局部推理。Tosi *et al.*
    [[114](#bib.bib114)] 引入了 LGC-Net，以超越局部推理。输入参考图像及其视差图被转发到一个局部网络，例如 C-CNN [[107](#bib.bib107)]，以及一个全局网络，例如具有大接收域的编码器/解码器结构。两个网络的输出和初始视差与参考图像连接后，进一步处理于三个独立的卷积塔，其输出被连接并通过三个
    $1\times 1$ 卷积层处理，最终推断置信度图。
- en: 5.5.5 Combining multiple estimators
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.5 结合多个估计器
- en: Some papers combine the estimates of multiple algorithms to achieve a better
    accuracy. Haeusler *et al.* [[104](#bib.bib104)] fed a random forest with a pool
    of $23$ confidence maps, estimated using conventional techniques, yielding a much
    better accuracy compared to any confidence map in the pool. Batsos *et al.* [[109](#bib.bib109)]
    followed a similar idea but combine the strengths and mitigate the weaknesses
    of four basic stereo matchers in order to generate a robust matching volume for
    the subsequent optimization and regularization steps. Poggi and Mattoccia [[58](#bib.bib58)]
    train an ensemble regression trees classifier. These methods are independent of
    the disparity estimation module, and rely on the availability of the cost volume.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文结合多个算法的估计以实现更好的准确性。Haeusler *et al.* [[104](#bib.bib104)] 将一个包含 $23$ 个置信度图的随机森林输入其中，这些置信度图是使用传统技术估计的，相比池中的任何置信度图，获得了更好的准确性。Batsos
    *et al.* [[109](#bib.bib109)] 采用了类似的想法，但结合了四种基本立体匹配器的优点并减轻其缺点，以生成一个鲁棒的匹配体积，用于随后的优化和正则化步骤。Poggi
    和 Mattoccia [[58](#bib.bib58)] 训练了一个集成回归树分类器。这些方法独立于视差估计模块，并依赖于代价体积的可用性。
- en: 6 Learning multiview stereo
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 学习多视图立体
- en: '| ![Refer to caption](img/bd0a040ac7e65662b0dc439907d960a2.png) | ![Refer to
    caption](img/312c709404753414edfe435f8901d026.png) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/bd0a040ac7e65662b0dc439907d960a2.png) | ![参见说明](img/312c709404753414edfe435f8901d026.png)
    |'
- en: '| (a) Hartmann *et al.* [[47](#bib.bib47)]. | (b) Flynn *et al.* [[66](#bib.bib66)].
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| (a) Hartmann *et al.* [[47](#bib.bib47)]。 | (b) Flynn *et al.* [[66](#bib.bib66)]。
    |'
- en: '| ![Refer to caption](img/e473e8d663929d1792abd90297ca50aa.png) |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/e473e8d663929d1792abd90297ca50aa.png) |'
- en: '| (c) Kar *et al.* [[60](#bib.bib60)] and Yao *et al.* [[93](#bib.bib93)].
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| (c) Kar *et al.* [[60](#bib.bib60)] 和 Yao *et al.* [[93](#bib.bib93)]。 |'
- en: '| ![Refer to caption](img/080e2675293a6cd93572e935861c8e0b.png) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/080e2675293a6cd93572e935861c8e0b.png) |'
- en: '| (d) Huang *et al.* [[27](#bib.bib27)]. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| (d) Huang *et al.* [[27](#bib.bib27)]。 |'
- en: 'Figure 6: Taxonomy of multivew stereo methods. (a), (b), and (c) perform early
    fusion, while (d) performs early fusion by aggregating features across depth plans,
    and late fusion by aggregating cost volumes across views.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：多视图立体方法的分类。(a)、(b) 和 (c) 执行早期融合，而 (d) 通过聚合深度平面上的特征进行早期融合，并通过在视图之间聚合代价体积进行晚期融合。
- en: Multiview Stereo (MVS) methods follow the same pipeline as of depth-from-stereo.
    Early works focused on computing the similarity between multiple patches. For
    instance, Hartmann *et al.* [[47](#bib.bib47)] (Fig. [6](#S6.F6 "Figure 6 ‣ 6
    Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(a)) replace the pairwise correlation layer used in stereo
    matching by an average pooling layer to aggregate the learned features of $n\geq
    2$ input patches, and then feed the output to a top network, which returns a matching
    score. With this method, computing the best match for a pixel on the reference
    image requires $n_{d}^{n-1}$ forward passes. ($n_{d}$ is the number of depth levels
    and $n$ is the number of images.) This is computationally very expensive especially
    when dealing with high resolution images.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 多视图立体（MVS）方法遵循与深度从立体图像相同的流程。早期的工作集中在计算多个补丁之间的相似性。例如，Hartmann *et al.* [[47](#bib.bib47)]（图[6](#S6.F6
    "Figure 6 ‣ 6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")-(a)）将立体匹配中使用的成对相关层替换为平均池化层，以聚合 $n\geq 2$ 个输入补丁的学习特征，然后将输出送入一个顶层网络，该网络返回一个匹配分数。使用这种方法，为参考图像上的像素计算最佳匹配需要
    $n_{d}^{n-1}$ 次前向传播。($n_{d}$ 是深度层级的数量，$n$ 是图像的数量。) 这在处理高分辨率图像时计算成本非常高。
- en: Techniques that compute depth maps in a single forward pass differ in the way
    the information from the multiple views is fed to the network and aggregated.
    We classify them into whether they are volumetric (Section [6.1](#S6.SS1 "6.1
    Volumetric representations ‣ 6 Learning multiview stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")) or Plane-Sweep Volume (PSV)-based
    (Section [6.2](#S6.SS2 "6.2 Plane-Sweep Volume representations ‣ 6 Learning multiview
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")).
    The latter does not rely on intermediate volumetric representations of the 3D
    geometry. The only exception is the approach of Hou *et al.* [[115](#bib.bib115)],
    which performs temporal fusion of the latent representations of the input images.
    The approach, however, requires temporally-ordered images. Table [IV](#S6.T4 "TABLE
    IV ‣ 6.2 Plane-Sweep Volume representations ‣ 6 Learning multiview stereo ‣ A
    Survey on Deep Learning Techniques for Stereo-based Depth Estimation") provides
    a taxonomy and compares $13$ state-of-the-art MVS techniques.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 计算深度图的单次前向传递技术在将多视图信息输入网络和汇聚信息的方式上有所不同。我们将其分类为体积表示（第 [6.1](#S6.SS1 "6.1 Volumetric
    representations ‣ 6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") 节）或平面扫描体积 (PSV) 基础（第 [6.2](#S6.SS2 "6.2 Plane-Sweep
    Volume representations ‣ 6 Learning multiview stereo ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation") 节）。后者不依赖于 3D 几何的中间体积表示。唯一的例外是 Hou
    *等人* [[115](#bib.bib115)] 的方法，该方法对输入图像的潜在表示进行时间上的融合。然而，该方法需要时间排序的图像。表 [IV](#S6.T4
    "TABLE IV ‣ 6.2 Plane-Sweep Volume representations ‣ 6 Learning multiview stereo
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation") 提供了一个分类，并比较了
    $13$ 种最先进的 MVS 技术。
- en: 6.1 Volumetric representations
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 体积表示
- en: 'One of the main issues for MVS reconstruction is how to match, in an efficient
    way, features across multiple images. Pairwise stereo methods rectify the images
    so that the search for correspondences is restricted to the horizontal epipolar
    lines. This is not possible with MVS due to the large view angle differences between
    the images. This has bee addressed using volumetric representations of the scene
    geometry [[116](#bib.bib116), [60](#bib.bib60)]. Depth maps are then generated
    by projection from the desired viewpoint. For a given input image, with known
    camera parameters, a ray from the viewpoint is cast through each image pixel.
    The voxels intersected by that ray are assigned the color [[116](#bib.bib116)]
    or the learned feature [[60](#bib.bib60)] of that pixel. Existing methods differ
    in the way information from multiple views are fused:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: MVS重建的主要问题之一是如何高效地匹配多幅图像中的特征。配对立体方法对图像进行校正，以便在水平视差线中限制对应点的搜索。但由于图像之间的视角差异较大，这在MVS中是不可行的。这一问题通过使用场景几何的体积表示得到解决
    [[116](#bib.bib116), [60](#bib.bib60)]。然后通过从所需视点的投影生成深度图。对于给定的输入图像，已知相机参数时，从视点发射一条光线穿过每个图像像素。与该光线相交的体素被赋予该像素的颜色
    [[116](#bib.bib116)] 或学习到的特征 [[60](#bib.bib60)]。现有方法在融合多视图信息的方式上存在差异：
- en: (1) Fusing feature grids. Kar *et al.* [[60](#bib.bib60)] (Fig. [6](#S6.F6 "Figure
    6 ‣ 6 Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(c)) fuse, recursively, the back-projected 3D feature grids
    using a recurrent neural network (RNN). The produced 3D grid is regularized using
    an encoder-decoder. To avoid dependency on the order of the images, Kar *et al.* [[60](#bib.bib60)]
    randomly permute the input images during training while constraining the output
    to be the same.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 融合特征网格。Kar *等人* [[60](#bib.bib60)] (图 [6](#S6.F6 "Figure 6 ‣ 6 Learning
    multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(c)) 使用递归神经网络 (RNN) 递归地融合反投影的 3D 特征网格。产生的 3D 网格通过编码器-解码器进行正则化。为了避免依赖图像的顺序，Kar
    *等人* [[60](#bib.bib60)] 在训练过程中随机排列输入图像，同时约束输出保持不变。
- en: (2) Fusing pairwise cost volumes. Choi *et al.* [[117](#bib.bib117)] fuse the
    cost volumes, computed from each pair of images, using a weighted sum where the
    weight of each volume is the confidence map computed from that cost volume.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 配对代价体积的融合。Choi *等人* [[117](#bib.bib117)] 使用加权和的方式融合从每对图像计算的代价体积，其中每个体积的权重是从该代价体积计算出的置信度图。
- en: (3) Fusing the reconstructed surfaces. Ji *et al.* [[116](#bib.bib116)] process
    each pair of volumetric grids using a 3D CNN, which classifies whether a voxel
    is a surface point or not. To avoid the exhaustive combination of every possible
    image pairs, Ji *et al.* [[116](#bib.bib116)] learn their relative importance,
    using a network composed of fully-connected layers, automatically select a few
    view pairs based on their relative importance to reconstruct multiple volumetric
    grids, and take their weighted sum to produce the final 3D reconstruction.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 融合重建的表面。Ji *等人* [[116](#bib.bib116)] 使用3D卷积神经网络（CNN）处理每对体积网格，该网络对体素是否为表面点进行分类。为了避免对每一对可能的图像进行穷举组合，Ji
    *等人* [[116](#bib.bib116)] 学习它们的相对重要性，使用由全连接层组成的网络，自动选择少量视图对来重建多个体积网格，并对其加权求和以生成最终的3D重建。
- en: To handle high resolution volumetric grids, Ji *et al.* [[116](#bib.bib116)]
    split the whole space into small Colored Voxel Cubes (CVCs) and regress the surface
    cube-by-cube. While this reduces the memory requirements, it requires multiple
    forward passes and thus increases the computation time. Paschalidou *et al.* [[91](#bib.bib91)]
    avoid the explicit use of the volumetric representation. Instead, each voxel of
    the grid is projected onto each of the input views, before computing the pairwise
    correlation between the corresponding learned features on each pair of views,
    and then averaging them over all pairs of views. Repeating this process for each
    depth value will result in the depth distribution on each pixel. This depth distribution
    is regularized using an MRF formulated as a differentiable function to enable
    end-to-end training.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理高分辨率的体积网格，Ji *等人* [[116](#bib.bib116)] 将整个空间分割成小的彩色体素立方体（CVCs），并逐立方体回归表面。虽然这减少了内存需求，但需要多次前向传递，从而增加了计算时间。Paschalidou
    *等人* [[91](#bib.bib91)] 避免显式使用体积表示。相反，网格的每个体素被投影到每个输入视图上，然后计算每对视图上对应学习特征之间的配对相关性，并对所有视图对的相关性进行平均。对每个深度值重复此过程将得到每个像素的深度分布。此深度分布通过一个以差分形式构建的MRF进行正则化，以实现端到端训练。
- en: In terms of performance, the volumetric approach of Ji *et al.* [[116](#bib.bib116)]
    requires $4$ hours to obtain a full reconstruction of a typical scene in DTU dataset [[24](#bib.bib24)].
    The approach of Paschalidou *et al.* [[91](#bib.bib91)] takes approximately $25$mins,
    on an Intel i7 computer with an Nvidia GTX Titan X GPU, for the same task. Finally,
    methods that perform fusion post-reconstruction have higher reconstruction errors
    compared to those that perform early fusion.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，Ji *等人* [[116](#bib.bib116)] 的体积方法需要 $4$ 小时才能获得DTU数据集中典型场景的完整重建。Paschalidou
    *等人* [[91](#bib.bib91)] 的方法在带有Nvidia GTX Titan X GPU的Intel i7计算机上完成相同任务大约需要 $25$
    分钟。最后，进行重建后融合的方法相比于早期融合的方法具有更高的重建误差。
- en: 6.2 Plane-Sweep Volume representations
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 平面扫掠体积表示
- en: These methods directly estimate depth maps from the input without using intermediate
    volumetric representations of the 3D geometry. As such, they are computationally
    more efficient. The main challenge to address is how to efficiently match features
    across multiple views in a single forward pass. This is done by using the Plane-Sweep
    Volumes (PSV) [[66](#bib.bib66), [27](#bib.bib27), [118](#bib.bib118), [93](#bib.bib93),
    [119](#bib.bib119), [90](#bib.bib90)], *i.e.,* they back project the input images [[66](#bib.bib66),
    [27](#bib.bib27), [118](#bib.bib118)] or their learned features [[93](#bib.bib93),
    [119](#bib.bib119), [90](#bib.bib90)] into planes at different depth values, forming
    PSVs from which the depth map is estimated. Existing methods differ in the way
    the PSVs are processed with the feature extraction and feature matching blocks.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法直接从输入中估计深度图，而不使用3D几何的中间体积表示。因此，它们在计算上更为高效。主要挑战是如何在单次前向传递中高效匹配多个视图中的特征。这是通过使用平面扫掠体积（PSV）[[66](#bib.bib66),
    [27](#bib.bib27), [118](#bib.bib118), [93](#bib.bib93), [119](#bib.bib119), [90](#bib.bib90)]
    实现的，即，它们将输入图像[[66](#bib.bib66), [27](#bib.bib27), [118](#bib.bib118)] 或其学习特征[[93](#bib.bib93),
    [119](#bib.bib119), [90](#bib.bib90)] 反投影到不同深度值的平面上，形成PSVs，从中估计深度图。现有的方法在特征提取和特征匹配模块处理中对PSVs的处理方式有所不同。
- en: Flynn *et al.*’s network [[66](#bib.bib66)] (Fig. [6](#S6.F6 "Figure 6 ‣ 6 Learning
    multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation")-(b)) is composed of $n_{d}$ branches, one for each depth plane. The
    $0pt-$th branch of the network takes as input the reference image and the planes
    of the PSVs of the other images which are located at depth $0pt$. These are packed
    together and fed to a two-stage network. The first stage computes matching features
    between the reference image and all the PSV planes located at depth $0pt$. The
    second stage models interactions across depth planes using convolutional layers.
    The final block of the network is a per-pixel softmax over depth, which returns
    the most probable depth value per pixel. The approach requires that the number
    of views and the camera parameters of each view to be known.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Flynn *等人*的网络[[66](#bib.bib66)]（图 [6](#S6.F6 "Figure 6 ‣ 6 Learning multiview
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(b)）由
    $n_{d}$ 个分支组成，每个深度平面一个。网络的 $0pt-$ 分支将参考图像和位于深度 $0pt$ 的其他图像的PSV平面作为输入。这些数据被打包在一起并输入到一个两阶段网络中。第一阶段计算参考图像与位于深度
    $0pt$ 的所有PSV平面之间的匹配特征。第二阶段使用卷积层建模深度平面之间的交互。网络的最终块是一个逐像素的 softmax 深度预测，返回每个像素最可能的深度值。该方法要求已知视图数量和每个视图的相机参数。
- en: Huang *et al.* [[27](#bib.bib27)]’s approach (Fig. [6](#S6.F6 "Figure 6 ‣ 6
    Learning multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(d)) starts with a pairwise matching step where a cost volume
    is computed between the reference image and each of the input images. For a given
    pair $(I_{1},I_{i}),i=2,\dots,n$, $I_{i}$ is first back-projected into a PSV.
    A siamese network then computes a matching cost volume between $I_{1}$ and each
    of the PSV planes. These volumes are aggregated into a single cost volume using
    an encoder-decoder network. This is referred to as intra-volume aggregation. Finally
    a max-pooling layer is used to aggregate the multi intra-volumes into a single
    inter-volume, which is then used to predict the depth map. Unlike Flynn *et al.* [[66](#bib.bib66)],
    Huang *et al.* [[27](#bib.bib27)]’s approach does not require a fixed number of
    input views since aggregation is performed using pooling. In fact, the number
    of views can vary between training and at runtime.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 黄*等人*[[27](#bib.bib27)]的方法（图 [6](#S6.F6 "Figure 6 ‣ 6 Learning multiview stereo
    ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(d)）从一对匹配步骤开始，在参考图像和每个输入图像之间计算成本体积。对于给定的对
    $(I_{1},I_{i}),i=2,\dots,n$，$I_{i}$ 首先被反投影到PSV中。然后，一个孪生网络计算 $I_{1}$ 和每个PSV平面之间的匹配成本体积。这些体积通过编码器-解码器网络聚合成一个单一的成本体积。这称为体内聚合。最后，使用最大池化层将多个体内体积聚合成一个单一的体间体积，然后用于预测深度图。与
    Flynn *等人*[[66](#bib.bib66)]的方法不同，黄*等人*[[27](#bib.bib27)]的方法不需要固定数量的输入视图，因为聚合是通过池化进行的。事实上，视图的数量在训练和运行时可以有所不同。
- en: Unlike [[66](#bib.bib66), [27](#bib.bib27)], which back-project the input images,
    the MVSNet of Yao *et al.* [[93](#bib.bib93)] use the camera parameters to back-project
    the learned features into a 3D frustum of a reference camera sliced into parallel
    frontal planes, one for each depth value. The approach then generates the matching
    cost volume upon a pixel-wise variance-based metric, and finally a generic 3D
    U-Net is used to regularize the matching cost volume to estimate the depth maps.
    Luo *et al.* [[119](#bib.bib119)] extend MVSNet [[93](#bib.bib93)] to P-MVSNet
    in two ways. First, a raw cost volume is processed with a learnable patch-wise
    aggregation function before feeding it to the regularization network. This improves
    the matching robustness and accuracy for noisy data. Second, instead of using
    a generic 3D-UNet network for regularization, P-MVSNet uses a hybrid isotropic-anisotropic
    3D-UNet. The plane-sweep volumes are essentially anisotropic in depth and spatial
    directions, but they are often approximated by isotropic cost volumes, which could
    be detrimental. In fact, one can infer the corresponding depth map along the depth
    direction of the matching cost volume, but cannot get the same information along
    other directions. Luo *et al.* [[119](#bib.bib119)] exploit this fact, through
    the proposed hybrid 3D U-Net with isotropic and anisotropic 3D convolutions, to
    guide the regularization of matching confidence volume.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[66](#bib.bib66)、[27](#bib.bib27)]不同，Yao *et al.* [[93](#bib.bib93)]的MVSNet通过相机参数将学习到的特征反投影到参考相机的3D截锥体中，该截锥体被切分为平行的前面平面，每个深度值对应一个平面。然后，该方法基于像素级方差度量生成匹配成本体积，最后使用通用的3D
    U-Net对匹配成本体积进行正则化，以估计深度图。Luo *et al.* [[119](#bib.bib119)]将MVSNet [[93](#bib.bib93)]扩展为P-MVSNet，主要有两个方面。首先，在将原始成本体积输入到正则化网络之前，使用可学习的补丁级聚合函数处理成本体积，这提高了对噪声数据的匹配鲁棒性和准确性。其次，P-MVSNet使用混合各向同性-各向异性3D
    U-Net进行正则化，而不是使用通用的3D-UNet网络。平面扫描体积在深度和空间方向上本质上是各向异性的，但它们通常被各向同性成本体积所近似，这可能有害。实际上，人们可以沿匹配成本体积的深度方向推断相应的深度图，但不能在其他方向上获得相同的信息。Luo
    *et al.* [[119](#bib.bib119)]利用这一点，通过提出的混合3D U-Net（具有各向同性和各向异性3D卷积）来指导匹配置信度体积的正则化。
- en: The main advantage of using PSVs is that they eliminate the need to supply rectified
    images. In other words, the camera parameters are implicitly encoded. However,
    in order to compute the PSVs, the intrinsic and extrinsic camera parameters need
    to be either provided in advance or estimated using, for example, Structure-from-Motion
    techniques as in [[27](#bib.bib27)]. Also, these methods require setting in advance
    the disparity range and its discretisation. Moreover, they often result in a complex
    network architecture. Wang *et al.* [[120](#bib.bib120)] propose a light-weight
    architecture. It stacks together the reference image and the cost volume, computed
    using the absolute difference between the reference image and each other image
    but at different depth planes, and feeds them to an encoder-decoder network, with
    skip connections, to estimate the inverse depth at three different resolutions.
    Wang *et al.* [[120](#bib.bib120)] use a view selection rule, which selects the
    frames that have enough angle or translation difference and then use the selected
    frames to compute the cost volume.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PSVs的主要优势在于，它们消除了提供校正图像的需要。换句话说，相机参数是隐式编码的。然而，为了计算PSVs，必须预先提供或使用例如结构从运动技术[[27](#bib.bib27)]进行估计的内在和外在相机参数。此外，这些方法还需要预先设置视差范围及其离散化。而且，这些方法通常导致复杂的网络结构。Wang
    *et al.* [[120](#bib.bib120)]提出了一种轻量级架构。该架构将参考图像和使用参考图像与其他图像（在不同深度平面上）的绝对差计算的成本体积堆叠在一起，并将其输入到具有跳跃连接的编码器-解码器网络中，以在三种不同分辨率下估计逆深度。Wang
    *et al.* [[120](#bib.bib120)]使用视图选择规则，选择具有足够角度或平移差异的帧，然后使用这些选定的帧计算成本体积。
- en: Finally, note that feature back-projection has been also used by Won *et al.* [[30](#bib.bib30)]
    for omnidirectional depth estimation from a wide-baseline multi-view stereo setup.
    The approach uses spherical maps and spherical cost volumes.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要注意的是，Won *et al.* [[30](#bib.bib30)]也使用了特征反投影来进行全向深度估计，这是一种基于宽基线多视图立体设置的方法。该方法使用了球面映射和球面成本体积。
- en: 'TABLE IV: Taxonomy and comparison of $13$ deep learning-based MVS techniques.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 表IV：$13$种基于深度学习的MVS技术的分类与比较。
- en: '| Method | Year | Representation | Fusion | Training |  | Peformance on (DTU,
    SUN3D, ETH3D) |  | Complexity |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 表示 | 融合 | 训练 |  | 在（DTU，SUN3D，ETH3D）上的性能 |  | 复杂度 |'
- en: '|  | #images | Error ($mm$) | % $<1mm$ | % $<2mm$ |  | # Params | Memory |
    Complexity | Time (s) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | #图像数量 | 错误（$mm$） | % $<1mm$ | % $<2mm$ |  | 参数数量 | 内存 | 复杂性 | 时间（秒） |'
- en: '| Kar *et al.* [[60](#bib.bib60)] | 2017 | Volumetric | Recurrent fusion |
    Supervised |  | Variable | $-$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Kar *等* [[60](#bib.bib60)] | 2017 | 体积测量 | 循环融合 | 监督学习 |  | 可变 | $-$ | $-$
    | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
- en: '|  |  |  | of 3D feature grids |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 3D特征网格 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Hartmann *et al.* [[47](#bib.bib47)] | 2017 | Replace correlation by pooling
    | Supervised |  | $5$ | $(1.356,-,-)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Hartmann *等* [[47](#bib.bib47)] | 2017 | 用池化替代相关性 | 监督学习 |  | $5$ | $(1.356,-,-)$
    | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
- en: '|  |  |  |  |  |  | (can vary) |  |  |  |  |  |  |  |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  | （可变） |  |  |  |  |  |  |  |  |'
- en: '| Ji *et al.* [[116](#bib.bib116)] | 2017 | Volumetric | Reconstructed surfaces
    | Supervised |  | $5$ | $(0.745,-,-)$ | $69.95$ | $74.4$ |  | $-$ | $-$ | $-$
    | $4$ hrs |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Ji *等* [[116](#bib.bib116)] | 2017 | 体积测量 | 重建表面 | 监督学习 |  | $5$ | $(0.745,-,-)$
    | $69.95$ | $74.4$ |  | $-$ | $-$ | $-$ | $4$小时 |'
- en: '| Choi *et al.* [[117](#bib.bib117)] | 2018 | Volumetric | Pairwise cost volumes
    | Supervised |  | $5$ | $(0.6511,-,-)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Choi *等* [[117](#bib.bib117)] | 2018 | 体积测量 | 成对成本体积 | 监督学习 |  | $5$ | $(0.6511,-,-)$
    | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
- en: '| Huang *et al.* [[27](#bib.bib27)] | 2018 | PSV | Encoder-decoder for intra-volume,
    | Supervised |  | Variable | $(-,0.419,0.412)$ | $-$ | $-$ |  | $-$ | $-$ | $-$
    | $-$ |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Huang *等* [[27](#bib.bib27)] | 2018 | PSV | 用于体积内的编码器-解码器 | 监督学习 |  | 可变
    | $(-,0.419,0.412)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
- en: '|  |  |  | Max pooling for inter-volume |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 对体积间进行最大池化 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Leroy *et al.* [[118](#bib.bib118)] | 2018 | PSV | Depth fusion | Supervised
    |  | Variable | $(0.599,-,-)$ | $-$ | $-$ |  | $72$K | $-$ | $-$ | $-$ |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Leroy *等* [[118](#bib.bib118)] | 2018 | PSV | 深度融合 | 监督学习 |  | 可变 | $(0.599,-,-)$
    | $-$ | $-$ |  | $72$K | $-$ | $-$ | $-$ |'
- en: '| Paschalidou *et al.* [[91](#bib.bib91)] | 2018 | Depth-based | Avg. pooling
    over | Supervised |  | Variable | $(-,-,-)$ | $-$ | $-$ |  | $-$ | $7$GB | $-$
    | $25$ mins |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Paschalidou *等* [[91](#bib.bib91)] | 2018 | 基于深度 | 对特征进行平均池化 | 监督学习 |  |
    可变 | $(-,-,-)$ | $-$ | $-$ |  | $-$ | $7$GB | $-$ | $25$分钟 |'
- en: '|  |  |  | pairwsie correlations |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 成对相关性 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Yao *et al.* [[93](#bib.bib93)] | 2018 | PSV | Feature pooling by variance
    | Supervised |  | $5$ | $(0.462,0.397,0.470)$ | $75.69$ | $80.25$ |  | $363$K
    | $5.28$GB | $O(0pt\times 0pt\times n_{d})$ | $0.9$s |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Yao *等* [[93](#bib.bib93)] | 2018 | PSV | 按方差进行特征池化 | 监督学习 |  | $5$ | $(0.462,0.397,0.470)$
    | $75.69$ | $80.25$ |  | $363$K | $5.28$GB | $O(0pt\times 0pt\times n_{d})$ |
    $0.9$s |'
- en: '| Wang *et al.* [[120](#bib.bib120)] | 2018 | PSV and abs. | Concatenation
    of pairwise | Supervised |  | Variable | $(-,0.114,0.257)$ | $-$ | $-$ |  | $33.9$M
    for | $-$ | $-$ | $0.04$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Wang *等* [[120](#bib.bib120)] | 2018 | PSV 和绝对值 | 成对的级联 | 监督学习 |  | 可变 |
    $(-,0.114,0.257)$ | $-$ | $-$ |  | $33.9$M | $-$ | $-$ | $0.04$ |'
- en: '|  |  | difference | cost volumes and ref. image |  |  |  |  |  |  |  | $n_{d}=64$
    |  |  |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 差异 | 成本体积和参考图像 |  |  |  |  |  |  |  | $n_{d}=64$ |  |  |  |'
- en: '| Hou *et al.* [[115](#bib.bib115)] | 2019 | $-$ | Temporal fusion of | Supervised
    |  | Variable | $(-,\textbf{0.101},0.229)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ |
    $-$ |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Hou *等* [[115](#bib.bib115)] | 2019 | $-$ | 时间融合 | 监督学习 |  | 可变 | $(-,\textbf{0.101},0.229)$
    | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
- en: '|  |  |  | the latent rep. |  |  | (video sequence) |  |  |  |  |  |  |  |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 潜在表示 |  |  | （视频序列） |  |  |  |  |  |  |  |  |'
- en: '| Luo *et al.* [[119](#bib.bib119)] | 2019 | PSV | Feature pooling by variance
    | Supervised |  | Variable | $(0.406,-,-)$ | $-$ | $-$ |  | $-$ | $-$ | $-$ |
    $-$ |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Luo *等* [[119](#bib.bib119)] | 2019 | PSV | 按方差进行特征池化 | 监督学习 |  | 可变 | $(0.406,-,-)$
    | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
- en: '| Xue *et al.**et al.* [[90](#bib.bib90)] | 2019 | PSV | Cost volume | Supervised
    |  | $5$ | $(\textbf{0.398},-,-)$ | $80.02$ | $83.84$ |  | $571$K | $5.43$GB |
    $O(0pt\times 0pt\times n_{d})$ | $1.8$s |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Xue *等* [[90](#bib.bib90)] | 2019 | PSV | 成本体积 | 监督学习 |  | $5$ | $(\textbf{0.398},-,-)$
    | $80.02$ | $83.84$ |  | $571$K | $5.43$GB | $O(0pt\times 0pt\times n_{d})$ |
    $1.8$s |'
- en: '|  |  |  | pooling by variance |  |  | (can vary) |  |  |  |  |  |  |  |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 按方差进行池化 |  |  | （可变） |  |  |  |  |  |  |  |  |'
- en: '| Won *et al.* [[30](#bib.bib30)] | 2019 | Spherical PSV | Concatenation |
    Supervised |  | $-$ | $-$ | $-$ | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Won *等* [[30](#bib.bib30)] | 2019 | 球面PSV | 级联 | 监督学习 |  | $-$ | $-$ | $-$
    | $-$ |  | $-$ | $-$ | $-$ | $-$ |'
- en: 7 Training end-to-end stereo methods
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 个端到端立体视觉方法的训练
- en: The training process aims to find the network parameters $W$ that minimize a
    loss function $\mathcal{L}(W;\hat{D},\Theta)$ where $\hat{D}$ is the estimated
    disparity, and $\Theta$ are the supervisory cues. The loss function is defined
    as the sum of a data term $\mathcal{L}_{1}(\hat{D},\Theta,W)$, which measures
    the discrepancy between the ground-truth and the estimated disparity, and a regularization
    or smoothness term $\mathcal{L}_{2}(\hat{D},W)$, which imposes local or global
    constraints on the solution. The type of supervisory cues defines the degree of
    supervision (Section [7.1](#S7.SS1 "7.1 Supervision methods ‣ 7 Training end-to-end
    stereo methods ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")),
    which can be supervised with 3D groundtruth (Section [7.1.1](#S7.SS1.SSS1 "7.1.1
    3D supervision methods ‣ 7.1 Supervision methods ‣ 7 Training end-to-end stereo
    methods ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")),
    self-supervised using auxiliary cues (Section [7.1.2](#S7.SS1.SSS2 "7.1.2 Self-supervised
    methods ‣ 7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation")), or weakly supervised
    (Section [7.1.3](#S7.SS1.SSS3 "7.1.3 Weakly supervised methods ‣ 7.1 Supervision
    methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")). Some methods use additional cues, in the
    form of constraints on the solution, to boost the accuracy and performance (Section [7.2](#S7.SS2
    "7.2 Incorporating additional cues ‣ 7 Training end-to-end stereo methods ‣ A
    Survey on Deep Learning Techniques for Stereo-based Depth Estimation")). One of
    the main challenges of deep learning-based techniques is their ability to generalize
    to new domains. Section [7.3](#S7.SS3 "7.3 Domain adaptation and transfer learning
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") reviews methods that addressed this issue.
    Finally, Section [7.4](#S7.SS4 "7.4 Learning the network architecture ‣ 7 Training
    end-to-end stereo methods ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") reviews methods that learn network architectures.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程的目标是找到网络参数 $W$，以最小化损失函数 $\mathcal{L}(W;\hat{D},\Theta)$，其中 $\hat{D}$ 是估计的差异，$\Theta$
    是监督提示。损失函数定义为数据项 $\mathcal{L}_{1}(\hat{D},\Theta,W)$ 的和，该数据项测量真实值与估计差异之间的差距，以及正则化或平滑项
    $\mathcal{L}_{2}(\hat{D},W)$，它对解施加局部或全局约束。监督提示的类型定义了监督的程度（第 [7.1](#S7.SS1 "7.1
    监督方法 ‣ 7 训练端到端立体方法 ‣ 深度学习技术在基于立体的深度估计中的调查") 节），可以通过 3D 真实值进行监督（第 [7.1.1](#S7.SS1.SSS1
    "7.1.1 3D 监督方法 ‣ 7.1 监督方法 ‣ 7 训练端到端立体方法 ‣ 深度学习技术在基于立体的深度估计中的调查") 节），使用辅助提示进行自我监督（第
    [7.1.2](#S7.SS1.SSS2 "7.1.2 自我监督方法 ‣ 7.1 监督方法 ‣ 7 训练端到端立体方法 ‣ 深度学习技术在基于立体的深度估计中的调查")
    节），或弱监督（第 [7.1.3](#S7.SS1.SSS3 "7.1.3 弱监督方法 ‣ 7.1 监督方法 ‣ 7 训练端到端立体方法 ‣ 深度学习技术在基于立体的深度估计中的调查")
    节）。一些方法使用额外的提示，以约束解的形式，来提高准确性和性能（第 [7.2](#S7.SS2 "7.2 引入额外提示 ‣ 7 训练端到端立体方法 ‣ 深度学习技术在基于立体的深度估计中的调查")
    节）。深度学习技术的主要挑战之一是其在新领域的泛化能力。第 [7.3](#S7.SS3 "7.3 域适应与迁移学习 ‣ 7 训练端到端立体方法 ‣ 深度学习技术在基于立体的深度估计中的调查")
    节回顾了针对这一问题的方法。最后，第 [7.4](#S7.SS4 "7.4 学习网络架构 ‣ 7 训练端到端立体方法 ‣ 深度学习技术在基于立体的深度估计中的调查")
    节回顾了学习网络架构的方法。
- en: 7.1 Supervision methods
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 监督方法
- en: 7.1.1 3D supervision methods
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1 3D 监督方法
- en: 'Supervised methods are trained to minimise a loss function that measures the
    error between the ground truth disparity and the estimated disparity. It is of
    the form:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 监督方法的训练旨在最小化一个损失函数，该函数测量真实差异与估计差异之间的误差。其形式为：
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum C(x)H(C(x)-\epsilon)\mathcal{D}\left(\Phi(d_{x}),\Phi(\hat{d}_{x})\right),$
    |  | (6) |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{N}\sum C(x)H(C(x)-\epsilon)\mathcal{D}\left(\Phi(d_{x}),\Phi(\hat{d}_{x})\right),$
    |  | (6) |'
- en: 'where: $d_{x}$ a $\hat{d}_{x}$ are, respectively, the groundtruth and the estimated
    disparity at pixel $x$. $\mathcal{D}$ is a measure of distance, which can be the
    $L_{2}$, the $L_{1}$ [[61](#bib.bib61), [121](#bib.bib121), [62](#bib.bib62),
    [99](#bib.bib99)], the smooth $L_{1}$ [[64](#bib.bib64)], or the smooth $L_{1}$
    but approximated using the two-parameter robust function $\rho(\cdot)$ [[72](#bib.bib72),
    [122](#bib.bib122)]. $C(x)\in[0,1]$ is the confidence of the estimated disparity
    at $x$. Setting $C(x)=1$ and the threshold $\epsilon=0,\forall x$ is equivalent
    to ignoring the confidence map. $H(x)$ is the heavyside function, which is equal
    to $1$ if $x\geq 0$, and $0$ otherwise. $\Phi(\cdot)$ is either the identify or
    the log function. The latter avoids overfitting the network to large disparities.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：$d_{x}$ 和 $\hat{d}_{x}$ 分别是像素 $x$ 处的真实值和估计的视差。$\mathcal{D}$ 是一种距离度量，可以是 $L_{2}$，$L_{1}$
    [[61](#bib.bib61), [121](#bib.bib121), [62](#bib.bib62), [99](#bib.bib99)]，平滑
    $L_{1}$ [[64](#bib.bib64)]，或使用双参数鲁棒函数 $\rho(\cdot)$ 近似的平滑 $L_{1}$ [[72](#bib.bib72),
    [122](#bib.bib122)]。$C(x)\in[0,1]$ 是 $x$ 处估计视差的置信度。设置 $C(x)=1$ 和阈值 $\epsilon=0,\forall
    x$ 等同于忽略置信度图。$H(x)$ 是 Heaviside 函数，当 $x\geq 0$ 时等于 $1$，否则等于 $0$。$\Phi(\cdot)$
    可以是恒等函数或对数函数。后者避免了网络对大视差的过拟合。
- en: Some papers restrict the sum in Eqn. ([6](#S7.E6 "In 7.1.1 3D supervision methods
    ‣ 7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation")) to be over only
    the valid pixels or regions of interest, *e.g.,* foreground or visible pixels [[123](#bib.bib123)],
    to avoid outliers. Other, *e.g.,* Yao *et al.* [[93](#bib.bib93)], divide the
    loss into two parts, one over the initial disparity and the other one over the
    refined disparity. The overall loss is then defined as the weighted sum of the
    two losses.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文将方程（[6](#S7.E6 "In 7.1.1 3D supervision methods ‣ 7.1 Supervision methods
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")）中的和限制为仅在有效像素或感兴趣区域上，例如前景或可见像素 [[123](#bib.bib123)]，以避免离群值。其他，如
    Yao *et al.* [[93](#bib.bib93)]，将损失分为两部分，一部分是初始视差，另一部分是精细化视差。总体损失定义为两种损失的加权和。
- en: 7.1.2 Self-supervised methods
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2 自监督方法
- en: 'Self-supervised methods, originally used in optical flow estimation [[124](#bib.bib124),
    [125](#bib.bib125)], have been proposed as a possible solution in the absence
    of sufficient ground-truth training data. These methods mainly rely on image reconstruction
    losses, taking advantage of the projective geometry, and the spatial and temporal
    coherence when multiple images of the same scene are available. The rationale
    is that if the estimated disparity map is as close as possible to the ground truth,
    then the discrepancy between the reference image and any of the other images but
    unprojected using the estimated depth map onto the reference image, is also minimized.
    The general loss function is of the form:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督方法最初用于光流估计 [[124](#bib.bib124), [125](#bib.bib125)]，在缺乏足够真实训练数据的情况下被提出作为一种可能的解决方案。这些方法主要依赖于图像重建损失，利用投影几何学以及在多个图像可用时的空间和时间一致性。其原理是，如果估计的视差图尽可能接近真实值，则参考图像与其他任何图像（但未通过估计的深度图在参考图像上重新投影）之间的差异也被最小化。一般的损失函数形式为：
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\mathcal{D}\left(\Phi\left(I_{ref}\right)(x)-\Phi\left(\tilde{I}_{ref}\right)(x)\right),$
    |  | (7) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\mathcal{D}\left(\Phi\left(I_{ref}\right)(x)-\Phi\left(\tilde{I}_{ref}\right)(x)\right),$
    |  | (7) |'
- en: 'where $\tilde{I}_{ref}$, which is $I_{right}$ but unwarped onto $I_{ref}$ using
    the estimated disparity, and $\mathcal{D}$ is a measure of distance. The mapping
    function $\Phi$ can be:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{I}_{ref}$ 是 $I_{right}$，但使用估计的视差未矫正到 $I_{ref}$ 上，而 $\mathcal{D}$
    是一种距离度量。映射函数 $\Phi$ 可以是：
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The identity [[126](#bib.bib126), [127](#bib.bib127), [70](#bib.bib70), [68](#bib.bib68)].
    In this case, the loss of Eqn. ([7](#S7.E7 "In 7.1.2 Self-supervised methods ‣
    7.1 Supervision methods ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation")) is called a photometric
    or image reconstruction loss.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 身份[[126](#bib.bib126), [127](#bib.bib127), [70](#bib.bib70), [68](#bib.bib68)]。在这种情况下，方程（[7](#S7.E7
    "In 7.1.2 Self-supervised methods ‣ 7.1 Supervision methods ‣ 7 Training end-to-end
    stereo methods ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")）的损失称为光度或图像重建损失。
- en: •
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A mapping to the feature space [[68](#bib.bib68)], *i.e.,* $\Phi\left(I_{ref}\right)=\textbf{f}$
    where f is the learned feature map.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 映射到特征空间 [[68](#bib.bib68)]，*即*，$\Phi\left(I_{ref}\right)=\textbf{f}$ 其中 f 是学习到的特征图。
- en: •
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The gradient of the image, *i.e.,* $\Phi\left(I_{ref}\right)=\nabla I_{ref}$,
    which is less sensitive to variations in lighting and acquisition conditions than
    the photometric loss.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像的梯度，*即*，$\Phi\left(I_{ref}\right)=\nabla I_{ref}$，相比光度损失，对光照和采集条件的变化更不敏感。
- en: The distance $\mathcal{D}$ can be the $L_{1}$ or $L_{2}$ distance. Some papers [[70](#bib.bib70)]
    also use more complex metrics such as the structural dissimilarity [[128](#bib.bib128)]
    between patches in $I_{ref}$ and in $\tilde{I}_{ref}$.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 距离$\mathcal{D}$可以是$L_{1}$或$L_{2}$距离。一些论文[[70](#bib.bib70)]也使用更复杂的度量，比如$I_{ref}$和$\tilde{I}_{ref}$之间的结构差异[[128](#bib.bib128)]。
- en: While stereo-based supervision methods do not require ground-truth 3D labels,
    they rely on the availability of calibrated stereo pairs during training.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于立体的监督方法不需要真实的3D标签，但它们依赖于训练期间可用的校准立体对。
- en: 7.1.3 Weakly supervised methods
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3 弱监督方法
- en: Supervised methods for disparity estimation can achieve promising results if
    trained on large quantities of ground truth depth data. However, manually obtaining
    ground-truth depth data is extremely difficult and expensive, and is prone to
    noise and inaccuracies. Weakly supervised methods rely on auxiliary signals to
    reduce the amount of manual labelling. In particular, Tonioni *et al.*[[129](#bib.bib129)]
    used as a supervisory signal the depth estimated using traditional stereo matching
    techniques to fine-tune depth estimation networks. Since such depth data can be
    sparse, noisy, and prone to errors, they propose a confidence-guided loss that
    penalizes ground-truth depth values that are deemed not reliable. It is defined
    using Eqn. ([6](#S7.E6 "In 7.1.1 3D supervision methods ‣ 7.1 Supervision methods
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")) by setting $\mathcal{D}(\cdot)$ to be the
    $L_{1}$ distance, and $\epsilon>0$. Kuznietsov *et al.* [[130](#bib.bib130)] use
    sparse ground-truth depth for supervised learning, while enforcing the deep network
    to produce photo-consistent dense depth maps in a stereo setup using a direct
    image alignment/reprojection loss. These two methods rely on an ad-hoc disparity
    estimator. To avoid that, Zhou *et al.* [[131](#bib.bib131)] propose an iterative
    approach, which starts with a randomly initialized network. At each iteration,
    it computes matches from the left to the right images, and matches from the right
    to the left images. It then selects the high confidence matches and adds them
    as labelled data for further training in the subsequent iterations. The confidence
    is computed using the left-right consistency of Eqn. ([12](#S7.E12 "In 7.2 Incorporating
    additional cues ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在大量的真实深度数据上训练，监督方法可以获得令人满意的结果。然而，手动获取真实深度数据是极其困难且昂贵的，并且容易受到噪声和不准确性的影响。弱监督方法依赖于辅助信号以减少手动标注的数量。特别是，Tonioni
    *et al.*[[129](#bib.bib129)]使用传统立体匹配技术估计的深度作为监督信号，来微调深度估计网络。由于这些深度数据可能稀疏、嘈杂且容易出错，他们提出了一种置信度引导的损失，惩罚那些被认为不可靠的真实深度值。它通过将$\mathcal{D}(\cdot)$设置为$L_{1}$距离，以及$\epsilon>0$来定义（见Eqn. ([6](#S7.E6
    "In 7.1.1 3D supervision methods ‣ 7.1 Supervision methods ‣ 7 Training end-to-end
    stereo methods ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")）。Kuznietsov
    *et al.*[[130](#bib.bib130)]使用稀疏的真实深度进行监督学习，同时强制深度网络在立体设置中生成光度一致的密集深度图，使用直接图像对齐/重投影损失。这两种方法依赖于临时的视差估计器。为了避免这种情况，Zhou
    *et al.*[[131](#bib.bib131)]提出了一种迭代方法，该方法从随机初始化的网络开始。在每次迭代中，它计算从左到右图像的匹配，以及从右到左图像的匹配。然后，它选择高置信度的匹配，并将它们添加为标注数据，以供后续迭代中的进一步训练。置信度是使用Eqn. ([12](#S7.E12
    "In 7.2 Incorporating additional cues ‣ 7 Training end-to-end stereo methods ‣
    A Survey on Deep Learning Techniques for Stereo-based Depth Estimation"))中的左右一致性计算的。
- en: 7.2 Incorporating additional cues
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 引入额外线索
- en: Several works incorporate additional cues and constraints to improve the quality
    of the disparity estimates. Examples include smoothness [[70](#bib.bib70)], left-right
    consistency [[70](#bib.bib70)], maximum depth [[70](#bib.bib70)], and scale-invariant
    gradient loss [[121](#bib.bib121)]. Such cues can also be in the form of auxiliary
    information such as semantic cues used to guide the disparity estimation network.
    Below, we discuss a number of these works.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究结合了额外的提示和约束来提高视差估计的质量。例如，平滑性 [[70](#bib.bib70)]、左右一致性 [[70](#bib.bib70)]、最大深度
    [[70](#bib.bib70)] 和尺度不变梯度损失 [[121](#bib.bib121)]。这些提示也可以是辅助信息，如语义提示，用于指导视差估计网络。下面，我们将讨论一些这些工作。
- en: '(1) Smoothness. In general, one can assume that neighboring pixels have similar
    disparity values. Such smoothness constraint can be enforced by minimizing:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 平滑性。通常，可以假设相邻像素具有类似的视差值。这种平滑性约束可以通过最小化以下内容来强制：
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The absolute difference between the disparity predicted at $x$ and those predicted
    at each pixel $y$ within a certain predefined neighborhood $\mathcal{N}_{x}$ around
    $x$:'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测在 $x$ 处的视差与在 $x$ 周围某个预定义邻域 $\mathcal{N}_{x}$ 内每个像素 $y$ 处预测的视差之间的绝对差异：
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\sum_{y\in\mathcal{N}_{x}}&#124;d_{x}-d_{y}&#124;.$
    |  | (8) |'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\sum_{y\in\mathcal{N}_{x}}&#124;d_{x}-d_{y}&#124;.$
    |  | (8) |'
- en: Here, $N$ is the total number of pixels.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，$N$ 是像素的总数。
- en: •
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The magnitude of the first-order gradient $\nabla$ of the estimated disparity
    map [[68](#bib.bib68)]:'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 估计视差图的一级梯度的大小 $\nabla$ [[68](#bib.bib68)]：
- en: '|  | $\small{\mathcal{L}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}d_{x})+(\nabla_{v}d_{x})\right\},x=(u,v).}$
    |  | (9) |'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\small{\mathcal{L}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}d_{x})+(\nabla_{v}d_{x})\right\},x=(u,v).}$
    |  | (9) |'
- en: •
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The magnitude of the second-order gradient of the estimated disparity [[127](#bib.bib127),
    [132](#bib.bib132)]:'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 估计视差的二级梯度的大小 [[127](#bib.bib127), [132](#bib.bib132)]：
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}^{2}d_{x})^{2}+(\nabla_{v}^{2}d_{x})^{2}\right\}.$
    |  | (10) |'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}\left\{(\nabla_{u}^{2}d_{x})^{2}+(\nabla_{v}^{2}d_{x})^{2}\right\}.$
    |  | (10) |'
- en: •
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The second-order gradient of the estimated disparity map weighted by the image’s
    second-order gradients [[70](#bib.bib70)]:'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由图像的二级梯度加权的估计视差图的二级梯度 [[70](#bib.bib70)]：
- en: '|  | $\small{\mathcal{L}=\frac{1}{N}\sum_{x}\left\{&#124;\nabla_{u}^{2}d_{x}&#124;e^{-&#124;\nabla_{u}^{2}I(x)&#124;}+&#124;\nabla_{v}^{2}d_{x}&#124;e^{-&#124;\nabla_{v}^{2}I(x)&#124;}\right\}.}$
    |  | (11) |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{\mathcal{L}=\frac{1}{N}\sum_{x}\left\{&#124;\nabla_{u}^{2}d_{x}&#124;e^{-&#124;\nabla_{u}^{2}I(x)&#124;}+&#124;\nabla_{v}^{2}d_{x}&#124;e^{-&#124;\nabla_{v}^{2}I(x)&#124;}\right\}.}$
    |  | (11) |'
- en: '(2) Consistency. Zhong *et al.* [[70](#bib.bib70)] introduced the loop-consistency
    loss, which is constructed as follows. Consider the left image $I_{left}$ and
    the synthesized image $\tilde{I}_{left}$ obtained by warping the right image to
    the left image coordinate using the disparity map defined on the right image.
    A second synthesized left image $\tilde{\tilde{I}}_{left}$ can also be generated
    by warping the left image to the right image coordinates, by using the disparities
    at the left image, and then warping it back to the left image using the disparity
    at the right image. The three versions of the left image provide two constraints:
    $I_{left}=\tilde{I}_{left}$ and $I_{left}=\tilde{\tilde{I}}_{left}$, which can
    be used to regularize the disparity maps. Godard *et al.* [[133](#bib.bib133)]
    introduce the left-right consistency term, which is a linear approximation of
    the loop consistency. The loss attempts to make the left-view disparity map equal
    to the projected right-view disparity map. It is defined as:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 一致性。Zhong *et al.* [[70](#bib.bib70)] 引入了回环一致性损失，其构造如下。考虑左图 $I_{left}$ 和通过将右图扭曲到左图坐标系中获得的合成图像
    $\tilde{I}_{left}$，使用定义在右图上的视差图。第二张合成左图 $\tilde{\tilde{I}}_{left}$ 也可以通过将左图扭曲到右图坐标系中，使用左图的视差，然后再通过右图的视差扭曲回左图生成。左图的三个版本提供了两个约束：$I_{left}=\tilde{I}_{left}$
    和 $I_{left}=\tilde{\tilde{I}}_{left}$，可以用来规范化视差图。Godard *et al.* [[133](#bib.bib133)]
    引入了左右一致性项，这是回环一致性的线性近似。该损失尝试使左视图的视差图等于投影的右视图视差图。定义为：
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}&#124;d_{x}-\tilde{d}_{x}&#124;,$ |  |
    (12) |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}&#124;d_{x}-\tilde{d}_{x}&#124;,$ |  |
    (12) |'
- en: where $\tilde{d}$ is the disparity at the right image but reprojected onto the
    coordinates of the left image.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{d}$ 是右图的视差，但重新投影到左图的坐标上。
- en: '(3) Maximum-depth heuristic. There may be multiple warping functions that achieve
    a similar warping loss, especially for textureless areas. To provide strong regularization
    in these areas, Zhong *et al.* [[70](#bib.bib70)] use the Maximum-Depth Heuristic
    (MDH) [[134](#bib.bib134)] defined as the sum of all depths/disparities:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 最大深度启发式。可能存在多个形变函数实现类似的形变损失，特别是在无纹理区域。为了在这些区域提供强大的正则化，Zhong *et al.* [[70](#bib.bib70)]
    使用定义为所有深度/视差总和的最大深度启发式（MDH）[[134](#bib.bib134)]：
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}&#124;d_{x}&#124;.$ |  | (13) |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{N}\sum_{x}&#124;d_{x}&#124;.$ |  | (13) |'
- en: '(4) Scale-invariant gradient loss [[121](#bib.bib121)]. It is defined as:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 尺度不变梯度损失 [[121](#bib.bib121)]。它的定义为：
- en: '|  | $\mathcal{L}=\sum_{h\in A}\sum_{x}\&#124;g_{h}[D](x)-g_{h}[\hat{D}](x)\&#124;_{2},$
    |  | (14) |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\sum_{h\in A}\sum_{x}\&#124;g_{h}[D](x)-g_{h}[\hat{D}](x)\&#124;_{2},$
    |  | (14) |'
- en: where $A=\{1,2,4,8,16\}$, $x=(i,j)$, $f_{i,j}\equiv f(i,j)$, and
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A=\{1,2,4,8,16\}$，$x=(i,j)$，$f_{i,j}\equiv f(i,j)$，以及
- en: '|  | $\small{g_{h}[f](i,j)=\left(\frac{f_{i+h,j}-f_{i,j}}{&#124;f_{i+h,j}-f_{i,j}&#124;},\frac{f_{i,j+h}-f_{i,j}}{&#124;f_{i,j+h}-f_{i,j}&#124;}\right)^{\top}.}$
    |  | (15) |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{g_{h}[f](i,j)=\left(\frac{f_{i+h,j}-f_{i,j}}{&#124;f_{i+h,j}-f_{i,j}&#124;},\frac{f_{i,j+h}-f_{i,j}}{&#124;f_{i,j+h}-f_{i,j}&#124;}\right)^{\top}.}$
    |  | (15) |'
- en: This loss penalizes relative depth errors between neighbouring pixels. This
    loss stimulates the network to compare depth values within a local neighbourhood
    for each pixel. It emphasizes depth discontinuities, stimulates sharp edges, and
    increases smoothness within homogeneous regions.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失惩罚邻近像素之间的相对深度误差。它促使网络在每个像素的局部邻域内比较深度值，强调深度的不连续性，刺激锐利的边缘，并增加同质区域的平滑度。
- en: '(5) Incorporating semantic cues. Some papers incorporate additional cues such
    as normal [[135](#bib.bib135)], segmentation [[68](#bib.bib68)], and edge [[76](#bib.bib76)]
    maps, to guide the disparity estimation. These can be either provided at the outset,
    *e.g.,* estimated with a separate method as in [[76](#bib.bib76)], or estimated
    jointly with the disparity map. Qi *et al.* [[135](#bib.bib135)] propose a mechanism
    that uses the depth map to refine the quality of the normal estimates, and the
    normal map to refine the quality of the depth estimates. This is done using a
    two-stream network: a depth-to-normal network for normal map refinement using
    the initial depth estimates, and a normal-to-depth network for depth refinement
    using the estimated normal map.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 融入语义提示。一些论文融入额外的提示，如法线 [[135](#bib.bib135)]、分割 [[68](#bib.bib68)] 和边缘 [[76](#bib.bib76)]
    图，以指导视差估计。这些可以在开始时提供，例如，通过 [[76](#bib.bib76)] 中的单独方法估计，或与视差图共同估计。Qi *et al.* [[135](#bib.bib135)]
    提出了一个机制，该机制使用深度图来细化法线估计的质量，以及使用法线图来细化深度估计的质量。这个过程是通过一个双流网络完成的：一个用于使用初始深度估计细化法线图的深度到法线网络，另一个用于使用估计的法线图进行深度细化的法线到深度网络。
- en: Yang *et al.* [[68](#bib.bib68)] and Song *et al.* [[76](#bib.bib76)] incorporate
    semantics by stacking semantic maps (segmentation masks in the case of [[68](#bib.bib68)]
    and edge features in the case of [[76](#bib.bib76)]) with the 3D cost volume.
    Yang *et al.* [[68](#bib.bib68)] train jointly a disparity estimation network
    and a segmentation network by using a loss function defined as a weighted sum
    of the reconstruction error, a smoothness term, and a segmentation error. Song
    *et al.* [[76](#bib.bib76)] further incorporate edge cues in the edge-aware smoothness
    loss to penalize drastic depth changes in flat regions. Also, to allow for depth
    discontinuities at object boundaries, the edge-aware smoothness loss is defined
    based on the gradient map obtained from the edge detection sub-network, which
    is more semantically meaningful than the variation in raw pixel intensities.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: Yang *et al.* [[68](#bib.bib68)] 和 Song *et al.* [[76](#bib.bib76)] 通过将语义图（在
    [[68](#bib.bib68)] 的情况下为分割掩码，在 [[76](#bib.bib76)] 的情况下为边缘特征）与3D成本体积堆叠来整合语义。Yang
    *et al.* [[68](#bib.bib68)] 通过使用定义为重建误差、平滑项和分割误差的加权和的损失函数，联合训练视差估计网络和分割网络。Song
    *et al.* [[76](#bib.bib76)] 进一步将边缘提示整合到边缘感知平滑损失中，以惩罚平坦区域中的剧烈深度变化。此外，为了允许物体边界处的深度不连续性，边缘感知平滑损失是基于从边缘检测子网络获得的梯度图定义的，这比原始像素强度的变化更具语义意义。
- en: Wu *et al.* [[79](#bib.bib79)] introduced an approach that fuses multiscale
    4D cost volumes with semantic features obtained using a segmentation sub-network.
    The approach uses the features of the left and the right images as input to a
    semantic segmentation network similar to PSPNet [[136](#bib.bib136)]. Semantic
    features for each image are then obtained from the output of the classification
    layer of the segmentation network. A 4D semantic cost volume is obtained by concatenating
    each unary semantic feature with their corresponding unary from the opposite stereo
    image across each disparity level. Both the spatial pyramid cost volumes and the
    semantic cost volume are fed into a 3D multi-cost aggregation module, which aggregates
    them, using an encoder-decoder followed by a 3D feature fusion module, into a
    single 3D cost volume in a pairwise manner starting with the smallest volume.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 吴*等*[[79](#bib.bib79)]提出了一种将多尺度4D代价体积与通过分割子网络获得的语义特征融合的方法。该方法将左右图像的特征作为输入，送入类似于PSPNet的语义分割网络[[136](#bib.bib136)]。然后，从分割网络的分类层输出中获得每幅图像的语义特征。通过将每个单一语义特征与其对应的来自相反立体图像的单一特征在每个视差层次上进行拼接，获得一个4D语义代价体积。空间金字塔代价体积和语义代价体积都被送入一个3D多代价聚合模块，该模块将它们通过编码器-解码器和一个3D特征融合模块以对对方式聚合成一个单一的3D代价体积，从最小体积开始。
- en: In summary, appending semantic features to the cost volume improves the reconstruction
    of fine details, especially near object boundaries.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，将语义特征附加到代价体积上可以改善细节的重建，特别是在物体边界附近。
- en: 7.3 Domain adaptation and transfer learning
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 领域自适应与迁移学习
- en: Deep architectures for depth estimation are severely affected by the domain
    shift issue, which hinders their effectiveness when performing inference on images
    significantly diverse from those used during the training stage. This can be observed,
    for instance, when moving between indoor and outdoor environments, from synthetic
    to real data, see Fig. [7](#S7.F7 "Figure 7 ‣ 7.3 Domain adaptation and transfer
    learning ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation"), or between different outdoor/indoor environments,
    and when changing the camera model/parameters. As such, deep learning networks
    trained on one domain, *e.g.,* by using synthetic data, suffer when applied to
    another domain, *e.g.,* real data, resulting in blurry object boundaries and errors
    in ill-posed regions such as object occlusions, repeated patterns, and textureless
    regions. These are referred to as *generalization glitches* [[137](#bib.bib137)].
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 深度估计的深度架构受到领域偏移问题的严重影响，这在对训练阶段使用的数据显著不同的图像进行推理时会降低其有效性。例如，这种情况可以在从室内环境到室外环境、从合成数据到真实数据（见图[7](#S7.F7
    "图7 ‣ 7.3 领域自适应与迁移学习 ‣ 7 端到端立体方法的训练 ‣ 关于基于立体的深度估计的深度学习技术的调查")）或在不同的室外/室内环境之间、以及更换相机模型/参数时观察到。因此，在一个领域（例如，通过使用合成数据）上训练的深度学习网络，当应用到另一个领域（例如，真实数据）时，会遭遇*泛化问题*[[137](#bib.bib137)]，导致物体边界模糊以及在诸如物体遮挡、重复模式和无纹理区域等不良区域出现错误。
- en: 'Several strategies have been proposed to adress this domain bias issue. They
    can be classified into two categories: adaptation by fine-tuning (Section [7.3.1](#S7.SS3.SSS1
    "7.3.1 Adaptation by fine-tuning ‣ 7.3 Domain adaptation and transfer learning
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")) and adaptation by data transformation (Section [7.3.2](#S7.SS3.SSS2
    "7.3.2 Adaptation by data transformation ‣ 7.3 Domain adaptation and transfer
    learning ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation")). In both cases, the adaptation can be offline
    or online.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出几种策略来解决这一领域偏差问题。它们可以分为两类：通过微调的适应（第[7.3.1](#S7.SS3.SSS1 "7.3.1 通过微调的适应 ‣ 7.3
    领域自适应与迁移学习 ‣ 7 端到端立体方法的训练 ‣ 关于基于立体的深度估计的深度学习技术的调查")）和通过数据转换的适应（第[7.3.2](#S7.SS3.SSS2
    "7.3.2 通过数据转换的适应 ‣ 7.3 领域自适应与迁移学习 ‣ 7 端到端立体方法的训练 ‣ 关于基于立体的深度估计的深度学习技术的调查")）。在这两种情况下，适应可以是离线的或在线的。
- en: '![Refer to caption](img/9287320ef827e6144299fb36b265bbdd.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9287320ef827e6144299fb36b265bbdd.png)'
- en: 'Figure 7: Illustration of the domain gap between synthetic (left) and real
    (right) images. The left image is from the FlyingThings synthetic dataset [[22](#bib.bib22)].'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：合成图像（左）和真实图像（右）之间领域差距的示意图。左侧图像来自FlyingThings合成数据集[[22](#bib.bib22)]。
- en: 7.3.1 Adaptation by fine-tuning
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1 通过微调进行适应
- en: Methods in this category perform domain adaptation by first training a network
    on images from a certain domain, *e.g.,* synthetic images as in [[22](#bib.bib22)],
    and then fine-tuning it on images from a target domain. A major difficulty is
    to collect accurate ground-truth depth for stereo or multiview images from the
    target domain. Relying on active sensors (*e.g.,* LiDAR) to obtain such supervised
    labeled data is not feasible in practical applications. As such, recent works,
    *e.g.,*  [[129](#bib.bib129), [138](#bib.bib138), [137](#bib.bib137)] rely on
    off-the-shelf stereo algorithms to obtain ground-truth disparity/depth labels
    in an unsupervised manner, together with state-of-the-art confidence measures
    to ascertain the correctness of the measurements of the off-the-shelf stereo algorithms.
    The latter is used in [[129](#bib.bib129), [138](#bib.bib138)] to discriminate
    between reliable and unreliable disparity measurements, to select the former and
    fine tune a pre-trained model, *e.g.,* DispNet [[22](#bib.bib22)], using such
    smaller and sparse set of points as if they were ground-truth labels.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法通过首先在来自某一领域的图像上训练网络，例如，[[22](#bib.bib22)]中的合成图像，然后在来自目标领域的图像上进行微调来实现领域适应。一个主要的困难是收集目标领域的立体或多视图图像的准确真实深度。依靠主动传感器，例如，LiDAR来获取这种监督标记数据在实际应用中是不可行的。因此，最近的工作，例如，[[129](#bib.bib129),
    [138](#bib.bib138), [137](#bib.bib137)] 依赖现成的立体算法以无监督的方式获取真实视差/深度标签，并结合最先进的置信度测量来验证现成立体算法的测量正确性。后者在[[129](#bib.bib129),
    [138](#bib.bib138)]中用于区分可靠和不可靠的视差测量，以选择前者并微调预训练模型，例如，DispNet [[22](#bib.bib22)]，使用这些较小且稀疏的点集，仿佛它们是实际标签。
- en: Pang *et al.* [[137](#bib.bib137)] also use a similar approach as in [[129](#bib.bib129),
    [138](#bib.bib138)] to address the generalization glitches. The approach, however,
    exploits the scale diversity, *i.e.,* up-sampling the stereo pairs enables the
    model to perform stereo matching in a localized manner with subpixel accuracy,
    by performing iterative optimisation of predictions obtained at multiple resolutions
    of the input.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: Pang *et al.* [[137](#bib.bib137)] 还使用类似于[[129](#bib.bib129), [138](#bib.bib138)]的方法来解决泛化问题。然而，这种方法利用了尺度多样性，即，通过对立体对进行上采样，使模型能够以亚像素精度在局部范围内执行立体匹配，通过对在输入的多个分辨率下获得的预测进行迭代优化。
- en: Note that self-supervised and weakly supervised techniques for disparity estimation,
    *e.g.,*  [[139](#bib.bib139), [133](#bib.bib133), [140](#bib.bib140), [141](#bib.bib141)]
    can also be used for offline domain adaptation. In particular, if stereo pairs
    of the target domain are available, these techniques can be fine-tuned, in an
    unsupervised manner, using reprojection losses, see Sections [7.1.2](#S7.SS1.SSS2
    "7.1.2 Self-supervised methods ‣ 7.1 Supervision methods ‣ 7 Training end-to-end
    stereo methods ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    and [7.1.3](#S7.SS1.SSS3 "7.1.3 Weakly supervised methods ‣ 7.1 Supervision methods
    ‣ 7 Training end-to-end stereo methods ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation").
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，自监督和弱监督技术用于视差估计，例如，[[139](#bib.bib139), [133](#bib.bib133), [140](#bib.bib140),
    [141](#bib.bib141)] 也可以用于离线领域适应。特别是，如果目标领域的立体对可用，这些技术可以在无监督的方式下，通过重投影损失进行微调，请参见第[7.1.2节](#S7.SS1.SSS2
    "7.1.2 自监督方法 ‣ 7.1 监督方法 ‣ 7 端到端立体训练方法 ‣ 深度学习技术的综述")和第[7.1.3节](#S7.SS1.SSS3 "7.1.3
    弱监督方法 ‣ 7.1 监督方法 ‣ 7 端到端立体训练方法 ‣ 深度学习技术的综述")。
- en: Although effective, these offline adaptation techniques reduce the usability
    of the methods since users are required to train the models every time they are
    exposed to a new domain. As a result, several recent papers developed online adaptation
    techniques. For example, Tonioni *et al.* [[84](#bib.bib84)] address the domain
    shift issue by casting adaptation as a continuous learning process whereby a stereo
    network can evolve online based on the images gathered by the camera during its
    real deployment. This is achieved in an unsupervised manner by computing error
    signals on the current frames, updating the whole network by a single back-propagation
    iteration, and moving to the next pair of input frames. To keep a high enough
    frame rate, Tonioni *et al.* [[84](#bib.bib84)] propose a lightweight, fast, and
    modular architecture, called MADNet, which allows training sub-portions of the
    whole network independently from each other. This allows adapting disparity estimation
    networks to unseen environments without supervision at approximately $25$ fps,
    while achieving an accuracy comparable to DispNetC [[22](#bib.bib22)]. Similarly,
    Zhong *et al.* [[142](#bib.bib142)] use video sequences to train a deep network
    online from a random initialization. They employ an LSTM in their model to leverage
    the temporal information during the prediction.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有效，这些离线适应技术降低了方法的可用性，因为用户需要每次面对新领域时都重新训练模型。因此，最近的几篇论文开发了在线适应技术。例如，托尼奥尼*等人*
    [[84](#bib.bib84)] 通过将适应视为一个持续学习过程来解决领域转移问题，从而使得立体网络能够基于在真实部署期间通过相机收集的图像进行在线演变。这是在无监督的方式下通过计算当前帧上的误差信号，单次反向传播迭代更新整个网络，然后转到下一对输入帧来实现的。为了保持足够高的帧率，托尼奥尼*等人*
    [[84](#bib.bib84)] 提出了一个轻量级、快速且模块化的架构，称为MADNet，这使得可以独立训练整个网络的子部分。这允许在约$25$ fps的速度下在无监督的情况下将视差估计网络适应到未见过的环境，同时达到与DispNetC
    [[22](#bib.bib22)] 相当的准确性。同样，钟*等人* [[142](#bib.bib142)] 使用视频序列从随机初始化中在线训练深度网络。他们在模型中使用LSTM来利用预测期间的时间信息。
- en: Zhong *et al.* [[142](#bib.bib142)] and Tonioni *et al.* [[84](#bib.bib84)]
    consider online adaptation separately from the initial training. Tonioni *et al.* [[143](#bib.bib143)],
    on the other hand, incorporate the adaptation procedure to the learning objective
    to obtain a set of initial parameters that are suitable for online adaptation,
    *i.e.,* they can be adapted quickly to unseen environments. This is implemented
    using the model agnostic meta-learning framework of [[144](#bib.bib144)], an explicit
    *learn-to-adapt* framework that enables stereo methods to adapt quickly and continuously
    to new target domains in an unsupervised manner.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 钟*等人* [[142](#bib.bib142)] 和托尼奥尼*等人* [[84](#bib.bib84)] 将在线适应与初始训练分开考虑。另一方面，托尼奥尼*等人*
    [[143](#bib.bib143)] 将适应过程纳入学习目标，以获得适合在线适应的一组初始参数，*即*，这些参数可以快速适应未见过的环境。这是通过[[144](#bib.bib144)]的模型无关元学习框架来实现的，这是一个明确的*学习以适应*框架，使立体方法能够以无监督的方式快速而持续地适应新的目标领域。
- en: 7.3.2 Adaptation by data transformation
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2 通过数据转换进行适应
- en: Methods in this category transform the data of one domain to look similar in
    style to the data of the other domain. For example, Atapour-Abarghoue *et al.* [[145](#bib.bib145)]
    proposed a two-staged approach. The first stage trains a depth estimation model
    using synthetic data. The second stage is trained to transfer the style of synthetic
    images to real-world images. By doing so, the style of real images is first transformed
    to match the style of synthetic data and then fed into the depth estimation network,
    which has been trained on synthetic data. Zheng *et al.* [[146](#bib.bib146)]
    perform the opposite by transforming the synthetic images to become more realistic
    and using them to train the depth estimation network. Zhao *et al.* [[147](#bib.bib147)]
    consider both synthetic-to-real [[146](#bib.bib146)] and real-to-synthetic [[148](#bib.bib148),
    [145](#bib.bib145)] translations. The two translators are trained in an adversarial
    manner using an adversarial loss and a cycle-consistency loss. That is, a synthetic
    image when converted to a real image and converted back to the synthetic domain
    should look similar to the original one.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法将一个领域的数据转换为与另一个领域的数据在风格上相似。例如，Atapour-Abarghoue *等人* [[145](#bib.bib145)]
    提出了一个两阶段的方法。第一阶段使用合成数据训练一个深度估计模型。第二阶段则训练将合成图像的风格转移到真实世界图像的能力。通过这种方式，首先将真实图像的风格转换为与合成数据的风格匹配，然后将其输入到已经在合成数据上训练的深度估计网络中。Zheng
    *等人* [[146](#bib.bib146)] 则采用相反的方法，将合成图像转换为更真实的图像，并使用这些图像训练深度估计网络。Zhao *等人* [[147](#bib.bib147)]
    考虑了合成到真实 [[146](#bib.bib146)] 和真实到合成 [[148](#bib.bib148), [145](#bib.bib145)]
    的翻译。两个转换器通过对抗损失和循环一致性损失以对抗的方式进行训练。也就是说，将合成图像转换为真实图像并再转换回合成领域时，应与原始图像相似。
- en: Although these methods have been used for monocular depth estimation, they are
    applicable to (multi-view) stereo matching methods.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法已用于单目深度估计，但它们同样适用于（多视角）立体匹配方法。
- en: 7.4 Learning the network architecture
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 学习网络架构
- en: Much research work in depth estimation is being spent on manually optimizing
    network architectures, but what about if the optimal network architecture, along
    with its parameters, could be also learnt from data? Saika *et al.* [[149](#bib.bib149)]
    show how to use and extend existing AutoML techniques [[150](#bib.bib150)] to
    efficiently optimize large-scale U-Net-like encoder-decoder architectures for
    stereo-based depth estimation. Traditional AutoML techniques have extreme computational
    demand limiting their usage to small-scale classification tasks. Saika *et al.* [[149](#bib.bib149)]
    applies Differentiable Architecture Search (DARTs) [[151](#bib.bib151)] to encoder-decoder
    architectures. Its main idea is to have a large network that includes all architectural
    choices and to select the best parts of this network by optimization. This can
    be relaxed to a continuous optimization problem, which, together with the regular
    network training, leads to a bilevel optimization problem. Experiments conducted
    on DispNet of [[75](#bib.bib75)], an improved version of [[22](#bib.bib22)], show
    that the automatically optimized DispNet (AutoDispNet) yields better performance
    compared to the baseline DispNet of [[75](#bib.bib75)], with about the same number
    of parameters. The paper also shows that the benefits of automated optimization
    carry over to large stacked networks.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 深度估计的许多研究工作集中在手动优化网络架构上，但如果最优的网络架构及其参数也可以通过数据学习得到呢？Saika *等人* [[149](#bib.bib149)]
    展示了如何使用和扩展现有的 AutoML 技术 [[150](#bib.bib150)] 来有效优化大规模 U-Net 类的编码器-解码器架构，用于基于立体的深度估计。传统的
    AutoML 技术具有极高的计算需求，限制了它们在小规模分类任务中的应用。Saika *等人* [[149](#bib.bib149)] 将可微分架构搜索（DARTs）
    [[151](#bib.bib151)] 应用于编码器-解码器架构。其主要思想是拥有一个包含所有架构选择的大型网络，并通过优化选择该网络的最佳部分。这可以放松为一个连续优化问题，这与常规的网络训练一起，形成一个双层优化问题。对
    [[75](#bib.bib75)] 的 DispNet（一种改进版本的 [[22](#bib.bib22)]）进行的实验表明，自动优化的 DispNet（AutoDispNet）相较于基线
    DispNet [[75](#bib.bib75)] 在性能上更优，参数数量大致相同。论文还显示，自动优化的好处也扩展到大型堆叠网络。
- en: 8 Discussion and comparison
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论与比较
- en: Tables [III](#S5.T3 "TABLE III ‣ 5.1 Feature learning ‣ 5 End-to-end depth from
    stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    and [IV](#S6.T4 "TABLE IV ‣ 6.2 Plane-Sweep Volume representations ‣ 6 Learning
    multiview stereo ‣ A Survey on Deep Learning Techniques for Stereo-based Depth
    Estimation"), respectively, compare the performance of the methods surveyed in
    this article on standard datasets such as KITTI2015 for pairwise stereo methods,
    and DTU, SUN3D and ETH3D for multiview stereo methods. Most of these methods have
    been trained on subsets of these publicly available datasets. A good disparity
    estimation method, once properly trained, should achieve good performance not
    only on publicly available benchmarks but on arbitrary novel images. They should
    not require re-training or fine-tuning every time the domain of usage changes.
    In this section, we will look at how some of these methods perform on novel unseen
    images. We will first describe in Section [8.1](#S8.SS1 "8.1 Evaluation protocol
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation") the evaluation protocol, the images that will be used, and
    the evaluation metrics. We then discuss the performance of these methods in Sections [8.2](#S8.SS2
    "8.2 Computation time and memory footprint ‣ 8 Discussion and comparison ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation") and [8.3](#S8.SS3
    "8.3 Reconstruction accuracy ‣ 8 Discussion and comparison ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation").
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [III](#S5.T3 "TABLE III ‣ 5.1 特征学习 ‣ 5 立体深度端到端 ‣ 深度学习技术在立体深度估计中的调查") 和 [IV](#S6.T4
    "TABLE IV ‣ 6.2 平面扫描体积表示 ‣ 6 多视角立体学习 ‣ 深度学习技术在立体深度估计中的调查") 分别比较了本文调查的方法在标准数据集上的表现，例如用于配对立体方法的KITTI2015，以及用于多视角立体方法的DTU、SUN3D和ETH3D。这些方法大多在这些公开数据集的子集上进行了训练。一种好的视差估计方法，一旦经过适当训练，应该能在公开基准和任意新图像上都能取得良好性能。它们不应在每次使用领域发生变化时都需要重新训练或微调。在本节中，我们将深入探讨这些方法在新颖未见图像上的表现。我们将首先在第
    [8.1](#S8.SS1 "8.1 评估协议 ‣ 8 讨论与比较 ‣ 深度学习技术在立体深度估计中的调查") 节描述评估协议、将使用的图像和评估指标。然后在第
    [8.2](#S8.SS2 "8.2 计算时间和内存占用 ‣ 8 讨论与比较 ‣ 深度学习技术在立体深度估计中的调查") 和第 [8.3](#S8.SS3
    "8.3 重建精度 ‣ 8 讨论与比较 ‣ 深度学习技术在立体深度估计中的调查") 节讨论这些方法的性能。
- en: 8.1 Evaluation protocol
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 评估协议
- en: 'We consider several key methods and evaluate their performance on the stereo
    subset of the ApolloScape dataset [[34](#bib.bib34)], and on an in-house collected
    set of four images. The motivation behind this choice is two-fold. First, the
    ApolloScape dataset is composed of stereo images taken outdoor in autonomous driving
    setups. Thus, it exhibits several challenges related to uncontrolled complex and
    varying lighting conditions, and heavy occlusions. Second, the dataset is novel
    and existing methods have not been trained or exposed to this dataset. Thus, it
    can be used to assess how these methods generalize to novel scenarios. In this
    dataset, ground truth disparities have been acquired by accumulating 3D point
    clouds from Lidar and fitting 3D CAD models to individually moving cars. We also
    use four in-house images of size $0pt=640$ and $0pt=480$, see Fig. [9](#S8.F9
    "Figure 9 ‣ 8.1 Evaluation protocol ‣ 8 Discussion and comparison ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation"), specifically designed
    to challenge these methods. Two of the images are of real scenes: a Bicycles scene
    composed of bicycles in a parking, and an indoor Desk scene composed of office
    furnitures. We use a moving stereo camera to capture multiple stereo pairs, and
    Structure-from-Motion (SfM) to build a 3D model of the scenes. We then render
    depth maps from the real cameras’ viewpoints. Regions where depth is estimated
    with high confidence will be used as ground-truth. The remaining two images are
    synthetic, but real-looking. They include objects with complex structures, *e.g.,*
    thin structures such as plants, large surfaces with either uniform colors or textures
    and repetitive patterns, presenting several challenges to stereo-based depth estimation
    algorithms.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了几种关键方法，并在ApolloScape数据集的立体子集 [[34](#bib.bib34)]及一组自采集的四张图像上评估了它们的性能。选择这两个数据集的原因有两个方面。首先，ApolloScape数据集由在自动驾驶环境中户外拍摄的立体图像组成。因此，它展示了与无法控制的复杂和变化的光照条件以及严重遮挡相关的多个挑战。其次，这个数据集是新的，现有方法尚未在该数据集上进行训练或测试。因此，它可以用来评估这些方法在新场景中的泛化能力。在此数据集中，真实差异通过从Lidar累积3D点云并将3D
    CAD模型拟合到单独移动的汽车上来获得。我们还使用了四张尺寸为$0pt=640$和$0pt=480$的自采集图像，参见图 [9](#S8.F9 "图 9 ‣
    8.1 评估协议 ‣ 8 讨论与比较 ‣ 深度学习技术在立体深度估计中的应用调研")，这些图像专门设计用于挑战这些方法。其中两张图像是实际场景：一个包含停车自行车的自行车场景和一个包含办公家具的室内办公桌场景。我们使用移动立体相机捕捉多个立体对，并使用运动结构（SfM）构建场景的3D模型。然后，我们从真实相机的视角渲染深度图。深度估计具有高置信度的区域将用作真实值。其余两张图像是合成的，但外观逼真。它们包含具有复杂结构的物体，如植物等细小结构、大面积的均匀颜色或纹理及重复图案的表面，这对基于立体的深度估计算法提出了多个挑战。
- en: '| ![Refer to caption](img/d4e5fa875ce77113e45aa19a75208fae.png) | ![Refer to
    caption](img/69fb50d6f40a3c5a9403bb02d43acb47.png) | ![Refer to caption](img/9a63a621259d32226f508ec3059b8804.png)
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/d4e5fa875ce77113e45aa19a75208fae.png) | ![参见说明](img/69fb50d6f40a3c5a9403bb02d43acb47.png)
    | ![参见说明](img/9a63a621259d32226f508ec3059b8804.png) |'
- en: '| (a) Baseline: images with good lighting conditions. |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| (a) 基准：具有良好光照条件的图像。 |'
- en: '| ![Refer to caption](img/40b2b768f198eea2f28d2c8053c35323.png) | ![Refer to
    caption](img/f8eec6998abbc0e36d1baccff1fe0c4e.png) | ![Refer to caption](img/3e1085666e93c19f7f486b70d5c8398c.png)
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/40b2b768f198eea2f28d2c8053c35323.png) | ![参见说明](img/f8eec6998abbc0e36d1baccff1fe0c4e.png)
    | ![参见说明](img/3e1085666e93c19f7f486b70d5c8398c.png) |'
- en: '| (b) Challenge: images with challenging lighting conditions. |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| (b) 挑战：具有挑战性光照条件的图像。 |'
- en: 'Figure 8: Examples of stereo pairs and their ground-truth disparity maps from
    the ApolloScape dataset [[34](#bib.bib34)].'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：来自ApolloScape数据集的立体对及其真实差异图的示例 [[34](#bib.bib34)]。
- en: '| ![Refer to caption](img/b7681081c5af890f9d55a205db7d6642.png) | ![Refer to
    caption](img/61f108d235377a4c737baec5fe987e40.png) | ![Refer to caption](img/56528b0194b72940760e5fe83c12f49b.png)
    | ![Refer to caption](img/ce1c906a4c76a6940921b51dd1ce240c.png) |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/b7681081c5af890f9d55a205db7d6642.png) | ![参见说明](img/61f108d235377a4c737baec5fe987e40.png)
    | ![参见说明](img/56528b0194b72940760e5fe83c12f49b.png) | ![参见说明](img/ce1c906a4c76a6940921b51dd1ce240c.png)
    |'
- en: '| (a) Left image. |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| (a) 左侧图像。 |'
- en: '| ![Refer to caption](img/3251304de12bead33aabaaedc70c6a47.png) | ![Refer to
    caption](img/430afd96d07e6eef22b75c1fe2c03df2.png) | ![Refer to caption](img/de0e8acc261202f6fb59497bb9dd892e.png)
    | ![Refer to caption](img/dab16633f31bb813568d5b370e6c3106.png) |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3251304de12bead33aabaaedc70c6a47.png) | ![参见说明](img/430afd96d07e6eef22b75c1fe2c03df2.png)
    | ![参见说明](img/de0e8acc261202f6fb59497bb9dd892e.png) | ![参见说明](img/dab16633f31bb813568d5b370e6c3106.png)
    |'
- en: '| (b) Highlights of regions of interest where ground-truth disparity is |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| (b) 突出显示了地面真实视差可用的感兴趣区域 |'
- en: '| estimated with high confidence. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 以高置信度估计。 |'
- en: '| ![Refer to caption](img/0a93fafa45ddd6abea49e5897210c1a8.png) | ![Refer to
    caption](img/b4ab087f886eb30058f75a8140b34953.png) | ![Refer to caption](img/31b7633c422a5f48e795be9f06fb52a2.png)
    | ![Refer to caption](img/9c606a83f4766c7ccc7ab1be35b344e7.png) |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/0a93fafa45ddd6abea49e5897210c1a8.png) | ![参见说明](img/b4ab087f886eb30058f75a8140b34953.png)
    | ![参见说明](img/31b7633c422a5f48e795be9f06fb52a2.png) | ![参见说明](img/9c606a83f4766c7ccc7ab1be35b344e7.png)
    |'
- en: '| (c) Right image. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| (c) 右侧图像。 |'
- en: '| ![Refer to caption](img/4c748812459cd7227a45fbde7eb25495.png) | ![Refer to
    caption](img/200e1efd1a15a78a22deb4733035fdfb.png) | ![Refer to caption](img/d5214a179752895f29e47a0f5015dc82.png)
    | ![Refer to caption](img/ac84a30b96faae181c5efeb83299854d.png) |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/4c748812459cd7227a45fbde7eb25495.png) | ![参见说明](img/200e1efd1a15a78a22deb4733035fdfb.png)
    | ![参见说明](img/d5214a179752895f29e47a0f5015dc82.png) | ![参见说明](img/ac84a30b96faae181c5efeb83299854d.png)
    |'
- en: '| Disp. $\in[9.3,34.0]$ | Disp. $\in[18.7,29.9]$ | Disp. $\in[5.6,14.5]$ |
    Disp. $\in[5.5,13.2]$ |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 视差 $\in[9.3,34.0]$ | 视差 $\in[18.7,29.9]$ | 视差 $\in[5.6,14.5]$ | 视差 $\in[5.5,13.2]$
    |'
- en: '| Depth $\in[2.1,7.8]$ | Depth $\in[2.4,3.3]$ | Depth $\in[7.8,25.0]$ | Depth
    $\in[10.8,25.6]$ |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 深度 $\in[2.1,7.8]$ | 深度 $\in[2.4,3.3]$ | 深度 $\in[7.8,25.0]$ | 深度 $\in[10.8,25.6]$
    |'
- en: '| (d) Ground-truth disparity maps. |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| (d) 地面真实视差图。 |'
- en: 'Figure 9: Four images, collected in-house and used to test $16$ state-of-the-art
    methods. The green masks on some of the left images highlight the pixels where
    the ground-truth disparity is available. The disparity range is shown in pixels
    while the depth range is in meters.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '图9: 四张内部收集的图像，用于测试$16$种最先进的方法。某些左侧图像上的绿色掩膜突出显示了地面真实视差可用的像素。视差范围以像素显示，深度范围以米为单位。'
- en: We have tested $16$ stereo-based methods published in $9$ papers (between $2018$
    and $2019$), see below. We use the network weights as provided by the authors.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了$16$种立体视觉方法，发布于$9$篇论文（2018至2019年之间），见下文。我们使用了作者提供的网络权重。
- en: '(1) AnyNet [[88](#bib.bib88)]: It is a four-stages network, which builds 3D
    cost volumes in a coarse-to-fine manner. The first stage estimates a low resolution
    disparity map by searching on a small disparity range. The subsequent stages estimate
    refined disparity maps using residual learning.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '(1) AnyNet [[88](#bib.bib88)]: 这是一个四阶段的网络，采用粗到细的方式构建3D代价体积。第一阶段通过在较小的视差范围内搜索来估计低分辨率的视差图。随后的阶段使用残差学习来估计精细化的视差图。'
- en: '(2) DeepPruner [[83](#bib.bib83)]: It combines deep learning with PatchMatch [[101](#bib.bib101)]
    to speed up inference by adaptively pruning out the potentially large search space
    for correspondences. Two variants have been proposed: DeepPruner (Best), which
    downsamples the cost volume by a factor of $4$, and DeepPruner (Fast), which downsamples
    it by a factor of $8$.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '(2) DeepPruner [[83](#bib.bib83)]: 它结合了深度学习和PatchMatch [[101](#bib.bib101)]，通过自适应地剪枝可能的大搜索空间来加快推理速度。提出了两个变体：DeepPruner
    (Best)，其代价体积下采样因子为$4$，以及DeepPruner (Fast)，其下采样因子为$8$。'
- en: (3) DispNet3 [[75](#bib.bib75)], an improved version of DispNet [[22](#bib.bib22)]
    where occlusions and disparity maps are jointly estimated.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: (3) DispNet3 [[75](#bib.bib75)]，是DispNet [[22](#bib.bib22)]的改进版，其中遮挡和视差图被联合估计。
- en: '(4) GANet [[85](#bib.bib85)]: It replaces a large number of the 3D convolutional
    layers in the regularization block with (1) two 3D convolutional layers, (2) a
    semi-global aggregation layer (SGA), and (3) a local guided aggregation layer
    (LGA). SGA and LGA layers capture local and whole-image cost dependencies. They
    are meant to improve the accuracy in challenging regions such as occlusions, large
    textureless/reflective regions, and thin structures.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '(4) GANet [[85](#bib.bib85)]: 它用（1）两个3D卷积层，（2）一个半全局聚合层（SGA）和（3）一个局部引导聚合层（LGA）替换了常规化块中的大量3D卷积层。SGA和LGA层捕捉局部和全图的代价依赖关系。它们旨在提高在遮挡、大的无纹理/反射区域和细结构等挑战性区域的准确性。'
- en: '(5) HighResNet [[32](#bib.bib32)]: To refine both the spatial and the depth
    resolutions while operating on high resolution images, this method searches for
    correspondences incrementally using a coarse-to-fine hierarchy. Its hierarchical
    design also allows for anytime on-demand reports of disparity.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '(5) HighResNet [[32](#bib.bib32)]: 为了在处理高分辨率图像时同时提升空间和深度分辨率，该方法使用粗到细的层级结构逐步寻找对应关系。其层级设计还允许随时按需生成视差报告。'
- en: '(6) PSMNet [[64](#bib.bib64)]: It progressively regularizes a low resolution
    4D cost volume, estimated from a pyramid of features.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '(6) PSMNet [[64](#bib.bib64)]: 它逐步规范化低分辨率的 4D 成本体积，该体积是从特征金字塔中估计得到的。'
- en: '(7) iResNet [[63](#bib.bib63)]: The initial disparity and the learned features
    are used to calculate a feature constancy map, which measures the correctness
    of the stereo matching. The initial disparity map and the feature constancy map
    are then fed into a sub-network for disparity refinement.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '(7) iResNet [[63](#bib.bib63)]: 初始视差和学习到的特征用于计算特征一致性图，该图测量立体匹配的正确性。初始视差图和特征一致性图随后被送入子网络进行视差细化。'
- en: '(8) UnsupAdpt [[129](#bib.bib129)]: It is an unsupervised adaptation approach
    that enables fine-tuning without any ground-truth information. It first trains
    DispNet-Corr1D [[22](#bib.bib22)] using the KITTI 2012 training dataset and then
    adapts the network to KITTI2015 and Middlebury 2014.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '(8) UnsupAdpt [[129](#bib.bib129)]: 这是一种无监督的适应方法，能够在没有真实信息的情况下进行微调。它首先使用 KITTI
    2012 训练数据集训练 DispNet-Corr1D [[22](#bib.bib22)]，然后将网络适应到 KITTI2015 和 Middlebury
    2014。'
- en: '(9) SegStereo [[68](#bib.bib68)]: It is an unsupervised disparity estimation
    method, which uses segmentation masks to guide the disparity estimation. Both
    segmentation and disparity maps are jointly estimated with an end-to-end network.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '(9) SegStereo [[68](#bib.bib68)]: 这是一种无监督的视差估计方法，它使用分割掩模来引导视差估计。分割图和视差图是通过端到端网络联合估计的。'
- en: 'The methods (1) to (7) are supervised with ground-truth depth maps while the
    methods (8) and (9) are self-supervised. We compare their accuracy at runtime
    using the overall Root Mean Square Error (RMSE) defined as:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 (1) 到 (7) 使用真实深度图进行监督，而方法 (8) 和 (9) 是自监督的。我们使用定义为的整体均方根误差 (RMSE) 比较它们的准确性：
- en: '|  | $\text{RMSE}^{2}_{\text{linear}}={\frac{1}{N}\sum_{N}{&#124;0pt_{i}-\hat{0}pt_{i}&#124;}^{2}},$
    |  | (16) |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{RMSE}^{2}_{\text{linear}}={\frac{1}{N}\sum_{N}{&#124;0pt_{i}-\hat{0}pt_{i}&#124;}^{2}},$
    |  | (16) |'
- en: and the Bad-n error defined as the percentage of pixels whose estimated disparity
    deviates with more than $n$ pixels from the ground truth. We use $n\in\{0.5,1,2,3,4,5\}$.
    The Bad-n error considers the distribution and spread of the error and thus provides
    a better insight on the accuracy of the methods. In addition to accuracy, we also
    report the computation time and memory footprint at runtime.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 以及定义为视差估计偏差超过 $n$ 像素的像素百分比的 Bad-n 错误。我们使用 $n\in\{0.5,1,2,3,4,5\}$。Bad-n 错误考虑了错误的分布和扩散，因此能更好地洞察方法的准确性。除了准确性，我们还报告了运行时的计算时间和内存占用。
- en: 8.2 Computation time and memory footprint
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 计算时间和内存占用
- en: 'TABLE V: Computation time and memory consumption, at runtime, on images of
    size $640\times 480$. SegStereo [[68](#bib.bib68)] has been tested on a PC equipped
    with an Nvidia GeForce RTX 2080\. The other methods have been tested on a PC equipped
    with an Nvidia Tesla K40 GPU with a 12 Go graphic memory. See the Supplementary
    Material for a visual representation.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 计算时间和内存消耗，在大小为 $640\times 480$ 的图像上运行。SegStereo [[68](#bib.bib68)] 在配备
    Nvidia GeForce RTX 2080 的 PC 上进行了测试。其他方法在配备 Nvidia Tesla K40 GPU 和 12 GB 显存的 PC
    上进行了测试。请参阅补充材料以获取视觉表示。'
- en: '| Method | Supervision | Cost vol. | Time (s) | Memory (GB) | Training set
    | Baseline |  | Challenge |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 监督 | 成本体积 | 时间 (秒) | 内存 (GB) | 训练集 | 基线 |  | 挑战 |'
- en: '| mode | Bkg | Fg | Bkg$+$Fg |  | Bkg | Fg | Bkg$+$Fg |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 背景 | 前景 | 背景$+$前景 |  | 背景 | 前景 | 背景$+$前景 |'
- en: '| AnyNet [[88](#bib.bib88)] | Supervised | 3D | $0.285$ | 0.232 | KITTI2015
    | 9.46 | 10.74 | 10.34 |  | 9.83 | 11.60 | 11.15 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| AnyNet [[88](#bib.bib88)] | 有监督 | 3D | $0.285$ | 0.232 | KITTI2015 | 9.46
    | 10.74 | 10.34 |  | 9.83 | 11.60 | 11.15 |'
- en: '|  |  |  |  |  | KITTI2012 | 9.80 | 10.29 | 10.20 |  | 9.34 | 10.62 | 10.61
    |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | KITTI2012 | 9.80 | 10.29 | 10.20 |  | 9.34 | 10.62 | 10.61
    |'
- en: '| DeepPruner (Best) [[83](#bib.bib83)] | Supervised | 3D | $8.430$ | $8.845$
    | KITTI2012$+$2015 | 9.64 | 9.43 | 9.46 |  | 12.38 | 8.74 | 10.48 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| DeepPruner (Best) [[83](#bib.bib83)] | 有监督 | 3D | $8.430$ | $8.845$ | KITTI2012$+$2015
    | 9.64 | 9.43 | 9.46 |  | 12.38 | 8.74 | 10.48 |'
- en: '| DeepPruner (Fast) [[83](#bib.bib83)] | Supervised | 3D | $3.930$ | $6.166$
    | KITTI2012$+$2015 | 9.56 | 9.90 | 9.94 |  | 8.74 | 9.75 | 9.86 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| DeepPruner (Fast) [[83](#bib.bib83)] | 有监督 | 3D | $3.930$ | $6.166$ | KITTI2012$+$2015
    | 9.56 | 9.90 | 9.94 |  | 8.74 | 9.75 | 9.86 |'
- en: '| DispNet3 [[75](#bib.bib75)] | Supervised | 3D | $-$ | $10.953$ | CSS-ft-KITTI
    | 9.68 | 9.62 | 9.70 |  | 8.38 | 11.00 | 11.11 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| DispNet3 [[75](#bib.bib75)] | 有监督 | 3D | $-$ | $10.953$ | CSS-ft-KITTI |
    9.68 | 9.62 | 9.70 |  | 8.38 | 11.00 | 11.11 |'
- en: '|  |  |  |  |  | CSS-FlyingThings3D [[22](#bib.bib22)] | 9.11 | 9.64 | 9.54
    |  | 8.97 | 9.91 | 10.19 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | CSS-FlyingThings3D [[22](#bib.bib22)] | 9.11 | 9.64 | 9.54
    |  | 8.97 | 9.91 | 10.19 |'
- en: '|  |  |  |  |  | css-FlyingThings3D [[22](#bib.bib22)] | 9.29 | 9.98 | 9.87
    |  | 9.66 | 10.34 | 10.61 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | css-FlyingThings3D [[22](#bib.bib22)] | 9.29 | 9.98 | 9.87
    |  | 9.66 | 10.34 | 10.61 |'
- en: '| GANet [[85](#bib.bib85)] | Supervised | 4D | $8.336$ | 3.017 | KITTI2015
    | 9.55 | 9.38 | 9.39 |  | 9.37 | 9.50 | 9.89 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| GANet [[85](#bib.bib85)] | 有监督 | 4D | $8.336$ | 3.017 | KITTI2015 | 9.55
    | 9.38 | 9.39 |  | 9.37 | 9.50 | 9.89 |'
- en: '|  |  |  |  |  | KITTI2012 | 9.98 | 10.29 | 10.25 |  | 10.69 | 10.95 | 11.55
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | KITTI2012 | 9.98 | 10.29 | 10.25 |  | 10.69 | 10.95 | 11.55
    |'
- en: '| HighResNet [[32](#bib.bib32)] | Supervised | 4D | 0.037 | $0.474$ | Middleburry [[20](#bib.bib20)],
    KITTI2015 [[21](#bib.bib21)], | 9.47 | 9.91 | 9.94 |  | 8.58 | 9.64 | 9.78 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| HighResNet [[32](#bib.bib32)] | 有监督 | 4D | 0.037 | $0.474$ | Middleburry [[20](#bib.bib20)],
    KITTI2015 [[21](#bib.bib21)], | 9.47 | 9.91 | 9.94 |  | 8.58 | 9.64 | 9.78 |'
- en: '|  |  |  |  |  | ETH3D [[25](#bib.bib25)], HR-VS [[32](#bib.bib32)] |  |  |  |  |  |  |  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | ETH3D [[25](#bib.bib25)], HR-VS [[32](#bib.bib32)] |  |  |  |  |  |  |  |'
- en: '| PSMNet [[64](#bib.bib64)] | Supervised | 4D | $1.314$ | $1.900$ | KITTI2015
    | 9.88 | 9.81 | 9.80 |  | 10.10 | 9.42 | 9.93 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| PSMNet [[64](#bib.bib64)] | 有监督 | 4D | $1.314$ | $1.900$ | KITTI2015 | 9.88
    | 9.81 | 9.80 |  | 10.10 | 9.42 | 9.93 |'
- en: '|  |  |  |  |  | KITTI2012 | 10.17 | 10.24 | 10.29 |  | 10.66 | 10.33 | 11.00
    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | KITTI2012 | 10.17 | 10.24 | 10.29 |  | 10.66 | 10.33 | 11.00
    |'
- en: '| iResNet [[63](#bib.bib63)] | Supervised | 3D | $0.939$ | $7.656$ | KITTI2015
    | 60.04 | 61.72 | 60.54 |  | 45.87 | 46.85 | 47.86 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| iResNet [[63](#bib.bib63)] | 有监督 | 3D | $0.939$ | $7.656$ | KITTI2015 | 60.04
    | 61.72 | 60.54 |  | 45.87 | 46.85 | 47.86 |'
- en: '|  |  |  |  |  | ROB [[152](#bib.bib152)] | 22.08 | 17.16 | 18.08 |  | 23.01
    | 16.51 | 18.83 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | ROB [[152](#bib.bib152)] | 22.08 | 17.16 | 18.08 |  | 23.01
    | 16.51 | 18.83 |'
- en: '| UnsupAdpt [[129](#bib.bib129)] | Self-supervised | 3D | $-$ | $-$ | KITTI2012
    adapted to KITTI2015 | 9.44 | 10.39 | 10.19 |  | 10.10 | 10.42 | 10.78 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| UnsupAdpt [[129](#bib.bib129)] | 自监督 | 3D | $-$ | $-$ | KITTI2012 调整至 KITTI2015
    | 9.44 | 10.39 | 10.19 |  | 10.10 | 10.42 | 10.78 |'
- en: '|  |  |  |  |  | Shadow-on-Truck | 8.52 | 10.08 | 9.58 |  | 10.66 | 10.88 |
    10.27 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | Shadow-on-Truck | 8.52 | 10.08 | 9.58 |  | 10.66 | 10.88 |
    10.27 |'
- en: '| SegStereo [[68](#bib.bib68)] | Self-supervised | 3D | $0.195$ | $\sim 12.00$
    | CityScapes [[23](#bib.bib23)] | 9.26 | 10.30 | 10.17 |  | 9.03 | 10.49 | 10.54
    |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| SegStereo [[68](#bib.bib68)] | 自监督 | 3D | $0.195$ | $\sim 12.00$ | CityScapes [[23](#bib.bib23)]
    | 9.26 | 10.30 | 10.17 |  | 9.03 | 10.49 | 10.54 |'
- en: 'From Table [V](#S8.T5 "TABLE V ‣ 8.2 Computation time and memory footprint
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation"), we can distinguish three types of methods; slow methods, *e.g.,*
    PSMNet [[64](#bib.bib64)], DeepPruner (Best) and (Fast) [[83](#bib.bib83)], and
    GANet [[85](#bib.bib85)], require more than $1$ second to estimate one disparity
    map. They also require between $3$GB and $10$GB (for DispNet3 [[75](#bib.bib75)])
    of memory at runtime. As such, these methods are very hard to deploy on mobile
    platforms. Average-speed methods, *e.g.,* AnyNet [[88](#bib.bib88)] and iResNet [[63](#bib.bib63)],
    produce a disparity map in around one second. Finally, fast methods, *e.g.,* HighResNet [[32](#bib.bib32)],
    require less than $0.1$ seconds. In general, methods that use 3D cost volumes
    are faster and less memory demanding than those that use 4D cost volumes. There
    are, however, two exceptions: iResNet [[63](#bib.bib63)] and DeepPruner [[83](#bib.bib83)],
    which use 3D cost volumes but require a large amount of memory at runtime. While
    iResNet requires less than a second to process images of size $0pt=640,0pt=480$,
    since it uses 2D convolutions to regularize the cost volume, DeepPruner [[83](#bib.bib83)]
    requires more than $3$ seconds. We also observe that HighResNet [[32](#bib.bib32)],
    which uses 4D cost volumes but adopts a hierarchical approach to produce disparity
    on demand, is very efficient in terms of computation time as it only requires
    $37$ms, which is almost $8$ times faster than AnyNet [[88](#bib.bib88)], which
    uses 3D cost volumes. Note also that AnyNet [[88](#bib.bib88)] can run on mobile
    devices due to its memory efficiency.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 [V](#S8.T5 "TABLE V ‣ 8.2 计算时间和内存占用 ‣ 8 讨论与比较 ‣ 深度学习技术在立体深度估计中的应用调查") 中，我们可以区分三种类型的方法：慢速方法，如
    PSMNet [[64](#bib.bib64)]、DeepPruner（最佳）和（快速）[[83](#bib.bib83)]，以及 GANet [[85](#bib.bib85)]，这些方法需要超过
    $1$ 秒来估计一个视差图。它们在运行时还需要 $3$GB 到 $10$GB（对于 DispNet3 [[75](#bib.bib75)]）的内存。因此，这些方法在移动平台上非常难以部署。中速方法，如
    AnyNet [[88](#bib.bib88)] 和 iResNet [[63](#bib.bib63)]，大约需要一秒钟来生成一个视差图。最后，快速方法，如
    HighResNet [[32](#bib.bib32)]，需要少于 $0.1$ 秒。一般来说，使用 3D 成本体积的方法比使用 4D 成本体积的方法更快，内存需求也更少。然而，有两个例外：iResNet
    [[63](#bib.bib63)] 和 DeepPruner [[83](#bib.bib83)]，这两种方法使用 3D 成本体积，但在运行时需要大量内存。虽然
    iResNet 处理 $640\times480$ 大小图像的时间不到一秒，因为它使用 2D 卷积来规整成本体积，但 DeepPruner [[83](#bib.bib83)]
    需要超过 $3$ 秒。我们还观察到，高分辨率网络 HighResNet [[32](#bib.bib32)] 使用 4D 成本体积，但采用分层方法按需生成视差，在计算时间方面非常高效，只需
    $37$ 毫秒，这几乎是 AnyNet [[88](#bib.bib88)]（使用 3D 成本体积）的 $8$ 倍速度。另请注意，AnyNet [[88](#bib.bib88)]
    由于其内存效率，可以在移动设备上运行。
- en: 8.3 Reconstruction accuracy
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 重建准确性
- en: '![Refer to caption](img/1316059820018b62c7c95008b95d2a09.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/1316059820018b62c7c95008b95d2a09.png)'
- en: 'Figure 10: Overall Bad-n error, $n\in[0.5,5.0]$ on a selection of $141$ (baseline)
    images from the stereo vision challenge of ApolloScape dataset [[34](#bib.bib34)].
    A similar behaviour is observed on the challenge subset, see the supplementary
    material. The horizontal axis is the error $n$ while the vertical axis is the
    percentage of pixels whose estimated disparity deviates with more than $n$ pixels
    from the ground truth.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：总体 Bad-n 误差，$n\in[0.5,5.0]$，基于 ApolloScape 数据集中 $141$ 张（基准）立体视觉挑战的图像 [[34](#bib.bib34)]。在挑战子集上观察到类似的行为，请参见补充材料。水平轴为误差
    $n$，垂直轴为估计视差与真实值偏差超过 $n$ 像素的像素百分比。
- en: Table [V](#S8.T5 "TABLE V ‣ 8.2 Computation time and memory footprint ‣ 8 Discussion
    and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")
    shows the average RMSE of each of the methods described in Section [8.1](#S8.SS1
    "8.1 Evaluation protocol ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation"). We report the results on a baseline
    subset composed of $141$ images that look more or less like KITTI2012 images,
    hereinafter referred to as *baseline*, and on another subset composed of $33$
    images with challenging lighting conditions, hereinafter referred to as *challenge*.
    Here, we focus on the relative comparison across methods since some of the high
    errors observed might be attributed to the way the ground-truth has been acquired
    in ApolloScape [[34](#bib.bib34)] dataset, rather than to the methods themselves.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [V](#S8.T5 "TABLE V ‣ 8.2 Computation time and memory footprint ‣ 8 Discussion
    and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")显示了第 [8.1](#S8.SS1
    "8.1 Evaluation protocol ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")节中描述的每种方法的平均 RMSE。我们报告了由 $141$ 张图像组成的基线子集的结果，这些图像或多或少类似于
    KITTI2012 图像，此后称为 *baseline*，以及由 $33$ 张具有挑战性光照条件的图像组成的另一个子集，此后称为 *challenge*。在这里，我们专注于方法之间的相对比较，因为一些高误差可能与
    ApolloScape [[34](#bib.bib34)] 数据集中地面真实值的获取方式有关，而不是方法本身。
- en: We observe that these methods behave almost equally on the two subsets. However,
    the reconstruction error, is significantly important, $>8$ pixels, compared to
    the errors reported on standard datasets such as KITTI2012 and KITTI2015\. This
    suggests that, when there is a significant domain gap between training and testing
    then the reconstruction accuracy can be significantly affected.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到这些方法在两个子集上的表现几乎相同。然而，重建误差显著重要，$>8$ 像素，相比于在标准数据集如 KITTI2012 和 KITTI2015
    上报告的误差。这表明，当训练和测试之间存在显著的领域差距时，重建精度可能会受到显著影响。
- en: We also observe the same trend on the Bad-n curves of Fig. [10](#S8.F10 "Figure
    10 ‣ 8.3 Reconstruction accuracy ‣ 8 Discussion and comparison ‣ A Survey on Deep
    Learning Techniques for Stereo-based Depth Estimation") where, in all methods,
    more than $25\%$ of the pixels had a reconstruction error that is larger than
    $5$ pixels. The Bad-n curves show that the errors are large on the foreground
    pixels, *i.e.,* pixels that correspond to cars, with more than $55\%$ of the pixels
    having an error that is larger than $3$ pixels (against $35\%$ on the background
    pixels). Interestingly, Table [V](#S8.T5 "TABLE V ‣ 8.2 Computation time and memory
    footprint ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") and Fig. [10](#S8.F10 "Figure 10 ‣ 8.3 Reconstruction
    accuracy ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques
    for Stereo-based Depth Estimation") show that most of the methods achieve similar
    reconstruction accuracies. The only exception is iResNet [[63](#bib.bib63)] trained
    on Kitti2015 and on ROB [[152](#bib.bib152)], which had more than $90\%$, respectively
    $55\%$, of pixels with an error that is larger than $5$ pixels. In all methods,
    less than $5\%$ of the pixels had an error that is less than $2$ pixels. This
    suggests that achieving sub-pixel accuracy remains an important challenge for
    future research.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在图 [10](#S8.F10 "Figure 10 ‣ 8.3 Reconstruction accuracy ‣ 8 Discussion and
    comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")的
    Bad-n 曲线中观察到了相同的趋势，其中，在所有方法中，超过 $25\%$ 的像素的重建误差大于 $5$ 像素。Bad-n 曲线显示前景像素上的误差很大，*即*对应于汽车的像素，其中超过
    $55\%$ 的像素的误差大于 $3$ 像素（背景像素的误差为 $35\%$）。有趣的是，表 [V](#S8.T5 "TABLE V ‣ 8.2 Computation
    time and memory footprint ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")和图 [10](#S8.F10 "Figure 10 ‣ 8.3
    Reconstruction accuracy ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning
    Techniques for Stereo-based Depth Estimation")显示，大多数方法实现了类似的重建精度。唯一的例外是 iResNet [[63](#bib.bib63)]，它在
    Kitti2015 和 ROB [[152](#bib.bib152)] 上训练，分别有超过 $90\%$ 和 $55\%$ 的像素的误差大于 $5$ 像素。在所有方法中，少于
    $5\%$ 的像素的误差小于 $2$ 像素。这表明，实现亚像素精度仍然是未来研究的重要挑战。
- en: Note that SegStereo [[68](#bib.bib68)], which is self-supervised, achieves a
    similar or better performance than many of the supervised methods. Also, the unsupervised
    self-adaptation method of Tonioni *et al.* [[129](#bib.bib129)], which takes the
    baseline DispNet-Corr1D network [[22](#bib.bib22)] trained on KITTI 2012 and adapts
    it to KITTI2015 and Middlebury 2014, achieves one of the best performances on
    the foreground regions.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，SegStereo [[68](#bib.bib68)] 是一种自监督方法，其性能与许多监督方法相当或更好。此外，Tonioni *et al.*
    [[129](#bib.bib129)] 提出的无监督自适应方法，将基于 KITTI 2012 训练的 DispNet-Corr1D 网络 [[22](#bib.bib22)]
    适配到 KITTI2015 和 Middlebury 2014，在前景区域实现了最佳性能之一。
- en: '| ![Refer to caption](img/ce7eb8921bd7d9014164e59d8522b055.png) | ![Refer to
    caption](img/a75310f3dd91c3d1680007cc9da1fadd.png) | ![Refer to caption](img/184bf118de495b271b498737930021a8.png)
    | ![Refer to caption](img/7bb0f3f9377a9116e885398debb15ba5.png) | ![Refer to caption](img/040764ab56d19d299b0a48e640e6fe50.png)
    | ![Refer to caption](img/12a94cf3469c6f6583735f3bbef27630.png) |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/ce7eb8921bd7d9014164e59d8522b055.png) | ![参见说明](img/a75310f3dd91c3d1680007cc9da1fadd.png)
    | ![参见说明](img/184bf118de495b271b498737930021a8.png) | ![参见说明](img/7bb0f3f9377a9116e885398debb15ba5.png)
    | ![参见说明](img/040764ab56d19d299b0a48e640e6fe50.png) | ![参见说明](img/12a94cf3469c6f6583735f3bbef27630.png)
    |'
- en: '| AnyNet (Kitti2012) | AnyNet (Kitti2015) | DeepPruner (fast) | DispNet3 (css)
    | GANet (Kittit2015) | Unsup.Adapt (Kitti) |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| AnyNet (Kitti2012) | AnyNet (Kitti2015) | DeepPruner (fast) | DispNet3 (css)
    | GANet (Kittit2015) | Unsup.Adapt (Kitti) |'
- en: '| ![Refer to caption](img/7bf58590d3eee444a74b1fee64ef90dc.png) | ![Refer to
    caption](img/a9d106d7a62b3f2a75297fc7679f43db.png) | ![Refer to caption](img/a3922c235ee795402226f7f815f08018.png)
    | ![Refer to caption](img/d1151e3ec50e7d1d8ec3a5f39050a5ff.png) | ![Refer to caption](img/f949650509aabf375bd4ba4f41026bbb.png)
    | ![Refer to caption](img/d0ce84bec16fccc1d77342dde6e63853.png) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/7bf58590d3eee444a74b1fee64ef90dc.png) | ![参见说明](img/a9d106d7a62b3f2a75297fc7679f43db.png)
    | ![参见说明](img/a3922c235ee795402226f7f815f08018.png) | ![参见说明](img/d1151e3ec50e7d1d8ec3a5f39050a5ff.png)
    | ![参见说明](img/f949650509aabf375bd4ba4f41026bbb.png) | ![参见说明](img/d0ce84bec16fccc1d77342dde6e63853.png)
    |'
- en: '| iResNet (Kitti2015) | iResNet (ROB) | PSMNet (Kitti2012) | PSMNet (Kitti2015)
    | SegStereo | Unsup.Adapt |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| iResNet (Kitti2015) | iResNet (ROB) | PSMNet (Kitti2012) | PSMNet (Kitti2015)
    | SegStereo | Unsup.Adapt |'
- en: '|  |  |  |  |  | (shadowsontruck) |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | (shadowsontruck) |'
- en: '| (a) Results on the images of Fig. [8](#S8.F8 "Figure 8 ‣ 8.1 Evaluation protocol
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(a). |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| (a) 图中的结果见[8](#S8.F8 "Figure 8 ‣ 8.1 Evaluation protocol ‣ 8 Discussion and
    comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(a)。
    |'
- en: '| ![Refer to caption](img/67a0d29509cdd07efde38d0677e8dd04.png) | ![Refer to
    caption](img/3774f5aab19b7b9e75fd558ef8c970fb.png) | ![Refer to caption](img/6d2c5dccb69cfbae1bad33ef48661acf.png)
    | ![Refer to caption](img/749d43771444e18c6c704f47c1cca416.png) | ![Refer to caption](img/23a56666ed3d929a3f60d98d6cfc1cd9.png)
    | ![Refer to caption](img/3bfbf8e8371d8e7a113dafe22a5cdfd6.png) |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/67a0d29509cdd07efde38d0677e8dd04.png) | ![参见说明](img/3774f5aab19b7b9e75fd558ef8c970fb.png)
    | ![参见说明](img/6d2c5dccb69cfbae1bad33ef48661acf.png) | ![参见说明](img/749d43771444e18c6c704f47c1cca416.png)
    | ![参见说明](img/23a56666ed3d929a3f60d98d6cfc1cd9.png) | ![参见说明](img/3bfbf8e8371d8e7a113dafe22a5cdfd6.png)
    |'
- en: '| AnyNet (Kitti2012) | AnyNet (Kitti2015) | DeepPruner (fast) | DispNet3 (css)
    | GANet (Kittit2015) | Unsup.Adapt (Kitti) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| AnyNet (Kitti2012) | AnyNet (Kitti2015) | DeepPruner (fast) | DispNet3 (css)
    | GANet (Kittit2015) | Unsup.Adapt (Kitti) |'
- en: '| ![Refer to caption](img/6fd1c2367f38981d4bf6c1f530e4a637.png) | ![Refer to
    caption](img/a14ea61e1f272980068d03bd31863822.png) | ![Refer to caption](img/14cba94081c3ddcfec657925edfbebe6.png)
    | ![Refer to caption](img/2ccf61259b936e000f474ff36c82c554.png) | ![Refer to caption](img/88583835d361a976e0b12266fcd5a4e0.png)
    | ![Refer to caption](img/efeac9434b2526ebf78e9310ddeadc03.png) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/6fd1c2367f38981d4bf6c1f530e4a637.png) | ![参见说明](img/a14ea61e1f272980068d03bd31863822.png)
    | ![参见说明](img/14cba94081c3ddcfec657925edfbebe6.png) | ![参见说明](img/2ccf61259b936e000f474ff36c82c554.png)
    | ![参见说明](img/88583835d361a976e0b12266fcd5a4e0.png) | ![参见说明](img/efeac9434b2526ebf78e9310ddeadc03.png)
    |'
- en: '| iResNet (Kitti2015) | iResNet (ROB) | PSMNet (Kitti2012) | PSMNet (Kitti2015)
    | SegStereo | Unsup.Adapt |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| iResNet (Kitti2015) | iResNet (ROB) | PSMNet (Kitti2012) | PSMNet (Kitti2015)
    | SegStereo | Unsup.Adapt |'
- en: '|  |  |  |  |  | (shadowsontruck) |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | (shadowsontruck) |'
- en: '| (b) Results on the images of Fig. [8](#S8.F8 "Figure 8 ‣ 8.1 Evaluation protocol
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(b). |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| (b) 图中的结果见[8](#S8.F8 "Figure 8 ‣ 8.1 Evaluation protocol ‣ 8 Discussion and
    comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(b)。
    |'
- en: 'Figure 11: Pixel-wise errors between the ground-truth disparities and the disparities
    estimated from the images of Fig. [8](#S8.F8 "Figure 8 ‣ 8.1 Evaluation protocol
    ‣ 8 Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation").'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 地面实况视差与图像中视差估计之间的像素级误差，见图 [8](#S8.F8 "图 8 ‣ 8.1 评估协议 ‣ 8 讨论与比较 ‣ 基于立体视觉的深度估计的深度学习技术调查")。'
- en: In terms of the visual quality of the estimated disparities, see Fig. [11](#S8.F11
    "Figure 11 ‣ 8.3 Reconstruction accuracy ‣ 8 Discussion and comparison ‣ A Survey
    on Deep Learning Techniques for Stereo-based Depth Estimation"), we observe that
    most of the methods were able to recover the overall shape of trees but fail to
    reconstruct the details especially the leaves. The reconstruction errors are high
    in flat areas and around object boundaries. Also, highly reflective materials
    and poor lighting conditions remain a big challenge to these methods as shown
    in Fig. [11](#S8.F11 "Figure 11 ‣ 8.3 Reconstruction accuracy ‣ 8 Discussion and
    comparison ‣ A Survey on Deep Learning Techniques for Stereo-based Depth Estimation")-(b).
    The supplementary material provides more results on the four stereo pairs of Fig. [9](#S8.F9
    "Figure 9 ‣ 8.1 Evaluation protocol ‣ 8 Discussion and comparison ‣ A Survey on
    Deep Learning Techniques for Stereo-based Depth Estimation").
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 就估计的视差的视觉质量而言，参见图 [11](#S8.F11 "图 11 ‣ 8.3 重建精度 ‣ 8 讨论与比较 ‣ 基于立体视觉的深度估计的深度学习技术调查")，我们观察到大多数方法能够恢复树木的整体形状，但在重建细节，尤其是叶子方面存在困难。平坦区域和物体边界周围的重建误差较高。此外，高反射材料和较差的光照条件仍然对这些方法构成重大挑战，如图 [11](#S8.F11
    "图 11 ‣ 8.3 重建精度 ‣ 8 讨论与比较 ‣ 基于立体视觉的深度估计的深度学习技术调查")-(b)所示。补充材料提供了图 [9](#S8.F9
    "图 9 ‣ 8.1 评估协议 ‣ 8 讨论与比较 ‣ 基于立体视觉的深度估计的深度学习技术调查")中四对立体图像的更多结果。
- en: 9 Future research directions
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 未来研究方向
- en: Deep learning methods for stereo-based depth estimation have achieved promising
    results. The topic, however, is still in its infancy and further developments
    are yet to be expected. In this section, we present some of the current issues
    and highlight directions for future research.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的立体视觉深度估计方法已取得了有希望的结果。然而，该领域仍处于起步阶段，未来还有待进一步发展。在本节中，我们介绍了一些当前的问题，并强调了未来研究的方向。
- en: (1) Camera parameters. Most of the stereo-based techniques surveyed in this
    article require rectified images. Multi-view stereo techniques use Plane-Sweep
    Volumes or back-projected images/features. Both image rectification and PSVs require
    known camera parameters, which are challenging to estimate in the wild. Many papers
    attempted to address this problem for monocular depth estimation and for 3D shape
    reconstruction by jointly optimising for the camera parameters and the geometry
    of the 3D scene [[153](#bib.bib153)].
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 相机参数。本文调查的大多数基于立体视觉的技术都需要矫正图像。多视角立体技术使用平面扫描体积（Plane-Sweep Volumes）或反向投影图像/特征。图像矫正和PSVs都需要已知的相机参数，这在实际环境中难以估计。许多论文尝试通过联合优化相机参数和3D场景几何来解决单目深度估计和3D形状重建的问题 [[153](#bib.bib153)]。
- en: (2) Lighting conditions and complex material properties. Poor lighting conditions
    and complex materials properties remain a challenge to most of the current methods,
    see for example Fig. [11](#S8.F11 "Figure 11 ‣ 8.3 Reconstruction accuracy ‣ 8
    Discussion and comparison ‣ A Survey on Deep Learning Techniques for Stereo-based
    Depth Estimation")-(b). Combining object recognition, high-level scene understanding,
    and low-level feature learning can be one promising avenue to address these issues.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 光照条件和复杂材料特性。较差的光照条件和复杂的材料特性对大多数当前方法仍然构成挑战，例如见图 [11](#S8.F11 "图 11 ‣ 8.3
    重建精度 ‣ 8 讨论与比较 ‣ 基于立体视觉的深度估计的深度学习技术调查")-(b)。结合物体识别、高级场景理解和低级特征学习，可能是解决这些问题的一个有希望的途径。
- en: (3) Spatial and depth resolution. Most of the current techniques do not handle
    high resolution input images and generally produce depth maps of low spatial and
    depth resolution. Depth resolution is particularly limited, making the methods
    unable to reconstruct thin structures, *e.g.,* vegetation and hair, and structures
    located at a far distance from the camera. Although refinement modules can improve
    the resolution of the estimated depth maps, the gain is still small compared to
    the resolution of the input images. This has recently been addressed using hierarchical
    techniques, which allow on-demand reports of disparity by capping the resolution
    of the intermediate results [[32](#bib.bib32)]. In these methods, low resolution
    depth maps can be produced in realtime, and thus can be used on mobile platforms,
    while high resolution maps would require more computation time. Producing, in
    realtime, accurate maps of high spatial and depth resolutions remains a challenge
    for future research.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 空间和深度分辨率。当前的大多数技术无法处理高分辨率输入图像，并且通常生成低空间和深度分辨率的深度图。深度分辨率特别有限，使得这些方法无法重建细结构，如*例如*，植被和头发，以及位于远离相机的地方的结构。尽管细化模块可以改善估计深度图的分辨率，但与输入图像的分辨率相比，增益仍然很小。最近使用分层技术解决了这一问题，这允许按需报告视差，通过限制中间结果的分辨率[[32](#bib.bib32)]。在这些方法中，可以实时生成低分辨率深度图，因此可以在移动平台上使用，而高分辨率图则需要更多计算时间。实时生成高空间和深度分辨率的准确图仍然是未来研究的挑战。
- en: (4) Realtime processing. Most deep learning methods for disparity estimation
    use 3D and 4D cost volumes, which are processed and regularized using 2D and 3D
    convolutions. They are expensive in terms of memory requirements and processing
    time. Developing lightweight, and subsequently fast, end-to-end deep networks
    remains a challenging avenue for future research.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 实时处理。大多数深度学习方法用于视差估计，使用3D和4D代价体积，这些体积使用2D和3D卷积进行处理和正则化。在内存需求和处理时间方面，它们都很昂贵。开发轻量级、快速的端到端深度网络仍然是未来研究的一个具有挑战性的方向。
- en: (5) Disparity range. Existing techniques uniformly discretize the disparity
    range. This results in multiple issues. In particular, although the reconstruction
    error can be small in the disparity space, it can result in an error of meters
    in the depth space, especially at far ranges. One way to mitigate this is by discritizing
    disparity and depth uniformly in the log space. Also, changing the disparity range
    requires retraining the networks. Treating depth as a continuum could be one promising
    avenue for future research.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 视差范围。现有技术均匀离散化视差范围。这会导致多个问题。特别是，尽管视差空间中的重建误差可能很小，但在深度空间中可能导致米级误差，尤其是在远距离处。缓解这个问题的一种方法是均匀离散视差和深度的对数空间。此外，改变视差范围需要重新训练网络。将深度视为一个连续体可能是未来研究的一个有希望的方向。
- en: (6) Training. Deep networks heavily rely on the availability of training images
    annotated with ground-truth labels. This is very expensive and labor intensive
    for depth/disparity reconstruction. As such, the performance of the methods and
    their generalization ability can significantly be affected including the risk
    of overfitting the models to specific domains. Existing techniques mitigate this
    problem by either designing loss functions that do not require 3D annotations,
    or by using domain adaptation and transfer learning strategies. The former, however,
    requires calibrated cameras. Domain adaptation techniques, especially unsupervised
    ones [[138](#bib.bib138)], are recently attracting more attention since, with
    these techniques, one can train with both synthetic data, which are easy to obtain,
    and real-world data. They also adapt, in an unsupervised manner and at run-time
    to ever-changing environments as soon as new images are gathered. Early results
    are very encouraging and thus expect in the future to see the emergence of large
    datasets, similar to ImageNet but for 3D reconstruction.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 训练。深度网络在很大程度上依赖于标注了真实标签的训练图像。这对深度/视差重建来说非常昂贵且劳动密集。因此，方法的性能及其泛化能力可能会显著受到影响，包括模型过拟合特定领域的风险。现有技术通过设计不需要3D注释的损失函数，或使用领域适应和迁移学习策略来缓解这个问题。然而，前者需要标定相机。领域适应技术，特别是无监督的[[138](#bib.bib138)]，最近引起了更多关注，因为通过这些技术，可以同时使用易于获得的合成数据和真实数据进行训练。它们还会在无监督的情况下，并在运行时适应不断变化的环境，只要有新的图像收集到。早期结果非常令人鼓舞，因此预计未来会出现类似于ImageNet的大型数据集，用于3D重建。
- en: (7) Automatically learning the network architecture, its activation functions,
    and its parameters from data. Most existing research has focused on designing
    novel network architectures and novel training methods for optimizing their parameters.
    It is only recently that some papers started to focus on automatically learning
    optimal architectures. Early attempts, *e.g.,*  [[149](#bib.bib149)], focus on
    simple architectures. We expect in the future to see more research on automatically
    learning complex disparity estimation architectures and their activation functions,
    using, for example, the neuro-evolution theory [[154](#bib.bib154), [155](#bib.bib155)],
    which will free the need for manual network design.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: (7) 从数据中自动学习网络架构、激活函数及其参数。现有的大部分研究都集中在设计新颖的网络架构和优化参数的新训练方法上。直到最近，一些论文才开始关注自动学习最佳架构。早期的尝试，*例如*
    [[149](#bib.bib149)]，集中在简单架构上。我们期望未来能看到更多关于自动学习复杂视差估计架构及其激活函数的研究，例如利用神经进化理论[[154](#bib.bib154),
    [155](#bib.bib155)]，这将免去手动设计网络的需求。
- en: 10 Conclusion
  id: totrans-485
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 结论
- en: This paper provides a comprehensive survey of the recent developments in stereo-based
    depth estimation using deep learning techniques. Despite their infancy, these
    techniques are achieving state-of-the-art results. Since 2014, we have entered
    a new era where data-driven and machine learning techniques play a central role
    in image-based depth reconstruction. We have seen that, from $2014$ to $2019$,
    more than $150$ papers on the topic have been published in the major computer
    vision, computer graphics, and machine learning conferences and journals. Even
    during the final stages of this submission, more new papers are being published
    making it difficult to keep track of the recent developments, and more importantly,
    understand their differences and similarities, especially for new comers to the
    field. This timely survey can thus serve as a guide to the reader to navigate
    this fast-growing field of research.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了基于立体视觉的深度估计技术的最新进展的全面综述。尽管这些技术仍处于初期阶段，但已取得了最先进的成果。自 2014 年以来，我们进入了一个数据驱动和机器学习技术在基于图像的深度重建中发挥核心作用的新纪元。从
    $2014$ 年到 $2019$ 年，相关领域的主要计算机视觉、计算机图形学和机器学习会议及期刊上发表了超过 $150$ 篇论文。即使在提交的最后阶段，新的论文仍在不断发表，这使得跟踪最新进展变得困难，尤其是对该领域的新手来说，了解这些进展的差异和相似性更是重要。因此，这份及时的综述可以作为读者导航这一快速发展的研究领域的指南。
- en: Finally, there are several related topics that have not been covered in this
    survey. Examples include image-based 3D object reconstruction using deep learning,
    which has been recently surveyed by Han *et al.* [[153](#bib.bib153)], and monocular
    and video-based depth estimation, which requires a separate survey paper given
    the large amount of papers that have been published on the topic in the past $5$
    to $6$ years. Other topics include photometric stereo and active stereo [[156](#bib.bib156),
    [157](#bib.bib157)], which are outside the scope of this paper.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本综述未涵盖的相关主题还有很多。例如，基于图像的 3D 物体重建，最近由 Han *et al.* [[153](#bib.bib153)] 进行了综述，以及单目和视频基础的深度估计，鉴于过去
    $5$ 到 $6$ 年发表了大量相关论文，需要单独的综述论文。其他主题包括光度立体和主动立体[[156](#bib.bib156), [157](#bib.bib157)]，这些都超出了本文的范围。
- en: Acknowledgements. We would like to thank all the authors of the reference papers
    who have made their codes and datasets publicly available. This work is supported
    in part by Murdoch University’s Vice Chancellor’s Small Steps of Innovation Funding
    Program, and by ARC DP150100294 and DP150104251.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。我们感谢所有公开其代码和数据集的参考文献的作者。此项工作部分得到了默多克大学副校长创新小步资金计划的支持，以及 ARC DP150100294 和
    DP150104251 的资助。
- en: References
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Khan, H. Rahmani, S. A. A. Shah, and M. Bennamoun, “A guide to convolutional
    neural networks for computer vision,” *Synthesis Lectures on Computer Vision*,
    vol. 8, no. 1, pp. 1–207, 2018.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Khan, H. Rahmani, S. A. A. Shah, 和 M. Bennamoun, “计算机视觉中的卷积神经网络指南，”
    *计算机视觉综合讲座*，第 8 卷，第 1 期，页码 1–207，2018 年。'
- en: '[2] B. Li, C. Shen, Y. Dai, A. Van Den Hengel, and M. He, “Depth and surface
    normal estimation from monocular images using regression on deep features and
    hierarchical CRFs,” in *IEEE CVPR*, 2015, pp. 1119–1127.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] B. Li, C. Shen, Y. Dai, A. Van Den Hengel, 和 M. He, “使用深度特征和层次 CRFs 的回归从单目图像中估计深度和表面法线，”
    *IEEE CVPR*，2015 年，页码 1119–1127。'
- en: '[3] D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic
    labels with a common multi-scale convolutional architecture,” in *IEEE ICCV*,
    2015, pp. 2650–2658.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D. Eigen 和 R. Fergus， “使用通用多尺度卷积架构预测深度、表面法线和语义标签”，发表于 *IEEE ICCV*，2015年，页码
    2650–2658。'
- en: '[4] R. Garg, V. K. BG, G. Carneiro, and I. Reid, “Unsupervised CNN for single
    view depth estimation: Geometry to the rescue,” in *ECCV*, 2016, pp. 740–756.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] R. Garg, V. K. BG, G. Carneiro, 和 I. Reid， “用于单视图深度估计的无监督 CNN：几何学的拯救”，发表于
    *ECCV*，2016年，页码 740–756。'
- en: '[5] F. Liu, C. Shen, G. Lin, and I. D. Reid, “Learning Depth from Single Monocular
    Images Using Deep Convolutional Neural Fields,” *IEEE PAMI*, vol. 38, no. 10,
    pp. 2024–2039, 2016.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] F. Liu, C. Shen, G. Lin, 和 I. D. Reid， “利用深度卷积神经场从单一单目图像中学习深度”， *IEEE PAMI*，第38卷，第10期，页码
    2024–2039，2016年。'
- en: '[6] J. Yang, S. E. Reed, M.-H. Yang, and H. Lee, “Weakly-supervised disentangling
    with recurrent transformations for 3D view synthesis,” in *NIPS*, 2015, pp. 1099–1107.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Yang, S. E. Reed, M.-H. Yang, 和 H. Lee， “用于 3D 视图合成的弱监督解缠结与递归变换”，发表于
    *NIPS*，2015年，页码 1099–1107。'
- en: '[7] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum, “Deep convolutional
    inverse graphics network,” in *NIPS*, 2015, pp. 2539–2547.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. D. Kulkarni, W. F. Whitney, P. Kohli, 和 J. Tenenbaum， “深度卷积逆向图形网络”，发表于
    *NIPS*，2015年，页码 2539–2547。'
- en: '[8] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Multi-view 3D models from
    single images with a convolutional network,” in *ECCV*, 2016, pp. 322–337.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox， “利用卷积网络从单张图像生成多视角 3D 模型”，发表于
    *ECCV*，2016年，页码 322–337。'
- en: '[9] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros, “View synthesis
    by appearance flow,” in *ECCV*, 2016, pp. 286–301.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. Zhou, S. Tulsiani, W. Sun, J. Malik, 和 A. A. Efros， “通过外观流进行视图合成”，发表于
    *ECCV*，2016年，页码 286–301。'
- en: '[10] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C. Berg, “Transformation-grounded
    image generation network for novel 3D view synthesis,” in *IEEE CVPR*, 2017, pp.
    702–711.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] E. Park, J. Yang, E. Yumer, D. Ceylan, 和 A. C. Berg， “基于变换的图像生成网络用于新颖的
    3D 视图合成”，发表于 *IEEE CVPR*，2017年，页码 702–711。'
- en: '[11] D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame
    stereo correspondence algorithms,” *IJCV*, vol. 47, no. 1-3, pp. 7–42, 2002.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] D. Scharstein 和 R. Szeliski， “稠密双帧立体对应算法的分类和评估”， *IJCV*，第47卷，第1-3期，页码
    7–42，2002年。'
- en: '[12] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from a single
    image using a multi-scale deep network,” in *NIPS*, 2014, pp. 2366–2374.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. Eigen, C. Puhrsch, 和 R. Fergus， “使用多尺度深度网络从单张图像预测深度图”，发表于 *NIPS*，2014年，页码
    2366–2374。'
- en: '[13] X. Wang, D. Fouhey, and A. Gupta, “Designing deep networks for surface
    normal estimation,” in *IEEE CVPR*, 2015, pp. 539–547.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Wang, D. Fouhey, 和 A. Gupta， “为表面法线估计设计深度网络”，发表于 *IEEE CVPR*，2015年，页码
    539–547。'
- en: '[14] A. Saxena, M. Sun, and A. Y. Ng, “Make3d: Learning 3d scene structure
    from a single still image,” *IEEE PAMI*, vol. 31, no. 5, pp. 824–840, 2009.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. Saxena, M. Sun, 和 A. Y. Ng， “Make3d：从单张静态图像学习 3D 场景结构”， *IEEE PAMI*，第31卷，第5期，页码
    824–840，2009年。'
- en: '[15] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *IEEE CVPR*, 2012, pp. 3354–3361.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Geiger, P. Lenz, 和 R. Urtasun， “我们准备好进行自主驾驶了吗？KITTI 视觉基准套件”，发表于 *IEEE
    CVPR*，2012年，页码 3354–3361。'
- en: '[16] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic
    open source movie for optical flow evaluation,” in *ECCV*, 2012, pp. 611–625.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] D. J. Butler, J. Wulff, G. B. Stanley, 和 M. J. Black， “用于光流评估的自然开放源电影”，发表于
    *ECCV*，2012年，页码 611–625。'
- en: '[17] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” *ECCV*, pp. 746–760, 2012.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus， “从 rgbd 图像中进行室内分割和支持推断”，
    *ECCV*，页码 746–760，2012年。'
- en: '[18] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark
    for the evaluation of rgb-d slam systems,” in *IROS*, 2012, pp. 573–580.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Sturm, N. Engelhard, F. Endres, W. Burgard, 和 D. Cremers， “rgb-d slam
    系统评估的基准”，发表于 *IROS*，2012年，页码 573–580。'
- en: '[19] J. Xiao, A. Owens, and A. Torralba, “Sun3d: A database of big spaces reconstructed
    using sfm and object labels,” in *IEEE ICCV*, 2013, pp. 1625–1632.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Xiao, A. Owens, 和 A. Torralba， “Sun3d：一个使用 sfm 和对象标签重建的大型空间数据库”，发表于
    *IEEE ICCV*，2013年，页码 1625–1632。'
- en: '[20] D. Scharstein, H. Hirschmüller, Y. Kitajima, G. Krathwohl, N. Nešić, X. Wang,
    and P. Westling, “High-resolution stereo datasets with subpixel-accurate ground
    truth,” in *German conference on pattern recognition*, 2014, pp. 31–42.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] D. Scharstein, H. Hirschmüller, Y. Kitajima, G. Krathwohl, N. Nešić, X. Wang,
    和 P. Westling， “具有亚像素精确地面真实值的高分辨率立体数据集”，发表于 *德国模式识别会议*，2014年，页码 31–42。'
- en: '[21] M. Menze and A. Geiger, “Object scene flow for autonomous vehicles,” in
    *IEEE CVPR*, 2015, pp. 3061–3070.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] M. Menze 和 A. Geiger， “用于自主车辆的对象场景流”，发表于 *IEEE CVPR*，2015年，页码 3061–3070。'
- en: '[22] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy,
    and T. Brox, “A large dataset to train convolutional networks for disparity, optical
    flow, and scene flow estimation,” in *IEEE CVPR*, 2016, pp. 4040–4048.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *IEEE CVPR*, 2016, pp. 3213–3223.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, “Large-scale
    data for multiple-view stereopsis,” *IJCV*, vol. 120, no. 2, pp. 153–168, 2016.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] T. Schöps, J. L. Schönberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys,
    and A. Geiger, “A multi-view stereo benchmark with high-resolution images and
    multi-camera videos,” in *IEEE CVPR*, 2017, pp. 3260–3269.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, “Semantic
    scene completion from a single depth image,” in *IEEE CVPR*, 2017, pp. 1746–1754.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, “DeepMVS:
    Learning Multi-view Stereopsis,” in *IEEE CVPR*, 2018, pp. 2821–2830.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Z. Li and N. Snavely, “MegaDepth: Learning Single-View Depth Prediction
    From Internet Photos,” in *IEEE CVPR*, 2018, pp. 2041–2050.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Jeon and S. Lee, “Reconstruction-based Pairwise Depth Dataset for Depth
    Image Enhancement Using CNN,” in *ECCV*, 2018, pp. 422–438.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] C. Won, J. Ryu, and J. Lim, “OmniMVS: End-to-End Learning for Omnidirectional
    Stereo Matching,” in *IEEE ICCV*, 2019, pp. 8987–8996.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] ——, “End-to-End Learning for Omnidirectional Stereo Matching with Uncertainty
    Prior,” *IEEE PAMI*, 2020.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] G. Yang, J. Manela, M. Happold, and D. Ramanan, “Hierarchical Deep Stereo
    Matching on High-Resolution Images,” in *IEEE CVPR*, 2019, pp. 5515–5524.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] G. Yang, X. Song, C. Huang, Z. Deng, J. Shi, and B. Zhou, “Drivingstereo:
    A large-scale dataset for stereo matching in autonomous driving scenarios,” in
    *IEEE CVPR*, 2019, pp. 899–908.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, “The apolloscape
    open dataset for autonomous driving and its application,” *IEEE transactions on
    pattern analysis and machine intelligence*, 2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung,
    L. Hauswald, V. H. Pham, M. Mühlegg, S. Dorn *et al.*, “A2d2: Audi autonomous
    driving dataset,” *arXiv preprint arXiv:2004.06320*, 2020.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] N. Mayer, E. Ilg, P. Fischer, C. Hazirbas, D. Cremers, A. Dosovitskiy,
    and T. Brox, “What makes good synthetic training data for learning disparity and
    optical flow estimation?” *International Journal of Computer Vision*, vol. 126,
    no. 9, pp. 942–960, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Zagoruyko and N. Komodakis, “Learning to compare image patches via
    convolutional neural networks,” in *IEEE CVPR*, 2015, pp. 4353–4361.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg, “MatchNet: Unifying
    feature and metric learning for patch-based matching,” in *IEEE CVPR*, 2015, pp.
    3279–3286.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] X. Han, T. Leung, Y. Jia, R. Sukthankar, 和 A. C. Berg，“MatchNet：统一特征和度量学习用于基于补丁的匹配，”在*IEEE
    CVPR*，2015年，第3279–3286页。'
- en: '[39] J. Zbontar and Y. LeCun, “Computing the stereo matching cost with a convolutional
    neural network,” in *IEEE CVPR*, 2015, pp. 1592–1599.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Zbontar 和 Y. LeCun，“使用卷积神经网络计算立体匹配成本，”在*IEEE CVPR*，2015年，第1592–1599页。'
- en: '[40] W. Chen, X. Sun, L. Wang, Y. Yu, and C. Huang, “A deep visual correspondence
    embedding model for stereo matching costs,” in *IEEE ICCV*, 2015, pp. 972–980.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] W. Chen, X. Sun, L. Wang, Y. Yu, 和 C. Huang，“用于立体匹配成本的深度视觉对应嵌入模型，”在*IEEE
    ICCV*，2015年，第972–980页。'
- en: '[41] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer,
    “Discriminative learning of deep convolutional feature point descriptors,” in
    *IEEE ICCV*, 2015, pp. 118–126.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, 和 F. Moreno-Noguer，“深度卷积特征点描述符的辨别学习，”在*IEEE
    ICCV*，2015年，第118–126页。'
- en: '[42] J. Zbontar and Y. LeCun, “Stereo matching by training a convolutional
    neural network to compare image patches,” *Journal of Machine Learning Research*,
    vol. 17, no. 1-32, p. 2, 2016.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Zbontar 和 Y. LeCun，“通过训练卷积神经网络比较图像补丁进行立体匹配，”*Journal of Machine Learning
    Research*，第17卷，第1-32期，第2页，2016年。'
- en: '[43] V. Balntas, E. Johns, L. Tang, and K. Mikolajczyk, “PN-Net: Conjoined
    Triple Deep Network for Learning Local Image Descriptors,” *CoRR*, 2016.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] V. Balntas, E. Johns, L. Tang, 和 K. Mikolajczyk，“PN-Net：用于学习局部图像描述符的联合三重深度网络，”*CoRR*，2016年。'
- en: '[44] W. Luo, A. G. Schwing, and R. Urtasun, “Efficient deep learning for stereo
    matching,” in *IEEE CVPR*, 2016, pp. 5695–5703.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] W. Luo, A. G. Schwing, 和 R. Urtasun，“高效的深度学习用于立体匹配，”在*IEEE CVPR*，2016年，第5695–5703页。'
- en: '[45] B. Kumar, G. Carneiro, I. Reid *et al.*, “Learning local image descriptors
    with deep siamese and triplet convolutional networks by minimising global loss
    functions,” in *IEEE CVPR*, 2016, pp. 5385–5394.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] B. Kumar, G. Carneiro, I. Reid *等*，“通过最小化全局损失函数使用深度孪生和三重卷积网络学习局部图像描述符，”在*IEEE
    CVPR*，2016年，第5385–5394页。'
- en: '[46] A. Shaked and L. Wolf, “Improved stereo matching with constant highway
    networks and reflective confidence learning,” in *IEEE CVPR*, 2017, pp. 4641–4650.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Shaked 和 L. Wolf，“通过常数高速公路网络和反射置信学习改进立体匹配，”在*IEEE CVPR*，2017年，第4641–4650页。'
- en: '[47] W. Hartmann, S. Galliani, M. Havlena, L. Van Gool, and K. Schindler, “Learned
    multi-patch similarity,” in *IEEE ICCV*, 2017, pp. 1595–1603.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] W. Hartmann, S. Galliani, M. Havlena, L. Van Gool, 和 K. Schindler，“学习的多补丁相似性，”在*IEEE
    ICCV*，2017年，第1595–1603页。'
- en: '[48] H. Park and K. M. Lee, “Look wider to match image patches with convolutional
    neural networks,” *IEEE Signal Processing Letters*, vol. 24, no. 12, pp. 1788–1792,
    2017.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] H. Park 和 K. M. Lee，“通过卷积神经网络扩大视野来匹配图像补丁，”*IEEE Signal Processing Letters*，第24卷，第12期，第1788–1792页，2017年。'
- en: '[49] X. Ye, J. Li, H. Wang, H. Huang, and X. Zhang, “Efficient stereo matching
    leveraging deep local and context information,” *IEEE Access*, vol. 5, pp. 18 745–18 755,
    2017.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. Ye, J. Li, H. Wang, H. Huang, 和 X. Zhang，“利用深度局部和上下文信息的高效立体匹配，”*IEEE
    Access*，第5卷，第18,745–18,755页，2017年。'
- en: '[50] S. Tulyakov, A. Ivanov, and F. Fleuret, “Weakly supervised learning of
    deep metrics for stereo reconstruction,” in *IEEE ICCV*, 2017, pp. 1339–1348.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Tulyakov, A. Ivanov, 和 F. Fleuret，“弱监督学习深度度量用于立体重建，”在*IEEE ICCV*，2017年，第1339–1348页。'
- en: '[51] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, and T. Brox, “FlowNet: Learning optical flow with
    convolutional networks,” in *IEEE ICCV*, 2015, pp. 2758–2766.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, 和 T. Brox，“FlowNet：使用卷积网络学习光流，”在*IEEE ICCV*，2015年，第2758–2766页。'
- en: '[52] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, “Deep Ordinal Regression
    Network for Monocular Depth Estimation,” in *IEEE CVPR*, June 2018.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] H. Fu, M. Gong, C. Wang, K. Batmanghelich, 和 D. Tao，“用于单目深度估计的深度序数回归网络，”在*IEEE
    CVPR*，2018年6月。'
- en: '[53] S. Sukhbaatar and R. Fergus, “Learning from noisy labels with deep neural
    networks,” *ICLR Workshop*, 2014.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] S. Sukhbaatar 和 R. Fergus，“通过深度神经网络从噪声标签中学习，”*ICLR Workshop*，2014年。'
- en: '[54] P. Fischer, A. Dosovitskiy, and T. Brox, “Descriptor matching with convolutional
    neural networks: a comparison to sift,” *CoRR*, 2014.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] P. Fischer, A. Dosovitskiy, 和 T. Brox，“使用卷积神经网络进行描述符匹配：与sift的比较，”*CoRR*，2014年。'
- en: '[55] H. Hirschmuller, “Stereo processing by semiglobal matching and mutual
    information,” *IEEE PAMI*, vol. 30, no. 2, pp. 328–341, 2008.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] H. Hirschmuller，“通过半全局匹配和互信息进行立体处理，”*IEEE PAMI*，第30卷，第2期，第328–341页，2008年。'
- en: '[56] A. Seki and M. Pollefeys, “SGM-Nets: Semi-global matching with neural
    networks,” in *IEEE CVPR Workshops*, 2017, pp. 21–26.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Seki 和 M. Pollefeys，“SGM-Nets: 使用神经网络进行半全局匹配”，在 *IEEE CVPR Workshops*，2017年，第21–26页。'
- en: '[57] J. L. Schönberger, S. N. Sinha, and M. Pollefeys, “Learning to Fuse Proposals
    from Multiple Scanline Optimizations in Semi-Global Matching,” in *Proceedings
    of ECCV*, 2018, pp. 739–755.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. L. Schönberger, S. N. Sinha, 和 M. Pollefeys，“学习从多条扫描线优化中融合提案以进行半全局匹配”，在
    *ECCV 会议论文集*，2018年，第739–755页。'
- en: '[58] M. Poggi and S. Mattoccia, “Learning a general-purpose confidence measure
    based on o(1) features and a smarter aggregation strategy for semi global matching,”
    in *3DV*, 2016, pp. 509–518.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Poggi 和 S. Mattoccia，“基于 o(1) 特征和更智能的聚合策略的通用置信度度量学习，用于半全局匹配”，在 *3DV*，2016年，第509–518页。'
- en: '[59] X. Hu and P. Mordohai, “A quantitative evaluation of confidence measures
    for stereo vision,” *IEEE PAMI*, vol. 34, no. 11, pp. 2121–2133, 2012.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] X. Hu 和 P. Mordohai，“立体视觉中置信度测量的定量评估”，*IEEE PAMI*，第34卷，第11期，第2121–2133页，2012年。'
- en: '[60] A. Kar, C. Häne, and J. Malik, “Learning a multi-view stereo machine,”
    in *NIPS*, 2017, pp. 364–375.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] A. Kar, C. Häné, 和 J. Malik，“学习一个多视角立体机器”，在 *NIPS*，2017年，第364–375页。'
- en: '[61] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach,
    and A. Bry, “End-to-end learning of geometry and context for deep stereo regression,”
    *IEEE ICCV*, pp. 66–75, 2017.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach,
    和 A. Bry，“深度立体回归的端到端几何和上下文学习”，*IEEE ICCV*，第66–75页，2017年。'
- en: '[62] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan, “Cascade residual learning:
    A two-stage convolutional neural network for stereo matching,” in *ICCV Workshops*,
    vol. 7, no. 8, 2017.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] J. Pang, W. Sun, J. S. Ren, C. Yang, 和 Q. Yan，“级联残差学习：一种用于立体匹配的两阶段卷积神经网络”，在
    *ICCV Workshops*，第7卷，第8期，2017年。'
- en: '[63] Z. Liang, Y. Feng, Y. G. H. L. W. Chen, and L. Q. L. Z. J. Zhang, “Learning
    for Disparity Estimation Through Feature Constancy,” in *IEEE CVPR*, 2018, pp.
    2811–2820.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Z. Liang, Y. Feng, Y. G. H. L. W. Chen, 和 L. Q. L. Z. J. Zhang，“通过特征一致性学习视差估计”，在
    *IEEE CVPR*，2018年，第2811–2820页。'
- en: '[64] J. Chang and Y. Chen, “Pyramid Stereo Matching Network,” *IEEE CVPR*,
    pp. 5410–5418, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Chang 和 Y. Chen，“金字塔立体匹配网络”，*IEEE CVPR*，第5410–5418页，2018年。'
- en: '[65] G.-Y. Nie, M.-M. Cheng, Y. Liu, Z. Liang, D.-P. Fan, Y. Liu, and Y. Wang,
    “Multi-Level Context Ultra-Aggregation for Stereo Matching,” in *IEEE CVPR*, 2019,
    pp. 3283–3291.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] G.-Y. Nie, M.-M. Cheng, Y. Liu, Z. Liang, D.-P. Fan, Y. Liu, 和 Y. Wang，“用于立体匹配的多层次上下文超聚合”，在
    *IEEE CVPR*，2019年，第3283–3291页。'
- en: '[66] J. Flynn, I. Neulander, J. Philbin, and N. Snavely, “DeepStereo: Learning
    to predict new views from the world’s imagery,” in *IEEE CVPR*, 2016, pp. 5515–5524.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Flynn, I. Neulander, J. Philbin, 和 N. Snavely，“DeepStereo: 从世界图像中学习预测新视图”，在
    *IEEE CVPR*，2016年，第5515–5524页。'
- en: '[67] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *IEEE CVPR*, 2016, pp. 770–778.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] K. He, X. Zhang, S. Ren, 和 J. Sun，“深度残差学习用于图像识别”，在 *IEEE CVPR*，2016年，第770–778页。'
- en: '[68] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia, “SegStereo: Exploiting
    semantic information for disparity estimation,” in *ECCV*, 2018, pp. 636–651.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] G. Yang, H. Zhao, J. Shi, Z. Deng, 和 J. Jia，“SegStereo: 利用语义信息进行视差估计”，在
    *ECCV*，2018年，第636–651页。'
- en: '[69] F. Yu, D. Wang, E. Shelhamer, and T. Darrell, “Deep layer aggregation,”
    in *IEEE CVPR*, 2018, pp. 2403–2412.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] F. Yu, D. Wang, E. Shelhamer, 和 T. Darrell，“深度层次聚合”，在 *IEEE CVPR*，2018年，第2403–2412页。'
- en: '[70] Y. Zhong, Y. Dai, and H. Li, “Self-supervised learning for stereo matching
    with self-improving ability,” *arXiv:1709.00930*, 2017.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Y. Zhong, Y. Dai, 和 H. Li，“具有自我改进能力的立体匹配自监督学习”，*arXiv:1709.00930*，2017年。'
- en: '[71] P. Knöbelreiter, C. Reinbacher, A. Shekhovtsov, and T. Pock, “End-to-end
    training of hybrid CNN-CRF models for stereo,” in *IEEE CVPR*, 2017, pp. 1456–1465.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] P. Knöbelreiter, C. Reinbacher, A. Shekhovtsov, 和 T. Pock，“用于立体的混合CNN-CRF模型的端到端训练”，在
    *IEEE CVPR*，2017年，第1456–1465页。'
- en: '[72] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, and S. Izadi,
    “StereoNet: Guided hierarchical refinement for real-time edge-aware depth prediction,”
    *ECCV*, 2018.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] S. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, 和 S. Izadi，“StereoNet:
    实时边缘感知深度预测的引导层次细化”，*ECCV*，2018年。'
- en: '[73] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich,
    M. Schoenberg, S. Izadi, T. Funkhouser, and S. Fanello, “ActiveStereoNet: end-to-end
    self-supervised learning for active stereo systems,” *ECCV*, pp. 784–801, 2018.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Zhang, S. Khamis, C. Rhemann, J. Valentin, A. Kowdle, V. Tankovich,
    M. Schoenberg, S. Izadi, T. Funkhouser, 和 S. Fanello，“ActiveStereoNet: 端到端自监督学习用于主动立体系统”，*ECCV*，第784–801页，2018年。'
- en: '[74] Z. Jie, P. Wang, Y. Ling, B. Zhao, Y. Wei, J. Feng, and W. Liu, “Left-Right
    Comparative Recurrent Model for Stereo Matching,” in *IEEE CVPR*, 2018, pp. 3838–3846.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Z. Jie, P. Wang, Y. Ling, B. Zhao, Y. Wei, J. Feng, 和 W. Liu，“用于立体匹配的左右比较递归模型，”在*IEEE
    CVPR*，2018年，第3838–3846页。'
- en: '[75] E. Ilg, T. Saikia, M. Keuper, and T. Brox, “Occlusions, Motion and Depth
    Boundaries with a Generic Network for Disparity, Optical Flow or Scene Flow Estimation,”
    in *ECCV*, 2018, pp. 614–630.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] E. Ilg, T. Saikia, M. Keuper, 和 T. Brox，“具有通用网络的遮挡、运动和深度边界估计，”在*ECCV*，2018年，第614–630页。'
- en: '[76] X. Song, X. Zhao, H. Hu, and L. Fang, “EdgeStereo: A Context Integrated
    Residual Pyramid Network for Stereo Matching,” *ACCV*, 2018.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] X. Song, X. Zhao, H. Hu, 和 L. Fang，“EdgeStereo: 用于立体匹配的上下文集成残差金字塔网络，”*ACCV*，2018年。'
- en: '[77] L. Yu, Y. Wang, Y. Wu, and Y. Jia, “Deep Stereo Matching with Explicit
    Cost Aaggregation Sub-architecture,” *AAAI*, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] L. Yu, Y. Wang, Y. Wu, 和 Y. Jia，“具有显式代价聚合子架构的深度立体匹配，”*AAAI*，2018年。'
- en: '[78] S. Tulyakov, A. Ivanov, and F. Fleuret, “Practical Deep Stereo (PDS):
    Toward applications-friendly deep stereo matching,” *NIPS*, pp. 5871–5881, 2018.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] S. Tulyakov, A. Ivanov, 和 F. Fleuret，“实用深度立体（PDS）：面向应用友好的深度立体匹配，”*NIPS*，第5871–5881页，2018年。'
- en: '[79] Z. Wu, X. Wu, X. Zhang, S. Wang, and L. Ju, “Semantic Stereo Matching
    With Pyramid Cost Volumes,” in *IEEE ICCV*, 2019, pp. 7484–7493.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Z. Wu, X. Wu, X. Zhang, S. Wang, 和 L. Ju，“具有金字塔代价体积的语义立体匹配，”在*IEEE ICCV*，2019年，第7484–7493页。'
- en: '[80] Z. Yin, T. Darrell, and F. Yu, “Hierarchical Discrete Distribution Decomposition
    for Match Density Estimation,” in *IEEE CVPR*, 2019, pp. 6044–6053.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Z. Yin, T. Darrell, 和 F. Yu，“用于匹配密度估计的分层离散分布分解，”在*IEEE CVPR*，2019年，第6044–6053页。'
- en: '[81] R. Chabra, J. Straub, C. Sweeney, R. Newcombe, and H. Fuchs, “StereoDRNet:
    Dilated Residual StereoNet,” in *IEEE CVPR*, 2019, pp. 11 786–11 795.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] R. Chabra, J. Straub, C. Sweeney, R. Newcombe, 和 H. Fuchs，“StereoDRNet:
    膨胀残差StereoNet，”在*IEEE CVPR*，2019年，第11 786–11 795页。'
- en: '[82] C.-W. Xie, H.-Y. Zhou, and J. Wu, “Vortex pooling: Improving context representation
    in semantic segmentation,” *arXiv:1804.06242*, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C.-W. Xie, H.-Y. Zhou, 和 J. Wu，“漩涡池化：改善语义分割中的上下文表示，”*arXiv:1804.06242*，2018年。'
- en: '[83] S. Duggal, S. Wang, W.-C. Ma, R. Hu, and R. Urtasun, “DeepPruner: Learning
    Efficient Stereo Matching via Differentiable PatchMatch,” in *IEEE ICCV*, 2019,
    pp. 4384–4393.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Duggal, S. Wang, W.-C. Ma, R. Hu, 和 R. Urtasun，“DeepPruner: 通过可微分PatchMatch学习高效立体匹配，”在*IEEE
    ICCV*，2019年，第4384–4393页。'
- en: '[84] A. Tonioni, F. Tosi, M. Poggi, S. Mattoccia, and L. D. Stefano, “Real-time
    self-adaptive deep stereo,” in *IEEE CVPR*, 2019, pp. 195–204.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A. Tonioni, F. Tosi, M. Poggi, S. Mattoccia, 和 L. D. Stefano，“实时自适应深度立体，”在*IEEE
    CVPR*，2019年，第195–204页。'
- en: '[85] F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, “GA-Net: Guided Aggregation
    Net for End-to-End Stereo Matching,” in *IEEE CVPR*, 2019, pp. 185–194.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] F. Zhang, V. Prisacariu, R. Yang, 和 P. H. Torr，“GA-Net: 用于端到端立体匹配的引导聚合网络，”在*IEEE
    CVPR*，2019年，第185–194页。'
- en: '[86] X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, “Group-wise Correlation
    Stereo Network,” in *IEEE CVPR*, 2019, pp. 3273–3282.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] X. Guo, K. Yang, W. Yang, X. Wang, 和 H. Li，“组级相关立体网络，”在*IEEE CVPR*，2019年，第3273–3282页。'
- en: '[87] C. Chen, X. Chen, and H. Cheng, “On the Over-Smoothing Problem of CNN
    Based Disparity Estimation,” in *IEEE ICCV*, 2019, pp. 8997–9005.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] C. Chen, X. Chen, 和 H. Cheng，“关于基于CNN的视差估计的过度平滑问题，”在*IEEE ICCV*，2019年，第8997–9005页。'
- en: '[88] Y. Wang, Z. Lai, G. Huang, B. H. Wang, L. van der Maaten, M. Campbell,
    and K. Q. Weinberger, “Anytime stereo image depth estimation on mobile devices,”
    in *ICRA*, 2019, pp. 5893–5900.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Wang, Z. Lai, G. Huang, B. H. Wang, L. van der Maaten, M. Campbell,
    和 K. Q. Weinberger，“移动设备上的任何时间立体图像深度估计，”在*ICRA*，2019年，第5893–5900页。'
- en: '[89] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang,
    and P. H. Torr, “Conditional random fields as recurrent neural networks,” in *IEEE
    CVPR*, 2015, pp. 1529–1537.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C.
    Huang, 和 P. H. Torr，“条件随机场作为递归神经网络，”在*IEEE CVPR*，2015年，第1529–1537页。'
- en: '[90] Y. Xue, J. Chen, W. Wan, Y. Huang, C. Yu, T. Li, and J. Bao, “MVSCRF:
    Learning Multi-View Stereo With Conditional Random Fields,” in *IEEE ICCV*, 2019,
    pp. 4312–4321.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Y. Xue, J. Chen, W. Wan, Y. Huang, C. Yu, T. Li, 和 J. Bao，“MVSCRF: 使用条件随机场学习多视图立体，”在*IEEE
    ICCV*，2019年，第4312–4321页。'
- en: '[91] D. Paschalidou, O. Ulusoy, C. Schmitt, L. Van Gool, and A. Geiger, “RayNet:
    Learning Volumetric 3D Reconstruction With Ray Potentials,” in *IEEE CVPR*, June
    2018, pp. 3897–3906.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] D. Paschalidou, O. Ulusoy, C. Schmitt, L. Van Gool, 和 A. Geiger，“RayNet:
    使用射线势学习体积三维重建，”在*IEEE CVPR*，2018年6月，第3897–3906页。'
- en: '[92] Y. Yao, Z. Luo, S. Li, T. Shen, T. Fang, and L. Quan, “Recurrent MVSNet
    for High-Resolution Multi-View Stereo Depth Inference,” in *IEEE CVPR*, 2019,
    pp. 5525–5534.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Y. Yao, Z. Luo, S. Li, T. Shen, T. Fang, 和 L. Quan, “递归MVSNet用于高分辨率多视角立体深度推断，”
    收录于 *IEEE CVPR*，2019年，第5525–5534页。'
- en: '[93] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, “MVSNet: Depth Inference
    for Unstructured Multi-view Stereo,” *ECCV*, pp. 767–783, 2018.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Y. Yao, Z. Luo, S. Li, T. Fang, 和 L. Quan, “MVSNet: 用于非结构化多视角立体深度推断，”
    *ECCV*，第767–783页，2018年。'
- en: '[94] J.-H. Lee, M. Heo, K.-R. Kim, and C.-S. Kim, “Single-Image Depth Estimation
    Based on Fourier Domain Analysis,” in *IEEE CVPR*, June 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J.-H. Lee, M. Heo, K.-R. Kim, 和 C.-S. Kim, “基于傅里叶域分析的单幅图像深度估计，” 收录于 *IEEE
    CVPR*，2018年6月。'
- en: '[95] T. Brox and J. Malik, “Large displacement optical flow: descriptor matching
    in variational motion estimation,” *IEEE PAMI*, vol. 33, no. 3, pp. 500–513, 2011.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] T. Brox 和 J. Malik, “大位移光流：在变分运动估计中的描述符匹配，” *IEEE PAMI*，第33卷，第3期，第500–513页，2011年。'
- en: '[96] P. Krähenbühl and V. Koltun, “Efficient inference in fully connected crfs
    with gaussian edge potentials,” in *NIPS*, 2011, pp. 109–117.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] P. Krähenbühl 和 V. Koltun, “在全连接CRF中使用高斯边缘势的高效推断，” 收录于 *NIPS*，2011年，第109–117页。'
- en: '[97] X. Sun, X. Mei, S. Jiao, M. Zhou, Z. Liu, and H. Wang, “Real-time local
    stereo via edge-aware disparity propagation,” *Pattern Recognition Letters*, vol. 49,
    pp. 201–206, 2014.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] X. Sun, X. Mei, S. Jiao, M. Zhou, Z. Liu, 和 H. Wang, “实时局部立体通过边缘感知视差传播，”
    *Pattern Recognition Letters*，第49卷，第201–206页，2014年。'
- en: '[98] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, and J. Kautz, “Learning
    affinity via spatial propagation networks,” in *NIPS*, 2017, pp. 1520–1530.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, 和 J. Kautz, “通过空间传播网络学习亲和力，”
    收录于 *NIPS*，2017年，第1520–1530页。'
- en: '[99] X. Cheng, P. Wang, and R. Yang, “Depth Estimation via Affinity Learned
    with Convolutional Spatial Propagation Network,” in *ECCV*, 2018, pp. 103–119.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] X. Cheng, P. Wang, 和 R. Yang, “通过卷积空间传播网络学习的亲和力进行深度估计，” 收录于 *ECCV*，2018年，第103–119页。'
- en: '[100] S. Imran, Y. Long, X. Liu, and D. Morris, “Depth Coefficients for Depth
    Completion,” in *IEEE CVPR*, June 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Imran, Y. Long, X. Liu, 和 D. Morris, “用于深度补全的深度系数，” 收录于 *IEEE CVPR*，2019年6月。'
- en: '[101] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman, “Patchmatch:
    A randomized correspondence algorithm for structural image editing,” in *ACM TOG*,
    vol. 28, no. 3, 2009, p. 24.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C. Barnes, E. Shechtman, A. Finkelstein, 和 D. B. Goldman, “Patchmatch:
    一种用于结构图像编辑的随机对应算法，” 收录于 *ACM TOG*，第28卷，第3期，2009年，第24页。'
- en: '[102] A. Seki and M. Pollefeys, “Patch Based Confidence Prediction for Dense
    Disparity Map,” in *BMVC*, vol. 2, no. 3, 2016, p. 4.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. Seki 和 M. Pollefeys, “基于Patch的稠密视差图置信度预测，” 收录于 *BMVC*，第2卷，第3期，2016年，第4页。'
- en: '[103] S. Gidaris and N. Komodakis, “Detect, Replace, Refine: Deep structured
    prediction for pixel wise labeling,” in *IEEE CVPR*, 2017, pp. 5248–5257.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] S. Gidaris 和 N. Komodakis, “检测、替换、优化：像素级标注的深度结构化预测，” 收录于 *IEEE CVPR*，2017年，第5248–5257页。'
- en: '[104] R. Haeusler, R. Nair, and D. Kondermann, “Ensemble learning for confidence
    measures in stereo vision,” in *IEEE CVPR*, 2013, pp. 305–312.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] R. Haeusler, R. Nair, 和 D. Kondermann, “用于立体视觉的置信度度量的集成学习，” 收录于 *IEEE
    CVPR*，2013年，第305–312页。'
- en: '[105] A. Spyropoulos, N. Komodakis, and P. Mordohai, “Learning to detect ground
    control points for improving the accuracy of stereo matching,” in *IEEE CVPR*,
    2014, pp. 1621–1628.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] A. Spyropoulos, N. Komodakis, 和 P. Mordohai, “学习检测地面控制点以提高立体匹配的准确性，”
    收录于 *IEEE CVPR*，2014年，第1621–1628页。'
- en: '[106] M.-G. Park and K.-J. Yoon, “Leveraging stereo matching with learning-based
    confidence measures,” in *IEEE CVPR*, 2015, pp. 101–109.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] M.-G. Park 和 K.-J. Yoon, “利用基于学习的置信度度量进行立体匹配，” 收录于 *IEEE CVPR*，2015年，第101–109页。'
- en: '[107] M. Poggi and S. Mattoccia, “Learning from scratch a confidence measure,”
    in *BMVC*, 2016.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M. Poggi 和 S. Mattoccia, “从头开始学习置信度度量，” 收录于 *BMVC*，2016年。'
- en: '[108] A. S. Wannenwetsch, M. Keuper, and S. Roth, “Probflow: Joint optical
    flow and uncertainty estimation,” in *IEEE ICCV*, 2017, pp. 1173–1182.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] A. S. Wannenwetsch, M. Keuper, 和 S. Roth, “Probflow: 联合光流和不确定性估计，” 收录于
    *IEEE ICCV*，2017年，第1173–1182页。'
- en: '[109] K. Batsos, C. Cai, and P. Mordohai, “CBMV: A Coalesced Bidirectional
    Matching Volume for Disparity Estimation,” *IEEE CVPR*, 2018.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] K. Batsos, C. Cai, 和 P. Mordohai, “CBMV: 一种用于视差估计的合并双向匹配体积，” *IEEE CVPR*，2018年。'
- en: '[110] M. Poggi, F. Tosi, and S. Mattoccia, “Quantitative evaluation of confidence
    measures in a machine learning world,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2017, pp. 5228–5237.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] M. Poggi, F. Tosi, 和 S. Mattoccia, “在机器学习世界中对置信度度量的定量评估，” 收录于 *IEEE国际计算机视觉会议论文集*，2017年，第5228–5237页。'
- en: '[111] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo,
    “Convolutional lstm network: A machine learning approach for precipitation nowcasting,”
    in *NIPS*, 2015, pp. 802–810.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong 和 W.-c. Woo, “卷积
    LSTM 网络：一种用于降水即刻预报的机器学习方法，” 在 *NIPS*，2015 年，第 802–810 页。'
- en: '[112] M. Poggi and S. Mattoccia, “Learning to predict stereo reliability enforcing
    local consistency of confidence maps,” in *IEEE CVPR*, 2017, pp. 2452–2461.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] M. Poggi 和 S. Mattoccia, “学习预测立体可靠性，强化置信图的局部一致性，” 在 *IEEE CVPR*，2017
    年，第 2452–2461 页。'
- en: '[113] J. Gast and S. Roth, “Lightweight probabilistic deep networks,” in *IEEE
    CVPR*, 2018, pp. 3369–3378.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] J. Gast 和 S. Roth, “轻量级概率深度网络，” 在 *IEEE CVPR*，2018 年，第 3369–3378 页。'
- en: '[114] F. Tosi, M. Poggi, A. Benincasa, and S. Mattoccia, “Beyond local reasoning
    for stereo confidence estimation with deep learning,” in *ECCV*, 2018, pp. 319–334.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] F. Tosi, M. Poggi, A. Benincasa 和 S. Mattoccia, “超越局部推理的深度学习立体置信度估计，”
    在 *ECCV*，2018 年，第 319–334 页。'
- en: '[115] Y. Hou, J. Kannala, and A. Solin, “Multi-View Stereo by Temporal Nonparametric
    Fusion,” in *IEEE ICCV*, 2019, pp. 2651–2660.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Y. Hou, J. Kannala 和 A. Solin, “通过时间非参数融合的多视角立体视觉，” 在 *IEEE ICCV*，2019
    年，第 2651–2660 页。'
- en: '[116] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, “SurfaceNet: an end-to-end
    3D neural network for multiview stereopsis,” *IEEE ICCV*, pp. 2307–2315, 2017.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] M. Ji, J. Gall, H. Zheng, Y. Liu 和 L. Fang, “SurfaceNet: 一种端到端的 3D 神经网络用于多视角立体视觉，”
    *IEEE ICCV*，第 2307–2315 页，2017 年。'
- en: '[117] S. Choi, S. Kim, K. Park, and K. Sohn, “Learning Descriptor, Confidence,
    and Depth Estimation in Multi-view Stereo,” in *IEEE CVPR Workshops*, 2018, pp.
    276–282.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Choi, S. Kim, K. Park 和 K. Sohn, “在多视角立体视觉中学习描述符、置信度和深度估计，” 在 *IEEE
    CVPR Workshops*，2018 年，第 276–282 页。'
- en: '[118] V. Leroy, J.-S. Franco, and E. Boyer, “Shape reconstruction using volume
    sweeping and learned photoconsistency,” in *ECCV*, 2018, pp. 781–796.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] V. Leroy, J.-S. Franco 和 E. Boyer, “利用体积扫描和学习的光度一致性进行形状重建，” 在 *ECCV*，2018
    年，第 781–796 页。'
- en: '[119] K. Luo, T. Guan, L. Ju, H. Huang, and Y. Luo, “P-MVSNet: Learning Patch-Wise
    Matching Confidence Aggregation for Multi-View Stereo,” in *IEEE ICCV*, 2019,
    pp. 10 452–10 461.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] K. Luo, T. Guan, L. Ju, H. Huang 和 Y. Luo, “P-MVSNet: 学习基于补丁的匹配置信度聚合用于多视角立体视觉，”
    在 *IEEE ICCV*，2019 年，第 10 452–10 461 页。'
- en: '[120] K. Wang and S. Shen, “Mvdepthnet: real-time multiview depth estimation
    neural network,” in *2018 3DV*, 2018, pp. 248–257.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] K. Wang 和 S. Shen, “Mvdepthnet: 实时多视角深度估计神经网络，” 在 *2018 3DV*，2018 年，第
    248–257 页。'
- en: '[121] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and
    T. Brox, “Demon: Depth and motion network for learning monocular stereo,” in *IEEE
    CVPR*, vol. 5, 2017, p. 6.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy 和
    T. Brox, “Demon: 学习单目立体视觉的深度和运动网络，” 在 *IEEE CVPR*，第 5 卷，2017 年，第 6 页。'
- en: '[122] J. T. Barron, “A more general robust loss function,” *arXiv:1701.03077*,
    2017.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] J. T. Barron, “更通用的鲁棒损失函数，” *arXiv:1701.03077*，2017 年。'
- en: '[123] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and A. A. Efros, “Learning
    dense correspondence via 3D-guided cycle consistency,” in *IEEE CVPR*, 2016, pp.
    117–126.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang 和 A. A. Efros, “通过 3D 指导的循环一致性学习稠密对应关系，”
    在 *IEEE CVPR*，2016 年，第 117–126 页。'
- en: '[124] A. Ahmadi and I. Patras, “Unsupervised convolutional neural networks
    for motion estimation,” in *ICIP*, 2016, pp. 1629–1633.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] A. Ahmadi 和 I. Patras, “用于运动估计的无监督卷积神经网络，” 在 *ICIP*，2016 年，第 1629–1633
    页。'
- en: '[125] J. Y. Jason, A. W. Harley, and K. G. Derpanis, “Back to basics: Unsupervised
    learning of optical flow via brightness constancy and motion smoothness,” in *ECCV*,
    2016, pp. 3–10.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] J. Y. Jason, A. W. Harley 和 K. G. Derpanis, “回归基础：通过亮度一致性和运动平滑性无监督学习光流，”
    在 *ECCV*，2016 年，第 3–10 页。'
- en: '[126] M. Bai, W. Luo, K. Kundu, and R. Urtasun, “Exploiting semantic information
    and deep matching for optical flow,” in *ECCV*, 2016, pp. 154–170.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] M. Bai, W. Luo, K. Kundu 和 R. Urtasun, “利用语义信息和深度匹配进行光流估计，” 在 *ECCV*，2016
    年，第 154–170 页。'
- en: '[127] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised learning
    of depth and ego-motion from video,” in *IEEE CVPR*, vol. 2, no. 6, 2017, p. 7.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] T. Zhou, M. Brown, N. Snavely 和 D. G. Lowe, “从视频中无监督学习深度和自我运动，” 在 *IEEE
    CVPR*，第 2 卷，第 6 期，2017 年，第 7 页。'
- en: '[128] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE TIP*, vol. 13,
    no. 4, pp. 600–612, 2004.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Z. Wang, A. C. Bovik, H. R. Sheikh 和 E. P. Simoncelli, “图像质量评估：从错误可见性到结构相似性，”
    *IEEE TIP*，第 13 卷，第 4 期，第 600–612 页，2004 年。'
- en: '[129] A. Tonioni, M. Poggi, S. Mattoccia, and L. Di Stefano, “Unsupervised
    adaptation for deep stereo,” in *IEEE ICCV*, 2017, pp. 1614–1622.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] A. Tonioni, M. Poggi, S. Mattoccia 和 L. Di Stefano, “深度立体视觉的无监督适应，” 在
    *IEEE ICCV*，2017 年，第 1614–1622 页。'
- en: '[130] Y. Kuznietsov, J. Stuckler, and B. Leibe, “Semi-supervised deep learning
    for monocular depth map prediction,” in *IEEE CVPR*, 2017, pp. 6647–6655.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. Kuznietsov, J. Stuckler, 和 B. Leibe，“Semi-supervised deep learning
    for monocular depth map prediction，” 发表在*IEEE CVPR*，2017，第6647–6655页。'
- en: '[131] C. Zhou, H. Zhang, X. Shen, and J. Jia, “Unsupervised learning of stereo
    matching,” in *IEEE ICCV*, 2017, pp. 1567–1575.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] C. Zhou, H. Zhang, X. Shen, 和 J. Jia，“Unsupervised learning of stereo
    matching，” 发表在*IEEE ICCV*，2017，第1567–1575页。'
- en: '[132] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki,
    “Sfm-net: Learning of structure and motion from video,” *arXiv:1704.07804*, 2017.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, 和 K. Fragkiadaki，“Sfm-net:
    Learning of structure and motion from video，” 发表在*arXiv:1704.07804*，2017。'
- en: '[133] C. Godard, O. Mac Aodha, and G. J. Brostow, “Unsupervised monocular depth
    estimation with left-right consistency,” in *CVPR*, vol. 2, no. 6, 2017, p. 7.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] C. Godard, O. Mac Aodha, 和 G. J. Brostow，“Unsupervised monocular depth
    estimation with left-right consistency，” 发表在*CVPR*，第2卷，第6期，2017，第7页。'
- en: '[134] M. Perriollat, R. Hartley, and A. Bartoli, “Monocular template-based
    reconstruction of inextensible surfaces,” *IJCV*, vol. 95, no. 2, pp. 124–137,
    2011.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] M. Perriollat, R. Hartley, 和 A. Bartoli，“Monocular template-based reconstruction
    of inextensible surfaces，” 发表在*IJCV*，第95卷，第2期，第124–137页，2011。'
- en: '[135] X. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia, “GeoNet: Geometric Neural
    Network for Joint Depth and Surface Normal Estimation,” in *IEEE CVPR*, June 2018.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] X. Qi, R. Liao, Z. Liu, R. Urtasun, 和 J. Jia，“GeoNet: Geometric Neural
    Network for Joint Depth and Surface Normal Estimation，” 发表在*IEEE CVPR*，2018年6月。'
- en: '[136] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *IEEE CVPR*, 2017, pp. 2881–2890.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. Zhao, J. Shi, X. Qi, X. Wang, 和 J. Jia，“Pyramid scene parsing network，”
    发表在*IEEE CVPR*，2017，第2881–2890页。'
- en: '[137] J. Pang, W. Sun, C. Yang, J. Ren, R. Xiao, J. Zeng, and L. Lin, “Zoom
    and learn: Generalizing deep stereo matching to novel domains,” in *IEEE CVPR*,
    2018, pp. 2070–2079.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. Pang, W. Sun, C. Yang, J. Ren, R. Xiao, J. Zeng, 和 L. Lin，“Zoom and
    learn: Generalizing deep stereo matching to novel domains，” 发表在*IEEE CVPR*，2018，第2070–2079页。'
- en: '[138] A. Tonioni, M. Poggi, S. Mattoccia, and L. Di Stefano, “Unsupervised
    domain adaptation for depth prediction from images,” *IEEE TPAMI*, 2019.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Tonioni, M. Poggi, S. Mattoccia, 和 L. Di Stefano，“Unsupervised domain
    adaptation for depth prediction from images，” 发表在*IEEE TPAMI*，2019。'
- en: '[139] X. Zhou, Q. Huang, X. Sun, X. Xue, and Y. Wei, “Weakly-supervised transfer
    for 3d human pose estimation in the wild,” *arXiv:1704.02447*, 2017.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] X. Zhou, Q. Huang, X. Sun, X. Xue, 和 Y. Wei，“Weakly-supervised transfer
    for 3d human pose estimation in the wild，” 发表在*arXiv:1704.02447*，2017。'
- en: '[140] Z. Zhang, C. Xu, J. Yang, Y. Tai, and L. Chen, “Deep hierarchical guidance
    and regularization learning for end-to-end depth estimation,” *Pattern Recognition*,
    vol. 83, pp. 430–442, 2018.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Z. Zhang, C. Xu, J. Yang, Y. Tai, 和 L. Chen，“Deep hierarchical guidance
    and regularization learning for end-to-end depth estimation，” 发表在*Pattern Recognition*，第83卷，第430–442页，2018。'
- en: '[141] M. Poggi, F. Aleotti, F. Tosi, and S. Mattoccia, “Towards real-time unsupervised
    monocular depth estimation on CPU,” in *IROS*, 2018, pp. 5848–5854.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] M. Poggi, F. Aleotti, F. Tosi, 和 S. Mattoccia，“Towards real-time unsupervised
    monocular depth estimation on CPU，” 发表在*IROS*，2018，第5848–5854页。'
- en: '[142] Y. Zhong, H. Li, and Y. Dai, “Open-world stereo video matching with deep
    RNN,” in *ECCV*, 2018, pp. 101–116.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Y. Zhong, H. Li, 和 Y. Dai，“Open-world stereo video matching with deep
    RNN，” 发表在*ECCV*，2018，第101–116页。'
- en: '[143] A. Tonioni, O. Rahnama, T. Joy, L. D. Stefano, T. Ajanthan, and P. H.
    Torr, “Learning to Adapt for Stereo,” in *IEEE CVPR*, 2019, pp. 9661–9670.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] A. Tonioni, O. Rahnama, T. Joy, L. D. Stefano, T. Ajanthan, 和 P. H. Torr，“Learning
    to Adapt for Stereo，” 发表在*IEEE CVPR*，2019，第9661–9670页。'
- en: '[144] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *Proceedings of the 34th International Conference
    on Machine Learning-Volume 70*, 2017, pp. 1126–1135.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] C. Finn, P. Abbeel, 和 S. Levine，“Model-agnostic meta-learning for fast
    adaptation of deep networks，” 发表在*Proceedings of the 34th International Conference
    on Machine Learning-Volume 70*，2017，第1126–1135页。'
- en: '[145] A. Atapour-Abarghouei and T. P. Breckon, “Real-Time Monocular Depth Estimation
    Using Synthetic Data With Domain Adaptation via Image Style Transfer,” in *IEEE
    CVPR*, 2018, pp. 2800–2810.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] A. Atapour-Abarghouei 和 T. P. Breckon，“Real-Time Monocular Depth Estimation
    Using Synthetic Data With Domain Adaptation via Image Style Transfer，” 发表在*IEEE
    CVPR*，2018，第2800–2810页。'
- en: '[146] C. Zheng, T.-J. Cham, and J. Cai, “T2Net: Synthetic-to-Realistic Translation
    for Solving Single-Image Depth Estimation Tasks,” in *ECCV*, 2018, pp. 767–783.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] C. Zheng, T.-J. Cham, 和 J. Cai，“T2Net: Synthetic-to-Realistic Translation
    for Solving Single-Image Depth Estimation Tasks，” 发表在*ECCV*，2018，第767–783页。'
- en: '[147] S. Zhao, H. Fu, M. Gong, and D. Tao, “Geometry-Aware Symmetric Domain
    Adaptation for Monocular Depth Estimation,” in *IEEE CVPR*, 2019, pp. 9788–9798.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. Zhao, H. Fu, M. Gong, 和 D. Tao，“Geometry-Aware Symmetric Domain Adaptation
    for Monocular Depth Estimation，” 发表在*IEEE CVPR*，2019，第9788–9798页。'
- en: '[148] J. Nath Kundu, P. Krishna Uppala, A. Pahuja, and R. Venkatesh Babu, “AdaDepth:
    Unsupervised Content Congruent Adaptation for Depth Estimation,” in *IEEE CVPR*,
    2018, pp. 2656–2665.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Nath Kundu, P. Krishna Uppala, A. Pahuja, 和 R. Venkatesh Babu，“AdaDepth:
    用于深度估计的无监督内容一致适应”，发表于 *IEEE CVPR*，2018年，第2656–2665页。'
- en: '[149] T. Saikia, Y. Marrakchi, A. Zela, F. Hutter, and T. Brox, “AutoDispNet:
    Improving Disparity Estimation With AutoML,” in *IEEE ICCV*, 2019, pp. 1812–1823.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] T. Saikia, Y. Marrakchi, A. Zela, F. Hutter, 和 T. Brox，“AutoDispNet:
    通过AutoML改进视差估计”，发表于 *IEEE ICCV*，2019年，第1812–1823页。'
- en: '[150] F. Hutter, L. Kotthoff, and J. Vanschoren, “Automated machine learning-methods,
    systems, challenges,” 2019.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] F. Hutter, L. Kotthoff, 和 J. Vanschoren，“自动化机器学习—方法、系统、挑战”，2019年。'
- en: '[151] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture
    search,” in *International Conference on Learning Representations*, 2019.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] H. Liu, K. Simonyan, 和 Y. Yang，“Darts: 可微分架构搜索”，发表于 *国际学习表示会议*，2019年。'
- en: '[152] “Robust vision challenge,” *http://www.robustvision.net/*.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] “鲁棒视觉挑战，” *http://www.robustvision.net/*。'
- en: '[153] X. Han, H. Laga, and M. Bennamoun, “Image-based 3d object reconstruction:
    State-of-the-art and trends in the deep learning era,” *IEEE PAMI*, 2020.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] X. Han, H. Laga, 和 M. Bennamoun，“基于图像的3D物体重建：深度学习时代的现状与趋势”， *IEEE PAMI*，2020年。'
- en: '[154] K. O. Stanley, J. Clune, J. Lehman, and R. Miikkulainen, “Designing neural
    networks through neuroevolution,” *Nature Machine Intelligence*, vol. 1, no. 1,
    pp. 24–35, 2019.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] K. O. Stanley, J. Clune, J. Lehman, 和 R. Miikkulainen，“通过神经进化设计神经网络”，
    *自然机器智能*，第1卷，第1期，第24–35页，2019年。'
- en: '[155] G. Bingham, W. Macke, and R. Miikkulainen, “Evolutionary optimization
    of deep learning activation functions,” *arXiv preprint arXiv:2002.07224*, 2020.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] G. Bingham, W. Macke, 和 R. Miikkulainen，“深度学习激活函数的进化优化”， *arXiv 预印本 arXiv:2002.07224*，2020年。'
- en: '[156] B. Haefner, Z. Ye, M. Gao, T. Wu, Y. Queau, and D. Cremers, “Variational
    Uncalibrated Photometric Stereo Under General Lighting,” in *IEEE ICCV*, 2019.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] B. Haefner, Z. Ye, M. Gao, T. Wu, Y. Queau, 和 D. Cremers，“一般光照下的变分非校准光度立体视觉”，发表于
    *IEEE ICCV*，2019年。'
- en: '[157] Q. Zheng, Y. Jia, B. Shi, X. Jiang, L.-Y. Duan, and A. C. Kot, “SPLINE-Net:
    Sparse Photometric Stereo Through Lighting Interpolation and Normal Estimation
    Networks,” in *IEEE ICCV*, 2019.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Q. Zheng, Y. Jia, B. Shi, X. Jiang, L.-Y. Duan, 和 A. C. Kot，“SPLINE-Net:
    通过光照插值和法线估计网络进行稀疏光度立体视觉”，发表于 *IEEE ICCV*，2019年。'
- en: '| ![[Uncaptioned image]](img/16e086e850516e7c4f2e78643ba70a3f.png) | Hamid
    Laga received the PhD degrees in Computer Science from Tokyo Institute of Technology
    in 2006\. He is currently an Associate Professor at Murdoch University (Australia)
    and an Adjunct Associate Professor with the Phenomics and Bioinformatics Research
    Centre (PBRC) of the University of South Australia. His research interests span
    various fields of machine learning, computer vision, computer graphics, and pattern
    recognition, with a special focus on the 3D reconstruction, modeling and analysis
    of static and deformable 3D objects, and on image analysis and big data in agriculture
    and health. He is the recipient of the Best Paper Awards at SGP2017, DICTA2012,
    and SMI2006. |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/16e086e850516e7c4f2e78643ba70a3f.png) | Hamid Laga于2006年获得东京工业大学计算机科学博士学位。目前他是澳大利亚墨尔本大学的副教授，同时也是南澳大利亚大学表型学与生物信息学研究中心（PBRC）的兼职副教授。他的研究兴趣涵盖机器学习、计算机视觉、计算机图形学和模式识别的多个领域，特别关注静态和可变形3D物体的3D重建、建模和分析，以及农业和健康领域的图像分析和大数据。他曾获得SGP2017、DICTA2012和SMI2006的最佳论文奖。
    |'
- en: '| ![[Uncaptioned image]](img/4a3320fd363a27b311e54d30ed236cfa.png) | Laurent
    Valentin Jospin Laurent Valentin Jospin is a young research student in the field
    of computer vision. His main research interests include 3d reconstruction, sampling
    and image acquisition strategies, computer vision applied to robotic navigation
    and computer vision applied to environmental sciences. Holder of a master of science
    in environmental engineering from EPFL since 2017 with a thesis on accuracy prediction
    in aerial mapping, his research career started as intern in two EPFL labs, publishing
    his first three papers in the process, before starting a PhD in computer science
    at the University of Western Australia in 2019\. His thesis project focus on real
    time 3D reconstruction with different computer vision techniques. |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/4a3320fd363a27b311e54d30ed236cfa.png) | Laurent Valentin Jospin
    是计算机视觉领域的一名年轻研究生。他的主要研究兴趣包括 3D 重建、采样和图像获取策略、计算机视觉在机器人导航和环境科学中的应用。自 2017 年起，他持有
    EPFL 环境工程硕士学位，论文主题为航空制图中的准确性预测。他的研究生涯从 EPFL 两个实验室的实习生开始，在此过程中发表了前三篇论文，然后于 2019
    年在西澳大学开始了计算机科学博士学位研究。他的论文项目专注于使用不同计算机视觉技术进行实时 3D 重建。'
- en: '| ![[Uncaptioned image]](img/57419594ca27d67ab20d45120fcb016a.png) | Faird
    Boussaid received the M.S. and Ph.D. degrees in microelectronics from the National
    Institute of Applied Science (INSA), Toulouse, France, in 1996 and 1999 respectively.
    He joined Edith Cowan University, Perth, Australia, as a Postdoctoral Research
    Fellow, and a Member of the Visual Information Processing Research Group in 2000\.
    He joined the University of Western Australia, Crawley, Australia, in 2005, where
    he is currently a Professor. His current research interests include neuromorphic
    engineering, smart sensors, and machine learning. |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/57419594ca27d67ab20d45120fcb016a.png) | Faird Boussaid 于 1996
    年和 1999 年分别在法国图卢兹应用科学国家学院（INSA）获得微电子学硕士和博士学位。2000 年，他加入了澳大利亚珀斯的 Edith Cowan University，担任博士后研究员，并成为视觉信息处理研究小组的成员。2005
    年，他加入了澳大利亚克劳利的西澳大学，目前担任教授。他的当前研究兴趣包括类脑工程、智能传感器和机器学习。 |'
- en: '| ![[Uncaptioned image]](img/084657efb02b745d94d8b2dee70502dd.png) | Mohammed
    Bennamoun is Winthrop Professor in the Department of Computer Science and Software
    Engineering at UWA and is a researcher in computer vision, machine/deep learning,
    robotics, and signal/speech processing. He has published 4 books (available on
    Amazon, 1 edited book, 1 Encyclopedia article, 14 book chapters, 120+ journal
    papers, 250+ conference publications, 16 invited and keynote publications. His
    h-index is 47 and his number of citations is 10,000+ (Google Scholar). He was
    awarded 65+ competitive research grants, from the Australian Research Council,
    and numerous other Government, UWA and industry Research Grants. He successfully
    supervised +26 PhD students to completion. He won the Best Supervisor of the Year
    Award at QUT (1998), and received award for research supervision at UWA (2008
    and 2016) and Vice-Chancellor Award for mentorship (2016). He delivered conference
    tutorials at major conferences, including: IEEE CVPR 2016, Interspeech 2014, IEEE
    ICASSP, and ECCV. He was also invited to give a Tutorial at an International Summer
    School on Deep Learning (DeepLearn 2017). |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/084657efb02b745d94d8b2dee70502dd.png) | Mohammed Bennamoun
    是西澳大学计算机科学与软件工程系的 Winthrop 教授，并在计算机视觉、机器/深度学习、机器人技术以及信号/语音处理领域进行研究。他已出版 4 本书（可在亚马逊上购买），1
    本编辑书，1 篇百科全书文章，14 个书籍章节，120+ 篇期刊论文，250+ 篇会议论文，16 篇邀请和主旨演讲论文。他的 h-index 为 47，引用次数超过
    10,000（Google Scholar）。他获得了 65+ 项来自澳大利亚研究委员会及其他政府、西澳大学和行业研究资助的竞争性研究资助。他成功指导了 26
    名博士生完成学业。他曾获得 QUT 的年度最佳导师奖（1998 年），并在西澳大学获得研究指导奖（2008 年和 2016 年）以及副校长奖（2016 年）。他在包括
    IEEE CVPR 2016、Interspeech 2014、IEEE ICASSP 和 ECCV 在内的主要会议上进行了大会教程，并应邀在国际深度学习暑期学校（DeepLearn
    2017）上做了教程演讲。 |'
