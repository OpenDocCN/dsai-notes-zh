- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:49:55'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2111.05193] A Survey on Green Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.05193](https://ar5iv.labs.arxiv.org/html/2111.05193)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Green Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jingjing Xu
  prefs: []
  type: TYPE_NORMAL
- en: ByteDance AI Lab
  prefs: []
  type: TYPE_NORMAL
- en: xujingjing.melody@bytedance.com
  prefs: []
  type: TYPE_NORMAL
- en: 'jingjingxu@pku.edu.cn &Wangchunshu Zhou¹¹footnotemark: 1'
  prefs: []
  type: TYPE_NORMAL
- en: ByteDance AI Lab
  prefs: []
  type: TYPE_NORMAL
- en: 'zhouwangchunshu.7@bytedance.com &Zhiyi Fu¹¹footnotemark: 1'
  prefs: []
  type: TYPE_NORMAL
- en: Peking University
  prefs: []
  type: TYPE_NORMAL
- en: ypfzy@pku.edu.cn &Hao Zhou
  prefs: []
  type: TYPE_NORMAL
- en: ByteDance AI Lab
  prefs: []
  type: TYPE_NORMAL
- en: zhouhao.nlp@bytedance.com &Lei Li
  prefs: []
  type: TYPE_NORMAL
- en: University of California, Santa Barbara
  prefs: []
  type: TYPE_NORMAL
- en: lilei@ucsb.edu Equal ContributionThis work is done during internship at ByteDance
    AI Lab.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, larger and deeper models are springing up and continuously
    pushing state-of-the-art (SOTA) results across various fields like natural language
    processing (NLP) and computer vision (CV). However, despite promising results,
    it needs to be noted that the computations required by SOTA models have been increased
    at an exponential rate. Massive computations not only have a surprisingly large
    carbon footprint but also have negative effects on research inclusiveness and
    deployment on real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Green deep learning is an increasingly hot research field that appeals to researchers
    to pay attention to energy usage and carbon emission during model training and
    inference. The target is to yield novel results with lightweight and efficient
    technologies. Many technologies can be used to achieve this goal, like model compression
    and knowledge distillation. This paper focuses on presenting a systematic review
    of the development of Green deep learning technologies. We classify these approaches
    into four categories: (1) compact networks, (2) energy-efficient training strategies,
    (3) energy-efficient inference approaches, and (4) efficient data usage. For each
    category, we discuss the progress that has been achieved and the unresolved challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[1 Introduction](#Ch1 "In A Survey on Green Deep Learning")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[1.1 Deep Learning](#Ch1.S1 "In Chapter 1 Introduction ‣ A Survey on Green
    Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[1.2 Green Deep Learning](#Ch1.S2 "In Chapter 1 Introduction ‣ A Survey on
    Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[1.2.1 Definition](#Ch1.S2.SS1 "In 1.2 Green Deep Learning ‣ Chapter 1 Introduction
    ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[1.2.2 Measure](#Ch1.S2.SS2 "In 1.2 Green Deep Learning ‣ Chapter 1 Introduction
    ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[1.2.3 Broader Impact](#Ch1.S2.SS3 "In 1.2 Green Deep Learning ‣ Chapter 1
    Introduction ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[1.3 Outline of the Survey](#Ch1.S3 "In Chapter 1 Introduction ‣ A Survey on
    Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2 Compact Architecture](#Ch2 "In A Survey on Green Deep Learning")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.1 Component Design](#Ch2.S1 "In Chapter 2 Compact Architecture ‣ A Survey
    on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.1.1 Compact Convolution](#Ch2.S1.SS1 "In 2.1 Component Design ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.1.2 Efficient Attention](#Ch2.S1.SS2 "In 2.1 Component Design ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.1.3 Lightweight Softmax](#Ch2.S1.SS3 "In 2.1 Component Design ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.1.4 Compact Embeddings](#Ch2.S1.SS4 "In 2.1 Component Design ‣ Chapter 2
    Compact Architecture ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.2 Component Assembling](#Ch2.S2 "In Chapter 2 Compact Architecture ‣ A Survey
    on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.2.1 Memory Sharing](#Ch2.S2.SS1 "In 2.2 Component Assembling ‣ Chapter 2
    Compact Architecture ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.2.2 Static Weight Sharing](#Ch2.S2.SS2 "In 2.2 Component Assembling ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.2.3 Dynamic Weight Sharing](#Ch2.S2.SS3 "In 2.2 Component Assembling ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.2.4 Deployment Sharing](#Ch2.S2.SS4 "In 2.2 Component Assembling ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2.3 Compact-architecture Search](#Ch2.S3 "In Chapter 2 Compact Architecture
    ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3 Energy-Efficient Training](#Ch3 "In A Survey on Green Deep Learning")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1 Initialization](#Ch3.S1 "In Chapter 3 Energy-Efficient Training ‣ A Survey
    on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.1 Random Initialization](#Ch3.S1.SS1 "In 3.1 Initialization ‣ Chapter
    3 Energy-Efficient Training ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1.2 Pre-trained Models for Initialization](#Ch3.S1.SS2 "In 3.1 Initialization
    ‣ Chapter 3 Energy-Efficient Training ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2 Normalization](#Ch3.S2 "In Chapter 3 Energy-Efficient Training ‣ A Survey
    on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.3 Progressive Training](#Ch3.S3 "In Chapter 3 Energy-Efficient Training
    ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.4 Efficient Hyper-parameter Optimization](#Ch3.S4 "In Chapter 3 Energy-Efficient
    Training ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4 Energy-Efficient Inference](#Ch4 "In A Survey on Green Deep Learning")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1 Model Pruning](#Ch4.S1 "In Chapter 4 Energy-Efficient Inference ‣ A Survey
    on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2 Low-rank Factorization](#Ch4.S2 "In Chapter 4 Energy-Efficient Inference
    ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.3 Quantization](#Ch4.S3 "In Chapter 4 Energy-Efficient Inference ‣ A Survey
    on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.4 Knowledge Distillation](#Ch4.S4 "In Chapter 4 Energy-Efficient Inference
    ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5 Efficient Data Usage](#Ch5 "In A Survey on Green Deep Learning")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.1 Active Learning](#Ch5.S1 "In Chapter 5 Efficient Data Usage ‣ A Survey
    on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2 Pre-training as Few-shot Learners](#Ch5.S2 "In Chapter 5 Efficient Data
    Usage ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.1 Self-supervised Learning](#Ch5.S2.SS1 "In 5.2 Pre-training as Few-shot
    Learners ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.2 Contrastive Learning](#Ch5.S2.SS2 "In 5.2 Pre-training as Few-shot Learners
    ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5.2.3 Prompt Learning](#Ch5.S2.SS3 "In 5.2 Pre-training as Few-shot Learners
    ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep Learning")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6 Conclusions and Future Directions](#Ch6 "In A Survey on Green Deep Learning")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning, based on deep neural networks, is part of machine learning methods.
    In this chapter, we first introduce the development of deep learning in section [1.1](#Ch1.S1
    "1.1 Deep Learning ‣ Chapter 1 Introduction ‣ A Survey on Green Deep Learning").
    Then, we elucidate what is Green deep learning, why Green deep learning matters,
    and how to evaluate the “greenness” of deep learning in section [1.2](#Ch1.S2
    "1.2 Green Deep Learning ‣ Chapter 1 Introduction ‣ A Survey on Green Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While a decade ago, artificial intelligence (AI) mainly focuses on shallow models,
    like structure perceptrons (McDonald et al., [2010](#bib.bib217); Huang et al.,
    [2012](#bib.bib135); Li & Ji, [2014](#bib.bib184)) and conditional random fields (Ghosh
    et al., [2011](#bib.bib93); Sutton & McCallum, [2012](#bib.bib312); Zheng et al.,
    [2015](#bib.bib397)). These shallow models only require limited computations.
    Most AI approaches can be deployed on CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, powerful GPUs have become increasingly accessible, making it
    possible to deploy larger models, which accelerates the development of deep learning.
    The ideas of widely-used deep learning models have been proposed in the 1990s,
    such as convolutional neural networks (CNNs) (LeCun et al., [1998](#bib.bib173))
    and long-short term networks (LSTMs) (Hochreiter & Schmidhuber, [1997](#bib.bib129)).
    Confined by hardware capacity and large-scale data resources, these models began
    to be popular until the past few years. Collobert et al. ([2011](#bib.bib55))
    proposed the first systematic deep learning framework for NLP tasks. Krizhevsky
    et al. ([2012](#bib.bib164)) proposed a convolution-based deep network, which
    ranked the first in the image classification challenge. These studies are good
    pioneers that motivate AI participants to dive into deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/089b10984a83f7e8900a82f6cf67d136.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: The ratio of papers using neural networks in ACL, a top-tier NLP
    conference, from 2010 to 2020\. We manually count how many papers use neural networks
    among sampled 50 papers.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning methods currently have become a prime choice for AI. The promising
    prospect of neural networks attracts more AI participants to engage with deep
    learning, in the meanwhile, deep learning is highly competitive in yielding profits
    when applied to real-world applications. The industry continuously develops more
    efficient hardware and launches better programming platforms, such as Theano,
    Caffe, MxNet, Tensorflow, and Pytorch. Advanced infrastructures further enable
    AI participants to develop stronger deep models. Therefore, deep learning takes
    a high-speed train since 2010\. To visualize the transition process from shallow
    models to deep networks in the AI community, we analyze papers from a top-tier
    AI conference, ACL, starting from 2010 to 2020\. We randomly select 50 works from
    each year and manually count the number of papers using deep learning. As we can
    see from Figure [1.1](#Ch1.F1 "Figure 1.1 ‣ 1.1 Deep Learning ‣ Chapter 1 Introduction
    ‣ A Survey on Green Deep Learning"), the number of papers using neural networks
    is growing fast from 2012 to 2020\. All papers adopt deep networks as backbone
    since 2020\. From then, the AI field is fully entered into the age of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the age of deep learning, a hot direction is to obtain SOTA results. Following Schwartz
    et al. ([2020a](#bib.bib284)), we call such research trend as Red AI. Recently,
    researchers have noticed that it was harder to gain an advantage over SOTA results.
    For traditional AI fields, like CV and NLP, the improvements achieved by new AI
    models/algorithms are diminishing. Many popular research benchmarks are reaching
    their performance ceiling. Figure [1.2](#Ch1.F2 "Figure 1.2 ‣ 1.1 Deep Learning
    ‣ Chapter 1 Introduction ‣ A Survey on Green Deep Learning") and [1.3](#Ch1.F3
    "Figure 1.3 ‣ 1.1 Deep Learning ‣ Chapter 1 Introduction ‣ A Survey on Green Deep
    Learning") list several examples showing how the returns of deep learning are
    diminishing over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The trend of red AI requires massive computations to achieve better results.
    For example, as reported in Schwartz et al. ([2020a](#bib.bib284)), the amount
    of computations used to train deep learning models has increased 300,000x in 6
    years. These computations not only cause expensive financial costs but also contribute
    to an excessive carbon footprint. The former harms AI inclusiveness and the latter
    harms our environment. We classify the computation source required by deep learning
    into the following three categories: model size, parameter tuning, and training
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/473283eafe2ebd8dd8edc4f0e7c77266.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Results of WMT English-German translation from 2016 to 2020\. As
    we can see, the recent published results are reaching into the ceiling. The data
    is collected from [https://paperswithcode.com](https://paperswithcode.com).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0bc4436cd4d87966d464ae785d22852e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: The image classification results on ImageNet from 2011 to 2021\.
    As we can see, the recent published results are reaching into the ceiling. The
    data is collected from [https://paperswithcode.com](https://paperswithcode.com).'
  prefs: []
  type: TYPE_NORMAL
- en: 1) With access to large-scale data resources, increasing model size is the simplest
    way to improve results. For example, on WMT English-German translation, the performance
    can be increased from 27.3 to 28.4 while the size of the machine translation models
    is increased from 60M (Transformer-base) to 180M (Transformer-large). To achieve
    better results, more and more AI participants would like to increase model size
    as much as possible, especially for rich organizations. For example, researchers
    from OpenAI first pre-trained a large-scale text generation model, called GPT-3
    with 175B parameters. It shows that a super-large model can generate human-like
    texts. However, according to Strubell et al. ([2019](#bib.bib307)), training GPT-3
    can emit almost 500M carbons, almost emissions of five cars in their lifetime.
    These studies are key milestones in a long run. However, we believe that larger
    models are not always better if we consider “invisible” computation cost. We are
    still concerned about “the crazy love” to super-large models no matter whether
    the increased computations bring significant benefits. In addition, bigger models
    largely increase the burden of inference serving. Amazon estimates that 90% of
    production ML infrastructure costs are for inference, not training (Jain et al.,
    [2019](#bib.bib143)).
  prefs: []
  type: TYPE_NORMAL
- en: 2) Model experiments are also an overlooked computation consumer. To verify
    the effectiveness of a new model/algorithm, AI participants usually conduct massive
    experiments, including model/algorithm implementation, baseline re-implementation,
    and hyper-parameter tuning. First, baseline re-implementation is an redundant
    computation source. For example, the original Transformer paper has 18K citations.
    Assume each citation represents a single implementation. Each re-implementation
    takes 100 hours on a single GPU (following the cost of running a Transformer-base
    model on English-German translation). It means that only baseline re-implementation
    on a single dataset can take 1.8M GPU hours. In addition, hyper-parameter tuning
    is an overlooked computation source. We design a simple questionnaire to ask the
    ratio of experiments experiments for hyper-parameter tuning while developing a
    new model/algorithm, answered by 64 AI specialists, including researchers and
    engineers. All of the surveyed choose tuning hyper-parameters. To be specific,
    10.7%, 32.1%, 35.7%, 21.4% individuals take 80%-100%, 50%-80%, 30%-50%, 0%-30%
    experiments to tune hyper-parameters. In sum, 42.8% individuals spend over 50%
    experiments for hyper-parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Starting from shallow models, it is popular to increase the amount of training
    data to achieve better generalization ability, especially in semi-supervised settings.
    One recent hot topic is pre-training a super-large model on billions of raw data.
    In the NLP field, ELMo (Peters et al., [2018](#bib.bib241)) is the first well-known
    work to explore large-scale pre-training. Following ELMo, BERT pre-trains a Transformer
    encoder on 3 billion word pieces. Researchers from OpenAI recently proposed GPT-3,
    a generative model pre-trained on 45TB data. These massive training examples largely
    increase the training costs compared to previous shallow models.
  prefs: []
  type: TYPE_NORMAL
- en: In all, the trend of Red AI brings heavy computation costs. These computations
    exacerbate the research inequality, making it difficult to involve all researchers
    in Red AI. Furthermore, massive computation requirements bring huge carbon emissions.
    To address these problems, Green deep learning, or Green AI, was first proposed
    by Schwartz et al. ([2020a](#bib.bib284)) to encourage the AI community to focus
    more on energy costs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Green Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we mainly describe what is Green deep learning, how to evaluate
    “greenness” in deep learning, and why Green deep learning matters.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Green learning, a term first proposed by Schwartz et al. ([2020a](#bib.bib284)),
    is gaining mounting attention. Formally, Green deep learning, or Green AI, appeals
    to researchers to obtain novel results without increasing computational cost rather,
    ideally reducing it. Unlike Red AI pushing state-of-the-art results at any cost,
    Green deep learning encourages AI participants to achieve comparable or better
    results using as few computations as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Measure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Green deep learning, computations are important evaluation metrics. Currently,
    the whole community lacks a comprehensive and widely-accepted measure to evaluate
    computations because multiple aspects can attribute to computations, including
    model size, training examples, and so on. A comprehensive measure is expected
    for a fair comparison. Here we list several computation measures and discuss their
    merits and demerits.
  prefs: []
  type: TYPE_NORMAL
- en: Running time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some studies adopt the total training time as a kind of computations measure.
    If all models/algorithms adopt the same hardware and software settings, it is
    the most natural measure to evaluate training/inference computations. However,
    since running time heavily relies on infrastructure settings, it is not suitable
    for comparing models running on different infrastructures. Even so, we still encourage
    AI participants to report the running time for an intuitive understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Carbon emission
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Carbon emission is the most direct approach to evaluate environmental effects.
    In order to quantify carbon emissions, Lacoste et al. ([2019](#bib.bib167)) used
    $CO_{2}$-equivalents ($CO_{2}eq$) as the amount of $CO_{2}$ which would have the
    equivalent global warming impact. However, the main challenge of this measure
    lies in accurate estimation. First, computations via electricity consumption are
    easily influenced by local infrastructures. Furthermore, it is hard for AI participants
    to estimate the amount of $CO_{2}$ if they do not run experiments on well-known
    cloud platforms. Therefore, it is also not suitable as a standard metric to compare
    different models running on different regions and different computing infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: Model size
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model size is also an important factor in deciding training and inference
    costs. We encourage researchers to report model size to corporate with other measures
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: FLOPs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Floating-Point Operations (FLOPs) count the number of works required for running
    a model when executing a specific instance. Previous studies usually adopt this
    metric to evaluate efficiency. FLOPs are almost independent of hardware and software
    platforms, being the simplest measure to conduct a fair comparison between different
    models. However, FLOPs are theoretical values, and there is a gap between FLOPs
    and running time. In addition to the total amount of works (FLOPs), the degree
    of parallelism also affects the running time.
  prefs: []
  type: TYPE_NORMAL
- en: According to these measures, we summarize evaluation strategies towards fair
    comparison and intuitive understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Fair measure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generally speaking, AI participants prefer well-performing models/algorithms
    with fewer computations. Therefore, it is an important question to fairly compare
    computations required for training and inference. We strongly suggest reporting
    FLOPs during model training and inference. Last but not least, to evaluate the
    wasted and redundant computations required for developing a new model/algorithm,
    we also encourage researchers to report the total FLOPs during all experiments,
    including but not limited to parameter tuning and baseline implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitive understanding
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To increase the intuitively understanding about computations, we encourage researchers
    to report running time, model size, and carbon emission as optional results.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 Broader Impact
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, Green deep learning can help deep learning empower real society applications
    better. In the past decade, deep learning continuously pushed state-of-the-art
    results on research benchmarks, like machine translation, image classification,
    and so on. In the research community, the cost of deep learning seems to be nothing
    compared to energy consumption of all human activities. Nowadays, deep learning
    is widely applied to real society tasks, such as auto-driving, face recognition,
    drug discovery, and so on. Once deep learning is involved in large-scale applications,
    the cost of deep learning will be multiplied hundreds of millions of times. Furthermore,
    some edge applications, like mobiles with extremely few computation resources,
    also require Green deep learning. Therefore, Green deep learning is a necessary
    research direction in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Second, Green deep learning can largely improve AI inclusiveness. We note the
    contributions of rich organizations for pushing higher results on many downstream
    tasks. Meanwhile, we also notice the dilemma of researchers from academics and
    developing countries on engaging Red AI research. Most researchers only have limited
    computations, which could not support them to develop super-large models with
    state-of-the-art results. Unfortunately, compared to ideas with state-of-the-art
    results, novel and innovative ideas without state-of-the-art results are losing
    their sounds. For example, news media would like to report studies with state-of-the-art
    results. The attractive propaganda of No.1 also pushes the rich organizations
    to pour more money on super-big models. These cases may confuse researchers on
    how they engage in deep learning research without strong financial support. We
    argue that state-of-the-art results are good, but not the only criteria to evaluate
    the quality of new models/algorithms. We encourage rich organizations to continuously
    explore data and model boundaries, also encourage the AI community to pay attention
    to innovative ideas. In fact, deep learning struggles many years until it outperforms
    shallow models with feature engineers. We believe that the development of AI should
    be diverse. Green deep learning can improve AI inclusion and motivate more AI
    participants to explore deep learning possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Outline of the Survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a long-term goal to develop tiny yet strong networks for all AI researchers
    and engineers. Driven by this target, several popular tiny networks have been
    proposed (Howard et al., [2017](#bib.bib132); Chollet, [2017](#bib.bib50); Tan
    & Le, [2019](#bib.bib319)). For example, MobileNet proposed by Howard et al. ([2017](#bib.bib132))
    is an efficient architecture based on depthwise separable convolution. Similar
    idea has been adopted at Xception (Chollet, [2017](#bib.bib50)). Recently, to
    explore extremely tiny networks, advanced training/inference/network surgery methods
    have been proposed. For example, EdgeBERT (Tambe et al., [2021](#bib.bib318))
    is proposed to build an extremely tiny network that can run on IoT devices. It
    adopts advanced methods like quantization, pruning, early exit to further reduce
    model parameters and running computations.
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we give a systematic review of Green deep learning technologies.
    We first build a green technology taxonomy and then classify the related technologies
    into four categories, including compact networks, energy-saving training strategies,
    energy-saving inference, and efficient data usage. In each category, we review
    the current progress on Green technologies and explore potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that building a Green technology taxonomy is challenging
    since there lacks a unified standard measurement. For example, BERT requires massive
    computations during training. If we only consider training costs, BERT can not
    be treated as a Green technology. However, BERT can improve downstream performance
    with fewer training examples. If we consider its transfer ability, BERT is absolutely
    a Green technology. Therefore, whether a technology is defined as Green or not
    is open to doubt. We will try our best to avoid giving a biased definition. If
    a technology has the potential to reduce the costs of deep learning, we will include
    it in the green technology taxonomy. We review Green deep learning technologies
    in the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compact Architecture Design. This part focuses on small networks. We split this
    chapter into two sub-chapters, i.e., component design, and component assembling.
    The component design focuses on subtle components with competitive results but
    much fewer computations. Component assembling describes how to build a network
    efficiently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Energy-efficient Training Strategies. Previous studies have proposed several
    efficient training approaches. In this survey, we classify these studies into
    four categories, including initialization, normalization, progressive training,
    and efficient AutoML.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Energy-efficient Inference. In this chapter, we describe approaches that aim
    to get a smaller yet comparable network from a larger network for efficient inference,
    including model pruning, low-rank factorization, quantization, distillation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Efficient Data Usage. This chapter lists algorithms that leverage training
    data efficiently. We focus on two popular directions: active learning and pre-trained
    models as few-shot learners.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 2 Compact Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Developing efficient neural networks has been a long-standing goal towards Green
    AI. In this survey, we define Green networks as neural networks that are efficient
    in terms of computational costs. We can generate compact networks via subtle design,
    model surgery, and network search. Subtle design means that we can manually define
    efficient architectures requiring fewer computations. Model surgery means that
    we can generate compact architectures from a larger model via parameter reduction.
    In this chapter, we focus on architectures with subtle design and leave the details
    of network surgery to Chapter 4\. An overview of this section is shown in Figure [2.1](#Ch2.F1
    "Figure 2.1 ‣ Chapter 2 Compact Architecture ‣ A Survey on Green Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,
    align=left, minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt,
    inner ysep=1pt, , where level=1font=, where level=2font=, where level=3font=,
    where level=4font=, where level=5font=, [Compact Architecture Design [Component
  prefs: []
  type: TYPE_NORMAL
- en: Design [Compact Convolution [Depth-wise Separable Convolution] [Fire Convolution]
    [Flattened Convolution] [Shrinked Convolution] ] [Efficient Attention [Sparse
    Attention] [Attention Approximation] ] [Lightweight Softmax] [Compact Embedding]
    ] [Component
  prefs: []
  type: TYPE_NORMAL
- en: Assembling [Memory Sharing] [Static Weight Sharing [Cross-layer Parameter Sharing]
    [Cross-data Parameter Sharing] ] [Dynamic Weight Sharing [Cascading] [Early Exit]
    [Skipping] [Mixture of Experts (MoE)] ] [Deployment Weight Sharing] ] [Compact-architecture
  prefs: []
  type: TYPE_NORMAL
- en: Search] ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: Taxonomy of compact architecture design with representative examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Component Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we describe efficient variants of popular components, including
    convolution, attention, softmax, and embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Compact Convolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Starting from AlexNet (Krizhevsky et al., [2012](#bib.bib164)), it has been
    a hot direction to build deeper and larger CNNs to achieve better performance (Iandola
    et al., [2014](#bib.bib138); Simonyan & Zisserman, [2015](#bib.bib299); Szegedy
    et al., [2015](#bib.bib314); He et al., [2016a](#bib.bib118); Szegedy et al.,
    [2017](#bib.bib315)). Currently, even a simple CNN baseline contains hundreds
    of layers and thousands of channels. To reduce deployment costs, previous studies
    proposed efficient variants. Here we list several widely-used variants.
  prefs: []
  type: TYPE_NORMAL
- en: Depthwise Separable Convolution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This architecture has been adopted in Xception (Chollet, [2017](#bib.bib50))
    and MobileNet (Howard et al., [2017](#bib.bib132)). Depthwise separable convolution
    contains two components: depthwise convolution and pointwise convolution. The
    depthwise convolution applies a single filter for each input channel. The pointwise
    convolution is a kind of $1\times 1$ convolution. Following this research line,
    many advanced variants have been proposed (Hoang & Jo, [2018](#bib.bib128); Sandler
    et al., [2018](#bib.bib275)). For example,  Wang et al. ([2017](#bib.bib342))
    also proposed a factorized convolution by unravelling the standard convolution
    and arranging the spatial convolution sequentially.'
  prefs: []
  type: TYPE_NORMAL
- en: Fire Convolution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Iandola et al. ([2016](#bib.bib139)) proposed a vision model SqueezeNet. The
    fire module is the key building block. It is similar with depthwise separable
    convolutions. A fire module contains two components: a squeeze convolution layer
    with $1\times 1$ filters and an expand layer with a mixture of $1\times 1$ and
    $3\times 3$ convolution filters.'
  prefs: []
  type: TYPE_NORMAL
- en: Flattened Convolution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is proposed by Jin et al. ([2015](#bib.bib147)) to decrease the redundancy
    of the filters. It separates the 3D convolution filters into three consecutive
    1D filters: convolution across channels (lateral), vertical, and horizontal direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Shrinked Convolution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Traditional convolutions usually have fixed hyper-parameter settings, like the
    number of filters. Different from these models, MobileNet (Howard et al., [2017](#bib.bib132))
    adopts a dynamic setting, also called shrinked convolution. It introduces a width
    multiplier to thin a network uniformly at each layer. The standard convolutions
    with $M$ input channels and $N$ output channels become a shrinked convolution
    with $\alpha M$ input channels and $\alpha N$ output channels where $\alpha\leq
    1$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Efficient Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention (Bahdanau et al., [2015](#bib.bib10)) is first proposed for handling
    long-distance dependency in machine translation. Currently, it has been widely
    used in tasks like summarization, natural language understanding, and so on. The
    key idea is to dynamically attend all tokens at each step. All tokens can be directly
    aligned together, which can address long-distance dependencies to some extent.
    Since any two tokens have an attention score, the required computations grow quadratically
    with the input length. To address this problem, previous studies proposed several
    efficient attention variants. Currently, dop-product-based attention (Vaswani
    et al., [2017](#bib.bib330)) becomes the dominant choice for NLP and CV applications.
    For simplification, we refer to attention as dot-product attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'We roughly classify these variants into two categories: sparse attention that
    reduces the span of attention, and attention approximation with different attention
    estimation formats. Let us review the original self-attention definition. Formally,
    given a sequence of hidden vectors $x$, we can map it into different representation
    space $Q$, $K$, and $V$. Then, attention takes $Q$, $K$, and $V$ as inputs and
    is responsible for generating vector via the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V$
    |  | (2.1) |'
  prefs: []
  type: TYPE_TB
- en: where $Q$, $K$, $V$ are 3-dimension tensors, with dimensions of sequence length,
    head number, hidden dimension. The computations mainly come from $(\frac{QK^{T}}{\sqrt{d_{k}}})$
    and softmax operations. This section describes several approaches to reduce dot-product
    computations. We leave the details of efficient softmax variants in Section 2.1.3.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Attention
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see from Eq. [2.1](#Ch2.E1 "In 2.1.2 Efficient Attention ‣ 2.1 Component
    Design ‣ Chapter 2 Compact Architecture ‣ A Survey on Green Deep Learning"), all
    tokens are required to be attended at each step. Several approaches are proposed
    to reduce attention length by only attending local tokens at each step. A natural
    solution is to reduce the number of attended tokens by assigning some tokens with
    zero weights. It is the idea of sparse attention (Martins & Astudillo, [2016](#bib.bib214);
    Child et al., [2019](#bib.bib48); Correia et al., [2019](#bib.bib56); Dai et al.,
    [2019](#bib.bib60); Zaheer et al., [2020](#bib.bib388)).  Martins & Astudillo
    ([2016](#bib.bib214)) proposed Sparsemax, which added a $L_{2}$ regularization
    to encourage the attention matrix to be sparse. Sparsemax has been applied to
    various architectures (Niculae & Blondel, [2017](#bib.bib231); Maruf et al., [2019](#bib.bib215);
    Peters et al., [2019](#bib.bib240)).  Child et al. ([2019](#bib.bib48)) introduced
    heuristic rules and defined two sparse attention variants. One attends previous
    $l$ tokens. The other splits a sequence into different spans where each span has
    $l$ tokens. Each head attends to every $l$ tokens where $l$ is much smaller than
    the length of inputs.  Sukhbaatar et al. ([2019](#bib.bib308)) believed that the
    naive sparse attention was somehow arbitrary. They found that some attention heads
    focused on the recent tokens, while other heads took information from the whole
    context. Motivated by this, they proposed adaptive attention by learning the attention
    span of each head. An similar idea is proposed by Correia et al. ([2019](#bib.bib56)).
    They introduced an adaptive sparse attention approach. They replaced full softmax
    operations with $\alpha$ softmax that allowed low-scoring words to receive precisely
    zero weight. In addition to attention length, the attention head is also an important
    sparse factor.  Voita et al. ([2019](#bib.bib335)) found that only a subset of
    heads matter and the rest can be pruned.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Approximation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kitaev et al. ([2020](#bib.bib162)) proposed an efficient attention model Reformer.
    It approximates the dot-product attention computation by one that uses locality-sensitive
    hashing, reducing the complexity from $O(L^{2})$ to $O(L\log L)$, where $L$ is
    the length of the sequence.  Choromanski et al. ([2020](#bib.bib51)) further proposed
    a more efficient model Performer with an unbiased positive random feature map
    estimator. Compared to the original attention, Performer is a linear architecture
    with compatible performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Lightweight Softmax
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Softmax layer is a necessary component for deep learning. The key idea is to
    normalize a vector to a probability distribution of possible labels. Traditional
    softmax is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(y_{i}&#124;x)=\frac{\exp(h_{i}\cdot w)}{\sum_{j=1}\exp(h_{j}\cdot w)}$
    |  | (2.2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $y_{i}$ is the $i$-th label and $w$ are learnable parameters. $h_{i}$
    is the $i$-th dimension of hidden vector. $x$ is the input sequence. The denominator
    requires the dot-product over label candidates. If the task has a large label
    set, the denominator will require large computations. Since the complexity of
    softmax is proportional to the number of labels and sequence generation (Mikolov
    et al., [2013a](#bib.bib220), [b](#bib.bib221)) tasks usually have a large vocabulary
    size, we take sequence generation as an example to show several lightweight variants
    with fewer computations. In sequence generation tasks, token vocabulary is equal
    to the label set. Formally, given a hidden vector $h$ and all token embeddings,
    the softmax for sequence generation is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(y_{i}&#124;x)=\frac{\exp(h^{T}v_{i})}{\sum_{j=1}\exp(h^{T}v_{j})}$
    |  | (2.3) |'
  prefs: []
  type: TYPE_TB
- en: where $x$ is the input and $v_{i}$ is the embedding of the $i$-th token. Eq. [2.3](#Ch2.E3
    "In 2.1.3 Lightweight Softmax ‣ 2.1 Component Design ‣ Chapter 2 Compact Architecture
    ‣ A Survey on Green Deep Learning") shows that the softmax layer introduces embeddings
    for all tokens and requires the inner-product between hidden vector and all embeddings.
    A large vocabulary will require many computations. Therefore, several efficient
    softmax variants have been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Fewer Parameters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To reduce memory usage,  Press & Wolf ([2017](#bib.bib250)) proposed to tie
    input embeddings and embeddings in Eq. [2.3](#Ch2.E3 "In 2.1.3 Lightweight Softmax
    ‣ 2.1 Component Design ‣ Chapter 2 Compact Architecture ‣ A Survey on Green Deep
    Learning"). They conducted experiments on machine translation. Results show that
    weight sharing can reduce the size of neural translation models without harming
    translation results. In addition, reducing the number of labels is another important
    research direction. Recently, several studies have been  (Kim et al., [2016b](#bib.bib161);
    Costa-jussà & Fonollosa, [2016](#bib.bib57)) proposed to generate a sequence in
    character level, rather than in word level. The number of characters is largely
    less than that of words and the computations for softmax can be largely reduced.
    Similarly,  Józefowicz et al. ([2016](#bib.bib154)) implemented character-based
    softmax on language modeling, which achieved promising results. It is important
    to note that these character-based methods also bring longer sequences. Current
    sequence generation models usually adopt auto-regressive generation frameworks.
    The longer sequence brings higher decoding costs. In all, it should be considered
    case-by-case whether character-based methods reduce the whole decoding cost. Recently,
    a trade-off is achieved by sub-word level vocabularies (Sennrich et al., [2016](#bib.bib289)).
    Sub-word level vocabularies have a tradeoff granularity between character vocabularies
    and word vocabularies. Sub-word level vocabularies have more tokens than character-level
    vocabulary but also have much shorter segmented sequences. Therefore, sub-word
    level vocabularies become the popular choice for almost all sequence generation
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Fewer Computations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We classify softmax variants with fewer computations into five categories:
    hierarchical softmax, softmax with dynamic embeddings, sampling-based softmax,
    hashing-based softmax, and normalization-based softmax. Hierarchical softmax (H-Softmax) (Morin
    & Bengio, [2005](#bib.bib226); Mnih & Hinton, [2008](#bib.bib224)) is a kind of
    softmax variant. To be specific, it formulates a label set as a tree and all labels
    in the set is the leaf node. The complexity can be dropped from $O(N)$ to $O(log(N))$
    where $N$ is the size of the label set. In this way, the traditional one single
    probability over labels is decomposed into a product of a sequence of probability
    over each tree layer. The regular softmax can be regarded as a tree of depth $1$,
    with all labels as leaf nodes. The second research direction focuses on dynamic
    label embeddings (Chen et al., [2016b](#bib.bib41)). The intuition is that not
    all labels require the same parameter size. It assigns variable parameter sizes
    for different labels. In particular, the approach assigns more parameters to frequent
    labels. The embedding size affects the computation costs. Therefore, this kind
    of method can reduce the computations required by softmax operations. In addition,
    sampling-based softmax aims to estimate the full softmax computations with sampled
    label candidates. The key idea is to sample several label embeddings to estimate
    all embeddings (Bengio & Senecal, [2003](#bib.bib18), [2008](#bib.bib19); Jean
    et al., [2015](#bib.bib144)). Hoever, it only reduces training costs while the
    full softmax is still be computed to obtain a variance-free result during inference.
    Hashing-based softmax is another kind of estimation variant.  Vijayanarasimhan
    et al. ([2015](#bib.bib332)) proposed a fast locality-sensitive hashing technique
    to approximate the actual dot-product. Normalization-based softmax (Devlin et al.,
    [2014](#bib.bib64); Andreas & Klein, [2015](#bib.bib6)) aims to avoid explicit
    denominator. The target is to output a vector as close as the probability distribution
    with the sum being 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4 Compact Embeddings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Building token embeddings is the first step for NLP tasks. The parameters of
    embeddings are decided by vocabulary size and embedding length. How to reduce
    embedding parameters is an important and interesting topic. Learning compact token
    vectors is related to learning compressed neural networks. There have been several
    techniques for learning compact neural networks, like pruning, knowledge distillation,
    low-rank approximation, and quantization. In this part, we only focus on related
    approaches for compact embeddings. We classify these approaches into four categories:
    reuse-based approaches, knowledge-distillation-based approaches, low-rank-based
    approaches, fine-grained vocabularies.'
  prefs: []
  type: TYPE_NORMAL
- en: Reuse-based approaches focus on compositional embeddings (Faruqui et al., [2015](#bib.bib81);
    Chen et al., [2016c](#bib.bib46); Shu & Nakayama, [2018](#bib.bib296); Joshi et al.,
    [2019](#bib.bib152); Shi et al., [2020](#bib.bib294)). For example,  Faruqui et al.
    ([2015](#bib.bib81)) aimed to represent each token embedding as a sparse linear
    combination of basis vectors. The size of basis vectors is much less than token
    embeddings. Similar idea has been proposed by Chen et al. ([2016c](#bib.bib46)).
    They split the vocabulary into two parts. One part is a base set containing frequent
    tokens with fixed size (e.g., 8K), the other part is a set of rare tokens whose
    embeddings are encoded by the base set’s embeddings. Following these studies,
     Shu & Nakayama ([2018](#bib.bib296)) adopted the quantization approach to construct
    embeddings with few basis vectors. Recently, this idea has been adapted to other
    fields beyond NLP, like recommendation systems (Shi et al., [2020](#bib.bib294)).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, traditional compression approaches have been applied to compress
    embeddings.  Mou et al. ([2016](#bib.bib227)) used knowledge distillation to transfer
    knowledge from a big token embedding layer into a smaller embedding layer.  Chen
    et al. ([2018a](#bib.bib37)) used vocabulary-partition (block) based low-rank
    matrix approximation to reduce parameter size.  Lam ([2018](#bib.bib168)) used
    1-2 bits per parameter, rather than traditional 32-bits, for token embedding.
    Vocabulary size is also an important factor in deciding embedding size. Therefore,
    fine-grained vocabularies have been proposed to reduce the vocabulary length,
    like character-level vocabulary (Kim et al., [2016b](#bib.bib161)), subword-level
    vocabulary (Sennrich et al., [2016](#bib.bib289)), and byte-level vocabulary (Wang
    et al., [2020a](#bib.bib338)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Component Assembling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This part presents several component assembling solutions for efficient architecture
    design. Many widely-used architectures are efficient component assembling solutions.
    CNNs and LSTMs are representative models. A single filter in CNNs can handle all
    input spans. LSTMs adopt the same parameters for all steps. The key idea of efficient
    component assembling lies in sharing. We classify these assembling solutions into
    four categories: memory sharing, static weight sharing, dynamic weight sharing,
    and deployment weight sharing.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Memory Sharing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Memory sharing is a common technique to store a large model on devices with
    limited memories. A natural idea is to share the same storage among intermediate
    forward vectors (Pleiss et al., [2017](#bib.bib245)) or backward vectors (Chen
    et al., [2016a](#bib.bib39); Gruslys et al., [2016](#bib.bib103)). There are also
    some reversible models (Gomez et al., [2017](#bib.bib96); MacKay et al., [2018](#bib.bib213))
    where the activation of each layer can be reconstructed from the next layer to
    reduce memory requirements during the backward process. The models do not need
    to save intermediate activation vectors. Since several vectors share the same
    storage space, recomputation is necessary in some cases. To achieve a trade-off
    between efficient memory usage and fewer computations, some studies (Wang et al.,
    [2018a](#bib.bib341)) proposed to combine memory sharing with liveness analysis (Wang
    et al., [2016](#bib.bib340)). During graph computation, GPUs adopt liveness analysis
    to create tensors and free tensors. For large intermediate tensors, frequent allocation/deallocation
    operations are time-consuming. Therefore, the runtime can be reduced by directly
    reusing memory segments from a huge pre-allocated memory pool. In addition to
    memory optimization on a single node,  Rajbhandari et al. ([2020](#bib.bib256))
    further explored memory sharing on distributed settings across multiple computation
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Static Weight Sharing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike memory sharing, static weight sharing aims at exploring how to reuse
    weights for a neural network. The difference between weights and intermediate
    vectors is that weights are fixed during inference and shared by all examples.
    To save memory, many models choose to reuse parameters across different layers
    or different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-layer Parameter Sharing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Cross-layer parameter sharing is a common technique for parameter efficiency.
    The idea of sharing parameters across layers has been well explored (Dehghani
    et al., [2019](#bib.bib61); Bai et al., [2019](#bib.bib13); Lan et al., [2020](#bib.bib170)).
    Savarese & Maire ([2019](#bib.bib278)) proposed a parameter sharing scheme that
    defined a global bank of templates. The parameters of each layer of a CNN come
    from the linear combination of these templates.  Dehghani et al. ([2019](#bib.bib61))
    proposed a model, called Universal Transformer, where all layers shared the same
    parameters. Following these study,  Lan et al. ([2020](#bib.bib170)) applied cross-layer
    sharing mechanism on pre-train/fine-tune settings and  Takase & Kiyono ([2021](#bib.bib317))
    proposed diverse sharing strategies. Recently,  Plummer et al. ([2020](#bib.bib246))
    adopted the idea of network architecture search to automatically learn how to
    share parameters between all layers in a network.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-task Parameter Sharing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Cross-task parameter sharing is also a popular solution to handle multi-task,
    multi-domain, or multi-lingual problems (Ramsundar et al., [2015](#bib.bib259);
    Duong et al., [2015](#bib.bib73); Søgaard & Goldberg, [2016](#bib.bib302); Hashimoto
    et al., [2017](#bib.bib115); Yang et al., [2017](#bib.bib374); Raffel et al.,
    [2020](#bib.bib255)). The key idea of cross-task is to enable all tasks (or languages/domains)
    to share parameters. Multi-task learning (Ruder, [2019](#bib.bib268)) has two
    popular implementations, including hard and soft parameter sharing. Compared to
    soft parameter sharing where different tasks do not have shared networks, hard
    parameter sharing uses fewer parameters. Therefore, we only focus on hard parameter
    sharing in this work.
  prefs: []
  type: TYPE_NORMAL
- en: To be specific, cross-task sharing is initially implemented by sharing the hidden
    layers between all tasks, while keeping several task-specific output layers (Yang
    et al., [2017](#bib.bib374); Houlsby et al., [2019](#bib.bib130); Raffel et al.,
    [2020](#bib.bib255)). For the CV field, multi-task solutions often share CNN layers.
    For the NLP filed, in addition to naive sharing, researchers also focus on finding
    better parameter reusing solutions for different tasks. For example,  Søgaard
    & Goldberg ([2016](#bib.bib302)) found that low-level tasks, e.g., part-of-speech
    tagging, should share parameters at lower layers. Motivated by these findings,
     Hashimoto et al. ([2017](#bib.bib115)) proposed a parameter-sharing network across
    multiple NLP tasks. Currently, the trend of developing large-scale models encourages
    researchers to directly use one single model to support multiple tasks. T5 (Raffel
    et al., [2020](#bib.bib255)) is one representative model.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual is also a special cross-task variant. At the early stage of multilingual
    models, researchers usually choose to share a part of parameters across different
    languages (Firat et al., [2016](#bib.bib86); Upadhyay et al., [2016](#bib.bib327);
    Blackwood et al., [2018](#bib.bib26)). Recently, multilingual approaches usually
    treated all languages equally and mixed them together to train a single model (Ha
    et al., [2016](#bib.bib108); Firat et al., [2017](#bib.bib87); Johnson et al.,
    [2017](#bib.bib149); Fan et al., [2020a](#bib.bib77)). More recently, adapter-based
    solutions have been widely used for modeling task-specific features beyond shared
    parameters (Houlsby et al., [2019](#bib.bib130); Bapna & Firat, [2019](#bib.bib15);
    Pfeiffer et al., [2020](#bib.bib243)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Dynamic Weight Sharing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Static parameter sharing usually relies on pre-specified networks. Researchers
    define heuristic rules based on shared features to decide which layers/components
    should be shared by different inputs/tasks. Although this solution is natural,
    hard sharing usually fails in handling tasks that are not closely related. Dynamic
    solutions are proposed to decide which layers/components should be shared among
    different input samples. Specifically, dynamic networks are neural networks with
    dynamic computational graphs where the computational topology or parameters are
    decided on the fly. Therefore, this kind of network can reduce computation costs
    and improve the adaptiveness of networks. In this survey, we describe the overview
    of general dynamic architectures. If you are interested in other dynamic features,
    you can find surveys focusing on dynamic networks (Han et al., [2021b](#bib.bib112)).
    Networks with dynamic architecture can be classified into the following classes:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cascading-style Networks. Multiple basic networks are cascaded in a directed
    acyclic graph (DAG) in a from-small-to-big manner, where the model first executes
    smaller networks, then larger networks. If a smaller network can handle the input
    sample, the model will stop the execution process and does not run execute models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early-exit-style Networks. A single network contains multiple internal classifiers,
    allowing “easy” samples to exit at shallow layers. The difference with cascading-style
    networks lies in early-exiting networks feed the output of previous layer to the
    next layer while cascading-style networks cut off this information flow and every
    network only takes raw samples as inputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skipping-style Networks. It accelerates inference by either skipping certain
    layers, or skipping unimportant input spans in the whole input sequence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixture-of-experts-style Networks. Multiple experts are provided as candidates
    in the same block. Only a small part of experts are used in each block for inference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Cascading-style dynamic networks have a historical development (Viola & Jones,
    [2001](#bib.bib333); Lienhart & Maydt, [2002](#bib.bib189); Viola & Jones, [2004](#bib.bib334)).
    The authors cascade architectures are originally proposed for unbalanced binary
    classification tasks. They cascaded multiple basic models and fed the input to
    the next model only if the current model was not confident of its prediction.
    For example, Park et al. ([2015](#bib.bib236)) cascaded two VGG networks in a
    small-first manner to obtain a better trade-off between classification accuracy
    and energy consumption. The smaller model can handle most samples, which largely
    reduce inference costs. Bolukbasi et al. ([2017](#bib.bib27)) cascaded AlexNet,
    GoogLeNet, and ResNet together. Wang et al. ([2018c](#bib.bib348)) introduced
    a cost-aware objective for jointly training criterion functions among basic models.
    More recently, Li et al. ([2020b](#bib.bib183)) proposed a dynamic framework for
    accelerating the inference of pre-trained language models, CascadeBERT, which
    dynamically selected proper-sized and complete models in a cascading manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2.1: An overview of widely-used confidence criteria deciding whether
    the forward process should be terminated in cascading-style and early-exiting-style
    networks. In the Formulation column, $\mathbbm{1}(\cdot)\in\{0,1\}$ indicates
    action {“continue”, “terminate”}. $\alpha$ is the threshold. $\lambda$ and $\tau$
    are hyper-parameters. $\text{MLP}(\cdot)$ is a learnable module.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Criterion | Descriptions | Formulation |'
  prefs: []
  type: TYPE_TB
- en: '| Confidence-based Criterion |'
  prefs: []
  type: TYPE_TB
- en: '| Score margin (Park et al., [2015](#bib.bib236)) | The gap between the largest
    and the second largest values among the predicted probability distribution. |
    $\mathbbm{1}(\bm{\hat{y}}^{1\rm{st}}-\bm{\hat{y}}^{2\rm{nd}}<\alpha)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Entropy (Teerapittayanon et al., [2016](#bib.bib324)) (Liu et al., [2020a](#bib.bib200))
    (Li et al., [2021](#bib.bib186)) | The entropy or normalized entropy of the predicted
    probability distribution. | $\mathbbm{1}(H(\bm{\hat{y}})>\alpha)$ |'
  prefs: []
  type: TYPE_TB
- en: '| Max probability (Kaya et al., [2019](#bib.bib155)) (Wang et al., [2020d](#bib.bib351))
    | The maximum predicted probability. | $\mathbbm{1}(\max(\bm{\hat{y}})<\alpha)$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Counting-based Criterion |'
  prefs: []
  type: TYPE_TB
- en: '| Patience (Zhou et al., [2020](#bib.bib399)) | The number of identical predictions.
    | $cnt_{i}^{cls}=\begin{cases}cnt_{i-1}+1&amp;\arg\max(\bm{\hat{y}}_{i})=\arg\max(\bm{\hat{y}}_{i-1})\\
    0&amp;\arg\max(\bm{\hat{y}}_{i})\neq\arg\max(\bm{\hat{y}}_{i-1})\lor i=0\end{cases}$
    <math  class="ltx_Math" alttext="cnt_{i}^{reg}=\begin{cases}cnt_{i-1}+1&amp;&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;<\tau\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;\geq\tau\lor i=0\end{cases}"
    display="inline"><semantics ><mrow 
    ><mrow 
    ><mi 
    >c</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msubsup
     ><mi
     >t</mi><mi
     >i</mi><mrow
     ><mi
     >r</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >g</mi></mrow></msubsup></mrow><mo
     >=</mo><mrow
     ><mo
     >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr
     ><mtd
    class="ltx_align_left" columnalign="left" 
    ><mrow 
    ><mrow 
    ><mi 
    >c</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >t</mi><mrow
     ><mi
     >i</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub></mrow><mo
     >+</mo><mn
     >1</mn></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" 
    ><mrow 
    ><mrow 
    ><mo stretchy="false"
     >&#124;</mo><mrow
     ><msub
     ><mover
    accent="true"  ><mi
     >𝒚</mi><mo
    class="ltx_mathvariant_bold" mathvariant="bold" 
    >^</mo></mover><mi
     >i</mi></msub><mo
     >−</mo><msub
     ><mover
    accent="true"  ><mi
     >𝒚</mi><mo
    class="ltx_mathvariant_bold" mathvariant="bold" 
    >^</mo></mover><mrow
     ><mi
     >i</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub></mrow><mo
    stretchy="false"  >&#124;</mo></mrow><mo
     ><</mo><mi
     >τ</mi></mrow></mtd></mtr><mtr
     ><mtd
    class="ltx_align_left" columnalign="left" 
    ><mn 
    >0</mn></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mrow
     ><mo
    stretchy="false"  >&#124;</mo><mrow
     ><msub
     ><mover
    accent="true"  ><mi
     >𝒚</mi><mo
    class="ltx_mathvariant_bold" mathvariant="bold" 
    >^</mo></mover><mi
     >i</mi></msub><mo
     >−</mo><msub
     ><mover
    accent="true"  ><mi
     >𝒚</mi><mo
    class="ltx_mathvariant_bold" mathvariant="bold" 
    >^</mo></mover><mrow
     ><mi
     >i</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub></mrow><mo
    stretchy="false"  >&#124;</mo></mrow><mo
     >≥</mo><mrow
     ><mi
     >τ</mi><mo
     >∨</mo><mi
     >i</mi></mrow><mo
     >=</mo><mn
     >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply 
    ><ci 
    >𝑐</ci><ci 
    >𝑛</ci><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑡</ci><ci 
    >𝑖</ci></apply><apply 
    ><ci 
    >𝑟</ci><ci 
    >𝑒</ci><ci 
    >𝑔</ci></apply></apply></apply><apply
     ><csymbol
    cd="latexml"  >cases</csymbol><apply
     ><apply
     ><ci
     >𝑐</ci><ci
     >𝑛</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑡</ci><apply
     ><ci
     >𝑖</ci><cn
    type="integer"  >1</cn></apply></apply></apply><cn
    type="integer"  >1</cn></apply><apply
     ><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><ci
     >bold-^</ci><ci
     >𝒚</ci></apply><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><ci
     >bold-^</ci><ci
     >𝒚</ci></apply><apply
     ><ci
     >𝑖</ci><cn
    type="integer" 
    >1</cn></apply></apply></apply></apply><ci
     >𝜏</ci></apply><cn
    type="integer"  >0</cn><apply
     ><apply
     ><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><ci
     >bold-^</ci><ci
     >𝒚</ci></apply><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><ci
     >bold-^</ci><ci
     >𝒚</ci></apply><apply
     ><ci
     >𝑖</ci><cn
    type="integer" 
    >1</cn></apply></apply></apply></apply><apply
     ><ci
     >𝜏</ci><ci
     >𝑖</ci></apply></apply><apply
     ><cn
    type="integer"  >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >cnt_{i}^{reg}=\begin{cases}cnt_{i-1}+1&&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;<\tau\\
    0&&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;\geq\tau\lor i=0\end{cases}</annotation></semantics></math>
    $\mathbbm{1}(cnt_{i}<\alpha)$ |
  prefs: []
  type: TYPE_NORMAL
- en: '| Voting (Sun et al., [2021](#bib.bib310)) | The number of most predictions.
    | $V_{i}=\max_{c}\{\sum_{l=1}^{i}\mathbbm{1}(\arg\max(\bm{\hat{y}}_{i})=y_{c})\}/i^{\lambda}$
    $\mathbbm{1}(V_{i})<\alpha$ |'
  prefs: []
  type: TYPE_TB
- en: '| Learning-based Criterion |'
  prefs: []
  type: TYPE_TB
- en: '| After-prediction (Bolukbasi et al., [2017](#bib.bib27)) (Wang et al., [2018c](#bib.bib348))
    (Schuster et al., [2021](#bib.bib283)) | Take the predicted probability distribution
    as input and generate the label deciding whether to execute the forward process.
    | $\text{MLP}(\bm{\hat{y}})$ |'
  prefs: []
  type: TYPE_TB
- en: '| Before-prediction (Elbayad et al., [2020](#bib.bib74)) (Xin et al., [2021](#bib.bib364))
    | Take features as input and generate the label deciding whether to execute the
    forward process. | $\text{MLP}(\bm{h})$ |'
  prefs: []
  type: TYPE_TB
- en: Early-exiting-style dynamic networks might be the most popular dynamic architecture
    nowadays (Teerapittayanon et al., [2016](#bib.bib324); Bolukbasi et al., [2017](#bib.bib27);
    Gormez & Koyuncu, [2021](#bib.bib99); Huang et al., [2018](#bib.bib133); Yang
    et al., [2020b](#bib.bib373); Wang et al., [2021c](#bib.bib352)). With multiple
    internal classifiers on intermediate layers, a network is capable to give intermediate
    predictions and make decisions about whether to execute the forward process or
    not. If the answer is yes, current state would be fed to the next layer. Otherwise,
    the network outputs the intermediate prediction as the final prediction. Different
    from cascading architectures that cuts off the information flow between networks,
    early-exiting networks reuse the feature computed by previous layer. For example,
    BranchyNet (Teerapittayanon et al., [2016](#bib.bib324)) inserted several branch
    classifiers into a CNN to speedup inference. MSDNet (Huang et al., [2018](#bib.bib133))
    designed an exquisite two-dimensional multi-scale architecture to enable early
    exiting along two dimensions and RANet (Yang et al., [2020b](#bib.bib373)) further
    utilized the spatial redundancy for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the CV field, existing studies also apply early-existing-style
    dynamic networks to the NLP field (Xin et al., [2020](#bib.bib363); Liu et al.,
    [2020a](#bib.bib200); Zhu, [2021](#bib.bib402)). The Two Stage fine-tuning is
    the most representative approach to train early-existing-style dynamic networks
    in NLP where the backbone is fine-tuned with the final classifier in the first
    stage, and the intermediate classifiers are fine-tuned in the second stage. In
    addition, joint training is also a trend to tune all parameters together including
    basic backbones and intermediate classifiers (Schwartz et al., [2020b](#bib.bib285);
    Liao et al., [2021](#bib.bib188); Geng et al., [2021](#bib.bib91)). In addition
    to training algorithms, recent researchers also focus on criterion design. For
    example,  Zhou et al. ([2020](#bib.bib399)) and Sun et al. ([2021](#bib.bib310))
    utilized counting-based criteria to support early exiting. Without relying on
    heuristic criteria, several approaches (Xin et al., [2021](#bib.bib364); Schuster
    et al., [2021](#bib.bib283)) directly learned the criteria by introducing a small
    module to decide whether to execute the forward process. Due to the simplicity
    of single-step prediction, dynamic networks are widely applied to classification
    models. Recently,  Elbayad et al. ([2020](#bib.bib74)) and  Li et al. ([2021](#bib.bib186))
    extended multi-exit design to translation tasks and sequence labeling tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic halting is a special case of early exiting, where the parameters across
    layers are shared and, therefore, the final classifier can also be shared. Specifically,
    these networks infer samples through a shared layer iteratively, rather than infer
    through multiple stacked individual layers. One representative network is proposed
    by Graves ([2016](#bib.bib101)). They proposed the adaptive computation time (ACT)
    mechanism for recurrent models to automatically decide how many times (iterations)
    each input symbol or token should be computed. Following this work, the ACT mechanism
    has been applied to various architectures, like ResNets and Transformers. For
    example, SACT (Figurnov et al., [2017](#bib.bib85)) performed dynamic halting
    in two dimensions, including the coarse ACT among multiple layers within the same
    block and the fine-grained ACT on all spatial positions. Universal Transformer (Dehghani
    et al., [2019](#bib.bib61)) shared all layers within the encoder (or decoder)
    in Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In cascading-style or early-exiting style dynamic networks, the key question
    is to figure out how confident the intermediate classifier is. Previous studies
    proposed various criteria for judging the reliability of an intermediate prediction.
    We categorize them into types as shown in Table [2.1](#Ch2.T1 "Table 2.1 ‣ 2.2.3
    Dynamic Weight Sharing ‣ 2.2 Component Assembling ‣ Chapter 2 Compact Architecture
    ‣ A Survey on Green Deep Learning"). Score margin is the gap between the largest
    and the second largest scores in the predicted probability distribution (Park
    et al., [2015](#bib.bib236)). Entropy-based criterion is based on the entropy
    of the predicted probability distribution (Teerapittayanon et al., [2016](#bib.bib324);
    Li et al., [2021](#bib.bib186); Liu et al., [2020a](#bib.bib200)). The model executes
    the forward process only if the entropy is larger than the pre-defined threshold.
    Max-probability based criterion is the gap between the max value of the predicted
    probability distribution and the pre-defined threshold (Kaya et al., [2019](#bib.bib155);
    Wang et al., [2020d](#bib.bib351)). Patience-based criterion terminates the forward
    process only if the model generates continuously identical predictions (Zhou et al.,
    [2020](#bib.bib399)). Voting-based criterion is inspired by the ensemble technique,
    which terminates the forward process if the most of historic predictions reach
    an agreement (Sun et al., [2021](#bib.bib310)). After-prediction based criterion
    and before-prediction based criterion introduce additional learning functions
    to learn whether to execute the forward process. The only difference lies in that
    after-prediction based criterion uses the prediction distribution as inputs and
    before-prediction based criterion uses the naive hidden vector as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Skipping-style dynamic networks skip some computations during forward process.
    These dynamic networks are capable to obtain higher efficiency. This dynamic solution
    has been widely-use in various models, like SkipNet (Wang et al., [2018d](#bib.bib349)),
    ConvNet-AIG (Veit & Belongie, [2020](#bib.bib331)), and BlockDrop (Wu et al.,
    [2018b](#bib.bib361)). They introduced additional policy networks responsible
    for deciding to skip certain layers or not. The main formula for those dynamic
    networks could be summarized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | SkipNet: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+(1-z_{l})\*\bm{x}_{l}$
    |  | (2.4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | ConvNet-AIG: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+\bm{x}_{l}$
    |  | (2.5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | BlockDrop: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+\bm{x}_{l}$
    |  | (2.6) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{x}_{l}$ is the input of the $l$-th residual unit, $F_{l}(\cdot)$
    is the network layers within the $l$-th residual unit except skip connection,
    and $z_{l}\in\{0,1\}$ is a binary value predicted by the policy network or the
    $l$-th policy module. By utilizing reinforcement learning or the Gumbel re-parameterization
    trick, the network can be trainable in an end-to-end way.
  prefs: []
  type: TYPE_NORMAL
- en: Another research skipping-style line chooses to skip inputs given a long input
    sequence (Yu et al., [2017](#bib.bib381); Campos et al., [2018](#bib.bib32); Yu
    et al., [2018](#bib.bib384)) or assign fewer computations to unimportant steps (Jernite
    et al., [2017](#bib.bib145); Seo et al., [2018](#bib.bib290)) or exit reading (Yu
    et al., [2017](#bib.bib381); Liu et al., [2018c](#bib.bib201); Yu et al., [2018](#bib.bib384)).
    1) Skipping unimportant inputs is the natural way.  Campos et al. ([2018](#bib.bib32))
    introduced Skip-RNN where a binary gate unit was used to learn to skip current
    input token or not. If the answer is yes, Skip-RNN copies current hidden state
    to the next time step, saving computations on those unimportant inputs. LSTM-Jump (Yu
    et al., [2017](#bib.bib381)) achieved the same goal by directly predicting how
    many steps to jump through, or whether to exit reading inputs. Although skipping
    partial inputs saves computations largely, these models, like Skip-RNN and LSTM-Jump,
    suffer from missing or repeating outputs at skipped positions thus are not suitable
    for token-level tasks. 2) To address this problem, assigning fewer computations
    to unimportant steps is a flexible solution. To this end,  Seo et al. ([2018](#bib.bib290))
    proposed Skim-RNN that dynamically decided to update the full-sized hidden state
    or partial-sized hidden state at each time step. 3) Exiting-style reading is a
    special kind of skipping reading (Shen et al., [2017](#bib.bib292); Yu et al.,
    [2018](#bib.bib384); Liu et al., [2018c](#bib.bib201)). It decides to truncate
    the next inputs. For example,  Liu et al. ([2018c](#bib.bib201)) applied the exit
    mechanism to multi-task scenario. ReasoNet (Shen et al., [2017](#bib.bib292))
    adopted the exit mechanism for machine comprehension tasks. Despite good trade-off
    between accuracy and inference speed, skipping-style dynamic networks are harder
    to train, introducing more tuning overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Mixture-of-experts-style dynamic networks are representative dynamic models (Lepikhin
    et al., [2021](#bib.bib177); Lin et al., [2021](#bib.bib191); Fedus et al., [2021](#bib.bib82)).
    In those models, a layer contains multiple experts and only part of these experts
    will be activated for each instance. For example, Switch Transformer (Fedus et al.,
    [2021](#bib.bib82)) is the representative model that has trillion-level parameters.
    It replaces the normal feed-forward layer in the Transformer with a switch feed-forward
    layer, consisting of a routing module and multiple structure-identical experts.
    In each switch layer, only a single expert will be executed for each token. Compared
    to general dense computation architectures, mixture-of-expert-style networks provide
    an affordable and practical way to modify and train large models with sparse activation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Deployment Sharing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When deploying deep learning models on edge devices, we have to consider realistic
    constraints, such as storage, memory, computation, latency, and power consumption.
    Previous researchers have designed lightweight and compact models for mobile devices
    or other edge devices, such as MobileNets (Howard et al., [2017](#bib.bib132);
    Sandler et al., [2018](#bib.bib275); Howard et al., [2019](#bib.bib131)). However,
    with different hardware resources, the optimal neural network architecture varies
    significantly (Cai et al., [2020](#bib.bib31)). Thus, developing elastic or dynamic
    models to satisfy different constraints is critical for practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the recent two years, some studies have paid attention to efficient deployment.
    In these studies, a super-network is trained together with its massive sub-networks
    by task-specific losses. During inference, the appropriate sub-network is selected
    to satisfy the resource constraints. By amortizing the only-once training cost,
    the total cost of specialized designing is reduced from O(N) to O(1). During inference,
    the model can dynamically choose an appropriate network for different devices.
    To be specific, Yu et al. ([2019](#bib.bib383)) proposed slimmable neural networks
    where several widths are predefined, supporting instant and adaptive accuracy-efficiency
    trade-offs by selecting corresponding width. Following this work, Yu & Huang ([2019](#bib.bib382))
    further proposed US-Nets to support arbitrary width selection. Fan et al. ([2020b](#bib.bib78))
    proposed an elastic network that can select sub-networks of any depth from one
    large network without having to finetune them. Beyond aforementioned studies,
    the temporal or input length is also an elastic selection. For example, Kim &
    Cho ([2021](#bib.bib156)) proposed length-adaptive Transformer to support arbitrary
    progressively length deduction. The Length-adaptive Transformer can be directly
    adopted into the downstream task and satisfy any efficiency constraints by searching
    the corresponding length deduction configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Compact-architecture Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to model design, there are studies working on search efficient networks
    towards resource-constraint devices, like mobile. They borrow the idea of neural
    architecture search and apply it to design tiny networks. For example,  Tan et al.
    ([2019a](#bib.bib320)) proposed a neural architecture search approach, which explicitly
    incorporated model latency into the main objective so that the search can identify
    a model with a good trade-off between accuracy and latency. On the ImageNet classification
    task, this approach achieved 75.2% top-1 accuracy with 1.8x faster than MobileNet
    V2\.  Howard et al. ([2019](#bib.bib131)) combined neural architecture search
    and network design together to develop a stronger mobile net MobileNet V3\.  Cai
    et al. ([2019](#bib.bib30)) directly learned the architectures on the target task
    and hardware.  Wu et al. ([2019](#bib.bib357)) proposed a differentiable neural
    architecture search framework that used gradient-based methods to optimize ConvNet
    architectures towards mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3 Energy-Efficient Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many advanced approaches have been proposed to reduce training costs for deep
    learning. In Chapter 2, we describe efficient networks that can reduce computations
    in a single execution. In this chapter, we focus on the computations required
    during the whole training, including weight tuning and hyper-parameter tuning.
    To be specific, we survey approaches that aim to accelerate weight tuning/hyper-parameter
    tuning by using fewer iterations, including initialization, normalization, progressive
    training, and efficient NAS. An overview is shown in Figure [3.1](#Ch3.F1 "Figure
    3.1 ‣ Chapter 3 Energy-Efficient Training ‣ A Survey on Green Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,
    align=left, minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt,
    inner ysep=1pt, , where level=1text width=6em, where level=2text width=7em,font=,
    where level=3font=, where level=4font=, where level=5font=, [Energy-efficient
    Training [Initialization [Random Initialization [Kaiming Initialization] [Xaiver
    Initialization] [Fixup Initialization] [LSUV Initialization] ] [Pre-trained Models
  prefs: []
  type: TYPE_NORMAL
- en: for Initialization [Feature based Initialization] [Fine-tuning based Initialization]
    [Supervised Initialization] [Self-supervised Initialization] ] ] [Normalization
    [Batch Normalization] [Layer Normalization] [Group Normalization] ] [Progressive
  prefs: []
  type: TYPE_NORMAL
- en: Training ] [Efficient AutoML [Search Space [Continuous] [Discrete] [Cell block]
    [Meta-architecture] ] [Search Method [RL-based Search] [Evolution-based Search]
    [Differentiable Search] ] [Evaluation Method [Early Stop] [Weight Sharing] [Hypernetworks]
    ] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: Taxonomy of energy-efficient training with representative examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Initialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training of deep learning starts from architecture design and parameter
    initialization. We have explored efficient architecture design in Section 2\.
    In this part, we focus on how weight initialization affects model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3.1: Summarization of two common initialization approaches. $d$ and $u$
    mean the dimensions of weight matrix $W$. $Uniform$ and $Normal$ mean the uniform
    distribution and Gassuian distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Initialization Approach | Description | Formulation |'
  prefs: []
  type: TYPE_TB
- en: '| Kaiming initialization (He et al., [2015](#bib.bib117)) | Distribution of
    standard deviation of $\sqrt{\frac{2}{d}}$ | $W\sim Normal(0,\frac{2}{d})$ or
    $W\sim Uniform(-\sqrt{\frac{6}{d}},\sqrt{\frac{6}{d}})$ |'
  prefs: []
  type: TYPE_TB
- en: '| Xaiver initialization (Glorot & Bengio, [2010](#bib.bib95)) | Distribution
    of standard deviation of $\sqrt{\frac{2}{d+u}}$ | $W\sim Normal(0,\frac{2}{d+u})$
    or $W\sim Uniform(-\sqrt{\frac{6}{d+u}},\sqrt{\frac{6}{d+u}})$ |'
  prefs: []
  type: TYPE_TB
- en: 3.1.1 Random Initialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is widely accepted that good initialization of weights in a neural network
    is critical to convergence (Glorot & Bengio, [2010](#bib.bib95); Krizhevsky et al.,
    [2012](#bib.bib164); He et al., [2015](#bib.bib117); Mishkin & Matas, [2016](#bib.bib223);
    Kumar, [2017](#bib.bib166)). At the beginning, deep networks are usually initialized
    via random weights drawn from uniform distributions or Gaussian distributions.
    Many previous studies found that these kinds of initialization failed in handling
    very deep models (Glorot & Bengio, [2010](#bib.bib95); Saxe et al., [2014](#bib.bib279);
    Romero et al., [2015](#bib.bib266); Hanin & Rolnick, [2018](#bib.bib113)). The
    problem is caused that the mean/variance of activations and gradients exponentially
    with the depth. To enable training with a very deep model, some advanced initialization
    solutions, like Kaiming initialization (He et al., [2015](#bib.bib117)), Xaiver
    initialization (Glorot & Bengio, [2010](#bib.bib95)), LSUV initialization (Mishkin
    & Matas, [2016](#bib.bib223)) and Fixup initialization (Zhang et al., [2019](#bib.bib391)),
    have been proposed. The key idea is to normalize the variance of weights to make
    the variance of activation in each layer to be around 1\. We list the details
    of two widely-used initialization approaches in Table [3.1](#Ch3.T1 "Table 3.1
    ‣ 3.1 Initialization ‣ Chapter 3 Energy-Efficient Training ‣ A Survey on Green
    Deep Learning"). Compared to the naive baseline, these initialization approaches
    can achieve better performance and faster convergence (Mishkin & Matas, [2016](#bib.bib223)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Pre-trained Models for Initialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to random initialization, many approaches borrow models pre-trained
    from other domains (or other tasks) as initialization. It is widely believed that
    initialization from existing models is an effective technique to improve the generalization
    ability with fewer training iterations. We split these pre-training initialization
    into different categories according to different dimensions. First, according
    to whether the borrowed parameters keep unchanged, these methods can be classified
    into feature-based initialization, and fine-tuning-based initialization. Second,
    according to the knowledge source of pre-trained parameters, these methods can
    be classified into supervised initialization and self-supervised initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-based initialization borrows the parameters (usually from low-layers
    or mid-layers) as initialization from other domains/tasks while these parameters
    keep fixed during training. Generally speaking, feature based initialization can
    keep the generalization ability of the borrowed parameters better and thus is
    more suitable for extremely few-shot settings.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning-based initialization uses the target data to train all parameters,
    including new parameters and borrowed parameters. Fine-tuning based initialization
    can further optimize the target objectives via fine-tuning all parameters and
    thus can better fit training data. It is the most popular solution nowadays for
    the NLP field.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised initialization is widely investigated in the earlier stage of deep
    learning. A common solution is to pre-train the target model on similar tasks/datasets,
    and then reuse the pre-trained parameters as initialization for the target task (Huang
    et al., [2013](#bib.bib134); Oquab et al., [2014](#bib.bib233); Yosinski et al.,
    [2014](#bib.bib379); Duong et al., [2015](#bib.bib73); Long et al., [2016](#bib.bib206)).
    This solution is especially popular for low-resource settings and is extensively
    studied on domain adaptation/transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: The most representative example for supervised initialization is the pre-training
    of deep CNN backbones (Simonyan & Zisserman, [2015](#bib.bib299); Ren et al.,
    [2015](#bib.bib264); He et al., [2016a](#bib.bib118); Simon et al., [2016](#bib.bib298);
    He et al., [2017a](#bib.bib119); Iglovikov & Shvets, [2018](#bib.bib140)). Fine-tuning
    pre-trained CNNs on different downstream datasets usually leads to improved performance
    compared to training from scratch and also reduces the number of training steps.
    Previous researches have explored advanced initialization methods. It is widely
    accepted that layers near to inputs usually are responsible to capture local features (Zeiler
    & Fergus, [2014](#bib.bib389)). Therefore, many studies focus on transferring
    knowledge via initialization from low-layer features and mid-layer features. Interestingly,
    with the increase of large-scale training data, current trends directly adopt
    the simplest solution that uses all parameters for initialization (Li et al.,
    [2020a](#bib.bib181)).
  prefs: []
  type: TYPE_NORMAL
- en: Supervised initialization is also successfully applied to NLP (Socher et al.,
    [2013](#bib.bib301)). It is widely-accepted that features computed in higher layers
    of the network usually depend on the specific dataset and task. Following this
    belief, many studies borrow the parameters of low-level layers and mid-level layers
    from other domains as initialization (Dong et al., [2015](#bib.bib67); Luong et al.,
    [2016](#bib.bib211); Yang et al., [2017](#bib.bib374); Lin et al., [2018](#bib.bib193);
    Liu et al., [2018b](#bib.bib198)). Recently, the trend of large-scale networks
    enables researchers to reuse all parameters from pre-trained networks. Similar
    with CV, the widely-adopt setting in NLP is directly reusing all parameters (Johnson
    et al., [2017](#bib.bib149); Aharoni et al., [2019](#bib.bib3); Tan et al., [2019b](#bib.bib321);
    Bapna & Firat, [2019](#bib.bib15); Lin et al., [2020](#bib.bib194)).
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised initialization is also a popular direction. With the increasing
    parameters in state-of-the-art DNNs, more and more training data are required
    to achieve better generalization results. To reduce the requirements of supervised
    data, previous studies investigated self-supervised pre-training that exploited
    unlabeled data to construct supervision signals to learn representations. Since
    self-supervised pre-training does not require any human-annotated labels, it is
    easy to get sufficient training data. To this end, researchers designed various
    methods to construct self-supervised training signals with unlabeled data. Here
    we take CV and NLP as examples to review recent self-supervised pre-training models.
  prefs: []
  type: TYPE_NORMAL
- en: For the NLP fields, using a model pre-trained on self-supervised data as initialization
    is the most popular solution. At the start, researchers use language modeling
    to pre-train word embeddings which are then used to initialize downstream word
    embeddings (Joshi et al., [2016](#bib.bib150); Qi et al., [2018](#bib.bib252);
    Ruder et al., [2019](#bib.bib270)). Glove is one widely-used word embedding toolkit (Pennington
    et al., [2014](#bib.bib239)) which trains word embeddings based on global word-word
    co-occurrence counts. With the development of representation learning, researchers
    begin to explore and reuse contextualized models. Contextualized models define
    that the representation of a word depends on its contexts and each word has two
    representations, fixed word embeddings and contextualized representations.  Peters
    et al. ([2018](#bib.bib241)) proposed the first widely-used contextualized representations,
    ELMo. Following this work, many advanced contextualized representation models
    begin to spring up, like BERT (Devlin et al., [2019](#bib.bib65)), GPT (Schick
    & Schütze, [2021](#bib.bib281)), T5 (Raffel et al., [2020](#bib.bib255)). The
    development of pre-trained networks also affect the application of CV. In recent
    years, CV began to explore large-scale self-supervised models for initialization (Lu
    et al., [2019](#bib.bib207); Li et al., [2020a](#bib.bib181); Chen et al., [2020b](#bib.bib36)).
    The learning objective is similar with NLP’s pre-trained networks, either recovering
    masked/noised regions, or generating the original image from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Empirical results demonstrate that these pre-trained networks for initialization
    can achieve better performance and faster convergence. However, since current
    pre-trained networks generally require downstream tasks to use the exactly same
    networks, the training time still depends on architecture execution in addition
    to convergence speed. Therefore, it should be considered case by case to conclude
    whether pre-trained models for initialization reduce downstream training costs
    in implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to initialization approaches, normalization is another solution
    to accelerate training. Strictly speaking, normalization is a special component.
    Considering that it can accelerate convergence (Bjorck et al., [2018](#bib.bib25);
    Santurkar et al., [2018](#bib.bib277); Zhang et al., [2019](#bib.bib391)), we
    describe normalization in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalization is a technique to normalize hidden outputs in deep neural networks.
    Batch normalization (Ioffe & Szegedy, [2015](#bib.bib141)) is the first widely-used
    normalization for deep models. The key idea is to normalize the hidden vectors
    of neural networks to the distribution with mean $\mu=0$ and standard deviation
    $\sigma=1$. The hidden vectors usually are tensors and batch normalization is
    applied on the batch dimension. To be specific, it generates the output given
    the hidden output $h$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{i}=\frac{y_{i}-\mu}{\sigma+\epsilon},\mu=\frac{1}{&#124;B&#124;}\sum_{i=1}^{&#124;B&#124;}h_{b,i},\sigma=\frac{1}{&#124;B&#124;}\sum_{i=1}^{&#124;B&#124;}\sqrt{(h_{b,i}-\mu)^{2}}$
    |  | (3.1) |'
  prefs: []
  type: TYPE_TB
- en: where $h$ is a intermediate tensor where the first is batch dimension. $|B|$
    is the batch size. $y$ and $h$ are the output and input of the normalization component.
     Ioffe & Szegedy ([2015](#bib.bib141)) fond that applied to a state-of-the-art
    image classification model, batch normalization achieved the same accuracy with
    14 times fewer training steps, and beat the original model by a significant margin.
  prefs: []
  type: TYPE_NORMAL
- en: Following batch normalization, many normalization variants have been proposed,
    like layer normalization (Ba et al., [2016](#bib.bib9)), group normalization (Wu
    & He, [2020](#bib.bib359)), weight normalization (Salimans & Kingma, [2016](#bib.bib274)).
    These variants have almost the same calculation process except they are applied
    to different dimensions or different objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Despite good performance, it is still controversial where the benefits of normalization
    come. At the start, normalization is proposed to address internal covariate shift
    by normalizing layer inputs. Internal covariate shift is a phenomenon where the
    distribution of each layer’s inputs changes during training. The parameters of
    the higher layer are required to continuously fit for the new distribution of
    lower layers, which slows down the training. To keep distribution steady, normalization
    is proposed to fix the distribution of input to a standard distribution. However,
     Santurkar et al. ([2018](#bib.bib277)) overturned this belief and they found
    that the distributional stability of layer inputs had little to do with the success
    of batch normalization. Instead, normalization makes the optimization landscape
    significantly smoother. This smoothness induces more predictive and stable gradients,
    allowing for faster training. Motivated by this paper,  Xu et al. ([2019](#bib.bib367))
    proved that normalization indeed normalized backward gradients, which plays an
    important role in deciding the success of normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Progressive Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Progressive training is another strategy to effectively train DNNs. The key
    idea is constructively adding layers. Compared to full training, progressive training
    does not require full gradients to all parameters, thus can largely reduce computations
    required for training. In addition, the well-trained lower layers also accelerate
    the training of higher layers.  Hinton et al. ([2006](#bib.bib126)) applied progressive
    training to deep belief networks. They trained layers sequentially starting from
    bottom layers in a greedy, layer-wise fashion. It is based on an assumption that
    upper layers represent more “abstract” concepts whereas lower layers extract “low-level
    features”. This method is unsupervised because each layer learns a higher-level
    representation of the layer below and the training criterion does not depend on
    the labels. Following this work,  Bengio et al. ([2006](#bib.bib20)) extended
    this method to handle continuous inputs.
  prefs: []
  type: TYPE_NORMAL
- en: With the development of deep learning, layer-wise progressive training methods
    are exploited to train CNNs (Rueda-Plata et al., [2015](#bib.bib271); Kulkarni
    & Karande, [2017](#bib.bib165); Belilovsky et al., [2019](#bib.bib16)) and RNNs (Xu
    et al., [2018](#bib.bib369)). Recently,  Gong et al. ([2019](#bib.bib97)) and Yang
    et al. ([2020a](#bib.bib372)) extended the idea of layer-wise training to large-scale
    NLP models by progressively stacking new layers on top of previously trained layers.
    Their experimental results show that layer-wise training can successfully improve
    the efficiency of training large transformer language models with huge amounts
    of data. To be specific, experimental results show that such progressive training
    policy can achieve more than 110% training speedup without significant performance
    degradation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Efficient Hyper-parameter Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During training, hyper-parameter optimization (HPO) is a common and fundamental
    step for AI participants to find better model settings. Hyper-parameters keep
    fixed during training, including but not limited to optimization settings (e.g.,
    learning rate, batch size) and model settings (e.g., the number of layers). Since
    deep learning performs like a black-box model and the learning landscape is non-convex,
    current optimization approaches usually find a random local minimum. Due to the
    uncertainty, AI engineers tend to taking a lot of computations to find better
    hyper-parameter settings on real-world applications  (Yu & Zhu, [2020](#bib.bib385)).
    HPO or autoML is a field to automatically find the optimal settings. Considering
    that previous approaches mainly study architecture settings, we take efficient
    neural architecture search (NAS) as an example in this survey to review recent
    progress. Following previous studies, we split NAS methods into three components:
    search space, search strategy, and architecture evaluation. In this survey, we
    give an overview of efficient NAS. There are also surveys describing more details
    of NAS (Elsken et al., [2018](#bib.bib75)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b4e608a2efa31c3901ce7524a61798d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: An overview of NAS components.'
  prefs: []
  type: TYPE_NORMAL
- en: The search space defines all architecture candidates. At the first, the search
    space is defined as a discrete space, including structured space or unstructured
    space. Considering that network candidates in unstructured space is too massive,
    researchers usually incorporate inductive bias to build a structured search space (Liu
    et al., [2018a](#bib.bib195); Dong & Yang, [2020](#bib.bib70); Shu et al., [2020](#bib.bib297)).
    One of the representative methods is cell-based search space. Cell-based search
    space assumes that each architecture contains repetitions of fixed structures.
    In this way, the search space can be limited to the cell space where the number
    of candidates is largely reduced. In addition, to enable faster search, differentiable
    approaches (Jiang et al., [2019](#bib.bib146)) adopt a continuous search space
    where edge weights are considered.
  prefs: []
  type: TYPE_NORMAL
- en: The search strategy defines a policy to explore search space. Random search
    is one of traditional search approaches. The key idea is to randomly evaluate
    architectures and to select the best one based on their validation results. To
    reduce wasted evaluation costs, researchers proposed reinforcement learning based
    search policies (Ying et al., [2019](#bib.bib377)). It is a direction that introduces
    an architecture generator to generate well-performing architectures. Since random
    search and reinforcement learning require validation accuracy as search criterion,
    these methods usually need expensive computations. To reduce search costs, evolution
    based search has been proposed (Real et al., [2019](#bib.bib262)). It is a two-stage
    search approach. The first stage selects several well-performing parent architectures.
    The second stage applies mutation on these parent architectures to select the
    best one. The second stage starts from pre-trained parent networks and does not
    require too much computations to train child networks. Recently,  So et al. ([2019](#bib.bib300))
    applied the evolution search on Transformer networks and achieved new state-of-the-art
    results on machine translation and language modeling tasks. Although these approaches
    can reduce exploration costs, the dependence on validation accuracy still leads
    to considerable computation costs. To fully get rid of the dependence on validation
    accuracy, several studies (Jiang et al., [2019](#bib.bib146); Liu et al., [2019a](#bib.bib196);
    Dong & Yang, [2019](#bib.bib69); Zela et al., [2020](#bib.bib390); Chu et al.,
    [2020](#bib.bib52)) proposed differentiable search that re-formulated the task
    in a differentiable manner and allowed efficient search using gradient descent.
    In addition, another research line aims to represent a model into a continuous
    space where there is a mapping between structures and results. In this way, the
    model only learns how to predict the performance of architectures based on their
    continuous representations where the downstream training is not required (Luo
    et al., [2018](#bib.bib210)). To further reduce training costs, researchers proposed
    training-free NAS approaches that directly extracted features from randomly-initialized
    models and used these features as evaluation criterion to select networks (Mellor
    et al., [2020](#bib.bib218); Chen et al., [2021a](#bib.bib42); Abdelfattah et al.,
    [2021](#bib.bib1); Xu et al., [2021b](#bib.bib368)).
  prefs: []
  type: TYPE_NORMAL
- en: Architecture evaluation takes almost all computations in NAS approaches. At
    the first, full training is required to evaluate the performance of a network,
    which is very heavy. Early stop is a widely used trick to estimate the results
    of a network. Besides, parameter-sharing is also a popular solution that network
    candidates can share parameters with each other (Pham et al., [2018](#bib.bib244)).
    In this way, the model can reuse pre-trained blocks during downstream training.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Current NAS solutions are well-explored in the CV field, and widely-used benchmarks
    are also based on CV datasets. In the future, it is a promising direction to apply
    NAS in other fields, like NLP, to address more real-world problems. In addition,
    existing models focus more on models towards a single task for simplification.
    The multi-task/domain/lingual model is attracting more attention. Therefore, how
    to use NAS to search a shared multi-task/domain/lingual model is also a promising
    direction. Furthermore, the essential question of NAS is how model architecture
    affects downstream results. More understanding studies are expected to reveal
    the fundamental connection between architecture and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4 Energy-Efficient Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we describe common network surgery methods for reducing inference
    costs, including pruning, low-rank factorization, quantization, and knowledge
    distillation. A brief review of these methods is presented in Figure [4.1](#Ch4.F1
    "Figure 4.1 ‣ Chapter 4 Energy-Efficient Inference ‣ A Survey on Green Deep Learning")
    and Table [4.1](#Ch4.T1 "Table 4.1 ‣ Chapter 4 Energy-Efficient Inference ‣ A
    Survey on Green Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,align=left,
    minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt, inner ysep=1pt,
    , where level=1text width=5em, where level=2text width=5.8em,font=, where level=3font=,
    where level=4font=, where level=5font=, [Efficient Inference [Pruning [Pruning
    Unit [Unstructured [Neurons, Connections]] [Structured [Filters, Channels, Layers]]
    ] [Scoring Function [Magnitude] [Important Coefficients] [Gradient-based] [Movement
    Pruning] ] [Scheduling [Single-step Pruning] [Iterative Pruning] [Lottery Ticket]
    ] ] [Low-rank
  prefs: []
  type: TYPE_NORMAL
- en: Factorization [Matrix Factorization [Low-rank Matrix Factorization, SVD]] [Tensor
    Factorization [CP, VBMF, Tucker Decomposition, BTD]] ] [Quantization [Deterministic
  prefs: []
  type: TYPE_NORMAL
- en: Quantization [Rounding, Vector Quantization]] [Stochastic
  prefs: []
  type: TYPE_NORMAL
- en: Quantization [Random Rounding, Probabilistic Quantization]] ] [Knowledge
  prefs: []
  type: TYPE_NORMAL
- en: Distillation [Distillation Target [Logits-based (Vanilla KD)] [Feature-based]
    [Relation-based] ] [Teacher Numbers [Dynamic KD, Multi-teacher KD, Mutual Learning]]
    ] ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1: Taxonomy of efficient inference methods with representative examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4.1: Different approaches for efficient inference.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Approaches | Descriptions | Characteristics |'
  prefs: []
  type: TYPE_TB
- en: '| Pruning | Reduce redundant parameters which | Can be applied to various settings.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | are not sensitive to results. | Fine-tuning is optional. |'
  prefs: []
  type: TYPE_TB
- en: '| Low-rank Factorization | Use matrix/tensor decomposition to | Matrix decomposition
    is computationally |'
  prefs: []
  type: TYPE_TB
- en: '|  | approximate the original parameters. | complicated, but can support train
    from scratch. |'
  prefs: []
  type: TYPE_TB
- en: '| Quantization | Reduce the number of bits used to | Easy to implement. |'
  prefs: []
  type: TYPE_TB
- en: '|  | represent weights and activations. | Sensitive to hardware architecture.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Knowledge Distillation | Train a compact neural network with | Easy to implement.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | knowledge distilled from a teacher model. | Sensitive to network parameters.
    |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Model Pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model pruning is a popular solution to reduce redundant parameters in DNNs.
    Back to 1980s, Hanson & Pratt ([1988](#bib.bib114)) and LeCun et al. ([1989](#bib.bib172))
    already verified that parameters are not equally important to the final performance.
    By removing unimportant weights from a network, we can simultaneously reduce the
    number of parameters, accelerate training/inference, save training examples, and
    improve generalization. This motivates a great amount of studies on pruning neural
    networks in the past 30 years. Specifically, given an initial network that is
    large and accurate, the key idea of pruning is to remove parameters from the original
    network to produce a smaller network that can retain the accuracy of the original
    model.
  prefs: []
  type: TYPE_NORMAL
- en: We first provide a formal definition of pruning. Let us define a neural network
    model as $f(X;\theta)$, which is a function over an input set $X$ and the set
    of parameters $\theta$. Pruning approaches usually take a model $f(X;\theta)$
    as input and then produce a new model $f(X;\theta^{\prime})$. $\theta^{\prime}$
    is a set of parameters with the size of $\theta^{\prime}$ being less than that
    of $\theta$. Usually, $\theta^{\prime}$ is a subset of $\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 The generic framework of pruning
  prefs: []
  type: TYPE_NORMAL
- en: 0:  $N$ is the number of iterations of pruning; $X$ is the dataset  $\theta^{\prime}\leftarrow\text{train-to-convergence}(f(X;\theta))$  for $i$
    in $1$ to $N$ do     $\theta^{\prime}\leftarrow\text{prune}(score(\theta^{\prime}))$     $\theta^{\prime}\leftarrow\text{fine-tune}(f(X;\theta^{\prime}))$  end for  return
    $\theta^{\prime}$
  prefs: []
  type: TYPE_NORMAL
- en: Previous pruning approaches mainly follow the work of  Han et al. ([2015](#bib.bib109))
    to produce a pruned model $f(X;\theta^{\prime})$ from an original model $f(X;\theta)$.
    We show the generic framework in Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Model
    Pruning ‣ Chapter 4 Energy-Efficient Inference ‣ A Survey on Green Deep Learning").
    First, the network is trained to convergence to get pre-trained parameters. Second,
    each parameter or structural element in the network is assigned a score. This
    score indicates the relative importance to the final performance. The network
    is then pruned based on these scores. Third, since pruning generally reduces the
    accuracy of the network, it is a general practice to fine-tune (train further
    after pruning) the pruned network. The process of pruning and fine-tuning is usually
    iterated several times.
  prefs: []
  type: TYPE_NORMAL
- en: 'We describe some key components of network pruning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning Unit refers to the basic unit that the algorithm aims to prune. According
    to pruning units, we can classify modern pruning approaches into two categories,
    unstructured pruning, and structured pruning. Some pruning algorithms prune individual
    parameters (i.e., unstructured pruning), which produce a sparse neural network.
    While the resulting sparse network is smaller in terms of the number of parameters,
    it is hard to yield speedups since the pruned weights are not well arranged. In
    contrast, structured pruning considers parameters in groups. They keep the dense
    features of the network by removing entire weight matrices, filters, channels,
    or layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoring Function defines the metric used to prune parameters. Common practices
    for parameter scoring usually are based on importance coefficients, network activations,
    or gradients. After assigning scores to each part of the parameters, we have two
    choices to prune networks. First, we can choose to prune a fraction of the parameters
    with the locally lowest scores within each structural sub-component of the network
    (e.g., layers). Second, we also can choose parameters with the globally lowest
    scores within the entire network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling decides the total step that pruning algorithms use to prune parameters.
    Some methods prune weights in a single step while another methods use multiple
    steps to prune parameters where each step only prunes a part of parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning is usually required for the pruned network to recover the original
    accuracy. Many methods choose to fine-tune the pruned network, or re-train the
    pruned network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pruning is first applied to fully-connected networks. For example,  LeCun et al.
    ([1989](#bib.bib172)) analyzed the importance of parameters and showed that small
    magnitude weights had less impact on training losses. Specifically, they computed
    the saliency of parameters based on the second derivative. Then, they pruned parameters
    with small saliency scores. To recover the original performance of the network,
    the network was fine-tuned after pruning.  Hassibi et al. ([1993](#bib.bib116))
    extended this idea by using the inverse of the Hessian as saliency score. In addition
    to weight pruning, Suzuki et al. ([2001](#bib.bib313)) proposed to prune network
    connections based on their influence on training losses, and then re-train the
    network to compensate the performance drop. Unlike these approaches,  Srinivas
    & Babu ([2015](#bib.bib304)) argued that similar neurons were redundant. They
    proposed to remove redundant neurons, instead of removing individual weight connections
    one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, many pruning algorithms are applied to CNNs.  Han et al. ([2015](#bib.bib109))
    proposed a simple magnitude-based method to remove unimportant connections in
    fully-connected layers and convolutional layers. However, the resulting model,
    despite being sparse, does not bring significant inference speedups due to the
    feature of sparsity. To address this problem, several structured pruning algorithms
    have been proposed to prune dense blocks, like filters, channels, or layers (Li
    et al., [2017](#bib.bib182); Molchanov et al., [2017](#bib.bib225); He et al.,
    [2017b](#bib.bib123); Lin et al., [2017](#bib.bib190); He et al., [2018](#bib.bib124);
    Luo et al., [2019](#bib.bib209)).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to CV models, pruning has been successfully applied to NLP tasks.
    At the early stage, several studies successfully pruned recurrent neural networks
    (RNNs).  See et al. ([2016](#bib.bib286)) used iterative pruning and retraining
    to prune a recurrent model for neural translation.  Narang et al. ([2017a](#bib.bib228))
    pruned RNNs via magnitude based pruning.  Narang et al. ([2017b](#bib.bib229))
    used iterative ground lasso regularization to induce block sparsity in RNNs.  Lee
    et al. ([2019](#bib.bib174)) and Zhang & Stadie ([2020](#bib.bib392)) proposed
    one-shot RNN pruning methods based on connection sensitivity and Jacobian spectrum.
    More recently, with the success of Transformer, several studies investigated pruning
    transformer models (Michel et al., [2019](#bib.bib219); Voita et al., [2019](#bib.bib335);
    McCarley et al., [2019](#bib.bib216); Fan et al., [2020b](#bib.bib78); Wang et al.,
    [2020c](#bib.bib350); Sanh et al., [2020](#bib.bib276)). One trend is to use structured
    pruning since the transformer architecture is highly parallelized. For instance, Fan
    et al. ([2020b](#bib.bib78)) introduced LayerDrop to prune transformer layers
    for efficient inference.  Michel et al. ([2019](#bib.bib219)) and  Voita et al.
    ([2019](#bib.bib335)) revealed that the multi-head attention mechanism in the
    transformer architecture led to redundant attention heads. Motivated by this finding,
    they proposed to directly prune attention heads.  McCarley et al. ([2019](#bib.bib216))
    used structured pruning to compress a BERT-based question answering model.  Xu
    et al. ([2020](#bib.bib365)) recently proposed a progressive module replacing
    approach by replacing a whole module from the original model with a compact module
    to reduce model size. Unlike these studies,  Guo et al. ([2020](#bib.bib104))
    proposed an unstructured pruning approach, diff pruning, to compress a multi-task
    model.
  prefs: []
  type: TYPE_NORMAL
- en: While most aforementioned pruning algorithms require re-using or re-training
    the originally trained network, a recent research direction (Frankle & Carbin,
    [2019](#bib.bib88)) suggested that dense, randomly-initialized, feed-forward networks
    contained sub-networks (winning tickets), which can reached the test accuracy
    of the original network. They also found that a standard pruning technique naturally
    uncovered some of the winning tickets and then proposed an algorithm to identify
    winning tickets at the early stage of training. Also,  Frankle & Carbin ([2019](#bib.bib88))
    and Liu et al. ([2019c](#bib.bib205)) found that once the “winning ticket” is
    found, it can be trained from scratch to get an equivalent or better performance
    compared to pruned and fine-tuned one. Recently, there are also several studies
    investigating the lottery ticket hypothesis for BERT-like models (Prasanna et al.,
    [2020](#bib.bib248); Chen et al., [2020c](#bib.bib38)). For example,  Prasanna
    et al. ([2020](#bib.bib248)) found that with structured pruning, the “random”
    sub-networks are still almost as good as the “good” ones, and even the “worst”
    ones perform on par with a strong baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pruning is very effective for reducing the number of parameters in DNNs. With
    structured pruning, it can accelerate inference and reduce computations. However,
    there are also several limitations of pruning methods. First, it requires iteratively
    scoring weights and re-training the network for many iterations. Also, pruning
    often leads to a non-negligible performance drop when applied to powerful DNNs.
    The lottery ticket hypothesis provides an interesting direction for more efficient
    “pruning” algorithms. In this survey, we give an overview of pruning methods.
    If you are interested in this direction, there are several surveys describing
    more details of pruning (Liang et al., [2021](#bib.bib187)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Low-rank Factorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tensor (including matrix) operation is the basic building block and contributes
    the bulk of most computations and parameters in DNNs. Therefore, compressing tensors
    or matrices in DNNs is promising for reducing the number of parameters and computation
    costs. The motivation of low-rank factorization is that a large amount of redundant
    information exist in large weight matrices. The super-large matrices are generally
    of low rank and can be factorized into several tiny matrices to save parameters.
    For example, we can apply singular value decomposition (SVD) to factorize a super
    large matrix. SVD factorizes the original weight matrix into three smaller matrices.
    Formally, for any matrix $A\in\mathbb{R}^{m\times n}$, there exists a factorization,
    $A=USV^{T}$ where $U\in\mathbb{R}^{m\times r}$ and $V^{T}\in\mathbb{R}^{r\times
    n}$ are orthogonal matrices, $S\in\mathbb{R}^{r\times r}$ is a diagonal matrix
    with only singular values of A on the diagonal. With SVD decomposition, the spatial
    complexity can be reduced from $\mathcal{O}(mn)$ to $\mathcal{O}(r(m+n+1))$.
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar to pruning algorithms, low-rank factorization is successfully applied
    to CV models.  Sainath et al. ([2013](#bib.bib273)) applied matrix factorization
    to the final weight layer. If the original weight matrix has the dimension $m\times
    n$ and rank $r$, the full rank matrix can be factorized in two weight matrices
    as $m\times r$ and $r\times n$. Their approach reduced the number of parameters
    and achieved up to 30–50% speedup. Similarly, Xue et al. ([2014](#bib.bib370))
    proposed to use SVD decomposition to compress fully-connected neural networks.
     Rigamonti et al. ([2013](#bib.bib265)) proposed to approximate trained CNNs with
    low-rank filters.  Denton et al. ([2014](#bib.bib63)) further exploited the linear
    structure present within the convolutional filters. Their approach was able to
    reduce the memory requirement of the weights in the first two convolutional layers
    by 2–3 times. While low-rank matrix factorization can optimize both the spatial
    and computational complexity of neural networks, the plane view of a matrix limits
    the potential for extreme compression.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor factorization algorithms, in contrast, are more flexible and can be employed
    to achieve an extremely high compression ratio. Among popular tensor factorization
    methods, classical prolongation (CP) (Kolda & Bader, [2009](#bib.bib163)), where
    each factor matrix has the same rank and the kernel tensor is a superdialognal
    tensor, generally achieves better compression performance.  Lebedev et al. ([2015](#bib.bib171))
    leveraged CP decomposition to compress CNN weight kernels into several sub-kernels
    to reduce the number of parameters. Specifically, non-linear least squares were
    used to compute low-rank CP-decomposition of the 4-D tensor into a sum of rank-one
    tensors. Using this decomposition, the original convolutional layer was replaced
    by a sequence of 4 convolutional layers with smaller filters. Following this idea,
     Kim et al. ([2016a](#bib.bib160)) introduced a one-shot compression method to
    compress the whole network. In addition,  Chen et al. ([2018b](#bib.bib47)) introduced
    a collective residual unit based on block term decomposition (BTD), which is a
    combination of Tucker and CP, to enhance the utilization of parameters in residual
    CNNs.  Zhou et al. ([2019](#bib.bib398)) conversely used neural networks to learn
    an appropriate CP rank for tensor decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the applications on CV models, low-rank factorization has also been
    applied to NLP models. For example, Grachev et al. ([2017](#bib.bib100)) used
    low-rank factorization to train RNN language models.  Winata et al. ([2019](#bib.bib356))
    investigated the use of low-rank factorization as an effective post-processing
    compression method for LSTMs. They applied low-rank factorization on ELMo, one
    of widely-used pre-trained models. Recently, low-rank factorization has also been
    applied on Transformer models (Ma et al., [2019](#bib.bib212)).  Noach & Goldberg
    ([2020](#bib.bib232)) further proposed a two-stage model-compression method to
    reduce the inference time cost of BERT, a kind of Transformer-based model. Their
    approach decomposed the matrices into smaller matrices and then performed feature
    distillation on the internal representation. Also, Lan et al. ([2020](#bib.bib170))
    applied embedding matrix factorization along with layer sharing to reduce the
    amount of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Compared with other popular compression methods, low-rank factorization can
    effectively reduce the size of models with a large compression ratio while preserving
    the performance well. Low-rank factorization is also relatively flexible. However,
    low-rank factorization also suffers from the issue of computational efficiency
    because SVD over large weight matrices can be computationally heavy. Also, compared
    with the compression ratio in terms of model size, low-rank factorization is less
    effective for reducing the computational cost and inference time.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of quantization is to compress the original network by reducing the
    number of bits. The idea of network quantization can be back to early 1990s (Fiesler
    et al., [1990](#bib.bib84); Balzer et al., [1991](#bib.bib14); Tang & Kwan, [1993](#bib.bib322)).
    Recently, due to the success of DNNs and their growing sizes, the research of
    quantization has received increasing attention. In the beginning of 2010s,  Vanhoucke
    et al. ([2011](#bib.bib329)) discovered that CNNs encoded with 32-bit can be converted
    to CNNs encoded with 8-bit, which significantly reduced both storage and computation
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, quantization techniques can be classified into two types:
    deterministic quantization and stochastic quantization. In deterministic quantization,
    there is an deterministic mapping between the quantized value and the real value.
    In stochastic quantization, the quantized value is sampled from discrete distributions (Guo,
    [2018](#bib.bib105)). Usually, post-training quantization is the most simplest
    solution by applying quantization on a trained model to reduce inference costs.
    Despite simple, post-training quantization may brings dropped performance. quantization-aware
    training is proposed to address this problem by fine-tuning the quantized model
    before inference.'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic Quantization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'defines a deterministic mapping between real weights and quantized weights.
    Rounding quantization is the simplest mapping function. The key idea of rounding
    quantization is to map a high-bit floating-point number to its nearest fixed-point
    low-bit number (Gupta et al., [2015](#bib.bib106)). For example, suppose a number
    $x$ and the target fixed-point representation [IL, FL]. The number of integer
    bits IL plus the number of fractional bits FL yields the total number of bits
    used to represent the number. This approach considers the following rounding scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\textnormal{Convert}(x,\textnormal{[IL,
    FL]})=\begin{cases}-2^{\textnormal{IL-1}}\hskip 50.00008pt\textnormal{if}\ x\leq-2^{\textnormal{IL-1}},\\
    2^{\textnormal{IL-1}}-2^{-\textnormal{FL}}\hskip 26.00009pt\textnormal{if}\ x\geq
    2^{\textnormal{IL-1}}-2^{-\textnormal{FL}},\\'
  prefs: []
  type: TYPE_NORMAL
- en: \textnormal{Round}(x)\hskip 33.99998pt\ \textnormal{otherwise}\end{cases}" display="block"><semantics
    ><mrow  ><mrow 
    ><mtext  >Convert</mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >x</mi><mo  >,</mo><mtext
     >[IL, FL]</mtext><mo stretchy="false"
     >)</mo></mrow></mrow><mo
     >=</mo><mrow 
    ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mrow 
    ><mrow  ><mo
     >−</mo><mrow
     ><msup
     ><mn
     >2</mn><mtext
     >IL-1</mtext></msup><mo
    lspace="0em" rspace="0em"  >​</mo><mtext
     >if</mtext><mo
    lspace="0.500em" rspace="0em"  >​</mo><mi
     >x</mi></mrow></mrow><mo
     >≤</mo><mrow
     ><mo 
    >−</mo><msup 
    ><mn 
    >2</mn><mtext 
    >IL-1</mtext></msup></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mrow  ><mrow
     ><msup
     ><mn
     >2</mn><mtext
     >IL-1</mtext></msup><mo
     >−</mo><mrow
     ><msup
     ><mn
     >2</mn><mrow
     ><mo
     >−</mo><mtext
     >FL</mtext></mrow></msup><mo
    lspace="0em" rspace="0em"  >​</mo><mtext
     >if</mtext><mo
    lspace="0.500em" rspace="0em"  >​</mo><mi
     >x</mi></mrow></mrow><mo
     >≥</mo><mrow
     ><msup
     ><mn
     >2</mn><mtext
     >IL-1</mtext></msup><mo
     >−</mo><msup
     ><mn
     >2</mn><mrow
     ><mo
     >−</mo><mtext
     >FL</mtext></mrow></msup></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mtext  >Round</mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >x</mi><mo stretchy="false" 
    >)</mo></mrow><mo lspace="0.500em" rspace="0em"
     >​</mo><mtext
     >otherwise</mtext></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><ci 
    ><mtext  >Convert</mtext></ci><interval
    closure="open"  ><ci 
    >𝑥</ci><ci  ><mtext
     >[IL, FL]</mtext></ci></interval></apply><apply
     ><csymbol cd="latexml" 
    >cases</csymbol><apply 
    ><apply  ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn
    type="integer"  >2</cn><ci
     ><mtext
    mathsize="70%"  >IL-1</mtext></ci></apply><ci
     ><mtext
     >if</mtext></ci><ci
     >𝑥</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn
    type="integer"  >2</cn><ci
     ><mtext
    mathsize="70%"  >IL-1</mtext></ci></apply></apply></apply><ci
     ><mtext class="ltx_mathvariant_italic"
     >otherwise</mtext></ci><apply
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><cn type="integer" 
    >2</cn><ci 
    ><mtext mathsize="70%" 
    >IL-1</mtext></ci></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><cn type="integer"
     >2</cn><apply
     ><ci
     ><mtext
    mathsize="70%"  >FL</mtext></ci></apply></apply><ci
     ><mtext
     >if</mtext></ci><ci
     >𝑥</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn
    type="integer"  >2</cn><ci
     ><mtext
    mathsize="70%"  >IL-1</mtext></ci></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn
    type="integer"  >2</cn><apply
     ><ci
     ><mtext
    mathsize="70%"  >FL</mtext></ci></apply></apply></apply></apply><ci
     ><mtext class="ltx_mathvariant_italic"
     >otherwise</mtext></ci><apply
     ><ci 
    ><mtext  >Round</mtext></ci><ci
     >𝑥</ci><ci 
    ><mtext  >otherwise</mtext></ci></apply><ci
     ><mtext class="ltx_mathvariant_italic"
     >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\textnormal{Convert}(x,\textnormal{[IL,
    FL]})=\begin{cases}-2^{\textnormal{IL-1}}\hskip 50.00008pt\textnormal{if}\ x\leq-2^{\textnormal{IL-1}},\\
    2^{\textnormal{IL-1}}-2^{-\textnormal{FL}}\hskip 26.00009pt\textnormal{if}\ x\geq
    2^{\textnormal{IL-1}}-2^{-\textnormal{FL}},\\ \textnormal{Round}(x)\hskip 33.99998pt\
    \textnormal{otherwise}\end{cases}</annotation></semantics></math> |  | (4.1) |
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\textnormal{Round}(x)=\begin{cases}\lfloor
    x\rfloor\hskip 26.00009pt\textnormal{if}\ \lfloor x\rfloor\leq x\leq\lfloor x\rfloor+\frac{\epsilon}{2},\\
    \lfloor x\rfloor+\epsilon\hskip 10.00002pt\textnormal{if}\ \lfloor x\rfloor+\frac{\epsilon}{2}<x\leq\lfloor
    x\rfloor+\epsilon\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics ><mrow 
    ><mrow  ><mtext
     >Round</mtext><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo stretchy="false" 
    >(</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >=</mo><mrow 
    ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mrow 
    ><mrow  ><mrow
     ><mo
    stretchy="false"  >⌊</mo><mi
     >x</mi><mo stretchy="false"
     >⌋</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mtext
     >if</mtext><mo
    lspace="0.500em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >⌊</mo><mi
     >x</mi><mo stretchy="false"
     >⌋</mo></mrow></mrow><mo
     >≤</mo><mi
     >x</mi><mo
     >≤</mo><mrow
     ><mrow
     ><mo
    stretchy="false"  >⌊</mo><mi
     >x</mi><mo stretchy="false"
     >⌋</mo></mrow><mo
     >+</mo><mstyle
    displaystyle="false"  ><mfrac
     ><mi
     >ϵ</mi><mn
     >2</mn></mfrac></mstyle></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mrow  ><mrow
     ><mo stretchy="false"
     >⌊</mo><mi
     >x</mi><mo stretchy="false"
     >⌋</mo></mrow><mo
     >+</mo><mrow
     ><mi 
    >ϵ</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mtext 
    >if</mtext><mo lspace="0.500em" rspace="0em"
     >​</mo><mrow
     ><mo
    stretchy="false"  >⌊</mo><mi
     >x</mi><mo stretchy="false"
     >⌋</mo></mrow></mrow><mo
     >+</mo><mstyle
    displaystyle="false"  ><mfrac
     ><mi 
    >ϵ</mi><mn 
    >2</mn></mfrac></mstyle></mrow><mo 
    ><</mo><mi  >x</mi><mo
     >≤</mo><mrow 
    ><mrow  ><mo
    stretchy="false"  >⌊</mo><mi
     >x</mi><mo stretchy="false"
     >⌋</mo></mrow><mo
     >+</mo><mi
     >ϵ</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><ci 
    ><mtext  >Round</mtext></ci><ci
     >𝑥</ci></apply><apply 
    ><csymbol cd="latexml"  >cases</csymbol><apply
     ><apply 
    ><apply  ><apply
     ><ci
     >𝑥</ci></apply><ci
     ><mtext
     >if</mtext></ci><apply
     ><ci
     >𝑥</ci></apply></apply><ci
     >𝑥</ci></apply><apply
     ><apply 
    ><apply 
    ><ci  >𝑥</ci></apply><apply
     ><ci
     >italic-ϵ</ci><cn
    type="integer"  >2</cn></apply></apply></apply></apply><ci
     ><mtext class="ltx_mathvariant_italic"
     >otherwise</mtext></ci><apply
     ><apply 
    ><apply  ><apply
     ><ci 
    >𝑥</ci></apply><apply 
    ><ci  >italic-ϵ</ci><ci
     ><mtext
     >if</mtext></ci><apply
     ><ci
     >𝑥</ci></apply></apply><apply
     ><ci 
    >italic-ϵ</ci><cn type="integer" 
    >2</cn></apply></apply><ci 
    >𝑥</ci></apply><apply 
    ><apply  ><apply
     ><ci 
    >𝑥</ci></apply><ci 
    >italic-ϵ</ci></apply></apply></apply><ci 
    ><mtext class="ltx_mathvariant_italic" 
    >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\textnormal{Round}(x)=\begin{cases}\lfloor
    x\rfloor\hskip 26.00009pt\textnormal{if}\ \lfloor x\rfloor\leq x\leq\lfloor x\rfloor+\frac{\epsilon}{2},\\
    \lfloor x\rfloor+\epsilon\hskip 10.00002pt\textnormal{if}\ \lfloor x\rfloor+\frac{\epsilon}{2}<x\leq\lfloor
    x\rfloor+\epsilon\\ \end{cases}</annotation></semantics></math> |  | (4.2) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\epsilon(=2^{-\textnormal{FL}})$ is the smallest positive number that
    can be represented in this fixed-point format, $\lfloor x\rfloor$ is defined as
    the largest integer multiple of $\epsilon$ smaller than $x$. Following this approach,
    more advanced approaches have been proposed (Rastegari et al., [2016](#bib.bib261);
    Polino et al., [2018](#bib.bib247); Wu et al., [2018a](#bib.bib358)).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to scalar quantization for individual numbers in weight vectors,
    there is also a research line focusing on clustering-based quantization.  Gong
    et al. ([2014](#bib.bib98)) proposed to classify weights into groups and to use
    the centroid of each group to replace the actual weights during inference.  Han
    et al. ([2016](#bib.bib110)) employed a similar approach but fine-tuned the quantized
    centroids for better performance. Following this idea,  Choi et al. ([2017](#bib.bib49))
    further proposed a Hessian weighted k-means clustering approach.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Quantization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'does not define one-to-one mapping from real weights to quantized weights.
    In random rounding, the quantized value is sampled from a discrete distribution
    parameterized by real values. For example, Courbariaux et al. ([2015](#bib.bib58))
    proposed the following random rounding function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="x^{b}=\begin{cases}+1\quad\textnormal{with
    probability}\ p=\sigma(x),\\ -1\quad\textnormal{with probability}\ 1-p\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics ><mrow 
    ><msup  ><mi
     >x</mi><mi 
    >b</mi></msup><mo  >=</mo><mrow
     ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mrow 
    ><mrow 
    ><mrow 
    ><mo 
    >+</mo><mn 
    >1</mn></mrow><mrow 
    ><mtext 
    >with probability</mtext><mo lspace="0.500em"
    rspace="0em"  >​</mo><mi
     >p</mi></mrow></mrow><mo
     >=</mo><mrow
     ><mi
     >σ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >x</mi><mo stretchy="false"
     >)</mo></mrow></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow
     ><mrow 
    ><mo  >−</mo><mn
     >1</mn></mrow><mrow
     ><mrow 
    ><mtext 
    >with probability</mtext><mo lspace="0em"
    rspace="0em"  >​</mo>
    <mn  >1</mn></mrow><mo
     >−</mo><mi
     >p</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑥</ci><ci  >𝑏</ci></apply><apply
     ><csymbol cd="latexml" 
    >cases</csymbol><apply 
    ><list 
    ><apply 
    ><cn type="integer" 
    >1</cn></apply><apply 
    ><ci 
    ><mtext 
    >with probability</mtext></ci><ci 
    >𝑝</ci></apply></list><apply 
    ><ci 
    >𝜎</ci><ci 
    >𝑥</ci></apply></apply><ci 
    ><mtext class="ltx_mathvariant_italic" 
    >otherwise</mtext></ci><list 
    ><apply  ><cn
    type="integer"  >1</cn></apply><apply
     ><apply
     ><ci
     ><mtext
     >with
    probability</mtext></ci><cn type="integer" 
    >1</cn></apply><ci 
    >𝑝</ci></apply></list><ci 
    ><mtext class="ltx_mathvariant_italic" 
    >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >x^{b}=\begin{cases}+1\quad\textnormal{with
    probability}\ p=\sigma(x),\\ -1\quad\textnormal{with probability}\ 1-p\\ \end{cases}</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\sigma$ is the “hard sigmoid” function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma(x)=\textnormal{clip}(\frac{x+1}{2},0,1)=\max(0,\min(1,\frac{x+1}{2}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: If $x$ is a positive value, we will have a high probability to quantize it to
    $+1$, otherwise to $-1$. This gives us a more flexible quantization scheme. In
    probabilistic quantization, the weights are assumed to be discretely distributed
    and a learning algorithm is used to infer the parameters of the distributions. Soudry
    et al. ([2014](#bib.bib303)) proposed an expectation back-propagation algorithm
    to train neural networks with binary or ternary weights. They first assumed some
    discrete prior distribution on the weights and then updated the weights in an
    online setting based on the Bayesian formula.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-Aware Training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, quantization-aware training (Jacob et al., [2018](#bib.bib142); Dong
    et al., [2017](#bib.bib71); Fan et al., [2020c](#bib.bib79)) has become the de
    facto approach towards designing robust quantized models. It simulated quantization
    effects in the forward pass of training and the backward pass was accomplished
    via straight through estimator (Bengio et al., [2013b](#bib.bib22)). It generally
    relied on techniques like gradient clipping to make the training stable. Recently,
    several studies analyzed and introduced new quantization-aware training approaches.
    For example, Fan et al. ([2020c](#bib.bib79)) and Dong et al. ([2017](#bib.bib71))
    stochastically applied quantization to a portion of the weights at each training
    step, while Sheng et al. ([2018](#bib.bib293)) and Alizadeh et al. ([2019](#bib.bib5))
    re-ordered the blocks or layers.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Network quantization is first widely applied to CNNs. In addition to CNNs, quantization
    has been also applied to other models, like RNN, Transformer.  Ott et al. ([2016](#bib.bib234))
    first investigated RNN quantization and found that weight binarization did not
    work well on RNNs. For simplification, they proposed to apply weight quantization
    for RNN weights and to leave activations as floating-point numbers. Hubara et al.
    ([2017](#bib.bib137)) explored different combinations of bit-widths for weights
    and activations.  He et al. ([2016b](#bib.bib121)) proposed to quantize the structure
    of gates and interlinks in LSTM and GRU cells. Recently, Wang et al. ([2018b](#bib.bib343))
    proposed to use a threshold ternary quantization method for weight quantization
    and a Bernoulli ternary quantization method for activation quantization.
  prefs: []
  type: TYPE_NORMAL
- en: With the recent success of Transformer, a number of studies investigated the
    application of quantization on Transformers. For example, Bhandare et al. ([2019](#bib.bib24))
    and Prato et al. ([2020](#bib.bib249)) showed that 8-bit quantization can successfully
    reduce the size of a Transformer-based model and accelerate inference without
    compromising translation quality. Recently, quantization has been applied on Transformer-based
    language models (Zafrir et al., [2019](#bib.bib386); Zhang et al., [2020](#bib.bib393);
    Bai et al., [2020](#bib.bib12); Kim et al., [2021](#bib.bib159)).  Zafrir et al.
    ([2019](#bib.bib386)) first applied 8-bit quantization on BERT. Following this
    work, Kim et al. ([2021](#bib.bib159)) proposed I-BERT, which employed lightweight
    integer-only approximation methods for nonlinear operations to quantize BERT.
     Zhang et al. ([2020](#bib.bib393)) proposed TernaryBERT, which ternarized the
    weights in a fine-tuned BERT model with both approximation-based and loss-aware
    ternarization methods.
  prefs: []
  type: TYPE_NORMAL
- en: Very recently, several studies have investigated the application of quantization
    on GNNs.  Feng et al. ([2020](#bib.bib83)) proposed a GNN-tailored quantization
    algorithm, and used an automatic bit-selecting approach to pinpoint the most appropriate
    quantization bits.  Wang et al. ([2021a](#bib.bib339)) and Bahri et al. ([2020](#bib.bib11))
    further proposed binarized GNNs.  Tailor et al. ([2021](#bib.bib316)) proposed
    Degree-Quant, an architecture-agnostic method for quantization-aware training
    on graphs. Moreover, Zhao et al. ([2020](#bib.bib396)) proposed to use neural
    architecture search to find the optimal architecture coupled with the best quantisation
    strategy for different components in GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Quantization is very effective for reducing the size of neural networks. However,
    post-training quantization often leads to non-neglieable performance drop. In
    contrast, quantization-aware training can effectively reduces the performance
    drop. Incorporating knowledge distillation for quantization can also improve the
    performance of quantized models. In this section, we give an overview of quantization.
    If you are interested in more details, please refer to surveys  (Guo, [2018](#bib.bib105);
    Gholami et al., [2021](#bib.bib92)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Knowledge Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of knowledge distillation (KD) is exploiting the knowledge inside a
    large trained “teacher” model to help the training of a “student” model (Bucila
    et al., [2006](#bib.bib29); Ba & Caruana, [2014](#bib.bib8); Hinton et al., [2015](#bib.bib127)).
    In this way, we can use a smaller student model to distill a trained model as
    a replacement for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The traditional KD solution is to minimize the difference between the output
    produced by the teacher model and that produced by the student model. Formally,
    given a labeled dataset $\mathcal{D}$ of $N$ samples $\mathcal{D}=\{\left(x_{1},y_{1}\right),\dots,\left(x_{N},y_{N}\right)\}$,
    we can write the loss function of the student network during the process of knowledge
    distillation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{S}\left(\mathcal{D};\theta_{S};\theta_{T}\right)=\frac{1}{N}\sum_{i=1}^{N}\left[\alpha\mathcal{L}_{\mathcal{T}}\left(y_{i},S\left(x_{i};\theta_{S}\right)\right)+\left(1-\alpha\right)\mathcal{L}_{\mathit{KD}}\left(T\left(x_{i};\theta_{T}\right),S\left(x_{i};\theta_{S}\right)\right)\right]$
    |  | (4.3) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ is a hyper-parameter to control the relative importance of the
    two terms; $\theta_{T}$ and $\theta_{S}$ are the parameters of teacher $T$ and
    student $S$, respectively. $\mathcal{L}_{\mathcal{T}}$ refers to the task-specific
    loss and $\mathcal{L}_{\mathit{KD}}$ refers to the knowledge distillation loss
    which measures the similarity of the student and the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, KD exploits the knowledge from the teacher model to help train
    the student model by minimizing the discrepancy between the knowledge in the teacher
    model and that in the student model. According to the source of teacher knowledge,
    we can classify KD approaches into three categories: logits-based KD, feature-based
    KD, and relation-based KD. According to teacher types, we also can classify KD
    approaches into three categories: KD with static teacher, KD with multiple teachers,
    and KD with dynamic teacher.'
  prefs: []
  type: TYPE_NORMAL
- en: Logits-based KD   focuses on the output class distribution of the teacher model,
    also referred as “soft labels”. This is the vanilla form of knowledge distillation (Hinton
    et al., [2015](#bib.bib127); Ba & Caruana, [2014](#bib.bib8)). Soft targets generated
    by the teacher model provide much more information than hard targets. Therefore,
    training the student model to fit soft targets can help the student model generalize
    better like the teacher model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature-based KD   exploits intermediate features to teach the student model,
    which is believed to be important for representation learning (Bengio et al.,
    [2013a](#bib.bib21)). The simplest solution is to minimize the distance between
    intermediate representation of each student layer and its corresponding teacher
    layer (Romero et al., [2015](#bib.bib266); Sun et al., [2019a](#bib.bib309); Aguilar
    et al., [2020](#bib.bib2)). It enables the student model to exploit richer information
    from the teacher model. Recently, a number of feature-based KD studies have been
    proposed. In summary, these studies mainly focused on two key factors: selection
    of intermediate representations, and distance metric. The intuition of investigating
    better intermediate representations is that the knowledge of teacher should be
    easy to learn for the student model (Zhang et al., [2017](#bib.bib395); Zagoruyko
    & Komodakis, [2017](#bib.bib387); Huang & Wang, [2017](#bib.bib136); Ahn et al.,
    [2019](#bib.bib4); Heo et al., [2019](#bib.bib125); Sun et al., [2019a](#bib.bib309);
    Aguilar et al., [2020](#bib.bib2)). For example,  Huang & Wang ([2017](#bib.bib136))
    proposed to match the distributions of neuron selectivity patterns between the
    teacher and the student models.  Kim et al. ([2018](#bib.bib157)) trained a paraphrase
    model as $\textnormal{TF}_{t}$ to extract transferable features from the teacher’s
    intermediate representations, and a translator model as $\textnormal{TF}_{s}$
    to map the student intermediate representation to teacher’s representations. As
    for distance metrics, $L_{2}$ distance is the most widely used distance metric.
    Besides, $L_{1}$ distance (Wang et al., [2019b](#bib.bib344)) and KL-divergence (Liu
    et al., [2019b](#bib.bib197); Aguilar et al., [2020](#bib.bib2)) are also used
    in previous KD approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Relation-based KD   aims to minimize the correlation between feature pairs from
    the teacher model and the student model (Yim et al., [2017](#bib.bib376); Srinivas
    & Fleuret, [2018](#bib.bib305); Lee et al., [2018](#bib.bib175); Tung & Mori,
    [2019](#bib.bib326); Lee & Song, [2019](#bib.bib176); Peng et al., [2019](#bib.bib238)).
    Distance can be seen a special kind of relation measure. Recently, many approaches
    have been proposed to explore better relation measures. For example, MHGD (Lee
    & Song, [2019](#bib.bib176)) employed a multi-head attention network to encode
    relations between any two feature maps in a certain batch of training instances.
    CCKD (Peng et al., [2019](#bib.bib238)) transferred correlation between instances
    with a generalized kernel method based on Taylor series expansion.
  prefs: []
  type: TYPE_NORMAL
- en: KD with Multiple Teachers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Traditional KD methods focus on transferring the knowledge from one teacher
    model to the student model. A number of recent studies investigated knowledge
    transfer from multiple teachers or an ensemble of teachers. The most popular and
    straightforward way is to learn the ensemble of teacher logits (Tarvainen & Valpola,
    [2017](#bib.bib323); You et al., [2017](#bib.bib380)). Following this idea, multiple
    studies have been proposed to model the diversity of teachers using a weighted
    average of teacher logits (Ruder et al., [2017](#bib.bib269); Lan et al., [2018](#bib.bib169);
    Xiang et al., [2020](#bib.bib362)). Apart from averaged logits, using the ensemble
    of features from multiple teachers is another line of research.  (Park & Kwak,
    [2019](#bib.bib237)) proposed to train the student’s feature map to minimize the
    gap from the feature maps of multiple teachers with different feature transformation
    functions. While achieving promising performance, traditional KD methods using
    multiple teachers suffer from expensive computations because they require multiple
    pre-trained teachers. To alleviate this problem, Zhang et al. ([2018](#bib.bib394))
    proposed a deep mutual learning approach, which was an initial form of online
    KD that has been developed by various studies (Lan et al., [2018](#bib.bib169);
    Anil et al., [2018](#bib.bib7); Chen et al., [2020a](#bib.bib35); Chung et al.,
    [2020](#bib.bib53); Kim et al., [2020](#bib.bib158)). In online KD, a set of student
    models, or peers, was trained simultaneously by learning from each other in a
    peer-teaching fashion.
  prefs: []
  type: TYPE_NORMAL
- en: KD with Dynamic Teacher
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In traditional KD, the teacher model is fixed during KD. However, this can be
    sub-optimal because the generalization ability of the student model is dynamic
    during training. A number of studies explored KD with an evolving teacher model
    to keep a reasonable capacity difference between student and teacher (Mirzadeh
    et al., [2020](#bib.bib222); Shi et al., [2021](#bib.bib295); Zhou et al., [2021b](#bib.bib401);
    Park et al., [2021](#bib.bib235)). For example,  Jin et al. ([2019](#bib.bib148))
    designed a sequence of intermediate targets to impose curriculum-based constraint
    on the optimization path of the student model for improved KD.  Shi et al. ([2021](#bib.bib295))
    and Park et al. ([2021](#bib.bib235)) proposed to jointly update the teacher model
    and the student model with task specific objectives during KD.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Knowledge distillation is a widely-used approach to get a smaller but more competitive
    model. However, although the idea of KD is easy to implement, it also has several
    limitations. First, the performance of KD is very sensitive to the size gap between
    the teacher model and the student model. The discrepancy between the expressive
    power of the models would make it hard to teach the student model. Second, it
    relies on training data and may not be suitable for few-shot or zero-shot settings.
    In addition, recent studies (Xu et al., [2021a](#bib.bib366); Stanton et al.,
    [2021](#bib.bib306)) have revealed that while knowledge distillation can effectively
    improve student generalization, there was still a large discrepancy between the
    predictive distributions of student and teacher models. It means that there is
    still a long way to distill full knowledge in a teacher model to a student model.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5 Efficient Data Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following the definition of Green deep learning, this chapter mainly explores
    how to achieve competitive results with fewer data resources, including active
    learning and pre-training. It is worth noticing that although pre-trained models
    take massive computations during pre-training, they are widely believed to be
    a practical solution to release the burden of data in downstream tasks. Therefore,
    we also include pre-trained models in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,align=left,
    minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt, inner ysep=1pt,
    , where level=1font=, where level=2font=,where level=3font=, where level=4font=,
    where level=5font=, [Green Data Usage [Active Learning [Uncertainty-based] [Diversity-based]
    [Expected Model Change] ] [Pre-training as Few-shot Learners [Self-supervised
    Pre-training] [Contrastive Pre-training] [Prompt Pre-training] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.1: Taxonomy of efficient data usage methods with representative examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Active Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Active learning is a research direction aiming at using as few samples as possibles
    to achieve good results. It is initially proposed to reduce annotation costs.
    Nowadays, pool-based active learning is widely-used to reduce training costs by
    selecting the most useful examples to train networks. The intuitive behind active
    learning is quite simple. The annotated training data do not equally contribute
    to the final performance. If we can always select the most useful example to train
    models, the wasted training on unimportant examples can be largely reduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Active learning usually starts from a randomly-initialized model or a pre-trained
    model. It defines several query strategies to select new unlabeled data to query
    annotation. The new data associated with labels are then used to train the model.
    This process keeps running until it reaches the maximum number of labelled data
    or other termination conditions are satisfied. Previous active learning approaches
    mainly focused on query strategies to improve performance. Following the work
    of Yoo & Kweon ([2019](#bib.bib378)), given a pool of unlabeled data, we classify
    active learning approaches into three categories according to the selection criteria:
    uncertainty-based approaches, diversity-based approaches, and expected-model-change
    based approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty-based active learning is used to be the most common choice in active
    learning, using uncertainty scores to select data (Ranganathan et al., [2017](#bib.bib260);
    Schröder et al., [2021](#bib.bib282); He et al., [2019](#bib.bib122); Shen et al.,
    [2018](#bib.bib291)). The simplest solution is to utilize class posterior probabilities
    to define uncertainty. To be specific,  Lewis & Gale ([1994](#bib.bib179)) used
    the probability of a predicted class as uncertainty score.  Joshi et al. ([2009](#bib.bib151))
    defined an entropy of class posterior probabilities as uncertainty score. Another
    research line is to train multiple models to construct a committee, and used the
    committee to evaluate uncertainty (Beluch et al., [2018](#bib.bib17)). Recently,
    uncertainty-based active learning has been widely applied to various fields.  Ranganathan
    et al. ([2017](#bib.bib260)) applied active learning on image classification which
    selected the most informative unlabeled samples to train a deep belief network
    model.  Shen et al. ([2018](#bib.bib291)) applied uncertainty-based active learning
    to sequence tagging. They selected sentences for which the length-normalized log
    probability of the current prediction was the lowest.  Ducoffe & Precioso ([2018](#bib.bib72))
    focused on margin-based active learning for deep networks. Despite promising effectiveness,
    uncertainty-based sampling can easily lead to insufficient diversity of batch
    query samples.
  prefs: []
  type: TYPE_NORMAL
- en: Diversity-based active learning has been proposed to address the challenges
    of uncertainty based approaches. For example,  Sener & Savarese ([2018](#bib.bib288))
    defined a core-set to estimate the whole training data.  Nguyen & Smeulders ([2004](#bib.bib230))
    proposed to incorporate clustering into active learning. It first constructed
    a classifier on the set of the cluster representatives, and then propagated the
    classification decision to the other samples via a local noise model.
  prefs: []
  type: TYPE_NORMAL
- en: Expected-model-change based active learning selected data points that would
    cause the greatest change to current model. For example,  Roy & McCallum ([2001](#bib.bib267))
    selected examples according to the reduced error rate on future test examples.
     Freytag et al. ([2014](#bib.bib89)) measured the expected change of model outputs
    and incorporated the underlying data distribution. For each example of an unlabeled
    set, the expected change of model predictions was calculated and marginalized
    over the unknown label.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Pre-training as Few-shot Learners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is widely-believed that pre-trained models as initialization is an effective
    approach to reduce data requirements in downstream tasks. In this survey, we describe
    widely-used pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Self-supervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: self-supervised learning (SSL) is the most popular solution to get pre-trained
    models. We take NLP as an example to review recent progress of self-supervised
    learning. Self-supervised objectives can be classified into types including masked
    language modeling (MLM), language modeling (LM), de-nosing auto-encoding (DAE).
    In this survey, we give an overview of these models. We refer the reader to Qiu
    et al. ([2020](#bib.bib253)) and Han et al. ([2021a](#bib.bib111)) for more details
    of pre-trained networks.
  prefs: []
  type: TYPE_NORMAL
- en: Masked Language Modeling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Devlin et al. ([2019](#bib.bib65)) proposed to pre-train an Transformer encoder,
    i.e., BERT, via MLM objective on unlabeled text. MLM builds a corrupted token
    sequence where 15% tokens are randomly sampled and replaced with a special token
    [MASK], then requires the Transformer to predict the original tokens. Formally,
    given a sequence $\bm{x}_{1:L}=[x_{1},x_{2},\cdots,x_{L}]$ and the masked token
    set $\mathcal{M}$, the MLM objective can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{MLM}}=-\sum_{x_{t}\in\mathcal{M}}\log p(x_{t}&#124;\bm{x}_{\setminus\mathcal{M}})$
    |  | (5.1) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{x}_{\setminus\mathcal{M}}$ indicates the unmasked part of the input
    sequence. With the inherited knowledge, fine-tuned BERT performs well compared
    to baselines without pre-training on many classification tasks, including single
    sentence classification (Warstadt et al., [2019](#bib.bib353); Socher et al.,
    [2013](#bib.bib301)), sentence pair classification or matching (Dolan & Brockett,
    [2005](#bib.bib66); Cer et al., [2017](#bib.bib34)), natural language inference (Williams
    et al., [2018](#bib.bib355); Wang et al., [2019a](#bib.bib337); Bentivogli et al.,
    [2009](#bib.bib23); Levesque et al., [2012](#bib.bib178)), and question answering (Rajpurkar
    et al., [2016](#bib.bib257), [2018](#bib.bib258)), etc.
  prefs: []
  type: TYPE_NORMAL
- en: Several studies have made effort to improve MLM through developing more effective
    MLM-like objectives or exploring more efficient training tricks. SpanBERT (Joshi
    et al., [2020](#bib.bib153)), ERNIE (Sun et al., [2019b](#bib.bib311)), and BERT-WWM (Cui
    et al., [2019](#bib.bib59)) proposed to mask adjacent tokens. ELECTRA (Clark et al.,
    [2020](#bib.bib54)) proposed a GAN-like replaced token prediction objective which
    required discriminator to discriminate whether a token is replaced or not. Instead
    of masking, StructBERT (Wang et al., [2020b](#bib.bib346)) learned to predict
    the shuffled span in its original order, which incorporated language structures
    into pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: Language Modeling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: predicts next tokens one by one in an auto-regressive way. Specifically, given
    a sequence $\bm{x}_{1:L}=[x_{1},x_{2},\cdots,x_{L}]$, the joint probability of
    $\bm{x}$ can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(\bm{x}_{1:L})=\prod_{t=1}^{L}p(x_{t}&#124;\bm{x}_{0:t-1})$ |  | (5.2)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathcal{L}_{\text{LM}}=-\sum_{t}\log p(x_{t}&#124;\bm{x}_{<t})$ |  |
    (5.3) |'
  prefs: []
  type: TYPE_TB
- en: while $x_{0}$ is the special [BOS] token which indicates the beginning of a
    sentence. LM is usually implemented with a Transformer decoder. GPT-2 (Radford
    et al., [2019](#bib.bib254)), and GPT-3 (Brown et al., [2020](#bib.bib28)) are
    two representative LM-based pre-trained models. The LM objective directly models
    the probability of next token given its left context. LM-based pre-trained models
    as initialization can largely improve conditional natural language generation
    tasks like summarization (Rush et al., [2015](#bib.bib272); See et al., [2017](#bib.bib287))
    and question answering (Rajpurkar et al., [2018](#bib.bib258); Reddy et al., [2019](#bib.bib263)).
    Therefore, LM models usually are used to initialize generative models.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of follow-up studies improving the original LM objective from
    various aspects. For example, inspired by the fact that neural networks could
    have different reading orders with human, XLNet (Yang et al., [2019](#bib.bib375))
    proposed to use permuted language modeling to predict a sentence in permuted orders
    in an auto-regressive way. UniLM (Dong et al., [2019](#bib.bib68)) proposed a
    prefix LM where next tokens are predicted in an auto-regressive way with all context
    tokens visible to each other. ProphetNet (Qi et al., [2020](#bib.bib251)) introduced
    a future $n$-gram prediction objective to predict the next $n$ tokens simultaneously
    based on previous context at each time step, through a multi-stream attention
    similar to XLNet.
  prefs: []
  type: TYPE_NORMAL
- en: De-nosing Auto-encoding
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: requires a model to recover the original sentence based on the corrupted version.
    Formally, DAE-like objectives can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{DAE}}=-\sum_{t}\log p(x_{t}&#124;\bm{\hat{x}},\bm{x}_{<t})$
    |  | (5.4) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{\hat{x}}$ is the corrupted input. By learning to distinguish which
    part of content is corrupted in a text and recovering it in a natural order, DAE-based
    pre-trained models as initialization can improve various models, including language
    understanding and language generation.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, there are also flexible variants to corrupt a sequence, such as shuffling,
    masking, dropping, rotating, etc. BART (Lewis et al., [2020](#bib.bib180)) corrupted
    its input sequence with arbitrary noise transformations. The transformations include
    token masking, token deletion, sentence permutation, document rotation, and text
    infilling. More recently, Zhou et al. ([2021a](#bib.bib400)) proposed to use text
    rewriting instead of text infilling (e.g., BART, T5) for improving seq2seq pre-trained
    transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual Objectives
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another line of SSL objectives is multilingual SSL objectives. For example,
    mBERT (Devlin et al., [2019](#bib.bib65)) ¹¹1[https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md),
    mBART (Liu et al., [2020b](#bib.bib204)), mT5 (Xue et al., [2021](#bib.bib371))
    pre-trained the multilingual version BERT, BART, T5 via the multilingual masked
    language modeling objective. These models highly improve the results of few-shot
    or low-shot multilingual learning or multilingual generation tasks. In addition
    to these simple variants, recent researchers also designed more sophisticated
    cross-lingual objectives, such as cross-lingual word recovery, cross-lingual paraphrase
    classification, and cross-lingual masked language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Contrastive Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Contrastive learning focuses on pair-wise relationships, aiming at learning
    closer or similar representation for positive samples while pushing negative samples
    away. The recent years have witnessed the rapid progress of contrastive-based
    pre-trained models, especially in CV (van den Oord et al., [2018](#bib.bib328);
    He et al., [2020](#bib.bib120); Chen et al., [2020d](#bib.bib40), [2021b](#bib.bib45)).
  prefs: []
  type: TYPE_NORMAL
- en: The standard contrastive learning utilizes positive and negative pairs at the
    same time to construct its objective. This kind of loss can back to noise contrastive
    estimation (NCE) loss (Gutmann & Hyvärinen, [2010](#bib.bib107)) that is defined
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{NCE}}=-\log\frac{\exp(f(q,k^{+})/\tau)}{\exp(f(q,k^{+})/\tau)+\exp(f(q,k^{-})/\tau)}$
    |  | (5.5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $q$ is the anchor sample; $k^{+}$ and $k^{-}$ are its positive sample
    and negative sample; $q$, $k^{+}$ and $k^{-}$ are vectors generated by a neural
    network; $f(\cdot,\cdot)$ is a similarity function; $\tau$ is the temperature
    controlling the concentration of the induced distribution. When more than one
    negative samples exist, the NCE loss becomes the InfoNCE loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{InfoNCE}}=-\log\frac{\exp(f(q,k^{+})/\tau)}{\exp(f(q,k^{+})/\tau)+\sum_{i=1}^{K}\exp(f(q,k_{i})/\tau)}$
    |  | (5.6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $k_{i}$ represents the $i$-th negative sample of anchor/query sample
    $q$; $K$ is the size of negative samples. van den Oord et al. ([2018](#bib.bib328))
    proved that minimizing the InfoNCE loss was equivalent to maximizing the lower
    bound of mutual information between $q$ and $k^{+}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I(q,k^{+})\geq\log(K)-\mathcal{L}_{\text{InfoNCE}}$ |  | (5.7) |'
  prefs: []
  type: TYPE_TB
- en: where the more negative samples are, the tighter the lower bound is.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, contrastive losses are easy to implement and contrastive-based pre-trained
    models generally have strong transfer ability. In the CV field, it largely improve
    downstream tasks, such as ImageNet classification (Deng et al., [2009](#bib.bib62)),
    object detection (Everingham et al., [2010](#bib.bib76)), and egmentation (Lin
    et al., [2014](#bib.bib192)), etc. MoCo (He et al., [2020](#bib.bib120); Chen
    et al., [2020e](#bib.bib44), [2021b](#bib.bib45)) is one of representative models
    which applied advanced contrastive learning methods to train pre-trained models
    on CV fields.
  prefs: []
  type: TYPE_NORMAL
- en: Existing state-of-the-art contrastive-based pre-trained models are simple variants
    of Eq. [5.6](#Ch5.E6 "In 5.2.2 Contrastive Learning ‣ 5.2 Pre-training as Few-shot
    Learners ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep Learning")
    where positive pairs and negative paris are required. For example, Deep InfoMax
    applied contrastive losses to learn image representations via maximizing the mutual
    information between local patches and the whole image; CMC (Tian et al., [2020](#bib.bib325))
    applied contrastive losses to learn representations where different views of the
    same scene or instance were sampled as positive pairs. According to Wang & Isola
    ([2020](#bib.bib345)), the InfoNCE loss optimized two properties of a representation
    space, including the alignment of representations between positive samples and
    the uniformity of the induced distribution of normalized representations on a
    hypersphere in the representation space. Nevertheless, the recent studies such
    as BYOL (Grill et al., [2020](#bib.bib102)), SwAV (Caron et al., [2020](#bib.bib33)),
    Simsiam (Chen & He, [2021](#bib.bib43)) found that contrastive learning also works
    without negative samples. In all, the research of contrastive-based pre-training
    is still in rapid development. More understanding studies are expected to better
    explain how contrastive losses work.
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies also applied contrastive learning to NLP tasks. For example,
    CAPT (Luo et al., [2020](#bib.bib208)) taken a sentence $s_{i}$ and its masked
    version $\tilde{s}_{i}$ as a positive pair. CERT (Fang & Xie, [2020](#bib.bib80))
    utilized back-translation to generate noised positive samples for the source English
    sentence; CLEAR (Wu et al., [2020](#bib.bib360)) directly integrated four text
    augmentation mechanisms including word deletion, span deletion, reordering, and
    substitution, and taken two augmented versions of an sentence as positive pairs;
    DeCLUTR (Giorgi et al., [2021](#bib.bib94)) samples nearby even overlapping spans
    as positive pairs; SimCSE (Gao et al., [2021](#bib.bib90)) added noise to encoded
    representations via dropout and regard noised representations as positive samples.
    SimCSE performed pertty well on retrieval tasks and achieved new SoTA on semantic
    textual similarity tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Prompt Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As the size of pre-trained models grows rapidly, fine-tuning the super large
    pre-trained model usually requires massive data to get better generalization ability,
    thus failing on few-shot/zero-shot applications. The objective gap between pre-training
    and fine-tuning stages are one of important reasons behind failure. Therefore,
    to improve data efficiency, prompt learning is proposed by extracting the similar/same
    template for pre-training and fine-tuning stages.  Scao & Rush ([2021](#bib.bib280))
    demonstrated that a prompt may be equal to 100 conventional data points, indicating
    that prompts can greatly improve the sample efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In the recent survey about prompt learning (Liu et al., [2021a](#bib.bib199)),
    prompt is defined as $\bm{x}^{\prime}$ = $f_{\text{prompt}}(\bm{x})$, where $f_{\text{prompt}}(\cdot)$
    is the prompting function to rewrite the input $\bm{x}$ with an task-specific
    template. Suppose that we have a review “I dozed off when watching this movie.”
    as input $\bm{x}$ associated with sentiment label $\bm{y}$. A prompt example $\bm{x}^{\prime}$
    is “I dozed off when watching this movie. It is so [MASK]”. In this way, we transfer
    sentiment prediction to a masked language modeling task. As pointed by Wei et al.
    ([2021](#bib.bib354)), language models at scale like GPT-3 (Brown et al., [2020](#bib.bib28))
    contain substantial world knowledge and can perform a range of NLP tasks, which
    makes large pre-trained models a kind of “neural knowledge bases” (Petroni et al.,
    [2019](#bib.bib242)). In fact, prompts can be seen as queries for those neural
    knowledge bases. The difficulty is (1) how to generate appropriate queries to
    achieve your goals, and (2) how to understand or interpret the response in the
    format of predicted textual content.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, prompts are defined as discrete templates. In addition, there are
    also studies focusing on continuous prompts, rather than textual prompts. For
    example, Prefix Tuning (Li & Liang, [2021](#bib.bib185)) and P-tuning (Liu et al.,
    [2021c](#bib.bib203)) defined virtual tokens where only parameters of virtual
    tokens were fine-tuned. The more recent P-tuning V2 (Liu et al., [2021b](#bib.bib202))
    used multi-task learning objectives and obtained competitive even better results
    than vanilla fine-tuning. SPoT (Vu et al., [2021](#bib.bib336)) further utilized
    transfer learning to support unseen tasks, where the prefix was pre-tuning on
    related tasks then used for initialization in unseen task. Overall, these methods
    are ideologically similar to adaptation approaches (Houlsby et al., [2019](#bib.bib130);
    Bapna & Firat, [2019](#bib.bib15); Pfeiffer et al., [2020](#bib.bib243)) because
    both of them do not change the most of parameters of the pre-trained model. However,
    the adaptation methods usually insert adaptors between layers, undermining the
    original model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6 Conclusions and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We believe that Green deep learning will be an important research direction
    in the future. With recent advances in deep learning, the community have made
    significant progress in developing super-large models for downstream tasks, making
    it possible to apply AI models on complicated applications. Considering that it
    is the ultimate goal to deploy AI models on real-world devices with high requirements
    on resource usage, how to transfer strong models to resource-constraint devices
    (e.g., mobile) becomes a priority goal. In this section, we list several challenges
    for Green deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Green deep learning theory. We have harnessed some advanced Green deep learning
    techniques, but many questions still remain to be explored. For example, 1) If
    we already have a well-performing model, why was additional training required
    to transfer knowledge to a small model? 2) How many parameters do we need at least
    for feasible training and inference? 3) How to design Green learning algorithms
    to enable efficient zero-shot learning or few-shot learning like human? 4) How
    our model store knowledge and how to make models to achieve lifelong learning
    without forgetting learned knowledge during training? 5) Is linear algebra is
    the only basic theory for deep learning and whether can we develop a new operation
    system beyond linear algebra? In this survey, we just show limited questions due
    to page limitation. The community still have a long way to go in the theory of
    Green deep learning
  prefs: []
  type: TYPE_NORMAL
- en: Green deep learning under extreme computation constraints. Deploying models
    on edge devices enjoys multiple advantages. First, it can avoid uploading user
    information to the cloud amid the tide of protecting user privacy. Second, it
    can empower more applications with high requirements on latency if deployed a
    light-weight model on edge devices. While recent advances in machine learning
    greatly facilitate efficient training and inference in the cloud, edge devices
    bring new challenges caused by extremely strict memory and computation constraints.
    Therefore, how to design advanced training and inference algorithms towards tiny
    devices is also an important and challenging direction. First, algorithm-hardware
    cooperation is a promising direction to satisfy speed requirements when deploying
    large models. For example, Lightseq (Wang et al., [2021b](#bib.bib347)) and Faster
    Transformer are two representative models that use CUDA implementation to accelerate
    Transformer execution. Second, edge-cloud cooperation is also a practical solution
    by combing powerful and elastic cloud computing and immediate edge computing.
    An intuitive solution is to design a dynamic network where edge devices can handle
    simple samples with smaller models and the cloud is responsible for handing complicated
    samples with larger models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abdelfattah et al. (2021) Mohamed S. Abdelfattah, Abhinav Mehrotra, Lukasz Dudziak,
    and Nicholas D. Lane. Zero-cost proxies for lightweight NAS. *ICLR*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aguilar et al. (2020) Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing
    Fan, and Chenlei Guo. Knowledge distillation from internal representations. In
    *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020*, pp.  7350–7357\. AAAI Press,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aharoni et al. (2019) Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively
    multilingual neural machine translation. In *NAACL-HLT (1)*, pp.  3874–3884\.
    Association for Computational Linguistics, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahn et al. (2019) Sungsoo Ahn, Shell Xu Hu, Andreas C. Damianou, Neil D. Lawrence,
    and Zhenwen Dai. Variational information distillation for knowledge transfer.
    In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
    Beach, CA, USA, June 16-20, 2019*, pp. 9163–9171\. Computer Vision Foundation
    / IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alizadeh et al. (2019) Milad Alizadeh, Javier Fernández-Marqués, Nicholas D.
    Lane, and Yarin Gal. An empirical study of binary neural networks’ optimisation.
    In *7th International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019*. OpenReview.net, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andreas & Klein (2015) Jacob Andreas and Dan Klein. When and why are log-linear
    models self-normalizing? In *HLT-NAACL*, pp.  244–249\. The Association for Computational
    Linguistics, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2018) Rohan Anil, Gabriel Pereyra, Alexandre Passos, Róbert Ormándi,
    George E. Dahl, and Geoffrey E. Hinton. Large scale distributed neural network
    training through online distillation. In *6th International Conference on Learning
    Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
    Track Proceedings*. OpenReview.net, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ba & Caruana (2014) Jimmy Ba and Rich Caruana. Do deep nets really need to
    be deep? In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence,
    and Kilian Q. Weinberger (eds.), *Advances in Neural Information Processing Systems
    27: Annual Conference on Neural Information Processing Systems 2014, December
    8-13 2014, Montreal, Quebec, Canada*, pp. 2654–2662, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ba et al. (2016) Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer
    normalization. *CoRR*, abs/1607.06450, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural
    machine translation by jointly learning to align and translate. In *ICLR*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahri et al. (2020) Mehdi Bahri, Gaétan Bahl, and Stefanos Zafeiriou. Binary
    graph neural networks. *CoRR*, abs/2012.15823, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2020) Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael R. Lyu, and Irwin King. Binarybert: Pushing the limit
    of BERT quantization. *CoRR*, abs/2012.15701, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2019) Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium
    models. In *NeurIPS*, pp.  688–699, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balzer et al. (1991) Wolfgang Balzer, Masanobu Takahashi, Jun Ohta, and Kazuo
    Kyuma. Weight quantization in boltzmann machines. *Neural Networks*, 4(3):405–409,
    1991.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bapna & Firat (2019) Ankur Bapna and Orhan Firat. Simple, scalable adaptation
    for neural machine translation. In *EMNLP/IJCNLP (1)*, pp.  1538–1548\. Association
    for Computational Linguistics, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belilovsky et al. (2019) Eugene Belilovsky, Michael Eickenberg, and Edouard
    Oyallon. Greedy layerwise learning can scale to imagenet. In Kamalika Chaudhuri
    and Ruslan Salakhutdinov (eds.), *Proceedings of the 36th International Conference
    on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA*,
    volume 97 of *Proceedings of Machine Learning Research*, pp.  583–593\. PMLR,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beluch et al. (2018) William H. Beluch, Tim Genewein, Andreas Nürnberger, and
    Jan M. Köhler. The power of ensembles for active learning in image classification.
    In *CVPR*, pp.  9368–9377\. Computer Vision Foundation / IEEE Computer Society,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio & Senecal (2003) Yoshua Bengio and Jean-Sébastien Senecal. Quick training
    of probabilistic neural nets by importance sampling. In *AISTATS*. Society for
    Artificial Intelligence and Statistics, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio & Senecal (2008) Yoshua Bengio and Jean-Sébastien Senecal. Adaptive importance
    sampling to accelerate training of a neural probabilistic language model. *IEEE
    Trans. Neural Networks*, 19(4):713–722, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2006) Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.
    Greedy layer-wise training of deep networks. In Bernhard Schölkopf, John C. Platt,
    and Thomas Hofmann (eds.), *Advances in Neural Information Processing Systems
    19, Proceedings of the Twentieth Annual Conference on Neural Information Processing
    Systems, Vancouver, British Columbia, Canada, December 4-7, 2006*, pp.  153–160\.
    MIT Press, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. (2013a) Yoshua Bengio, Aaron C. Courville, and Pascal Vincent.
    Representation learning: A review and new perspectives. *IEEE Trans. Pattern Anal.
    Mach. Intell.*, 35(8):1798–1828, 2013a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2013b) Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *CoRR*, abs/1308.3432, 2013b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bentivogli et al. (2009) Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang
    Dang, and Danilo Giampiccolo. The fifth PASCAL recognizing textual entailment
    challenge. In *Proceedings of the Second Text Analysis Conference, TAC 2009, Gaithersburg,
    Maryland, USA, November 16-17, 2009*. NIST, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhandare et al. (2019) Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada,
    Vivek Menon, Sun Choi, Kushal Datta, and Vikram Saletore. Efficient 8-bit quantization
    of transformer neural machine language translation model. *CoRR*, abs/1906.00532,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bjorck et al. (2018) Johan Bjorck, Carla P. Gomes, Bart Selman, and Kilian Q.
    Weinberger. Understanding batch normalization. In *NeurIPS*, pp.  7705–7716, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blackwood et al. (2018) Graeme W. Blackwood, Miguel Ballesteros, and Todd Ward.
    Multilingual neural machine translation with task-specific attention. In *COLING*,
    pp.  3112–3122\. Association for Computational Linguistics, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolukbasi et al. (2017) Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh
    Saligrama. Adaptive neural networks for efficient inference. In Doina Precup and
    Yee Whye Teh (eds.), *Proceedings of the 34th International Conference on Machine
    Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, volume 70 of *Proceedings
    of Machine Learning Research*, pp.  527–536\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucila et al. (2006) Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil.
    Model compression. In Tina Eliassi-Rad, Lyle H. Ungar, Mark Craven, and Dimitrios
    Gunopulos (eds.), *Proceedings of the Twelfth ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining, Philadelphia, PA, USA, August 20-23, 2006*,
    pp.  535–541\. ACM, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2019) Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural
    architecture search on target task and hardware. In *7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*.
    OpenReview.net, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2020) Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song
    Han. Once-for-all: Train one network and specialize it for efficient deployment.
    In *8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Campos et al. (2018) Víctor Campos, Brendan Jou, Xavier Giró-i-Nieto, Jordi
    Torres, and Shih-Fu Chang. Skip RNN: learning to skip state updates in recurrent
    neural networks. In *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.
    OpenReview.net, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caron et al. (2020) Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,
    Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features
    by contrasting cluster assignments. In Hugo Larochelle, Marc’Aurelio Ranzato,
    Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cer et al. (2017) Daniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo Lopez-Gazpio,
    and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual
    and crosslingual focused evaluation. In Steven Bethard, Marine Carpuat, Marianna
    Apidianaki, Saif M. Mohammad, Daniel M. Cer, and David Jurgens (eds.), *Proceedings
    of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver,
    Canada, August 3-4, 2017*, pp.  1–14\. Association for Computational Linguistics,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun
    Chen. Online knowledge distillation with diverse peers. In *The Thirty-Fourth
    AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
    Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
    NY, USA, February 7-12, 2020*, pp.  3430–3437\. AAAI Press, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020b) Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng,
    Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing
    transformer. *CoRR*, abs/2012.00364, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018a) Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui
    Hsieh. Groupreduce: Block-wise low-rank approximation for neural language model
    shrinking. In *NeurIPS*, pp.  11011–11021, 2018a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020c) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis
    for pre-trained BERT networks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
    Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016a) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *CoRR*, abs/1604.06174, 2016a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020d) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E.
    Hinton. A simple framework for contrastive learning of visual representations.
    In *Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning
    Research*, pp.  1597–1607\. PMLR, 2020d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016b) Wenlin Chen, David Grangier, and Michael Auli. Strategies
    for training large vocabulary neural language models. In *ACL (1)*. The Association
    for Computer Linguistics, 2016b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021a) Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture
    search on imagenet in four GPU hours: A theoretically inspired perspective. *ICLR*,
    2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen & He (2021) Xinlei Chen and Kaiming He. Exploring simple siamese representation
    learning. In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR
    2021, virtual, June 19-25, 2021*, pp.  15750–15758. Computer Vision Foundation
    / IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020e) Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He.
    Improved baselines with momentum contrastive learning. *CoRR*, abs/2003.04297,
    2020e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021b) Xinlei Chen, Saining Xie, and Kaiming He. An empirical study
    of training self-supervised vision transformers. *CoRR*, abs/2104.02057, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016c) Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. Compressing
    neural language models by sparse word representations. In *ACL (1)*. The Association
    for Computer Linguistics, 2016c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018b) Yunpeng Chen, Xiaojie Jin, Bingyi Kang, Jiashi Feng, and
    Shuicheng Yan. Sharing residual units through collective tensor factorization
    to improve deep neural networks. In *IJCAI*, pp.  635–641\. ijcai.org, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. *CoRR*, abs/1904.10509, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. (2017) Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the
    limit of network quantization. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) François Chollet. Xception: Deep learning with depthwise separable
    convolutions. In *CVPR*, pp.  1800–1807\. IEEE Computer Society, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choromanski et al. (2020) Krzysztof Choromanski, Valerii Likhosherstov, David
    Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz
    Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking
    attention with performers. *CoRR*, abs/2009.14794, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu et al. (2020) Xiangxiang Chu, Bo Zhang, and Xudong Li. Noisy differentiable
    architecture search. *CoRR*, abs/2005.03566, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2020) Inseop Chung, Seonguk Park, Jangho Kim, and Nojun Kwak.
    Feature-map-level online adversarial knowledge distillation. In *Proceedings of
    the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,
    Virtual Event*, volume 119 of *Proceedings of Machine Learning Research*, pp. 
    2006–2015\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D.
    Manning. ELECTRA: pre-training text encoders as discriminators rather than generators.
    In *8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collobert et al. (2011) Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. Natural language processing (almost)
    from scratch. *J. Mach. Learn. Res.*, 12:2493–2537, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correia et al. (2019) Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins.
    Adaptively sparse transformers. In *EMNLP/IJCNLP (1)*, pp.  2174–2184\. Association
    for Computational Linguistics, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Costa-jussà & Fonollosa (2016) Marta R. Costa-jussà and José A. R. Fonollosa.
    Character-based neural machine translation. In *ACL (2)*. The Association for
    Computer Linguistics, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Courbariaux et al. (2015) Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
    David. Binaryconnect: Training deep neural networks with binary weights during
    propagations. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama,
    and Roman Garnett (eds.), *Advances in Neural Information Processing Systems 28:
    Annual Conference on Neural Information Processing Systems 2015, December 7-12,
    2015, Montreal, Quebec, Canada*, pp.  3123–3131, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui et al. (2019) Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang,
    Shijin Wang, and Guoping Hu. Pre-training with whole word masking for chinese
    BERT. *CoRR*, abs/1906.08101, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell,
    Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models
    beyond a fixed-length context. In *ACL (1)*, pp.  2978–2988\. Association for
    Computational Linguistics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dehghani et al. (2019) Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob
    Uszkoreit, and Lukasz Kaiser. Universal transformers. In *7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*.
    OpenReview.net, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE
    Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009),
    20-25 June 2009, Miami, Florida, USA*, pp.  248–255\. IEEE Computer Society, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denton et al. (2014) Emily L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun,
    and Rob Fergus. Exploiting linear structure within convolutional networks for
    efficient evaluation. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D.
    Lawrence, and Kilian Q. Weinberger (eds.), *Advances in Neural Information Processing
    Systems 27: Annual Conference on Neural Information Processing Systems 2014, December
    8-13 2014, Montreal, Quebec, Canada*, pp. 1269–1277, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Devlin et al. (2014) Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar,
    Richard M. Schwartz, and John Makhoul. Fast and robust neural network joint models
    for statistical machine translation. In *ACL (1)*, pp.  1370–1380\. The Association
    for Computer Linguistics, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, pp.  4171–4186, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dolan & Brockett (2005) William B. Dolan and Chris Brockett. Automatically constructing
    a corpus of sentential paraphrases. In *Proceedings of the Third International
    Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005*.
    Asian Federation of Natural Language Processing, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2015) Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang.
    Multi-task learning for multiple language translation. In *ACL (1)*, pp.  1723–1732\.
    The Association for Computer Linguistics, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,
    Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training
    for natural language understanding and generation. In Hanna M. Wallach, Hugo Larochelle,
    Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.),
    *Advances in Neural Information Processing Systems 32: Annual Conference on Neural
    Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
    BC, Canada*, pp.  13042–13054, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong & Yang (2019) Xuanyi Dong and Yi Yang. Searching for a robust neural architecture
    in four GPU hours. In *IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*, pp. 1761–1770, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong & Yang (2020) Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope
    of reproducible neural architecture search. In *ICLR*. OpenReview.net, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2017) Yinpeng Dong, Jianguo Li, and Renkun Ni. Learning accurate
    low-bit deep neural networks with stochastic quantization. In *British Machine
    Vision Conference 2017, BMVC 2017, London, UK, September 4-7, 2017*. BMVA Press,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ducoffe & Precioso (2018) Melanie Ducoffe and Frédéric Precioso. Adversarial
    active learning for deep networks: a margin based approach. *CoRR*, abs/1802.09841,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duong et al. (2015) Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. Low
    resource dependency parsing: Cross-lingual parameter sharing in a neural network
    parser. In *ACL (2)*, pp.  845–850\. The Association for Computer Linguistics,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elbayad et al. (2020) Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
    Depth-adaptive transformer. In *8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken et al. (2018) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural
    architecture search: A survey. *CoRR*, abs/1808.05377, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everingham et al. (2010) Mark Everingham, Luc Van Gool, Christopher K. I. Williams,
    John M. Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge.
    *Int. J. Comput. Vis.*, 88(2):303–338, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020a) Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed
    El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav
    Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard
    Grave, Michael Auli, and Armand Joulin. Beyond english-centric multilingual machine
    translation. *CoRR*, abs/2010.11125, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020b) Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer
    depth on demand with structured dropout. In *ICLR*. OpenReview.net, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020c) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Hervé Jégou, and Armand Joulin. Training with quantization noise
    for extreme model compression. *CoRR*, abs/2004.07320, 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang & Xie (2020) Hongchao Fang and Pengtao Xie. CERT: contrastive self-supervised
    learning for language understanding. *CoRR*, abs/2005.12766, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faruqui et al. (2015) Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer,
    and Noah A. Smith. Sparse overcomplete word vector representations. In *ACL (1)*,
    pp.  1491–1500\. The Association for Computer Linguistics, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fedus et al. (2021) William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers:
    Scaling to trillion parameter models with simple and efficient sparsity. *CoRR*,
    abs/2101.03961, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and
    Yufei Ding. Sgquant: Squeezing the last bit on graph neural networks with specialized
    quantization. In *32nd IEEE International Conference on Tools with Artificial
    Intelligence, ICTAI 2020, Baltimore, MD, USA, November 9-11, 2020*, pp.  1044–1052\.
    IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fiesler et al. (1990) Emile Fiesler, Amar Choudry, and H. John Caulfield. Weight
    discretization paradigm for optical neural networks. In Hartmut Bartelt (ed.),
    *Optical Interconnections and Networks*, volume 1281, pp.  164 – 173\. International
    Society for Optics and Photonics, SPIE, 1990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figurnov et al. (2017) Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang,
    Jonathan Huang, Dmitry P. Vetrov, and Ruslan Salakhutdinov. Spatially adaptive
    computation time for residual networks. In *2017 IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017*, pp. 1790–1799\.
    IEEE Computer Society, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firat et al. (2016) Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way,
    multilingual neural machine translation with a shared attention mechanism. In
    *HLT-NAACL*, pp.  866–875\. The Association for Computational Linguistics, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firat et al. (2017) Orhan Firat, Kyunghyun Cho, Baskaran Sankaran, Fatos T.
    Yarman-Vural, and Yoshua Bengio. Multi-way, multilingual neural machine translation.
    *Comput. Speech Lang.*, 45:236–252, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2019) Jonathan Frankle and Michael Carbin. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. In *7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*.
    OpenReview.net, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Freytag et al. (2014) Alexander Freytag, Erik Rodner, and Joachim Denzler.
    Selecting influential examples: Active learning with expected model output changes.
    In *ECCV (4)*, volume 8692 of *Lecture Notes in Computer Science*, pp.  562–577\.
    Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple
    contrastive learning of sentence embeddings. *CoRR*, abs/2104.08821, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng et al. (2021) Shijie Geng, Peng Gao, Zuohui Fu, and Yongfeng Zhang. Romebert:
    Robust training of multi-exit BERT. *CoRR*, abs/2101.09755, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gholami et al. (2021) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W.
    Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural
    network inference. *CoRR*, abs/2103.13630, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghosh et al. (2011) Sucheta Ghosh, Richard Johansson, Giuseppe Riccardi, and
    Sara Tonelli. Shallow discourse parsing with conditional random fields. In *Fifth
    International Joint Conference on Natural Language Processing, IJCNLP 2011, Chiang
    Mai, Thailand, November 8-13, 2011*, pp. 1071–1079\. The Association for Computer
    Linguistics, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giorgi et al. (2021) John M. Giorgi, Osvald Nitski, Bo Wang, and Gary D. Bader.
    Declutr: Deep contrastive learning for unsupervised textual representations. In
    Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
    2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pp.  879–895\.
    Association for Computational Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glorot & Bengio (2010) Xavier Glorot and Yoshua Bengio. Understanding the difficulty
    of training deep feedforward neural networks. In *AISTATS*, volume 9 of *JMLR
    Proceedings*, pp. 249–256\. JMLR.org, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gomez et al. (2017) Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B.
    Grosse. The reversible residual network: Backpropagation without storing activations.
    In *NIPS*, pp.  2214–2224, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2019) Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and
    Tie-Yan Liu. Efficient training of BERT by progressively stacking. In Kamalika
    Chaudhuri and Ruslan Salakhutdinov (eds.), *Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA*, volume 97 of *Proceedings of Machine Learning Research*, pp.  2337–2346\.
    PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2014) Yunchao Gong, Liu Liu, Ming Yang, and Lubomir D. Bourdev.
    Compressing deep convolutional networks using vector quantization. *CoRR*, abs/1412.6115,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gormez & Koyuncu (2021) Alperen Gormez and Erdem Koyuncu. Class means as an
    early exit decision mechanism. *CoRR*, abs/2103.01148, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grachev et al. (2017) Artem M. Grachev, Dmitry I. Ignatov, and Andrey V. Savchenko.
    Neural networks compression for language modeling. In *PReMI*, volume 10597 of
    *Lecture Notes in Computer Science*, pp.  351–357\. Springer, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves (2016) Alex Graves. Adaptive computation time for recurrent neural networks.
    *CoRR*, abs/1603.08983, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grill et al. (2020) Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin
    Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires,
    Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos,
    and Michal Valko. Bootstrap your own latent - A new approach to self-supervised
    learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
    Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural Information Processing
    Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
    2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gruslys et al. (2016) Audrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot,
    and Alex Graves. Memory-efficient backpropagation through time. In *NIPS*, pp. 
    4125–4133, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Demi Guo, Alexander M. Rush, and Yoon Kim. Parameter-efficient
    transfer learning with diff pruning. *CoRR*, abs/2012.07463, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo (2018) Yunhui Guo. A survey on methods and theories of quantized neural
    networks. *CoRR*, abs/1808.04752, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2015) Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and
    Pritish Narayanan. Deep learning with limited numerical precision. In Francis R.
    Bach and David M. Blei (eds.), *Proceedings of the 32nd International Conference
    on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015*, volume 37 of *JMLR
    Workshop and Conference Proceedings*, pp.  1737–1746\. JMLR.org, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gutmann & Hyvärinen (2010) Michael Gutmann and Aapo Hyvärinen. Noise-contrastive
    estimation: A new estimation principle for unnormalized statistical models. In
    Yee Whye Teh and D. Mike Titterington (eds.), *Proceedings of the Thirteenth International
    Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna
    Resort, Sardinia, Italy, May 13-15, 2010*, volume 9 of *JMLR Proceedings*, pp. 
    297–304\. JMLR.org, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ha et al. (2016) Thanh-Le Ha, Jan Niehues, and Alexander H. Waibel. Toward multilingual
    neural machine translation with universal encoder and decoder. *CoRR*, abs/1611.04798,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William J. Dally. Learning
    both weights and connections for efficient neural network. In *NIPS*, pp.  1135–1143,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2016) Song Han, Huizi Mao, and William J. Dally. Deep compression:
    Compressing deep neural network with pruning, trained quantization and huffman
    coding. In Yoshua Bengio and Yann LeCun (eds.), *4th International Conference
    on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
    Conference Track Proceedings*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2021a) Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu,
    Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan
    Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong
    Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. Pre-trained models: Past, present
    and future. *CoRR*, abs/2106.07139, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2021b) Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang,
    and Yulin Wang. Dynamic neural networks: A survey. *CoRR*, abs/2102.04906, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hanin & Rolnick (2018) Boris Hanin and David Rolnick. How to start training:
    The effect of initialization and architecture. In *NeurIPS*, pp.  569–579, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanson & Pratt (1988) Stephen Jose Hanson and Lorien Y. Pratt. Comparing biases
    for minimal network construction with back-propagation. In David S. Touretzky
    (ed.), *Advances in Neural Information Processing Systems 1, [NIPS Conference,
    Denver, Colorado, USA, 1988]*, pp. 177–185\. Morgan Kaufmann, 1988.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hashimoto et al. (2017) Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka,
    and Richard Socher. A joint many-task model: Growing a neural network for multiple
    NLP tasks. In *EMNLP*, pp.  1923–1933\. Association for Computational Linguistics,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hassibi et al. (1993) Babak Hassibi, David G. Stork, and Gregory J. Wolff.
    Optimal brain surgeon: Extensions and performance comparison. In *NIPS*, pp. 
    263–270\. Morgan Kaufmann, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving
    deep into rectifiers: Surpassing human-level performance on imagenet classification.
    In *ICCV*, pp.  1026–1034\. IEEE Computer Society, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016a) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *2016 IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*,
    pp. 770–778\. IEEE Computer Society, 2016a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017a) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick.
    Mask R-CNN. In *ICCV*, pp.  2980–2988\. IEEE Computer Society, 2017a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick.
    Momentum contrast for unsupervised visual representation learning. In *2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
    USA, June 13-19, 2020*, pp. 9726–9735\. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016b) Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu
    Zhou, and Yuheng Zou. Effective quantization methods for recurrent neural networks.
    *CoRR*, abs/1611.10176, 2016b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2019) Tao He, Xiaoming Jin, Guiguang Ding, Lan Yi, and Chenggang
    Yan. Towards better uncertainty sampling: Active learning with multiple views
    for deep convolutional neural network. In *ICME*, pp.  1360–1365\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017b) Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for
    accelerating very deep neural networks. In *ICCV*, pp.  1398–1406\. IEEE Computer
    Society, 2017b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2018) Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. AMC: automl for model compression and acceleration on mobile devices.
    In *Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September
    8-14, 2018, Proceedings, Part VII*, pp. 815–832, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heo et al. (2019) Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi.
    Knowledge transfer via distillation of activation boundaries formed by hidden
    neurons. In *The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI
    2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference,
    IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019*, pp.  3779–3787\.
    AAAI Press, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2006) Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A
    fast learning algorithm for deep belief nets. *Neural Comput.*, 18(7):1527–1554,
    2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling
    the knowledge in a neural network. *CoRR*, abs/1503.02531, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoang & Jo (2018) Van-Thanh Hoang and Kang-Hyun Jo. Pydmobilenet: Improved
    version of mobilenets with pyramid depthwise separable convolution. *CoRR*, abs/1811.07083,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter & Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural Comput.*, 9(8):1735–1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for NLP. In *ICML*, volume 97 of
    *Proceedings of Machine Learning Research*, pp.  2790–2799\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard et al. (2019) Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le,
    Mark Sandler, Bo Chen, Weijun Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu,
    Vijay Vasudevan, and Yukun Zhu. Searching for mobilenetv3. In *2019 IEEE/CVF International
    Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November
    2, 2019*, pp. 1314–1324\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Howard et al. (2017) Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient
    convolutional neural networks for mobile vision applications. *CoRR*, abs/1704.04861,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der
    Maaten, and Kilian Q. Weinberger. Multi-scale dense networks for resource efficient
    image classification. In *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.
    OpenReview.net, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2013) Jui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yifan Gong.
    Cross-language knowledge transfer using multilingual deep neural network with
    shared hidden layers. In *ICASSP*, pp.  7304–7308\. IEEE, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2012) Liang Huang, Suphan Fayong, and Yang Guo. Structured perceptron
    with inexact search. In *Human Language Technologies: Conference of the North
    American Chapter of the Association of Computational Linguistics, Proceedings,
    June 3-8, 2012, Montréal, Canada*, pp.  142–151\. The Association for Computational
    Linguistics, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang & Wang (2017) Zehao Huang and Naiyan Wang. Like what you like: Knowledge
    distill via neuron selectivity transfer. *CoRR*, abs/1707.01219, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
    El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks
    with low precision weights and activations. *J. Mach. Learn. Res.*, 18:187:1–187:30,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2014) Forrest N. Iandola, Matthew W. Moskewicz, Sergey Karayev,
    Ross B. Girshick, Trevor Darrell, and Kurt Keutzer. Densenet: Implementing efficient
    convnet descriptor pyramids. *CoRR*, abs/1404.1869, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2016) Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf,
    Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy
    with 50x fewer parameters and <1mb model size. *CoRR*, abs/1602.07360, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iglovikov & Shvets (2018) Vladimir Iglovikov and Alexey Shvets. Ternausnet:
    U-net with VGG11 encoder pre-trained on imagenet for image segmentation. *CoRR*,
    abs/1801.05746, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe & Szegedy (2015) Sergey Ioffe and Christian Szegedy. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In *ICML*,
    volume 37 of *JMLR Workshop and Conference Proceedings*, pp.  448–456\. JMLR.org,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew G. Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,
    Salt Lake City, UT, USA, June 18-22, 2018*, pp. 2704–2713\. IEEE Computer Society,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2019) Paras Jain, Xiangxi Mo, Ajay Jain, Alexey Tumanov, Joseph E.
    Gonzalez, and Ion Stoica. The ooo VLIW JIT compiler for GPU inference. *CoRR*,
    abs/1901.10008, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jean et al. (2015) Sébastien Jean, KyungHyun Cho, Roland Memisevic, and Yoshua
    Bengio. On using very large target vocabulary for neural machine translation.
    In *ACL (1)*, pp.  1–10\. The Association for Computer Linguistics, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jernite et al. (2017) Yacine Jernite, Edouard Grave, Armand Joulin, and Tomás
    Mikolov. Variable computation in recurrent neural networks. In *5th International
    Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
    2017, Conference Track Proceedings*. OpenReview.net, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2019) Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, and Jingbo
    Zhu. Improved differentiable architecture search for language modeling and named
    entity recognition. In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*,
    pp.  3583–3588, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2015) Jonghoon Jin, Aysegul Dundar, and Eugenio Culurciello. Flattened
    convolutional neural networks for feedforward acceleration. In *ICLR (Workshop)*,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2019) Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding
    Liang, Junjie Yan, and Xiaolin Hu. Knowledge distillation via route constrained
    optimization. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV
    2019, Seoul, Korea (South), October 27 - November 2, 2019*, pp. 1345–1354\. IEEE,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson et al. (2017) Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun,
    Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Viégas, Martin Wattenberg,
    Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s multilingual neural machine
    translation system: Enabling zero-shot translation. *Trans. Assoc. Comput. Linguistics*,
    5:339–351, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi et al. (2016) Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak Bhattacharyya,
    and Mark James Carman. Are word embedding-based features useful for sarcasm detection?
    In *EMNLP*, pp.  1006–1011\. The Association for Computational Linguistics, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi et al. (2009) Ajay J. Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos.
    Multi-class active learning for image classification. In *CVPR*, pp.  2372–2379\.
    IEEE Computer Society, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2019) Mandar Joshi, Eunsol Choi, Omer Levy, Daniel S. Weld, and
    Luke Zettlemoyer. pair2vec: Compositional word-pair embeddings for cross-sentence
    inference. In *NAACL-HLT (1)*, pp.  3597–3608\. Association for Computational
    Linguistics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke
    Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and
    predicting spans. *Trans. Assoc. Comput. Linguistics*, 8:64–77, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Józefowicz et al. (2016) Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam
    Shazeer, and Yonghui Wu. Exploring the limits of language modeling. *CoRR*, abs/1602.02410,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaya et al. (2019) Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. Shallow-deep
    networks: Understanding and mitigating network overthinking. In Kamalika Chaudhuri
    and Ruslan Salakhutdinov (eds.), *Proceedings of the 36th International Conference
    on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA*,
    volume 97 of *Proceedings of Machine Learning Research*, pp.  3301–3310\. PMLR,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim & Cho (2021) Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer:
    Train once with length drop, use anytime with search. In Chengqing Zong, Fei Xia,
    Wenjie Li, and Roberto Navigli (eds.), *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers),
    Virtual Event, August 1-6, 2021*, pp.  6501–6511\. Association for Computational
    Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2018) Jangho Kim, Seonguk Park, and Nojun Kwak. Paraphrasing complex
    network: Network compression via factor transfer. In Samy Bengio, Hanna M. Wallach,
    Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.),
    *Advances in Neural Information Processing Systems 31: Annual Conference on Neural
    Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
    Canada*, pp.  2765–2774, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2020) Jangho Kim, Minsung Hyun, Inseop Chung, and Nojun Kwak. Feature
    fusion for online mutual knowledge distillation. In *25th International Conference
    on Pattern Recognition, ICPR 2020, Virtual Event / Milan, Italy, January 10-15,
    2021*, pp.  4619–4625. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021) Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney,
    and Kurt Keutzer. I-BERT: integer-only BERT quantization. *CoRR*, abs/2101.01321,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2016a) Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang,
    and Dongjun Shin. Compression of deep convolutional neural networks for fast and
    low power mobile applications. In *ICLR (Poster)*, 2016a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2016b) Yoon Kim, Yacine Jernite, David A. Sontag, and Alexander M.
    Rush. Character-aware neural language models. In *AAAI*, pp.  2741–2749\. AAAI
    Press, 2016b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. In *ICLR*. OpenReview.net, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kolda & Bader (2009) Tamara G. Kolda and Brett W. Bader. Tensor decompositions
    and applications. *SIAM Review*, 51(3):455–500, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
    Imagenet classification with deep convolutional neural networks. In *Advances
    in Neural Information Processing Systems 25: 26th Annual Conference on Neural
    Information Processing Systems 2012\. Proceedings of a meeting held December 3-6,
    2012, Lake Tahoe, Nevada, United States*, pp.  1106–1114, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kulkarni & Karande (2017) Mandar Kulkarni and Shirish Subhash Karande. Layer-wise
    training of deep networks using kernel similarity. *CoRR*, abs/1703.07115, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar (2017) Siddharth Krishna Kumar. On weight initialization in deep neural
    networks. *CoRR*, abs/1704.08863, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lacoste et al. (2019) Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt,
    and Thomas Dandres. Quantifying the carbon emissions of machine learning. *CoRR*,
    abs/1910.09700, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lam (2018) Maximilian Lam. Word2bits - quantized word vectors. *CoRR*, abs/1803.05651,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2018) Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge distillation
    by on-the-fly native ensemble. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
    Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), *Advances in Neural
    Information Processing Systems 31: Annual Conference on Neural Information Processing
    Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*, pp.  7528–7538,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning
    of language representations. In *ICLR*. OpenReview.net, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lebedev et al. (2015) Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V.
    Oseledets, and Victor S. Lempitsky. Speeding-up convolutional neural networks
    using fine-tuned cp-decomposition. In *ICLR (Poster)*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain
    damage. In *NIPS*, pp.  598–605\. Morgan Kaufmann, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    Gradient-based learning applied to document recognition. *Proceedings of the IEEE*,
    86(11):2278–2324, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2019) Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr.
    Snip: single-shot network pruning based on connection sensitivity. In *7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2018) Seung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self-supervised
    knowledge distillation using singular value decomposition. In Vittorio Ferrari,
    Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), *Computer Vision
    - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018,
    Proceedings, Part VI*, volume 11210 of *Lecture Notes in Computer Science*, pp. 
    339–354\. Springer, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee & Song (2019) Seunghyun Lee and Byung Cheol Song. Graph-based knowledge
    distillation by multi-head attention network. In *30th British Machine Vision
    Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019*, pp.  141\. BMVA
    Press, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lepikhin et al. (2021) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
    Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
    Gshard: Scaling giant models with conditional computation and automatic sharding.
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levesque et al. (2012) Hector J. Levesque, Ernest Davis, and Leora Morgenstern.
    The winograd schema challenge. In Gerhard Brewka, Thomas Eiter, and Sheila A.
    McIlraith (eds.), *Principles of Knowledge Representation and Reasoning: Proceedings
    of the Thirteenth International Conference, KR 2012, Rome, Italy, June 10-14,
    2012*. AAAI Press, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis & Gale (1994) David D. Lewis and William A. Gale. A sequential algorithm
    for training text classifiers. In *SIGIR*, pp.  3–12\. ACM/Springer, 1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART:
    denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
    Tetreault (eds.), *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics, ACL 2020, Online, July 5-10, 2020*, pp. 7871–7880\.
    Association for Computational Linguistics, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020a) Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang.
    Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training.
    In *AAAI*, pp.  11336–11344\. AAAI Press, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. Pruning filters for efficient convnets. In *5th International Conference
    on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
    Track Proceedings*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020b) Lei Li, Yankai Lin, Shuhuai Ren, Deli Chen, Xuancheng Ren,
    Peng Li, Jie Zhou, and Xu Sun. Accelerating pre-trained language models via calibrated
    cascade. *CoRR*, abs/2012.14682, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Ji (2014) Qi Li and Heng Ji. Incremental joint extraction of entity mentions
    and relations. In *Proceedings of the 52nd Annual Meeting of the Association for
    Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume
    1: Long Papers*, pp.  402–412\. The Association for Computer Linguistics, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and
    Roberto Navigli (eds.), *Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event,
    August 1-6, 2021*, pp.  4582–4597\. Association for Computational Linguistics,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu,
    and Xuanjing Huang. Accelerating BERT inference for sequence labeling via early-exit.
    In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
    2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pp.  189–199\.
    Association for Computational Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021) Tailin Liang, John Glossner, Lei Wang, and Shaobo Shi.
    Pruning and quantization for deep neural network acceleration: A survey. *CoRR*,
    abs/2101.09671, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2021) Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, and
    Bin He. A global past-future early exit method for accelerating inference of pre-trained
    language models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
    Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and
    Yichao Zhou (eds.), *Proceedings of the 2021 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2021, Online, June 6-11, 2021*, pp.  2013–2023\. Association for Computational
    Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lienhart & Maydt (2002) Rainer Lienhart and Jochen Maydt. An extended set of
    haar-like features for rapid object detection. In *Proceedings. international
    conference on image processing*, volume 1, pp.  I–I. IEEE, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2017) Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural
    pruning. In *Advances in Neural Information Processing Systems 30: Annual Conference
    on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
    CA, USA*, pp.  2181–2191, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021) Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang
    Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou,
    Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin
    Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, and Hongxia Yang. M6: A chinese
    multimodal pretrainer. *CoRR*, abs/2103.00823, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft
    COCO: common objects in context. In David J. Fleet, Tomás Pajdla, Bernt Schiele,
    and Tinne Tuytelaars (eds.), *Computer Vision - ECCV 2014 - 13th European Conference,
    Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V*, volume 8693 of
    *Lecture Notes in Computer Science*, pp. 740–755\. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2018) Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng Ji. A multi-lingual
    multi-task architecture for low-resource sequence labeling. In *ACL (1)*, pp. 
    799–809\. Association for Computational Linguistics, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2020) Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng,
    Hao Zhou, and Lei Li. Pre-training multilingual neural machine translation by
    leveraging alignment information. In *EMNLP (1)*, pp.  2649–2663\. Association
    for Computational Linguistics, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018a) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L. Yuille, Jonathan Huang, and Kevin Murphy.
    Progressive neural architecture search. In *ECCV (1)*, volume 11205 of *Lecture
    Notes in Computer Science*, pp.  19–35\. Springer, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable
    architecture search. In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*, 2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Iou-Jen Liu, Jian Peng, and Alexander G. Schwing. Knowledge
    flow: Improve upon your teachers. In *7th International Conference on Learning
    Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net,
    2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018b) Liyuan Liu, Jingbo Shang, Xiang Ren, Frank Fangzheng Xu,
    Huan Gui, Jian Peng, and Jiawei Han. Empower sequence labeling with task-aware
    neural language model. In *AAAI*, pp.  5253–5260\. AAAI Press, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *CoRR*, abs/2107.13586, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020a) Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng,
    and Qi Ju. Fastbert: a self-distilling BERT with adaptive inference time. In Dan
    Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020*, pp. 6035–6044\. Association for Computational
    Linguistics, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018c) Xianggen Liu, Lili Mou, Haotian Cui, Zhengdong Lu, and Sen
    Song. JUMPER: learning when to make classification decisions in reading. *CoRR*,
    abs/1807.02314, 2018c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021b) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang,
    and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally
    across scales and tasks, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021c) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. GPT understands, too. *CoRR*, abs/2103.10385, 2021c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,
    Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising
    pre-training for neural machine translation. *Trans. Assoc. Comput. Linguistics*,
    8:726–742, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019c) Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor
    Darrell. Rethinking the value of network pruning. In *ICLR (Poster)*. OpenReview.net,
    2019c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2016) Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan.
    Unsupervised domain adaptation with residual transfer networks. In *NIPS*, pp. 
    136–144, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2019) Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
    Pretraining task-agnostic visiolinguistic representations for vision-and-language
    tasks. In *NeurIPS*, pp.  13–23, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2020) Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, and
    Xu Sun. CAPT: contrastive pre-training for learning denoised sequence representations.
    *CoRR*, abs/2010.06351, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2019) Jian-Hao Luo, Hao Zhang, Hong-Yu Zhou, Chen-Wei Xie, Jianxin
    Wu, and Weiyao Lin. Thinet: Pruning CNN filters for a thinner net. *IEEE Trans.
    Pattern Anal. Mach. Intell.*, 41(10):2525–2538, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2018) Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
    Neural architecture optimization. In *NeurIPS*, pp.  7827–7838, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luong et al. (2016) Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals,
    and Lukasz Kaiser. Multi-task sequence to sequence learning. In *ICLR (Poster)*,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2019) Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou,
    Ming Zhou, and Dawei Song. A tensorized transformer for language modeling. In
    *NeurIPS*, pp.  2229–2239, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MacKay et al. (2018) Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger B. Grosse.
    Reversible recurrent neural networks. In *NeurIPS*, pp.  9043–9054, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martins & Astudillo (2016) André F. T. Martins and Ramón Fernandez Astudillo.
    From softmax to sparsemax: A sparse model of attention and multi-label classification.
    In *ICML*, volume 48 of *JMLR Workshop and Conference Proceedings*, pp.  1614–1623\.
    JMLR.org, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maruf et al. (2019) Sameen Maruf, André F. T. Martins, and Gholamreza Haffari.
    Selective attention for context-aware neural machine translation. In *NAACL-HLT
    (1)*, pp.  3092–3102\. Association for Computational Linguistics, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCarley et al. (2019) JS McCarley, Rishav Chakravarti, and Avirup Sil. Structured
    pruning of a bert-based question answering model. *arXiv preprint arXiv:1910.06360*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McDonald et al. (2010) Ryan T. McDonald, Keith B. Hall, and Gideon Mann. Distributed
    training strategies for the structured perceptron. In *Human Language Technologies:
    Conference of the North American Chapter of the Association of Computational Linguistics,
    Proceedings, June 2-4, 2010, Los Angeles, California, USA*, pp.  456–464. The
    Association for Computational Linguistics, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mellor et al. (2020) Joseph Mellor, Jack Turner, Amos J. Storkey, and Elliot J.
    Crowley. Neural architecture search without training. *CoRR*, abs/2006.04647,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. Are sixteen
    heads really better than one? In *Advances in Neural Information Processing Systems
    32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, 2019, Vancouver, BC, Canada*, pp.  14014–14024, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013a) Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    Efficient estimation of word representations in vector space. In *ICLR (Workshop
    Poster)*, 2013a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013b) Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado,
    and Jeffrey Dean. Distributed representations of words and phrases and their compositionality.
    In *NIPS*, pp.  3111–3119, 2013b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirzadeh et al. (2020) Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir
    Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation
    via teacher assistant. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*, pp.  5191–5198\.
    AAAI Press, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishkin & Matas (2016) Dmytro Mishkin and Jiri Matas. All you need is a good
    init. In *ICLR (Poster)*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih & Hinton (2008) Andriy Mnih and Geoffrey E. Hinton. A scalable hierarchical
    distributed language model. In *NIPS*, pp.  1081–1088\. Curran Associates, Inc.,
    2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molchanov et al. (2017) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. Pruning convolutional neural networks for resource efficient inference.
    In *5th International Conference on Learning Representations, ICLR 2017, Toulon,
    France, April 24-26, 2017, Conference Track Proceedings*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morin & Bengio (2005) Frederic Morin and Yoshua Bengio. Hierarchical probabilistic
    neural network language model. In *AISTATS*. Society for Artificial Intelligence
    and Statistics, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mou et al. (2016) Lili Mou, Ran Jia, Yan Xu, Ge Li, Lu Zhang, and Zhi Jin.
    Distilling word embeddings: An encoding approach. In *CIKM*, pp.  1977–1980\.
    ACM, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narang et al. (2017a) Sharan Narang, Greg Diamos, Shubho Sengupta, and Erich
    Elsen. Exploring sparsity in recurrent neural networks. In *5th International
    Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
    2017, Conference Track Proceedings*, 2017a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narang et al. (2017b) Sharan Narang, Eric Undersander, and Gregory F. Diamos.
    Block-sparse recurrent neural networks. *CoRR*, abs/1711.02782, 2017b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen & Smeulders (2004) Hieu Tat Nguyen and Arnold W. M. Smeulders. Active
    learning using pre-clustering. In *ICML*, volume 69 of *ACM International Conference
    Proceeding Series*. ACM, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niculae & Blondel (2017) Vlad Niculae and Mathieu Blondel. A regularized framework
    for sparse and structured neural attention. In *NIPS*, pp.  3338–3348, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noach & Goldberg (2020) Matan Ben Noach and Yoav Goldberg. Compressing pre-trained
    language models by matrix decomposition. In Kam-Fai Wong, Kevin Knight, and Hua
    Wu (eds.), *Proceedings of the 1st Conference of the Asia-Pacific Chapter of the
    Association for Computational Linguistics and the 10th International Joint Conference
    on Natural Language Processing, AACL/IJCNLP 2020, Suzhou, China, December 4-7,
    2020*, pp.  884–889\. Association for Computational Linguistics, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oquab et al. (2014) Maxime Oquab, Léon Bottou, Ivan Laptev, and Josef Sivic.
    Learning and transferring mid-level image representations using convolutional
    neural networks. In *CVPR*, pp.  1717–1724\. IEEE Computer Society, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ott et al. (2016) Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, and Yoshua
    Bengio. Recurrent neural networks with limited numerical precision. *CoRR*, abs/1611.07065,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2021) Dae Young Park, Moon-Hyun Cha, Changwook Jeong, Daesin Kim,
    and Bohyung Han. Learning student-friendly teacher networks for knowledge distillation.
    *CoRR*, abs/2102.07650, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2015) Eunhyeok Park, Dongyoung Kim, Soobeom Kim, Yong-Deok Kim,
    Gunhee Kim, Sungroh Yoon, and Sungjoo Yoo. Big/little deep neural network for
    ultra low power inference. In Gabriela Nicolescu and Andreas Gerstlauer (eds.),
    *2015 International Conference on Hardware/Software Codesign and System Synthesis,
    CODES+ISSS 2015, Amsterdam, Netherlands, October 4-9, 2015*, pp. 124–132\. IEEE,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park & Kwak (2019) Seonguk Park and Nojun Kwak. FEED: feature-level ensemble
    for knowledge distillation. *CoRR*, abs/1909.10754, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2019) Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao
    Wu, Jiaheng Liu, Zhaoning Zhang, and Yu Liu. Correlation congruence for knowledge
    distillation. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV
    2019, Seoul, Korea (South), October 27 - November 2, 2019*, pp. 5006–5015\. IEEE,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D.
    Manning. Glove: Global vectors for word representation. In *EMNLP*, pp.  1532–1543\.
    ACL, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2019) Ben Peters, Vlad Niculae, and André F. T. Martins. Sparse
    sequence-to-sequence models. In *ACL (1)*, pp.  1504–1519\. Association for Computational
    Linguistics, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word
    representations. In *NAACL-HLT*, pp.  2227–2237\. Association for Computational
    Linguistics, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. Language models
    as knowledge bases? In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.),
    *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
    and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
    2019, Hong Kong, China, November 3-7, 2019*, pp.  2463–2473\. Association for
    Computational Linguistics, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pfeiffer et al. (2020) Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya
    Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. Adapterhub:
    A framework for adapting transformers. In *EMNLP (Demos)*, pp.  46–54\. Association
    for Computational Linguistics, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pham et al. (2018) Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.
    Efficient neural architecture search via parameters sharing. In *International
    Conference on Machine Learning*, pp. 4095–4104\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pleiss et al. (2017) Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens
    van der Maaten, and Kilian Q. Weinberger. Memory-efficient implementation of densenets.
    *CoRR*, abs/1707.06990, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plummer et al. (2020) Bryan A Plummer, Nikoli Dryden, Julius Frost, Torsten
    Hoefler, and Kate Saenko. Neural parameter allocation search. *arXiv preprint
    arXiv:2006.10598*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model
    compression via distillation and quantization. In *6th International Conference
    on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
    3, 2018, Conference Track Proceedings*. OpenReview.net, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prasanna et al. (2020) Sai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT
    plays the lottery, all tickets are winning. In *EMNLP (1)*, pp.  3208–3229\. Association
    for Computational Linguistics, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prato et al. (2020) Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh.
    Fully quantized transformer for machine translation. In Trevor Cohn, Yulan He,
    and Yang Liu (eds.), *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November
    2020*, pp.  1–14. Association for Computational Linguistics, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Press & Wolf (2017) Ofir Press and Lior Wolf. Using the output embedding to
    improve language models. In *EACL (2)*, pp.  157–163\. Association for Computational
    Linguistics, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2020) Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng
    Chen, Ruofei Zhang, and Ming Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence
    pre-training. In Trevor Cohn, Yulan He, and Yang Liu (eds.), *Findings of the
    Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November
    2020*, volume EMNLP 2020 of *Findings of ACL*, pp. 2401–2410, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2018) Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Padmanabhan,
    and Graham Neubig. When and why are pre-trained word embeddings useful for neural
    machine translation? In *NAACL-HLT (2)*, pp.  529–535\. Association for Computational
    Linguistics, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2020) Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai,
    and Xuanjing Huang. Pre-trained models for natural language processing: A survey.
    *CoRR*, abs/2003.08271, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21:140:1–140:67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: memory optimizations toward training trillion parameter
    models. In *SC*, pp.  20\. IEEE/ACM, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In
    Jian Su, Xavier Carreras, and Kevin Duh (eds.), *Proceedings of the 2016 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas,
    USA, November 1-4, 2016*, pp.  2383–2392\. The Association for Computational Linguistics,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. Know
    what you don’t know: Unanswerable questions for squad. In Iryna Gurevych and Yusuke
    Miyao (eds.), *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short
    Papers*, pp. 784–789\. Association for Computational Linguistics, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramsundar et al. (2015) Bharath Ramsundar, Steven M. Kearnes, Patrick Riley,
    Dale Webster, David E. Konerding, and Vijay S. Pande. Massively multitask networks
    for drug discovery. *CoRR*, abs/1502.02072, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranganathan et al. (2017) Hiranmayi Ranganathan, Hemanth Venkateswara, Shayok
    Chakraborty, and Sethuraman Panchanathan. Deep active learning for image classification.
    In *ICIP*, pp.  3934–3938\. IEEE, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional
    neural networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.),
    *Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands,
    October 11-14, 2016, Proceedings, Part IV*, volume 9908 of *Lecture Notes in Computer
    Science*, pp.  525–542\. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le.
    Regularized evolution for image classifier architecture search. In *The Thirty-Third
    AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative
    Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,
    Hawaii, USA, January 27 - February 1, 2019*, pp.  4780–4789, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reddy et al. (2019) Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa:
    A conversational question answering challenge. *Trans. Assoc. Comput. Linguistics*,
    7:249–266, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
    Faster R-CNN: towards real-time object detection with region proposal networks.
    In *NIPS*, pp.  91–99, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rigamonti et al. (2013) Roberto Rigamonti, Amos Sironi, Vincent Lepetit, and
    Pascal Fua. Learning separable filters. In *2013 IEEE Conference on Computer Vision
    and Pattern Recognition, Portland, OR, USA, June 23-28, 2013*, pp.  2754–2761\.
    IEEE Computer Society, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Romero et al. (2015) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
    Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep
    nets. In Yoshua Bengio and Yann LeCun (eds.), *3rd International Conference on
    Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
    Track Proceedings*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy & McCallum (2001) Nicholas Roy and Andrew McCallum. Toward optimal active
    learning through monte carlo estimation of error reduction. *ICML, Williamstown*,
    2:441–448, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder (2019) Sebastian Ruder. *Neural transfer learning for natural language
    processing*. PhD thesis, NUI Galway, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ruder et al. (2017) Sebastian Ruder, Parsa Ghaffari, and John G. Breslin. Knowledge
    adaptation: Teaching to adapt. *CoRR*, abs/1702.02052, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder et al. (2019) Sebastian Ruder, Ivan Vulic, and Anders Søgaard. A survey
    of cross-lingual word embedding models. *J. Artif. Intell. Res.*, 65:569–631,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rueda-Plata et al. (2015) Diego Rueda-Plata, Raúl Ramos-Pollán, and Fabio A.
    González. Supervised greedy layer-wise training for deep convolutional networks
    with small datasets. In Manuel Núñez, Ngoc Thanh Nguyen, David Camacho, and Bogdan
    Trawinski (eds.), *Computational Collective Intelligence - 7th International Conference,
    ICCCI 2015, Madrid, Spain, September 21-23, 2015. Proceedings, Part I*, volume
    9329 of *Lecture Notes in Computer Science*, pp.  275–284\. Springer, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rush et al. (2015) Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural
    attention model for abstractive sentence summarization. In Lluís Màrquez, Chris
    Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (eds.), *Proceedings
    of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2015, Lisbon, Portugal, September 17-21, 2015*, pp.  379–389\. The Association
    for Computational Linguistics, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sainath et al. (2013) Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru
    Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural
    network training with high-dimensional output targets. In *ICASSP*, pp.  6655–6659\.
    IEEE, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salimans & Kingma (2016) Tim Salimans and Diederik P. Kingma. Weight normalization:
    A simple reparameterization to accelerate training of deep neural networks. In
    *NIPS*, pp.  901, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler et al. (2018) Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey
    Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks.
    In *2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,
    Salt Lake City, UT, USA, June 18-22, 2018*, pp. 4510–4520\. Computer Vision Foundation
    / IEEE Computer Society, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement
    pruning: Adaptive sparsity by fine-tuning. In Hugo Larochelle, Marc’Aurelio Ranzato,
    Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santurkar et al. (2018) Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and
    Aleksander Madry. How does batch normalization help optimization? In *NeurIPS*,
    pp.  2488–2498, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Savarese & Maire (2019) Pedro Savarese and Michael Maire. Learning implicitly
    recurrent cnns through parameter sharing. In *ICLR (Poster)*. OpenReview.net,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saxe et al. (2014) Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact
    solutions to the nonlinear dynamics of learning in deep linear neural networks.
    In *ICLR*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scao & Rush (2021) Teven Le Scao and Alexander M. Rush. How many data points
    is a prompt worth? In *NAACL-HLT*, pp.  2627–2636\. Association for Computational
    Linguistics, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick & Schütze (2021) Timo Schick and Hinrich Schütze. It’s not just size
    that matters: Small language models are also few-shot learners. In *NAACL-HLT*,
    pp.  2339–2352\. Association for Computational Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schröder et al. (2021) Christopher Schröder, Andreas Niekler, and Martin Potthast.
    Uncertainty-based query strategies for active learning with transformers. *CoRR*,
    abs/2107.05687, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schuster et al. (2021) Tal Schuster, Adam Fisch, Tommi S. Jaakkola, and Regina
    Barzilay. Consistent accelerated inference via confident adaptive transformers.
    *CoRR*, abs/2104.08803, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwartz et al. (2020a) Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni.
    Green AI. *Commun. ACM*, 63(12):54–63, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schwartz et al. (2020b) Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta,
    Jesse Dodge, and Noah A. Smith. The right tool for the job: Matching model and
    instance complexities. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
    Tetreault (eds.), *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics, ACL 2020, Online, July 5-10, 2020*, pp. 6640–6651\.
    Association for Computational Linguistics, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See et al. (2016) Abigail See, Minh-Thang Luong, and Christopher D. Manning.
    Compression of neural machine translation models via pruning. In *Proceedings
    of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL
    2016, Berlin, Germany, August 11-12, 2016*, pp.  291–301, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. Get
    to the point: Summarization with pointer-generator networks. In Regina Barzilay
    and Min-Yen Kan (eds.), *Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4,
    Volume 1: Long Papers*, pp. 1073–1083\. Association for Computational Linguistics,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sener & Savarese (2018) Ozan Sener and Silvio Savarese. Active learning for
    convolutional neural networks: A core-set approach. In *ICLR (Poster)*. OpenReview.net,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural
    machine translation of rare words with subword units. In *ACL (1)*. The Association
    for Computer Linguistics, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seo et al. (2018) Min Joon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi.
    Neural speed reading via skim-rnn. In *6th International Conference on Learning
    Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
    Track Proceedings*. OpenReview.net, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2018) Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod,
    and Animashree Anandkumar. Deep active learning for named entity recognition.
    In *ICLR (Poster)*. OpenReview.net, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2017) Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen.
    Reasonet: Learning to stop reading in machine comprehension. In *Proceedings of
    the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
    Halifax, NS, Canada, August 13 - 17, 2017*, pp.  1047–1055\. ACM, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. (2018) Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang
    Shen, and Mickey Aleksic. A quantization-friendly separable convolution for mobilenets.
    *CoRR*, abs/1803.08607, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2020) Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and
    Jiyan Yang. Compositional embeddings using complementary partitions for memory-efficient
    recommendation systems. In *KDD*, pp.  165–175\. ACM, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2021) Wenxian Shi, Yuxuan Song, Hao Zhou, Bohan Li, and Lei Li.
    Learning from deep model via exploring local targets, 2021. URL [https://openreview.net/forum?id=5slGDu_bVc6](https://openreview.net/forum?id=5slGDu_bVc6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu & Nakayama (2018) Raphael Shu and Hideki Nakayama. Compressing word embeddings
    via deep compositional code learning. In *ICLR (Poster)*. OpenReview.net, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu et al. (2020) Yao Shu, Wei Wang, and Shaofeng Cai. Understanding architectures
    learnt by cell-based neural architecture search. In *ICLR*. OpenReview.net, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simon et al. (2016) Marcel Simon, Erik Rodner, and Joachim Denzler. Imagenet
    pre-trained models with batch normalization. *CoRR*, abs/1612.01452, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan & Zisserman (2015) Karen Simonyan and Andrew Zisserman. Very deep convolutional
    networks for large-scale image recognition. In *ICLR*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So et al. (2019) David R. So, Quoc V. Le, and Chen Liang. The evolved transformer.
    In *Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA*, pp. 5877–5886, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In *Proceedings of the
    2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013,
    18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of
    SIGDAT, a Special Interest Group of the ACL*, pp.  1631–1642\. ACL, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Søgaard & Goldberg (2016) Anders Søgaard and Yoav Goldberg. Deep multi-task
    learning with low level tasks supervised at lower layers. In *ACL (2)*. The Association
    for Computer Linguistics, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soudry et al. (2014) Daniel Soudry, Itay Hubara, and Ron Meir. Expectation
    backpropagation: Parameter-free training of multilayer neural networks with continuous
    or discrete weights. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D.
    Lawrence, and Kilian Q. Weinberger (eds.), *Advances in Neural Information Processing
    Systems 27: Annual Conference on Neural Information Processing Systems 2014, December
    8-13 2014, Montreal, Quebec, Canada*, pp.  963–971, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas & Babu (2015) Suraj Srinivas and R. Venkatesh Babu. Data-free parameter
    pruning for deep neural networks. In *BMVC*, pp.  31.1–31.12\. BMVA Press, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas & Fleuret (2018) Suraj Srinivas and François Fleuret. Knowledge transfer
    with jacobian matching. In Jennifer G. Dy and Andreas Krause (eds.), *Proceedings
    of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
    Stockholm, Sweden, July 10-15, 2018*, volume 80 of *Proceedings of Machine Learning
    Research*, pp.  4730–4738\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stanton et al. (2021) Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A.
    Alemi, and Andrew Gordon Wilson. Does knowledge distillation really work? *CoRR*,
    abs/2106.05945, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strubell et al. (2019) Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy
    and policy considerations for deep learning in NLP. In *Proceedings of the 57th
    Conference of the Association for Computational Linguistics, ACL 2019, Florence,
    Italy, July 28- August 2, 2019, Volume 1: Long Papers*, pp.  3645–3650, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sukhbaatar et al. (2019) Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski,
    and Armand Joulin. Adaptive attention span in transformers. In *ACL (1)*, pp. 
    331–335\. Association for Computational Linguistics, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019a) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge
    distillation for BERT model compression. In Kentaro Inui, Jing Jiang, Vincent
    Ng, and Xiaojun Wan (eds.), *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*,
    pp.  4322–4331\. Association for Computational Linguistics, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Tianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu Zhang, Hao
    Jiang, Zhao Cao, Xuanjing Huang, and Xipeng Qiu. Early exiting with ensemble internal
    classifiers. *CoRR*, abs/2105.13792, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019b) Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen,
    Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. ERNIE: enhanced representation
    through knowledge integration. *CoRR*, abs/1904.09223, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton & McCallum (2012) Charles Sutton and Andrew McCallum. An introduction
    to conditional random fields. *Found. Trends Mach. Learn.*, 4(4):267–373, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzuki et al. (2001) Kenji Suzuki, Isao Horiba, and Noboru Sugie. A simple neural
    network pruning algorithm with application to filter synthesis. *Neural Process.
    Lett.*, 13(1):43–53, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
    Rabinovich. Going deeper with convolutions. In *CVPR*, pp.  1–9\. IEEE Computer
    Society, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2017) Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
    Alexander A. Alemi. Inception-v4, inception-resnet and the impact of residual
    connections on learning. In *AAAI*, pp.  4278–4284\. AAAI Press, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tailor et al. (2021) Shyam Anil Tailor, Javier Fernandez-Marques, and Nicholas Donald
    Lane. Degree-quant: Quantization-aware training for graph neural networks. In
    *International Conference on Learning Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takase & Kiyono (2021) Sho Takase and Shun Kiyono. Lessons on parameter sharing
    across layers in transformers. *CoRR*, abs/2104.06022, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tambe et al. (2021) Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu
    Jia, En-Yu Yang, Marco Donato, Victor Sanh, Paul N. Whatmough, Alexander M. Rush,
    David Brooks, and Gu-Yeon Wei. Edgebert: Sentence-level energy optimizations for
    latency-aware multi-task NLP inference. In *MICRO*, pp.  830–844\. ACM, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan & Le (2019) Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model
    scaling for convolutional neural networks. 97:6105–6114, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2019a) Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark
    Sandler, Andrew Howard, and Quoc V. Le. Mnasnet: Platform-aware neural architecture
    search for mobile. In *CVPR*, pp.  2820–2828\. Computer Vision Foundation / IEEE,
    2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2019b) Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu.
    Multilingual neural machine translation with knowledge distillation. In *ICLR
    (Poster)*. OpenReview.net, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang & Kwan (1993) C.Z. Tang and H.K. Kwan. Multilayer feedforward neural networks
    with single powers-of-two weights. *IEEE Transactions on Signal Processing*, 41(8):2724–2727,
    1993. doi: 10.1109/78.229903.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tarvainen & Valpola (2017) Antti Tarvainen and Harri Valpola. Mean teachers
    are better role models: Weight-averaged consistency targets improve semi-supervised
    deep learning results. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
    Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), *Advances
    in Neural Information Processing Systems 30: Annual Conference on Neural Information
    Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, pp.  1195–1204,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teerapittayanon et al. (2016) Surat Teerapittayanon, Bradley McDanel, and H. T.
    Kung. Branchynet: Fast inference via early exiting from deep neural networks.
    In *23rd International Conference on Pattern Recognition, ICPR 2016, Cancún, Mexico,
    December 4-8, 2016*, pp.  2464–2469\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2020) Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive
    multiview coding. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael
    Frahm (eds.), *Computer Vision - ECCV 2020 - 16th European Conference, Glasgow,
    UK, August 23-28, 2020, Proceedings, Part XI*, volume 12356 of *Lecture Notes
    in Computer Science*, pp.  776–794\. Springer, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tung & Mori (2019) Frederick Tung and Greg Mori. Similarity-preserving knowledge
    distillation. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV
    2019, Seoul, Korea (South), October 27 - November 2, 2019*, pp. 1365–1374\. IEEE,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upadhyay et al. (2016) Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and Dan
    Roth. Cross-lingual models of word embeddings: An empirical comparison. In *ACL
    (1)*. The Association for Computer Linguistics, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van den Oord et al. (2018) Aäron van den Oord, Yazhe Li, and Oriol Vinyals.
    Representation learning with contrastive predictive coding. *CoRR*, abs/1807.03748,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanhoucke et al. (2011) Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. Improving
    the speed of neural networks on CPUs. In *Proc. Deep Learning and Unsupervised
    Feature Learning NIPS Workshop*, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *NIPS*, pp.  5998–6008, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veit & Belongie (2020) Andreas Veit and Serge J. Belongie. Convolutional networks
    with adaptive inference graphs. *Int. J. Comput. Vis.*, 128(3):730–741, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vijayanarasimhan et al. (2015) Sudheendra Vijayanarasimhan, Jonathon Shlens,
    Rajat Monga, and Jay Yagnik. Deep networks with large output spaces. In *ICLR
    (Workshop)*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola & Jones (2001) Paul Viola and Michael Jones. Rapid object detection using
    a boosted cascade of simple features. In *Proceedings of the 2001 IEEE computer
    society conference on computer vision and pattern recognition. CVPR 2001*, volume 1,
    pp.  I–I. Ieee, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola & Jones (2004) Paul Viola and Michael J Jones. Robust real-time face detection.
    *International journal of computer vision*, 57(2):137–154, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. In *Proceedings of the 57th Conference
    of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
    28- August 2, 2019, Volume 1: Long Papers*, pp.  5797–5808, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. (2021) Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel
    Cer. Spot: Better frozen model adaptation through soft prompt transfer. *arXiv
    preprint arXiv:2110.07904*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform
    for natural language understanding. In *7th International Conference on Learning
    Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net,
    2019a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine
    translation with byte-level subwords. In *AAAI*, pp.  9154–9160\. AAAI Press,
    2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Hanchen Wang, Defu Lian, Ying Zhang, Lu Qin, Xiangjian He,
    Yiguang Lin, and Xuemin Lin. Binarized graph neural network. *World Wide Web*,
    24(3):825–848, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Linnan Wang, Wei Wu, Zenglin Xu, Jianxiong Xiao, and Yi Yang.
    BLASX: A high performance level-3 BLAS library for heterogeneous multi-gpu computing.
    In *ICS*, pp.  20:1–20:11\. ACM, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018a) Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon
    Song, Zenglin Xu, and Tim Kraska. Superneurons: dynamic GPU memory management
    for training deep neural networks. In *PPOPP*, pp.  41–53\. ACM, 2018a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Min Wang, Baoyuan Liu, and Hassan Foroosh. Factorized convolutional
    neural networks. In *ICCV Workshops*, pp.  545–553\. IEEE Computer Society, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018b) Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng
    Wang, and Yuan Xie. Hitnet: Hybrid ternary recurrent neural network. In Samy Bengio,
    Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman
    Garnett (eds.), *Advances in Neural Information Processing Systems 31: Annual
    Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
    3-8, 2018, Montréal, Canada*, pp.  602–612, 2018b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019b) Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Distilling
    object detectors with fine-grained feature imitation. In *IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*,
    pp. 4933–4942\. Computer Vision Foundation / IEEE, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang & Isola (2020) Tongzhou Wang and Phillip Isola. Understanding contrastive
    representation learning through alignment and uniformity on the hypersphere. In
    *Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
    13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning
    Research*, pp.  9929–9939\. PMLR, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Wei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi
    Bao, Liwei Peng, and Luo Si. Structbert: Incorporating language structures into
    pre-training for deep language understanding. In *8th International Conference
    on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*.
    OpenReview.net, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, and
    Lei Li. LightSeq: A high performance inference library for transformers. In *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT)*, pp.  113–120\.
    Association for Computational Linguistics, June 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018c) Xin Wang, Yujia Luo, Daniel Crankshaw, Alexey Tumanov,
    Fisher Yu, and Joseph E. Gonzalez. IDK cascades: Fast deep learning by learning
    not to overthink. In Amir Globerson and Ricardo Silva (eds.), *Proceedings of
    the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018,
    Monterey, California, USA, August 6-10, 2018*, pp.  580–590\. AUAI Press, 2018c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018d) Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E.
    Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In Vittorio
    Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), *Computer
    Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14,
    2018, Proceedings, Part XIII*, volume 11217 of *Lecture Notes in Computer Science*,
    pp.  420–436\. Springer, 2018d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Yong Wang, Longyue Wang, Victor O. K. Li, and Zhaopeng Tu.
    On the sparsity of neural machine translation models. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
    November 16-20, 2020*, pp.  1060–1066, 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020d) Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang,
    and Gao Huang. Glance and focus: a dynamic approach to reducing spatial redundancy
    in image classification. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
    Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021c) Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao
    Huang. Not all images are worth 16x16 words: Dynamic vision transformers with
    adaptive sequence length. *CoRR*, abs/2105.15075, 2021c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warstadt et al. (2019) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman.
    Neural network acceptability judgments. *Trans. Assoc. Comput. Linguistics*, 7:625–641,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models
    are zero-shot learners. *CoRR*, abs/2109.01652, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R. Bowman.
    A broad-coverage challenge corpus for sentence understanding through inference.
    In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), *Proceedings of the 2018
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana,
    USA, June 1-6, 2018, Volume 1 (Long Papers)*, pp.  1112–1122\. Association for
    Computational Linguistics, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Winata et al. (2019) Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J.
    Barezi, and Pascale Fung. On the effectiveness of low-rank matrix factorization
    for LSTM model compression. *CoRR*, abs/1908.09982, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei
    Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet:
    Hardware-aware efficient convnet design via differentiable neural architecture
    search. In *CVPR*, pp.  10734–10742\. Computer Vision Foundation / IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2018a) Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and
    inference with integers in deep neural networks. In *6th International Conference
    on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
    3, 2018, Conference Track Proceedings*. OpenReview.net, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu & He (2020) Yuxin Wu and Kaiming He. Group normalization. *Int. J. Comput.
    Vis.*, 128(3):742–755, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun,
    and Hao Ma. CLEAR: contrastive learning for sentence representation. *CoRR*, abs/2012.15466,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2018b) Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie,
    Larry S. Davis, Kristen Grauman, and Rogério Schmidt Feris. Blockdrop: Dynamic
    inference paths in residual networks. In *2018 IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*,
    pp. 8817–8826\. Computer Vision Foundation / IEEE Computer Society, 2018b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiang et al. (2020) Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from
    multiple experts: Self-paced knowledge distillation for long-tailed classification.
    In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), *Computer
    Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020,
    Proceedings, Part V*, volume 12350 of *Lecture Notes in Computer Science*, pp. 
    247–263\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xin et al. (2020) Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy
    Lin. Deebert: Dynamic early exiting for accelerating BERT inference. In Dan Jurafsky,
    Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,
    Online, July 5-10, 2020*, pp. 2246–2251\. Association for Computational Linguistics,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xin et al. (2021) Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. Berxit:
    Early exiting for BERT with better fine-tuning and extension to regression. In
    Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (eds.), *Proceedings of the 16th
    Conference of the European Chapter of the Association for Computational Linguistics:
    Main Volume, EACL 2021, Online, April 19 - 23, 2021*, pp.  91–104\. Association
    for Computational Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020) Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.
    Bert-of-theseus: Compressing BERT by progressive module replacing. In Bonnie Webber,
    Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November
    16-20, 2020*, pp. 7859–7869\. Association for Computational Linguistics, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021a) Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian J. McAuley,
    and Furu Wei. Beyond preserved accuracy: Evaluating loyalty and robustness of
    BERT compression. *CoRR*, abs/2109.03228, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang
    Lin. Understanding and improving layer normalization. In *NeurIPS*, pp.  4383–4393,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021b) Jingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun,
    and Hongxia Yang. KNAS: green neural architecture search. In *ICML*, volume 139
    of *Proceedings of Machine Learning Research*, pp.  11613–11625\. PMLR, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018) Kaisheng Xu, Xu Shen, Ting Yao, Xinmei Tian, and Tao Mei. Greedy
    layer-wise training of long short term memory networks. In *2018 IEEE International
    Conference on Multimedia & Expo Workshops, ICME Workshops 2018, San Diego, CA,
    USA, July 23-27, 2018*, pp. 1–6\. IEEE Computer Society, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2014) Jian Xue, Jinyu Li, Dong Yu, Mike Seltzer, and Yifan Gong.
    Singular value decomposition based low-footprint speaker adaptation and personalization
    for deep neural network. In *IEEE International Conference on Acoustics, Speech
    and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014*, pp. 6359–6363,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual
    pre-trained text-to-text transformer. In Kristina Toutanova, Anna Rumshisky, Luke
    Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy
    Chakraborty, and Yichao Zhou (eds.), *Proceedings of the 2021 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*, pp.  483–498\.
    Association for Computational Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020a) Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He,
    and Jingqiao Zhang. Progressively stacking 2.0: A multi-stage layerwise training
    method for BERT training speedup. *CoRR*, abs/2011.13635, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2020b) Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and
    Gao Huang. Resolution adaptive networks for efficient inference. In *2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
    USA, June 13-19, 2020*, pp. 2366–2375\. Computer Vision Foundation / IEEE, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2017) Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen.
    Transfer learning for sequence tagging with hierarchical recurrent networks. In
    *ICLR (Poster)*. OpenReview.net, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell,
    Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining
    for language understanding. In *Advances in Neural Information Processing Systems
    32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, 2019, Vancouver, BC, Canada*, pp.  5754–5764, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yim et al. (2017) Junho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim. A gift
    from knowledge distillation: Fast optimization, network minimization and transfer
    learning. In *2017 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2017, Honolulu, HI, USA, July 21-26, 2017*, pp. 7130–7138\. IEEE Computer
    Society, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ying et al. (2019) Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real,
    Kevin Murphy, and Frank Hutter. Nas-bench-101: Towards reproducible neural architecture
    search. In *Proceedings of the 36th International Conference on Machine Learning,
    ICML 2019, 9-15 June 2019, Long Beach, California, USA*, pp. 7105–7114, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoo & Kweon (2019) Donggeun Yoo and In So Kweon. Learning loss for active learning.
    In *CVPR*, pp.  93–102\. Computer Vision Foundation / IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yosinski et al. (2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
    How transferable are features in deep neural networks? In *NIPS*, pp.  3320–3328,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You et al. (2017) Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from
    multiple teacher networks. In *Proceedings of the 23rd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August
    13 - 17, 2017*, pp.  1285–1294\. ACM, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2017) Adams Wei Yu, Hongrae Lee, and Quoc V. Le. Learning to skim
    text. In Regina Barzilay and Min-Yen Kan (eds.), *Proceedings of the 55th Annual
    Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,
    Canada, July 30 - August 4, Volume 1: Long Papers*, pp. 1880–1890\. Association
    for Computational Linguistics, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu & Huang (2019) Jiahui Yu and Thomas S. Huang. Universally slimmable networks
    and improved training techniques. In *2019 IEEE/CVF International Conference on
    Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019*,
    pp. 1803–1811\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019) Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas S.
    Huang. Slimmable neural networks. In *7th International Conference on Learning
    Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Keyi Yu, Yang Liu, Alexander G. Schwing, and Jian Peng. Fast
    and accurate text classification: Skimming, rereading and early stopping. In *6th
    International Conference on Learning Representations, ICLR 2018, Vancouver, BC,
    Canada, April 30 - May 3, 2018, Workshop Track Proceedings*. OpenReview.net, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu & Zhu (2020) Tong Yu and Hong Zhu. Hyper-parameter optimization: A review
    of algorithms and applications. *CoRR*, abs/2003.05689, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    Q8BERT: quantized 8bit BERT. *CoRR*, abs/1910.06188, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zagoruyko & Komodakis (2017) Sergey Zagoruyko and Nikos Komodakis. Paying more
    attention to attention: Improving the performance of convolutional neural networks
    via attention transfer. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In *NeurIPS*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler & Fergus (2014) Matthew D. Zeiler and Rob Fergus. Visualizing and understanding
    convolutional networks. In *ECCV (1)*, volume 8689 of *Lecture Notes in Computer
    Science*, pp.  818–833\. Springer, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zela et al. (2020) Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi,
    Thomas Brox, and Frank Hutter. Understanding and robustifying differentiable architecture
    search. In *8th International Conference on Learning Representations, ICLR 2020,
    Addis Ababa, Ethiopia, April 26-30, 2020*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019) Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Fixup initialization:
    Residual learning without normalization. In *ICLR (Poster)*. OpenReview.net, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang & Stadie (2020) Matthew Shunshi Zhang and Bradly C. Stadie. One-shot pruning
    of recurrent neural networks by jacobian spectrum evaluation. In *8th International
    Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
    26-30, 2020*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit BERT. In
    Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,
    Online, November 16-20, 2020*, pp. 509–521\. Association for Computational Linguistics,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Ying Zhang, Tao Xiang, Timothy M. Hospedales, and Huchuan
    Lu. Deep mutual learning. In *2018 IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*, pp. 4320–4328\.
    IEEE Computer Society, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Zhi Zhang, Guanghan Ning, and Zhihai He. Knowledge projection
    for deep neural networks. *CoRR*, abs/1710.09505, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020) Yiren Zhao, Duo Wang, Daniel Bates, Robert D. Mullins, Mateja
    Jamnik, and Pietro Liò. Learned low precision graph neural networks. *CoRR*, abs/2009.09232,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2015) Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes,
    Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr. Conditional
    random fields as recurrent neural networks. In *2015 IEEE International Conference
    on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015*, pp.  1529–1537,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2019) Mingyi Zhou, Yipeng Liu, Zhen Long, Longxi Chen, and Ce Zhu.
    Tensor rank learning in CP decomposition via convolutional neural network. *Signal
    Process. Image Commun.*, 73:12–21, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020) Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley,
    Ke Xu, and Furu Wei. BERT loses patience: Fast and robust inference with early
    exit. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
    and Hsuan-Tien Lin (eds.), *Advances in Neural Information Processing Systems
    33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
    December 6-12, 2020, virtual*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021a) Wangchunshu Zhou, Tao Ge, Ke Xu, and Furu Wei. Improving
    sequence-to-sequence pre-training via sequence span rewriting. *CoRR*, abs/2101.00416,
    2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021b) Wangchunshu Zhou, Canwen Xu, and Julian J. McAuley. Meta
    learning for knowledge distillation. *CoRR*, abs/2106.04570, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu (2021) Wei Zhu. Leebert: Learned early exit for BERT with cross-level optimization.
    In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
    2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pp.  2968–2980\.
    Association for Computational Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
