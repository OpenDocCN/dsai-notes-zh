- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:49:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:49:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2111.05193] A Survey on Green Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2111.05193] 关于绿色深度学习的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.05193](https://ar5iv.labs.arxiv.org/html/2111.05193)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2111.05193](https://ar5iv.labs.arxiv.org/html/2111.05193)
- en: A Survey on Green Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于绿色深度学习的调查
- en: Jingjing Xu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jingjing Xu
- en: ByteDance AI Lab
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 字节跳动 AI 实验室
- en: xujingjing.melody@bytedance.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: xujingjing.melody@bytedance.com
- en: 'jingjingxu@pku.edu.cn &Wangchunshu Zhou¹¹footnotemark: 1'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: jingjingxu@pku.edu.cn & Wangchunshu Zhou¹¹脚注标记：1
- en: ByteDance AI Lab
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 字节跳动 AI 实验室
- en: 'zhouwangchunshu.7@bytedance.com &Zhiyi Fu¹¹footnotemark: 1'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: zhouwangchunshu.7@bytedance.com & Zhiyi Fu¹¹脚注标记：1
- en: Peking University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: ypfzy@pku.edu.cn &Hao Zhou
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ypfzy@pku.edu.cn & Hao Zhou
- en: ByteDance AI Lab
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 字节跳动 AI 实验室
- en: zhouhao.nlp@bytedance.com &Lei Li
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: zhouhao.nlp@bytedance.com & Lei Li
- en: University of California, Santa Barbara
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学圣塔芭芭拉分校
- en: lilei@ucsb.edu Equal ContributionThis work is done during internship at ByteDance
    AI Lab.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: lilei@ucsb.edu 平等贡献 本工作在字节跳动 AI 实验室实习期间完成。
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, larger and deeper models are springing up and continuously
    pushing state-of-the-art (SOTA) results across various fields like natural language
    processing (NLP) and computer vision (CV). However, despite promising results,
    it needs to be noted that the computations required by SOTA models have been increased
    at an exponential rate. Massive computations not only have a surprisingly large
    carbon footprint but also have negative effects on research inclusiveness and
    deployment on real-world applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，更大更深的模型层出不穷，并在自然语言处理（NLP）和计算机视觉（CV）等多个领域不断推动**最先进（SOTA）**的结果。然而，尽管结果令人期待，但需要注意的是，**SOTA**模型所需的计算量呈指数级增长。大量计算不仅具有惊人的碳足迹，还有对研究包容性和现实应用部署产生负面影响。
- en: 'Green deep learning is an increasingly hot research field that appeals to researchers
    to pay attention to energy usage and carbon emission during model training and
    inference. The target is to yield novel results with lightweight and efficient
    technologies. Many technologies can be used to achieve this goal, like model compression
    and knowledge distillation. This paper focuses on presenting a systematic review
    of the development of Green deep learning technologies. We classify these approaches
    into four categories: (1) compact networks, (2) energy-efficient training strategies,
    (3) energy-efficient inference approaches, and (4) efficient data usage. For each
    category, we discuss the progress that has been achieved and the unresolved challenges.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色深度学习是一个越来越热门的研究领域，吸引研究人员关注模型训练和推断中的能量使用和碳排放。目标是通过轻量化和高效的技术获得新颖的结果。许多技术可以实现这一目标，如模型压缩和知识蒸馏。本文重点介绍了绿色深度学习技术的发展系统性综述。我们将这些方法分为四类：（1）紧凑网络，（2）节能训练策略，（3）节能推断方法，以及（4）高效数据使用。对于每一类，我们讨论了已取得的进展和尚未解决的挑战。
- en: Contents
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#Ch1 "In A Survey on Green Deep Learning")'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 介绍](#Ch1 "在关于绿色深度学习的调查")'
- en: '[1.1 Deep Learning](#Ch1.S1 "In Chapter 1 Introduction ‣ A Survey on Green
    Deep Learning")'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.1 深度学习](#Ch1.S1 "在第1章介绍 ‣ 关于绿色深度学习的调查")'
- en: '[1.2 Green Deep Learning](#Ch1.S2 "In Chapter 1 Introduction ‣ A Survey on
    Green Deep Learning")'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.2 绿色深度学习](#Ch1.S2 "在第1章介绍 ‣ 关于绿色深度学习的调查")'
- en: '[1.2.1 Definition](#Ch1.S2.SS1 "In 1.2 Green Deep Learning ‣ Chapter 1 Introduction
    ‣ A Survey on Green Deep Learning")'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.2.1 定义](#Ch1.S2.SS1 "在1.2绿色深度学习 ‣ 第1章介绍 ‣ 关于绿色深度学习的调查")'
- en: '[1.2.2 Measure](#Ch1.S2.SS2 "In 1.2 Green Deep Learning ‣ Chapter 1 Introduction
    ‣ A Survey on Green Deep Learning")'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.2.2 测量](#Ch1.S2.SS2 "在1.2绿色深度学习 ‣ 第1章介绍 ‣ 关于绿色深度学习的调查")'
- en: '[1.2.3 Broader Impact](#Ch1.S2.SS3 "In 1.2 Green Deep Learning ‣ Chapter 1
    Introduction ‣ A Survey on Green Deep Learning")'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.2.3 更广泛的影响](#Ch1.S2.SS3 "在1.2绿色深度学习 ‣ 第1章介绍 ‣ 关于绿色深度学习的调查")'
- en: '[1.3 Outline of the Survey](#Ch1.S3 "In Chapter 1 Introduction ‣ A Survey on
    Green Deep Learning")'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3 调查大纲](#Ch1.S3 "在第1章介绍 ‣ 关于绿色深度学习的调查")'
- en: '[2 Compact Architecture](#Ch2 "In A Survey on Green Deep Learning")'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 紧凑架构](#Ch2 "在关于绿色深度学习的调查")'
- en: '[2.1 Component Design](#Ch2.S1 "In Chapter 2 Compact Architecture ‣ A Survey
    on Green Deep Learning")'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1 组件设计](#Ch2.S1 "在第2章紧凑架构 ‣ 关于绿色深度学习的调查")'
- en: '[2.1.1 Compact Convolution](#Ch2.S1.SS1 "In 2.1 Component Design ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1.1 紧凑卷积](#Ch2.S1.SS1 "在2.1组件设计 ‣ 第2章紧凑架构 ‣ 关于绿色深度学习的调查")'
- en: '[2.1.2 Efficient Attention](#Ch2.S1.SS2 "In 2.1 Component Design ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1.2 高效注意力](#Ch2.S1.SS2 "在2.1 组件设计 ‣ 第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[2.1.3 Lightweight Softmax](#Ch2.S1.SS3 "In 2.1 Component Design ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1.3 轻量级Softmax](#Ch2.S1.SS3 "在2.1 组件设计 ‣ 第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[2.1.4 Compact Embeddings](#Ch2.S1.SS4 "In 2.1 Component Design ‣ Chapter 2
    Compact Architecture ‣ A Survey on Green Deep Learning")'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1.4 紧凑嵌入](#Ch2.S1.SS4 "在2.1 组件设计 ‣ 第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[2.2 Component Assembling](#Ch2.S2 "In Chapter 2 Compact Architecture ‣ A Survey
    on Green Deep Learning")'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2 组件组装](#Ch2.S2 "在第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[2.2.1 Memory Sharing](#Ch2.S2.SS1 "In 2.2 Component Assembling ‣ Chapter 2
    Compact Architecture ‣ A Survey on Green Deep Learning")'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2.1 内存共享](#Ch2.S2.SS1 "在2.2 组件组装 ‣ 第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[2.2.2 Static Weight Sharing](#Ch2.S2.SS2 "In 2.2 Component Assembling ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2.2 静态权重共享](#Ch2.S2.SS2 "在2.2 组件组装 ‣ 第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[2.2.3 Dynamic Weight Sharing](#Ch2.S2.SS3 "In 2.2 Component Assembling ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2.3 动态权重共享](#Ch2.S2.SS3 "在2.2 组件组装 ‣ 第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[2.2.4 Deployment Sharing](#Ch2.S2.SS4 "In 2.2 Component Assembling ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2.4 部署共享](#Ch2.S2.SS4 "在2.2 组件组装 ‣ 第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[2.3 Compact-architecture Search](#Ch2.S3 "In Chapter 2 Compact Architecture
    ‣ A Survey on Green Deep Learning")'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.3 紧凑架构搜索](#Ch2.S3 "在第2章 紧凑架构 ‣ 绿色深度学习调查")'
- en: '[3 Energy-Efficient Training](#Ch3 "In A Survey on Green Deep Learning")'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 能效训练](#Ch3 "在绿色深度学习调查")'
- en: '[3.1 Initialization](#Ch3.S1 "In Chapter 3 Energy-Efficient Training ‣ A Survey
    on Green Deep Learning")'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 初始化](#Ch3.S1 "在第3章 能效训练 ‣ 绿色深度学习调查")'
- en: '[3.1.1 Random Initialization](#Ch3.S1.SS1 "In 3.1 Initialization ‣ Chapter
    3 Energy-Efficient Training ‣ A Survey on Green Deep Learning")'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.1 随机初始化](#Ch3.S1.SS1 "在3.1 初始化 ‣ 第3章 能效训练 ‣ 绿色深度学习调查")'
- en: '[3.1.2 Pre-trained Models for Initialization](#Ch3.S1.SS2 "In 3.1 Initialization
    ‣ Chapter 3 Energy-Efficient Training ‣ A Survey on Green Deep Learning")'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.2 用于初始化的预训练模型](#Ch3.S1.SS2 "在3.1 初始化 ‣ 第3章 能效训练 ‣ 绿色深度学习调查")'
- en: '[3.2 Normalization](#Ch3.S2 "In Chapter 3 Energy-Efficient Training ‣ A Survey
    on Green Deep Learning")'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 归一化](#Ch3.S2 "在第3章 能效训练 ‣ 绿色深度学习调查")'
- en: '[3.3 Progressive Training](#Ch3.S3 "In Chapter 3 Energy-Efficient Training
    ‣ A Survey on Green Deep Learning")'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 渐进训练](#Ch3.S3 "在第3章 能效训练 ‣ 绿色深度学习调查")'
- en: '[3.4 Efficient Hyper-parameter Optimization](#Ch3.S4 "In Chapter 3 Energy-Efficient
    Training ‣ A Survey on Green Deep Learning")'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.4 高效超参数优化](#Ch3.S4 "在第3章 能效训练 ‣ 绿色深度学习调查")'
- en: '[4 Energy-Efficient Inference](#Ch4 "In A Survey on Green Deep Learning")'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 能效推理](#Ch4 "在绿色深度学习调查")'
- en: '[4.1 Model Pruning](#Ch4.S1 "In Chapter 4 Energy-Efficient Inference ‣ A Survey
    on Green Deep Learning")'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 模型剪枝](#Ch4.S1 "在第4章 能效推理 ‣ 绿色深度学习调查")'
- en: '[4.2 Low-rank Factorization](#Ch4.S2 "In Chapter 4 Energy-Efficient Inference
    ‣ A Survey on Green Deep Learning")'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 低秩分解](#Ch4.S2 "在第4章 能效推理 ‣ 绿色深度学习调查")'
- en: '[4.3 Quantization](#Ch4.S3 "In Chapter 4 Energy-Efficient Inference ‣ A Survey
    on Green Deep Learning")'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 量化](#Ch4.S3 "在第4章 能效推理 ‣ 绿色深度学习调查")'
- en: '[4.4 Knowledge Distillation](#Ch4.S4 "In Chapter 4 Energy-Efficient Inference
    ‣ A Survey on Green Deep Learning")'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4 知识蒸馏](#Ch4.S4 "在第4章 能效推理 ‣ 绿色深度学习调查")'
- en: '[5 Efficient Data Usage](#Ch5 "In A Survey on Green Deep Learning")'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 高效数据使用](#Ch5 "在绿色深度学习调查")'
- en: '[5.1 Active Learning](#Ch5.S1 "In Chapter 5 Efficient Data Usage ‣ A Survey
    on Green Deep Learning")'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 主动学习](#Ch5.S1 "在第5章 高效数据使用 ‣ 绿色深度学习调查")'
- en: '[5.2 Pre-training as Few-shot Learners](#Ch5.S2 "In Chapter 5 Efficient Data
    Usage ‣ A Survey on Green Deep Learning")'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 预训练作为少量学习者](#Ch5.S2 "在第5章 高效数据使用 ‣ 绿色深度学习调查")'
- en: '[5.2.1 Self-supervised Learning](#Ch5.S2.SS1 "In 5.2 Pre-training as Few-shot
    Learners ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep Learning")'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.1 自监督学习](#Ch5.S2.SS1 "在5.2 预训练作为少量学习者 ‣ 第5章 高效数据使用 ‣ 绿色深度学习调查")'
- en: '[5.2.2 Contrastive Learning](#Ch5.S2.SS2 "In 5.2 Pre-training as Few-shot Learners
    ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep Learning")'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.2 对比学习](#Ch5.S2.SS2 "在 5.2 预训练作为少样本学习者 ‣ 第 5 章 高效数据使用 ‣ 绿色深度学习调查")'
- en: '[5.2.3 Prompt Learning](#Ch5.S2.SS3 "In 5.2 Pre-training as Few-shot Learners
    ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep Learning")'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2.3 提示学习](#Ch5.S2.SS3 "在 5.2 预训练作为少样本学习者 ‣ 第 5 章 高效数据使用 ‣ 绿色深度学习调查")'
- en: '[6 Conclusions and Future Directions](#Ch6 "In A Survey on Green Deep Learning")'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 结论和未来方向](#Ch6 "在 绿色深度学习调查")'
- en: Chapter 1 Introduction
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 章 引言
- en: Deep learning, based on deep neural networks, is part of machine learning methods.
    In this chapter, we first introduce the development of deep learning in section [1.1](#Ch1.S1
    "1.1 Deep Learning ‣ Chapter 1 Introduction ‣ A Survey on Green Deep Learning").
    Then, we elucidate what is Green deep learning, why Green deep learning matters,
    and how to evaluate the “greenness” of deep learning in section [1.2](#Ch1.S2
    "1.2 Green Deep Learning ‣ Chapter 1 Introduction ‣ A Survey on Green Deep Learning").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度神经网络的深度学习是机器学习方法的一部分。在本章中，我们首先在第 [1.1](#Ch1.S1 "1.1 深度学习 ‣ 第 1 章 引言 ‣ 绿色深度学习调查")
    节介绍深度学习的发展。然后，我们在第 [1.2](#Ch1.S2 "1.2 绿色深度学习 ‣ 第 1 章 引言 ‣ 绿色深度学习调查") 节阐述什么是绿色深度学习，绿色深度学习的重要性以及如何评估深度学习的“绿色”。
- en: 1.1 Deep Learning
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 深度学习
- en: While a decade ago, artificial intelligence (AI) mainly focuses on shallow models,
    like structure perceptrons (McDonald et al., [2010](#bib.bib217); Huang et al.,
    [2012](#bib.bib135); Li & Ji, [2014](#bib.bib184)) and conditional random fields (Ghosh
    et al., [2011](#bib.bib93); Sutton & McCallum, [2012](#bib.bib312); Zheng et al.,
    [2015](#bib.bib397)). These shallow models only require limited computations.
    Most AI approaches can be deployed on CPUs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 十年前，人工智能（AI）主要集中在浅层模型上，如结构感知机（McDonald 等，[2010](#bib.bib217)；Huang 等，[2012](#bib.bib135)；Li
    & Ji，[2014](#bib.bib184)）和条件随机场（Ghosh 等，[2011](#bib.bib93)；Sutton & McCallum，[2012](#bib.bib312)；Zheng
    等，[2015](#bib.bib397)）。这些浅层模型只需要有限的计算。大多数 AI 方法可以在 CPU 上部署。
- en: In recent years, powerful GPUs have become increasingly accessible, making it
    possible to deploy larger models, which accelerates the development of deep learning.
    The ideas of widely-used deep learning models have been proposed in the 1990s,
    such as convolutional neural networks (CNNs) (LeCun et al., [1998](#bib.bib173))
    and long-short term networks (LSTMs) (Hochreiter & Schmidhuber, [1997](#bib.bib129)).
    Confined by hardware capacity and large-scale data resources, these models began
    to be popular until the past few years. Collobert et al. ([2011](#bib.bib55))
    proposed the first systematic deep learning framework for NLP tasks. Krizhevsky
    et al. ([2012](#bib.bib164)) proposed a convolution-based deep network, which
    ranked the first in the image classification challenge. These studies are good
    pioneers that motivate AI participants to dive into deep learning.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，强大的 GPU 变得越来越易于获取，使得部署更大规模的模型成为可能，从而加速了深度学习的发展。广泛使用的深度学习模型的概念早在 1990 年代就已提出，例如卷积神经网络（CNNs）（LeCun
    等，[1998](#bib.bib173)）和长短期记忆网络（LSTMs）（Hochreiter & Schmidhuber，[1997](#bib.bib129)）。由于硬件能力和大规模数据资源的限制，这些模型直到最近几年才开始流行。Collobert
    等人（[2011](#bib.bib55)）提出了第一个系统的深度学习框架用于 NLP 任务。Krizhevsky 等人（[2012](#bib.bib164)）提出了基于卷积的深度网络，在图像分类挑战中排名第一。这些研究是激励
    AI 参与者深入深度学习的优秀先驱。
- en: '![Refer to caption](img/089b10984a83f7e8900a82f6cf67d136.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/089b10984a83f7e8900a82f6cf67d136.png)'
- en: 'Figure 1.1: The ratio of papers using neural networks in ACL, a top-tier NLP
    conference, from 2010 to 2020\. We manually count how many papers use neural networks
    among sampled 50 papers.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：2010 到 2020 年间 ACL 顶级 NLP 会议中使用神经网络的论文比例。我们手动统计了在抽样的 50 篇论文中使用神经网络的数量。
- en: Deep learning methods currently have become a prime choice for AI. The promising
    prospect of neural networks attracts more AI participants to engage with deep
    learning, in the meanwhile, deep learning is highly competitive in yielding profits
    when applied to real-world applications. The industry continuously develops more
    efficient hardware and launches better programming platforms, such as Theano,
    Caffe, MxNet, Tensorflow, and Pytorch. Advanced infrastructures further enable
    AI participants to develop stronger deep models. Therefore, deep learning takes
    a high-speed train since 2010\. To visualize the transition process from shallow
    models to deep networks in the AI community, we analyze papers from a top-tier
    AI conference, ACL, starting from 2010 to 2020\. We randomly select 50 works from
    each year and manually count the number of papers using deep learning. As we can
    see from Figure [1.1](#Ch1.F1 "Figure 1.1 ‣ 1.1 Deep Learning ‣ Chapter 1 Introduction
    ‣ A Survey on Green Deep Learning"), the number of papers using neural networks
    is growing fast from 2012 to 2020\. All papers adopt deep networks as backbone
    since 2020\. From then, the AI field is fully entered into the age of deep learning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法目前已成为AI的首选。神经网络的良好前景吸引了更多AI参与者投入深度学习，同时，深度学习在实际应用中具有很高的竞争力。行业不断开发更高效的硬件，并推出更好的编程平台，如Theano、Caffe、MxNet、Tensorflow和Pytorch。先进的基础设施进一步使AI参与者能够开发更强大的深度模型。因此，自2010年起，深度学习如同乘坐高铁。为了可视化AI社区从浅层模型到深层网络的过渡过程，我们分析了从2010年到2020年间的顶级AI会议ACL的论文。我们随机选择了每年的50篇论文，并手动统计使用深度学习的论文数量。从图[1.1](#Ch1.F1
    "Figure 1.1 ‣ 1.1 Deep Learning ‣ Chapter 1 Introduction ‣ A Survey on Green Deep
    Learning")可以看到，从2012年到2020年，使用神经网络的论文数量增长迅速。从2020年起，所有论文都将深度网络作为基础。从那时起，AI领域完全进入了深度学习时代。
- en: In the age of deep learning, a hot direction is to obtain SOTA results. Following Schwartz
    et al. ([2020a](#bib.bib284)), we call such research trend as Red AI. Recently,
    researchers have noticed that it was harder to gain an advantage over SOTA results.
    For traditional AI fields, like CV and NLP, the improvements achieved by new AI
    models/algorithms are diminishing. Many popular research benchmarks are reaching
    their performance ceiling. Figure [1.2](#Ch1.F2 "Figure 1.2 ‣ 1.1 Deep Learning
    ‣ Chapter 1 Introduction ‣ A Survey on Green Deep Learning") and [1.3](#Ch1.F3
    "Figure 1.3 ‣ 1.1 Deep Learning ‣ Chapter 1 Introduction ‣ A Survey on Green Deep
    Learning") list several examples showing how the returns of deep learning are
    diminishing over time.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习时代，一个热门方向是获得SOTA（State Of The Art）结果。参考Schwartz等人（[2020a](#bib.bib284)），我们将这种研究趋势称为红色AI。最近，研究人员注意到相较于SOTA结果，取得优势变得更加困难。对于传统AI领域，如计算机视觉（CV）和自然语言处理（NLP），新AI模型/算法带来的改进在减少。许多流行的研究基准正在达到其性能极限。图[1.2](#Ch1.F2
    "Figure 1.2 ‣ 1.1 Deep Learning ‣ Chapter 1 Introduction ‣ A Survey on Green Deep
    Learning")和[1.3](#Ch1.F3 "Figure 1.3 ‣ 1.1 Deep Learning ‣ Chapter 1 Introduction
    ‣ A Survey on Green Deep Learning")列出了几个例子，展示了深度学习的回报随着时间的推移在减少。
- en: 'The trend of red AI requires massive computations to achieve better results.
    For example, as reported in Schwartz et al. ([2020a](#bib.bib284)), the amount
    of computations used to train deep learning models has increased 300,000x in 6
    years. These computations not only cause expensive financial costs but also contribute
    to an excessive carbon footprint. The former harms AI inclusiveness and the latter
    harms our environment. We classify the computation source required by deep learning
    into the following three categories: model size, parameter tuning, and training
    data.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 红色AI的趋势需要大量计算以获得更好的结果。例如，如Schwartz等人（[2020a](#bib.bib284)）报告的，训练深度学习模型所使用的计算量在6年内增加了300,000倍。这些计算不仅带来昂贵的经济成本，还造成过度的碳足迹。前者损害了AI的包容性，后者则对环境造成了危害。我们将深度学习所需的计算源分类为以下三类：模型大小、参数调整和训练数据。
- en: '![Refer to caption](img/473283eafe2ebd8dd8edc4f0e7c77266.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/473283eafe2ebd8dd8edc4f0e7c77266.png)'
- en: 'Figure 1.2: Results of WMT English-German translation from 2016 to 2020\. As
    we can see, the recent published results are reaching into the ceiling. The data
    is collected from [https://paperswithcode.com](https://paperswithcode.com).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：2016年至2020年WMT英语-德语翻译的结果。如图所示，最近公布的结果已经达到极限。数据收集自[https://paperswithcode.com](https://paperswithcode.com)。
- en: '![Refer to caption](img/0bc4436cd4d87966d464ae785d22852e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0bc4436cd4d87966d464ae785d22852e.png)'
- en: 'Figure 1.3: The image classification results on ImageNet from 2011 to 2021\.
    As we can see, the recent published results are reaching into the ceiling. The
    data is collected from [https://paperswithcode.com](https://paperswithcode.com).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3：2011 到 2021 年在 ImageNet 上的图像分类结果。正如我们所见，最近发布的结果已经达到了瓶颈。数据来自 [https://paperswithcode.com](https://paperswithcode.com)。
- en: 1) With access to large-scale data resources, increasing model size is the simplest
    way to improve results. For example, on WMT English-German translation, the performance
    can be increased from 27.3 to 28.4 while the size of the machine translation models
    is increased from 60M (Transformer-base) to 180M (Transformer-large). To achieve
    better results, more and more AI participants would like to increase model size
    as much as possible, especially for rich organizations. For example, researchers
    from OpenAI first pre-trained a large-scale text generation model, called GPT-3
    with 175B parameters. It shows that a super-large model can generate human-like
    texts. However, according to Strubell et al. ([2019](#bib.bib307)), training GPT-3
    can emit almost 500M carbons, almost emissions of five cars in their lifetime.
    These studies are key milestones in a long run. However, we believe that larger
    models are not always better if we consider “invisible” computation cost. We are
    still concerned about “the crazy love” to super-large models no matter whether
    the increased computations bring significant benefits. In addition, bigger models
    largely increase the burden of inference serving. Amazon estimates that 90% of
    production ML infrastructure costs are for inference, not training (Jain et al.,
    [2019](#bib.bib143)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 利用大规模数据资源，增加模型大小是提高结果的最简单方法。例如，在 WMT 英德翻译中，当机器翻译模型的大小从 60M（Transformer-base）增加到
    180M（Transformer-large）时，性能可以从 27.3 提升到 28.4。为了获得更好的结果，越来越多的 AI 参与者愿意尽可能增加模型大小，尤其是对于资源丰富的组织。例如，OpenAI
    的研究人员首先预训练了一个大规模文本生成模型，称为 GPT-3，拥有 175B 参数。这表明一个超大模型可以生成类似人类的文本。然而，根据 Strubell
    等人（[2019](#bib.bib307)）的研究，训练 GPT-3 可能会排放近 500M 碳，相当于五辆汽车的生命周期排放。这些研究是长远中的关键里程碑。然而，我们相信如果考虑“看不见”的计算成本，更大的模型并不总是更好的。我们仍然对对超大模型的“疯狂热爱”感到担忧，无论增加的计算是否带来显著的好处。此外，更大的模型大幅增加了推理服务的负担。亚马逊估计
    90% 的生产 ML 基础设施成本用于推理，而不是训练（Jain 等人，[2019](#bib.bib143)）。
- en: 2) Model experiments are also an overlooked computation consumer. To verify
    the effectiveness of a new model/algorithm, AI participants usually conduct massive
    experiments, including model/algorithm implementation, baseline re-implementation,
    and hyper-parameter tuning. First, baseline re-implementation is an redundant
    computation source. For example, the original Transformer paper has 18K citations.
    Assume each citation represents a single implementation. Each re-implementation
    takes 100 hours on a single GPU (following the cost of running a Transformer-base
    model on English-German translation). It means that only baseline re-implementation
    on a single dataset can take 1.8M GPU hours. In addition, hyper-parameter tuning
    is an overlooked computation source. We design a simple questionnaire to ask the
    ratio of experiments experiments for hyper-parameter tuning while developing a
    new model/algorithm, answered by 64 AI specialists, including researchers and
    engineers. All of the surveyed choose tuning hyper-parameters. To be specific,
    10.7%, 32.1%, 35.7%, 21.4% individuals take 80%-100%, 50%-80%, 30%-50%, 0%-30%
    experiments to tune hyper-parameters. In sum, 42.8% individuals spend over 50%
    experiments for hyper-parameter tuning.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 模型实验也是一个被忽视的计算消耗者。为了验证新模型/算法的有效性，AI 参与者通常会进行大量实验，包括模型/算法实现、基准再实现和超参数调优。首先，基准再实现是一个冗余计算源。例如，原始的
    Transformer 论文有 18K 引用。假设每个引用代表一个单独的实现。每次再实现需要 100 小时在单个 GPU 上（遵循在英德翻译中运行 Transformer-base
    模型的成本）。这意味着仅在单个数据集上的基准再实现就可能需要 1.8M GPU 小时。此外，超参数调优是一个被忽视的计算源。我们设计了一个简单的问卷，询问在开发新模型/算法时，超参数调优的实验比例，由
    64 位 AI 专家，包括研究人员和工程师回答。所有受访者都选择了调优超参数。具体来说，10.7%、32.1%、35.7%、21.4% 的个人在调优超参数时分别花费
    80%-100%、50%-80%、30%-50%、0%-30% 的实验。总的来说，42.8% 的个人花费超过 50% 的实验时间用于超参数调优。
- en: 3) Starting from shallow models, it is popular to increase the amount of training
    data to achieve better generalization ability, especially in semi-supervised settings.
    One recent hot topic is pre-training a super-large model on billions of raw data.
    In the NLP field, ELMo (Peters et al., [2018](#bib.bib241)) is the first well-known
    work to explore large-scale pre-training. Following ELMo, BERT pre-trains a Transformer
    encoder on 3 billion word pieces. Researchers from OpenAI recently proposed GPT-3,
    a generative model pre-trained on 45TB data. These massive training examples largely
    increase the training costs compared to previous shallow models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 从浅层模型开始，增加训练数据量以实现更好的泛化能力是非常流行的，特别是在半监督环境中。一个近期的热门话题是对数十亿原始数据进行超大规模预训练。在
    NLP 领域，ELMo (Peters 等人，[2018](#bib.bib241)) 是第一个探索大规模预训练的知名工作。继 ELMo 之后，BERT 在
    30 亿词片上预训练了一个 Transformer 编码器。OpenAI 的研究人员最近提出了 GPT-3，一个在 45TB 数据上预训练的生成模型。这些大量的训练样本大大增加了训练成本，相比于之前的浅层模型。
- en: In all, the trend of Red AI brings heavy computation costs. These computations
    exacerbate the research inequality, making it difficult to involve all researchers
    in Red AI. Furthermore, massive computation requirements bring huge carbon emissions.
    To address these problems, Green deep learning, or Green AI, was first proposed
    by Schwartz et al. ([2020a](#bib.bib284)) to encourage the AI community to focus
    more on energy costs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Red AI 的趋势带来了巨大的计算成本。这些计算加剧了研究不平等，使得所有研究人员难以参与 Red AI。此外，大量的计算需求带来了巨大的碳排放。为了解决这些问题，Schwartz
    等人 ([2020a](#bib.bib284)) 首次提出了 Green 深度学习或 Green AI，旨在鼓励 AI 社区更多地关注能源成本。
- en: 1.2 Green Deep Learning
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 Green 深度学习
- en: In this section, we mainly describe what is Green deep learning, how to evaluate
    “greenness” in deep learning, and why Green deep learning matters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们主要描述什么是 Green 深度学习，如何评估深度学习中的“绿色性”，以及为什么 Green 深度学习很重要。
- en: 1.2.1 Definition
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.1 定义
- en: Green learning, a term first proposed by Schwartz et al. ([2020a](#bib.bib284)),
    is gaining mounting attention. Formally, Green deep learning, or Green AI, appeals
    to researchers to obtain novel results without increasing computational cost rather,
    ideally reducing it. Unlike Red AI pushing state-of-the-art results at any cost,
    Green deep learning encourages AI participants to achieve comparable or better
    results using as few computations as possible.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Green 学习，这一术语首次由 Schwartz 等人 ([2020a](#bib.bib284)) 提出，正受到越来越多的关注。正式来说，Green
    深度学习或 Green AI，呼吁研究人员在不增加计算成本的情况下获得新的成果，理想情况下还能降低计算成本。与 Red AI 以任何代价推动最先进的结果不同，Green
    深度学习鼓励 AI 参与者使用尽可能少的计算来实现相当或更好的结果。
- en: 1.2.2 Measure
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.2 度量
- en: In Green deep learning, computations are important evaluation metrics. Currently,
    the whole community lacks a comprehensive and widely-accepted measure to evaluate
    computations because multiple aspects can attribute to computations, including
    model size, training examples, and so on. A comprehensive measure is expected
    for a fair comparison. Here we list several computation measures and discuss their
    merits and demerits.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Green 深度学习中，计算是重要的评估指标。目前，整个社区缺乏一个全面且广泛接受的计算评估标准，因为多个方面可以影响计算，包括模型大小、训练样本等。需要一个全面的度量标准来实现公平比较。在这里，我们列出了几个计算度量标准，并讨论它们的优点和缺点。
- en: Running time
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 运行时间
- en: Some studies adopt the total training time as a kind of computations measure.
    If all models/algorithms adopt the same hardware and software settings, it is
    the most natural measure to evaluate training/inference computations. However,
    since running time heavily relies on infrastructure settings, it is not suitable
    for comparing models running on different infrastructures. Even so, we still encourage
    AI participants to report the running time for an intuitive understanding.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究采用总训练时间作为计算度量的一种。如果所有模型/算法使用相同的硬件和软件设置，这是一种最自然的评估训练/推理计算的方式。然而，由于运行时间严重依赖基础设施设置，它不适合用于比较在不同基础设施上运行的模型。即便如此，我们仍然鼓励
    AI 参与者报告运行时间，以便直观理解。
- en: Carbon emission
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 碳排放
- en: Carbon emission is the most direct approach to evaluate environmental effects.
    In order to quantify carbon emissions, Lacoste et al. ([2019](#bib.bib167)) used
    $CO_{2}$-equivalents ($CO_{2}eq$) as the amount of $CO_{2}$ which would have the
    equivalent global warming impact. However, the main challenge of this measure
    lies in accurate estimation. First, computations via electricity consumption are
    easily influenced by local infrastructures. Furthermore, it is hard for AI participants
    to estimate the amount of $CO_{2}$ if they do not run experiments on well-known
    cloud platforms. Therefore, it is also not suitable as a standard metric to compare
    different models running on different regions and different computing infrastructures.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 碳排放是评估环境影响的最直接方法。为了量化碳排放，Lacoste 等人（[2019](#bib.bib167)）使用 $CO_{2}$-当量 ($CO_{2}eq$)
    作为与 $CO_{2}$ 具有相同全球变暖影响的量。然而，这一指标的主要挑战在于准确估算。首先，通过电力消耗计算的碳排放容易受到当地基础设施的影响。此外，如果
    AI 参与者没有在知名云平台上进行实验，很难估算 $CO_{2}$ 的量。因此，它也不适合作为比较不同地区和不同计算基础设施上运行的不同模型的标准指标。
- en: Model size
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 模型大小
- en: The model size is also an important factor in deciding training and inference
    costs. We encourage researchers to report model size to corporate with other measures
    in practice.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小也是决定训练和推理成本的重要因素。我们鼓励研究人员报告模型大小，以便与实际操作中的其他指标结合使用。
- en: FLOPs
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: FLOPs
- en: Floating-Point Operations (FLOPs) count the number of works required for running
    a model when executing a specific instance. Previous studies usually adopt this
    metric to evaluate efficiency. FLOPs are almost independent of hardware and software
    platforms, being the simplest measure to conduct a fair comparison between different
    models. However, FLOPs are theoretical values, and there is a gap between FLOPs
    and running time. In addition to the total amount of works (FLOPs), the degree
    of parallelism also affects the running time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点运算（FLOPs）计算在执行特定实例时运行模型所需的工作量。以往的研究通常采用这一指标来评估效率。FLOPs 几乎不依赖于硬件和软件平台，是进行不同模型公平比较的最简单度量。然而，FLOPs
    是理论值，FLOPs 和运行时间之间存在差距。除了总工作量（FLOPs）外，平行度也会影响运行时间。
- en: According to these measures, we summarize evaluation strategies towards fair
    comparison and intuitive understanding.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些指标，我们总结了针对公平比较和直观理解的评估策略。
- en: Fair measure
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 公平度量
- en: Generally speaking, AI participants prefer well-performing models/algorithms
    with fewer computations. Therefore, it is an important question to fairly compare
    computations required for training and inference. We strongly suggest reporting
    FLOPs during model training and inference. Last but not least, to evaluate the
    wasted and redundant computations required for developing a new model/algorithm,
    we also encourage researchers to report the total FLOPs during all experiments,
    including but not limited to parameter tuning and baseline implementation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，AI 参与者更倾向于计算量较少且性能良好的模型/算法。因此，公平比较训练和推理所需的计算量是一个重要问题。我们强烈建议在模型训练和推理过程中报告
    FLOPs。最后但同样重要的是，为了评估开发新模型/算法所需的浪费和冗余计算，我们还鼓励研究人员报告所有实验中的总 FLOPs，包括但不限于参数调整和基线实现。
- en: Intuitive understanding
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 直观理解
- en: To increase the intuitively understanding about computations, we encourage researchers
    to report running time, model size, and carbon emission as optional results.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加对计算的直观理解，我们鼓励研究人员报告运行时间、模型大小和碳排放作为可选结果。
- en: 1.2.3 Broader Impact
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.3 更广泛的影响
- en: First, Green deep learning can help deep learning empower real society applications
    better. In the past decade, deep learning continuously pushed state-of-the-art
    results on research benchmarks, like machine translation, image classification,
    and so on. In the research community, the cost of deep learning seems to be nothing
    compared to energy consumption of all human activities. Nowadays, deep learning
    is widely applied to real society tasks, such as auto-driving, face recognition,
    drug discovery, and so on. Once deep learning is involved in large-scale applications,
    the cost of deep learning will be multiplied hundreds of millions of times. Furthermore,
    some edge applications, like mobiles with extremely few computation resources,
    also require Green deep learning. Therefore, Green deep learning is a necessary
    research direction in the future.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，绿色深度学习可以更好地帮助深度学习赋能真实社会应用。在过去十年中，深度学习在研究基准测试中不断推动最先进的结果，如机器翻译、图像分类等。在研究界，与人类所有活动的能源消耗相比，深度学习的成本似乎微不足道。如今，深度学习广泛应用于实际社会任务，如自动驾驶、人脸识别、药物发现等。一旦深度学习涉及大规模应用，其成本将成百上千万倍地增加。此外，一些边缘应用，如计算资源极少的移动设备，也需要绿色深度学习。因此，绿色深度学习是未来的必要研究方向。
- en: Second, Green deep learning can largely improve AI inclusiveness. We note the
    contributions of rich organizations for pushing higher results on many downstream
    tasks. Meanwhile, we also notice the dilemma of researchers from academics and
    developing countries on engaging Red AI research. Most researchers only have limited
    computations, which could not support them to develop super-large models with
    state-of-the-art results. Unfortunately, compared to ideas with state-of-the-art
    results, novel and innovative ideas without state-of-the-art results are losing
    their sounds. For example, news media would like to report studies with state-of-the-art
    results. The attractive propaganda of No.1 also pushes the rich organizations
    to pour more money on super-big models. These cases may confuse researchers on
    how they engage in deep learning research without strong financial support. We
    argue that state-of-the-art results are good, but not the only criteria to evaluate
    the quality of new models/algorithms. We encourage rich organizations to continuously
    explore data and model boundaries, also encourage the AI community to pay attention
    to innovative ideas. In fact, deep learning struggles many years until it outperforms
    shallow models with feature engineers. We believe that the development of AI should
    be diverse. Green deep learning can improve AI inclusion and motivate more AI
    participants to explore deep learning possibilities.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，绿色深度学习可以大幅提高人工智能的包容性。我们注意到许多富有组织在推动许多下游任务上取得更高结果的贡献。同时，我们也注意到学术界和发展中国家的研究人员在参与红色人工智能研究方面的困境。大多数研究人员的计算资源有限，无法支持他们开发具有最先进结果的超大模型。不幸的是，与具有最先进结果的想法相比，缺乏最先进结果的创新想法越来越被忽视。例如，新闻媒体更愿意报道具有最先进结果的研究。No.1
    的吸引性宣传也促使富有组织在超大模型上投入更多资金。这些情况可能会让研究人员困惑，如何在没有强大经济支持的情况下进行深度学习研究。我们认为，最先进的结果固然重要，但并不是评估新模型/算法质量的唯一标准。我们鼓励富有组织不断探索数据和模型的边界，同时也鼓励人工智能社区关注创新想法。事实上，深度学习经过多年的努力，才超越了使用特征工程的浅层模型。我们相信，人工智能的发展应该是多样化的。绿色深度学习可以提高人工智能的包容性，并激励更多人工智能参与者探索深度学习的可能性。
- en: 1.3 Outline of the Survey
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 调查概述
- en: It is a long-term goal to develop tiny yet strong networks for all AI researchers
    and engineers. Driven by this target, several popular tiny networks have been
    proposed (Howard et al., [2017](#bib.bib132); Chollet, [2017](#bib.bib50); Tan
    & Le, [2019](#bib.bib319)). For example, MobileNet proposed by Howard et al. ([2017](#bib.bib132))
    is an efficient architecture based on depthwise separable convolution. Similar
    idea has been adopted at Xception (Chollet, [2017](#bib.bib50)). Recently, to
    explore extremely tiny networks, advanced training/inference/network surgery methods
    have been proposed. For example, EdgeBERT (Tambe et al., [2021](#bib.bib318))
    is proposed to build an extremely tiny network that can run on IoT devices. It
    adopts advanced methods like quantization, pruning, early exit to further reduce
    model parameters and running computations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 开发小巧而强大的网络是所有AI研究人员和工程师的长期目标。为了实现这一目标，已经提出了几种流行的小型网络（Howard等，[2017](#bib.bib132)；Chollet，[2017](#bib.bib50)；Tan
    & Le，[2019](#bib.bib319)）。例如，Howard等人提出的MobileNet（[2017](#bib.bib132)）是一种基于深度可分离卷积的高效架构。类似的思想已被Xception（Chollet，[2017](#bib.bib50)）采纳。最近，为了探索极其微小的网络，提出了先进的训练/推理/网络手术方法。例如，EdgeBERT（Tambe等，[2021](#bib.bib318)）被提出用于构建一个可以在物联网设备上运行的极小网络。它采用了量化、剪枝、早期退出等先进方法来进一步减少模型参数和运行计算。
- en: In this survey, we give a systematic review of Green deep learning technologies.
    We first build a green technology taxonomy and then classify the related technologies
    into four categories, including compact networks, energy-saving training strategies,
    energy-saving inference, and efficient data usage. In each category, we review
    the current progress on Green technologies and explore potential issues.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们对绿色深度学习技术进行了系统回顾。我们首先建立了绿色技术分类法，然后将相关技术分类为四个类别，包括紧凑型网络、节能训练策略、节能推理和高效数据使用。在每个类别中，我们回顾了绿色技术的当前进展，并探讨了潜在问题。
- en: 'It is important to note that building a Green technology taxonomy is challenging
    since there lacks a unified standard measurement. For example, BERT requires massive
    computations during training. If we only consider training costs, BERT can not
    be treated as a Green technology. However, BERT can improve downstream performance
    with fewer training examples. If we consider its transfer ability, BERT is absolutely
    a Green technology. Therefore, whether a technology is defined as Green or not
    is open to doubt. We will try our best to avoid giving a biased definition. If
    a technology has the potential to reduce the costs of deep learning, we will include
    it in the green technology taxonomy. We review Green deep learning technologies
    in the following categories:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，构建绿色技术分类法具有挑战性，因为缺乏统一的标准衡量。例如，BERT在训练过程中需要大量计算。如果仅考虑训练成本，BERT不能被视为绿色技术。然而，BERT可以在较少的训练示例下提高下游性能。如果考虑到其迁移能力，BERT绝对是一项绿色技术。因此，是否将某项技术定义为绿色技术仍存在疑问。我们将尽力避免给出偏颇的定义。如果一项技术有潜力降低深度学习的成本，我们将把它纳入绿色技术分类法。我们在以下类别中回顾绿色深度学习技术：
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compact Architecture Design. This part focuses on small networks. We split this
    chapter into two sub-chapters, i.e., component design, and component assembling.
    The component design focuses on subtle components with competitive results but
    much fewer computations. Component assembling describes how to build a network
    efficiently.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 紧凑架构设计。本部分专注于小型网络。我们将本章分为两个子章节，即组件设计和组件组装。组件设计专注于细微的组件，具有竞争力的结果但计算量更少。组件组装描述了如何高效地构建网络。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Energy-efficient Training Strategies. Previous studies have proposed several
    efficient training approaches. In this survey, we classify these studies into
    four categories, including initialization, normalization, progressive training,
    and efficient AutoML.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 节能训练策略。先前的研究提出了几种高效的训练方法。在这项调查中，我们将这些研究分为四类，包括初始化、归一化、渐进训练和高效AutoML。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Energy-efficient Inference. In this chapter, we describe approaches that aim
    to get a smaller yet comparable network from a larger network for efficient inference,
    including model pruning, low-rank factorization, quantization, distillation.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 节能推理。在本章中，我们描述了旨在从较大的网络中获得一个较小但可比较的网络以实现高效推理的方法，包括模型剪枝、低秩分解、量化和蒸馏。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Efficient Data Usage. This chapter lists algorithms that leverage training
    data efficiently. We focus on two popular directions: active learning and pre-trained
    models as few-shot learners.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效数据使用。本章列出了有效利用训练数据的算法。我们关注两个热门方向：主动学习和预训练模型作为少样本学习者。
- en: Chapter 2 Compact Architecture
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章 紧凑架构
- en: Developing efficient neural networks has been a long-standing goal towards Green
    AI. In this survey, we define Green networks as neural networks that are efficient
    in terms of computational costs. We can generate compact networks via subtle design,
    model surgery, and network search. Subtle design means that we can manually define
    efficient architectures requiring fewer computations. Model surgery means that
    we can generate compact architectures from a larger model via parameter reduction.
    In this chapter, we focus on architectures with subtle design and leave the details
    of network surgery to Chapter 4\. An overview of this section is shown in Figure [2.1](#Ch2.F1
    "Figure 2.1 ‣ Chapter 2 Compact Architecture ‣ A Survey on Green Deep Learning").
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 开发高效的神经网络一直是迈向绿色AI的长期目标。在这项调查中，我们将绿色网络定义为在计算成本方面高效的神经网络。我们可以通过微妙设计、模型修剪和网络搜索来生成紧凑的网络。微妙设计意味着我们可以手动定义计算量更少的高效架构。模型修剪意味着我们可以通过参数减少从更大的模型中生成紧凑架构。在本章中，我们专注于微妙设计的架构，并将网络修剪的详细信息留给第4章。本节的概述如图[2.1](#Ch2.F1
    "图2.1 ‣ 第2章 紧凑架构 ‣ 绿色深度学习的调查")所示。
- en: '{forest}'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,
    align=left, minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt,
    inner ysep=1pt, , where level=1font=, where level=2font=, where level=3font=,
    where level=4font=, where level=5font=, [Compact Architecture Design [Component
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 分叉边缘，对于树=生长=东，反向=true，锚点=基线西，父锚点=东，子锚点=西，基准=左，字体=，矩形，绘制=隐藏绘制，圆角，左对齐，最小宽度=2.5em，最小高度=1.2em，s
    sep=6pt，内部xsep=3pt，内部ysep=1pt，，其中级别=1字体=，其中级别=2字体=，其中级别=3字体=，其中级别=4字体=，其中级别=5字体=，[紧凑架构设计[组件
- en: Design [Compact Convolution [Depth-wise Separable Convolution] [Fire Convolution]
    [Flattened Convolution] [Shrinked Convolution] ] [Efficient Attention [Sparse
    Attention] [Attention Approximation] ] [Lightweight Softmax] [Compact Embedding]
    ] [Component
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 设计[紧凑卷积[深度可分离卷积][Fire卷积][展平卷积][缩小卷积]][高效注意力[稀疏注意力][注意力近似]][轻量级Softmax][紧凑嵌入]][组件
- en: Assembling [Memory Sharing] [Static Weight Sharing [Cross-layer Parameter Sharing]
    [Cross-data Parameter Sharing] ] [Dynamic Weight Sharing [Cascading] [Early Exit]
    [Skipping] [Mixture of Experts (MoE)] ] [Deployment Weight Sharing] ] [Compact-architecture
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 组装[内存共享][静态权重共享[跨层参数共享][跨数据参数共享]][动态权重共享[级联][早期退出][跳过][专家混合（MoE）]][部署权重共享]][紧凑架构
- en: Search] ]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索]
- en: 'Figure 2.1: Taxonomy of compact architecture design with representative examples.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：紧凑架构设计的分类及代表性示例。
- en: 2.1 Component Design
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 组件设计
- en: In this section, we describe efficient variants of popular components, including
    convolution, attention, softmax, and embedding.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了流行组件的高效变体，包括卷积、注意力、Softmax 和嵌入。
- en: 2.1.1 Compact Convolution
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1 紧凑卷积
- en: Starting from AlexNet (Krizhevsky et al., [2012](#bib.bib164)), it has been
    a hot direction to build deeper and larger CNNs to achieve better performance (Iandola
    et al., [2014](#bib.bib138); Simonyan & Zisserman, [2015](#bib.bib299); Szegedy
    et al., [2015](#bib.bib314); He et al., [2016a](#bib.bib118); Szegedy et al.,
    [2017](#bib.bib315)). Currently, even a simple CNN baseline contains hundreds
    of layers and thousands of channels. To reduce deployment costs, previous studies
    proposed efficient variants. Here we list several widely-used variants.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从AlexNet（Krizhevsky等，[2012](#bib.bib164)）开始，构建更深更大的CNN以实现更好的性能已经成为一个热门方向（Iandola等，[2014](#bib.bib138)；Simonyan
    & Zisserman，[2015](#bib.bib299)；Szegedy等，[2015](#bib.bib314)；He等，[2016a](#bib.bib118)；Szegedy等，[2017](#bib.bib315)）。目前，即使是一个简单的CNN基线也包含数百层和数千个通道。为了降低部署成本，以前的研究提出了高效变体。在这里，我们列出几个广泛使用的变体。
- en: Depthwise Separable Convolution
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: 'This architecture has been adopted in Xception (Chollet, [2017](#bib.bib50))
    and MobileNet (Howard et al., [2017](#bib.bib132)). Depthwise separable convolution
    contains two components: depthwise convolution and pointwise convolution. The
    depthwise convolution applies a single filter for each input channel. The pointwise
    convolution is a kind of $1\times 1$ convolution. Following this research line,
    many advanced variants have been proposed (Hoang & Jo, [2018](#bib.bib128); Sandler
    et al., [2018](#bib.bib275)). For example,  Wang et al. ([2017](#bib.bib342))
    also proposed a factorized convolution by unravelling the standard convolution
    and arranging the spatial convolution sequentially.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构已被 Xception (Chollet，[2017](#bib.bib50)) 和 MobileNet (Howard 等人，[2017](#bib.bib132))
    采用。深度可分卷积包含两个部分：深度卷积和逐点卷积。深度卷积对每个输入通道应用一个单独的过滤器。逐点卷积是一种 $1\times 1$ 卷积。沿着这一研究方向，提出了许多先进的变体
    (Hoang & Jo，[2018](#bib.bib128); Sandler 等人，[2018](#bib.bib275))。例如，Wang 等人 ([2017](#bib.bib342))
    还通过展开标准卷积并将空间卷积按顺序排列提出了一种因式分解卷积。
- en: Fire Convolution
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 火焰卷积
- en: 'Iandola et al. ([2016](#bib.bib139)) proposed a vision model SqueezeNet. The
    fire module is the key building block. It is similar with depthwise separable
    convolutions. A fire module contains two components: a squeeze convolution layer
    with $1\times 1$ filters and an expand layer with a mixture of $1\times 1$ and
    $3\times 3$ convolution filters.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Iandola 等人 ([2016](#bib.bib139)) 提出了一个视觉模型 SqueezeNet。火焰模块是关键构建块。它类似于深度可分卷积。一个火焰模块包含两个部分：一个使用
    $1\times 1$ 过滤器的收缩卷积层和一个使用 $1\times 1$ 和 $3\times 3$ 卷积过滤器混合的扩展层。
- en: Flattened Convolution
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 扁平卷积
- en: 'It is proposed by Jin et al. ([2015](#bib.bib147)) to decrease the redundancy
    of the filters. It separates the 3D convolution filters into three consecutive
    1D filters: convolution across channels (lateral), vertical, and horizontal direction.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Jin 等人 ([2015](#bib.bib147)) 提出了火焰卷积以减少过滤器的冗余。它将 3D 卷积过滤器分解为三个连续的 1D 过滤器：跨通道（侧向）、垂直和水平方向的卷积。
- en: Shrinked Convolution
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 收缩卷积
- en: Traditional convolutions usually have fixed hyper-parameter settings, like the
    number of filters. Different from these models, MobileNet (Howard et al., [2017](#bib.bib132))
    adopts a dynamic setting, also called shrinked convolution. It introduces a width
    multiplier to thin a network uniformly at each layer. The standard convolutions
    with $M$ input channels and $N$ output channels become a shrinked convolution
    with $\alpha M$ input channels and $\alpha N$ output channels where $\alpha\leq
    1$.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 传统卷积通常具有固定的超参数设置，如过滤器的数量。与这些模型不同，MobileNet (Howard 等人，[2017](#bib.bib132)) 采用了一种动态设置，也称为收缩卷积。它引入了一个宽度乘数，以在每一层均匀地缩小网络。标准卷积具有
    $M$ 个输入通道和 $N$ 个输出通道，变成了具有 $\alpha M$ 个输入通道和 $\alpha N$ 个输出通道的收缩卷积，其中 $\alpha\leq
    1$。
- en: 2.1.2 Efficient Attention
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 高效注意力
- en: Attention (Bahdanau et al., [2015](#bib.bib10)) is first proposed for handling
    long-distance dependency in machine translation. Currently, it has been widely
    used in tasks like summarization, natural language understanding, and so on. The
    key idea is to dynamically attend all tokens at each step. All tokens can be directly
    aligned together, which can address long-distance dependencies to some extent.
    Since any two tokens have an attention score, the required computations grow quadratically
    with the input length. To address this problem, previous studies proposed several
    efficient attention variants. Currently, dop-product-based attention (Vaswani
    et al., [2017](#bib.bib330)) becomes the dominant choice for NLP and CV applications.
    For simplification, we refer to attention as dot-product attention.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力 (Bahdanau 等人，[2015](#bib.bib10)) 首次提出用于处理机器翻译中的长距离依赖。目前，它已广泛用于总结、自然语言理解等任务。关键思想是在每一步动态关注所有标记。所有标记可以直接对齐在一起，这在一定程度上可以解决长距离依赖问题。由于任何两个标记都有一个注意力分数，因此所需的计算量随着输入长度的增加而呈二次增长。为了解决这个问题，以前的研究提出了几种高效的注意力变体。目前，基于点积的注意力
    (Vaswani 等人，[2017](#bib.bib330)) 成为 NLP 和 CV 应用中的主流选择。为了简化，我们将注意力称为点积注意力。
- en: 'We roughly classify these variants into two categories: sparse attention that
    reduces the span of attention, and attention approximation with different attention
    estimation formats. Let us review the original self-attention definition. Formally,
    given a sequence of hidden vectors $x$, we can map it into different representation
    space $Q$, $K$, and $V$. Then, attention takes $Q$, $K$, and $V$ as inputs and
    is responsible for generating vector via the following equation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大致将这些变体分为两类：稀疏注意力，它减少了注意力的范围，以及具有不同注意力估计格式的注意力近似。让我们回顾一下原始的自注意力定义。正式地说，给定一个隐藏向量序列
    $x$，我们可以将其映射到不同的表示空间 $Q$、$K$ 和 $V$。然后，注意力将 $Q$、$K$ 和 $V$ 作为输入，并负责通过以下方程生成向量：
- en: '|  | $\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V$
    |  | (2.1) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V$
    |  | (2.1) |'
- en: where $Q$, $K$, $V$ are 3-dimension tensors, with dimensions of sequence length,
    head number, hidden dimension. The computations mainly come from $(\frac{QK^{T}}{\sqrt{d_{k}}})$
    and softmax operations. This section describes several approaches to reduce dot-product
    computations. We leave the details of efficient softmax variants in Section 2.1.3.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q$、$K$、$V$ 是三维张量，维度为序列长度、头部数量、隐藏维度。计算主要来自 $(\frac{QK^{T}}{\sqrt{d_{k}}})$
    和 softmax 操作。本节描述了几种减少点积计算的方法。我们将在第 2.1.3 节中介绍高效 softmax 变体的详细信息。
- en: Sparse Attention
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 稀疏注意力
- en: As we can see from Eq. [2.1](#Ch2.E1 "In 2.1.2 Efficient Attention ‣ 2.1 Component
    Design ‣ Chapter 2 Compact Architecture ‣ A Survey on Green Deep Learning"), all
    tokens are required to be attended at each step. Several approaches are proposed
    to reduce attention length by only attending local tokens at each step. A natural
    solution is to reduce the number of attended tokens by assigning some tokens with
    zero weights. It is the idea of sparse attention (Martins & Astudillo, [2016](#bib.bib214);
    Child et al., [2019](#bib.bib48); Correia et al., [2019](#bib.bib56); Dai et al.,
    [2019](#bib.bib60); Zaheer et al., [2020](#bib.bib388)).  Martins & Astudillo
    ([2016](#bib.bib214)) proposed Sparsemax, which added a $L_{2}$ regularization
    to encourage the attention matrix to be sparse. Sparsemax has been applied to
    various architectures (Niculae & Blondel, [2017](#bib.bib231); Maruf et al., [2019](#bib.bib215);
    Peters et al., [2019](#bib.bib240)).  Child et al. ([2019](#bib.bib48)) introduced
    heuristic rules and defined two sparse attention variants. One attends previous
    $l$ tokens. The other splits a sequence into different spans where each span has
    $l$ tokens. Each head attends to every $l$ tokens where $l$ is much smaller than
    the length of inputs.  Sukhbaatar et al. ([2019](#bib.bib308)) believed that the
    naive sparse attention was somehow arbitrary. They found that some attention heads
    focused on the recent tokens, while other heads took information from the whole
    context. Motivated by this, they proposed adaptive attention by learning the attention
    span of each head. An similar idea is proposed by Correia et al. ([2019](#bib.bib56)).
    They introduced an adaptive sparse attention approach. They replaced full softmax
    operations with $\alpha$ softmax that allowed low-scoring words to receive precisely
    zero weight. In addition to attention length, the attention head is also an important
    sparse factor.  Voita et al. ([2019](#bib.bib335)) found that only a subset of
    heads matter and the rest can be pruned.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从公式 [2.1](#Ch2.E1 "In 2.1.2 Efficient Attention ‣ 2.1 Component Design ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning") 中可以看到，所有标记在每一步都需要被关注。提出了几种方法来通过仅关注每一步的局部标记来减少注意力长度。一个自然的解决方案是通过为一些标记分配零权重来减少被关注的标记数量。这就是稀疏注意力的思想
    (Martins & Astudillo, [2016](#bib.bib214); Child et al., [2019](#bib.bib48); Correia
    et al., [2019](#bib.bib56); Dai et al., [2019](#bib.bib60); Zaheer et al., [2020](#bib.bib388))。Martins
    & Astudillo ([2016](#bib.bib214)) 提出了 Sparsemax，通过增加 $L_{2}$ 正则化来鼓励注意力矩阵稀疏。Sparsemax
    已应用于各种架构 (Niculae & Blondel, [2017](#bib.bib231); Maruf et al., [2019](#bib.bib215);
    Peters et al., [2019](#bib.bib240))。Child et al. ([2019](#bib.bib48)) 引入了启发式规则并定义了两种稀疏注意力变体。一种关注前
    $l$ 个标记。另一种将序列分成不同的跨度，每个跨度有 $l$ 个标记。每个头部关注每 $l$ 个标记，其中 $l$ 远小于输入的长度。Sukhbaatar
    et al. ([2019](#bib.bib308)) 认为天真的稀疏注意力有些随意。他们发现一些注意力头专注于最近的标记，而其他头则从整个上下文中获取信息。受此启发，他们提出了通过学习每个头的注意力跨度来实现自适应注意力。Correia
    et al. ([2019](#bib.bib56)) 提出了一个类似的想法。他们引入了一种自适应稀疏注意力方法。他们用 $\alpha$ softmax
    替代了完整的 softmax 操作，使低得分的词能够获得精确的零权重。除了注意力长度，注意力头也是一个重要的稀疏因素。Voita et al. ([2019](#bib.bib335))
    发现只有一部分头部是重要的，其余的可以被剪枝。
- en: Attention Approximation
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意力近似
- en: Kitaev et al. ([2020](#bib.bib162)) proposed an efficient attention model Reformer.
    It approximates the dot-product attention computation by one that uses locality-sensitive
    hashing, reducing the complexity from $O(L^{2})$ to $O(L\log L)$, where $L$ is
    the length of the sequence.  Choromanski et al. ([2020](#bib.bib51)) further proposed
    a more efficient model Performer with an unbiased positive random feature map
    estimator. Compared to the original attention, Performer is a linear architecture
    with compatible performance.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Kitaev 等人 ([2020](#bib.bib162)) 提出了高效的注意力模型 Reformer。它通过使用局部敏感哈希来近似点积注意力计算，将复杂度从
    $O(L^{2})$ 降低到 $O(L\log L)$，其中 $L$ 是序列的长度。 Choromanski 等人 ([2020](#bib.bib51))
    进一步提出了一个更高效的模型 Performer，具有无偏正随机特征映射估计器。与原始注意力相比，Performer 是一个线性结构，性能相当。
- en: 2.1.3 Lightweight Softmax
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 轻量级 Softmax
- en: 'Softmax layer is a necessary component for deep learning. The key idea is to
    normalize a vector to a probability distribution of possible labels. Traditional
    softmax is computed as:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 层是深度学习的必要组件。其关键思想是将向量标准化为可能标签的概率分布。传统的 softmax 计算公式为：
- en: '|  | $p(y_{i}&#124;x)=\frac{\exp(h_{i}\cdot w)}{\sum_{j=1}\exp(h_{j}\cdot w)}$
    |  | (2.2) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(y_{i} \mid x)=\frac{\exp(h_{i}\cdot w)}{\sum_{j=1}\exp(h_{j}\cdot w)}$
    |  | (2.2) |'
- en: 'where $y_{i}$ is the $i$-th label and $w$ are learnable parameters. $h_{i}$
    is the $i$-th dimension of hidden vector. $x$ is the input sequence. The denominator
    requires the dot-product over label candidates. If the task has a large label
    set, the denominator will require large computations. Since the complexity of
    softmax is proportional to the number of labels and sequence generation (Mikolov
    et al., [2013a](#bib.bib220), [b](#bib.bib221)) tasks usually have a large vocabulary
    size, we take sequence generation as an example to show several lightweight variants
    with fewer computations. In sequence generation tasks, token vocabulary is equal
    to the label set. Formally, given a hidden vector $h$ and all token embeddings,
    the softmax for sequence generation is computed as:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{i}$ 是第 $i$ 个标签，$w$ 是可学习的参数。$h_{i}$ 是隐藏向量的第 $i$ 维。$x$ 是输入序列。分母需要对标签候选进行点积计算。如果任务有较大的标签集，分母将需要大量计算。由于
    softmax 的复杂度与标签数量成正比，而序列生成 (Mikolov 等，[2013a](#bib.bib220)，[b](#bib.bib221)) 任务通常具有较大的词汇量，我们以序列生成为例，展示几种计算量较少的轻量级变体。在序列生成任务中，token
    词汇等于标签集。形式上，给定一个隐藏向量 $h$ 和所有 token 嵌入，序列生成的 softmax 计算公式为：
- en: '|  | $p(y_{i}&#124;x)=\frac{\exp(h^{T}v_{i})}{\sum_{j=1}\exp(h^{T}v_{j})}$
    |  | (2.3) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(y_{i} \mid x)=\frac{\exp(h^{T}v_{i})}{\sum_{j=1}\exp(h^{T}v_{j})}$
    |  | (2.3) |'
- en: where $x$ is the input and $v_{i}$ is the embedding of the $i$-th token. Eq. [2.3](#Ch2.E3
    "In 2.1.3 Lightweight Softmax ‣ 2.1 Component Design ‣ Chapter 2 Compact Architecture
    ‣ A Survey on Green Deep Learning") shows that the softmax layer introduces embeddings
    for all tokens and requires the inner-product between hidden vector and all embeddings.
    A large vocabulary will require many computations. Therefore, several efficient
    softmax variants have been proposed.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是输入，$v_{i}$ 是第 $i$ 个 token 的嵌入。方程式 [2.3](#Ch2.E3 "在 2.1.3 轻量级 Softmax
    ‣ 2.1 组件设计 ‣ 第 2 章 紧凑架构 ‣ 绿色深度学习综述") 显示 softmax 层为所有 token 引入嵌入，并需要隐藏向量与所有嵌入之间的内积。大词汇量将需要大量计算。因此，已经提出了几种高效的
    softmax 变体。
- en: Fewer Parameters
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 更少参数
- en: To reduce memory usage,  Press & Wolf ([2017](#bib.bib250)) proposed to tie
    input embeddings and embeddings in Eq. [2.3](#Ch2.E3 "In 2.1.3 Lightweight Softmax
    ‣ 2.1 Component Design ‣ Chapter 2 Compact Architecture ‣ A Survey on Green Deep
    Learning"). They conducted experiments on machine translation. Results show that
    weight sharing can reduce the size of neural translation models without harming
    translation results. In addition, reducing the number of labels is another important
    research direction. Recently, several studies have been  (Kim et al., [2016b](#bib.bib161);
    Costa-jussà & Fonollosa, [2016](#bib.bib57)) proposed to generate a sequence in
    character level, rather than in word level. The number of characters is largely
    less than that of words and the computations for softmax can be largely reduced.
    Similarly,  Józefowicz et al. ([2016](#bib.bib154)) implemented character-based
    softmax on language modeling, which achieved promising results. It is important
    to note that these character-based methods also bring longer sequences. Current
    sequence generation models usually adopt auto-regressive generation frameworks.
    The longer sequence brings higher decoding costs. In all, it should be considered
    case-by-case whether character-based methods reduce the whole decoding cost. Recently,
    a trade-off is achieved by sub-word level vocabularies (Sennrich et al., [2016](#bib.bib289)).
    Sub-word level vocabularies have a tradeoff granularity between character vocabularies
    and word vocabularies. Sub-word level vocabularies have more tokens than character-level
    vocabulary but also have much shorter segmented sequences. Therefore, sub-word
    level vocabularies become the popular choice for almost all sequence generation
    tasks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少内存使用，Press & Wolf ([2017](#bib.bib250)) 提出了将输入嵌入和 Eq. [2.3](#Ch2.E3 "In
    2.1.3 Lightweight Softmax ‣ 2.1 Component Design ‣ Chapter 2 Compact Architecture
    ‣ A Survey on Green Deep Learning") 中的嵌入结合起来。他们在机器翻译上进行了实验。结果表明，权重共享可以在不影响翻译结果的情况下减少神经翻译模型的大小。此外，减少标签数量是另一个重要的研究方向。最近，一些研究（Kim
    et al., [2016b](#bib.bib161); Costa-jussà & Fonollosa, [2016](#bib.bib57)）提出在字符级别生成序列，而不是在词级别。字符数量远少于词数，且软最大值的计算可以大大减少。同样，Józefowicz
    et al. ([2016](#bib.bib154)) 在语言建模中实现了基于字符的软最大值，取得了良好的结果。值得注意的是，这些基于字符的方法也带来了更长的序列。目前的序列生成模型通常采用自回归生成框架。更长的序列带来了更高的解码成本。总体来说，是否减少整体解码成本需要逐案考虑。最近，通过子词级词汇（Sennrich
    et al., [2016](#bib.bib289)）达成了一种折衷。子词级词汇在字符词汇和词词汇之间具有折衷粒度。子词级词汇的标记数量多于字符级词汇，但分段序列要短得多。因此，子词级词汇成为几乎所有序列生成任务中的流行选择。
- en: Fewer Computations
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 更少的计算
- en: 'We classify softmax variants with fewer computations into five categories:
    hierarchical softmax, softmax with dynamic embeddings, sampling-based softmax,
    hashing-based softmax, and normalization-based softmax. Hierarchical softmax (H-Softmax) (Morin
    & Bengio, [2005](#bib.bib226); Mnih & Hinton, [2008](#bib.bib224)) is a kind of
    softmax variant. To be specific, it formulates a label set as a tree and all labels
    in the set is the leaf node. The complexity can be dropped from $O(N)$ to $O(log(N))$
    where $N$ is the size of the label set. In this way, the traditional one single
    probability over labels is decomposed into a product of a sequence of probability
    over each tree layer. The regular softmax can be regarded as a tree of depth $1$,
    with all labels as leaf nodes. The second research direction focuses on dynamic
    label embeddings (Chen et al., [2016b](#bib.bib41)). The intuition is that not
    all labels require the same parameter size. It assigns variable parameter sizes
    for different labels. In particular, the approach assigns more parameters to frequent
    labels. The embedding size affects the computation costs. Therefore, this kind
    of method can reduce the computations required by softmax operations. In addition,
    sampling-based softmax aims to estimate the full softmax computations with sampled
    label candidates. The key idea is to sample several label embeddings to estimate
    all embeddings (Bengio & Senecal, [2003](#bib.bib18), [2008](#bib.bib19); Jean
    et al., [2015](#bib.bib144)). Hoever, it only reduces training costs while the
    full softmax is still be computed to obtain a variance-free result during inference.
    Hashing-based softmax is another kind of estimation variant.  Vijayanarasimhan
    et al. ([2015](#bib.bib332)) proposed a fast locality-sensitive hashing technique
    to approximate the actual dot-product. Normalization-based softmax (Devlin et al.,
    [2014](#bib.bib64); Andreas & Klein, [2015](#bib.bib6)) aims to avoid explicit
    denominator. The target is to output a vector as close as the probability distribution
    with the sum being 1.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算量较少的 softmax 变体分为五类：层次 softmax、动态嵌入的 softmax、基于采样的 softmax、基于哈希的 softmax
    和基于归一化的 softmax。层次 softmax（H-Softmax）(Morin & Bengio, [2005](#bib.bib226); Mnih
    & Hinton, [2008](#bib.bib224)) 是一种 softmax 变体。具体来说，它将标签集表示为树形结构，所有标签在集合中都是叶节点。复杂度可以从
    $O(N)$ 降到 $O(log(N))$，其中 $N$ 是标签集的大小。通过这种方式，传统的标签上的单一概率被分解为每个树层的概率序列的乘积。常规 softmax
    可以看作是深度为 $1$ 的树，所有标签作为叶节点。第二个研究方向关注于动态标签嵌入 (Chen et al., [2016b](#bib.bib41))。其直觉是，并非所有标签都需要相同的参数大小。它为不同标签分配变量参数大小。特别地，该方法为频繁标签分配更多参数。嵌入大小会影响计算成本。因此，这种方法可以减少
    softmax 操作所需的计算。此外，基于采样的 softmax 旨在通过采样标签候选来估算完整的 softmax 计算。关键思想是通过采样几个标签嵌入来估算所有嵌入
    (Bengio & Senecal, [2003](#bib.bib18), [2008](#bib.bib19); Jean et al., [2015](#bib.bib144))。然而，它只减少了训练成本，而完整的
    softmax 仍需计算以获得无方差的结果。基于哈希的 softmax 是另一种估算变体。Vijayanarasimhan et al. ([2015](#bib.bib332))
    提出了快速局部敏感哈希技术来近似实际的点积。基于归一化的 softmax (Devlin et al., [2014](#bib.bib64); Andreas
    & Klein, [2015](#bib.bib6)) 旨在避免显式的分母。目标是输出一个尽可能接近概率分布且总和为 1 的向量。
- en: 2.1.4 Compact Embeddings
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 紧凑嵌入
- en: 'Building token embeddings is the first step for NLP tasks. The parameters of
    embeddings are decided by vocabulary size and embedding length. How to reduce
    embedding parameters is an important and interesting topic. Learning compact token
    vectors is related to learning compressed neural networks. There have been several
    techniques for learning compact neural networks, like pruning, knowledge distillation,
    low-rank approximation, and quantization. In this part, we only focus on related
    approaches for compact embeddings. We classify these approaches into four categories:
    reuse-based approaches, knowledge-distillation-based approaches, low-rank-based
    approaches, fine-grained vocabularies.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 构建词元嵌入是 NLP 任务的第一步。嵌入的参数由词汇表大小和嵌入长度决定。如何减少嵌入参数是一个重要且有趣的话题。学习紧凑的词元向量与学习压缩神经网络相关。已经有几种学习紧凑神经网络的技术，如剪枝、知识蒸馏、低秩近似和量化。在这一部分，我们仅关注与紧凑嵌入相关的方法。我们将这些方法分为四类：基于重用的方法、基于知识蒸馏的方法、基于低秩的方法和细粒度词汇。
- en: Reuse-based approaches focus on compositional embeddings (Faruqui et al., [2015](#bib.bib81);
    Chen et al., [2016c](#bib.bib46); Shu & Nakayama, [2018](#bib.bib296); Joshi et al.,
    [2019](#bib.bib152); Shi et al., [2020](#bib.bib294)). For example,  Faruqui et al.
    ([2015](#bib.bib81)) aimed to represent each token embedding as a sparse linear
    combination of basis vectors. The size of basis vectors is much less than token
    embeddings. Similar idea has been proposed by Chen et al. ([2016c](#bib.bib46)).
    They split the vocabulary into two parts. One part is a base set containing frequent
    tokens with fixed size (e.g., 8K), the other part is a set of rare tokens whose
    embeddings are encoded by the base set’s embeddings. Following these studies,
     Shu & Nakayama ([2018](#bib.bib296)) adopted the quantization approach to construct
    embeddings with few basis vectors. Recently, this idea has been adapted to other
    fields beyond NLP, like recommendation systems (Shi et al., [2020](#bib.bib294)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 基于重用的方法侧重于组合嵌入（Faruqui 等，[2015](#bib.bib81)；Chen 等，[2016c](#bib.bib46)；Shu &
    Nakayama，[2018](#bib.bib296)；Joshi 等，[2019](#bib.bib152)；Shi 等，[2020](#bib.bib294)）。例如，Faruqui
    等（[2015](#bib.bib81)）旨在将每个词嵌入表示为基础向量的稀疏线性组合。基础向量的大小远小于词嵌入。Chen 等（[2016c](#bib.bib46)）提出了类似的想法。他们将词汇分为两个部分。一部分是包含频繁词汇的基集合（如
    8K），另一部分是通过基集合的嵌入编码的稀有词汇集合。在这些研究的基础上，Shu & Nakayama（[2018](#bib.bib296)）采用量化方法用少量基础向量构造嵌入。最近，这一想法已被应用到自然语言处理以外的领域，如推荐系统（Shi
    等，[2020](#bib.bib294)）。
- en: In addition, traditional compression approaches have been applied to compress
    embeddings.  Mou et al. ([2016](#bib.bib227)) used knowledge distillation to transfer
    knowledge from a big token embedding layer into a smaller embedding layer.  Chen
    et al. ([2018a](#bib.bib37)) used vocabulary-partition (block) based low-rank
    matrix approximation to reduce parameter size.  Lam ([2018](#bib.bib168)) used
    1-2 bits per parameter, rather than traditional 32-bits, for token embedding.
    Vocabulary size is also an important factor in deciding embedding size. Therefore,
    fine-grained vocabularies have been proposed to reduce the vocabulary length,
    like character-level vocabulary (Kim et al., [2016b](#bib.bib161)), subword-level
    vocabulary (Sennrich et al., [2016](#bib.bib289)), and byte-level vocabulary (Wang
    et al., [2020a](#bib.bib338)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，传统的压缩方法已被应用于压缩嵌入。Mou 等（[2016](#bib.bib227)）使用知识蒸馏将知识从大型词嵌入层转移到较小的嵌入层。Chen
    等（[2018a](#bib.bib37)）使用词汇分区（块）基础的低秩矩阵近似来减少参数大小。Lam（[2018](#bib.bib168)）为词嵌入使用了每个参数
    1-2 位，而不是传统的 32 位。词汇大小也是决定嵌入大小的重要因素。因此，提出了细粒度词汇来减少词汇长度，如字符级词汇（Kim 等，[2016b](#bib.bib161)）、子词级词汇（Sennrich
    等，[2016](#bib.bib289)）和字节级词汇（Wang 等，[2020a](#bib.bib338)）。
- en: 2.2 Component Assembling
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 组件组装
- en: 'This part presents several component assembling solutions for efficient architecture
    design. Many widely-used architectures are efficient component assembling solutions.
    CNNs and LSTMs are representative models. A single filter in CNNs can handle all
    input spans. LSTMs adopt the same parameters for all steps. The key idea of efficient
    component assembling lies in sharing. We classify these assembling solutions into
    four categories: memory sharing, static weight sharing, dynamic weight sharing,
    and deployment weight sharing.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分介绍了几种组件组装解决方案以实现高效的架构设计。许多广泛使用的架构都是高效组件组装的解决方案。CNN 和 LSTM 是代表性模型。CNN 中的单个滤波器可以处理所有输入跨度。LSTM
    在所有步骤中采用相同的参数。高效组件组装的关键思想在于共享。我们将这些组装解决方案分为四类：记忆共享、静态权重共享、动态权重共享和部署权重共享。
- en: 2.2.1 Memory Sharing
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 记忆共享
- en: Memory sharing is a common technique to store a large model on devices with
    limited memories. A natural idea is to share the same storage among intermediate
    forward vectors (Pleiss et al., [2017](#bib.bib245)) or backward vectors (Chen
    et al., [2016a](#bib.bib39); Gruslys et al., [2016](#bib.bib103)). There are also
    some reversible models (Gomez et al., [2017](#bib.bib96); MacKay et al., [2018](#bib.bib213))
    where the activation of each layer can be reconstructed from the next layer to
    reduce memory requirements during the backward process. The models do not need
    to save intermediate activation vectors. Since several vectors share the same
    storage space, recomputation is necessary in some cases. To achieve a trade-off
    between efficient memory usage and fewer computations, some studies (Wang et al.,
    [2018a](#bib.bib341)) proposed to combine memory sharing with liveness analysis (Wang
    et al., [2016](#bib.bib340)). During graph computation, GPUs adopt liveness analysis
    to create tensors and free tensors. For large intermediate tensors, frequent allocation/deallocation
    operations are time-consuming. Therefore, the runtime can be reduced by directly
    reusing memory segments from a huge pre-allocated memory pool. In addition to
    memory optimization on a single node,  Rajbhandari et al. ([2020](#bib.bib256))
    further explored memory sharing on distributed settings across multiple computation
    nodes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 内存共享是一种常见的技术，用于在内存有限的设备上存储大型模型。一种自然的想法是共享中间前向向量（Pleiss 等， [2017](#bib.bib245)）或反向向量（Chen
    等， [2016a](#bib.bib39)；Gruslys 等， [2016](#bib.bib103)）的相同存储空间。也有一些可逆模型（Gomez 等，
    [2017](#bib.bib96)；MacKay 等， [2018](#bib.bib213)），其中每一层的激活可以从下一层重构，以减少在反向过程中对内存的需求。这些模型不需要保存中间激活向量。由于多个向量共享相同的存储空间，某些情况下需要重新计算。为了在高效内存使用和较少计算之间取得平衡，一些研究（Wang
    等， [2018a](#bib.bib341)）提出将内存共享与活跃性分析（Wang 等， [2016](#bib.bib340)）相结合。在图计算过程中，GPU
    采用活跃性分析来创建张量和释放张量。对于大型中间张量，频繁的分配/释放操作是耗时的。因此，可以通过直接重用来自巨大预分配内存池的内存段来减少运行时间。除了对单节点的内存优化之外，Rajbhandari
    等（[2020](#bib.bib256)）进一步探讨了在多个计算节点分布式设置中的内存共享。
- en: 2.2.2 Static Weight Sharing
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 静态权重共享
- en: Unlike memory sharing, static weight sharing aims at exploring how to reuse
    weights for a neural network. The difference between weights and intermediate
    vectors is that weights are fixed during inference and shared by all examples.
    To save memory, many models choose to reuse parameters across different layers
    or different tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与内存共享不同，静态权重共享旨在探索如何在神经网络中重用权重。权重与中间向量的区别在于，权重在推理过程中是固定的，并被所有示例共享。为了节省内存，许多模型选择在不同层或不同任务之间重用参数。
- en: Cross-layer Parameter Sharing
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 跨层参数共享
- en: Cross-layer parameter sharing is a common technique for parameter efficiency.
    The idea of sharing parameters across layers has been well explored (Dehghani
    et al., [2019](#bib.bib61); Bai et al., [2019](#bib.bib13); Lan et al., [2020](#bib.bib170)).
    Savarese & Maire ([2019](#bib.bib278)) proposed a parameter sharing scheme that
    defined a global bank of templates. The parameters of each layer of a CNN come
    from the linear combination of these templates.  Dehghani et al. ([2019](#bib.bib61))
    proposed a model, called Universal Transformer, where all layers shared the same
    parameters. Following these study,  Lan et al. ([2020](#bib.bib170)) applied cross-layer
    sharing mechanism on pre-train/fine-tune settings and  Takase & Kiyono ([2021](#bib.bib317))
    proposed diverse sharing strategies. Recently,  Plummer et al. ([2020](#bib.bib246))
    adopted the idea of network architecture search to automatically learn how to
    share parameters between all layers in a network.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 跨层参数共享是一种常见的参数效率技术。跨层共享参数的思想已被充分探讨（Dehghani 等， [2019](#bib.bib61)；Bai 等， [2019](#bib.bib13)；Lan
    等， [2020](#bib.bib170)）。Savarese & Maire（[2019](#bib.bib278)）提出了一种参数共享方案，定义了一个全局模板库。CNN
    的每一层的参数都来自这些模板的线性组合。Dehghani 等（[2019](#bib.bib61)）提出了一种名为 Universal Transformer
    的模型，其中所有层共享相同的参数。继这些研究之后，Lan 等（[2020](#bib.bib170)）在预训练/微调设置中应用了跨层共享机制，Takase
    & Kiyono（[2021](#bib.bib317)）提出了多样化的共享策略。最近，Plummer 等（[2020](#bib.bib246)）采用了网络架构搜索的思想，以自动学习如何在网络的所有层之间共享参数。
- en: Cross-task Parameter Sharing
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 跨任务参数共享
- en: Cross-task parameter sharing is also a popular solution to handle multi-task,
    multi-domain, or multi-lingual problems (Ramsundar et al., [2015](#bib.bib259);
    Duong et al., [2015](#bib.bib73); Søgaard & Goldberg, [2016](#bib.bib302); Hashimoto
    et al., [2017](#bib.bib115); Yang et al., [2017](#bib.bib374); Raffel et al.,
    [2020](#bib.bib255)). The key idea of cross-task is to enable all tasks (or languages/domains)
    to share parameters. Multi-task learning (Ruder, [2019](#bib.bib268)) has two
    popular implementations, including hard and soft parameter sharing. Compared to
    soft parameter sharing where different tasks do not have shared networks, hard
    parameter sharing uses fewer parameters. Therefore, we only focus on hard parameter
    sharing in this work.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 跨任务参数共享也是处理多任务、多领域或多语言问题的一个流行解决方案（Ramsundar 等，[2015](#bib.bib259)；Duong 等，[2015](#bib.bib73)；Søgaard
    & Goldberg，[2016](#bib.bib302)；Hashimoto 等，[2017](#bib.bib115)；Yang 等，[2017](#bib.bib374)；Raffel
    等，[2020](#bib.bib255)）。跨任务的关键思想是使所有任务（或语言/领域）共享参数。多任务学习（Ruder，[2019](#bib.bib268)）有两种流行的实现，包括硬参数共享和软参数共享。与不同任务没有共享网络的软参数共享相比，硬参数共享使用的参数更少。因此，本工作只关注硬参数共享。
- en: To be specific, cross-task sharing is initially implemented by sharing the hidden
    layers between all tasks, while keeping several task-specific output layers (Yang
    et al., [2017](#bib.bib374); Houlsby et al., [2019](#bib.bib130); Raffel et al.,
    [2020](#bib.bib255)). For the CV field, multi-task solutions often share CNN layers.
    For the NLP filed, in addition to naive sharing, researchers also focus on finding
    better parameter reusing solutions for different tasks. For example,  Søgaard
    & Goldberg ([2016](#bib.bib302)) found that low-level tasks, e.g., part-of-speech
    tagging, should share parameters at lower layers. Motivated by these findings,
     Hashimoto et al. ([2017](#bib.bib115)) proposed a parameter-sharing network across
    multiple NLP tasks. Currently, the trend of developing large-scale models encourages
    researchers to directly use one single model to support multiple tasks. T5 (Raffel
    et al., [2020](#bib.bib255)) is one representative model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，跨任务共享最初是通过在所有任务之间共享隐藏层来实现的，同时保留几个任务特定的输出层（Yang 等，[2017](#bib.bib374)；Houlsby
    等，[2019](#bib.bib130)；Raffel 等，[2020](#bib.bib255)）。对于计算机视觉领域，多任务解决方案通常共享CNN层。对于自然语言处理领域，除了简单共享外，研究人员还专注于为不同任务寻找更好的参数重用解决方案。例如，Søgaard
    & Goldberg（[2016](#bib.bib302)）发现低级任务，如词性标注，应在较低层共享参数。受这些发现的启发，Hashimoto 等（[2017](#bib.bib115)）提出了一个跨多个NLP任务的参数共享网络。目前，开发大规模模型的趋势鼓励研究人员直接使用一个单一模型来支持多个任务。T5（Raffel
    等，[2020](#bib.bib255)）是一个代表性模型。
- en: Multilingual is also a special cross-task variant. At the early stage of multilingual
    models, researchers usually choose to share a part of parameters across different
    languages (Firat et al., [2016](#bib.bib86); Upadhyay et al., [2016](#bib.bib327);
    Blackwood et al., [2018](#bib.bib26)). Recently, multilingual approaches usually
    treated all languages equally and mixed them together to train a single model (Ha
    et al., [2016](#bib.bib108); Firat et al., [2017](#bib.bib87); Johnson et al.,
    [2017](#bib.bib149); Fan et al., [2020a](#bib.bib77)). More recently, adapter-based
    solutions have been widely used for modeling task-specific features beyond shared
    parameters (Houlsby et al., [2019](#bib.bib130); Bapna & Firat, [2019](#bib.bib15);
    Pfeiffer et al., [2020](#bib.bib243)).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言也是一种特殊的跨任务变体。在多语言模型的早期阶段，研究人员通常选择在不同语言之间共享一部分参数（Firat 等，[2016](#bib.bib86)；Upadhyay
    等，[2016](#bib.bib327)；Blackwood 等，[2018](#bib.bib26)）。最近，多语言方法通常将所有语言平等对待，并将其混合在一起以训练单一模型（Ha
    等，[2016](#bib.bib108)；Firat 等，[2017](#bib.bib87)；Johnson 等，[2017](#bib.bib149)；Fan
    等，[2020a](#bib.bib77)）。更近期，基于适配器的解决方案已被广泛用于建模任务特定特征，超越了共享参数（Houlsby 等，[2019](#bib.bib130)；Bapna
    & Firat，[2019](#bib.bib15)；Pfeiffer 等，[2020](#bib.bib243)）。
- en: 2.2.3 Dynamic Weight Sharing
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 动态权重共享
- en: 'Static parameter sharing usually relies on pre-specified networks. Researchers
    define heuristic rules based on shared features to decide which layers/components
    should be shared by different inputs/tasks. Although this solution is natural,
    hard sharing usually fails in handling tasks that are not closely related. Dynamic
    solutions are proposed to decide which layers/components should be shared among
    different input samples. Specifically, dynamic networks are neural networks with
    dynamic computational graphs where the computational topology or parameters are
    decided on the fly. Therefore, this kind of network can reduce computation costs
    and improve the adaptiveness of networks. In this survey, we describe the overview
    of general dynamic architectures. If you are interested in other dynamic features,
    you can find surveys focusing on dynamic networks (Han et al., [2021b](#bib.bib112)).
    Networks with dynamic architecture can be classified into the following classes:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 静态参数共享通常依赖于预先指定的网络。研究人员根据共享特征定义启发式规则，以决定哪些层/组件应由不同的输入/任务共享。尽管这种解决方案很自然，但硬共享通常在处理不紧密相关的任务时失败。动态解决方案被提出以决定在不同输入样本之间共享哪些层/组件。具体而言，动态网络是具有动态计算图的神经网络，其中计算拓扑或参数是即时决定的。因此，这种网络可以降低计算成本并提高网络的适应性。在本综述中，我们描述了通用动态架构的概述。如果你对其他动态特性感兴趣，可以查阅专注于动态网络的综述（Han
    等，[2021b](#bib.bib112)）。具有动态架构的网络可以分为以下几类：
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Cascading-style Networks. Multiple basic networks are cascaded in a directed
    acyclic graph (DAG) in a from-small-to-big manner, where the model first executes
    smaller networks, then larger networks. If a smaller network can handle the input
    sample, the model will stop the execution process and does not run execute models.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 级联风格网络。多个基本网络在有向无环图（DAG）中以从小到大的方式级联，其中模型首先执行较小的网络，然后执行较大的网络。如果较小的网络可以处理输入样本，模型将停止执行过程，并且不会运行后续模型。
- en: •
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Early-exit-style Networks. A single network contains multiple internal classifiers,
    allowing “easy” samples to exit at shallow layers. The difference with cascading-style
    networks lies in early-exiting networks feed the output of previous layer to the
    next layer while cascading-style networks cut off this information flow and every
    network only takes raw samples as inputs.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 早退出风格网络。一个网络包含多个内部分类器，允许“简单”的样本在较浅的层次退出。与级联风格网络的不同之处在于，早退出网络将前一层的输出传递给下一层，而级联风格网络则切断这些信息流，每个网络仅接受原始样本作为输入。
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Skipping-style Networks. It accelerates inference by either skipping certain
    layers, or skipping unimportant input spans in the whole input sequence.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跳过风格网络。通过跳过某些层或跳过整个输入序列中不重要的输入跨度来加速推理。
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixture-of-experts-style Networks. Multiple experts are provided as candidates
    in the same block. Only a small part of experts are used in each block for inference.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 专家混合风格网络。在同一个块中提供多个专家作为候选。每个块中仅使用一小部分专家进行推理。
- en: Cascading-style dynamic networks have a historical development (Viola & Jones,
    [2001](#bib.bib333); Lienhart & Maydt, [2002](#bib.bib189); Viola & Jones, [2004](#bib.bib334)).
    The authors cascade architectures are originally proposed for unbalanced binary
    classification tasks. They cascaded multiple basic models and fed the input to
    the next model only if the current model was not confident of its prediction.
    For example, Park et al. ([2015](#bib.bib236)) cascaded two VGG networks in a
    small-first manner to obtain a better trade-off between classification accuracy
    and energy consumption. The smaller model can handle most samples, which largely
    reduce inference costs. Bolukbasi et al. ([2017](#bib.bib27)) cascaded AlexNet,
    GoogLeNet, and ResNet together. Wang et al. ([2018c](#bib.bib348)) introduced
    a cost-aware objective for jointly training criterion functions among basic models.
    More recently, Li et al. ([2020b](#bib.bib183)) proposed a dynamic framework for
    accelerating the inference of pre-trained language models, CascadeBERT, which
    dynamically selected proper-sized and complete models in a cascading manner.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 螺旋样式动态网络有着悠久的发展历史（Viola & Jones, [2001](#bib.bib333)；Lienhart & Maydt, [2002](#bib.bib189)；Viola
    & Jones, [2004](#bib.bib334)）。作者们最初提出的级联架构是为了应对不平衡的二分类任务。他们将多个基础模型级联，并仅在当前模型对其预测不够自信时，将输入传递到下一个模型。例如，Park
    et al. ([2015](#bib.bib236)) 以小模型优先的方式级联了两个VGG网络，以获得分类准确率与能耗之间的更好平衡。较小的模型可以处理大多数样本，从而大幅减少推断成本。Bolukbasi
    et al. ([2017](#bib.bib27)) 级联了AlexNet、GoogLeNet和ResNet。Wang et al. ([2018c](#bib.bib348))
    为基础模型之间的标准函数联合训练引入了成本感知目标。最近，Li et al. ([2020b](#bib.bib183)) 提出了一个加速预训练语言模型推断的动态框架CascadeBERT，该框架以级联方式动态选择合适大小和完整的模型。
- en: 'Table 2.1: An overview of widely-used confidence criteria deciding whether
    the forward process should be terminated in cascading-style and early-exiting-style
    networks. In the Formulation column, $\mathbbm{1}(\cdot)\in\{0,1\}$ indicates
    action {“continue”, “terminate”}. $\alpha$ is the threshold. $\lambda$ and $\tau$
    are hyper-parameters. $\text{MLP}(\cdot)$ is a learnable module.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：广泛使用的置信度标准概述，决定了级联样式和早期退出样式网络中是否应该终止前向过程。在公式列中，$\mathbbm{1}(\cdot)\in\{0,1\}$
    表示动作 {“继续”，“终止”}。$\alpha$ 是阈值。$\lambda$ 和 $\tau$ 是超参数。$\text{MLP}(\cdot)$ 是一个可学习的模块。
- en: '| Criterion | Descriptions | Formulation |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 描述 | 公式 |'
- en: '| Confidence-based Criterion |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 基于置信度的标准 |'
- en: '| Score margin (Park et al., [2015](#bib.bib236)) | The gap between the largest
    and the second largest values among the predicted probability distribution. |
    $\mathbbm{1}(\bm{\hat{y}}^{1\rm{st}}-\bm{\hat{y}}^{2\rm{nd}}<\alpha)$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 得分边界（Park et al., [2015](#bib.bib236)) | 预测概率分布中最大值与第二大值之间的差距。 | $\mathbbm{1}(\bm{\hat{y}}^{1\rm{st}}-\bm{\hat{y}}^{2\rm{nd}}<\alpha)$
    |'
- en: '| Entropy (Teerapittayanon et al., [2016](#bib.bib324)) (Liu et al., [2020a](#bib.bib200))
    (Li et al., [2021](#bib.bib186)) | The entropy or normalized entropy of the predicted
    probability distribution. | $\mathbbm{1}(H(\bm{\hat{y}})>\alpha)$ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 熵（Teerapittayanon et al., [2016](#bib.bib324)）（Liu et al., [2020a](#bib.bib200)）（Li
    et al., [2021](#bib.bib186)） | 预测概率分布的熵或归一化熵。 | $\mathbbm{1}(H(\bm{\hat{y}})>\alpha)$
    |'
- en: '| Max probability (Kaya et al., [2019](#bib.bib155)) (Wang et al., [2020d](#bib.bib351))
    | The maximum predicted probability. | $\mathbbm{1}(\max(\bm{\hat{y}})<\alpha)$
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 最大概率（Kaya et al., [2019](#bib.bib155))（Wang et al., [2020d](#bib.bib351))
    | 预测的最大概率。 | $\mathbbm{1}(\max(\bm{\hat{y}})<\alpha)$ |'
- en: '| Counting-based Criterion |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 基于计数的标准 |'
- en: '| Patience (Zhou et al., [2020](#bib.bib399)) | The number of identical predictions.
    | $cnt_{i}^{cls}=\begin{cases}cnt_{i-1}+1&amp;\arg\max(\bm{\hat{y}}_{i})=\arg\max(\bm{\hat{y}}_{i-1})\\
    0&amp;\arg\max(\bm{\hat{y}}_{i})\neq\arg\max(\bm{\hat{y}}_{i-1})\lor i=0\end{cases}$
    <math   alttext="cnt_{i}^{reg}=\begin{cases}cnt_{i-1}+1&amp;&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;<\tau\\'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '| 耐心（Zhou et al., [2020](#bib.bib399)) | 相同预测的次数。 | $cnt_{i}^{cls}=\begin{cases}cnt_{i-1}+1&amp;\arg\max(\bm{\hat{y}}_{i})=\arg\max(\bm{\hat{y}}_{i-1})\\
    0&amp;\arg\max(\bm{\hat{y}}_{i})\neq\arg\max(\bm{\hat{y}}_{i-1})\lor i=0\end{cases}$
    <math alttext="cnt_{i}^{reg}=\begin{cases}cnt_{i-1}+1&amp;&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;<\tau\\'
- en: 0&amp;&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;\geq\tau\lor i=0\end{cases}"
    display="inline"><semantics ><mrow ><mrow ><mi >c</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><msubsup ><mi >t</mi><mi
    >i</mi><mrow ><mi >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >g</mi></mrow></msubsup></mrow><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mrow ><mrow ><mi >c</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >t</mi><mrow ><mi >i</mi><mo >−</mo><mn
    >1</mn></mrow></msub></mrow><mo >+</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"
    ><mrow ><mrow ><mo stretchy="false" >&#124;</mo><mrow ><msub ><mover accent="true"  ><mi
    >𝒚</mi><mo mathvariant="bold" >^</mo></mover><mi >i</mi></msub><mo >−</mo><msub
    ><mover accent="true"  ><mi >𝒚</mi><mo mathvariant="bold" >^</mo></mover><mrow
    ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo stretchy="false"  >&#124;</mo></mrow><mo
    ><</mo><mi >τ</mi></mrow></mtd></mtr><mtr ><mtd columnalign="left" ><mn >0</mn></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mo stretchy="false"  >&#124;</mo><mrow ><msub
    ><mover accent="true"  ><mi >𝒚</mi><mo mathvariant="bold" >^</mo></mover><mi >i</mi></msub><mo
    >−</mo><msub ><mover accent="true"  ><mi >𝒚</mi><mo mathvariant="bold" >^</mo></mover><mrow
    ><mi >i</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo stretchy="false"  >&#124;</mo></mrow><mo
    >≥</mo><mrow ><mi >τ</mi><mo >∨</mo><mi >i</mi></mrow><mo >=</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply ><ci >𝑐</ci><ci >𝑛</ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑡</ci><ci >𝑖</ci></apply><apply ><ci >𝑟</ci><ci >𝑒</ci><ci >𝑔</ci></apply></apply></apply><apply
    ><csymbol cd="latexml"  >cases</csymbol><apply ><apply ><ci >𝑐</ci><ci >𝑛</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><apply ><ci >𝑖</ci><cn
    type="integer"  >1</cn></apply></apply></apply><cn type="integer"  >1</cn></apply><apply
    ><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci
    >bold-^</ci><ci >𝒚</ci></apply><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><ci >bold-^</ci><ci >𝒚</ci></apply><apply ><ci >𝑖</ci><cn type="integer" >1</cn></apply></apply></apply></apply><ci
    >𝜏</ci></apply><cn type="integer"  >0</cn><apply ><apply ><apply ><apply ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >bold-^</ci><ci >𝒚</ci></apply><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci
    >bold-^</ci><ci >𝒚</ci></apply><apply ><ci >𝑖</ci><cn type="integer" >1</cn></apply></apply></apply></apply><apply
    ><ci >𝜏</ci><ci >𝑖</ci></apply></apply><apply ><cn type="integer"  >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >cnt_{i}^{reg}=\begin{cases}cnt_{i-1}+1&&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;<\tau\\
    0&&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;\geq\tau\lor i=0\end{cases}</annotation></semantics></math>
    $\mathbbm{1}(cnt_{i}<\alpha)$ |
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`cnt_{i}^{reg}=\begin{cases}cnt_{i-1}+1&&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;<\tau\\
    0&&#124;\bm{\hat{y}}_{i}-\bm{\hat{y}}_{i-1}&#124;\geq\tau\lor i=0\end{cases}`'
- en: '| Voting (Sun et al., [2021](#bib.bib310)) | The number of most predictions.
    | $V_{i}=\max_{c}\{\sum_{l=1}^{i}\mathbbm{1}(\arg\max(\bm{\hat{y}}_{i})=y_{c})\}/i^{\lambda}$
    $\mathbbm{1}(V_{i})<\alpha$ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 投票（Sun et al., [2021](#bib.bib310)） | 预测数量最多的分类。 | $V_{i}=\max_{c}\{\sum_{l=1}^{i}\mathbbm{1}(\arg\max(\bm{\hat{y}}_{i})=y_{c})\}/i^{\lambda}$
    $\mathbbm{1}(V_{i})<\alpha$ |'
- en: '| Learning-based Criterion |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 基于学习的标准 |'
- en: '| After-prediction (Bolukbasi et al., [2017](#bib.bib27)) (Wang et al., [2018c](#bib.bib348))
    (Schuster et al., [2021](#bib.bib283)) | Take the predicted probability distribution
    as input and generate the label deciding whether to execute the forward process.
    | $\text{MLP}(\bm{\hat{y}})$ |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 预测后（Bolukbasi et al., [2017](#bib.bib27)）（Wang et al., [2018c](#bib.bib348)）（Schuster
    et al., [2021](#bib.bib283)） | 以预测概率分布作为输入，生成决定是否执行前向过程的标签。 | $\text{MLP}(\bm{\hat{y}})$
    |'
- en: '| Before-prediction (Elbayad et al., [2020](#bib.bib74)) (Xin et al., [2021](#bib.bib364))
    | Take features as input and generate the label deciding whether to execute the
    forward process. | $\text{MLP}(\bm{h})$ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 预测前（Elbayad et al., [2020](#bib.bib74)）（Xin et al., [2021](#bib.bib364)）
    | 以特征作为输入，生成决定是否执行前向过程的标签。 | $\text{MLP}(\bm{h})$ |'
- en: Early-exiting-style dynamic networks might be the most popular dynamic architecture
    nowadays (Teerapittayanon et al., [2016](#bib.bib324); Bolukbasi et al., [2017](#bib.bib27);
    Gormez & Koyuncu, [2021](#bib.bib99); Huang et al., [2018](#bib.bib133); Yang
    et al., [2020b](#bib.bib373); Wang et al., [2021c](#bib.bib352)). With multiple
    internal classifiers on intermediate layers, a network is capable to give intermediate
    predictions and make decisions about whether to execute the forward process or
    not. If the answer is yes, current state would be fed to the next layer. Otherwise,
    the network outputs the intermediate prediction as the final prediction. Different
    from cascading architectures that cuts off the information flow between networks,
    early-exiting networks reuse the feature computed by previous layer. For example,
    BranchyNet (Teerapittayanon et al., [2016](#bib.bib324)) inserted several branch
    classifiers into a CNN to speedup inference. MSDNet (Huang et al., [2018](#bib.bib133))
    designed an exquisite two-dimensional multi-scale architecture to enable early
    exiting along two dimensions and RANet (Yang et al., [2020b](#bib.bib373)) further
    utilized the spatial redundancy for image classification.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 早期退出式动态网络可能是目前最流行的动态架构（Teerapittayanon et al., [2016](#bib.bib324); Bolukbasi
    et al., [2017](#bib.bib27); Gormez & Koyuncu, [2021](#bib.bib99); Huang et al.,
    [2018](#bib.bib133); Yang et al., [2020b](#bib.bib373); Wang et al., [2021c](#bib.bib352)）。通过在中间层上设置多个内部分类器，网络能够提供中间预测并决定是否执行前向过程。如果答案是肯定的，当前状态将被传递到下一层。否则，网络将输出中间预测作为最终预测。与切断网络间信息流的级联架构不同，早期退出网络重用先前层计算的特征。例如，BranchyNet（Teerapittayanon
    et al., [2016](#bib.bib324)）在CNN中插入了几个分支分类器以加快推理速度。MSDNet（Huang et al., [2018](#bib.bib133)）设计了一种精巧的二维多尺度架构，以便在两个维度上实现早期退出，RANet（Yang
    et al., [2020b](#bib.bib373)）进一步利用空间冗余进行图像分类。
- en: In addition to the CV field, existing studies also apply early-existing-style
    dynamic networks to the NLP field (Xin et al., [2020](#bib.bib363); Liu et al.,
    [2020a](#bib.bib200); Zhu, [2021](#bib.bib402)). The Two Stage fine-tuning is
    the most representative approach to train early-existing-style dynamic networks
    in NLP where the backbone is fine-tuned with the final classifier in the first
    stage, and the intermediate classifiers are fine-tuned in the second stage. In
    addition, joint training is also a trend to tune all parameters together including
    basic backbones and intermediate classifiers (Schwartz et al., [2020b](#bib.bib285);
    Liao et al., [2021](#bib.bib188); Geng et al., [2021](#bib.bib91)). In addition
    to training algorithms, recent researchers also focus on criterion design. For
    example,  Zhou et al. ([2020](#bib.bib399)) and Sun et al. ([2021](#bib.bib310))
    utilized counting-based criteria to support early exiting. Without relying on
    heuristic criteria, several approaches (Xin et al., [2021](#bib.bib364); Schuster
    et al., [2021](#bib.bib283)) directly learned the criteria by introducing a small
    module to decide whether to execute the forward process. Due to the simplicity
    of single-step prediction, dynamic networks are widely applied to classification
    models. Recently,  Elbayad et al. ([2020](#bib.bib74)) and  Li et al. ([2021](#bib.bib186))
    extended multi-exit design to translation tasks and sequence labeling tasks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算机视觉领域，现有研究还将早期存在的动态网络风格应用于自然语言处理领域（Xin et al., [2020](#bib.bib363); Liu
    et al., [2020a](#bib.bib200); Zhu, [2021](#bib.bib402)）。两阶段微调是训练早期存在的动态网络风格在自然语言处理中的最具代表性的方法，其中骨干网络在第一阶段与最终分类器一起微调，第二阶段则对中间分类器进行微调。此外，联合训练也是一种趋势，它对所有参数进行一起调优，包括基本骨干网络和中间分类器（Schwartz
    et al., [2020b](#bib.bib285); Liao et al., [2021](#bib.bib188); Geng et al., [2021](#bib.bib91)）。除了训练算法，最近的研究者也关注标准设计。例如，Zhou
    et al. ([2020](#bib.bib399)) 和 Sun et al. ([2021](#bib.bib310)) 采用了基于计数的标准来支持早期退出。几种方法（Xin
    et al., [2021](#bib.bib364); Schuster et al., [2021](#bib.bib283)）在不依赖启发式标准的情况下，通过引入小模块直接学习标准，以决定是否执行前向过程。由于单步预测的简单性，动态网络被广泛应用于分类模型。最近，Elbayad
    et al. ([2020](#bib.bib74)) 和 Li et al. ([2021](#bib.bib186)) 将多退出设计扩展到翻译任务和序列标注任务。
- en: Dynamic halting is a special case of early exiting, where the parameters across
    layers are shared and, therefore, the final classifier can also be shared. Specifically,
    these networks infer samples through a shared layer iteratively, rather than infer
    through multiple stacked individual layers. One representative network is proposed
    by Graves ([2016](#bib.bib101)). They proposed the adaptive computation time (ACT)
    mechanism for recurrent models to automatically decide how many times (iterations)
    each input symbol or token should be computed. Following this work, the ACT mechanism
    has been applied to various architectures, like ResNets and Transformers. For
    example, SACT (Figurnov et al., [2017](#bib.bib85)) performed dynamic halting
    in two dimensions, including the coarse ACT among multiple layers within the same
    block and the fine-grained ACT on all spatial positions. Universal Transformer (Dehghani
    et al., [2019](#bib.bib61)) shared all layers within the encoder (or decoder)
    in Transformer.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 动态停顿是早期退出的一个特例，其中跨层的参数是共享的，因此最终分类器也可以共享。具体而言，这些网络通过共享层迭代推断样本，而不是通过多个堆叠的单独层进行推断。一个代表性的网络是由Graves
    ([2016](#bib.bib101)) 提出的。他们为递归模型提出了自适应计算时间（ACT）机制，以自动决定每个输入符号或标记应该计算多少次（迭代）。继这一工作之后，ACT机制已被应用于各种架构，如ResNets和Transformers。例如，SACT（Figurnov
    et al., [2017](#bib.bib85)）在两个维度上执行动态停顿，包括同一块内多个层之间的粗略ACT和所有空间位置上的细粒度ACT。Universal
    Transformer（Dehghani et al., [2019](#bib.bib61)）在Transformer中共享了编码器（或解码器）中的所有层。
- en: Discussion
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 讨论
- en: In cascading-style or early-exiting style dynamic networks, the key question
    is to figure out how confident the intermediate classifier is. Previous studies
    proposed various criteria for judging the reliability of an intermediate prediction.
    We categorize them into types as shown in Table [2.1](#Ch2.T1 "Table 2.1 ‣ 2.2.3
    Dynamic Weight Sharing ‣ 2.2 Component Assembling ‣ Chapter 2 Compact Architecture
    ‣ A Survey on Green Deep Learning"). Score margin is the gap between the largest
    and the second largest scores in the predicted probability distribution (Park
    et al., [2015](#bib.bib236)). Entropy-based criterion is based on the entropy
    of the predicted probability distribution (Teerapittayanon et al., [2016](#bib.bib324);
    Li et al., [2021](#bib.bib186); Liu et al., [2020a](#bib.bib200)). The model executes
    the forward process only if the entropy is larger than the pre-defined threshold.
    Max-probability based criterion is the gap between the max value of the predicted
    probability distribution and the pre-defined threshold (Kaya et al., [2019](#bib.bib155);
    Wang et al., [2020d](#bib.bib351)). Patience-based criterion terminates the forward
    process only if the model generates continuously identical predictions (Zhou et al.,
    [2020](#bib.bib399)). Voting-based criterion is inspired by the ensemble technique,
    which terminates the forward process if the most of historic predictions reach
    an agreement (Sun et al., [2021](#bib.bib310)). After-prediction based criterion
    and before-prediction based criterion introduce additional learning functions
    to learn whether to execute the forward process. The only difference lies in that
    after-prediction based criterion uses the prediction distribution as inputs and
    before-prediction based criterion uses the naive hidden vector as inputs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在级联式或早期退出式动态网络中，关键问题是确定中间分类器的自信程度。以往的研究提出了各种标准来判断中间预测的可靠性。我们将这些标准分类，如表 [2.1](#Ch2.T1
    "Table 2.1 ‣ 2.2.3 Dynamic Weight Sharing ‣ 2.2 Component Assembling ‣ Chapter
    2 Compact Architecture ‣ A Survey on Green Deep Learning")所示。得分边际是预测概率分布中最大值与第二大值之间的差距（Park
    et al., [2015](#bib.bib236)）。基于熵的标准是基于预测概率分布的熵（Teerapittayanon et al., [2016](#bib.bib324);
    Li et al., [2021](#bib.bib186); Liu et al., [2020a](#bib.bib200)）。模型仅在熵大于预定义的阈值时执行前向过程。基于最大概率的标准是预测概率分布的最大值与预定义阈值之间的差距（Kaya
    et al., [2019](#bib.bib155); Wang et al., [2020d](#bib.bib351)）。基于耐心的标准仅在模型生成连续相同的预测时终止前向过程（Zhou
    et al., [2020](#bib.bib399)）。基于投票的标准受集成技术的启发，如果大多数历史预测达成一致，则终止前向过程（Sun et al.,
    [2021](#bib.bib310)）。基于预测后的标准和基于预测前的标准引入了额外的学习函数来学习是否执行前向过程。唯一的区别在于，基于预测后的标准使用预测分布作为输入，而基于预测前的标准使用原始隐藏向量作为输入。
- en: 'Skipping-style dynamic networks skip some computations during forward process.
    These dynamic networks are capable to obtain higher efficiency. This dynamic solution
    has been widely-use in various models, like SkipNet (Wang et al., [2018d](#bib.bib349)),
    ConvNet-AIG (Veit & Belongie, [2020](#bib.bib331)), and BlockDrop (Wu et al.,
    [2018b](#bib.bib361)). They introduced additional policy networks responsible
    for deciding to skip certain layers or not. The main formula for those dynamic
    networks could be summarized as:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃式动态网络在前向过程中跳过一些计算。这些动态网络能够获得更高的效率。这种动态解决方案已广泛应用于各种模型中，如 SkipNet（Wang et al.,
    [2018d](#bib.bib349)）、ConvNet-AIG（Veit & Belongie, [2020](#bib.bib331)）和 BlockDrop（Wu
    et al., [2018b](#bib.bib361)）。它们引入了额外的策略网络，负责决定是否跳过某些层。这些动态网络的主要公式可以总结如下：
- en: '|  | SkipNet: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+(1-z_{l})\*\bm{x}_{l}$
    |  | (2.4) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | SkipNet: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+(1-z_{l})\*\bm{x}_{l}$
    |  | (2.4) |'
- en: '|  | ConvNet-AIG: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+\bm{x}_{l}$
    |  | (2.5) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | ConvNet-AIG: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+\bm{x}_{l}$
    |  | (2.5) |'
- en: '|  | BlockDrop: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+\bm{x}_{l}$
    |  | (2.6) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | BlockDrop: | $\displaystyle\quad\bm{x}_{l+1}=z_{l}\*F_{l}(\bm{x}_{l})+\bm{x}_{l}$
    |  | (2.6) |'
- en: where $\bm{x}_{l}$ is the input of the $l$-th residual unit, $F_{l}(\cdot)$
    is the network layers within the $l$-th residual unit except skip connection,
    and $z_{l}\in\{0,1\}$ is a binary value predicted by the policy network or the
    $l$-th policy module. By utilizing reinforcement learning or the Gumbel re-parameterization
    trick, the network can be trainable in an end-to-end way.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{x}_{l}$ 是第 $l$ 个残差单元的输入，$F_{l}(\cdot)$ 是第 $l$ 个残差单元中除跳跃连接外的网络层，而 $z_{l}\in\{0,1\}$
    是由策略网络或第 $l$ 个策略模块预测的二进制值。通过利用强化学习或 Gumbel 再参数化技巧，网络可以以端到端的方式进行训练。
- en: Another research skipping-style line chooses to skip inputs given a long input
    sequence (Yu et al., [2017](#bib.bib381); Campos et al., [2018](#bib.bib32); Yu
    et al., [2018](#bib.bib384)) or assign fewer computations to unimportant steps (Jernite
    et al., [2017](#bib.bib145); Seo et al., [2018](#bib.bib290)) or exit reading (Yu
    et al., [2017](#bib.bib381); Liu et al., [2018c](#bib.bib201); Yu et al., [2018](#bib.bib384)).
    1) Skipping unimportant inputs is the natural way.  Campos et al. ([2018](#bib.bib32))
    introduced Skip-RNN where a binary gate unit was used to learn to skip current
    input token or not. If the answer is yes, Skip-RNN copies current hidden state
    to the next time step, saving computations on those unimportant inputs. LSTM-Jump (Yu
    et al., [2017](#bib.bib381)) achieved the same goal by directly predicting how
    many steps to jump through, or whether to exit reading inputs. Although skipping
    partial inputs saves computations largely, these models, like Skip-RNN and LSTM-Jump,
    suffer from missing or repeating outputs at skipped positions thus are not suitable
    for token-level tasks. 2) To address this problem, assigning fewer computations
    to unimportant steps is a flexible solution. To this end,  Seo et al. ([2018](#bib.bib290))
    proposed Skim-RNN that dynamically decided to update the full-sized hidden state
    or partial-sized hidden state at each time step. 3) Exiting-style reading is a
    special kind of skipping reading (Shen et al., [2017](#bib.bib292); Yu et al.,
    [2018](#bib.bib384); Liu et al., [2018c](#bib.bib201)). It decides to truncate
    the next inputs. For example,  Liu et al. ([2018c](#bib.bib201)) applied the exit
    mechanism to multi-task scenario. ReasoNet (Shen et al., [2017](#bib.bib292))
    adopted the exit mechanism for machine comprehension tasks. Despite good trade-off
    between accuracy and inference speed, skipping-style dynamic networks are harder
    to train, introducing more tuning overhead.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种研究跳跃风格的方法选择在给定长输入序列时跳过输入 (Yu et al., [2017](#bib.bib381); Campos et al.,
    [2018](#bib.bib32); Yu et al., [2018](#bib.bib384))，或者将较少的计算分配给不重要的步骤 (Jernite
    et al., [2017](#bib.bib145); Seo et al., [2018](#bib.bib290))，或退出读取 (Yu et al.,
    [2017](#bib.bib381); Liu et al., [2018c](#bib.bib201); Yu et al., [2018](#bib.bib384))。1)
    跳过不重要的输入是一种自然的方式。 Campos et al. ([2018](#bib.bib32)) 提出了 Skip-RNN，在其中使用了一个二进制门单元来学习是否跳过当前输入标记。如果答案是肯定的，Skip-RNN
    会将当前的隐藏状态复制到下一个时间步骤，从而节省对这些不重要输入的计算。LSTM-Jump (Yu et al., [2017](#bib.bib381))
    通过直接预测要跳过多少步或是否退出读取输入来实现相同的目标。尽管跳过部分输入大大节省了计算，但这些模型，如 Skip-RNN 和 LSTM-Jump，由于在跳过的位置缺失或重复输出，因此不适合处理标记级任务。2)
    为了解决这个问题，将较少的计算分配给不重要的步骤是一种灵活的解决方案。为此，Seo et al. ([2018](#bib.bib290)) 提出了 Skim-RNN，该模型动态决定在每个时间步骤更新全尺寸隐藏状态或部分尺寸隐藏状态。3)
    退出式读取是一种特殊的跳跃读取 (Shen et al., [2017](#bib.bib292); Yu et al., [2018](#bib.bib384);
    Liu et al., [2018c](#bib.bib201))。它决定截断下一个输入。例如，Liu et al. ([2018c](#bib.bib201))
    将退出机制应用于多任务场景。ReasoNet (Shen et al., [2017](#bib.bib292)) 将退出机制用于机器理解任务。尽管在准确性和推理速度之间有良好的折衷，跳跃风格的动态网络训练起来更困难，带来了更多的调优开销。
- en: Mixture-of-experts-style dynamic networks are representative dynamic models (Lepikhin
    et al., [2021](#bib.bib177); Lin et al., [2021](#bib.bib191); Fedus et al., [2021](#bib.bib82)).
    In those models, a layer contains multiple experts and only part of these experts
    will be activated for each instance. For example, Switch Transformer (Fedus et al.,
    [2021](#bib.bib82)) is the representative model that has trillion-level parameters.
    It replaces the normal feed-forward layer in the Transformer with a switch feed-forward
    layer, consisting of a routing module and multiple structure-identical experts.
    In each switch layer, only a single expert will be executed for each token. Compared
    to general dense computation architectures, mixture-of-expert-style networks provide
    an affordable and practical way to modify and train large models with sparse activation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合式动态网络是具有代表性的动态模型（Lepikhin et al., [2021](#bib.bib177); Lin et al., [2021](#bib.bib191);
    Fedus et al., [2021](#bib.bib82)）。在这些模型中，一层包含多个专家，并且每个实例只会激活其中一部分专家。例如，Switch
    Transformer（Fedus et al., [2021](#bib.bib82)）是一个具有万亿级参数的代表性模型。它将 Transformer 中的普通前馈层替换为一个开关前馈层，由一个路由模块和多个结构相同的专家组成。在每个开关层中，每个
    token 只会执行一个专家。与一般的密集计算架构相比，专家混合式网络提供了一种经济且实用的方式来修改和训练具有稀疏激活的大型模型。
- en: 2.2.4 Deployment Sharing
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 部署共享
- en: When deploying deep learning models on edge devices, we have to consider realistic
    constraints, such as storage, memory, computation, latency, and power consumption.
    Previous researchers have designed lightweight and compact models for mobile devices
    or other edge devices, such as MobileNets (Howard et al., [2017](#bib.bib132);
    Sandler et al., [2018](#bib.bib275); Howard et al., [2019](#bib.bib131)). However,
    with different hardware resources, the optimal neural network architecture varies
    significantly (Cai et al., [2020](#bib.bib31)). Thus, developing elastic or dynamic
    models to satisfy different constraints is critical for practical applications.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘设备上部署深度学习模型时，我们必须考虑实际约束，例如存储、内存、计算、延迟和功耗。以前的研究者为移动设备或其他边缘设备设计了轻量级和紧凑的模型，例如
    MobileNets（Howard et al., [2017](#bib.bib132); Sandler et al., [2018](#bib.bib275);
    Howard et al., [2019](#bib.bib131)）。然而，由于硬件资源不同，最佳的神经网络架构差异显著（Cai et al., [2020](#bib.bib31)）。因此，开发弹性或动态模型以满足不同的约束对于实际应用至关重要。
- en: In the recent two years, some studies have paid attention to efficient deployment.
    In these studies, a super-network is trained together with its massive sub-networks
    by task-specific losses. During inference, the appropriate sub-network is selected
    to satisfy the resource constraints. By amortizing the only-once training cost,
    the total cost of specialized designing is reduced from O(N) to O(1). During inference,
    the model can dynamically choose an appropriate network for different devices.
    To be specific, Yu et al. ([2019](#bib.bib383)) proposed slimmable neural networks
    where several widths are predefined, supporting instant and adaptive accuracy-efficiency
    trade-offs by selecting corresponding width. Following this work, Yu & Huang ([2019](#bib.bib382))
    further proposed US-Nets to support arbitrary width selection. Fan et al. ([2020b](#bib.bib78))
    proposed an elastic network that can select sub-networks of any depth from one
    large network without having to finetune them. Beyond aforementioned studies,
    the temporal or input length is also an elastic selection. For example, Kim &
    Cho ([2021](#bib.bib156)) proposed length-adaptive Transformer to support arbitrary
    progressively length deduction. The Length-adaptive Transformer can be directly
    adopted into the downstream task and satisfy any efficiency constraints by searching
    the corresponding length deduction configurations.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近两年，一些研究关注于高效的部署。在这些研究中，通过任务特定的损失一起训练一个超级网络及其大量的子网络。在推理过程中，选择合适的子网络以满足资源约束。通过摊销一次性训练成本，专业化设计的总成本从
    O(N) 降低到 O(1)。在推理过程中，模型可以动态选择适合不同设备的网络。具体来说，Yu et al. ([2019](#bib.bib383)) 提出了可缩放神经网络，其中预定义了多个宽度，通过选择相应的宽度支持即时和自适应的准确性-效率权衡。在此基础上，Yu
    & Huang ([2019](#bib.bib382)) 进一步提出了 US-Nets，以支持任意宽度选择。Fan et al. ([2020b](#bib.bib78))
    提出了一个弹性网络，该网络可以从一个大型网络中选择任何深度的子网络，而无需对其进行微调。除了上述研究之外，时间或输入长度也是一种弹性选择。例如，Kim &
    Cho ([2021](#bib.bib156)) 提出了长度自适应 Transformer，以支持任意的逐步长度缩减。长度自适应 Transformer
    可以直接用于下游任务，并通过搜索相应的长度缩减配置来满足任何效率约束。
- en: 2.3 Compact-architecture Search
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 紧凑架构搜索
- en: In addition to model design, there are studies working on search efficient networks
    towards resource-constraint devices, like mobile. They borrow the idea of neural
    architecture search and apply it to design tiny networks. For example,  Tan et al.
    ([2019a](#bib.bib320)) proposed a neural architecture search approach, which explicitly
    incorporated model latency into the main objective so that the search can identify
    a model with a good trade-off between accuracy and latency. On the ImageNet classification
    task, this approach achieved 75.2% top-1 accuracy with 1.8x faster than MobileNet
    V2\.  Howard et al. ([2019](#bib.bib131)) combined neural architecture search
    and network design together to develop a stronger mobile net MobileNet V3\.  Cai
    et al. ([2019](#bib.bib30)) directly learned the architectures on the target task
    and hardware.  Wu et al. ([2019](#bib.bib357)) proposed a differentiable neural
    architecture search framework that used gradient-based methods to optimize ConvNet
    architectures towards mobile devices.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型设计，还有研究致力于为资源受限的设备（如移动设备）设计高效网络。他们借鉴了神经架构搜索的思想，并将其应用于设计小型网络。例如，Tan等人（[2019a](#bib.bib320)）提出了一种神经架构搜索方法，该方法将模型延迟明确地纳入主要目标，从而使搜索能够识别在准确性和延迟之间具有良好权衡的模型。在ImageNet分类任务中，该方法在比MobileNet
    V2快1.8倍的情况下达到了75.2%的Top-1准确率。Howard等人（[2019](#bib.bib131)）将神经架构搜索与网络设计结合起来，开发了更强大的移动网络MobileNet
    V3。Cai等人（[2019](#bib.bib30)）直接在目标任务和硬件上学习架构。Wu等人（[2019](#bib.bib357)）提出了一个可微分的神经架构搜索框架，利用基于梯度的方法优化针对移动设备的ConvNet架构。
- en: Chapter 3 Energy-Efficient Training
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章 能效训练
- en: Many advanced approaches have been proposed to reduce training costs for deep
    learning. In Chapter 2, we describe efficient networks that can reduce computations
    in a single execution. In this chapter, we focus on the computations required
    during the whole training, including weight tuning and hyper-parameter tuning.
    To be specific, we survey approaches that aim to accelerate weight tuning/hyper-parameter
    tuning by using fewer iterations, including initialization, normalization, progressive
    training, and efficient NAS. An overview is shown in Figure [3.1](#Ch3.F1 "Figure
    3.1 ‣ Chapter 3 Energy-Efficient Training ‣ A Survey on Green Deep Learning").
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 许多先进的方法已经被提出，用于减少深度学习的训练成本。在第2章中，我们描述了可以在单次执行中减少计算的高效网络。在本章中，我们重点关注整个训练过程中所需的计算，包括权重调整和超参数调整。具体而言，我们调查了旨在通过使用更少迭代来加速权重调整/超参数调整的方法，包括初始化、归一化、渐进训练和高效的NAS。概述见图 [3.1](#Ch3.F1
    "Figure 3.1 ‣ Chapter 3 Energy-Efficient Training ‣ A Survey on Green Deep Learning")。
- en: '{forest}'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,
    align=left, minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt,
    inner ysep=1pt, , where level=1text width=6em, where level=2text width=7em,font=,
    where level=3font=, where level=4font=, where level=5font=, [Energy-efficient
    Training [Initialization [Random Initialization [Kaiming Initialization] [Xaiver
    Initialization] [Fixup Initialization] [LSUV Initialization] ] [Pre-trained Models
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,
    align=left, minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt,
    inner ysep=1pt, , where level=1text width=6em, where level=2text width=7em,font=,
    where level=3font=, where level=4font=, where level=5font=, [能效训练 [初始化 [随机初始化
    [Kaiming初始化] [Xaiver初始化] [Fixup初始化] [LSUV初始化] ] [预训练模型
- en: for Initialization [Feature based Initialization] [Fine-tuning based Initialization]
    [Supervised Initialization] [Self-supervised Initialization] ] ] [Normalization
    [Batch Normalization] [Layer Normalization] [Group Normalization] ] [Progressive
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初始化 [基于特征的初始化] [基于微调的初始化] [监督初始化] [自监督初始化] ] ] [归一化 [批量归一化] [层归一化] [组归一化]
    ] [渐进
- en: Training ] [Efficient AutoML [Search Space [Continuous] [Discrete] [Cell block]
    [Meta-architecture] ] [Search Method [RL-based Search] [Evolution-based Search]
    [Differentiable Search] ] [Evaluation Method [Early Stop] [Weight Sharing] [Hypernetworks]
    ] ] ]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 ] [高效AutoML [搜索空间 [连续] [离散] [单元块] [元架构] ] [搜索方法 [基于RL的搜索] [基于进化的搜索] [可微分搜索]
    ] [评估方法 [早停] [权重共享] [超网络] ] ] ]
- en: 'Figure 3.1: Taxonomy of energy-efficient training with representative examples.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：能效训练的分类及代表性示例。
- en: 3.1 Initialization
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初始化
- en: The training of deep learning starts from architecture design and parameter
    initialization. We have explored efficient architecture design in Section 2\.
    In this part, we focus on how weight initialization affects model training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的训练从架构设计和参数初始化开始。我们在第2节探讨了高效的架构设计。在这一部分，我们重点关注权重初始化如何影响模型训练。
- en: 'Table 3.1: Summarization of two common initialization approaches. $d$ and $u$
    mean the dimensions of weight matrix $W$. $Uniform$ and $Normal$ mean the uniform
    distribution and Gassuian distribution.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1：两种常见初始化方法的总结。$d$ 和 $u$ 代表权重矩阵 $W$ 的维度。$Uniform$ 和 $Normal$ 分别表示均匀分布和高斯分布。
- en: '| Initialization Approach | Description | Formulation |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 初始化方法 | 描述 | 公式 |'
- en: '| Kaiming initialization (He et al., [2015](#bib.bib117)) | Distribution of
    standard deviation of $\sqrt{\frac{2}{d}}$ | $W\sim Normal(0,\frac{2}{d})$ or
    $W\sim Uniform(-\sqrt{\frac{6}{d}},\sqrt{\frac{6}{d}})$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Kaiming 初始化（He 等， [2015](#bib.bib117)） | 标准差的分布为 $\sqrt{\frac{2}{d}}$ | $W\sim
    Normal(0,\frac{2}{d})$ 或 $W\sim Uniform(-\sqrt{\frac{6}{d}},\sqrt{\frac{6}{d}})$
    |'
- en: '| Xaiver initialization (Glorot & Bengio, [2010](#bib.bib95)) | Distribution
    of standard deviation of $\sqrt{\frac{2}{d+u}}$ | $W\sim Normal(0,\frac{2}{d+u})$
    or $W\sim Uniform(-\sqrt{\frac{6}{d+u}},\sqrt{\frac{6}{d+u}})$ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Xaiver 初始化（Glorot & Bengio，[2010](#bib.bib95)） | 标准差的分布为 $\sqrt{\frac{2}{d+u}}$
    | $W\sim Normal(0,\frac{2}{d+u})$ 或 $W\sim Uniform(-\sqrt{\frac{6}{d+u}},\sqrt{\frac{6}{d+u}})$
    |'
- en: 3.1.1 Random Initialization
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 随机初始化
- en: It is widely accepted that good initialization of weights in a neural network
    is critical to convergence (Glorot & Bengio, [2010](#bib.bib95); Krizhevsky et al.,
    [2012](#bib.bib164); He et al., [2015](#bib.bib117); Mishkin & Matas, [2016](#bib.bib223);
    Kumar, [2017](#bib.bib166)). At the beginning, deep networks are usually initialized
    via random weights drawn from uniform distributions or Gaussian distributions.
    Many previous studies found that these kinds of initialization failed in handling
    very deep models (Glorot & Bengio, [2010](#bib.bib95); Saxe et al., [2014](#bib.bib279);
    Romero et al., [2015](#bib.bib266); Hanin & Rolnick, [2018](#bib.bib113)). The
    problem is caused that the mean/variance of activations and gradients exponentially
    with the depth. To enable training with a very deep model, some advanced initialization
    solutions, like Kaiming initialization (He et al., [2015](#bib.bib117)), Xaiver
    initialization (Glorot & Bengio, [2010](#bib.bib95)), LSUV initialization (Mishkin
    & Matas, [2016](#bib.bib223)) and Fixup initialization (Zhang et al., [2019](#bib.bib391)),
    have been proposed. The key idea is to normalize the variance of weights to make
    the variance of activation in each layer to be around 1\. We list the details
    of two widely-used initialization approaches in Table [3.1](#Ch3.T1 "Table 3.1
    ‣ 3.1 Initialization ‣ Chapter 3 Energy-Efficient Training ‣ A Survey on Green
    Deep Learning"). Compared to the naive baseline, these initialization approaches
    can achieve better performance and faster convergence (Mishkin & Matas, [2016](#bib.bib223)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛接受的观点是，神经网络中良好的权重初始化对收敛至关重要（Glorot & Bengio，[2010](#bib.bib95)；Krizhevsky
    等，[2012](#bib.bib164)；He 等，[2015](#bib.bib117)；Mishkin & Matas，[2016](#bib.bib223)；Kumar，[2017](#bib.bib166)）。在开始时，深度网络通常通过从均匀分布或高斯分布中抽取的随机权重进行初始化。许多先前的研究发现，这些初始化方法在处理非常深的模型时会失败（Glorot
    & Bengio，[2010](#bib.bib95)；Saxe 等，[2014](#bib.bib279)；Romero 等，[2015](#bib.bib266)；Hanin
    & Rolnick，[2018](#bib.bib113)）。问题在于激活值和梯度的均值/方差随着深度指数增长。为了能够训练非常深的模型，一些先进的初始化解决方案，如
    Kaiming 初始化（He 等，[2015](#bib.bib117)）、Xaiver 初始化（Glorot & Bengio，[2010](#bib.bib95)）、LSUV
    初始化（Mishkin & Matas，[2016](#bib.bib223)）和 Fixup 初始化（Zhang 等，[2019](#bib.bib391)），已经被提出。关键思想是规范化权重的方差，使每一层的激活方差保持在
    1 左右。我们在表[3.1](#Ch3.T1 "Table 3.1 ‣ 3.1 Initialization ‣ Chapter 3 Energy-Efficient
    Training ‣ A Survey on Green Deep Learning")中列出了两种广泛使用的初始化方法的详细信息。与简单基线相比，这些初始化方法能够实现更好的性能和更快的收敛（Mishkin
    & Matas，[2016](#bib.bib223)）。
- en: 3.1.2 Pre-trained Models for Initialization
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 用于初始化的预训练模型
- en: In addition to random initialization, many approaches borrow models pre-trained
    from other domains (or other tasks) as initialization. It is widely believed that
    initialization from existing models is an effective technique to improve the generalization
    ability with fewer training iterations. We split these pre-training initialization
    into different categories according to different dimensions. First, according
    to whether the borrowed parameters keep unchanged, these methods can be classified
    into feature-based initialization, and fine-tuning-based initialization. Second,
    according to the knowledge source of pre-trained parameters, these methods can
    be classified into supervised initialization and self-supervised initialization.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 除了随机初始化，许多方法借用从其他领域（或其他任务）预训练的模型作为初始化。普遍认为，从现有模型中初始化是一种有效的技术，可以在更少的训练迭代下提高泛化能力。我们根据不同的维度将这些预训练初始化方法分为不同的类别。首先，根据借用的参数是否保持不变，这些方法可以分为基于特征的初始化和基于微调的初始化。其次，根据预训练参数的知识来源，这些方法可以分为监督初始化和自监督初始化。
- en: Feature-based initialization borrows the parameters (usually from low-layers
    or mid-layers) as initialization from other domains/tasks while these parameters
    keep fixed during training. Generally speaking, feature based initialization can
    keep the generalization ability of the borrowed parameters better and thus is
    more suitable for extremely few-shot settings.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的初始化借用来自其他领域/任务的参数（通常来自低层或中层）作为初始化，同时在训练过程中这些参数保持固定。一般来说，基于特征的初始化可以更好地保持借用参数的泛化能力，因此更适合极少样本的设置。
- en: Fine-tuning-based initialization uses the target data to train all parameters,
    including new parameters and borrowed parameters. Fine-tuning based initialization
    can further optimize the target objectives via fine-tuning all parameters and
    thus can better fit training data. It is the most popular solution nowadays for
    the NLP field.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 基于微调的初始化使用目标数据来训练所有参数，包括新参数和借用的参数。基于微调的初始化可以通过微调所有参数进一步优化目标任务，因此可以更好地拟合训练数据。这是目前在自然语言处理领域最受欢迎的解决方案。
- en: Supervised initialization is widely investigated in the earlier stage of deep
    learning. A common solution is to pre-train the target model on similar tasks/datasets,
    and then reuse the pre-trained parameters as initialization for the target task (Huang
    et al., [2013](#bib.bib134); Oquab et al., [2014](#bib.bib233); Yosinski et al.,
    [2014](#bib.bib379); Duong et al., [2015](#bib.bib73); Long et al., [2016](#bib.bib206)).
    This solution is especially popular for low-resource settings and is extensively
    studied on domain adaptation/transfer learning.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 监督初始化在深度学习的早期阶段被广泛研究。一种常见的解决方案是先在类似任务/数据集上对目标模型进行预训练，然后将预训练的参数作为目标任务的初始化（Huang
    et al., [2013](#bib.bib134); Oquab et al., [2014](#bib.bib233); Yosinski et al.,
    [2014](#bib.bib379); Duong et al., [2015](#bib.bib73); Long et al., [2016](#bib.bib206)）。这种解决方案在资源稀缺的环境中尤其受欢迎，并且在领域适应/迁移学习方面进行了广泛研究。
- en: The most representative example for supervised initialization is the pre-training
    of deep CNN backbones (Simonyan & Zisserman, [2015](#bib.bib299); Ren et al.,
    [2015](#bib.bib264); He et al., [2016a](#bib.bib118); Simon et al., [2016](#bib.bib298);
    He et al., [2017a](#bib.bib119); Iglovikov & Shvets, [2018](#bib.bib140)). Fine-tuning
    pre-trained CNNs on different downstream datasets usually leads to improved performance
    compared to training from scratch and also reduces the number of training steps.
    Previous researches have explored advanced initialization methods. It is widely
    accepted that layers near to inputs usually are responsible to capture local features (Zeiler
    & Fergus, [2014](#bib.bib389)). Therefore, many studies focus on transferring
    knowledge via initialization from low-layer features and mid-layer features. Interestingly,
    with the increase of large-scale training data, current trends directly adopt
    the simplest solution that uses all parameters for initialization (Li et al.,
    [2020a](#bib.bib181)).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 监督初始化的最具代表性的例子是深度CNN主干网络的预训练（Simonyan & Zisserman, [2015](#bib.bib299); Ren
    et al., [2015](#bib.bib264); He et al., [2016a](#bib.bib118); Simon et al., [2016](#bib.bib298);
    He et al., [2017a](#bib.bib119); Iglovikov & Shvets, [2018](#bib.bib140)）。在不同下游数据集上微调预训练的CNN通常会比从头训练带来更好的性能，并且减少了训练步骤的数量。之前的研究探讨了先进的初始化方法。普遍接受的观点是，靠近输入的层通常负责捕捉局部特征（Zeiler
    & Fergus, [2014](#bib.bib389)）。因此，许多研究专注于通过初始化从低层特征和中层特征传递知识。有趣的是，随着大规模训练数据的增加，目前的趋势直接采用使用所有参数进行初始化的最简单方案（Li
    et al., [2020a](#bib.bib181)）。
- en: Supervised initialization is also successfully applied to NLP (Socher et al.,
    [2013](#bib.bib301)). It is widely-accepted that features computed in higher layers
    of the network usually depend on the specific dataset and task. Following this
    belief, many studies borrow the parameters of low-level layers and mid-level layers
    from other domains as initialization (Dong et al., [2015](#bib.bib67); Luong et al.,
    [2016](#bib.bib211); Yang et al., [2017](#bib.bib374); Lin et al., [2018](#bib.bib193);
    Liu et al., [2018b](#bib.bib198)). Recently, the trend of large-scale networks
    enables researchers to reuse all parameters from pre-trained networks. Similar
    with CV, the widely-adopt setting in NLP is directly reusing all parameters (Johnson
    et al., [2017](#bib.bib149); Aharoni et al., [2019](#bib.bib3); Tan et al., [2019b](#bib.bib321);
    Bapna & Firat, [2019](#bib.bib15); Lin et al., [2020](#bib.bib194)).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 监督初始化在自然语言处理（Socher et al., [2013](#bib.bib301)）中也取得了成功应用。普遍接受的观点是，网络中较高层计算的特征通常依赖于特定的数据集和任务。基于这一信念，许多研究借用其他领域的低层和中层参数作为初始化（Dong
    et al., [2015](#bib.bib67); Luong et al., [2016](#bib.bib211); Yang et al., [2017](#bib.bib374);
    Lin et al., [2018](#bib.bib193); Liu et al., [2018b](#bib.bib198)）。最近，大规模网络的趋势使得研究人员能够重用所有来自预训练网络的参数。与计算机视觉类似，自然语言处理中的广泛采用设置是直接重用所有参数（Johnson
    et al., [2017](#bib.bib149); Aharoni et al., [2019](#bib.bib3); Tan et al., [2019b](#bib.bib321);
    Bapna & Firat, [2019](#bib.bib15); Lin et al., [2020](#bib.bib194)）。
- en: Self-supervised initialization is also a popular direction. With the increasing
    parameters in state-of-the-art DNNs, more and more training data are required
    to achieve better generalization results. To reduce the requirements of supervised
    data, previous studies investigated self-supervised pre-training that exploited
    unlabeled data to construct supervision signals to learn representations. Since
    self-supervised pre-training does not require any human-annotated labels, it is
    easy to get sufficient training data. To this end, researchers designed various
    methods to construct self-supervised training signals with unlabeled data. Here
    we take CV and NLP as examples to review recent self-supervised pre-training models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督初始化也是一个热门方向。随着最先进的深度神经网络中参数的增加，需要更多的训练数据以实现更好的泛化结果。为了减少对监督数据的需求，以往的研究探讨了自监督预训练，它利用未标记的数据构建监督信号来学习表示。由于自监督预训练不需要任何人工标注的标签，因此很容易获得足够的训练数据。为此，研究人员设计了各种方法来使用未标记数据构建自监督训练信号。这里我们以计算机视觉和自然语言处理为例，回顾最近的自监督预训练模型。
- en: For the NLP fields, using a model pre-trained on self-supervised data as initialization
    is the most popular solution. At the start, researchers use language modeling
    to pre-train word embeddings which are then used to initialize downstream word
    embeddings (Joshi et al., [2016](#bib.bib150); Qi et al., [2018](#bib.bib252);
    Ruder et al., [2019](#bib.bib270)). Glove is one widely-used word embedding toolkit (Pennington
    et al., [2014](#bib.bib239)) which trains word embeddings based on global word-word
    co-occurrence counts. With the development of representation learning, researchers
    begin to explore and reuse contextualized models. Contextualized models define
    that the representation of a word depends on its contexts and each word has two
    representations, fixed word embeddings and contextualized representations.  Peters
    et al. ([2018](#bib.bib241)) proposed the first widely-used contextualized representations,
    ELMo. Following this work, many advanced contextualized representation models
    begin to spring up, like BERT (Devlin et al., [2019](#bib.bib65)), GPT (Schick
    & Schütze, [2021](#bib.bib281)), T5 (Raffel et al., [2020](#bib.bib255)). The
    development of pre-trained networks also affect the application of CV. In recent
    years, CV began to explore large-scale self-supervised models for initialization (Lu
    et al., [2019](#bib.bib207); Li et al., [2020a](#bib.bib181); Chen et al., [2020b](#bib.bib36)).
    The learning objective is similar with NLP’s pre-trained networks, either recovering
    masked/noised regions, or generating the original image from scratch.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NLP领域，使用在自监督数据上预训练的模型作为初始化是最受欢迎的解决方案。一开始，研究人员使用语言建模来预训练词嵌入，然后用这些词嵌入来初始化下游词嵌入（Joshi
    et al., [2016](#bib.bib150); Qi et al., [2018](#bib.bib252); Ruder et al., [2019](#bib.bib270)）。Glove是一个广泛使用的词嵌入工具包（Pennington
    et al., [2014](#bib.bib239)），它基于全局词对词共现计数训练词嵌入。随着表示学习的发展，研究人员开始探索和重用上下文化模型。上下文化模型定义了一个词的表示依赖于其上下文，每个词有两个表示：固定的词嵌入和上下文化的表示。Peters
    et al. ([2018](#bib.bib241)) 提出了第一个广泛使用的上下文化表示，ELMo。继这项工作之后，许多先进的上下文化表示模型相继出现，如BERT（Devlin
    et al., [2019](#bib.bib65)）、GPT（Schick & Schütze, [2021](#bib.bib281)）、T5（Raffel
    et al., [2020](#bib.bib255)）。预训练网络的发展也影响了CV的应用。近年来，CV开始探索大规模自监督模型作为初始化（Lu et al.,
    [2019](#bib.bib207); Li et al., [2020a](#bib.bib181); Chen et al., [2020b](#bib.bib36)）。学习目标与NLP的预训练网络类似，要么恢复遮挡/噪声区域，要么从头生成原始图像。
- en: Empirical results demonstrate that these pre-trained networks for initialization
    can achieve better performance and faster convergence. However, since current
    pre-trained networks generally require downstream tasks to use the exactly same
    networks, the training time still depends on architecture execution in addition
    to convergence speed. Therefore, it should be considered case by case to conclude
    whether pre-trained models for initialization reduce downstream training costs
    in implementation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 实证结果表明，这些用于初始化的预训练网络可以实现更好的性能和更快的收敛。然而，由于当前的预训练网络通常要求下游任务使用完全相同的网络，因此训练时间仍然依赖于架构执行，除了收敛速度。因此，应根据具体情况考虑是否预训练模型在实现中降低了下游训练成本。
- en: 3.2 Normalization
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 归一化
- en: In addition to initialization approaches, normalization is another solution
    to accelerate training. Strictly speaking, normalization is a special component.
    Considering that it can accelerate convergence (Bjorck et al., [2018](#bib.bib25);
    Santurkar et al., [2018](#bib.bib277); Zhang et al., [2019](#bib.bib391)), we
    describe normalization in this chapter.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 除了初始化方法外，归一化是加速训练的另一种解决方案。严格来说，归一化是一种特殊的组件。考虑到它可以加速收敛（Bjorck et al., [2018](#bib.bib25);
    Santurkar et al., [2018](#bib.bib277); Zhang et al., [2019](#bib.bib391)），我们在本章中描述归一化。
- en: 'Normalization is a technique to normalize hidden outputs in deep neural networks.
    Batch normalization (Ioffe & Szegedy, [2015](#bib.bib141)) is the first widely-used
    normalization for deep models. The key idea is to normalize the hidden vectors
    of neural networks to the distribution with mean $\mu=0$ and standard deviation
    $\sigma=1$. The hidden vectors usually are tensors and batch normalization is
    applied on the batch dimension. To be specific, it generates the output given
    the hidden output $h$:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化是一种在深度神经网络中归一化隐藏输出的技术。批量归一化（Ioffe & Szegedy, [2015](#bib.bib141)）是第一个广泛使用的深度模型归一化方法。其核心思想是将神经网络的隐藏向量归一化为均值为$\mu=0$和标准差为$\sigma=1$的分布。隐藏向量通常是张量，批量归一化应用于批量维度。具体而言，它根据隐藏输出$h$生成输出：
- en: '|  | $y_{i}=\frac{y_{i}-\mu}{\sigma+\epsilon},\mu=\frac{1}{&#124;B&#124;}\sum_{i=1}^{&#124;B&#124;}h_{b,i},\sigma=\frac{1}{&#124;B&#124;}\sum_{i=1}^{&#124;B&#124;}\sqrt{(h_{b,i}-\mu)^{2}}$
    |  | (3.1) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{i}=\frac{y_{i}-\mu}{\sigma+\epsilon},\mu=\frac{1}{|B|}\sum_{i=1}^{|B|}h_{b,i},\sigma=\frac{1}{|B|}\sum_{i=1}^{|B|}\sqrt{(h_{b,i}-\mu)^{2}}$
    |  | (3.1) |'
- en: where $h$ is a intermediate tensor where the first is batch dimension. $|B|$
    is the batch size. $y$ and $h$ are the output and input of the normalization component.
     Ioffe & Szegedy ([2015](#bib.bib141)) fond that applied to a state-of-the-art
    image classification model, batch normalization achieved the same accuracy with
    14 times fewer training steps, and beat the original model by a significant margin.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h$ 是一个中间张量，其中第一个维度是批次维度。$|B|$ 是批次大小。$y$ 和 $h$ 是归一化组件的输出和输入。Ioffe & Szegedy
    ([2015](#bib.bib141)) 发现，应用于最先进的图像分类模型时，批量归一化在减少14倍训练步骤的情况下达到了相同的准确率，并且大幅度超越了原始模型。
- en: Following batch normalization, many normalization variants have been proposed,
    like layer normalization (Ba et al., [2016](#bib.bib9)), group normalization (Wu
    & He, [2020](#bib.bib359)), weight normalization (Salimans & Kingma, [2016](#bib.bib274)).
    These variants have almost the same calculation process except they are applied
    to different dimensions or different objectives.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量归一化之后，提出了许多归一化变体，如层归一化 (Ba et al., [2016](#bib.bib9))、组归一化 (Wu & He, [2020](#bib.bib359))、权重归一化
    (Salimans & Kingma, [2016](#bib.bib274))。这些变体几乎有相同的计算过程，只是它们应用于不同的维度或目标。
- en: Despite good performance, it is still controversial where the benefits of normalization
    come. At the start, normalization is proposed to address internal covariate shift
    by normalizing layer inputs. Internal covariate shift is a phenomenon where the
    distribution of each layer’s inputs changes during training. The parameters of
    the higher layer are required to continuously fit for the new distribution of
    lower layers, which slows down the training. To keep distribution steady, normalization
    is proposed to fix the distribution of input to a standard distribution. However,
     Santurkar et al. ([2018](#bib.bib277)) overturned this belief and they found
    that the distributional stability of layer inputs had little to do with the success
    of batch normalization. Instead, normalization makes the optimization landscape
    significantly smoother. This smoothness induces more predictive and stable gradients,
    allowing for faster training. Motivated by this paper,  Xu et al. ([2019](#bib.bib367))
    proved that normalization indeed normalized backward gradients, which plays an
    important role in deciding the success of normalization.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管表现良好，但关于归一化的好处来源仍存在争议。最初，归一化被提出是为了通过对层输入进行归一化来解决内部协变量偏移的问题。内部协变量偏移是一种现象，其中每一层输入的分布在训练过程中发生变化。高层的参数需要不断适应低层的新分布，这会减慢训练速度。为了保持分布稳定，归一化被提出以将输入的分布固定为标准分布。然而，Santurkar
    等人 ([2018](#bib.bib277)) 推翻了这种观点，他们发现层输入的分布稳定性与批量归一化的成功几乎没有关系。相反，归一化使优化过程变得显著平滑。这种平滑性引发了更具预测性和稳定性的梯度，从而加快了训练速度。受到这篇论文的启发，Xu
    等人 ([2019](#bib.bib367)) 证明了归一化确实规范了反向梯度，这在决定归一化的成功中起着重要作用。
- en: 3.3 Progressive Training
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 进阶训练
- en: Progressive training is another strategy to effectively train DNNs. The key
    idea is constructively adding layers. Compared to full training, progressive training
    does not require full gradients to all parameters, thus can largely reduce computations
    required for training. In addition, the well-trained lower layers also accelerate
    the training of higher layers.  Hinton et al. ([2006](#bib.bib126)) applied progressive
    training to deep belief networks. They trained layers sequentially starting from
    bottom layers in a greedy, layer-wise fashion. It is based on an assumption that
    upper layers represent more “abstract” concepts whereas lower layers extract “low-level
    features”. This method is unsupervised because each layer learns a higher-level
    representation of the layer below and the training criterion does not depend on
    the labels. Following this work,  Bengio et al. ([2006](#bib.bib20)) extended
    this method to handle continuous inputs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步训练是另一种有效训练深度神经网络的策略。其关键思想是逐步添加层。与全量训练相比，逐步训练不需要对所有参数进行全梯度计算，因此可以大幅减少训练所需的计算量。此外，训练良好的底层也加速了高层的训练。Hinton等（[2006](#bib.bib126)）将逐步训练应用于深度信念网络。他们以贪婪的层级方式按顺序训练层，从底层开始。这基于一个假设，即上层表示更“抽象”的概念，而下层提取“低级特征”。这种方法是无监督的，因为每一层学习下层的更高层次表示，并且训练标准不依赖于标签。在此基础上，Bengio等（[2006](#bib.bib20)）将这一方法扩展到处理连续输入。
- en: With the development of deep learning, layer-wise progressive training methods
    are exploited to train CNNs (Rueda-Plata et al., [2015](#bib.bib271); Kulkarni
    & Karande, [2017](#bib.bib165); Belilovsky et al., [2019](#bib.bib16)) and RNNs (Xu
    et al., [2018](#bib.bib369)). Recently,  Gong et al. ([2019](#bib.bib97)) and Yang
    et al. ([2020a](#bib.bib372)) extended the idea of layer-wise training to large-scale
    NLP models by progressively stacking new layers on top of previously trained layers.
    Their experimental results show that layer-wise training can successfully improve
    the efficiency of training large transformer language models with huge amounts
    of data. To be specific, experimental results show that such progressive training
    policy can achieve more than 110% training speedup without significant performance
    degradation.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的发展，层级逐步训练方法被用来训练CNN（Rueda-Plata等，[2015](#bib.bib271)；Kulkarni & Karande，[2017](#bib.bib165)；Belilovsky等，[2019](#bib.bib16)）和RNN（Xu等，[2018](#bib.bib369)）。最近，Gong等（[2019](#bib.bib97)）和Yang等（[2020a](#bib.bib372)）将层级训练的思想扩展到大规模的自然语言处理模型，通过在先前训练过的层上逐步堆叠新层。他们的实验结果表明，层级训练可以成功提高训练大规模变换器语言模型的效率，尤其是在处理大量数据时。具体来说，实验结果显示，这种逐步训练策略能够实现超过110%的训练速度提升，而不会显著降低性能。
- en: 3.4 Efficient Hyper-parameter Optimization
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 高效的超参数优化
- en: 'During training, hyper-parameter optimization (HPO) is a common and fundamental
    step for AI participants to find better model settings. Hyper-parameters keep
    fixed during training, including but not limited to optimization settings (e.g.,
    learning rate, batch size) and model settings (e.g., the number of layers). Since
    deep learning performs like a black-box model and the learning landscape is non-convex,
    current optimization approaches usually find a random local minimum. Due to the
    uncertainty, AI engineers tend to taking a lot of computations to find better
    hyper-parameter settings on real-world applications  (Yu & Zhu, [2020](#bib.bib385)).
    HPO or autoML is a field to automatically find the optimal settings. Considering
    that previous approaches mainly study architecture settings, we take efficient
    neural architecture search (NAS) as an example in this survey to review recent
    progress. Following previous studies, we split NAS methods into three components:
    search space, search strategy, and architecture evaluation. In this survey, we
    give an overview of efficient NAS. There are also surveys describing more details
    of NAS (Elsken et al., [2018](#bib.bib75)).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，超参数优化（HPO）是 AI 参与者寻找更好模型设置的常见且基础的步骤。超参数在训练过程中保持固定，包括但不限于优化设置（如学习率、批量大小）和模型设置（如层数）。由于深度学习表现得像一个黑箱模型，学习景观是非凸的，当前的优化方法通常会找到一个随机的局部最小值。由于不确定性，AI
    工程师倾向于进行大量计算，以在实际应用中找到更好的超参数设置  (Yu & Zhu, [2020](#bib.bib385))。HPO 或 AutoML 是一个自动寻找最佳设置的领域。考虑到以前的方法主要研究架构设置，我们以高效的神经架构搜索（NAS）作为本调查中的一个例子来回顾近期的进展。遵循以前的研究，我们将
    NAS 方法分为三个组件：搜索空间、搜索策略和架构评估。在本调查中，我们概述了高效的 NAS。还有一些调查描述了 NAS 的更多细节 (Elsken et al.,
    [2018](#bib.bib75))。
- en: '![Refer to caption](img/b4e608a2efa31c3901ce7524a61798d8.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b4e608a2efa31c3901ce7524a61798d8.png)'
- en: 'Figure 3.2: An overview of NAS components.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：NAS 组件概述。
- en: The search space defines all architecture candidates. At the first, the search
    space is defined as a discrete space, including structured space or unstructured
    space. Considering that network candidates in unstructured space is too massive,
    researchers usually incorporate inductive bias to build a structured search space (Liu
    et al., [2018a](#bib.bib195); Dong & Yang, [2020](#bib.bib70); Shu et al., [2020](#bib.bib297)).
    One of the representative methods is cell-based search space. Cell-based search
    space assumes that each architecture contains repetitions of fixed structures.
    In this way, the search space can be limited to the cell space where the number
    of candidates is largely reduced. In addition, to enable faster search, differentiable
    approaches (Jiang et al., [2019](#bib.bib146)) adopt a continuous search space
    where edge weights are considered.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间定义了所有架构候选。在开始时，搜索空间被定义为离散空间，包括结构化空间或非结构化空间。考虑到非结构化空间中的网络候选过于庞大，研究人员通常会结合归纳偏差来构建结构化搜索空间 (Liu
    et al., [2018a](#bib.bib195); Dong & Yang, [2020](#bib.bib70); Shu et al., [2020](#bib.bib297))。一种代表性的方法是基于单元的搜索空间。基于单元的搜索空间假设每个架构包含固定结构的重复。在这种方式下，搜索空间可以限制在单元空间中，从而大大减少候选数量。此外，为了实现更快的搜索，可微分的方法 (Jiang
    et al., [2019](#bib.bib146)) 采用了考虑边缘权重的连续搜索空间。
- en: The search strategy defines a policy to explore search space. Random search
    is one of traditional search approaches. The key idea is to randomly evaluate
    architectures and to select the best one based on their validation results. To
    reduce wasted evaluation costs, researchers proposed reinforcement learning based
    search policies (Ying et al., [2019](#bib.bib377)). It is a direction that introduces
    an architecture generator to generate well-performing architectures. Since random
    search and reinforcement learning require validation accuracy as search criterion,
    these methods usually need expensive computations. To reduce search costs, evolution
    based search has been proposed (Real et al., [2019](#bib.bib262)). It is a two-stage
    search approach. The first stage selects several well-performing parent architectures.
    The second stage applies mutation on these parent architectures to select the
    best one. The second stage starts from pre-trained parent networks and does not
    require too much computations to train child networks. Recently,  So et al. ([2019](#bib.bib300))
    applied the evolution search on Transformer networks and achieved new state-of-the-art
    results on machine translation and language modeling tasks. Although these approaches
    can reduce exploration costs, the dependence on validation accuracy still leads
    to considerable computation costs. To fully get rid of the dependence on validation
    accuracy, several studies (Jiang et al., [2019](#bib.bib146); Liu et al., [2019a](#bib.bib196);
    Dong & Yang, [2019](#bib.bib69); Zela et al., [2020](#bib.bib390); Chu et al.,
    [2020](#bib.bib52)) proposed differentiable search that re-formulated the task
    in a differentiable manner and allowed efficient search using gradient descent.
    In addition, another research line aims to represent a model into a continuous
    space where there is a mapping between structures and results. In this way, the
    model only learns how to predict the performance of architectures based on their
    continuous representations where the downstream training is not required (Luo
    et al., [2018](#bib.bib210)). To further reduce training costs, researchers proposed
    training-free NAS approaches that directly extracted features from randomly-initialized
    models and used these features as evaluation criterion to select networks (Mellor
    et al., [2020](#bib.bib218); Chen et al., [2021a](#bib.bib42); Abdelfattah et al.,
    [2021](#bib.bib1); Xu et al., [2021b](#bib.bib368)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索策略定义了一种探索搜索空间的策略。随机搜索是传统搜索方法之一。其关键思想是随机评估架构，并根据其验证结果选择最佳架构。为了减少浪费的评估成本，研究人员提出了基于强化学习的搜索策略（Ying
    et al., [2019](#bib.bib377)）。这是一种引入架构生成器生成表现良好的架构的方向。由于随机搜索和强化学习需要将验证准确率作为搜索标准，这些方法通常需要昂贵的计算。为了减少搜索成本，提出了基于进化的搜索方法（Real
    et al., [2019](#bib.bib262)）。这是一种两阶段的搜索方法。第一阶段选择几个表现良好的父架构。第二阶段对这些父架构进行变异，以选择最佳架构。第二阶段从预训练的父网络开始，并且不需要过多的计算来训练子网络。最近，So
    et al. ([2019](#bib.bib300)) 将进化搜索应用于Transformer网络，并在机器翻译和语言建模任务上取得了新的最先进结果。虽然这些方法可以减少探索成本，但对验证准确率的依赖仍然导致了相当大的计算成本。为了彻底摆脱对验证准确率的依赖，一些研究（Jiang
    et al., [2019](#bib.bib146); Liu et al., [2019a](#bib.bib196); Dong & Yang, [2019](#bib.bib69);
    Zela et al., [2020](#bib.bib390); Chu et al., [2020](#bib.bib52)）提出了可微分搜索，这种方法将任务重新公式化为可微分的形式，并允许使用梯度下降进行高效搜索。此外，另一研究方向旨在将模型表示为连续空间，其中结构与结果之间存在映射。通过这种方式，模型仅学习如何根据架构的连续表示预测性能，而下游训练则不需要（Luo
    et al., [2018](#bib.bib210)）。为了进一步减少训练成本，研究人员提出了无训练NAS方法，这些方法直接从随机初始化的模型中提取特征，并使用这些特征作为评估标准来选择网络（Mellor
    et al., [2020](#bib.bib218); Chen et al., [2021a](#bib.bib42); Abdelfattah et
    al., [2021](#bib.bib1); Xu et al., [2021b](#bib.bib368)）。
- en: Architecture evaluation takes almost all computations in NAS approaches. At
    the first, full training is required to evaluate the performance of a network,
    which is very heavy. Early stop is a widely used trick to estimate the results
    of a network. Besides, parameter-sharing is also a popular solution that network
    candidates can share parameters with each other (Pham et al., [2018](#bib.bib244)).
    In this way, the model can reuse pre-trained blocks during downstream training.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 架构评估几乎占用了NAS方法中的所有计算。首先，评估网络性能需要完整训练，这是一项非常繁重的任务。早停是估计网络结果的一种广泛使用的技巧。此外，参数共享也是一种流行的解决方案，网络候选者可以相互共享参数（Pham
    et al., [2018](#bib.bib244)）。通过这种方式，模型可以在下游训练中重用预训练的模块。
- en: Discussion
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 讨论
- en: Current NAS solutions are well-explored in the CV field, and widely-used benchmarks
    are also based on CV datasets. In the future, it is a promising direction to apply
    NAS in other fields, like NLP, to address more real-world problems. In addition,
    existing models focus more on models towards a single task for simplification.
    The multi-task/domain/lingual model is attracting more attention. Therefore, how
    to use NAS to search a shared multi-task/domain/lingual model is also a promising
    direction. Furthermore, the essential question of NAS is how model architecture
    affects downstream results. More understanding studies are expected to reveal
    the fundamental connection between architecture and performance.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的 NAS 解决方案在计算机视觉领域得到充分探讨，广泛使用的基准测试也基于计算机视觉数据集。未来，将 NAS 应用于其他领域，如自然语言处理（NLP），以解决更多现实问题，是一个有前景的方向。此外，现有模型更多关注简化单任务模型。多任务/领域/语言模型正受到更多关注。因此，如何使用
    NAS 寻找共享的多任务/领域/语言模型也是一个有前景的方向。此外，NAS 的核心问题是模型架构如何影响下游结果。更多的理解性研究有望揭示架构与性能之间的基本联系。
- en: Chapter 4 Energy-Efficient Inference
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四章 节能推理
- en: In this chapter, we describe common network surgery methods for reducing inference
    costs, including pruning, low-rank factorization, quantization, and knowledge
    distillation. A brief review of these methods is presented in Figure [4.1](#Ch4.F1
    "Figure 4.1 ‣ Chapter 4 Energy-Efficient Inference ‣ A Survey on Green Deep Learning")
    and Table [4.1](#Ch4.T1 "Table 4.1 ‣ Chapter 4 Energy-Efficient Inference ‣ A
    Survey on Green Deep Learning").
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们描述了减少推理成本的常见网络优化方法，包括剪枝、低秩分解、量化和知识蒸馏。这些方法的简要回顾展示在图 [4.1](#Ch4.F1 "图
    4.1 ‣ 第四章 节能推理 ‣ 绿色深度学习综述") 和表 [4.1](#Ch4.T1 "表 4.1 ‣ 第四章 节能推理 ‣ 绿色深度学习综述")中。
- en: '{forest}'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,align=left,
    minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt, inner ysep=1pt,
    , where level=1text width=5em, where level=2text width=5.8em,font=, where level=3font=,
    where level=4font=, where level=5font=, [Efficient Inference [Pruning [Pruning
    Unit [Unstructured [Neurons, Connections]] [Structured [Filters, Channels, Layers]]
    ] [Scoring Function [Magnitude] [Important Coefficients] [Gradient-based] [Movement
    Pruning] ] [Scheduling [Single-step Pruning] [Iterative Pruning] [Lottery Ticket]
    ] ] [Low-rank
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 分叉边缘，树=向东生长，反转=true，锚点=基座西，父锚点=东，子锚点=西，基座=左，字体=，矩形，绘制=隐藏绘制，圆角，左对齐，最小宽度=2.5em，最小高度=1.2em，s
    sep=6pt，内xsep=3pt，内ysep=1pt， ，其中级别=1text width=5em，其中级别=2text width=5.8em，字体=，其中级别=3字体=，其中级别=4字体=，其中级别=5字体=，
    [高效推理 [剪枝 [剪枝单元 [非结构化 [神经元，连接]] [结构化 [过滤器，通道，层]] ] [评分函数 [幅度] [重要系数] [基于梯度] [移动剪枝]
    ] [调度 [单步剪枝] [迭代剪枝] [彩票票据] ] ] [低秩
- en: Factorization [Matrix Factorization [Low-rank Matrix Factorization, SVD]] [Tensor
    Factorization [CP, VBMF, Tucker Decomposition, BTD]] ] [Quantization [Deterministic
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 分解 [矩阵分解 [低秩矩阵分解，SVD]] [张量分解 [CP，VBMF，Tucker 分解，BTD]] ] [量化 [确定性
- en: Quantization [Rounding, Vector Quantization]] [Stochastic
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 量化 [舍入，向量量化]] [随机
- en: Quantization [Random Rounding, Probabilistic Quantization]] ] [Knowledge
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 量化 [随机舍入，概率量化]] ] [知识
- en: Distillation [Distillation Target [Logits-based (Vanilla KD)] [Feature-based]
    [Relation-based] ] [Teacher Numbers [Dynamic KD, Multi-teacher KD, Mutual Learning]]
    ] ]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏 [蒸馏目标 [基于Logits的（Vanilla KD）] [基于特征] [基于关系] ] [教师数量 [动态 KD，多教师 KD，相互学习]]
    ] ]
- en: 'Figure 4.1: Taxonomy of efficient inference methods with representative examples.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：高效推理方法的分类及代表性例子。
- en: 'Table 4.1: Different approaches for efficient inference.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：高效推理的不同方法。
- en: '| Approaches | Descriptions | Characteristics |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 | 特点 |'
- en: '| Pruning | Reduce redundant parameters which | Can be applied to various settings.
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝 | 减少冗余参数 | 可应用于各种设置。 |'
- en: '|  | are not sensitive to results. | Fine-tuning is optional. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | 对结果不敏感。 | 微调是可选的。 |'
- en: '| Low-rank Factorization | Use matrix/tensor decomposition to | Matrix decomposition
    is computationally |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 低秩分解 | 使用矩阵/张量分解 | 矩阵分解在计算上是 |'
- en: '|  | approximate the original parameters. | complicated, but can support train
    from scratch. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | 复杂，但可以支持从头训练。 |'
- en: '| Quantization | Reduce the number of bits used to | Easy to implement. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 减少用于近似原始参数的位数。 | 易于实现。 |'
- en: '|  | represent weights and activations. | Sensitive to hardware architecture.
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | 表示权重和激活。 | 对硬件架构敏感。 |'
- en: '| Knowledge Distillation | Train a compact neural network with | Easy to implement.
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 知识蒸馏 | 训练一个紧凑的神经网络与 | 易于实现。 |'
- en: '|  | knowledge distilled from a teacher model. | Sensitive to network parameters.
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | 从教师模型中蒸馏的知识。 | 对网络参数敏感。 |'
- en: 4.1 Model Pruning
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型剪枝
- en: Model pruning is a popular solution to reduce redundant parameters in DNNs.
    Back to 1980s, Hanson & Pratt ([1988](#bib.bib114)) and LeCun et al. ([1989](#bib.bib172))
    already verified that parameters are not equally important to the final performance.
    By removing unimportant weights from a network, we can simultaneously reduce the
    number of parameters, accelerate training/inference, save training examples, and
    improve generalization. This motivates a great amount of studies on pruning neural
    networks in the past 30 years. Specifically, given an initial network that is
    large and accurate, the key idea of pruning is to remove parameters from the original
    network to produce a smaller network that can retain the accuracy of the original
    model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 模型剪枝是减少深度神经网络中冗余参数的一个流行解决方案。早在1980年代，Hanson 和 Pratt ([1988](#bib.bib114)) 以及
    LeCun 等人 ([1989](#bib.bib172)) 已经验证了参数对最终性能的重要性不尽相同。通过从网络中删除不重要的权重，我们可以同时减少参数数量，加快训练/推理速度，节省训练样本，并改善泛化。这激发了过去30年大量关于神经网络剪枝的研究。具体来说，给定一个初始的大型且准确的网络，剪枝的关键思想是从原始网络中移除参数，以生成一个较小的网络，同时保持原始模型的准确性。
- en: We first provide a formal definition of pruning. Let us define a neural network
    model as $f(X;\theta)$, which is a function over an input set $X$ and the set
    of parameters $\theta$. Pruning approaches usually take a model $f(X;\theta)$
    as input and then produce a new model $f(X;\theta^{\prime})$. $\theta^{\prime}$
    is a set of parameters with the size of $\theta^{\prime}$ being less than that
    of $\theta$. Usually, $\theta^{\prime}$ is a subset of $\theta$.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提供剪枝的正式定义。我们将神经网络模型定义为 $f(X;\theta)$，它是对输入集 $X$ 和参数集 $\theta$ 的函数。剪枝方法通常将模型
    $f(X;\theta)$ 作为输入，然后生成一个新的模型 $f(X;\theta^{\prime})$。$\theta^{\prime}$ 是一个参数集，其大小小于
    $\theta$。通常，$\theta^{\prime}$ 是 $\theta$ 的一个子集。
- en: Algorithm 1 The generic framework of pruning
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 剪枝的通用框架
- en: 0:  $N$ is the number of iterations of pruning; $X$ is the dataset  $\theta^{\prime}\leftarrow\text{train-to-convergence}(f(X;\theta))$  for $i$
    in $1$ to $N$ do     $\theta^{\prime}\leftarrow\text{prune}(score(\theta^{\prime}))$     $\theta^{\prime}\leftarrow\text{fine-tune}(f(X;\theta^{\prime}))$  end for  return
    $\theta^{\prime}$
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  $N$ 是剪枝的迭代次数；$X$ 是数据集  $\theta^{\prime}\leftarrow\text{train-to-convergence}(f(X;\theta))$  对于 $i$
    从 $1$ 到 $N$ 做     $\theta^{\prime}\leftarrow\text{prune}(score(\theta^{\prime}))$     $\theta^{\prime}\leftarrow\text{fine-tune}(f(X;\theta^{\prime}))$  结束  返回
    $\theta^{\prime}$'
- en: Previous pruning approaches mainly follow the work of  Han et al. ([2015](#bib.bib109))
    to produce a pruned model $f(X;\theta^{\prime})$ from an original model $f(X;\theta)$.
    We show the generic framework in Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1 Model
    Pruning ‣ Chapter 4 Energy-Efficient Inference ‣ A Survey on Green Deep Learning").
    First, the network is trained to convergence to get pre-trained parameters. Second,
    each parameter or structural element in the network is assigned a score. This
    score indicates the relative importance to the final performance. The network
    is then pruned based on these scores. Third, since pruning generally reduces the
    accuracy of the network, it is a general practice to fine-tune (train further
    after pruning) the pruned network. The process of pruning and fine-tuning is usually
    iterated several times.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的剪枝方法主要遵循 Han 等人 ([2015](#bib.bib109)) 的工作，从原始模型 $f(X;\theta)$ 中生成剪枝模型 $f(X;\theta^{\prime})$。我们在算法
    [1](#alg1 "Algorithm 1 ‣ 4.1 Model Pruning ‣ Chapter 4 Energy-Efficient Inference
    ‣ A Survey on Green Deep Learning") 中展示了通用框架。首先，训练网络直到收敛以获得预训练参数。其次，网络中的每个参数或结构元素被分配一个分数。这个分数表示相对于最终性能的重要性。然后，网络根据这些分数进行剪枝。第三，由于剪枝通常会降低网络的准确性，因此一般做法是对剪枝后的网络进行微调（剪枝后进一步训练）。剪枝和微调的过程通常会迭代多次。
- en: 'We describe some key components of network pruning algorithms:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了网络剪枝算法的一些关键组件：
- en: •
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pruning Unit refers to the basic unit that the algorithm aims to prune. According
    to pruning units, we can classify modern pruning approaches into two categories,
    unstructured pruning, and structured pruning. Some pruning algorithms prune individual
    parameters (i.e., unstructured pruning), which produce a sparse neural network.
    While the resulting sparse network is smaller in terms of the number of parameters,
    it is hard to yield speedups since the pruned weights are not well arranged. In
    contrast, structured pruning considers parameters in groups. They keep the dense
    features of the network by removing entire weight matrices, filters, channels,
    or layers.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝单元是指算法旨在剪枝的基本单元。根据剪枝单元，我们可以将现代剪枝方法分为两类：非结构化剪枝和结构化剪枝。一些剪枝算法剪枝单个参数（即非结构化剪枝），这会产生一个稀疏神经网络。尽管结果稀疏网络在参数数量上较小，但由于剪枝的权重排列不佳，因此很难提高速度。相比之下，结构化剪枝将参数按组考虑。通过去除整个权重矩阵、滤波器、通道或层，它们保持了网络的密集特征。
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scoring Function defines the metric used to prune parameters. Common practices
    for parameter scoring usually are based on importance coefficients, network activations,
    or gradients. After assigning scores to each part of the parameters, we have two
    choices to prune networks. First, we can choose to prune a fraction of the parameters
    with the locally lowest scores within each structural sub-component of the network
    (e.g., layers). Second, we also can choose parameters with the globally lowest
    scores within the entire network.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评分函数定义了用于剪枝参数的指标。参数评分的常见做法通常基于重要性系数、网络激活或梯度。在为每个参数部分分配分数后，我们有两种选择来剪枝网络。首先，我们可以选择剪枝网络每个结构子组件（例如，层）中局部最低分的部分参数。其次，我们也可以选择全网络中全局最低分的参数。
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scheduling decides the total step that pruning algorithms use to prune parameters.
    Some methods prune weights in a single step while another methods use multiple
    steps to prune parameters where each step only prunes a part of parameters.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调度决定了剪枝算法用于剪枝参数的总步数。一些方法在一步中剪枝权重，而另一些方法使用多步剪枝，每一步仅剪枝一部分参数。
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fine-tuning is usually required for the pruned network to recover the original
    accuracy. Many methods choose to fine-tune the pruned network, or re-train the
    pruned network.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调通常是恢复剪枝网络原始准确性的必要步骤。许多方法选择对剪枝网络进行微调或重新训练剪枝网络。
- en: Application
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 应用
- en: Pruning is first applied to fully-connected networks. For example,  LeCun et al.
    ([1989](#bib.bib172)) analyzed the importance of parameters and showed that small
    magnitude weights had less impact on training losses. Specifically, they computed
    the saliency of parameters based on the second derivative. Then, they pruned parameters
    with small saliency scores. To recover the original performance of the network,
    the network was fine-tuned after pruning.  Hassibi et al. ([1993](#bib.bib116))
    extended this idea by using the inverse of the Hessian as saliency score. In addition
    to weight pruning, Suzuki et al. ([2001](#bib.bib313)) proposed to prune network
    connections based on their influence on training losses, and then re-train the
    network to compensate the performance drop. Unlike these approaches,  Srinivas
    & Babu ([2015](#bib.bib304)) argued that similar neurons were redundant. They
    proposed to remove redundant neurons, instead of removing individual weight connections
    one by one.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝首先应用于全连接网络。例如，LeCun 等人 ([1989](#bib.bib172)) 分析了参数的重要性，并显示出小幅度权重对训练损失的影响较小。具体而言，他们基于二阶导数计算参数的显著性。然后，他们剪枝显著性分数较低的参数。为了恢复网络的原始性能，剪枝后对网络进行了微调。Hassibi
    等人 ([1993](#bib.bib116)) 使用 Hessian 的逆作为显著性分数，扩展了这一思想。除了权重剪枝，Suzuki 等人 ([2001](#bib.bib313))
    提出了基于对训练损失影响的网络连接剪枝，然后重新训练网络以补偿性能下降。与这些方法不同，Srinivas & Babu ([2015](#bib.bib304))
    认为相似的神经元是冗余的。他们提出去除冗余神经元，而不是逐个去除单独的权重连接。
- en: Currently, many pruning algorithms are applied to CNNs.  Han et al. ([2015](#bib.bib109))
    proposed a simple magnitude-based method to remove unimportant connections in
    fully-connected layers and convolutional layers. However, the resulting model,
    despite being sparse, does not bring significant inference speedups due to the
    feature of sparsity. To address this problem, several structured pruning algorithms
    have been proposed to prune dense blocks, like filters, channels, or layers (Li
    et al., [2017](#bib.bib182); Molchanov et al., [2017](#bib.bib225); He et al.,
    [2017b](#bib.bib123); Lin et al., [2017](#bib.bib190); He et al., [2018](#bib.bib124);
    Luo et al., [2019](#bib.bib209)).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，许多剪枝算法被应用于卷积神经网络（CNNs）。Han 等人 ([2015](#bib.bib109)) 提出了一种简单的基于幅度的方法来移除全连接层和卷积层中不重要的连接。然而，尽管得到的模型是稀疏的，但由于稀疏性的特性，推理速度并没有显著提升。为了解决这个问题，已经提出了几种结构化剪枝算法，用于剪枝密集块，如滤波器、通道或层（Li
    等人，[2017](#bib.bib182)；Molchanov 等人，[2017](#bib.bib225)；He 等人，[2017b](#bib.bib123)；Lin
    等人，[2017](#bib.bib190)；He 等人，[2018](#bib.bib124)；Luo 等人，[2019](#bib.bib209)）。
- en: In addition to CV models, pruning has been successfully applied to NLP tasks.
    At the early stage, several studies successfully pruned recurrent neural networks
    (RNNs).  See et al. ([2016](#bib.bib286)) used iterative pruning and retraining
    to prune a recurrent model for neural translation.  Narang et al. ([2017a](#bib.bib228))
    pruned RNNs via magnitude based pruning.  Narang et al. ([2017b](#bib.bib229))
    used iterative ground lasso regularization to induce block sparsity in RNNs.  Lee
    et al. ([2019](#bib.bib174)) and Zhang & Stadie ([2020](#bib.bib392)) proposed
    one-shot RNN pruning methods based on connection sensitivity and Jacobian spectrum.
    More recently, with the success of Transformer, several studies investigated pruning
    transformer models (Michel et al., [2019](#bib.bib219); Voita et al., [2019](#bib.bib335);
    McCarley et al., [2019](#bib.bib216); Fan et al., [2020b](#bib.bib78); Wang et al.,
    [2020c](#bib.bib350); Sanh et al., [2020](#bib.bib276)). One trend is to use structured
    pruning since the transformer architecture is highly parallelized. For instance, Fan
    et al. ([2020b](#bib.bib78)) introduced LayerDrop to prune transformer layers
    for efficient inference.  Michel et al. ([2019](#bib.bib219)) and  Voita et al.
    ([2019](#bib.bib335)) revealed that the multi-head attention mechanism in the
    transformer architecture led to redundant attention heads. Motivated by this finding,
    they proposed to directly prune attention heads.  McCarley et al. ([2019](#bib.bib216))
    used structured pruning to compress a BERT-based question answering model.  Xu
    et al. ([2020](#bib.bib365)) recently proposed a progressive module replacing
    approach by replacing a whole module from the original model with a compact module
    to reduce model size. Unlike these studies,  Guo et al. ([2020](#bib.bib104))
    proposed an unstructured pruning approach, diff pruning, to compress a multi-task
    model.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算机视觉（CV）模型，剪枝也成功应用于自然语言处理（NLP）任务。在早期阶段，几项研究成功地对递归神经网络（RNNs）进行了剪枝。See 等人 ([2016](#bib.bib286))
    使用了迭代剪枝和再训练的方法对递归模型进行剪枝。Narang 等人 ([2017a](#bib.bib228)) 通过基于幅度的剪枝方法对 RNNs 进行了剪枝。Narang
    等人 ([2017b](#bib.bib229)) 使用了迭代的地面套索正则化方法来引入 RNNs 的块稀疏性。Lee 等人 ([2019](#bib.bib174))
    和 Zhang & Stadie ([2020](#bib.bib392)) 提出了基于连接敏感性和雅可比谱的一次性 RNN 剪枝方法。最近，随着 Transformer
    的成功，几项研究探讨了剪枝 Transformer 模型的技术（Michel 等人，[2019](#bib.bib219)；Voita 等人，[2019](#bib.bib335)；McCarley
    等人，[2019](#bib.bib216)；Fan 等人，[2020b](#bib.bib78)；Wang 等人，[2020c](#bib.bib350)；Sanh
    等人，[2020](#bib.bib276)）。一种趋势是使用结构化剪枝，因为 Transformer 架构高度并行。例如，Fan 等人 ([2020b](#bib.bib78))
    引入了 LayerDrop 方法，用于剪枝 Transformer 层以提高推理效率。Michel 等人 ([2019](#bib.bib219)) 和 Voita
    等人 ([2019](#bib.bib335)) 揭示了 Transformer 架构中的多头注意力机制导致了冗余的注意力头。受到这一发现的启发，他们提出了直接剪枝注意力头的方法。McCarley
    等人 ([2019](#bib.bib216)) 使用结构化剪枝方法对基于 BERT 的问答模型进行压缩。Xu 等人 ([2020](#bib.bib365))
    最近提出了一种渐进式模块替换方法，通过用紧凑的模块替换原始模型中的整个模块来减小模型大小。与这些研究不同，Guo 等人 ([2020](#bib.bib104))
    提出了无结构剪枝方法——差异剪枝，以压缩多任务模型。
- en: While most aforementioned pruning algorithms require re-using or re-training
    the originally trained network, a recent research direction (Frankle & Carbin,
    [2019](#bib.bib88)) suggested that dense, randomly-initialized, feed-forward networks
    contained sub-networks (winning tickets), which can reached the test accuracy
    of the original network. They also found that a standard pruning technique naturally
    uncovered some of the winning tickets and then proposed an algorithm to identify
    winning tickets at the early stage of training. Also,  Frankle & Carbin ([2019](#bib.bib88))
    and Liu et al. ([2019c](#bib.bib205)) found that once the “winning ticket” is
    found, it can be trained from scratch to get an equivalent or better performance
    compared to pruned and fine-tuned one. Recently, there are also several studies
    investigating the lottery ticket hypothesis for BERT-like models (Prasanna et al.,
    [2020](#bib.bib248); Chen et al., [2020c](#bib.bib38)). For example,  Prasanna
    et al. ([2020](#bib.bib248)) found that with structured pruning, the “random”
    sub-networks are still almost as good as the “good” ones, and even the “worst”
    ones perform on par with a strong baseline.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数上述的剪枝算法需要重新使用或重新训练原始训练的网络，但最近的研究方向（Frankle & Carbin，[2019](#bib.bib88)）表明，密集的、随机初始化的前馈网络包含子网络（胜利票），这些子网络可以达到原始网络的测试精度。他们还发现标准剪枝技术自然揭示了一些胜利票，并提出了一种算法来在训练的早期阶段识别胜利票。此外，Frankle
    & Carbin（[2019](#bib.bib88)）和Liu et al.（[2019c](#bib.bib205)）发现，一旦找到“胜利票”，它可以从头开始训练，得到与剪枝和微调后的网络相当或更好的性能。最近，也有几个研究调查了BERT类似模型的“彩票票据假设”（Prasanna
    et al.，[2020](#bib.bib248)；Chen et al.，[2020c](#bib.bib38)）。例如，Prasanna et al.（[2020](#bib.bib248)）发现通过结构化剪枝，“随机”子网络仍然几乎与“好”子网络一样好，甚至“最差”子网络的表现也与强基线相当。
- en: Discussion
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 讨论
- en: Pruning is very effective for reducing the number of parameters in DNNs. With
    structured pruning, it can accelerate inference and reduce computations. However,
    there are also several limitations of pruning methods. First, it requires iteratively
    scoring weights and re-training the network for many iterations. Also, pruning
    often leads to a non-negligible performance drop when applied to powerful DNNs.
    The lottery ticket hypothesis provides an interesting direction for more efficient
    “pruning” algorithms. In this survey, we give an overview of pruning methods.
    If you are interested in this direction, there are several surveys describing
    more details of pruning (Liang et al., [2021](#bib.bib187)).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝在减少DNN中的参数数量方面非常有效。通过结构化剪枝，它可以加速推理并减少计算。然而，剪枝方法也有几个限制。首先，它需要对权重进行迭代评分并重新训练网络许多次。此外，剪枝在应用于强大的DNN时往往会导致性能显著下降。彩票票据假设为更高效的“剪枝”算法提供了一个有趣的方向。在本综述中，我们概述了剪枝方法。如果你对这一方向感兴趣，有几个综述描述了更多的剪枝细节（Liang
    et al.，[2021](#bib.bib187)）。
- en: 4.2 Low-rank Factorization
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 低秩分解
- en: Tensor (including matrix) operation is the basic building block and contributes
    the bulk of most computations and parameters in DNNs. Therefore, compressing tensors
    or matrices in DNNs is promising for reducing the number of parameters and computation
    costs. The motivation of low-rank factorization is that a large amount of redundant
    information exist in large weight matrices. The super-large matrices are generally
    of low rank and can be factorized into several tiny matrices to save parameters.
    For example, we can apply singular value decomposition (SVD) to factorize a super
    large matrix. SVD factorizes the original weight matrix into three smaller matrices.
    Formally, for any matrix $A\in\mathbb{R}^{m\times n}$, there exists a factorization,
    $A=USV^{T}$ where $U\in\mathbb{R}^{m\times r}$ and $V^{T}\in\mathbb{R}^{r\times
    n}$ are orthogonal matrices, $S\in\mathbb{R}^{r\times r}$ is a diagonal matrix
    with only singular values of A on the diagonal. With SVD decomposition, the spatial
    complexity can be reduced from $\mathcal{O}(mn)$ to $\mathcal{O}(r(m+n+1))$.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 张量（包括矩阵）操作是基础构建块，并且在DNN中贡献了大部分计算和参数。因此，压缩DNN中的张量或矩阵对于减少参数数量和计算成本是有前景的。低秩分解的动机在于大量冗余信息存在于大权重矩阵中。超大矩阵通常是低秩的，可以分解为几个小矩阵以节省参数。例如，我们可以应用奇异值分解（SVD）来分解一个超大矩阵。SVD将原始权重矩阵分解为三个较小的矩阵。形式上，对于任何矩阵
    $A\in\mathbb{R}^{m\times n}$，存在一个分解，$A=USV^{T}$ 其中 $U\in\mathbb{R}^{m\times r}$
    和 $V^{T}\in\mathbb{R}^{r\times n}$ 是正交矩阵，$S\in\mathbb{R}^{r\times r}$ 是一个对角矩阵，对角线上只有A的奇异值。通过SVD分解，空间复杂度可以从
    $\mathcal{O}(mn)$ 减少到 $\mathcal{O}(r(m+n+1))$。
- en: Application
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 应用
- en: Similar to pruning algorithms, low-rank factorization is successfully applied
    to CV models.  Sainath et al. ([2013](#bib.bib273)) applied matrix factorization
    to the final weight layer. If the original weight matrix has the dimension $m\times
    n$ and rank $r$, the full rank matrix can be factorized in two weight matrices
    as $m\times r$ and $r\times n$. Their approach reduced the number of parameters
    and achieved up to 30–50% speedup. Similarly, Xue et al. ([2014](#bib.bib370))
    proposed to use SVD decomposition to compress fully-connected neural networks.
     Rigamonti et al. ([2013](#bib.bib265)) proposed to approximate trained CNNs with
    low-rank filters.  Denton et al. ([2014](#bib.bib63)) further exploited the linear
    structure present within the convolutional filters. Their approach was able to
    reduce the memory requirement of the weights in the first two convolutional layers
    by 2–3 times. While low-rank matrix factorization can optimize both the spatial
    and computational complexity of neural networks, the plane view of a matrix limits
    the potential for extreme compression.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 与修剪算法类似，低秩因式分解已成功应用于计算机视觉模型。Sainath 等人（[2013](#bib.bib273)）将矩阵因式分解应用于最终的权重层。如果原始权重矩阵的维度为
    $m\times n$ 并且秩为 $r$，则可以将完整秩矩阵因式分解为两个权重矩阵，分别为 $m\times r$ 和 $r\times n$。他们的方法减少了参数数量，并实现了高达
    30–50% 的加速。类似地，Xue 等人（[2014](#bib.bib370)）提出使用 SVD 分解来压缩全连接神经网络。Rigamonti 等人（[2013](#bib.bib265)）建议使用低秩滤波器来逼近训练好的
    CNN。Denton 等人（[2014](#bib.bib63)）进一步利用卷积滤波器中的线性结构。他们的方法能够将前两层卷积层的内存需求减少 2–3 倍。虽然低秩矩阵因式分解可以优化神经网络的空间和计算复杂度，但矩阵的平面视图限制了极端压缩的潜力。
- en: Tensor factorization algorithms, in contrast, are more flexible and can be employed
    to achieve an extremely high compression ratio. Among popular tensor factorization
    methods, classical prolongation (CP) (Kolda & Bader, [2009](#bib.bib163)), where
    each factor matrix has the same rank and the kernel tensor is a superdialognal
    tensor, generally achieves better compression performance.  Lebedev et al. ([2015](#bib.bib171))
    leveraged CP decomposition to compress CNN weight kernels into several sub-kernels
    to reduce the number of parameters. Specifically, non-linear least squares were
    used to compute low-rank CP-decomposition of the 4-D tensor into a sum of rank-one
    tensors. Using this decomposition, the original convolutional layer was replaced
    by a sequence of 4 convolutional layers with smaller filters. Following this idea,
     Kim et al. ([2016a](#bib.bib160)) introduced a one-shot compression method to
    compress the whole network. In addition,  Chen et al. ([2018b](#bib.bib47)) introduced
    a collective residual unit based on block term decomposition (BTD), which is a
    combination of Tucker and CP, to enhance the utilization of parameters in residual
    CNNs.  Zhou et al. ([2019](#bib.bib398)) conversely used neural networks to learn
    an appropriate CP rank for tensor decomposition.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相比，张量因式分解算法更具灵活性，并且可以实现极高的压缩比。在流行的张量因式分解方法中，经典的延拓（CP）（Kolda & Bader，[2009](#bib.bib163)），其中每个因子矩阵具有相同的秩，并且核张量是一个超对角张量，通常能够实现更好的压缩性能。Lebedev
    等人（[2015](#bib.bib171)）利用 CP 分解将 CNN 权重核压缩成多个子核，以减少参数数量。具体而言，使用非线性最小二乘法计算 4-D
    张量的低秩 CP 分解，将其分解为一组秩一的张量。使用这种分解，原始卷积层被替换为一系列具有更小滤波器的 4 个卷积层。基于这一思路，Kim 等人（[2016a](#bib.bib160)）引入了一种一次性压缩方法来压缩整个网络。此外，Chen
    等人（[2018b](#bib.bib47)）引入了一种基于块项分解（BTD）的集成残差单元，这是 Tucker 和 CP 的组合，用于增强残差 CNN 中参数的利用率。Zhou
    等人（[2019](#bib.bib398)）则相反地使用神经网络学习适当的 CP 秩以进行张量分解。
- en: Apart from the applications on CV models, low-rank factorization has also been
    applied to NLP models. For example, Grachev et al. ([2017](#bib.bib100)) used
    low-rank factorization to train RNN language models.  Winata et al. ([2019](#bib.bib356))
    investigated the use of low-rank factorization as an effective post-processing
    compression method for LSTMs. They applied low-rank factorization on ELMo, one
    of widely-used pre-trained models. Recently, low-rank factorization has also been
    applied on Transformer models (Ma et al., [2019](#bib.bib212)).  Noach & Goldberg
    ([2020](#bib.bib232)) further proposed a two-stage model-compression method to
    reduce the inference time cost of BERT, a kind of Transformer-based model. Their
    approach decomposed the matrices into smaller matrices and then performed feature
    distillation on the internal representation. Also, Lan et al. ([2020](#bib.bib170))
    applied embedding matrix factorization along with layer sharing to reduce the
    amount of parameters.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在CV模型上的应用，低秩分解还被应用于NLP模型。例如，Grachev et al.（[2017](#bib.bib100)）使用低秩分解训练RNN语言模型。Winata
    et al.（[2019](#bib.bib356)）研究了将低秩分解作为LSTM的有效后处理压缩方法。他们在广泛使用的预训练模型ELMo上应用了低秩分解。最近，低秩分解还被应用于Transformer模型（Ma
    et al.，[2019](#bib.bib212)）。Noach & Goldberg（[2020](#bib.bib232)）进一步提出了一种两阶段模型压缩方法，以减少BERT（一种基于Transformer的模型）的推理时间成本。他们的方法将矩阵分解为较小的矩阵，然后对内部表示进行特征蒸馏。此外，Lan
    et al.（[2020](#bib.bib170)）结合嵌入矩阵分解和层共享来减少参数量。
- en: Discussion
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 讨论
- en: Compared with other popular compression methods, low-rank factorization can
    effectively reduce the size of models with a large compression ratio while preserving
    the performance well. Low-rank factorization is also relatively flexible. However,
    low-rank factorization also suffers from the issue of computational efficiency
    because SVD over large weight matrices can be computationally heavy. Also, compared
    with the compression ratio in terms of model size, low-rank factorization is less
    effective for reducing the computational cost and inference time.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他流行的压缩方法相比，低秩分解可以有效地减少具有大压缩比的模型大小，同时良好地保持性能。低秩分解也相对灵活。然而，低秩分解也面临计算效率的问题，因为对大型权重矩阵进行SVD可能计算负担较重。此外，与模型大小的压缩比相比，低秩分解在减少计算成本和推理时间方面效果较差。
- en: 4.3 Quantization
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 量化
- en: The goal of quantization is to compress the original network by reducing the
    number of bits. The idea of network quantization can be back to early 1990s (Fiesler
    et al., [1990](#bib.bib84); Balzer et al., [1991](#bib.bib14); Tang & Kwan, [1993](#bib.bib322)).
    Recently, due to the success of DNNs and their growing sizes, the research of
    quantization has received increasing attention. In the beginning of 2010s,  Vanhoucke
    et al. ([2011](#bib.bib329)) discovered that CNNs encoded with 32-bit can be converted
    to CNNs encoded with 8-bit, which significantly reduced both storage and computation
    costs.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 量化的目标是通过减少比特数来压缩原始网络。网络量化的思想可以追溯到1990年代初（Fiesler et al.，[1990](#bib.bib84)；Balzer
    et al.，[1991](#bib.bib14)；Tang & Kwan，[1993](#bib.bib322)）。近年来，由于DNN的成功及其日益增长的规模，量化的研究受到越来越多的关注。在2010年代初，Vanhoucke
    et al.（[2011](#bib.bib329)）发现32位编码的CNN可以转换为8位编码的CNN，这显著降低了存储和计算成本。
- en: 'Generally speaking, quantization techniques can be classified into two types:
    deterministic quantization and stochastic quantization. In deterministic quantization,
    there is an deterministic mapping between the quantized value and the real value.
    In stochastic quantization, the quantized value is sampled from discrete distributions (Guo,
    [2018](#bib.bib105)). Usually, post-training quantization is the most simplest
    solution by applying quantization on a trained model to reduce inference costs.
    Despite simple, post-training quantization may brings dropped performance. quantization-aware
    training is proposed to address this problem by fine-tuning the quantized model
    before inference.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，量化技术可以分为两种类型：确定性量化和随机量化。在确定性量化中，量化值与实际值之间存在确定性的映射。而在随机量化中，量化值是从离散分布中抽样得到的（Guo，[2018](#bib.bib105)）。通常，后训练量化是最简单的解决方案，通过对训练后的模型进行量化以减少推理成本。尽管简单，后训练量化可能会导致性能下降。量化感知训练被提出以解决这个问题，通过在推理前微调量化模型来应对。
- en: Deterministic Quantization
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 确定性量化
- en: 'defines a deterministic mapping between real weights and quantized weights.
    Rounding quantization is the simplest mapping function. The key idea of rounding
    quantization is to map a high-bit floating-point number to its nearest fixed-point
    low-bit number (Gupta et al., [2015](#bib.bib106)). For example, suppose a number
    $x$ and the target fixed-point representation [IL, FL]. The number of integer
    bits IL plus the number of fractional bits FL yields the total number of bits
    used to represent the number. This approach considers the following rounding scheme:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了实数权重和量化权重之间的确定性映射。舍入量化是最简单的映射函数。舍入量化的关键思想是将高位浮点数映射到其最接近的定点低位数（Gupta 等人，[2015](#bib.bib106)）。例如，假设一个数字
    $x$ 和目标定点表示 [IL, FL]。整数位数 IL 加上小数位数 FL 得出表示该数字所用的总位数。该方法考虑了以下舍入方案：
- en: '|  | <math   alttext="\textnormal{Convert}(x,\textnormal{[IL, FL]})=\begin{cases}-2^{\textnormal{IL-1}}\hskip
    50.00008pt\textnormal{if}\ x\leq-2^{\textnormal{IL-1}},\\ 2^{\textnormal{IL-1}}-2^{-\textnormal{FL}}\hskip
    26.00009pt\textnormal{if}\ x\geq 2^{\textnormal{IL-1}}-2^{-\textnormal{FL}},\\'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\textnormal{Convert}(x,\textnormal{[IL, FL]})=\begin{cases}-2^{\textnormal{IL-1}}\hskip
    50.00008pt\textnormal{如果}\ x\leq-2^{\textnormal{IL-1}},\\ 2^{\textnormal{IL-1}}-2^{-\textnormal{FL}}\hskip
    26.00009pt\textnormal{如果}\ x\geq 2^{\textnormal{IL-1}}-2^{-\textnormal{FL}},\\'
- en: \textnormal{Round}(x)\hskip 33.99998pt\ \textnormal{otherwise}\end{cases}" display="block"><semantics
    ><mrow  ><mrow ><mtext  >Convert</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi >x</mi><mo  >,</mo><mtext >[IL, FL]</mtext><mo
    stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><mrow  ><mo >−</mo><mrow ><msup ><mn >2</mn><mtext >IL-1</mtext></msup><mo lspace="0em"
    rspace="0em"  >​</mo><mtext >if</mtext><mo lspace="0.500em" rspace="0em"  >​</mo><mi
    >x</mi></mrow></mrow><mo >≤</mo><mrow ><mo >−</mo><msup ><mn >2</mn><mtext >IL-1</mtext></msup></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mrow  ><mrow
    ><msup ><mn >2</mn><mtext >IL-1</mtext></msup><mo >−</mo><mrow ><msup ><mn >2</mn><mrow
    ><mo >−</mo><mtext >FL</mtext></mrow></msup><mo lspace="0em" rspace="0em"  >​</mo><mtext
    >if</mtext><mo lspace="0.500em" rspace="0em"  >​</mo><mi >x</mi></mrow></mrow><mo
    >≥</mo><mrow ><msup ><mn >2</mn><mtext >IL-1</mtext></msup><mo >−</mo><msup ><mn
    >2</mn><mrow ><mo >−</mo><mtext >FL</mtext></mrow></msup></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mtext  >Round</mtext><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow><mo
    lspace="0.500em" rspace="0em" >​</mo><mtext >otherwise</mtext></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci ><mtext  >Convert</mtext></ci><interval
    closure="open"  ><ci >𝑥</ci><ci  ><mtext >[IL, FL]</mtext></ci></interval></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><apply ><apply  ><apply ><apply ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn><ci ><mtext mathsize="70%"  >IL-1</mtext></ci></apply><ci
    ><mtext >if</mtext></ci><ci >𝑥</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><cn
    type="integer"  >2</cn><ci ><mtext mathsize="70%"  >IL-1</mtext></ci></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><cn type="integer" >2</cn><ci ><mtext mathsize="70%" >IL-1</mtext></ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer" >2</cn><apply
    ><ci ><mtext mathsize="70%"  >FL</mtext></ci></apply></apply><ci ><mtext >if</mtext></ci><ci
    >𝑥</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><cn
    type="integer"  >2</cn><ci ><mtext mathsize="70%"  >IL-1</mtext></ci></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn><apply
    ><ci ><mtext mathsize="70%"  >FL</mtext></ci></apply></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><ci ><mtext  >Round</mtext></ci><ci >𝑥</ci><ci
    ><mtext  >otherwise</mtext></ci></apply><ci ><mtext >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\textnormal{Convert}(x,\textnormal{[IL, FL]})=\begin{cases}-2^{\textnormal{IL-1}}\hskip
    50.00008pt\textnormal{if}\ x\leq-2^{\textnormal{IL-1}},\\ 2^{\textnormal{IL-1}}-2^{-\textnormal{FL}}\hskip
    26.00009pt\textnormal{if}\ x\geq 2^{\textnormal{IL-1}}-2^{-\textnormal{FL}},\\
    \textnormal{Round}(x)\hskip 33.99998pt\ \textnormal{otherwise}\end{cases}</annotation></semantics></math>
    |  | (4.1) |
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: \textnormal{转换}(x,\textnormal{[IL, FL]})=\begin{cases}-2^{\textnormal{IL-1}}\hskip
    50.00008pt\textnormal{如果}\ x\leq-2^{\textnormal{IL-1}},\\ 2^{\textnormal{IL-1}}-2^{-\textnormal{FL}}\hskip
    26.00009pt\textnormal{如果}\ x\geq 2^{\textnormal{IL-1}}-2^{-\textnormal{FL}},\\
    \textnormal{四舍五入}(x)\hskip 33.99998pt\ \textnormal{否则}\end{cases}
- en: where
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里
- en: '|  | <math   alttext="\textnormal{Round}(x)=\begin{cases}\lfloor x\rfloor\hskip
    26.00009pt\textnormal{if}\ \lfloor x\rfloor\leq x\leq\lfloor x\rfloor+\frac{\epsilon}{2},\\
    \lfloor x\rfloor+\epsilon\hskip 10.00002pt\textnormal{if}\ \lfloor x\rfloor+\frac{\epsilon}{2}<x\leq\lfloor
    x\rfloor+\epsilon\\'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\textnormal{Round}(x)=\begin{cases}\lfloor x\rfloor\hskip
    26.00009pt\textnormal{if}\ \lfloor x\rfloor\leq x\leq\lfloor x\rfloor+\frac{\epsilon}{2},\\
    \lfloor x\rfloor+\epsilon\hskip 10.00002pt\textnormal{if}\ \lfloor x\rfloor+\frac{\epsilon}{2}<x\leq\lfloor
    x\rfloor+\epsilon\\'
- en: \end{cases}" display="block"><semantics ><mrow ><mrow  ><mtext >Round</mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><mrow  ><mrow ><mo stretchy="false"  >⌊</mo><mi >x</mi><mo stretchy="false" >⌋</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mtext >if</mtext><mo lspace="0.500em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >⌊</mo><mi >x</mi><mo stretchy="false" >⌋</mo></mrow></mrow><mo
    >≤</mo><mi >x</mi><mo >≤</mo><mrow ><mrow ><mo stretchy="false"  >⌊</mo><mi >x</mi><mo
    stretchy="false" >⌋</mo></mrow><mo >+</mo><mstyle displaystyle="false"  ><mfrac
    ><mi >ϵ</mi><mn >2</mn></mfrac></mstyle></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><mrow ><mo stretchy="false" >⌊</mo><mi
    >x</mi><mo stretchy="false" >⌋</mo></mrow><mo >+</mo><mrow ><mi >ϵ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mtext >if</mtext><mo lspace="0.500em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false"  >⌊</mo><mi >x</mi><mo stretchy="false" >⌋</mo></mrow></mrow><mo
    >+</mo><mstyle displaystyle="false"  ><mfrac ><mi >ϵ</mi><mn >2</mn></mfrac></mstyle></mrow><mo
    ><</mo><mi  >x</mi><mo >≤</mo><mrow ><mrow  ><mo stretchy="false"  >⌊</mo><mi
    >x</mi><mo stretchy="false" >⌋</mo></mrow><mo >+</mo><mi >ϵ</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci ><mtext  >Round</mtext></ci><ci
    >𝑥</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><apply ><apply  ><apply
    ><ci >𝑥</ci></apply><ci ><mtext >if</mtext></ci><apply ><ci >𝑥</ci></apply></apply><ci
    >𝑥</ci></apply><apply ><apply ><apply ><ci  >𝑥</ci></apply><apply ><ci >italic-ϵ</ci><cn
    type="integer"  >2</cn></apply></apply></apply></apply><ci ><mtext >otherwise</mtext></ci><apply
    ><apply ><apply  ><apply ><ci >𝑥</ci></apply><apply ><ci  >italic-ϵ</ci><ci ><mtext
    >if</mtext></ci><apply ><ci >𝑥</ci></apply></apply><apply ><ci >italic-ϵ</ci><cn
    type="integer" >2</cn></apply></apply><ci >𝑥</ci></apply><apply ><apply  ><apply
    ><ci >𝑥</ci></apply><ci >italic-ϵ</ci></apply></apply></apply><ci ><mtext >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\textnormal{Round}(x)=\begin{cases}\lfloor x\rfloor\hskip
    26.00009pt\textnormal{if}\ \lfloor x\rfloor\leq x\leq\lfloor x\rfloor+\frac{\epsilon}{2},\\
    \lfloor x\rfloor+\epsilon\hskip 10.00002pt\textnormal{if}\ \lfloor x\rfloor+\frac{\epsilon}{2}<x\leq\lfloor
    x\rfloor+\epsilon\\ \end{cases}</annotation></semantics></math> |  | (4.2) |
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><mrow  ><mtext >取整</mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mi  >x</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><mrow  ><mrow ><mo stretchy="false"  >⌊</mo><mi >x</mi><mo stretchy="false" >⌋</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mtext >如果</mtext><mo lspace="0.500em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >⌊</mo><mi >x</mi><mo stretchy="false" >⌋</mo></mrow></mrow><mo
    >≤</mo><mi >x</mi><mo >≤</mo><mrow ><mrow ><mo stretchy="false"  >⌊</mo><mi >x</mi><mo
    stretchy="false" >⌋</mo></mrow><mo >+</mo><mstyle displaystyle="false"  ><mfrac
    ><mi >ϵ</mi><mn >2</mn></mfrac></mstyle></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><mrow ><mo stretchy="false" >⌊</mo><mi
    >x</mi><mo stretchy="false" >⌋</mo></mrow><mo >+</mo><mrow ><mi >ϵ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mtext >如果</mtext><mo lspace="0.500em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false"  >⌊</mo><mi >x</mi><mo stretchy="false" >⌋</mo></mrow></mrow><mo
    >+</mo><mstyle displaystyle="false"  ><mfrac ><mi >ϵ</mi><mn >2</mn></mfrac></mstyle></mrow><mo
    ><</mo><mi  >x</mi><mo >≤</mo><mrow ><mrow  ><mo stretchy="false"  >⌊</mo><mi
    >x</mi><mo stretchy="false" >⌋</mo></mrow><mo >+</mo><mi >ϵ</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci ><mtext  >Round</mtext></ci><ci
    >𝑥</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply ><apply ><apply  ><apply
    ><ci >𝑥</ci></apply><ci ><mtext >如果</mtext></ci><apply ><ci >𝑥</ci></apply></apply><ci
    >𝑥</ci></apply><apply ><apply ><apply ><ci  >𝑥</ci></apply><apply ><ci >italic-ϵ</ci><cn
    type="integer"  >2</cn></apply></apply></apply></apply><ci ><mtext >否则</mtext></ci><apply
    ><apply ><apply  ><apply ><ci >𝑥</ci></apply><apply ><ci  >italic-ϵ</ci><ci ><mtext
    >如果</mtext></ci><apply ><ci >𝑥</ci></apply></apply><apply ><ci >italic-ϵ</ci><cn
    type="integer" >2</cn></apply></apply><ci >𝑥</ci></apply><apply ><apply  ><apply
    ><ci >𝑥</ci></apply><ci >italic-ϵ</ci></apply></apply></apply><ci ><mtext >否则</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\textnormal{Round}(x)=\begin{cases}\lfloor x\rfloor\hskip
    26.00009pt\textnormal{if}\ \lfloor x\rfloor\leq x\leq\lfloor x\rfloor+\frac{\epsilon}{2},\\
    \lfloor x\rfloor+\epsilon\hskip 10.00002pt\textnormal{if}\ \lfloor x\rfloor+\frac{\epsilon}{2}<x\leq\lfloor
    x\rfloor+\epsilon\\ \end{cases}</annotation></semantics></math> |  | (4.2) |
- en: where $\epsilon(=2^{-\textnormal{FL}})$ is the smallest positive number that
    can be represented in this fixed-point format, $\lfloor x\rfloor$ is defined as
    the largest integer multiple of $\epsilon$ smaller than $x$. Following this approach,
    more advanced approaches have been proposed (Rastegari et al., [2016](#bib.bib261);
    Polino et al., [2018](#bib.bib247); Wu et al., [2018a](#bib.bib358)).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon(=2^{-\textnormal{FL}})$ 是在这种定点格式中可以表示的最小正数，$\lfloor x\rfloor$ 被定义为小于
    $x$ 的最大 $\epsilon$ 的整数倍。沿用这种方法，已经提出了更先进的方法 (Rastegari 等人, [2016](#bib.bib261);
    Polino 等人, [2018](#bib.bib247); Wu 等人, [2018a](#bib.bib358))。
- en: In addition to scalar quantization for individual numbers in weight vectors,
    there is also a research line focusing on clustering-based quantization.  Gong
    et al. ([2014](#bib.bib98)) proposed to classify weights into groups and to use
    the centroid of each group to replace the actual weights during inference.  Han
    et al. ([2016](#bib.bib110)) employed a similar approach but fine-tuned the quantized
    centroids for better performance. Following this idea,  Choi et al. ([2017](#bib.bib49))
    further proposed a Hessian weighted k-means clustering approach.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对权重向量中的单个数字进行标量量化外，还有一个研究方向专注于基于聚类的量化。Gong 等人 ([2014](#bib.bib98)) 提出了将权重分类为多个组，并在推理过程中使用每个组的质心来替代实际权重。Han
    等人 ([2016](#bib.bib110)) 采用了类似的方法，但对量化的质心进行了微调以获得更好的性能。继此想法之后，Choi 等人 ([2017](#bib.bib49))
    进一步提出了一种Hessian加权k-means聚类方法。
- en: Stochastic Quantization
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 随机量化
- en: 'does not define one-to-one mapping from real weights to quantized weights.
    In random rounding, the quantized value is sampled from a discrete distribution
    parameterized by real values. For example, Courbariaux et al. ([2015](#bib.bib58))
    proposed the following random rounding function:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 不定义从真实权重到量化权重的一对一映射。在随机舍入中，量化值是从由真实值参数化的离散分布中采样得到的。例如，Courbariaux 等人 ([2015](#bib.bib58))
    提出了以下随机舍入函数：
- en: '|  | <math   alttext="x^{b}=\begin{cases}+1\quad\textnormal{with probability}\
    p=\sigma(x),\\ -1\quad\textnormal{with probability}\ 1-p\\'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="x^{b}=\begin{cases}+1\quad\textnormal{with probability}\
    p=\sigma(x),\\ -1\quad\textnormal{with probability}\ 1-p\\'
- en: \end{cases}" display="block"><semantics ><mrow ><msup  ><mi >x</mi><mi >b</mi></msup><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mrow ><mrow ><mrow ><mo >+</mo><mn >1</mn></mrow><mrow
    ><mtext >with probability</mtext><mo lspace="0.500em" rspace="0em"  >​</mo><mi
    >p</mi></mrow></mrow><mo >=</mo><mrow ><mi >σ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mrow ><mo  >−</mo><mn
    >1</mn></mrow><mrow ><mrow ><mtext >with probability</mtext><mo lspace="0em" rspace="0em"  >​</mo>
    <mn  >1</mn></mrow><mo >−</mo><mi >p</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑥</ci><ci  >𝑏</ci></apply><apply ><csymbol cd="latexml" >cases</csymbol><apply
    ><list ><apply ><cn type="integer" >1</cn></apply><apply ><ci ><mtext >with probability</mtext></ci><ci
    >𝑝</ci></apply></list><apply ><ci >𝜎</ci><ci >𝑥</ci></apply></apply><ci ><mtext
    >otherwise</mtext></ci><list ><apply  ><cn type="integer"  >1</cn></apply><apply
    ><apply ><ci ><mtext >with probability</mtext></ci><cn type="integer" >1</cn></apply><ci
    >𝑝</ci></apply></list><ci ><mtext >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >x^{b}=\begin{cases}+1\quad\textnormal{with probability}\
    p=\sigma(x),\\ -1\quad\textnormal{with probability}\ 1-p\\ \end{cases}</annotation></semantics></math>
    |  |
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><msup  ><mi >x</mi><mi >b</mi></msup><mo  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mrow ><mrow ><mrow ><mo >+</mo><mn >1</mn></mrow><mrow
    ><mtext >的概率</mtext><mo lspace="0.500em" rspace="0em"  >​</mo><mi >p</mi></mrow></mrow><mo
    >=</mo><mrow ><mi >σ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >x</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow ><mo  >−</mo><mn >1</mn></mrow><mrow ><mrow
    ><mtext >的概率</mtext><mo lspace="0em" rspace="0em"  >​</mo> <mn  >1</mn></mrow><mo
    >−</mo><mi >p</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑥</ci><ci  >𝑏</ci></apply><apply ><csymbol cd="latexml" >cases</csymbol><apply
    ><list ><apply ><cn type="integer" >1</cn></apply><apply ><ci ><mtext >的概率</mtext></ci><ci
    >𝑝</ci></apply></list><apply ><ci >𝜎</ci><ci >𝑥</ci></apply></apply><ci ><mtext
    >否则</mtext></ci><list ><apply  ><cn type="integer"  >1</cn></apply><apply ><apply
    ><ci ><mtext >的概率</mtext></ci><cn type="integer" >1</cn></apply><ci >𝑝</ci></apply></list><ci
    ><mtext >否则</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >x^{b}=\begin{cases}+1\quad\textnormal{with probability}\ p=\sigma(x),\\ -1\quad\textnormal{with
    probability}\ 1-p\\ \end{cases}</annotation></semantics></math> |  |
- en: 'where $\sigma$ is the “hard sigmoid” function:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 是“硬 sigmoid”函数：
- en: '|  | $\sigma(x)=\textnormal{clip}(\frac{x+1}{2},0,1)=\max(0,\min(1,\frac{x+1}{2}))$
    |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma(x)=\textnormal{clip}(\frac{x+1}{2},0,1)=\max(0,\min(1,\frac{x+1}{2}))$
    |  |'
- en: If $x$ is a positive value, we will have a high probability to quantize it to
    $+1$, otherwise to $-1$. This gives us a more flexible quantization scheme. In
    probabilistic quantization, the weights are assumed to be discretely distributed
    and a learning algorithm is used to infer the parameters of the distributions. Soudry
    et al. ([2014](#bib.bib303)) proposed an expectation back-propagation algorithm
    to train neural networks with binary or ternary weights. They first assumed some
    discrete prior distribution on the weights and then updated the weights in an
    online setting based on the Bayesian formula.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $x$ 是一个正值，我们将有很高的概率将其量化为 $+1$，否则为 $-1$。这为我们提供了一个更灵活的量化方案。在概率量化中，权重被假设为离散分布，并使用学习算法来推断分布的参数。Soudry
    等人 ([2014](#bib.bib303)) 提出了一个期望反向传播算法来训练具有二值或三值权重的神经网络。他们首先假设权重具有某种离散的先验分布，然后根据贝叶斯公式在在线设置中更新权重。
- en: Quantization-Aware Training
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 量化感知训练
- en: Recently, quantization-aware training (Jacob et al., [2018](#bib.bib142); Dong
    et al., [2017](#bib.bib71); Fan et al., [2020c](#bib.bib79)) has become the de
    facto approach towards designing robust quantized models. It simulated quantization
    effects in the forward pass of training and the backward pass was accomplished
    via straight through estimator (Bengio et al., [2013b](#bib.bib22)). It generally
    relied on techniques like gradient clipping to make the training stable. Recently,
    several studies analyzed and introduced new quantization-aware training approaches.
    For example, Fan et al. ([2020c](#bib.bib79)) and Dong et al. ([2017](#bib.bib71))
    stochastically applied quantization to a portion of the weights at each training
    step, while Sheng et al. ([2018](#bib.bib293)) and Alizadeh et al. ([2019](#bib.bib5))
    re-ordered the blocks or layers.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，量化感知训练（Jacob 等，[2018](#bib.bib142); Dong 等，[2017](#bib.bib71); Fan 等，[2020c](#bib.bib79)）已经成为设计鲁棒量化模型的实际方法。它在训练的前向传播中模拟了量化效应，后向传播则通过直接通过估计器（Bengio
    等，[2013b](#bib.bib22)）完成。它通常依赖于像梯度裁剪这样的技术来保持训练的稳定性。最近，几项研究分析并引入了新的量化感知训练方法。例如，Fan
    等（[2020c](#bib.bib79)）和 Dong 等（[2017](#bib.bib71)）在每个训练步骤中随机对一部分权重进行量化，而 Sheng
    等（[2018](#bib.bib293)）和 Alizadeh 等（[2019](#bib.bib5)）则重新排序了块或层。
- en: Applications
  id: totrans-323
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 应用
- en: Network quantization is first widely applied to CNNs. In addition to CNNs, quantization
    has been also applied to other models, like RNN, Transformer.  Ott et al. ([2016](#bib.bib234))
    first investigated RNN quantization and found that weight binarization did not
    work well on RNNs. For simplification, they proposed to apply weight quantization
    for RNN weights and to leave activations as floating-point numbers. Hubara et al.
    ([2017](#bib.bib137)) explored different combinations of bit-widths for weights
    and activations.  He et al. ([2016b](#bib.bib121)) proposed to quantize the structure
    of gates and interlinks in LSTM and GRU cells. Recently, Wang et al. ([2018b](#bib.bib343))
    proposed to use a threshold ternary quantization method for weight quantization
    and a Bernoulli ternary quantization method for activation quantization.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 网络量化首先广泛应用于 CNN。除了 CNN，量化也已应用于其他模型，如 RNN、Transformer。Ott 等（[2016](#bib.bib234)）首次研究了
    RNN 量化，并发现权重二值化在 RNN 上效果不佳。为了简化，他们建议对 RNN 权重进行量化，而将激活值保持为浮点数。Hubara 等（[2017](#bib.bib137)）探索了不同的权重和激活位宽组合。He
    等（[2016b](#bib.bib121)）提出量化 LSTM 和 GRU 单元中的门和连接结构。最近，Wang 等（[2018b](#bib.bib343)）提出使用阈值三值量化方法对权重进行量化，并使用伯努利三值量化方法对激活进行量化。
- en: With the recent success of Transformer, a number of studies investigated the
    application of quantization on Transformers. For example, Bhandare et al. ([2019](#bib.bib24))
    and Prato et al. ([2020](#bib.bib249)) showed that 8-bit quantization can successfully
    reduce the size of a Transformer-based model and accelerate inference without
    compromising translation quality. Recently, quantization has been applied on Transformer-based
    language models (Zafrir et al., [2019](#bib.bib386); Zhang et al., [2020](#bib.bib393);
    Bai et al., [2020](#bib.bib12); Kim et al., [2021](#bib.bib159)).  Zafrir et al.
    ([2019](#bib.bib386)) first applied 8-bit quantization on BERT. Following this
    work, Kim et al. ([2021](#bib.bib159)) proposed I-BERT, which employed lightweight
    integer-only approximation methods for nonlinear operations to quantize BERT.
     Zhang et al. ([2020](#bib.bib393)) proposed TernaryBERT, which ternarized the
    weights in a fine-tuned BERT model with both approximation-based and loss-aware
    ternarization methods.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Transformer 的近期成功，许多研究调查了量化在 Transformers 上的应用。例如，Bhandare 等（[2019](#bib.bib24)）和
    Prato 等（[2020](#bib.bib249)）表明，8 位量化可以成功减少基于 Transformer 的模型的大小并加速推理而不影响翻译质量。最近，量化已应用于基于
    Transformer 的语言模型（Zafrir 等，[2019](#bib.bib386); Zhang 等，[2020](#bib.bib393); Bai
    等，[2020](#bib.bib12); Kim 等，[2021](#bib.bib159)）。Zafrir 等（[2019](#bib.bib386)）首次在
    BERT 上应用了 8 位量化。在这项工作之后，Kim 等（[2021](#bib.bib159)）提出了 I-BERT，该方法采用轻量级整数近似方法对 BERT
    进行量化。Zhang 等（[2020](#bib.bib393)）提出了 TernaryBERT，通过近似基础和损失感知三值化方法对经过微调的 BERT 模型中的权重进行三值化。
- en: Very recently, several studies have investigated the application of quantization
    on GNNs.  Feng et al. ([2020](#bib.bib83)) proposed a GNN-tailored quantization
    algorithm, and used an automatic bit-selecting approach to pinpoint the most appropriate
    quantization bits.  Wang et al. ([2021a](#bib.bib339)) and Bahri et al. ([2020](#bib.bib11))
    further proposed binarized GNNs.  Tailor et al. ([2021](#bib.bib316)) proposed
    Degree-Quant, an architecture-agnostic method for quantization-aware training
    on graphs. Moreover, Zhao et al. ([2020](#bib.bib396)) proposed to use neural
    architecture search to find the optimal architecture coupled with the best quantisation
    strategy for different components in GNNs.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些研究调查了量化在图神经网络（GNNs）上的应用。Feng等人（[2020](#bib.bib83)）提出了一种针对GNN的量化算法，并使用自动位选择方法来确定最合适的量化位。Wang等人（[2021a](#bib.bib339)）和Bahri等人（[2020](#bib.bib11)）进一步提出了二值化的GNNs。Tailor等人（[2021](#bib.bib316)）提出了Degree-Quant，一种用于图上的量化感知训练的架构无关方法。此外，Zhao等人（[2020](#bib.bib396)）建议使用神经架构搜索来找到最优架构，并结合GNN中不同组件的最佳量化策略。
- en: Discussion
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 讨论
- en: Quantization is very effective for reducing the size of neural networks. However,
    post-training quantization often leads to non-neglieable performance drop. In
    contrast, quantization-aware training can effectively reduces the performance
    drop. Incorporating knowledge distillation for quantization can also improve the
    performance of quantized models. In this section, we give an overview of quantization.
    If you are interested in more details, please refer to surveys  (Guo, [2018](#bib.bib105);
    Gholami et al., [2021](#bib.bib92)).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 量化对于减少神经网络的大小非常有效。然而，训练后量化往往会导致不可忽视的性能下降。相比之下，量化感知训练可以有效减少性能下降。将知识蒸馏应用于量化也可以提升量化模型的性能。在本节中，我们对量化进行了概述。如果您对更多细节感兴趣，请参阅调研（Guo,
    [2018](#bib.bib105); Gholami et al., [2021](#bib.bib92)）。
- en: 4.4 Knowledge Distillation
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 知识蒸馏
- en: The idea of knowledge distillation (KD) is exploiting the knowledge inside a
    large trained “teacher” model to help the training of a “student” model (Bucila
    et al., [2006](#bib.bib29); Ba & Caruana, [2014](#bib.bib8); Hinton et al., [2015](#bib.bib127)).
    In this way, we can use a smaller student model to distill a trained model as
    a replacement for inference.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏（KD）的思想是利用大规模训练的“教师”模型中的知识来帮助“学生”模型的训练（Bucila et al., [2006](#bib.bib29);
    Ba & Caruana, [2014](#bib.bib8); Hinton et al., [2015](#bib.bib127)）。通过这种方式，我们可以使用较小的学生模型来蒸馏训练好的模型，作为推理的替代。
- en: 'The traditional KD solution is to minimize the difference between the output
    produced by the teacher model and that produced by the student model. Formally,
    given a labeled dataset $\mathcal{D}$ of $N$ samples $\mathcal{D}=\{\left(x_{1},y_{1}\right),\dots,\left(x_{N},y_{N}\right)\}$,
    we can write the loss function of the student network during the process of knowledge
    distillation as follows:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的知识蒸馏（KD）解决方案是最小化教师模型和学生模型产生的输出之间的差异。正式地，给定一个标记数据集$\mathcal{D}$，包含$N$个样本$\mathcal{D}=\{\left(x_{1},y_{1}\right),\dots,\left(x_{N},y_{N}\right)\}$，我们可以将学生网络在知识蒸馏过程中的损失函数写作如下：
- en: '|  | $\mathcal{L}_{S}\left(\mathcal{D};\theta_{S};\theta_{T}\right)=\frac{1}{N}\sum_{i=1}^{N}\left[\alpha\mathcal{L}_{\mathcal{T}}\left(y_{i},S\left(x_{i};\theta_{S}\right)\right)+\left(1-\alpha\right)\mathcal{L}_{\mathit{KD}}\left(T\left(x_{i};\theta_{T}\right),S\left(x_{i};\theta_{S}\right)\right)\right]$
    |  | (4.3) |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{S}\left(\mathcal{D};\theta_{S};\theta_{T}\right)=\frac{1}{N}\sum_{i=1}^{N}\left[\alpha\mathcal{L}_{\mathcal{T}}\left(y_{i},S\left(x_{i};\theta_{S}\right)\right)+\left(1-\alpha\right)\mathcal{L}_{\mathit{KD}}\left(T\left(x_{i};\theta_{T}\right),S\left(x_{i};\theta_{S}\right)\right)\right]$
    |  | (4.3) |'
- en: where $\alpha$ is a hyper-parameter to control the relative importance of the
    two terms; $\theta_{T}$ and $\theta_{S}$ are the parameters of teacher $T$ and
    student $S$, respectively. $\mathcal{L}_{\mathcal{T}}$ refers to the task-specific
    loss and $\mathcal{L}_{\mathit{KD}}$ refers to the knowledge distillation loss
    which measures the similarity of the student and the teacher.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$是一个超参数，用于控制两个项的相对重要性；$\theta_{T}$和$\theta_{S}$分别是教师模型$T$和学生模型$S$的参数。$\mathcal{L}_{\mathcal{T}}$指任务特定的损失，$\mathcal{L}_{\mathit{KD}}$指知识蒸馏损失，它衡量学生模型和教师模型之间的相似性。
- en: 'In general, KD exploits the knowledge from the teacher model to help train
    the student model by minimizing the discrepancy between the knowledge in the teacher
    model and that in the student model. According to the source of teacher knowledge,
    we can classify KD approaches into three categories: logits-based KD, feature-based
    KD, and relation-based KD. According to teacher types, we also can classify KD
    approaches into three categories: KD with static teacher, KD with multiple teachers,
    and KD with dynamic teacher.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，KD通过最小化教师模型与学生模型之间知识的差异，利用教师模型的知识来帮助训练学生模型。根据教师知识的来源，我们可以将KD方法分为三类：基于logits的KD、基于特征的KD和基于关系的KD。根据教师类型，我们还可以将KD方法分为三类：静态教师KD、多教师KD和动态教师KD。
- en: Logits-based KD   focuses on the output class distribution of the teacher model,
    also referred as “soft labels”. This is the vanilla form of knowledge distillation (Hinton
    et al., [2015](#bib.bib127); Ba & Caruana, [2014](#bib.bib8)). Soft targets generated
    by the teacher model provide much more information than hard targets. Therefore,
    training the student model to fit soft targets can help the student model generalize
    better like the teacher model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 基于logits的KD关注教师模型的输出类别分布，也称为“软标签”。这是知识蒸馏的基础形式（Hinton et al., [2015](#bib.bib127);
    Ba & Caruana, [2014](#bib.bib8)）。教师模型生成的软目标提供的信息远多于硬目标。因此，训练学生模型以适应软目标可以帮助学生模型更好地进行泛化，就像教师模型一样。
- en: 'Feature-based KD   exploits intermediate features to teach the student model,
    which is believed to be important for representation learning (Bengio et al.,
    [2013a](#bib.bib21)). The simplest solution is to minimize the distance between
    intermediate representation of each student layer and its corresponding teacher
    layer (Romero et al., [2015](#bib.bib266); Sun et al., [2019a](#bib.bib309); Aguilar
    et al., [2020](#bib.bib2)). It enables the student model to exploit richer information
    from the teacher model. Recently, a number of feature-based KD studies have been
    proposed. In summary, these studies mainly focused on two key factors: selection
    of intermediate representations, and distance metric. The intuition of investigating
    better intermediate representations is that the knowledge of teacher should be
    easy to learn for the student model (Zhang et al., [2017](#bib.bib395); Zagoruyko
    & Komodakis, [2017](#bib.bib387); Huang & Wang, [2017](#bib.bib136); Ahn et al.,
    [2019](#bib.bib4); Heo et al., [2019](#bib.bib125); Sun et al., [2019a](#bib.bib309);
    Aguilar et al., [2020](#bib.bib2)). For example,  Huang & Wang ([2017](#bib.bib136))
    proposed to match the distributions of neuron selectivity patterns between the
    teacher and the student models.  Kim et al. ([2018](#bib.bib157)) trained a paraphrase
    model as $\textnormal{TF}_{t}$ to extract transferable features from the teacher’s
    intermediate representations, and a translator model as $\textnormal{TF}_{s}$
    to map the student intermediate representation to teacher’s representations. As
    for distance metrics, $L_{2}$ distance is the most widely used distance metric.
    Besides, $L_{1}$ distance (Wang et al., [2019b](#bib.bib344)) and KL-divergence (Liu
    et al., [2019b](#bib.bib197); Aguilar et al., [2020](#bib.bib2)) are also used
    in previous KD approaches.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的KD利用中间特征来指导学生模型，这被认为对表示学习非常重要（Bengio et al., [2013a](#bib.bib21)）。最简单的解决方案是最小化每个学生层的中间表示与其对应的教师层之间的距离（Romero
    et al., [2015](#bib.bib266); Sun et al., [2019a](#bib.bib309); Aguilar et al.,
    [2020](#bib.bib2)）。这使得学生模型能够从教师模型中提取更丰富的信息。最近，提出了许多基于特征的KD研究。总的来说，这些研究主要关注两个关键因素：中间表示的选择和距离度量。研究更好的中间表示的直觉是教师的知识应该对学生模型来说易于学习（Zhang
    et al., [2017](#bib.bib395); Zagoruyko & Komodakis, [2017](#bib.bib387); Huang
    & Wang, [2017](#bib.bib136); Ahn et al., [2019](#bib.bib4); Heo et al., [2019](#bib.bib125);
    Sun et al., [2019a](#bib.bib309); Aguilar et al., [2020](#bib.bib2)）。例如，Huang
    & Wang ([2017](#bib.bib136)) 提出了匹配教师模型和学生模型之间的神经元选择模式的分布。Kim et al. ([2018](#bib.bib157))
    训练了一个改述模型作为 $\textnormal{TF}_{t}$ 以从教师的中间表示中提取可转移的特征，并训练了一个翻译模型作为 $\textnormal{TF}_{s}$
    以将学生的中间表示映射到教师的表示。至于距离度量，$L_{2}$ 距离是最广泛使用的距离度量。此外，$L_{1}$ 距离（Wang et al., [2019b](#bib.bib344)）和KL散度（Liu
    et al., [2019b](#bib.bib197); Aguilar et al., [2020](#bib.bib2)）也被用于之前的KD方法。
- en: Relation-based KD   aims to minimize the correlation between feature pairs from
    the teacher model and the student model (Yim et al., [2017](#bib.bib376); Srinivas
    & Fleuret, [2018](#bib.bib305); Lee et al., [2018](#bib.bib175); Tung & Mori,
    [2019](#bib.bib326); Lee & Song, [2019](#bib.bib176); Peng et al., [2019](#bib.bib238)).
    Distance can be seen a special kind of relation measure. Recently, many approaches
    have been proposed to explore better relation measures. For example, MHGD (Lee
    & Song, [2019](#bib.bib176)) employed a multi-head attention network to encode
    relations between any two feature maps in a certain batch of training instances.
    CCKD (Peng et al., [2019](#bib.bib238)) transferred correlation between instances
    with a generalized kernel method based on Taylor series expansion.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 基于关系的知识蒸馏（KD）旨在最小化教师模型和学生模型中特征对之间的相关性（Yim et al., [2017](#bib.bib376); Srinivas
    & Fleuret, [2018](#bib.bib305); Lee et al., [2018](#bib.bib175); Tung & Mori,
    [2019](#bib.bib326); Lee & Song, [2019](#bib.bib176); Peng et al., [2019](#bib.bib238)）。距离可以被视为一种特殊的关系度量。最近，许多方法被提出以探索更好的关系度量。例如，MHGD（Lee
    & Song, [2019](#bib.bib176)）采用了多头注意力网络来编码某一批训练实例中任意两个特征图之间的关系。CCKD（Peng et al.,
    [2019](#bib.bib238)）使用基于泰勒级数展开的广义核方法转移实例之间的相关性。
- en: KD with Multiple Teachers
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 多教师的知识蒸馏
- en: Traditional KD methods focus on transferring the knowledge from one teacher
    model to the student model. A number of recent studies investigated knowledge
    transfer from multiple teachers or an ensemble of teachers. The most popular and
    straightforward way is to learn the ensemble of teacher logits (Tarvainen & Valpola,
    [2017](#bib.bib323); You et al., [2017](#bib.bib380)). Following this idea, multiple
    studies have been proposed to model the diversity of teachers using a weighted
    average of teacher logits (Ruder et al., [2017](#bib.bib269); Lan et al., [2018](#bib.bib169);
    Xiang et al., [2020](#bib.bib362)). Apart from averaged logits, using the ensemble
    of features from multiple teachers is another line of research.  (Park & Kwak,
    [2019](#bib.bib237)) proposed to train the student’s feature map to minimize the
    gap from the feature maps of multiple teachers with different feature transformation
    functions. While achieving promising performance, traditional KD methods using
    multiple teachers suffer from expensive computations because they require multiple
    pre-trained teachers. To alleviate this problem, Zhang et al. ([2018](#bib.bib394))
    proposed a deep mutual learning approach, which was an initial form of online
    KD that has been developed by various studies (Lan et al., [2018](#bib.bib169);
    Anil et al., [2018](#bib.bib7); Chen et al., [2020a](#bib.bib35); Chung et al.,
    [2020](#bib.bib53); Kim et al., [2020](#bib.bib158)). In online KD, a set of student
    models, or peers, was trained simultaneously by learning from each other in a
    peer-teaching fashion.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的KD方法侧重于将知识从一个教师模型转移到学生模型。最近有很多研究探讨了从多个教师或教师集成中转移知识。最流行和直接的方法是学习教师logits的集成（Tarvainen
    & Valpola, [2017](#bib.bib323); You et al., [2017](#bib.bib380)）。沿着这一思路，多个研究提出了使用加权平均教师logits来建模教师的多样性（Ruder
    et al., [2017](#bib.bib269); Lan et al., [2018](#bib.bib169); Xiang et al., [2020](#bib.bib362)）。除了平均logits之外，使用来自多个教师的特征集成是另一条研究方向。（Park
    & Kwak, [2019](#bib.bib237)）建议训练学生的特征图，以最小化与具有不同特征变换函数的多个教师的特征图之间的差距。尽管取得了令人鼓舞的成果，但使用多个教师的传统KD方法由于需要多个预训练教师，计算成本昂贵。为了解决这个问题，Zhang
    et al. ([2018](#bib.bib394)) 提出了深度互学习方法，这是一种在线KD的初步形式，已被各种研究发展（Lan et al., [2018](#bib.bib169);
    Anil et al., [2018](#bib.bib7); Chen et al., [2020a](#bib.bib35); Chung et al.,
    [2020](#bib.bib53); Kim et al., [2020](#bib.bib158)）。在在线KD中，一组学生模型或同伴通过彼此之间的同伴教学方式进行同时训练。
- en: KD with Dynamic Teacher
  id: totrans-340
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 动态教师的知识蒸馏
- en: In traditional KD, the teacher model is fixed during KD. However, this can be
    sub-optimal because the generalization ability of the student model is dynamic
    during training. A number of studies explored KD with an evolving teacher model
    to keep a reasonable capacity difference between student and teacher (Mirzadeh
    et al., [2020](#bib.bib222); Shi et al., [2021](#bib.bib295); Zhou et al., [2021b](#bib.bib401);
    Park et al., [2021](#bib.bib235)). For example,  Jin et al. ([2019](#bib.bib148))
    designed a sequence of intermediate targets to impose curriculum-based constraint
    on the optimization path of the student model for improved KD.  Shi et al. ([2021](#bib.bib295))
    and Park et al. ([2021](#bib.bib235)) proposed to jointly update the teacher model
    and the student model with task specific objectives during KD.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的知识蒸馏（KD）中，教师模型在蒸馏过程中是固定的。然而，这可能会导致次优效果，因为学生模型在训练过程中其泛化能力是动态变化的。一些研究探索了使用不断演变的教师模型来保持学生模型与教师模型之间的合理容量差异（Mirzadeh
    et al., [2020](#bib.bib222); Shi et al., [2021](#bib.bib295); Zhou et al., [2021b](#bib.bib401);
    Park et al., [2021](#bib.bib235)）。例如，Jin et al. ([2019](#bib.bib148)) 设计了一系列中间目标，以对学生模型的优化路径施加基于课程的约束，从而改进知识蒸馏。Shi
    et al. ([2021](#bib.bib295)) 和 Park et al. ([2021](#bib.bib235)) 提出了在知识蒸馏过程中，联合更新教师模型和学生模型，以实现任务特定目标。
- en: Discussion
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 讨论
- en: Knowledge distillation is a widely-used approach to get a smaller but more competitive
    model. However, although the idea of KD is easy to implement, it also has several
    limitations. First, the performance of KD is very sensitive to the size gap between
    the teacher model and the student model. The discrepancy between the expressive
    power of the models would make it hard to teach the student model. Second, it
    relies on training data and may not be suitable for few-shot or zero-shot settings.
    In addition, recent studies (Xu et al., [2021a](#bib.bib366); Stanton et al.,
    [2021](#bib.bib306)) have revealed that while knowledge distillation can effectively
    improve student generalization, there was still a large discrepancy between the
    predictive distributions of student and teacher models. It means that there is
    still a long way to distill full knowledge in a teacher model to a student model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏是一种广泛使用的方法，用于获得一个更小但更具竞争力的模型。然而，尽管知识蒸馏的思想容易实现，它也有几个局限性。首先，知识蒸馏的性能对教师模型和学生模型之间的大小差距非常敏感。模型表达能力之间的差异会使得教导学生模型变得困难。其次，它依赖于训练数据，可能不适用于少样本或零样本设置。此外，最近的研究（Xu
    et al., [2021a](#bib.bib366); Stanton et al., [2021](#bib.bib306)）揭示了尽管知识蒸馏可以有效提高学生模型的泛化能力，但学生模型和教师模型的预测分布之间仍存在较大差异。这意味着将教师模型中的全部知识蒸馏到学生模型中还有很长的路要走。
- en: Chapter 5 Efficient Data Usage
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章 高效数据使用
- en: Following the definition of Green deep learning, this chapter mainly explores
    how to achieve competitive results with fewer data resources, including active
    learning and pre-training. It is worth noticing that although pre-trained models
    take massive computations during pre-training, they are widely believed to be
    a practical solution to release the burden of data in downstream tasks. Therefore,
    we also include pre-trained models in this chapter.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 根据绿色深度学习的定义，本章主要探讨如何在数据资源较少的情况下实现具有竞争力的结果，包括主动学习和预训练。值得注意的是，尽管预训练模型在预训练过程中需要大量计算，但它们被广泛认为是一种释放下游任务数据负担的实际解决方案。因此，我们也将预训练模型纳入本章讨论。
- en: '{forest}'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,align=left,
    minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt, inner ysep=1pt,
    , where level=1font=, where level=2font=,where level=3font=, where level=4font=,
    where level=5font=, [Green Data Usage [Active Learning [Uncertainty-based] [Diversity-based]
    [Expected Model Change] ] [Pre-training as Few-shot Learners [Self-supervised
    Pre-training] [Contrastive Pre-training] [Prompt Pre-training] ] ]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=hiddendraw, rounded corners,
    align=left, minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt,
    inner ysep=1pt, , where level=1font=, where level=2font=, where level=3font=,
    where level=4font=, where level=5font=, [绿色数据使用 [主动学习 [基于不确定性] [基于多样性] [期望模型变化]
    ] [作为少样本学习者的预训练 [自监督预训练] [对比预训练] [提示预训练] ] ]
- en: 'Figure 5.1: Taxonomy of efficient data usage methods with representative examples.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：高效数据使用方法的分类，带有代表性示例。
- en: 5.1 Active Learning
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主动学习
- en: Active learning is a research direction aiming at using as few samples as possibles
    to achieve good results. It is initially proposed to reduce annotation costs.
    Nowadays, pool-based active learning is widely-used to reduce training costs by
    selecting the most useful examples to train networks. The intuitive behind active
    learning is quite simple. The annotated training data do not equally contribute
    to the final performance. If we can always select the most useful example to train
    models, the wasted training on unimportant examples can be largely reduced.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习是一种研究方向，旨在利用尽可能少的样本实现良好的结果。它最初提出是为了减少标注成本。如今，基于池的主动学习被广泛应用，通过选择最有用的例子来训练网络，从而减少训练成本。主动学习的直观理解非常简单。标注的训练数据对最终性能的贡献是不均等的。如果我们能够始终选择最有用的例子来训练模型，那么对不重要的例子的浪费训练可以大大减少。
- en: 'Active learning usually starts from a randomly-initialized model or a pre-trained
    model. It defines several query strategies to select new unlabeled data to query
    annotation. The new data associated with labels are then used to train the model.
    This process keeps running until it reaches the maximum number of labelled data
    or other termination conditions are satisfied. Previous active learning approaches
    mainly focused on query strategies to improve performance. Following the work
    of Yoo & Kweon ([2019](#bib.bib378)), given a pool of unlabeled data, we classify
    active learning approaches into three categories according to the selection criteria:
    uncertainty-based approaches, diversity-based approaches, and expected-model-change
    based approaches.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习通常从一个随机初始化的模型或一个预训练的模型开始。它定义了几种查询策略来选择新的未标记数据以查询标注。随后，将与标签相关的新数据用于训练模型。这个过程持续进行，直到达到标记数据的最大数量或满足其他终止条件为止。之前的主动学习方法主要集中于查询策略以提高性能。根据
    Yoo & Kweon（[2019](#bib.bib378)）的工作，给定一个未标记数据池，我们将主动学习方法根据选择标准分为三类：基于不确定性的方法、基于多样性的方法和基于期望模型变化的方法。
- en: Uncertainty-based active learning is used to be the most common choice in active
    learning, using uncertainty scores to select data (Ranganathan et al., [2017](#bib.bib260);
    Schröder et al., [2021](#bib.bib282); He et al., [2019](#bib.bib122); Shen et al.,
    [2018](#bib.bib291)). The simplest solution is to utilize class posterior probabilities
    to define uncertainty. To be specific,  Lewis & Gale ([1994](#bib.bib179)) used
    the probability of a predicted class as uncertainty score.  Joshi et al. ([2009](#bib.bib151))
    defined an entropy of class posterior probabilities as uncertainty score. Another
    research line is to train multiple models to construct a committee, and used the
    committee to evaluate uncertainty (Beluch et al., [2018](#bib.bib17)). Recently,
    uncertainty-based active learning has been widely applied to various fields.  Ranganathan
    et al. ([2017](#bib.bib260)) applied active learning on image classification which
    selected the most informative unlabeled samples to train a deep belief network
    model.  Shen et al. ([2018](#bib.bib291)) applied uncertainty-based active learning
    to sequence tagging. They selected sentences for which the length-normalized log
    probability of the current prediction was the lowest.  Ducoffe & Precioso ([2018](#bib.bib72))
    focused on margin-based active learning for deep networks. Despite promising effectiveness,
    uncertainty-based sampling can easily lead to insufficient diversity of batch
    query samples.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不确定性的主动学习曾经是主动学习中最常见的选择，利用不确定性分数来选择数据（Ranganathan 等，[2017](#bib.bib260); Schröder
    等，[2021](#bib.bib282); He 等，[2019](#bib.bib122); Shen 等，[2018](#bib.bib291)）。最简单的解决方案是利用类别后验概率来定义不确定性。具体来说，Lewis
    & Gale（[1994](#bib.bib179)）使用预测类别的概率作为不确定性分数。Joshi 等（[2009](#bib.bib151)）将类别后验概率的熵定义为不确定性分数。另一项研究是训练多个模型以构建一个委员会，并使用该委员会来评估不确定性（Beluch
    等，[2018](#bib.bib17)）。最近，基于不确定性的主动学习已被广泛应用于各个领域。Ranganathan 等（[2017](#bib.bib260)）将主动学习应用于图像分类，选择最具信息量的未标记样本来训练深度置信网络模型。Shen
    等（[2018](#bib.bib291)）将基于不确定性的主动学习应用于序列标注。他们选择了当前预测的长度归一化对数概率最低的句子。Ducoffe & Precioso（[2018](#bib.bib72)）专注于深度网络的基于边际的主动学习。尽管效果颇具前景，但基于不确定性的采样容易导致批量查询样本的多样性不足。
- en: Diversity-based active learning has been proposed to address the challenges
    of uncertainty based approaches. For example,  Sener & Savarese ([2018](#bib.bib288))
    defined a core-set to estimate the whole training data.  Nguyen & Smeulders ([2004](#bib.bib230))
    proposed to incorporate clustering into active learning. It first constructed
    a classifier on the set of the cluster representatives, and then propagated the
    classification decision to the other samples via a local noise model.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 基于多样性的主动学习被提出用来解决不确定性方法面临的挑战。例如，Sener 和 Savarese ([2018](#bib.bib288)) 定义了一个核心集来估计整个训练数据。Nguyen
    和 Smeulders ([2004](#bib.bib230)) 提出了将聚类方法引入主动学习。他们首先在簇代表的集合上构建分类器，然后通过局部噪声模型将分类决策传播到其他样本。
- en: Expected-model-change based active learning selected data points that would
    cause the greatest change to current model. For example,  Roy & McCallum ([2001](#bib.bib267))
    selected examples according to the reduced error rate on future test examples.
     Freytag et al. ([2014](#bib.bib89)) measured the expected change of model outputs
    and incorporated the underlying data distribution. For each example of an unlabeled
    set, the expected change of model predictions was calculated and marginalized
    over the unknown label.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 基于期望模型变化的主动学习选择那些能够对当前模型造成最大变化的数据点。例如，Roy 和 McCallum ([2001](#bib.bib267)) 根据未来测试样本的误差率降低来选择样本。Freytag
    等人 ([2014](#bib.bib89)) 测量了模型输出的预期变化，并考虑了数据的基础分布。对于每个未标记集中的样本，计算了模型预测的预期变化，并对未知标签进行了边际化处理。
- en: 5.2 Pre-training as Few-shot Learners
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 作为少样本学习者的预训练
- en: It is widely-believed that pre-trained models as initialization is an effective
    approach to reduce data requirements in downstream tasks. In this survey, we describe
    widely-used pre-trained models.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛认为，使用预训练模型作为初始化是一种减少下游任务数据需求的有效方法。在本调查中，我们描述了广泛使用的预训练模型。
- en: 5.2.1 Self-supervised Learning
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 自监督学习
- en: self-supervised learning (SSL) is the most popular solution to get pre-trained
    models. We take NLP as an example to review recent progress of self-supervised
    learning. Self-supervised objectives can be classified into types including masked
    language modeling (MLM), language modeling (LM), de-nosing auto-encoding (DAE).
    In this survey, we give an overview of these models. We refer the reader to Qiu
    et al. ([2020](#bib.bib253)) and Han et al. ([2021a](#bib.bib111)) for more details
    of pre-trained networks.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习 (SSL) 是获得预训练模型的最流行的解决方案。我们以 NLP 为例，回顾自监督学习的最新进展。自监督目标可以分类为包括掩码语言建模 (MLM)、语言建模
    (LM)、去噪自编码 (DAE) 等类型。在本调查中，我们概述了这些模型。有关预训练网络的更多细节，请参阅 Qiu 等人 ([2020](#bib.bib253))
    和 Han 等人 ([2021a](#bib.bib111))。
- en: Masked Language Modeling
  id: totrans-359
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 掩码语言建模
- en: Devlin et al. ([2019](#bib.bib65)) proposed to pre-train an Transformer encoder,
    i.e., BERT, via MLM objective on unlabeled text. MLM builds a corrupted token
    sequence where 15% tokens are randomly sampled and replaced with a special token
    [MASK], then requires the Transformer to predict the original tokens. Formally,
    given a sequence $\bm{x}_{1:L}=[x_{1},x_{2},\cdots,x_{L}]$ and the masked token
    set $\mathcal{M}$, the MLM objective can be formulated as
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Devlin 等人 ([2019](#bib.bib65)) 提出了通过 MLM 目标在未标记文本上预训练一个 Transformer 编码器，即 BERT。MLM
    构建了一个损坏的标记序列，其中 15% 的标记被随机抽取并用特殊标记 [MASK] 替换，然后要求 Transformer 预测原始标记。形式上，给定一个序列
    $\bm{x}_{1:L}=[x_{1},x_{2},\cdots,x_{L}]$ 和被掩码的标记集 $\mathcal{M}$，MLM 目标可以被公式化为
- en: '|  | $\mathcal{L}_{\text{MLM}}=-\sum_{x_{t}\in\mathcal{M}}\log p(x_{t}&#124;\bm{x}_{\setminus\mathcal{M}})$
    |  | (5.1) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{MLM}}=-\sum_{x_{t}\in\mathcal{M}}\log p(x_{t}&#124;\bm{x}_{\setminus\mathcal{M}})$
    |  | (5.1) |'
- en: where $\bm{x}_{\setminus\mathcal{M}}$ indicates the unmasked part of the input
    sequence. With the inherited knowledge, fine-tuned BERT performs well compared
    to baselines without pre-training on many classification tasks, including single
    sentence classification (Warstadt et al., [2019](#bib.bib353); Socher et al.,
    [2013](#bib.bib301)), sentence pair classification or matching (Dolan & Brockett,
    [2005](#bib.bib66); Cer et al., [2017](#bib.bib34)), natural language inference (Williams
    et al., [2018](#bib.bib355); Wang et al., [2019a](#bib.bib337); Bentivogli et al.,
    [2009](#bib.bib23); Levesque et al., [2012](#bib.bib178)), and question answering (Rajpurkar
    et al., [2016](#bib.bib257), [2018](#bib.bib258)), etc.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Several studies have made effort to improve MLM through developing more effective
    MLM-like objectives or exploring more efficient training tricks. SpanBERT (Joshi
    et al., [2020](#bib.bib153)), ERNIE (Sun et al., [2019b](#bib.bib311)), and BERT-WWM (Cui
    et al., [2019](#bib.bib59)) proposed to mask adjacent tokens. ELECTRA (Clark et al.,
    [2020](#bib.bib54)) proposed a GAN-like replaced token prediction objective which
    required discriminator to discriminate whether a token is replaced or not. Instead
    of masking, StructBERT (Wang et al., [2020b](#bib.bib346)) learned to predict
    the shuffled span in its original order, which incorporated language structures
    into pre-training.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Language Modeling
  id: totrans-364
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: predicts next tokens one by one in an auto-regressive way. Specifically, given
    a sequence $\bm{x}_{1:L}=[x_{1},x_{2},\cdots,x_{L}]$, the joint probability of
    $\bm{x}$ can be written as
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(\bm{x}_{1:L})=\prod_{t=1}^{L}p(x_{t}&#124;\bm{x}_{0:t-1})$ |  | (5.2)
    |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathcal{L}_{\text{LM}}=-\sum_{t}\log p(x_{t}&#124;\bm{x}_{<t})$ |  |
    (5.3) |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: while $x_{0}$ is the special [BOS] token which indicates the beginning of a
    sentence. LM is usually implemented with a Transformer decoder. GPT-2 (Radford
    et al., [2019](#bib.bib254)), and GPT-3 (Brown et al., [2020](#bib.bib28)) are
    two representative LM-based pre-trained models. The LM objective directly models
    the probability of next token given its left context. LM-based pre-trained models
    as initialization can largely improve conditional natural language generation
    tasks like summarization (Rush et al., [2015](#bib.bib272); See et al., [2017](#bib.bib287))
    and question answering (Rajpurkar et al., [2018](#bib.bib258); Reddy et al., [2019](#bib.bib263)).
    Therefore, LM models usually are used to initialize generative models.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of follow-up studies improving the original LM objective from
    various aspects. For example, inspired by the fact that neural networks could
    have different reading orders with human, XLNet (Yang et al., [2019](#bib.bib375))
    proposed to use permuted language modeling to predict a sentence in permuted orders
    in an auto-regressive way. UniLM (Dong et al., [2019](#bib.bib68)) proposed a
    prefix LM where next tokens are predicted in an auto-regressive way with all context
    tokens visible to each other. ProphetNet (Qi et al., [2020](#bib.bib251)) introduced
    a future $n$-gram prediction objective to predict the next $n$ tokens simultaneously
    based on previous context at each time step, through a multi-stream attention
    similar to XLNet.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: De-nosing Auto-encoding
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: requires a model to recover the original sentence based on the corrupted version.
    Formally, DAE-like objectives can be formulated as
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{DAE}}=-\sum_{t}\log p(x_{t}&#124;\bm{\hat{x}},\bm{x}_{<t})$
    |  | (5.4) |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: where $\bm{\hat{x}}$ is the corrupted input. By learning to distinguish which
    part of content is corrupted in a text and recovering it in a natural order, DAE-based
    pre-trained models as initialization can improve various models, including language
    understanding and language generation.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Actually, there are also flexible variants to corrupt a sequence, such as shuffling,
    masking, dropping, rotating, etc. BART (Lewis et al., [2020](#bib.bib180)) corrupted
    its input sequence with arbitrary noise transformations. The transformations include
    token masking, token deletion, sentence permutation, document rotation, and text
    infilling. More recently, Zhou et al. ([2021a](#bib.bib400)) proposed to use text
    rewriting instead of text infilling (e.g., BART, T5) for improving seq2seq pre-trained
    transformers.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual Objectives
  id: totrans-375
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another line of SSL objectives is multilingual SSL objectives. For example,
    mBERT (Devlin et al., [2019](#bib.bib65)) ¹¹1[https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md),
    mBART (Liu et al., [2020b](#bib.bib204)), mT5 (Xue et al., [2021](#bib.bib371))
    pre-trained the multilingual version BERT, BART, T5 via the multilingual masked
    language modeling objective. These models highly improve the results of few-shot
    or low-shot multilingual learning or multilingual generation tasks. In addition
    to these simple variants, recent researchers also designed more sophisticated
    cross-lingual objectives, such as cross-lingual word recovery, cross-lingual paraphrase
    classification, and cross-lingual masked language modeling.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Contrastive Learning
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Contrastive learning focuses on pair-wise relationships, aiming at learning
    closer or similar representation for positive samples while pushing negative samples
    away. The recent years have witnessed the rapid progress of contrastive-based
    pre-trained models, especially in CV (van den Oord et al., [2018](#bib.bib328);
    He et al., [2020](#bib.bib120); Chen et al., [2020d](#bib.bib40), [2021b](#bib.bib45)).
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: The standard contrastive learning utilizes positive and negative pairs at the
    same time to construct its objective. This kind of loss can back to noise contrastive
    estimation (NCE) loss (Gutmann & Hyvärinen, [2010](#bib.bib107)) that is defined
    as
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{NCE}}=-\log\frac{\exp(f(q,k^{+})/\tau)}{\exp(f(q,k^{+})/\tau)+\exp(f(q,k^{-})/\tau)}$
    |  | (5.5) |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: 'where $q$ is the anchor sample; $k^{+}$ and $k^{-}$ are its positive sample
    and negative sample; $q$, $k^{+}$ and $k^{-}$ are vectors generated by a neural
    network; $f(\cdot,\cdot)$ is a similarity function; $\tau$ is the temperature
    controlling the concentration of the induced distribution. When more than one
    negative samples exist, the NCE loss becomes the InfoNCE loss:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{InfoNCE}}=-\log\frac{\exp(f(q,k^{+})/\tau)}{\exp(f(q,k^{+})/\tau)+\sum_{i=1}^{K}\exp(f(q,k_{i})/\tau)}$
    |  | (5.6) |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: 'where $k_{i}$ represents the $i$-th negative sample of anchor/query sample
    $q$; $K$ is the size of negative samples. van den Oord et al. ([2018](#bib.bib328))
    proved that minimizing the InfoNCE loss was equivalent to maximizing the lower
    bound of mutual information between $q$ and $k^{+}$:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I(q,k^{+})\geq\log(K)-\mathcal{L}_{\text{InfoNCE}}$ |  | (5.7) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: where the more negative samples are, the tighter the lower bound is.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Overall, contrastive losses are easy to implement and contrastive-based pre-trained
    models generally have strong transfer ability. In the CV field, it largely improve
    downstream tasks, such as ImageNet classification (Deng et al., [2009](#bib.bib62)),
    object detection (Everingham et al., [2010](#bib.bib76)), and egmentation (Lin
    et al., [2014](#bib.bib192)), etc. MoCo (He et al., [2020](#bib.bib120); Chen
    et al., [2020e](#bib.bib44), [2021b](#bib.bib45)) is one of representative models
    which applied advanced contrastive learning methods to train pre-trained models
    on CV fields.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Existing state-of-the-art contrastive-based pre-trained models are simple variants
    of Eq. [5.6](#Ch5.E6 "In 5.2.2 Contrastive Learning ‣ 5.2 Pre-training as Few-shot
    Learners ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep Learning")
    where positive pairs and negative paris are required. For example, Deep InfoMax
    applied contrastive losses to learn image representations via maximizing the mutual
    information between local patches and the whole image; CMC (Tian et al., [2020](#bib.bib325))
    applied contrastive losses to learn representations where different views of the
    same scene or instance were sampled as positive pairs. According to Wang & Isola
    ([2020](#bib.bib345)), the InfoNCE loss optimized two properties of a representation
    space, including the alignment of representations between positive samples and
    the uniformity of the induced distribution of normalized representations on a
    hypersphere in the representation space. Nevertheless, the recent studies such
    as BYOL (Grill et al., [2020](#bib.bib102)), SwAV (Caron et al., [2020](#bib.bib33)),
    Simsiam (Chen & He, [2021](#bib.bib43)) found that contrastive learning also works
    without negative samples. In all, the research of contrastive-based pre-training
    is still in rapid development. More understanding studies are expected to better
    explain how contrastive losses work.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的最先进对比基础预训练模型是 Eq. [5.6](#Ch5.E6 "In 5.2.2 Contrastive Learning ‣ 5.2 Pre-training
    as Few-shot Learners ‣ Chapter 5 Efficient Data Usage ‣ A Survey on Green Deep
    Learning") 的简单变体，其中需要正样本对和负样本对。例如，Deep InfoMax 通过最大化局部补丁与整个图像之间的互信息，应用对比损失来学习图像表示；CMC
    (Tian et al., [2020](#bib.bib325)) 应用对比损失来学习表示，其中同一场景或实例的不同视角被采样为正样本对。根据 Wang
    & Isola ([2020](#bib.bib345))，InfoNCE 损失优化了表示空间的两个属性，包括正样本之间的表示对齐和在表示空间中标准化表示的诱导分布的均匀性。然而，最近的研究如
    BYOL (Grill et al., [2020](#bib.bib102))、SwAV (Caron et al., [2020](#bib.bib33))、Simsiam
    (Chen & He, [2021](#bib.bib43)) 发现对比学习在没有负样本的情况下也能有效。在整体上，对比基础预训练的研究仍在快速发展中。更多的理解研究有望更好地解释对比损失的作用。
- en: Previous studies also applied contrastive learning to NLP tasks. For example,
    CAPT (Luo et al., [2020](#bib.bib208)) taken a sentence $s_{i}$ and its masked
    version $\tilde{s}_{i}$ as a positive pair. CERT (Fang & Xie, [2020](#bib.bib80))
    utilized back-translation to generate noised positive samples for the source English
    sentence; CLEAR (Wu et al., [2020](#bib.bib360)) directly integrated four text
    augmentation mechanisms including word deletion, span deletion, reordering, and
    substitution, and taken two augmented versions of an sentence as positive pairs;
    DeCLUTR (Giorgi et al., [2021](#bib.bib94)) samples nearby even overlapping spans
    as positive pairs; SimCSE (Gao et al., [2021](#bib.bib90)) added noise to encoded
    representations via dropout and regard noised representations as positive samples.
    SimCSE performed pertty well on retrieval tasks and achieved new SoTA on semantic
    textual similarity tasks.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究也将对比学习应用于自然语言处理任务。例如，CAPT (Luo et al., [2020](#bib.bib208)) 将一个句子 $s_{i}$
    及其掩码版本 $\tilde{s}_{i}$ 视为正样本对。CERT (Fang & Xie, [2020](#bib.bib80)) 利用回译生成源英语句子的噪声正样本；CLEAR
    (Wu et al., [2020](#bib.bib360)) 直接整合了四种文本增强机制，包括词删除、跨度删除、重排和替换，并将两个增强版本的句子视为正样本对；DeCLUTR
    (Giorgi et al., [2021](#bib.bib94)) 采样附近甚至重叠的跨度作为正样本对；SimCSE (Gao et al., [2021](#bib.bib90))
    通过丢弃法向编码表示添加噪声，并将带噪声的表示视为正样本。SimCSE 在检索任务中表现相当出色，并在语义文本相似性任务中取得了新的**SoTA**。
- en: 5.2.3 Prompt Learning
  id: totrans-389
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 提示学习
- en: As the size of pre-trained models grows rapidly, fine-tuning the super large
    pre-trained model usually requires massive data to get better generalization ability,
    thus failing on few-shot/zero-shot applications. The objective gap between pre-training
    and fine-tuning stages are one of important reasons behind failure. Therefore,
    to improve data efficiency, prompt learning is proposed by extracting the similar/same
    template for pre-training and fine-tuning stages.  Scao & Rush ([2021](#bib.bib280))
    demonstrated that a prompt may be equal to 100 conventional data points, indicating
    that prompts can greatly improve the sample efficiency.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预训练模型规模的快速增长，对超大预训练模型进行微调通常需要大量数据以获得更好的泛化能力，从而在少样本/零样本应用中失败。预训练和微调阶段之间的目标差距是导致失败的重要原因之一。因此，为了提高数据效率，提出了通过提取预训练和微调阶段相似/相同的模板来进行提示学习。Scao
    & Rush ([2021](#bib.bib280)) 证明了一个提示可能等于100个传统数据点，表明提示可以显著提高样本效率。
- en: In the recent survey about prompt learning (Liu et al., [2021a](#bib.bib199)),
    prompt is defined as $\bm{x}^{\prime}$ = $f_{\text{prompt}}(\bm{x})$, where $f_{\text{prompt}}(\cdot)$
    is the prompting function to rewrite the input $\bm{x}$ with an task-specific
    template. Suppose that we have a review “I dozed off when watching this movie.”
    as input $\bm{x}$ associated with sentiment label $\bm{y}$. A prompt example $\bm{x}^{\prime}$
    is “I dozed off when watching this movie. It is so [MASK]”. In this way, we transfer
    sentiment prediction to a masked language modeling task. As pointed by Wei et al.
    ([2021](#bib.bib354)), language models at scale like GPT-3 (Brown et al., [2020](#bib.bib28))
    contain substantial world knowledge and can perform a range of NLP tasks, which
    makes large pre-trained models a kind of “neural knowledge bases” (Petroni et al.,
    [2019](#bib.bib242)). In fact, prompts can be seen as queries for those neural
    knowledge bases. The difficulty is (1) how to generate appropriate queries to
    achieve your goals, and (2) how to understand or interpret the response in the
    format of predicted textual content.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近关于提示学习的调查中（Liu et al., [2021a](#bib.bib199)），提示被定义为 $\bm{x}^{\prime}$ = $f_{\text{prompt}}(\bm{x})$，其中
    $f_{\text{prompt}}(\cdot)$ 是用于用任务特定模板重写输入 $\bm{x}$ 的提示函数。假设我们有一个评论 “我看这部电影时打了瞌睡。”
    作为输入 $\bm{x}$ 和情感标签 $\bm{y}$。一个提示示例 $\bm{x}^{\prime}$ 是 “我看这部电影时打了瞌睡。它是如此 [MASK]”。通过这种方式，我们将情感预测转化为掩码语言建模任务。正如
    Wei et al. ([2021](#bib.bib354)) 所指出的，大规模语言模型如 GPT-3（Brown et al., [2020](#bib.bib28)）包含大量的世界知识，并能执行各种
    NLP 任务，这使得大型预训练模型成为一种“神经知识库”（Petroni et al., [2019](#bib.bib242)）。实际上，提示可以被看作是对这些神经知识库的查询。难点在于（1）如何生成合适的查询以实现你的目标，以及（2）如何理解或解释以预测文本内容格式呈现的响应。
- en: Generally, prompts are defined as discrete templates. In addition, there are
    also studies focusing on continuous prompts, rather than textual prompts. For
    example, Prefix Tuning (Li & Liang, [2021](#bib.bib185)) and P-tuning (Liu et al.,
    [2021c](#bib.bib203)) defined virtual tokens where only parameters of virtual
    tokens were fine-tuned. The more recent P-tuning V2 (Liu et al., [2021b](#bib.bib202))
    used multi-task learning objectives and obtained competitive even better results
    than vanilla fine-tuning. SPoT (Vu et al., [2021](#bib.bib336)) further utilized
    transfer learning to support unseen tasks, where the prefix was pre-tuning on
    related tasks then used for initialization in unseen task. Overall, these methods
    are ideologically similar to adaptation approaches (Houlsby et al., [2019](#bib.bib130);
    Bapna & Firat, [2019](#bib.bib15); Pfeiffer et al., [2020](#bib.bib243)) because
    both of them do not change the most of parameters of the pre-trained model. However,
    the adaptation methods usually insert adaptors between layers, undermining the
    original model architecture.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，提示被定义为离散模板。此外，还有一些研究集中于连续提示，而非文本提示。例如，Prefix Tuning（Li & Liang, [2021](#bib.bib185)）和
    P-tuning（Liu et al., [2021c](#bib.bib203)）定义了虚拟标记，其中只有虚拟标记的参数进行了微调。最近的 P-tuning
    V2（Liu et al., [2021b](#bib.bib202)）使用了多任务学习目标，获得了比传统微调更具竞争力的甚至更好的结果。SPoT（Vu et
    al., [2021](#bib.bib336)）进一步利用迁移学习支持未见任务，其中前缀在相关任务上进行了预训练，然后用于未见任务的初始化。总体而言，这些方法在理念上类似于适应方法（Houlsby
    et al., [2019](#bib.bib130); Bapna & Firat, [2019](#bib.bib15); Pfeiffer et al.,
    [2020](#bib.bib243)），因为它们都不改变预训练模型的大多数参数。然而，适应方法通常在层之间插入适配器，从而破坏了原始模型结构。
- en: Chapter 6 Conclusions and Future Directions
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第六章 结论与未来方向
- en: We believe that Green deep learning will be an important research direction
    in the future. With recent advances in deep learning, the community have made
    significant progress in developing super-large models for downstream tasks, making
    it possible to apply AI models on complicated applications. Considering that it
    is the ultimate goal to deploy AI models on real-world devices with high requirements
    on resource usage, how to transfer strong models to resource-constraint devices
    (e.g., mobile) becomes a priority goal. In this section, we list several challenges
    for Green deep learning.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信绿色深度学习将是未来一个重要的研究方向。随着深度学习的最新进展，社区在开发超大规模模型用于下游任务方面取得了显著进展，使得在复杂应用中应用 AI
    模型成为可能。考虑到将 AI 模型部署在对资源使用有高要求的现实设备上是终极目标，如何将强大的模型转移到资源受限的设备（如移动设备）成为优先目标。在这一部分，我们列出了一些绿色深度学习的挑战。
- en: Green deep learning theory. We have harnessed some advanced Green deep learning
    techniques, but many questions still remain to be explored. For example, 1) If
    we already have a well-performing model, why was additional training required
    to transfer knowledge to a small model? 2) How many parameters do we need at least
    for feasible training and inference? 3) How to design Green learning algorithms
    to enable efficient zero-shot learning or few-shot learning like human? 4) How
    our model store knowledge and how to make models to achieve lifelong learning
    without forgetting learned knowledge during training? 5) Is linear algebra is
    the only basic theory for deep learning and whether can we develop a new operation
    system beyond linear algebra? In this survey, we just show limited questions due
    to page limitation. The community still have a long way to go in the theory of
    Green deep learning
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色深度学习理论。我们已经掌握了一些先进的绿色深度学习技术，但仍然存在许多待探讨的问题。例如，1) 如果我们已经有一个表现良好的模型，为什么还需要额外的训练来将知识转移到小模型上？2)
    我们至少需要多少参数才能实现可行的训练和推理？3) 如何设计绿色学习算法以实现像人类一样高效的零样本学习或少样本学习？4) 我们的模型如何存储知识，以及如何使模型在训练过程中实现终身学习而不忘记已学到的知识？5)
    线性代数是否是深度学习的唯一基础理论，我们是否可以开发一种超越线性代数的新操作系统？在这项调查中，由于页面限制，我们只展示了有限的问题。社区在绿色深度学习理论方面仍有很长的路要走。
- en: Green deep learning under extreme computation constraints. Deploying models
    on edge devices enjoys multiple advantages. First, it can avoid uploading user
    information to the cloud amid the tide of protecting user privacy. Second, it
    can empower more applications with high requirements on latency if deployed a
    light-weight model on edge devices. While recent advances in machine learning
    greatly facilitate efficient training and inference in the cloud, edge devices
    bring new challenges caused by extremely strict memory and computation constraints.
    Therefore, how to design advanced training and inference algorithms towards tiny
    devices is also an important and challenging direction. First, algorithm-hardware
    cooperation is a promising direction to satisfy speed requirements when deploying
    large models. For example, Lightseq (Wang et al., [2021b](#bib.bib347)) and Faster
    Transformer are two representative models that use CUDA implementation to accelerate
    Transformer execution. Second, edge-cloud cooperation is also a practical solution
    by combing powerful and elastic cloud computing and immediate edge computing.
    An intuitive solution is to design a dynamic network where edge devices can handle
    simple samples with smaller models and the cloud is responsible for handing complicated
    samples with larger models.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端计算约束下的绿色深度学习。在边缘设备上部署模型具有多个优势。首先，它可以避免在保护用户隐私的潮流中将用户信息上传到云端。其次，如果在边缘设备上部署轻量级模型，它可以赋能更多对延迟有高要求的应用。尽管最近的机器学习进展极大地促进了云端高效训练和推理，边缘设备带来了由于极其严格的内存和计算约束而产生的新挑战。因此，如何设计先进的训练和推理算法以适应微小设备也是一个重要且具有挑战性的方向。首先，算法与硬件的合作是满足大型模型部署时速度要求的一个有前景的方向。例如，Lightseq
    (Wang et al., [2021b](#bib.bib347)) 和 Faster Transformer 是两个使用 CUDA 实现加速 Transformer
    执行的代表性模型。其次，边缘与云的合作也是一种实际解决方案，通过结合强大且灵活的云计算和即时边缘计算。一个直观的解决方案是设计一个动态网络，其中边缘设备可以处理简单样本的小模型，而云端则负责处理复杂样本的大模型。
- en: References
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdelfattah et al. (2021) Mohamed S. Abdelfattah, Abhinav Mehrotra, Lukasz Dudziak,
    and Nicholas D. Lane. Zero-cost proxies for lightweight NAS. *ICLR*, 2021.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelfattah 等 (2021) Mohamed S. Abdelfattah, Abhinav Mehrotra, Lukasz Dudziak
    和 Nicholas D. Lane。轻量级 NAS 的零成本代理。*ICLR*，2021年。
- en: Aguilar et al. (2020) Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing
    Fan, and Chenlei Guo. Knowledge distillation from internal representations. In
    *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020*, pp.  7350–7357\. AAAI Press,
    2020.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aguilar 等 (2020) Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan
    和 Chenlei Guo。来自内部表示的知识蒸馏。在 *第34届人工智能AAAI会议，AAAI 2020，第32届人工智能创新应用会议，IAAI 2020，第10届人工智能教育进展AAAI研讨会，EAAI
    2020，美国纽约，2020年2月7-12日*，第 7350–7357 页。AAAI Press，2020年。
- en: Aharoni et al. (2019) Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively
    multilingual neural machine translation. In *NAACL-HLT (1)*, pp.  3874–3884\.
    Association for Computational Linguistics, 2019.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahn et al. (2019) Sungsoo Ahn, Shell Xu Hu, Andreas C. Damianou, Neil D. Lawrence,
    and Zhenwen Dai. Variational information distillation for knowledge transfer.
    In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
    Beach, CA, USA, June 16-20, 2019*, pp. 9163–9171\. Computer Vision Foundation
    / IEEE, 2019.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alizadeh et al. (2019) Milad Alizadeh, Javier Fernández-Marqués, Nicholas D.
    Lane, and Yarin Gal. An empirical study of binary neural networks’ optimisation.
    In *7th International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019*. OpenReview.net, 2019.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andreas & Klein (2015) Jacob Andreas and Dan Klein. When and why are log-linear
    models self-normalizing? In *HLT-NAACL*, pp.  244–249\. The Association for Computational
    Linguistics, 2015.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2018) Rohan Anil, Gabriel Pereyra, Alexandre Passos, Róbert Ormándi,
    George E. Dahl, and Geoffrey E. Hinton. Large scale distributed neural network
    training through online distillation. In *6th International Conference on Learning
    Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
    Track Proceedings*. OpenReview.net, 2018.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ba & Caruana (2014) Jimmy Ba and Rich Caruana. Do deep nets really need to
    be deep? In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence,
    and Kilian Q. Weinberger (eds.), *Advances in Neural Information Processing Systems
    27: Annual Conference on Neural Information Processing Systems 2014, December
    8-13 2014, Montreal, Quebec, Canada*, pp. 2654–2662, 2014.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ba et al. (2016) Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer
    normalization. *CoRR*, abs/1607.06450, 2016.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural
    machine translation by jointly learning to align and translate. In *ICLR*, 2015.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahri et al. (2020) Mehdi Bahri, Gaétan Bahl, and Stefanos Zafeiriou. Binary
    graph neural networks. *CoRR*, abs/2012.15823, 2020.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2020) Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin
    Jiang, Qun Liu, Michael R. Lyu, and Irwin King. Binarybert: Pushing the limit
    of BERT quantization. *CoRR*, abs/2012.15701, 2020.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2019) Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium
    models. In *NeurIPS*, pp.  688–699, 2019.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balzer et al. (1991) Wolfgang Balzer, Masanobu Takahashi, Jun Ohta, and Kazuo
    Kyuma. Weight quantization in boltzmann machines. *Neural Networks*, 4(3):405–409,
    1991.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bapna & Firat (2019) Ankur Bapna and Orhan Firat. Simple, scalable adaptation
    for neural machine translation. In *EMNLP/IJCNLP (1)*, pp.  1538–1548\. Association
    for Computational Linguistics, 2019.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belilovsky et al. (2019) Eugene Belilovsky, Michael Eickenberg, and Edouard
    Oyallon. Greedy layerwise learning can scale to imagenet. In Kamalika Chaudhuri
    and Ruslan Salakhutdinov (eds.), *Proceedings of the 36th International Conference
    on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA*,
    volume 97 of *Proceedings of Machine Learning Research*, pp.  583–593\. PMLR,
    2019.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beluch et al. (2018) William H. Beluch, Tim Genewein, Andreas Nürnberger, and
    Jan M. Köhler. The power of ensembles for active learning in image classification.
    In *CVPR*, pp.  9368–9377\. Computer Vision Foundation / IEEE Computer Society,
    2018.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio & Senecal (2003) Yoshua Bengio and Jean-Sébastien Senecal. Quick training
    of probabilistic neural nets by importance sampling. In *AISTATS*. Society for
    Artificial Intelligence and Statistics, 2003.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio & Senecal (2008) Yoshua Bengio and Jean-Sébastien Senecal. Adaptive importance
    sampling to accelerate training of a neural probabilistic language model. *IEEE
    Trans. Neural Networks*, 19(4):713–722, 2008.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2006) Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.
    Greedy layer-wise training of deep networks. In Bernhard Schölkopf, John C. Platt,
    and Thomas Hofmann (eds.), *Advances in Neural Information Processing Systems
    19, Proceedings of the Twentieth Annual Conference on Neural Information Processing
    Systems, Vancouver, British Columbia, Canada, December 4-7, 2006*, pp.  153–160\.
    MIT Press, 2006.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. (2013a) Yoshua Bengio, Aaron C. Courville, and Pascal Vincent.
    Representation learning: A review and new perspectives. *IEEE Trans. Pattern Anal.
    Mach. Intell.*, 35(8):1798–1828, 2013a.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2013b) Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *CoRR*, abs/1308.3432, 2013b.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bentivogli et al. (2009) Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang
    Dang, and Danilo Giampiccolo. The fifth PASCAL recognizing textual entailment
    challenge. In *Proceedings of the Second Text Analysis Conference, TAC 2009, Gaithersburg,
    Maryland, USA, November 16-17, 2009*. NIST, 2009.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhandare et al. (2019) Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada,
    Vivek Menon, Sun Choi, Kushal Datta, and Vikram Saletore. Efficient 8-bit quantization
    of transformer neural machine language translation model. *CoRR*, abs/1906.00532,
    2019.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bjorck et al. (2018) Johan Bjorck, Carla P. Gomes, Bart Selman, and Kilian Q.
    Weinberger. Understanding batch normalization. In *NeurIPS*, pp.  7705–7716, 2018.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blackwood et al. (2018) Graeme W. Blackwood, Miguel Ballesteros, and Todd Ward.
    Multilingual neural machine translation with task-specific attention. In *COLING*,
    pp.  3112–3122\. Association for Computational Linguistics, 2018.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bolukbasi et al. (2017) Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh
    Saligrama. Adaptive neural networks for efficient inference. In Doina Precup and
    Yee Whye Teh (eds.), *Proceedings of the 34th International Conference on Machine
    Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, volume 70 of *Proceedings
    of Machine Learning Research*, pp.  527–536\. PMLR, 2017.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucila et al. (2006) Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil.
    Model compression. In Tina Eliassi-Rad, Lyle H. Ungar, Mark Craven, and Dimitrios
    Gunopulos (eds.), *Proceedings of the Twelfth ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining, Philadelphia, PA, USA, August 20-23, 2006*,
    pp.  535–541\. ACM, 2006.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2019) Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural
    architecture search on target task and hardware. In *7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*.
    OpenReview.net, 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2020) Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song
    Han. Once-for-all: Train one network and specialize it for efficient deployment.
    In *8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Campos et al. (2018) Víctor Campos, Brendan Jou, Xavier Giró-i-Nieto, Jordi
    Torres, and Shih-Fu Chang. Skip RNN: learning to skip state updates in recurrent
    neural networks. In *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.
    OpenReview.net, 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caron et al. (2020) Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,
    Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features
    by contrasting cluster assignments. In Hugo Larochelle, Marc’Aurelio Ranzato,
    Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cer et al. (2017) Daniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo Lopez-Gazpio,
    and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual
    and crosslingual focused evaluation. In Steven Bethard, Marine Carpuat, Marianna
    Apidianaki, Saif M. Mohammad, Daniel M. Cer, and David Jurgens (eds.), *Proceedings
    of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver,
    Canada, August 3-4, 2017*, pp.  1–14\. Association for Computational Linguistics,
    2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun
    Chen. Online knowledge distillation with diverse peers. In *The Thirty-Fourth
    AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
    Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
    NY, USA, February 7-12, 2020*, pp.  3430–3437\. AAAI Press, 2020a.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020b) Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng,
    Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing
    transformer. *CoRR*, abs/2012.00364, 2020b.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018a) Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui
    Hsieh. Groupreduce: Block-wise low-rank approximation for neural language model
    shrinking. In *NeurIPS*, pp.  11011–11021, 2018a.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020c) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis
    for pre-trained BERT networks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
    Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020c.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016a) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *CoRR*, abs/1604.06174, 2016a.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020d) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E.
    Hinton. A simple framework for contrastive learning of visual representations.
    In *Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning
    Research*, pp.  1597–1607\. PMLR, 2020d.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016b) Wenlin Chen, David Grangier, and Michael Auli. Strategies
    for training large vocabulary neural language models. In *ACL (1)*. The Association
    for Computer Linguistics, 2016b.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021a) Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture
    search on imagenet in four GPU hours: A theoretically inspired perspective. *ICLR*,
    2021a.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen & He (2021) Xinlei Chen and Kaiming He. Exploring simple siamese representation
    learning. In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR
    2021, virtual, June 19-25, 2021*, pp.  15750–15758. Computer Vision Foundation
    / IEEE, 2021.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020e) Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He.
    Improved baselines with momentum contrastive learning. *CoRR*, abs/2003.04297,
    2020e.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021b) Xinlei Chen, Saining Xie, and Kaiming He. An empirical study
    of training self-supervised vision transformers. *CoRR*, abs/2104.02057, 2021b.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016c) Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. Compressing
    neural language models by sparse word representations. In *ACL (1)*. The Association
    for Computer Linguistics, 2016c.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018b) Yunpeng Chen, Xiaojie Jin, Bingyi Kang, Jiashi Feng, and
    Shuicheng Yan. Sharing residual units through collective tensor factorization
    to improve deep neural networks. In *IJCAI*, pp.  635–641\. ijcai.org, 2018b.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. *CoRR*, abs/1904.10509, 2019.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. (2017) Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the
    limit of network quantization. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net,
    2017.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) François Chollet. Xception: Deep learning with depthwise separable
    convolutions. In *CVPR*, pp.  1800–1807\. IEEE Computer Society, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choromanski et al. (2020) Krzysztof Choromanski, Valerii Likhosherstov, David
    Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz
    Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking
    attention with performers. *CoRR*, abs/2009.14794, 2020.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu et al. (2020) Xiangxiang Chu, Bo Zhang, and Xudong Li. Noisy differentiable
    architecture search. *CoRR*, abs/2005.03566, 2020.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2020) Inseop Chung, Seonguk Park, Jangho Kim, and Nojun Kwak.
    Feature-map-level online adversarial knowledge distillation. In *Proceedings of
    the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,
    Virtual Event*, volume 119 of *Proceedings of Machine Learning Research*, pp. 
    2006–2015\. PMLR, 2020.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D.
    Manning. ELECTRA: pre-training text encoders as discriminators rather than generators.
    In *8th International Conference on Learning Representations, ICLR 2020, Addis
    Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collobert et al. (2011) Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. Natural language processing (almost)
    from scratch. *J. Mach. Learn. Res.*, 12:2493–2537, 2011.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correia et al. (2019) Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins.
    Adaptively sparse transformers. In *EMNLP/IJCNLP (1)*, pp.  2174–2184\. Association
    for Computational Linguistics, 2019.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Costa-jussà & Fonollosa (2016) Marta R. Costa-jussà and José A. R. Fonollosa.
    Character-based neural machine translation. In *ACL (2)*. The Association for
    Computer Linguistics, 2016.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Courbariaux et al. (2015) Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
    David. Binaryconnect: Training deep neural networks with binary weights during
    propagations. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama,
    and Roman Garnett (eds.), *Advances in Neural Information Processing Systems 28:
    Annual Conference on Neural Information Processing Systems 2015, December 7-12,
    2015, Montreal, Quebec, Canada*, pp.  3123–3131, 2015.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui et al. (2019) Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang,
    Shijin Wang, and Guoping Hu. Pre-training with whole word masking for chinese
    BERT. *CoRR*, abs/1906.08101, 2019.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell,
    Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models
    beyond a fixed-length context. In *ACL (1)*, pp.  2978–2988\. Association for
    Computational Linguistics, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dehghani et al. (2019) Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob
    Uszkoreit, and Lukasz Kaiser. Universal transformers. In *7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*.
    OpenReview.net, 2019.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE
    Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009),
    20-25 June 2009, Miami, Florida, USA*, pp.  248–255\. IEEE Computer Society, 2009.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denton et al. (2014) Emily L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun,
    and Rob Fergus. Exploiting linear structure within convolutional networks for
    efficient evaluation. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D.
    Lawrence, and Kilian Q. Weinberger (eds.), *Advances in Neural Information Processing
    Systems 27: Annual Conference on Neural Information Processing Systems 2014, December
    8-13 2014, Montreal, Quebec, Canada*, pp. 1269–1277, 2014.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Devlin et al. (2014) Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar,
    Richard M. Schwartz, and John Makhoul. Fast and robust neural network joint models
    for statistical machine translation. In *ACL (1)*, pp.  1370–1380\. The Association
    for Computer Linguistics, 2014.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, pp.  4171–4186, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dolan & Brockett (2005) William B. Dolan and Chris Brockett. Automatically constructing
    a corpus of sentential paraphrases. In *Proceedings of the Third International
    Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005*.
    Asian Federation of Natural Language Processing, 2005.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2015) Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang.
    Multi-task learning for multiple language translation. In *ACL (1)*, pp.  1723–1732\.
    The Association for Computer Linguistics, 2015.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,
    Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training
    for natural language understanding and generation. In Hanna M. Wallach, Hugo Larochelle,
    Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.),
    *Advances in Neural Information Processing Systems 32: Annual Conference on Neural
    Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
    BC, Canada*, pp.  13042–13054, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong & Yang (2019) Xuanyi Dong and Yi Yang. Searching for a robust neural architecture
    in four GPU hours. In *IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*, pp. 1761–1770, 2019.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong & Yang (2020) Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope
    of reproducible neural architecture search. In *ICLR*. OpenReview.net, 2020.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2017) Yinpeng Dong, Jianguo Li, and Renkun Ni. Learning accurate
    low-bit deep neural networks with stochastic quantization. In *British Machine
    Vision Conference 2017, BMVC 2017, London, UK, September 4-7, 2017*. BMVA Press,
    2017.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ducoffe & Precioso (2018) Melanie Ducoffe and Frédéric Precioso. Adversarial
    active learning for deep networks: a margin based approach. *CoRR*, abs/1802.09841,
    2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duong et al. (2015) Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. Low
    resource dependency parsing: Cross-lingual parameter sharing in a neural network
    parser. In *ACL (2)*, pp.  845–850\. The Association for Computer Linguistics,
    2015.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elbayad et al. (2020) Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
    Depth-adaptive transformer. In *8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken et al. (2018) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural
    architecture search: A survey. *CoRR*, abs/1808.05377, 2018.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everingham et al. (2010) Mark Everingham, Luc Van Gool, Christopher K. I. Williams,
    John M. Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge.
    *Int. J. Comput. Vis.*, 88(2):303–338, 2010.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020a) Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed
    El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav
    Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard
    Grave, Michael Auli, and Armand Joulin. Beyond english-centric multilingual machine
    translation. *CoRR*, abs/2010.11125, 2020a.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020b) Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer
    depth on demand with structured dropout. In *ICLR*. OpenReview.net, 2020b.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020c) Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave,
    Rémi Gribonval, Hervé Jégou, and Armand Joulin. Training with quantization noise
    for extreme model compression. *CoRR*, abs/2004.07320, 2020c.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang & Xie (2020) Hongchao Fang and Pengtao Xie. CERT: contrastive self-supervised
    learning for language understanding. *CoRR*, abs/2005.12766, 2020.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faruqui et al. (2015) Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer,
    and Noah A. Smith. Sparse overcomplete word vector representations. In *ACL (1)*,
    pp.  1491–1500\. The Association for Computer Linguistics, 2015.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fedus et al. (2021) William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers:
    Scaling to trillion parameter models with simple and efficient sparsity. *CoRR*,
    abs/2101.03961, 2021.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and
    Yufei Ding. Sgquant: Squeezing the last bit on graph neural networks with specialized
    quantization. In *32nd IEEE International Conference on Tools with Artificial
    Intelligence, ICTAI 2020, Baltimore, MD, USA, November 9-11, 2020*, pp.  1044–1052\.
    IEEE, 2020.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fiesler et al. (1990) Emile Fiesler, Amar Choudry, and H. John Caulfield. Weight
    discretization paradigm for optical neural networks. In Hartmut Bartelt (ed.),
    *Optical Interconnections and Networks*, volume 1281, pp.  164 – 173\. International
    Society for Optics and Photonics, SPIE, 1990.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figurnov et al. (2017) Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang,
    Jonathan Huang, Dmitry P. Vetrov, and Ruslan Salakhutdinov. Spatially adaptive
    computation time for residual networks. In *2017 IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017*, pp. 1790–1799\.
    IEEE Computer Society, 2017.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firat et al. (2016) Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. Multi-way,
    multilingual neural machine translation with a shared attention mechanism. In
    *HLT-NAACL*, pp.  866–875\. The Association for Computational Linguistics, 2016.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firat et al. (2017) Orhan Firat, Kyunghyun Cho, Baskaran Sankaran, Fatos T.
    Yarman-Vural, and Yoshua Bengio. Multi-way, multilingual neural machine translation.
    *Comput. Speech Lang.*, 45:236–252, 2017.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frankle & Carbin (2019) Jonathan Frankle and Michael Carbin. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. In *7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*.
    OpenReview.net, 2019.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Freytag et al. (2014) Alexander Freytag, Erik Rodner, and Joachim Denzler.
    Selecting influential examples: Active learning with expected model output changes.
    In *ECCV (4)*, volume 8692 of *Lecture Notes in Computer Science*, pp.  562–577\.
    Springer, 2014.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple
    contrastive learning of sentence embeddings. *CoRR*, abs/2104.08821, 2021.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng et al. (2021) Shijie Geng, Peng Gao, Zuohui Fu, and Yongfeng Zhang. Romebert:
    Robust training of multi-exit BERT. *CoRR*, abs/2101.09755, 2021.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gholami et al. (2021) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W.
    Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural
    network inference. *CoRR*, abs/2103.13630, 2021.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghosh et al. (2011) Sucheta Ghosh, Richard Johansson, Giuseppe Riccardi, and
    Sara Tonelli. Shallow discourse parsing with conditional random fields. In *Fifth
    International Joint Conference on Natural Language Processing, IJCNLP 2011, Chiang
    Mai, Thailand, November 8-13, 2011*, pp. 1071–1079\. The Association for Computer
    Linguistics, 2011.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giorgi et al. (2021) John M. Giorgi, Osvald Nitski, Bo Wang, and Gary D. Bader.
    Declutr: Deep contrastive learning for unsupervised textual representations. In
    Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
    2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pp.  879–895\.
    Association for Computational Linguistics, 2021.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glorot & Bengio (2010) Xavier Glorot and Yoshua Bengio. Understanding the difficulty
    of training deep feedforward neural networks. In *AISTATS*, volume 9 of *JMLR
    Proceedings*, pp. 249–256\. JMLR.org, 2010.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gomez et al. (2017) Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B.
    Grosse. The reversible residual network: Backpropagation without storing activations.
    In *NIPS*, pp.  2214–2224, 2017.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2019) Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and
    Tie-Yan Liu. Efficient training of BERT by progressively stacking. In Kamalika
    Chaudhuri and Ruslan Salakhutdinov (eds.), *Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA*, volume 97 of *Proceedings of Machine Learning Research*, pp.  2337–2346\.
    PMLR, 2019.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2014) Yunchao Gong, Liu Liu, Ming Yang, and Lubomir D. Bourdev.
    Compressing deep convolutional networks using vector quantization. *CoRR*, abs/1412.6115,
    2014.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gormez & Koyuncu (2021) Alperen Gormez and Erdem Koyuncu. Class means as an
    early exit decision mechanism. *CoRR*, abs/2103.01148, 2021.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grachev et al. (2017) Artem M. Grachev, Dmitry I. Ignatov, and Andrey V. Savchenko.
    Neural networks compression for language modeling. In *PReMI*, volume 10597 of
    *Lecture Notes in Computer Science*, pp.  351–357\. Springer, 2017.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves (2016) Alex Graves. Adaptive computation time for recurrent neural networks.
    *CoRR*, abs/1603.08983, 2016.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grill et al. (2020) Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin
    Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires,
    Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos,
    and Michal Valko. Bootstrap your own latent - A new approach to self-supervised
    learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
    Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural Information Processing
    Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
    2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gruslys et al. (2016) Audrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot,
    and Alex Graves. Memory-efficient backpropagation through time. In *NIPS*, pp. 
    4125–4133, 2016.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Demi Guo, Alexander M. Rush, and Yoon Kim. Parameter-efficient
    transfer learning with diff pruning. *CoRR*, abs/2012.07463, 2020.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo (2018) Yunhui Guo. A survey on methods and theories of quantized neural
    networks. *CoRR*, abs/1808.04752, 2018.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2015) Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and
    Pritish Narayanan. Deep learning with limited numerical precision. In Francis R.
    Bach and David M. Blei (eds.), *Proceedings of the 32nd International Conference
    on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015*, volume 37 of *JMLR
    Workshop and Conference Proceedings*, pp.  1737–1746\. JMLR.org, 2015.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gutmann & Hyvärinen (2010) Michael Gutmann and Aapo Hyvärinen. Noise-contrastive
    estimation: A new estimation principle for unnormalized statistical models. In
    Yee Whye Teh and D. Mike Titterington (eds.), *Proceedings of the Thirteenth International
    Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna
    Resort, Sardinia, Italy, May 13-15, 2010*, volume 9 of *JMLR Proceedings*, pp. 
    297–304\. JMLR.org, 2010.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ha et al. (2016) Thanh-Le Ha, Jan Niehues, and Alexander H. Waibel. Toward multilingual
    neural machine translation with universal encoder and decoder. *CoRR*, abs/1611.04798,
    2016.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William J. Dally. Learning
    both weights and connections for efficient neural network. In *NIPS*, pp.  1135–1143,
    2015.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2016) Song Han, Huizi Mao, and William J. Dally. Deep compression:
    Compressing deep neural network with pruning, trained quantization and huffman
    coding. In Yoshua Bengio and Yann LeCun (eds.), *4th International Conference
    on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
    Conference Track Proceedings*, 2016.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2021a) Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu,
    Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan
    Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong
    Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. Pre-trained models: Past, present
    and future. *CoRR*, abs/2106.07139, 2021a.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2021b) Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang,
    and Yulin Wang. Dynamic neural networks: A survey. *CoRR*, abs/2102.04906, 2021b.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hanin & Rolnick (2018) Boris Hanin and David Rolnick. How to start training:
    The effect of initialization and architecture. In *NeurIPS*, pp.  569–579, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanson & Pratt (1988) Stephen Jose Hanson and Lorien Y. Pratt. Comparing biases
    for minimal network construction with back-propagation. In David S. Touretzky
    (ed.), *Advances in Neural Information Processing Systems 1, [NIPS Conference,
    Denver, Colorado, USA, 1988]*, pp. 177–185\. Morgan Kaufmann, 1988.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hashimoto et al. (2017) Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka,
    and Richard Socher. A joint many-task model: Growing a neural network for multiple
    NLP tasks. In *EMNLP*, pp.  1923–1933\. Association for Computational Linguistics,
    2017.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hassibi et al. (1993) Babak Hassibi, David G. Stork, and Gregory J. Wolff.
    Optimal brain surgeon: Extensions and performance comparison. In *NIPS*, pp. 
    263–270\. Morgan Kaufmann, 1993.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving
    deep into rectifiers: Surpassing human-level performance on imagenet classification.
    In *ICCV*, pp.  1026–1034\. IEEE Computer Society, 2015.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016a) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *2016 IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*,
    pp. 770–778\. IEEE Computer Society, 2016a.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017a) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick.
    Mask R-CNN. In *ICCV*, pp.  2980–2988\. IEEE Computer Society, 2017a.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick.
    Momentum contrast for unsupervised visual representation learning. In *2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
    USA, June 13-19, 2020*, pp. 9726–9735\. IEEE, 2020.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016b) Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu
    Zhou, and Yuheng Zou. Effective quantization methods for recurrent neural networks.
    *CoRR*, abs/1611.10176, 2016b.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2019) Tao He, Xiaoming Jin, Guiguang Ding, Lan Yi, and Chenggang
    Yan. Towards better uncertainty sampling: Active learning with multiple views
    for deep convolutional neural network. In *ICME*, pp.  1360–1365\. IEEE, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017b) Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for
    accelerating very deep neural networks. In *ICCV*, pp.  1398–1406\. IEEE Computer
    Society, 2017b.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2018) Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. AMC: automl for model compression and acceleration on mobile devices.
    In *Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September
    8-14, 2018, Proceedings, Part VII*, pp. 815–832, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heo et al. (2019) Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young Choi.
    Knowledge transfer via distillation of activation boundaries formed by hidden
    neurons. In *The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI
    2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference,
    IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019*, pp.  3779–3787\.
    AAAI Press, 2019.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2006) Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A
    fast learning algorithm for deep belief nets. *Neural Comput.*, 18(7):1527–1554,
    2006.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling
    the knowledge in a neural network. *CoRR*, abs/1503.02531, 2015.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoang & Jo (2018) Van-Thanh Hoang and Kang-Hyun Jo. Pydmobilenet: Improved
    version of mobilenets with pyramid depthwise separable convolution. *CoRR*, abs/1811.07083,
    2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter & Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural Comput.*, 9(8):1735–1780, 1997.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for NLP. In *ICML*, volume 97 of
    *Proceedings of Machine Learning Research*, pp.  2790–2799\. PMLR, 2019.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard et al. (2019) Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le,
    Mark Sandler, Bo Chen, Weijun Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu,
    Vijay Vasudevan, and Yukun Zhu. Searching for mobilenetv3. In *2019 IEEE/CVF International
    Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November
    2, 2019*, pp. 1314–1324\. IEEE, 2019.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Howard et al. (2017) Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient
    convolutional neural networks for mobile vision applications. *CoRR*, abs/1704.04861,
    2017.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der
    Maaten, and Kilian Q. Weinberger. Multi-scale dense networks for resource efficient
    image classification. In *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.
    OpenReview.net, 2018.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2013) Jui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yifan Gong.
    Cross-language knowledge transfer using multilingual deep neural network with
    shared hidden layers. In *ICASSP*, pp.  7304–7308\. IEEE, 2013.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2012) Liang Huang, Suphan Fayong, and Yang Guo. Structured perceptron
    with inexact search. In *Human Language Technologies: Conference of the North
    American Chapter of the Association of Computational Linguistics, Proceedings,
    June 3-8, 2012, Montréal, Canada*, pp.  142–151\. The Association for Computational
    Linguistics, 2012.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang & Wang (2017) Zehao Huang and Naiyan Wang. Like what you like: Knowledge
    distill via neuron selectivity transfer. *CoRR*, abs/1707.01219, 2017.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
    El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks
    with low precision weights and activations. *J. Mach. Learn. Res.*, 18:187:1–187:30,
    2017.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2014) Forrest N. Iandola, Matthew W. Moskewicz, Sergey Karayev,
    Ross B. Girshick, Trevor Darrell, and Kurt Keutzer. Densenet: Implementing efficient
    convnet descriptor pyramids. *CoRR*, abs/1404.1869, 2014.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2016) Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf,
    Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy
    with 50x fewer parameters and <1mb model size. *CoRR*, abs/1602.07360, 2016.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iglovikov & Shvets (2018) Vladimir Iglovikov and Alexey Shvets. Ternausnet:
    U-net with VGG11 encoder pre-trained on imagenet for image segmentation. *CoRR*,
    abs/1801.05746, 2018.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe & Szegedy (2015) Sergey Ioffe and Christian Szegedy. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In *ICML*,
    volume 37 of *JMLR Workshop and Conference Proceedings*, pp.  448–456\. JMLR.org,
    2015.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew G. Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    In *2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,
    Salt Lake City, UT, USA, June 18-22, 2018*, pp. 2704–2713\. IEEE Computer Society,
    2018.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2019) Paras Jain, Xiangxi Mo, Ajay Jain, Alexey Tumanov, Joseph E.
    Gonzalez, and Ion Stoica. The ooo VLIW JIT compiler for GPU inference. *CoRR*,
    abs/1901.10008, 2019.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jean et al. (2015) Sébastien Jean, KyungHyun Cho, Roland Memisevic, and Yoshua
    Bengio. On using very large target vocabulary for neural machine translation.
    In *ACL (1)*, pp.  1–10\. The Association for Computer Linguistics, 2015.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jernite et al. (2017) Yacine Jernite, Edouard Grave, Armand Joulin, and Tomás
    Mikolov. Variable computation in recurrent neural networks. In *5th International
    Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
    2017, Conference Track Proceedings*. OpenReview.net, 2017.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2019) Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, and Jingbo
    Zhu. Improved differentiable architecture search for language modeling and named
    entity recognition. In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*,
    pp.  3583–3588, 2019.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2015) Jonghoon Jin, Aysegul Dundar, and Eugenio Culurciello. Flattened
    convolutional neural networks for feedforward acceleration. In *ICLR (Workshop)*,
    2015.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2019) Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding
    Liang, Junjie Yan, and Xiaolin Hu. Knowledge distillation via route constrained
    optimization. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV
    2019, Seoul, Korea (South), October 27 - November 2, 2019*, pp. 1345–1354\. IEEE,
    2019.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson et al. (2017) Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun,
    Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Viégas, Martin Wattenberg,
    Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s multilingual neural machine
    translation system: Enabling zero-shot translation. *Trans. Assoc. Comput. Linguistics*,
    5:339–351, 2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi et al. (2016) Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak Bhattacharyya,
    and Mark James Carman. Are word embedding-based features useful for sarcasm detection?
    In *EMNLP*, pp.  1006–1011\. The Association for Computational Linguistics, 2016.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi et al. (2009) Ajay J. Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos.
    Multi-class active learning for image classification. In *CVPR*, pp.  2372–2379\.
    IEEE Computer Society, 2009.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2019) Mandar Joshi, Eunsol Choi, Omer Levy, Daniel S. Weld, and
    Luke Zettlemoyer. pair2vec: Compositional word-pair embeddings for cross-sentence
    inference. In *NAACL-HLT (1)*, pp.  3597–3608\. Association for Computational
    Linguistics, 2019.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke
    Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and
    predicting spans. *Trans. Assoc. Comput. Linguistics*, 8:64–77, 2020.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Józefowicz et al. (2016) Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam
    Shazeer, and Yonghui Wu. Exploring the limits of language modeling. *CoRR*, abs/1602.02410,
    2016.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaya et al. (2019) Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. Shallow-deep
    networks: Understanding and mitigating network overthinking. In Kamalika Chaudhuri
    and Ruslan Salakhutdinov (eds.), *Proceedings of the 36th International Conference
    on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA*,
    volume 97 of *Proceedings of Machine Learning Research*, pp.  3301–3310\. PMLR,
    2019.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim & Cho (2021) Gyuwan Kim and Kyunghyun Cho. Length-adaptive transformer:
    Train once with length drop, use anytime with search. In Chengqing Zong, Fei Xia,
    Wenjie Li, and Roberto Navigli (eds.), *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers),
    Virtual Event, August 1-6, 2021*, pp.  6501–6511\. Association for Computational
    Linguistics, 2021.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2018) Jangho Kim, Seonguk Park, and Nojun Kwak. Paraphrasing complex
    network: Network compression via factor transfer. In Samy Bengio, Hanna M. Wallach,
    Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.),
    *Advances in Neural Information Processing Systems 31: Annual Conference on Neural
    Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
    Canada*, pp.  2765–2774, 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2020) Jangho Kim, Minsung Hyun, Inseop Chung, and Nojun Kwak. Feature
    fusion for online mutual knowledge distillation. In *25th International Conference
    on Pattern Recognition, ICPR 2020, Virtual Event / Milan, Italy, January 10-15,
    2021*, pp.  4619–4625. IEEE, 2020.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2021) Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney,
    and Kurt Keutzer. I-BERT: integer-only BERT quantization. *CoRR*, abs/2101.01321,
    2021.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2016a) Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang,
    and Dongjun Shin. Compression of deep convolutional neural networks for fast and
    low power mobile applications. In *ICLR (Poster)*, 2016a.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2016b) Yoon Kim, Yacine Jernite, David A. Sontag, and Alexander M.
    Rush. Character-aware neural language models. In *AAAI*, pp.  2741–2749\. AAAI
    Press, 2016b.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. In *ICLR*. OpenReview.net, 2020.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kolda & Bader (2009) Tamara G. Kolda and Brett W. Bader. Tensor decompositions
    and applications. *SIAM Review*, 51(3):455–500, 2009.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
    Imagenet classification with deep convolutional neural networks. In *Advances
    in Neural Information Processing Systems 25: 26th Annual Conference on Neural
    Information Processing Systems 2012\. Proceedings of a meeting held December 3-6,
    2012, Lake Tahoe, Nevada, United States*, pp.  1106–1114, 2012.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kulkarni & Karande (2017) Mandar Kulkarni and Shirish Subhash Karande. Layer-wise
    training of deep networks using kernel similarity. *CoRR*, abs/1703.07115, 2017.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar (2017) Siddharth Krishna Kumar. On weight initialization in deep neural
    networks. *CoRR*, abs/1704.08863, 2017.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lacoste et al. (2019) Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt,
    and Thomas Dandres. Quantifying the carbon emissions of machine learning. *CoRR*,
    abs/1910.09700, 2019.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lam (2018) Maximilian Lam. Word2bits - quantized word vectors. *CoRR*, abs/1803.05651,
    2018.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2018) Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge distillation
    by on-the-fly native ensemble. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
    Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett (eds.), *Advances in Neural
    Information Processing Systems 31: Annual Conference on Neural Information Processing
    Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*, pp.  7528–7538,
    2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning
    of language representations. In *ICLR*. OpenReview.net, 2020.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lebedev et al. (2015) Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V.
    Oseledets, and Victor S. Lempitsky. Speeding-up convolutional neural networks
    using fine-tuned cp-decomposition. In *ICLR (Poster)*, 2015.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain
    damage. In *NIPS*, pp.  598–605\. Morgan Kaufmann, 1989.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    Gradient-based learning applied to document recognition. *Proceedings of the IEEE*,
    86(11):2278–2324, 1998.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2019) Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr.
    Snip: single-shot network pruning based on connection sensitivity. In *7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019*, 2019.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2018) Seung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self-supervised
    knowledge distillation using singular value decomposition. In Vittorio Ferrari,
    Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), *Computer Vision
    - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018,
    Proceedings, Part VI*, volume 11210 of *Lecture Notes in Computer Science*, pp. 
    339–354\. Springer, 2018.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee & Song (2019) Seunghyun Lee and Byung Cheol Song. Graph-based knowledge
    distillation by multi-head attention network. In *30th British Machine Vision
    Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019*, pp.  141\. BMVA
    Press, 2019.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lepikhin et al. (2021) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
    Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
    Gshard: Scaling giant models with conditional computation and automatic sharding.
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net, 2021.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levesque et al. (2012) Hector J. Levesque, Ernest Davis, and Leora Morgenstern.
    The winograd schema challenge. In Gerhard Brewka, Thomas Eiter, and Sheila A.
    McIlraith (eds.), *Principles of Knowledge Representation and Reasoning: Proceedings
    of the Thirteenth International Conference, KR 2012, Rome, Italy, June 10-14,
    2012*. AAAI Press, 2012.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis & Gale (1994) David D. Lewis and William A. Gale. A sequential algorithm
    for training text classifiers. In *SIGIR*, pp.  3–12\. ACM/Springer, 1994.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART:
    denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
    Tetreault (eds.), *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics, ACL 2020, Online, July 5-10, 2020*, pp. 7871–7880\.
    Association for Computational Linguistics, 2020.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020a) Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang.
    Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training.
    In *AAAI*, pp.  11336–11344\. AAAI Press, 2020a.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. Pruning filters for efficient convnets. In *5th International Conference
    on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
    Track Proceedings*, 2017.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020b) Lei Li, Yankai Lin, Shuhuai Ren, Deli Chen, Xuancheng Ren,
    Peng Li, Jie Zhou, and Xu Sun. Accelerating pre-trained language models via calibrated
    cascade. *CoRR*, abs/2012.14682, 2020b.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Ji (2014) Qi Li and Heng Ji. Incremental joint extraction of entity mentions
    and relations. In *Proceedings of the 52nd Annual Meeting of the Association for
    Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume
    1: Long Papers*, pp.  402–412\. The Association for Computer Linguistics, 2014.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and
    Roberto Navigli (eds.), *Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event,
    August 1-6, 2021*, pp.  4582–4597\. Association for Computational Linguistics,
    2021.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu,
    and Xuanjing Huang. Accelerating BERT inference for sequence labeling via early-exit.
    In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
    2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pp.  189–199\.
    Association for Computational Linguistics, 2021.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021) Tailin Liang, John Glossner, Lei Wang, and Shaobo Shi.
    Pruning and quantization for deep neural network acceleration: A survey. *CoRR*,
    abs/2101.09671, 2021.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2021) Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, and
    Bin He. A global past-future early exit method for accelerating inference of pre-trained
    language models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
    Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and
    Yichao Zhou (eds.), *Proceedings of the 2021 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2021, Online, June 6-11, 2021*, pp.  2013–2023\. Association for Computational
    Linguistics, 2021.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lienhart & Maydt (2002) Rainer Lienhart and Jochen Maydt. An extended set of
    haar-like features for rapid object detection. In *Proceedings. international
    conference on image processing*, volume 1, pp.  I–I. IEEE, 2002.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2017) Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural
    pruning. In *Advances in Neural Information Processing Systems 30: Annual Conference
    on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
    CA, USA*, pp.  2181–2191, 2017.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021) Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang
    Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou,
    Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin
    Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, and Hongxia Yang. M6: A chinese
    multimodal pretrainer. *CoRR*, abs/2103.00823, 2021.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft
    COCO: common objects in context. In David J. Fleet, Tomás Pajdla, Bernt Schiele,
    and Tinne Tuytelaars (eds.), *Computer Vision - ECCV 2014 - 13th European Conference,
    Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V*, volume 8693 of
    *Lecture Notes in Computer Science*, pp. 740–755\. Springer, 2014.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2018) Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng Ji. A multi-lingual
    multi-task architecture for low-resource sequence labeling. In *ACL (1)*, pp. 
    799–809\. Association for Computational Linguistics, 2018.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2020) Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng,
    Hao Zhou, and Lei Li. Pre-training multilingual neural machine translation by
    leveraging alignment information. In *EMNLP (1)*, pp.  2649–2663\. Association
    for Computational Linguistics, 2020.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018a) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,
    Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L. Yuille, Jonathan Huang, and Kevin Murphy.
    Progressive neural architecture search. In *ECCV (1)*, volume 11205 of *Lecture
    Notes in Computer Science*, pp.  19–35\. Springer, 2018a.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable
    architecture search. In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*, 2019a.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Iou-Jen Liu, Jian Peng, and Alexander G. Schwing. Knowledge
    flow: Improve upon your teachers. In *7th International Conference on Learning
    Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net,
    2019b.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018b) Liyuan Liu, Jingbo Shang, Xiang Ren, Frank Fangzheng Xu,
    Huan Gui, Jian Peng, and Jiawei Han. Empower sequence labeling with task-aware
    neural language model. In *AAAI*, pp.  5253–5260\. AAAI Press, 2018b.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *CoRR*, abs/2107.13586, 2021a.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020a) Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng,
    and Qi Ju. Fastbert: a self-distilling BERT with adaptive inference time. In Dan
    Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020*, pp. 6035–6044\. Association for Computational
    Linguistics, 2020a.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018c) Xianggen Liu, Lili Mou, Haotian Cui, Zhengdong Lu, and Sen
    Song. JUMPER: learning when to make classification decisions in reading. *CoRR*,
    abs/1807.02314, 2018c.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021b) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang,
    and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally
    across scales and tasks, 2021b.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021c) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. GPT understands, too. *CoRR*, abs/2103.10385, 2021c.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,
    Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising
    pre-training for neural machine translation. *Trans. Assoc. Comput. Linguistics*,
    8:726–742, 2020b.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019c) Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor
    Darrell. Rethinking the value of network pruning. In *ICLR (Poster)*. OpenReview.net,
    2019c.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2016) Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan.
    Unsupervised domain adaptation with residual transfer networks. In *NIPS*, pp. 
    136–144, 2016.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2019) Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
    Pretraining task-agnostic visiolinguistic representations for vision-and-language
    tasks. In *NeurIPS*, pp.  13–23, 2019.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2020) Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, and
    Xu Sun. CAPT: contrastive pre-training for learning denoised sequence representations.
    *CoRR*, abs/2010.06351, 2020.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2019) Jian-Hao Luo, Hao Zhang, Hong-Yu Zhou, Chen-Wei Xie, Jianxin
    Wu, and Weiyao Lin. Thinet: Pruning CNN filters for a thinner net. *IEEE Trans.
    Pattern Anal. Mach. Intell.*, 41(10):2525–2538, 2019.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2018) Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
    Neural architecture optimization. In *NeurIPS*, pp.  7827–7838, 2018.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luong et al. (2016) Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals,
    and Lukasz Kaiser. Multi-task sequence to sequence learning. In *ICLR (Poster)*,
    2016.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2019) Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou,
    Ming Zhou, and Dawei Song. A tensorized transformer for language modeling. In
    *NeurIPS*, pp.  2229–2239, 2019.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MacKay et al. (2018) Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger B. Grosse.
    Reversible recurrent neural networks. In *NeurIPS*, pp.  9043–9054, 2018.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martins & Astudillo (2016) André F. T. Martins and Ramón Fernandez Astudillo.
    From softmax to sparsemax: A sparse model of attention and multi-label classification.
    In *ICML*, volume 48 of *JMLR Workshop and Conference Proceedings*, pp.  1614–1623\.
    JMLR.org, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maruf et al. (2019) Sameen Maruf, André F. T. Martins, and Gholamreza Haffari.
    Selective attention for context-aware neural machine translation. In *NAACL-HLT
    (1)*, pp.  3092–3102\. Association for Computational Linguistics, 2019.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCarley et al. (2019) JS McCarley, Rishav Chakravarti, and Avirup Sil. Structured
    pruning of a bert-based question answering model. *arXiv preprint arXiv:1910.06360*,
    2019.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McDonald et al. (2010) Ryan T. McDonald, Keith B. Hall, and Gideon Mann. Distributed
    training strategies for the structured perceptron. In *Human Language Technologies:
    Conference of the North American Chapter of the Association of Computational Linguistics,
    Proceedings, June 2-4, 2010, Los Angeles, California, USA*, pp.  456–464. The
    Association for Computational Linguistics, 2010.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mellor et al. (2020) Joseph Mellor, Jack Turner, Amos J. Storkey, and Elliot J.
    Crowley. Neural architecture search without training. *CoRR*, abs/2006.04647,
    2020.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. Are sixteen
    heads really better than one? In *Advances in Neural Information Processing Systems
    32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, 2019, Vancouver, BC, Canada*, pp.  14014–14024, 2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013a) Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    Efficient estimation of word representations in vector space. In *ICLR (Workshop
    Poster)*, 2013a.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013b) Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado,
    and Jeffrey Dean. Distributed representations of words and phrases and their compositionality.
    In *NIPS*, pp.  3111–3119, 2013b.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirzadeh et al. (2020) Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir
    Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation
    via teacher assistant. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*, pp.  5191–5198\.
    AAAI Press, 2020.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishkin & Matas (2016) Dmytro Mishkin and Jiri Matas. All you need is a good
    init. In *ICLR (Poster)*, 2016.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih & Hinton (2008) Andriy Mnih and Geoffrey E. Hinton. A scalable hierarchical
    distributed language model. In *NIPS*, pp.  1081–1088\. Curran Associates, Inc.,
    2008.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Molchanov et al. (2017) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. Pruning convolutional neural networks for resource efficient inference.
    In *5th International Conference on Learning Representations, ICLR 2017, Toulon,
    France, April 24-26, 2017, Conference Track Proceedings*, 2017.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morin & Bengio (2005) Frederic Morin and Yoshua Bengio. Hierarchical probabilistic
    neural network language model. In *AISTATS*. Society for Artificial Intelligence
    and Statistics, 2005.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mou et al. (2016) Lili Mou, Ran Jia, Yan Xu, Ge Li, Lu Zhang, and Zhi Jin.
    Distilling word embeddings: An encoding approach. In *CIKM*, pp.  1977–1980\.
    ACM, 2016.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narang et al. (2017a) Sharan Narang, Greg Diamos, Shubho Sengupta, and Erich
    Elsen. Exploring sparsity in recurrent neural networks. In *5th International
    Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
    2017, Conference Track Proceedings*, 2017a.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narang et al. (2017b) Sharan Narang, Eric Undersander, and Gregory F. Diamos.
    Block-sparse recurrent neural networks. *CoRR*, abs/1711.02782, 2017b.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen & Smeulders (2004) Hieu Tat Nguyen and Arnold W. M. Smeulders. Active
    learning using pre-clustering. In *ICML*, volume 69 of *ACM International Conference
    Proceeding Series*. ACM, 2004.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niculae & Blondel (2017) Vlad Niculae and Mathieu Blondel. A regularized framework
    for sparse and structured neural attention. In *NIPS*, pp.  3338–3348, 2017.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noach & Goldberg (2020) Matan Ben Noach and Yoav Goldberg. Compressing pre-trained
    language models by matrix decomposition. In Kam-Fai Wong, Kevin Knight, and Hua
    Wu (eds.), *Proceedings of the 1st Conference of the Asia-Pacific Chapter of the
    Association for Computational Linguistics and the 10th International Joint Conference
    on Natural Language Processing, AACL/IJCNLP 2020, Suzhou, China, December 4-7,
    2020*, pp.  884–889\. Association for Computational Linguistics, 2020.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oquab et al. (2014) Maxime Oquab, Léon Bottou, Ivan Laptev, and Josef Sivic.
    Learning and transferring mid-level image representations using convolutional
    neural networks. In *CVPR*, pp.  1717–1724\. IEEE Computer Society, 2014.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ott et al. (2016) Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, and Yoshua
    Bengio. Recurrent neural networks with limited numerical precision. *CoRR*, abs/1611.07065,
    2016.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2021) Dae Young Park, Moon-Hyun Cha, Changwook Jeong, Daesin Kim,
    and Bohyung Han. Learning student-friendly teacher networks for knowledge distillation.
    *CoRR*, abs/2102.07650, 2021.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2015) Eunhyeok Park, Dongyoung Kim, Soobeom Kim, Yong-Deok Kim,
    Gunhee Kim, Sungroh Yoon, and Sungjoo Yoo. Big/little deep neural network for
    ultra low power inference. In Gabriela Nicolescu and Andreas Gerstlauer (eds.),
    *2015 International Conference on Hardware/Software Codesign and System Synthesis,
    CODES+ISSS 2015, Amsterdam, Netherlands, October 4-9, 2015*, pp. 124–132\. IEEE,
    2015.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park & Kwak (2019) Seonguk Park and Nojun Kwak. FEED: feature-level ensemble
    for knowledge distillation. *CoRR*, abs/1909.10754, 2019.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2019) Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao
    Wu, Jiaheng Liu, Zhaoning Zhang, and Yu Liu. Correlation congruence for knowledge
    distillation. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV
    2019, Seoul, Korea (South), October 27 - November 2, 2019*, pp. 5006–5015\. IEEE,
    2019.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D.
    Manning. Glove: Global vectors for word representation. In *EMNLP*, pp.  1532–1543\.
    ACL, 2014.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2019) Ben Peters, Vlad Niculae, and André F. T. Martins. Sparse
    sequence-to-sequence models. In *ACL (1)*, pp.  1504–1519\. Association for Computational
    Linguistics, 2019.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word
    representations. In *NAACL-HLT*, pp.  2227–2237\. Association for Computational
    Linguistics, 2018.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. Language models
    as knowledge bases? In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.),
    *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
    and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
    2019, Hong Kong, China, November 3-7, 2019*, pp.  2463–2473\. Association for
    Computational Linguistics, 2019.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pfeiffer et al. (2020) Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya
    Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. Adapterhub:
    A framework for adapting transformers. In *EMNLP (Demos)*, pp.  46–54\. Association
    for Computational Linguistics, 2020.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pham et al. (2018) Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.
    Efficient neural architecture search via parameters sharing. In *International
    Conference on Machine Learning*, pp. 4095–4104\. PMLR, 2018.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pleiss et al. (2017) Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens
    van der Maaten, and Kilian Q. Weinberger. Memory-efficient implementation of densenets.
    *CoRR*, abs/1707.06990, 2017.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plummer et al. (2020) Bryan A Plummer, Nikoli Dryden, Julius Frost, Torsten
    Hoefler, and Kate Saenko. Neural parameter allocation search. *arXiv preprint
    arXiv:2006.10598*, 2020.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polino et al. (2018) Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model
    compression via distillation and quantization. In *6th International Conference
    on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
    3, 2018, Conference Track Proceedings*. OpenReview.net, 2018.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prasanna et al. (2020) Sai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT
    plays the lottery, all tickets are winning. In *EMNLP (1)*, pp.  3208–3229\. Association
    for Computational Linguistics, 2020.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prato et al. (2020) Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh.
    Fully quantized transformer for machine translation. In Trevor Cohn, Yulan He,
    and Yang Liu (eds.), *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November
    2020*, pp.  1–14. Association for Computational Linguistics, 2020.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Press & Wolf (2017) Ofir Press and Lior Wolf. Using the output embedding to
    improve language models. In *EACL (2)*, pp.  157–163\. Association for Computational
    Linguistics, 2017.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2020) Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng
    Chen, Ruofei Zhang, and Ming Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence
    pre-training. In Trevor Cohn, Yulan He, and Yang Liu (eds.), *Findings of the
    Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November
    2020*, volume EMNLP 2020 of *Findings of ACL*, pp. 2401–2410, 2020.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2018) Ye Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Padmanabhan,
    and Graham Neubig. When and why are pre-trained word embeddings useful for neural
    machine translation? In *NAACL-HLT (2)*, pp.  529–535\. Association for Computational
    Linguistics, 2018.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2020) Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai,
    and Xuanjing Huang. Pre-trained models for natural language processing: A survey.
    *CoRR*, abs/2003.08271, 2020.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21:140:1–140:67, 2020.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: memory optimizations toward training trillion parameter
    models. In *SC*, pp.  20\. IEEE/ACM, 2020.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. Squad: 100, 000+ questions for machine comprehension of text. In
    Jian Su, Xavier Carreras, and Kevin Duh (eds.), *Proceedings of the 2016 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas,
    USA, November 1-4, 2016*, pp.  2383–2392\. The Association for Computational Linguistics,
    2016.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. Know
    what you don’t know: Unanswerable questions for squad. In Iryna Gurevych and Yusuke
    Miyao (eds.), *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short
    Papers*, pp. 784–789\. Association for Computational Linguistics, 2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramsundar et al. (2015) Bharath Ramsundar, Steven M. Kearnes, Patrick Riley,
    Dale Webster, David E. Konerding, and Vijay S. Pande. Massively multitask networks
    for drug discovery. *CoRR*, abs/1502.02072, 2015.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranganathan et al. (2017) Hiranmayi Ranganathan, Hemanth Venkateswara, Shayok
    Chakraborty, and Sethuraman Panchanathan. Deep active learning for image classification.
    In *ICIP*, pp.  3934–3938\. IEEE, 2017.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
    and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional
    neural networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.),
    *Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands,
    October 11-14, 2016, Proceedings, Part IV*, volume 9908 of *Lecture Notes in Computer
    Science*, pp.  525–542\. Springer, 2016.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le.
    Regularized evolution for image classifier architecture search. In *The Thirty-Third
    AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative
    Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,
    Hawaii, USA, January 27 - February 1, 2019*, pp.  4780–4789, 2019.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reddy et al. (2019) Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa:
    A conversational question answering challenge. *Trans. Assoc. Comput. Linguistics*,
    7:249–266, 2019.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
    Faster R-CNN: towards real-time object detection with region proposal networks.
    In *NIPS*, pp.  91–99, 2015.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rigamonti et al. (2013) Roberto Rigamonti, Amos Sironi, Vincent Lepetit, and
    Pascal Fua. Learning separable filters. In *2013 IEEE Conference on Computer Vision
    and Pattern Recognition, Portland, OR, USA, June 23-28, 2013*, pp.  2754–2761\.
    IEEE Computer Society, 2013.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Romero et al. (2015) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
    Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep
    nets. In Yoshua Bengio and Yann LeCun (eds.), *3rd International Conference on
    Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
    Track Proceedings*, 2015.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy & McCallum (2001) Nicholas Roy and Andrew McCallum. Toward optimal active
    learning through monte carlo estimation of error reduction. *ICML, Williamstown*,
    2:441–448, 2001.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder (2019) Sebastian Ruder. *Neural transfer learning for natural language
    processing*. PhD thesis, NUI Galway, 2019.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ruder et al. (2017) Sebastian Ruder, Parsa Ghaffari, and John G. Breslin. Knowledge
    adaptation: Teaching to adapt. *CoRR*, abs/1702.02052, 2017.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder et al. (2019) Sebastian Ruder, Ivan Vulic, and Anders Søgaard. A survey
    of cross-lingual word embedding models. *J. Artif. Intell. Res.*, 65:569–631,
    2019.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rueda-Plata et al. (2015) Diego Rueda-Plata, Raúl Ramos-Pollán, and Fabio A.
    González. Supervised greedy layer-wise training for deep convolutional networks
    with small datasets. In Manuel Núñez, Ngoc Thanh Nguyen, David Camacho, and Bogdan
    Trawinski (eds.), *Computational Collective Intelligence - 7th International Conference,
    ICCCI 2015, Madrid, Spain, September 21-23, 2015. Proceedings, Part I*, volume
    9329 of *Lecture Notes in Computer Science*, pp.  275–284\. Springer, 2015.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rush et al. (2015) Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural
    attention model for abstractive sentence summarization. In Lluís Màrquez, Chris
    Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (eds.), *Proceedings
    of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2015, Lisbon, Portugal, September 17-21, 2015*, pp.  379–389\. The Association
    for Computational Linguistics, 2015.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sainath et al. (2013) Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru
    Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural
    network training with high-dimensional output targets. In *ICASSP*, pp.  6655–6659\.
    IEEE, 2013.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salimans & Kingma (2016) Tim Salimans and Diederik P. Kingma. Weight normalization:
    A simple reparameterization to accelerate training of deep neural networks. In
    *NIPS*, pp.  901, 2016.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler et al. (2018) Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey
    Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks.
    In *2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,
    Salt Lake City, UT, USA, June 18-22, 2018*, pp. 4510–4520\. Computer Vision Foundation
    / IEEE Computer Society, 2018.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement
    pruning: Adaptive sparsity by fine-tuning. In Hugo Larochelle, Marc’Aurelio Ranzato,
    Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santurkar et al. (2018) Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and
    Aleksander Madry. How does batch normalization help optimization? In *NeurIPS*,
    pp.  2488–2498, 2018.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Savarese & Maire (2019) Pedro Savarese and Michael Maire. Learning implicitly
    recurrent cnns through parameter sharing. In *ICLR (Poster)*. OpenReview.net,
    2019.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saxe et al. (2014) Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact
    solutions to the nonlinear dynamics of learning in deep linear neural networks.
    In *ICLR*, 2014.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scao & Rush (2021) Teven Le Scao and Alexander M. Rush. How many data points
    is a prompt worth? In *NAACL-HLT*, pp.  2627–2636\. Association for Computational
    Linguistics, 2021.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick & Schütze (2021) Timo Schick and Hinrich Schütze. It’s not just size
    that matters: Small language models are also few-shot learners. In *NAACL-HLT*,
    pp.  2339–2352\. Association for Computational Linguistics, 2021.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schröder et al. (2021) Christopher Schröder, Andreas Niekler, and Martin Potthast.
    Uncertainty-based query strategies for active learning with transformers. *CoRR*,
    abs/2107.05687, 2021.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schuster et al. (2021) Tal Schuster, Adam Fisch, Tommi S. Jaakkola, and Regina
    Barzilay. Consistent accelerated inference via confident adaptive transformers.
    *CoRR*, abs/2104.08803, 2021.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwartz et al. (2020a) Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni.
    Green AI. *Commun. ACM*, 63(12):54–63, 2020a.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schwartz et al. (2020b) Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta,
    Jesse Dodge, and Noah A. Smith. The right tool for the job: Matching model and
    instance complexities. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
    Tetreault (eds.), *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics, ACL 2020, Online, July 5-10, 2020*, pp. 6640–6651\.
    Association for Computational Linguistics, 2020b.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See et al. (2016) Abigail See, Minh-Thang Luong, and Christopher D. Manning.
    Compression of neural machine translation models via pruning. In *Proceedings
    of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL
    2016, Berlin, Germany, August 11-12, 2016*, pp.  291–301, 2016.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. Get
    to the point: Summarization with pointer-generator networks. In Regina Barzilay
    and Min-Yen Kan (eds.), *Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4,
    Volume 1: Long Papers*, pp. 1073–1083\. Association for Computational Linguistics,
    2017.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sener & Savarese (2018) Ozan Sener and Silvio Savarese. Active learning for
    convolutional neural networks: A core-set approach. In *ICLR (Poster)*. OpenReview.net,
    2018.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural
    machine translation of rare words with subword units. In *ACL (1)*. The Association
    for Computer Linguistics, 2016.
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seo et al. (2018) Min Joon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi.
    Neural speed reading via skim-rnn. In *6th International Conference on Learning
    Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
    Track Proceedings*. OpenReview.net, 2018.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2018) Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod,
    and Animashree Anandkumar. Deep active learning for named entity recognition.
    In *ICLR (Poster)*. OpenReview.net, 2018.
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2017) Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen.
    Reasonet: Learning to stop reading in machine comprehension. In *Proceedings of
    the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
    Halifax, NS, Canada, August 13 - 17, 2017*, pp.  1047–1055\. ACM, 2017.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. (2018) Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang
    Shen, and Mickey Aleksic. A quantization-friendly separable convolution for mobilenets.
    *CoRR*, abs/1803.08607, 2018.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2020) Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and
    Jiyan Yang. Compositional embeddings using complementary partitions for memory-efficient
    recommendation systems. In *KDD*, pp.  165–175\. ACM, 2020.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2021) Wenxian Shi, Yuxuan Song, Hao Zhou, Bohan Li, and Lei Li.
    Learning from deep model via exploring local targets, 2021. URL [https://openreview.net/forum?id=5slGDu_bVc6](https://openreview.net/forum?id=5slGDu_bVc6).
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu & Nakayama (2018) Raphael Shu and Hideki Nakayama. Compressing word embeddings
    via deep compositional code learning. In *ICLR (Poster)*. OpenReview.net, 2018.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu et al. (2020) Yao Shu, Wei Wang, and Shaofeng Cai. Understanding architectures
    learnt by cell-based neural architecture search. In *ICLR*. OpenReview.net, 2020.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simon et al. (2016) Marcel Simon, Erik Rodner, and Joachim Denzler. Imagenet
    pre-trained models with batch normalization. *CoRR*, abs/1612.01452, 2016.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan & Zisserman (2015) Karen Simonyan and Andrew Zisserman. Very deep convolutional
    networks for large-scale image recognition. In *ICLR*, 2015.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So et al. (2019) David R. So, Quoc V. Le, and Chen Liang. The evolved transformer.
    In *Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA*, pp. 5877–5886, 2019.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In *Proceedings of the
    2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013,
    18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of
    SIGDAT, a Special Interest Group of the ACL*, pp.  1631–1642\. ACL, 2013.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Søgaard & Goldberg (2016) Anders Søgaard and Yoav Goldberg. Deep multi-task
    learning with low level tasks supervised at lower layers. In *ACL (2)*. The Association
    for Computer Linguistics, 2016.
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soudry et al. (2014) Daniel Soudry, Itay Hubara, and Ron Meir. Expectation
    backpropagation: Parameter-free training of multilayer neural networks with continuous
    or discrete weights. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D.
    Lawrence, and Kilian Q. Weinberger (eds.), *Advances in Neural Information Processing
    Systems 27: Annual Conference on Neural Information Processing Systems 2014, December
    8-13 2014, Montreal, Quebec, Canada*, pp.  963–971, 2014.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas & Babu (2015) Suraj Srinivas and R. Venkatesh Babu. Data-free parameter
    pruning for deep neural networks. In *BMVC*, pp.  31.1–31.12\. BMVA Press, 2015.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas & Fleuret (2018) Suraj Srinivas and François Fleuret. Knowledge transfer
    with jacobian matching. In Jennifer G. Dy and Andreas Krause (eds.), *Proceedings
    of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
    Stockholm, Sweden, July 10-15, 2018*, volume 80 of *Proceedings of Machine Learning
    Research*, pp.  4730–4738\. PMLR, 2018.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stanton et al. (2021) Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A.
    Alemi, and Andrew Gordon Wilson. Does knowledge distillation really work? *CoRR*,
    abs/2106.05945, 2021.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strubell et al. (2019) Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy
    and policy considerations for deep learning in NLP. In *Proceedings of the 57th
    Conference of the Association for Computational Linguistics, ACL 2019, Florence,
    Italy, July 28- August 2, 2019, Volume 1: Long Papers*, pp.  3645–3650, 2019.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sukhbaatar et al. (2019) Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski,
    and Armand Joulin. Adaptive attention span in transformers. In *ACL (1)*, pp. 
    331–335\. Association for Computational Linguistics, 2019.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019a) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge
    distillation for BERT model compression. In Kentaro Inui, Jing Jiang, Vincent
    Ng, and Xiaojun Wan (eds.), *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*,
    pp.  4322–4331\. Association for Computational Linguistics, 2019a.
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Tianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu Zhang, Hao
    Jiang, Zhao Cao, Xuanjing Huang, and Xipeng Qiu. Early exiting with ensemble internal
    classifiers. *CoRR*, abs/2105.13792, 2021.
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019b) Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen,
    Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. ERNIE: enhanced representation
    through knowledge integration. *CoRR*, abs/1904.09223, 2019b.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton & McCallum (2012) Charles Sutton and Andrew McCallum. An introduction
    to conditional random fields. *Found. Trends Mach. Learn.*, 4(4):267–373, 2012.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzuki et al. (2001) Kenji Suzuki, Isao Horiba, and Noboru Sugie. A simple neural
    network pruning algorithm with application to filter synthesis. *Neural Process.
    Lett.*, 13(1):43–53, 2001.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
    Rabinovich. Going deeper with convolutions. In *CVPR*, pp.  1–9\. IEEE Computer
    Society, 2015.
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2017) Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
    Alexander A. Alemi. Inception-v4, inception-resnet and the impact of residual
    connections on learning. In *AAAI*, pp.  4278–4284\. AAAI Press, 2017.
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tailor et al. (2021) Shyam Anil Tailor, Javier Fernandez-Marques, and Nicholas Donald
    Lane. Degree-quant: Quantization-aware training for graph neural networks. In
    *International Conference on Learning Representations*, 2021.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takase & Kiyono (2021) Sho Takase and Shun Kiyono. Lessons on parameter sharing
    across layers in transformers. *CoRR*, abs/2104.06022, 2021.
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tambe et al. (2021) Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu
    Jia, En-Yu Yang, Marco Donato, Victor Sanh, Paul N. Whatmough, Alexander M. Rush,
    David Brooks, and Gu-Yeon Wei. Edgebert: Sentence-level energy optimizations for
    latency-aware multi-task NLP inference. In *MICRO*, pp.  830–844\. ACM, 2021.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan & Le (2019) Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model
    scaling for convolutional neural networks. 97:6105–6114, 2019.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2019a) Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark
    Sandler, Andrew Howard, and Quoc V. Le. Mnasnet: Platform-aware neural architecture
    search for mobile. In *CVPR*, pp.  2820–2828\. Computer Vision Foundation / IEEE,
    2019a.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2019b) Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu.
    Multilingual neural machine translation with knowledge distillation. In *ICLR
    (Poster)*. OpenReview.net, 2019b.
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang & Kwan (1993) C.Z. Tang and H.K. Kwan. Multilayer feedforward neural networks
    with single powers-of-two weights. *IEEE Transactions on Signal Processing*, 41(8):2724–2727,
    1993. doi: 10.1109/78.229903.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tarvainen & Valpola (2017) Antti Tarvainen and Harri Valpola. Mean teachers
    are better role models: Weight-averaged consistency targets improve semi-supervised
    deep learning results. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
    Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), *Advances
    in Neural Information Processing Systems 30: Annual Conference on Neural Information
    Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, pp.  1195–1204,
    2017.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teerapittayanon et al. (2016) Surat Teerapittayanon, Bradley McDanel, and H. T.
    Kung. Branchynet: Fast inference via early exiting from deep neural networks.
    In *23rd International Conference on Pattern Recognition, ICPR 2016, Cancún, Mexico,
    December 4-8, 2016*, pp.  2464–2469\. IEEE, 2016.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2020) Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive
    multiview coding. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael
    Frahm (eds.), *Computer Vision - ECCV 2020 - 16th European Conference, Glasgow,
    UK, August 23-28, 2020, Proceedings, Part XI*, volume 12356 of *Lecture Notes
    in Computer Science*, pp.  776–794\. Springer, 2020.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tung & Mori (2019) Frederick Tung and Greg Mori. Similarity-preserving knowledge
    distillation. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV
    2019, Seoul, Korea (South), October 27 - November 2, 2019*, pp. 1365–1374\. IEEE,
    2019.
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upadhyay et al. (2016) Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and Dan
    Roth. Cross-lingual models of word embeddings: An empirical comparison. In *ACL
    (1)*. The Association for Computer Linguistics, 2016.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van den Oord et al. (2018) Aäron van den Oord, Yazhe Li, and Oriol Vinyals.
    Representation learning with contrastive predictive coding. *CoRR*, abs/1807.03748,
    2018.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanhoucke et al. (2011) Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. Improving
    the speed of neural networks on CPUs. In *Proc. Deep Learning and Unsupervised
    Feature Learning NIPS Workshop*, 2011.
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *NIPS*, pp.  5998–6008, 2017.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veit & Belongie (2020) Andreas Veit and Serge J. Belongie. Convolutional networks
    with adaptive inference graphs. *Int. J. Comput. Vis.*, 128(3):730–741, 2020.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vijayanarasimhan et al. (2015) Sudheendra Vijayanarasimhan, Jonathon Shlens,
    Rajat Monga, and Jay Yagnik. Deep networks with large output spaces. In *ICLR
    (Workshop)*, 2015.
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola & Jones (2001) Paul Viola and Michael Jones. Rapid object detection using
    a boosted cascade of simple features. In *Proceedings of the 2001 IEEE computer
    society conference on computer vision and pattern recognition. CVPR 2001*, volume 1,
    pp.  I–I. Ieee, 2001.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola & Jones (2004) Paul Viola and Michael J Jones. Robust real-time face detection.
    *International journal of computer vision*, 57(2):137–154, 2004.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. In *Proceedings of the 57th Conference
    of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
    28- August 2, 2019, Volume 1: Long Papers*, pp.  5797–5808, 2019.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. (2021) Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel
    Cer. Spot: Better frozen model adaptation through soft prompt transfer. *arXiv
    preprint arXiv:2110.07904*, 2021.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform
    for natural language understanding. In *7th International Conference on Learning
    Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net,
    2019a.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine
    translation with byte-level subwords. In *AAAI*, pp.  9154–9160\. AAAI Press,
    2020a.
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Hanchen Wang, Defu Lian, Ying Zhang, Lu Qin, Xiangjian He,
    Yiguang Lin, and Xuemin Lin. Binarized graph neural network. *World Wide Web*,
    24(3):825–848, 2021a.
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Linnan Wang, Wei Wu, Zenglin Xu, Jianxiong Xiao, and Yi Yang.
    BLASX: A high performance level-3 BLAS library for heterogeneous multi-gpu computing.
    In *ICS*, pp.  20:1–20:11\. ACM, 2016.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018a) Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon
    Song, Zenglin Xu, and Tim Kraska. Superneurons: dynamic GPU memory management
    for training deep neural networks. In *PPOPP*, pp.  41–53\. ACM, 2018a.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Min Wang, Baoyuan Liu, and Hassan Foroosh. Factorized convolutional
    neural networks. In *ICCV Workshops*, pp.  545–553\. IEEE Computer Society, 2017.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018b) Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng
    Wang, and Yuan Xie. Hitnet: Hybrid ternary recurrent neural network. In Samy Bengio,
    Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman
    Garnett (eds.), *Advances in Neural Information Processing Systems 31: Annual
    Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
    3-8, 2018, Montréal, Canada*, pp.  602–612, 2018b.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019b) Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Distilling
    object detectors with fine-grained feature imitation. In *IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*,
    pp. 4933–4942\. Computer Vision Foundation / IEEE, 2019b.
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang & Isola (2020) Tongzhou Wang and Phillip Isola. Understanding contrastive
    representation learning through alignment and uniformity on the hypersphere. In
    *Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
    13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning
    Research*, pp.  9929–9939\. PMLR, 2020.
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020b) Wei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi
    Bao, Liwei Peng, and Luo Si. Structbert: Incorporating language structures into
    pre-training for deep language understanding. In *8th International Conference
    on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*.
    OpenReview.net, 2020b.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, and
    Lei Li. LightSeq: A high performance inference library for transformers. In *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT)*, pp.  113–120\.
    Association for Computational Linguistics, June 2021b.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018c) Xin Wang, Yujia Luo, Daniel Crankshaw, Alexey Tumanov,
    Fisher Yu, and Joseph E. Gonzalez. IDK cascades: Fast deep learning by learning
    not to overthink. In Amir Globerson and Ricardo Silva (eds.), *Proceedings of
    the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018,
    Monterey, California, USA, August 6-10, 2018*, pp.  580–590\. AUAI Press, 2018c.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018d) Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E.
    Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In Vittorio
    Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), *Computer
    Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14,
    2018, Proceedings, Part XIII*, volume 11217 of *Lecture Notes in Computer Science*,
    pp.  420–436\. Springer, 2018d.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Yong Wang, Longyue Wang, Victor O. K. Li, and Zhaopeng Tu.
    On the sparsity of neural machine translation models. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
    November 16-20, 2020*, pp.  1060–1066, 2020c.
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020d) Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang,
    and Gao Huang. Glance and focus: a dynamic approach to reducing spatial redundancy
    in image classification. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
    Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020d.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021c) Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao
    Huang. Not all images are worth 16x16 words: Dynamic vision transformers with
    adaptive sequence length. *CoRR*, abs/2105.15075, 2021c.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warstadt et al. (2019) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman.
    Neural network acceptability judgments. *Trans. Assoc. Comput. Linguistics*, 7:625–641,
    2019.
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models
    are zero-shot learners. *CoRR*, abs/2109.01652, 2021.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R. Bowman.
    A broad-coverage challenge corpus for sentence understanding through inference.
    In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), *Proceedings of the 2018
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana,
    USA, June 1-6, 2018, Volume 1 (Long Papers)*, pp.  1112–1122\. Association for
    Computational Linguistics, 2018.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Winata et al. (2019) Genta Indra Winata, Andrea Madotto, Jamin Shin, Elham J.
    Barezi, and Pascale Fung. On the effectiveness of low-rank matrix factorization
    for LSTM model compression. *CoRR*, abs/1908.09982, 2019.
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei
    Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet:
    Hardware-aware efficient convnet design via differentiable neural architecture
    search. In *CVPR*, pp.  10734–10742\. Computer Vision Foundation / IEEE, 2019.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2018a) Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and
    inference with integers in deep neural networks. In *6th International Conference
    on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
    3, 2018, Conference Track Proceedings*. OpenReview.net, 2018a.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu & He (2020) Yuxin Wu and Kaiming He. Group normalization. *Int. J. Comput.
    Vis.*, 128(3):742–755, 2020.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun,
    and Hao Ma. CLEAR: contrastive learning for sentence representation. *CoRR*, abs/2012.15466,
    2020.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2018b) Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie,
    Larry S. Davis, Kristen Grauman, and Rogério Schmidt Feris. Blockdrop: Dynamic
    inference paths in residual networks. In *2018 IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*,
    pp. 8817–8826\. Computer Vision Foundation / IEEE Computer Society, 2018b.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiang et al. (2020) Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from
    multiple experts: Self-paced knowledge distillation for long-tailed classification.
    In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), *Computer
    Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020,
    Proceedings, Part V*, volume 12350 of *Lecture Notes in Computer Science*, pp. 
    247–263\. Springer, 2020.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xin et al. (2020) Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy
    Lin. Deebert: Dynamic early exiting for accelerating BERT inference. In Dan Jurafsky,
    Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,
    Online, July 5-10, 2020*, pp. 2246–2251\. Association for Computational Linguistics,
    2020.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xin et al. (2021) Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. Berxit:
    Early exiting for BERT with better fine-tuning and extension to regression. In
    Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty (eds.), *Proceedings of the 16th
    Conference of the European Chapter of the Association for Computational Linguistics:
    Main Volume, EACL 2021, Online, April 19 - 23, 2021*, pp.  91–104\. Association
    for Computational Linguistics, 2021.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020) Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.
    Bert-of-theseus: Compressing BERT by progressive module replacing. In Bonnie Webber,
    Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November
    16-20, 2020*, pp. 7859–7869\. Association for Computational Linguistics, 2020.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021a) Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian J. McAuley,
    and Furu Wei. Beyond preserved accuracy: Evaluating loyalty and robustness of
    BERT compression. *CoRR*, abs/2109.03228, 2021a.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang
    Lin. Understanding and improving layer normalization. In *NeurIPS*, pp.  4383–4393,
    2019.
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021b) Jingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun,
    and Hongxia Yang. KNAS: green neural architecture search. In *ICML*, volume 139
    of *Proceedings of Machine Learning Research*, pp.  11613–11625\. PMLR, 2021b.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018) Kaisheng Xu, Xu Shen, Ting Yao, Xinmei Tian, and Tao Mei. Greedy
    layer-wise training of long short term memory networks. In *2018 IEEE International
    Conference on Multimedia & Expo Workshops, ICME Workshops 2018, San Diego, CA,
    USA, July 23-27, 2018*, pp. 1–6\. IEEE Computer Society, 2018.
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2014) Jian Xue, Jinyu Li, Dong Yu, Mike Seltzer, and Yifan Gong.
    Singular value decomposition based low-footprint speaker adaptation and personalization
    for deep neural network. In *IEEE International Conference on Acoustics, Speech
    and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014*, pp. 6359–6363,
    2014.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual
    pre-trained text-to-text transformer. In Kristina Toutanova, Anna Rumshisky, Luke
    Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy
    Chakraborty, and Yichao Zhou (eds.), *Proceedings of the 2021 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*, pp.  483–498\.
    Association for Computational Linguistics, 2021.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020a) Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He,
    and Jingqiao Zhang. Progressively stacking 2.0: A multi-stage layerwise training
    method for BERT training speedup. *CoRR*, abs/2011.13635, 2020a.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2020b) Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and
    Gao Huang. Resolution adaptive networks for efficient inference. In *2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
    USA, June 13-19, 2020*, pp. 2366–2375\. Computer Vision Foundation / IEEE, 2020b.
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2017) Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen.
    Transfer learning for sequence tagging with hierarchical recurrent networks. In
    *ICLR (Poster)*. OpenReview.net, 2017.
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell,
    Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining
    for language understanding. In *Advances in Neural Information Processing Systems
    32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, 2019, Vancouver, BC, Canada*, pp.  5754–5764, 2019.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yim et al. (2017) Junho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim. A gift
    from knowledge distillation: Fast optimization, network minimization and transfer
    learning. In *2017 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2017, Honolulu, HI, USA, July 21-26, 2017*, pp. 7130–7138\. IEEE Computer
    Society, 2017.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ying et al. (2019) Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real,
    Kevin Murphy, and Frank Hutter. Nas-bench-101: Towards reproducible neural architecture
    search. In *Proceedings of the 36th International Conference on Machine Learning,
    ICML 2019, 9-15 June 2019, Long Beach, California, USA*, pp. 7105–7114, 2019.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoo & Kweon (2019) Donggeun Yoo and In So Kweon. Learning loss for active learning.
    In *CVPR*, pp.  93–102\. Computer Vision Foundation / IEEE, 2019.
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yosinski et al. (2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
    How transferable are features in deep neural networks? In *NIPS*, pp.  3320–3328,
    2014.
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You et al. (2017) Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from
    multiple teacher networks. In *Proceedings of the 23rd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August
    13 - 17, 2017*, pp.  1285–1294\. ACM, 2017.
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2017) Adams Wei Yu, Hongrae Lee, and Quoc V. Le. Learning to skim
    text. In Regina Barzilay and Min-Yen Kan (eds.), *Proceedings of the 55th Annual
    Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,
    Canada, July 30 - August 4, Volume 1: Long Papers*, pp. 1880–1890\. Association
    for Computational Linguistics, 2017.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu & Huang (2019) Jiahui Yu and Thomas S. Huang. Universally slimmable networks
    and improved training techniques. In *2019 IEEE/CVF International Conference on
    Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019*,
    pp. 1803–1811\. IEEE, 2019.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019) Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas S.
    Huang. Slimmable neural networks. In *7th International Conference on Learning
    Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net,
    2019.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Keyi Yu, Yang Liu, Alexander G. Schwing, and Jian Peng. Fast
    and accurate text classification: Skimming, rereading and early stopping. In *6th
    International Conference on Learning Representations, ICLR 2018, Vancouver, BC,
    Canada, April 30 - May 3, 2018, Workshop Track Proceedings*. OpenReview.net, 2018.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu & Zhu (2020) Tong Yu and Hong Zhu. Hyper-parameter optimization: A review
    of algorithms and applications. *CoRR*, abs/2003.05689, 2020.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    Q8BERT: quantized 8bit BERT. *CoRR*, abs/1910.06188, 2019.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zagoruyko & Komodakis (2017) Sergey Zagoruyko and Nikos Komodakis. Paying more
    attention to attention: Improving the performance of convolutional neural networks
    via attention transfer. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net,
    2017.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In *NeurIPS*,
    2020.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler & Fergus (2014) Matthew D. Zeiler and Rob Fergus. Visualizing and understanding
    convolutional networks. In *ECCV (1)*, volume 8689 of *Lecture Notes in Computer
    Science*, pp.  818–833\. Springer, 2014.
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zela et al. (2020) Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi,
    Thomas Brox, and Frank Hutter. Understanding and robustifying differentiable architecture
    search. In *8th International Conference on Learning Representations, ICLR 2020,
    Addis Ababa, Ethiopia, April 26-30, 2020*, 2020.
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019) Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Fixup initialization:
    Residual learning without normalization. In *ICLR (Poster)*. OpenReview.net, 2019.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang & Stadie (2020) Matthew Shunshi Zhang and Bradly C. Stadie. One-shot pruning
    of recurrent neural networks by jacobian spectrum evaluation. In *8th International
    Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
    26-30, 2020*, 2020.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit BERT. In
    Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,
    Online, November 16-20, 2020*, pp. 509–521\. Association for Computational Linguistics,
    2020.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Ying Zhang, Tao Xiang, Timothy M. Hospedales, and Huchuan
    Lu. Deep mutual learning. In *2018 IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*, pp. 4320–4328\.
    IEEE Computer Society, 2018.
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Zhi Zhang, Guanghan Ning, and Zhihai He. Knowledge projection
    for deep neural networks. *CoRR*, abs/1710.09505, 2017.
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020) Yiren Zhao, Duo Wang, Daniel Bates, Robert D. Mullins, Mateja
    Jamnik, and Pietro Liò. Learned low precision graph neural networks. *CoRR*, abs/2009.09232,
    2020.
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2015) Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes,
    Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr. Conditional
    random fields as recurrent neural networks. In *2015 IEEE International Conference
    on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015*, pp.  1529–1537,
    2015.
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2019) Mingyi Zhou, Yipeng Liu, Zhen Long, Longxi Chen, and Ce Zhu.
    Tensor rank learning in CP decomposition via convolutional neural network. *Signal
    Process. Image Commun.*, 73:12–21, 2019.
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020) Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley,
    Ke Xu, and Furu Wei. BERT loses patience: Fast and robust inference with early
    exit. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
    and Hsuan-Tien Lin (eds.), *Advances in Neural Information Processing Systems
    33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
    December 6-12, 2020, virtual*, 2020.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021a) Wangchunshu Zhou, Tao Ge, Ke Xu, and Furu Wei. Improving
    sequence-to-sequence pre-training via sequence span rewriting. *CoRR*, abs/2101.00416,
    2021a.
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021b) Wangchunshu Zhou, Canwen Xu, and Julian J. McAuley. Meta
    learning for knowledge distillation. *CoRR*, abs/2106.04570, 2021b.
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu (2021) Wei Zhu. Leebert: Learned early exit for BERT with cross-level optimization.
    In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
    2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pp.  2968–2980\.
    Association for Computational Linguistics, 2021.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
